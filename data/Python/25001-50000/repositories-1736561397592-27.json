{
  "metadata": {
    "timestamp": 1736561397592,
    "page": 27,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "apache/airflow",
      "stars": 38241,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".asf.yaml",
          "type": "blob",
          "size": 4.7509765625,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# https://cwiki.apache.org/confluence/display/INFRA/git+-+.asf.yaml+features\n---\ngithub:\n  description: \"Apache Airflow - A platform to programmatically author, schedule, and monitor workflows\"\n  homepage: https://airflow.apache.org/\n  # Social media preview image is not supported by Github API/asf.yaml, need to be uploaded\n  # manually in Github repository --> Settings --> click \"Edit\" in \"Social preview\"\n  # See also:\n  # https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/customizing-your-repositorys-social-media-preview\n  # social_media_preview: docs/apache-airflow/img/logos/github_repository_social_image.png\n  labels:\n    # Note that Github only supports <=20 labels/topics per repo! Pipeline will fail if you add more.\n    - airflow\n    - apache\n    - apache-airflow\n    - automation\n    - dag\n    - data-engineering\n    - data-integration\n    - data-orchestrator\n    - data-pipelines\n    - data-science\n    - elt\n    - etl\n    - machine-learning\n    - mlops\n    - orchestration\n    - python\n    - scheduler\n    - workflow\n    - workflow-engine\n    - workflow-orchestration\n  features:\n    # Enable issues management\n    issues: true\n    # Enable projects for project management boards\n    projects: true\n    # Enable wiki for documentation\n    wiki: false\n\n  enabled_merge_buttons:\n    squash: true\n    merge: false\n    rebase: false\n\n  protected_branches:\n    main:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_conversation_resolution: true\n    v1-10-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n    v2-0-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n    v2-1-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n    v2-2-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n    v2-3-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n    v2-4-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n    v2-5-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n    v2-6-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n    v2-7-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n    v2-8-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n    v2-9-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n    v2-10-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n    v2-10-test:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_conversation_resolution: true\n    providers-fab/v1-5:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_conversation_resolution: true\n  collaborators:\n    # Max 10 collaborators allowed\n    # https://github.com/apache/infrastructure-asfyaml/blob/main/README.md#assigning-the-github-triage-role-to-external-collaborators\n    - aritra24\n    - omkar-foss\n    - rawwar\n    - nathadfield\n    - sunank200\n    - vatsrahul1001\n    - cmarteepants\n    - bugraoz93\n    - briana-okyere\n\nnotifications:\n  jobs: jobs@airflow.apache.org\n  commits: commits@airflow.apache.org\n  issues: commits@airflow.apache.org\n  pullrequests: commits@airflow.apache.org\n  discussions: commits@airflow.apache.org\n"
        },
        {
          "name": ".bash_completion",
          "type": "blob",
          "size": 0.88671875,
          "content": "#!/usr/bin/env bash\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfor BCFILE in \"${HOME}\"/.bash_completion.d/* ; do\n  # shellcheck disable=SC1090\n  . \"${BCFILE}\"\ndone\n"
        },
        {
          "name": ".cherry_picker.toml",
          "type": "blob",
          "size": 0.9365234375,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nteam = \"apache\"\nrepo = \"airflow\"\n\"check_sha\"= \"a85d94e6cdcd09efe93c3acee0b4ce5c9508bc23\"\nfix_commit_msg = false\ndefault_branch = \"main\"\nrequire_version_in_branch_name=false\n"
        },
        {
          "name": ".codespellignorelines",
          "type": "blob",
          "size": 0.4150390625,
          "content": "            f\"DELETE {source_table} FROM { ', '.join(_from_name(tbl) for tbl in stmt.froms) }\"\n        for frm in source_query.selectable.froms:\n    roles = relationship(\"Role\", secondary=assoc_user_role, backref=\"user\", lazy=\"selectin\")\n    The platform supports **C**reate, **R**ead, **U**pdate, and **D**elete operations on most resources.\n<pre><code>Code block\\ndoes not\\nrespect\\nnewlines\\n</code></pre>\n      \"trough\",\n"
        },
        {
          "name": ".devcontainer",
          "type": "tree",
          "content": null
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 2.9951171875,
          "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# NOTE! This docker ignore uses recommended technique\n# Where everything is excluded by default and you deliberately\n# Add only those directories/files you need. This is very useful\n# To make sure that Docker context is always the same on any machine\n# So that generated files are not accidentally added to the context\n# This allows Docker's `COPY .` to behave in predictable way\n\n# Ignore everything\n**\n\n# Allow only these directories\n!airflow\n!common\n!dags\n!dev\n!chart\n!docs\n!licenses\n!providers/\n!task_sdk/\n\n# Add those folders to the context so that they are available in the CI container\n!scripts\n\n# Add tests and kubernetes_tests to context.\n!tests\n!tests_common\n!kubernetes_tests\n!helm_tests\n!docker_tests\n\n!.rat-excludes\n!.dockerignore\n!RELEASE_NOTES.rst\n!LICENSE\n!NOTICE\n!.github\n!empty\n!Dockerfile\n!hatch_build.py\n!prod_image_installed_providers.txt\n\n# This folder is for you if you want to add any packages to the docker context when you build your own\n# docker image. most of other files and any new folder you add will be excluded by default\n!docker-context-files\n!constraints\n\n# Avoid triggering context change on README change (new companies using Airflow)\n# So please do not uncomment this line ;)\n# !README.md\n\n# Run tests command with bash completion\n!.bash_completion\n!.bash_completion.d\n\n# Setup/version configuration\n!pyproject.toml\n!manifests\n!generated\n# Now - ignore unnecessary files inside allowed directories\n# This goes after the allowed directories\n\n# Git version is dynamically generated\nairflow/git_version\n# Exclude mode_modules pulled by \"yarn\" for compilation of www files generated by NPM\nairflow/www/node_modules\nairflow/ui/node_modules\n\n# Exclude link to docs\nairflow/www/static/docs\n\n# Exclude python generated files\n**/__pycache__/\n**/*.py[cod]\n**/*$py.class\n**/.pytest_cache/\n**/env/\n**/build/\n**/develop-eggs/\n/dist/\n**/downloads/\n**/eggs/\n**/.eggs/\n**/lib/\n**/lib64/\n**/parts/\n**/sdist/\n**/var/\n**/wheels/\n**/*.egg-info/\n**/.installed.cfg\n**/*.egg\n\n# Exclude temporary vi files\n**/*~\n\n# Exclude output files\n**/*.out\n**/hive_scratch_dir/\n\n# Exclude auto-generated Finder files on Mac OS\n**/.DS_Store\n**/Thumbs.db\n\n# Exclude docs generated files\ndocs/_build/\ndocs/_api/\ndocs/_doctrees/\n\n# files generated by memray\n*.py.*.html\n*.py.*.bin\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 1.19921875,
          "content": "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nroot = true\n\n[*]\nend_of_line = lf\nindent_style = space\ninsert_final_newline = true\ntrim_trailing_whitespace = true\ncharset = utf-8\n\n[*.py]\nindent_size = 4\nmax_line_length = 110\n\n[*.sh]\nindent_size = 4\n\n[*.sql]\nindent_size = 4\n\n[*.js]\nindent_size = 2\n\n[*.ts]\nindent_size = 2\n\n[*.css]\nindent_size = 2\n\n[*.{md,rst}]\nindent_size = 2\n\n[*.{yml,yaml}]\nindent_size = 2\nmax_line_length = 110\n\n[*.{htm,html}]\nindent_size = 2\n\n[*.json]\nindent_size = 4\n"
        },
        {
          "name": ".git-blame-ignore-revs",
          "type": "blob",
          "size": 0.4541015625,
          "content": "# Black enabled.\n4e8f9cc8d02b29c325b8a5a76b4837671bdf5f68\nfdd9b6f65b608c516b8a062b058972d9a45ec9e3\n\n# PEP-563 (Postponed Evaluation of Annotations).\nd67ac5932dabbf06ae733fc57b48491a8029b8c2\n\n# Mass converting string literals to use double quotes.\n2a34dc9e8470285b0ed2db71109ef4265e29688b\nbfcae349b88fd959e32bfacd027a5be976fe2132\n01a819a42daa7990c30ab9776208b3dcb9f3a28b\n\n# Mass of newlines added after the module docstring.\n0a74928894fb57b0160208262ccacad12da23fc7\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.810546875,
          "content": "clients export-ignore\nclients export-ignore\ndev export-ignore\ndocker-context-files export-ignore\ndocker_tests export-ignore\nempty export-ignore\nkubernetes_tests export-ignore\nmanifests export-ignore\nnewsfragments export-ignore\nscripts export-ignore\ntests export-ignore\nchart/charts/** export-ignore\n\nDockerfile.ci export-ignore\n\nISSUE_TRIAGE_PROCESS.rst export-ignore\nCONTRIBUTING.rst export-ignore\ncontributing_docs/ export-ignore\n\n.devcontainer export-ignore\n.github export-ignore\n.readthedocs.yml export-ignore\n.hadolint.yaml export-ignore\n.pre-commit-config.yaml export-ignore\n.mailmap export-ignore\n.editorconfig export-ignore\n.inputrc export-ignore\n.codespellignorelines export-ignore\n.gitmodules export-ignore\n.gitpod.yml export-ignore\n.markdownlint.yml export-ignore\n.bash_completion export-ignore\n.asf.yaml export-ignore\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.5439453125,
          "content": "# Airflow configuration\nairflow.cfg\nairflow_login.py\ndbinit.py\ninitdb.py\nsecrets.py\n\n# Airflow sqlite databases\nairflow.db\n\n# Airflow temporary artifacts\nairflow/git_version\nairflow/www/static/coverage/\nairflow/www/*.log\nairflow/ui/coverage/\nlogs/\nairflow-webserver.pid\nstandalone_admin_password.txt\nwarnings.txt\nwarn-summary-*.txt\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n.pytest_cache/\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage*.xml\n*,cover\n.hypothesis/\n.pytest_cache\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n# *.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n/webserver_config.py\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\ndocs/rtd-deprecation/_build/\ndocs/_doctrees/\ndocs/_inventory_cache/\ndocs/*/_api/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# nvm (Node Version Manager)\n.nvmrc\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n.env.local\n.autoenv*.zsh\n\n# virtualenv\n.venv*\nvenv*\nENV/\n\n# Spyder project settings\n.spyderproject\n\n# Rope project settings\n.ropeproject\n\n# PyCharm\n.idea/\n*.iml\n\n# vim\n*.swp\n\n# Emacs\n*~\n\\#*\\#\n/.emacs.desktop\n/.emacs.desktop.lock\n*.elc\nauto-save-list\ntramp\n.\\#*\n\n# OSX\n.DS_Store\n\n# SQL Server backups\n*.bkp\n\n# Spark\nrat-results.txt\n\n# Git stuff\n# Kubernetes generated templated files\n*.generated\n*.tar.gz\nscripts/ci/kubernetes/kube/.generated/airflow.yaml\nscripts/ci/kubernetes/docker/requirements.txt\n\n# Node & Webpack Stuff\n*.entry.js\nnode_modules\nnpm-debug.log*\nderby.log\nmetastore_db\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\npnpm-debug.log*\n.vscode/*\n!.vscode/extensions.json\n/.vite/\n.pnpm-store\n*.tsbuildinfo\n\n# Airflow log files when airflow is run locally\nairflow-*.err\nairflow-*.out\nairflow-*.log\nairflow-*.pid\n.airflow_db_initialised\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Needed for CI Dockerfile.ci build system\n.build\n/tmp\n/files\n\n/hive_scratch_dir/\n/.bash_aliases\n/.bash_history\n/.kube\n/.inputrc\nlog.txt*\n\n# Provider-related ignores\n/airflow/providers/__init__.py\n\n# Docker context files\n/docker-context-files/*\n!/docker-context-files/.README.md\n# Local .terraform directories\n**/.terraform/*\n\n# .tfstate files\n*.tfstate\n*.tfstate.*\n\n# Terraform variables\n*.tfvars\n\n# Might be generated when you build wheels\npip-wheel-metadata\n\n.pypirc\n\n/.docs-venv\n\n# Dev files\n/dev/packages.txt\n/dev/Dockerfile.pmc\n\n# Generated UI licenses\n3rd-party-licenses/LICENSES-ui.txt\nlicenses/LICENSES-ui.txt\n\n# Packaged breeze on Windows\n/breeze.exe\n\n# Generated out dir\n\n/out\n\n# files generated by memray\n*.py.*.html\n*.py.*.bin\n\n# used to checkout target-branch in CI\n/target-airflow\n\n# This directory used for generate provider packages before https://github.com/apache/airflow/pull/35617\n/provider_packages/\n\n# This directory used for store autogenerated images\n/images\n\n# Dask Executor tests generate this directory\n/tests/executors/dask-worker-space/\n\n# airflow-build-dockerfile and correconding ignore file\nairflow-build-dockerfile*\n\n# Temporary ignore uv.lock until we integrate it fully in our constraint preparation mechanism\n/uv.lock\n"
        },
        {
          "name": ".gitpod.yml",
          "type": "blob",
          "size": 1.3701171875,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n---\n\n# Reference: https://www.gitpod.io/docs/references/gitpod-yml\n# Docs: https://www.gitpod.io/docs/config-gitpod-file/\n\n# default \"gitpod/workspace-full\" python version > 3.11\nimage: gitpod/workspace-python-3.11\n\ntasks:\n  - init: ./scripts/ci/install_breeze.sh\n  - name: Install pre-commit\n    openMode: split-right\n    command: |\n      printf '%s\\n' \"export PIP_USER=no\" >> \"$HOME/.bashrc\"\n      source \"$HOME/.bashrc\"\n      pip install pre-commit\n      pre-commit install\n      echo \"for running integration test with breeze\"\n\n# Ports to expose on workspace startup\nports:\n  - port: 8000\n    onOpen: open-preview\n"
        },
        {
          "name": ".hadolint.yaml",
          "type": "blob",
          "size": 0.8330078125,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n---\nignored:\n  - DL3006\n  - DL3008\n  - DL3005\n  - DL3013\n  - SC1091\n"
        },
        {
          "name": ".mailmap",
          "type": "blob",
          "size": 5.783203125,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nAnita Fronczak <afronczak@google.com> <akarbow@gmail.com>\nAntony Mayi <antonymayi@users.noreply.github.com>\nArthur Wiedmer <arthur@apache.org> <arthur_wiedmer@arthur-wiedmer.local>\nArthur Wiedmer <arthur@apache.org> <arthur.wiedmer@airbnb.com>\nArthur Wiedmer <arthur@apache.org> <arthur.wiedmer@gmail.com>\nBolke de Bruin <bolke@xs4all.nl> <bolke@Bolkes-MacBook-Pro.local>\nBolke de Bruin <bolke@xs4all.nl> <bolkedebruin@users.noreply.github.com>\nBenjamin Grenier <52801483+benjamingrenier@users.noreply.github.com>\nChip Myers Jr. <chip.myersjr@gmail.com>\nChris Riccomini <criccomini@apache.org> <chrisr@wepay.com>\nChris Riccomini <criccomini@apache.org> <criccomini@users.noreply.github.com>\nDan Davydov <ddavydov@twitter.com> <dan.davydov@airbnb.com>\nDaniel Imberman <daniel.imberman@gmail.com> <daniel@astronomer.io>\nDaniel Imberman <daniel.imberman@gmail.com> <danielryan2430@gmail.com>\nDaniel Standish <dstandish@users.noreply.github.com>\nDependabot [bot] <49699333+dependabot[bot]@users.noreply.github.com>\nEphraim Anierobi <splendidzigy24@gmail.com> <4122866+ephraimbuddy@users.noreply.github.com>\nFelix Uellendall <feluelle@users.noreply.github.com> <feluelle@users.noreply.github.com>\nFeng Lu <fenglu@fengcloud.hot.corp.google.com> <fenglu@google.com>\nFokko Driesprong <fokko@apache.org> <fokko@driesprong.frl>\nFokko Driesprong <fokko@apache.org> <fokkodriesprong@godatadriven.com>\nGerard Toonstra <gtoonstra@gmail.com>\nGreg Neiheisel <greg@astronomer.io>\nHossein Torabi <hossein.torabi@alopeyk.com> <blcksrx@gmail.com>\nJames Timmins <james@astronomer.io> <jameshtimmins@gmail.com>\nJarek Potiuk <jarek@potiuk.com> <jarek.potiuk@polidea.com> <potiuk@apache.org>\nJeremiah Lowin <jlowin@apache.org>\nJeremiah Lowin <jlowin@apache.org> <jlowin@gmail.com>\nJeremiah Lowin <jlowin@apache.org> <jlowin@iHal.local>\nJeremiah Lowin <jlowin@apache.org> <jlowin@Jeremiahs-MacBook-Air.local>\nJeremiah Lowin <jlowin@apache.org> <jlowin@jlowin.local>\nJeremiah Lowin <jlowin@apache.org> <jlowin@users.noreply.github.com>\nJiajie Zhong <zhongjiajie955@hotmail.com>\nJorge Machado <jorge.w.machado@hotmail.com>\nJoseph Yen <joseph.yen@gmail.com>\nJoy Gao <Joygao@apache.org> <joyg@wepay.com>\nKamil Breguła <mik-laj@users.noreply.github.com> <kamil.bregula@polidea.com>\nKaveri Sharma <kaverisharma09@gmail.com>\nKaxil Naik <kaxilnaik@apache.org>\nKaxil Naik <kaxilnaik@apache.org> <kaxilnaik@gmail.com>\nKousuke Saruta <sarutak@oss.nttdata.co.jp>\nKousuke Saruta <sarutak@oss.nttdata.co.jp> <sarutak@oss.nttdata.com>\nMarcin Szymański <ms32035@gmail.com>\nMartijn Pieters <mj@zopatista.com>\nMartijn Pieters <mj@zopatista.com> <github.com@zopatista.com>\nMatthew Bruce <matthew.bruce@shopify.com> <matthew.robert.bruce@gmail.com>\nMaxime Beauchemin <maxime.beauchemin@apache.org> <maxime_beauchemin@i-0963e1d9.inst.aws.airbnb.com>\nMaxime Beauchemin <maxime.beauchemin@apache.org> <maxime_beauchemin@i-a94af485.inst.aws.airbnb.com>\nMaxime Beauchemin <maxime.beauchemin@apache.org> <maxime.beauchemin@airbnb.com>\nMaxime Beauchemin <maxime.beauchemin@apache.org> <maximebeauchemin@gmail.com>\nMaxime Beauchemin <maxime.beauchemin@apache.org> <maxime_beauchemin@maxime-beauchemin.local>\nMichael Chirico <michaelchirico4@gmail.com>\nMichael Chirico <michaelchirico4@gmail.com> <michael.chirico@grabtaxi.com>\nMichał Słowikowski <michalslowikowski00@gmail.com>\nMustafa Gök <sd.mustafagok@gmail.com>\nMustafa Gök <sd.mustafagok@gmail.com> <gokmust@itu.edu.tr>\nNiels Zeilemaker <niels@zeilemaker.nl>\nNiels Zeilemaker <niels@zeilemaker.nl> <nielszeilemaker@godatadriven.com>\nNikolay Kolev <nikolays@wepay.com>\nPatrick McKenna <patrick.b.mckenna@gmail.com>\nPatrick McKenna <patrick.b.mckenna@gmail.com> <patrickmckenna@github.com>\nPatrick Leo Tardif <plt@rdif.me>\nPatrick Leo Tardif <plt@rdif.me> <patrick_tardif@unassigned0391.local>\nPeng Chen <pengchen@xiaohongshu.com> <348707924@qq.com>\nPradeep Bhadani <pradeep.bhadani@gmail.com>\nRafael Bottega <rbottega@worldremit.com> <boittega@gmail.com>\nSergio Kefalas <sergiokef@gmail.com>\nSevak Avetisyan <sevak.avet@gmail.com>\nSid Anand <r39132@gmail.com> <sanand@agari.com>\nSid Anand <r39132@gmail.com> <sianand@paypal.com>\nSid Anand <r39132@gmail.com> <siddharthanand@yahoo.com>\nSumit Maheshwari <sumeet.manit@gmail.com> <smaheshwari@twitter.com>\nSumit Maheshwari <sumeet.manit@gmail.com> <sumitm@qubole.com>\nTao Feng <fengtao04@gmail.com> <tfeng@lyft.com>\nTobiasz Kedzierski <tobiasz.kedzierski@polidea.com>\nTomek Kzukowski <tomek@tomekzukowski.com>\nTomek Urbaszek <turbaszek@apache.org> <turbaszek@gmail.com>\nTomek Urbaszek <turbaszek@apache.org> <tomasz.urbaszek@polidea.com>\nXiaodong Deng <xd_deng@hotmail.com>\nXiaodong Deng <xd_deng@hotmail.com> <11539188+XD-DENG@users.noreply.github.com>\nXiaodong Deng <xd_deng@hotmail.com> <xd.deng.r@gmail.com>\nYuen-Kuei Hsueh <ph.study@gmail.com>\nZikun Zhu <33176974+zikun@users.noreply.github.com>\nAshish Patel <ashishpatel0720@gmail.com> <ashish.patel@walmart.com>\nAshish Patel <ashishpatel0720@gmail.com> <ashish.patel@walmartlabs.com>\nAshish Patel <ashishpatel0720@gmail.com> <ashish.patel@skyscanner.net>\nAshish Patel <ashishpatel0720@gmail.com> <ashishpatel@skyscanner.net>\n"
        },
        {
          "name": ".markdownlint.yml",
          "type": "blob",
          "size": 1.76953125,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n---\n# https://github.com/DavidAnson/markdownlint#rules--aliases\n# MD004 ul-style - Unordered list style\nMD004: false\n\n# MD007 ul-indent - Unordered list indentation\nMD007: false\n\n# MD012 no-multiple-blanks - Multiple consecutive blank lines\nMD012: false\n\n# MD013 line-length - Line length\nMD013: false\n\n# MD024 no-duplicate-heading/no-duplicate-header - Multiple headings with the same content\nMD024: false\n\n# MD026 no-trailing-punctuation - Trailing punctuation in heading\nMD026: false\n\n# MD029 ol-prefix - Ordered list item prefix\nMD029: false\n\n# MD030 list-marker-space - Spaces after list markers\nMD030: false\n\n# MD033 no-inline-html - Inline HTML\nMD033: false\n\n# MD034 no-bare-urls - Bare URL used\nMD034: false\n\n# MD036 no-emphasis-as-heading/no-emphasis-as-header - Emphasis used instead of a heading\nMD036: false\n\n# MD040 fenced-code-language - Fenced code blocks should have a language specified\nMD040: false\n\n# MD041 first-line-heading/first-line-h1 - First line in a file should be a top-level heading\nMD041: false\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 60.6044921875,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n---\ndefault_stages: [pre-commit, pre-push]\ndefault_language_version:\n  python: python3\n  node: 22.2.0\nminimum_pre_commit_version: '3.2.0'\nexclude: ^.*/.*_vendor/\nrepos:\n  - repo: meta\n    hooks:\n      - id: identity\n        name: Print checked files\n        description: Print input to the static check hooks for troubleshooting\n      - id: check-hooks-apply\n        name: Check if all hooks apply to the repository\n  - repo: https://github.com/thlorenz/doctoc.git\n    rev: v2.2.0\n    hooks:\n      - id: doctoc\n        name: Add TOC for Markdown and RST files\n        files:\n          ^README\\.md$|^UPDATING.*\\.md$|^chart/UPDATING.*\\.md$|^dev/.*\\.md$|^dev/.*\\.rst$|^.github/.*\\.md|^tests/system/README.md$\n        args:\n          - \"--maxlevel\"\n          - \"2\"\n  - repo: https://github.com/Lucas-C/pre-commit-hooks\n    rev: v1.5.5\n    hooks:\n      - id: insert-license\n        name: Add license for all SQL files\n        files: \\.sql$\n        exclude: |\n          (?x)\n          ^\\.github/\n        args:\n          - --comment-style\n          - \"/*||*/\"\n          - --license-filepath\n          - scripts/ci/license-templates/LICENSE.txt\n          - --fuzzy-match-generates-todo\n      - id: insert-license\n        name: Add license for all RST files\n        exclude: ^\\.github/.*$|newsfragments/.*\\.rst$\n        args:\n          - --comment-style\n          - \"||\"\n          - --license-filepath\n          - scripts/ci/license-templates/LICENSE.rst\n          - --fuzzy-match-generates-todo\n        files: \\.rst$\n      - id: insert-license\n        name: Add license for CSS/JS/JSX/PUML/TS/TSX\n        files: \\.(css|jsx?|puml|tsx?)$\n        exclude: ^\\.github/.*$|^airflow/www/static/js/types/api-generated.ts$|ui/openapi-gen/\n        args:\n          - --comment-style\n          - \"/*!| *| */\"\n          - --license-filepath\n          - scripts/ci/license-templates/LICENSE.txt\n          - --fuzzy-match-generates-todo\n      - id: insert-license\n        name: Add license for all JINJA template files\n        files: ^airflow/www/templates/.*\\.html$\n        exclude: ^\\.github/.*$\n        args:\n          - --comment-style\n          - \"{#||#}\"\n          - --license-filepath\n          - scripts/ci/license-templates/LICENSE.txt\n          - --fuzzy-match-generates-todo\n      - id: insert-license\n        name: Add license for all Shell files\n        exclude: ^\\.github/.*$|^dev/breeze/autocomplete/.*$\n        files: \\.bash$|\\.sh$\n        args:\n          - --comment-style\n          - \"|#|\"\n          - --license-filepath\n          - scripts/ci/license-templates/LICENSE.txt\n          - --fuzzy-match-generates-todo\n      - id: insert-license\n        name: Add license for all toml files\n        exclude: ^\\.github/.*$|^dev/breeze/autocomplete/.*$\n        files: \\.toml$\n        args:\n          - --comment-style\n          - \"|#|\"\n          - --license-filepath\n          - scripts/ci/license-templates/LICENSE.txt\n          - --fuzzy-match-generates-todo\n      - id: insert-license\n        name: Add license for all Python files\n        exclude: ^\\.github/.*$\n        files: \\.py$|\\.pyi$\n        args:\n          - --comment-style\n          - \"|#|\"\n          - --license-filepath\n          - scripts/ci/license-templates/LICENSE.txt\n          - --fuzzy-match-generates-todo\n      - id: insert-license\n        name: Add license for all XML files\n        exclude: ^\\.github/.*$\n        files: \\.xml$\n        args:\n          - --comment-style\n          - \"<!--||-->\"\n          - --license-filepath\n          - scripts/ci/license-templates/LICENSE.txt\n          - --fuzzy-match-generates-todo\n      - id: insert-license\n        name: Add license for all Helm template files\n        files: ^chart/templates/.*\n        args:\n          - --comment-style\n          - \"{{/*||*/}}\"\n          - --license-filepath\n          - scripts/ci/license-templates/LICENSE.txt\n          - --fuzzy-match-generates-todo\n      - id: insert-license\n        name: Add license for all YAML files except Helm templates\n        exclude: ^\\.github/.*$|^chart/templates/.*|.*/reproducible_build.yaml$|^airflow/api_fastapi/core_api/openapi/v1-generated.yaml$|^.*/pnpm-lock.yaml$\n        types: [yaml]\n        files: \\.ya?ml$\n        args:\n          - --comment-style\n          - \"|#|\"\n          - --license-filepath\n          - scripts/ci/license-templates/LICENSE.txt\n          - --fuzzy-match-generates-todo\n      - id: insert-license\n        name: Add license for all Markdown files\n        files: \\.md$\n        exclude: PROVIDER_CHANGES.*\\.md$\n        args:\n          - --comment-style\n          - \"<!--|| -->\"\n          - --license-filepath\n          - scripts/ci/license-templates/LICENSE.txt\n          - --fuzzy-match-generates-todo\n      - id: insert-license\n        name: Add license for all other files\n        exclude: ^\\.github/.*$\n        args:\n          - --comment-style\n          - \"|#|\"\n          - --license-filepath\n          - scripts/ci/license-templates/LICENSE.txt\n          - --fuzzy-match-generates-todo\n        files: >\n          \\.cfg$|\\.conf$|\\.ini$|\\.ldif$|\\.properties$|\\.readthedocs$|\\.service$|\\.tf$|Dockerfile.*$\n  - repo: local\n    hooks:\n      - id: check-min-python-version\n        name: Check minimum Python version\n        entry: ./scripts/ci/pre_commit/check_min_python_version.py\n        language: python\n        additional_dependencies: ['rich>=12.4.4']\n        require_serial: true\n      - id: check-imports-in-providers\n        name: Check imports in providers\n        entry: ./scripts/ci/pre_commit/check_imports_in_providers.py\n        language: python\n        additional_dependencies: ['rich>=12.4.4', \"ruff==0.8.1\"]\n        files: ^providers/src/airflow/providers/.*\\.py$\n        require_serial: true\n      - id: update-common-sql-api-stubs\n        name: Check and update common.sql API stubs\n        entry: ./scripts/ci/pre_commit/update_common_sql_api_stubs.py\n        language: python\n        files: ^scripts/ci/pre_commit/update_common_sql_api\\.py|^providers/src/airflow/providers/common/sql/.*\\.pyi?$\n        additional_dependencies: ['rich>=12.4.4', 'mypy==1.9.0', 'black==24.10.0', 'jinja2']\n        pass_filenames: false\n        require_serial: true\n      - id: update-black-version\n        name: Update black versions everywhere (manual)\n        entry: ./scripts/ci/pre_commit/update_black_version.py\n        stages: ['manual']\n        language: python\n        files: ^.pre-commit-config.yaml$\n        additional_dependencies: ['pyyaml']\n        pass_filenames: false\n        require_serial: true\n      - id: update-installers-and-pre-commit\n        name: Update installers and pre-commit to latest (manual)\n        entry: ./scripts/ci/pre_commit/update_installers_and_pre_commit.py\n        stages: ['manual']\n        language: python\n        files: ^.pre-commit-config.yaml$|^scripts/ci/pre_commit/update_installers_and_pre_commit.py$\n        pass_filenames: false\n        require_serial: true\n        additional_dependencies: ['pyyaml', 'rich>=12.4.4', 'requests']\n      - id: update-chart-dependencies\n        name: Update chart dependencies to latest (manual)\n        entry: ./scripts/ci/pre_commit/update_chart_dependencies.py\n        stages: ['manual']\n        language: python\n        additional_dependencies: ['pyyaml', 'rich>=12.4.4', 'requests']\n        files: ^.pre-commit-config.yaml$|^scripts/ci/pre_commit/update_build_dependencies.py$\n        pass_filenames: false\n        require_serial: true\n      - id: check-taskinstance-tis-attrs\n        name: Check that TI and TIS have the same attributes\n        entry: ./scripts/ci/pre_commit/check_ti_vs_tis_attributes.py\n        language: python\n        additional_dependencies: ['rich>=12.4.4']\n        files: ^airflow/models/taskinstance.py$|^airflow/models/taskinstancehistory.py$\n        pass_filenames: false\n        require_serial: true\n      - id: check-deferrable-default\n        name: Check and fix default value of default_deferrable\n        language: python\n        entry: ./scripts/ci/pre_commit/check_deferrable_default.py\n        pass_filenames: false\n        additional_dependencies: [\"libcst>=1.1.0\"]\n        files: ^(providers/src/)?airflow/.*/(sensors|operators)/.*\\.py$\n  - repo: https://github.com/asottile/blacken-docs\n    rev: 1.19.1\n    hooks:\n      - id: blacken-docs\n        name: Run black on docs\n        args:\n          - --line-length=110\n          - --target-version=py39\n          - --target-version=py310\n          - --target-version=py311\n          - --target-version=py312\n        alias: blacken-docs\n        additional_dependencies: [black==24.10.0]\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n      - id: check-merge-conflict\n        name: Check that merge conflicts are not being committed\n      - id: debug-statements\n        name: Detect accidentally committed debug statements\n      - id: check-builtin-literals\n        name: Require literal syntax when initializing builtins\n      - id: detect-private-key\n        name: Detect if private key is added to the repository\n        exclude: ^docs/apache-airflow-providers-ssh/connections/ssh.rst$\n      - id: end-of-file-fixer\n        name: Make sure that there is an empty line at the end\n        exclude: ^docs/apache-airflow/img/.*\\.dot|^docs/apache-airflow/img/.*\\.sha256\n      - id: mixed-line-ending\n        name: Detect if mixed line ending is used (\\r vs. \\r\\n)\n      - id: check-executables-have-shebangs\n        name: Check that executables have shebang\n      - id: check-xml\n        name: Check XML files with xmllint\n      - id: trailing-whitespace\n        name: Remove trailing whitespace at end of line\n        exclude: ^docs/apache-airflow/img/.*\\.dot|^dev/breeze/doc/images/output.*$\n      - id: fix-encoding-pragma\n        name: Remove encoding header from Python files\n        args:\n          - --remove\n      - id: pretty-format-json\n        name: Format JSON files\n        args:\n          - --autofix\n          - --no-sort-keys\n          - --indent\n          - \"4\"\n        files: ^chart/values\\.schema\\.json$|^chart/values_schema\\.schema\\.json$\n        pass_filenames: true\n  - repo: https://github.com/pre-commit/pygrep-hooks\n    rev: v1.10.0\n    hooks:\n      - id: rst-backticks\n        name: Check if RST files use double backticks for code\n      - id: python-no-log-warn\n        name: Check if there are no deprecate log warn\n  - repo: https://github.com/adrienverge/yamllint\n    rev: v1.35.1\n    hooks:\n      - id: yamllint\n        name: Check YAML files with yamllint\n        entry: yamllint -c yamllint-config.yml --strict\n        types: [yaml]\n        exclude: ^.*airflow\\.template\\.yaml$|^.*init_git_sync\\.template\\.yaml$|^chart/(?:templates|files)/.*\\.yaml$|openapi/.*\\.yaml$|^\\.pre-commit-config\\.yaml$|^.*/reproducible_build.yaml$|^.*pnpm-lock\\.yaml$\n  - repo: https://github.com/ikamensh/flynt\n    rev: '1.0.1'\n    hooks:\n      - id: flynt\n        name: Run flynt string format converter for Python\n        args:\n         # If flynt detects too long text it ignores it. So we set a very large limit to make it easy\n         # to split the text by hand. Too long lines are detected by flake8 (below),\n         # so the user is informed to take action.\n         - --line-length\n         - '99999'\n  - repo: https://github.com/codespell-project/codespell\n    rev: v2.3.0\n    hooks:\n      - id: codespell\n        name: Run codespell\n        description: Run codespell to check for common misspellings in files\n        entry: bash -c 'echo \"If you think that this failure is an error, consider adding the word(s)\n          to the codespell dictionary at docs/spelling_wordlist.txt.\n          The word(s) should be in lowercase.\" && exec codespell \"$@\"' --\n        language: python\n        types: [text]\n        exclude: material-icons\\.css$|^images/.*$|^RELEASE_NOTES\\.txt$|^.*package-lock\\.json$|^.*/kinglear\\.txt$|^.*pnpm-lock\\.yaml$\n        args:\n          - --ignore-words=docs/spelling_wordlist.txt\n          - --skip=providers/src/airflow/providers/*/*.rst,airflow/www/*.log,docs/*/commits.rst,docs/apache-airflow/tutorial/pipeline_example.csv,*.min.js,*.lock,INTHEWILD.md\n          - --exclude-file=.codespellignorelines\n  - repo: https://github.com/woodruffw/zizmor-pre-commit\n    rev: v1.0.0\n    hooks:\n      - id: zizmor\n        name: Run zizmor to check for github workflow syntax errors\n        types: [yaml]\n        files: \\.github/workflows/.*$|\\.github/actions/.*$\n        require_serial: true\n        entry: zizmor\n  - repo: local\n    # Note that this is the 2nd \"local\" repo group in the .pre-commit-config.yaml file. This is because\n    # we try to minimise the number of passes that must happen in order to apply some of the changes\n    # done by pre-commits. Some of the pre-commits not only check for errors but also fix them. This means\n    # that output from an earlier pre-commit becomes input to another pre-commit. Splitting the local\n    # scripts of our and adding some other non-local pre-commit in-between allows us to handle such\n    # changes quickly - especially when we want the early modifications from the first local group\n    # to be applied before the non-local pre-commits are run\n    hooks:\n      - id: validate-operators-init\n        name: No templated field logic checks in operator __init__\n        description: Prevent templated field logic checks in operators' __init__\n        language: python\n        entry: ./scripts/ci/pre_commit/validate_operators_init.py\n        pass_filenames: true\n        files: ^providers/src/airflow/providers/.*/(operators|transfers|sensors)/.*\\.py$\n        additional_dependencies: [ 'rich>=12.4.4' ]\n      - id: update-providers-init-py\n        name: Update providers __init__.py files\n        entry: ./scripts/ci/pre_commit/update_providers_init.py\n        language: python\n        pass_filenames: true\n        files: ^providers/[^\\/]*/__init__.py$|^providers/[^\\/]*/[^\\/]*/__init__.py$|^providers/.*/provider.yaml$|^airflow_breeze/templates/PROVIDER__INIT__PY_TEMPLATE.py.jinja2^\n        additional_dependencies: ['rich>=12.4.4','requests']\n        require_serial: true\n      - id: ruff\n        name: Run 'ruff' for extremely fast Python linting\n        description: \"Run 'ruff' for extremely fast Python linting\"\n        entry: ruff check --force-exclude\n        language: python\n        types_or: [python, pyi]\n        args: [--fix]\n        require_serial: true\n        additional_dependencies: [\"ruff==0.8.1\"]\n        exclude: ^tests/dags/test_imports.py|^performance/tests/test_.*.py\n      - id: ruff-format\n        name: Run 'ruff format'\n        description: \"Run 'ruff format' for extremely fast Python formatting\"\n        entry: ./scripts/ci/pre_commit/ruff_format.py\n        language: python\n        types_or: [python, pyi]\n        args: []\n        require_serial: true\n        additional_dependencies: [\"ruff==0.8.1\"]\n        exclude: ^tests/dags/test_imports.py$\n      - id: replace-bad-characters\n        name: Replace bad characters\n        entry: ./scripts/ci/pre_commit/replace_bad_characters.py\n        language: python\n        types: [file, text]\n        exclude: ^clients/gen/go\\.sh$|^\\.gitmodules$\n        additional_dependencies: ['rich>=12.4.4']\n      - id: lint-openapi\n        name: Lint OpenAPI using spectral\n        language: docker_image\n        entry: stoplight/spectral lint -r ./scripts/ci/spectral_rules/connexion.yml\n        files: ^airflow/api_connexion/openapi/\n      - id: lint-openapi\n        name: Lint OpenAPI using openapi-spec-validator\n        entry: openapi-spec-validator --schema 3.0.0\n        language: python\n        additional_dependencies: ['openapi-spec-validator>=0.7.1', 'openapi-schema-validator>=0.6.2']\n        files: ^airflow/api_connexion/openapi/\n      - id: lint-dockerfile\n        name: Lint Dockerfile\n        language: python\n        entry: ./scripts/ci/pre_commit/lint_dockerfile.py\n        files: Dockerfile.*$\n        pass_filenames: true\n        require_serial: true\n      - id: check-airflow-k8s-not-used\n        name: Check airflow.kubernetes imports are not used\n        language: python\n        files: ^airflow/.*\\.py$\n        require_serial: true\n        exclude: ^airflow/kubernetes/\n        entry: ./scripts/ci/pre_commit/check_airflow_k8s_not_used.py\n        additional_dependencies: ['rich>=12.4.4']\n      - id: check-common-compat-used-for-openlineage\n        name: Check common.compat is used for OL deprecated classes\n        language: python\n        files: ^airflow/.*\\.py$\n        require_serial: true\n        exclude: ^airflow/openlineage/\n        entry: ./scripts/ci/pre_commit/check_common_compat_used_for_openlineage.py\n        additional_dependencies: ['rich>=12.4.4']\n      - id: check-airflow-providers-bug-report-template\n        name: Sort airflow-bug-report provider list\n        language: python\n        files: ^.github/ISSUE_TEMPLATE/airflow_providers_bug_report\\.yml$\n        require_serial: true\n        entry: ./scripts/ci/pre_commit/check_airflow_bug_report_template.py\n        additional_dependencies: ['rich>=12.4.4', 'pyyaml']\n      - id: check-cncf-k8s-only-for-executors\n        name: Check cncf.kubernetes imports used for executors only\n        language: python\n        files: ^airflow/.*\\.py$\n        require_serial: true\n        exclude: ^airflow/kubernetes/|^providers/src/airflow/providers/\n        entry: ./scripts/ci/pre_commit/check_cncf_k8s_used_for_k8s_executor_only.py\n        additional_dependencies: ['rich>=12.4.4']\n      - id: check-airflow-provider-compatibility\n        name: Check compatibility of Providers with Airflow\n        entry: ./scripts/ci/pre_commit/check_provider_airflow_compatibility.py\n        language: python\n        pass_filenames: true\n        files: ^providers/src/airflow/providers/.*\\.py$\n        additional_dependencies: ['rich>=12.4.4']\n      - id: check-google-re2-as-dependency\n        name: Check google-re2 declared as dep\n        description: Check google-re2 is declared as dependency when needed\n        entry: ./scripts/ci/pre_commit/check_google_re2_imports.py\n        language: python\n        pass_filenames: true\n        require_serial: true\n        files: ^providers/src/airflow/providers/.*\\.py$\n        additional_dependencies: ['rich>=12.4.4']\n      - id: update-local-yml-file\n        name: Update mounts in the local yml file\n        entry: ./scripts/ci/pre_commit/local_yml_mounts.py\n        language: python\n        files: ^dev/breeze/src/airflow_breeze/utils/docker_command_utils\\.py$|^scripts/ci/docker_compose/local\\.yml$\n        pass_filenames: false\n        additional_dependencies: ['rich>=12.4.4']\n      - id: check-sql-dependency-common-data-structure\n        name: Check dependency of SQL providers\n        description: Check dependency of SQL Providers with common data structure\n        entry: ./scripts/ci/pre_commit/check_common_sql_dependency.py\n        language: python\n        files: ^providers/src/airflow/providers/.*/hooks/.*\\.py$\n        additional_dependencies: ['rich>=12.4.4', 'pyyaml', 'packaging']\n      - id: update-providers-dependencies\n        name: Update dependencies for provider packages\n        entry: ./scripts/ci/pre_commit/update_providers_dependencies.py\n        language: python\n        files: |\n          (?x)\n          ^providers/src/airflow/providers/.*\\.py$ |\n          ^providers/src/airflow/providers/.*/provider\\.yaml$ |\n          ^providers/tests/.*\\.py$ |\n          ^scripts/ci/pre_commit/update_providers_dependencies\\.py$\n        pass_filenames: false\n        additional_dependencies: ['setuptools', 'rich>=12.4.4', 'pyyaml', 'tomli']\n      - id: check-extra-packages-references\n        name: Checks setup extra packages\n        description: Checks if all the extras defined in hatch_build.py are listed in extra-packages-ref.rst file\n        language: python\n        files: ^docs/apache-airflow/extra-packages-ref\\.rst$|^hatch_build.py\n        pass_filenames: false\n        entry: ./scripts/ci/pre_commit/check_extra_packages_ref.py\n        additional_dependencies: ['rich>=12.4.4', 'hatchling==1.27.0', 'tabulate']\n      - id: check-hatch-build-order\n        name: Check order of dependencies in hatch_build.py\n        language: python\n        files: ^hatch_build.py$\n        pass_filenames: false\n        entry: ./scripts/ci/pre_commit/check_order_hatch_build.py\n        additional_dependencies: ['rich>=12.4.4', 'hatchling==1.27.0']\n      - id: update-extras\n        name: Update extras in documentation\n        entry: ./scripts/ci/pre_commit/insert_extras.py\n        language: python\n        files: ^contributing-docs/12_airflow_dependencies_and_extras.rst$|^INSTALL$|^providers/src/airflow/providers/.*/provider\\.yaml$|^Dockerfile.*\n        pass_filenames: false\n        additional_dependencies: ['rich>=12.4.4', 'hatchling==1.27.0']\n      - id: check-extras-order\n        name: Check order of extras in Dockerfile\n        entry: ./scripts/ci/pre_commit/check_order_dockerfile_extras.py\n        language: python\n        files: ^Dockerfile$\n        pass_filenames: false\n        additional_dependencies: ['rich>=12.4.4']\n      - id: generate-airflow-diagrams\n        name: Generate airflow diagrams\n        entry: ./scripts/ci/pre_commit/generate_airflow_diagrams.py\n        language: python\n        files: ^docs/.*/diagram_[^/]*\\.py$\n        pass_filenames: true\n        additional_dependencies: ['rich>=12.4.4', \"diagrams>=0.23.4\"]\n      - id: update-supported-versions\n        name: Updates supported versions in documentation\n        entry: ./scripts/ci/pre_commit/supported_versions.py\n        language: python\n        files: ^docs/apache-airflow/installation/supported-versions\\.rst$|^scripts/ci/pre_commit/supported_versions\\.py$|^README\\.md$\n        pass_filenames: false\n        additional_dependencies: ['tabulate']\n      - id: check-revision-heads-map\n        name: Check that the REVISION_HEADS_MAP is up-to-date\n        language: python\n        entry: ./scripts/ci/pre_commit/version_heads_map.py\n        pass_filenames: false\n        files: >\n          (?x)\n          ^scripts/ci/pre_commit/version_heads_map\\.py$|\n          ^airflow/migrations/versions/.*$|^airflow/migrations/versions|\n          ^providers/src/airflow/providers/fab/migrations/versions/.*$|^providers/src/airflow/providers/fab/migrations/versions|\n          ^airflow/utils/db.py$|\n          ^providers/src/airflow/providers/fab/auth_manager/models/db.py$\n        additional_dependencies: ['packaging','google-re2']\n      - id: update-version\n        name: Update versions in docs\n        entry: ./scripts/ci/pre_commit/update_versions.py\n        language: python\n        files: ^docs|^airflow/__init__.py$\n        pass_filenames: false\n        additional_dependencies: ['rich>=12.4.4']\n      - id: check-pydevd-left-in-code\n        language: pygrep\n        name: Check for pydevd debug statements accidentally left\n        entry: \"pydevd.*settrace\\\\(\"\n        pass_filenames: true\n        files: \\.py$\n      - id: check-links-to-example-dags-do-not-use-hardcoded-versions\n        name: Verify no hard-coded version in example dags\n        description: The links to example dags should use |version| as version specification\n        language: pygrep\n        entry: >\n          (?i)\n          .*https://github.*[0-9]/providers/tests/system/|\n          .*https://github.*/main/providers/tests/system/|\n          .*https://github.*/master/providers/tests/system/|\n          .*https://github.*/main/providers/src/airflow/providers/.*/example_dags/|\n          .*https://github.*/master/providers/src/airflow/providers/.*/example_dags/\n        pass_filenames: true\n        files: ^docs/apache-airflow-providers-.*\\.rst\n      - id: check-safe-filter-usage-in-html\n        language: pygrep\n        name: Don't use safe in templates\n        description: the Safe filter is error-prone, use Markup() in code instead\n        entry: \"\\\\|\\\\s*safe\"\n        files: \\.html$\n        pass_filenames: true\n      - id: check-no-providers-in-core-examples\n        language: pygrep\n        name: No providers imports in core example DAGs\n        description: The core example DAGs have no dependencies other than standard provider or core Airflow\n        entry: \"^\\\\s*from airflow\\\\.providers.(?!standard.)\"\n        pass_filenames: true\n        files: ^airflow/example_dags/.*\\.py$\n      - id: check-no-airflow-deprecation-in-providers\n        language: pygrep\n        name: Do not use DeprecationWarning in providers\n        description: Use AirflowProviderDeprecationWarning in providers\n        entry: \"^\\\\s*DeprecationWarning*\"\n        pass_filenames: true\n        files: ^providers/src/airflow/providers/.*\\.py$\n      - id: check-urlparse-usage-in-code\n        language: pygrep\n        name: Don't use urlparse in code\n        description: urlparse is not recommended, use urlsplit() in code instead\n        entry: \"^\\\\s*from urllib\\\\.parse import ((\\\\|, )(urlparse\\\\|urlunparse))+$\"\n        pass_filenames: true\n        files: \\.py$\n      - id: check-only-new-session-with-provide-session\n        name: Check NEW_SESSION is only used with @provide_session\n        language: python\n        entry: ./scripts/ci/pre_commit/new_session_in_provide_session.py\n        pass_filenames: true\n        files: ^airflow/.+\\.py$\n        exclude: ^airflow/serialization/pydantic/.*\n      - id: check-for-inclusive-language\n        language: pygrep\n        name: Check for language that we do not accept as community\n        description: Please use more appropriate words for community documentation.\n        entry: >\n          (?ix)\n          (black|white)[_-]?list|\n          \\bshe\\b|\n          \\bhe\\b|\n          \\bher\\b|\n          \\bhis\\b|\n          \\bmaster\\b|\n          \\bslave\\b|\n          \\bsanity\\b|\n          \\bdummy\\b\n        pass_filenames: true\n        exclude: >\n          (?x)\n          ^airflow/api_connexion/openapi/v1.yaml$|\n          ^airflow/ui/openapi-gen/|\n          ^airflow/cli/commands/local_commands/fastapi_api_command.py$|\n          ^airflow/cli/commands/local_commands/webserver_command.py$|\n          ^airflow/config_templates/|\n          ^airflow/models/baseoperator.py$|\n          ^airflow/operators/__init__.py$|\n          ^providers/src/airflow/providers/amazon/aws/hooks/emr.py$|\n          ^providers/src/airflow/providers/amazon/aws/operators/emr.py$|\n          ^providers/src/airflow/providers/apache/cassandra/hooks/cassandra.py$|\n          ^providers/src/airflow/providers/apache/hive/operators/hive_stats.py$|\n          ^providers/src/airflow/providers/apache/hive/transfers/vertica_to_hive.py$|\n          ^providers/src/airflow/providers/apache/spark/decorators/|\n          ^providers/src/airflow/providers/apache/spark/hooks/|\n          ^providers/src/airflow/providers/apache/spark/operators/|\n          ^providers/src/airflow/providers/exasol/hooks/exasol.py$|\n          ^providers/src/airflow/providers/fab/auth_manager/security_manager/|\n          ^providers/src/airflow/providers/fab/www/static/css/bootstrap-theme.css$|\n          ^providers/src/airflow/providers/google/cloud/hooks/bigquery.py$|\n          ^providers/src/airflow/providers/google/cloud/operators/cloud_build.py$|\n          ^providers/src/airflow/providers/google/cloud/operators/dataproc.py$|\n          ^providers/src/airflow/providers/google/cloud/operators/mlengine.py$|\n          ^providers/src/airflow/providers/microsoft/azure/hooks/cosmos.py$|\n          ^providers/src/airflow/providers/microsoft/winrm/hooks/winrm.py$|\n          ^airflow/www/fab_security/manager.py$|\n          ^docs/.*commits.rst$|\n          ^docs/apache-airflow-providers-apache-cassandra/connections/cassandra.rst$|\n          ^providers/src/airflow/providers/microsoft/winrm/operators/winrm.py$|\n          ^providers/src/airflow/providers/opsgenie/hooks/opsgenie.py$|\n          ^providers/src/airflow/providers/redis/provider.yaml$|\n          ^airflow/serialization/serialized_objects.py$|\n          ^airflow/ui/pnpm-lock.yaml$|\n          ^airflow/utils/db.py$|\n          ^airflow/utils/trigger_rule.py$|\n          ^airflow/www/static/css/bootstrap-theme.css$|\n          ^airflow/www/static/js/types/api-generated.ts$|\n          ^airflow/www/templates/appbuilder/flash.html$|\n          ^chart/values.schema.json$|\n          ^dev/|\n          ^docs/README.rst$|\n          ^docs/apache-airflow-providers-amazon/secrets-backends/aws-ssm-parameter-store.rst$|\n          ^docs/apache-airflow-providers-apache-hdfs/connections.rst$|\n          ^docs/apache-airflow-providers-apache-kafka/connections/kafka.rst$|\n          ^docs/apache-airflow-providers-apache-spark/decorators/pyspark.rst$|\n          ^docs/apache-airflow-providers-fab/auth-manager/webserver-authentication.rst$|\n          ^docs/apache-airflow-providers-google/operators/cloud/kubernetes_engine.rst$|\n          ^docs/apache-airflow-providers-microsoft-azure/connections/azure_cosmos.rst$|\n          ^docs/apache-airflow-providers-cncf-kubernetes/operators.rst$|\n          ^docs/conf.py$|\n          ^docs/exts/removemarktransform.py$|\n          ^newsfragments/41761.significant.rst$|\n          ^scripts/ci/pre_commit/vendor_k8s_json_schema.py$|\n          ^scripts/ci/docker-compose/integration-keycloak.yml$|\n          ^scripts/ci/docker-compose/keycloak/keycloak-entrypoint.sh$|\n          ^tests/|\n          ^providers/tests/|\n          ^.pre-commit-config\\.yaml$|\n          ^.*CHANGELOG\\.(rst|txt)$|\n          ^.*RELEASE_NOTES\\.rst$|\n          ^contributing-docs/03_contributors_quick_start.rst$|\n          ^.*\\.(png|gif|jp[e]?g|tgz|lock)$|\n          git|\n          ^newsfragments/43349\\.significant\\.rst$\n      - id: check-base-operator-partial-arguments\n        name: Check BaseOperator and partial() arguments\n        language: python\n        entry: ./scripts/ci/pre_commit/base_operator_partial_arguments.py\n        pass_filenames: false\n        files: ^airflow/models/(?:base|mapped)operator\\.py$\n      - id: check-init-decorator-arguments\n        name: Sync model __init__ and decorator arguments\n        language: python\n        entry: ./scripts/ci/pre_commit/sync_init_decorator.py\n        pass_filenames: false\n        files: ^airflow/models/dag\\.py$|^airflow/(?:decorators|utils)/task_group\\.py$\n      - id: check-template-context-variable-in-sync\n        name: Sync template context variable refs\n        language: python\n        entry: ./scripts/ci/pre_commit/template_context_key_sync.py\n        files: ^airflow/models/taskinstance\\.py$|^airflow/utils/context\\.pyi?$|^docs/apache-airflow/templates-ref\\.rst$\n      - id: check-base-operator-usage\n        language: pygrep\n        name: Check BaseOperator core imports\n        description: Make sure BaseOperator is imported from airflow.models.baseoperator in core\n        entry: \"from airflow\\\\.models import.* BaseOperator\\\\b\"\n        files: \\.py$\n        pass_filenames: true\n        exclude: >\n          (?x)\n          ^airflow/decorators/.*$|\n          ^airflow/hooks/.*$|\n          ^airflow/operators/.*$|\n          ^providers/src/airflow/providers/.*$|\n          ^providers/src/airflow/providers/standard/sensors/.*$|\n          ^dev/provider_packages/.*$\n      - id: check-base-operator-usage\n        language: pygrep\n        name: Check BaseOperatorLink core imports\n        description: Make sure BaseOperatorLink is imported from airflow.models.baseoperatorlink in core\n        entry: \"from airflow\\\\.models import.* BaseOperatorLink\"\n        files: \\.py$\n        pass_filenames: true\n        exclude: >\n          (?x)\n          ^airflow/decorators/.*$|\n          ^airflow/hooks/.*$|\n          ^airflow/operators/.*$|\n          ^providers/src/airflow/providers/.*$|\n          ^providers/src/airflow/providers/standard/sensors/.*$|\n          ^dev/provider_packages/.*$\n      - id: check-base-operator-usage\n        language: pygrep\n        name: Check BaseOperator[Link] other imports\n        description: Make sure BaseOperator[Link] is imported from airflow.models outside of core\n        entry: \"from airflow\\\\.models\\\\.baseoperator(link)? import.* BaseOperator\"\n        pass_filenames: true\n        files: >\n          (?x)\n          ^providers/src/airflow/providers/.*\\.py$\n        exclude: providers/src/airflow/providers/standard/operators/bash.py|providers/src/airflow/providers/standard/operators/python.py|providers/src/airflow/providers/standard/sensors/external_task.py\n      - id: check-get-lineage-collector-providers\n        language: python\n        name: Check providers import hook lineage code from compat\n        description: Make sure you import from airflow.provider.common.compat.lineage.hook instead of\n          airflow.lineage.hook.\n        entry: ./scripts/ci/pre_commit/check_get_lineage_collector_providers.py\n        files: ^providers/src/airflow/providers/.*\\.py$\n        exclude: ^providers/src/airflow/providers/common/compat/.*\\.py$\n        additional_dependencies: [ 'rich>=12.4.4' ]\n      - id: check-decorated-operator-implements-custom-name\n        name: Check @task decorator implements custom_operator_name\n        language: python\n        entry: ./scripts/ci/pre_commit/decorator_operator_implements_custom_name.py\n        pass_filenames: true\n        files: ^airflow/.*\\.py$\n      - id: check-core-deprecation-classes\n        language: pygrep\n        name: Verify usage of Airflow deprecation classes in core\n        entry: category=DeprecationWarning|category=PendingDeprecationWarning\n        files: \\.py$\n        exclude: ^airflow/configuration\\.py$|^providers/src/airflow/providers/|^scripts/in_container/verify_providers\\.py$|^(providers/)?tests/.*$|^tests_common/\n        pass_filenames: true\n      - id: check-provide-create-sessions-imports\n        language: pygrep\n        name: Check session util imports\n        description: NEW_SESSION, provide_session, and create_session should be imported from airflow.utils.session to avoid import cycles.\n        entry: \"from airflow\\\\.utils\\\\.db import.* (NEW_SESSION|provide_session|create_session)\"\n        files: \\.py$\n        pass_filenames: true\n      - id: check-incorrect-use-of-LoggingMixin\n        language: pygrep\n        name: Make sure LoggingMixin is not used alone\n        entry: \"LoggingMixin\\\\(\\\\)\"\n        files: \\.py$\n        pass_filenames: true\n      - id: check-daysago-import-from-utils\n        language: pygrep\n        name: days_ago imported from airflow.utils.dates\n        entry: \"(airflow\\\\.){0,1}utils\\\\.dates\\\\.days_ago\"\n        files: \\.py$\n        pass_filenames: true\n      - id: check-start-date-not-used-in-defaults\n        language: pygrep\n        name: start_date not in default_args\n        entry: \"default_args\\\\s*=\\\\s*{\\\\s*(\\\"|')start_date(\\\"|')|(\\\"|')start_date(\\\"|'):\"\n        files: \\.*example_dags.*\\.py$\n        pass_filenames: true\n      - id: check-apache-license-rat\n        name: Check if licenses are OK for Apache\n        entry: ./scripts/ci/pre_commit/check_license.py\n        language: python\n        files: ^.*LICENSE.*$|^.*LICENCE.*$\n        pass_filenames: false\n      - id: check-aiobotocore-optional\n        name: Check if aiobotocore is an optional dependency only\n        entry: ./scripts/ci/pre_commit/check_aiobotocore_optional.py\n        language: python\n        files: ^providers/src/airflow/providers/.*/provider\\.yaml$\n        pass_filenames: true\n        additional_dependencies: ['click', 'rich>=12.4.4', 'pyyaml']\n        require_serial: true\n      - id: check-boring-cyborg-configuration\n        name: Checks for Boring Cyborg configuration consistency\n        language: python\n        entry: ./scripts/ci/pre_commit/boring_cyborg.py\n        pass_filenames: false\n        require_serial: true\n        additional_dependencies: ['pyyaml', 'termcolor==2.5.0', 'wcmatch==8.2']\n      - id: update-in-the-wild-to-be-sorted\n        name: Sort INTHEWILD.md alphabetically\n        entry: ./scripts/ci/pre_commit/sort_in_the_wild.py\n        language: python\n        files: ^\\.pre-commit-config\\.yaml$|^INTHEWILD\\.md$\n        pass_filenames: false\n        require_serial: true\n      - id: update-installed-providers-to-be-sorted\n        name: Sort and uniquify installed_providers.txt\n        entry: ./scripts/ci/pre_commit/sort_installed_providers.py\n        language: python\n        files: ^\\.pre-commit-config\\.yaml$|^.*_installed_providers\\.txt$\n        pass_filenames: false\n        require_serial: true\n      - id: update-spelling-wordlist-to-be-sorted\n        name: Sort spelling_wordlist.txt\n        entry: ./scripts/ci/pre_commit/sort_spelling_wordlist.py\n        language: python\n        files: ^\\.pre-commit-config\\.yaml$|^docs/spelling_wordlist\\.txt$\n        require_serial: true\n        pass_filenames: false\n      - id: update-openapi-spec-tags-to-be-sorted\n        name: Sort alphabetically openapi spec tags\n        entry: ./scripts/ci/pre_commit/sort_tags_in_openapi_spec.py\n        types: [yaml]\n        language: python\n        files: ^\\.pre-commit-config\\.yaml$|^airflow/api_connexion/openapi/v1\\.yaml$\n        require_serial: true\n        pass_filenames: false\n      - id: lint-helm-chart\n        name: Lint Helm Chart\n        entry: ./scripts/ci/pre_commit/helm_lint.py\n        language: python\n        pass_filenames: false\n        files: ^chart\n        require_serial: true\n        additional_dependencies: ['rich>=12.4.4','requests']\n      - id: kubeconform\n        name: Kubeconform check on our helm chart\n        entry: ./scripts/ci/pre_commit/kubeconform.py\n        language: python\n        pass_filenames: false\n        files: ^chart\n        require_serial: true\n        additional_dependencies: ['rich>=12.4.4','requests']\n      - id: shellcheck\n        name: Check Shell scripts syntax correctness\n        language: docker_image\n        entry: koalaman/shellcheck:v0.8.0 -x -a\n        files: \\.(bash|sh)$|^hooks/build$|^hooks/push$\n        exclude: ^dev/breeze/autocomplete/.*$\n      - id: lint-css\n        name: stylelint\n        entry: \"stylelint\"\n        language: node\n        files: ^airflow/www/.*\\.(css|sass|scss)$\n        # Keep dependency versions in sync w/ airflow/www/package.json\n        additional_dependencies: ['stylelint@13.3.1', 'stylelint-config-standard@20.0.0', 'stylelint-config-prettier@9.0.5']\n      - id: compile-ui-assets\n        name: Compile ui assets (manual)\n        language: node\n        stages: ['manual']\n        types_or: [javascript, ts, tsx]\n        files: ^airflow/ui/\n        entry: ./scripts/ci/pre_commit/compile_ui_assets.py\n        pass_filenames: false\n        additional_dependencies: ['pnpm@9.7.1']\n      - id: compile-ui-assets-dev\n        name: Compile ui assets in dev mode (manual)\n        language: node\n        stages: ['manual']\n        types_or: [javascript, ts, tsx]\n        files: ^airflow/ui/\n        entry: ./scripts/ci/pre_commit/compile_ui_assets_dev.py\n        pass_filenames: false\n        additional_dependencies: ['pnpm@9.7.1']\n      - id: compile-www-assets\n        name: Compile www assets (manual)\n        language: node\n        stages: ['manual']\n        'types_or': [javascript, ts, tsx]\n        files: ^airflow/www/\n        entry: ./scripts/ci/pre_commit/compile_www_assets.py\n        pass_filenames: false\n        additional_dependencies: ['yarn@1.22.21']\n      - id: compile-www-assets-dev\n        name: Compile www assets in dev mode (manual)\n        language: node\n        stages: ['manual']\n        'types_or': [javascript, ts, tsx]\n        files: ^airflow/www/\n        entry: ./scripts/ci/pre_commit/compile_www_assets_dev.py\n        pass_filenames: false\n        additional_dependencies: ['yarn@1.22.21']\n      - id: check-providers-subpackages-init-file-exist\n        name: Provider subpackage init files are there\n        pass_filenames: false\n        always_run: true\n        entry: ./scripts/ci/pre_commit/check_providers_subpackages_all_have_init.py\n        language: python\n        require_serial: true\n      - id: check-pre-commit-information-consistent\n        name: Validate hook IDs & names and sync with docs\n        entry: ./scripts/ci/pre_commit/check_pre_commit_hooks.py\n        args:\n          - --max-length=53\n        language: python\n        files: ^\\.pre-commit-config\\.yaml$|^scripts/ci/pre_commit/check_pre_commit_hooks\\.py$\n        additional_dependencies: ['pyyaml', 'jinja2', 'black==24.10.0', 'tabulate', 'rich>=12.4.4']\n        require_serial: true\n        pass_filenames: false\n      - id: check-integrations-list-consistent\n        name: Sync integrations list with docs\n        entry: ./scripts/ci/pre_commit/check_integrations_list.py\n        language: python\n        files: ^scripts/ci/docker-compose/integration-.*\\.yml$|^contributing-docs/testing/integration_tests.rst$\n        additional_dependencies: ['black==24.10.0', 'tabulate', 'rich>=12.4.4', 'pyyaml']\n        require_serial: true\n        pass_filenames: false\n      - id: update-breeze-readme-config-hash\n        name: Update Breeze README.md with config files hash\n        language: python\n        entry: ./scripts/ci/pre_commit/update_breeze_config_hash.py\n        files: ^dev/breeze/pyproject\\.toml$|^dev/breeze/README\\.md$\n        pass_filenames: false\n        require_serial: true\n      - id: update-reproducible-source-date-epoch\n        name: Update Source Date Epoch for reproducible builds\n        language: python\n        entry: ./scripts/ci/pre_commit/update_source_date_epoch.py\n        files: ^RELEASE_NOTES.rst$|^chart/RELEASE_NOTES.rst$\n        additional_dependencies: ['rich>=12.4.4', 'pyyaml']\n        pass_filenames: false\n        require_serial: true\n      - id: check-breeze-top-dependencies-limited\n        name: Check top-level breeze deps\n        description: Breeze should have small number of top-level dependencies\n        language: python\n        entry: ./scripts/tools/check_if_limited_dependencies.py\n        files: ^dev/breeze/.*$\n        pass_filenames: false\n        require_serial: true\n        additional_dependencies: ['click', 'rich>=12.4.4', 'pyyaml']\n      - id: check-tests-in-the-right-folders\n        name: Check if tests are in the right folders\n        entry: ./scripts/ci/pre_commit/check_tests_in_right_folders.py\n        language: python\n        files: ^tests/.*\\.py\n        pass_filenames: true\n        require_serial: true\n        additional_dependencies: ['rich>=12.4.4']\n      - id: check-system-tests-present\n        name: Check if system tests have required segments of code\n        entry: ./scripts/ci/pre_commit/check_system_tests.py\n        language: python\n        files: ^(providers/)?tests/system/.*/example_[^/]*\\.py$\n        exclude: ^providers/tests/system/google/cloud/bigquery/example_bigquery_queries\\.py$\n        pass_filenames: true\n        additional_dependencies: ['rich>=12.4.4']\n      - id: generate-pypi-readme\n        name: Generate PyPI README\n        entry: ./scripts/ci/pre_commit/generate_pypi_readme.py\n        language: python\n        files: ^README\\.md$\n        pass_filenames: false\n      - id: lint-markdown\n        name: Run markdownlint\n        description: Checks the style of Markdown files.\n        entry: markdownlint\n        language: node\n        types: [markdown]\n        files: \\.(md|mdown|markdown)$\n        additional_dependencies: ['markdownlint-cli@0.38.0']\n      - id: lint-json-schema\n        name: Lint JSON Schema files\n        entry: ./scripts/ci/pre_commit/json_schema.py\n        args:\n          - --spec-file\n          - scripts/ci/pre_commit/draft7_schema.json\n        language: python\n        pass_filenames: true\n        files: .*\\.schema\\.json$\n        require_serial: true\n        additional_dependencies: ['jsonschema>=3.2.0,<5.0', 'PyYAML==6.0.2', 'requests==2.32.3']\n      - id: lint-json-schema\n        name: Lint NodePort Service\n        entry: ./scripts/ci/pre_commit/json_schema.py\n        args:\n          - --spec-url\n          - https://raw.githubusercontent.com/yannh/kubernetes-json-schema/master/v1.20.2-standalone/service-v1.json\n        language: python\n        pass_filenames: true\n        files: ^scripts/ci/kubernetes/nodeport\\.yaml$\n        require_serial: true\n        additional_dependencies: ['jsonschema>=3.2.0,<5.0', 'PyYAML==6.0.2', 'requests==2.32.3']\n      - id: lint-json-schema\n        name: Lint Docker compose files\n        entry: ./scripts/ci/pre_commit/json_schema.py\n        args:\n          - --spec-url\n          - https://raw.githubusercontent.com/compose-spec/compose-spec/master/schema/compose-spec.json\n        language: python\n        pass_filenames: true\n        files: ^scripts/ci/docker-compose/.+\\.ya?ml$|docker-compose\\.ya?ml$\n        exclude: >\n          (?x)\n          ^scripts/ci/docker-compose/grafana/.|\n          ^scripts/ci/docker-compose/.+-config\\.ya?ml\n        require_serial: true\n        additional_dependencies: ['jsonschema>=3.2.0,<5.0', 'PyYAML==6.0.2', 'requests==2.32.3']\n      - id: lint-json-schema\n        name: Lint chart/values.schema.json\n        entry: ./scripts/ci/pre_commit/json_schema.py\n        args:\n          - --spec-file\n          - chart/values_schema.schema.json\n          - chart/values.schema.json\n        language: python\n        pass_filenames: false\n        files: ^chart/values\\.schema\\.json$|^chart/values_schema\\.schema\\.json$\n        require_serial: true\n        additional_dependencies: ['jsonschema>=3.2.0,<5.0', 'PyYAML==6.0.2', 'requests==2.32.3']\n      - id: update-vendored-in-k8s-json-schema\n        name: Vendor k8s definitions into values.schema.json\n        entry: ./scripts/ci/pre_commit/vendor_k8s_json_schema.py\n        language: python\n        files: ^chart/values\\.schema\\.json$\n        additional_dependencies: ['requests==2.32.3']\n      - id: lint-json-schema\n        name: Lint chart/values.yaml\n        entry: ./scripts/ci/pre_commit/json_schema.py\n        args:\n          - --enforce-defaults\n          - --spec-file\n          - chart/values.schema.json\n          - chart/values.yaml\n        language: python\n        pass_filenames: false\n        files: ^chart/values\\.yaml$|^chart/values\\.schema\\.json$\n        require_serial: true\n        additional_dependencies: ['jsonschema>=3.2.0,<5.0', 'PyYAML==6.0.2', 'requests==2.32.3']\n      - id: lint-json-schema\n        name: Lint config_templates/config.yml\n        entry: ./scripts/ci/pre_commit/json_schema.py\n        args:\n          - --spec-file\n          - airflow/config_templates/config.yml.schema.json\n        language: python\n        pass_filenames: true\n        files: ^airflow/config_templates/config\\.yml$\n        require_serial: true\n        additional_dependencies: ['jsonschema>=3.2.0,<5.0', 'PyYAML==6.0.2', 'requests==2.32.3']\n      - id: check-persist-credentials-disabled-in-github-workflows\n        name: Check persistent creds in workflow files\n        description: Check that workflow files have persist-credentials disabled\n        entry: ./scripts/ci/pre_commit/checkout_no_credentials.py\n        language: python\n        pass_filenames: true\n        files: ^\\.github/workflows/.*\\.yml$\n        additional_dependencies: ['PyYAML', 'rich>=12.4.4']\n      - id: check-docstring-param-types\n        name: Check that docstrings do not specify param types\n        entry: ./scripts/ci/pre_commit/docstring_param_type.py\n        language: python\n        pass_filenames: true\n        files: \\.py$\n        additional_dependencies: ['rich>=12.4.4']\n      - id: check-code-deprecations\n        name: Check deprecations categories in decorators\n        entry: ./scripts/ci/pre_commit/check_deprecations.py\n        language: python\n        pass_filenames: true\n        files: ^airflow/.*\\.py$\n        additional_dependencies: [\"rich>=12.4.4\", \"python-dateutil\"]\n      - id: lint-chart-schema\n        name: Lint chart/values.schema.json file\n        entry: ./scripts/ci/pre_commit/chart_schema.py\n        language: python\n        pass_filenames: false\n        files: ^chart/values\\.schema\\.json$\n        require_serial: true\n      - id: update-inlined-dockerfile-scripts\n        name: Inline Dockerfile and Dockerfile.ci scripts\n        entry: ./scripts/ci/pre_commit/inline_scripts_in_docker.py\n        language: python\n        pass_filenames: false\n        files: ^Dockerfile$|^Dockerfile\\.ci$|^scripts/docker/.*$\n        require_serial: true\n      - id: check-changelog-has-no-duplicates\n        name: Check changelogs for duplicate entries\n        language: python\n        files: CHANGELOG\\.(rst|txt)$\n        entry: ./scripts/ci/pre_commit/changelog_duplicates.py\n        pass_filenames: true\n      - id: check-changelog-format\n        name: Check changelog format\n        language: python\n        files: CHANGELOG\\.(rst|txt)$\n        entry: ./scripts/ci/pre_commit/check_changelog_format.py\n        pass_filenames: true\n      - id: check-newsfragments-are-valid\n        name: Check newsfragments are valid\n        language: python\n        files: newsfragments/.*\\.rst\n        entry: ./scripts/ci/pre_commit/newsfragments.py\n        pass_filenames: true\n        # We sometimes won't have newsfragments in the repo, so always run it so `check-hooks-apply` passes\n        # This is fast, so not too much downside\n        always_run: true\n      - id: update-breeze-cmd-output\n        name: Update breeze docs\n        description: Update output of breeze commands in Breeze documentation\n        entry: ./scripts/ci/pre_commit/breeze_cmd_line.py\n        language: python\n        files: >\n          (?x)\n          ^dev/breeze/.*$|\n          ^\\.pre-commit-config\\.yaml$|\n          ^scripts/ci/pre_commit/breeze_cmd_line.py$|\n          ^generated/provider_dependencies.json$\n        require_serial: true\n        pass_filenames: false\n        additional_dependencies: ['rich>=12.4.4']\n      - id: check-example-dags-urls\n        name: Check that example dags url include provider versions\n        entry: ./scripts/ci/pre_commit/update_example_dags_paths.py\n        language: python\n        pass_filenames: true\n        files: ^docs/.*example-dags\\.rst$|^docs/.*index\\.rst$\n        additional_dependencies: ['rich>=12.4.4', 'pyyaml']\n        always_run: true\n      - id: check-system-tests-tocs\n        name: Check that system tests is properly added\n        entry: ./scripts/ci/pre_commit/check_system_tests_hidden_in_index.py\n        language: python\n        pass_filenames: true\n        files: ^docs/apache-airflow-providers-[^/]*/index\\.rst$\n        additional_dependencies: ['rich>=12.4.4', 'pyyaml']\n      - id: check-lazy-logging\n        name: Check that all logging methods are lazy\n        entry: ./scripts/ci/pre_commit/check_lazy_logging.py\n        language: python\n        pass_filenames: true\n        files: \\.py$\n        additional_dependencies: ['rich>=12.4.4', 'astor']\n      - id: create-missing-init-py-files-tests\n        name: Create missing init.py files in tests\n        entry: ./scripts/ci/pre_commit/check_init_in_tests.py\n        language: python\n        additional_dependencies: ['rich>=12.4.4']\n        pass_filenames: false\n        files: ^tests/.*\\.py$\n      - id: ts-compile-format-lint-www\n        name: Compile / format / lint WWW\n        description: TS types generation / ESLint / Prettier against UI files\n        language: node\n        'types_or': [javascript, ts, tsx, yaml, css, json]\n        files: ^airflow/www/static/(js|css)/|^airflow/api_connexion/openapi/v1\\.yaml$\n        entry: ./scripts/ci/pre_commit/lint_www.py\n        additional_dependencies: ['yarn@1.22.21', \"openapi-typescript@>=6.7.4\"]\n        pass_filenames: false\n      - id: ts-compile-format-lint-ui\n        name: Compile / format / lint UI\n        description: TS types generation / ESLint / Prettier new UI files\n        language: node\n        types_or: [javascript, ts, tsx, yaml, css, json]\n        files: ^airflow/ui/|^airflow/api_fastapi/core_api/openapi/v1-generated\\.yaml$\n        entry: ./scripts/ci/pre_commit/lint_ui.py\n        additional_dependencies: ['pnpm@9.7.1']\n        pass_filenames: false\n      - id: check-tests-unittest-testcase\n        name: Unit tests do not inherit from unittest.TestCase\n        description: Check that unit tests do not inherit from unittest.TestCase\n        entry: ./scripts/ci/pre_commit/unittest_testcase.py\n        language: python\n        pass_filenames: true\n        files: ^tests/.*\\.py$\n      - id: check-usage-of-re2-over-re\n        language: pygrep\n        name: Use re2 module instead of re\n        description: Use re2 module instead of re\n        entry: \"^\\\\s*from re\\\\s|^\\\\s*import re\\\\s\"\n        pass_filenames: true\n        files: \\.py$\n        exclude: |\n          (?x)\n          ^airflow/configuration.py$ |\n          ^airflow/metrics/validators.py$ |\n          ^airflow/models/dag.py$ |\n          ^airflow/serialization/serde.py$ |\n          ^airflow/utils/file.py$ |\n          ^airflow/utils/helpers.py$ |\n          ^airflow/utils/log/secrets_masker.py$ |\n          ^providers/src/airflow/providers/ |\n          ^(providers/)?tests/ |\n          task_sdk/src/airflow/sdk/definitions/dag.py$ |\n          task_sdk/src/airflow/sdk/definitions/_internal/node.py$ |\n          ^dev/.*\\.py$ |\n          ^scripts/.*\\.py$ |\n          ^docker_tests/.*$ |\n          ^helm_tests/.*$ |\n          ^tests_common/.*$ |\n          ^docs/.*\\.py$ |\n          ^hatch_build.py$\n      - id: check-provider-docs-valid\n        name: Validate provider doc files\n        entry: ./scripts/ci/pre_commit/check_provider_docs.py\n        language: python\n        files: ^providers/src/airflow/providers/.*/provider\\.yaml|^docs/.*\n        additional_dependencies: ['rich>=12.4.4', 'pyyaml', 'jinja2']\n        require_serial: true\n      - id: bandit\n        name: bandit\n        description: \"Bandit is a tool for finding common security issues in Python code\"\n        entry: bandit\n        language: python\n        language_version: python3\n        types: [ python ]\n        additional_dependencies: ['bandit==1.7.6']\n        require_serial: true\n        files: ^airflow/.*\n        exclude:\n          airflow/example_dags/.*\n        args:\n          - \"--skip\"\n          - \"B101,B301,B324,B403,B404,B603\"\n          - \"--severity-level\"\n          - \"high\"  # TODO: remove this line when we fix all the issues\n      - id: pylint\n        name: pylint\n        description: \"Pylint is a static code analyser for Python 2 or 3.\"\n        entry: pylint\n        language: python\n        language_version: python3\n        types: [ python ]\n        additional_dependencies: ['pylint==3.1.0']\n        require_serial: true\n        files: ^airflow/.*\n        exclude:\n          airflow/example_dags/.*\n        args:\n          # Use pylint only for the specific check, which are not available into the ruff\n          - \"--disable=all\"\n          # W0133: \"Exception statement has no effect\"\n          # see: https://github.com/astral-sh/ruff/issues/10145\n          - \"--enable=W0133\"\n      - id: check-fab-migrations\n        language: pygrep\n        name: Check no migration is done on FAB related table\n        description: >\n          FAB tables are no longer used in core Airflow but in FAB provider.\n          As such, it is forbidden to create migrations related to FAB tables in core Airflow.\n          Such migrations should be in FAB provider. To achieve this, a new capability must be implemented:\n          support migrations for providers. In other words, providers need to be able to specify migrations\n          so that, any FAB related migration (besides the legacy ones) is defined in FAB provider.\n          See https://github.com/apache/airflow/issues/32210\n        entry: >\n          (?ix)\n          \\bab_permission\\b|\n          \\bab_view_menu\\b|\n          \\bab_role\\b|\n          \\bab_permission_view\\b|\n          \\bab_permission_view_role\\b|\n          \\bab_user\\b|\n          \\bab_user_role\\b|\n          \\bab_register_user\\b\n        pass_filenames: true\n        files: ^airflow/migrations/versions/.*\\.py$\n        # These migrations contain FAB related changes but existed before moving FAB auth manager\n        # to its own provider\n        exclude: >\n          (?ix)^(\n            airflow/migrations/versions/00.*\\.py|\n            airflow/migrations/versions/0106.*\\.py|\n            airflow/migrations/versions/0118.*\\.py|\n            airflow/migrations/versions/0119.*\\.py|\n            airflow/migrations/versions/0121.*\\.py|\n            airflow/migrations/versions/0124.*\\.py\n          )$\n        ## ADD MOST PRE-COMMITS ABOVE THAT LINE\n        # The below pre-commits are those requiring CI image to be built\n      - id: mypy-dev\n        name: Run mypy for dev\n        language: python\n        entry: ./scripts/ci/pre_commit/mypy.py\n        files: ^dev/.*\\.py$|^scripts/.*\\.py$\n        require_serial: true\n        additional_dependencies: ['rich>=12.4.4']\n      - id: mypy-dev\n        stages: [ 'manual' ]\n        name: Run mypy for dev (manual)\n        language: python\n        entry: ./scripts/ci/pre_commit/mypy_folder.py dev\n        pass_filenames: false\n        files: ^.*\\.py$\n        require_serial: true\n        additional_dependencies: ['rich>=12.4.4']\n      - id: mypy-airflow\n        name: Run mypy for airflow\n        language: python\n        entry: ./scripts/ci/pre_commit/mypy.py\n        files: \\.py$\n        exclude: |\n          (?x)^(\n            airflow/migrations|\n            clients/python/test_.*\\.py|\n            dev|\n            docs|\n            performance/|\n            provider_packages|\n            providers/|\n            scripts|\n            task_sdk/|\n            tests/dags/test_imports\\.py\n          )\n        require_serial: true\n        additional_dependencies: ['rich>=12.4.4']\n      - id: mypy-airflow\n        stages: ['manual']\n        name: Run mypy for airflow (manual)\n        language: python\n        entry: ./scripts/ci/pre_commit/mypy_folder.py airflow\n        pass_filenames: false\n        files: ^.*\\.py$\n        require_serial: true\n        additional_dependencies: ['rich>=12.4.4']\n      - id: mypy-providers\n        name: Run mypy for providers\n        language: python\n        entry: ./scripts/ci/pre_commit/mypy.py --namespace-packages\n        files: ^providers/src/airflow/providers/.*\\.py$|^providers/tests//.*\\.py$\n        require_serial: true\n        additional_dependencies: ['rich>=12.4.4']\n      - id: mypy-providers\n        stages: ['manual']\n        name: Run mypy for providers (manual)\n        language: python\n        entry: ./scripts/ci/pre_commit/mypy_folder.py providers/src/airflow/providers\n        pass_filenames: false\n        files: ^.*\\.py$\n        require_serial: true\n        additional_dependencies: ['rich>=12.4.4']\n      - id: mypy-docs\n        name: Run mypy for /docs/ folder\n        language: python\n        entry: ./scripts/ci/pre_commit/mypy.py\n        files: ^docs/.*\\.py$\n        exclude: ^docs/rtd-deprecation\n        require_serial: true\n        additional_dependencies: ['rich>=12.4.4']\n      - id: mypy-docs\n        stages: ['manual']\n        name: Run mypy for /docs/ folder (manual)\n        language: python\n        entry: ./scripts/ci/pre_commit/mypy_folder.py docs\n        pass_filenames: false\n        files: ^.*\\.py$\n        require_serial: true\n        additional_dependencies: ['rich>=12.4.4']\n      - id: mypy-task-sdk\n        name: Run mypy for Task SDK\n        language: python\n        entry: ./scripts/ci/pre_commit/mypy.py --namespace-packages\n        files: ^task_sdk/src/airflow/sdk/.*\\.py$|^task_sdk/tests//.*\\.py$\n        require_serial: true\n        additional_dependencies: ['rich>=12.4.4']\n      - id: mypy-task-sdk\n        stages: ['manual']\n        name: Run mypy for Task SDK (manual)\n        language: python\n        entry: ./scripts/ci/pre_commit/mypy_folder.py task_sdk/src/airflow/sdk\n        pass_filenames: false\n        files: ^.*\\.py$\n        require_serial: true\n        additional_dependencies: ['rich>=12.4.4']\n      - id: check-provider-yaml-valid\n        name: Validate provider.yaml files\n        entry: ./scripts/ci/pre_commit/check_provider_yaml_files.py\n        language: python\n        files: ^providers/src/airflow/providers/.*/provider\\.yaml$\n        additional_dependencies: ['rich>=12.4.4']\n        require_serial: true\n      - id: check-template-fields-valid\n        name: Check templated fields mapped in operators/sensors\n        language: python\n        entry: ./scripts/ci/pre_commit/check_template_fields.py\n        files: ^(providers/src/)?airflow/.*/(sensors|operators)/.*\\.py$\n        additional_dependencies: [ 'rich>=12.4.4' ]\n        require_serial: true\n      - id: update-migration-references\n        name: Update migration ref doc\n        language: python\n        entry: ./scripts/ci/pre_commit/migration_reference.py\n        pass_filenames: false\n        files: ^airflow/migrations/versions/.*\\.py$|^docs/apache-airflow/migrations-ref\\.rst$\n        additional_dependencies: ['rich>=12.4.4']\n      - id: generate-openapi-spec\n        name: Generate the FastAPI API spec\n        language: python\n        entry: ./scripts/ci/pre_commit/update_fastapi_api_spec.py\n        pass_filenames: false\n        files: ^airflow/api_fastapi/.*\\.py$\n        additional_dependencies: ['rich>=12.4.4']\n      - id: update-er-diagram\n        name: Update ER diagram\n        language: python\n        entry: ./scripts/ci/pre_commit/update_er_diagram.py\n        pass_filenames: false\n        files: ^airflow/migrations/versions/.*\\.py$|^docs/apache-airflow/migrations-ref\\.rst$\n        additional_dependencies: ['rich>=12.4.4']\n        ## ONLY ADD PRE-COMMITS HERE THAT REQUIRE CI IMAGE\n"
        },
        {
          "name": ".rat-excludes",
          "type": "blob",
          "size": 2.6025390625,
          "content": "# Note: these patterns are applied to single files or directories, not full paths\n# coverage/* will ignore any coverage dir, but airflow/www/static/coverage/* will match nothing\n\n.git-blame-ignore-revs\n.github/*\n.gitignore\n.gitattributes\n.gitrepo\n.airflow_db_initialised\n.airflowignore\n.babelrc\n.coverage\n.codecov.yml\n.codespellignorelines\n.eslintignore\n.eslintrc\n.prettierignore\n.prettierrc\n.rat-excludes\n.stylelintignore\n.stylelintrc\n.venv\nrequirements\nrequirements.txt\n.*log\n.travis.yml\n.*pyc\n.*lock\ndist\nbuild\nairflow.egg-info\napache_airflow.egg-info\n.idea\nmetastore_db\n.*sql\n.*svg\n.*csv\n.*md5\n.*zip\n.*lock\nlogs\n.bash_aliases\nvenv\nclients/*\nfiles/*\ndags/*\nairflow.iml\n.gitmodules\nprod_image_installed_providers.txt\nairflow_pre_installed_providers.txt\n\n# Generated doc files\n.*html\n_build/*\n_static/*\n_images/*\n.buildinfo\nsearchindex.js\n_api/*\n\n# the word list for checking spellings\nspelling_wordlist.txt\n\n# Apache Rat does not detect BSD-2 clause properly\n# it is compatible according to http://www.apache.org/legal/resolved.html#category-a\nkerberos_auth.py\nairflow_api_auth_backend_kerberos_auth_py.html\n3rd-party-licenses/*\nparallel.js\nunderscore.js\njquery.dataTables.min.js\njqClock.min.js\ndagre-d3.min.js\nd3.v3.min.js\nace.js\nnode_modules/*\n.*json\ncoverage/*\ngit_version\nflake8_diff.sh\ncoverage*.xml\n_sources/*\n\nrobots.txt\nrat-results.txt\napache-airflow-.*\\+source.tar.gz.*\napache-airflow-.*\\+bin.tar.gz.*\nPULL_REQUEST_TEMPLATE.md\nPROVIDER_CHANGES*.md\nmanifests/*\nredirects.txt\nreproducible_build.yaml\n\n# Locally mounted files\n.*egg-info/*\n.bash_history\n.bash_aliases\n.inputrc\n\n# the example notebook is ASF 2 licensed but RAT cannot read this\ninput_notebook.ipynb\n\n# .git might be a file in case of worktree\n.git\ntmp\n\n# Vendored-in code\n.*_vendor\n\n# generated doc-only-changes.txt\n.latest-doc-only-change.txt\n\n# generated provider_packages\nprovider_packages\n\n# Chart ignored files\nchart/.gitignore\nchart/values.schema.json\nchart/Chart.lock\nchart/values_schema.schema.json\nchart/Chart.yaml\n\n# Generated autocomplete files\n./dev/breeze/autocomplete/*\n\n# Generated devel_deps files\ndevel_deps.txt\n\n# Newsfragments are snippets that will be, eventually, consumed into RELEASE_NOTES\nnewsfragments/*\n\n# Warning file generated\nwarnings.txt\nwarn-summary-*.txt\n\n# Dev stuff\ntests/*\nscripts/*\nimages/*\ndev/*\nchart/*.iml\nout/*\nairflow-build-dockerfile*\n\n# Sha files\n.*sha256\n\n# DOAP file\ndoap_airflow.rdf\n\n# pyenv\n.python-version\n\n# nvm (Node Version Manager)\n.nvmrc\n\n# PKG-INFO file\nPKG-INFO\n\n# checksum files\n.*\\.md5sum\n\n# Openapi files\n.openapi-generator-ignore\nversion.txt\nv1-generated.yaml\n\n# Front end generated files\napi-generated.ts\nopenapi-gen\npnpm-lock.yaml\n"
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 1,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n---\nversion: 2\nformats: []\nsphinx:\n    configuration: docs/rtd-deprecation/conf.py\npython:\n    version: \"3.9\"\n    install:\n        - method: pip\n          path: .\n          extra_requirements:\n              - doc\n    system_packages: true\n"
        },
        {
          "name": "3rd-party-licenses",
          "type": "tree",
          "content": null
        },
        {
          "name": "BREEZE.rst",
          "type": "blob",
          "size": 0.8896484375,
          "content": " .. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\nContent of this guide has been moved to `Breeze docs internal folder <dev/breeze/doc/README.rst>`_\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 1.1044921875,
          "content": "<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n-->\n\n# Contributor Covenant Code of Conduct\n\nThe Apache Airflow project follows the [Apache Software Foundation code of conduct](https://www.apache.org/foundation/policies/conduct.html).\n\nIf you observe behavior that violates those rules please follow the [ASF reporting guidelines](https://www.apache.org/foundation/policies/conduct#reporting-guidelines).\n"
        },
        {
          "name": "COMMITTERS.rst",
          "type": "blob",
          "size": 11.2646484375,
          "content": ".. Licensed to the Apache Software Foundation (ASF) under one\n   or more contributor license agreements.  See the NOTICE file\n   distributed with this work for additional information\n   regarding copyright ownership.  The ASF licenses this file\n   to you under the Apache License, Version 2.0 (the\n   \"License\"); you may not use this file except in compliance\n   with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\nCommitters and PMC members\n==========================\n\nBefore reading this document, you should be familiar with `Contributors' guide <contributing-docs/README.rst>`__.\nThis document assumes that you are a bit familiar how Airflow's community work, but you would like to learn more\nabout the rules by which we add new members.\n\n**The outline for this document in GitHub is available at top-right corner button (with 3-dots and 3 lines).**\n\nCommitters vs. Maintainers\n--------------------------\n\nOften you can hear two different terms about people who have write access to the Airflow repository -\n\"committers\" and \"maintainers\". This is because those two terms are used in different contexts.\n\n* \"Maintainers\" is term used in GitHub documentation and configuration and is a generic term referring to\n  people who have write access to the repository. They can merge PRs, push to the repository, etc.\n* \"Committers\" is a term used in Apache Software Foundation (ASF) and is a term referring to people who have\n  write access to the code repository and has a signed\n  [Contributor License Agreement (CLA)](https://www.apache.org/licenses/#clas) on file.  They have an\n  apache.org mail address. This is an official [role](https://www.apache.org/foundation/how-it-works/#roles)\n  defined and governed by the Apache Software Foundation.\n\nFor all practical purposes, both terms are interchangeable because the Apache Software Foundation rule is\nthe only Committers can have write access to the repositories managed by the PMC (Project Management Committee)\nand that all Committers get write access to the repository.\n\nYou will see both terms used in different documentation, therefore our goal is not to use one of the terms\nonly - it is unavoidable to see both terms anyway. As a rule, we are using \"committer\" term in the context\nof the official rules concerning Apache Software Foundation and \"maintainer\" term in the context where\ntechnical GitHub access and permissions to the project are important.\n\nGuidelines to become an Airflow Committer\n------------------------------------------\n\nCommitters are community members who have write access to the project's\nrepositories, i.e., they can modify the code, documentation, and website by themselves and also\naccept other contributions. There is no strict protocol for becoming a committer. Candidates for new\ncommitters are typically people that are active contributors and community members.\n\nSome people might be active in several of those areas and while they might have not enough 'achievements' in any\nsingle one of those, their combined contributions in several areas all count.\n\nAs a community, we appreciate contributions to the Airflow codebase, but we also place equal value\non those who help Airflow by improving the community in some way. It is entirely possible to become\na committer (and eventually a PMC member) without ever having to change a single line of code.\n\n\nPrerequisites\n^^^^^^^^^^^^^^\n\nGeneral prerequisites that we look for in all candidates:\n\n1.  Consistent contribution over last few months\n2.  Visibility on discussions on the dev mailing list, Slack channels or GitHub issues/discussions\n3.  Contributions to community health and project's sustainability for the long-term\n4.  Understands contributor/committer guidelines: `Contributors' Guide <contributing-docs/README.rst>`__\n\n\nCode contribution\n^^^^^^^^^^^^^^^^^^\n\n1.  Makes high-quality commits (especially commit messages), and assess the impact of the changes, including\n    upgrade paths or deprecation policies\n2.  Testing Release Candidates to help the release cycle\n3.  Proposed and led to completion Airflow Improvement Proposal(s)\n4.  Demonstrates an understanding of one of the following areas or has displayed a holistic understanding\n    of a particular part and made contributions towards a more strategic goal\n\n    - Airflow Core\n    - API\n    - Docker Image\n    - Helm Chart\n    - Dev Tools (Breeze / CI)\n    - Certain Providers\n\n5.  Has made a significant improvement or added an integration with services/technologies important to the Airflow\n    Ecosystem\n\n6.  Actively participated in the security process, as a member of security team, discussing, assessing and\n    fixing security issues.\n\n\nCommunity contributions\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n1.  Actively participates in `triaging issues <ISSUE_TRIAGE_PROCESS.rst>`_ showing their understanding\n    of various areas of Airflow and willingness to help other community members.\n2.  Improves documentation of Airflow in significant way\n3.  Leads/implements changes and improvements introduction in the \"community\" processes and tools\n4.  Actively spreads the word about Airflow, for example organising Airflow summit, workshops for\n    community members, giving and recording talks, writing blogs\n5.  Reporting bugs with detailed reproduction steps\n\n\nCommitter Responsibilities\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nCommitters are more than contributors. While it's important for committers to maintain standing by\ncommitting code, their key role is to build and foster a healthy and active community.\nThis means that committers should:\n\n* Review PRs in a timely and reliable fashion\n* They should also help to actively whittle down the PR backlog\n* Answer questions (i.e. on the dev list, in PRs, in GitHub Issues, slack, etc...)\n* Take on core changes/bugs/feature requests\n* Some changes are important enough that a committer needs to ensure it gets done. This is especially\n  the case if no one from the community is taking it on.\n* Improve processes and tooling\n* Refactoring code\n\n\nGuidelines for promoting Committers to Airflow PMC\n---------------------------------------------------\n\nTo become a PMC member the committers should meet all **general prerequisites**.\nApart from that the person should demonstrate distinct **community involvement** or **code contributions**.\n\nGuidelines from ASF are listed at\n`ASF: New Candidates for Committership <http://community.apache.org/newcommitter.html#guidelines-for-assessing-new-candidates-for-committership>`__.\n\nPrerequisites\n^^^^^^^^^^^^^^\n\n* Has been a committer for at least 3 months\n* Is still active community member (Visible on mailing list or reviewing PRs at the minimum)\n\nCommunity involvement\n^^^^^^^^^^^^^^^^^^^^^^\n\n* Visibility on discussions on the dev mailing list\n* Spreading the word for \"Airflow\" either:\n\n  * Talks at meetups, conferences, etc\n  * Creating content like videos, blogs, etc\n\n* Growing the community:\n\n  * Mentors new members/contributors\n  * Answers users/contributors via GitHub issues, dev list or slack\n\nCode contribution\n^^^^^^^^^^^^^^^^^^\n\n* Consistent voting on RCs for at least past 3 releases lifecycles\n* Engagement in Airflow Improvements Proposals either:\n\n  * Has been actively voting on AIPs\n  * Has been proposing and leading their implementation\n\n* Actively involved in code contributions:\n\n  * Code reviews\n  * Merging pull requests\n  * Fixing bugs and implementing improvements\n  * Actively participating in the security process and significantly contributing to overall security of\n    Airflow\n\n\nOnly a current PMC member can nominate a current committer to be part of PMC.\n\nIf the vote fails or PMC members needs more evidence, then one of the PMC Member (who is not the Proposer)\ncan become the Mentor and guide the proposed candidates on how they can become a PMC member.\n\n1.  Candidate Proposer\n\n    This is the person who launches the DISCUSS thread & makes the case for a PMC member promotion\n\n2.  Candidate Mentor\n\n    If the committee does not have enough information, requires more time, or requires more evidence of\n    candidate's eligibility, a mentor, who is not the proposer, is selected to help mentor the candidate\n    The mentor should try to remain impartial -- their goal is to provide the missing evidence and to\n    try to coach/mentor the candidate to success.\n\n    In order to re-raise a candidate vote, both Proposer and Mentor must be in favor. Again,\n    the mentor must try to remain impartial and cannot be the Proposer.\n\n\nInactive Committers\n-------------------\nIf you know you are not going to be able to contribute for a long time\n(for instance, due to a change of job or circumstances), you should inform the PMC and we will mark you\nas \"inactive\". Inactive committers will be removed from the \"roster\" on ASF and will no longer have the power\nof being a Committer (especially write access to the repos). As merit earned never expires, once you\nbecome active again you can simply email the PMC and ask to be reinstated.\n\nThe PMC also can mark committers as inactive after they have not been involved in the community for\nmore than 12 months.\n\nNew Committer Onboarding Steps\n------------------------------\n\nTo be able to merge PRs, committers have to integrate their GitHub ID with Apache systems. To do that follow steps:\n\n1.  Verify you have a GitHub ID `enabled with 2FA <https://help.github.com/articles/securing-your-account-with-two-factor-authentication-2fa/>`__.\n2.  Merge your Apache and GitHub accounts using `GitBox (Apache Account Linking utility) <https://gitbox.apache.org/setup/>`__. This also asks you to link your\n    Github id to your Apache account. You should see 5 green checks in GitBox.\n3.  Wait at least 30  minutes for an email inviting you to Apache GitHub Organization and accept invitation.\n4.  After accepting the GitHub Invitation verify that you are a member of the `Airflow committers team on GitHub <https://github.com/orgs/apache/teams/airflow-committers>`__.\n5.  Ask in ``#internal-airflow-ci-cd`` channel to be `configured in self-hosted runners <https://github.com/apache/airflow-ci-infra/blob/main/scripts/list_committers>`_\n    by the CI team. Wait for confirmation that this is done and some helpful tips from the CI team\n6.  After confirming that step 5 is done, open a PR to include your GitHub ID in:\n\n    * ``dev/breeze/src/airflow_breeze/global_constants.py`` (COMMITTERS variable)\n    * name and GitHub ID in `project.rst <https://github.com/apache/airflow/blob/main/docs/apache-airflow/project.rst>`__.\n    * If you had been a collaborator role before getting committer, remove your Github ID from ``.asf.yaml``.\n7.  Raise a PR to `airflow-site <https://github.com/apache/airflow-site>`_ repository with the following additions:\n\n    * List your name(s) in the `committers list <https://github.com/apache/airflow-site/blob/main/landing-pages/site/data/committers.json>`__.\n    * Post an entry in `Announcements <https://github.com/apache/airflow-site/blob/main/landing-pages/site/content/en/announcements/_index.md>`__.\n\n    **A kind request**: If there are other committers who joined around the same time, please create a unified PR for all of you together.\n"
        },
        {
          "name": "CONTRIBUTING.rst",
          "type": "blob",
          "size": 1.1005859375,
          "content": ".. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\nContributing\n============\n\n**The outline for this document in GitHub is available at top-right corner button (with 3-dots and 3 lines).**\n\nContributions are welcome and are greatly appreciated! Every little bit helps, and credit will always be given.\n\nGo to `Contributors' guide <./contributing-docs/README.rst>`__.\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 67.98046875,
          "content": "# syntax=docker/dockerfile:1.4\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# THIS DOCKERFILE IS INTENDED FOR PRODUCTION USE AND DEPLOYMENT.\n# NOTE! IT IS ALPHA-QUALITY FOR NOW - WE ARE IN A PROCESS OF TESTING IT\n#\n#\n# This is a multi-segmented image. It actually contains two images:\n#\n# airflow-build-image  - there all airflow dependencies can be installed (and\n#                        built - for those dependencies that require\n#                        build essentials). Airflow is installed there with\n#                        ${HOME}/.local virtualenv which is also considered\n#                        As --user folder by python when creating venv with\n#                        --system-site-packages\n#\n# main                 - this is the actual production image that is much\n#                        smaller because it does not contain all the build\n#                        essentials. Instead the ${HOME}/.local folder\n#                        is copied from the build-image - this way we have\n#                        only result of installation and we do not need\n#                        all the build essentials. This makes the image\n#                        much smaller.\n#\n# Use the same builder frontend version for everyone\nARG AIRFLOW_EXTRAS=\"aiobotocore,amazon,async,celery,cncf-kubernetes,common-io,docker,elasticsearch,fab,ftp,google,google-auth,graphviz,grpc,hashicorp,http,ldap,microsoft-azure,mysql,odbc,openlineage,pandas,postgres,redis,sendgrid,sftp,slack,snowflake,ssh,statsd,uv\"\nARG ADDITIONAL_AIRFLOW_EXTRAS=\"\"\nARG ADDITIONAL_PYTHON_DEPS=\"\"\n\nARG AIRFLOW_HOME=/opt/airflow\nARG AIRFLOW_UID=\"50000\"\nARG AIRFLOW_USER_HOME_DIR=/home/airflow\n\n# latest released version here\nARG AIRFLOW_VERSION=\"2.10.4\"\n\nARG PYTHON_BASE_IMAGE=\"python:3.9-slim-bookworm\"\n\n\n# You can swap comments between those two args to test pip from the main version\n# When you attempt to test if the version of `pip` from specified branch works for our builds\n# Also use `force pip` label on your PR to swap all places we use `uv` to `pip`\nARG AIRFLOW_PIP_VERSION=24.3.1\n# ARG AIRFLOW_PIP_VERSION=\"git+https://github.com/pypa/pip.git@main\"\nARG AIRFLOW_UV_VERSION=0.5.14\nARG AIRFLOW_USE_UV=\"false\"\nARG UV_HTTP_TIMEOUT=\"300\"\nARG AIRFLOW_IMAGE_REPOSITORY=\"https://github.com/apache/airflow\"\nARG AIRFLOW_IMAGE_README_URL=\"https://raw.githubusercontent.com/apache/airflow/main/docs/docker-stack/README.md\"\n\n# By default we install latest airflow from PyPI so we do not need to copy sources of Airflow\n# from the host - so we are using Dockerfile and copy it to /Dockerfile in target image\n# because this is the only file we know exists locally. This way you can build the image in PyPI with\n# **just** the Dockerfile and no need for any other files from Airflow repository.\n# However, in case of breeze/development use we use latest sources and we override those\n# SOURCES_FROM/TO with \".\" and \"/opt/airflow\" respectively - so that sources of Airflow (and all providers)\n# are used to build the PROD image used in tests.\nARG AIRFLOW_SOURCES_FROM=\"Dockerfile\"\nARG AIRFLOW_SOURCES_TO=\"/Dockerfile\"\n\n# By default latest released version of airflow is installed (when empty) but this value can be overridden\n# and we can install version according to specification (For example ==2.0.2 or <3.0.0).\nARG AIRFLOW_VERSION_SPECIFICATION=\"\"\n\n# By default PIP has progress bar but you can disable it.\nARG PIP_PROGRESS_BAR=\"on\"\n\n##############################################################################################\n# This is the script image where we keep all inlined bash scripts needed in other segments\n##############################################################################################\nFROM scratch as scripts\n\n##############################################################################################\n# Please DO NOT modify the inlined scripts manually. The content of those files will be\n# replaced by pre-commit automatically from the \"scripts/docker/\" folder.\n# This is done in order to avoid problems with caching and file permissions and in order to\n# make the PROD Dockerfile standalone\n##############################################################################################\n\n# The content below is automatically copied from scripts/docker/install_os_dependencies.sh\nCOPY <<\"EOF\" /install_os_dependencies.sh\n#!/usr/bin/env bash\nset -euo pipefail\n\nif [[ \"$#\" != 1 ]]; then\n    echo \"ERROR! There should be 'runtime' or 'dev' parameter passed as argument.\".\n    exit 1\nfi\n\nif [[ \"${1}\" == \"runtime\" ]]; then\n    INSTALLATION_TYPE=\"RUNTIME\"\nelif   [[ \"${1}\" == \"dev\" ]]; then\n    INSTALLATION_TYPE=\"dev\"\nelse\n    echo \"ERROR! Wrong argument. Passed ${1} and it should be one of 'runtime' or 'dev'.\".\n    exit 1\nfi\n\nfunction get_dev_apt_deps() {\n    if [[ \"${DEV_APT_DEPS=}\" == \"\" ]]; then\n        DEV_APT_DEPS=\"apt-transport-https apt-utils build-essential ca-certificates dirmngr \\\nfreetds-bin freetds-dev git graphviz graphviz-dev krb5-user ldap-utils libev4 libev-dev libffi-dev libgeos-dev \\\nlibkrb5-dev libldap2-dev libleveldb1d libleveldb-dev libsasl2-2 libsasl2-dev libsasl2-modules \\\nlibssl-dev libxmlsec1 libxmlsec1-dev locales lsb-release openssh-client pkgconf sasl2-bin \\\nsoftware-properties-common sqlite3 sudo unixodbc unixodbc-dev zlib1g-dev\"\n        export DEV_APT_DEPS\n    fi\n}\n\nfunction get_runtime_apt_deps() {\n    local debian_version\n    local debian_version_apt_deps\n    # Get debian version without installing lsb_release\n    # shellcheck disable=SC1091\n    debian_version=$(. /etc/os-release;   printf '%s\\n' \"$VERSION_CODENAME\";)\n    echo\n    echo \"DEBIAN CODENAME: ${debian_version}\"\n    echo\n    debian_version_apt_deps=\"libffi8 libldap-2.5-0 libssl3 netcat-openbsd\"\n    echo\n    echo \"APPLIED INSTALLATION CONFIGURATION FOR DEBIAN VERSION: ${debian_version}\"\n    echo\n    if [[ \"${RUNTIME_APT_DEPS=}\" == \"\" ]]; then\n        RUNTIME_APT_DEPS=\"apt-transport-https apt-utils ca-certificates \\\ncurl dumb-init freetds-bin krb5-user libev4 libgeos-dev \\\nldap-utils libsasl2-2 libsasl2-modules libxmlsec1 locales ${debian_version_apt_deps} \\\nlsb-release openssh-client python3-selinux rsync sasl2-bin sqlite3 sudo unixodbc\"\n        export RUNTIME_APT_DEPS\n    fi\n}\n\nfunction install_docker_cli() {\n    apt-get update\n    apt-get install ca-certificates curl\n    install -m 0755 -d /etc/apt/keyrings\n    curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc\n    chmod a+r /etc/apt/keyrings/docker.asc\n    # shellcheck disable=SC1091\n    echo \\\n      \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian \\\n      $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n      tee /etc/apt/sources.list.d/docker.list > /dev/null\n    apt-get update\n    apt-get install -y --no-install-recommends docker-ce-cli\n}\n\nfunction install_debian_dev_dependencies() {\n    apt-get update\n    apt-get install -yqq --no-install-recommends apt-utils >/dev/null 2>&1\n    apt-get install -y --no-install-recommends curl gnupg2 lsb-release\n    # shellcheck disable=SC2086\n    export ${ADDITIONAL_DEV_APT_ENV?}\n    if [[ ${DEV_APT_COMMAND} != \"\" ]]; then\n        bash -o pipefail -o errexit -o nounset -o nolog -c \"${DEV_APT_COMMAND}\"\n    fi\n    if [[ ${ADDITIONAL_DEV_APT_COMMAND} != \"\" ]]; then\n        bash -o pipefail -o errexit -o nounset -o nolog -c \"${ADDITIONAL_DEV_APT_COMMAND}\"\n    fi\n    apt-get update\n    local debian_version\n    local debian_version_apt_deps\n    # Get debian version without installing lsb_release\n    # shellcheck disable=SC1091\n    debian_version=$(. /etc/os-release;   printf '%s\\n' \"$VERSION_CODENAME\";)\n    echo\n    echo \"DEBIAN CODENAME: ${debian_version}\"\n    echo\n    # shellcheck disable=SC2086\n    apt-get install -y --no-install-recommends ${DEV_APT_DEPS} ${ADDITIONAL_DEV_APT_DEPS}\n}\n\nfunction install_debian_runtime_dependencies() {\n    apt-get update\n    apt-get install --no-install-recommends -yqq apt-utils >/dev/null 2>&1\n    apt-get install -y --no-install-recommends curl gnupg2 lsb-release\n    # shellcheck disable=SC2086\n    export ${ADDITIONAL_RUNTIME_APT_ENV?}\n    if [[ \"${RUNTIME_APT_COMMAND}\" != \"\" ]]; then\n        bash -o pipefail -o errexit -o nounset -o nolog -c \"${RUNTIME_APT_COMMAND}\"\n    fi\n    if [[ \"${ADDITIONAL_RUNTIME_APT_COMMAND}\" != \"\" ]]; then\n        bash -o pipefail -o errexit -o nounset -o nolog -c \"${ADDITIONAL_RUNTIME_APT_COMMAND}\"\n    fi\n    apt-get update\n    # shellcheck disable=SC2086\n    apt-get install -y --no-install-recommends ${RUNTIME_APT_DEPS} ${ADDITIONAL_RUNTIME_APT_DEPS}\n    apt-get autoremove -yqq --purge\n    apt-get clean\n    rm -rf /var/lib/apt/lists/* /var/log/*\n}\n\nif [[ \"${INSTALLATION_TYPE}\" == \"RUNTIME\" ]]; then\n    get_runtime_apt_deps\n    install_debian_runtime_dependencies\n    install_docker_cli\n\nelse\n    get_dev_apt_deps\n    install_debian_dev_dependencies\n    install_docker_cli\nfi\nEOF\n\n# The content below is automatically copied from scripts/docker/install_mysql.sh\nCOPY <<\"EOF\" /install_mysql.sh\n#!/usr/bin/env bash\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\n\nset -euo pipefail\n\ncommon::get_colors\ndeclare -a packages\n\nreadonly MYSQL_LTS_VERSION=\"8.0\"\nreadonly MARIADB_LTS_VERSION=\"10.11\"\n\n: \"${INSTALL_MYSQL_CLIENT:?Should be true or false}\"\n: \"${INSTALL_MYSQL_CLIENT_TYPE:-mariadb}\"\n\ninstall_mysql_client() {\n    if [[ \"${1}\" == \"dev\" ]]; then\n        packages=(\"libmysqlclient-dev\" \"mysql-client\")\n    elif [[ \"${1}\" == \"prod\" ]]; then\n        # `libmysqlclientXX` where XX is number, and it should be increased every new GA MySQL release, for example\n        # 18 - MySQL 5.6.48\n        # 20 - MySQL 5.7.42\n        # 21 - MySQL 8.0.34\n        # 22 - MySQL 8.1\n        packages=(\"libmysqlclient21\" \"mysql-client\")\n    else\n        echo\n        echo \"${COLOR_RED}Specify either prod or dev${COLOR_RESET}\"\n        echo\n        exit 1\n    fi\n\n    common::import_trusted_gpg \"B7B3B788A8D3785C\" \"mysql\"\n\n    echo\n    echo \"${COLOR_BLUE}Installing Oracle MySQL client version ${MYSQL_LTS_VERSION}: ${1}${COLOR_RESET}\"\n    echo\n\n    echo \"deb http://repo.mysql.com/apt/debian/ $(lsb_release -cs) mysql-${MYSQL_LTS_VERSION}\" > \\\n        /etc/apt/sources.list.d/mysql.list\n    apt-get update\n    apt-get install --no-install-recommends -y \"${packages[@]}\"\n    apt-get autoremove -yqq --purge\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n\n    # Remove mysql repository from sources.list.d as MySQL repos have a basic flaw that they put expiry\n    # date on their GPG signing keys and they sign their repo with those keys. This means that after a\n    # certain date, the GPG key becomes invalid and if you have the repository added in your sources.list\n    # then you will not be able to install anything from any other repository. This id unlike any other\n    # repository we have seen (for example Postgres, MariaDB, MsSQL - all have non-expiring signing keys)\n    rm /etc/apt/sources.list.d/mysql.list\n}\n\ninstall_mariadb_client() {\n    # List of compatible package Oracle MySQL -> MariaDB:\n    # `mysql-client` -> `mariadb-client` or `mariadb-client-compat` (11+)\n    # `libmysqlclientXX` (where XX is a number) -> `libmariadb3-compat`\n    # `libmysqlclient-dev` -> `libmariadb-dev-compat`\n    #\n    # Different naming against Debian repo which we used before\n    # that some of packages might contains `-compat` suffix, Debian repo -> MariaDB repo:\n    # `libmariadb-dev` -> `libmariadb-dev-compat`\n    # `mariadb-client-core` -> `mariadb-client` or `mariadb-client-compat` (11+)\n    if [[ \"${1}\" == \"dev\" ]]; then\n        packages=(\"libmariadb-dev-compat\" \"mariadb-client\")\n    elif [[ \"${1}\" == \"prod\" ]]; then\n        packages=(\"libmariadb3-compat\" \"mariadb-client\")\n    else\n        echo\n        echo \"${COLOR_RED}Specify either prod or dev${COLOR_RESET}\"\n        echo\n        exit 1\n    fi\n\n    common::import_trusted_gpg \"0xF1656F24C74CD1D8\" \"mariadb\"\n\n    echo\n    echo \"${COLOR_BLUE}Installing MariaDB client version ${MARIADB_LTS_VERSION}: ${1}${COLOR_RESET}\"\n    echo \"${COLOR_YELLOW}MariaDB client protocol-compatible with MySQL client.${COLOR_RESET}\"\n    echo\n\n    echo \"deb [arch=amd64,arm64] https://archive.mariadb.org/mariadb-${MARIADB_LTS_VERSION}/repo/debian/ $(lsb_release -cs) main\" > \\\n        /etc/apt/sources.list.d/mariadb.list\n    # Make sure that dependencies from MariaDB repo are preferred over Debian dependencies\n    printf \"Package: *\\nPin: release o=MariaDB\\nPin-Priority: 999\\n\" > /etc/apt/preferences.d/mariadb\n    apt-get update\n    apt-get install --no-install-recommends -y \"${packages[@]}\"\n    apt-get autoremove -yqq --purge\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n}\n\nif [[ ${INSTALL_MYSQL_CLIENT:=\"true\"} == \"true\" ]]; then\n    if [[ $(uname -m) == \"arm64\" || $(uname -m) == \"aarch64\" ]]; then\n        INSTALL_MYSQL_CLIENT_TYPE=\"mariadb\"\n        echo\n        echo \"${COLOR_YELLOW}Client forced to mariadb for ARM${COLOR_RESET}\"\n        echo\n    fi\n\n    if [[ \"${INSTALL_MYSQL_CLIENT_TYPE}\" == \"mysql\" ]]; then\n        install_mysql_client \"${@}\"\n    elif [[ \"${INSTALL_MYSQL_CLIENT_TYPE}\" == \"mariadb\" ]]; then\n        install_mariadb_client \"${@}\"\n    else\n        echo\n        echo \"${COLOR_RED}Specify either mysql or mariadb, got ${INSTALL_MYSQL_CLIENT_TYPE}${COLOR_RESET}\"\n        echo\n        exit 1\n    fi\nfi\nEOF\n\n# The content below is automatically copied from scripts/docker/install_mssql.sh\nCOPY <<\"EOF\" /install_mssql.sh\n#!/usr/bin/env bash\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\n\nset -euo pipefail\n\ncommon::get_colors\ndeclare -a packages\n\n: \"${INSTALL_MSSQL_CLIENT:?Should be true or false}\"\n\n\nfunction install_mssql_client() {\n    # Install MsSQL client from Microsoft repositories\n    if [[ ${INSTALL_MSSQL_CLIENT:=\"true\"} != \"true\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Skip installing mssql client${COLOR_RESET}\"\n        echo\n        return\n    fi\n    packages=(\"msodbcsql18\")\n\n    common::import_trusted_gpg \"EB3E94ADBE1229CF\" \"microsoft\"\n\n    echo\n    echo \"${COLOR_BLUE}Installing mssql client${COLOR_RESET}\"\n    echo\n\n    echo \"deb [arch=amd64,arm64] https://packages.microsoft.com/debian/$(lsb_release -rs)/prod $(lsb_release -cs) main\" > \\\n        /etc/apt/sources.list.d/mssql-release.list\n    apt-get update -yqq\n    apt-get upgrade -yqq\n    ACCEPT_EULA=Y apt-get -yqq install --no-install-recommends \"${packages[@]}\"\n    rm -rf /var/lib/apt/lists/*\n    apt-get autoremove -yqq --purge\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n}\n\ninstall_mssql_client \"${@}\"\nEOF\n\n# The content below is automatically copied from scripts/docker/install_postgres.sh\nCOPY <<\"EOF\" /install_postgres.sh\n#!/usr/bin/env bash\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\nset -euo pipefail\n\ncommon::get_colors\ndeclare -a packages\n\n: \"${INSTALL_POSTGRES_CLIENT:?Should be true or false}\"\n\ninstall_postgres_client() {\n    echo\n    echo \"${COLOR_BLUE}Installing postgres client${COLOR_RESET}\"\n    echo\n\n    if [[ \"${1}\" == \"dev\" ]]; then\n        packages=(\"libpq-dev\" \"postgresql-client\")\n    elif [[ \"${1}\" == \"prod\" ]]; then\n        packages=(\"postgresql-client\")\n    else\n        echo\n        echo \"Specify either prod or dev\"\n        echo\n        exit 1\n    fi\n\n    common::import_trusted_gpg \"7FCC7D46ACCC4CF8\" \"postgres\"\n\n    echo \"deb [arch=amd64,arm64] https://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" > \\\n        /etc/apt/sources.list.d/pgdg.list\n    apt-get update\n    apt-get install --no-install-recommends -y \"${packages[@]}\"\n    apt-get autoremove -yqq --purge\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n}\n\nif [[ ${INSTALL_POSTGRES_CLIENT:=\"true\"} == \"true\" ]]; then\n    install_postgres_client \"${@}\"\nfi\nEOF\n\n# The content below is automatically copied from scripts/docker/install_packaging_tools.sh\nCOPY <<\"EOF\" /install_packaging_tools.sh\n#!/usr/bin/env bash\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\n\ncommon::get_colors\ncommon::get_packaging_tool\ncommon::show_packaging_tool_version_and_location\ncommon::install_packaging_tools\nEOF\n\n# The content below is automatically copied from scripts/docker/common.sh\nCOPY <<\"EOF\" /common.sh\n#!/usr/bin/env bash\nset -euo pipefail\n\nfunction common::get_colors() {\n    COLOR_BLUE=$'\\e[34m'\n    COLOR_GREEN=$'\\e[32m'\n    COLOR_RED=$'\\e[31m'\n    COLOR_RESET=$'\\e[0m'\n    COLOR_YELLOW=$'\\e[33m'\n    export COLOR_BLUE\n    export COLOR_GREEN\n    export COLOR_RED\n    export COLOR_RESET\n    export COLOR_YELLOW\n}\n\nfunction common::get_packaging_tool() {\n    : \"${AIRFLOW_USE_UV:?Should be set}\"\n\n    ## IMPORTANT: IF YOU MODIFY THIS FUNCTION YOU SHOULD ALSO MODIFY CORRESPONDING FUNCTION IN\n    ## `scripts/in_container/_in_container_utils.sh`\n    if [[ ${AIRFLOW_USE_UV} == \"true\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Using 'uv' to install Airflow${COLOR_RESET}\"\n        echo\n        export PACKAGING_TOOL=\"uv\"\n        export PACKAGING_TOOL_CMD=\"uv pip\"\n        if [[ -z ${VIRTUAL_ENV=} ]]; then\n            export EXTRA_INSTALL_FLAGS=\"--system\"\n            export EXTRA_UNINSTALL_FLAGS=\"--system\"\n        else\n            export EXTRA_INSTALL_FLAGS=\"\"\n            export EXTRA_UNINSTALL_FLAGS=\"\"\n        fi\n        export UPGRADE_EAGERLY=\"--upgrade --resolution highest\"\n        export UPGRADE_IF_NEEDED=\"--upgrade\"\n        UV_CONCURRENT_DOWNLOADS=$(nproc --all)\n        export UV_CONCURRENT_DOWNLOADS\n    else\n        echo\n        echo \"${COLOR_BLUE}Using 'pip' to install Airflow${COLOR_RESET}\"\n        echo\n        export PACKAGING_TOOL=\"pip\"\n        export PACKAGING_TOOL_CMD=\"pip\"\n        export EXTRA_INSTALL_FLAGS=\"--root-user-action ignore\"\n        export EXTRA_UNINSTALL_FLAGS=\"--yes\"\n        export UPGRADE_EAGERLY=\"--upgrade --upgrade-strategy eager\"\n        export UPGRADE_IF_NEEDED=\"--upgrade --upgrade-strategy only-if-needed\"\n    fi\n}\n\nfunction common::get_airflow_version_specification() {\n    if [[ -z ${AIRFLOW_VERSION_SPECIFICATION=}\n        && -n ${AIRFLOW_VERSION}\n        && ${AIRFLOW_INSTALLATION_METHOD} != \".\" ]]; then\n        AIRFLOW_VERSION_SPECIFICATION=\"==${AIRFLOW_VERSION}\"\n    fi\n}\n\nfunction common::get_constraints_location() {\n    # auto-detect Airflow-constraint reference and location\n    if [[ -z \"${AIRFLOW_CONSTRAINTS_REFERENCE=}\" ]]; then\n        if  [[ ${AIRFLOW_VERSION} =~ v?2.* && ! ${AIRFLOW_VERSION} =~ .*dev.* ]]; then\n            AIRFLOW_CONSTRAINTS_REFERENCE=constraints-${AIRFLOW_VERSION}\n        else\n            AIRFLOW_CONSTRAINTS_REFERENCE=${DEFAULT_CONSTRAINTS_BRANCH}\n        fi\n    fi\n\n    if [[ -z ${AIRFLOW_CONSTRAINTS_LOCATION=} ]]; then\n        local constraints_base=\"https://raw.githubusercontent.com/${CONSTRAINTS_GITHUB_REPOSITORY}/${AIRFLOW_CONSTRAINTS_REFERENCE}\"\n        local python_version\n        python_version=$(python -c 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")')\n        AIRFLOW_CONSTRAINTS_LOCATION=\"${constraints_base}/${AIRFLOW_CONSTRAINTS_MODE}-${python_version}.txt\"\n    fi\n\n    if [[ ${AIRFLOW_CONSTRAINTS_LOCATION} =~ http.* ]]; then\n        echo\n        echo \"${COLOR_BLUE}Downloading constraints from ${AIRFLOW_CONSTRAINTS_LOCATION} to ${HOME}/constraints.txt ${COLOR_RESET}\"\n        echo\n        curl -sSf -o \"${HOME}/constraints.txt\" \"${AIRFLOW_CONSTRAINTS_LOCATION}\"\n    else\n        echo\n        echo \"${COLOR_BLUE}Copying constraints from ${AIRFLOW_CONSTRAINTS_LOCATION} to ${HOME}/constraints.txt ${COLOR_RESET}\"\n        echo\n        cp \"${AIRFLOW_CONSTRAINTS_LOCATION}\" \"${HOME}/constraints.txt\"\n    fi\n}\n\nfunction common::show_packaging_tool_version_and_location() {\n   echo \"PATH=${PATH}\"\n   echo \"Installed pip: $(pip --version): $(which pip)\"\n   if [[ ${PACKAGING_TOOL} == \"pip\" ]]; then\n       echo \"${COLOR_BLUE}Using 'pip' to install Airflow${COLOR_RESET}\"\n   else\n       echo \"${COLOR_BLUE}Using 'uv' to install Airflow${COLOR_RESET}\"\n       echo \"Installed uv: $(uv --version 2>/dev/null || echo \"Not installed yet\"): $(which uv 2>/dev/null)\"\n   fi\n}\n\nfunction common::install_packaging_tools() {\n    : \"${AIRFLOW_USE_UV:?Should be set}\"\n    if [[ \"${VIRTUAL_ENV=}\" != \"\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Checking packaging tools in venv: ${VIRTUAL_ENV}${COLOR_RESET}\"\n        echo\n    else\n        echo\n        echo \"${COLOR_BLUE}Checking packaging tools for system Python installation: $(which python)${COLOR_RESET}\"\n        echo\n    fi\n    if [[ ${AIRFLOW_PIP_VERSION=} == \"\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Installing latest pip version${COLOR_RESET}\"\n        echo\n        pip install --root-user-action ignore --disable-pip-version-check --upgrade pip\n    elif [[ ! ${AIRFLOW_PIP_VERSION} =~ ^[0-9].* ]]; then\n        echo\n        echo \"${COLOR_BLUE}Installing pip version from spec ${AIRFLOW_PIP_VERSION}${COLOR_RESET}\"\n        echo\n        # shellcheck disable=SC2086\n        pip install --root-user-action ignore --disable-pip-version-check \"pip @ ${AIRFLOW_PIP_VERSION}\"\n    else\n        local installed_pip_version\n        installed_pip_version=$(python -c 'from importlib.metadata import version; print(version(\"pip\"))')\n        if [[ ${installed_pip_version} != \"${AIRFLOW_PIP_VERSION}\" ]]; then\n            echo\n            echo \"${COLOR_BLUE}(Re)Installing pip version: ${AIRFLOW_PIP_VERSION}${COLOR_RESET}\"\n            echo\n            pip install --root-user-action ignore --disable-pip-version-check \"pip==${AIRFLOW_PIP_VERSION}\"\n        fi\n    fi\n    if [[ ${AIRFLOW_UV_VERSION=} == \"\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Installing latest uv version${COLOR_RESET}\"\n        echo\n        pip install --root-user-action ignore --disable-pip-version-check --upgrade uv\n    elif [[ ! ${AIRFLOW_UV_VERSION} =~ ^[0-9].* ]]; then\n        echo\n        echo \"${COLOR_BLUE}Installing uv version from spec ${AIRFLOW_UV_VERSION}${COLOR_RESET}\"\n        echo\n        # shellcheck disable=SC2086\n        pip install --root-user-action ignore --disable-pip-version-check \"uv @ ${AIRFLOW_UV_VERSION}\"\n    else\n        local installed_uv_version\n        installed_uv_version=$(python -c 'from importlib.metadata import version; print(version(\"uv\"))' 2>/dev/null || echo \"Not installed yet\")\n        if [[ ${installed_uv_version} != \"${AIRFLOW_UV_VERSION}\" ]]; then\n            echo\n            echo \"${COLOR_BLUE}(Re)Installing uv version: ${AIRFLOW_UV_VERSION}${COLOR_RESET}\"\n            echo\n            # shellcheck disable=SC2086\n            pip install --root-user-action ignore --disable-pip-version-check \"uv==${AIRFLOW_UV_VERSION}\"\n        fi\n    fi\n    if  [[ ${AIRFLOW_PRE_COMMIT_VERSION=} == \"\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Installing latest pre-commit with pre-commit-uv uv${COLOR_RESET}\"\n        echo\n        uv tool install pre-commit --with pre-commit-uv --with uv\n        # make sure that the venv/user in .local exists\n        mkdir -p \"${HOME}/.local/bin\"\n    else\n        echo\n        echo \"${COLOR_BLUE}Installing predefined versions of pre-commit with pre-commit-uv and uv:${COLOR_RESET}\"\n        echo \"${COLOR_BLUE}pre_commit(${AIRFLOW_PRE_COMMIT_VERSION}) uv(${AIRFLOW_UV_VERSION}) pre_commit-uv(${AIRFLOW_PRE_COMMIT_UV_VERSION})${COLOR_RESET}\"\n        echo\n        uv tool install \"pre-commit==${AIRFLOW_PRE_COMMIT_VERSION}\" \\\n            --with \"uv==${AIRFLOW_UV_VERSION}\" --with \"pre-commit-uv==${AIRFLOW_PRE_COMMIT_UV_VERSION}\"\n        # make sure that the venv/user in .local exists\n        mkdir -p \"${HOME}/.local/bin\"\n    fi\n}\n\nfunction common::import_trusted_gpg() {\n    common::get_colors\n\n    local key=${1:?${COLOR_RED}First argument expects OpenPGP Key ID${COLOR_RESET}}\n    local name=${2:?${COLOR_RED}Second argument expected trust storage name${COLOR_RESET}}\n    # Please note that not all servers could be used for retrieve keys\n    #  sks-keyservers.net: Unmaintained and DNS taken down due to GDPR requests.\n    #  keys.openpgp.org: User ID Mandatory, not suitable for APT repositories\n    #  keyring.debian.org: Only accept keys in Debian keyring.\n    #  pgp.mit.edu: High response time.\n    local keyservers=(\n        \"hkps://keyserver.ubuntu.com\"\n        \"hkps://pgp.surf.nl\"\n    )\n\n    GNUPGHOME=\"$(mktemp -d)\"\n    export GNUPGHOME\n    set +e\n    for keyserver in $(shuf -e \"${keyservers[@]}\"); do\n        echo \"${COLOR_BLUE}Try to receive GPG public key ${key} from ${keyserver}${COLOR_RESET}\"\n        gpg --keyserver \"${keyserver}\" --recv-keys \"${key}\" 2>&1 && break\n        echo \"${COLOR_YELLOW}Unable to receive GPG public key ${key} from ${keyserver}${COLOR_RESET}\"\n    done\n    set -e\n    gpg --export \"${key}\" > \"/etc/apt/trusted.gpg.d/${name}.gpg\"\n    gpgconf --kill all\n    rm -rf \"${GNUPGHOME}\"\n    unset GNUPGHOME\n}\nEOF\n\n# The content below is automatically copied from scripts/docker/pip\nCOPY <<\"EOF\" /pip\n#!/usr/bin/env bash\nCOLOR_RED=$'\\e[31m'\nCOLOR_RESET=$'\\e[0m'\nCOLOR_YELLOW=$'\\e[33m'\n\nif [[ $(id -u) == \"0\" ]]; then\n    echo\n    echo \"${COLOR_RED}You are running pip as root. Please use 'airflow' user to run pip!${COLOR_RESET}\"\n    echo\n    echo \"${COLOR_YELLOW}See: https://airflow.apache.org/docs/docker-stack/build.html#adding-a-new-pypi-package${COLOR_RESET}\"\n    echo\n    exit 1\nfi\nexec \"${HOME}\"/.local/bin/pip \"${@}\"\nEOF\n\n# The content below is automatically copied from scripts/docker/install_from_docker_context_files.sh\nCOPY <<\"EOF\" /install_from_docker_context_files.sh\n\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\n\n\nfunction install_airflow_and_providers_from_docker_context_files(){\n    local flags=()\n    if [[ ${INSTALL_MYSQL_CLIENT} != \"true\" ]]; then\n        AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS/mysql,}\n    fi\n    if [[ ${INSTALL_POSTGRES_CLIENT} != \"true\" ]]; then\n        AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS/postgres,}\n    fi\n\n    if [[ ! -d /docker-context-files ]]; then\n        echo\n        echo \"${COLOR_RED}You must provide a folder via --build-arg DOCKER_CONTEXT_FILES=<FOLDER> and you missed it!${COLOR_RESET}\"\n        echo\n        exit 1\n    fi\n\n    # This is needed to get package names for local context packages\n    ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} ${ADDITIONAL_PIP_INSTALL_FLAGS} --constraint ${HOME}/constraints.txt packaging\n\n    if [[ -n ${AIRFLOW_EXTRAS=} ]]; then\n        AIRFLOW_EXTRAS_TO_INSTALL=\"[${AIRFLOW_EXTRAS}]\"\n    else\n        AIRFLOW_EXTRAS_TO_INSTALL=\"\"\n    fi\n\n    # Find Apache Airflow package in docker-context files\n    readarray -t install_airflow_package < <(EXTRAS=\"${AIRFLOW_EXTRAS_TO_INSTALL}\" \\\n        python /scripts/docker/get_package_specs.py /docker-context-files/apache?airflow?[0-9]*.{whl,tar.gz} 2>/dev/null || true)\n    echo\n    echo \"${COLOR_BLUE}Found airflow packages in docker-context-files folder: ${install_airflow_package[*]}${COLOR_RESET}\"\n    echo\n\n    if [[ -z \"${install_airflow_package[*]}\" && ${AIRFLOW_VERSION=} != \"\" ]]; then\n        # When we install only provider packages from docker-context files, we need to still\n        # install airflow from PyPI when AIRFLOW_VERSION is set. This handles the case where\n        # pre-release dockerhub image of airflow is built, but we want to install some providers from\n        # docker-context files\n        install_airflow_package=(\"apache-airflow[${AIRFLOW_EXTRAS}]==${AIRFLOW_VERSION}\")\n    fi\n\n    # Find Provider/TaskSDK packages in docker-context files\n    readarray -t airflow_packages< <(python /scripts/docker/get_package_specs.py /docker-context-files/apache?airflow?{providers,task?sdk}*.{whl,tar.gz} 2>/dev/null || true)\n    echo\n    echo \"${COLOR_BLUE}Found provider packages in docker-context-files folder: ${airflow_packages[*]}${COLOR_RESET}\"\n    echo\n\n    if [[ ${USE_CONSTRAINTS_FOR_CONTEXT_PACKAGES=} == \"true\" ]]; then\n        local python_version\n        python_version=$(python -c 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")')\n        local local_constraints_file=/docker-context-files/constraints-\"${python_version}\"/${AIRFLOW_CONSTRAINTS_MODE}-\"${python_version}\".txt\n\n        if [[ -f \"${local_constraints_file}\" ]]; then\n            echo\n            echo \"${COLOR_BLUE}Installing docker-context-files packages with constraints found in ${local_constraints_file}${COLOR_RESET}\"\n            echo\n            # force reinstall all airflow + provider packages with constraints found in\n            flags=(--upgrade --constraint \"${local_constraints_file}\")\n            echo\n            echo \"${COLOR_BLUE}Copying ${local_constraints_file} to ${HOME}/constraints.txt${COLOR_RESET}\"\n            echo\n            cp \"${local_constraints_file}\" \"${HOME}/constraints.txt\"\n        else\n            echo\n            echo \"${COLOR_BLUE}Installing docker-context-files packages with constraints from GitHub${COLOR_RESET}\"\n            echo\n            flags=(--constraint \"${HOME}/constraints.txt\")\n        fi\n    else\n        echo\n        echo \"${COLOR_BLUE}Installing docker-context-files packages without constraints${COLOR_RESET}\"\n        echo\n        flags=()\n    fi\n\n    set -x\n    ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} \\\n        ${ADDITIONAL_PIP_INSTALL_FLAGS} \\\n        \"${flags[@]}\" \\\n        \"${install_airflow_package[@]}\" \"${airflow_packages[@]}\"\n    set +x\n    common::install_packaging_tools\n    pip check\n}\n\nfunction install_all_other_packages_from_docker_context_files() {\n    echo\n    echo \"${COLOR_BLUE}Force re-installing all other package from local files without dependencies${COLOR_RESET}\"\n    echo\n    local reinstalling_other_packages\n    # shellcheck disable=SC2010\n    reinstalling_other_packages=$(ls /docker-context-files/*.{whl,tar.gz} 2>/dev/null | \\\n        grep -v apache_airflow | grep -v apache-airflow || true)\n    if [[ -n \"${reinstalling_other_packages}\" ]]; then\n        set -x\n        ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} ${ADDITIONAL_PIP_INSTALL_FLAGS} \\\n            --force-reinstall --no-deps --no-index ${reinstalling_other_packages}\n        common::install_packaging_tools\n        set +x\n    fi\n}\n\ncommon::get_colors\ncommon::get_packaging_tool\ncommon::get_airflow_version_specification\ncommon::get_constraints_location\ncommon::show_packaging_tool_version_and_location\n\ninstall_airflow_and_providers_from_docker_context_files\n\ninstall_all_other_packages_from_docker_context_files\nEOF\n\n# The content below is automatically copied from scripts/docker/get_package_specs.py\nCOPY <<\"EOF\" /get_package_specs.py\n#!/usr/bin/env python\nfrom __future__ import annotations\n\nimport os\nimport sys\nfrom pathlib import Path\n\nfrom packaging.utils import (\n    InvalidSdistFilename,\n    InvalidWheelFilename,\n    parse_sdist_filename,\n    parse_wheel_filename,\n)\n\n\ndef print_package_specs(extras: str = \"\") -> None:\n    for package_path in sys.argv[1:]:\n        try:\n            package, _, _, _ = parse_wheel_filename(Path(package_path).name)\n        except InvalidWheelFilename:\n            try:\n                package, _ = parse_sdist_filename(Path(package_path).name)\n            except InvalidSdistFilename:\n                print(f\"Could not parse package name from {package_path}\", file=sys.stderr)\n                continue\n        print(f\"{package}{extras} @ file://{package_path}\")\n\n\nif __name__ == \"__main__\":\n    print_package_specs(extras=os.environ.get(\"EXTRAS\", \"\"))\nEOF\n\n\n# The content below is automatically copied from scripts/docker/install_airflow.sh\nCOPY <<\"EOF\" /install_airflow.sh\n#!/usr/bin/env bash\n\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\n\nfunction install_airflow() {\n    # Remove mysql from extras if client is not going to be installed\n    if [[ ${INSTALL_MYSQL_CLIENT} != \"true\" ]]; then\n        AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS/mysql,}\n        echo \"${COLOR_YELLOW}MYSQL client installation is disabled. Extra 'mysql' installations were therefore omitted.${COLOR_RESET}\"\n    fi\n    # Remove postgres from extras if client is not going to be installed\n    if [[ ${INSTALL_POSTGRES_CLIENT} != \"true\" ]]; then\n        AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS/postgres,}\n        echo \"${COLOR_YELLOW}Postgres client installation is disabled. Extra 'postgres' installations were therefore omitted.${COLOR_RESET}\"\n    fi\n    # Determine the installation_command_flags based on AIRFLOW_INSTALLATION_METHOD method\n    local installation_command_flags\n    if [[ ${AIRFLOW_INSTALLATION_METHOD} == \".\" ]]; then\n        # When installing from sources - we always use `--editable` mode\n        installation_command_flags=\"--editable .[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION} --editable ./task_sdk\"\n        while IFS= read -r -d '' pyproject_toml_file; do\n            project_folder=$(dirname ${pyproject_toml_file})\n            installation_command_flags=\"${installation_command_flags} --editable ${project_folder}\"\n        done < <(find \"providers\" -name \"pyproject.toml\" -print0)\n    elif [[ ${AIRFLOW_INSTALLATION_METHOD} == \"apache-airflow\" ]]; then\n        installation_command_flags=\"apache-airflow[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION}\"\n    elif [[ ${AIRFLOW_INSTALLATION_METHOD} == apache-airflow\\ @\\ * ]]; then\n        installation_command_flags=\"apache-airflow[${AIRFLOW_EXTRAS}] @ ${AIRFLOW_VERSION_SPECIFICATION/apache-airflow @//}\"\n    else\n        echo\n        echo \"${COLOR_RED}The '${INSTALLATION_METHOD}' installation method is not supported${COLOR_RESET}\"\n        echo\n        echo \"${COLOR_YELLOW}Supported methods are ('.', 'apache-airflow', 'apache-airflow @ URL')${COLOR_RESET}\"\n        echo\n        exit 1\n    fi\n    if [[ \"${UPGRADE_INVALIDATION_STRING=}\" != \"\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Remove airflow and all provider packages installed before potentially${COLOR_RESET}\"\n        echo\n        set -x\n        ${PACKAGING_TOOL_CMD} freeze | grep apache-airflow | xargs ${PACKAGING_TOOL_CMD} uninstall ${EXTRA_UNINSTALL_FLAGS} 2>/dev/null || true\n        set +x\n        echo\n        echo \"${COLOR_BLUE}Installing all packages in eager upgrade mode. Installation method: ${AIRFLOW_INSTALLATION_METHOD}${COLOR_RESET}\"\n        echo\n        set -x\n        ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} ${UPGRADE_EAGERLY} ${ADDITIONAL_PIP_INSTALL_FLAGS} ${installation_command_flags} ${EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=}\n        set +x\n        common::install_packaging_tools\n        echo\n        echo \"${COLOR_BLUE}Running 'pip check'${COLOR_RESET}\"\n        echo\n        pip check\n    else\n        echo\n        echo \"${COLOR_BLUE}Installing all packages with constraints. Installation method: ${AIRFLOW_INSTALLATION_METHOD}${COLOR_RESET}\"\n        echo\n        set -x\n        # Install all packages with constraints\n        if ! ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} ${ADDITIONAL_PIP_INSTALL_FLAGS} ${installation_command_flags} --constraint \"${HOME}/constraints.txt\"; then\n            set +x\n            echo\n            echo \"${COLOR_YELLOW}Likely pyproject.toml has new dependencies conflicting with constraints.${COLOR_RESET}\"\n            echo\n            echo \"${COLOR_BLUE}Falling back to no-constraints installation.${COLOR_RESET}\"\n            echo\n            set -x\n            ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} ${UPGRADE_IF_NEEDED} ${ADDITIONAL_PIP_INSTALL_FLAGS} ${installation_command_flags}\n        fi\n        set +x\n        common::install_packaging_tools\n        echo\n        echo \"${COLOR_BLUE}Running 'pip check'${COLOR_RESET}\"\n        echo\n        pip check\n    fi\n\n}\n\ncommon::get_colors\ncommon::get_packaging_tool\ncommon::get_airflow_version_specification\ncommon::get_constraints_location\ncommon::show_packaging_tool_version_and_location\n\ninstall_airflow\nEOF\n\n# The content below is automatically copied from scripts/docker/install_additional_dependencies.sh\nCOPY <<\"EOF\" /install_additional_dependencies.sh\n#!/usr/bin/env bash\nset -euo pipefail\n\n: \"${ADDITIONAL_PYTHON_DEPS:?Should be set}\"\n\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\n\nfunction install_additional_dependencies() {\n    if [[ \"${UPGRADE_INVALIDATION_STRING=}\" != \"\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Installing additional dependencies while upgrading to newer dependencies${COLOR_RESET}\"\n        echo\n        set -x\n        ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} ${UPGRADE_EAGERLY} \\\n            ${ADDITIONAL_PIP_INSTALL_FLAGS} \\\n            ${ADDITIONAL_PYTHON_DEPS} ${EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=}\n        set +x\n        common::install_packaging_tools\n        echo\n        echo \"${COLOR_BLUE}Running 'pip check'${COLOR_RESET}\"\n        echo\n        pip check\n    else\n        echo\n        echo \"${COLOR_BLUE}Installing additional dependencies upgrading only if needed${COLOR_RESET}\"\n        echo\n        set -x\n        ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} ${UPGRADE_IF_NEEDED} \\\n            ${ADDITIONAL_PIP_INSTALL_FLAGS} \\\n            ${ADDITIONAL_PYTHON_DEPS}\n        set +x\n        common::install_packaging_tools\n        echo\n        echo \"${COLOR_BLUE}Running 'pip check'${COLOR_RESET}\"\n        echo\n        pip check\n    fi\n}\n\ncommon::get_colors\ncommon::get_packaging_tool\ncommon::get_airflow_version_specification\ncommon::get_constraints_location\ncommon::show_packaging_tool_version_and_location\n\ninstall_additional_dependencies\nEOF\n\n# The content below is automatically copied from scripts/docker/create_prod_venv.sh\nCOPY <<\"EOF\" /create_prod_venv.sh\n#!/usr/bin/env bash\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\n\nfunction create_prod_venv() {\n    echo\n    echo \"${COLOR_BLUE}Removing ${HOME}/.local and re-creating it as virtual environment.${COLOR_RESET}\"\n    rm -rf ~/.local\n    python -m venv ~/.local\n    echo \"${COLOR_BLUE}The ${HOME}/.local virtualenv created.${COLOR_RESET}\"\n}\n\ncommon::get_colors\ncommon::get_packaging_tool\ncommon::show_packaging_tool_version_and_location\ncreate_prod_venv\ncommon::install_packaging_tools\nEOF\n\n\n# The content below is automatically copied from scripts/docker/entrypoint_prod.sh\nCOPY <<\"EOF\" /entrypoint_prod.sh\n#!/usr/bin/env bash\nAIRFLOW_COMMAND=\"${1:-}\"\n\nset -euo pipefail\n\nLD_PRELOAD=\"/usr/lib/$(uname -m)-linux-gnu/libstdc++.so.6\"\nexport LD_PRELOAD\n\nfunction run_check_with_retries {\n    local cmd\n    cmd=\"${1}\"\n    local countdown\n    countdown=\"${CONNECTION_CHECK_MAX_COUNT}\"\n\n    while true\n    do\n        set +e\n        local last_check_result\n        local res\n        last_check_result=$(eval \"${cmd} 2>&1\")\n        res=$?\n        set -e\n        if [[ ${res} == 0 ]]; then\n            echo\n            break\n        else\n            echo -n \".\"\n            countdown=$((countdown-1))\n        fi\n        if [[ ${countdown} == 0 ]]; then\n            echo\n            echo \"ERROR! Maximum number of retries (${CONNECTION_CHECK_MAX_COUNT}) reached.\"\n            echo\n            echo \"Last check result:\"\n            echo \"$ ${cmd}\"\n            echo \"${last_check_result}\"\n            echo\n            exit 1\n        else\n            sleep \"${CONNECTION_CHECK_SLEEP_TIME}\"\n        fi\n    done\n}\n\nfunction run_nc() {\n    # Checks if it is possible to connect to the host using netcat.\n    #\n    # We want to avoid misleading messages and perform only forward lookup of the service IP address.\n    # Netcat when run without -n performs both forward and reverse lookup and fails if the reverse\n    # lookup name does not match the original name even if the host is reachable via IP. This happens\n    # randomly with docker-compose in GitHub Actions.\n    # Since we are not using reverse lookup elsewhere, we can perform forward lookup in python\n    # And use the IP in NC and add '-n' switch to disable any DNS use.\n    # Even if this message might be harmless, it might hide the real reason for the problem\n    # Which is the long time needed to start some services, seeing this message might be totally misleading\n    # when you try to analyse the problem, that's why it's best to avoid it,\n    local host=\"${1}\"\n    local port=\"${2}\"\n    local ip\n    ip=$(python -c \"import socket; print(socket.gethostbyname('${host}'))\")\n    nc -zvvn \"${ip}\" \"${port}\"\n}\n\n\nfunction wait_for_connection {\n    # Waits for Connection to the backend specified via URL passed as first parameter\n    # Detects backend type depending on the URL schema and assigns\n    # default port numbers if not specified in the URL.\n    # Then it loops until connection to the host/port specified can be established\n    # It tries `CONNECTION_CHECK_MAX_COUNT` times and sleeps `CONNECTION_CHECK_SLEEP_TIME` between checks\n    local connection_url\n    connection_url=\"${1}\"\n    local detected_backend\n    detected_backend=$(python -c \"from urllib.parse import urlsplit; import sys; print(urlsplit(sys.argv[1]).scheme)\" \"${connection_url}\")\n    local detected_host\n    detected_host=$(python -c \"from urllib.parse import urlsplit; import sys; print(urlsplit(sys.argv[1]).hostname or '')\" \"${connection_url}\")\n    local detected_port\n    detected_port=$(python -c \"from urllib.parse import urlsplit; import sys; print(urlsplit(sys.argv[1]).port or '')\" \"${connection_url}\")\n\n    echo BACKEND=\"${BACKEND:=${detected_backend}}\"\n    readonly BACKEND\n\n    if [[ -z \"${detected_port=}\" ]]; then\n        if [[ ${BACKEND} == \"postgres\"* ]]; then\n            detected_port=5432\n        elif [[ ${BACKEND} == \"mysql\"* ]]; then\n            detected_port=3306\n        elif [[ ${BACKEND} == \"mssql\"* ]]; then\n            detected_port=1433\n        elif [[ ${BACKEND} == \"redis\"* ]]; then\n            detected_port=6379\n        elif [[ ${BACKEND} == \"amqp\"* ]]; then\n            detected_port=5672\n        fi\n    fi\n\n    detected_host=${detected_host:=\"localhost\"}\n\n    # Allow the DB parameters to be overridden by environment variable\n    echo DB_HOST=\"${DB_HOST:=${detected_host}}\"\n    readonly DB_HOST\n\n    echo DB_PORT=\"${DB_PORT:=${detected_port}}\"\n    readonly DB_PORT\n    if [[ -n \"${DB_HOST=}\" ]] && [[ -n \"${DB_PORT=}\" ]]; then\n        run_check_with_retries \"run_nc ${DB_HOST@Q} ${DB_PORT@Q}\"\n    else\n        >&2 echo \"The connection details to the broker could not be determined. Connectivity checks were skipped.\"\n    fi\n}\n\nfunction create_www_user() {\n    local local_password=\"\"\n    # Warning: command environment variables (*_CMD) have priority over usual configuration variables\n    # for configuration parameters that require sensitive information. This is the case for the SQL database\n    # and the broker backend in this entrypoint script.\n    if [[ -n \"${_AIRFLOW_WWW_USER_PASSWORD_CMD=}\" ]]; then\n        local_password=$(eval \"${_AIRFLOW_WWW_USER_PASSWORD_CMD}\")\n        unset _AIRFLOW_WWW_USER_PASSWORD_CMD\n    elif [[ -n \"${_AIRFLOW_WWW_USER_PASSWORD=}\" ]]; then\n        local_password=\"${_AIRFLOW_WWW_USER_PASSWORD}\"\n        unset _AIRFLOW_WWW_USER_PASSWORD\n    fi\n    if [[ -z ${local_password} ]]; then\n        echo\n        echo \"ERROR! Airflow Admin password not set via _AIRFLOW_WWW_USER_PASSWORD or _AIRFLOW_WWW_USER_PASSWORD_CMD variables!\"\n        echo\n        exit 1\n    fi\n\n    airflow users create \\\n       --username \"${_AIRFLOW_WWW_USER_USERNAME=\"admin\"}\" \\\n       --firstname \"${_AIRFLOW_WWW_USER_FIRSTNAME=\"Airflow\"}\" \\\n       --lastname \"${_AIRFLOW_WWW_USER_LASTNAME=\"Admin\"}\" \\\n       --email \"${_AIRFLOW_WWW_USER_EMAIL=\"airflowadmin@example.com\"}\" \\\n       --role \"${_AIRFLOW_WWW_USER_ROLE=\"Admin\"}\" \\\n       --password \"${local_password}\" || true\n}\n\nfunction create_system_user_if_missing() {\n    # This is needed in case of OpenShift-compatible container execution. In case of OpenShift random\n    # User id is used when starting the image, however group 0 is kept as the user group. Our production\n    # Image is OpenShift compatible, so all permissions on all folders are set so that 0 group can exercise\n    # the same privileges as the default \"airflow\" user, this code checks if the user is already\n    # present in /etc/passwd and will create the system user dynamically, including setting its\n    # HOME directory to the /home/airflow so that (for example) the ${HOME}/.local folder where airflow is\n    # Installed can be automatically added to PYTHONPATH\n    if ! whoami &> /dev/null; then\n      if [[ -w /etc/passwd ]]; then\n        echo \"${USER_NAME:-default}:x:$(id -u):0:${USER_NAME:-default} user:${AIRFLOW_USER_HOME_DIR}:/sbin/nologin\" \\\n            >> /etc/passwd\n      fi\n      export HOME=\"${AIRFLOW_USER_HOME_DIR}\"\n    fi\n}\n\nfunction set_pythonpath_for_root_user() {\n    # Airflow is installed as a local user application which means that if the container is running as root\n    # the application is not available. because Python then only load system-wide applications.\n    # Now also adds applications installed as local user \"airflow\".\n    if [[ $UID == \"0\" ]]; then\n        local python_major_minor\n        python_major_minor=$(python -c 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")')\n        export PYTHONPATH=\"${AIRFLOW_USER_HOME_DIR}/.local/lib/python${python_major_minor}/site-packages:${PYTHONPATH:-}\"\n        >&2 echo \"The container is run as root user. For security, consider using a regular user account.\"\n    fi\n}\n\nfunction wait_for_airflow_db() {\n    # Wait for the command to run successfully to validate the database connection.\n    run_check_with_retries \"airflow db check\"\n}\n\nfunction migrate_db() {\n    # Runs airflow db migrate\n    airflow db migrate || true\n}\n\nfunction wait_for_celery_broker() {\n    # Verifies connection to Celery Broker\n    local executor\n    executor=\"$(airflow config get-value core executor)\"\n    if [[ \"${executor}\" == \"CeleryExecutor\" ]]; then\n        local connection_url\n        connection_url=\"$(airflow config get-value celery broker_url)\"\n        wait_for_connection \"${connection_url}\"\n    fi\n}\n\nfunction exec_to_bash_or_python_command_if_specified() {\n    # If one of the commands: 'bash', 'python' is used, either run appropriate\n    # command with exec\n    if [[ ${AIRFLOW_COMMAND} == \"bash\" ]]; then\n       shift\n       exec \"/bin/bash\" \"${@}\"\n    elif [[ ${AIRFLOW_COMMAND} == \"python\" ]]; then\n       shift\n       exec \"python\" \"${@}\"\n    fi\n}\n\nfunction check_uid_gid() {\n    if [[ $(id -g) == \"0\" ]]; then\n        return\n    fi\n    if [[ $(id -u) == \"50000\" ]]; then\n        >&2 echo\n        >&2 echo \"WARNING! You should run the image with GID (Group ID) set to 0\"\n        >&2 echo \"         even if you use 'airflow' user (UID=50000)\"\n        >&2 echo\n        >&2 echo \" You started the image with UID=$(id -u) and GID=$(id -g)\"\n        >&2 echo\n        >&2 echo \" This is to make sure you can run the image with an arbitrary UID in the future.\"\n        >&2 echo\n        >&2 echo \" See more about it in the Airflow's docker image documentation\"\n        >&2 echo \"     http://airflow.apache.org/docs/docker-stack/entrypoint\"\n        >&2 echo\n        # We still allow the image to run with `airflow` user.\n        return\n    else\n        >&2 echo\n        >&2 echo \"ERROR! You should run the image with GID=0\"\n        >&2 echo\n        >&2 echo \" You started the image with UID=$(id -u) and GID=$(id -g)\"\n        >&2 echo\n        >&2 echo \"The image should always be run with GID (Group ID) set to 0 regardless of the UID used.\"\n        >&2 echo \" This is to make sure you can run the image with an arbitrary UID.\"\n        >&2 echo\n        >&2 echo \" See more about it in the Airflow's docker image documentation\"\n        >&2 echo \"     http://airflow.apache.org/docs/docker-stack/entrypoint\"\n        # This will not work so we fail hard\n        exit 1\n    fi\n}\n\nunset PIP_USER\n\ncheck_uid_gid\n\numask 0002\n\nCONNECTION_CHECK_MAX_COUNT=${CONNECTION_CHECK_MAX_COUNT:=20}\nreadonly CONNECTION_CHECK_MAX_COUNT\n\nCONNECTION_CHECK_SLEEP_TIME=${CONNECTION_CHECK_SLEEP_TIME:=3}\nreadonly CONNECTION_CHECK_SLEEP_TIME\n\ncreate_system_user_if_missing\nset_pythonpath_for_root_user\nif [[ \"${CONNECTION_CHECK_MAX_COUNT}\" -gt \"0\" ]]; then\n    wait_for_airflow_db\nfi\n\nif [[ -n \"${_AIRFLOW_DB_UPGRADE=}\" ]] || [[ -n \"${_AIRFLOW_DB_MIGRATE=}\" ]] ; then\n    migrate_db\nfi\n\nif [[ -n \"${_AIRFLOW_DB_UPGRADE=}\" ]] ; then\n    >&2 echo \"WARNING: Environment variable '_AIRFLOW_DB_UPGRADE' is deprecated please use '_AIRFLOW_DB_MIGRATE' instead\"\nfi\n\nif [[ -n \"${_AIRFLOW_WWW_USER_CREATE=}\" ]] ; then\n    create_www_user\nfi\n\nif [[ -n \"${_PIP_ADDITIONAL_REQUIREMENTS=}\" ]] ; then\n    >&2 echo\n    >&2 echo \"!!!!!  Installing additional requirements: '${_PIP_ADDITIONAL_REQUIREMENTS}' !!!!!!!!!!!!\"\n    >&2 echo\n    >&2 echo \"WARNING: This is a development/test feature only. NEVER use it in production!\"\n    >&2 echo \"         Instead, build a custom image as described in\"\n    >&2 echo\n    >&2 echo \"         https://airflow.apache.org/docs/docker-stack/build.html\"\n    >&2 echo\n    >&2 echo \"         Adding requirements at container startup is fragile and is done every time\"\n    >&2 echo \"         the container starts, so it is only useful for testing and trying out\"\n    >&2 echo \"         of adding dependencies.\"\n    >&2 echo\n    pip install --root-user-action ignore ${_PIP_ADDITIONAL_REQUIREMENTS}\nfi\n\n\nexec_to_bash_or_python_command_if_specified \"${@}\"\n\nif [[ ${AIRFLOW_COMMAND} == \"airflow\" ]]; then\n   AIRFLOW_COMMAND=\"${2:-}\"\n   shift\nfi\n\nif [[ ${AIRFLOW_COMMAND} =~ ^(scheduler|celery)$ ]] \\\n    && [[ \"${CONNECTION_CHECK_MAX_COUNT}\" -gt \"0\" ]]; then\n    wait_for_celery_broker\nfi\n\nexec \"airflow\" \"${@}\"\nEOF\n\n# The content below is automatically copied from scripts/docker/clean-logs.sh\nCOPY <<\"EOF\" /clean-logs.sh\n#!/usr/bin/env bash\n\n\nset -euo pipefail\n\nreadonly DIRECTORY=\"${AIRFLOW_HOME:-/usr/local/airflow}\"\nreadonly RETENTION=\"${AIRFLOW__LOG_RETENTION_DAYS:-15}\"\n\ntrap \"exit\" INT TERM\n\nreadonly EVERY=$((15*60))\n\necho \"Cleaning logs every $EVERY seconds\"\n\nwhile true; do\n  echo \"Trimming airflow logs to ${RETENTION} days.\"\n  find \"${DIRECTORY}\"/logs \\\n    -type d -name 'lost+found' -prune -o \\\n    -type f -mtime +\"${RETENTION}\" -name '*.log' -print0 | \\\n    xargs -0 rm -f || true\n\n  find \"${DIRECTORY}\"/logs -type d -empty -delete || true\n\n  seconds=$(( $(date -u +%s) % EVERY))\n  (( seconds < 1 )) || sleep $((EVERY - seconds - 1))\n  sleep 1\ndone\nEOF\n\n# The content below is automatically copied from scripts/docker/airflow-scheduler-autorestart.sh\nCOPY <<\"EOF\" /airflow-scheduler-autorestart.sh\n#!/usr/bin/env bash\n\nwhile echo \"Running\"; do\n    airflow scheduler -n 5\n    return_code=$?\n    if (( return_code != 0 )); then\n        echo \"Scheduler crashed with exit code $return_code. Respawning..\" >&2\n        date >> /tmp/airflow_scheduler_errors.txt\n    fi\n\n    sleep 1\ndone\nEOF\n\n##############################################################################################\n# This is the build image where we build all dependencies\n##############################################################################################\nFROM ${PYTHON_BASE_IMAGE} as airflow-build-image\n\n# Nolog bash flag is currently ignored - but you can replace it with\n# xtrace - to show commands executed)\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"nounset\", \"-o\", \"nolog\", \"-c\"]\n\nARG PYTHON_BASE_IMAGE\nENV PYTHON_BASE_IMAGE=${PYTHON_BASE_IMAGE} \\\n    DEBIAN_FRONTEND=noninteractive LANGUAGE=C.UTF-8 LANG=C.UTF-8 LC_ALL=C.UTF-8 \\\n    LC_CTYPE=C.UTF-8 LC_MESSAGES=C.UTF-8 \\\n    PIP_CACHE_DIR=/tmp/.cache/pip \\\n    UV_CACHE_DIR=/tmp/.cache/uv\n\nARG DEV_APT_DEPS=\"\"\nARG ADDITIONAL_DEV_APT_DEPS=\"\"\nARG DEV_APT_COMMAND=\"\"\nARG ADDITIONAL_DEV_APT_COMMAND=\"\"\nARG ADDITIONAL_DEV_APT_ENV=\"\"\n\nENV DEV_APT_DEPS=${DEV_APT_DEPS} \\\n    ADDITIONAL_DEV_APT_DEPS=${ADDITIONAL_DEV_APT_DEPS} \\\n    DEV_APT_COMMAND=${DEV_APT_COMMAND} \\\n    ADDITIONAL_DEV_APT_COMMAND=${ADDITIONAL_DEV_APT_COMMAND} \\\n    ADDITIONAL_DEV_APT_ENV=${ADDITIONAL_DEV_APT_ENV}\n\nCOPY --from=scripts install_os_dependencies.sh /scripts/docker/\nRUN bash /scripts/docker/install_os_dependencies.sh dev\n\nARG INSTALL_MYSQL_CLIENT=\"true\"\nARG INSTALL_MYSQL_CLIENT_TYPE=\"mariadb\"\nARG INSTALL_MSSQL_CLIENT=\"true\"\nARG INSTALL_POSTGRES_CLIENT=\"true\"\n\nENV INSTALL_MYSQL_CLIENT=${INSTALL_MYSQL_CLIENT} \\\n    INSTALL_MYSQL_CLIENT_TYPE=${INSTALL_MYSQL_CLIENT_TYPE} \\\n    INSTALL_MSSQL_CLIENT=${INSTALL_MSSQL_CLIENT} \\\n    INSTALL_POSTGRES_CLIENT=${INSTALL_POSTGRES_CLIENT}\n\nCOPY --from=scripts common.sh /scripts/docker/\n\n# Only copy mysql/mssql installation scripts for now - so that changing the other\n# scripts which are needed much later will not invalidate the docker layer here\nCOPY --from=scripts install_mysql.sh install_mssql.sh install_postgres.sh /scripts/docker/\n\nRUN bash /scripts/docker/install_mysql.sh dev && \\\n    bash /scripts/docker/install_mssql.sh dev && \\\n    bash /scripts/docker/install_postgres.sh dev\nENV PATH=${PATH}:/opt/mssql-tools/bin\n\n# By default we do not install from docker context files but if we decide to install from docker context\n# files, we should override those variables to \"docker-context-files\"\nARG DOCKER_CONTEXT_FILES=\"Dockerfile\"\nARG AIRFLOW_HOME\nARG AIRFLOW_USER_HOME_DIR\nARG AIRFLOW_UID\n\nRUN adduser --gecos \"First Last,RoomNumber,WorkPhone,HomePhone\" --disabled-password \\\n       --quiet \"airflow\" --uid \"${AIRFLOW_UID}\" --gid \"0\" --home \"${AIRFLOW_USER_HOME_DIR}\" && \\\n    mkdir -p ${AIRFLOW_HOME} && chown -R \"airflow:0\" \"${AIRFLOW_USER_HOME_DIR}\" ${AIRFLOW_HOME}\n\nCOPY --chown=${AIRFLOW_UID}:0 ${DOCKER_CONTEXT_FILES} /docker-context-files\n\nUSER airflow\n\nARG AIRFLOW_REPO=apache/airflow\nARG AIRFLOW_BRANCH=main\nARG AIRFLOW_EXTRAS\nARG ADDITIONAL_AIRFLOW_EXTRAS=\"\"\n# Allows to override constraints source\nARG CONSTRAINTS_GITHUB_REPOSITORY=\"apache/airflow\"\nARG AIRFLOW_CONSTRAINTS_MODE=\"constraints\"\nARG AIRFLOW_CONSTRAINTS_REFERENCE=\"\"\nARG AIRFLOW_CONSTRAINTS_LOCATION=\"\"\nARG DEFAULT_CONSTRAINTS_BRANCH=\"constraints-main\"\n\n# By default PIP has progress bar but you can disable it.\nARG PIP_PROGRESS_BAR\n# This is airflow version that is put in the label of the image build\nARG AIRFLOW_VERSION\n# By default latest released version of airflow is installed (when empty) but this value can be overridden\n# and we can install version according to specification (For example ==2.0.2 or <3.0.0).\nARG AIRFLOW_VERSION_SPECIFICATION\n# Determines the way airflow is installed. By default we install airflow from PyPI `apache-airflow` package\n# But it also can be `.` from local installation or GitHub URL pointing to specific branch or tag\n# Of Airflow. Note That for local source installation you need to have local sources of\n# Airflow checked out together with the Dockerfile and AIRFLOW_SOURCES_FROM and AIRFLOW_SOURCES_TO\n# set to \".\" and \"/opt/airflow\" respectively.\nARG AIRFLOW_INSTALLATION_METHOD=\"apache-airflow\"\n# By default we do not upgrade to latest dependencies\nARG UPGRADE_INVALIDATION_STRING=\"\"\nARG AIRFLOW_SOURCES_FROM\nARG AIRFLOW_SOURCES_TO\n\n\nRUN if [[ -f /docker-context-files/pip.conf ]]; then \\\n        mkdir -p ${AIRFLOW_USER_HOME_DIR}/.config/pip; \\\n        cp /docker-context-files/pip.conf \"${AIRFLOW_USER_HOME_DIR}/.config/pip/pip.conf\"; \\\n    fi; \\\n    if [[ -f /docker-context-files/.piprc ]]; then \\\n        cp /docker-context-files/.piprc \"${AIRFLOW_USER_HOME_DIR}/.piprc\"; \\\n    fi\n\n# Additional PIP flags passed to all pip install commands except reinstalling pip itself\nARG ADDITIONAL_PIP_INSTALL_FLAGS=\"\"\n\nARG AIRFLOW_PIP_VERSION\nARG AIRFLOW_UV_VERSION\nARG AIRFLOW_USE_UV\nARG UV_HTTP_TIMEOUT\n\nENV AIRFLOW_PIP_VERSION=${AIRFLOW_PIP_VERSION} \\\n    AIRFLOW_UV_VERSION=${AIRFLOW_UV_VERSION} \\\n    UV_HTTP_TIMEOUT=${UV_HTTP_TIMEOUT} \\\n    AIRFLOW_USE_UV=${AIRFLOW_USE_UV} \\\n    AIRFLOW_VERSION=${AIRFLOW_VERSION} \\\n    AIRFLOW_INSTALLATION_METHOD=${AIRFLOW_INSTALLATION_METHOD} \\\n    AIRFLOW_VERSION_SPECIFICATION=${AIRFLOW_VERSION_SPECIFICATION} \\\n    AIRFLOW_SOURCES_FROM=${AIRFLOW_SOURCES_FROM} \\\n    AIRFLOW_SOURCES_TO=${AIRFLOW_SOURCES_TO} \\\n    AIRFLOW_REPO=${AIRFLOW_REPO} \\\n    AIRFLOW_BRANCH=${AIRFLOW_BRANCH} \\\n    AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS}${ADDITIONAL_AIRFLOW_EXTRAS:+,}${ADDITIONAL_AIRFLOW_EXTRAS} \\\n    CONSTRAINTS_GITHUB_REPOSITORY=${CONSTRAINTS_GITHUB_REPOSITORY} \\\n    AIRFLOW_CONSTRAINTS_MODE=${AIRFLOW_CONSTRAINTS_MODE} \\\n    AIRFLOW_CONSTRAINTS_REFERENCE=${AIRFLOW_CONSTRAINTS_REFERENCE} \\\n    AIRFLOW_CONSTRAINTS_LOCATION=${AIRFLOW_CONSTRAINTS_LOCATION} \\\n    DEFAULT_CONSTRAINTS_BRANCH=${DEFAULT_CONSTRAINTS_BRANCH} \\\n    PATH=${AIRFLOW_USER_HOME_DIR}/.local/bin:${PATH} \\\n    PIP_PROGRESS_BAR=${PIP_PROGRESS_BAR} \\\n    ADDITIONAL_PIP_INSTALL_FLAGS=${ADDITIONAL_PIP_INSTALL_FLAGS} \\\n    AIRFLOW_USER_HOME_DIR=${AIRFLOW_USER_HOME_DIR} \\\n    AIRFLOW_HOME=${AIRFLOW_HOME} \\\n    AIRFLOW_UID=${AIRFLOW_UID} \\\n    UPGRADE_INVALIDATION_STRING=${UPGRADE_INVALIDATION_STRING}\n\n\n# Copy all scripts required for installation - changing any of those should lead to\n# rebuilding from here\nCOPY --from=scripts common.sh install_packaging_tools.sh create_prod_venv.sh /scripts/docker/\n\n# We can set this value to true in case we want to install .whl/.tar.gz packages placed in the\n# docker-context-files folder. This can be done for both additional packages you want to install\n# as well as Airflow and Provider packages (it will be automatically detected if airflow\n# is installed from docker-context files rather than from PyPI)\nARG INSTALL_PACKAGES_FROM_CONTEXT=\"false\"\n\n# Normally constraints are not used when context packages are build - because we might have packages\n# that are conflicting with Airflow constraints, however there are cases when we want to use constraints\n# for example in CI builds when we already have source-package constraints - either from github branch or\n# from eager-upgraded constraints by the CI builds\nARG USE_CONSTRAINTS_FOR_CONTEXT_PACKAGES=\"false\"\n\n# By changing the epoch we can force reinstalling Airflow and pip all dependencies\n# It can also be overwritten manually by setting the AIRFLOW_CI_BUILD_EPOCH environment variable.\nARG AIRFLOW_CI_BUILD_EPOCH=\"11\"\nENV AIRFLOW_CI_BUILD_EPOCH=${AIRFLOW_CI_BUILD_EPOCH}\n\n\n# In case of Production build image segment we want to pre-install main version of airflow\n# dependencies from GitHub so that we do not have to always reinstall it from the scratch.\n# The Airflow and providers are uninstalled, only dependencies remain\n# the cache is only used when \"upgrade to newer dependencies\" is not set to automatically\n# account for removed dependencies (we do not install them in the first place) and in case\n# INSTALL_PACKAGES_FROM_CONTEXT is not set (because then caching it from main makes no sense).\n\n# By default PIP installs everything to ~/.local and it's also treated as VIRTUALENV\nENV VIRTUAL_ENV=\"${AIRFLOW_USER_HOME_DIR}/.local\"\n\nRUN bash /scripts/docker/install_packaging_tools.sh; bash /scripts/docker/create_prod_venv.sh\n\nCOPY --chown=airflow:0 ${AIRFLOW_SOURCES_FROM} ${AIRFLOW_SOURCES_TO}\n\n# Add extra python dependencies\nARG ADDITIONAL_PYTHON_DEPS=\"\"\n\n\nARG VERSION_SUFFIX_FOR_PYPI=\"\"\n\nENV ADDITIONAL_PYTHON_DEPS=${ADDITIONAL_PYTHON_DEPS} \\\n    INSTALL_PACKAGES_FROM_CONTEXT=${INSTALL_PACKAGES_FROM_CONTEXT} \\\n    USE_CONSTRAINTS_FOR_CONTEXT_PACKAGES=${USE_CONSTRAINTS_FOR_CONTEXT_PACKAGES} \\\n    VERSION_SUFFIX_FOR_PYPI=${VERSION_SUFFIX_FOR_PYPI}\n\nWORKDIR ${AIRFLOW_HOME}\n\nCOPY --from=scripts install_from_docker_context_files.sh install_airflow.sh \\\n     install_additional_dependencies.sh create_prod_venv.sh get_package_specs.py /scripts/docker/\n\n# Useful for creating a cache id based on the underlying architecture, preventing the use of cached python packages from\n# an incorrect architecture.\nARG TARGETARCH\n# Value to be able to easily change cache id and therefore use a bare new cache\nARG DEPENDENCY_CACHE_EPOCH=\"9\"\n\n# hadolint ignore=SC2086, SC2010, DL3042\nRUN --mount=type=cache,id=prod-$TARGETARCH-$DEPENDENCY_CACHE_EPOCH,target=/tmp/.cache/,uid=${AIRFLOW_UID} \\\n    if [[ ${INSTALL_PACKAGES_FROM_CONTEXT} == \"true\" ]]; then \\\n        bash /scripts/docker/install_from_docker_context_files.sh; \\\n    fi; \\\n    if ! airflow version 2>/dev/null >/dev/null; then \\\n        bash /scripts/docker/install_airflow.sh; \\\n    fi; \\\n    if [[ -n \"${ADDITIONAL_PYTHON_DEPS}\" ]]; then \\\n        bash /scripts/docker/install_additional_dependencies.sh; \\\n    fi; \\\n    find \"${AIRFLOW_USER_HOME_DIR}/.local/\" -name '*.pyc' -print0 | xargs -0 rm -f || true ; \\\n    find \"${AIRFLOW_USER_HOME_DIR}/.local/\" -type d -name '__pycache__' -print0 | xargs -0 rm -rf || true ; \\\n    # make sure that all directories and files in .local are also group accessible\n    find \"${AIRFLOW_USER_HOME_DIR}/.local\" -executable ! -type l -print0 | xargs --null chmod g+x; \\\n    find \"${AIRFLOW_USER_HOME_DIR}/.local\" ! -type l -print0 | xargs --null chmod g+rw\n\n# In case there is a requirements.txt file in \"docker-context-files\" it will be installed\n# during the build additionally to whatever has been installed so far. It is recommended that\n# the requirements.txt contains only dependencies with == version specification\n# hadolint ignore=DL3042\nRUN --mount=type=cache,id=prod-$TARGETARCH-$DEPENDENCY_CACHE_EPOCH,target=/tmp/.cache/,uid=${AIRFLOW_UID} \\\n    if [[ -f /docker-context-files/requirements.txt ]]; then \\\n        pip install -r /docker-context-files/requirements.txt; \\\n    fi\n\n##############################################################################################\n# This is the actual Airflow image - much smaller than the build one. We copy\n# installed Airflow and all its dependencies from the build image to make it smaller.\n##############################################################################################\nFROM ${PYTHON_BASE_IMAGE} as main\n\n# Nolog bash flag is currently ignored - but you can replace it with other flags (for example\n# xtrace - to show commands executed)\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"nounset\", \"-o\", \"nolog\", \"-c\"]\n\nARG AIRFLOW_UID\n\nLABEL org.apache.airflow.distro=\"debian\" \\\n  org.apache.airflow.module=\"airflow\" \\\n  org.apache.airflow.component=\"airflow\" \\\n  org.apache.airflow.image=\"airflow\" \\\n  org.apache.airflow.uid=\"${AIRFLOW_UID}\"\n\nARG PYTHON_BASE_IMAGE\n\nENV PYTHON_BASE_IMAGE=${PYTHON_BASE_IMAGE} \\\n    # Make sure noninteractive debian install is used and language variables set\n    DEBIAN_FRONTEND=noninteractive LANGUAGE=C.UTF-8 LANG=C.UTF-8 LC_ALL=C.UTF-8 \\\n    LC_CTYPE=C.UTF-8 LC_MESSAGES=C.UTF-8 LD_LIBRARY_PATH=/usr/local/lib \\\n    PIP_CACHE_DIR=/tmp/.cache/pip \\\n    UV_CACHE_DIR=/tmp/.cache/uv\n\nARG RUNTIME_APT_DEPS=\"\"\nARG ADDITIONAL_RUNTIME_APT_DEPS=\"\"\nARG RUNTIME_APT_COMMAND=\"echo\"\nARG ADDITIONAL_RUNTIME_APT_COMMAND=\"\"\nARG ADDITIONAL_RUNTIME_APT_ENV=\"\"\nARG INSTALL_MYSQL_CLIENT=\"true\"\nARG INSTALL_MYSQL_CLIENT_TYPE=\"mariadb\"\nARG INSTALL_MSSQL_CLIENT=\"true\"\nARG INSTALL_POSTGRES_CLIENT=\"true\"\n\nENV RUNTIME_APT_DEPS=${RUNTIME_APT_DEPS} \\\n    ADDITIONAL_RUNTIME_APT_DEPS=${ADDITIONAL_RUNTIME_APT_DEPS} \\\n    RUNTIME_APT_COMMAND=${RUNTIME_APT_COMMAND} \\\n    ADDITIONAL_RUNTIME_APT_COMMAND=${ADDITIONAL_RUNTIME_APT_COMMAND} \\\n    INSTALL_MYSQL_CLIENT=${INSTALL_MYSQL_CLIENT} \\\n    INSTALL_MYSQL_CLIENT_TYPE=${INSTALL_MYSQL_CLIENT_TYPE} \\\n    INSTALL_MSSQL_CLIENT=${INSTALL_MSSQL_CLIENT} \\\n    INSTALL_POSTGRES_CLIENT=${INSTALL_POSTGRES_CLIENT} \\\n    GUNICORN_CMD_ARGS=\"--worker-tmp-dir /dev/shm\" \\\n    AIRFLOW_INSTALLATION_METHOD=${AIRFLOW_INSTALLATION_METHOD}\n\nCOPY --from=scripts install_os_dependencies.sh /scripts/docker/\nRUN bash /scripts/docker/install_os_dependencies.sh runtime\n\n# Having the variable in final image allows to disable providers manager warnings when\n# production image is prepared from sources rather than from package\nARG AIRFLOW_INSTALLATION_METHOD=\"apache-airflow\"\nARG AIRFLOW_IMAGE_REPOSITORY\nARG AIRFLOW_IMAGE_README_URL\nARG AIRFLOW_USER_HOME_DIR\nARG AIRFLOW_HOME\n\n# By default PIP installs everything to ~/.local\nENV PATH=\"${AIRFLOW_USER_HOME_DIR}/.local/bin:${PATH}\" \\\n    VIRTUAL_ENV=\"${AIRFLOW_USER_HOME_DIR}/.local\" \\\n    AIRFLOW_UID=${AIRFLOW_UID} \\\n    AIRFLOW_USER_HOME_DIR=${AIRFLOW_USER_HOME_DIR} \\\n    AIRFLOW_HOME=${AIRFLOW_HOME}\n\nCOPY --from=scripts common.sh /scripts/docker/\n\n# Only copy mysql/mssql installation scripts for now - so that changing the other\n# scripts which are needed much later will not invalidate the docker layer here.\nCOPY --from=scripts install_mysql.sh install_mssql.sh install_postgres.sh /scripts/docker/\n# We run scripts with bash here to make sure we can execute the scripts. Changing to +x might have an\n# unexpected result - the cache for Dockerfiles might get invalidated in case the host system\n# had different umask set and group x bit was not set. In Azure the bit might be not set at all.\n# That also protects against AUFS Docker backend problem where changing the executable bit required sync\nRUN bash /scripts/docker/install_mysql.sh prod \\\n    && bash /scripts/docker/install_mssql.sh prod \\\n    && bash /scripts/docker/install_postgres.sh prod \\\n    && adduser --gecos \"First Last,RoomNumber,WorkPhone,HomePhone\" --disabled-password \\\n           --quiet \"airflow\" --uid \"${AIRFLOW_UID}\" --gid \"0\" --home \"${AIRFLOW_USER_HOME_DIR}\" \\\n# Make Airflow files belong to the root group and are accessible. This is to accommodate the guidelines from\n# OpenShift https://docs.openshift.com/enterprise/3.0/creating_images/guidelines.html\n    && mkdir -pv \"${AIRFLOW_HOME}\" \\\n    && mkdir -pv \"${AIRFLOW_HOME}/dags\" \\\n    && mkdir -pv \"${AIRFLOW_HOME}/logs\" \\\n    && chown -R airflow:0 \"${AIRFLOW_USER_HOME_DIR}\" \"${AIRFLOW_HOME}\" \\\n    && chmod -R g+rw \"${AIRFLOW_USER_HOME_DIR}\" \"${AIRFLOW_HOME}\" \\\n    && find \"${AIRFLOW_HOME}\" -executable ! -type l -print0 | xargs --null chmod g+x \\\n    && find \"${AIRFLOW_USER_HOME_DIR}\" -executable ! -type l -print0 | xargs --null chmod g+x\n\nARG AIRFLOW_SOURCES_FROM\nARG AIRFLOW_SOURCES_TO\n\nCOPY --from=airflow-build-image --chown=airflow:0 \\\n     \"${AIRFLOW_USER_HOME_DIR}/.local\" \"${AIRFLOW_USER_HOME_DIR}/.local\"\nCOPY --from=airflow-build-image --chown=airflow:0 \\\n     \"${AIRFLOW_USER_HOME_DIR}/constraints.txt\" \"${AIRFLOW_USER_HOME_DIR}/constraints.txt\"\n# In case of editable build also copy airflow sources so that they are available in the main image\n# For regular image (non-editable) this will be just Dockerfile copied to /Dockerfile\nCOPY --from=airflow-build-image --chown=airflow:0 \"${AIRFLOW_SOURCES_TO}\" \"${AIRFLOW_SOURCES_TO}\"\n\nCOPY --from=scripts entrypoint_prod.sh /entrypoint\nCOPY --from=scripts clean-logs.sh /clean-logs\nCOPY --from=scripts airflow-scheduler-autorestart.sh /airflow-scheduler-autorestart\n\n# Make /etc/passwd root-group-writeable so that user can be dynamically added by OpenShift\n# See https://github.com/apache/airflow/issues/9248\n# Set default groups for airflow and root user\n\nRUN chmod a+rx /entrypoint /clean-logs \\\n    && chmod g=u /etc/passwd \\\n    && chmod g+w \"${AIRFLOW_USER_HOME_DIR}/.local\" \\\n    && usermod -g 0 airflow -G 0\n\n# make sure that the venv is activated for all users\n# including plain sudo, sudo with --interactive flag\nRUN sed --in-place=.bak \"s/secure_path=\\\"/secure_path=\\\"$(echo -n ${AIRFLOW_USER_HOME_DIR} | \\\n        sed 's/\\//\\\\\\//g')\\/.local\\/bin:/\" /etc/sudoers\n\nARG AIRFLOW_VERSION\nARG AIRFLOW_PIP_VERSION\nARG AIRFLOW_UV_VERSION\nARG AIRFLOW_USE_UV\n\n# See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation\n# to learn more about the way how signals are handled by the image\n# Also set airflow as nice PROMPT message.\nENV DUMB_INIT_SETSID=\"1\" \\\n    PS1=\"(airflow)\" \\\n    AIRFLOW_VERSION=${AIRFLOW_VERSION} \\\n    AIRFLOW__CORE__LOAD_EXAMPLES=\"false\" \\\n    PATH=\"/root/bin:${PATH}\" \\\n    AIRFLOW_PIP_VERSION=${AIRFLOW_PIP_VERSION} \\\n    AIRFLOW_UV_VERSION=${AIRFLOW_UV_VERSION} \\\n    AIRFLOW_USE_UV=${AIRFLOW_USE_UV}\n\n# Add protection against running pip as root user\nRUN mkdir -pv /root/bin\nCOPY --from=scripts pip /root/bin/pip\nRUN chmod u+x /root/bin/pip\n\nWORKDIR ${AIRFLOW_HOME}\n\nEXPOSE 8080\n\nUSER ${AIRFLOW_UID}\n\n# Those should be set and used as late as possible as any change in commit/build otherwise invalidates the\n# layers right after\nARG BUILD_ID\nARG COMMIT_SHA\nARG AIRFLOW_IMAGE_REPOSITORY\nARG AIRFLOW_IMAGE_DATE_CREATED\n\nENV BUILD_ID=${BUILD_ID} COMMIT_SHA=${COMMIT_SHA}\n\nLABEL org.apache.airflow.distro=\"debian\" \\\n  org.apache.airflow.module=\"airflow\" \\\n  org.apache.airflow.component=\"airflow\" \\\n  org.apache.airflow.image=\"airflow\" \\\n  org.apache.airflow.version=\"${AIRFLOW_VERSION}\" \\\n  org.apache.airflow.uid=\"${AIRFLOW_UID}\" \\\n  org.apache.airflow.main-image.build-id=\"${BUILD_ID}\" \\\n  org.apache.airflow.main-image.commit-sha=\"${COMMIT_SHA}\" \\\n  org.opencontainers.image.source=\"${AIRFLOW_IMAGE_REPOSITORY}\" \\\n  org.opencontainers.image.created=${AIRFLOW_IMAGE_DATE_CREATED} \\\n  org.opencontainers.image.authors=\"dev@airflow.apache.org\" \\\n  org.opencontainers.image.url=\"https://airflow.apache.org\" \\\n  org.opencontainers.image.documentation=\"https://airflow.apache.org/docs/docker-stack/index.html\" \\\n  org.opencontainers.image.version=\"${AIRFLOW_VERSION}\" \\\n  org.opencontainers.image.revision=\"${COMMIT_SHA}\" \\\n  org.opencontainers.image.vendor=\"Apache Software Foundation\" \\\n  org.opencontainers.image.licenses=\"Apache-2.0\" \\\n  org.opencontainers.image.ref.name=\"airflow\" \\\n  org.opencontainers.image.title=\"Production Airflow Image\" \\\n  org.opencontainers.image.description=\"Reference, production-ready Apache Airflow image\"\n\nENTRYPOINT [\"/usr/bin/dumb-init\", \"--\", \"/entrypoint\"]\nCMD []\n"
        },
        {
          "name": "Dockerfile.ci",
          "type": "blob",
          "size": 55.3212890625,
          "content": "# syntax=docker/dockerfile:1.4\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# WARNING: THIS DOCKERFILE IS NOT INTENDED FOR PRODUCTION USE OR DEPLOYMENT.\n#\nARG PYTHON_BASE_IMAGE=\"python:3.9-slim-bookworm\"\n\n##############################################################################################\n# This is the script image where we keep all inlined bash scripts needed in other segments\n# We use PYTHON_BASE_IMAGE to make sure that the scripts are different for different platforms.\n##############################################################################################\nFROM ${PYTHON_BASE_IMAGE} as scripts\n\n##############################################################################################\n# Please DO NOT modify the inlined scripts manually. The content of those files will be\n# replaced by pre-commit automatically from the \"scripts/docker/\" folder.\n# This is done in order to avoid problems with caching and file permissions and in order to\n# make the PROD Dockerfile standalone\n##############################################################################################\n\n# The content below is automatically copied from scripts/docker/install_os_dependencies.sh\nCOPY <<\"EOF\" /install_os_dependencies.sh\n#!/usr/bin/env bash\nset -euo pipefail\n\nif [[ \"$#\" != 1 ]]; then\n    echo \"ERROR! There should be 'runtime' or 'dev' parameter passed as argument.\".\n    exit 1\nfi\n\nif [[ \"${1}\" == \"runtime\" ]]; then\n    INSTALLATION_TYPE=\"RUNTIME\"\nelif   [[ \"${1}\" == \"dev\" ]]; then\n    INSTALLATION_TYPE=\"dev\"\nelse\n    echo \"ERROR! Wrong argument. Passed ${1} and it should be one of 'runtime' or 'dev'.\".\n    exit 1\nfi\n\nfunction get_dev_apt_deps() {\n    if [[ \"${DEV_APT_DEPS=}\" == \"\" ]]; then\n        DEV_APT_DEPS=\"apt-transport-https apt-utils build-essential ca-certificates dirmngr \\\nfreetds-bin freetds-dev git graphviz graphviz-dev krb5-user ldap-utils libev4 libev-dev libffi-dev libgeos-dev \\\nlibkrb5-dev libldap2-dev libleveldb1d libleveldb-dev libsasl2-2 libsasl2-dev libsasl2-modules \\\nlibssl-dev libxmlsec1 libxmlsec1-dev locales lsb-release openssh-client pkgconf sasl2-bin \\\nsoftware-properties-common sqlite3 sudo unixodbc unixodbc-dev zlib1g-dev\"\n        export DEV_APT_DEPS\n    fi\n}\n\nfunction get_runtime_apt_deps() {\n    local debian_version\n    local debian_version_apt_deps\n    # Get debian version without installing lsb_release\n    # shellcheck disable=SC1091\n    debian_version=$(. /etc/os-release;   printf '%s\\n' \"$VERSION_CODENAME\";)\n    echo\n    echo \"DEBIAN CODENAME: ${debian_version}\"\n    echo\n    debian_version_apt_deps=\"libffi8 libldap-2.5-0 libssl3 netcat-openbsd\"\n    echo\n    echo \"APPLIED INSTALLATION CONFIGURATION FOR DEBIAN VERSION: ${debian_version}\"\n    echo\n    if [[ \"${RUNTIME_APT_DEPS=}\" == \"\" ]]; then\n        RUNTIME_APT_DEPS=\"apt-transport-https apt-utils ca-certificates \\\ncurl dumb-init freetds-bin krb5-user libev4 libgeos-dev \\\nldap-utils libsasl2-2 libsasl2-modules libxmlsec1 locales ${debian_version_apt_deps} \\\nlsb-release openssh-client python3-selinux rsync sasl2-bin sqlite3 sudo unixodbc\"\n        export RUNTIME_APT_DEPS\n    fi\n}\n\nfunction install_docker_cli() {\n    apt-get update\n    apt-get install ca-certificates curl\n    install -m 0755 -d /etc/apt/keyrings\n    curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc\n    chmod a+r /etc/apt/keyrings/docker.asc\n    # shellcheck disable=SC1091\n    echo \\\n      \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian \\\n      $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n      tee /etc/apt/sources.list.d/docker.list > /dev/null\n    apt-get update\n    apt-get install -y --no-install-recommends docker-ce-cli\n}\n\nfunction install_debian_dev_dependencies() {\n    apt-get update\n    apt-get install -yqq --no-install-recommends apt-utils >/dev/null 2>&1\n    apt-get install -y --no-install-recommends curl gnupg2 lsb-release\n    # shellcheck disable=SC2086\n    export ${ADDITIONAL_DEV_APT_ENV?}\n    if [[ ${DEV_APT_COMMAND} != \"\" ]]; then\n        bash -o pipefail -o errexit -o nounset -o nolog -c \"${DEV_APT_COMMAND}\"\n    fi\n    if [[ ${ADDITIONAL_DEV_APT_COMMAND} != \"\" ]]; then\n        bash -o pipefail -o errexit -o nounset -o nolog -c \"${ADDITIONAL_DEV_APT_COMMAND}\"\n    fi\n    apt-get update\n    local debian_version\n    local debian_version_apt_deps\n    # Get debian version without installing lsb_release\n    # shellcheck disable=SC1091\n    debian_version=$(. /etc/os-release;   printf '%s\\n' \"$VERSION_CODENAME\";)\n    echo\n    echo \"DEBIAN CODENAME: ${debian_version}\"\n    echo\n    # shellcheck disable=SC2086\n    apt-get install -y --no-install-recommends ${DEV_APT_DEPS} ${ADDITIONAL_DEV_APT_DEPS}\n}\n\nfunction install_debian_runtime_dependencies() {\n    apt-get update\n    apt-get install --no-install-recommends -yqq apt-utils >/dev/null 2>&1\n    apt-get install -y --no-install-recommends curl gnupg2 lsb-release\n    # shellcheck disable=SC2086\n    export ${ADDITIONAL_RUNTIME_APT_ENV?}\n    if [[ \"${RUNTIME_APT_COMMAND}\" != \"\" ]]; then\n        bash -o pipefail -o errexit -o nounset -o nolog -c \"${RUNTIME_APT_COMMAND}\"\n    fi\n    if [[ \"${ADDITIONAL_RUNTIME_APT_COMMAND}\" != \"\" ]]; then\n        bash -o pipefail -o errexit -o nounset -o nolog -c \"${ADDITIONAL_RUNTIME_APT_COMMAND}\"\n    fi\n    apt-get update\n    # shellcheck disable=SC2086\n    apt-get install -y --no-install-recommends ${RUNTIME_APT_DEPS} ${ADDITIONAL_RUNTIME_APT_DEPS}\n    apt-get autoremove -yqq --purge\n    apt-get clean\n    rm -rf /var/lib/apt/lists/* /var/log/*\n}\n\nif [[ \"${INSTALLATION_TYPE}\" == \"RUNTIME\" ]]; then\n    get_runtime_apt_deps\n    install_debian_runtime_dependencies\n    install_docker_cli\n\nelse\n    get_dev_apt_deps\n    install_debian_dev_dependencies\n    install_docker_cli\nfi\nEOF\n\n# The content below is automatically copied from scripts/docker/install_mysql.sh\nCOPY <<\"EOF\" /install_mysql.sh\n#!/usr/bin/env bash\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\n\nset -euo pipefail\n\ncommon::get_colors\ndeclare -a packages\n\nreadonly MYSQL_LTS_VERSION=\"8.0\"\nreadonly MARIADB_LTS_VERSION=\"10.11\"\n\n: \"${INSTALL_MYSQL_CLIENT:?Should be true or false}\"\n: \"${INSTALL_MYSQL_CLIENT_TYPE:-mariadb}\"\n\ninstall_mysql_client() {\n    if [[ \"${1}\" == \"dev\" ]]; then\n        packages=(\"libmysqlclient-dev\" \"mysql-client\")\n    elif [[ \"${1}\" == \"prod\" ]]; then\n        # `libmysqlclientXX` where XX is number, and it should be increased every new GA MySQL release, for example\n        # 18 - MySQL 5.6.48\n        # 20 - MySQL 5.7.42\n        # 21 - MySQL 8.0.34\n        # 22 - MySQL 8.1\n        packages=(\"libmysqlclient21\" \"mysql-client\")\n    else\n        echo\n        echo \"${COLOR_RED}Specify either prod or dev${COLOR_RESET}\"\n        echo\n        exit 1\n    fi\n\n    common::import_trusted_gpg \"B7B3B788A8D3785C\" \"mysql\"\n\n    echo\n    echo \"${COLOR_BLUE}Installing Oracle MySQL client version ${MYSQL_LTS_VERSION}: ${1}${COLOR_RESET}\"\n    echo\n\n    echo \"deb http://repo.mysql.com/apt/debian/ $(lsb_release -cs) mysql-${MYSQL_LTS_VERSION}\" > \\\n        /etc/apt/sources.list.d/mysql.list\n    apt-get update\n    apt-get install --no-install-recommends -y \"${packages[@]}\"\n    apt-get autoremove -yqq --purge\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n\n    # Remove mysql repository from sources.list.d as MySQL repos have a basic flaw that they put expiry\n    # date on their GPG signing keys and they sign their repo with those keys. This means that after a\n    # certain date, the GPG key becomes invalid and if you have the repository added in your sources.list\n    # then you will not be able to install anything from any other repository. This id unlike any other\n    # repository we have seen (for example Postgres, MariaDB, MsSQL - all have non-expiring signing keys)\n    rm /etc/apt/sources.list.d/mysql.list\n}\n\ninstall_mariadb_client() {\n    # List of compatible package Oracle MySQL -> MariaDB:\n    # `mysql-client` -> `mariadb-client` or `mariadb-client-compat` (11+)\n    # `libmysqlclientXX` (where XX is a number) -> `libmariadb3-compat`\n    # `libmysqlclient-dev` -> `libmariadb-dev-compat`\n    #\n    # Different naming against Debian repo which we used before\n    # that some of packages might contains `-compat` suffix, Debian repo -> MariaDB repo:\n    # `libmariadb-dev` -> `libmariadb-dev-compat`\n    # `mariadb-client-core` -> `mariadb-client` or `mariadb-client-compat` (11+)\n    if [[ \"${1}\" == \"dev\" ]]; then\n        packages=(\"libmariadb-dev-compat\" \"mariadb-client\")\n    elif [[ \"${1}\" == \"prod\" ]]; then\n        packages=(\"libmariadb3-compat\" \"mariadb-client\")\n    else\n        echo\n        echo \"${COLOR_RED}Specify either prod or dev${COLOR_RESET}\"\n        echo\n        exit 1\n    fi\n\n    common::import_trusted_gpg \"0xF1656F24C74CD1D8\" \"mariadb\"\n\n    echo\n    echo \"${COLOR_BLUE}Installing MariaDB client version ${MARIADB_LTS_VERSION}: ${1}${COLOR_RESET}\"\n    echo \"${COLOR_YELLOW}MariaDB client protocol-compatible with MySQL client.${COLOR_RESET}\"\n    echo\n\n    echo \"deb [arch=amd64,arm64] https://archive.mariadb.org/mariadb-${MARIADB_LTS_VERSION}/repo/debian/ $(lsb_release -cs) main\" > \\\n        /etc/apt/sources.list.d/mariadb.list\n    # Make sure that dependencies from MariaDB repo are preferred over Debian dependencies\n    printf \"Package: *\\nPin: release o=MariaDB\\nPin-Priority: 999\\n\" > /etc/apt/preferences.d/mariadb\n    apt-get update\n    apt-get install --no-install-recommends -y \"${packages[@]}\"\n    apt-get autoremove -yqq --purge\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n}\n\nif [[ ${INSTALL_MYSQL_CLIENT:=\"true\"} == \"true\" ]]; then\n    if [[ $(uname -m) == \"arm64\" || $(uname -m) == \"aarch64\" ]]; then\n        INSTALL_MYSQL_CLIENT_TYPE=\"mariadb\"\n        echo\n        echo \"${COLOR_YELLOW}Client forced to mariadb for ARM${COLOR_RESET}\"\n        echo\n    fi\n\n    if [[ \"${INSTALL_MYSQL_CLIENT_TYPE}\" == \"mysql\" ]]; then\n        install_mysql_client \"${@}\"\n    elif [[ \"${INSTALL_MYSQL_CLIENT_TYPE}\" == \"mariadb\" ]]; then\n        install_mariadb_client \"${@}\"\n    else\n        echo\n        echo \"${COLOR_RED}Specify either mysql or mariadb, got ${INSTALL_MYSQL_CLIENT_TYPE}${COLOR_RESET}\"\n        echo\n        exit 1\n    fi\nfi\nEOF\n\n# The content below is automatically copied from scripts/docker/install_mssql.sh\nCOPY <<\"EOF\" /install_mssql.sh\n#!/usr/bin/env bash\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\n\nset -euo pipefail\n\ncommon::get_colors\ndeclare -a packages\n\n: \"${INSTALL_MSSQL_CLIENT:?Should be true or false}\"\n\n\nfunction install_mssql_client() {\n    # Install MsSQL client from Microsoft repositories\n    if [[ ${INSTALL_MSSQL_CLIENT:=\"true\"} != \"true\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Skip installing mssql client${COLOR_RESET}\"\n        echo\n        return\n    fi\n    packages=(\"msodbcsql18\")\n\n    common::import_trusted_gpg \"EB3E94ADBE1229CF\" \"microsoft\"\n\n    echo\n    echo \"${COLOR_BLUE}Installing mssql client${COLOR_RESET}\"\n    echo\n\n    echo \"deb [arch=amd64,arm64] https://packages.microsoft.com/debian/$(lsb_release -rs)/prod $(lsb_release -cs) main\" > \\\n        /etc/apt/sources.list.d/mssql-release.list\n    apt-get update -yqq\n    apt-get upgrade -yqq\n    ACCEPT_EULA=Y apt-get -yqq install --no-install-recommends \"${packages[@]}\"\n    rm -rf /var/lib/apt/lists/*\n    apt-get autoremove -yqq --purge\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n}\n\ninstall_mssql_client \"${@}\"\nEOF\n\n# The content below is automatically copied from scripts/docker/install_postgres.sh\nCOPY <<\"EOF\" /install_postgres.sh\n#!/usr/bin/env bash\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\nset -euo pipefail\n\ncommon::get_colors\ndeclare -a packages\n\n: \"${INSTALL_POSTGRES_CLIENT:?Should be true or false}\"\n\ninstall_postgres_client() {\n    echo\n    echo \"${COLOR_BLUE}Installing postgres client${COLOR_RESET}\"\n    echo\n\n    if [[ \"${1}\" == \"dev\" ]]; then\n        packages=(\"libpq-dev\" \"postgresql-client\")\n    elif [[ \"${1}\" == \"prod\" ]]; then\n        packages=(\"postgresql-client\")\n    else\n        echo\n        echo \"Specify either prod or dev\"\n        echo\n        exit 1\n    fi\n\n    common::import_trusted_gpg \"7FCC7D46ACCC4CF8\" \"postgres\"\n\n    echo \"deb [arch=amd64,arm64] https://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main\" > \\\n        /etc/apt/sources.list.d/pgdg.list\n    apt-get update\n    apt-get install --no-install-recommends -y \"${packages[@]}\"\n    apt-get autoremove -yqq --purge\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n}\n\nif [[ ${INSTALL_POSTGRES_CLIENT:=\"true\"} == \"true\" ]]; then\n    install_postgres_client \"${@}\"\nfi\nEOF\n\n# The content below is automatically copied from scripts/docker/install_packaging_tools.sh\nCOPY <<\"EOF\" /install_packaging_tools.sh\n#!/usr/bin/env bash\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\n\ncommon::get_colors\ncommon::get_packaging_tool\ncommon::show_packaging_tool_version_and_location\ncommon::install_packaging_tools\nEOF\n\n# The content below is automatically copied from scripts/docker/common.sh\nCOPY <<\"EOF\" /common.sh\n#!/usr/bin/env bash\nset -euo pipefail\n\nfunction common::get_colors() {\n    COLOR_BLUE=$'\\e[34m'\n    COLOR_GREEN=$'\\e[32m'\n    COLOR_RED=$'\\e[31m'\n    COLOR_RESET=$'\\e[0m'\n    COLOR_YELLOW=$'\\e[33m'\n    export COLOR_BLUE\n    export COLOR_GREEN\n    export COLOR_RED\n    export COLOR_RESET\n    export COLOR_YELLOW\n}\n\nfunction common::get_packaging_tool() {\n    : \"${AIRFLOW_USE_UV:?Should be set}\"\n\n    ## IMPORTANT: IF YOU MODIFY THIS FUNCTION YOU SHOULD ALSO MODIFY CORRESPONDING FUNCTION IN\n    ## `scripts/in_container/_in_container_utils.sh`\n    if [[ ${AIRFLOW_USE_UV} == \"true\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Using 'uv' to install Airflow${COLOR_RESET}\"\n        echo\n        export PACKAGING_TOOL=\"uv\"\n        export PACKAGING_TOOL_CMD=\"uv pip\"\n        if [[ -z ${VIRTUAL_ENV=} ]]; then\n            export EXTRA_INSTALL_FLAGS=\"--system\"\n            export EXTRA_UNINSTALL_FLAGS=\"--system\"\n        else\n            export EXTRA_INSTALL_FLAGS=\"\"\n            export EXTRA_UNINSTALL_FLAGS=\"\"\n        fi\n        export UPGRADE_EAGERLY=\"--upgrade --resolution highest\"\n        export UPGRADE_IF_NEEDED=\"--upgrade\"\n        UV_CONCURRENT_DOWNLOADS=$(nproc --all)\n        export UV_CONCURRENT_DOWNLOADS\n    else\n        echo\n        echo \"${COLOR_BLUE}Using 'pip' to install Airflow${COLOR_RESET}\"\n        echo\n        export PACKAGING_TOOL=\"pip\"\n        export PACKAGING_TOOL_CMD=\"pip\"\n        export EXTRA_INSTALL_FLAGS=\"--root-user-action ignore\"\n        export EXTRA_UNINSTALL_FLAGS=\"--yes\"\n        export UPGRADE_EAGERLY=\"--upgrade --upgrade-strategy eager\"\n        export UPGRADE_IF_NEEDED=\"--upgrade --upgrade-strategy only-if-needed\"\n    fi\n}\n\nfunction common::get_airflow_version_specification() {\n    if [[ -z ${AIRFLOW_VERSION_SPECIFICATION=}\n        && -n ${AIRFLOW_VERSION}\n        && ${AIRFLOW_INSTALLATION_METHOD} != \".\" ]]; then\n        AIRFLOW_VERSION_SPECIFICATION=\"==${AIRFLOW_VERSION}\"\n    fi\n}\n\nfunction common::get_constraints_location() {\n    # auto-detect Airflow-constraint reference and location\n    if [[ -z \"${AIRFLOW_CONSTRAINTS_REFERENCE=}\" ]]; then\n        if  [[ ${AIRFLOW_VERSION} =~ v?2.* && ! ${AIRFLOW_VERSION} =~ .*dev.* ]]; then\n            AIRFLOW_CONSTRAINTS_REFERENCE=constraints-${AIRFLOW_VERSION}\n        else\n            AIRFLOW_CONSTRAINTS_REFERENCE=${DEFAULT_CONSTRAINTS_BRANCH}\n        fi\n    fi\n\n    if [[ -z ${AIRFLOW_CONSTRAINTS_LOCATION=} ]]; then\n        local constraints_base=\"https://raw.githubusercontent.com/${CONSTRAINTS_GITHUB_REPOSITORY}/${AIRFLOW_CONSTRAINTS_REFERENCE}\"\n        local python_version\n        python_version=$(python -c 'import sys; print(f\"{sys.version_info.major}.{sys.version_info.minor}\")')\n        AIRFLOW_CONSTRAINTS_LOCATION=\"${constraints_base}/${AIRFLOW_CONSTRAINTS_MODE}-${python_version}.txt\"\n    fi\n\n    if [[ ${AIRFLOW_CONSTRAINTS_LOCATION} =~ http.* ]]; then\n        echo\n        echo \"${COLOR_BLUE}Downloading constraints from ${AIRFLOW_CONSTRAINTS_LOCATION} to ${HOME}/constraints.txt ${COLOR_RESET}\"\n        echo\n        curl -sSf -o \"${HOME}/constraints.txt\" \"${AIRFLOW_CONSTRAINTS_LOCATION}\"\n    else\n        echo\n        echo \"${COLOR_BLUE}Copying constraints from ${AIRFLOW_CONSTRAINTS_LOCATION} to ${HOME}/constraints.txt ${COLOR_RESET}\"\n        echo\n        cp \"${AIRFLOW_CONSTRAINTS_LOCATION}\" \"${HOME}/constraints.txt\"\n    fi\n}\n\nfunction common::show_packaging_tool_version_and_location() {\n   echo \"PATH=${PATH}\"\n   echo \"Installed pip: $(pip --version): $(which pip)\"\n   if [[ ${PACKAGING_TOOL} == \"pip\" ]]; then\n       echo \"${COLOR_BLUE}Using 'pip' to install Airflow${COLOR_RESET}\"\n   else\n       echo \"${COLOR_BLUE}Using 'uv' to install Airflow${COLOR_RESET}\"\n       echo \"Installed uv: $(uv --version 2>/dev/null || echo \"Not installed yet\"): $(which uv 2>/dev/null)\"\n   fi\n}\n\nfunction common::install_packaging_tools() {\n    : \"${AIRFLOW_USE_UV:?Should be set}\"\n    if [[ \"${VIRTUAL_ENV=}\" != \"\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Checking packaging tools in venv: ${VIRTUAL_ENV}${COLOR_RESET}\"\n        echo\n    else\n        echo\n        echo \"${COLOR_BLUE}Checking packaging tools for system Python installation: $(which python)${COLOR_RESET}\"\n        echo\n    fi\n    if [[ ${AIRFLOW_PIP_VERSION=} == \"\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Installing latest pip version${COLOR_RESET}\"\n        echo\n        pip install --root-user-action ignore --disable-pip-version-check --upgrade pip\n    elif [[ ! ${AIRFLOW_PIP_VERSION} =~ ^[0-9].* ]]; then\n        echo\n        echo \"${COLOR_BLUE}Installing pip version from spec ${AIRFLOW_PIP_VERSION}${COLOR_RESET}\"\n        echo\n        # shellcheck disable=SC2086\n        pip install --root-user-action ignore --disable-pip-version-check \"pip @ ${AIRFLOW_PIP_VERSION}\"\n    else\n        local installed_pip_version\n        installed_pip_version=$(python -c 'from importlib.metadata import version; print(version(\"pip\"))')\n        if [[ ${installed_pip_version} != \"${AIRFLOW_PIP_VERSION}\" ]]; then\n            echo\n            echo \"${COLOR_BLUE}(Re)Installing pip version: ${AIRFLOW_PIP_VERSION}${COLOR_RESET}\"\n            echo\n            pip install --root-user-action ignore --disable-pip-version-check \"pip==${AIRFLOW_PIP_VERSION}\"\n        fi\n    fi\n    if [[ ${AIRFLOW_UV_VERSION=} == \"\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Installing latest uv version${COLOR_RESET}\"\n        echo\n        pip install --root-user-action ignore --disable-pip-version-check --upgrade uv\n    elif [[ ! ${AIRFLOW_UV_VERSION} =~ ^[0-9].* ]]; then\n        echo\n        echo \"${COLOR_BLUE}Installing uv version from spec ${AIRFLOW_UV_VERSION}${COLOR_RESET}\"\n        echo\n        # shellcheck disable=SC2086\n        pip install --root-user-action ignore --disable-pip-version-check \"uv @ ${AIRFLOW_UV_VERSION}\"\n    else\n        local installed_uv_version\n        installed_uv_version=$(python -c 'from importlib.metadata import version; print(version(\"uv\"))' 2>/dev/null || echo \"Not installed yet\")\n        if [[ ${installed_uv_version} != \"${AIRFLOW_UV_VERSION}\" ]]; then\n            echo\n            echo \"${COLOR_BLUE}(Re)Installing uv version: ${AIRFLOW_UV_VERSION}${COLOR_RESET}\"\n            echo\n            # shellcheck disable=SC2086\n            pip install --root-user-action ignore --disable-pip-version-check \"uv==${AIRFLOW_UV_VERSION}\"\n        fi\n    fi\n    if  [[ ${AIRFLOW_PRE_COMMIT_VERSION=} == \"\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Installing latest pre-commit with pre-commit-uv uv${COLOR_RESET}\"\n        echo\n        uv tool install pre-commit --with pre-commit-uv --with uv\n        # make sure that the venv/user in .local exists\n        mkdir -p \"${HOME}/.local/bin\"\n    else\n        echo\n        echo \"${COLOR_BLUE}Installing predefined versions of pre-commit with pre-commit-uv and uv:${COLOR_RESET}\"\n        echo \"${COLOR_BLUE}pre_commit(${AIRFLOW_PRE_COMMIT_VERSION}) uv(${AIRFLOW_UV_VERSION}) pre_commit-uv(${AIRFLOW_PRE_COMMIT_UV_VERSION})${COLOR_RESET}\"\n        echo\n        uv tool install \"pre-commit==${AIRFLOW_PRE_COMMIT_VERSION}\" \\\n            --with \"uv==${AIRFLOW_UV_VERSION}\" --with \"pre-commit-uv==${AIRFLOW_PRE_COMMIT_UV_VERSION}\"\n        # make sure that the venv/user in .local exists\n        mkdir -p \"${HOME}/.local/bin\"\n    fi\n}\n\nfunction common::import_trusted_gpg() {\n    common::get_colors\n\n    local key=${1:?${COLOR_RED}First argument expects OpenPGP Key ID${COLOR_RESET}}\n    local name=${2:?${COLOR_RED}Second argument expected trust storage name${COLOR_RESET}}\n    # Please note that not all servers could be used for retrieve keys\n    #  sks-keyservers.net: Unmaintained and DNS taken down due to GDPR requests.\n    #  keys.openpgp.org: User ID Mandatory, not suitable for APT repositories\n    #  keyring.debian.org: Only accept keys in Debian keyring.\n    #  pgp.mit.edu: High response time.\n    local keyservers=(\n        \"hkps://keyserver.ubuntu.com\"\n        \"hkps://pgp.surf.nl\"\n    )\n\n    GNUPGHOME=\"$(mktemp -d)\"\n    export GNUPGHOME\n    set +e\n    for keyserver in $(shuf -e \"${keyservers[@]}\"); do\n        echo \"${COLOR_BLUE}Try to receive GPG public key ${key} from ${keyserver}${COLOR_RESET}\"\n        gpg --keyserver \"${keyserver}\" --recv-keys \"${key}\" 2>&1 && break\n        echo \"${COLOR_YELLOW}Unable to receive GPG public key ${key} from ${keyserver}${COLOR_RESET}\"\n    done\n    set -e\n    gpg --export \"${key}\" > \"/etc/apt/trusted.gpg.d/${name}.gpg\"\n    gpgconf --kill all\n    rm -rf \"${GNUPGHOME}\"\n    unset GNUPGHOME\n}\nEOF\n\n# The content below is automatically copied from scripts/docker/install_airflow.sh\nCOPY <<\"EOF\" /install_airflow.sh\n#!/usr/bin/env bash\n\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\n\nfunction install_airflow() {\n    # Remove mysql from extras if client is not going to be installed\n    if [[ ${INSTALL_MYSQL_CLIENT} != \"true\" ]]; then\n        AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS/mysql,}\n        echo \"${COLOR_YELLOW}MYSQL client installation is disabled. Extra 'mysql' installations were therefore omitted.${COLOR_RESET}\"\n    fi\n    # Remove postgres from extras if client is not going to be installed\n    if [[ ${INSTALL_POSTGRES_CLIENT} != \"true\" ]]; then\n        AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS/postgres,}\n        echo \"${COLOR_YELLOW}Postgres client installation is disabled. Extra 'postgres' installations were therefore omitted.${COLOR_RESET}\"\n    fi\n    # Determine the installation_command_flags based on AIRFLOW_INSTALLATION_METHOD method\n    local installation_command_flags\n    if [[ ${AIRFLOW_INSTALLATION_METHOD} == \".\" ]]; then\n        # When installing from sources - we always use `--editable` mode\n        installation_command_flags=\"--editable .[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION} --editable ./task_sdk\"\n        while IFS= read -r -d '' pyproject_toml_file; do\n            project_folder=$(dirname ${pyproject_toml_file})\n            installation_command_flags=\"${installation_command_flags} --editable ${project_folder}\"\n        done < <(find \"providers\" -name \"pyproject.toml\" -print0)\n    elif [[ ${AIRFLOW_INSTALLATION_METHOD} == \"apache-airflow\" ]]; then\n        installation_command_flags=\"apache-airflow[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION}\"\n    elif [[ ${AIRFLOW_INSTALLATION_METHOD} == apache-airflow\\ @\\ * ]]; then\n        installation_command_flags=\"apache-airflow[${AIRFLOW_EXTRAS}] @ ${AIRFLOW_VERSION_SPECIFICATION/apache-airflow @//}\"\n    else\n        echo\n        echo \"${COLOR_RED}The '${INSTALLATION_METHOD}' installation method is not supported${COLOR_RESET}\"\n        echo\n        echo \"${COLOR_YELLOW}Supported methods are ('.', 'apache-airflow', 'apache-airflow @ URL')${COLOR_RESET}\"\n        echo\n        exit 1\n    fi\n    if [[ \"${UPGRADE_INVALIDATION_STRING=}\" != \"\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Remove airflow and all provider packages installed before potentially${COLOR_RESET}\"\n        echo\n        set -x\n        ${PACKAGING_TOOL_CMD} freeze | grep apache-airflow | xargs ${PACKAGING_TOOL_CMD} uninstall ${EXTRA_UNINSTALL_FLAGS} 2>/dev/null || true\n        set +x\n        echo\n        echo \"${COLOR_BLUE}Installing all packages in eager upgrade mode. Installation method: ${AIRFLOW_INSTALLATION_METHOD}${COLOR_RESET}\"\n        echo\n        set -x\n        ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} ${UPGRADE_EAGERLY} ${ADDITIONAL_PIP_INSTALL_FLAGS} ${installation_command_flags} ${EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=}\n        set +x\n        common::install_packaging_tools\n        echo\n        echo \"${COLOR_BLUE}Running 'pip check'${COLOR_RESET}\"\n        echo\n        pip check\n    else\n        echo\n        echo \"${COLOR_BLUE}Installing all packages with constraints. Installation method: ${AIRFLOW_INSTALLATION_METHOD}${COLOR_RESET}\"\n        echo\n        set -x\n        # Install all packages with constraints\n        if ! ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} ${ADDITIONAL_PIP_INSTALL_FLAGS} ${installation_command_flags} --constraint \"${HOME}/constraints.txt\"; then\n            set +x\n            echo\n            echo \"${COLOR_YELLOW}Likely pyproject.toml has new dependencies conflicting with constraints.${COLOR_RESET}\"\n            echo\n            echo \"${COLOR_BLUE}Falling back to no-constraints installation.${COLOR_RESET}\"\n            echo\n            set -x\n            ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} ${UPGRADE_IF_NEEDED} ${ADDITIONAL_PIP_INSTALL_FLAGS} ${installation_command_flags}\n        fi\n        set +x\n        common::install_packaging_tools\n        echo\n        echo \"${COLOR_BLUE}Running 'pip check'${COLOR_RESET}\"\n        echo\n        pip check\n    fi\n\n}\n\ncommon::get_colors\ncommon::get_packaging_tool\ncommon::get_airflow_version_specification\ncommon::get_constraints_location\ncommon::show_packaging_tool_version_and_location\n\ninstall_airflow\nEOF\n\n# The content below is automatically copied from scripts/docker/install_additional_dependencies.sh\nCOPY <<\"EOF\" /install_additional_dependencies.sh\n#!/usr/bin/env bash\nset -euo pipefail\n\n: \"${ADDITIONAL_PYTHON_DEPS:?Should be set}\"\n\n. \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"\n\nfunction install_additional_dependencies() {\n    if [[ \"${UPGRADE_INVALIDATION_STRING=}\" != \"\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Installing additional dependencies while upgrading to newer dependencies${COLOR_RESET}\"\n        echo\n        set -x\n        ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} ${UPGRADE_EAGERLY} \\\n            ${ADDITIONAL_PIP_INSTALL_FLAGS} \\\n            ${ADDITIONAL_PYTHON_DEPS} ${EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=}\n        set +x\n        common::install_packaging_tools\n        echo\n        echo \"${COLOR_BLUE}Running 'pip check'${COLOR_RESET}\"\n        echo\n        pip check\n    else\n        echo\n        echo \"${COLOR_BLUE}Installing additional dependencies upgrading only if needed${COLOR_RESET}\"\n        echo\n        set -x\n        ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} ${UPGRADE_IF_NEEDED} \\\n            ${ADDITIONAL_PIP_INSTALL_FLAGS} \\\n            ${ADDITIONAL_PYTHON_DEPS}\n        set +x\n        common::install_packaging_tools\n        echo\n        echo \"${COLOR_BLUE}Running 'pip check'${COLOR_RESET}\"\n        echo\n        pip check\n    fi\n}\n\ncommon::get_colors\ncommon::get_packaging_tool\ncommon::get_airflow_version_specification\ncommon::get_constraints_location\ncommon::show_packaging_tool_version_and_location\n\ninstall_additional_dependencies\nEOF\n\n# The content below is automatically copied from scripts/docker/entrypoint_ci.sh\nCOPY <<\"EOF\" /entrypoint_ci.sh\n#!/usr/bin/env bash\nif [[ ${VERBOSE_COMMANDS:=\"false\"} == \"true\" ]]; then\n    set -x\nfi\n\n\n. \"${AIRFLOW_SOURCES:-/opt/airflow}\"/scripts/in_container/_in_container_script_init.sh\n\nLD_PRELOAD=\"/usr/lib/$(uname -m)-linux-gnu/libstdc++.so.6\"\nexport LD_PRELOAD\n\nchmod 1777 /tmp\n\nAIRFLOW_SOURCES=$(cd \"${IN_CONTAINER_DIR}/../..\" || exit 1; pwd)\n\nPYTHON_MAJOR_MINOR_VERSION=${PYTHON_MAJOR_MINOR_VERSION:=3.9}\n\nexport AIRFLOW_HOME=${AIRFLOW_HOME:=${HOME}}\n\nmkdir \"${AIRFLOW_HOME}/sqlite\" -p || true\n\nASSET_COMPILATION_WAIT_MULTIPLIER=${ASSET_COMPILATION_WAIT_MULTIPLIER:=1}\n\n. \"${IN_CONTAINER_DIR}/check_connectivity.sh\"\n\nfunction wait_for_asset_compilation() {\n    if [[ -f \"${AIRFLOW_SOURCES}/.build/www/.asset_compile.lock\" ]]; then\n        echo\n        echo \"${COLOR_YELLOW}Waiting for asset compilation to complete in the background.${COLOR_RESET}\"\n        echo\n        local counter=0\n        while [[ -f \"${AIRFLOW_SOURCES}/.build/www/.asset_compile.lock\" ]]; do\n            if (( counter % 5 == 2 )); then\n                echo \"${COLOR_BLUE}Still waiting .....${COLOR_RESET}\"\n            fi\n            sleep 1\n            ((counter=counter+1))\n            if [[ ${counter} == 30*$ASSET_COMPILATION_WAIT_MULTIPLIER ]]; then\n                echo\n                echo \"${COLOR_YELLOW}The asset compilation is taking too long.${COLOR_YELLOW}\"\n                echo \"\"\"\nIf it does not complete soon, you might want to stop it and remove file lock:\n   * press Ctrl-C\n   * run 'rm ${AIRFLOW_SOURCES}/.build/www/.asset_compile.lock'\n\"\"\"\n            fi\n            if [[ ${counter} == 60*$ASSET_COMPILATION_WAIT_MULTIPLIER ]]; then\n                echo\n                echo \"${COLOR_RED}The asset compilation is taking too long. Exiting.${COLOR_RED}\"\n                echo \"${COLOR_RED}refer to dev/breeze/doc/04_troubleshooting.rst for resolution steps.${COLOR_RED}\"\n                echo\n                exit 1\n            fi\n        done\n    fi\n    if [ -f \"${AIRFLOW_SOURCES}/.build/www/asset_compile.out\" ]; then\n        echo\n        echo \"${COLOR_RED}The asset compilation failed. Exiting.${COLOR_RESET}\"\n        echo\n        cat \"${AIRFLOW_SOURCES}/.build/www/asset_compile.out\"\n        rm \"${AIRFLOW_SOURCES}/.build/www/asset_compile.out\"\n        echo\n        exit 1\n    fi\n}\n\nfunction environment_initialization() {\n    if [[ ${SKIP_ENVIRONMENT_INITIALIZATION=} == \"true\" ]]; then\n        return\n    fi\n    echo\n    echo \"${COLOR_BLUE}Running Initialization. Your basic configuration is:${COLOR_RESET}\"\n    echo\n    echo \"  * ${COLOR_BLUE}Airflow home:${COLOR_RESET} ${AIRFLOW_HOME}\"\n    echo \"  * ${COLOR_BLUE}Airflow sources:${COLOR_RESET} ${AIRFLOW_SOURCES}\"\n    echo \"  * ${COLOR_BLUE}Airflow core SQL connection:${COLOR_RESET} ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:=}\"\n    if [[ ${BACKEND=} == \"postgres\" ]]; then\n        echo \"  * ${COLOR_BLUE}Airflow backend:${COLOR_RESET} Postgres: ${POSTGRES_VERSION}\"\n    elif [[ ${BACKEND=} == \"mysql\" ]]; then\n        echo \"  * ${COLOR_BLUE}Airflow backend:${COLOR_RESET} MySQL: ${MYSQL_VERSION}\"\n    elif [[ ${BACKEND=} == \"sqlite\" ]]; then\n        echo \"  * ${COLOR_BLUE}Airflow backend:${COLOR_RESET} Sqlite\"\n    fi\n    echo\n\n    if [[ ${STANDALONE_DAG_PROCESSOR=} == \"true\" ]]; then\n        echo\n        echo \"${COLOR_BLUE}Running forcing scheduler/standalone_dag_processor to be True${COLOR_RESET}\"\n        echo\n        export AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR=True\n    fi\n\n    RUN_TESTS=${RUN_TESTS:=\"false\"}\n    CI=${CI:=\"false\"}\n\n    # Added to have run-tests on path\n    export PATH=${PATH}:${AIRFLOW_SOURCES}\n\n    # Directory where simple auth manager store generated passwords\n    export AIRFLOW_AUTH_MANAGER_CREDENTIAL_DIRECTORY=\"/files\"\n\n    mkdir -pv \"${AIRFLOW_HOME}/logs/\"\n\n    # Change the default worker_concurrency for tests\n    export AIRFLOW__CELERY__WORKER_CONCURRENCY=8\n\n    set +e\n\n    \"${IN_CONTAINER_DIR}/check_environment.sh\"\n    ENVIRONMENT_EXIT_CODE=$?\n    set -e\n    if [[ ${ENVIRONMENT_EXIT_CODE} != 0 ]]; then\n        echo\n        echo \"Error: check_environment returned ${ENVIRONMENT_EXIT_CODE}. Exiting.\"\n        echo\n        exit ${ENVIRONMENT_EXIT_CODE}\n    fi\n    mkdir -p /usr/lib/google-cloud-sdk/bin\n    touch /usr/lib/google-cloud-sdk/bin/gcloud\n    ln -s -f /usr/bin/gcloud /usr/lib/google-cloud-sdk/bin/gcloud\n\n    if [[ ${SKIP_SSH_SETUP=\"false\"} == \"false\" ]]; then\n        # Set up ssh keys\n        echo 'yes' | ssh-keygen -t rsa -C your_email@youremail.com -m PEM -P '' -f ~/.ssh/id_rsa \\\n            >\"${AIRFLOW_HOME}/logs/ssh-keygen.log\" 2>&1\n\n        cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n        ln -s -f ~/.ssh/authorized_keys ~/.ssh/authorized_keys2\n        chmod 600 ~/.ssh/*\n\n        # SSH Service\n        sudo service ssh restart >/dev/null 2>&1\n\n        # Sometimes the server is not quick enough to load the keys!\n        while [[ $(ssh-keyscan -H localhost 2>/dev/null | wc -l) != \"3\" ]] ; do\n            echo \"Not all keys yet loaded by the server\"\n            sleep 0.05\n        done\n\n        ssh-keyscan -H localhost >> ~/.ssh/known_hosts 2>/dev/null\n    fi\n\n    # shellcheck source=scripts/in_container/configure_environment.sh\n    . \"${IN_CONTAINER_DIR}/configure_environment.sh\"\n\n    # shellcheck source=scripts/in_container/run_init_script.sh\n    . \"${IN_CONTAINER_DIR}/run_init_script.sh\"\n\n    cd \"${AIRFLOW_SOURCES}\"\n\n    if [[ ${START_AIRFLOW:=\"false\"} == \"true\" || ${START_AIRFLOW} == \"True\" ]]; then\n        export AIRFLOW__CORE__LOAD_EXAMPLES=${LOAD_EXAMPLES}\n        wait_for_asset_compilation\n        # shellcheck source=scripts/in_container/bin/run_tmux\n        exec run_tmux\n    fi\n}\n\nfunction determine_airflow_to_use() {\n    USE_AIRFLOW_VERSION=\"${USE_AIRFLOW_VERSION:=\"\"}\"\n    if [[ ${USE_AIRFLOW_VERSION} == \"\" && ${USE_PACKAGES_FROM_DIST=} != \"true\" ]]; then\n        export PYTHONPATH=${AIRFLOW_SOURCES}\n        echo\n        echo \"${COLOR_BLUE}Using airflow version from current sources${COLOR_RESET}\"\n        echo\n        # Cleanup the logs, tmp when entering the environment\n        sudo rm -rf \"${AIRFLOW_SOURCES}\"/logs/*\n        sudo rm -rf \"${AIRFLOW_SOURCES}\"/tmp/*\n        mkdir -p \"${AIRFLOW_SOURCES}\"/logs/\n        mkdir -p \"${AIRFLOW_SOURCES}\"/tmp/\n    else\n        if [[ ${USE_AIRFLOW_VERSION} =~ 2\\.[7-8].* && ${TEST_TYPE} == \"Providers[fab]\" ]]; then\n            echo\n            echo \"${COLOR_YELLOW}Skipping FAB tests on Airflow 2.7 and 2.8 because of FAB incompatibility with them${COLOR_RESET}\"\n            echo\n            exit 0\n        fi\n        if [[ ${CLEAN_AIRFLOW_INSTALLATION=} == \"true\" ]]; then\n            echo\n            echo \"${COLOR_BLUE}Uninstalling all packages first${COLOR_RESET}\"\n            echo\n            # shellcheck disable=SC2086\n            ${PACKAGING_TOOL_CMD} freeze | grep -ve \"^-e\" | grep -ve \"^#\" | grep -ve \"^uv\" | \\\n                xargs ${PACKAGING_TOOL_CMD} uninstall ${EXTRA_UNINSTALL_FLAGS}\n            # Now install rich ad click first to use the installation script\n            # shellcheck disable=SC2086\n            ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} rich rich-click click --python \"/usr/local/bin/python\" \\\n                --constraint https://raw.githubusercontent.com/apache/airflow/constraints-main/constraints-${PYTHON_MAJOR_MINOR_VERSION}.txt\n        fi\n        python \"${IN_CONTAINER_DIR}/install_airflow_and_providers.py\"\n        echo\n        echo \"${COLOR_BLUE}Reinstalling all development dependencies${COLOR_RESET}\"\n        echo\n        python \"${IN_CONTAINER_DIR}/install_devel_deps.py\" \\\n           --constraint https://raw.githubusercontent.com/apache/airflow/constraints-main/constraints-${PYTHON_MAJOR_MINOR_VERSION}.txt\n        # Some packages might leave legacy typing module which causes test issues\n        # shellcheck disable=SC2086\n        ${PACKAGING_TOOL_CMD} uninstall ${EXTRA_UNINSTALL_FLAGS} typing || true\n        if [[ ${LINK_PROVIDERS_TO_AIRFLOW_PACKAGE=} == \"true\" ]]; then\n            echo\n            echo \"${COLOR_BLUE}Linking providers to airflow package as we are using them from mounted sources.${COLOR_RESET}\"\n            echo\n            rm -rf /usr/local/lib/python${PYTHON_MAJOR_MINOR_VERSION}/site-packages/airflow/providers\n            ln -s \"${AIRFLOW_SOURCES}/providers/src/airflow/providers\" \"/usr/local/lib/python${PYTHON_MAJOR_MINOR_VERSION}/site-packages/airflow/providers\"\n        fi\n    fi\n\n    if [[ \"${USE_AIRFLOW_VERSION}\" =~ ^2\\.2\\..*|^2\\.1\\..*|^2\\.0\\..* && \"${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=}\" != \"\" ]]; then\n        # make sure old variable is used for older airflow versions\n        export AIRFLOW__CORE__SQL_ALCHEMY_CONN=\"${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}\"\n    fi\n}\n\nfunction check_boto_upgrade() {\n    if [[ ${UPGRADE_BOTO=} != \"true\" ]]; then\n        return\n    fi\n    echo\n    echo \"${COLOR_BLUE}Upgrading boto3, botocore to latest version to run Amazon tests with them${COLOR_RESET}\"\n    echo\n    # shellcheck disable=SC2086\n    ${PACKAGING_TOOL_CMD} uninstall ${EXTRA_UNINSTALL_FLAGS} aiobotocore s3fs yandexcloud opensearch-py || true\n    # We need to include few dependencies to pass pip check with other dependencies:\n    #   * oss2 as dependency as otherwise jmespath will be bumped (sync with alibaba provider)\n    #   * cryptography is kept for snowflake-connector-python limitation (sync with snowflake provider)\n    #   * requests needs to be limited to be compatible with apache beam (sync with apache-beam provider)\n    #   * yandexcloud requirements for requests does not match those of apache.beam and latest botocore\n    #   Both requests and yandexcloud exclusion above might be removed after\n    #   https://github.com/apache/beam/issues/32080 is addressed\n    #   This is already addressed and planned for 2.59.0 release.\n    #   When you remove yandexcloud and opensearch from the above list, you can also remove the\n    #   optional providers_dependencies exclusions from \"test_example_dags.py\" in \"tests/always\".\n    set -x\n    # shellcheck disable=SC2086\n    ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} --upgrade boto3 botocore \\\n       \"oss2>=2.14.0\" \"cryptography<43.0.0\" \"requests!=2.32.*,<3.0.0,>=2.24.0\"\n    set +x\n    pip check\n}\n\nfunction check_downgrade_sqlalchemy() {\n    if [[ ${DOWNGRADE_SQLALCHEMY=} != \"true\" ]]; then\n        return\n    fi\n    min_sqlalchemy_version=$(grep \"\\\"sqlalchemy>=\" hatch_build.py | sed \"s/.*>=\\([0-9\\.]*\\).*/\\1/\" | xargs)\n    echo\n    echo \"${COLOR_BLUE}Downgrading sqlalchemy to minimum supported version: ${min_sqlalchemy_version}${COLOR_RESET}\"\n    echo\n    # shellcheck disable=SC2086\n    ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} \"sqlalchemy==${min_sqlalchemy_version}\"\n    pip check\n}\n\nfunction check_downgrade_pendulum() {\n    if [[ ${DOWNGRADE_PENDULUM=} != \"true\" || ${PYTHON_MAJOR_MINOR_VERSION} == \"3.12\" ]]; then\n        return\n    fi\n    local MIN_PENDULUM_VERSION=\"2.1.2\"\n    echo\n    echo \"${COLOR_BLUE}Downgrading pendulum to minimum supported version: ${MIN_PENDULUM_VERSION}${COLOR_RESET}\"\n    echo\n    # shellcheck disable=SC2086\n    ${PACKAGING_TOOL_CMD} install ${EXTRA_INSTALL_FLAGS} \"pendulum==${MIN_PENDULUM_VERSION}\"\n    pip check\n}\n\nfunction check_run_tests() {\n    if [[ ${RUN_TESTS=} != \"true\" ]]; then\n        return\n    fi\n\n    if [[ ${REMOVE_ARM_PACKAGES:=\"false\"} == \"true\" ]]; then\n        # Test what happens if we do not have ARM packages installed.\n        # This is useful to see if pytest collection works without ARM packages which is important\n        # for the MacOS M1 users running tests in their ARM machines with `breeze testing *-tests` command\n        python \"${IN_CONTAINER_DIR}/remove_arm_packages.py\"\n    fi\n\n    if [[ ${TEST_GROUP:=\"\"} == \"system\" ]]; then\n        exec \"${IN_CONTAINER_DIR}/run_system_tests.sh\" \"${@}\"\n    else\n        exec \"${IN_CONTAINER_DIR}/run_ci_tests.sh\" \"${@}\"\n    fi\n}\n\nfunction check_force_lowest_dependencies() {\n    if [[ ${FORCE_LOWEST_DEPENDENCIES=} != \"true\" ]]; then\n        return\n    fi\n    export EXTRA=\"\"\n    if [[ ${TEST_TYPE=} =~ Providers\\[.*\\] ]]; then\n        # shellcheck disable=SC2001\n        EXTRA=$(echo \"[${TEST_TYPE}]\" | sed 's/Providers\\[\\(.*\\)\\]/\\1/' | sed 's/\\./-/')\n        export EXTRA\n        echo\n        echo \"${COLOR_BLUE}Forcing dependencies to lowest versions for provider: ${EXTRA}${COLOR_RESET}\"\n        echo\n        if ! /opt/airflow/scripts/in_container/is_provider_excluded.py; then\n            echo\n            echo \"Skipping ${EXTRA} provider check on Python ${PYTHON_MAJOR_MINOR_VERSION}!\"\n            echo\n            exit 0\n        fi\n    else\n        echo\n        echo \"${COLOR_BLUE}Forcing dependencies to lowest versions for Airflow.${COLOR_RESET}\"\n        echo\n    fi\n    set -x\n    uv pip install --python \"$(which python)\" --resolution lowest-direct --upgrade --editable \".${EXTRA}\" --editable \"./task_sdk\"\n    set +x\n}\n\nfunction check_airflow_python_client_installation() {\n    if [[ ${INSTALL_AIRFLOW_PYTHON_CLIENT=} != \"true\" ]]; then\n        return\n    fi\n    python \"${IN_CONTAINER_DIR}/install_airflow_python_client.py\"\n}\n\nfunction start_webserver_with_examples(){\n    if [[ ${START_WEBSERVER_WITH_EXAMPLES=} != \"true\" ]]; then\n        return\n    fi\n    export AIRFLOW__CORE__LOAD_EXAMPLES=True\n    export AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.session,airflow.providers.fab.auth_manager.api.auth.backend.basic_auth\n    export AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True\n    echo\n    echo \"${COLOR_BLUE}Initializing database${COLOR_RESET}\"\n    echo\n    airflow db migrate\n    echo\n    echo \"${COLOR_BLUE}Database initialized${COLOR_RESET}\"\n    echo\n    echo \"${COLOR_BLUE}Parsing example dags${COLOR_RESET}\"\n    echo\n    airflow scheduler --num-runs 100\n    echo \"Example dags parsing finished\"\n    echo \"Create admin user\"\n    airflow users create -u admin -p admin -f Thor -l Administrator -r Admin -e admin@email.domain\n    echo \"Admin user created\"\n    echo\n    echo \"${COLOR_BLUE}Starting airflow webserver${COLOR_RESET}\"\n    echo\n    airflow webserver --port 8080 --daemon\n    echo\n    echo \"${COLOR_BLUE}Waiting for webserver to start${COLOR_RESET}\"\n    echo\n    check_service_connection \"Airflow webserver\" \"run_nc localhost 8080\" 100\n    EXIT_CODE=$?\n    if [[ ${EXIT_CODE} != 0 ]]; then\n        echo\n        echo \"${COLOR_RED}Webserver did not start properly${COLOR_RESET}\"\n        echo\n        exit ${EXIT_CODE}\n    fi\n    echo\n    echo \"${COLOR_BLUE}Airflow webserver started${COLOR_RESET}\"\n}\n\ndetermine_airflow_to_use\nenvironment_initialization\ncheck_boto_upgrade\ncheck_downgrade_sqlalchemy\ncheck_downgrade_pendulum\ncheck_force_lowest_dependencies\ncheck_airflow_python_client_installation\nstart_webserver_with_examples\ncheck_run_tests \"${@}\"\n\nexec /bin/bash \"${@}\"\nEOF\n\n# The content below is automatically copied from scripts/docker/entrypoint_exec.sh\nCOPY <<\"EOF\" /entrypoint_exec.sh\n#!/usr/bin/env bash\n. /opt/airflow/scripts/in_container/_in_container_script_init.sh\n\n. /opt/airflow/scripts/in_container/configure_environment.sh\n\n. /opt/airflow/scripts/in_container/run_init_script.sh\n\nexec /bin/bash \"${@}\"\nEOF\n\nFROM ${PYTHON_BASE_IMAGE} as main\n\n# Nolog bash flag is currently ignored - but you can replace it with other flags (for example\n# xtrace - to show commands executed)\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"nounset\", \"-o\", \"nolog\", \"-c\"]\n\nARG PYTHON_BASE_IMAGE\nARG AIRFLOW_IMAGE_REPOSITORY=\"https://github.com/apache/airflow\"\n\n# By increasing this number we can do force build of all dependencies.\n# NOTE! When you want to make sure dependencies are installed from scratch in your PR after removing\n# some dependencies, you also need to set \"disable image cache\" in your PR to make sure the image is\n# not built using the \"main\" version of those dependencies.\nARG DEPENDENCIES_EPOCH_NUMBER=\"14\"\n\n# Make sure noninteractive debian install is used and language variables set\nENV PYTHON_BASE_IMAGE=${PYTHON_BASE_IMAGE} \\\n    DEBIAN_FRONTEND=noninteractive LANGUAGE=C.UTF-8 LANG=C.UTF-8 LC_ALL=C.UTF-8 \\\n    LC_CTYPE=C.UTF-8 LC_MESSAGES=C.UTF-8 \\\n    DEPENDENCIES_EPOCH_NUMBER=${DEPENDENCIES_EPOCH_NUMBER} \\\n    INSTALL_MYSQL_CLIENT=\"true\" \\\n    INSTALL_MSSQL_CLIENT=\"true\" \\\n    INSTALL_POSTGRES_CLIENT=\"true\" \\\n    PIP_CACHE_DIR=/root/.cache/pip \\\n    UV_CACHE_DIR=/root/.cache/uv\n\n\nRUN echo \"Base image version: ${PYTHON_BASE_IMAGE}\"\n\nARG DEV_APT_COMMAND=\"\"\nARG ADDITIONAL_DEV_APT_COMMAND=\"\"\nARG ADDITIONAL_DEV_ENV_VARS=\"\"\nARG ADDITIONAL_DEV_APT_DEPS=\"bash-completion dumb-init git graphviz krb5-user \\\nless libenchant-2-2 libgcc-11-dev libgeos-dev libpq-dev net-tools netcat-openbsd \\\nopenssh-server postgresql-client software-properties-common rsync tmux unzip vim xxd\"\n\nARG ADDITIONAL_DEV_APT_ENV=\"\"\n\nENV DEV_APT_COMMAND=${DEV_APT_COMMAND} \\\n    ADDITIONAL_DEV_APT_DEPS=${ADDITIONAL_DEV_APT_DEPS} \\\n    ADDITIONAL_DEV_APT_COMMAND=${ADDITIONAL_DEV_APT_COMMAND}\n\nCOPY --from=scripts install_os_dependencies.sh /scripts/docker/\nRUN bash /scripts/docker/install_os_dependencies.sh dev\n\nCOPY --from=scripts common.sh /scripts/docker/\n\n# Only copy mysql/mssql installation scripts for now - so that changing the other\n# scripts which are needed much later will not invalidate the docker layer here.\nCOPY --from=scripts install_mysql.sh install_mssql.sh install_postgres.sh /scripts/docker/\n\nARG HOME=/root\nARG AIRFLOW_HOME=/root/airflow\nARG AIRFLOW_SOURCES=/opt/airflow\nARG INSTALL_MYSQL_CLIENT_TYPE=\"mysql\"\n\nENV HOME=${HOME} \\\n    AIRFLOW_HOME=${AIRFLOW_HOME} \\\n    AIRFLOW_SOURCES=${AIRFLOW_SOURCES} \\\n    INSTALL_MYSQL_CLIENT_TYPE=${INSTALL_MYSQL_CLIENT_TYPE}\n\n# We run scripts with bash here to make sure we can execute the scripts. Changing to +x might have an\n# unexpected result - the cache for Dockerfiles might get invalidated in case the host system\n# had different umask set and group x bit was not set. In Azure the bit might be not set at all.\n# That also protects against AUFS Docker backend problem where changing the executable bit required sync\nRUN bash /scripts/docker/install_mysql.sh prod \\\n    && bash /scripts/docker/install_mysql.sh dev \\\n    && bash /scripts/docker/install_mssql.sh dev \\\n    && bash /scripts/docker/install_postgres.sh dev \\\n    # The user is added to allow ssh debugging (you can connect with airflow/airflow over ssh)\n    && adduser --gecos \"First Last,RoomNumber,WorkPhone,HomePhone\" --disabled-password \\\n              --quiet \"airflow\" --home \"/home/airflow\" \\\n    && echo -e \"airflow\\nairflow\" | passwd airflow 2>&1 \\\n    && echo \"airflow ALL=(ALL) NOPASSWD: ALL\" > /etc/sudoers.d/airflow \\\n    && chmod 0440 /etc/sudoers.d/airflow\n\n# Install Helm\nARG HELM_VERSION=\"v3.16.4\"\n\nRUN SYSTEM=$(uname -s | tr '[:upper:]' '[:lower:]') \\\n    && PLATFORM=$([ \"$(uname -m)\" = \"aarch64\" ] && echo \"arm64\" || echo \"amd64\" ) \\\n    && HELM_URL=\"https://get.helm.sh/helm-${HELM_VERSION}-${SYSTEM}-${PLATFORM}.tar.gz\" \\\n    && curl --silent --location \"${HELM_URL}\" | tar -xz -O \"${SYSTEM}-${PLATFORM}/helm\" > /usr/local/bin/helm \\\n    && chmod +x /usr/local/bin/helm\n\nWORKDIR ${AIRFLOW_SOURCES}\n\nRUN mkdir -pv ${AIRFLOW_HOME} && \\\n    mkdir -pv ${AIRFLOW_HOME}/dags && \\\n    mkdir -pv ${AIRFLOW_HOME}/logs\n\nARG AIRFLOW_REPO=apache/airflow\nARG AIRFLOW_BRANCH=main\n# Airflow Extras installed\nARG AIRFLOW_EXTRAS=\"devel-ci\"\nARG ADDITIONAL_AIRFLOW_EXTRAS=\"\"\n# Allows to override constraints source\nARG CONSTRAINTS_GITHUB_REPOSITORY=\"apache/airflow\"\nARG AIRFLOW_CONSTRAINTS_MODE=\"constraints-source-providers\"\nARG AIRFLOW_CONSTRAINTS_REFERENCE=\"\"\nARG AIRFLOW_CONSTRAINTS_LOCATION=\"\"\nARG DEFAULT_CONSTRAINTS_BRANCH=\"constraints-main\"\n# By changing the epoch we can force reinstalling Airflow and pip all dependencies\n# It can also be overwritten manually by setting the AIRFLOW_CI_BUILD_EPOCH environment variable.\nARG AIRFLOW_CI_BUILD_EPOCH=\"10\"\n# Setup PIP\nARG UV_HTTP_TIMEOUT=\"300\"\n# By default PIP has progress bar but you can disable it.\nARG PIP_PROGRESS_BAR=\"on\"\n# Optimizing installation of Cassandra driver (in case there are no prebuilt wheels which is the\n# case as of 20.04.2021 with Python 3.9\n# Speeds up building the image - cassandra driver without CYTHON saves around 10 minutes\nARG CASS_DRIVER_NO_CYTHON=\"1\"\n# Build cassandra driver on multiple CPUs\nARG CASS_DRIVER_BUILD_CONCURRENCY=\"8\"\n\n# This value should be set by the CI image build system to the current timestamp\nARG AIRFLOW_VERSION=\"\"\n\n# Additional PIP flags passed to all pip install commands except reinstalling pip itself\nARG ADDITIONAL_PIP_INSTALL_FLAGS=\"\"\n\nARG AIRFLOW_USE_UV=\"true\"\n\nENV AIRFLOW_REPO=${AIRFLOW_REPO}\\\n    AIRFLOW_BRANCH=${AIRFLOW_BRANCH} \\\n    AIRFLOW_EXTRAS=${AIRFLOW_EXTRAS}${ADDITIONAL_AIRFLOW_EXTRAS:+,}${ADDITIONAL_AIRFLOW_EXTRAS} \\\n    CONSTRAINTS_GITHUB_REPOSITORY=${CONSTRAINTS_GITHUB_REPOSITORY} \\\n    AIRFLOW_CONSTRAINTS_MODE=${AIRFLOW_CONSTRAINTS_MODE} \\\n    AIRFLOW_CONSTRAINTS_REFERENCE=${AIRFLOW_CONSTRAINTS_REFERENCE} \\\n    AIRFLOW_CONSTRAINTS_LOCATION=${AIRFLOW_CONSTRAINTS_LOCATION} \\\n    DEFAULT_CONSTRAINTS_BRANCH=${DEFAULT_CONSTRAINTS_BRANCH} \\\n    AIRFLOW_CI_BUILD_EPOCH=${AIRFLOW_CI_BUILD_EPOCH} \\\n    AIRFLOW_VERSION=${AIRFLOW_VERSION} \\\n    AIRFLOW_PIP_VERSION=${AIRFLOW_PIP_VERSION} \\\n    AIRFLOW_UV_VERSION=${AIRFLOW_UV_VERSION} \\\n    AIRFLOW_USE_UV=${AIRFLOW_USE_UV} \\\n    UV_HTTP_TIMEOUT=${UV_HTTP_TIMEOUT} \\\n    INSTALL_MYSQL_CLIENT=\"true\" \\\n    INSTALL_MYSQL_CLIENT_TYPE=${INSTALL_MYSQL_CLIENT_TYPE} \\\n    INSTALL_MSSQL_CLIENT=\"true\" \\\n    INSTALL_POSTGRES_CLIENT=\"true\" \\\n    AIRFLOW_INSTALLATION_METHOD=\".\" \\\n    AIRFLOW_VERSION_SPECIFICATION=\"\" \\\n    PIP_PROGRESS_BAR=${PIP_PROGRESS_BAR} \\\n    ADDITIONAL_PIP_INSTALL_FLAGS=${ADDITIONAL_PIP_INSTALL_FLAGS} \\\n    CASS_DRIVER_BUILD_CONCURRENCY=${CASS_DRIVER_BUILD_CONCURRENCY} \\\n    CASS_DRIVER_NO_CYTHON=${CASS_DRIVER_NO_CYTHON}\n\nRUN echo \"Airflow version: ${AIRFLOW_VERSION}\"\n\n# Copy all scripts required for installation - changing any of those should lead to\n# rebuilding from here\nCOPY --from=scripts common.sh install_packaging_tools.sh install_additional_dependencies.sh /scripts/docker/\n\n# We are first creating a venv where all python packages and .so binaries needed by those are\n# installed.\n\n# Here we fix the versions so all subsequent commands will use the versions\n# from the sources\n# You can swap comments between those two args to test pip from the main version\n# When you attempt to test if the version of `pip` from specified branch works for our builds\n# Also use `force pip` label on your PR to swap all places we use `uv` to `pip`\nARG AIRFLOW_PIP_VERSION=24.3.1\n# ARG AIRFLOW_PIP_VERSION=\"git+https://github.com/pypa/pip.git@main\"\nARG AIRFLOW_UV_VERSION=0.5.14\n# TODO(potiuk): automate with upgrade check (possibly)\nARG AIRFLOW_PRE_COMMIT_VERSION=\"4.0.1\"\nARG AIRFLOW_PRE_COMMIT_UV_VERSION=\"4.1.4\"\n\nENV AIRFLOW_PIP_VERSION=${AIRFLOW_PIP_VERSION} \\\n    AIRFLOW_UV_VERSION=${AIRFLOW_UV_VERSION} \\\n    # This is needed since we are using cache mounted from the host\n    UV_LINK_MODE=copy \\\n    AIRFLOW_PRE_COMMIT_VERSION=${AIRFLOW_PRE_COMMIT_VERSION}\n\n# The PATH is needed for PIPX to find the tools installed\nENV PATH=\"/root/.local/bin:${PATH}\"\n\n# Useful for creating a cache id based on the underlying architecture, preventing the use of cached python packages from\n# an incorrect architecture.\nARG TARGETARCH\n# Value to be able to easily change cache id and therefore use a bare new cache\nARG DEPENDENCY_CACHE_EPOCH=\"0\"\n\n# Install useful command line tools in their own virtualenv so that they do not clash with\n# dependencies installed in Airflow also reinstall PIP and UV to make sure they are installed\n# in the version specified above\nRUN bash /scripts/docker/install_packaging_tools.sh\n\nCOPY --from=scripts install_airflow.sh /scripts/docker/\n\n# We can copy everything here. The Context is filtered by dockerignore. This makes sure we are not\n# copying over stuff that is accidentally generated or that we do not need (such as egg-info)\n# if you want to add something that is missing and you expect to see it in the image you can\n# add it with ! in .dockerignore next to the airflow, test etc. directories there\nCOPY . ${AIRFLOW_SOURCES}/\n\n# Those are additional constraints that are needed for some extras but we do not want to\n# force them on the main Airflow package. Currently we need no extra limits as PIP 23.1+ has much better\n# dependency resolution and we do not need to limit the versions of the dependencies\n#\nARG EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=\"\"\nARG UPGRADE_INVALIDATION_STRING=\"\"\nARG VERSION_SUFFIX_FOR_PYPI=\"\"\n\nENV EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=${EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS} \\\n    UPGRADE_INVALIDATION_STRING=${UPGRADE_INVALIDATION_STRING} \\\n    VERSION_SUFFIX_FOR_PYPI=${VERSION_SUFFIX_FOR_PYPI}\n\n# The goal of this line is to install the dependencies from the most current pyproject.toml from sources\n# This will be usually incremental small set of packages in CI optimized build, so it will be very fast\n# In non-CI optimized build this will install all dependencies before installing sources.\n# Usually we will install versions based on the dependencies in pyproject.toml and upgraded only if needed.\n# But in cron job we will install latest versions matching pyproject.toml to see if there is no breaking change\n# and push the constraints if everything is successful\nRUN --mount=type=cache,id=ci-$TARGETARCH-$DEPENDENCY_CACHE_EPOCH,target=/root/.cache/ bash /scripts/docker/install_airflow.sh\n\nCOPY --from=scripts install_packaging_tools.sh install_additional_dependencies.sh /scripts/docker/\n\nARG ADDITIONAL_PYTHON_DEPS=\"\"\n\nENV ADDITIONAL_PYTHON_DEPS=${ADDITIONAL_PYTHON_DEPS}\n\nRUN --mount=type=cache,id=ci-$TARGETARCH-$DEPENDENCY_CACHE_EPOCH,target=/root/.cache/ \\\n    bash /scripts/docker/install_packaging_tools.sh; \\\n    if [[ -n \"${ADDITIONAL_PYTHON_DEPS}\" ]]; then \\\n        bash /scripts/docker/install_additional_dependencies.sh; \\\n    fi\n\nCOPY --from=scripts entrypoint_ci.sh /entrypoint\nCOPY --from=scripts entrypoint_exec.sh /entrypoint-exec\nRUN chmod a+x /entrypoint /entrypoint-exec\n\n\n# Install autocomplete for airflow and kubectl\nRUN if command -v airflow; then \\\n        register-python-argcomplete airflow >> ~/.bashrc ; \\\n    fi; \\\n    echo \"source /etc/bash_completion\" >> ~/.bashrc\n\nWORKDIR ${AIRFLOW_SOURCES}\n\nARG BUILD_ID\nARG COMMIT_SHA\nARG AIRFLOW_IMAGE_DATE_CREATED\n\nENV PATH=\"/files/bin/:/opt/airflow/scripts/in_container/bin/:${PATH}\" \\\n    GUNICORN_CMD_ARGS=\"--worker-tmp-dir /dev/shm/\" \\\n    BUILD_ID=${BUILD_ID} \\\n    COMMIT_SHA=${COMMIT_SHA} \\\n    # When we enter the image, the /root/.cache is not mounted from temporary mount cache.\n    # We do not want to share the cache from host to avoid all kinds of problems where cache\n    # is different with different platforms / python versions. We want to have a clean cache\n    # in the image - and in this case /root/.cache is on the same filesystem as the installed packages.\n    # so we can go back to the default link mode being hardlink.\n    UV_LINK_MODE=hardlink\n\n# Link dumb-init for backwards compatibility (so that older images also work)\nRUN ln -sf /usr/bin/dumb-init /usr/local/bin/dumb-init\n\nEXPOSE 8080\n\nLABEL org.apache.airflow.distro=\"debian\" \\\n  org.apache.airflow.module=\"airflow\" \\\n  org.apache.airflow.component=\"airflow\" \\\n  org.apache.airflow.image=\"airflow-ci\" \\\n  org.apache.airflow.version=\"${AIRFLOW_VERSION}\" \\\n  org.apache.airflow.uid=\"0\" \\\n  org.apache.airflow.gid=\"0\" \\\n  org.apache.airflow.build-id=\"${BUILD_ID}\" \\\n  org.apache.airflow.commit-sha=\"${COMMIT_SHA}\" \\\n  org.opencontainers.image.source=\"${AIRFLOW_IMAGE_REPOSITORY}\" \\\n  org.opencontainers.image.created=\"${AIRFLOW_IMAGE_DATE_CREATED}\" \\\n  org.opencontainers.image.authors=\"dev@airflow.apache.org\" \\\n  org.opencontainers.image.url=\"https://airflow.apache.org\" \\\n  org.opencontainers.image.documentation=\"https://airflow.apache.org/docs/docker-stack/index.html\" \\\n  org.opencontainers.image.source=\"https://github.com/apache/airflow\" \\\n  org.opencontainers.image.version=\"${AIRFLOW_VERSION}\" \\\n  org.opencontainers.image.revision=\"${COMMIT_SHA}\" \\\n  org.opencontainers.image.vendor=\"Apache Software Foundation\" \\\n  org.opencontainers.image.licenses=\"Apache-2.0\" \\\n  org.opencontainers.image.ref.name=\"airflow-ci-image\" \\\n  org.opencontainers.image.title=\"Continuous Integration Airflow Image\" \\\n  org.opencontainers.image.description=\"Installed Apache Airflow with Continuous Integration dependencies\"\n\nENTRYPOINT [\"/usr/bin/dumb-init\", \"--\", \"/entrypoint\"]\nCMD []\n"
        },
        {
          "name": "INSTALL",
          "type": "blob",
          "size": 13.759765625,
          "content": "INSTALL / BUILD instructions for Apache Airflow\n\nBasic installation of Airflow from sources and development environment setup\n============================================================================\n\nThis is a generic installation method that requires minimum standard tools to develop Airflow and\ntest it in a local virtual environment (using standard CPython installation and `pip`).\n\nDepending on your system, you might need different prerequisites, but the following\nsystems/prerequisites are known to work:\n\nLinux (Debian Bookworm):\n\n    sudo apt install -y --no-install-recommends apt-transport-https apt-utils ca-certificates \\\n    curl dumb-init freetds-bin krb5-user libgeos-dev \\\n    ldap-utils libsasl2-2 libsasl2-modules libxmlsec1 locales libffi8 libldap-2.5-0 libssl3 netcat-openbsd \\\n    lsb-release openssh-client python3-selinux rsync sasl2-bin sqlite3 sudo unixodbc\n\nYou might need to install MariaDB development headers to build some of the dependencies\n\n    sudo apt-get install libmariadb-dev libmariadbclient-dev\n\nMacOS (Mojave/Catalina) you might need to install XCode command line tools and brew and those packages:\n\n    brew install sqlite mysql postgresql\n\nThe `pip` is one of the build packaging front-ends that might be used to install Airflow. It's the one\nthat we recommend (see below) for reproducible installation of specific versions of Airflow.\n\nAs of version 2.8, Airflow follows PEP 517/518 and uses `pyproject.toml` file to define build dependencies\nand build process, and it requires relatively modern versions of packaging tools to get airflow built from\nlocal sources or sdist packages, as PEP 517 compliant build hooks are used to determine dynamic build\ndependencies. In the case of `pip` it means that at least version 22.1.0 is needed (released at the beginning of\n2022) to build or install Airflow from sources. This does not affect the ability to install Airflow from\nreleased wheel packages.\n\nDownloading and installing Airflow from sources\n-----------------------------------------------\n\nWhile you can get Airflow sources in various ways (including cloning https://github.com/apache/airflow/), the\ncanonical way to download it is to fetch the tarball (published at https://downloads.apache.org), after\nverifying the checksum and signatures of the downloaded file.\n\nWhen you download source packages from https://downloads.apache.org, you download sources of Airflow and\nall providers separately. However, when you clone the GitHub repository at https://github.com/apache/airflow/\nyou get all sources in one place. This is the most convenient way to develop Airflow and Providers together.\nOtherwise, you have to install Airflow and Providers separately from sources in the same environment, which\nis not as convenient.\n\nCreating virtualenv\n-------------------\n\nAirflow pulls in quite a lot of dependencies to connect to other services. You generally want to\ntest or run Airflow from a virtualenv to ensure those dependencies are separated from your system-wide versions. Using system-installed Python installation is strongly discouraged as the versions of Python\nshipped with the operating system often have some limitations and are not up to date. It is recommended to install Python using the official release (https://www.python.org/downloads/), or Python project management tools such as Hatch. See later\nfor a description of `Hatch` as one of the tools that is Airflow's tool of choice to build Airflow packages.\n\nOnce you have a suitable Python version installed, you can create a virtualenv and activate it:\n\n    python3 -m venv PATH_TO_YOUR_VENV\n    source PATH_TO_YOUR_VENV/bin/activate\n\nInstalling Airflow locally\n--------------------------\n\nInstalling Airflow locally can be done using pip - note that this will install \"development\" version of\nAirflow, where all providers are installed from local sources (if available), not from `pypi`.\nIt will also not include pre-installed providers installed from PyPI. If you install from sources of\njust Airflow, you need to install separately each provider you want to develop. If you install\nfrom the GitHub repository, all the current providers are available after installing Airflow.\n\n    pip install .\n\nIf you develop Airflow and iterate on it, you should install it in editable mode (with -e) flag, and then\nyou do not need to re-install it after each change to sources. This is useful if you want to develop and\niterate on Airflow and Providers (together) if you install sources from the cloned GitHub repository.\n\nNote that you might want to install `devel` extra when you install airflow for development in editable env\nthis one contains the minimum set of tools and dependencies needed to run unit tests.\n\n\n    pip install -e \".[devel]\"\n\nYou can also install optional packages that are needed to run certain tests. In case of local installation\nfor example, you can install all prerequisites for Google provider, tests, and\nall Hadoop providers with this command:\n\n    pip install -e \".[google,devel-tests,devel-hadoop]\"\n\n\nor you can install all packages needed to run tests for core, providers, and all extensions of airflow:\n\n    pip install -e \".[devel-all]\"\n\nYou can see the list of all available extras below.\n\nAdditionally when you want to develop providers you need to install providers code in editable mode:\n\n    pip install -e \"./providers\"\n\nUsing ``uv`` to manage your Python, virtualenvs, and install airflow for development\n====================================================================================\n\nWhile you can manually install airflow locally from sources, Airflow committers recommend using\n[uv](https://docs.astral.sh/uv/) as a build and development tool. It is a modern,\nrecently introduced popular packaging front-end tool and environment managers for Python.\nIt is an optional tool that is only really needed when you want to build packages from sources, you can use\nmany other packaging frontends (for example ``hatch``) but ``uv`` is very fast and convenient to manage\nalso your Python versions and virtualenvs. Also we use ``\n\nInstalling ``uv``\n-----------------\n\nYou can install uv following [the instructions](https://docs.astral.sh/uv/getting-started/installation/ on\n\nUsing ``uv`` to manage your virtualenvs\n---------------------------------------\n\nYou can create a virtualenv with ``uv`` using the following command:\n\n    uv venv create\n\nYou can sync to latest versions of airflow dependencies using:\n\n    uv sync\n\nAnd if you want to use some extras (for example because you want to develop providers) you can add\nextras to the command:\n\n    uv sync --extra devel\n\nYou can also synchronize all extras:\n\n    uv sync --all-extras\n\nBuilding airflow packages with Hatch\n====================================\n\nWhile building packages will work with any compliant packaging front-end tool, for reproducibility, we\nrecommend using ``hatch``. It is a modern, fast, and convenient tool to build packages from sources managed\nby the Python Packaging Authority. It is also used by Airflow to build packages in CI/CD as well as by\nrelease managers to build locally packages for verification of reproducibility of the build.\n\nInstalling ``hatch``\n--------------------\n\nMore information about hatch can be found in https://hatch.pypa.io/\n\nWe recommend to install ``hatch`` using ```uv tool`` command which will make hatch available as a CLI\ncommand globally:\n\n    uv tool install hatch\n\nYou can still install ``hatch`` using ``pipx`` if you prefer:\n\n    pipx install hatch\n\n\nIt's important to keep your hatch up to date. You can do this by running:\n\n    uv tool upgrade hatch\n\n\nUsing Hatch to build your packages\n----------------------------------\n\nYou can use Hatch to build installable packages from the Airflow sources. Such package will\ninclude all metadata configured in `pyproject.toml` and will be installable with ``pip`` and and any other\nPEP-compliant packaging front-end.\n\nThe packages will have pre-installed dependencies for providers that are available when Airflow is i\nonstalled from PyPI. Both `wheel` and `sdist` packages are built by default.\n\n    hatch build\n\nYou can also build only `wheel` or `sdist` packages:\n\n    hatch build -t wheel\n    hatch build -t sdist\n\nInstalling recommended version of dependencies\n==============================================\n\nWhatever virtualenv solution you use, when you want to make sure you are using the same\nversion of dependencies as in main, you can install the recommended version of the dependencies by using\nconstraint-python<PYTHON_MAJOR_MINOR_VERSION>.txt files as `constraint` file. This might be useful\nto avoid \"works-for-me\" syndrome, where you use different versions of dependencies than the ones\nthat are used in main CI tests and by other contributors.\n\nThere are different constraint files for different Python versions. For example, this command will install\nall basic devel requirements and requirements of Google provider as last successfully tested for Python 3.9:\n\n    pip install -e \".[devel,google]\"\" \\\n      --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-main/constraints-3.9.txt\"\n\nUsing the 'constraints-no-providers' constraint files, you can upgrade Airflow without paying attention to the provider's dependencies. This allows you to keep installed provider dependencies and install the latest supported ones using pure Airflow core.\n\npip install -e \".[devel]\" \\\n  --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-main/constraints-no-providers-3.9.txt\"\n\nAirflow extras\n==============\n\nAirflow has several extras that you can install to get additional dependencies. They sometimes install\nproviders, sometimes enable other features where packages are not installed by default.\n\nYou can read more about those extras in the extras reference:\nhttps://airflow.apache.org/docs/apache-airflow/stable/extra-packages-ref.html\n\nThe list of available extras is below.\n\nCore extras\n-----------\n\nThose extras are available as regular core airflow extras - they install optional features of Airflow.\n\n# START CORE EXTRAS HERE\n\naiobotocore, apache-atlas, apache-webhdfs, async, cgroups, cloudpickle, github-enterprise, google-\nauth, graphviz, kerberos, ldap, leveldb, otel, pandas, password, rabbitmq, s3fs, sentry, statsd, uv\n\n# END CORE EXTRAS HERE\n\nProvider extras\n---------------\n\nThose extras are available as regular Airflow extras; they install provider packages in standard builds\nor dependencies that are necessary to enable the feature in an editable build.\n\n# START PROVIDER EXTRAS HERE\n\nairbyte, alibaba, amazon, apache.beam, apache.cassandra, apache.drill, apache.druid, apache.flink,\napache.hdfs, apache.hive, apache.iceberg, apache.impala, apache.kafka, apache.kylin, apache.livy,\napache.pig, apache.pinot, apache.spark, apprise, arangodb, asana, atlassian.jira, celery, cloudant,\ncncf.kubernetes, cohere, common.compat, common.io, common.sql, databricks, datadog, dbt.cloud,\ndingding, discord, docker, edge, elasticsearch, exasol, fab, facebook, ftp, github, google, grpc,\nhashicorp, http, imap, influxdb, jdbc, jenkins, microsoft.azure, microsoft.mssql, microsoft.psrp,\nmicrosoft.winrm, mongo, mysql, neo4j, odbc, openai, openfaas, openlineage, opensearch, opsgenie,\noracle, pagerduty, papermill, pgvector, pinecone, postgres, presto, qdrant, redis, salesforce,\nsamba, segment, sendgrid, sftp, singularity, slack, smtp, snowflake, sqlite, ssh, standard, tableau,\ntelegram, teradata, trino, vertica, weaviate, yandex, ydb, zendesk\n\n# END PROVIDER EXTRAS HERE\n\nDevel extras\n------------\n\nThe `devel` extras are not available in the released packages. They are only available when you install\nAirflow from sources in `editable` installation - i.e., one that you are usually using to contribute to\nAirflow. They provide tools like `pytest` and `mypy` for general-purpose development and testing.\n\n# START DEVEL EXTRAS HERE\n\ndevel, devel-all-dbs, devel-ci, devel-debuggers, devel-devscripts, devel-duckdb, devel-hadoop,\ndevel-mypy, devel-sentry, devel-static-checks, devel-tests\n\n# END DEVEL EXTRAS HERE\n\nBundle extras\n-------------\n\nThose extras are bundles dynamically generated from other extras.\n\n# START BUNDLE EXTRAS HERE\n\nall, all-core, all-dbs, devel-all, devel-ci\n\n# END BUNDLE EXTRAS HERE\n\n\nDoc extras\n----------\n\nDoc extras are used to install dependencies that are needed to build documentation. Only available during\neditable install.\n\n# START DOC EXTRAS HERE\n\ndoc, doc-gen\n\n# END DOC EXTRAS HERE\n\nDeprecated extras\n-----------------\n\nThe deprecated extras are from Airflow 1 and will be removed in future versions.\n\n# START DEPRECATED EXTRAS HERE\n\natlas, aws, azure, cassandra, crypto, druid, gcp, gcp-api, hdfs, hive, kubernetes, mssql, pinot, s3,\nspark, webhdfs, winrm\n\n# END DEPRECATED EXTRAS HERE\n\nCompiling front-end assets\n--------------------------\n\nSometimes, you can see that front-end assets are missing, and the website looks broken. This is because\nyou need to compile front-end assets. This is done automatically when you create a virtualenv\nwith hatch, but if you want to do it manually, you can do it after installing node and yarn and running:\n\n    yarn install --frozen-lockfile\n    yarn run build\n\nCurrently, we are running yarn coming with note 18.6.0, but you should check the version in\nour `.pre-commit-config.yaml` file (node version).\n\nInstalling yarn is described in https://classic.yarnpkg.com/en/docs/install\n\nAlso - in case you use `breeze` or have `pre-commit` installed, you can build the assets with the following:\n\n    pre-commit run --hook-stage manual compile-www-assets --all-files\n\nor\n\n    breeze compile-www-assets\n\nBoth commands will install node and yarn, if needed, to a dedicated pre-commit node environment and\nthen build the assets.\n\nFinally, you can also clean and recompile assets with `custom` build target when running the Hatch build\n\n    hatch build -t custom -t wheel -t sdist\n\nThis will also update `git_version` file in the Airflow package that should contain the git commit hash of the\nbuild. This is used to display the commit hash in the UI.\n"
        },
        {
          "name": "INTHEWILD.md",
          "type": "blob",
          "size": 56.84375,
          "content": "<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n -->\n\n## Who uses Apache Airflow?\n\nAs the Apache Airflow community grows, we'd like to keep track of who is using\nthe platform. Please send a PR with your company name and @githubhandle.\n\nCurrently, **officially** using Airflow:\n\n1. [2RP Net](https://www.2rpnet.com.br/en)\n1. [4G Capital](http://www.4g-capital.com/) [[@posei](https://github.com/posei)]\n1. [6play](https://www.6play.fr) [[@lemourA](https://github.com/lemoura), [@achaussende](https://github.com/achaussende), [@d-nguyen](https://github.com/d-nguyen), [@julien-gm](https://github.com/julien-gm)]\n1. [8fit](https://8fit.com/) [[@nicor88](https://github.com/nicor88), [@frnzska](https://github.com/frnzska)]\n1. [90 Seconds](https://90seconds.tv/) [[@aaronmak](https://github.com/aaronmak)]\n1. [99](https://99taxis.com) [[@fbenevides](https://github.com/fbenevides), [@gustavoamigo](https://github.com/gustavoamigo) & [@mmmaia](https://github.com/mmmaia)]\n1. [Accenture](https://www.accenture.com/au-en) [[@nijanthanvijayakumar](https://github.com/nijanthanvijayakumar)]\n1. [AdBOOST](https://www.adboost.sk) [[AdBOOST](https://github.com/AdBOOST)]\n1. [Adobe](https://www.adobe.com/) [[@mishikaSingh](https://github.com/mishikaSingh), [@ramandumcs](https://github.com/ramandumcs), [@vardancse](https://github.com/vardancse)]\n1. [Adyen](https://www.adyen.com/) [[@jorricks](https://github.com/jorricks), [@MaicoTimmerman](https://github.com/MaicoTimmerman)]\n1. [Agari](https://github.com/agaridata) [[@r39132](https://github.com/r39132)]\n1. [Agoda](https://agoda.com) [[@akki](https://github.com/akki)]\n1. [Airbnb](https://airbnb.io/) [[@mistercrunch](https://github.com/mistercrunch), [@artwr](https://github.com/artwr)]\n1. [AirDNA](https://www.airdna.co)\n1. [Airfinity](https://www.airfinity.com) [[@sibowyer](https://github.com/sibowyer)]\n1. [Airtel](https://www.airtel.in/) [[@harishbisht](https://github.com/harishbisht)]\n1. [Aisera](https://aisera.com/) [[@mayankymailusfedu](https://github.com/mayankymailusfedu)]\n1. [Akamai](https://www.akamai.com/) [[@anirudhbagri](https://github.com/anirudhbagri)]\n1. [Akamas](https://akamas.io) [[@GiovanniPaoloGibilisco](https://github.com/GiovanniPaoloGibilisco), [@lucacavazzana](https://github.com/lucacavazzana)]\n1. [Alan](https://alan.eu) [[@charles-go](https://github.com/charles-go)]\n1. [AliFone](https://www.alifone.com.br/)\n1. [allegro.pl](http://allegro.tech/) [[@kretes](https://github.com/kretes)]\n1. [Allo-Media](https://www.allo-media.net) [[@merwan](https://github.com/merwan/)]\n1. [AloPeyk](https://alopeyk.com) [[@blcksrx](https://github.com/blcksrx), [@AloPeyk](https://github.com/AloPeyk)]\n1. [Altafino](https://altafino.com) [[@altafino](https://github.com/altafino)]\n1. [AltX](https://www.getaltx.com/about) [[@pedromduarte](https://github.com/pedromduarte)]\n1. [American Family Insurance](https://www.amfam.com/about) [[@di1eep](https://github.com/di1eep)]\n1. [AMPATH](https://www.ampathkenya.org/) [[@AMPATH](https://github.com/AMPATH), [@fatmali](https://github.com/fatmali)]\n1. [Apigee](https://apigee.com) [[@btallman](https://github.com/btallman)]\n1. [ARGO Labs](http://www.argolabs.org) [[@California Data Collaborative](https://github.com/California-Data-Collaborative)]\n1. [ARMEDANGELS](https://www.armedangels.de) [[@swiffer](https://github.com/swiffer)]\n1. [Arquivei](https://www.arquivei.com.br/) [[@arquivei](https://github.com/arquivei)]\n1. [Arrive](https://www.arrive.com/)\n1. [Artelys](https://www.artelys.com/) [[@fortierq](https://github.com/fortierq)]\n1. [Asana](https://asana.com/) [[@chang](https://github.com/chang), [@dima-asana](https://github.com/dima-asana), [@jdavidheiser](https://github.com/jdavidheiser), [@ricardoandresrojas](https://github.com/ricardoandresrojas)]\n1. [Astronomer](https://www.astronomer.io) [[@schnie](https://github.com/schnie), [@ashb](https://github.com/ashb), [@kaxil](https://github.com/kaxil), [@dimberman](https://github.com/dimberman), [@andriisoldatenko](https://github.com/andriisoldatenko), [@ryw](https://github.com/ryw), [@ryanahamilton](https://github.com/ryanahamilton), [@jhtimmins](https://github.com/jhtimmins), [@vikramkoka](https://github.com/vikramkoka), [@jedcunningham](https://github.com/jedcunningham), [@BasPH](https://github.com/basph), [@ephraimbuddy](https://github.com/ephraimbuddy), [@feluelle](https://github.com/feluelle)]\n1. [Audiomack](https://audiomack.com) [[@billcrook](https://github.com/billcrook)]\n1. [Auth0](https://auth0.com) [[@scottypate](https://github.com/scottypate)], [[@dm03514](https://github.com/dm03514)], [[@karangale](https://github.com/karangale)]\n1. [Autodesk](https://autodesk.com)\n1. [Automattic](https://automattic.com/) [[@anandnalya](https://github.com/anandnalya), [@bperson](https://github.com/bperson), [@khrol](https://github.com/Khrol), [@xyu](https://github.com/xyu)]\n1. [Avesta Technologies](https://avestatechnologies.com) [[@TheRum](https://github.com/TheRum)]\n1. [Aviva plc](https://www.aviva.com) [[@panatimahesh](https://github.com/panatimahesh)]\n1. [Away](https://awaytravel.com) [[@trunsky](https://github.com/trunsky)]\n1. [Axesor designeted activity company](https://www.axesor.es/)\n1. [Azri Solutions](http://www.azrisolutions.com/) [[@userimack](https://github.com/userimack)]\n1. [Bagelcode](https://www.bagelcode.com/)\n1. [BalanceHero](http://truebalance.io/) [[@swalloow](https://github.com/swalloow)]\n1. [Banco de Formaturas](https://www.bancodeformaturas.com.br) [[@guiligan](https://github.com/guiligan)]\n1. [Banco Inter](https://www.bancointer.com.br) [[@ignitz](https://github.com/ignitz)]\n1. [BandwidthX](http://www.bandwidthx.com) [[@dineshdsharma](https://github.com/dineshdsharma)]\n1. [Basetis](http://www.basetis.com)\n1. [BBM](https://www.bbm.com/)\n1. [Beamly](https://www.beamly.com/) [[@christopheralcock](https://github.com/christopheralcock)]\n1. [Beeswax](https://beeswax.com/)\n1. [Bellhops](https://github.com/bellhops)\n1. [BelugaDB](https://belugadb.com) [[@fabio-nukui](https://github.com/fabio-nukui) & [@joao-sallaberry](https://github.com/joao-sallaberry) & [@lucianoviola](https://github.com/lucianoviola) & [@tmatuki](https://github.com/tmatuki)]\n1. [Bentego](https://bentego.com/)\n1. [Betterment](https://www.betterment.com/) [[@betterment](https://github.com/Betterment)]\n1. [Bexs Bank](https://www.bexs.com.br/en) [[@felipefb](https://github.com/felipefb) & [@ilarsen](https://github.com/ishvann)]\n1. [Bidnamic](https://www.bidnamic.com/) [[@robinedwards](https://github.com/robinedwards) & [@leonsmith](https://github.com/leonsmith) & [@sanjayhallan](https://github.com/sanjayhallan)]\n1. [BigQuant](https://bigquant.com/) [[@bigquant](https://github.com/bigquant)]\n1. [Birdz by Veolia](https://www.birdz.com/en/) [[@benjamingrenier](https://github.com/benjamingrenier)]\n1. [BlaBlaCar](https://www.blablacar.com) [[@puckel](https://github.com/puckel) & [@wmorin](https://github.com/wmorin)]\n1. [Blacklane](https://www.blacklane.com) [[@serkef](https://github.com/serkef)]\n1. [Bloc](https://www.bloc.io) [[@dpaola2](https://github.com/dpaola2)]\n1. [Bloomberg](https://www.techatbloomberg.com) [[@skandala23] (https://github.com/skandala23) & [@vfeldsher](https://https://github.com/vfeldsher)]\n1. [Bloomreach](https://www.bloomreach.com/) [[@neelborooah](https://github.com/neelborooah) & [@debodirno](https://github.com/debodirno) & [@ayushmnnit](https://github.com/ayushmnnit)]\n1. [Blue Yonder](http://www.blue-yonder.com) [[@blue-yonder](https://github.com/blue-yonder)]\n1. [BlueApron](https://www.blueapron.com) [[@jasonjho](https://github.com/jasonjho) & [@matthewdavidhauser](https://github.com/matthewdavidhauser)]\n1. [Bluecore](https://www.bluecore.com) [[@JLDLaughlin](https://github.com/JLDLaughlin)]\n1. [Bluekiri](https://bluekiri.com) [[@Bluekiri](https://github.com/bluekiri)]\n1. [BNY Mellon](https://www.bnymellon.com/) [[@sellist](https://github.com/sellist)]\n1. [Boda Telecom Suite - CE](https://github.com/bodastage/bts-ce) [[@erssebaggala](https://github.com/erssebaggala), [@bodastage](https://github.com/bodastage)]\n1. [Bodastage Solutions](http://bodastage.com) [[@erssebaggala](https://github.com/erssebaggala), [@bodastage](https://github.com/bodastage)]\n1. [Bombora Inc](https://bombora.com/) [[@jeffkpayne](https://github.com/jeffkpayne), [@pakelley](https://github.com/pakelley), [@dNavalta](https://github.com/dNavalta), [@austynh](https://github.com/austynh), [@TheOriginalAlex](https://github.com/TheOriginalAlex)]\n1. [Bonial International GmbH](https://www.bonial.com/)\n1. [Bonnier Broadcasting](http://www.bonnierbroadcasting.com) [[@wileeam](https://github.com/wileeam)]\n1. [Bosch (Robert Bosch GmbH)](https://www.bosch.com/stories/topics/automated-driving/) [[@jscheffl](https://github.com/jscheffl), [@clellmann](https://github.com/clellmann), [@wolfdn](https://github.com/wolfdn), [@AutomationDev85](https://github.com/AutomationDev85), [@majorosdonat](https://github.com/majorosdonat), [@OliverWannenwetsch](https://github.com/OliverWannenwetsch)]\n1. [BounceX](http://www.bouncex.com) [[@JoshFerge](https://github.com/JoshFerge), [@hudsonrio](https://github.com/hudsonrio), [@ronniekritou](https://github.com/ronniekritou)]\n1. [Braintree](https://www.braintreepayments.com) [[@coopergillan](https://github.com/coopergillan), [@curiousjazz77](https://github.com/curiousjazz77), [@raymondberg](https://github.com/raymondberg)]\n1. [Branch](https://branch.io) [[@sdebarshi](https://github.com/sdebarshi), [@dmitrig01](https://github.com/dmitrig01)]\n1. [Breezeline (formerly Atlantic Broadband)](https://www.breezeline.com/) [[@IanDoarn](https://github.com/IanDoarn), [@willsims14](https://github.com/willsims14)]\n1. [BTIG](https://www.btig.com/) [[@hikaruhk](https://github.com/hikaruhk)]\n1. [BWGI](https://www.bwgi.com.br/) [[@jgmarcel](https://github.com/jgmarcel)]\n1. [Bwtech](https://www.bwtech.com/) [[@wolvery](https://github.com/wolvery)]\n1. [C2FO](https://www.c2fo.com/)\n1. [Caesars Entertainment](https://www.caesars.com)\n1. [Cafe Bazaar](https://cafebazaar.ir/about?l=en) [[@cafebazaar](https://github.com/cafebazaar)]\n1. [California Data Collaborative](https://github.com/California-Data-Collaborative) powered by [ARGO Labs](http://www.argolabs.org)\n1. [Capital One](https://www.capitalone.com) [[@anoopengineer](https://github.com/anoopengineer)]\n1. [Carbonite](https://www.carbonite.com) [[@ajbosco](https://github.com/ajbosco)]\n1. [CarLabs](https://www.carlabs.ai/) [[@sganz](https://github.com/sganz) & [@odannyc](https://github.com/odannyc)]\n1. [Carpe Data](https://www.carpe.io/) [[@manugarri](https://github.com/manugarri)]]\n1. [CAVA](https://www.cava.com) [[@minh5](https://github.com/minh5) & [@patchus](https://github.com/patchus)]\n1. [Celect](http://www.celect.com) [[@superdosh](https://github.com/superdosh) & [@chadcelect](https://github.com/chadcelect)]\n1. [Censys](https://censys.io) [[@zakird](https://github.com/zakird), [@dadrian](https://github.com/dadrian), & [@andrewsardone](https://github.com/andrewsardone)]\n1. [Change.org](https://www.change.org) [[@change](https://github.com/change), [@vijaykramesh](https://github.com/vijaykramesh)]\n1. [Chartboost](https://www.chartboost.com) [[@cgelman](https://github.com/cgelman) & [@dclubb](https://github.com/dclubb)]\n1. [Checkr](https://checkr.com) [[@tongboh](https://github.com/tongboh)]\n1. [Children's Hospital of Philadelphia Division of Genomic Diagnostics](http://www.chop.edu/centers-programs/division-genomic-diagnostics) [[@genomics-geek](https://github.com/genomics-geek/)]\n1. [Cinimex DataLab](http://cinimex.ru) [[@kdubovikov](https://github.com/kdubovikov)]\n1. [City of San Diego](http://sandiego.gov) [[@MrMaksimize](https://github.com/mrmaksimize), [@andrell81](https://github.com/andrell81) & [@arnaudvedy](https://github.com/arnaudvedy)]\n1. [City of Toronto](https://www.toronto.ca/) [[@CityofToronto](https://github.com/CityofToronto), [@radumas](https://github.com/radumas)]\n1. [ciValue](https://civalue.com/) [[@chencivalue](https://github.com/chencivalue), [@YoavGaudin](https://github.com/YoavGaudin), [@saleem-boshnak](https://github.com/saleem-boshnak)]\n1. [Civey](https://civey.com/) [[@WesleyBatista](https://github.com/WesleyBatista)]\n1. [Clairvoyant](https://clairvoyantsoft.com) [[@shekharv](https://github.com/shekharv)]\n1. [Clarity AI](https://clarity.ai/) [[@clarityai-eng/data-eng](https://github.com/orgs/clarityai-eng/teams/data-eng)]\n1. [Clarum](https://clarum.it/) [[@avanelli](https://github.com/avanelli)]\n1. [Classmethod, Inc.](https://classmethod.jp/) [[@shoito](https://github.com/shoito)]\n1. [Cleartax](https://cleartax.in/) [[@anks](https://github.com/anks) & [@codebuff](https://github.com/codebuff)]\n1. [Clicksign](https://clicksign.com/) [[@mbbernstein](https://github.com/mbbernstein) & [@jorgeac12](https://github.com/jorgeac12) & [@franklin390](https://github.com/franklin390)]\n1. [Cloudera](https://www.cloudera.com/) [[@phraniiac](https://github.com/phraniiac) & [@VivekPemawat](https://github.com/VivekPemawat) & [@amoghrajesh](https://github.com/amoghrajesh) & [@vedantlodha](https://github.com/vedantlodha) & [@shubhamraj-git](https://github.com/shubhamraj-git) & [@Samit-Maharjan](https://github.com/Samit-Maharjan)] & [@anukrati1507](https://github.com/anukrati1507)\n1. [Clover Health](https://www.cloverhealth.com) [[@ryansiu1995](https://github.com/ryansiu1995)]\n1. [Coinbase](https://www.coinbase.com) [[@mingshi-wang](https://github.com/mingshi-wang)]\n1. [Coinone](https://www.coinonecorp.com) [[@jx2lee](https://github.com/jx2lee)]\n1. [Colgate-Palmolive](https://www.colgatepalmolive.com/) [[@fhoda](https://github.com/fhoda)]\n1. [Collectivehealth Inc.](https://www.collectivehealth.com) [[@retornam](https://github.com/retornam)]\n1. [Compass](https://www.compass.com) [[@wdhorton](https://github.com/wdhorton)]\n1. [ConnectWise](https://www.connectwise.com/) [[@jacobeturpin](https://github.com/jacobeturpin)]\n1. [ContaAzul](https://www.contaazul.com) [[@bern4rdelli](https://github.com/bern4rdelli), [@renanleme](https://github.com/renanleme) & [@sabino](https://github.com/sabino)]\n1. [Corsearch](https://corsearch.com/) [[@silvantonio](https://github.com/silvantonio), [@erjanmx](https://github.com/erjanmx), [@honarkhah](https://github.com/honarkhah), [@RogierVanDoggenaar](https://github.com/RogierVanDoggenaar), [@arekgorecki](https://github.com/arekgorecki), [@jmolpointerbp](https://github.com/jmolpointerbp)]\n1. [Cotap](https://github.com/cotap/) [[@maraca](https://github.com/maraca) & [@richardchew](https://github.com/richardchew)]\n1. [Coursera](https://coursera.org/) [[@darrenmk](https://github.com/darrenmk)]\n1. [CoverGenius](https://covergenius.com) [[@lihan](https://github.com/lihan), [@tyomo4ka](https://github.com/tyomo4ka), [@girish-l](https://github.com/girish-l)]\n1. [Craig@Work](https://www.craigatwork.com)\n1. [CrazyLabs](https://www.crazylabs.com/) [[@HadarSha](https://github.com/HadarSha), [@gil-tober](https://github.com/gil-tober)]\n1. [Crealytics](https://crealytics.com)\n1. [Credit Karma](https://www.creditkarma.com/) [[@preete-dixit-ck](https://github.com/preete-dixit-ck) & [@harish-gaggar-ck](https://github.com/harish-gaggar-ck) & [@greg-finley-ck](https://github.com/greg-finley-ck)]\n1. [Creditas](https://www.creditas.com.br) [[@dcassiano](https://github.com/dcassiano)]\n1. [CreditCards.com](https://www.creditcards.com/) [[@vmAggies](https://github.com/vmAggies) &  [@jay-wallaby](https://github.com/jay-wallaby)]\n1. [CRST - The Transportation Solution, Inc.](https://crst.com)\n1. [Cryptalizer.com](https://www.cryptalizer.com/)\n1. [Currency](https://www.gocurrency.com/) [[@FCLI](https://github.com/FCLI) & [@alexbegg](https://github.com/alexbegg)]\n1. [Custom Ink](https://www.customink.com/) [[@david-dalisay](https://github.com/david-dalisay), [@dmartin11](https://github.com/dmartin11) & [@mpeteuil](https://github.com/mpeteuil)]\n1. [Cyberdino](https://www.cyberdino.io) [[@cyberdino-io](https://github.com/cyberdino-io)]\n1. [Cyscale](https://cyscale.com) [[@ocical](https://github.com/ocical)]\n1. [Dailymotion](http://www.dailymotion.com/fr) [[@germaintanguy](https://github.com/germaintanguy) & [@hc](https://github.com/hc)]\n1. [DANA](https://www.dana.id/) [[@imamdigmi](https://github.com/imamdigmi)]\n1. [Danamica](https://www.danamica.dk) [[@testvinder](https://github.com/testvinder)]\n1. [Data Minded](https://www.dataminded.com) [[@datamindedbe](https://github.com/datamindedbe)]\n1. [DataCamp](https://datacamp.com/) [[@dgrtwo](https://github.com/dgrtwo)]\n1. [DataFox](https://www.datafox.com/) [[@sudowork](https://github.com/sudowork)]\n1. [Datamaran](https://www.datamaran.com) [[@valexharo](https://github.com/valexharo)]\n1. [DataPipes](https://www.home.datapipes.io/) [[@rishabh-cldcvr](https://github.com/rishabh-cldcvr)]\n1. [dataroots](https://dataroots.io/) [[@datarootsio]](https://github.com/datarootsio)\n1. [DataSprints](https://datasprints.com/) [[@lopesdiego12](https://github.com/lopesdiego12) & [@rafaelsantanaep](https://github.com/rafaelsantanaep)]\n1. [Datatonic](https://datatonic.com/) [[@teamdatatonic](https://github.com/teamdatatonic)]\n1. [Datavant](https://datavant.com)/) [@althati(https://github.com/althati)]\n1. [Datumo](https://datumo.io) [[@michalmisiewicz](https://github.com/michalmisiewicz)]\n1. [Dcard](https://www.dcard.tw/) [[@damon09273](https://github.com/damon09273) & [@bruce3557](https://github.com/bruce3557) & [@kevin1kevin1k](http://github.com/kevin1kevin1k)]\n1. [Delft University of Technology](https://www.tudelft.nl/en/) [[@saveriogzz](https://github.com/saveriogzz)]\n1. [Dentsu Inc.](http://www.dentsu.com/) [[@bryan831](https://github.com/bryan831) & [@loozhengyuan](https://github.com/loozhengyuan)]\n1. [Deseret Digital Media](http://deseretdigital.com/) [[@formigone](https://github.com/formigone)\n1. [DevITJobs.com](https://devitjobs.com/)\n1. [DFDS](https://www.dfds.com/) [[@timonviola](https://github.com/timonviola)]\n1. [Digital First Media](http://www.digitalfirstmedia.com/) [[@duffn](https://github.com/duffn) & [@mschmo](https://github.com/mschmo) & [@seanmuth](https://github.com/seanmuth)]\n1. [Disney](https://www.disney.com/) [[@coolbeans201](https://github.com/coolbeans201)]\n1. [Docsity](https://www.docsity.com/)\n1. [Doctrine](https://www.doctrine.fr/)[[@anteverse](https://github.com/anteverse)]\n1. [DoorDash](https://www.doordash.com/)\n1. [Dotmodus](http://dotmodus.com) [[@dannylee12](https://github.com/dannylee12)]\n1. [Drivy](https://www.drivy.com) [[@AntoineAugusti](https://github.com/AntoineAugusti)]\n1. [Dropbox](https://www.dropbox.com) [[@AlexeySanko](https://github.com/AlexeySanko)]\n1. [Dunnhumby](https://www.dunnhumby.com)\n1. [Dunzo](https://www.dunzo.com)[[@masterlittle](https://github.com/masterlittle)]\n1. [Dynata](https://www.dynata.com) [[@neil3handari](https://github.com/neil3handari)]\n1. [e-MPS](https://e-mps.co.uk/)[[@IanDanielM](https://github.com/IanDanielM)\n1. [Easy Taxi](http://www.easytaxi.com/) [[@caique-lima](https://github.com/caique-lima) & [@diraol](https://github.com/diraol)]\n1. [EBANX](https://www.ebanx.com/) [[@diogodilcl](https://github.com/diogodilcl) & [@estevammr](https://github.com/estevammr) & [@filipe-banzoli](https://github.com/filipe-banzoli) & [@lara-clink](https://github.com/lara-clink) & [@Lucasdsvenancio](https://github.com/Lucasdsvenancio) & [@mariotaddeucci](https://github.com/mariotaddeucci) & [@nadiapetramont](https://github.com/nadiapetramont) & [@nathangngencissk](https://github.com/nathangngencissk) & [@patrickjuan](https://github.com/patrickjuan) & [@raafaadg](https://github.com/raafaadg) & [@samebanx](https://github.com/samebanx) & [@thiagoschonrock](https://github.com/thiagoschonrock) & [@whrocha](https://github.com/whrocha)]\n1. [Elai Data](https://www.elaidata.com/) [[@lgov](https://github.com/lgov)]\n1. [Electronic Arts](https://www.ea.com/) [[@yuan-glu](https://github.com/yuan-glu)]\n1. [EllisDon](http://www.ellisdon.com/) [[@d2kalra](https://github.com/d2kalra) & [@zbasama](https://github.com/zbasama)]\n1. [Endesa](https://www.endesa.com) [[@drexpp](https://github.com/drexpp)]\n1. [ENECHANGE Ltd.](https://enechange.co.jp/) [[@enechange](https://github.com/enechange)]\n1. [Energy Solutions](https://www.energy-solution.com) [[@energy-solution](https://github.com/energy-solution/)]\n1. [Enigma](https://www.enigma.com) [[@hydrosquall](https://github.com/hydrosquall)]\n1. [EnterpriseDB](https://www.Enterprisedb.com) [[@kaydee-edb](https://github.com/kaydee-edb)]\n1. [Envestnet](https://www.envestnet.com/) [[@svellaiyan](https://github.com/svellaiyan)]\n1. [Ericsson](https://www.ericsson.com) [[@khalidzohaib](https://github.com/khalidzohaib)]\n1. [Estrategia Educacional](https://github.com/estrategiahq) [[@jonasrla](https://github.com/jonasrla)]\n1. [Etsy](https://www.etsy.com) [[@mchalek](https://github.com/mchalek)]\n1. [EUIGS - Admiral Group](https://www.linkedin.com/company/euiitglobalservices) [[@emilioego](https://github.com/emilioego)]\n1. [Europcar](https://www.europcar.com/en-us) [[@Conformist101](https://github.com/Conformist101) & [@davidpr91](https://github.com/davidpr91) & [@jcarbonell](https://github.com/jcarbonell)& [@marc-rf](https://github.com/marc-rf)& [@VictorGeaGarcia](https://github.com/VictorGeaGarcia)]\n1. [Everis](https://www.everis.com) [[@diegobenedicto](https://github.com/diegobenedicto)]\n1. [Everlane](https://everlane.com) [[@NickBenthem](https://github.com/NickBenthem)]\n1. [evo.company](https://evo.company/) [[@orhideous](https://github.com/orhideous)]\n1. [Experity (formerly DocuTAP)](https://www.experityhealth.com/) [[@cloneluke](https://github.com/cloneluke) & [@tobyjoliver](https://github.com/tobyjoliver)]\n1. [FanDuel](https://www.fanduel.com/)\n1. [Farfetch](https://github.com/farfetch) [[@davidmarques78](https://github.com/davidmarques78)]\n1. [Fathom Health](https://www.fathomhealth.co/)\n1. [Firestone Inventing](https://www.hsmap.com/) [[@zihengCat](https://github.com/zihengCat)]\n1. [Five9](https://https://www.five9.com/) [[srabasti](https://github.com/Srabasti)]\n1. [Fleek Fashion](https://www.fleekfashion.app/) [[@ghodouss](https://github.com/ghodoussG)]\n1. [Flipp](https://www.flipp.com) [[@sethwilsonwishabi](https://github.com/sethwilsonwishabi)]\n1. [Format](https://www.format.com) [[@format](https://github.com/4ormat) & [@jasonicarter](https://github.com/jasonicarter)]\n1. [FreeNow](https://free-now.com) [[@freenowtech](https://github.com/freenowtech)]\n1. [FreshBooks](https://github.com/freshbooks) [[@DinoCow](https://github.com/DinoCow)]\n1. [Freshworks](https://www.freshworks.com/) [[@shaikshakeel](https://github.com/shaikshakeel), [@aa3pankaj](https://github.com/aa3pankaj)]\n1. [FullContact](https://github.com/fullcontact)\n1. [Fuller, Inc.](https://en.fuller-inc.com/) [[@wutali](https://github.com/wutali) & [@sh-tech](https://github.com/sh-tech)]\n1. [Fundera](https://fundera.com) [[@andyxhadji](https://github.com/andyxhadji)]\n1. [G Adventures](https://gadventures.com) [[@chchtv11](https://github.com/chchtv11), [@tgumbley](https://github.com/tgumbley), [@tomwross](https://github.com/tomwross)]\n1. [GameWisp](https://gamewisp.com) [[@tjbiii](https://github.com/TJBIII) & [@theryanwalls](https://github.com/theryanwalls)]\n1. [Geekie](https://www.geekie.com.br) [[@wolney](https://github.com/wolney)]\n1. [GeneCards](https://www.genecards.org) [[@oferze](https://github.com/oferze)]\n1. [Gentner Lab](https://github.com/gentnerlab) [[@neuromusic](https://github.com/neuromusic)]\n1. [Get Simpl](https://getsimpl.com/) [[@rootcss](https://github.com/rootcss)]\n1. [Getir](https://www.getir.com/) [[@mpolatcan](https://github.com/mpolatcan)]\n1. [Giant Steps Capital](https://www.gscap.com.br/) [[@flaviokr](https://github.com/flaviokr) & [@giant-steps](https://github.com/giant-steps) & [@mateuslatrova](https://github.com/mateuslatrova) & [@NataliaGodot](https://github.com/NataliaGodot)]\n1. [GitLab](https://about.gitlab.com/) [[@tayloramurphy](https://gitlab.com/tayloramurphy) & [@m_walker](https://gitlab.com/m_walker)]\n1. [Glassdoor](https://github.com/Glassdoor) [[@syvineckruyk](https://github.com/syvineckruyk) & [@sid88in](https://github.com/sid88in)]\n1. [Global Fashion Group](http://global-fashion-group.com) [[@GFG](https://github.com/GFG)]\n1. [GoDataDriven](https://godatadriven.com/) [[@danielvdende](https://github.com/danielvdende), [@ffinfo](https://github.com/ffinfo), [@Fokko](https://github.com/Fokko), [@gglanzani](https://github.com/gglanzani), [@hgrif](https://github.com/hgrif), [@jrderuiter](https://github.com/jrderuiter), [@NielsZeilemaker](https://github.com/NielsZeilemaker)]\n1. [Gojek](https://gojek.com/) [[@gojek](https://github.com/gojek), [@rootcss](https://github.com/rootcss)]\n1. [Google](https://google.com/) [[@akashsriv07](https://github.com/akashsriv07)]\n1. [GovTech GDS](https://gds-gov.tech) [[@chrissng](https://github.com/chrissng) & [@datagovsg](https://github.com/datagovsg)]\n1. [GR8 Tech](https://gr8.tech/) [[@KulykDmytro](https://github.com/KulykDmytro) & [@Tonkonozhenko](https://github.com/Tonkonozhenko)]\n1. [Grab](https://www.grab.com/sg/) [[@calvintran](https://github.com/canhtran)]\n1. [Gradeup](https://gradeup.co) [[@gradeup](https://github.com/gradeup)]\n1. [Grand Rounds](https://www.grandrounds.com/) [[@richddr](https://github.com/richddr), [@timz1290](https://github.com/timz1290), [@wenever](https://github.com/@wenever), & [@runongirlrunon](https://github.com/runongirlrunon)]\n1. [Greenbids](https://www.greenbids.ai/) [[@greenbids](https://github.com/greenbids), [@AlexisBRENON](https://github.com/AlexisBRENON)]\n1. [Greytip](https://www.greytip.com) [[@greytip](https://github.com/greytip)]\n1. [Groupalia](http://es.groupalia.com) [[@jesusfcr](https://github.com/jesusfcr)]\n1. [Groupon](https://groupon.com) [[@stevencasey](https://github.com/stevencasey)]\n1. [Growbots](https://www.growbots.com/) [[@exploy](https://github.com/exploy)]\n1. [GrowthSimple](https://growthsimple.ai/)\n1. [GSN Games](https://www.gsngames.com)\n1. [Gusto](https://gusto.com) [[@frankhsu](https://github.com/frankhsu)]\n1. [Handshake](https://joinhandshake.com/) [[@mhickman](https://github.com/mhickman)]\n1. [Handy](http://www.handy.com/careers/73115?gh_jid=73115&gh_src=o5qcxn) [[@marcintustin](https://github.com/marcintustin) / [@mtustin-handy](https://github.com/mtustin-handy)]\n1. [happn](https://www.happn.com) [[@pcorbel](https://github.com/pcorbel)]\n1. [HAVAN](https://www.havan.com.br) [[@botbiz](https://github.com/botbiz)]\n1. [HBC Digital](http://tech.hbc.com) [[@tmccartan](https://github.com/tmccartan) & [@dmateusp](https://github.com/dmateusp)]\n1. [HBO](http://www.hbo.com/) [[@yiwang](https://github.com/yiwang)]\n1. [Headout](https://headout.com/) [[@shivanshs9](https://github.com/shivanshs9/)]\n1. [Healthjump](http://www.healthjump.com/) [[@miscbits](https://github.com/miscbits)]\n1. [HelloFresh](https://www.hellofresh.com) [[@tammymendt](https://github.com/tammymendt) & [@davidsbatista](https://github.com/davidsbatista) & [@iuriinedostup](https://github.com/iuriinedostup)]\n1. [Hipages](https://www.hipages.com.au/) [[@arihantsurana](https://github.com/arihantsurana) & [@koconder](https://github.com/koconder)]\n1. [Holimetrix](http://holimetrix.com/) [[@thibault-ketterer](https://github.com/thibault-ketterer)]\n1. [HomeToGo](https://www.hometogo.com/) [[@HomeToGo](https://github.com/hometogo), [@AurimasGr](https://github.com/AurimasGr)]\n1. [Hootsuite](https://github.com/hootsuite)\n1. [Hostnfly](https://www.hostnfly.com/) [[@CyrilLeMat](https://github.com/CyrilLeMat) & [@pierrechopin](https://github.com/pierrechopin) & [@alexisrosuel](https://github.com/alexisrosuel)]\n1. [HotelQuickly](https://github.com/HotelQuickly) [[@zinuzoid](https://github.com/zinuzoid)]\n1. [Hotstar](https://hotstar.com)\n1. [HP Inc](https://www.hp.com/) [[@hpinc](https://github.com/HPInc)]\n1. [Huq Industries](https://huq.io) [[@huqindustries](https://github.com/huq-industries), [@alepuccetti](https://github.com/alepuccetti), [@turbomerl](https://github.com/turbomerl)]\n1. [Hurb](https://hurb.com/) [[@hurbcom](https://github.com/hurbcom)]\n1. [Iflix](https://piay.iflix.com) [[@ChaturvediSulabh](https://github.com/ChaturvediSulabh)]\n1. [IFTTT](https://www.ifttt.com/) [[@apurvajoshi](https://github.com/apurvajoshi)]\n1. [iHeartRadio](http://www.iheart.com/) [[@yiwang](https://github.com/yiwang)]\n1. [imgix](https://www.imgix.com/) [[@dclubb](https://github.com/dclubb)]\n1. [Indeed](https://www.indeed.com/) [[@chrismclennon](https://github.com/chrismclennon), [@raj-manvar](https://github.com/raj-manvar), [@Adil-Ibragimov](https://github.com/Adil-Ibragimov)]\n1. [IndiaMART](https://www.indiamart.com) [[@sumit-kushwah](https://github.com/sumit-kushwah)]\n1. [Infoedge](https://www.infoedge.in/) [[@amolsr](https://github.com/amolsr)]\n1. [Infofarm](https://infofarm.be/) [[@infofarm](https://github.com/infofarm), [@kobethuwis](https://github.com/kobethuwis)]\n1. [InfoWorks.io](https://www.infoworks.io/) [[@pavansharma36](https://github.com/pavansharma36)]\n1. [Infrabel](https://infrabel.be) [[@Joffreybvn](https://github.com/Joffreybvn), [@dabla](https://github.com/dabla), [@robbydb88](https://github.com/robbydb88), [@jarnedemunter1](https://github.com/jarnedemunter1)]\n1. [ING](http://www.ing.com/)\n1. [Inoopa](https://www.inoopa.com/) [[@GraphtyLove](https://github.com/GraphtyLove)]\n1. [Instacart 🥕](http://www.instacart.com/) [[@arp1t](https://github.com/arp1t) & [@code-sauce](https://github.com/code-sauce) & [@jasonlew](https://github.com/jasonlew) & [@j4p3](https://github.com/j4p3) & [@lubert](https://github.com/lubert) & [@mmontagna](https://github.com/mmontagna) & [@RyanAD](https://github.com/RyanAD) &[@zzadeh](https://github.com/zzadeh)]\n1. [Intellischool 🎓](https://intellischool.co/) [[@intelliscl](https://github.com/intelliscl) & [@dave-philp](https://github.com/dave-philp)]\n1. [Inter Platform Inc.](https://www.bancointer.com.br/) [[@wolvery](https://github.com/wolvery)\n1. [Intercom](http://www.intercom.com/) [[@fox](https://github.com/fox) & [@paulvic](https://github.com/paulvic)]\n1. [Interia](http://www.interia.pl)\n1. [Investorise](https://investorise.com/) [[@svenvarkel](https://github.com/svenvarkel)]\n1. [iS2.co](https://www.is2.co) [[@iS2co](https://github.com/iS2co)]\n1. [Jagex](https://www.jagex.com) [[@anumsheraz](https://github.com/AnumSheraz), [@trucnguyenlam](https://github.com/trucnguyenlam), [@lumez](https://github.com/lumez)]\n1. [Jampp](https://github.com/jampp)\n1. [Jeitto](https://www.jeitto.com.br) [[@BrennerPablo](https://github.com/BrennerPablo) & [@ds-mauri](https://github.com/ds-mauri)]\n1. [Jetlore](http://www.jetlore.com/) [[@bderose](https://github.com/bderose)]\n1. [Jobrapido](https://www.jobrapido.com/) [[@mattiagiupponi](https://github.com/mattiagiupponi)]\n1. [JobTeaser](https://www.jobteaser.com) [[@stefani75](https://github.com/stefani75) &  [@knil-sama](https://github.com/knil-sama)]\n1. [JULO](https://www.julo.co.id/) [[@sepam](https://github.com/sepam) & [@tenapril](https://github.com/tenapril) & [@verzqy](https://github.com/verzqy)]\n1. [Kalibrr](https://www.kalibrr.com/) [[@charlesverdad](https://github.com/charlesverdad)]\n1. [Kargo](https://kargo.com) [[@chaithra-yenikapati](https://github.com/chaithra-yenikapati), [@akarsh3007](https://github.com/akarsh3007) & [@dineshanchan](https://github.com/dineshanchan)]\n1. [Karmic](https://karmiclabs.com) [[@hyw](https://github.com/hyw)]\n1. [Kayzen](https://kayzen.io) [[@arvindeybram](https://github.com/arvindeybram)]\n1. [King Abdullah Petroleum Studies and Research Center(KAPSARC)](https://github.com/kapsarc) [[@saianupkumarp](https://github.com/saianupkumarp)]\n1. [King](https://king.com) [[@nathadfield](https://github.com/nathadfield)]\n1. [Kiwi.com](https://kiwi.com/) [[@underyx](https://github.com/underyx)]\n1. [Kogan.com](https://github.com/kogan) [[@geeknam](https://github.com/geeknam)]\n1. [Korbit](https://www.korbit.co.kr/) [[@jensenity](https://github.com/jensenity)]\n1. [KPN B.V.](https://www.kpn.com/) [[@biyanisuraj](https://github.com/biyanisuraj) & [@gmic](https://github.com/gmic)]\n1. [Kroton Educacional](http://www.kroton.com.br/)\n1. [Leboncoin](https://www.leboncoin.fr/) [[@hussein-awala](https://github.com/hussein-awala) & [@V0lantis](https://github.com/V0lantis)]\n1. [Lemann Foundation](http://fundacaolemann.org.br) [[@fernandosjp](https://github.com/fernandosjp)]\n1. [LeMans Corporation](https://www.parts-unlimited.com/) [[@alloydwhitlock](https://github.com/alloydwhitlock)] & [[@tinyrye](https://github.com/tinyrye)]\n1. [LendUp](https://www.lendup.com/) [[@lendup](https://github.com/lendup)]\n1. [LetsBonus](http://www.letsbonus.com) [[@jesusfcr](https://github.com/jesusfcr) & [@OpringaoDoTurno](https://github.com/OpringaoDoTurno)]\n1. [Liberty Global](https://www.libertyglobal.com/) [[@LibertyGlobal](https://github.com/LibertyGlobal/)]\n1. [liligo](http://liligo.com/) [[@tromika](https://github.com/tromika)]\n1. [Lineas](https://lineas.net/) [[@lineashub](https://github.com/lineashub), [@kobethuwis](https://github.com/kobethuwis)]\n1. [LingoChamp](http://www.liulishuo.com/) [[@haitaoyao](https://github.com/haitaoyao)]\n1. [LinkedIn](http://linkedin.com/) [[@csm10495](https://github.com/csm10495)]\n1. [Loadsmart](https://loadsmart.com/) [[@loadsmart](https://github.com/loadsmart)]\n1. [Logitravel Group](https://www.logitravel.com/)\n1. [LokSuvidha](http://loksuvidha.com/) [[@saurabhwahile](https://github.com/saurabhwahile)]\n1. [Los Angeles Times](http://www.latimes.com/) [[@standyro](https://github.com/standyro)]\n1. [Lucid](http://luc.id) [[@jbrownlucid](https://github.com/jbrownlucid) & [@kkourtchikov](https://github.com/kkourtchikov)]\n1. [Lumos Labs](https://www.lumosity.com/) [[@rfroetscher](https://github.com/rfroetscher/) & [@zzztimbo](https://github.com/zzztimbo/)]\n1. [Lyft](https://www.lyft.com/) [[@feng-tao](https://github.com/feng-tao), [@milton0825](https://github.com/milton0825), [@astahlman](https://github.com/astahlman), [@youngyjd](https://github.com/youngyjd), [@ArgentFalcon](https://github.com/ArgentFalcon)]\n1. [M4U](https://www.m4u.com.br/) [[@msantino](https://github.com/msantino)]\n1. [Macquarie Group](https://www.macquarie.com) [[@PApostol](https://github.com/PApostol)]\n1. [Madrone](http://madroneco.com/) [[@mbreining](https://github.com/mbreining) & [@scotthb](https://github.com/scotthb)]\n1. [Markovian](https://markovian.com/) [[@al-xv](https://github.com/al-xv), [@skogsbaeck](https://github.com/skogsbaeck), [@waltherg](https://github.com/waltherg)]\n1. [Match.com](https://www.match.com/) [[@match-gabeflores](https://github.com/match-gabeflores)]\n1. [Menhir Financial](https://www.menhir.ai/) [[@pablo-menhir](https://github.com/pablo-menhir), [@dionisio-menhir](https://github.com/dionisio-menhir) & [@luisjvca-menhir](https://github.com/luisjvca-menhir)]\n1. [Mercadoni](https://www.mercadoni.com.co) [[@demorenoc](https://github.com/demorenoc)]\n1. [Mercari](http://www.mercari.com/) [[@yu-iskw](https://github.com/yu-iskw)]\n1. [MeuVendoo](https://www.meuvendoo.com.br) [[@CarlosDutra](https://github.com/CarlosDutra)]\n1. [MFG Labs](https://github.com/MfgLabs)\n1. [Ministry of Economy of Brazil](https://www.gov.br/economia/) [[@nitaibezerra](https://github.com/nitaibezerra), [@vitorbellini](https://github.com/vitorbellini)]\n1. [MiNODES](https://www.minodes.com) [[@dice89](https://github.com/dice89), [@diazcelsa](https://github.com/diazcelsa)]\n1. [MobiKwik](https://www.mobikwik.com) [[@mobikwik](https://github.com/mobikwik)]\n1. [Modernizing Medicine](https://www.modmed.com/) [[@kehv1n](https://github.com/kehv1n), [@dalupus](https://github.com/dalupus)]\n1. [Moka](https://www.mokapos.com) [[@willioktavega](https://github.com/willioktavega)]\n1. [Movember](https://movember.com)\n1. [MTsolutions](https://www.mtsolutions.io)[[@slara](https://github.com/slara)]\n1. [Multiply](https://www.multiply.com) [[@nrhvyc](https://github.com/nrhvyc)]\n1. [Mutt Data](https://muttdata.ai/) [[@plorenzatto](https://github.com/plorenzatto)]\n1. [My Money Bank](https://www.mymoneybank.com/) [[@renardguill](https://github.com/renardguill), [@NBardelot](https://github.com/NBardelot), [@vliger](https://github.com/vliger), [@Gaelleb22](https://github.com/Gaelleb22)]\n1. [Narrativa](https://www.narrativa.com/)\n1. [NASA Jet Propulsion Laboratory](https://www.jpl.nasa.gov) [[@lewismc](https://github.com/lewismc)]\n1. [National Bank of Canada](https://nbc.ca) [[@brilhana](https://github.com/brilhana)]\n1. [Nav, Inc.](https://nav.com/) [[@tigerjz32](https://github.com/tigerjz32)]\n1. [Neoway](https://www.neoway.com.br/) [[@neowaylabs](https://github.com/orgs/NeowayLabs/people)]\n1. [Nerdwallet](https://www.nerdwallet.com)\n1. [New Relic](https://www.newrelic.com) [[@marcweil](https://github.com/marcweil)]\n1. [Newzoo](https://www.newzoo.com) [[@newzoo-nexus](https://github.com/newzoo-nexus)]\n1. [NEXT Trucking](https://www.nexttrucking.com/) [[@earthmancash2](https://github.com/earthmancash2), [@kppullin](https://github.com/kppullin)]\n1. [Nextdoor](https://nextdoor.com) [[@SivaPandeti](https://github.com/SivaPandeti), [@zshapiro](https://github.com/zshapiro) & [@jthomas123](https://github.com/jthomas123)]\n1. [Nielsen](https://www.nielsen.com) [[@roitvt](https://github.com/roitvt) & [@itaiy](https://github.com/itaiy)]\n1. [Nine](https://nine.com.au) [[@TheZepto](https://github.com/TheZepto)]\n1. [Notino](https://www.notino.com/) [[@marekhanus](https://github.com/marekhanus)]\n1. [Novelis](https://www.novelis.com/)[[@Rushabh-Garambha](https://github.com/Rushabh-Garambha)]\n1. [Nutanix](https://www.nutanix.com/)\n1. [OdysseyPrime](https://www.goprime.io/) [[@davideberdin](https://github.com/davideberdin)]\n1. [OfferUp](https://offerupnow.com)\n1. [Oisix ra daichi, Inc.](https://en.oisixradaichi.co.jp/) [[@morihaya](https://github.com/morihaya)]\n1. [OK.ru](https://ok.ru/) [[@Mikhail-M](https://github.com/Mikhail-M)]\n1. [OneFineStay](https://www.onefinestay.com) [[@slangwald](https://github.com/slangwald)]\n1. [Open Knowledge International](https://okfn.org) [@vitorbaptista](https://github.com/vitorbaptista)\n1. [OpenBlock Labs](https://www.openblocklabs.com/) [[@melotik](https://github.com/melotik), [@l-jhon](https://github.com/l-jhon) & [@nitish-91](https://github.com/nitish-91)]\n1. [Opensignal](https://www.opensignal.com) [@harrisjoseph](https://github.com/harrisjoseph)\n1. [OpenSlate](https://openslate.com) [@marcusianlevine](https://github.com/marcusianlevine)\n1. [Openverse](https://wordpress.org/openverse)\n1. [Optum](https://www.optum.com/) - [UnitedHealthGroup](https://www.unitedhealthgroup.com/) [[@fhoda](https://github.com/fhoda), [@ianstanton](https://github.com/ianstanton), [@nilaybhatt](https://github.com/NilayBhatt),[@hiteshrd](https://github.com/hiteshrd)]\n1. [Opus Interactive](https://www.opusinteractive.com/) [[@Opus-Interactive](https://github.com/Opus-Interactive)]\n1. [OrangeBank](https://www.orangebank.fr/) [[@HamzaBoukraa](https://github.com/HamzaBoukraa)]\n1. [Outcome Health](https://www.outcomehealth.com/) [[@mikethoun](https://github.com/mikethoun), [@rolandotribo](https://github.com/rolandotribo)]\n1. [Overstock](https://www.github.com/overstock) [[@mhousley](https://github.com/mhousley) & [@mct0006](https://github.com/mct0006)]\n1. [OVH](https://www.ovh.com) [[@ncrocfer](https://github.com/ncrocfer) & [@anthonyolea](https://github.com/anthonyolea)]\n1. [Pagar.me](https://pagar.me/) [[@pagarme](https://github.com/pagarme)]\n1. [Palo Alto Networks](https://www.paloaltonetworks.com/) [[@PaloAltoNetworks](https://github.com/PaloAltoNetworks)]\n1. [Pandora Media](https://www.pandora.com/) [[@Acehaidrey](https://github.com/Acehaidrey) & [@wolfier](https://github.com/wolfier)]\n1. [Paradigma Digital](https://www.paradigmadigital.com/) [[@paradigmadigital](https://github.com/paradigmadigital)]\n1. [Paraná Banco](https://paranabanco.com.br/) [[@lopesdiego12](https://github.com/lopesdiego12/)]\n1. [Pathstream](https://pathstream.com) [[@pJackDanger](https://github.com/JackDanger)]\n1. [Paxful](https://paxful.com) [[@ne1r0n](https://github.com/ne1r0n)]\n1. [PayFit](https://payfit.com) [[@pcorbel](https://github.com/pcorbel)]\n1. [PAYMILL](https://www.paymill.com/) [[@paymill](https://github.com/paymill) & [@matthiashuschle](https://github.com/matthiashuschle)]\n1. [PayPal](https://www.paypal.com/) [[@kaddynator](https://github.com/kaddynator), [@r39132](https://github.com/r39132) & [@jhsenjaliya](https://github.com/jhsenjaliya)]\n1. [Pecan](https://www.pecan.ai) [[@ohadmata](https://github.com/ohadmata)]\n1. [Pernod-Ricard](https://www.pernod-ricard.com/) [[@romain-nio](https://github.com/romain-nio)]\n1. [PEXA](https://www.pexa.com.au/) [[@andriyfedorov](https://github.com/andriyfedorov)]\n1. [Pinterest](https://www.pinterest.com/) [[@Acehaidrey](https://github.com/Acehaidrey), [@Arshrestha](https://github.com/arshrestha), [@Dinghang](https://github.com/Dinghang), [@Euccas](https://github.com/euccas), [@Yulei-li](https://github.com/yulei-li)]\n1. [Plaetos](https://www.plaetos.com/) [[@morningcloud](https://github.com/morningcloud) & [@csight](https://github.com/csight)]\n1. [Plaid](https://www.plaid.com/) [[@plaid](https://github.com/plaid), [@AustinBGibbons](https://github.com/AustinBGibbons) & [@jeeyoungk](https://github.com/jeeyoungk)]\n1. [Playbuzz](https://www.playbuzz.com/) [[@clintonboys](https://github.com/clintonboys) & [@dbn](https://github.com/dbn)]\n1. [Playsimple Games](https://playsimple.in/) [[@joshi95](https://github.com/joshi95)]\n1. [PMC](https://pmc.com/) [[@andrewm4894](https://github.com/andrewm4894)]\n1. [Polidea-ex](https://github.com/politools/) [[@potiuk](https://github.com/potiuk), [@mschickensoup](https://github.com/mschickensoup), [@mik-laj](https://github.com/mik-laj), [@turbaszek](https://github.com/turbaszek), [@michalslowikowski00](https://github.com/michalslowikowski00), [@olchas](https://github.com/olchas), [@debek](https://github.com/debek), [@FHoffmannCode](https://github.com/FHoffmannCode), [@TobKed](https://github.com/TobKed)]\n1. [Poshmark](https://www.poshmark.com)\n1. [Postmates](http://www.postmates.com) [[@syeoryn](https://github.com/syeoryn)]\n1. [Premise](http://www.premise.com) [[@jmccallum-premise](https://github.com/jmccallum-premise)]\n1. [Pretius](https://www.pretius.com) [[@lmaczulajtys](https://github.com/lmaczulajtys)]\n1. [Promofarma](https://www.promofarma.com/) [[@JavierLopezT](https://github.com/JavierLopezT)]\n1. [Pronto Tools](http://www.prontotools.io/) [[@zkan](https://github.com/zkan) & [@mesodiar](https://github.com/mesodiar)]\n1. [Prophecy.io](https://www.prophecy.io/) [[@pateash](https://github.com/pateash)]\n1. [ProSiebenSat.1](https://www.prosiebensat1.com/) [[@nicolamarangoni](https://github.com/nicolamarangoni)]\n1. [proton.ai](https://proton.ai/) [[@prmsolutions](https://github.com/prmsolutions)]\n1. [PubNub](https://pubnub.com) [[@jzucker2](https://github.com/jzucker2)]\n1. [Pura Scents](https://www.trypura.com/) [[@mfjackson](https://github.com/mfjackson)]\n1. [PXYData](https://www.pxydata.com) [[@patchus](https://github.com/patchus)]\n1. [Qliro](https://www.qliro.com) [[@kvackkvackanka](https://github.com/kvackkvackanka)]\n1. [Qoala](https://www.qoala.id) [[@gnomeria](https://github.com/gnomeria), [@qoala-engineering](https://github.com/qoala-engineering)]\n1. [Qplum](https://qplum.co) [[@manti](https://github.com/manti)]\n1. [Quantopian](https://www.quantopian.com/) [[@eronarn](https://github.com/eronarn)]\n1. [Quero Educação](https://querobolsa.com.br/) [[@joaopedro02](https://github.com/joaopedro02)]\n1. [Quick Algorithm Analytics](https://quickalgorithm.com) [[@QuickAlgorithm](https://github.com/QuickAlgorithm), [@jacojaki](https://github.com/jacojaki), [@Giuzzilla](https://github.com/Giuzzilla), [@breizh24](https://github.com/breizh24) & [@AndreaSoprani](https://github.com/andreasoprani)]\n1. [QuintoAndar](https://quintoandar.com.br) [[@quintoandar](https://github.com/quintoandar)]\n1. [Quizlet](https://quizlet.com) [[@quizlet](https://github.com/quizlet)]\n1. [Quora](https://www.quora.com/)\n1. [Raisin](https://www.raisin.com/) [[@davido912](https://github.com/davido912)]\n1. [Rakuten](https://www.rakuten.com)\n1. [Rapido](https://rapido.bike/) [[@ChethanUK](https://github.com/ChethanUK)]\n1. [Raízen](https://www.raizen.com.br/) [[@rudlac](https://github.com/rudlac) & [@guifneves](https://github.com/guifneves)]\n1. [REA Group](https://www.rea-group.com/)\n1. [Reddit](https://www.reddit.com/) [[@reddit](https://github.com/reddit/)]\n1. [Restb.ai](https://restb.ai/) [[@ferrannoguera](https://github.com/ferrannoguera)]\n1. [Retailink](https://retailink.fr/) [[@Harisonm](https://github.com/Harisonm)]\n1. [ReUp Education](https://reupeducation.com/) [[@Reup-Education](https://github.com/Reup-Education)]\n1. [Reverb](https://reverb.com)[[@reverbdotcom](https://github.com/reverbdotcom)]\n1. [Revolut](https://www.revolut.com/) [[@sztanko](https://github.com/sztanko) & [@nautilus28](https://github.com/nautilus28)]\n1. [Robinhood](https://robinhood.com) [[@vineet-rh](https://github.com/vineet-rh)]\n1. [RushOwl](https://www.rushowl.sg) [[@songyanho](https://github.com/songyanho)]\n1. [SailPoint](https://www.sailpoint.com/) [[@andrew-stein-sp](https://github.com/andrew-stein-sp)]\n1. [Scaleway](https://scaleway.com) [[@kdeldycke](https://github.com/kdeldycke)]\n1. [Scribd](https://www.scribd.com) [[@houqp](https://github.com/houqp), [@dimonchik-suvorov](https://github.com/dimonchik-suvorov) & [@zbstof](https://github.com/zbstof)]\n1. [Seasoned](https://www.seasoned.co/) [[@joshuacano](https://github.com/joshuacano)] & [[@mmyers](https://github.com/mmyers5)] & [[@tjward](https://github.com/tjward)]\n1. [Secret Escapes](https://www.secretescapes.com) [[@secretescapes](https://github.com/secretescapes)]\n1. [Semantics3](https://www.semantics3.com) [[@abishekk92](https://github.com/abishekk92)]\n1. [Sense360](https://github.com/Sense360) [[@kamilmroczek](https://github.com/KamilMroczek)]\n1. [Sentry.io](https://www.sentry.io) [[@tiopi](https://github.com/tiopi)]\n1. [Sequoia Consulting Group](https://www.sequoia.com) [[@SequoiaConsulting](https://github.com/SequoiaConsulting) & [@rawwar](https://github.com/rawwar)]\n1. [ShopBack](https://www.shopback.sg/) [[@shopback](https://github.com/shopback)]\n1. [Shopify](https://shopify.com/) [[@shopify](https://github.com/Shopify)]\n1. [Shopkick](https://shopkick.com/) [[@shopkick](https://github.com/shopkick)]\n1. [Sidecar](https://hello.getsidecar.com/) [[@getsidecar](https://github.com/getsidecar)]\n1. [SimilarWeb](https://www.similarweb.com/) [[@similarweb](https://github.com/similarweb)]\n1. [Simply Business](https://www.simplybusiness.com/) [[@simplybusiness](https://github.com/simplybusiness)]\n1. [SIRCLO](https://www.sirclo.com/)\n1. [Skai](https://www.skai.io/) [[@kenshoo](https://github.com/kenshoo)]\n1. [Skyscanner](https://www.skyscanner.net/) [[@skyscanner](https://github.com/Skyscanner)]\n1. [SMAP Energy Ltd.](https://smapenergy.com) [[@camenergydatalab](https://github.com/camenergydatalab)]\n1. [SmartNews](https://www.smartnews.com/) [[@takus](https://github.com/takus)]\n1. [SmartQ](https://www.thesmartq.com/)\n1. [SnapTravel](https://www.snaptravel.com/)\n1. [SocialCops](https://www.socialcops.com/) [[@vinayak-mehta](https://github.com/vinayak-mehta) & [@sharky93](https://github.com/sharky93)]\n1. [Société générale](https://www.societegenerale.fr/) [[@medmrgh](https://github.com/medmrgh) & [@s83](https://github.com/s83)]\n1. [Spotahome](https://www.spotahome.com/) [[@spotahome](https://github.com/spotahome)]\n1. [SpotHero](https://github.com/spothero) [[@benjigoldberg](https://github.com/benjigoldberg)]\n1. [Spotify](https://github.com/spotify) [[@znichols](https://github.com/znichols)]\n1. [Sprylab](https://www.sprylab.com) [[@MeTaNoV](https://github.com/MeTaNoV)]\n1. [Square](https://squareup.com/)\n1. [Stackspace](https://beta.stackspace.io/)\n1. [StoneCo](https://www.stone.co) [[@lgwacker](https://github.com/lgwacker)]\n1. [Strava](https://strava.com) [[@strava](https://github.com/strava), [@dhuang](https://github.com/dhuang) & [@liamstewart](https://github.com/liamstewart)]\n1. [Stripe](https://stripe.com) [[@jbalogh](https://github.com/jbalogh)]\n1. [Strongmind](https://www.strongmind.com) [[@tomchapin](https://github.com/tomchapin) & [@wongstein](https://github.com/wongstein)]\n1. [Ströer Labs](https://stroeer-labs.com) [[@mbrtargeting](https://github.com/mbrtargeting/)]\n1. [Surfline](https://www.surfline.com/) [[@jawang35](https://github.com/jawang35)]\n1. [SUSE](https://www.suse.com) [[@ruospalo](https://github.com/ruospalo)]\n1. [Swisscom](https://www.swisscom.ch) [[@JulesTriomphe](https://github.com/JulesTriomphe)]\n1. [Syapse](https://www.syapse.com/) [[@zedmor](https://github.com/zedmor)]\n1. [T2 Systems](http://t2systems.com) [[@unclaimedpants](https://github.com/unclaimedpants)]\n1. [Tails.com](https://tails.com/) [[@alanmcruickshank](https://github.com/alanmcruickshank)]\n1. [Talkdesk](https://www.talkdesk.com)\n1. [Tapsi](https://tapsi.ir/)\n1. [TEK](https://www.tek.fi/en) [[@telac](https://github.com/telac)]\n1. [Tekmetric](https://www.tekmetric.com/)\n1. [Telefonica Innovation Alpha](https://www.alpha.company/) [[@Alpha-Health](https://github.com/Alpha-health)]\n1. [Telia Company](https://www.teliacompany.com/en)\n1. [Ternary Data](https://ternarydata.com/) [[@mhousley](https://github.com/mhousley), [@JoeReis](https://github.com/JoeReis)]\n1. [Tesla](https://www.tesla.com/) [[@thoralf-gutierrez](https://github.com/thoralf-gutierrez)]\n1. [Tessian](https://www.tessian.com/) [[@ChethanUK](https://github.com/ChethanUK)]\n1. [TextNow](https://www.textnow.com/)\n1. [The Climate Corporation](https://climate.com/) [[@jmelching](https://github.com/jmelching)]\n1. [The Dyrt](https://thedyrt.com/)\n1. [The Home Depot](https://www.homedepot.com/) [[@apekshithr](https://github.com/apekshithr)]\n1. [THE ICONIC](https://www.theiconic.com.au/) [[@revathijay](https://github.com/revathijay), [@ilikedata](https://github.com/ilikedata)]\n1. [theScore](https://www.thescore.com/) [[@kristenmalikk](https://github.com/kristenmalikk)]\n1. [Thinking Machines](https://thinkingmachin.es) [[@marksteve](https://github.com/marksteve)]\n1. [Thinknear](https://www.thinknear.com/) [[@d3cay1](https://github.com/d3cay1), [@ccson](https://github.com/ccson), & [@ababian](https://github.com/ababian)]\n1. [ThoughtWorks](https://www.thoughtworks.com/) [[@sann3](https://github.com/sann3)]\n1. [ThredUP](https://www.thredup.com/) [[@kosteev](https://github.com/kosteev)]\n1. [Thumbtack](https://www.thumbtack.com/) [[@kamalacharya](https://github.com/kamalacharya), [@dwjoss](https://github.com/dwjoss)]\n1. [Tictail](https://tictail.com/)\n1. [Tile](https://tile.com/) [[@ranjanmanish](https://github.com/ranjanmanish)]\n1. [Tinder](https://tinder.com/) [[@kbendick](https://github.com/kbendick)]\n1. [Tink](https://tink.com/) [[@tink-ab](https://github.com/tink-ab)]\n1. [Tiqets](https://www.tiqets.com/) [[@prakharcode](https://github.com/prakharcode), [@akocukcu](https://github.com/akocukcu), [@sbenza](https://github.com/sbenza), [@larissa-de-moura](https://github.com/larissa-de-moura)]\n1. [TokenAnalyst](https://github.com/tokenanalyst) [[@simonohanlon101](https://github.com/simonohanlon101), [@ankitchiplunkar](https://github.com/ankitchiplunkar), [@sidshekhar](https://github.com/sidshekhar), [@sp6pe](https://github.com/sp6pe)]\n1. [Tokopedia](https://www.tokopedia.com/) [[@tokopedia](https://github.com/tokopedia)]\n1. [Topgolf](https://topgolf.com/)[[@BhaveshSK](https://github.com/BhaveshSK)]\n1. [Toplyne](https://toplyne.io)[[@Toplyne](https://github.com/Toplyne/)]\n1. [Trade Republic](https://traderepublic.com/)\n1. [Trakken](https://www.trkkn.com/) [[@itroulli](https://github.com/itroulli), [@gthar](https://github.com/gthar), [@qulo](https://github.com/qulo), [@Oscar-Rod](https://github.com/Oscar-Rod), [@kondla](https://github.com/kondla), [@semuar](https://github.com/semuar), [@ManuelFreytag](https://github.com/ManuelFreytag)\n1. [Travix](https://www.travix.com/)\n1. [Trocafone](https://www.trocafone.com/) [[@idontdomath](https://github.com/idontdomath) & [@gseva](https://github.com/gseva) & [@ordonezf](https://github.com/ordonezf) & [@PalmaLeandro](https://github.com/PalmaLeandro)]\n1. [TruFactor](https://trufactor.io/) [[@gholmes](https://github.com/gholmes) & [@angadsingh](https://github.com/angadsingh/)]\n1. [Twilio](https://www.twilio.com/) [[@twilio](https://github.com/twilio)]\n1. [Twine Labs](https://www.twinelabs.com/) [[@ivorpeles](https://github.com/ivorpeles)]\n1. [Twitter](https://www.twitter.com/) [[@aoen](https://github.com/aoen)]\n1. [Ubisoft](https://www.ubisoft.com/) [[@Walkoss](https://github.com/Walkoss)]\n1. [Udacity](https://www.udacity.com/) [[@dandikunited](https://github.com/DandikUnited), [@simon-uc](https://github.com/simon-uc)]\n1. [Uh!ive](https://uh.live/) [[@merwan](https://github.com/merwan/)]\n1. [Umami Collective](https://umamicollective.com) [[@juanuicich](https://github.com/juanuicich)]\n1. [Unacast](https://unacast.com)[[@unacast](https://github.com/unacast)]\n1. [United Airlines](https://www.united.com/) [[@ilopezfr](https://github.com/ilopezfr)]\n1. [Upsight](https://www.upsight.com)\n1. [US Bank](https://www.usbank.com) [@BShraman]\n1. [USC Graduate School, University of Southern California](https://graduateschool.usc.edu/) [[@abhilash1in](https://github.com/abhilash1in), [@sudarshansunder](https://github.com/sudarshansunder)]\n1. [uSmart Securities](https://www.usmartsecurities.com/hk/en/) [[@yangrong688](https://github.com/yangrong688)]\n1. [VeeR VR](https://veer.tv) [[@pishilong](https://github.com/pishilong)]\n1. [Veikkaus](https://www.veikkaus.fi) [[@hixus](https://github.com/hixus)]\n1. [Vente-Exclusive.com](http://www.vente-exclusive.com/) [[@alexvanboxel](https://github.com/alexvanboxel)]\n1. [Vestiaire Collective](https://www.vestiairecollective.com/) [[@AdriMarteau](https://github.com/AdriMarteau), [@benbenbang](https://github.com/benbenbang)]\n1. [Vevo](https://www.vevo.com/) [[@csetiawan](https://github.com/csetiawan) & [@jerrygillespie](https://github.com/jerrygillespie)]\n1. [Vidio](https://www.vidio.com/)\n1. [Vidora](https://www.vidora.com/)\n1. [Ville de Montréal](http://ville.montreal.qc.ca/) [[@VilledeMontreal](https://github.com/VilledeMontreal/)]\n1. [Viscovery](https://www.viscovery.com) [[@Jasonnor](https://github.com/Jasonnor), [@goan15910](https://github.com/goan15910), [@elic](https://github.com/elic), [@kertansul](https://github.com/kertansul)]\n1. [VLMedia](https://www.vlmedia.com.tr) [[@iercan](https://github.com/iercan)]\n1. [Vnomics](https://github.com/vnomics) [[@lpalum](https://github.com/lpalum)]\n1. [Vodafone](https://www.vodafone.com) [[@nijanthanvijayakumar](https://github.com/nijanthanvijayakumar)]\n1. [Vroom](https://www.vroom.com) [[@liu431](https://github.com/liu431)]\n1. [Walmart Labs](https://www.walmartlabs.com) [[@bharathpalaksha](https://github.com/bharathpalaksha), [@vipul007ravi](https://github.com/vipul007ravi), [@pateash](https://github.com/pateash)]\n1. [Wayfair](https://www.wayfair.com) [[@wayfair](https://github.com/wayfair)]\n1. [Waze](https://www.waze.com) [[@waze](https://github.com/wazeHQ)]\n1. [WePay](http://www.wepay.com) [[@criccomini](https://github.com/criccomini) & [@mtagle](https://github.com/mtagle)]\n1. [WeTransfer](https://github.com/WeTransfer) [[@coredipper](https://github.com/coredipper) & [@higee](https://github.com/higee) & [@azclub](https://github.com/azclub)]\n1. [Whistle Labs](http://www.whistle.com) [[@ananya77041](https://github.com/ananya77041)]\n1. [Wikimedia Foundation](https://wikimediafoundation.org) [[@tullis](https://github.com/tullis)]\n1. [Wildlifestudios](https://wildlifestudios.com/)\n1. [Wise](https://wise.com) [[@koszti](https://github.com/koszti)]\n1. [WiseBanyan](https://wisebanyan.com/)\n1. [Wisr](https://wisr.com.au/) [[@fsodano](https://github.com/fsodano) & [@vincyf1](https://github.com/vincyf1)]\n1. [Wix](https://www.wix.com/)\n1. [Wooga](https://www.wooga.com/)\n1. [Workpath GmbH](https://www.workpath.com/en/product) [[WorkpathHQ](https://github.com/WorkpathHQ)]\n1. [WorldRemit](https://www.worldremit.com/) [[@boittega](https://github.com/boittega)]\n1. [Wrike](https://www.wrike.com) [[@eliseealex](https://github.com/eliseealex) & [teoretic6](https://github.com/Teoretic6)]\n1. [Xero](https://www.xero.com/) [[@yan9yu](https://github.com/yan9yu) & [adamantnz](https://github.com/adamantnz/)]\n1. [Xoom](https://www.xoom.com/)\n1. [Yahoo!](https://www.yahoo.com/)\n1. [Yektanet!](https://yektanet.com/)\n1. [Yieldr](https://www.yieldr.com/) [[@ggeorgiadis](https://github.com/ggeorgiadis)]\n1. [Zalando](https://tech.zalando.com) [[@zalando](https://github.com/zalando) & [@zalando-incubator](https://github.com/zalando-incubator)]\n1. [Zapier](https://www.zapier.com) [[@drknexus](https://github.com/drknexus) & [@statwonk](https://github.com/statwonk)]\n1. [Zego](https://www.zego.com/) [[@ruimffl](https://github.com/ruimffl), [@james-welly](https://github.com/james-welly), [@ken-payne](https://github.com/ken-payne)]\n1. [Zendesk](https://www.github.com/zendesk)\n1. [Zenly](https://zen.ly) [[@cerisier](https://github.com/cerisier) & [@jbdalido](https://github.com/jbdalido)]\n1. [Zerodha](https://zerodha.com/) [[@johnnybravo-xyz](https://github.com/johnnybravo-xyz)]\n1. [Zymergen](https://www.zymergen.com/)\n1. [Zynga](https://www.zynga.com)\n1. [Ørsted](https://orsted.com/en) [[@arjunanan6](https://github.com/arjunanan6)]\n1. [好大夫在线](https://www.haodf.com/) [[@leiguorui](https://github.com/leiguorui)]\n"
        },
        {
          "name": "ISSUE_TRIAGE_PROCESS.rst",
          "type": "blob",
          "size": 18.8583984375,
          "content": " .. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\nIssue reporting and resolution process\n======================================\n\nThis document explains the issue tracking and triage process within Apache\nAirflow including labels, milestones, and priorities as well as the process\nof resolving issues.\n\nAn unusual element of the Apache Airflow project is that you can open a PR\nto fix an issue or make an enhancement, without needing to open an issue first.\nThis is intended to make it as easy as possible to contribute to the project.\n\nUsually, users report `Issues <https://github.com/apache/airflow/issues>`_ where they describe\nthe issues they think are Airflow issues and should be solved. There are two kinds of issues:\n\n* Bugs - when the user thinks the reported issue is a bug in Airflow\n* Features - when there are small features that the user would like to see in Airflow\n\nWe have `templates <https://github.com/apache/airflow/tree/main/.github/ISSUE_TEMPLATE>`_ for both types\nof issues defined in Airflow.\n\nHowever, important part of our issue reporting process are\n`GitHub Discussions <https://github.com/apache/airflow/discussions>`_ . Issues should represent\nclear, small feature requests or reproducible bugs which can/should be either implemented or fixed.\nUsers are encouraged to open discussions rather than issues if there are no clear, reproducible\nsteps, or when they have troubleshooting problems, and one of the important points of issue triaging is\nto determine if the issue reported should be rather a discussion. Converting an issue to a discussion\nwhile explaining the user why is an important part of triaging process.\n\nResponding to issues/discussions (relatively) quickly\n'''''''''''''''''''''''''''''''''''''''''''''''''''''\n\nIt is vital to provide rather quick feedback to issues and discussions opened by our users, so that they\nfeel listened to rather than ignored. Even if the response is \"we are not going to work on it because ...\",\nor \"converting this issue to discussion because ...\" or \"closing because it is a duplicate of #xxx\", it is\nfar more welcoming than leaving issues and discussions unanswered. Sometimes issues and discussions are\nanswered by other users (and this is cool) but if an issue/discussion is not responded to for a few days or\nweeks, this gives an impression that the user was ignored and that the Airflow project is unwelcoming.\n\nWe strive to provide relatively quick responses to all such issues and discussions. Users should exercise\npatience while waiting for those (knowing that people might be busy, on vacations etc.) however they should\nnot wait weeks until someone looks at their issues.\n\n\nIssue Triage Team\n''''''''''''''''''\n\nWhile many of the issues can be responded to by other users and committers, the committer team is not\nbig enough to handle all such requests and sometimes they are busy with implementing or resolving more important issues.\nTherefore, some people who are regularly contributing and helping other users and shown their deep interest\nin the project can be invited to join the triage team.\n`the .asf.yaml <.asf.yaml>`_ file in the ``collaborators`` section.\n\nCommitters can invite people to become members of the triage team if they see that the users are already\nhelping and responding to issues and when they see the users are involved regularly. But you can also ask\nto become a member of the team (on devlist) if you can show that you have done that and when you want to have\nmore ways to help others.\n\nThe triage team members do not have committer privileges but they can\nassign, edit, and close issues and pull requests without having capabilities to merge the code. They can\nalso convert issues into discussions and back. The expectation for the issue triage team is that they\nspend a bit of their time on those efforts. Triaging means not only assigning the labels but often responding\nto the issues and answering user concerns or if additional input is needed - tagging the committers or other community members who might be able to help provide more complete answers.\n\nBeing an active and helpful member of the \"Issue Triage Team\" is actually one of the paths towards\nbecoming a committer. By actively helping the users, triaging the issues, responding to them and\ninvolving others (when needed) shows that you are not only willing to help our users and the community,\nbut are also ready to learn about parts of the projects you are not actively contributing to - all of that\nare super valuable components of being eligible to `become a committer <COMMITTERS.rst>`_.\n\nIf you are a member of the triage team and not able to make any commitment, it's best to ask to have yourself\nremoved from the triage team.\n\nBTW. Every committer is pretty much automatically part of the \"Issue Triage Team\" - so if you are committer,\nfeel free to follow the process for every issue you stumble upon.\n\nActions that can be taken by the issue triager\n''''''''''''''''''''''''''''''''''''''''''''''\n\nThere are several actions an issue triager might take:\n\n* Closing and issue with \"invalid\" label explaining why it is closed in case the issue is invalid. This\n  should be accompanied by information that we can always re-open an issue if our understanding was wrong\n  or if the user provides more information.\n\n* Converting an issue to a discussion, if it is not very likely it is an Airflow issue or when it is not\n  responsible, or when it is a bigger feature proposal requiring discussion or when it's really users\n  troubleshooting or when the issue description is not at all clear. This also involves inviting the user\n  to a discussion if more information might change it.\n\n* Assigning the issue to a milestone, if the issue seems important enough that it should likely be looked\n  at before the next release but there is not enough information or doubts on why and what can be fixed.\n  Usually we assign to the the next bugfix release - then, no matter what the issue will be looked at\n  by the release manager and it might trigger additional actions during the release preparation.\n  This is usually followed by one of the actions below.\n\n* Fixing the issue in a PR if you see it is easy to fix. This is a great way also to learn and\n  contribute to parts that you usually are not contributing to, and sometimes it is surprisingly easy.\n\n* Assigning \"good first issue\" label if an issue is clear but not important to be fixed immediately, This\n  often lead to contributors picking up the issues when they are interested. This can be followed by assigning\n  the user who comments \"I want to work on this\" in the issue (which is most welcome).\n\n* Asking the user for additional information if it is needed to perform further investigations. This should\n  be accompanied by assigning ``pending response`` label so that we can clearly see the issues that need\n  extra information.\n\n* Calling other people who might be knowledgeable in the area by @-mentioning them in a comment.\n\n* Assigning other labels to the issue as described below.\n\n\nLabels\n''''''\n\nSince Apache Airflow uses \"GitHub Issues\" and \"Github Discussions\" as the\nissue tracking systems, the use of labels is extensive. Though issue\nlabels tend to change over time based on components within the project,\nthe majority of the ones listed below should stand the test of time.\n\nThe intention with the use of labels with the Apache Airflow project is\nthat they should ideally be non-temporal in nature and primarily used\nto indicate the following elements:\n\n**Kind**\n\nThe \"kind\" labels indicate \"what kind of issue it is\". The most\ncommonly used \"kind\" labels are: bug, feature, documentation, or task.\n\nTherefore, when reporting an issue, the label of ``kind:bug`` is to\nindicate a problem with the functionality, whereas the label of\n``kind:feature`` is a desire to extend the functionality.\n\nThere has been discussion within the project about whether to separate\nthe desire for \"new features\" from \"enhancements to existing features\",\nbut in practice most \"feature requests\" are actually enhancement requests,\nso we decided to combine them both into ``kind:feature``.\n\nThe ``kind:task`` is used to categorize issues which are\nidentified elements of work to be done, primarily as part of a larger\nchange to be done as part of an AIP or something which needs to be cleaned\nup in the project.\n\nIssues of ``kind:documentation`` are for changes which need to be\nmade to the documentation within the project.\n\n\n**Area**\n\nThe \"area\" set of labels should indicate the component of the code\nreferenced by the issue. At a high level, the biggest areas of the project\nare: Airflow Core and Airflow Providers, which are referenced by ``area:core``\nand ``area:providers``. This is especially important since these are now\nbeing released and versioned independently.\n\nThere are more detailed areas of the Core Airflow project such as Scheduler, Webserver,\nAPI, UI, Logging, and Metrics, which are all conceptually under the\n\"Airflow Core\" area of the project.\n\nSimilarly within Airflow Providers, the larger providers such as Apache, AWS, Azure,\nand Google who have many hooks and operators within them, have labels directly\nassociated with them such as ``provider:amazon-aws``, ``provider:microsoft-azure``, and ``provider:google``.\n\nThese make it easier for developers working on a single provider to\ntrack issues for that provider.\n\nNote: each provider has it's own unique label. It is possible for issue to be tagged with more than 1 provider label.\n\nMost issues need a combination of \"kind\" and \"area\" labels to be actionable.\nFor example:\n\n* Feature request for an additional API would have ``kind:feature`` and ``area:API``\n* Bug report on the User Interface would have ``kind:bug`` and ``area:UI``\n* Documentation request on the Kubernetes Executor, would have ``kind:documentation`` and ``area:kubernetes``\n\nResponse to issues\n''''''''''''''''''\n\nOnce an issue has been created on the Airflow project, someone from the\nAirflow team or the Airflow community typically responds to this issue.\nThis response can have multiple elements.\n\n**Priority**\n\nAfter significant discussion about the different priority schemes currently\nbeing used across various projects, we decided to use a priority scheme based\non the Kubernetes project, since the team felt it was easier for people to\nunderstand.\n\nTherefore, the priority labels used are:\n\n* ``priority:critical``: Showstopper bug that should be resolved immediately and a patch issued as soon as possible. Typically, this is because it affects most users and would take down production systems.\n* ``priority:high``: A high priority bug that affects many users and should be resolved quickly, but can wait for the next scheduled patch release.\n* ``priority:medium``: A bug that should be fixed before the next release, but would not block a release if found during the release process.\n* ``priority:low``: A bug with a simple workaround or a nuisance that does not stop mainstream functionality.\n\n\nIt's important to use priority labels effectively so we can triage incoming issues\nappropriately and make sure that when we release a new version of Airflow,\nwe can ship a release confident that there are no \"production blocker\" issues in it.\n\nThis applies to both Core Airflow as well as the Airflow Providers. With the separation\nof the Providers release from Core Airflow, a ``priority:critical`` bug in a single\nprovider could trigger an unplanned patch release of the Airflow Providers.\n\n\n**Milestones**\n\nThe key temporal element in the issue triage process is the concept of milestones.\nThis is critical for release management purposes and will be used represent upcoming\nrelease targets.\n\nIssues currently being resolved will get assigned to one of the upcoming releases.\nFor example a feature request may be targeted for the next feature release milestone\nsuch as ``2.x``, where a bug may be targeted for the next patch release milestone\nsuch as ``2.x.y``.\n\nIn the interest of being precise, when an issue is tagged with a milestone, it\nrepresents that it will be considered for that release, not that it is committed to\na release. Once a PR is created to fix that issue and when that PR is tagged with a\nmilestone, it implies that the PR is intended to released in that milestone.\n\nPlease note that Airflow Core and Airflow Providers are now released and\nversioned separately. The use of milestones as described above is directed towards\nAirflow Core releases.\n\n\n**Transient Labels**\n\nSometimes, there is more information needed to either understand the issue or\nto be able to reproduce the issue. Typically, this may require a response to the\nissue creator asking for more information, with the issue then being tagged with\nthe label ``pending-response``.\nAlso, during this stage, additional labels may be added to the issue to help\nclassification and triage, such as ``affected_version`` and ``area``.\n\nNew issues are automatically assigned with ``needs-triage`` label. This labels goal\nis to help us detect issues that are waiting for initial triage. The label will be removed by the triager\nonce the issue is accepted (and assigned with relevant kind and area labels). This sometimes can take a while as we might\nask for other members of the community for consultation or ask for further information from the issue author.\nRemoving the ``needs-triage`` label means that the issue has been accepted and awaits implementation (no further triage action required),\nas long as the ``needs-triage`` label remains the triage team will keep an eye on the issue and check periodically\nif it needs to be accepted or closed/converted to Github Discussion.\n``needs-triage`` label may also be applied manually by committers if they think a further action from the triage team is required.\n\n**area Label**\n\n``area:providers`` must be present for any provider issue. The ``provider:x`` is specifying the exact provider.\nWe have ``provider:x`` for any provider that we own.\n``area:helm-chart`` must be for any helm chart issue.\n``area:core`` must be for any core issue. Additional labels like ``area:scheduler``, ``area:UI`` is specifying the exact\ncore area relevant.\n\nThis method allow us to quickly filter issues by the 3 major components of our code base: core, providers and helm-chart.\n\n**affected version Label**\n\nThe ``affected_version:x`` will never be present with ``kind:feature`` as feature requests are not related to specific Airflow version.\nFor bugs, The ``affected_version:x`` is expected to be used with core issues thus normally it appears with ``area:core`` label.\nWhen issue is reproducible on multiple Airflow versions we apply only the latest version with the label.\nThis policy is best effort, we should try to have as little exceptions as possible.\n\n**Good First Issue**\n\nIssues which are relatively straight forward to solve, will be tagged with\nthe ``good first issue`` label.\n\nThe intention here is to galvanize contributions from new and inexperienced\ncontributors who are looking to contribute to the project. This has been successful\nin other open source projects and early signs are that this has been helpful in the\nAirflow project as well.\n\nIdeally, these issues only require one or two files to be changed. The intention\nhere is that incremental changes to existing files are a lot easier for a new\ncontributor as compared to adding something completely new.\n\nAnother possibility here is to add \"how to fix\" in the comments of such issues, so\nthat new contributors have a running start when then pick up these issues.\n\n\n**Timeliness**\n\nFor the sake of quick responses, the general \"soft\" rule within the Airflow project\nis that if there is no assignee, anyone can take an issue to solve.\n\nHowever, this depends on timely resolution of the issue by the assignee. The\nexpectation is as follows:\n\n* If there is no activity on the issue for 2 weeks, the assignee will be reminded about the issue and asked if they are still working on it.\n* If there is no activity even after 1 more week, the issue will be unassigned, so that someone else can pick it up and work on it.\n\n\nThere is a similar process when additional information is requested from the\nissue creator. After the pending-response label has been assigned, if there is no\nfurther information for a period of 1 month, the issue will be automatically closed.\n\n\n**Invalidity**\n\nAt times issues are marked as invalid and later closed because of one of the\nfollowing situations:\n\n* The issue is a duplicate of an already reported issue. In such cases, the latter issue is marked as ``duplicate``.\n* Despite attempts to reproduce the issue to resolve it, the issue cannot be reproduced by the Airflow team based on the given information. In such cases, the issue is marked as ``Can't Reproduce``.\n* In some cases, the original creator realizes that the issue was incorrectly reported and then marks it as ``invalid``. Also, a committer could mark it as ``invalid`` if the issue being reported is for an unsupported operation or environment.\n* In some cases, the issue may be legitimate, but may not be addressed in the short to medium term based on current project priorities or because this will be irrelevant because of an upcoming change. The committer could mark this as ``wontfix`` to set expectations that it won't be directly addressed in the near term.\n\n**GitHub Discussions**\n\nIssues should represent clear feature requests which can/should be implemented. If the idea is vague or can be solved with easier steps\nwe normally convert such issues to discussions in the Ideas category.\nIssues that seems more like support requests are also converted to discussions in the Q&A category.\nWe use judgment about which Issues to convert to discussions, it's best to always clarify with a comment why the issue is being converted.\nNote that we can always convert discussions back to issues.\n\n\n**Stale Policy**\n\nAs time passes bug reports that have been accepted may be out dated.\nBot will scan older bug reports and if the report is inactive it will comment\nand ask the author to recheck if the bug is still reproducible on latest version.\nIf the issue is reconfirmed triage team will check if labels needs to be updated (for example: ``reported_version`` label)\nIf no one respond after some time, we will consider the issue as resolved (may have already been fixed) and bot will resolve the issue.\nThe exact timeframes for each one of the actions is subject to change from time to time.\nThe updated values can be checked in ``.github/workflow`` where we define the bots policy.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 13.2900390625,
          "content": "                              Apache License\n                        Version 2.0, January 2004\n                     http://www.apache.org/licenses/\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n1. Definitions.\n\n   \"License\" shall mean the terms and conditions for use, reproduction,\n   and distribution as defined by Sections 1 through 9 of this document.\n\n   \"Licensor\" shall mean the copyright owner or entity authorized by\n   the copyright owner that is granting the License.\n\n   \"Legal Entity\" shall mean the union of the acting entity and all\n   other entities that control, are controlled by, or are under common\n   control with that entity. For the purposes of this definition,\n   \"control\" means (i) the power, direct or indirect, to cause the\n   direction or management of such entity, whether by contract or\n   otherwise, or (ii) ownership of fifty percent (50%) or more of the\n   outstanding shares, or (iii) beneficial ownership of such entity.\n\n   \"You\" (or \"Your\") shall mean an individual or Legal Entity\n   exercising permissions granted by this License.\n\n   \"Source\" form shall mean the preferred form for making modifications,\n   including but not limited to software source code, documentation\n   source, and configuration files.\n\n   \"Object\" form shall mean any form resulting from mechanical\n   transformation or translation of a Source form, including but\n   not limited to compiled object code, generated documentation,\n   and conversions to other media types.\n\n   \"Work\" shall mean the work of authorship, whether in Source or\n   Object form, made available under the License, as indicated by a\n   copyright notice that is included in or attached to the work\n   (an example is provided in the Appendix below).\n\n   \"Derivative Works\" shall mean any work, whether in Source or Object\n   form, that is based on (or derived from) the Work and for which the\n   editorial revisions, annotations, elaborations, or other modifications\n   represent, as a whole, an original work of authorship. For the purposes\n   of this License, Derivative Works shall not include works that remain\n   separable from, or merely link (or bind by name) to the interfaces of,\n   the Work and Derivative Works thereof.\n\n   \"Contribution\" shall mean any work of authorship, including\n   the original version of the Work and any modifications or additions\n   to that Work or Derivative Works thereof, that is intentionally\n   submitted to Licensor for inclusion in the Work by the copyright owner\n   or by an individual or Legal Entity authorized to submit on behalf of\n   the copyright owner. For the purposes of this definition, \"submitted\"\n   means any form of electronic, verbal, or written communication sent\n   to the Licensor or its representatives, including but not limited to\n   communication on electronic mailing lists, source code control systems,\n   and issue tracking systems that are managed by, or on behalf of, the\n   Licensor for the purpose of discussing and improving the Work, but\n   excluding communication that is conspicuously marked or otherwise\n   designated in writing by the copyright owner as \"Not a Contribution.\"\n\n   \"Contributor\" shall mean Licensor and any individual or Legal Entity\n   on behalf of whom a Contribution has been received by Licensor and\n   subsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of\n   this License, each Contributor hereby grants to You a perpetual,\n   worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n   copyright license to reproduce, prepare Derivative Works of,\n   publicly display, publicly perform, sublicense, and distribute the\n   Work and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of\n   this License, each Contributor hereby grants to You a perpetual,\n   worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n   (except as stated in this section) patent license to make, have made,\n   use, offer to sell, sell, import, and otherwise transfer the Work,\n   where such license applies only to those patent claims licensable\n   by such Contributor that are necessarily infringed by their\n   Contribution(s) alone or by combination of their Contribution(s)\n   with the Work to which such Contribution(s) was submitted. If You\n   institute patent litigation against any entity (including a\n   cross-claim or counterclaim in a lawsuit) alleging that the Work\n   or a Contribution incorporated within the Work constitutes direct\n   or contributory patent infringement, then any patent licenses\n   granted to You under this License for that Work shall terminate\n   as of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the\n   Work or Derivative Works thereof in any medium, with or without\n   modifications, and in Source or Object form, provided that You\n   meet the following conditions:\n\n   (a) You must give any other recipients of the Work or\n       Derivative Works a copy of this License; and\n\n   (b) You must cause any modified files to carry prominent notices\n       stating that You changed the files; and\n\n   (c) You must retain, in the Source form of any Derivative Works\n       that You distribute, all copyright, patent, trademark, and\n       attribution notices from the Source form of the Work,\n       excluding those notices that do not pertain to any part of\n       the Derivative Works; and\n\n   (d) If the Work includes a \"NOTICE\" text file as part of its\n       distribution, then any Derivative Works that You distribute must\n       include a readable copy of the attribution notices contained\n       within such NOTICE file, excluding those notices that do not\n       pertain to any part of the Derivative Works, in at least one\n       of the following places: within a NOTICE text file distributed\n       as part of the Derivative Works; within the Source form or\n       documentation, if provided along with the Derivative Works; or,\n       within a display generated by the Derivative Works, if and\n       wherever such third-party notices normally appear. The contents\n       of the NOTICE file are for informational purposes only and\n       do not modify the License. You may add Your own attribution\n       notices within Derivative Works that You distribute, alongside\n       or as an addendum to the NOTICE text from the Work, provided\n       that such additional attribution notices cannot be construed\n       as modifying the License.\n\n   You may add Your own copyright statement to Your modifications and\n   may provide additional or different license terms and conditions\n   for use, reproduction, or distribution of Your modifications, or\n   for any such Derivative Works as a whole, provided Your use,\n   reproduction, and distribution of the Work otherwise complies with\n   the conditions stated in this License.\n\n5. Submission of Contributions. Unless You explicitly state otherwise,\n   any Contribution intentionally submitted for inclusion in the Work\n   by You to the Licensor shall be under the terms and conditions of\n   this License, without any additional terms or conditions.\n   Notwithstanding the above, nothing herein shall supersede or modify\n   the terms of any separate license agreement you may have executed\n   with Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade\n   names, trademarks, service marks, or product names of the Licensor,\n   except as required for reasonable and customary use in describing the\n   origin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or\n   agreed to in writing, Licensor provides the Work (and each\n   Contributor provides its Contributions) on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n   implied, including, without limitation, any warranties or conditions\n   of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n   PARTICULAR PURPOSE. You are solely responsible for determining the\n   appropriateness of using or redistributing the Work and assume any\n   risks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory,\n   whether in tort (including negligence), contract, or otherwise,\n   unless required by applicable law (such as deliberate and grossly\n   negligent acts) or agreed to in writing, shall any Contributor be\n   liable to You for damages, including any direct, indirect, special,\n   incidental, or consequential damages of any character arising as a\n   result of this License or out of the use or inability to use the\n   Work (including but not limited to damages for loss of goodwill,\n   work stoppage, computer failure or malfunction, or any and all\n   other commercial damages or losses), even if such Contributor\n   has been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing\n   the Work or Derivative Works thereof, You may choose to offer,\n   and charge a fee for, acceptance of support, warranty, indemnity,\n   or other liability obligations and/or rights consistent with this\n   License. However, in accepting such obligations, You may act only\n   on Your own behalf and on Your sole responsibility, not on behalf\n   of any other Contributor, and only if You agree to indemnify,\n   defend, and hold each Contributor harmless for any liability\n   incurred by, or claims asserted against, such Contributor by reason\n   of your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\nAPPENDIX: How to apply the Apache License to your work.\n\n   To apply the Apache License to your work, attach the following\n   boilerplate notice, with the fields enclosed by brackets \"[]\"\n   replaced with your own identifying information. (Don't include\n   the brackets!)  The text should be enclosed in the appropriate\n   comment syntax for the file format. We also recommend that a\n   file or class name and description of purpose be included on the\n   same \"printed page\" as the copyright notice for easier\n   identification within third-party archives.\n\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n============================================================================\n   APACHE AIRFLOW SUBCOMPONENTS:\n\n   The Apache Airflow project contains subcomponents with separate copyright\n   notices and license terms. Your use of the source code for the these\n   subcomponents is subject to the terms and conditions of the following\n   licenses.\n\n\n========================================================================\nThird party Apache 2.0 licenses\n========================================================================\n\nThe following components are provided under the Apache 2.0 License.\nSee project link for details. The text of each license is also included\nat 3rd-party-licenses/LICENSE-[project].txt.\n\n    (ALv2 License) hue v4.3.0 (https://github.com/cloudera/hue/)\n    (ALv2 License) jqclock v2.3.0 (https://github.com/JohnRDOrazio/jQuery-Clock-Plugin)\n    (ALv2 License) bootstrap3-typeahead v4.0.2 (https://github.com/bassjobsen/Bootstrap-3-Typeahead)\n    (ALv2 License) connexion v2.7.0 (https://github.com/zalando/connexion)\n\n========================================================================\nMIT licenses\n========================================================================\n\nThe following components are provided under the MIT License. See project link for details.\nThe text of each license is also included at 3rd-party-licenses/LICENSE-[project].txt.\n\n    (MIT License) jquery v3.5.1 (https://jquery.org/license/)\n    (MIT License) dagre-d3 v0.6.4 (https://github.com/cpettitt/dagre-d3)\n    (MIT License) bootstrap v3.4.1 (https://github.com/twbs/bootstrap/)\n    (MIT License) d3-tip v0.9.1 (https://github.com/Caged/d3-tip)\n    (MIT License) dataTables v1.10.25 (https://datatables.net)\n    (MIT License) normalize.css v3.0.2 (http://necolas.github.io/normalize.css/)\n    (MIT License) ElasticMock v1.3.2 (https://github.com/vrcmarcos/elasticmock)\n    (MIT License) MomentJS v2.24.0 (http://momentjs.com/)\n    (MIT License) eonasdan-bootstrap-datetimepicker v4.17.49 (https://github.com/eonasdan/bootstrap-datetimepicker/)\n\n========================================================================\nBSD 3-Clause licenses\n========================================================================\nThe following components are provided under the BSD 3-Clause license. See project links for details.\nThe text of each license is also included at 3rd-party-licenses/LICENSE-[project].txt.\n\n    (BSD 3 License) d3 v5.16.0 (https://d3js.org)\n    (BSD 3 License) d3-shape v2.1.0 (https://github.com/d3/d3-shape)\n    (BSD 3 License) cgroupspy 0.2.1 (https://github.com/cloudsigma/cgroupspy)\n\n========================================================================\nSee 3rd-party-licenses/LICENSES-ui.txt for packages used in `/airflow/www`\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 0.7705078125,
          "content": "Apache Airflow\nCopyright 2016-2025 The Apache Software Foundation\n\nThis product includes software developed at The Apache Software\nFoundation (http://www.apache.org/).\n\n=======================================================================\n\nhue:\n-----\nThis product contains a modified portion of 'Hue' developed by Cloudera, Inc.\n(https://github.com/cloudera/hue/).\n\n* Copyright 2009-2017 Cloudera Inc.\n\n\nFlask App Builder:\n-----\nThis product contains a modified portion of 'Flask App Builder' developed by Daniel Vaz Gaspar.\n(https://github.com/dpgaspar/Flask-AppBuilder).\n\n* Copyright 2013, Daniel Vaz Gaspar\n\nChakra UI:\n-----\nThis product contains a modified portion of 'Chakra UI' developed by Segun Adebayo.\n(https://github.com/chakra-ui/chakra-ui).\n\n* Copyright 2019, Segun Adebayo\n"
        },
        {
          "name": "PROVIDERS.rst",
          "type": "blob",
          "size": 23.810546875,
          "content": " .. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n************************\nApache Airflow Providers\n************************\n\n**The outline for this document in GitHub is available at top-right corner button (with 3-dots and 3 lines).**\n\nWhat is a provider?\n===================\n\nAirflow 2.0 introduced the concept of providers. Providers are packages that contain integrations with\nexternal systems. They are meant to extend capabilities of the core \"Apache Airflow\". Thus they are\npart of the vision of Airflow-as-a-Platform - where the Airflow Core provides basic data-workflow scheduling\nand management capabilities and can be extended by implementing Open APIs Airflow supports, adding\nPlugins that can add new features to the Core, and adding Providers that allow to interact with external\nsystems.\n\nThe providers are released separately from the core Airflow and they are versioned independently. The\nways how providers can extend the Airflow Core, including the types of providers, can be found at the\n`Providers page <https://airflow.apache.org/docs/apache-airflow-providers/index.html>`_. You can also find\nout there, how you can create your own provider.\n\nProviders can be maintained and released by the Airflow community or by 3rd-party teams. In any case -\nwhether community-managed, or 3rd-party managed - they are released independently of the Airflow Core package.\n\nWhen community releases the Airflow Core, it is released together with constraints, those constraints use\nthe latest released version of providers, and our published convenience images contain a subset of most\npopular community providers. However our users are free to upgrade and downgrade providers independently of\nthe Airflow Core version as they see fit, as long as it does not cause conflicting dependencies.\n\nYou can read more about it in the\n`Installation and upgrade scenarios <https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html#installation-and-upgrade-scenarios>`_\nchapter of our user documentation.\n\nCommunity managed providers\n===========================\n\nWhen providers are accepted by the community, the process of managing and releasing them must follow the\nApache Software Foundation rules and policies. This is especially, about accepting contributions and\nreleasing new versions of the providers. This means that the code changes in the providers must be\nreviewed by Airflow committers and merged when they are accepted by them. Also we must have sufficient\ntest coverage and documentation that allow us to maintain the providers, and our users to use them.\n\nThe providers - their latest version in \"main\" branch of airflow repository - are installed and tested together\nwith other community providers and one of the key properties of the community providers is that the latest\nversion of providers contribute their dependencies to constraints of Airflow, published when Airflow Core is\nreleased. This means that when users are using constraints published by Airflow, they can install all\nthe providers together and they are more likely to not interfere with each other, especially they should\nbe able to be installed together, without conflicting dependencies. This allows to add an optional\n\"extra\" to Airflow for each provider, so that the providers can be installed together with Airflow by\nspecifying the \"extra\" in the installation command.\n\nBecause of the constraint and potential conflicting dependencies, the community providers have to be regularly\nupdated and the community might decide to suspend releases of a provider if we find out that we have trouble\nwith updating the dependencies, or if we find out that the provider is not compatible with other more\npopular providers and when the popular providers are limited by the constraints of the less popular ones.\nSee the section below for more details on suspending releases of the community providers.\n\nList of all available community providers is available at the `Providers index <https://airflow.apache.org/docs/>`_.\n\n\nCommunity providers lifecycle\n=============================\n\nThis document describes the complete life-cycle of community providers - from inception and approval to\nAirflow main branch to being decommissioned and removed from the main branch in Airflow repository.\n\n.. note::\n\n   Technical details on how to manage lifecycle of providers are described in the document:\n\n   `Managing provider's lifecycle <https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/MANAGING_PROVIDERS_LIFECYCLE.rst>`_\n\n\nAccepting new community providers\n---------------------------------\n\nAccepting new community providers should be a deliberate process that requires ``[DISCUSSION]``\nfollowed by ``[VOTE]`` thread at the airflow `devlist <https://airflow.apache.org/community/#mailing-list>`_.\n\nIn case the provider is integration with an open-source software rather than service we can relax the vote\nprocedure a bit. Particularly if the open-source software is an Apache Software Foundation,\nLinux Software Foundation or similar organisation with well established governance processes that are not\nstrictly vendor-controlled, and when the software is well established an popular, it might be enough to\nhave a good and complete PR of the provider, ideally with a great test coverage, including integration tests,\nand documentation. Then it should be enough to request the provider acceptance by a ``[LAZY CONSENSUS]`` mail\non the devlist and assuming such lazy consensus is not objected by anyone in the community, the provider\nmight be merged.\n\nFor service providers, the ``[DISCUSSION]`` thread is aimed to gather information about the reasons why\nthe one who proposes the new provider thinks it should be accepted by the community. Maintaining the provider\nin the community is a burden. Contrary to many people's beliefs, code is often liability rather than asset,\nand accepting the code to be managed by the community, especially when it involves significant effort on\nmaintenance is often undesired, especially that the community consists of volunteers. There must be a really\ngood reason why we would believe that the provider is better to be maintained by the community if there\nare 3rd-party teams that can be paid to manage it on their own. We have to believe that the current\ncommunity interest is in managing the provider and that enough volunteers in the community will be\nwilling to maintain it in the future in order to accept the provider.\n\nThe ``[VOTE]`` thread is aimed to gather votes from the community on whether the provider should be accepted\nor not and it follows the usual Apache Software Foundation voting rules concerning\n`Votes on Code Modification <https://www.apache.org/foundation/voting.html#votes-on-code-modification>`_\n\nThe Ecosystem page and registries, and own resources of the 3rd-party teams are the best places to increase\nvisibility that such providers exist, so there is no \"great\" visibility achieved by getting the provider in\nthe community. Also it is often easier to advertise and promote usage of the provider by the service providers\nthemselves when they own, manage and release their provider, especially when they can synchronize releases\nof their provider with new feature, the service might get added.\n\nExamples:\n\nHuawei Cloud provider - `Discussion <https://lists.apache.org/thread/f5tk9c734wlyv616vyy8r34ymth3dqbc>`_\nCloudera provider - `Discussion <https://lists.apache.org/thread/2z0lvgj466ksxxrbvofx41qvn03jrwwb>`_, `Vote <https://lists.apache.org/thread/8b1jvld3npgzz2z0o3gv14lvtornbdrm>`_\nPgVector / Weaviate/ OpenAI provider - `Discussion <https://lists.apache.org/thread/0d669fmy4hn29h5c0wj0ottdskd77ktp>`_, `Lazy Consensus vote <https://lists.apache.org/thread/zrq6554lwobhngtwyzp7tpgnyfsxxybh>`_\nPinecone / OpenAI / Cohere provider - `Discussion <https://lists.apache.org/thread/0d669fmy4hn29h5c0wj0ottdskd77ktp>`_, `Vote <https://lists.apache.org/thread/skh32jksvcf4yx4fhhsfz8lq6w5nhfjc>`_, `VOTE Result <https://lists.apache.org/thread/oq7h2n88zfo3dzldy5w8xlp9kyngs7x8>`_\n\nNote that some providers have regular vote and some lazy consensus, please refer to the above sections for explanation why it's not the same for all providers\n\nCommunity providers release process\n-----------------------------------\n\nThe community providers are released regularly (usually every 2 weeks) in batches consisting of any providers\nthat need to be released because they changed since last release. The release manager decides which providers\nto include and whether some or all providers should be released (see the next chapter about upgrading the\nminimum version of Airflow for example the case where we release all active meaning non-suspended providers,\ntogether in a single batch). Also Release Manager decides on the version bump of the provider (depending on\nclassification, whether there are breaking changes, new features or just bugs comparing to previous version).\n\nUpgrading Minimum supported version of Airflow\n----------------------------------------------\n\nOne of the important limitations of the Providers released by the community is that we introduce the limit\nof a minimum supported version of Airflow. The minimum version of Airflow is the ``MINOR`` version (2.4, 2.5 etc.)\nindicating that the providers might use features that appeared in this release. The default support timespan\nfor the minimum version of Airflow (there could be justified exceptions) is that we increase the minimum\nAirflow version to the next MINOR release, when 12 months passed since the first release for the\nMINOR version of Airflow.\n\nFor example this means that by default we upgrade the minimum version of Airflow supported by providers\nto 2.9.0 in the first Provider's release after 8th of April 2025. 8th of April 2024 is the date when the\nfirst ``PATCHLEVEL`` of 2.9 (2.9.0) has been released.\n\nWhen we increase the minimum Airflow version, this is not a reason to bump ``MAJOR`` version of the providers\n(unless there are other breaking changes in the provider). The reason for that is that people who use\nolder version of Airflow will not be able to use that provider (so it is not a breaking change for them)\nand for people who are using supported version of Airflow this is not a breaking change on its own - they\nwill be able to use the new version without breaking their workflows. When we upgraded min-version to\n2.2+, our approach was different but as of 2.3+ upgrade (November 2022) we only bump ``MINOR`` version of the\nprovider when we increase minimum Airflow version.\n\nIncreasing the minimum version of the Providers is one of the reasons why 3rd-party provider maintainers\nmight want to maintain their own providers - as they can decide to support older versions of Airflow.\n\n3rd-parties relation to community providers\n-------------------------------------------\n\nProviders, can (and it is recommended for 3rd-party services) also be maintained and released by 3rd parties,\nbut for multiple reasons we might decide to keep those providers as community managed providers - mostly\ndue to prevalence and popularity of the 3rd-party services and use cases they serve among our community. There\nare however certain conditions and expectations we have in order.\n\nThere is no difference between the community and 3rd party providers - they have all the same capabilities\nand limitations. The consensus in the Airflow community is that usually it is better for the community and\nfor the health of the provider to be managed by the 3rd party team, rather than by the Airflow community.\nThis is especially in case the provider concerns 3rd-party service that has a team that can manage provider\non their own. For the Airflow community, managing and releasing a 3rd-party provider that we cannot test\nand verify is a lot of effort and uncertainty, especially including the cases where the external service is\nlive and going to evolve in the future, and it is better to let the 3rd party team manage it,\nas they can better keep pace with the changes in the service.\n\nInformation about such 3rd-party providers are usually published at the\n`Ecosystem: plugins and providers <https://airflow.apache.org/ecosystem/#third-party-airflow-plugins-and-providers>`_\npage of the Airflow website and we encourage the service providers to publish their providers there. You can also\nfind a 3rd-party registries of such providers, that you can use if you search for existing providers (they\nare also listed at the \"Ecosystem\" page in the same chapter)\n\nWhile we already have - historically - a number of 3rd-party service providers managed by the community,\nmost of those services have dedicated teams that keep an eye on the community providers and not only take\nactive part in managing them (see mixed-governance model below), but also provide a way that we can\nverify whether the provider works with the latest version of the service via dashboards that show\nstatus of System Tests for the provider. This allows us to have a high level of confidence that when we\nrelease the provider it works with the latest version of the service. System Tests are part of the Airflow\ncode, but they are executed and verified by those 3rd party service teams. We are working with the 3rd\nparty service teams (who are often important stakeholders of the Apache Airflow project) to add dashboards\nfor the historical providers that are managed by the community, and current set of Dashboards can be also\nfound at the\n`Ecosystem: system test dashboards <https://airflow.apache.org/ecosystem/#airflow-provider-system-test-dashboards>`_\n\nMixed governance model for 3rd-party related community providers\n----------------------------------------------------------------\n\nProviders are often connected with some stakeholders that are vitally interested in maintaining backwards\ncompatibilities in their integrations (for example cloud providers, or specific service providers). But,\nwe are also bound with the `Apache Software Foundation release policy <https://www.apache.org/legal/release-policy.html>`_\nwhich describes who releases, and how to release the ASF software. The provider's governance model is something we name\n``mixed governance`` - where we follow the release policies, while the burden of maintaining and testing\nthe cherry-picked versions is on those who commit to perform the cherry-picks and make PRs to older\nbranches.\n\nThe \"mixed governance\" (optional, per-provider) means that:\n\n* The Airflow Community and release manager decide when to release those providers.\n  This is fully managed by the community and the usual release-management process following the\n  `Apache Software Foundation release policy <https://www.apache.org/legal/release-policy.html>`_\n* The contributors (who might or might not be direct stakeholders in the provider) will carry the burden\n  of cherry-picking and testing the older versions of providers.\n* There is no \"selection\" and acceptance process to determine which version of the provider is released.\n  It is determined by the actions of contributors raising the PR with cherry-picked changes and it follows\n  the usual PR review process where maintainer approves (or not) and merges (or not) such PR. Simply\n  speaking - the completed action of cherry-picking and testing the older version of the provider make\n  it eligible to be released. Unless there is someone who volunteers and perform the cherry-picking and\n  testing, the provider is not released.\n* Branches to raise PR against are created when a contributor commits to perform the cherry-picking\n  (as a comment in PR to cherry-pick for example)\n\nUsually, community effort is focused on the most recent version of each provider. The community approach is\nthat we should rather aggressively remove deprecations in \"major\" versions of the providers - whenever\nthere is an opportunity to increase major version of a provider, we attempt to remove all deprecations.\nHowever, sometimes there is a contributor (who might or might not represent stakeholder),\nwilling to make their effort on cherry-picking and testing the non-breaking changes to a selected,\nprevious major branch of the provider. This results in releasing at most two versions of a\nprovider at a time:\n\n* potentially breaking \"latest\" major version\n* selected past major version with non-breaking changes applied by the contributor\n\nCherry-picking such changes follows the same process for releasing Airflow\npatch-level releases for a previous minor Airflow version. Usually such cherry-picking is done when\nthere is an important bugfix and the latest version contains breaking changes that are not\ncoupled with the bugfix. Releasing them together in the latest version of the provider effectively couples\nthem, and therefore they're released separately. The cherry-picked changes have to be merged by the committer following the usual rules of the\ncommunity.\n\nThere is no obligation to cherry-pick and release older versions of the providers.\nThe community continues to release such older versions of the providers for as long as there is an effort\nof the contributors to perform the cherry-picks and carry-on testing of the older provider version.\n\nThe availability of stakeholder that can manage \"service-oriented\" maintenance and agrees to such a\nresponsibility, will also drive our willingness to accept future, new providers to become community managed.\n\nSuspending releases for providers\n---------------------------------\n\nIn case a provider is found to require old dependencies that are not compatible with upcoming versions of\nthe Apache Airflow or with newer dependencies required by other providers, the provider's release\nprocess can be suspended.\n\nThis means:\n\n* The provider's state in ``provider.yaml`` is set to \"suspended\"\n* No new releases of the provider will be made until the problem with dependencies is solved\n* Sources of the provider remain in the repository for now (in the future we might add process to remove them)\n* No new changes will be accepted for the provider (other than the ones that fix the dependencies)\n* The provider will be removed from the list of Apache Airflow extras in the next Airflow release\n  (including patch-level release if it is possible/easy to cherry-pick the suspension change)\n* Tests of the provider will not be run on our CI (in main branch)\n* Dependencies of the provider will not be installed in our main branch CI image nor included in constraints\n* We can still decide to apply security fixes to released providers - by adding fixes to the main branch\n  but cherry-picking, testing and releasing them in the patch-level branch of the provider similar to the\n  mixed governance model described above.\n\nThe suspension may be triggered by any committer after the following criteria are met:\n\n* The maintainers of dependencies of the provider are notified about the issue and are given a reasonable\n  time to resolve it (at least 1 week)\n* Other options to resolve the issue have been exhausted and there are good reasons for upgrading\n  the old dependencies in question\n* Explanation why we need to suspend the provider is stated in a public discussion in the devlist. Followed\n  by ``[LAZY CONSENSUS]`` or ``[VOTE]`` discussion at the devlist (with the majority of the binding votes\n  agreeing that we should suspend the provider)\n\nThe suspension will be lifted when the dependencies of the provider are made compatible with the Apache\nAirflow and with other providers - by merging a PR that removes the suspension and succeeds.\n\nRemoving community providers\n----------------------------\n\nThe providers can be removed from main branch of Airflow when the community agrees that there should be no\nmore updates to the providers done by the community - except maybe potentially security fixes found. There\nmight be various reasons for the providers to be removed:\n\n* the service they connect to is no longer available\n* the dependencies for the provider are not maintained anymore and there is no viable alternative\n* there is another, more popular provider that supersedes community provider\n* etc. etc.\n\nEach case of removing provider should be discussed individually and separate ``[VOTE]`` thread should start,\nwhere regular rules for code modification apply (following the\n`Apache Software Foundation voting rules <https://www.apache.org/foundation/voting.html#votes-on-code-modification>`_).\nIn cases where the reasons for removal are ``obvious``, and discussed before, also ``[LAZY CONSENSUS]`` thread\ncan be started. Generally speaking a discussion thread ``[DISCUSS]`` is advised before such removal and\nsufficient time should pass (at least a week) to give a chance for community members to express their\nopinion on the removal.\n\nThere are the following consequences (or lack of them) of removing the provider:\n\n* One last release of the provider is done with documentation updated informing that the provider is no\n  longer maintained by the Apache Airflow community - linking to this page. This information should also\n  find its way to the package documentation and consequently - to the description of the package in PyPI.\n* An ``[ANNOUNCE]`` thread is sent to the devlist and user list announcing removal of the provider\n* The released provider packages remain available on PyPI and in the\n   `Archives <https://archive.apache.org/dist/airflow/providers/>`_ of the Apache\n   Software Foundation, while they are removed from the\n   `Downloads <https://downloads.apache.org/airflow/providers/>`_ .\n   Also it remains in the Index of the Apache Airflow Providers documentation at\n   `Airflow Documentation <https://airflow.apache.org/docs/>`_ with note ``(not maintained)`` next to it.\n* The code of the provider is removed from ``main`` branch of the Apache Airflow repository - including\n  the tests and documentation. It is no longer built in CI and dependencies of the provider no longer\n  contribute to the CI image/constraints of Apache Airflow for development and future ``MINOR`` release.\n* The provider is removed from the list of Apache Airflow extras in the next ``MINOR`` Airflow release\n* The dependencies of the provider are removed from the constraints of the Apache Airflow\n  (and the constraints are updated in the next ``MINOR`` release of Airflow)\n* In case of confirmed security issues that need fixing that are reported to the provider after it has been\n  removed, there are two options:\n  * in case there is a viable alternative or in case the provider is anyhow not useful to be installed, we\n    might issue advisory to the users to remove the provider (and use alternatives if applicable)\n  * in case the users might still need the provider, we still might decide to release new version of the\n    provider with security issue fixed, starting from the source code in Git history where the provider was\n    last released. This however, should only be done in case there are no viable alternatives for the users.\n* Removed provider might be re-instated as maintained provider, but it needs to go through the regular process\n  of accepting new provider described above.\n\nProvider Dependencies\n=====================\n\nThe dependencies for Airflow providers are managed in the ``provider.yaml`` file.\n\nAll provider dependencies, including versions and constraints, are listed in this file.\nWhen adding or updating a provider or its dependencies, changes should be made to this file accordingly.\n\nTo ensure consistency and manage dependencies, ``pre-commit`` is configured to automatically update all dependencies.\nOnce you have ``pre-commit`` installed, it will automatically handle the dependency updates.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 34.07421875,
          "content": "<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n-->\n\n<!-- START Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n# Apache Airflow\n\n[![PyPI version](https://badge.fury.io/py/apache-airflow.svg)](https://badge.fury.io/py/apache-airflow)\n[![GitHub Build](https://github.com/apache/airflow/actions/workflows/ci.yml/badge.svg)](https://github.com/apache/airflow/actions)\n[![Coverage Status](https://codecov.io/gh/apache/airflow/graph/badge.svg?token=WdLKlKHOAU)](https://codecov.io/gh/apache/airflow)\n[![License](https://img.shields.io/:license-Apache%202-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0.txt)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/apache-airflow.svg)](https://pypi.org/project/apache-airflow/)\n[![Docker Pulls](https://img.shields.io/docker/pulls/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow)\n[![Docker Stars](https://img.shields.io/docker/stars/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/apache-airflow)](https://pypi.org/project/apache-airflow/)\n[![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow)](https://artifacthub.io/packages/search?repo=apache-airflow)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Twitter Follow](https://img.shields.io/twitter/follow/ApacheAirflow.svg?style=social&label=Follow)](https://x.com/ApacheAirflow)\n[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://s.apache.org/airflow-slack)\n[![Contributors](https://img.shields.io/github/contributors/apache/airflow)](https://github.com/apache/airflow/graphs/contributors)\n[![OSSRank](https://shields.io/endpoint?url=https://ossrank.com/shield/6)](https://ossrank.com/p/6)\n\n<picture width=\"500\">\n  <img\n    src=\"https://github.com/apache/airflow/blob/19ebcac2395ef9a6b6ded3a2faa29dc960c1e635/docs/apache-airflow/img/logos/wordmark_1.png?raw=true\"\n    alt=\"Apache Airflow logo\"\n  />\n</picture>\n\n[Apache Airflow](https://airflow.apache.org/docs/apache-airflow/stable/) (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.\n\nWhen workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.\n\nUse Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\n\n<!-- END Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of contents**\n\n- [Project Focus](#project-focus)\n- [Principles](#principles)\n- [Requirements](#requirements)\n- [Getting started](#getting-started)\n- [Installing from PyPI](#installing-from-pypi)\n- [Official source code](#official-source-code)\n- [Convenience packages](#convenience-packages)\n- [User Interface](#user-interface)\n- [Semantic versioning](#semantic-versioning)\n- [Version Life Cycle](#version-life-cycle)\n- [Support for Python and Kubernetes versions](#support-for-python-and-kubernetes-versions)\n- [Base OS support for reference Airflow images](#base-os-support-for-reference-airflow-images)\n- [Approach to dependencies of Airflow](#approach-to-dependencies-of-airflow)\n- [Contributing](#contributing)\n- [Voting Policy](#voting-policy)\n- [Who uses Apache Airflow?](#who-uses-apache-airflow)\n- [Who maintains Apache Airflow?](#who-maintains-apache-airflow)\n- [What goes into the next release?](#what-goes-into-the-next-release)\n- [Can I use the Apache Airflow logo in my presentation?](#can-i-use-the-apache-airflow-logo-in-my-presentation)\n- [Links](#links)\n- [Sponsors](#sponsors)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## Project Focus\n\nAirflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include [Luigi](https://github.com/spotify/luigi), [Oozie](https://oozie.apache.org/) and [Azkaban](https://azkaban.github.io/).\n\nAirflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's [XCom feature](https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html)). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.\n\nAirflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.\n\n## Principles\n\n- **Dynamic**: Airflow pipelines are configuration as code (Python), allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.\n- **Extensible**: Easily define your own operators, executors and extend the library so that it fits the level of abstraction that suits your environment.\n- **Elegant**: Airflow pipelines are lean and explicit. Parameterizing your scripts is built into the core of Airflow using the powerful **Jinja** templating engine.\n- **Scalable**: Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers.\n\n<!-- START Requirements, please keep comment here to allow auto update of PyPI readme.md -->\n## Requirements\n\nApache Airflow is tested with:\n\n|            | Main version (dev)         | Stable version (2.10.4)    |\n|------------|----------------------------|----------------------------|\n| Python     | 3.9, 3.10, 3.11, 3.12      | 3.8, 3.9, 3.10, 3.11, 3.12 |\n| Platform   | AMD64/ARM64(\\*)            | AMD64/ARM64(\\*)            |\n| Kubernetes | 1.28, 1.29, 1.30, 1.31     | 1.27, 1.28, 1.29, 1.30     |\n| PostgreSQL | 13, 14, 15, 16, 17         | 12, 13, 14, 15, 16         |\n| MySQL      | 8.0, 8.4, Innovation       | 8.0, 8.4, Innovation       |\n| SQLite     | 3.15.0+                    | 3.15.0+                    |\n\n\\* Experimental\n\n**Note**: MariaDB is not tested/recommended.\n\n**Note**: SQLite is used in Airflow tests. Do not use it in production. We recommend\nusing the latest stable version of SQLite for local development.\n\n**Note**: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly\ntested on fairly modern Linux Distros and recent versions of macOS.\nOn Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers.\nThe work to add Windows support is tracked via [#10388](https://github.com/apache/airflow/issues/10388), but\nit is not a high priority. You should only use Linux-based distros as \"Production\" execution environment\nas this is the only environment that is supported. The only distro that is used in our CI tests and that\nis used in the [Community managed DockerHub image](https://hub.docker.com/p/apache/airflow) is\n`Debian Bookworm`.\n\n<!-- END Requirements, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Getting started, please keep comment here to allow auto update of PyPI readme.md -->\n## Getting started\n\nVisit the official Airflow website documentation (latest **stable** release) for help with\n[installing Airflow](https://airflow.apache.org/docs/apache-airflow/stable/installation/),\n[getting started](https://airflow.apache.org/docs/apache-airflow/stable/start.html), or walking\nthrough a more complete [tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/).\n\n> Note: If you're looking for documentation for the main branch (latest development branch): you can find it on [s.apache.org/airflow-docs](https://s.apache.org/airflow-docs/).\n\nFor more information on Airflow Improvement Proposals (AIPs), visit\nthe [Airflow Wiki](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals).\n\nDocumentation for dependent projects like provider packages, Docker image, Helm Chart, you'll find it in [the documentation index](https://airflow.apache.org/docs/).\n\n<!-- END Getting started, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md -->\n## Installing from PyPI\n\nWe publish Apache Airflow as `apache-airflow` package in PyPI. Installing it however might be sometimes tricky\nbecause Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and\napplications usually pin them, but we should do neither and both simultaneously. We decided to keep\nour dependencies as open as possible (in `pyproject.toml`) so users can install different versions of libraries\nif needed. This means that `pip install apache-airflow` will not work from time to time or will\nproduce unusable Airflow installation.\n\nTo have repeatable installation, however, we keep a set of \"known-to-be-working\" constraint\nfiles in the orphan `constraints-main` and `constraints-2-0` branches. We keep those \"known-to-be-working\"\nconstraints files separately per major/minor Python version.\nYou can use them as constraint files when installing Airflow from PyPI. Note that you have to specify\ncorrect Airflow tag/version/branch and Python versions in the URL.\n\n\n1. Installing just Airflow:\n\n> Note: Only `pip` installation is currently officially supported.\n\nWhile it is possible to install Airflow with tools like [Poetry](https://python-poetry.org) or\n[pip-tools](https://pypi.org/project/pip-tools), they do not share the same workflow as\n`pip` - especially when it comes to constraint vs. requirements management.\nInstalling via `Poetry` or `pip-tools` is not currently supported.\n\nThere are known issues with ``bazel`` that might lead to circular dependencies when using it to install\nAirflow. Please switch to ``pip`` if you encounter such problems. ``Bazel`` community works on fixing\nthe problem in `this PR <https://github.com/bazelbuild/rules_python/pull/1166>`_ so it might be that\nnewer versions of ``bazel`` will handle it.\n\nIf you wish to install Airflow using those tools, you should use the constraint files and convert\nthem to the appropriate format and workflow that your tool requires.\n\n\n```bash\npip install 'apache-airflow==2.10.4' \\\n --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.10.4/constraints-3.9.txt\"\n```\n\n2. Installing with extras (i.e., postgres, google)\n\n```bash\npip install 'apache-airflow[postgres,google]==2.10.4' \\\n --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.10.4/constraints-3.9.txt\"\n```\n\nFor information on installing provider packages, check\n[providers](http://airflow.apache.org/docs/apache-airflow-providers/index.html).\n\n<!-- END Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Official source code, please keep comment here to allow auto update of PyPI readme.md -->\n## Official source code\n\nApache Airflow is an [Apache Software Foundation](https://www.apache.org) (ASF) project,\nand our official source code releases:\n\n- Follow the [ASF Release Policy](https://www.apache.org/legal/release-policy.html)\n- Can be downloaded from [the ASF Distribution Directory](https://downloads.apache.org/airflow)\n- Are cryptographically signed by the release manager\n- Are officially voted on by the PMC members during the\n  [Release Approval Process](https://www.apache.org/legal/release-policy.html#release-approval)\n\nFollowing the ASF rules, the source packages released must be sufficient for a user to build and test the\nrelease provided they have access to the appropriate platform and tools.\n\n<!-- END Official source code, please keep comment here to allow auto update of PyPI readme.md -->\n## Convenience packages\n\nThere are other ways of installing and using Airflow. Those are \"convenience\" methods - they are\nnot \"official releases\" as stated by the `ASF Release Policy`, but they can be used by the users\nwho do not want to build the software themselves.\n\nThose are - in the order of most common ways people install Airflow:\n\n- [PyPI releases](https://pypi.org/project/apache-airflow/) to install Airflow using standard `pip` tool\n- [Docker Images](https://hub.docker.com/r/apache/airflow) to install airflow via\n  `docker` tool, use them in Kubernetes, Helm Charts, `docker-compose`, `docker swarm`, etc. You can\n  read more about using, customizing, and extending the images in the\n  [Latest docs](https://airflow.apache.org/docs/docker-stack/index.html), and\n  learn details on the internals in the [images](https://airflow.apache.org/docs/docker-stack/index.html) document.\n- [Tags in GitHub](https://github.com/apache/airflow/tags) to retrieve the git project sources that\n  were used to generate official source packages via git\n\nAll those artifacts are not official releases, but they are prepared using officially released sources.\nSome of those artifacts are \"development\" or \"pre-release\" ones, and they are clearly marked as such\nfollowing the ASF Policy.\n\n## User Interface\n\n- **DAGs**: Overview of all DAGs in your environment.\n\n  ![DAGs](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/dags.png)\n\n- **Grid**: Grid representation of a DAG that spans across time.\n\n  ![Grid](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/grid.png)\n\n- **Graph**: Visualization of a DAG's dependencies and their current status for a specific run.\n\n  ![Graph](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/graph.png)\n\n- **Task Duration**: Total time spent on different tasks over time.\n\n  ![Task Duration](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/duration.png)\n\n- **Gantt**: Duration and overlap of a DAG.\n\n  ![Gantt](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/gantt.png)\n\n- **Code**: Quick way to view source code of a DAG.\n\n  ![Code](https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/img/code.png)\n\n## Semantic versioning\n\nAs of Airflow 2.0.0, we support a strict [SemVer](https://semver.org/) approach for all packages released.\n\nThere are few specific rules that we agreed to that define details of versioning of the different\npackages:\n\n* **Airflow**: SemVer rules apply to core airflow only (excludes any changes to providers).\n  Changing limits for versions of Airflow dependencies is not a breaking change on its own.\n* **Airflow Providers**: SemVer rules apply to changes in the particular provider's code only.\n  SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version.\n  For example, `google 4.1.0` and `amazon 3.0.3` providers can happily be installed\n  with `Airflow 2.1.2`. If there are limits of cross-dependencies between providers and Airflow packages,\n  they are present in providers as `install_requires` limitations. We aim to keep backwards\n  compatibility of providers with all previously released Airflow 2 versions but\n  there will sometimes be breaking changes that might make some, or all\n  providers, have minimum Airflow version specified.\n* **Airflow Helm Chart**: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR\n  versions for the chart are independent of the Airflow version. We aim to keep backwards\n  compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might\n  only work starting from specific Airflow releases. We might however limit the Helm\n  Chart to depend on minimal Airflow version.\n* **Airflow API clients**: Their versioning is independent from Airflow versions. They follow their own\n  SemVer rules for breaking changes and new features - which for example allows to change the way we generate\n  the clients.\n\n## Version Life Cycle\n\nApache Airflow version life cycle:\n\n<!-- This table is automatically updated by pre-commit scripts/ci/pre_commit/supported_versions.py -->\n<!-- Beginning of auto-generated table -->\n\n| Version   | Current Patch/Minor   | State     | First Release   | Limited Support   | EOL/Terminated   |\n|-----------|-----------------------|-----------|-----------------|-------------------|------------------|\n| 2         | 2.10.4                | Supported | Dec 17, 2020    | TBD               | TBD              |\n| 1.10      | 1.10.15               | EOL       | Aug 27, 2018    | Dec 17, 2020      | June 17, 2021    |\n| 1.9       | 1.9.0                 | EOL       | Jan 03, 2018    | Aug 27, 2018      | Aug 27, 2018     |\n| 1.8       | 1.8.2                 | EOL       | Mar 19, 2017    | Jan 03, 2018      | Jan 03, 2018     |\n| 1.7       | 1.7.1.2               | EOL       | Mar 28, 2016    | Mar 19, 2017      | Mar 19, 2017     |\n\n<!-- End of auto-generated table -->\n\nLimited support versions will be supported with security and critical bug fix only.\nEOL versions will not get any fixes nor support.\nWe always recommend that all users run the latest available minor release for whatever major version is in use.\nWe **highly** recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.\n\n## Support for Python and Kubernetes versions\n\nAs of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support.\nThey are based on the official release schedule of Python and Kubernetes, nicely summarized in the\n[Python Developer's Guide](https://devguide.python.org/#status-of-python-branches) and\n[Kubernetes version skew policy](https://kubernetes.io/docs/setup/release/version-skew-policy/).\n\n1. We drop support for Python and Kubernetes versions when they reach EOL. Except for Kubernetes, a\n   version stays supported by Airflow if two major cloud providers still provide support for it. We drop\n   support for those EOL versions in main right after EOL date, and it is effectively removed when we release\n   the first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow. For example, for Python 3.9 it\n   means that we will drop support in main right after 27.06.2023, and the first MAJOR or MINOR version of\n   Airflow released after will not have it.\n\n2. We support a new version of Python/Kubernetes in main after they are officially released, as soon as we\n   make them work in our CI pipeline (which might not be immediate due to dependencies catching up with\n   new versions of Python mostly) we release new images/support in Airflow based on the working CI setup.\n\n3. This policy is best-effort which means there may be situations where we might terminate support earlier\n   if circumstances require it.\n\n## Base OS support for reference Airflow images\n\nThe Airflow Community provides conveniently packaged container images that are published whenever\nwe publish an Apache Airflow release. Those images contain:\n\n* Base OS with necessary packages to install Airflow (stable Debian OS)\n* Base Python installation in versions supported at the time of release for the MINOR version of\n  Airflow released (so there could be different versions for 2.3 and 2.2 line for example)\n* Libraries required to connect to supported Databases (again the set of databases supported depends\n  on the MINOR version of Airflow)\n* Predefined set of popular providers (for details see the [Dockerfile](https://raw.githubusercontent.com/apache/airflow/main/Dockerfile)).\n* Possibility of building your own, custom image where the user can choose their own set of providers\n  and libraries (see [Building the image](https://airflow.apache.org/docs/docker-stack/build.html))\n* In the future Airflow might also support a \"slim\" version without providers nor database clients installed\n\nThe version of the base OS image is the stable version of Debian. Airflow supports using all currently active\nstable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for\nbuilding and testing the OS version. Approximately 6 months before the end-of-regular support of a\nprevious stable version of the OS, Airflow switches the images released to use the latest supported\nversion of the OS.\n\nFor example switch from ``Debian Bullseye`` to ``Debian Bookworm`` has been implemented\nbefore 2.8.0 release in October 2023 and ``Debian Bookworm`` will be the only option supported as of\nAirflow 2.10.0.\n\nUsers will continue to be able to build their images using stable Debian releases until the end of regular\nsupport and building and verifying of the images happens in our CI but no unit tests were executed using\nthis image in the `main` branch.\n\n## Approach to dependencies of Airflow\n\nAirflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application,\ntherefore our policies to dependencies has to include both - stability of installation of application,\nbut also ability to install newer version of dependencies for those users who develop DAGs. We developed\nthe approach where `constraints` are used to make sure airflow can be installed in a repeatable way, while\nwe do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound\nversion of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is\nneeded because of importance of the dependency as well as risk it involves to upgrade specific dependency.\nWe also upper-bound the dependencies that we know cause problems.\n\nThe constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies\nautomatically (providing that all the tests pass). Our `main` build failures will indicate in case there\nare versions of dependencies that break our tests - indicating that we should either upper-bind them or\nthat we should fix our code/tests to account for the upstream changes from those dependencies.\n\nWhenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have\na good reason why dependency is upper-bound. And we should also mention what is the condition to remove the\nbinding.\n\n### Approach for dependencies for Airflow Core\n\nThose dependencies are maintained in ``pyproject.toml``.\n\nThere are few dependencies that we decided are important enough to upper-bound them by default, as they are\nknown to follow predictable versioning scheme, and we know that new versions of those are very likely to\nbring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of\nthe dependencies as they are released, but this is manual process.\n\nThe important dependencies are:\n\n* `SQLAlchemy`: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and\n   introduce breaking changes especially that support for different Databases varies and changes at\n   various speed)\n* `Alembic`: it is important to handle our migrations in predictable and performant way. It is developed\n   together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version\n* `Flask`: We are using Flask as the back-bone of our web UI and API. We know major version of Flask\n   are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense\n* `werkzeug`: the library is known to cause problems in new versions. It is tightly coupled with Flask\n   libraries, and we should update them together\n* `celery`: Celery is a crucial component of Airflow as it used for CeleryExecutor (and similar). Celery\n   [follows SemVer](https://docs.celeryq.dev/en/stable/contributing.html?highlight=semver#versions), so\n   we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library,\n   we should make sure Celery Provider minimum Airflow version is updated.\n* `kubernetes`: Kubernetes is a crucial component of Airflow as it is used for the KubernetesExecutor\n   (and similar). Kubernetes Python library [follows SemVer](https://github.com/kubernetes-client/python#compatibility),\n   so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library,\n   we should make sure Kubernetes Provider minimum Airflow version is updated.\n\n### Approach for dependencies in Airflow Providers and extras\n\nThe main part of the Airflow is the Airflow Core, but the power of Airflow also comes from a number of\nproviders that extend the core functionality and are released separately, even if we keep them (for now)\nin the same monorepo for convenience. You can read more about the providers in the\n[Providers documentation](https://airflow.apache.org/docs/apache-airflow-providers/index.html). We also\nhave set of policies implemented for maintaining and releasing community-managed providers as well\nas the approach for community vs. 3rd party providers in the [providers](https://github.com/apache/airflow/blob/main/PROVIDERS.rst) document.\n\nThose `extras` and `providers` dependencies are maintained in `provider.yaml` of each provider.\n\nBy default, we should not upper-bound dependencies for providers, however each provider's maintainer\nmight decide to add additional limits (and justify them with comment).\n\n<!-- START Contributing, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Contributing\n\nWant to help build Apache Airflow? Check out our [contributors' guide](https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) for a comprehensive overview of how to contribute, including setup instructions, coding standards, and pull request guidelines.\n\nIf you can't wait to contribute, and want to get started asap, check out the [contribution quickstart](https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst) here!\n\nOfficial Docker (container) images for Apache Airflow are described in [images](dev/breeze/doc/ci/02_images.md).\n\n<!-- END Contributing, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Voting Policy\n\n* Commits need a +1 vote from a committer who is not the author\n* When we do AIP voting, both PMC member's and committer's `+1s` are considered a binding vote.\n\n## Who uses Apache Airflow?\n\nWe know about around 500 organizations that are using Apache Airflow (but there are likely many more)\n[in the wild](https://github.com/apache/airflow/blob/main/INTHEWILD.md).\n\nIf you use Airflow - feel free to make a PR to add your organisation to the list.\n\n<!-- END Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Who maintains Apache Airflow?\n\nAirflow is the work of the [community](https://github.com/apache/airflow/graphs/contributors),\nbut the [core committers/maintainers](https://people.apache.org/committers-by-project.html#airflow)\nare responsible for reviewing and merging PRs as well as steering conversations around new feature requests.\nIf you would like to become a maintainer, please review the Apache Airflow\n[committer requirements](https://github.com/apache/airflow/blob/main/COMMITTERS.rst#guidelines-to-become-an-airflow-committer).\n\n<!-- END Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n\n## What goes into the next release?\n\nOften you will see an issue that is assigned to specific milestone with Airflow version, or a PR that gets merged\nto the main branch and you might wonder which release the merged PR(s) will be released in or which release the fixed\nissues will be in. The answer to this is as usual - it depends on various scenarios. The answer is different for PRs and Issues.\n\nTo add a bit of context, we are following the [Semver](https://semver.org/) versioning scheme as described in\n[Airflow release process](https://airflow.apache.org/docs/apache-airflow/stable/release-process.html). More\ndetails are explained in detail in this README under the [Semantic versioning](#semantic-versioning) chapter, but\nin short, we have `MAJOR.MINOR.PATCH` versions of Airflow.\n\n* `MAJOR` version is incremented in case of breaking changes\n* `MINOR` version is incremented when there are new features added\n* `PATCH` version is incremented when there are only bug-fixes and doc-only changes\n\nGenerally we release `MINOR` versions of Airflow from a branch that is named after the MINOR version. For example\n`2.7.*` releases are released from `v2-7-stable` branch, `2.8.*` releases are released from `v2-8-stable`\nbranch, etc.\n\n1. Most of the time in our release cycle, when the branch for next `MINOR` branch is not yet created, all\nPRs merged to `main` (unless they get reverted), will find their way to the next `MINOR` release. For example\nif the last release is `2.7.3` and `v2-8-stable` branch is not created yet, the next `MINOR` release\nis `2.8.0` and all PRs merged to main will be released in `2.8.0`. However, some PRs (bug-fixes and\ndoc-only changes) when merged, can be cherry-picked to current `MINOR` branch and released in the\nnext `PATCHLEVEL` release. For example, if `2.8.1` is already released and we are working on `2.9.0dev`,  then\nmarking a PR with `2.8.2` milestone means that it will be cherry-picked to `v2-8-test` branch and\nreleased in `2.8.2rc1`, and eventually in `2.8.2`.\n\n2. When we prepare for the next `MINOR` release, we cut new `v2-*-test` and `v2-*-stable` branch\nand prepare `alpha`, `beta` releases for the next `MINOR` version, the PRs merged to main will still be\nreleased in the next `MINOR` release until `rc` version is cut. This is happening because the `v2-*-test`\nand `v2-*-stable` branches are rebased on top of main when next `beta` and `rc` releases are prepared.\nFor example, when we cut `2.10.0beta1` version, anything merged to main before `2.10.0rc1` is released,\nwill find its way to 2.10.0rc1.\n\n3. Then, once we prepare the first RC candidate for the MINOR release, we stop moving the `v2-*-test` and\n`v2-*-stable` branches and the PRs merged to main will be released in the next `MINOR` release.\nHowever, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current `MINOR`\nbranch and released in the next `PATCHLEVEL` release - for example when the last released version from `v2-10-stable`\nbranch is `2.10.0rc1`, some of the PRs from main can be marked as `2.10.0` milestone by committers,\nthe release manager will try to cherry-pick them into the release branch.\nIf successful, they will be released in `2.10.0rc2` and subsequently in `2.10.0`. This also applies to\nsubsequent `PATCHLEVEL` versions. When for example `2.10.1` is already released, marking a PR with\n`2.10.2` milestone will mean that it will be cherry-picked to `v2-10-stable` branch and released in `2.10.2rc1`\nand eventually in `2.10.2`.\n\nThe final decision about cherry-picking is made by the release manager.\n\nMarking issues with a milestone is a bit different. Maintainers do not mark issues with a milestone usually,\nnormally they are only marked in PRs. If PR linked to the issue (and \"fixing it\") gets merged and released\nin a specific version following the process described above, the issue will be automatically closed, no\nmilestone will be set for the issue, you need to check the PR that fixed the issue to see which version\nit was released in.\n\nHowever, sometimes maintainers mark issues with specific milestone, which means that the\nissue is important to become a candidate to take a look when the release is being prepared. Since this is an\nOpen-Source project, where basically all contributors volunteer their time, there is no guarantee that specific\nissue will be fixed in specific version. We do not want to hold the release because some issue is not fixed,\nso in such case release manager will reassign such unfixed issues to the next milestone in case they are not\nfixed in time for the current release. Therefore, the milestone for issue is more of an intent that it should be\nlooked at, than promise it will be fixed in the version.\n\nMore context and **FAQ** about the patchlevel release can be found in the\n[What goes into the next release](dev/WHAT_GOES_INTO_THE_NEXT_RELEASE.md) document in the `dev` folder of the\nrepository.\n\n## Can I use the Apache Airflow logo in my presentation?\n\nYes! Be sure to abide by the Apache Foundation [trademark policies](https://www.apache.org/foundation/marks/#books) and the Apache Airflow [Brandbook](https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook). The most up-to-date logos are found in [this repo](https://github.com/apache/airflow/tree/main/docs/apache-airflow/img/logos/) and on the Apache Software Foundation [website](https://www.apache.org/logos/about.html).\n\n## Links\n\n- [Documentation](https://airflow.apache.org/docs/apache-airflow/stable/)\n- [Chat](https://s.apache.org/airflow-slack)\n- [Community Information](https://airflow.apache.org/community/)\n\n## Sponsors\n\nThe CI infrastructure for Apache Airflow has been sponsored by:\n\n<!-- Ordered by most recently \"funded\" -->\n\n<a href=\"https://astronomer.io\"><img src=\"https://assets2.astronomer.io/logos/logoForLIGHTbackground.png\" alt=\"astronomer.io\" width=\"250px\"></a>\n<a href=\"https://aws.amazon.com/opensource/\"><img src=\"docs/integration-logos/aws/AWS-Cloud-alt_light-bg@4x.png\" alt=\"AWS OpenSource\" width=\"130px\"></a>\n\n<!-- telemetry/analytics pixel: -->\n<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=1b5a5e3c-da81-42f5-befa-42d836bf1b54\" alt=\"Tracking Pixel\" />\n"
        },
        {
          "name": "RELEASE_NOTES.rst",
          "type": "blob",
          "size": 741.8671875,
          "content": " .. Licensed to the Apache Software Foundation (ASF) under one\n    or more contributor license agreements.  See the NOTICE file\n    distributed with this work for additional information\n    regarding copyright ownership.  The ASF licenses this file\n    to you under the Apache License, Version 2.0 (the\n    \"License\"); you may not use this file except in compliance\n    with the License.  You may obtain a copy of the License at\n\n ..   http://www.apache.org/licenses/LICENSE-2.0\n\n .. Unless required by applicable law or agreed to in writing,\n    software distributed under the License is distributed on an\n    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n    KIND, either express or implied.  See the License for the\n    specific language governing permissions and limitations\n    under the License.\n\n.. contents:: Apache Airflow Releases\n   :local:\n   :depth: 1\n\n.. towncrier release notes start\n\nAirflow 2.10.4 (2024-12-16)\n---------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nTaskInstance ``priority_weight`` is capped in 32-bit signed integer ranges (#43611)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nSome database engines are limited to 32-bit integer values. As some users reported errors in\nweight rolled-over to negative values, we decided to cap the value to the 32-bit integer. Even\nif internally in python smaller or larger values to 64 bit are supported, ``priority_weight`` is\ncapped and only storing values from -2147483648 to 2147483647.\n\nBug Fixes\n^^^^^^^^^\n\n- Fix stats of dynamic mapped tasks after automatic retries of failed tasks (#44300)\n- Fix wrong display of multi-line messages in the log after filtering (#44457)\n- Allow \"/\" in metrics validator (#42934) (#44515)\n- Fix gantt flickering (#44488) (#44517)\n- Fix problem with inability to remove fields from Connection form (#40421) (#44442)\n- Check pool_slots on partial task import instead of execution (#39724) (#42693)\n- Avoid grouping task instance stats by try_number for dynamic mapped tasks (#44300) (#44319)\n- Re-queue task when they are stuck in queued (#43520) (#44158)\n- Suppress the warnings where we check for sensitive values (#44148) (#44167)\n- Fix get_task_instance_try_details to return appropriate schema (#43830) (#44133)\n- Log message source details are grouped (#43681) (#44070)\n- Fix duplication of Task tries in the UI (#43891) (#43950)\n- Add correct mime-type in OpenAPI spec (#43879) (#43901)\n- Disable extra links button if link is null or empty (#43844) (#43851)\n- Disable XCom list ordering by execution_date (#43680) (#43696)\n- Fix venv numpy example which needs to be 1.26 at least to be working in Python 3.12 (#43659)\n- Fix Try Selector in Mapped Tasks also on Index 0 (#43590) (#43591)\n- Prevent using ``trigger_rule=\"always\"`` in a dynamic mapped task (#43810)\n- Prevent using ``trigger_rule=TriggerRule.ALWAYS`` in a task-generated mapping within bare tasks (#44751)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Update XCom docs around containers/helm (#44570) (#44573)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Raise deprecation warning when accessing inlet or outlet events through str (#43922)\n\n\nAirflow 2.10.3 (2024-11-05)\n---------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Improves the handling of value masking when setting Airflow variables for enhanced security.  (#43123) (#43278)\n- Adds support for task_instance_mutation_hook to handle mapped operators with index 0. (#42661) (#43089)\n- Fixes executor cleanup to properly handle zombie tasks when task instances are terminated. (#43065)\n- Adds retry logic for HTTP 502 and 504 errors in internal API calls to handle webserver startup issues. (#42994) (#43044)\n- Restores the use of separate sessions for writing and deleting RTIF data to prevent StaleDataError. (#42928) (#43012)\n- Fixes PythonOperator error by replacing hyphens with underscores in DAG names. (#42993)\n- Improving validation of task retries to handle None values (#42532) (#42915)\n- Fixes error handling in dataset managers when resolving dataset aliases into new datasets (#42733)\n- Enables clicking on task names in the DAG Graph View to correctly select the corresponding task. (#38782) (#42697)\n- Prevent redirect loop on /home with tags/last run filters (#42607) (#42609) (#42628)\n- Support of host.name in OTEL metrics and usage of OTEL_RESOURCE_ATTRIBUTES in metrics (#42428) (#42604)\n- Reduce eyestrain in dark mode with reduced contrast and saturation (#42567) (#42583)\n- Handle ENTER key correctly in trigger form and allow manual JSON (#42525) (#42535)\n- Ensure DAG trigger form submits with updated parameters upon keyboard submit (#42487) (#42499)\n- Do not attempt to provide not ``stringified`` objects to UI via xcom if pickling is active (#42388) (#42486)\n- Fix the span link of task instance to point to the correct span in the scheduler_job_loop (#42430) (#42480)\n- Bugfix task execution from runner in Windows (#42426) (#42478)\n- Allows overriding the hardcoded OTEL_SERVICE_NAME with an environment variable (#42242) (#42441)\n- Improves trigger performance by using ``selectinload`` instead of ``joinedload`` (#40487) (#42351)\n- Suppress warnings when masking sensitive configs (#43335) (#43337)\n- Masking configuration values irrelevant to DAG author (#43040) (#43336)\n- Execute templated bash script as file in BashOperator (#43191)\n- Fixes schedule_downstream_tasks to include upstream tasks for one_success trigger rule (#42582) (#43299)\n- Add retry logic in the scheduler for updating trigger timeouts in case of deadlocks. (#41429) (#42651)\n- Mark all tasks as skipped when failing a dag_run manually (#43572)\n- Fix ``TrySelector`` for Mapped Tasks in Logs and Details Grid Panel (#43566)\n- Conditionally add OTEL events when processing executor events (#43558) (#43567)\n- Fix broken stat ``scheduler_loop_duration`` (#42886) (#43544)\n- Ensure total_entries in /api/v1/dags (#43377) (#43429)\n- Include limit and offset in request body schema for List task instances (batch) endpoint (#43479)\n- Don't raise a warning in ExecutorSafeguard when execute is called from an extended operator (#42849) (#43577)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Deprecate session auth backend (#42911)\n- Removed unicodecsv dependency for providers with Airflow version 2.8.0 and above (#42765) (#42970)\n- Remove the referrer from Webserver to Scarf (#42901) (#42942)\n- Bump ``dompurify`` from 2.2.9 to 2.5.6 in /airflow/www (#42263) (#42270)\n- Correct docstring format in _get_template_context (#42244) (#42272)\n- Backport: Bump Flask-AppBuilder to ``4.5.2`` (#43309) (#43318)\n- Check python version that was used to install pre-commit venvs (#43282) (#43310)\n- Resolve warning in Dataset Alias migration (#43425)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Clarifying PLUGINS_FOLDER permissions by DAG authors (#43022) (#43029)\n- Add templating info to TaskFlow tutorial (#42992)\n- Airflow local settings no longer importable from dags folder (#42231) (#42603)\n- Fix documentation for cpu and memory usage (#42147) (#42256)\n- Fix instruction for docker compose (#43119) (#43321)\n- Updates documentation to reflect that dag_warnings is returned instead of import_errors. (#42858) (#42888)\n\n\nAirflow 2.10.2 (2024-09-18)\n---------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Revert \"Fix: DAGs are not marked as stale if the dags folder change\" (#42220, #42217)\n- Add missing open telemetry span and correct scheduled slots documentation (#41985)\n- Fix require_confirmation_dag_change (#42063) (#42211)\n- Only treat null/undefined as falsy when rendering XComEntry (#42199) (#42213)\n- Add extra and ``renderedTemplates`` as keys to skip ``camelCasing`` (#42206) (#42208)\n- Do not ``camelcase`` xcom entries (#42182) (#42187)\n- Fix task_instance and dag_run links from list views (#42138) (#42143)\n- Support multi-line input for Params of type string in trigger UI form (#40414) (#42139)\n- Fix details tab log url detection (#42104) (#42114)\n- Add new type of exception to catch timeout (#42064) (#42078)\n- Rewrite how DAG to dataset / dataset alias are stored (#41987) (#42055)\n- Allow dataset alias to add more than one dataset events (#42189) (#42247)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Limit universal-pathlib below ``0.2.4`` as it breaks our integration (#42101)\n- Auto-fix default deferrable with ``LibCST`` (#42089)\n- Deprecate ``--tree`` flag for ``tasks list`` cli command (#41965)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Update ``security_model.rst`` to clear unauthenticated endpoints exceptions (#42085)\n- Add note about dataclasses and attrs to XComs page (#42056)\n- Improve docs on markdown docs in DAGs (#42013)\n- Add warning that listeners can be dangerous (#41968)\n\n\nAirflow 2.10.1 (2024-09-05)\n---------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Handle Example dags case when checking for missing files (#41874)\n- Fix logout link in \"no roles\" error page (#41845)\n- Set end_date and duration for triggers completed with end_from_trigger as True. (#41834)\n- DAGs are not marked as stale if the dags folder change (#41829)\n- Fix compatibility with FAB provider versions <1.3.0 (#41809)\n- Don't Fail LocalTaskJob on heartbeat (#41810)\n- Remove deprecation warning for cgitb in Plugins Manager (#41793)\n- Fix log for notifier(instance) without ``__name__`` (#41699)\n- Splitting syspath preparation into stages (#41694)\n- Adding url sanitization for extra links (#41680)\n- Fix InletEventsAccessors type stub (#41607)\n- Fix UI rendering when XCom is INT, FLOAT, BOOL or NULL (#41605)\n- Fix try selector refresh (#41503)\n- Incorrect try number subtraction producing invalid span id for OTEL airflow (#41535)\n- Add WebEncoder for trigger page rendering to avoid render failure (#41485)\n- Adding ``tojson`` filter to example_inlet_event_extra example dag (#41890)\n- Add backward compatibility check for executors that don't inherit BaseExecutor (#41927)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Bump webpack from 5.76.0 to 5.94.0 in /airflow/www (#41879)\n- Adding rel property to hyperlinks in logs (#41783)\n- Field Deletion Warning when editing Connections (#41504)\n- Make Scarf usage reporting in major+minor versions and counters in buckets (#41900)\n- Lower down universal-pathlib minimum to 0.2.2 (#41943)\n- Protect against None components of universal pathlib xcom backend (#41938)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Remove Debian bullseye support (#41569)\n- Add an example for auth with ``keycloak`` (#41791)\n\n\nAirflow 2.10.0 (2024-08-15)\n---------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nScarf based telemetry: Airflow now collect telemetry data (#39510)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nAirflow integrates Scarf to collect basic usage data during operation. Deployments can opt-out of data collection by\nsetting the ``[usage_data_collection]enabled`` option to ``False``, or the ``SCARF_ANALYTICS=false`` environment variable.\nSee :ref:`Usage data collection FAQ <usage-data-collection>` for more information.\n\nDatasets no longer trigger inactive DAGs (#38891)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPreviously, when a DAG is paused or removed, incoming dataset events would still\ntrigger it, and the DAG would run when it is unpaused or added back in a DAG\nfile. This has been changed; a DAG's dataset schedule can now only be satisfied\nby events that occur when the DAG is active. While this is a breaking change,\nthe previous behavior is considered a bug.\n\nThe behavior of time-based scheduling is unchanged, including the timetable part\nof ``DatasetOrTimeSchedule``.\n\n``try_number`` is no longer incremented during task execution (#39336)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPreviously, the try number (``try_number``) was incremented at the beginning of task execution on the worker. This was problematic for many reasons.\nFor one it meant that the try number was incremented when it was not supposed to, namely when resuming from reschedule or deferral. And it also resulted in\nthe try number being \"wrong\" when the task had not yet started. The workarounds for these two issues caused a lot of confusion.\n\nNow, instead, the try number for a task run is determined at the time the task is scheduled, and does not change in flight, and it is never decremented.\nSo after the task runs, the observed try number remains the same as it was when the task was running; only when there is a \"new try\" will the try number be incremented again.\n\nOne consequence of this change is, if users were \"manually\" running tasks (e.g. by calling ``ti.run()`` directly, or command line ``airflow tasks run``),\ntry number will no longer be incremented. Airflow assumes that tasks are always run after being scheduled by the scheduler, so we do not regard this as a breaking change.\n\n``/logout`` endpoint in FAB Auth Manager is now CSRF protected (#40145)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe ``/logout`` endpoint's method in FAB Auth Manager has been changed from ``GET`` to ``POST`` in all existing\nAuthViews (``AuthDBView``, ``AuthLDAPView``, ``AuthOAuthView``, ``AuthOIDView``, ``AuthRemoteUserView``), and\nnow includes CSRF protection to enhance security and prevent unauthorized logouts.\n\nOpenTelemetry Traces for Apache Airflow (#37948).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThis new feature adds capability for Apache Airflow to emit 1) airflow system traces of scheduler,\ntriggerer, executor, processor 2) DAG run traces for deployed DAG runs in OpenTelemetry format. Previously, only metrics were supported which emitted metrics in OpenTelemetry.\nThis new feature will add richer data for users to use OpenTelemetry standard to emit and send their trace data to OTLP compatible endpoints.\n\nDecorator for Task Flow ``(@skip_if, @run_if)`` to make it simple to apply whether or not to skip a Task. (#41116)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThis feature adds a decorator to make it simple to skip a Task.\n\nUsing Multiple Executors Concurrently (#40701)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nPreviously known as hybrid executors, this new feature allows Airflow to use multiple executors concurrently. DAGs, or even individual tasks, can be configured\nto use a specific executor that suits its needs best. A single DAG can contain tasks all using different executors. Please see the Airflow documentation for\nmore details. Note: This feature is still experimental. See `documentation on Executor <https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/index.html#using-multiple-executors-concurrently>`_ for a more detailed description.\n\nNew Features\n\"\"\"\"\"\"\"\"\"\"\"\"\n- AIP-61 Hybrid Execution (`AIP-61 <https://github.com/apache/airflow/pulls?q=is%3Apr+label%3Aarea%3Ahybrid-executors+is%3Aclosed+milestone%3A%22Airflow+2.10.0%22>`_)\n- AIP-62 Getting Lineage from Hook Instrumentation (`AIP-62 <https://github.com/apache/airflow/pulls?q=is%3Apr+is%3Amerged+label%3AAIP-62+milestone%3A%22Airflow+2.10.0%22>`_)\n- AIP-64 TaskInstance Try History (`AIP-64 <https://github.com/apache/airflow/pulls?q=is%3Apr+is%3Amerged+label%3AAIP-64+milestone%3A%22Airflow+2.10.0%22>`_)\n- AIP-44 Internal API (`AIP-44 <https://github.com/apache/airflow/pulls?q=is%3Apr+label%3AAIP-44+milestone%3A%22Airflow+2.10.0%22+is%3Aclosed>`_)\n- Enable ending the task directly from the triggerer without going into the worker. (#40084)\n- Extend dataset dependencies (#40868)\n- Feature/add token authentication to internal api (#40899)\n- Add DatasetAlias to support dynamic Dataset Event Emission and Dataset Creation (#40478)\n- Add example DAGs for inlet_events (#39893)\n- Implement ``accessors`` to read dataset events defined as inlet (#39367)\n- Decorator for Task Flow, to make it simple to apply whether or not to skip a Task. (#41116)\n- Add start execution from triggerer support to dynamic task mapping (#39912)\n- Add try_number to log table (#40739)\n- Added ds_format_locale method in macros which allows localizing datetime formatting using Babel (#40746)\n- Add DatasetAlias to support dynamic Dataset Event Emission and Dataset Creation (#40478, #40723, #40809, #41264, #40830, #40693, #41302)\n- Use sentinel to mark dag as removed on re-serialization (#39825)\n- Add parameter for the last number of queries to the DB in DAG file processing stats (#40323)\n- Add prototype version dark mode for Airflow UI (#39355)\n- Add ability to mark some tasks as successful in ``dag test``  (#40010)\n- Allow use of callable for template_fields (#37028)\n- Filter running/failed and active/paused dags on the home page(#39701)\n- Add metrics about task CPU and memory usage (#39650)\n- UI changes for DAG Re-parsing feature (#39636)\n- Add Scarf based telemetry (#39510, #41318)\n- Add dag re-parsing request endpoint (#39138)\n- Redirect to new DAGRun after trigger from Grid view (#39569)\n- Display ``endDate`` in task instance tooltip. (#39547)\n- Implement ``accessors`` to read dataset events defined as inlet (#39367, #39893)\n- Add color to log lines in UI for error and warnings based on keywords (#39006)\n- Add Rendered k8s pod spec tab to ti details view (#39141)\n- Make audit log before/after filterable (#39120)\n- Consolidate grid collapse actions to a single full screen toggle (#39070)\n- Implement Metadata to emit runtime extra (#38650)\n- Add executor field to the DB and parameter to the operators (#38474)\n- Implement context accessor for DatasetEvent extra (#38481)\n- Add dataset event info to dag graph (#41012)\n- Add button to toggle datasets on/off in dag graph (#41200)\n- Add ``run_if`` & ``skip_if`` decorators (#41116)\n- Add dag_stats rest api endpoint (#41017)\n- Add listeners for Dag import errors (#39739)\n- Allowing DateTimeSensorAsync, FileSensor and TimeSensorAsync to start execution from trigger during dynamic task mapping (#41182)\n\n\nImprovements\n\"\"\"\"\"\"\"\"\"\"\"\"\n- Allow set Dag Run resource into Dag Level permission: extends Dag's access_control feature to allow Dag Run resource permissions. (#40703)\n- Improve security and error handling for the internal API (#40999)\n- Datasets UI Improvements (#40871)\n- Change DAG Audit log tab to Event Log (#40967)\n- Make standalone dag file processor works in DB isolation mode (#40916)\n- Show only the source on the consumer DAG page and only triggered DAG run in the producer DAG page (#41300)\n- Update metrics names to allow multiple executors to report metrics (#40778)\n- Format DAG run count (#39684)\n- Update styles for ``renderedjson`` component (#40964)\n- Improve ATTRIBUTE_REMOVED sentinel to use class and more context (#40920)\n- Make XCom display as react json (#40640)\n- Replace usages of task context logger with the log table (#40867)\n- Rollback for all retry exceptions (#40882) (#40883)\n- Support rendering ObjectStoragePath value (#40638)\n- Add try_number and map_index as params for log event endpoint (#40845)\n- Rotate fernet key in batches to limit memory usage (#40786)\n- Add gauge metric for 'last_num_of_db_queries' parameter (#40833)\n- Set parallelism log messages to warning level for better visibility (#39298)\n- Add error handling for encoding the dag runs (#40222)\n- Use params instead of dag_run.conf in example DAG (#40759)\n- Load Example Plugins with Example DAGs (#39999)\n- Stop deferring TimeDeltaSensorAsync task when the target_dttm is in the past (#40719)\n- Send important executor logs to task logs (#40468)\n- Open external links in new tabs (#40635)\n- Attempt to add ReactJSON view to rendered templates (#40639)\n- Speeding up regex match time for custom warnings (#40513)\n- Refactor DAG.dataset_triggers into the timetable class (#39321)\n- add next_kwargs to StartTriggerArgs (#40376)\n- Improve UI error handling (#40350)\n- Remove double warning in CLI  when config value is deprecated (#40319)\n- Implement XComArg concat() (#40172)\n- Added ``get_extra_dejson`` method with nested parameter which allows you to specify if you want the nested json as string to be also deserialized (#39811)\n- Add executor field to the task instance API (#40034)\n- Support checking for db path absoluteness on Windows (#40069)\n- Introduce StartTriggerArgs and prevent start trigger initialization in scheduler (#39585)\n- Add task documentation to details tab in grid view (#39899)\n- Allow executors to be specified with only the class name of the Executor (#40131)\n- Remove obsolete conditional logic related to try_number (#40104)\n- Allow Task Group Ids to be passed as branches in BranchMixIn (#38883)\n- Javascript connection form will apply CodeMirror to all textarea's dynamically (#39812)\n- Determine needs_expansion at time of serialization (#39604)\n- Add indexes on dag_id column in referencing tables to speed up deletion of dag records (#39638)\n- Add task failed dependencies to details page (#38449)\n- Remove webserver try_number adjustment (#39623)\n- Implement slicing in lazy sequence (#39483)\n- Unify lazy db sequence implementations (#39426)\n- Add ``__getattr__`` to task decorator stub (#39425)\n- Allow passing labels to FAB Views registered via Plugins (#39444)\n- Simpler error message when trying to offline migrate with sqlite (#39441)\n- Add soft_fail to TriggerDagRunOperator (#39173)\n- Rename \"dataset event\" in context to use \"outlet\" (#39397)\n- Resolve ``RemovedIn20Warning`` in ``airflow task`` command (#39244)\n- Determine fail_stop on client side when db isolated (#39258)\n- Refactor cloudpickle support in Python operators/decorators (#39270)\n- Update trigger kwargs migration to specify existing_nullable (#39361)\n- Allowing tasks to start execution directly from triggerer without going to worker (#38674)\n- Better ``db migrate`` error messages (#39268)\n- Add stacklevel into the ``suppress_and_warn`` warning (#39263)\n- Support searching by dag_display_name (#39008)\n- Allow sort by on all fields in MappedInstances.tsx (#38090)\n- Expose count of scheduled tasks in metrics (#38899)\n- Use ``declarative_base`` from ``sqlalchemy.orm`` instead of ``sqlalchemy.ext.declarative`` (#39134)\n- Add example DAG to demonstrate emitting approaches (#38821)\n- Give ``on_task_instance_failed`` access to the error that caused the failure (#38155)\n- Simplify dataset serialization (#38694)\n- Add heartbeat recovery message to jobs (#34457)\n- Remove select_column option in TaskInstance.get_task_instance (#38571)\n- Don't create session in get_dag if not reading dags from database (#38553)\n- Add a migration script for encrypted trigger kwargs (#38358)\n- Implement render_templates on TaskInstancePydantic (#38559)\n- Handle optional session in _refresh_from_db (#38572)\n- Make type annotation less confusing in task_command.py (#38561)\n- Use fetch_dagrun directly to avoid session creation (#38557)\n- Added ``output_processor`` parameter to ``BashProcessor`` (#40843)\n- Improve serialization for Database Isolation Mode (#41239)\n- Only orphan non-orphaned Datasets (#40806)\n- Adjust gantt width based on task history dates (#41192)\n- Enable scrolling on legend with high number of elements. (#41187)\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Bugfix for get_parsing_context() when ran with LocalExecutor (#40738)\n- Validating provider documentation urls before displaying in views (#40933)\n- Move import to make PythonOperator working on Windows (#40424)\n- Fix dataset_with_extra_from_classic_operator example DAG (#40747)\n- Call listener on_task_instance_failed() after ti state is changed (#41053)\n- Add ``never_fail`` in BaseSensor (#40915)\n- Fix tasks API endpoint when DAG doesn't have ``start_date`` (#40878)\n- Fix and adjust URL generation for UI grid and older runs (#40764)\n- Rotate fernet key optimization (#40758)\n- Fix class instance vs. class type in validate_database_executor_compatibility() call (#40626)\n- Clean up dark mode (#40466)\n- Validate expected types for args for DAG, BaseOperator and TaskGroup (#40269)\n- Exponential Backoff Not Functioning in BaseSensorOperator Reschedule Mode (#39823)\n- local task job: add timeout, to not kill on_task_instance_success listener prematurely (#39890)\n- Move Post Execution Log Grouping behind Exception Print (#40146)\n- Fix triggerer race condition in HA setting (#38666)\n- Pass triggered or existing DAG Run logical date to DagStateTrigger (#39960)\n- Passing ``external_task_group_id`` to ``WorkflowTrigger`` (#39617)\n- ECS Executor: Set tasks to RUNNING state once active (#39212)\n- Only heartbeat if necessary in backfill loop (#39399)\n- Fix trigger kwarg encryption migration (#39246)\n- Fix decryption of trigger kwargs when downgrading. (#38743)\n- Fix wrong link in TriggeredDagRuns (#41166)\n- Pass MapIndex to LogLink component for external log systems (#41125)\n- Add NonCachingRotatingFileHandler for worker task (#41064)\n- Add argument include_xcom in method resolve an optional value (#41062)\n- Sanitizing file names in example_bash_decorator DAG (#40949)\n- Show dataset aliases in dependency graphs (#41128)\n- Render Dataset Conditions in DAG Graph view (#41137)\n- Add task duration plot across dagruns (#40755)\n- Add start execution from trigger support for existing core sensors (#41021)\n- add example dag for dataset_alias (#41037)\n- Add dataset alias unique constraint and remove wrong dataset alias removing logic (#41097)\n- Set \"has_outlet_datasets\" to true if \"dataset alias\" exists (#41091)\n- Make HookLineageCollector group datasets by (#41034)\n- Enhance start_trigger_args serialization (#40993)\n- Refactor ``BaseSensorOperator`` introduce ``skip_policy`` parameter (#40924)\n- Fix viewing logs from triggerer when task is deferred (#41272)\n- Refactor how triggered dag run url is replaced (#41259)\n- Added support for additional sql alchemy session args (#41048)\n- Allow empty list in TriggerDagRun failed_state (#41249)\n- Clean up the exception handler when run_as_user is the airflow user  (#41241)\n- Collapse docs when click and folded (#41214)\n- Update updated_at when saving to db as session.merge does not trigger on-update (#40782)\n- Fix query count statistics when parsing DAF file (#41149)\n- Method Resolution Order in operators without ``__init__`` (#41086)\n- Ensure try_number incremented for empty operator (#40426)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Remove the Experimental flag from ``OTel`` Traces (#40874)\n- Bump packaging version to 23.0 in order to fix issue with older otel (#40865)\n- Simplify _auth_manager_is_authorized_map function (#40803)\n- Use correct unknown executor exception in scheduler job (#40700)\n- Add D1 ``pydocstyle`` rules to pyproject.toml (#40569)\n- Enable enforcing ``pydocstyle`` rule D213 in ruff. (#40448, #40464)\n- Update ``Dag.test()`` to run with an executor if desired (#40205)\n- Update jest and babel minor versions (#40203)\n- Refactor BashOperator and Bash decorator for consistency and simplicity (#39871)\n- Add ``AirflowInternalRuntimeError`` for raise ``non catchable`` errors (#38778)\n- ruff version bump 0.4.5 (#39849)\n- Bump ``pytest`` to 8.0+ (#39450)\n- Remove stale comment about TI index (#39470)\n- Configure ``back_populates`` between ``DagScheduleDatasetReference.dag`` and ``DagModel.schedule_dataset_references`` (#39392)\n- Remove deprecation warnings in endpoints.py (#39389)\n- Fix SQLA deprecations in Airflow core (#39211)\n- Use class-bound attribute directly in SA (#39198, #39195)\n- Fix stacklevel for TaskContextLogger (#39142)\n- Capture warnings during collect DAGs (#39109)\n- Resolve ``B028`` (no-explicit-stacklevel) in core (#39123)\n- Rename model ``ImportError`` to ``ParseImportError`` for avoid shadowing with builtin exception (#39116)\n- Add option to support cloudpickle in PythonVenv/External Operator (#38531)\n- Suppress ``SubDagOperator`` examples warnings (#39057)\n- Add log for running callback (#38892)\n- Use ``model_dump`` instead of ``dict`` for serialize Pydantic V2 model (#38933)\n- Widen cheat sheet column to avoid wrapping commands (#38888)\n- Update hatchling to latest version (1.22.5) (#38780)\n- bump uv to 0.1.29 (#38758)\n- Add missing serializations found during provider tests fixing (#41252)\n- Bump ``ws`` from 7.5.5 to 7.5.10 in /airflow/www (#40288)\n- Improve typing for allowed/failed_states in TriggerDagRunOperator (#39855)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Add ``filesystems`` and ``dataset-uris`` to \"how to create your own provider\" page (#40801)\n- Fix (TM) to (R) in Airflow repository (#40783)\n- Set ``otel_on`` to True in example airflow.cfg (#40712)\n- Add warning for _AIRFLOW_PATCH_GEVENT  (#40677)\n- Update multi-team diagram proposal after Airflow 3 discussions (#40671)\n- Add stronger warning that MSSQL is not supported and no longer functional (#40565)\n- Fix misleading mac menu structure in howto (#40440)\n- Update k8s supported version in docs (#39878)\n- Add compatibility note for Listeners (#39544)\n- Update edge label image in documentation example with the new graph view (#38802)\n- Update UI doc screenshots (#38680)\n- Add section \"Manipulating queued dataset events through REST API\" (#41022)\n- Add information about lack of security guarantees for docker compose (#41072)\n- Add links to example dags in use params section (#41031)\n- Change ``task_id`` from ``send_email`` to ``send_email_notification`` in ``taskflow.rst`` (#41060)\n- Remove unnecessary nginx redirect rule from reverse proxy documentation (#38953)\n\n\n\nAirflow 2.9.3 (2024-07-15)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nTime unit for ``scheduled_duration`` and ``queued_duration`` changed (#37936)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n``scheduled_duration`` and ``queued_duration`` metrics are now emitted in milliseconds instead of seconds.\n\nBy convention all statsd metrics should be emitted in milliseconds, this is later expected in e.g. ``prometheus`` statsd-exporter.\n\n\nSupport for OpenTelemetry Metrics is no longer \"Experimental\" (#40286)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nExperimental support for OpenTelemetry was added in 2.7.0 since then fixes and improvements were added and now we announce the feature as stable.\n\n\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Fix calendar view scroll (#40458)\n- Validating provider description for urls in provider list view (#40475)\n- Fix compatibility with old MySQL 8.0 (#40314)\n- Fix dag (un)pausing won't work on environment where dag files are missing (#40345)\n- Extra being passed to SQLalchemy (#40391)\n- Handle unsupported operand int + str when value of tag is int (job_id) (#40407)\n- Fix TriggeredDagRunOperator triggered link (#40336)\n- Add ``[webserver]update_fab_perms`` to deprecated configs (#40317)\n- Swap dag run link from legacy graph to grid with graph tab (#40241)\n- Change ``httpx`` to ``requests`` in ``file_task_handler`` (#39799)\n- Fix import future annotations in venv jinja template (#40208)\n- Ensures DAG params order regardless of backend (#40156)\n- Use a join for TI notes in TI batch API endpoint (#40028)\n- Improve trigger UI for string array format validation (#39993)\n- Disable jinja2 rendering for doc_md (#40522)\n- Skip checking sub dags list if taskinstance state is skipped (#40578)\n- Recognize quotes when parsing urls in logs (#40508)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Add notes about passing secrets via environment variables (#40519)\n- Revamp some confusing log messages (#40334)\n- Add more precise description of masking sensitive field names (#40512)\n- Add slightly more detailed guidance about upgrading to the docs (#40227)\n- Metrics allow_list complete example (#40120)\n- Add warning to deprecated api docs that access control isn't applied (#40129)\n- Simpler command to check local scheduler is alive (#40074)\n- Add a note and an example clarifying the usage of DAG-level params (#40541)\n- Fix highlight of example code in dags.rst (#40114)\n- Add warning about the PostgresOperator being deprecated (#40662)\n- Updating airflow download links to CDN based links (#40618)\n- Fix import statement for DatasetOrTimetable example (#40601)\n- Further clarify triage process (#40536)\n- Fix param order in PythonOperator docstring (#40122)\n- Update serializers.rst to mention that bytes are not supported (#40597)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Upgrade build installers and dependencies (#40177)\n- Bump braces from 3.0.2 to 3.0.3 in /airflow/www (#40180)\n- Upgrade to another version of trove-classifier (new CUDA classifiers) (#40564)\n- Rename \"try_number\" increments that are unrelated to the airflow concept (#39317)\n- Update trove classifiers to the latest version as build dependency (#40542)\n- Upgrade to latest version of hatchling as build dependency (#40387)\n- Fix bug in ``SchedulerJobRunner._process_executor_events`` (#40563)\n- Remove logging for \"blocked\" events (#40446)\n\n\n\nAirflow 2.9.2 (2024-06-10)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Fix bug that makes ``AirflowSecurityManagerV2`` leave transactions in the ``idle in transaction`` state (#39935)\n- Fix alembic auto-generation and rename mismatching constraints (#39032)\n- Add the existing_nullable to the downgrade side of the migration (#39374)\n- Fix Mark Instance state buttons stay disabled if user lacks permission (#37451). (#38732)\n- Use SKIP LOCKED instead of NOWAIT in mini scheduler (#39745)\n- Remove DAG Run Add option from FAB view (#39881)\n- Add max_consecutive_failed_dag_runs in API spec (#39830)\n- Fix example_branch_operator failing in python 3.12 (#39783)\n- Fetch served logs also when task attempt is up for retry and no remote logs available (#39496)\n- Change dataset URI validation to raise warning instead of error in Airflow 2.9 (#39670)\n- Visible DAG RUN doesn't point to the same dag run id (#38365)\n- Refactor ``SafeDogStatsdLogger`` to use ``get_validator`` to enable pattern matching (#39370)\n- Fix custom actions in security manager ``has_access`` (#39421)\n- Fix HTTP 500 Internal Server Error if DAG is triggered with bad params (#39409)\n- Fix static file caching is disabled in Airflow Webserver. (#39345)\n- Fix TaskHandlerWithCustomFormatter now adds prefix only once (#38502)\n- Do not provide deprecated ``execution_date`` in ``@apply_lineage`` (#39327)\n- Add missing conn_id to string representation of ObjectStoragePath (#39313)\n- Fix ``sql_alchemy_engine_args`` config example (#38971)\n- Add Cache-Control \"no-store\" to all dynamically generated content (#39550)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Limit ``yandex`` provider to avoid ``mypy`` errors (#39990)\n- Warn on mini scheduler failures instead of debug (#39760)\n- Change type definition for ``provider_info_cache`` decorator (#39750)\n- Better typing for BaseOperator ``defer`` (#39742)\n- More typing in TimeSensor and TimeSensorAsync (#39696)\n- Re-raise exception from strict dataset URI checks (#39719)\n- Fix stacklevel for _log_state helper (#39596)\n- Resolve SA warnings in migrations scripts (#39418)\n- Remove unused index ``idx_last_scheduling_decision`` on ``dag_run`` table (#39275)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Provide extra tip on labeling DynamicTaskMapping (#39977)\n- Improve visibility of links / variables / other configs in Configuration Reference (#39916)\n- Remove 'legacy' definition for ``CronDataIntervalTimetable`` (#39780)\n- Update plugins.rst examples to use pyproject.toml over setup.py (#39665)\n- Fix nit in pg set-up doc (#39628)\n- Add Matomo to Tracking User Activity docs (#39611)\n- Fix Connection.get -> Connection. get_connection_from_secrets (#39560)\n- Adding note for provider dependencies (#39512)\n- Update docker-compose command (#39504)\n- Update note about restarting triggerer process (#39436)\n- Updating S3LogLink with an invalid bucket link (#39424)\n- Update testing_packages.rst (#38996)\n- Add multi-team diagrams (#38861)\n\n\n\nAirflow 2.9.1 (2024-05-03)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nStackdriver logging bugfix requires Google provider ``10.17.0`` or later (#38071)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIf you use Stackdriver logging, you must use Google provider version ``10.17.0`` or later. Airflow ``2.9.1`` now passes ``gcp_log_name`` to the ``StackdriverTaskHandler`` instead of ``name``, and this will fail on earlier provider versions.\n\nThis fixes a bug where the log name configured in ``[logging] remove_base_log_folder`` was overridden when Airflow configured logging, resulting in task logs going to the wrong destination.\n\n\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Make task log messages include run_id (#39280)\n- Copy menu_item ``href`` for nav bar (#39282)\n- Fix trigger kwarg encryption migration (#39246, #39361, #39374)\n- Add workaround for datetime-local input in ``firefox`` (#39261)\n- Add Grid button to Task Instance view (#39223)\n- Get served logs when remote or executor logs not available for non-running task try (#39177)\n- Fixed side effect of menu filtering causing disappearing menus (#39229)\n- Use grid view for Task Instance's ``log_url`` (#39183)\n- Improve task filtering ``UX`` (#39119)\n- Improve rendered_template ``ux`` in react dag page (#39122)\n- Graph view improvements (#38940)\n- Check that the dataset<>task exists before trying to render graph (#39069)\n- Hostname was \"redacted\", not \"redact\"; remove it when there is no context (#39037)\n- Check whether ``AUTH_ROLE_PUBLIC`` is set in ``check_authentication`` (#39012)\n- Move rendering of ``map_index_template`` so it renders for failed tasks as long as it was defined before the point of failure (#38902)\n- ``Undeprecate`` ``BaseXCom.get_one`` method for now (#38991)\n- Add ``inherit_cache`` attribute for ``CreateTableAs`` custom SA Clause (#38985)\n- Don't wait for DagRun lock in mini scheduler (#38914)\n- Fix calendar view with no DAG Run (#38964)\n- Changed the background color of external task in graph (#38969)\n- Fix dag run selection (#38941)\n- Fix ``SAWarning`` 'Coercing Subquery object into a select() for use in IN()' (#38926)\n- Fix implicit ``cartesian`` product in AirflowSecurityManagerV2 (#38913)\n- Fix problem that links in legacy log view can not be clicked (#38882)\n- Fix dag run link params (#38873)\n- Use async db calls in WorkflowTrigger (#38689)\n- Fix audit log events filter (#38719)\n- Use ``methodtools.lru_cache`` instead of ``functools.lru_cache`` in class methods (#37757)\n- Raise deprecated warning in ``airflow dags backfill`` only if ``-I`` / ``--ignore-first-depends-on-past`` provided (#38676)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- ``TriggerDagRunOperator`` deprecate ``execution_date`` in favor of ``logical_date`` (#39285)\n- Force to use Airflow Deprecation warnings categories on ``@deprecated`` decorator (#39205)\n- Add warning about run/import Airflow under the Windows (#39196)\n- Update ``is_authorized_custom_view`` from auth manager to handle custom actions (#39167)\n- Add in Trove classifiers Python 3.12 support (#39004)\n- Use debug level for ``minischeduler`` skip (#38976)\n- Bump ``undici`` from ``5.28.3 to 5.28.4`` in ``/airflow/www`` (#38751)\n\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Fix supported k8s version in docs (#39172)\n- Dynamic task mapping ``PythonOperator`` op_kwargs (#39242)\n- Add link to ``user`` and ``role`` commands (#39224)\n- Add ``k8s 1.29`` to supported version in docs (#39168)\n- Data aware scheduling docs edits (#38687)\n- Update ``DagBag`` class docstring to include all params (#38814)\n- Correcting an example taskflow example (#39015)\n- Remove decorator from rendering fields example (#38827)\n\n\n\nAirflow 2.9.0 (2024-04-08)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nFollowing Listener API methods are considered stable and can be used for production system (were experimental feature in older Airflow versions) (#36376):\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nLifecycle events:\n\n- ``on_starting``\n- ``before_stopping``\n\nDagRun State Change Events:\n\n- ``on_dag_run_running``\n- ``on_dag_run_success``\n- ``on_dag_run_failed``\n\nTaskInstance State Change Events:\n\n- ``on_task_instance_running``\n- ``on_task_instance_success``\n- ``on_task_instance_failed``\n\nSupport for Microsoft SQL-Server for Airflow Meta Database has been removed (#36514)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nAfter `discussion <https://lists.apache.org/thread/r06j306hldg03g2my1pd4nyjxg78b3h4>`__\nand a `voting process <https://lists.apache.org/thread/pgcgmhf6560k8jbsmz8nlyoxosvltph2>`__,\nthe Airflow's PMC members and Committers have reached a resolution to no longer maintain MsSQL as a\nsupported Database Backend.\n\nAs of Airflow 2.9.0 support of MsSQL has been removed for Airflow Database Backend.\n\nA migration script which can help migrating the database *before* upgrading to Airflow 2.9.0 is available in\n`airflow-mssql-migration repo on Github <https://github.com/apache/airflow-mssql-migration>`_.\nNote that the migration script is provided without support and warranty.\n\nThis does not affect the existing provider packages (operators and hooks), DAGs can still access and process data from MsSQL.\n\nDataset URIs are now validated on input (#37005)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nDatasets must use a URI that conform to rules laid down in AIP-60, and the value\nwill be automatically normalized when the DAG file is parsed. See\n`documentation on Datasets <https://airflow.apache.org/docs/apache-airflow/2.9.0/authoring-and-scheduling/datasets.html>`_ for\na more detailed description on the rules.\n\nYou may need to change your Dataset identifiers if they look like a URI, but are\nused in a less mainstream way, such as relying on the URI's auth section, or\nhave a case-sensitive protocol name.\n\nThe method ``get_permitted_menu_items`` in ``BaseAuthManager`` has been renamed ``filter_permitted_menu_items`` (#37627)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nAdd REST API actions to Audit Log events (#37734)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe Audit Log ``event`` name for REST API events will be prepended with ``api.`` or ``ui.``, depending on if it came from the Airflow UI or externally.\n\nOfficial support for Python 3.12 (#38025)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThere are a few caveats though:\n\n* Pendulum2 does not support Python 3.12. For Python 3.12 you need to use\n  `Pendulum 3 <https://pendulum.eustace.io/blog/announcing-pendulum-3-0-0.html>`_\n\n* Minimum SQLAlchemy version supported when Pandas is installed for Python 3.12 is ``1.4.36`` released in\n  April 2022. Airflow 2.9.0 increases the minimum supported version of SQLAlchemy to ``1.4.36`` for all\n  Python versions.\n\nNot all Providers support Python 3.12. At the initial release of Airflow 2.9.0 the following providers\nare released without support for Python 3.12:\n\n  * ``apache.beam`` - pending on `Apache Beam support for 3.12 <https://github.com/apache/beam/issues/29149>`_\n  * ``papermill`` - pending on Releasing Python 3.12 compatible papermill client version\n    `including this merged issue <https://github.com/nteract/papermill/pull/771>`_\n\nPrevent large string objects from being stored in the Rendered Template Fields (#38094)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThere's now a limit to the length of data that can be stored in the Rendered Template Fields.\nThe limit is set to 4096 characters. If the data exceeds this limit, it will be truncated. You can change this limit\nby setting the ``[core]max_template_field_length`` configuration option in your airflow config.\n\nChange xcom table column value type to longblob for MySQL backend (#38401)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nXcom table column ``value`` type has changed from ``blob`` to ``longblob``. This will allow you to store relatively big data in Xcom but process can take a significant amount of time if you have a lot of large data stored in Xcom.\n\nTo downgrade from revision: ``b4078ac230a1``, ensure that you don't have Xcom values larger than 65,535 bytes. Otherwise, you'll need to clean those rows or run ``airflow db clean xcom`` to clean the Xcom table.\n\nStronger validation for key parameter defaults in taskflow context variables (#38015)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nAs for the taskflow implementation in conjunction with context variable defaults invalid parameter orders can be\ngenerated, it is now not accepted anymore (and validated) that taskflow functions are defined with defaults\nother than ``None``. If you have done this before you most likely will see a broken DAG and a error message like\n``Error message: Context key parameter my_param can't have a default other than None``.\n\nNew Features\n\"\"\"\"\"\"\"\"\"\"\"\"\n- Allow users to write dag_id and task_id in their national characters, added display name for dag / task (v2) (#38446)\n- Prevent large objects from being stored in the RTIF (#38094)\n- Use current time to calculate duration when end date is not present. (#38375)\n- Add average duration mark line in task and dagrun duration charts. (#38214, #38434)\n- Add button to manually create dataset events (#38305)\n- Add ``Matomo`` as an option for analytics_tool. (#38221)\n- Experimental: Support custom weight_rule implementation to calculate the TI priority_weight (#38222)\n- Adding ability to automatically set DAG to off after X times it failed sequentially (#36935)\n- Add dataset conditions to next run datasets modal (#38123)\n- Add task log grouping to UI (#38021)\n- Add dataset_expression to grid dag details (#38121)\n- Introduce mechanism to support multiple executor configuration (#37635)\n- Add color formatting for ANSI chars in logs from task executions (#37985)\n- Add the dataset_expression as part of DagModel and DAGDetailSchema (#37826)\n- Allow longer rendered_map_index (#37798)\n- Inherit the run_ordering from DatasetTriggeredTimetable for DatasetOrTimeSchedule (#37775)\n- Implement AIP-60 Dataset URI formats (#37005)\n- Introducing Logical Operators for dataset conditional logic (#37101)\n- Add post endpoint for dataset events (#37570)\n- Show custom instance names for a mapped task in UI (#36797)\n- Add excluded/included events to get_event_logs api (#37641)\n- Add datasets to dag graph (#37604)\n- Show dataset events above task/run details in grid view (#37603)\n- Introduce new config variable to control whether DAG processor outputs to stdout (#37439)\n- Make Datasets ``hashable`` (#37465)\n- Add conditional logic for dataset triggering (#37016)\n- Implement task duration page in react. (#35863)\n- Add ``queuedEvent`` endpoint to get/delete DatasetDagRunQueue (#37176)\n- Support multiple XCom output in the BaseOperator (#37297)\n- AIP-58: Add object storage backend for xcom (#37058)\n- Introduce ``DatasetOrTimeSchedule`` (#36710)\n- Add ``on_skipped_callback`` to ``BaseOperator`` (#36374)\n- Allow override of hovered navbar colors (#36631)\n- Create new Metrics with Tagging (#36528)\n- Add support for openlineage to AFS and common.io (#36410)\n- Introduce ``@task.bash`` TaskFlow decorator (#30176, #37875)\n\nImprovements\n\"\"\"\"\"\"\"\"\"\"\"\"\n- More human friendly \"show tables\" output for db cleanup (#38654)\n- Improve trigger assign_unassigned by merging alive_triggerer_ids and get_sorted_triggers queries (#38664)\n- Add exclude/include events filters to audit log (#38506)\n- Clean up unused triggers in a single query for all dialects except MySQL (#38663)\n- Update Confirmation Logic for Config Changes on Sensitive Environments Like Production (#38299)\n- Improve datasets graph UX (#38476)\n- Only show latest dataset event timestamp after last run (#38340)\n- Add button to clear only failed tasks in a dagrun. (#38217)\n- Delete all old dag pages and redirect to grid view (#37988)\n- Check task attribute before use in sentry.add_tagging() (#37143)\n- Mysql change xcom value col type for MySQL backend (#38401)\n- ``ExternalPythonOperator`` use version from ``sys.version_info`` (#38377)\n- Replace too broad exceptions into the Core (#38344)\n- Add CLI support for bulk pause and resume of DAGs (#38265)\n- Implement methods on TaskInstancePydantic and DagRunPydantic (#38295, #38302, #38303, #38297)\n- Made filters bar collapsible and add a full screen toggle (#38296)\n- Encrypt all trigger attributes (#38233, #38358, #38743)\n- Upgrade react-table package. Use with Audit Log table (#38092)\n- Show if dag page filters are active (#38080)\n- Add try number to mapped instance (#38097)\n- Add retries to job heartbeat (#37541)\n- Add REST API events to Audit Log (#37734)\n- Make current working directory as templated field in BashOperator (#37968)\n- Add calendar view to react (#37909)\n- Add ``run_id`` column to log table (#37731)\n- Add ``tryNumber`` to grid task instance tooltip (#37911)\n- Session is not used in _do_render_template_fields (#37856)\n- Improve MappedOperator property types (#37870)\n- Remove provide_session decorator from TaskInstancePydantic methods (#37853)\n- Ensure the \"airflow.task\" logger used for TaskInstancePydantic and TaskInstance (#37857)\n- Better error message for internal api call error (#37852)\n- Increase tooltip size of dag grid view (#37782) (#37805)\n- Use named loggers instead of root logger (#37801)\n- Add Run Duration in React (#37735)\n- Avoid non-recommended usage of logging (#37792)\n- Improve DateTimeTrigger typing (#37694)\n- Make sure all unique run_ids render a task duration bar (#37717)\n- Add Dag Audit Log to React (#37682)\n- Add log event for auto pause (#38243)\n- Better message for exception for templated base operator fields (#37668)\n- Clean up webserver endpoints adding to audit log (#37580)\n- Filter datasets graph by dag_id (#37464)\n- Use new exception type inheriting BaseException for SIGTERMs (#37613)\n- Refactor dataset class inheritance (#37590)\n- Simplify checks for package versions (#37585)\n- Filter Datasets by associated dag_ids (GET /datasets) (#37512)\n- Enable \"airflow tasks test\" to run deferrable operator (#37542)\n- Make datasets list/graph width adjustable (#37425)\n- Speedup determine installed airflow version in ``ExternalPythonOperator`` (#37409)\n- Add more task details from rest api (#37394)\n- Add confirmation dialog box for DAG run actions (#35393)\n- Added shutdown color to the STATE_COLORS (#37295)\n- Remove legacy dag details page and redirect to grid (#37232)\n- Order XCom entries by map index in API (#37086)\n- Add data_interval_start and data_interval_end in dagrun create API endpoint (#36630)\n- Making links in task logs as hyperlinks by preventing HTML injection (#36829)\n- Improve ExternalTaskSensor Async Implementation (#36916)\n- Make Datasets ``Pathlike`` (#36947)\n- Simplify query for orphaned tasks (#36566)\n- Add deferrable param in FileSensor (#36840)\n- Run Trigger Page: Configurable number of recent configs (#36878)\n- Merge ``nowait`` and skip_locked into with_row_locks (#36889)\n- Return the specified field when get ``dag/dagRun`` in the REST API (#36641)\n- Only iterate over the items if debug is enabled for DagFileProcessorManager (#36761)\n- Add a fuzzy/regex pattern-matching for metric allow and block list (#36250)\n- Allow custom columns in cli dags list (#35250)\n- Make it possible to change the default cron timetable (#34851)\n- Some improvements to Airflow IO code (#36259)\n- Improve TaskInstance typing hints (#36487)\n- Remove dependency of ``Connexion`` from auth manager interface (#36209)\n- Refactor ExternalDagLink to not create ad hoc TaskInstances (#36135)\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Load providers configuration when gunicorn workers start (#38795)\n- Fix grid header rendering (#38720)\n- Add a task instance dependency for mapped dependencies (#37498)\n- Improve stability of remove_task_decorator function (#38649)\n- Mark more fields on API as dump-only (#38616)\n- Fix ``total_entries`` count on the event logs endpoint (#38625)\n- Add padding to bottom of log block. (#38610)\n- Properly serialize nested attrs classes (#38591)\n- Fixing the ``tz`` in next run ID info (#38482)\n- Show abandoned tasks in Grid View (#38511)\n- Apply task instance mutation hook consistently (#38440)\n- Override ``chakra`` styles to keep ``dropdowns`` in filter bar (#38456)\n- Store duration in seconds and scale to handle case when a value in the series has a larger unit than the preceding durations. (#38374)\n- Don't allow defaults other than None in context parameters, and improve error message (#38015)\n- Make postgresql default engine args comply with SA 2.0 (#38362)\n- Add return statement to yield within a while loop in triggers (#38389)\n- Ensure ``__exit__`` is called in decorator context managers (#38383)\n- Make the method ``BaseAuthManager.is_authorized_custom_view`` abstract (#37915)\n- Add upper limit to planned calendar events calculation (#38310)\n- Fix Scheduler in daemon mode doesn't create PID at the specified location (#38117)\n- Properly serialize TaskInstancePydantic and DagRunPydantic (#37855)\n- Fix graph task state border color (#38084)\n- Add back methods removed in security manager (#37997)\n- Don't log \"403\" from worker serve-logs as \"Unknown error\". (#37933)\n- Fix execution data validation error in ``/get_logs_with_metadata`` endpoint (#37756)\n- Fix task duration selection (#37630)\n- Refrain from passing ``encoding`` to the SQL engine in SQLAlchemy v2 (#37545)\n- Fix 'implicitly coercing SELECT object to scalar subquery' in latest dag run statement (#37505)\n- Clean up typing with max_execution_date query builder (#36958)\n- Optimize max_execution_date query in single dag case (#33242)\n- Fix list dags command for get_dagmodel is None (#36739)\n- Load ``consuming_dags`` attr eagerly before dataset listener (#36247)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Remove display of param from the UI (#38660)\n- Update log level to debug from warning about scheduled_duration metric (#38180)\n- Use ``importlib_metadata`` with compat to Python 3.10/3.12 ``stdlib`` (#38366)\n- Refactored ``__new__`` magic method of BaseOperatorMeta to avoid bad mixing classic and decorated operators (#37937)\n- Use ``sys.version_info`` for determine Python Major.Minor (#38372)\n- Add missing deprecated Fab auth manager (#38376)\n- Remove unused loop variable from airflow package (#38308)\n- Adding max consecutive failed dag runs info in UI (#38229)\n- Bump minimum version of ``blinker`` add where it requires (#38140)\n- Bump follow-redirects from 1.15.4 to 1.15.6 in /airflow/www (#38156)\n- Bump Cryptography to ``> 39.0.0`` (#38112)\n- Add Python 3.12 support (#36755, #38025, #36595)\n- Avoid use of ``assert`` outside of the tests (#37718)\n- Update ObjectStoragePath for universal_pathlib>=v0.2.2 (#37930)\n- Resolve G004: Logging statement uses f-string (#37873)\n- Update build and install dependencies. (#37910)\n- Bump sanitize-html from 2.11.0 to 2.12.1 in /airflow/www (#37833)\n- Update to latest installer versions. (#37754)\n- Deprecate smtp configs in airflow settings / local_settings (#37711)\n- Deprecate PY* constants into the airflow module (#37575)\n- Remove usage of deprecated ``flask._request_ctx_stack`` (#37522)\n- Remove redundant ``login`` attribute in ``airflow.__init__.py`` (#37565)\n- Upgrade to FAB 4.3.11 (#37233)\n- Remove SCHEDULED_DEPS which is no longer used anywhere since 2.0.0 (#37140)\n- Replace ``datetime.datetime.utcnow`` by ``airflow.utils.timezone.utcnow`` in core (#35448)\n- Bump aiohttp min version to avoid CVE-2024-23829 and CVE-2024-23334 (#37110)\n- Move config related to FAB auth manager to FAB provider (#36232)\n- Remove MSSQL support form Airflow core (#36514)\n- Remove ``is_authorized_cluster_activity`` from auth manager (#36175)\n- Create FAB provider and move FAB auth manager in it (#35926)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Improve timetable documentation (#38505)\n- Reorder OpenAPI Spec tags alphabetically (#38717)\n- Update UI screenshots in the documentation (#38680, #38403, #38438, #38435)\n- Remove section as it's no longer true with dataset expressions PR (#38370)\n- Refactor DatasetOrTimeSchedule timetable docs (#37771)\n- Migrate executor docs to respective providers (#37728)\n- Add directive to render a list of URI schemes (#37700)\n- Add doc page with providers deprecations (#37075)\n- Add a cross reference to security policy (#37004)\n- Improve AIRFLOW__WEBSERVER__BASE_URL docs (#37003)\n- Update faq.rst with (hopefully) clearer description of start_date (#36846)\n- Update public interface doc re operators (#36767)\n- Add ``exception`` to templates ref list (#36656)\n- Add auth manager interface as public interface (#36312)\n- Reference fab provider documentation in Airflow documentation (#36310)\n- Create auth manager documentation (#36211)\n- Update permission docs (#36120)\n- Docstring improvement to _covers_every_hour (#36081)\n- Add note that task instance, dag and lifecycle listeners are non-experimental (#36376)\n\n\nAirflow 2.8.4 (2024-03-25)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Fix incorrect serialization of ``FixedTimezone`` (#38139)\n- Fix excessive permission changing for log task handler (#38164)\n- Fix task instances list link (#38096)\n- Fix a bug where scheduler heartrate parameter was not used (#37992)\n- Add padding to prevent grid horizontal scroll overlapping tasks (#37942)\n- Fix hash caching in ``ObjectStoragePath`` (#37769)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Limit importlib_resources as it breaks ``pytest_rewrites`` (#38095, #38139)\n- Limit ``pandas`` to ``<2.2`` (#37748)\n- Bump ``croniter`` to fix an issue with 29 Feb cron expressions (#38198)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Tell users what to do if their scanners find issues in the image (#37652)\n- Add a section about debugging in Docker Compose with PyCharm (#37940)\n- Update deferrable docs to clarify kwargs when trigger resumes operator (#38122)\n\n\nAirflow 2.8.3 (2024-03-11)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nThe smtp provider is now pre-installed when you install Airflow. (#37713)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Add \"MENU\" permission in auth manager (#37881)\n- Fix external_executor_id being overwritten (#37784)\n- Make more MappedOperator members modifiable (#37828)\n- Set parsing context dag_id in dag test command (#37606)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Remove useless methods from security manager (#37889)\n- Improve code coverage for TriggerRuleDep (#37680)\n- The SMTP provider is now preinstalled when installing Airflow (#37713)\n- Bump min versions of openapi validators (#37691)\n- Properly include ``airflow_pre_installed_providers.txt`` artifact (#37679)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Clarify lack of sync between workers and scheduler (#37913)\n- Simplify some docs around airflow_local_settings (#37835)\n- Add section about local settings configuration (#37829)\n- Fix docs of ``BranchDayOfWeekOperator`` (#37813)\n- Write to secrets store is not supported by design (#37814)\n- ``ERD`` generating doc improvement (#37808)\n- Update incorrect config value (#37706)\n- Update security model to clarify Connection Editing user's capabilities (#37688)\n- Fix ImportError on examples dags (#37571)\n\n\nAirflow 2.8.2 (2024-02-26)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nThe ``allowed_deserialization_classes`` flag now follows a glob pattern (#36147).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nFor example if one wants to add the class ``airflow.tests.custom_class`` to the\n``allowed_deserialization_classes`` list, it can be done by writing the full class\nname (``airflow.tests.custom_class``) or a pattern such as the ones used in glob\nsearch (e.g., ``airflow.*``, ``airflow.tests.*``).\n\nIf you currently use a custom regexp path make sure to rewrite it as a glob pattern.\n\nAlternatively, if you still wish to match it as a regexp pattern, add it under the new\nlist ``allowed_deserialization_classes_regexp`` instead.\n\nThe audit_logs permissions have been updated for heightened security (#37501).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThis was done under the policy that we do not want users like Viewer, Ops,\nand other users apart from Admin to have access to audit_logs. The intention behind\nthis change is to restrict users with less permissions from viewing user details\nlike First Name, Email etc. from the audit_logs when they are not permitted to.\n\nThe impact of this change is that the existing users with non admin rights won't be able\nto view or access the audit_logs, both from the Browse tab or from the DAG run.\n\n``AirflowTimeoutError`` is no longer ``except`` by default through ``Exception`` (#35653).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe ``AirflowTimeoutError`` is now inheriting ``BaseException`` instead of\n``AirflowException``->``Exception``.\nSee https://docs.python.org/3/library/exceptions.html#exception-hierarchy\n\nThis prevents code catching ``Exception`` from accidentally\ncatching ``AirflowTimeoutError`` and continuing to run.\n``AirflowTimeoutError`` is an explicit intent to cancel the task, and should not\nbe caught in attempts to handle the error and return some default value.\n\nCatching ``AirflowTimeoutError`` is still possible by explicitly ``except``ing\n``AirflowTimeoutError`` or ``BaseException``.\nThis is discouraged, as it may allow the code to continue running even after\nsuch cancellation requests.\nCode that previously depended on performing strict cleanup in every situation\nafter catching ``Exception`` is advised to use ``finally`` blocks or\ncontext managers. To perform only the cleanup and then automatically\nre-raise the exception.\nSee similar considerations about catching ``KeyboardInterrupt`` in\nhttps://docs.python.org/3/library/exceptions.html#KeyboardInterrupt\n\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Sort dag processing stats by last_runtime (#37302)\n- Allow pre-population of trigger form values via URL parameters (#37497)\n- Base date for fetching dag grid view must include selected run_id (#34887)\n- Check permissions for ImportError (#37468)\n- Move ``IMPORT_ERROR`` from DAG related permissions to view related permissions (#37292)\n- Change ``AirflowTaskTimeout`` to inherit ``BaseException`` (#35653)\n- Revert \"Fix future DagRun rarely triggered by race conditions when max_active_runs reached its upper limit. (#31414)\" (#37596)\n- Change margin to padding so first task can be selected (#37527)\n- Fix Airflow serialization for ``namedtuple`` (#37168)\n- Fix bug with clicking url-unsafe tags (#37395)\n- Set deterministic and new getter for ``Treeview`` function (#37162)\n- Fix permissions of parent folders for log file handler (#37310)\n- Fix permission check on DAGs when ``access_entity`` is specified (#37290)\n- Fix the value of ``dateTimeAttrFormat`` constant (#37285)\n- Resolve handler close race condition at triggerer shutdown (#37206)\n- Fixing status icon alignment for various views (#36804)\n- Remove superfluous ``@Sentry.enrich_errors``  (#37002)\n- Use execution_date= param as a backup to base date for grid view (#37018)\n- Handle SystemExit raised in the task. (#36986)\n- Revoking audit_log permission from all users except admin (#37501)\n- Fix broken regex for allowed_deserialization_classes (#36147)\n- Fix the bug that affected the DAG end date. (#36144)\n- Adjust node width based on task name length (#37254)\n- fix: PythonVirtualenvOperator crashes if any python_callable function is defined in the same source as DAG (#37165)\n- Fix collapsed grid width, line up selected bar with gantt (#37205)\n- Adjust graph node layout (#37207)\n- Revert the sequence of initializing configuration defaults (#37155)\n- Displaying \"actual\" try number in TaskInstance view (#34635)\n- Bugfix Triggering DAG with parameters is mandatory when show_trigger_form_if_no_params is enabled (#37063)\n- Secret masker ignores passwords with special chars (#36692)\n- Fix DagRuns with UPSTREAM_FAILED tasks get stuck in the backfill. (#36954)\n- Disable ``dryrun`` auto-fetch (#36941)\n- Fix copy button on a DAG run's config (#36855)\n- Fix bug introduced by replacing spaces by + in run_id (#36877)\n- Fix webserver always redirecting to home page if user was not logged in (#36833)\n- REST API set description on POST to ``/variables`` endpoint (#36820)\n- Sanitize the conn_id to disallow potential script execution (#32867)\n- Fix task id copy button copying wrong id (#34904)\n- Fix security manager inheritance in fab provider (#36538)\n- Avoid ``pendulum.from_timestamp`` usage (#37160)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Install latest docker ``CLI`` instead of specific one (#37651)\n- Bump ``undici`` from ``5.26.3`` to ``5.28.3`` in ``/airflow/www`` (#37493)\n- Add Python ``3.12`` exclusions in ``providers/pyproject.toml`` (#37404)\n- Remove ``markdown`` from core dependencies (#37396)\n- Remove unused ``pageSize`` method. (#37319)\n- Add more-itertools as dependency of common-sql (#37359)\n- Replace other ``Python 3.11`` and ``3.12`` deprecations (#37478)\n- Include ``airflow_pre_installed_providers.txt`` into ``sdist`` distribution (#37388)\n- Turn Pydantic into an optional dependency (#37320)\n- Limit ``universal-pathlib to < 0.2.0`` (#37311)\n- Allow running airflow against sqlite in-memory DB for tests (#37144)\n- Add description to ``queue_when`` (#36997)\n- Updated ``config.yml`` for environment variable ``sql_alchemy_connect_args``  (#36526)\n- Bump min version of ``Alembic to 1.13.1`` (#36928)\n- Limit ``flask-session`` to ``<0.6`` (#36895)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Fix upgrade docs to reflect true ``CLI`` flags available (#37231)\n- Fix a bug in fundamentals doc (#37440)\n- Add redirect for deprecated page (#37384)\n- Fix the ``otel`` config descriptions (#37229)\n- Update ``Objectstore`` tutorial with ``prereqs`` section (#36983)\n- Add more precise description on avoiding generic ``package/module`` names (#36927)\n- Add airflow version substitution into Docker Compose Howto (#37177)\n- Add clarification about DAG author capabilities to security model (#37141)\n- Move docs for cron basics to Authoring and Scheduling section (#37049)\n- Link to release notes in the upgrade docs (#36923)\n- Prevent templated field logic checks in ``__init__`` of operators automatically (#33786)\n\n\nAirflow 2.8.1 (2024-01-19)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nTarget version for core dependency ``pendulum`` package set to 3 (#36281).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nSupport for pendulum 2.1.2 will be saved for a while, presumably until the next feature version of Airflow.\nIt is advised to upgrade user code to use pendulum 3 as soon as possible.\n\nPendulum 3 introduced some subtle incompatibilities that you might rely on in your code - for example\ndefault rendering of dates is missing ``T`` in the rendered date representation, which is not ISO8601\ncompliant. If you rely on the default rendering of dates, you might need to adjust your code to use\n``isoformat()`` method to render dates in ISO8601 format.\n\nAirflow packaging specification follows modern Python packaging standards (#36537).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nWe standardized Airflow dependency configuration to follow latest development in Python packaging by\nusing ``pyproject.toml``. Airflow is now compliant with those accepted PEPs:\n\n* `PEP-440 Version Identification and Dependency Specification <https://www.python.org/dev/peps/pep-0440/>`__\n* `PEP-517 A build-system independent format for source trees <https://www.python.org/dev/peps/pep-0517/>`__\n* `PEP-518 Specifying Minimum Build System Requirements for Python Projects <https://www.python.org/dev/peps/pep-0518/>`__\n* `PEP-561 Distributing and Packaging Type Information <https://www.python.org/dev/peps/pep-0561/>`__\n* `PEP-621 Storing project metadata in pyproject.toml <https://www.python.org/dev/peps/pep-0621/>`__\n* `PEP-660 Editable installs for pyproject.toml based builds (wheel based) <https://www.python.org/dev/peps/pep-0660/>`__\n* `PEP-685 Comparison of extra names for optional distribution dependencies <https://www.python.org/dev/peps/pep-0685/>`__\n\nAlso we implement multiple license files support coming from Draft, not yet accepted (but supported by hatchling) PEP:\n* `PEP 639 Improving License Clarity with Better Package Metadata <https://peps.python.org/pep-0639/>`__\n\nThis has almost no noticeable impact on users if they are using modern Python packaging and development tools, generally\nspeaking Airflow should behave as it did before when installing it from PyPI and it should be much easier to install\nit for development purposes using ``pip install -e \".[devel]\"``.\n\nThe differences from the user side are:\n\n* Airflow extras now get extras normalized to ``-`` (following PEP-685) instead of ``_`` and ``.``\n  (as it was before in some extras). When you install airflow with such extras (for example ``dbt.core`` or\n  ``all_dbs``) you should use ``-`` instead of ``_`` and ``.``.\n\nIn most modern tools this will work in backwards-compatible way, but in some old version of those tools you might need to\nreplace ``_`` and ``.`` with ``-``. You can also get warnings that the extra you are installing does not exist - but usually\nthis warning is harmless and the extra is installed anyway. It is, however, recommended to change to use ``-`` in extras in your dependency\nspecifications for all Airflow extras.\n\n* Released airflow package does not contain ``devel``, ``devel-*``, ``doc`` and ``doc-gen`` extras.\n  Those extras are only available when you install Airflow from sources in ``--editable`` mode. This is\n  because those extras are only used for development and documentation building purposes and are not needed\n  when you install Airflow for production use. Those dependencies had unspecified and varying behaviour for\n  released packages anyway and you were not supposed to use them in released packages.\n\n* The ``all`` and ``all-*`` extras were not always working correctly when installing Airflow using constraints\n  because they were also considered as development-only dependencies. With this change, those dependencies are\n  now properly handling constraints and they will install properly with constraints, pulling the right set\n  of providers and dependencies when constraints are used.\n\nGraphviz dependency is now an optional one, not required one (#36647).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThe ``graphviz`` dependency has been problematic as Airflow required dependency - especially for\nARM-based installations. Graphviz packages require binary graphviz libraries - which is already a\nlimitation, but they also require to install graphviz Python bindings to be build and installed.\nThis does not work for older Linux installation but - more importantly - when you try to install\nGraphviz libraries for Python 3.8, 3.9 for ARM M1 MacBooks, the packages fail to install because\nPython bindings compilation for M1 can only work for Python 3.10+.\n\nThis is not a breaking change technically - the CLIs to render the DAGs is still there and IF you\nalready have graphviz installed, it will continue working as it did before. The only problem when it\ndoes not work is where you do not have graphviz installed it will raise an error and inform that you need it.\n\nGraphviz will remain to be installed for most users:\n\n* the Airflow Image will still contain graphviz library, because\n  it is added there as extra\n* when previous version of Airflow has been installed already, then\n  graphviz library is already installed there and Airflow will\n  continue working as it did\n\nThe only change will be a new installation of new version of Airflow from the scratch, where graphviz will\nneed to be specified as extra or installed separately in order to enable DAG rendering option.\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Fix airflow-scheduler exiting with code 0 on exceptions (#36800)\n- Fix Callback exception when a removed task is the last one in the ``taskinstance`` list (#36693)\n- Allow anonymous user edit/show resource when set ``AUTH_ROLE_PUBLIC=admin`` (#36750)\n- Better error message when sqlite URL uses relative path (#36774)\n- Explicit string cast required to force integer-type run_ids to be passed as strings instead of integers (#36756)\n- Add log lookup exception for empty ``op`` subtypes (#35536)\n- Remove unused index on task instance (#36737)\n- Fix check on subclass for ``typing.Union`` in ``_infer_multiple_outputs`` for Python 3.10+ (#36728)\n- Make sure ``multiple_outputs`` is inferred correctly even when using ``TypedDict`` (#36652)\n- Add back FAB constant in legacy security manager (#36719)\n- Fix AttributeError when using ``Dagrun.update_state`` (#36712)\n- Do not let ``EventsTimetable`` schedule past events if ``catchup=False`` (#36134)\n- Support encryption for triggers parameters (#36492)\n- Fix the type hint for ``tis_query`` in ``_process_executor_events`` (#36655)\n- Redirect to index when user does not have permission to access a page (#36623)\n- Avoid using dict as default value in ``call_regular_interval`` (#36608)\n- Remove option to set a task instance to running state in UI (#36518)\n- Fix details tab not showing when using dynamic task mapping (#36522)\n- Raise error when ``DagRun`` fails while running ``dag test`` (#36517)\n- Refactor ``_manage_executor_state`` by refreshing TIs in batch (#36502)\n- Add flask config: ``MAX_CONTENT_LENGTH`` (#36401)\n- Fix get_leaves calculation for teardown in nested group (#36456)\n- Stop serializing timezone-naive datetime to timezone-aware datetime with UTC tz (#36379)\n- Make ``kubernetes`` decorator type annotation consistent with operator (#36405)\n- Fix Webserver returning 500 for POST requests to ``api/dag/*/dagrun`` from anonymous user (#36275)\n- Fix the required access for get_variable endpoint (#36396)\n- Fix datetime reference in ``DAG.is_fixed_time_schedule`` (#36370)\n- Fix AirflowSkipException message raised by BashOperator (#36354)\n- Allow PythonVirtualenvOperator.skip_on_exit_code to be zero (#36361)\n- Increase width of execution_date input in trigger.html (#36278)\n- Fix logging for pausing DAG (#36182)\n- Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n- Check DAG read permission before accessing DAG code (#36257)\n- Enable mark task as failed/success always (#36254)\n- Create latest log dir symlink as relative link (#36019)\n- Fix Python-based decorators templating (#36103)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Rename concurrency label to max active tasks (#36691)\n- Restore function scoped ``httpx`` import in file_task_handler for performance (#36753)\n- Add support of Pendulum 3 (#36281)\n- Standardize airflow build process and switch to Hatchling build backend (#36537)\n- Get rid of ``pyarrow-hotfix`` for ``CVE-2023-47248`` (#36697)\n- Make ``graphviz`` dependency optional (#36647)\n- Announce MSSQL support end in Airflow 2.9.0, add migration script hints (#36509)\n- Set min ``pandas`` dependency to 1.2.5 for all providers and airflow (#36698)\n- Bump follow-redirects from 1.15.3 to 1.15.4 in ``/airflow/www`` (#36700)\n- Provide the logger_name param to base hook in order to override the logger name (#36674)\n- Fix run type icon alignment with run type text (#36616)\n- Follow BaseHook connection fields method signature in FSHook (#36444)\n- Remove redundant ``docker`` decorator type annotations (#36406)\n- Straighten typing in workday timetable (#36296)\n- Use ``batch_is_authorized_dag`` to check if user has permission to read DAGs (#36279)\n- Replace deprecated get_accessible_dag_ids and use get_readable_dags in get_dag_warnings (#36256)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Metrics tagging documentation (#36627)\n- In docs use logical_date instead of deprecated execution_date (#36654)\n- Add section about live-upgrading Airflow (#36637)\n- Replace ``numpy`` example with practical exercise demonstrating top-level code (#35097)\n- Improve and add more complete description in the architecture diagrams (#36513)\n- Improve the error message displayed when there is a webserver error (#36570)\n- Update ``dags.rst`` with information on DAG pausing (#36540)\n- Update installation prerequisites after upgrading to Debian Bookworm (#36521)\n- Add description on the ways how users should approach DB monitoring (#36483)\n- Add branching based on mapped task group example to dynamic-task-mapping.rst (#36480)\n- Add further details to replacement documentation (#36485)\n- Use cards when describing priority weighting methods (#36411)\n- Update ``metrics.rst`` for param ``dagrun.schedule_delay`` (#36404)\n- Update admonitions in Python operator doc to reflect sentiment (#36340)\n- Improve audit_logs.rst (#36213)\n- Remove Redshift mention from the list of managed Postgres backends (#36217)\n\nAirflow 2.8.0 (2023-12-18)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nRaw HTML code in DAG docs and DAG params descriptions is disabled by default (#35460)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nTo ensure that no malicious javascript can be injected with DAG descriptions or trigger UI forms by DAG authors\na new parameter ``webserver.allow_raw_html_descriptions`` was added with default value of ``False``.\nIf you trust your DAG authors code and want to allow using raw HTML in DAG descriptions and params, you can restore the previous\nbehavior by setting the configuration value to ``True``.\n\nTo ensure Airflow is secure by default, the raw HTML support in trigger UI has been super-seeded by markdown support via\nthe ``description_md`` attribute. If you have been using ``description_html`` please migrate to ``description_md``.\nThe ``custom_html_form`` is now deprecated.\n\nNew Features\n\"\"\"\"\"\"\"\"\"\"\"\"\n- AIP-58: Add Airflow ObjectStore (AFS) (`AIP-58 <https://github.com/apache/airflow/pulls?q=is%3Apr+is%3Amerged+label%3AAIP-58+milestone%3A%22Airflow+2.8.0%22>`_)\n- Add XCom tab to Grid (#35719)\n- Add \"literal\" wrapper to disable field templating (#35017)\n- Add task context logging feature to allow forwarding messages to task logs (#32646, #32693, #35857)\n- Add Listener hooks for Datasets (#34418, #36247)\n- Allow override of navbar text color (#35505)\n- Add lightweight serialization for deltalake tables (#35462)\n- Add support for serialization of iceberg tables (#35456)\n- ``prev_end_date_success`` method access (#34528)\n- Add task parameter to set custom logger name (#34964)\n- Add pyspark decorator (#35247)\n- Add trigger as a valid option for the db clean command (#34908)\n- Add decorators for external and venv python branching operators (#35043)\n- Allow PythonVenvOperator using other index url (#33017)\n- Add Python Virtualenv Operator Caching (#33355)\n- Introduce a generic export for containerized executor logging (#34903)\n- Add ability to clear downstream tis in ``List Task Instances`` view  (#34529)\n- Attribute ``clear_number`` to track DAG run being cleared (#34126)\n- Add BranchPythonVirtualenvOperator (#33356)\n- Allow PythonVenvOperator using other index url (#33017)\n- Add CLI notification commands to providers (#33116)\n- Use dropdown instead of buttons when there are more than 10 retries in log tab (#36025)\n\nImprovements\n\"\"\"\"\"\"\"\"\"\"\"\"\n- Add ``multiselect`` to run state in grid view (#35403)\n- Fix warning message in ``Connection.get_hook`` in case of ImportError (#36005)\n- Add processor_subdir to import_error table to handle multiple dag processors (#35956)\n- Consolidate the call of change_state to fail or success in the core executors (#35901)\n- Relax mandatory requirement for start_date when schedule=None (#35356)\n- Use ExitStack to manage mutation of secrets_backend_list in dag.test (#34620)\n- improved visibility of tasks in ActionModal for ``taskinstance`` (#35810)\n- Create directories based on ``AIRFLOW_CONFIG`` path (#35818)\n- Implements ``JSON-string`` connection representation generator (#35723)\n- Move ``BaseOperatorLink`` into the separate module (#35032)\n- Set mark_end_on_close after set_context (#35761)\n- Move external logs links to top of react logs page (#35668)\n- Change terminal mode to ``cbreak`` in ``execute_interactive`` and handle ``SIGINT`` (#35602)\n- Make raw HTML descriptions configurable (#35460)\n- Allow email field to be templated (#35546)\n- Hide logical date and run id in trigger UI form (#35284)\n- Improved instructions for adding dependencies in TaskFlow (#35406)\n- Add optional exit code to list import errors (#35378)\n- Limit query result on DB rather than client in ``synchronize_log_template`` function (#35366)\n- Allow description to be passed in when using variables CLI (#34791)\n- Allow optional defaults in required fields with manual triggered dags (#31301)\n- Permitting airflow kerberos to run in different modes (#35146)\n- Refactor commands to unify daemon context handling (#34945)\n- Add extra fields to plugins endpoint (#34913)\n- Add description to pools view (#34862)\n- Move cli's Connection export and Variable export command print logic to a separate function (#34647)\n- Extract and reuse get_kerberos_principle func from get_kerberos_principle (#34936)\n- Change type annotation for ``BaseOperatorLink.operators`` (#35003)\n- Optimise and migrate to ``SA2-compatible`` syntax for TaskReschedule (#33720)\n- Consolidate the permissions name in SlaMissModelView (#34949)\n- Add debug log saying what's being run to ``EventScheduler`` (#34808)\n- Increase log reader stream loop sleep duration to 1 second (#34789)\n- Resolve pydantic deprecation warnings re ``update_forward_refs`` (#34657)\n- Unify mapped task group lookup logic (#34637)\n- Allow filtering event logs by attributes (#34417)\n- Make connection login and password TEXT (#32815)\n- Ban import ``Dataset`` from ``airflow`` package in codebase (#34610)\n- Use ``airflow.datasets.Dataset`` in examples and tests (#34605)\n- Enhance task status visibility (#34486)\n- Simplify DAG trigger UI (#34567)\n- Ban import AirflowException from airflow (#34512)\n- Add descriptions for airflow resource config parameters (#34438)\n- Simplify trigger name expression (#34356)\n- Move definition of Pod*Exceptions to pod_generator (#34346)\n- Add deferred tasks to the cluster_activity view Pools Slots (#34275)\n- heartbeat failure log message fix (#34160)\n- Rename variables for dag runs (#34049)\n- Clarify new_state in OpenAPI spec (#34056)\n- Remove ``version`` top-level element from docker compose files (#33831)\n- Remove generic trigger cancelled error log (#33874)\n- Use ``NOT EXISTS`` subquery instead of ``tuple_not_in_condition`` (#33527)\n- Allow context key args to not provide a default (#33430)\n- Order triggers by - TI priority_weight when assign unassigned triggers (#32318)\n- Add metric ``triggerer_heartbeat`` (#33320)\n- Allow ``airflow variables export`` to print to stdout (#33279)\n- Workaround failing deadlock when running backfill (#32991)\n- add dag_run_ids and task_ids filter for the batch task instance API endpoint (#32705)\n- Configurable health check threshold for triggerer (#33089)\n- Rework provider manager to treat Airflow core hooks like other provider hooks (#33051)\n- Ensure DAG-level references are filled on unmap (#33083)\n- Affix webserver access_denied warning to be configurable (#33022)\n- Add support for arrays of different data types in the Trigger Form UI (#32734)\n- Add a mechanism to warn if executors override existing CLI commands (#33423)\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Account for change in UTC offset when calculating next schedule (#35887)\n- Add read access to pools for viewer role (#35352)\n- Fix gantt chart queued duration when queued_dttm is greater than start_date for deferred tasks (#35984)\n- Avoid crushing container when directory is not found on rm (#36050)\n- Update ``reset_user_sessions`` to work from either CLI or web (#36056)\n- Fix UI Grid error when DAG has been removed. (#36028)\n- Change Trigger UI to use HTTP POST in web ui (#36026)\n- Fix airflow db shell needing an extra key press to exit (#35982)\n- Change dag grid ``overscroll`` behaviour to auto (#35717)\n- Run triggers inline with dag test (#34642)\n- Add ``borderWidthRight`` to grid for Firefox ``scrollbar`` (#35346)\n- Fix for infinite recursion due to secrets_masker (#35048)\n- Fix write ``processor_subdir`` in serialized_dag table (#35661)\n- Reload configuration for standalone dag file processor (#35725)\n- Long custom operator name overflows in graph view (#35382)\n- Add try_number to extra links query (#35317)\n- Prevent assignment of non JSON serializable values to DagRun.conf dict (#35096)\n- Numeric values in DAG details are incorrectly rendered as timestamps (#35538)\n- Fix Scheduler and triggerer crashes in daemon mode when statsd metrics are enabled (#35181)\n- Infinite UI redirection loop after deactivating an active user (#35486)\n- Bug fix fetch_callback of Partial Subset DAG (#35256)\n- Fix DagRun data interval for DeltaDataIntervalTimetable (#35391)\n- Fix query in ``get_dag_by_pickle`` util function (#35339)\n- Fix TriggerDagRunOperator failing to trigger subsequent runs when reset_dag_run=True (#35429)\n- Fix weight_rule property type in ``mappedoperator`` (#35257)\n- Bugfix/prevent concurrency with cached venv (#35258)\n- Fix dag serialization (#34042)\n- Fix py/url-redirection by replacing request.referrer by get_redirect() (#34237)\n- Fix updating variables during variable imports (#33932)\n- Use Literal from airflow.typing_compat in Airflow core (#33821)\n- Always use ``Literal`` from ``typing_extensions`` (#33794)\n\nMiscellaneous\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Change default MySQL client to MariaDB (#36243)\n- Mark daskexecutor provider as removed (#35965)\n- Bump FAB to ``4.3.10`` (#35991)\n- Mark daskexecutor provider as removed (#35965)\n- Rename ``Connection.to_json_dict`` to ``Connection.to_dict`` (#35894)\n- Upgrade to Pydantic v2 (#35551)\n- Bump ``moto`` version to ``>= 4.2.9`` (#35687)\n- Use ``pyarrow-hotfix`` to mitigate CVE-2023-47248 (#35650)\n- Bump ``axios`` from ``0.26.0 to 1.6.0`` in ``/airflow/www/`` (#35624)\n- Make docker decorator's type annotation consistent with operator (#35568)\n- Add default to ``navbar_text_color`` and ``rm`` condition in style (#35553)\n- Avoid initiating session twice in ``dag_next_execution`` (#35539)\n- Work around typing issue in examples and providers (#35494)\n- Enable ``TCH004`` and ``TCH005`` rules (#35475)\n- Humanize log output about retrieved DAG(s) (#35338)\n- Switch from Black to Ruff formatter (#35287)\n- Upgrade to Flask Application Builder 4.3.9 (#35085)\n- D401 Support (#34932, #34933)\n- Use requires_access to check read permission on dag instead of checking it explicitly (#34940)\n- Deprecate lazy import ``AirflowException`` from airflow (#34541)\n- View util refactoring on mapped stuff use cases (#34638)\n- Bump ``postcss`` from ``8.4.25 to 8.4.31`` in ``/airflow/www`` (#34770)\n- Refactor Sqlalchemy queries to 2.0 style (#34763, #34665, #32883, #35120)\n- Change to lazy loading of io in pandas serializer (#34684)\n- Use ``airflow.models.dag.DAG`` in examples (#34617)\n- Use airflow.exceptions.AirflowException in core (#34510)\n- Check that dag_ids passed in request are consistent (#34366)\n- Refactors to make code better (#34278, #34113, #34110, #33838, #34260, #34409, #34377, #34350)\n- Suspend qubole provider (#33889)\n- Generate Python API docs for Google ADS (#33814)\n- Improve importing in modules (#33812, #33811, #33810, #33806, #33807, #33805, #33804, #33803,\n  #33801, #33799, #33800, #33797, #33798, #34406, #33808)\n- Upgrade Elasticsearch to 8 (#33135)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Add support for tabs (and other UX components) to docs (#36041)\n- Replace architecture diagram of Airflow with diagrams-generated one (#36035)\n- Add the section describing the security model of DAG Author capabilities (#36022)\n- Enhance docs for zombie tasks (#35825)\n- Reflect drop/add support of DB Backends versions in documentation (#35785)\n- More detail on mandatory task arguments (#35740)\n- Indicate usage of the ``re2`` regex engine in the .airflowignore documentation. (#35663)\n- Update ``best-practices.rst`` (#35692)\n- Update ``dag-run.rst`` to mention Airflow's support for extended cron syntax through croniter (#35342)\n- Update ``webserver.rst`` to include information of supported OAuth2 providers (#35237)\n- Add back dag_run to docs (#35142)\n- Fix ``rst`` code block format (#34708)\n- Add typing to concrete taskflow examples (#33417)\n- Add concrete examples for accessing context variables from TaskFlow tasks (#33296)\n- Fix links in security docs (#33329)\n\n\nAirflow 2.7.3 (2023-11-06)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Fix pre-mature evaluation of tasks in mapped task group (#34337)\n- Add TriggerRule missing value in rest API (#35194)\n- Fix Scheduler crash looping when dagrun creation fails (#35135)\n- Fix test connection with ``codemirror`` and extra (#35122)\n- Fix usage of cron-descriptor since BC in v1.3.0 (#34836)\n- Fix ``get_plugin_info`` for class based listeners. (#35022)\n- Some improvements/fixes for dag_run and task_instance endpoints (#34942)\n- Fix the dags count filter in webserver home page (#34944)\n- Return only the TIs of the readable dags when ~ is provided as a dag_id (#34939)\n- Fix triggerer thread crash in daemon mode (#34931)\n- Fix wrong plugin schema (#34858)\n- Use DAG timezone in TimeSensorAsync (#33406)\n- Mark tasks with ``all_skipped`` trigger rule as ``skipped`` if any task is in ``upstream_failed`` state (#34392)\n- Add read only validation to read only fields (#33413)\n\nMisc/Internal\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Improve testing harness to separate DB and non-DB tests (#35160, #35333)\n- Add pytest db_test markers to our tests (#35264)\n- Add pip caching for faster build (#35026)\n- Upper bound ``pendulum`` requirement to ``<3.0`` (#35336)\n- Limit ``sentry_sdk`` to ``1.33.0`` (#35298)\n- Fix subtle bug in mocking processor_agent in our tests (#35221)\n- Bump ``@babel/traverse`` from ``7.16.0 to 7.23.2`` in ``/airflow/www`` (#34988)\n- Bump ``undici`` from ``5.19.1 to 5.26.3`` in ``/airflow/www`` (#34971)\n- Remove unused set from ``SchedulerJobRunner`` (#34810)\n- Remove warning about ``max_tis per query > parallelism`` (#34742)\n- Improve modules import in Airflow core by moving some of them into a type-checking block (#33755)\n- Fix tests to respond to Python 3.12 handling of utcnow in sentry-sdk (#34946)\n- Add ``connexion<3.0`` upper bound (#35218)\n- Limit Airflow to ``< 3.12`` (#35123)\n- update moto version (#34938)\n- Limit WTForms to below ``3.1.0`` (#34943)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Fix variables substitution in Airflow Documentation (#34462)\n- Added example for defaults in ``conn.extras`` (#35165)\n- Update datasets.rst issue with running example code (#35035)\n- Remove ``mysql-connector-python`` from recommended MySQL driver (#34287)\n- Fix syntax error in task dependency ``set_downstream`` example (#35075)\n- Update documentation to enable test connection (#34905)\n- Update docs errors.rst - Mention sentry \"transport\" configuration option (#34912)\n- Update dags.rst to put SubDag deprecation note right after the SubDag section heading (#34925)\n- Add info on getting variables and config in custom secrets backend (#34834)\n- Document BaseExecutor interface in more detail to help users in writing custom executors (#34324)\n- Fix broken link to ``airflow_local_settings.py`` template (#34826)\n- Fixes python_callable function assignment context kwargs example in params.rst (#34759)\n- Add missing multiple_outputs=True param in the TaskFlow example (#34812)\n- Remove extraneous ``'>'`` in provider section name (#34813)\n- Fix imports in extra link documentation (#34547)\n\n\n\nAirflow 2.7.2 (2023-10-12)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes\n\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Check if the lower of provided values are sensitives in config endpoint (#34712)\n- Add support for ZoneInfo and generic UTC to fix datetime serialization (#34683, #34804)\n- Fix AttributeError: 'Select' object has no attribute 'count' during the airflow db migrate command (#34348)\n- Make dry run optional for patch task instance  (#34568)\n- Fix non deterministic datetime deserialization (#34492)\n- Use iterative loop to look for mapped parent (#34622)\n- Fix is_parent_mapped value by checking if any of the parent ``taskgroup`` is mapped (#34587)\n- Avoid top-level airflow import to avoid circular dependency (#34586)\n- Add more exemptions to lengthy metric list (#34531)\n- Fix dag warning endpoint permissions (#34355)\n- Fix task instance access issue in the batch endpoint (#34315)\n- Correcting wrong time showing in grid view (#34179)\n- Fix www ``cluster_activity`` view not loading due to ``standaloneDagProcessor`` templating (#34274)\n- Set ``loglevel=DEBUG`` in 'Not syncing ``DAG-level`` permissions' (#34268)\n- Make param validation consistent for DAG validation and triggering (#34248)\n- Ensure details panel is shown when any tab is selected (#34136)\n- Fix issues related to ``access_control={}`` (#34114)\n- Fix not found ``ab_user`` table in the CLI session (#34120)\n- Fix FAB-related logging format interpolation (#34139)\n- Fix query bug in ``next_run_datasets_summary`` endpoint (#34143)\n- Fix for TaskGroup toggles for duplicated labels (#34072)\n- Fix the required permissions to clear a TI from the UI (#34123)\n- Reuse ``_run_task_session`` in mapped ``render_template_fields`` (#33309)\n- Fix scheduler logic to plan new dag runs by ignoring manual runs (#34027)\n- Add missing audit logs for Flask actions add, edit and delete (#34090)\n- Hide Irrelevant Dag Processor from Cluster Activity Page (#33611)\n- Remove infinite animation for pinwheel, spin for 1.5s (#34020)\n- Restore rendering of provider configuration with ``version_added`` (#34011)\n\nDoc Only Changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Clarify audit log permissions (#34815)\n- Add explanation for Audit log users (#34814)\n- Import ``AUTH_REMOTE_USER`` from FAB in WSGI middleware example (#34721)\n- Add information about drop support MsSQL as DB Backend in the future (#34375)\n- Document how to use the system's timezone database (#34667)\n- Clarify what landing time means in doc (#34608)\n- Fix screenshot in dynamic task mapping docs (#34566)\n- Fix class reference in Public Interface documentation (#34454)\n- Clarify var.value.get  and var.json.get usage (#34411)\n- Schedule default value description (#34291)\n- Docs for triggered_dataset_event (#34410)\n- Add DagRun events (#34328)\n- Provide tabular overview about trigger form param types (#34285)\n- Add link to Amazon Provider Configuration in Core documentation (#34305)\n- Add \"security infrastructure\" paragraph to security model (#34301)\n- Change links to SQLAlchemy 1.4 (#34288)\n- Add SBOM entry in security documentation (#34261)\n- Added more example code for XCom push and pull (#34016)\n- Add state utils to Public Airflow Interface (#34059)\n- Replace markdown style link with rst style link (#33990)\n- Fix broken link to the \"UPDATING.md\" file (#33583)\n\nMisc/Internal\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Update min-sqlalchemy version to account for latest features used (#34293)\n- Fix SesssionExemptMixin spelling (#34696)\n- Restrict ``astroid`` version < 3 (#34658)\n- Fail dag test if defer without triggerer (#34619)\n- Fix connections exported output (#34640)\n- Don't run isort when creating new alembic migrations (#34636)\n- Deprecate numeric type python version in PythonVirtualEnvOperator (#34359)\n- Refactor ``os.path.splitext`` to ``Path.*`` (#34352, #33669)\n- Replace = by is for type comparison (#33983)\n- Refactor integer division (#34180)\n- Refactor: Simplify comparisons (#34181)\n- Refactor: Simplify string generation (#34118)\n- Replace unnecessary dict comprehension with dict() in core (#33858)\n- Change \"not all\" to \"any\" for ease of readability (#34259)\n- Replace assert by if...raise in code (#34250, #34249)\n- Move default timezone to except block (#34245)\n- Combine similar if logic in core (#33988)\n- Refactor: Consolidate import and usage of random (#34108)\n- Consolidate importing of os.path.* (#34060)\n- Replace sequence concatenation by unpacking in Airflow core (#33934)\n- Refactor unneeded 'continue' jumps around the repo (#33849, #33845, #33846, #33848, #33839, #33844, #33836, #33842)\n- Remove [project] section from ``pyproject.toml`` (#34014)\n- Move the try outside the loop when this is possible in Airflow core (#33975)\n- Replace loop by any when looking for a positive value in core (#33985)\n- Do not create lists we don't need (#33519)\n- Remove useless string join from core (#33969)\n- Add TCH001 and TCH002 rules to pre-commit to detect and move type checking modules (#33865)\n- Add cancel_trigger_ids to to_cancel dequeue in batch (#33944)\n- Avoid creating unnecessary list when parsing stats datadog tags (#33943)\n- Replace dict.items by dict.values when key is not used in core (#33940)\n- Replace lambdas with comprehensions (#33745)\n- Improve modules import in Airflow core by some of them into a type-checking block (#33755)\n- Refactor: remove unused state - SHUTDOWN (#33746, #34063, #33893)\n- Refactor: Use in-place .sort() (#33743)\n- Use literal dict instead of calling dict() in Airflow core (#33762)\n- remove unnecessary map and rewrite it using list in Airflow core (#33764)\n- Replace lambda by a def method in Airflow core (#33758)\n- Replace type func by ``isinstance`` in fab_security manager (#33760)\n- Replace single quotes by double quotes in all Airflow modules (#33766)\n- Merge multiple ``isinstance`` calls for the same object in a single call (#33767)\n- Use a single  statement with multiple contexts instead of nested  statements in core (#33769)\n- Refactor: Use f-strings (#33734, #33455)\n- Refactor: Use random.choices (#33631)\n- Use ``str.splitlines()`` to split lines (#33592)\n- Refactor: Remove useless str() calls (#33629)\n- Refactor: Improve detection of duplicates and list sorting (#33675)\n- Simplify conditions on ``len()`` (#33454)\n\n\nAirflow 2.7.1 (2023-09-07)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nCronTriggerTimetable is now less aggressive when trying to skip a run (#33404)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWhen setting ``catchup=False``, CronTriggerTimetable no longer skips a run if\nthe scheduler does not query the timetable immediately after the previous run\nhas been triggered.\n\nThis should not affect scheduling in most cases, but can change the behaviour if\na DAG is paused-unpaused to manually skip a run. Previously, the timetable (with\n``catchup=False``) would only start a run after a DAG is unpaused, but with this\nchange, the scheduler would try to look at little bit back to schedule the\nprevious run that covers a part of the period when the DAG was paused. This\nmeans you will need to keep a DAG paused longer (namely, for the entire cron\nperiod to pass) to really skip a run.\n\nNote that this is also the behaviour exhibited by various other cron-based\nscheduling tools, such as ``anacron``.\n\n``conf.set()`` becomes case insensitive to match ``conf.get()`` behavior (#33452)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nAlso, ``conf.get()`` will now break if used with non-string parameters.\n\n``conf.set(section, key, value)`` used to be case sensitive, i.e. ``conf.set(\"SECTION\", \"KEY\", value)``\nand ``conf.set(\"section\", \"key\", value)`` were stored as two distinct configurations.\nThis was inconsistent with the behavior of ``conf.get(section, key)``, which was always converting the section and key to lower case.\n\nAs a result, configuration options set with upper case characters in the section or key were unreachable.\nThat's why we are now converting section and key to lower case in ``conf.set`` too.\n\nWe also changed a bit the behavior of ``conf.get()``. It used to allow objects that are not strings in the section or key.\nDoing this will now result in an exception. For instance, ``conf.get(\"section\", 123)`` needs to be replaced with ``conf.get(\"section\", \"123\")``.\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Ensure that tasks wait for running indirect setup (#33903)\n- Respect \"soft_fail\" for core async sensors (#33403)\n- Differentiate 0 and unset as a default param values (#33965)\n- Raise 404 from Variable PATCH API if variable is not found (#33885)\n- Fix ``MappedTaskGroup`` tasks not respecting upstream dependency (#33732)\n- Add limit 1 if required first value from query result (#33672)\n- Fix UI DAG counts including deleted DAGs (#33778)\n- Fix cleaning zombie RESTARTING tasks (#33706)\n- ``SECURITY_MANAGER_CLASS`` should be a reference to class, not a string (#33690)\n- Add back ``get_url_for_login`` in security manager (#33660)\n- Fix ``2.7.0 db`` migration job errors (#33652)\n- Set context inside templates (#33645)\n- Treat dag-defined access_control as authoritative if defined (#33632)\n- Bind engine before attempting to drop archive tables (#33622)\n- Add a fallback in case no first name and last name are set (#33617)\n- Sort data before ``groupby`` in TIS duration calculation (#33535)\n- Stop adding values to rendered templates UI when there is no dagrun (#33516)\n- Set strict to True when parsing dates in webserver views (#33512)\n- Use ``dialect.name`` in custom SA types (#33503)\n- Do not return ongoing dagrun when a ``end_date`` is less than ``utcnow`` (#33488)\n- Fix a bug in ``formatDuration`` method (#33486)\n- Make ``conf.set`` case insensitive (#33452)\n- Allow timetable to slightly miss catchup cutoff (#33404)\n- Respect ``soft_fail`` argument when ``poke`` is called (#33401)\n- Create a new method used to resume the task in order to implement specific logic for operators (#33424)\n- Fix DagFileProcessor interfering with dags outside its ``processor_subdir`` (#33357)\n- Remove the unnecessary ``<br>`` text in Provider's view (#33326)\n- Respect ``soft_fail`` argument when ExternalTaskSensor runs in deferrable mode (#33196)\n- Fix handling of default value and serialization of Param class (#33141)\n- Check if the dynamically-added index is in the table schema before adding (#32731)\n- Fix rendering the mapped parameters when using ``expand_kwargs`` method (#32272)\n- Fix dependencies for celery and opentelemetry for Python 3.8 (#33579)\n\nMisc/Internal\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Bring back ``Pydantic`` 1 compatibility (#34081, #33998)\n- Use a trimmed version of README.md for PyPI (#33637)\n- Upgrade to ``Pydantic`` 2 (#33956)\n- Reorganize ``devel_only`` extra in Airflow's setup.py (#33907)\n- Bumping ``FAB`` to ``4.3.4`` in order to fix issues with filters (#33931)\n- Add minimum requirement for ``sqlalchemy to 1.4.24`` (#33892)\n- Update version_added field for configs in config file (#33509)\n- Replace ``OrderedDict`` with plain dict (#33508)\n- Consolidate import and usage of itertools (#33479)\n- Static check fixes (#33462)\n- Import utc from datetime and normalize its import (#33450)\n- D401 Support (#33352, #33339, #33337, #33336, #33335, #33333, #33338)\n- Fix some missing type hints (#33334)\n- D205 Support - Stragglers (#33301, #33298, #33297)\n- Refactor: Simplify code (#33160, #33270, #33268, #33267, #33266, #33264, #33292, #33453, #33476, #33567,\n  #33568, #33480, #33753, #33520, #33623)\n- Fix ``Pydantic`` warning about ``orm_mode`` rename (#33220)\n- Add MySQL 8.1 to supported versions. (#33576)\n- Remove ``Pydantic`` limitation for version < 2 (#33507)\n\nDoc only changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Add documentation explaining template_ext (and how to override it) (#33735)\n- Explain how users can check if python code is top-level (#34006)\n- Clarify that DAG authors can also run code in DAG File Processor (#33920)\n- Fix broken link in Modules Management page (#33499)\n- Fix secrets backend docs (#33471)\n- Fix config description for base_log_folder (#33388)\n\n\nAirflow 2.7.0 (2023-08-18)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nRemove Python 3.7 support (#30963)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nAs of now, Python 3.7 is no longer supported by the Python community.\nTherefore, to use Airflow 2.7.0, you must ensure your Python version is\neither 3.8, 3.9, 3.10, or 3.11.\n\nOld Graph View is removed (#32958)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThe old Graph View is removed. The new Graph View is the default view now.\n\nThe trigger UI form is skipped in web UI if no parameters are defined in a DAG (#33351)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIf you are using ``dag_run.conf`` dictionary and web UI JSON entry to run your DAG you should either:\n\n* `Add params to your DAG <https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/params.html#use-params-to-provide-a-trigger-ui-form>`_\n* Enable the new configuration ``show_trigger_form_if_no_params`` to bring back old behaviour\n\nThe \"db init\", \"db upgrade\" commands and \"[database] load_default_connections\" configuration options are deprecated (#33136).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nInstead, you should use \"airflow db migrate\" command to create or upgrade database. This command will not create default connections.\nIn order to create default connections you need to run \"airflow connections create-default-connections\" explicitly,\nafter running \"airflow db migrate\".\n\nIn case of SMTP SSL connection, the context now uses the \"default\" context (#33070)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThe \"default\" context is Python's ``default_ssl_contest`` instead of previously used \"none\". The\n``default_ssl_context`` provides a balance between security and compatibility but in some cases,\nwhen certificates are old, self-signed or misconfigured, it might not work. This can be configured\nby setting \"ssl_context\" in \"email\" configuration of Airflow.\n\nSetting it to \"none\" brings back the \"none\" setting that was used in Airflow 2.6 and before,\nbut it is not recommended due to security reasons ad this setting disables validation of certificates and allows MITM attacks.\n\nDisable default allowing the testing of connections in UI, API and CLI(#32052)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nFor security reasons, the test connection functionality is disabled by default across Airflow UI,\nAPI and CLI. The availability of the functionality can be controlled by the\n``test_connection`` flag in the ``core`` section of the Airflow\nconfiguration (``airflow.cfg``). It can also be controlled by the\nenvironment variable ``AIRFLOW__CORE__TEST_CONNECTION``.\n\nThe following values are accepted for this config param:\n1. ``Disabled``: Disables the test connection functionality and\ndisables the Test Connection button in the UI.\n\nThis is also the default value set in the Airflow configuration.\n2. ``Enabled``: Enables the test connection functionality and\nactivates the Test Connection button in the UI.\n\n3. ``Hidden``: Disables the test connection functionality and\nhides the Test Connection button in UI.\n\nFor more information on capabilities of users, see the documentation:\nhttps://airflow.apache.org/docs/apache-airflow/stable/security/security_model.html#capabilities-of-authenticated-ui-users\nIt is strongly advised to **not** enable the feature until you make sure that only\nhighly trusted UI/API users have \"edit connection\" permissions.\n\nThe ``xcomEntries`` API disables support for the ``deserialize`` flag by default (#32176)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nFor security reasons, the ``/dags/*/dagRuns/*/taskInstances/*/xcomEntries/*``\nAPI endpoint now disables the ``deserialize`` option to deserialize arbitrary\nXCom values in the webserver. For backward compatibility, server admins may set\nthe ``[api] enable_xcom_deserialize_support`` config to *True* to enable the\nflag and restore backward compatibility.\n\nHowever, it is strongly advised to **not** enable the feature, and perform\ndeserialization at the client side instead.\n\nChange of the default Celery application name (#32526)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nDefault name of the Celery application changed from ``airflow.executors.celery_executor`` to ``airflow.providers.celery.executors.celery_executor``.\n\nYou should change both your configuration and Health check command to use the new name:\n  * in configuration (``celery_app_name`` configuration in ``celery`` section) use ``airflow.providers.celery.executors.celery_executor``\n  * in your Health check command use ``airflow.providers.celery.executors.celery_executor.app``\n\n\nThe default value for ``scheduler.max_tis_per_query`` is changed from 512 to 16 (#32572)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThis change is expected to make the Scheduler more responsive.\n\n``scheduler.max_tis_per_query`` needs to be lower than ``core.parallelism``.\nIf both were left to their default value previously, the effective default value of ``scheduler.max_tis_per_query`` was 32\n(because it was capped at ``core.parallelism``).\n\nTo keep the behavior as close as possible to the old config, one can set ``scheduler.max_tis_per_query = 0``,\nin which case it'll always use the value of ``core.parallelism``.\n\nSome executors have been moved to corresponding providers (#32767)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nIn order to use the executors, you need to install the providers:\n\n* for Celery executors you need to install ``apache-airflow-providers-celery`` package >= 3.3.0\n* for Kubernetes executors you need to install ``apache-airflow-providers-cncf-kubernetes`` package >= 7.4.0\n* For Dask executors you need to install ``apache-airflow-providers-daskexecutor`` package in any version\n\nYou can achieve it also by installing airflow with ``[celery]``, ``[cncf.kubernetes]``, ``[daskexecutor]`` extras respectively.\n\nUsers who base their images on the ``apache/airflow`` reference image (not slim) should be unaffected - the base\nreference image comes with all the three providers installed.\n\nImprovement Changes\n^^^^^^^^^^^^^^^^^^^\n\nPostgreSQL only improvement: Added index on taskinstance table (#30762)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThis index seems to have great positive effect in a setup with tens of millions such rows.\n\nNew Features\n\"\"\"\"\"\"\"\"\"\"\"\"\n- Add OpenTelemetry to Airflow (`AIP-49 <https://github.com/apache/airflow/pulls?q=is%3Apr+is%3Amerged+label%3AAIP-49+milestone%3A%22Airflow+2.7.0%22>`_)\n- Trigger Button - Implement Part 2 of AIP-50 (#31583)\n- Removing Executor Coupling from Core Airflow (`AIP-51 <https://github.com/apache/airflow/pulls?q=is%3Apr+is%3Amerged+label%3AAIP-51+milestone%3A%22Airflow+2.7.0%22>`_)\n- Automatic setup and teardown tasks (`AIP-52 <https://github.com/apache/airflow/pulls?q=is%3Apr+is%3Amerged+label%3AAIP-52+milestone%3A%22Airflow+2.7.0%22>`_)\n- OpenLineage in Airflow (`AIP-53 <https://github.com/apache/airflow/pulls?q=is%3Apr+is%3Amerged+milestone%3A%22Airflow+2.7.0%22+label%3Aprovider%3Aopenlineage>`_)\n- Experimental: Add a cache to Variable and Connection when called at dag parsing time (#30259)\n- Enable pools to consider deferred tasks (#32709)\n- Allows to choose SSL context for SMTP connection (#33070)\n- New gantt tab (#31806)\n- Load plugins from providers (#32692)\n- Add ``BranchExternalPythonOperator`` (#32787, #33360)\n- Add option for storing configuration description in providers (#32629)\n- Introduce Heartbeat Parameter to Allow ``Per-LocalTaskJob`` Configuration (#32313)\n- Add Executors discovery and documentation (#32532)\n- Add JobState for job state constants (#32549)\n- Add config to disable the 'deserialize' XCom API flag (#32176)\n- Show task instance in web UI by custom operator name (#31852)\n- Add default_deferrable config (#31712)\n- Introducing ``AirflowClusterPolicySkipDag`` exception (#32013)\n- Use ``reactflow`` for datasets graph (#31775)\n- Add an option to load the dags from db for command tasks run (#32038)\n- Add version of ``chain`` which doesn't require matched lists (#31927)\n- Use operator_name instead of task_type in UI (#31662)\n- Add ``--retry`` and ``--retry-delay`` to ``airflow db check`` (#31836)\n- Allow skipped task state task_instance_schema.py (#31421)\n- Add a new config for celery result_backend engine options (#30426)\n- UI Add Cluster Activity Page (#31123, #32446)\n- Adding keyboard shortcuts to common actions (#30950)\n- Adding more information to kubernetes executor logs (#29929)\n- Add support for configuring custom alembic file (#31415)\n- Add running and failed status tab for DAGs on the UI (#30429)\n- Add multi-select, proposals and labels for trigger form (#31441)\n- Making webserver config customizable (#29926)\n- Render DAGCode in the Grid View as a tab (#31113)\n- Add rest endpoint to get option of configuration (#31056)\n- Add ``section`` query param in get config rest API (#30936)\n- Create metrics to track ``Scheduled->Queued->Running`` task state transition times (#30612)\n- Mark Task Groups as Success/Failure (#30478)\n- Add CLI command to list the provider trigger info (#30822)\n- Add Fail Fast feature for DAGs (#29406)\n\nImprovements\n\"\"\"\"\"\"\"\"\"\"\"\"\n- Improve graph nesting logic (#33421)\n- Configurable health check threshold for triggerer (#33089, #33084)\n- add dag_run_ids and task_ids filter for the batch task instance API endpoint (#32705)\n- Ensure DAG-level references are filled on unmap (#33083)\n- Add support for arrays of different data types in the Trigger Form UI (#32734)\n- Always show gantt and code tabs (#33029)\n- Move listener success hook to after SQLAlchemy commit (#32988)\n- Rename ``db upgrade`` to ``db migrate`` and add ``connections create-default-connections`` (#32810, #33136)\n- Remove old gantt chart and redirect to grid views gantt tab (#32908)\n- Adjust graph zoom based on selected task (#32792)\n- Call listener on_task_instance_running after rendering templates (#32716)\n- Display execution_date in graph view task instance tooltip. (#32527)\n- Allow configuration to be contributed by providers (#32604, #32755, #32812)\n- Reduce default for max TIs per query, enforce ``<=`` parallelism (#32572)\n- Store config description in Airflow configuration object (#32669)\n- Use ``isdisjoint`` instead of ``not intersection`` (#32616)\n- Speed up calculation of leaves and roots for task groups (#32592)\n- Kubernetes Executor Load Time Optimizations (#30727)\n- Save DAG parsing time if dag is not schedulable (#30911)\n- Updates health check endpoint to include ``dag_processor`` status. (#32382)\n- Disable default allowing the testing of connections in UI, API and CLI (#32052, #33342)\n- Fix config var types under the scheduler section (#32132)\n- Allow to sort Grid View alphabetically (#32179)\n- Add hostname to triggerer metric ``[triggers.running]`` (#32050)\n- Improve DAG ORM cleanup code (#30614)\n- ``TriggerDagRunOperator``: Add ``wait_for_completion`` to ``template_fields`` (#31122)\n- Open links in new tab that take us away from Airflow UI (#32088)\n- Only show code tab when a task is not selected (#31744)\n- Add descriptions for celery and dask cert configs (#31822)\n- ``PythonVirtualenvOperator`` termination log in alert (#31747)\n- Migration of all DAG details to existing grid view dag details panel (#31690)\n- Add a diagram to help visualize timer metrics (#30650)\n- Celery Executor load time optimizations (#31001)\n- Update code style for ``airflow db`` commands to SQLAlchemy 2.0 style (#31486)\n- Mark uses of md5 as \"not-used-for-security\" in FIPS environments (#31171)\n- Add pydantic support to serde (#31565)\n- Enable search in note column in DagRun and TaskInstance (#31455)\n- Save scheduler execution time by adding new Index idea for dag_run (#30827)\n- Save scheduler execution time by caching dags (#30704)\n- Support for sorting DAGs by Last Run Date in the web UI (#31234)\n- Better typing for Job and JobRunners (#31240)\n- Add sorting logic by created_date for fetching triggers (#31151)\n- Remove DAGs.can_create on access control doc, adjust test fixture (#30862)\n- Split Celery logs into stdout/stderr (#30485)\n- Decouple metrics clients and ``validators`` into their own modules (#30802)\n- Description added for pagination in ``get_log`` api (#30729)\n- Optimize performance of scheduling mapped tasks (#30372)\n- Add sentry transport configuration option (#30419)\n- Better message on deserialization error (#30588)\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Remove user sessions when resetting password (#33347)\n- ``Gantt chart:`` Use earliest/oldest ti dates if different than dag run start/end (#33215)\n- Fix ``virtualenv`` detection for Python ``virtualenv`` operator (#33223)\n- Correctly log when there are problems trying to ``chmod`` ``airflow.cfg`` (#33118)\n- Pass app context to webserver_config.py (#32759)\n- Skip served logs for non-running task try (#32561)\n- Fix reload gunicorn workers (#32102)\n- Fix future DagRun rarely triggered by race conditions when ``max_active_runs`` reached its upper limit. (#31414)\n- Fix BaseOperator ``get_task_instances`` query (#33054)\n- Fix issue with using the various state enum value in logs (#33065)\n- Use string concatenation to prepend base URL for log_url (#33063)\n- Update graph nodes with operator style attributes (#32822)\n- Affix webserver access_denied warning to be configurable (#33022)\n- Only load task action modal if user can edit (#32992)\n- OpenAPI Spec fix nullable alongside ``$ref`` (#32887)\n- Make the decorators of ``PythonOperator`` sub-classes extend its decorator (#32845)\n- Fix check if ``virtualenv`` is installed in ``PythonVirtualenvOperator`` (#32939)\n- Unwrap Proxy before checking ``__iter__`` in is_container() (#32850)\n- Override base log folder by using task handler's base_log_folder (#32781)\n- Catch arbitrary exception from run_job to prevent zombie scheduler (#32707)\n- Fix depends_on_past work for dynamic tasks (#32397)\n- Sort extra_links for predictable order in UI. (#32762)\n- Fix prefix group false graph (#32764)\n- Fix bad delete logic for dagruns (#32684)\n- Fix bug in prune_dict where empty dict and list would be removed even in strict mode (#32573)\n- Add explicit browsers list and correct rel for blank target links (#32633)\n- Handle returned None when multiple_outputs is True (#32625)\n- Fix returned value when ShortCircuitOperator condition is falsy and there is not downstream tasks (#32623)\n- Fix returned value when ShortCircuitOperator condition is falsy (#32569)\n- Fix rendering of ``dagRunTimeout`` (#32565)\n- Fix permissions on ``/blocked`` endpoint (#32571)\n- Bugfix, prevent force of unpause on trigger DAG (#32456)\n- Fix data interval in ``cli.dags.trigger`` command output (#32548)\n- Strip ``whitespaces`` from airflow connections form (#32292)\n- Add timedelta support for applicable arguments of sensors (#32515)\n- Fix incorrect default on ``readonly`` property in our API (#32510)\n- Add xcom map_index as a filter to xcom endpoint (#32453)\n- Fix CLI commands when custom timetable is used (#32118)\n- Use WebEncoder to encode DagRun.conf in DagRun's list view (#32385)\n- Fix logic of the skip_all_except method (#31153)\n- Ensure dynamic tasks inside dynamic task group only marks the (#32354)\n- Handle the cases that webserver.expose_config is set to non-sensitive-only instead of boolean value (#32261)\n- Add retry functionality for handling process termination caused by database network issues (#31998)\n- Adapt Notifier for sla_miss_callback (#31887)\n- Fix XCOM view (#31807)\n- Fix for \"Filter dags by tag\" flickering on initial load of dags.html (#31578)\n- Fix where expanding ``resizer`` would not expanse grid view (#31581)\n- Fix MappedOperator-BaseOperator attr sync check (#31520)\n- Always pass named ``type_`` arg to drop_constraint (#31306)\n- Fix bad ``drop_constraint`` call in migrations (#31302)\n- Resolving problems with redesigned grid view (#31232)\n- Support ``requirepass`` redis sentinel (#30352)\n- Fix webserver crash when calling get ``/config`` (#31057)\n\nMisc/Internal\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Modify pathspec version restriction (#33349)\n- Refactor: Simplify code in ``dag_processing`` (#33161)\n- For now limit ``Pydantic`` to ``< 2.0.0`` (#33235)\n- Refactor: Simplify code in models (#33181)\n- Add elasticsearch group to pre-2.7 defaults (#33166)\n- Refactor: Simplify dict manipulation in airflow/cli (#33159)\n- Remove redundant dict.keys() call (#33158)\n- Upgrade ruff to latest 0.0.282 version in pre-commits (#33152)\n- Move openlineage configuration to provider (#33124)\n- Replace State by TaskInstanceState in Airflow executors (#32627)\n- Get rid of Python 2 numeric relics (#33050)\n- Remove legacy dag code (#33058)\n- Remove legacy task instance modal (#33060)\n- Remove old graph view (#32958)\n- Move CeleryExecutor to the celery provider (#32526, #32628)\n- Move all k8S classes to ``cncf.kubernetes`` provider (#32767, #32891)\n- Refactor existence-checking SQL to helper (#32790)\n- Extract Dask executor to new daskexecutor provider (#32772)\n- Remove atlas configuration definition (#32776)\n- Add Redis task handler (#31855)\n- Move writing configuration for webserver to main (webserver limited) (#32766)\n- Improve getting the query count in Airflow API endpoints (#32630)\n- Remove click upper bound (#32634)\n- Add D400 ``pydocstyle`` check - core Airflow only (#31297)\n- D205 Support (#31742, #32575, #32213, #32212, #32591, #32449, #32450)\n- Bump word-wrap from ``1.2.3 to 1.2.4`` in ``/airflow/www`` (#32680)\n- Strong-type all single-state enum values (#32537)\n- More strong typed state conversion (#32521)\n- SQL query improvements in utils/db.py (#32518)\n- Bump semver from ``6.3.0 to 6.3.1`` in ``/airflow/www`` (#32506)\n- Bump jsonschema version to ``4.18.0`` (#32445)\n- Bump ``stylelint`` from ``13.13.1 to 15.10.1`` in ``/airflow/www`` (#32435)\n- Bump tough-cookie from ``4.0.0 to 4.1.3`` in ``/airflow/www`` (#32443)\n- upgrade flask-appbuilder (#32054)\n- Support ``Pydantic`` 2 (#32366)\n- Limit click until we fix mypy issues (#32413)\n- A couple of minor cleanups (#31890)\n- Replace State usages with strong-typed ``enums`` (#31735)\n- Upgrade ruff to ``0.272`` (#31966)\n- Better error message when serializing callable without name (#31778)\n- Improve the views module a bit (#31661)\n- Remove ``asynctest`` (#31664)\n- Refactor sqlalchemy queries to ``2.0`` style (#31569, #31772, #32350, #32339, #32474, #32645)\n- Remove Python ``3.7`` support (#30963)\n- Bring back min-airflow-version for preinstalled providers (#31469)\n- Docstring improvements (#31375)\n- Improve typing in SchedulerJobRunner (#31285)\n- Upgrade ruff to ``0.0.262`` (#30809)\n- Upgrade to MyPy ``1.2.0`` (#30687)\n\nDocs only changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Clarify UI user types in security model (#33021)\n- Add links to ``DAGRun / DAG / Task`` in templates-ref.rst (#33013)\n- Add docs of how to test for DAG Import Errors (#32811)\n- Clean-up of our new security page (#32951)\n- Cleans up Extras reference page (#32954)\n- Update Dag trigger API and command docs (#32696)\n- Add deprecation info to the Airflow modules and classes docstring (#32635)\n- Formatting installation doc to improve readability (#32502)\n- Fix triggerer HA doc (#32454)\n- Add type annotation to code examples (#32422)\n- Document cron and delta timetables (#32392)\n- Update index.rst doc to correct grammar (#32315)\n- Fixing small typo in python.py (#31474)\n- Separate out and clarify policies for providers (#30657)\n- Fix docs: add an \"apache\" prefix to pip install (#30681)\n\n\nAirflow 2.6.3 (2023-07-10)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nDefault allowed pattern of a run_id has been changed to ``^[A-Za-z0-9_.~:+-]+$`` (#32293).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nPreviously, there was no validation on the run_id string. There is now a validation regex that\ncan be set by configuring ``allowed_run_id_pattern`` in ``scheduler`` section.\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Use linear time regular expressions (#32303)\n- Fix triggerers alive check and add a new conf for triggerer heartbeat rate (#32123)\n- Catch the exception that triggerer initialization failed (#31999)\n- Hide sensitive values from extra in connection edit form (#32309)\n- Sanitize ``DagRun.run_id`` and allow flexibility (#32293)\n- Add triggerer canceled log (#31757)\n- Fix try number shown in the task view (#32361)\n- Retry transactions on occasional deadlocks for rendered fields (#32341)\n- Fix behaviour of LazyDictWithCache when import fails (#32248)\n- Remove ``executor_class`` from Job - fixing backfill for custom executors (#32219)\n- Fix bugged singleton implementation (#32218)\n- Use ``mapIndex`` to display extra links per mapped task. (#32154)\n- Ensure that main triggerer thread exits if the async thread fails (#32092)\n- Use ``re2`` for matching untrusted regex (#32060)\n- Render list items in rendered fields view (#32042)\n- Fix hashing of ``dag_dependencies`` in serialized dag (#32037)\n- Return ``None`` if an XComArg fails to resolve in a multiple_outputs Task (#32027)\n- Check for DAG ID in query param from url as well as kwargs (#32014)\n- Flash an error message instead of failure in ``rendered-templates`` when map index is not found (#32011)\n- Fix ``ExternalTaskSensor`` when there is no task group TIs for the current execution date (#32009)\n- Fix number param html type in trigger template (#31980, #31946)\n- Fix masking nested variable fields (#31964)\n- Fix ``operator_extra_links`` property serialization in mapped tasks (#31904)\n- Decode old-style nested Xcom value (#31866)\n- Add a check for trailing slash in webserver base_url (#31833)\n- Fix connection uri parsing when the host includes a scheme (#31465)\n- Fix database session closing with ``xcom_pull`` and ``inlets`` (#31128)\n- Fix DAG's ``on_failure_callback`` is not invoked when task failed during testing dag. (#30965)\n- Fix airflow module version check when using ``ExternalPythonOperator`` and debug logging level (#30367)\n\nMisc/Internal\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Fix ``task.sensor`` annotation in type stub (#31954)\n- Limit ``Pydantic`` to ``< 2.0.0`` until we solve ``2.0.0`` incompatibilities (#32312)\n- Fix ``Pydantic`` 2 pickiness about model definition (#32307)\n\nDoc only changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Add explanation about tag creation and cleanup (#32406)\n- Minor updates to docs (#32369, #32315, #32310, #31794)\n- Clarify Listener API behavior (#32269)\n- Add information for users who ask for requirements (#32262)\n- Add links to DAGRun / DAG / Task in Templates Reference (#32245)\n- Add comment to warn off a potential wrong fix (#32230)\n- Add a note that we'll need to restart triggerer to reflect any trigger change (#32140)\n- Adding missing hyperlink to the tutorial documentation (#32105)\n- Added difference between Deferrable and Non-Deferrable Operators (#31840)\n- Add comments explaining need for special \"trigger end\" log message (#31812)\n- Documentation update on Plugin updates. (#31781)\n- Fix SemVer link in security documentation (#32320)\n- Update security model of Airflow (#32098)\n- Update references to restructured documentation from Airflow core (#32282)\n- Separate out advanced logging configuration (#32131)\n- Add ``™`` to Airflow in prominent places (#31977)\n\n\nAirflow 2.6.2 (2023-06-17)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n^^^^^^^^^\n- Cascade update of TaskInstance to TaskMap table (#31445)\n- Fix Kubernetes executors detection of deleted pods (#31274)\n- Use keyword parameters for migration methods for mssql (#31309)\n- Control permissibility of driver config in extra from airflow.cfg (#31754)\n- Fixing broken links in openapi/v1.yaml (#31619)\n- Hide old alert box when testing connection with different value (#31606)\n- Add TriggererStatus to OpenAPI spec (#31579)\n- Resolving issue where Grid won't un-collapse when Details is collapsed (#31561)\n- Fix sorting of tags (#31553)\n- Add the missing ``map_index`` to the xcom key when skipping downstream tasks (#31541)\n- Fix airflow users delete CLI command (#31539)\n- Include triggerer health status in Airflow ``/health`` endpoint (#31529)\n- Remove dependency already registered for this task warning (#31502)\n- Use kube_client over default CoreV1Api for deleting pods (#31477)\n- Ensure min backoff in base sensor is at least 1 (#31412)\n- Fix ``max_active_tis_per_dagrun`` for Dynamic Task Mapping (#31406)\n- Fix error handling when pre-importing modules in DAGs (#31401)\n- Fix dropdown default and adjust tutorial to use 42 as default for proof (#31400)\n- Fix crash when clearing run with task from normal to mapped (#31352)\n- Make BaseJobRunner a generic on the job class (#31287)\n- Fix ``url_for_asset`` fallback and 404 on DAG Audit Log (#31233)\n- Don't present an undefined execution date (#31196)\n- Added spinner activity while the logs load (#31165)\n- Include rediss to the list of supported URL schemes (#31028)\n- Optimize scheduler by skipping \"non-schedulable\" DAGs (#30706)\n- Save scheduler execution time during search for queued dag_runs (#30699)\n- Fix ExternalTaskSensor to work correctly with task groups (#30742)\n- Fix DAG.access_control can't sync when clean access_control (#30340)\n- Fix failing get_safe_url tests for latest Python 3.8 and 3.9 (#31766)\n- Fix typing for POST user endpoint (#31767)\n- Fix wrong update for nested group default args (#31776)\n- Fix overriding ``default_args`` in nested task groups (#31608)\n- Mark ``[secrets] backend_kwargs`` as a sensitive config (#31788)\n- Executor events are not always \"exited\" here (#30859)\n- Validate connection IDs (#31140)\n\nMisc/Internal\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Add Python 3.11 support (#27264)\n- Replace unicodecsv with standard csv library (#31693)\n- Bring back unicodecsv as dependency of Airflow (#31814)\n- Remove found_descendents param from get_flat_relative_ids (#31559)\n- Fix typing in external task triggers (#31490)\n- Wording the next and last run DAG columns better (#31467)\n- Skip auto-document things with :meta private: (#31380)\n- Add an example for sql_alchemy_connect_args conf (#31332)\n- Convert dask upper-binding into exclusion (#31329)\n- Upgrade FAB to 4.3.1 (#31203)\n- Added metavar and choices to --state flag in airflow dags list-jobs CLI for suggesting valid state arguments. (#31308)\n- Use only one line for tmp dir log (#31170)\n- Rephrase comment in setup.py (#31312)\n- Add fullname to owner on logging (#30185)\n- Make connection id validation consistent across interface (#31282)\n- Use single source of truth for sensitive config items (#31820)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n- Add docstring and signature for _read_remote_logs (#31623)\n- Remove note about triggerer being 3.7+ only (#31483)\n- Fix version support information (#31468)\n- Add missing BashOperator import to documentation example (#31436)\n- Fix task.branch error caused by incorrect initial parameter (#31265)\n- Update callbacks documentation (errors and context) (#31116)\n- Add an example for dynamic task mapping with non-TaskFlow operator (#29762)\n- Few doc fixes - links, grammar and wording (#31719)\n- Add description in a few more places about adding airflow to pip install (#31448)\n- Fix table formatting in docker build documentation (#31472)\n- Update documentation for constraints installation (#31882)\n\nAirflow 2.6.1 (2023-05-16)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nClarifications of the external Health Check mechanism and using ``Job`` classes (#31277).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIn the past SchedulerJob and other ``*Job`` classes are known to have been used to perform\nexternal health checks for Airflow components. Those are, however, Airflow DB ORM related classes.\nThe DB models and database structure of Airflow are considered as internal implementation detail, following\n`public interface <https://airflow.apache.org/docs/apache-airflow/stable/public-airflow-interface.html>`_).\nTherefore, they should not be used for external health checks. Instead, you should use the\n``airflow jobs check`` CLI command (introduced in Airflow 2.1) for that purpose.\n\nBug Fixes\n^^^^^^^^^\n- Fix calculation of health check threshold for SchedulerJob (#31277)\n- Fix timestamp parse failure for k8s executor pod tailing (#31175)\n- Make sure that DAG processor job row has filled value in ``job_type`` column (#31182)\n- Fix section name reference for ``api_client_retry_configuration`` (#31174)\n- Ensure the KPO runs pod mutation hooks correctly (#31173)\n- Remove worrying log message about redaction from the OpenLineage plugin (#31149)\n- Move ``interleave_timestamp_parser`` config to the logging section (#31102)\n- Ensure that we check worker for served logs if no local or remote logs found (#31101)\n- Fix ``MappedTaskGroup`` import in taskinstance file (#31100)\n- Format DagBag.dagbag_report() Output (#31095)\n- Mask task attribute on task detail view (#31125)\n- Fix template error when iterating None value and fix params documentation (#31078)\n- Fix ``apache-hive`` extra so it installs the correct package (#31068)\n- Fix issue with zip files in DAGs folder when pre-importing Airflow modules (#31061)\n- Move TaskInstanceKey to a separate file to fix circular import (#31033, #31204)\n- Fix deleting DagRuns and TaskInstances that have a note (#30987)\n- Fix ``airflow providers get`` command output (#30978)\n- Fix Pool schema in the OpenAPI spec (#30973)\n- Add support for dynamic tasks with template fields that contain ``pandas.DataFrame`` (#30943)\n- Use the Task Group explicitly passed to 'partial' if any (#30933)\n- Fix ``order_by`` request in list DAG rest api (#30926)\n- Include node height/width in center-on-task logic (#30924)\n- Remove print from dag trigger command (#30921)\n- Improve task group UI in new graph (#30918)\n- Fix mapped states in grid view (#30916)\n- Fix problem with displaying graph (#30765)\n- Fix backfill KeyError when try_number out of sync (#30653)\n- Re-enable clear and setting state in the TaskInstance UI (#30415)\n- Prevent DagRun's ``state`` and ``start_date`` from being reset when clearing a task in a running DagRun (#30125)\n\nMisc/Internal\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Upper bind dask until they solve a side effect in their test suite (#31259)\n- Show task instances affected by clearing in a table (#30633)\n- Fix missing models in API documentation (#31021)\n\nDoc only changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Improve description of the ``dag_processing.processes`` metric (#30891)\n- Improve Quick Start instructions (#30820)\n- Add section about missing task logs to the FAQ (#30717)\n- Mount the ``config`` directory in docker compose (#30662)\n- Update ``version_added`` config field for ``might_contain_dag`` and ``metrics_allow_list`` (#30969)\n\n\nAirflow 2.6.0 (2023-04-30)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nDefault permissions of file task handler log directories and files has been changed to \"owner + group\" writeable (#29506).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nDefault setting handles case where impersonation is needed and both users (airflow and the impersonated user)\nhave the same group set as main group. Previously the default was also other-writeable and the user might choose\nto use the other-writeable setting if they wish by configuring ``file_task_handler_new_folder_permissions``\nand ``file_task_handler_new_file_permissions`` in ``logging`` section.\n\nSLA callbacks no longer add files to the dag processor manager's queue (#30076)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThis stops SLA callbacks from keeping the dag processor manager permanently busy. It means reduced CPU,\nand fixes issues where SLAs stop the system from seeing changes to existing dag files. Additional metrics added to help track queue state.\n\nThe ``cleanup()`` method in BaseTrigger is now defined as asynchronous (following async/await) pattern (#30152).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThis is potentially a breaking change for any custom trigger implementations that override the ``cleanup()``\nmethod and uses synchronous code, however using synchronous operations in cleanup was technically wrong,\nbecause the method was executed in the main loop of the Triggerer and it was introducing unnecessary delays\nimpacting other triggers. The change is unlikely to affect any existing trigger implementations.\n\nThe gauge ``scheduler.tasks.running`` no longer exist (#30374)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThe gauge has never been working and its value has always been 0. Having an accurate\nvalue for this metric is complex so it has been decided that removing this gauge makes\nmore sense than fixing it with no certainty of the correctness of its value.\n\nConsolidate handling of tasks stuck in queued under new ``task_queued_timeout`` config (#30375)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nLogic for handling tasks stuck in the queued state has been consolidated, and the all configurations\nresponsible for timing out stuck queued tasks have been deprecated and merged into\n``[scheduler] task_queued_timeout``. The configurations that have been deprecated are\n``[kubernetes] worker_pods_pending_timeout``, ``[celery] stalled_task_timeout``, and\n``[celery] task_adoption_timeout``. If any of these configurations are set, the longest timeout will be\nrespected. For example, if ``[celery] stalled_task_timeout`` is 1200, and ``[scheduler] task_queued_timeout``\nis 600, Airflow will set ``[scheduler] task_queued_timeout`` to 1200.\n\nImprovement Changes\n^^^^^^^^^^^^^^^^^^^\n\nDisplay only the running configuration in configurations view (#28892)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThe configurations view now only displays the running configuration. Previously, the default configuration\nwas displayed at the top but it was not obvious whether this default configuration was overridden or not.\nSubsequently, the non-documented endpoint ``/configuration?raw=true`` is deprecated and will be removed in\nAirflow 3.0. The HTTP response now returns an additional ``Deprecation`` header. The ``/config`` endpoint on\nthe REST API is the standard way to fetch Airflow configuration programmatically.\n\nExplicit skipped states list for ExternalTaskSensor (#29933)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nExternalTaskSensor now has an explicit ``skipped_states`` list\n\nMiscellaneous Changes\n^^^^^^^^^^^^^^^^^^^^^\n\nHandle OverflowError on exponential backoff in next_run_calculation (#28172)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nMaximum retry task delay is set to be 24h (86400s) by default. You can change it globally via ``core.max_task_retry_delay``\nparameter.\n\nMove Hive macros to the provider (#28538)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThe Hive Macros (``hive.max_partition``, ``hive.closest_ds_partition``) are available only when Hive Provider is\ninstalled. Please install Hive Provider > 5.1.0 when using those macros.\n\nUpdated app to support configuring the caching hash method for FIPS v2 (#30675)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nVarious updates for FIPS-compliance when running Airflow in Python 3.9+. This includes a new webserver option, ``caching_hash_method``,\nfor changing the default flask caching method.\n\nNew Features\n^^^^^^^^^^^^\n- AIP-50 Trigger DAG UI Extension with Flexible User Form Concept (#27063,#29376)\n- Skip PythonVirtualenvOperator task when it returns a provided exit code (#30690)\n- rename skip_exit_code to skip_on_exit_code and allow providing multiple codes (#30692)\n- Add skip_on_exit_code also to ExternalPythonOperator (#30738)\n- Add ``max_active_tis_per_dagrun`` for Dynamic Task Mapping (#29094)\n- Add serializer for pandas dataframe (#30390)\n- Deferrable ``TriggerDagRunOperator`` (#30292)\n- Add command to get DAG Details via CLI (#30432)\n- Adding ContinuousTimetable and support for @continuous schedule_interval (#29909)\n- Allow customized rules to check if a file has dag (#30104)\n- Add a new Airflow conf to specify a SSL ca cert for Kubernetes client (#30048)\n- Bash sensor has an explicit retry code (#30080)\n- Add filter task upstream/downstream to grid view (#29885)\n- Add testing a connection via Airflow CLI (#29892)\n- Support deleting the local log files when using remote logging (#29772)\n- ``Blocklist`` to disable specific metric tags or metric names (#29881)\n- Add a new graph inside of the grid view (#29413)\n- Add database ``check_migrations`` config (#29714)\n- add output format arg for ``cli.dags.trigger`` (#29224)\n- Make json and yaml available in templates (#28930)\n- Enable tagged metric names for existing Statsd metric publishing events | influxdb-statsd support (#29093)\n- Add arg --yes to ``db export-archived`` command. (#29485)\n- Make the policy functions pluggable (#28558)\n- Add ``airflow db drop-archived`` command (#29309)\n- Enable individual trigger logging (#27758)\n- Implement new filtering options in graph view (#29226)\n- Add triggers for ExternalTask (#29313)\n- Add command to export purged records to CSV files (#29058)\n- Add ``FileTrigger`` (#29265)\n- Emit DataDog statsd metrics with metadata tags (#28961)\n- Add some statsd metrics for dataset (#28907)\n- Add --overwrite option to ``connections import`` CLI command (#28738)\n- Add general-purpose \"notifier\" concept to DAGs (#28569)\n- Add a new conf to wait past_deps before skipping a task (#27710)\n- Add Flink on K8s Operator  (#28512)\n- Allow Users to disable SwaggerUI via configuration (#28354)\n- Show mapped task groups in graph (#28392)\n- Log FileTaskHandler to work with KubernetesExecutor's multi_namespace_mode (#28436)\n- Add a new config for adapting masked secrets to make it easier to prevent secret leakage in logs (#28239)\n- List specific config section and its values using the cli (#28334)\n- KubernetesExecutor multi_namespace_mode can use namespace list to avoid requiring cluster role (#28047)\n- Automatically save and allow restore of recent DAG run configs (#27805)\n- Added exclude_microseconds to cli (#27640)\n\nImprovements\n\"\"\"\"\"\"\"\"\"\"\"\"\n- Rename most pod_id usage to pod_name in KubernetesExecutor (#29147)\n- Update the error message for invalid use of poke-only sensors (#30821)\n- Update log level in scheduler critical section edge case (#30694)\n- AIP-51 Removing Executor Coupling from Core Airflow (`AIP-51 <https://github.com/apache/airflow/pulls?q=is%3Apr+is%3Amerged+label%3AAIP-51+milestone%3A%22Airflow+2.6.0%22>`__)\n- Add multiple exit code handling in skip logic for BashOperator (#30739)\n- Updated app to support configuring the caching hash method for FIPS v2 (#30675)\n- Preload airflow imports before dag parsing to save time (#30495)\n- Improve task & run actions ``UX`` in grid view (#30373)\n- Speed up TaskGroups with caching property of group_id (#30284)\n- Use the engine provided in the session (#29804)\n- Type related import optimization for Executors (#30361)\n- Add more type hints to the code base (#30503)\n- Always use self.appbuilder.get_session in security managers (#30233)\n- Update SQLAlchemy ``select()`` to new style (#30515)\n- Refactor out xcom constants from models (#30180)\n- Add exception class name to DAG-parsing error message (#30105)\n- Rename statsd_allow_list and statsd_block_list to ``metrics_*_list`` (#30174)\n- Improve serialization of tuples and sets (#29019)\n- Make cleanup method in trigger an async one (#30152)\n- Lazy load serialization modules (#30094)\n- SLA callbacks no longer add files to the dag_processing manager queue (#30076)\n- Add task.trigger rule to grid_data (#30130)\n- Speed up log template sync by avoiding ORM (#30119)\n- Separate cli_parser.py into two modules (#29962)\n- Explicit skipped states list for ExternalTaskSensor (#29933)\n- Add task state hover highlighting to new graph (#30100)\n- Store grid tabs in url params (#29904)\n- Use custom Connexion resolver to load lazily (#29992)\n- Delay Kubernetes import in secret masker (#29993)\n- Delay ConnectionModelView init until it's accessed (#29946)\n- Scheduler, make stale DAG deactivation threshold configurable instead of using dag processing timeout (#29446)\n- Improve grid view height calculations (#29563)\n- Avoid importing executor during conf validation (#29569)\n- Make permissions for FileTaskHandler group-writeable and configurable (#29506)\n- Add colors in help outputs of Airflow CLI commands #28789 (#29116)\n- Add a param for get_dags endpoint to list only unpaused dags (#28713)\n- Expose updated_at filter for dag run and task instance endpoints (#28636)\n- Increase length of user identifier columns (#29061)\n- Update gantt chart UI to display queued state of tasks (#28686)\n- Add index on log.dttm (#28944)\n- Display only the running configuration in configurations view (#28892)\n- Cap dropdown menu size dynamically (#28736)\n- Added JSON linter to connection edit / add UI for field extra. On connection edit screen, existing extra data will be displayed indented (#28583)\n- Use labels instead of pod name for pod log read in k8s exec (#28546)\n- Use time not tries for queued & running re-checks. (#28586)\n- CustomTTYColoredFormatter should inherit TimezoneAware formatter (#28439)\n- Improve past depends handling in Airflow CLI tasks.run command (#28113)\n- Support using a list of callbacks in ``on_*_callback/sla_miss_callbacks`` (#28469)\n- Better table name validation for db clean (#28246)\n- Use object instead of array in config.yml for config template (#28417)\n- Add markdown rendering for task notes. (#28245)\n- Show mapped task groups in grid view (#28208)\n- Add ``renamed`` and ``previous_name`` in config sections (#28324)\n- Speed up most Users/Role CLI commands (#28259)\n- Speed up Airflow role list command (#28244)\n- Refactor serialization (#28067, #30819, #30823)\n- Allow longer pod names for k8s executor / KPO (#27736)\n- Updates health check endpoint to include ``triggerer`` status (#27755)\n\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Fix static_folder for cli app (#30952)\n- Initialize plugins for cli appbuilder (#30934)\n- Fix dag file processor heartbeat to run only if necessary (#30899)\n- Fix KubernetesExecutor sending state to scheduler (#30872)\n- Count mapped upstream only if all are finished (#30641)\n- ExternalTaskSensor: add external_task_group_id to template_fields (#30401)\n- Improve url detection for task instance details (#30779)\n- Use material icons for dag import error banner (#30771)\n- Fix misc grid/graph view UI bugs (#30752)\n- Add a collapse grid button (#30711)\n- Fix d3 dependencies (#30702)\n- Simplify logic to resolve tasks stuck in queued despite stalled_task_timeout (#30375)\n- When clearing task instances try to get associated DAGs from database (#29065)\n- Fix mapped tasks partial arguments when DAG default args are provided (#29913)\n- Deactivate DAGs deleted from within zip files (#30608)\n- Recover from ``too old resource version exception`` by retrieving the latest ``resource_version`` (#30425)\n- Fix possible race condition when refreshing DAGs (#30392)\n- Use custom validator for OpenAPI request body (#30596)\n- Fix ``TriggerDagRunOperator`` with deferrable parameter (#30406)\n- Speed up dag runs deletion (#30330)\n- Do not use template literals to construct html elements (#30447)\n- Fix deprecation warning in ``example_sensor_decorator`` DAG (#30513)\n- Avoid logging sensitive information in triggerer job log (#30110)\n- Add a new parameter for base sensor to catch the exceptions in poke method (#30293)\n- Fix dag run conf encoding with non-JSON serializable values (#28777)\n- Added fixes for Airflow to be usable on Windows Dask-Workers (#30249)\n- Force DAG last modified time to UTC (#30243)\n- Fix EmptySkipOperator in example dag (#30269)\n- Make the webserver startup respect update_fab_perms (#30246)\n- Ignore error when changing log folder permissions (#30123)\n- Disable ordering DagRuns by note (#30043)\n- Fix reading logs from finished KubernetesExecutor worker pod (#28817)\n- Mask out non-access bits when comparing file modes (#29886)\n- Remove Run task action from UI (#29706)\n- Fix log tailing issues with legacy log view (#29496)\n- Fixes to how DebugExecutor handles sensors (#28528)\n- Ensure that pod_mutation_hook is called before logging the pod name (#28534)\n- Handle OverflowError on exponential backoff in next_run_calculation (#28172)\n\nMisc/Internal\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Make eager upgrade additional dependencies optional (#30811)\n- Upgrade to pip 23.1.1 (#30808)\n- Remove protobuf limitation from eager upgrade (#30182)\n- Remove protobuf limitation from eager upgrade (#30182)\n- Deprecate ``skip_exit_code`` in ``BashOperator`` (#30734)\n- Remove gauge ``scheduler.tasks.running`` (#30374)\n- Bump json5 to 1.0.2 and eslint-plugin-import to 2.27.5 in ``/airflow/www`` (#30568)\n- Add tests to PythonOperator (#30362)\n- Add asgiref as a core dependency (#30527)\n- Discovery safe mode toggle comment clarification (#30459)\n- Upgrade moment-timezone package to fix Tehran tz (#30455)\n- Bump loader-utils from 2.0.0 to 2.0.4 in ``/airflow/www`` (#30319)\n- Bump babel-loader from 8.1.0 to 9.1.0 in ``/airflow/www`` (#30316)\n- DagBag: Use ``dag.fileloc`` instead of ``dag.full_filepath`` in exception message (#30610)\n- Change log level of serialization information (#30239)\n- Minor DagRun helper method cleanup (#30092)\n- Improve type hinting in stats.py (#30024)\n- Limit ``importlib-metadata`` backport to < 5.0.0 (#29924)\n- Align cncf provider file names with AIP-21 (#29905)\n- Upgrade FAB to 4.3.0 (#29766)\n- Clear ExecutorLoader cache in tests (#29849)\n- Lazy load Task Instance logs in UI (#29827)\n- added warning log for max page limit exceeding api calls (#29788)\n- Aggressively cache entry points in process (#29625)\n- Don't use ``importlib.metadata`` to get Version for speed (#29723)\n- Upgrade Mypy to 1.0 (#29468)\n- Rename ``db export-cleaned`` to ``db export-archived`` (#29450)\n- listener: simplify API by replacing SQLAlchemy event-listening by direct calls (#29289)\n- No multi-line log entry for bash env vars (#28881)\n- Switch to ruff for faster static checks (#28893)\n- Remove horizontal lines in TI logs (#28876)\n- Make allowed_deserialization_classes more intuitive (#28829)\n- Propagate logs to stdout when in k8s executor pod (#28440, #30860)\n- Fix code readability, add docstrings to json_client (#28619)\n- AIP-51 - Misc. Compatibility Checks (#28375)\n- Fix is_local for LocalKubernetesExecutor (#28288)\n- Move Hive macros to the provider (#28538)\n- Rerun flaky PinotDB integration test (#28562)\n- Add pre-commit hook to check session default value (#28007)\n- Refactor get_mapped_group_summaries for web UI (#28374)\n- Add support for k8s 1.26 (#28320)\n- Replace ``freezegun`` with time-machine (#28193)\n- Completed D400 for ``airflow/kubernetes/*`` (#28212)\n- Completed D400 for multiple folders (#27969)\n- Drop k8s 1.21 and 1.22 support (#28168)\n- Remove unused task_queue attr from k8s scheduler class (#28049)\n- Completed D400 for multiple folders (#27767, #27768)\n\n\nDoc only changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- Add instructions on how to avoid accidental airflow upgrade/downgrade (#30813)\n- Add explicit information about how to write task logs (#30732)\n- Better explanation on how to log from tasks (#30746)\n- Use correct import path for Dataset (#30617)\n- Create ``audit_logs.rst`` (#30405)\n- Adding taskflow API example for sensors (#30344)\n- Add clarification about timezone aware dags (#30467)\n- Clarity params documentation (#30345)\n- Fix unit for task duration metric (#30273)\n- Update dag-run.rst for dead links of cli commands (#30254)\n- Add Write efficient Python code section to Reducing DAG complexity (#30158)\n- Allow to specify which connection, variable or config are being looked up in the backend using ``*_lookup_pattern`` parameters (#29580)\n- Add Documentation for notification feature extension (#29191)\n- Clarify that executor interface is public but instances are not (#29200)\n- Add Public Interface description to Airflow documentation (#28300)\n- Add documentation for task group mapping (#28001)\n- Some fixes to metrics doc (#30290)\n\n\nAirflow 2.5.3 (2023-04-01)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n^^^^^^^^^\n- Fix DagProcessorJob integration for standalone dag-processor (#30278)\n- Fix proper termination of gunicorn when it hangs (#30188)\n- Fix XCom.get_one exactly one exception text (#30183)\n- Correct the VARCHAR size to 250. (#30178)\n- Revert fix for on_failure_callback when task receives a SIGTERM (#30165)\n- Move read only property to DagState to fix generated docs (#30149)\n- Ensure that ``dag.partial_subset`` doesn't mutate task group properties (#30129)\n- Fix inconsistent returned value of ``airflow dags next-execution`` cli command (#30117)\n- Fix www/utils.dag_run_link redirection (#30098)\n- Fix ``TriggerRuleDep`` when the mapped tasks count is 0 (#30084)\n- Dag processor manager, add retry_db_transaction to _fetch_callbacks (#30079)\n- Fix db clean command for mysql db (#29999)\n- Avoid considering EmptyOperator in mini scheduler (#29979)\n- Fix some long known Graph View UI problems (#29971, #30355, #30360)\n- Fix dag docs toggle icon initial angle (#29970)\n- Fix tags selection in DAGs UI (#29944)\n- Including airflow/example_dags/sql/sample.sql in MANIFEST.in (#29883)\n- Fixing broken filter in /taskinstance/list view (#29850)\n- Allow generic param dicts (#29782)\n- Fix update_mask in patch variable route (#29711)\n- Strip markup from app_name if instance_name_has_markup = True (#28894)\n\nMisc/Internal\n^^^^^^^^^^^^^\n- Revert \"Also limit importlib on Python 3.9 (#30069)\" (#30209)\n- Add custom_operator_name to @task.sensor tasks (#30131)\n- Bump webpack from 5.73.0 to 5.76.0 in /airflow/www (#30112)\n- Formatted config (#30103)\n- Remove upper bound limit of astroid (#30033)\n- Remove accidentally merged vendor daemon patch code (#29895)\n- Fix warning in airflow tasks test command regarding absence of data_interval (#27106)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n- Adding more information regarding top level code (#30040)\n- Update workday example (#30026)\n- Fix some typos in the DAGs docs (#30015)\n- Update set-up-database.rst (#29991)\n- Fix some typos on the kubernetes documentation (#29936)\n- Fix some punctuation and grammar (#29342)\n\n\nAirflow 2.5.2 (2023-03-15)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nThe date-time fields passed as API parameters or Params should be RFC3339-compliant (#29395)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIn case of API calls, it was possible that \"+\" passed as part of the date-time fields were not URL-encoded, and\nsuch date-time fields could pass validation. Such date-time parameters should now be URL-encoded (as ``%2B``).\n\nIn case of parameters, we still allow IS8601-compliant date-time (so for example it is possible that\n' ' was used instead of ``T`` separating date from time and no timezone was specified) but we raise\ndeprecation warning.\n\nDefault for ``[webserver] expose_hostname`` changed to ``False`` (#29547)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe default for ``[webserver] expose_hostname`` has been set to ``False``, instead of ``True``. This means administrators must opt-in to expose webserver hostnames to end users.\n\nBug Fixes\n^^^^^^^^^\n- Fix validation of date-time field in API and Parameter schemas (#29395)\n- Fix grid logs for large logs (#29390)\n- Fix on_failure_callback when task receives a SIGTERM (#29743)\n- Update min version of python-daemon to fix containerd file limits (#29916)\n- POST ``/dagRuns`` API should 404 if dag not active (#29860)\n- DAG list sorting lost when switching page (#29756)\n- Fix Scheduler crash when clear a previous run of a normal task that is now a mapped task (#29645)\n- Convert moment with timezone to UTC instead of raising an exception (#29606)\n- Fix clear dag run ``openapi`` spec responses by adding additional return type (#29600)\n- Don't display empty rendered attrs in Task Instance Details page (#29545)\n- Remove section check from get-value command (#29541)\n- Do not show version/node in UI traceback for unauthenticated user (#29501)\n- Make ``prev_logical_date`` variable offset-aware (#29454)\n- Fix nested fields rendering in mapped operators (#29451)\n- Datasets, next_run_datasets, remove unnecessary timestamp filter (#29441)\n- ``Edgemodifier`` refactoring w/ labels in TaskGroup edge case (#29410)\n- Fix Rest API update user output (#29409)\n- Ensure Serialized DAG is deleted (#29407)\n- Persist DAG and task doc values in TaskFlow API if explicitly set (#29399)\n- Redirect to the origin page with all the params (#29212)\n- Fixing Task Duration view in case of manual DAG runs only (#22015) (#29195)\n- Remove poke method to fall back to parent implementation (#29146)\n- PR: Introduced fix to run tasks on Windows systems (#29107)\n- Fix warning in migrations about old config. (#29092)\n- Emit dagrun failed duration when timeout (#29076)\n- Handling error on cluster policy itself (#29056)\n- Fix kerberos authentication for the REST API. (#29054)\n- Fix leak sensitive field via V1EnvVar on exception (#29016)\n- Sanitize url_for arguments before they are passed (#29039)\n- Fix dag run trigger with a note. (#29228)\n- Write action log to DB when DAG run is triggered via API (#28998)\n- Resolve all variables in pickled XCom iterator (#28982)\n- Allow URI without authority and host blocks in ``airflow connections add`` (#28922)\n- Be more selective when adopting pods with KubernetesExecutor (#28899)\n- KubenetesExecutor sends state even when successful (#28871)\n- Annotate KubernetesExecutor pods that we don't delete (#28844)\n- Throttle streaming log reads (#28818)\n- Introduce dag processor job (#28799)\n- Fix #28391 manual task trigger from UI fails for k8s executor (#28394)\n- Logging poke info when external dag is not none and task_id and task_ids are none (#28097)\n- Fix inconsistencies in checking edit permissions for a DAG (#20346)\n\nMisc/Internal\n^^^^^^^^^^^^^\n- Add a check for not templateable fields (#29821)\n- Removed continue for not in (#29791)\n- Move extra links position in grid view (#29703)\n- Bump ``undici`` from ``5.9.1`` to ``5.19.1`` (#29583)\n- Change expose_hostname default to false (#29547)\n- Change permissions of config/password files created by airflow (#29495)\n- Use newer setuptools ``v67.2.0`` (#29465)\n- Increase max height for grid view elements (#29367)\n- Clarify description of worker control config (#29247)\n- Bump ``ua-parser-js`` from ``0.7.31`` to ``0.7.33`` in ``/airflow/www`` (#29172)\n- Remove upper bound limitation for ``pytest`` (#29086)\n- Check for ``run_id`` url param when linking to ``graph/gantt`` views (#29066)\n- Clarify graph view dynamic task labels (#29042)\n- Fixing import error for dataset (#29007)\n- Update how PythonSensor returns values from ``python_callable`` (#28932)\n- Add dep context description for better log message (#28875)\n- Bump ``swagger-ui-dist`` from ``3.52.0`` to ``4.1.3`` in ``/airflow/www`` (#28824)\n- Limit ``importlib-metadata`` backport to ``< 5.0.0`` (#29924, #30069)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n- Update pipeline.rst - Fix query in ``merge_data()`` task (#29158)\n- Correct argument name of Workday timetable in timetable.rst (#29896)\n- Update ref anchor for env var link in Connection how-to doc (#29816)\n- Better description for limit in api (#29773)\n- Description of dag_processing.last_duration (#29740)\n- Update docs re: template_fields typing and subclasses (#29725)\n- Fix formatting of Dataset inlet/outlet note in TaskFlow concepts (#29678)\n- Specific use-case: adding packages via requirements.txt in compose (#29598)\n- Detect is 'docker-compose' existing (#29544)\n- Add Landing Times entry to UI docs (#29511)\n- Improve health checks in example docker-compose and clarify usage (#29408)\n- Remove ``notes`` param from TriggerDagRunOperator docstring (#29298)\n- Use ``schedule`` param rather than ``timetable`` in Timetables docs (#29255)\n- Add trigger process to Airflow Docker docs (#29203)\n- Update set-up-database.rst (#29104)\n- Several improvements to the Params doc (#29062)\n- Email Config docs more explicit env var examples (#28845)\n- Listener plugin example added (#27905)\n\n\nAirflow 2.5.1 (2023-01-20)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nTrigger gevent ``monkeypatching`` via environment variable (#28283)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIf you are using gevent for your webserver deployment and used local settings to ``monkeypatch`` gevent,\nyou might want to replace local settings patching with an ``_AIRFLOW_PATCH_GEVENT`` environment variable\nset to 1 in your webserver. This ensures gevent patching is done as early as possible.\n\nBug Fixes\n^^^^^^^^^\n- Fix masking of non-sensitive environment variables (#28802)\n- Remove swagger-ui extra from connexion and install ``swagger-ui-dist`` via npm package (#28788)\n- Fix ``UIAlert`` should_show when ``AUTH_ROLE_PUBLIC`` set (#28781)\n- Only patch single label when adopting pod (#28776)\n- Update CSRF token to expire with session (#28730)\n- Fix \"airflow tasks render\" cli command for mapped task instances (#28698)\n- Allow XComArgs for ``external_task_ids`` of ExternalTaskSensor (#28692)\n- Row-lock TIs to be removed during mapped task expansion (#28689)\n- Handle ConnectionReset exception in Executor cleanup (#28685)\n- Fix description of output redirection for access_log for gunicorn (#28672)\n- Add back join to zombie query that was dropped in #28198 (#28544)\n- Fix calendar view for CronTriggerTimeTable dags (#28411)\n- After running the DAG the employees table is empty. (#28353)\n- Fix ``DetachedInstanceError`` when finding zombies in Dag Parsing process (#28198)\n- Nest header blocks in ``divs`` to fix ``dagid`` copy nit on dag.html (#28643)\n- Fix UI caret direction (#28624)\n- Guard not-yet-expanded ti in trigger rule dep (#28592)\n- Move TI ``setNote`` endpoints under TaskInstance in OpenAPI (#28566)\n- Consider previous run in ``CronTriggerTimetable`` (#28532)\n- Ensure correct log dir in file task handler (#28477)\n- Fix bad pods pickled in executor_config (#28454)\n- Add ``ensure_ascii=False`` in trigger dag run API (#28451)\n- Add setters to MappedOperator on_*_callbacks (#28313)\n- Fix ``ti._try_number`` for deferred and up_for_reschedule tasks (#26993)\n- separate ``callModal`` from dag.js (#28410)\n- A manual run can't look like a scheduled one (#28397)\n- Dont show task/run durations when there is no start_date (#28395)\n- Maintain manual scroll position in task logs (#28386)\n- Correctly select a mapped task's \"previous\" task (#28379)\n- Trigger gevent ``monkeypatching`` via environment variable (#28283)\n- Fix db clean warnings (#28243)\n- Make arguments 'offset' and 'length' not required (#28234)\n- Make live logs reading work for \"other\" k8s executors (#28213)\n- Add custom pickling hooks to ``LazyXComAccess`` (#28191)\n- fix next run datasets error (#28165)\n- Ensure that warnings from ``@dag`` decorator are reported in dag file (#28153)\n- Do not warn when airflow dags tests command is used (#28138)\n- Ensure the ``dagbag_size`` metric decreases when files are deleted (#28135)\n- Improve run/task grid view actions (#28130)\n- Make BaseJob.most_recent_job favor \"running\" jobs (#28119)\n- Don't emit FutureWarning when code not calling old key (#28109)\n- Add ``airflow.api.auth.backend.session`` to backend sessions in compose (#28094)\n- Resolve false warning about calling conf.get on moved item (#28075)\n- Return list of tasks that will be changed (#28066)\n- Handle bad zip files nicely when parsing DAGs. (#28011)\n- Prevent double loading of providers from local paths (#27988)\n- Fix deadlock when chaining multiple empty mapped tasks (#27964)\n- fix: current_state method on TaskInstance doesn't filter by map_index (#27898)\n- Don't log CLI actions if db not initialized (#27851)\n- Make sure we can get out of a faulty scheduler state (#27834)\n- dagrun, ``next_dagruns_to_examine``, add MySQL index hint (#27821)\n- Handle DAG disappearing mid-flight when dag verification happens (#27720)\n- fix: continue checking sla (#26968)\n- Allow generation of connection URI to work when no conn type (#26765)\n\nMisc/Internal\n^^^^^^^^^^^^^\n- Remove limit for ``dnspython`` after eventlet got fixed (#29004)\n- Limit ``dnspython`` to < ``2.3.0`` until eventlet incompatibility is solved (#28962)\n- Add automated version replacement in example dag indexes (#28090)\n- Cleanup and do housekeeping with plugin examples (#28537)\n- Limit ``SQLAlchemy`` to below ``2.0`` (#28725)\n- Bump ``json5`` from ``1.0.1`` to ``1.0.2`` in ``/airflow/www`` (#28715)\n- Fix some docs on using sensors with taskflow (#28708)\n- Change Architecture and OperatingSystem classes into ``Enums`` (#28627)\n- Add doc-strings and small improvement to email util (#28634)\n- Fix ``Connection.get_extra`` type (#28594)\n- navbar, cap dropdown size, and add scroll bar (#28561)\n- Emit warnings for ``conf.get*`` from the right source location (#28543)\n- Move MyPY plugins of ours to dev folder (#28498)\n- Add retry to ``purge_inactive_dag_warnings`` (#28481)\n- Re-enable Plyvel on ARM as it now builds cleanly (#28443)\n- Add SIGUSR2 handler for LocalTaskJob and workers to aid debugging (#28309)\n- Convert ``test_task_command`` to Pytest and ``unquarantine`` tests in it (#28247)\n- Make invalid characters exception more readable (#28181)\n- Bump decode-uri-component from ``0.2.0`` to ``0.2.2`` in ``/airflow/www`` (#28080)\n- Use asserts instead of exceptions for executor not started (#28019)\n- Simplify dataset ``subgraph`` logic (#27987)\n- Order TIs by ``map_index`` (#27904)\n- Additional info about Segmentation Fault in ``LocalTaskJob`` (#27381)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n- Mention mapped operator in cluster policy doc (#28885)\n- Slightly improve description of Dynamic DAG generation preamble (#28650)\n- Restructure Docs  (#27235)\n- Update scheduler docs about low priority tasks (#28831)\n- Clarify that versioned constraints are fixed at release time (#28762)\n- Clarify about docker compose (#28729)\n- Adding an example dag for dynamic task mapping (#28325)\n- Use docker compose v2 command (#28605)\n- Add AIRFLOW_PROJ_DIR to docker-compose example (#28517)\n- Remove outdated Optional Provider Feature outdated documentation (#28506)\n- Add documentation for [core] mp_start_method config (#27993)\n- Documentation for the LocalTaskJob return code counter (#27972)\n- Note which versions of Python are supported (#27798)\n\n\nAirflow 2.5.0 (2022-12-02)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\n``airflow dags test`` no longer performs a backfill job (#26400)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIn order to make ``airflow dags test`` more useful as a testing and debugging tool, we no\nlonger run a backfill job and instead run a \"local task runner\". Users can still backfill\ntheir DAGs using the ``airflow dags backfill`` command.\n\nAirflow config section ``kubernetes`` renamed to ``kubernetes_executor`` (#26873)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nKubernetesPodOperator no longer considers any core kubernetes config params, so this section now only applies to kubernetes executor. Renaming it reduces potential for confusion.\n\n``AirflowException`` is now thrown as soon as any dependent tasks of ExternalTaskSensor fails (#27190)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n``ExternalTaskSensor`` no longer hangs indefinitely when ``failed_states`` is set, an ``execute_date_fn`` is used, and some but not all of the dependent tasks fail.\nInstead, an ``AirflowException`` is thrown as soon as any of the dependent tasks fail.\nAny code handling this failure in addition to timeouts should move to caching the ``AirflowException`` ``BaseClass`` and not only the ``AirflowSensorTimeout`` subclass.\n\nThe Airflow config option ``scheduler.deactivate_stale_dags_interval`` has been renamed to ``scheduler.parsing_cleanup_interval`` (#27828).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe old option will continue to work but will issue deprecation warnings, and will be removed entirely in Airflow 3.\n\nNew Features\n^^^^^^^^^^^^\n- ``TaskRunner``: notify of component start and finish (#27855)\n- Add DagRun state change to the Listener plugin system(#27113)\n- Metric for raw task return codes (#27155)\n- Add logic for XComArg to pull specific map indexes (#27771)\n- Clear TaskGroup (#26658, #28003)\n- Add critical section query duration metric (#27700)\n- Add: #23880 :: Audit log for ``AirflowModelViews(Variables/Connection)`` (#24079, #27994, #27923)\n- Add postgres 15 support (#27444)\n- Expand tasks in mapped group at run time (#27491)\n- reset commits, clean submodules (#27560)\n- scheduler_job, add metric for scheduler loop timer (#27605)\n- Allow datasets to be used in taskflow (#27540)\n- Add expanded_ti_count to ti context (#27680)\n- Add user comment to task instance and dag run (#26457, #27849, #27867)\n- Enable copying DagRun JSON to clipboard (#27639)\n- Implement extra controls for SLAs (#27557)\n- add dag parsed time in DAG view (#27573)\n- Add max_wait for exponential_backoff in BaseSensor (#27597)\n- Expand tasks in mapped group at parse time (#27158)\n- Add disable retry flag on backfill (#23829)\n- Adding sensor decorator (#22562)\n- Api endpoint update ti (#26165)\n- Filtering datasets by recent update events (#26942)\n- Support ``Is /not`` Null filter for value is None on ``webui`` (#26584)\n- Add search to datasets list (#26893)\n- Split out and handle 'params' in mapped operator (#26100)\n- Add authoring API for TaskGroup mapping (#26844)\n- Add ``one_done`` trigger rule (#26146)\n- Create a more efficient  airflow dag test command that also has better local logging (#26400)\n- Support add/remove permissions to roles commands (#26338)\n- Auto tail file logs in Web UI (#26169)\n- Add triggerer info to task instance in API (#26249)\n- Flag to deserialize value on custom XCom backend (#26343)\n\nImprovements\n^^^^^^^^^^^^\n- Allow depth-first execution (#27827)\n- UI: Update offset height if data changes (#27865)\n- Improve TriggerRuleDep typing and readability (#27810)\n- Make views requiring session, keyword only args (#27790)\n- Optimize ``TI.xcom_pull()`` with explicit task_ids and map_indexes (#27699)\n- Allow hyphens in pod id used by k8s executor (#27737)\n- optimise task instances filtering (#27102)\n- Use context managers to simplify log serve management (#27756)\n- Fix formatting leftovers (#27750)\n- Improve task deadlock messaging (#27734)\n- Improve \"sensor timeout\" messaging (#27733)\n- Replace urlparse with ``urlsplit`` (#27389)\n- Align TaskGroup semantics to AbstractOperator (#27723)\n- Add new files to parsing queue on every loop of dag processing (#27060)\n- Make Kubernetes Executor & Scheduler resilient to error during PMH execution (#27611)\n- Separate dataset deps into individual graphs (#27356)\n- Use log.exception where more economical than log.error (#27517)\n- Move validation ``branch_task_ids`` into ``SkipMixin`` (#27434)\n- Coerce LazyXComAccess to list when pushed to XCom (#27251)\n- Update cluster-policies.rst docs (#27362)\n- Add warning if connection type already registered within the provider (#27520)\n- Activate debug logging in commands with --verbose option (#27447)\n- Add classic examples for Python Operators (#27403)\n- change ``.first()`` to ``.scalar()`` (#27323)\n- Improve reset_dag_run description (#26755)\n- Add examples and ``howtos`` about sensors (#27333)\n- Make grid view widths adjustable (#27273)\n- Sorting plugins custom menu links by category before name (#27152)\n- Simplify DagRun.verify_integrity (#26894)\n- Add mapped task group info to serialization (#27027)\n- Correct the JSON style used for Run config in Grid View (#27119)\n- No ``extra__conn_type__`` prefix required for UI behaviors (#26995)\n- Improve dataset update blurb (#26878)\n- Rename kubernetes config section to kubernetes_executor (#26873)\n- decode params for dataset searches (#26941)\n- Get rid of the DAGRun details page & rely completely on Grid (#26837)\n- Fix scheduler ``crashloopbackoff`` when using ``hostname_callable`` (#24999)\n- Reduce log verbosity in KubernetesExecutor. (#26582)\n- Don't iterate tis list twice for no reason (#26740)\n- Clearer code for PodGenerator.deserialize_model_file (#26641)\n- Don't import kubernetes unless you have a V1Pod (#26496)\n- Add updated_at column to DagRun and Ti tables (#26252)\n- Move the deserialization of custom XCom Backend to 2.4.0 (#26392)\n- Avoid calculating all elements when one item is needed (#26377)\n- Add ``__future__``.annotations automatically by isort (#26383)\n- Handle list when serializing expand_kwargs (#26369)\n- Apply PEP-563 (Postponed Evaluation of Annotations) to core airflow (#26290)\n- Add more weekday operator and sensor examples #26071 (#26098)\n- Align TaskGroup semantics to AbstractOperator (#27723)\n\nBug Fixes\n^^^^^^^^^\n- Gracefully handle whole config sections being renamed (#28008)\n- Add allow list for imports during deserialization (#27887)\n- Soft delete datasets that are no longer referenced in DAG schedules or task outlets (#27828)\n- Redirect to home view when there are no valid tags in the URL (#25715)\n- Refresh next run datasets info in dags view (#27839)\n- Make MappedTaskGroup depend on its expand inputs (#27876)\n- Make DagRun state updates for paused DAGs faster (#27725)\n- Don't explicitly set include_examples to False on task run command (#27813)\n- Fix menu border color (#27789)\n- Fix  backfill  queued  task getting reset to scheduled state.  (#23720)\n- Fix clearing child dag mapped tasks from parent dag (#27501)\n- Handle json encoding of ``V1Pod`` in task callback (#27609)\n- Fix ExternalTaskSensor can't check zipped dag (#27056)\n- Avoid re-fetching DAG run in TriggerDagRunOperator (#27635)\n- Continue on exception when retrieving metadata (#27665)\n- External task sensor fail fix (#27190)\n- Add the default None when pop actions (#27537)\n- Display parameter values from serialized dag in trigger dag view. (#27482, #27944)\n- Move TriggerDagRun conf check to execute (#27035)\n- Resolve trigger assignment race condition (#27072)\n- Update google_analytics.html (#27226)\n- Fix some bug in web ui dags list page (auto-refresh & jump search null state) (#27141)\n- Fixed broken URL for docker-compose.yaml (#26721)\n- Fix xcom arg.py .zip bug (#26636)\n- Fix 404 ``taskInstance`` errors and split into two tables (#26575)\n- Fix browser warning of improper thread usage (#26551)\n- template rendering issue fix (#26390)\n- Clear ``autoregistered`` DAGs if there are any import errors (#26398)\n- Fix ``from airflow import version`` lazy import (#26239)\n- allow scroll in triggered dag runs modal (#27965)\n\nMisc/Internal\n^^^^^^^^^^^^^\n- Remove ``is_mapped`` attribute (#27881)\n- Simplify FAB table resetting (#27869)\n- Fix old-style typing in Base Sensor (#27871)\n- Switch (back) to late imports (#27730)\n- Completed D400 for multiple folders (#27748)\n- simplify notes accordion test (#27757)\n- completed D400 for ``airflow/callbacks/* airflow/cli/*`` (#27721)\n- Completed D400 for ``airflow/api_connexion/* directory`` (#27718)\n- Completed D400 for ``airflow/listener/* directory`` (#27731)\n- Completed D400 for ``airflow/lineage/* directory`` (#27732)\n- Update API & Python Client versions (#27642)\n- Completed D400 & D401 for ``airflow/api/*`` directory (#27716)\n- Completed D400 for multiple folders (#27722)\n- Bump ``minimatch`` from ``3.0.4 to 3.0.8`` in ``/airflow/www`` (#27688)\n- Bump loader-utils from ``1.4.1 to 1.4.2 ``in ``/airflow/www`` (#27697)\n- Disable nested task mapping for now (#27681)\n- bump alembic minimum version (#27629)\n- remove unused code.html (#27585)\n- Enable python string normalization everywhere (#27588)\n- Upgrade dependencies in order to avoid backtracking (#27531)\n- Strengthen a bit and clarify importance of triaging issues (#27262)\n- Deduplicate type hints (#27508)\n- Add stub 'yield' to ``BaseTrigger.run`` (#27416)\n- Remove upper-bound limit to dask (#27415)\n- Limit Dask to under ``2022.10.1`` (#27383)\n- Update old style typing (#26872)\n- Enable string normalization for docs (#27269)\n- Slightly faster up/downgrade tests (#26939)\n- Deprecate use of core get_kube_client in PodManager (#26848)\n- Add ``memray`` files to ``gitignore / dockerignore`` (#27001)\n- Bump sphinx and ``sphinx-autoapi`` (#26743)\n- Simplify ``RTIF.delete_old_records()`` (#26667)\n- migrate last react files to typescript (#26112)\n- Work around ``pyupgrade`` edge cases (#26384)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n- Document dag_file_processor_timeouts metric as deprecated (#27067)\n- Drop support for PostgreSQL 10 (#27594)\n- Update index.rst (#27529)\n- Add note about pushing the lazy XCom proxy to XCom (#27250)\n- Fix BaseOperator link (#27441)\n- [docs] best-practices add use variable with template example. (#27316)\n- docs for custom view using plugin (#27244)\n- Update graph view and grid view on overview page (#26909)\n- Documentation fixes (#26819)\n- make consistency on markup title string level (#26696)\n- Add documentation to dag test function (#26713)\n- Fix broken URL for ``docker-compose.yaml`` (#26726)\n- Add a note against use of top level code in timetable (#26649)\n- Fix example_datasets dag names (#26495)\n- Update docs: zip-like effect is now possible in task mapping (#26435)\n- changing to task decorator in docs from classic operator use (#25711)\n\nAirflow 2.4.3 (2022-11-14)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nMake ``RotatingFilehandler`` used in ``DagProcessor`` non-caching (#27223)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIn case you want to decrease cache memory when ``CONFIG_PROCESSOR_MANAGER_LOGGER=True``, and you have your local settings created before,\nyou can update ``processor_manager_handler`` to use ``airflow.utils.log.non_caching_file_handler.NonCachingRotatingFileHandler`` handler instead of ``logging.RotatingFileHandler``.\n\nBug Fixes\n^^^^^^^^^\n- Fix double logging with some task logging handler (#27591)\n- Replace FAB url filtering function with Airflow's (#27576)\n- Fix mini scheduler expansion of mapped task  (#27506)\n- ``SLAMiss`` is nullable and not always given back when pulling task instances (#27423)\n- Fix behavior of ``_`` when searching for DAGs (#27448)\n- Fix getting the ``dag/task`` ids from BaseExecutor (#27550)\n- Fix SQLAlchemy primary key black-out error on DDRQ (#27538)\n- Fix IntegrityError during webserver startup (#27297)\n- Add case insensitive constraint to username (#27266)\n- Fix python external template keys (#27256)\n- Reduce extraneous task log requests (#27233)\n- Make ``RotatingFilehandler`` used in ``DagProcessor`` non-caching (#27223)\n- Listener: Set task on SQLAlchemy TaskInstance object (#27167)\n- Fix dags list page auto-refresh & jump search null state (#27141)\n- Set ``executor.job_id`` to ``BackfillJob.id`` for backfills (#27020)\n\nMisc/Internal\n^^^^^^^^^^^^^\n- Bump loader-utils from ``1.4.0`` to ``1.4.1`` in ``/airflow/www`` (#27552)\n- Reduce log level for k8s ``TCP_KEEPALIVE`` etc warnings (#26981)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n- Use correct executable in docker compose docs (#27529)\n- Fix wording in DAG Runs description (#27470)\n- Document that ``KubernetesExecutor`` overwrites container args (#27450)\n- Fix ``BaseOperator`` links (#27441)\n- Correct timer units to seconds from milliseconds. (#27360)\n- Add missed import in the Trigger Rules example (#27309)\n- Update SLA wording to reflect it is relative to ``Dag Run`` start. (#27111)\n- Add ``kerberos`` environment variables to the docs (#27028)\n\nAirflow 2.4.2 (2022-10-23)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nDefault for ``[webserver] expose_stacktrace`` changed to ``False`` (#27059)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe default for ``[webserver] expose_stacktrace`` has been set to ``False``, instead of ``True``. This means administrators must opt-in to expose tracebacks to end users.\n\nBug Fixes\n^^^^^^^^^\n- Make tracebacks opt-in (#27059)\n- Add missing AUTOINC/SERIAL for FAB tables (#26885)\n- Add separate error handler for 405(Method not allowed) errors (#26880)\n- Don't re-patch pods that are already controlled by current worker (#26778)\n- Handle mapped tasks in task duration chart (#26722)\n- Fix task duration cumulative chart (#26717)\n- Avoid 500 on dag redirect (#27064)\n- Filter dataset dependency data on webserver (#27046)\n- Remove double collection of dags in ``airflow dags reserialize``  (#27030)\n- Fix auto refresh for graph view (#26926)\n- Don't overwrite connection extra with invalid json (#27142)\n- Fix next run dataset modal links (#26897)\n- Change dag audit log sort by date from asc to desc (#26895)\n- Bump min version of jinja2 (#26866)\n- Add missing colors to ``state_color_mapping`` jinja global (#26822)\n- Fix running debuggers inside ``airflow tasks test`` (#26806)\n- Fix warning when using xcomarg dependencies (#26801)\n- demote Removed state in priority for displaying task summaries (#26789)\n- Ensure the log messages from operators during parsing go somewhere (#26779)\n- Add restarting state to TaskState Enum in REST API (#26776)\n- Allow retrieving error message from data.detail (#26762)\n- Simplify origin string cleaning (#27143)\n- Remove DAG parsing from StandardTaskRunner (#26750)\n- Fix non-hidden cumulative chart on duration view (#26716)\n- Remove TaskFail duplicates check (#26714)\n- Fix airflow tasks run --local when dags_folder differs from that of processor (#26509)\n- Fix yarn warning from d3-color (#27139)\n- Fix version for a couple configurations (#26491)\n- Revert \"No grid auto-refresh for backfill dag runs (#25042)\" (#26463)\n- Retry on Airflow Schedule DAG Run DB Deadlock (#26347)\n\nMisc/Internal\n^^^^^^^^^^^^^\n- Clean-ups around task-mapping code (#26879)\n- Move user-facing string to template (#26815)\n- add icon legend to datasets graph (#26781)\n- Bump ``sphinx`` and ``sphinx-autoapi`` (#26743)\n- Simplify ``RTIF.delete_old_records()`` (#26667)\n- Bump FAB to ``4.1.4`` (#26393)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n- Fixed triple quotes in task group example (#26829)\n- Documentation fixes (#26819)\n- make consistency on markup title string level (#26696)\n- Add a note against use of top level code in timetable (#26649)\n- Fix broken URL for ``docker-compose.yaml`` (#26726)\n\n\nAirflow 2.4.1 (2022-09-30)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n^^^^^^^^^\n\n- When rendering template, unmap task in context (#26702)\n- Fix scroll overflow for ConfirmDialog (#26681)\n- Resolve deprecation warning re ``Table.exists()`` (#26616)\n- Fix XComArg zip bug (#26636)\n- Use COALESCE when ordering runs to handle NULL (#26626)\n- Check user is active (#26635)\n- No missing user warning for public admin (#26611)\n- Allow MapXComArg to resolve after serialization  (#26591)\n- Resolve warning about DISTINCT ON query on dags view (#26608)\n- Log warning when secret backend kwargs is invalid (#26580)\n- Fix grid view log try numbers (#26556)\n- Template rendering issue in passing ``templates_dict`` to task decorator (#26390)\n- Fix Deferrable stuck as ``scheduled`` during backfill (#26205)\n- Suppress SQLALCHEMY_TRACK_MODIFICATIONS warning in db init (#26617)\n- Correctly set ``json_provider_class`` on Flask app so it uses our encoder (#26554)\n- Fix WSGI root app (#26549)\n- Fix deadlock when mapped task with removed upstream is rerun (#26518)\n- ExecutorConfigType should be ``cacheable`` (#26498)\n- Fix proper joining of the path for logs retrieved from celery workers (#26493)\n- DAG Deps extends ``base_template`` (#26439)\n- Don't update backfill run from the scheduler (#26342)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Clarify owner links document (#26515)\n- Fix invalid RST in dataset concepts doc (#26434)\n- Document the ``non-sensitive-only`` option for ``expose_config`` (#26507)\n- Fix ``example_datasets`` dag names (#26495)\n- Zip-like effect is now possible in task mapping (#26435)\n- Use task decorator in docs instead of classic operators (#25711)\n\nAirflow 2.4.0 (2022-09-19)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nData-aware Scheduling and ``Dataset`` concept added to Airflow\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nNew to this release of Airflow is the concept of Datasets to Airflow, and with it a new way of scheduling dags:\ndata-aware scheduling.\n\nThis allows DAG runs to be automatically created as a result of a task \"producing\" a dataset. In some ways\nthis can be thought of as the inverse of ``TriggerDagRunOperator``, where instead of the producing DAG\ncontrolling which DAGs get created, the consuming DAGs can \"listen\" for changes.\n\nA dataset is identified by a URI:\n\n.. code-block:: python\n\n    from airflow import Dataset\n\n    # The URI doesn't have to be absolute\n    dataset = Dataset(uri=\"my-dataset\")\n    # Or you can use a scheme to show where it lives.\n    dataset2 = Dataset(uri=\"s3://bucket/prefix\")\n\nTo create a DAG that runs whenever a Dataset is updated use the new ``schedule`` parameter (see below) and\npass a list of 1 or more Datasets:\n\n..  code-block:: python\n\n    with DAG(dag_id='dataset-consumer', schedule=[dataset]):\n        ...\n\nAnd to mark a task as producing a dataset pass the dataset(s) to the ``outlets`` attribute:\n\n.. code-block:: python\n\n    @task(outlets=[dataset])\n    def my_task(): ...\n\n\n    # Or for classic operators\n    BashOperator(task_id=\"update-ds\", bash_command=..., outlets=[dataset])\n\nIf you have the producer and consumer in different files you do not need to use the same Dataset object, two\n``Dataset()``\\s created with the same URI are equal.\n\nDatasets represent the abstract concept of a dataset, and (for now) do not have any direct read or write\ncapability - in this release we are adding the foundational feature that we will build upon.\n\nFor more info on Datasets please see `Datasets documentation <https://airflow.apache.org/docs/apache-airflow/2.4.0/authoring-and-scheduling/datasets.html>`_.\n\nExpanded dynamic task mapping support\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nDynamic task mapping now includes support for ``expand_kwargs``, ``zip`` and ``map``.\n\nFor more info on dynamic task mapping please see :doc:`/authoring-and-scheduling/dynamic-task-mapping`.\n\nDAGS used in a context manager no longer need to be assigned to a module variable (#23592)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPreviously you had to assign a DAG to a module-level variable in order for Airflow to pick it up. For example this\n\n\n.. code-block:: python\n\n   with DAG(dag_id=\"example\") as dag:\n       ...\n\n\n   @dag\n   def dag_maker(): ...\n\n\n   dag2 = dag_maker()\n\n\ncan become\n\n.. code-block:: python\n\n   with DAG(dag_id=\"example\"):\n       ...\n\n\n   @dag\n   def dag_maker(): ...\n\n\n   dag_maker()\n\nIf you want to disable the behaviour for any reason then set ``auto_register=False`` on the dag:\n\n.. code-block:: python\n\n   # This dag will not be picked up by Airflow as it's not assigned to a variable\n   with DAG(dag_id=\"example\", auto_register=False):\n       ...\n\nDeprecation of ``schedule_interval`` and ``timetable`` arguments (#25410)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWe added new DAG argument ``schedule`` that can accept a cron expression, timedelta object, *timetable* object, or list of dataset objects. Arguments ``schedule_interval`` and ``timetable`` are deprecated.\n\nIf you previously used the ``@daily`` cron preset, your DAG may have looked like this:\n\n.. code-block:: python\n\n    with DAG(\n        dag_id=\"my_example\",\n        start_date=datetime(2021, 1, 1),\n        schedule_interval=\"@daily\",\n    ):\n        ...\n\nGoing forward, you should use the ``schedule`` argument instead:\n\n.. code-block:: python\n\n    with DAG(\n        dag_id=\"my_example\",\n        start_date=datetime(2021, 1, 1),\n        schedule=\"@daily\",\n    ):\n        ...\n\nThe same is true if you used a custom timetable.  Previously you would have used the ``timetable`` argument:\n\n.. code-block:: python\n\n    with DAG(\n        dag_id=\"my_example\",\n        start_date=datetime(2021, 1, 1),\n        timetable=EventsTimetable(event_dates=[pendulum.datetime(2022, 4, 5)]),\n    ):\n        ...\n\nNow you should use the ``schedule`` argument:\n\n.. code-block:: python\n\n    with DAG(\n        dag_id=\"my_example\",\n        start_date=datetime(2021, 1, 1),\n        schedule=EventsTimetable(event_dates=[pendulum.datetime(2022, 4, 5)]),\n    ):\n        ...\n\nRemoval of experimental Smart Sensors (#25507)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nSmart Sensors were added in 2.0 and deprecated in favor of Deferrable operators in 2.2, and have now been removed.\n\n``airflow.contrib`` packages and deprecated modules are dynamically generated (#26153, #26179, #26167)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe ``airflow.contrib`` packages and deprecated modules from Airflow 1.10 in ``airflow.hooks``, ``airflow.operators``, ``airflow.sensors`` packages are now dynamically generated modules and while users can continue using the deprecated contrib classes, they are no longer visible for static code check tools and will be reported as missing. It is recommended for the users to move to the non-deprecated classes.\n\n``DBApiHook`` and ``SQLSensor`` have moved (#24836)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n``DBApiHook`` and ``SQLSensor`` have been moved to the ``apache-airflow-providers-common-sql`` provider.\n\nDAG runs sorting logic changed in grid view (#25090)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe ordering of DAG runs in the grid view has been changed to be more \"natural\".\nThe new logic generally orders by data interval, but a custom ordering can be\napplied by setting the DAG to use a custom timetable.\n\n\nNew Features\n^^^^^^^^^^^^\n- Add Data-aware Scheduling (`AIP-48 <https://github.com/apache/airflow/pulls?q=is%3Apr+is%3Amerged+label%3AAIP-48+milestone%3A%22Airflow+2.4.0%22>`_)\n- Add ``@task.short_circuit`` TaskFlow decorator (#25752)\n- Make ``execution_date_or_run_id`` optional in ``tasks test`` command (#26114)\n- Automatically register DAGs that are used in a context manager (#23592, #26398)\n- Add option of sending DAG parser logs to stdout. (#25754)\n- Support multiple ``DagProcessors`` parsing files from different locations. (#25935)\n- Implement ``ExternalPythonOperator`` (#25780)\n- Make execution_date optional for command ``dags test`` (#26111)\n- Implement ``expand_kwargs()`` against a literal list (#25925)\n- Add trigger rule tooltip (#26043)\n- Add conf parameter to CLI for airflow dags test (#25900)\n- Include scheduled slots in pools view (#26006)\n- Add ``output`` property to ``MappedOperator`` (#25604)\n- Add roles delete command to cli (#25854)\n- Add Airflow specific warning classes (#25799)\n- Add support for ``TaskGroup`` in ``ExternalTaskSensor`` (#24902)\n- Add ``@task.kubernetes`` taskflow decorator (#25663)\n- Add a way to import Airflow without side-effects (#25832)\n- Let timetables control generated run_ids. (#25795)\n- Allow per-timetable ordering override in grid view (#25633)\n- Grid logs for mapped instances (#25610, #25621, #25611)\n- Consolidate to one ``schedule`` param (#25410)\n- DAG regex flag in backfill command (#23870)\n- Adding support for owner links in the Dags view UI (#25280)\n- Ability to clear a specific DAG Run's task instances via REST API (#23516)\n- Possibility to document DAG with a separate markdown file (#25509)\n- Add parsing context to DAG Parsing (#25161)\n- Implement ``CronTriggerTimetable`` (#23662)\n- Add option to mask sensitive data in UI configuration page (#25346)\n- Create new databases from the ORM (#24156)\n- Implement ``XComArg.zip(*xcom_args)`` (#25176)\n- Introduce ``sla_miss`` metric (#23402)\n- Implement ``map()`` semantic (#25085)\n- Add override method to TaskGroupDecorator (#25160)\n- Implement ``expand_kwargs()`` (#24989)\n- Add parameter to turn off SQL query logging (#24570)\n- Add ``DagWarning`` model, and a check for missing pools (#23317)\n- Add Task Logs to Grid details panel (#24249)\n- Added small health check server and endpoint in scheduler(#23905)\n- Add built-in External Link for ``ExternalTaskMarker`` operator (#23964)\n- Add default task retry delay config (#23861)\n- Add clear DagRun endpoint. (#23451)\n- Add support for timezone as string in cron interval timetable (#23279)\n- Add auto-refresh to dags home page (#22900, #24770)\n\nImprovements\n^^^^^^^^^^^^\n\n- Add more weekday operator and sensor examples #26071 (#26098)\n- Add subdir parameter to dags reserialize command (#26170)\n- Update zombie message to be more descriptive (#26141)\n- Only send an ``SlaCallbackRequest`` if the DAG is scheduled (#26089)\n- Promote ``Operator.output`` more (#25617)\n- Upgrade API files to typescript (#25098)\n- Less ``hacky`` double-rendering prevention in mapped task (#25924)\n- Improve Audit log (#25856)\n- Remove mapped operator validation code (#25870)\n- More ``DAG(schedule=...)`` improvements (#25648)\n- Reduce ``operator_name`` dupe in serialized JSON (#25819)\n- Make grid view group/mapped summary UI more consistent (#25723)\n- Remove useless statement in ``task_group_to_grid`` (#25654)\n- Add optional data interval to ``CronTriggerTimetable`` (#25503)\n- Remove unused code in ``/grid`` endpoint (#25481)\n- Add and document description fields (#25370)\n- Improve Airflow logging for operator Jinja template processing (#25452)\n- Update core example DAGs to use ``@task.branch`` decorator (#25242)\n- Update DAG ``audit_log`` route (#25415)\n- Change stdout and stderr access mode to append in commands (#25253)\n- Remove ``getTasks`` from Grid view (#25359)\n- Improve taskflow type hints with ParamSpec (#25173)\n- Use tables in grid details panes (#25258)\n- Explicitly list ``@dag`` arguments (#25044)\n- More typing in ``SchedulerJob`` and ``TaskInstance`` (#24912)\n- Patch ``getfqdn`` with more resilient version (#24981)\n- Replace all ``NBSP`` characters by ``whitespaces`` (#24797)\n- Re-serialize all DAGs on ``airflow db upgrade`` (#24518)\n- Rework contract of try_adopt_task_instances method (#23188)\n- Make ``expand()`` error vague so it's not misleading (#24018)\n- Add enum validation for ``[webserver]analytics_tool`` (#24032)\n- Add ``dttm`` searchable field in audit log (#23794)\n- Allow more parameters to be piped through via ``execute_in_subprocess`` (#23286)\n- Use ``func.count`` to count rows (#23657)\n- Remove stale serialized dags (#22917)\n- AIP45 Remove dag parsing in airflow run local (#21877)\n- Add support for queued state in DagRun update endpoint. (#23481)\n- Add fields to dagrun endpoint (#23440)\n- Use ``sql_alchemy_conn`` for celery result backend when ``result_backend`` is not set (#24496)\n\nBug Fixes\n^^^^^^^^^\n\n- Have consistent types between the ORM and the migration files (#24044, #25869)\n- Disallow any dag tags longer than 100 char (#25196)\n- Add the dag_id to ``AirflowDagCycleException`` message (#26204)\n- Properly build URL to retrieve logs independently from system (#26337)\n- For worker log servers only bind to IPV6 when dual stack is available (#26222)\n- Fix ``TaskInstance.task`` not defined before ``handle_failure`` (#26040)\n- Undo secrets backend config caching (#26223)\n- Fix faulty executor config serialization logic (#26191)\n- Show ``DAGs`` and ``Datasets`` menu links based on role permission (#26183)\n- Allow setting ``TaskGroup`` tooltip via function docstring (#26028)\n- Fix RecursionError on graph view of a DAG with many tasks (#26175)\n- Fix backfill occasional deadlocking (#26161)\n- Fix ``DagRun.start_date`` not set during backfill with ``--reset-dagruns`` True (#26135)\n- Use label instead of id for dynamic task labels in graph (#26108)\n- Don't fail DagRun when leaf ``mapped_task`` is SKIPPED (#25995)\n- Add group prefix to decorated mapped task (#26081)\n- Fix UI flash when triggering with dup logical date (#26094)\n- Fix Make items nullable for ``TaskInstance`` related endpoints to avoid API errors (#26076)\n- Fix ``BranchDateTimeOperator`` to be ``timezone-awreness-insensitive`` (#25944)\n- Fix legacy timetable schedule interval params (#25999)\n- Fix response schema for ``list-mapped-task-instance`` (#25965)\n- Properly check the existence of missing mapped TIs (#25788)\n- Fix broken auto-refresh on grid view (#25950)\n- Use per-timetable ordering in grid UI (#25880)\n- Rewrite recursion when parsing DAG into iteration (#25898)\n- Find cross-group tasks in ``iter_mapped_dependants`` (#25793)\n- Fail task if mapping upstream fails (#25757)\n- Support ``/`` in variable get endpoint (#25774)\n- Use cfg default_wrap value for grid logs (#25731)\n- Add origin request args when triggering a run (#25729)\n- Operator name separate from class (#22834)\n- Fix incorrect data interval alignment due to assumption on input time alignment (#22658)\n- Return None if an ``XComArg`` fails to resolve (#25661)\n- Correct ``json`` arg help in ``airflow variables set`` command (#25726)\n- Added MySQL index hint to use ``ti_state`` on ``find_zombies`` query (#25725)\n- Only excluded actually expanded fields from render (#25599)\n- Grid, fix toast for ``axios`` errors (#25703)\n- Fix UI redirect (#26409)\n- Require dag_id arg for dags list-runs (#26357)\n- Check for queued states for dags auto-refresh (#25695)\n- Fix upgrade code for the ``dag_owner_attributes`` table (#25579)\n- Add map index to task logs api (#25568)\n- Ensure that zombie tasks for dags with errors get cleaned up (#25550)\n- Make extra link work in UI (#25500)\n- Sync up plugin API schema and definition (#25524)\n- First/last names can be empty (#25476)\n- Refactor DAG pages to be consistent (#25402)\n- Check ``expand_kwargs()`` input type before unmapping (#25355)\n- Filter XCOM by key when calculating map lengths (#24530)\n- Fix ``ExternalTaskSensor`` not working with dynamic task (#25215)\n- Added exception catching to send default email if template file raises any exception (#24943)\n- Bring ``MappedOperator`` members in sync with ``BaseOperator`` (#24034)\n\n\nMisc/Internal\n^^^^^^^^^^^^^\n\n- Add automatically generated ``ERD`` schema for the ``MetaData`` DB (#26217)\n- Mark serialization functions as internal (#26193)\n- Remove remaining deprecated classes and replace them with ``PEP562`` (#26167)\n- Move ``dag_edges`` and ``task_group_to_dict`` to corresponding util modules (#26212)\n- Lazily import many modules to improve import speed (#24486, #26239)\n- FIX Incorrect typing information (#26077)\n- Add missing contrib classes to deprecated dictionaries (#26179)\n- Re-configure/connect the ``ORM`` after forking to run a DAG processor (#26216)\n- Remove cattrs from lineage processing. (#26134)\n- Removed deprecated contrib files and replace them with ``PEP-562`` getattr (#26153)\n- Make ``BaseSerialization.serialize`` \"public\" to other classes. (#26142)\n- Change the template to use human readable task_instance description (#25960)\n- Bump ``moment-timezone`` from ``0.5.34`` to ``0.5.35`` in ``/airflow/www`` (#26080)\n- Fix Flask deprecation warning (#25753)\n- Add ``CamelCase`` to generated operations types (#25887)\n- Fix migration issues and tighten the CI upgrade/downgrade test (#25869)\n- Fix type annotations in ``SkipMixin`` (#25864)\n- Workaround setuptools editable packages path issue (#25848)\n- Bump ``undici`` from ``5.8.0 to 5.9.1`` in /airflow/www (#25801)\n- Add custom_operator_name attr to ``_BranchPythonDecoratedOperator`` (#25783)\n- Clarify ``filename_template`` deprecation message (#25749)\n- Use ``ParamSpec`` to replace ``...`` in Callable (#25658)\n- Remove deprecated modules (#25543)\n- Documentation on task mapping additions (#24489)\n- Remove Smart Sensors (#25507)\n- Fix ``elasticsearch`` test config to avoid warning on deprecated template (#25520)\n- Bump ``terser`` from ``4.8.0 to 4.8.1`` in /airflow/ui (#25178)\n- Generate ``typescript`` types from rest ``API`` docs (#25123)\n- Upgrade utils files to ``typescript`` (#25089)\n- Upgrade remaining context file to ``typescript``. (#25096)\n- Migrate files to ``ts`` (#25267)\n- Upgrade grid Table component to ``ts.`` (#25074)\n- Skip mapping against mapped ``ti`` if it returns None (#25047)\n- Refactor ``js`` file structure (#25003)\n- Move mapped kwargs introspection to separate type (#24971)\n- Only assert stuff for mypy when type checking (#24937)\n- Bump ``moment`` from ``2.29.3 to 2.29.4`` in ``/airflow/www`` (#24885)\n- Remove \"bad characters\" from our codebase (#24841)\n- Remove ``xcom_push`` flag from ``BashOperator`` (#24824)\n- Move Flask hook registration to end of file (#24776)\n- Upgrade more javascript files to ``typescript`` (#24715)\n- Clean up task decorator type hints and docstrings (#24667)\n- Preserve original order of providers' connection extra fields in UI (#24425)\n- Rename ``charts.css`` to ``chart.css`` (#24531)\n- Rename ``grid.css`` to ``chart.css`` (#24529)\n- Misc: create new process group by ``set_new_process_group`` utility (#24371)\n- Airflow UI fix Prototype Pollution (#24201)\n- Bump ``moto`` version (#24222)\n- Remove unused ``[github_enterprise]`` from ref docs (#24033)\n- Clean up ``f-strings`` in logging calls (#23597)\n- Add limit for ``JPype1`` (#23847)\n- Simply json responses (#25518)\n- Add min attrs version (#26408)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n- Add url prefix setting for ``Celery`` Flower (#25986)\n- Updating deprecated configuration in examples (#26037)\n- Fix wrong link for taskflow tutorial (#26007)\n- Reorganize tutorials into a section (#25890)\n- Fix concept doc for dynamic task map (#26002)\n- Update code examples from \"classic\" operators to taskflow (#25845, #25657)\n- Add instructions on manually fixing ``MySQL`` Charset problems (#25938)\n- Prefer the local Quick Start in docs (#25888)\n- Fix broken link to ``Trigger Rules`` (#25840)\n- Improve docker documentation (#25735)\n- Correctly link to Dag parsing context in docs (#25722)\n- Add note on ``task_instance_mutation_hook`` usage (#25607)\n- Note that TaskFlow API automatically passes data between tasks (#25577)\n- Update DAG run to clarify when a DAG actually runs (#25290)\n- Update tutorial docs to include a definition of operators (#25012)\n- Rewrite the Airflow documentation home page (#24795)\n- Fix ``task-generated mapping`` example (#23424)\n- Add note on subtle logical date change in ``2.2.0`` (#24413)\n- Add missing import in best-practices code example (#25391)\n\n\n\nAirflow 2.3.4 (2022-08-23)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nAdded new config ``[logging]log_formatter_class`` to fix timezone display for logs on UI (#24811)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIf you are using a custom Formatter subclass in your ``[logging]logging_config_class``, please inherit from ``airflow.utils.log.timezone_aware.TimezoneAware`` instead of ``logging.Formatter``.\nFor example, in your ``custom_config.py``:\n\n.. code-block:: python\n\n    from airflow.utils.log.timezone_aware import TimezoneAware\n\n\n    # before\n    class YourCustomFormatter(logging.Formatter): ...\n\n\n    # after\n    class YourCustomFormatter(TimezoneAware): ...\n\n\n    AIRFLOW_FORMATTER = LOGGING_CONFIG[\"formatters\"][\"airflow\"]\n    AIRFLOW_FORMATTER[\"class\"] = \"somewhere.your.custom_config.YourCustomFormatter\"\n    # or use TimezoneAware class directly. If you don't have custom Formatter.\n    AIRFLOW_FORMATTER[\"class\"] = \"airflow.utils.log.timezone_aware.TimezoneAware\"\n\nBug Fixes\n^^^^^^^^^\n\n- Disable ``attrs`` state management on ``MappedOperator`` (#24772)\n- Serialize ``pod_override`` to JSON before pickling ``executor_config`` (#24356)\n- Fix ``pid`` check (#24636)\n- Rotate session id during login (#25771)\n- Fix mapped sensor with reschedule mode (#25594)\n- Cache the custom secrets backend so the same instance gets reused (#25556)\n- Add right padding (#25554)\n- Fix reducing mapped length of a mapped task at runtime after a clear (#25531)\n- Fix ``airflow db reset`` when dangling tables exist (#25441)\n- Change ``disable_verify_ssl`` behaviour (#25023)\n- Set default task group in dag.add_task method (#25000)\n- Removed interfering force of index. (#25404)\n- Remove useless logging line (#25347)\n- Adding mysql index hint to use index on ``task_instance.state`` in critical section query (#25673)\n- Configurable umask to all daemonized processes. (#25664)\n- Fix the errors raised when None is passed to template filters (#25593)\n- Allow wildcarded CORS origins (#25553)\n- Fix \"This Session's transaction has been rolled back\" (#25532)\n- Fix Serialization error in ``TaskCallbackRequest`` (#25471)\n- fix - resolve bash by absolute path (#25331)\n- Add ``__repr__`` to ParamsDict class (#25305)\n- Only load distribution of a name once (#25296)\n- convert ``TimeSensorAsync`` ``target_time`` to utc on call time (#25221)\n- call ``updateNodeLabels`` after ``expandGroup`` (#25217)\n- Stop SLA callbacks gazumping other callbacks and DOS'ing the ``DagProcessorManager`` queue (#25147)\n- Fix ``invalidateQueries`` call (#25097)\n- ``airflow/www/package.json``: Add name, version fields. (#25065)\n- No grid auto-refresh for backfill dag runs (#25042)\n- Fix tag link on dag detail page (#24918)\n- Fix zombie task handling with multiple schedulers (#24906)\n- Bind log server on worker to ``IPv6`` address (#24755) (#24846)\n- Add ``%z`` for ``%(asctime)s`` to fix timezone for logs on UI (#24811)\n- ``TriggerDagRunOperator.operator_extra_links`` is attr (#24676)\n- Send DAG timeout callbacks to processor outside of ``prohibit_commit`` (#24366)\n- Don't rely on current ORM structure for db clean command (#23574)\n- Clear next method when clearing TIs (#23929)\n- Two typing fixes (#25690)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Update set-up-database.rst (#24983)\n- Fix syntax in mysql setup documentation (#24893 (#24939)\n- Note how DAG policy works with default_args (#24804)\n- Update PythonVirtualenvOperator Howto (#24782)\n- Doc: Add hyperlinks to Github PRs for Release Notes (#24532)\n\nMisc/Internal\n^^^^^^^^^^^^^\n\n- Remove depreciation warning when use default remote tasks logging handlers (#25764)\n- clearer method name in scheduler_job.py (#23702)\n- Bump cattrs version (#25689)\n- Include missing mention of ``external_executor_id`` in ``sql_engine_collation_for_ids`` docs (#25197)\n- Refactor ``DR.task_instance_scheduling_decisions`` (#24774)\n- Sort operator extra links (#24992)\n- Extends ``resolve_xcom_backend`` function level documentation (#24965)\n- Upgrade FAB to 4.1.3 (#24884)\n- Limit Flask to <2.3 in the wake of 2.2 breaking our tests (#25511)\n- Limit astroid version to < 2.12 (#24982)\n- Move javascript compilation to host (#25169)\n- Bump typing-extensions and mypy for ParamSpec (#25088)\n\n\nAirflow 2.3.3 (2022-07-09)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nWe've upgraded Flask App Builder to a major version 4.* (#24399)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nFlask App Builder is one of the important components of Airflow Webserver, as\nit uses a lot of dependencies that are essential to run the webserver and integrate it\nin enterprise environments - especially authentication.\n\nThe FAB 4.* upgrades a number of dependencies to major releases, which upgrades them to versions\nthat have a number of security issues fixed. A lot of tests were performed to bring the dependencies\nin a backwards-compatible way, however the dependencies themselves implement breaking changes in their\ninternals so it might be that some of those changes might impact the users in case they are using the\nlibraries for their own purposes.\n\nOne important change that you likely will need to apply to Oauth configuration is to add\n``server_metadata_url`` or ``jwks_uri`` and you can read about it more\nin `this issue <https://github.com/dpgaspar/Flask-AppBuilder/issues/1861>`_.\n\nHere is the list of breaking changes in dependencies that comes together with FAB 4:\n\n  * ``Flask`` from 1.X to 2.X `breaking changes <https://flask.palletsprojects.com/en/2.0.x/changes/#version-2-0-0>`__\n\n  * ``flask-jwt-extended`` 3.X to 4.X `breaking changes: <https://flask-jwt-extended.readthedocs.io/en/stable/v4_upgrade_guide/>`__\n\n  * ``Jinja2`` 2.X to 3.X `breaking changes: <https://jinja.palletsprojects.com/en/3.0.x/changes/#version-3-0-0>`__\n\n  * ``Werkzeug`` 1.X to 2.X `breaking changes <https://werkzeug.palletsprojects.com/en/2.0.x/changes/#version-2-0-0>`__\n\n  * ``pyJWT`` 1.X to 2.X `breaking changes: <https://pyjwt.readthedocs.io/en/stable/changelog.html#v2-0-0>`__\n\n  * ``Click`` 7.X to 8.X `breaking changes: <https://click.palletsprojects.com/en/8.0.x/changes/#version-8-0-0>`__\n\n  * ``itsdangerous`` 1.X to 2.X `breaking changes <https://github.com/pallets/itsdangerous/blob/main/CHANGES.rst#version-200>`__\n\nBug Fixes\n^^^^^^^^^\n\n- Fix exception in mini task scheduler (#24865)\n- Fix cycle bug with attaching label to task group (#24847)\n- Fix timestamp defaults for ``sensorinstance`` (#24638)\n- Move fallible ``ti.task.dag`` assignment back inside ``try/except`` block (#24533) (#24592)\n- Add missing types to ``FSHook`` (#24470)\n- Mask secrets in ``stdout`` for ``airflow tasks test`` (#24362)\n- ``DebugExecutor`` use ``ti.run()`` instead of ``ti._run_raw_task`` (#24357)\n- Fix bugs in ``URI`` constructor for ``MySQL`` connection (#24320)\n- Missing ``scheduleinterval`` nullable true added in ``openapi`` (#24253)\n- Unify ``return_code`` interface for task runner (#24093)\n- Handle occasional deadlocks in trigger with retries (#24071)\n- Remove special serde logic for mapped ``op_kwargs`` (#23860)\n- ``ExternalTaskSensor`` respects ``soft_fail`` if the external task enters a ``failed_state`` (#23647)\n- Fix ``StatD`` timing metric units (#21106)\n- Add ``cache_ok`` flag to sqlalchemy TypeDecorators. (#24499)\n- Allow for ``LOGGING_LEVEL=DEBUG`` (#23360)\n- Fix grid date ticks (#24738)\n- Debounce status highlighting in Grid view (#24710)\n- Fix Grid vertical scrolling (#24684)\n- don't try to render child rows for closed groups (#24637)\n- Do not calculate grid root instances (#24528)\n- Maintain grid view selection on filtering upstream (#23779)\n- Speed up ``grid_data`` endpoint by 10x (#24284)\n- Apply per-run log templates to log handlers (#24153)\n- Don't crash scheduler if exec config has old k8s objects (#24117)\n- ``TI.log_url`` fix for ``map_index`` (#24335)\n- Fix migration ``0080_2_0_2`` - Replace null values before setting column not null (#24585)\n- Patch ``sql_alchemy_conn`` if old Postgres schemes used (#24569)\n- Seed ``log_template`` table (#24511)\n- Fix deprecated ``log_id_template`` value (#24506)\n- Fix toast messages (#24505)\n- Add indexes for CASCADE deletes for ``task_instance`` (#24488)\n- Return empty dict if Pod JSON encoding fails (#24478)\n- Improve grid rendering performance with a custom tooltip (#24417, #24449)\n- Check for ``run_id`` for grid group summaries (#24327)\n- Optimize calendar view for cron scheduled DAGs (#24262)\n- Use ``get_hostname`` instead of ``socket.getfqdn`` (#24260)\n- Check that edge nodes actually exist (#24166)\n- Fix ``useTasks`` crash on error (#24152)\n- Do not fail re-queued TIs (#23846)\n- Reduce grid view API calls (#24083)\n- Rename Permissions to Permission Pairs. (#24065)\n- Replace ``use_task_execution_date`` with ``use_task_logical_date`` (#23983)\n- Grid fix details button truncated and small UI tweaks (#23934)\n- Add TaskInstance State ``REMOVED`` to finished states and success states (#23797)\n- Fix mapped task immutability after clear (#23667)\n- Fix permission issue for dag that has dot in name (#23510)\n- Fix closing connection ``dbapi.get_pandas_df`` (#23452)\n- Check bag DAG ``schedule_interval`` match timetable (#23113)\n- Parse error for task added to multiple groups (#23071)\n- Fix flaky order of returned dag runs (#24405)\n- Migrate ``jsx`` files that affect run/task selection to ``tsx`` (#24509)\n- Fix links to sources for examples (#24386)\n- Set proper ``Content-Type`` and ``chartset`` on ``grid_data`` endpoint (#24375)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Update templates doc to mention ``extras`` and format Airflow ``Vars`` / ``Conns`` (#24735)\n- Document built in Timetables (#23099)\n- Alphabetizes two tables (#23923)\n- Clarify that users should not use Maria DB (#24556)\n- Add imports to deferring code samples (#24544)\n- Add note about image regeneration in June 2022 (#24524)\n- Small cleanup of ``get_current_context()`` chapter (#24482)\n- Fix default 2.2.5 ``log_id_template`` (#24455)\n- Update description of installing providers separately from core (#24454)\n- Mention context variables and logging (#24304)\n\nMisc/Internal\n^^^^^^^^^^^^^\n\n- Remove internet explorer support (#24495)\n- Removing magic status code numbers from ``api_connexion`` (#24050)\n- Upgrade FAB to ``4.1.2`` (#24619)\n- Switch Markdown engine to ``markdown-it-py`` (#19702)\n- Update ``rich`` to latest version across the board. (#24186)\n- Get rid of ``TimedJSONWebSignatureSerializer`` (#24519)\n- Update flask-appbuilder ``authlib``/ ``oauth`` dependency (#24516)\n- Upgrade to ``webpack`` 5 (#24485)\n- Add ``typescript`` (#24337)\n- The JWT claims in the request to retrieve logs have been standardized: we use ``nbf`` and ``aud`` claims for\n  maturity and audience of the requests. Also \"filename\" payload field is used to keep log name. (#24519)\n- Address all ``yarn`` test warnings (#24722)\n- Upgrade to react 18 and chakra 2 (#24430)\n- Refactor ``DagRun.verify_integrity`` (#24114)\n- Upgrade FAB to ``4.1.1`` (#24399)\n- We now need at least ``Flask-WTF 0.15`` (#24621)\n\n\nAirflow 2.3.2 (2022-06-04)\n--------------------------\n\nNo significant changes.\n\nBug Fixes\n^^^^^^^^^\n\n- Run the ``check_migration`` loop at least once\n- Fix grid view for mapped tasks (#24059)\n- Icons in grid view for different DAG run types (#23970)\n- Faster grid view (#23951)\n- Disallow calling expand with no arguments (#23463)\n- Add missing ``is_mapped`` field to Task response. (#23319)\n- DagFileProcessorManager: Start a new process group only if current process not a session leader (#23872)\n- Mask sensitive values for not-yet-running TIs (#23807)\n- Add cascade to ``dag_tag`` to ``dag`` foreign key (#23444)\n- Use ``--subdir`` argument value for standalone dag processor. (#23864)\n- Highlight task states by hovering on legend row (#23678)\n- Fix and speed up grid view (#23947)\n- Prevent UI from crashing if grid task instances are null (#23939)\n- Remove redundant register exit signals in ``dag-processor`` command (#23886)\n- Add ``__wrapped__`` property to ``_TaskDecorator`` (#23830)\n- Fix UnboundLocalError when ``sql`` is empty list in DbApiHook (#23816)\n- Enable clicking on DAG owner in autocomplete dropdown (#23804)\n- Simplify flash message for ``_airflow_moved`` tables (#23635)\n- Exclude missing tasks from the gantt view (#23627)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Add column names for DB Migration Reference (#23853)\n\nMisc/Internal\n^^^^^^^^^^^^^\n\n- Remove pinning for xmltodict (#23992)\n\n\nAirflow 2.3.1 (2022-05-25)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\nNo significant changes.\n\nBug Fixes\n^^^^^^^^^\n\n- Automatically reschedule stalled queued tasks in ``CeleryExecutor`` (#23690)\n- Fix expand/collapse all buttons (#23590)\n- Grid view status filters (#23392)\n- Expand/collapse all groups (#23487)\n- Fix retrieval of deprecated non-config values (#23723)\n- Fix secrets rendered in UI when task is not executed. (#22754)\n- Fix provider import error matching (#23825)\n- Fix regression in ignoring symlinks (#23535)\n- Fix ``dag-processor`` fetch metadata database config (#23575)\n- Fix auto upstream dep when expanding non-templated field (#23771)\n- Fix task log is not captured (#23684)\n- Add ``reschedule`` to the serialized fields for the ``BaseSensorOperator`` (#23674)\n- Modify db clean to also catch the ProgrammingError exception (#23699)\n- Remove titles from link buttons (#23736)\n- Fix grid details header text overlap (#23728)\n- Ensure ``execution_timeout`` as timedelta (#23655)\n- Don't run pre-migration checks for downgrade (#23634)\n- Add index for event column in log table (#23625)\n- Implement ``send_callback`` method for ``CeleryKubernetesExecutor`` and ``LocalKubernetesExecutor`` (#23617)\n- Fix ``PythonVirtualenvOperator`` templated_fields (#23559)\n- Apply specific ID collation to ``root_dag_id`` too (#23536)\n- Prevent ``KubernetesJobWatcher`` getting stuck on resource too old (#23521)\n- Fix scheduler crash when expanding with mapped task that returned none (#23486)\n- Fix broken dagrun links when many runs start at the same time (#23462)\n- Fix: Exception when parsing log #20966 (#23301)\n- Handle invalid date parsing in webserver views. (#23161)\n- Pools with negative open slots should not block other pools (#23143)\n- Move around overflow, position and padding (#23044)\n- Change approach to finding bad rows to LEFT OUTER JOIN. (#23528)\n- Only count bad refs when ``moved`` table exists (#23491)\n- Visually distinguish task group summary (#23488)\n- Remove color change for highly nested groups (#23482)\n- Optimize 2.3.0 pre-upgrade check queries (#23458)\n- Add backward compatibility for ``core__sql_alchemy_conn__cmd`` (#23441)\n- Fix literal cross product expansion (#23434)\n- Fix broken task instance link in xcom list (#23367)\n- Fix connection test button (#23345)\n- fix cli ``airflow dags show`` for mapped operator (#23339)\n- Hide some task instance attributes (#23338)\n- Don't show grid actions if server would reject with permission denied (#23332)\n- Use run_id for ``ti.mark_success_url`` (#23330)\n- Fix update user auth stats (#23314)\n- Use ``<Time />`` in Mapped Instance table (#23313)\n- Fix duplicated Kubernetes DeprecationWarnings (#23302)\n- Store grid view selection in url params (#23290)\n- Remove custom signal handling in Triggerer (#23274)\n- Override pool for TaskInstance when pool is passed from cli. (#23258)\n- Show warning if '/' is used in a DAG run ID (#23106)\n- Use kubernetes queue in kubernetes hybrid executors (#23048)\n- Add tags inside try block. (#21784)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Move ``dag_processing.processor_timeouts`` to counters section (#23393)\n- Clarify that bundle extras should not be used for PyPi installs (#23697)\n- Synchronize support for Postgres and K8S in docs (#23673)\n- Replace DummyOperator references in docs (#23502)\n- Add doc notes for keyword-only args for ``expand()`` and ``partial()`` (#23373)\n- Document fix for broken elasticsearch logs with 2.3.0+ upgrade (#23821)\n\nMisc/Internal\n^^^^^^^^^^^^^\n\n- Add typing for airflow/configuration.py (#23716)\n- Disable Flower by default from docker-compose (#23685)\n- Added postgres 14 to support versions(including breeze) (#23506)\n- add K8S 1.24 support (#23637)\n- Refactor code references from tree to grid (#23254)\n\n\nAirflow 2.3.0 (2022-04-30)\n--------------------------\n\nFor production docker image related changes, see the `Docker Image Changelog <https://airflow.apache.org/docs/docker-stack/changelog.html>`_.\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nPassing ``execution_date`` to ``XCom.set()``, ``XCom.clear()`` , ``XCom.get_one()`` , and ``XCom.get_many()`` is deprecated (#19825)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nContinuing the effort to bind TaskInstance to a DagRun, XCom entries are now also tied to a DagRun. Use the ``run_id`` argument to specify the DagRun instead.\n\nTask log templates are now read from the metadata database instead of ``airflow.cfg`` (#20165)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPreviously, a task's log is dynamically rendered from the ``[core] log_filename_template`` and ``[elasticsearch] log_id_template`` config values at runtime. This resulted in unfortunate characteristics, e.g. it is impractical to modify the config value after an Airflow instance is running for a while, since all existing task logs have be saved under the previous format and cannot be found with the new config value.\n\nA new ``log_template`` table is introduced to solve this problem. This table is synchronized with the aforementioned config values every time Airflow starts, and a new field ``log_template_id`` is added to every DAG run to point to the format used by tasks (``NULL`` indicates the first ever entry for compatibility).\n\nMinimum kubernetes library version bumped from ``3.0.0`` to ``21.7.0`` (#20759)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n.. note::\n\n   This is only about changing the ``kubernetes`` library, not the Kubernetes cluster. Airflow support for\n   Kubernetes version is described in `Installation prerequisites <https://airflow.apache.org/docs/apache-airflow/stable/installation/prerequisites.html>`_.\n\nNo change in behavior is expected.  This was necessary in order to take advantage of a `bugfix <https://github.com/kubernetes-client/python-base/commit/70b78cd8488068c014b6d762a0c8d358273865b4>`_ concerning refreshing of Kubernetes API tokens with EKS, which enabled the removal of some `workaround code <https://github.com/apache/airflow/pull/20759>`_.\n\nXCom now defined by ``run_id`` instead of ``execution_date`` (#20975)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nAs a continuation to the TaskInstance-DagRun relation change started in Airflow 2.2, the ``execution_date`` columns on XCom has been removed from the database, and replaced by an `association proxy <https://docs.sqlalchemy.org/en/13/orm/extensions/associationproxy.html>`_ field at the ORM level. If you access Airflow's metadata database directly, you should rewrite the implementation to use the ``run_id`` column instead.\n\nNote that Airflow's metadatabase definition on both the database and ORM levels are considered implementation detail without strict backward compatibility guarantees.\n\nNon-JSON-serializable params deprecated (#21135).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIt was previously possible to use dag or task param defaults that were not JSON-serializable.\n\nFor example this worked previously:\n\n.. code-block:: python\n\n  @dag.task(params={\"a\": {1, 2, 3}, \"b\": pendulum.now()})\n  def datetime_param(value):\n      print(value)\n\n\n  datetime_param(\"{{ params.a }} | {{ params.b }}\")\n\nNote the use of ``set`` and ``datetime`` types, which are not JSON-serializable.  This behavior is problematic because to override these values in a dag run conf, you must use JSON, which could make these params non-overridable.  Another problem is that the support for param validation assumes JSON.  Use of non-JSON-serializable params will be removed in Airflow 3.0 and until then, use of them will produce a warning at parse time.\n\nYou must use ``postgresql://`` instead of ``postgres://`` in ``sql_alchemy_conn`` for SQLAlchemy 1.4.0+ (#21205)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWhen you use SQLAlchemy 1.4.0+, you need to use ``postgresql://`` as the scheme in the ``sql_alchemy_conn``.\nIn the previous versions of SQLAlchemy it was possible to use ``postgres://`` , but using it in\nSQLAlchemy 1.4.0+ results in:\n\n.. code-block::\n\n  >       raise exc.NoSuchModuleError(\n              \"Can't load plugin: %s:%s\" % (self.group, name)\n          )\n  E       sqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:postgres\n\nIf you cannot change the scheme of your URL immediately, Airflow continues to work with SQLAlchemy\n1.3 and you can downgrade SQLAlchemy, but we recommend updating the scheme.\nDetails in the `SQLAlchemy Changelog <https://docs.sqlalchemy.org/en/14/changelog/changelog_14.html#change-3687655465c25a39b968b4f5f6e9170b>`_.\n\n``auth_backends`` replaces ``auth_backend`` configuration setting (#21472)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPreviously, only one backend was used to authorize use of the REST API. In 2.3 this was changed to support multiple backends, separated by comma. Each will be tried in turn until a successful response is returned.\n\n``airflow.models.base.Operator`` is removed (#21505)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPreviously, there was an empty class ``airflow.models.base.Operator`` for \"type hinting\". This class was never really useful for anything (everything it did could be done better with ``airflow.models.baseoperator.BaseOperator``), and has been removed. If you are relying on the class's existence, use ``BaseOperator`` (for concrete operators), ``airflow.models.abstractoperator.AbstractOperator`` (the base class of both ``BaseOperator`` and the AIP-42 ``MappedOperator``), or ``airflow.models.operator.Operator`` (a union type ``BaseOperator | MappedOperator`` for type annotation).\n\nZip files in the DAGs folder can no longer have a ``.py`` extension (#21538)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIt was previously possible to have any extension for zip files in the DAGs folder. Now ``.py`` files are going to be loaded as modules without checking whether it is a zip file, as it leads to less IO. If a ``.py`` file in the DAGs folder is a zip compressed file, parsing it will fail with an exception.\n\n``auth_backends`` includes session (#21640)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nTo allow the Airflow UI to use the API, the previous default authorization backend ``airflow.api.auth.backend.deny_all`` is changed to ``airflow.api.auth.backend.session``, and this is automatically added to the list of API authorization backends if a non-default value is set.\n\nDefault templates for log filenames and elasticsearch log_id changed (#21734)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIn order to support Dynamic Task Mapping the default templates for per-task instance logging has changed. If your config contains the old default values they will be upgraded-in-place.\n\nIf you are happy with the new config values you should *remove* the setting in ``airflow.cfg`` and let the default value be used. Old default values were:\n\n\n* ``[core] log_filename_template``: ``{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log``\n* ``[elasticsearch] log_id_template``: ``{dag_id}-{task_id}-{execution_date}-{try_number}``\n\n``[core] log_filename_template`` now uses \"hive partition style\" of ``dag_id=<id>/run_id=<id>`` by default, which may cause problems on some older FAT filesystems. If this affects you then you will have to change the log template.\n\nIf you have customized the templates you should ensure that they contain ``{{ ti.map_index }}`` if you want to use dynamically mapped tasks.\n\nIf after upgrading you find your task logs are no longer accessible, try adding a row in the ``log_template`` table with ``id=0``\ncontaining your previous ``log_id_template`` and ``log_filename_template``. For example, if you used the defaults in 2.2.5:\n\n.. code-block:: sql\n\n    INSERT INTO log_template (id, filename, elasticsearch_id, created_at) VALUES (0, '{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log', '{dag_id}-{task_id}-{execution_date}-{try_number}', NOW());\n\nBaseOperatorLink's ``get_link`` method changed to take a ``ti_key`` keyword argument (#21798)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIn v2.2 we \"deprecated\" passing an execution date to XCom.get methods, but there was no other option for operator links as they were only passed an execution_date.\n\nNow in 2.3 as part of Dynamic Task Mapping (AIP-42) we will need to add map_index to the XCom row to support the \"reduce\" part of the API.\n\nIn order to support that cleanly we have changed the interface for BaseOperatorLink to take an TaskInstanceKey as the ``ti_key`` keyword argument (as execution_date + task is no longer unique for mapped operators).\n\nThe existing signature will be detected (by the absence of the ``ti_key`` argument) and continue to work.\n\n``ReadyToRescheduleDep`` now only runs when ``reschedule`` is *True* (#21815)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWhen a ``ReadyToRescheduleDep`` is run, it now checks whether the ``reschedule`` attribute on the operator, and always reports itself as *passed* unless it is set to *True*. If you use this dep class on your custom operator, you will need to add this attribute to the operator class. Built-in operator classes that use this dep class (including sensors and all subclasses) already have this attribute and are not affected.\n\nThe ``deps`` attribute on an operator class should be a class level attribute (#21815)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nTo support operator-mapping (AIP 42), the ``deps`` attribute on operator class must be a set at the class level. This means that if a custom operator implements this as an instance-level variable, it will not be able to be used for operator-mapping. This does not affect existing code, but we highly recommend you to restructure the operator's dep logic in order to support the new feature.\n\nDeprecation: ``Connection.extra`` must be JSON-encoded dict (#21816)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nTLDR\n~~~~\n\nFrom Airflow 3.0, the ``extra`` field in airflow connections must be a JSON-encoded Python dict.\n\nWhat, why, and when?\n~~~~~~~~~~~~~~~~~~~~\n\nAirflow's Connection is used for storing credentials.  For storage of information that does not\nfit into user / password / host / schema / port, we have the ``extra`` string field.  Its intention\nwas always to provide for storage of arbitrary key-value pairs, like ``no_host_key_check`` in the SSH\nhook, or ``keyfile_dict`` in GCP.\n\nBut since the field is string, it's technically been permissible to store any string value.  For example\none could have stored the string value ``'my-website.com'`` and used this in the hook.  But this is a very\nbad practice. One reason is intelligibility: when you look at the value for ``extra`` , you don't have any idea\nwhat its purpose is.  Better would be to store ``{\"api_host\": \"my-website.com\"}`` which at least tells you\n*something* about the value.  Another reason is extensibility: if you store the API host as a simple string\nvalue, what happens if you need to add more information, such as the API endpoint, or credentials?  Then\nyou would need to convert the string to a dict, and this would be a breaking change.\n\nFor these reason, starting in Airflow 3.0 we will require that the ``Connection.extra`` field store\na JSON-encoded Python dict.\n\nHow will I be affected?\n~~~~~~~~~~~~~~~~~~~~~~~\n\nFor users of providers that are included in the Airflow codebase, you should not have to make any changes\nbecause in the Airflow codebase we should not allow hooks to misuse the ``Connection.extra`` field in this way.\n\nHowever, if you have any custom hooks that store something other than JSON dict, you will have to update it.\nIf you do, you should see a warning any time that this connection is retrieved or instantiated (e.g. it should show up in\ntask logs).\n\nTo see if you have any connections that will need to be updated, you can run this command:\n\n.. code-block:: shell\n\n  airflow connections export - 2>&1 >/dev/null | grep 'non-JSON'\n\nThis will catch any warnings about connections that are storing something other than JSON-encoded Python dict in the ``extra`` field.\n\nThe ``tree`` default view setting has been renamed to ``grid`` (#22167)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIf you set the ``dag_default_view`` config option or the ``default_view`` argument to ``DAG()`` to ``tree`` you will need to update your deployment. The old name will continue to work but will issue warnings.\n\nDatabase configuration moved to new section (#22284)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe following configurations have been moved from ``[core]`` to the new ``[database]`` section. However when reading the new option, the old option will be checked to see if it exists. If it does a DeprecationWarning will be issued and the old option will be used instead.\n\n* sql_alchemy_conn\n* sql_engine_encoding\n* sql_engine_collation_for_ids\n* sql_alchemy_pool_enabled\n* sql_alchemy_pool_size\n* sql_alchemy_max_overflow\n* sql_alchemy_pool_recycle\n* sql_alchemy_pool_pre_ping\n* sql_alchemy_schema\n* sql_alchemy_connect_args\n* load_default_connections\n* max_db_retries\n\nRemove requirement that custom connection UI fields be prefixed (#22607)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nHooks can define custom connection fields for their connection type by implementing method ``get_connection_form_widgets``.  These custom fields appear in the web UI as additional connection attributes, but internally they are stored in the connection ``extra`` dict field.  For technical reasons, previously, when stored in the ``extra`` dict, the custom field's dict key had to take the form ``extra__<conn type>__<field name>``.  This had the consequence of making it more cumbersome to define connections outside of the UI, since the prefix ``extra__<conn type>__`` makes it tougher to read and work with. With #22607, we make it so that you can now define custom fields such that they can be read from and stored in ``extra`` without the prefix.\n\nTo enable this, update the dict returned by the ``get_connection_form_widgets`` method to remove the prefix from the keys.  Internally, the providers manager will still use a prefix to ensure each custom field is globally unique, but the absence of a prefix in the returned widget dict will signal to the Web UI to read and store custom fields without the prefix.  Note that this is only a change to the Web UI behavior; when updating your hook in this way, you must make sure that when your *hook* reads the ``extra`` field, it will also check for the prefixed value for backward compatibility.\n\nThe webserver.X_FRAME_ENABLED configuration works according to description now (#23222).\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIn Airflow 2.0.0 - 2.2.4 the webserver.X_FRAME_ENABLED parameter worked the opposite of its description,\nsetting the value to \"true\" caused \"X-Frame-Options\" header to \"DENY\" (not allowing Airflow to be used\nin an iframe). When you set it to \"false\", the header was not added, so Airflow could be embedded in an\niframe. By default Airflow could not be embedded in an iframe.\n\nIn Airflow 2.2.5 there was a bug introduced that made it impossible to disable Airflow to\nwork in iframe. No matter what the configuration was set, it was possible to embed Airflow in an iframe.\n\nAirflow 2.3.0 restores the original meaning to the parameter. If you set it to \"true\" (default) Airflow\ncan be embedded in an iframe (no header is added), but when you set it to \"false\" the header is added\nand Airflow cannot be embedded in an iframe.\n\n\nNew Features\n^^^^^^^^^^^^\n\n- Add dynamic task mapping (`AIP-42 <https://github.com/apache/airflow/pulls?q=is%3Apr+is%3Amerged+label%3Aarea%3Adynamic-task-mapping+milestone%3A%22Airflow+2.3.0%22>`_)\n- New Grid View replaces Tree View (#18675)\n- Templated ``requirements.txt`` in Python Operators (#17349)\n- Allow reuse of decorated tasks (#22941)\n- Move the database configuration to a new section (#22284)\n- Add ``SmoothOperator`` (#22813)\n- Make operator's ``execution_timeout`` configurable (#22389)\n- Events Timetable (#22332)\n- Support dag serialization with custom ``ti_deps`` rules (#22698)\n- Support log download in task log view (#22804)\n- support for continue backfill on failures (#22697)\n- Add ``dag-processor`` cli command (#22305)\n- Add possibility to create users in LDAP mode (#22619)\n- Add ``ignore_first_depends_on_past`` for scheduled jobs (#22491)\n- Update base sensor operator to support XCOM return value (#20656)\n- Add an option for run id in the ui trigger screen (#21851)\n- Enable JSON serialization for connections (#19857)\n- Add REST API endpoint for bulk update of DAGs (#19758)\n- Add queue button to click-on-DagRun interface. (#21555)\n- Add ``list-import-errors`` to ``airflow dags`` command (#22084)\n- Store callbacks in database if ``standalone_dag_processor`` config is True. (#21731)\n- Add LocalKubernetesExecutor (#19729)\n- Add ``celery.task_timeout_error`` metric (#21602)\n- Airflow ``db downgrade`` cli command (#21596)\n- Add ``ALL_SKIPPED`` trigger rule (#21662)\n- Add ``db clean`` CLI command for purging old data (#20838)\n- Add ``celery_logging_level`` (#21506)\n- Support different timeout value for dag file parsing (#21501)\n- Support generating SQL script for upgrades (#20962)\n- Add option to compress Serialized dag data (#21332)\n- Branch python operator decorator (#20860)\n- Add Audit Log View to Dag View (#20733)\n- Add missing StatsD metric for failing SLA Callback notification (#20924)\n- Add ``ShortCircuitOperator`` configurability for respecting downstream trigger rules (#20044)\n- Allow using Markup in page title in Webserver (#20888)\n- Add Listener Plugin API that tracks TaskInstance state changes (#20443)\n- Add context var hook to inject more env vars (#20361)\n- Add a button to set all tasks to skipped (#20455)\n- Cleanup pending pods (#20438)\n- Add config to warn public deployment exposure in UI (#18557)\n- Log filename template records (#20165)\n- Added windows extensions (#16110)\n- Showing approximate time until next dag_run in Airflow  (#20273)\n- Extend config window on UI (#20052)\n- Add show dag dependencies feature to CLI (#19985)\n- Add cli command for 'airflow dags reserialize` (#19471)\n- Add missing description field to Pool schema(REST API) (#19841)\n- Introduce DagRun action to change state to queued. (#19353)\n- Add DAG run details page (#19705)\n- Add role export/import to cli tools (#18916)\n- Adding ``dag_id_pattern`` parameter to the ``/dags`` endpoint (#18924)\n\n\nImprovements\n^^^^^^^^^^^^\n\n- Show schedule_interval/timetable description in UI (#16931)\n- Added column duration to DAG runs view (#19482)\n- Enable use of custom conn extra fields without prefix (#22607)\n- Initialize finished counter at zero (#23080)\n- Improve logging of optional provider features messages (#23037)\n- Meaningful error message in resolve_template_files (#23027)\n- Update ImportError items instead of deleting and recreating them (#22928)\n- Add option ``--skip-init`` to db reset command (#22989)\n- Support importing connections from files with \".yml\" extension (#22872)\n- Support glob syntax in ``.airflowignore`` files (#21392) (#22051)\n- Hide pagination when data is a single page (#22963)\n- Support for sorting DAGs in the web UI (#22671)\n- Speed up ``has_access`` decorator by ~200ms (#22858)\n- Add XComArg to lazy-imported list of Airflow module (#22862)\n- Add more fields to REST API dags/dag_id/details endpoint (#22756)\n- Don't show irrelevant/duplicated/\"internal\" Task attrs in UI (#22812)\n- No need to load whole ti in current_state (#22764)\n- Pickle dag exception string fix (#22760)\n- Better verification of Localexecutor's parallelism option (#22711)\n- log backfill exceptions to sentry (#22704)\n- retry commit on MySQL deadlocks during backfill (#22696)\n- Add more fields to REST API get DAG(dags/dag_id) endpoint (#22637)\n- Use timetable to generate planned days for current year (#22055)\n- Disable connection pool for celery worker (#22493)\n- Make date picker label visible in trigger dag view (#22379)\n- Expose ``try_number`` in airflow vars (#22297)\n- Add generic connection type (#22310)\n- Add a few more fields to the taskinstance finished log message (#22262)\n- Pause auto-refresh if scheduler isn't running (#22151)\n- Show DagModel details. (#21868)\n- Add pip_install_options to PythonVirtualenvOperator (#22158)\n- Show import error for ``airflow dags list`` CLI command (#21991)\n- Pause auto-refresh when page is hidden (#21904)\n- Default args type check (#21809)\n- Enhance magic methods on XComArg for UX (#21882)\n- py files don't have to be checked ``is_zipfiles`` in refresh_dag (#21926)\n- Fix TaskDecorator type hints (#21881)\n- Add 'Show record' option for variables (#21342)\n- Use DB where possible for quicker ``airflow dag`` subcommands (#21793)\n- REST API: add rendered fields in task instance. (#21741)\n- Change the default auth backend to session (#21640)\n- Don't check if ``py`` DAG files are zipped during parsing (#21538)\n- Switch XCom implementation to use ``run_id`` (#20975)\n- Action log on Browse Views (#21569)\n- Implement multiple API auth backends (#21472)\n- Change logging level details of connection info in ``get_connection()`` (#21162)\n- Support mssql in airflow db shell (#21511)\n- Support config ``worker_enable_remote_control`` for celery (#21507)\n- Log memory usage in ``CgroupTaskRunner`` (#21481)\n- Modernize DAG-related URL routes and rename \"tree\" to \"grid\" (#20730)\n- Move Zombie detection to ``SchedulerJob`` (#21181)\n- Improve speed to run ``airflow`` by 6x (#21438)\n- Add more SQL template fields renderers (#21237)\n- Simplify fab has access lookup (#19294)\n- Log context only for default method (#21244)\n- Log trigger status only if at least one is running (#21191)\n- Add optional features in providers. (#21074)\n- Better multiple_outputs inferral for @task.python (#20800)\n- Improve handling of string type and non-attribute ``template_fields`` (#21054)\n- Remove un-needed deps/version requirements (#20979)\n- Correctly specify overloads for TaskFlow API for type-hinting (#20933)\n- Introduce notification_sent to SlaMiss view (#20923)\n- Rewrite the task decorator as a composition (#20868)\n- Add \"Greater/Smaller than or Equal\" to filters in the browse views (#20602) (#20798)\n- Rewrite DAG run retrieval in task command (#20737)\n- Speed up creation of DagRun for large DAGs (5k+ tasks) by 25-130% (#20722)\n- Make native environment Airflow-flavored like sandbox (#20704)\n- Better error when param value has unexpected type (#20648)\n- Add filter by state in DagRun REST API (List Dag Runs) (#20485)\n- Prevent exponential memory growth in Tasks with custom logging handler  (#20541)\n- Set default logger in logging Mixin (#20355)\n- Reduce deprecation warnings from www (#20378)\n- Add hour and minute to time format on x-axis of all charts using nvd3.lineChart (#20002)\n- Add specific warning when Task asks for more slots than pool defined with (#20178)\n- UI: Update duration column for better human readability (#20112)\n- Use Viewer role as example public role (#19215)\n- Properly implement DAG param dict copying (#20216)\n- ``ShortCircuitOperator`` push XCom by returning python_callable result (#20071)\n- Add clear logging to tasks killed due to a Dagrun timeout (#19950)\n- Change log level for Zombie detection messages (#20204)\n- Better confirmation prompts (#20183)\n- Only execute TIs of running DagRuns (#20182)\n- Check and run migration in commands if necessary (#18439)\n- Log only when Zombies exists (#20118)\n- Increase length of the email and username (#19932)\n- Add more filtering options for TI's in the UI (#19910)\n- Dynamically enable \"Test Connection\" button by connection type (#19792)\n- Avoid littering postgres server logs with \"could not obtain lock\" with HA schedulers (#19842)\n- Renamed ``Connection.get_hook`` parameter to make it the same as in ``SqlSensor`` and ``SqlOperator``. (#19849)\n- Add hook_params in SqlSensor using the latest changes from PR #18718. (#18431)\n- Speed up webserver boot time by delaying provider initialization (#19709)\n- Configurable logging of ``XCOM`` value in PythonOperator (#19378)\n- Minimize production js files (#19658)\n- Add ``hook_params`` in ``BaseSqlOperator`` (#18718)\n- Add missing \"end_date\" to hash components (#19281)\n- More friendly output of the airflow plugins command + add timetables (#19298)\n- Add sensor default timeout config (#19119)\n- Update ``taskinstance`` REST API schema to include dag_run_id field (#19105)\n- Adding feature in bash operator to append the user defined env variable to system env variable (#18944)\n- Duplicate Connection: Added logic to query if a connection id exists before creating one (#18161)\n\n\nBug Fixes\n^^^^^^^^^\n\n- Use inherited 'trigger_tasks' method (#23016)\n- In DAG dependency detector, use class type instead of class name (#21706)\n- Fix tasks being wrongly skipped by schedule_after_task_execution (#23181)\n- Fix X-Frame enabled behaviour (#23222)\n- Allow ``extra`` to be nullable in connection payload as per schema(REST API). (#23183)\n- Fix ``dag_id`` extraction for dag level access checks in web ui (#23015)\n- Fix timezone display for logs on UI (#23075)\n- Include message in graph errors (#23021)\n- Change trigger dropdown left position (#23013)\n- Don't add planned tasks for legacy DAG runs (#23007)\n- Add dangling rows check for TaskInstance references (#22924)\n- Validate the input params in connection ``CLI`` command (#22688)\n- Fix trigger event payload is not persisted in db (#22944)\n- Drop \"airflow moved\" tables in command ``db reset`` (#22990)\n- Add max width to task group tooltips (#22978)\n- Add template support for ``external_task_ids``. (#22809)\n- Allow ``DagParam`` to hold falsy values (#22964)\n- Fix regression in pool metrics (#22939)\n- Priority order tasks even when using pools (#22483)\n- Do not clear XCom when resuming from deferral (#22932)\n- Handle invalid JSON metadata in ``get_logs_with_metadata endpoint``. (#22898)\n- Fix pre-upgrade check for rows dangling w.r.t. dag_run (#22850)\n- Fixed backfill interference with scheduler (#22701)\n- Support conf param override for backfill runs (#22837)\n- Correctly interpolate pool name in ``PoolSlotsAvailableDep`` statues (#22807)\n- Fix ``email_on_failure`` with ``render_template_as_native_obj`` (#22770)\n- Fix processor cleanup on ``DagFileProcessorManager`` (#22685)\n- Prevent meta name clash for task instances (#22783)\n- remove json parse for gantt chart (#22780)\n- Check for missing dagrun should know version (#22752)\n- Fixes ``ScheduleInterval`` spec (#22635)\n- Fixing task status for non-running and non-committed tasks  (#22410)\n- Do not log the hook connection details even at DEBUG level (#22627)\n- Stop crashing when empty logs are received from kubernetes client (#22566)\n- Fix bugs about timezone change (#22525)\n- Fix entire DAG stops when one task has end_date (#20920)\n- Use logger to print message during task execution. (#22488)\n- Make sure finalizers are not skipped during exception handling (#22475)\n- update smart sensor docs and minor fix on ``is_smart_sensor_compatible()`` (#22386)\n- Fix ``run_id`` k8s and elasticsearch compatibility with Airflow 2.1 (#22385)\n- Allow to ``except_skip`` None on ``BranchPythonOperator`` (#20411)\n- Fix incorrect datetime details (DagRun views) (#21357)\n- Remove incorrect deprecation warning in secrets backend (#22326)\n- Remove ``RefreshConfiguration`` workaround for K8s token refreshing (#20759)\n- Masking extras in GET ``/connections/<connection>`` endpoint (#22227)\n- Set ``queued_dttm`` when submitting task to directly to executor (#22259)\n- Addressed some issues in the tutorial mentioned in discussion #22233 (#22236)\n- Change default python executable to python3 for docker decorator (#21973)\n- Don't validate that Params are JSON when NOTSET (#22000)\n- Add per-DAG delete permissions (#21938)\n- Fix handling some None parameters in kubernetes 23 libs. (#21905)\n- Fix handling of empty (None) tags in ``bulk_write_to_db`` (#21757)\n- Fix DAG date range bug (#20507)\n- Removed ``request.referrer`` from views.py  (#21751)\n- Make ``DbApiHook`` use ``get_uri`` from Connection (#21764)\n- Fix some migrations (#21670)\n- [de]serialize resources on task correctly (#21445)\n- Add params ``dag_id``, ``task_id`` etc to ``XCom.serialize_value`` (#19505)\n- Update test connection functionality to use custom form fields (#21330)\n- fix all \"high\" npm vulnerabilities (#21526)\n- Fix bug incorrectly removing action from role, rather than permission. (#21483)\n- Fix relationship join bug in FAB/SecurityManager with SQLA 1.4 (#21296)\n- Use Identity instead of Sequence in SQLAlchemy 1.4 for MSSQL (#21238)\n- Ensure ``on_task_instance_running`` listener can get at task (#21157)\n- Return to the same place when triggering a DAG (#20955)\n- Fix task ID deduplication in ``@task_group`` (#20870)\n- Add downgrade to some FAB migrations (#20874)\n- Only validate Params when DAG is triggered (#20802)\n- Fix ``airflow trigger`` cli (#20781)\n- Fix task instances iteration in a pool to prevent blocking (#20816)\n- Allow depending to a ``@task_group`` as a whole (#20671)\n- Use original task's ``start_date`` if a task continues after deferral (#20062)\n- Disabled edit button in task instances list view page (#20659)\n- Fix a package name import error (#20519) (#20519)\n- Remove ``execution_date`` label when get cleanup pods list (#20417)\n- Remove unneeded FAB REST API endpoints (#20487)\n- Fix parsing of Cloudwatch log group arn containing slashes (#14667) (#19700)\n- Sanity check for MySQL's TIMESTAMP column (#19821)\n- Allow using default celery command group with executors subclassed from Celery-based executors. (#18189)\n- Move ``class_permission_name`` to mixin so it applies to all classes (#18749)\n- Adjust trimmed_pod_id and replace '.' with '-' (#19036)\n- Pass custom_headers to send_email and send_email_smtp (#19009)\n- Ensure ``catchup=False`` is used in example dags (#19396)\n- Edit permalinks in OpenApi description file (#19244)\n- Navigate directly to DAG when selecting from search typeahead list (#18991)\n- [Minor] Fix padding on home page (#19025)\n\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Update doc for DAG file processing (#23209)\n- Replace changelog/updating with release notes and ``towncrier`` now (#22003)\n- Fix wrong reference in tracking-user-activity.rst (#22745)\n- Remove references to ``rbac = True`` from docs (#22725)\n- Doc: Update description for executor-bound dependencies (#22601)\n- Update check-health.rst (#22372)\n- Stronger language about Docker Compose customizability (#22304)\n- Update logging-tasks.rst (#22116)\n- Add example config of ``sql_alchemy_connect_args`` (#22045)\n- Update best-practices.rst (#22053)\n- Add information on DAG pausing/deactivation/deletion (#22025)\n- Add brief examples of integration test dags you might want (#22009)\n- Run inclusive language check on CHANGELOG (#21980)\n- Add detailed email docs for Sendgrid (#21958)\n- Add docs for ``db upgrade`` / ``db downgrade`` (#21879)\n- Update modules_management.rst (#21889)\n- Fix UPDATING section on SqlAlchemy 1.4 scheme changes (#21887)\n- Update TaskFlow tutorial doc to show how to pass \"operator-level\" args. (#21446)\n- Fix doc - replace decreasing by increasing (#21805)\n- Add another way to dynamically generate DAGs to docs (#21297)\n- Add extra information about time synchronization needed (#21685)\n- Update debug.rst docs (#21246)\n- Replaces the usage of ``postgres://`` with ``postgresql://`` (#21205)\n- Fix task execution process in ``CeleryExecutor`` docs (#20783)\n\n\nMisc/Internal\n^^^^^^^^^^^^^\n\n- Bring back deprecated security manager functions (#23243)\n- Replace usage of ``DummyOperator`` with ``EmptyOperator`` (#22974)\n- Deprecate ``DummyOperator`` in favor of ``EmptyOperator`` (#22832)\n- Remove unnecessary python 3.6 conditionals (#20549)\n- Bump ``moment`` from 2.29.1 to 2.29.2 in /airflow/www (#22873)\n- Bump ``prismjs`` from 1.26.0 to 1.27.0 in /airflow/www (#22823)\n- Bump ``nanoid`` from 3.1.23 to 3.3.2 in /airflow/www (#22803)\n- Bump ``minimist`` from 1.2.5 to 1.2.6 in /airflow/www (#22798)\n- Remove dag parsing from db init command (#22531)\n- Update our approach for executor-bound dependencies (#22573)\n- Use ``Airflow.Base.metadata`` in FAB models (#22353)\n- Limit docutils to make our documentation pretty again (#22420)\n- Add Python 3.10 support (#22050)\n- [FEATURE] add 1.22 1.23 K8S support (#21902)\n- Remove pandas upper limit now that SQLA is 1.4+ (#22162)\n- Patch ``sql_alchemy_conn`` if old postgres scheme used (#22333)\n- Protect against accidental misuse of XCom.get_value() (#22244)\n- Order filenames for migrations (#22168)\n- Don't try to auto generate migrations for Celery tables (#22120)\n- Require SQLAlchemy 1.4 (#22114)\n- bump sphinx-jinja (#22101)\n- Add compat shim for SQLAlchemy to avoid warnings (#21959)\n- Rename ``xcom.dagrun_id`` to ``xcom.dag_run_id`` (#21806)\n- Deprecate non-JSON ``conn.extra`` (#21816)\n- Bump upper bound version of ``jsonschema`` to 5.0 (#21712)\n- Deprecate helper utility ``days_ago`` (#21653)\n- Remove ```:type``` lines now ``sphinx-autoapi`` supports type hints (#20951)\n- Silence deprecation warning in tests (#20900)\n- Use ``DagRun.run_id`` instead of ``execution_date`` when updating state of TIs (UI & REST API) (#18724)\n- Add Context stub to Airflow packages (#20817)\n- Update Kubernetes library version (#18797)\n- Rename ``PodLauncher`` to ``PodManager`` (#20576)\n- Removes Python 3.6 support (#20467)\n- Add deprecation warning for non-json-serializable params (#20174)\n- Rename TaskMixin to DependencyMixin (#20297)\n- Deprecate passing execution_date to XCom methods (#19825)\n- Remove ``get_readable_dags`` and ``get_editable_dags``, and ``get_accessible_dags``. (#19961)\n- Remove postgres 9.6 support (#19987)\n- Removed hardcoded connection types. Check if hook is instance of DbApiHook. (#19639)\n- add kubernetes 1.21 support (#19557)\n- Add FAB base class and set import_name explicitly. (#19667)\n- Removes unused state transitions to handle auto-changing view permissions. (#19153)\n- Chore: Use enum for ``__var`` and ``__type`` members (#19303)\n- Use fab models (#19121)\n- Consolidate method names between Airflow Security Manager and FAB default (#18726)\n- Remove distutils usages for Python 3.10 (#19064)\n- Removing redundant ``max_tis_per_query`` initialisation on SchedulerJob (#19020)\n- Remove deprecated usage of ``init_role()`` from API (#18820)\n- Remove duplicate code on dbapi hook (#18821)\n\n\nAirflow 2.2.5, (2022-04-04)\n---------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n\"\"\"\"\"\"\"\"\"\n- Check and disallow a relative path for sqlite (#22530)\n- Fixed dask executor and tests (#22027)\n- Fix broken links to celery documentation (#22364)\n- Fix incorrect data provided to tries & landing times charts (#21928)\n- Fix assignment of unassigned triggers (#21770)\n- Fix triggerer ``--capacity`` parameter (#21753)\n- Fix graph auto-refresh on page load (#21736)\n- Fix filesystem sensor for directories (#21729)\n- Fix stray ``order_by(TaskInstance.execution_date)`` (#21705)\n- Correctly handle multiple '=' in LocalFileSystem secrets. (#21694)\n- Log exception in local executor (#21667)\n- Disable ``default_pool`` delete on web ui (#21658)\n- Extends ``typing-extensions`` to be installed with python 3.8+ #21566 (#21567)\n- Dispose unused connection pool (#21565)\n- Fix logging JDBC SQL error when task fails (#21540)\n- Filter out default configs when overrides exist. (#21539)\n- Fix Resources ``__eq__`` check (#21442)\n- Fix ``max_active_runs=1`` not scheduling runs when ``min_file_process_interval`` is high (#21413)\n- Reduce DB load incurred by Stale DAG deactivation (#21399)\n- Fix race condition between triggerer and scheduler (#21316)\n- Fix trigger dag redirect from task instance log view (#21239)\n- Log traceback in trigger exceptions (#21213)\n- A trigger might use a connection; make sure we mask passwords (#21207)\n- Update ``ExternalTaskSensorLink`` to handle templated ``external_dag_id`` (#21192)\n- Ensure ``clear_task_instances`` sets valid run state (#21116)\n- Fix: Update custom connection field processing (#20883)\n- Truncate stack trace to DAG user code for exceptions raised during execution (#20731)\n- Fix duplicate trigger creation race condition (#20699)\n- Fix Tasks getting stuck in scheduled state (#19747)\n- Fix: Do not render undefined graph edges (#19684)\n- Set ``X-Frame-Options`` header to DENY only if ``X_FRAME_ENABLED`` is set to true. (#19491)\n\nDoc only changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n- adding ``on_execute_callback`` to callbacks docs (#22362)\n- Add documentation on specifying a DB schema. (#22347)\n- Fix postgres part of pipeline example of tutorial (#21586)\n- Extend documentation for states of DAGs & tasks and update trigger rules docs (#21382)\n- DB upgrade is required when updating Airflow (#22061)\n- Remove misleading MSSQL information from the docs (#21998)\n\nMisc\n\"\"\"\"\n- Add the new Airflow Trove Classifier to setup.cfg (#22241)\n- Rename ``to_delete`` to ``to_cancel`` in TriggerRunner (#20658)\n- Update Flask-AppBuilder to ``3.4.5`` (#22596)\n\nAirflow 2.2.4, (2022-02-22)\n---------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nSmart sensors deprecated\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nSmart sensors, an \"early access\" feature added in Airflow 2, are now deprecated and will be removed in Airflow 2.4.0. They have been superseded by Deferrable Operators, added in Airflow 2.2.0.\n\nSee `Migrating to Deferrable Operators <https://airflow.apache.org/docs/apache-airflow/2.2.4/concepts/smart-sensors.html#migrating-to-deferrable-operators>`_ for details on how to migrate.\n\nBug Fixes\n^^^^^^^^^\n\n- Adding missing login provider related methods from Flask-Appbuilder (#21294)\n- Fix slow DAG deletion due to missing ``dag_id`` index for job table (#20282)\n- Add a session backend to store session data in the database (#21478)\n- Show task status only for running dags or only for the last finished dag (#21352)\n- Use compat data interval shim in log handlers (#21289)\n- Fix mismatch in generated run_id and logical date of DAG run (#18707)\n- Fix TriggerDagRunOperator extra link (#19410)\n- Add possibility to create user in the Remote User mode (#19963)\n- Avoid deadlock when rescheduling task (#21362)\n- Fix the incorrect scheduling time for the first run of dag (#21011)\n- Fix Scheduler crash when executing task instances of missing DAG (#20349)\n- Deferred tasks does not cancel when DAG is marked fail (#20649)\n- Removed duplicated dag_run join in ``Dag.get_task_instances()`` (#20591)\n- Avoid unintentional data loss when deleting DAGs (#20758)\n- Fix session usage in ``/rendered-k8s`` view (#21006)\n- Fix ``airflow dags backfill --reset-dagruns`` errors when run twice (#21062)\n- Do not set ``TaskInstance.max_tries`` in ``refresh_from_task`` (#21018)\n- Don't require dag_id in body in dagrun REST API endpoint (#21024)\n- Add Roles from Azure OAUTH Response in internal Security Manager (#20707)\n- Allow Viewing DagRuns and TIs if a user has DAG \"read\" perms (#20663)\n- Fix running ``airflow dags test <dag_id> <execution_dt>`` results in error when run twice (#21031)\n- Switch to non-vendored latest connexion library (#20910)\n- Bump flask-appbuilder to ``>=3.3.4`` (#20628)\n- upgrade celery to ``5.2.3`` (#19703)\n- Bump croniter from ``<1.1`` to ``<1.2`` (#20489)\n- Avoid calling ``DAG.following_schedule()`` for ``TaskInstance.get_template_context()`` (#20486)\n- Fix(standalone): Remove hardcoded Webserver port (#20429)\n- Remove unnecessary logging in experimental API (#20356)\n- Un-ignore DeprecationWarning (#20322)\n- Deepcopying Kubernetes Secrets attributes causing issues (#20318)\n- Fix(dag-dependencies): fix arrow styling (#20303)\n- Adds retry on taskinstance retrieval lock (#20030)\n- Correctly send timing metrics when using dogstatsd (fix schedule_delay metric) (#19973)\n- Enhance ``multiple_outputs`` inference of dict typing (#19608)\n- Fixing Amazon SES email backend (#18042)\n- Pin MarkupSafe until we are able to upgrade Flask/Jinja (#21664)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Added explaining concept of logical date in DAG run docs (#21433)\n- Add note about Variable precedence with env vars (#21568)\n- Update error docs to include before_send option (#21275)\n- Augment xcom docs (#20755)\n- Add documentation and release policy on \"latest\" constraints (#21093)\n- Add a link to the DAG model in the Python API reference (#21060)\n- Added an enum param example (#20841)\n- Compare taskgroup and subdag (#20700)\n- Add note about reserved ``params`` keyword (#20640)\n- Improve documentation on ``Params`` (#20567)\n- Fix typo in MySQL Database creation code (Set up DB docs)  (#20102)\n- Add requirements.txt description (#20048)\n- Clean up ``default_args`` usage in docs (#19803)\n- Add docker-compose explanation to conn localhost (#19076)\n- Update CSV ingest code for tutorial (#18960)\n- Adds Pendulum 1.x -> 2.x upgrade documentation (#18955)\n- Clean up dynamic ``start_date`` values from docs (#19607)\n- Docs for multiple pool slots (#20257)\n- Update upgrading.rst with detailed code example of how to resolve post-upgrade warning (#19993)\n\nMisc\n^^^^\n\n- Deprecate some functions in the experimental API (#19931)\n- Deprecate smart sensors (#20151)\n\nAirflow 2.2.3, (2021-12-21)\n---------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n^^^^^^^^^\n\n- Lazy Jinja2 context (#20217)\n- Exclude ``snowflake-sqlalchemy`` v1.2.5 (#20245)\n- Move away from legacy ``importlib.resources`` API (#19091)\n- Move ``setgid`` as the first command executed in forked task runner (#20040)\n- Fix race condition when starting ``DagProcessorAgent`` (#19935)\n- Limit ``httpx`` to <0.20.0 (#20218)\n- Log provider import errors as debug warnings (#20172)\n- Bump minimum required ``alembic`` version (#20153)\n- Fix log link in gantt view (#20121)\n- fixing #19028 by moving chown to use sudo (#20114)\n- Lift off upper bound for ``MarkupSafe`` (#20113)\n- Fix infinite recursion on redact log (#20039)\n- Fix db downgrades (#19994)\n- Context class handles deprecation (#19886)\n- Fix possible reference to undeclared variable (#19933)\n- Validate ``DagRun`` state is valid on assignment (#19898)\n- Workaround occasional deadlocks with MSSQL (#19856)\n- Enable task run setting to be able reinitialize (#19845)\n- Fix log endpoint for same task (#19672)\n- Cast macro datetime string inputs explicitly (#19592)\n- Do not crash with stacktrace when task instance is missing (#19478)\n- Fix log timezone in task log view (#19342) (#19401)\n- Fix: Add taskgroup tooltip to graph view (#19083)\n- Rename execution date in forms and tables (#19063)\n- Simplify \"invalid TI state\" message (#19029)\n- Handle case of nonexistent file when preparing file path queue (#18998)\n- Do not create dagruns for DAGs with import errors  (#19367)\n- Fix field relabeling when switching between conn types (#19411)\n- ``KubernetesExecutor`` should default to template image if used (#19484)\n- Fix task instance api cannot list task instances with ``None`` state (#19487)\n- Fix IntegrityError in ``DagFileProcessor.manage_slas`` (#19553)\n- Declare data interval fields as serializable (#19616)\n- Relax timetable class validation (#19878)\n- Fix labels used to find queued ``KubernetesExecutor`` pods (#19904)\n- Fix moved data migration check for MySQL when replication is used (#19999)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Warn without tracebacks when example_dags are missing deps (#20295)\n- Deferrable operators doc clarification (#20150)\n- Ensure the example DAGs are all working (#19355)\n- Updating core example DAGs to use TaskFlow API where applicable (#18562)\n- Add xcom clearing behaviour on task retries (#19968)\n- Add a short chapter focusing on adapting secret format for connections (#19859)\n- Add information about supported OS-es for Apache Airflow (#19855)\n- Update docs to reflect that changes to the ``base_log_folder`` require updating other configs (#19793)\n- Disclaimer in ``KubernetesExecutor`` pod template docs (#19686)\n- Add upgrade note on ``execution_date`` -> ``run_id`` (#19593)\n- Expanding ``.output`` operator property information in TaskFlow tutorial doc (#19214)\n- Add example SLA DAG (#19563)\n- Add a proper example to patch DAG (#19465)\n- Add DAG file processing description to Scheduler Concepts (#18954)\n- Updating explicit arg example in TaskFlow API tutorial doc (#18907)\n- Adds back documentation about context usage in Python/@task (#18868)\n- Add release date for when an endpoint/field is added in the REST API (#19203)\n- Better ``pod_template_file`` examples (#19691)\n- Add description on how you can customize image entrypoint (#18915)\n- Dags-in-image pod template example should not have dag mounts (#19337)\n\nAirflow 2.2.2 (2021-11-15)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n^^^^^^^^^\n\n- Fix bug when checking for existence of a Variable (#19395)\n- Fix Serialization when ``relativedelta`` is passed as ``schedule_interval``  (#19418)\n- Fix moving of dangling TaskInstance rows for SQL Server (#19425)\n- Fix task instance modal in gantt view (#19258)\n- Fix serialization of ``Params`` with set data type (#19267)\n- Check if job object is ``None`` before calling ``.is_alive()`` (#19380)\n- Task should fail immediately when pod is unprocessable (#19359)\n- Fix downgrade for a DB Migration (#19390)\n- Only mark SchedulerJobs as failed, not any jobs (#19375)\n- Fix message on \"Mark as\" confirmation page (#19363)\n- Bugfix: Check next run exists before reading data interval (#19307)\n- Fix MySQL db migration with default encoding/collation (#19268)\n- Fix hidden tooltip position (#19261)\n- ``sqlite_default`` Connection has been hard-coded to ``/tmp``, use ``gettempdir`` instead (#19255)\n- Fix Toggle Wrap on DAG code page (#19211)\n- Clarify \"dag not found\" error message in CLI (#19338)\n- Add Note to SLA regarding ``schedule_interval`` (#19173)\n- Use ``execution_date`` to check for existing ``DagRun`` for ``TriggerDagRunOperator`` (#18968)\n- Add explicit session parameter in ``PoolSlotsAvailableDep`` (#18875)\n- FAB still requires ``WTForms<3.0`` (#19466)\n- Fix missing dagruns when ``catchup=True`` (#19528)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Add missing parameter documentation for \"timetable\" (#19282)\n- Improve Kubernetes Executor docs (#19339)\n- Update image tag used in docker docs\n\nAirflow 2.2.1 (2021-10-29)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\n``Param``'s default value for ``default`` removed\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n``Param``, introduced in Airflow 2.2.0, accidentally set the default value to ``None``. This default has been removed. If you want ``None`` as your default, explicitly set it as such. For example:\n\n.. code-block:: python\n\n   Param(None, type=[\"null\", \"string\"])\n\nNow if you resolve a ``Param`` without a default and don't pass a value, you will get an ``TypeError``. For Example:\n\n.. code-block:: python\n\n   Param().resolve()  # raises TypeError\n\n``max_queued_runs_per_dag`` configuration has been removed\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe ``max_queued_runs_per_dag`` configuration option in ``[core]`` section has been removed. Previously, this controlled the number of queued dagrun\nthe scheduler can create in a dag. Now, the maximum number is controlled internally by the DAG's ``max_active_runs``\n\nBug Fixes\n^^^^^^^^^\n\n- Fix Unexpected commit error in SchedulerJob (#19213)\n- Add DagRun.logical_date as a property (#19198)\n- Clear ``ti.next_method`` and ``ti.next_kwargs`` on task finish (#19183)\n- Faster PostgreSQL db migration to Airflow 2.2 (#19166)\n- Remove incorrect type comment in ``Swagger2Specification._set_defaults`` classmethod (#19065)\n- Add TriggererJob to jobs check command (#19179, #19185)\n- Hide tooltip when next run is ``None`` (#19112)\n- Create TI context with data interval compat layer (#19148)\n- Fix queued dag runs changes ``catchup=False`` behaviour (#19130, #19145)\n- add detailed information to logging when a dag or a task finishes. (#19097)\n- Warn about unsupported Python 3.10 (#19060)\n- Fix catchup by limiting queued dagrun creation using ``max_active_runs`` (#18897)\n- Prevent scheduler crash when serialized dag is missing (#19113)\n- Don't install SQLAlchemy/Pendulum adapters for other DBs (#18745)\n- Workaround ``libstdcpp`` TLS error (#19010)\n- Change ``ds``, ``ts``, etc. back to use logical date (#19088)\n- Ensure task state doesn't change when marked as failed/success/skipped (#19095)\n- Relax packaging requirement (#19087)\n- Rename trigger page label to Logical Date (#19061)\n- Allow Param to support a default value of ``None`` (#19034)\n- Upgrade old DAG/task param format when deserializing from the DB (#18986)\n- Don't bake ENV and _cmd into tmp config for non-sudo (#18772)\n- CLI: Fail ``backfill`` command before loading DAGs if missing args (#18994)\n- BugFix: Null execution date on insert to ``task_fail`` violating NOT NULL (#18979)\n- Try to move \"dangling\" rows in db upgrade (#18953)\n- Row lock TI query in ``SchedulerJob._process_executor_events`` (#18975)\n- Sentry before send fallback (#18980)\n- Fix ``XCom.delete`` error in Airflow 2.2.0 (#18956)\n- Check python version before starting triggerer (#18926)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Update access control documentation for TaskInstances and DagRuns (#18644)\n- Add information about keepalives for managed Postgres (#18850)\n- Doc: Add Callbacks Section to Logging & Monitoring (#18842)\n- Group PATCH DAGrun together with other DAGRun endpoints (#18885)\n\nAirflow 2.2.0 (2021-10-11)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNote: Upgrading the database to ``2.2.0`` or later can take some time to complete, particularly if you have a large ``task_instance`` table.\n\n``worker_log_server_port`` configuration has been moved to the ``logging`` section.\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe ``worker_log_server_port`` configuration option has been moved from ``[celery]`` section to ``[logging]`` section to allow for reuse between different executors.\n\n``pandas`` is now an optional dependency\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPreviously ``pandas`` was a core requirement so when you run ``pip install apache-airflow`` it looked for ``pandas``\nlibrary and installed it if it does not exist.\n\nIf you want to install ``pandas`` compatible with Airflow, you can use ``[pandas]`` extra while\ninstalling Airflow, example for Python 3.8 and Airflow 2.1.2:\n\n.. code-block:: shell\n\n   pip install -U \"apache-airflow[pandas]==2.1.2\" \\\n     --constraint https://raw.githubusercontent.com/apache/airflow/constraints-2.1.2/constraints-3.8.txt\"\n\n``none_failed_or_skipped`` trigger rule has been deprecated\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n``TriggerRule.NONE_FAILED_OR_SKIPPED`` is replaced by ``TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS``.\nThis is only name change, no functionality changes made.\nThis change is backward compatible however ``TriggerRule.NONE_FAILED_OR_SKIPPED`` will be removed in next major release.\n\nDummy trigger rule has been deprecated\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n``TriggerRule.DUMMY`` is replaced by ``TriggerRule.ALWAYS``.\nThis is only name change, no functionality changes made.\nThis change is backward compatible however ``TriggerRule.DUMMY`` will be removed in next major release.\n\nDAG concurrency settings have been renamed\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n``[core] dag_concurrency`` setting in ``airflow.cfg`` has been renamed to ``[core] max_active_tasks_per_dag``\nfor better understanding.\n\nIt is the maximum number of task instances allowed to run concurrently in each DAG. To calculate\nthe number of tasks that is running concurrently for a DAG, add up the number of running\ntasks for all DAG runs of the DAG.\n\nThis is configurable at the DAG level with ``max_active_tasks`` and a default can be set in ``airflow.cfg`` as\n``[core] max_active_tasks_per_dag``.\n\n**Before**\\ :\n\n.. code-block:: ini\n\n   [core]\n   dag_concurrency = 16\n\n**Now**\\ :\n\n.. code-block:: ini\n\n   [core]\n   max_active_tasks_per_dag = 16\n\nSimilarly, ``DAG.concurrency`` has been renamed to ``DAG.max_active_tasks``.\n\n**Before**\\ :\n\n.. code-block:: python\n\n   dag = DAG(\n       dag_id=\"example_dag\",\n       start_date=datetime(2021, 1, 1),\n       catchup=False,\n       concurrency=3,\n   )\n\n**Now**\\ :\n\n.. code-block:: python\n\n   dag = DAG(\n       dag_id=\"example_dag\",\n       start_date=datetime(2021, 1, 1),\n       catchup=False,\n       max_active_tasks=3,\n   )\n\nIf you are using DAGs Details API endpoint, use ``max_active_tasks`` instead of ``concurrency``.\n\nTask concurrency parameter has been renamed\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n``BaseOperator.task_concurrency`` has been deprecated and renamed to ``max_active_tis_per_dag`` for\nbetter understanding.\n\nThis parameter controls the number of concurrent running task instances across ``dag_runs``\nper task.\n\n**Before**\\ :\n\n.. code-block:: python\n\n   with DAG(dag_id=\"task_concurrency_example\"):\n       BashOperator(task_id=\"t1\", task_concurrency=2, bash_command=\"echo Hi\")\n\n**After**\\ :\n\n.. code-block:: python\n\n   with DAG(dag_id=\"task_concurrency_example\"):\n       BashOperator(task_id=\"t1\", max_active_tis_per_dag=2, bash_command=\"echo Hi\")\n\n``processor_poll_interval`` config have been renamed to ``scheduler_idle_sleep_time``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n``[scheduler] processor_poll_interval`` setting in ``airflow.cfg`` has been renamed to ``[scheduler] scheduler_idle_sleep_time``\nfor better understanding.\n\nIt controls the 'time to sleep' at the end of the Scheduler loop if nothing was scheduled inside ``SchedulerJob``.\n\n**Before**\\ :\n\n.. code-block:: ini\n\n   [scheduler]\n   processor_poll_interval = 16\n\n**Now**\\ :\n\n.. code-block:: ini\n\n   [scheduler]\n   scheduler_idle_sleep_time = 16\n\nMarking success/failed automatically clears failed downstream tasks\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWhen marking a task success/failed in Graph View, its downstream tasks that are in failed/upstream_failed state are automatically cleared.\n\n``[core] store_dag_code`` has been removed\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWhile DAG Serialization is a strict requirements since Airflow 2, we allowed users to control\nwhere the Webserver looked for when showing the **Code View**.\n\nIf ``[core] store_dag_code`` was set to ``True``\\ , the Scheduler stored the code in the DAG file in the\nDB (in ``dag_code`` table) as a plain string, and the webserver just read it from the same table.\nIf the value was set to ``False``\\ , the webserver read it from the DAG file.\n\nWhile this setting made sense for Airflow < 2, it caused some confusion to some users where they thought\nthis setting controlled DAG Serialization.\n\nFrom Airflow 2.2, Airflow will only look for DB when a user clicks on **Code View** for a DAG.\n\nClearing a running task sets its state to ``RESTARTING``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPreviously, clearing a running task sets its state to ``SHUTDOWN``. The task gets killed and goes into ``FAILED`` state. After `#16681 <https://github.com/apache/airflow/pull/16681>`_\\ , clearing a running task sets its state to ``RESTARTING``. The task is eligible for retry without going into ``FAILED`` state.\n\nRemove ``TaskInstance.log_filepath`` attribute\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThis method returned incorrect values for a long time, because it did not take into account the different\nlogger configuration and task retries. We have also started supporting more advanced tools that don't use\nfiles, so it is impossible to determine the correct file path in every case e.g. Stackdriver doesn't use files\nbut identifies logs based on labels.  For this reason, we decided to delete this attribute.\n\nIf you need to read logs, you can use ``airflow.utils.log.log_reader.TaskLogReader`` class, which does not have\nthe above restrictions.\n\nIf a sensor times out, it will not retry\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPreviously, a sensor is retried when it times out until the number of ``retries`` are exhausted. So the effective timeout of a sensor is ``timeout * (retries + 1)``. This behaviour is now changed. A sensor will immediately fail without retrying if ``timeout`` is reached. If it's desirable to let the sensor continue running for longer time, set a larger ``timeout`` instead.\n\nDefault Task Pools Slots can be set using ``[core] default_pool_task_slot_count``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nBy default tasks are running in ``default_pool``. ``default_pool`` is initialized with ``128`` slots and user can change the\nnumber of slots through UI/CLI/API for an existing deployment.\n\nFor new deployments, you can use ``default_pool_task_slot_count`` setting in ``[core]`` section. This setting would\nnot have any effect in an existing deployment where the ``default_pool`` already exists.\n\nPreviously this was controlled by ``non_pooled_task_slot_count`` in ``[core]`` section, which was not documented.\n\nWebserver DAG refresh buttons removed\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nNow that the DAG parser syncs DAG permissions there is no longer a need for manually refreshing DAGs. As such, the buttons to refresh a DAG have been removed from the UI.\n\nIn addition, the ``/refresh`` and ``/refresh_all`` webserver endpoints have also been removed.\n\nTaskInstances now *require* a DagRun\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nUnder normal operation every TaskInstance row in the database would have DagRun row too, but it was possible to manually delete the DagRun and Airflow would still schedule the TaskInstances.\n\nIn Airflow 2.2 we have changed this and now there is a database-level foreign key constraint ensuring that every TaskInstance has a DagRun row.\n\nBefore updating to this 2.2 release you will have to manually resolve any inconsistencies (add back DagRun rows, or delete TaskInstances) if you have any \"dangling\" TaskInstance\" rows.\n\nAs part of this change the ``clean_tis_without_dagrun_interval`` config option under ``[scheduler]`` section has been removed and has no effect.\n\nTaskInstance and TaskReschedule now define ``run_id`` instead of ``execution_date``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nAs a part of the TaskInstance-DagRun relation change, the ``execution_date`` columns on TaskInstance and TaskReschedule have been removed from the database, and replaced by `association proxy <https://docs.sqlalchemy.org/en/13/orm/extensions/associationproxy.html>`_ fields at the ORM level. If you access Airflow's metadatabase directly, you should rewrite the implementation to use the ``run_id`` columns instead.\n\nNote that Airflow's metadatabase definition on both the database and ORM levels are considered implementation detail without strict backward compatibility guarantees.\n\nDaskExecutor - Dask Worker Resources and queues\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIf dask workers are not started with complementary resources to match the specified queues, it will now result in an ``AirflowException``\\ , whereas before it would have just ignored the ``queue`` argument.\n\nLogical date of a DAG run triggered from the web UI now have its sub-second component set to zero\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nDue to a change in how the logical date (``execution_date``) is generated for a manual DAG run, a manual DAG run's logical date may not match its time-of-trigger, but have its sub-second part zero-ed out. For example, a DAG run triggered on ``2021-10-11T12:34:56.78901`` would have its logical date set to ``2021-10-11T12:34:56.00000``.\n\nThis may affect some logic that expects on this quirk to detect whether a run is triggered manually or not. Note that ``dag_run.run_type`` is a more authoritative value for this purpose. Also, if you need this distinction between automated and manually-triggered run for \"next execution date\" calculation, please also consider using the new data interval variables instead, which provide a more consistent behavior between the two run types.\n\nNew Features\n^^^^^^^^^^^^\n\n- AIP-39: Add (customizable) Timetable class to Airflow for richer scheduling behaviour (#15397, #16030,\n  #16352, #17030, #17122, #17414, #17552, #17755, #17989, #18084, #18088, #18244, #18266, #18420, #18434,\n  #18421, #18475, #18499, #18573, #18522, #18729, #18706, #18742, #18786, #18804)\n- AIP-40: Add Deferrable \"Async\" Tasks (#15389, #17564, #17565, #17601, #17745, #17747, #17748, #17875,\n  #17876, #18129, #18210, #18214, #18552, #18728, #18414)\n- Add a Docker Taskflow decorator (#15330, #18739)\n- Add Airflow Standalone command (#15826)\n- Display alert messages on dashboard from local settings (#18284)\n- Advanced Params using json-schema (#17100)\n- Ability to test connections from UI or API (#15795, #18750)\n- Add Next Run to UI (#17732)\n- Add default weight rule configuration option (#18627)\n- Add a calendar field to choose the execution date of the DAG when triggering it (#16141)\n- Allow setting specific ``cwd`` for BashOperator (#17751)\n- Show import errors in DAG views (#17818)\n- Add pre/post execution hooks [Experimental] (#17576)\n- Added table to view providers in Airflow ui under admin tab (#15385)\n- Adds secrets backend/logging/auth information to provider yaml (#17625)\n- Add date format filters to Jinja environment (#17451)\n- Introduce ``RESTARTING`` state (#16681)\n- Webserver: Unpause DAG on manual trigger (#16569)\n- API endpoint to create new user (#16609)\n- Add ``insert_args`` for support transfer replace (#15825)\n- Add recursive flag to glob in filesystem sensor (#16894)\n- Add conn to jinja template context (#16686)\n- Add ``default_args`` for ``TaskGroup`` (#16557)\n- Allow adding duplicate connections from UI (#15574)\n- Allow specifying multiple URLs via the CORS config option (#17941)\n- Implement API endpoint for DAG deletion (#17980)\n- Add DAG run endpoint for marking a dagrun success or failed(#17839)\n- Add support for ``kinit`` options ``[-f|-F]`` and ``[-a|-A]`` (#17816)\n- Queue support for ``DaskExecutor`` using Dask Worker Resources (#16829, #18720)\n- Make auto refresh interval configurable (#18107)\n\nImprovements\n^^^^^^^^^^^^\n\n- Small improvements for Airflow UI (#18715, #18795)\n- Rename ``processor_poll_interval`` to ``scheduler_idle_sleep_time`` (#18704)\n- Check the allowed values for the logging level (#18651)\n- Fix error on triggering a dag that doesn't exist using ``dagrun_conf`` (#18655)\n- Add muldelete action to ``TaskInstanceModelView`` (#18438)\n- Avoid importing DAGs during clean DB installation (#18450)\n- Require can_edit on DAG privileges to modify TaskInstances and DagRuns (#16634)\n- Make Kubernetes job description fit on one log line (#18377)\n- Always draw borders if task instance state is null or undefined (#18033)\n- Inclusive Language (#18349)\n- Improved log handling for zombie tasks (#18277)\n- Adding ``Variable.update`` method and improving detection of variable key collisions (#18159)\n- Add note about params on trigger DAG page (#18166)\n- Change ``TaskInstance`` and ``TaskReschedule`` PK from ``execution_date`` to ``run_id`` (#17719)\n- Adding ``TaskGroup`` support in ``BaseOperator.chain()`` (#17456)\n- Allow filtering DAGS by tags in the REST API (#18090)\n- Optimize imports of Providers Manager (#18052)\n- Adds capability of Warnings for incompatible community providers (#18020)\n- Serialize the ``template_ext`` attribute to show it in UI (#17985)\n- Add ``robots.txt`` and ``X-Robots-Tag`` header (#17946)\n- Refactor ``BranchDayOfWeekOperator``, ``DayOfWeekSensor`` (#17940)\n- Update error message to guide the user into self-help mostly (#17929)\n- Update to Celery 5 (#17397)\n- Add links to provider's documentation (#17736)\n- Remove Marshmallow schema warnings (#17753)\n- Rename ``none_failed_or_skipped`` by ``none_failed_min_one_success`` trigger rule (#17683)\n- Remove ``[core] store_dag_code`` & use DB to get Dag Code (#16342)\n- Rename ``task_concurrency`` to ``max_active_tis_per_dag`` (#17708)\n- Import Hooks lazily individually in providers manager (#17682)\n- Adding support for multiple task-ids in the external task sensor (#17339)\n- Replace ``execution_date`` with ``run_id`` in airflow tasks run command (#16666)\n- Make output from users cli command more consistent (#17642)\n- Open relative extra links in place (#17477)\n- Move ``worker_log_server_port`` option to the logging section (#17621)\n- Use gunicorn to serve logs generated by worker (#17591)\n- Improve validation of Group id (#17578)\n- Simplify 404 page (#17501)\n- Add XCom.clear so it's hookable in custom XCom backend (#17405)\n- Add deprecation notice for ``SubDagOperator`` (#17488)\n- Support DAGS folder being in different location on scheduler and runners (#16860)\n- Remove /dagrun/create and disable edit form generated by F.A.B (#17376)\n- Enable specifying dictionary paths in ``template_fields_renderers`` (#17321)\n- error early if virtualenv is missing (#15788)\n- Handle connection parameters added to Extra and custom fields (#17269)\n- Fix ``airflow celery stop`` to accept the pid file. (#17278)\n- Remove DAG refresh buttons (#17263)\n- Deprecate dummy trigger rule in favor of always (#17144)\n- Be verbose about failure to import ``airflow_local_settings`` (#17195)\n- Include exit code in ``AirflowException`` str when ``BashOperator`` fails. (#17151)\n- Adding EdgeModifier support for chain() (#17099)\n- Only allows supported field types to be used in custom connections (#17194)\n- Secrets backend failover (#16404)\n- Warn on Webserver when using ``SQLite`` or ``SequentialExecutor`` (#17133)\n- Extend ``init_containers`` defined in ``pod_override`` (#17537)\n- Client-side filter dag dependencies (#16253)\n- Improve executor validation in CLI (#17071)\n- Prevent running ``airflow db init/upgrade`` migrations and setup in parallel. (#17078)\n- Update ``chain()`` and ``cross_downstream()`` to support ``XComArgs`` (#16732)\n- Improve graph view refresh (#16696)\n- When a task instance fails with exception, log it (#16805)\n- Set process title for ``serve-logs`` and ``LocalExecutor`` (#16644)\n- Rename ``test_cycle`` to ``check_cycle`` (#16617)\n- Add schema as ``DbApiHook`` instance attribute (#16521, #17423)\n- Improve compatibility with MSSQL (#9973)\n- Add transparency for unsupported connection type (#16220)\n- Call resource based fab methods (#16190)\n- Format more dates with timezone (#16129)\n- Replace deprecated ``dag.sub_dag`` with ``dag.partial_subset`` (#16179)\n- Treat ``AirflowSensorTimeout`` as immediate failure without retrying (#12058)\n- Marking success/failed automatically clears failed downstream tasks  (#13037)\n- Add close/open indicator for import dag errors (#16073)\n- Add collapsible import errors (#16072)\n- Always return a response in TI's ``action_clear`` view (#15980)\n- Add cli command to delete user by email (#15873)\n- Use resource and action names for FAB permissions (#16410)\n- Rename DAG concurrency (``[core] dag_concurrency``) settings for easier understanding (#16267, #18730)\n- Calendar UI improvements (#16226)\n- Refactor: ``SKIPPED`` should not be logged again as ``SUCCESS`` (#14822)\n- Remove  version limits for ``dnspython`` (#18046, #18162)\n- Accept custom run ID in TriggerDagRunOperator (#18788)\n\nBug Fixes\n^^^^^^^^^\n\n- Make REST API patch user endpoint work the same way as the UI (#18757)\n- Properly set ``start_date`` for cleared tasks (#18708)\n- Ensure task_instance exists before running update on its state(REST API) (#18642)\n- Make ``AirflowDateTimePickerWidget`` a required field (#18602)\n- Retry deadlocked transactions on deleting old rendered task fields (#18616)\n- Fix ``retry_exponential_backoff`` divide by zero error when retry delay is zero (#17003)\n- Improve how UI handles datetimes (#18611, #18700)\n- Bugfix: dag_bag.get_dag should return None, not raise exception (#18554)\n- Only show the task modal if it is a valid instance (#18570)\n- Fix accessing rendered ``{{ task.x }}`` attributes from within templates (#18516)\n- Add missing email type of connection (#18502)\n- Don't use flash for \"same-page\" UI messages. (#18462)\n- Fix task group tooltip (#18406)\n- Properly fix dagrun update state endpoint (#18370)\n- Properly handle ti state difference between executor and scheduler (#17819)\n- Fix stuck \"queued\" tasks in KubernetesExecutor (#18152)\n- Don't permanently add zip DAGs to ``sys.path`` (#18384)\n- Fix random deadlocks in MSSQL database (#18362)\n- Deactivating DAGs which have been removed from files (#17121)\n- When syncing dags to db remove ``dag_tag`` rows that are now unused (#8231)\n- Graceful scheduler shutdown on error (#18092)\n- Fix mini scheduler not respecting ``wait_for_downstream`` dep (#18338)\n- Pass exception to ``run_finished_callback`` for Debug Executor (#17983)\n- Make ``XCom.get_one`` return full, not abbreviated values (#18274)\n- Use try/except when closing temporary file in task_runner (#18269)\n- show next run if not none (#18273)\n- Fix DB session handling in ``XCom.set`` (#18240)\n- Fix external_executor_id not being set for manually run jobs (#17207)\n- Fix deleting of zipped Dags in Serialized Dag Table (#18243)\n- Return explicit error on user-add for duplicated email (#18224)\n- Remove loading dots even when last run data is empty (#18230)\n- Swap dag import error dropdown icons (#18207)\n- Automatically create section when migrating config (#16814)\n- Set encoding to utf-8 by default while reading task logs (#17965)\n- Apply parent dag permissions to subdags (#18160)\n- Change id collation for MySQL to case-sensitive (#18072)\n- Logs task launch exception in ``StandardTaskRunner`` (#17967)\n- Applied permissions to ``self._error_file`` (#15947)\n- Fix blank dag dependencies view (#17990)\n- Add missing menu access for dag dependencies and configurations pages (#17450)\n- Fix passing Jinja templates in ``DateTimeSensor`` (#17959)\n- Fixing bug which restricted the visibility of ImportErrors (#17924)\n- Fix grammar in ``traceback.html`` (#17942)\n- Fix ``DagRunState`` enum query for ``MySQLdb`` driver (#17886)\n- Fixed button size in \"Actions\" group. (#17902)\n- Only show import errors for DAGs a user can access (#17835)\n- Show all import_errors from zip files (#17759)\n- fix EXTRA_LOGGER_NAMES param and related docs (#17808)\n- Use one interpreter for Airflow and gunicorn (#17805)\n- Fix: Mysql 5.7 id utf8mb3 (#14535)\n- Fix dag_processing.last_duration metric random holes (#17769)\n- Automatically use ``utf8mb3_general_ci`` collation for MySQL (#17729)\n- fix: filter condition of ``TaskInstance`` does not work #17535 (#17548)\n- Dont use TaskInstance in CeleryExecutor.trigger_tasks (#16248)\n- Remove locks for upgrades in MSSQL (#17213)\n- Create virtualenv via python call (#17156)\n- Ensure a DAG is acyclic when running ``DAG.cli()`` (#17105)\n- Translate non-ascii characters (#17057)\n- Change the logic of ``None`` comparison in ``model_list`` template (#16893)\n- Have UI and POST /task_instances_state API endpoint have same behaviour (#16539)\n- ensure task is skipped if missing sla (#16719)\n- Fix direct use of ``cached_property`` module (#16710)\n- Fix TI success confirm page (#16650)\n- Modify return value check in python virtualenv jinja template (#16049)\n- Fix dag dependency search (#15924)\n- Make custom JSON encoder support ``Decimal`` (#16383)\n- Bugfix: Allow clearing tasks with just ``dag_id`` and empty ``subdir`` (#16513)\n- Convert port value to a number before calling test connection (#16497)\n- Handle missing/null serialized DAG dependencies (#16393)\n- Correctly set ``dag.fileloc`` when using the ``@dag`` decorator (#16384)\n- Fix TI success/failure links (#16233)\n- Correctly implement autocomplete early return in ``airflow/www/views.py`` (#15940)\n- Backport fix to allow pickling of Loggers to Python 3.6 (#18798)\n- Fix bug that Backfill job fail to run when there are tasks run into ``reschedule`` state (#17305, #18806)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Update ``dagbag_size`` documentation (#18824)\n- Update documentation about bundle extras (#18828)\n- Fix wrong Postgres ``search_path`` set up instructions (#17600)\n- Remove ``AIRFLOW_GID`` from Docker images (#18747)\n- Improve error message for BranchPythonOperator when no task_id to follow (#18471)\n- Improve guidance to users telling them what to do on import timeout (#18478)\n- Explain scheduler fine-tuning better (#18356)\n- Added example JSON for airflow pools import (#18376)\n- Add ``sla_miss_callback`` section to the documentation (#18305)\n- Explain sentry default environment variable for subprocess hook (#18346)\n- Refactor installation pages (#18282)\n- Improves quick-start docker-compose warnings and documentation (#18164)\n- Production-level support for MSSQL (#18382)\n- Update non-working example in documentation (#18067)\n- Remove default_args pattern + added get_current_context() use for Core Airflow example DAGs (#16866)\n- Update max_tis_per_query to better render on the webpage (#17971)\n- Adds Github Oauth example with team based authorization (#17896)\n- Update docker.rst (#17882)\n- Example xcom update (#17749)\n- Add doc warning about connections added via env vars (#17915)\n- fix wrong documents around upgrade-check.rst (#17903)\n- Add Brent to Committers list (#17873)\n- Improves documentation about modules management (#17757)\n- Remove deprecated metrics from metrics.rst (#17772)\n- Make sure \"production-readiness\" of docker-compose is well explained (#17731)\n- Doc: Update Upgrade to v2 docs with Airflow 1.10.x EOL dates (#17710)\n- Doc: Replace deprecated param from docstrings (#17709)\n- Describe dag owner more carefully (#17699)\n- Update note so avoid misinterpretation (#17701)\n- Docs: Make ``DAG.is_active`` read-only in API (#17667)\n- Update documentation regarding Python 3.9 support (#17611)\n- Fix MySQL database character set instruction (#17603)\n- Document overriding ``XCom.clear`` for data lifecycle management (#17589)\n- Path correction in docs for airflow core (#17567)\n- docs(celery): reworded, add actual multiple queues example (#17541)\n- Doc: Add FAQ to speed up parsing with tons of dag files (#17519)\n- Improve image building documentation for new users (#17409)\n- Doc: Strip unnecessary arguments from MariaDB JIRA URL (#17296)\n- Update warning about MariaDB and multiple schedulers (#17287)\n- Doc: Recommend using same configs on all Airflow components (#17146)\n- Move docs about masking to a new page (#17007)\n- Suggest use of Env vars instead of Airflow Vars in best practices doc (#16926)\n- Docs: Better description for ``pod_template_file`` (#16861)\n- Add Aneesh Joseph as Airflow Committer (#16835)\n- Docs: Added new pipeline example for the tutorial docs (#16548)\n- Remove upstart from docs (#16672)\n- Add new committers: ``Jed`` and ``TP`` (#16671)\n- Docs: Fix ``flask-ouathlib`` to ``flask-oauthlib`` in Upgrading docs (#16320)\n- Docs: Fix creating a connection docs (#16312)\n- Docs: Fix url for ``Elasticsearch`` (#16275)\n- Small improvements for README.md files (#16244)\n- Fix docs for ``dag_concurrency`` (#16177)\n- Check syntactic correctness for code-snippets (#16005)\n- Add proper link for wheel packages in docs. (#15999)\n- Add Docs for ``default_pool`` slots (#15997)\n- Add memory usage warning in quick-start documentation (#15967)\n- Update example ``KubernetesExecutor`` ``git-sync`` pod template file (#15904)\n- Docs: Fix Taskflow API docs (#16574)\n- Added new pipeline example for the tutorial docs (#16084)\n- Updating the DAG docstring to include ``render_template_as_native_obj`` (#16534)\n- Update docs on setting up SMTP (#16523)\n- Docs: Fix API verb from ``POST`` to ``PATCH`` (#16511)\n\nMisc/Internal\n^^^^^^^^^^^^^\n\n- Renaming variables to be consistent with code logic (#18685)\n- Simplify strings previously split across lines (#18679)\n- fix exception string of ``BranchPythonOperator`` (#18623)\n- Add multiple roles when creating users (#18617)\n- Move FABs base Security Manager into Airflow. (#16647)\n- Remove unnecessary css state colors (#18461)\n- Update ``boto3`` to ``<1.19`` (#18389)\n- Improve coverage for ``airflow.security.kerberos module`` (#18258)\n- Fix Amazon Kinesis test (#18337)\n- Fix provider test accessing ``importlib-resources`` (#18228)\n- Silence warnings in tests from using SubDagOperator (#18275)\n- Fix usage of ``range(len())`` to ``enumerate`` (#18174)\n- Test coverage on the autocomplete view (#15943)\n- Add \"packaging\" to core requirements (#18122)\n- Adds LoggingMixins to BaseTrigger (#18106)\n- Fix building docs in ``main`` builds (#18035)\n- Remove upper-limit on ``tenacity`` (#17593)\n- Remove  redundant ``numpy`` dependency (#17594)\n- Bump ``mysql-connector-python`` to latest version (#17596)\n- Make ``pandas`` an optional core dependency (#17575)\n- Add more typing to airflow.utils.helpers (#15582)\n- Chore: Some code cleanup in ``airflow/utils/db.py`` (#17090)\n- Refactor: Remove processor_factory from DAG processing (#16659)\n- Remove AbstractDagFileProcessorProcess from dag processing (#16816)\n- Update TaskGroup typing (#16811)\n- Update ``click`` to 8.x (#16779)\n- Remove remaining Pylint disables (#16760)\n- Remove duplicated try, there is already a try in create_session (#16701)\n- Removes pylint from our toolchain (#16682)\n- Refactor usage of unneeded function call (#16653)\n- Add type annotations to setup.py (#16658)\n- Remove SQLAlchemy <1.4 constraint (#16630) (Note: our dependencies still have a requirement on <1.4)\n- Refactor ``dag.clear`` method (#16086)\n- Use ``DAG_ACTIONS`` constant (#16232)\n- Use updated ``_get_all_non_dag_permissions`` method (#16317)\n- Add updated-name wrappers for built-in FAB methods (#16077)\n- Remove ``TaskInstance.log_filepath`` attribute (#15217)\n- Removes unnecessary function call in ``airflow/www/app.py`` (#15956)\n- Move ``plyvel`` to google provider extra (#15812)\n- Update permission migrations to use new naming scheme (#16400)\n- Use resource and action names for FAB (#16380)\n- Swap out calls to ``find_permission_view_menu`` for ``get_permission`` wrapper (#16377)\n- Fix deprecated default for ``fab_logging_level`` to ``WARNING`` (#18783)\n- Allow running tasks from UI when using ``CeleryKubernetesExecutor`` (#18441)\n\nAirflow 2.1.4 (2021-09-18)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n^^^^^^^^^\n\n- Fix deprecation error message rather than silencing it (#18126)\n- Limit the number of queued dagruns created by the Scheduler (#18065)\n- Fix ``DagRun`` execution order from queued to running not being properly followed (#18061)\n- Fix ``max_active_runs`` not allowing moving of queued dagruns to running (#17945)\n- Avoid redirect loop for users with no permissions (#17838)\n- Avoid endless redirect loop when user has no roles (#17613)\n- Fix log links on graph TI modal (#17862)\n- Hide variable import form if user lacks permission (#18000)\n- Improve dag/task concurrency check (#17786)\n- Fix Clear task instances endpoint resets all DAG runs bug (#17961)\n- Fixes incorrect parameter passed to views (#18083) (#18085)\n- Fix Sentry handler from ``LocalTaskJob`` causing error (#18119)\n- Limit ``colorlog`` version (6.x is incompatible) (#18099)\n- Only show Pause/Unpause tooltip on hover (#17957)\n- Improve graph view load time for dags with open groups (#17821)\n- Increase width for Run column (#17817)\n- Fix wrong query on running tis (#17631)\n- Add root to tree refresh url (#17633)\n- Do not delete running DAG from the UI (#17630)\n- Improve discoverability of Provider packages' functionality\n- Do not let ``create_dagrun`` overwrite explicit ``run_id`` (#17728)\n- Regression on pid reset to allow task start after heartbeat (#17333)\n- Set task state to failed when pod is DELETED while running (#18095)\n- Advises the kernel to not cache log files generated by Airflow (#18054)\n- Sort adopted tasks in ``_check_for_stalled_adopted_tasks`` method (#18208)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Update version added fields in airflow/config_templates/config.yml (#18128)\n- Improve the description of how to handle dynamic task generation (#17963)\n- Improve cross-links to operators and hooks references (#17622)\n- Doc: Fix replacing Airflow version for Docker stack (#17711)\n- Make the providers operators/hooks reference much more usable (#17768)\n- Update description about the new ``connection-types`` provider meta-data\n- Suggest to use secrets backend for variable when it contains sensitive data (#17319)\n- Separate Installing from sources section and add more details (#18171)\n- Doc: Use ``closer.lua`` script for downloading sources (#18179)\n- Doc: Improve installing from sources (#18194)\n- Improves installing from sources pages for all components (#18251)\n\nAirflow 2.1.3 (2021-08-23)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n^^^^^^^^^\n\n- Fix task retries when they receive ``sigkill`` and have retries and properly handle ``sigterm`` (#16301)\n- Fix redacting secrets in context exceptions. (#17618)\n- Fix race condition with dagrun callbacks (#16741)\n- Add 'queued' to DagRunState (#16854)\n- Add 'queued' state to DagRun (#16401)\n- Fix external elasticsearch logs link (#16357)\n- Add proper warning message when recorded PID is different from current PID (#17411)\n- Fix running tasks with ``default_impersonation`` config (#17229)\n- Rescue if a DagRun's DAG was removed from db (#17544)\n- Fixed broken json_client (#17529)\n- Handle and log exceptions raised during task callback (#17347)\n- Fix CLI ``kubernetes cleanup-pods`` which fails on invalid label key (#17298)\n- Show serialization exceptions in DAG parsing log (#17277)\n- Fix: ``TaskInstance`` does not show ``queued_by_job_id`` & ``external_executor_id`` (#17179)\n- Adds more explanatory message when ``SecretsMasker`` is not configured (#17101)\n- Enable the use of ``__init_subclass__`` in subclasses of ``BaseOperator`` (#17027)\n- Fix task instance retrieval in XCom view (#16923)\n- Validate type of ``priority_weight`` during parsing (#16765)\n- Correctly handle custom ``deps`` and ``task_group`` during DAG Serialization (#16734)\n- Fix slow (cleared) tasks being be adopted by Celery worker. (#16718)\n- Fix calculating duration in tree view (#16695)\n- Fix ``AttributeError``: ``datetime.timezone`` object has no attribute ``name`` (#16599)\n- Redact conn secrets in webserver logs (#16579)\n- Change graph focus to top of view instead of center (#16484)\n- Fail tasks in scheduler when executor reports they failed (#15929)\n- fix(smart_sensor): Unbound variable errors (#14774)\n- Add back missing permissions to ``UserModelView`` controls. (#17431)\n- Better diagnostics and self-healing of docker-compose (#17484)\n- Improve diagnostics message when users have ``secret_key`` misconfigured (#17410)\n- Stop checking ``execution_date`` in ``task_instance.refresh_from_db`` (#16809)\n\nImprovements\n^^^^^^^^^^^^\n\n- Run mini scheduler in ``LocalTaskJob`` during task exit (#16289)\n- Remove ``SQLAlchemy<1.4`` constraint (#16630)\n- Bump Jinja2 upper-bound from 2.12.0 to 4.0.0 (#16595)\n- Bump ``dnspython`` (#16698)\n- Updates to ``FlaskAppBuilder`` 3.3.2+ (#17208)\n- Add State types for tasks and DAGs (#15285)\n- Set Process title for Worker when using ``LocalExecutor`` (#16623)\n- Move ``DagFileProcessor`` and ``DagFileProcessorProcess`` out of ``scheduler_job.py`` (#16581)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Fix inconsistencies in configuration docs (#17317)\n- Fix docs link for using SQLite as Metadata DB (#17308)\n\nMisc\n^^^^\n\n- Switch back http provider after requests removes LGPL dependency (#16974)\n\nAirflow 2.1.2 (2021-07-14)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n^^^^^^^^^\n\n- Only allow webserver to request from the worker log server (#16754)\n- Fix \"Invalid JSON configuration, must be a dict\" bug (#16648)\n- Fix ``CeleryKubernetesExecutor`` (#16700)\n- Mask value if the key is ``token`` (#16474)\n- Fix impersonation issue with ``LocalTaskJob`` (#16852)\n- Resolve all npm vulnerabilities including bumping ``jQuery`` to ``3.5`` (#16440)\n\nMisc\n^^^^\n\n- Add Python 3.9 support (#15515)\n\n\nAirflow 2.1.1 (2021-07-02)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\n``activate_dag_runs`` argument of the function ``clear_task_instances`` is replaced with ``dag_run_state``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nTo achieve the previous default behaviour of ``clear_task_instances`` with ``activate_dag_runs=True``\\ , no change is needed. To achieve the previous behaviour of ``activate_dag_runs=False``\\ , pass ``dag_run_state=False`` instead. (The previous parameter is still accepted, but is deprecated)\n\n``dag.set_dag_runs_state`` is deprecated\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe method ``set_dag_runs_state`` is no longer needed after a bug fix in PR: `#15382 <https://github.com/apache/airflow/pull/15382>`_. This method is now deprecated and will be removed in a future version.\n\nBug Fixes\n^^^^^^^^^\n\n- Don't crash attempting to mask secrets in dict with non-string keys (#16601)\n- Always install sphinx_airflow_theme from ``PyPI`` (#16594)\n- Remove limitation for elasticsearch library (#16553)\n- Adding extra requirements for build and runtime of the PROD image. (#16170)\n- Cattrs 1.7.0 released by the end of May 2021 break lineage usage (#16173)\n- Removes unnecessary packages from setup_requires (#16139)\n- Pins docutils to <0.17 until breaking behaviour is fixed (#16133)\n- Improvements for Docker Image docs (#14843)\n- Ensure that ``dag_run.conf`` is a dict (#15057)\n- Fix CLI connections import and migrate logic from secrets to Connection model (#15425)\n- Fix Dag Details start date bug (#16206)\n- Fix DAG run state not updated while DAG is paused (#16343)\n- Allow null value for operator field in task_instance schema(REST API) (#16516)\n- Avoid recursion going too deep when redacting logs (#16491)\n- Backfill: Don't create a DagRun if no tasks match task regex (#16461)\n- Tree View UI for larger DAGs & more consistent spacing in Tree View (#16522)\n- Correctly handle None returns from Query.scalar() (#16345)\n- Adding ``only_active`` parameter to /dags endpoint (#14306)\n- Don't show stale Serialized DAGs if they are deleted in DB (#16368)\n- Make REST API List DAGs endpoint consistent with UI/CLI behaviour (#16318)\n- Support remote logging in elasticsearch with ``filebeat 7`` (#14625)\n- Queue tasks with higher priority and earlier execution_date first. (#15210)\n- Make task ID on legend have enough width and width of line chart to be 100%.  (#15915)\n- Fix normalize-url vulnerability (#16375)\n- Validate retries value on init for better errors (#16415)\n- add num_runs query param for tree refresh (#16437)\n- Fix templated default/example values in config ref docs (#16442)\n- Add ``passphrase`` and ``private_key`` to default sensitive field names (#16392)\n- Fix tasks in an infinite slots pool were never scheduled (#15247)\n- Fix Orphaned tasks stuck in CeleryExecutor as running (#16550)\n- Don't fail to log if we can't redact something (#16118)\n- Set max tree width to 1200 pixels (#16067)\n- Fill the \"job_id\" field for ``airflow task run`` without ``--local``/``--raw`` for KubeExecutor (#16108)\n- Fixes problem where conf variable was used before initialization (#16088)\n- Fix apply defaults for task decorator (#16085)\n- Parse recently modified files even if just parsed (#16075)\n- Ensure that we don't try to mask empty string in logs (#16057)\n- Don't die when masking ``log.exception`` when there is no exception (#16047)\n- Restores apply_defaults import in base_sensor_operator (#16040)\n- Fix auto-refresh in tree view When webserver ui is not in ``/`` (#16018)\n- Fix dag.clear() to set multiple dags to running when necessary (#15382)\n- Fix Celery executor getting stuck randomly because of reset_signals in multiprocessing (#15989)\n\n\nAirflow 2.1.0 (2021-05-21)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNew \"deprecated_api\" extra\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWe have a new '[deprecated_api]' extra that should be used when installing airflow when the deprecated API\nis going to be used. This is now an optional feature of Airflow now because it pulls in ``requests`` which\n(as of 14 May 2021) pulls LGPL ``chardet`` dependency.\n\nThe ``http`` provider is not installed by default\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe ``http`` provider is now optional and not installed by default, until ``chardet`` becomes an optional\ndependency of ``requests``.\nSee `PR to replace chardet with charset-normalizer <https://github.com/psf/requests/pull/5797>`_\n\n``@apply_default`` decorator isn't longer necessary\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThis decorator is now automatically added to all operators via the metaclass on BaseOperator\n\nChange the configuration options for field masking\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWe've improved masking for sensitive data in Web UI and logs. As part of it, the following configurations have been changed:\n\n\n* ``hide_sensitive_variable_fields`` option in ``admin`` section has been replaced by ``hide_sensitive_var_conn_fields`` section in ``core`` section,\n* ``sensitive_variable_fields`` option in ``admin`` section has been replaced by ``sensitive_var_conn_names`` section in ``core`` section.\n\nDeprecated PodDefaults and add_xcom_sidecar in airflow.kubernetes.pod_generator\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWe have moved PodDefaults from ``airflow.kubernetes.pod_generator.PodDefaults`` to\n``airflow.providers.cncf.kubernetes.utils.xcom_sidecar.PodDefaults`` and moved add_xcom_sidecar\nfrom ``airflow.kubernetes.pod_generator.PodGenerator.add_xcom_sidecar``\\ to\n``airflow.providers.cncf.kubernetes.utils.xcom_sidecar.add_xcom_sidecar``.\nThis change will allow us to modify the KubernetesPodOperator XCom functionality without requiring airflow upgrades.\n\nRemoved pod_launcher from core airflow\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nMoved the pod launcher from ``airflow.kubernetes.pod_launcher`` to ``airflow.providers.cncf.kubernetes.utils.pod_launcher``\n\nThis will allow users to update the pod_launcher for the KubernetesPodOperator without requiring an airflow upgrade\n\nDefault ``[webserver] worker_refresh_interval`` is changed to ``6000`` seconds\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe default value for ``[webserver] worker_refresh_interval`` was ``30`` seconds for\nAirflow <=2.0.1. However, since Airflow 2.0 DAG Serialization is a hard requirement\nand the Webserver used the serialized DAGs, there is no need to kill an existing\nworker and create a new one as frequently as ``30`` seconds.\n\nThis setting can be raised to an even higher value, currently it is\nset to ``6000`` seconds (100 minutes) to\nserve as a DagBag cache burst time.\n\n``default_queue`` configuration has been moved to the ``operators`` section.\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe ``default_queue`` configuration option has been moved from ``[celery]`` section to ``[operators]`` section to allow for reuse between different executors.\n\nNew Features\n^^^^^^^^^^^^\n\n- Add ``PythonVirtualenvDecorator`` to Taskflow API (#14761)\n- Add ``Taskgroup`` decorator (#15034)\n- Create a DAG Calendar View (#15423)\n- Create cross-DAG dependencies view (#13199)\n- Add rest API to query for providers (#13394)\n- Mask passwords and sensitive info in task logs and UI (#15599)\n- Add ``SubprocessHook`` for running commands from operators (#13423)\n- Add DAG Timeout in UI page \"DAG Details\" (#14165)\n- Add ``WeekDayBranchOperator`` (#13997)\n- Add JSON linter to DAG Trigger UI (#13551)\n- Add DAG Description Doc to Trigger UI Page (#13365)\n- Add airflow webserver URL into SLA miss email. (#13249)\n- Add read only REST API endpoints for users (#14735)\n- Add files to generate Airflow's Python SDK (#14739)\n- Add dynamic fields to snowflake connection (#14724)\n- Add read only REST API endpoint for roles and permissions (#14664)\n- Add new datetime branch operator (#11964)\n- Add Google leveldb hook and operator (#13109) (#14105)\n- Add plugins endpoint to the REST API (#14280)\n- Add ``worker_pod_pending_timeout`` support (#15263)\n- Add support for labeling DAG edges (#15142)\n- Add CUD REST API endpoints for Roles (#14840)\n- Import connections from a file (#15177)\n- A bunch of ``template_fields_renderers`` additions (#15130)\n- Add REST API query sort and order to some endpoints (#14895)\n- Add timezone context in new ui (#15096)\n- Add query mutations to new UI (#15068)\n- Add different modes to sort dag files for parsing (#15046)\n- Auto refresh on Tree View (#15474)\n- BashOperator to raise ``AirflowSkipException`` on exit code 99 (by default, configurable) (#13421) (#14963)\n- Clear tasks by task ids in REST API (#14500)\n- Support jinja2 native Python types (#14603)\n- Allow celery workers without gossip or mingle modes (#13880)\n- Add ``airflow jobs check`` CLI command to check health of jobs (Scheduler etc) (#14519)\n- Rename ``DateTimeBranchOperator`` to ``BranchDateTimeOperator`` (#14720)\n\nImprovements\n^^^^^^^^^^^^\n\n- Add optional result handler callback to ``DbApiHook`` (#15581)\n- Update Flask App Builder limit to recently released 3.3 (#15792)\n- Prevent creating flask sessions on REST API requests (#15295)\n- Sync DAG specific permissions when parsing (#15311)\n- Increase maximum length of pool name on Tasks to 256 characters (#15203)\n- Enforce READ COMMITTED isolation when using mysql (#15714)\n- Auto-apply ``apply_default`` to subclasses of ``BaseOperator`` (#15667)\n- Emit error on duplicated DAG ID (#15302)\n- Update ``KubernetesExecutor`` pod templates to allow access to IAM permissions (#15669)\n- More verbose logs when running ``airflow db check-migrations`` (#15662)\n- When one_success mark task as failed if no success (#15467)\n- Add an option to trigger a dag w/o changing conf (#15591)\n- Add Airflow UI instance_name configuration option (#10162)\n- Add a decorator to retry functions with DB transactions (#14109)\n- Add return to PythonVirtualenvOperator's execute method (#14061)\n- Add verify_ssl config for kubernetes (#13516)\n- Add description about ``secret_key`` when Webserver > 1 (#15546)\n- Add Traceback in LogRecord in ``JSONFormatter`` (#15414)\n- Add support for arbitrary json in conn uri format (#15100)\n- Adds description field in variable (#12413) (#15194)\n- Add logs to show last modified in SFTP, FTP and Filesystem sensor (#15134)\n- Execute ``on_failure_callback`` when SIGTERM is received (#15172)\n- Allow hiding of all edges when highlighting states (#15281)\n- Display explicit error in case UID has no actual username (#15212)\n- Serve logs with Scheduler when using Local or Sequential Executor (#15557)\n- Deactivate trigger, refresh, and delete controls on dag detail view. (#14144)\n- Turn off autocomplete for connection forms (#15073)\n- Increase default ``worker_refresh_interval`` to ``6000`` seconds (#14970)\n- Only show User's local timezone if it's not UTC (#13904)\n- Suppress LOG/WARNING for a few tasks CLI for better CLI experience (#14567)\n- Configurable API response (CORS) headers (#13620)\n- Allow viewers to see all docs links (#14197)\n- Update Tree View date ticks (#14141)\n- Make the tooltip to Pause / Unpause a DAG clearer (#13642)\n- Warn about precedence of env var when getting variables (#13501)\n- Move ``[celery] default_queue`` config to ``[operators] default_queue`` to reuse between executors  (#14699)\n\nBug Fixes\n^^^^^^^^^\n\n- Fix 500 error from ``updateTaskInstancesState`` API endpoint when ``dry_run`` not passed (#15889)\n- Ensure that task preceding a PythonVirtualenvOperator doesn't fail (#15822)\n- Prevent mixed case env vars from crashing processes like worker (#14380)\n- Fixed type annotations in DAG decorator (#15778)\n- Fix on_failure_callback when task receive SIGKILL (#15537)\n- Fix dags table overflow (#15660)\n- Fix changing the parent dag state on subdag clear (#15562)\n- Fix reading from zip package to default to text (#13962)\n- Fix wrong parameter for ``drawDagStatsForDag`` in dags.html (#13884)\n- Fix QueuedLocalWorker crashing with EOFError (#13215)\n- Fix typo in ``NotPreviouslySkippedDep`` (#13933)\n- Fix parallelism after KubeExecutor pod adoption (#15555)\n- Fix kube client on mac with keepalive enabled (#15551)\n- Fixes wrong limit for dask for python>3.7 (should be <3.7) (#15545)\n- Fix Task Adoption in ``KubernetesExecutor`` (#14795)\n- Fix timeout when using XCom with ``KubernetesPodOperator`` (#15388)\n- Fix deprecated provider aliases in \"extras\" not working (#15465)\n- Fixed default XCom deserialization. (#14827)\n- Fix used_group_ids in ``dag.partial_subset`` (#13700) (#15308)\n- Further fix trimmed ``pod_id`` for ``KubernetesPodOperator`` (#15445)\n- Bugfix: Invalid name when trimmed ``pod_id`` ends with hyphen in ``KubernetesPodOperator`` (#15443)\n- Fix incorrect slots stats when TI ``pool_slots > 1`` (#15426)\n- Fix DAG last run link (#15327)\n- Fix ``sync-perm`` to work correctly when update_fab_perms = False (#14847)\n- Fixes limits on Arrow for plexus test (#14781)\n- Fix UI bugs in tree view (#14566)\n- Fix AzureDataFactoryHook failing to instantiate its connection (#14565)\n- Fix permission error on non-POSIX filesystem (#13121)\n- Fix spelling in \"ignorable\" (#14348)\n- Fix get_context_data doctest import (#14288)\n- Correct typo in ``GCSObjectsWtihPrefixExistenceSensor``  (#14179)\n- Fix order of failed deps (#14036)\n- Fix critical ``CeleryKubernetesExecutor`` bug (#13247)\n- Fix four bugs in ``StackdriverTaskHandler`` (#13784)\n- ``func.sum`` may return ``Decimal`` that break rest APIs (#15585)\n- Persist tags params in pagination (#15411)\n- API: Raise ``AlreadyExists`` exception when the ``execution_date`` is same (#15174)\n- Remove duplicate call to ``sync_metadata`` inside ``DagFileProcessorManager`` (#15121)\n- Extra ``docker-py`` update to resolve docker op issues (#15731)\n- Ensure executors end method is called (#14085)\n- Remove ``user_id`` from API schema (#15117)\n- Prevent clickable bad links on disabled pagination (#15074)\n- Acquire lock on db for the time of migration (#10151)\n- Skip SLA check only if SLA is None (#14064)\n- Print right version in airflow info command (#14560)\n- Make ``airflow info`` work with pipes (#14528)\n- Rework client-side script for connection form. (#14052)\n- API: Add ``CollectionInfo`` in all Collections that have ``total_entries`` (#14366)\n- Fix ``task_instance_mutation_hook`` when importing airflow.models.dagrun (#15851)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Fix docstring of SqlSensor (#15466)\n- Small changes on \"DAGs and Tasks documentation\" (#14853)\n- Add note on changes to configuration options (#15696)\n- Add docs to the ``markdownlint`` and ``yamllint`` config files (#15682)\n- Rename old \"Experimental\" API to deprecated in the docs. (#15653)\n- Fix documentation error in ``git_sync_template.yaml`` (#13197)\n- Fix doc link permission name (#14972)\n- Fix link to Helm chart docs (#14652)\n- Fix docstrings for Kubernetes code (#14605)\n- docs: Capitalize & minor fixes (#14283) (#14534)\n- Fixed reading from zip package to default to text. (#13984)\n- An initial rework of the \"Concepts\" docs (#15444)\n- Improve docstrings for various modules (#15047)\n- Add documentation on database connection URI (#14124)\n- Add Helm Chart logo to docs index (#14762)\n- Create a new documentation package for Helm Chart (#14643)\n- Add docs about supported logging levels (#14507)\n- Update docs about tableau and salesforce provider (#14495)\n- Replace deprecated doc links to the correct one (#14429)\n- Refactor redundant doc url logic to use utility (#14080)\n- docs: NOTICE: Updated 2016-2019 to 2016-now (#14248)\n- Skip DAG perm sync during parsing if possible (#15464)\n- Add picture and examples for Edge Labels (#15310)\n- Add example DAG & how-to guide for sqlite (#13196)\n- Add links to new modules for deprecated modules (#15316)\n- Add note in Updating.md about FAB data model change (#14478)\n\nMisc/Internal\n^^^^^^^^^^^^^\n\n- Fix ``logging.exception`` redundancy (#14823)\n- Bump ``stylelint`` to remove vulnerable sub-dependency (#15784)\n- Add resolution to force dependencies to use patched version of lodash (#15777)\n- Update croniter to 1.0.x series (#15769)\n- Get rid of Airflow 1.10 in Breeze (#15712)\n- Run helm chart tests in parallel (#15706)\n- Bump ``ssri`` from 6.0.1 to 6.0.2 in /airflow/www (#15437)\n- Remove the limit on Gunicorn dependency (#15611)\n- Better \"dependency already registered\" warning message for tasks #14613 (#14860)\n- Pin pandas-gbq to <0.15.0 (#15114)\n- Use Pip 21.* to install airflow officially (#15513)\n- Bump mysqlclient to support the 1.4.x and 2.x series (#14978)\n- Finish refactor of DAG resource name helper (#15511)\n- Refactor/Cleanup Presentation of Graph Task and Path Highlighting (#15257)\n- Standardize default fab perms (#14946)\n- Remove ``datepicker`` for task instance detail view (#15284)\n- Turn provider's import warnings into debug logs (#14903)\n- Remove left-over fields from required in provider_info schema. (#14119)\n- Deprecate ``tableau`` extra (#13595)\n- Use built-in ``cached_property`` on Python 3.8 where possible (#14606)\n- Clean-up JS code in UI templates (#14019)\n- Bump elliptic from 6.5.3 to 6.5.4 in /airflow/www (#14668)\n- Switch to f-strings using ``flynt``. (#13732)\n- use ``jquery`` ready instead of vanilla js (#15258)\n- Migrate task instance log (ti_log) js (#15309)\n- Migrate graph js (#15307)\n- Migrate dags.html javascript (#14692)\n- Removes unnecessary AzureContainerInstance connection type (#15514)\n- Separate Kubernetes pod_launcher from core airflow (#15165)\n- update remaining old import paths of operators (#15127)\n- Remove broken and undocumented \"demo mode\" feature (#14601)\n- Simplify configuration/legibility of ``Webpack`` entries (#14551)\n- remove inline tree js (#14552)\n- Js linting and inline migration for simple scripts (#14215)\n- Remove use of repeated constant in AirflowConfigParser (#14023)\n- Deprecate email credentials from environment variables. (#13601)\n- Remove unused 'context' variable in task_instance.py (#14049)\n- Disable suppress_logs_and_warning in cli when debugging (#13180)\n\n\nAirflow 2.0.2 (2021-04-19)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nDefault ``[kubernetes] enable_tcp_keepalive`` is changed to ``True``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThis allows Airflow to work more reliably with some environments (like Azure) by default.\n\n``sync-perm`` CLI no longer syncs DAG specific permissions by default\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe ``sync-perm`` CLI command will no longer sync DAG specific permissions by default as they are now being handled during\nDAG parsing. If you need or want the old behavior, you can pass ``--include-dags`` to have ``sync-perm`` also sync DAG\nspecific permissions.\n\nBug Fixes\n^^^^^^^^^\n\n* Bugfix: ``TypeError`` when Serializing & sorting iterable properties of DAGs (#15395)\n* Fix missing ``on_load`` trigger for folder-based plugins (#15208)\n* ``kubernetes cleanup-pods`` subcommand will only clean up Airflow-created Pods (#15204)\n* Fix password masking in CLI action_logging (#15143)\n* Fix url generation for TriggerDagRunOperatorLink (#14990)\n* Restore base lineage backend (#14146)\n* Unable to trigger backfill or manual jobs with Kubernetes executor. (#14160)\n* Bugfix: Task docs are not shown in the Task Instance Detail View (#15191)\n* Bugfix: Fix overriding ``pod_template_file`` in KubernetesExecutor (#15197)\n* Bugfix: resources in ``executor_config`` breaks Graph View in UI (#15199)\n* Fix celery executor bug trying to call len on map (#14883)\n* Fix bug in airflow.stats timing that broke dogstatsd mode (#15132)\n* Avoid scheduler/parser manager deadlock by using non-blocking IO (#15112)\n* Re-introduce ``dagrun.schedule_delay`` metric (#15105)\n* Compare string values, not if strings are the same object in Kube executor(#14942)\n* Pass queue to BaseExecutor.execute_async like in airflow 1.10 (#14861)\n* Scheduler: Remove TIs from starved pools from the critical path. (#14476)\n* Remove extra/needless deprecation warnings from airflow.contrib module (#15065)\n* Fix support for long dag_id and task_id in KubernetesExecutor (#14703)\n* Sort lists, sets and tuples in Serialized DAGs (#14909)\n* Simplify cleaning string passed to origin param (#14738) (#14905)\n* Fix error when running tasks with Sentry integration enabled. (#13929)\n* Webserver: Sanitize string passed to origin param (#14738)\n* Fix losing duration < 1 secs in tree (#13537)\n* Pin SQLAlchemy to <1.4 due to breakage of sqlalchemy-utils (#14812)\n* Fix KubernetesExecutor issue with deleted pending pods (#14810)\n* Default to Celery Task model when backend model does not exist (#14612)\n* Bugfix: Plugins endpoint was unauthenticated (#14570)\n* BugFix: fix DAG doc display (especially for TaskFlow DAGs) (#14564)\n* BugFix: TypeError in airflow.kubernetes.pod_launcher's monitor_pod (#14513)\n* Bugfix: Fix wrong output of tags and owners in dag detail API endpoint (#14490)\n* Fix logging error with task error when JSON logging is enabled (#14456)\n* Fix StatsD metrics not sending when using daemon mode (#14454)\n* Gracefully handle missing start_date and end_date for DagRun (#14452)\n* BugFix: Serialize max_retry_delay as a timedelta (#14436)\n* Fix crash when user clicks on  \"Task Instance Details\" caused by start_date being None (#14416)\n* BugFix: Fix TaskInstance API call fails if a task is removed from running DAG (#14381)\n* Scheduler should not fail when invalid ``executor_config`` is passed (#14323)\n* Fix bug allowing task instances to survive when dagrun_timeout is exceeded (#14321)\n* Fix bug where DAG timezone was not always shown correctly in UI tooltips (#14204)\n* Use ``Lax`` for ``cookie_samesite`` when empty string is passed (#14183)\n* [AIRFLOW-6076] fix ``dag.cli()`` KeyError (#13647)\n* Fix running child tasks in a subdag after clearing a successful subdag (#14776)\n\nImprovements\n^^^^^^^^^^^^\n\n* Remove unused JS packages causing false security alerts (#15383)\n* Change default of ``[kubernetes] enable_tcp_keepalive`` for new installs to ``True`` (#15338)\n* Fixed #14270: Add error message in OOM situations (#15207)\n* Better compatibility/diagnostics for arbitrary UID in docker image (#15162)\n* Updates 3.6 limits for latest versions of a few libraries (#15209)\n* Adds Blinker dependency which is missing after recent changes (#15182)\n* Remove 'conf' from search_columns in DagRun View (#15099)\n* More proper default value for namespace in K8S cleanup-pods CLI (#15060)\n* Faster default role syncing during webserver start (#15017)\n* Speed up webserver start when there are many DAGs (#14993)\n* Much easier to use and better documented Docker image (#14911)\n* Use ``libyaml`` C library when available. (#14577)\n* Don't create unittest.cfg when not running in unit test mode (#14420)\n* Webserver: Allow Filtering TaskInstances by queued_dttm (#14708)\n* Update Flask-AppBuilder dependency to allow 3.2 (and all 3.x series) (#14665)\n* Remember expanded task groups in browser local storage (#14661)\n* Add plain format output to cli tables (#14546)\n* Make ``airflow dags show`` command display TaskGroups (#14269)\n* Increase maximum size of ``extra`` connection field. (#12944)\n* Speed up clear_task_instances by doing a single sql delete for TaskReschedule (#14048)\n* Add more flexibility with FAB menu links (#13903)\n* Add better description and guidance in case of sqlite version mismatch (#14209)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n* Add documentation create/update community providers (#15061)\n* Fix mistake and typos in airflow.utils.timezone docstrings (#15180)\n* Replace new url for Stable Airflow Docs (#15169)\n* Docs: Clarify behavior of delete_worker_pods_on_failure (#14958)\n* Create a documentation package for Docker image (#14846)\n* Multiple minor doc (OpenAPI) fixes (#14917)\n* Replace Graph View Screenshot to show Auto-refresh (#14571)\n\nMisc/Internal\n^^^^^^^^^^^^^\n\n* Import Connection lazily in hooks to avoid cycles (#15361)\n* Rename last_scheduler_run into last_parsed_time, and ensure it's updated in DB (#14581)\n* Make TaskInstance.pool_slots not nullable with a default of 1 (#14406)\n* Log migrations info in consistent way (#14158)\n\nAirflow 2.0.1 (2021-02-08)\n--------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nPermission to view Airflow Configurations has been removed from ``User`` and ``Viewer`` role\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPreviously, Users with ``User`` or ``Viewer`` role were able to get/view configurations using\nthe REST API or in the Webserver. From Airflow 2.0.1, only users with ``Admin`` or ``Op`` role would be able\nto get/view Configurations.\n\nTo allow users with other roles to view configuration, add ``can read on Configurations`` permissions to that role.\n\nNote that if ``[webserver] expose_config`` is set to ``False``\\ , the API will throw a ``403`` response even if\nthe user has role with ``can read on Configurations`` permission.\n\nDefault ``[celery] worker_concurrency`` is changed to ``16``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe default value for ``[celery] worker_concurrency`` was ``16`` for Airflow <2.0.0.\nHowever, it was unintentionally changed to ``8`` in 2.0.0.\n\nFrom Airflow 2.0.1, we revert to the old default of ``16``.\n\nDefault ``[scheduler] min_file_process_interval`` is changed to ``30``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe default value for ``[scheduler] min_file_process_interval`` was ``0``\\ ,\ndue to which the CPU Usage mostly stayed around 100% as the DAG files are parsed\nconstantly.\n\nFrom Airflow 2.0.0, the scheduling decisions have been moved from\nDagFileProcessor to Scheduler, so we can keep the default a bit higher: ``30``.\n\nBug Fixes\n^^^^^^^^^\n\n- Bugfix: Return XCom Value in the XCom Endpoint API (#13684)\n- Bugfix: Import error when using custom backend and ``sql_alchemy_conn_secret`` (#13260)\n- Allow PID file path to be relative when daemonize a process (scheduler, kerberos, etc) (#13232)\n- Bugfix: no generic ``DROP CONSTRAINT`` in MySQL during ``airflow db upgrade`` (#13239)\n- Bugfix: Sync Access Control defined in DAGs when running ``sync-perm`` (#13377)\n- Stop sending Callback Requests if no callbacks are defined on DAG (#13163)\n- BugFix: Dag-level Callback Requests were not run (#13651)\n- Stop creating duplicate Dag File Processors (#13662)\n- Filter DagRuns with Task Instances in removed State while Scheduling (#13165)\n- Bump ``datatables.net`` from 1.10.21 to 1.10.22 in /airflow/www (#13143)\n- Bump ``datatables.net`` JS to 1.10.23 (#13253)\n- Bump ``dompurify`` from 2.0.12 to 2.2.6 in /airflow/www (#13164)\n- Update minimum ``cattrs`` version (#13223)\n- Remove inapplicable arg 'output' for CLI pools import/export (#13071)\n- Webserver: Fix the behavior to deactivate the authentication option and add docs (#13191)\n- Fix: add support for no-menu plugin views (#11742)\n- Add ``python-daemon`` limit for Python 3.8+ to fix daemon crash (#13540)\n- Change the default celery ``worker_concurrency`` to 16 (#13612)\n- Audit Log records View should not contain link if ``dag_id`` is None (#13619)\n- Fix invalid ``continue_token`` for cleanup list pods (#13563)\n- Switches to latest version of snowflake connector (#13654)\n- Fix backfill crash on task retry or reschedule (#13712)\n- Setting ``max_tis_per_query`` to ``0`` now correctly removes the limit (#13512)\n- Fix race conditions in task callback invocations (#10917)\n- Fix webserver exiting when gunicorn master crashes (#13518)(#13780)\n- Fix SQL syntax to check duplicate connections (#13783)\n- ``BaseBranchOperator`` will push to xcom by default (#13704) (#13763)\n- Fix Deprecation for ``configuration.getsection`` (#13804)\n- Fix TaskNotFound in log endpoint (#13872)\n- Fix race condition when using Dynamic DAGs (#13893)\n- Fix: Linux/Chrome window bouncing in Webserver\n- Fix db shell for sqlite (#13907)\n- Only compare updated time when Serialized DAG exists (#13899)\n- Fix dag run type enum query for mysqldb driver (#13278)\n- Add authentication to lineage endpoint for experimental API (#13870)\n- Do not add User role perms to custom roles. (#13856)\n- Do not add ``Website.can_read`` access to default roles. (#13923)\n- Fix invalid value error caused by long Kubernetes pod name (#13299)\n- Fix DB Migration for SQLite to upgrade to 2.0 (#13921)\n- Bugfix: Manual DagRun trigger should not skip scheduled runs (#13963)\n- Stop loading Extra Operator links in Scheduler (#13932)\n- Added missing return parameter in read function of ``FileTaskHandler`` (#14001)\n- Bugfix: Do not try to create a duplicate Dag Run in Scheduler (#13920)\n- Make ``v1/config`` endpoint respect webserver ``expose_config`` setting (#14020)\n- Disable row level locking for Mariadb and MySQL <8 (#14031)\n- Bugfix: Fix permissions to triggering only specific DAGs (#13922)\n- Fix broken SLA Mechanism (#14056)\n- Bugfix: Scheduler fails if task is removed at runtime (#14057)\n- Remove permissions to read Configurations for User and Viewer roles (#14067)\n- Fix DB Migration from 2.0.1rc1\n\nImprovements\n^^^^^^^^^^^^\n\n- Increase the default ``min_file_process_interval`` to decrease CPU Usage (#13664)\n- Dispose connections when running tasks with ``os.fork`` & ``CeleryExecutor`` (#13265)\n- Make function purpose clearer in ``example_kubernetes_executor`` example dag (#13216)\n- Remove unused libraries - ``flask-swagger``, ``funcsigs`` (#13178)\n- Display alternative tooltip when a Task has yet to run (no TI) (#13162)\n- User werkzeug's own type conversion for request args (#13184)\n- UI: Add ``queued_by_job_id`` & ``external_executor_id`` Columns to TI View (#13266)\n- Make ``json-merge-patch`` an optional library and unpin it (#13175)\n- Adds missing LDAP \"extra\" dependencies to ldap provider. (#13308)\n- Refactor ``setup.py`` to better reflect changes in providers (#13314)\n- Pin ``pyjwt`` and Add integration tests for Apache Pinot (#13195)\n- Removes provider-imposed requirements from ``setup.cfg`` (#13409)\n- Replace deprecated decorator (#13443)\n- Streamline & simplify ``__eq__`` methods in models Dag and BaseOperator (#13449)\n- Additional properties should be allowed in provider schema (#13440)\n- Remove unused dependency - ``contextdecorator`` (#13455)\n- Remove 'typing' dependency (#13472)\n- Log migrations info in consistent way (#13458)\n- Unpin ``mysql-connector-python`` to allow ``8.0.22`` (#13370)\n- Remove thrift as a core dependency (#13471)\n- Add ``NotFound`` response for DELETE methods in OpenAPI YAML (#13550)\n- Stop Log Spamming when ``[core] lazy_load_plugins`` is ``False`` (#13578)\n- Display message and docs link when no plugins are loaded (#13599)\n- Unpin restriction for ``colorlog`` dependency (#13176)\n- Add missing Dag Tag for Example DAGs (#13665)\n- Support tables in DAG docs (#13533)\n- Add ``python3-openid`` dependency (#13714)\n- Add ``__repr__`` for Executors (#13753)\n- Add description to hint if ``conn_type`` is missing (#13778)\n- Upgrade Azure blob to v12 (#12188)\n- Add extra field to ``get_connnection`` REST endpoint (#13885)\n- Make Smart Sensors DB Migration idempotent (#13892)\n- Improve the error when DAG does not exist when running dag pause command (#13900)\n- Update ``airflow_local_settings.py`` to fix an error message (#13927)\n- Only allow passing JSON Serializable conf to ``TriggerDagRunOperator`` (#13964)\n- Bugfix: Allow getting details of a DAG with null ``start_date`` (REST API) (#13959)\n- Add params to the DAG details endpoint (#13790)\n- Make the role assigned to anonymous users customizable (#14042)\n- Retry critical methods in Scheduler loop in case of ``OperationalError`` (#14032)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Add Missing StatsD Metrics in Docs (#13708)\n- Add Missing Email configs in Configuration doc (#13709)\n- Add quick start for Airflow on Docker (#13660)\n- Describe which Python versions are supported (#13259)\n- Add note block to 2.x migration docs (#13094)\n- Add documentation about webserver_config.py (#13155)\n- Add missing version information to recently added configs (#13161)\n- API: Use generic information in UpdateMask component (#13146)\n- Add Airflow 2.0.0 to requirements table (#13140)\n- Avoid confusion in doc for CeleryKubernetesExecutor (#13116)\n- Update docs link in REST API spec (#13107)\n- Add link to PyPI Repository to provider docs (#13064)\n- Fix link to Airflow master branch documentation (#13179)\n- Minor enhancements to Sensors docs (#13381)\n- Use 2.0.0 in Airflow docs & Breeze (#13379)\n- Improves documentation regarding providers and custom connections (#13375)(#13410)\n- Fix malformed table in production-deployment.rst (#13395)\n- Update celery.rst to fix broken links (#13400)\n- Remove reference to scheduler run_duration param in docs (#13346)\n- Set minimum SQLite version supported (#13412)\n- Fix installation doc (#13462)\n- Add docs about mocking variables and connections (#13502)\n- Add docs about Flask CLI (#13500)\n- Fix Upgrading to 2 guide to use ``rbac`` UI (#13569)\n- Make docs clear that Auth can not be disabled for Stable API (#13568)\n- Remove archived links from docs & add link for AIPs (#13580)\n- Minor fixes in upgrading-to-2.rst (#13583)\n- Fix Link in Upgrading to 2.0 guide (#13584)\n- Fix heading for Mocking section in best-practices.rst (#13658)\n- Add docs on how to use custom operators within plugins folder (#13186)\n- Update docs to register Operator Extra Links (#13683)\n- Improvements for database setup docs (#13696)\n- Replace module path to Class with just Class Name (#13719)\n- Update DAG Serialization docs (#13722)\n- Fix link to Apache Airflow docs in webserver (#13250)\n- Clarifies differences between extras and provider packages (#13810)\n- Add information about all access methods to the environment (#13940)\n- Docs: Fix FAQ on scheduler latency (#13969)\n- Updated taskflow api doc to show dependency with sensor (#13968)\n- Add deprecated config options to docs (#13883)\n- Added a FAQ section to the Upgrading to 2 doc (#13979)\n\nAirflow 2.0.0 (2020-12-18)\n--------------------------\n\nThe full changelog is about 3,000 lines long (already excluding everything backported to 1.10)\nso please check `Airflow 2.0.0 Highlights Blog Post <https://airflow.apache.org/blog/airflow-two-point-oh-is-here/>`_\ninstead.\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nThe 2.0 release of the Airflow is a significant upgrade, and includes substantial major changes,\nand some of them may be breaking. Existing code written for earlier versions of this project will may require updates\nto use this version. Sometimes necessary configuration changes are also required.\nThis document describes the changes that have been made, and what you need to do to update your usage.\n\nIf you experience issues or have questions, please file `an issue <https://github.com/apache/airflow/issues/new/choose>`_.\n\nMajor changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThis section describes the major changes that have been made in this release.\n\nThe experimental REST API is disabled by default\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe experimental REST API is disabled by default. To restore these APIs while migrating to\nthe stable REST API, set ``enable_experimental_api`` option in ``[api]`` section to ``True``.\n\nPlease note that the experimental REST API do not have access control.\nThe authenticated user has full access.\n\nSparkJDBCHook default connection\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nFor SparkJDBCHook default connection was ``spark-default``\\ , and for SparkSubmitHook it was\n``spark_default``. Both hooks now use the ``spark_default`` which is a common pattern for the connection\nnames used across all providers.\n\nChanges to output argument in commands\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nFrom Airflow 2.0, We are replacing `tabulate <https://pypi.org/project/tabulate/>`_ with `rich <https://github.com/willmcgugan/rich>`_ to render commands output. Due to this change, the ``--output`` argument\nwill no longer accept formats of tabulate tables. Instead, it now accepts:\n\n\n* ``table`` - will render the output in predefined table\n* ``json`` - will render the output as a json\n* ``yaml`` - will render the output as yaml\n\nBy doing this we increased consistency and gave users possibility to manipulate the\noutput programmatically (when using json or yaml).\n\nAffected commands:\n\n\n* ``airflow dags list``\n* ``airflow dags report``\n* ``airflow dags list-runs``\n* ``airflow dags list-jobs``\n* ``airflow connections list``\n* ``airflow connections get``\n* ``airflow pools list``\n* ``airflow pools get``\n* ``airflow pools set``\n* ``airflow pools delete``\n* ``airflow pools import``\n* ``airflow pools export``\n* ``airflow role list``\n* ``airflow providers list``\n* ``airflow providers get``\n* ``airflow providers hooks``\n* ``airflow tasks states-for-dag-run``\n* ``airflow users list``\n* ``airflow variables list``\n\nAzure Wasb Hook does not work together with Snowflake hook\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe WasbHook in Apache Airflow use a legacy version of Azure library. While the conflict is not\nsignificant for most of the Azure hooks, it is a problem for Wasb Hook because the ``blob`` folders\nfor both libraries overlap. Installing both Snowflake and Azure extra will result in non-importable\nWasbHook.\n\nRename ``all`` to ``devel_all`` extra\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe ``all`` extras were reduced to include only user-facing dependencies. This means\nthat this extra does not contain development dependencies. If you were relying on\n``all`` extra then you should use now ``devel_all`` or figure out if you need development\nextras at all.\n\nContext variables ``prev_execution_date_success`` and ``prev_execution_date_success`` are now ``pendulum.DateTime``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nRename policy to task_policy\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nBecause Airflow introduced DAG level policy (\\ ``dag_policy``\\ ) we decided to rename existing ``policy``\nfunction to ``task_policy`` to make the distinction more profound and avoid any confusion.\n\nUsers using cluster policy need to rename their ``policy`` functions in ``airflow_local_settings.py``\nto ``task_policy``.\n\nDefault value for ``[celery] operation_timeout`` has changed to ``1.0``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nFrom Airflow 2, by default Airflow will retry 3 times to publish task to Celery broker. This is controlled by\n``[celery] task_publish_max_retries``. Because of this we can now have a lower Operation timeout that raises\n``AirflowTaskTimeout``. This generally occurs during network blips or intermittent DNS issues.\n\nAdding Operators and Sensors via plugins is no longer supported\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nOperators and Sensors should no longer be registered or imported via Airflow's plugin mechanism -- these types of classes are just treated as plain python classes by Airflow, so there is no need to register them with Airflow.\n\nIf you previously had a ``plugins/my_plugin.py`` and you used it like this in a DAG:\n\n.. code-block::\n\n   from airflow.operators.my_plugin import MyOperator\n\nYou should instead import it as:\n\n.. code-block::\n\n   from my_plugin import MyOperator\n\nThe name under ``airflow.operators.`` was the plugin name, where as in the second example it is the python module name where the operator is defined.\n\nSee https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html for more info.\n\nImporting Hooks via plugins is no longer supported\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nImporting hooks added in plugins via ``airflow.hooks.<plugin_name>`` is no longer supported, and hooks should just be imported as regular python modules.\n\n.. code-block::\n\n   from airflow.hooks.my_plugin import MyHook\n\nYou should instead import it as:\n\n.. code-block::\n\n   from my_plugin import MyHook\n\nIt is still possible (but not required) to \"register\" hooks in plugins. This is to allow future support for dynamically populating the Connections form in the UI.\n\nSee https://airflow.apache.org/docs/apache-airflow/stable/howto/custom-operator.html for more info.\n\nThe default value for ``[core] enable_xcom_pickling`` has been changed to ``False``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe pickle type for XCom messages has been replaced to JSON by default to prevent RCE attacks.\nNote that JSON serialization is stricter than pickling, so for example if you want to pass\nraw bytes through XCom you must encode them using an encoding like ``base64``.\nIf you understand the risk and still want to use `pickling <https://docs.python.org/3/library/pickle.html>`_\\ ,\nset ``enable_xcom_pickling = True`` in your Airflow config's ``core`` section.\n\nAirflowignore of base path\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThere was a bug fixed in https://github.com/apache/airflow/pull/11993 that the \"airflowignore\" checked\nthe base path of the dag folder for forbidden dags, not only the relative part. This had the effect\nthat if the base path contained the excluded word the whole dag folder could have been excluded. For\nexample if the airflowignore file contained x, and the dags folder was '/var/x/dags', then all dags in\nthe folder would be excluded. The fix only matches the relative path only now which means that if you\npreviously used full path as ignored, you should change it to relative one. For example if your dag\nfolder was '/var/dags/' and your airflowignore contained '/var/dag/excluded/', you should change it\nto 'excluded/'.\n\n``ExternalTaskSensor`` provides all task context variables to ``execution_date_fn`` as keyword arguments\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe old syntax of passing ``context`` as a dictionary will continue to work with the caveat that the argument must be named ``context``. The following will break. To fix it, change ``ctx`` to ``context``.\n\n.. code-block:: python\n\n   def execution_date_fn(execution_date, ctx): ...\n\n``execution_date_fn`` can take in any number of keyword arguments available in the task context dictionary. The following forms of ``execution_date_fn`` are all supported:\n\n.. code-block:: python\n\n   def execution_date_fn(dt): ...\n\n\n   def execution_date_fn(execution_date): ...\n\n\n   def execution_date_fn(execution_date, ds_nodash): ...\n\n\n   def execution_date_fn(execution_date, ds_nodash, dag): ...\n\nThe default value for ``[webserver] cookie_samesite`` has been changed to ``Lax``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nAs `recommended <https://flask.palletsprojects.com/en/1.1.x/config/#SESSION_COOKIE_SAMESITE>`_ by Flask, the\n``[webserver] cookie_samesite`` has been changed to ``Lax`` from ``''`` (empty string) .\n\nChanges to import paths\n~~~~~~~~~~~~~~~~~~~~~~~\n\nFormerly the core code was maintained by the original creators - Airbnb. The code that was in the contrib\npackage was supported by the community. The project was passed to the Apache community and currently the\nentire code is maintained by the community, so now the division has no justification, and it is only due\nto historical reasons. In Airflow 2.0, we want to organize packages and move integrations\nwith third party services to the ``airflow.providers`` package.\n\nAll changes made are backward compatible, but if you use the old import paths you will\nsee a deprecation warning. The old import paths can be abandoned in the future.\n\nAccording to `AIP-21 <https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-21%3A+Changes+in+import+paths>`_\n``_operator`` suffix has been removed from operators. A deprecation warning has also been raised for paths\nimporting with the suffix.\n\nThe following table shows changes in import paths.\n\n.. list-table::\n   :header-rows: 1\n\n   * - Old path\n     - New path\n   * - ``airflow.hooks.base_hook.BaseHook``\n     - ``airflow.hooks.base.BaseHook``\n   * - ``airflow.hooks.dbapi_hook.DbApiHook``\n     - ``airflow.hooks.dbapi.DbApiHook``\n   * - ``airflow.operators.dummy_operator.DummyOperator``\n     - ``airflow.operators.dummy.DummyOperator``\n   * - ``airflow.operators.dagrun_operator.TriggerDagRunOperator``\n     - ``airflow.operators.trigger_dagrun.TriggerDagRunOperator``\n   * - ``airflow.operators.branch_operator.BaseBranchOperator``\n     - ``airflow.operators.branch.BaseBranchOperator``\n   * - ``airflow.operators.subdag_operator.SubDagOperator``\n     - ``airflow.operators.subdag.SubDagOperator``\n   * - ``airflow.sensors.base_sensor_operator.BaseSensorOperator``\n     - ``airflow.sensors.base.BaseSensorOperator``\n   * - ``airflow.sensors.date_time_sensor.DateTimeSensor``\n     - ``airflow.sensors.date_time.DateTimeSensor``\n   * - ``airflow.sensors.external_task_sensor.ExternalTaskMarker``\n     - ``airflow.sensors.external_task.ExternalTaskMarker``\n   * - ``airflow.sensors.external_task_sensor.ExternalTaskSensor``\n     - ``airflow.sensors.external_task.ExternalTaskSensor``\n   * - ``airflow.sensors.sql_sensor.SqlSensor``\n     - ``airflow.sensors.sql.SqlSensor``\n   * - ``airflow.sensors.time_delta_sensor.TimeDeltaSensor``\n     - ``airflow.sensors.time_delta.TimeDeltaSensor``\n   * - ``airflow.contrib.sensors.weekday_sensor.DayOfWeekSensor``\n     - ``airflow.sensors.weekday.DayOfWeekSensor``\n\n\nDatabase schema changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIn order to migrate the database, you should use the command ``airflow db upgrade``\\ , but in\nsome cases manual steps are required.\n\nUnique conn_id in connection table\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPreviously, Airflow allowed users to add more than one connection with the same ``conn_id`` and on access it would choose one connection randomly. This acted as a basic load balancing and fault tolerance technique, when used in conjunction with retries.\n\nThis behavior caused some confusion for users, and there was no clear evidence if it actually worked well or not.\n\nNow the ``conn_id`` will be unique. If you already have duplicates in your metadata database, you will have to manage those duplicate connections before upgrading the database.\n\nNot-nullable conn_type column in connection table\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe ``conn_type`` column in the ``connection`` table must contain content. Previously, this rule was enforced\nby application logic, but was not enforced by the database schema.\n\nIf you made any modifications to the table directly, make sure you don't have\nnull in the ``conn_type`` column.\n\nConfiguration changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThis release contains many changes that require a change in the configuration of this application or\nother application that integrate with it.\n\nThis section describes the changes that have been made, and what you need to do to.\n\nairflow.contrib.utils.log has been moved\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormerly the core code was maintained by the original creators - Airbnb. The code that was in the contrib\npackage was supported by the community. The project was passed to the Apache community and currently the\nentire code is maintained by the community, so now the division has no justification, and it is only due\nto historical reasons. In Airflow 2.0, we want to organize packages and move integrations\nwith third party services to the ``airflow.providers`` package.\n\nTo clean up, the following packages were moved:\n\n.. list-table::\n   :header-rows: 1\n\n   * - Old package\n     - New package\n   * - ``airflow.contrib.utils.log``\n     - ``airflow.utils.log``\n   * - ``airflow.utils.log.gcs_task_handler``\n     - ``airflow.providers.google.cloud.log.gcs_task_handler``\n   * - ``airflow.utils.log.wasb_task_handler``\n     -  ``airflow.providers.microsoft.azure.log.wasb_task_handler``\n   * - ``airflow.utils.log.stackdriver_task_handler``\n     -  ``airflow.providers.google.cloud.log.stackdriver_task_handler``\n   * - ``airflow.utils.log.s3_task_handler``\n     - ``airflow.providers.amazon.aws.log.s3_task_handler``\n   * - ``airflow.utils.log.es_task_handler``\n     - ``airflow.providers.elasticsearch.log.es_task_handler``\n   * - ``airflow.utils.log.cloudwatch_task_handler``\n     - ``airflow.providers.amazon.aws.log.cloudwatch_task_handler``\n\nYou should update the import paths if you are setting log configurations with the ``logging_config_class`` option.\nThe old import paths still works but can be abandoned.\n\nSendGrid emailer has been moved\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormerly the core code was maintained by the original creators - Airbnb. The code that was in the contrib\npackage was supported by the community. The project was passed to the Apache community and currently the\nentire code is maintained by the community, so now the division has no justification, and it is only due\nto historical reasons.\n\nTo clean up, the ``send_mail`` function from the ``airflow.contrib.utils.sendgrid`` module has been moved.\n\nIf your configuration file looks like this:\n\n.. code-block:: ini\n\n   [email]\n   email_backend = airflow.contrib.utils.sendgrid.send_email\n\nIt should look like this now:\n\n.. code-block:: ini\n\n   [email]\n   email_backend = airflow.providers.sendgrid.utils.emailer.send_email\n\nThe old configuration still works but can be abandoned.\n\nUnify ``hostname_callable`` option in ``core`` section\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe previous option used a colon(\\ ``:``\\ ) to split the module from function. Now the dot(\\ ``.``\\ ) is used.\n\nThe change aims to unify the format of all options that refer to objects in the ``airflow.cfg`` file.\n\nCustom executors is loaded using full import path\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn previous versions of Airflow it was possible to use plugins to load custom executors. It is still\npossible, but the configuration has changed. Now you don't have to create a plugin to configure a\ncustom executor, but you need to provide the full path to the module in the ``executor`` option\nin the ``core`` section. The purpose of this change is to simplify the plugin mechanism and make\nit easier to configure executor.\n\nIf your module was in the path ``my_acme_company.executors.MyCustomExecutor``  and the plugin was\ncalled ``my_plugin`` then your configuration looks like this\n\n.. code-block:: ini\n\n   [core]\n   executor = my_plugin.MyCustomExecutor\n\nAnd now it should look like this:\n\n.. code-block:: ini\n\n   [core]\n   executor = my_acme_company.executors.MyCustomExecutor\n\nThe old configuration is still works but can be abandoned at any time.\n\nUse ``CustomSQLAInterface`` instead of ``SQLAInterface`` for custom data models.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFrom Airflow 2.0, if you want to define your own Flask App Builder data models you need to use CustomSQLAInterface\ninstead of SQLAInterface.\n\nFor Non-RBAC replace:\n\n.. code-block:: python\n\n   from flask_appbuilder.models.sqla.interface import SQLAInterface\n\n   datamodel = SQLAInterface(your_data_model)\n\nwith RBAC (in 1.10):\n\n.. code-block:: python\n\n   from airflow.www_rbac.utils import CustomSQLAInterface\n\n   datamodel = CustomSQLAInterface(your_data_model)\n\nand in 2.0:\n\n.. code-block:: python\n\n   from airflow.www.utils import CustomSQLAInterface\n\n   datamodel = CustomSQLAInterface(your_data_model)\n\nDrop plugin support for stat_name_handler\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn previous version, you could use plugins mechanism to configure ``stat_name_handler``. You should now use the ``stat_name_handler``\noption in ``[scheduler]`` section to achieve the same effect.\n\nIf your plugin looked like this and was available through the ``test_plugin`` path:\n\n.. code-block:: python\n\n   def my_stat_name_handler(stat):\n       return stat\n\n\n   class AirflowTestPlugin(AirflowPlugin):\n       name = \"test_plugin\"\n       stat_name_handler = my_stat_name_handler\n\nthen your ``airflow.cfg`` file should look like this:\n\n.. code-block:: ini\n\n   [scheduler]\n   stat_name_handler=test_plugin.my_stat_name_handler\n\nThis change is intended to simplify the statsd configuration.\n\nLogging configuration has been moved to new section\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe following configurations have been moved from ``[core]`` to the new ``[logging]`` section.\n\n\n* ``base_log_folder``\n* ``remote_logging``\n* ``remote_log_conn_id``\n* ``remote_base_log_folder``\n* ``encrypt_s3_logs``\n* ``logging_level``\n* ``fab_logging_level``\n* ``logging_config_class``\n* ``colored_console_log``\n* ``colored_log_format``\n* ``colored_formatter_class``\n* ``log_format``\n* ``simple_log_format``\n* ``task_log_prefix_template``\n* ``log_filename_template``\n* ``log_processor_filename_template``\n* ``dag_processor_manager_log_location``\n* ``task_log_reader``\n\nMetrics configuration has been moved to new section\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe following configurations have been moved from ``[scheduler]`` to the new ``[metrics]`` section.\n\n\n* ``statsd_on``\n* ``statsd_host``\n* ``statsd_port``\n* ``statsd_prefix``\n* ``statsd_allow_list``\n* ``stat_name_handler``\n* ``statsd_datadog_enabled``\n* ``statsd_datadog_tags``\n* ``statsd_custom_client_path``\n\nChanges to Elasticsearch logging provider\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhen JSON output to stdout is enabled, log lines will now contain the ``log_id`` & ``offset`` fields, this should make reading task logs from elasticsearch on the webserver work out of the box. Example configuration:\n\n.. code-block:: ini\n\n   [logging]\n   remote_logging = True\n   [elasticsearch]\n   host = http://es-host:9200\n   write_stdout = True\n   json_format = True\n\nNote that the webserver expects the log line data itself to be present in the ``message`` field of the document.\n\nRemove gcp_service_account_keys option in airflow.cfg file\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis option has been removed because it is no longer supported by the Google Kubernetes Engine. The new\nrecommended service account keys for the Google Cloud management method is\n`Workload Identity <https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity>`_.\n\nFernet is enabled by default\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe fernet mechanism is enabled by default to increase the security of the default installation.  In order to\nrestore the previous behavior, the user must consciously set an empty key in the ``fernet_key`` option of\nsection ``[core]`` in the ``airflow.cfg`` file.\n\nAt the same time, this means that the ``apache-airflow[crypto]`` extra-packages are always installed.\nHowever, this requires that your operating system has ``libffi-dev`` installed.\n\nChanges to propagating Kubernetes worker annotations\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``kubernetes_annotations`` configuration section has been removed.\nA new key ``worker_annotations`` has been added to existing ``kubernetes`` section instead.\nThat is to remove restriction on the character set for k8s annotation keys.\nAll key/value pairs from ``kubernetes_annotations`` should now go to ``worker_annotations`` as a json. I.e. instead of e.g.\n\n.. code-block::\n\n   [kubernetes_annotations]\n   annotation_key = annotation_value\n   annotation_key2 = annotation_value2\n\nit should be rewritten to\n\n.. code-block::\n\n   [kubernetes]\n   worker_annotations = { \"annotation_key\" : \"annotation_value\", \"annotation_key2\" : \"annotation_value2\" }\n\nRemove run_duration\n~~~~~~~~~~~~~~~~~~~\n\nWe should not use the ``run_duration`` option anymore. This used to be for restarting the scheduler from time to time, but right now the scheduler is getting more stable and therefore using this setting is considered bad and might cause an inconsistent state.\n\nRename pool statsd metrics\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nUsed slot has been renamed to running slot to make the name self-explanatory\nand the code more maintainable.\n\nThis means ``pool.used_slots.<pool_name>`` metric has been renamed to\n``pool.running_slots.<pool_name>``. The ``Used Slots`` column in Pools Web UI view\nhas also been changed to ``Running Slots``.\n\nRemoval of Mesos Executor\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe Mesos Executor is removed from the code base as it was not widely used and not maintained. `Mailing List Discussion on deleting it <https://lists.apache.org/thread.html/daa9500026b820c6aaadeffd66166eae558282778091ebbc68819fb7@%3Cdev.airflow.apache.org%3E>`_.\n\nChange dag loading duration metric name\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nChange DAG file loading duration metric from\n``dag.loading-duration.<dag_id>`` to ``dag.loading-duration.<dag_file>``. This is to\nbetter handle the case when a DAG file has multiple DAGs.\n\nSentry is disabled by default\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSentry is disabled by default. To enable these integrations, you need set ``sentry_on`` option\nin ``[sentry]`` section to ``\"True\"``.\n\nSimplified GCSTaskHandler configuration\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn previous versions, in order to configure the service account key file, you had to create a connection entry.\nIn the current version, you can configure ``google_key_path`` option in ``[logging]`` section to set\nthe key file path.\n\nUsers using Application Default Credentials (ADC) need not take any action.\n\nThe change aims to simplify the configuration of logging, to prevent corruption of\nthe instance configuration by changing the value controlled by the user - connection entry. If you\nconfigure a backend secret, it also means the webserver doesn't need to connect to it. This\nsimplifies setups with multiple GCP projects, because only one project will require the Secret Manager API\nto be enabled.\n\nChanges to the core operators/hooks\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWe strive to ensure that there are no changes that may affect the end user and your files, but this\nrelease may contain changes that will require changes to your DAG files.\n\nThis section describes the changes that have been made, and what you need to do to update your DAG File,\nif you use core operators or any other.\n\nBaseSensorOperator now respects the trigger_rule of downstream tasks\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPreviously, BaseSensorOperator with setting ``soft_fail=True`` skips itself\nand skips all its downstream tasks unconditionally, when it fails i.e the trigger_rule of downstream tasks is not\nrespected.\n\nIn the new behavior, the trigger_rule of downstream tasks is respected.\nUser can preserve/achieve the original behaviour by setting the trigger_rule of each downstream task to ``all_success``.\n\nBaseOperator uses metaclass\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``BaseOperator`` class uses a ``BaseOperatorMeta`` as a metaclass. This meta class is based on\n``abc.ABCMeta``. If your custom operator uses different metaclass then you will have to adjust it.\n\nRemove SQL support in BaseHook\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nRemove ``get_records`` and ``get_pandas_df`` and ``run`` from BaseHook, which only apply for SQL-like hook,\nIf want to use them, or your custom hook inherit them, please use ``airflow.hooks.dbapi.DbApiHook``\n\nAssigning task to a DAG using bitwise shift (bit-shift) operators are no longer supported\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPreviously, you could assign a task to a DAG as follows:\n\n.. code-block:: python\n\n   dag = DAG(\"my_dag\")\n   dummy = DummyOperator(task_id=\"dummy\")\n\n   dag >> dummy\n\nThis is no longer supported. Instead, we recommend using the DAG as context manager:\n\n.. code-block:: python\n\n   with DAG(\"my_dag\") as dag:\n       dummy = DummyOperator(task_id=\"dummy\")\n\nRemoved deprecated import mechanism\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe deprecated import mechanism has been removed so the import of modules becomes more consistent and explicit.\n\nFor example: ``from airflow.operators import BashOperator``\nbecomes ``from airflow.operators.bash_operator import BashOperator``\n\nChanges to sensor imports\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSensors are now accessible via ``airflow.sensors`` and no longer via ``airflow.operators.sensors``.\n\nFor example: ``from airflow.operators.sensors import BaseSensorOperator``\nbecomes ``from airflow.sensors.base import BaseSensorOperator``\n\nSkipped tasks can satisfy wait_for_downstream\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPreviously, a task instance with ``wait_for_downstream=True`` will only run if the downstream task of\nthe previous task instance is successful. Meanwhile, a task instance with ``depends_on_past=True``\nwill run if the previous task instance is either successful or skipped. These two flags are close siblings\nyet they have different behavior. This inconsistency in behavior made the API less intuitive to users.\nTo maintain consistent behavior, both successful or skipped downstream task can now satisfy the\n``wait_for_downstream=True`` flag.\n\n``airflow.utils.helpers.cross_downstream``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.utils.helpers.chain``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe ``chain`` and ``cross_downstream`` methods are now moved to airflow.models.baseoperator module from\n``airflow.utils.helpers`` module.\n\nThe ``baseoperator`` module seems to be a better choice to keep\nclosely coupled methods together. Helpers module is supposed to contain standalone helper methods\nthat can be imported by all classes.\n\nThe ``chain`` method and ``cross_downstream`` method both use BaseOperator. If any other package imports\nany classes or functions from helpers module, then it automatically has an\nimplicit dependency to BaseOperator. That can often lead to cyclic dependencies.\n\nMore information in `AIRFLOW-6392 <https://issues.apache.org/jira/browse/AIRFLOW-6392>`_\n\nIn Airflow < 2.0 you imported those two methods like this:\n\n.. code-block:: python\n\n   from airflow.utils.helpers import chain\n   from airflow.utils.helpers import cross_downstream\n\nIn Airflow 2.0 it should be changed to:\n\n.. code-block:: python\n\n   from airflow.models.baseoperator import chain\n   from airflow.models.baseoperator import cross_downstream\n\n``airflow.operators.python.BranchPythonOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``BranchPythonOperator`` will now return a value equal to the ``task_id`` of the chosen branch,\nwhere previously it returned None. Since it inherits from BaseOperator it will do an\n``xcom_push`` of this value if ``do_xcom_push=True``. This is useful for downstream decision-making.\n\n``airflow.sensors.sql_sensor.SqlSensor``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSQLSensor now consistent with python ``bool()`` function and the ``allow_null`` parameter has been removed.\n\nIt will resolve after receiving any value  that is casted to ``True`` with python ``bool(value)``. That\nchanges the previous response receiving ``NULL`` or ``'0'``. Earlier ``'0'`` has been treated as success\ncriteria. ``NULL`` has been treated depending on value of ``allow_null``\\ parameter.  But all the previous\nbehaviour is still achievable setting param ``success`` to ``lambda x: x is None or str(x) not in ('0', '')``.\n\n``airflow.operators.trigger_dagrun.TriggerDagRunOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe TriggerDagRunOperator now takes a ``conf`` argument to which a dict can be provided as conf for the DagRun.\nAs a result, the ``python_callable`` argument was removed. PR: https://github.com/apache/airflow/pull/6317.\n\n``airflow.operators.python.PythonOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``provide_context`` argument on the PythonOperator was removed. The signature of the callable passed to the PythonOperator is now inferred and argument values are always automatically provided. There is no need to explicitly provide or not provide the context anymore. For example:\n\n.. code-block:: python\n\n   def myfunc(execution_date):\n       print(execution_date)\n\n\n   python_operator = PythonOperator(task_id=\"mytask\", python_callable=myfunc, dag=dag)\n\nNotice you don't have to set provide_context=True, variables from the task context are now automatically detected and provided.\n\nAll context variables can still be provided with a double-asterisk argument:\n\n.. code-block:: python\n\n   def myfunc(**context):\n       print(context)  # all variables will be provided to context\n\n\n   python_operator = PythonOperator(task_id=\"mytask\", python_callable=myfunc)\n\nThe task context variable names are reserved names in the callable function, hence a clash with ``op_args`` and ``op_kwargs`` results in an exception:\n\n.. code-block:: python\n\n   def myfunc(dag):\n       # raises a ValueError because \"dag\" is a reserved name\n       # valid signature example: myfunc(mydag)\n       print(\"output\")\n\n\n   python_operator = PythonOperator(\n       task_id=\"mytask\",\n       op_args=[1],\n       python_callable=myfunc,\n   )\n\nThe change is backwards compatible, setting ``provide_context`` will add the ``provide_context`` variable to the ``kwargs`` (but won't do anything).\n\nPR: `#5990 <https://github.com/apache/airflow/pull/5990>`_\n\n``airflow.providers.standard.sensors.filesystem.FileSensor``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFileSensor is now takes a glob pattern, not just a filename. If the filename you are looking for has ``*``\\ , ``?``\\ , or ``[`` in it then you should replace these with ``[*]``\\ , ``[?]``\\ , and ``[[]``.\n\n``airflow.operators.subdag_operator.SubDagOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``SubDagOperator`` is changed to use Airflow scheduler instead of backfill\nto schedule tasks in the subdag. User no longer need to specify the executor\nin ``SubDagOperator``.\n\n``airflow.providers.google.cloud.operators.datastore.CloudDatastoreExportEntitiesOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.datastore.CloudDatastoreImportEntitiesOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.cncf.kubernetes.operators.kubernetes_pod.KubernetesPodOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.ssh.operators.ssh.SSHOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.microsoft.winrm.operators.winrm.WinRMOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.operators.bash.BashOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.docker.operators.docker.DockerOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.http.operators.http.SimpleHttpOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe ``do_xcom_push`` flag (a switch to push the result of an operator to xcom or not) was appearing in different incarnations in different operators. It's function has been unified under a common name (\\ ``do_xcom_push``\\ ) on ``BaseOperator``. This way it is also easy to globally disable pushing results to xcom.\n\nThe following operators were affected:\n\n\n* DatastoreExportOperator (Backwards compatible)\n* DatastoreImportOperator (Backwards compatible)\n* KubernetesPodOperator (Not backwards compatible)\n* SSHOperator (Not backwards compatible)\n* WinRMOperator (Not backwards compatible)\n* BashOperator (Not backwards compatible)\n* DockerOperator (Not backwards compatible)\n* SimpleHttpOperator (Not backwards compatible)\n\nSee `AIRFLOW-3249 <https://jira.apache.org/jira/browse/AIRFLOW-3249>`_ for details\n\n``airflow.operators.latest_only_operator.LatestOnlyOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn previous versions, the ``LatestOnlyOperator`` forcefully skipped all (direct and indirect) downstream tasks on its own. From this version on the operator will **only skip direct downstream** tasks and the scheduler will handle skipping any further downstream dependencies.\n\nNo change is needed if only the default trigger rule ``all_success`` is being used.\n\nIf the DAG relies on tasks with other trigger rules (i.e. ``all_done``\\ ) being skipped by the ``LatestOnlyOperator``\\ , adjustments to the DAG need to be made to accommodate the change in behaviour, i.e. with additional edges from the ``LatestOnlyOperator``.\n\nThe goal of this change is to achieve a more consistent and configurable cascading behaviour based on the ``BaseBranchOperator`` (see `AIRFLOW-2923 <https://jira.apache.org/jira/browse/AIRFLOW-2923>`_ and `AIRFLOW-1784 <https://jira.apache.org/jira/browse/AIRFLOW-1784>`_\\ ).\n\nChanges to the core Python API\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWe strive to ensure that there are no changes that may affect the end user, and your Python files, but this\nrelease may contain changes that will require changes to your plugins, DAG File or other integration.\n\nOnly changes unique to this provider are described here. You should still pay attention to the changes that\nhave been made to the core (including core operators) as they can affect the integration behavior\nof this provider.\n\nThis section describes the changes that have been made, and what you need to do to update your Python files.\n\nRemoved sub-package imports from ``airflow/__init__.py``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe imports ``LoggingMixin``\\ , ``conf``\\ , and ``AirflowException`` have been removed from ``airflow/__init__.py``.\nAll implicit references of these objects will no longer be valid. To migrate, all usages of each old path must be\nreplaced with its corresponding new path.\n\n.. list-table::\n   :header-rows: 1\n\n   * - Old Path (Implicit Import)\n     - New Path (Explicit Import)\n   * - ``airflow.LoggingMixin``\n     - ``airflow.utils.log.logging_mixin.LoggingMixin``\n   * - ``airflow.conf``\n     - ``airflow.configuration.conf``\n   * - ``airflow.AirflowException``\n     - ``airflow.exceptions.AirflowException``\n\n\nVariables removed from the task instance context\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe following variables were removed from the task instance context:\n\n\n* end_date\n* latest_date\n* tables\n\n``airflow.contrib.utils.Weekday``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFormerly the core code was maintained by the original creators - Airbnb. The code that was in the contrib\npackage was supported by the community. The project was passed to the Apache community and currently the\nentire code is maintained by the community, so now the division has no justification, and it is only due\nto historical reasons.\n\nTo clean up, ``Weekday`` enum has been moved from ``airflow.contrib.utils`` into ``airflow.utils`` module.\n\n``airflow.models.connection.Connection``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe connection module has new deprecated methods:\n\n\n* ``Connection.parse_from_uri``\n* ``Connection.log_info``\n* ``Connection.debug_info``\n\nand one deprecated function:\n\n\n* ``parse_netloc_to_hostname``\n\nPreviously, users could create a connection object in two ways\n\n.. code-block::\n\n   conn_1 = Connection(conn_id=\"conn_a\", uri=\"mysql://AAA/\")\n   # or\n   conn_2 = Connection(conn_id=\"conn_a\")\n   conn_2.parse_uri(uri=\"mysql://AAA/\")\n\nNow the second way is not supported.\n\n``Connection.log_info`` and ``Connection.debug_info`` method have been deprecated. Read each Connection field individually or use the\ndefault representation (\\ ``__repr__``\\ ).\n\nThe old method is still works but can be abandoned at any time. The changes are intended to delete method\nthat are rarely used.\n\n``airflow.models.dag.DAG.create_dagrun``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDAG.create_dagrun accepts run_type and does not require run_id\nThis change is caused by adding ``run_type`` column to ``DagRun``.\n\nPrevious signature:\n\n.. code-block:: python\n\n   def create_dagrun(\n       self,\n       run_id,\n       state,\n       execution_date=None,\n       start_date=None,\n       external_trigger=False,\n       conf=None,\n       session=None,\n   ): ...\n\ncurrent:\n\n.. code-block:: python\n\n   def create_dagrun(\n       self,\n       state,\n       execution_date=None,\n       run_id=None,\n       start_date=None,\n       external_trigger=False,\n       conf=None,\n       run_type=None,\n       session=None,\n   ): ...\n\nIf user provides ``run_id`` then the ``run_type`` will be derived from it by checking prefix, allowed types\n: ``manual``\\ , ``scheduled``\\ , ``backfill`` (defined by ``airflow.utils.types.DagRunType``\\ ).\n\nIf user provides ``run_type`` and ``execution_date`` then ``run_id`` is constructed as\n``{run_type}__{execution_data.isoformat()}``.\n\nAirflow should construct dagruns using ``run_type`` and ``execution_date``\\ , creation using\n``run_id`` is preserved for user actions.\n\n``airflow.models.dagrun.DagRun``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nUse DagRunType.SCHEDULED.value instead of DagRun.ID_PREFIX\n\nAll the run_id prefixes for different kind of DagRuns have been grouped into a single\nenum in ``airflow.utils.types.DagRunType``.\n\nPreviously, there were defined in various places, example as ``ID_PREFIX`` class variables for\n``DagRun``\\ , ``BackfillJob`` and in ``_trigger_dag`` function.\n\nWas:\n\n.. code-block:: pycon\n\n   >> from airflow.models.dagrun import DagRun\n   >> DagRun.ID_PREFIX\n   scheduled__\n\nReplaced by:\n\n.. code-block:: pycon\n\n   >> from airflow.utils.types import DagRunType\n   >> DagRunType.SCHEDULED.value\n   scheduled\n\n``airflow.utils.file.TemporaryDirectory``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe remove ``airflow.utils.file.TemporaryDirectory``\nSince Airflow dropped support for Python < 3.5 there's no need to have this custom\nimplementation of ``TemporaryDirectory`` because the same functionality is provided by\n``tempfile.TemporaryDirectory``.\n\nNow users instead of ``import from airflow.utils.files import TemporaryDirectory`` should\ndo ``from tempfile import TemporaryDirectory``. Both context managers provide the same\ninterface, thus no additional changes should be required.\n\n``airflow.AirflowMacroPlugin``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe removed ``airflow.AirflowMacroPlugin`` class. The class was there in airflow package but it has not been used (apparently since 2015).\nIt has been removed.\n\n``airflow.settings.CONTEXT_MANAGER_DAG``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nCONTEXT_MANAGER_DAG was removed from settings. Its role has been taken by ``DagContext`` in\n'airflow.models.dag'. One of the reasons was that settings should be rather static than store\ndynamic context from the DAG, but the main one is that moving the context out of settings allowed to\nuntangle cyclic imports between DAG, BaseOperator, SerializedDAG, SerializedBaseOperator which was\npart of AIRFLOW-6010.\n\n``airflow.utils.log.logging_mixin.redirect_stderr``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.utils.log.logging_mixin.redirect_stdout``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFunction ``redirect_stderr`` and ``redirect_stdout`` from ``airflow.utils.log.logging_mixin`` module has\nbeen deleted because it can be easily replaced by the standard library.\nThe functions of the standard library are more flexible and can be used in larger cases.\n\nThe code below\n\n.. code-block:: python\n\n   import logging\n\n   from airflow.utils.log.logging_mixin import redirect_stderr, redirect_stdout\n\n   logger = logging.getLogger(\"custom-logger\")\n   with redirect_stdout(logger, logging.INFO), redirect_stderr(logger, logging.WARN):\n       print(\"I love Airflow\")\n\ncan be replaced by the following code:\n\n.. code-block:: python\n\n   from contextlib import redirect_stdout, redirect_stderr\n   import logging\n\n   from airflow.utils.log.logging_mixin import StreamLogWriter\n\n   logger = logging.getLogger(\"custom-logger\")\n\n   with (\n       redirect_stdout(StreamLogWriter(logger, logging.INFO)),\n       redirect_stderr(StreamLogWriter(logger, logging.WARN)),\n   ):\n       print(\"I Love Airflow\")\n\n``airflow.models.baseoperator.BaseOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNow, additional arguments passed to BaseOperator cause an exception. Previous versions of Airflow took additional arguments and displayed a message on the console. When the\nmessage was not noticed by users, it caused very difficult to detect errors.\n\nIn order to restore the previous behavior, you must set an ``True`` in  the ``allow_illegal_arguments``\noption of section ``[operators]`` in the ``airflow.cfg`` file. In the future it is possible to completely\ndelete this option.\n\n``airflow.models.dagbag.DagBag``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPassing ``store_serialized_dags`` argument to DagBag.\\ **init** and accessing ``DagBag.store_serialized_dags`` property\nare deprecated and will be removed in future versions.\n\n**Previous signature**\\ :\n\n.. code-block:: python\n\n   def __init__(\n       dag_folder=None,\n       include_examples=conf.getboolean(\"core\", \"LOAD_EXAMPLES\"),\n       safe_mode=conf.getboolean(\"core\", \"DAG_DISCOVERY_SAFE_MODE\"),\n       store_serialized_dags=False,\n   ): ...\n\n**current**\\ :\n\n.. code-block:: python\n\n   def __init__(\n       dag_folder=None,\n       include_examples=conf.getboolean(\"core\", \"LOAD_EXAMPLES\"),\n       safe_mode=conf.getboolean(\"core\", \"DAG_DISCOVERY_SAFE_MODE\"),\n       read_dags_from_db=False,\n   ): ...\n\nIf you were using positional arguments, it requires no change but if you were using keyword\narguments, please change ``store_serialized_dags`` to ``read_dags_from_db``.\n\nSimilarly, if you were using ``DagBag().store_serialized_dags`` property, change it to\n``DagBag().read_dags_from_db``.\n\nChanges in ``google`` provider package\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWe strive to ensure that there are no changes that may affect the end user and your Python files, but this\nrelease may contain changes that will require changes to your configuration, DAG Files or other integration\ne.g. custom operators.\n\nOnly changes unique to this provider are described here. You should still pay attention to the changes that\nhave been made to the core (including core operators) as they can affect the integration behavior\nof this provider.\n\nThis section describes the changes that have been made, and what you need to do to update your if\nyou use operators or hooks which integrate with Google services (including Google Cloud - GCP).\n\nDirect impersonation added to operators communicating with Google services\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n`Directly impersonating a service account <https://cloud.google.com/iam/docs/understanding-service-accounts#directly_impersonating_a_service_account>`_\nhas been made possible for operators communicating with Google services via new argument called ``impersonation_chain``\n(\\ ``google_impersonation_chain`` in case of operators that also communicate with services of other cloud providers).\nAs a result, GCSToS3Operator no longer derivatives from GCSListObjectsOperator.\n\nNormalize gcp_conn_id for Google Cloud\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPreviously not all hooks and operators related to Google Cloud use\n``gcp_conn_id`` as parameter for GCP connection. There is currently one parameter\nwhich apply to most services. Parameters like ``datastore_conn_id``\\ , ``bigquery_conn_id``\\ ,\n``google_cloud_storage_conn_id`` and similar have been deprecated. Operators that require two connections are not changed.\n\nFollowing components were affected by normalization:\n\n\n* ``airflow.providers.google.cloud.hooks.datastore.DatastoreHook``\n* ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook``\n* ``airflow.providers.google.cloud.hooks.gcs.GoogleCloudStorageHook``\n* ``airflow.providers.google.cloud.operators.bigquery.BigQueryCheckOperator``\n* ``airflow.providers.google.cloud.operators.bigquery.BigQueryValueCheckOperator``\n* ``airflow.providers.google.cloud.operators.bigquery.BigQueryIntervalCheckOperator``\n* ``airflow.providers.google.cloud.operators.bigquery.BigQueryGetDataOperator``\n* ``airflow.providers.google.cloud.operators.bigquery.BigQueryOperator``\n* ``airflow.providers.google.cloud.operators.bigquery.BigQueryDeleteDatasetOperator``\n* ``airflow.providers.google.cloud.operators.bigquery.BigQueryCreateEmptyDatasetOperator``\n* ``airflow.providers.google.cloud.operators.bigquery.BigQueryTableDeleteOperator``\n* ``airflow.providers.google.cloud.operators.gcs.GoogleCloudStorageCreateBucketOperator``\n* ``airflow.providers.google.cloud.operators.gcs.GoogleCloudStorageListOperator``\n* ``airflow.providers.google.cloud.operators.gcs.GoogleCloudStorageDownloadOperator``\n* ``airflow.providers.google.cloud.operators.gcs.GoogleCloudStorageDeleteOperator``\n* ``airflow.providers.google.cloud.operators.gcs.GoogleCloudStorageBucketCreateAclEntryOperator``\n* ``airflow.providers.google.cloud.operators.gcs.GoogleCloudStorageObjectCreateAclEntryOperator``\n* ``airflow.operators.sql_to_gcs.BaseSQLToGoogleCloudStorageOperator``\n* ``airflow.operators.adls_to_gcs.AdlsToGoogleCloudStorageOperator``\n* ``airflow.operators.gcs_to_s3.GoogleCloudStorageToS3Operator``\n* ``airflow.operators.gcs_to_gcs.GoogleCloudStorageToGoogleCloudStorageOperator``\n* ``airflow.operators.bigquery_to_gcs.BigQueryToCloudStorageOperator``\n* ``airflow.operators.local_to_gcs.FileToGoogleCloudStorageOperator``\n* ``airflow.operators.cassandra_to_gcs.CassandraToGoogleCloudStorageOperator``\n* ``airflow.operators.bigquery_to_bigquery.BigQueryToBigQueryOperator``\n\nChanges to import paths and names of GCP operators and hooks\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAccording to `AIP-21 <https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-21%3A+Changes+in+import+paths>`_\noperators related to Google Cloud has been moved from contrib to core.\nThe following table shows changes in import paths.\n\n.. list-table::\n   :header-rows: 1\n\n   * - Old path\n     - New path\n   * - ``airflow.contrib.hooks.bigquery_hook.BigQueryHook``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook``\n   * - ``airflow.contrib.hooks.datastore_hook.DatastoreHook``\n     - ``airflow.providers.google.cloud.hooks.datastore.DatastoreHook``\n   * - ``airflow.contrib.hooks.gcp_bigtable_hook.BigtableHook``\n     - ``airflow.providers.google.cloud.hooks.bigtable.BigtableHook``\n   * - ``airflow.contrib.hooks.gcp_cloud_build_hook.CloudBuildHook``\n     - ``airflow.providers.google.cloud.hooks.cloud_build.CloudBuildHook``\n   * - ``airflow.contrib.hooks.gcp_container_hook.GKEClusterHook``\n     - ``airflow.providers.google.cloud.hooks.kubernetes_engine.GKEHook``\n   * - ``airflow.contrib.hooks.gcp_compute_hook.GceHook``\n     - ``airflow.providers.google.cloud.hooks.compute.ComputeEngineHook``\n   * - ``airflow.contrib.hooks.gcp_dataflow_hook.DataFlowHook``\n     - ``airflow.providers.google.cloud.hooks.dataflow.DataflowHook``\n   * - ``airflow.contrib.hooks.gcp_dataproc_hook.DataProcHook``\n     - ``airflow.providers.google.cloud.hooks.dataproc.DataprocHook``\n   * - ``airflow.contrib.hooks.gcp_dlp_hook.CloudDLPHook``\n     - ``airflow.providers.google.cloud.hooks.dlp.CloudDLPHook``\n   * - ``airflow.contrib.hooks.gcp_function_hook.GcfHook``\n     - ``airflow.providers.google.cloud.hooks.functions.CloudFunctionsHook``\n   * - ``airflow.contrib.hooks.gcp_kms_hook.GoogleCloudKMSHook``\n     - ``airflow.providers.google.cloud.hooks.kms.CloudKMSHook``\n   * - ``airflow.contrib.hooks.gcp_mlengine_hook.MLEngineHook``\n     - ``airflow.providers.google.cloud.hooks.mlengine.MLEngineHook``\n   * - ``airflow.contrib.hooks.gcp_natural_language_hook.CloudNaturalLanguageHook``\n     - ``airflow.providers.google.cloud.hooks.natural_language.CloudNaturalLanguageHook``\n   * - ``airflow.contrib.hooks.gcp_pubsub_hook.PubSubHook``\n     - ``airflow.providers.google.cloud.hooks.pubsub.PubSubHook``\n   * - ``airflow.contrib.hooks.gcp_speech_to_text_hook.GCPSpeechToTextHook``\n     - ``airflow.providers.google.cloud.hooks.speech_to_text.CloudSpeechToTextHook``\n   * - ``airflow.contrib.hooks.gcp_spanner_hook.CloudSpannerHook``\n     - ``airflow.providers.google.cloud.hooks.spanner.SpannerHook``\n   * - ``airflow.contrib.hooks.gcp_sql_hook.CloudSqlDatabaseHook``\n     - ``airflow.providers.google.cloud.hooks.cloud_sql.CloudSQLDatabaseHook``\n   * - ``airflow.contrib.hooks.gcp_sql_hook.CloudSqlHook``\n     - ``airflow.providers.google.cloud.hooks.cloud_sql.CloudSQLHook``\n   * - ``airflow.contrib.hooks.gcp_tasks_hook.CloudTasksHook``\n     - ``airflow.providers.google.cloud.hooks.tasks.CloudTasksHook``\n   * - ``airflow.contrib.hooks.gcp_text_to_speech_hook.GCPTextToSpeechHook``\n     - ``airflow.providers.google.cloud.hooks.text_to_speech.CloudTextToSpeechHook``\n   * - ``airflow.contrib.hooks.gcp_transfer_hook.GCPTransferServiceHook``\n     - ``airflow.providers.google.cloud.hooks.cloud_storage_transfer_service.CloudDataTransferServiceHook``\n   * - ``airflow.contrib.hooks.gcp_translate_hook.CloudTranslateHook``\n     - ``airflow.providers.google.cloud.hooks.translate.CloudTranslateHook``\n   * - ``airflow.contrib.hooks.gcp_video_intelligence_hook.CloudVideoIntelligenceHook``\n     - ``airflow.providers.google.cloud.hooks.video_intelligence.CloudVideoIntelligenceHook``\n   * - ``airflow.contrib.hooks.gcp_vision_hook.CloudVisionHook``\n     - ``airflow.providers.google.cloud.hooks.vision.CloudVisionHook``\n   * - ``airflow.contrib.hooks.gcs_hook.GoogleCloudStorageHook``\n     - ``airflow.providers.google.cloud.hooks.gcs.GCSHook``\n   * - ``airflow.contrib.operators.adls_to_gcs.AdlsToGoogleCloudStorageOperator``\n     - ``airflow.operators.adls_to_gcs.AdlsToGoogleCloudStorageOperator``\n   * - ``airflow.contrib.operators.bigquery_check_operator.BigQueryCheckOperator``\n     - ``airflow.providers.google.cloud.operators.bigquery.BigQueryCheckOperator``\n   * - ``airflow.contrib.operators.bigquery_check_operator.BigQueryIntervalCheckOperator``\n     - ``airflow.providers.google.cloud.operators.bigquery.BigQueryIntervalCheckOperator``\n   * - ``airflow.contrib.operators.bigquery_check_operator.BigQueryValueCheckOperator``\n     - ``airflow.providers.google.cloud.operators.bigquery.BigQueryValueCheckOperator``\n   * - ``airflow.contrib.operators.bigquery_get_data.BigQueryGetDataOperator``\n     - ``airflow.providers.google.cloud.operators.bigquery.BigQueryGetDataOperator``\n   * - ``airflow.contrib.operators.bigquery_operator.BigQueryCreateEmptyDatasetOperator``\n     - ``airflow.providers.google.cloud.operators.bigquery.BigQueryCreateEmptyDatasetOperator``\n   * - ``airflow.contrib.operators.bigquery_operator.BigQueryCreateEmptyTableOperator``\n     - ``airflow.providers.google.cloud.operators.bigquery.BigQueryCreateEmptyTableOperator``\n   * - ``airflow.contrib.operators.bigquery_operator.BigQueryCreateExternalTableOperator``\n     - ``airflow.providers.google.cloud.operators.bigquery.BigQueryCreateExternalTableOperator``\n   * - ``airflow.contrib.operators.bigquery_operator.BigQueryDeleteDatasetOperator``\n     - ``airflow.providers.google.cloud.operators.bigquery.BigQueryDeleteDatasetOperator``\n   * - ``airflow.contrib.operators.bigquery_operator.BigQueryOperator``\n     - ``airflow.providers.google.cloud.operators.bigquery.BigQueryExecuteQueryOperator``\n   * - ``airflow.contrib.operators.bigquery_table_delete_operator.BigQueryTableDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.bigquery.BigQueryDeleteTableOperator``\n   * - ``airflow.contrib.operators.bigquery_to_bigquery.BigQueryToBigQueryOperator``\n     - ``airflow.operators.bigquery_to_bigquery.BigQueryToBigQueryOperator``\n   * - ``airflow.contrib.operators.bigquery_to_gcs.BigQueryToCloudStorageOperator``\n     - ``airflow.operators.bigquery_to_gcs.BigQueryToCloudStorageOperator``\n   * - ``airflow.contrib.operators.bigquery_to_mysql_operator.BigQueryToMySqlOperator``\n     - ``airflow.operators.bigquery_to_mysql.BigQueryToMySqlOperator``\n   * - ``airflow.contrib.operators.dataflow_operator.DataFlowJavaOperator``\n     - ``airflow.providers.google.cloud.operators.dataflow.DataFlowJavaOperator``\n   * - ``airflow.contrib.operators.dataflow_operator.DataFlowPythonOperator``\n     - ``airflow.providers.google.cloud.operators.dataflow.DataFlowPythonOperator``\n   * - ``airflow.contrib.operators.dataflow_operator.DataflowTemplateOperator``\n     - ``airflow.providers.google.cloud.operators.dataflow.DataflowTemplateOperator``\n   * - ``airflow.contrib.operators.dataproc_operator.DataProcHadoopOperator``\n     - ``airflow.providers.google.cloud.operators.dataproc.DataprocSubmitHadoopJobOperator``\n   * - ``airflow.contrib.operators.dataproc_operator.DataProcHiveOperator``\n     - ``airflow.providers.google.cloud.operators.dataproc.DataprocSubmitHiveJobOperator``\n   * - ``airflow.contrib.operators.dataproc_operator.DataProcJobBaseOperator``\n     - ``airflow.providers.google.cloud.operators.dataproc.DataprocJobBaseOperator``\n   * - ``airflow.contrib.operators.dataproc_operator.DataProcPigOperator``\n     - ``airflow.providers.google.cloud.operators.dataproc.DataprocSubmitPigJobOperator``\n   * - ``airflow.contrib.operators.dataproc_operator.DataProcPySparkOperator``\n     - ``airflow.providers.google.cloud.operators.dataproc.DataprocSubmitPySparkJobOperator``\n   * - ``airflow.contrib.operators.dataproc_operator.DataProcSparkOperator``\n     - ``airflow.providers.google.cloud.operators.dataproc.DataprocSubmitSparkJobOperator``\n   * - ``airflow.contrib.operators.dataproc_operator.DataProcSparkSqlOperator``\n     - ``airflow.providers.google.cloud.operators.dataproc.DataprocSubmitSparkSqlJobOperator``\n   * - ``airflow.contrib.operators.dataproc_operator.DataprocClusterCreateOperator``\n     - ``airflow.providers.google.cloud.operators.dataproc.DataprocCreateClusterOperator``\n   * - ``airflow.contrib.operators.dataproc_operator.DataprocClusterDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.dataproc.DataprocDeleteClusterOperator``\n   * - ``airflow.contrib.operators.dataproc_operator.DataprocClusterScaleOperator``\n     - ``airflow.providers.google.cloud.operators.dataproc.DataprocScaleClusterOperator``\n   * - ``airflow.contrib.operators.dataproc_operator.DataprocOperationBaseOperator``\n     - ``airflow.providers.google.cloud.operators.dataproc.DataprocOperationBaseOperator``\n   * - ``airflow.contrib.operators.dataproc_operator.DataprocWorkflowTemplateInstantiateInlineOperator``\n     - ``airflow.providers.google.cloud.operators.dataproc.DataprocInstantiateInlineWorkflowTemplateOperator``\n   * - ``airflow.contrib.operators.dataproc_operator.DataprocWorkflowTemplateInstantiateOperator``\n     - ``airflow.providers.google.cloud.operators.dataproc.DataprocInstantiateWorkflowTemplateOperator``\n   * - ``airflow.contrib.operators.datastore_export_operator.DatastoreExportOperator``\n     - ``airflow.providers.google.cloud.operators.datastore.DatastoreExportOperator``\n   * - ``airflow.contrib.operators.datastore_import_operator.DatastoreImportOperator``\n     - ``airflow.providers.google.cloud.operators.datastore.DatastoreImportOperator``\n   * - ``airflow.contrib.operators.file_to_gcs.FileToGoogleCloudStorageOperator``\n     - ``airflow.providers.google.cloud.transfers.local_to_gcs.FileToGoogleCloudStorageOperator``\n   * - ``airflow.contrib.operators.gcp_bigtable_operator.BigtableClusterUpdateOperator``\n     - ``airflow.providers.google.cloud.operators.bigtable.BigtableUpdateClusterOperator``\n   * - ``airflow.contrib.operators.gcp_bigtable_operator.BigtableInstanceCreateOperator``\n     - ``airflow.providers.google.cloud.operators.bigtable.BigtableCreateInstanceOperator``\n   * - ``airflow.contrib.operators.gcp_bigtable_operator.BigtableInstanceDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.bigtable.BigtableDeleteInstanceOperator``\n   * - ``airflow.contrib.operators.gcp_bigtable_operator.BigtableTableCreateOperator``\n     - ``airflow.providers.google.cloud.operators.bigtable.BigtableCreateTableOperator``\n   * - ``airflow.contrib.operators.gcp_bigtable_operator.BigtableTableDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.bigtable.BigtableDeleteTableOperator``\n   * - ``airflow.contrib.operators.gcp_bigtable_operator.BigtableTableWaitForReplicationSensor``\n     - ``airflow.providers.google.cloud.sensors.bigtable.BigtableTableReplicationCompletedSensor``\n   * - ``airflow.contrib.operators.gcp_cloud_build_operator.CloudBuildCreateBuildOperator``\n     - ``airflow.providers.google.cloud.operators.cloud_build.CloudBuildCreateBuildOperator``\n   * - ``airflow.contrib.operators.gcp_compute_operator.GceBaseOperator``\n     - ``airflow.providers.google.cloud.operators.compute.GceBaseOperator``\n   * - ``airflow.contrib.operators.gcp_compute_operator.GceInstanceGroupManagerUpdateTemplateOperator``\n     - ``airflow.providers.google.cloud.operators.compute.GceInstanceGroupManagerUpdateTemplateOperator``\n   * - ``airflow.contrib.operators.gcp_compute_operator.GceInstanceStartOperator``\n     - ``airflow.providers.google.cloud.operators.compute.GceInstanceStartOperator``\n   * - ``airflow.contrib.operators.gcp_compute_operator.GceInstanceStopOperator``\n     - ``airflow.providers.google.cloud.operators.compute.GceInstanceStopOperator``\n   * - ``airflow.contrib.operators.gcp_compute_operator.GceInstanceTemplateCopyOperator``\n     - ``airflow.providers.google.cloud.operators.compute.GceInstanceTemplateCopyOperator``\n   * - ``airflow.contrib.operators.gcp_compute_operator.GceSetMachineTypeOperator``\n     - ``airflow.providers.google.cloud.operators.compute.GceSetMachineTypeOperator``\n   * - ``airflow.contrib.operators.gcp_container_operator.GKEClusterCreateOperator``\n     - ``airflow.providers.google.cloud.operators.kubernetes_engine.GKECreateClusterOperator``\n   * - ``airflow.contrib.operators.gcp_container_operator.GKEClusterDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.kubernetes_engine.GKEDeleteClusterOperator``\n   * - ``airflow.contrib.operators.gcp_container_operator.GKEPodOperator``\n     - ``airflow.providers.google.cloud.operators.kubernetes_engine.GKEStartPodOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPCancelDLPJobOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPCancelDLPJobOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPCreateDLPJobOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPCreateDLPJobOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPCreateDeidentifyTemplateOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPCreateDeidentifyTemplateOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPCreateInspectTemplateOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPCreateInspectTemplateOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPCreateJobTriggerOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPCreateJobTriggerOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPCreateStoredInfoTypeOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPCreateStoredInfoTypeOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPDeidentifyContentOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPDeidentifyContentOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPDeleteDeidentifyTemplateOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPDeleteDeidentifyTemplateOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPDeleteDlpJobOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPDeleteDLPJobOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPDeleteInspectTemplateOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPDeleteInspectTemplateOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPDeleteJobTriggerOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPDeleteJobTriggerOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPDeleteStoredInfoTypeOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPDeleteStoredInfoTypeOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPGetDeidentifyTemplateOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPGetDeidentifyTemplateOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPGetDlpJobOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPGetDLPJobOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPGetInspectTemplateOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPGetInspectTemplateOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPGetJobTripperOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPGetJobTriggerOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPGetStoredInfoTypeOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPGetStoredInfoTypeOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPInspectContentOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPInspectContentOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPListDeidentifyTemplatesOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPListDeidentifyTemplatesOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPListDlpJobsOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPListDLPJobsOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPListInfoTypesOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPListInfoTypesOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPListInspectTemplatesOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPListInspectTemplatesOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPListJobTriggersOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPListJobTriggersOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPListStoredInfoTypesOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPListStoredInfoTypesOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPRedactImageOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPRedactImageOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPReidentifyContentOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPReidentifyContentOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPUpdateDeidentifyTemplateOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPUpdateDeidentifyTemplateOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPUpdateInspectTemplateOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPUpdateInspectTemplateOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPUpdateJobTriggerOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPUpdateJobTriggerOperator``\n   * - ``airflow.contrib.operators.gcp_dlp_operator.CloudDLPUpdateStoredInfoTypeOperator``\n     - ``airflow.providers.google.cloud.operators.dlp.CloudDLPUpdateStoredInfoTypeOperator``\n   * - ``airflow.contrib.operators.gcp_function_operator.GcfFunctionDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.functions.GcfFunctionDeleteOperator``\n   * - ``airflow.contrib.operators.gcp_function_operator.GcfFunctionDeployOperator``\n     - ``airflow.providers.google.cloud.operators.functions.GcfFunctionDeployOperator``\n   * - ``airflow.contrib.operators.gcp_natural_language_operator.CloudNaturalLanguageAnalyzeEntitiesOperator``\n     - ``airflow.providers.google.cloud.operators.natural_language.CloudNaturalLanguageAnalyzeEntitiesOperator``\n   * - ``airflow.contrib.operators.gcp_natural_language_operator.CloudNaturalLanguageAnalyzeEntitySentimentOperator``\n     - ``airflow.providers.google.cloud.operators.natural_language.CloudNaturalLanguageAnalyzeEntitySentimentOperator``\n   * - ``airflow.contrib.operators.gcp_natural_language_operator.CloudNaturalLanguageAnalyzeSentimentOperator``\n     - ``airflow.providers.google.cloud.operators.natural_language.CloudNaturalLanguageAnalyzeSentimentOperator``\n   * - ``airflow.contrib.operators.gcp_natural_language_operator.CloudNaturalLanguageClassifyTextOperator``\n     - ``airflow.providers.google.cloud.operators.natural_language.CloudNaturalLanguageClassifyTextOperator``\n   * - ``airflow.contrib.operators.gcp_spanner_operator.CloudSpannerInstanceDatabaseDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.spanner.SpannerDeleteDatabaseInstanceOperator``\n   * - ``airflow.contrib.operators.gcp_spanner_operator.CloudSpannerInstanceDatabaseDeployOperator``\n     - ``airflow.providers.google.cloud.operators.spanner.SpannerDeployDatabaseInstanceOperator``\n   * - ``airflow.contrib.operators.gcp_spanner_operator.CloudSpannerInstanceDatabaseQueryOperator``\n     - ``airflow.providers.google.cloud.operators.spanner.SpannerQueryDatabaseInstanceOperator``\n   * - ``airflow.contrib.operators.gcp_spanner_operator.CloudSpannerInstanceDatabaseUpdateOperator``\n     - ``airflow.providers.google.cloud.operators.spanner.SpannerUpdateDatabaseInstanceOperator``\n   * - ``airflow.contrib.operators.gcp_spanner_operator.CloudSpannerInstanceDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.spanner.SpannerDeleteInstanceOperator``\n   * - ``airflow.contrib.operators.gcp_spanner_operator.CloudSpannerInstanceDeployOperator``\n     - ``airflow.providers.google.cloud.operators.spanner.SpannerDeployInstanceOperator``\n   * - ``airflow.contrib.operators.gcp_speech_to_text_operator.GcpSpeechToTextRecognizeSpeechOperator``\n     - ``airflow.providers.google.cloud.operators.speech_to_text.CloudSpeechToTextRecognizeSpeechOperator``\n   * - ``airflow.contrib.operators.gcp_text_to_speech_operator.GcpTextToSpeechSynthesizeOperator``\n     - ``airflow.providers.google.cloud.operators.text_to_speech.CloudTextToSpeechSynthesizeOperator``\n   * - ``airflow.contrib.operators.gcp_transfer_operator.GcpTransferServiceJobCreateOperator``\n     - ``airflow.providers.google.cloud.operators.cloud_storage_transfer_service.CloudDataTransferServiceCreateJobOperator``\n   * - ``airflow.contrib.operators.gcp_transfer_operator.GcpTransferServiceJobDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.cloud_storage_transfer_service.CloudDataTransferServiceDeleteJobOperator``\n   * - ``airflow.contrib.operators.gcp_transfer_operator.GcpTransferServiceJobUpdateOperator``\n     - ``airflow.providers.google.cloud.operators.cloud_storage_transfer_service.CloudDataTransferServiceUpdateJobOperator``\n   * - ``airflow.contrib.operators.gcp_transfer_operator.GcpTransferServiceOperationCancelOperator``\n     - ``airflow.providers.google.cloud.operators.cloud_storage_transfer_service.CloudDataTransferServiceCancelOperationOperator``\n   * - ``airflow.contrib.operators.gcp_transfer_operator.GcpTransferServiceOperationGetOperator``\n     - ``airflow.providers.google.cloud.operators.cloud_storage_transfer_service.CloudDataTransferServiceGetOperationOperator``\n   * - ``airflow.contrib.operators.gcp_transfer_operator.GcpTransferServiceOperationPauseOperator``\n     - ``airflow.providers.google.cloud.operators.cloud_storage_transfer_service.CloudDataTransferServicePauseOperationOperator``\n   * - ``airflow.contrib.operators.gcp_transfer_operator.GcpTransferServiceOperationResumeOperator``\n     - ``airflow.providers.google.cloud.operators.cloud_storage_transfer_service.CloudDataTransferServiceResumeOperationOperator``\n   * - ``airflow.contrib.operators.gcp_transfer_operator.GcpTransferServiceOperationsListOperator``\n     - ``airflow.providers.google.cloud.operators.cloud_storage_transfer_service.CloudDataTransferServiceListOperationsOperator``\n   * - ``airflow.contrib.operators.gcp_transfer_operator.GoogleCloudStorageToGoogleCloudStorageTransferOperator``\n     - ``airflow.providers.google.cloud.operators.cloud_storage_transfer_service.CloudDataTransferServiceGCSToGCSOperator``\n   * - ``airflow.contrib.operators.gcp_translate_operator.CloudTranslateTextOperator``\n     - ``airflow.providers.google.cloud.operators.translate.CloudTranslateTextOperator``\n   * - ``airflow.contrib.operators.gcp_translate_speech_operator.GcpTranslateSpeechOperator``\n     - ``airflow.providers.google.cloud.operators.translate_speech.GcpTranslateSpeechOperator``\n   * - ``airflow.contrib.operators.gcp_video_intelligence_operator.CloudVideoIntelligenceDetectVideoExplicitContentOperator``\n     - ``airflow.providers.google.cloud.operators.video_intelligence.CloudVideoIntelligenceDetectVideoExplicitContentOperator``\n   * - ``airflow.contrib.operators.gcp_video_intelligence_operator.CloudVideoIntelligenceDetectVideoLabelsOperator``\n     - ``airflow.providers.google.cloud.operators.video_intelligence.CloudVideoIntelligenceDetectVideoLabelsOperator``\n   * - ``airflow.contrib.operators.gcp_video_intelligence_operator.CloudVideoIntelligenceDetectVideoShotsOperator``\n     - ``airflow.providers.google.cloud.operators.video_intelligence.CloudVideoIntelligenceDetectVideoShotsOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionAddProductToProductSetOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionAddProductToProductSetOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionAnnotateImageOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionImageAnnotateOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionDetectDocumentTextOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionTextDetectOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionDetectImageLabelsOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionDetectImageLabelsOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionDetectImageSafeSearchOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionDetectImageSafeSearchOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionDetectTextOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionDetectTextOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductCreateOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionCreateProductOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionDeleteProductOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductGetOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionGetProductOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionCreateProductSetOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionDeleteProductSetOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetGetOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionGetProductSetOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetUpdateOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionUpdateProductSetOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductUpdateOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionUpdateProductOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionReferenceImageCreateOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionCreateReferenceImageOperator``\n   * - ``airflow.contrib.operators.gcp_vision_operator.CloudVisionRemoveProductFromProductSetOperator``\n     - ``airflow.providers.google.cloud.operators.vision.CloudVisionRemoveProductFromProductSetOperator``\n   * - ``airflow.contrib.operators.gcs_acl_operator.GoogleCloudStorageBucketCreateAclEntryOperator``\n     - ``airflow.providers.google.cloud.operators.gcs.GCSBucketCreateAclEntryOperator``\n   * - ``airflow.contrib.operators.gcs_acl_operator.GoogleCloudStorageObjectCreateAclEntryOperator``\n     - ``airflow.providers.google.cloud.operators.gcs.GCSObjectCreateAclEntryOperator``\n   * - ``airflow.contrib.operators.gcs_delete_operator.GoogleCloudStorageDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.gcs.GCSDeleteObjectsOperator``\n   * - ``airflow.contrib.operators.gcs_download_operator.GoogleCloudStorageDownloadOperator``\n     - ``airflow.providers.google.cloud.operators.gcs.GCSToLocalFilesystemOperator``\n   * - ``airflow.contrib.operators.gcs_list_operator.GoogleCloudStorageListOperator``\n     - ``airflow.providers.google.cloud.operators.gcs.GCSListObjectsOperator``\n   * - ``airflow.contrib.operators.gcs_operator.GoogleCloudStorageCreateBucketOperator``\n     - ``airflow.providers.google.cloud.operators.gcs.GCSCreateBucketOperator``\n   * - ``airflow.contrib.operators.gcs_to_bq.GoogleCloudStorageToBigQueryOperator``\n     - ``airflow.operators.gcs_to_bq.GoogleCloudStorageToBigQueryOperator``\n   * - ``airflow.contrib.operators.gcs_to_gcs.GoogleCloudStorageToGoogleCloudStorageOperator``\n     - ``airflow.operators.gcs_to_gcs.GoogleCloudStorageToGoogleCloudStorageOperator``\n   * - ``airflow.contrib.operators.gcs_to_s3.GoogleCloudStorageToS3Operator``\n     - ``airflow.operators.gcs_to_s3.GCSToS3Operator``\n   * - ``airflow.contrib.operators.mlengine_operator.MLEngineBatchPredictionOperator``\n     - ``airflow.providers.google.cloud.operators.mlengine.MLEngineStartBatchPredictionJobOperator``\n   * - ``airflow.contrib.operators.mlengine_operator.MLEngineModelOperator``\n     - ``airflow.providers.google.cloud.operators.mlengine.MLEngineManageModelOperator``\n   * - ``airflow.contrib.operators.mlengine_operator.MLEngineTrainingOperator``\n     - ``airflow.providers.google.cloud.operators.mlengine.MLEngineStartTrainingJobOperator``\n   * - ``airflow.contrib.operators.mlengine_operator.MLEngineVersionOperator``\n     - ``airflow.providers.google.cloud.operators.mlengine.MLEngineManageVersionOperator``\n   * - ``airflow.contrib.operators.mssql_to_gcs.MsSqlToGoogleCloudStorageOperator``\n     - ``airflow.operators.mssql_to_gcs.MsSqlToGoogleCloudStorageOperator``\n   * - ``airflow.contrib.operators.mysql_to_gcs.MySqlToGoogleCloudStorageOperator``\n     - ``airflow.operators.mysql_to_gcs.MySqlToGoogleCloudStorageOperator``\n   * - ``airflow.contrib.operators.postgres_to_gcs_operator.PostgresToGoogleCloudStorageOperator``\n     - ``airflow.operators.postgres_to_gcs.PostgresToGoogleCloudStorageOperator``\n   * - ``airflow.contrib.operators.pubsub_operator.PubSubPublishOperator``\n     - ``airflow.providers.google.cloud.operators.pubsub.PubSubPublishMessageOperator``\n   * - ``airflow.contrib.operators.pubsub_operator.PubSubSubscriptionCreateOperator``\n     - ``airflow.providers.google.cloud.operators.pubsub.PubSubCreateSubscriptionOperator``\n   * - ``airflow.contrib.operators.pubsub_operator.PubSubSubscriptionDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.pubsub.PubSubDeleteSubscriptionOperator``\n   * - ``airflow.contrib.operators.pubsub_operator.PubSubTopicCreateOperator``\n     - ``airflow.providers.google.cloud.operators.pubsub.PubSubCreateTopicOperator``\n   * - ``airflow.contrib.operators.pubsub_operator.PubSubTopicDeleteOperator``\n     - ``airflow.providers.google.cloud.operators.pubsub.PubSubDeleteTopicOperator``\n   * - ``airflow.contrib.operators.sql_to_gcs.BaseSQLToGoogleCloudStorageOperator``\n     - ``airflow.operators.sql_to_gcs.BaseSQLToGoogleCloudStorageOperator``\n   * - ``airflow.contrib.sensors.bigquery_sensor.BigQueryTableSensor``\n     - ``airflow.providers.google.cloud.sensors.bigquery.BigQueryTableExistenceSensor``\n   * - ``airflow.contrib.sensors.gcp_transfer_sensor.GCPTransferServiceWaitForJobStatusSensor``\n     - ``airflow.providers.google.cloud.sensors.cloud_storage_transfer_service.DataTransferServiceJobStatusSensor``\n   * - ``airflow.contrib.sensors.gcs_sensor.GoogleCloudStorageObjectSensor``\n     - ``airflow.providers.google.cloud.sensors.gcs.GCSObjectExistenceSensor``\n   * - ``airflow.contrib.sensors.gcs_sensor.GoogleCloudStorageObjectUpdatedSensor``\n     - ``airflow.providers.google.cloud.sensors.gcs.GCSObjectUpdateSensor``\n   * - ``airflow.contrib.sensors.gcs_sensor.GoogleCloudStoragePrefixSensor``\n     - ``airflow.providers.google.cloud.sensors.gcs.GCSObjectsWithPrefixExistenceSensor``\n   * - ``airflow.contrib.sensors.gcs_sensor.GoogleCloudStorageUploadSessionCompleteSensor``\n     - ``airflow.providers.google.cloud.sensors.gcs.GCSUploadSessionCompleteSensor``\n   * - ``airflow.contrib.sensors.pubsub_sensor.PubSubPullSensor``\n     - ``airflow.providers.google.cloud.sensors.pubsub.PubSubPullSensor``\n\n\nUnify default conn_id for Google Cloud\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPreviously not all hooks and operators related to Google Cloud use\n``google_cloud_default`` as a default conn_id. There is currently one default\nvariant. Values like ``google_cloud_storage_default``\\ , ``bigquery_default``\\ ,\n``google_cloud_datastore_default`` have been deprecated. The configuration of\nexisting relevant connections in the database have been preserved. To use those\ndeprecated GCP conn_id, you need to explicitly pass their conn_id into\noperators/hooks. Otherwise, ``google_cloud_default`` will be used as GCP's conn_id\nby default.\n\n``airflow.providers.google.cloud.hooks.dataflow.DataflowHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.dataflow.DataflowCreateJavaJobOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.dataflow.DataflowTemplatedJobStartOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.dataflow.DataflowCreatePythonJobOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo use project_id argument consistently across GCP hooks and operators, we did the following changes:\n\n\n* Changed order of arguments in DataflowHook.start_python_dataflow. Uses\n    with positional arguments may break.\n* Changed order of arguments in DataflowHook.is_job_dataflow_running. Uses\n    with positional arguments may break.\n* Changed order of arguments in DataflowHook.cancel_job. Uses\n    with positional arguments may break.\n* Added optional project_id argument to DataflowCreateJavaJobOperator\n    constructor.\n* Added optional project_id argument to DataflowTemplatedJobStartOperator\n    constructor.\n* Added optional project_id argument to DataflowCreatePythonJobOperator\n    constructor.\n\n``airflow.providers.google.cloud.sensors.gcs.GCSUploadSessionCompleteSensor``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo provide more precise control in handling of changes to objects in\nunderlying GCS Bucket the constructor of this sensor now has changed.\n\n\n* Old Behavior: This constructor used to optionally take ``previous_num_objects: int``.\n* New replacement constructor kwarg: ``previous_objects: Optional[Set[str]]``.\n\nMost users would not specify this argument because the bucket begins empty\nand the user wants to treat any files as new.\n\nExample of Updating usage of this sensor:\nUsers who used to call:\n\n``GCSUploadSessionCompleteSensor(bucket='my_bucket', prefix='my_prefix', previous_num_objects=1)``\n\nWill now call:\n\n``GCSUploadSessionCompleteSensor(bucket='my_bucket', prefix='my_prefix', previous_num_objects={'.keep'})``\n\nWhere '.keep' is a single file at your prefix that the sensor should not consider new.\n\n``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo simplify BigQuery operators (no need of ``Cursor``\\ ) and standardize usage of hooks within all GCP integration methods from ``BiqQueryBaseCursor``\nwere moved to ``BigQueryHook``. Using them by from ``Cursor`` object is still possible due to preserved backward compatibility but they will raise ``DeprecationWarning``.\nThe following methods were moved:\n\n.. list-table::\n   :header-rows: 1\n\n   * - Old path\n     - New path\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.cancel_query``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.cancel_query``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.create_empty_dataset``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.create_empty_dataset``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.create_empty_table``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.create_empty_table``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.create_external_table``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.create_external_table``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.delete_dataset``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.delete_dataset``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.get_dataset``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.get_dataset``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.get_dataset_tables``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.get_dataset_tables``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.get_dataset_tables_list``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.get_dataset_tables_list``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.get_datasets_list``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.get_datasets_list``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.get_schema``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.get_schema``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.get_tabledata``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.get_tabledata``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.insert_all``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.insert_all``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.patch_dataset``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.patch_dataset``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.patch_table``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.patch_table``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.poll_job_complete``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.poll_job_complete``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.run_copy``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.run_copy``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.run_extract``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.run_extract``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.run_grant_dataset_view_access``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.run_grant_dataset_view_access``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.run_load``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.run_load``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.run_query``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.run_query``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.run_table_delete``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.run_table_delete``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.run_table_upsert``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.run_table_upsert``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.run_with_configuration``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.run_with_configuration``\n   * - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.update_dataset``\n     - ``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook.update_dataset``\n\n\n``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSince BigQuery is the part of the GCP it was possible to simplify the code by handling the exceptions\nby usage of the ``airflow.providers.google.common.hooks.base.GoogleBaseHook.catch_http_exception`` decorator however it changes\nexceptions raised by the following methods:\n\n\n* ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.run_table_delete`` raises ``AirflowException`` instead of ``Exception``.\n* ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.create_empty_dataset`` raises ``AirflowException`` instead of ``ValueError``.\n* ``airflow.providers.google.cloud.hooks.bigquery.BigQueryBaseCursor.get_dataset`` raises ``AirflowException`` instead of ``ValueError``.\n\n``airflow.providers.google.cloud.operators.bigquery.BigQueryCreateEmptyTableOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.bigquery.BigQueryCreateEmptyDatasetOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIdempotency was added to ``BigQueryCreateEmptyTableOperator`` and ``BigQueryCreateEmptyDatasetOperator``.\nBut to achieve that try / except clause was removed from ``create_empty_dataset`` and ``create_empty_table``\nmethods of ``BigQueryHook``.\n\n``airflow.providers.google.cloud.hooks.dataflow.DataflowHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.hooks.mlengine.MLEngineHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.hooks.pubsub.PubSubHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe change in GCP operators implies that GCP Hooks for those operators require now keyword parameters rather\nthan positional ones in all methods where ``project_id`` is used. The methods throw an explanatory exception\nin case they are called using positional parameters.\n\nOther GCP hooks are unaffected.\n\n``airflow.providers.google.cloud.hooks.pubsub.PubSubHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.pubsub.PubSubTopicCreateOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.pubsub.PubSubSubscriptionCreateOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.pubsub.PubSubTopicDeleteOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.pubsub.PubSubSubscriptionDeleteOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.pubsub.PubSubPublishOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.sensors.pubsub.PubSubPullSensor``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn the ``PubSubPublishOperator`` and ``PubSubHook.publish`` method the data field in a message should be bytestring (utf-8 encoded) rather than base64 encoded string.\n\nDue to the normalization of the parameters within GCP operators and hooks a parameters like ``project`` or ``topic_project``\nare deprecated and will be substituted by parameter ``project_id``.\nIn ``PubSubHook.create_subscription`` hook method in the parameter ``subscription_project`` is replaced by ``subscription_project_id``.\nTemplate fields are updated accordingly and old ones may not work.\n\nIt is required now to pass key-word only arguments to ``PubSub`` hook.\n\nThese changes are not backward compatible.\n\n``airflow.providers.google.cloud.operators.kubernetes_engine.GKEStartPodOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe gcp_conn_id parameter in GKEPodOperator is required. In previous versions, it was possible to pass\nthe ``None`` value to the ``gcp_conn_id`` in the GKEStartPodOperator\noperator, which resulted in credentials being determined according to the\n`Application Default Credentials <https://cloud.google.com/docs/authentication/production>`_ strategy.\n\nNow this parameter requires a value. To restore the previous behavior, configure the connection without\nspecifying the service account.\n\nDetailed information about connection management is available:\n`Google Cloud Connection <https://airflow.apache.org/howto/connection/gcp.html>`_.\n\n``airflow.providers.google.cloud.hooks.gcs.GCSHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n*\n  The following parameters have been replaced in all the methods in GCSHook:\n\n\n  * ``bucket`` is changed to ``bucket_name``\n  * ``object`` is changed to ``object_name``\n\n*\n  The ``maxResults`` parameter in ``GoogleCloudStorageHook.list`` has been renamed to ``max_results`` for consistency.\n\n``airflow.providers.google.cloud.operators.dataproc.DataprocSubmitPigJobOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.dataproc.DataprocSubmitHiveJobOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.dataproc.DataprocSubmitSparkSqlJobOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.dataproc.DataprocSubmitSparkJobOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.dataproc.DataprocSubmitHadoopJobOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.dataproc.DataprocSubmitPySparkJobOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe 'properties' and 'jars' properties for the Dataproc related operators (\\ ``DataprocXXXOperator``\\ ) have been renamed from\n``dataproc_xxxx_properties`` and ``dataproc_xxx_jars``  to ``dataproc_properties``\nand ``dataproc_jars``\\ respectively.\nArguments for dataproc_properties dataproc_jars\n\n``airflow.providers.google.cloud.operators.cloud_storage_transfer_service.CloudDataTransferServiceCreateJobOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo obtain pylint compatibility the ``filter`` argument in ``CloudDataTransferServiceCreateJobOperator``\nhas been renamed to ``request_filter``.\n\n``airflow.providers.google.cloud.hooks.cloud_storage_transfer_service.CloudDataTransferServiceHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n To obtain pylint compatibility the ``filter`` argument in ``CloudDataTransferServiceHook.list_transfer_job`` and\n ``CloudDataTransferServiceHook.list_transfer_operations`` has been renamed to ``request_filter``.\n\n``airflow.providers.google.cloud.hooks.bigquery.BigQueryHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn general all hook methods are decorated with ``@GoogleBaseHook.fallback_to_default_project_id`` thus\nparameters to hook can only be passed via keyword arguments.\n\n\n* ``create_empty_table`` method accepts now ``table_resource`` parameter. If provided all\n  other parameters are ignored.\n* ``create_empty_dataset`` will now use values from ``dataset_reference`` instead of raising error\n  if parameters were passed in ``dataset_reference`` and as arguments to method. Additionally validation\n  of ``dataset_reference`` is done using ``Dataset.from_api_repr``. Exception and log messages has been\n  changed.\n* ``update_dataset`` requires now new ``fields`` argument (breaking change)\n* ``delete_dataset`` has new signature (dataset_id, project_id, ...)\n  previous one was (project_id, dataset_id, ...) (breaking change)\n* ``get_tabledata`` returns list of rows instead of API response in dict format. This method is deprecated in\n  favor of ``list_rows``. (breaking change)\n\n``airflow.providers.google.cloud.hooks.cloud_build.CloudBuildHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.cloud_build.CloudBuildCreateBuildOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe ``api_version`` has been removed and will not be used since we migrate ``CloudBuildHook`` from using\n Discovery API to native google-cloud-build python library.\n\nThe ``body`` parameter in ``CloudBuildCreateBuildOperator`` has been deprecated.\n Instead, you should pass body using the ``build`` parameter.\n\n``airflow.providers.google.cloud.hooks.dataflow.DataflowHook.start_python_dataflow``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.hooks.dataflow.DataflowHook.start_python_dataflow``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.google.cloud.operators.dataflow.DataflowCreatePythonJobOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nChange python3 as Dataflow Hooks/Operators default interpreter\n\nNow the ``py_interpreter`` argument for DataFlow Hooks/Operators has been changed from python2 to python3.\n\n``airflow.providers.google.common.hooks.base_google.GoogleBaseHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo simplify the code, the decorator provide_gcp_credential_file has been moved from the inner-class.\n\nInstead of ``@GoogleBaseHook._Decorators.provide_gcp_credential_file``\\ ,\nyou should write ``@GoogleBaseHook.provide_gcp_credential_file``\n\n``airflow.providers.google.cloud.operators.dataproc.DataprocCreateClusterOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIt is highly recommended to have 1TB+ disk size for Dataproc to have sufficient throughput:\nhttps://cloud.google.com/compute/docs/disks/performance\n\nHence, the default value for ``master_disk_size`` in ``DataprocCreateClusterOperator`` has been changed from 500GB to 1TB.\n\nGenerating Cluster Config\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIf you are upgrading from Airflow 1.10.x and are not using **CLUSTER_CONFIG**\\ ,\nYou can easily generate config using **make()** of ``airflow.providers.google.cloud.operators.dataproc.ClusterGenerator``\n\nThis has been proved specially useful if you are using **metadata** argument from older API, refer `AIRFLOW-16911 <https://github.com/apache/airflow/issues/16911>`_ for details.\n\neg. your cluster creation may look like this in **v1.10.x**\n\n.. code-block:: python\n\n   path = f\"gs://goog-dataproc-initialization-actions-us-central1/python/pip-install.sh\"\n\n   create_cluster = DataprocClusterCreateOperator(\n       task_id=\"create_dataproc_cluster\",\n       cluster_name=\"test\",\n       project_id=\"test\",\n       zone=\"us-central1-a\",\n       region=\"us-central1\",\n       master_machine_type=\"n1-standard-4\",\n       worker_machine_type=\"n1-standard-4\",\n       num_workers=2,\n       storage_bucket=\"test_bucket\",\n       init_actions_uris=[path],\n       metadata={\"PIP_PACKAGES\": \"pyyaml requests pandas openpyxl\"},\n   )\n\nAfter upgrading to **v2.x.x** and using **CLUSTER_CONFIG**\\ , it will look like followed:\n\n.. code-block:: python\n\n   path = f\"gs://goog-dataproc-initialization-actions-us-central1/python/pip-install.sh\"\n\n   CLUSTER_CONFIG = ClusterGenerator(\n       project_id=\"test\",\n       zone=\"us-central1-a\",\n       master_machine_type=\"n1-standard-4\",\n       worker_machine_type=\"n1-standard-4\",\n       num_workers=2,\n       storage_bucket=\"test\",\n       init_actions_uris=[path],\n       metadata={\"PIP_PACKAGES\": \"pyyaml requests pandas openpyxl\"},\n   ).make()\n\n   create_cluster_operator = DataprocClusterCreateOperator(\n       task_id=\"create_dataproc_cluster\",\n       cluster_name=\"test\",\n       project_id=\"test\",\n       region=\"us-central1\",\n       cluster_config=CLUSTER_CONFIG,\n   )\n\n``airflow.providers.google.cloud.operators.bigquery.BigQueryGetDatasetTablesOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe changed signature of ``BigQueryGetDatasetTablesOperator``.\n\nBefore:\n\n.. code-block:: python\n\n   def __init__(\n       dataset_id: str,\n       dataset_resource: dict,\n       # ...\n   ): ...\n\nAfter:\n\n.. code-block:: python\n\n   def __init__(\n       dataset_resource: dict,\n       dataset_id: Optional[str] = None,\n       # ...\n   ): ...\n\nChanges in ``amazon`` provider package\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWe strive to ensure that there are no changes that may affect the end user, and your Python files, but this\nrelease may contain changes that will require changes to your configuration, DAG Files or other integration\ne.g. custom operators.\n\nOnly changes unique to this provider are described here. You should still pay attention to the changes that\nhave been made to the core (including core operators) as they can affect the integration behavior\nof this provider.\n\nThis section describes the changes that have been made, and what you need to do to update your if\nyou use operators or hooks which integrate with Amazon services (including Amazon Web Service - AWS).\n\nMigration of AWS components\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAll AWS components (hooks, operators, sensors, example DAGs) will be grouped together as decided in\n`AIP-21 <https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-21%3A+Changes+in+import+paths>`_. Migrated\ncomponents remain backwards compatible but raise a ``DeprecationWarning`` when imported from the old module.\nMigrated are:\n\n.. list-table::\n   :header-rows: 1\n\n   * - Old path\n     - New path\n   * - ``airflow.hooks.S3_hook.S3Hook``\n     - ``airflow.providers.amazon.aws.hooks.s3.S3Hook``\n   * - ``airflow.contrib.hooks.aws_athena_hook.AWSAthenaHook``\n     - ``airflow.providers.amazon.aws.hooks.athena.AWSAthenaHook``\n   * - ``airflow.contrib.hooks.aws_lambda_hook.AwsLambdaHook``\n     - ``airflow.providers.amazon.aws.hooks.lambda_function.AwsLambdaHook``\n   * - ``airflow.contrib.hooks.aws_sqs_hook.SQSHook``\n     - ``airflow.providers.amazon.aws.hooks.sqs.SQSHook``\n   * - ``airflow.contrib.hooks.aws_sns_hook.AwsSnsHook``\n     - ``airflow.providers.amazon.aws.hooks.sns.AwsSnsHook``\n   * - ``airflow.contrib.operators.aws_athena_operator.AWSAthenaOperator``\n     - ``airflow.providers.amazon.aws.operators.athena.AWSAthenaOperator``\n   * - ``airflow.contrib.operators.awsbatch.AWSBatchOperator``\n     - ``airflow.providers.amazon.aws.operators.batch.AwsBatchOperator``\n   * - ``airflow.contrib.operators.awsbatch.BatchProtocol``\n     - ``airflow.providers.amazon.aws.hooks.batch_client.AwsBatchProtocol``\n   * - private attrs and methods on ``AWSBatchOperator``\n     - ``airflow.providers.amazon.aws.hooks.batch_client.AwsBatchClient``\n   * - n/a\n     - ``airflow.providers.amazon.aws.hooks.batch_waiters.AwsBatchWaiters``\n   * - ``airflow.contrib.operators.aws_sqs_publish_operator.SQSPublishOperator``\n     - ``airflow.providers.amazon.aws.operators.sqs.SQSPublishOperator``\n   * - ``airflow.contrib.operators.aws_sns_publish_operator.SnsPublishOperator``\n     - ``airflow.providers.amazon.aws.operators.sns.SnsPublishOperator``\n   * - ``airflow.contrib.sensors.aws_athena_sensor.AthenaSensor``\n     - ``airflow.providers.amazon.aws.sensors.athena.AthenaSensor``\n   * - ``airflow.contrib.sensors.aws_sqs_sensor.SQSSensor``\n     - ``airflow.providers.amazon.aws.sensors.sqs.SQSSensor``\n\n\n``airflow.providers.amazon.aws.hooks.emr.EmrHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.amazon.aws.operators.emr_add_steps.EmrAddStepsOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.amazon.aws.operators.emr_create_job_flow.EmrCreateJobFlowOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.amazon.aws.operators.emr_terminate_job_flow.EmrTerminateJobFlowOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe default value for the `aws_conn_id <https://airflow.apache.org/howto/manage-connections.html#amazon-web-services>`_ was accidentally set to 's3_default' instead of 'aws_default' in some of the emr operators in previous\nversions. This was leading to EmrStepSensor not being able to find their corresponding emr cluster. With the new\nchanges in the EmrAddStepsOperator, EmrTerminateJobFlowOperator and EmrCreateJobFlowOperator this issue is\nsolved.\n\n``airflow.providers.amazon.aws.operators.batch.AwsBatchOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe ``AwsBatchOperator`` was refactored to extract an ``AwsBatchClient`` (and inherit from it).  The\nchanges are mostly backwards compatible and clarify the public API for these classes; some\nprivate methods on ``AwsBatchOperator`` for polling a job status were relocated and renamed\nto surface new public methods on ``AwsBatchClient`` (and via inheritance on ``AwsBatchOperator``\\ ).  A\ncouple of job attributes are renamed on an instance of ``AwsBatchOperator``\\ ; these were mostly\nused like private attributes but they were surfaced in the public API, so any use of them needs\nto be updated as follows:\n\n\n* ``AwsBatchOperator().jobId`` -> ``AwsBatchOperator().job_id``\n* ``AwsBatchOperator().jobName`` -> ``AwsBatchOperator().job_name``\n\nThe ``AwsBatchOperator`` gets a new option to define a custom model for waiting on job status changes.\nThe ``AwsBatchOperator`` can use a new ``waiters`` parameter, an instance of ``AwsBatchWaiters``\\ , to\nspecify that custom job waiters will be used to monitor a batch job.  See the latest API\ndocumentation for details.\n\n``airflow.providers.amazon.aws.sensors.athena.AthenaSensor``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nReplace parameter ``max_retires`` with ``max_retries`` to fix typo.\n\n``airflow.providers.amazon.aws.hooks.s3.S3Hook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNote: The order of arguments has changed for ``check_for_prefix``.\nThe ``bucket_name`` is now optional. It falls back to the ``connection schema`` attribute.\nThe ``delete_objects`` now returns ``None`` instead of a response, since the method now makes multiple api requests when the keys list length is > 1000.\n\nChanges in other provider packages\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWe strive to ensure that there are no changes that may affect the end user and your Python files, but this\nrelease may contain changes that will require changes to your configuration, DAG Files or other integration\ne.g. custom operators.\n\nOnly changes unique to providers are described here. You should still pay attention to the changes that\nhave been made to the core (including core operators) as they can affect the integration behavior\nof this provider.\n\nThis section describes the changes that have been made, and what you need to do to update your if\nyou use any code located in ``airflow.providers`` package.\n\nChanged return type of ``list_prefixes`` and ``list_keys`` methods in ``S3Hook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPreviously, the ``list_prefixes`` and ``list_keys`` methods returned ``None`` when there were no\nresults. The behavior has been changed to return an empty list instead of ``None`` in this\ncase.\n\nRemoved HipChat integration\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nHipChat has reached end of life and is no longer available.\n\nFor more information please see\nhttps://community.atlassian.com/t5/Stride-articles/Stride-and-Hipchat-Cloud-have-reached-End-of-Life-updated/ba-p/940248\n\n``airflow.providers.salesforce.hooks.salesforce.SalesforceHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nReplace parameter ``sandbox`` with ``domain``. According to change in simple-salesforce package.\n\nRename ``sign_in`` function to ``get_conn``.\n\n``airflow.providers.apache.pinot.hooks.pinot.PinotAdminHook.create_segment``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nRename parameter name from ``format`` to ``segment_format`` in PinotAdminHook function create_segment for pylint compatible\n\n``airflow.providers.apache.hive.hooks.hive.HiveMetastoreHook.get_partitions``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nRename parameter name from ``filter`` to ``partition_filter`` in HiveMetastoreHook function get_partitions for pylint compatible\n\n``airflow.providers.ftp.hooks.ftp.FTPHook.list_directory``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nRemove unnecessary parameter ``nlst`` in FTPHook function ``list_directory`` for pylint compatible\n\n``airflow.providers.postgres.hooks.postgres.PostgresHook.copy_expert``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nRemove unnecessary parameter ``open`` in PostgresHook function ``copy_expert`` for pylint compatible\n\n``airflow.providers.opsgenie.operators.opsgenie_alert.OpsgenieAlertOperator``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nChange parameter name from ``visibleTo`` to ``visible_to`` in OpsgenieAlertOperator for pylint compatible\n\n``airflow.providers.imap.hooks.imap.ImapHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``airflow.providers.imap.sensors.imap_attachment.ImapAttachmentSensor``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nImapHook:\n\n\n* The order of arguments has changed for ``has_mail_attachment``\\ ,\n  ``retrieve_mail_attachments`` and ``download_mail_attachments``.\n* A new ``mail_filter`` argument has been added to each of those.\n\n``airflow.providers.http.hooks.http.HttpHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe HTTPHook is now secured by default: ``verify=True`` (before: ``verify=False``\\ )\nThis can be overwritten by using the extra_options param as ``{'verify': False}``.\n\n``airflow.providers.cloudant.hooks.cloudant.CloudantHook``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n* upgraded cloudant version from ``>=0.5.9,<2.0`` to ``>=2.0``\n* removed the use of the ``schema`` attribute in the connection\n* removed ``db`` function since the database object can also be retrieved by calling ``cloudant_session['database_name']``\n\nFor example:\n\n.. code-block:: python\n\n   from airflow.providers.cloudant.hooks.cloudant import CloudantHook\n\n   with CloudantHook().get_conn() as cloudant_session:\n       database = cloudant_session[\"database_name\"]\n\nSee the `docs <https://python-cloudant.readthedocs.io/en/latest/>`_ for more information on how to use the new cloudant version.\n\n``airflow.providers.snowflake``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhen initializing a Snowflake hook or operator, the value used for ``snowflake_conn_id`` was always ``snowflake_conn_id``\\ , regardless of whether or not you specified a value for it. The default ``snowflake_conn_id`` value is now switched to ``snowflake_default`` for consistency and will be properly overridden when specified.\n\nOther changes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThis release also includes changes that fall outside any of the sections above.\n\nStandardized \"extra\" requirements\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe standardized the Extras names and synchronized providers package names with the main airflow extras.\n\nWe deprecated a number of extras in 2.0.\n\n.. list-table::\n   :header-rows: 1\n\n   * - Deprecated extras\n     - New extras\n   * - atlas\n     - apache.atlas\n   * - aws\n     - amazon\n   * - azure\n     - microsoft.azure\n   * - azure_blob_storage\n     - microsoft.azure\n   * - azure_data_lake\n     - microsoft.azure\n   * - azure_cosmos\n     - microsoft.azure\n   * - azure_container_instances\n     - microsoft.azure\n   * - cassandra\n     - apache.cassandra\n   * - druid\n     - apache.druid\n   * - gcp\n     - google\n   * - gcp_api\n     - google\n   * - hdfs\n     - apache.hdfs\n   * - hive\n     - apache.hive\n   * - kubernetes\n     - cncf.kubernetes\n   * - mssql\n     - microsoft.mssql\n   * - pinot\n     - apache.pinot\n   * - webhdfs\n     - apache.webhdfs\n   * - winrm\n     - apache.winrm\n\n\nFor example:\n\nIf you want to install integration for Apache Atlas, then instead of ``pip install apache-airflow[atlas]``\nyou should use ``pip install apache-airflow[apache.atlas]``.\n\nNOTE!\n\nIf you want to install integration for Microsoft Azure, then instead of\n\n.. code-block::\n\n   pip install 'apache-airflow[azure_blob_storage,azure_data_lake,azure_cosmos,azure_container_instances]'\n\nyou should run ``pip install 'apache-airflow[microsoft.azure]'``\n\nIf you want to install integration for Amazon Web Services, then instead of\n``pip install 'apache-airflow[s3,emr]'``\\ , you should execute ``pip install 'apache-airflow[aws]'``\n\nThe deprecated extras will be removed in 3.0.\n\nSimplify the response payload of endpoints /dag_stats and /task_stats\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe response of endpoints ``/dag_stats`` and ``/task_stats`` help UI fetch brief statistics about DAGs and Tasks. The format was like\n\n.. code-block:: json\n\n   {\n       \"example_http_operator\": [\n           {\n               \"state\": \"success\",\n               \"count\": 0,\n               \"dag_id\": \"example_http_operator\",\n               \"color\": \"green\"\n           },\n           {\n               \"state\": \"running\",\n               \"count\": 0,\n               \"dag_id\": \"example_http_operator\",\n               \"color\": \"lime\"\n           }\n       ]\n   }\n\nThe ``dag_id`` was repeated in the payload, which makes the response payload unnecessarily bigger.\n\nNow the ``dag_id`` will not appear repeated in the payload, and the response format is like\n\n.. code-block:: json\n\n   {\n       \"example_http_operator\": [\n           {\n               \"state\": \"success\",\n               \"count\": 0,\n               \"color\": \"green\"\n           },\n           {\n               \"state\": \"running\",\n               \"count\": 0,\n               \"color\": \"lime\"\n           }\n       ]\n   }\n\nAirflow 1.10.15 (2021-03-17)\n----------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n^^^^^^^^^\n\n- Fix ``airflow db upgrade`` to upgrade db as intended (#13267)\n- Moved boto3 limitation to snowflake (#13286)\n- ``KubernetesExecutor`` should accept images from ``executor_config`` (#13074)\n- Scheduler should acknowledge active runs properly (#13803)\n- Include ``airflow/contrib/executors`` in the dist package\n- Pin Click version for Python 2.7 users\n- Ensure all StatsD timers use millisecond values. (#10633)\n- [``kubernetes_generate_dag_yaml``] - Fix dag yaml generate function (#13816)\n- Fix ``airflow tasks clear`` cli command with ``--yes`` (#14188)\n- Fix permission error on non-POSIX filesystem (#13121) (#14383)\n- Fixed deprecation message for \"variables\" command (#14457)\n- BugFix: fix the ``delete_dag`` function of json_client (#14441)\n- Fix merging of secrets and configmaps for ``KubernetesExecutor`` (#14090)\n- Fix webserver exiting when gunicorn master crashes (#13470)\n- Bump ``ini`` from 1.3.5 to 1.3.8 in ``airflow/www_rbac``\n- Bump ``datatables.net`` from 1.10.21 to 1.10.23 in ``airflow/www_rbac``\n- Webserver: Sanitize string passed to origin param (#14738)\n- Make ``rbac_app``'s ``db.session`` use the same timezone with ``@provide_session`` (#14025)\n\nImprovements\n^^^^^^^^^^^^\n\n- Adds airflow as viable docker command in official image (#12878)\n- ``StreamLogWriter``: Provide (no-op) close method (#10885)\n- Add 'airflow variables list' command for 1.10.x transition version (#14462)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Update URL for Airflow docs (#13561)\n- Clarifies version args for installing 1.10 in Docker (#12875)\n\nAirflow 1.10.14 (2020-12-10)\n----------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\n``[scheduler] max_threads`` config has been renamed to ``[scheduler] parsing_processes``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nFrom Airflow 1.10.14, ``max_threads`` config under ``[scheduler]`` section has been renamed to ``parsing_processes``.\n\nThis is to align the name with the actual code where the Scheduler launches the number of processes defined by\n``[scheduler] parsing_processes`` to parse the DAG files.\n\nAirflow CLI changes in line with 2.0\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe Airflow CLI has been organized so that related commands are grouped together as subcommands,\nwhich means that if you use these commands in your scripts, they will now raise a DeprecationWarning and\nyou have to make changes to them before you upgrade to Airflow 2.0.\n\nThis section describes the changes that have been made, and what you need to do to update your script.\n\nThe ability to manipulate users from the command line has been changed. ``airflow create_user``\\ ,  ``airflow delete_user``\n and ``airflow list_users`` has been grouped to a single command ``airflow users`` with optional flags ``create``\\ , ``list`` and ``delete``.\n\nThe ``airflow list_dags`` command is now ``airflow dags list``\\ , ``airflow pause`` is ``airflow dags pause``\\ , etc.\n\nIn Airflow 1.10 and 2.0 there is an ``airflow config`` command but there is a difference in behavior. In Airflow 1.10,\nit prints all config options while in Airflow 2.0, it's a command group. ``airflow config`` is now ``airflow config list``.\nYou can check other options by running the command ``airflow config --help``\n\nCompatibility with the old CLI has been maintained, but they will no longer appear in the help\n\nYou can learn about the commands by running ``airflow --help``. For example to get help about the ``celery`` group command,\nyou have to run the help command: ``airflow celery --help``.\n\n.. list-table::\n   :header-rows: 1\n\n   * - Old command\n     - New command\n     - Group\n   * - ``airflow worker``\n     - ``airflow celery worker``\n     - ``celery``\n   * - ``airflow flower``\n     - ``airflow celery flower``\n     - ``celery``\n   * - ``airflow trigger_dag``\n     - ``airflow dags trigger``\n     - ``dags``\n   * - ``airflow delete_dag``\n     - ``airflow dags delete``\n     - ``dags``\n   * - ``airflow show_dag``\n     - ``airflow dags show``\n     - ``dags``\n   * - ``airflow list_dag``\n     - ``airflow dags list``\n     - ``dags``\n   * - ``airflow dag_status``\n     - ``airflow dags status``\n     - ``dags``\n   * - ``airflow backfill``\n     - ``airflow dags backfill``\n     - ``dags``\n   * - ``airflow list_dag_runs``\n     - ``airflow dags list-runs``\n     - ``dags``\n   * - ``airflow pause``\n     - ``airflow dags pause``\n     - ``dags``\n   * - ``airflow unpause``\n     - ``airflow dags unpause``\n     - ``dags``\n   * - ``airflow next_execution``\n     - ``airflow dags next-execution``\n     - ``dags``\n   * - ``airflow test``\n     - ``airflow tasks test``\n     - ``tasks``\n   * - ``airflow clear``\n     - ``airflow tasks clear``\n     - ``tasks``\n   * - ``airflow list_tasks``\n     - ``airflow tasks list``\n     - ``tasks``\n   * - ``airflow task_failed_deps``\n     - ``airflow tasks failed-deps``\n     - ``tasks``\n   * - ``airflow task_state``\n     - ``airflow tasks state``\n     - ``tasks``\n   * - ``airflow run``\n     - ``airflow tasks run``\n     - ``tasks``\n   * - ``airflow render``\n     - ``airflow tasks render``\n     - ``tasks``\n   * - ``airflow initdb``\n     - ``airflow db init``\n     - ``db``\n   * - ``airflow resetdb``\n     - ``airflow db reset``\n     - ``db``\n   * - ``airflow upgradedb``\n     - ``airflow db upgrade``\n     - ``db``\n   * - ``airflow checkdb``\n     - ``airflow db check``\n     - ``db``\n   * - ``airflow shell``\n     - ``airflow db shell``\n     - ``db``\n   * - ``airflow pool``\n     - ``airflow pools``\n     - ``pools``\n   * - ``airflow create_user``\n     - ``airflow users create``\n     - ``users``\n   * - ``airflow delete_user``\n     - ``airflow users delete``\n     - ``users``\n   * - ``airflow list_users``\n     - ``airflow users list``\n     - ``users``\n   * - ``airflow rotate_fernet_key``\n     - ``airflow rotate-fernet-key``\n     -\n   * - ``airflow sync_perm``\n     - ``airflow sync-perm``\n     -\n\nBug Fixes\n^^^^^^^^^\n\n- BugFix: Tasks with ``depends_on_past`` or ``task_concurrency`` are stuck (#12663)\n- Fix issue with empty Resources in executor_config (#12633)\n- Fix: Deprecated config ``force_log_out_after`` was not used (#12661)\n- Fix empty asctime field in JSON formatted logs (#10515)\n- [AIRFLOW-2809] Fix security issue regarding Flask SECRET_KEY (#3651)\n- [AIRFLOW-2884] Fix Flask SECRET_KEY security issue in www_rbac (#3729)\n- [AIRFLOW-2886] Generate random Flask SECRET_KEY in default config (#3738)\n- Add missing comma in setup.py (#12790)\n- Bugfix: Unable to import Airflow plugins on Python 3.8 (#12859)\n- Fix setup.py missing comma in ``setup_requires`` (#12880)\n- Don't emit first_task_scheduling_delay metric for only-once dags (#12835)\n\nImprovements\n^^^^^^^^^^^^\n\n- Update setup.py to get non-conflicting set of dependencies (#12636)\n- Rename ``[scheduler] max_threads`` to ``[scheduler] parsing_processes`` (#12605)\n- Add metric for scheduling delay between first run task & expected start time (#9544)\n- Add new-style 2.0 command names for Airflow 1.10.x (#12725)\n- Add Kubernetes cleanup-pods CLI command for Helm Chart (#11802)\n- Don't let webserver run with dangerous config (#12747)\n- Replace pkg_resources with ``importlib.metadata`` to avoid VersionConflict errors (#12694)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Clarified information about supported Databases\n\nAirflow 1.10.13 (2020-11-25)\n----------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nTimeSensor is now timezone aware\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPreviously ``TimeSensor`` always compared the ``target_time`` with the current time in UTC.\n\nNow it will compare ``target_time`` with the current time in the timezone of the DAG,\ndefaulting to the ``default_timezone`` in the global config.\n\nRemoved Kerberos support for HDFS hook\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe HDFS hook's Kerberos support has been removed due to removed ``python-krbV`` dependency from PyPI\nand generally lack of support for SSL in Python3 (Snakebite-py3 we use as dependency has no\nsupport for SSL connection to HDFS).\n\nSSL support still works for WebHDFS hook.\n\nUnify user session lifetime configuration\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIn previous version of Airflow user session lifetime could be configured by\n``session_lifetime_days`` and ``force_log_out_after`` options. In practice only ``session_lifetime_days``\nhad impact on session lifetime, but it was limited to values in day.\nWe have removed mentioned options and introduced new ``session_lifetime_minutes``\noption which simplify session lifetime configuration.\n\nBefore\n\n.. code-block:: ini\n\n   [webserver]\n   force_log_out_after = 0\n   session_lifetime_days = 30\n\nAfter\n\n.. code-block:: ini\n\n   [webserver]\n   session_lifetime_minutes = 43200\n\nAdding Operators, Hooks and Sensors via Airflow Plugins is deprecated\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe ability to import Operators, Hooks and Sensors via the plugin mechanism has been deprecated and will raise warnings\nin Airflow 1.10.13 and will be removed completely in Airflow 2.0.\n\nCheck https://airflow.apache.org/docs/1.10.13/howto/custom-operator.html to see how you can create and import\nCustom Hooks, Operators and Sensors.\n\nNew Features\n^^^^^^^^^^^^\n\n- Add \"already checked\" to failed pods in K8sPodOperator (#11368)\n- Pass SQLAlchemy engine options to FAB based UI (#11395)\n- [AIRFLOW-4438] Add Gzip compression to S3_hook (#8571)\n- Add permission \"extra_links\" for Viewer role and above (#10719)\n- Add generate_yaml command to easily test KubernetesExecutor before deploying pods (#10677)\n- Add Secrets backend for Microsoft Azure Key Vault (#10898)\n\nBug Fixes\n^^^^^^^^^\n\n- SkipMixin: Handle empty branches (#11120)\n- [AIRFLOW-5274] dag loading duration metric name too long (#5890)\n- Handle no Dagrun in DagrunIdDep (#8389) (#11343)\n- Fix Kubernetes Executor logs for long dag names (#10942)\n- Add on_kill support for the KubernetesPodOperator (#10666)\n- KubernetesPodOperator template fix (#10963)\n- Fix displaying of add serialized_dag table migration\n- Fix Start Date tooltip on DAGs page (#10637)\n- URL encode execution date in the Last Run link (#10595)\n- Fixes issue with affinity backcompat in Airflow 1.10\n- Fix KubernetesExecutor import in views.py\n- Fix issues with Gantt View (#12419)\n- Fix Entrypoint and _CMD config variables (#12411)\n- Fix operator field update for SerializedBaseOperator (#10924)\n- Limited cryptography to < 3.2 for Python 2.7\n- Install cattr on Python 3.7 - Fix docs build on RTD (#12045)\n- Limit version of marshmallow-sqlalchemy\n- Pin ``kubernetes`` to a max version of 11.0.0 (#11974)\n- Use snakebite-py3 for HDFS dependency for Python3 (#12340)\n- Removes snakebite kerberos dependency (#10865)\n- Fix failing dependencies for FAB and Celery (#10828)\n- Fix pod_mutation_hook for 1.10.13 (#10850)\n- Fix formatting of Host information\n- Fix Logout Google Auth issue in Non-RBAC UI (#11890)\n- Add missing imports to app.py (#10650)\n- Show Generic Error for Charts & Query View in old UI (#12495)\n- TimeSensor should respect the default_timezone config (#9699)\n- TimeSensor should respect DAG timezone (#9882)\n- Unify user session lifetime configuration (#11970)\n- Handle outdated webserver session timeout gracefully. (#12332)\n\n\nImprovements\n^^^^^^^^^^^^\n\n- Add XCom.deserialize_value to Airflow 1.10.13 (#12328)\n- Mount airflow.cfg to pod_template_file (#12311)\n- All k8s object must comply with JSON Schema (#12003)\n- Validate Airflow chart values.yaml & values.schema.json (#11990)\n- Pod template file uses custom custom env variable (#11480)\n- Bump attrs and cattrs dependencies (#11969)\n- Bump attrs to > 20.0 (#11799)\n- [AIRFLOW-3607] Only query DB once per DAG run for TriggerRuleDep (#4751)\n- Rename task with duplicate task_id\n- Manage Flask AppBuilder Tables using Alembic Migrations (#12352)\n- ``airflow test`` only works for tasks in 1.10, not whole dags (#11191)\n- Improve warning messaging for duplicate task_ids in a DAG (#11126)\n- Pins moto to 1.3.14 (#10986)\n- DbApiHook: Support kwargs in get_pandas_df (#9730)\n- Make grace_period_seconds option on K8sPodOperator (#10727)\n- Fix syntax error in Dockerfile 'maintainer' Label (#10899)\n- The entrypoints in Docker Image should be owned by Airflow (#10853)\n- Make dockerfiles Google Shell Guide Compliant (#10734)\n- clean-logs script for Dockerfile: trim logs before sleep (#10685)\n- When sending tasks to celery from a sub-process, reset signal handlers (#11278)\n- SkipMixin: Add missing session.commit() and test (#10421)\n- Webserver: Further Sanitize values passed to origin param (#12459)\n- Security upgrade lodash from 4.17.19 to 4.17.20 (#11095)\n- Log instead of raise an Error for unregistered OperatorLinks (#11959)\n- Mask Password in Log table when using the CLI (#11468)\n- [AIRFLOW-3607] Optimize dep checking when depends on past set and concurrency limit\n- Execute job cancel HTTPRequest in Dataproc Hook (#10361)\n- Use rst lexer to format Airflow upgrade check output (#11259)\n- Remove deprecation warning from contrib/kubernetes/pod.py\n- adding body as templated field for CloudSqlImportOperator (#10510)\n- Change log level for User's session to DEBUG (#12414)\n\nDeprecations\n^^^^^^^^^^^^\n\n- Deprecate importing Hooks from plugin-created module (#12133)\n- Deprecate adding Operators and Sensors via plugins (#12069)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- [Doc] Correct description for macro task_instance_key_str (#11062)\n- Checks if all the libraries in setup.py are listed in installation.rst file (#12023)\n- Revise \"Project Focus\" copy (#12011)\n- Move Project focus and Principles higher in the README (#11973)\n- Remove archived link from README.md (#11945)\n- Update download url for Airflow Version (#11800)\n- Add Project URLs for PyPI page (#11801)\n- Move Backport Providers docs to our docsite (#11136)\n- Refactor rebase copy (#11030)\n- Add missing images for kubernetes executor docs (#11083)\n- Fix indentation in executor_config example (#10467)\n- Enhanced the Kubernetes Executor doc  (#10433)\n- Refactor content to a markdown table (#10863)\n- Rename \"Beyond the Horizon\" section and refactor content (#10802)\n- Refactor official source section to use bullets (#10801)\n- Add section for official source code (#10678)\n- Add redbubble link to Airflow merchandise (#10359)\n- README Doc: Link to Airflow directory in ASF Directory (#11137)\n- Fix the default value for VaultBackend's config_path (#12518)\n\nAirflow 1.10.12 (2020-08-25)\n----------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nClearing tasks skipped by SkipMixin will skip them\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPreviously, when tasks skipped by SkipMixin (such as BranchPythonOperator, BaseBranchOperator and ShortCircuitOperator) are cleared, they execute. Since 1.10.12, when such skipped tasks are cleared,\nthey will be skipped again by the newly introduced NotPreviouslySkippedDep.\n\nThe pod_mutation_hook function will now accept a kubernetes V1Pod object\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nAs of airflow 1.10.12, using the ``airflow.contrib.kubernetes.Pod`` class in the ``pod_mutation_hook`` is now deprecated. Instead we recommend that users\ntreat the ``pod`` parameter as a ``kubernetes.client.models.V1Pod`` object. This means that users now have access to the full Kubernetes API\nwhen modifying airflow pods\n\npod_template_file option now available in the KubernetesPodOperator\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nUsers can now offer a path to a yaml for the KubernetesPodOperator using the ``pod_template_file`` parameter.\n\nNew Features\n^^^^^^^^^^^^\n\n- Add DateTimeSensor (#9697)\n- Add AirflowClusterPolicyViolation support to Airflow local settings (#10282)\n- Get Airflow configs with sensitive data from Secret Backends (#9645)\n- [AIRFLOW-4734] Upsert functionality for PostgresHook.insert_rows() (#8625)\n- Allow defining custom XCom class (#8560)\n\nBug Fixes\n^^^^^^^^^\n\n- Add pre 1.10.11 Kubernetes Paths back with Deprecation Warning (#10067)\n- Fixes PodMutationHook for backwards compatibility (#9903)\n- Fix bug in executor_config when defining resources (#9935)\n- Respect DAG Serialization setting when running sync_perm (#10321)\n- Show correct duration on graph view for running task (#8311) (#8675)\n- Fix regression in SQLThresholdCheckOperator (#9312)\n- [AIRFLOW-6931] Fixed migrations to find all dependencies for MSSQL (#9891)\n- Avoid sharing session with RenderedTaskInstanceFields write and delete (#9993)\n- Fix clear future recursive when ExternalTaskMarker is used (#9515)\n- Handle IntegrityError while creating TIs (#10136)\n- Fix airflow-webserver startup errors when using Kerberos Auth (#10047)\n- Fixes treatment of open slots in scheduler (#9316) (#9505)\n- Fix KubernetesPodOperator reattachment (#10230)\n- Fix more PodMutationHook issues for backwards compatibility (#10084)\n- [AIRFLOW-5391] Do not re-run skipped tasks when they are cleared (#7276)\n- Fix task_instance_mutation_hook (#9910)\n- Fixes failing formatting of DAG file containing {} in docstring (#9779)\n- Fix is_terminal_support_colors function (#9734)\n- Fix PythonVirtualenvOperator when using ``provide_context=True`` (#8256)\n- Fix issue with mounting volumes from secrets (#10366)\n- BugFix: K8s Executor Multinamespace mode is evaluated to true by default (#10410)\n- Make KubernetesExecutor recognize kubernetes_labels (#10412)\n- Fix broken Kubernetes PodRuntimeInfoEnv (#10478)\n\nImprovements\n^^^^^^^^^^^^\n\n- Use Hash of Serialized DAG to determine DAG is changed or not (#10227)\n- Update Serialized DAGs in Webserver when DAGs are Updated (#9851)\n- Do not Update Serialized DAGs in DB if DAG did not change (#9850)\n- Add __repr__ to SerializedDagModel (#9862)\n- Update JS packages to latest versions (#9811) (#9921)\n- UI Graph View: Focus upstream / downstream task dependencies on mouseover (#9303)\n- Allow ``image`` in ``KubernetesPodOperator`` to be templated (#10068)\n- [AIRFLOW-6843] Add delete_option_kwargs to delete_namespaced_pod (#7523)\n- Improve process terminating in scheduler_job (#8064)\n- Replace deprecated base classes used in bigquery_check_operator (#10272)\n- [AIRFLOW-5897] Allow setting -1 as pool slots value in webserver (#6550)\n- Limit all google-cloud api to <2.0.0 (#10317)\n- [AIRFLOW-6706] Lazy load operator extra links (#7327) (#10318)\n- Add Snowflake support to SQL operator and sensor (#9843)\n- Makes multi-namespace mode optional (#9570)\n- Pin pyarrow < 1.0\n- Pin pymongo version to <3.11.0\n- Pin google-cloud-container to <2 (#9901)\n- Dockerfile: Remove package.json and yarn.lock from the prod image (#9814)\n- Dockerfile: The group of embedded DAGs should be root to be OpenShift compatible (#9794)\n- Update upper limit of flask-swagger, gunicorn & jinja2 (#9684)\n- Webserver: Sanitize values passed to origin param (#10334)\n- Sort connection type list in add/edit page alphabetically (#8692)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Add new committers: Ry Walker & Leah Cole to project.rst (#9892)\n- Add Qingping Hou to committers list (#9725)\n- Updated link to official documentation (#9629)\n- Create a short-link for Airflow Slack Invites (#10034)\n- Set language on code-block on docs/howto/email-config.rst (#10238)\n- Remove duplicate line from 1.10.10 CHANGELOG (#10289)\n- Improve heading on Email Configuration page (#10175)\n- Fix link for the Jinja Project in docs/tutorial.rst (#10245)\n- Create separate section for Cron Presets (#10247)\n- Add Syntax Highlights to code-blocks in docs/best-practices.rst (#10258)\n- Fix docstrings in BigQueryGetDataOperator (#10042)\n- Fix typo in Task Lifecycle section (#9867)\n- Make Secret Backend docs clearer about Variable & Connection View (#8913)\n\nAirflow 1.10.11 (2020-07-10)\n----------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nUse NULL as default value for dag.description\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nNow use NULL as default value for dag.description in dag table\n\nRestrict editing DagRun State in the old UI (Flask-admin based UI)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nBefore 1.10.11 it was possible to edit DagRun State in the ``/admin/dagrun/`` page\n to any text.\n\nIn Airflow 1.10.11+, the user can only choose the states from the list.\n\nExperimental API will deny all request by default.\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe previous default setting was to allow all API requests without authentication, but this poses security\nrisks to users who miss this fact. This changes the default for new installs to deny all requests by default.\n\n**Note**\\ : This will not change the behavior for existing installs, please update check your airflow.cfg\n\nIf you wish to have the experimental API work, and aware of the risks of enabling this without authentication\n(or if you have your own authentication layer in front of Airflow) you can get\nthe previous behaviour on a new install by setting this in your airflow.cfg:\n\n.. code-block::\n\n   [api]\n   auth_backend = airflow.api.auth.backend.default\n\nXCom Values can no longer be added or changed from the Webserver\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nSince XCom values can contain pickled data, we would no longer allow adding or\nchanging XCom values from the UI.\n\nDefault for ``run_as_user`` configured has been changed to 50000 from 0\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe UID to run the first process of the Worker PODs when using has been changed to ``50000``\nfrom the previous default of ``0``. The previous default was an empty string but the code used ``0`` if it was\nempty string.\n\n**Before**\\ :\n\n.. code-block:: ini\n\n   [kubernetes]\n   run_as_user =\n\n**After**\\ :\n\n.. code-block:: ini\n\n   [kubernetes]\n   run_as_user = 50000\n\nThis is done to avoid running the container as ``root`` user.\n\nNew Features\n^^^^^^^^^^^^\n\n- Add task instance mutation hook (#8852)\n- Allow changing Task States Colors (#9520)\n- Add support for AWS Secrets Manager as Secrets Backend (#8186)\n- Add Airflow info command to the CLI (#8704)\n- Add Local Filesystem Secret Backend (#8596)\n- Add Airflow config CLI command (#8694)\n- Add Support for Python 3.8 (#8836)(#8823)\n- Allow K8S worker pod to be configured from JSON/YAML file (#6230)\n- Add quarterly to crontab presets (#6873)\n- Add support for ephemeral storage on KubernetesPodOperator (#6337)\n- Add AirflowFailException to fail without any retry (#7133)\n- Add SQL Branch Operator (#8942)\n\nBug Fixes\n^^^^^^^^^\n\n- Use NULL as dag.description default value (#7593)\n- BugFix: DAG trigger via UI error in RBAC UI (#8411)\n- Fix logging issue when running tasks (#9363)\n- Fix JSON encoding error in DockerOperator (#8287)\n- Fix alembic crash due to typing import (#6547)\n- Correctly restore upstream_task_ids when deserializing Operators (#8775)\n- Correctly store non-default Nones in serialized tasks/dags (#8772)\n- Correctly deserialize dagrun_timeout field on DAGs (#8735)\n- Fix tree view if config contains \" (#9250)\n- Fix Dag Run UI execution date with timezone cannot be saved issue (#8902)\n- Fix Migration for MSSQL (#8385)\n- RBAC ui: Fix missing Y-axis labels with units in plots (#8252)\n- RBAC ui: Fix missing task runs being rendered as circles instead (#8253)\n- Fix: DagRuns page renders the state column with artifacts in old UI (#9612)\n- Fix task and dag stats on home page (#8865)\n- Fix the trigger_dag api in the case of nested subdags (#8081)\n- UX Fix: Prevent undesired text selection with DAG title selection in Chrome (#8912)\n- Fix connection add/edit for spark (#8685)\n- Fix retries causing constraint violation on MySQL with DAG Serialization (#9336)\n- [AIRFLOW-4472] Use json.dumps/loads for templating lineage data (#5253)\n- Restrict google-cloud-texttospeech to <v2 (#9137)\n- Fix pickling failure when spawning processes (#8671)\n- Pin Version of azure-cosmos to <4 (#8956)\n- Azure storage 0.37.0 is not installable any more (#8833)\n- Fix modal_backdrop z-index in the UI (#7313)\n- Fix Extra Links in Gantt View (#8308)\n- Bug fix for EmrAddStepOperator init with cluster_name error (#9235)\n- Fix KubernetesPodOperator pod name length validation (#8829)\n- Fix non updating DAG code by checking against last modification time (#8266)\n- BugFix: Unpausing a DAG with catchup=False creates an extra DAG run (#8776)\n\n\nImprovements\n^^^^^^^^^^^^\n\n- Improve add_dag_code_table migration (#8176)\n- Persistent display/filtering of DAG status (#8106)\n- Set unique logger names (#7330)\n- Update the version of cattrs from 0.9 to 1.0 to support Python 3.8 (#7100)\n- Reduce response payload size of /dag_stats and /task_stats (#8655)\n- Add authenticator parameter to snowflake_hook (#8642)\n- Show \"Task Reschedule\" table in Airflow Webserver (#9521)\n- Change worker_refresh_interval fallback to default of 30 (#9588)\n- Use pformat instead of str to render arguments in WebUI (#9587)\n- Simplify DagFileProcessorManager (#7521)\n- Reload gunicorn when plugins has been changed (#8997)\n- Fix the default value for store_dag_code (#9554)\n- Add support for fetching logs from running pods (#8626)\n- Persist start/end date and duration for DummyOperator Task Instance (#8663)\n- Ensure \"started\"/\"ended\" in tooltips are not shown if job not started (#8667)\n- Add context to execution_date_fn in ExternalTaskSensor (#8702)\n- Avoid color info in response of ``/dag_stats`` & ``/task_stats`` (#8742)\n- Make loading plugins from entrypoint fault-tolerant (#8732)\n- Refactor Kubernetes worker config (#7114)\n- Add default ``conf`` parameter to Spark JDBC Hook (#8787)\n- Allow passing backend_kwargs to AWS SSM client (#8802)\n- Filter dags by clicking on tag (#8897)\n- Support k8s auth method in Vault Secrets provider (#8640)\n- Monitor pods by labels instead of names (#6377)\n- Optimize count query on /home (#8729)\n- Fix JSON string escape in tree view (#8551)\n- Add TaskInstance state to TI Tooltip to be colour-blind friendlier (#8910)\n- Add a tip to trigger DAG screen (#9049)\n- Use Markup for htmlcontent for landing_times (#9242)\n- Pinning max pandas version to 2.0 (lesser than) to allow pandas 1.0 (#7954)\n- Update example webserver_config.py to show correct CSRF config (#8944)\n- Fix displaying Executor Class Name in \"Base Job\" table (#8679)\n- Use existing DagBag for 'dag_details' & 'trigger' Endpoints (#8501)\n- Flush pending Sentry exceptions before exiting (#7232)\n- Display DAG run conf in the list view (#6794)\n- Fix performance degradation when updating dagrun state (#8435)\n- Don't use the ``|safe`` filter in code, it's risky (#9180)\n- Validate only task commands are run by executors (#9178)\n- Show Deprecation warning on duplicate Task ids (#8728)\n- Move DAG._schedule_interval logic out of ``DAG.__init__`` (#8225)\n- Make retrieving Paused Dag ids a separate method (#7587)\n- Bulk fetch paused_dag_ids (#7476)\n- Add a configurable DAGs volume mount path for Kubernetes (#8147)\n- Add schedulername option for KubernetesPodOperator (#6088)\n- Support running git sync container as root (#6312)\n- Add extra options for Slack Webhook operator and Slack hook (#9409)\n- Monkey patch greenlet Celery pools (#8559)\n- Decrypt secrets from SystemsManagerParameterStoreBackend (#9214)\n- Prevent clickable sorting on non sortable columns in TI view (#8681)\n- Make hive macros py3 compatible (#8598)\n- Fix SVG tooltip positioning with custom scripting (#8269)\n- Avoid unnecessary sleep to maintain local task job heart rate (#6553)\n- Include some missing RBAC roles on User and Viewer roles (#9133)\n- Show Dag's Markdown docs on Tree View (#9448)\n- Improved compatibility with Python 3.5+ - Convert signal.SIGTERM to int (#9207)\n- Add 'main' param to template_fields in DataprocSubmitPySparkJobOperator (#9154)\n- Make it possible to silence warnings from Airflow (#9208)\n- Remove redundant count query in BaseOperator.clear() (#9362)\n- Fix DB migration message (#8988)\n- Fix awkward log info in dbapi_hook (#8482)\n- Fix Celery default to no longer allow pickle (#7205)\n- Further validation that only task commands are run by executors (#9240)\n- Remove vendored nvd3 and slugify libraries (#9136)\n- Enable configurable git sync depth  (#9094)\n- Reduce the required resources for the Kubernetes's sidecar (#6062)\n- Refactor K8S codebase with k8s API models (#5481)\n- Move k8s executor out of contrib to closer match master (#8904)\n- Allow filtering using \"event\" and \"owner\" in \"Log\" view (#4881)\n- Add Yandex.Cloud custom connection to 1.10 (#8791)\n- Add table-hover css class to DAGs table (#5033)\n- Show un/pause errors in dags view. (#7669)\n- Restructure database queries on /home (#4872)\n- Add Cross Site Scripting defense (#6913)\n- Make Gantt tooltip the same as Tree and Graph view (#8220)\n- Add config to only delete worker pod on task failure (#7507)(#8312)\n- Remove duplicate error message on chart connection failure (#8476)\n- Remove default value spark_binary (#8508)\n- Expose Airflow Webserver Port in Production Docker Image (#8228)\n- Commit after each alembic migration (#4797)\n- KubernetesPodOperator fixes and test (#6524)\n- Docker Image: Add ADDITIONAL_AIRFLOW_EXTRAS (#9032)\n- Docker Image: Add ADDITIONAL_PYTHON_DEPS (#9031)\n- Remove httplib2 from Google requirements (#9194)\n- Merging multiple SQL operators (#9124)\n- Adds hive as extra in pyhive dependency (#9075)\n- Change default auth for experimental backend to deny_all (#9611)\n- Restrict changing XCom values from the Webserver (#9614)\n- Add __repr__ for DagTag so tags display properly in /dagmodel/show (#8719)\n- Functionality to shuffle HMS connections used by HiveMetastoreHook facilitating load balancing (#9280)\n- Expose SQLAlchemy's connect_args and make it configurable (#6478)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Add docs on using DAGRun.conf (#9578)\n- Enforce code-block directives in doc (#9443)\n- Carefully parse warning messages when building documentation (#8693)\n- Make KubernetesPodOperator clear in docs (#8444)\n- Improve language in Pod Mutation Hook docs (#8445)\n- Fix formatting of Pool docs in concepts.rst (#8443)\n- Make doc clearer about Airflow Variables using Environment Variables (#8427)\n- Fix pools doc for LocalExecutor (#7643)\n- Add Local and Sequential Executors to Doc (#8084)\n- Add documentation for CLI command Airflow dags test (#8251)\n- Fix typo in DAG Serialization documentation (#8317)\n- Add scheduler in production section (#7351)\n- Add a structural dag validation example (#6727)\n- Adding Task re-run documentation (#6295)\n- Fix outdated doc on settings.policy (#7532)\n- Add docs about reload_on_plugin_change option (#9575)\n- Add copy button to Code Blocks in Airflow Docs (#9450)\n- Update commands in docs for v1.10+ (#9585)\n- Add more info on dry-run CLI option (#9582)\n- Document default timeout value for SSHOperator (#8744)\n- Fix docs on creating CustomOperator (#8678)\n- Enhanced documentation around Cluster Policy (#8661)\n- Use sphinx syntax in concepts.rst (#7729)\n- Update README to remove Python 3.8 limitation for Master (#9451)\n- Add note about using dag_run.conf in BashOperator (#9143)\n- Improve tutorial - Include all imports statements (#8670)\n- Added more precise Python requirements to README.md (#8455)\n- Fix Airflow Stable version in README.md (#9360)\n- Update AWS connection example to show how to set from env var (#9191)\n- Fix list formatting of plugins doc. (#8873)\n- Add 'Version Added' on Secrets Backend docs (#8264)\n- Simplify language re roll-your-own secrets backend (#8257)\n- Add installation description for repeatable PyPi installation (#8513)\n- Add note extra links only render on when using RBAC webserver  (#8788)\n- Remove unused Airflow import from docs (#9274)\n- Add PR/issue note in Contribution Workflow Example (#9177)\n- Use inclusive language - language matters (#9174)\n- Add docs to change Colors on the Webserver (#9607)\n- Change 'initiate' to 'initialize' in installation.rst (#9619)\n- Replace old Variables View Screenshot with new (#9620)\n- Replace old SubDag zoom screenshot with new (#9621)\n- Update docs about the change to default auth for experimental API (#9617)\n\n\nAirflow 1.10.10 (2020-04-09)\n----------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nSetting Empty string to a Airflow Variable will return an empty string\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPreviously when you set an Airflow Variable with an empty string (\\ ``''``\\ ), the value you used to get\nback was ``None``. This will now return an empty string (\\ ``'''``\\ )\n\nExample:\n\n.. code-block:: python\n\n   Variable.set(\"test_key\", \"\")\n   Variable.get(\"test_key\")\n\nThe above code returned ``None`` previously, now it will return ``''``.\n\nMake behavior of ``none_failed`` trigger rule consistent with documentation\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe behavior of the ``none_failed`` trigger rule is documented as \"all parents have not failed (\\ ``failed`` or\n    ``upstream_failed``\\ ) i.e. all parents have succeeded or been skipped.\" As previously implemented, the actual behavior\n    would skip if all parents of a task had also skipped.\n\nAdd new trigger rule ``none_failed_or_skipped``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe fix to ``none_failed`` trigger rule breaks workflows that depend on the previous behavior.\n    If you need the old behavior, you should change the tasks with ``none_failed`` trigger rule to ``none_failed_or_skipped``.\n\nSuccess Callback will be called when a task in marked as success from UI\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWhen a task is marked as success by a user from Airflow UI - ``on_success_callback`` will be called\n\nNew Features\n^^^^^^^^^^^^\n\n- [AIRFLOW-7048] Allow user to chose timezone to use in UI (#8046)\n- Add Production Docker image support (#7832)\n- Get Airflow Variables from Environment Variables (#7923)\n- Get Airflow Variables from Hashicorp Vault (#7944)\n- Get Airflow Variables from AWS Systems Manager Parameter Store (#7945)\n- Get Airflow Variables from GCP Secrets Manager (#7946)\n- [AIRFLOW-5705] Add secrets backend and support for AWS SSM / Get Airflow Connections from AWS Parameter Store(#6376)\n- [AIRFLOW-7104] Add Secret backend for GCP Secrets Manager / Get Airflow Connections from GCP Secrets Manager (#7795)\n- [AIRFLOW-7076] Add support for HashiCorp Vault as Secrets Backend / Get Airflow Connections from Hashicorp Vault (#7741)\n- [AIRFLOW-6685] Add ThresholdCheckOperator (#7353)\n- [AIRFLOW-7080] Add API endpoint to return a DAG's paused state (#7737)\n\nBug Fixes\n^^^^^^^^^\n\n- BugFix: Show task_id in the Graph View tooltip (#7859)\n- [AIRFLOW-6730] Use total_seconds instead of seconds (#7363)\n- [AIRFLOW-6167] Escape column name in create table in hive (#6741)\n- [AIRFLOW-6628] DAG auto-complete now suggests from all accessible DAGs (#7251)\n- [AIRFLOW-7113] Fix gantt render error (#7913)\n- [AIRFLOW-6399] Add _access control to validate deserialized DAGs (#7896)\n- [AIRFLOW-6399] Serialization: DAG access_control field should be decorated field in DAG serialization (#7879)\n- [AIRFLOW-4453] Make behavior of ``none_failed`` consistent with documentation (#7464)\n- [AIRFLOW-4363] Fix JSON encoding error (#7628)\n- [AIRFLOW-6683] Run REST API tests when DAGs are serialized (#7352)\n- [AIRFLOW-6704] Copy common TaskInstance attributes from Task (#7324)\n- [AIRFLOW-6734] Use configured base_template instead of hard-coding (#7367)\n- [AIRFLOW-7098] Simple salesforce release 1.0.0 breaks the build (#7775)\n- [AIRFLOW-6062] Executor would only delete workers in its own namespace (#7123)\n- [AIRFLOW-7074] Add Permissions to view SubDAGs (#7752)\n- [AIRFLOW-7025] Fix SparkSqlHook.run_query to handle its parameter properly (#7677)\n- [AIRFLOW-6855] Escape project_dataset_table in SQL query in gcs to bq operator (#7475)\n- [AIRFLOW-6949] Respect explicit conf to SparkSubmitOperator (#7575)\n- [AIRFLOW-6588] write_stdout and json_format are boolean (#7199)\n- [AIRFLOW-3439] Decode logs with  'utf-8' (#4474)\n- [AIRFLOW-6878] Fix misconfigured default value for kube_client_request_args\n- [AIRFLOW-5167] Update dependencies for GCP packages (#7116)\n- [AIRFLOW-6821] Success callback not called when task marked as success from UI (#7447)\n- [AIRFLOW-6740] Remove Undocumented, deprecated, dysfunctional PROXY_FIX_NUM_PROXIES (#7359)\n- [AIRFLOW-6728] Change various DAG info methods to POST (#7364)\n- [AIRFLOW-6997] Make sure worker pods initcontainers obtain env vars from config (#7663)\n- [AIRFLOW-7062] Fix pydruid release breaking the build (#7720)\n- [AIRFLOW-6040] ReadTimoutError in KubernetesExecutor should not raise exception (#7616)\n- [AIRFLOW-6943] Fix utf-8 encoded description in DAG in Python 2 (#7567)\n- [AIRFLOW-6892] Fix broken non-wheel releases (#7514)\n- [AIRFLOW-6789] BugFix: Fix Default Worker concurrency (#7494)\n- [AIRFLOW-6840] Bump up version of future (#7471)\n- [AIRFLOW-5705] Fix bugs in AWS SSM Secrets Backend (#7745)\n- [AIRFLOW-5705] Fix bug in Secrets Backend (#7742)\n- Fix CloudSecretsManagerBackend invalid connections_prefix (#7861)\n- [AIRFLOW-7045] BugFix: DebugExecutor fails to change task state. (#8073)\n- BugFix: Datetimepicker is stuck on the UI (#8092)\n- [AIRFLOW-5277] Gantt chart respects per-user the Timezone UI setting (#8096)\n- Fix timezones displayed in Task Instance tooltip (#8103)\n- BugFix: Fix writing & deleting Dag Code for Serialized DAGs (#8151)\n- Make the default TI pool slots '1' (#8153)\n- Fix 500 error in Security screens (#8165)\n- Fix Viewing Dag Code for Stateless Webserver (#8159)\n- Fix issue with sqlalchemy 1.3.16 (#8230)\n\n\nImprovements\n^^^^^^^^^^^^\n\n- Use same tooltip for Graph and Tree views for TaskInstances (#8043)\n- Allow DateTimePicker in Webserver to actually pick times too (#8034)\n- [AIRFLOW-5590] Add run_id to trigger DAG run API response (#6256)\n- [AIRFLOW-6695] Can now pass dagrun conf when triggering dags via UI (#7312)\n- [AIRFLOW-5336] Add ability to make updating FAB perms on webserver in it optional (#5940)\n- [AIRFLOW-1467] Allow tasks to use more than one pool slot (#7160)\n- [AIRFLOW-6987] Avoid creating default connections (#7629)\n- [AIRFLOW-4175] S3Hook load_file should support ACL policy parameter (#7733)\n- [AIRFLOW-4438] Add Gzip compression to S3_hook (#7680)\n- Allow setting Airflow Variable values to empty string (#8021)\n- Dont schedule dummy tasks (#7880)\n- Prevent sequential scan of task instance table (#8014)\n- [AIRFLOW-7017] Respect default dag view in trigger dag origin (#7667)\n- [AIRFLOW-6837] Limit description length of a Dag on HomePage (#7457)\n- [AIRFLOW-6989] Display Rendered template_fields without accessing Dag files (#7633)\n- [AIRFLOW-5944] Rendering templated_fields without accessing DAG files (#6788)\n- [AIRFLOW-5946] DAG Serialization: Store source code in db (#7217)\n- [AIRFLOW-7079] Remove redundant code for storing template_fields (#7750)\n- [AIRFLOW-7024] Add the verbose parameter support to SparkSqlOperator (#7676)\n- [AIRFLOW-6733] Extend not replace template (#7366)\n- [AIRFLOW-7001] Further fix for the MySQL 5.7 UtcDateTime (#7655)\n- [AIRFLOW-6014] Handle pods which are preempted & deleted by kubernetes but not restarted (#6606)\n- [AIRFLOW-6950] Remove refresh_executor_config from ti.refresh_from_db (#7577)\n- [AIRFLOW-7016] Sort dag tags in the UI (#7661)\n- [AIRFLOW-6762] Fix link to \"Suggest changes on this page\" (#7387)\n- [AIRFLOW-6948] Remove ASCII Airflow from version command (#7572)\n- [AIRFLOW-6767] Correct name for default Athena workgroup (#7394)\n- [AIRFLOW-6905] Update pin.svg with new pinwheel (#7524)\n- [AIRFLOW-6801] Make use of ImportError.timestamp (#7425)\n- [AIRFLOW-6830] Add Subject/MessageAttributes to SNS hook and operator (#7451)\n- [AIRFLOW-6630] Resolve handlebars advisory (#7284)\n- [AIRFLOW-6945] MySQL 5.7 is used in v1-10-test as an option\n- [AIRFLOW-6871] Optimize tree view for large DAGs (#7492)\n- [AIRFLOW-7063] Fix dag.clear() slowness caused by count (#7723)\n- [AIRFLOW-7023] Remove duplicated package definitions in setup.py (#7675)\n- [AIRFLOW-7001] Time zone removed from MySQL TIMESTAMP field inserts\n- [AIRFLOW-7105] Unify Secrets Backend method interfaces (#7830)\n- Make BaseSecretsBackend.build_path generic (#7948)\n- Allow hvac package installation using 'hashicorp' extra (#7915)\n- Standardize SecretBackend class names (#7846)\n- [AIRFLOW-5705] Make AwsSsmSecretsBackend consistent with VaultBackend (#7753)\n- [AIRFLOW-7045] Update SQL query to delete RenderedTaskInstanceFields (#8051)\n- Handle DST better in Task Instance tool tips (#8104)\n\nMisc/Internal\n^^^^^^^^^^^^^\n\n- Fix Flaky TriggerDAG UI test (#8022)\n- Remove unnecessary messages in CI (#7951)\n- Fixes too high parallelism in CI (#7947)\n- Install version is not persistent in breeze (#7914)\n- Fixed automated check for image rebuild (#7912)\n- Move Dockerfile to Dockerfile.ci (#7829)\n- Generate requirements are now sorted (#8040)\n- Change name of the common environment initialization function (#7805)\n- Requirements now depend on Python version (#7841)\n- Bring back reset db explicitly called at CI entry (#7798)\n- Fixes unclean installation of Airflow 1.10 (#7796)\n- [AIRFLOW-7029] Use separate docker image for running license check (#7678)\n- [AIRFLOW-5842] Switch to Debian buster image as a base (#7647)\n- [AIRFLOW-5828] Move build logic out from hooks/build (#7618)\n- [AIRFLOW-6839] Even more mypy speed improvements (#7460)\n- [AIRFLOW-6820] split breeze into functions (#7433)\n- [AIRFLOW-7097] Install gcloud beta components in CI image (#7772)\n- [AIRFLOW-7018] fixing travis's job name escaping problem (#7668)\n- [AIRFLOW-7054] Breeze has an option now to reset db at entry (#7710)\n- [AIRFLOW-7005] Added exec command to Breeze (#7649)\n- [AIRFLOW-7015] Detect Dockerhub repo/user when building on Dockerhub (#7673)\n- [AIRFLOW-6727] Fix minor bugs in Release Management scripts (#7355)\n- [AIRFLOW-7013] Automated check if Breeze image needs to be pulled (#7656)\n- [AIRFLOW-7010] Skip in-container checks for Dockerhub builds (#7652)\n- [AIRFLOW-7011] Pin JPype release to allow to build 1.10 images\n- [AIRFLOW-7006] Fix missing +e in Breeze script (#7648)\n- [AIRFLOW-6979] Fix breeze test-target specific test param issue (#7614)\n- [AIRFLOW-6932] Add restart-environment command to Breeze\n- [AIRFLOW-6919] Make Breeze DAG-test friendly (#7539)\n- [AIRFLOW-6838] Introduce real subcommands for Breeze (#7515)\n- [AIRFLOW-6763] Make systems tests ready for backport tests (#7389)\n- [AIRFLOW-6866] Fix wrong export for Mac on Breeze (#7485)\n- [AIRFLOW-6842] Skip fixing ownership on Mac (#7469)\n- [AIRFLOW-6841] Fixed unbounded variable on Mac (#7465)\n- [AIRFLOW-7067] Pinned version of Apache Airflow (#7730)\n- [AIRFLOW-7058] Add support for different DB versions (#7717)\n- [AIRFLOW-7002] Get rid of yaml \"parser\" in bash (#7646)\n- [AIRFLOW-6972] Shorter frequently used commands in Breeze (#7608)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- Fix typo for store_serialized_dags config (#7952)\n- Fix broken link in README.md (#7893)\n- Separate supported Postgres versions with comma (#7892)\n- Fix grammar in setup.py (#7877)\n- Add Jiajie Zhong to committers list (#8047)\n- Update Security doc for 1.10.* for auth backends (#8072)\n- Fix Example in config_templates for Secrets Backend (#8074)\n- Add backticks in IMAGES.rst command description (#8075)\n- Change version_added for store_dag_code config (#8076)\n- [AIRFLOW-XXXX] Remove duplicate docs\n- [AIRFLOW-XXXX] Remove the defunct limitation of Dag Serialization (#7716)\n- [AIRFLOW-XXXX] Add prerequisite tasks for all GCP operators guide (#6049)\n- [AIRFLOW-XXXX] Simplify AWS/Azure/Databricks operators listing (#6047)\n- [AIRFLOW-XXXX] Add external reference to all GCP operator guide (#6048)\n- [AIRFLOW-XXXX] Simplify GCP operators listing\n- [AIRFLOW-XXXX] Simplify Qubole operators listing\n- [AIRFLOW-XXXX] Add autogenerated TOC (#6038)\n- [AIRFLOW-XXXX] Create \"Using the CLI\" page (#5823)\n- [AIRFLOW-XXXX] Group references in one section (#5776)\n- [AIRFLOW-XXXX] Fix analytics doc (#5885)\n- [AIRFLOW-XXXX] Add S3 Logging section (#6039)\n- [AIRFLOW-XXXX] Move Azure Logging section above operators (#6040)\n- [AIRFLOW-XXXX] Update temp link to a fixed link (#7715)\n- [AIRFLOW-XXXX] Add Updating.md section for 1.10.9 (#7385)\n- [AIRFLOW-XXXX] Remove duplication in BaseOperator docstring (#7321)\n- [AIRFLOW-XXXX] Update tests info in CONTRIBUTING.rst (#7466)\n- [AIRFLOW-XXXX] Small BREEZE.rst update (#7487)\n- [AIRFLOW-XXXX] Add instructions for logging to localstack S3 (#7461)\n- [AIRFLOW-XXXX] Remove travis config warnings (#7467)\n- [AIRFLOW-XXXX] Add communication chapter to contributing (#7204)\n- [AIRFLOW-XXXX] Add known issue - example_dags/__init__.py (#7444)\n- [AIRFLOW-XXXX] Fix breeze build-docs (#7445)\n- [AIRFLOW-XXXX] Less verbose docker builds\n- [AIRFLOW-XXXX] Speed up mypy runs (#7421)\n- [AIRFLOW-XXXX] Fix location of kubernetes tests (#7373)\n- [AIRFLOW-XXXX] Remove quotes from domains in Google Oauth (#4226)\n- [AIRFLOW-XXXX] Add explicit info about JIRAs for code-related PRs (#7318)\n- [AIRFLOW-XXXX] Fix typo in the word committer (#7392)\n- [AIRFLOW-XXXX] Remove duplicated paragraph in docs (#7662)\n- Fix reference to KubernetesPodOperator (#8100)\n\n\nAirflow 1.10.9 (2020-02-07)\n---------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nNo significant changes.\n\nBug Fixes\n^^^^^^^^^\n\n- [AIRFLOW-6751] Pin Werkzeug (dependency of a number of our dependencies) to < 1.0.0 (#7377)\n\nAirflow 1.10.8 (2020-02-07)\n---------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nFailure callback will be called when task is marked failed\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWhen task is marked failed by user or task fails due to system failures - on failure call back will be called as part of clean up\n\nSee `AIRFLOW-5621 <https://jira.apache.org/jira/browse/AIRFLOW-5621>`_ for details\n\nNew Features\n^^^^^^^^^^^^\n\n- [AIRFLOW-4026] Add filter by DAG tags (#6489)\n- [AIRFLOW-6613] Center dag on graph view load (#7238)\n- [AIRFLOW-5843] Add conf option to Add DAG Run view (#7281)\n- [AIRFLOW-4495] Allow externally triggered dags to run for future exec dates (#7038)\n\nImprovements\n^^^^^^^^^^^^\n\n- [AIRFLOW-6438] Filter DAGs returned by blocked (#7019)\n- [AIRFLOW-6666] Resolve js-yaml advisory (#7283)\n- [AIRFLOW-6632] Bump dagre-d3 to resolve lodash CVE advisory (#7280)\n- [AIRFLOW-6667] Resolve serialize-javascript advisory (#7282)\n- [AIRFLOW-6451] self._print_stat() in dag_processing.py should be skipable (#7134)\n- [AIRFLOW-6495] Load DAG only once when running a task using StandardTaskRunner (#7090)\n- [AIRFLOW-6319] Add support for AWS Athena workgroups (#6871)\n- [AIRFLOW-6677] Remove deprecation warning from SQLAlchmey (#7289)\n- [AIRFLOW-6428] Fix import path for airflow.utils.dates.days_ago in Example DAGs (#7007)\n- [AIRFLOW-6595] Use TaskNotFound exception instead of AirflowException (#7210)\n- [AIRFLOW-6620] Mock celery in worker cli test (#7243)\n- [AIRFLOW-6608] Change logging level for Bash & PyOperator Env exports\n- [AIRFLOW-2279] Clear tasks across DAGs if marked by ExternalTaskMarker (#6633)\n- [AIRFLOW-6359] Make Spark status_poll_interval explicit (#6978)\n- [AIRFLOW-6359] spark_submit_hook.py status polling interval config (#6909)\n- [AIRFLOW-6316] Use exampleinclude directives in tutorial.rst (#6868)\n- [AIRFLOW-6519] Make TI logs constants in Webserver configurable (#7113)\n- [AIRFLOW-6327] http_hook: Accept json= parameter for payload (#6886)\n- [AIRFLOW-6261] flower_basic_auth eligible to _cmd (#6825)\n- [AIRFLOW-6238] Filter dags returned by dag_stats\n- [AIRFLOW-5616] Switch PrestoHook from pyhive to presto-python-client\n- [AIRFLOW-6611] Add proxy_fix configs to default_airflow.cfg (#7236)\n- [AIRFLOW-6557] Add test for newly added fields in BaseOperator (#7162)\n- [AIRFLOW-6584] Pin cassandra driver (#7194)\n- [AIRFLOW-6537] Fix backticks in RST files (#7140)\n- [AIRFLOW-4428] Error if exec_date before default_args.start_date in trigger_dag (#6948)\n- [AIRFLOW-6330] Show cli help when param blank or typo (#6883)\n- [AIRFLOW-4113] Unpin boto3 (#6884)\n- [AIRFLOW-6181] Add DebugExecutor (#6740)\n- [AIRFLOW-6504] Allow specifying configmap for Airflow Local Setting (#7097)\n- [AIRFLOW-6436] Cleanup for Airflow configs doc generator code (#7036)\n- [AIRFLOW-6436] Add x_frame_enabled config in config.yml (#7024)\n- [AIRFLOW-6436] Create & Automate docs on Airflow Configs (#7015)\n- [AIRFLOW-6527] Make send_task_to_executor timeout configurable (#7143)\n- [AIRFLOW-6272] Switch from npm to yarnpkg for managing front-end dependencies (#6844)\n- [AIRFLOW-6350] Security - spark submit operator logging+exceptions should mask passwords\n- [AIRFLOW-6358] Log details of failed task (#6908)\n- [AIRFLOW-5149] Skip SLA checks config (#6923)\n- [AIRFLOW-6057] Update template_fields of the PythonSensor (#6656)\n- [AIRFLOW-4445] Mushroom cloud errors too verbose (#6952)\n- [AIRFLOW-6394] Simplify github PR template (#6955)\n- [AIRFLOW-5385] spark hook does not work on spark 2.3/2.4 (#6976)\n\nBug Fixes\n^^^^^^^^^\n\n- [AIRFLOW-6345] Ensure arguments to ProxyFix are integers (#6901)\n- [AIRFLOW-6576] Fix scheduler crash caused by deleted task with sla misses (#7187)\n- [AIRFLOW-6686] Fix syntax error constructing list of process ids (#7298)\n- [AIRFLOW-6683] REST API respects store_serialized_dag setting (#7296)\n- [AIRFLOW-6553] Add upstream_failed in instance state filter to WebUI (#7159)\n- [AIRFLOW-6357] Highlight nodes in Graph UI if task id contains dots (#6904)\n- [AIRFLOW-3349] Use None instead of False as value for encoding in StreamLogWriter (#7329)\n- [AIRFLOW-6627] Email with incorrect DAG not delivered (#7250)\n- [AIRFLOW-6637] Fix Airflow test command in 1.10.x\n- [AIRFLOW-6636] Avoid exceptions when printing task instance\n- [AIRFLOW-6522] Clear task log file before starting to fix duplication in S3TaskHandler (#7120)\n- [AIRFLOW-5501] Make default ``in_cluster`` value in KubernetesPodOperator respect config (#6124)\n- [AIRFLOW-6514] Use RUNNING_DEPS to check run from UI (#6367)\n- [AIRFLOW-6381] Remove styling based on DAG id from DAGs page (#6985)\n- [AIRFLOW-6434] Add return statement back to DockerOperator.execute (#7013)\n- [AIRFLOW-2516] Fix mysql deadlocks (#6988)\n- [AIRFLOW-6528] Disable flake8 W503 line break before binary operator (#7124)\n- [AIRFLOW-6517] Make merge_dicts function recursive (#7111)\n- [AIRFLOW-5621] Failure callback is not triggered when marked Failed on UI (#7025)\n- [AIRFLOW-6353] Security - ui - add click jacking defense (#6995)\n- [AIRFLOW-6348] Security - cli.py is currently printing logs with password (#6915)\n- [AIRFLOW-6323] Remove non-ascii letters from default config (#6878)\n- [AIRFLOW-6506] Fix do_xcom_push defaulting to True in KubernetesPodOperator (#7122)\n- [AIRFLOW-6516] BugFix: airflow.cfg does not exist in Volume Mounts (#7109)\n- [AIRFLOW-6427] Fix broken example_qubole_operator dag (#7005)\n- [AIRFLOW-6385] BugFix: SlackAPIPostOperator fails when blocks not set (#7022)\n- [AIRFLOW-6347] BugFix: Can't get task logs when serialization is enabled (#7092)\n- [AIRFLOW-XXXX] Fix downgrade of db migration 0e2a74e0fc9f (#6859)\n- [AIRFLOW-6366] Fix migrations for MS SQL Server (#6920)\n- [AIRFLOW-5406] Allow spark without kubernetes (#6921)\n- [AIRFLOW-6229] SparkSubmitOperator polls forever if status JSON can't… (#6918)\n- [AIRFLOW-6352] Security - ui - add login timeout (#6912)\n- [AIRFLOW-6397] Ensure sub_process attribute exists before trying to kill it (#6958)\n- [AIRFLOW-6400] Fix pytest not working on Windows (#6964)\n- [AIRFLOW-6418] Remove SystemTest.skip decorator (#6991)\n- [AIRFLOW-6425] Serialization: Add missing DAG parameters to JSON Schema (#7002)\n\nMisc/Internal\n^^^^^^^^^^^^^\n\n- [AIRFLOW-6467] Use self.dag i/o creating a new one (#7067)\n- [AIRFLOW-6490] Improve time delta comparison in local task job tests (#7083)\n- [AIRFLOW-5814] Implementing Presto hook tests (#6491)\n- [AIRFLOW-5704] Improve Kind Kubernetes scripts for local testing (#6516)\n- [AIRFLOW-XXXX] Move airflow-config-yaml pre-commit before pylint (#7108)\n- [AIRFLOW-XXXX] Improve clarity of confirm message (#7110)\n- [AIRFLOW-6662] install dumb init (#7300)\n- [AIRFLOW-6705] One less chatty message at breeze initialisation (#7326)\n- [AIRFLOW-6705] Less chatty integration/backend checks (#7325)\n- [AIRFLOW-6662] Switch to --init docker flag for signal propagation (#7278)\n- [AIRFLOW-6661] Fail after 50 failing tests (#7277)\n- [AIRFLOW-6607] Get rid of old local scripts for Breeze (#7225)\n- [AIRFLOW-6589] BAT tests run in pre-commit on bash script changes (#7203)\n- [AIRFLOW-6592] Doc build is moved to test phase (#7208)\n- [AIRFLOW-6641] Better diagnostics for kubernetes flaky tests (#7261)\n- [AIRFLOW-6642] Make local task job test less flaky (#7262)\n- [AIRFLOW-6643] Fix flakiness of kerberos tests\n- [AIRFLOW-6638] Remove flakiness test from test_serialized_db remove\n- [AIRFLOW-6701] Rat is downloaded from stable backup/mirrors (#7323)\n- [AIRFLOW-6702] Dumping kind logs to file.io. (#7319)\n- [AIRFLOW-6491] Improve handling of Breeze parameters (#7084)\n- [AIRFLOW-6470] Avoid pipe to file when do curl (#7063)\n- [AIRFLOW-6471] Add pytest-instafail plugin (#7064)\n- [AIRFLOW-6462] Limit exported variables in Dockerfile/Breeze (#7057)\n- [AIRFLOW-6465] Add bash autocomplete for Airflow in Breeze (#7060)\n- [AIRFLOW-6464] Add cloud providers CLI tools in Breeze (#7059)\n- [AIRFLOW-6461] Remove silent flags in Dockerfile (#7052)\n- [AIRFLOW-6459] Increase verbosity of pytest (#7049)\n- [AIRFLOW-6370] Skip Cassandra tests if cluster is not up (#6926)\n- [AIRFLOW-6511] Remove BATS docker containers (#7103)\n- [AIRFLOW-6475] Remove duplication of volume mount specs in Breeze.. (#7065)\n- [AIRFLOW-6489] Add BATS support for Bash unit testing (#7081)\n- [AIRFLOW-6387] print details of success/skipped task (#6956)\n- [AIRFLOW-6568] Add Emacs related files to .gitignore (#7175)\n- [AIRFLOW-6575] Entropy source for CI tests is changed to unblocking (#7185)\n- [AIRFLOW-6496] Separate integrations in tests (#7091)\n- [AIRFLOW-6634] Set PYTHONPATH in interactive Breeze\n- [AIRFLOW-6564] Additional diagnostics information on CI check failure (#7172)\n- [AIRFLOW-6383] Add no trailing-whitespace pre-commit hook (#6941)\n\nDoc only changes\n^^^^^^^^^^^^^^^^\n\n- [AIRFLOW-XXXX] Consistency fixes in new documentation (#7207)\n- [AIRFLOW-XXXX] Improve grammar and structure in FAQ doc (#7291)\n- [AIRFLOW-XXXX] Fix email configuration link in CONTRIBUTING.rst (#7311)\n- [AIRFLOW-XXXX] Update docs with new BranchPythonOperator behaviour (#4682)\n- [AIRFLOW-XXXX] Fix Typo in scripts/ci/ci_run_airflow_testing.sh (#7235)\n- [AIRFLOW-XXXX] Screenshot showing disk space configuration for OSX (#7226)\n- [AIRFLOW-XXXX] Add mentoring information to contributing docs (#7202)\n- [AIRFLOW-XXXX] Add rebase info to contributing (#7201)\n- [AIRFLOW-XXXX] Increase verbosity of static checks in CI (#7200)\n- [AIRFLOW-XXXX] Adds branching strategy to documentation (#7193)\n- [AIRFLOW-XXXX] Move email configuration from the concept page (#7189)\n- [AIRFLOW-XXXX] Update task lifecycle diagram (#7161)\n- [AIRFLOW-XXXX] Fix reference in concepts doc (#7135)\n- [AIRFLOW-XXXX] Clear debug docs (#7104)\n- [AIRFLOW-XXXX] Fix typos and broken links in development docs (#7086)\n- [AIRFLOW-XXXX] Clarify wait_for_downstream and execution_date (#6999)\n- [AIRFLOW-XXXX] Add ``airflow dags show`` command guide (#7014)\n- [AIRFLOW-XXXX] Update operation chaining documentation (#7018)\n- [AIRFLOW-XXXX] Add ``.autoenv_leave.zsh`` to .gitignore (#6986)\n- [AIRFLOW-XXXX] Fix development packages installation instructions (#6942)\n- [AIRFLOW-XXXX] Update committers list (#7212)\n- [AIRFLOW-XXXX] Move UPDATING changes into correct versions (#7166)\n- [AIRFLOW-XXXX] Add Documentation for check_slas flag (#6974)\n- [AIRFLOW-XXXX] Fix gcp keyfile_dict typo (#6962)\n- [AIRFLOW-XXXX] Add tips for writing a note in UPDATIND.md (#6960)\n- [AIRFLOW-XXXX] Add note warning that bash>4.0 is required for docs build script (#6947)\n- [AIRFLOW-XXXX] Add autoenv to gitignore (#6946)\n- [AIRFLOW-XXXX] Fix GCSTaskHandler Comment Typo (#6928)\n- [AIRFLOW-XXXX] Fix broken DAG Serialization Link (#6891)\n- [AIRFLOW-XXXX] Add versions_added field to configs\n\n\nAirflow 1.10.7 (2019-12-24)\n---------------------------\n\nSignificant Changes\n^^^^^^^^^^^^^^^^^^^\n\nChanges in experimental API execution_date microseconds replacement\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe default behavior was to strip the microseconds (and milliseconds, etc) off of all dag runs triggered by\nby the experimental REST API.  The default behavior will change when an explicit execution_date is\npassed in the request body.  It will also now be possible to have the execution_date generated, but\nkeep the microseconds by sending ``replace_microseconds=false`` in the request body.  The default\nbehavior can be overridden by sending ``replace_microseconds=true`` along with an explicit execution_date\n\nInfinite pool size and pool size query optimization\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nPool size can now be set to -1 to indicate infinite size (it also includes\noptimization of pool query which lead to poor task n^2 performance of task\npool queries in MySQL).\n\nViewer won't have edit permissions on DAG view.\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nGoogle Cloud Storage Hook\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe ``GoogleCloudStorageDownloadOperator`` can either write to a supplied ``filename`` or\nreturn the content of a file via xcom through ``store_to_xcom_key`` - both options are mutually exclusive.\n\nNew Features\n^^^^^^^^^^^^\n\n- [AIRFLOW-5088][AIP-24] Persisting serialized DAG in DB for webserver scalability (#5992)\n- [AIRFLOW-6083] Adding ability to pass custom configuration to AWS Lambda client. (#6678)\n- [AIRFLOW-5117] Automatically refresh EKS API tokens when needed (#5731)\n- [AIRFLOW-5118] Add ability to specify optional components in DataprocClusterCreateOperator (#5821)\n- [AIRFLOW-5681] Allow specification of a tag or hash for the git_sync init container (#6350)\n- [AIRFLOW-6025] Add label to uniquely identify creator of Pod (#6621)\n- [AIRFLOW-4843] Allow orchestration via Docker Swarm (SwarmOperator) (#5489)\n- [AIRFLOW-5751] add get_uri method to Connection (#6426)\n- [AIRFLOW-6056] Allow EmrAddStepsOperator to accept job_flow_name as alternative to job_flow_id (#6655)\n- [AIRFLOW-2694] Declare permissions in DAG definition (#4642)\n- [AIRFLOW-4940] Add DynamoDB to S3 operator (#5663)\n- [AIRFLOW-4161] BigQuery to MySQL Operator (#5711)\n- [AIRFLOW-6041] Add user agent to the Discovery API client (#6636)\n- [AIRFLOW-6089] Reorder setup.py dependencies and add ci (#6681)\n- [AIRFLOW-5921] Add bulk_load_custom to MySqlHook (#6575)\n- [AIRFLOW-5854] Add support for ``tty`` parameter in Docker related operators (#6542)\n- [AIRFLOW-4758] Add GcsToGDriveOperator operator (#5822)\n\nImprovements\n^^^^^^^^^^^^\n\n- [AIRFLOW-3656] Show doc link for the current installed version (#6690)\n- [AIRFLOW-5665] Add path_exists method to SFTPHook (#6344)\n- [AIRFLOW-5729] Make InputDataConfig optional in Sagemaker's training config (#6398)\n- [AIRFLOW-5045] Add ability to create Google Dataproc cluster with custom image from a different project (#5752)\n- [AIRFLOW-6132] Allow to pass in tags for the AzureContainerInstancesOperator (#6694)\n- [AIRFLOW-5945] Make inbuilt OperatorLinks work when using Serialization (#6715)\n- [AIRFLOW-5947] Make the JSON backend pluggable for DAG Serialization (#6630)\n- [AIRFLOW-6239] Filter dags return by last_dagruns (to only select visible dags, not all dags) (#6804)\n- [AIRFLOW-6095] Filter dags returned by task_stats (to only select visible dags, not all dags) (#6684)\n- [AIRFLOW-4482] Add execution_date to \"trigger DagRun\" API response (#5260)\n- [AIRFLOW-1076] Add get method for template variable accessor (#6793)\n- [AIRFLOW-5194] Add error handler to action log (#5883)\n- [AIRFLOW-5936] Allow explicit get_pty in"
        },
        {
          "name": "airflow",
          "type": "tree",
          "content": null
        },
        {
          "name": "chart",
          "type": "tree",
          "content": null
        },
        {
          "name": "clients",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 2.8134765625,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n---\ncodecov:\n  branch: main\n  max_report_age: 6h\n  require_ci_to_pass: true\n  notify:\n    wait_for_ci: false\n    after_n_builds: 10\n\ncoverage:\n  precision: 1\n  round: down\n  range: 65..90\n  status:\n    project:\n      default:\n        # basic\n        target: auto\n        threshold: 0%\n        base: auto\n        paths:\n          - \"airflow\"\n        # advanced\n        branches:\n          - main\n          - v2-0-stable\n          - v2-0-test\n          - v2-1-stable\n          - v2-1-test\n          - v2-2-stable\n          - v2-2-test\n          - v2-3-stable\n          - v2-3-test\n          - v2-4-stable\n          - v2-4-test\n          - v2-5-stable\n          - v2-5-test\n          - v2-6-stable\n          - v2-6-test\n          - v2-7-stable\n          - v2-7-test\n          - v2-8-stable\n          - v2-8-test\n          - v2-9-stable\n          - v2-9-test\n          - v2-10-stable\n          - v2-10-test\n        if_not_found: success\n        if_ci_failed: error\n        informational: true\n        only_pulls: false\n    patch:\n      default:\n        # basic\n        target: auto\n        threshold: 0%\n        base: auto\n        # advanced\n        branches:\n          - main\n          - v2-0-stable\n          - v2-0-test\n          - v2-1-stable\n          - v2-1-test\n          - v2-2-stable\n          - v2-2-test\n          - v2-3-stable\n          - v2-3-test\n          - v2-4-stable\n          - v2-4-test\n          - v2-5-stable\n          - v2-5-test\n          - v2-6-stable\n          - v2-6-test\n          - v2-7-stable\n          - v2-7-test\n          - v2-8-stable\n          - v2-8-test\n          - v2-9-stable\n          - v2-9-test\n          - v2-10-stable\n          - v2-10-test\n        if_no_uploads: error\n        if_not_found: success\n        if_ci_failed: error\n        only_pulls: false\n        paths:\n          - \"airflow\"\n\nignore:\n  - \"airflow/_vendor\"\n  - \"airflow/providers/google/ads/_vendor\"\n  - \"airflow/contrib\"\n  - \"airflow/example_dags\"\n  - \"airflow/migrations\"\n  - \"airflow/providers/**/example_dags\"\n  - \"airflow/www/node_modules\"\n  - \"airflow/ui/node_modules\"\n\ncomment: false\n"
        },
        {
          "name": "constraints",
          "type": "tree",
          "content": null
        },
        {
          "name": "contributing-docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "dev",
          "type": "tree",
          "content": null
        },
        {
          "name": "doap_airflow.rdf",
          "type": "blob",
          "size": 2.318359375,
          "content": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\"?>\n<rdf:RDF xml:lang=\"en\"\n         xmlns=\"http://usefulinc.com/ns/doap#\"\n         xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n         xmlns:asfext=\"http://projects.apache.org/ns/asfext#\"\n         xmlns:foaf=\"http://xmlns.com/foaf/0.1/\">\n<!--\n    Licensed to the Apache Software Foundation (ASF) under one or more\n    contributor license agreements.  See the NOTICE file distributed with\n    this work for additional information regarding copyright ownership.\n    The ASF licenses this file to You under the Apache License, Version 2.0\n    (the \"License\"); you may not use this file except in compliance with\n    the License.  You may obtain a copy of the License at\n\n         https://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n-->\n  <Project rdf:about=\"https://airflow.apache.org\">\n    <created>2023-09-04</created>\n    <license rdf:resource=\"https://spdx.org/licenses/Apache-2.0\" />\n    <name>Apache Airflow</name>\n    <homepage rdf:resource=\"https://airflow.apache.org\" />\n    <asfext:pmc rdf:resource=\"https://airflow.apache.org\" />\n    <shortdesc>Workflow automation platform</shortdesc>\n    <description>The mission of Apache Airflow is the creation and maintenance of software\nrelated to workflow automation and scheduling that can be used to author and\nmanage data pipelines.\n</description>\n    <bug-database rdf:resource=\"https://github.com/apache/airflow/issues\" />\n    <mailing-list rdf:resource=\"dev@airflow.apache.org\" />\n    <download-page rdf:resource=\"https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html\" />\n    <programming-language>Python</programming-language>\n    <programming-language>JavaScript</programming-language>\n    <category rdf:resource=\"https://projects.apache.org/category/big-data\" />\n    <repository>\n      <GitRepository>\n        <location rdf:resource=\"https://github.com/apache/airflow.git\"/>\n        <browse rdf:resource=\"https://github.com/apache/airflow\"/>\n      </GitRepository>\n    </repository>\n  </Project>\n</rdf:RDF>\n"
        },
        {
          "name": "docker-context-files",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker_tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "empty",
          "type": "tree",
          "content": null
        },
        {
          "name": "generated",
          "type": "tree",
          "content": null
        },
        {
          "name": "hatch_build.py",
          "type": "blob",
          "size": 37.501953125,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nfrom __future__ import annotations\n\nimport itertools\nimport json\nimport logging\nimport os\nimport re\nimport sys\nfrom collections.abc import Iterable\nfrom pathlib import Path\nfrom subprocess import run\nfrom typing import Any, Callable\n\nfrom hatchling.builders.config import BuilderConfig\nfrom hatchling.builders.hooks.plugin.interface import BuildHookInterface\nfrom hatchling.builders.plugin.interface import BuilderInterface\nfrom hatchling.plugin.manager import PluginManager\n\nlog = logging.getLogger(__name__)\nlog_level = logging.getLevelName(os.getenv(\"CUSTOM_AIRFLOW_BUILD_LOG_LEVEL\", \"INFO\"))\nlog.setLevel(log_level)\n\nAIRFLOW_ROOT_PATH = Path(__file__).parent.resolve()\nGENERATED_PROVIDERS_DEPENDENCIES_FILE = AIRFLOW_ROOT_PATH / \"generated\" / \"provider_dependencies.json\"\nPROVIDER_DEPENDENCIES = json.loads(GENERATED_PROVIDERS_DEPENDENCIES_FILE.read_text())\n\nPRE_INSTALLED_PROVIDERS = [\n    \"common.compat\",\n    \"common.io\",\n    \"common.sql\",\n    \"fab>=1.0.2\",\n    \"ftp\",\n    \"http\",\n    \"imap\",\n    \"smtp\",\n    \"sqlite\",\n    \"standard\",\n]\n\n# Those extras are dynamically added by hatch in the build hook to metadata optional dependencies\n# when project is installed locally (editable build) or when wheel package is built based on it.\nCORE_EXTRAS: dict[str, list[str]] = {\n    # Aiobotocore required for AWS deferrable operators.\n    # There is conflict between boto3 and aiobotocore dependency botocore.\n    # TODO: We can remove it once boto3 and aiobotocore both have compatible botocore version or\n    # boto3 have native aync support and we move away from aio aiobotocore\n    \"aiobotocore\": [\n        \"aiobotocore>=2.9.0\",\n    ],\n    \"async\": [\n        \"eventlet>=0.33.3\",\n        \"gevent>=0.13\",\n        \"greenlet>=0.4.9\",\n    ],\n    \"apache-atlas\": [\n        \"atlasclient>=0.1.2\",\n    ],\n    \"apache-webhdfs\": [\n        \"hdfs[avro,dataframe,kerberos]>=2.0.4\",\n    ],\n    \"cgroups\": [\n        # Cgroupspy 0.2.2 added Python 3.10 compatibility\n        \"cgroupspy>=0.2.2\",\n    ],\n    \"cloudpickle\": [\n        # Latest version of apache-beam requires cloudpickle~=2.2.1\n        \"cloudpickle>=2.2.1\",\n    ],\n    \"github-enterprise\": [\n        \"apache-airflow[fab]\",\n        \"authlib>=1.0.0\",\n    ],\n    \"google-auth\": [\n        \"apache-airflow[fab]\",\n        \"authlib>=1.0.0\",\n    ],\n    \"graphviz\": [\n        # The graphviz package creates friction when installing on MacOS as it needs graphviz system package to\n        # be installed, and it's really only used for very obscure features of Airflow, so we can skip it on MacOS\n        # Instead, if someone attempts to use it on MacOS, they will get explanatory error on how to install it\n        \"graphviz>=0.12; sys_platform != 'darwin'\",\n    ],\n    \"kerberos\": [\n        \"pykerberos>=1.1.13\",\n        \"requests-kerberos>=0.10.0\",\n        \"thrift-sasl>=0.2.0\",\n    ],\n    \"ldap\": [\n        \"python-ldap>=3.4.4\",\n    ],\n    \"leveldb\": [\n        # The plyvel package is a huge pain when installing on MacOS - especially when Apple releases new\n        # OS version. It's usually next to impossible to install it at least for a few months after the new\n        # MacOS version is released. We can skip it on MacOS as this is an optional feature anyway.\n        \"plyvel>=1.5.1; sys_platform != 'darwin'\",\n    ],\n    \"otel\": [\n        \"opentelemetry-exporter-prometheus>=0.47b0\",\n    ],\n    \"pandas\": [\n        # In pandas 2.2 minimal version of the sqlalchemy is 2.0\n        # https://pandas.pydata.org/docs/whatsnew/v2.2.0.html#increased-minimum-versions-for-dependencies\n        # However Airflow not fully supports it yet: https://github.com/apache/airflow/issues/28723\n        # In addition FAB also limit sqlalchemy to < 2.0\n        \"pandas>=1.2.5,<2.2\",\n    ],\n    \"password\": [\n        \"bcrypt>=2.0.0\",\n        \"flask-bcrypt>=0.7.1\",\n    ],\n    \"rabbitmq\": [\n        \"amqp>=5.2.0\",\n    ],\n    \"s3fs\": [\n        # This is required for support of S3 file system which uses aiobotocore\n        # which can have a conflict with boto3 as mentioned in aiobotocore extra\n        \"s3fs>=2023.10.0\",\n    ],\n    \"sentry\": [\n        \"blinker>=1.1\",\n        # Sentry SDK 1.33 is broken when greenlets are installed and fails to import\n        # See https://github.com/getsentry/sentry-python/issues/2473\n        \"sentry-sdk>=1.32.0,!=1.33.0\",\n    ],\n    \"statsd\": [\n        \"statsd>=3.3.0\",\n    ],\n    \"uv\": [\n        \"uv>=0.5.14\",\n    ],\n}\n\nDOC_EXTRAS: dict[str, list[str]] = {\n    \"doc\": [\n        \"astroid>=2.12.3,<3.0\",\n        \"checksumdir>=1.2.0\",\n        \"click>=8.1.8\",\n        # Docutils 0.17.0 converts generated <div class=\"section\"> into <section> and breaks our doc formatting\n        # By adding a lot of whitespace separation. This limit can be lifted when we update our doc to handle\n        # <section> tags for sections\n        \"docutils<0.17,>=0.16\",\n        \"sphinx-airflow-theme>=0.0.12\",\n        \"sphinx-argparse>=0.4.0\",\n        # sphinx-autoapi fails with astroid 3.0, see: https://github.com/readthedocs/sphinx-autoapi/issues/407\n        # This was fixed in sphinx-autoapi 3.0, however it has requirement sphinx>=6.1, but we stuck on 5.x\n        \"sphinx-autoapi>=2.1.1\",\n        \"sphinx-copybutton>=0.5.2\",\n        \"sphinx-design>=0.5.0\",\n        \"sphinx-jinja>=2.0.2\",\n        \"sphinx-rtd-theme>=2.0.0\",\n        # Currently we are using sphinx 5 but we need to migrate to Sphinx 7\n        \"sphinx>=5.3.0,<6.0.0\",\n        \"sphinxcontrib-applehelp>=1.0.4\",\n        \"sphinxcontrib-devhelp>=1.0.2\",\n        \"sphinxcontrib-htmlhelp>=2.0.1\",\n        \"sphinxcontrib-httpdomain>=1.8.1\",\n        \"sphinxcontrib-jquery>=4.1\",\n        \"sphinxcontrib-jsmath>=1.0.1\",\n        \"sphinxcontrib-qthelp>=1.0.3\",\n        \"sphinxcontrib-redoc>=1.6.0\",\n        \"sphinxcontrib-serializinghtml==1.1.5\",\n        \"sphinxcontrib-spelling>=8.0.0\",\n    ],\n    \"doc-gen\": [\n        \"apache-airflow[doc]\",\n        # The graphviz package creates friction when installing on MacOS as it needs graphviz system package to\n        # be installed, and it's really only used for very obscure features of Airflow, so we can skip it on MacOS\n        # Instead, if someone attempts to use it on MacOS, they will get explanatory error on how to install it\n        \"diagrams>=0.23.4; sys_platform != 'darwin'\",\n        \"eralchemy2>=1.3.8; sys_platform != 'darwin'\",\n    ],\n    # END OF doc extras\n}\n\nDEVEL_EXTRAS: dict[str, list[str]] = {\n    # START OF devel extras\n    \"devel-debuggers\": [\n        \"ipdb>=0.13.13\",\n        \"pdbr>=0.8.9\",\n    ],\n    \"devel-devscripts\": [\n        \"click>=8.0\",\n        \"gitpython>=3.1.40\",\n        \"incremental>=24.7.2\",\n        \"pipdeptree>=2.13.1\",\n        \"pygithub>=2.1.1\",\n        \"restructuredtext-lint>=1.4.0\",\n        \"rich-click>=1.7.0\",\n        \"semver>=3.0.2\",\n        \"towncrier>=23.11.0\",\n        \"twine>=4.0.2\",\n    ],\n    \"devel-duckdb\": [\n        # Python 3.12 support was added in 0.10.0\n        \"duckdb>=0.10.0; python_version >= '3.12'\",\n        \"duckdb>=0.9.0; python_version < '3.12'\",\n    ],\n    # Mypy 0.900 and above ships only with stubs from stdlib so if we need other stubs, we need to install them\n    # manually as `types-*`. See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-imports\n    # for details. We want to install them explicitly because we want to eventually move to\n    # mypyd which does not support installing the types dynamically with --install-types\n    \"devel-mypy\": [\n        # TODO: upgrade to newer versions of MyPy continuously as they are released\n        # Make sure to upgrade the mypy version in update-common-sql-api-stubs in .pre-commit-config.yaml\n        # when you upgrade it here !!!!\n        \"mypy==1.9.0\",\n        \"types-Deprecated>=1.2.9.20240311\",\n        \"types-Markdown>=3.6.0.20240316\",\n        \"types-PyMySQL>=1.1.0.20240425\",\n        \"types-PyYAML>=6.0.12.20240724\",\n        \"types-aiofiles>=23.2.0.20240403\",\n        \"types-certifi>=2021.10.8.3\",\n        \"types-croniter>=2.0.0.20240423\",\n        \"types-docutils>=0.21.0.20240704\",\n        \"types-paramiko>=3.4.0.20240423\",\n        \"types-protobuf>=5.26.0.20240422\",\n        \"types-python-dateutil>=2.9.0.20240316\",\n        \"types-python-slugify>=8.0.2.20240310\",\n        \"types-pytz>=2024.1.0.20240417\",\n        \"types-redis>=4.6.0.20240425\",\n        # aiobotocore>=2.9.0 requires urllib<2. types-requests>=2.31.0.7 uses urllib>2.\n        # hence, 2.31.0.6 is required for aiobotocore>=2.9.0\n        \"types-requests>=2.31.0.6\",\n        \"types-setuptools>=69.5.0.20240423\",\n        \"types-tabulate>=0.9.0.20240106\",\n        \"types-toml>=0.10.8.20240310\",\n    ],\n    \"devel-sentry\": [\n        \"blinker>=1.7.0\",\n    ],\n    \"devel-static-checks\": [\n        \"black>=23.12.0\",\n        \"ruff==0.8.1\",\n        \"yamllint>=1.33.0\",\n    ],\n    \"devel-tests\": [\n        \"aiofiles>=23.2.0\",\n        \"aioresponses>=0.7.6\",\n        \"beautifulsoup4>=4.7.1\",\n        # Coverage 7.4.0 added experimental support for Python 3.12 PEP669 which we use in Airflow\n        \"coverage>=7.4.0\",\n        \"deepdiff>=8.1.1\",\n        \"jmespath>=0.7.0\",\n        \"kgb>=7.0.0\",\n        # We need to adjust all our tests to work with \"proper\" handiing of the async loops in pytest-asyncio\n        # Implemented in Pytest-asyncio 0.25.1 and 0.25.2. See: https://github.com/apache/airflow/issues/45355\n        \"pytest-asyncio>=0.23.6,<0.25.1\",\n        \"pytest-cov>=4.1.0\",\n        \"pytest-custom-exit-code>=0.3.0\",\n        \"pytest-icdiff>=0.9\",\n        \"pytest-instafail>=0.5.0\",\n        \"pytest-mock>=3.12.0\",\n        \"pytest-rerunfailures>=13.0\",\n        \"pytest-timeouts>=1.2.1\",\n        \"pytest-unordered>=0.6.1\",\n        \"pytest-xdist>=3.5.0\",\n        \"pytest>=8.2,<9\",\n        \"requests_mock>=1.11.0\",\n        \"semver>=3.0.2\",\n        \"time-machine>=2.13.0\",\n        \"wheel>=0.42.0\",\n    ],\n    \"devel\": [\n        \"apache-airflow[celery]\",\n        \"apache-airflow[cncf-kubernetes]\",\n        \"apache-airflow[devel-debuggers]\",\n        \"apache-airflow[devel-devscripts]\",\n        \"apache-airflow[devel-duckdb]\",\n        \"apache-airflow[devel-mypy]\",\n        \"apache-airflow[devel-sentry]\",\n        \"apache-airflow[devel-static-checks]\",\n        \"apache-airflow[devel-tests]\",\n    ],\n    \"devel-all-dbs\": [\n        \"apache-airflow[apache-cassandra]\",\n        \"apache-airflow[apache-drill]\",\n        \"apache-airflow[apache-druid]\",\n        \"apache-airflow[apache-hdfs]\",\n        \"apache-airflow[apache-hive]\",\n        \"apache-airflow[apache-impala]\",\n        \"apache-airflow[apache-pinot]\",\n        \"apache-airflow[arangodb]\",\n        \"apache-airflow[cloudant]\",\n        \"apache-airflow[databricks]\",\n        \"apache-airflow[exasol]\",\n        \"apache-airflow[influxdb]\",\n        \"apache-airflow[microsoft-mssql]\",\n        \"apache-airflow[mongo]\",\n        \"apache-airflow[mysql]\",\n        \"apache-airflow[neo4j]\",\n        \"apache-airflow[postgres]\",\n        \"apache-airflow[presto]\",\n        \"apache-airflow[trino]\",\n        \"apache-airflow[vertica]\",\n    ],\n    \"devel-ci\": [\n        \"apache-airflow[devel-all]\",\n    ],\n    \"devel-hadoop\": [\n        \"apache-airflow[apache-hdfs]\",\n        \"apache-airflow[apache-hive]\",\n        \"apache-airflow[apache-impala]\",\n        \"apache-airflow[devel]\",\n        \"apache-airflow[hdfs]\",\n        \"apache-airflow[kerberos]\",\n        \"apache-airflow[presto]\",\n    ],\n}\n\nBUNDLE_EXTRAS: dict[str, list[str]] = {\n    \"all-dbs\": [\n        \"apache-airflow[apache-cassandra]\",\n        \"apache-airflow[apache-drill]\",\n        \"apache-airflow[apache-druid]\",\n        \"apache-airflow[apache-hdfs]\",\n        \"apache-airflow[apache-hive]\",\n        \"apache-airflow[apache-impala]\",\n        \"apache-airflow[apache-pinot]\",\n        \"apache-airflow[arangodb]\",\n        \"apache-airflow[cloudant]\",\n        \"apache-airflow[databricks]\",\n        \"apache-airflow[exasol]\",\n        \"apache-airflow[influxdb]\",\n        \"apache-airflow[microsoft-mssql]\",\n        \"apache-airflow[mongo]\",\n        \"apache-airflow[mysql]\",\n        \"apache-airflow[neo4j]\",\n        \"apache-airflow[postgres]\",\n        \"apache-airflow[presto]\",\n        \"apache-airflow[trino]\",\n        \"apache-airflow[vertica]\",\n    ],\n}\n\nDEPENDENCIES = [\n    # Alembic is important to handle our migrations in predictable and performant way. It is developed\n    # together with SQLAlchemy. Our experience with Alembic is that it very stable in minor version\n    # The 1.13.0 of alembic marked some migration code as SQLAlchemy 2+ only so we limit it to 1.13.1\n    \"alembic>=1.13.1, <2.0\",\n    \"argcomplete>=1.10\",\n    \"asgiref>=2.3.0\",\n    \"attrs>=22.1.0\",\n    # Blinker use for signals in Flask, this is an optional dependency in Flask 2.2 and lower.\n    # In Flask 2.3 it becomes a mandatory dependency, and flask signals are always available.\n    \"blinker>=1.6.2\",\n    \"colorlog>=6.8.2\",\n    \"configupdater>=3.1.1\",\n    # `airflow/www/extensions/init_views` imports `connexion.decorators.validation.RequestBodyValidator`\n    # connexion v3 has refactored the entire module to middleware, see: /spec-first/connexion/issues/1525\n    # Specifically, RequestBodyValidator was removed in: /spec-first/connexion/pull/1595\n    # The usage was added in #30596, seemingly only to override and improve the default error message.\n    # Either revert that change or find another way, preferably without using connexion internals.\n    # This limit can be removed after https://github.com/apache/airflow/issues/35234 is fixed\n    \"connexion[flask]>=2.14.2,<3.0\",\n    \"cron-descriptor>=1.2.24\",\n    \"croniter>=2.0.2\",\n    \"cryptography>=41.0.0\",\n    \"deprecated>=1.2.13\",\n    \"dill>=0.2.2\",\n    # Required for python 3.9 to work with new annotations styles. Check package\n    # description on PyPI for more details: https://pypi.org/project/eval-type-backport/\n    'eval-type-backport>=0.2.0;python_version<\"3.10\"',\n    \"fastapi[standard]>=0.112.2\",\n    \"flask-caching>=2.0.0\",\n    # Flask-Session 0.6 add new arguments into the SqlAlchemySessionInterface constructor as well as\n    # all parameters now are mandatory which make AirflowDatabaseSessionInterface incompatible with this version.\n    \"flask-session>=0.4.0,<0.6\",\n    \"flask-wtf>=1.1.0\",\n    # Flask 2.3 is scheduled to introduce a number of deprecation removals - some of them might be breaking\n    # for our dependencies - notably `_app_ctx_stack` and `_request_ctx_stack` removals.\n    # We should remove the limitation after 2.3 is released and our dependencies are updated to handle it\n    \"flask>=2.2.1,<2.3\",\n    \"fsspec>=2023.10.0\",\n    \"gitpython>=3.1.40\",\n    'google-re2>=1.0;python_version<\"3.12\"',\n    'google-re2>=1.1;python_version>=\"3.12\"',\n    \"gunicorn>=20.1.0\",\n    \"httpx>=0.25.0\",\n    'importlib_metadata>=6.5;python_version<\"3.12\"',\n    \"itsdangerous>=2.0\",\n    \"jinja2>=3.0.0\",\n    \"jsonschema>=4.18.0\",\n    \"lazy-object-proxy>=1.2.0\",\n    \"linkify-it-py>=2.0.0\",\n    \"lockfile>=0.12.2\",\n    \"markdown-it-py>=2.1.0\",\n    \"markupsafe>=1.1.1\",\n    \"marshmallow-oneofschema>=2.0.1\",\n    \"mdit-py-plugins>=0.3.0\",\n    \"methodtools>=0.4.7\",\n    \"opentelemetry-api>=1.24.0\",\n    \"opentelemetry-exporter-otlp>=1.24.0\",\n    \"packaging>=23.0\",\n    \"pathspec>=0.9.0\",\n    'pendulum>=2.1.2,<4.0;python_version<\"3.12\"',\n    'pendulum>=3.0.0,<4.0;python_version>=\"3.12\"',\n    \"pluggy>=1.5.0\",\n    \"psutil>=5.8.0\",\n    \"pydantic>=2.10.2\",\n    # Pygments 2.19.0 improperly renders .ini files with dictionaries as values\n    # See https://github.com/pygments/pygments/issues/2834\n    \"pygments>=2.0.1,!=2.19.0\",\n    \"pyjwt>=2.0.0\",\n    \"python-daemon>=3.0.0\",\n    \"python-dateutil>=2.7.0\",\n    \"python-nvd3>=0.15.0\",\n    \"python-slugify>=5.0\",\n    # Requests 3 if it will be released, will be heavily breaking.\n    \"requests>=2.27.0,<3\",\n    \"requests-toolbelt>=0.4.0\",\n    \"rfc3339-validator>=0.1.4\",\n    \"rich-argparse>=1.0.0\",\n    \"rich>=12.4.4\",\n    \"setproctitle>=1.3.3\",\n    # We use some deprecated features of sqlalchemy 2.0 and we should replace them before we can upgrade\n    # See https://sqlalche.me/e/b8d9 for details of deprecated features\n    # you can set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.\n    # The issue tracking it is https://github.com/apache/airflow/issues/28723\n    \"sqlalchemy>=1.4.36,<2.0\",\n    \"sqlalchemy-jsonfield>=1.0\",\n    \"sqlalchemy-utils>=0.41.2\",\n    \"tabulate>=0.7.5\",\n    \"tenacity>=8.0.0,!=8.2.0\",\n    \"termcolor>=2.5.0\",\n    # Universal Pathlib 0.2.4 adds extra validation for Paths and our integration with local file paths\n    # Does not work with it Tracked in https://github.com/fsspec/universal_pathlib/issues/276\n    \"universal-pathlib>=0.2.2,!=0.2.4\",\n    \"uuid6>=2024.7.10\",\n    # Werkzug 3 breaks Flask-Login 0.6.2, also connexion needs to be updated to >= 3.0\n    # we should remove this limitation when FAB supports Flask 2.3 and we migrate connexion to 3+\n    \"werkzeug>=2.0,<3\",\n]\n\n\nALL_DYNAMIC_EXTRA_DICTS: list[tuple[dict[str, list[str]], str]] = [\n    (CORE_EXTRAS, \"Core extras\"),\n    (DOC_EXTRAS, \"Doc extras\"),\n    (DEVEL_EXTRAS, \"Devel extras\"),\n    (BUNDLE_EXTRAS, \"Bundle extras\"),\n]\n\nALL_GENERATED_BUNDLE_EXTRAS = [\"all\", \"all-core\", \"devel-all\", \"devel-ci\"]\n\n\ndef normalize_extra(dependency_id: str) -> str:\n    return dependency_id.replace(\".\", \"-\").replace(\"_\", \"-\")\n\n\ndef normalize_requirement(requirement: str):\n    from packaging.requirements import Requirement\n    from packaging.utils import NormalizedName, canonicalize_name\n\n    req = Requirement(requirement)\n    package: NormalizedName = canonicalize_name(req.name)\n    package_str = str(package)\n    if req.extras:\n        # Sort extras by name\n        package_str += f\"[{','.join(sorted([normalize_extra(extra) for extra in req.extras]))}]\"\n    version_required = \"\"\n    if req.specifier:\n        version_required = \",\".join(map(str, sorted(req.specifier, key=lambda spec: spec.version)))\n    if req.marker:\n        version_required += f\"; {req.marker}\"\n    return str(package_str + version_required)\n\n\nALL_DYNAMIC_EXTRAS: list[str] = sorted(\n    set(\n        itertools.chain(\n            *[d for d, desc in ALL_DYNAMIC_EXTRA_DICTS],\n            [normalize_extra(provider) for provider in PROVIDER_DEPENDENCIES],\n            ALL_GENERATED_BUNDLE_EXTRAS,\n        )\n    )\n)\n\n\ndef get_provider_id(provider_spec: str) -> str:\n    \"\"\"\n    Extract provider id from provider specification.\n\n    :param provider_spec: provider specification can be in the form of the \"PROVIDER_ID\" or\n           \"apache-airflow-providers-PROVIDER\", optionally followed by \">=VERSION\".\n\n    :return: short provider_id with `.` instead of `-` in case of `apache` and other providers with\n             `-` in the name.\n    \"\"\"\n    _provider_id = provider_spec.split(\">=\")[0]\n    if _provider_id.startswith(\"apache-airflow-providers-\"):\n        _provider_id = _provider_id.replace(\"apache-airflow-providers-\", \"\").replace(\"-\", \".\")\n    return _provider_id\n\n\ndef get_provider_requirement(provider_spec: str) -> str:\n    \"\"\"\n    Convert provider specification with provider_id to provider requirement.\n\n    The requirement can be used when constructing dependencies. It automatically adds pre-release specifier\n    in case we are building pre-release version of Airflow. This way we can handle the case when airflow\n    depends on specific version of the provider that has not yet been released - then we release the\n    pre-release version of provider to PyPI and airflow built in CI, or Airflow pre-release version will\n    automatically depend on that pre-release version of the provider.\n\n    :param provider_spec: provider specification can be in the form of the \"PROVIDER_ID\" optionally followed\n       by >=VERSION.\n    :return: requirement for the provider that can be used as dependency.\n    \"\"\"\n    if \">=\" in provider_spec:\n        # we cannot import `airflow` here directly as it would pull re2 and a number of airflow\n        # dependencies so we need to read airflow version by matching a regexp\n        airflow_init_content = (AIRFLOW_ROOT_PATH / \"airflow\" / \"__init__.py\").read_text()\n        airflow_version_pattern = r'__version__ = \"(\\d+\\.\\d+\\.\\d+\\S*)\"'\n        airflow_version_match = re.search(airflow_version_pattern, airflow_init_content)\n        if not airflow_version_match:\n            raise RuntimeError(\"Cannot find Airflow version in airflow/__init__.py\")\n        from packaging.version import Version\n\n        current_airflow_version = Version(airflow_version_match.group(1))\n        provider_id, min_version = provider_spec.split(\">=\")\n        provider_version = Version(min_version)\n        if provider_version.is_prerelease and not current_airflow_version.is_prerelease:\n            # strip pre-release version from the pre-installed provider's version when we are preparing\n            # the official package\n            min_version = str(provider_version.base_version)\n        return f\"apache-airflow-providers-{provider_id.replace('.', '-')}>={min_version}\"\n    else:\n        return f\"apache-airflow-providers-{provider_spec.replace('.', '-')}\"\n\n\n# if providers are ready, we build provider requirements for them\nPREINSTALLED_PROVIDER_REQUIREMENTS = [\n    get_provider_requirement(provider_spec)\n    for provider_spec in PRE_INSTALLED_PROVIDERS\n    if PROVIDER_DEPENDENCIES[get_provider_id(provider_spec)][\"state\"] == \"ready\"\n]\n\n# Here we keep all pre-installed provider dependencies, so that we can add them as requirements in\n# editable build to make sure that all dependencies are installed when we install Airflow in editable mode\n# We need to skip apache-airflow min-versions and flag (exit) when pre-installed provider has\n# dependency to another provider\nALL_PREINSTALLED_PROVIDER_DEPS: list[str] = []\n\n# We very rarely - and only for the time when we plan to release a new preinstalled provider in next release\n# we have the preinstalled provider that is in non-ready state.\n# If provider is in not-ready state, we need to install its dependencies in editable mode as well as\n# when we are building the wheel in CI. In pre-release branch we should never have a non-ready provider\n# added, so this will only be used in main branch for CI builds.\nPREINSTALLED_NOT_READY_PROVIDER_DEPS: list[str] = []\n\nfor provider_spec in PRE_INSTALLED_PROVIDERS:\n    provider_id = get_provider_id(provider_spec)\n    for dependency in PROVIDER_DEPENDENCIES[provider_id][\"deps\"]:\n        if (\n            dependency.startswith(\"apache-airflow-providers\")\n            and get_provider_id(dependency) not in PRE_INSTALLED_PROVIDERS\n        ):\n            msg = (\n                f\"The provider {provider_id} is pre-installed and it has a dependency \"\n                f\"to another provider {dependency} which is not preinstalled. This is not allowed. \"\n                f\"Pre-installed providers should only have 'apache-airflow', other preinstalled providers\"\n                f\"and regular non-airflow dependencies.\"\n            )\n            raise SystemExit(msg)\n        if not dependency.startswith(\"apache-airflow\"):\n            if PROVIDER_DEPENDENCIES[provider_id][\"state\"] not in [\"suspended\", \"removed\"]:\n                ALL_PREINSTALLED_PROVIDER_DEPS.append(dependency)\n                if PROVIDER_DEPENDENCIES[provider_id][\"state\"] in [\"not-ready\"]:\n                    PREINSTALLED_NOT_READY_PROVIDER_DEPS.append(dependency)\n\nALL_PREINSTALLED_PROVIDER_DEPS = sorted(set(ALL_PREINSTALLED_PROVIDER_DEPS))\nPREINSTALLED_NOT_READY_PROVIDER_DEPS = sorted(set(PREINSTALLED_NOT_READY_PROVIDER_DEPS))\n\n\nclass CustomBuild(BuilderInterface[BuilderConfig, PluginManager]):\n    \"\"\"Custom build class for Airflow assets and git version.\"\"\"\n\n    # Note that this name of the plugin MUST be `custom` - as long as we use it from custom\n    # hatch_build.py file and not from external plugin. See note in the:\n    # https://hatch.pypa.io/latest/plugins/build-hook/custom/#example\n    PLUGIN_NAME = \"custom\"\n\n    def clean(self, directory: str, versions: Iterable[str]) -> None:\n        work_dir = Path(self.root)\n        commands = [\n            [\"rm -rf airflow/www/static/dist\"],\n            [\"rm -rf airflow/www/node_modules\"],\n        ]\n        for cmd in commands:\n            run(cmd, cwd=work_dir.as_posix(), check=True, shell=True)\n\n    def get_version_api(self) -> dict[str, Callable[..., str]]:\n        \"\"\"Get custom build target for standard package preparation.\"\"\"\n        return {\"standard\": self.build_standard}\n\n    def build_standard(self, directory: str, artifacts: Any, **build_data: Any) -> str:\n        self.write_git_version()\n        work_dir = Path(self.root)\n        commands = [\n            [\"pre-commit run --hook-stage manual compile-www-assets --all-files\"],\n        ]\n        for cmd in commands:\n            run(cmd, cwd=work_dir.as_posix(), check=True, shell=True)\n        dist_path = work_dir / \"airflow\" / \"www\" / \"static\" / \"dist\"\n        return dist_path.resolve().as_posix()\n\n    def get_git_version(self) -> str:\n        \"\"\"\n        Return a version to identify the state of the underlying git repo.\n\n        The version will indicate whether the head of the current git-backed working directory\n        is tied to a release tag or not. It will indicate the former with a 'release:{version}'\n        prefix and the latter with a '.dev0' suffix. Following the prefix will be a sha of the\n        current branch head. Finally, a \"dirty\" suffix is appended to indicate that uncommitted\n        changes are present.\n\n        Example pre-release version: \".dev0+2f635dc265e78db6708f59f68e8009abb92c1e65\".\n        Example release version: \".release+2f635dc265e78db6708f59f68e8009abb92c1e65\".\n        Example modified release version: \".release+2f635dc265e78db6708f59f68e8009abb92c1e65\".dirty\n\n        :return: Found Airflow version in Git repo.\n        \"\"\"\n        try:\n            import git\n\n            try:\n                repo = git.Repo(str(Path(self.root) / \".git\"))\n            except git.NoSuchPathError:\n                log.warning(\".git directory not found: Cannot compute the git version\")\n                return \"\"\n            except git.InvalidGitRepositoryError:\n                log.warning(\"Invalid .git directory not found: Cannot compute the git version\")\n                return \"\"\n        except ImportError:\n            log.warning(\"gitpython not found: Cannot compute the git version.\")\n            return \"\"\n        if repo:\n            sha = repo.head.commit.hexsha\n            if repo.is_dirty():\n                return f\".dev0+{sha}.dirty\"\n            # commit is clean\n            return f\".release:{sha}\"\n        return \"no_git_version\"\n\n    def write_git_version(self) -> None:\n        \"\"\"Write git version to git_version file.\"\"\"\n        version = self.get_git_version()\n        git_version_file = Path(self.root) / \"airflow\" / \"git_version\"\n        self.app.display(f\"Writing version {version} to {git_version_file}\")\n        git_version_file.write_text(version)\n\n\ndef _is_devel_extra(extra: str) -> bool:\n    return extra.startswith(\"devel\") or extra in [\"doc\", \"doc-gen\"]\n\n\nGENERATED_DEPENDENCIES_START = \"# START OF GENERATED DEPENDENCIES\"\nGENERATED_DEPENDENCIES_END = \"# END OF GENERATED DEPENDENCIES\"\n\n\ndef convert_to_extra_dependency(provider_requirement: str) -> str:\n    \"\"\"\n    Convert provider specification to extra dependency.\n\n    :param provider_requirement: requirement of the provider in the form of apache-airflow-provider-*,\n        optionally followed by >=VERSION.\n    :return: extra dependency in the form of apache-airflow[extra]\n    \"\"\"\n    # if there is version in dependency - remove it as we do not need it in extra specification\n    # for editable installation\n    if \">=\" in provider_requirement:\n        provider_requirement = provider_requirement.split(\">=\")[0]\n    extra = provider_requirement.replace(\"apache-airflow-providers-\", \"\").replace(\"-\", \"_\").replace(\".\", \"_\")\n    return f\"apache-airflow[{extra}]\"\n\n\ndef get_python_exclusion(excluded_python_versions: list[str]):\n    \"\"\"\n    Produce the Python exclusion that should be used - converted from the list of python versions.\n\n    :param excluded_python_versions: list of python versions to exclude the dependency for.\n    :return: python version exclusion string that can be added to dependency in specification.\n    \"\"\"\n    exclusion = \"\"\n    if excluded_python_versions:\n        separator = \";\"\n        for version in excluded_python_versions:\n            exclusion += f'{separator}python_version != \"{version}\"'\n            separator = \" and \"\n    return exclusion\n\n\ndef skip_for_editable_build(excluded_python_versions: list[str]) -> bool:\n    \"\"\"\n    Whether the dependency should be skipped for editable build for current python version.\n\n    :param excluded_python_versions: list of excluded python versions.\n    :return: True if the dependency should be skipped for editable build for the current python version.\n    \"\"\"\n    current_python_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n    if current_python_version in excluded_python_versions:\n        return True\n    return False\n\n\nclass CustomBuildHook(BuildHookInterface[BuilderConfig]):\n    \"\"\"\n    Custom build hook for Airflow.\n\n    Generates required and optional dependencies depends on the build `version`.\n\n    - standard: Generates all dependencies for the standard (.whl) package:\n       * devel and doc extras not included\n       * core extras and \"production\" bundle extras included\n       * provider optional dependencies resolve to \"apache-airflow-providers-{provider}\"\n       * pre-installed providers added as required dependencies\n\n    - editable: Generates all dependencies for the editable installation:\n       * devel and doc extras (including devel bundle extras are included)\n       * core extras and \"production\" bundles included\n       * provider optional dependencies resolve to provider dependencies including devel dependencies\n       * pre-installed providers not included - their dependencies included in devel extras\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        # Stores all dependencies that that any of the airflow extras (including devel) use\n        self.all_devel_ci_dependencies: set[str] = set()\n        # All extras that should be included in the wheel package\n        self.all_non_devel_extras: set[str] = set()\n        # All extras that should be available in the editable install\n        self.all_devel_extras: set[str] = set()\n        self.optional_dependencies: dict[str, list[str]] = {}\n        self._dependencies: list[str] = []\n        super().__init__(*args, **kwargs)\n\n    def initialize(self, version: str, build_data: dict[str, Any]) -> None:\n        \"\"\"\n        Initialize hook immediately before each build.\n\n        Any modifications to the build data will be seen by the build target.\n\n        :param version: \"standard\" or \"editable\" build.\n        :param build_data: build data dictionary.\n        \"\"\"\n        self._process_all_built_in_extras(version)\n        self._process_all_provider_extras(version)\n\n        # Adds all-core extras for the extras that are built-in and not devel\n        self.optional_dependencies[\"all-core\"] = sorted(\n            set([f\"apache-airflow[{extra}]\" for extra in CORE_EXTRAS.keys()])\n        )\n        # Adds \"apache-airflow[extra]\" for all extras that are not devel extras for wheel and editable builds\n        self.optional_dependencies[\"all\"] = [\n            f\"apache-airflow[{extra}]\" for extra in sorted(self.all_non_devel_extras)\n        ]\n        # Adds all devel extras for the extras that are built-in only for editable builds\n        if version != \"standard\":\n            self.optional_dependencies[\"devel-all\"] = [\n                f\"apache-airflow[{extra}]\" for extra in sorted(self.all_devel_extras)\n            ]\n        # This is special dependency that is used to install all possible\n        # 3rd-party dependencies for airflow for the CI image. It is exposed in the wheel package\n        # because we want to use for building the image cache from GitHub URL.\n        self.optional_dependencies[\"devel-ci\"] = sorted(self.all_devel_ci_dependencies)\n        self._dependencies = DEPENDENCIES\n\n        if version == \"standard\":\n            # Inject preinstalled providers into the dependencies for standard packages\n            self._dependencies.extend(PREINSTALLED_PROVIDER_REQUIREMENTS)\n            self._dependencies.extend(PREINSTALLED_NOT_READY_PROVIDER_DEPS)\n        else:\n            self._dependencies.extend(ALL_PREINSTALLED_PROVIDER_DEPS)\n\n        # with hatchling, we can modify dependencies dynamically by modifying the build_data\n        build_data[\"dependencies\"] = self._dependencies\n\n        # unfortunately hatchling currently does not have a way to override optional_dependencies\n        # via build_data (or so it seem) so we need to modify internal _optional_dependencies\n        # field in core.metadata until this is possible\n        self.metadata.core._optional_dependencies = self.optional_dependencies\n\n        # Add entrypoints dynamically for all provider packages, in editable build\n        # else they will not be found by plugin manager\n        if version != \"standard\":\n            entry_points = self.metadata.core._entry_points or {}\n            plugins = entry_points.get(\"airflow.plugins\") or {}\n            for provider in PROVIDER_DEPENDENCIES.values():\n                for plugin in provider[\"plugins\"]:\n                    plugin_class: str = plugin[\"plugin-class\"]\n                    plugins[plugin[\"name\"]] = plugin_class[::-1].replace(\".\", \":\", 1)[::-1]\n            entry_points[\"airflow.plugins\"] = plugins\n            self.metadata.core._entry_points = entry_points\n\n    def _add_devel_ci_dependencies(self, deps: list[str], python_exclusion: str) -> None:\n        \"\"\"\n        Add devel_ci_dependencies.\n\n        Adds all external dependencies which are not apache-airflow deps to the list of dependencies\n        that are going to be added to `devel-ci` extra. Optionally exclude dependencies for specific\n        python versions.\n\n        :param deps: list of dependencies to add\n        :param python_exclusion: Python version exclusion string.\n        \"\"\"\n        for dep in deps:\n            if not dep.startswith(\"apache-airflow\"):\n                self.all_devel_ci_dependencies.add(normalize_requirement(dep) + python_exclusion)\n\n    def _process_all_provider_extras(self, version: str) -> None:\n        \"\"\"\n        Process all provider extras.\n\n        Processes all provider dependencies. This generates dependencies for editable builds\n        and providers for wheel builds.\n\n        :param version: \"standard\" or \"editable\" build.\n        \"\"\"\n        for dependency_id in PROVIDER_DEPENDENCIES.keys():\n            if PROVIDER_DEPENDENCIES[dependency_id][\"state\"] != \"ready\":\n                continue\n            excluded_python_versions = PROVIDER_DEPENDENCIES[dependency_id].get(\"excluded-python-versions\")\n            if version != \"standard\" and skip_for_editable_build(excluded_python_versions):\n                continue\n            normalized_extra_name = normalize_extra(dependency_id)\n            deps: list[str] = PROVIDER_DEPENDENCIES[dependency_id][\"deps\"]\n\n            deps = [dep for dep in deps if not dep.startswith(\"apache-airflow>=\")]\n            devel_deps: list[str] = PROVIDER_DEPENDENCIES[dependency_id].get(\"devel-deps\", [])\n\n            if version == \"standard\":\n                # add providers instead of dependencies for wheel builds\n                self.optional_dependencies[normalized_extra_name] = [\n                    f\"apache-airflow-providers-{normalized_extra_name}\"\n                    f\"{get_python_exclusion(excluded_python_versions)}\"\n                ]\n            else:\n                # for editable packages - add regular + devel dependencies retrieved from provider.yaml\n                # but convert the provider dependencies to apache-airflow[extras]\n                # and adding python exclusions where needed\n                editable_deps = []\n                for dep in itertools.chain(deps, devel_deps):\n                    if dep.startswith(\"apache-airflow-providers-\"):\n                        dep = convert_to_extra_dependency(dep)\n                    editable_deps.append(dep)\n                self.optional_dependencies[normalized_extra_name] = sorted(set(editable_deps))\n                self._add_devel_ci_dependencies(editable_deps, python_exclusion=\"\")\n            self.all_devel_extras.add(normalized_extra_name)\n            self.all_non_devel_extras.add(normalized_extra_name)\n\n    def _process_all_built_in_extras(self, version: str) -> None:\n        \"\"\"\n        Process all built-in extras.\n\n        Adds all core extras (for editable builds) minus devel and doc extras (for wheel builds)\n        to the list of dependencies. It also builds the list of all non-devel built-in extras that will be\n        used to produce \"all\" extra.\n\n        :param version: \"standard\" or \"editable\" build.\n        \"\"\"\n        for dict, _ in ALL_DYNAMIC_EXTRA_DICTS:\n            for extra, deps in dict.items():\n                self.all_devel_extras.add(extra)\n                self._add_devel_ci_dependencies(deps, python_exclusion=\"\")\n                if dict not in [DEVEL_EXTRAS, DOC_EXTRAS]:\n                    # do not add deprecated extras to \"all\" extras\n                    self.all_non_devel_extras.add(extra)\n                if version == \"standard\":\n                    # for wheel builds we skip devel and doc extras\n                    if dict not in [DEVEL_EXTRAS, DOC_EXTRAS]:\n                        self.optional_dependencies[extra] = deps\n                else:\n                    # for editable builds we add all extras\n                    self.optional_dependencies[extra] = deps\n"
        },
        {
          "name": "helm_tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "kubernetes_tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "manifests",
          "type": "tree",
          "content": null
        },
        {
          "name": "newsfragments",
          "type": "tree",
          "content": null
        },
        {
          "name": "performance",
          "type": "tree",
          "content": null
        },
        {
          "name": "prod_image_installed_providers.txt",
          "type": "blob",
          "size": 0.2958984375,
          "content": "# List of all the providers installed in regular airflow PROD image\namazon\ncelery\ncncf.kubernetes\ncommon.compat\ncommon.io\ncommon.sql\ndocker\nelasticsearch\nfab\nftp\ngoogle\ngrpc\nhashicorp\nhttp\nimap\nmicrosoft.azure\nmysql\nodbc\nopenlineage\npostgres\nredis\nsendgrid\nsftp\nslack\nsmtp\nsnowflake\nsqlite\nssh\nstandard\n"
        },
        {
          "name": "providers",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 25.1162109375,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n[build-system]\n# build dependencies should be fixed - including all transitive dependencies. This way we can ensure\n# reproducibility of the build and make sure that any future releases of any dependencies will not\n# break the build of released airflow sources in the future.\n# The dependencies can be automatically upgraded by running:\n# pre-commit run --hook-stage manual update-build-dependencies --all-files\nrequires = [\n    \"GitPython==3.1.44\",\n    \"gitdb==4.0.12\",\n    \"hatchling==1.27.0\",\n    \"packaging==24.2\",\n    \"pathspec==0.12.1\",\n    \"pluggy==1.5.0\",\n    \"smmap==5.0.2\",\n    \"tomli==2.2.1; python_version < '3.11'\",\n    \"trove-classifiers==2025.1.10.15\",\n]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"apache-airflow\"\ndescription = \"Programmatically author, schedule and monitor data pipelines\"\nreadme = { file = \"generated/PYPI_README.md\", content-type = \"text/markdown\" }\nlicense-files.globs = [\"LICENSE\", \"3rd-party-licenses/*.txt\"]\nrequires-python = \"~=3.9,<3.13\"\nauthors = [\n    { name = \"Apache Software Foundation\", email = \"dev@airflow.apache.org\" },\n]\nmaintainers = [\n    { name = \"Apache Software Foundation\", email=\"dev@airflow.apache.org\" },\n]\nkeywords = [ \"airflow\", \"orchestration\", \"workflow\", \"dag\", \"pipelines\", \"automation\", \"data\" ]\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Environment :: Console\",\n    \"Environment :: Web Environment\",\n    \"Framework :: Apache Airflow\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: System Administrators\",\n    \"License :: OSI Approved :: Apache Software License\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: System :: Monitoring\",\n]\n\ndynamic = [\"version\", \"optional-dependencies\", \"dependencies\"]\n\n# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n# !!! YOU MIGHT BE SURPRISED NOT SEEING THE DEPENDENCIES AS `project.dependencies`     !!!!!!!!!\n# !!! AND EXTRAS AS `project.optional-dependencies`                                    !!!!!!!!!\n# !!! THEY ARE marked as `dynamic`  GENERATED by `hatch_build.py`                      !!!!!!!!!\n# !!! SEE COMMENTS BELOW TO FIND WHERE DEPENDENCIES ARE MAINTAINED                     !!!!!!!!!\n# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n#\n# !!!!!! Those providers are defined in `hatch_build.py` and should be maintained there  !!!!!!!\n#\n# Those extras are available as regular core airflow extras - they install optional features of Airflow.\n#\n# START CORE EXTRAS HERE\n#\n# aiobotocore, apache-atlas, apache-webhdfs, async, cgroups, cloudpickle, github-enterprise, google-\n# auth, graphviz, kerberos, ldap, leveldb, otel, pandas, password, rabbitmq, s3fs, sentry, statsd, uv\n#\n# END CORE EXTRAS HERE\n#\n# The ``devel`` extras are not available in the released packages. They are only available when you install\n# Airflow from sources in ``editable`` installation - i.e. one that you are usually using to contribute to\n# Airflow. They provide tools such as ``pytest`` and ``mypy`` for general purpose development and testing.\n#\n# START DEVEL EXTRAS HERE\n#\n# devel, devel-all-dbs, devel-ci, devel-debuggers, devel-devscripts, devel-duckdb, devel-hadoop,\n# devel-mypy, devel-sentry, devel-static-checks, devel-tests\n#\n# END DEVEL EXTRAS HERE\n#\n# Those extras are bundles dynamically generated from other extras.\n#\n# START BUNDLE EXTRAS HERE\n#\n# all, all-core, all-dbs, devel-all, devel-ci\n#\n# END BUNDLE EXTRAS HERE\n#\n# The ``doc`` extras are not available in the released packages. They are only available when you install\n# Airflow from sources in ``editable`` installation - i.e. one that you are usually using to contribute to\n# Airflow. They provide tools needed when you want to build Airflow documentation (note that you also need\n# ``devel`` extras installed for airflow and providers in order to build documentation for airflow and\n# provider packages respectively). The ``doc`` package is enough to build regular documentation, where\n# ``doc_gen`` is needed to generate ER diagram we have describing our database.\n#\n# START DOC EXTRAS HERE\n#\n# doc, doc-gen\n#\n# END DOC EXTRAS HERE\n#\n# The `deprecated` extras are deprecated extras from Airflow 1 that will be removed in future versions.\n#\n# START DEPRECATED EXTRAS HERE\n#\n# atlas, aws, azure, cassandra, crypto, druid, gcp, gcp-api, hdfs, hive, kubernetes, mssql, pinot, s3,\n# spark, webhdfs, winrm\n#\n# END DEPRECATED EXTRAS HERE\n#\n# !!!!!! Those providers are defined in the `airflow/providers/<provider>/provider.yaml` files  !!!!!!!\n#\n# Those extras are available as regular Airflow extras, they install provider packages in standard builds\n# or dependencies that are necessary to enable the feature in editable build.\n# START PROVIDER EXTRAS HERE\n#\n# airbyte, alibaba, amazon, apache.beam, apache.cassandra, apache.drill, apache.druid, apache.flink,\n# apache.hdfs, apache.hive, apache.iceberg, apache.impala, apache.kafka, apache.kylin, apache.livy,\n# apache.pig, apache.pinot, apache.spark, apprise, arangodb, asana, atlassian.jira, celery, cloudant,\n# cncf.kubernetes, cohere, common.compat, common.io, common.sql, databricks, datadog, dbt.cloud,\n# dingding, discord, docker, edge, elasticsearch, exasol, fab, facebook, ftp, github, google, grpc,\n# hashicorp, http, imap, influxdb, jdbc, jenkins, microsoft.azure, microsoft.mssql, microsoft.psrp,\n# microsoft.winrm, mongo, mysql, neo4j, odbc, openai, openfaas, openlineage, opensearch, opsgenie,\n# oracle, pagerduty, papermill, pgvector, pinecone, postgres, presto, qdrant, redis, salesforce,\n# samba, segment, sendgrid, sftp, singularity, slack, smtp, snowflake, sqlite, ssh, standard, tableau,\n# telegram, teradata, trino, vertica, weaviate, yandex, ydb, zendesk\n#\n# END PROVIDER EXTRAS HERE\n\n[project.scripts]\nairflow = \"airflow.__main__:main\"\n[project.urls]\n\"Bug Tracker\" = \"https://github.com/apache/airflow/issues\"\nDocumentation = \"https://airflow.apache.org/docs/\"\nDownloads = \"https://archive.apache.org/dist/airflow/\"\nHomepage = \"https://airflow.apache.org/\"\n\"Release Notes\" = \"https://airflow.apache.org/docs/apache-airflow/stable/release_notes.html\"\n\"Slack Chat\" = \"https://s.apache.org/airflow-slack\"\n\"Source Code\" = \"https://github.com/apache/airflow\"\nX = \"https://x.com/ApacheAirflow\"\nLinkedIn = \"https://www.linkedin.com/company/apache-airflow/\"\nMastodon = \"https://fosstodon.org/@airflow\"\nBluesky = \"https://bsky.app/profile/apache-airflow.bsky.social\"\nYouTube = \"https://www.youtube.com/channel/UCSXwxpWZQ7XZ1WL3wqevChA/\"\n\n[tool.hatch.version]\npath = \"airflow/__init__.py\"\n\n[tool.hatch.build.targets.wheel.hooks.custom]\npath = \"./hatch_build.py\"\n\n[tool.hatch.build.hooks.custom]\npath = \"./hatch_build.py\"\n\n[tool.hatch.build.targets.custom]\npath = \"./hatch_build.py\"\n\n[tool.hatch.build.targets.sdist]\ninclude = [\n    \"/airflow\",\n    \"/airflow/git_version\"\n]\nexclude = [\n    \"/airflow/www/node_modules/\"\n]\nartifacts = [\n    \"/airflow/www/static/dist/\",\n    \"/airflow/git_version\",\n    \"/generated/\",\n]\n\n[tool.hatch.build.targets.wheel]\ninclude = [\n    \"/airflow\",\n]\nartifacts = [\n    \"/airflow/www/static/dist/\",\n    \"/airflow/git_version\"\n]\n\n## black settings ##\n# Used to format code examples inside .rst doc files\n# Needed until https://github.com/astral-sh/ruff/issues/8237 is available.\n[tool.black]\nline-length = 110\ntarget-version = ['py39', 'py310', 'py311', 'py312']\n\n\n## ruff settings ##\n[tool.ruff]\ntarget-version = \"py39\"\nline-length = 110\nextend-exclude = [\n    \".eggs\",\n    \"*/_vendor/*\",\n    \"tests/dags/test_imports.py\",\n]\n\nnamespace-packages = [\"airflow/providers\"]\n\n[tool.ruff.lint]\ntyping-modules = [\"airflow.typing_compat\"]\nextend-select = [\n    # Enable entire ruff rule section\n    \"I\", # Missing required import (auto-fixable)\n    \"UP\", # Pyupgrade\n    \"ASYNC\", # subset of flake8-async rules\n    \"ISC\",  # Checks for implicit literal string concatenation (auto-fixable)\n    \"TC\", # Rules around TYPE_CHECKING blocks\n    \"G\", # flake8-logging-format rules\n    \"LOG\", # flake8-logging rules, most of them autofixable\n    \"PT\", # flake8-pytest-style rules\n    \"TID25\", # flake8-tidy-imports rules\n    \"E\", # pycodestyle rules\n    \"W\", # pycodestyle rules\n    # Per rule enables\n    \"RUF006\", # Checks for asyncio dangling task\n    \"RUF015\", # Checks for unnecessary iterable allocation for first element\n    \"RUF019\", # Checks for unnecessary key check\n    \"RUF100\", # Unused noqa (auto-fixable)\n    # We ignore more pydocstyle than we enable, so be more selective at what we enable\n    \"D1\",\n    \"D2\",\n    \"D213\", # Conflicts with D212.  Both can not be enabled.\n    \"D3\",\n    \"D400\",\n    \"D401\",\n    \"D402\",\n    \"D403\",\n    \"D412\",\n    \"D419\",\n    \"PGH004\",  # Use specific rule codes when using noqa\n    \"PGH005\", # Invalid unittest.mock.Mock methods/attributes/properties\n    \"S101\", # Checks use `assert` outside the test cases, test cases should be added into the exclusions\n    \"SIM300\", # Checks for conditions that position a constant on the left-hand side of the comparison\n              # operator, rather than the right-hand side.\n    \"B004\", # Checks for use of hasattr(x, \"__call__\") and replaces it with callable(x)\n    \"B006\", # Checks for uses of mutable objects as function argument defaults.\n    \"B007\", # Checks for unused variables in the loop\n    \"B017\", # Checks for pytest.raises context managers that catch Exception or BaseException.\n    \"B019\", # Use of functools.lru_cache or functools.cache on methods can lead to memory leaks\n    \"B028\", # No explicit stacklevel keyword argument found\n    \"TRY002\", # Prohibit use of `raise Exception`, use specific exceptions instead.\n]\nignore = [\n    \"D100\", # Unwanted; Docstring at the top of every file.\n    \"D102\", # TODO: Missing docstring in public method\n    \"D103\", # TODO: Missing docstring in public function\n    \"D104\", # Unwanted; Docstring at the top of every `__init__.py` file.\n    \"D105\", # Unwanted; See https://lists.apache.org/thread/8jbg1dd2lr2cfydtqbjxsd6pb6q2wkc3\n    \"D107\", # Unwanted; Docstring in every constructor is unnecessary if the class has a docstring.\n    \"D203\",\n    \"D212\", # Conflicts with D213.  Both can not be enabled.\n    \"E731\", # Do not assign a lambda expression, use a def\n    \"TC003\", # Do not move imports from stdlib to TYPE_CHECKING block\n    \"PT006\", # Wrong type of names in @pytest.mark.parametrize\n    \"PT007\", # Wrong type of values in @pytest.mark.parametrize\n    \"PT013\", # silly rule prohibiting e.g. `from pytest import param`\n    \"PT011\", # pytest.raises() is too broad, set the match parameter\n    \"PT019\", # fixture without value is injected as parameter, use @pytest.mark.usefixtures instead\n    # Rules below explicitly set off which could overlap with Ruff's formatter\n    # as it recommended by https://docs.astral.sh/ruff/formatter/#conflicting-lint-rules\n    # Except ISC rules\n    \"W191\",\n    \"E111\",\n    \"E114\",\n    \"E117\",\n    \"D206\",\n    \"D300\",\n    \"Q000\",\n    \"Q001\",\n    \"Q002\",\n    \"Q003\",\n    \"COM812\",\n    \"COM819\",\n    \"E501\", # Formatted code may exceed the line length, leading to line-too-long (E501) errors.\n    \"ASYNC110\", # TODO: Use `anyio.Event` instead of awaiting `anyio.sleep` in a `while` loop\n]\nunfixable = [\n    # PT022 replace empty `yield` to empty `return`. Might be fixed with a combination of PLR1711\n    # In addition, it can't do anything with invalid typing annotations, protected by mypy.\n    \"PT022\",\n]\n\n[tool.ruff.format]\ndocstring-code-format = true\n\n[tool.ruff.lint.isort]\nrequired-imports = [\"from __future__ import annotations\"]\ncombine-as-imports = true\n\n\nsection-order = [\n  \"future\",\n  \"standard-library\",\n  \"third-party\",\n  \"first-party\",\n  \"local-folder\",\n  \"testing\"\n]\n\n# Make sure we put the \"dev\" imports at the end, not as a third-party module\n[tool.ruff.lint.isort.sections]\ntesting = [\"dev\", \"providers.tests\", \"task_sdk.tests\", \"tests_common\", \"tests\"]\n\n[tool.ruff.lint.extend-per-file-ignores]\n\"airflow/__init__.py\" = [\"F401\", \"TC004\", \"I002\"]\n\"airflow/models/__init__.py\" = [\"F401\", \"TC004\"]\n\"airflow/models/sqla_models.py\" = [\"F401\"]\n\"providers/src/airflow/providers/__init__.py\" = [\"I002\"]\n\"providers/src/airflow/__init__.py\" = [\"I002\"]\n\n# The test_python.py is needed because adding __future__.annotations breaks runtime checks that are\n# needed for the test to work\n\"tests/decorators/test_python.py\" = [\"I002\"]\n\n# The Pydantic representations of SqlAlchemy Models are not parsed well with Pydantic\n# when __future__.annotations is used so we need to skip them from upgrading\n# Pydantic also require models to be imported during execution\n\"airflow/serialization/pydantic/*.py\" = [\"I002\", \"UP007\", \"TC001\"]\n\n# Failing to detect types and functions used in `Annotated[...]` syntax as required at runtime.\n# Annotated is central for FastAPI dependency injection, skipping rules for FastAPI folders.\n\"airflow/api_fastapi/*\" = [\"TC001\", \"TC002\"]\n\"tests/api_fastapi/*\" = [\"T001\", \"TC002\"]\n\n# Ignore pydoc style from these\n\"*.pyi\" = [\"D\"]\n\"scripts/*\" = [\"D\", \"PT\"]  # In addition ignore pytest specific rules\n\"docs/*\" = [\"D\"]\n\"provider_packages/*\" = [\"D\"]\n\"*/example_dags/*\" = [\"D\"]\n\"chart/*\" = [\"D\"]\n\"dev/*\" = [\"D\"]\n# In addition, ignore in tests\n# TID253: Banned top level imports, e.g. pandas, numpy\n# S101: Use `assert`\n# TRY002: Use `raise Exception`\n\"dev/perf/*\" = [\"TID253\"]\n\"dev/check_files.py\" = [\"S101\"]\n\"tests_common/*\" = [\"S101\", \"TRY002\"]\n\"dev/breeze/tests/*\" = [\"TID253\", \"S101\", \"TRY002\"]\n\"tests/*\" = [\"D\", \"TID253\", \"S101\", \"TRY002\"]\n\"docker_tests/*\" = [\"D\", \"TID253\", \"S101\", \"TRY002\"]\n\"kubernetes_tests/*\" = [\"D\", \"TID253\", \"S101\", \"TRY002\"]\n\"helm_tests/*\" = [\"D\", \"TID253\", \"S101\", \"TRY002\"]\n\n# All of the modules which have an extra license header (i.e. that we copy from another project) need to\n# ignore E402 -- module level import not at top level\n\"scripts/ci/pre_commit/*.py\" = [\"E402\"]\n\"airflow/api/auth/backend/kerberos_auth.py\" = [\"E402\"]\n\"airflow/security/kerberos.py\" = [\"E402\"]\n\"airflow/security/utils.py\" = [\"E402\"]\n\n# All the modules which do not follow B028 yet: https://docs.astral.sh/ruff/rules/no-explicit-stacklevel/\n\"helm_tests/airflow_aux/test_basic_helm_chart.py\" = [\"B028\"]\n\n# Test compat imports banned imports to allow testing against older airflow versions\n\"tests_common/test_utils/compat.py\" = [\"TID251\", \"F401\"]\n\"tests_common/pytest_plugin.py\" = [\"F811\"]\n\n[tool.ruff.lint.flake8-tidy-imports]\n# Disallow all relative imports.\nban-relative-imports = \"all\"\n# Ban certain modules from being imported at module level, instead requiring\n# that they're imported lazily (e.g., within a function definition).\nbanned-module-level-imports = [\"numpy\", \"pandas\"]\n\n[tool.ruff.lint.flake8-tidy-imports.banned-api]\n# Direct import from the airflow package modules and constraints\n\"airflow.AirflowException\".msg = \"Use airflow.exceptions.AirflowException instead.\"\n\"airflow.Dataset\".msg = \"Use airflow.datasets.Dataset instead.\"\n# Deprecated imports\n\"airflow.models.baseoperator.BaseOperatorLink\".msg = \"Use airflow.models.baseoperatorlink.BaseOperatorLink\"\n\"airflow.models.errors.ImportError\".msg = \"Use airflow.models.errors.ParseImportError\"\n\"airflow.models.ImportError\".msg = \"Use airflow.models.errors.ParseImportError\"\n# Deprecated in Python 3.11, Pending Removal in Python 3.15: https://github.com/python/cpython/issues/90817\n# Deprecation warning in Python 3.11 also recommends using locale.getencoding but it available in Python 3.11\n\"locale.getdefaultlocale\".msg = \"Use locale.setlocale() and locale.getlocale() instead.\"\n# Deprecated in Python 3.12: https://github.com/python/cpython/issues/103857\n\"datetime.datetime.utcnow\".msg = \"Use airflow.utils.timezone.utcnow or datetime.datetime.now(tz=datetime.timezone.utc)\"\n\"datetime.datetime.utcfromtimestamp\".msg = \"Use airflow.utils.timezone.from_timestamp or datetime.datetime.fromtimestamp(tz=datetime.timezone.utc)\"\n# Deprecated in Python 3.12: https://github.com/python/cpython/issues/94309\n\"typing.Hashable\".msg = \"Use collections.abc.Hashable\"\n\"typing.Sized\".msg = \"Use collections.abc.Sized\"\n# Uses deprecated in Python 3.12 `datetime.datetime.utcfromtimestamp`\n\"pendulum.from_timestamp\".msg = \"Use airflow.utils.timezone.from_timestamp\"\n# Flask deprecations, worthwhile to keep it until we migrate to Flask 3.0+\n\"flask._app_ctx_stack\".msg = \"Deprecated in Flask 2.2, removed in Flask 3.0\"\n\"flask._request_ctx_stack\".msg = \"Deprecated in Flask 2.2, removed in Flask 3.0\"\n\"flask.escape\".msg = \"Use markupsafe.escape instead. Deprecated in Flask 2.3, removed in Flask 3.0\"\n\"flask.Markup\".msg = \"Use markupsafe.Markup instead. Deprecated in Flask 2.3, removed in Flask 3.0\"\n\"flask.signals_available\".msg = \"Signals are always available. Deprecated in Flask 2.3, removed in Flask 3.0\"\n# Use root logger by a mistake / IDE autosuggestion\n# If for some reason root logger required it could obtained by logging.getLogger(\"root\")\n\"logging.debug\".msg = \"Instantiate new `logger = logging.getLogger(__name__)` and use it instead of root logger\"\n\"logging.info\".msg = \"Instantiate new `logger = logging.getLogger(__name__)` and use it instead of root logger\"\n\"logging.warning\".msg = \"Instantiate new `logger = logging.getLogger(__name__)` and use it instead of root logger\"\n\"logging.error\".msg = \"Instantiate new `logger = logging.getLogger(__name__)` and use it instead of root logger\"\n\"logging.exception\".msg = \"Instantiate new `logger = logging.getLogger(__name__)` and use it instead of root logger\"\n\"logging.fatal\".msg = \"Instantiate new `logger = logging.getLogger(__name__)` and use it instead of root logger\"\n\"logging.critical\".msg = \"Instantiate new `logger = logging.getLogger(__name__)` and use it instead of root logger\"\n\"logging.log\".msg = \"Instantiate new `logger = logging.getLogger(__name__)` and use it instead of root logger\"\n# unittest related restrictions\n\"unittest.TestCase\".msg = \"Use pytest compatible classes: https://docs.pytest.org/en/stable/getting-started.html#group-multiple-tests-in-a-class\"\n\"unittest.skip\".msg = \"Use `pytest.mark.skip` instead: https://docs.pytest.org/en/stable/reference/reference.html#marks\"\n\"unittest.skipIf\".msg = \"Use `pytest.mark.skipif` instead: https://docs.pytest.org/en/stable/reference/reference.html#marks\"\n\"unittest.skipUnless\".msg = \"Use `pytest.mark.skipif` instead: https://docs.pytest.org/en/stable/reference/reference.html#marks\"\n\"unittest.expectedFailure\".msg = \"Use `pytest.mark.xfail` instead: https://docs.pytest.org/en/stable/reference/reference.html#marks\"\n# Moved in SQLAlchemy 2.0\n\"sqlalchemy.ext.declarative.declarative_base\".msg = \"Use `sqlalchemy.orm.declarative_base`. Moved in SQLAlchemy 2.0\"\n\"sqlalchemy.ext.declarative.as_declarative\".msg = \"Use `sqlalchemy.orm.as_declarative`. Moved in SQLAlchemy 2.0\"\n\"sqlalchemy.ext.declarative.has_inherited_table\".msg = \"Use `sqlalchemy.orm.has_inherited_table`. Moved in SQLAlchemy 2.0\"\n\"sqlalchemy.ext.declarative.synonym_for\".msg = \"Use `sqlalchemy.orm.synonym_for`. Moved in SQLAlchemy 2.0\"\n\n[tool.ruff.lint.flake8-type-checking]\nexempt-modules = [\"typing\", \"typing_extensions\"]\n\n[tool.ruff.lint.flake8-pytest-style]\nmark-parentheses = false\nfixture-parentheses = false\n\n## pytest settings ##\n[tool.pytest.ini_options]\naddopts = [\n    \"--tb=short\",\n    \"-rasl\",\n    \"--verbosity=2\",\n    # Disable `flaky` plugin for pytest. This plugin conflicts with `rerunfailures` because provide the same marker.\n    \"-p\", \"no:flaky\",\n    # Disable `nose` builtin plugin for pytest. This feature is deprecated in 7.2 and will be removed in pytest>=8\n    \"-p\", \"no:nose\",\n    # Disable support of a legacy `LocalPath` in favor of stdlib `pathlib.Path`.\n    \"-p\", \"no:legacypath\",\n    # Disable warnings summary, because we use our warning summary.\n    \"--disable-warnings\",\n    \"--asyncio-mode=strict\",\n]\nnorecursedirs = [\n    \".eggs\",\n    \"airflow\",\n    \"tests/_internals\",\n    \"tests/dags_with_system_exit\",\n    \"tests/dags_corrupted\",\n    \"tests/dags\",\n    \"providers/tests/system/google/cloud/dataproc/resources\",\n    \"providers/tests/system/google/cloud/gcs/resources\",\n]\nlog_level = \"INFO\"\nfilterwarnings = [\n    \"error::pytest.PytestCollectionWarning\",\n    \"error::pytest.PytestReturnNotNoneWarning\",\n    # Avoid building cartesian product which might impact performance\n    \"error:SELECT statement has a cartesian product between FROM:sqlalchemy.exc.SAWarning:airflow\",\n    'error:Coercing Subquery object into a select\\(\\) for use in IN\\(\\):sqlalchemy.exc.SAWarning:airflow',\n    'error:Class.*will not make use of SQL compilation caching',\n    \"ignore::DeprecationWarning:flask_appbuilder.filemanager\",\n    \"ignore::DeprecationWarning:flask_appbuilder.widgets\",\n    # FAB do not support SQLAclhemy 2\n    \"ignore::sqlalchemy.exc.MovedIn20Warning:flask_appbuilder\",\n    # https://github.com/dpgaspar/Flask-AppBuilder/issues/2194\n    \"ignore::DeprecationWarning:marshmallow_sqlalchemy.convert\",\n    # https://github.com/dpgaspar/Flask-AppBuilder/pull/1940\n    \"ignore::DeprecationWarning:flask_sqlalchemy\",\n    # https://github.com/dpgaspar/Flask-AppBuilder/pull/1903\n    \"ignore::DeprecationWarning:apispec.utils\",\n    # Connexion 2 use different deprecated objects, this should be resolved into Connexion 3\n    # https://github.com/spec-first/connexion/pull/1536\n    'ignore::DeprecationWarning:connexion.spec',\n    'ignore:jsonschema\\.RefResolver:DeprecationWarning:connexion.json_schema',\n    'ignore:jsonschema\\.exceptions\\.RefResolutionError:DeprecationWarning:connexion.json_schema',\n    'ignore:Accessing jsonschema\\.draft4_format_checker:DeprecationWarning:connexion.decorators.validation',\n]\n# We cannot add warnings from the airflow package into `filterwarnings`,\n# because it invokes import airflow before we set up test environment which breaks the tests.\n# Instead of that, we use a separate parameter and dynamically add it into `filterwarnings` marker.\nforbidden_warnings = [\n    \"airflow.exceptions.RemovedInAirflow3Warning\",\n    \"airflow.utils.context.AirflowContextDeprecationWarning\",\n    \"airflow.exceptions.AirflowProviderDeprecationWarning\",\n]\npython_files = [\n    \"test_*.py\",\n    \"example_*.py\",\n]\ntestpaths = [\n    \"tests\",\n]\n\nasyncio_default_fixture_loop_scope = \"function\"\n\n# Keep temporary directories (created by `tmp_path`) for 2 recent runs only failed tests.\ntmp_path_retention_count = \"2\"\ntmp_path_retention_policy = \"failed\"\n\n\n## coverage.py settings ##\n[tool.coverage.run]\nbranch = true\nrelative_files = true\nsource = [\"airflow\"]\nomit = [\n    \"airflow/_vendor/**\",\n    \"airflow/contrib/**\",\n    \"airflow/example_dags/**\",\n    \"airflow/migrations/**\",\n    \"airflow/www/node_modules/**\",\n    \"providers/src/airflow/providers/**/example_dags/**\",\n    \"providers/src/airflow/providers/google/ads/_vendor/**\",\n]\n\n[tool.coverage.report]\nskip_empty = true\nexclude_also = [\n    \"def __repr__\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n    \"if __name__ == .__main__.:\",\n    \"@(abc\\\\.)?abstractmethod\",\n    \"@(typing(_extensions)?\\\\.)?overload\",\n    \"if (typing(_extensions)?\\\\.)?TYPE_CHECKING:\"\n]\n\n\n## mypy settings ##\n[tool.mypy]\nignore_missing_imports = true\nno_implicit_optional = true\nwarn_redundant_casts = true\nwarn_unused_ignores = false\nplugins = [\n    \"dev/mypy/plugin/decorators.py\",\n    \"dev/mypy/plugin/outputs.py\",\n]\npretty = true\nshow_error_codes = true\ndisable_error_code = [\n    \"annotation-unchecked\",\n]\n# Since there are no __init__.py files in\n# providers/src/apache/airflow/providers we need to tell MyPy where the \"base\"\n# is, otherwise when it sees\n# providers/src/apache/airflow/providers/redis/__init__.py, it thinks this is\n# the toplevel `redis` folder.\nexplicit_package_bases = true\nmypy_path = [\n  \"$MYPY_CONFIG_FILE_DIR\",\n  \"$MYPY_CONFIG_FILE_DIR/providers/src\",\n  \"$MYPY_CONFIG_FILE_DIR/task_sdk/src\",\n]\n\n[[tool.mypy.overrides]]\nmodule=\"airflow.config_templates.default_webserver_config\"\ndisable_error_code = [\n    \"var-annotated\",\n]\n\n[[tool.mypy.overrides]]\nmodule=\"airflow.migrations.*\"\nignore_errors = true\n\n[[tool.mypy.overrides]]\nmodule=\"airflow.*._vendor.*\"\nignore_errors = true\n\n[[tool.mypy.overrides]]\nmodule= [\n    \"google.cloud.*\",\n    \"azure.*\",\n]\nno_implicit_optional = false\n\n[[tool.mypy.overrides]]\nmodule=[\n    \"referencing.*\",\n    # Beam has some old type annotations, and they introduced an error recently with bad signature of\n    # a function. This is captured in https://github.com/apache/beam/issues/29927\n    # and we should remove this exclusion when it is fixed.\n    \"apache_beam.*\"\n]\nignore_errors = true\n\n[tool.uv]\ndev-dependencies = [\n  \"local-providers\",\n  \"apache-airflow-task-sdk\"\n]\n\n[tool.uv.sources]\n# These names must match the names as defined in the pyproject.toml of the workspace items,\n# *not* the workspace folder paths\nlocal-providers = { workspace = true }\napache-airflow-task-sdk = { workspace = true }\n\n[tool.uv.workspace]\nmembers = [\"providers\", \"task_sdk\"]\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "task_sdk",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests_common",
          "type": "tree",
          "content": null
        },
        {
          "name": "yamllint-config.yml",
          "type": "blob",
          "size": 0.8662109375,
          "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n---\n# https://yamllint.readthedocs.io/en/stable/\nextends: default\n\nrules:\n  line-length:\n    max: 110\n"
        }
      ]
    }
  ]
}