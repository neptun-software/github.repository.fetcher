{
  "metadata": {
    "timestamp": 1736561432142,
    "page": 80,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "xinntao/Real-ESRGAN",
      "stars": 29184,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.8564453125,
          "content": "# ignored folders\ndatasets/*\nexperiments/*\nresults/*\ntb_logger/*\nwandb/*\ntmp/*\nweights/*\n\nversion.py\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.4482421875,
          "content": "repos:\n  # flake8\n  - repo: https://github.com/PyCQA/flake8\n    rev: 3.8.3\n    hooks:\n      - id: flake8\n        args: [\"--config=setup.cfg\", \"--ignore=W504, W503\"]\n\n  # modify known_third_party\n  - repo: https://github.com/asottile/seed-isort-config\n    rev: v2.2.0\n    hooks:\n      - id: seed-isort-config\n\n  # isort\n  - repo: https://github.com/timothycrosley/isort\n    rev: 5.2.2\n    hooks:\n      - id: isort\n\n  # yapf\n  - repo: https://github.com/pre-commit/mirrors-yapf\n    rev: v0.30.0\n    hooks:\n      - id: yapf\n\n  # codespell\n  - repo: https://github.com/codespell-project/codespell\n    rev: v2.1.0\n    hooks:\n      - id: codespell\n\n  # pre-commit-hooks\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v3.2.0\n    hooks:\n      - id: trailing-whitespace  # Trim trailing whitespace\n      - id: check-yaml  # Attempt to load all yaml files to verify syntax\n      - id: check-merge-conflict  # Check for files that contain merge conflict strings\n      - id: double-quote-string-fixer  # Replace double quoted strings with single quoted strings\n      - id: end-of-file-fixer  # Make sure files end in a newline and only a newline\n      - id: requirements-txt-fixer  # Sort entries in requirements.txt and remove incorrect entry for pkg-resources==0.0.0\n      - id: fix-encoding-pragma  # Remove the coding pragma: # -*- coding: utf-8 -*-\n        args: [\"--remove\"]\n      - id: mixed-line-ending  # Replace or check mixed line ending\n        args: [\"--fix=lf\"]\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.1279296875,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nxintao.wang@outlook.com or xintaowang@tencent.com.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior,  harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.4833984375,
          "content": "BSD 3-Clause License\n\nCopyright (c) 2021, Xintao Wang\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\n   contributors may be used to endorse or promote products derived from\n   this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.166015625,
          "content": "include assets/*\ninclude inputs/*\ninclude scripts/*.py\ninclude inference_realesrgan.py\ninclude VERSION\ninclude LICENSE\ninclude requirements.txt\ninclude weights/README.md\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.16015625,
          "content": "<p align=\"center\">\n  <img src=\"assets/realesrgan_logo.png\" height=120>\n</p>\n\n## <div align=\"center\"><b><a href=\"README.md\">English</a> | <a href=\"README_CN.md\">ç®€ä½“ä¸­æ–‡</a></b></div>\n\n<div align=\"center\">\n\nğŸ‘€[**Demos**](#-demos-videos) **|** ğŸš©[**Updates**](#-updates) **|** âš¡[**Usage**](#-quick-inference) **|** ğŸ°[**Model Zoo**](docs/model_zoo.md) **|** ğŸ”§[Install](#-dependencies-and-installation)  **|** ğŸ’»[Train](docs/Training.md) **|** â“[FAQ](docs/FAQ.md) **|** ğŸ¨[Contribution](docs/CONTRIBUTING.md)\n\n[![download](https://img.shields.io/github/downloads/xinntao/Real-ESRGAN/total.svg)](https://github.com/xinntao/Real-ESRGAN/releases)\n[![PyPI](https://img.shields.io/pypi/v/realesrgan)](https://pypi.org/project/realesrgan/)\n[![Open issue](https://img.shields.io/github/issues/xinntao/Real-ESRGAN)](https://github.com/xinntao/Real-ESRGAN/issues)\n[![Closed issue](https://img.shields.io/github/issues-closed/xinntao/Real-ESRGAN)](https://github.com/xinntao/Real-ESRGAN/issues)\n[![LICENSE](https://img.shields.io/github/license/xinntao/Real-ESRGAN.svg)](https://github.com/xinntao/Real-ESRGAN/blob/master/LICENSE)\n[![python lint](https://github.com/xinntao/Real-ESRGAN/actions/workflows/pylint.yml/badge.svg)](https://github.com/xinntao/Real-ESRGAN/blob/master/.github/workflows/pylint.yml)\n[![Publish-pip](https://github.com/xinntao/Real-ESRGAN/actions/workflows/publish-pip.yml/badge.svg)](https://github.com/xinntao/Real-ESRGAN/blob/master/.github/workflows/publish-pip.yml)\n\n</div>\n\nğŸ”¥ **AnimeVideo-v3 model (åŠ¨æ¼«è§†é¢‘å°æ¨¡å‹)**. Please see [[*anime video models*](docs/anime_video_model.md)] and [[*comparisons*](docs/anime_comparisons.md)]<br>\nğŸ”¥ **RealESRGAN_x4plus_anime_6B** for anime images **(åŠ¨æ¼«æ’å›¾æ¨¡å‹)**. Please see [[*anime_model*](docs/anime_model.md)]\n\n<!-- 1. You can try in our website: [ARC Demo](https://arc.tencent.com/en/ai-demos/imgRestore) (now only support RealESRGAN_x4plus_anime_6B) -->\n1. :boom: **Update** online Replicate demo: [![Replicate](https://img.shields.io/static/v1?label=Demo&message=Replicate&color=blue)](https://replicate.com/xinntao/realesrgan)\n1. Online Colab demo for Real-ESRGAN: [![Colab](https://img.shields.io/static/v1?label=Demo&message=Colab&color=orange)](https://colab.research.google.com/drive/1k2Zod6kSHEvraybHl50Lys0LerhyTMCo?usp=sharing) **|** Online Colab demo for for Real-ESRGAN (**anime videos**): [![Colab](https://img.shields.io/static/v1?label=Demo&message=Colab&color=orange)](https://colab.research.google.com/drive/1yNl9ORUxxlL4N0keJa2SEPB61imPQd1B?usp=sharing)\n1. Portable [Windows](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-windows.zip) / [Linux](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-ubuntu.zip) / [MacOS](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-macos.zip) **executable files for Intel/AMD/Nvidia GPU**. You can find more information [here](#portable-executable-files-ncnn). The ncnn implementation is in [Real-ESRGAN-ncnn-vulkan](https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan)\n<!-- 1. You can watch enhanced animations in [Tencent Video](https://v.qq.com/s/topic/v_child/render/fC4iyCAM.html). æ¬¢è¿è§‚çœ‹[è…¾è®¯è§†é¢‘åŠ¨æ¼«ä¿®å¤](https://v.qq.com/s/topic/v_child/render/fC4iyCAM.html) -->\n\nReal-ESRGAN aims at developing **Practical Algorithms for General Image/Video Restoration**.<br>\nWe extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data.\n\nğŸŒŒ Thanks for your valuable feedbacks/suggestions. All the feedbacks are updated in [feedback.md](docs/feedback.md).\n\n---\n\nIf Real-ESRGAN is helpful, please help to â­ this repo or recommend it to your friends ğŸ˜Š <br>\nOther recommended projects:<br>\nâ–¶ï¸ [GFPGAN](https://github.com/TencentARC/GFPGAN): A practical algorithm for real-world face restoration <br>\nâ–¶ï¸ [BasicSR](https://github.com/xinntao/BasicSR): An open-source image and video restoration toolbox<br>\nâ–¶ï¸ [facexlib](https://github.com/xinntao/facexlib): A collection that provides useful face-relation functions.<br>\nâ–¶ï¸ [HandyView](https://github.com/xinntao/HandyView): A PyQt5-based image viewer that is handy for view and comparison <br>\nâ–¶ï¸ [HandyFigure](https://github.com/xinntao/HandyFigure): Open source of paper figures <br>\n\n---\n\n### ğŸ“– Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data\n\n> [[Paper](https://arxiv.org/abs/2107.10833)] &emsp; [[YouTube Video](https://www.youtube.com/watch?v=fxHWoDSSvSc)] &emsp; [[Bç«™è®²è§£](https://www.bilibili.com/video/BV1H34y1m7sS/)] &emsp; [[Poster](https://xinntao.github.io/projects/RealESRGAN_src/RealESRGAN_poster.pdf)] &emsp; [[PPT slides](https://docs.google.com/presentation/d/1QtW6Iy8rm8rGLsJ0Ldti6kP-7Qyzy6XL/edit?usp=sharing&ouid=109799856763657548160&rtpof=true&sd=true)]<br>\n> [Xintao Wang](https://xinntao.github.io/), Liangbin Xie, [Chao Dong](https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ), [Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en) <br>\n> [Tencent ARC Lab](https://arc.tencent.com/en/ai-demos/imgRestore); Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences\n\n<p align=\"center\">\n  <img src=\"assets/teaser.jpg\">\n</p>\n\n---\n\n<!---------------------------------- Updates --------------------------->\n## ğŸš© Updates\n\n- âœ… Add the **realesr-general-x4v3** model - a tiny small model for general scenes. It also supports the **-dn** option to balance the noise (avoiding over-smooth results). **-dn** is short for denoising strength.\n- âœ… Update the **RealESRGAN AnimeVideo-v3** model. Please see [anime video models](docs/anime_video_model.md) and [comparisons](docs/anime_comparisons.md) for more details.\n- âœ… Add small models for anime videos. More details are in [anime video models](docs/anime_video_model.md).\n- âœ… Add the ncnn implementation [Real-ESRGAN-ncnn-vulkan](https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan).\n- âœ… Add [*RealESRGAN_x4plus_anime_6B.pth*](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth), which is optimized for **anime** images with much smaller model size. More details and comparisons with [waifu2x](https://github.com/nihui/waifu2x-ncnn-vulkan) are in [**anime_model.md**](docs/anime_model.md)\n- âœ… Support finetuning on your own data or paired data (*i.e.*, finetuning ESRGAN). See [here](docs/Training.md#Finetune-Real-ESRGAN-on-your-own-dataset)\n- âœ… Integrate [GFPGAN](https://github.com/TencentARC/GFPGAN) to support **face enhancement**.\n- âœ… Integrated to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/Real-ESRGAN). Thanks [@AK391](https://github.com/AK391)\n- âœ… Support arbitrary scale with `--outscale` (It actually further resizes outputs with `LANCZOS4`). Add *RealESRGAN_x2plus.pth* model.\n- âœ… [The inference code](inference_realesrgan.py) supports: 1) **tile** options; 2) images with **alpha channel**; 3) **gray** images; 4) **16-bit** images.\n- âœ… The training codes have been released. A detailed guide can be found in [Training.md](docs/Training.md).\n\n---\n\n<!---------------------------------- Demo videos --------------------------->\n## ğŸ‘€ Demos Videos\n\n#### Bilibili\n\n- [å¤§é—¹å¤©å®«ç‰‡æ®µ](https://www.bilibili.com/video/BV1ja41117zb)\n- [Anime dance cut åŠ¨æ¼«é­”æ€§èˆè¹ˆ](https://www.bilibili.com/video/BV1wY4y1L7hT/)\n- [æµ·è´¼ç‹ç‰‡æ®µ](https://www.bilibili.com/video/BV1i3411L7Gy/)\n\n#### YouTube\n\n## ğŸ”§ Dependencies and Installation\n\n- Python >= 3.7 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html))\n- [PyTorch >= 1.7](https://pytorch.org/)\n\n### Installation\n\n1. Clone repo\n\n    ```bash\n    git clone https://github.com/xinntao/Real-ESRGAN.git\n    cd Real-ESRGAN\n    ```\n\n1. Install dependent packages\n\n    ```bash\n    # Install basicsr - https://github.com/xinntao/BasicSR\n    # We use BasicSR for both training and inference\n    pip install basicsr\n    # facexlib and gfpgan are for face enhancement\n    pip install facexlib\n    pip install gfpgan\n    pip install -r requirements.txt\n    python setup.py develop\n    ```\n\n---\n\n## âš¡ Quick Inference\n\nThere are usually three ways to inference Real-ESRGAN.\n\n1. [Online inference](#online-inference)\n1. [Portable executable files (NCNN)](#portable-executable-files-ncnn)\n1. [Python script](#python-script)\n\n### Online inference\n\n1. You can try in our website: [ARC Demo](https://arc.tencent.com/en/ai-demos/imgRestore) (now only support RealESRGAN_x4plus_anime_6B)\n1. [Colab Demo](https://colab.research.google.com/drive/1k2Zod6kSHEvraybHl50Lys0LerhyTMCo?usp=sharing) for Real-ESRGAN **|** [Colab Demo](https://colab.research.google.com/drive/1yNl9ORUxxlL4N0keJa2SEPB61imPQd1B?usp=sharing) for Real-ESRGAN (**anime videos**).\n\n### Portable executable files (NCNN)\n\nYou can download [Windows](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-windows.zip) / [Linux](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-ubuntu.zip) / [MacOS](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-macos.zip) **executable files for Intel/AMD/Nvidia GPU**.\n\nThis executable file is **portable** and includes all the binaries and models required. No CUDA or PyTorch environment is needed.<br>\n\nYou can simply run the following command (the Windows example, more information is in the README.md of each executable files):\n\n```bash\n./realesrgan-ncnn-vulkan.exe -i input.jpg -o output.png -n model_name\n```\n\nWe have provided five models:\n\n1. realesrgan-x4plus  (default)\n2. realesrnet-x4plus\n3. realesrgan-x4plus-anime (optimized for anime images, small model size)\n4. realesr-animevideov3 (animation video)\n\nYou can use the `-n` argument for other models, for example, `./realesrgan-ncnn-vulkan.exe -i input.jpg -o output.png -n realesrnet-x4plus`\n\n#### Usage of portable executable files\n\n1. Please refer to [Real-ESRGAN-ncnn-vulkan](https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan#computer-usages) for more details.\n1. Note that it does not support all the functions (such as `outscale`) as the python script `inference_realesrgan.py`.\n\n```console\nUsage: realesrgan-ncnn-vulkan.exe -i infile -o outfile [options]...\n\n  -h                   show this help\n  -i input-path        input image path (jpg/png/webp) or directory\n  -o output-path       output image path (jpg/png/webp) or directory\n  -s scale             upscale ratio (can be 2, 3, 4. default=4)\n  -t tile-size         tile size (>=32/0=auto, default=0) can be 0,0,0 for multi-gpu\n  -m model-path        folder path to the pre-trained models. default=models\n  -n model-name        model name (default=realesr-animevideov3, can be realesr-animevideov3 | realesrgan-x4plus | realesrgan-x4plus-anime | realesrnet-x4plus)\n  -g gpu-id            gpu device to use (default=auto) can be 0,1,2 for multi-gpu\n  -j load:proc:save    thread count for load/proc/save (default=1:2:2) can be 1:2,2,2:2 for multi-gpu\n  -x                   enable tta mode\"\n  -f format            output image format (jpg/png/webp, default=ext/png)\n  -v                   verbose output\n```\n\nNote that it may introduce block inconsistency (and also generate slightly different results from the PyTorch implementation), because this executable file first crops the input image into several tiles, and then processes them separately, finally stitches together.\n\n### Python script\n\n#### Usage of python script\n\n1. You can use X4 model for **arbitrary output size** with the argument `outscale`. The program will further perform cheap resize operation after the Real-ESRGAN output.\n\n```console\nUsage: python inference_realesrgan.py -n RealESRGAN_x4plus -i infile -o outfile [options]...\n\nA common command: python inference_realesrgan.py -n RealESRGAN_x4plus -i infile --outscale 3.5 --face_enhance\n\n  -h                   show this help\n  -i --input           Input image or folder. Default: inputs\n  -o --output          Output folder. Default: results\n  -n --model_name      Model name. Default: RealESRGAN_x4plus\n  -s, --outscale       The final upsampling scale of the image. Default: 4\n  --suffix             Suffix of the restored image. Default: out\n  -t, --tile           Tile size, 0 for no tile during testing. Default: 0\n  --face_enhance       Whether to use GFPGAN to enhance face. Default: False\n  --fp32               Use fp32 precision during inference. Default: fp16 (half precision).\n  --ext                Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto\n```\n\n#### Inference general images\n\nDownload pre-trained models: [RealESRGAN_x4plus.pth](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth)\n\n```bash\nwget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P weights\n```\n\nInference!\n\n```bash\npython inference_realesrgan.py -n RealESRGAN_x4plus -i inputs --face_enhance\n```\n\nResults are in the `results` folder\n\n#### Inference anime images\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/xinntao/public-figures/master/Real-ESRGAN/cmp_realesrgan_anime_1.png\">\n</p>\n\nPre-trained models: [RealESRGAN_x4plus_anime_6B](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth)<br>\n More details and comparisons with [waifu2x](https://github.com/nihui/waifu2x-ncnn-vulkan) are in [**anime_model.md**](docs/anime_model.md)\n\n```bash\n# download model\nwget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P weights\n# inference\npython inference_realesrgan.py -n RealESRGAN_x4plus_anime_6B -i inputs\n```\n\nResults are in the `results` folder\n\n---\n\n## BibTeX\n\n    @InProceedings{wang2021realesrgan,\n        author    = {Xintao Wang and Liangbin Xie and Chao Dong and Ying Shan},\n        title     = {Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data},\n        booktitle = {International Conference on Computer Vision Workshops (ICCVW)},\n        date      = {2021}\n    }\n\n## ğŸ“§ Contact\n\nIf you have any question, please email `xintao.wang@outlook.com` or `xintaowang@tencent.com`.\n\n<!---------------------------------- Projects that use Real-ESRGAN --------------------------->\n## ğŸ§© Projects that use Real-ESRGAN\n\nIf you develop/use Real-ESRGAN in your projects, welcome to let me know.\n\n- NCNN-Android: [RealSR-NCNN-Android](https://github.com/tumuyan/RealSR-NCNN-Android) by [tumuyan](https://github.com/tumuyan)\n- VapourSynth: [vs-realesrgan](https://github.com/HolyWu/vs-realesrgan) by [HolyWu](https://github.com/HolyWu)\n- NCNN: [Real-ESRGAN-ncnn-vulkan](https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan)\n\n&nbsp;&nbsp;&nbsp;&nbsp;**GUI**\n\n- [Waifu2x-Extension-GUI](https://github.com/AaronFeng753/Waifu2x-Extension-GUI) by [AaronFeng753](https://github.com/AaronFeng753)\n- [Squirrel-RIFE](https://github.com/Justin62628/Squirrel-RIFE) by [Justin62628](https://github.com/Justin62628)\n- [Real-GUI](https://github.com/scifx/Real-GUI) by [scifx](https://github.com/scifx)\n- [Real-ESRGAN_GUI](https://github.com/net2cn/Real-ESRGAN_GUI) by [net2cn](https://github.com/net2cn)\n- [Real-ESRGAN-EGUI](https://github.com/WGzeyu/Real-ESRGAN-EGUI) by [WGzeyu](https://github.com/WGzeyu)\n- [anime_upscaler](https://github.com/shangar21/anime_upscaler) by [shangar21](https://github.com/shangar21)\n- [Upscayl](https://github.com/upscayl/upscayl) by [Nayam Amarshe](https://github.com/NayamAmarshe) and [TGS963](https://github.com/TGS963)\n\n## ğŸ¤— Acknowledgement\n\nThanks for all the contributors.\n\n- [AK391](https://github.com/AK391): Integrate RealESRGAN to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/Real-ESRGAN).\n- [Asiimoviet](https://github.com/Asiimoviet): Translate the README.md to Chinese (ä¸­æ–‡).\n- [2ji3150](https://github.com/2ji3150): Thanks for the [detailed and valuable feedbacks/suggestions](https://github.com/xinntao/Real-ESRGAN/issues/131).\n- [Jared-02](https://github.com/Jared-02): Translate the Training.md to Chinese (ä¸­æ–‡).\n"
        },
        {
          "name": "README_CN.md",
          "type": "blob",
          "size": 15.4091796875,
          "content": "<p align=\"center\">\n  <img src=\"assets/realesrgan_logo.png\" height=120>\n</p>\n\n## <div align=\"center\"><b><a href=\"README.md\">English</a> | <a href=\"README_CN.md\">ç®€ä½“ä¸­æ–‡</a></b></div>\n\n[![download](https://img.shields.io/github/downloads/xinntao/Real-ESRGAN/total.svg)](https://github.com/xinntao/Real-ESRGAN/releases)\n[![PyPI](https://img.shields.io/pypi/v/realesrgan)](https://pypi.org/project/realesrgan/)\n[![Open issue](https://img.shields.io/github/issues/xinntao/Real-ESRGAN)](https://github.com/xinntao/Real-ESRGAN/issues)\n[![Closed issue](https://img.shields.io/github/issues-closed/xinntao/Real-ESRGAN)](https://github.com/xinntao/Real-ESRGAN/issues)\n[![LICENSE](https://img.shields.io/github/license/xinntao/Real-ESRGAN.svg)](https://github.com/xinntao/Real-ESRGAN/blob/master/LICENSE)\n[![python lint](https://github.com/xinntao/Real-ESRGAN/actions/workflows/pylint.yml/badge.svg)](https://github.com/xinntao/Real-ESRGAN/blob/master/.github/workflows/pylint.yml)\n[![Publish-pip](https://github.com/xinntao/Real-ESRGAN/actions/workflows/publish-pip.yml/badge.svg)](https://github.com/xinntao/Real-ESRGAN/blob/master/.github/workflows/publish-pip.yml)\n\n:fire: æ›´æ–°åŠ¨æ¼«è§†é¢‘çš„å°æ¨¡å‹ **RealESRGAN AnimeVideo-v3**. æ›´å¤šä¿¡æ¯åœ¨ [[åŠ¨æ¼«è§†é¢‘æ¨¡å‹ä»‹ç»](docs/anime_video_model.md)] å’Œ [[æ¯”è¾ƒ](docs/anime_comparisons_CN.md)] ä¸­.\n\n1. Real-ESRGANçš„[Colab Demo](https://colab.research.google.com/drive/1k2Zod6kSHEvraybHl50Lys0LerhyTMCo?usp=sharing) | Real-ESRGAN**åŠ¨æ¼«è§†é¢‘** çš„[Colab Demo](https://colab.research.google.com/drive/1yNl9ORUxxlL4N0keJa2SEPB61imPQd1B?usp=sharing)\n2. **æ”¯æŒIntel/AMD/Nvidiaæ˜¾å¡**çš„ç»¿è‰²ç‰ˆexeæ–‡ä»¶ï¼š [Windowsç‰ˆ](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-windows.zip) / [Linuxç‰ˆ](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-ubuntu.zip) / [macOSç‰ˆ](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-macos.zip)ï¼Œè¯¦æƒ…è¯·ç§»æ­¥[è¿™é‡Œ](#ä¾¿æºç‰ˆï¼ˆç»¿è‰²ç‰ˆï¼‰å¯æ‰§è¡Œæ–‡ä»¶)ã€‚NCNNçš„å®ç°åœ¨ [Real-ESRGAN-ncnn-vulkan](https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan)ã€‚\n\nReal-ESRGAN çš„ç›®æ ‡æ˜¯å¼€å‘å‡º**å®ç”¨çš„å›¾åƒ/è§†é¢‘ä¿®å¤ç®—æ³•**ã€‚<br>\næˆ‘ä»¬åœ¨ ESRGAN çš„åŸºç¡€ä¸Šä½¿ç”¨çº¯åˆæˆçš„æ•°æ®æ¥è¿›è¡Œè®­ç»ƒï¼Œä»¥ä½¿å…¶èƒ½è¢«åº”ç”¨äºå®é™…çš„å›¾ç‰‡ä¿®å¤çš„åœºæ™¯ï¼ˆé¡¾åæ€ä¹‰ï¼šReal-ESRGANï¼‰ã€‚\n\n:art: Real-ESRGAN éœ€è¦ï¼Œä¹Ÿå¾ˆæ¬¢è¿ä½ çš„è´¡çŒ®ï¼Œå¦‚æ–°åŠŸèƒ½ã€æ¨¡å‹ã€bugä¿®å¤ã€å»ºè®®ã€ç»´æŠ¤ç­‰ç­‰ã€‚è¯¦æƒ…å¯ä»¥æŸ¥çœ‹[CONTRIBUTING.md](docs/CONTRIBUTING.md)ï¼Œæ‰€æœ‰çš„è´¡çŒ®è€…éƒ½ä¼šè¢«åˆ—åœ¨[æ­¤å¤„](README_CN.md#hugs-æ„Ÿè°¢)ã€‚\n\n:milky_way: æ„Ÿè°¢å¤§å®¶æä¾›äº†å¾ˆå¥½çš„åé¦ˆã€‚è¿™äº›åé¦ˆä¼šé€æ­¥æ›´æ–°åœ¨ [è¿™ä¸ªæ–‡æ¡£](docs/feedback.md)ã€‚\n\n:question: å¸¸è§çš„é—®é¢˜å¯ä»¥åœ¨[FAQ.md](docs/FAQ.md)ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚ï¼ˆå¥½å§ï¼Œç°åœ¨è¿˜æ˜¯ç©ºç™½çš„=-=||ï¼‰\n\n---\n\nå¦‚æœ Real-ESRGAN å¯¹ä½ æœ‰å¸®åŠ©ï¼Œå¯ä»¥ç»™æœ¬é¡¹ç›®ä¸€ä¸ª Star :star: ï¼Œæˆ–è€…æ¨èç»™ä½ çš„æœ‹å‹ä»¬ï¼Œè°¢è°¢ï¼:blush: <br/>\nå…¶ä»–æ¨èçš„é¡¹ç›®ï¼š<br/>\n:arrow_forward: [GFPGAN](https://github.com/TencentARC/GFPGAN): å®ç”¨çš„äººè„¸å¤åŸç®—æ³• <br>\n:arrow_forward: [BasicSR](https://github.com/xinntao/BasicSR): å¼€æºçš„å›¾åƒå’Œè§†é¢‘å·¥å…·ç®±<br>\n:arrow_forward: [facexlib](https://github.com/xinntao/facexlib): æä¾›ä¸äººè„¸ç›¸å…³çš„å·¥å…·ç®±<br>\n:arrow_forward: [HandyView](https://github.com/xinntao/HandyView): åŸºäºPyQt5çš„å›¾ç‰‡æŸ¥çœ‹å™¨ï¼Œæ–¹ä¾¿æŸ¥çœ‹ä»¥åŠæ¯”è¾ƒ <br>\n\n---\n\n<!---------------------------------- Updates --------------------------->\n<details>\n<summary>ğŸš©<b>æ›´æ–°</b></summary>\n\n- âœ… æ›´æ–°åŠ¨æ¼«è§†é¢‘çš„å°æ¨¡å‹ **RealESRGAN AnimeVideo-v3**. æ›´å¤šä¿¡æ¯åœ¨ [anime video models](docs/anime_video_model.md) å’Œ [comparisons](docs/anime_comparisons.md)ä¸­.\n- âœ… æ·»åŠ äº†é’ˆå¯¹åŠ¨æ¼«è§†é¢‘çš„å°æ¨¡å‹, æ›´å¤šä¿¡æ¯åœ¨ [anime video models](docs/anime_video_model.md) ä¸­.\n- âœ… æ·»åŠ äº†ncnn å®ç°ï¼š[Real-ESRGAN-ncnn-vulkan](https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan).\n- âœ… æ·»åŠ äº† [*RealESRGAN_x4plus_anime_6B.pth*](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth)ï¼Œå¯¹äºŒæ¬¡å…ƒå›¾ç‰‡è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¹¶å‡å°‘äº†modelçš„å¤§å°ã€‚è¯¦æƒ… ä»¥åŠ ä¸[waifu2x](https://github.com/nihui/waifu2x-ncnn-vulkan)çš„å¯¹æ¯”è¯·æŸ¥çœ‹[**anime_model.md**](docs/anime_model.md)\n- âœ…æ”¯æŒç”¨æˆ·åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒ (finetune)ï¼š[è¯¦æƒ…](docs/Training.md#Finetune-Real-ESRGAN-on-your-own-dataset)\n- âœ… æ”¯æŒä½¿ç”¨[GFPGAN](https://github.com/TencentARC/GFPGAN)**å¢å¼ºäººè„¸**\n- âœ… é€šè¿‡[Gradio](https://github.com/gradio-app/gradio)æ·»åŠ åˆ°äº†[Huggingface Spaces](https://huggingface.co/spaces)ï¼ˆä¸€ä¸ªæœºå™¨å­¦ä¹ åº”ç”¨çš„åœ¨çº¿å¹³å°ï¼‰ï¼š[Gradioåœ¨çº¿ç‰ˆ](https://huggingface.co/spaces/akhaliq/Real-ESRGAN)ã€‚æ„Ÿè°¢[@AK391](https://github.com/AK391)\n- âœ… æ”¯æŒä»»æ„æ¯”ä¾‹çš„ç¼©æ”¾ï¼š`--outscale`ï¼ˆå®é™…ä¸Šä½¿ç”¨`LANCZOS4`æ¥æ›´è¿›ä¸€æ­¥è°ƒæ•´è¾“å‡ºå›¾åƒçš„å°ºå¯¸ï¼‰ã€‚æ·»åŠ äº†*RealESRGAN_x2plus.pth*æ¨¡å‹\n- âœ… [æ¨æ–­è„šæœ¬](inference_realesrgan.py)æ”¯æŒ: 1) åˆ†å—å¤„ç†**tile**; 2) å¸¦**alphaé€šé“**çš„å›¾åƒ; 3) **ç°è‰²**å›¾åƒ; 4) **16-bit**å›¾åƒ.\n- âœ… è®­ç»ƒä»£ç å·²ç»å‘å¸ƒï¼Œå…·ä½“åšæ³•å¯æŸ¥çœ‹ï¼š[Training.md](docs/Training.md)ã€‚\n\n</details>\n\n<!---------------------------------- Projects that use Real-ESRGAN --------------------------->\n<details>\n<summary>ğŸ§©<b>ä½¿ç”¨Real-ESRGANçš„é¡¹ç›®</b></summary>\n\n&nbsp;&nbsp;&nbsp;&nbsp;ğŸ‘‹ å¦‚æœä½ å¼€å‘/ä½¿ç”¨/é›†æˆäº†Real-ESRGAN, æ¬¢è¿è”ç³»æˆ‘æ·»åŠ \n\n- NCNN-Android: [RealSR-NCNN-Android](https://github.com/tumuyan/RealSR-NCNN-Android) by [tumuyan](https://github.com/tumuyan)\n- VapourSynth: [vs-realesrgan](https://github.com/HolyWu/vs-realesrgan) by [HolyWu](https://github.com/HolyWu)\n- NCNN: [Real-ESRGAN-ncnn-vulkan](https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan)\n\n&nbsp;&nbsp;&nbsp;&nbsp;**æ˜“ç”¨çš„å›¾å½¢ç•Œé¢**\n\n- [Waifu2x-Extension-GUI](https://github.com/AaronFeng753/Waifu2x-Extension-GUI) by [AaronFeng753](https://github.com/AaronFeng753)\n- [Squirrel-RIFE](https://github.com/Justin62628/Squirrel-RIFE) by [Justin62628](https://github.com/Justin62628)\n- [Real-GUI](https://github.com/scifx/Real-GUI) by [scifx](https://github.com/scifx)\n- [Real-ESRGAN_GUI](https://github.com/net2cn/Real-ESRGAN_GUI) by [net2cn](https://github.com/net2cn)\n- [Real-ESRGAN-EGUI](https://github.com/WGzeyu/Real-ESRGAN-EGUI) by [WGzeyu](https://github.com/WGzeyu)\n- [anime_upscaler](https://github.com/shangar21/anime_upscaler) by [shangar21](https://github.com/shangar21)\n- [RealESRGAN-GUI](https://github.com/Baiyuetribe/paper2gui/blob/main/Video%20Super%20Resolution/RealESRGAN-GUI.md) by [Baiyuetribe](https://github.com/Baiyuetribe)\n\n</details>\n\n<details>\n<summary>ğŸ‘€<b>Demoè§†é¢‘ï¼ˆBç«™ï¼‰</b></summary>\n\n- [å¤§é—¹å¤©å®«ç‰‡æ®µ](https://www.bilibili.com/video/BV1ja41117zb)\n\n</details>\n\n### :book: Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data\n\n> [[è®ºæ–‡](https://arxiv.org/abs/2107.10833)] &emsp; [é¡¹ç›®ä¸»é¡µ] &emsp; [[YouTube è§†é¢‘](https://www.youtube.com/watch?v=fxHWoDSSvSc)] &emsp; [[Bç«™è§†é¢‘](https://www.bilibili.com/video/BV1H34y1m7sS/)] &emsp; [[Poster](https://xinntao.github.io/projects/RealESRGAN_src/RealESRGAN_poster.pdf)] &emsp; [[PPT](https://docs.google.com/presentation/d/1QtW6Iy8rm8rGLsJ0Ldti6kP-7Qyzy6XL/edit?usp=sharing&ouid=109799856763657548160&rtpof=true&sd=true)]<br>\n> [Xintao Wang](https://xinntao.github.io/), Liangbin Xie, [Chao Dong](https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ), [Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en) <br>\n> Tencent ARC Lab; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences\n\n<p align=\"center\">\n  <img src=\"assets/teaser.jpg\">\n</p>\n\n---\n\næˆ‘ä»¬æä¾›äº†ä¸€å¥—è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆ*RealESRGAN_x4plus.pth*)ï¼Œå¯ä»¥è¿›è¡Œ4å€çš„è¶…åˆ†è¾¨ç‡ã€‚<br>\n**ç°åœ¨çš„ Real-ESRGAN è¿˜æ˜¯æœ‰å‡ ç‡å¤±è´¥çš„ï¼Œå› ä¸ºç°å®ç”Ÿæ´»çš„é™è´¨è¿‡ç¨‹æ¯”è¾ƒå¤æ‚ã€‚**<br>\nè€Œä¸”ï¼Œæœ¬é¡¹ç›®å¯¹**äººè„¸ä»¥åŠæ–‡å­—ä¹‹ç±»**çš„æ•ˆæœè¿˜ä¸æ˜¯å¤ªå¥½ï¼Œä½†æ˜¯æˆ‘ä»¬ä¼šæŒç»­è¿›è¡Œä¼˜åŒ–çš„ã€‚<br>\n\nReal-ESRGAN å°†ä¼šè¢«é•¿æœŸæ”¯æŒï¼Œæˆ‘ä¼šåœ¨ç©ºé—²çš„æ—¶é—´ä¸­æŒç»­ç»´æŠ¤æ›´æ–°ã€‚\n\nè¿™äº›æ˜¯æœªæ¥è®¡åˆ’çš„å‡ ä¸ªæ–°åŠŸèƒ½ï¼š\n\n- [ ] ä¼˜åŒ–äººè„¸\n- [ ] ä¼˜åŒ–æ–‡å­—\n- [x] ä¼˜åŒ–åŠ¨ç”»å›¾åƒ\n- [ ] æ”¯æŒæ›´å¤šçš„è¶…åˆ†è¾¨ç‡æ¯”ä¾‹\n- [ ] å¯è°ƒèŠ‚çš„å¤åŸ\n\nå¦‚æœä½ æœ‰å¥½ä¸»æ„æˆ–éœ€æ±‚ï¼Œæ¬¢è¿åœ¨ issue æˆ– discussion ä¸­æå‡ºã€‚<br/>\nå¦‚æœä½ æœ‰ä¸€äº› Real-ESRGAN ä¸­æœ‰é—®é¢˜çš„ç…§ç‰‡ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨ issue æˆ–è€… discussion ä¸­å‘å‡ºæ¥ã€‚æˆ‘ä¼šç•™æ„ï¼ˆä½†æ˜¯ä¸ä¸€å®šèƒ½è§£å†³:stuck_out_tongue:ï¼‰ã€‚å¦‚æœæœ‰å¿…è¦çš„è¯ï¼Œæˆ‘è¿˜ä¼šä¸“é—¨å¼€ä¸€é¡µæ¥è®°å½•é‚£äº›æœ‰å¾…è§£å†³çš„å›¾åƒã€‚\n\n---\n\n### ä¾¿æºç‰ˆï¼ˆç»¿è‰²ç‰ˆï¼‰å¯æ‰§è¡Œæ–‡ä»¶\n\nä½ å¯ä»¥ä¸‹è½½**æ”¯æŒIntel/AMD/Nvidiaæ˜¾å¡**çš„ç»¿è‰²ç‰ˆexeæ–‡ä»¶ï¼š [Windowsç‰ˆ](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-windows.zip) / [Linuxç‰ˆ](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-ubuntu.zip) / [macOSç‰ˆ](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesrgan-ncnn-vulkan-20220424-macos.zip)ã€‚\n\nç»¿è‰²ç‰ˆæŒ‡çš„æ˜¯è¿™äº›exeä½ å¯ä»¥ç›´æ¥è¿è¡Œï¼ˆæ”¾Uç›˜é‡Œæ‹·èµ°éƒ½æ²¡é—®é¢˜ï¼‰ï¼Œå› ä¸ºé‡Œé¢å·²ç»æœ‰æ‰€éœ€çš„æ–‡ä»¶å’Œæ¨¡å‹äº†ã€‚å®ƒä¸éœ€è¦ CUDA æˆ–è€… PyTorchè¿è¡Œç¯å¢ƒã€‚<br>\n\nä½ å¯ä»¥é€šè¿‡ä¸‹é¢è¿™ä¸ªå‘½ä»¤æ¥è¿è¡Œï¼ˆWindowsç‰ˆæœ¬çš„ä¾‹å­ï¼Œæ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹å¯¹åº”ç‰ˆæœ¬çš„README.mdï¼‰ï¼š\n\n```bash\n./realesrgan-ncnn-vulkan.exe -i è¾“å…¥å›¾åƒ.jpg -o è¾“å‡ºå›¾åƒ.png -n æ¨¡å‹åå­—\n```\n\næˆ‘ä»¬æä¾›äº†äº”ç§æ¨¡å‹ï¼š\n\n1. realesrgan-x4plusï¼ˆé»˜è®¤ï¼‰\n2. reaesrnet-x4plus\n3. realesrgan-x4plus-animeï¼ˆé’ˆå¯¹åŠ¨æ¼«æ’ç”»å›¾åƒä¼˜åŒ–ï¼Œæœ‰æ›´å°çš„ä½“ç§¯ï¼‰\n4. realesr-animevideov3 (é’ˆå¯¹åŠ¨æ¼«è§†é¢‘)\n\nä½ å¯ä»¥é€šè¿‡`-n`å‚æ•°æ¥ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œä¾‹å¦‚`./realesrgan-ncnn-vulkan.exe -i äºŒæ¬¡å…ƒå›¾ç‰‡.jpg -o äºŒåˆºèˆå›¾ç‰‡.png -n realesrgan-x4plus-anime`\n\n### å¯æ‰§è¡Œæ–‡ä»¶çš„ç”¨æ³•\n\n1. æ›´å¤šç»†èŠ‚å¯ä»¥å‚è€ƒ [Real-ESRGAN-ncnn-vulkan](https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan#computer-usages).\n2. æ³¨æ„ï¼šå¯æ‰§è¡Œæ–‡ä»¶å¹¶æ²¡æœ‰æ”¯æŒ python è„šæœ¬ `inference_realesrgan.py` ä¸­æ‰€æœ‰çš„åŠŸèƒ½ï¼Œæ¯”å¦‚ `outscale` é€‰é¡¹) .\n\n```console\nUsage: realesrgan-ncnn-vulkan.exe -i infile -o outfile [options]...\n\n  -h                   show this help\n  -i input-path        input image path (jpg/png/webp) or directory\n  -o output-path       output image path (jpg/png/webp) or directory\n  -s scale             upscale ratio (can be 2, 3, 4. default=4)\n  -t tile-size         tile size (>=32/0=auto, default=0) can be 0,0,0 for multi-gpu\n  -m model-path        folder path to the pre-trained models. default=models\n  -n model-name        model name (default=realesr-animevideov3, can be realesr-animevideov3 | realesrgan-x4plus | realesrgan-x4plus-anime | realesrnet-x4plus)\n  -g gpu-id            gpu device to use (default=auto) can be 0,1,2 for multi-gpu\n  -j load:proc:save    thread count for load/proc/save (default=1:2:2) can be 1:2,2,2:2 for multi-gpu\n  -x                   enable tta mode\"\n  -f format            output image format (jpg/png/webp, default=ext/png)\n  -v                   verbose output\n```\n\nç”±äºè¿™äº›exeæ–‡ä»¶ä¼šæŠŠå›¾åƒåˆ†æˆå‡ ä¸ªæ¿å—ï¼Œç„¶åæ¥åˆ†åˆ«è¿›è¡Œå¤„ç†ï¼Œå†åˆæˆå¯¼å‡ºï¼Œè¾“å‡ºçš„å›¾åƒå¯èƒ½ä¼šæœ‰ä¸€ç‚¹å‰²è£‚æ„Ÿï¼ˆè€Œä¸”å¯èƒ½è·ŸPyTorchçš„è¾“å‡ºä¸å¤ªä¸€æ ·ï¼‰\n\n---\n\n## :wrench: ä¾èµ–ä»¥åŠå®‰è£…\n\n- Python >= 3.7 (æ¨èä½¿ç”¨[Anaconda](https://www.anaconda.com/download/#linux)æˆ–[Miniconda](https://docs.conda.io/en/latest/miniconda.html))\n- [PyTorch >= 1.7](https://pytorch.org/)\n\n#### å®‰è£…\n\n1. æŠŠé¡¹ç›®å…‹éš†åˆ°æœ¬åœ°\n\n    ```bash\n    git clone https://github.com/xinntao/Real-ESRGAN.git\n    cd Real-ESRGAN\n    ```\n\n2. å®‰è£…å„ç§ä¾èµ–\n\n    ```bash\n    # å®‰è£… basicsr - https://github.com/xinntao/BasicSR\n    # æˆ‘ä»¬ä½¿ç”¨BasicSRæ¥è®­ç»ƒä»¥åŠæ¨æ–­\n    pip install basicsr\n    # facexlibå’Œgfpganæ˜¯ç”¨æ¥å¢å¼ºäººè„¸çš„\n    pip install facexlib\n    pip install gfpgan\n    pip install -r requirements.txt\n    python setup.py develop\n    ```\n\n## :zap: å¿«é€Ÿä¸Šæ‰‹\n\n### æ™®é€šå›¾ç‰‡\n\nä¸‹è½½æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹: [RealESRGAN_x4plus.pth](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth)\n\n```bash\nwget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P weights\n```\n\næ¨æ–­!\n\n```bash\npython inference_realesrgan.py -n RealESRGAN_x4plus -i inputs --face_enhance\n```\n\nç»“æœåœ¨`results`æ–‡ä»¶å¤¹\n\n### åŠ¨ç”»å›¾ç‰‡\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/xinntao/public-figures/master/Real-ESRGAN/cmp_realesrgan_anime_1.png\">\n</p>\n\nè®­ç»ƒå¥½çš„æ¨¡å‹: [RealESRGAN_x4plus_anime_6B](https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth)<br>\næœ‰å…³[waifu2x](https://github.com/nihui/waifu2x-ncnn-vulkan)çš„æ›´å¤šä¿¡æ¯å’Œå¯¹æ¯”åœ¨[**anime_model.md**](docs/anime_model.md)ä¸­ã€‚\n\n```bash\n# ä¸‹è½½æ¨¡å‹\nwget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P weights\n# æ¨æ–­\npython inference_realesrgan.py -n RealESRGAN_x4plus_anime_6B -i inputs\n```\n\nç»“æœåœ¨`results`æ–‡ä»¶å¤¹\n\n### Python è„šæœ¬çš„ç”¨æ³•\n\n1. è™½ç„¶ä½ ä½¿ç”¨äº† X4 æ¨¡å‹ï¼Œä½†æ˜¯ä½ å¯ä»¥ **è¾“å‡ºä»»æ„å°ºå¯¸æ¯”ä¾‹çš„å›¾ç‰‡**ï¼Œåªè¦å®ç”¨äº† `outscale` å‚æ•°. ç¨‹åºä¼šè¿›ä¸€æ­¥å¯¹æ¨¡å‹çš„è¾“å‡ºå›¾åƒè¿›è¡Œç¼©æ”¾ã€‚\n\n```console\nUsage: python inference_realesrgan.py -n RealESRGAN_x4plus -i infile -o outfile [options]...\n\nA common command: python inference_realesrgan.py -n RealESRGAN_x4plus -i infile --outscale 3.5 --face_enhance\n\n  -h                   show this help\n  -i --input           Input image or folder. Default: inputs\n  -o --output          Output folder. Default: results\n  -n --model_name      Model name. Default: RealESRGAN_x4plus\n  -s, --outscale       The final upsampling scale of the image. Default: 4\n  --suffix             Suffix of the restored image. Default: out\n  -t, --tile           Tile size, 0 for no tile during testing. Default: 0\n  --face_enhance       Whether to use GFPGAN to enhance face. Default: False\n  --fp32               Whether to use half precision during inference. Default: False\n  --ext                Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto\n```\n\n## :european_castle: æ¨¡å‹åº“\n\nè¯·å‚è§ [docs/model_zoo.md](docs/model_zoo.md)\n\n## :computer: è®­ç»ƒï¼Œåœ¨ä½ çš„æ•°æ®ä¸Šå¾®è°ƒï¼ˆFine-tuneï¼‰\n\nè¿™é‡Œæœ‰ä¸€ä»½è¯¦ç»†çš„æŒ‡å—ï¼š[Training.md](docs/Training.md).\n\n## BibTeX å¼•ç”¨\n\n    @Article{wang2021realesrgan,\n        title={Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data},\n        author={Xintao Wang and Liangbin Xie and Chao Dong and Ying Shan},\n        journal={arXiv:2107.10833},\n        year={2021}\n    }\n\n## :e-mail: è”ç³»æˆ‘ä»¬\n\nå¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·é€šè¿‡ `xintao.wang@outlook.com` æˆ– `xintaowang@tencent.com` è”ç³»æˆ‘ä»¬ã€‚\n\n## :hugs: æ„Ÿè°¢\n\næ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…å¤§å¤§ä»¬~\n\n- [AK391](https://github.com/AK391): é€šè¿‡[Gradio](https://github.com/gradio-app/gradio)æ·»åŠ åˆ°äº†[Huggingface Spaces](https://huggingface.co/spaces)ï¼ˆä¸€ä¸ªæœºå™¨å­¦ä¹ åº”ç”¨çš„åœ¨çº¿å¹³å°ï¼‰ï¼š[Gradioåœ¨çº¿ç‰ˆ](https://huggingface.co/spaces/akhaliq/Real-ESRGAN)ã€‚\n- [Asiimoviet](https://github.com/Asiimoviet): æŠŠ README.md æ–‡æ¡£ ç¿»è¯‘æˆäº†ä¸­æ–‡ã€‚\n- [2ji3150](https://github.com/2ji3150): æ„Ÿè°¢è¯¦å°½å¹¶ä¸”å¯Œæœ‰ä»·å€¼çš„[åé¦ˆã€å»ºè®®](https://github.com/xinntao/Real-ESRGAN/issues/131).\n- [Jared-02](https://github.com/Jared-02): æŠŠ Training.md æ–‡æ¡£ ç¿»è¯‘æˆäº†ä¸­æ–‡ã€‚\n"
        },
        {
          "name": "VERSION",
          "type": "blob",
          "size": 0.005859375,
          "content": "0.3.0\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cog.yaml",
          "type": "blob",
          "size": 0.4736328125,
          "content": "# This file is used for constructing replicate env\nimage: \"r8.im/tencentarc/realesrgan\"\n\nbuild:\n  gpu: true\n  python_version: \"3.8\"\n  system_packages:\n    - \"libgl1-mesa-glx\"\n    - \"libglib2.0-0\"\n  python_packages:\n    - \"torch==1.7.1\"\n    - \"torchvision==0.8.2\"\n    - \"numpy==1.21.1\"\n    - \"lmdb==1.2.1\"\n    - \"opencv-python==4.5.3.56\"\n    - \"PyYAML==5.4.1\"\n    - \"tqdm==4.62.2\"\n    - \"yapf==0.31.0\"\n    - \"basicsr==1.4.2\"\n    - \"facexlib==0.2.5\"\n\npredict: \"cog_predict.py:Predictor\"\n"
        },
        {
          "name": "cog_predict.py",
          "type": "blob",
          "size": 6.53125,
          "content": "# flake8: noqa\n# This file is used for deploying replicate models\n# running: cog predict -i img=@inputs/00017_gray.png -i version='General - v3' -i scale=2 -i face_enhance=True -i tile=0\n# push: cog push r8.im/xinntao/realesrgan\n\nimport os\n\nos.system('pip install gfpgan')\nos.system('python setup.py develop')\n\nimport cv2\nimport shutil\nimport tempfile\nimport torch\nfrom basicsr.archs.rrdbnet_arch import RRDBNet\nfrom basicsr.archs.srvgg_arch import SRVGGNetCompact\n\nfrom realesrgan.utils import RealESRGANer\n\ntry:\n    from cog import BasePredictor, Input, Path\n    from gfpgan import GFPGANer\nexcept Exception:\n    print('please install cog and realesrgan package')\n\n\nclass Predictor(BasePredictor):\n\n    def setup(self):\n        os.makedirs('output', exist_ok=True)\n        # download weights\n        if not os.path.exists('weights/realesr-general-x4v3.pth'):\n            os.system(\n                'wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth -P ./weights'\n            )\n        if not os.path.exists('weights/GFPGANv1.4.pth'):\n            os.system('wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth -P ./weights')\n        if not os.path.exists('weights/RealESRGAN_x4plus.pth'):\n            os.system(\n                'wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P ./weights'\n            )\n        if not os.path.exists('weights/RealESRGAN_x4plus_anime_6B.pth'):\n            os.system(\n                'wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P ./weights'\n            )\n        if not os.path.exists('weights/realesr-animevideov3.pth'):\n            os.system(\n                'wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth -P ./weights'\n            )\n\n    def choose_model(self, scale, version, tile=0):\n        half = True if torch.cuda.is_available() else False\n        if version == 'General - RealESRGANplus':\n            model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n            model_path = 'weights/RealESRGAN_x4plus.pth'\n            self.upsampler = RealESRGANer(\n                scale=4, model_path=model_path, model=model, tile=tile, tile_pad=10, pre_pad=0, half=half)\n        elif version == 'General - v3':\n            model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')\n            model_path = 'weights/realesr-general-x4v3.pth'\n            self.upsampler = RealESRGANer(\n                scale=4, model_path=model_path, model=model, tile=tile, tile_pad=10, pre_pad=0, half=half)\n        elif version == 'Anime - anime6B':\n            model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)\n            model_path = 'weights/RealESRGAN_x4plus_anime_6B.pth'\n            self.upsampler = RealESRGANer(\n                scale=4, model_path=model_path, model=model, tile=tile, tile_pad=10, pre_pad=0, half=half)\n        elif version == 'AnimeVideo - v3':\n            model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu')\n            model_path = 'weights/realesr-animevideov3.pth'\n            self.upsampler = RealESRGANer(\n                scale=4, model_path=model_path, model=model, tile=tile, tile_pad=10, pre_pad=0, half=half)\n\n        self.face_enhancer = GFPGANer(\n            model_path='weights/GFPGANv1.4.pth',\n            upscale=scale,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=self.upsampler)\n\n    def predict(\n        self,\n        img: Path = Input(description='Input'),\n        version: str = Input(\n            description='RealESRGAN version. Please see [Readme] below for more descriptions',\n            choices=['General - RealESRGANplus', 'General - v3', 'Anime - anime6B', 'AnimeVideo - v3'],\n            default='General - v3'),\n        scale: float = Input(description='Rescaling factor', default=2),\n        face_enhance: bool = Input(\n            description='Enhance faces with GFPGAN. Note that it does not work for anime images/vidoes', default=False),\n        tile: int = Input(\n            description=\n            'Tile size. Default is 0, that is no tile. When encountering the out-of-GPU-memory issue, please specify it, e.g., 400 or 200',\n            default=0)\n    ) -> Path:\n        if tile <= 100 or tile is None:\n            tile = 0\n        print(f'img: {img}. version: {version}. scale: {scale}. face_enhance: {face_enhance}. tile: {tile}.')\n        try:\n            extension = os.path.splitext(os.path.basename(str(img)))[1]\n            img = cv2.imread(str(img), cv2.IMREAD_UNCHANGED)\n            if len(img.shape) == 3 and img.shape[2] == 4:\n                img_mode = 'RGBA'\n            elif len(img.shape) == 2:\n                img_mode = None\n                img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n            else:\n                img_mode = None\n\n            h, w = img.shape[0:2]\n            if h < 300:\n                img = cv2.resize(img, (w * 2, h * 2), interpolation=cv2.INTER_LANCZOS4)\n\n            self.choose_model(scale, version, tile)\n\n            try:\n                if face_enhance:\n                    _, _, output = self.face_enhancer.enhance(\n                        img, has_aligned=False, only_center_face=False, paste_back=True)\n                else:\n                    output, _ = self.upsampler.enhance(img, outscale=scale)\n            except RuntimeError as error:\n                print('Error', error)\n                print('If you encounter CUDA out of memory, try to set \"tile\" to a smaller size, e.g., 400.')\n\n            if img_mode == 'RGBA':  # RGBA images should be saved in png format\n                extension = 'png'\n            # save_path = f'output/out.{extension}'\n            # cv2.imwrite(save_path, output)\n            out_path = Path(tempfile.mkdtemp()) / f'out.{extension}'\n            cv2.imwrite(str(out_path), output)\n        except Exception as error:\n            print('global exception: ', error)\n        finally:\n            clean_folder('output')\n        return out_path\n\n\ndef clean_folder(folder):\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(f'Failed to delete {file_path}. Reason: {e}')\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "experiments",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference_realesrgan.py",
          "type": "blob",
          "size": 7.5634765625,
          "content": "import argparse\nimport cv2\nimport glob\nimport os\nfrom basicsr.archs.rrdbnet_arch import RRDBNet\nfrom basicsr.utils.download_util import load_file_from_url\n\nfrom realesrgan import RealESRGANer\nfrom realesrgan.archs.srvgg_arch import SRVGGNetCompact\n\n\ndef main():\n    \"\"\"Inference demo for Real-ESRGAN.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-i', '--input', type=str, default='inputs', help='Input image or folder')\n    parser.add_argument(\n        '-n',\n        '--model_name',\n        type=str,\n        default='RealESRGAN_x4plus',\n        help=('Model names: RealESRGAN_x4plus | RealESRNet_x4plus | RealESRGAN_x4plus_anime_6B | RealESRGAN_x2plus | '\n              'realesr-animevideov3 | realesr-general-x4v3'))\n    parser.add_argument('-o', '--output', type=str, default='results', help='Output folder')\n    parser.add_argument(\n        '-dn',\n        '--denoise_strength',\n        type=float,\n        default=0.5,\n        help=('Denoise strength. 0 for weak denoise (keep noise), 1 for strong denoise ability. '\n              'Only used for the realesr-general-x4v3 model'))\n    parser.add_argument('-s', '--outscale', type=float, default=4, help='The final upsampling scale of the image')\n    parser.add_argument(\n        '--model_path', type=str, default=None, help='[Option] Model path. Usually, you do not need to specify it')\n    parser.add_argument('--suffix', type=str, default='out', help='Suffix of the restored image')\n    parser.add_argument('-t', '--tile', type=int, default=0, help='Tile size, 0 for no tile during testing')\n    parser.add_argument('--tile_pad', type=int, default=10, help='Tile padding')\n    parser.add_argument('--pre_pad', type=int, default=0, help='Pre padding size at each border')\n    parser.add_argument('--face_enhance', action='store_true', help='Use GFPGAN to enhance face')\n    parser.add_argument(\n        '--fp32', action='store_true', help='Use fp32 precision during inference. Default: fp16 (half precision).')\n    parser.add_argument(\n        '--alpha_upsampler',\n        type=str,\n        default='realesrgan',\n        help='The upsampler for the alpha channels. Options: realesrgan | bicubic')\n    parser.add_argument(\n        '--ext',\n        type=str,\n        default='auto',\n        help='Image extension. Options: auto | jpg | png, auto means using the same extension as inputs')\n    parser.add_argument(\n        '-g', '--gpu-id', type=int, default=None, help='gpu device to use (default=None) can be 0,1,2 for multi-gpu')\n\n    args = parser.parse_args()\n\n    # determine models according to model names\n    args.model_name = args.model_name.split('.')[0]\n    if args.model_name == 'RealESRGAN_x4plus':  # x4 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth']\n    elif args.model_name == 'RealESRNet_x4plus':  # x4 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.1/RealESRNet_x4plus.pth']\n    elif args.model_name == 'RealESRGAN_x4plus_anime_6B':  # x4 RRDBNet model with 6 blocks\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth']\n    elif args.model_name == 'RealESRGAN_x2plus':  # x2 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n        netscale = 2\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth']\n    elif args.model_name == 'realesr-animevideov3':  # x4 VGG-style model (XS size)\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu')\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth']\n    elif args.model_name == 'realesr-general-x4v3':  # x4 VGG-style model (S size)\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')\n        netscale = 4\n        file_url = [\n            'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-wdn-x4v3.pth',\n            'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth'\n        ]\n\n    # determine model paths\n    if args.model_path is not None:\n        model_path = args.model_path\n    else:\n        model_path = os.path.join('weights', args.model_name + '.pth')\n        if not os.path.isfile(model_path):\n            ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n            for url in file_url:\n                # model_path will be updated\n                model_path = load_file_from_url(\n                    url=url, model_dir=os.path.join(ROOT_DIR, 'weights'), progress=True, file_name=None)\n\n    # use dni to control the denoise strength\n    dni_weight = None\n    if args.model_name == 'realesr-general-x4v3' and args.denoise_strength != 1:\n        wdn_model_path = model_path.replace('realesr-general-x4v3', 'realesr-general-wdn-x4v3')\n        model_path = [model_path, wdn_model_path]\n        dni_weight = [args.denoise_strength, 1 - args.denoise_strength]\n\n    # restorer\n    upsampler = RealESRGANer(\n        scale=netscale,\n        model_path=model_path,\n        dni_weight=dni_weight,\n        model=model,\n        tile=args.tile,\n        tile_pad=args.tile_pad,\n        pre_pad=args.pre_pad,\n        half=not args.fp32,\n        gpu_id=args.gpu_id)\n\n    if args.face_enhance:  # Use GFPGAN for face enhancement\n        from gfpgan import GFPGANer\n        face_enhancer = GFPGANer(\n            model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth',\n            upscale=args.outscale,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=upsampler)\n    os.makedirs(args.output, exist_ok=True)\n\n    if os.path.isfile(args.input):\n        paths = [args.input]\n    else:\n        paths = sorted(glob.glob(os.path.join(args.input, '*')))\n\n    for idx, path in enumerate(paths):\n        imgname, extension = os.path.splitext(os.path.basename(path))\n        print('Testing', idx, imgname)\n\n        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n        if len(img.shape) == 3 and img.shape[2] == 4:\n            img_mode = 'RGBA'\n        else:\n            img_mode = None\n\n        try:\n            if args.face_enhance:\n                _, _, output = face_enhancer.enhance(img, has_aligned=False, only_center_face=False, paste_back=True)\n            else:\n                output, _ = upsampler.enhance(img, outscale=args.outscale)\n        except RuntimeError as error:\n            print('Error', error)\n            print('If you encounter CUDA out of memory, try to set --tile with a smaller number.')\n        else:\n            if args.ext == 'auto':\n                extension = extension[1:]\n            else:\n                extension = args.ext\n            if img_mode == 'RGBA':  # RGBA images should be saved in png format\n                extension = 'png'\n            if args.suffix == '':\n                save_path = os.path.join(args.output, f'{imgname}.{extension}')\n            else:\n                save_path = os.path.join(args.output, f'{imgname}_{args.suffix}.{extension}')\n            cv2.imwrite(save_path, output)\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "inference_realesrgan_video.py",
          "type": "blob",
          "size": 16.513671875,
          "content": "import argparse\nimport cv2\nimport glob\nimport mimetypes\nimport numpy as np\nimport os\nimport shutil\nimport subprocess\nimport torch\nfrom basicsr.archs.rrdbnet_arch import RRDBNet\nfrom basicsr.utils.download_util import load_file_from_url\nfrom os import path as osp\nfrom tqdm import tqdm\n\nfrom realesrgan import RealESRGANer\nfrom realesrgan.archs.srvgg_arch import SRVGGNetCompact\n\ntry:\n    import ffmpeg\nexcept ImportError:\n    import pip\n    pip.main(['install', '--user', 'ffmpeg-python'])\n    import ffmpeg\n\n\ndef get_video_meta_info(video_path):\n    ret = {}\n    probe = ffmpeg.probe(video_path)\n    video_streams = [stream for stream in probe['streams'] if stream['codec_type'] == 'video']\n    has_audio = any(stream['codec_type'] == 'audio' for stream in probe['streams'])\n    ret['width'] = video_streams[0]['width']\n    ret['height'] = video_streams[0]['height']\n    ret['fps'] = eval(video_streams[0]['avg_frame_rate'])\n    ret['audio'] = ffmpeg.input(video_path).audio if has_audio else None\n    ret['nb_frames'] = int(video_streams[0]['nb_frames'])\n    return ret\n\n\ndef get_sub_video(args, num_process, process_idx):\n    if num_process == 1:\n        return args.input\n    meta = get_video_meta_info(args.input)\n    duration = int(meta['nb_frames'] / meta['fps'])\n    part_time = duration // num_process\n    print(f'duration: {duration}, part_time: {part_time}')\n    os.makedirs(osp.join(args.output, f'{args.video_name}_inp_tmp_videos'), exist_ok=True)\n    out_path = osp.join(args.output, f'{args.video_name}_inp_tmp_videos', f'{process_idx:03d}.mp4')\n    cmd = [\n        args.ffmpeg_bin, f'-i {args.input}', '-ss', f'{part_time * process_idx}',\n        f'-to {part_time * (process_idx + 1)}' if process_idx != num_process - 1 else '', '-async 1', out_path, '-y'\n    ]\n    print(' '.join(cmd))\n    subprocess.call(' '.join(cmd), shell=True)\n    return out_path\n\n\nclass Reader:\n\n    def __init__(self, args, total_workers=1, worker_idx=0):\n        self.args = args\n        input_type = mimetypes.guess_type(args.input)[0]\n        self.input_type = 'folder' if input_type is None else input_type\n        self.paths = []  # for image&folder type\n        self.audio = None\n        self.input_fps = None\n        if self.input_type.startswith('video'):\n            video_path = get_sub_video(args, total_workers, worker_idx)\n            self.stream_reader = (\n                ffmpeg.input(video_path).output('pipe:', format='rawvideo', pix_fmt='bgr24',\n                                                loglevel='error').run_async(\n                                                    pipe_stdin=True, pipe_stdout=True, cmd=args.ffmpeg_bin))\n            meta = get_video_meta_info(video_path)\n            self.width = meta['width']\n            self.height = meta['height']\n            self.input_fps = meta['fps']\n            self.audio = meta['audio']\n            self.nb_frames = meta['nb_frames']\n\n        else:\n            if self.input_type.startswith('image'):\n                self.paths = [args.input]\n            else:\n                paths = sorted(glob.glob(os.path.join(args.input, '*')))\n                tot_frames = len(paths)\n                num_frame_per_worker = tot_frames // total_workers + (1 if tot_frames % total_workers else 0)\n                self.paths = paths[num_frame_per_worker * worker_idx:num_frame_per_worker * (worker_idx + 1)]\n\n            self.nb_frames = len(self.paths)\n            assert self.nb_frames > 0, 'empty folder'\n            from PIL import Image\n            tmp_img = Image.open(self.paths[0])\n            self.width, self.height = tmp_img.size\n        self.idx = 0\n\n    def get_resolution(self):\n        return self.height, self.width\n\n    def get_fps(self):\n        if self.args.fps is not None:\n            return self.args.fps\n        elif self.input_fps is not None:\n            return self.input_fps\n        return 24\n\n    def get_audio(self):\n        return self.audio\n\n    def __len__(self):\n        return self.nb_frames\n\n    def get_frame_from_stream(self):\n        img_bytes = self.stream_reader.stdout.read(self.width * self.height * 3)  # 3 bytes for one pixel\n        if not img_bytes:\n            return None\n        img = np.frombuffer(img_bytes, np.uint8).reshape([self.height, self.width, 3])\n        return img\n\n    def get_frame_from_list(self):\n        if self.idx >= self.nb_frames:\n            return None\n        img = cv2.imread(self.paths[self.idx])\n        self.idx += 1\n        return img\n\n    def get_frame(self):\n        if self.input_type.startswith('video'):\n            return self.get_frame_from_stream()\n        else:\n            return self.get_frame_from_list()\n\n    def close(self):\n        if self.input_type.startswith('video'):\n            self.stream_reader.stdin.close()\n            self.stream_reader.wait()\n\n\nclass Writer:\n\n    def __init__(self, args, audio, height, width, video_save_path, fps):\n        out_width, out_height = int(width * args.outscale), int(height * args.outscale)\n        if out_height > 2160:\n            print('You are generating video that is larger than 4K, which will be very slow due to IO speed.',\n                  'We highly recommend to decrease the outscale(aka, -s).')\n\n        if audio is not None:\n            self.stream_writer = (\n                ffmpeg.input('pipe:', format='rawvideo', pix_fmt='bgr24', s=f'{out_width}x{out_height}',\n                             framerate=fps).output(\n                                 audio,\n                                 video_save_path,\n                                 pix_fmt='yuv420p',\n                                 vcodec='libx264',\n                                 loglevel='error',\n                                 acodec='copy').overwrite_output().run_async(\n                                     pipe_stdin=True, pipe_stdout=True, cmd=args.ffmpeg_bin))\n        else:\n            self.stream_writer = (\n                ffmpeg.input('pipe:', format='rawvideo', pix_fmt='bgr24', s=f'{out_width}x{out_height}',\n                             framerate=fps).output(\n                                 video_save_path, pix_fmt='yuv420p', vcodec='libx264',\n                                 loglevel='error').overwrite_output().run_async(\n                                     pipe_stdin=True, pipe_stdout=True, cmd=args.ffmpeg_bin))\n\n    def write_frame(self, frame):\n        frame = frame.astype(np.uint8).tobytes()\n        self.stream_writer.stdin.write(frame)\n\n    def close(self):\n        self.stream_writer.stdin.close()\n        self.stream_writer.wait()\n\n\ndef inference_video(args, video_save_path, device=None, total_workers=1, worker_idx=0):\n    # ---------------------- determine models according to model names ---------------------- #\n    args.model_name = args.model_name.split('.pth')[0]\n    if args.model_name == 'RealESRGAN_x4plus':  # x4 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth']\n    elif args.model_name == 'RealESRNet_x4plus':  # x4 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.1/RealESRNet_x4plus.pth']\n    elif args.model_name == 'RealESRGAN_x4plus_anime_6B':  # x4 RRDBNet model with 6 blocks\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth']\n    elif args.model_name == 'RealESRGAN_x2plus':  # x2 RRDBNet model\n        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n        netscale = 2\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth']\n    elif args.model_name == 'realesr-animevideov3':  # x4 VGG-style model (XS size)\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu')\n        netscale = 4\n        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth']\n    elif args.model_name == 'realesr-general-x4v3':  # x4 VGG-style model (S size)\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')\n        netscale = 4\n        file_url = [\n            'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-wdn-x4v3.pth',\n            'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth'\n        ]\n\n    # ---------------------- determine model paths ---------------------- #\n    model_path = os.path.join('weights', args.model_name + '.pth')\n    if not os.path.isfile(model_path):\n        ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n        for url in file_url:\n            # model_path will be updated\n            model_path = load_file_from_url(\n                url=url, model_dir=os.path.join(ROOT_DIR, 'weights'), progress=True, file_name=None)\n\n    # use dni to control the denoise strength\n    dni_weight = None\n    if args.model_name == 'realesr-general-x4v3' and args.denoise_strength != 1:\n        wdn_model_path = model_path.replace('realesr-general-x4v3', 'realesr-general-wdn-x4v3')\n        model_path = [model_path, wdn_model_path]\n        dni_weight = [args.denoise_strength, 1 - args.denoise_strength]\n\n    # restorer\n    upsampler = RealESRGANer(\n        scale=netscale,\n        model_path=model_path,\n        dni_weight=dni_weight,\n        model=model,\n        tile=args.tile,\n        tile_pad=args.tile_pad,\n        pre_pad=args.pre_pad,\n        half=not args.fp32,\n        device=device,\n    )\n\n    if 'anime' in args.model_name and args.face_enhance:\n        print('face_enhance is not supported in anime models, we turned this option off for you. '\n              'if you insist on turning it on, please manually comment the relevant lines of code.')\n        args.face_enhance = False\n\n    if args.face_enhance:  # Use GFPGAN for face enhancement\n        from gfpgan import GFPGANer\n        face_enhancer = GFPGANer(\n            model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth',\n            upscale=args.outscale,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=upsampler)  # TODO support custom device\n    else:\n        face_enhancer = None\n\n    reader = Reader(args, total_workers, worker_idx)\n    audio = reader.get_audio()\n    height, width = reader.get_resolution()\n    fps = reader.get_fps()\n    writer = Writer(args, audio, height, width, video_save_path, fps)\n\n    pbar = tqdm(total=len(reader), unit='frame', desc='inference')\n    while True:\n        img = reader.get_frame()\n        if img is None:\n            break\n\n        try:\n            if args.face_enhance:\n                _, _, output = face_enhancer.enhance(img, has_aligned=False, only_center_face=False, paste_back=True)\n            else:\n                output, _ = upsampler.enhance(img, outscale=args.outscale)\n        except RuntimeError as error:\n            print('Error', error)\n            print('If you encounter CUDA out of memory, try to set --tile with a smaller number.')\n        else:\n            writer.write_frame(output)\n\n        torch.cuda.synchronize(device)\n        pbar.update(1)\n\n    reader.close()\n    writer.close()\n\n\ndef run(args):\n    args.video_name = osp.splitext(os.path.basename(args.input))[0]\n    video_save_path = osp.join(args.output, f'{args.video_name}_{args.suffix}.mp4')\n\n    if args.extract_frame_first:\n        tmp_frames_folder = osp.join(args.output, f'{args.video_name}_inp_tmp_frames')\n        os.makedirs(tmp_frames_folder, exist_ok=True)\n        os.system(f'ffmpeg -i {args.input} -qscale:v 1 -qmin 1 -qmax 1 -vsync 0  {tmp_frames_folder}/frame%08d.png')\n        args.input = tmp_frames_folder\n\n    num_gpus = torch.cuda.device_count()\n    num_process = num_gpus * args.num_process_per_gpu\n    if num_process == 1:\n        inference_video(args, video_save_path)\n        return\n\n    ctx = torch.multiprocessing.get_context('spawn')\n    pool = ctx.Pool(num_process)\n    os.makedirs(osp.join(args.output, f'{args.video_name}_out_tmp_videos'), exist_ok=True)\n    pbar = tqdm(total=num_process, unit='sub_video', desc='inference')\n    for i in range(num_process):\n        sub_video_save_path = osp.join(args.output, f'{args.video_name}_out_tmp_videos', f'{i:03d}.mp4')\n        pool.apply_async(\n            inference_video,\n            args=(args, sub_video_save_path, torch.device(i % num_gpus), num_process, i),\n            callback=lambda arg: pbar.update(1))\n    pool.close()\n    pool.join()\n\n    # combine sub videos\n    # prepare vidlist.txt\n    with open(f'{args.output}/{args.video_name}_vidlist.txt', 'w') as f:\n        for i in range(num_process):\n            f.write(f'file \\'{args.video_name}_out_tmp_videos/{i:03d}.mp4\\'\\n')\n\n    cmd = [\n        args.ffmpeg_bin, '-f', 'concat', '-safe', '0', '-i', f'{args.output}/{args.video_name}_vidlist.txt', '-c',\n        'copy', f'{video_save_path}'\n    ]\n    print(' '.join(cmd))\n    subprocess.call(cmd)\n    shutil.rmtree(osp.join(args.output, f'{args.video_name}_out_tmp_videos'))\n    if osp.exists(osp.join(args.output, f'{args.video_name}_inp_tmp_videos')):\n        shutil.rmtree(osp.join(args.output, f'{args.video_name}_inp_tmp_videos'))\n    os.remove(f'{args.output}/{args.video_name}_vidlist.txt')\n\n\ndef main():\n    \"\"\"Inference demo for Real-ESRGAN.\n    It mainly for restoring anime videos.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-i', '--input', type=str, default='inputs', help='Input video, image or folder')\n    parser.add_argument(\n        '-n',\n        '--model_name',\n        type=str,\n        default='realesr-animevideov3',\n        help=('Model names: realesr-animevideov3 | RealESRGAN_x4plus_anime_6B | RealESRGAN_x4plus | RealESRNet_x4plus |'\n              ' RealESRGAN_x2plus | realesr-general-x4v3'\n              'Default:realesr-animevideov3'))\n    parser.add_argument('-o', '--output', type=str, default='results', help='Output folder')\n    parser.add_argument(\n        '-dn',\n        '--denoise_strength',\n        type=float,\n        default=0.5,\n        help=('Denoise strength. 0 for weak denoise (keep noise), 1 for strong denoise ability. '\n              'Only used for the realesr-general-x4v3 model'))\n    parser.add_argument('-s', '--outscale', type=float, default=4, help='The final upsampling scale of the image')\n    parser.add_argument('--suffix', type=str, default='out', help='Suffix of the restored video')\n    parser.add_argument('-t', '--tile', type=int, default=0, help='Tile size, 0 for no tile during testing')\n    parser.add_argument('--tile_pad', type=int, default=10, help='Tile padding')\n    parser.add_argument('--pre_pad', type=int, default=0, help='Pre padding size at each border')\n    parser.add_argument('--face_enhance', action='store_true', help='Use GFPGAN to enhance face')\n    parser.add_argument(\n        '--fp32', action='store_true', help='Use fp32 precision during inference. Default: fp16 (half precision).')\n    parser.add_argument('--fps', type=float, default=None, help='FPS of the output video')\n    parser.add_argument('--ffmpeg_bin', type=str, default='ffmpeg', help='The path to ffmpeg')\n    parser.add_argument('--extract_frame_first', action='store_true')\n    parser.add_argument('--num_process_per_gpu', type=int, default=1)\n\n    parser.add_argument(\n        '--alpha_upsampler',\n        type=str,\n        default='realesrgan',\n        help='The upsampler for the alpha channels. Options: realesrgan | bicubic')\n    parser.add_argument(\n        '--ext',\n        type=str,\n        default='auto',\n        help='Image extension. Options: auto | jpg | png, auto means using the same extension as inputs')\n    args = parser.parse_args()\n\n    args.input = args.input.rstrip('/').rstrip('\\\\')\n    os.makedirs(args.output, exist_ok=True)\n\n    if mimetypes.guess_type(args.input)[0] is not None and mimetypes.guess_type(args.input)[0].startswith('video'):\n        is_video = True\n    else:\n        is_video = False\n\n    if is_video and args.input.endswith('.flv'):\n        mp4_path = args.input.replace('.flv', '.mp4')\n        os.system(f'ffmpeg -i {args.input} -codec copy {mp4_path}')\n        args.input = mp4_path\n\n    if args.extract_frame_first and not is_video:\n        args.extract_frame_first = False\n\n    run(args)\n\n    if args.extract_frame_first:\n        tmp_frames_folder = osp.join(args.output, f'{args.video_name}_inp_tmp_frames')\n        shutil.rmtree(tmp_frames_folder)\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "inputs",
          "type": "tree",
          "content": null
        },
        {
          "name": "options",
          "type": "tree",
          "content": null
        },
        {
          "name": "realesrgan",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.09765625,
          "content": "basicsr>=1.4.2\nfacexlib>=0.2.5\ngfpgan>=1.3.5\nnumpy\nopencv-python\nPillow\ntorch>=1.7\ntorchvision\ntqdm\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.66796875,
          "content": "[flake8]\nignore =\n    # line break before binary operator (W503)\n    W503,\n    # line break after binary operator (W504)\n    W504,\nmax-line-length=120\n\n[yapf]\nbased_on_style = pep8\ncolumn_limit = 120\nblank_line_before_nested_class_or_def = true\nsplit_before_expression_after_opening_paren = true\n\n[isort]\nline_length = 120\nmulti_line_output = 0\nknown_standard_library = pkg_resources,setuptools\nknown_first_party = realesrgan\nknown_third_party = PIL,basicsr,cv2,numpy,pytest,torch,torchvision,tqdm,yaml\nno_lines_before = STDLIB,LOCALFOLDER\ndefault_section = THIRDPARTY\n\n[codespell]\nskip = .git,./docs/build\ncount =\nquiet-level = 3\n\n[aliases]\ntest=pytest\n\n[tool:pytest]\naddopts=tests/\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 3.0712890625,
          "content": "#!/usr/bin/env python\n\nfrom setuptools import find_packages, setup\n\nimport os\nimport subprocess\nimport time\n\nversion_file = 'realesrgan/version.py'\n\n\ndef readme():\n    with open('README.md', encoding='utf-8') as f:\n        content = f.read()\n    return content\n\n\ndef get_git_hash():\n\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in ['SYSTEMROOT', 'PATH', 'HOME']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env['LANGUAGE'] = 'C'\n        env['LANG'] = 'C'\n        env['LC_ALL'] = 'C'\n        out = subprocess.Popen(cmd, stdout=subprocess.PIPE, env=env).communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd(['git', 'rev-parse', 'HEAD'])\n        sha = out.strip().decode('ascii')\n    except OSError:\n        sha = 'unknown'\n\n    return sha\n\n\ndef get_hash():\n    if os.path.exists('.git'):\n        sha = get_git_hash()[:7]\n    else:\n        sha = 'unknown'\n\n    return sha\n\n\ndef write_version_py():\n    content = \"\"\"# GENERATED VERSION FILE\n# TIME: {}\n__version__ = '{}'\n__gitsha__ = '{}'\nversion_info = ({})\n\"\"\"\n    sha = get_hash()\n    with open('VERSION', 'r') as f:\n        SHORT_VERSION = f.read().strip()\n    VERSION_INFO = ', '.join([x if x.isdigit() else f'\"{x}\"' for x in SHORT_VERSION.split('.')])\n\n    version_file_str = content.format(time.asctime(), SHORT_VERSION, sha, VERSION_INFO)\n    with open(version_file, 'w') as f:\n        f.write(version_file_str)\n\n\ndef get_version():\n    with open(version_file, 'r') as f:\n        exec(compile(f.read(), version_file, 'exec'))\n    return locals()['__version__']\n\n\ndef get_requirements(filename='requirements.txt'):\n    here = os.path.dirname(os.path.realpath(__file__))\n    with open(os.path.join(here, filename), 'r') as f:\n        requires = [line.replace('\\n', '') for line in f.readlines()]\n    return requires\n\n\nif __name__ == '__main__':\n    write_version_py()\n    setup(\n        name='realesrgan',\n        version=get_version(),\n        description='Real-ESRGAN aims at developing Practical Algorithms for General Image Restoration',\n        long_description=readme(),\n        long_description_content_type='text/markdown',\n        author='Xintao Wang',\n        author_email='xintao.wang@outlook.com',\n        keywords='computer vision, pytorch, image restoration, super-resolution, esrgan, real-esrgan',\n        url='https://github.com/xinntao/Real-ESRGAN',\n        include_package_data=True,\n        packages=find_packages(exclude=('options', 'datasets', 'experiments', 'results', 'tb_logger', 'wandb')),\n        classifiers=[\n            'Development Status :: 4 - Beta',\n            'License :: OSI Approved :: Apache Software License',\n            'Operating System :: OS Independent',\n            'Programming Language :: Python :: 3',\n            'Programming Language :: Python :: 3.7',\n            'Programming Language :: Python :: 3.8',\n        ],\n        license='BSD-3-Clause License',\n        setup_requires=['cython', 'numpy'],\n        install_requires=get_requirements(),\n        zip_safe=False)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "weights",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}