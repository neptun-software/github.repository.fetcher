{
  "metadata": {
    "timestamp": 1736561429188,
    "page": 72,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/fairseq",
      "stars": 30789,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.701171875,
          "content": "# JetBrains PyCharm IDE\n.idea/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# macOS dir files\n.DS_Store\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# Checkpoints\ncheckpoints\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# Generated files\n/fairseq/temporal_convolution_tbc\n/fairseq/modules/*_layer/*_forward.cu\n/fairseq/modules/*_layer/*_backward.cu\n/fairseq/version.py\n\n# data\ndata-bin/\n\n# reranking\n/examples/reranking/rerank_data\n\n# Cython-generated C++ source files\n/fairseq/data/data_utils_fast.cpp\n/fairseq/data/token_block_utils_fast.cpp\n\n# VSCODE\n.vscode/ftp-sync.json\n.vscode/settings.json\n\n# Experimental Folder\nexperimental/*\n\n# Weights and Biases logs\nwandb/\n\n# Hydra artifacts\nnohup.out\nmultirun\noutputs\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.158203125,
          "content": "[submodule \"fairseq/model_parallel/megatron\"]\n    path = fairseq/model_parallel/megatron\n    url = https://github.com/ngoyal2707/Megatron-LM\n    branch = fairseq\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.9130859375,
          "content": "exclude: 'build|stubs'\n\ndefault_language_version:\n    python: python3\n\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.1.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: check-ast\n    -   id: check-merge-conflict\n    -   id: no-commit-to-branch\n        args: ['--branch=master']\n    -   id: check-added-large-files\n        args: ['--maxkb=500']\n    -   id: end-of-file-fixer\n\n-   repo: https://github.com/ambv/black\n    rev: 22.3.0\n    hooks:\n    - id: black\n      language_version: python3.8\n\n-   repo: https://gitlab.com/pycqa/flake8\n    rev: 3.9.2\n    hooks:\n    -   id: flake8\n        args: [\n            # only error for syntax errors and undefined names\n            \"--select=E9,F63,F7,F82\",\n        ]\n\n-   repo: https://github.com/pycqa/isort\n    rev: 5.10.1\n    hooks:\n    -   id: isort\n        exclude: README.md\n        additional_dependencies: [toml]\n        args: [\"--profile\", \"black\"]\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.271484375,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <conduct@pytorch.org>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 3.2509765625,
          "content": "# Contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq)\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `main`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\n## License\nBy contributing to Facebook AI Research Sequence-to-Sequence Toolkit (fairseq),\nyou agree that your contributions will be licensed under the LICENSE file in\nthe root directory of this source tree.\n\n## Pre-commit hooks\nIn order to ensure your code lints, there are pre-commit hooks configured in the repository which you can install.\nAfter installation, they will automatically run each time you commit.\nAn abbreviated guide is given below; for more information, refer to [the offical pre-commit documentation](https://pre-commit.com/).\n\n### Installation\n```\npip install pre-commit\npre-commit install\n```\n\n### Usage\nJust commit your changes:\n```\ngit commit -m \"My informative commit message\"\n```\n\nIf there was a failure, you will get feedback\n```\n[INFO] Initializing environment for https://github.com/PyCQA/flake8.\n[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.\n[INFO] Once installed this environment will be reused.\n[INFO] This may take a few minutes...\n[INFO] Installing environment for https://github.com/PyCQA/flake8.\n[INFO] Once installed this environment will be reused.\n[INFO] This may take a few minutes...\nTrim Trailing Whitespace.................................................Failed\n- hook id: trailing-whitespace\n- exit code: 1\n- files were modified by this hook\nFixing examples/nllb/modeling/wmt15_benchmark/eval_langs2.sh\nFix End of Files.........................................................Failed\n- hook id: end-of-file-fixer\n- exit code: 1\n- files were modified by this hook\nFixing examples/few_shot/scripts/schedule_jobs_few_shot.py\nflake8...................................................................Passed\n```\n\nCertain hooks modify your files to comply.\nTo include these modifications, you will need to add them (i.e. `git add ...`) and commit again.\n\nIf all is well, you should see something like:\n```\nTrim Trailing Whitespace.................................................Passed\nFix End of Files.........................................................Passed\nflake8...................................................................Passed\n[gshard-fix-ci 8698644e1] Fix lint, add pre-commit hooks\n 10 files changed, 148 insertions(+), 110 deletions(-)\n create mode 100644 .flake8\n create mode 100644 .pre-commit-config.yaml\n rename examples/nllb/modeling/wmt15_benchmark/{eval_langs2.py => eval_langs2.sh} (99%)\n ```\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.060546875,
          "content": "MIT License\n\nCopyright (c) Facebook, Inc. and its affiliates.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.02734375,
          "content": "include fairseq/version.txt\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.990234375,
          "content": "<p align=\"center\">\n  <img src=\"docs/fairseq_logo.png\" width=\"150\">\n  <br />\n  <br />\n  <a href=\"https://opensource.fb.com/support-ukraine\"><img alt=\"Support Ukraine\" src=\"https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/blob/main/LICENSE\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/license-MIT-blue.svg\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/releases\"><img alt=\"Latest Release\" src=\"https://img.shields.io/github/release/pytorch/fairseq.svg\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/actions?query=workflow:build\"><img alt=\"Build Status\" src=\"https://github.com/pytorch/fairseq/workflows/build/badge.svg\" /></a>\n  <a href=\"https://fairseq.readthedocs.io/en/latest/?badge=latest\"><img alt=\"Documentation Status\" src=\"https://readthedocs.org/projects/fairseq/badge/?version=latest\" /></a>\n  <a href=\"https://app.circleci.com/pipelines/github/facebookresearch/fairseq/\"><img alt=\"CicleCI Status\" src=\"https://circleci.com/gh/facebookresearch/fairseq.svg?style=shield\" /></a>\n</p>\n\n--------------------------------------------------------------------------------\n\nFairseq(-py) is a sequence modeling toolkit that allows researchers and\ndevelopers to train custom models for translation, summarization, language\nmodeling and other text generation tasks.\n\nWe provide reference implementations of various sequence modeling papers:\n\n<details><summary>List of implemented papers</summary><p>\n\n* **Convolutional Neural Networks (CNN)**\n  + [Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)](examples/language_model/conv_lm/README.md)\n  + [Convolutional Sequence to Sequence Learning (Gehring et al., 2017)](examples/conv_seq2seq/README.md)\n  + [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel)\n  + [Hierarchical Neural Story Generation (Fan et al., 2018)](examples/stories/README.md)\n  + [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)](examples/wav2vec/README.md)\n* **LightConv and DynamicConv models**\n  + [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)](examples/pay_less_attention_paper/README.md)\n* **Long Short-Term Memory (LSTM) networks**\n  + Effective Approaches to Attention-based Neural Machine Translation (Luong et al., 2015)\n* **Transformer (self-attention) networks**\n  + Attention Is All You Need (Vaswani et al., 2017)\n  + [Scaling Neural Machine Translation (Ott et al., 2018)](examples/scaling_nmt/README.md)\n  + [Understanding Back-Translation at Scale (Edunov et al., 2018)](examples/backtranslation/README.md)\n  + [Adaptive Input Representations for Neural Language Modeling (Baevski and Auli, 2018)](examples/language_model/README.adaptive_inputs.md)\n  + [Lexically constrained decoding with dynamic beam allocation (Post & Vilar, 2018)](examples/constrained_decoding/README.md)\n  + [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Dai et al., 2019)](examples/truncated_bptt/README.md)\n  + [Adaptive Attention Span in Transformers (Sukhbaatar et al., 2019)](examples/adaptive_span/README.md)\n  + [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)](examples/translation_moe/README.md)\n  + [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)](examples/roberta/README.md)\n  + [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)](examples/wmt19/README.md)\n  + [Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)](examples/joint_alignment_translation/README.md )\n  + [Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)](examples/mbart/README.md)\n  + [Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)](examples/byte_level_bpe/README.md)\n  + [Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)](examples/unsupervised_quality_estimation/README.md)\n  + [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)](examples/wav2vec/README.md)\n  + [Generating Medical Reports from Patient-Doctor Conversations Using Sequence-to-Sequence Models (Enarvi et al., 2020)](examples/pointer_generator/README.md)\n  + [Linformer: Self-Attention with Linear Complexity (Wang et al., 2020)](examples/linformer/README.md)\n  + [Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)](examples/criss/README.md)\n  + [Deep Transformers with Latent Depth (Li et al., 2020)](examples/latent_depth/README.md)\n  + [Unsupervised Cross-lingual Representation Learning for Speech Recognition (Conneau et al., 2020)](https://arxiv.org/abs/2006.13979)\n  + [Self-training and Pre-training are Complementary for Speech Recognition (Xu et al., 2020)](https://arxiv.org/abs/2010.11430)\n  + [Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training (Hsu, et al., 2021)](https://arxiv.org/abs/2104.01027)\n  + [Unsupervised Speech Recognition (Baevski, et al., 2021)](https://arxiv.org/abs/2105.11084)\n  + [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition (Xu et al., 2021)](https://arxiv.org/abs/2109.11680)\n  + [VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding (Xu et. al., 2021)](https://arxiv.org/pdf/2109.14084.pdf)\n  + [VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding (Xu et. al., 2021)](https://aclanthology.org/2021.findings-acl.370.pdf)\n  + [NormFormer: Improved Transformer Pretraining with Extra Normalization (Shleifer et. al, 2021)](examples/normformer/README.md)\n* **Non-autoregressive Transformers**\n  + Non-Autoregressive Neural Machine Translation (Gu et al., 2017)\n  + Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement (Lee et al. 2018)\n  + Insertion Transformer: Flexible Sequence Generation via Insertion Operations (Stern et al. 2019)\n  + Mask-Predict: Parallel Decoding of Conditional Masked Language Models (Ghazvininejad et al., 2019)\n  + [Levenshtein Transformer (Gu et al., 2019)](examples/nonautoregressive_translation/README.md)\n* **Finetuning**\n  + [Better Fine-Tuning by Reducing Representational Collapse (Aghajanyan et al. 2020)](examples/rxf/README.md)\n\n</p></details>\n\n### What's New:\n* May 2023 [Released models for Scaling Speech Technology to 1,000+ Languages  (Pratap, et al., 2023)](examples/mms/README.md)\n* June 2022 [Released code for wav2vec-U 2.0 from Towards End-to-end Unsupervised Speech Recognition (Liu, et al., 2022)](examples/wav2vec/unsupervised/README.md)\n* May 2022 [Integration with xFormers](https://github.com/facebookresearch/xformers)\n* December 2021 [Released Direct speech-to-speech translation code](examples/speech_to_speech/README.md)\n* October 2021 [Released VideoCLIP and VLM models](examples/MMPT/README.md)\n* October 2021 [Released multilingual finetuned XLSR-53 model](examples/wav2vec/README.md)\n* September 2021 [`master` branch renamed to `main`](https://github.com/github/renaming).\n* July 2021 [Released DrNMT code](examples/discriminative_reranking_nmt/README.md)\n* July 2021 [Released Robust wav2vec 2.0 model](examples/wav2vec/README.md)\n* June 2021 [Released XLMR-XL and XLMR-XXL models](examples/xlmr/README.md)\n* May 2021 [Released Unsupervised Speech Recognition code](examples/wav2vec/unsupervised/README.md)\n* March 2021 [Added full parameter and optimizer state sharding + CPU offloading](examples/fully_sharded_data_parallel/README.md)\n* February 2021 [Added LASER training code](examples/laser/README.md)\n* December 2020: [Added Adaptive Attention Span code](examples/adaptive_span/README.md)\n* December 2020: [GottBERT model and code released](examples/gottbert/README.md)\n* November 2020: Adopted the [Hydra](https://github.com/facebookresearch/hydra) configuration framework\n  * [see documentation explaining how to use it for new and existing projects](docs/hydra_integration.md)\n* November 2020: [fairseq 0.10.0 released](https://github.com/pytorch/fairseq/releases/tag/v0.10.0)\n* October 2020: [Added R3F/R4F (Better Fine-Tuning) code](examples/rxf/README.md)\n* October 2020: [Deep Transformer with Latent Depth code released](examples/latent_depth/README.md)\n* October 2020: [Added CRISS models and code](examples/criss/README.md)\n\n<details><summary>Previous updates</summary><p>\n\n* September 2020: [Added Linformer code](examples/linformer/README.md)\n* September 2020: [Added pointer-generator networks](examples/pointer_generator/README.md)\n* August 2020: [Added lexically constrained decoding](examples/constrained_decoding/README.md)\n* August 2020: [wav2vec2 models and code released](examples/wav2vec/README.md)\n* July 2020: [Unsupervised Quality Estimation code released](examples/unsupervised_quality_estimation/README.md)\n* May 2020: [Follow fairseq on Twitter](https://twitter.com/fairseq)\n* April 2020: [Monotonic Multihead Attention code released](examples/simultaneous_translation/README.md)\n* April 2020: [Quant-Noise code released](examples/quant_noise/README.md)\n* April 2020: [Initial model parallel support and 11B parameters unidirectional LM released](examples/megatron_11b/README.md)\n* March 2020: [Byte-level BPE code released](examples/byte_level_bpe/README.md)\n* February 2020: [mBART model and code released](examples/mbart/README.md)\n* February 2020: [Added tutorial for back-translation](https://github.com/pytorch/fairseq/tree/main/examples/backtranslation#training-your-own-model-wmt18-english-german)\n* December 2019: [fairseq 0.9.0 released](https://github.com/pytorch/fairseq/releases/tag/v0.9.0)\n* November 2019: [VizSeq released (a visual analysis toolkit for evaluating fairseq models)](https://facebookresearch.github.io/vizseq/docs/getting_started/fairseq_example)\n* November 2019: [CamemBERT model and code released](examples/camembert/README.md)\n* November 2019: [BART model and code released](examples/bart/README.md)\n* November 2019: [XLM-R models and code released](examples/xlmr/README.md)\n* September 2019: [Nonautoregressive translation code released](examples/nonautoregressive_translation/README.md)\n* August 2019: [WMT'19 models released](examples/wmt19/README.md)\n* July 2019: fairseq relicensed under MIT license\n* July 2019: [RoBERTa models and code released](examples/roberta/README.md)\n* June 2019: [wav2vec models and code released](examples/wav2vec/README.md)\n\n</p></details>\n\n### Features:\n\n* multi-GPU training on one machine or across multiple machines (data and model parallel)\n* fast generation on both CPU and GPU with multiple search algorithms implemented:\n  + beam search\n  + Diverse Beam Search ([Vijayakumar et al., 2016](https://arxiv.org/abs/1610.02424))\n  + sampling (unconstrained, top-k and top-p/nucleus)\n  + [lexically constrained decoding](examples/constrained_decoding/README.md) (Post & Vilar, 2018)\n* [gradient accumulation](https://fairseq.readthedocs.io/en/latest/getting_started.html#large-mini-batch-training-with-delayed-updates) enables training with large mini-batches even on a single GPU\n* [mixed precision training](https://fairseq.readthedocs.io/en/latest/getting_started.html#training-with-half-precision-floating-point-fp16) (trains faster with less GPU memory on [NVIDIA tensor cores](https://developer.nvidia.com/tensor-cores))\n* [extensible](https://fairseq.readthedocs.io/en/latest/overview.html): easily register new models, criterions, tasks, optimizers and learning rate schedulers\n* [flexible configuration](docs/hydra_integration.md) based on [Hydra](https://github.com/facebookresearch/hydra) allowing a combination of code, command-line and file based configuration\n* [full parameter and optimizer state sharding](examples/fully_sharded_data_parallel/README.md)\n* [offloading parameters to CPU](examples/fully_sharded_data_parallel/README.md)\n\nWe also provide [pre-trained models for translation and language modeling](#pre-trained-models-and-examples)\nwith a convenient `torch.hub` interface:\n\n``` python\nen2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model')\nen2de.translate('Hello world', beam=5)\n# 'Hallo Welt'\n```\n\nSee the PyTorch Hub tutorials for [translation](https://pytorch.org/hub/pytorch_fairseq_translation/)\nand [RoBERTa](https://pytorch.org/hub/pytorch_fairseq_roberta/) for more examples.\n\n# Requirements and Installation\n\n* [PyTorch](http://pytorch.org/) version >= 1.10.0\n* Python version >= 3.8\n* For training new models, you'll also need an NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl)\n* **To install fairseq** and develop locally:\n\n``` bash\ngit clone https://github.com/pytorch/fairseq\ncd fairseq\npip install --editable ./\n\n# on MacOS:\n# CFLAGS=\"-stdlib=libc++\" pip install --editable ./\n\n# to install the latest stable release (0.10.x)\n# pip install fairseq\n```\n\n* **For faster training** install NVIDIA's [apex](https://github.com/NVIDIA/apex) library:\n\n``` bash\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" \\\n  --global-option=\"--deprecated_fused_adam\" --global-option=\"--xentropy\" \\\n  --global-option=\"--fast_multihead_attn\" ./\n```\n\n* **For large datasets** install [PyArrow](https://arrow.apache.org/docs/python/install.html#using-pip): `pip install pyarrow`\n* If you use Docker make sure to increase the shared memory size either with `--ipc=host` or `--shm-size`\n as command line options to `nvidia-docker run` .\n\n# Getting Started\n\nThe [full documentation](https://fairseq.readthedocs.io/) contains instructions\nfor getting started, training new models and extending fairseq with new model\ntypes and tasks.\n\n# Pre-trained models and examples\n\nWe provide pre-trained models and pre-processed, binarized test sets for several tasks listed below,\nas well as example training and evaluation commands.\n\n* [Translation](examples/translation/README.md): convolutional and transformer models are available\n* [Language Modeling](examples/language_model/README.md): convolutional and transformer models are available\n\nWe also have more detailed READMEs to reproduce results from specific papers:\n\n* [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale (Babu et al., 2021)](examples/wav2vec/xlsr/README.md)\n* [Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)](examples/criss/README.md)\n* [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)](examples/wav2vec/README.md)\n* [Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)](examples/unsupervised_quality_estimation/README.md)\n* [Training with Quantization Noise for Extreme Model Compression ({Fan*, Stock*} et al., 2020)](examples/quant_noise/README.md)\n* [Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)](examples/byte_level_bpe/README.md)\n* [Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)](examples/mbart/README.md)\n* [Reducing Transformer Depth on Demand with Structured Dropout (Fan et al., 2019)](examples/layerdrop/README.md)\n* [Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)](examples/joint_alignment_translation/README.md)\n* [Levenshtein Transformer (Gu et al., 2019)](examples/nonautoregressive_translation/README.md)\n* [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)](examples/wmt19/README.md)\n* [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)](examples/roberta/README.md)\n* [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)](examples/wav2vec/README.md)\n* [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)](examples/translation_moe/README.md)\n* [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)](examples/pay_less_attention_paper/README.md)\n* [Understanding Back-Translation at Scale (Edunov et al., 2018)](examples/backtranslation/README.md)\n* [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel)\n* [Hierarchical Neural Story Generation (Fan et al., 2018)](examples/stories/README.md)\n* [Scaling Neural Machine Translation (Ott et al., 2018)](examples/scaling_nmt/README.md)\n* [Convolutional Sequence to Sequence Learning (Gehring et al., 2017)](examples/conv_seq2seq/README.md)\n* [Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)](examples/language_model/README.conv.md)\n\n# Join the fairseq community\n\n* Twitter: https://twitter.com/fairseq\n* Facebook page: https://www.facebook.com/groups/fairseq.users\n* Google group: https://groups.google.com/forum/#!forum/fairseq-users\n\n# License\n\nfairseq(-py) is MIT-licensed.\nThe license applies to the pre-trained models as well.\n\n# Citation\n\nPlease cite as:\n\n``` bibtex\n@inproceedings{ott2019fairseq,\n  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},\n  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},\n  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},\n  year = {2019},\n}\n```\n"
        },
        {
          "name": "RELEASE.md",
          "type": "blob",
          "size": 0.638671875,
          "content": "# Creating a New Release\n\nIn order to create a new release:\n\n1. Navigate to the [Fairseq Workflows](https://github.com/facebookresearch/fairseq/actions) and find the one named _Fairseq Release_. \n\n2. Under _Run Workflow_ choose the branch `main` and for _Release Type_ enter either `major`, `minor`, or `patch`.  \n\n3. A branch named `$new_version-release` will be created where the `version.txt` file is updated. Merge those changes into `main`.\n\n4. Make sure that a [new PYPI package](https://pypi.org/project/fairseq/) has been uploaded.\n\n5. Make sure that a [new github release](https://github.com/facebookresearch/fairseq/releases) has been created.\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "fairseq",
          "type": "tree",
          "content": null
        },
        {
          "name": "fairseq_cli",
          "type": "tree",
          "content": null
        },
        {
          "name": "hubconf.py",
          "type": "blob",
          "size": 2.0498046875,
          "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"isort:skip_file\"\"\"\n\nimport functools\nimport importlib\n\n\ndependencies = [\n    \"dataclasses\",\n    \"hydra\",\n    \"numpy\",\n    \"omegaconf\",\n    \"regex\",\n    \"requests\",\n    \"torch\",\n]\n\n\n# Check for required dependencies and raise a RuntimeError if any are missing.\nmissing_deps = []\nfor dep in dependencies:\n    try:\n        importlib.import_module(dep)\n    except ImportError:\n        # Hack: the hydra package is provided under the \"hydra-core\" name in\n        # pypi. We don't want the user mistakenly calling `pip install hydra`\n        # since that will install an unrelated package.\n        if dep == \"hydra\":\n            dep = \"hydra-core\"\n        missing_deps.append(dep)\nif len(missing_deps) > 0:\n    raise RuntimeError(\"Missing dependencies: {}\".format(\", \".join(missing_deps)))\n\n\n# only do fairseq imports after checking for dependencies\nfrom fairseq.hub_utils import (  # noqa; noqa\n    BPEHubInterface as bpe,\n    TokenizerHubInterface as tokenizer,\n)\nfrom fairseq.models import MODEL_REGISTRY  # noqa\n\n\n# torch.hub doesn't build Cython components, so if they are not found then try\n# to build them here\ntry:\n    import fairseq.data.token_block_utils_fast  # noqa\nexcept ImportError:\n    try:\n        import cython  # noqa\n        import os\n        from setuptools import sandbox\n\n        sandbox.run_setup(\n            os.path.join(os.path.dirname(__file__), \"setup.py\"),\n            [\"build_ext\", \"--inplace\"],\n        )\n    except ImportError:\n        print(\n            \"Unable to build Cython components. Please make sure Cython is \"\n            \"installed if the torch.hub model you are loading depends on it.\"\n        )\n\n\n# automatically expose models defined in FairseqModel::hub_models\nfor _model_type, _cls in MODEL_REGISTRY.items():\n    for model_name in _cls.hub_models().keys():\n        globals()[model_name] = functools.partial(\n            _cls.from_pretrained,\n            model_name,\n        )\n"
        },
        {
          "name": "hydra_plugins",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.9189453125,
          "content": "[build-system]\nrequires = [\n  \"setuptools>=18.0\",\n  \"wheel\",\n  \"cython\",\n  \"numpy>=1.21.3\",\n  \"torch>=1.10\",\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.black]\nextend-exclude = '''\n(\n^/examples/|\n^/fairseq/model_parallel/megatron|\n^/build/\n)\n'''\n\n[tool.isort]\nprofile = \"black\"\nknown_third_party = \"_cffi_backend,agg_results,aml,bitarray,boto3,botocore,dump_hubert_feature,dynamicconv_cuda,editdistance,faiss,fasttext,feature_utils,ffmpeg,g2p_en,h5py,hydra,hypothesis,indicnlp,inflect,iopath,joblib,kaldi_io,kenlm,libfb,librosa,lightconv_cuda,matplotlib,misc,mmpt,mmpt_cli,model,nltk,npy_append_array,numpy,omegaconf,pandas,pathbuilder,preprocessing,progressbar,pythainlp,random_sequence_shuffler,regex,sacrebleu,sacremoses,scipy,sentencepiece,setuptools,six,sklearn,soundfile,sweep,sweep_wmt_en2de_transformer_big_common,tabulate,torch,torchaudio,tqdm,unidecode,utils,videoreader,wav2vec_cluster_faiss,wget,yaml\"\nskip_gitignore = true\n"
        },
        {
          "name": "release_utils.py",
          "type": "blob",
          "size": 2.0283203125,
          "content": "import argparse\nfrom typing import Tuple\n\n\ndef get_next_version(release_type) -> Tuple[Tuple[int, int, int], str, str]:\n    current_ver = find_version(\"fairseq/version.txt\")\n    version_list = [int(x) for x in current_ver.strip(\"'\").split(\".\")]\n    major, minor, patch = version_list[0], version_list[1], version_list[2]\n    if release_type == \"patch\":\n        patch += 1\n    elif release_type == \"minor\":\n        minor += 1\n        patch = 0\n    elif release_type == \"major\":\n        major += 1\n        minor = patch = 0\n    else:\n        raise ValueError(\n            \"Incorrect release type specified. Acceptable types are major, minor and patch.\"\n        )\n\n    new_version_tuple = (major, minor, patch)\n    new_version_str = \".\".join([str(x) for x in new_version_tuple])\n    new_tag_str = \"v\" + new_version_str\n    return new_version_tuple, new_version_str, new_tag_str\n\n\ndef find_version(version_file_path) -> str:\n    with open(version_file_path) as f:\n        version = f.read().strip()\n        return version\n\n\ndef update_version(new_version_str) -> None:\n    \"\"\"\n    given the current version, update the version to the\n    next version depending on the type of release.\n    \"\"\"\n\n    with open(\"fairseq/version.txt\", \"w\") as writer:\n        writer.write(new_version_str)\n\n\ndef main(args):\n    if args.release_type in [\"major\", \"minor\", \"patch\"]:\n        new_version_tuple, new_version, new_tag = get_next_version(args.release_type)\n    else:\n        raise ValueError(\"Incorrect release type specified\")\n\n    if args.update_version:\n        update_version(new_version)\n\n    print(new_version, new_tag)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Versioning utils\")\n    parser.add_argument(\n        \"--release-type\",\n        type=str,\n        required=True,\n        help=\"type of release = major/minor/patch\",\n    )\n    parser.add_argument(\n        \"--update-version\",\n        action=\"store_true\",\n        required=False,\n        help=\"updates the version in fairseq/version.txt\",\n    )\n\n    args = parser.parse_args()\n    main(args)\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.1044921875,
          "content": "[flake8]\nmax-line-length = 127\nextend-ignore = E203, W503\nextend-exclude = fairseq/model_parallel/megatron\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 7.462890625,
          "content": "#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport subprocess\nimport sys\n\nfrom setuptools import Extension, find_packages, setup\nfrom torch.utils import cpp_extension\n\nif sys.version_info < (3, 6):\n    sys.exit(\"Sorry, Python >= 3.6 is required for fairseq.\")\n\n\ndef write_version_py():\n    with open(os.path.join(\"fairseq\", \"version.txt\")) as f:\n        version = f.read().strip()\n\n    # write version info to fairseq/version.py\n    with open(os.path.join(\"fairseq\", \"version.py\"), \"w\") as f:\n        f.write('__version__ = \"{}\"\\n'.format(version))\n    return version\n\n\nversion = write_version_py()\n\n\nwith open(\"README.md\") as f:\n    readme = f.read()\n\n\nif sys.platform == \"darwin\":\n    extra_compile_args = [\"-stdlib=libc++\", \"-O3\"]\nelse:\n    extra_compile_args = [\"-std=c++11\", \"-O3\"]\n\n\nclass NumpyExtension(Extension):\n    \"\"\"Source: https://stackoverflow.com/a/54128391\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.__include_dirs = []\n        super().__init__(*args, **kwargs)\n\n    @property\n    def include_dirs(self):\n        import numpy\n\n        return self.__include_dirs + [numpy.get_include()]\n\n    @include_dirs.setter\n    def include_dirs(self, dirs):\n        self.__include_dirs = dirs\n\n\nextensions = [\n    Extension(\n        \"fairseq.libbleu\",\n        sources=[\n            \"fairseq/clib/libbleu/libbleu.cpp\",\n            \"fairseq/clib/libbleu/module.cpp\",\n        ],\n        extra_compile_args=extra_compile_args,\n    ),\n    NumpyExtension(\n        \"fairseq.data.data_utils_fast\",\n        sources=[\"fairseq/data/data_utils_fast.pyx\"],\n        language=\"c++\",\n        extra_compile_args=extra_compile_args,\n    ),\n    NumpyExtension(\n        \"fairseq.data.token_block_utils_fast\",\n        sources=[\"fairseq/data/token_block_utils_fast.pyx\"],\n        language=\"c++\",\n        extra_compile_args=extra_compile_args,\n    ),\n]\n\n\nextensions.extend(\n    [\n        cpp_extension.CppExtension(\n            \"fairseq.libbase\",\n            sources=[\n                \"fairseq/clib/libbase/balanced_assignment.cpp\",\n            ],\n        ),\n        cpp_extension.CppExtension(\n            \"fairseq.libnat\",\n            sources=[\n                \"fairseq/clib/libnat/edit_dist.cpp\",\n            ],\n        ),\n        cpp_extension.CppExtension(\n            \"alignment_train_cpu_binding\",\n            sources=[\n                \"examples/operators/alignment_train_cpu.cpp\",\n            ],\n        ),\n    ]\n)\nif \"CUDA_HOME\" in os.environ:\n    extensions.extend(\n        [\n            cpp_extension.CppExtension(\n                \"fairseq.libnat_cuda\",\n                sources=[\n                    \"fairseq/clib/libnat_cuda/edit_dist.cu\",\n                    \"fairseq/clib/libnat_cuda/binding.cpp\",\n                ],\n            ),\n            cpp_extension.CppExtension(\n                \"fairseq.ngram_repeat_block_cuda\",\n                sources=[\n                    \"fairseq/clib/cuda/ngram_repeat_block_cuda.cpp\",\n                    \"fairseq/clib/cuda/ngram_repeat_block_cuda_kernel.cu\",\n                ],\n            ),\n            cpp_extension.CppExtension(\n                \"alignment_train_cuda_binding\",\n                sources=[\n                    \"examples/operators/alignment_train_kernel.cu\",\n                    \"examples/operators/alignment_train_cuda.cpp\",\n                ],\n            ),\n        ]\n    )\n\ncmdclass = {\"build_ext\": cpp_extension.BuildExtension}\n\nif \"READTHEDOCS\" in os.environ:\n    # don't build extensions when generating docs\n    extensions = []\n    if \"build_ext\" in cmdclass:\n        del cmdclass[\"build_ext\"]\n\n    # use CPU build of PyTorch\n    dependency_links = [\n        \"https://download.pytorch.org/whl/cpu/torch-1.7.0%2Bcpu-cp36-cp36m-linux_x86_64.whl\"\n    ]\nelse:\n    dependency_links = []\n\n\nif \"clean\" in sys.argv[1:]:\n    # Source: https://bit.ly/2NLVsgE\n    print(\"deleting Cython files...\")\n\n    subprocess.run(\n        [\"rm -f fairseq/*.so fairseq/**/*.so fairseq/*.pyd fairseq/**/*.pyd\"],\n        shell=True,\n    )\n\n\nextra_packages = []\nif os.path.exists(os.path.join(\"fairseq\", \"model_parallel\", \"megatron\", \"mpu\")):\n    extra_packages.append(\"fairseq.model_parallel.megatron.mpu\")\n\n\ndef do_setup(package_data):\n    setup(\n        name=\"fairseq\",\n        version=version,\n        description=\"Facebook AI Research Sequence-to-Sequence Toolkit\",\n        url=\"https://github.com/pytorch/fairseq\",\n        classifiers=[\n            \"Intended Audience :: Science/Research\",\n            \"License :: OSI Approved :: MIT License\",\n            \"Programming Language :: Python :: 3.6\",\n            \"Programming Language :: Python :: 3.7\",\n            \"Programming Language :: Python :: 3.8\",\n            \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        ],\n        long_description=readme,\n        long_description_content_type=\"text/markdown\",\n        install_requires=[\n            \"cffi\",\n            \"cython\",\n            \"hydra-core>=1.0.7,<1.1\",\n            \"omegaconf<2.1\",\n            \"numpy>=1.21.3\",\n            \"regex\",\n            \"sacrebleu>=1.4.12\",\n            \"torch>=1.13\",\n            \"tqdm\",\n            \"bitarray\",\n            \"torchaudio>=0.8.0\",\n            \"scikit-learn\",\n            \"packaging\",\n        ],\n        extras_require={\n            \"dev\": [\"flake8\", \"pytest\", \"black==22.3.0\"],\n            \"docs\": [\"sphinx\", \"sphinx-argparse\"],\n        },\n        dependency_links=dependency_links,\n        packages=find_packages(\n            exclude=[\n                \"examples\",\n                \"examples.*\",\n                \"scripts\",\n                \"scripts.*\",\n                \"tests\",\n                \"tests.*\",\n            ]\n        )\n        + extra_packages,\n        package_data=package_data,\n        ext_modules=extensions,\n        test_suite=\"tests\",\n        entry_points={\n            \"console_scripts\": [\n                \"fairseq-eval-lm = fairseq_cli.eval_lm:cli_main\",\n                \"fairseq-generate = fairseq_cli.generate:cli_main\",\n                \"fairseq-hydra-train = fairseq_cli.hydra_train:cli_main\",\n                \"fairseq-interactive = fairseq_cli.interactive:cli_main\",\n                \"fairseq-preprocess = fairseq_cli.preprocess:cli_main\",\n                \"fairseq-score = fairseq_cli.score:cli_main\",\n                \"fairseq-train = fairseq_cli.train:cli_main\",\n                \"fairseq-validate = fairseq_cli.validate:cli_main\",\n            ],\n        },\n        cmdclass=cmdclass,\n        zip_safe=False,\n    )\n\n\ndef get_files(path, relative_to=\"fairseq\"):\n    all_files = []\n    for root, _dirs, files in os.walk(path, followlinks=True):\n        root = os.path.relpath(root, relative_to)\n        for file in files:\n            if file.endswith(\".pyc\"):\n                continue\n            all_files.append(os.path.join(root, file))\n    return all_files\n\n\nif __name__ == \"__main__\":\n    try:\n        # symlink examples into fairseq package so package_data accepts them\n        fairseq_examples = os.path.join(\"fairseq\", \"examples\")\n        if \"build_ext\" not in sys.argv[1:] and not os.path.exists(fairseq_examples):\n            os.symlink(os.path.join(\"..\", \"examples\"), fairseq_examples)\n\n        package_data = {\n            \"fairseq\": (\n                get_files(fairseq_examples)\n                + get_files(os.path.join(\"fairseq\", \"config\"))\n            )\n        }\n        do_setup(package_data)\n    finally:\n        if \"build_ext\" not in sys.argv[1:] and os.path.islink(fairseq_examples):\n            os.unlink(fairseq_examples)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 0.357421875,
          "content": "#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nLegacy entry point. Use fairseq_cli/train.py or fairseq-train instead.\n\"\"\"\n\nfrom fairseq_cli.train import cli_main\n\n\nif __name__ == \"__main__\":\n    cli_main()\n"
        }
      ]
    }
  ]
}