{
  "metadata": {
    "timestamp": 1736561406434,
    "page": 41,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "TencentARC/GFPGAN",
      "stars": 36140,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.8466796875,
          "content": "# ignored folders\ndatasets/*\nexperiments/*\nresults/*\ntb_logger/*\nwandb/*\ntmp/*\n\nversion.py\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.4482421875,
          "content": "repos:\n  # flake8\n  - repo: https://github.com/PyCQA/flake8\n    rev: 3.8.3\n    hooks:\n      - id: flake8\n        args: [\"--config=setup.cfg\", \"--ignore=W504, W503\"]\n\n  # modify known_third_party\n  - repo: https://github.com/asottile/seed-isort-config\n    rev: v2.2.0\n    hooks:\n      - id: seed-isort-config\n\n  # isort\n  - repo: https://github.com/timothycrosley/isort\n    rev: 5.2.2\n    hooks:\n      - id: isort\n\n  # yapf\n  - repo: https://github.com/pre-commit/mirrors-yapf\n    rev: v0.30.0\n    hooks:\n      - id: yapf\n\n  # codespell\n  - repo: https://github.com/codespell-project/codespell\n    rev: v2.1.0\n    hooks:\n      - id: codespell\n\n  # pre-commit-hooks\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v3.2.0\n    hooks:\n      - id: trailing-whitespace  # Trim trailing whitespace\n      - id: check-yaml  # Attempt to load all yaml files to verify syntax\n      - id: check-merge-conflict  # Check for files that contain merge conflict strings\n      - id: double-quote-string-fixer  # Replace double quoted strings with single quoted strings\n      - id: end-of-file-fixer  # Make sure files end in a newline and only a newline\n      - id: requirements-txt-fixer  # Sort entries in requirements.txt and remove incorrect entry for pkg-resources==0.0.0\n      - id: fix-encoding-pragma  # Remove the coding pragma: # -*- coding: utf-8 -*-\n        args: [\"--remove\"]\n      - id: mixed-line-ending  # Replace or check mixed line ending\n        args: [\"--fix=lf\"]\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.1279296875,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nxintao.wang@outlook.com or xintaowang@tencent.com.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior,  harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.\n"
        },
        {
          "name": "Comparisons.md",
          "type": "blob",
          "size": 3.9619140625,
          "content": "# Comparisons\n\n## Comparisons among different model versions\n\nNote that V1.3 is not always better than V1.2. You may need to try different models based on your purpose and inputs.\n\n| Version | Strengths  | Weaknesses |\n| :---: | :---:        |     :---:      |\n|V1.3 |  ‚úì natural outputs<br> ‚úìbetter results on very low-quality inputs <br> ‚úì work on relatively high-quality inputs <br>‚úì can have repeated (twice) restorations | ‚úó not very sharp <br> ‚úó have a slight change on identity |\n|V1.2 |  ‚úì sharper output <br> ‚úì with beauty makeup | ‚úó some outputs are unnatural|\n\nFor the following images, you may need to **zoom in** for comparing details, or **click the image** to see in the full size.\n\n| Input | V1  | V1.2 | V1.3\n| :---: | :---:        |     :---:      |  :---:      |\n|![019_Anne_Hathaway_01_00](https://user-images.githubusercontent.com/17445847/153762146-96b25999-4ddd-42a5-a3fe-bb90565f4c4f.png)|  ![](https://user-images.githubusercontent.com/17445847/153762256-ef41e749-5a27-495c-8a9c-d8403be55869.png)  | ![](https://user-images.githubusercontent.com/17445847/153762297-d41582fc-6253-4e7e-a1ce-4dc237ae3bf3.png)   | ![](https://user-images.githubusercontent.com/17445847/153762215-e0535e94-b5ba-426e-97b5-35c00873604d.png)  |\n| ![106_Harry_Styles_00_00](https://user-images.githubusercontent.com/17445847/153789040-632c0eda-c15a-43e9-a63c-9ead64f92d4a.png) | ![](https://user-images.githubusercontent.com/17445847/153789172-93cd4980-5318-4633-a07e-1c8f8064ff89.png) | ![](https://user-images.githubusercontent.com/17445847/153789185-f7b268a7-d1db-47b0-ae4a-335e5d657a18.png) | ![](https://user-images.githubusercontent.com/17445847/153789198-7c7f3bca-0ef0-4494-92f0-20aa6f7d7464.png)|\n| ![076_Paris_Hilton_00_00](https://user-images.githubusercontent.com/17445847/153789607-86387770-9db8-441f-b08a-c9679b121b85.png) | ![](https://user-images.githubusercontent.com/17445847/153789619-e56b438a-78a0-425d-8f44-ec4692a43dda.png) | ![](https://user-images.githubusercontent.com/17445847/153789633-5b28f778-3b7f-4e08-8a1d-740ca6e82d8a.png) | ![](https://user-images.githubusercontent.com/17445847/153789645-bc623f21-b32d-4fc3-bfe9-61203407a180.png)|\n| ![008_George_Clooney_00_00](https://user-images.githubusercontent.com/17445847/153790017-0c3ca94d-1c9d-4a0e-b539-ab12d4da98ff.png) | ![](https://user-images.githubusercontent.com/17445847/153790028-fb0d38ab-399d-4a30-8154-2dcd72ca90e8.png) | ![](https://user-images.githubusercontent.com/17445847/153790044-1ef68e34-6120-4439-a5d9-0b6cdbe9c3d0.png) | ![](https://user-images.githubusercontent.com/17445847/153790059-a8d3cece-8989-4e9a-9ffe-903e1690cfd6.png)|\n| ![057_Madonna_01_00](https://user-images.githubusercontent.com/17445847/153790624-2d0751d0-8fb4-4806-be9d-71b833c2c226.png) | ![](https://user-images.githubusercontent.com/17445847/153790639-7eb870e5-26b2-41dc-b139-b698bb40e6e6.png) | ![](https://user-images.githubusercontent.com/17445847/153790651-86899b7a-a1b6-4242-9e8a-77b462004998.png) | ![](https://user-images.githubusercontent.com/17445847/153790655-c8f6c25b-9b4e-4633-b16f-c43da86cff8f.png)|\n| ![044_Amy_Schumer_01_00](https://user-images.githubusercontent.com/17445847/153790811-3fb4fc46-5b4f-45fe-8fcb-a128de2bfa60.png) | ![](https://user-images.githubusercontent.com/17445847/153790817-d45aa4ff-bfc4-4163-b462-75eef9426fab.png) | ![](https://user-images.githubusercontent.com/17445847/153790824-5f93c3a0-fe5a-42f6-8b4b-5a5de8cd0ac3.png) | ![](https://user-images.githubusercontent.com/17445847/153790835-0edf9944-05c7-41c4-8581-4dc5ffc56c9d.png)|\n| ![012_Jackie_Chan_01_00](https://user-images.githubusercontent.com/17445847/153791176-737b016a-e94f-4898-8db7-43e7762141c9.png) | ![](https://user-images.githubusercontent.com/17445847/153791183-2f25a723-56bf-4cd5-aafe-a35513a6d1c5.png) | ![](https://user-images.githubusercontent.com/17445847/153791194-93416cf9-2b58-4e70-b806-27e14c58d4fd.png) | ![](https://user-images.githubusercontent.com/17445847/153791202-aa98659c-b702-4bce-9c47-a2fa5eccc5ae.png)|\n\n<!-- | ![]() | ![]() | ![]() | ![]()|  -->\n"
        },
        {
          "name": "FAQ.md",
          "type": "blob",
          "size": 0.4140625,
          "content": "# FAQ\n\n1. **How to finetune the GFPGANCleanv1-NoCE-C2 (v1.2) model**\n\n**A:** 1) The GFPGANCleanv1-NoCE-C2 (v1.2) model uses the *clean* architecture, which is more friendly for deploying.\n2) This model is not directly trained. Instead, it is converted from another *bilinear* model.\n3) If you want to finetune the GFPGANCleanv1-NoCE-C2 (v1.2), you need to finetune its original *bilinear* model, and then do the conversion.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 22.451171875,
          "content": "Tencent is pleased to support the open source community by making GFPGAN available.\n\nCopyright (C) 2021 THL A29 Limited, a Tencent company.  All rights reserved.\n\nGFPGAN is licensed under the Apache License Version 2.0 except for the third-party components listed below.\n\n\nTerms of the Apache License Version 2.0:\n---------------------------------------------\nApache License\n\nVersion 2.0, January 2004\n\nhttp://www.apache.org/licenses/\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n1. Definitions.\n\n‚ÄúLicense‚Äù shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\n\n‚ÄúLicensor‚Äù shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\n\n‚ÄúLegal Entity‚Äù shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, ‚Äúcontrol‚Äù means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\n\n‚ÄúYou‚Äù (or ‚ÄúYour‚Äù) shall mean an individual or Legal Entity exercising permissions granted by this License.\n\n‚ÄúSource‚Äù form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n\n‚ÄúObject‚Äù form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n\n‚ÄúWork‚Äù shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\n\n‚ÄúDerivative Works‚Äù shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\n\n‚ÄúContribution‚Äù shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, ‚Äúsubmitted‚Äù means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as ‚ÄúNot a Contribution.‚Äù\n\n‚ÄúContributor‚Äù shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n\nYou must give any other recipients of the Work or Derivative Works a copy of this License; and\n\nYou must cause any modified files to carry prominent notices stating that You changed the files; and\n\nYou must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and\n\nIf the Work includes a ‚ÄúNOTICE‚Äù text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.\n\nYou may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.\n\n5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\n\n\nOther  dependencies and licenses:\n\n\nOpen Source Software licensed under the Apache 2.0 license and Other Licenses of the Third-Party Components therein:\n---------------------------------------------\n1. basicsr\nCopyright 2018-2020 BasicSR Authors\n\n\nThis BasicSR project is released under the Apache 2.0 license.\n\nA copy of Apache 2.0 is included in this file.\n\nStyleGAN2\nThe codes are modified from the repository stylegan2-pytorch. Many thanks to the author - Kim Seonghyeon üòä for translating from the official TensorFlow codes to PyTorch ones. Here is the license of stylegan2-pytorch.\nThe official repository is https://github.com/NVlabs/stylegan2, and here is the NVIDIA license.\nDFDNet\nThe codes are largely modified from the repository DFDNet. Their license is Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\nTerms of the Nvidia License:\n---------------------------------------------\n\n1. Definitions\n\n\"Licensor\" means any person or entity that distributes its Work.\n\n\"Software\" means the original work of authorship made available under\nthis License.\n\n\"Work\" means the Software and any additions to or derivative works of\nthe Software that are made available under this License.\n\n\"Nvidia Processors\" means any central processing unit (CPU), graphics\nprocessing unit (GPU), field-programmable gate array (FPGA),\napplication-specific integrated circuit (ASIC) or any combination\nthereof designed, made, sold, or provided by Nvidia or its affiliates.\n\nThe terms \"reproduce,\" \"reproduction,\" \"derivative works,\" and\n\"distribution\" have the meaning as provided under U.S. copyright law;\nprovided, however, that for the purposes of this License, derivative\nworks shall not include works that remain separable from, or merely\nlink (or bind by name) to the interfaces of, the Work.\n\nWorks, including the Software, are \"made available\" under this License\nby including in or with the Work either (a) a copyright notice\nreferencing the applicability of this License to the Work, or (b) a\ncopy of this License.\n\n2. License Grants\n\n    2.1 Copyright Grant. Subject to the terms and conditions of this\n    License, each Licensor grants to you a perpetual, worldwide,\n    non-exclusive, royalty-free, copyright license to reproduce,\n    prepare derivative works of, publicly display, publicly perform,\n    sublicense and distribute its Work and any resulting derivative\n    works in any form.\n\n3. Limitations\n\n    3.1 Redistribution. You may reproduce or distribute the Work only\n    if (a) you do so under this License, (b) you include a complete\n    copy of this License with your distribution, and (c) you retain\n    without modification any copyright, patent, trademark, or\n    attribution notices that are present in the Work.\n\n    3.2 Derivative Works. You may specify that additional or different\n    terms apply to the use, reproduction, and distribution of your\n    derivative works of the Work (\"Your Terms\") only if (a) Your Terms\n    provide that the use limitation in Section 3.3 applies to your\n    derivative works, and (b) you identify the specific derivative\n    works that are subject to Your Terms. Notwithstanding Your Terms,\n    this License (including the redistribution requirements in Section\n    3.1) will continue to apply to the Work itself.\n\n    3.3 Use Limitation. The Work and any derivative works thereof only\n    may be used or intended for use non-commercially. The Work or\n    derivative works thereof may be used or intended for use by Nvidia\n    or its affiliates commercially or non-commercially. As used herein,\n    \"non-commercially\" means for research or evaluation purposes only.\n\n    3.4 Patent Claims. If you bring or threaten to bring a patent claim\n    against any Licensor (including any claim, cross-claim or\n    counterclaim in a lawsuit) to enforce any patents that you allege\n    are infringed by any Work, then your rights under this License from\n    such Licensor (including the grants in Sections 2.1 and 2.2) will\n    terminate immediately.\n\n    3.5 Trademarks. This License does not grant any rights to use any\n    Licensor's or its affiliates' names, logos, or trademarks, except\n    as necessary to reproduce the notices described in this License.\n\n    3.6 Termination. If you violate any term of this License, then your\n    rights under this License (including the grants in Sections 2.1 and\n    2.2) will terminate immediately.\n\n4. Disclaimer of Warranty.\n\nTHE WORK IS PROVIDED \"AS IS\" WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, EITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OR CONDITIONS OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR\nNON-INFRINGEMENT. YOU BEAR THE RISK OF UNDERTAKING ANY ACTIVITIES UNDER\nTHIS LICENSE.\n\n5. Limitation of Liability.\n\nEXCEPT AS PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL\nTHEORY, WHETHER IN TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE\nSHALL ANY LICENSOR BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY DIRECT,\nINDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF\nOR RELATED TO THIS LICENSE, THE USE OR INABILITY TO USE THE WORK\n(INCLUDING BUT NOT LIMITED TO LOSS OF GOODWILL, BUSINESS INTERRUPTION,\nLOST PROFITS OR DATA, COMPUTER FAILURE OR MALFUNCTION, OR ANY OTHER\nCOMMERCIAL DAMAGES OR LOSSES), EVEN IF THE LICENSOR HAS BEEN ADVISED OF\nTHE POSSIBILITY OF SUCH DAMAGES.\n\nMIT License\n\nCopyright (c) 2019 Kim Seonghyeon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n\n\nOpen Source Software licensed under the BSD 3-Clause license:\n---------------------------------------------\n1. torchvision\nCopyright (c) Soumith Chintala 2016,\nAll rights reserved.\n\n2. torch\nCopyright (c) 2016-     Facebook, Inc            (Adam Paszke)\nCopyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\nCopyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\nCopyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\nCopyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\nCopyright (c) 2011-2013 NYU                      (Clement Farabet)\nCopyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\nCopyright (c) 2006      Idiap Research Institute (Samy Bengio)\nCopyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n\n\nTerms of the BSD 3-Clause License:\n---------------------------------------------\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ‚ÄúAS IS‚Äù AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\nOpen Source Software licensed under the BSD 3-Clause License and Other Licenses of the Third-Party Components therein:\n---------------------------------------------\n1. numpy\nCopyright (c) 2005-2020, NumPy Developers.\nAll rights reserved.\n\nA copy of BSD 3-Clause License is included in this file.\n\nThe NumPy repository and source distributions bundle several libraries that are\ncompatibly licensed.  We list these here.\n\nName: Numpydoc\nFiles: doc/sphinxext/numpydoc/*\nLicense: BSD-2-Clause\n  For details, see doc/sphinxext/LICENSE.txt\n\nName: scipy-sphinx-theme\nFiles: doc/scipy-sphinx-theme/*\nLicense: BSD-3-Clause AND PSF-2.0 AND Apache-2.0\n  For details, see doc/scipy-sphinx-theme/LICENSE.txt\n\nName: lapack-lite\nFiles: numpy/linalg/lapack_lite/*\nLicense: BSD-3-Clause\n  For details, see numpy/linalg/lapack_lite/LICENSE.txt\n\nName: tempita\nFiles: tools/npy_tempita/*\nLicense: MIT\n  For details, see tools/npy_tempita/license.txt\n\nName: dragon4\nFiles: numpy/core/src/multiarray/dragon4.c\nLicense: MIT\n  For license text, see numpy/core/src/multiarray/dragon4.c\n\n\n\nOpen Source Software licensed under the MIT license:\n---------------------------------------------\n1. facexlib\nCopyright (c) 2020 Xintao Wang\n\n2. opencv-python\nCopyright (c) Olli-Pekka Heinisuo\nPlease note that only files in cv2 package are used.\n\n\nTerms of the MIT License:\n---------------------------------------------\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\nOpen Source Software licensed under the MIT license and Other Licenses of the Third-Party Components therein:\n---------------------------------------------\n1. tqdm\nCopyright (c) 2013 noamraph\n\n`tqdm` is a product of collaborative work.\nUnless otherwise stated, all authors (see commit logs) retain copyright\nfor their respective work, and release the work under the MIT licence\n(text below).\n\nExceptions or notable authors are listed below\nin reverse chronological order:\n\n* files: *\n  MPLv2.0 2015-2020 (c) Casper da Costa-Luis\n  [casperdcl](https://github.com/casperdcl).\n* files: tqdm/_tqdm.py\n  MIT 2016 (c) [PR #96] on behalf of Google Inc.\n* files: tqdm/_tqdm.py setup.py README.rst MANIFEST.in .gitignore\n  MIT 2013 (c) Noam Yorav-Raphael, original author.\n\n[PR #96]: https://github.com/tqdm/tqdm/pull/96\n\n\nMozilla Public Licence (MPL) v. 2.0 - Exhibit A\n-----------------------------------------------\n\nThis Source Code Form is subject to the terms of the\nMozilla Public License, v. 2.0.\nIf a copy of the MPL was not distributed with this file,\nYou can obtain one at https://mozilla.org/MPL/2.0/.\n\n\nMIT License (MIT)\n-----------------\n\nCopyright (c) 2013 noamraph\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.1689453125,
          "content": "include assets/*\ninclude inputs/*\ninclude scripts/*.py\ninclude inference_gfpgan.py\ninclude VERSION\ninclude LICENSE\ninclude requirements.txt\ninclude gfpgan/weights/README.md\n"
        },
        {
          "name": "PaperModel.md",
          "type": "blob",
          "size": 2.7578125,
          "content": "# Installation\n\nWe now provide a *clean* version of GFPGAN, which does not require customized CUDA extensions. See [here](README.md#installation) for this easier installation.<br>\nIf you want want to use the original model in our paper, please follow the instructions below.\n\n1. Clone repo\n\n    ```bash\n    git clone https://github.com/xinntao/GFPGAN.git\n    cd GFPGAN\n    ```\n\n1. Install dependent packages\n\n    As StyleGAN2 uses customized PyTorch C++ extensions, you need to **compile them during installation** or **load them just-in-time(JIT)**.\n    You can refer to [BasicSR-INSTALL.md](https://github.com/xinntao/BasicSR/blob/master/INSTALL.md) for more details.\n\n    **Option 1: Load extensions just-in-time(JIT)** (For those just want to do simple inferences, may have less issues)\n\n    ```bash\n    # Install basicsr - https://github.com/xinntao/BasicSR\n    # We use BasicSR for both training and inference\n    pip install basicsr\n\n    # Install facexlib - https://github.com/xinntao/facexlib\n    # We use face detection and face restoration helper in the facexlib package\n    pip install facexlib\n\n    pip install -r requirements.txt\n    python setup.py develop\n\n    # remember to set BASICSR_JIT=True before your running commands\n    ```\n\n    **Option 2: Compile extensions during installation** (For those need to train/inference for many times)\n\n    ```bash\n    # Install basicsr - https://github.com/xinntao/BasicSR\n    # We use BasicSR for both training and inference\n    # Set BASICSR_EXT=True to compile the cuda extensions in the BasicSR - It may take several minutes to compile, please be patient\n    # Add -vvv for detailed log prints\n    BASICSR_EXT=True pip install basicsr -vvv\n\n    # Install facexlib - https://github.com/xinntao/facexlib\n    # We use face detection and face restoration helper in the facexlib package\n    pip install facexlib\n\n    pip install -r requirements.txt\n    python setup.py develop\n    ```\n\n## :zap: Quick Inference\n\nDownload pre-trained models: [GFPGANv1.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth)\n\n```bash\nwget https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth -P experiments/pretrained_models\n```\n\n- Option 1: Load extensions just-in-time(JIT)\n\n    ```bash\n    BASICSR_JIT=True python inference_gfpgan.py --input inputs/whole_imgs --output results --version 1\n\n    # for aligned images\n    BASICSR_JIT=True python inference_gfpgan.py --input inputs/whole_imgs --output results --version 1 --aligned\n    ```\n\n- Option 2: Have successfully compiled extensions during installation\n\n    ```bash\n    python inference_gfpgan.py --input inputs/whole_imgs --output results --version 1\n\n    # for aligned images\n    python inference_gfpgan.py --input inputs/whole_imgs --output results --version 1 --aligned\n    ```\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.46875,
          "content": "<p align=\"center\">\n  <img src=\"assets/gfpgan_logo.png\" height=130>\n</p>\n\n## <div align=\"center\"><b><a href=\"README.md\">English</a> | <a href=\"README_CN.md\">ÁÆÄ‰Ωì‰∏≠Êñá</a></b></div>\n\n<div align=\"center\">\n<!-- <a href=\"https://twitter.com/_Xintao_\" style=\"text-decoration:none;\">\n    <img src=\"https://user-images.githubusercontent.com/17445847/187162058-c764ced6-952f-404b-ac85-ba95cce18e7b.png\" width=\"4%\" alt=\"\" />\n</a> -->\n\n[![download](https://img.shields.io/github/downloads/TencentARC/GFPGAN/total.svg)](https://github.com/TencentARC/GFPGAN/releases)\n[![PyPI](https://img.shields.io/pypi/v/gfpgan)](https://pypi.org/project/gfpgan/)\n[![Open issue](https://img.shields.io/github/issues/TencentARC/GFPGAN)](https://github.com/TencentARC/GFPGAN/issues)\n[![Closed issue](https://img.shields.io/github/issues-closed/TencentARC/GFPGAN)](https://github.com/TencentARC/GFPGAN/issues)\n[![LICENSE](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/TencentARC/GFPGAN/blob/master/LICENSE)\n[![python lint](https://github.com/TencentARC/GFPGAN/actions/workflows/pylint.yml/badge.svg)](https://github.com/TencentARC/GFPGAN/blob/master/.github/workflows/pylint.yml)\n[![Publish-pip](https://github.com/TencentARC/GFPGAN/actions/workflows/publish-pip.yml/badge.svg)](https://github.com/TencentARC/GFPGAN/blob/master/.github/workflows/publish-pip.yml)\n</div>\n\n1. :boom: **Updated** online demo: [![Replicate](https://img.shields.io/static/v1?label=Demo&message=Replicate&color=blue)](https://replicate.com/tencentarc/gfpgan). Here is the [backup](https://replicate.com/xinntao/gfpgan).\n1. :boom: **Updated** online demo: [![Huggingface Gradio](https://img.shields.io/static/v1?label=Demo&message=Huggingface%20Gradio&color=orange)](https://huggingface.co/spaces/Xintao/GFPGAN)\n1. [Colab Demo](https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo) for GFPGAN <a href=\"https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"google colab logo\"></a>; (Another [Colab Demo](https://colab.research.google.com/drive/1Oa1WwKB4M4l1GmR7CtswDVgOCOeSLChA?usp=sharing) for the original paper model)\n\n<!-- 3. Online demo: [Replicate.ai](https://replicate.com/xinntao/gfpgan) (may need to sign in, return the whole image)\n4. Online demo: [Baseten.co](https://app.baseten.co/applications/Q04Lz0d/operator_views/8qZG6Bg) (backed by GPU, returns the whole image)\n5. We provide a *clean* version of GFPGAN, which can run without CUDA extensions. So that it can run in **Windows** or on **CPU mode**. -->\n\n> :rocket: **Thanks for your interest in our work. You may also want to check our new updates on the *tiny models* for *anime images and videos* in [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN/blob/master/docs/anime_video_model.md)** :blush:\n\nGFPGAN aims at developing a **Practical Algorithm for Real-world Face Restoration**.<br>\nIt leverages rich and diverse priors encapsulated in a pretrained face GAN (*e.g.*, StyleGAN2) for blind face restoration.\n\n:question: Frequently Asked Questions can be found in [FAQ.md](FAQ.md).\n\n:triangular_flag_on_post: **Updates**\n\n- :white_check_mark: Add [RestoreFormer](https://github.com/wzhouxiff/RestoreFormer) inference codes.\n- :white_check_mark: Add [V1.4 model](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth), which produces slightly more details and better identity than V1.3.\n- :white_check_mark: Add **[V1.3 model](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth)**, which produces **more natural** restoration results, and better results on *very low-quality* / *high-quality* inputs. See more in [Model zoo](#european_castle-model-zoo), [Comparisons.md](Comparisons.md)\n- :white_check_mark: Integrated to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/GFPGAN).\n- :white_check_mark: Support enhancing non-face regions (background) with [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN).\n- :white_check_mark: We provide a *clean* version of GFPGAN, which does not require CUDA extensions.\n- :white_check_mark: We provide an updated model without colorizing faces.\n\n---\n\nIf GFPGAN is helpful in your photos/projects, please help to :star: this repo or recommend it to your friends. Thanks:blush:\nOther recommended projects:<br>\n:arrow_forward: [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN): A practical algorithm for general image restoration<br>\n:arrow_forward: [BasicSR](https://github.com/xinntao/BasicSR): An open-source image and video restoration toolbox<br>\n:arrow_forward: [facexlib](https://github.com/xinntao/facexlib): A collection that provides useful face-relation functions<br>\n:arrow_forward: [HandyView](https://github.com/xinntao/HandyView): A PyQt5-based image viewer that is handy for view and comparison<br>\n\n---\n\n### :book: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior\n\n> [[Paper](https://arxiv.org/abs/2101.04061)] &emsp; [[Project Page](https://xinntao.github.io/projects/gfpgan)] &emsp; [Demo] <br>\n> [Xintao Wang](https://xinntao.github.io/), [Yu Li](https://yu-li.github.io/), [Honglun Zhang](https://scholar.google.com/citations?hl=en&user=KjQLROoAAAAJ), [Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en) <br>\n> Applied Research Center (ARC), Tencent PCG\n\n<p align=\"center\">\n  <img src=\"https://xinntao.github.io/projects/GFPGAN_src/gfpgan_teaser.jpg\">\n</p>\n\n---\n\n## :wrench: Dependencies and Installation\n\n- Python >= 3.7 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html))\n- [PyTorch >= 1.7](https://pytorch.org/)\n- Option: NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads)\n- Option: Linux\n\n### Installation\n\nWe now provide a *clean* version of GFPGAN, which does not require customized CUDA extensions. <br>\nIf you want to use the original model in our paper, please see [PaperModel.md](PaperModel.md) for installation.\n\n1. Clone repo\n\n    ```bash\n    git clone https://github.com/TencentARC/GFPGAN.git\n    cd GFPGAN\n    ```\n\n1. Install dependent packages\n\n    ```bash\n    # Install basicsr - https://github.com/xinntao/BasicSR\n    # We use BasicSR for both training and inference\n    pip install basicsr\n\n    # Install facexlib - https://github.com/xinntao/facexlib\n    # We use face detection and face restoration helper in the facexlib package\n    pip install facexlib\n\n    pip install -r requirements.txt\n    python setup.py develop\n\n    # If you want to enhance the background (non-face) regions with Real-ESRGAN,\n    # you also need to install the realesrgan package\n    pip install realesrgan\n    ```\n\n## :zap: Quick Inference\n\nWe take the v1.3 version for an example. More models can be found [here](#european_castle-model-zoo).\n\nDownload pre-trained models: [GFPGANv1.3.pth](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth)\n\n```bash\nwget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n```\n\n**Inference!**\n\n```bash\npython inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2\n```\n\n```console\nUsage: python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2 [options]...\n\n  -h                   show this help\n  -i input             Input image or folder. Default: inputs/whole_imgs\n  -o output            Output folder. Default: results\n  -v version           GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3\n  -s upscale           The final upsampling scale of the image. Default: 2\n  -bg_upsampler        background upsampler. Default: realesrgan\n  -bg_tile             Tile size for background sampler, 0 for no tile during testing. Default: 400\n  -suffix              Suffix of the restored faces\n  -only_center_face    Only restore the center face\n  -aligned             Input are aligned faces\n  -ext                 Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto\n```\n\nIf you want to use the original model in our paper, please see [PaperModel.md](PaperModel.md) for installation and inference.\n\n## :european_castle: Model Zoo\n\n| Version | Model Name  | Description |\n| :---: | :---:        |     :---:      |\n| V1.3 | [GFPGANv1.3.pth](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth) | Based on V1.2; **more natural** restoration results; better results on very low-quality / high-quality inputs. |\n| V1.2 | [GFPGANCleanv1-NoCE-C2.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth) | No colorization; no CUDA extensions are required. Trained with more data with pre-processing. |\n| V1 | [GFPGANv1.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth) | The paper model, with colorization. |\n\nThe comparisons are in [Comparisons.md](Comparisons.md).\n\nNote that V1.3 is not always better than V1.2. You may need to select different models based on your purpose and inputs.\n\n| Version | Strengths  | Weaknesses |\n| :---: | :---:        |     :---:      |\n|V1.3 |  ‚úì natural outputs<br> ‚úìbetter results on very low-quality inputs <br> ‚úì work on relatively high-quality inputs <br>‚úì can have repeated (twice) restorations | ‚úó not very sharp <br> ‚úó have a slight change on identity |\n|V1.2 |  ‚úì sharper output <br> ‚úì with beauty makeup | ‚úó some outputs are unnatural |\n\nYou can find **more models (such as the discriminators)** here: [[Google Drive](https://drive.google.com/drive/folders/17rLiFzcUMoQuhLnptDsKolegHWwJOnHu?usp=sharing)], OR [[Tencent Cloud ËÖæËÆØÂæÆ‰∫ë](https://share.weiyun.com/ShYoCCoc)]\n\n## :computer: Training\n\nWe provide the training codes for GFPGAN (used in our paper). <br>\nYou could improve it according to your own needs.\n\n**Tips**\n\n1. More high quality faces can improve the restoration quality.\n2. You may need to perform some pre-processing, such as beauty makeup.\n\n**Procedures**\n\n(You can try a simple version ( `options/train_gfpgan_v1_simple.yml`) that does not require face component landmarks.)\n\n1. Dataset preparation: [FFHQ](https://github.com/NVlabs/ffhq-dataset)\n\n1. Download pre-trained models and other data. Put them in the `experiments/pretrained_models` folder.\n    1. [Pre-trained StyleGAN2 model: StyleGAN2_512_Cmul1_FFHQ_B12G4_scratch_800k.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/StyleGAN2_512_Cmul1_FFHQ_B12G4_scratch_800k.pth)\n    1. [Component locations of FFHQ: FFHQ_eye_mouth_landmarks_512.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/FFHQ_eye_mouth_landmarks_512.pth)\n    1. [A simple ArcFace model: arcface_resnet18.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/arcface_resnet18.pth)\n\n1. Modify the configuration file `options/train_gfpgan_v1.yml` accordingly.\n\n1. Training\n\n> python -m torch.distributed.launch --nproc_per_node=4 --master_port=22021 gfpgan/train.py -opt options/train_gfpgan_v1.yml --launcher pytorch\n\n## :scroll: License and Acknowledgement\n\nGFPGAN is released under Apache License Version 2.0.\n\n## BibTeX\n\n    @InProceedings{wang2021gfpgan,\n        author = {Xintao Wang and Yu Li and Honglun Zhang and Ying Shan},\n        title = {Towards Real-World Blind Face Restoration with Generative Facial Prior},\n        booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n        year = {2021}\n    }\n\n## :e-mail: Contact\n\nIf you have any question, please email `xintao.wang@outlook.com` or `xintaowang@tencent.com`.\n"
        },
        {
          "name": "README_CN.md",
          "type": "blob",
          "size": 0.2099609375,
          "content": "<p align=\"center\">\n  <img src=\"assets/gfpgan_logo.png\" height=130>\n</p>\n\n## <div align=\"center\"><b><a href=\"README.md\">English</a> | <a href=\"README_CN.md\">ÁÆÄ‰Ωì‰∏≠Êñá</a></b></div>\n\nËøòÊú™ÂÆåÂ∑•ÔºåÊ¨¢ËøéË¥°ÁåÆÔºÅ\n"
        },
        {
          "name": "VERSION",
          "type": "blob",
          "size": 0.005859375,
          "content": "1.3.8\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cog.yaml",
          "type": "blob",
          "size": 0.4697265625,
          "content": "# This file is used for constructing replicate env\nimage: \"r8.im/tencentarc/gfpgan\"\n\nbuild:\n  gpu: true\n  python_version: \"3.8\"\n  system_packages:\n    - \"libgl1-mesa-glx\"\n    - \"libglib2.0-0\"\n  python_packages:\n    - \"torch==1.7.1\"\n    - \"torchvision==0.8.2\"\n    - \"numpy==1.21.1\"\n    - \"lmdb==1.2.1\"\n    - \"opencv-python==4.5.3.56\"\n    - \"PyYAML==5.4.1\"\n    - \"tqdm==4.62.2\"\n    - \"yapf==0.31.0\"\n    - \"basicsr==1.4.2\"\n    - \"facexlib==0.2.5\"\n\npredict: \"cog_predict.py:Predictor\"\n"
        },
        {
          "name": "cog_predict.py",
          "type": "blob",
          "size": 6.568359375,
          "content": "# flake8: noqa\n# This file is used for deploying replicate models\n# running: cog predict -i img=@inputs/whole_imgs/10045.png -i version='v1.4' -i scale=2\n# push: cog push r8.im/tencentarc/gfpgan\n# push (backup): cog push r8.im/xinntao/gfpgan\n\nimport os\n\nos.system('python setup.py develop')\nos.system('pip install realesrgan')\n\nimport cv2\nimport shutil\nimport tempfile\nimport torch\nfrom basicsr.archs.srvgg_arch import SRVGGNetCompact\n\nfrom gfpgan import GFPGANer\n\ntry:\n    from cog import BasePredictor, Input, Path\n    from realesrgan.utils import RealESRGANer\nexcept Exception:\n    print('please install cog and realesrgan package')\n\n\nclass Predictor(BasePredictor):\n\n    def setup(self):\n        os.makedirs('output', exist_ok=True)\n        # download weights\n        if not os.path.exists('gfpgan/weights/realesr-general-x4v3.pth'):\n            os.system(\n                'wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth -P ./gfpgan/weights'\n            )\n        if not os.path.exists('gfpgan/weights/GFPGANv1.2.pth'):\n            os.system(\n                'wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.2.pth -P ./gfpgan/weights')\n        if not os.path.exists('gfpgan/weights/GFPGANv1.3.pth'):\n            os.system(\n                'wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P ./gfpgan/weights')\n        if not os.path.exists('gfpgan/weights/GFPGANv1.4.pth'):\n            os.system(\n                'wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth -P ./gfpgan/weights')\n        if not os.path.exists('gfpgan/weights/RestoreFormer.pth'):\n            os.system(\n                'wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/RestoreFormer.pth -P ./gfpgan/weights'\n            )\n\n        # background enhancer with RealESRGAN\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')\n        model_path = 'gfpgan/weights/realesr-general-x4v3.pth'\n        half = True if torch.cuda.is_available() else False\n        self.upsampler = RealESRGANer(\n            scale=4, model_path=model_path, model=model, tile=0, tile_pad=10, pre_pad=0, half=half)\n\n        # Use GFPGAN for face enhancement\n        self.face_enhancer = GFPGANer(\n            model_path='gfpgan/weights/GFPGANv1.4.pth',\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=self.upsampler)\n        self.current_version = 'v1.4'\n\n    def predict(\n            self,\n            img: Path = Input(description='Input'),\n            version: str = Input(\n                description='GFPGAN version. v1.3: better quality. v1.4: more details and better identity.',\n                choices=['v1.2', 'v1.3', 'v1.4', 'RestoreFormer'],\n                default='v1.4'),\n            scale: float = Input(description='Rescaling factor', default=2),\n    ) -> Path:\n        weight = 0.5\n        print(img, version, scale, weight)\n        try:\n            extension = os.path.splitext(os.path.basename(str(img)))[1]\n            img = cv2.imread(str(img), cv2.IMREAD_UNCHANGED)\n            if len(img.shape) == 3 and img.shape[2] == 4:\n                img_mode = 'RGBA'\n            elif len(img.shape) == 2:\n                img_mode = None\n                img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n            else:\n                img_mode = None\n\n            h, w = img.shape[0:2]\n            if h < 300:\n                img = cv2.resize(img, (w * 2, h * 2), interpolation=cv2.INTER_LANCZOS4)\n\n            if self.current_version != version:\n                if version == 'v1.2':\n                    self.face_enhancer = GFPGANer(\n                        model_path='gfpgan/weights/GFPGANv1.2.pth',\n                        upscale=2,\n                        arch='clean',\n                        channel_multiplier=2,\n                        bg_upsampler=self.upsampler)\n                    self.current_version = 'v1.2'\n                elif version == 'v1.3':\n                    self.face_enhancer = GFPGANer(\n                        model_path='gfpgan/weights/GFPGANv1.3.pth',\n                        upscale=2,\n                        arch='clean',\n                        channel_multiplier=2,\n                        bg_upsampler=self.upsampler)\n                    self.current_version = 'v1.3'\n                elif version == 'v1.4':\n                    self.face_enhancer = GFPGANer(\n                        model_path='gfpgan/weights/GFPGANv1.4.pth',\n                        upscale=2,\n                        arch='clean',\n                        channel_multiplier=2,\n                        bg_upsampler=self.upsampler)\n                    self.current_version = 'v1.4'\n                elif version == 'RestoreFormer':\n                    self.face_enhancer = GFPGANer(\n                        model_path='gfpgan/weights/RestoreFormer.pth',\n                        upscale=2,\n                        arch='RestoreFormer',\n                        channel_multiplier=2,\n                        bg_upsampler=self.upsampler)\n\n            try:\n                _, _, output = self.face_enhancer.enhance(\n                    img, has_aligned=False, only_center_face=False, paste_back=True, weight=weight)\n            except RuntimeError as error:\n                print('Error', error)\n\n            try:\n                if scale != 2:\n                    interpolation = cv2.INTER_AREA if scale < 2 else cv2.INTER_LANCZOS4\n                    h, w = img.shape[0:2]\n                    output = cv2.resize(output, (int(w * scale / 2), int(h * scale / 2)), interpolation=interpolation)\n            except Exception as error:\n                print('wrong scale input.', error)\n\n            if img_mode == 'RGBA':  # RGBA images should be saved in png format\n                extension = 'png'\n            # save_path = f'output/out.{extension}'\n            # cv2.imwrite(save_path, output)\n            out_path = Path(tempfile.mkdtemp()) / f'out.{extension}'\n            cv2.imwrite(str(out_path), output)\n        except Exception as error:\n            print('global exception: ', error)\n        finally:\n            clean_folder('output')\n        return out_path\n\n\ndef clean_folder(folder):\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(f'Failed to delete {file_path}. Reason: {e}')\n"
        },
        {
          "name": "experiments",
          "type": "tree",
          "content": null
        },
        {
          "name": "gfpgan",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference_gfpgan.py",
          "type": "blob",
          "size": 6.9736328125,
          "content": "import argparse\nimport cv2\nimport glob\nimport numpy as np\nimport os\nimport torch\nfrom basicsr.utils import imwrite\n\nfrom gfpgan import GFPGANer\n\n\ndef main():\n    \"\"\"Inference demo for GFPGAN (for users).\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '-i',\n        '--input',\n        type=str,\n        default='inputs/whole_imgs',\n        help='Input image or folder. Default: inputs/whole_imgs')\n    parser.add_argument('-o', '--output', type=str, default='results', help='Output folder. Default: results')\n    # we use version to select models, which is more user-friendly\n    parser.add_argument(\n        '-v', '--version', type=str, default='1.3', help='GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3')\n    parser.add_argument(\n        '-s', '--upscale', type=int, default=2, help='The final upsampling scale of the image. Default: 2')\n\n    parser.add_argument(\n        '--bg_upsampler', type=str, default='realesrgan', help='background upsampler. Default: realesrgan')\n    parser.add_argument(\n        '--bg_tile',\n        type=int,\n        default=400,\n        help='Tile size for background sampler, 0 for no tile during testing. Default: 400')\n    parser.add_argument('--suffix', type=str, default=None, help='Suffix of the restored faces')\n    parser.add_argument('--only_center_face', action='store_true', help='Only restore the center face')\n    parser.add_argument('--aligned', action='store_true', help='Input are aligned faces')\n    parser.add_argument(\n        '--ext',\n        type=str,\n        default='auto',\n        help='Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto')\n    parser.add_argument('-w', '--weight', type=float, default=0.5, help='Adjustable weights.')\n    args = parser.parse_args()\n\n    args = parser.parse_args()\n\n    # ------------------------ input & output ------------------------\n    if args.input.endswith('/'):\n        args.input = args.input[:-1]\n    if os.path.isfile(args.input):\n        img_list = [args.input]\n    else:\n        img_list = sorted(glob.glob(os.path.join(args.input, '*')))\n\n    os.makedirs(args.output, exist_ok=True)\n\n    # ------------------------ set up background upsampler ------------------------\n    if args.bg_upsampler == 'realesrgan':\n        if not torch.cuda.is_available():  # CPU\n            import warnings\n            warnings.warn('The unoptimized RealESRGAN is slow on CPU. We do not use it. '\n                          'If you really want to use it, please modify the corresponding codes.')\n            bg_upsampler = None\n        else:\n            from basicsr.archs.rrdbnet_arch import RRDBNet\n            from realesrgan import RealESRGANer\n            model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n            bg_upsampler = RealESRGANer(\n                scale=2,\n                model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth',\n                model=model,\n                tile=args.bg_tile,\n                tile_pad=10,\n                pre_pad=0,\n                half=True)  # need to set False in CPU mode\n    else:\n        bg_upsampler = None\n\n    # ------------------------ set up GFPGAN restorer ------------------------\n    if args.version == '1':\n        arch = 'original'\n        channel_multiplier = 1\n        model_name = 'GFPGANv1'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth'\n    elif args.version == '1.2':\n        arch = 'clean'\n        channel_multiplier = 2\n        model_name = 'GFPGANCleanv1-NoCE-C2'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth'\n    elif args.version == '1.3':\n        arch = 'clean'\n        channel_multiplier = 2\n        model_name = 'GFPGANv1.3'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth'\n    elif args.version == '1.4':\n        arch = 'clean'\n        channel_multiplier = 2\n        model_name = 'GFPGANv1.4'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth'\n    elif args.version == 'RestoreFormer':\n        arch = 'RestoreFormer'\n        channel_multiplier = 2\n        model_name = 'RestoreFormer'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/RestoreFormer.pth'\n    else:\n        raise ValueError(f'Wrong model version {args.version}.')\n\n    # determine model paths\n    model_path = os.path.join('experiments/pretrained_models', model_name + '.pth')\n    if not os.path.isfile(model_path):\n        model_path = os.path.join('gfpgan/weights', model_name + '.pth')\n    if not os.path.isfile(model_path):\n        # download pre-trained models from url\n        model_path = url\n\n    restorer = GFPGANer(\n        model_path=model_path,\n        upscale=args.upscale,\n        arch=arch,\n        channel_multiplier=channel_multiplier,\n        bg_upsampler=bg_upsampler)\n\n    # ------------------------ restore ------------------------\n    for img_path in img_list:\n        # read image\n        img_name = os.path.basename(img_path)\n        print(f'Processing {img_name} ...')\n        basename, ext = os.path.splitext(img_name)\n        input_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n\n        # restore faces and background if necessary\n        cropped_faces, restored_faces, restored_img = restorer.enhance(\n            input_img,\n            has_aligned=args.aligned,\n            only_center_face=args.only_center_face,\n            paste_back=True,\n            weight=args.weight)\n\n        # save faces\n        for idx, (cropped_face, restored_face) in enumerate(zip(cropped_faces, restored_faces)):\n            # save cropped face\n            save_crop_path = os.path.join(args.output, 'cropped_faces', f'{basename}_{idx:02d}.png')\n            imwrite(cropped_face, save_crop_path)\n            # save restored face\n            if args.suffix is not None:\n                save_face_name = f'{basename}_{idx:02d}_{args.suffix}.png'\n            else:\n                save_face_name = f'{basename}_{idx:02d}.png'\n            save_restore_path = os.path.join(args.output, 'restored_faces', save_face_name)\n            imwrite(restored_face, save_restore_path)\n            # save comparison image\n            cmp_img = np.concatenate((cropped_face, restored_face), axis=1)\n            imwrite(cmp_img, os.path.join(args.output, 'cmp', f'{basename}_{idx:02d}.png'))\n\n        # save restored img\n        if restored_img is not None:\n            if args.ext == 'auto':\n                extension = ext[1:]\n            else:\n                extension = args.ext\n\n            if args.suffix is not None:\n                save_restore_path = os.path.join(args.output, 'restored_imgs', f'{basename}_{args.suffix}.{extension}')\n            else:\n                save_restore_path = os.path.join(args.output, 'restored_imgs', f'{basename}.{extension}')\n            imwrite(restored_img, save_restore_path)\n\n    print(f'Results are in the [{args.output}] folder.')\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "inputs",
          "type": "tree",
          "content": null
        },
        {
          "name": "options",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1103515625,
          "content": "basicsr>=1.4.2\nfacexlib>=0.2.5\nlmdb\nnumpy\nopencv-python\npyyaml\nscipy\ntb-nightly\ntorch>=1.7\ntorchvision\ntqdm\nyapf\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.6689453125,
          "content": "[flake8]\nignore =\n    # line break before binary operator (W503)\n    W503,\n    # line break after binary operator (W504)\n    W504,\nmax-line-length=120\n\n[yapf]\nbased_on_style = pep8\ncolumn_limit = 120\nblank_line_before_nested_class_or_def = true\nsplit_before_expression_after_opening_paren = true\n\n[isort]\nline_length = 120\nmulti_line_output = 0\nknown_standard_library = pkg_resources,setuptools\nknown_first_party = gfpgan\nknown_third_party = basicsr,cv2,facexlib,numpy,pytest,torch,torchvision,tqdm,yaml\nno_lines_before = STDLIB,LOCALFOLDER\ndefault_section = THIRDPARTY\n\n[codespell]\nskip = .git,./docs/build\ncount =\nquiet-level = 3\n\n[aliases]\ntest=pytest\n\n[tool:pytest]\naddopts=tests/\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 3.07421875,
          "content": "#!/usr/bin/env python\n\nfrom setuptools import find_packages, setup\n\nimport os\nimport subprocess\nimport time\n\nversion_file = 'gfpgan/version.py'\n\n\ndef readme():\n    with open('README.md', encoding='utf-8') as f:\n        content = f.read()\n    return content\n\n\ndef get_git_hash():\n\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in ['SYSTEMROOT', 'PATH', 'HOME']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env['LANGUAGE'] = 'C'\n        env['LANG'] = 'C'\n        env['LC_ALL'] = 'C'\n        out = subprocess.Popen(cmd, stdout=subprocess.PIPE, env=env).communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd(['git', 'rev-parse', 'HEAD'])\n        sha = out.strip().decode('ascii')\n    except OSError:\n        sha = 'unknown'\n\n    return sha\n\n\ndef get_hash():\n    if os.path.exists('.git'):\n        sha = get_git_hash()[:7]\n    else:\n        sha = 'unknown'\n\n    return sha\n\n\ndef write_version_py():\n    content = \"\"\"# GENERATED VERSION FILE\n# TIME: {}\n__version__ = '{}'\n__gitsha__ = '{}'\nversion_info = ({})\n\"\"\"\n    sha = get_hash()\n    with open('VERSION', 'r') as f:\n        SHORT_VERSION = f.read().strip()\n    VERSION_INFO = ', '.join([x if x.isdigit() else f'\"{x}\"' for x in SHORT_VERSION.split('.')])\n\n    version_file_str = content.format(time.asctime(), SHORT_VERSION, sha, VERSION_INFO)\n    with open(version_file, 'w') as f:\n        f.write(version_file_str)\n\n\ndef get_version():\n    with open(version_file, 'r') as f:\n        exec(compile(f.read(), version_file, 'exec'))\n    return locals()['__version__']\n\n\ndef get_requirements(filename='requirements.txt'):\n    here = os.path.dirname(os.path.realpath(__file__))\n    with open(os.path.join(here, filename), 'r') as f:\n        requires = [line.replace('\\n', '') for line in f.readlines()]\n    return requires\n\n\nif __name__ == '__main__':\n    write_version_py()\n    setup(\n        name='gfpgan',\n        version=get_version(),\n        description='GFPGAN aims at developing Practical Algorithms for Real-world Face Restoration',\n        long_description=readme(),\n        long_description_content_type='text/markdown',\n        author='Xintao Wang',\n        author_email='xintao.wang@outlook.com',\n        keywords='computer vision, pytorch, image restoration, super-resolution, face restoration, gan, gfpgan',\n        url='https://github.com/TencentARC/GFPGAN',\n        include_package_data=True,\n        packages=find_packages(exclude=('options', 'datasets', 'experiments', 'results', 'tb_logger', 'wandb')),\n        classifiers=[\n            'Development Status :: 4 - Beta',\n            'License :: OSI Approved :: Apache Software License',\n            'Operating System :: OS Independent',\n            'Programming Language :: Python :: 3',\n            'Programming Language :: Python :: 3.7',\n            'Programming Language :: Python :: 3.8',\n        ],\n        license='Apache License Version 2.0',\n        setup_requires=['cython', 'numpy'],\n        install_requires=get_requirements(),\n        zip_safe=False)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}