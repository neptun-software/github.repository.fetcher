{
  "metadata": {
    "timestamp": 1736561407316,
    "page": 43,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "XingangPan/DragGAN",
      "stars": 35824,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.2275390625,
          "content": "# Created by .ignore support plugin (hsz.mobi)\n### Python template\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# IPython Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# dotenv\n.env\n\n# virtualenv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n\n# Rope project settings\n.ropeproject\n### VirtualEnv template\n# Virtualenv\n# http://iamzed.com/2009/05/07/a-primer-on-virtualenv/\n.Python\n[Bb]in\n[Ii]nclude\n[Ll]ib\n[Ll]ib64\n[Ll]ocal\n[Ss]cripts\n!scripts/\npyvenv.cfg\n.venv\npip-selfcheck.json\n### JetBrains template\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio and Webstorm\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff:\n.idea/workspace.xml\n.idea/tasks.xml\n.idea/dictionaries\n.idea/vcs.xml\n.idea/jsLibraryMappings.xml\n\n# Sensitive or high-churn files:\n.idea/dataSources.ids\n.idea/dataSources.xml\n.idea/dataSources.local.xml\n.idea/sqlDataSources.xml\n.idea/dynamic.xml\n.idea/uiDesigner.xml\n\n# Gradle:\n.idea/gradle.xml\n.idea/libraries\n\n# Mongo Explorer plugin:\n.idea/mongoSettings.xml\n\n.idea/\n\n## File-based project format:\n*.iws\n\n## Plugin-specific files:\n\n# IntelliJ\n/out/\n\n# mpeltonen/sbt-idea plugin\n.idea_modules/\n\n# JIRA plugin\natlassian-ide-plugin.xml\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n\n# Mac related\n.DS_Store\n\ncheckpoints\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.6728515625,
          "content": "FROM nvcr.io/nvidia/pytorch:23.05-py3\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        make \\\n        pkgconf \\\n        xz-utils \\\n        xorg-dev \\\n        libgl1-mesa-dev \\\n        libglu1-mesa-dev \\\n        libxrandr-dev \\\n        libxinerama-dev \\\n        libxcursor-dev \\\n        libxi-dev \\\n        libxxf86vm-dev \\\n        && rm -rf /var/lib/apt/lists/*\n\nRUN pip install --no-cache-dir --upgrade pip\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nWORKDIR /workspace\n\nRUN (printf '#!/bin/bash\\nexec \\\"$@\\\"\\n' >> /entry.sh) && chmod a+x /entry.sh\nENTRYPOINT [\"/entry.sh\"]\n"
        },
        {
          "name": "DragGAN.gif",
          "type": "blob",
          "size": 21133.861328125,
          "content": ""
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 4.2841796875,
          "content": "Copyright (c) 2021, NVIDIA Corporation & affiliates. All rights reserved.\n\n\nNVIDIA Source Code License for StyleGAN3\n\n\n=======================================================================\n\n1. Definitions\n\n\"Licensor\" means any person or entity that distributes its Work.\n\n\"Software\" means the original work of authorship made available under\nthis License.\n\n\"Work\" means the Software and any additions to or derivative works of\nthe Software that are made available under this License.\n\nThe terms \"reproduce,\" \"reproduction,\" \"derivative works,\" and\n\"distribution\" have the meaning as provided under U.S. copyright law;\nprovided, however, that for the purposes of this License, derivative\nworks shall not include works that remain separable from, or merely\nlink (or bind by name) to the interfaces of, the Work.\n\nWorks, including the Software, are \"made available\" under this License\nby including in or with the Work either (a) a copyright notice\nreferencing the applicability of this License to the Work, or (b) a\ncopy of this License.\n\n2. License Grants\n\n    2.1 Copyright Grant. Subject to the terms and conditions of this\n    License, each Licensor grants to you a perpetual, worldwide,\n    non-exclusive, royalty-free, copyright license to reproduce,\n    prepare derivative works of, publicly display, publicly perform,\n    sublicense and distribute its Work and any resulting derivative\n    works in any form.\n\n3. Limitations\n\n    3.1 Redistribution. You may reproduce or distribute the Work only\n    if (a) you do so under this License, (b) you include a complete\n    copy of this License with your distribution, and (c) you retain\n    without modification any copyright, patent, trademark, or\n    attribution notices that are present in the Work.\n\n    3.2 Derivative Works. You may specify that additional or different\n    terms apply to the use, reproduction, and distribution of your\n    derivative works of the Work (\"Your Terms\") only if (a) Your Terms\n    provide that the use limitation in Section 3.3 applies to your\n    derivative works, and (b) you identify the specific derivative\n    works that are subject to Your Terms. Notwithstanding Your Terms,\n    this License (including the redistribution requirements in Section\n    3.1) will continue to apply to the Work itself.\n\n    3.3 Use Limitation. The Work and any derivative works thereof only\n    may be used or intended for use non-commercially. Notwithstanding\n    the foregoing, NVIDIA and its affiliates may use the Work and any\n    derivative works commercially. As used herein, \"non-commercially\"\n    means for research or evaluation purposes only.\n\n    3.4 Patent Claims. If you bring or threaten to bring a patent claim\n    against any Licensor (including any claim, cross-claim or\n    counterclaim in a lawsuit) to enforce any patents that you allege\n    are infringed by any Work, then your rights under this License from\n    such Licensor (including the grant in Section 2.1) will terminate\n    immediately.\n\n    3.5 Trademarks. This License does not grant any rights to use any\n    Licensor’s or its affiliates’ names, logos, or trademarks, except\n    as necessary to reproduce the notices described in this License.\n\n    3.6 Termination. If you violate any term of this License, then your\n    rights under this License (including the grant in Section 2.1) will\n    terminate immediately.\n\n4. Disclaimer of Warranty.\n\nTHE WORK IS PROVIDED \"AS IS\" WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, EITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OR CONDITIONS OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR\nNON-INFRINGEMENT. YOU BEAR THE RISK OF UNDERTAKING ANY ACTIVITIES UNDER\nTHIS LICENSE.\n\n5. Limitation of Liability.\n\nEXCEPT AS PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL\nTHEORY, WHETHER IN TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE\nSHALL ANY LICENSOR BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY DIRECT,\nINDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF\nOR RELATED TO THIS LICENSE, THE USE OR INABILITY TO USE THE WORK\n(INCLUDING BUT NOT LIMITED TO LOSS OF GOODWILL, BUSINESS INTERRUPTION,\nLOST PROFITS OR DATA, COMPUTER FAILURE OR MALFUNCTION, OR ANY OTHER\nCOMMERCIAL DAMAGES OR LOSSES), EVEN IF THE LICENSOR HAS BEEN ADVISED OF\nTHE POSSIBILITY OF SUCH DAMAGES.\n\n=======================================================================\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.751953125,
          "content": "<p align=\"center\">\n\n  <h1 align=\"center\">Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</h1>\n  <p align=\"center\">\n    <a href=\"https://xingangpan.github.io/\"><strong>Xingang Pan</strong></a>\n    ·\n    <a href=\"https://ayushtewari.com/\"><strong>Ayush Tewari</strong></a>\n    ·\n    <a href=\"https://people.mpi-inf.mpg.de/~tleimkue/\"><strong>Thomas Leimkühler</strong></a>\n    ·\n    <a href=\"https://lingjie0206.github.io/\"><strong>Lingjie Liu</strong></a>\n    ·\n    <a href=\"https://www.meka.page/\"><strong>Abhimitra Meka</strong></a>\n    ·\n    <a href=\"http://www.mpi-inf.mpg.de/~theobalt/\"><strong>Christian Theobalt</strong></a>\n  </p>\n  <h2 align=\"center\">SIGGRAPH 2023 Conference Proceedings</h2>\n  <div align=\"center\">\n    <img src=\"DragGAN.gif\", width=\"600\">\n  </div>\n\n  <p align=\"center\">\n  <br>\n    <a href=\"https://pytorch.org/get-started/locally/\"><img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white\"></a>\n    <a href=\"https://twitter.com/XingangP\"><img alt='Twitter' src=\"https://img.shields.io/twitter/follow/XingangP?label=%40XingangP\"></a>\n    <a href=\"https://arxiv.org/abs/2305.10973\">\n      <img src='https://img.shields.io/badge/Paper-PDF-green?style=for-the-badge&logo=adobeacrobatreader&logoWidth=20&logoColor=white&labelColor=66cc00&color=94DD15' alt='Paper PDF'>\n    </a>\n    <a href='https://vcai.mpi-inf.mpg.de/projects/DragGAN/'>\n      <img src='https://img.shields.io/badge/DragGAN-Page-orange?style=for-the-badge&logo=Google%20chrome&logoColor=white&labelColor=D35400' alt='Project Page'></a>\n    <a href=\"https://colab.research.google.com/drive/1mey-IXPwQC_qSthI5hO-LTX7QL4ivtPh?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n  </p>\n</p>\n\n## Web Demos\n\n[![Open in OpenXLab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)](https://openxlab.org.cn/apps/detail/XingangPan/DragGAN)\n\n<p align=\"left\">\n  <a href=\"https://huggingface.co/spaces/radames/DragGan\"><img alt=\"Huggingface\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DragGAN-orange\"></a>\n</p>\n\n## Requirements\n\nIf you have CUDA graphic card, please follow the requirements of [NVlabs/stylegan3](https://github.com/NVlabs/stylegan3#requirements).  \n\nThe usual installation steps involve the following commands, they should set up the correct CUDA version and all the python packages\n\n```\nconda env create -f environment.yml\nconda activate stylegan3\n```\n\nThen install the additional requirements\n\n```\npip install -r requirements.txt\n```\n\nOtherwise (for GPU acceleration on MacOS with Silicon Mac M1/M2, or just CPU) try the following:\n\n```sh\ncat environment.yml | \\\n  grep -v -E 'nvidia|cuda' > environment-no-nvidia.yml && \\\n    conda env create -f environment-no-nvidia.yml\nconda activate stylegan3\n\n# On MacOS\nexport PYTORCH_ENABLE_MPS_FALLBACK=1\n```\n\n## Run Gradio visualizer in Docker \n\nProvided docker image is based on NGC PyTorch repository. To quickly try out visualizer in Docker, run the following:  \n\n```sh\n# before you build the docker container, make sure you have cloned this repo, and downloaded the pretrained model by `python scripts/download_model.py`.\ndocker build . -t draggan:latest  \ndocker run -p 7860:7860 -v \"$PWD\":/workspace/src -it draggan:latest bash\n# (Use GPU)if you want to utilize your Nvidia gpu to accelerate in docker, please add command tag `--gpus all`, like:\n#   docker run --gpus all  -p 7860:7860 -v \"$PWD\":/workspace/src -it draggan:latest bash\n\ncd src && python visualizer_drag_gradio.py --listen\n```\nNow you can open a shared link from Gradio (printed in the terminal console).   \nBeware the Docker image takes about 25GB of disk space!\n\n## Download pre-trained StyleGAN2 weights\n\nTo download pre-trained weights, simply run:\n\n```\npython scripts/download_model.py\n```\nIf you want to try StyleGAN-Human and the Landscapes HQ (LHQ) dataset, please download weights from these links: [StyleGAN-Human](https://drive.google.com/file/d/1dlFEHbu-WzQWJl7nBBZYcTyo000H9hVm/view?usp=sharing), [LHQ](https://drive.google.com/file/d/16twEf0T9QINAEoMsWefoWiyhcTd-aiWc/view?usp=sharing), and put them under `./checkpoints`.\n\nFeel free to try other pretrained StyleGAN.\n\n## Run DragGAN GUI\n\nTo start the DragGAN GUI, simply run:\n```sh\nsh scripts/gui.sh\n```\nIf you are using windows, you can run:\n```\n.\\scripts\\gui.bat\n```\n\nThis GUI supports editing GAN-generated images. To edit a real image, you need to first perform GAN inversion using tools like [PTI](https://github.com/danielroich/PTI). Then load the new latent code and model weights to the GUI.\n\nYou can run DragGAN Gradio demo as well, this is universal for both windows and linux:\n```sh\npython visualizer_drag_gradio.py\n```\n\n## Acknowledgement\n\nThis code is developed based on [StyleGAN3](https://github.com/NVlabs/stylegan3). Part of the code is borrowed from [StyleGAN-Human](https://github.com/stylegan-human/StyleGAN-Human).\n\n(cheers to the community as well)\n## License\n\nThe code related to the DragGAN algorithm is licensed under [CC-BY-NC](https://creativecommons.org/licenses/by-nc/4.0/).\nHowever, most of this project are available under a separate license terms: all codes used or modified from [StyleGAN3](https://github.com/NVlabs/stylegan3) is under the [Nvidia Source Code License](https://github.com/NVlabs/stylegan3/blob/main/LICENSE.txt).\n\nAny form of use and derivative of this code must preserve the watermarking functionality showing \"AI Generated\".\n\n## BibTeX\n\n```bibtex\n@inproceedings{pan2023draggan,\n    title={Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold},\n    author={Pan, Xingang and Tewari, Ayush, and Leimk{\\\"u}hler, Thomas and Liu, Lingjie and Meka, Abhimitra and Theobalt, Christian},\n    booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},\n    year={2023}\n}\n```\n"
        },
        {
          "name": "arial.ttf",
          "type": "blob",
          "size": 269.11328125,
          "content": null
        },
        {
          "name": "dnnlib",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.537109375,
          "content": "name: stylegan3\nchannels:\n  - pytorch\n  - nvidia\ndependencies:\n  - python >= 3.8\n  - pip\n  - numpy>=1.25\n  - click>=8.0\n  - pillow=9.4.0\n  - scipy=1.11.1\n  - pytorch>=2.0.1\n  - torchvision>=0.15.2\n  - cudatoolkit=11.1\n  - requests=2.26.0\n  - tqdm=4.62.2\n  - ninja=1.10.2\n  - matplotlib=3.4.2\n  - imageio=2.9.0\n  - pip:\n    - imgui==2.0.0\n    - glfw==2.6.1\n    - gradio==3.35.2\n    - pyopengl==3.1.5\n    - imageio-ffmpeg==0.4.3\n    # pyspng is currently broken on MacOS (see https://github.com/nurpax/pyspng/pull/6 for instance)\n    - pyspng-seunglab\n"
        },
        {
          "name": "gen_images.py",
          "type": "blob",
          "size": 5.9482421875,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Generate images using pretrained network pickle.\"\"\"\n\nimport os\nimport re\nfrom typing import List, Optional, Tuple, Union\n\nimport click\nimport dnnlib\nimport numpy as np\nimport PIL.Image\nimport torch\n\nimport legacy\n\n#----------------------------------------------------------------------------\n\ndef parse_range(s: Union[str, List]) -> List[int]:\n    '''Parse a comma separated list of numbers or ranges and return a list of ints.\n\n    Example: '1,2,5-10' returns [1, 2, 5, 6, 7]\n    '''\n    if isinstance(s, list): return s\n    ranges = []\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    for p in s.split(','):\n        m = range_re.match(p)\n        if m:\n            ranges.extend(range(int(m.group(1)), int(m.group(2))+1))\n        else:\n            ranges.append(int(p))\n    return ranges\n\n#----------------------------------------------------------------------------\n\ndef parse_vec2(s: Union[str, Tuple[float, float]]) -> Tuple[float, float]:\n    '''Parse a floating point 2-vector of syntax 'a,b'.\n\n    Example:\n        '0,1' returns (0,1)\n    '''\n    if isinstance(s, tuple): return s\n    parts = s.split(',')\n    if len(parts) == 2:\n        return (float(parts[0]), float(parts[1]))\n    raise ValueError(f'cannot parse 2-vector {s}')\n\n#----------------------------------------------------------------------------\n\ndef make_transform(translate: Tuple[float,float], angle: float):\n    m = np.eye(3)\n    s = np.sin(angle/360.0*np.pi*2)\n    c = np.cos(angle/360.0*np.pi*2)\n    m[0][0] = c\n    m[0][1] = s\n    m[0][2] = translate[0]\n    m[1][0] = -s\n    m[1][1] = c\n    m[1][2] = translate[1]\n    return m\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.option('--network', 'network_pkl', help='Network pickle filename', required=True)\n@click.option('--seeds', type=parse_range, help='List of random seeds (e.g., \\'0,1,4-6\\')', required=True)\n@click.option('--trunc', 'truncation_psi', type=float, help='Truncation psi', default=1, show_default=True)\n@click.option('--class', 'class_idx', type=int, help='Class label (unconditional if not specified)')\n@click.option('--noise-mode', help='Noise mode', type=click.Choice(['const', 'random', 'none']), default='const', show_default=True)\n@click.option('--translate', help='Translate XY-coordinate (e.g. \\'0.3,1\\')', type=parse_vec2, default='0,0', show_default=True, metavar='VEC2')\n@click.option('--rotate', help='Rotation angle in degrees', type=float, default=0, show_default=True, metavar='ANGLE')\n@click.option('--outdir', help='Where to save the output images', type=str, required=True, metavar='DIR')\ndef generate_images(\n    network_pkl: str,\n    seeds: List[int],\n    truncation_psi: float,\n    noise_mode: str,\n    outdir: str,\n    translate: Tuple[float,float],\n    rotate: float,\n    class_idx: Optional[int]\n):\n    \"\"\"Generate images using pretrained network pickle.\n\n    Examples:\n\n    \\b\n    # Generate an image using pre-trained AFHQv2 model (\"Ours\" in Figure 1, left).\n    python gen_images.py --outdir=out --trunc=1 --seeds=2 \\\\\n        --network=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-afhqv2-512x512.pkl\n\n    \\b\n    # Generate uncurated images with truncation using the MetFaces-U dataset\n    python gen_images.py --outdir=out --trunc=0.7 --seeds=600-605 \\\\\n        --network=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-metfacesu-1024x1024.pkl\n    \"\"\"\n\n    print('Loading networks from \"%s\"...' % network_pkl)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n    dtype = torch.float32 if device.type == 'mps' else torch.float64\n    with dnnlib.util.open_url(network_pkl) as f:\n        G = legacy.load_network_pkl(f)['G_ema'].to(device, dtype=dtype) # type: ignore\n        # import pickle\n        # G = legacy.load_network_pkl(f)\n        # output = open('checkpoints/stylegan2-car-config-f-pt.pkl', 'wb')\n        # pickle.dump(G, output)\n\n    os.makedirs(outdir, exist_ok=True)\n\n    # Labels.\n    label = torch.zeros([1, G.c_dim], device=device)\n    if G.c_dim != 0:\n        if class_idx is None:\n            raise click.ClickException('Must specify class label with --class when using a conditional network')\n        label[:, class_idx] = 1\n    else:\n        if class_idx is not None:\n            print ('warn: --class=lbl ignored when running on an unconditional network')\n\n    # Generate images.\n    for seed_idx, seed in enumerate(seeds):\n        print('Generating image for seed %d (%d/%d) ...' % (seed, seed_idx, len(seeds)))\n        z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(device, dtype=dtype)\n\n        # Construct an inverse rotation/translation matrix and pass to the generator.  The\n        # generator expects this matrix as an inverse to avoid potentially failing numerical\n        # operations in the network.\n        if hasattr(G.synthesis, 'input'):\n            m = make_transform(translate, rotate)\n            m = np.linalg.inv(m)\n            G.synthesis.input.transform.copy_(torch.from_numpy(m))\n\n        img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n        img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n        PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/seed{seed:04d}.png')\n\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    generate_images() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "gradio_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "gui_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "legacy.py",
          "type": "blob",
          "size": 16.1728515625,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Converting legacy network pickle into the new format.\"\"\"\n\nimport click\nimport pickle\nimport re\nimport copy\nimport numpy as np\nimport torch\nimport dnnlib\nfrom torch_utils import misc\n\n#----------------------------------------------------------------------------\n\ndef load_network_pkl(f, force_fp16=False):\n    data = _LegacyUnpickler(f).load()\n\n    # Legacy TensorFlow pickle => convert.\n    if isinstance(data, tuple) and len(data) == 3 and all(isinstance(net, _TFNetworkStub) for net in data):\n        tf_G, tf_D, tf_Gs = data\n        G = convert_tf_generator(tf_G)\n        D = convert_tf_discriminator(tf_D)\n        G_ema = convert_tf_generator(tf_Gs)\n        data = dict(G=G, D=D, G_ema=G_ema)\n\n    # Add missing fields.\n    if 'training_set_kwargs' not in data:\n        data['training_set_kwargs'] = None\n    if 'augment_pipe' not in data:\n        data['augment_pipe'] = None\n\n    # Validate contents.\n    assert isinstance(data['G'], torch.nn.Module)\n    assert isinstance(data['D'], torch.nn.Module)\n    assert isinstance(data['G_ema'], torch.nn.Module)\n    assert isinstance(data['training_set_kwargs'], (dict, type(None)))\n    assert isinstance(data['augment_pipe'], (torch.nn.Module, type(None)))\n\n    # Force FP16.\n    if force_fp16:\n        for key in ['G', 'D', 'G_ema']:\n            old = data[key]\n            kwargs = copy.deepcopy(old.init_kwargs)\n            fp16_kwargs = kwargs.get('synthesis_kwargs', kwargs)\n            fp16_kwargs.num_fp16_res = 4\n            fp16_kwargs.conv_clamp = 256\n            if kwargs != old.init_kwargs:\n                new = type(old)(**kwargs).eval().requires_grad_(False)\n                misc.copy_params_and_buffers(old, new, require_all=True)\n                data[key] = new\n    return data\n\n#----------------------------------------------------------------------------\n\nclass _TFNetworkStub(dnnlib.EasyDict):\n    pass\n\nclass _LegacyUnpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        if module == 'dnnlib.tflib.network' and name == 'Network':\n            return _TFNetworkStub\n        return super().find_class(module, name)\n\n#----------------------------------------------------------------------------\n\ndef _collect_tf_params(tf_net):\n    # pylint: disable=protected-access\n    tf_params = dict()\n    def recurse(prefix, tf_net):\n        for name, value in tf_net.variables:\n            tf_params[prefix + name] = value\n        for name, comp in tf_net.components.items():\n            recurse(prefix + name + '/', comp)\n    recurse('', tf_net)\n    return tf_params\n\n#----------------------------------------------------------------------------\n\ndef _populate_module_params(module, *patterns):\n    for name, tensor in misc.named_params_and_buffers(module):\n        found = False\n        value = None\n        for pattern, value_fn in zip(patterns[0::2], patterns[1::2]):\n            match = re.fullmatch(pattern, name)\n            if match:\n                found = True\n                if value_fn is not None:\n                    value = value_fn(*match.groups())\n                break\n        try:\n            assert found\n            if value is not None:\n                tensor.copy_(torch.from_numpy(np.array(value)))\n        except:\n            print(name, list(tensor.shape))\n            raise\n\n#----------------------------------------------------------------------------\n\ndef convert_tf_generator(tf_G):\n    if tf_G.version < 4:\n        raise ValueError('TensorFlow pickle version too low')\n\n    # Collect kwargs.\n    tf_kwargs = tf_G.static_kwargs\n    known_kwargs = set()\n    def kwarg(tf_name, default=None, none=None):\n        known_kwargs.add(tf_name)\n        val = tf_kwargs.get(tf_name, default)\n        return val if val is not None else none\n\n    # Convert kwargs.\n    from training import networks_stylegan2\n    network_class = networks_stylegan2.Generator\n    kwargs = dnnlib.EasyDict(\n        z_dim               = kwarg('latent_size',          512),\n        c_dim               = kwarg('label_size',           0),\n        w_dim               = kwarg('dlatent_size',         512),\n        img_resolution      = kwarg('resolution',           1024),\n        img_channels        = kwarg('num_channels',         3),\n        channel_base        = kwarg('fmap_base',            16384) * 2,\n        channel_max         = kwarg('fmap_max',             512),\n        num_fp16_res        = kwarg('num_fp16_res',         0),\n        conv_clamp          = kwarg('conv_clamp',           None),\n        architecture        = kwarg('architecture',         'skip'),\n        resample_filter     = kwarg('resample_kernel',      [1,3,3,1]),\n        use_noise           = kwarg('use_noise',            True),\n        activation          = kwarg('nonlinearity',         'lrelu'),\n        mapping_kwargs      = dnnlib.EasyDict(\n            num_layers      = kwarg('mapping_layers',       8),\n            embed_features  = kwarg('label_fmaps',          None),\n            layer_features  = kwarg('mapping_fmaps',        None),\n            activation      = kwarg('mapping_nonlinearity', 'lrelu'),\n            lr_multiplier   = kwarg('mapping_lrmul',        0.01),\n            w_avg_beta      = kwarg('w_avg_beta',           0.995,  none=1),\n        ),\n    )\n\n    # Check for unknown kwargs.\n    kwarg('truncation_psi')\n    kwarg('truncation_cutoff')\n    kwarg('style_mixing_prob')\n    kwarg('structure')\n    kwarg('conditioning')\n    kwarg('fused_modconv')\n    unknown_kwargs = list(set(tf_kwargs.keys()) - known_kwargs)\n    if len(unknown_kwargs) > 0:\n        raise ValueError('Unknown TensorFlow kwarg', unknown_kwargs[0])\n\n    # Collect params.\n    tf_params = _collect_tf_params(tf_G)\n    for name, value in list(tf_params.items()):\n        match = re.fullmatch(r'ToRGB_lod(\\d+)/(.*)', name)\n        if match:\n            r = kwargs.img_resolution // (2 ** int(match.group(1)))\n            tf_params[f'{r}x{r}/ToRGB/{match.group(2)}'] = value\n            kwargs.synthesis.kwargs.architecture = 'orig'\n    #for name, value in tf_params.items(): print(f'{name:<50s}{list(value.shape)}')\n\n    # Convert params.\n    G = network_class(**kwargs).eval().requires_grad_(False)\n    # pylint: disable=unnecessary-lambda\n    # pylint: disable=f-string-without-interpolation\n    _populate_module_params(G,\n        r'mapping\\.w_avg',                                  lambda:     tf_params[f'dlatent_avg'],\n        r'mapping\\.embed\\.weight',                          lambda:     tf_params[f'mapping/LabelEmbed/weight'].transpose(),\n        r'mapping\\.embed\\.bias',                            lambda:     tf_params[f'mapping/LabelEmbed/bias'],\n        r'mapping\\.fc(\\d+)\\.weight',                        lambda i:   tf_params[f'mapping/Dense{i}/weight'].transpose(),\n        r'mapping\\.fc(\\d+)\\.bias',                          lambda i:   tf_params[f'mapping/Dense{i}/bias'],\n        r'synthesis\\.b4\\.const',                            lambda:     tf_params[f'synthesis/4x4/Const/const'][0],\n        r'synthesis\\.b4\\.conv1\\.weight',                    lambda:     tf_params[f'synthesis/4x4/Conv/weight'].transpose(3, 2, 0, 1),\n        r'synthesis\\.b4\\.conv1\\.bias',                      lambda:     tf_params[f'synthesis/4x4/Conv/bias'],\n        r'synthesis\\.b4\\.conv1\\.noise_const',               lambda:     tf_params[f'synthesis/noise0'][0, 0],\n        r'synthesis\\.b4\\.conv1\\.noise_strength',            lambda:     tf_params[f'synthesis/4x4/Conv/noise_strength'],\n        r'synthesis\\.b4\\.conv1\\.affine\\.weight',            lambda:     tf_params[f'synthesis/4x4/Conv/mod_weight'].transpose(),\n        r'synthesis\\.b4\\.conv1\\.affine\\.bias',              lambda:     tf_params[f'synthesis/4x4/Conv/mod_bias'] + 1,\n        r'synthesis\\.b(\\d+)\\.conv0\\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/weight'][::-1, ::-1].transpose(3, 2, 0, 1),\n        r'synthesis\\.b(\\d+)\\.conv0\\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/bias'],\n        r'synthesis\\.b(\\d+)\\.conv0\\.noise_const',           lambda r:   tf_params[f'synthesis/noise{int(np.log2(int(r)))*2-5}'][0, 0],\n        r'synthesis\\.b(\\d+)\\.conv0\\.noise_strength',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/noise_strength'],\n        r'synthesis\\.b(\\d+)\\.conv0\\.affine\\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/mod_weight'].transpose(),\n        r'synthesis\\.b(\\d+)\\.conv0\\.affine\\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/mod_bias'] + 1,\n        r'synthesis\\.b(\\d+)\\.conv1\\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/weight'].transpose(3, 2, 0, 1),\n        r'synthesis\\.b(\\d+)\\.conv1\\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/bias'],\n        r'synthesis\\.b(\\d+)\\.conv1\\.noise_const',           lambda r:   tf_params[f'synthesis/noise{int(np.log2(int(r)))*2-4}'][0, 0],\n        r'synthesis\\.b(\\d+)\\.conv1\\.noise_strength',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/noise_strength'],\n        r'synthesis\\.b(\\d+)\\.conv1\\.affine\\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/mod_weight'].transpose(),\n        r'synthesis\\.b(\\d+)\\.conv1\\.affine\\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/mod_bias'] + 1,\n        r'synthesis\\.b(\\d+)\\.torgb\\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/weight'].transpose(3, 2, 0, 1),\n        r'synthesis\\.b(\\d+)\\.torgb\\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/bias'],\n        r'synthesis\\.b(\\d+)\\.torgb\\.affine\\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/mod_weight'].transpose(),\n        r'synthesis\\.b(\\d+)\\.torgb\\.affine\\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/mod_bias'] + 1,\n        r'synthesis\\.b(\\d+)\\.skip\\.weight',                 lambda r:   tf_params[f'synthesis/{r}x{r}/Skip/weight'][::-1, ::-1].transpose(3, 2, 0, 1),\n        r'.*\\.resample_filter',                             None,\n        r'.*\\.act_filter',                                  None,\n    )\n    return G\n\n#----------------------------------------------------------------------------\n\ndef convert_tf_discriminator(tf_D):\n    if tf_D.version < 4:\n        raise ValueError('TensorFlow pickle version too low')\n\n    # Collect kwargs.\n    tf_kwargs = tf_D.static_kwargs\n    known_kwargs = set()\n    def kwarg(tf_name, default=None):\n        known_kwargs.add(tf_name)\n        return tf_kwargs.get(tf_name, default)\n\n    # Convert kwargs.\n    kwargs = dnnlib.EasyDict(\n        c_dim                   = kwarg('label_size',           0),\n        img_resolution          = kwarg('resolution',           1024),\n        img_channels            = kwarg('num_channels',         3),\n        architecture            = kwarg('architecture',         'resnet'),\n        channel_base            = kwarg('fmap_base',            16384) * 2,\n        channel_max             = kwarg('fmap_max',             512),\n        num_fp16_res            = kwarg('num_fp16_res',         0),\n        conv_clamp              = kwarg('conv_clamp',           None),\n        cmap_dim                = kwarg('mapping_fmaps',        None),\n        block_kwargs = dnnlib.EasyDict(\n            activation          = kwarg('nonlinearity',         'lrelu'),\n            resample_filter     = kwarg('resample_kernel',      [1,3,3,1]),\n            freeze_layers       = kwarg('freeze_layers',        0),\n        ),\n        mapping_kwargs = dnnlib.EasyDict(\n            num_layers          = kwarg('mapping_layers',       0),\n            embed_features      = kwarg('mapping_fmaps',        None),\n            layer_features      = kwarg('mapping_fmaps',        None),\n            activation          = kwarg('nonlinearity',         'lrelu'),\n            lr_multiplier       = kwarg('mapping_lrmul',        0.1),\n        ),\n        epilogue_kwargs = dnnlib.EasyDict(\n            mbstd_group_size    = kwarg('mbstd_group_size',     None),\n            mbstd_num_channels  = kwarg('mbstd_num_features',   1),\n            activation          = kwarg('nonlinearity',         'lrelu'),\n        ),\n    )\n\n    # Check for unknown kwargs.\n    kwarg('structure')\n    kwarg('conditioning')\n    unknown_kwargs = list(set(tf_kwargs.keys()) - known_kwargs)\n    if len(unknown_kwargs) > 0:\n        raise ValueError('Unknown TensorFlow kwarg', unknown_kwargs[0])\n\n    # Collect params.\n    tf_params = _collect_tf_params(tf_D)\n    for name, value in list(tf_params.items()):\n        match = re.fullmatch(r'FromRGB_lod(\\d+)/(.*)', name)\n        if match:\n            r = kwargs.img_resolution // (2 ** int(match.group(1)))\n            tf_params[f'{r}x{r}/FromRGB/{match.group(2)}'] = value\n            kwargs.architecture = 'orig'\n    #for name, value in tf_params.items(): print(f'{name:<50s}{list(value.shape)}')\n\n    # Convert params.\n    from training import networks_stylegan2\n    D = networks_stylegan2.Discriminator(**kwargs).eval().requires_grad_(False)\n    # pylint: disable=unnecessary-lambda\n    # pylint: disable=f-string-without-interpolation\n    _populate_module_params(D,\n        r'b(\\d+)\\.fromrgb\\.weight',     lambda r:       tf_params[f'{r}x{r}/FromRGB/weight'].transpose(3, 2, 0, 1),\n        r'b(\\d+)\\.fromrgb\\.bias',       lambda r:       tf_params[f'{r}x{r}/FromRGB/bias'],\n        r'b(\\d+)\\.conv(\\d+)\\.weight',   lambda r, i:    tf_params[f'{r}x{r}/Conv{i}{[\"\",\"_down\"][int(i)]}/weight'].transpose(3, 2, 0, 1),\n        r'b(\\d+)\\.conv(\\d+)\\.bias',     lambda r, i:    tf_params[f'{r}x{r}/Conv{i}{[\"\",\"_down\"][int(i)]}/bias'],\n        r'b(\\d+)\\.skip\\.weight',        lambda r:       tf_params[f'{r}x{r}/Skip/weight'].transpose(3, 2, 0, 1),\n        r'mapping\\.embed\\.weight',      lambda:         tf_params[f'LabelEmbed/weight'].transpose(),\n        r'mapping\\.embed\\.bias',        lambda:         tf_params[f'LabelEmbed/bias'],\n        r'mapping\\.fc(\\d+)\\.weight',    lambda i:       tf_params[f'Mapping{i}/weight'].transpose(),\n        r'mapping\\.fc(\\d+)\\.bias',      lambda i:       tf_params[f'Mapping{i}/bias'],\n        r'b4\\.conv\\.weight',            lambda:         tf_params[f'4x4/Conv/weight'].transpose(3, 2, 0, 1),\n        r'b4\\.conv\\.bias',              lambda:         tf_params[f'4x4/Conv/bias'],\n        r'b4\\.fc\\.weight',              lambda:         tf_params[f'4x4/Dense0/weight'].transpose(),\n        r'b4\\.fc\\.bias',                lambda:         tf_params[f'4x4/Dense0/bias'],\n        r'b4\\.out\\.weight',             lambda:         tf_params[f'Output/weight'].transpose(),\n        r'b4\\.out\\.bias',               lambda:         tf_params[f'Output/bias'],\n        r'.*\\.resample_filter',         None,\n    )\n    return D\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.option('--source', help='Input pickle', required=True, metavar='PATH')\n@click.option('--dest', help='Output pickle', required=True, metavar='PATH')\n@click.option('--force-fp16', help='Force the networks to use FP16', type=bool, default=False, metavar='BOOL', show_default=True)\ndef convert_network_pickle(source, dest, force_fp16):\n    \"\"\"Convert legacy network pickle into the native PyTorch format.\n\n    The tool is able to load the main network configurations exported using the TensorFlow version of StyleGAN2 or StyleGAN2-ADA.\n    It does not support e.g. StyleGAN2-ADA comparison methods, StyleGAN2 configs A-D, or StyleGAN1 networks.\n\n    Example:\n\n    \\b\n    python legacy.py \\\\\n        --source=https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl \\\\\n        --dest=stylegan2-cat-config-f.pkl\n    \"\"\"\n    print(f'Loading \"{source}\"...')\n    with dnnlib.util.open_url(source) as f:\n        data = load_network_pkl(f, force_fp16=force_fp16)\n    print(f'Saving \"{dest}\"...')\n    with open(dest, 'wb') as f:\n        pickle.dump(data, f)\n    print('Done.')\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    convert_network_pickle() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.177734375,
          "content": "torch>=2.0.0\nscipy>=1.11.1\nNinja==1.10.2\ngradio>=3.35.2\nimageio-ffmpeg>=0.4.3\nhuggingface_hub\nhf_transfer\npyopengl\nimgui\nglfw==2.6.1\npillow>=9.4.0\ntorchvision>=0.15.2\nimageio>=2.9.0\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "stylegan_human",
          "type": "tree",
          "content": null
        },
        {
          "name": "torch_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "training",
          "type": "tree",
          "content": null
        },
        {
          "name": "visualizer_drag.py",
          "type": "blob",
          "size": 16.748046875,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\nimport click\nimport os\n\nimport multiprocessing\nimport numpy as np\nimport torch\nimport imgui\nimport dnnlib\nfrom gui_utils import imgui_window\nfrom gui_utils import imgui_utils\nfrom gui_utils import gl_utils\nfrom gui_utils import text_utils\nfrom viz import renderer\nfrom viz import pickle_widget\nfrom viz import latent_widget\nfrom viz import drag_widget\nfrom viz import capture_widget\n\n#----------------------------------------------------------------------------\n\nclass Visualizer(imgui_window.ImguiWindow):\n    def __init__(self, capture_dir=None):\n        super().__init__(title='DragGAN', window_width=3840, window_height=2160)\n\n        # Internals.\n        self._last_error_print  = None\n        self._async_renderer    = AsyncRenderer()\n        self._defer_rendering   = 0\n        self._tex_img           = None\n        self._tex_obj           = None\n        self._mask_obj          = None\n        self._image_area        = None\n        self._status            = dnnlib.EasyDict()\n\n        # Widget interface.\n        self.args               = dnnlib.EasyDict()\n        self.result             = dnnlib.EasyDict()\n        self.pane_w             = 0\n        self.label_w            = 0\n        self.button_w           = 0\n        self.image_w            = 0\n        self.image_h            = 0\n\n        # Widgets.\n        self.pickle_widget      = pickle_widget.PickleWidget(self)\n        self.latent_widget      = latent_widget.LatentWidget(self)\n        self.drag_widget        = drag_widget.DragWidget(self)\n        self.capture_widget     = capture_widget.CaptureWidget(self)\n\n        if capture_dir is not None:\n            self.capture_widget.path = capture_dir\n\n        # Initialize window.\n        self.set_position(0, 0)\n        self._adjust_font_size()\n        self.skip_frame() # Layout may change after first frame.\n\n    def close(self):\n        super().close()\n        if self._async_renderer is not None:\n            self._async_renderer.close()\n            self._async_renderer = None\n\n    def add_recent_pickle(self, pkl, ignore_errors=False):\n        self.pickle_widget.add_recent(pkl, ignore_errors=ignore_errors)\n\n    def load_pickle(self, pkl, ignore_errors=False):\n        self.pickle_widget.load(pkl, ignore_errors=ignore_errors)\n\n    def print_error(self, error):\n        error = str(error)\n        if error != self._last_error_print:\n            print('\\n' + error + '\\n')\n            self._last_error_print = error\n\n    def defer_rendering(self, num_frames=1):\n        self._defer_rendering = max(self._defer_rendering, num_frames)\n\n    def clear_result(self):\n        self._async_renderer.clear_result()\n\n    def set_async(self, is_async):\n        if is_async != self._async_renderer.is_async:\n            self._async_renderer.set_async(is_async)\n            self.clear_result()\n            if 'image' in self.result:\n                self.result.message = 'Switching rendering process...'\n                self.defer_rendering()\n\n    def _adjust_font_size(self):\n        old = self.font_size\n        self.set_font_size(min(self.content_width / 120, self.content_height / 60))\n        if self.font_size != old:\n            self.skip_frame() # Layout changed.\n\n    def check_update_mask(self, **args):\n        update_mask = False\n        if 'pkl' in self._status:\n            if self._status.pkl != args['pkl']:\n                update_mask = True\n        self._status.pkl = args['pkl']\n        if 'w0_seed' in self._status:\n            if self._status.w0_seed != args['w0_seed']:\n                update_mask = True\n        self._status.w0_seed = args['w0_seed']\n        return update_mask\n\n    def capture_image_frame(self):\n        self.capture_next_frame()\n        captured_frame = self.pop_captured_frame()\n        captured_image = None\n        if captured_frame is not None:\n            x1, y1, w, h = self._image_area\n            captured_image = captured_frame[y1:y1+h, x1:x1+w, :]\n        return captured_image\n\n    def get_drag_info(self):\n        seed = self.latent_widget.seed\n        points = self.drag_widget.points\n        targets = self.drag_widget.targets\n        mask = self.drag_widget.mask\n        w = self._async_renderer._renderer_obj.w\n        return seed, points, targets, mask, w\n\n    def draw_frame(self):\n        self.begin_frame()\n        self.args = dnnlib.EasyDict()\n        self.pane_w = self.font_size * 18\n        self.button_w = self.font_size * 5\n        self.label_w = round(self.font_size * 4.5)\n\n        # Detect mouse dragging in the result area.\n        if self._image_area is not None:\n            if not hasattr(self.drag_widget, 'width'):\n                self.drag_widget.init_mask(self.image_w, self.image_h)\n            clicked, down, img_x, img_y = imgui_utils.click_hidden_window(\n                '##image_area', self._image_area[0], self._image_area[1], self._image_area[2], self._image_area[3], self.image_w, self.image_h)\n            self.drag_widget.action(clicked, down, img_x, img_y)\n\n        # Begin control pane.\n        imgui.set_next_window_position(0, 0)\n        imgui.set_next_window_size(self.pane_w, self.content_height)\n        imgui.begin('##control_pane', closable=False, flags=(imgui.WINDOW_NO_TITLE_BAR | imgui.WINDOW_NO_RESIZE | imgui.WINDOW_NO_MOVE))\n\n        # Widgets.\n        expanded, _visible = imgui_utils.collapsing_header('Network & latent', default=True)\n        self.pickle_widget(expanded)\n        self.latent_widget(expanded)\n        expanded, _visible = imgui_utils.collapsing_header('Drag', default=True)\n        self.drag_widget(expanded)\n        expanded, _visible = imgui_utils.collapsing_header('Capture', default=True)\n        self.capture_widget(expanded)\n\n        # Render.\n        if self.is_skipping_frames():\n            pass\n        elif self._defer_rendering > 0:\n            self._defer_rendering -= 1\n        elif self.args.pkl is not None:\n            self._async_renderer.set_args(**self.args)\n            result = self._async_renderer.get_result()\n            if result is not None:\n                self.result = result        \n                if 'stop' in self.result and self.result.stop:\n                    self.drag_widget.stop_drag()\n                if 'points' in self.result:\n                    self.drag_widget.set_points(self.result.points)\n                if 'init_net' in self.result:\n                    if self.result.init_net:\n                        self.drag_widget.reset_point()\n\n        # Display.\n        max_w = self.content_width - self.pane_w\n        max_h = self.content_height\n        pos = np.array([self.pane_w + max_w / 2, max_h / 2])\n        if 'image' in self.result:\n            # Reset mask after loading a new pickle or changing seed.\n            if self.check_update_mask(**self.args):\n                h, w, _ = self.result.image.shape\n                self.drag_widget.init_mask(w, h)\n\n            if self._tex_img is not self.result.image:\n                self._tex_img = self.result.image\n                if self._tex_obj is None or not self._tex_obj.is_compatible(image=self._tex_img):\n                    self._tex_obj = gl_utils.Texture(image=self._tex_img, bilinear=False, mipmap=False)\n                else:\n                    self._tex_obj.update(self._tex_img)\n                self.image_h, self.image_w = self._tex_obj.height, self._tex_obj.width\n            zoom = min(max_w / self._tex_obj.width, max_h / self._tex_obj.height)\n            zoom = np.floor(zoom) if zoom >= 1 else zoom\n            self._tex_obj.draw(pos=pos, zoom=zoom, align=0.5, rint=True)\n            if self.drag_widget.show_mask and hasattr(self.drag_widget, 'mask'):\n                mask = ((1-self.drag_widget.mask.unsqueeze(-1)) * 255).to(torch.uint8)\n                if self._mask_obj is None or not self._mask_obj.is_compatible(image=self._tex_img):\n                    self._mask_obj = gl_utils.Texture(image=mask, bilinear=False, mipmap=False)\n                else:\n                    self._mask_obj.update(mask)\n                self._mask_obj.draw(pos=pos, zoom=zoom, align=0.5, rint=True, alpha=0.15)\n\n            if self.drag_widget.mode in ['flexible', 'fixed']:\n                posx, posy = imgui.get_mouse_pos()\n                if posx >= self.pane_w:\n                    pos_c = np.array([posx, posy])\n                    gl_utils.draw_circle(center=pos_c, radius=self.drag_widget.r_mask * zoom, alpha=0.5)\n            \n            rescale = self._tex_obj.width / 512 * zoom\n            \n            for point in self.drag_widget.targets:\n                pos_x = self.pane_w + max_w / 2 + (point[1] - self.image_w//2) * zoom\n                pos_y = max_h / 2 + (point[0] - self.image_h//2) * zoom\n                gl_utils.draw_circle(center=np.array([pos_x, pos_y]), color=[0,0,1], radius=9 * rescale)\n            \n            for point in self.drag_widget.points:\n                pos_x = self.pane_w + max_w / 2 + (point[1] - self.image_w//2) * zoom\n                pos_y = max_h / 2 + (point[0] - self.image_h//2) * zoom\n                gl_utils.draw_circle(center=np.array([pos_x, pos_y]), color=[1,0,0], radius=9 * rescale)\n\n            for point, target in zip(self.drag_widget.points, self.drag_widget.targets):\n                t_x = self.pane_w + max_w / 2 + (target[1] - self.image_w//2) * zoom\n                t_y = max_h / 2 + (target[0] - self.image_h//2) * zoom\n\n                p_x = self.pane_w + max_w / 2 + (point[1] - self.image_w//2) * zoom\n                p_y = max_h / 2 + (point[0] - self.image_h//2) * zoom\n\n                gl_utils.draw_arrow(p_x, p_y, t_x, t_y, l=8 * rescale, width = 3 * rescale)\n\n            imshow_w = int(self._tex_obj.width * zoom)\n            imshow_h = int(self._tex_obj.height * zoom)\n            self._image_area = [int(self.pane_w + max_w / 2 - imshow_w / 2), int(max_h / 2 - imshow_h / 2), imshow_w, imshow_h]\n        if 'error' in self.result:\n            self.print_error(self.result.error)\n            if 'message' not in self.result:\n                self.result.message = str(self.result.error)\n        if 'message' in self.result:\n            tex = text_utils.get_texture(self.result.message, size=self.font_size, max_width=max_w, max_height=max_h, outline=2)\n            tex.draw(pos=pos, align=0.5, rint=True, color=1)\n\n        # End frame.\n        self._adjust_font_size()\n        imgui.end()\n        self.end_frame()\n\n#----------------------------------------------------------------------------\n\nclass AsyncRenderer:\n    def __init__(self):\n        self._closed        = False\n        self._is_async      = False\n        self._cur_args      = None\n        self._cur_result    = None\n        self._cur_stamp     = 0\n        self._renderer_obj  = None\n        self._args_queue    = None\n        self._result_queue  = None\n        self._process       = None\n\n    def close(self):\n        self._closed = True\n        self._renderer_obj = None\n        if self._process is not None:\n            self._process.terminate()\n        self._process = None\n        self._args_queue = None\n        self._result_queue = None\n\n    @property\n    def is_async(self):\n        return self._is_async\n\n    def set_async(self, is_async):\n        self._is_async = is_async\n\n    def set_args(self, **args):\n        assert not self._closed\n        args2 = args.copy()\n        args_mask = args2.pop('mask')\n        if self._cur_args:\n            _cur_args = self._cur_args.copy()\n            cur_args_mask = _cur_args.pop('mask')\n        else:\n            _cur_args = self._cur_args\n        # if args != self._cur_args:\n        if args2 != _cur_args:\n            if self._is_async:\n                self._set_args_async(**args)\n            else:\n                self._set_args_sync(**args)\n            self._cur_args = args\n\n    def _set_args_async(self, **args):\n        if self._process is None:\n            self._args_queue = multiprocessing.Queue()\n            self._result_queue = multiprocessing.Queue()\n            try:\n                multiprocessing.set_start_method('spawn')\n            except RuntimeError:\n                pass\n            self._process = multiprocessing.Process(target=self._process_fn, args=(self._args_queue, self._result_queue), daemon=True)\n            self._process.start()\n        self._args_queue.put([args, self._cur_stamp])\n\n    def _set_args_sync(self, **args):\n        if self._renderer_obj is None:\n            self._renderer_obj = renderer.Renderer()\n        self._cur_result = self._renderer_obj.render(**args)\n\n    def get_result(self):\n        assert not self._closed\n        if self._result_queue is not None:\n            while self._result_queue.qsize() > 0:\n                result, stamp = self._result_queue.get()\n                if stamp == self._cur_stamp:\n                    self._cur_result = result\n        return self._cur_result\n\n    def clear_result(self):\n        assert not self._closed\n        self._cur_args = None\n        self._cur_result = None\n        self._cur_stamp += 1\n\n    @staticmethod\n    def _process_fn(args_queue, result_queue):\n        renderer_obj = renderer.Renderer()\n        cur_args = None\n        cur_stamp = None\n        while True:\n            args, stamp = args_queue.get()\n            while args_queue.qsize() > 0:\n                args, stamp = args_queue.get()\n            if args != cur_args or stamp != cur_stamp:\n                result = renderer_obj.render(**args)\n                if 'error' in result:\n                    result.error = renderer.CapturedException(result.error)\n                result_queue.put([result, stamp])\n                cur_args = args\n                cur_stamp = stamp\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.argument('pkls', metavar='PATH', nargs=-1)\n@click.option('--capture-dir', help='Where to save screenshot captures', metavar='PATH', default=None)\n@click.option('--browse-dir', help='Specify model path for the \\'Browse...\\' button', metavar='PATH')\ndef main(\n    pkls,\n    capture_dir,\n    browse_dir\n):\n    \"\"\"Interactive model visualizer.\n\n    Optional PATH argument can be used specify which .pkl file to load.\n    \"\"\"\n    viz = Visualizer(capture_dir=capture_dir)\n\n    if browse_dir is not None:\n        viz.pickle_widget.search_dirs = [browse_dir]\n\n    # List pickles.\n    if len(pkls) > 0:\n        for pkl in pkls:\n            viz.add_recent_pickle(pkl)\n        viz.load_pickle(pkls[0])\n    else:\n        pretrained = [\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqcat-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqdog-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqv2-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqwild-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-brecahad-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-celebahq-256x256.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-cifar10-32x32.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhqu-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhqu-256x256.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-lsundog-256x256.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-metfaces-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-metfacesu-1024x1024.pkl'\n        ]\n\n        # Populate recent pickles list with pretrained model URLs.\n        for url in pretrained:\n            viz.add_recent_pickle(url)\n\n    # Run.\n    while not viz.should_close():\n        viz.draw_frame()\n    viz.close()\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "visualizer_drag_gradio.py",
          "type": "blob",
          "size": 31.6328125,
          "content": "import os\nimport os.path as osp\nfrom argparse import ArgumentParser\nfrom functools import partial\n\nimport gradio as gr\nimport numpy as np\nimport torch\nfrom PIL import Image\n\nimport dnnlib\nfrom gradio_utils import (ImageMask, draw_mask_on_image, draw_points_on_image,\n                          get_latest_points_pair, get_valid_mask,\n                          on_change_single_global_state)\nfrom viz.renderer import Renderer, add_watermark_np\n\nparser = ArgumentParser()\nparser.add_argument('--share', action='store_true',default='True')\nparser.add_argument('--cache-dir', type=str, default='./checkpoints')\nparser.add_argument(\n    \"--listen\",\n    action=\"store_true\",\n    help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\",\n)\nargs = parser.parse_args()\n\ncache_dir = args.cache_dir\n\ndevice = 'cuda'\n\n\ndef reverse_point_pairs(points):\n    new_points = []\n    for p in points:\n        new_points.append([p[1], p[0]])\n    return new_points\n\n\ndef clear_state(global_state, target=None):\n    \"\"\"Clear target history state from global_state\n    If target is not defined, points and mask will be both removed.\n    1. set global_state['points'] as empty dict\n    2. set global_state['mask'] as full-one mask.\n    \"\"\"\n    if target is None:\n        target = ['point', 'mask']\n    if not isinstance(target, list):\n        target = [target]\n    if 'point' in target:\n        global_state['points'] = dict()\n        print('Clear Points State!')\n    if 'mask' in target:\n        image_raw = global_state[\"images\"][\"image_raw\"]\n        global_state['mask'] = np.ones((image_raw.size[1], image_raw.size[0]),\n                                       dtype=np.uint8)\n        print('Clear mask State!')\n\n    return global_state\n\n\ndef init_images(global_state):\n    \"\"\"This function is called only ones with Gradio App is started.\n    0. pre-process global_state, unpack value from global_state of need\n    1. Re-init renderer\n    2. run `renderer._render_drag_impl` with `is_drag=False` to generate\n       new image\n    3. Assign images to global state and re-generate mask\n    \"\"\"\n\n    if isinstance(global_state, gr.State):\n        state = global_state.value\n    else:\n        state = global_state\n\n    state['renderer'].init_network(\n        state['generator_params'],  # res\n        valid_checkpoints_dict[state['pretrained_weight']],  # pkl\n        state['params']['seed'],  # w0_seed,\n        None,  # w_load\n        state['params']['latent_space'] == 'w+',  # w_plus\n        'const',\n        state['params']['trunc_psi'],  # trunc_psi,\n        state['params']['trunc_cutoff'],  # trunc_cutoff,\n        None,  # input_transform\n        state['params']['lr']  # lr,\n    )\n\n    state['renderer']._render_drag_impl(state['generator_params'],\n                                        is_drag=False,\n                                        to_pil=True)\n\n    init_image = state['generator_params'].image\n    state['images']['image_orig'] = init_image\n    state['images']['image_raw'] = init_image\n    state['images']['image_show'] = Image.fromarray(\n        add_watermark_np(np.array(init_image)))\n    state['mask'] = np.ones((init_image.size[1], init_image.size[0]),\n                            dtype=np.uint8)\n    return global_state\n\n\ndef update_image_draw(image, points, mask, show_mask, global_state=None):\n\n    image_draw = draw_points_on_image(image, points)\n    if show_mask and mask is not None and not (mask == 0).all() and not (\n            mask == 1).all():\n        image_draw = draw_mask_on_image(image_draw, mask)\n\n    image_draw = Image.fromarray(add_watermark_np(np.array(image_draw)))\n    if global_state is not None:\n        global_state['images']['image_show'] = image_draw\n    return image_draw\n\n\ndef preprocess_mask_info(global_state, image):\n    \"\"\"Function to handle mask information.\n    1. last_mask is None: Do not need to change mask, return mask\n    2. last_mask is not None:\n        2.1 global_state is remove_mask:\n        2.2 global_state is add_mask:\n    \"\"\"\n    if isinstance(image, dict):\n        last_mask = get_valid_mask(image['mask'])\n    else:\n        last_mask = None\n    mask = global_state['mask']\n\n    # mask in global state is a placeholder with all 1.\n    if (mask == 1).all():\n        mask = last_mask\n\n    # last_mask = global_state['last_mask']\n    editing_mode = global_state['editing_state']\n\n    if last_mask is None:\n        return global_state\n\n    if editing_mode == 'remove_mask':\n        updated_mask = np.clip(mask - last_mask, 0, 1)\n        print(f'Last editing_state is {editing_mode}, do remove.')\n    elif editing_mode == 'add_mask':\n        updated_mask = np.clip(mask + last_mask, 0, 1)\n        print(f'Last editing_state is {editing_mode}, do add.')\n    else:\n        updated_mask = mask\n        print(f'Last editing_state is {editing_mode}, '\n              'do nothing to mask.')\n\n    global_state['mask'] = updated_mask\n    # global_state['last_mask'] = None  # clear buffer\n    return global_state\n\n\nvalid_checkpoints_dict = {\n    f.split('/')[-1].split('.')[0]: osp.join(cache_dir, f)\n    for f in os.listdir(cache_dir)\n    if (f.endswith('pkl') and osp.exists(osp.join(cache_dir, f)))\n}\nprint(f'File under cache_dir ({cache_dir}):')\nprint(os.listdir(cache_dir))\nprint('Valid checkpoint file:')\nprint(valid_checkpoints_dict)\n\ninit_pkl = 'stylegan2_lions_512_pytorch'\n\nwith gr.Blocks() as app:\n\n    # renderer = Renderer()\n    global_state = gr.State({\n        \"images\": {\n            # image_orig: the original image, change with seed/model is changed\n            # image_raw: image with mask and points, change durning optimization\n            # image_show: image showed on screen\n        },\n        \"temporal_params\": {\n            # stop\n        },\n        'mask':\n        None,  # mask for visualization, 1 for editing and 0 for unchange\n        'last_mask': None,  # last edited mask\n        'show_mask': True,  # add button\n        \"generator_params\": dnnlib.EasyDict(),\n        \"params\": {\n            \"seed\": 0,\n            \"motion_lambda\": 20,\n            \"r1_in_pixels\": 3,\n            \"r2_in_pixels\": 12,\n            \"magnitude_direction_in_pixels\": 1.0,\n            \"latent_space\": \"w+\",\n            \"trunc_psi\": 0.7,\n            \"trunc_cutoff\": None,\n            \"lr\": 0.001,\n        },\n        \"device\": device,\n        \"draw_interval\": 1,\n        \"renderer\": Renderer(disable_timing=True),\n        \"points\": {},\n        \"curr_point\": None,\n        \"curr_type_point\": \"start\",\n        'editing_state': 'add_points',\n        'pretrained_weight': init_pkl\n    })\n\n    # init image\n    global_state = init_images(global_state)\n\n    with gr.Row():\n\n        with gr.Row():\n\n            # Left --> tools\n            with gr.Column(scale=3):\n\n                # Pickle\n                with gr.Row():\n\n                    with gr.Column(scale=1, min_width=10):\n                        gr.Markdown(value='Pickle', show_label=False)\n\n                    with gr.Column(scale=4, min_width=10):\n                        form_pretrained_dropdown = gr.Dropdown(\n                            choices=list(valid_checkpoints_dict.keys()),\n                            label=\"Pretrained Model\",\n                            value=init_pkl,\n                        )\n\n                # Latent\n                with gr.Row():\n                    with gr.Column(scale=1, min_width=10):\n                        gr.Markdown(value='Latent', show_label=False)\n\n                    with gr.Column(scale=4, min_width=10):\n                        form_seed_number = gr.Number(\n                            value=global_state.value['params']['seed'],\n                            interactive=True,\n                            label=\"Seed\",\n                        )\n                        form_lr_number = gr.Number(\n                            value=global_state.value[\"params\"][\"lr\"],\n                            interactive=True,\n                            label=\"Step Size\")\n\n                        with gr.Row():\n                            with gr.Column(scale=2, min_width=10):\n                                form_reset_image = gr.Button(\"Reset Image\")\n                            with gr.Column(scale=3, min_width=10):\n                                form_latent_space = gr.Radio(\n                                    ['w', 'w+'],\n                                    value=global_state.value['params']\n                                    ['latent_space'],\n                                    interactive=True,\n                                    label='Latent space to optimize',\n                                    show_label=False,\n                                )\n\n                # Drag\n                with gr.Row():\n                    with gr.Column(scale=1, min_width=10):\n                        gr.Markdown(value='Drag', show_label=False)\n                    with gr.Column(scale=4, min_width=10):\n                        with gr.Row():\n                            with gr.Column(scale=1, min_width=10):\n                                enable_add_points = gr.Button('Add Points')\n                            with gr.Column(scale=1, min_width=10):\n                                undo_points = gr.Button('Reset Points')\n                        with gr.Row():\n                            with gr.Column(scale=1, min_width=10):\n                                form_start_btn = gr.Button(\"Start\")\n                            with gr.Column(scale=1, min_width=10):\n                                form_stop_btn = gr.Button(\"Stop\")\n\n                        form_steps_number = gr.Number(value=0,\n                                                      label=\"Steps\",\n                                                      interactive=False)\n\n                # Mask\n                with gr.Row():\n                    with gr.Column(scale=1, min_width=10):\n                        gr.Markdown(value='Mask', show_label=False)\n                    with gr.Column(scale=4, min_width=10):\n                        enable_add_mask = gr.Button('Edit Flexible Area')\n                        with gr.Row():\n                            with gr.Column(scale=1, min_width=10):\n                                form_reset_mask_btn = gr.Button(\"Reset mask\")\n                            with gr.Column(scale=1, min_width=10):\n                                show_mask = gr.Checkbox(\n                                    label='Show Mask',\n                                    value=global_state.value['show_mask'],\n                                    show_label=False)\n\n                        with gr.Row():\n                            form_lambda_number = gr.Number(\n                                value=global_state.value[\"params\"]\n                                [\"motion_lambda\"],\n                                interactive=True,\n                                label=\"Lambda\",\n                            )\n\n                form_draw_interval_number = gr.Number(\n                    value=global_state.value[\"draw_interval\"],\n                    label=\"Draw Interval (steps)\",\n                    interactive=True,\n                    visible=False)\n\n            # Right --> Image\n            with gr.Column(scale=8):\n                form_image = ImageMask(\n                    value=global_state.value['images']['image_show'],\n                    brush_radius=20).style(\n                        width=768,\n                        height=768)  # NOTE: hard image size code here.\n    gr.Markdown(\"\"\"\n        ## Quick Start\n\n        1. Select desired `Pretrained Model` and adjust `Seed` to generate an\n           initial image.\n        2. Click on image to add control points.\n        3. Click `Start` and enjoy it!\n\n        ## Advance Usage\n\n        1. Change `Step Size` to adjust learning rate in drag optimization.\n        2. Select `w` or `w+` to change latent space to optimize:\n        * Optimize on `w` space may cause greater influence to the image.\n        * Optimize on `w+` space may work slower than `w`, but usually achieve\n          better results.\n        * Note that changing the latent space will reset the image, points and\n          mask (this has the same effect as `Reset Image` button).\n        3. Click `Edit Flexible Area` to create a mask and constrain the\n           unmasked region to remain unchanged.\n        \"\"\")\n    gr.HTML(\"\"\"\n        <style>\n            .container {\n                position: absolute;\n                height: 50px;\n                text-align: center;\n                line-height: 50px;\n                width: 100%;\n            }\n        </style>\n        <div class=\"container\">\n        Gradio demo supported by\n        <img src=\"https://avatars.githubusercontent.com/u/10245193?s=200&v=4\" height=\"20\" width=\"20\" style=\"display:inline;\">\n        <a href=\"https://github.com/open-mmlab/mmagic\">OpenMMLab MMagic</a>\n        </div>\n        \"\"\")\n\n    # Network & latents tab listeners\n    def on_change_pretrained_dropdown(pretrained_value, global_state):\n        \"\"\"Function to handle model change.\n        1. Set pretrained value to global_state\n        2. Re-init images and clear all states\n        \"\"\"\n\n        global_state['pretrained_weight'] = pretrained_value\n        init_images(global_state)\n        clear_state(global_state)\n\n        return global_state, global_state[\"images\"]['image_show']\n\n    form_pretrained_dropdown.change(\n        on_change_pretrained_dropdown,\n        inputs=[form_pretrained_dropdown, global_state],\n        outputs=[global_state, form_image],\n    )\n\n    def on_click_reset_image(global_state):\n        \"\"\"Reset image to the original one and clear all states\n        1. Re-init images\n        2. Clear all states\n        \"\"\"\n\n        init_images(global_state)\n        clear_state(global_state)\n\n        return global_state, global_state['images']['image_show']\n\n    form_reset_image.click(\n        on_click_reset_image,\n        inputs=[global_state],\n        outputs=[global_state, form_image],\n    )\n\n    # Update parameters\n    def on_change_update_image_seed(seed, global_state):\n        \"\"\"Function to handle generation seed change.\n        1. Set seed to global_state\n        2. Re-init images and clear all states\n        \"\"\"\n\n        global_state[\"params\"][\"seed\"] = int(seed)\n        init_images(global_state)\n        clear_state(global_state)\n\n        return global_state, global_state['images']['image_show']\n\n    form_seed_number.change(\n        on_change_update_image_seed,\n        inputs=[form_seed_number, global_state],\n        outputs=[global_state, form_image],\n    )\n\n    def on_click_latent_space(latent_space, global_state):\n        \"\"\"Function to reset latent space to optimize.\n        NOTE: this function we reset the image and all controls\n        1. Set latent-space to global_state\n        2. Re-init images and clear all state\n        \"\"\"\n\n        global_state['params']['latent_space'] = latent_space\n        init_images(global_state)\n        clear_state(global_state)\n\n        return global_state, global_state['images']['image_show']\n\n    form_latent_space.change(on_click_latent_space,\n                             inputs=[form_latent_space, global_state],\n                             outputs=[global_state, form_image])\n\n    # ==== Params\n    form_lambda_number.change(\n        partial(on_change_single_global_state, [\"params\", \"motion_lambda\"]),\n        inputs=[form_lambda_number, global_state],\n        outputs=[global_state],\n    )\n\n    def on_change_lr(lr, global_state):\n        if lr == 0:\n            print('lr is 0, do nothing.')\n            return global_state\n        else:\n            global_state[\"params\"][\"lr\"] = lr\n            renderer = global_state['renderer']\n            renderer.update_lr(lr)\n            print('New optimizer: ')\n            print(renderer.w_optim)\n        return global_state\n\n    form_lr_number.change(\n        on_change_lr,\n        inputs=[form_lr_number, global_state],\n        outputs=[global_state],\n    )\n\n    def on_click_start(global_state, image):\n        p_in_pixels = []\n        t_in_pixels = []\n        valid_points = []\n\n        # handle of start drag in mask editing mode\n        global_state = preprocess_mask_info(global_state, image)\n\n        # Prepare the points for the inference\n        if len(global_state[\"points\"]) == 0:\n            # yield on_click_start_wo_points(global_state, image)\n            image_raw = global_state['images']['image_raw']\n            update_image_draw(\n                image_raw,\n                global_state['points'],\n                global_state['mask'],\n                global_state['show_mask'],\n                global_state,\n            )\n\n            yield (\n                global_state,\n                0,\n                global_state['images']['image_show'],\n                # gr.File.update(visible=False),\n                gr.Button.update(interactive=True),\n                gr.Button.update(interactive=True),\n                gr.Button.update(interactive=True),\n                gr.Button.update(interactive=True),\n                gr.Button.update(interactive=True),\n                # latent space\n                gr.Radio.update(interactive=True),\n                gr.Button.update(interactive=True),\n                # NOTE: disable stop button\n                gr.Button.update(interactive=False),\n\n                # update other comps\n                gr.Dropdown.update(interactive=True),\n                gr.Number.update(interactive=True),\n                gr.Number.update(interactive=True),\n                gr.Button.update(interactive=True),\n                gr.Button.update(interactive=True),\n                gr.Checkbox.update(interactive=True),\n                # gr.Number.update(interactive=True),\n                gr.Number.update(interactive=True),\n            )\n        else:\n\n            # Transform the points into torch tensors\n            for key_point, point in global_state[\"points\"].items():\n                try:\n                    p_start = point.get(\"start_temp\", point[\"start\"])\n                    p_end = point[\"target\"]\n\n                    if p_start is None or p_end is None:\n                        continue\n\n                except KeyError:\n                    continue\n\n                p_in_pixels.append(p_start)\n                t_in_pixels.append(p_end)\n                valid_points.append(key_point)\n\n            mask = torch.tensor(global_state['mask']).float()\n            drag_mask = 1 - mask\n\n            renderer: Renderer = global_state[\"renderer\"]\n            global_state['temporal_params']['stop'] = False\n            global_state['editing_state'] = 'running'\n\n            # reverse points order\n            p_to_opt = reverse_point_pairs(p_in_pixels)\n            t_to_opt = reverse_point_pairs(t_in_pixels)\n            print('Running with:')\n            print(f'    Source: {p_in_pixels}')\n            print(f'    Target: {t_in_pixels}')\n            step_idx = 0\n            while True:\n                if global_state[\"temporal_params\"][\"stop\"]:\n                    break\n\n                # do drage here!\n                renderer._render_drag_impl(\n                    global_state['generator_params'],\n                    p_to_opt,  # point\n                    t_to_opt,  # target\n                    drag_mask,  # mask,\n                    global_state['params']['motion_lambda'],  # lambda_mask\n                    reg=0,\n                    feature_idx=5,  # NOTE: do not support change for now\n                    r1=global_state['params']['r1_in_pixels'],  # r1\n                    r2=global_state['params']['r2_in_pixels'],  # r2\n                    # random_seed     = 0,\n                    # noise_mode      = 'const',\n                    trunc_psi=global_state['params']['trunc_psi'],\n                    # force_fp32      = False,\n                    # layer_name      = None,\n                    # sel_channels    = 3,\n                    # base_channel    = 0,\n                    # img_scale_db    = 0,\n                    # img_normalize   = False,\n                    # untransform     = False,\n                    is_drag=True,\n                    to_pil=True)\n\n                if step_idx % global_state['draw_interval'] == 0:\n                    print('Current Source:')\n                    for key_point, p_i, t_i in zip(valid_points, p_to_opt,\n                                                   t_to_opt):\n                        global_state[\"points\"][key_point][\"start_temp\"] = [\n                            p_i[1],\n                            p_i[0],\n                        ]\n                        global_state[\"points\"][key_point][\"target\"] = [\n                            t_i[1],\n                            t_i[0],\n                        ]\n                        start_temp = global_state[\"points\"][key_point][\n                            \"start_temp\"]\n                        print(f'    {start_temp}')\n\n                    image_result = global_state['generator_params']['image']\n                    image_draw = update_image_draw(\n                        image_result,\n                        global_state['points'],\n                        global_state['mask'],\n                        global_state['show_mask'],\n                        global_state,\n                    )\n                    global_state['images']['image_raw'] = image_result\n\n                yield (\n                    global_state,\n                    step_idx,\n                    global_state['images']['image_show'],\n                    # gr.File.update(visible=False),\n                    gr.Button.update(interactive=False),\n                    gr.Button.update(interactive=False),\n                    gr.Button.update(interactive=False),\n                    gr.Button.update(interactive=False),\n                    gr.Button.update(interactive=False),\n                    # latent space\n                    gr.Radio.update(interactive=False),\n                    gr.Button.update(interactive=False),\n                    # enable stop button in loop\n                    gr.Button.update(interactive=True),\n\n                    # update other comps\n                    gr.Dropdown.update(interactive=False),\n                    gr.Number.update(interactive=False),\n                    gr.Number.update(interactive=False),\n                    gr.Button.update(interactive=False),\n                    gr.Button.update(interactive=False),\n                    gr.Checkbox.update(interactive=False),\n                    # gr.Number.update(interactive=False),\n                    gr.Number.update(interactive=False),\n                )\n\n                # increate step\n                step_idx += 1\n\n            image_result = global_state['generator_params']['image']\n            global_state['images']['image_raw'] = image_result\n            image_draw = update_image_draw(image_result,\n                                           global_state['points'],\n                                           global_state['mask'],\n                                           global_state['show_mask'],\n                                           global_state)\n\n            # fp = NamedTemporaryFile(suffix=\".png\", delete=False)\n            # image_result.save(fp, \"PNG\")\n\n            global_state['editing_state'] = 'add_points'\n\n            yield (\n                global_state,\n                0,  # reset step to 0 after stop.\n                global_state['images']['image_show'],\n                # gr.File.update(visible=True, value=fp.name),\n                gr.Button.update(interactive=True),\n                gr.Button.update(interactive=True),\n                gr.Button.update(interactive=True),\n                gr.Button.update(interactive=True),\n                gr.Button.update(interactive=True),\n                # latent space\n                gr.Radio.update(interactive=True),\n                gr.Button.update(interactive=True),\n                # NOTE: disable stop button with loop finish\n                gr.Button.update(interactive=False),\n\n                # update other comps\n                gr.Dropdown.update(interactive=True),\n                gr.Number.update(interactive=True),\n                gr.Number.update(interactive=True),\n                gr.Checkbox.update(interactive=True),\n                gr.Number.update(interactive=True),\n            )\n\n    form_start_btn.click(\n        on_click_start,\n        inputs=[global_state, form_image],\n        outputs=[\n            global_state,\n            form_steps_number,\n            form_image,\n            # form_download_result_file,\n            # >>> buttons\n            form_reset_image,\n            enable_add_points,\n            enable_add_mask,\n            undo_points,\n            form_reset_mask_btn,\n            form_latent_space,\n            form_start_btn,\n            form_stop_btn,\n            # <<< buttonm\n            # >>> inputs comps\n            form_pretrained_dropdown,\n            form_seed_number,\n            form_lr_number,\n            show_mask,\n            form_lambda_number,\n        ],\n    )\n\n    def on_click_stop(global_state):\n        \"\"\"Function to handle stop button is clicked.\n        1. send a stop signal by set global_state[\"temporal_params\"][\"stop\"] as True\n        2. Disable Stop button\n        \"\"\"\n        global_state[\"temporal_params\"][\"stop\"] = True\n\n        return global_state, gr.Button.update(interactive=False)\n\n    form_stop_btn.click(on_click_stop,\n                        inputs=[global_state],\n                        outputs=[global_state, form_stop_btn])\n\n    form_draw_interval_number.change(\n        partial(\n            on_change_single_global_state,\n            \"draw_interval\",\n            map_transform=lambda x: int(x),\n        ),\n        inputs=[form_draw_interval_number, global_state],\n        outputs=[global_state],\n    )\n\n    def on_click_remove_point(global_state):\n        choice = global_state[\"curr_point\"]\n        del global_state[\"points\"][choice]\n\n        choices = list(global_state[\"points\"].keys())\n\n        if len(choices) > 0:\n            global_state[\"curr_point\"] = choices[0]\n\n        return (\n            gr.Dropdown.update(choices=choices, value=choices[0]),\n            global_state,\n        )\n\n    # Mask\n    def on_click_reset_mask(global_state):\n        global_state['mask'] = np.ones(\n            (\n                global_state[\"images\"][\"image_raw\"].size[1],\n                global_state[\"images\"][\"image_raw\"].size[0],\n            ),\n            dtype=np.uint8,\n        )\n        image_draw = update_image_draw(global_state['images']['image_raw'],\n                                       global_state['points'],\n                                       global_state['mask'],\n                                       global_state['show_mask'], global_state)\n        return global_state, image_draw\n\n    form_reset_mask_btn.click(\n        on_click_reset_mask,\n        inputs=[global_state],\n        outputs=[global_state, form_image],\n    )\n\n    # Image\n    def on_click_enable_draw(global_state, image):\n        \"\"\"Function to start add mask mode.\n        1. Preprocess mask info from last state\n        2. Change editing state to add_mask\n        3. Set curr image with points and mask\n        \"\"\"\n        global_state = preprocess_mask_info(global_state, image)\n        global_state['editing_state'] = 'add_mask'\n        image_raw = global_state['images']['image_raw']\n        image_draw = update_image_draw(image_raw, global_state['points'],\n                                       global_state['mask'], True,\n                                       global_state)\n        return (global_state,\n                gr.Image.update(value=image_draw, interactive=True))\n\n    def on_click_remove_draw(global_state, image):\n        \"\"\"Function to start remove mask mode.\n        1. Preprocess mask info from last state\n        2. Change editing state to remove_mask\n        3. Set curr image with points and mask\n        \"\"\"\n        global_state = preprocess_mask_info(global_state, image)\n        global_state['edinting_state'] = 'remove_mask'\n        image_raw = global_state['images']['image_raw']\n        image_draw = update_image_draw(image_raw, global_state['points'],\n                                       global_state['mask'], True,\n                                       global_state)\n        return (global_state,\n                gr.Image.update(value=image_draw, interactive=True))\n\n    enable_add_mask.click(on_click_enable_draw,\n                          inputs=[global_state, form_image],\n                          outputs=[\n                              global_state,\n                              form_image,\n                          ])\n\n    def on_click_add_point(global_state, image: dict):\n        \"\"\"Function switch from add mask mode to add points mode.\n        1. Updaste mask buffer if need\n        2. Change global_state['editing_state'] to 'add_points'\n        3. Set current image with mask\n        \"\"\"\n\n        global_state = preprocess_mask_info(global_state, image)\n        global_state['editing_state'] = 'add_points'\n        mask = global_state['mask']\n        image_raw = global_state['images']['image_raw']\n        image_draw = update_image_draw(image_raw, global_state['points'], mask,\n                                       global_state['show_mask'], global_state)\n\n        return (global_state,\n                gr.Image.update(value=image_draw, interactive=False))\n\n    enable_add_points.click(on_click_add_point,\n                            inputs=[global_state, form_image],\n                            outputs=[global_state, form_image])\n\n    def on_click_image(global_state, evt: gr.SelectData):\n        \"\"\"This function only support click for point selection\n        \"\"\"\n        xy = evt.index\n        if global_state['editing_state'] != 'add_points':\n            print(f'In {global_state[\"editing_state\"]} state. '\n                  'Do not add points.')\n\n            return global_state, global_state['images']['image_show']\n\n        points = global_state[\"points\"]\n\n        point_idx = get_latest_points_pair(points)\n        if point_idx is None:\n            points[0] = {'start': xy, 'target': None}\n            print(f'Click Image - Start - {xy}')\n        elif points[point_idx].get('target', None) is None:\n            points[point_idx]['target'] = xy\n            print(f'Click Image - Target - {xy}')\n        else:\n            points[point_idx + 1] = {'start': xy, 'target': None}\n            print(f'Click Image - Start - {xy}')\n\n        image_raw = global_state['images']['image_raw']\n        image_draw = update_image_draw(\n            image_raw,\n            global_state['points'],\n            global_state['mask'],\n            global_state['show_mask'],\n            global_state,\n        )\n\n        return global_state, image_draw\n\n    form_image.select(\n        on_click_image,\n        inputs=[global_state],\n        outputs=[global_state, form_image],\n    )\n\n    def on_click_clear_points(global_state):\n        \"\"\"Function to handle clear all control points\n        1. clear global_state['points'] (clear_state)\n        2. re-init network\n        2. re-draw image\n        \"\"\"\n        clear_state(global_state, target='point')\n\n        renderer: Renderer = global_state[\"renderer\"]\n        renderer.feat_refs = None\n\n        image_raw = global_state['images']['image_raw']\n        image_draw = update_image_draw(image_raw, {}, global_state['mask'],\n                                       global_state['show_mask'], global_state)\n        return global_state, image_draw\n\n    undo_points.click(on_click_clear_points,\n                      inputs=[global_state],\n                      outputs=[global_state, form_image])\n\n    def on_click_show_mask(global_state, show_mask):\n        \"\"\"Function to control whether show mask on image.\"\"\"\n        global_state['show_mask'] = show_mask\n\n        image_raw = global_state['images']['image_raw']\n        image_draw = update_image_draw(\n            image_raw,\n            global_state['points'],\n            global_state['mask'],\n            global_state['show_mask'],\n            global_state,\n        )\n        return global_state, image_draw\n\n    show_mask.change(\n        on_click_show_mask,\n        inputs=[global_state, show_mask],\n        outputs=[global_state, form_image],\n    )\n\ngr.close_all()\napp.queue(concurrency_count=3, max_size=20)\napp.launch(share=args.share, server_name=\"0.0.0.0\" if args.listen else \"127.0.0.1\")\n"
        },
        {
          "name": "viz",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}