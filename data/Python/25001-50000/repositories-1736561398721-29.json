{
  "metadata": {
    "timestamp": 1736561398721,
    "page": 29,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "hiyouga/LLaMA-Factory",
      "stars": 37678,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.1220703125,
          "content": ".vscode\n.git\n.github\n.venv\ncache\ndata\ndocker\nsaves\nhf_cache\nms_cache\nom_cache\noutput\n.dockerignore\n.gitattributes\n.gitignore\n"
        },
        {
          "name": ".env.local",
          "type": "blob",
          "size": 0.603515625,
          "content": "# Note: actually we do not support .env, just for reference\n# api\nAPI_HOST=\nAPI_PORT=\nAPI_KEY=\nAPI_MODEL_NAME=\nFASTAPI_ROOT_PATH=\nMAX_CONCURRENT=\n# general\nDISABLE_VERSION_CHECK=\nFORCE_CHECK_IMPORTS=\nLLAMAFACTORY_VERBOSITY=\nUSE_MODELSCOPE_HUB=\nUSE_OPENMIND_HUB=\nUSE_RAY=\nRECORD_VRAM=\n# torchrun\nFORCE_TORCHRUN=\nMASTER_ADDR=\nMASTER_PORT=\nNNODES=\nNODE_RANK=\nNPROC_PER_NODE=\n# wandb\nWANDB_DISABLED=\nWANDB_PROJECT=\nWANDB_API_KEY=\n# gradio ui\nGRADIO_SHARE=\nGRADIO_SERVER_NAME=\nGRADIO_SERVER_PORT=\nGRADIO_ROOT_PATH=\nGRADIO_IPV6=\n# setup\nENABLE_SHORT_CONSOLE=1\n# reserved (do not use)\nLLAMABOARD_ENABLED=\nLLAMABOARD_WORKDIR=\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.064453125,
          "content": "# Auto detect text files and perform LF normalization\n* text=auto\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.1455078125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n.idea/\n\n# vscode\n.vscode/\n\n# custom .gitignore\nms_cache/\nhf_cache/\nom_cache/\ncache/\nconfig/\nsaves/\noutput/\nwandb/\nswanlog/\ngenerated_predictions.jsonl\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.6953125,
          "content": "repos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n    -   id: check-ast\n    -   id: check-added-large-files\n        args: ['--maxkb=25000']\n    -   id: check-merge-conflict\n    -   id: check-yaml\n    -   id: debug-statements\n    -   id: end-of-file-fixer\n    -   id: trailing-whitespace\n        args: [--markdown-linebreak-ext=md]\n    -   id: no-commit-to-branch\n        args: ['--branch', 'main']\n\n-   repo: https://github.com/asottile/pyupgrade\n    rev: v3.17.0\n    hooks:\n    -   id: pyupgrade\n        args: [--py38-plus]\n\n-   repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.6.9\n    hooks:\n    -   id: ruff\n        args: [--fix]\n    -   id: ruff-format\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 1.345703125,
          "content": "cff-version: 1.2.0\ndate-released: 2024-03\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n- family-names: \"Zheng\"\n  given-names: \"Yaowei\"\n- family-names: \"Zhang\"\n  given-names: \"Richong\"\n- family-names: \"Zhang\"\n  given-names: \"Junhao\"\n- family-names: \"Ye\"\n  given-names: \"Yanhan\"\n- family-names: \"Luo\"\n  given-names: \"Zheyan\"\n- family-names: \"Feng\"\n  given-names: \"Zhangchi\"\n- family-names: \"Ma\"\n  given-names: \"Yongqiang\"\ntitle: \"LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models\"\nurl: \"https://arxiv.org/abs/2403.13372\"\npreferred-citation:\n  type: conference-paper\n  conference:\n    name: \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\"\n  authors:\n    - family-names: \"Zheng\"\n      given-names: \"Yaowei\"\n    - family-names: \"Zhang\"\n      given-names: \"Richong\"\n    - family-names: \"Zhang\"\n      given-names: \"Junhao\"\n    - family-names: \"Ye\"\n      given-names: \"Yanhan\"\n    - family-names: \"Luo\"\n      given-names: \"Zheyan\"\n    - family-names: \"Feng\"\n      given-names: \"Zhangchi\"\n    - family-names: \"Ma\"\n      given-names: \"Yongqiang\"\n  title: \"LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models\"\n  url: \"https://arxiv.org/abs/2403.13372\"\n  year: 2024\n  publisher: \"Association for Computational Linguistics\"\n  address: \"Bangkok, Thailand\"\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.05859375,
          "content": "Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0322265625,
          "content": "include LICENSE requirements.txt\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.3818359375,
          "content": ".PHONY: build commit quality style test\n\ncheck_dirs := scripts src tests setup.py\n\nbuild:\n\tpip install build && python -m build\n\ncommit:\n\tpre-commit install\n\tpre-commit run --all-files\n\nquality:\n\truff check $(check_dirs)\n\truff format --check $(check_dirs)\n\nstyle:\n\truff check $(check_dirs) --fix\n\truff format $(check_dirs)\n\ntest:\n\tCUDA_VISIBLE_DEVICES= WANDB_DISABLED=true pytest -vv tests/\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 55.16015625,
          "content": "![# LLaMA Factory](assets/logo.png)\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)\n[![GitHub Code License](https://img.shields.io/github/license/hiyouga/LLaMA-Factory)](LICENSE)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)\n[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)\n[![Citation](https://img.shields.io/badge/citation-210-green)](https://scholar.google.com/scholar?cites=12620864006390196564)\n[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/LLaMA-Factory/pulls)\n[![Discord](https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&style=flat)](https://discord.gg/rKfvV9r9FK)\n[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)\n[![Open in DSW](https://gallery.pai-ml.com/assets/open-in-dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)\n[![Spaces](https://img.shields.io/badge/ðŸ¤—-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)\n[![Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)\n[![SageMaker](https://img.shields.io/badge/SageMaker-Open%20in%20AWS-blue)](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/)\n[![GitCode](https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg)](https://gitcode.com/zhengyaowei/LLaMA-Factory)\n\n[![GitHub Tread](https://trendshift.io/api/badge/repositories/4535)](https://trendshift.io/repositories/4535)\n\nðŸ‘‹ Join our [WeChat](assets/wechat.jpg) or [NPU user group](assets/wechat_npu.jpg).\n\n\\[ English | [ä¸­æ–‡](README_zh.md) \\]\n\n**Fine-tuning a large language model can be easy as...**\n\nhttps://github.com/user-attachments/assets/7c96b465-9df7-45f4-8053-bf03e58386d3\n\nChoose your path:\n\n- **Documentation (WIP)**: https://llamafactory.readthedocs.io/zh-cn/latest/\n- **Colab**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing\n- **Local machine**: Please refer to [usage](#getting-started)\n- **PAI-DSW**: [Llama3 Example](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) | [Qwen2-VL Example](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl)\n- **Amazon SageMaker**: [Blog](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/)\n\n> [!NOTE]\n> Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.\n\n## Table of Contents\n\n- [Features](#features)\n- [Benchmark](#benchmark)\n- [Changelog](#changelog)\n- [Supported Models](#supported-models)\n- [Supported Training Approaches](#supported-training-approaches)\n- [Provided Datasets](#provided-datasets)\n- [Requirement](#requirement)\n- [Getting Started](#getting-started)\n  - [Installation](#installation)\n  - [Data Preparation](#data-preparation)\n  - [Quickstart](#quickstart)\n  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n  - [Build Docker](#build-docker)\n  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)\n  - [Download from ModelScope Hub](#download-from-modelscope-hub)\n  - [Download from Modelers Hub](#download-from-modelers-hub)\n  - [Use W&B Logger](#use-wb-logger)\n  - [Use SwanLab Logger](#use-swanlab-logger)\n- [Projects using LLaMA Factory](#projects-using-llama-factory)\n- [License](#license)\n- [Citation](#citation)\n- [Acknowledgement](#acknowledgement)\n\n## Features\n\n- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, Yi, Gemma, Baichuan, ChatGLM, Phi, etc.\n- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.\n- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.\n- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [Adam-mini](https://github.com/zyushun/Adam-mini), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ, PiSSA and Agent tuning.\n- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), RoPE scaling, NEFTune and rsLoRA.\n- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, SwanLab, etc.\n- **Faster inference**: OpenAI-style API, Gradio UI and CLI with vLLM worker.\n\n## Benchmark\n\nCompared to ChatGLM's [P-Tuning](https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning), LLaMA Factory's LoRA tuning offers up to **3.7 times faster** training speed with a better Rouge score on the advertising text generation task. By leveraging 4-bit quantization technique, LLaMA Factory's QLoRA further improves the efficiency regarding the GPU memory.\n\n![benchmark](assets/benchmark.svg)\n\n<details><summary>Definitions</summary>\n\n- **Training Speed**: the number of training samples processed per second during the training. (bs=4, cutoff_len=1024)\n- **Rouge Score**: Rouge-2 score on the development set of the [advertising text generation](https://aclanthology.org/D19-1321.pdf) task. (bs=4, cutoff_len=1024)\n- **GPU Memory**: Peak GPU memory usage in 4-bit quantized training. (bs=1, cutoff_len=1024)\n- We adopt `pre_seq_len=128` for ChatGLM's P-Tuning and `lora_rank=32` for LLaMA Factory's LoRA tuning.\n\n</details>\n\n## Changelog\n\n[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.\n\n[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.\n\n[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.\n\n<details><summary>Full Changelog</summary>\n\n[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.\n\n[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.\n\n[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)'s PR.\n\n[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.\n\n[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)'s PR.\n\n[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)'s PR.\n\n[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.\n\n[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.\n\n[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.\n\n[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.\n\n[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.\n\n[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.\n\n[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI's implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).\n\n[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.\n\n[24/03/21] Our paper \"[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)\" is available at arXiv!\n\n[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.\n\n[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.\n\n[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.\n\n[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.\n\n[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.\n\n[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.\n\n[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.\n\n[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**'s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.\n\n[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).\n\n[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelscope-hub) for usage.\n\n[23/10/21] We supported **[NEFTune](https://arxiv.org/abs/2310.05914)** trick for fine-tuning. Try `neftune_noise_alpha: 5` argument to activate NEFTune.\n\n[23/09/27] We supported **$S^2$-Attn** proposed by [LongLoRA](https://github.com/dvlab-research/LongLoRA) for the LLaMA models. Try `shift_attn: true` argument to enable shift short attention.\n\n[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See [examples](examples/README.md) for usage.\n\n[23/09/10] We supported **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**. Try `flash_attn: fa2` argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.\n\n[23/08/12] We supported **RoPE scaling** to extend the context length of the LLaMA models. Try `rope_scaling: linear` argument in training and `rope_scaling: dynamic` argument at inference to extrapolate the position embeddings.\n\n[23/08/11] We supported **[DPO training](https://arxiv.org/abs/2305.18290)** for instruction-tuned models. See [examples](examples/README.md) for usage.\n\n[23/07/31] We supported **dataset streaming**. Try `streaming: true` and `max_steps: 10000` arguments to load your dataset in streaming mode.\n\n[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos ([LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)) for details.\n\n[23/07/18] We developed an **all-in-one Web UI** for training, evaluation and inference. Try `train_web.py` to fine-tune models in your Web browser. Thank [@KanadeSiina](https://github.com/KanadeSiina) and [@codemayq](https://github.com/codemayq) for their efforts in the development.\n\n[23/07/09] We released **[FastEdit](https://github.com/hiyouga/FastEdit)** âš¡ðŸ©¹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow [FastEdit](https://github.com/hiyouga/FastEdit) if you are interested.\n\n[23/06/29] We provided a **reproducible example** of training a chat model using instruction-following datasets, see [Baichuan-7B-sft](https://huggingface.co/hiyouga/Baichuan-7B-sft) for details.\n\n[23/06/22] We aligned the [demo API](src/api_demo.py) with the [OpenAI's](https://platform.openai.com/docs/api-reference/chat) format where you can insert the fine-tuned model in **arbitrary ChatGPT-based applications**.\n\n[23/06/03] We supported quantized training and inference (aka **[QLoRA](https://github.com/artidoro/qlora)**). See [examples](examples/README.md) for usage.\n\n</details>\n\n## Supported Models\n\n| Model                                                             | Model size                       | Template         |\n| ----------------------------------------------------------------- | -------------------------------- | ---------------- |\n| [Baichuan 2](https://huggingface.co/baichuan-inc)                 | 7B/13B                           | baichuan2        |\n| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)                 | 560M/1.1B/1.7B/3B/7.1B/176B      | -                |\n| [ChatGLM3](https://huggingface.co/THUDM)                          | 6B                               | chatglm3         |\n| [Command R](https://huggingface.co/CohereForAI)                   | 35B/104B                         | cohere           |\n| [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)         | 7B/16B/67B/236B                  | deepseek         |\n| [DeepSeek 2.5/3](https://huggingface.co/deepseek-ai)              | 236B/685B                        | deepseek3        |\n| [Falcon](https://huggingface.co/tiiuae)                           | 7B/11B/40B/180B                  | falcon           |\n| [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)          | 2B/7B/9B/27B                     | gemma            |\n| [GLM-4](https://huggingface.co/THUDM)                             | 9B                               | glm4             |\n| [GPT-2](https://huggingface.co/openai-community)                  | 0.1B/0.4B/0.8B/1.5B              | -                |\n| [Granite 3.0-3.1](https://huggingface.co/ibm-granite)             | 1B/2B/3B/8B                      | granite3         |\n| [Index](https://huggingface.co/IndexTeam)                         | 1.9B                             | index            |\n| [InternLM2/InternLM2.5](https://huggingface.co/internlm)          | 7B/20B                           | intern2          |\n| [Llama](https://github.com/facebookresearch/llama)                | 7B/13B/33B/65B                   | -                |\n| [Llama 2](https://huggingface.co/meta-llama)                      | 7B/13B/70B                       | llama2           |\n| [Llama 3-3.3](https://huggingface.co/meta-llama)                  | 1B/3B/8B/70B                     | llama3           |\n| [Llama 3.2 Vision](https://huggingface.co/meta-llama)             | 11B/90B                          | mllama           |\n| [LLaVA-1.5](https://huggingface.co/llava-hf)                      | 7B/13B                           | llava            |\n| [LLaVA-NeXT](https://huggingface.co/llava-hf)                     | 7B/8B/13B/34B/72B/110B           | llava_next       |\n| [LLaVA-NeXT-Video](https://huggingface.co/llava-hf)               | 7B/34B                           | llava_next_video |\n| [MiniCPM](https://huggingface.co/openbmb)                         | 1B/2B/4B                         | cpm/cpm3         |\n| [Mistral/Mixtral](https://huggingface.co/mistralai)               | 7B/8x7B/8x22B                    | mistral          |\n| [OLMo](https://huggingface.co/allenai)                            | 1B/7B                            | -                |\n| [PaliGemma/PaliGemma2](https://huggingface.co/google)             | 3B/10B/28B                       | paligemma        |\n| [Phi-1.5/Phi-2](https://huggingface.co/microsoft)                 | 1.3B/2.7B                        | -                |\n| [Phi-3/Phi-3.5](https://huggingface.co/microsoft)                 | 4B/14B                           | phi              |\n| [Phi-3-small](https://huggingface.co/microsoft)                   | 7B                               | phi_small        |\n| [Phi-4](https://huggingface.co/microsoft)                         | 14B                              | phi4             |\n| [Pixtral](https://huggingface.co/mistralai)                       | 12B                              | pixtral          |\n| [Qwen/QwQ (1-2.5) (Code/Math/MoE)](https://huggingface.co/Qwen)   | 0.5B/1.5B/3B/7B/14B/32B/72B/110B | qwen             |\n| [Qwen2-VL/QVQ](https://huggingface.co/Qwen)                       | 2B/7B/72B                        | qwen2_vl         |\n| [Skywork o1](https://huggingface.co/Skywork)                      | 8B                               | skywork_o1       |\n| [StarCoder 2](https://huggingface.co/bigcode)                     | 3B/7B/15B                        | -                |\n| [TeleChat2](https://huggingface.co/Tele-AI)                       | 3B/7B/35B/115B                   | telechat2        |\n| [XVERSE](https://huggingface.co/xverse)                           | 7B/13B/65B                       | xverse           |\n| [Yi/Yi-1.5 (Code)](https://huggingface.co/01-ai)                  | 1.5B/6B/9B/34B                   | yi               |\n| [Yi-VL](https://huggingface.co/01-ai)                             | 6B/34B                           | yi_vl            |\n| [Yuan 2](https://huggingface.co/IEITYuan)                         | 2B/51B/102B                      | yuan             |\n\n> [!NOTE]\n> For the \"base\" models, the `template` argument can be chosen from `default`, `alpaca`, `vicuna` etc. But make sure to use the **corresponding template** for the \"instruct/chat\" models.\n>\n> Remember to use the **SAME** template in training and inference.\n\nPlease refer to [constants.py](src/llamafactory/extras/constants.py) for a full list of models we supported.\n\nYou also can add a custom chat template to [template.py](src/llamafactory/data/template.py).\n\n## Supported Training Approaches\n\n| Approach               |     Full-tuning    |    Freeze-tuning   |       LoRA         |       QLoRA        |\n| ---------------------- | ------------------ | ------------------ | ------------------ | ------------------ |\n| Pre-Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Supervised Fine-Tuning | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Reward Modeling        | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| PPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| DPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| KTO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| ORPO Training          | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| SimPO Training         | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n\n> [!TIP]\n> The implementation details of PPO can be found in [this blog](https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html).\n\n## Provided Datasets\n\n<details><summary>Pre-training datasets</summary>\n\n- [Wiki Demo (en)](data/wiki_demo.txt)\n- [RefinedWeb (en)](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)\n- [RedPajama V2 (en)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)\n- [Wikipedia (en)](https://huggingface.co/datasets/olm/olm-wikipedia-20221220)\n- [Wikipedia (zh)](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)\n- [Pile (en)](https://huggingface.co/datasets/EleutherAI/pile)\n- [SkyPile (zh)](https://huggingface.co/datasets/Skywork/SkyPile-150B)\n- [FineWeb (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb)\n- [FineWeb-Edu (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n- [The Stack (en)](https://huggingface.co/datasets/bigcode/the-stack)\n- [StarCoder (en)](https://huggingface.co/datasets/bigcode/starcoderdata)\n\n</details>\n\n<details><summary>Supervised fine-tuning datasets</summary>\n\n- [Identity (en&zh)](data/identity.json)\n- [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n- [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)\n- [Alpaca GPT4 (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [Glaive Function Calling V2 (en&zh)](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2)\n- [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n- [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n- [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n- [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n- [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n- [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n- [UltraChat (en)](https://github.com/thunlp/UltraChat)\n- [OpenPlatypus (en)](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)\n- [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n- [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n- [OpenOrca (en)](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n- [SlimOrca (en)](https://huggingface.co/datasets/Open-Orca/SlimOrca)\n- [MathInstruct (en)](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n- [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [Wiki QA (en)](https://huggingface.co/datasets/wiki_qa)\n- [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n- [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [deepctrl (en&zh)](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\n- [Advertise Generating (zh)](https://huggingface.co/datasets/HasturOfficial/adgen)\n- [ShareGPT Hyperfiltered (en)](https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k)\n- [ShareGPT4 (en&zh)](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n- [UltraChat 200k (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)\n- [AgentInstruct (en)](https://huggingface.co/datasets/THUDM/AgentInstruct)\n- [LMSYS Chat 1M (en)](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)\n- [Evol Instruct V2 (en)](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)\n- [Cosmopedia (en)](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)\n- [STEM (zh)](https://huggingface.co/datasets/hfl/stem_zh_instruction)\n- [Ruozhiba (zh)](https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo)\n- [Neo-sft (zh)](https://huggingface.co/datasets/m-a-p/neo_sft_phase2)\n- [Magpie-Pro-300K-Filtered (en)](https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered)\n- [Magpie-ultra-v0.1 (en)](https://huggingface.co/datasets/argilla/magpie-ultra-v0.1)\n- [WebInstructSub (en)](https://huggingface.co/datasets/TIGER-Lab/WebInstructSub)\n- [OpenO1-SFT (en&zh)](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)\n- [LLaVA mixed (en&zh)](https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k)\n- [Pokemon-gpt4o-captions (en&zh)](https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions)\n- [Open Assistant (de)](https://huggingface.co/datasets/mayflowergmbh/oasst_de)\n- [Dolly 15k (de)](https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de)\n- [Alpaca GPT4 (de)](https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de)\n- [OpenSchnabeltier (de)](https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de)\n- [Evol Instruct (de)](https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de)\n- [Dolphin (de)](https://huggingface.co/datasets/mayflowergmbh/dolphin_de)\n- [Booksum (de)](https://huggingface.co/datasets/mayflowergmbh/booksum_de)\n- [Airoboros (de)](https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de)\n- [Ultrachat (de)](https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de)\n\n</details>\n\n<details><summary>Preference datasets</summary>\n\n- [DPO mixed (en&zh)](https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k)\n- [UltraFeedback (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n- [RLHF-V (en)](https://huggingface.co/datasets/openbmb/RLHF-V-Dataset)\n- [VLFeedback (en)](https://huggingface.co/datasets/Zhihui/VLFeedback)\n- [Orca DPO Pairs (en)](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n- [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [Orca DPO (de)](https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de)\n- [KTO mixed (en)](https://huggingface.co/datasets/argilla/kto-mix-15k)\n\n</details>\n\nSome datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.\n\n```bash\npip install --upgrade huggingface_hub\nhuggingface-cli login\n```\n\n## Requirement\n\n| Mandatory    | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| python       | 3.8     | 3.11      |\n| torch        | 1.13.1  | 2.4.0     |\n| transformers | 4.41.2  | 4.43.4    |\n| datasets     | 2.16.0  | 2.20.0    |\n| accelerate   | 0.30.1  | 0.32.0    |\n| peft         | 0.11.1  | 0.12.0    |\n| trl          | 0.8.6   | 0.9.6     |\n\n| Optional     | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| CUDA         | 11.6    | 12.2      |\n| deepspeed    | 0.10.0  | 0.14.0    |\n| bitsandbytes | 0.39.0  | 0.43.1    |\n| vllm         | 0.4.3   | 0.5.0     |\n| flash-attn   | 2.3.0   | 2.6.3     |\n\n### Hardware Requirement\n\n\\* *estimated*\n\n| Method            | Bits |   7B  |  13B  |  30B  |   70B  |  110B  |  8x7B |  8x22B |\n| ----------------- | ---- | ----- | ----- | ----- | ------ | ------ | ----- | ------ |\n| Full              | AMP  | 120GB | 240GB | 600GB | 1200GB | 2000GB | 900GB | 2400GB |\n| Full              |  16  |  60GB | 120GB | 300GB |  600GB |  900GB | 400GB | 1200GB |\n| Freeze            |  16  |  20GB |  40GB |  80GB |  200GB |  360GB | 160GB |  400GB |\n| LoRA/GaLore/BAdam |  16  |  16GB |  32GB |  64GB |  160GB |  240GB | 120GB |  320GB |\n| QLoRA             |   8  |  10GB |  20GB |  40GB |   80GB |  140GB |  60GB |  160GB |\n| QLoRA             |   4  |   6GB |  12GB |  24GB |   48GB |   72GB |  30GB |   96GB |\n| QLoRA             |   2  |   4GB |   8GB |  16GB |   24GB |   48GB |  18GB |   48GB |\n\n## Getting Started\n\n### Installation\n\n> [!IMPORTANT]\n> Installation is mandatory.\n\n```bash\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\"\n```\n\nExtra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, awq, aqlm, vllm, galore, badam, adam-mini, qwen, modelscope, openmind, swanlab, quality\n\n> [!TIP]\n> Use `pip install --no-deps -e .` to resolve package conflicts.\n\n<details><summary>For Windows users</summary>\n\nIf you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of `bitsandbytes` library, which supports CUDA 11.1 to 12.2, please select the appropriate [release version](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels) based on your CUDA version.\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl\n```\n\nTo enable FlashAttention-2 on the Windows platform, you need to install the precompiled `flash-attn` library, which supports CUDA 12.1 to 12.2. Please download the corresponding version from [flash-attention](https://github.com/bdashore3/flash-attention/releases) based on your requirements.\n\n</details>\n\n<details><summary>For Ascend NPU users</summary>\n\nTo install LLaMA Factory on Ascend NPU devices, please specify extra dependencies: `pip install -e \".[torch-npu,metrics]\"`. Additionally, you need to install the **[Ascend CANN Toolkit and Kernels](https://www.hiascend.com/developer/download/community/result?module=cann)**. Please follow the [installation tutorial](https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html) or use the following commands:\n\n```bash\n# replace the url according to your CANN version and devices\n# install CANN Toolkit\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-\"$(uname -i)\".run\nbash Ascend-cann-toolkit_8.0.RC1.alpha001_linux-\"$(uname -i)\".run --install\n\n# install CANN Kernels\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run\nbash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install\n\n# set env variables\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n| Requirement  | Minimum | Recommend   |\n| ------------ | ------- | ----------- |\n| CANN         | 8.0.RC1 | 8.0.RC1     |\n| torch        | 2.1.0   | 2.1.0       |\n| torch-npu    | 2.1.0   | 2.1.0.post3 |\n| deepspeed    | 0.13.2  | 0.13.2      |\n\nRemember to use `ASCEND_RT_VISIBLE_DEVICES` instead of `CUDA_VISIBLE_DEVICES` to specify the device to use.\n\nIf you cannot infer model on NPU devices, try setting `do_sample: false` in the configurations.\n\nDownload the pre-built Docker images: [32GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html) | [64GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html)\n\n</details>\n\n### Data Preparation\n\nPlease refer to [data/README.md](data/README.md) for checking the details about the format of dataset files. You can either use datasets on HuggingFace / ModelScope / Modelers hub or load the dataset in local disk.\n\n> [!NOTE]\n> Please update `data/dataset_info.json` to use your custom dataset.\n\n### Quickstart\n\nUse the following 3 commands to run LoRA **fine-tuning**, **inference** and **merging** of the Llama3-8B-Instruct model, respectively.\n\n```bash\nllamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\nllamafactory-cli chat examples/inference/llama3_lora_sft.yaml\nllamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n```\n\nSee [examples/README.md](examples/README.md) for advanced usage (including distributed training).\n\n> [!TIP]\n> Use `llamafactory-cli help` to show help information.\n\n### Fine-Tuning with LLaMA Board GUI (powered by [Gradio](https://github.com/gradio-app/gradio))\n\n```bash\nllamafactory-cli webui\n```\n\n### Build Docker\n\nFor CUDA users:\n\n```bash\ncd docker/docker-cuda/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ncd docker/docker-npu/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ncd docker/docker-rocm/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\n<details><summary>Build without Docker Compose</summary>\n\nFor CUDA users:\n\n```bash\ndocker build -f ./docker/docker-cuda/Dockerfile \\\n    --build-arg INSTALL_BNB=false \\\n    --build-arg INSTALL_VLLM=false \\\n    --build-arg INSTALL_DEEPSPEED=false \\\n    --build-arg INSTALL_FLASHATTN=false \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    -t llamafactory:latest .\n\ndocker run -dit --gpus=all \\\n    -v ./hf_cache:/root/.cache/huggingface \\\n    -v ./ms_cache:/root/.cache/modelscope \\\n    -v ./om_cache:/root/.cache/openmind \\\n    -v ./data:/app/data \\\n    -v ./output:/app/output \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --shm-size 16G \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\n# Choose docker image upon your environment\ndocker build -f ./docker/docker-npu/Dockerfile \\\n    --build-arg INSTALL_DEEPSPEED=false \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    -t llamafactory:latest .\n\n# Change `device` upon your resources\ndocker run -dit \\\n    -v ./hf_cache:/root/.cache/huggingface \\\n    -v ./ms_cache:/root/.cache/modelscope \\\n    -v ./om_cache:/root/.cache/openmind \\\n    -v ./data:/app/data \\\n    -v ./output:/app/output \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    --shm-size 16G \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ndocker build -f ./docker/docker-rocm/Dockerfile \\\n    --build-arg INSTALL_BNB=false \\\n    --build-arg INSTALL_VLLM=false \\\n    --build-arg INSTALL_DEEPSPEED=false \\\n    --build-arg INSTALL_FLASHATTN=false \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    -t llamafactory:latest .\n\ndocker run -dit \\\n    -v ./hf_cache:/root/.cache/huggingface \\\n    -v ./ms_cache:/root/.cache/modelscope \\\n    -v ./om_cache:/root/.cache/openmind \\\n    -v ./data:/app/data \\\n    -v ./output:/app/output \\\n    -v ./saves:/app/saves \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    --shm-size 16G \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\n</details>\n\n<details><summary>Details about volume</summary>\n\n- `hf_cache`: Utilize Hugging Face cache on the host machine. Reassignable if a cache already exists in a different directory.\n- `ms_cache`: Similar to Hugging Face cache but for ModelScope users.\n- `om_cache`: Similar to Hugging Face cache but for Modelers users.\n- `data`: Place datasets on this dir of the host machine so that they can be selected on LLaMA Board GUI.\n- `output`: Set export dir to this location so that the merged result can be accessed directly on the host machine.\n\n</details>\n\n### Deploy with OpenAI-style API and vLLM\n\n```bash\nAPI_PORT=8000 llamafactory-cli api examples/inference/llama3_vllm.yaml\n```\n\n> [!TIP]\n> Visit [this page](https://platform.openai.com/docs/api-reference/chat/create) for API document.\n>\n> Examples: [Image understanding](scripts/api_example/test_image.py) | [Function calling](scripts/api_example/test_toolcall.py)\n\n### Download from ModelScope Hub\n\nIf you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.\n\n```bash\nexport USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the ModelScope Hub as the `model_name_or_path`. You can find a full list of model IDs at [ModelScope Hub](https://modelscope.cn/models), e.g., `LLM-Research/Meta-Llama-3-8B-Instruct`.\n\n### Download from Modelers Hub\n\nYou can also use Modelers Hub to download models and datasets.\n\n```bash\nexport USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the Modelers Hub as the `model_name_or_path`. You can find a full list of model IDs at [Modelers Hub](https://modelers.cn/models), e.g., `TeleAI/TeleChat-7B-pt`.\n\n### Use W&B Logger\n\nTo use [Weights & Biases](https://wandb.ai) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nreport_to: wandb\nrun_name: test_run # optional\n```\n\nSet `WANDB_API_KEY` to [your key](https://wandb.ai/authorize) when launching training tasks to log in with your W&B account.\n\n### Use SwanLab Logger\n\nTo use [SwanLab](https://github.com/SwanHubX/SwanLab) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nuse_swanlab: true\nswanlab_run_name: test_run # optional\n```\n\nWhen launching training tasks, you can log in to SwanLab in three ways:\n\n1. Add `swanlab_api_key=<your_api_key>` to the yaml file, and set it to your [API key](https://swanlab.cn/settings).\n2. Set the environment variable `SWANLAB_API_KEY` to your [API key](https://swanlab.cn/settings).\n3. Use the `swanlab login` command to complete the login.\n\n## Projects using LLaMA Factory\n\nIf you have a project that should be incorporated, please contact via email or create a pull request.\n\n<details><summary>Click to show</summary>\n\n1. Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. [[arxiv]](https://arxiv.org/abs/2308.02223)\n1. Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. [[arxiv]](https://arxiv.org/abs/2308.10092)\n1. Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. [[arxiv]](https://arxiv.org/abs/2308.10526)\n1. Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. [[arxiv]](https://arxiv.org/abs/2311.07816)\n1. Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. [[arxiv]](https://arxiv.org/abs/2312.15710)\n1. Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. [[arxiv]](https://arxiv.org/abs/2401.04319)\n1. Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2401.07286)\n1. Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2402.05904)\n1. Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. [[arxiv]](https://arxiv.org/abs/2402.07625)\n1. Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11176)\n1. Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. [[arxiv]](https://arxiv.org/abs/2402.11187)\n1. Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. [[arxiv]](https://arxiv.org/abs/2402.11746)\n1. Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11801)\n1. Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2402.11809)\n1. Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11819)\n1. Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. [[arxiv]](https://arxiv.org/abs/2402.12204)\n1. Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.14714)\n1. Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. [[arxiv]](https://arxiv.org/abs/2402.15043)\n1. Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. [[arxiv]](https://arxiv.org/abs/2403.02333)\n1. Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. [[arxiv]](https://arxiv.org/abs/2403.03419)\n1. Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2403.08228)\n1. Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2403.09073)\n1. Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. [[arxiv]](https://arxiv.org/abs/2403.14541)\n1. Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2403.15246)\n1. Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. [[arxiv]](https://arxiv.org/abs/2403.16008)\n1. Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. [[arxiv]](https://arxiv.org/abs/2403.16443)\n1. Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2404.00604)\n1. Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.02827)\n1. Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2404.04167)\n1. Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. [[arxiv]](https://arxiv.org/abs/2404.04316)\n1. Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.07084)\n1. Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.09836)\n1. Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.11581)\n1. Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. [[arxiv]](https://arxiv.org/abs/2404.14215)\n1. Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2404.16621)\n1. Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2404.17140)\n1. Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. [[arxiv]](https://arxiv.org/abs/2404.18585)\n1. Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. [[arxiv]](https://arxiv.org/abs/2405.04760)\n1. Dammu et al. \"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. [[arxiv]](https://arxiv.org/abs/2405.05378)\n1. Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. [[arxiv]](https://arxiv.org/abs/2405.09055)\n1. Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. [[arxiv]](https://arxiv.org/abs/2405.12739)\n1. Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2405.13816)\n1. Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2405.20215)\n1. Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. [[paper]](https://aclanthology.org/2024.lt4hala-1.30)\n1. Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2406.00380)\n1. Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. [[arxiv]](https://arxiv.org/abs/2406.02106)\n1. Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. [[arxiv]](https://arxiv.org/abs/2406.03136)\n1. Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2406.04496)\n1. Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. [[arxiv]](https://arxiv.org/abs/2406.05688)\n1. Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. [[arxiv]](https://arxiv.org/abs/2406.05955)\n1. Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. [[arxiv]](https://arxiv.org/abs/2406.06973)\n1. Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. [[arxiv]](https://arxiv.org/abs/2406.07115)\n1. Zhu et al. Are Large Language Models Good Statisticians?. 2024. [[arxiv]](https://arxiv.org/abs/2406.07815)\n1. Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2406.10099)\n1. Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. [[arxiv]](https://arxiv.org/abs/2406.10173)\n1. He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. [[arxiv]](https://arxiv.org/abs/2406.12074)\n1. Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. [[arxiv]](https://arxiv.org/abs/2406.14408)\n1. Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. [[arxiv]](https://arxiv.org/abs/2406.14546)\n1. Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. [[arxiv]](https://arxiv.org/abs/2406.15695)\n1. Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. [[arxiv]](https://arxiv.org/abs/2406.17233)\n1. Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. [[arxiv]](https://arxiv.org/abs/2406.18069)\n1. Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. [[paper]](https://aclanthology.org/2024.americasnlp-1.25)\n1. Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. [[arxiv]](https://arxiv.org/abs/2406.19949)\n1. Yang et al. Financial Knowledge Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2407.00365)\n1. Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. [[arxiv]](https://arxiv.org/abs/2407.01470)\n1. Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. [[arxiv]](https://arxiv.org/abs/2407.06129)\n1. Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. [[arxiv]](https://arxiv.org/abs/2407.08044)\n1. Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. [[arxiv]](https://arxiv.org/abs/2407.09756)\n1. Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. [[paper]](https://scholarcommons.scu.edu/cseng_senior/272/)\n1. Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. [[arxiv]](https://arxiv.org/abs/2407.13561)\n1. Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. [[arxiv]](https://arxiv.org/abs/2407.16637)\n1. Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. [[arxiv]](https://arxiv.org/abs/2407.17535)\n1. Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2407.19705)\n1. Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2408.00137)\n1. Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. [[paper]](https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf)\n1. Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11)\n1. Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23)\n1. Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2408.04693)\n1. Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2408.04168)\n1. Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. [[paper]](https://aclanthology.org/2024.finnlp-2.1/)\n1. Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. [[arxiv]](https://arxiv.org/abs/2408.08072)\n1. Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. [[paper]](https://dl.acm.org/doi/10.1145/3627673.3679611)\n1. **[StarWhisper](https://github.com/Yu-Yang-Li/StarWhisper)**: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.\n1. **[DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)**: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.\n1. **[Sunsimiao](https://github.com/X-D-Lab/Sunsimiao)**: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.\n1. **[CareGPT](https://github.com/WangRongsheng/CareGPT)**: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.\n1. **[MachineMindset](https://github.com/PKU-YuanGroup/Machine-Mindset/)**: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.\n1. **[Luminia-13B-v3](https://huggingface.co/Nekochu/Luminia-13B-v3)**: A large language model specialized in generate metadata for stable diffusion. [[demo]](https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt)\n1. **[Chinese-LLaVA-Med](https://github.com/BUAADreamer/Chinese-LLaVA-Med)**: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.\n1. **[AutoRE](https://github.com/THUDM/AutoRE)**: A document-level relation extraction system based on large language models.\n1. **[NVIDIA RTX AI Toolkit](https://github.com/NVIDIA/RTX-AI-Toolkit)**: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.\n1. **[LazyLLM](https://github.com/LazyAGI/LazyLLM)**: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.\n1. **[RAG-Retrieval](https://github.com/NLPJCL/RAG-Retrieval)**: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. [[blog]](https://zhuanlan.zhihu.com/p/987727357)\n1. **[360-LLaMA-Factory](https://github.com/Qihoo360/360-LLaMA-Factory)**: A modified library that supports long sequence SFT & DPO using ring attention.\n\n</details>\n\n## License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n\nPlease follow the model licenses to use the corresponding model weights: [Baichuan 2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf) / [BLOOM](https://huggingface.co/spaces/bigscience/license) / [ChatGLM3](https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE) / [Command R](https://cohere.com/c4ai-cc-by-nc-license) / [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL) / [Falcon](https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt) / [Gemma](https://ai.google.dev/gemma/terms) / [GLM-4](https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE) / [GPT-2](https://github.com/openai/gpt-2/blob/master/LICENSE) / [Granite](LICENSE) / [Index](https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE) / [InternLM2](https://github.com/InternLM/InternLM#license) / [Llama](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) / [Llama 2 (LLaVA-1.5)](https://ai.meta.com/llama/license/) / [Llama 3](https://llama.meta.com/llama3/license/) / [MiniCPM](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md) / [Mistral/Mixtral/Pixtral](LICENSE) / [OLMo](LICENSE) / [Phi-1.5/Phi-2](https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx) / [Phi-3/Phi-4](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE) / [Qwen](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) / [Skywork](https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf) / [StarCoder 2](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) / [TeleChat2](https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) / [XVERSE](https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf) / [Yi](https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE) / [Yi-1.5](LICENSE) / [Yuan 2](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan)\n\n## Citation\n\nIf this work is helpful, please kindly cite as:\n\n```bibtex\n@inproceedings{zheng2024llamafactory,\n  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},\n  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},\n  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},\n  address={Bangkok, Thailand},\n  publisher={Association for Computational Linguistics},\n  year={2024},\n  url={http://arxiv.org/abs/2403.13372}\n}\n```\n\n## Acknowledgement\n\nThis repo benefits from [PEFT](https://github.com/huggingface/peft), [TRL](https://github.com/huggingface/trl), [QLoRA](https://github.com/artidoro/qlora) and [FastChat](https://github.com/lm-sys/FastChat). Thanks for their wonderful works.\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&type=Date)\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 54.8974609375,
          "content": "![# LLaMA Factory](assets/logo.png)\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)\n[![GitHub Code License](https://img.shields.io/github/license/hiyouga/LLaMA-Factory)](LICENSE)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)\n[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)\n[![Citation](https://img.shields.io/badge/citation-210-green)](https://scholar.google.com/scholar?cites=12620864006390196564)\n[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/LLaMA-Factory/pulls)\n[![Discord](https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&style=flat)](https://discord.gg/rKfvV9r9FK)\n[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1d5KQtbemerlSDSxZIfAaWXhKr30QypiK?usp=sharing)\n[![Open in DSW](https://gallery.pai-ml.com/assets/open-in-dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)\n[![Spaces](https://img.shields.io/badge/ðŸ¤—-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)\n[![Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)\n[![SageMaker](https://img.shields.io/badge/SageMaker-Open%20in%20AWS-blue)](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/)\n[![GitCode](https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg)](https://gitcode.com/zhengyaowei/LLaMA-Factory)\n\n[![GitHub Tread](https://trendshift.io/api/badge/repositories/4535)](https://trendshift.io/repositories/4535)\n\nðŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„[å¾®ä¿¡ç¾¤](assets/wechat.jpg)æˆ– [NPU ç”¨æˆ·ç¾¤](assets/wechat_npu.jpg)ã€‚\n\n\\[ [English](README.md) | ä¸­æ–‡ \\]\n\n**å¾®è°ƒå¤§æ¨¡åž‹å¯ä»¥åƒè¿™æ ·è½»æ¾â€¦**\n\nhttps://github.com/user-attachments/assets/e6ce34b0-52d5-4f3e-a830-592106c4c272\n\né€‰æ‹©ä½ çš„æ‰“å¼€æ–¹å¼ï¼š\n\n- **å…¥é—¨æ•™ç¨‹**ï¼šhttps://zhuanlan.zhihu.com/p/695287607\n- **æ¡†æž¶æ–‡æ¡£**ï¼šhttps://llamafactory.readthedocs.io/zh-cn/latest/\n- **Colab**ï¼šhttps://colab.research.google.com/drive/1d5KQtbemerlSDSxZIfAaWXhKr30QypiK?usp=sharing\n- **æœ¬åœ°æœºå™¨**ï¼šè¯·è§[å¦‚ä½•ä½¿ç”¨](#å¦‚ä½•ä½¿ç”¨)\n- **PAI-DSW**ï¼š[Llama3 æ¡ˆä¾‹](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) | [Qwen2-VL æ¡ˆä¾‹](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl)\n- **Amazon SageMaker**ï¼š[åšå®¢](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/)\n\n> [!NOTE]\n> é™¤ä¸Šè¿°é“¾æŽ¥ä»¥å¤–çš„å…¶ä»–ç½‘ç«™å‡ä¸ºæœªç»è®¸å¯çš„ç¬¬ä¸‰æ–¹ç½‘ç«™ï¼Œè¯·å°å¿ƒç”„åˆ«ã€‚\n\n## ç›®å½•\n\n- [é¡¹ç›®ç‰¹è‰²](#é¡¹ç›®ç‰¹è‰²)\n- [æ€§èƒ½æŒ‡æ ‡](#æ€§èƒ½æŒ‡æ ‡)\n- [æ›´æ–°æ—¥å¿—](#æ›´æ–°æ—¥å¿—)\n- [æ¨¡åž‹](#æ¨¡åž‹)\n- [è®­ç»ƒæ–¹æ³•](#è®­ç»ƒæ–¹æ³•)\n- [æ•°æ®é›†](#æ•°æ®é›†)\n- [è½¯ç¡¬ä»¶ä¾èµ–](#è½¯ç¡¬ä»¶ä¾èµ–)\n- [å¦‚ä½•ä½¿ç”¨](#å¦‚ä½•ä½¿ç”¨)\n  - [å®‰è£… LLaMA Factory](#å®‰è£…-llama-factory)\n  - [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)\n  - [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)\n  - [LLaMA Board å¯è§†åŒ–å¾®è°ƒ](#llama-board-å¯è§†åŒ–å¾®è°ƒç”±-gradio-é©±åŠ¨)\n  - [æž„å»º Docker](#æž„å»º-docker)\n  - [åˆ©ç”¨ vLLM éƒ¨ç½² OpenAI API](#åˆ©ç”¨-vllm-éƒ¨ç½²-openai-api)\n  - [ä»Žé­”æ­ç¤¾åŒºä¸‹è½½](#ä»Žé­”æ­ç¤¾åŒºä¸‹è½½)\n  - [ä»Žé­”ä¹ç¤¾åŒºä¸‹è½½](#ä»Žé­”ä¹ç¤¾åŒºä¸‹è½½)\n  - [ä½¿ç”¨ W&B é¢æ¿](#ä½¿ç”¨-wb-é¢æ¿)\n  - [ä½¿ç”¨ SwanLab é¢æ¿](#ä½¿ç”¨-swanlab-é¢æ¿)\n- [ä½¿ç”¨äº† LLaMA Factory çš„é¡¹ç›®](#ä½¿ç”¨äº†-llama-factory-çš„é¡¹ç›®)\n- [åè®®](#åè®®)\n- [å¼•ç”¨](#å¼•ç”¨)\n- [è‡´è°¢](#è‡´è°¢)\n\n## é¡¹ç›®ç‰¹è‰²\n\n- **å¤šç§æ¨¡åž‹**ï¼šLLaMAã€LLaVAã€Mistralã€Mixtral-MoEã€Qwenã€Qwen2-VLã€Yiã€Gemmaã€Baichuanã€ChatGLMã€Phi ç­‰ç­‰ã€‚\n- **é›†æˆæ–¹æ³•**ï¼šï¼ˆå¢žé‡ï¼‰é¢„è®­ç»ƒã€ï¼ˆå¤šæ¨¡æ€ï¼‰æŒ‡ä»¤ç›‘ç£å¾®è°ƒã€å¥–åŠ±æ¨¡åž‹è®­ç»ƒã€PPO è®­ç»ƒã€DPO è®­ç»ƒã€KTO è®­ç»ƒã€ORPO è®­ç»ƒç­‰ç­‰ã€‚\n- **å¤šç§ç²¾åº¦**ï¼š16 æ¯”ç‰¹å…¨å‚æ•°å¾®è°ƒã€å†»ç»“å¾®è°ƒã€LoRA å¾®è°ƒå’ŒåŸºäºŽ AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ çš„ 2/3/4/5/6/8 æ¯”ç‰¹ QLoRA å¾®è°ƒã€‚\n- **å…ˆè¿›ç®—æ³•**ï¼š[GaLore](https://github.com/jiaweizzhao/GaLore)ã€[BAdam](https://github.com/Ledzy/BAdam)ã€[Adam-mini](https://github.com/zyushun/Adam-mini)ã€DoRAã€LongLoRAã€LLaMA Proã€Mixture-of-Depthsã€LoRA+ã€LoftQã€PiSSA å’Œ Agent å¾®è°ƒã€‚\n- **å®žç”¨æŠ€å·§**ï¼š[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)ã€[Unsloth](https://github.com/unslothai/unsloth)ã€[Liger Kernel](https://github.com/linkedin/Liger-Kernel)ã€RoPE scalingã€NEFTune å’Œ rsLoRAã€‚\n- **å®žéªŒç›‘æŽ§**ï¼šLlamaBoardã€TensorBoardã€Wandbã€MLflowã€SwanLab ç­‰ç­‰ã€‚\n- **æžé€ŸæŽ¨ç†**ï¼šåŸºäºŽ vLLM çš„ OpenAI é£Žæ ¼ APIã€æµè§ˆå™¨ç•Œé¢å’Œå‘½ä»¤è¡ŒæŽ¥å£ã€‚\n\n## æ€§èƒ½æŒ‡æ ‡\n\nä¸Ž ChatGLM å®˜æ–¹çš„ [P-Tuning](https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning) å¾®è°ƒç›¸æ¯”ï¼ŒLLaMA Factory çš„ LoRA å¾®è°ƒæä¾›äº† **3.7 å€**çš„åŠ é€Ÿæ¯”ï¼ŒåŒæ—¶åœ¨å¹¿å‘Šæ–‡æ¡ˆç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—äº†æ›´é«˜çš„ Rouge åˆ†æ•°ã€‚ç»“åˆ 4 æ¯”ç‰¹é‡åŒ–æŠ€æœ¯ï¼ŒLLaMA Factory çš„ QLoRA å¾®è°ƒè¿›ä¸€æ­¥é™ä½Žäº† GPU æ˜¾å­˜æ¶ˆè€—ã€‚\n\n![benchmark](assets/benchmark.svg)\n\n<details><summary>å˜é‡å®šä¹‰</summary>\n\n- **Training Speed**: è®­ç»ƒé˜¶æ®µæ¯ç§’å¤„ç†çš„æ ·æœ¬æ•°é‡ã€‚ï¼ˆæ‰¹å¤„ç†å¤§å°=4ï¼Œæˆªæ–­é•¿åº¦=1024ï¼‰\n- **Rouge Score**: [å¹¿å‘Šæ–‡æ¡ˆç”Ÿæˆ](https://aclanthology.org/D19-1321.pdf)ä»»åŠ¡éªŒè¯é›†ä¸Šçš„ Rouge-2 åˆ†æ•°ã€‚ï¼ˆæ‰¹å¤„ç†å¤§å°=4ï¼Œæˆªæ–­é•¿åº¦=1024ï¼‰\n- **GPU Memory**: 4 æ¯”ç‰¹é‡åŒ–è®­ç»ƒçš„ GPU æ˜¾å­˜å³°å€¼ã€‚ï¼ˆæ‰¹å¤„ç†å¤§å°=1ï¼Œæˆªæ–­é•¿åº¦=1024ï¼‰\n- æˆ‘ä»¬åœ¨ ChatGLM çš„ P-Tuning ä¸­é‡‡ç”¨ `pre_seq_len=128`ï¼Œåœ¨ LLaMA Factory çš„ LoRA å¾®è°ƒä¸­é‡‡ç”¨ `lora_rank=32`ã€‚\n\n</details>\n\n## æ›´æ–°æ—¥å¿—\n\n[25/01/10] æˆ‘ä»¬æ”¯æŒäº† **[Phi-4](https://huggingface.co/microsoft/phi-4)** æ¨¡åž‹çš„å¾®è°ƒã€‚\n\n[24/12/21] æˆ‘ä»¬æ”¯æŒäº†ä½¿ç”¨ **[SwanLab](https://github.com/SwanHubX/SwanLab)** è·Ÿè¸ªä¸Žå¯è§†åŒ–å®žéªŒã€‚è¯¦ç»†ç”¨æ³•è¯·å‚è€ƒ [æ­¤éƒ¨åˆ†](#ä½¿ç”¨-swanlab-é¢æ¿)ã€‚\n\n[24/11/27] æˆ‘ä»¬æ”¯æŒäº† **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** æ¨¡åž‹çš„å¾®è°ƒå’Œ **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** æ•°æ®é›†ã€‚\n\n<details><summary>å±•å¼€æ—¥å¿—</summary>\n\n[24/10/09] æˆ‘ä»¬æ”¯æŒäº†ä»Ž **[é­”ä¹ç¤¾åŒº](https://modelers.cn/models)** ä¸‹è½½é¢„è®­ç»ƒæ¨¡åž‹å’Œæ•°æ®é›†ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [æ­¤æ•™ç¨‹](#ä»Žé­”ä¹ç¤¾åŒºä¸‹è½½)ã€‚\n\n[24/09/19] æˆ‘ä»¬æ”¯æŒäº† **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** æ¨¡åž‹çš„å¾®è°ƒã€‚\n\n[24/08/30] æˆ‘ä»¬æ”¯æŒäº† **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** æ¨¡åž‹çš„å¾®è°ƒã€‚æ„Ÿè°¢ [@simonJJJ](https://github.com/simonJJJ) çš„ PRã€‚\n\n[24/08/27] æˆ‘ä»¬æ”¯æŒäº† **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**ã€‚è¯·ä½¿ç”¨ `enable_liger_kernel: true` æ¥åŠ é€Ÿè®­ç»ƒã€‚\n\n[24/08/09] æˆ‘ä»¬æ”¯æŒäº† **[Adam-mini](https://github.com/zyushun/Adam-mini)** ä¼˜åŒ–å™¨ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚æ„Ÿè°¢ [@relic-yuexi](https://github.com/relic-yuexi) çš„ PRã€‚\n\n[24/07/04] æˆ‘ä»¬æ”¯æŒäº†[æ— æ±¡æŸ“æ‰“åŒ…è®­ç»ƒ](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)ã€‚è¯·ä½¿ç”¨ `neat_packing: true` å‚æ•°ã€‚æ„Ÿè°¢ [@chuan298](https://github.com/chuan298) çš„ PRã€‚\n\n[24/06/16] æˆ‘ä»¬æ”¯æŒäº† **[PiSSA](https://arxiv.org/abs/2404.02948)** ç®—æ³•ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/06/07] æˆ‘ä»¬æ”¯æŒäº† **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** å’Œ **[GLM-4](https://github.com/THUDM/GLM-4)** æ¨¡åž‹çš„å¾®è°ƒã€‚\n\n[24/05/26] æˆ‘ä»¬æ”¯æŒäº† **[SimPO](https://arxiv.org/abs/2405.14734)** åå¥½å¯¹é½ç®—æ³•ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/05/20] æˆ‘ä»¬æ”¯æŒäº† **PaliGemma** ç³»åˆ—æ¨¡åž‹çš„å¾®è°ƒã€‚æ³¨æ„ PaliGemma æ˜¯é¢„è®­ç»ƒæ¨¡åž‹ï¼Œä½ éœ€è¦ä½¿ç”¨ `paligemma` æ¨¡æ¿è¿›è¡Œå¾®è°ƒä½¿å…¶èŽ·å¾—å¯¹è¯èƒ½åŠ›ã€‚\n\n[24/05/18] æˆ‘ä»¬æ”¯æŒäº† **[KTO](https://arxiv.org/abs/2402.01306)** åå¥½å¯¹é½ç®—æ³•ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/05/14] æˆ‘ä»¬æ”¯æŒäº†æ˜‡è…¾ NPU è®¾å¤‡çš„è®­ç»ƒå’ŒæŽ¨ç†ã€‚è¯¦æƒ…è¯·æŸ¥é˜…[å®‰è£…](#å®‰è£…-llama-factory)éƒ¨åˆ†ã€‚\n\n[24/04/26] æˆ‘ä»¬æ”¯æŒäº†å¤šæ¨¡æ€æ¨¡åž‹ **LLaVA-1.5** çš„å¾®è°ƒã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/04/22] æˆ‘ä»¬æä¾›äº†åœ¨å…è´¹ T4 GPU ä¸Šå¾®è°ƒ Llama-3 æ¨¡åž‹çš„ **[Colab ç¬”è®°æœ¬](https://colab.research.google.com/drive/1d5KQtbemerlSDSxZIfAaWXhKr30QypiK?usp=sharing)**ã€‚Hugging Face ç¤¾åŒºå…¬å¼€äº†ä¸¤ä¸ªåˆ©ç”¨ LLaMA Factory å¾®è°ƒçš„ Llama-3 æ¨¡åž‹ï¼Œè¯¦æƒ…è¯·è§ [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) å’Œ [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese)ã€‚\n\n[24/04/21] æˆ‘ä»¬åŸºäºŽ [AstraMindAI çš„ä»“åº“](https://github.com/astramind-ai/Mixture-of-depths)æ”¯æŒäº† **[æ··åˆæ·±åº¦è®­ç»ƒ](https://arxiv.org/abs/2404.02258)**ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/04/16] æˆ‘ä»¬æ”¯æŒäº† **[BAdam](https://arxiv.org/abs/2404.02827)** ä¼˜åŒ–å™¨ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/04/16] æˆ‘ä»¬æ”¯æŒäº† **[unsloth](https://github.com/unslothai/unsloth)** çš„é•¿åºåˆ—è®­ç»ƒï¼ˆ24GB å¯è®­ç»ƒ Llama-2-7B-56kï¼‰ã€‚è¯¥æ–¹æ³•ç›¸æ¯” FlashAttention-2 æä¾›äº† **117%** çš„è®­ç»ƒé€Ÿåº¦å’Œ **50%** çš„æ˜¾å­˜èŠ‚çº¦ã€‚æ›´å¤šæ•°æ®è¯·è§[æ­¤é¡µé¢](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison)ã€‚\n\n[24/03/31] æˆ‘ä»¬æ”¯æŒäº† **[ORPO](https://arxiv.org/abs/2403.07691)**ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/03/21] æˆ‘ä»¬çš„è®ºæ–‡ \"[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)\" å¯åœ¨ arXiv ä¸ŠæŸ¥çœ‹ï¼\n\n[24/03/20] æˆ‘ä»¬æ”¯æŒäº†èƒ½åœ¨ 2x24GB GPU ä¸Šå¾®è°ƒ 70B æ¨¡åž‹çš„ **FSDP+QLoRA**ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/03/13] æˆ‘ä»¬æ”¯æŒäº† **[LoRA+](https://arxiv.org/abs/2402.12354)**ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/03/07] æˆ‘ä»¬æ”¯æŒäº† **[GaLore](https://arxiv.org/abs/2403.03507)** ä¼˜åŒ–å™¨ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/03/07] æˆ‘ä»¬é›†æˆäº† **[vLLM](https://github.com/vllm-project/vllm)** ä»¥å®žçŽ°æžé€Ÿå¹¶å‘æŽ¨ç†ã€‚è¯·ä½¿ç”¨ `infer_backend: vllm` æ¥èŽ·å¾— **270%** çš„æŽ¨ç†é€Ÿåº¦ã€‚\n\n[24/02/28] æˆ‘ä»¬æ”¯æŒäº† **[DoRA](https://arxiv.org/abs/2402.09353)** å¾®è°ƒã€‚è¯·ä½¿ç”¨ `use_dora: true` å‚æ•°è¿›è¡Œ DoRA å¾®è°ƒã€‚\n\n[24/02/15] æˆ‘ä»¬æ”¯æŒäº† [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro) æå‡ºçš„**å—æ‰©å±•**æ–¹æ³•ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[24/02/05] Qwen1.5ï¼ˆQwen2 æµ‹è¯•ç‰ˆï¼‰ç³»åˆ—æ¨¡åž‹å·²åœ¨ LLaMA-Factory ä¸­å®žçŽ°å¾®è°ƒæ”¯æŒã€‚è¯¦æƒ…è¯·æŸ¥é˜…è¯¥[åšå®¢é¡µé¢](https://qwenlm.github.io/zh/blog/qwen1.5/)ã€‚\n\n[24/01/18] æˆ‘ä»¬é’ˆå¯¹ç»å¤§å¤šæ•°æ¨¡åž‹å®žçŽ°äº† **Agent å¾®è°ƒ**ï¼Œå¾®è°ƒæ—¶æŒ‡å®š `dataset: glaive_toolcall_zh` å³å¯ä½¿æ¨¡åž‹èŽ·å¾—å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚\n\n[23/12/23] æˆ‘ä»¬é’ˆå¯¹ LLaMA, Mistral å’Œ Yi æ¨¡åž‹æ”¯æŒäº† **[unsloth](https://github.com/unslothai/unsloth)** çš„ LoRA è®­ç»ƒåŠ é€Ÿã€‚è¯·ä½¿ç”¨ `use_unsloth: true` å‚æ•°å¯ç”¨ unsloth ä¼˜åŒ–ã€‚è¯¥æ–¹æ³•å¯æä¾› **170%** çš„è®­ç»ƒé€Ÿåº¦ï¼Œè¯¦æƒ…è¯·æŸ¥é˜…[æ­¤é¡µé¢](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison)ã€‚\n\n[23/12/12] æˆ‘ä»¬æ”¯æŒäº†å¾®è°ƒæœ€æ–°çš„æ··åˆä¸“å®¶æ¨¡åž‹ **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)**ã€‚ç¡¬ä»¶éœ€æ±‚è¯·æŸ¥é˜…[æ­¤å¤„](#ç¡¬ä»¶ä¾èµ–)ã€‚\n\n[23/12/01] æˆ‘ä»¬æ”¯æŒäº†ä»Ž **[é­”æ­ç¤¾åŒº](https://modelscope.cn/models)** ä¸‹è½½é¢„è®­ç»ƒæ¨¡åž‹å’Œæ•°æ®é›†ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [æ­¤æ•™ç¨‹](#ä»Žé­”æ­ç¤¾åŒºä¸‹è½½)ã€‚\n\n[23/10/21] æˆ‘ä»¬æ”¯æŒäº† **[NEFTune](https://arxiv.org/abs/2310.05914)** è®­ç»ƒæŠ€å·§ã€‚è¯·ä½¿ç”¨ `neftune_noise_alpha: 5` å‚æ•°å¯ç”¨ NEFTuneã€‚\n\n[23/09/27] æˆ‘ä»¬é’ˆå¯¹ LLaMA æ¨¡åž‹æ”¯æŒäº† [LongLoRA](https://github.com/dvlab-research/LongLoRA) æå‡ºçš„ **$S^2$-Attn**ã€‚è¯·ä½¿ç”¨ `shift_attn: true` å‚æ•°ä»¥å¯ç”¨è¯¥åŠŸèƒ½ã€‚\n\n[23/09/23] æˆ‘ä»¬åœ¨é¡¹ç›®ä¸­é›†æˆäº† MMLUã€C-Eval å’Œ CMMLU è¯„ä¼°é›†ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[23/09/10] æˆ‘ä»¬æ”¯æŒäº† **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**ã€‚å¦‚æžœæ‚¨ä½¿ç”¨çš„æ˜¯ RTX4090ã€A100 æˆ– H100 GPUï¼Œè¯·ä½¿ç”¨ `flash_attn: fa2` å‚æ•°ä»¥å¯ç”¨ FlashAttention-2ã€‚\n\n[23/08/12] æˆ‘ä»¬æ”¯æŒäº† **RoPE æ’å€¼**æ¥æ‰©å±• LLaMA æ¨¡åž‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚è¯·ä½¿ç”¨ `rope_scaling: linear` å‚æ•°è®­ç»ƒæ¨¡åž‹æˆ–ä½¿ç”¨ `rope_scaling: dynamic` å‚æ•°è¯„ä¼°æ¨¡åž‹ã€‚\n\n[23/08/11] æˆ‘ä»¬æ”¯æŒäº†æŒ‡ä»¤æ¨¡åž‹çš„ **[DPO è®­ç»ƒ](https://arxiv.org/abs/2305.18290)**ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n[23/07/31] æˆ‘ä»¬æ”¯æŒäº†**æ•°æ®æµå¼åŠ è½½**ã€‚è¯·ä½¿ç”¨ `streaming: true` å’Œ `max_steps: 10000` å‚æ•°æ¥æµå¼åŠ è½½æ•°æ®é›†ã€‚\n\n[23/07/29] æˆ‘ä»¬åœ¨ Hugging Face å‘å¸ƒäº†ä¸¤ä¸ª 13B æŒ‡ä»¤å¾®è°ƒæ¨¡åž‹ã€‚è¯¦ç»†å†…å®¹è¯·æŸ¥é˜…æˆ‘ä»¬çš„ Hugging Face é¡¹ç›®ï¼ˆ[LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)ï¼‰ã€‚\n\n[23/07/18] æˆ‘ä»¬å¼€å‘äº†æ”¯æŒè®­ç»ƒå’Œæµ‹è¯•çš„**æµè§ˆå™¨ä¸€ä½“åŒ–ç•Œé¢**ã€‚è¯·ä½¿ç”¨ `train_web.py` åœ¨æ‚¨çš„æµè§ˆå™¨ä¸­å¾®è°ƒæ¨¡åž‹ã€‚æ„Ÿè°¢ [@KanadeSiina](https://github.com/KanadeSiina) å’Œ [@codemayq](https://github.com/codemayq) åœ¨è¯¥åŠŸèƒ½å¼€å‘ä¸­ä»˜å‡ºçš„åŠªåŠ›ã€‚\n\n[23/07/09] æˆ‘ä»¬å¼€æºäº† **[FastEdit](https://github.com/hiyouga/FastEdit)** âš¡ðŸ©¹ï¼Œä¸€ä¸ªç®€å•æ˜“ç”¨çš„ã€èƒ½è¿…é€Ÿç¼–è¾‘å¤§æ¨¡åž‹äº‹å®žè®°å¿†çš„å·¥å…·åŒ…ã€‚å¦‚æžœæ‚¨æ„Ÿå…´è¶£è¯·å…³æ³¨æˆ‘ä»¬çš„ [FastEdit](https://github.com/hiyouga/FastEdit) é¡¹ç›®ã€‚\n\n[23/06/29] æˆ‘ä»¬æä¾›äº†ä¸€ä¸ª**å¯å¤çŽ°çš„**æŒ‡ä»¤æ¨¡åž‹å¾®è°ƒç¤ºä¾‹ï¼Œè¯¦ç»†å†…å®¹è¯·æŸ¥é˜… [Baichuan-7B-sft](https://huggingface.co/hiyouga/Baichuan-7B-sft)ã€‚\n\n[23/06/22] æˆ‘ä»¬å¯¹é½äº†[ç¤ºä¾‹ API](src/api_demo.py) ä¸Ž [OpenAI API](https://platform.openai.com/docs/api-reference/chat) çš„æ ¼å¼ï¼Œæ‚¨å¯ä»¥å°†å¾®è°ƒæ¨¡åž‹æŽ¥å…¥**ä»»æ„åŸºäºŽ ChatGPT çš„åº”ç”¨**ä¸­ã€‚\n\n[23/06/03] æˆ‘ä»¬å®žçŽ°äº† 4 æ¯”ç‰¹çš„ LoRA è®­ç»ƒï¼ˆä¹Ÿç§° **[QLoRA](https://github.com/artidoro/qlora)**ï¼‰ã€‚è¯¦ç»†ç”¨æ³•è¯·å‚ç…§ [examples](examples/README_zh.md)ã€‚\n\n</details>\n\n## æ¨¡åž‹\n\n| æ¨¡åž‹å                                                            | æ¨¡åž‹å¤§å°                          | Template         |\n| ----------------------------------------------------------------- | -------------------------------- | ---------------- |\n| [Baichuan 2](https://huggingface.co/baichuan-inc)                 | 7B/13B                           | baichuan2        |\n| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)                 | 560M/1.1B/1.7B/3B/7.1B/176B      | -                |\n| [ChatGLM3](https://huggingface.co/THUDM)                          | 6B                               | chatglm3         |\n| [Command R](https://huggingface.co/CohereForAI)                   | 35B/104B                         | cohere           |\n| [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)         | 7B/16B/67B/236B                  | deepseek         |\n| [DeepSeek 2.5/3](https://huggingface.co/deepseek-ai)              | 236B/685B                        | deepseek3        |\n| [Falcon](https://huggingface.co/tiiuae)                           | 7B/11B/40B/180B                  | falcon           |\n| [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)          | 2B/7B/9B/27B                     | gemma            |\n| [GLM-4](https://huggingface.co/THUDM)                             | 9B                               | glm4             |\n| [GPT-2](https://huggingface.co/openai-community)                  | 0.1B/0.4B/0.8B/1.5B              | -                |\n| [Granite 3.0-3.1](https://huggingface.co/ibm-granite)             | 1B/2B/3B/8B                      | granite3         |\n| [Index](https://huggingface.co/IndexTeam)                         | 1.9B                             | index            |\n| [InternLM2/InternLM2.5](https://huggingface.co/internlm)          | 7B/20B                           | intern2          |\n| [Llama](https://github.com/facebookresearch/llama)                | 7B/13B/33B/65B                   | -                |\n| [Llama 2](https://huggingface.co/meta-llama)                      | 7B/13B/70B                       | llama2           |\n| [Llama 3-3.3](https://huggingface.co/meta-llama)                  | 1B/3B/8B/70B                     | llama3           |\n| [Llama 3.2 Vision](https://huggingface.co/meta-llama)             | 11B/90B                          | mllama           |\n| [LLaVA-1.5](https://huggingface.co/llava-hf)                      | 7B/13B                           | llava            |\n| [LLaVA-NeXT](https://huggingface.co/llava-hf)                     | 7B/8B/13B/34B/72B/110B           | llava_next       |\n| [LLaVA-NeXT-Video](https://huggingface.co/llava-hf)               | 7B/34B                           | llava_next_video |\n| [MiniCPM](https://huggingface.co/openbmb)                         | 1B/2B/4B                         | cpm/cpm3         |\n| [Mistral/Mixtral](https://huggingface.co/mistralai)               | 7B/8x7B/8x22B                    | mistral          |\n| [OLMo](https://huggingface.co/allenai)                            | 1B/7B                            | -                |\n| [PaliGemma/PaliGemma2](https://huggingface.co/google)             | 3B/10B/28B                       | paligemma        |\n| [Phi-1.5/Phi-2](https://huggingface.co/microsoft)                 | 1.3B/2.7B                        | -                |\n| [Phi-3/Phi-3.5](https://huggingface.co/microsoft)                 | 4B/14B                           | phi              |\n| [Phi-3-small](https://huggingface.co/microsoft)                   | 7B                               | phi_small        |\n| [Phi-4](https://huggingface.co/microsoft)                         | 14B                              | phi4             |\n| [Pixtral](https://huggingface.co/mistralai)                       | 12B                              | pixtral          |\n| [Qwen/QwQ (1-2.5) (Code/Math/MoE)](https://huggingface.co/Qwen)   | 0.5B/1.5B/3B/7B/14B/32B/72B/110B | qwen             |\n| [Qwen2-VL/QVQ](https://huggingface.co/Qwen)                       | 2B/7B/72B                        | qwen2_vl         |\n| [Skywork o1](https://huggingface.co/Skywork)                      | 8B                               | skywork_o1       |\n| [StarCoder 2](https://huggingface.co/bigcode)                     | 3B/7B/15B                        | -                |\n| [TeleChat2](https://huggingface.co/Tele-AI)                       | 3B/7B/35B/115B                   | telechat2        |\n| [XVERSE](https://huggingface.co/xverse)                           | 7B/13B/65B                       | xverse           |\n| [Yi/Yi-1.5 (Code)](https://huggingface.co/01-ai)                  | 1.5B/6B/9B/34B                   | yi               |\n| [Yi-VL](https://huggingface.co/01-ai)                             | 6B/34B                           | yi_vl            |\n| [Yuan 2](https://huggingface.co/IEITYuan)                         | 2B/51B/102B                      | yuan             |\n\n> [!NOTE]\n> å¯¹äºŽæ‰€æœ‰â€œåŸºåº§â€ï¼ˆBaseï¼‰æ¨¡åž‹ï¼Œ`template` å‚æ•°å¯ä»¥æ˜¯ `default`, `alpaca`, `vicuna` ç­‰ä»»æ„å€¼ã€‚ä½†â€œå¯¹è¯â€ï¼ˆInstruct/Chatï¼‰æ¨¡åž‹è¯·åŠ¡å¿…ä½¿ç”¨**å¯¹åº”çš„æ¨¡æ¿**ã€‚\n>\n> è¯·åŠ¡å¿…åœ¨è®­ç»ƒå’ŒæŽ¨ç†æ—¶é‡‡ç”¨**å®Œå…¨ä¸€è‡´**çš„æ¨¡æ¿ã€‚\n\né¡¹ç›®æ‰€æ”¯æŒæ¨¡åž‹çš„å®Œæ•´åˆ—è¡¨è¯·å‚é˜… [constants.py](src/llamafactory/extras/constants.py)ã€‚\n\næ‚¨ä¹Ÿå¯ä»¥åœ¨ [template.py](src/llamafactory/data/template.py) ä¸­æ·»åŠ è‡ªå·±çš„å¯¹è¯æ¨¡æ¿ã€‚\n\n## è®­ç»ƒæ–¹æ³•\n\n| æ–¹æ³•                   |     å…¨å‚æ•°è®­ç»ƒ      |    éƒ¨åˆ†å‚æ•°è®­ç»ƒ     |       LoRA         |       QLoRA        |\n| ---------------------- | ------------------ | ------------------ | ------------------ | ------------------ |\n| é¢„è®­ç»ƒ                 | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| æŒ‡ä»¤ç›‘ç£å¾®è°ƒ            | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| å¥–åŠ±æ¨¡åž‹è®­ç»ƒ            | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| PPO è®­ç»ƒ               | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| DPO è®­ç»ƒ               | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| KTO è®­ç»ƒ               | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| ORPO è®­ç»ƒ              | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| SimPO è®­ç»ƒ             | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n\n> [!TIP]\n> æœ‰å…³ PPO çš„å®žçŽ°ç»†èŠ‚ï¼Œè¯·å‚è€ƒ[æ­¤åšå®¢](https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html)ã€‚\n\n## æ•°æ®é›†\n\n<details><summary>é¢„è®­ç»ƒæ•°æ®é›†</summary>\n\n- [Wiki Demo (en)](data/wiki_demo.txt)\n- [RefinedWeb (en)](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)\n- [RedPajama V2 (en)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)\n- [Wikipedia (en)](https://huggingface.co/datasets/olm/olm-wikipedia-20221220)\n- [Wikipedia (zh)](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)\n- [Pile (en)](https://huggingface.co/datasets/EleutherAI/pile)\n- [SkyPile (zh)](https://huggingface.co/datasets/Skywork/SkyPile-150B)\n- [FineWeb (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb)\n- [FineWeb-Edu (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n- [The Stack (en)](https://huggingface.co/datasets/bigcode/the-stack)\n- [StarCoder (en)](https://huggingface.co/datasets/bigcode/starcoderdata)\n\n</details>\n\n<details><summary>æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†</summary>\n\n- [Identity (en&zh)](data/identity.json)\n- [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n- [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)\n- [Alpaca GPT4 (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [Glaive Function Calling V2 (en&zh)](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2)\n- [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n- [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n- [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n- [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n- [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n- [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n- [UltraChat (en)](https://github.com/thunlp/UltraChat)\n- [OpenPlatypus (en)](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)\n- [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n- [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n- [OpenOrca (en)](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n- [SlimOrca (en)](https://huggingface.co/datasets/Open-Orca/SlimOrca)\n- [MathInstruct (en)](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n- [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [Wiki QA (en)](https://huggingface.co/datasets/wiki_qa)\n- [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n- [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [deepctrl (en&zh)](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\n- [Advertise Generating (zh)](https://huggingface.co/datasets/HasturOfficial/adgen)\n- [ShareGPT Hyperfiltered (en)](https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k)\n- [ShareGPT4 (en&zh)](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n- [UltraChat 200k (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)\n- [AgentInstruct (en)](https://huggingface.co/datasets/THUDM/AgentInstruct)\n- [LMSYS Chat 1M (en)](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)\n- [Evol Instruct V2 (en)](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)\n- [Cosmopedia (en)](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)\n- [STEM (zh)](https://huggingface.co/datasets/hfl/stem_zh_instruction)\n- [Ruozhiba (zh)](https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo)\n- [Neo-sft (zh)](https://huggingface.co/datasets/m-a-p/neo_sft_phase2)\n- [Magpie-Pro-300K-Filtered (en)](https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered)\n- [Magpie-ultra-v0.1 (en)](https://huggingface.co/datasets/argilla/magpie-ultra-v0.1)\n- [WebInstructSub (en)](https://huggingface.co/datasets/TIGER-Lab/WebInstructSub)\n- [OpenO1-SFT (en&zh)](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)\n- [LLaVA mixed (en&zh)](https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k)\n- [Pokemon-gpt4o-captions (en&zh)](https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions)\n- [Open Assistant (de)](https://huggingface.co/datasets/mayflowergmbh/oasst_de)\n- [Dolly 15k (de)](https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de)\n- [Alpaca GPT4 (de)](https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de)\n- [OpenSchnabeltier (de)](https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de)\n- [Evol Instruct (de)](https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de)\n- [Dolphin (de)](https://huggingface.co/datasets/mayflowergmbh/dolphin_de)\n- [Booksum (de)](https://huggingface.co/datasets/mayflowergmbh/booksum_de)\n- [Airoboros (de)](https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de)\n- [Ultrachat (de)](https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de)\n\n</details>\n\n<details><summary>åå¥½æ•°æ®é›†</summary>\n\n- [DPO mixed (en&zh)](https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k)\n- [UltraFeedback (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n- [RLHF-V (en)](https://huggingface.co/datasets/openbmb/RLHF-V-Dataset)\n- [VLFeedback (en)](https://huggingface.co/datasets/Zhihui/VLFeedback)\n- [Orca DPO Pairs (en)](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n- [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [Orca DPO (de)](https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de)\n- [KTO mixed (en)](https://huggingface.co/datasets/argilla/kto-mix-15k)\n\n</details>\n\néƒ¨åˆ†æ•°æ®é›†çš„ä½¿ç”¨éœ€è¦ç¡®è®¤ï¼Œæˆ‘ä»¬æŽ¨èä½¿ç”¨ä¸‹è¿°å‘½ä»¤ç™»å½•æ‚¨çš„ Hugging Face è´¦æˆ·ã€‚\n\n```bash\npip install --upgrade huggingface_hub\nhuggingface-cli login\n```\n\n## è½¯ç¡¬ä»¶ä¾èµ–\n\n| å¿…éœ€é¡¹       | è‡³å°‘     | æŽ¨è      |\n| ------------ | ------- | --------- |\n| python       | 3.8     | 3.11      |\n| torch        | 1.13.1  | 2.4.0     |\n| transformers | 4.41.2  | 4.43.4    |\n| datasets     | 2.16.0  | 2.20.0    |\n| accelerate   | 0.30.1  | 0.32.0    |\n| peft         | 0.11.1  | 0.12.0    |\n| trl          | 0.8.6   | 0.9.6     |\n\n| å¯é€‰é¡¹       | è‡³å°‘     | æŽ¨è      |\n| ------------ | ------- | --------- |\n| CUDA         | 11.6    | 12.2      |\n| deepspeed    | 0.10.0  | 0.14.0    |\n| bitsandbytes | 0.39.0  | 0.43.1    |\n| vllm         | 0.4.3   | 0.5.0     |\n| flash-attn   | 2.3.0   | 2.6.3     |\n\n### ç¡¬ä»¶ä¾èµ–\n\n\\* *ä¼°ç®—å€¼*\n\n| æ–¹æ³•               | ç²¾åº¦ |   7B  |  13B  |  30B  |   70B  |  110B  |  8x7B |  8x22B |\n| ----------------- | ---- | ----- | ----- | ----- | ------ | ------ | ----- | ------ |\n| Full              | AMP  | 120GB | 240GB | 600GB | 1200GB | 2000GB | 900GB | 2400GB |\n| Full              |  16  |  60GB | 120GB | 300GB |  600GB |  900GB | 400GB | 1200GB |\n| Freeze            |  16  |  20GB |  40GB |  80GB |  200GB |  360GB | 160GB |  400GB |\n| LoRA/GaLore/BAdam |  16  |  16GB |  32GB |  64GB |  160GB |  240GB | 120GB |  320GB |\n| QLoRA             |   8  |  10GB |  20GB |  40GB |   80GB |  140GB |  60GB |  160GB |\n| QLoRA             |   4  |   6GB |  12GB |  24GB |   48GB |   72GB |  30GB |   96GB |\n| QLoRA             |   2  |   4GB |   8GB |  16GB |   24GB |   48GB |  18GB |   48GB |\n\n## å¦‚ä½•ä½¿ç”¨\n\n### å®‰è£… LLaMA Factory\n\n> [!IMPORTANT]\n> æ­¤æ­¥éª¤ä¸ºå¿…éœ€ã€‚\n\n```bash\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e \".[torch,metrics]\"\n```\n\nå¯é€‰çš„é¢å¤–ä¾èµ–é¡¹ï¼štorchã€torch-npuã€metricsã€deepspeedã€liger-kernelã€bitsandbytesã€hqqã€eetqã€gptqã€awqã€aqlmã€vllmã€galoreã€badamã€adam-miniã€qwenã€modelscopeã€openmindã€swanlabã€quality\n\n> [!TIP]\n> é‡åˆ°åŒ…å†²çªæ—¶ï¼Œå¯ä½¿ç”¨ `pip install --no-deps -e .` è§£å†³ã€‚\n\n<details><summary>Windows ç”¨æˆ·æŒ‡å—</summary>\n\nå¦‚æžœè¦åœ¨ Windows å¹³å°ä¸Šå¼€å¯é‡åŒ– LoRAï¼ˆQLoRAï¼‰ï¼Œéœ€è¦å®‰è£…é¢„ç¼–è¯‘çš„ `bitsandbytes` åº“, æ”¯æŒ CUDA 11.1 åˆ° 12.2, è¯·æ ¹æ®æ‚¨çš„ CUDA ç‰ˆæœ¬æƒ…å†µé€‰æ‹©é€‚åˆçš„[å‘å¸ƒç‰ˆæœ¬](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels)ã€‚\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl\n```\n\nå¦‚æžœè¦åœ¨ Windows å¹³å°ä¸Šå¼€å¯ FlashAttention-2ï¼Œéœ€è¦å®‰è£…é¢„ç¼–è¯‘çš„ `flash-attn` åº“ï¼Œæ”¯æŒ CUDA 12.1 åˆ° 12.2ï¼Œè¯·æ ¹æ®éœ€æ±‚åˆ° [flash-attention](https://github.com/bdashore3/flash-attention/releases) ä¸‹è½½å¯¹åº”ç‰ˆæœ¬å®‰è£…ã€‚\n\n</details>\n\n<details><summary>æ˜‡è…¾ NPU ç”¨æˆ·æŒ‡å—</summary>\n\nåœ¨æ˜‡è…¾ NPU è®¾å¤‡ä¸Šå®‰è£… LLaMA Factory æ—¶ï¼Œéœ€è¦æŒ‡å®šé¢å¤–ä¾èµ–é¡¹ï¼Œä½¿ç”¨ `pip install -e \".[torch-npu,metrics]\"` å‘½ä»¤å®‰è£…ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦å®‰è£… **[Ascend CANN Toolkit ä¸Ž Kernels](https://www.hiascend.com/developer/download/community/result?module=cann)**ï¼Œå®‰è£…æ–¹æ³•è¯·å‚è€ƒ[å®‰è£…æ•™ç¨‹](https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/80RC2alpha002/quickstart/quickstart/quickstart_18_0004.html)æˆ–ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š\n\n```bash\n# è¯·æ›¿æ¢ URL ä¸º CANN ç‰ˆæœ¬å’Œè®¾å¤‡åž‹å·å¯¹åº”çš„ URL\n# å®‰è£… CANN Toolkit\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-toolkit_8.0.RC1.alpha001_linux-\"$(uname -i)\".run\nbash Ascend-cann-toolkit_8.0.RC1.alpha001_linux-\"$(uname -i)\".run --install\n\n# å®‰è£… CANN Kernels\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C17SPC701/Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run\nbash Ascend-cann-kernels-910b_8.0.RC1.alpha001_linux.run --install\n\n# è®¾ç½®çŽ¯å¢ƒå˜é‡\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n| ä¾èµ–é¡¹       | è‡³å°‘     | æŽ¨è        |\n| ------------ | ------- | ----------- |\n| CANN         | 8.0.RC1 | 8.0.RC1     |\n| torch        | 2.1.0   | 2.1.0       |\n| torch-npu    | 2.1.0   | 2.1.0.post3 |\n| deepspeed    | 0.13.2  | 0.13.2      |\n\nè¯·ä½¿ç”¨ `ASCEND_RT_VISIBLE_DEVICES` è€Œéž `CUDA_VISIBLE_DEVICES` æ¥æŒ‡å®šè¿ç®—è®¾å¤‡ã€‚\n\nå¦‚æžœé‡åˆ°æ— æ³•æ­£å¸¸æŽ¨ç†çš„æƒ…å†µï¼Œè¯·å°è¯•è®¾ç½® `do_sample: false`ã€‚\n\nä¸‹è½½é¢„æž„å»º Docker é•œåƒï¼š[32GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html) | [64GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html)\n\n</details>\n\n### æ•°æ®å‡†å¤‡\n\nå…³äºŽæ•°æ®é›†æ–‡ä»¶çš„æ ¼å¼ï¼Œè¯·å‚è€ƒ [data/README_zh.md](data/README_zh.md) çš„å†…å®¹ã€‚ä½ å¯ä»¥ä½¿ç”¨ HuggingFace / ModelScope / Modelers ä¸Šçš„æ•°æ®é›†æˆ–åŠ è½½æœ¬åœ°æ•°æ®é›†ã€‚\n\n> [!NOTE]\n> ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†æ—¶ï¼Œè¯·æ›´æ–° `data/dataset_info.json` æ–‡ä»¶ã€‚\n\n### å¿«é€Ÿå¼€å§‹\n\nä¸‹é¢ä¸‰è¡Œå‘½ä»¤åˆ†åˆ«å¯¹ Llama3-8B-Instruct æ¨¡åž‹è¿›è¡Œ LoRA **å¾®è°ƒ**ã€**æŽ¨ç†**å’Œ**åˆå¹¶**ã€‚\n\n```bash\nllamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\nllamafactory-cli chat examples/inference/llama3_lora_sft.yaml\nllamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n```\n\né«˜çº§ç”¨æ³•è¯·å‚è€ƒ [examples/README_zh.md](examples/README_zh.md)ï¼ˆåŒ…æ‹¬å¤š GPU å¾®è°ƒï¼‰ã€‚\n\n> [!TIP]\n> ä½¿ç”¨ `llamafactory-cli help` æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯ã€‚\n\n### LLaMA Board å¯è§†åŒ–å¾®è°ƒï¼ˆç”± [Gradio](https://github.com/gradio-app/gradio) é©±åŠ¨ï¼‰\n\n```bash\nllamafactory-cli webui\n```\n\n### æž„å»º Docker\n\nCUDA ç”¨æˆ·ï¼š\n\n```bash\ncd docker/docker-cuda/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\næ˜‡è…¾ NPU ç”¨æˆ·ï¼š\n\n```bash\ncd docker/docker-npu/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nAMD ROCm ç”¨æˆ·ï¼š\n\n```bash\ncd docker/docker-rocm/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\n<details><summary>ä¸ä½¿ç”¨ Docker Compose æž„å»º</summary>\n\nCUDA ç”¨æˆ·ï¼š\n\n```bash\ndocker build -f ./docker/docker-cuda/Dockerfile \\\n    --build-arg INSTALL_BNB=false \\\n    --build-arg INSTALL_VLLM=false \\\n    --build-arg INSTALL_DEEPSPEED=false \\\n    --build-arg INSTALL_FLASHATTN=false \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    -t llamafactory:latest .\n\ndocker run -dit --gpus=all \\\n    -v ./hf_cache:/root/.cache/huggingface \\\n    -v ./ms_cache:/root/.cache/modelscope \\\n    -v ./om_cache:/root/.cache/openmind \\\n    -v ./data:/app/data \\\n    -v ./output:/app/output \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --shm-size 16G \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\næ˜‡è…¾ NPU ç”¨æˆ·ï¼š\n\n```bash\n# æ ¹æ®æ‚¨çš„çŽ¯å¢ƒé€‰æ‹©é•œåƒ\ndocker build -f ./docker/docker-npu/Dockerfile \\\n    --build-arg INSTALL_DEEPSPEED=false \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    -t llamafactory:latest .\n\n# æ ¹æ®æ‚¨çš„èµ„æºæ›´æ”¹ `device`\ndocker run -dit \\\n    -v ./hf_cache:/root/.cache/huggingface \\\n    -v ./ms_cache:/root/.cache/modelscope \\\n    -v ./om_cache:/root/.cache/openmind \\\n    -v ./data:/app/data \\\n    -v ./output:/app/output \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    --shm-size 16G \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nAMD ROCm ç”¨æˆ·ï¼š\n\n```bash\ndocker build -f ./docker/docker-rocm/Dockerfile \\\n    --build-arg INSTALL_BNB=false \\\n    --build-arg INSTALL_VLLM=false \\\n    --build-arg INSTALL_DEEPSPEED=false \\\n    --build-arg INSTALL_FLASHATTN=false \\\n    --build-arg PIP_INDEX=https://pypi.org/simple \\\n    -t llamafactory:latest .\n\ndocker run -dit \\\n    -v ./hf_cache:/root/.cache/huggingface \\\n    -v ./ms_cache:/root/.cache/modelscope \\\n    -v ./om_cache:/root/.cache/openmind \\\n    -v ./data:/app/data \\\n    -v ./output:/app/output \\\n    -v ./saves:/app/saves \\\n    -p 7860:7860 \\\n    -p 8000:8000 \\\n    --device /dev/kfd \\\n    --device /dev/dri \\\n    --shm-size 16G \\\n    --name llamafactory \\\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\n</details>\n\n<details><summary>æ•°æ®å·è¯¦æƒ…</summary>\n\n- `hf_cache`ï¼šä½¿ç”¨å®¿ä¸»æœºçš„ Hugging Face ç¼“å­˜æ–‡ä»¶å¤¹ï¼Œå…è®¸æ›´æ”¹ä¸ºæ–°çš„ç›®å½•ã€‚\n- `ms_cache`ï¼šç±»ä¼¼ Hugging Face ç¼“å­˜æ–‡ä»¶å¤¹ï¼Œä¸º ModelScope ç”¨æˆ·æä¾›ã€‚\n- `om_cache`ï¼šç±»ä¼¼ Hugging Face ç¼“å­˜æ–‡ä»¶å¤¹ï¼Œä¸º Modelers ç”¨æˆ·æä¾›ã€‚\n- `data`ï¼šå®¿ä¸»æœºä¸­å­˜æ”¾æ•°æ®é›†çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚\n- `output`ï¼šå°†å¯¼å‡ºç›®å½•è®¾ç½®ä¸ºè¯¥è·¯å¾„åŽï¼Œå³å¯åœ¨å®¿ä¸»æœºä¸­è®¿é—®å¯¼å‡ºåŽçš„æ¨¡åž‹ã€‚\n\n</details>\n\n### åˆ©ç”¨ vLLM éƒ¨ç½² OpenAI API\n\n```bash\nAPI_PORT=8000 llamafactory-cli api examples/inference/llama3_vllm.yaml\n```\n\n> [!TIP]\n> API æ–‡æ¡£è¯·æŸ¥é˜…[è¿™é‡Œ](https://platform.openai.com/docs/api-reference/chat/create)ã€‚\n>\n> ç¤ºä¾‹ï¼š[å›¾åƒç†è§£](scripts/api_example/test_image.py) | [å·¥å…·è°ƒç”¨](scripts/api_example/test_toolcall.py)\n\n### ä»Žé­”æ­ç¤¾åŒºä¸‹è½½\n\nå¦‚æžœæ‚¨åœ¨ Hugging Face æ¨¡åž‹å’Œæ•°æ®é›†çš„ä¸‹è½½ä¸­é‡åˆ°äº†é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ä¸‹è¿°æ–¹æ³•ä½¿ç”¨é­”æ­ç¤¾åŒºã€‚\n\n```bash\nexport USE_MODELSCOPE_HUB=1 # Windows ä½¿ç”¨ `set USE_MODELSCOPE_HUB=1`\n```\n\nå°† `model_name_or_path` è®¾ç½®ä¸ºæ¨¡åž‹ ID æ¥åŠ è½½å¯¹åº”çš„æ¨¡åž‹ã€‚åœ¨[é­”æ­ç¤¾åŒº](https://modelscope.cn/models)æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ¨¡åž‹ï¼Œä¾‹å¦‚ `LLM-Research/Meta-Llama-3-8B-Instruct`ã€‚\n\n### ä»Žé­”ä¹ç¤¾åŒºä¸‹è½½\n\næ‚¨ä¹Ÿå¯ä»¥é€šè¿‡ä¸‹è¿°æ–¹æ³•ï¼Œä½¿ç”¨é­”ä¹ç¤¾åŒºä¸‹è½½æ•°æ®é›†å’Œæ¨¡åž‹ã€‚\n\n```bash\nexport USE_OPENMIND_HUB=1 # Windows ä½¿ç”¨ `set USE_OPENMIND_HUB=1`\n```\n\nå°† `model_name_or_path` è®¾ç½®ä¸ºæ¨¡åž‹ ID æ¥åŠ è½½å¯¹åº”çš„æ¨¡åž‹ã€‚åœ¨[é­”ä¹ç¤¾åŒº](https://modelers.cn/models)æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ¨¡åž‹ï¼Œä¾‹å¦‚ `TeleAI/TeleChat-7B-pt`ã€‚\n\n### ä½¿ç”¨ W&B é¢æ¿\n\nè‹¥è¦ä½¿ç”¨ [Weights & Biases](https://wandb.ai) è®°å½•å®žéªŒæ•°æ®ï¼Œè¯·åœ¨ yaml æ–‡ä»¶ä¸­æ·»åŠ ä¸‹é¢çš„å‚æ•°ã€‚\n\n```yaml\nreport_to: wandb\nrun_name: test_run # å¯é€‰\n```\n\nåœ¨å¯åŠ¨è®­ç»ƒä»»åŠ¡æ—¶ï¼Œå°† `WANDB_API_KEY` è®¾ç½®ä¸º[å¯†é’¥](https://wandb.ai/authorize)æ¥ç™»å½• W&B è´¦æˆ·ã€‚\n\n### ä½¿ç”¨ SwanLab é¢æ¿\n\nè‹¥è¦ä½¿ç”¨ [SwanLab](https://github.com/SwanHubX/SwanLab) è®°å½•å®žéªŒæ•°æ®ï¼Œè¯·åœ¨ yaml æ–‡ä»¶ä¸­æ·»åŠ ä¸‹é¢çš„å‚æ•°ã€‚\n\n```yaml\nuse_swanlab: true\nswanlab_run_name: test_run # å¯é€‰\n```\n\nåœ¨å¯åŠ¨è®­ç»ƒä»»åŠ¡æ—¶ï¼Œç™»å½•SwanLabè´¦æˆ·æœ‰ä»¥ä¸‹ä¸‰ç§æ–¹å¼ï¼š\n\næ–¹å¼ä¸€ï¼šåœ¨ yaml æ–‡ä»¶ä¸­æ·»åŠ  `swanlab_api_key=<your_api_key>` ï¼Œå¹¶è®¾ç½®ä¸ºä½ çš„ [API å¯†é’¥](https://swanlab.cn/settings)ã€‚\næ–¹å¼äºŒï¼šå°†çŽ¯å¢ƒå˜é‡ `SWANLAB_API_KEY` è®¾ç½®ä¸ºä½ çš„ [API å¯†é’¥](https://swanlab.cn/settings)ã€‚\næ–¹å¼ä¸‰ï¼šå¯åŠ¨å‰ä½¿ç”¨ `swanlab login` å‘½ä»¤å®Œæˆç™»å½•ã€‚\n\n## ä½¿ç”¨äº† LLaMA Factory çš„é¡¹ç›®\n\nå¦‚æžœæ‚¨æœ‰é¡¹ç›®å¸Œæœ›æ·»åŠ è‡³ä¸‹è¿°åˆ—è¡¨ï¼Œè¯·é€šè¿‡é‚®ä»¶è”ç³»æˆ–è€…åˆ›å»ºä¸€ä¸ª PRã€‚\n\n<details><summary>ç‚¹å‡»æ˜¾ç¤º</summary>\n\n1. Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. [[arxiv]](https://arxiv.org/abs/2308.02223)\n1. Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. [[arxiv]](https://arxiv.org/abs/2308.10092)\n1. Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. [[arxiv]](https://arxiv.org/abs/2308.10526)\n1. Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. [[arxiv]](https://arxiv.org/abs/2311.07816)\n1. Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. [[arxiv]](https://arxiv.org/abs/2312.15710)\n1. Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. [[arxiv]](https://arxiv.org/abs/2401.04319)\n1. Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2401.07286)\n1. Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2402.05904)\n1. Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. [[arxiv]](https://arxiv.org/abs/2402.07625)\n1. Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11176)\n1. Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. [[arxiv]](https://arxiv.org/abs/2402.11187)\n1. Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. [[arxiv]](https://arxiv.org/abs/2402.11746)\n1. Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11801)\n1. Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2402.11809)\n1. Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11819)\n1. Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. [[arxiv]](https://arxiv.org/abs/2402.12204)\n1. Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.14714)\n1. Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. [[arxiv]](https://arxiv.org/abs/2402.15043)\n1. Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. [[arxiv]](https://arxiv.org/abs/2403.02333)\n1. Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. [[arxiv]](https://arxiv.org/abs/2403.03419)\n1. Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2403.08228)\n1. Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2403.09073)\n1. Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. [[arxiv]](https://arxiv.org/abs/2403.14541)\n1. Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2403.15246)\n1. Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. [[arxiv]](https://arxiv.org/abs/2403.16008)\n1. Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. [[arxiv]](https://arxiv.org/abs/2403.16443)\n1. Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2404.00604)\n1. Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.02827)\n1. Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2404.04167)\n1. Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. [[arxiv]](https://arxiv.org/abs/2404.04316)\n1. Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.07084)\n1. Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.09836)\n1. Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.11581)\n1. Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. [[arxiv]](https://arxiv.org/abs/2404.14215)\n1. Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2404.16621)\n1. Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2404.17140)\n1. Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. [[arxiv]](https://arxiv.org/abs/2404.18585)\n1. Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. [[arxiv]](https://arxiv.org/abs/2405.04760)\n1. Dammu et al. \"They are uncultured\": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. [[arxiv]](https://arxiv.org/abs/2405.05378)\n1. Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. [[arxiv]](https://arxiv.org/abs/2405.09055)\n1. Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. [[arxiv]](https://arxiv.org/abs/2405.12739)\n1. Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2405.13816)\n1. Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2405.20215)\n1. Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. [[paper]](https://aclanthology.org/2024.lt4hala-1.30)\n1. Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2406.00380)\n1. Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. [[arxiv]](https://arxiv.org/abs/2406.02106)\n1. Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. [[arxiv]](https://arxiv.org/abs/2406.03136)\n1. Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2406.04496)\n1. Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. [[arxiv]](https://arxiv.org/abs/2406.05688)\n1. Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. [[arxiv]](https://arxiv.org/abs/2406.05955)\n1. Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. [[arxiv]](https://arxiv.org/abs/2406.06973)\n1. Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. [[arxiv]](https://arxiv.org/abs/2406.07115)\n1. Zhu et al. Are Large Language Models Good Statisticians?. 2024. [[arxiv]](https://arxiv.org/abs/2406.07815)\n1. Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2406.10099)\n1. Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. [[arxiv]](https://arxiv.org/abs/2406.10173)\n1. He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. [[arxiv]](https://arxiv.org/abs/2406.12074)\n1. Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. [[arxiv]](https://arxiv.org/abs/2406.14408)\n1. Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. [[arxiv]](https://arxiv.org/abs/2406.14546)\n1. Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. [[arxiv]](https://arxiv.org/abs/2406.15695)\n1. Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. [[arxiv]](https://arxiv.org/abs/2406.17233)\n1. Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. [[arxiv]](https://arxiv.org/abs/2406.18069)\n1. Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. [[paper]](https://aclanthology.org/2024.americasnlp-1.25)\n1. Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. [[arxiv]](https://arxiv.org/abs/2406.19949)\n1. Yang et al. Financial Knowledge Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2407.00365)\n1. Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. [[arxiv]](https://arxiv.org/abs/2407.01470)\n1. Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. [[arxiv]](https://arxiv.org/abs/2407.06129)\n1. Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. [[arxiv]](https://arxiv.org/abs/2407.08044)\n1. Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. [[arxiv]](https://arxiv.org/abs/2407.09756)\n1. Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. [[paper]](https://scholarcommons.scu.edu/cseng_senior/272/)\n1. Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. [[arxiv]](https://arxiv.org/abs/2407.13561)\n1. Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. [[arxiv]](https://arxiv.org/abs/2407.16637)\n1. Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. [[arxiv]](https://arxiv.org/abs/2407.17535)\n1. Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2407.19705)\n1. Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2408.00137)\n1. Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. [[paper]](https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf)\n1. Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11)\n1. Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23)\n1. Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2408.04693)\n1. Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2408.04168)\n1. Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. [[paper]](https://aclanthology.org/2024.finnlp-2.1/)\n1. Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. [[arxiv]](https://arxiv.org/abs/2408.08072)\n1. Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. [[paper]](https://dl.acm.org/doi/10.1145/3627673.3679611)\n1. **[StarWhisper](https://github.com/Yu-Yang-Li/StarWhisper)**: å¤©æ–‡å¤§æ¨¡åž‹ StarWhisperï¼ŒåŸºäºŽ ChatGLM2-6B å’Œ Qwen-14B åœ¨å¤©æ–‡æ•°æ®ä¸Šå¾®è°ƒè€Œå¾—ã€‚\n1. **[DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)**: ä¸­æ–‡æ³•å¾‹é¢†åŸŸå¤§æ¨¡åž‹ DISC-LawLLMï¼ŒåŸºäºŽ Baichuan-13B å¾®è°ƒè€Œå¾—ï¼Œå…·æœ‰æ³•å¾‹æŽ¨ç†å’ŒçŸ¥è¯†æ£€ç´¢èƒ½åŠ›ã€‚\n1. **[Sunsimiao](https://github.com/X-D-Lab/Sunsimiao)**: å­™æ€é‚ˆä¸­æ–‡åŒ»ç–—å¤§æ¨¡åž‹ Sumsimiaoï¼ŒåŸºäºŽ Baichuan-7B å’Œ ChatGLM-6B åœ¨ä¸­æ–‡åŒ»ç–—æ•°æ®ä¸Šå¾®è°ƒè€Œå¾—ã€‚\n1. **[CareGPT](https://github.com/WangRongsheng/CareGPT)**: åŒ»ç–—å¤§æ¨¡åž‹é¡¹ç›® CareGPTï¼ŒåŸºäºŽ LLaMA2-7B å’Œ Baichuan-13B åœ¨ä¸­æ–‡åŒ»ç–—æ•°æ®ä¸Šå¾®è°ƒè€Œå¾—ã€‚\n1. **[MachineMindset](https://github.com/PKU-YuanGroup/Machine-Mindset/)**ï¼šMBTIæ€§æ ¼å¤§æ¨¡åž‹é¡¹ç›®ï¼Œæ ¹æ®æ•°æ®é›†ä¸Žè®­ç»ƒæ–¹å¼è®©ä»»æ„ LLM æ‹¥æœ‰ 16 ä¸ªä¸åŒçš„æ€§æ ¼ç±»åž‹ã€‚\n1. **[Luminia-13B-v3](https://huggingface.co/Nekochu/Luminia-13B-v3)**ï¼šä¸€ä¸ªç”¨äºŽç”Ÿæˆ Stable Diffusion æç¤ºè¯çš„å¤§åž‹è¯­è¨€æ¨¡åž‹ã€‚[[demo]](https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt)\n1. **[Chinese-LLaVA-Med](https://github.com/BUAADreamer/Chinese-LLaVA-Med)**ï¼šä¸­æ–‡å¤šæ¨¡æ€åŒ»å­¦å¤§æ¨¡åž‹ï¼ŒåŸºäºŽ LLaVA-1.5-7B åœ¨ä¸­æ–‡å¤šæ¨¡æ€åŒ»ç–—æ•°æ®ä¸Šå¾®è°ƒè€Œå¾—ã€‚\n1. **[AutoRE](https://github.com/THUDM/AutoRE)**ï¼šåŸºäºŽå¤§è¯­è¨€æ¨¡åž‹çš„æ–‡æ¡£çº§å…³ç³»æŠ½å–ç³»ç»Ÿã€‚\n1. **[NVIDIA RTX AI Toolkit](https://github.com/NVIDIA/RTX-AI-Toolkit)**ï¼šåœ¨ Windows ä¸»æœºä¸Šåˆ©ç”¨è‹±ä¼Ÿè¾¾ RTX è®¾å¤‡è¿›è¡Œå¤§åž‹è¯­è¨€æ¨¡åž‹å¾®è°ƒçš„å¼€å‘åŒ…ã€‚\n1. **[LazyLLM](https://github.com/LazyAGI/LazyLLM)**ï¼šä¸€ä¸ªä½Žä»£ç æž„å»ºå¤š Agent å¤§æ¨¡åž‹åº”ç”¨çš„å¼€å‘å·¥å…·ï¼Œæ”¯æŒåŸºäºŽ LLaMA Factory çš„æ¨¡åž‹å¾®è°ƒ.\n1. **[RAG-Retrieval](https://github.com/NLPJCL/RAG-Retrieval)**ï¼šä¸€ä¸ªå…¨é“¾è·¯ RAG æ£€ç´¢æ¨¡åž‹å¾®è°ƒã€æŽ¨ç†å’Œè’¸é¦ä»£ç åº“ã€‚[[blog]](https://zhuanlan.zhihu.com/p/987727357)\n1. **[360-LLaMA-Factory](https://github.com/Qihoo360/360-LLaMA-Factory)**ï¼šä¸€ä¸ªé­”æ”¹åŽçš„ä»£ç åº“ï¼Œé€šè¿‡ Ring Attention æ”¯æŒé•¿åºåˆ—çš„ SFT å’Œ DPO è®­ç»ƒã€‚\n\n</details>\n\n## åè®®\n\næœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](LICENSE) åè®®å¼€æºã€‚\n\nä½¿ç”¨æ¨¡åž‹æƒé‡æ—¶ï¼Œè¯·éµå¾ªå¯¹åº”çš„æ¨¡åž‹åè®®ï¼š[Baichuan 2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf) / [BLOOM](https://huggingface.co/spaces/bigscience/license) / [ChatGLM3](https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE) / [Command R](https://cohere.com/c4ai-cc-by-nc-license) / [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL) / [Falcon](https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt) / [Gemma](https://ai.google.dev/gemma/terms) / [GLM-4](https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE) / [GPT-2](https://github.com/openai/gpt-2/blob/master/LICENSE) / [Granite](LICENSE) / [Index](https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE) / [InternLM2](https://github.com/InternLM/InternLM#license) / [Llama](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) / [Llama 2 (LLaVA-1.5)](https://ai.meta.com/llama/license/) / [Llama 3](https://llama.meta.com/llama3/license/) / [MiniCPM](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md) / [Mistral/Mixtral/Pixtral](LICENSE) / [OLMo](LICENSE) / [Phi-1.5/Phi-2](https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx) / [Phi-3/Phi-4](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE) / [Qwen](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) / [Skywork](https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf) / [StarCoder 2](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) / [TeleChat2](https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) / [XVERSE](https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf) / [Yi](https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE) / [Yi-1.5](LICENSE) / [Yuan 2](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan)\n\n## å¼•ç”¨\n\nå¦‚æžœæ‚¨è§‰å¾—æ­¤é¡¹ç›®æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ä»¥ä¸‹åˆ—æ ¼å¼å¼•ç”¨\n\n```bibtex\n@inproceedings{zheng2024llamafactory,\n  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},\n  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},\n  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},\n  address={Bangkok, Thailand},\n  publisher={Association for Computational Linguistics},\n  year={2024},\n  url={http://arxiv.org/abs/2403.13372}\n}\n```\n\n## è‡´è°¢\n\næœ¬é¡¹ç›®å—ç›ŠäºŽ [PEFT](https://github.com/huggingface/peft)ã€[TRL](https://github.com/huggingface/trl)ã€[QLoRA](https://github.com/artidoro/qlora) å’Œ [FastChat](https://github.com/lm-sys/FastChat)ï¼Œæ„Ÿè°¢ä»¥ä¸Šè¯¸ä½ä½œè€…çš„ä»˜å‡ºã€‚\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&type=Date)\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "evaluation",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.6298828125,
          "content": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.ruff]\ntarget-version = \"py38\"\nline-length = 119\nindent-width = 4\n\n[tool.ruff.lint]\nignore = [\"C408\", \"C901\", \"E501\", \"E731\", \"E741\", \"W605\"]\nselect = [\"C\", \"E\", \"F\", \"I\", \"W\"]\n\n[tool.ruff.lint.isort]\nlines-after-imports = 2\nknown-first-party = [\"llamafactory\"]\nknown-third-party = [\n    \"accelerate\",\n    \"datasets\",\n    \"gradio\",\n    \"numpy\",\n    \"peft\",\n    \"torch\",\n    \"transformers\",\n    \"trl\"\n]\n\n[tool.ruff.format]\nquote-style = \"double\"\nindent-style = \"space\"\ndocstring-code-format = true\nskip-magic-trailing-comma = false\nline-ending = \"auto\"\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.3271484375,
          "content": "transformers>=4.41.2,<=4.46.1\ndatasets>=2.16.0,<=3.1.0\naccelerate>=0.34.0,<=1.0.1\npeft>=0.11.1,<=0.12.0\ntrl>=0.8.6,<=0.9.6\ntokenizers>=0.19.0,<0.20.4\ngradio>=4.0.0,<5.0.0\npandas>=2.0.0\nscipy\neinops\nsentencepiece\ntiktoken\nprotobuf\nuvicorn\npydantic\nfastapi\nsse-starlette\nmatplotlib>=3.7.0\nfire\npackaging\npyyaml\nnumpy<2.0.0\nav\ntyro<0.9.0\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 3.708984375,
          "content": "# Copyright 2024 the LlamaFactory team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport re\nfrom typing import List\n\nfrom setuptools import find_packages, setup\n\n\ndef get_version() -> str:\n    with open(os.path.join(\"src\", \"llamafactory\", \"extras\", \"env.py\"), encoding=\"utf-8\") as f:\n        file_content = f.read()\n        pattern = r\"{}\\W*=\\W*\\\"([^\\\"]+)\\\"\".format(\"VERSION\")\n        (version,) = re.findall(pattern, file_content)\n        return version\n\n\ndef get_requires() -> List[str]:\n    with open(\"requirements.txt\", encoding=\"utf-8\") as f:\n        file_content = f.read()\n        lines = [line.strip() for line in file_content.strip().split(\"\\n\") if not line.startswith(\"#\")]\n        return lines\n\n\ndef get_console_scripts() -> List[str]:\n    console_scripts = [\"llamafactory-cli = llamafactory.cli:main\"]\n    if os.environ.get(\"ENABLE_SHORT_CONSOLE\", \"1\").lower() in [\"true\", \"1\"]:\n        console_scripts.append(\"lmf = llamafactory.cli:main\")\n\n    return console_scripts\n\n\nextra_require = {\n    \"torch\": [\"torch>=1.13.1\"],\n    \"torch-npu\": [\"torch==2.1.0\", \"torch-npu==2.1.0.post3\", \"decorator\"],\n    \"metrics\": [\"nltk\", \"jieba\", \"rouge-chinese\"],\n    \"deepspeed\": [\"deepspeed>=0.10.0,<=0.14.4\"],\n    \"liger-kernel\": [\"liger-kernel\"],\n    \"bitsandbytes\": [\"bitsandbytes>=0.39.0\"],\n    \"hqq\": [\"hqq\"],\n    \"eetq\": [\"eetq\"],\n    \"gptq\": [\"optimum>=1.17.0\", \"auto-gptq>=0.5.0\"],\n    \"awq\": [\"autoawq\"],\n    \"aqlm\": [\"aqlm[gpu]>=1.1.0\"],\n    \"vllm\": [\"vllm>=0.4.3,<0.6.7\"],\n    \"galore\": [\"galore-torch\"],\n    \"badam\": [\"badam>=1.2.1\"],\n    \"adam-mini\": [\"adam-mini\"],\n    \"qwen\": [\"transformers_stream_generator\"],\n    \"modelscope\": [\"modelscope\"],\n    \"openmind\": [\"openmind\"],\n    \"swanlab\": [\"swanlab\"],\n    \"dev\": [\"pre-commit\", \"ruff\", \"pytest\"],\n}\n\n\ndef main():\n    setup(\n        name=\"llamafactory\",\n        version=get_version(),\n        author=\"hiyouga\",\n        author_email=\"hiyouga AT buaa.edu.cn\",\n        description=\"Easy-to-use LLM fine-tuning framework\",\n        long_description=open(\"README.md\", encoding=\"utf-8\").read(),\n        long_description_content_type=\"text/markdown\",\n        keywords=[\"LLaMA\", \"BLOOM\", \"Falcon\", \"LLM\", \"ChatGPT\", \"transformer\", \"pytorch\", \"deep learning\"],\n        license=\"Apache 2.0 License\",\n        url=\"https://github.com/hiyouga/LLaMA-Factory\",\n        package_dir={\"\": \"src\"},\n        packages=find_packages(\"src\"),\n        python_requires=\">=3.8.0\",\n        install_requires=get_requires(),\n        extras_require=extra_require,\n        entry_points={\"console_scripts\": get_console_scripts()},\n        classifiers=[\n            \"Development Status :: 4 - Beta\",\n            \"Intended Audience :: Developers\",\n            \"Intended Audience :: Education\",\n            \"Intended Audience :: Science/Research\",\n            \"License :: OSI Approved :: Apache Software License\",\n            \"Operating System :: OS Independent\",\n            \"Programming Language :: Python :: 3\",\n            \"Programming Language :: Python :: 3.8\",\n            \"Programming Language :: Python :: 3.9\",\n            \"Programming Language :: Python :: 3.10\",\n            \"Programming Language :: Python :: 3.11\",\n            \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}