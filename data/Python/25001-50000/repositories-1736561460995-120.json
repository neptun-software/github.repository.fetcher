{
  "metadata": {
    "timestamp": 1736561460995,
    "page": 120,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "JaidedAI/EasyOCR",
      "stars": 25130,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.177734375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# macOS\n*.DS_Store\n\n# IDEs\n.vscode/\n.vs/\n.idea/\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.8564453125,
          "content": "FROM docker.io/pytorch/pytorch\n\n# if you forked EasyOCR, you can pass in your own GitHub username to use your fork\n# i.e. gh_username=myname\nARG gh_username=JaidedAI\nARG service_home=\"/home/EasyOCR\"\n\n# Configure apt and install packages\nRUN apt-get update -y && \\\n    apt-get install -y \\\n    libglib2.0-0 \\\n    libsm6 \\\n    libxext6 \\\n    libxrender-dev \\\n    libgl1-mesa-dev \\\n    git \\\n    # cleanup\n    && apt-get autoremove -y \\\n    && apt-get clean -y \\\n    && rm -rf /var/lib/apt/lists\n\n# Clone EasyOCR repo\nRUN mkdir \"$service_home\" \\\n    && git clone \"https://github.com/$gh_username/EasyOCR.git\" \"$service_home\" \\\n    && cd \"$service_home\" \\\n    && git remote add upstream \"https://github.com/JaidedAI/EasyOCR.git\" \\\n    && git pull upstream master\n\n# Build\nRUN cd \"$service_home\" \\\n    && python setup.py build_ext --inplace -j 4 \\\n    && python -m pip install -e .\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.1884765625,
          "content": "include LICENSE.txt\ninclude README.md\n\ninclude easyocr/model/*\ninclude easyocr/character/*\ninclude easyocr/dict/*\ninclude easyocr/scripts/compile_dbnet_dcn.py\nrecursive-include easyocr/DBNet *\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.748046875,
          "content": "# EasyOCR\n\n[![PyPI Status](https://badge.fury.io/py/easyocr.svg)](https://badge.fury.io/py/easyocr)\n[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/JaidedAI/EasyOCR/blob/master/LICENSE)\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.to/easyocr)\n[![Tweet](https://img.shields.io/twitter/url/https/github.com/JaidedAI/EasyOCR.svg?style=social)](https://twitter.com/intent/tweet?text=Check%20out%20this%20awesome%20library:%20EasyOCR%20https://github.com/JaidedAI/EasyOCR)\n[![Twitter](https://img.shields.io/badge/twitter-@JaidedAI-blue.svg?style=flat)](https://twitter.com/JaidedAI)\n\nReady-to-use OCR with 80+ [supported languages](https://www.jaided.ai/easyocr) and all popular writing scripts including: Latin, Chinese, Arabic, Devanagari, Cyrillic, etc.\n\n[Try Demo on our website](https://www.jaided.ai/easyocr)\n\nIntegrated into [Huggingface Spaces 🤗](https://huggingface.co/spaces) using [Gradio](https://github.com/gradio-app/gradio). Try out the Web Demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/tomofi/EasyOCR)\n\n\n## What's new\n- 24 September 2024 - Version 1.7.2\n    - Fix several compatibilities\n\n- [Read all release notes](https://github.com/JaidedAI/EasyOCR/blob/master/releasenotes.md)\n\n## What's coming next\n- Handwritten text support\n\n## Examples\n\n![example](examples/example.png)\n\n![example2](examples/example2.png)\n\n![example3](examples/example3.png)\n\n\n## Installation\n\nInstall using `pip`\n\nFor the latest stable release:\n\n``` bash\npip install easyocr\n```\n\nFor the latest development release:\n\n``` bash\npip install git+https://github.com/JaidedAI/EasyOCR.git\n```\n\nNote 1: For Windows, please install torch and torchvision first by following the official instructions here https://pytorch.org. On the pytorch website, be sure to select the right CUDA version you have. If you intend to run on CPU mode only, select `CUDA = None`.\n\nNote 2: We also provide a Dockerfile [here](https://github.com/JaidedAI/EasyOCR/blob/master/Dockerfile).\n\n## Usage\n\n``` python\nimport easyocr\nreader = easyocr.Reader(['ch_sim','en']) # this needs to run only once to load the model into memory\nresult = reader.readtext('chinese.jpg')\n```\n\nThe output will be in a list format, each item represents a bounding box, the text detected and confident level, respectively.\n\n``` bash\n[([[189, 75], [469, 75], [469, 165], [189, 165]], '愚园路', 0.3754989504814148),\n ([[86, 80], [134, 80], [134, 128], [86, 128]], '西', 0.40452659130096436),\n ([[517, 81], [565, 81], [565, 123], [517, 123]], '东', 0.9989598989486694),\n ([[78, 126], [136, 126], [136, 156], [78, 156]], '315', 0.8125889301300049),\n ([[514, 126], [574, 126], [574, 156], [514, 156]], '309', 0.4971577227115631),\n ([[226, 170], [414, 170], [414, 220], [226, 220]], 'Yuyuan Rd.', 0.8261902332305908),\n ([[79, 173], [125, 173], [125, 213], [79, 213]], 'W', 0.9848111271858215),\n ([[529, 173], [569, 173], [569, 213], [529, 213]], 'E', 0.8405593633651733)]\n```\nNote 1: `['ch_sim','en']` is the list of languages you want to read. You can pass\nseveral languages at once but not all languages can be used together.\nEnglish is compatible with every language and languages that share common characters are usually compatible with each other.\n\nNote 2: Instead of the filepath `chinese.jpg`, you can also pass an OpenCV image object (numpy array) or an image file as bytes. A URL to a raw image is also acceptable.\n\nNote 3: The line `reader = easyocr.Reader(['ch_sim','en'])` is for loading a model into memory. It takes some time but it needs to be run only once.\n\nYou can also set `detail=0` for simpler output.\n\n``` python\nreader.readtext('chinese.jpg', detail = 0)\n```\nResult:\n``` bash\n['愚园路', '西', '东', '315', '309', 'Yuyuan Rd.', 'W', 'E']\n```\n\nModel weights for the chosen language will be automatically downloaded or you can\ndownload them manually from the [model hub](https://www.jaided.ai/easyocr/modelhub) and put them in the '~/.EasyOCR/model' folder\n\nIn case you do not have a GPU, or your GPU has low memory, you can run the model in CPU-only mode by adding `gpu=False`.\n\n``` python\nreader = easyocr.Reader(['ch_sim','en'], gpu=False)\n```\n\nFor more information, read the [tutorial](https://www.jaided.ai/easyocr/tutorial) and [API Documentation](https://www.jaided.ai/easyocr/documentation).\n\n#### Run on command line\n\n```shell\n$ easyocr -l ch_sim en -f chinese.jpg --detail=1 --gpu=True\n```\n\n## Train/use your own model\n\nFor recognition model, [Read here](https://github.com/JaidedAI/EasyOCR/blob/master/custom_model.md).\n\nFor detection model (CRAFT), [Read here](https://github.com/JaidedAI/EasyOCR/blob/master/trainer/craft/README.md).\n\n## Implementation Roadmap\n\n- Handwritten support\n- Restructure code to support swappable detection and recognition algorithms\nThe api should be as easy as\n``` python\nreader = easyocr.Reader(['en'], detection='DB', recognition = 'Transformer')\n```\nThe idea is to be able to plug in any state-of-the-art model into EasyOCR. There are a lot of geniuses trying to make better detection/recognition models, but we are not trying to be geniuses here. We just want to make their works quickly accessible to the public ... for free. (well, we believe most geniuses want their work to create a positive impact as fast/big as possible) The pipeline should be something like the below diagram. Grey slots are placeholders for changeable light blue modules.\n\n![plan](examples/easyocr_framework.jpeg)\n\n## Acknowledgement and References\n\nThis project is based on research and code from several papers and open-source repositories.\n\nAll deep learning execution is based on [Pytorch](https://pytorch.org). :heart:\n\nDetection execution uses the CRAFT algorithm from this [official repository](https://github.com/clovaai/CRAFT-pytorch) and their [paper](https://arxiv.org/abs/1904.01941) (Thanks @YoungminBaek from [@clovaai](https://github.com/clovaai)). We also use their pretrained model. Training script is provided by [@gmuffiness](https://github.com/gmuffiness).\n\nThe recognition model is a CRNN ([paper](https://arxiv.org/abs/1507.05717)). It is composed of 3 main components: feature extraction (we are currently using [Resnet](https://arxiv.org/abs/1512.03385)) and VGG, sequence labeling ([LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf)) and decoding ([CTC](https://www.cs.toronto.edu/~graves/icml_2006.pdf)). The training pipeline for recognition execution is a modified version of the [deep-text-recognition-benchmark](https://github.com/clovaai/deep-text-recognition-benchmark) framework. (Thanks [@ku21fan](https://github.com/ku21fan) from [@clovaai](https://github.com/clovaai)) This repository is a gem that deserves more recognition.\n\nBeam search code is based on this [repository](https://github.com/githubharald/CTCDecoder) and his [blog](https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7). (Thanks [@githubharald](https://github.com/githubharald))\n\nData synthesis is based on [TextRecognitionDataGenerator](https://github.com/Belval/TextRecognitionDataGenerator). (Thanks [@Belval](https://github.com/Belval))\n\nAnd a good read about CTC from distill.pub [here](https://distill.pub/2017/ctc/).\n\n## Want To Contribute?\n\nLet's advance humanity together by making AI available to everyone!\n\n3 ways to contribute:\n\n**Coder:** Please send a PR for small bugs/improvements. For bigger ones, discuss with us by opening an issue first. There is a list of possible bug/improvement issues tagged with ['PR WELCOME'](https://github.com/JaidedAI/EasyOCR/issues?q=is%3Aissue+is%3Aopen+label%3A%22PR+WELCOME%22).\n\n**User:** Tell us how EasyOCR benefits you/your organization to encourage further development. Also post failure cases in [Issue  Section](https://github.com/JaidedAI/EasyOCR/issues) to help improve future models.\n\n**Tech leader/Guru:** If you found this library useful, please spread the word! (See [Yann Lecun's post](https://www.facebook.com/yann.lecun/posts/10157018122787143) about EasyOCR)\n\n## Guideline for new language request\n\nTo request a new language, we need you to send a PR with the 2 following files:\n\n1. In folder [easyocr/character](https://github.com/JaidedAI/EasyOCR/tree/master/easyocr/character),\nwe need 'yourlanguagecode_char.txt' that contains list of all characters. Please see format examples from other files in that folder.\n2. In folder [easyocr/dict](https://github.com/JaidedAI/EasyOCR/tree/master/easyocr/dict),\nwe need 'yourlanguagecode.txt' that contains list of words in your language.\nOn average, we have ~30000 words per language with more than 50000 words for more popular ones.\nMore is better in this file.\n\nIf your language has unique elements (such as 1. Arabic: characters change form when attached to each other + write from right to left 2. Thai: Some characters need to be above the line and some below), please educate us to the best of your ability and/or give useful links. It is important to take care of the detail to achieve a system that really works.\n\nLastly, please understand that our priority will have to go to popular languages or sets of languages that share large portions of their characters with each other (also tell us if this is the case for your language). It takes us at least a week to develop a new model, so you may have to wait a while for the new model to be released.\n\nSee [List of languages in development](https://github.com/JaidedAI/EasyOCR/issues/91)\n\n## Github Issues\n\nDue to limited resources, an issue older than 6 months will be automatically closed. Please open an issue again if it is critical.\n\n## Business Inquiries\n\nFor Enterprise Support, [Jaided AI](https://www.jaided.ai/) offers full service for custom OCR/AI systems from implementation, training/finetuning and deployment. Click [here](https://www.jaided.ai/contactus?ref=github) to contact us.\n"
        },
        {
          "name": "custom_model.md",
          "type": "blob",
          "size": 2.0732421875,
          "content": "# Custom recognition models\n\n## How to train your custom model\n\nYou can use your own data or generate your own dataset. To generate your own data, we recommend using\n[TextRecognitionDataGenerator](https://github.com/Belval/TextRecognitionDataGenerator). We provide an example of a dataset [here](https://jaided.ai/easyocr/modelhub/).\nAfter you have a dataset, you can train your own model by following this repository\n[deep-text-recognition-benchmark](https://github.com/clovaai/deep-text-recognition-benchmark).\nThe network needs to be fully convolutional in order to predict flexible text length. Our current network is 'None-VGG-BiLSTM-CTC'.\nOnce you have your trained model (a `.pth` file), you need 2 additional files describing recognition network architecture and model configuration.\nAn example is provided in `custom_example.zip` file [here](https://jaided.ai/easyocr/modelhub/).\n\nPlease do not create an issue about data generation and model training in this repository. If you have any question regarding data generation and model training, please ask in the respective repositories.\n\nNote: We also provide our version of a training script [here](https://github.com/JaidedAI/EasyOCR/tree/master/trainer). It is a modified version from [deep-text-recognition-benchmark](https://github.com/clovaai/deep-text-recognition-benchmark).\n\n## How to use your custom model\n\nTo use your own recognition model, you need the three files as explained above. These three files have to share the same name (i.e. `yourmodel.pth`, `yourmodel.yaml`, `yourmodel.py`) that you will then use to call your model with EasyOCR API.\n\nWe provide [custom_example.zip](https://jaided.ai/easyocr/modelhub/)\nas an example. Please download, extract and place `custom_example.py`, `custom_example.yaml` in the `user_network_directory` (default = `~/.EasyOCR/user_network`) and place `custom_example.pth` in model directory (default = `~/.EasyOCR/model`)\nOnce you place all 3 files in their respective places, you can use `custom_example` by\nspecifying `recog_network` like this `reader = easyocr.Reader(['en'], recog_network='custom_example')`.\n"
        },
        {
          "name": "easyocr",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "releasenotes.md",
          "type": "blob",
          "size": 6.6220703125,
          "content": "- 24 September 2024 - Version 1.7.2\n    - Fix several compatibilities\n- 4 September 2023 - Version 1.7.1\n    - Fix several compatibilities\n- 25 May 2023 - Version 1.7.0\n    - Add Apple Silicon support (thanks[@rayeesoft](https://github.com/rayeesoft) and [@ArtemBernatskyy](https://github.com/ArtemBernatskyy), see [PR](https://github.com/JaidedAI/EasyOCR/pull/1004))\n    - Fix several compatibilities\n- 15 September 2022 - Version 1.6.2\n    - Add CPU support for DBnet\n    - DBNet will only be compiled when users initialize DBnet detector.  \n- 1 September 2022 - Version 1.6.1\n    - Fix DBNet path bug for Windows\n    - Add new built-in model `cyrillic_g2`. This model is a new default for Cyrillic script. (partial financial support by Alejandro Cabrerizo)\n- 24 August 2022 - Version 1.6.0\n    - Restructure code to support alternative text detectors.\n    - Add detector `DBNet`, see [paper](https://arxiv.org/abs/2202.10304v1). It can be used by initializing like this `reader = easyocr.Reader(['en'], detect_network = 'dbnet18')`. *Currently, DBNet text detector requires running with GPU.*\n- 2 June 2022 - Version 1.5.0\n    - Add trainer for CRAFT detection model (thanks[@gmuffiness](https://github.com/gmuffiness), see [PR](https://github.com/JaidedAI/EasyOCR/pull/739))\n- 9 April 2022 - Version 1.4.2\n    - Update dependencies (opencv and pillow issues)\n- 11 September 2021 - Version 1.4.1\n    - Add trainer folder\n    - Add `readtextlang` method (thanks[@arkya-art](https://github.com/arkya-art), see [PR](https://github.com/JaidedAI/EasyOCR/pull/525))\n    - Extend `rotation_info` argument to support all possible angle (thanks[abde0103](https://github.com/abde0103), see [PR](https://github.com/JaidedAI/EasyOCR/pull/515))\n- 29 June 2021 - Version 1.4\n    - [Instruction](https://github.com/JaidedAI/EasyOCR/blob/master/custom_model.md) on training/using custom recognition model\n    - [Example dataset](https://www.jaided.ai/easyocr/modelhub)\n    - Batched image inference for GPU (thanks [@SamSamhuns](https://github.com/SamSamhuns), see [PR](https://github.com/JaidedAI/EasyOCR/pull/458))\n    - Vertical text support (thanks [@interactivetech](https://github.com/interactivetech)). This is for rotated text, not to be confused with vertical Chinese or Japanese text. (see [PR](https://github.com/JaidedAI/EasyOCR/pull/450))\n    - Output in dictionary format (thanks [@A2va](https://github.com/A2va), see [PR](https://github.com/JaidedAI/EasyOCR/pull/441))\n- 30 May 2021 - Version 1.3.2\n    - Faster greedy decoder (thanks [@samayala22](https://github.com/samayala22))\n    - Fix bug when text box's aspect ratio is disproportional (thanks [iQuartic](https://iquartic.com/) for bug report)\n- 20 April 2021 - Version 1.3.1\n    - Add support for PIL image (thanks [@prays](https://github.com/prays))\n    - Add Tajik language (tjk)\n    - Update argument setting for command line\n    - Add `x_ths` and `y_ths` to control merging behavior when `paragraph=True`\n- 21 March 2021 - Version 1.3\n    - Second-generation models: multiple times smaller size, multiple times faster inference, additional characters, comparable accuracy to the first generation models.\n    EasyOCR will choose the latest model by default but you can also specify which model to use by passing `recog_network` argument when creating `Reader` instance.\n    For example, `reader = easyocr.Reader(['en','fr'], recog_network = 'latin_g1')` will use the 1st generation Latin model.\n    - List of all models: [Model hub](https://www.jaided.ai/easyocr/modelhub)\n- 22 February 2021 - Version 1.2.5\n    - Add dynamic quantization for faster CPU inference (it is enabled by default for CPU mode)\n    - More sensible confident score\n- 7 February 2021 - Version 1.2.4\n    - Faster CPU inference speed by using dynamic input shape (recognition rate increases by around 100% for images with a lot of text)\n- 1 February 2021 - Version 1.2.3\n    - Add `setLanguageList` method to `Reader` class. This is a convenient api for changing languages (within the same model) after creating class instance.\n    - Small change on text box merging. (thanks [z-pc](https://github.com/z-pc), see [PR](https://github.com/JaidedAI/EasyOCR/pull/338))\n    - [Basic Demo on website](https://www.jaided.ai/easyocr)\n- 5 January 2021 - Version 1.2.2\n    - Add `optimal_num_chars` to `detect` method. If specified, bounding boxes with estimated number of characters near this value are returned first. (thanks [@adamfrees](https://github.com/adamfrees))\n    - Add `rotation_info` to `readtext` method. Allow EasyOCR to rotate each text box and return the one with the best confident score. Eligible values are 90, 180 and 270. For example, try [90, 180 ,270] for all possible text orientations. (thanks [@mijoo308](https://github.com/mijoo308))\n    - Update [documentation](https://www.jaided.ai/easyocr/documentation).\n- 24 November 2020 - Version 1.2.1\n    - Preparation for user-created models\n- 17 November 2020 - Version 1.2\n    - New language supports for Telugu and Kannada. These are experimental lite recognition models. Their file sizes are only around 7% of other models and they are ~6x faster at inference with CPU.\n- 12 October 2020 - Version 1.1.10\n    - Faster beamsearch decoder (thanks @amitbcp)\n    - Better code structure (thanks @susmith98)\n    - New language supports for Haryanvi, Sanskrit (Devanagari Script) and Manipuri (Bengari script)\n- 31 August 2020 - Version 1.1.9\n    - Add `detect` and `recognize` method for performing text detection and recognition separately\n- 23 August 2020 - Version 1.1.8\n    - 20 new language supports for Bengali, Assamese, Abaza, Adyghe, Kabardian, Avar,\n    Dargwa, Ingush, Chechen, Lak, Lezgian, Tabassaran, Bihari, Maithili, Angika,\n    Bhojpuri, Magahi, Nagpuri, Newari, Goan Konkani\n    - Support RGBA input format\n    - Add `min_size` argument for `readtext`: for filtering out small text box\n- 10 August 2020 - Version 1.1.7\n    - New language support for Tamil\n    - Temporary fix for memory leakage on CPU mode\n- 4 August 2020 - Version 1.1.6\n    - New language support for Russian, Serbian, Belarusian, Bulgarian, Mongolian, Ukranian (Cyrillic Script) and Arabic, Persian(Farsi), Urdu, Uyghur (Arabic Script)\n    - Docker file and Ainize demo (thanks @ghandic and @Wook-2)\n    - Better production friendly with Logger and custom model folder location (By setting ` model_storage_directory` when create `Reader` instance) (thanks @jpotter)\n    - Model files are now downloaded from github's releases\n    - readtext can now accept grayscale image\n- 24 July 2020 - Version 1.1.5\n    - New language support for Hindi, Marathi, Nepali (Devanagari Script)\n    - Automatic word merging into paragraph (Use this feature by setting `readtext`'s parameter `'paragraph' = True`)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1181640625,
          "content": "torch\ntorchvision>=0.5\nopencv-python-headless\nscipy\nnumpy\nPillow\nscikit-image\npython-bidi\nPyYAML\nShapely\npyclipper\nninja\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.0390625,
          "content": "[metadata]\ndescription_file = README.md\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.07421875,
          "content": "\"\"\"\nEnd-to-End Multi-Lingual Optical Character Recognition (OCR) Solution\n\"\"\"\nfrom io import open\nfrom setuptools import setup\n\nwith open('requirements.txt', encoding=\"utf-8-sig\") as f:\n    requirements = f.readlines()\n\ndef readme():\n    with open('README.md', encoding=\"utf-8-sig\") as f:\n        README = f.read()\n    return README\n\nsetup(\n    name='easyocr',\n    packages=['easyocr'],\n    include_package_data=True,\n    version='1.7.2',\n    install_requires=requirements,\n    entry_points={\"console_scripts\": [\"easyocr= easyocr.cli:main\"]},\n    license='Apache License 2.0',\n    description='End-to-End Multi-Lingual Optical Character Recognition (OCR) Solution',\n    long_description=readme(),\n    long_description_content_type=\"text/markdown\",\n    author='Rakpong Kittinaradorn',\n    author_email='r.kittinaradorn@gmail.com',\n    url='https://github.com/jaidedai/easyocr',\n    download_url='https://github.com/jaidedai/easyocr.git',\n    keywords=['ocr optical character recognition deep learning neural network'],\n    classifiers=[\n        'Development Status :: 5 - Production/Stable'\n    ],\n\n)\n"
        },
        {
          "name": "trainer",
          "type": "tree",
          "content": null
        },
        {
          "name": "unit_test",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}