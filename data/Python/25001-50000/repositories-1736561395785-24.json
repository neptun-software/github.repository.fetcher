{
  "metadata": {
    "timestamp": 1736561395785,
    "page": 24,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "RVC-Boss/GPT-SoVITS",
      "stars": 38536,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0625,
          "content": "docs\nlogs\noutput\nreference\nSoVITS_weights\nGPT_weights\nTEMP\n.git\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1650390625,
          "content": ".DS_Store\n.vscode\n__pycache__\n*.pyc\nenv\nruntime\n.idea\noutput\nlogs\nreference\nGPT_weights\nSoVITS_weights\nGPT_weights_v2\nSoVITS_weights_v2\nTEMP\nweight.json\nffmpeg*\nffprobe*"
        },
        {
          "name": "Docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.271484375,
          "content": "# Base CUDA image\nFROM cnstark/pytorch:2.0.1-py3.9.17-cuda11.8.0-ubuntu20.04\n\nLABEL maintainer=\"breakstring@hotmail.com\"\nLABEL version=\"dev-20240209\"\nLABEL description=\"Docker image for GPT-SoVITS\"\n\n\n# Install 3rd party apps\nENV DEBIAN_FRONTEND=noninteractive\nENV TZ=Etc/UTC\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends tzdata ffmpeg libsox-dev parallel aria2 git git-lfs && \\\n    git lfs install && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Copy only requirements.txt initially to leverage Docker cache\nWORKDIR /workspace\nCOPY requirements.txt /workspace/\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Define a build-time argument for image type\nARG IMAGE_TYPE=full\n\n# Conditional logic based on the IMAGE_TYPE argument\n# Always copy the Docker directory, but only use it if IMAGE_TYPE is not \"elite\"\nCOPY ./Docker /workspace/Docker \n# elite 类型的镜像里面不包含额外的模型\nRUN if [ \"$IMAGE_TYPE\" != \"elite\" ]; then \\\n        chmod +x /workspace/Docker/download.sh && \\\n        /workspace/Docker/download.sh && \\\n        python /workspace/Docker/download.py && \\\n        python -m nltk.downloader averaged_perceptron_tagger cmudict; \\\n    fi\n\n\n# Copy the rest of the application\nCOPY . /workspace\n\nEXPOSE 9871 9872 9873 9874 9880\n\nCMD [\"python\", \"webui.py\"]\n"
        },
        {
          "name": "GPT_SoVITS",
          "type": "tree",
          "content": null
        },
        {
          "name": "GPT_SoVITS_Inference.ipynb",
          "type": "blob",
          "size": 5.53515625,
          "content": "{\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0,\n  \"metadata\": {\n    \"colab\": {\n      \"provenance\": []\n    },\n    \"kernelspec\": {\n      \"name\": \"python3\",\n      \"display_name\": \"Python 3\"\n    },\n    \"accelerator\": \"GPU\"\n  },\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"# Credits for bubarino giving me the huggingface import code (感谢 bubarino 给了我 huggingface 导入代码)\"\n      ],\n      \"metadata\": {\n        \"id\": \"himHYZmra7ix\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"e9b7iFV3dm1f\"\n      },\n      \"source\": [\n        \"!git clone https://github.com/RVC-Boss/GPT-SoVITS.git\\n\",\n        \"%cd GPT-SoVITS\\n\",\n        \"!apt-get update && apt-get install -y --no-install-recommends tzdata ffmpeg libsox-dev parallel aria2 git git-lfs && git lfs install\\n\",\n        \"!pip install -r requirements.txt\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# @title Download pretrained models 下载预训练模型\\n\",\n        \"!mkdir -p /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\\n\",\n        \"!mkdir -p /content/GPT-SoVITS/tools/damo_asr/models\\n\",\n        \"!mkdir -p /content/GPT-SoVITS/tools/uvr5\\n\",\n        \"%cd /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\\n\",\n        \"!git clone https://huggingface.co/lj1995/GPT-SoVITS\\n\",\n        \"%cd /content/GPT-SoVITS/tools/damo_asr/models\\n\",\n        \"!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\\n\",\n        \"!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\\n\",\n        \"!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\\n\",\n        \"# @title UVR5 pretrains 安装uvr5模型\\n\",\n        \"%cd /content/GPT-SoVITS/tools/uvr5\\n\",\n        \"!git clone https://huggingface.co/Delik/uvr5_weights\\n\",\n        \"!git config core.sparseCheckout true\\n\",\n        \"!mv /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/\"\n      ],\n      \"metadata\": {\n        \"id\": \"0NgxXg5sjv7z\",\n        \"cellView\": \"form\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"#@title Create folder models 创建文件夹模型\\n\",\n        \"import os\\n\",\n        \"base_directory = \\\"/content/GPT-SoVITS\\\"\\n\",\n        \"folder_names = [\\\"SoVITS_weights\\\", \\\"GPT_weights\\\"]\\n\",\n        \"\\n\",\n        \"for folder_name in folder_names:\\n\",\n        \"  if os.path.exists(os.path.join(base_directory, folder_name)):\\n\",\n        \"    print(f\\\"The folder '{folder_name}' already exists. (文件夹'{folder_name}'已经存在。)\\\")\\n\",\n        \"  else:\\n\",\n        \"    os.makedirs(os.path.join(base_directory, folder_name))\\n\",\n        \"    print(f\\\"The folder '{folder_name}' was created successfully! (文件夹'{folder_name}'已成功创建！)\\\")\\n\",\n        \"\\n\",\n        \"print(\\\"All folders have been created. (所有文件夹均已创建。)\\\")\"\n      ],\n      \"metadata\": {\n        \"cellView\": \"form\",\n        \"id\": \"cPDEH-9czOJF\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"import requests\\n\",\n        \"import zipfile\\n\",\n        \"import shutil\\n\",\n        \"import os\\n\",\n        \"\\n\",\n        \"#@title Import model 导入模型 (HuggingFace)\\n\",\n        \"hf_link = 'https://huggingface.co/modelloosrvcc/Nagisa_Shingetsu_GPT-SoVITS/resolve/main/Nagisa.zip' #@param {type: \\\"string\\\"}\\n\",\n        \"\\n\",\n        \"output_path = '/content/'\\n\",\n        \"\\n\",\n        \"response = requests.get(hf_link)\\n\",\n        \"with open(output_path + 'file.zip', 'wb') as file:\\n\",\n        \"    file.write(response.content)\\n\",\n        \"\\n\",\n        \"with zipfile.ZipFile(output_path + 'file.zip', 'r') as zip_ref:\\n\",\n        \"    zip_ref.extractall(output_path)\\n\",\n        \"\\n\",\n        \"os.remove(output_path + \\\"file.zip\\\")\\n\",\n        \"\\n\",\n        \"source_directory = output_path\\n\",\n        \"SoVITS_destination_directory = '/content/GPT-SoVITS/SoVITS_weights'\\n\",\n        \"GPT_destination_directory = '/content/GPT-SoVITS/GPT_weights'\\n\",\n        \"\\n\",\n        \"for filename in os.listdir(source_directory):\\n\",\n        \"    if filename.endswith(\\\".pth\\\"):\\n\",\n        \"        source_path = os.path.join(source_directory, filename)\\n\",\n        \"        destination_path = os.path.join(SoVITS_destination_directory, filename)\\n\",\n        \"        shutil.move(source_path, destination_path)\\n\",\n        \"\\n\",\n        \"for filename in os.listdir(source_directory):\\n\",\n        \"    if filename.endswith(\\\".ckpt\\\"):\\n\",\n        \"        source_path = os.path.join(source_directory, filename)\\n\",\n        \"        destination_path = os.path.join(GPT_destination_directory, filename)\\n\",\n        \"        shutil.move(source_path, destination_path)\\n\",\n        \"\\n\",\n        \"print(f'Model downloaded. (模型已下载。)')\"\n      ],\n      \"metadata\": {\n        \"cellView\": \"form\",\n        \"id\": \"vbZY-LnM0tzq\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# @title launch WebUI 启动WebUI\\n\",\n        \"!/usr/local/bin/pip install ipykernel\\n\",\n        \"!sed -i '10s/False/True/' /content/GPT-SoVITS/config.py\\n\",\n        \"%cd /content/GPT-SoVITS/\\n\",\n        \"!/usr/local/bin/python  webui.py\"\n      ],\n      \"metadata\": {\n        \"id\": \"4oRGUzkrk8C7\",\n        \"cellView\": \"form\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    }\n  ]\n}"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0400390625,
          "content": "MIT License\n\nCopyright (c) 2024 RVC-Boss\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 13.7666015625,
          "content": "<div align=\"center\">\n\n\n<h1>GPT-SoVITS-WebUI</h1>\nA Powerful Few-shot Voice Conversion and Text-to-Speech WebUI.<br><br>\n\n[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&labelColor=orange)](https://github.com/RVC-Boss/GPT-SoVITS)\n\n<a href=\"https://trendshift.io/repositories/7033\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/7033\" alt=\"RVC-Boss%2FGPT-SoVITS | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n<!-- img src=\"https://counter.seku.su/cmoe?name=gptsovits&theme=r34\" /><br> -->\n\n[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/RVC-Boss/GPT-SoVITS/blob/main/colab_webui.ipynb)\n[![License](https://img.shields.io/badge/LICENSE-MIT-green.svg?style=for-the-badge)](https://github.com/RVC-Boss/GPT-SoVITS/blob/main/LICENSE)\n[![Huggingface](https://img.shields.io/badge/🤗%20-online%20demo-yellow.svg?style=for-the-badge)](https://huggingface.co/spaces/lj1995/GPT-SoVITS-v2)\n[![Discord](https://img.shields.io/discord/1198701940511617164?color=%23738ADB&label=Discord&style=for-the-badge)](https://discord.gg/dnrgs5GHfG)\n\n**English** | [**中文简体**](./docs/cn/README.md) | [**日本語**](./docs/ja/README.md) | [**한국어**](./docs/ko/README.md) | [**Türkçe**](./docs/tr/README.md)\n\n</div>\n\n---\n\n## Features:\n\n1. **Zero-shot TTS:** Input a 5-second vocal sample and experience instant text-to-speech conversion.\n\n2. **Few-shot TTS:** Fine-tune the model with just 1 minute of training data for improved voice similarity and realism.\n\n3. **Cross-lingual Support:** Inference in languages different from the training dataset, currently supporting English, Japanese, Korean, Cantonese and Chinese.\n\n4. **WebUI Tools:** Integrated tools include voice accompaniment separation, automatic training set segmentation, Chinese ASR, and text labeling, assisting beginners in creating training datasets and GPT/SoVITS models.\n\n**Check out our [demo video](https://www.bilibili.com/video/BV12g4y1m7Uw) here!**\n\nUnseen speakers few-shot fine-tuning demo:\n\nhttps://github.com/RVC-Boss/GPT-SoVITS/assets/129054828/05bee1fa-bdd8-4d85-9350-80c060ab47fb\n\n**User guide: [简体中文](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e) | [English](https://rentry.co/GPT-SoVITS-guide#/)**\n\n## Installation\n\nFor users in China, you can [click here](https://www.codewithgpu.com/i/RVC-Boss/GPT-SoVITS/GPT-SoVITS-Official) to use AutoDL Cloud Docker to experience the full functionality online.\n\n### Tested Environments\n\n- Python 3.9, PyTorch 2.0.1, CUDA 11\n- Python 3.10.13, PyTorch 2.1.2, CUDA 12.3\n- Python 3.9, PyTorch 2.2.2, macOS 14.4.1 (Apple silicon)\n- Python 3.9, PyTorch 2.2.2, CPU devices\n\n_Note: numba==0.56.4 requires py<3.11_\n\n### Windows\n\nIf you are a Windows user (tested with win>=10), you can [download the integrated package](https://huggingface.co/lj1995/GPT-SoVITS-windows-package/resolve/main/GPT-SoVITS-beta.7z?download=true) and double-click on _go-webui.bat_ to start GPT-SoVITS-WebUI.\n\n**Users in China can [download the package here](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/dkxgpiy9zb96hob4#KTvnO).**\n\n### Linux\n\n```bash\nconda create -n GPTSoVits python=3.9\nconda activate GPTSoVits\nbash install.sh\n```\n\n### macOS\n\n**Note: The models trained with GPUs on Macs result in significantly lower quality compared to those trained on other devices, so we are temporarily using CPUs instead.**\n\n1. Install Xcode command-line tools by running `xcode-select --install`.\n2. Install FFmpeg by running `brew install ffmpeg`.\n3. Install the program by running the following commands:\n\n```bash\nconda create -n GPTSoVits python=3.9\nconda activate GPTSoVits\npip install -r requirements.txt\n```\n\n### Install Manually\n\n#### Install FFmpeg\n\n##### Conda Users\n\n```bash\nconda install ffmpeg\n```\n\n##### Ubuntu/Debian Users\n\n```bash\nsudo apt install ffmpeg\nsudo apt install libsox-dev\nconda install -c conda-forge 'ffmpeg<7'\n```\n\n##### Windows Users\n\nDownload and place [ffmpeg.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffmpeg.exe) and [ffprobe.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffprobe.exe) in the GPT-SoVITS root.\n\nInstall [Visual Studio 2017](https://aka.ms/vs/17/release/vc_redist.x86.exe) (Korean TTS Only)\n\n##### MacOS Users\n```bash\nbrew install ffmpeg\n```\n\n#### Install Dependences\n\n```bash\npip install -r requirements.txt\n```\n\n### Using Docker\n\n#### docker-compose.yaml configuration\n\n0. Regarding image tags: Due to rapid updates in the codebase and the slow process of packaging and testing images, please check [Docker Hub](https://hub.docker.com/r/breakstring/gpt-sovits) for the currently packaged latest images and select as per your situation, or alternatively, build locally using a Dockerfile according to your own needs.\n1. Environment Variables：\n\n- is_half: Controls half-precision/double-precision. This is typically the cause if the content under the directories 4-cnhubert/5-wav32k is not generated correctly during the \"SSL extracting\" step. Adjust to True or False based on your actual situation.\n\n2. Volumes Configuration，The application's root directory inside the container is set to /workspace. The default docker-compose.yaml lists some practical examples for uploading/downloading content.\n3. shm_size： The default available memory for Docker Desktop on Windows is too small, which can cause abnormal operations. Adjust according to your own situation.\n4. Under the deploy section, GPU-related settings should be adjusted cautiously according to your system and actual circumstances.\n\n#### Running with docker compose\n\n```\ndocker compose -f \"docker-compose.yaml\" up -d\n```\n\n#### Running with docker command\n\nAs above, modify the corresponding parameters based on your actual situation, then run the following command:\n\n```\ndocker run --rm -it --gpus=all --env=is_half=False --volume=G:\\GPT-SoVITS-DockerTest\\output:/workspace/output --volume=G:\\GPT-SoVITS-DockerTest\\logs:/workspace/logs --volume=G:\\GPT-SoVITS-DockerTest\\SoVITS_weights:/workspace/SoVITS_weights --workdir=/workspace -p 9880:9880 -p 9871:9871 -p 9872:9872 -p 9873:9873 -p 9874:9874 --shm-size=\"16G\" -d breakstring/gpt-sovits:xxxxx\n```\n\n## Pretrained Models\n\n**Users in China can [download all these models here](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/dkxgpiy9zb96hob4#nVNhX).**\n\n1. Download pretrained models from [GPT-SoVITS Models](https://huggingface.co/lj1995/GPT-SoVITS) and place them in `GPT_SoVITS/pretrained_models`.\n\n2. Download G2PW models from [G2PWModel_1.1.zip](https://paddlespeech.bj.bcebos.com/Parakeet/released_models/g2p/G2PWModel_1.1.zip), unzip and rename to `G2PWModel`, and then place them in `GPT_SoVITS/text`.(Chinese TTS Only)\n\n3. For UVR5 (Vocals/Accompaniment Separation & Reverberation Removal, additionally), download models from [UVR5 Weights](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/uvr5_weights) and place them in `tools/uvr5/uvr5_weights`.\n\n4. For Chinese ASR (additionally), download models from [Damo ASR Model](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/files), [Damo VAD Model](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/files), and [Damo Punc Model](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/files) and place them in `tools/asr/models`.\n\n5. For English or Japanese ASR (additionally), download models from [Faster Whisper Large V3](https://huggingface.co/Systran/faster-whisper-large-v3) and place them in `tools/asr/models`. Also, [other models](https://huggingface.co/Systran) may have the similar effect with smaller disk footprint. \n\n## Dataset Format\n\nThe TTS annotation .list file format:\n\n```\nvocal_path|speaker_name|language|text\n```\n\nLanguage dictionary:\n\n- 'zh': Chinese\n- 'ja': Japanese\n- 'en': English\n- 'ko': Korean\n- 'yue': Cantonese\n  \nExample:\n\n```\nD:\\GPT-SoVITS\\xxx/xxx.wav|xxx|en|I like playing Genshin.\n```\n\n## Finetune and inference\n\n ### Open WebUI\n\n #### Integrated Package Users\n\n Double-click `go-webui.bat`or use `go-webui.ps1`\n if you want to switch to V1,then double-click`go-webui-v1.bat` or use `go-webui-v1.ps1`\n\n #### Others\n\n ```bash\n python webui.py <language(optional)>\n ```\n\n if you want to switch to V1,then\n\n ```bash\n python webui.py v1 <language(optional)>\n ```\nOr maunally switch version in WebUI\n\n ### Finetune\n\n #### Path Auto-filling is now supported\n\n     1.Fill in the audio path\n\n     2.Slice the audio into small chunks\n\n     3.Denoise(optinal)\n\n     4.ASR\n\n     5.Proofreading ASR transcriptions\n\n     6.Go to the next Tab, then finetune the model\n\n ### Open Inference WebUI\n\n #### Integrated Package Users\n\n Double-click `go-webui-v2.bat` or use `go-webui-v2.ps1` ,then open the inference webui at  `1-GPT-SoVITS-TTS/1C-inference` \n\n #### Others\n\n ```bash\n python GPT_SoVITS/inference_webui.py <language(optional)>\n ```\n OR\n\n ```bash\n python webui.py\n ```\nthen open the inference webui at `1-GPT-SoVITS-TTS/1C-inference`\n\n ## V2 Release Notes\n\nNew Features:\n\n1. Support Korean and Cantonese\n\n2. An optimized text frontend\n\n3. Pre-trained model extended from 2k hours to 5k hours\n\n4. Improved synthesis quality for low-quality reference audio \n\n    [more details](https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90v2%E2%80%90features-(%E6%96%B0%E7%89%B9%E6%80%A7) ) \n\nUse v2 from v1 environment: \n\n1. `pip install -r requirements.txt` to update some packages\n\n2. Clone the latest codes from github.\n\n3. Download v2 pretrained models from [huggingface](https://huggingface.co/lj1995/GPT-SoVITS/tree/main/gsv-v2final-pretrained) and put them into `GPT_SoVITS\\pretrained_models\\gsv-v2final-pretrained`.\n\n    Chinese v2 additional: [G2PWModel_1.1.zip](https://paddlespeech.bj.bcebos.com/Parakeet/released_models/g2p/G2PWModel_1.1.zip)（Download G2PW models,  unzip and rename to `G2PWModel`, and then place them in `GPT_SoVITS/text`.\n     \n## Todo List\n\n- [x] **High Priority:**\n\n  - [x] Localization in Japanese and English.\n  - [x] User guide.\n  - [x] Japanese and English dataset fine tune training.\n\n- [ ] **Features:**\n  - [x] Zero-shot voice conversion (5s) / few-shot voice conversion (1min).\n  - [x] TTS speaking speed control.\n  - [ ] ~~Enhanced TTS emotion control.~~\n  - [ ] Experiment with changing SoVITS token inputs to probability distribution of GPT vocabs (transformer latent).\n  - [x] Improve English and Japanese text frontend.\n  - [ ] Develop tiny and larger-sized TTS models.\n  - [x] Colab scripts.\n  - [ ] Try expand training dataset (2k hours -> 10k hours).\n  - [x] better sovits base model (enhanced audio quality)\n  - [ ] model mix\n\n## (Additional) Method for running from the command line\nUse the command line to open the WebUI for UVR5\n```\npython tools/uvr5/webui.py \"<infer_device>\" <is_half> <webui_port_uvr5>\n```\n<!-- If you can't open a browser, follow the format below for UVR processing,This is using mdxnet for audio processing\n```\npython mdxnet.py --model --input_root --output_vocal --output_ins --agg_level --format --device --is_half_precision \n``` -->\nThis is how the audio segmentation of the dataset is done using the command line\n```\npython audio_slicer.py \\\n    --input_path \"<path_to_original_audio_file_or_directory>\" \\\n    --output_root \"<directory_where_subdivided_audio_clips_will_be_saved>\" \\\n    --threshold <volume_threshold> \\\n    --min_length <minimum_duration_of_each_subclip> \\\n    --min_interval <shortest_time_gap_between_adjacent_subclips> \n    --hop_size <step_size_for_computing_volume_curve>\n```\nThis is how dataset ASR processing is done using the command line(Only Chinese)\n```\npython tools/asr/funasr_asr.py -i <input> -o <output>\n```\nASR processing is performed through Faster_Whisper(ASR marking except Chinese)\n\n(No progress bars, GPU performance may cause time delays)\n```\npython ./tools/asr/fasterwhisper_asr.py -i <input> -o <output> -l <language> -p <precision>\n```\nA custom list save path is enabled\n\n## Credits\n\nSpecial thanks to the following projects and contributors:\n\n### Theoretical Research\n- [ar-vits](https://github.com/innnky/ar-vits)\n- [SoundStorm](https://github.com/yangdongchao/SoundStorm/tree/master/soundstorm/s1/AR)\n- [vits](https://github.com/jaywalnut310/vits)\n- [TransferTTS](https://github.com/hcy71o/TransferTTS/blob/master/models.py#L556)\n- [contentvec](https://github.com/auspicious3000/contentvec/)\n- [hifi-gan](https://github.com/jik876/hifi-gan)\n- [fish-speech](https://github.com/fishaudio/fish-speech/blob/main/tools/llama/generate.py#L41)\n### Pretrained Models\n- [Chinese Speech Pretrain](https://github.com/TencentGameMate/chinese_speech_pretrain)\n- [Chinese-Roberta-WWM-Ext-Large](https://huggingface.co/hfl/chinese-roberta-wwm-ext-large)\n### Text Frontend for Inference\n- [paddlespeech zh_normalization](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/paddlespeech/t2s/frontend/zh_normalization)\n- [LangSegment](https://github.com/juntaosun/LangSegment)\n- [g2pW](https://github.com/GitYCC/g2pW)\n- [pypinyin-g2pW](https://github.com/mozillazg/pypinyin-g2pW)\n- [paddlespeech g2pw](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/paddlespeech/t2s/frontend/g2pw)\n### WebUI Tools\n- [ultimatevocalremovergui](https://github.com/Anjok07/ultimatevocalremovergui)\n- [audio-slicer](https://github.com/openvpi/audio-slicer)\n- [SubFix](https://github.com/cronrpc/SubFix)\n- [FFmpeg](https://github.com/FFmpeg/FFmpeg)\n- [gradio](https://github.com/gradio-app/gradio)\n- [faster-whisper](https://github.com/SYSTRAN/faster-whisper)\n- [FunASR](https://github.com/alibaba-damo-academy/FunASR)\n\nThankful to @Naozumi520 for providing the Cantonese training set and for the guidance on Cantonese-related knowledge.\n\n## Thanks to all contributors for their efforts\n\n<a href=\"https://github.com/RVC-Boss/GPT-SoVITS/graphs/contributors\" target=\"_blank\">\n  <img src=\"https://contrib.rocks/image?repo=RVC-Boss/GPT-SoVITS\" />\n</a>\n"
        },
        {
          "name": "api.py",
          "type": "blob",
          "size": 33.1171875,
          "content": "\"\"\"\r\n# api.py usage\r\n\r\n` python api.py -dr \"123.wav\" -dt \"一二三。\" -dl \"zh\" `\r\n\r\n## 执行参数:\r\n\r\n`-s` - `SoVITS模型路径, 可在 config.py 中指定`\r\n`-g` - `GPT模型路径, 可在 config.py 中指定`\r\n\r\n调用请求缺少参考音频时使用\r\n`-dr` - `默认参考音频路径`\r\n`-dt` - `默认参考音频文本`\r\n`-dl` - `默认参考音频语种, \"中文\",\"英文\",\"日文\",\"韩文\",\"粤语,\"zh\",\"en\",\"ja\",\"ko\",\"yue\"`\r\n\r\n`-d` - `推理设备, \"cuda\",\"cpu\"`\r\n`-a` - `绑定地址, 默认\"127.0.0.1\"`\r\n`-p` - `绑定端口, 默认9880, 可在 config.py 中指定`\r\n`-fp` - `覆盖 config.py 使用全精度`\r\n`-hp` - `覆盖 config.py 使用半精度`\r\n`-sm` - `流式返回模式, 默认不启用, \"close\",\"c\", \"normal\",\"n\", \"keepalive\",\"k\"`\r\n·-mt` - `返回的音频编码格式, 流式默认ogg, 非流式默认wav, \"wav\", \"ogg\", \"aac\"`\r\n·-st` - `返回的音频数据类型, 默认int16, \"int16\", \"int32\"`\r\n·-cp` - `文本切分符号设定, 默认为空, 以\",.，。\"字符串的方式传入`\r\n\r\n`-hb` - `cnhubert路径`\r\n`-b` - `bert路径`\r\n\r\n## 调用:\r\n\r\n### 推理\r\n\r\nendpoint: `/`\r\n\r\n使用执行参数指定的参考音频:\r\nGET:\r\n    `http://127.0.0.1:9880?text=先帝创业未半而中道崩殂，今天下三分，益州疲弊，此诚危急存亡之秋也。&text_language=zh`\r\nPOST:\r\n```json\r\n{\r\n    \"text\": \"先帝创业未半而中道崩殂，今天下三分，益州疲弊，此诚危急存亡之秋也。\",\r\n    \"text_language\": \"zh\"\r\n}\r\n```\r\n\r\n使用执行参数指定的参考音频并设定分割符号:\r\nGET:\r\n    `http://127.0.0.1:9880?text=先帝创业未半而中道崩殂，今天下三分，益州疲弊，此诚危急存亡之秋也。&text_language=zh&cut_punc=，。`\r\nPOST:\r\n```json\r\n{\r\n    \"text\": \"先帝创业未半而中道崩殂，今天下三分，益州疲弊，此诚危急存亡之秋也。\",\r\n    \"text_language\": \"zh\",\r\n    \"cut_punc\": \"，。\",\r\n}\r\n```\r\n\r\n手动指定当次推理所使用的参考音频:\r\nGET:\r\n    `http://127.0.0.1:9880?refer_wav_path=123.wav&prompt_text=一二三。&prompt_language=zh&text=先帝创业未半而中道崩殂，今天下三分，益州疲弊，此诚危急存亡之秋也。&text_language=zh`\r\nPOST:\r\n```json\r\n{\r\n    \"refer_wav_path\": \"123.wav\",\r\n    \"prompt_text\": \"一二三。\",\r\n    \"prompt_language\": \"zh\",\r\n    \"text\": \"先帝创业未半而中道崩殂，今天下三分，益州疲弊，此诚危急存亡之秋也。\",\r\n    \"text_language\": \"zh\"\r\n}\r\n```\r\n\r\nRESP:\r\n成功: 直接返回 wav 音频流， http code 200\r\n失败: 返回包含错误信息的 json, http code 400\r\n\r\n手动指定当次推理所使用的参考音频，并提供参数:\r\nGET:\r\n    `http://127.0.0.1:9880?refer_wav_path=123.wav&prompt_text=一二三。&prompt_language=zh&text=先帝创业未半而中道崩殂，今天下三分，益州疲弊，此诚危急存亡之秋也。&text_language=zh&top_k=20&top_p=0.6&temperature=0.6&speed=1&inp_refs=\"456.wav\"&inp_refs=\"789.wav\"`\r\nPOST:\r\n```json\r\n{\r\n    \"refer_wav_path\": \"123.wav\",\r\n    \"prompt_text\": \"一二三。\",\r\n    \"prompt_language\": \"zh\",\r\n    \"text\": \"先帝创业未半而中道崩殂，今天下三分，益州疲弊，此诚危急存亡之秋也。\",\r\n    \"text_language\": \"zh\",\r\n    \"top_k\": 20,\r\n    \"top_p\": 0.6,\r\n    \"temperature\": 0.6,\r\n    \"speed\": 1,\r\n    \"inp_refs\": [\"456.wav\",\"789.wav\"]\r\n}\r\n```\r\n\r\nRESP:\r\n成功: 直接返回 wav 音频流， http code 200\r\n失败: 返回包含错误信息的 json, http code 400\r\n\r\n\r\n### 更换默认参考音频\r\n\r\nendpoint: `/change_refer`\r\n\r\nkey与推理端一样\r\n\r\nGET:\r\n    `http://127.0.0.1:9880/change_refer?refer_wav_path=123.wav&prompt_text=一二三。&prompt_language=zh`\r\nPOST:\r\n```json\r\n{\r\n    \"refer_wav_path\": \"123.wav\",\r\n    \"prompt_text\": \"一二三。\",\r\n    \"prompt_language\": \"zh\"\r\n}\r\n```\r\n\r\nRESP:\r\n成功: json, http code 200\r\n失败: json, 400\r\n\r\n\r\n### 命令控制\r\n\r\nendpoint: `/control`\r\n\r\ncommand:\r\n\"restart\": 重新运行\r\n\"exit\": 结束运行\r\n\r\nGET:\r\n    `http://127.0.0.1:9880/control?command=restart`\r\nPOST:\r\n```json\r\n{\r\n    \"command\": \"restart\"\r\n}\r\n```\r\n\r\nRESP: 无\r\n\r\n\"\"\"\r\n\r\n\r\nimport argparse\r\nimport os,re\r\nimport sys\r\n\r\nnow_dir = os.getcwd()\r\nsys.path.append(now_dir)\r\nsys.path.append(\"%s/GPT_SoVITS\" % (now_dir))\r\n\r\nimport signal\r\nimport LangSegment\r\nfrom time import time as ttime\r\nimport torch\r\nimport librosa\r\nimport soundfile as sf\r\nfrom fastapi import FastAPI, Request, Query, HTTPException\r\nfrom fastapi.responses import StreamingResponse, JSONResponse\r\nimport uvicorn\r\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\r\nimport numpy as np\r\nfrom feature_extractor import cnhubert\r\nfrom io import BytesIO\r\nfrom module.models import SynthesizerTrn\r\nfrom AR.models.t2s_lightning_module import Text2SemanticLightningModule\r\nfrom text import cleaned_text_to_sequence\r\nfrom text.cleaner import clean_text\r\nfrom module.mel_processing import spectrogram_torch\r\nfrom tools.my_utils import load_audio\r\nimport config as global_config\r\nimport logging\r\nimport subprocess\r\n\r\n\r\nclass DefaultRefer:\r\n    def __init__(self, path, text, language):\r\n        self.path = args.default_refer_path\r\n        self.text = args.default_refer_text\r\n        self.language = args.default_refer_language\r\n\r\n    def is_ready(self) -> bool:\r\n        return is_full(self.path, self.text, self.language)\r\n\r\n\r\ndef is_empty(*items):  # 任意一项不为空返回False\r\n    for item in items:\r\n        if item is not None and item != \"\":\r\n            return False\r\n    return True\r\n\r\n\r\ndef is_full(*items):  # 任意一项为空返回False\r\n    for item in items:\r\n        if item is None or item == \"\":\r\n            return False\r\n    return True\r\n\r\n\r\nclass Speaker:\r\n    def __init__(self, name, gpt, sovits, phones = None, bert = None, prompt = None):\r\n        self.name = name\r\n        self.sovits = sovits\r\n        self.gpt = gpt\r\n        self.phones = phones\r\n        self.bert = bert\r\n        self.prompt = prompt\r\n        \r\nspeaker_list = {}\r\n\r\n\r\nclass Sovits:\r\n    def __init__(self, vq_model, hps):\r\n        self.vq_model = vq_model\r\n        self.hps = hps\r\n\r\ndef get_sovits_weights(sovits_path):\r\n    dict_s2 = torch.load(sovits_path, map_location=\"cpu\")\r\n    hps = dict_s2[\"config\"]\r\n    hps = DictToAttrRecursive(hps)\r\n    hps.model.semantic_frame_rate = \"25hz\"\r\n    if dict_s2['weight']['enc_p.text_embedding.weight'].shape[0] == 322:\r\n        hps.model.version = \"v1\"\r\n    else:\r\n        hps.model.version = \"v2\"\r\n    logger.info(f\"模型版本: {hps.model.version}\")\r\n    model_params_dict = vars(hps.model)\r\n    vq_model = SynthesizerTrn(\r\n        hps.data.filter_length // 2 + 1,\r\n        hps.train.segment_size // hps.data.hop_length,\r\n        n_speakers=hps.data.n_speakers,\r\n        **model_params_dict\r\n    )\r\n    if (\"pretrained\" not in sovits_path):\r\n        del vq_model.enc_q\r\n    if is_half == True:\r\n        vq_model = vq_model.half().to(device)\r\n    else:\r\n        vq_model = vq_model.to(device)\r\n    vq_model.eval()\r\n    vq_model.load_state_dict(dict_s2[\"weight\"], strict=False)\r\n\r\n    sovits = Sovits(vq_model, hps)\r\n    return sovits\r\n\r\nclass Gpt:\r\n    def __init__(self, max_sec, t2s_model):\r\n        self.max_sec = max_sec\r\n        self.t2s_model = t2s_model\r\n\r\nglobal hz\r\nhz = 50\r\ndef get_gpt_weights(gpt_path):\r\n    dict_s1 = torch.load(gpt_path, map_location=\"cpu\")\r\n    config = dict_s1[\"config\"]\r\n    max_sec = config[\"data\"][\"max_sec\"]\r\n    t2s_model = Text2SemanticLightningModule(config, \"****\", is_train=False)\r\n    t2s_model.load_state_dict(dict_s1[\"weight\"])\r\n    if is_half == True:\r\n        t2s_model = t2s_model.half()\r\n    t2s_model = t2s_model.to(device)\r\n    t2s_model.eval()\r\n    total = sum([param.nelement() for param in t2s_model.parameters()])\r\n    logger.info(\"Number of parameter: %.2fM\" % (total / 1e6))\r\n\r\n    gpt = Gpt(max_sec, t2s_model)\r\n    return gpt\r\n\r\ndef change_gpt_sovits_weights(gpt_path,sovits_path):\r\n    try:\r\n        gpt = get_gpt_weights(gpt_path)\r\n        sovits = get_sovits_weights(sovits_path)\r\n    except Exception as e:\r\n        return JSONResponse({\"code\": 400, \"message\": str(e)}, status_code=400)\r\n\r\n    speaker_list[\"default\"] = Speaker(name=\"default\", gpt=gpt, sovits=sovits)\r\n    return JSONResponse({\"code\": 0, \"message\": \"Success\"}, status_code=200)\r\n\r\n\r\ndef get_bert_feature(text, word2ph):\r\n    with torch.no_grad():\r\n        inputs = tokenizer(text, return_tensors=\"pt\")\r\n        for i in inputs:\r\n            inputs[i] = inputs[i].to(device)  #####输入是long不用管精度问题，精度随bert_model\r\n        res = bert_model(**inputs, output_hidden_states=True)\r\n        res = torch.cat(res[\"hidden_states\"][-3:-2], -1)[0].cpu()[1:-1]\r\n    assert len(word2ph) == len(text)\r\n    phone_level_feature = []\r\n    for i in range(len(word2ph)):\r\n        repeat_feature = res[i].repeat(word2ph[i], 1)\r\n        phone_level_feature.append(repeat_feature)\r\n    phone_level_feature = torch.cat(phone_level_feature, dim=0)\r\n    # if(is_half==True):phone_level_feature=phone_level_feature.half()\r\n    return phone_level_feature.T\r\n\r\n\r\ndef clean_text_inf(text, language, version):\r\n    phones, word2ph, norm_text = clean_text(text, language, version)\r\n    phones = cleaned_text_to_sequence(phones, version)\r\n    return phones, word2ph, norm_text\r\n\r\n\r\ndef get_bert_inf(phones, word2ph, norm_text, language):\r\n    language=language.replace(\"all_\",\"\")\r\n    if language == \"zh\":\r\n        bert = get_bert_feature(norm_text, word2ph).to(device)#.to(dtype)\r\n    else:\r\n        bert = torch.zeros(\r\n            (1024, len(phones)),\r\n            dtype=torch.float16 if is_half == True else torch.float32,\r\n        ).to(device)\r\n\r\n    return bert\r\n\r\nfrom text import chinese\r\ndef get_phones_and_bert(text,language,version,final=False):\r\n    if language in {\"en\", \"all_zh\", \"all_ja\", \"all_ko\", \"all_yue\"}:\r\n        language = language.replace(\"all_\",\"\")\r\n        if language == \"en\":\r\n            LangSegment.setfilters([\"en\"])\r\n            formattext = \" \".join(tmp[\"text\"] for tmp in LangSegment.getTexts(text))\r\n        else:\r\n            # 因无法区别中日韩文汉字,以用户输入为准\r\n            formattext = text\r\n        while \"  \" in formattext:\r\n            formattext = formattext.replace(\"  \", \" \")\r\n        if language == \"zh\":\r\n            if re.search(r'[A-Za-z]', formattext):\r\n                formattext = re.sub(r'[a-z]', lambda x: x.group(0).upper(), formattext)\r\n                formattext = chinese.mix_text_normalize(formattext)\r\n                return get_phones_and_bert(formattext,\"zh\",version)\r\n            else:\r\n                phones, word2ph, norm_text = clean_text_inf(formattext, language, version)\r\n                bert = get_bert_feature(norm_text, word2ph).to(device)\r\n        elif language == \"yue\" and re.search(r'[A-Za-z]', formattext):\r\n                formattext = re.sub(r'[a-z]', lambda x: x.group(0).upper(), formattext)\r\n                formattext = chinese.mix_text_normalize(formattext)\r\n                return get_phones_and_bert(formattext,\"yue\",version)\r\n        else:\r\n            phones, word2ph, norm_text = clean_text_inf(formattext, language, version)\r\n            bert = torch.zeros(\r\n                (1024, len(phones)),\r\n                dtype=torch.float16 if is_half == True else torch.float32,\r\n            ).to(device)\r\n    elif language in {\"zh\", \"ja\", \"ko\", \"yue\", \"auto\", \"auto_yue\"}:\r\n        textlist=[]\r\n        langlist=[]\r\n        LangSegment.setfilters([\"zh\",\"ja\",\"en\",\"ko\"])\r\n        if language == \"auto\":\r\n            for tmp in LangSegment.getTexts(text):\r\n                langlist.append(tmp[\"lang\"])\r\n                textlist.append(tmp[\"text\"])\r\n        elif language == \"auto_yue\":\r\n            for tmp in LangSegment.getTexts(text):\r\n                if tmp[\"lang\"] == \"zh\":\r\n                    tmp[\"lang\"] = \"yue\"\r\n                langlist.append(tmp[\"lang\"])\r\n                textlist.append(tmp[\"text\"])\r\n        else:\r\n            for tmp in LangSegment.getTexts(text):\r\n                if tmp[\"lang\"] == \"en\":\r\n                    langlist.append(tmp[\"lang\"])\r\n                else:\r\n                    # 因无法区别中日韩文汉字,以用户输入为准\r\n                    langlist.append(language)\r\n                textlist.append(tmp[\"text\"])\r\n        phones_list = []\r\n        bert_list = []\r\n        norm_text_list = []\r\n        for i in range(len(textlist)):\r\n            lang = langlist[i]\r\n            phones, word2ph, norm_text = clean_text_inf(textlist[i], lang, version)\r\n            bert = get_bert_inf(phones, word2ph, norm_text, lang)\r\n            phones_list.append(phones)\r\n            norm_text_list.append(norm_text)\r\n            bert_list.append(bert)\r\n        bert = torch.cat(bert_list, dim=1)\r\n        phones = sum(phones_list, [])\r\n        norm_text = ''.join(norm_text_list)\r\n\r\n    if not final and len(phones) < 6:\r\n        return get_phones_and_bert(\".\" + text,language,version,final=True)\r\n\r\n    return phones,bert.to(torch.float16 if is_half == True else torch.float32),norm_text\r\n\r\n\r\nclass DictToAttrRecursive(dict):\r\n    def __init__(self, input_dict):\r\n        super().__init__(input_dict)\r\n        for key, value in input_dict.items():\r\n            if isinstance(value, dict):\r\n                value = DictToAttrRecursive(value)\r\n            self[key] = value\r\n            setattr(self, key, value)\r\n\r\n    def __getattr__(self, item):\r\n        try:\r\n            return self[item]\r\n        except KeyError:\r\n            raise AttributeError(f\"Attribute {item} not found\")\r\n\r\n    def __setattr__(self, key, value):\r\n        if isinstance(value, dict):\r\n            value = DictToAttrRecursive(value)\r\n        super(DictToAttrRecursive, self).__setitem__(key, value)\r\n        super().__setattr__(key, value)\r\n\r\n    def __delattr__(self, item):\r\n        try:\r\n            del self[item]\r\n        except KeyError:\r\n            raise AttributeError(f\"Attribute {item} not found\")\r\n\r\n\r\ndef get_spepc(hps, filename):\r\n    audio,_ = librosa.load(filename, int(hps.data.sampling_rate))\r\n    audio = torch.FloatTensor(audio)\r\n    maxx=audio.abs().max()\r\n    if(maxx>1):\r\n        audio/=min(2,maxx)\r\n    audio_norm = audio\r\n    audio_norm = audio_norm.unsqueeze(0)\r\n    spec = spectrogram_torch(audio_norm, hps.data.filter_length, hps.data.sampling_rate, hps.data.hop_length,\r\n                             hps.data.win_length, center=False)\r\n    return spec\r\n\r\n\r\ndef pack_audio(audio_bytes, data, rate):\r\n    if media_type == \"ogg\":\r\n        audio_bytes = pack_ogg(audio_bytes, data, rate)\r\n    elif media_type == \"aac\":\r\n        audio_bytes = pack_aac(audio_bytes, data, rate)\r\n    else:\r\n        # wav无法流式, 先暂存raw\r\n        audio_bytes = pack_raw(audio_bytes, data, rate)\r\n\r\n    return audio_bytes\r\n\r\n\r\ndef pack_ogg(audio_bytes, data, rate):\r\n    # Author: AkagawaTsurunaki\r\n    # Issue:\r\n    #   Stack overflow probabilistically occurs\r\n    #   when the function `sf_writef_short` of `libsndfile_64bit.dll` is called\r\n    #   using the Python library `soundfile`\r\n    # Note:\r\n    #   This is an issue related to `libsndfile`, not this project itself.\r\n    #   It happens when you generate a large audio tensor (about 499804 frames in my PC)\r\n    #   and try to convert it to an ogg file.\r\n    # Related:\r\n    #   https://github.com/RVC-Boss/GPT-SoVITS/issues/1199\r\n    #   https://github.com/libsndfile/libsndfile/issues/1023\r\n    #   https://github.com/bastibe/python-soundfile/issues/396\r\n    # Suggestion:\r\n    #   Or split the whole audio data into smaller audio segment to avoid stack overflow?\r\n\r\n    def handle_pack_ogg():\r\n        with sf.SoundFile(audio_bytes, mode='w', samplerate=rate, channels=1, format='ogg') as audio_file:\r\n            audio_file.write(data)\r\n\r\n    import threading\r\n    # See: https://docs.python.org/3/library/threading.html\r\n    # The stack size of this thread is at least 32768\r\n    # If stack overflow error still occurs, just modify the `stack_size`.\r\n    # stack_size = n * 4096, where n should be a positive integer.\r\n    # Here we chose n = 4096.\r\n    stack_size = 4096 * 4096\r\n    try:\r\n        threading.stack_size(stack_size)\r\n        pack_ogg_thread = threading.Thread(target=handle_pack_ogg)\r\n        pack_ogg_thread.start()\r\n        pack_ogg_thread.join()\r\n    except RuntimeError as e:\r\n        # If changing the thread stack size is unsupported, a RuntimeError is raised.\r\n        print(\"RuntimeError: {}\".format(e))\r\n        print(\"Changing the thread stack size is unsupported.\")\r\n    except ValueError as e:\r\n        # If the specified stack size is invalid, a ValueError is raised and the stack size is unmodified.\r\n        print(\"ValueError: {}\".format(e))\r\n        print(\"The specified stack size is invalid.\")\r\n\r\n    return audio_bytes\r\n\r\n\r\ndef pack_raw(audio_bytes, data, rate):\r\n    audio_bytes.write(data.tobytes())\r\n\r\n    return audio_bytes\r\n\r\n\r\ndef pack_wav(audio_bytes, rate):\r\n    if is_int32:\r\n        data = np.frombuffer(audio_bytes.getvalue(),dtype=np.int32)\r\n        wav_bytes = BytesIO()\r\n        sf.write(wav_bytes, data, rate, format='WAV', subtype='PCM_32')\r\n    else:\r\n        data = np.frombuffer(audio_bytes.getvalue(),dtype=np.int16)\r\n        wav_bytes = BytesIO()\r\n        sf.write(wav_bytes, data, rate, format='WAV')\r\n    return wav_bytes\r\n\r\n\r\ndef pack_aac(audio_bytes, data, rate):\r\n    if is_int32:\r\n        pcm = 's32le'\r\n        bit_rate = '256k'\r\n    else:\r\n        pcm = 's16le'\r\n        bit_rate = '128k'\r\n    process = subprocess.Popen([\r\n        'ffmpeg',\r\n        '-f', pcm,  # 输入16位有符号小端整数PCM\r\n        '-ar', str(rate),  # 设置采样率\r\n        '-ac', '1',  # 单声道\r\n        '-i', 'pipe:0',  # 从管道读取输入\r\n        '-c:a', 'aac',  # 音频编码器为AAC\r\n        '-b:a', bit_rate,  # 比特率\r\n        '-vn',  # 不包含视频\r\n        '-f', 'adts',  # 输出AAC数据流格式\r\n        'pipe:1'  # 将输出写入管道\r\n    ], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\r\n    out, _ = process.communicate(input=data.tobytes())\r\n    audio_bytes.write(out)\r\n\r\n    return audio_bytes\r\n\r\n\r\ndef read_clean_buffer(audio_bytes):\r\n    audio_chunk = audio_bytes.getvalue()\r\n    audio_bytes.truncate(0)\r\n    audio_bytes.seek(0)\r\n\r\n    return audio_bytes, audio_chunk\r\n\r\n\r\ndef cut_text(text, punc):\r\n    punc_list = [p for p in punc if p in {\",\", \".\", \";\", \"?\", \"!\", \"、\", \"，\", \"。\", \"？\", \"！\", \"；\", \"：\", \"…\"}]\r\n    if len(punc_list) > 0:\r\n        punds = r\"[\" + \"\".join(punc_list) + r\"]\"\r\n        text = text.strip(\"\\n\")\r\n        items = re.split(f\"({punds})\", text)\r\n        mergeitems = [\"\".join(group) for group in zip(items[::2], items[1::2])]\r\n        # 在句子不存在符号或句尾无符号的时候保证文本完整\r\n        if len(items)%2 == 1:\r\n            mergeitems.append(items[-1])\r\n        text = \"\\n\".join(mergeitems)\r\n\r\n    while \"\\n\\n\" in text:\r\n        text = text.replace(\"\\n\\n\", \"\\n\")\r\n\r\n    return text\r\n\r\n\r\ndef only_punc(text):\r\n    return not any(t.isalnum() or t.isalpha() for t in text)\r\n\r\n\r\nsplits = {\"，\", \"。\", \"？\", \"！\", \",\", \".\", \"?\", \"!\", \"~\", \":\", \"：\", \"—\", \"…\", }\r\ndef get_tts_wav(ref_wav_path, prompt_text, prompt_language, text, text_language, top_k= 15, top_p = 0.6, temperature = 0.6, speed = 1, inp_refs = None, spk = \"default\"):\r\n    infer_sovits = speaker_list[spk].sovits\r\n    vq_model = infer_sovits.vq_model\r\n    hps = infer_sovits.hps\r\n\r\n    infer_gpt = speaker_list[spk].gpt\r\n    t2s_model = infer_gpt.t2s_model\r\n    max_sec = infer_gpt.max_sec\r\n\r\n    t0 = ttime()\r\n    prompt_text = prompt_text.strip(\"\\n\")\r\n    if (prompt_text[-1] not in splits): prompt_text += \"。\" if prompt_language != \"en\" else \".\"\r\n    prompt_language, text = prompt_language, text.strip(\"\\n\")\r\n    dtype = torch.float16 if is_half == True else torch.float32\r\n    zero_wav = np.zeros(int(hps.data.sampling_rate * 0.3), dtype=np.float16 if is_half == True else np.float32)\r\n    with torch.no_grad():\r\n        wav16k, sr = librosa.load(ref_wav_path, sr=16000)\r\n        wav16k = torch.from_numpy(wav16k)\r\n        zero_wav_torch = torch.from_numpy(zero_wav)\r\n        if (is_half == True):\r\n            wav16k = wav16k.half().to(device)\r\n            zero_wav_torch = zero_wav_torch.half().to(device)\r\n        else:\r\n            wav16k = wav16k.to(device)\r\n            zero_wav_torch = zero_wav_torch.to(device)\r\n        wav16k = torch.cat([wav16k, zero_wav_torch])\r\n        ssl_content = ssl_model.model(wav16k.unsqueeze(0))[\"last_hidden_state\"].transpose(1, 2)  # .float()\r\n        codes = vq_model.extract_latent(ssl_content)\r\n        prompt_semantic = codes[0, 0]\r\n        prompt = prompt_semantic.unsqueeze(0).to(device)\r\n\r\n        refers=[]\r\n        if(inp_refs):\r\n            for path in inp_refs:\r\n                try:\r\n                    refer = get_spepc(hps, path).to(dtype).to(device)\r\n                    refers.append(refer)\r\n                except Exception as e:\r\n                    logger.error(e)\r\n        if(len(refers)==0):\r\n            refers = [get_spepc(hps, ref_wav_path).to(dtype).to(device)]\r\n\r\n    t1 = ttime()\r\n    version = vq_model.version\r\n    os.environ['version'] = version\r\n    prompt_language = dict_language[prompt_language.lower()]\r\n    text_language = dict_language[text_language.lower()]\r\n    phones1, bert1, norm_text1 = get_phones_and_bert(prompt_text, prompt_language, version)\r\n    texts = text.split(\"\\n\")\r\n    audio_bytes = BytesIO()\r\n\r\n    for text in texts:\r\n        # 简单防止纯符号引发参考音频泄露\r\n        if only_punc(text):\r\n            continue\r\n\r\n        audio_opt = []\r\n        if (text[-1] not in splits): text += \"。\" if text_language != \"en\" else \".\"\r\n        phones2, bert2, norm_text2 = get_phones_and_bert(text, text_language, version)\r\n        bert = torch.cat([bert1, bert2], 1)\r\n\r\n        all_phoneme_ids = torch.LongTensor(phones1 + phones2).to(device).unsqueeze(0)\r\n        bert = bert.to(device).unsqueeze(0)\r\n        all_phoneme_len = torch.tensor([all_phoneme_ids.shape[-1]]).to(device)\r\n        t2 = ttime()\r\n        with torch.no_grad():\r\n            pred_semantic, idx = t2s_model.model.infer_panel(\r\n                all_phoneme_ids,\r\n                all_phoneme_len,\r\n                prompt,\r\n                bert,\r\n                # prompt_phone_len=ph_offset,\r\n                top_k = top_k,\r\n                top_p = top_p,\r\n                temperature = temperature,\r\n                early_stop_num=hz * max_sec)\r\n            pred_semantic = pred_semantic[:, -idx:].unsqueeze(0)\r\n        t3 = ttime()\r\n        audio = \\\r\n            vq_model.decode(pred_semantic, torch.LongTensor(phones2).to(device).unsqueeze(0),\r\n                            refers,speed=speed).detach().cpu().numpy()[\r\n                0, 0]  ###试试重建不带上prompt部分\r\n        max_audio=np.abs(audio).max()\r\n        if max_audio>1:\r\n            audio/=max_audio\r\n        audio_opt.append(audio)\r\n        audio_opt.append(zero_wav)\r\n        t4 = ttime()\r\n        if is_int32:\r\n            audio_bytes = pack_audio(audio_bytes,(np.concatenate(audio_opt, 0) * 2147483647).astype(np.int32),hps.data.sampling_rate)\r\n        else:\r\n            audio_bytes = pack_audio(audio_bytes,(np.concatenate(audio_opt, 0) * 32768).astype(np.int16),hps.data.sampling_rate)\r\n    # logger.info(\"%.3f\\t%.3f\\t%.3f\\t%.3f\" % (t1 - t0, t2 - t1, t3 - t2, t4 - t3))\r\n        if stream_mode == \"normal\":\r\n            audio_bytes, audio_chunk = read_clean_buffer(audio_bytes)\r\n            yield audio_chunk\r\n    \r\n    if not stream_mode == \"normal\": \r\n        if media_type == \"wav\":\r\n            audio_bytes = pack_wav(audio_bytes,hps.data.sampling_rate)\r\n        yield audio_bytes.getvalue()\r\n\r\n\r\n\r\ndef handle_control(command):\r\n    if command == \"restart\":\r\n        os.execl(g_config.python_exec, g_config.python_exec, *sys.argv)\r\n    elif command == \"exit\":\r\n        os.kill(os.getpid(), signal.SIGTERM)\r\n        exit(0)\r\n\r\n\r\ndef handle_change(path, text, language):\r\n    if is_empty(path, text, language):\r\n        return JSONResponse({\"code\": 400, \"message\": '缺少任意一项以下参数: \"path\", \"text\", \"language\"'}, status_code=400)\r\n\r\n    if path != \"\" or path is not None:\r\n        default_refer.path = path\r\n    if text != \"\" or text is not None:\r\n        default_refer.text = text\r\n    if language != \"\" or language is not None:\r\n        default_refer.language = language\r\n\r\n    logger.info(f\"当前默认参考音频路径: {default_refer.path}\")\r\n    logger.info(f\"当前默认参考音频文本: {default_refer.text}\")\r\n    logger.info(f\"当前默认参考音频语种: {default_refer.language}\")\r\n    logger.info(f\"is_ready: {default_refer.is_ready()}\")\r\n\r\n\r\n    return JSONResponse({\"code\": 0, \"message\": \"Success\"}, status_code=200)\r\n\r\n\r\ndef handle(refer_wav_path, prompt_text, prompt_language, text, text_language, cut_punc, top_k, top_p, temperature, speed, inp_refs):\r\n    if (\r\n            refer_wav_path == \"\" or refer_wav_path is None\r\n            or prompt_text == \"\" or prompt_text is None\r\n            or prompt_language == \"\" or prompt_language is None\r\n    ):\r\n        refer_wav_path, prompt_text, prompt_language = (\r\n            default_refer.path,\r\n            default_refer.text,\r\n            default_refer.language,\r\n        )\r\n        if not default_refer.is_ready():\r\n            return JSONResponse({\"code\": 400, \"message\": \"未指定参考音频且接口无预设\"}, status_code=400)\r\n\r\n    if cut_punc == None:\r\n        text = cut_text(text,default_cut_punc)\r\n    else:\r\n        text = cut_text(text,cut_punc)\r\n\r\n    return StreamingResponse(get_tts_wav(refer_wav_path, prompt_text, prompt_language, text, text_language, top_k, top_p, temperature, speed, inp_refs), media_type=\"audio/\"+media_type)\r\n\r\n\r\n\r\n\r\n# --------------------------------\r\n# 初始化部分\r\n# --------------------------------\r\ndict_language = {\r\n    \"中文\": \"all_zh\",\r\n    \"粤语\": \"all_yue\",\r\n    \"英文\": \"en\",\r\n    \"日文\": \"all_ja\",\r\n    \"韩文\": \"all_ko\",\r\n    \"中英混合\": \"zh\",\r\n    \"粤英混合\": \"yue\",\r\n    \"日英混合\": \"ja\",\r\n    \"韩英混合\": \"ko\",\r\n    \"多语种混合\": \"auto\",    #多语种启动切分识别语种\r\n    \"多语种混合(粤语)\": \"auto_yue\",\r\n    \"all_zh\": \"all_zh\",\r\n    \"all_yue\": \"all_yue\",\r\n    \"en\": \"en\",\r\n    \"all_ja\": \"all_ja\",\r\n    \"all_ko\": \"all_ko\",\r\n    \"zh\": \"zh\",\r\n    \"yue\": \"yue\",\r\n    \"ja\": \"ja\",\r\n    \"ko\": \"ko\",\r\n    \"auto\": \"auto\",\r\n    \"auto_yue\": \"auto_yue\",\r\n}\r\n\r\n# logger\r\nlogging.config.dictConfig(uvicorn.config.LOGGING_CONFIG)\r\nlogger = logging.getLogger('uvicorn')\r\n\r\n# 获取配置\r\ng_config = global_config.Config()\r\n\r\n# 获取参数\r\nparser = argparse.ArgumentParser(description=\"GPT-SoVITS api\")\r\n\r\nparser.add_argument(\"-s\", \"--sovits_path\", type=str, default=g_config.sovits_path, help=\"SoVITS模型路径\")\r\nparser.add_argument(\"-g\", \"--gpt_path\", type=str, default=g_config.gpt_path, help=\"GPT模型路径\")\r\nparser.add_argument(\"-dr\", \"--default_refer_path\", type=str, default=\"\", help=\"默认参考音频路径\")\r\nparser.add_argument(\"-dt\", \"--default_refer_text\", type=str, default=\"\", help=\"默认参考音频文本\")\r\nparser.add_argument(\"-dl\", \"--default_refer_language\", type=str, default=\"\", help=\"默认参考音频语种\")\r\nparser.add_argument(\"-d\", \"--device\", type=str, default=g_config.infer_device, help=\"cuda / cpu\")\r\nparser.add_argument(\"-a\", \"--bind_addr\", type=str, default=\"0.0.0.0\", help=\"default: 0.0.0.0\")\r\nparser.add_argument(\"-p\", \"--port\", type=int, default=g_config.api_port, help=\"default: 9880\")\r\nparser.add_argument(\"-fp\", \"--full_precision\", action=\"store_true\", default=False, help=\"覆盖config.is_half为False, 使用全精度\")\r\nparser.add_argument(\"-hp\", \"--half_precision\", action=\"store_true\", default=False, help=\"覆盖config.is_half为True, 使用半精度\")\r\n# bool值的用法为 `python ./api.py -fp ...`\r\n# 此时 full_precision==True, half_precision==False\r\nparser.add_argument(\"-sm\", \"--stream_mode\", type=str, default=\"close\", help=\"流式返回模式, close / normal / keepalive\")\r\nparser.add_argument(\"-mt\", \"--media_type\", type=str, default=\"wav\", help=\"音频编码格式, wav / ogg / aac\")\r\nparser.add_argument(\"-st\", \"--sub_type\", type=str, default=\"int16\", help=\"音频数据类型, int16 / int32\")\r\nparser.add_argument(\"-cp\", \"--cut_punc\", type=str, default=\"\", help=\"文本切分符号设定, 符号范围,.;?!、，。？！；：…\")\r\n# 切割常用分句符为 `python ./api.py -cp \".?!。？！\"`\r\nparser.add_argument(\"-hb\", \"--hubert_path\", type=str, default=g_config.cnhubert_path, help=\"覆盖config.cnhubert_path\")\r\nparser.add_argument(\"-b\", \"--bert_path\", type=str, default=g_config.bert_path, help=\"覆盖config.bert_path\")\r\n\r\nargs = parser.parse_args()\r\nsovits_path = args.sovits_path\r\ngpt_path = args.gpt_path\r\ndevice = args.device\r\nport = args.port\r\nhost = args.bind_addr\r\ncnhubert_base_path = args.hubert_path\r\nbert_path = args.bert_path\r\ndefault_cut_punc = args.cut_punc\r\n\r\n# 应用参数配置\r\ndefault_refer = DefaultRefer(args.default_refer_path, args.default_refer_text, args.default_refer_language)\r\n\r\n# 模型路径检查\r\nif sovits_path == \"\":\r\n    sovits_path = g_config.pretrained_sovits_path\r\n    logger.warn(f\"未指定SoVITS模型路径, fallback后当前值: {sovits_path}\")\r\nif gpt_path == \"\":\r\n    gpt_path = g_config.pretrained_gpt_path\r\n    logger.warn(f\"未指定GPT模型路径, fallback后当前值: {gpt_path}\")\r\n\r\n# 指定默认参考音频, 调用方 未提供/未给全 参考音频参数时使用\r\nif default_refer.path == \"\" or default_refer.text == \"\" or default_refer.language == \"\":\r\n    default_refer.path, default_refer.text, default_refer.language = \"\", \"\", \"\"\r\n    logger.info(\"未指定默认参考音频\")\r\nelse:\r\n    logger.info(f\"默认参考音频路径: {default_refer.path}\")\r\n    logger.info(f\"默认参考音频文本: {default_refer.text}\")\r\n    logger.info(f\"默认参考音频语种: {default_refer.language}\")\r\n\r\n# 获取半精度\r\nis_half = g_config.is_half\r\nif args.full_precision:\r\n    is_half = False\r\nif args.half_precision:\r\n    is_half = True\r\nif args.full_precision and args.half_precision:\r\n    is_half = g_config.is_half  # 炒饭fallback\r\nlogger.info(f\"半精: {is_half}\")\r\n\r\n# 流式返回模式\r\nif args.stream_mode.lower() in [\"normal\",\"n\"]:\r\n    stream_mode = \"normal\"\r\n    logger.info(\"流式返回已开启\")\r\nelse:\r\n    stream_mode = \"close\"\r\n\r\n# 音频编码格式\r\nif args.media_type.lower() in [\"aac\",\"ogg\"]:\r\n    media_type = args.media_type.lower()\r\nelif stream_mode == \"close\":\r\n    media_type = \"wav\"\r\nelse:\r\n    media_type = \"ogg\"\r\nlogger.info(f\"编码格式: {media_type}\")\r\n\r\n# 音频数据类型\r\nif args.sub_type.lower() == 'int32':\r\n    is_int32 = True\r\n    logger.info(f\"数据类型: int32\")\r\nelse:\r\n    is_int32 = False\r\n    logger.info(f\"数据类型: int16\")\r\n\r\n# 初始化模型\r\ncnhubert.cnhubert_base_path = cnhubert_base_path\r\ntokenizer = AutoTokenizer.from_pretrained(bert_path)\r\nbert_model = AutoModelForMaskedLM.from_pretrained(bert_path)\r\nssl_model = cnhubert.get_model()\r\nif is_half:\r\n    bert_model = bert_model.half().to(device)\r\n    ssl_model = ssl_model.half().to(device)\r\nelse:\r\n    bert_model = bert_model.to(device)\r\n    ssl_model = ssl_model.to(device)\r\nchange_gpt_sovits_weights(gpt_path = gpt_path, sovits_path = sovits_path)\r\n\r\n\r\n\r\n# --------------------------------\r\n# 接口部分\r\n# --------------------------------\r\napp = FastAPI()\r\n\r\n@app.post(\"/set_model\")\r\nasync def set_model(request: Request):\r\n    json_post_raw = await request.json()\r\n    return change_gpt_sovits_weights(\r\n        gpt_path = json_post_raw.get(\"gpt_model_path\"), \r\n        sovits_path = json_post_raw.get(\"sovits_model_path\")\r\n    )\r\n\r\n\r\n@app.get(\"/set_model\")\r\nasync def set_model(\r\n        gpt_model_path: str = None,\r\n        sovits_model_path: str = None,\r\n):\r\n    return change_gpt_sovits_weights(gpt_path = gpt_model_path, sovits_path = sovits_model_path)\r\n\r\n\r\n@app.post(\"/control\")\r\nasync def control(request: Request):\r\n    json_post_raw = await request.json()\r\n    return handle_control(json_post_raw.get(\"command\"))\r\n\r\n\r\n@app.get(\"/control\")\r\nasync def control(command: str = None):\r\n    return handle_control(command)\r\n\r\n\r\n@app.post(\"/change_refer\")\r\nasync def change_refer(request: Request):\r\n    json_post_raw = await request.json()\r\n    return handle_change(\r\n        json_post_raw.get(\"refer_wav_path\"),\r\n        json_post_raw.get(\"prompt_text\"),\r\n        json_post_raw.get(\"prompt_language\")\r\n    )\r\n\r\n\r\n@app.get(\"/change_refer\")\r\nasync def change_refer(\r\n        refer_wav_path: str = None,\r\n        prompt_text: str = None,\r\n        prompt_language: str = None\r\n):\r\n    return handle_change(refer_wav_path, prompt_text, prompt_language)\r\n\r\n\r\n@app.post(\"/\")\r\nasync def tts_endpoint(request: Request):\r\n    json_post_raw = await request.json()\r\n    return handle(\r\n        json_post_raw.get(\"refer_wav_path\"),\r\n        json_post_raw.get(\"prompt_text\"),\r\n        json_post_raw.get(\"prompt_language\"),\r\n        json_post_raw.get(\"text\"),\r\n        json_post_raw.get(\"text_language\"),\r\n        json_post_raw.get(\"cut_punc\"),\r\n        json_post_raw.get(\"top_k\", 15),\r\n        json_post_raw.get(\"top_p\", 1.0),\r\n        json_post_raw.get(\"temperature\", 1.0),\r\n        json_post_raw.get(\"speed\", 1.0),\r\n        json_post_raw.get(\"inp_refs\", [])\r\n    )\r\n\r\n\r\n@app.get(\"/\")\r\nasync def tts_endpoint(\r\n        refer_wav_path: str = None,\r\n        prompt_text: str = None,\r\n        prompt_language: str = None,\r\n        text: str = None,\r\n        text_language: str = None,\r\n        cut_punc: str = None,\r\n        top_k: int = 15,\r\n        top_p: float = 1.0,\r\n        temperature: float = 1.0,\r\n        speed: float = 1.0,\r\n        inp_refs: list = Query(default=[])\r\n):\r\n    return handle(refer_wav_path, prompt_text, prompt_language, text, text_language, cut_punc, top_k, top_p, temperature, speed, inp_refs)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    uvicorn.run(app, host=host, port=port, workers=1)\r\n"
        },
        {
          "name": "api_v2.py",
          "type": "blob",
          "size": 17.5419921875,
          "content": "\"\"\"\n# WebAPI文档\n\n` python api_v2.py -a 127.0.0.1 -p 9880 -c GPT_SoVITS/configs/tts_infer.yaml `\n\n## 执行参数:\n    `-a` - `绑定地址, 默认\"127.0.0.1\"`\n    `-p` - `绑定端口, 默认9880`\n    `-c` - `TTS配置文件路径, 默认\"GPT_SoVITS/configs/tts_infer.yaml\"`\n\n## 调用:\n\n### 推理\n\nendpoint: `/tts`\nGET:\n```\nhttp://127.0.0.1:9880/tts?text=先帝创业未半而中道崩殂，今天下三分，益州疲弊，此诚危急存亡之秋也。&text_lang=zh&ref_audio_path=archive_jingyuan_1.wav&prompt_lang=zh&prompt_text=我是「罗浮」云骑将军景元。不必拘谨，「将军」只是一时的身份，你称呼我景元便可&text_split_method=cut5&batch_size=1&media_type=wav&streaming_mode=true\n```\n\nPOST:\n```json\n{\n    \"text\": \"\",                   # str.(required) text to be synthesized\n    \"text_lang: \"\",               # str.(required) language of the text to be synthesized\n    \"ref_audio_path\": \"\",         # str.(required) reference audio path\n    \"aux_ref_audio_paths\": [],    # list.(optional) auxiliary reference audio paths for multi-speaker tone fusion\n    \"prompt_text\": \"\",            # str.(optional) prompt text for the reference audio\n    \"prompt_lang\": \"\",            # str.(required) language of the prompt text for the reference audio\n    \"top_k\": 5,                   # int. top k sampling\n    \"top_p\": 1,                   # float. top p sampling\n    \"temperature\": 1,             # float. temperature for sampling\n    \"text_split_method\": \"cut0\",  # str. text split method, see text_segmentation_method.py for details.\n    \"batch_size\": 1,              # int. batch size for inference\n    \"batch_threshold\": 0.75,      # float. threshold for batch splitting.\n    \"split_bucket: True,          # bool. whether to split the batch into multiple buckets.\n    \"speed_factor\":1.0,           # float. control the speed of the synthesized audio.\n    \"streaming_mode\": False,      # bool. whether to return a streaming response.\n    \"seed\": -1,                   # int. random seed for reproducibility.\n    \"parallel_infer\": True,       # bool. whether to use parallel inference.\n    \"repetition_penalty\": 1.35    # float. repetition penalty for T2S model.\n}\n```\n\nRESP:\n成功: 直接返回 wav 音频流， http code 200\n失败: 返回包含错误信息的 json, http code 400\n\n### 命令控制\n\nendpoint: `/control`\n\ncommand:\n\"restart\": 重新运行\n\"exit\": 结束运行\n\nGET:\n```\nhttp://127.0.0.1:9880/control?command=restart\n```\nPOST:\n```json\n{\n    \"command\": \"restart\"\n}\n```\n\nRESP: 无\n\n\n### 切换GPT模型\n\nendpoint: `/set_gpt_weights`\n\nGET:\n```\nhttp://127.0.0.1:9880/set_gpt_weights?weights_path=GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt\n```\nRESP: \n成功: 返回\"success\", http code 200\n失败: 返回包含错误信息的 json, http code 400\n\n\n### 切换Sovits模型\n\nendpoint: `/set_sovits_weights`\n\nGET:\n```\nhttp://127.0.0.1:9880/set_sovits_weights?weights_path=GPT_SoVITS/pretrained_models/s2G488k.pth\n```\n\nRESP: \n成功: 返回\"success\", http code 200\n失败: 返回包含错误信息的 json, http code 400\n    \n\"\"\"\nimport os\nimport sys\nimport traceback\nfrom typing import Generator\n\nnow_dir = os.getcwd()\nsys.path.append(now_dir)\nsys.path.append(\"%s/GPT_SoVITS\" % (now_dir))\n\nimport argparse\nimport subprocess\nimport wave\nimport signal\nimport numpy as np\nimport soundfile as sf\nfrom fastapi import FastAPI, Request, HTTPException, Response\nfrom fastapi.responses import StreamingResponse, JSONResponse\nfrom fastapi import FastAPI, UploadFile, File\nimport uvicorn\nfrom io import BytesIO\nfrom tools.i18n.i18n import I18nAuto\nfrom GPT_SoVITS.TTS_infer_pack.TTS import TTS, TTS_Config\nfrom GPT_SoVITS.TTS_infer_pack.text_segmentation_method import get_method_names as get_cut_method_names\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\n# print(sys.path)\ni18n = I18nAuto()\ncut_method_names = get_cut_method_names()\n\nparser = argparse.ArgumentParser(description=\"GPT-SoVITS api\")\nparser.add_argument(\"-c\", \"--tts_config\", type=str, default=\"GPT_SoVITS/configs/tts_infer.yaml\", help=\"tts_infer路径\")\nparser.add_argument(\"-a\", \"--bind_addr\", type=str, default=\"127.0.0.1\", help=\"default: 127.0.0.1\")\nparser.add_argument(\"-p\", \"--port\", type=int, default=\"9880\", help=\"default: 9880\")\nargs = parser.parse_args()\nconfig_path = args.tts_config\n# device = args.device\nport = args.port\nhost = args.bind_addr\nargv = sys.argv\n\nif config_path in [None, \"\"]:\n    config_path = \"GPT-SoVITS/configs/tts_infer.yaml\"\n\ntts_config = TTS_Config(config_path)\nprint(tts_config)\ntts_pipeline = TTS(tts_config)\n\nAPP = FastAPI()\nclass TTS_Request(BaseModel):\n    text: str = None\n    text_lang: str = None\n    ref_audio_path: str = None\n    aux_ref_audio_paths: list = None\n    prompt_lang: str = None\n    prompt_text: str = \"\"\n    top_k:int = 5\n    top_p:float = 1\n    temperature:float = 1\n    text_split_method:str = \"cut5\"\n    batch_size:int = 1\n    batch_threshold:float = 0.75\n    split_bucket:bool = True\n    speed_factor:float = 1.0\n    fragment_interval:float = 0.3\n    seed:int = -1\n    media_type:str = \"wav\"\n    streaming_mode:bool = False\n    parallel_infer:bool = True\n    repetition_penalty:float = 1.35\n\n### modify from https://github.com/RVC-Boss/GPT-SoVITS/pull/894/files\ndef pack_ogg(io_buffer:BytesIO, data:np.ndarray, rate:int):\n    with sf.SoundFile(io_buffer, mode='w', samplerate=rate, channels=1, format='ogg') as audio_file:\n        audio_file.write(data)\n    return io_buffer\n\n\ndef pack_raw(io_buffer:BytesIO, data:np.ndarray, rate:int):\n    io_buffer.write(data.tobytes())\n    return io_buffer\n\n\ndef pack_wav(io_buffer:BytesIO, data:np.ndarray, rate:int):\n    io_buffer = BytesIO()\n    sf.write(io_buffer, data, rate, format='wav')\n    return io_buffer\n\ndef pack_aac(io_buffer:BytesIO, data:np.ndarray, rate:int):\n    process = subprocess.Popen([\n        'ffmpeg',\n        '-f', 's16le',  # 输入16位有符号小端整数PCM\n        '-ar', str(rate),  # 设置采样率\n        '-ac', '1',  # 单声道\n        '-i', 'pipe:0',  # 从管道读取输入\n        '-c:a', 'aac',  # 音频编码器为AAC\n        '-b:a', '192k',  # 比特率\n        '-vn',  # 不包含视频\n        '-f', 'adts',  # 输出AAC数据流格式\n        'pipe:1'  # 将输出写入管道\n    ], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, _ = process.communicate(input=data.tobytes())\n    io_buffer.write(out)\n    return io_buffer\n\ndef pack_audio(io_buffer:BytesIO, data:np.ndarray, rate:int, media_type:str):\n    if media_type == \"ogg\":\n        io_buffer = pack_ogg(io_buffer, data, rate)\n    elif media_type == \"aac\":\n        io_buffer = pack_aac(io_buffer, data, rate)\n    elif media_type == \"wav\":\n        io_buffer = pack_wav(io_buffer, data, rate)\n    else:\n        io_buffer = pack_raw(io_buffer, data, rate)\n    io_buffer.seek(0)\n    return io_buffer\n\n\n\n# from https://huggingface.co/spaces/coqui/voice-chat-with-mistral/blob/main/app.py\ndef wave_header_chunk(frame_input=b\"\", channels=1, sample_width=2, sample_rate=32000):\n    # This will create a wave header then append the frame input\n    # It should be first on a streaming wav file\n    # Other frames better should not have it (else you will hear some artifacts each chunk start)\n    wav_buf = BytesIO()\n    with wave.open(wav_buf, \"wb\") as vfout:\n        vfout.setnchannels(channels)\n        vfout.setsampwidth(sample_width)\n        vfout.setframerate(sample_rate)\n        vfout.writeframes(frame_input)\n\n    wav_buf.seek(0)\n    return wav_buf.read()\n\n\ndef handle_control(command:str):\n    if command == \"restart\":\n        os.execl(sys.executable, sys.executable, *argv)\n    elif command == \"exit\":\n        os.kill(os.getpid(), signal.SIGTERM)\n        exit(0)\n\n\ndef check_params(req:dict):\n    text:str = req.get(\"text\", \"\")\n    text_lang:str = req.get(\"text_lang\", \"\")\n    ref_audio_path:str = req.get(\"ref_audio_path\", \"\")\n    streaming_mode:bool = req.get(\"streaming_mode\", False)\n    media_type:str = req.get(\"media_type\", \"wav\")\n    prompt_lang:str = req.get(\"prompt_lang\", \"\")\n    text_split_method:str = req.get(\"text_split_method\", \"cut5\")\n\n    if ref_audio_path in [None, \"\"]:\n        return JSONResponse(status_code=400, content={\"message\": \"ref_audio_path is required\"})\n    if text in [None, \"\"]:\n        return JSONResponse(status_code=400, content={\"message\": \"text is required\"})\n    if (text_lang in [None, \"\"]) :\n        return JSONResponse(status_code=400, content={\"message\": \"text_lang is required\"})\n    elif text_lang.lower() not in tts_config.languages:\n        return JSONResponse(status_code=400, content={\"message\": f\"text_lang: {text_lang} is not supported in version {tts_config.version}\"})\n    if (prompt_lang in [None, \"\"]) :\n        return JSONResponse(status_code=400, content={\"message\": \"prompt_lang is required\"})\n    elif prompt_lang.lower() not in tts_config.languages:\n        return JSONResponse(status_code=400, content={\"message\": f\"prompt_lang: {prompt_lang} is not supported in version {tts_config.version}\"})\n    if media_type not in [\"wav\", \"raw\", \"ogg\", \"aac\"]:\n        return JSONResponse(status_code=400, content={\"message\": f\"media_type: {media_type} is not supported\"})\n    elif media_type == \"ogg\" and  not streaming_mode:\n        return JSONResponse(status_code=400, content={\"message\": \"ogg format is not supported in non-streaming mode\"})\n    \n    if text_split_method not in cut_method_names:\n        return JSONResponse(status_code=400, content={\"message\": f\"text_split_method:{text_split_method} is not supported\"})\n\n    return None\n\nasync def tts_handle(req:dict):\n    \"\"\"\n    Text to speech handler.\n    \n    Args:\n        req (dict): \n            {\n                \"text\": \"\",                   # str.(required) text to be synthesized\n                \"text_lang: \"\",               # str.(required) language of the text to be synthesized\n                \"ref_audio_path\": \"\",         # str.(required) reference audio path\n                \"aux_ref_audio_paths\": [],    # list.(optional) auxiliary reference audio paths for multi-speaker synthesis\n                \"prompt_text\": \"\",            # str.(optional) prompt text for the reference audio\n                \"prompt_lang\": \"\",            # str.(required) language of the prompt text for the reference audio\n                \"top_k\": 5,                   # int. top k sampling\n                \"top_p\": 1,                   # float. top p sampling\n                \"temperature\": 1,             # float. temperature for sampling\n                \"text_split_method\": \"cut5\",  # str. text split method, see text_segmentation_method.py for details.\n                \"batch_size\": 1,              # int. batch size for inference\n                \"batch_threshold\": 0.75,      # float. threshold for batch splitting.\n                \"split_bucket: True,          # bool. whether to split the batch into multiple buckets.\n                \"speed_factor\":1.0,           # float. control the speed of the synthesized audio.\n                \"fragment_interval\":0.3,      # float. to control the interval of the audio fragment.\n                \"seed\": -1,                   # int. random seed for reproducibility.\n                \"media_type\": \"wav\",          # str. media type of the output audio, support \"wav\", \"raw\", \"ogg\", \"aac\".\n                \"streaming_mode\": False,      # bool. whether to return a streaming response.\n                \"parallel_infer\": True,       # bool.(optional) whether to use parallel inference.\n                \"repetition_penalty\": 1.35    # float.(optional) repetition penalty for T2S model.          \n            }\n    returns:\n        StreamingResponse: audio stream response.\n    \"\"\"\n    \n    streaming_mode = req.get(\"streaming_mode\", False)\n    return_fragment = req.get(\"return_fragment\", False)\n    media_type = req.get(\"media_type\", \"wav\")\n\n    check_res = check_params(req)\n    if check_res is not None:\n        return check_res\n\n    if streaming_mode or return_fragment:\n        req[\"return_fragment\"] = True\n    \n    try:\n        tts_generator=tts_pipeline.run(req)\n        \n        if streaming_mode:\n            def streaming_generator(tts_generator:Generator, media_type:str):\n                if media_type == \"wav\":\n                    yield wave_header_chunk()\n                    media_type = \"raw\"\n                for sr, chunk in tts_generator:\n                    yield pack_audio(BytesIO(), chunk, sr, media_type).getvalue()\n            # _media_type = f\"audio/{media_type}\" if not (streaming_mode and media_type in [\"wav\", \"raw\"]) else f\"audio/x-{media_type}\"\n            return StreamingResponse(streaming_generator(tts_generator, media_type, ), media_type=f\"audio/{media_type}\")\n    \n        else:\n            sr, audio_data = next(tts_generator)\n            audio_data = pack_audio(BytesIO(), audio_data, sr, media_type).getvalue()\n            return Response(audio_data, media_type=f\"audio/{media_type}\")\n    except Exception as e:\n        return JSONResponse(status_code=400, content={\"message\": f\"tts failed\", \"Exception\": str(e)})\n    \n\n\n\n\n\n@APP.get(\"/control\")\nasync def control(command: str = None):\n    if command is None:\n        return JSONResponse(status_code=400, content={\"message\": \"command is required\"})\n    handle_control(command)\n\n\n\n@APP.get(\"/tts\")\nasync def tts_get_endpoint(\n                        text: str = None,\n                        text_lang: str = None,\n                        ref_audio_path: str = None,\n                        aux_ref_audio_paths:list = None,\n                        prompt_lang: str = None,\n                        prompt_text: str = \"\",\n                        top_k:int = 5,\n                        top_p:float = 1,\n                        temperature:float = 1,\n                        text_split_method:str = \"cut0\",\n                        batch_size:int = 1,\n                        batch_threshold:float = 0.75,\n                        split_bucket:bool = True,\n                        speed_factor:float = 1.0,\n                        fragment_interval:float = 0.3,\n                        seed:int = -1,\n                        media_type:str = \"wav\",\n                        streaming_mode:bool = False,\n                        parallel_infer:bool = True,\n                        repetition_penalty:float = 1.35\n                        ):\n    req = {\n        \"text\": text,\n        \"text_lang\": text_lang.lower(),\n        \"ref_audio_path\": ref_audio_path,\n        \"aux_ref_audio_paths\": aux_ref_audio_paths,\n        \"prompt_text\": prompt_text,\n        \"prompt_lang\": prompt_lang.lower(),\n        \"top_k\": top_k,\n        \"top_p\": top_p,\n        \"temperature\": temperature,\n        \"text_split_method\": text_split_method,\n        \"batch_size\":int(batch_size),\n        \"batch_threshold\":float(batch_threshold),\n        \"speed_factor\":float(speed_factor),\n        \"split_bucket\":split_bucket,\n        \"fragment_interval\":fragment_interval,\n        \"seed\":seed,\n        \"media_type\":media_type,\n        \"streaming_mode\":streaming_mode,\n        \"parallel_infer\":parallel_infer,\n        \"repetition_penalty\":float(repetition_penalty)\n    }\n    return await tts_handle(req)\n                \n\n@APP.post(\"/tts\")\nasync def tts_post_endpoint(request: TTS_Request):\n    req = request.dict()\n    return await tts_handle(req)\n\n\n@APP.get(\"/set_refer_audio\")\nasync def set_refer_aduio(refer_audio_path: str = None):\n    try:\n        tts_pipeline.set_ref_audio(refer_audio_path)\n    except Exception as e:\n        return JSONResponse(status_code=400, content={\"message\": f\"set refer audio failed\", \"Exception\": str(e)})\n    return JSONResponse(status_code=200, content={\"message\": \"success\"})\n\n\n# @APP.post(\"/set_refer_audio\")\n# async def set_refer_aduio_post(audio_file: UploadFile = File(...)):\n#     try:\n#         # 检查文件类型，确保是音频文件\n#         if not audio_file.content_type.startswith(\"audio/\"):\n#             return JSONResponse(status_code=400, content={\"message\": \"file type is not supported\"})\n        \n#         os.makedirs(\"uploaded_audio\", exist_ok=True)\n#         save_path = os.path.join(\"uploaded_audio\", audio_file.filename)\n#         # 保存音频文件到服务器上的一个目录\n#         with open(save_path , \"wb\") as buffer:\n#             buffer.write(await audio_file.read())\n            \n#         tts_pipeline.set_ref_audio(save_path)\n#     except Exception as e:\n#         return JSONResponse(status_code=400, content={\"message\": f\"set refer audio failed\", \"Exception\": str(e)})\n#     return JSONResponse(status_code=200, content={\"message\": \"success\"})\n\n@APP.get(\"/set_gpt_weights\")\nasync def set_gpt_weights(weights_path: str = None):\n    try:\n        if weights_path in [\"\", None]:\n            return JSONResponse(status_code=400, content={\"message\": \"gpt weight path is required\"})\n        tts_pipeline.init_t2s_weights(weights_path)\n    except Exception as e:\n        return JSONResponse(status_code=400, content={\"message\": f\"change gpt weight failed\", \"Exception\": str(e)})\n\n    return JSONResponse(status_code=200, content={\"message\": \"success\"})\n\n\n@APP.get(\"/set_sovits_weights\")\nasync def set_sovits_weights(weights_path: str = None):\n    try:\n        if weights_path in [\"\", None]:\n            return JSONResponse(status_code=400, content={\"message\": \"sovits weight path is required\"})\n        tts_pipeline.init_vits_weights(weights_path)\n    except Exception as e:\n        return JSONResponse(status_code=400, content={\"message\": f\"change sovits weight failed\", \"Exception\": str(e)})\n    return JSONResponse(status_code=200, content={\"message\": \"success\"})\n\n\n\nif __name__ == \"__main__\":\n    try:\n        if host == 'None':   # 在调用时使用 -a None 参数，可以让api监听双栈\n            host = None\n        uvicorn.run(app=APP, host=host, port=port, workers=1)\n    except Exception as e:\n        traceback.print_exc()\n        os.kill(os.getpid(), signal.SIGTERM)\n        exit(0)\n"
        },
        {
          "name": "colab_webui.ipynb",
          "type": "blob",
          "size": 3.2890625,
          "content": "{\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0,\n  \"metadata\": {\n    \"colab\": {\n      \"provenance\": [],\n      \"include_colab_link\": true\n    },\n    \"kernelspec\": {\n      \"name\": \"python3\",\n      \"display_name\": \"Python 3\"\n    },\n    \"accelerator\": \"GPU\"\n  },\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"view-in-github\",\n        \"colab_type\": \"text\"\n      },\n      \"source\": [\n        \"<a href=\\\"https://colab.research.google.com/github/RVC-Boss/GPT-SoVITS/blob/main/colab_webui.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"环境配置 environment\"\n      ],\n      \"metadata\": {\n        \"id\": \"_o6a8GS2lWQM\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"e9b7iFV3dm1f\"\n      },\n      \"source\": [\n        \"!pip install -q condacolab\\n\",\n        \"# Setting up condacolab and installing packages\\n\",\n        \"import condacolab\\n\",\n        \"condacolab.install_from_url(\\\"https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh\\\")\\n\",\n        \"%cd -q /content\\n\",\n        \"!git clone https://github.com/RVC-Boss/GPT-SoVITS\\n\",\n        \"!conda install -y -q -c pytorch -c nvidia cudatoolkit\\n\",\n        \"%cd -q /content/GPT-SoVITS\\n\",\n        \"!conda install -y -q -c conda-forge gcc gxx ffmpeg cmake -c pytorch -c nvidia\\n\",\n        \"!/usr/local/bin/pip install -r requirements.txt\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# @title Download pretrained models 下载预训练模型\\n\",\n        \"!mkdir -p /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\\n\",\n        \"!mkdir -p /content/GPT-SoVITS/tools/damo_asr/models\\n\",\n        \"!mkdir -p /content/GPT-SoVITS/tools/uvr5\\n\",\n        \"%cd /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\\n\",\n        \"!git clone https://huggingface.co/lj1995/GPT-SoVITS\\n\",\n        \"%cd /content/GPT-SoVITS/tools/damo_asr/models\\n\",\n        \"!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\\n\",\n        \"!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\\n\",\n        \"!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\\n\",\n        \"# @title UVR5 pretrains 安装uvr5模型\\n\",\n        \"%cd /content/GPT-SoVITS/tools/uvr5\\n\",\n        \"%rm -r uvr5_weights\\n\",\n        \"!git clone https://huggingface.co/Delik/uvr5_weights\\n\",\n        \"!git config core.sparseCheckout true\\n\",\n        \"!mv /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/\"\n      ],\n      \"metadata\": {\n        \"id\": \"0NgxXg5sjv7z\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# @title launch WebUI 启动WebUI\\n\",\n        \"!/usr/local/bin/pip install ipykernel\\n\",\n        \"!sed -i '10s/False/True/' /content/GPT-SoVITS/config.py\\n\",\n        \"%cd /content/GPT-SoVITS/\\n\",\n        \"!/usr/local/bin/python  webui.py\"\n      ],\n      \"metadata\": {\n        \"id\": \"4oRGUzkrk8C7\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    }\n  ]\n}\n"
        },
        {
          "name": "config.py",
          "type": "blob",
          "size": 1.984375,
          "content": "import sys,os\r\n\r\nimport torch\r\n\r\n# 推理用的指定模型\r\nsovits_path = \"\"\r\ngpt_path = \"\"\r\nis_half_str = os.environ.get(\"is_half\", \"True\")\r\nis_half = True if is_half_str.lower() == 'true' else False\r\nis_share_str = os.environ.get(\"is_share\",\"False\")\r\nis_share= True if is_share_str.lower() == 'true' else False\r\n\r\ncnhubert_path = \"GPT_SoVITS/pretrained_models/chinese-hubert-base\"\r\nbert_path = \"GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\"\r\npretrained_sovits_path = \"GPT_SoVITS/pretrained_models/s2G488k.pth\"\r\npretrained_gpt_path = \"GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt\"\r\n\r\nexp_root = \"logs\"\r\npython_exec = sys.executable or \"python\"\r\nif torch.cuda.is_available():\r\n    infer_device = \"cuda\"\r\nelse:\r\n    infer_device = \"cpu\"\r\n\r\nwebui_port_main = 9874\r\nwebui_port_uvr5 = 9873\r\nwebui_port_infer_tts = 9872\r\nwebui_port_subfix = 9871\r\n\r\napi_port = 9880\r\n\r\nif infer_device == \"cuda\":\r\n    gpu_name = torch.cuda.get_device_name(0)\r\n    if (\r\n            (\"16\" in gpu_name and \"V100\" not in gpu_name.upper())\r\n            or \"P40\" in gpu_name.upper()\r\n            or \"P10\" in gpu_name.upper()\r\n            or \"1060\" in gpu_name\r\n            or \"1070\" in gpu_name\r\n            or \"1080\" in gpu_name\r\n    ):\r\n        is_half=False\r\n\r\nif(infer_device==\"cpu\"):is_half=False\r\n\r\nclass Config:\r\n    def __init__(self):\r\n        self.sovits_path = sovits_path\r\n        self.gpt_path = gpt_path\r\n        self.is_half = is_half\r\n\r\n        self.cnhubert_path = cnhubert_path\r\n        self.bert_path = bert_path\r\n        self.pretrained_sovits_path = pretrained_sovits_path\r\n        self.pretrained_gpt_path = pretrained_gpt_path\r\n\r\n        self.exp_root = exp_root\r\n        self.python_exec = python_exec\r\n        self.infer_device = infer_device\r\n\r\n        self.webui_port_main = webui_port_main\r\n        self.webui_port_uvr5 = webui_port_uvr5\r\n        self.webui_port_infer_tts = webui_port_infer_tts\r\n        self.webui_port_subfix = webui_port_subfix\r\n\r\n        self.api_port = api_port\r\n"
        },
        {
          "name": "docker-compose.yaml",
          "type": "blob",
          "size": 1.0107421875,
          "content": "version: '3.8'\n\nservices:\n  gpt-sovits:\n    image: breakstring/gpt-sovits:latest   # please change the image name and tag base your environment. If the tag contains the word 'elite', such as \"latest-elite\", it indicates that the image does not include the necessary models such as GPT-SoVITS, UVR5, Damo ASR, etc. You will need to download them yourself and map them into the container.\n    container_name: gpt-sovits-container\n    environment:\n      - is_half=False\n      - is_share=False\n    volumes:\n      - ./output:/workspace/output\n      - ./logs:/workspace/logs\n      - ./SoVITS_weights:/workspace/SoVITS_weights\n      - ./reference:/workspace/reference\n    working_dir: /workspace\n    ports:\n      - \"9880:9880\"\n      - \"9871:9871\"\n      - \"9872:9872\"\n      - \"9873:9873\"\n      - \"9874:9874\"\n    shm_size: 16G\n    deploy:\n      resources:\n        reservations:\n          devices:\n          - driver: nvidia\n            count: \"all\"\n            capabilities: [gpu]\n    stdin_open: true\n    tty: true\n    restart: unless-stopped\n"
        },
        {
          "name": "dockerbuild.sh",
          "type": "blob",
          "size": 1,
          "content": "#!/bin/bash\n\n# 获取当前日期，格式为 YYYYMMDD\nDATE=$(date +%Y%m%d)\n# 获取最新的 Git commit 哈希值的前 7 位\nCOMMIT_HASH=$(git rev-parse HEAD | cut -c 1-7)\n\n# 构建 full 版本的镜像\ndocker build --build-arg IMAGE_TYPE=full -t breakstring/gpt-sovits:latest .\n# 为同一个镜像添加带日期的标签\ndocker tag breakstring/gpt-sovits:latest breakstring/gpt-sovits:dev-$DATE\n# 为同一个镜像添加带当前代码库Commit哈希值的标签\ndocker tag breakstring/gpt-sovits:latest breakstring/gpt-sovits:dev-$COMMIT_HASH\n\n\n# 构建 elite 版本的镜像(无模型下载步骤，需手工将模型下载安装进容器)\ndocker build --build-arg IMAGE_TYPE=elite -t breakstring/gpt-sovits:latest-elite .\n# 为同一个镜像添加带日期的标签\ndocker tag breakstring/gpt-sovits:latest-elite breakstring/gpt-sovits:dev-$DATE-elite\n# 为同一个镜像添加带当前代码库Commit哈希值的标签\ndocker tag breakstring/gpt-sovits:latest-elite breakstring/gpt-sovits:dev-$COMMIT_HASH-elite\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "go-webui.bat",
          "type": "blob",
          "size": 0.041015625,
          "content": "runtime\\python.exe webui.py zh_CN\r\npause\r\n"
        },
        {
          "name": "go-webui.ps1",
          "type": "blob",
          "size": 0.1259765625,
          "content": "$ErrorActionPreference = \"SilentlyContinue\"\nchcp 65001\n& \"$PSScriptRoot\\runtime\\python.exe\" \"$PSScriptRoot\\webui.py zh_CN\"\npause\n"
        },
        {
          "name": "gpt-sovits_kaggle.ipynb",
          "type": "blob",
          "size": 6.9677734375,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"45857cb2\",\n   \"metadata\": {\n    \"_cell_guid\": \"b1076dfc-b9ad-4769-8c92-a6c4dae69d19\",\n    \"_uuid\": \"8f2839f25d086af736a60e9eeb907d3b93b6e0e5\",\n    \"execution\": {\n     \"iopub.execute_input\": \"2024-02-18T14:43:46.735480Z\",\n     \"iopub.status.busy\": \"2024-02-18T14:43:46.735183Z\",\n     \"iopub.status.idle\": \"2024-02-18T14:48:10.724175Z\",\n     \"shell.execute_reply\": \"2024-02-18T14:48:10.723059Z\"\n    },\n    \"papermill\": {\n     \"duration\": 263.994935,\n     \"end_time\": \"2024-02-18T14:48:10.726613\",\n     \"exception\": false,\n     \"start_time\": \"2024-02-18T14:43:46.731678\",\n     \"status\": \"completed\"\n    },\n    \"tags\": []\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!git clone https://github.com/RVC-Boss/GPT-SoVITS.git\\n\",\n    \"%cd GPT-SoVITS\\n\",\n    \"!apt-get update && apt-get install -y --no-install-recommends tzdata ffmpeg libsox-dev parallel aria2 git git-lfs && git lfs install\\n\",\n    \"!pip install -r requirements.txt\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"b9d346b4\",\n   \"metadata\": {\n    \"execution\": {\n     \"iopub.execute_input\": \"2024-02-18T14:48:10.815802Z\",\n     \"iopub.status.busy\": \"2024-02-18T14:48:10.814899Z\",\n     \"iopub.status.idle\": \"2024-02-18T14:50:31.253276Z\",\n     \"shell.execute_reply\": \"2024-02-18T14:50:31.252024Z\"\n    },\n    \"papermill\": {\n     \"duration\": 140.484893,\n     \"end_time\": \"2024-02-18T14:50:31.255720\",\n     \"exception\": false,\n     \"start_time\": \"2024-02-18T14:48:10.770827\",\n     \"status\": \"completed\"\n    },\n    \"tags\": []\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# @title Download pretrained models 下载预训练模型\\n\",\n    \"!mkdir -p /kaggle/working/GPT-SoVITS/GPT_SoVITS/pretrained_models\\n\",\n    \"!mkdir -p /kaggle/working/GPT-SoVITS/tools/asr/models\\n\",\n    \"!mkdir -p /kaggle/working/GPT-SoVITS/tools/uvr5\\n\",\n    \"%cd /kaggle/working/GPT-SoVITS/GPT_SoVITS/pretrained_models\\n\",\n    \"!git clone https://huggingface.co/lj1995/GPT-SoVITS\\n\",\n    \"%cd /kaggle/working/GPT-SoVITS/tools/asr/models\\n\",\n    \"!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\\n\",\n    \"!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\\n\",\n    \"!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\\n\",\n    \"# # @title UVR5 pretrains 安装uvr5模型\\n\",\n    \"%cd /kaggle/working/GPT-SoVITS/tools/uvr5\\n\",\n    \"!git clone https://huggingface.co/Delik/uvr5_weights\\n\",\n    \"!git config core.sparseCheckout true\\n\",\n    \"!mv /kaggle/working/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /kaggle/working/GPT-SoVITS/GPT_SoVITS/pretrained_models/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"ea94d245\",\n   \"metadata\": {\n    \"execution\": {\n     \"iopub.execute_input\": \"2024-02-18T14:29:01.071549Z\",\n     \"iopub.status.busy\": \"2024-02-18T14:29:01.070592Z\",\n     \"iopub.status.idle\": \"2024-02-18T14:40:45.318368Z\",\n     \"shell.execute_reply\": \"2024-02-18T14:40:45.317130Z\",\n     \"shell.execute_reply.started\": \"2024-02-18T14:29:01.071512Z\"\n    },\n    \"papermill\": {\n     \"duration\": null,\n     \"end_time\": null,\n     \"exception\": false,\n     \"start_time\": \"2024-02-18T14:50:31.309013\",\n     \"status\": \"running\"\n    },\n    \"tags\": []\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# @title launch WebUI 启动WebUI\\n\",\n    \"%cd /kaggle/working/GPT-SoVITS/\\n\",\n    \"!npm install -g localtunnel\\n\",\n    \"import subprocess\\n\",\n    \"import threading\\n\",\n    \"import time\\n\",\n    \"import socket\\n\",\n    \"import urllib.request\\n\",\n    \"def iframe_thread(port):\\n\",\n    \"    while True:\\n\",\n    \"        time.sleep(0.5)\\n\",\n    \"        sock= socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n\",\n    \"        result = sock.connect_ex(('127.0.0.1', port))\\n\",\n    \"        if result == 0:\\n\",\n    \"            break\\n\",\n    \"        sock.close()\\n\",\n    \"\\n\",\n    \"        from colorama import Fore, Style\\n\",\n    \"    print (Fore.GREEN + \\\"\\\\nIP: \\\", Fore. RED, urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\\\"\\\\n\\\"), \\\"\\\\n\\\", Style. RESET_ALL)\\n\",\n    \"    p = subprocess.Popen([\\\"lt\\\", \\\"--port\\\", \\\"{}\\\".format(port)], stdout=subprocess.PIPE)\\n\",\n    \"    for line in p.stdout:\\n\",\n    \"        print(line.decode(), end='')\\n\",\n    \"threading.Thread (target=iframe_thread, daemon=True, args=(9874,)).start()\\n\",\n    \"\\n\",\n    \"!python  webui.py\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"dda88a6d\",\n   \"metadata\": {\n    \"execution\": {\n     \"iopub.execute_input\": \"2024-02-18T14:40:56.880608Z\",\n     \"iopub.status.busy\": \"2024-02-18T14:40:56.879879Z\"\n    },\n    \"papermill\": {\n     \"duration\": null,\n     \"end_time\": null,\n     \"exception\": null,\n     \"start_time\": null,\n     \"status\": \"pending\"\n    },\n    \"tags\": []\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# 开启推理页面\\n\",\n    \"%cd /kaggle/working/GPT-SoVITS/\\n\",\n    \"!npm install -g localtunnel\\n\",\n    \"import subprocess\\n\",\n    \"import threading\\n\",\n    \"import time\\n\",\n    \"import socket\\n\",\n    \"import urllib.request\\n\",\n    \"def iframe_thread(port):\\n\",\n    \"    while True:\\n\",\n    \"        time.sleep(0.5)\\n\",\n    \"        sock= socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n\",\n    \"        result = sock.connect_ex(('127.0.0.1', port))\\n\",\n    \"        if result == 0:\\n\",\n    \"            break\\n\",\n    \"        sock.close()\\n\",\n    \"\\n\",\n    \"        from colorama import Fore, Style\\n\",\n    \"    print (Fore.GREEN + \\\"\\\\nIP: \\\", Fore. RED, urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\\\"\\\\n\\\"), \\\"\\\\n\\\", Style. RESET_ALL)\\n\",\n    \"    p = subprocess.Popen([\\\"lt\\\", \\\"--port\\\", \\\"{}\\\".format(port)], stdout=subprocess.PIPE)\\n\",\n    \"    for line in p.stdout:\\n\",\n    \"        print(line.decode(), end='')\\n\",\n    \"threading.Thread (target=iframe_thread, daemon=True, args=(9872,)).start()\\n\",\n    \"\\n\",\n    \"!python  ./GPT_SoVITS/inference_webui.py\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kaggle\": {\n   \"accelerator\": \"nvidiaTeslaT4\",\n   \"dataSources\": [\n    {\n     \"datasetId\": 4459328,\n     \"sourceId\": 7649639,\n     \"sourceType\": \"datasetVersion\"\n    }\n   ],\n   \"dockerImageVersionId\": 30646,\n   \"isGpuEnabled\": true,\n   \"isInternetEnabled\": true,\n   \"language\": \"python\",\n   \"sourceType\": \"notebook\"\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.10.13\"\n  },\n  \"papermill\": {\n   \"default_parameters\": {},\n   \"duration\": null,\n   \"end_time\": null,\n   \"environment_variables\": {},\n   \"exception\": null,\n   \"input_path\": \"__notebook__.ipynb\",\n   \"output_path\": \"__notebook__.ipynb\",\n   \"parameters\": {},\n   \"start_time\": \"2024-02-18T14:43:44.011910\",\n   \"version\": \"2.5.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}\n"
        },
        {
          "name": "install.sh",
          "type": "blob",
          "size": 0.2373046875,
          "content": "#!/bin/bash\nconda install -c conda-forge gcc\nconda install -c conda-forge gxx\nconda install ffmpeg cmake\nconda install pytorch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 pytorch-cuda=11.8 -c pytorch -c nvidia\npip install -r requirements.txt\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.560546875,
          "content": "numpy==1.23.4\nscipy\ntensorboard\nlibrosa==0.9.2\nnumba==0.56.4\npytorch-lightning\ngradio>=4.0,<=4.24.0\nffmpeg-python\nonnxruntime; sys_platform == 'darwin'\nonnxruntime-gpu; sys_platform != 'darwin'\ntqdm\nfunasr==1.0.27\ncn2an\npypinyin\npyopenjtalk>=0.3.4\ng2p_en\ntorchaudio\nmodelscope==1.10.0\nsentencepiece\ntransformers\nchardet\nPyYAML\npsutil\njieba_fast\njieba\nLangSegment>=0.2.0\nFaster_Whisper\nwordsegment\nrotary_embedding_torch\npyjyutping \ng2pk2\nko_pron\nopencc; sys_platform != 'linux'\nopencc==1.1.1; sys_platform == 'linux'\npython_mecab_ko; sys_platform != 'win32'\nfastapi<0.112.2\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "webui.py",
          "type": "blob",
          "size": 58.5859375,
          "content": "import os,sys\r\nif len(sys.argv)==1:sys.argv.append('v2')\r\nversion=\"v1\"if sys.argv[1]==\"v1\" else\"v2\"\r\nos.environ[\"version\"]=version\r\nnow_dir = os.getcwd()\r\nsys.path.insert(0, now_dir)\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\nimport json,yaml,torch,pdb,re,shutil\r\nimport platform\r\nimport psutil\r\nimport signal\r\ntorch.manual_seed(233333)\r\ntmp = os.path.join(now_dir, \"TEMP\")\r\nos.makedirs(tmp, exist_ok=True)\r\nos.environ[\"TEMP\"] = tmp\r\nif(os.path.exists(tmp)):\r\n    for name in os.listdir(tmp):\r\n        if(name==\"jieba.cache\"):continue\r\n        path=\"%s/%s\"%(tmp,name)\r\n        delete=os.remove if os.path.isfile(path) else shutil.rmtree\r\n        try:\r\n            delete(path)\r\n        except Exception as e:\r\n            print(str(e))\r\n            pass\r\nimport site\r\nimport traceback\r\nsite_packages_roots = []\r\nfor path in site.getsitepackages():\r\n    if \"packages\" in path:\r\n        site_packages_roots.append(path)\r\nif(site_packages_roots==[]):site_packages_roots=[\"%s/runtime/Lib/site-packages\" % now_dir]\r\n#os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\r\nos.environ[\"no_proxy\"] = \"localhost, 127.0.0.1, ::1\"\r\nos.environ[\"all_proxy\"] = \"\"\r\nfor site_packages_root in site_packages_roots:\r\n    if os.path.exists(site_packages_root):\r\n        try:\r\n            with open(\"%s/users.pth\" % (site_packages_root), \"w\") as f:\r\n                f.write(\r\n                    \"%s\\n%s/tools\\n%s/tools/asr\\n%s/GPT_SoVITS\\n%s/tools/uvr5\"\r\n                    % (now_dir, now_dir, now_dir, now_dir, now_dir)\r\n                )\r\n            break\r\n        except PermissionError as e:\r\n            traceback.print_exc()\r\nfrom tools import my_utils\r\nimport shutil\r\nimport pdb\r\nfrom subprocess import Popen\r\nimport signal\r\nfrom config import python_exec,infer_device,is_half,exp_root,webui_port_main,webui_port_infer_tts,webui_port_uvr5,webui_port_subfix,is_share\r\nfrom tools.i18n.i18n import I18nAuto, scan_language_list\r\nlanguage=sys.argv[-1] if sys.argv[-1] in scan_language_list() else \"Auto\"\r\nos.environ[\"language\"]=language\r\ni18n = I18nAuto(language=language)\r\nfrom scipy.io import wavfile\r\nfrom tools.my_utils import load_audio, check_for_existance, check_details\r\nfrom multiprocessing import cpu_count\r\n# os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1' # 当遇到mps不支持的步骤时使用cpu\r\ntry:\r\n    import gradio.analytics as analytics\r\n    analytics.version_check = lambda:None\r\nexcept:...\r\nimport gradio as gr\r\nn_cpu=cpu_count()\r\n           \r\nngpu = torch.cuda.device_count()\r\ngpu_infos = []\r\nmem = []\r\nif_gpu_ok = False\r\n\r\n# 判断是否有能用来训练和加速推理的N卡\r\nok_gpu_keywords={\"10\",\"16\",\"20\",\"30\",\"40\",\"A2\",\"A3\",\"A4\",\"P4\",\"A50\",\"500\",\"A60\",\"70\",\"80\",\"90\",\"M4\",\"T4\",\"TITAN\",\"L4\",\"4060\",\"H\",\"600\"}\r\nset_gpu_numbers=set()\r\nif torch.cuda.is_available() or ngpu != 0:\r\n    for i in range(ngpu):\r\n        gpu_name = torch.cuda.get_device_name(i)\r\n        if any(value in gpu_name.upper()for value in ok_gpu_keywords):\r\n            # A10#A100#V100#A40#P40#M40#K80#A4500\r\n            if_gpu_ok = True  # 至少有一张能用的N卡\r\n            gpu_infos.append(\"%s\\t%s\" % (i, gpu_name))\r\n            set_gpu_numbers.add(i)\r\n            mem.append(int(torch.cuda.get_device_properties(i).total_memory/ 1024/ 1024/ 1024+ 0.4))\r\n# # 判断是否支持mps加速\r\n# if torch.backends.mps.is_available():\r\n#     if_gpu_ok = True\r\n#     gpu_infos.append(\"%s\\t%s\" % (\"0\", \"Apple GPU\"))\r\n#     mem.append(psutil.virtual_memory().total/ 1024 / 1024 / 1024) # 实测使用系统内存作为显存不会爆显存\r\n\r\nif if_gpu_ok and len(gpu_infos) > 0:\r\n    gpu_info = \"\\n\".join(gpu_infos)\r\n    default_batch_size = min(mem) // 2\r\nelse:\r\n    gpu_info = (\"%s\\t%s\" % (\"0\", \"CPU\"))\r\n    gpu_infos.append(\"%s\\t%s\" % (\"0\", \"CPU\"))\r\n    set_gpu_numbers.add(0)\r\n    default_batch_size = int(psutil.virtual_memory().total/ 1024 / 1024 / 1024 / 2)\r\ngpus = \"-\".join([i[0] for i in gpu_infos])\r\ndefault_gpu_numbers=str(sorted(list(set_gpu_numbers))[0])\r\ndef fix_gpu_number(input):#将越界的number强制改到界内\r\n    try:\r\n        if(int(input)not in set_gpu_numbers):return default_gpu_numbers\r\n    except:return input\r\n    return input\r\ndef fix_gpu_numbers(inputs):\r\n    output=[]\r\n    try:\r\n        for input in inputs.split(\",\"):output.append(str(fix_gpu_number(input)))\r\n        return \",\".join(output)\r\n    except:\r\n        return inputs\r\n\r\npretrained_sovits_name=[\"GPT_SoVITS/pretrained_models/gsv-v2final-pretrained/s2G2333k.pth\", \"GPT_SoVITS/pretrained_models/s2G488k.pth\"]\r\npretrained_gpt_name=[\"GPT_SoVITS/pretrained_models/gsv-v2final-pretrained/s1bert25hz-5kh-longer-epoch=12-step=369668.ckpt\", \"GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt\"]\r\n\r\npretrained_model_list = (pretrained_sovits_name[-int(version[-1])+2],pretrained_sovits_name[-int(version[-1])+2].replace(\"s2G\",\"s2D\"),pretrained_gpt_name[-int(version[-1])+2],\"GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\",\"GPT_SoVITS/pretrained_models/chinese-hubert-base\")\r\n\r\n_=''\r\nfor i in pretrained_model_list:\r\n    if os.path.exists(i):...\r\n    else:_+=f'\\n    {i}'\r\nif _:\r\n    print(\"warning:\",i18n('以下模型不存在:')+_)\r\n\r\n_ =[[],[]]\r\nfor i in range(2):\r\n    if os.path.exists(pretrained_gpt_name[i]):_[0].append(pretrained_gpt_name[i])\r\n    else:_[0].append(\"\")##没有下pretrained模型的，说不定他们是想自己从零训底模呢\r\n    if os.path.exists(pretrained_sovits_name[i]):_[-1].append(pretrained_sovits_name[i])\r\n    else:_[-1].append(\"\")\r\npretrained_gpt_name,pretrained_sovits_name = _\r\n\r\nSoVITS_weight_root=[\"SoVITS_weights_v2\",\"SoVITS_weights\"]\r\nGPT_weight_root=[\"GPT_weights_v2\",\"GPT_weights\"]\r\nfor root in SoVITS_weight_root+GPT_weight_root:\r\n    os.makedirs(root,exist_ok=True)\r\ndef get_weights_names():\r\n    SoVITS_names = [name for name in pretrained_sovits_name if name!=\"\"]\r\n    for path in SoVITS_weight_root:\r\n        for name in os.listdir(path):\r\n            if name.endswith(\".pth\"): SoVITS_names.append(\"%s/%s\" % (path, name))\r\n    GPT_names = [name for name in pretrained_gpt_name if name!=\"\"]\r\n    for path in GPT_weight_root:\r\n        for name in os.listdir(path):\r\n            if name.endswith(\".ckpt\"): GPT_names.append(\"%s/%s\" % (path, name))\r\n    return SoVITS_names, GPT_names\r\n\r\nSoVITS_names,GPT_names = get_weights_names()\r\nfor path in SoVITS_weight_root+GPT_weight_root:\r\n    os.makedirs(path,exist_ok=True)\r\n\r\n\r\ndef custom_sort_key(s):\r\n    # 使用正则表达式提取字符串中的数字部分和非数字部分\r\n    parts = re.split('(\\d+)', s)\r\n    # 将数字部分转换为整数，非数字部分保持不变\r\n    parts = [int(part) if part.isdigit() else part for part in parts]\r\n    return parts\r\n\r\ndef change_choices():\r\n    SoVITS_names, GPT_names = get_weights_names()\r\n    return {\"choices\": sorted(SoVITS_names,key=custom_sort_key), \"__type__\": \"update\"}, {\"choices\": sorted(GPT_names,key=custom_sort_key), \"__type__\": \"update\"}\r\n\r\np_label=None\r\np_uvr5=None\r\np_asr=None\r\np_denoise=None\r\np_tts_inference=None\r\n\r\ndef kill_proc_tree(pid, including_parent=True):  \r\n    try:\r\n        parent = psutil.Process(pid)\r\n    except psutil.NoSuchProcess:\r\n        # Process already terminated\r\n        return\r\n\r\n    children = parent.children(recursive=True)\r\n    for child in children:\r\n        try:\r\n            os.kill(child.pid, signal.SIGTERM)  # or signal.SIGKILL\r\n        except OSError:\r\n            pass\r\n    if including_parent:\r\n        try:\r\n            os.kill(parent.pid, signal.SIGTERM)  # or signal.SIGKILL\r\n        except OSError:\r\n            pass\r\n\r\nsystem=platform.system()\r\ndef kill_process(pid):\r\n    if(system==\"Windows\"):\r\n        cmd = \"taskkill /t /f /pid %s\" % pid\r\n        os.system(cmd)\r\n    else:\r\n        kill_proc_tree(pid)\r\n    \r\n\r\ndef change_label(path_list):\r\n    global p_label\r\n    if(p_label==None):\r\n        check_for_existance([path_list])\r\n        path_list=my_utils.clean_path(path_list)\r\n        cmd = '\"%s\" tools/subfix_webui.py --load_list \"%s\" --webui_port %s --is_share %s'%(python_exec,path_list,webui_port_subfix,is_share)\r\n        yield i18n(\"打标工具WebUI已开启\"), {'__type__':'update','visible':False}, {'__type__':'update','visible':True}\r\n        print(cmd)\r\n        p_label = Popen(cmd, shell=True)\r\n    elif(p_label!=None):\r\n        kill_process(p_label.pid)\r\n        p_label=None\r\n        yield i18n(\"打标工具WebUI已关闭\"), {'__type__':'update','visible':True}, {'__type__':'update','visible':False}\r\n\r\ndef change_uvr5():\r\n    global p_uvr5\r\n    if(p_uvr5==None):\r\n        cmd = '\"%s\" tools/uvr5/webui.py \"%s\" %s %s %s'%(python_exec,infer_device,is_half,webui_port_uvr5,is_share)\r\n        yield i18n(\"UVR5已开启\"), {'__type__':'update','visible':False}, {'__type__':'update','visible':True}\r\n        print(cmd)\r\n        p_uvr5 = Popen(cmd, shell=True)\r\n    elif(p_uvr5!=None):\r\n        kill_process(p_uvr5.pid)\r\n        p_uvr5=None\r\n        yield i18n(\"UVR5已关闭\"), {'__type__':'update','visible':True}, {'__type__':'update','visible':False}\r\n\r\ndef change_tts_inference(bert_path,cnhubert_base_path,gpu_number,gpt_path,sovits_path, batched_infer_enabled):\r\n    global p_tts_inference\r\n    if batched_infer_enabled:\r\n        cmd = '\"%s\" GPT_SoVITS/inference_webui_fast.py \"%s\"'%(python_exec, language)\r\n    else:\r\n        cmd = '\"%s\" GPT_SoVITS/inference_webui.py \"%s\"'%(python_exec, language)\r\n    if(p_tts_inference==None):\r\n        os.environ[\"gpt_path\"]=gpt_path if \"/\" in gpt_path else \"%s/%s\"%(GPT_weight_root,gpt_path)\r\n        os.environ[\"sovits_path\"]=sovits_path if \"/\"in sovits_path else \"%s/%s\"%(SoVITS_weight_root,sovits_path)\r\n        os.environ[\"cnhubert_base_path\"]=cnhubert_base_path\r\n        os.environ[\"bert_path\"]=bert_path\r\n        os.environ[\"_CUDA_VISIBLE_DEVICES\"]=fix_gpu_number(gpu_number)\r\n        os.environ[\"is_half\"]=str(is_half)\r\n        os.environ[\"infer_ttswebui\"]=str(webui_port_infer_tts)\r\n        os.environ[\"is_share\"]=str(is_share)\r\n        yield i18n(\"TTS推理进程已开启\"), {'__type__':'update','visible':False}, {'__type__':'update','visible':True}\r\n        print(cmd)\r\n        p_tts_inference = Popen(cmd, shell=True)\r\n    elif(p_tts_inference!=None):\r\n        kill_process(p_tts_inference.pid)\r\n        p_tts_inference=None\r\n        yield i18n(\"TTS推理进程已关闭\"), {'__type__':'update','visible':True}, {'__type__':'update','visible':False}\r\n\r\nfrom tools.asr.config import asr_dict\r\ndef open_asr(asr_inp_dir, asr_opt_dir, asr_model, asr_model_size, asr_lang, asr_precision):\r\n    global p_asr\r\n    if(p_asr==None):\r\n        asr_inp_dir=my_utils.clean_path(asr_inp_dir)\r\n        asr_opt_dir=my_utils.clean_path(asr_opt_dir)\r\n        check_for_existance([asr_inp_dir])\r\n        cmd = f'\"{python_exec}\" tools/asr/{asr_dict[asr_model][\"path\"]}'\r\n        cmd += f' -i \"{asr_inp_dir}\"'\r\n        cmd += f' -o \"{asr_opt_dir}\"'\r\n        cmd += f' -s {asr_model_size}'\r\n        cmd += f' -l {asr_lang}'\r\n        cmd += f\" -p {asr_precision}\"\r\n        output_file_name = os.path.basename(asr_inp_dir)\r\n        output_folder = asr_opt_dir or \"output/asr_opt\"\r\n        output_file_path = os.path.abspath(f'{output_folder}/{output_file_name}.list')\r\n        yield \"ASR任务开启：%s\"%cmd, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\"}, {\"__type__\":\"update\"}, {\"__type__\":\"update\"}\r\n        print(cmd)\r\n        p_asr = Popen(cmd, shell=True)\r\n        p_asr.wait()\r\n        p_asr=None\r\n        yield f\"ASR任务完成, 查看终端进行下一步\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"value\":output_file_path}, {\"__type__\":\"update\",\"value\":output_file_path}, {\"__type__\":\"update\",\"value\":asr_inp_dir}\r\n    else:\r\n        yield \"已有正在进行的ASR任务，需先终止才能开启下一次任务\", {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\"}, {\"__type__\":\"update\"}, {\"__type__\":\"update\"}\r\n        # return None\r\n\r\ndef close_asr():\r\n    global p_asr\r\n    if(p_asr!=None):\r\n        kill_process(p_asr.pid)\r\n        p_asr=None\r\n    return \"已终止ASR进程\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\ndef open_denoise(denoise_inp_dir, denoise_opt_dir):\r\n    global p_denoise\r\n    if(p_denoise==None):\r\n        denoise_inp_dir=my_utils.clean_path(denoise_inp_dir)\r\n        denoise_opt_dir=my_utils.clean_path(denoise_opt_dir)\r\n        check_for_existance([denoise_inp_dir])\r\n        cmd = '\"%s\" tools/cmd-denoise.py -i \"%s\" -o \"%s\" -p %s'%(python_exec,denoise_inp_dir,denoise_opt_dir,\"float16\"if is_half==True else \"float32\")\r\n\r\n        yield \"语音降噪任务开启：%s\"%cmd, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\"}, {\"__type__\":\"update\"}\r\n        print(cmd)\r\n        p_denoise = Popen(cmd, shell=True)\r\n        p_denoise.wait()\r\n        p_denoise=None\r\n        yield f\"语音降噪任务完成, 查看终端进行下一步\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"value\":denoise_opt_dir}, {\"__type__\":\"update\",\"value\":denoise_opt_dir}\r\n    else:\r\n        yield \"已有正在进行的语音降噪任务，需先终止才能开启下一次任务\", {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\"}, {\"__type__\":\"update\"}\r\n        # return None\r\n\r\ndef close_denoise():\r\n    global p_denoise\r\n    if(p_denoise!=None):\r\n        kill_process(p_denoise.pid)\r\n        p_denoise=None\r\n    return \"已终止语音降噪进程\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n\r\np_train_SoVITS=None\r\ndef open1Ba(batch_size,total_epoch,exp_name,text_low_lr_rate,if_save_latest,if_save_every_weights,save_every_epoch,gpu_numbers1Ba,pretrained_s2G,pretrained_s2D):\r\n    global p_train_SoVITS\r\n    if(p_train_SoVITS==None):\r\n        with open(\"GPT_SoVITS/configs/s2.json\")as f:\r\n            data=f.read()\r\n            data=json.loads(data)\r\n        s2_dir=\"%s/%s\"%(exp_root,exp_name)\r\n        os.makedirs(\"%s/logs_s2\"%(s2_dir),exist_ok=True)\r\n        if check_for_existance([s2_dir],is_train=True):\r\n            check_details([s2_dir],is_train=True)\r\n        if(is_half==False):\r\n            data[\"train\"][\"fp16_run\"]=False\r\n            batch_size=max(1,batch_size//2)\r\n        data[\"train\"][\"batch_size\"]=batch_size\r\n        data[\"train\"][\"epochs\"]=total_epoch\r\n        data[\"train\"][\"text_low_lr_rate\"]=text_low_lr_rate\r\n        data[\"train\"][\"pretrained_s2G\"]=pretrained_s2G\r\n        data[\"train\"][\"pretrained_s2D\"]=pretrained_s2D\r\n        data[\"train\"][\"if_save_latest\"]=if_save_latest\r\n        data[\"train\"][\"if_save_every_weights\"]=if_save_every_weights\r\n        data[\"train\"][\"save_every_epoch\"]=save_every_epoch\r\n        data[\"train\"][\"gpu_numbers\"]=gpu_numbers1Ba\r\n        data[\"model\"][\"version\"]=version\r\n        data[\"data\"][\"exp_dir\"]=data[\"s2_ckpt_dir\"]=s2_dir\r\n        data[\"save_weight_dir\"]=SoVITS_weight_root[-int(version[-1])+2]\r\n        data[\"name\"]=exp_name\r\n        data[\"version\"]=version\r\n        tmp_config_path=\"%s/tmp_s2.json\"%tmp\r\n        with open(tmp_config_path,\"w\")as f:f.write(json.dumps(data))\r\n\r\n        cmd = '\"%s\" GPT_SoVITS/s2_train.py --config \"%s\"'%(python_exec,tmp_config_path)\r\n        yield \"SoVITS训练开始：%s\"%cmd, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}\r\n        print(cmd)\r\n        p_train_SoVITS = Popen(cmd, shell=True)\r\n        p_train_SoVITS.wait()\r\n        p_train_SoVITS=None\r\n        yield \"SoVITS训练完成\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n    else:\r\n        yield \"已有正在进行的SoVITS训练任务，需先终止才能开启下一次任务\", {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}\r\n\r\ndef close1Ba():\r\n    global p_train_SoVITS\r\n    if(p_train_SoVITS!=None):\r\n        kill_process(p_train_SoVITS.pid)\r\n        p_train_SoVITS=None\r\n    return \"已终止SoVITS训练\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n\r\np_train_GPT=None\r\ndef open1Bb(batch_size,total_epoch,exp_name,if_dpo,if_save_latest,if_save_every_weights,save_every_epoch,gpu_numbers,pretrained_s1):\r\n    global p_train_GPT\r\n    if(p_train_GPT==None):\r\n        with open(\"GPT_SoVITS/configs/s1longer.yaml\"if version==\"v1\"else \"GPT_SoVITS/configs/s1longer-v2.yaml\")as f:\r\n            data=f.read()\r\n            data=yaml.load(data, Loader=yaml.FullLoader)\r\n        s1_dir=\"%s/%s\"%(exp_root,exp_name)\r\n        os.makedirs(\"%s/logs_s1\"%(s1_dir),exist_ok=True)\r\n        if check_for_existance([s1_dir],is_train=True):\r\n            check_details([s1_dir],is_train=True)\r\n        if(is_half==False):\r\n            data[\"train\"][\"precision\"]=\"32\"\r\n            batch_size = max(1, batch_size // 2)\r\n        data[\"train\"][\"batch_size\"]=batch_size\r\n        data[\"train\"][\"epochs\"]=total_epoch\r\n        data[\"pretrained_s1\"]=pretrained_s1\r\n        data[\"train\"][\"save_every_n_epoch\"]=save_every_epoch\r\n        data[\"train\"][\"if_save_every_weights\"]=if_save_every_weights\r\n        data[\"train\"][\"if_save_latest\"]=if_save_latest\r\n        data[\"train\"][\"if_dpo\"]=if_dpo\r\n        data[\"train\"][\"half_weights_save_dir\"]=GPT_weight_root[-int(version[-1])+2]\r\n        data[\"train\"][\"exp_name\"]=exp_name\r\n        data[\"train_semantic_path\"]=\"%s/6-name2semantic.tsv\"%s1_dir\r\n        data[\"train_phoneme_path\"]=\"%s/2-name2text.txt\"%s1_dir\r\n        data[\"output_dir\"]=\"%s/logs_s1\"%s1_dir\r\n        # data[\"version\"]=version\r\n\r\n        os.environ[\"_CUDA_VISIBLE_DEVICES\"]=fix_gpu_numbers(gpu_numbers.replace(\"-\",\",\"))\r\n        os.environ[\"hz\"]=\"25hz\"\r\n        tmp_config_path=\"%s/tmp_s1.yaml\"%tmp\r\n        with open(tmp_config_path, \"w\") as f:f.write(yaml.dump(data, default_flow_style=False))\r\n        # cmd = '\"%s\" GPT_SoVITS/s1_train.py --config_file \"%s\" --train_semantic_path \"%s/6-name2semantic.tsv\" --train_phoneme_path \"%s/2-name2text.txt\" --output_dir \"%s/logs_s1\"'%(python_exec,tmp_config_path,s1_dir,s1_dir,s1_dir)\r\n        cmd = '\"%s\" GPT_SoVITS/s1_train.py --config_file \"%s\" '%(python_exec,tmp_config_path)\r\n        yield \"GPT训练开始：%s\"%cmd, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}\r\n        print(cmd)\r\n        p_train_GPT = Popen(cmd, shell=True)\r\n        p_train_GPT.wait()\r\n        p_train_GPT=None\r\n        yield \"GPT训练完成\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n    else:\r\n        yield \"已有正在进行的GPT训练任务，需先终止才能开启下一次任务\", {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}\r\n\r\ndef close1Bb():\r\n    global p_train_GPT\r\n    if(p_train_GPT!=None):\r\n        kill_process(p_train_GPT.pid)\r\n        p_train_GPT=None\r\n    return \"已终止GPT训练\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n\r\nps_slice=[]\r\ndef open_slice(inp,opt_root,threshold,min_length,min_interval,hop_size,max_sil_kept,_max,alpha,n_parts):\r\n    global ps_slice\r\n    inp = my_utils.clean_path(inp)\r\n    opt_root = my_utils.clean_path(opt_root)\r\n    check_for_existance([inp])\r\n    if(os.path.exists(inp)==False):\r\n        yield \"输入路径不存在\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}\r\n        return\r\n    if os.path.isfile(inp):n_parts=1\r\n    elif os.path.isdir(inp):pass\r\n    else:\r\n        yield \"输入路径存在但既不是文件也不是文件夹\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}\r\n        return\r\n    if (ps_slice == []):\r\n        for i_part in range(n_parts):\r\n            cmd = '\"%s\" tools/slice_audio.py \"%s\" \"%s\" %s %s %s %s %s %s %s %s %s''' % (python_exec,inp, opt_root, threshold, min_length, min_interval, hop_size, max_sil_kept, _max, alpha, i_part, n_parts)\r\n            print(cmd)\r\n            p = Popen(cmd, shell=True)\r\n            ps_slice.append(p)\r\n        yield \"切割执行中\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}\r\n        for p in ps_slice:\r\n            p.wait()\r\n        ps_slice=[]\r\n        yield \"切割结束\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\": \"update\", \"value\":opt_root}, {\"__type__\": \"update\", \"value\":opt_root}, {\"__type__\": \"update\", \"value\":opt_root}\r\n    else:\r\n        yield \"已有正在进行的切割任务，需先终止才能开启下一次任务\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}\r\n\r\ndef close_slice():\r\n    global ps_slice\r\n    if (ps_slice != []):\r\n        for p_slice in ps_slice:\r\n            try:\r\n                kill_process(p_slice.pid)\r\n            except:\r\n                traceback.print_exc()\r\n        ps_slice=[]\r\n    return \"已终止所有切割进程\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n\r\nps1a=[]\r\ndef open1a(inp_text,inp_wav_dir,exp_name,gpu_numbers,bert_pretrained_dir):\r\n    global ps1a\r\n    inp_text = my_utils.clean_path(inp_text)\r\n    inp_wav_dir = my_utils.clean_path(inp_wav_dir)\r\n    if check_for_existance([inp_text,inp_wav_dir], is_dataset_processing=True):\r\n        check_details([inp_text,inp_wav_dir], is_dataset_processing=True)\r\n    if (ps1a == []):\r\n        opt_dir=\"%s/%s\"%(exp_root,exp_name)\r\n        config={\r\n            \"inp_text\":inp_text,\r\n            \"inp_wav_dir\":inp_wav_dir,\r\n            \"exp_name\":exp_name,\r\n            \"opt_dir\":opt_dir,\r\n            \"bert_pretrained_dir\":bert_pretrained_dir,\r\n        }\r\n        gpu_names=gpu_numbers.split(\"-\")\r\n        all_parts=len(gpu_names)\r\n        for i_part in range(all_parts):\r\n            config.update(\r\n                {\r\n                    \"i_part\": str(i_part),\r\n                    \"all_parts\": str(all_parts),\r\n                    \"_CUDA_VISIBLE_DEVICES\": fix_gpu_number(gpu_names[i_part]),\r\n                    \"is_half\": str(is_half)\r\n                }\r\n            )\r\n            os.environ.update(config)\r\n            cmd = '\"%s\" GPT_SoVITS/prepare_datasets/1-get-text.py'%python_exec\r\n            print(cmd)\r\n            p = Popen(cmd, shell=True)\r\n            ps1a.append(p)\r\n        yield \"文本进程执行中\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n        for p in ps1a:\r\n            p.wait()\r\n        opt = []\r\n        for i_part in range(all_parts):\r\n            txt_path = \"%s/2-name2text-%s.txt\" % (opt_dir, i_part)\r\n            with open(txt_path, \"r\", encoding=\"utf8\") as f:\r\n                opt += f.read().strip(\"\\n\").split(\"\\n\")\r\n            os.remove(txt_path)\r\n        path_text = \"%s/2-name2text.txt\" % opt_dir\r\n        with open(path_text, \"w\", encoding=\"utf8\") as f:\r\n            f.write(\"\\n\".join(opt) + \"\\n\")\r\n        ps1a=[]\r\n        if len(\"\".join(opt)) > 0:\r\n            yield \"文本进程成功\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n        else:\r\n            yield \"文本进程失败\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n    else:\r\n        yield \"已有正在进行的文本任务，需先终止才能开启下一次任务\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n\r\ndef close1a():\r\n    global ps1a\r\n    if (ps1a != []):\r\n        for p1a in ps1a:\r\n            try:\r\n                kill_process(p1a.pid)\r\n            except:\r\n                traceback.print_exc()\r\n        ps1a=[]\r\n    return \"已终止所有1a进程\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n\r\nps1b=[]\r\ndef open1b(inp_text,inp_wav_dir,exp_name,gpu_numbers,ssl_pretrained_dir):\r\n    global ps1b\r\n    inp_text = my_utils.clean_path(inp_text)\r\n    inp_wav_dir = my_utils.clean_path(inp_wav_dir)\r\n    if check_for_existance([inp_text,inp_wav_dir], is_dataset_processing=True):\r\n        check_details([inp_text,inp_wav_dir], is_dataset_processing=True)\r\n    if (ps1b == []):\r\n        config={\r\n            \"inp_text\":inp_text,\r\n            \"inp_wav_dir\":inp_wav_dir,\r\n            \"exp_name\":exp_name,\r\n            \"opt_dir\":\"%s/%s\"%(exp_root,exp_name),\r\n            \"cnhubert_base_dir\":ssl_pretrained_dir,\r\n            \"is_half\": str(is_half)\r\n        }\r\n        gpu_names=gpu_numbers.split(\"-\")\r\n        all_parts=len(gpu_names)\r\n        for i_part in range(all_parts):\r\n            config.update(\r\n                {\r\n                    \"i_part\": str(i_part),\r\n                    \"all_parts\": str(all_parts),\r\n                    \"_CUDA_VISIBLE_DEVICES\": fix_gpu_number(gpu_names[i_part]),\r\n                }\r\n            )\r\n            os.environ.update(config)\r\n            cmd = '\"%s\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py'%python_exec\r\n            print(cmd)\r\n            p = Popen(cmd, shell=True)\r\n            ps1b.append(p)\r\n        yield \"SSL提取进程执行中\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n        for p in ps1b:\r\n            p.wait()\r\n        ps1b=[]\r\n        yield \"SSL提取进程结束\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n    else:\r\n        yield \"已有正在进行的SSL提取任务，需先终止才能开启下一次任务\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n\r\ndef close1b():\r\n    global ps1b\r\n    if (ps1b != []):\r\n        for p1b in ps1b:\r\n            try:\r\n                kill_process(p1b.pid)\r\n            except:\r\n                traceback.print_exc()\r\n        ps1b=[]\r\n    return \"已终止所有1b进程\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n\r\nps1c=[]\r\ndef open1c(inp_text,exp_name,gpu_numbers,pretrained_s2G_path):\r\n    global ps1c\r\n    inp_text = my_utils.clean_path(inp_text)\r\n    if check_for_existance([inp_text,''], is_dataset_processing=True):\r\n        check_details([inp_text,''], is_dataset_processing=True)\r\n    if (ps1c == []):\r\n        opt_dir=\"%s/%s\"%(exp_root,exp_name)\r\n        config={\r\n            \"inp_text\":inp_text,\r\n            \"exp_name\":exp_name,\r\n            \"opt_dir\":opt_dir,\r\n            \"pretrained_s2G\":pretrained_s2G_path,\r\n            \"s2config_path\":\"GPT_SoVITS/configs/s2.json\",\r\n            \"is_half\": str(is_half)\r\n        }\r\n        gpu_names=gpu_numbers.split(\"-\")\r\n        all_parts=len(gpu_names)\r\n        for i_part in range(all_parts):\r\n            config.update(\r\n                {\r\n                    \"i_part\": str(i_part),\r\n                    \"all_parts\": str(all_parts),\r\n                    \"_CUDA_VISIBLE_DEVICES\": fix_gpu_number(gpu_names[i_part]),\r\n                }\r\n            )\r\n            os.environ.update(config)\r\n            cmd = '\"%s\" GPT_SoVITS/prepare_datasets/3-get-semantic.py'%python_exec\r\n            print(cmd)\r\n            p = Popen(cmd, shell=True)\r\n            ps1c.append(p)\r\n        yield \"语义token提取进程执行中\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n        for p in ps1c:\r\n            p.wait()\r\n        opt = [\"item_name\\tsemantic_audio\"]\r\n        path_semantic = \"%s/6-name2semantic.tsv\" % opt_dir\r\n        for i_part in range(all_parts):\r\n            semantic_path = \"%s/6-name2semantic-%s.tsv\" % (opt_dir, i_part)\r\n            with open(semantic_path, \"r\", encoding=\"utf8\") as f:\r\n                opt += f.read().strip(\"\\n\").split(\"\\n\")\r\n            os.remove(semantic_path)\r\n        with open(path_semantic, \"w\", encoding=\"utf8\") as f:\r\n            f.write(\"\\n\".join(opt) + \"\\n\")\r\n        ps1c=[]\r\n        yield \"语义token提取进程结束\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n    else:\r\n        yield \"已有正在进行的语义token提取任务，需先终止才能开启下一次任务\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n\r\ndef close1c():\r\n    global ps1c\r\n    if (ps1c != []):\r\n        for p1c in ps1c:\r\n            try:\r\n                kill_process(p1c.pid)\r\n            except:\r\n                traceback.print_exc()\r\n        ps1c=[]\r\n    return \"已终止所有语义token进程\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n#####inp_text,inp_wav_dir,exp_name,gpu_numbers1a,gpu_numbers1Ba,gpu_numbers1c,bert_pretrained_dir,cnhubert_base_dir,pretrained_s2G\r\nps1abc=[]\r\ndef open1abc(inp_text,inp_wav_dir,exp_name,gpu_numbers1a,gpu_numbers1Ba,gpu_numbers1c,bert_pretrained_dir,ssl_pretrained_dir,pretrained_s2G_path):\r\n    global ps1abc\r\n    inp_text = my_utils.clean_path(inp_text)\r\n    inp_wav_dir = my_utils.clean_path(inp_wav_dir)\r\n    if check_for_existance([inp_text,inp_wav_dir], is_dataset_processing=True):\r\n        check_details([inp_text,inp_wav_dir], is_dataset_processing=True)\r\n    if (ps1abc == []):\r\n        opt_dir=\"%s/%s\"%(exp_root,exp_name)\r\n        try:\r\n            #############################1a\r\n            path_text=\"%s/2-name2text.txt\" % opt_dir\r\n            if(os.path.exists(path_text)==False or (os.path.exists(path_text)==True and len(open(path_text,\"r\",encoding=\"utf8\").read().strip(\"\\n\").split(\"\\n\"))<2)):\r\n                config={\r\n                    \"inp_text\":inp_text,\r\n                    \"inp_wav_dir\":inp_wav_dir,\r\n                    \"exp_name\":exp_name,\r\n                    \"opt_dir\":opt_dir,\r\n                    \"bert_pretrained_dir\":bert_pretrained_dir,\r\n                    \"is_half\": str(is_half)\r\n                }\r\n                gpu_names=gpu_numbers1a.split(\"-\")\r\n                all_parts=len(gpu_names)\r\n                for i_part in range(all_parts):\r\n                    config.update(\r\n                        {\r\n                            \"i_part\": str(i_part),\r\n                            \"all_parts\": str(all_parts),\r\n                            \"_CUDA_VISIBLE_DEVICES\": fix_gpu_number(gpu_names[i_part]),\r\n                        }\r\n                    )\r\n                    os.environ.update(config)\r\n                    cmd = '\"%s\" GPT_SoVITS/prepare_datasets/1-get-text.py'%python_exec\r\n                    print(cmd)\r\n                    p = Popen(cmd, shell=True)\r\n                    ps1abc.append(p)\r\n                yield \"进度：1a-ing\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n                for p in ps1abc:p.wait()\r\n\r\n                opt = []\r\n                for i_part in range(all_parts):#txt_path=\"%s/2-name2text-%s.txt\"%(opt_dir,i_part)\r\n                    txt_path = \"%s/2-name2text-%s.txt\" % (opt_dir, i_part)\r\n                    with open(txt_path, \"r\",encoding=\"utf8\") as f:\r\n                        opt += f.read().strip(\"\\n\").split(\"\\n\")\r\n                    os.remove(txt_path)\r\n                with open(path_text, \"w\",encoding=\"utf8\") as f:\r\n                    f.write(\"\\n\".join(opt) + \"\\n\")\r\n                assert len(\"\".join(opt)) > 0, \"1Aa-文本获取进程失败\"\r\n            yield \"进度：1a-done\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n            ps1abc=[]\r\n            #############################1b\r\n            config={\r\n                \"inp_text\":inp_text,\r\n                \"inp_wav_dir\":inp_wav_dir,\r\n                \"exp_name\":exp_name,\r\n                \"opt_dir\":opt_dir,\r\n                \"cnhubert_base_dir\":ssl_pretrained_dir,\r\n            }\r\n            gpu_names=gpu_numbers1Ba.split(\"-\")\r\n            all_parts=len(gpu_names)\r\n            for i_part in range(all_parts):\r\n                config.update(\r\n                    {\r\n                        \"i_part\": str(i_part),\r\n                        \"all_parts\": str(all_parts),\r\n                        \"_CUDA_VISIBLE_DEVICES\": fix_gpu_number(gpu_names[i_part]),\r\n                    }\r\n                )\r\n                os.environ.update(config)\r\n                cmd = '\"%s\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py'%python_exec\r\n                print(cmd)\r\n                p = Popen(cmd, shell=True)\r\n                ps1abc.append(p)\r\n            yield \"进度：1a-done, 1b-ing\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n            for p in ps1abc:p.wait()\r\n            yield \"进度：1a1b-done\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n            ps1abc=[]\r\n            #############################1c\r\n            path_semantic = \"%s/6-name2semantic.tsv\" % opt_dir\r\n            if(os.path.exists(path_semantic)==False or (os.path.exists(path_semantic)==True and os.path.getsize(path_semantic)<31)):\r\n                config={\r\n                    \"inp_text\":inp_text,\r\n                    \"exp_name\":exp_name,\r\n                    \"opt_dir\":opt_dir,\r\n                    \"pretrained_s2G\":pretrained_s2G_path,\r\n                    \"s2config_path\":\"GPT_SoVITS/configs/s2.json\",\r\n                }\r\n                gpu_names=gpu_numbers1c.split(\"-\")\r\n                all_parts=len(gpu_names)\r\n                for i_part in range(all_parts):\r\n                    config.update(\r\n                        {\r\n                            \"i_part\": str(i_part),\r\n                            \"all_parts\": str(all_parts),\r\n                            \"_CUDA_VISIBLE_DEVICES\": fix_gpu_number(gpu_names[i_part]),\r\n                        }\r\n                    )\r\n                    os.environ.update(config)\r\n                    cmd = '\"%s\" GPT_SoVITS/prepare_datasets/3-get-semantic.py'%python_exec\r\n                    print(cmd)\r\n                    p = Popen(cmd, shell=True)\r\n                    ps1abc.append(p)\r\n                yield \"进度：1a1b-done, 1cing\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n                for p in ps1abc:p.wait()\r\n\r\n                opt = [\"item_name\\tsemantic_audio\"]\r\n                for i_part in range(all_parts):\r\n                    semantic_path = \"%s/6-name2semantic-%s.tsv\" % (opt_dir, i_part)\r\n                    with open(semantic_path, \"r\",encoding=\"utf8\") as f:\r\n                        opt += f.read().strip(\"\\n\").split(\"\\n\")\r\n                    os.remove(semantic_path)\r\n                with open(path_semantic, \"w\",encoding=\"utf8\") as f:\r\n                    f.write(\"\\n\".join(opt) + \"\\n\")\r\n                yield \"进度：all-done\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n            ps1abc = []\r\n            yield \"一键三连进程结束\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n        except:\r\n            traceback.print_exc()\r\n            close1abc()\r\n            yield \"一键三连中途报错\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n    else:\r\n        yield \"已有正在进行的一键三连任务，需先终止才能开启下一次任务\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n\r\ndef close1abc():\r\n    global ps1abc\r\n    if (ps1abc != []):\r\n        for p1abc in ps1abc:\r\n            try:\r\n                kill_process(p1abc.pid)\r\n            except:\r\n                traceback.print_exc()\r\n        ps1abc=[]\r\n    return \"已终止所有一键三连进程\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n\r\ndef switch_version(version_):\r\n    os.environ['version']=version_\r\n    global version\r\n    version = version_\r\n    if pretrained_sovits_name[-int(version[-1])+2] !='' and pretrained_gpt_name[-int(version[-1])+2] !='':...\r\n    else:   \r\n        gr.Warning(i18n(f'未下载{version.upper()}模型'))\r\n    return  {'__type__':'update', 'value':pretrained_sovits_name[-int(version[-1])+2]}, {'__type__':'update', 'value':pretrained_sovits_name[-int(version[-1])+2].replace(\"s2G\",\"s2D\")}, {'__type__':'update', 'value':pretrained_gpt_name[-int(version[-1])+2]}, {'__type__':'update', 'value':pretrained_gpt_name[-int(version[-1])+2]}, {'__type__':'update', 'value':pretrained_sovits_name[-int(version[-1])+2]}\r\n\r\nif os.path.exists('GPT_SoVITS/text/G2PWModel'):...\r\nelse:\r\n    cmd = '\"%s\" GPT_SoVITS/download.py'%python_exec\r\n    p = Popen(cmd, shell=True)\r\n    p.wait()\r\n\r\ndef sync(text):\r\n    return {'__type__':'update','value':text}\r\nwith gr.Blocks(title=\"GPT-SoVITS WebUI\") as app:\r\n    gr.Markdown(\r\n        value=\r\n            i18n(\"本软件以MIT协议开源, 作者不对软件具备任何控制力, 使用软件者、传播软件导出的声音者自负全责. <br>如不认可该条款, 则不能使用或引用软件包内任何代码和文件. 详见根目录<b>LICENSE</b>.\")\r\n    )\r\n    gr.Markdown(\r\n        value=\r\n            i18n(\"中文教程文档：https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e\")\r\n    )\r\n\r\n    with gr.Tabs():\r\n        with gr.TabItem(i18n(\"0-前置数据集获取工具\")):#提前随机切片防止uvr5爆内存->uvr5->slicer->asr->打标\r\n            gr.Markdown(value=i18n(\"0a-UVR5人声伴奏分离&去混响去延迟工具\"))\r\n            with gr.Row():\r\n                with gr.Column(scale=3):\r\n                    with gr.Row():\r\n                        uvr5_info = gr.Textbox(label=i18n(\"UVR5进程输出信息\"))\r\n                open_uvr5 = gr.Button(value=i18n(\"开启UVR5-WebUI\"),variant=\"primary\",visible=True)\r\n                close_uvr5 = gr.Button(value=i18n(\"关闭UVR5-WebUI\"),variant=\"primary\",visible=False)\r\n            gr.Markdown(value=i18n(\"0b-语音切分工具\"))\r\n            with gr.Row():\r\n                with gr.Column(scale=3):\r\n                    with gr.Row():\r\n                        slice_inp_path=gr.Textbox(label=i18n(\"音频自动切分输入路径，可文件可文件夹\"),value=\"\")\r\n                        slice_opt_root=gr.Textbox(label=i18n(\"切分后的子音频的输出根目录\"),value=\"output/slicer_opt\")\r\n                    with gr.Row():\r\n                        threshold=gr.Textbox(label=i18n(\"threshold:音量小于这个值视作静音的备选切割点\"),value=\"-34\")\r\n                        min_length=gr.Textbox(label=i18n(\"min_length:每段最小多长，如果第一段太短一直和后面段连起来直到超过这个值\"),value=\"4000\")\r\n                        min_interval=gr.Textbox(label=i18n(\"min_interval:最短切割间隔\"),value=\"300\")\r\n                        hop_size=gr.Textbox(label=i18n(\"hop_size:怎么算音量曲线，越小精度越大计算量越高（不是精度越大效果越好）\"),value=\"10\")\r\n                        max_sil_kept=gr.Textbox(label=i18n(\"max_sil_kept:切完后静音最多留多长\"),value=\"500\")\r\n                    with gr.Row():\r\n                        _max=gr.Slider(minimum=0,maximum=1,step=0.05,label=i18n(\"max:归一化后最大值多少\"),value=0.9,interactive=True)\r\n                        alpha=gr.Slider(minimum=0,maximum=1,step=0.05,label=i18n(\"alpha_mix:混多少比例归一化后音频进来\"),value=0.25,interactive=True)   \r\n                    with gr.Row():\r\n                        n_process=gr.Slider(minimum=1,maximum=n_cpu,step=1,label=i18n(\"切割使用的进程数\"),value=4,interactive=True)\r\n                        slicer_info = gr.Textbox(label=i18n(\"语音切割进程输出信息\"))\r\n                open_slicer_button=gr.Button(i18n(\"开启语音切割\"), variant=\"primary\",visible=True)\r\n                close_slicer_button=gr.Button(i18n(\"终止语音切割\"), variant=\"primary\",visible=False)\r\n            gr.Markdown(value=i18n(\"0bb-语音降噪工具\"))\r\n            with gr.Row():\r\n                with gr.Column(scale=3):\r\n                    with gr.Row():\r\n                        denoise_input_dir=gr.Textbox(label=i18n(\"降噪音频文件输入文件夹\"),value=\"\")\r\n                        denoise_output_dir=gr.Textbox(label=i18n(\"降噪结果输出文件夹\"),value=\"output/denoise_opt\")\r\n                    with gr.Row():\r\n                        denoise_info = gr.Textbox(label=i18n(\"语音降噪进程输出信息\"))\r\n                open_denoise_button = gr.Button(i18n(\"开启语音降噪\"), variant=\"primary\",visible=True)\r\n                close_denoise_button = gr.Button(i18n(\"终止语音降噪进程\"), variant=\"primary\",visible=False)\r\n            gr.Markdown(value=i18n(\"0c-中文批量离线ASR工具\"))\r\n            with gr.Row():\r\n                with gr.Column(scale=3):\r\n                    with gr.Row():\r\n                        asr_inp_dir = gr.Textbox(\r\n                            label=i18n(\"输入文件夹路径\"),\r\n                            value=\"D:\\\\GPT-SoVITS\\\\raw\\\\xxx\",\r\n                            interactive=True,\r\n                        )\r\n                        asr_opt_dir = gr.Textbox(\r\n                            label       = i18n(\"输出文件夹路径\"),\r\n                            value       = \"output/asr_opt\",\r\n                            interactive = True,\r\n                        )\r\n                    with gr.Row():\r\n                        asr_model = gr.Dropdown(\r\n                            label       = i18n(\"ASR 模型\"),\r\n                            choices     = list(asr_dict.keys()),\r\n                            interactive = True,\r\n                            value=\"达摩 ASR (中文)\"\r\n                        )\r\n                        asr_size = gr.Dropdown(\r\n                            label       = i18n(\"ASR 模型尺寸\"),\r\n                            choices     = [\"large\"],\r\n                            interactive = True,\r\n                            value=\"large\"\r\n                        )\r\n                        asr_lang = gr.Dropdown(\r\n                            label       = i18n(\"ASR 语言设置\"),\r\n                            choices     = [\"zh\",\"yue\"],\r\n                            interactive = True,\r\n                            value=\"zh\"\r\n                        )\r\n                        asr_precision = gr.Dropdown(\r\n                            label       = i18n(\"数据类型精度\"),\r\n                            choices     = [\"float32\"],\r\n                            interactive = True,\r\n                            value=\"float32\"\r\n                        ) \r\n                    with gr.Row():\r\n                        asr_info = gr.Textbox(label=i18n(\"ASR进程输出信息\"))     \r\n                open_asr_button = gr.Button(i18n(\"开启离线批量ASR\"), variant=\"primary\",visible=True)\r\n                close_asr_button = gr.Button(i18n(\"终止ASR进程\"), variant=\"primary\",visible=False)                  \r\n\r\n                def change_lang_choices(key): #根据选择的模型修改可选的语言\r\n                    # return gr.Dropdown(choices=asr_dict[key]['lang'])\r\n                    return {\"__type__\": \"update\", \"choices\": asr_dict[key]['lang'],\"value\":asr_dict[key]['lang'][0]}\r\n                def change_size_choices(key): # 根据选择的模型修改可选的模型尺寸\r\n                    # return gr.Dropdown(choices=asr_dict[key]['size'])\r\n                    return {\"__type__\": \"update\", \"choices\": asr_dict[key]['size'],\"value\":asr_dict[key]['size'][-1]}\r\n                def change_precision_choices(key): #根据选择的模型修改可选的语言\r\n                    if key ==\"Faster Whisper (多语种)\":\r\n                        if default_batch_size <= 4:\r\n                            precision = 'int8'\r\n                        elif is_half:\r\n                            precision = 'float16'\r\n                        else:\r\n                            precision = 'float32'\r\n                    else:\r\n                        precision = 'float32'\r\n                    # return gr.Dropdown(choices=asr_dict[key]['precision'])\r\n                    return {\"__type__\": \"update\", \"choices\": asr_dict[key]['precision'],\"value\":precision}\r\n                asr_model.change(change_lang_choices, [asr_model], [asr_lang])\r\n                asr_model.change(change_size_choices, [asr_model], [asr_size])\r\n                asr_model.change(change_precision_choices, [asr_model], [asr_precision])\r\n\r\n                \r\n            gr.Markdown(value=i18n(\"0d-语音文本校对标注工具\"))\r\n            with gr.Row():\r\n                with gr.Column(scale=3):\r\n                    with gr.Row():\r\n                        path_list = gr.Textbox(\r\n                        label=i18n(\".list标注文件的路径\"),\r\n                        value=\"D:\\\\RVC1006\\\\GPT-SoVITS\\\\raw\\\\xxx.list\",\r\n                        interactive=True,\r\n                    )\r\n                        label_info = gr.Textbox(label=i18n(\"打标工具进程输出信息\"))\r\n                \r\n                open_label = gr.Button(value=i18n(\"开启打标WebUI\"),variant=\"primary\",visible=True)\r\n                close_label = gr.Button(value=i18n(\"关闭打标WebUI\"),variant=\"primary\",visible=False)\r\n            open_label.click(change_label, [path_list], [label_info,open_label,close_label])\r\n            close_label.click(change_label, [path_list], [label_info,open_label,close_label])\r\n            open_uvr5.click(change_uvr5, [], [uvr5_info,open_uvr5,close_uvr5])\r\n            close_uvr5.click(change_uvr5, [], [uvr5_info,open_uvr5,close_uvr5])\r\n\r\n        with gr.TabItem(i18n(\"1-GPT-SoVITS-TTS\")):\r\n            with gr.Row():\r\n                with gr.Row():\r\n                    exp_name = gr.Textbox(label=i18n(\"*实验/模型名\"), value=\"xxx\", interactive=True)\r\n                    gpu_info = gr.Textbox(label=i18n(\"显卡信息\"), value=gpu_info, visible=True, interactive=False)\r\n                    version_checkbox = gr.Radio(label=i18n(\"版本\"),value=version,choices=['v1','v2'])\r\n                with gr.Row():\r\n                    pretrained_s2G = gr.Textbox(label=i18n(\"预训练的SoVITS-G模型路径\"), value=pretrained_sovits_name[-int(version[-1])+2], interactive=True, lines=2, max_lines=3,scale=9)\r\n                    pretrained_s2D = gr.Textbox(label=i18n(\"预训练的SoVITS-D模型路径\"), value=pretrained_sovits_name[-int(version[-1])+2].replace(\"s2G\",\"s2D\"), interactive=True, lines=2, max_lines=3,scale=9)\r\n                    pretrained_s1 = gr.Textbox(label=i18n(\"预训练的GPT模型路径\"), value=pretrained_gpt_name[-int(version[-1])+2], interactive=True, lines=2, max_lines=3,scale=10)\r\n            with gr.TabItem(i18n(\"1A-训练集格式化工具\")):\r\n                gr.Markdown(value=i18n(\"输出logs/实验名目录下应有23456开头的文件和文件夹\"))\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        inp_text = gr.Textbox(label=i18n(\"*文本标注文件\"),value=r\"D:\\RVC1006\\GPT-SoVITS\\raw\\xxx.list\",interactive=True,scale=10)\r\n                    with gr.Row():\r\n                        inp_wav_dir = gr.Textbox(\r\n                            label=i18n(\"*训练集音频文件目录\"),\r\n                            # value=r\"D:\\RVC1006\\GPT-SoVITS\\raw\\xxx\",\r\n                            interactive=True,\r\n                            placeholder=i18n(\"填切割后音频所在目录！读取的音频文件完整路径=该目录-拼接-list文件里波形对应的文件名（不是全路径）。如果留空则使用.list文件里的绝对全路径。\"), scale=10\r\n                        )\r\n                gr.Markdown(value=i18n(\"1Aa-文本内容\"))\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        gpu_numbers1a = gr.Textbox(label=i18n(\"GPU卡号以-分割，每个卡号一个进程\"),value=\"%s-%s\"%(gpus,gpus),interactive=True)\r\n                    with gr.Row():\r\n                        bert_pretrained_dir = gr.Textbox(label=i18n(\"预训练的中文BERT模型路径\"),value=\"GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\",interactive=False,lines=2)\r\n                    with gr.Row():\r\n                        button1a_open = gr.Button(i18n(\"开启文本获取\"), variant=\"primary\",visible=True)\r\n                        button1a_close = gr.Button(i18n(\"终止文本获取进程\"), variant=\"primary\",visible=False)\r\n                    with gr.Row():\r\n                        info1a=gr.Textbox(label=i18n(\"文本进程输出信息\"))\r\n                gr.Markdown(value=i18n(\"1Ab-SSL自监督特征提取\"))\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        gpu_numbers1Ba = gr.Textbox(label=i18n(\"GPU卡号以-分割，每个卡号一个进程\"),value=\"%s-%s\"%(gpus,gpus),interactive=True)\r\n                    with gr.Row():\r\n                        cnhubert_base_dir = gr.Textbox(label=i18n(\"预训练的SSL模型路径\"),value=\"GPT_SoVITS/pretrained_models/chinese-hubert-base\",interactive=False,lines=2)\r\n                    with gr.Row():\r\n                        button1b_open = gr.Button(i18n(\"开启SSL提取\"), variant=\"primary\",visible=True)\r\n                        button1b_close = gr.Button(i18n(\"终止SSL提取进程\"), variant=\"primary\",visible=False)\r\n                    with gr.Row():\r\n                        info1b=gr.Textbox(label=i18n(\"SSL进程输出信息\"))\r\n                gr.Markdown(value=i18n(\"1Ac-语义token提取\"))\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        gpu_numbers1c = gr.Textbox(label=i18n(\"GPU卡号以-分割，每个卡号一个进程\"),value=\"%s-%s\"%(gpus,gpus),interactive=True)\r\n                    with gr.Row():\r\n                        pretrained_s2G_ = gr.Textbox(label=i18n(\"预训练的SoVITS-G模型路径\"), value=pretrained_sovits_name[-int(version[-1])+2], interactive=False,lines=2)\r\n                    with gr.Row():\r\n                        button1c_open = gr.Button(i18n(\"开启语义token提取\"), variant=\"primary\",visible=True)\r\n                        button1c_close = gr.Button(i18n(\"终止语义token提取进程\"), variant=\"primary\",visible=False)\r\n                    with gr.Row():\r\n                        info1c=gr.Textbox(label=i18n(\"语义token提取进程输出信息\"))\r\n                gr.Markdown(value=i18n(\"1Aabc-训练集格式化一键三连\"))\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        button1abc_open = gr.Button(i18n(\"开启一键三连\"), variant=\"primary\",visible=True)\r\n                        button1abc_close = gr.Button(i18n(\"终止一键三连\"), variant=\"primary\",visible=False)\r\n                    with gr.Row():\r\n                        info1abc=gr.Textbox(label=i18n(\"一键三连进程输出信息\"))\r\n\r\n            pretrained_s2G.change(sync,[pretrained_s2G],[pretrained_s2G_])\r\n            open_asr_button.click(open_asr, [asr_inp_dir, asr_opt_dir, asr_model, asr_size, asr_lang, asr_precision], [asr_info,open_asr_button,close_asr_button,path_list,inp_text,inp_wav_dir])\r\n            close_asr_button.click(close_asr, [], [asr_info,open_asr_button,close_asr_button])\r\n            open_slicer_button.click(open_slice, [slice_inp_path,slice_opt_root,threshold,min_length,min_interval,hop_size,max_sil_kept,_max,alpha,n_process], [slicer_info,open_slicer_button,close_slicer_button,asr_inp_dir,denoise_input_dir,inp_wav_dir])\r\n            close_slicer_button.click(close_slice, [], [slicer_info,open_slicer_button,close_slicer_button])\r\n            open_denoise_button.click(open_denoise, [denoise_input_dir,denoise_output_dir], [denoise_info,open_denoise_button,close_denoise_button,asr_inp_dir,inp_wav_dir])\r\n            close_denoise_button.click(close_denoise, [], [denoise_info,open_denoise_button,close_denoise_button])\r\n            \r\n            button1a_open.click(open1a, [inp_text,inp_wav_dir,exp_name,gpu_numbers1a,bert_pretrained_dir], [info1a,button1a_open,button1a_close])\r\n            button1a_close.click(close1a, [], [info1a,button1a_open,button1a_close])\r\n            button1b_open.click(open1b, [inp_text,inp_wav_dir,exp_name,gpu_numbers1Ba,cnhubert_base_dir], [info1b,button1b_open,button1b_close])\r\n            button1b_close.click(close1b, [], [info1b,button1b_open,button1b_close])\r\n            button1c_open.click(open1c, [inp_text,exp_name,gpu_numbers1c,pretrained_s2G], [info1c,button1c_open,button1c_close])\r\n            button1c_close.click(close1c, [], [info1c,button1c_open,button1c_close])\r\n            button1abc_open.click(open1abc, [inp_text,inp_wav_dir,exp_name,gpu_numbers1a,gpu_numbers1Ba,gpu_numbers1c,bert_pretrained_dir,cnhubert_base_dir,pretrained_s2G], [info1abc,button1abc_open,button1abc_close])\r\n            button1abc_close.click(close1abc, [], [info1abc,button1abc_open,button1abc_close])\r\n            with gr.TabItem(i18n(\"1B-微调训练\")):\r\n                gr.Markdown(value=i18n(\"1Ba-SoVITS训练。用于分享的模型文件输出在SoVITS_weights下。\"))\r\n                with gr.Row():\r\n                    with gr.Column():\r\n                        with gr.Row():\r\n                            batch_size = gr.Slider(minimum=1,maximum=40,step=1,label=i18n(\"每张显卡的batch_size\"),value=default_batch_size,interactive=True)\r\n                            total_epoch = gr.Slider(minimum=1,maximum=25,step=1,label=i18n(\"总训练轮数total_epoch，不建议太高\"),value=8,interactive=True)\r\n                        with gr.Row():\r\n                            text_low_lr_rate = gr.Slider(minimum=0.2,maximum=0.6,step=0.05,label=i18n(\"文本模块学习率权重\"),value=0.4,interactive=True)\r\n                            save_every_epoch = gr.Slider(minimum=1,maximum=25,step=1,label=i18n(\"保存频率save_every_epoch\"),value=4,interactive=True)\r\n                    with gr.Column():     \r\n                        with gr.Column():                   \r\n                            if_save_latest = gr.Checkbox(label=i18n(\"是否仅保存最新的ckpt文件以节省硬盘空间\"), value=True, interactive=True, show_label=True)\r\n                            if_save_every_weights = gr.Checkbox(label=i18n(\"是否在每次保存时间点将最终小模型保存至weights文件夹\"), value=True, interactive=True, show_label=True)\r\n                        with gr.Row():\r\n                            gpu_numbers1Ba = gr.Textbox(label=i18n(\"GPU卡号以-分割，每个卡号一个进程\"), value=\"%s\" % (gpus), interactive=True)\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        button1Ba_open = gr.Button(i18n(\"开启SoVITS训练\"), variant=\"primary\",visible=True)\r\n                        button1Ba_close = gr.Button(i18n(\"终止SoVITS训练\"), variant=\"primary\",visible=False)\r\n                    with gr.Row():\r\n                        info1Ba=gr.Textbox(label=i18n(\"SoVITS训练进程输出信息\"))\r\n                gr.Markdown(value=i18n(\"1Bb-GPT训练。用于分享的模型文件输出在GPT_weights下。\"))\r\n                with gr.Row():\r\n                    with gr.Column():\r\n                        with gr.Row():\r\n                            batch_size1Bb = gr.Slider(minimum=1,maximum=40,step=1,label=i18n(\"每张显卡的batch_size\"),value=default_batch_size,interactive=True)\r\n                            total_epoch1Bb = gr.Slider(minimum=2,maximum=50,step=1,label=i18n(\"总训练轮数total_epoch\"),value=15,interactive=True)\r\n                        with gr.Row():\r\n                            save_every_epoch1Bb = gr.Slider(minimum=1,maximum=50,step=1,label=i18n(\"保存频率save_every_epoch\"),value=5,interactive=True) \r\n                            if_dpo = gr.Checkbox(label=i18n(\"是否开启dpo训练选项(实验性)\"), value=False, interactive=True, show_label=True)   \r\n                    with gr.Column():\r\n                        with gr.Column():\r\n                                if_save_latest1Bb = gr.Checkbox(label=i18n(\"是否仅保存最新的ckpt文件以节省硬盘空间\"), value=True, interactive=True, show_label=True)\r\n                                if_save_every_weights1Bb = gr.Checkbox(label=i18n(\"是否在每次保存时间点将最终小模型保存至weights文件夹\"), value=True, interactive=True, show_label=True)\r\n                        with gr.Row():                            \r\n                            gpu_numbers1Bb = gr.Textbox(label=i18n(\"GPU卡号以-分割，每个卡号一个进程\"), value=\"%s\" % (gpus), interactive=True)\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        button1Bb_open = gr.Button(i18n(\"开启GPT训练\"), variant=\"primary\",visible=True)\r\n                        button1Bb_close = gr.Button(i18n(\"终止GPT训练\"), variant=\"primary\",visible=False)\r\n                    with gr.Row():\r\n                        info1Bb=gr.Textbox(label=i18n(\"GPT训练进程输出信息\"))\r\n            button1Ba_open.click(open1Ba, [batch_size,total_epoch,exp_name,text_low_lr_rate,if_save_latest,if_save_every_weights,save_every_epoch,gpu_numbers1Ba,pretrained_s2G,pretrained_s2D], [info1Ba,button1Ba_open,button1Ba_close])\r\n            button1Ba_close.click(close1Ba, [], [info1Ba,button1Ba_open,button1Ba_close])\r\n            button1Bb_open.click(open1Bb, [batch_size1Bb,total_epoch1Bb,exp_name,if_dpo,if_save_latest1Bb,if_save_every_weights1Bb,save_every_epoch1Bb,gpu_numbers1Bb,pretrained_s1],   [info1Bb,button1Bb_open,button1Bb_close])\r\n            button1Bb_close.click(close1Bb, [], [info1Bb,button1Bb_open,button1Bb_close])\r\n            with gr.TabItem(i18n(\"1C-推理\")):\r\n                gr.Markdown(value=i18n(\"选择训练完存放在SoVITS_weights和GPT_weights下的模型。默认的一个是底模，体验5秒Zero Shot TTS用。\"))\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        GPT_dropdown = gr.Dropdown(label=i18n(\"*GPT模型列表\"), choices=sorted(GPT_names,key=custom_sort_key),value=pretrained_gpt_name[0],interactive=True)\r\n                        SoVITS_dropdown = gr.Dropdown(label=i18n(\"*SoVITS模型列表\"), choices=sorted(SoVITS_names,key=custom_sort_key),value=pretrained_sovits_name[0],interactive=True)\r\n                    with gr.Row():\r\n                        gpu_number_1C=gr.Textbox(label=i18n(\"GPU卡号,只能填1个整数\"), value=gpus, interactive=True)\r\n                        refresh_button = gr.Button(i18n(\"刷新模型路径\"), variant=\"primary\")\r\n                    refresh_button.click(fn=change_choices,inputs=[],outputs=[SoVITS_dropdown,GPT_dropdown])\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        batched_infer_enabled = gr.Checkbox(label=i18n(\"启用并行推理版本(推理速度更快)\"), value=False, interactive=True, show_label=True)\r\n                    with gr.Row():\r\n                        open_tts = gr.Button(value=i18n(\"开启TTS推理WebUI\"),variant='primary',visible=True)\r\n                        close_tts = gr.Button(value=i18n(\"关闭TTS推理WebUI\"),variant='primary',visible=False)\r\n                    with gr.Row():\r\n                        tts_info = gr.Textbox(label=i18n(\"TTS推理WebUI进程输出信息\"))\r\n                    open_tts.click(change_tts_inference, [bert_pretrained_dir,cnhubert_base_dir,gpu_number_1C,GPT_dropdown,SoVITS_dropdown, batched_infer_enabled], [tts_info,open_tts,close_tts])\r\n                    close_tts.click(change_tts_inference, [bert_pretrained_dir,cnhubert_base_dir,gpu_number_1C,GPT_dropdown,SoVITS_dropdown, batched_infer_enabled], [tts_info,open_tts,close_tts])\r\n            version_checkbox.change(switch_version,[version_checkbox],[pretrained_s2G,pretrained_s2D,pretrained_s1,GPT_dropdown,SoVITS_dropdown])\r\n        with gr.TabItem(i18n(\"2-GPT-SoVITS-变声\")):gr.Markdown(value=i18n(\"施工中，请静候佳音\"))\r\n    app.queue().launch(#concurrency_count=511, max_size=1022\r\n        server_name=\"0.0.0.0\",\r\n        inbrowser=True,\r\n        share=is_share,\r\n        server_port=webui_port_main,\r\n        quiet=True,\r\n    )\r\n"
        }
      ]
    }
  ]
}