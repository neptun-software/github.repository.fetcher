{
  "metadata": {
    "timestamp": 1736561395785,
    "page": 24,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "RVC-Boss/GPT-SoVITS",
      "stars": 38536,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0625,
          "content": "docs\nlogs\noutput\nreference\nSoVITS_weights\nGPT_weights\nTEMP\n.git\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1650390625,
          "content": ".DS_Store\n.vscode\n__pycache__\n*.pyc\nenv\nruntime\n.idea\noutput\nlogs\nreference\nGPT_weights\nSoVITS_weights\nGPT_weights_v2\nSoVITS_weights_v2\nTEMP\nweight.json\nffmpeg*\nffprobe*"
        },
        {
          "name": "Docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.271484375,
          "content": "# Base CUDA image\nFROM cnstark/pytorch:2.0.1-py3.9.17-cuda11.8.0-ubuntu20.04\n\nLABEL maintainer=\"breakstring@hotmail.com\"\nLABEL version=\"dev-20240209\"\nLABEL description=\"Docker image for GPT-SoVITS\"\n\n\n# Install 3rd party apps\nENV DEBIAN_FRONTEND=noninteractive\nENV TZ=Etc/UTC\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends tzdata ffmpeg libsox-dev parallel aria2 git git-lfs && \\\n    git lfs install && \\\n    rm -rf /var/lib/apt/lists/*\n\n# Copy only requirements.txt initially to leverage Docker cache\nWORKDIR /workspace\nCOPY requirements.txt /workspace/\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Define a build-time argument for image type\nARG IMAGE_TYPE=full\n\n# Conditional logic based on the IMAGE_TYPE argument\n# Always copy the Docker directory, but only use it if IMAGE_TYPE is not \"elite\"\nCOPY ./Docker /workspace/Docker \n# elite Á±ªÂûãÁöÑÈïúÂÉèÈáåÈù¢‰∏çÂåÖÂê´È¢ùÂ§ñÁöÑÊ®°Âûã\nRUN if [ \"$IMAGE_TYPE\" != \"elite\" ]; then \\\n        chmod +x /workspace/Docker/download.sh && \\\n        /workspace/Docker/download.sh && \\\n        python /workspace/Docker/download.py && \\\n        python -m nltk.downloader averaged_perceptron_tagger cmudict; \\\n    fi\n\n\n# Copy the rest of the application\nCOPY . /workspace\n\nEXPOSE 9871 9872 9873 9874 9880\n\nCMD [\"python\", \"webui.py\"]\n"
        },
        {
          "name": "GPT_SoVITS",
          "type": "tree",
          "content": null
        },
        {
          "name": "GPT_SoVITS_Inference.ipynb",
          "type": "blob",
          "size": 5.53515625,
          "content": "{\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0,\n  \"metadata\": {\n    \"colab\": {\n      \"provenance\": []\n    },\n    \"kernelspec\": {\n      \"name\": \"python3\",\n      \"display_name\": \"Python 3\"\n    },\n    \"accelerator\": \"GPU\"\n  },\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"# Credits for bubarino giving me the huggingface import code (ÊÑüË∞¢ bubarino Áªô‰∫ÜÊàë huggingface ÂØºÂÖ•‰ª£Á†Å)\"\n      ],\n      \"metadata\": {\n        \"id\": \"himHYZmra7ix\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"e9b7iFV3dm1f\"\n      },\n      \"source\": [\n        \"!git clone https://github.com/RVC-Boss/GPT-SoVITS.git\\n\",\n        \"%cd GPT-SoVITS\\n\",\n        \"!apt-get update && apt-get install -y --no-install-recommends tzdata ffmpeg libsox-dev parallel aria2 git git-lfs && git lfs install\\n\",\n        \"!pip install -r requirements.txt\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# @title Download pretrained models ‰∏ãËΩΩÈ¢ÑËÆ≠ÁªÉÊ®°Âûã\\n\",\n        \"!mkdir -p /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\\n\",\n        \"!mkdir -p /content/GPT-SoVITS/tools/damo_asr/models\\n\",\n        \"!mkdir -p /content/GPT-SoVITS/tools/uvr5\\n\",\n        \"%cd /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\\n\",\n        \"!git clone https://huggingface.co/lj1995/GPT-SoVITS\\n\",\n        \"%cd /content/GPT-SoVITS/tools/damo_asr/models\\n\",\n        \"!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\\n\",\n        \"!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\\n\",\n        \"!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\\n\",\n        \"# @title UVR5 pretrains ÂÆâË£Öuvr5Ê®°Âûã\\n\",\n        \"%cd /content/GPT-SoVITS/tools/uvr5\\n\",\n        \"!git clone https://huggingface.co/Delik/uvr5_weights\\n\",\n        \"!git config core.sparseCheckout true\\n\",\n        \"!mv /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/\"\n      ],\n      \"metadata\": {\n        \"id\": \"0NgxXg5sjv7z\",\n        \"cellView\": \"form\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"#@title Create folder models ÂàõÂª∫Êñá‰ª∂Â§πÊ®°Âûã\\n\",\n        \"import os\\n\",\n        \"base_directory = \\\"/content/GPT-SoVITS\\\"\\n\",\n        \"folder_names = [\\\"SoVITS_weights\\\", \\\"GPT_weights\\\"]\\n\",\n        \"\\n\",\n        \"for folder_name in folder_names:\\n\",\n        \"  if os.path.exists(os.path.join(base_directory, folder_name)):\\n\",\n        \"    print(f\\\"The folder '{folder_name}' already exists. (Êñá‰ª∂Â§π'{folder_name}'Â∑≤ÁªèÂ≠òÂú®„ÄÇ)\\\")\\n\",\n        \"  else:\\n\",\n        \"    os.makedirs(os.path.join(base_directory, folder_name))\\n\",\n        \"    print(f\\\"The folder '{folder_name}' was created successfully! (Êñá‰ª∂Â§π'{folder_name}'Â∑≤ÊàêÂäüÂàõÂª∫ÔºÅ)\\\")\\n\",\n        \"\\n\",\n        \"print(\\\"All folders have been created. (ÊâÄÊúâÊñá‰ª∂Â§πÂùáÂ∑≤ÂàõÂª∫„ÄÇ)\\\")\"\n      ],\n      \"metadata\": {\n        \"cellView\": \"form\",\n        \"id\": \"cPDEH-9czOJF\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"import requests\\n\",\n        \"import zipfile\\n\",\n        \"import shutil\\n\",\n        \"import os\\n\",\n        \"\\n\",\n        \"#@title Import model ÂØºÂÖ•Ê®°Âûã (HuggingFace)\\n\",\n        \"hf_link = 'https://huggingface.co/modelloosrvcc/Nagisa_Shingetsu_GPT-SoVITS/resolve/main/Nagisa.zip' #@param {type: \\\"string\\\"}\\n\",\n        \"\\n\",\n        \"output_path = '/content/'\\n\",\n        \"\\n\",\n        \"response = requests.get(hf_link)\\n\",\n        \"with open(output_path + 'file.zip', 'wb') as file:\\n\",\n        \"    file.write(response.content)\\n\",\n        \"\\n\",\n        \"with zipfile.ZipFile(output_path + 'file.zip', 'r') as zip_ref:\\n\",\n        \"    zip_ref.extractall(output_path)\\n\",\n        \"\\n\",\n        \"os.remove(output_path + \\\"file.zip\\\")\\n\",\n        \"\\n\",\n        \"source_directory = output_path\\n\",\n        \"SoVITS_destination_directory = '/content/GPT-SoVITS/SoVITS_weights'\\n\",\n        \"GPT_destination_directory = '/content/GPT-SoVITS/GPT_weights'\\n\",\n        \"\\n\",\n        \"for filename in os.listdir(source_directory):\\n\",\n        \"    if filename.endswith(\\\".pth\\\"):\\n\",\n        \"        source_path = os.path.join(source_directory, filename)\\n\",\n        \"        destination_path = os.path.join(SoVITS_destination_directory, filename)\\n\",\n        \"        shutil.move(source_path, destination_path)\\n\",\n        \"\\n\",\n        \"for filename in os.listdir(source_directory):\\n\",\n        \"    if filename.endswith(\\\".ckpt\\\"):\\n\",\n        \"        source_path = os.path.join(source_directory, filename)\\n\",\n        \"        destination_path = os.path.join(GPT_destination_directory, filename)\\n\",\n        \"        shutil.move(source_path, destination_path)\\n\",\n        \"\\n\",\n        \"print(f'Model downloaded. (Ê®°ÂûãÂ∑≤‰∏ãËΩΩ„ÄÇ)')\"\n      ],\n      \"metadata\": {\n        \"cellView\": \"form\",\n        \"id\": \"vbZY-LnM0tzq\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# @title launch WebUI ÂêØÂä®WebUI\\n\",\n        \"!/usr/local/bin/pip install ipykernel\\n\",\n        \"!sed -i '10s/False/True/' /content/GPT-SoVITS/config.py\\n\",\n        \"%cd /content/GPT-SoVITS/\\n\",\n        \"!/usr/local/bin/python  webui.py\"\n      ],\n      \"metadata\": {\n        \"id\": \"4oRGUzkrk8C7\",\n        \"cellView\": \"form\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    }\n  ]\n}"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0400390625,
          "content": "MIT License\n\nCopyright (c) 2024 RVC-Boss\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 13.7666015625,
          "content": "<div align=\"center\">\n\n\n<h1>GPT-SoVITS-WebUI</h1>\nA Powerful Few-shot Voice Conversion and Text-to-Speech WebUI.<br><br>\n\n[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&labelColor=orange)](https://github.com/RVC-Boss/GPT-SoVITS)\n\n<a href=\"https://trendshift.io/repositories/7033\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/7033\" alt=\"RVC-Boss%2FGPT-SoVITS | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n<!-- img src=\"https://counter.seku.su/cmoe?name=gptsovits&theme=r34\" /><br> -->\n\n[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/RVC-Boss/GPT-SoVITS/blob/main/colab_webui.ipynb)\n[![License](https://img.shields.io/badge/LICENSE-MIT-green.svg?style=for-the-badge)](https://github.com/RVC-Boss/GPT-SoVITS/blob/main/LICENSE)\n[![Huggingface](https://img.shields.io/badge/ü§ó%20-online%20demo-yellow.svg?style=for-the-badge)](https://huggingface.co/spaces/lj1995/GPT-SoVITS-v2)\n[![Discord](https://img.shields.io/discord/1198701940511617164?color=%23738ADB&label=Discord&style=for-the-badge)](https://discord.gg/dnrgs5GHfG)\n\n**English** | [**‰∏≠ÊñáÁÆÄ‰Ωì**](./docs/cn/README.md) | [**Êó•Êú¨Ë™û**](./docs/ja/README.md) | [**ÌïúÍµ≠Ïñ¥**](./docs/ko/README.md) | [**T√ºrk√ße**](./docs/tr/README.md)\n\n</div>\n\n---\n\n## Features:\n\n1. **Zero-shot TTS:** Input a 5-second vocal sample and experience instant text-to-speech conversion.\n\n2. **Few-shot TTS:** Fine-tune the model with just 1 minute of training data for improved voice similarity and realism.\n\n3. **Cross-lingual Support:** Inference in languages different from the training dataset, currently supporting English, Japanese, Korean, Cantonese and Chinese.\n\n4. **WebUI Tools:** Integrated tools include voice accompaniment separation, automatic training set segmentation, Chinese ASR, and text labeling, assisting beginners in creating training datasets and GPT/SoVITS models.\n\n**Check out our [demo video](https://www.bilibili.com/video/BV12g4y1m7Uw) here!**\n\nUnseen speakers few-shot fine-tuning demo:\n\nhttps://github.com/RVC-Boss/GPT-SoVITS/assets/129054828/05bee1fa-bdd8-4d85-9350-80c060ab47fb\n\n**User guide: [ÁÆÄ‰Ωì‰∏≠Êñá](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e) | [English](https://rentry.co/GPT-SoVITS-guide#/)**\n\n## Installation\n\nFor users in China, you can [click here](https://www.codewithgpu.com/i/RVC-Boss/GPT-SoVITS/GPT-SoVITS-Official) to use AutoDL Cloud Docker to experience the full functionality online.\n\n### Tested Environments\n\n- Python 3.9, PyTorch 2.0.1, CUDA 11\n- Python 3.10.13, PyTorch 2.1.2, CUDA 12.3\n- Python 3.9, PyTorch 2.2.2, macOS 14.4.1 (Apple silicon)\n- Python 3.9, PyTorch 2.2.2, CPU devices\n\n_Note: numba==0.56.4 requires py<3.11_\n\n### Windows\n\nIf you are a Windows user (tested with win>=10), you can [download the integrated package](https://huggingface.co/lj1995/GPT-SoVITS-windows-package/resolve/main/GPT-SoVITS-beta.7z?download=true) and double-click on _go-webui.bat_ to start GPT-SoVITS-WebUI.\n\n**Users in China can [download the package here](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/dkxgpiy9zb96hob4#KTvnO).**\n\n### Linux\n\n```bash\nconda create -n GPTSoVits python=3.9\nconda activate GPTSoVits\nbash install.sh\n```\n\n### macOS\n\n**Note: The models trained with GPUs on Macs result in significantly lower quality compared to those trained on other devices, so we are temporarily using CPUs instead.**\n\n1. Install Xcode command-line tools by running `xcode-select --install`.\n2. Install FFmpeg by running `brew install ffmpeg`.\n3. Install the program by running the following commands:\n\n```bash\nconda create -n GPTSoVits python=3.9\nconda activate GPTSoVits\npip install -r requirements.txt\n```\n\n### Install Manually\n\n#### Install FFmpeg\n\n##### Conda Users\n\n```bash\nconda install ffmpeg\n```\n\n##### Ubuntu/Debian Users\n\n```bash\nsudo apt install ffmpeg\nsudo apt install libsox-dev\nconda install -c conda-forge 'ffmpeg<7'\n```\n\n##### Windows Users\n\nDownload and place [ffmpeg.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffmpeg.exe) and [ffprobe.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffprobe.exe) in the GPT-SoVITS root.\n\nInstall [Visual Studio 2017](https://aka.ms/vs/17/release/vc_redist.x86.exe) (Korean TTS Only)\n\n##### MacOS Users\n```bash\nbrew install ffmpeg\n```\n\n#### Install Dependences\n\n```bash\npip install -r requirements.txt\n```\n\n### Using Docker\n\n#### docker-compose.yaml configuration\n\n0. Regarding image tags: Due to rapid updates in the codebase and the slow process of packaging and testing images, please check [Docker Hub](https://hub.docker.com/r/breakstring/gpt-sovits) for the currently packaged latest images and select as per your situation, or alternatively, build locally using a Dockerfile according to your own needs.\n1. Environment VariablesÔºö\n\n- is_half: Controls half-precision/double-precision. This is typically the cause if the content under the directories 4-cnhubert/5-wav32k is not generated correctly during the \"SSL extracting\" step. Adjust to True or False based on your actual situation.\n\n2. Volumes ConfigurationÔºåThe application's root directory inside the container is set to /workspace. The default docker-compose.yaml lists some practical examples for uploading/downloading content.\n3. shm_sizeÔºö The default available memory for Docker Desktop on Windows is too small, which can cause abnormal operations. Adjust according to your own situation.\n4. Under the deploy section, GPU-related settings should be adjusted cautiously according to your system and actual circumstances.\n\n#### Running with docker compose\n\n```\ndocker compose -f \"docker-compose.yaml\" up -d\n```\n\n#### Running with docker command\n\nAs above, modify the corresponding parameters based on your actual situation, then run the following command:\n\n```\ndocker run --rm -it --gpus=all --env=is_half=False --volume=G:\\GPT-SoVITS-DockerTest\\output:/workspace/output --volume=G:\\GPT-SoVITS-DockerTest\\logs:/workspace/logs --volume=G:\\GPT-SoVITS-DockerTest\\SoVITS_weights:/workspace/SoVITS_weights --workdir=/workspace -p 9880:9880 -p 9871:9871 -p 9872:9872 -p 9873:9873 -p 9874:9874 --shm-size=\"16G\" -d breakstring/gpt-sovits:xxxxx\n```\n\n## Pretrained Models\n\n**Users in China can [download all these models here](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/dkxgpiy9zb96hob4#nVNhX).**\n\n1. Download pretrained models from [GPT-SoVITS Models](https://huggingface.co/lj1995/GPT-SoVITS) and place them in `GPT_SoVITS/pretrained_models`.\n\n2. Download G2PW models from [G2PWModel_1.1.zip](https://paddlespeech.bj.bcebos.com/Parakeet/released_models/g2p/G2PWModel_1.1.zip), unzip and rename to `G2PWModel`, and then place them in `GPT_SoVITS/text`.(Chinese TTS Only)\n\n3. For UVR5 (Vocals/Accompaniment Separation & Reverberation Removal, additionally), download models from [UVR5 Weights](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/uvr5_weights) and place them in `tools/uvr5/uvr5_weights`.\n\n4. For Chinese ASR (additionally), download models from [Damo ASR Model](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/files), [Damo VAD Model](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/files), and [Damo Punc Model](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/files) and place them in `tools/asr/models`.\n\n5. For English or Japanese ASR (additionally), download models from [Faster Whisper Large V3](https://huggingface.co/Systran/faster-whisper-large-v3) and place them in `tools/asr/models`. Also, [other models](https://huggingface.co/Systran) may have the similar effect with smaller disk footprint. \n\n## Dataset Format\n\nThe TTS annotation .list file format:\n\n```\nvocal_path|speaker_name|language|text\n```\n\nLanguage dictionary:\n\n- 'zh': Chinese\n- 'ja': Japanese\n- 'en': English\n- 'ko': Korean\n- 'yue': Cantonese\n  \nExample:\n\n```\nD:\\GPT-SoVITS\\xxx/xxx.wav|xxx|en|I like playing Genshin.\n```\n\n## Finetune and inference\n\n ### Open WebUI\n\n #### Integrated Package Users\n\n Double-click `go-webui.bat`or use `go-webui.ps1`\n if you want to switch to V1,then double-click`go-webui-v1.bat` or use `go-webui-v1.ps1`\n\n #### Others\n\n ```bash\n python webui.py <language(optional)>\n ```\n\n if you want to switch to V1,then\n\n ```bash\n python webui.py v1 <language(optional)>\n ```\nOr maunally switch version in WebUI\n\n ### Finetune\n\n #### Path Auto-filling is now supported\n\n     1.Fill in the audio path\n\n     2.Slice the audio into small chunks\n\n     3.Denoise(optinal)\n\n     4.ASR\n\n     5.Proofreading ASR transcriptions\n\n     6.Go to the next Tab, then finetune the model\n\n ### Open Inference WebUI\n\n #### Integrated Package Users\n\n Double-click `go-webui-v2.bat` or use `go-webui-v2.ps1` ,then open the inference webui at  `1-GPT-SoVITS-TTS/1C-inference` \n\n #### Others\n\n ```bash\n python GPT_SoVITS/inference_webui.py <language(optional)>\n ```\n OR\n\n ```bash\n python webui.py\n ```\nthen open the inference webui at `1-GPT-SoVITS-TTS/1C-inference`\n\n ## V2 Release Notes\n\nNew Features:\n\n1. Support Korean and Cantonese\n\n2. An optimized text frontend\n\n3. Pre-trained model extended from 2k hours to 5k hours\n\n4. Improved synthesis quality for low-quality reference audio \n\n    [more details](https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90v2%E2%80%90features-(%E6%96%B0%E7%89%B9%E6%80%A7) ) \n\nUse v2 from v1 environment: \n\n1. `pip install -r requirements.txt` to update some packages\n\n2. Clone the latest codes from github.\n\n3. Download v2 pretrained models from [huggingface](https://huggingface.co/lj1995/GPT-SoVITS/tree/main/gsv-v2final-pretrained) and put them into `GPT_SoVITS\\pretrained_models\\gsv-v2final-pretrained`.\n\n    Chinese v2 additional: [G2PWModel_1.1.zip](https://paddlespeech.bj.bcebos.com/Parakeet/released_models/g2p/G2PWModel_1.1.zip)ÔºàDownload G2PW models,  unzip and rename to `G2PWModel`, and then place them in `GPT_SoVITS/text`.\n     \n## Todo List\n\n- [x] **High Priority:**\n\n  - [x] Localization in Japanese and English.\n  - [x] User guide.\n  - [x] Japanese and English dataset fine tune training.\n\n- [ ] **Features:**\n  - [x] Zero-shot voice conversion (5s) / few-shot voice conversion (1min).\n  - [x] TTS speaking speed control.\n  - [ ] ~~Enhanced TTS emotion control.~~\n  - [ ] Experiment with changing SoVITS token inputs to probability distribution of GPT vocabs (transformer latent).\n  - [x] Improve English and Japanese text frontend.\n  - [ ] Develop tiny and larger-sized TTS models.\n  - [x] Colab scripts.\n  - [ ] Try expand training dataset (2k hours -> 10k hours).\n  - [x] better sovits base model (enhanced audio quality)\n  - [ ] model mix\n\n## (Additional) Method for running from the command line\nUse the command line to open the WebUI for UVR5\n```\npython tools/uvr5/webui.py \"<infer_device>\" <is_half> <webui_port_uvr5>\n```\n<!-- If you can't open a browser, follow the format below for UVR processing,This is using mdxnet for audio processing\n```\npython mdxnet.py --model --input_root --output_vocal --output_ins --agg_level --format --device --is_half_precision \n``` -->\nThis is how the audio segmentation of the dataset is done using the command line\n```\npython audio_slicer.py \\\n    --input_path \"<path_to_original_audio_file_or_directory>\" \\\n    --output_root \"<directory_where_subdivided_audio_clips_will_be_saved>\" \\\n    --threshold <volume_threshold> \\\n    --min_length <minimum_duration_of_each_subclip> \\\n    --min_interval <shortest_time_gap_between_adjacent_subclips> \n    --hop_size <step_size_for_computing_volume_curve>\n```\nThis is how dataset ASR processing is done using the command line(Only Chinese)\n```\npython tools/asr/funasr_asr.py -i <input> -o <output>\n```\nASR processing is performed through Faster_Whisper(ASR marking except Chinese)\n\n(No progress bars, GPU performance may cause time delays)\n```\npython ./tools/asr/fasterwhisper_asr.py -i <input> -o <output> -l <language> -p <precision>\n```\nA custom list save path is enabled\n\n## Credits\n\nSpecial thanks to the following projects and contributors:\n\n### Theoretical Research\n- [ar-vits](https://github.com/innnky/ar-vits)\n- [SoundStorm](https://github.com/yangdongchao/SoundStorm/tree/master/soundstorm/s1/AR)\n- [vits](https://github.com/jaywalnut310/vits)\n- [TransferTTS](https://github.com/hcy71o/TransferTTS/blob/master/models.py#L556)\n- [contentvec](https://github.com/auspicious3000/contentvec/)\n- [hifi-gan](https://github.com/jik876/hifi-gan)\n- [fish-speech](https://github.com/fishaudio/fish-speech/blob/main/tools/llama/generate.py#L41)\n### Pretrained Models\n- [Chinese Speech Pretrain](https://github.com/TencentGameMate/chinese_speech_pretrain)\n- [Chinese-Roberta-WWM-Ext-Large](https://huggingface.co/hfl/chinese-roberta-wwm-ext-large)\n### Text Frontend for Inference\n- [paddlespeech zh_normalization](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/paddlespeech/t2s/frontend/zh_normalization)\n- [LangSegment](https://github.com/juntaosun/LangSegment)\n- [g2pW](https://github.com/GitYCC/g2pW)\n- [pypinyin-g2pW](https://github.com/mozillazg/pypinyin-g2pW)\n- [paddlespeech g2pw](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/paddlespeech/t2s/frontend/g2pw)\n### WebUI Tools\n- [ultimatevocalremovergui](https://github.com/Anjok07/ultimatevocalremovergui)\n- [audio-slicer](https://github.com/openvpi/audio-slicer)\n- [SubFix](https://github.com/cronrpc/SubFix)\n- [FFmpeg](https://github.com/FFmpeg/FFmpeg)\n- [gradio](https://github.com/gradio-app/gradio)\n- [faster-whisper](https://github.com/SYSTRAN/faster-whisper)\n- [FunASR](https://github.com/alibaba-damo-academy/FunASR)\n\nThankful to @Naozumi520 for providing the Cantonese training set and for the guidance on Cantonese-related knowledge.\n\n## Thanks to all contributors for their efforts\n\n<a href=\"https://github.com/RVC-Boss/GPT-SoVITS/graphs/contributors\" target=\"_blank\">\n  <img src=\"https://contrib.rocks/image?repo=RVC-Boss/GPT-SoVITS\" />\n</a>\n"
        },
        {
          "name": "api.py",
          "type": "blob",
          "size": 33.1171875,
          "content": "\"\"\"\r\n# api.py usage\r\n\r\n` python api.py -dr \"123.wav\" -dt \"‰∏Ä‰∫å‰∏â„ÄÇ\" -dl \"zh\" `\r\n\r\n## ÊâßË°åÂèÇÊï∞:\r\n\r\n`-s` - `SoVITSÊ®°ÂûãË∑ØÂæÑ, ÂèØÂú® config.py ‰∏≠ÊåáÂÆö`\r\n`-g` - `GPTÊ®°ÂûãË∑ØÂæÑ, ÂèØÂú® config.py ‰∏≠ÊåáÂÆö`\r\n\r\nË∞ÉÁî®ËØ∑Ê±ÇÁº∫Â∞ëÂèÇËÄÉÈü≥È¢ëÊó∂‰ΩøÁî®\r\n`-dr` - `ÈªòËÆ§ÂèÇËÄÉÈü≥È¢ëË∑ØÂæÑ`\r\n`-dt` - `ÈªòËÆ§ÂèÇËÄÉÈü≥È¢ëÊñáÊú¨`\r\n`-dl` - `ÈªòËÆ§ÂèÇËÄÉÈü≥È¢ëËØ≠Áßç, \"‰∏≠Êñá\",\"Ëã±Êñá\",\"Êó•Êñá\",\"Èü©Êñá\",\"Á≤§ËØ≠,\"zh\",\"en\",\"ja\",\"ko\",\"yue\"`\r\n\r\n`-d` - `Êé®ÁêÜËÆæÂ§á, \"cuda\",\"cpu\"`\r\n`-a` - `ÁªëÂÆöÂú∞ÂùÄ, ÈªòËÆ§\"127.0.0.1\"`\r\n`-p` - `ÁªëÂÆöÁ´ØÂè£, ÈªòËÆ§9880, ÂèØÂú® config.py ‰∏≠ÊåáÂÆö`\r\n`-fp` - `Ë¶ÜÁõñ config.py ‰ΩøÁî®ÂÖ®Á≤æÂ∫¶`\r\n`-hp` - `Ë¶ÜÁõñ config.py ‰ΩøÁî®ÂçäÁ≤æÂ∫¶`\r\n`-sm` - `ÊµÅÂºèËøîÂõûÊ®°Âºè, ÈªòËÆ§‰∏çÂêØÁî®, \"close\",\"c\", \"normal\",\"n\", \"keepalive\",\"k\"`\r\n¬∑-mt` - `ËøîÂõûÁöÑÈü≥È¢ëÁºñÁ†ÅÊ†ºÂºè, ÊµÅÂºèÈªòËÆ§ogg, ÈùûÊµÅÂºèÈªòËÆ§wav, \"wav\", \"ogg\", \"aac\"`\r\n¬∑-st` - `ËøîÂõûÁöÑÈü≥È¢ëÊï∞ÊçÆÁ±ªÂûã, ÈªòËÆ§int16, \"int16\", \"int32\"`\r\n¬∑-cp` - `ÊñáÊú¨ÂàáÂàÜÁ¨¶Âè∑ËÆæÂÆö, ÈªòËÆ§‰∏∫Á©∫, ‰ª•\",.Ôºå„ÄÇ\"Â≠óÁ¨¶‰∏≤ÁöÑÊñπÂºè‰º†ÂÖ•`\r\n\r\n`-hb` - `cnhubertË∑ØÂæÑ`\r\n`-b` - `bertË∑ØÂæÑ`\r\n\r\n## Ë∞ÉÁî®:\r\n\r\n### Êé®ÁêÜ\r\n\r\nendpoint: `/`\r\n\r\n‰ΩøÁî®ÊâßË°åÂèÇÊï∞ÊåáÂÆöÁöÑÂèÇËÄÉÈü≥È¢ë:\r\nGET:\r\n    `http://127.0.0.1:9880?text=ÂÖàÂ∏ùÂàõ‰∏öÊú™ÂçäËÄå‰∏≠ÈÅìÂ¥©ÊÆÇÔºå‰ªäÂ§©‰∏ã‰∏âÂàÜÔºåÁõäÂ∑ûÁñ≤ÂºäÔºåÊ≠§ËØöÂç±ÊÄ•Â≠ò‰∫°‰πãÁßã‰πü„ÄÇ&text_language=zh`\r\nPOST:\r\n```json\r\n{\r\n    \"text\": \"ÂÖàÂ∏ùÂàõ‰∏öÊú™ÂçäËÄå‰∏≠ÈÅìÂ¥©ÊÆÇÔºå‰ªäÂ§©‰∏ã‰∏âÂàÜÔºåÁõäÂ∑ûÁñ≤ÂºäÔºåÊ≠§ËØöÂç±ÊÄ•Â≠ò‰∫°‰πãÁßã‰πü„ÄÇ\",\r\n    \"text_language\": \"zh\"\r\n}\r\n```\r\n\r\n‰ΩøÁî®ÊâßË°åÂèÇÊï∞ÊåáÂÆöÁöÑÂèÇËÄÉÈü≥È¢ëÂπ∂ËÆæÂÆöÂàÜÂâ≤Á¨¶Âè∑:\r\nGET:\r\n    `http://127.0.0.1:9880?text=ÂÖàÂ∏ùÂàõ‰∏öÊú™ÂçäËÄå‰∏≠ÈÅìÂ¥©ÊÆÇÔºå‰ªäÂ§©‰∏ã‰∏âÂàÜÔºåÁõäÂ∑ûÁñ≤ÂºäÔºåÊ≠§ËØöÂç±ÊÄ•Â≠ò‰∫°‰πãÁßã‰πü„ÄÇ&text_language=zh&cut_punc=Ôºå„ÄÇ`\r\nPOST:\r\n```json\r\n{\r\n    \"text\": \"ÂÖàÂ∏ùÂàõ‰∏öÊú™ÂçäËÄå‰∏≠ÈÅìÂ¥©ÊÆÇÔºå‰ªäÂ§©‰∏ã‰∏âÂàÜÔºåÁõäÂ∑ûÁñ≤ÂºäÔºåÊ≠§ËØöÂç±ÊÄ•Â≠ò‰∫°‰πãÁßã‰πü„ÄÇ\",\r\n    \"text_language\": \"zh\",\r\n    \"cut_punc\": \"Ôºå„ÄÇ\",\r\n}\r\n```\r\n\r\nÊâãÂä®ÊåáÂÆöÂΩìÊ¨°Êé®ÁêÜÊâÄ‰ΩøÁî®ÁöÑÂèÇËÄÉÈü≥È¢ë:\r\nGET:\r\n    `http://127.0.0.1:9880?refer_wav_path=123.wav&prompt_text=‰∏Ä‰∫å‰∏â„ÄÇ&prompt_language=zh&text=ÂÖàÂ∏ùÂàõ‰∏öÊú™ÂçäËÄå‰∏≠ÈÅìÂ¥©ÊÆÇÔºå‰ªäÂ§©‰∏ã‰∏âÂàÜÔºåÁõäÂ∑ûÁñ≤ÂºäÔºåÊ≠§ËØöÂç±ÊÄ•Â≠ò‰∫°‰πãÁßã‰πü„ÄÇ&text_language=zh`\r\nPOST:\r\n```json\r\n{\r\n    \"refer_wav_path\": \"123.wav\",\r\n    \"prompt_text\": \"‰∏Ä‰∫å‰∏â„ÄÇ\",\r\n    \"prompt_language\": \"zh\",\r\n    \"text\": \"ÂÖàÂ∏ùÂàõ‰∏öÊú™ÂçäËÄå‰∏≠ÈÅìÂ¥©ÊÆÇÔºå‰ªäÂ§©‰∏ã‰∏âÂàÜÔºåÁõäÂ∑ûÁñ≤ÂºäÔºåÊ≠§ËØöÂç±ÊÄ•Â≠ò‰∫°‰πãÁßã‰πü„ÄÇ\",\r\n    \"text_language\": \"zh\"\r\n}\r\n```\r\n\r\nRESP:\r\nÊàêÂäü: Áõ¥Êé•ËøîÂõû wav Èü≥È¢ëÊµÅÔºå http code 200\r\nÂ§±Ë¥•: ËøîÂõûÂåÖÂê´ÈîôËØØ‰ø°ÊÅØÁöÑ json, http code 400\r\n\r\nÊâãÂä®ÊåáÂÆöÂΩìÊ¨°Êé®ÁêÜÊâÄ‰ΩøÁî®ÁöÑÂèÇËÄÉÈü≥È¢ëÔºåÂπ∂Êèê‰æõÂèÇÊï∞:\r\nGET:\r\n    `http://127.0.0.1:9880?refer_wav_path=123.wav&prompt_text=‰∏Ä‰∫å‰∏â„ÄÇ&prompt_language=zh&text=ÂÖàÂ∏ùÂàõ‰∏öÊú™ÂçäËÄå‰∏≠ÈÅìÂ¥©ÊÆÇÔºå‰ªäÂ§©‰∏ã‰∏âÂàÜÔºåÁõäÂ∑ûÁñ≤ÂºäÔºåÊ≠§ËØöÂç±ÊÄ•Â≠ò‰∫°‰πãÁßã‰πü„ÄÇ&text_language=zh&top_k=20&top_p=0.6&temperature=0.6&speed=1&inp_refs=\"456.wav\"&inp_refs=\"789.wav\"`\r\nPOST:\r\n```json\r\n{\r\n    \"refer_wav_path\": \"123.wav\",\r\n    \"prompt_text\": \"‰∏Ä‰∫å‰∏â„ÄÇ\",\r\n    \"prompt_language\": \"zh\",\r\n    \"text\": \"ÂÖàÂ∏ùÂàõ‰∏öÊú™ÂçäËÄå‰∏≠ÈÅìÂ¥©ÊÆÇÔºå‰ªäÂ§©‰∏ã‰∏âÂàÜÔºåÁõäÂ∑ûÁñ≤ÂºäÔºåÊ≠§ËØöÂç±ÊÄ•Â≠ò‰∫°‰πãÁßã‰πü„ÄÇ\",\r\n    \"text_language\": \"zh\",\r\n    \"top_k\": 20,\r\n    \"top_p\": 0.6,\r\n    \"temperature\": 0.6,\r\n    \"speed\": 1,\r\n    \"inp_refs\": [\"456.wav\",\"789.wav\"]\r\n}\r\n```\r\n\r\nRESP:\r\nÊàêÂäü: Áõ¥Êé•ËøîÂõû wav Èü≥È¢ëÊµÅÔºå http code 200\r\nÂ§±Ë¥•: ËøîÂõûÂåÖÂê´ÈîôËØØ‰ø°ÊÅØÁöÑ json, http code 400\r\n\r\n\r\n### Êõ¥Êç¢ÈªòËÆ§ÂèÇËÄÉÈü≥È¢ë\r\n\r\nendpoint: `/change_refer`\r\n\r\nkey‰∏éÊé®ÁêÜÁ´Ø‰∏ÄÊ†∑\r\n\r\nGET:\r\n    `http://127.0.0.1:9880/change_refer?refer_wav_path=123.wav&prompt_text=‰∏Ä‰∫å‰∏â„ÄÇ&prompt_language=zh`\r\nPOST:\r\n```json\r\n{\r\n    \"refer_wav_path\": \"123.wav\",\r\n    \"prompt_text\": \"‰∏Ä‰∫å‰∏â„ÄÇ\",\r\n    \"prompt_language\": \"zh\"\r\n}\r\n```\r\n\r\nRESP:\r\nÊàêÂäü: json, http code 200\r\nÂ§±Ë¥•: json, 400\r\n\r\n\r\n### ÂëΩ‰ª§ÊéßÂà∂\r\n\r\nendpoint: `/control`\r\n\r\ncommand:\r\n\"restart\": ÈáçÊñ∞ËøêË°å\r\n\"exit\": ÁªìÊùüËøêË°å\r\n\r\nGET:\r\n    `http://127.0.0.1:9880/control?command=restart`\r\nPOST:\r\n```json\r\n{\r\n    \"command\": \"restart\"\r\n}\r\n```\r\n\r\nRESP: Êó†\r\n\r\n\"\"\"\r\n\r\n\r\nimport argparse\r\nimport os,re\r\nimport sys\r\n\r\nnow_dir = os.getcwd()\r\nsys.path.append(now_dir)\r\nsys.path.append(\"%s/GPT_SoVITS\" % (now_dir))\r\n\r\nimport signal\r\nimport LangSegment\r\nfrom time import time as ttime\r\nimport torch\r\nimport librosa\r\nimport soundfile as sf\r\nfrom fastapi import FastAPI, Request, Query, HTTPException\r\nfrom fastapi.responses import StreamingResponse, JSONResponse\r\nimport uvicorn\r\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\r\nimport numpy as np\r\nfrom feature_extractor import cnhubert\r\nfrom io import BytesIO\r\nfrom module.models import SynthesizerTrn\r\nfrom AR.models.t2s_lightning_module import Text2SemanticLightningModule\r\nfrom text import cleaned_text_to_sequence\r\nfrom text.cleaner import clean_text\r\nfrom module.mel_processing import spectrogram_torch\r\nfrom tools.my_utils import load_audio\r\nimport config as global_config\r\nimport logging\r\nimport subprocess\r\n\r\n\r\nclass DefaultRefer:\r\n    def __init__(self, path, text, language):\r\n        self.path = args.default_refer_path\r\n        self.text = args.default_refer_text\r\n        self.language = args.default_refer_language\r\n\r\n    def is_ready(self) -> bool:\r\n        return is_full(self.path, self.text, self.language)\r\n\r\n\r\ndef is_empty(*items):  # ‰ªªÊÑè‰∏ÄÈ°π‰∏ç‰∏∫Á©∫ËøîÂõûFalse\r\n    for item in items:\r\n        if item is not None and item != \"\":\r\n            return False\r\n    return True\r\n\r\n\r\ndef is_full(*items):  # ‰ªªÊÑè‰∏ÄÈ°π‰∏∫Á©∫ËøîÂõûFalse\r\n    for item in items:\r\n        if item is None or item == \"\":\r\n            return False\r\n    return True\r\n\r\n\r\nclass Speaker:\r\n    def __init__(self, name, gpt, sovits, phones = None, bert = None, prompt = None):\r\n        self.name = name\r\n        self.sovits = sovits\r\n        self.gpt = gpt\r\n        self.phones = phones\r\n        self.bert = bert\r\n        self.prompt = prompt\r\n        \r\nspeaker_list = {}\r\n\r\n\r\nclass Sovits:\r\n    def __init__(self, vq_model, hps):\r\n        self.vq_model = vq_model\r\n        self.hps = hps\r\n\r\ndef get_sovits_weights(sovits_path):\r\n    dict_s2 = torch.load(sovits_path, map_location=\"cpu\")\r\n    hps = dict_s2[\"config\"]\r\n    hps = DictToAttrRecursive(hps)\r\n    hps.model.semantic_frame_rate = \"25hz\"\r\n    if dict_s2['weight']['enc_p.text_embedding.weight'].shape[0] == 322:\r\n        hps.model.version = \"v1\"\r\n    else:\r\n        hps.model.version = \"v2\"\r\n    logger.info(f\"Ê®°ÂûãÁâàÊú¨: {hps.model.version}\")\r\n    model_params_dict = vars(hps.model)\r\n    vq_model = SynthesizerTrn(\r\n        hps.data.filter_length // 2 + 1,\r\n        hps.train.segment_size // hps.data.hop_length,\r\n        n_speakers=hps.data.n_speakers,\r\n        **model_params_dict\r\n    )\r\n    if (\"pretrained\" not in sovits_path):\r\n        del vq_model.enc_q\r\n    if is_half == True:\r\n        vq_model = vq_model.half().to(device)\r\n    else:\r\n        vq_model = vq_model.to(device)\r\n    vq_model.eval()\r\n    vq_model.load_state_dict(dict_s2[\"weight\"], strict=False)\r\n\r\n    sovits = Sovits(vq_model, hps)\r\n    return sovits\r\n\r\nclass Gpt:\r\n    def __init__(self, max_sec, t2s_model):\r\n        self.max_sec = max_sec\r\n        self.t2s_model = t2s_model\r\n\r\nglobal hz\r\nhz = 50\r\ndef get_gpt_weights(gpt_path):\r\n    dict_s1 = torch.load(gpt_path, map_location=\"cpu\")\r\n    config = dict_s1[\"config\"]\r\n    max_sec = config[\"data\"][\"max_sec\"]\r\n    t2s_model = Text2SemanticLightningModule(config, \"****\", is_train=False)\r\n    t2s_model.load_state_dict(dict_s1[\"weight\"])\r\n    if is_half == True:\r\n        t2s_model = t2s_model.half()\r\n    t2s_model = t2s_model.to(device)\r\n    t2s_model.eval()\r\n    total = sum([param.nelement() for param in t2s_model.parameters()])\r\n    logger.info(\"Number of parameter: %.2fM\" % (total / 1e6))\r\n\r\n    gpt = Gpt(max_sec, t2s_model)\r\n    return gpt\r\n\r\ndef change_gpt_sovits_weights(gpt_path,sovits_path):\r\n    try:\r\n        gpt = get_gpt_weights(gpt_path)\r\n        sovits = get_sovits_weights(sovits_path)\r\n    except Exception as e:\r\n        return JSONResponse({\"code\": 400, \"message\": str(e)}, status_code=400)\r\n\r\n    speaker_list[\"default\"] = Speaker(name=\"default\", gpt=gpt, sovits=sovits)\r\n    return JSONResponse({\"code\": 0, \"message\": \"Success\"}, status_code=200)\r\n\r\n\r\ndef get_bert_feature(text, word2ph):\r\n    with torch.no_grad():\r\n        inputs = tokenizer(text, return_tensors=\"pt\")\r\n        for i in inputs:\r\n            inputs[i] = inputs[i].to(device)  #####ËæìÂÖ•ÊòØlong‰∏çÁî®ÁÆ°Á≤æÂ∫¶ÈóÆÈ¢òÔºåÁ≤æÂ∫¶Èöèbert_model\r\n        res = bert_model(**inputs, output_hidden_states=True)\r\n        res = torch.cat(res[\"hidden_states\"][-3:-2], -1)[0].cpu()[1:-1]\r\n    assert len(word2ph) == len(text)\r\n    phone_level_feature = []\r\n    for i in range(len(word2ph)):\r\n        repeat_feature = res[i].repeat(word2ph[i], 1)\r\n        phone_level_feature.append(repeat_feature)\r\n    phone_level_feature = torch.cat(phone_level_feature, dim=0)\r\n    # if(is_half==True):phone_level_feature=phone_level_feature.half()\r\n    return phone_level_feature.T\r\n\r\n\r\ndef clean_text_inf(text, language, version):\r\n    phones, word2ph, norm_text = clean_text(text, language, version)\r\n    phones = cleaned_text_to_sequence(phones, version)\r\n    return phones, word2ph, norm_text\r\n\r\n\r\ndef get_bert_inf(phones, word2ph, norm_text, language):\r\n    language=language.replace(\"all_\",\"\")\r\n    if language == \"zh\":\r\n        bert = get_bert_feature(norm_text, word2ph).to(device)#.to(dtype)\r\n    else:\r\n        bert = torch.zeros(\r\n            (1024, len(phones)),\r\n            dtype=torch.float16 if is_half == True else torch.float32,\r\n        ).to(device)\r\n\r\n    return bert\r\n\r\nfrom text import chinese\r\ndef get_phones_and_bert(text,language,version,final=False):\r\n    if language in {\"en\", \"all_zh\", \"all_ja\", \"all_ko\", \"all_yue\"}:\r\n        language = language.replace(\"all_\",\"\")\r\n        if language == \"en\":\r\n            LangSegment.setfilters([\"en\"])\r\n            formattext = \" \".join(tmp[\"text\"] for tmp in LangSegment.getTexts(text))\r\n        else:\r\n            # Âõ†Êó†Ê≥ïÂå∫Âà´‰∏≠Êó•Èü©ÊñáÊ±âÂ≠ó,‰ª•Áî®Êà∑ËæìÂÖ•‰∏∫ÂáÜ\r\n            formattext = text\r\n        while \"  \" in formattext:\r\n            formattext = formattext.replace(\"  \", \" \")\r\n        if language == \"zh\":\r\n            if re.search(r'[A-Za-z]', formattext):\r\n                formattext = re.sub(r'[a-z]', lambda x: x.group(0).upper(), formattext)\r\n                formattext = chinese.mix_text_normalize(formattext)\r\n                return get_phones_and_bert(formattext,\"zh\",version)\r\n            else:\r\n                phones, word2ph, norm_text = clean_text_inf(formattext, language, version)\r\n                bert = get_bert_feature(norm_text, word2ph).to(device)\r\n        elif language == \"yue\" and re.search(r'[A-Za-z]', formattext):\r\n                formattext = re.sub(r'[a-z]', lambda x: x.group(0).upper(), formattext)\r\n                formattext = chinese.mix_text_normalize(formattext)\r\n                return get_phones_and_bert(formattext,\"yue\",version)\r\n        else:\r\n            phones, word2ph, norm_text = clean_text_inf(formattext, language, version)\r\n            bert = torch.zeros(\r\n                (1024, len(phones)),\r\n                dtype=torch.float16 if is_half == True else torch.float32,\r\n            ).to(device)\r\n    elif language in {\"zh\", \"ja\", \"ko\", \"yue\", \"auto\", \"auto_yue\"}:\r\n        textlist=[]\r\n        langlist=[]\r\n        LangSegment.setfilters([\"zh\",\"ja\",\"en\",\"ko\"])\r\n        if language == \"auto\":\r\n            for tmp in LangSegment.getTexts(text):\r\n                langlist.append(tmp[\"lang\"])\r\n                textlist.append(tmp[\"text\"])\r\n        elif language == \"auto_yue\":\r\n            for tmp in LangSegment.getTexts(text):\r\n                if tmp[\"lang\"] == \"zh\":\r\n                    tmp[\"lang\"] = \"yue\"\r\n                langlist.append(tmp[\"lang\"])\r\n                textlist.append(tmp[\"text\"])\r\n        else:\r\n            for tmp in LangSegment.getTexts(text):\r\n                if tmp[\"lang\"] == \"en\":\r\n                    langlist.append(tmp[\"lang\"])\r\n                else:\r\n                    # Âõ†Êó†Ê≥ïÂå∫Âà´‰∏≠Êó•Èü©ÊñáÊ±âÂ≠ó,‰ª•Áî®Êà∑ËæìÂÖ•‰∏∫ÂáÜ\r\n                    langlist.append(language)\r\n                textlist.append(tmp[\"text\"])\r\n        phones_list = []\r\n        bert_list = []\r\n        norm_text_list = []\r\n        for i in range(len(textlist)):\r\n            lang = langlist[i]\r\n            phones, word2ph, norm_text = clean_text_inf(textlist[i], lang, version)\r\n            bert = get_bert_inf(phones, word2ph, norm_text, lang)\r\n            phones_list.append(phones)\r\n            norm_text_list.append(norm_text)\r\n            bert_list.append(bert)\r\n        bert = torch.cat(bert_list, dim=1)\r\n        phones = sum(phones_list, [])\r\n        norm_text = ''.join(norm_text_list)\r\n\r\n    if not final and len(phones) < 6:\r\n        return get_phones_and_bert(\".\" + text,language,version,final=True)\r\n\r\n    return phones,bert.to(torch.float16 if is_half == True else torch.float32),norm_text\r\n\r\n\r\nclass DictToAttrRecursive(dict):\r\n    def __init__(self, input_dict):\r\n        super().__init__(input_dict)\r\n        for key, value in input_dict.items():\r\n            if isinstance(value, dict):\r\n                value = DictToAttrRecursive(value)\r\n            self[key] = value\r\n            setattr(self, key, value)\r\n\r\n    def __getattr__(self, item):\r\n        try:\r\n            return self[item]\r\n        except KeyError:\r\n            raise AttributeError(f\"Attribute {item} not found\")\r\n\r\n    def __setattr__(self, key, value):\r\n        if isinstance(value, dict):\r\n            value = DictToAttrRecursive(value)\r\n        super(DictToAttrRecursive, self).__setitem__(key, value)\r\n        super().__setattr__(key, value)\r\n\r\n    def __delattr__(self, item):\r\n        try:\r\n            del self[item]\r\n        except KeyError:\r\n            raise AttributeError(f\"Attribute {item} not found\")\r\n\r\n\r\ndef get_spepc(hps, filename):\r\n    audio,_ = librosa.load(filename, int(hps.data.sampling_rate))\r\n    audio = torch.FloatTensor(audio)\r\n    maxx=audio.abs().max()\r\n    if(maxx>1):\r\n        audio/=min(2,maxx)\r\n    audio_norm = audio\r\n    audio_norm = audio_norm.unsqueeze(0)\r\n    spec = spectrogram_torch(audio_norm, hps.data.filter_length, hps.data.sampling_rate, hps.data.hop_length,\r\n                             hps.data.win_length, center=False)\r\n    return spec\r\n\r\n\r\ndef pack_audio(audio_bytes, data, rate):\r\n    if media_type == \"ogg\":\r\n        audio_bytes = pack_ogg(audio_bytes, data, rate)\r\n    elif media_type == \"aac\":\r\n        audio_bytes = pack_aac(audio_bytes, data, rate)\r\n    else:\r\n        # wavÊó†Ê≥ïÊµÅÂºè, ÂÖàÊöÇÂ≠òraw\r\n        audio_bytes = pack_raw(audio_bytes, data, rate)\r\n\r\n    return audio_bytes\r\n\r\n\r\ndef pack_ogg(audio_bytes, data, rate):\r\n    # Author: AkagawaTsurunaki\r\n    # Issue:\r\n    #   Stack overflow probabilistically occurs\r\n    #   when the function `sf_writef_short` of `libsndfile_64bit.dll` is called\r\n    #   using the Python library `soundfile`\r\n    # Note:\r\n    #   This is an issue related to `libsndfile`, not this project itself.\r\n    #   It happens when you generate a large audio tensor (about 499804 frames in my PC)\r\n    #   and try to convert it to an ogg file.\r\n    # Related:\r\n    #   https://github.com/RVC-Boss/GPT-SoVITS/issues/1199\r\n    #   https://github.com/libsndfile/libsndfile/issues/1023\r\n    #   https://github.com/bastibe/python-soundfile/issues/396\r\n    # Suggestion:\r\n    #   Or split the whole audio data into smaller audio segment to avoid stack overflow?\r\n\r\n    def handle_pack_ogg():\r\n        with sf.SoundFile(audio_bytes, mode='w', samplerate=rate, channels=1, format='ogg') as audio_file:\r\n            audio_file.write(data)\r\n\r\n    import threading\r\n    # See: https://docs.python.org/3/library/threading.html\r\n    # The stack size of this thread is at least 32768\r\n    # If stack overflow error still occurs, just modify the `stack_size`.\r\n    # stack_size = n * 4096, where n should be a positive integer.\r\n    # Here we chose n = 4096.\r\n    stack_size = 4096 * 4096\r\n    try:\r\n        threading.stack_size(stack_size)\r\n        pack_ogg_thread = threading.Thread(target=handle_pack_ogg)\r\n        pack_ogg_thread.start()\r\n        pack_ogg_thread.join()\r\n    except RuntimeError as e:\r\n        # If changing the thread stack size is unsupported, a RuntimeError is raised.\r\n        print(\"RuntimeError: {}\".format(e))\r\n        print(\"Changing the thread stack size is unsupported.\")\r\n    except ValueError as e:\r\n        # If the specified stack size is invalid, a ValueError is raised and the stack size is unmodified.\r\n        print(\"ValueError: {}\".format(e))\r\n        print(\"The specified stack size is invalid.\")\r\n\r\n    return audio_bytes\r\n\r\n\r\ndef pack_raw(audio_bytes, data, rate):\r\n    audio_bytes.write(data.tobytes())\r\n\r\n    return audio_bytes\r\n\r\n\r\ndef pack_wav(audio_bytes, rate):\r\n    if is_int32:\r\n        data = np.frombuffer(audio_bytes.getvalue(),dtype=np.int32)\r\n        wav_bytes = BytesIO()\r\n        sf.write(wav_bytes, data, rate, format='WAV', subtype='PCM_32')\r\n    else:\r\n        data = np.frombuffer(audio_bytes.getvalue(),dtype=np.int16)\r\n        wav_bytes = BytesIO()\r\n        sf.write(wav_bytes, data, rate, format='WAV')\r\n    return wav_bytes\r\n\r\n\r\ndef pack_aac(audio_bytes, data, rate):\r\n    if is_int32:\r\n        pcm = 's32le'\r\n        bit_rate = '256k'\r\n    else:\r\n        pcm = 's16le'\r\n        bit_rate = '128k'\r\n    process = subprocess.Popen([\r\n        'ffmpeg',\r\n        '-f', pcm,  # ËæìÂÖ•16‰ΩçÊúâÁ¨¶Âè∑Â∞èÁ´ØÊï¥Êï∞PCM\r\n        '-ar', str(rate),  # ËÆæÁΩÆÈááÊ†∑Áéá\r\n        '-ac', '1',  # ÂçïÂ£∞ÈÅì\r\n        '-i', 'pipe:0',  # ‰ªéÁÆ°ÈÅìËØªÂèñËæìÂÖ•\r\n        '-c:a', 'aac',  # Èü≥È¢ëÁºñÁ†ÅÂô®‰∏∫AAC\r\n        '-b:a', bit_rate,  # ÊØîÁâπÁéá\r\n        '-vn',  # ‰∏çÂåÖÂê´ËßÜÈ¢ë\r\n        '-f', 'adts',  # ËæìÂá∫AACÊï∞ÊçÆÊµÅÊ†ºÂºè\r\n        'pipe:1'  # Â∞ÜËæìÂá∫ÂÜôÂÖ•ÁÆ°ÈÅì\r\n    ], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\r\n    out, _ = process.communicate(input=data.tobytes())\r\n    audio_bytes.write(out)\r\n\r\n    return audio_bytes\r\n\r\n\r\ndef read_clean_buffer(audio_bytes):\r\n    audio_chunk = audio_bytes.getvalue()\r\n    audio_bytes.truncate(0)\r\n    audio_bytes.seek(0)\r\n\r\n    return audio_bytes, audio_chunk\r\n\r\n\r\ndef cut_text(text, punc):\r\n    punc_list = [p for p in punc if p in {\",\", \".\", \";\", \"?\", \"!\", \"„ÄÅ\", \"Ôºå\", \"„ÄÇ\", \"Ôºü\", \"ÔºÅ\", \"Ôºõ\", \"Ôºö\", \"‚Ä¶\"}]\r\n    if len(punc_list) > 0:\r\n        punds = r\"[\" + \"\".join(punc_list) + r\"]\"\r\n        text = text.strip(\"\\n\")\r\n        items = re.split(f\"({punds})\", text)\r\n        mergeitems = [\"\".join(group) for group in zip(items[::2], items[1::2])]\r\n        # Âú®Âè•Â≠ê‰∏çÂ≠òÂú®Á¨¶Âè∑ÊàñÂè•Â∞æÊó†Á¨¶Âè∑ÁöÑÊó∂ÂÄô‰øùËØÅÊñáÊú¨ÂÆåÊï¥\r\n        if len(items)%2 == 1:\r\n            mergeitems.append(items[-1])\r\n        text = \"\\n\".join(mergeitems)\r\n\r\n    while \"\\n\\n\" in text:\r\n        text = text.replace(\"\\n\\n\", \"\\n\")\r\n\r\n    return text\r\n\r\n\r\ndef only_punc(text):\r\n    return not any(t.isalnum() or t.isalpha() for t in text)\r\n\r\n\r\nsplits = {\"Ôºå\", \"„ÄÇ\", \"Ôºü\", \"ÔºÅ\", \",\", \".\", \"?\", \"!\", \"~\", \":\", \"Ôºö\", \"‚Äî\", \"‚Ä¶\", }\r\ndef get_tts_wav(ref_wav_path, prompt_text, prompt_language, text, text_language, top_k= 15, top_p = 0.6, temperature = 0.6, speed = 1, inp_refs = None, spk = \"default\"):\r\n    infer_sovits = speaker_list[spk].sovits\r\n    vq_model = infer_sovits.vq_model\r\n    hps = infer_sovits.hps\r\n\r\n    infer_gpt = speaker_list[spk].gpt\r\n    t2s_model = infer_gpt.t2s_model\r\n    max_sec = infer_gpt.max_sec\r\n\r\n    t0 = ttime()\r\n    prompt_text = prompt_text.strip(\"\\n\")\r\n    if (prompt_text[-1] not in splits): prompt_text += \"„ÄÇ\" if prompt_language != \"en\" else \".\"\r\n    prompt_language, text = prompt_language, text.strip(\"\\n\")\r\n    dtype = torch.float16 if is_half == True else torch.float32\r\n    zero_wav = np.zeros(int(hps.data.sampling_rate * 0.3), dtype=np.float16 if is_half == True else np.float32)\r\n    with torch.no_grad():\r\n        wav16k, sr = librosa.load(ref_wav_path, sr=16000)\r\n        wav16k = torch.from_numpy(wav16k)\r\n        zero_wav_torch = torch.from_numpy(zero_wav)\r\n        if (is_half == True):\r\n            wav16k = wav16k.half().to(device)\r\n            zero_wav_torch = zero_wav_torch.half().to(device)\r\n        else:\r\n            wav16k = wav16k.to(device)\r\n            zero_wav_torch = zero_wav_torch.to(device)\r\n        wav16k = torch.cat([wav16k, zero_wav_torch])\r\n        ssl_content = ssl_model.model(wav16k.unsqueeze(0))[\"last_hidden_state\"].transpose(1, 2)  # .float()\r\n        codes = vq_model.extract_latent(ssl_content)\r\n        prompt_semantic = codes[0, 0]\r\n        prompt = prompt_semantic.unsqueeze(0).to(device)\r\n\r\n        refers=[]\r\n        if(inp_refs):\r\n            for path in inp_refs:\r\n                try:\r\n                    refer = get_spepc(hps, path).to(dtype).to(device)\r\n                    refers.append(refer)\r\n                except Exception as e:\r\n                    logger.error(e)\r\n        if(len(refers)==0):\r\n            refers = [get_spepc(hps, ref_wav_path).to(dtype).to(device)]\r\n\r\n    t1 = ttime()\r\n    version = vq_model.version\r\n    os.environ['version'] = version\r\n    prompt_language = dict_language[prompt_language.lower()]\r\n    text_language = dict_language[text_language.lower()]\r\n    phones1, bert1, norm_text1 = get_phones_and_bert(prompt_text, prompt_language, version)\r\n    texts = text.split(\"\\n\")\r\n    audio_bytes = BytesIO()\r\n\r\n    for text in texts:\r\n        # ÁÆÄÂçïÈò≤Ê≠¢Á∫ØÁ¨¶Âè∑ÂºïÂèëÂèÇËÄÉÈü≥È¢ëÊ≥ÑÈú≤\r\n        if only_punc(text):\r\n            continue\r\n\r\n        audio_opt = []\r\n        if (text[-1] not in splits): text += \"„ÄÇ\" if text_language != \"en\" else \".\"\r\n        phones2, bert2, norm_text2 = get_phones_and_bert(text, text_language, version)\r\n        bert = torch.cat([bert1, bert2], 1)\r\n\r\n        all_phoneme_ids = torch.LongTensor(phones1 + phones2).to(device).unsqueeze(0)\r\n        bert = bert.to(device).unsqueeze(0)\r\n        all_phoneme_len = torch.tensor([all_phoneme_ids.shape[-1]]).to(device)\r\n        t2 = ttime()\r\n        with torch.no_grad():\r\n            pred_semantic, idx = t2s_model.model.infer_panel(\r\n                all_phoneme_ids,\r\n                all_phoneme_len,\r\n                prompt,\r\n                bert,\r\n                # prompt_phone_len=ph_offset,\r\n                top_k = top_k,\r\n                top_p = top_p,\r\n                temperature = temperature,\r\n                early_stop_num=hz * max_sec)\r\n            pred_semantic = pred_semantic[:, -idx:].unsqueeze(0)\r\n        t3 = ttime()\r\n        audio = \\\r\n            vq_model.decode(pred_semantic, torch.LongTensor(phones2).to(device).unsqueeze(0),\r\n                            refers,speed=speed).detach().cpu().numpy()[\r\n                0, 0]  ###ËØïËØïÈáçÂª∫‰∏çÂ∏¶‰∏äpromptÈÉ®ÂàÜ\r\n        max_audio=np.abs(audio).max()\r\n        if max_audio>1:\r\n            audio/=max_audio\r\n        audio_opt.append(audio)\r\n        audio_opt.append(zero_wav)\r\n        t4 = ttime()\r\n        if is_int32:\r\n            audio_bytes = pack_audio(audio_bytes,(np.concatenate(audio_opt, 0) * 2147483647).astype(np.int32),hps.data.sampling_rate)\r\n        else:\r\n            audio_bytes = pack_audio(audio_bytes,(np.concatenate(audio_opt, 0) * 32768).astype(np.int16),hps.data.sampling_rate)\r\n    # logger.info(\"%.3f\\t%.3f\\t%.3f\\t%.3f\" % (t1 - t0, t2 - t1, t3 - t2, t4 - t3))\r\n        if stream_mode == \"normal\":\r\n            audio_bytes, audio_chunk = read_clean_buffer(audio_bytes)\r\n            yield audio_chunk\r\n    \r\n    if not stream_mode == \"normal\": \r\n        if media_type == \"wav\":\r\n            audio_bytes = pack_wav(audio_bytes,hps.data.sampling_rate)\r\n        yield audio_bytes.getvalue()\r\n\r\n\r\n\r\ndef handle_control(command):\r\n    if command == \"restart\":\r\n        os.execl(g_config.python_exec, g_config.python_exec, *sys.argv)\r\n    elif command == \"exit\":\r\n        os.kill(os.getpid(), signal.SIGTERM)\r\n        exit(0)\r\n\r\n\r\ndef handle_change(path, text, language):\r\n    if is_empty(path, text, language):\r\n        return JSONResponse({\"code\": 400, \"message\": 'Áº∫Â∞ë‰ªªÊÑè‰∏ÄÈ°π‰ª•‰∏ãÂèÇÊï∞: \"path\", \"text\", \"language\"'}, status_code=400)\r\n\r\n    if path != \"\" or path is not None:\r\n        default_refer.path = path\r\n    if text != \"\" or text is not None:\r\n        default_refer.text = text\r\n    if language != \"\" or language is not None:\r\n        default_refer.language = language\r\n\r\n    logger.info(f\"ÂΩìÂâçÈªòËÆ§ÂèÇËÄÉÈü≥È¢ëË∑ØÂæÑ: {default_refer.path}\")\r\n    logger.info(f\"ÂΩìÂâçÈªòËÆ§ÂèÇËÄÉÈü≥È¢ëÊñáÊú¨: {default_refer.text}\")\r\n    logger.info(f\"ÂΩìÂâçÈªòËÆ§ÂèÇËÄÉÈü≥È¢ëËØ≠Áßç: {default_refer.language}\")\r\n    logger.info(f\"is_ready: {default_refer.is_ready()}\")\r\n\r\n\r\n    return JSONResponse({\"code\": 0, \"message\": \"Success\"}, status_code=200)\r\n\r\n\r\ndef handle(refer_wav_path, prompt_text, prompt_language, text, text_language, cut_punc, top_k, top_p, temperature, speed, inp_refs):\r\n    if (\r\n            refer_wav_path == \"\" or refer_wav_path is None\r\n            or prompt_text == \"\" or prompt_text is None\r\n            or prompt_language == \"\" or prompt_language is None\r\n    ):\r\n        refer_wav_path, prompt_text, prompt_language = (\r\n            default_refer.path,\r\n            default_refer.text,\r\n            default_refer.language,\r\n        )\r\n        if not default_refer.is_ready():\r\n            return JSONResponse({\"code\": 400, \"message\": \"Êú™ÊåáÂÆöÂèÇËÄÉÈü≥È¢ë‰∏îÊé•Âè£Êó†È¢ÑËÆæ\"}, status_code=400)\r\n\r\n    if cut_punc == None:\r\n        text = cut_text(text,default_cut_punc)\r\n    else:\r\n        text = cut_text(text,cut_punc)\r\n\r\n    return StreamingResponse(get_tts_wav(refer_wav_path, prompt_text, prompt_language, text, text_language, top_k, top_p, temperature, speed, inp_refs), media_type=\"audio/\"+media_type)\r\n\r\n\r\n\r\n\r\n# --------------------------------\r\n# ÂàùÂßãÂåñÈÉ®ÂàÜ\r\n# --------------------------------\r\ndict_language = {\r\n    \"‰∏≠Êñá\": \"all_zh\",\r\n    \"Á≤§ËØ≠\": \"all_yue\",\r\n    \"Ëã±Êñá\": \"en\",\r\n    \"Êó•Êñá\": \"all_ja\",\r\n    \"Èü©Êñá\": \"all_ko\",\r\n    \"‰∏≠Ëã±Ê∑∑Âêà\": \"zh\",\r\n    \"Á≤§Ëã±Ê∑∑Âêà\": \"yue\",\r\n    \"Êó•Ëã±Ê∑∑Âêà\": \"ja\",\r\n    \"Èü©Ëã±Ê∑∑Âêà\": \"ko\",\r\n    \"Â§öËØ≠ÁßçÊ∑∑Âêà\": \"auto\",    #Â§öËØ≠ÁßçÂêØÂä®ÂàáÂàÜËØÜÂà´ËØ≠Áßç\r\n    \"Â§öËØ≠ÁßçÊ∑∑Âêà(Á≤§ËØ≠)\": \"auto_yue\",\r\n    \"all_zh\": \"all_zh\",\r\n    \"all_yue\": \"all_yue\",\r\n    \"en\": \"en\",\r\n    \"all_ja\": \"all_ja\",\r\n    \"all_ko\": \"all_ko\",\r\n    \"zh\": \"zh\",\r\n    \"yue\": \"yue\",\r\n    \"ja\": \"ja\",\r\n    \"ko\": \"ko\",\r\n    \"auto\": \"auto\",\r\n    \"auto_yue\": \"auto_yue\",\r\n}\r\n\r\n# logger\r\nlogging.config.dictConfig(uvicorn.config.LOGGING_CONFIG)\r\nlogger = logging.getLogger('uvicorn')\r\n\r\n# Ëé∑ÂèñÈÖçÁΩÆ\r\ng_config = global_config.Config()\r\n\r\n# Ëé∑ÂèñÂèÇÊï∞\r\nparser = argparse.ArgumentParser(description=\"GPT-SoVITS api\")\r\n\r\nparser.add_argument(\"-s\", \"--sovits_path\", type=str, default=g_config.sovits_path, help=\"SoVITSÊ®°ÂûãË∑ØÂæÑ\")\r\nparser.add_argument(\"-g\", \"--gpt_path\", type=str, default=g_config.gpt_path, help=\"GPTÊ®°ÂûãË∑ØÂæÑ\")\r\nparser.add_argument(\"-dr\", \"--default_refer_path\", type=str, default=\"\", help=\"ÈªòËÆ§ÂèÇËÄÉÈü≥È¢ëË∑ØÂæÑ\")\r\nparser.add_argument(\"-dt\", \"--default_refer_text\", type=str, default=\"\", help=\"ÈªòËÆ§ÂèÇËÄÉÈü≥È¢ëÊñáÊú¨\")\r\nparser.add_argument(\"-dl\", \"--default_refer_language\", type=str, default=\"\", help=\"ÈªòËÆ§ÂèÇËÄÉÈü≥È¢ëËØ≠Áßç\")\r\nparser.add_argument(\"-d\", \"--device\", type=str, default=g_config.infer_device, help=\"cuda / cpu\")\r\nparser.add_argument(\"-a\", \"--bind_addr\", type=str, default=\"0.0.0.0\", help=\"default: 0.0.0.0\")\r\nparser.add_argument(\"-p\", \"--port\", type=int, default=g_config.api_port, help=\"default: 9880\")\r\nparser.add_argument(\"-fp\", \"--full_precision\", action=\"store_true\", default=False, help=\"Ë¶ÜÁõñconfig.is_half‰∏∫False, ‰ΩøÁî®ÂÖ®Á≤æÂ∫¶\")\r\nparser.add_argument(\"-hp\", \"--half_precision\", action=\"store_true\", default=False, help=\"Ë¶ÜÁõñconfig.is_half‰∏∫True, ‰ΩøÁî®ÂçäÁ≤æÂ∫¶\")\r\n# boolÂÄºÁöÑÁî®Ê≥ï‰∏∫ `python ./api.py -fp ...`\r\n# Ê≠§Êó∂ full_precision==True, half_precision==False\r\nparser.add_argument(\"-sm\", \"--stream_mode\", type=str, default=\"close\", help=\"ÊµÅÂºèËøîÂõûÊ®°Âºè, close / normal / keepalive\")\r\nparser.add_argument(\"-mt\", \"--media_type\", type=str, default=\"wav\", help=\"Èü≥È¢ëÁºñÁ†ÅÊ†ºÂºè, wav / ogg / aac\")\r\nparser.add_argument(\"-st\", \"--sub_type\", type=str, default=\"int16\", help=\"Èü≥È¢ëÊï∞ÊçÆÁ±ªÂûã, int16 / int32\")\r\nparser.add_argument(\"-cp\", \"--cut_punc\", type=str, default=\"\", help=\"ÊñáÊú¨ÂàáÂàÜÁ¨¶Âè∑ËÆæÂÆö, Á¨¶Âè∑ËåÉÂõ¥,.;?!„ÄÅÔºå„ÄÇÔºüÔºÅÔºõÔºö‚Ä¶\")\r\n# ÂàáÂâ≤Â∏∏Áî®ÂàÜÂè•Á¨¶‰∏∫ `python ./api.py -cp \".?!„ÄÇÔºüÔºÅ\"`\r\nparser.add_argument(\"-hb\", \"--hubert_path\", type=str, default=g_config.cnhubert_path, help=\"Ë¶ÜÁõñconfig.cnhubert_path\")\r\nparser.add_argument(\"-b\", \"--bert_path\", type=str, default=g_config.bert_path, help=\"Ë¶ÜÁõñconfig.bert_path\")\r\n\r\nargs = parser.parse_args()\r\nsovits_path = args.sovits_path\r\ngpt_path = args.gpt_path\r\ndevice = args.device\r\nport = args.port\r\nhost = args.bind_addr\r\ncnhubert_base_path = args.hubert_path\r\nbert_path = args.bert_path\r\ndefault_cut_punc = args.cut_punc\r\n\r\n# Â∫îÁî®ÂèÇÊï∞ÈÖçÁΩÆ\r\ndefault_refer = DefaultRefer(args.default_refer_path, args.default_refer_text, args.default_refer_language)\r\n\r\n# Ê®°ÂûãË∑ØÂæÑÊ£ÄÊü•\r\nif sovits_path == \"\":\r\n    sovits_path = g_config.pretrained_sovits_path\r\n    logger.warn(f\"Êú™ÊåáÂÆöSoVITSÊ®°ÂûãË∑ØÂæÑ, fallbackÂêéÂΩìÂâçÂÄº: {sovits_path}\")\r\nif gpt_path == \"\":\r\n    gpt_path = g_config.pretrained_gpt_path\r\n    logger.warn(f\"Êú™ÊåáÂÆöGPTÊ®°ÂûãË∑ØÂæÑ, fallbackÂêéÂΩìÂâçÂÄº: {gpt_path}\")\r\n\r\n# ÊåáÂÆöÈªòËÆ§ÂèÇËÄÉÈü≥È¢ë, Ë∞ÉÁî®Êñπ Êú™Êèê‰æõ/Êú™ÁªôÂÖ® ÂèÇËÄÉÈü≥È¢ëÂèÇÊï∞Êó∂‰ΩøÁî®\r\nif default_refer.path == \"\" or default_refer.text == \"\" or default_refer.language == \"\":\r\n    default_refer.path, default_refer.text, default_refer.language = \"\", \"\", \"\"\r\n    logger.info(\"Êú™ÊåáÂÆöÈªòËÆ§ÂèÇËÄÉÈü≥È¢ë\")\r\nelse:\r\n    logger.info(f\"ÈªòËÆ§ÂèÇËÄÉÈü≥È¢ëË∑ØÂæÑ: {default_refer.path}\")\r\n    logger.info(f\"ÈªòËÆ§ÂèÇËÄÉÈü≥È¢ëÊñáÊú¨: {default_refer.text}\")\r\n    logger.info(f\"ÈªòËÆ§ÂèÇËÄÉÈü≥È¢ëËØ≠Áßç: {default_refer.language}\")\r\n\r\n# Ëé∑ÂèñÂçäÁ≤æÂ∫¶\r\nis_half = g_config.is_half\r\nif args.full_precision:\r\n    is_half = False\r\nif args.half_precision:\r\n    is_half = True\r\nif args.full_precision and args.half_precision:\r\n    is_half = g_config.is_half  # ÁÇíÈ•≠fallback\r\nlogger.info(f\"ÂçäÁ≤æ: {is_half}\")\r\n\r\n# ÊµÅÂºèËøîÂõûÊ®°Âºè\r\nif args.stream_mode.lower() in [\"normal\",\"n\"]:\r\n    stream_mode = \"normal\"\r\n    logger.info(\"ÊµÅÂºèËøîÂõûÂ∑≤ÂºÄÂêØ\")\r\nelse:\r\n    stream_mode = \"close\"\r\n\r\n# Èü≥È¢ëÁºñÁ†ÅÊ†ºÂºè\r\nif args.media_type.lower() in [\"aac\",\"ogg\"]:\r\n    media_type = args.media_type.lower()\r\nelif stream_mode == \"close\":\r\n    media_type = \"wav\"\r\nelse:\r\n    media_type = \"ogg\"\r\nlogger.info(f\"ÁºñÁ†ÅÊ†ºÂºè: {media_type}\")\r\n\r\n# Èü≥È¢ëÊï∞ÊçÆÁ±ªÂûã\r\nif args.sub_type.lower() == 'int32':\r\n    is_int32 = True\r\n    logger.info(f\"Êï∞ÊçÆÁ±ªÂûã: int32\")\r\nelse:\r\n    is_int32 = False\r\n    logger.info(f\"Êï∞ÊçÆÁ±ªÂûã: int16\")\r\n\r\n# ÂàùÂßãÂåñÊ®°Âûã\r\ncnhubert.cnhubert_base_path = cnhubert_base_path\r\ntokenizer = AutoTokenizer.from_pretrained(bert_path)\r\nbert_model = AutoModelForMaskedLM.from_pretrained(bert_path)\r\nssl_model = cnhubert.get_model()\r\nif is_half:\r\n    bert_model = bert_model.half().to(device)\r\n    ssl_model = ssl_model.half().to(device)\r\nelse:\r\n    bert_model = bert_model.to(device)\r\n    ssl_model = ssl_model.to(device)\r\nchange_gpt_sovits_weights(gpt_path = gpt_path, sovits_path = sovits_path)\r\n\r\n\r\n\r\n# --------------------------------\r\n# Êé•Âè£ÈÉ®ÂàÜ\r\n# --------------------------------\r\napp = FastAPI()\r\n\r\n@app.post(\"/set_model\")\r\nasync def set_model(request: Request):\r\n    json_post_raw = await request.json()\r\n    return change_gpt_sovits_weights(\r\n        gpt_path = json_post_raw.get(\"gpt_model_path\"), \r\n        sovits_path = json_post_raw.get(\"sovits_model_path\")\r\n    )\r\n\r\n\r\n@app.get(\"/set_model\")\r\nasync def set_model(\r\n        gpt_model_path: str = None,\r\n        sovits_model_path: str = None,\r\n):\r\n    return change_gpt_sovits_weights(gpt_path = gpt_model_path, sovits_path = sovits_model_path)\r\n\r\n\r\n@app.post(\"/control\")\r\nasync def control(request: Request):\r\n    json_post_raw = await request.json()\r\n    return handle_control(json_post_raw.get(\"command\"))\r\n\r\n\r\n@app.get(\"/control\")\r\nasync def control(command: str = None):\r\n    return handle_control(command)\r\n\r\n\r\n@app.post(\"/change_refer\")\r\nasync def change_refer(request: Request):\r\n    json_post_raw = await request.json()\r\n    return handle_change(\r\n        json_post_raw.get(\"refer_wav_path\"),\r\n        json_post_raw.get(\"prompt_text\"),\r\n        json_post_raw.get(\"prompt_language\")\r\n    )\r\n\r\n\r\n@app.get(\"/change_refer\")\r\nasync def change_refer(\r\n        refer_wav_path: str = None,\r\n        prompt_text: str = None,\r\n        prompt_language: str = None\r\n):\r\n    return handle_change(refer_wav_path, prompt_text, prompt_language)\r\n\r\n\r\n@app.post(\"/\")\r\nasync def tts_endpoint(request: Request):\r\n    json_post_raw = await request.json()\r\n    return handle(\r\n        json_post_raw.get(\"refer_wav_path\"),\r\n        json_post_raw.get(\"prompt_text\"),\r\n        json_post_raw.get(\"prompt_language\"),\r\n        json_post_raw.get(\"text\"),\r\n        json_post_raw.get(\"text_language\"),\r\n        json_post_raw.get(\"cut_punc\"),\r\n        json_post_raw.get(\"top_k\", 15),\r\n        json_post_raw.get(\"top_p\", 1.0),\r\n        json_post_raw.get(\"temperature\", 1.0),\r\n        json_post_raw.get(\"speed\", 1.0),\r\n        json_post_raw.get(\"inp_refs\", [])\r\n    )\r\n\r\n\r\n@app.get(\"/\")\r\nasync def tts_endpoint(\r\n        refer_wav_path: str = None,\r\n        prompt_text: str = None,\r\n        prompt_language: str = None,\r\n        text: str = None,\r\n        text_language: str = None,\r\n        cut_punc: str = None,\r\n        top_k: int = 15,\r\n        top_p: float = 1.0,\r\n        temperature: float = 1.0,\r\n        speed: float = 1.0,\r\n        inp_refs: list = Query(default=[])\r\n):\r\n    return handle(refer_wav_path, prompt_text, prompt_language, text, text_language, cut_punc, top_k, top_p, temperature, speed, inp_refs)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    uvicorn.run(app, host=host, port=port, workers=1)\r\n"
        },
        {
          "name": "api_v2.py",
          "type": "blob",
          "size": 17.5419921875,
          "content": "\"\"\"\n# WebAPIÊñáÊ°£\n\n` python api_v2.py -a 127.0.0.1 -p 9880 -c GPT_SoVITS/configs/tts_infer.yaml `\n\n## ÊâßË°åÂèÇÊï∞:\n    `-a` - `ÁªëÂÆöÂú∞ÂùÄ, ÈªòËÆ§\"127.0.0.1\"`\n    `-p` - `ÁªëÂÆöÁ´ØÂè£, ÈªòËÆ§9880`\n    `-c` - `TTSÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑ, ÈªòËÆ§\"GPT_SoVITS/configs/tts_infer.yaml\"`\n\n## Ë∞ÉÁî®:\n\n### Êé®ÁêÜ\n\nendpoint: `/tts`\nGET:\n```\nhttp://127.0.0.1:9880/tts?text=ÂÖàÂ∏ùÂàõ‰∏öÊú™ÂçäËÄå‰∏≠ÈÅìÂ¥©ÊÆÇÔºå‰ªäÂ§©‰∏ã‰∏âÂàÜÔºåÁõäÂ∑ûÁñ≤ÂºäÔºåÊ≠§ËØöÂç±ÊÄ•Â≠ò‰∫°‰πãÁßã‰πü„ÄÇ&text_lang=zh&ref_audio_path=archive_jingyuan_1.wav&prompt_lang=zh&prompt_text=ÊàëÊòØ„ÄåÁΩóÊµÆ„Äç‰∫ëÈ™ëÂ∞ÜÂÜõÊôØÂÖÉ„ÄÇ‰∏çÂøÖÊãòË∞®Ôºå„ÄåÂ∞ÜÂÜõ„ÄçÂè™ÊòØ‰∏ÄÊó∂ÁöÑË∫´‰ªΩÔºå‰Ω†Áß∞ÂëºÊàëÊôØÂÖÉ‰æøÂèØ&text_split_method=cut5&batch_size=1&media_type=wav&streaming_mode=true\n```\n\nPOST:\n```json\n{\n    \"text\": \"\",                   # str.(required) text to be synthesized\n    \"text_lang: \"\",               # str.(required) language of the text to be synthesized\n    \"ref_audio_path\": \"\",         # str.(required) reference audio path\n    \"aux_ref_audio_paths\": [],    # list.(optional) auxiliary reference audio paths for multi-speaker tone fusion\n    \"prompt_text\": \"\",            # str.(optional) prompt text for the reference audio\n    \"prompt_lang\": \"\",            # str.(required) language of the prompt text for the reference audio\n    \"top_k\": 5,                   # int. top k sampling\n    \"top_p\": 1,                   # float. top p sampling\n    \"temperature\": 1,             # float. temperature for sampling\n    \"text_split_method\": \"cut0\",  # str. text split method, see text_segmentation_method.py for details.\n    \"batch_size\": 1,              # int. batch size for inference\n    \"batch_threshold\": 0.75,      # float. threshold for batch splitting.\n    \"split_bucket: True,          # bool. whether to split the batch into multiple buckets.\n    \"speed_factor\":1.0,           # float. control the speed of the synthesized audio.\n    \"streaming_mode\": False,      # bool. whether to return a streaming response.\n    \"seed\": -1,                   # int. random seed for reproducibility.\n    \"parallel_infer\": True,       # bool. whether to use parallel inference.\n    \"repetition_penalty\": 1.35    # float. repetition penalty for T2S model.\n}\n```\n\nRESP:\nÊàêÂäü: Áõ¥Êé•ËøîÂõû wav Èü≥È¢ëÊµÅÔºå http code 200\nÂ§±Ë¥•: ËøîÂõûÂåÖÂê´ÈîôËØØ‰ø°ÊÅØÁöÑ json, http code 400\n\n### ÂëΩ‰ª§ÊéßÂà∂\n\nendpoint: `/control`\n\ncommand:\n\"restart\": ÈáçÊñ∞ËøêË°å\n\"exit\": ÁªìÊùüËøêË°å\n\nGET:\n```\nhttp://127.0.0.1:9880/control?command=restart\n```\nPOST:\n```json\n{\n    \"command\": \"restart\"\n}\n```\n\nRESP: Êó†\n\n\n### ÂàáÊç¢GPTÊ®°Âûã\n\nendpoint: `/set_gpt_weights`\n\nGET:\n```\nhttp://127.0.0.1:9880/set_gpt_weights?weights_path=GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt\n```\nRESP: \nÊàêÂäü: ËøîÂõû\"success\", http code 200\nÂ§±Ë¥•: ËøîÂõûÂåÖÂê´ÈîôËØØ‰ø°ÊÅØÁöÑ json, http code 400\n\n\n### ÂàáÊç¢SovitsÊ®°Âûã\n\nendpoint: `/set_sovits_weights`\n\nGET:\n```\nhttp://127.0.0.1:9880/set_sovits_weights?weights_path=GPT_SoVITS/pretrained_models/s2G488k.pth\n```\n\nRESP: \nÊàêÂäü: ËøîÂõû\"success\", http code 200\nÂ§±Ë¥•: ËøîÂõûÂåÖÂê´ÈîôËØØ‰ø°ÊÅØÁöÑ json, http code 400\n    \n\"\"\"\nimport os\nimport sys\nimport traceback\nfrom typing import Generator\n\nnow_dir = os.getcwd()\nsys.path.append(now_dir)\nsys.path.append(\"%s/GPT_SoVITS\" % (now_dir))\n\nimport argparse\nimport subprocess\nimport wave\nimport signal\nimport numpy as np\nimport soundfile as sf\nfrom fastapi import FastAPI, Request, HTTPException, Response\nfrom fastapi.responses import StreamingResponse, JSONResponse\nfrom fastapi import FastAPI, UploadFile, File\nimport uvicorn\nfrom io import BytesIO\nfrom tools.i18n.i18n import I18nAuto\nfrom GPT_SoVITS.TTS_infer_pack.TTS import TTS, TTS_Config\nfrom GPT_SoVITS.TTS_infer_pack.text_segmentation_method import get_method_names as get_cut_method_names\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\n# print(sys.path)\ni18n = I18nAuto()\ncut_method_names = get_cut_method_names()\n\nparser = argparse.ArgumentParser(description=\"GPT-SoVITS api\")\nparser.add_argument(\"-c\", \"--tts_config\", type=str, default=\"GPT_SoVITS/configs/tts_infer.yaml\", help=\"tts_inferË∑ØÂæÑ\")\nparser.add_argument(\"-a\", \"--bind_addr\", type=str, default=\"127.0.0.1\", help=\"default: 127.0.0.1\")\nparser.add_argument(\"-p\", \"--port\", type=int, default=\"9880\", help=\"default: 9880\")\nargs = parser.parse_args()\nconfig_path = args.tts_config\n# device = args.device\nport = args.port\nhost = args.bind_addr\nargv = sys.argv\n\nif config_path in [None, \"\"]:\n    config_path = \"GPT-SoVITS/configs/tts_infer.yaml\"\n\ntts_config = TTS_Config(config_path)\nprint(tts_config)\ntts_pipeline = TTS(tts_config)\n\nAPP = FastAPI()\nclass TTS_Request(BaseModel):\n    text: str = None\n    text_lang: str = None\n    ref_audio_path: str = None\n    aux_ref_audio_paths: list = None\n    prompt_lang: str = None\n    prompt_text: str = \"\"\n    top_k:int = 5\n    top_p:float = 1\n    temperature:float = 1\n    text_split_method:str = \"cut5\"\n    batch_size:int = 1\n    batch_threshold:float = 0.75\n    split_bucket:bool = True\n    speed_factor:float = 1.0\n    fragment_interval:float = 0.3\n    seed:int = -1\n    media_type:str = \"wav\"\n    streaming_mode:bool = False\n    parallel_infer:bool = True\n    repetition_penalty:float = 1.35\n\n### modify from https://github.com/RVC-Boss/GPT-SoVITS/pull/894/files\ndef pack_ogg(io_buffer:BytesIO, data:np.ndarray, rate:int):\n    with sf.SoundFile(io_buffer, mode='w', samplerate=rate, channels=1, format='ogg') as audio_file:\n        audio_file.write(data)\n    return io_buffer\n\n\ndef pack_raw(io_buffer:BytesIO, data:np.ndarray, rate:int):\n    io_buffer.write(data.tobytes())\n    return io_buffer\n\n\ndef pack_wav(io_buffer:BytesIO, data:np.ndarray, rate:int):\n    io_buffer = BytesIO()\n    sf.write(io_buffer, data, rate, format='wav')\n    return io_buffer\n\ndef pack_aac(io_buffer:BytesIO, data:np.ndarray, rate:int):\n    process = subprocess.Popen([\n        'ffmpeg',\n        '-f', 's16le',  # ËæìÂÖ•16‰ΩçÊúâÁ¨¶Âè∑Â∞èÁ´ØÊï¥Êï∞PCM\n        '-ar', str(rate),  # ËÆæÁΩÆÈááÊ†∑Áéá\n        '-ac', '1',  # ÂçïÂ£∞ÈÅì\n        '-i', 'pipe:0',  # ‰ªéÁÆ°ÈÅìËØªÂèñËæìÂÖ•\n        '-c:a', 'aac',  # Èü≥È¢ëÁºñÁ†ÅÂô®‰∏∫AAC\n        '-b:a', '192k',  # ÊØîÁâπÁéá\n        '-vn',  # ‰∏çÂåÖÂê´ËßÜÈ¢ë\n        '-f', 'adts',  # ËæìÂá∫AACÊï∞ÊçÆÊµÅÊ†ºÂºè\n        'pipe:1'  # Â∞ÜËæìÂá∫ÂÜôÂÖ•ÁÆ°ÈÅì\n    ], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, _ = process.communicate(input=data.tobytes())\n    io_buffer.write(out)\n    return io_buffer\n\ndef pack_audio(io_buffer:BytesIO, data:np.ndarray, rate:int, media_type:str):\n    if media_type == \"ogg\":\n        io_buffer = pack_ogg(io_buffer, data, rate)\n    elif media_type == \"aac\":\n        io_buffer = pack_aac(io_buffer, data, rate)\n    elif media_type == \"wav\":\n        io_buffer = pack_wav(io_buffer, data, rate)\n    else:\n        io_buffer = pack_raw(io_buffer, data, rate)\n    io_buffer.seek(0)\n    return io_buffer\n\n\n\n# from https://huggingface.co/spaces/coqui/voice-chat-with-mistral/blob/main/app.py\ndef wave_header_chunk(frame_input=b\"\", channels=1, sample_width=2, sample_rate=32000):\n    # This will create a wave header then append the frame input\n    # It should be first on a streaming wav file\n    # Other frames better should not have it (else you will hear some artifacts each chunk start)\n    wav_buf = BytesIO()\n    with wave.open(wav_buf, \"wb\") as vfout:\n        vfout.setnchannels(channels)\n        vfout.setsampwidth(sample_width)\n        vfout.setframerate(sample_rate)\n        vfout.writeframes(frame_input)\n\n    wav_buf.seek(0)\n    return wav_buf.read()\n\n\ndef handle_control(command:str):\n    if command == \"restart\":\n        os.execl(sys.executable, sys.executable, *argv)\n    elif command == \"exit\":\n        os.kill(os.getpid(), signal.SIGTERM)\n        exit(0)\n\n\ndef check_params(req:dict):\n    text:str = req.get(\"text\", \"\")\n    text_lang:str = req.get(\"text_lang\", \"\")\n    ref_audio_path:str = req.get(\"ref_audio_path\", \"\")\n    streaming_mode:bool = req.get(\"streaming_mode\", False)\n    media_type:str = req.get(\"media_type\", \"wav\")\n    prompt_lang:str = req.get(\"prompt_lang\", \"\")\n    text_split_method:str = req.get(\"text_split_method\", \"cut5\")\n\n    if ref_audio_path in [None, \"\"]:\n        return JSONResponse(status_code=400, content={\"message\": \"ref_audio_path is required\"})\n    if text in [None, \"\"]:\n        return JSONResponse(status_code=400, content={\"message\": \"text is required\"})\n    if (text_lang in [None, \"\"]) :\n        return JSONResponse(status_code=400, content={\"message\": \"text_lang is required\"})\n    elif text_lang.lower() not in tts_config.languages:\n        return JSONResponse(status_code=400, content={\"message\": f\"text_lang: {text_lang} is not supported in version {tts_config.version}\"})\n    if (prompt_lang in [None, \"\"]) :\n        return JSONResponse(status_code=400, content={\"message\": \"prompt_lang is required\"})\n    elif prompt_lang.lower() not in tts_config.languages:\n        return JSONResponse(status_code=400, content={\"message\": f\"prompt_lang: {prompt_lang} is not supported in version {tts_config.version}\"})\n    if media_type not in [\"wav\", \"raw\", \"ogg\", \"aac\"]:\n        return JSONResponse(status_code=400, content={\"message\": f\"media_type: {media_type} is not supported\"})\n    elif media_type == \"ogg\" and  not streaming_mode:\n        return JSONResponse(status_code=400, content={\"message\": \"ogg format is not supported in non-streaming mode\"})\n    \n    if text_split_method not in cut_method_names:\n        return JSONResponse(status_code=400, content={\"message\": f\"text_split_method:{text_split_method} is not supported\"})\n\n    return None\n\nasync def tts_handle(req:dict):\n    \"\"\"\n    Text to speech handler.\n    \n    Args:\n        req (dict): \n            {\n                \"text\": \"\",                   # str.(required) text to be synthesized\n                \"text_lang: \"\",               # str.(required) language of the text to be synthesized\n                \"ref_audio_path\": \"\",         # str.(required) reference audio path\n                \"aux_ref_audio_paths\": [],    # list.(optional) auxiliary reference audio paths for multi-speaker synthesis\n                \"prompt_text\": \"\",            # str.(optional) prompt text for the reference audio\n                \"prompt_lang\": \"\",            # str.(required) language of the prompt text for the reference audio\n                \"top_k\": 5,                   # int. top k sampling\n                \"top_p\": 1,                   # float. top p sampling\n                \"temperature\": 1,             # float. temperature for sampling\n                \"text_split_method\": \"cut5\",  # str. text split method, see text_segmentation_method.py for details.\n                \"batch_size\": 1,              # int. batch size for inference\n                \"batch_threshold\": 0.75,      # float. threshold for batch splitting.\n                \"split_bucket: True,          # bool. whether to split the batch into multiple buckets.\n                \"speed_factor\":1.0,           # float. control the speed of the synthesized audio.\n                \"fragment_interval\":0.3,      # float. to control the interval of the audio fragment.\n                \"seed\": -1,                   # int. random seed for reproducibility.\n                \"media_type\": \"wav\",          # str. media type of the output audio, support \"wav\", \"raw\", \"ogg\", \"aac\".\n                \"streaming_mode\": False,      # bool. whether to return a streaming response.\n                \"parallel_infer\": True,       # bool.(optional) whether to use parallel inference.\n                \"repetition_penalty\": 1.35    # float.(optional) repetition penalty for T2S model.          \n            }\n    returns:\n        StreamingResponse: audio stream response.\n    \"\"\"\n    \n    streaming_mode = req.get(\"streaming_mode\", False)\n    return_fragment = req.get(\"return_fragment\", False)\n    media_type = req.get(\"media_type\", \"wav\")\n\n    check_res = check_params(req)\n    if check_res is not None:\n        return check_res\n\n    if streaming_mode or return_fragment:\n        req[\"return_fragment\"] = True\n    \n    try:\n        tts_generator=tts_pipeline.run(req)\n        \n        if streaming_mode:\n            def streaming_generator(tts_generator:Generator, media_type:str):\n                if media_type == \"wav\":\n                    yield wave_header_chunk()\n                    media_type = \"raw\"\n                for sr, chunk in tts_generator:\n                    yield pack_audio(BytesIO(), chunk, sr, media_type).getvalue()\n            # _media_type = f\"audio/{media_type}\" if not (streaming_mode and media_type in [\"wav\", \"raw\"]) else f\"audio/x-{media_type}\"\n            return StreamingResponse(streaming_generator(tts_generator, media_type, ), media_type=f\"audio/{media_type}\")\n    \n        else:\n            sr, audio_data = next(tts_generator)\n            audio_data = pack_audio(BytesIO(), audio_data, sr, media_type).getvalue()\n            return Response(audio_data, media_type=f\"audio/{media_type}\")\n    except Exception as e:\n        return JSONResponse(status_code=400, content={\"message\": f\"tts failed\", \"Exception\": str(e)})\n    \n\n\n\n\n\n@APP.get(\"/control\")\nasync def control(command: str = None):\n    if command is None:\n        return JSONResponse(status_code=400, content={\"message\": \"command is required\"})\n    handle_control(command)\n\n\n\n@APP.get(\"/tts\")\nasync def tts_get_endpoint(\n                        text: str = None,\n                        text_lang: str = None,\n                        ref_audio_path: str = None,\n                        aux_ref_audio_paths:list = None,\n                        prompt_lang: str = None,\n                        prompt_text: str = \"\",\n                        top_k:int = 5,\n                        top_p:float = 1,\n                        temperature:float = 1,\n                        text_split_method:str = \"cut0\",\n                        batch_size:int = 1,\n                        batch_threshold:float = 0.75,\n                        split_bucket:bool = True,\n                        speed_factor:float = 1.0,\n                        fragment_interval:float = 0.3,\n                        seed:int = -1,\n                        media_type:str = \"wav\",\n                        streaming_mode:bool = False,\n                        parallel_infer:bool = True,\n                        repetition_penalty:float = 1.35\n                        ):\n    req = {\n        \"text\": text,\n        \"text_lang\": text_lang.lower(),\n        \"ref_audio_path\": ref_audio_path,\n        \"aux_ref_audio_paths\": aux_ref_audio_paths,\n        \"prompt_text\": prompt_text,\n        \"prompt_lang\": prompt_lang.lower(),\n        \"top_k\": top_k,\n        \"top_p\": top_p,\n        \"temperature\": temperature,\n        \"text_split_method\": text_split_method,\n        \"batch_size\":int(batch_size),\n        \"batch_threshold\":float(batch_threshold),\n        \"speed_factor\":float(speed_factor),\n        \"split_bucket\":split_bucket,\n        \"fragment_interval\":fragment_interval,\n        \"seed\":seed,\n        \"media_type\":media_type,\n        \"streaming_mode\":streaming_mode,\n        \"parallel_infer\":parallel_infer,\n        \"repetition_penalty\":float(repetition_penalty)\n    }\n    return await tts_handle(req)\n                \n\n@APP.post(\"/tts\")\nasync def tts_post_endpoint(request: TTS_Request):\n    req = request.dict()\n    return await tts_handle(req)\n\n\n@APP.get(\"/set_refer_audio\")\nasync def set_refer_aduio(refer_audio_path: str = None):\n    try:\n        tts_pipeline.set_ref_audio(refer_audio_path)\n    except Exception as e:\n        return JSONResponse(status_code=400, content={\"message\": f\"set refer audio failed\", \"Exception\": str(e)})\n    return JSONResponse(status_code=200, content={\"message\": \"success\"})\n\n\n# @APP.post(\"/set_refer_audio\")\n# async def set_refer_aduio_post(audio_file: UploadFile = File(...)):\n#     try:\n#         # Ê£ÄÊü•Êñá‰ª∂Á±ªÂûãÔºåÁ°Æ‰øùÊòØÈü≥È¢ëÊñá‰ª∂\n#         if not audio_file.content_type.startswith(\"audio/\"):\n#             return JSONResponse(status_code=400, content={\"message\": \"file type is not supported\"})\n        \n#         os.makedirs(\"uploaded_audio\", exist_ok=True)\n#         save_path = os.path.join(\"uploaded_audio\", audio_file.filename)\n#         # ‰øùÂ≠òÈü≥È¢ëÊñá‰ª∂Âà∞ÊúçÂä°Âô®‰∏äÁöÑ‰∏Ä‰∏™ÁõÆÂΩï\n#         with open(save_path , \"wb\") as buffer:\n#             buffer.write(await audio_file.read())\n            \n#         tts_pipeline.set_ref_audio(save_path)\n#     except Exception as e:\n#         return JSONResponse(status_code=400, content={\"message\": f\"set refer audio failed\", \"Exception\": str(e)})\n#     return JSONResponse(status_code=200, content={\"message\": \"success\"})\n\n@APP.get(\"/set_gpt_weights\")\nasync def set_gpt_weights(weights_path: str = None):\n    try:\n        if weights_path in [\"\", None]:\n            return JSONResponse(status_code=400, content={\"message\": \"gpt weight path is required\"})\n        tts_pipeline.init_t2s_weights(weights_path)\n    except Exception as e:\n        return JSONResponse(status_code=400, content={\"message\": f\"change gpt weight failed\", \"Exception\": str(e)})\n\n    return JSONResponse(status_code=200, content={\"message\": \"success\"})\n\n\n@APP.get(\"/set_sovits_weights\")\nasync def set_sovits_weights(weights_path: str = None):\n    try:\n        if weights_path in [\"\", None]:\n            return JSONResponse(status_code=400, content={\"message\": \"sovits weight path is required\"})\n        tts_pipeline.init_vits_weights(weights_path)\n    except Exception as e:\n        return JSONResponse(status_code=400, content={\"message\": f\"change sovits weight failed\", \"Exception\": str(e)})\n    return JSONResponse(status_code=200, content={\"message\": \"success\"})\n\n\n\nif __name__ == \"__main__\":\n    try:\n        if host == 'None':   # Âú®Ë∞ÉÁî®Êó∂‰ΩøÁî® -a None ÂèÇÊï∞ÔºåÂèØ‰ª•ËÆ©apiÁõëÂê¨ÂèåÊ†à\n            host = None\n        uvicorn.run(app=APP, host=host, port=port, workers=1)\n    except Exception as e:\n        traceback.print_exc()\n        os.kill(os.getpid(), signal.SIGTERM)\n        exit(0)\n"
        },
        {
          "name": "colab_webui.ipynb",
          "type": "blob",
          "size": 3.2890625,
          "content": "{\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0,\n  \"metadata\": {\n    \"colab\": {\n      \"provenance\": [],\n      \"include_colab_link\": true\n    },\n    \"kernelspec\": {\n      \"name\": \"python3\",\n      \"display_name\": \"Python 3\"\n    },\n    \"accelerator\": \"GPU\"\n  },\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"view-in-github\",\n        \"colab_type\": \"text\"\n      },\n      \"source\": [\n        \"<a href=\\\"https://colab.research.google.com/github/RVC-Boss/GPT-SoVITS/blob/main/colab_webui.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"source\": [\n        \"ÁéØÂ¢ÉÈÖçÁΩÆ environment\"\n      ],\n      \"metadata\": {\n        \"id\": \"_o6a8GS2lWQM\"\n      }\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"e9b7iFV3dm1f\"\n      },\n      \"source\": [\n        \"!pip install -q condacolab\\n\",\n        \"# Setting up condacolab and installing packages\\n\",\n        \"import condacolab\\n\",\n        \"condacolab.install_from_url(\\\"https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh\\\")\\n\",\n        \"%cd -q /content\\n\",\n        \"!git clone https://github.com/RVC-Boss/GPT-SoVITS\\n\",\n        \"!conda install -y -q -c pytorch -c nvidia cudatoolkit\\n\",\n        \"%cd -q /content/GPT-SoVITS\\n\",\n        \"!conda install -y -q -c conda-forge gcc gxx ffmpeg cmake -c pytorch -c nvidia\\n\",\n        \"!/usr/local/bin/pip install -r requirements.txt\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# @title Download pretrained models ‰∏ãËΩΩÈ¢ÑËÆ≠ÁªÉÊ®°Âûã\\n\",\n        \"!mkdir -p /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\\n\",\n        \"!mkdir -p /content/GPT-SoVITS/tools/damo_asr/models\\n\",\n        \"!mkdir -p /content/GPT-SoVITS/tools/uvr5\\n\",\n        \"%cd /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\\n\",\n        \"!git clone https://huggingface.co/lj1995/GPT-SoVITS\\n\",\n        \"%cd /content/GPT-SoVITS/tools/damo_asr/models\\n\",\n        \"!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\\n\",\n        \"!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\\n\",\n        \"!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\\n\",\n        \"# @title UVR5 pretrains ÂÆâË£Öuvr5Ê®°Âûã\\n\",\n        \"%cd /content/GPT-SoVITS/tools/uvr5\\n\",\n        \"%rm -r uvr5_weights\\n\",\n        \"!git clone https://huggingface.co/Delik/uvr5_weights\\n\",\n        \"!git config core.sparseCheckout true\\n\",\n        \"!mv /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/\"\n      ],\n      \"metadata\": {\n        \"id\": \"0NgxXg5sjv7z\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"source\": [\n        \"# @title launch WebUI ÂêØÂä®WebUI\\n\",\n        \"!/usr/local/bin/pip install ipykernel\\n\",\n        \"!sed -i '10s/False/True/' /content/GPT-SoVITS/config.py\\n\",\n        \"%cd /content/GPT-SoVITS/\\n\",\n        \"!/usr/local/bin/python  webui.py\"\n      ],\n      \"metadata\": {\n        \"id\": \"4oRGUzkrk8C7\"\n      },\n      \"execution_count\": null,\n      \"outputs\": []\n    }\n  ]\n}\n"
        },
        {
          "name": "config.py",
          "type": "blob",
          "size": 1.984375,
          "content": "import sys,os\r\n\r\nimport torch\r\n\r\n# Êé®ÁêÜÁî®ÁöÑÊåáÂÆöÊ®°Âûã\r\nsovits_path = \"\"\r\ngpt_path = \"\"\r\nis_half_str = os.environ.get(\"is_half\", \"True\")\r\nis_half = True if is_half_str.lower() == 'true' else False\r\nis_share_str = os.environ.get(\"is_share\",\"False\")\r\nis_share= True if is_share_str.lower() == 'true' else False\r\n\r\ncnhubert_path = \"GPT_SoVITS/pretrained_models/chinese-hubert-base\"\r\nbert_path = \"GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\"\r\npretrained_sovits_path = \"GPT_SoVITS/pretrained_models/s2G488k.pth\"\r\npretrained_gpt_path = \"GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt\"\r\n\r\nexp_root = \"logs\"\r\npython_exec = sys.executable or \"python\"\r\nif torch.cuda.is_available():\r\n    infer_device = \"cuda\"\r\nelse:\r\n    infer_device = \"cpu\"\r\n\r\nwebui_port_main = 9874\r\nwebui_port_uvr5 = 9873\r\nwebui_port_infer_tts = 9872\r\nwebui_port_subfix = 9871\r\n\r\napi_port = 9880\r\n\r\nif infer_device == \"cuda\":\r\n    gpu_name = torch.cuda.get_device_name(0)\r\n    if (\r\n            (\"16\" in gpu_name and \"V100\" not in gpu_name.upper())\r\n            or \"P40\" in gpu_name.upper()\r\n            or \"P10\" in gpu_name.upper()\r\n            or \"1060\" in gpu_name\r\n            or \"1070\" in gpu_name\r\n            or \"1080\" in gpu_name\r\n    ):\r\n        is_half=False\r\n\r\nif(infer_device==\"cpu\"):is_half=False\r\n\r\nclass Config:\r\n    def __init__(self):\r\n        self.sovits_path = sovits_path\r\n        self.gpt_path = gpt_path\r\n        self.is_half = is_half\r\n\r\n        self.cnhubert_path = cnhubert_path\r\n        self.bert_path = bert_path\r\n        self.pretrained_sovits_path = pretrained_sovits_path\r\n        self.pretrained_gpt_path = pretrained_gpt_path\r\n\r\n        self.exp_root = exp_root\r\n        self.python_exec = python_exec\r\n        self.infer_device = infer_device\r\n\r\n        self.webui_port_main = webui_port_main\r\n        self.webui_port_uvr5 = webui_port_uvr5\r\n        self.webui_port_infer_tts = webui_port_infer_tts\r\n        self.webui_port_subfix = webui_port_subfix\r\n\r\n        self.api_port = api_port\r\n"
        },
        {
          "name": "docker-compose.yaml",
          "type": "blob",
          "size": 1.0107421875,
          "content": "version: '3.8'\n\nservices:\n  gpt-sovits:\n    image: breakstring/gpt-sovits:latest   # please change the image name and tag base your environment. If the tag contains the word 'elite', such as \"latest-elite\", it indicates that the image does not include the necessary models such as GPT-SoVITS, UVR5, Damo ASR, etc. You will need to download them yourself and map them into the container.\n    container_name: gpt-sovits-container\n    environment:\n      - is_half=False\n      - is_share=False\n    volumes:\n      - ./output:/workspace/output\n      - ./logs:/workspace/logs\n      - ./SoVITS_weights:/workspace/SoVITS_weights\n      - ./reference:/workspace/reference\n    working_dir: /workspace\n    ports:\n      - \"9880:9880\"\n      - \"9871:9871\"\n      - \"9872:9872\"\n      - \"9873:9873\"\n      - \"9874:9874\"\n    shm_size: 16G\n    deploy:\n      resources:\n        reservations:\n          devices:\n          - driver: nvidia\n            count: \"all\"\n            capabilities: [gpu]\n    stdin_open: true\n    tty: true\n    restart: unless-stopped\n"
        },
        {
          "name": "dockerbuild.sh",
          "type": "blob",
          "size": 1,
          "content": "#!/bin/bash\n\n# Ëé∑ÂèñÂΩìÂâçÊó•ÊúüÔºåÊ†ºÂºè‰∏∫ YYYYMMDD\nDATE=$(date +%Y%m%d)\n# Ëé∑ÂèñÊúÄÊñ∞ÁöÑ Git commit ÂìàÂ∏åÂÄºÁöÑÂâç 7 ‰Ωç\nCOMMIT_HASH=$(git rev-parse HEAD | cut -c 1-7)\n\n# ÊûÑÂª∫ full ÁâàÊú¨ÁöÑÈïúÂÉè\ndocker build --build-arg IMAGE_TYPE=full -t breakstring/gpt-sovits:latest .\n# ‰∏∫Âêå‰∏Ä‰∏™ÈïúÂÉèÊ∑ªÂä†Â∏¶Êó•ÊúüÁöÑÊ†áÁ≠æ\ndocker tag breakstring/gpt-sovits:latest breakstring/gpt-sovits:dev-$DATE\n# ‰∏∫Âêå‰∏Ä‰∏™ÈïúÂÉèÊ∑ªÂä†Â∏¶ÂΩìÂâç‰ª£Á†ÅÂ∫ìCommitÂìàÂ∏åÂÄºÁöÑÊ†áÁ≠æ\ndocker tag breakstring/gpt-sovits:latest breakstring/gpt-sovits:dev-$COMMIT_HASH\n\n\n# ÊûÑÂª∫ elite ÁâàÊú¨ÁöÑÈïúÂÉè(Êó†Ê®°Âûã‰∏ãËΩΩÊ≠•È™§ÔºåÈúÄÊâãÂ∑•Â∞ÜÊ®°Âûã‰∏ãËΩΩÂÆâË£ÖËøõÂÆπÂô®)\ndocker build --build-arg IMAGE_TYPE=elite -t breakstring/gpt-sovits:latest-elite .\n# ‰∏∫Âêå‰∏Ä‰∏™ÈïúÂÉèÊ∑ªÂä†Â∏¶Êó•ÊúüÁöÑÊ†áÁ≠æ\ndocker tag breakstring/gpt-sovits:latest-elite breakstring/gpt-sovits:dev-$DATE-elite\n# ‰∏∫Âêå‰∏Ä‰∏™ÈïúÂÉèÊ∑ªÂä†Â∏¶ÂΩìÂâç‰ª£Á†ÅÂ∫ìCommitÂìàÂ∏åÂÄºÁöÑÊ†áÁ≠æ\ndocker tag breakstring/gpt-sovits:latest-elite breakstring/gpt-sovits:dev-$COMMIT_HASH-elite\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "go-webui.bat",
          "type": "blob",
          "size": 0.041015625,
          "content": "runtime\\python.exe webui.py zh_CN\r\npause\r\n"
        },
        {
          "name": "go-webui.ps1",
          "type": "blob",
          "size": 0.1259765625,
          "content": "$ErrorActionPreference = \"SilentlyContinue\"\nchcp 65001\n& \"$PSScriptRoot\\runtime\\python.exe\" \"$PSScriptRoot\\webui.py zh_CN\"\npause\n"
        },
        {
          "name": "gpt-sovits_kaggle.ipynb",
          "type": "blob",
          "size": 6.9677734375,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"45857cb2\",\n   \"metadata\": {\n    \"_cell_guid\": \"b1076dfc-b9ad-4769-8c92-a6c4dae69d19\",\n    \"_uuid\": \"8f2839f25d086af736a60e9eeb907d3b93b6e0e5\",\n    \"execution\": {\n     \"iopub.execute_input\": \"2024-02-18T14:43:46.735480Z\",\n     \"iopub.status.busy\": \"2024-02-18T14:43:46.735183Z\",\n     \"iopub.status.idle\": \"2024-02-18T14:48:10.724175Z\",\n     \"shell.execute_reply\": \"2024-02-18T14:48:10.723059Z\"\n    },\n    \"papermill\": {\n     \"duration\": 263.994935,\n     \"end_time\": \"2024-02-18T14:48:10.726613\",\n     \"exception\": false,\n     \"start_time\": \"2024-02-18T14:43:46.731678\",\n     \"status\": \"completed\"\n    },\n    \"tags\": []\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"!git clone https://github.com/RVC-Boss/GPT-SoVITS.git\\n\",\n    \"%cd GPT-SoVITS\\n\",\n    \"!apt-get update && apt-get install -y --no-install-recommends tzdata ffmpeg libsox-dev parallel aria2 git git-lfs && git lfs install\\n\",\n    \"!pip install -r requirements.txt\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"b9d346b4\",\n   \"metadata\": {\n    \"execution\": {\n     \"iopub.execute_input\": \"2024-02-18T14:48:10.815802Z\",\n     \"iopub.status.busy\": \"2024-02-18T14:48:10.814899Z\",\n     \"iopub.status.idle\": \"2024-02-18T14:50:31.253276Z\",\n     \"shell.execute_reply\": \"2024-02-18T14:50:31.252024Z\"\n    },\n    \"papermill\": {\n     \"duration\": 140.484893,\n     \"end_time\": \"2024-02-18T14:50:31.255720\",\n     \"exception\": false,\n     \"start_time\": \"2024-02-18T14:48:10.770827\",\n     \"status\": \"completed\"\n    },\n    \"tags\": []\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# @title Download pretrained models ‰∏ãËΩΩÈ¢ÑËÆ≠ÁªÉÊ®°Âûã\\n\",\n    \"!mkdir -p /kaggle/working/GPT-SoVITS/GPT_SoVITS/pretrained_models\\n\",\n    \"!mkdir -p /kaggle/working/GPT-SoVITS/tools/asr/models\\n\",\n    \"!mkdir -p /kaggle/working/GPT-SoVITS/tools/uvr5\\n\",\n    \"%cd /kaggle/working/GPT-SoVITS/GPT_SoVITS/pretrained_models\\n\",\n    \"!git clone https://huggingface.co/lj1995/GPT-SoVITS\\n\",\n    \"%cd /kaggle/working/GPT-SoVITS/tools/asr/models\\n\",\n    \"!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\\n\",\n    \"!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\\n\",\n    \"!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\\n\",\n    \"# # @title UVR5 pretrains ÂÆâË£Öuvr5Ê®°Âûã\\n\",\n    \"%cd /kaggle/working/GPT-SoVITS/tools/uvr5\\n\",\n    \"!git clone https://huggingface.co/Delik/uvr5_weights\\n\",\n    \"!git config core.sparseCheckout true\\n\",\n    \"!mv /kaggle/working/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /kaggle/working/GPT-SoVITS/GPT_SoVITS/pretrained_models/\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"ea94d245\",\n   \"metadata\": {\n    \"execution\": {\n     \"iopub.execute_input\": \"2024-02-18T14:29:01.071549Z\",\n     \"iopub.status.busy\": \"2024-02-18T14:29:01.070592Z\",\n     \"iopub.status.idle\": \"2024-02-18T14:40:45.318368Z\",\n     \"shell.execute_reply\": \"2024-02-18T14:40:45.317130Z\",\n     \"shell.execute_reply.started\": \"2024-02-18T14:29:01.071512Z\"\n    },\n    \"papermill\": {\n     \"duration\": null,\n     \"end_time\": null,\n     \"exception\": false,\n     \"start_time\": \"2024-02-18T14:50:31.309013\",\n     \"status\": \"running\"\n    },\n    \"tags\": []\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# @title launch WebUI ÂêØÂä®WebUI\\n\",\n    \"%cd /kaggle/working/GPT-SoVITS/\\n\",\n    \"!npm install -g localtunnel\\n\",\n    \"import subprocess\\n\",\n    \"import threading\\n\",\n    \"import time\\n\",\n    \"import socket\\n\",\n    \"import urllib.request\\n\",\n    \"def iframe_thread(port):\\n\",\n    \"    while True:\\n\",\n    \"        time.sleep(0.5)\\n\",\n    \"        sock= socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n\",\n    \"        result = sock.connect_ex(('127.0.0.1', port))\\n\",\n    \"        if result == 0:\\n\",\n    \"            break\\n\",\n    \"        sock.close()\\n\",\n    \"\\n\",\n    \"        from colorama import Fore, Style\\n\",\n    \"    print (Fore.GREEN + \\\"\\\\nIP: \\\", Fore. RED, urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\\\"\\\\n\\\"), \\\"\\\\n\\\", Style. RESET_ALL)\\n\",\n    \"    p = subprocess.Popen([\\\"lt\\\", \\\"--port\\\", \\\"{}\\\".format(port)], stdout=subprocess.PIPE)\\n\",\n    \"    for line in p.stdout:\\n\",\n    \"        print(line.decode(), end='')\\n\",\n    \"threading.Thread (target=iframe_thread, daemon=True, args=(9874,)).start()\\n\",\n    \"\\n\",\n    \"!python  webui.py\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"dda88a6d\",\n   \"metadata\": {\n    \"execution\": {\n     \"iopub.execute_input\": \"2024-02-18T14:40:56.880608Z\",\n     \"iopub.status.busy\": \"2024-02-18T14:40:56.879879Z\"\n    },\n    \"papermill\": {\n     \"duration\": null,\n     \"end_time\": null,\n     \"exception\": null,\n     \"start_time\": null,\n     \"status\": \"pending\"\n    },\n    \"tags\": []\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# ÂºÄÂêØÊé®ÁêÜÈ°µÈù¢\\n\",\n    \"%cd /kaggle/working/GPT-SoVITS/\\n\",\n    \"!npm install -g localtunnel\\n\",\n    \"import subprocess\\n\",\n    \"import threading\\n\",\n    \"import time\\n\",\n    \"import socket\\n\",\n    \"import urllib.request\\n\",\n    \"def iframe_thread(port):\\n\",\n    \"    while True:\\n\",\n    \"        time.sleep(0.5)\\n\",\n    \"        sock= socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n\",\n    \"        result = sock.connect_ex(('127.0.0.1', port))\\n\",\n    \"        if result == 0:\\n\",\n    \"            break\\n\",\n    \"        sock.close()\\n\",\n    \"\\n\",\n    \"        from colorama import Fore, Style\\n\",\n    \"    print (Fore.GREEN + \\\"\\\\nIP: \\\", Fore. RED, urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\\\"\\\\n\\\"), \\\"\\\\n\\\", Style. RESET_ALL)\\n\",\n    \"    p = subprocess.Popen([\\\"lt\\\", \\\"--port\\\", \\\"{}\\\".format(port)], stdout=subprocess.PIPE)\\n\",\n    \"    for line in p.stdout:\\n\",\n    \"        print(line.decode(), end='')\\n\",\n    \"threading.Thread (target=iframe_thread, daemon=True, args=(9872,)).start()\\n\",\n    \"\\n\",\n    \"!python  ./GPT_SoVITS/inference_webui.py\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kaggle\": {\n   \"accelerator\": \"nvidiaTeslaT4\",\n   \"dataSources\": [\n    {\n     \"datasetId\": 4459328,\n     \"sourceId\": 7649639,\n     \"sourceType\": \"datasetVersion\"\n    }\n   ],\n   \"dockerImageVersionId\": 30646,\n   \"isGpuEnabled\": true,\n   \"isInternetEnabled\": true,\n   \"language\": \"python\",\n   \"sourceType\": \"notebook\"\n  },\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.10.13\"\n  },\n  \"papermill\": {\n   \"default_parameters\": {},\n   \"duration\": null,\n   \"end_time\": null,\n   \"environment_variables\": {},\n   \"exception\": null,\n   \"input_path\": \"__notebook__.ipynb\",\n   \"output_path\": \"__notebook__.ipynb\",\n   \"parameters\": {},\n   \"start_time\": \"2024-02-18T14:43:44.011910\",\n   \"version\": \"2.5.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}\n"
        },
        {
          "name": "install.sh",
          "type": "blob",
          "size": 0.2373046875,
          "content": "#!/bin/bash\nconda install -c conda-forge gcc\nconda install -c conda-forge gxx\nconda install ffmpeg cmake\nconda install pytorch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 pytorch-cuda=11.8 -c pytorch -c nvidia\npip install -r requirements.txt\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.560546875,
          "content": "numpy==1.23.4\nscipy\ntensorboard\nlibrosa==0.9.2\nnumba==0.56.4\npytorch-lightning\ngradio>=4.0,<=4.24.0\nffmpeg-python\nonnxruntime; sys_platform == 'darwin'\nonnxruntime-gpu; sys_platform != 'darwin'\ntqdm\nfunasr==1.0.27\ncn2an\npypinyin\npyopenjtalk>=0.3.4\ng2p_en\ntorchaudio\nmodelscope==1.10.0\nsentencepiece\ntransformers\nchardet\nPyYAML\npsutil\njieba_fast\njieba\nLangSegment>=0.2.0\nFaster_Whisper\nwordsegment\nrotary_embedding_torch\npyjyutping \ng2pk2\nko_pron\nopencc; sys_platform != 'linux'\nopencc==1.1.1; sys_platform == 'linux'\npython_mecab_ko; sys_platform != 'win32'\nfastapi<0.112.2\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "webui.py",
          "type": "blob",
          "size": 58.5859375,
          "content": "import os,sys\r\nif len(sys.argv)==1:sys.argv.append('v2')\r\nversion=\"v1\"if sys.argv[1]==\"v1\" else\"v2\"\r\nos.environ[\"version\"]=version\r\nnow_dir = os.getcwd()\r\nsys.path.insert(0, now_dir)\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\nimport json,yaml,torch,pdb,re,shutil\r\nimport platform\r\nimport psutil\r\nimport signal\r\ntorch.manual_seed(233333)\r\ntmp = os.path.join(now_dir, \"TEMP\")\r\nos.makedirs(tmp, exist_ok=True)\r\nos.environ[\"TEMP\"] = tmp\r\nif(os.path.exists(tmp)):\r\n    for name in os.listdir(tmp):\r\n        if(name==\"jieba.cache\"):continue\r\n        path=\"%s/%s\"%(tmp,name)\r\n        delete=os.remove if os.path.isfile(path) else shutil.rmtree\r\n        try:\r\n            delete(path)\r\n        except Exception as e:\r\n            print(str(e))\r\n            pass\r\nimport site\r\nimport traceback\r\nsite_packages_roots = []\r\nfor path in site.getsitepackages():\r\n    if \"packages\" in path:\r\n        site_packages_roots.append(path)\r\nif(site_packages_roots==[]):site_packages_roots=[\"%s/runtime/Lib/site-packages\" % now_dir]\r\n#os.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\r\nos.environ[\"no_proxy\"] = \"localhost, 127.0.0.1, ::1\"\r\nos.environ[\"all_proxy\"] = \"\"\r\nfor site_packages_root in site_packages_roots:\r\n    if os.path.exists(site_packages_root):\r\n        try:\r\n            with open(\"%s/users.pth\" % (site_packages_root), \"w\") as f:\r\n                f.write(\r\n                    \"%s\\n%s/tools\\n%s/tools/asr\\n%s/GPT_SoVITS\\n%s/tools/uvr5\"\r\n                    % (now_dir, now_dir, now_dir, now_dir, now_dir)\r\n                )\r\n            break\r\n        except PermissionError as e:\r\n            traceback.print_exc()\r\nfrom tools import my_utils\r\nimport shutil\r\nimport pdb\r\nfrom subprocess import Popen\r\nimport signal\r\nfrom config import python_exec,infer_device,is_half,exp_root,webui_port_main,webui_port_infer_tts,webui_port_uvr5,webui_port_subfix,is_share\r\nfrom tools.i18n.i18n import I18nAuto, scan_language_list\r\nlanguage=sys.argv[-1] if sys.argv[-1] in scan_language_list() else \"Auto\"\r\nos.environ[\"language\"]=language\r\ni18n = I18nAuto(language=language)\r\nfrom scipy.io import wavfile\r\nfrom tools.my_utils import load_audio, check_for_existance, check_details\r\nfrom multiprocessing import cpu_count\r\n# os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1' # ÂΩìÈÅáÂà∞mps‰∏çÊîØÊåÅÁöÑÊ≠•È™§Êó∂‰ΩøÁî®cpu\r\ntry:\r\n    import gradio.analytics as analytics\r\n    analytics.version_check = lambda:None\r\nexcept:...\r\nimport gradio as gr\r\nn_cpu=cpu_count()\r\n           \r\nngpu = torch.cuda.device_count()\r\ngpu_infos = []\r\nmem = []\r\nif_gpu_ok = False\r\n\r\n# Âà§Êñ≠ÊòØÂê¶ÊúâËÉΩÁî®Êù•ËÆ≠ÁªÉÂíåÂä†ÈÄüÊé®ÁêÜÁöÑNÂç°\r\nok_gpu_keywords={\"10\",\"16\",\"20\",\"30\",\"40\",\"A2\",\"A3\",\"A4\",\"P4\",\"A50\",\"500\",\"A60\",\"70\",\"80\",\"90\",\"M4\",\"T4\",\"TITAN\",\"L4\",\"4060\",\"H\",\"600\"}\r\nset_gpu_numbers=set()\r\nif torch.cuda.is_available() or ngpu != 0:\r\n    for i in range(ngpu):\r\n        gpu_name = torch.cuda.get_device_name(i)\r\n        if any(value in gpu_name.upper()for value in ok_gpu_keywords):\r\n            # A10#A100#V100#A40#P40#M40#K80#A4500\r\n            if_gpu_ok = True  # Ëá≥Â∞ëÊúâ‰∏ÄÂº†ËÉΩÁî®ÁöÑNÂç°\r\n            gpu_infos.append(\"%s\\t%s\" % (i, gpu_name))\r\n            set_gpu_numbers.add(i)\r\n            mem.append(int(torch.cuda.get_device_properties(i).total_memory/ 1024/ 1024/ 1024+ 0.4))\r\n# # Âà§Êñ≠ÊòØÂê¶ÊîØÊåÅmpsÂä†ÈÄü\r\n# if torch.backends.mps.is_available():\r\n#     if_gpu_ok = True\r\n#     gpu_infos.append(\"%s\\t%s\" % (\"0\", \"Apple GPU\"))\r\n#     mem.append(psutil.virtual_memory().total/ 1024 / 1024 / 1024) # ÂÆûÊµã‰ΩøÁî®Á≥ªÁªüÂÜÖÂ≠ò‰Ωú‰∏∫ÊòæÂ≠ò‰∏ç‰ºöÁàÜÊòæÂ≠ò\r\n\r\nif if_gpu_ok and len(gpu_infos) > 0:\r\n    gpu_info = \"\\n\".join(gpu_infos)\r\n    default_batch_size = min(mem) // 2\r\nelse:\r\n    gpu_info = (\"%s\\t%s\" % (\"0\", \"CPU\"))\r\n    gpu_infos.append(\"%s\\t%s\" % (\"0\", \"CPU\"))\r\n    set_gpu_numbers.add(0)\r\n    default_batch_size = int(psutil.virtual_memory().total/ 1024 / 1024 / 1024 / 2)\r\ngpus = \"-\".join([i[0] for i in gpu_infos])\r\ndefault_gpu_numbers=str(sorted(list(set_gpu_numbers))[0])\r\ndef fix_gpu_number(input):#Â∞ÜË∂äÁïåÁöÑnumberÂº∫Âà∂ÊîπÂà∞ÁïåÂÜÖ\r\n    try:\r\n        if(int(input)not in set_gpu_numbers):return default_gpu_numbers\r\n    except:return input\r\n    return input\r\ndef fix_gpu_numbers(inputs):\r\n    output=[]\r\n    try:\r\n        for input in inputs.split(\",\"):output.append(str(fix_gpu_number(input)))\r\n        return \",\".join(output)\r\n    except:\r\n        return inputs\r\n\r\npretrained_sovits_name=[\"GPT_SoVITS/pretrained_models/gsv-v2final-pretrained/s2G2333k.pth\", \"GPT_SoVITS/pretrained_models/s2G488k.pth\"]\r\npretrained_gpt_name=[\"GPT_SoVITS/pretrained_models/gsv-v2final-pretrained/s1bert25hz-5kh-longer-epoch=12-step=369668.ckpt\", \"GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt\"]\r\n\r\npretrained_model_list = (pretrained_sovits_name[-int(version[-1])+2],pretrained_sovits_name[-int(version[-1])+2].replace(\"s2G\",\"s2D\"),pretrained_gpt_name[-int(version[-1])+2],\"GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\",\"GPT_SoVITS/pretrained_models/chinese-hubert-base\")\r\n\r\n_=''\r\nfor i in pretrained_model_list:\r\n    if os.path.exists(i):...\r\n    else:_+=f'\\n    {i}'\r\nif _:\r\n    print(\"warning:\",i18n('‰ª•‰∏ãÊ®°Âûã‰∏çÂ≠òÂú®:')+_)\r\n\r\n_ =[[],[]]\r\nfor i in range(2):\r\n    if os.path.exists(pretrained_gpt_name[i]):_[0].append(pretrained_gpt_name[i])\r\n    else:_[0].append(\"\")##Ê≤°Êúâ‰∏ãpretrainedÊ®°ÂûãÁöÑÔºåËØ¥‰∏çÂÆö‰ªñ‰ª¨ÊòØÊÉ≥Ëá™Â∑±‰ªéÈõ∂ËÆ≠Â∫ïÊ®°Âë¢\r\n    if os.path.exists(pretrained_sovits_name[i]):_[-1].append(pretrained_sovits_name[i])\r\n    else:_[-1].append(\"\")\r\npretrained_gpt_name,pretrained_sovits_name = _\r\n\r\nSoVITS_weight_root=[\"SoVITS_weights_v2\",\"SoVITS_weights\"]\r\nGPT_weight_root=[\"GPT_weights_v2\",\"GPT_weights\"]\r\nfor root in SoVITS_weight_root+GPT_weight_root:\r\n    os.makedirs(root,exist_ok=True)\r\ndef get_weights_names():\r\n    SoVITS_names = [name for name in pretrained_sovits_name if name!=\"\"]\r\n    for path in SoVITS_weight_root:\r\n        for name in os.listdir(path):\r\n            if name.endswith(\".pth\"): SoVITS_names.append(\"%s/%s\" % (path, name))\r\n    GPT_names = [name for name in pretrained_gpt_name if name!=\"\"]\r\n    for path in GPT_weight_root:\r\n        for name in os.listdir(path):\r\n            if name.endswith(\".ckpt\"): GPT_names.append(\"%s/%s\" % (path, name))\r\n    return SoVITS_names, GPT_names\r\n\r\nSoVITS_names,GPT_names = get_weights_names()\r\nfor path in SoVITS_weight_root+GPT_weight_root:\r\n    os.makedirs(path,exist_ok=True)\r\n\r\n\r\ndef custom_sort_key(s):\r\n    # ‰ΩøÁî®Ê≠£ÂàôË°®ËææÂºèÊèêÂèñÂ≠óÁ¨¶‰∏≤‰∏≠ÁöÑÊï∞Â≠óÈÉ®ÂàÜÂíåÈùûÊï∞Â≠óÈÉ®ÂàÜ\r\n    parts = re.split('(\\d+)', s)\r\n    # Â∞ÜÊï∞Â≠óÈÉ®ÂàÜËΩ¨Êç¢‰∏∫Êï¥Êï∞ÔºåÈùûÊï∞Â≠óÈÉ®ÂàÜ‰øùÊåÅ‰∏çÂèò\r\n    parts = [int(part) if part.isdigit() else part for part in parts]\r\n    return parts\r\n\r\ndef change_choices():\r\n    SoVITS_names, GPT_names = get_weights_names()\r\n    return {\"choices\": sorted(SoVITS_names,key=custom_sort_key), \"__type__\": \"update\"}, {\"choices\": sorted(GPT_names,key=custom_sort_key), \"__type__\": \"update\"}\r\n\r\np_label=None\r\np_uvr5=None\r\np_asr=None\r\np_denoise=None\r\np_tts_inference=None\r\n\r\ndef kill_proc_tree(pid, including_parent=True):  \r\n    try:\r\n        parent = psutil.Process(pid)\r\n    except psutil.NoSuchProcess:\r\n        # Process already terminated\r\n        return\r\n\r\n    children = parent.children(recursive=True)\r\n    for child in children:\r\n        try:\r\n            os.kill(child.pid, signal.SIGTERM)  # or signal.SIGKILL\r\n        except OSError:\r\n            pass\r\n    if including_parent:\r\n        try:\r\n            os.kill(parent.pid, signal.SIGTERM)  # or signal.SIGKILL\r\n        except OSError:\r\n            pass\r\n\r\nsystem=platform.system()\r\ndef kill_process(pid):\r\n    if(system==\"Windows\"):\r\n        cmd = \"taskkill /t /f /pid %s\" % pid\r\n        os.system(cmd)\r\n    else:\r\n        kill_proc_tree(pid)\r\n    \r\n\r\ndef change_label(path_list):\r\n    global p_label\r\n    if(p_label==None):\r\n        check_for_existance([path_list])\r\n        path_list=my_utils.clean_path(path_list)\r\n        cmd = '\"%s\" tools/subfix_webui.py --load_list \"%s\" --webui_port %s --is_share %s'%(python_exec,path_list,webui_port_subfix,is_share)\r\n        yield i18n(\"ÊâìÊ†áÂ∑•ÂÖ∑WebUIÂ∑≤ÂºÄÂêØ\"), {'__type__':'update','visible':False}, {'__type__':'update','visible':True}\r\n        print(cmd)\r\n        p_label = Popen(cmd, shell=True)\r\n    elif(p_label!=None):\r\n        kill_process(p_label.pid)\r\n        p_label=None\r\n        yield i18n(\"ÊâìÊ†áÂ∑•ÂÖ∑WebUIÂ∑≤ÂÖ≥Èó≠\"), {'__type__':'update','visible':True}, {'__type__':'update','visible':False}\r\n\r\ndef change_uvr5():\r\n    global p_uvr5\r\n    if(p_uvr5==None):\r\n        cmd = '\"%s\" tools/uvr5/webui.py \"%s\" %s %s %s'%(python_exec,infer_device,is_half,webui_port_uvr5,is_share)\r\n        yield i18n(\"UVR5Â∑≤ÂºÄÂêØ\"), {'__type__':'update','visible':False}, {'__type__':'update','visible':True}\r\n        print(cmd)\r\n        p_uvr5 = Popen(cmd, shell=True)\r\n    elif(p_uvr5!=None):\r\n        kill_process(p_uvr5.pid)\r\n        p_uvr5=None\r\n        yield i18n(\"UVR5Â∑≤ÂÖ≥Èó≠\"), {'__type__':'update','visible':True}, {'__type__':'update','visible':False}\r\n\r\ndef change_tts_inference(bert_path,cnhubert_base_path,gpu_number,gpt_path,sovits_path, batched_infer_enabled):\r\n    global p_tts_inference\r\n    if batched_infer_enabled:\r\n        cmd = '\"%s\" GPT_SoVITS/inference_webui_fast.py \"%s\"'%(python_exec, language)\r\n    else:\r\n        cmd = '\"%s\" GPT_SoVITS/inference_webui.py \"%s\"'%(python_exec, language)\r\n    if(p_tts_inference==None):\r\n        os.environ[\"gpt_path\"]=gpt_path if \"/\" in gpt_path else \"%s/%s\"%(GPT_weight_root,gpt_path)\r\n        os.environ[\"sovits_path\"]=sovits_path if \"/\"in sovits_path else \"%s/%s\"%(SoVITS_weight_root,sovits_path)\r\n        os.environ[\"cnhubert_base_path\"]=cnhubert_base_path\r\n        os.environ[\"bert_path\"]=bert_path\r\n        os.environ[\"_CUDA_VISIBLE_DEVICES\"]=fix_gpu_number(gpu_number)\r\n        os.environ[\"is_half\"]=str(is_half)\r\n        os.environ[\"infer_ttswebui\"]=str(webui_port_infer_tts)\r\n        os.environ[\"is_share\"]=str(is_share)\r\n        yield i18n(\"TTSÊé®ÁêÜËøõÁ®ãÂ∑≤ÂºÄÂêØ\"), {'__type__':'update','visible':False}, {'__type__':'update','visible':True}\r\n        print(cmd)\r\n        p_tts_inference = Popen(cmd, shell=True)\r\n    elif(p_tts_inference!=None):\r\n        kill_process(p_tts_inference.pid)\r\n        p_tts_inference=None\r\n        yield i18n(\"TTSÊé®ÁêÜËøõÁ®ãÂ∑≤ÂÖ≥Èó≠\"), {'__type__':'update','visible':True}, {'__type__':'update','visible':False}\r\n\r\nfrom tools.asr.config import asr_dict\r\ndef open_asr(asr_inp_dir, asr_opt_dir, asr_model, asr_model_size, asr_lang, asr_precision):\r\n    global p_asr\r\n    if(p_asr==None):\r\n        asr_inp_dir=my_utils.clean_path(asr_inp_dir)\r\n        asr_opt_dir=my_utils.clean_path(asr_opt_dir)\r\n        check_for_existance([asr_inp_dir])\r\n        cmd = f'\"{python_exec}\" tools/asr/{asr_dict[asr_model][\"path\"]}'\r\n        cmd += f' -i \"{asr_inp_dir}\"'\r\n        cmd += f' -o \"{asr_opt_dir}\"'\r\n        cmd += f' -s {asr_model_size}'\r\n        cmd += f' -l {asr_lang}'\r\n        cmd += f\" -p {asr_precision}\"\r\n        output_file_name = os.path.basename(asr_inp_dir)\r\n        output_folder = asr_opt_dir or \"output/asr_opt\"\r\n        output_file_path = os.path.abspath(f'{output_folder}/{output_file_name}.list')\r\n        yield \"ASR‰ªªÂä°ÂºÄÂêØÔºö%s\"%cmd, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\"}, {\"__type__\":\"update\"}, {\"__type__\":\"update\"}\r\n        print(cmd)\r\n        p_asr = Popen(cmd, shell=True)\r\n        p_asr.wait()\r\n        p_asr=None\r\n        yield f\"ASR‰ªªÂä°ÂÆåÊàê, Êü•ÁúãÁªàÁ´ØËøõË°å‰∏ã‰∏ÄÊ≠•\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"value\":output_file_path}, {\"__type__\":\"update\",\"value\":output_file_path}, {\"__type__\":\"update\",\"value\":asr_inp_dir}\r\n    else:\r\n        yield \"Â∑≤ÊúâÊ≠£Âú®ËøõË°åÁöÑASR‰ªªÂä°ÔºåÈúÄÂÖàÁªàÊ≠¢ÊâçËÉΩÂºÄÂêØ‰∏ã‰∏ÄÊ¨°‰ªªÂä°\", {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\"}, {\"__type__\":\"update\"}, {\"__type__\":\"update\"}\r\n        # return None\r\n\r\ndef close_asr():\r\n    global p_asr\r\n    if(p_asr!=None):\r\n        kill_process(p_asr.pid)\r\n        p_asr=None\r\n    return \"Â∑≤ÁªàÊ≠¢ASRËøõÁ®ã\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\ndef open_denoise(denoise_inp_dir, denoise_opt_dir):\r\n    global p_denoise\r\n    if(p_denoise==None):\r\n        denoise_inp_dir=my_utils.clean_path(denoise_inp_dir)\r\n        denoise_opt_dir=my_utils.clean_path(denoise_opt_dir)\r\n        check_for_existance([denoise_inp_dir])\r\n        cmd = '\"%s\" tools/cmd-denoise.py -i \"%s\" -o \"%s\" -p %s'%(python_exec,denoise_inp_dir,denoise_opt_dir,\"float16\"if is_half==True else \"float32\")\r\n\r\n        yield \"ËØ≠Èü≥ÈôçÂô™‰ªªÂä°ÂºÄÂêØÔºö%s\"%cmd, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\"}, {\"__type__\":\"update\"}\r\n        print(cmd)\r\n        p_denoise = Popen(cmd, shell=True)\r\n        p_denoise.wait()\r\n        p_denoise=None\r\n        yield f\"ËØ≠Èü≥ÈôçÂô™‰ªªÂä°ÂÆåÊàê, Êü•ÁúãÁªàÁ´ØËøõË°å‰∏ã‰∏ÄÊ≠•\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"value\":denoise_opt_dir}, {\"__type__\":\"update\",\"value\":denoise_opt_dir}\r\n    else:\r\n        yield \"Â∑≤ÊúâÊ≠£Âú®ËøõË°åÁöÑËØ≠Èü≥ÈôçÂô™‰ªªÂä°ÔºåÈúÄÂÖàÁªàÊ≠¢ÊâçËÉΩÂºÄÂêØ‰∏ã‰∏ÄÊ¨°‰ªªÂä°\", {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\"}, {\"__type__\":\"update\"}\r\n        # return None\r\n\r\ndef close_denoise():\r\n    global p_denoise\r\n    if(p_denoise!=None):\r\n        kill_process(p_denoise.pid)\r\n        p_denoise=None\r\n    return \"Â∑≤ÁªàÊ≠¢ËØ≠Èü≥ÈôçÂô™ËøõÁ®ã\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n\r\np_train_SoVITS=None\r\ndef open1Ba(batch_size,total_epoch,exp_name,text_low_lr_rate,if_save_latest,if_save_every_weights,save_every_epoch,gpu_numbers1Ba,pretrained_s2G,pretrained_s2D):\r\n    global p_train_SoVITS\r\n    if(p_train_SoVITS==None):\r\n        with open(\"GPT_SoVITS/configs/s2.json\")as f:\r\n            data=f.read()\r\n            data=json.loads(data)\r\n        s2_dir=\"%s/%s\"%(exp_root,exp_name)\r\n        os.makedirs(\"%s/logs_s2\"%(s2_dir),exist_ok=True)\r\n        if check_for_existance([s2_dir],is_train=True):\r\n            check_details([s2_dir],is_train=True)\r\n        if(is_half==False):\r\n            data[\"train\"][\"fp16_run\"]=False\r\n            batch_size=max(1,batch_size//2)\r\n        data[\"train\"][\"batch_size\"]=batch_size\r\n        data[\"train\"][\"epochs\"]=total_epoch\r\n        data[\"train\"][\"text_low_lr_rate\"]=text_low_lr_rate\r\n        data[\"train\"][\"pretrained_s2G\"]=pretrained_s2G\r\n        data[\"train\"][\"pretrained_s2D\"]=pretrained_s2D\r\n        data[\"train\"][\"if_save_latest\"]=if_save_latest\r\n        data[\"train\"][\"if_save_every_weights\"]=if_save_every_weights\r\n        data[\"train\"][\"save_every_epoch\"]=save_every_epoch\r\n        data[\"train\"][\"gpu_numbers\"]=gpu_numbers1Ba\r\n        data[\"model\"][\"version\"]=version\r\n        data[\"data\"][\"exp_dir\"]=data[\"s2_ckpt_dir\"]=s2_dir\r\n        data[\"save_weight_dir\"]=SoVITS_weight_root[-int(version[-1])+2]\r\n        data[\"name\"]=exp_name\r\n        data[\"version\"]=version\r\n        tmp_config_path=\"%s/tmp_s2.json\"%tmp\r\n        with open(tmp_config_path,\"w\")as f:f.write(json.dumps(data))\r\n\r\n        cmd = '\"%s\" GPT_SoVITS/s2_train.py --config \"%s\"'%(python_exec,tmp_config_path)\r\n        yield \"SoVITSËÆ≠ÁªÉÂºÄÂßãÔºö%s\"%cmd, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}\r\n        print(cmd)\r\n        p_train_SoVITS = Popen(cmd, shell=True)\r\n        p_train_SoVITS.wait()\r\n        p_train_SoVITS=None\r\n        yield \"SoVITSËÆ≠ÁªÉÂÆåÊàê\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n    else:\r\n        yield \"Â∑≤ÊúâÊ≠£Âú®ËøõË°åÁöÑSoVITSËÆ≠ÁªÉ‰ªªÂä°ÔºåÈúÄÂÖàÁªàÊ≠¢ÊâçËÉΩÂºÄÂêØ‰∏ã‰∏ÄÊ¨°‰ªªÂä°\", {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}\r\n\r\ndef close1Ba():\r\n    global p_train_SoVITS\r\n    if(p_train_SoVITS!=None):\r\n        kill_process(p_train_SoVITS.pid)\r\n        p_train_SoVITS=None\r\n    return \"Â∑≤ÁªàÊ≠¢SoVITSËÆ≠ÁªÉ\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n\r\np_train_GPT=None\r\ndef open1Bb(batch_size,total_epoch,exp_name,if_dpo,if_save_latest,if_save_every_weights,save_every_epoch,gpu_numbers,pretrained_s1):\r\n    global p_train_GPT\r\n    if(p_train_GPT==None):\r\n        with open(\"GPT_SoVITS/configs/s1longer.yaml\"if version==\"v1\"else \"GPT_SoVITS/configs/s1longer-v2.yaml\")as f:\r\n            data=f.read()\r\n            data=yaml.load(data, Loader=yaml.FullLoader)\r\n        s1_dir=\"%s/%s\"%(exp_root,exp_name)\r\n        os.makedirs(\"%s/logs_s1\"%(s1_dir),exist_ok=True)\r\n        if check_for_existance([s1_dir],is_train=True):\r\n            check_details([s1_dir],is_train=True)\r\n        if(is_half==False):\r\n            data[\"train\"][\"precision\"]=\"32\"\r\n            batch_size = max(1, batch_size // 2)\r\n        data[\"train\"][\"batch_size\"]=batch_size\r\n        data[\"train\"][\"epochs\"]=total_epoch\r\n        data[\"pretrained_s1\"]=pretrained_s1\r\n        data[\"train\"][\"save_every_n_epoch\"]=save_every_epoch\r\n        data[\"train\"][\"if_save_every_weights\"]=if_save_every_weights\r\n        data[\"train\"][\"if_save_latest\"]=if_save_latest\r\n        data[\"train\"][\"if_dpo\"]=if_dpo\r\n        data[\"train\"][\"half_weights_save_dir\"]=GPT_weight_root[-int(version[-1])+2]\r\n        data[\"train\"][\"exp_name\"]=exp_name\r\n        data[\"train_semantic_path\"]=\"%s/6-name2semantic.tsv\"%s1_dir\r\n        data[\"train_phoneme_path\"]=\"%s/2-name2text.txt\"%s1_dir\r\n        data[\"output_dir\"]=\"%s/logs_s1\"%s1_dir\r\n        # data[\"version\"]=version\r\n\r\n        os.environ[\"_CUDA_VISIBLE_DEVICES\"]=fix_gpu_numbers(gpu_numbers.replace(\"-\",\",\"))\r\n        os.environ[\"hz\"]=\"25hz\"\r\n        tmp_config_path=\"%s/tmp_s1.yaml\"%tmp\r\n        with open(tmp_config_path, \"w\") as f:f.write(yaml.dump(data, default_flow_style=False))\r\n        # cmd = '\"%s\" GPT_SoVITS/s1_train.py --config_file \"%s\" --train_semantic_path \"%s/6-name2semantic.tsv\" --train_phoneme_path \"%s/2-name2text.txt\" --output_dir \"%s/logs_s1\"'%(python_exec,tmp_config_path,s1_dir,s1_dir,s1_dir)\r\n        cmd = '\"%s\" GPT_SoVITS/s1_train.py --config_file \"%s\" '%(python_exec,tmp_config_path)\r\n        yield \"GPTËÆ≠ÁªÉÂºÄÂßãÔºö%s\"%cmd, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}\r\n        print(cmd)\r\n        p_train_GPT = Popen(cmd, shell=True)\r\n        p_train_GPT.wait()\r\n        p_train_GPT=None\r\n        yield \"GPTËÆ≠ÁªÉÂÆåÊàê\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n    else:\r\n        yield \"Â∑≤ÊúâÊ≠£Âú®ËøõË°åÁöÑGPTËÆ≠ÁªÉ‰ªªÂä°ÔºåÈúÄÂÖàÁªàÊ≠¢ÊâçËÉΩÂºÄÂêØ‰∏ã‰∏ÄÊ¨°‰ªªÂä°\", {\"__type__\":\"update\",\"visible\":False}, {\"__type__\":\"update\",\"visible\":True}\r\n\r\ndef close1Bb():\r\n    global p_train_GPT\r\n    if(p_train_GPT!=None):\r\n        kill_process(p_train_GPT.pid)\r\n        p_train_GPT=None\r\n    return \"Â∑≤ÁªàÊ≠¢GPTËÆ≠ÁªÉ\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n\r\nps_slice=[]\r\ndef open_slice(inp,opt_root,threshold,min_length,min_interval,hop_size,max_sil_kept,_max,alpha,n_parts):\r\n    global ps_slice\r\n    inp = my_utils.clean_path(inp)\r\n    opt_root = my_utils.clean_path(opt_root)\r\n    check_for_existance([inp])\r\n    if(os.path.exists(inp)==False):\r\n        yield \"ËæìÂÖ•Ë∑ØÂæÑ‰∏çÂ≠òÂú®\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}\r\n        return\r\n    if os.path.isfile(inp):n_parts=1\r\n    elif os.path.isdir(inp):pass\r\n    else:\r\n        yield \"ËæìÂÖ•Ë∑ØÂæÑÂ≠òÂú®‰ΩÜÊó¢‰∏çÊòØÊñá‰ª∂‰πü‰∏çÊòØÊñá‰ª∂Â§π\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}\r\n        return\r\n    if (ps_slice == []):\r\n        for i_part in range(n_parts):\r\n            cmd = '\"%s\" tools/slice_audio.py \"%s\" \"%s\" %s %s %s %s %s %s %s %s %s''' % (python_exec,inp, opt_root, threshold, min_length, min_interval, hop_size, max_sil_kept, _max, alpha, i_part, n_parts)\r\n            print(cmd)\r\n            p = Popen(cmd, shell=True)\r\n            ps_slice.append(p)\r\n        yield \"ÂàáÂâ≤ÊâßË°å‰∏≠\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}\r\n        for p in ps_slice:\r\n            p.wait()\r\n        ps_slice=[]\r\n        yield \"ÂàáÂâ≤ÁªìÊùü\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}, {\"__type__\": \"update\", \"value\":opt_root}, {\"__type__\": \"update\", \"value\":opt_root}, {\"__type__\": \"update\", \"value\":opt_root}\r\n    else:\r\n        yield \"Â∑≤ÊúâÊ≠£Âú®ËøõË°åÁöÑÂàáÂâ≤‰ªªÂä°ÔºåÈúÄÂÖàÁªàÊ≠¢ÊâçËÉΩÂºÄÂêØ‰∏ã‰∏ÄÊ¨°‰ªªÂä°\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}, {\"__type__\": \"update\"}\r\n\r\ndef close_slice():\r\n    global ps_slice\r\n    if (ps_slice != []):\r\n        for p_slice in ps_slice:\r\n            try:\r\n                kill_process(p_slice.pid)\r\n            except:\r\n                traceback.print_exc()\r\n        ps_slice=[]\r\n    return \"Â∑≤ÁªàÊ≠¢ÊâÄÊúâÂàáÂâ≤ËøõÁ®ã\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n\r\nps1a=[]\r\ndef open1a(inp_text,inp_wav_dir,exp_name,gpu_numbers,bert_pretrained_dir):\r\n    global ps1a\r\n    inp_text = my_utils.clean_path(inp_text)\r\n    inp_wav_dir = my_utils.clean_path(inp_wav_dir)\r\n    if check_for_existance([inp_text,inp_wav_dir], is_dataset_processing=True):\r\n        check_details([inp_text,inp_wav_dir], is_dataset_processing=True)\r\n    if (ps1a == []):\r\n        opt_dir=\"%s/%s\"%(exp_root,exp_name)\r\n        config={\r\n            \"inp_text\":inp_text,\r\n            \"inp_wav_dir\":inp_wav_dir,\r\n            \"exp_name\":exp_name,\r\n            \"opt_dir\":opt_dir,\r\n            \"bert_pretrained_dir\":bert_pretrained_dir,\r\n        }\r\n        gpu_names=gpu_numbers.split(\"-\")\r\n        all_parts=len(gpu_names)\r\n        for i_part in range(all_parts):\r\n            config.update(\r\n                {\r\n                    \"i_part\": str(i_part),\r\n                    \"all_parts\": str(all_parts),\r\n                    \"_CUDA_VISIBLE_DEVICES\": fix_gpu_number(gpu_names[i_part]),\r\n                    \"is_half\": str(is_half)\r\n                }\r\n            )\r\n            os.environ.update(config)\r\n            cmd = '\"%s\" GPT_SoVITS/prepare_datasets/1-get-text.py'%python_exec\r\n            print(cmd)\r\n            p = Popen(cmd, shell=True)\r\n            ps1a.append(p)\r\n        yield \"ÊñáÊú¨ËøõÁ®ãÊâßË°å‰∏≠\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n        for p in ps1a:\r\n            p.wait()\r\n        opt = []\r\n        for i_part in range(all_parts):\r\n            txt_path = \"%s/2-name2text-%s.txt\" % (opt_dir, i_part)\r\n            with open(txt_path, \"r\", encoding=\"utf8\") as f:\r\n                opt += f.read().strip(\"\\n\").split(\"\\n\")\r\n            os.remove(txt_path)\r\n        path_text = \"%s/2-name2text.txt\" % opt_dir\r\n        with open(path_text, \"w\", encoding=\"utf8\") as f:\r\n            f.write(\"\\n\".join(opt) + \"\\n\")\r\n        ps1a=[]\r\n        if len(\"\".join(opt)) > 0:\r\n            yield \"ÊñáÊú¨ËøõÁ®ãÊàêÂäü\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n        else:\r\n            yield \"ÊñáÊú¨ËøõÁ®ãÂ§±Ë¥•\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n    else:\r\n        yield \"Â∑≤ÊúâÊ≠£Âú®ËøõË°åÁöÑÊñáÊú¨‰ªªÂä°ÔºåÈúÄÂÖàÁªàÊ≠¢ÊâçËÉΩÂºÄÂêØ‰∏ã‰∏ÄÊ¨°‰ªªÂä°\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n\r\ndef close1a():\r\n    global ps1a\r\n    if (ps1a != []):\r\n        for p1a in ps1a:\r\n            try:\r\n                kill_process(p1a.pid)\r\n            except:\r\n                traceback.print_exc()\r\n        ps1a=[]\r\n    return \"Â∑≤ÁªàÊ≠¢ÊâÄÊúâ1aËøõÁ®ã\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n\r\nps1b=[]\r\ndef open1b(inp_text,inp_wav_dir,exp_name,gpu_numbers,ssl_pretrained_dir):\r\n    global ps1b\r\n    inp_text = my_utils.clean_path(inp_text)\r\n    inp_wav_dir = my_utils.clean_path(inp_wav_dir)\r\n    if check_for_existance([inp_text,inp_wav_dir], is_dataset_processing=True):\r\n        check_details([inp_text,inp_wav_dir], is_dataset_processing=True)\r\n    if (ps1b == []):\r\n        config={\r\n            \"inp_text\":inp_text,\r\n            \"inp_wav_dir\":inp_wav_dir,\r\n            \"exp_name\":exp_name,\r\n            \"opt_dir\":\"%s/%s\"%(exp_root,exp_name),\r\n            \"cnhubert_base_dir\":ssl_pretrained_dir,\r\n            \"is_half\": str(is_half)\r\n        }\r\n        gpu_names=gpu_numbers.split(\"-\")\r\n        all_parts=len(gpu_names)\r\n        for i_part in range(all_parts):\r\n            config.update(\r\n                {\r\n                    \"i_part\": str(i_part),\r\n                    \"all_parts\": str(all_parts),\r\n                    \"_CUDA_VISIBLE_DEVICES\": fix_gpu_number(gpu_names[i_part]),\r\n                }\r\n            )\r\n            os.environ.update(config)\r\n            cmd = '\"%s\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py'%python_exec\r\n            print(cmd)\r\n            p = Popen(cmd, shell=True)\r\n            ps1b.append(p)\r\n        yield \"SSLÊèêÂèñËøõÁ®ãÊâßË°å‰∏≠\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n        for p in ps1b:\r\n            p.wait()\r\n        ps1b=[]\r\n        yield \"SSLÊèêÂèñËøõÁ®ãÁªìÊùü\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n    else:\r\n        yield \"Â∑≤ÊúâÊ≠£Âú®ËøõË°åÁöÑSSLÊèêÂèñ‰ªªÂä°ÔºåÈúÄÂÖàÁªàÊ≠¢ÊâçËÉΩÂºÄÂêØ‰∏ã‰∏ÄÊ¨°‰ªªÂä°\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n\r\ndef close1b():\r\n    global ps1b\r\n    if (ps1b != []):\r\n        for p1b in ps1b:\r\n            try:\r\n                kill_process(p1b.pid)\r\n            except:\r\n                traceback.print_exc()\r\n        ps1b=[]\r\n    return \"Â∑≤ÁªàÊ≠¢ÊâÄÊúâ1bËøõÁ®ã\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n\r\nps1c=[]\r\ndef open1c(inp_text,exp_name,gpu_numbers,pretrained_s2G_path):\r\n    global ps1c\r\n    inp_text = my_utils.clean_path(inp_text)\r\n    if check_for_existance([inp_text,''], is_dataset_processing=True):\r\n        check_details([inp_text,''], is_dataset_processing=True)\r\n    if (ps1c == []):\r\n        opt_dir=\"%s/%s\"%(exp_root,exp_name)\r\n        config={\r\n            \"inp_text\":inp_text,\r\n            \"exp_name\":exp_name,\r\n            \"opt_dir\":opt_dir,\r\n            \"pretrained_s2G\":pretrained_s2G_path,\r\n            \"s2config_path\":\"GPT_SoVITS/configs/s2.json\",\r\n            \"is_half\": str(is_half)\r\n        }\r\n        gpu_names=gpu_numbers.split(\"-\")\r\n        all_parts=len(gpu_names)\r\n        for i_part in range(all_parts):\r\n            config.update(\r\n                {\r\n                    \"i_part\": str(i_part),\r\n                    \"all_parts\": str(all_parts),\r\n                    \"_CUDA_VISIBLE_DEVICES\": fix_gpu_number(gpu_names[i_part]),\r\n                }\r\n            )\r\n            os.environ.update(config)\r\n            cmd = '\"%s\" GPT_SoVITS/prepare_datasets/3-get-semantic.py'%python_exec\r\n            print(cmd)\r\n            p = Popen(cmd, shell=True)\r\n            ps1c.append(p)\r\n        yield \"ËØ≠‰πâtokenÊèêÂèñËøõÁ®ãÊâßË°å‰∏≠\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n        for p in ps1c:\r\n            p.wait()\r\n        opt = [\"item_name\\tsemantic_audio\"]\r\n        path_semantic = \"%s/6-name2semantic.tsv\" % opt_dir\r\n        for i_part in range(all_parts):\r\n            semantic_path = \"%s/6-name2semantic-%s.tsv\" % (opt_dir, i_part)\r\n            with open(semantic_path, \"r\", encoding=\"utf8\") as f:\r\n                opt += f.read().strip(\"\\n\").split(\"\\n\")\r\n            os.remove(semantic_path)\r\n        with open(path_semantic, \"w\", encoding=\"utf8\") as f:\r\n            f.write(\"\\n\".join(opt) + \"\\n\")\r\n        ps1c=[]\r\n        yield \"ËØ≠‰πâtokenÊèêÂèñËøõÁ®ãÁªìÊùü\", {\"__type__\":\"update\",\"visible\":True}, {\"__type__\":\"update\",\"visible\":False}\r\n    else:\r\n        yield \"Â∑≤ÊúâÊ≠£Âú®ËøõË°åÁöÑËØ≠‰πâtokenÊèêÂèñ‰ªªÂä°ÔºåÈúÄÂÖàÁªàÊ≠¢ÊâçËÉΩÂºÄÂêØ‰∏ã‰∏ÄÊ¨°‰ªªÂä°\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n\r\ndef close1c():\r\n    global ps1c\r\n    if (ps1c != []):\r\n        for p1c in ps1c:\r\n            try:\r\n                kill_process(p1c.pid)\r\n            except:\r\n                traceback.print_exc()\r\n        ps1c=[]\r\n    return \"Â∑≤ÁªàÊ≠¢ÊâÄÊúâËØ≠‰πâtokenËøõÁ®ã\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n#####inp_text,inp_wav_dir,exp_name,gpu_numbers1a,gpu_numbers1Ba,gpu_numbers1c,bert_pretrained_dir,cnhubert_base_dir,pretrained_s2G\r\nps1abc=[]\r\ndef open1abc(inp_text,inp_wav_dir,exp_name,gpu_numbers1a,gpu_numbers1Ba,gpu_numbers1c,bert_pretrained_dir,ssl_pretrained_dir,pretrained_s2G_path):\r\n    global ps1abc\r\n    inp_text = my_utils.clean_path(inp_text)\r\n    inp_wav_dir = my_utils.clean_path(inp_wav_dir)\r\n    if check_for_existance([inp_text,inp_wav_dir], is_dataset_processing=True):\r\n        check_details([inp_text,inp_wav_dir], is_dataset_processing=True)\r\n    if (ps1abc == []):\r\n        opt_dir=\"%s/%s\"%(exp_root,exp_name)\r\n        try:\r\n            #############################1a\r\n            path_text=\"%s/2-name2text.txt\" % opt_dir\r\n            if(os.path.exists(path_text)==False or (os.path.exists(path_text)==True and len(open(path_text,\"r\",encoding=\"utf8\").read().strip(\"\\n\").split(\"\\n\"))<2)):\r\n                config={\r\n                    \"inp_text\":inp_text,\r\n                    \"inp_wav_dir\":inp_wav_dir,\r\n                    \"exp_name\":exp_name,\r\n                    \"opt_dir\":opt_dir,\r\n                    \"bert_pretrained_dir\":bert_pretrained_dir,\r\n                    \"is_half\": str(is_half)\r\n                }\r\n                gpu_names=gpu_numbers1a.split(\"-\")\r\n                all_parts=len(gpu_names)\r\n                for i_part in range(all_parts):\r\n                    config.update(\r\n                        {\r\n                            \"i_part\": str(i_part),\r\n                            \"all_parts\": str(all_parts),\r\n                            \"_CUDA_VISIBLE_DEVICES\": fix_gpu_number(gpu_names[i_part]),\r\n                        }\r\n                    )\r\n                    os.environ.update(config)\r\n                    cmd = '\"%s\" GPT_SoVITS/prepare_datasets/1-get-text.py'%python_exec\r\n                    print(cmd)\r\n                    p = Popen(cmd, shell=True)\r\n                    ps1abc.append(p)\r\n                yield \"ËøõÂ∫¶Ôºö1a-ing\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n                for p in ps1abc:p.wait()\r\n\r\n                opt = []\r\n                for i_part in range(all_parts):#txt_path=\"%s/2-name2text-%s.txt\"%(opt_dir,i_part)\r\n                    txt_path = \"%s/2-name2text-%s.txt\" % (opt_dir, i_part)\r\n                    with open(txt_path, \"r\",encoding=\"utf8\") as f:\r\n                        opt += f.read().strip(\"\\n\").split(\"\\n\")\r\n                    os.remove(txt_path)\r\n                with open(path_text, \"w\",encoding=\"utf8\") as f:\r\n                    f.write(\"\\n\".join(opt) + \"\\n\")\r\n                assert len(\"\".join(opt)) > 0, \"1Aa-ÊñáÊú¨Ëé∑ÂèñËøõÁ®ãÂ§±Ë¥•\"\r\n            yield \"ËøõÂ∫¶Ôºö1a-done\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n            ps1abc=[]\r\n            #############################1b\r\n            config={\r\n                \"inp_text\":inp_text,\r\n                \"inp_wav_dir\":inp_wav_dir,\r\n                \"exp_name\":exp_name,\r\n                \"opt_dir\":opt_dir,\r\n                \"cnhubert_base_dir\":ssl_pretrained_dir,\r\n            }\r\n            gpu_names=gpu_numbers1Ba.split(\"-\")\r\n            all_parts=len(gpu_names)\r\n            for i_part in range(all_parts):\r\n                config.update(\r\n                    {\r\n                        \"i_part\": str(i_part),\r\n                        \"all_parts\": str(all_parts),\r\n                        \"_CUDA_VISIBLE_DEVICES\": fix_gpu_number(gpu_names[i_part]),\r\n                    }\r\n                )\r\n                os.environ.update(config)\r\n                cmd = '\"%s\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py'%python_exec\r\n                print(cmd)\r\n                p = Popen(cmd, shell=True)\r\n                ps1abc.append(p)\r\n            yield \"ËøõÂ∫¶Ôºö1a-done, 1b-ing\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n            for p in ps1abc:p.wait()\r\n            yield \"ËøõÂ∫¶Ôºö1a1b-done\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n            ps1abc=[]\r\n            #############################1c\r\n            path_semantic = \"%s/6-name2semantic.tsv\" % opt_dir\r\n            if(os.path.exists(path_semantic)==False or (os.path.exists(path_semantic)==True and os.path.getsize(path_semantic)<31)):\r\n                config={\r\n                    \"inp_text\":inp_text,\r\n                    \"exp_name\":exp_name,\r\n                    \"opt_dir\":opt_dir,\r\n                    \"pretrained_s2G\":pretrained_s2G_path,\r\n                    \"s2config_path\":\"GPT_SoVITS/configs/s2.json\",\r\n                }\r\n                gpu_names=gpu_numbers1c.split(\"-\")\r\n                all_parts=len(gpu_names)\r\n                for i_part in range(all_parts):\r\n                    config.update(\r\n                        {\r\n                            \"i_part\": str(i_part),\r\n                            \"all_parts\": str(all_parts),\r\n                            \"_CUDA_VISIBLE_DEVICES\": fix_gpu_number(gpu_names[i_part]),\r\n                        }\r\n                    )\r\n                    os.environ.update(config)\r\n                    cmd = '\"%s\" GPT_SoVITS/prepare_datasets/3-get-semantic.py'%python_exec\r\n                    print(cmd)\r\n                    p = Popen(cmd, shell=True)\r\n                    ps1abc.append(p)\r\n                yield \"ËøõÂ∫¶Ôºö1a1b-done, 1cing\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n                for p in ps1abc:p.wait()\r\n\r\n                opt = [\"item_name\\tsemantic_audio\"]\r\n                for i_part in range(all_parts):\r\n                    semantic_path = \"%s/6-name2semantic-%s.tsv\" % (opt_dir, i_part)\r\n                    with open(semantic_path, \"r\",encoding=\"utf8\") as f:\r\n                        opt += f.read().strip(\"\\n\").split(\"\\n\")\r\n                    os.remove(semantic_path)\r\n                with open(path_semantic, \"w\",encoding=\"utf8\") as f:\r\n                    f.write(\"\\n\".join(opt) + \"\\n\")\r\n                yield \"ËøõÂ∫¶Ôºöall-done\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n            ps1abc = []\r\n            yield \"‰∏ÄÈîÆ‰∏âËøûËøõÁ®ãÁªìÊùü\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n        except:\r\n            traceback.print_exc()\r\n            close1abc()\r\n            yield \"‰∏ÄÈîÆ‰∏âËøû‰∏≠ÈÄîÊä•Èîô\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n    else:\r\n        yield \"Â∑≤ÊúâÊ≠£Âú®ËøõË°åÁöÑ‰∏ÄÈîÆ‰∏âËøû‰ªªÂä°ÔºåÈúÄÂÖàÁªàÊ≠¢ÊâçËÉΩÂºÄÂêØ‰∏ã‰∏ÄÊ¨°‰ªªÂä°\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\r\n\r\ndef close1abc():\r\n    global ps1abc\r\n    if (ps1abc != []):\r\n        for p1abc in ps1abc:\r\n            try:\r\n                kill_process(p1abc.pid)\r\n            except:\r\n                traceback.print_exc()\r\n        ps1abc=[]\r\n    return \"Â∑≤ÁªàÊ≠¢ÊâÄÊúâ‰∏ÄÈîÆ‰∏âËøûËøõÁ®ã\", {\"__type__\": \"update\", \"visible\": True}, {\"__type__\": \"update\", \"visible\": False}\r\n\r\ndef switch_version(version_):\r\n    os.environ['version']=version_\r\n    global version\r\n    version = version_\r\n    if pretrained_sovits_name[-int(version[-1])+2] !='' and pretrained_gpt_name[-int(version[-1])+2] !='':...\r\n    else:   \r\n        gr.Warning(i18n(f'Êú™‰∏ãËΩΩ{version.upper()}Ê®°Âûã'))\r\n    return  {'__type__':'update', 'value':pretrained_sovits_name[-int(version[-1])+2]}, {'__type__':'update', 'value':pretrained_sovits_name[-int(version[-1])+2].replace(\"s2G\",\"s2D\")}, {'__type__':'update', 'value':pretrained_gpt_name[-int(version[-1])+2]}, {'__type__':'update', 'value':pretrained_gpt_name[-int(version[-1])+2]}, {'__type__':'update', 'value':pretrained_sovits_name[-int(version[-1])+2]}\r\n\r\nif os.path.exists('GPT_SoVITS/text/G2PWModel'):...\r\nelse:\r\n    cmd = '\"%s\" GPT_SoVITS/download.py'%python_exec\r\n    p = Popen(cmd, shell=True)\r\n    p.wait()\r\n\r\ndef sync(text):\r\n    return {'__type__':'update','value':text}\r\nwith gr.Blocks(title=\"GPT-SoVITS WebUI\") as app:\r\n    gr.Markdown(\r\n        value=\r\n            i18n(\"Êú¨ËΩØ‰ª∂‰ª•MITÂçèËÆÆÂºÄÊ∫ê, ‰ΩúËÄÖ‰∏çÂØπËΩØ‰ª∂ÂÖ∑Â§á‰ªª‰ΩïÊéßÂà∂Âäõ, ‰ΩøÁî®ËΩØ‰ª∂ËÄÖ„ÄÅ‰º†Êí≠ËΩØ‰ª∂ÂØºÂá∫ÁöÑÂ£∞Èü≥ËÄÖËá™Ë¥üÂÖ®Ë¥£. <br>Â¶Ç‰∏çËÆ§ÂèØËØ•Êù°Ê¨æ, Âàô‰∏çËÉΩ‰ΩøÁî®ÊàñÂºïÁî®ËΩØ‰ª∂ÂåÖÂÜÖ‰ªª‰Ωï‰ª£Á†ÅÂíåÊñá‰ª∂. ËØ¶ËßÅÊ†πÁõÆÂΩï<b>LICENSE</b>.\")\r\n    )\r\n    gr.Markdown(\r\n        value=\r\n            i18n(\"‰∏≠ÊñáÊïôÁ®ãÊñáÊ°£Ôºöhttps://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e\")\r\n    )\r\n\r\n    with gr.Tabs():\r\n        with gr.TabItem(i18n(\"0-ÂâçÁΩÆÊï∞ÊçÆÈõÜËé∑ÂèñÂ∑•ÂÖ∑\")):#ÊèêÂâçÈöèÊú∫ÂàáÁâáÈò≤Ê≠¢uvr5ÁàÜÂÜÖÂ≠ò->uvr5->slicer->asr->ÊâìÊ†á\r\n            gr.Markdown(value=i18n(\"0a-UVR5‰∫∫Â£∞‰º¥Â•èÂàÜÁ¶ª&ÂéªÊ∑∑ÂìçÂéªÂª∂ËøüÂ∑•ÂÖ∑\"))\r\n            with gr.Row():\r\n                with gr.Column(scale=3):\r\n                    with gr.Row():\r\n                        uvr5_info = gr.Textbox(label=i18n(\"UVR5ËøõÁ®ãËæìÂá∫‰ø°ÊÅØ\"))\r\n                open_uvr5 = gr.Button(value=i18n(\"ÂºÄÂêØUVR5-WebUI\"),variant=\"primary\",visible=True)\r\n                close_uvr5 = gr.Button(value=i18n(\"ÂÖ≥Èó≠UVR5-WebUI\"),variant=\"primary\",visible=False)\r\n            gr.Markdown(value=i18n(\"0b-ËØ≠Èü≥ÂàáÂàÜÂ∑•ÂÖ∑\"))\r\n            with gr.Row():\r\n                with gr.Column(scale=3):\r\n                    with gr.Row():\r\n                        slice_inp_path=gr.Textbox(label=i18n(\"Èü≥È¢ëËá™Âä®ÂàáÂàÜËæìÂÖ•Ë∑ØÂæÑÔºåÂèØÊñá‰ª∂ÂèØÊñá‰ª∂Â§π\"),value=\"\")\r\n                        slice_opt_root=gr.Textbox(label=i18n(\"ÂàáÂàÜÂêéÁöÑÂ≠êÈü≥È¢ëÁöÑËæìÂá∫Ê†πÁõÆÂΩï\"),value=\"output/slicer_opt\")\r\n                    with gr.Row():\r\n                        threshold=gr.Textbox(label=i18n(\"threshold:Èü≥ÈáèÂ∞è‰∫éËøô‰∏™ÂÄºËßÜ‰ΩúÈùôÈü≥ÁöÑÂ§áÈÄâÂàáÂâ≤ÁÇπ\"),value=\"-34\")\r\n                        min_length=gr.Textbox(label=i18n(\"min_length:ÊØèÊÆµÊúÄÂ∞èÂ§öÈïøÔºåÂ¶ÇÊûúÁ¨¨‰∏ÄÊÆµÂ§™Áü≠‰∏ÄÁõ¥ÂíåÂêéÈù¢ÊÆµËøûËµ∑Êù•Áõ¥Âà∞Ë∂ÖËøáËøô‰∏™ÂÄº\"),value=\"4000\")\r\n                        min_interval=gr.Textbox(label=i18n(\"min_interval:ÊúÄÁü≠ÂàáÂâ≤Èó¥Èöî\"),value=\"300\")\r\n                        hop_size=gr.Textbox(label=i18n(\"hop_size:ÊÄé‰πàÁÆóÈü≥ÈáèÊõ≤Á∫øÔºåË∂äÂ∞èÁ≤æÂ∫¶Ë∂äÂ§ßËÆ°ÁÆóÈáèË∂äÈ´òÔºà‰∏çÊòØÁ≤æÂ∫¶Ë∂äÂ§ßÊïàÊûúË∂äÂ•ΩÔºâ\"),value=\"10\")\r\n                        max_sil_kept=gr.Textbox(label=i18n(\"max_sil_kept:ÂàáÂÆåÂêéÈùôÈü≥ÊúÄÂ§öÁïôÂ§öÈïø\"),value=\"500\")\r\n                    with gr.Row():\r\n                        _max=gr.Slider(minimum=0,maximum=1,step=0.05,label=i18n(\"max:ÂΩí‰∏ÄÂåñÂêéÊúÄÂ§ßÂÄºÂ§öÂ∞ë\"),value=0.9,interactive=True)\r\n                        alpha=gr.Slider(minimum=0,maximum=1,step=0.05,label=i18n(\"alpha_mix:Ê∑∑Â§öÂ∞ëÊØî‰æãÂΩí‰∏ÄÂåñÂêéÈü≥È¢ëËøõÊù•\"),value=0.25,interactive=True)   \r\n                    with gr.Row():\r\n                        n_process=gr.Slider(minimum=1,maximum=n_cpu,step=1,label=i18n(\"ÂàáÂâ≤‰ΩøÁî®ÁöÑËøõÁ®ãÊï∞\"),value=4,interactive=True)\r\n                        slicer_info = gr.Textbox(label=i18n(\"ËØ≠Èü≥ÂàáÂâ≤ËøõÁ®ãËæìÂá∫‰ø°ÊÅØ\"))\r\n                open_slicer_button=gr.Button(i18n(\"ÂºÄÂêØËØ≠Èü≥ÂàáÂâ≤\"), variant=\"primary\",visible=True)\r\n                close_slicer_button=gr.Button(i18n(\"ÁªàÊ≠¢ËØ≠Èü≥ÂàáÂâ≤\"), variant=\"primary\",visible=False)\r\n            gr.Markdown(value=i18n(\"0bb-ËØ≠Èü≥ÈôçÂô™Â∑•ÂÖ∑\"))\r\n            with gr.Row():\r\n                with gr.Column(scale=3):\r\n                    with gr.Row():\r\n                        denoise_input_dir=gr.Textbox(label=i18n(\"ÈôçÂô™Èü≥È¢ëÊñá‰ª∂ËæìÂÖ•Êñá‰ª∂Â§π\"),value=\"\")\r\n                        denoise_output_dir=gr.Textbox(label=i18n(\"ÈôçÂô™ÁªìÊûúËæìÂá∫Êñá‰ª∂Â§π\"),value=\"output/denoise_opt\")\r\n                    with gr.Row():\r\n                        denoise_info = gr.Textbox(label=i18n(\"ËØ≠Èü≥ÈôçÂô™ËøõÁ®ãËæìÂá∫‰ø°ÊÅØ\"))\r\n                open_denoise_button = gr.Button(i18n(\"ÂºÄÂêØËØ≠Èü≥ÈôçÂô™\"), variant=\"primary\",visible=True)\r\n                close_denoise_button = gr.Button(i18n(\"ÁªàÊ≠¢ËØ≠Èü≥ÈôçÂô™ËøõÁ®ã\"), variant=\"primary\",visible=False)\r\n            gr.Markdown(value=i18n(\"0c-‰∏≠ÊñáÊâπÈáèÁ¶ªÁ∫øASRÂ∑•ÂÖ∑\"))\r\n            with gr.Row():\r\n                with gr.Column(scale=3):\r\n                    with gr.Row():\r\n                        asr_inp_dir = gr.Textbox(\r\n                            label=i18n(\"ËæìÂÖ•Êñá‰ª∂Â§πË∑ØÂæÑ\"),\r\n                            value=\"D:\\\\GPT-SoVITS\\\\raw\\\\xxx\",\r\n                            interactive=True,\r\n                        )\r\n                        asr_opt_dir = gr.Textbox(\r\n                            label       = i18n(\"ËæìÂá∫Êñá‰ª∂Â§πË∑ØÂæÑ\"),\r\n                            value       = \"output/asr_opt\",\r\n                            interactive = True,\r\n                        )\r\n                    with gr.Row():\r\n                        asr_model = gr.Dropdown(\r\n                            label       = i18n(\"ASR Ê®°Âûã\"),\r\n                            choices     = list(asr_dict.keys()),\r\n                            interactive = True,\r\n                            value=\"ËææÊë© ASR (‰∏≠Êñá)\"\r\n                        )\r\n                        asr_size = gr.Dropdown(\r\n                            label       = i18n(\"ASR Ê®°ÂûãÂ∞∫ÂØ∏\"),\r\n                            choices     = [\"large\"],\r\n                            interactive = True,\r\n                            value=\"large\"\r\n                        )\r\n                        asr_lang = gr.Dropdown(\r\n                            label       = i18n(\"ASR ËØ≠Ë®ÄËÆæÁΩÆ\"),\r\n                            choices     = [\"zh\",\"yue\"],\r\n                            interactive = True,\r\n                            value=\"zh\"\r\n                        )\r\n                        asr_precision = gr.Dropdown(\r\n                            label       = i18n(\"Êï∞ÊçÆÁ±ªÂûãÁ≤æÂ∫¶\"),\r\n                            choices     = [\"float32\"],\r\n                            interactive = True,\r\n                            value=\"float32\"\r\n                        ) \r\n                    with gr.Row():\r\n                        asr_info = gr.Textbox(label=i18n(\"ASRËøõÁ®ãËæìÂá∫‰ø°ÊÅØ\"))     \r\n                open_asr_button = gr.Button(i18n(\"ÂºÄÂêØÁ¶ªÁ∫øÊâπÈáèASR\"), variant=\"primary\",visible=True)\r\n                close_asr_button = gr.Button(i18n(\"ÁªàÊ≠¢ASRËøõÁ®ã\"), variant=\"primary\",visible=False)                  \r\n\r\n                def change_lang_choices(key): #Ê†πÊçÆÈÄâÊã©ÁöÑÊ®°Âûã‰øÆÊîπÂèØÈÄâÁöÑËØ≠Ë®Ä\r\n                    # return gr.Dropdown(choices=asr_dict[key]['lang'])\r\n                    return {\"__type__\": \"update\", \"choices\": asr_dict[key]['lang'],\"value\":asr_dict[key]['lang'][0]}\r\n                def change_size_choices(key): # Ê†πÊçÆÈÄâÊã©ÁöÑÊ®°Âûã‰øÆÊîπÂèØÈÄâÁöÑÊ®°ÂûãÂ∞∫ÂØ∏\r\n                    # return gr.Dropdown(choices=asr_dict[key]['size'])\r\n                    return {\"__type__\": \"update\", \"choices\": asr_dict[key]['size'],\"value\":asr_dict[key]['size'][-1]}\r\n                def change_precision_choices(key): #Ê†πÊçÆÈÄâÊã©ÁöÑÊ®°Âûã‰øÆÊîπÂèØÈÄâÁöÑËØ≠Ë®Ä\r\n                    if key ==\"Faster Whisper (Â§öËØ≠Áßç)\":\r\n                        if default_batch_size <= 4:\r\n                            precision = 'int8'\r\n                        elif is_half:\r\n                            precision = 'float16'\r\n                        else:\r\n                            precision = 'float32'\r\n                    else:\r\n                        precision = 'float32'\r\n                    # return gr.Dropdown(choices=asr_dict[key]['precision'])\r\n                    return {\"__type__\": \"update\", \"choices\": asr_dict[key]['precision'],\"value\":precision}\r\n                asr_model.change(change_lang_choices, [asr_model], [asr_lang])\r\n                asr_model.change(change_size_choices, [asr_model], [asr_size])\r\n                asr_model.change(change_precision_choices, [asr_model], [asr_precision])\r\n\r\n                \r\n            gr.Markdown(value=i18n(\"0d-ËØ≠Èü≥ÊñáÊú¨Ê†°ÂØπÊ†áÊ≥®Â∑•ÂÖ∑\"))\r\n            with gr.Row():\r\n                with gr.Column(scale=3):\r\n                    with gr.Row():\r\n                        path_list = gr.Textbox(\r\n                        label=i18n(\".listÊ†áÊ≥®Êñá‰ª∂ÁöÑË∑ØÂæÑ\"),\r\n                        value=\"D:\\\\RVC1006\\\\GPT-SoVITS\\\\raw\\\\xxx.list\",\r\n                        interactive=True,\r\n                    )\r\n                        label_info = gr.Textbox(label=i18n(\"ÊâìÊ†áÂ∑•ÂÖ∑ËøõÁ®ãËæìÂá∫‰ø°ÊÅØ\"))\r\n                \r\n                open_label = gr.Button(value=i18n(\"ÂºÄÂêØÊâìÊ†áWebUI\"),variant=\"primary\",visible=True)\r\n                close_label = gr.Button(value=i18n(\"ÂÖ≥Èó≠ÊâìÊ†áWebUI\"),variant=\"primary\",visible=False)\r\n            open_label.click(change_label, [path_list], [label_info,open_label,close_label])\r\n            close_label.click(change_label, [path_list], [label_info,open_label,close_label])\r\n            open_uvr5.click(change_uvr5, [], [uvr5_info,open_uvr5,close_uvr5])\r\n            close_uvr5.click(change_uvr5, [], [uvr5_info,open_uvr5,close_uvr5])\r\n\r\n        with gr.TabItem(i18n(\"1-GPT-SoVITS-TTS\")):\r\n            with gr.Row():\r\n                with gr.Row():\r\n                    exp_name = gr.Textbox(label=i18n(\"*ÂÆûÈ™å/Ê®°ÂûãÂêç\"), value=\"xxx\", interactive=True)\r\n                    gpu_info = gr.Textbox(label=i18n(\"ÊòæÂç°‰ø°ÊÅØ\"), value=gpu_info, visible=True, interactive=False)\r\n                    version_checkbox = gr.Radio(label=i18n(\"ÁâàÊú¨\"),value=version,choices=['v1','v2'])\r\n                with gr.Row():\r\n                    pretrained_s2G = gr.Textbox(label=i18n(\"È¢ÑËÆ≠ÁªÉÁöÑSoVITS-GÊ®°ÂûãË∑ØÂæÑ\"), value=pretrained_sovits_name[-int(version[-1])+2], interactive=True, lines=2, max_lines=3,scale=9)\r\n                    pretrained_s2D = gr.Textbox(label=i18n(\"È¢ÑËÆ≠ÁªÉÁöÑSoVITS-DÊ®°ÂûãË∑ØÂæÑ\"), value=pretrained_sovits_name[-int(version[-1])+2].replace(\"s2G\",\"s2D\"), interactive=True, lines=2, max_lines=3,scale=9)\r\n                    pretrained_s1 = gr.Textbox(label=i18n(\"È¢ÑËÆ≠ÁªÉÁöÑGPTÊ®°ÂûãË∑ØÂæÑ\"), value=pretrained_gpt_name[-int(version[-1])+2], interactive=True, lines=2, max_lines=3,scale=10)\r\n            with gr.TabItem(i18n(\"1A-ËÆ≠ÁªÉÈõÜÊ†ºÂºèÂåñÂ∑•ÂÖ∑\")):\r\n                gr.Markdown(value=i18n(\"ËæìÂá∫logs/ÂÆûÈ™åÂêçÁõÆÂΩï‰∏ãÂ∫îÊúâ23456ÂºÄÂ§¥ÁöÑÊñá‰ª∂ÂíåÊñá‰ª∂Â§π\"))\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        inp_text = gr.Textbox(label=i18n(\"*ÊñáÊú¨Ê†áÊ≥®Êñá‰ª∂\"),value=r\"D:\\RVC1006\\GPT-SoVITS\\raw\\xxx.list\",interactive=True,scale=10)\r\n                    with gr.Row():\r\n                        inp_wav_dir = gr.Textbox(\r\n                            label=i18n(\"*ËÆ≠ÁªÉÈõÜÈü≥È¢ëÊñá‰ª∂ÁõÆÂΩï\"),\r\n                            # value=r\"D:\\RVC1006\\GPT-SoVITS\\raw\\xxx\",\r\n                            interactive=True,\r\n                            placeholder=i18n(\"Â°´ÂàáÂâ≤ÂêéÈü≥È¢ëÊâÄÂú®ÁõÆÂΩïÔºÅËØªÂèñÁöÑÈü≥È¢ëÊñá‰ª∂ÂÆåÊï¥Ë∑ØÂæÑ=ËØ•ÁõÆÂΩï-ÊãºÊé•-listÊñá‰ª∂ÈáåÊ≥¢ÂΩ¢ÂØπÂ∫îÁöÑÊñá‰ª∂ÂêçÔºà‰∏çÊòØÂÖ®Ë∑ØÂæÑÔºâ„ÄÇÂ¶ÇÊûúÁïôÁ©∫Âàô‰ΩøÁî®.listÊñá‰ª∂ÈáåÁöÑÁªùÂØπÂÖ®Ë∑ØÂæÑ„ÄÇ\"), scale=10\r\n                        )\r\n                gr.Markdown(value=i18n(\"1Aa-ÊñáÊú¨ÂÜÖÂÆπ\"))\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        gpu_numbers1a = gr.Textbox(label=i18n(\"GPUÂç°Âè∑‰ª•-ÂàÜÂâ≤ÔºåÊØè‰∏™Âç°Âè∑‰∏Ä‰∏™ËøõÁ®ã\"),value=\"%s-%s\"%(gpus,gpus),interactive=True)\r\n                    with gr.Row():\r\n                        bert_pretrained_dir = gr.Textbox(label=i18n(\"È¢ÑËÆ≠ÁªÉÁöÑ‰∏≠ÊñáBERTÊ®°ÂûãË∑ØÂæÑ\"),value=\"GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\",interactive=False,lines=2)\r\n                    with gr.Row():\r\n                        button1a_open = gr.Button(i18n(\"ÂºÄÂêØÊñáÊú¨Ëé∑Âèñ\"), variant=\"primary\",visible=True)\r\n                        button1a_close = gr.Button(i18n(\"ÁªàÊ≠¢ÊñáÊú¨Ëé∑ÂèñËøõÁ®ã\"), variant=\"primary\",visible=False)\r\n                    with gr.Row():\r\n                        info1a=gr.Textbox(label=i18n(\"ÊñáÊú¨ËøõÁ®ãËæìÂá∫‰ø°ÊÅØ\"))\r\n                gr.Markdown(value=i18n(\"1Ab-SSLËá™ÁõëÁù£ÁâπÂæÅÊèêÂèñ\"))\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        gpu_numbers1Ba = gr.Textbox(label=i18n(\"GPUÂç°Âè∑‰ª•-ÂàÜÂâ≤ÔºåÊØè‰∏™Âç°Âè∑‰∏Ä‰∏™ËøõÁ®ã\"),value=\"%s-%s\"%(gpus,gpus),interactive=True)\r\n                    with gr.Row():\r\n                        cnhubert_base_dir = gr.Textbox(label=i18n(\"È¢ÑËÆ≠ÁªÉÁöÑSSLÊ®°ÂûãË∑ØÂæÑ\"),value=\"GPT_SoVITS/pretrained_models/chinese-hubert-base\",interactive=False,lines=2)\r\n                    with gr.Row():\r\n                        button1b_open = gr.Button(i18n(\"ÂºÄÂêØSSLÊèêÂèñ\"), variant=\"primary\",visible=True)\r\n                        button1b_close = gr.Button(i18n(\"ÁªàÊ≠¢SSLÊèêÂèñËøõÁ®ã\"), variant=\"primary\",visible=False)\r\n                    with gr.Row():\r\n                        info1b=gr.Textbox(label=i18n(\"SSLËøõÁ®ãËæìÂá∫‰ø°ÊÅØ\"))\r\n                gr.Markdown(value=i18n(\"1Ac-ËØ≠‰πâtokenÊèêÂèñ\"))\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        gpu_numbers1c = gr.Textbox(label=i18n(\"GPUÂç°Âè∑‰ª•-ÂàÜÂâ≤ÔºåÊØè‰∏™Âç°Âè∑‰∏Ä‰∏™ËøõÁ®ã\"),value=\"%s-%s\"%(gpus,gpus),interactive=True)\r\n                    with gr.Row():\r\n                        pretrained_s2G_ = gr.Textbox(label=i18n(\"È¢ÑËÆ≠ÁªÉÁöÑSoVITS-GÊ®°ÂûãË∑ØÂæÑ\"), value=pretrained_sovits_name[-int(version[-1])+2], interactive=False,lines=2)\r\n                    with gr.Row():\r\n                        button1c_open = gr.Button(i18n(\"ÂºÄÂêØËØ≠‰πâtokenÊèêÂèñ\"), variant=\"primary\",visible=True)\r\n                        button1c_close = gr.Button(i18n(\"ÁªàÊ≠¢ËØ≠‰πâtokenÊèêÂèñËøõÁ®ã\"), variant=\"primary\",visible=False)\r\n                    with gr.Row():\r\n                        info1c=gr.Textbox(label=i18n(\"ËØ≠‰πâtokenÊèêÂèñËøõÁ®ãËæìÂá∫‰ø°ÊÅØ\"))\r\n                gr.Markdown(value=i18n(\"1Aabc-ËÆ≠ÁªÉÈõÜÊ†ºÂºèÂåñ‰∏ÄÈîÆ‰∏âËøû\"))\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        button1abc_open = gr.Button(i18n(\"ÂºÄÂêØ‰∏ÄÈîÆ‰∏âËøû\"), variant=\"primary\",visible=True)\r\n                        button1abc_close = gr.Button(i18n(\"ÁªàÊ≠¢‰∏ÄÈîÆ‰∏âËøû\"), variant=\"primary\",visible=False)\r\n                    with gr.Row():\r\n                        info1abc=gr.Textbox(label=i18n(\"‰∏ÄÈîÆ‰∏âËøûËøõÁ®ãËæìÂá∫‰ø°ÊÅØ\"))\r\n\r\n            pretrained_s2G.change(sync,[pretrained_s2G],[pretrained_s2G_])\r\n            open_asr_button.click(open_asr, [asr_inp_dir, asr_opt_dir, asr_model, asr_size, asr_lang, asr_precision], [asr_info,open_asr_button,close_asr_button,path_list,inp_text,inp_wav_dir])\r\n            close_asr_button.click(close_asr, [], [asr_info,open_asr_button,close_asr_button])\r\n            open_slicer_button.click(open_slice, [slice_inp_path,slice_opt_root,threshold,min_length,min_interval,hop_size,max_sil_kept,_max,alpha,n_process], [slicer_info,open_slicer_button,close_slicer_button,asr_inp_dir,denoise_input_dir,inp_wav_dir])\r\n            close_slicer_button.click(close_slice, [], [slicer_info,open_slicer_button,close_slicer_button])\r\n            open_denoise_button.click(open_denoise, [denoise_input_dir,denoise_output_dir], [denoise_info,open_denoise_button,close_denoise_button,asr_inp_dir,inp_wav_dir])\r\n            close_denoise_button.click(close_denoise, [], [denoise_info,open_denoise_button,close_denoise_button])\r\n            \r\n            button1a_open.click(open1a, [inp_text,inp_wav_dir,exp_name,gpu_numbers1a,bert_pretrained_dir], [info1a,button1a_open,button1a_close])\r\n            button1a_close.click(close1a, [], [info1a,button1a_open,button1a_close])\r\n            button1b_open.click(open1b, [inp_text,inp_wav_dir,exp_name,gpu_numbers1Ba,cnhubert_base_dir], [info1b,button1b_open,button1b_close])\r\n            button1b_close.click(close1b, [], [info1b,button1b_open,button1b_close])\r\n            button1c_open.click(open1c, [inp_text,exp_name,gpu_numbers1c,pretrained_s2G], [info1c,button1c_open,button1c_close])\r\n            button1c_close.click(close1c, [], [info1c,button1c_open,button1c_close])\r\n            button1abc_open.click(open1abc, [inp_text,inp_wav_dir,exp_name,gpu_numbers1a,gpu_numbers1Ba,gpu_numbers1c,bert_pretrained_dir,cnhubert_base_dir,pretrained_s2G], [info1abc,button1abc_open,button1abc_close])\r\n            button1abc_close.click(close1abc, [], [info1abc,button1abc_open,button1abc_close])\r\n            with gr.TabItem(i18n(\"1B-ÂæÆË∞ÉËÆ≠ÁªÉ\")):\r\n                gr.Markdown(value=i18n(\"1Ba-SoVITSËÆ≠ÁªÉ„ÄÇÁî®‰∫éÂàÜ‰∫´ÁöÑÊ®°ÂûãÊñá‰ª∂ËæìÂá∫Âú®SoVITS_weights‰∏ã„ÄÇ\"))\r\n                with gr.Row():\r\n                    with gr.Column():\r\n                        with gr.Row():\r\n                            batch_size = gr.Slider(minimum=1,maximum=40,step=1,label=i18n(\"ÊØèÂº†ÊòæÂç°ÁöÑbatch_size\"),value=default_batch_size,interactive=True)\r\n                            total_epoch = gr.Slider(minimum=1,maximum=25,step=1,label=i18n(\"ÊÄªËÆ≠ÁªÉËΩÆÊï∞total_epochÔºå‰∏çÂª∫ËÆÆÂ§™È´ò\"),value=8,interactive=True)\r\n                        with gr.Row():\r\n                            text_low_lr_rate = gr.Slider(minimum=0.2,maximum=0.6,step=0.05,label=i18n(\"ÊñáÊú¨Ê®°ÂùóÂ≠¶‰π†ÁéáÊùÉÈáç\"),value=0.4,interactive=True)\r\n                            save_every_epoch = gr.Slider(minimum=1,maximum=25,step=1,label=i18n(\"‰øùÂ≠òÈ¢ëÁéásave_every_epoch\"),value=4,interactive=True)\r\n                    with gr.Column():     \r\n                        with gr.Column():                   \r\n                            if_save_latest = gr.Checkbox(label=i18n(\"ÊòØÂê¶‰ªÖ‰øùÂ≠òÊúÄÊñ∞ÁöÑckptÊñá‰ª∂‰ª•ËäÇÁúÅÁ°¨ÁõòÁ©∫Èó¥\"), value=True, interactive=True, show_label=True)\r\n                            if_save_every_weights = gr.Checkbox(label=i18n(\"ÊòØÂê¶Âú®ÊØèÊ¨°‰øùÂ≠òÊó∂Èó¥ÁÇπÂ∞ÜÊúÄÁªàÂ∞èÊ®°Âûã‰øùÂ≠òËá≥weightsÊñá‰ª∂Â§π\"), value=True, interactive=True, show_label=True)\r\n                        with gr.Row():\r\n                            gpu_numbers1Ba = gr.Textbox(label=i18n(\"GPUÂç°Âè∑‰ª•-ÂàÜÂâ≤ÔºåÊØè‰∏™Âç°Âè∑‰∏Ä‰∏™ËøõÁ®ã\"), value=\"%s\" % (gpus), interactive=True)\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        button1Ba_open = gr.Button(i18n(\"ÂºÄÂêØSoVITSËÆ≠ÁªÉ\"), variant=\"primary\",visible=True)\r\n                        button1Ba_close = gr.Button(i18n(\"ÁªàÊ≠¢SoVITSËÆ≠ÁªÉ\"), variant=\"primary\",visible=False)\r\n                    with gr.Row():\r\n                        info1Ba=gr.Textbox(label=i18n(\"SoVITSËÆ≠ÁªÉËøõÁ®ãËæìÂá∫‰ø°ÊÅØ\"))\r\n                gr.Markdown(value=i18n(\"1Bb-GPTËÆ≠ÁªÉ„ÄÇÁî®‰∫éÂàÜ‰∫´ÁöÑÊ®°ÂûãÊñá‰ª∂ËæìÂá∫Âú®GPT_weights‰∏ã„ÄÇ\"))\r\n                with gr.Row():\r\n                    with gr.Column():\r\n                        with gr.Row():\r\n                            batch_size1Bb = gr.Slider(minimum=1,maximum=40,step=1,label=i18n(\"ÊØèÂº†ÊòæÂç°ÁöÑbatch_size\"),value=default_batch_size,interactive=True)\r\n                            total_epoch1Bb = gr.Slider(minimum=2,maximum=50,step=1,label=i18n(\"ÊÄªËÆ≠ÁªÉËΩÆÊï∞total_epoch\"),value=15,interactive=True)\r\n                        with gr.Row():\r\n                            save_every_epoch1Bb = gr.Slider(minimum=1,maximum=50,step=1,label=i18n(\"‰øùÂ≠òÈ¢ëÁéásave_every_epoch\"),value=5,interactive=True) \r\n                            if_dpo = gr.Checkbox(label=i18n(\"ÊòØÂê¶ÂºÄÂêØdpoËÆ≠ÁªÉÈÄâÈ°π(ÂÆûÈ™åÊÄß)\"), value=False, interactive=True, show_label=True)   \r\n                    with gr.Column():\r\n                        with gr.Column():\r\n                                if_save_latest1Bb = gr.Checkbox(label=i18n(\"ÊòØÂê¶‰ªÖ‰øùÂ≠òÊúÄÊñ∞ÁöÑckptÊñá‰ª∂‰ª•ËäÇÁúÅÁ°¨ÁõòÁ©∫Èó¥\"), value=True, interactive=True, show_label=True)\r\n                                if_save_every_weights1Bb = gr.Checkbox(label=i18n(\"ÊòØÂê¶Âú®ÊØèÊ¨°‰øùÂ≠òÊó∂Èó¥ÁÇπÂ∞ÜÊúÄÁªàÂ∞èÊ®°Âûã‰øùÂ≠òËá≥weightsÊñá‰ª∂Â§π\"), value=True, interactive=True, show_label=True)\r\n                        with gr.Row():                            \r\n                            gpu_numbers1Bb = gr.Textbox(label=i18n(\"GPUÂç°Âè∑‰ª•-ÂàÜÂâ≤ÔºåÊØè‰∏™Âç°Âè∑‰∏Ä‰∏™ËøõÁ®ã\"), value=\"%s\" % (gpus), interactive=True)\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        button1Bb_open = gr.Button(i18n(\"ÂºÄÂêØGPTËÆ≠ÁªÉ\"), variant=\"primary\",visible=True)\r\n                        button1Bb_close = gr.Button(i18n(\"ÁªàÊ≠¢GPTËÆ≠ÁªÉ\"), variant=\"primary\",visible=False)\r\n                    with gr.Row():\r\n                        info1Bb=gr.Textbox(label=i18n(\"GPTËÆ≠ÁªÉËøõÁ®ãËæìÂá∫‰ø°ÊÅØ\"))\r\n            button1Ba_open.click(open1Ba, [batch_size,total_epoch,exp_name,text_low_lr_rate,if_save_latest,if_save_every_weights,save_every_epoch,gpu_numbers1Ba,pretrained_s2G,pretrained_s2D], [info1Ba,button1Ba_open,button1Ba_close])\r\n            button1Ba_close.click(close1Ba, [], [info1Ba,button1Ba_open,button1Ba_close])\r\n            button1Bb_open.click(open1Bb, [batch_size1Bb,total_epoch1Bb,exp_name,if_dpo,if_save_latest1Bb,if_save_every_weights1Bb,save_every_epoch1Bb,gpu_numbers1Bb,pretrained_s1],   [info1Bb,button1Bb_open,button1Bb_close])\r\n            button1Bb_close.click(close1Bb, [], [info1Bb,button1Bb_open,button1Bb_close])\r\n            with gr.TabItem(i18n(\"1C-Êé®ÁêÜ\")):\r\n                gr.Markdown(value=i18n(\"ÈÄâÊã©ËÆ≠ÁªÉÂÆåÂ≠òÊîæÂú®SoVITS_weightsÂíåGPT_weights‰∏ãÁöÑÊ®°Âûã„ÄÇÈªòËÆ§ÁöÑ‰∏Ä‰∏™ÊòØÂ∫ïÊ®°Ôºå‰ΩìÈ™å5ÁßíZero Shot TTSÁî®„ÄÇ\"))\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        GPT_dropdown = gr.Dropdown(label=i18n(\"*GPTÊ®°ÂûãÂàóË°®\"), choices=sorted(GPT_names,key=custom_sort_key),value=pretrained_gpt_name[0],interactive=True)\r\n                        SoVITS_dropdown = gr.Dropdown(label=i18n(\"*SoVITSÊ®°ÂûãÂàóË°®\"), choices=sorted(SoVITS_names,key=custom_sort_key),value=pretrained_sovits_name[0],interactive=True)\r\n                    with gr.Row():\r\n                        gpu_number_1C=gr.Textbox(label=i18n(\"GPUÂç°Âè∑,Âè™ËÉΩÂ°´1‰∏™Êï¥Êï∞\"), value=gpus, interactive=True)\r\n                        refresh_button = gr.Button(i18n(\"Âà∑Êñ∞Ê®°ÂûãË∑ØÂæÑ\"), variant=\"primary\")\r\n                    refresh_button.click(fn=change_choices,inputs=[],outputs=[SoVITS_dropdown,GPT_dropdown])\r\n                with gr.Row():\r\n                    with gr.Row():\r\n                        batched_infer_enabled = gr.Checkbox(label=i18n(\"ÂêØÁî®Âπ∂Ë°åÊé®ÁêÜÁâàÊú¨(Êé®ÁêÜÈÄüÂ∫¶Êõ¥Âø´)\"), value=False, interactive=True, show_label=True)\r\n                    with gr.Row():\r\n                        open_tts = gr.Button(value=i18n(\"ÂºÄÂêØTTSÊé®ÁêÜWebUI\"),variant='primary',visible=True)\r\n                        close_tts = gr.Button(value=i18n(\"ÂÖ≥Èó≠TTSÊé®ÁêÜWebUI\"),variant='primary',visible=False)\r\n                    with gr.Row():\r\n                        tts_info = gr.Textbox(label=i18n(\"TTSÊé®ÁêÜWebUIËøõÁ®ãËæìÂá∫‰ø°ÊÅØ\"))\r\n                    open_tts.click(change_tts_inference, [bert_pretrained_dir,cnhubert_base_dir,gpu_number_1C,GPT_dropdown,SoVITS_dropdown, batched_infer_enabled], [tts_info,open_tts,close_tts])\r\n                    close_tts.click(change_tts_inference, [bert_pretrained_dir,cnhubert_base_dir,gpu_number_1C,GPT_dropdown,SoVITS_dropdown, batched_infer_enabled], [tts_info,open_tts,close_tts])\r\n            version_checkbox.change(switch_version,[version_checkbox],[pretrained_s2G,pretrained_s2D,pretrained_s1,GPT_dropdown,SoVITS_dropdown])\r\n        with gr.TabItem(i18n(\"2-GPT-SoVITS-ÂèòÂ£∞\")):gr.Markdown(value=i18n(\"ÊñΩÂ∑•‰∏≠ÔºåËØ∑ÈùôÂÄô‰Ω≥Èü≥\"))\r\n    app.queue().launch(#concurrency_count=511, max_size=1022\r\n        server_name=\"0.0.0.0\",\r\n        inbrowser=True,\r\n        share=is_share,\r\n        server_port=webui_port_main,\r\n        quiet=True,\r\n    )\r\n"
        }
      ]
    }
  ]
}