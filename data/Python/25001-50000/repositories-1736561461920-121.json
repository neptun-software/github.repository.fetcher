{
  "metadata": {
    "timestamp": 1736561461920,
    "page": 121,
    "hasNextPage": false,
    "endCursor": "Y3Vyc29yOjEyMQ==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Stability-AI/generative-models",
      "stars": 25047,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.11328125,
          "content": "# extensions\n*.egg-info\n*.py[cod]\n\n# envs\n.pt13\n.pt2\n\n# directories\n/checkpoints\n/dist\n/outputs\n/build\n/src\n/.vscode"
        },
        {
          "name": "CODEOWNERS",
          "type": "blob",
          "size": 0.03515625,
          "content": ".github @Stability-AI/infrastructure"
        },
        {
          "name": "LICENSE-CODE",
          "type": "blob",
          "size": 1.04296875,
          "content": "MIT License\n\nCopyright (c) 2023 Stability AI\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 19.2109375,
          "content": "# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n\n\n**July 24, 2024**\n- We are releasing **[Stable Video 4D (SV4D)](https://huggingface.co/stabilityai/sv4d)**, a video-to-4D diffusion model for novel-view video synthesis. For research purposes:\n    - **SV4D** was trained to generate 40 frames (5 video frames x 8 camera views) at 576x576 resolution, given 5 context frames (the input video), and 8 reference views (synthesised from the first frame of the input video, using a multi-view diffusion model like SV3D) of the same size, ideally white-background images with one object.\n    - To generate longer novel-view videos (21 frames), we propose a novel sampling method using SV4D, by first sampling 5 anchor frames and then densely sampling the remaining frames while maintaining temporal consistency.\n    - To run the community-build gradio demo locally, run `python -m scripts.demo.gradio_app_sv4d`.\n    - Please check our [project page](https://sv4d.github.io), [tech report](https://sv4d.github.io/static/sv4d_technical_report.pdf) and [video summary](https://www.youtube.com/watch?v=RBP8vdAWTgk) for more details.\n\n**QUICKSTART** : `python scripts/sampling/simple_video_sample_4d.py --input_path assets/sv4d_videos/test_video1.mp4 --output_folder outputs/sv4d` (after downloading [sv4d.safetensors](https://huggingface.co/stabilityai/sv4d) and [sv3d_u.safetensors](https://huggingface.co/stabilityai/sv3d) from HuggingFace into `checkpoints/`)\n\nTo run **SV4D** on a single input video of 21 frames:\n- Download SV3D models (`sv3d_u.safetensors` and `sv3d_p.safetensors`) from [here](https://huggingface.co/stabilityai/sv3d) and SV4D model (`sv4d.safetensors`) from [here](https://huggingface.co/stabilityai/sv4d) to `checkpoints/`\n- Run `python scripts/sampling/simple_video_sample_4d.py --input_path <path/to/video>`\n    - `input_path` : The input video `<path/to/video>` can be\n      - a single video file in `gif` or `mp4` format, such as `assets/sv4d_videos/test_video1.mp4`, or\n      - a folder containing images of video frames in `.jpg`, `.jpeg`, or `.png` format, or\n      - a file name pattern matching images of video frames.\n    - `num_steps` : default is 20, can increase to 50 for better quality but longer sampling time.\n    - `sv3d_version` : To specify the SV3D model to generate reference multi-views, set `--sv3d_version=sv3d_u` for SV3D_u or `--sv3d_version=sv3d_p` for SV3D_p.\n    - `elevations_deg` : To generate novel-view videos at a specified elevation (default elevation is 10) using SV3D_p (default is SV3D_u), run `python scripts/sampling/simple_video_sample_4d.py --input_path assets/sv4d_videos/test_video1.mp4 --sv3d_version sv3d_p --elevations_deg 30.0`\n    - **Background removal** : For input videos with plain background, (optionally) use [rembg](https://github.com/danielgatis/rembg) to remove background and crop video frames by setting `--remove_bg=True`. To obtain higher quality outputs on real-world input videos with noisy background, try segmenting the foreground object using [Clipdrop](https://clipdrop.co/) or [SAM2](https://github.com/facebookresearch/segment-anything-2) before running SV4D.\n    - **Low VRAM environment** : To run on GPUs with low VRAM, try setting `--encoding_t=1` (of frames encoded at a time) and `--decoding_t=1` (of frames decoded at a time) or lower video resolution like `--img_size=512`.\n\n  ![tile](assets/sv4d.gif)\n\n\n**March 18, 2024**\n- We are releasing **[SV3D](https://huggingface.co/stabilityai/sv3d)**, an image-to-video model for novel multi-view synthesis, for research purposes:\n    - **SV3D** was trained to generate 21 frames at resolution 576x576, given 1 context frame of the same size, ideally a white-background image with one object.\n    - **SV3D_u**: This variant generates orbital videos based on single image inputs without camera conditioning..\n    - **SV3D_p**: Extending the capability of **SVD3_u**, this variant accommodates both single images and orbital views allowing for the creation of 3D video along specified camera paths.\n    - We extend the streamlit demo `scripts/demo/video_sampling.py` and the standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Please check our [project page](https://sv3d.github.io), [tech report](https://sv3d.github.io/static/paper.pdf) and [video summary](https://youtu.be/Zqw4-1LcfWg) for more details.\n\nTo run **SV3D_u** on a single image:\n- Download `sv3d_u.safetensors` from https://huggingface.co/stabilityai/sv3d to `checkpoints/sv3d_u.safetensors`\n- Run `python scripts/sampling/simple_video_sample.py --input_path <path/to/image.png> --version sv3d_u`\n\nTo run **SV3D_p** on a single image:\n- Download `sv3d_p.safetensors` from https://huggingface.co/stabilityai/sv3d to `checkpoints/sv3d_p.safetensors`\n1. Generate static orbit at a specified elevation eg. 10.0 : `python scripts/sampling/simple_video_sample.py --input_path <path/to/image.png> --version sv3d_p --elevations_deg 10.0`\n2. Generate dynamic orbit at a specified elevations and azimuths: specify sequences of 21 elevations (in degrees) to `elevations_deg` ([-90, 90]), and 21 azimuths (in degrees) to `azimuths_deg` [0, 360] in sorted order from 0 to 360. For example: `python scripts/sampling/simple_video_sample.py --input_path <path/to/image.png> --version sv3d_p --elevations_deg [<list of 21 elevations in degrees>] --azimuths_deg [<list of 21 azimuths in degrees>]`\n\nTo run SVD or SV3D on a streamlit server:\n`streamlit run scripts/demo/video_sampling.py`\n\n  ![tile](assets/sv3d.gif)\n\n\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - You can run the community-build gradio demo locally by running `python -m scripts.demo.gradio_app`.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the [\"denoiser framework\"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name=\"installation\"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install \"numpy>=1.17\" \"PyWavelets>=1.1.1\" \"opencv-python>=4.1.0.25\"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don't forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {\"jpg\": x,  # this is a tensor -1...1 chw\n           \"txt\": \"a beautiful image\"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 32.5732421875,
          "content": "import argparse\nimport datetime\nimport glob\nimport inspect\nimport os\nimport sys\nfrom inspect import Parameter\nfrom typing import Union\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nimport torchvision\nimport wandb\nfrom matplotlib import pyplot as plt\nfrom natsort import natsorted\nfrom omegaconf import OmegaConf\nfrom packaging import version\nfrom PIL import Image\nfrom pytorch_lightning import seed_everything\nfrom pytorch_lightning.callbacks import Callback\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.trainer import Trainer\nfrom pytorch_lightning.utilities import rank_zero_only\n\nfrom sgm.util import exists, instantiate_from_config, isheatmap\n\nMULTINODE_HACKS = True\n\n\ndef default_trainer_args():\n    argspec = dict(inspect.signature(Trainer.__init__).parameters)\n    argspec.pop(\"self\")\n    default_args = {\n        param: argspec[param].default\n        for param in argspec\n        if argspec[param] != Parameter.empty\n    }\n    return default_args\n\n\ndef get_parser(**parser_kwargs):\n    def str2bool(v):\n        if isinstance(v, bool):\n            return v\n        if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n            return True\n        elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n            return False\n        else:\n            raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\n    parser = argparse.ArgumentParser(**parser_kwargs)\n    parser.add_argument(\n        \"-n\",\n        \"--name\",\n        type=str,\n        const=True,\n        default=\"\",\n        nargs=\"?\",\n        help=\"postfix for logdir\",\n    )\n    parser.add_argument(\n        \"--no_date\",\n        type=str2bool,\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"if True, skip date generation for logdir and only use naming via opt.base or opt.name (+ opt.postfix, optionally)\",\n    )\n    parser.add_argument(\n        \"-r\",\n        \"--resume\",\n        type=str,\n        const=True,\n        default=\"\",\n        nargs=\"?\",\n        help=\"resume from logdir or checkpoint in logdir\",\n    )\n    parser.add_argument(\n        \"-b\",\n        \"--base\",\n        nargs=\"*\",\n        metavar=\"base_config.yaml\",\n        help=\"paths to base configs. Loaded from left-to-right. \"\n        \"Parameters can be overwritten or added with command-line options of the form `--key value`.\",\n        default=list(),\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--train\",\n        type=str2bool,\n        const=True,\n        default=True,\n        nargs=\"?\",\n        help=\"train\",\n    )\n    parser.add_argument(\n        \"--no-test\",\n        type=str2bool,\n        const=True,\n        default=False,\n        nargs=\"?\",\n        help=\"disable test\",\n    )\n    parser.add_argument(\n        \"-p\", \"--project\", help=\"name of new or path to existing project\"\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--debug\",\n        type=str2bool,\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"enable post-mortem debugging\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--seed\",\n        type=int,\n        default=23,\n        help=\"seed for seed_everything\",\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--postfix\",\n        type=str,\n        default=\"\",\n        help=\"post-postfix for default name\",\n    )\n    parser.add_argument(\n        \"--projectname\",\n        type=str,\n        default=\"stablediffusion\",\n    )\n    parser.add_argument(\n        \"-l\",\n        \"--logdir\",\n        type=str,\n        default=\"logs\",\n        help=\"directory for logging dat shit\",\n    )\n    parser.add_argument(\n        \"--scale_lr\",\n        type=str2bool,\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"scale base-lr by ngpu * batch_size * n_accumulate\",\n    )\n    parser.add_argument(\n        \"--legacy_naming\",\n        type=str2bool,\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"name run based on config file name if true, else by whole path\",\n    )\n    parser.add_argument(\n        \"--enable_tf32\",\n        type=str2bool,\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"enables the TensorFloat32 format both for matmuls and cuDNN for pytorch 1.12\",\n    )\n    parser.add_argument(\n        \"--startup\",\n        type=str,\n        default=None,\n        help=\"Startuptime from distributed script\",\n    )\n    parser.add_argument(\n        \"--wandb\",\n        type=str2bool,\n        nargs=\"?\",\n        const=True,\n        default=False,  # TODO: later default to True\n        help=\"log to wandb\",\n    )\n    parser.add_argument(\n        \"--no_base_name\",\n        type=str2bool,\n        nargs=\"?\",\n        const=True,\n        default=False,  # TODO: later default to True\n        help=\"log to wandb\",\n    )\n    if version.parse(torch.__version__) >= version.parse(\"2.0.0\"):\n        parser.add_argument(\n            \"--resume_from_checkpoint\",\n            type=str,\n            default=None,\n            help=\"single checkpoint file to resume from\",\n        )\n    default_args = default_trainer_args()\n    for key in default_args:\n        parser.add_argument(\"--\" + key, default=default_args[key])\n    return parser\n\n\ndef get_checkpoint_name(logdir):\n    ckpt = os.path.join(logdir, \"checkpoints\", \"last**.ckpt\")\n    ckpt = natsorted(glob.glob(ckpt))\n    print('available \"last\" checkpoints:')\n    print(ckpt)\n    if len(ckpt) > 1:\n        print(\"got most recent checkpoint\")\n        ckpt = sorted(ckpt, key=lambda x: os.path.getmtime(x))[-1]\n        print(f\"Most recent ckpt is {ckpt}\")\n        with open(os.path.join(logdir, \"most_recent_ckpt.txt\"), \"w\") as f:\n            f.write(ckpt + \"\\n\")\n        try:\n            version = int(ckpt.split(\"/\")[-1].split(\"-v\")[-1].split(\".\")[0])\n        except Exception as e:\n            print(\"version confusion but not bad\")\n            print(e)\n            version = 1\n        # version = last_version + 1\n    else:\n        # in this case, we only have one \"last.ckpt\"\n        ckpt = ckpt[0]\n        version = 1\n    melk_ckpt_name = f\"last-v{version}.ckpt\"\n    print(f\"Current melk ckpt name: {melk_ckpt_name}\")\n    return ckpt, melk_ckpt_name\n\n\nclass SetupCallback(Callback):\n    def __init__(\n        self,\n        resume,\n        now,\n        logdir,\n        ckptdir,\n        cfgdir,\n        config,\n        lightning_config,\n        debug,\n        ckpt_name=None,\n    ):\n        super().__init__()\n        self.resume = resume\n        self.now = now\n        self.logdir = logdir\n        self.ckptdir = ckptdir\n        self.cfgdir = cfgdir\n        self.config = config\n        self.lightning_config = lightning_config\n        self.debug = debug\n        self.ckpt_name = ckpt_name\n\n    def on_exception(self, trainer: pl.Trainer, pl_module, exception):\n        if not self.debug and trainer.global_rank == 0:\n            print(\"Summoning checkpoint.\")\n            if self.ckpt_name is None:\n                ckpt_path = os.path.join(self.ckptdir, \"last.ckpt\")\n            else:\n                ckpt_path = os.path.join(self.ckptdir, self.ckpt_name)\n            trainer.save_checkpoint(ckpt_path)\n\n    def on_fit_start(self, trainer, pl_module):\n        if trainer.global_rank == 0:\n            # Create logdirs and save configs\n            os.makedirs(self.logdir, exist_ok=True)\n            os.makedirs(self.ckptdir, exist_ok=True)\n            os.makedirs(self.cfgdir, exist_ok=True)\n\n            if \"callbacks\" in self.lightning_config:\n                if (\n                    \"metrics_over_trainsteps_checkpoint\"\n                    in self.lightning_config[\"callbacks\"]\n                ):\n                    os.makedirs(\n                        os.path.join(self.ckptdir, \"trainstep_checkpoints\"),\n                        exist_ok=True,\n                    )\n            print(\"Project config\")\n            print(OmegaConf.to_yaml(self.config))\n            if MULTINODE_HACKS:\n                import time\n\n                time.sleep(5)\n            OmegaConf.save(\n                self.config,\n                os.path.join(self.cfgdir, \"{}-project.yaml\".format(self.now)),\n            )\n\n            print(\"Lightning config\")\n            print(OmegaConf.to_yaml(self.lightning_config))\n            OmegaConf.save(\n                OmegaConf.create({\"lightning\": self.lightning_config}),\n                os.path.join(self.cfgdir, \"{}-lightning.yaml\".format(self.now)),\n            )\n\n        else:\n            # ModelCheckpoint callback created log directory --- remove it\n            if not MULTINODE_HACKS and not self.resume and os.path.exists(self.logdir):\n                dst, name = os.path.split(self.logdir)\n                dst = os.path.join(dst, \"child_runs\", name)\n                os.makedirs(os.path.split(dst)[0], exist_ok=True)\n                try:\n                    os.rename(self.logdir, dst)\n                except FileNotFoundError:\n                    pass\n\n\nclass ImageLogger(Callback):\n    def __init__(\n        self,\n        batch_frequency,\n        max_images,\n        clamp=True,\n        increase_log_steps=True,\n        rescale=True,\n        disabled=False,\n        log_on_batch_idx=False,\n        log_first_step=False,\n        log_images_kwargs=None,\n        log_before_first_step=False,\n        enable_autocast=True,\n    ):\n        super().__init__()\n        self.enable_autocast = enable_autocast\n        self.rescale = rescale\n        self.batch_freq = batch_frequency\n        self.max_images = max_images\n        self.log_steps = [2**n for n in range(int(np.log2(self.batch_freq)) + 1)]\n        if not increase_log_steps:\n            self.log_steps = [self.batch_freq]\n        self.clamp = clamp\n        self.disabled = disabled\n        self.log_on_batch_idx = log_on_batch_idx\n        self.log_images_kwargs = log_images_kwargs if log_images_kwargs else {}\n        self.log_first_step = log_first_step\n        self.log_before_first_step = log_before_first_step\n\n    @rank_zero_only\n    def log_local(\n        self,\n        save_dir,\n        split,\n        images,\n        global_step,\n        current_epoch,\n        batch_idx,\n        pl_module: Union[None, pl.LightningModule] = None,\n    ):\n        root = os.path.join(save_dir, \"images\", split)\n        for k in images:\n            if isheatmap(images[k]):\n                fig, ax = plt.subplots()\n                ax = ax.matshow(\n                    images[k].cpu().numpy(), cmap=\"hot\", interpolation=\"lanczos\"\n                )\n                plt.colorbar(ax)\n                plt.axis(\"off\")\n\n                filename = \"{}_gs-{:06}_e-{:06}_b-{:06}.png\".format(\n                    k, global_step, current_epoch, batch_idx\n                )\n                os.makedirs(root, exist_ok=True)\n                path = os.path.join(root, filename)\n                plt.savefig(path)\n                plt.close()\n                # TODO: support wandb\n            else:\n                grid = torchvision.utils.make_grid(images[k], nrow=4)\n                if self.rescale:\n                    grid = (grid + 1.0) / 2.0  # -1,1 -> 0,1; c,h,w\n                grid = grid.transpose(0, 1).transpose(1, 2).squeeze(-1)\n                grid = grid.numpy()\n                grid = (grid * 255).astype(np.uint8)\n                filename = \"{}_gs-{:06}_e-{:06}_b-{:06}.png\".format(\n                    k, global_step, current_epoch, batch_idx\n                )\n                path = os.path.join(root, filename)\n                os.makedirs(os.path.split(path)[0], exist_ok=True)\n                img = Image.fromarray(grid)\n                img.save(path)\n                if exists(pl_module):\n                    assert isinstance(\n                        pl_module.logger, WandbLogger\n                    ), \"logger_log_image only supports WandbLogger currently\"\n                    pl_module.logger.log_image(\n                        key=f\"{split}/{k}\",\n                        images=[\n                            img,\n                        ],\n                        step=pl_module.global_step,\n                    )\n\n    @rank_zero_only\n    def log_img(self, pl_module, batch, batch_idx, split=\"train\"):\n        check_idx = batch_idx if self.log_on_batch_idx else pl_module.global_step\n        if (\n            self.check_frequency(check_idx)\n            and hasattr(pl_module, \"log_images\")  # batch_idx % self.batch_freq == 0\n            and callable(pl_module.log_images)\n            and\n            # batch_idx > 5 and\n            self.max_images > 0\n        ):\n            logger = type(pl_module.logger)\n            is_train = pl_module.training\n            if is_train:\n                pl_module.eval()\n\n            gpu_autocast_kwargs = {\n                \"enabled\": self.enable_autocast,  # torch.is_autocast_enabled(),\n                \"dtype\": torch.get_autocast_gpu_dtype(),\n                \"cache_enabled\": torch.is_autocast_cache_enabled(),\n            }\n            with torch.no_grad(), torch.cuda.amp.autocast(**gpu_autocast_kwargs):\n                images = pl_module.log_images(\n                    batch, split=split, **self.log_images_kwargs\n                )\n\n            for k in images:\n                N = min(images[k].shape[0], self.max_images)\n                if not isheatmap(images[k]):\n                    images[k] = images[k][:N]\n                if isinstance(images[k], torch.Tensor):\n                    images[k] = images[k].detach().float().cpu()\n                    if self.clamp and not isheatmap(images[k]):\n                        images[k] = torch.clamp(images[k], -1.0, 1.0)\n\n            self.log_local(\n                pl_module.logger.save_dir,\n                split,\n                images,\n                pl_module.global_step,\n                pl_module.current_epoch,\n                batch_idx,\n                pl_module=pl_module\n                if isinstance(pl_module.logger, WandbLogger)\n                else None,\n            )\n\n            if is_train:\n                pl_module.train()\n\n    def check_frequency(self, check_idx):\n        if ((check_idx % self.batch_freq) == 0 or (check_idx in self.log_steps)) and (\n            check_idx > 0 or self.log_first_step\n        ):\n            try:\n                self.log_steps.pop(0)\n            except IndexError as e:\n                print(e)\n                pass\n            return True\n        return False\n\n    @rank_zero_only\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        if not self.disabled and (pl_module.global_step > 0 or self.log_first_step):\n            self.log_img(pl_module, batch, batch_idx, split=\"train\")\n\n    @rank_zero_only\n    def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):\n        if self.log_before_first_step and pl_module.global_step == 0:\n            print(f\"{self.__class__.__name__}: logging before training\")\n            self.log_img(pl_module, batch, batch_idx, split=\"train\")\n\n    @rank_zero_only\n    def on_validation_batch_end(\n        self, trainer, pl_module, outputs, batch, batch_idx, *args, **kwargs\n    ):\n        if not self.disabled and pl_module.global_step > 0:\n            self.log_img(pl_module, batch, batch_idx, split=\"val\")\n        if hasattr(pl_module, \"calibrate_grad_norm\"):\n            if (\n                pl_module.calibrate_grad_norm and batch_idx % 25 == 0\n            ) and batch_idx > 0:\n                self.log_gradients(trainer, pl_module, batch_idx=batch_idx)\n\n\n@rank_zero_only\ndef init_wandb(save_dir, opt, config, group_name, name_str):\n    print(f\"setting WANDB_DIR to {save_dir}\")\n    os.makedirs(save_dir, exist_ok=True)\n\n    os.environ[\"WANDB_DIR\"] = save_dir\n    if opt.debug:\n        wandb.init(project=opt.projectname, mode=\"offline\", group=group_name)\n    else:\n        wandb.init(\n            project=opt.projectname,\n            config=config,\n            settings=wandb.Settings(code_dir=\"./sgm\"),\n            group=group_name,\n            name=name_str,\n        )\n\n\nif __name__ == \"__main__\":\n    # custom parser to specify config files, train, test and debug mode,\n    # postfix, resume.\n    # `--key value` arguments are interpreted as arguments to the trainer.\n    # `nested.key=value` arguments are interpreted as config parameters.\n    # configs are merged from left-to-right followed by command line parameters.\n\n    # model:\n    #   base_learning_rate: float\n    #   target: path to lightning module\n    #   params:\n    #       key: value\n    # data:\n    #   target: main.DataModuleFromConfig\n    #   params:\n    #      batch_size: int\n    #      wrap: bool\n    #      train:\n    #          target: path to train dataset\n    #          params:\n    #              key: value\n    #      validation:\n    #          target: path to validation dataset\n    #          params:\n    #              key: value\n    #      test:\n    #          target: path to test dataset\n    #          params:\n    #              key: value\n    # lightning: (optional, has sane defaults and can be specified on cmdline)\n    #   trainer:\n    #       additional arguments to trainer\n    #   logger:\n    #       logger to instantiate\n    #   modelcheckpoint:\n    #       modelcheckpoint to instantiate\n    #   callbacks:\n    #       callback1:\n    #           target: importpath\n    #           params:\n    #               key: value\n\n    now = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n\n    # add cwd for convenience and to make classes in this file available when\n    # running as `python main.py`\n    # (in particular `main.DataModuleFromConfig`)\n    sys.path.append(os.getcwd())\n\n    parser = get_parser()\n\n    opt, unknown = parser.parse_known_args()\n\n    if opt.name and opt.resume:\n        raise ValueError(\n            \"-n/--name and -r/--resume cannot be specified both.\"\n            \"If you want to resume training in a new log folder, \"\n            \"use -n/--name in combination with --resume_from_checkpoint\"\n        )\n    melk_ckpt_name = None\n    name = None\n    if opt.resume:\n        if not os.path.exists(opt.resume):\n            raise ValueError(\"Cannot find {}\".format(opt.resume))\n        if os.path.isfile(opt.resume):\n            paths = opt.resume.split(\"/\")\n            # idx = len(paths)-paths[::-1].index(\"logs\")+1\n            # logdir = \"/\".join(paths[:idx])\n            logdir = \"/\".join(paths[:-2])\n            ckpt = opt.resume\n            _, melk_ckpt_name = get_checkpoint_name(logdir)\n        else:\n            assert os.path.isdir(opt.resume), opt.resume\n            logdir = opt.resume.rstrip(\"/\")\n            ckpt, melk_ckpt_name = get_checkpoint_name(logdir)\n\n        print(\"#\" * 100)\n        print(f'Resuming from checkpoint \"{ckpt}\"')\n        print(\"#\" * 100)\n\n        opt.resume_from_checkpoint = ckpt\n        base_configs = sorted(glob.glob(os.path.join(logdir, \"configs/*.yaml\")))\n        opt.base = base_configs + opt.base\n        _tmp = logdir.split(\"/\")\n        nowname = _tmp[-1]\n    else:\n        if opt.name:\n            name = \"_\" + opt.name\n        elif opt.base:\n            if opt.no_base_name:\n                name = \"\"\n            else:\n                if opt.legacy_naming:\n                    cfg_fname = os.path.split(opt.base[0])[-1]\n                    cfg_name = os.path.splitext(cfg_fname)[0]\n                else:\n                    assert \"configs\" in os.path.split(opt.base[0])[0], os.path.split(\n                        opt.base[0]\n                    )[0]\n                    cfg_path = os.path.split(opt.base[0])[0].split(os.sep)[\n                        os.path.split(opt.base[0])[0].split(os.sep).index(\"configs\")\n                        + 1 :\n                    ]  # cut away the first one (we assert all configs are in \"configs\")\n                    cfg_name = os.path.splitext(os.path.split(opt.base[0])[-1])[0]\n                    cfg_name = \"-\".join(cfg_path) + f\"-{cfg_name}\"\n                name = \"_\" + cfg_name\n        else:\n            name = \"\"\n        if not opt.no_date:\n            nowname = now + name + opt.postfix\n        else:\n            nowname = name + opt.postfix\n            if nowname.startswith(\"_\"):\n                nowname = nowname[1:]\n        logdir = os.path.join(opt.logdir, nowname)\n        print(f\"LOGDIR: {logdir}\")\n\n    ckptdir = os.path.join(logdir, \"checkpoints\")\n    cfgdir = os.path.join(logdir, \"configs\")\n    seed_everything(opt.seed, workers=True)\n\n    # move before model init, in case a torch.compile(...) is called somewhere\n    if opt.enable_tf32:\n        # pt_version = version.parse(torch.__version__)\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n        print(f\"Enabling TF32 for PyTorch {torch.__version__}\")\n    else:\n        print(f\"Using default TF32 settings for PyTorch {torch.__version__}:\")\n        print(\n            f\"torch.backends.cuda.matmul.allow_tf32={torch.backends.cuda.matmul.allow_tf32}\"\n        )\n        print(f\"torch.backends.cudnn.allow_tf32={torch.backends.cudnn.allow_tf32}\")\n\n    try:\n        # init and save configs\n        configs = [OmegaConf.load(cfg) for cfg in opt.base]\n        cli = OmegaConf.from_dotlist(unknown)\n        config = OmegaConf.merge(*configs, cli)\n        lightning_config = config.pop(\"lightning\", OmegaConf.create())\n        # merge trainer cli with config\n        trainer_config = lightning_config.get(\"trainer\", OmegaConf.create())\n\n        # default to gpu\n        trainer_config[\"accelerator\"] = \"gpu\"\n        #\n        standard_args = default_trainer_args()\n        for k in standard_args:\n            if getattr(opt, k) != standard_args[k]:\n                trainer_config[k] = getattr(opt, k)\n\n        ckpt_resume_path = opt.resume_from_checkpoint\n\n        if not \"devices\" in trainer_config and trainer_config[\"accelerator\"] != \"gpu\":\n            del trainer_config[\"accelerator\"]\n            cpu = True\n        else:\n            gpuinfo = trainer_config[\"devices\"]\n            print(f\"Running on GPUs {gpuinfo}\")\n            cpu = False\n        trainer_opt = argparse.Namespace(**trainer_config)\n        lightning_config.trainer = trainer_config\n\n        # model\n        model = instantiate_from_config(config.model)\n\n        # trainer and callbacks\n        trainer_kwargs = dict()\n\n        # default logger configs\n        default_logger_cfgs = {\n            \"wandb\": {\n                \"target\": \"pytorch_lightning.loggers.WandbLogger\",\n                \"params\": {\n                    \"name\": nowname,\n                    # \"save_dir\": logdir,\n                    \"offline\": opt.debug,\n                    \"id\": nowname,\n                    \"project\": opt.projectname,\n                    \"log_model\": False,\n                    # \"dir\": logdir,\n                },\n            },\n            \"csv\": {\n                \"target\": \"pytorch_lightning.loggers.CSVLogger\",\n                \"params\": {\n                    \"name\": \"testtube\",  # hack for sbord fanatics\n                    \"save_dir\": logdir,\n                },\n            },\n        }\n        default_logger_cfg = default_logger_cfgs[\"wandb\" if opt.wandb else \"csv\"]\n        if opt.wandb:\n            # TODO change once leaving \"swiffer\" config directory\n            try:\n                group_name = nowname.split(now)[-1].split(\"-\")[1]\n            except:\n                group_name = nowname\n            default_logger_cfg[\"params\"][\"group\"] = group_name\n            init_wandb(\n                os.path.join(os.getcwd(), logdir),\n                opt=opt,\n                group_name=group_name,\n                config=config,\n                name_str=nowname,\n            )\n        if \"logger\" in lightning_config:\n            logger_cfg = lightning_config.logger\n        else:\n            logger_cfg = OmegaConf.create()\n        logger_cfg = OmegaConf.merge(default_logger_cfg, logger_cfg)\n        trainer_kwargs[\"logger\"] = instantiate_from_config(logger_cfg)\n\n        # modelcheckpoint - use TrainResult/EvalResult(checkpoint_on=metric) to\n        # specify which metric is used to determine best models\n        default_modelckpt_cfg = {\n            \"target\": \"pytorch_lightning.callbacks.ModelCheckpoint\",\n            \"params\": {\n                \"dirpath\": ckptdir,\n                \"filename\": \"{epoch:06}\",\n                \"verbose\": True,\n                \"save_last\": True,\n            },\n        }\n        if hasattr(model, \"monitor\"):\n            print(f\"Monitoring {model.monitor} as checkpoint metric.\")\n            default_modelckpt_cfg[\"params\"][\"monitor\"] = model.monitor\n            default_modelckpt_cfg[\"params\"][\"save_top_k\"] = 3\n\n        if \"modelcheckpoint\" in lightning_config:\n            modelckpt_cfg = lightning_config.modelcheckpoint\n        else:\n            modelckpt_cfg = OmegaConf.create()\n        modelckpt_cfg = OmegaConf.merge(default_modelckpt_cfg, modelckpt_cfg)\n        print(f\"Merged modelckpt-cfg: \\n{modelckpt_cfg}\")\n\n        # https://pytorch-lightning.readthedocs.io/en/stable/extensions/strategy.html\n        # default to ddp if not further specified\n        default_strategy_config = {\"target\": \"pytorch_lightning.strategies.DDPStrategy\"}\n\n        if \"strategy\" in lightning_config:\n            strategy_cfg = lightning_config.strategy\n        else:\n            strategy_cfg = OmegaConf.create()\n            default_strategy_config[\"params\"] = {\n                \"find_unused_parameters\": False,\n                # \"static_graph\": True,\n                # \"ddp_comm_hook\": default.fp16_compress_hook  # TODO: experiment with this, also for DDPSharded\n            }\n        strategy_cfg = OmegaConf.merge(default_strategy_config, strategy_cfg)\n        print(\n            f\"strategy config: \\n ++++++++++++++ \\n {strategy_cfg} \\n ++++++++++++++ \"\n        )\n        trainer_kwargs[\"strategy\"] = instantiate_from_config(strategy_cfg)\n\n        # add callback which sets up log directory\n        default_callbacks_cfg = {\n            \"setup_callback\": {\n                \"target\": \"main.SetupCallback\",\n                \"params\": {\n                    \"resume\": opt.resume,\n                    \"now\": now,\n                    \"logdir\": logdir,\n                    \"ckptdir\": ckptdir,\n                    \"cfgdir\": cfgdir,\n                    \"config\": config,\n                    \"lightning_config\": lightning_config,\n                    \"debug\": opt.debug,\n                    \"ckpt_name\": melk_ckpt_name,\n                },\n            },\n            \"image_logger\": {\n                \"target\": \"main.ImageLogger\",\n                \"params\": {\"batch_frequency\": 1000, \"max_images\": 4, \"clamp\": True},\n            },\n            \"learning_rate_logger\": {\n                \"target\": \"pytorch_lightning.callbacks.LearningRateMonitor\",\n                \"params\": {\n                    \"logging_interval\": \"step\",\n                    # \"log_momentum\": True\n                },\n            },\n        }\n        if version.parse(pl.__version__) >= version.parse(\"1.4.0\"):\n            default_callbacks_cfg.update({\"checkpoint_callback\": modelckpt_cfg})\n\n        if \"callbacks\" in lightning_config:\n            callbacks_cfg = lightning_config.callbacks\n        else:\n            callbacks_cfg = OmegaConf.create()\n\n        if \"metrics_over_trainsteps_checkpoint\" in callbacks_cfg:\n            print(\n                \"Caution: Saving checkpoints every n train steps without deleting. This might require some free space.\"\n            )\n            default_metrics_over_trainsteps_ckpt_dict = {\n                \"metrics_over_trainsteps_checkpoint\": {\n                    \"target\": \"pytorch_lightning.callbacks.ModelCheckpoint\",\n                    \"params\": {\n                        \"dirpath\": os.path.join(ckptdir, \"trainstep_checkpoints\"),\n                        \"filename\": \"{epoch:06}-{step:09}\",\n                        \"verbose\": True,\n                        \"save_top_k\": -1,\n                        \"every_n_train_steps\": 10000,\n                        \"save_weights_only\": True,\n                    },\n                }\n            }\n            default_callbacks_cfg.update(default_metrics_over_trainsteps_ckpt_dict)\n\n        callbacks_cfg = OmegaConf.merge(default_callbacks_cfg, callbacks_cfg)\n        if \"ignore_keys_callback\" in callbacks_cfg and ckpt_resume_path is not None:\n            callbacks_cfg.ignore_keys_callback.params[\"ckpt_path\"] = ckpt_resume_path\n        elif \"ignore_keys_callback\" in callbacks_cfg:\n            del callbacks_cfg[\"ignore_keys_callback\"]\n\n        trainer_kwargs[\"callbacks\"] = [\n            instantiate_from_config(callbacks_cfg[k]) for k in callbacks_cfg\n        ]\n        if not \"plugins\" in trainer_kwargs:\n            trainer_kwargs[\"plugins\"] = list()\n\n        # cmd line trainer args (which are in trainer_opt) have always priority over config-trainer-args (which are in trainer_kwargs)\n        trainer_opt = vars(trainer_opt)\n        trainer_kwargs = {\n            key: val for key, val in trainer_kwargs.items() if key not in trainer_opt\n        }\n        trainer = Trainer(**trainer_opt, **trainer_kwargs)\n\n        trainer.logdir = logdir  ###\n\n        # data\n        data = instantiate_from_config(config.data)\n        # NOTE according to https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html\n        # calling these ourselves should not be necessary but it is.\n        # lightning still takes care of proper multiprocessing though\n        data.prepare_data()\n        # data.setup()\n        print(\"#### Data #####\")\n        try:\n            for k in data.datasets:\n                print(\n                    f\"{k}, {data.datasets[k].__class__.__name__}, {len(data.datasets[k])}\"\n                )\n        except:\n            print(\"datasets not yet initialized.\")\n\n        # configure learning rate\n        if \"batch_size\" in config.data.params:\n            bs, base_lr = config.data.params.batch_size, config.model.base_learning_rate\n        else:\n            bs, base_lr = (\n                config.data.params.train.loader.batch_size,\n                config.model.base_learning_rate,\n            )\n        if not cpu:\n            ngpu = len(lightning_config.trainer.devices.strip(\",\").split(\",\"))\n        else:\n            ngpu = 1\n        if \"accumulate_grad_batches\" in lightning_config.trainer:\n            accumulate_grad_batches = lightning_config.trainer.accumulate_grad_batches\n        else:\n            accumulate_grad_batches = 1\n        print(f\"accumulate_grad_batches = {accumulate_grad_batches}\")\n        lightning_config.trainer.accumulate_grad_batches = accumulate_grad_batches\n        if opt.scale_lr:\n            model.learning_rate = accumulate_grad_batches * ngpu * bs * base_lr\n            print(\n                \"Setting learning rate to {:.2e} = {} (accumulate_grad_batches) * {} (num_gpus) * {} (batchsize) * {:.2e} (base_lr)\".format(\n                    model.learning_rate, accumulate_grad_batches, ngpu, bs, base_lr\n                )\n            )\n        else:\n            model.learning_rate = base_lr\n            print(\"++++ NOT USING LR SCALING ++++\")\n            print(f\"Setting learning rate to {model.learning_rate:.2e}\")\n\n        # allow checkpointing via USR1\n        def melk(*args, **kwargs):\n            # run all checkpoint hooks\n            if trainer.global_rank == 0:\n                print(\"Summoning checkpoint.\")\n                if melk_ckpt_name is None:\n                    ckpt_path = os.path.join(ckptdir, \"last.ckpt\")\n                else:\n                    ckpt_path = os.path.join(ckptdir, melk_ckpt_name)\n                trainer.save_checkpoint(ckpt_path)\n\n        def divein(*args, **kwargs):\n            if trainer.global_rank == 0:\n                import pudb\n\n                pudb.set_trace()\n\n        import signal\n\n        signal.signal(signal.SIGUSR1, melk)\n        signal.signal(signal.SIGUSR2, divein)\n\n        # run\n        if opt.train:\n            try:\n                trainer.fit(model, data, ckpt_path=ckpt_resume_path)\n            except Exception:\n                if not opt.debug:\n                    melk()\n                raise\n        if not opt.no_test and not trainer.interrupted:\n            trainer.test(model, data)\n    except RuntimeError as err:\n        if MULTINODE_HACKS:\n            import datetime\n            import os\n            import socket\n\n            import requests\n\n            device = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"?\")\n            hostname = socket.gethostname()\n            ts = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n            resp = requests.get(\"http://169.254.169.254/latest/meta-data/instance-id\")\n            print(\n                f\"ERROR at {ts} on {hostname}/{resp.text} (CUDA_VISIBLE_DEVICES={device}): {type(err).__name__}: {err}\",\n                flush=True,\n            )\n        raise err\n    except Exception:\n        if opt.debug and trainer.global_rank == 0:\n            try:\n                import pudb as debugger\n            except ImportError:\n                import pdb as debugger\n            debugger.post_mortem()\n        raise\n    finally:\n        # move newly created debug project to debug_runs\n        if opt.debug and not opt.resume and trainer.global_rank == 0:\n            dst, name = os.path.split(logdir)\n            dst = os.path.join(dst, \"debug_runs\", name)\n            os.makedirs(os.path.split(dst)[0], exist_ok=True)\n            os.rename(logdir, dst)\n\n        if opt.wandb:\n            wandb.finish()\n        # if trainer.global_rank == 0:\n        #    print(trainer.profiler.summary())\n"
        },
        {
          "name": "model_licenses",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.25390625,
          "content": "[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"sgm\"\ndynamic = [\"version\"]\ndescription = \"Stability Generative Models\"\nreadme = \"README.md\"\nlicense-files = { paths = [\"LICENSE-CODE\"] }\nrequires-python = \">=3.8\"\n\n[project.urls]\nHomepage = \"https://github.com/Stability-AI/generative-models\"\n\n[tool.hatch.version]\npath = \"sgm/__init__.py\"\n\n[tool.hatch.build]\n# This needs to be explicitly set so the configuration files\n# grafted into the `sgm` directory get included in the wheel's\n# RECORD file.\ninclude = [\n    \"sgm\",\n]\n# The force-include configurations below make Hatch copy\n# the configs/ directory (containing the various YAML files required\n# to generatively model) into the source distribution and the wheel.\n\n[tool.hatch.build.targets.sdist.force-include]\n\"./configs\" = \"sgm/configs\"\n\n[tool.hatch.build.targets.wheel.force-include]\n\"./configs\" = \"sgm/configs\"\n\n[tool.hatch.envs.ci]\nskip-install = false\n\ndependencies = [\n    \"pytest\"\n]\n\n[tool.hatch.envs.ci.scripts]\ntest-inference = [\n    \"pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118\",\n    \"pip install -r requirements/pt2.txt\",    \n    \"pytest -v tests/inference/test_inference.py {args}\",\n]\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.08984375,
          "content": "[pytest]\nmarkers = \n  inference: mark as inference test (deselect with '-m \"not inference\"')"
        },
        {
          "name": "requirements",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "sgm",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}