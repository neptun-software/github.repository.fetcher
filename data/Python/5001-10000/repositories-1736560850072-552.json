{
  "metadata": {
    "timestamp": 1736560850072,
    "page": 552,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "taki0112/UGATIT",
      "stars": 6168,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".DS_Store",
          "type": "blob",
          "size": 6.00390625,
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1748046875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.041015625,
          "content": "MIT License\n\nCopyright (c) 2019 Junho Kim\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.392578125,
          "content": "## U-GAT-IT &mdash; Official TensorFlow Implementation (ICLR 2020)\n### : Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation\n\n<div align=\"center\">\n  <img src=\"./assets/teaser.png\">\n</div>\n\n### [Paper](https://arxiv.org/abs/1907.10830) | [Official Pytorch code](https://github.com/znxlwm/UGATIT-pytorch)\nThis repository provides the **official Tensorflow implementation** of the following paper:\n\n> **U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation**<br>\n> **Junho Kim (NCSOFT)**, Minjae Kim (NCSOFT), Hyeonwoo Kang (NCSOFT), Kwanghee Lee (Boeing Korea)\n>\n> **Abstract** *We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based methods which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters.*\n\n## Requirements\n* python == 3.6\n* tensorflow == 1.14\n\n## Pretrained model\n> We released 50 epoch and 100 epoch checkpoints so that people could test more widely.\n* [selfie2anime checkpoint (50 epoch)](https://drive.google.com/file/d/1V6GbSItG3HZKv3quYs7AP0rr1kOCT3QO/view?usp=sharing)\n* [selfie2anime checkpoint (100 epoch)](https://drive.google.com/file/d/19xQK2onIy-3S5W5K-XIh85pAg_RNvBVf/view?usp=sharing)\n\n## Dataset\n* [selfie2anime dataset](https://drive.google.com/file/d/1xOWj1UVgp6NKMT3HbPhBbtq2A4EDkghF/view?usp=sharing)\n\n## Web page\n* [Selfie2Anime](https://selfie2anime.com) by [Nathan Glover](https://github.com/t04glovern)\n* [Selfie2Waifu](https://waifu.lofiu.com) by [creke](https://github.com/creke)\n\n## Telegram Bot\n* [Selfie2AnimeBot](https://t.me/selfie2animebot) by [Alex Spirin](https://github.com/sxela)\n\n## Usage\n```\n├── dataset\n   └── YOUR_DATASET_NAME\n       ├── trainA\n           ├── xxx.jpg (name, format doesn't matter)\n           ├── yyy.png\n           └── ...\n       ├── trainB\n           ├── zzz.jpg\n           ├── www.png\n           └── ...\n       ├── testA\n           ├── aaa.jpg \n           ├── bbb.png\n           └── ...\n       └── testB\n           ├── ccc.jpg \n           ├── ddd.png\n           └── ...\n```\n\n### Train\n```\n> python main.py --dataset selfie2anime\n```\n* If the memory of gpu is **not sufficient**, set `--light` to **True**\n  * But it may **not** perform well\n  * paper version is `--light` to **False**\n\n### Test\n```\n> python main.py --dataset selfie2anime --phase test\n```\n\n## Architecture\n<div align=\"center\">\n  <img src = './assets/generator_fix.png' width = '785px' height = '500px'>\n</div>\n\n---\n\n<div align=\"center\">\n  <img src = './assets/discriminator_fix.png' width = '785px' height = '450px'>\n</div>\n\n## Results\n### Ablation study\n<div align=\"center\">\n  <img src = './assets/ablation.png' width = '438px' height = '346px'>\n</div>\n\n### User study\n<div align=\"center\">\n  <img src = './assets/user_study.png' width = '738px' height = '187px'>\n</div>\n\n### Kernel Inception Distance (KID)\n<div align=\"center\">\n  <img src = './assets/kid_fix2.png' width = '750px' height = '400px'>\n</div>\n\n## Citation\nIf you find this code useful for your research, please cite our paper:\n\n```\n@inproceedings{\nKim2020U-GAT-IT:,\ntitle={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},\nauthor={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlZ5ySKPH}\n}\n```\n\n## Author\n[Junho Kim](http://bit.ly/jhkim_ai), Minjae Kim, Hyeonwoo Kang, Kwanghee Lee\n"
        },
        {
          "name": "UGATIT.py",
          "type": "blob",
          "size": 29.2607421875,
          "content": "from ops import *\r\nfrom utils import *\r\nfrom glob import glob\r\nimport time\r\nfrom tensorflow.contrib.data import prefetch_to_device, shuffle_and_repeat, map_and_batch\r\nimport numpy as np\r\n\r\nclass UGATIT(object) :\r\n    def __init__(self, sess, args):\r\n        self.light = args.light\r\n\r\n        if self.light :\r\n            self.model_name = 'UGATIT_light'\r\n        else :\r\n            self.model_name = 'UGATIT'\r\n\r\n        self.sess = sess\r\n        self.phase = args.phase\r\n        self.checkpoint_dir = args.checkpoint_dir\r\n        self.result_dir = args.result_dir\r\n        self.log_dir = args.log_dir\r\n        self.dataset_name = args.dataset\r\n        self.augment_flag = args.augment_flag\r\n\r\n        self.epoch = args.epoch\r\n        self.iteration = args.iteration\r\n        self.decay_flag = args.decay_flag\r\n        self.decay_epoch = args.decay_epoch\r\n\r\n        self.gan_type = args.gan_type\r\n\r\n        self.batch_size = args.batch_size\r\n        self.print_freq = args.print_freq\r\n        self.save_freq = args.save_freq\r\n\r\n        self.init_lr = args.lr\r\n        self.ch = args.ch\r\n\r\n        \"\"\" Weight \"\"\"\r\n        self.adv_weight = args.adv_weight\r\n        self.cycle_weight = args.cycle_weight\r\n        self.identity_weight = args.identity_weight\r\n        self.cam_weight = args.cam_weight\r\n        self.ld = args.GP_ld\r\n        self.smoothing = args.smoothing\r\n\r\n        \"\"\" Generator \"\"\"\r\n        self.n_res = args.n_res\r\n\r\n        \"\"\" Discriminator \"\"\"\r\n        self.n_dis = args.n_dis\r\n        self.n_critic = args.n_critic\r\n        self.sn = args.sn\r\n\r\n        self.img_size = args.img_size\r\n        self.img_ch = args.img_ch\r\n\r\n\r\n        self.sample_dir = os.path.join(args.sample_dir, self.model_dir)\r\n        check_folder(self.sample_dir)\r\n\r\n        # self.trainA, self.trainB = prepare_data(dataset_name=self.dataset_name, size=self.img_size\r\n        self.trainA_dataset = glob('./dataset/{}/*.*'.format(self.dataset_name + '/trainA'))\r\n        self.trainB_dataset = glob('./dataset/{}/*.*'.format(self.dataset_name + '/trainB'))\r\n        self.dataset_num = max(len(self.trainA_dataset), len(self.trainB_dataset))\r\n\r\n        print()\r\n\r\n        print(\"##### Information #####\")\r\n        print(\"# light : \", self.light)\r\n        print(\"# gan type : \", self.gan_type)\r\n        print(\"# dataset : \", self.dataset_name)\r\n        print(\"# max dataset number : \", self.dataset_num)\r\n        print(\"# batch_size : \", self.batch_size)\r\n        print(\"# epoch : \", self.epoch)\r\n        print(\"# iteration per epoch : \", self.iteration)\r\n        print(\"# smoothing : \", self.smoothing)\r\n\r\n        print()\r\n\r\n        print(\"##### Generator #####\")\r\n        print(\"# residual blocks : \", self.n_res)\r\n\r\n        print()\r\n\r\n        print(\"##### Discriminator #####\")\r\n        print(\"# discriminator layer : \", self.n_dis)\r\n        print(\"# the number of critic : \", self.n_critic)\r\n        print(\"# spectral normalization : \", self.sn)\r\n\r\n        print()\r\n\r\n        print(\"##### Weight #####\")\r\n        print(\"# adv_weight : \", self.adv_weight)\r\n        print(\"# cycle_weight : \", self.cycle_weight)\r\n        print(\"# identity_weight : \", self.identity_weight)\r\n        print(\"# cam_weight : \", self.cam_weight)\r\n\r\n    ##################################################################################\r\n    # Generator\r\n    ##################################################################################\r\n\r\n    def generator(self, x_init, reuse=False, scope=\"generator\"):\r\n        channel = self.ch\r\n        with tf.variable_scope(scope, reuse=reuse) :\r\n            x = conv(x_init, channel, kernel=7, stride=1, pad=3, pad_type='reflect', scope='conv')\r\n            x = instance_norm(x, scope='ins_norm')\r\n            x = relu(x)\r\n\r\n            # Down-Sampling\r\n            for i in range(2) :\r\n                x = conv(x, channel*2, kernel=3, stride=2, pad=1, pad_type='reflect', scope='conv_'+str(i))\r\n                x = instance_norm(x, scope='ins_norm_'+str(i))\r\n                x = relu(x)\r\n\r\n                channel = channel * 2\r\n\r\n            # Down-Sampling Bottleneck\r\n            for i in range(self.n_res):\r\n                x = resblock(x, channel, scope='resblock_' + str(i))\r\n\r\n\r\n            # Class Activation Map\r\n            cam_x = global_avg_pooling(x)\r\n            cam_gap_logit, cam_x_weight = fully_connected_with_w(cam_x, scope='CAM_logit')\r\n            x_gap = tf.multiply(x, cam_x_weight)\r\n\r\n            cam_x = global_max_pooling(x)\r\n            cam_gmp_logit, cam_x_weight = fully_connected_with_w(cam_x, reuse=True, scope='CAM_logit')\r\n            x_gmp = tf.multiply(x, cam_x_weight)\r\n\r\n\r\n            cam_logit = tf.concat([cam_gap_logit, cam_gmp_logit], axis=-1)\r\n            x = tf.concat([x_gap, x_gmp], axis=-1)\r\n\r\n            x = conv(x, channel, kernel=1, stride=1, scope='conv_1x1')\r\n            x = relu(x)\r\n\r\n            heatmap = tf.squeeze(tf.reduce_sum(x, axis=-1))\r\n\r\n            # Gamma, Beta block\r\n            gamma, beta = self.MLP(x, reuse=reuse)\r\n\r\n            # Up-Sampling Bottleneck\r\n            for i in range(self.n_res):\r\n                x = adaptive_ins_layer_resblock(x, channel, gamma, beta, smoothing=self.smoothing, scope='adaptive_resblock' + str(i))\r\n\r\n            # Up-Sampling\r\n            for i in range(2) :\r\n                x = up_sample(x, scale_factor=2)\r\n                x = conv(x, channel//2, kernel=3, stride=1, pad=1, pad_type='reflect', scope='up_conv_'+str(i))\r\n                x = layer_instance_norm(x, scope='layer_ins_norm_'+str(i))\r\n                x = relu(x)\r\n\r\n                channel = channel // 2\r\n\r\n\r\n            x = conv(x, channels=3, kernel=7, stride=1, pad=3, pad_type='reflect', scope='G_logit')\r\n            x = tanh(x)\r\n\r\n            return x, cam_logit, heatmap\r\n\r\n    def MLP(self, x, use_bias=True, reuse=False, scope='MLP'):\r\n        channel = self.ch * self.n_res\r\n\r\n        if self.light :\r\n            x = global_avg_pooling(x)\r\n\r\n        with tf.variable_scope(scope, reuse=reuse):\r\n            for i in range(2) :\r\n                x = fully_connected(x, channel, use_bias, scope='linear_' + str(i))\r\n                x = relu(x)\r\n\r\n\r\n            gamma = fully_connected(x, channel, use_bias, scope='gamma')\r\n            beta = fully_connected(x, channel, use_bias, scope='beta')\r\n\r\n            gamma = tf.reshape(gamma, shape=[self.batch_size, 1, 1, channel])\r\n            beta = tf.reshape(beta, shape=[self.batch_size, 1, 1, channel])\r\n\r\n            return gamma, beta\r\n\r\n    ##################################################################################\r\n    # Discriminator\r\n    ##################################################################################\r\n\r\n    def discriminator(self, x_init, reuse=False, scope=\"discriminator\"):\r\n        D_logit = []\r\n        D_CAM_logit = []\r\n        with tf.variable_scope(scope, reuse=reuse) :\r\n            local_x, local_cam, local_heatmap = self.discriminator_local(x_init, reuse=reuse, scope='local')\r\n            global_x, global_cam, global_heatmap = self.discriminator_global(x_init, reuse=reuse, scope='global')\r\n\r\n            D_logit.extend([local_x, global_x])\r\n            D_CAM_logit.extend([local_cam, global_cam])\r\n\r\n            return D_logit, D_CAM_logit, local_heatmap, global_heatmap\r\n\r\n    def discriminator_global(self, x_init, reuse=False, scope='discriminator_global'):\r\n        with tf.variable_scope(scope, reuse=reuse):\r\n            channel = self.ch\r\n            x = conv(x_init, channel, kernel=4, stride=2, pad=1, pad_type='reflect', sn=self.sn, scope='conv_0')\r\n            x = lrelu(x, 0.2)\r\n\r\n            for i in range(1, self.n_dis - 1):\r\n                x = conv(x, channel * 2, kernel=4, stride=2, pad=1, pad_type='reflect', sn=self.sn, scope='conv_' + str(i))\r\n                x = lrelu(x, 0.2)\r\n\r\n                channel = channel * 2\r\n\r\n            x = conv(x, channel * 2, kernel=4, stride=1, pad=1, pad_type='reflect', sn=self.sn, scope='conv_last')\r\n            x = lrelu(x, 0.2)\r\n\r\n            channel = channel * 2\r\n\r\n            cam_x = global_avg_pooling(x)\r\n            cam_gap_logit, cam_x_weight = fully_connected_with_w(cam_x, sn=self.sn, scope='CAM_logit')\r\n            x_gap = tf.multiply(x, cam_x_weight)\r\n\r\n            cam_x = global_max_pooling(x)\r\n            cam_gmp_logit, cam_x_weight = fully_connected_with_w(cam_x, sn=self.sn, reuse=True, scope='CAM_logit')\r\n            x_gmp = tf.multiply(x, cam_x_weight)\r\n\r\n            cam_logit = tf.concat([cam_gap_logit, cam_gmp_logit], axis=-1)\r\n            x = tf.concat([x_gap, x_gmp], axis=-1)\r\n\r\n            x = conv(x, channel, kernel=1, stride=1, scope='conv_1x1')\r\n            x = lrelu(x, 0.2)\r\n\r\n            heatmap = tf.squeeze(tf.reduce_sum(x, axis=-1))\r\n\r\n\r\n            x = conv(x, channels=1, kernel=4, stride=1, pad=1, pad_type='reflect', sn=self.sn, scope='D_logit')\r\n\r\n            return x, cam_logit, heatmap\r\n\r\n    def discriminator_local(self, x_init, reuse=False, scope='discriminator_local'):\r\n        with tf.variable_scope(scope, reuse=reuse) :\r\n            channel = self.ch\r\n            x = conv(x_init, channel, kernel=4, stride=2, pad=1, pad_type='reflect', sn=self.sn, scope='conv_0')\r\n            x = lrelu(x, 0.2)\r\n\r\n            for i in range(1, self.n_dis - 2 - 1):\r\n                x = conv(x, channel * 2, kernel=4, stride=2, pad=1, pad_type='reflect', sn=self.sn, scope='conv_' + str(i))\r\n                x = lrelu(x, 0.2)\r\n\r\n                channel = channel * 2\r\n\r\n            x = conv(x, channel * 2, kernel=4, stride=1, pad=1, pad_type='reflect', sn=self.sn, scope='conv_last')\r\n            x = lrelu(x, 0.2)\r\n\r\n            channel = channel * 2\r\n\r\n            cam_x = global_avg_pooling(x)\r\n            cam_gap_logit, cam_x_weight = fully_connected_with_w(cam_x, sn=self.sn, scope='CAM_logit')\r\n            x_gap = tf.multiply(x, cam_x_weight)\r\n\r\n            cam_x = global_max_pooling(x)\r\n            cam_gmp_logit, cam_x_weight = fully_connected_with_w(cam_x, sn=self.sn, reuse=True, scope='CAM_logit')\r\n            x_gmp = tf.multiply(x, cam_x_weight)\r\n\r\n            cam_logit = tf.concat([cam_gap_logit, cam_gmp_logit], axis=-1)\r\n            x = tf.concat([x_gap, x_gmp], axis=-1)\r\n\r\n            x = conv(x, channel, kernel=1, stride=1, scope='conv_1x1')\r\n            x = lrelu(x, 0.2)\r\n\r\n            heatmap = tf.squeeze(tf.reduce_sum(x, axis=-1))\r\n\r\n            x = conv(x, channels=1, kernel=4, stride=1, pad=1, pad_type='reflect', sn=self.sn, scope='D_logit')\r\n\r\n            return x, cam_logit, heatmap\r\n\r\n    ##################################################################################\r\n    # Model\r\n    ##################################################################################\r\n\r\n    def generate_a2b(self, x_A, reuse=False):\r\n        out, cam, _ = self.generator(x_A, reuse=reuse, scope=\"generator_B\")\r\n\r\n        return out, cam\r\n\r\n    def generate_b2a(self, x_B, reuse=False):\r\n        out, cam, _ = self.generator(x_B, reuse=reuse, scope=\"generator_A\")\r\n\r\n        return out, cam\r\n\r\n    def discriminate_real(self, x_A, x_B):\r\n        real_A_logit, real_A_cam_logit, _, _ = self.discriminator(x_A, scope=\"discriminator_A\")\r\n        real_B_logit, real_B_cam_logit, _, _ = self.discriminator(x_B, scope=\"discriminator_B\")\r\n\r\n        return real_A_logit, real_A_cam_logit, real_B_logit, real_B_cam_logit\r\n\r\n    def discriminate_fake(self, x_ba, x_ab):\r\n        fake_A_logit, fake_A_cam_logit, _, _ = self.discriminator(x_ba, reuse=True, scope=\"discriminator_A\")\r\n        fake_B_logit, fake_B_cam_logit, _, _ = self.discriminator(x_ab, reuse=True, scope=\"discriminator_B\")\r\n\r\n        return fake_A_logit, fake_A_cam_logit, fake_B_logit, fake_B_cam_logit\r\n\r\n    def gradient_panalty(self, real, fake, scope=\"discriminator_A\"):\r\n        if self.gan_type.__contains__('dragan'):\r\n            eps = tf.random_uniform(shape=tf.shape(real), minval=0., maxval=1.)\r\n            _, x_var = tf.nn.moments(real, axes=[0, 1, 2, 3])\r\n            x_std = tf.sqrt(x_var)  # magnitude of noise decides the size of local region\r\n\r\n            fake = real + 0.5 * x_std * eps\r\n\r\n        alpha = tf.random_uniform(shape=[self.batch_size, 1, 1, 1], minval=0., maxval=1.)\r\n        interpolated = real + alpha * (fake - real)\r\n\r\n        logit, cam_logit, _, _ = self.discriminator(interpolated, reuse=True, scope=scope)\r\n\r\n\r\n        GP = []\r\n        cam_GP = []\r\n\r\n        for i in range(2) :\r\n            grad = tf.gradients(logit[i], interpolated)[0] # gradient of D(interpolated)\r\n            grad_norm = tf.norm(flatten(grad), axis=1) # l2 norm\r\n\r\n            # WGAN - LP\r\n            if self.gan_type == 'wgan-lp' :\r\n                GP.append(self.ld * tf.reduce_mean(tf.square(tf.maximum(0.0, grad_norm - 1.))))\r\n\r\n            elif self.gan_type == 'wgan-gp' or self.gan_type == 'dragan':\r\n                GP.append(self.ld * tf.reduce_mean(tf.square(grad_norm - 1.)))\r\n\r\n        for i in range(2) :\r\n            grad = tf.gradients(cam_logit[i], interpolated)[0] # gradient of D(interpolated)\r\n            grad_norm = tf.norm(flatten(grad), axis=1) # l2 norm\r\n\r\n            # WGAN - LP\r\n            if self.gan_type == 'wgan-lp' :\r\n                cam_GP.append(self.ld * tf.reduce_mean(tf.square(tf.maximum(0.0, grad_norm - 1.))))\r\n\r\n            elif self.gan_type == 'wgan-gp' or self.gan_type == 'dragan':\r\n                cam_GP.append(self.ld * tf.reduce_mean(tf.square(grad_norm - 1.)))\r\n\r\n\r\n        return sum(GP), sum(cam_GP)\r\n\r\n    def build_model(self):\r\n        if self.phase == 'train' :\r\n            self.lr = tf.placeholder(tf.float32, name='learning_rate')\r\n\r\n\r\n            \"\"\" Input Image\"\"\"\r\n            Image_Data_Class = ImageData(self.img_size, self.img_ch, self.augment_flag)\r\n\r\n            trainA = tf.data.Dataset.from_tensor_slices(self.trainA_dataset)\r\n            trainB = tf.data.Dataset.from_tensor_slices(self.trainB_dataset)\r\n\r\n\r\n            gpu_device = '/gpu:0'\r\n            trainA = trainA.apply(shuffle_and_repeat(self.dataset_num)).apply(map_and_batch(Image_Data_Class.image_processing, self.batch_size, num_parallel_batches=16, drop_remainder=True)).apply(prefetch_to_device(gpu_device, None))\r\n            trainB = trainB.apply(shuffle_and_repeat(self.dataset_num)).apply(map_and_batch(Image_Data_Class.image_processing, self.batch_size, num_parallel_batches=16, drop_remainder=True)).apply(prefetch_to_device(gpu_device, None))\r\n\r\n\r\n            trainA_iterator = trainA.make_one_shot_iterator()\r\n            trainB_iterator = trainB.make_one_shot_iterator()\r\n\r\n            self.domain_A = trainA_iterator.get_next()\r\n            self.domain_B = trainB_iterator.get_next()\r\n\r\n            \"\"\" Define Generator, Discriminator \"\"\"\r\n            x_ab, cam_ab = self.generate_a2b(self.domain_A) # real a\r\n            x_ba, cam_ba = self.generate_b2a(self.domain_B) # real b\r\n\r\n            x_aba, _ = self.generate_b2a(x_ab, reuse=True) # real b\r\n            x_bab, _ = self.generate_a2b(x_ba, reuse=True) # real a\r\n\r\n            x_aa, cam_aa = self.generate_b2a(self.domain_A, reuse=True) # fake b\r\n            x_bb, cam_bb = self.generate_a2b(self.domain_B, reuse=True) # fake a\r\n\r\n            real_A_logit, real_A_cam_logit, real_B_logit, real_B_cam_logit = self.discriminate_real(self.domain_A, self.domain_B)\r\n            fake_A_logit, fake_A_cam_logit, fake_B_logit, fake_B_cam_logit = self.discriminate_fake(x_ba, x_ab)\r\n\r\n\r\n            \"\"\" Define Loss \"\"\"\r\n            if self.gan_type.__contains__('wgan') or self.gan_type == 'dragan' :\r\n                GP_A, GP_CAM_A = self.gradient_panalty(real=self.domain_A, fake=x_ba, scope=\"discriminator_A\")\r\n                GP_B, GP_CAM_B = self.gradient_panalty(real=self.domain_B, fake=x_ab, scope=\"discriminator_B\")\r\n            else :\r\n                GP_A, GP_CAM_A  = 0, 0\r\n                GP_B, GP_CAM_B = 0, 0\r\n\r\n            G_ad_loss_A = (generator_loss(self.gan_type, fake_A_logit) + generator_loss(self.gan_type, fake_A_cam_logit))\r\n            G_ad_loss_B = (generator_loss(self.gan_type, fake_B_logit) + generator_loss(self.gan_type, fake_B_cam_logit))\r\n\r\n            D_ad_loss_A = (discriminator_loss(self.gan_type, real_A_logit, fake_A_logit) + discriminator_loss(self.gan_type, real_A_cam_logit, fake_A_cam_logit) + GP_A + GP_CAM_A)\r\n            D_ad_loss_B = (discriminator_loss(self.gan_type, real_B_logit, fake_B_logit) + discriminator_loss(self.gan_type, real_B_cam_logit, fake_B_cam_logit) + GP_B + GP_CAM_B)\r\n\r\n            reconstruction_A = L1_loss(x_aba, self.domain_A) # reconstruction\r\n            reconstruction_B = L1_loss(x_bab, self.domain_B) # reconstruction\r\n\r\n            identity_A = L1_loss(x_aa, self.domain_A)\r\n            identity_B = L1_loss(x_bb, self.domain_B)\r\n\r\n            cam_A = cam_loss(source=cam_ba, non_source=cam_aa)\r\n            cam_B = cam_loss(source=cam_ab, non_source=cam_bb)\r\n\r\n            Generator_A_gan = self.adv_weight * G_ad_loss_A\r\n            Generator_A_cycle = self.cycle_weight * reconstruction_B\r\n            Generator_A_identity = self.identity_weight * identity_A\r\n            Generator_A_cam = self.cam_weight * cam_A\r\n\r\n\r\n            Generator_B_gan = self.adv_weight * G_ad_loss_B\r\n            Generator_B_cycle = self.cycle_weight * reconstruction_A\r\n            Generator_B_identity = self.identity_weight * identity_B\r\n            Generator_B_cam = self.cam_weight * cam_B\r\n\r\n\r\n            Generator_A_loss = Generator_A_gan + Generator_A_cycle + Generator_A_identity + Generator_A_cam\r\n            Generator_B_loss = Generator_B_gan + Generator_B_cycle + Generator_B_identity + Generator_B_cam\r\n\r\n\r\n            Discriminator_A_loss = self.adv_weight * D_ad_loss_A\r\n            Discriminator_B_loss = self.adv_weight * D_ad_loss_B\r\n\r\n            self.Generator_loss = Generator_A_loss + Generator_B_loss + regularization_loss('generator')\r\n            self.Discriminator_loss = Discriminator_A_loss + Discriminator_B_loss + regularization_loss('discriminator')\r\n\r\n\r\n            \"\"\" Result Image \"\"\"\r\n            self.fake_A = x_ba\r\n            self.fake_B = x_ab\r\n\r\n            self.real_A = self.domain_A\r\n            self.real_B = self.domain_B\r\n\r\n\r\n            \"\"\" Training \"\"\"\r\n            t_vars = tf.trainable_variables()\r\n            G_vars = [var for var in t_vars if 'generator' in var.name]\r\n            D_vars = [var for var in t_vars if 'discriminator' in var.name]\r\n\r\n            self.G_optim = tf.train.AdamOptimizer(self.lr, beta1=0.5, beta2=0.999).minimize(self.Generator_loss, var_list=G_vars)\r\n            self.D_optim = tf.train.AdamOptimizer(self.lr, beta1=0.5, beta2=0.999).minimize(self.Discriminator_loss, var_list=D_vars)\r\n\r\n\r\n            \"\"\"\" Summary \"\"\"\r\n            self.all_G_loss = tf.summary.scalar(\"Generator_loss\", self.Generator_loss)\r\n            self.all_D_loss = tf.summary.scalar(\"Discriminator_loss\", self.Discriminator_loss)\r\n\r\n            self.G_A_loss = tf.summary.scalar(\"G_A_loss\", Generator_A_loss)\r\n            self.G_A_gan = tf.summary.scalar(\"G_A_gan\", Generator_A_gan)\r\n            self.G_A_cycle = tf.summary.scalar(\"G_A_cycle\", Generator_A_cycle)\r\n            self.G_A_identity = tf.summary.scalar(\"G_A_identity\", Generator_A_identity)\r\n            self.G_A_cam = tf.summary.scalar(\"G_A_cam\", Generator_A_cam)\r\n\r\n            self.G_B_loss = tf.summary.scalar(\"G_B_loss\", Generator_B_loss)\r\n            self.G_B_gan = tf.summary.scalar(\"G_B_gan\", Generator_B_gan)\r\n            self.G_B_cycle = tf.summary.scalar(\"G_B_cycle\", Generator_B_cycle)\r\n            self.G_B_identity = tf.summary.scalar(\"G_B_identity\", Generator_B_identity)\r\n            self.G_B_cam = tf.summary.scalar(\"G_B_cam\", Generator_B_cam)\r\n\r\n            self.D_A_loss = tf.summary.scalar(\"D_A_loss\", Discriminator_A_loss)\r\n            self.D_B_loss = tf.summary.scalar(\"D_B_loss\", Discriminator_B_loss)\r\n\r\n            self.rho_var = []\r\n            for var in tf.trainable_variables():\r\n                if 'rho' in var.name:\r\n                    self.rho_var.append(tf.summary.histogram(var.name, var))\r\n                    self.rho_var.append(tf.summary.scalar(var.name + \"_min\", tf.reduce_min(var)))\r\n                    self.rho_var.append(tf.summary.scalar(var.name + \"_max\", tf.reduce_max(var)))\r\n                    self.rho_var.append(tf.summary.scalar(var.name + \"_mean\", tf.reduce_mean(var)))\r\n\r\n            g_summary_list = [self.G_A_loss, self.G_A_gan, self.G_A_cycle, self.G_A_identity, self.G_A_cam,\r\n                              self.G_B_loss, self.G_B_gan, self.G_B_cycle, self.G_B_identity, self.G_B_cam,\r\n                              self.all_G_loss]\r\n\r\n            g_summary_list.extend(self.rho_var)\r\n            d_summary_list = [self.D_A_loss, self.D_B_loss, self.all_D_loss]\r\n\r\n            self.G_loss = tf.summary.merge(g_summary_list)\r\n            self.D_loss = tf.summary.merge(d_summary_list)\r\n\r\n        else :\r\n            \"\"\" Test \"\"\"\r\n            self.test_domain_A = tf.placeholder(tf.float32, [1, self.img_size, self.img_size, self.img_ch], name='test_domain_A')\r\n            self.test_domain_B = tf.placeholder(tf.float32, [1, self.img_size, self.img_size, self.img_ch], name='test_domain_B')\r\n\r\n\r\n            self.test_fake_B, _ = self.generate_a2b(self.test_domain_A)\r\n            self.test_fake_A, _ = self.generate_b2a(self.test_domain_B)\r\n\r\n\r\n    def train(self):\r\n        # initialize all variables\r\n        tf.global_variables_initializer().run()\r\n\r\n        # saver to save model\r\n        self.saver = tf.train.Saver()\r\n\r\n        # summary writer\r\n        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_dir, self.sess.graph)\r\n\r\n\r\n        # restore check-point if it exits\r\n        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\r\n        if could_load:\r\n            start_epoch = (int)(checkpoint_counter / self.iteration)\r\n            start_batch_id = checkpoint_counter - start_epoch * self.iteration\r\n            counter = checkpoint_counter\r\n            print(\" [*] Load SUCCESS\")\r\n        else:\r\n            start_epoch = 0\r\n            start_batch_id = 0\r\n            counter = 1\r\n            print(\" [!] Load failed...\")\r\n\r\n        # loop for epoch\r\n        start_time = time.time()\r\n        past_g_loss = -1.\r\n        lr = self.init_lr\r\n        for epoch in range(start_epoch, self.epoch):\r\n            # lr = self.init_lr if epoch < self.decay_epoch else self.init_lr * (self.epoch - epoch) / (self.epoch - self.decay_epoch)\r\n            if self.decay_flag :\r\n                #lr = self.init_lr * pow(0.5, epoch // self.decay_epoch)\r\n                lr = self.init_lr if epoch < self.decay_epoch else self.init_lr * (self.epoch - epoch) / (self.epoch - self.decay_epoch)\r\n            for idx in range(start_batch_id, self.iteration):\r\n                train_feed_dict = {\r\n                    self.lr : lr\r\n                }\r\n\r\n                # Update D\r\n                _, d_loss, summary_str = self.sess.run([self.D_optim,\r\n                                                        self.Discriminator_loss, self.D_loss], feed_dict = train_feed_dict)\r\n                self.writer.add_summary(summary_str, counter)\r\n\r\n                # Update G\r\n                g_loss = None\r\n                if (counter - 1) % self.n_critic == 0 :\r\n                    batch_A_images, batch_B_images, fake_A, fake_B, _, g_loss, summary_str = self.sess.run([self.real_A, self.real_B,\r\n                                                                                                            self.fake_A, self.fake_B,\r\n                                                                                                            self.G_optim,\r\n                                                                                                            self.Generator_loss, self.G_loss], feed_dict = train_feed_dict)\r\n                    self.writer.add_summary(summary_str, counter)\r\n                    past_g_loss = g_loss\r\n\r\n                # display training status\r\n                counter += 1\r\n                if g_loss == None :\r\n                    g_loss = past_g_loss\r\n                print(\"Epoch: [%2d] [%5d/%5d] time: %4.4f d_loss: %.8f, g_loss: %.8f\" % (epoch, idx, self.iteration, time.time() - start_time, d_loss, g_loss))\r\n\r\n                if np.mod(idx+1, self.print_freq) == 0 :\r\n                    save_images(batch_A_images, [self.batch_size, 1],\r\n                                './{}/real_A_{:03d}_{:05d}.png'.format(self.sample_dir, epoch, idx+1))\r\n                    # save_images(batch_B_images, [self.batch_size, 1],\r\n                    #             './{}/real_B_{:03d}_{:05d}.png'.format(self.sample_dir, epoch, idx+1))\r\n\r\n                    # save_images(fake_A, [self.batch_size, 1],\r\n                    #             './{}/fake_A_{:03d}_{:05d}.png'.format(self.sample_dir, epoch, idx+1))\r\n                    save_images(fake_B, [self.batch_size, 1],\r\n                                './{}/fake_B_{:03d}_{:05d}.png'.format(self.sample_dir, epoch, idx+1))\r\n\r\n                if np.mod(idx + 1, self.save_freq) == 0:\r\n                    self.save(self.checkpoint_dir, counter)\r\n\r\n\r\n\r\n            # After an epoch, start_batch_id is set to zero\r\n            # non-zero value is only for the first epoch after loading pre-trained model\r\n            start_batch_id = 0\r\n\r\n            # save model for final step\r\n            self.save(self.checkpoint_dir, counter)\r\n\r\n    @property\r\n    def model_dir(self):\r\n        n_res = str(self.n_res) + 'resblock'\r\n        n_dis = str(self.n_dis) + 'dis'\r\n\r\n        if self.smoothing :\r\n            smoothing = '_smoothing'\r\n        else :\r\n            smoothing = ''\r\n\r\n        if self.sn :\r\n            sn = '_sn'\r\n        else :\r\n            sn = ''\r\n\r\n        return \"{}_{}_{}_{}_{}_{}_{}_{}_{}_{}{}{}\".format(self.model_name, self.dataset_name,\r\n                                                         self.gan_type, n_res, n_dis,\r\n                                                         self.n_critic,\r\n                                                         self.adv_weight, self.cycle_weight, self.identity_weight, self.cam_weight, sn, smoothing)\r\n\r\n    def save(self, checkpoint_dir, step):\r\n        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\r\n\r\n        if not os.path.exists(checkpoint_dir):\r\n            os.makedirs(checkpoint_dir)\r\n\r\n        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name + '.model'), global_step=step)\r\n\r\n    def load(self, checkpoint_dir):\r\n        print(\" [*] Reading checkpoints...\")\r\n        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\r\n\r\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\r\n        if ckpt and ckpt.model_checkpoint_path:\r\n            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\r\n            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\r\n            counter = int(ckpt_name.split('-')[-1])\r\n            print(\" [*] Success to read {}\".format(ckpt_name))\r\n            return True, counter\r\n        else:\r\n            print(\" [*] Failed to find a checkpoint\")\r\n            return False, 0\r\n\r\n    def test(self):\r\n        tf.global_variables_initializer().run()\r\n        test_A_files = glob('./dataset/{}/*.*'.format(self.dataset_name + '/testA'))\r\n        test_B_files = glob('./dataset/{}/*.*'.format(self.dataset_name + '/testB'))\r\n\r\n        self.saver = tf.train.Saver()\r\n        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\r\n        self.result_dir = os.path.join(self.result_dir, self.model_dir)\r\n        check_folder(self.result_dir)\r\n\r\n        if could_load :\r\n            print(\" [*] Load SUCCESS\")\r\n        else :\r\n            print(\" [!] Load failed...\")\r\n\r\n        # write html for visual comparison\r\n        index_path = os.path.join(self.result_dir, 'index.html')\r\n        index = open(index_path, 'w')\r\n        index.write(\"<html><body><table><tr>\")\r\n        index.write(\"<th>name</th><th>input</th><th>output</th></tr>\")\r\n\r\n        for sample_file  in test_A_files : # A -> B\r\n            print('Processing A image: ' + sample_file)\r\n            sample_image = np.asarray(load_test_data(sample_file, size=self.img_size))\r\n            image_path = os.path.join(self.result_dir,'{0}'.format(os.path.basename(sample_file)))\r\n\r\n            fake_img = self.sess.run(self.test_fake_B, feed_dict = {self.test_domain_A : sample_image})\r\n            save_images(fake_img, [1, 1], image_path)\r\n\r\n            index.write(\"<td>%s</td>\" % os.path.basename(image_path))\r\n\r\n            index.write(\"<td><img src='%s' width='%d' height='%d'></td>\" % (sample_file if os.path.isabs(sample_file) else (\r\n                '../..' + os.path.sep + sample_file), self.img_size, self.img_size))\r\n            index.write(\"<td><img src='%s' width='%d' height='%d'></td>\" % (image_path if os.path.isabs(image_path) else (\r\n                '../..' + os.path.sep + image_path), self.img_size, self.img_size))\r\n            index.write(\"</tr>\")\r\n\r\n        for sample_file  in test_B_files : # B -> A\r\n            print('Processing B image: ' + sample_file)\r\n            sample_image = np.asarray(load_test_data(sample_file, size=self.img_size))\r\n            image_path = os.path.join(self.result_dir,'{0}'.format(os.path.basename(sample_file)))\r\n\r\n            fake_img = self.sess.run(self.test_fake_A, feed_dict = {self.test_domain_B : sample_image})\r\n\r\n            save_images(fake_img, [1, 1], image_path)\r\n            index.write(\"<td>%s</td>\" % os.path.basename(image_path))\r\n            index.write(\"<td><img src='%s' width='%d' height='%d'></td>\" % (sample_file if os.path.isabs(sample_file) else (\r\n                    '../..' + os.path.sep + sample_file), self.img_size, self.img_size))\r\n            index.write(\"<td><img src='%s' width='%d' height='%d'></td>\" % (image_path if os.path.isabs(image_path) else (\r\n                    '../..' + os.path.sep + image_path), self.img_size, self.img_size))\r\n            index.write(\"</tr>\")\r\n        index.close()\r\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 4.369140625,
          "content": "from UGATIT import UGATIT\nimport argparse\nfrom utils import *\n\n\"\"\"parsing and configuration\"\"\"\n\ndef parse_args():\n    desc = \"Tensorflow implementation of U-GAT-IT\"\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument('--phase', type=str, default='train', help='[train / test]')\n    parser.add_argument('--light', type=str2bool, default=False, help='[U-GAT-IT full version / U-GAT-IT light version]')\n    parser.add_argument('--dataset', type=str, default='selfie2anime', help='dataset_name')\n\n    parser.add_argument('--epoch', type=int, default=100, help='The number of epochs to run')\n    parser.add_argument('--iteration', type=int, default=10000, help='The number of training iterations')\n    parser.add_argument('--batch_size', type=int, default=1, help='The size of batch size')\n    parser.add_argument('--print_freq', type=int, default=1000, help='The number of image_print_freq')\n    parser.add_argument('--save_freq', type=int, default=1000, help='The number of ckpt_save_freq')\n    parser.add_argument('--decay_flag', type=str2bool, default=True, help='The decay_flag')\n    parser.add_argument('--decay_epoch', type=int, default=50, help='decay epoch')\n\n    parser.add_argument('--lr', type=float, default=0.0001, help='The learning rate')\n    parser.add_argument('--GP_ld', type=int, default=10, help='The gradient penalty lambda')\n    parser.add_argument('--adv_weight', type=int, default=1, help='Weight about GAN')\n    parser.add_argument('--cycle_weight', type=int, default=10, help='Weight about Cycle')\n    parser.add_argument('--identity_weight', type=int, default=10, help='Weight about Identity')\n    parser.add_argument('--cam_weight', type=int, default=1000, help='Weight about CAM')\n    parser.add_argument('--gan_type', type=str, default='lsgan', help='[gan / lsgan / wgan-gp / wgan-lp / dragan / hinge]')\n\n    parser.add_argument('--smoothing', type=str2bool, default=True, help='AdaLIN smoothing effect')\n\n    parser.add_argument('--ch', type=int, default=64, help='base channel number per layer')\n    parser.add_argument('--n_res', type=int, default=4, help='The number of resblock')\n    parser.add_argument('--n_dis', type=int, default=6, help='The number of discriminator layer')\n    parser.add_argument('--n_critic', type=int, default=1, help='The number of critic')\n    parser.add_argument('--sn', type=str2bool, default=True, help='using spectral norm')\n\n    parser.add_argument('--img_size', type=int, default=256, help='The size of image')\n    parser.add_argument('--img_ch', type=int, default=3, help='The size of image channel')\n    parser.add_argument('--augment_flag', type=str2bool, default=True, help='Image augmentation use or not')\n\n    parser.add_argument('--checkpoint_dir', type=str, default='checkpoint',\n                        help='Directory name to save the checkpoints')\n    parser.add_argument('--result_dir', type=str, default='results',\n                        help='Directory name to save the generated images')\n    parser.add_argument('--log_dir', type=str, default='logs',\n                        help='Directory name to save training logs')\n    parser.add_argument('--sample_dir', type=str, default='samples',\n                        help='Directory name to save the samples on training')\n\n    return check_args(parser.parse_args())\n\n\"\"\"checking arguments\"\"\"\ndef check_args(args):\n    # --checkpoint_dir\n    check_folder(args.checkpoint_dir)\n\n    # --result_dir\n    check_folder(args.result_dir)\n\n    # --result_dir\n    check_folder(args.log_dir)\n\n    # --sample_dir\n    check_folder(args.sample_dir)\n\n    # --epoch\n    try:\n        assert args.epoch >= 1\n    except:\n        print('number of epochs must be larger than or equal to one')\n\n    # --batch_size\n    try:\n        assert args.batch_size >= 1\n    except:\n        print('batch size must be larger than or equal to one')\n    return args\n\n\"\"\"main\"\"\"\ndef main():\n    # parse arguments\n    args = parse_args()\n    if args is None:\n      exit()\n\n    # open session\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n        gan = UGATIT(sess, args)\n\n        # build graph\n        gan.build_model()\n\n        # show network architecture\n        show_all_variables()\n\n        if args.phase == 'train' :\n            gan.train()\n            print(\" [*] Training finished!\")\n\n        if args.phase == 'test' :\n            gan.test()\n            print(\" [*] Test finished!\")\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "ops.py",
          "type": "blob",
          "size": 11.7861328125,
          "content": "import tensorflow as tf\nimport tensorflow.contrib as tf_contrib\n\n# Xavier : tf_contrib.layers.xavier_initializer()\n# He : tf_contrib.layers.variance_scaling_initializer()\n# Normal : tf.random_normal_initializer(mean=0.0, stddev=0.02)\n# l2_decay : tf_contrib.layers.l2_regularizer(0.0001)\n\nweight_init = tf.random_normal_initializer(mean=0.0, stddev=0.02)\nweight_regularizer = tf_contrib.layers.l2_regularizer(scale=0.0001)\n\n##################################################################################\n# Layer\n##################################################################################\n\ndef conv(x, channels, kernel=4, stride=2, pad=0, pad_type='zero', use_bias=True, sn=False, scope='conv_0'):\n    with tf.variable_scope(scope):\n        if pad > 0 :\n            if (kernel - stride) % 2 == 0:\n                pad_top = pad\n                pad_bottom = pad\n                pad_left = pad\n                pad_right = pad\n\n            else:\n                pad_top = pad\n                pad_bottom = kernel - stride - pad_top\n                pad_left = pad\n                pad_right = kernel - stride - pad_left\n\n            if pad_type == 'zero':\n                x = tf.pad(x, [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]])\n            if pad_type == 'reflect':\n                x = tf.pad(x, [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]], mode='REFLECT')\n\n        if sn :\n            w = tf.get_variable(\"kernel\", shape=[kernel, kernel, x.get_shape()[-1], channels], initializer=weight_init,\n                                regularizer=weight_regularizer)\n            x = tf.nn.conv2d(input=x, filter=spectral_norm(w),\n                             strides=[1, stride, stride, 1], padding='VALID')\n            if use_bias :\n                bias = tf.get_variable(\"bias\", [channels], initializer=tf.constant_initializer(0.0))\n                x = tf.nn.bias_add(x, bias)\n\n        else :\n            x = tf.layers.conv2d(inputs=x, filters=channels,\n                                 kernel_size=kernel, kernel_initializer=weight_init,\n                                 kernel_regularizer=weight_regularizer,\n                                 strides=stride, use_bias=use_bias)\n\n\n        return x\n\ndef fully_connected_with_w(x, use_bias=True, sn=False, reuse=False, scope='linear'):\n    with tf.variable_scope(scope, reuse=reuse):\n        x = flatten(x)\n        bias = 0.0\n        shape = x.get_shape().as_list()\n        channels = shape[-1]\n\n        w = tf.get_variable(\"kernel\", [channels, 1], tf.float32,\n                            initializer=weight_init, regularizer=weight_regularizer)\n\n        if sn :\n            w = spectral_norm(w)\n\n        if use_bias :\n            bias = tf.get_variable(\"bias\", [1],\n                                   initializer=tf.constant_initializer(0.0))\n\n            x = tf.matmul(x, w) + bias\n        else :\n            x = tf.matmul(x, w)\n\n        if use_bias :\n            weights = tf.gather(tf.transpose(tf.nn.bias_add(w, bias)), 0)\n        else :\n            weights = tf.gather(tf.transpose(w), 0)\n\n        return x, weights\n\ndef fully_connected(x, units, use_bias=True, sn=False, scope='linear'):\n    with tf.variable_scope(scope):\n        x = flatten(x)\n        shape = x.get_shape().as_list()\n        channels = shape[-1]\n\n        if sn:\n            w = tf.get_variable(\"kernel\", [channels, units], tf.float32,\n                                initializer=weight_init, regularizer=weight_regularizer)\n            if use_bias:\n                bias = tf.get_variable(\"bias\", [units],\n                                       initializer=tf.constant_initializer(0.0))\n\n                x = tf.matmul(x, spectral_norm(w)) + bias\n            else:\n                x = tf.matmul(x, spectral_norm(w))\n\n        else :\n            x = tf.layers.dense(x, units=units, kernel_initializer=weight_init, kernel_regularizer=weight_regularizer, use_bias=use_bias)\n\n        return x\n\ndef flatten(x) :\n    return tf.layers.flatten(x)\n\n##################################################################################\n# Residual-block\n##################################################################################\n\ndef resblock(x_init, channels, use_bias=True, scope='resblock_0'):\n    with tf.variable_scope(scope):\n        with tf.variable_scope('res1'):\n            x = conv(x_init, channels, kernel=3, stride=1, pad=1, pad_type='reflect', use_bias=use_bias)\n            x = instance_norm(x)\n            x = relu(x)\n\n        with tf.variable_scope('res2'):\n            x = conv(x, channels, kernel=3, stride=1, pad=1, pad_type='reflect', use_bias=use_bias)\n            x = instance_norm(x)\n\n        return x + x_init\n\ndef adaptive_ins_layer_resblock(x_init, channels, gamma, beta, use_bias=True, smoothing=True, scope='adaptive_resblock') :\n    with tf.variable_scope(scope):\n        with tf.variable_scope('res1'):\n            x = conv(x_init, channels, kernel=3, stride=1, pad=1, pad_type='reflect', use_bias=use_bias)\n            x = adaptive_instance_layer_norm(x, gamma, beta, smoothing)\n            x = relu(x)\n\n        with tf.variable_scope('res2'):\n            x = conv(x, channels, kernel=3, stride=1, pad=1, pad_type='reflect', use_bias=use_bias)\n            x = adaptive_instance_layer_norm(x, gamma, beta, smoothing)\n\n        return x + x_init\n\n\n##################################################################################\n# Sampling\n##################################################################################\n\ndef up_sample(x, scale_factor=2):\n    _, h, w, _ = x.get_shape().as_list()\n    new_size = [h * scale_factor, w * scale_factor]\n    return tf.image.resize_nearest_neighbor(x, size=new_size)\n\n\ndef global_avg_pooling(x):\n    gap = tf.reduce_mean(x, axis=[1, 2])\n    return gap\n\ndef global_max_pooling(x):\n    gmp = tf.reduce_max(x, axis=[1, 2])\n    return gmp\n\n##################################################################################\n# Activation function\n##################################################################################\n\ndef lrelu(x, alpha=0.01):\n    # pytorch alpha is 0.01\n    return tf.nn.leaky_relu(x, alpha)\n\n\ndef relu(x):\n    return tf.nn.relu(x)\n\n\ndef tanh(x):\n    return tf.tanh(x)\n\ndef sigmoid(x) :\n    return tf.sigmoid(x)\n\n##################################################################################\n# Normalization function\n##################################################################################\n\ndef adaptive_instance_layer_norm(x, gamma, beta, smoothing=True, scope='instance_layer_norm') :\n    with tf.variable_scope(scope):\n        ch = x.shape[-1]\n        eps = 1e-5\n\n        ins_mean, ins_sigma = tf.nn.moments(x, axes=[1, 2], keep_dims=True)\n        x_ins = (x - ins_mean) / (tf.sqrt(ins_sigma + eps))\n\n        ln_mean, ln_sigma = tf.nn.moments(x, axes=[1, 2, 3], keep_dims=True)\n        x_ln = (x - ln_mean) / (tf.sqrt(ln_sigma + eps))\n\n        rho = tf.get_variable(\"rho\", [ch], initializer=tf.constant_initializer(1.0), constraint=lambda x: tf.clip_by_value(x, clip_value_min=0.0, clip_value_max=1.0))\n\n        if smoothing :\n            rho = tf.clip_by_value(rho - tf.constant(0.1), 0.0, 1.0)\n\n        x_hat = rho * x_ins + (1 - rho) * x_ln\n\n\n        x_hat = x_hat * gamma + beta\n\n        return x_hat\n\ndef instance_norm(x, scope='instance_norm'):\n    return tf_contrib.layers.instance_norm(x,\n                                           epsilon=1e-05,\n                                           center=True, scale=True,\n                                           scope=scope)\n\ndef layer_norm(x, scope='layer_norm') :\n    return tf_contrib.layers.layer_norm(x,\n                                        center=True, scale=True,\n                                        scope=scope)\n\ndef layer_instance_norm(x, scope='layer_instance_norm') :\n    with tf.variable_scope(scope):\n        ch = x.shape[-1]\n        eps = 1e-5\n\n        ins_mean, ins_sigma = tf.nn.moments(x, axes=[1, 2], keep_dims=True)\n        x_ins = (x - ins_mean) / (tf.sqrt(ins_sigma + eps))\n\n        ln_mean, ln_sigma = tf.nn.moments(x, axes=[1, 2, 3], keep_dims=True)\n        x_ln = (x - ln_mean) / (tf.sqrt(ln_sigma + eps))\n\n        rho = tf.get_variable(\"rho\", [ch], initializer=tf.constant_initializer(0.0), constraint=lambda x: tf.clip_by_value(x, clip_value_min=0.0, clip_value_max=1.0))\n\n        gamma = tf.get_variable(\"gamma\", [ch], initializer=tf.constant_initializer(1.0))\n        beta = tf.get_variable(\"beta\", [ch], initializer=tf.constant_initializer(0.0))\n\n        x_hat = rho * x_ins + (1 - rho) * x_ln\n\n        x_hat = x_hat * gamma + beta\n\n        return x_hat\n\ndef spectral_norm(w, iteration=1):\n    w_shape = w.shape.as_list()\n    w = tf.reshape(w, [-1, w_shape[-1]])\n\n    u = tf.get_variable(\"u\", [1, w_shape[-1]], initializer=tf.random_normal_initializer(), trainable=False)\n\n    u_hat = u\n    v_hat = None\n    for i in range(iteration):\n        \"\"\"\n        power iteration\n        Usually iteration = 1 will be enough\n        \"\"\"\n        v_ = tf.matmul(u_hat, tf.transpose(w))\n        v_hat = tf.nn.l2_normalize(v_)\n\n        u_ = tf.matmul(v_hat, w)\n        u_hat = tf.nn.l2_normalize(u_)\n\n    u_hat = tf.stop_gradient(u_hat)\n    v_hat = tf.stop_gradient(v_hat)\n\n    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))\n\n    with tf.control_dependencies([u.assign(u_hat)]):\n        w_norm = w / sigma\n        w_norm = tf.reshape(w_norm, w_shape)\n\n\n    return w_norm\n\n##################################################################################\n# Loss function\n##################################################################################\n\ndef L1_loss(x, y):\n    loss = tf.reduce_mean(tf.abs(x - y))\n\n    return loss\n\ndef cam_loss(source, non_source) :\n\n    identity_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(source), logits=source))\n    non_identity_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(non_source), logits=non_source))\n\n    loss = identity_loss + non_identity_loss\n\n    return loss\n\ndef regularization_loss(scope_name) :\n    \"\"\"\n    If you want to use \"Regularization\"\n    g_loss += regularization_loss('generator')\n    d_loss += regularization_loss('discriminator')\n    \"\"\"\n    collection_regularization = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n\n    loss = []\n    for item in collection_regularization :\n        if scope_name in item.name :\n            loss.append(item)\n\n    return tf.reduce_sum(loss)\n\n\ndef discriminator_loss(loss_func, real, fake):\n    loss = []\n    real_loss = 0\n    fake_loss = 0\n\n    for i in range(2) :\n        if loss_func.__contains__('wgan') :\n            real_loss = -tf.reduce_mean(real[i])\n            fake_loss = tf.reduce_mean(fake[i])\n\n        if loss_func == 'lsgan' :\n            real_loss = tf.reduce_mean(tf.squared_difference(real[i], 1.0))\n            fake_loss = tf.reduce_mean(tf.square(fake[i]))\n\n        if loss_func == 'gan' or loss_func == 'dragan' :\n            real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(real[i]), logits=real[i]))\n            fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(fake[i]), logits=fake[i]))\n\n        if loss_func == 'hinge' :\n            real_loss = tf.reduce_mean(relu(1.0 - real[i]))\n            fake_loss = tf.reduce_mean(relu(1.0 + fake[i]))\n\n        loss.append(real_loss + fake_loss)\n\n    return sum(loss)\n\ndef generator_loss(loss_func, fake):\n    loss = []\n    fake_loss = 0\n\n    for i in range(2) :\n        if loss_func.__contains__('wgan') :\n            fake_loss = -tf.reduce_mean(fake[i])\n\n        if loss_func == 'lsgan' :\n            fake_loss = tf.reduce_mean(tf.squared_difference(fake[i], 1.0))\n\n        if loss_func == 'gan' or loss_func == 'dragan' :\n            fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(fake[i]), logits=fake[i]))\n\n        if loss_func == 'hinge' :\n            fake_loss = -tf.reduce_mean(fake[i])\n\n        loss.append(fake_loss)\n\n    return sum(loss)"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 2.2939453125,
          "content": "import tensorflow as tf\nfrom tensorflow.contrib import slim\nimport cv2\nimport os, random\nimport numpy as np\n\nclass ImageData:\n\n    def __init__(self, load_size, channels, augment_flag):\n        self.load_size = load_size\n        self.channels = channels\n        self.augment_flag = augment_flag\n\n    def image_processing(self, filename):\n        x = tf.read_file(filename)\n        x_decode = tf.image.decode_jpeg(x, channels=self.channels)\n        img = tf.image.resize_images(x_decode, [self.load_size, self.load_size])\n        img = tf.cast(img, tf.float32) / 127.5 - 1\n\n        if self.augment_flag :\n            augment_size = self.load_size + (30 if self.load_size == 256 else 15)\n            p = random.random()\n            if p > 0.5:\n                img = augmentation(img, augment_size)\n\n        return img\n\ndef load_test_data(image_path, size=256):\n    img = cv2.imread(image_path, flags=cv2.IMREAD_COLOR)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    img = cv2.resize(img, dsize=(size, size))\n\n    img = np.expand_dims(img, axis=0)\n    img = img/127.5 - 1\n\n    return img\n\ndef augmentation(image, augment_size):\n    seed = random.randint(0, 2 ** 31 - 1)\n    ori_image_shape = tf.shape(image)\n    image = tf.image.random_flip_left_right(image, seed=seed)\n    image = tf.image.resize_images(image, [augment_size, augment_size])\n    image = tf.random_crop(image, ori_image_shape, seed=seed)\n    return image\n\ndef save_images(images, size, image_path):\n    return imsave(inverse_transform(images), size, image_path)\n\ndef inverse_transform(images):\n    return ((images+1.) / 2) * 255.0\n\n\ndef imsave(images, size, path):\n    images = merge(images, size)\n    images = cv2.cvtColor(images.astype('uint8'), cv2.COLOR_RGB2BGR)\n\n    return cv2.imwrite(path, images)\n\ndef merge(images, size):\n    h, w = images.shape[1], images.shape[2]\n    img = np.zeros((h * size[0], w * size[1], 3))\n    for idx, image in enumerate(images):\n        i = idx % size[1]\n        j = idx // size[1]\n        img[h*j:h*(j+1), w*i:w*(i+1), :] = image\n\n    return img\n\ndef show_all_variables():\n    model_vars = tf.trainable_variables()\n    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n\ndef check_folder(log_dir):\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    return log_dir\n\ndef str2bool(x):\n    return x.lower() in ('true')\n"
        }
      ]
    }
  ]
}