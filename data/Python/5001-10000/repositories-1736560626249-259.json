{
  "metadata": {
    "timestamp": 1736560626249,
    "page": 259,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lucidrains/PaLM-rlhf-pytorch",
      "stars": 7746,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.7568359375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.041015625,
          "content": "MIT License\n\nCopyright (c) 2022 Phil Wang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.6435546875,
          "content": "<img src=\"./chatgpt.png\" width=\"450px\"></img>\n\n*<a href=\"https://openai.com/blog/chatgpt/\">official chatgpt blogpost</a>*\n\n## PaLM + RLHF - Pytorch (wip)\n\nImplementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Maybe I'll add retrieval functionality too, Ã  la <a href=\"https://github.com/lucidrains/RETRO-pytorch\">RETRO</a>\n\nIf you are interested in replicating something like ChatGPT out in the open, please consider joining <a href=\"https://discord.gg/xBPBXfcFHd\">Laion <img alt=\"Join us on Discord\" src=\"https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&logoColor=white\"></a>\n\nPotential successor: <a href=\"https://arxiv.org/abs/2305.18290\">Direct Preference Optimization</a> - all the code in this repo becomes ~ binary cross entropy loss, < 5 loc. So much for Reward models and PPO\n\n## FAQ\n\n- Does this contain a model for inference?\n\nThere is no trained model. This is just the ship and overall map. We still need millions of dollars of compute + data to sail to the correct point in high dimensional parameter space. Even then, you need professional sailors (like Robin Rombach of Stable Diffusion fame) to actually guide the ship through turbulent times to that point.\n\n## Community\n\n<a href=\"https://carper.ai/\">CarperAI</a> had been working on <a href=\"https://github.com/CarperAI/trlx\">an RLHF framework</a> for large language models for many months prior to the release of ChatGPT.\n\n<a href=\"https://www.youtube.com/watch?v=sswA4j_IUxg\">Yannic Kilcher</a> is also working on an <a href=\"https://github.com/LAION-AI/Open-Assistant\">open sourced implementation</a>\n\n<a href=\"https://www.youtube.com/watch?v=SWwQ3k-DWyo\">AI Coffeebreak w/ Letitia</a> | <a href=\"https://www.youtube.com/watch?v=NpmnWgQgcsA\">Code Emporium</a> | <a href=\"https://www.youtube.com/watch?v=_MPJ3CyDokU\">Code Emporium Part 2</a>\n\n## Appreciation\n\n- <a href=\"https://stability.ai/\">Stability.ai</a> for the generous sponsorship to work on cutting edge artificial intelligence research\n\n- <a href=\"https://huggingface.co/\">ðŸ¤— Hugging Face</a> and <a href=\"https://carper.ai/\">CarperAI</a> for penning the blog post <a href=\"https://huggingface.co/blog/rlhf\">Illustrating Reinforcement Learning from Human Feedback (RLHF)</a>, and the former also for their <a href=\"https://huggingface.co/docs/accelerate/index\">accelerate</a> library\n\n- <a href=\"https://github.com/kisseternity\">@kisseternity</a> and <a href=\"https://github.com/taynoel84\">@taynoel84</a> for the code review and finding bugs\n\n- <a href=\"https://github.com/conceptofmind\">Enrico</a> for integrating <a href=\"https://arxiv.org/abs/2205.14135\">Flash Attention</a> from Pytorch 2.0\n\n## Install\n\n```bash\n$ pip install palm-rlhf-pytorch\n```\n\n## Usage\n\nFirst train `PaLM`, like any other autoregressive transformer\n\n```python\nimport torch\nfrom palm_rlhf_pytorch import PaLM\n\npalm = PaLM(\n    num_tokens = 20000,\n    dim = 512,\n    depth = 12,\n    flash_attn = True # https://arxiv.org/abs/2205.14135\n).cuda()\n\nseq = torch.randint(0, 20000, (1, 2048)).cuda()\n\nloss = palm(seq, return_loss = True)\nloss.backward()\n\n# after much training, you can now generate sequences\n\ngenerated = palm.generate(2048) # (1, 2048)\n```\n\nThen train your reward model, with the curated human feedback. In the original paper, they could not get reward model to be finetuned from a pretrained transformer without overfitting, but I gave the option to finetune with `LoRA` anyways, since it is still open research.\n\n```python\nimport torch\nfrom palm_rlhf_pytorch import PaLM, RewardModel\n\npalm = PaLM(\n    num_tokens = 20000,\n    dim = 512,\n    depth = 12,\n    causal = False\n)\n\nreward_model = RewardModel(\n    palm,\n    num_binned_output = 5 # say rating from 1 to 5\n).cuda()\n\n# mock data\n\nseq = torch.randint(0, 20000, (1, 1024)).cuda()\nprompt_mask = torch.zeros(1, 1024).bool().cuda() # which part of the sequence is prompt, which part is response\nlabels = torch.randint(0, 5, (1,)).cuda()\n\n# train\n\nloss = reward_model(seq, prompt_mask = prompt_mask, labels = labels)\nloss.backward()\n\n# after much training\n\nreward = reward_model(seq, prompt_mask = prompt_mask)\n```\n\nThen you will pass your transformer and the rewards model to the `RLHFTrainer`\n\n```python\nimport torch\nfrom palm_rlhf_pytorch import PaLM, RewardModel, RLHFTrainer\n\n# load your pretrained palm\n\npalm = PaLM(\n    num_tokens = 20000,\n    dim = 512,\n    depth = 12\n).cuda()\n\npalm.load('./path/to/pretrained/palm.pt')\n\n# load your pretrained reward model\n\nreward_model = RewardModel(\n    palm,\n    num_binned_output = 5\n).cuda()\n\nreward_model.load('./path/to/pretrained/reward_model.pt')\n\n# ready your list of prompts for reinforcement learning\n\nprompts = torch.randint(0, 256, (50000, 512)).cuda() # 50k prompts\n\n# pass it all to the trainer and train\n\ntrainer = RLHFTrainer(\n    palm = palm,\n    reward_model = reward_model,\n    prompt_token_ids = prompts\n)\n\ntrainer.train(num_episodes = 50000)\n\n# then, if it succeeded...\n# generate say 10 samples and use the reward model to return the best one\n\nanswer = trainer.generate(2048, prompt = prompts[0], num_samples = 10) # (<= 2048,)\n```\n\n## Todo\n\n- [x] clone base transformer with separate lora for critic\n- [x] also allow for non-LoRA based finetuning\n- [x] redo normalize to be able to have a masked version, not sure if anyone will ever use per token rewards / values, but good practice to implement\n- [x] equip with <a href=\"https://github.com/hazyResearch/flash-attention\">the best attention</a>\n\n- [ ] add Hugging Face accelerate and test out wandb instrumentation\n- [ ] search literature to figure out what is the latest SOTA for PPO, assuming RL field is still making progress.\n- [ ] test the system using a pretrained sentiment network as reward model\n- [ ] write the memory in PPO to memmapped numpy file\n- [ ] get sampling with variable lengthed prompts working, even if it is not needed given bottleneck is human feedback\n- [ ] allow for finetuning penultimate N layers only in either actor or critic, assuming if pretrained\n- [ ] incorporate some learning points from Sparrow, given Letitia's video\n- [ ] simple web interface with django + htmx for collecting human feedback\n- [ ] consider <a href=\"https://www.anthropic.com/constitutional.pdf\">RLAIF</a>\n\n## Citations\n\n```bibtex\n@article{Stiennon2020LearningTS,\n    title   = {Learning to summarize from human feedback},\n    author  = {Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan J. Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},\n    journal = {ArXiv},\n    year    = {2020},\n    volume  = {abs/2009.01325}\n}\n```\n\n```bibtex\n@inproceedings{Chowdhery2022PaLMSL,\n    title   = {PaLM: Scaling Language Modeling with Pathways},\n    author  = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Benton C. Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garc{\\'i}a and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Oliveira Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathleen S. Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},\n    year    = {2022}\n}\n```\n\n```bibtex\n@article{Hu2021LoRALA,\n    title   = {LoRA: Low-Rank Adaptation of Large Language Models},\n    author  = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},\n    journal = {ArXiv},\n    year    = {2021},\n    volume  = {abs/2106.09685}\n}\n```\n\n```bibtex\n@inproceedings{Sun2022ALT,\n    title     = {A Length-Extrapolatable Transformer},\n    author    = {Yutao Sun and Li Dong and Barun Patra and Shuming Ma and Shaohan Huang and Alon Benhaim and Vishrav Chaudhary and Xia Song and Furu Wei},\n    year      = {2022}\n}\n```\n\n```bibtex\n@misc{gilmer2023intriguing\n    title  = {Intriguing Properties of Transformer Training Instabilities},\n    author = {Justin Gilmer, Andrea Schioppa, and Jeremy Cohen},\n    year   = {2023},\n    status = {to be published - one attention stabilization technique is circulating within Google Brain, being used by multiple teams}\n}\n```\n\n```bibtex\n@inproceedings{dao2022flashattention,\n    title   = {Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},\n    author  = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\\'e}, Christopher},\n    booktitle = {Advances in Neural Information Processing Systems},\n    year    = {2022}\n}\n```\n\n```bibtex\n@misc{Rubin2024,\n    author  = {Ohad Rubin},\n    url     = {https://medium.com/@ohadrubin/exploring-weight-decay-in-layer-normalization-challenges-and-a-reparameterization-solution-ad4d12c24950}\n}\n```\n\n```bibtex\n@inproceedings{Yuan2024FreePR,\n    title   = {Free Process Rewards without Process Labels},\n    author  = {Lifan Yuan and Wendi Li and Huayu Chen and Ganqu Cui and Ning Ding and Kaiyan Zhang and Bowen Zhou and Zhiyuan Liu and Hao Peng},\n    year    = {2024},\n    url     = {https://api.semanticscholar.org/CorpusID:274445748}\n}\n```\n"
        },
        {
          "name": "chatgpt.png",
          "type": "blob",
          "size": 82.6318359375,
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples.py",
          "type": "blob",
          "size": 1.5498046875,
          "content": "import torch\nfrom palm_rlhf_pytorch import PaLM, RewardModel, RLHFTrainer\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\ndevice = accelerator.device\n\n# load your pretrained palm\n\npalm = PaLM(\n    num_tokens = 20000,\n    dim = 512,\n    depth = 12\n).to(device)\n\n\n# load your pretrained reward model\n\nreward_model = RewardModel(\n    palm,\n    num_binned_output = 5\n).to(device)\n\n# Train you reward model on mock data :\n# mock data\n\nseq = torch.randint(0, 20000, (1, 1024)).to(device)\nprompt_mask = torch.zeros(1, 1024).bool().to(device) # which part of the sequence is prompt, which part is response\nlabels = torch.randint(0, 5, (1,)).to(device)\n\n# train\nloss = reward_model(seq, prompt_mask = prompt_mask, labels = labels)\naccelerator.backward(loss)\n\n# after much training\nreward = reward_model(seq, prompt_mask = prompt_mask)\n\n\n# ready your list of prompts for reinforcement learning\n\nprompts = torch.randint(0, 256, (1, 512)).to(device) # 1 prompt\n\n# pass it all to the trainer and train\n\ntrainer = RLHFTrainer(\n    palm = palm,\n    reward_model = reward_model,\n    prompt_token_ids = prompts\n)\n\naccelerator.print(\"Training\")\ntrainer.train(\n    num_episodes = 1,\n    max_timesteps = 1,\n    update_timesteps = 1,\n    max_batch_size = 256,\n    max_seq_len = 2048,\n    eos_token = None,\n    temperature = 1.\n)\n\n# then, if it succeeded...\n# generate say 10 samples and use the reward model to return the best one\naccelerator.print(\"Generating answer\")\nanswer = trainer.generate(2048, prompt = prompts[0], num_samples = 10) # (<= 2048,)\naccelerator.print(f\"answer: {answer}\")"
        },
        {
          "name": "palm_rlhf_pytorch",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.9716796875,
          "content": "from setuptools import setup, find_packages\n\nsetup(\n  name = 'PaLM-rlhf-pytorch',\n  packages = find_packages(exclude=[]),\n  version = '0.3.9',\n  license='MIT',\n  description = 'PaLM + Reinforcement Learning with Human Feedback - Pytorch',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  long_description_content_type = 'text/markdown',\n  url = 'https://github.com/lucidrains/PaLM-rlhf-pytorch',\n  keywords = [\n    'artificial intelligence',\n    'deep learning',\n    'transformers',\n    'attention mechanism',\n    'reinforcement learning',\n    'human feedback'\n  ],\n  install_requires=[\n    'accelerate',\n    'adam-atan2-pytorch',\n    'beartype',\n    'einops>=0.8',\n    'lion-pytorch',\n    'torch>=2.2',\n    'tqdm'\n  ],\n  classifiers=[\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'License :: OSI Approved :: MIT License',\n    'Programming Language :: Python :: 3.6',\n  ],\n)\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 3.228515625,
          "content": "import gzip\nimport random\nfrom accelerate.utils.tqdm import tqdm\nimport numpy as np\n\nimport torch\nfrom lion_pytorch import Lion\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom palm_rlhf_pytorch import PaLM\nfrom accelerate import Accelerator\n\n# constants\n\nNUM_BATCHES = int(1e5)\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATE_EVERY = 4\nLEARNING_RATE = 1e-4\nVALIDATE_EVERY = 100\nPRIME_LENGTH = 128\nGENERATE_EVERY = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 1024\n\n# helpers\n\ndef cycle(loader):\n    while True:\n        for data in loader:\n            yield data\n\ndef decode_token(token):\n    return str(chr(max(32, token)))\n\ndef decode_tokens(tokens):\n    return \"\".join(list(map(decode_token, tokens)))\n\n\n# accelerator\n\naccelerator = Accelerator(gradient_accumulation_steps=GRADIENT_ACCUMULATE_EVERY)\ndevice = accelerator.device\n\n# instantiate palm\n\nmodel = PaLM(\n    num_tokens=256,\n    dim=512,\n    depth=8,\n    flash_attn=True\n).to(device)\n\n# prepare enwik8 data\n\nwith gzip.open(\"./data/enwik8.gz\") as file:\n    data = np.frombuffer(file.read(int(95e6)), dtype=np.uint8).copy()\n    np_train, np_valid = np.split(data, [int(90e6)])\n    data_train, data_val = torch.from_numpy(np_train), torch.from_numpy(np_valid)\n\nclass TextSamplerDataset(Dataset):\n    def __init__(self, data, seq_len):\n        super().__init__()\n        self.data = data\n        self.seq_len = seq_len\n\n    def __getitem__(self, index):\n        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))\n        full_seq = self.data[rand_start : rand_start + self.seq_len + 1].long()\n        return full_seq.to(device)\n\n    def __len__(self):\n        return self.data.size(0) // self.seq_len\n\ntrain_dataset = TextSamplerDataset(data_train, SEQ_LEN)\nval_dataset = TextSamplerDataset(data_val, SEQ_LEN)\ntrain_loader = cycle(DataLoader(train_dataset, batch_size=BATCH_SIZE))\nval_loader = cycle(DataLoader(val_dataset, batch_size=BATCH_SIZE))\n\n# optimizer\n\noptim = Lion(model.palm_parameters(), lr = LEARNING_RATE)\n\nmodel, optim, train_loader, val_loader = accelerator.prepare(\n    model, optim, train_loader, val_loader\n)\n\n# training\n\nfor i in tqdm(range(NUM_BATCHES), mininterval=10.0, desc=\"training\"):\n    model.train()\n\n    with accelerator.accumulate(model):\n        loss = model(next(train_loader), return_loss = True)\n        accelerator.backward(loss / GRADIENT_ACCUMULATE_EVERY)\n\n        accelerator.print(f\"training loss: {loss.item()}\")\n        accelerator.clip_grad_norm_(model.parameters(), 0.5)\n    \n        optim.step()\n        optim.zero_grad()\n\n    if i % VALIDATE_EVERY == 0:\n        model.eval()\n        with torch.no_grad():\n            loss = model(next(val_loader), return_loss = True)\n            accelerator.print(f\"validation loss: {loss.item()}\")\n\n    if i % GENERATE_EVERY == 0:\n        model.eval()\n        inp = random.choice(val_dataset)[:PRIME_LENGTH]\n        prime = decode_tokens(inp)\n        accelerator.print(f\"%s \\n\\n %s\", (prime, \"*\" * 100))\n\n        # Check if model is wrapped\n        if hasattr(model, \"module\"):\n            sample = model.module.generate(GENERATE_LENGTH, inp[None, ...])\n        else:\n            sample = model.generate(GENERATE_LENGTH, inp[None, ...])\n\n        output_str = decode_tokens(sample[0])\n        accelerator.print(output_str, \"\\n\")\n"
        }
      ]
    }
  ]
}