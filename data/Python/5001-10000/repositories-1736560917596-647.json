{
  "metadata": {
    "timestamp": 1736560917596,
    "page": 647,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "WenDesi/lihang_book_algorithm",
      "stars": 5744,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".idea",
          "type": "tree",
          "content": null
        },
        {
          "name": "AdaBoost",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.986328125,
          "content": "<!--\n@Author: WenDesi\n@Date:   09-08-16\n@Email:  wendesi@foxmail.com\n@Last modified by:   WenDesi\n@Last modified time: 09-11-16\n-->\n\n\n\n# lihang_book_algorithm\n被李航老师肯定啦！！开心！\n<br>![](https://raw.githubusercontent.com/WenDesi/lihang_book_algorithm/master/weibo.png)\n\n## 简介\n我这里不介绍任何机器学习算法的原理，只是将《统计学习方法》中每一章的算法用我自己的方式实现一遍。\n除了李航书上的算法外，还实现了一些其他机器学习的算法。\n\n那么我们就按章节来了\n\n## 章节\n\n### 第二章 感知器模型\n博客：[李航《统计学习方法》第二章——用Python实现感知器模型（MNIST数据集）](http://blog.csdn.net/wds2006sdo/article/details/51923546)\n<br>代码：[perceptron/binary_perceptron.py](https://github.com/WenDesi/lihang_book_algorithm/blob/master/perceptron/binary_perceptron.py)\n\n### 第三章 K近邻法\n博客：[李航《统计学习方法》第三章——用Python实现KNN算法（MNIST数据集）](http://blog.csdn.net/wds2006sdo/article/details/51933044)\n<br>代码：[knn/knn.py](https://github.com/WenDesi/lihang_book_algorithm/blob/master/knn/knn.py)\n\n### 第四章 朴素贝叶斯\n博客：[李航《统计学习方法》第四章——用Python实现朴素贝叶斯分类器（MNIST数据集）](http://blog.csdn.net/wds2006sdo/article/details/51967839)\n<br>代码：[naive_bayes/naive_bayes.py](https://github.com/WenDesi/lihang_book_algorithm/blob/master/naive_bayes/naive_bayes.py)\n\n### 第五章 决策树\n博客：[李航《统计学习方法》第五章——用Python实现决策树（MNIST数据集）](http://blog.csdn.net/wds2006sdo/article/details/52849400)\n<br>代码：[decision_tree/decision_tree.py](https://github.com/WenDesi/lihang_book_algorithm/blob/master/decision_tree/decision_tree.py)\n\n### 第六章 逻辑斯提回归\n博客：[李航《统计学习方法》第六章——用Python实现逻辑斯谛回归（MNIST数据集）](http://blog.csdn.net/wds2006sdo/article/details/53084871)\n<br>代码：[logistic_regression/logistic_regression.py](https://github.com/WenDesi/lihang_book_algorithm/blob/master/logistic_regression/logistic_regression.py)\n\n### 第六章 最大熵模型\n博客：[李航《统计学习方法》第六章——用Python实现最大熵模型（MNIST数据集）](http://blog.csdn.net/wds2006sdo/article/details/53106579)\n<br>代码：[maxENT/maxENT.py](https://github.com/WenDesi/lihang_book_algorithm/blob/master/maxENT/maxENT.py)\n\n### 第七章 支持向量机\n博客：[李航《统计学习方法》第七章——用Python实现支持向量机模型（伪造数据集）](http://blog.csdn.net/wds2006sdo/article/details/53156589)\n<br>代码：[svm/svm.py](https://github.com/WenDesi/lihang_book_algorithm/blob/master/svm/svm.py)\n\n### 第八章 提升方法\n博客：[李航《统计学习方法》第八章——用Python+Cpp实现AdaBoost算法（MNIST数据集）](http://blog.csdn.net/wds2006sdo/article/details/53195725)\n<br>纯Python代码：[AdaBoost/adaboost.py](https://github.com/WenDesi/lihang_book_algorithm/blob/master/AdaBoost/adaboost.py)\n<br>Python C++代码：[AdaBoost/adaboost_cpp.py](https://github.com/WenDesi/lihang_book_algorithm/blob/master/AdaBoost/adaboost_cpp.py),[AdaBoost/Sign/Sign/sign.h](https://github.com/WenDesi/lihang_book_algorithm/blob/master/AdaBoost/Sign/Sign/sign.h),[AdaBoost/Sign/Sign/sign.cpp](https://github.com/WenDesi/lihang_book_algorithm/blob/master/AdaBoost/Sign/Sign/sign.cpp)\n\n### 第十章 隐马尔科夫模型\n博客：[李航《统计学习方法》第十章——用Python实现隐马尔科夫模型](http://blog.csdn.net/wds2006sdo/article/details/75212599)\n<br>代码：[hmm/hmm.py](https://github.com/WenDesi/lihang_book_algorithm/blob/master/hmm/hmm.py)\n\n\n## 额外章节\n\n###softmax分类器\n博客：[python 实现 softmax分类器（MNIST数据集）](http://blog.csdn.net/wds2006sdo/article/details/53699778)\n<br>代码：[softmax/softmax.py](https://github.com/WenDesi/lihang_book_algorithm/blob/master/softmax/softmax.py)\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "decision_tree",
          "type": "tree",
          "content": null
        },
        {
          "name": "extract_features.py",
          "type": "blob",
          "size": 4.4873046875,
          "content": "#encoding=utf-8\n\nimport numpy as np\nimport cv2\nimport time\nimport struct\nimport matplotlib.pyplot as plt\n\n\ndef loadImageSet(which=0):\n    print \"load image set\"\n    binfile=None\n    if which==0:\n        binfile = open(\"data/train-images.idx3-ubyte\", 'rb')\n    else:\n        binfile=  open(\"data/t10k-images.idx3-ubyte\", 'rb')\n    buffers = binfile.read()\n\n    head = struct.unpack_from('>IIII' , buffers ,0)\n    print \"head,\",head\n\n    offset=struct.calcsize('>IIII')\n    imgNum=head[1]\n    width=head[2]\n    height=head[3]\n    #[60000]*28*28\n    bits=imgNum*width*height\n    bitsString='>'+str(bits)+'B' #like '>47040000B'\n\n    imgs=struct.unpack_from(bitsString,buffers,offset)\n\n    binfile.close()\n    imgs=np.reshape(imgs,[imgNum,width,height])\n    print \"load imgs finished\"\n    return imgs\n\ndef loadLabelSet(which=0):\n    print \"load label set\"\n    binfile=None\n    if which==0:\n        binfile = open(\"data/train-labels.idx1-ubyte\", 'rb')\n    else:\n        binfile=  open(\"data/t10k-labels.idx1-ubyte\", 'rb')\n    buffers = binfile.read()\n\n    head = struct.unpack_from('>II' , buffers ,0)\n    print \"head,\",head\n    imgNum=head[1]\n\n    offset = struct.calcsize('>II')\n    numString='>'+str(imgNum)+\"B\"\n    labels= struct.unpack_from(numString , buffers , offset)\n    binfile.close()\n    labels=np.reshape(labels,[imgNum,1])\n\n    #print labels\n    print 'load label finished'\n    return labels\n\ndef get_features(imgs):\n    features = []\n    hog = cv2.HOGDescriptor('hog.xml')\n\n    # 二值化\n    for i in range(len(imgs)):\n        cv_img = imgs[i].astype(np.uint8)\n        cv2.threshold(cv_img,25,255,cv2.cv.CV_THRESH_BINARY_INV,imgs[i])\n\n    for img in imgs:\n        cv_img = img.astype(np.uint8)\n\n        hog_feature = hog.compute(cv_img)\n        hog_feature = np.transpose(hog_feature)\n\n        features.append(hog_feature)\n\n    return np.array(features)\n\ndef get_hog_features():\n    # trainset features\n    features_filepath = 'features/train.vec.npy'\n\n    imgs = loadImageSet()\n    labels = loadLabelSet()\n\n    features = get_features(imgs)\n    np.save(features_filepath,features)\n\n    # testset features\n    features_filepath = 'features/test.vec.npy'\n\n    imgs = loadImageSet(1)\n    labels = loadLabelSet(1)\n\n    features = get_features(imgs)\n    np.save(features_filepath,features)\n\n    features = np.load(features_filepath)\n\ndef manul_features(imgs):\n    features = []\n\n    tt = 0\n    for img in imgs:\n        print tt\n        tt += 1\n        feature = []\n        cv_img = img.astype(np.uint8)\n        cv2.threshold(cv_img,25,255,cv2.cv.CV_THRESH_BINARY_INV,cv_img)\n\n        range_list = [[0,7,0,7],\n                      [0,7,7,14],\n                      [0,7,14,21],\n                      [0,7,21,28],\n                      [7,14,0,7],\n                      [7,11,7,11],\n                      [7,11,11,14],\n                      [7,11,14,17],\n                      [7,11,17,21],\n                      [11,14,7,11],\n                      [11,14,11,14],\n                      [11,14,14,17],\n                      [11,14,17,21],\n                      [7,14,21,28],\n                      [14,21,21,28],\n                      [21,28,21,28],\n                      [14,21,0,7],\n                      [21,28,0,7],\n                      [14,17,7,11],\n                      [14,17,11,14],\n                      [14,17,14,17],\n                      [14,17,17,21],\n                      [17,21,7,11],\n                      [17,21,11,14],\n                      [17,21,14,17],\n                      [17,21,17,21],\n                      [21,24,7,11],\n                      [21,24,11,14],\n                      [21,24,14,17],\n                      [21,24,17,21],\n                      [24,28,7,11],\n                      [24,28,11,14],\n                      [24,28,14,17],\n                      [24,28,17,21]]\n\n\n\n        for range_ in range_list:\n            count = 0\n            for i in range(range_[0],range_[1]):\n                for j in range(range_[2],range_[3]):\n                    if cv_img[i][j] < 50:\n                        count += 1\n            feature.append(count)\n        features.append(feature)\n\n    return np.array(features)\n\ndef get_manual_features():\n    trainset_features_filepath = 'features/train.vec.npy'\n    testset_features_filepath = 'features/test.vec.npy'\n\n    imgs = loadImageSet()\n    features = manul_features(imgs)\n    np.save(trainset_features_filepath,features)\n\n    imgs = loadImageSet(1)\n    features = manul_features(imgs)\n    np.save(testset_features_filepath,features)\n\nif __name__==\"__main__\":\n    get_manual_features()\n    # get_hog_features()\n\n\n\n\n"
        },
        {
          "name": "features",
          "type": "tree",
          "content": null
        },
        {
          "name": "hmm",
          "type": "tree",
          "content": null
        },
        {
          "name": "hog.xml",
          "type": "blob",
          "size": 0.4775390625,
          "content": "<?xml version=\"1.0\"?>\n<opencv_storage>\n<hog type_id=\"opencv-object-detector-hog\">\n  <winSize>\n    28 28</winSize>\n  <blockSize>\n    14 14</blockSize>\n  <blockStride>\n    7 7</blockStride>\n  <cellSize>\n    7 7</cellSize>\n  <nbins>9</nbins>\n  <derivAperture>1</derivAperture>\n  <winSigma>4.</winSigma>\n  <histogramNormType>0</histogramNormType>\n  <L2HysThreshold>2.0000000000000001e-001</L2HysThreshold>\n  <gammaCorrection>1</gammaCorrection>\n  <nlevels>64</nlevels></hog>\n</opencv_storage>\n"
        },
        {
          "name": "knn",
          "type": "tree",
          "content": null
        },
        {
          "name": "logistic_regression",
          "type": "tree",
          "content": null
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 1.67578125,
          "content": "import numpy as np\nimport struct\nimport matplotlib.pyplot as plt\n\nfrom perceptron import *\n\ndef loadImageSet(which=0):\n    print \"load image set\"\n    binfile=None\n    if which==0:\n        binfile = open(\"data/train-images.idx3-ubyte\", 'rb')\n    else:\n        binfile=  open(\"data/t10k-images.idx3-ubyte\", 'rb')\n    buffers = binfile.read()\n\n    head = struct.unpack_from('>IIII' , buffers ,0)\n    print \"head,\",head\n\n    offset=struct.calcsize('>IIII')\n    imgNum=head[1]\n    width=head[2]\n    height=head[3]\n    #[60000]*28*28\n    bits=imgNum*width*height\n    bitsString='>'+str(bits)+'B' #like '>47040000B'\n\n    imgs=struct.unpack_from(bitsString,buffers,offset)\n\n    binfile.close()\n    imgs=np.reshape(imgs,[imgNum,width,height])\n    print \"load imgs finished\"\n    return imgs\n\ndef loadLabelSet(which=0):\n    print \"load label set\"\n    binfile=None\n    if which==0:\n        binfile = open(\"data/train-labels.idx1-ubyte\", 'rb')\n    else:\n        binfile=  open(\"data/t10k-labels.idx1-ubyte\", 'rb')\n    buffers = binfile.read()\n\n    head = struct.unpack_from('>II' , buffers ,0)\n    print \"head,\",head\n    imgNum=head[1]\n\n    offset = struct.calcsize('>II')\n    numString='>'+str(imgNum)+\"B\"\n    labels= struct.unpack_from(numString , buffers , offset)\n    binfile.close()\n    labels=np.reshape(labels,[imgNum,1])\n\n    #print labels\n    print 'load label finished'\n    return labels\n\nif __name__==\"__main__\":\n    imgs = loadImageSet()\n    labels = loadLabelSet()\n\n    index = 10\n\n    print imgs[index]\n    print labels[index]\n\n    fig = plt.figure()\n    plotwindow = fig.add_subplot(111)\n    plt.imshow(imgs[index] , cmap='gray')\n    # plt.show()\n\n    perceptron = Perceptron()\n    perceptron.hog_test(imgs[index])"
        },
        {
          "name": "maxENT",
          "type": "tree",
          "content": null
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "naive_bayes",
          "type": "tree",
          "content": null
        },
        {
          "name": "perceptron",
          "type": "tree",
          "content": null
        },
        {
          "name": "softmax",
          "type": "tree",
          "content": null
        },
        {
          "name": "svm",
          "type": "tree",
          "content": null
        },
        {
          "name": "weibo.png",
          "type": "blob",
          "size": 16.2158203125,
          "content": null
        }
      ]
    }
  ]
}