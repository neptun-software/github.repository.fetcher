{
  "metadata": {
    "timestamp": 1736560596516,
    "page": 218,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "brycedrennan/imaginAIry",
      "stars": 8030,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.2197265625,
          "content": "__pycache__\n*.pyc\n*.pyo\n*.pyd\n.Python\nenv\npip-log.txt\npip-delete-this-directory.txt\n.tox\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.log\n.git\n.mypy_cache\n.pytest_cache\n.hypothesis\n.DS_Store\nother\noutputs"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4951171875,
          "content": ".idea\n.vscode\n.DS_Store\n__pycache__\noutputs/*\nother\nprolly_delete\ncoverage\ndownloads\n.coverage\n.coveragerc\n/imaginairy/data/stable-diffusion-v1.yaml\n/imaginairy/data/stable-diffusion-v1-4.ckpt\nbuild\ndist\n**/*.ckpt\n**/*.egg-info\ntests/test_output\ngfpgan/**\n.python-version\n._.DS_Store\ntests/vastai_cli.py\n/tests/test_output_local_cuda/\n/testing_support/\n.unison*\n*.kgrind\n*.pyprof\n**/.polyscope.ini\n**/imgui.ini\n**/.eggs\n/img_size_memory_usage.csv\n/tests/test_cluster_output/\n/.env\n*.ipynb\n.ipynb_checkpoints"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.203125,
          "content": "FROM python:3.10.6-slim\n\nRUN apt-get update && apt-get install -y libgl1 libglib2.0-0 gcc\n\nENV PIP_DISABLE_PIP_VERSION_CHECK=1 \\\n    PIP_ROOT_USER_ACTION=ignore\n\nRUN pip install imaginairy\nRUN imagine --help\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.1328125,
          "content": "Copyright 2022 Bryce Drennan (and numerous other contributors as documented)\n(for modifications on top of CompVis code)\n\nThe MIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 13.333984375,
          "content": "SHELL := /bin/bash\npython_version = 3.10.13\nvenv_prefix = imaginairy\nvenv_name = $(venv_prefix)-$(python_version)\npyenv_instructions=https://github.com/pyenv/pyenv#installation\npyenv_virt_instructions=https://github.com/pyenv/pyenv-virtualenv#pyenv-virtualenv\n\n\ninit: require_pyenv  ## Setup a dev environment for local development.\n\t@pyenv install $(python_version) -s\n\t@echo -e \"\\033[0;32m ‚úîÔ∏è  üêç $(python_version) installed \\033[0m\"\n\t@if ! [ -d \"$$(pyenv root)/versions/$(venv_name)\" ]; then \\\n\t\tpyenv virtualenv $(python_version) $(venv_name); \\\n\tfi\n\t@pyenv local $(venv_name)\n\t@echo -e \"\\033[0;32m ‚úîÔ∏è  üêç $(venv_name) virtualenv activated \\033[0m\"\n\t@export VIRTUAL_ENV=$$(pyenv prefix); \\\n\tif command -v uv >/dev/null 2>&1; then \\\n\t\tuv pip install --upgrade uv; \\\n\telse \\\n\t\tpip install --upgrade pip uv; \\\n\tfi; \\\n\tuv pip sync requirements-dev.txt; \\\n\tuv pip install -e .\n\t@echo -e \"\\nEnvironment setup! ‚ú® üç∞ ‚ú® üêç \\n\\nCopy this path to tell PyCharm where your virtualenv is. You may have to click the refresh button in the PyCharm file explorer.\\n\"\n\t@echo -e \"\\033[0;32m$$(pyenv which python)\\033[0m\\n\"\n\t@echo -e \"The following commands are available to run in the Makefile:\\n\"\n\t@make -s help\n\n\naf: autoformat  ## Alias for `autoformat`\nautoformat:  ## Run the autoformatter.\n\t@-ruff check --config tests/ruff.toml . --fix-only\n\t@ruff format --config tests/ruff.toml .\n\ntest:  ## Run the tests.\n\t@pytest\n\t@echo -e \"The tests pass! ‚ú® üç∞ ‚ú®\"\n\ntest-fast:  ## Run the fast tests.\n\t@pytest -m \"not gputest\"\n\t@echo -e \"The non-gpu tests pass! ‚ú® üç∞ ‚ú®\"\n\nlint:  ## Run the code linter.\n\t@ruff check --config tests/ruff.toml .\n\t@echo -e \"No linting errors - well done! ‚ú® üç∞ ‚ú®\"\n\ntype-check: ## Run the type checker.\n\t@mypy --config-file tox.ini .\n\ncheck-fast:  ## Run autoformatter, linter, typechecker, and fast tests\n\t@make autoformat\n\t@make lint\n\t@make type-check\n\t@make test-fast\n\nbuild-pkg:  ## Build the package\n\tpython setup.py sdist bdist_wheel\n\tpython setup.py bdist_wheel --plat-name=win-amd64\n\ndeploy:  ## Deploy the package to pypi.org\n\tpip install twine wheel\n\t-git tag $$(python setup.py -V)\n\tgit push --tags\n\trm -rf dist\n\tmake build-pkg\n\t#python setup.py sdist\n\t@twine upload --verbose dist/* -u __token__;\n\trm -rf build\n\trm -rf dist\n\t@echo \"Deploy successful! ‚ú® üç∞ ‚ú®\"\n\nbuild-dev-image:\n\tdocker build -f tests/Dockerfile -t imaginairy-dev .\n\nrun-dev: build-dev-image\n\tdocker run -it -v $$HOME/.cache/huggingface:/root/.cache/huggingface -v $$HOME/.cache/torch:/root/.cache/torch -v `pwd`/outputs:/outputs imaginairy-dev /bin/bash\n\nrequirements:  ## Freeze the requirements.txt file\n\tpip-compile setup.py requirements-dev.in --output-file=requirements-dev.txt --upgrade --resolver=backtracking\n\nrequire_pyenv:\n\t@if ! [ -x \"$$(command -v pyenv)\" ]; then\\\n\t  echo -e '\\n\\033[0;31m ‚ùå pyenv is not installed.  Follow instructions here: $(pyenv_instructions)\\n\\033[0m';\\\n\t  exit 1;\\\n\telse\\\n\t  echo -e \"\\033[0;32m ‚úîÔ∏è  pyenv installed\\033[0m\";\\\n\tfi\n\n.PHONY: docs\n\ndocs:\n\tmkdocs serve\n\nupdate-stablestudio:\n\t@echo \"Updating stablestudio\"\n\tcd ../imaginAIry-StableStudio && \\\n\tyarn build && \\\n\tyarn build:production\n\trm -rf imaginairy/http/stablestudio/dist\n\tcp -R ../imaginAIry-StableStudio/packages/stablestudio-ui/dist imaginairy/http/stablestudio/dist\n\trm -rf imaginairy/http/stablestudio/dist/examples\n\trm -rf imaginairy/http/stablestudio/dist/media\n\trm -rf imaginairy/http/stablestudio/dist/presets\n\tcp ../imaginAIry-StableStudio/LICENSE imaginairy/http/stablestudio/dist/LICENSE\n\t@echo \"Updated stablestudio\"\n\nvendor_openai_clip:\n\tmkdir -p ./downloads\n\t-cd ./downloads && git clone git@github.com:openai/CLIP.git\n\tcd ./downloads/CLIP && git pull\n\trm -rf ./imaginairy/vendored/clip\n\tcp -R ./downloads/CLIP/clip imaginairy/vendored/\n\tgit --git-dir ./downloads/CLIP/.git rev-parse HEAD | tee ./imaginairy/vendored/clip/clip-commit-hash.txt\n\techo \"vendored from git@github.com:openai/CLIP.git\" | tee ./imaginairy/vendored/clip/readme.txt\n\nrevendorize: vendorize_kdiffusion\n\tmake vendorize REPO=git@github.com:openai/CLIP.git PKG=clip COMMIT=d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\n\tmake af\n\nvendorize_clipseg:\n\tmake download_repo REPO=git@github.com:timojl/clipseg.git PKG=clipseg COMMIT=ea54753df1e444c4445bac6e023546b6a41951d8\n\trm -rf ./imaginairy/vendored/clipseg\n\tmkdir -p ./imaginairy/vendored/clipseg\n\tcp -R ./downloads/clipseg/models/* ./imaginairy/vendored/clipseg/\n\tsed -i '' -e 's#import clip#from imaginairy.vendored import clip#g' ./imaginairy/vendored/clipseg/clipseg.py\n\trm ./imaginairy/vendored/clipseg/vitseg.py\n\tmv ./imaginairy/vendored/clipseg/clipseg.py ./imaginairy/vendored/clipseg/__init__.py\n\t# download weights\n\trm -rf ./downloads/clipseg-weights\n\tmkdir -p ./downloads/clipseg-weights\n\twget https://owncloud.gwdg.de/index.php/s/ioHbRzFx6th32hn/download -O ./downloads/clipseg-weights/weights.tar\n\tcd downloads/clipseg-weights && unzip -d weights -j weights.tar\n\tcp ./downloads/clipseg-weights/weights/rd64-uni-refined.pth ./imaginairy/vendored/clipseg/\n\nvendorize_blip:\n\tmake download_repo REPO=git@github.com:salesforce/BLIP.git PKG=blip COMMIT=48211a1594f1321b00f14c9f7a5b4813144b2fb9\n\trm -rf ./imaginairy/vendored/blip\n\tmkdir -p ./imaginairy/vendored/blip\n\tcp -R ./downloads/blip/models/* ./imaginairy/vendored/blip/\n\tcp -R ./downloads/blip/configs ./imaginairy/vendored/blip/\n\tsed -i '' -e 's#from models\\.#from imaginairy.vendored.blip.#g' ./imaginairy/vendored/blip/blip.py\n\tsed -i '' -e 's#print(#\\# print(#g' ./imaginairy/vendored/blip/blip.py\n\nvendorize_kdiffusion:\n\trm -rf ./imaginairy/vendored/k_diffusion\n\trm -rf ./downloads/k_diffusion\n    # version 0.0.9\n\tmake vendorize REPO=git@github.com:crowsonkb/k-diffusion.git PKG=k_diffusion COMMIT=5b3af030dd83e0297272d861c19477735d0317ec\n\t#sed -i '' -e 's/import\\sclip/from\\simaginairy.vendored\\simport\\sclip/g' imaginairy/vendored/k_diffusion/evaluation.py\n\tmv ./downloads/k_diffusion/LICENSE ./imaginairy/vendored/k_diffusion/\n\trm imaginairy/vendored/k_diffusion/evaluation.py\n\ttouch imaginairy/vendored/k_diffusion/evaluation.py\n\trm imaginairy/vendored/k_diffusion/config.py\n\ttouch imaginairy/vendored/k_diffusion/config.py\n\t# without this most of the k-diffusion samplers didn't work\n\tsed -i '' -e 's#return (x - denoised) / utils.append_dims(sigma, x.ndim)#return (x - denoised) / sigma#g' imaginairy/vendored/k_diffusion/sampling.py\n\tsed -i '' -e 's#torch.randn_like(x)#torch.randn_like(x, device=\"cpu\").to(x.device)#g' imaginairy/vendored/k_diffusion/sampling.py\n \t# https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/4558#issuecomment-1310387114\n\tsed -i '' -e 's#t_fn = lambda sigma: sigma.log().neg()#t_fn = lambda sigma: sigma.to(\"cpu\").log().neg().to(x.device)#g' imaginairy/vendored/k_diffusion/sampling.py\n\tsed -i '' -e 's#return (x - denoised) / sigma#return ((x - denoised) / sigma.to(\"cpu\")).to(x.device)#g' imaginairy/vendored/k_diffusion/sampling.py\n\tsed -i '' -e 's#return t.neg().exp()#return t.to(\"cpu\").neg().exp().to(self.model.device)#g' imaginairy/vendored/k_diffusion/sampling.py\n\tsed -i '' -e 's#import torchsde##g' imaginairy/vendored/k_diffusion/sampling.py\n\tsed -i '' -e 's#torch.randint(0, 2\\*\\*63 - 1, \\[\\])#torch.randint(0, 2**63 - 1, [], device=\"cpu\")#g' imaginairy/vendored/k_diffusion/sampling.py\n\tsed -i '' -e 's#torch.randint_like(x, 2)#torch.randint_like(x, 2, device=\"cpu\")#g' imaginairy/vendored/k_diffusion/sampling.py\n\tmake af\n\nvendorize_noodle_soup:\n\tmake download_repo REPO=git@github.com:WASasquatch/noodle-soup-prompts.git PKG=noodle-soup-prompts COMMIT=5642feb4d0e1340b9d145f5ff64f2b57eab1ae71\n\tmkdir -p ./imaginairy/vendored/noodle_soup_prompts\n\trm ./imaginairy/vendored/noodle_soup_prompts/*\n\tmv ./downloads/noodle-soup-prompts/LICENSE ./imaginairy/vendored/noodle_soup_prompts/\n\tpython scripts/prep_vocab_lists.py\n\tmake af\n\nvendorize_controlnet_annotators:\n\tmake download_repo REPO=git@github.com:lllyasviel/ControlNet-v1-1-nightly.git PKG=controlnet11 COMMIT=b9ae087ef56ca786d9a3ee1008f814bb171bb913\n\tmkdir -p ./imaginairy/vendored/controlnet_annotators\n\trm -rf ./imaginairy/vendored/controlnet_annotators/*\n\tcp -R ./downloads/controlnet11/annotator/* ./imaginairy/vendored/controlnet_annotators/\n\trm -rf ./imaginairy/vendored/controlnet_annotators/canny\n\trm -rf ./imaginairy/vendored/controlnet_annotators/ckpts\n\t#black imaginairy/vendored/controlnet_annotators\n\tsed -i '' -e 's#from annotator.uniformer.mmseg#from .mmseg#g' imaginairy/vendored/controlnet_annotators/uniformer/__init__.py\n\tfind imaginairy/vendored/controlnet_annotators -type f -name \"__init__.py\" -exec sed -i '' -e 's#checkpoint_file#remote_model_path#g' {} \\;\n\tfind imaginairy/vendored/controlnet_annotators -type f -name \"__init__.py\" -exec sed -i '' -e 's#modelpath#model_path#g' {} \\;\n\tfind imaginairy/vendored/controlnet_annotators -type f -name \"__init__.py\" -exec sed -i '' -e '/^ *model_path = os.path.join(annotator_ckpts_path, [^)]*/,/^ *load_file_from_url(remote_model_path, model_dir=annotator_ckpts_path)/c\\'$$'\\n''        model_path = get_cached_url_path(remote_model_path)' {} \\;\n\tfind imaginairy/vendored/controlnet_annotators -type f -name \"__init__.py\" -exec sed -i '' -e 's|^ *from annotator.util import annotator_ckpts_path|from imaginairy.model_manager import get_cached_url_path|' {} \\;\n\tfind imaginairy/vendored/controlnet_annotators -type f -name \"__init__.py\" -exec sed -i '' -e 's|^ *from annotator.util import|from imaginairy.vendored.controlnet_annotators.util import|' {} \\;\n\tfind imaginairy/vendored/controlnet_annotators -type f -name \"*.py\" -exec sed -i '' -e 's|^ *from annotator.|from imaginairy.vendored.controlnet_annotators.|' {} \\;\n\ttouch imaginairy/vendored/controlnet_annotators/__init__.py\n\tsed -i '' -e 's#from annotator.uniformer.mmseg#from .mmseg#g' imaginairy/vendored/controlnet_annotators/uniformer/__init__.py\n\tsed -i '' '11i\\'$$'\\n''annotator_ckpts_path = os.path.dirname(os.path.dirname(__file__))' imaginairy/vendored/controlnet_annotators/uniformer/__init__.py\n\trm ./imaginairy/vendored/controlnet_annotators/oneformer/oneformer/data/bpe_simple_voc*\n\trm -rf ./imaginairy/vendored/controlnet_annotators/zoe/zoedepth/models/base_models/midas_repo/mobile\n\tmake af\n\n\n\nvendorize_normal_map:\n\tmake download_repo REPO=git@github.com:brycedrennan/imaginairy-normal-map.git PKG=imaginairy_normal_map COMMIT=6b3b1692cbdc21d55c84a01e0b7875df030b6d79\n\tmkdir -p ./imaginairy/vendored/imaginairy_normal_map\n\trm -rf ./imaginairy/vendored/imaginairy_normal_map/*\n\tcp -R ./downloads/imaginairy_normal_map/imaginairy_normal_map/* ./imaginairy/vendored/imaginairy_normal_map/\n\tmake af\n\n\nvendorize_refiners:\n\texport REPO=git@github.com:finegrain-ai/refiners.git PKG=refiners COMMIT=91aea9b7ff63ddf93f99e2ce6a4452bd658b1948 && \\\n\tmake download_repo REPO=$$REPO PKG=$$PKG COMMIT=$$COMMIT && \\\n\tmkdir -p ./imaginairy/vendored/$$PKG && \\\n\trm -rf ./imaginairy/vendored/$$PKG/* && \\\n\tcp -R ./downloads/refiners/src/refiners/* ./imaginairy/vendored/$$PKG/ && \\\n\tcp ./downloads/refiners/LICENSE ./imaginairy/vendored/$$PKG/ && \\\n\trm -rf ./imaginairy/vendored/$$PKG/training_utils && \\\n\techo \"vendored from $$REPO @ $$COMMIT\" | tee ./imaginairy/vendored/$$PKG/readme.txt\n\tfind ./imaginairy/vendored/refiners/ -type f -name \"*.py\" -exec sed -i '' 's/from refiners/from imaginairy.vendored.refiners/g' {} + &&\\\n    find ./imaginairy/vendored/refiners/ -type f -name \"*.py\" -exec sed -i '' 's/import refiners/import imaginairy.vendored.refiners/g' {} + &&\\\n\tmake af\n\nvendorize_facexlib:\n\texport REPO=git@github.com:xinntao/facexlib.git PKG=facexlib COMMIT=260620ae93990a300f4b16448df9bb459f1caba9 && \\\n\tmake download_repo REPO=$$REPO PKG=$$PKG COMMIT=$$COMMIT && \\\n\tmkdir -p ./imaginairy/vendored/$$PKG && \\\n\trm -rf ./imaginairy/vendored/$$PKG/* && \\\n\tcp -R ./downloads/$$PKG/facexlib/* ./imaginairy/vendored/$$PKG/ && \\\n\trm -rf ./imaginairy/vendored/$$PKG/weights && \\\n\tcp ./downloads/$$PKG/LICENSE ./imaginairy/vendored/$$PKG/ && \\\n\techo \"vendored from $$REPO @ $$COMMIT\" | tee ./imaginairy/vendored/$$PKG/readme.txt\n\tfind ./imaginairy/vendored/facexlib/ -type f -name \"*.py\" -exec sed -i '' 's/from facexlib/from imaginairy.vendored.facexlib/g' {} + &&\\\n\tsed -i '' '/from \\.version import __gitsha__, __version__/d' ./imaginairy/vendored/facexlib/__init__.py\n\tmake af\n\nvendorize:  ## vendorize a github repo.  `make vendorize REPO=git@github.com:openai/CLIP.git PKG=clip`\n\tmkdir -p ./downloads\n\t-cd ./downloads && git clone $(REPO) $(PKG)\n\tcd ./downloads/$(PKG) && git fetch && git checkout $(COMMIT)\n\trm -rf ./imaginairy/vendored/$(PKG)\n\tcp -R ./downloads/$(PKG)/$(PKG) imaginairy/vendored/\n\tgit --git-dir ./downloads/$(PKG)/.git rev-parse HEAD | tee ./imaginairy/vendored/$(PKG)/source-commit-hash.txt\n\ttouch ./imaginairy/vendored/$(PKG)/version.py\n\techo \"vendored from $(REPO)\" | tee ./imaginairy/vendored/$(PKG)/readme.txt\n\ndownload_repo:\n\tmkdir -p ./downloads\n\trm -rf ./downloads/$(PKG)\n\t-cd ./downloads && git clone $(REPO) $(PKG)\n\tcd ./downloads/$(PKG) && git pull\n\nvendorize_whole_repo:\n\tmkdir -p ./downloads\n\t-cd ./downloads && git clone $(REPO) $(PKG)\n\tcd ./downloads/$(PKG) && git pull\n\trm -rf ./imaginairy/vendored/$(PKG)\n\tcp -R ./downloads/$(PKG) imaginairy/vendored/\n\tgit --git-dir ./downloads/$(PKG)/.git rev-parse HEAD | tee ./imaginairy/vendored/$(PKG)/clip-commit-hash.txt\n\ttouch ./imaginairy/vendored/$(PKG)/version.py\n\techo \"vendored from $(REPO)\" | tee ./imaginairy/vendored/$(PKG)/readme.txt\n\n\nhelp: ## Show this help message.\n\t@## https://gist.github.com/prwhite/8168133#gistcomment-1716694\n\t@echo -e \"$$(grep -hE '^\\S+:.*##' $(MAKEFILE_LIST) | sed -e 's/:.*##\\s*/:/' -e 's/^\\(.\\+\\):\\(.*\\)/\\\\x1b[36m\\1\\\\x1b[m:\\2/' | column -c2 -t -s :)\" | sort"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 25.6884765625,
          "content": "# ImaginAIry ü§ñüß†\n\n[![Downloads](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rOvQNs0Cmn_yU1bKWjCOHzGVDgZkaTtO?usp=sharing)\n[![Downloads](https://pepy.tech/badge/imaginairy)](https://pepy.tech/project/imaginairy)\n[![image](https://img.shields.io/pypi/v/imaginairy.svg)](https://pypi.org/project/imaginairy/)\n[![image](https://img.shields.io/badge/license-MIT-green)](https://github.com/brycedrennan/imaginAIry/blob/master/LICENSE/)\n[![Discord](https://flat.badgen.net/discord/members/FdD7ut3YjW)](https://discord.gg/FdD7ut3YjW)\n\nAI imagined images. Pythonic generation of stable diffusion images **and videos** *!.\n\n\"just works\" on Linux and macOS(M1) (and sometimes windows).\n\n\n```bash\n# on macOS, make sure rust is installed first\n# be sure to use Python 3.10, Python 3.11 is not supported at the moment\n>> pip install imaginairy\n>> imagine \"a scenic landscape\" \"a photo of a dog\" \"photo of a fruit bowl\" \"portrait photo of a freckled woman\" \"a bluejay\"\n# Make an AI video\n>> aimg videogen --start-image rocket.png\n```\n## Stable Video Diffusion\n<p float=\"left\">\n<img src=\"docs/assets/svd-rocket.gif\" height=\"190\">\n<img src=\"docs/assets/svd-athens.gif\" height=\"190\">\n<img src=\"docs/assets/svd-pearl-girl.gif\" height=\"190\">\n<img src=\"docs/assets/svd-starry-night.gif\" height=\"190\">\n<img src=\"docs/assets/svd-dog.gif\" height=\"190\">\n<img src=\"docs/assets/svd-xpbliss.gif\" height=\"190\">\n</p>\n\n### Rushed release of Stable Diffusion Video!\nWorks with Nvidia GPUs.  Does not work on Mac or CPU.\n\nOn Windows you'll need to install torch 2.0 first via https://pytorch.org/get-started/locally/\n```text\nUsage: aimg videogen [OPTIONS]\n\n  AI generate a video from an image\n\n  Example:\n\n      aimg videogen --start-image assets/rocket-wide.png\n\nOptions:\n  --start-image TEXT       Input path for image file.\n  --num-frames INTEGER     Number of frames.\n  --num-steps INTEGER      Number of steps.\n  --model TEXT             Model to use. One of: svd, svd_xt, svd_image_decoder, svd_xt_image_decoder\n  --fps INTEGER            FPS for the AI to target when generating video\n  --output-fps INTEGER     FPS for the output video\n  --motion-amount INTEGER  How much motion to generate. value between 0 and 255.\n  -r, --repeats INTEGER    How many times to repeat the renders.   [default: 1]\n  --cond-aug FLOAT         Conditional augmentation.\n  --seed INTEGER           Seed for random number generator.\n  --decoding_t INTEGER     Number of frames decoded at a time.\n  --output_folder TEXT     Output folder.\n  --help                   Show this message and exit.\n```\n\n### Images\n<p float=\"left\">\n<img src=\"docs/assets/026882_1_ddim50_PS7.5_a_scenic_landscape_[generated].jpg\" height=\"256\">\n<img src=\"docs/assets/026884_1_ddim50_PS7.5_photo_of_a_dog_[generated].jpg\" height=\"256\">\n<img src=\"docs/assets/026890_1_ddim50_PS7.5_photo_of_a_bowl_of_fruit._still_life_[generated].jpg\" height=\"256\">\n<img src=\"docs/assets/026885_1_ddim50_PS7.5_girl_with_a_pearl_earring_[generated].jpg\" height=\"256\">\n<img src=\"docs/assets/026891_1_ddim50_PS7.5_close-up_photo_of_a_bluejay_[generated].jpg\" height=\"256\">\n<img src=\"docs/assets/026893_1_ddim50_PS7.5_macro_photo_of_a_flower_[generated].jpg\" height=\"256\">\n</p>\n\n### Whats New\n[See full Changelog here](./docs/changelog.md)\n\n\n**14.3.0**\n- feature: integrates [spandrel](https://github.com/chaiNNer-org/spandrel) for upscaling \n- fix: allow loading sdxl models from local paths. \n\n**14.2.0**\n- üéâ feature: add image prompt support via `--image-prompt` and `--image-prompt-strength`\n\n**14.1.1**\n- tests: add installation tests for windows, mac, and conda\n- fix: dependency issues\n\n**14.1.0**\n- üéâ feature: make video generation smooth by adding frame interpolation\n- feature: SDXL weights in the compvis format can now be used\n- feature: allow video generation at any size specified by user\n- feature: video generations output in \"bounce\" format\n- feature: choose video output format: mp4, webp, or gif\n- feature: fix random seed handling in video generation\n- docs: auto-publish docs on push to master\n- build: remove imageio dependency\n- build: vendorize facexlib so we don't install its unneeded dependencies\n\n\n**14.0.4**\n- docs: add a documentation website at https://brycedrennan.github.io/imaginAIry/\n- build: remove fairscale dependency\n- fix: video generation was broken\n\n**14.0.3**\n- fix: several critical bugs with package\n- tests: add a wheel smoketest to detect these issues in the future\n\n**14.0.0**\n- üéâ video generation using [Stable Video Diffusion](https://github.com/Stability-AI/generative-models)\n  - add `--videogen` to any image generation to create a short video from the generated image\n  - or use `aimg videogen` to generate a video from an image\n- üéâ SDXL (Stable Diffusion Extra Large) models are now supported.\n  - try `--model opendalle` or `--model sdxl`\n  - inpainting and controlnets are not yet supported for SDXL\n- üéâ imaginairy is now backed by the [refiners library](https://github.com/finegrain-ai/refiners)\n  - This was a huge rewrite which is why some features are not yet supported.  On the plus side, refiners supports\ncutting edge features (SDXL, image prompts, etc) which will be added to imaginairy soon.\n  - [self-attention guidance](https://github.com/SusungHong/Self-Attention-Guidance) which makes details of images more accurate\n- üéâ feature: larger image generations now work MUCH better and stay faithful to the same image as it looks at a smaller size. \nFor example `--size 720p --seed 1` and `--size 1080p --seed 1` will produce the same image for SD15\n- üéâ feature: loading diffusers based models now supported. Example `--model https://huggingface.co/ainz/diseny-pixar --model-architecture sd15`\n- üéâ feature: qrcode controlnet!\n\n\n### Run API server and StableStudio web interface (alpha)\nGenerate images via API or web interface.  Much smaller featureset compared to the command line tool.\n```bash\n>> aimg server\n```\nVisit http://localhost:8000/ and http://localhost:8000/docs\n\n<img src=\"https://github.com/Stability-AI/StableStudio/blob/a65d4877ad7d309627808a169818f1add8c278ae/misc/GenerateScreenshot.png?raw=true\" width=\"512\">\n\n### Image Structure Control [by ControlNet](https://github.com/lllyasviel/ControlNet)\n#### (Not supported for SDXL yet)\nGenerate images guided by body poses, depth maps, canny edges, hed boundaries, or normal maps.\n\n**Openpose Control**\n\n```bash\nimagine --control-image assets/indiana.jpg  --control-mode openpose --caption-text openpose \"photo of a polar bear\"\n```\n\n<p float=\"left\">\n    <img src=\"docs/assets/indiana.jpg\" height=\"256\">\n    <img src=\"docs/assets/indiana-pose.jpg\" height=\"256\">\n    <img src=\"docs/assets/indiana-pose-polar-bear.jpg\" height=\"256\">\n</p>\n\n#### Canny Edge Control\n\n```bash\nimagine --control-image assets/lena.png  --control-mode canny \"photo of a woman with a hat looking at the camera\"\n```\n\n<p float=\"left\">\n    <img src=\"docs/assets/lena.png\" height=\"256\">\n    <img src=\"docs/assets/lena-canny.jpg\" height=\"256\">\n    <img src=\"docs/assets/lena-canny-generated.jpg\" height=\"256\">\n</p>\n\n#### HED Boundary Control\n\n```bash\nimagine --control-image dog.jpg  --control-mode hed  \"photo of a dalmation\"\n```\n\n<p float=\"left\">\n    <img src=\"docs/assets/000032_337692011_PLMS40_PS7.5_a_photo_of_a_dog.jpg\" height=\"256\">\n    <img src=\"docs/assets/dog-hed-boundary.jpg\" height=\"256\">\n    <img src=\"docs/assets/dog-hed-boundary-dalmation.jpg\" height=\"256\">\n</p>\n\n#### Depth Map Control\n\n```bash\nimagine --control-image fancy-living.jpg  --control-mode depth  \"a modern living room\"\n```\n\n<p float=\"left\">\n    <img src=\"docs/assets/fancy-living.jpg\" height=\"256\">\n    <img src=\"docs/assets/fancy-living-depth.jpg\" height=\"256\">\n    <img src=\"docs/assets/fancy-living-depth-generated.jpg\" height=\"256\">\n</p>\n\n#### Normal Map Control\n\n```bash\nimagine --control-image bird.jpg  --control-mode normal  \"a bird\"\n```\n\n<p float=\"left\">\n    <img src=\"docs/assets/013986_1_kdpmpp2m59_PS7.5_a_bluejay_[generated].jpg\" height=\"256\">\n    <img src=\"docs/assets/bird-normal.jpg\" height=\"256\">\n    <img src=\"docs/assets/bird-normal-generated.jpg\" height=\"256\">\n</p>\n\n#### Image Shuffle Control\n\nGenerates the image based on elements of the control image. Kind of similar to style transfer.\n```bash\nimagine --control-image pearl-girl.jpg  --control-mode shuffle  \"a clown\"\n```\nThe middle image is the \"shuffled\" input image\n<p float=\"left\">\n    <img src=\"docs/assets/girl_with_a_pearl_earring.jpg\" height=\"256\">\n    <img src=\"docs/assets/pearl_shuffle_019331_1_kdpmpp2m15_PS7.5_img2img-0.0_a_clown.jpg\" height=\"256\">\n    <img src=\"docs/assets/pearl_shuffle_clown_019331_1_kdpmpp2m15_PS7.5_img2img-0.0_a_clown.jpg\" height=\"256\">\n</p>\n\n#### Editing Instructions Control\n\nSimilar to instructPix2Pix (below) but works with any SD 1.5 based model.\n```bash\nimagine --control-image pearl-girl.jpg  --control-mode edit --init-image-strength 0.01 --steps 30  --negative-prompt \"\" --model openjourney-v2 \"make it anime\" \"make it at the beach\" \n```\n\n<p float=\"left\">\n    <img src=\"docs/assets/girl_with_a_pearl_earring.jpg\" height=\"256\">\n    <img src=\"docs/assets/pearl_anime_019537_521829407_kdpmpp2m30_PS9.0_img2img-0.01_make_it_anime.jpg\" height=\"256\">\n    <img src=\"docs/assets/pearl_beach_019561_862735879_kdpmpp2m30_PS7.0_img2img-0.01_make_it_at_the_beach.jpg\" height=\"256\">\n</p>\n\n#### Add Details Control (upscaling/super-resolution)\n\nReplaces existing details in an image. Good to use with --init-image-strength 0.2\n```bash\nimagine --control-image \"assets/wishbone.jpg\" --control-mode details \"sharp focus, high-resolution\" --init-image-strength 0.2 --steps 30 -w 2048 -h 2048 \n```\n\n<p float=\"left\">\n    <img src=\"docs/assets/wishbone_headshot_badscale.jpg\" height=\"256\">\n    <img src=\"docs/assets/wishbone_headshot_details.jpg\" height=\"256\">\n</p>\n\n\n### Image (re)Colorization (using brightness control)\nColorize black and white images or re-color existing images.\n\nThe generated colors will be applied back to the original image. You can either provide a caption or \nallow the tool to generate one for you.\n\n```bash\naimg colorize pearl-girl.jpg --caption \"photo of a woman\"\n```\n<p float=\"left\">\n    <img src=\"docs/assets/girl_with_a_pearl_earring.jpg\" height=\"256\">\n    <img src=\"docs/assets/pearl-gray.jpg\" height=\"256\">\n    <img src=\"docs/assets/pearl-recolor-a.jpg\" height=\"256\">\n</p>\n\n###  Instruction based image edits [by InstructPix2Pix](https://github.com/timothybrooks/instruct-pix2pix)\n#### (Broken as of 14.0.0)\nJust tell imaginairy how to edit the image and it will do it for you!\n<p float=\"left\">\n<img src=\"docs/assets/scenic_landscape_winter.jpg\" height=\"256\">\n<img src=\"docs/assets/dog_red.jpg\" height=\"256\">\n<img src=\"docs/assets/bowl_of_fruit_strawberries.jpg\" height=\"256\">\n<img src=\"docs/assets/freckled_woman_cyborg.jpg\" height=\"256\">\n<img src=\"docs/assets/014214_51293814_kdpmpp2m30_PS10.0_img2img-1.0_make_the_bird_wear_a_cowboy_hat_[generated].jpg\" height=\"256\">\n<img src=\"docs/assets/flower-make-the-flower-out-of-paper-origami.gif\" height=\"256\">\n<img src=\"docs/assets/girl-pearl-clown-compare.gif\" height=\"256\">\n<img src=\"docs/assets/mona-lisa-headshot-anim.gif\" height=\"256\">\n<img src=\"docs/assets/make-it-night-time.gif\" height=\"256\">\n</p>\n\n<details>\n<summary>Click to see shell commands</summary>\nUse prompt strength to control how strong the edit is. For extra control you can combine with prompt-based masking.\n\n```bash\n# enter imaginairy shell\n>> aimg\nü§ñüß†> edit scenic_landscape.jpg -p \"make it winter\" --prompt-strength 20\nü§ñüß†> edit dog.jpg -p \"make the dog red\" --prompt-strength 5\nü§ñüß†> edit bowl_of_fruit.jpg -p \"replace the fruit with strawberries\"\nü§ñüß†> edit freckled_woman.jpg -p \"make her a cyborg\" --prompt-strength 13\nü§ñüß†> edit bluebird.jpg -p \"make the bird wear a cowboy hat\" --prompt-strength 10\nü§ñüß†> edit flower.jpg -p \"make the flower out of paper origami\" --arg-schedule prompt-strength[1:11:0.3]  --steps 25 --compilation-anim gif\n\n# create a comparison gif\nü§ñüß†> edit pearl_girl.jpg -p \"make her wear clown makeup\" --compare-gif\n# create an animation showing the edit with increasing prompt strengths\nü§ñüß†> edit mona-lisa.jpg -p \"make it a color professional photo headshot\" --negative-prompt \"old, ugly, blurry\" --arg-schedule \"prompt-strength[2:8:0.5]\" --compilation-anim gif\nü§ñüß†> edit gg-bridge.jpg -p \"make it night time\" --prompt-strength 15  --steps 30 --arg-schedule prompt-strength[1:15:1] --compilation-anim gif\n```\n</details>\n\n\n\n### Quick Image Edit Demo\nWant just quickly have some fun? Try `edit-demo` to apply some pre-defined edits.\n```bash\n>> aimg edit-demo pearl_girl.jpg\n```\n<p float=\"left\">\n<img src=\"docs/assets/girl_with_a_pearl_earring_suprise.gif\" height=\"256\">\n<img src=\"docs/assets/mona-lisa-suprise.gif\" height=\"256\">\n<img src=\"docs/assets/luke-suprise.gif\" height=\"256\">\n<img src=\"docs/assets/spock-suprise.gif\" height=\"256\">\n<img src=\"docs/assets/gg-bridge-suprise.gif\" height=\"256\">\n<img src=\"docs/assets/shire-suprise.gif\" height=\"256\">\n</p>\n\n\n### Prompt Based Masking  [by clipseg](https://github.com/timojl/clipseg)\nSpecify advanced text based masks using boolean logic and strength modifiers. \nMask syntax:\n  - mask descriptions must be lowercase\n  - keywords (`AND`, `OR`, `NOT`) must be uppercase\n  - parentheses are supported \n  - mask modifiers may be appended to any mask or group of masks.  Example: `(dog OR cat){+5}` means that we'll\nselect any dog or cat and then expand the size of the mask area by 5 pixels.  Valid mask modifiers:\n    - `{+n}` - expand mask by n pixels\n    - `{-n}` - shrink mask by n pixels\n    - `{*n}` - multiply mask strength. will expand mask to areas that weakly matched the mask description\n    - `{/n}` - divide mask strength. will reduce mask to areas that most strongly matched the mask description. probably not useful\n\nWhen writing strength modifiers keep in mind that pixel values are between 0 and 1.\n\n```bash\n>> imagine \\\n    --init-image pearl_earring.jpg \\\n    --mask-prompt \"face AND NOT (bandana OR hair OR blue fabric){*6}\" \\\n    --mask-mode keep \\\n    --init-image-strength .2 \\\n    --fix-faces \\\n    \"a modern female president\" \"a female robot\" \"a female doctor\" \"a female firefighter\"\n```\n<img src=\"docs/assets/mask_examples/pearl000.jpg\" height=\"200\">‚û°Ô∏è \n<img src=\"docs/assets/mask_examples/pearl_pres.png\" height=\"200\">\n<img src=\"docs/assets/mask_examples/pearl_robot.png\" height=\"200\">\n<img src=\"docs/assets/mask_examples/pearl_doctor.png\" height=\"200\">\n<img src=\"docs/assets/mask_examples/pearl_firefighter.png\" height=\"200\">\n\n```bash\n>> imagine \\\n    --init-image fruit-bowl.jpg \\\n    --mask-prompt \"fruit OR fruit stem{*6}\" \\\n    --mask-mode replace \\\n    --mask-modify-original \\\n    --init-image-strength .1 \\\n    \"a bowl of kittens\" \"a bowl of gold coins\" \"a bowl of popcorn\" \"a bowl of spaghetti\"\n```\n<img src=\"docs/assets/000056_293284644_PLMS40_PS7.5_photo_of_a_bowl_of_fruit.jpg\" height=\"200\">‚û°Ô∏è \n<img src=\"docs/assets/mask_examples/bowl004.jpg\" height=\"200\">\n<img src=\"docs/assets/mask_examples/bowl001.jpg\" height=\"200\">\n<img src=\"docs/assets/mask_examples/bowl002.jpg\" height=\"200\">\n<img src=\"docs/assets/mask_examples/bowl003.jpg\" height=\"200\">\n\n\n### Face Enhancement [by CodeFormer](https://github.com/sczhou/CodeFormer)\n\n```bash\n>> imagine \"a couple smiling\" --steps 40 --seed 1 --fix-faces\n```\n<img src=\"https://github.com/brycedrennan/imaginAIry/raw/master/assets/000178_1_PLMS40_PS7.5_a_couple_smiling_nofix.png\" height=\"256\"> ‚û°Ô∏è \n<img src=\"https://github.com/brycedrennan/imaginAIry/raw/master/assets/000178_1_PLMS40_PS7.5_a_couple_smiling_fixed.png\" height=\"256\"> \n\n\n## Image Upscaling\nUpscale images easily.\n\n=== \"CLI\"\n    ```bash\n    aimg upscale assets/000206_856637805_PLMS40_PS7.5_colorful_smoke.jpg --upscale-model real-hat\n    ```\n\n=== \"Python\"\n    ```py\n    from imaginairy.api.upscale import upscale\n\n    img = upscale(img=\"assets/000206_856637805_PLMS40_PS7.5_colorful_smoke.jpg\")\n    img.save(\"colorful_smoke.upscaled.jpg\")\n\n    ```\n<img src=\"docs/assets/000206_856637805_PLMS40_PS7.5_colorful_smoke.jpg\" width=\"25%\" height=\"auto\"> ‚û°Ô∏è \n<img src=\"docs/assets/000206_856637805_PLMS40_PS7.5_colorful_smoke_upscaled.jpg\" width=\"50%\" height=\"auto\">\n\nUpscaling uses [Spandrel](https://github.com/chaiNNer-org/spandrel) to make it easy to use different upscaling models.\nYou can view different integrated models by running `aimg upscale --list-models`, and then use it with `--upscale-model <model-name>`.\nAlso accepts url's if you want to upscale an image with a different model. Control the new file format/location with --format.\n\n```python\nfrom imaginairy.enhancers.upscale_realesrgan import upscale_image\nfrom PIL import Image\nimg = Image.open(\"my-image.jpg\")\nbig_img = upscale_image(i)\n```\n\n\n\n### Tiled Images\n```bash\n>> imagine  \"gold coins\" \"a lush forest\" \"piles of old books\" leaves --tile\n```\n\n<img src=\"docs/assets/000066_801493266_PLMS40_PS7.5_gold_coins.jpg\" height=\"128\"><img src=\"docs/assets/000066_801493266_PLMS40_PS7.5_gold_coins.jpg\" height=\"128\"><img src=\"docs/assets/000066_801493266_PLMS40_PS7.5_gold_coins.jpg\" height=\"128\">\n<img src=\"docs/assets/000118_597948545_PLMS40_PS7.5_a_lush_forest.jpg\" height=\"128\"><img src=\"docs/assets/000118_597948545_PLMS40_PS7.5_a_lush_forest.jpg\" height=\"128\"><img src=\"docs/assets/000118_597948545_PLMS40_PS7.5_a_lush_forest.jpg\" height=\"128\">\n<br>\n<img src=\"docs/assets/000075_961095192_PLMS40_PS7.5_piles_of_old_books.jpg\" height=\"128\"><img src=\"docs/assets/000075_961095192_PLMS40_PS7.5_piles_of_old_books.jpg\" height=\"128\"><img src=\"docs/assets/000075_961095192_PLMS40_PS7.5_piles_of_old_books.jpg\" height=\"128\">\n<img src=\"docs/assets/000040_527733581_PLMS40_PS7.5_leaves.jpg\" height=\"128\"><img src=\"docs/assets/000040_527733581_PLMS40_PS7.5_leaves.jpg\" height=\"128\"><img src=\"docs/assets/000040_527733581_PLMS40_PS7.5_leaves.jpg\" height=\"128\">\n#### 360 degree images\n```bash\nimagine --tile-x -w 1024 -h 512 \"360 degree equirectangular panorama photograph of the desert\"  --upscale\n```\n<img src=\"docs/assets/desert_360.jpg\" height=\"128\">\n\n### Image-to-Image\nUse depth maps for amazing \"translations\" of existing images.\n\n```bash\n>> imagine --init-image girl_with_a_pearl_earring_large.jpg --init-image-strength 0.05  \"professional headshot photo of a woman with a pearl earring\" -r 4 -w 1024 -h 1024 --steps 50\n```\n<p float=\"left\">\n<img src=\"tests/data/girl_with_a_pearl_earring.jpg\" width=\"256\"> ‚û°Ô∏è \n<img src=\"docs/assets/pearl_depth_1.jpg\" width=\"256\">\n<img src=\"docs/assets/pearl_depth_2.jpg\" width=\"256\">\n</p>\n\n\n### Outpainting\n\nGiven a starting image, one can generate it's \"surroundings\".\n\nExample:\n`imagine --init-image pearl-earring.jpg --init-image-strength 0 --outpaint all250,up0,down600 \"woman standing\"`\n\n<img src=\"tests/data/girl_with_a_pearl_earring.jpg\" height=\"256\"> ‚û°Ô∏è \n<img src=\"tests/expected_output/test_outpainting_outpaint_.png\" height=\"256\">\n\n### Work with different generation models\n\n<p float=\"left\">\n    <img src=\"docs/assets/fairytale-treehouse-sd15.jpg\" height=\"256\">\n    <img src=\"docs/assets/fairytale-treehouse-openjourney-v1.jpg\" height=\"256\">\n    <img src=\"docs/assets/fairytale-treehouse-openjourney-v2.jpg\" height=\"256\">\n</p>\n\n<details>\n<summary>Click to see shell command</summary>\n\n```bash\nimagine \"valley, fairytale treehouse village covered, , matte painting, highly detailed, dynamic lighting, cinematic, realism, realistic, photo real, sunset, detailed, high contrast, denoised, centered, michael whelan\" --steps 60 --seed 1 --arg-schedule model[sd14,sd15,sd20,sd21,openjourney-v1,openjourney-v2] --arg-schedule \"caption-text[sd14,sd15,sd20,sd21,openjourney-v1,openjourney-v2]\"\n```\n</details>\n\n### Prompt Expansion\nYou can use `{}` to randomly pull values from lists.  A list of values separated by `|` \n and enclosed in `{ }` will be randomly drawn from in a non-repeating fashion. Values that are surrounded by `_ _` will \n pull from a phrase list of the same name.   Folders containing .txt phraselist files may be specified via\n`--prompt_library_path`. The option may be specified multiple times.  Built-in categories:\n    \n      3d-term, adj-architecture, adj-beauty, adj-detailed, adj-emotion, adj-general, adj-horror, animal, art-scene, art-movement, \n      art-site, artist, artist-botanical, artist-surreal, aspect-ratio, bird, body-of-water, body-pose, camera-brand,\n      camera-model, color, cosmic-galaxy, cosmic-nebula, cosmic-star, cosmic-term, desktop-background, dinosaur, eyecolor, f-stop, \n      fantasy-creature, fantasy-setting, fish, flower, focal-length, food, fruit, games, gen-modifier, hair, hd,\n      iso-stop, landscape-type, national-park, nationality, neg-weight, noun-beauty, noun-fantasy, noun-general, \n      noun-horror, occupation, painting-style, photo-term, pop-culture, pop-location, punk-style, quantity, rpg-item, scenario-desc, \n      skin-color, spaceship, style, tree-species, trippy, world-heritage-site\n\n   Examples:\n\n   `imagine \"a {lime|blue|silver|aqua} colored dog\" -r 4 --seed 0` (note that it generates a dog of each color without repetition)\n\n<img src=\"docs/assets/000184_0_plms40_PS7.5_a_silver_colored_dog_[generated].jpg\" height=\"200\"><img src=\"docs/assets/000186_0_plms40_PS7.5_a_aqua_colored_dog_[generated].jpg\" height=\"200\">\n<img src=\"docs/assets/000210_0_plms40_PS7.5_a_lime_colored_dog_[generated].jpg\" height=\"200\">\n<img src=\"docs/assets/000211_0_plms40_PS7.5_a_blue_colored_dog_[generated].jpg\" height=\"200\">\n\n   `imagine \"a {_color_} dog\" -r 4 --seed 0` will generate four, different colored dogs. The colors will be pulled from an included \n   phraselist of colors.\n    \n   `imagine \"a {_spaceship_|_fruit_|hot air balloon}. low-poly\" -r 4 --seed 0` will generate images of spaceships or fruits or a hot air balloon\n\n<details>\n<summary>Python example</summary>\n\n```python\nfrom imaginairy.enhancers.prompt_expansion import expand_prompts\n\nmy_prompt = \"a giant {_animal_}\"\n\nexpanded_prompts = expand_prompts(n=10, prompt_text=my_prompt, prompt_library_paths=[\"./prompts\"])\n```\n</details>\n\n   Credit to [noodle-soup-prompts](https://github.com/WASasquatch/noodle-soup-prompts/) where most, but not all, of the wordlists originate.\n\n### Generate image captions (via [BLIP](https://github.com/salesforce/BLIP))\n```bash\n>> aimg describe assets/mask_examples/bowl001.jpg\na bowl full of gold bars sitting on a table\n```\n\n### Example Use Cases\n\n```bash\n>> aimg\n# Generate endless 8k art\nü§ñüß†> imagine -w 1920 -h 1080 --upscale \"{_art-scene_}. {_painting-style_} by {_artist_}\" -r 1000 --steps 30 --model sd21v\n\n# generate endless desktop backgrounds \nü§ñüß†> imagine --tile \"{_desktop-background_}\" -r 100\n\n# convert a folder of images to pencil sketches\nü§ñüß†> edit other/images/*.jpg -p \"make it a pencil sketch\"\n\n# upscale a folder of images\nü§ñüß†> upscale my-images/*.jpg\n\n# generate kitchen remodel ideas\nü§ñüß†> imagine --control-image kitchen.jpg -w 1024 -h 1024 \"{_interior-style_} kitchen\" --control-mode depth -r 100 --init-image 0.01 --upscale --steps 35 --caption-text \"{prompt}\"\n```\n\n### Additional Features\n - Generate images either in code or from command line.\n - It just works. Proper requirements are installed. Model weights are automatically downloaded. No huggingface account needed. \n    (if you have the right hardware... and aren't on windows)\n - Noisy logs are gone (which was surprisingly hard to accomplish)\n - WeightedPrompts let you smash together separate prompts (cat-dog)\n - Prompt metadata saved into image file metadata\n - Have AI generate captions for images `aimg describe <filename-or-url>`\n - Interactive prompt: just run `aimg`\n \n## How To\n\nFor full command line instructions run `aimg --help`\n\n```python\nfrom imaginairy import imagine, imagine_image_files, ImaginePrompt, WeightedPrompt, LazyLoadingImage\n\nurl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Thomas_Cole_-_Architect%E2%80%99s_Dream_-_Google_Art_Project.jpg/540px-Thomas_Cole_-_Architect%E2%80%99s_Dream_-_Google_Art_Project.jpg\"\nprompts = [\n    ImaginePrompt(\"a scenic landscape\", seed=1, upscale=True),\n    ImaginePrompt(\"a bowl of fruit\"),\n    ImaginePrompt([\n        WeightedPrompt(\"cat\", weight=1),\n        WeightedPrompt(\"dog\", weight=1),\n    ]),\n    ImaginePrompt(\n        \"a spacious building\", \n        init_image=LazyLoadingImage(url=url)\n    ),\n    ImaginePrompt(\n        \"a bowl of strawberries\", \n        init_image=LazyLoadingImage(filepath=\"mypath/to/bowl_of_fruit.jpg\"),\n        mask_prompt=\"fruit OR stem{*2}\",  # amplify the stem mask x2\n        mask_mode=\"replace\",\n        mask_modify_original=True,\n    ),\n    ImaginePrompt(\"strawberries\", tile_mode=True),\n]\nfor result in imagine(prompts):\n    # do something\n    result.save(\"my_image.jpg\")\n\n# or\n\nimagine_image_files(prompts, outdir=\"./my-art\")\n\n```\n\n## Requirements\n- ~10 gb space for models to download\n- A CUDA supported graphics card with >= 11gb VRAM (and CUDA installed) or an M1 processor.\n- Python installed. Preferably Python 3.10.  (not conda)\n- For macOS [rust](https://www.rust-lang.org/tools/install) and setuptools-rust must be installed to compile the `tokenizer` library.\nThey can be installed via: `curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh` and `pip install setuptools-rust`\n    \n\n## Running in Docker\nSee example Dockerfile (works on machine where you can pass the gpu into the container)\n```bash\ndocker build . -t imaginairy\n# you really want to map the cache or you end up wasting a lot of time and space redownloading the model weights\ndocker run -it --gpus all -v $HOME/.cache/huggingface:/root/.cache/huggingface -v $HOME/.cache/torch:/root/.cache/torch -v `pwd`/outputs:/outputs imaginairy /bin/bash\n```\n\n## Running on Google Colab\n[Example Colab](https://colab.research.google.com/drive/1rOvQNs0Cmn_yU1bKWjCOHzGVDgZkaTtO?usp=sharing)\n\n## Q&A\n\n#### Q: How do I change the cache directory for where models are stored?\n\nA: Set the `HUGGINGFACE_HUB_CACHE` environment variable. \n\n#### Q: How do I free up disk space?\n\nA: The AI models are cached in `~/.cache/` (or `HUGGINGFACE_HUB_CACHE`). To delete the cache remove the following folders:\n - ~/.cache/imaginairy\n - ~/.cache/clip\n - ~/.cache/torch\n - ~/.cache/huggingface\n\n\n\n## Not Supported\n - exploratory features that don't work well\n\n\n"
        },
        {
          "name": "STABLE_DIFFUSION_LICENSE",
          "type": "blob",
          "size": 14.0478515625,
          "content": "Copyright (c) 2022 Robin Rombach and Patrick Esser and contributors\n\nCreativeML Open RAIL-M\ndated August 22, 2022\n\nSection I: PREAMBLE\n\nMultimodal generative models are being widely adopted and used, and have the potential to transform the way artists, among other individuals, conceive and benefit from AI or ML technologies as a tool for content creation.\n\nNotwithstanding the current and potential benefits that these artifacts can bring to society at large, there are also concerns about potential misuses of them, either due to their technical limitations or ethical considerations.\n\nIn short, this license strives for both the open and responsible downstream use of the accompanying model. When it comes to the open character, we took inspiration from open source permissive licenses regarding the grant of IP rights. Referring to the downstream responsible use, we added use-based restrictions not permitting the use of the Model in very specific scenarios, in order for the licensor to be able to enforce the license in case potential misuses of the Model may occur. At the same time, we strive to promote open and responsible research on generative models for art and content generation.\n\nEven though downstream derivative versions of the model could be released under different licensing terms, the latter will always have to include - at minimum - the same use-based restrictions as the ones in the original license (this license). We believe in the intersection between open and responsible AI development; thus, this License aims to strike a balance between both in order to enable responsible open-science in the field of AI.\n\nThis License governs the use of the model (and its derivatives) and is informed by the model card associated with the model.\n\nNOW THEREFORE, You and Licensor agree as follows:\n\n1. Definitions\n\n- \"License\" means the terms and conditions for use, reproduction, and Distribution as defined in this document.\n- \"Data\" means a collection of information and/or content extracted from the dataset used with the Model, including to train, pretrain, or otherwise evaluate the Model. The Data is not licensed under this License.\n- \"Output\" means the results of operating a Model as embodied in informational content resulting there from.\n- \"Model\" means any accompanying machine-learning based assemblies (including checkpoints), consisting of learnt weights, parameters (including optimizer states), corresponding to the model architecture as embodied in the Complementary Material, that have been trained or tuned, in whole or in part on the Data, using the Complementary Material.\n- \"Derivatives of the Model\" means all modifications to the Model, works based on the Model, or any other model which is created or initialized by transfer of patterns of the weights, parameters, activations or output of the Model, to the other model, in order to cause the other model to perform similarly to the Model, including - but not limited to - distillation methods entailing the use of intermediate data representations or methods based on the generation of synthetic data by the Model for training the other model.\n- \"Complementary Material\" means the accompanying source code and scripts used to define, run, load, benchmark or evaluate the Model, and used to prepare data for training or evaluation, if any. This includes any accompanying documentation, tutorials, examples, etc, if any.\n- \"Distribution\" means any transmission, reproduction, publication or other sharing of the Model or Derivatives of the Model to a third party, including providing the Model as a hosted service made available by electronic or other remote means - e.g. API-based or web access.\n- \"Licensor\" means the copyright owner or entity authorized by the copyright owner that is granting the License, including the persons or entities that may have rights in the Model and/or distributing the Model.\n- \"You\" (or \"Your\") means an individual or Legal Entity exercising permissions granted by this License and/or making use of the Model for whichever purpose and in any field of use, including usage of the Model in an end-use application - e.g. chatbot, translator, image generator.\n- \"Third Parties\" means individuals or legal entities that are not under common control with Licensor or You.\n- \"Contribution\" means any work of authorship, including the original version of the Model and any modifications or additions to that Model or Derivatives of the Model thereof, that is intentionally submitted to Licensor for inclusion in the Model by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Model, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"\n- \"Contributor\" means Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Model.\n\nSection II: INTELLECTUAL PROPERTY RIGHTS\n\nBoth copyright and patent grants apply to the Model, Derivatives of the Model and Complementary Material. The Model and Derivatives of the Model are subject to additional terms as described in Section III.\n\n2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare, publicly display, publicly perform, sublicense, and distribute the Complementary Material, the Model, and Derivatives of the Model.\n3. Grant of Patent License. Subject to the terms and conditions of this License and where and as applicable, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this paragraph) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Model and the Complementary Material, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Model to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Model and/or Complementary Material or a Contribution incorporated within the Model and/or Complementary Material constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for the Model and/or Work shall terminate as of the date such litigation is asserted or filed.\n\nSection III: CONDITIONS OF USAGE, DISTRIBUTION AND REDISTRIBUTION\n\n4. Distribution and Redistribution. You may host for Third Party remote access purposes (e.g. software-as-a-service), reproduce and distribute copies of the Model or Derivatives of the Model thereof in any medium, with or without modifications, provided that You meet the following conditions:\nUse-based restrictions as referenced in paragraph 5 MUST be included as an enforceable provision by You in any type of legal agreement (e.g. a license) governing the use and/or distribution of the Model or Derivatives of the Model, and You shall give notice to subsequent users You Distribute to, that the Model or Derivatives of the Model are subject to paragraph 5. This provision does not apply to the use of Complementary Material.\nYou must give any Third Party recipients of the Model or Derivatives of the Model a copy of this License;\nYou must cause any modified files to carry prominent notices stating that You changed the files;\nYou must retain all copyright, patent, trademark, and attribution notices excluding those notices that do not pertain to any part of the Model, Derivatives of the Model.\nYou may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions - respecting paragraph 4.a. - for use, reproduction, or Distribution of Your modifications, or for any such Derivatives of the Model as a whole, provided Your use, reproduction, and Distribution of the Model otherwise complies with the conditions stated in this License.\n5. Use-based restrictions. The restrictions set forth in Attachment A are considered Use-based restrictions. Therefore You cannot use the Model and the Derivatives of the Model for the specified restricted uses. You may use the Model subject to this License, including only for lawful purposes and in accordance with the License. Use may include creating any content with, finetuning, updating, running, training, evaluating and/or reparametrizing the Model. You shall require all of Your users who use the Model or a Derivative of the Model to comply with the terms of this paragraph (paragraph 5).\n6. The Output You Generate. Except as set forth herein, Licensor claims no rights in the Output You generate using the Model. You are accountable for the Output you generate and its subsequent uses. No use of the output can contravene any provision as stated in the License.\n\nSection IV: OTHER PROVISIONS\n\n7. Updates and Runtime Restrictions. To the maximum extent permitted by law, Licensor reserves the right to restrict (remotely or otherwise) usage of the Model in violation of this License, update the Model through electronic means, or modify the Output of the Model based on updates. You shall undertake reasonable efforts to use the latest version of the Model.\n8. Trademarks and related. Nothing in this License permits You to make use of Licensors‚Äô trademarks, trade names, logos or to otherwise suggest endorsement or misrepresent the relationship between the parties; and any rights not expressly granted herein are reserved by the Licensors.\n9. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Model and the Complementary Material (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Model, Derivatives of the Model, and the Complementary Material and assume any risks associated with Your exercise of permissions under this License.\n10. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Model and the Complementary Material (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\n11. Accepting Warranty or Additional Liability. While redistributing the Model, Derivatives of the Model and the Complementary Material thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\n12. If any provision of this License is held to be invalid, illegal or unenforceable, the remaining provisions shall be unaffected thereby and remain valid as if such provision had not been set forth herein.\n\nEND OF TERMS AND CONDITIONS\n\n\n\nAttachment A\n\nUse Restrictions\n\nYou agree not to use the Model or Derivatives of the Model:\n- In any way that violates any applicable national, federal, state, local or international law or regulation;\n- For the purpose of exploiting, harming or attempting to exploit or harm minors in any way;\n- To generate or disseminate verifiably false information and/or content with the purpose of harming others;\n- To generate or disseminate personal identifiable information that can be used to harm an individual;\n- To defame, disparage or otherwise harass others;\n- For fully automated decision making that adversely impacts an individual‚Äôs legal rights or otherwise creates or modifies a binding, enforceable obligation;\n- For any use intended to or which has the effect of discriminating against or harming individuals or groups based on online or offline social behavior or known or predicted personal or personality characteristics;\n- To exploit any of the vulnerabilities of a specific group of persons based on their age, social, physical or mental characteristics, in order to materially distort the behavior of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harm;\n- For any use intended to or which has the effect of discriminating against individuals or groups based on legally protected characteristics or categories;\n- To provide medical advice and medical results interpretation;\n- To generate or disseminate information for the purpose to be used for administration of justice, law enforcement, immigration or asylum processes, such as predicting an individual will commit fraud/crime commitment (e.g. by text profiling, drawing causal relationships between assertions made in documents, indiscriminate and arbitrarily-targeted use).\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "imaginairy",
          "type": "tree",
          "content": null
        },
        {
          "name": "mkdocs.yml",
          "type": "blob",
          "size": 1.5634765625,
          "content": "site_name: ImaginAIry\ntheme:\n  name: material\n  features:\n    - content.tabs.link\n    - content.code.copy\n  palette:\n    primary: cyan\n\nmarkdown_extensions:\n  - pymdownx.highlight:\n        anchor_linenums: true\n        line_spans: __span\n        pygments_lang_class: true\n  - mkdocs-click\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.superfences\n  - pymdownx.superfences\n  - pymdownx.tabbed:\n      slugify: !!python/object/apply:pymdownx.slugs.slugify\n        kwds:\n          case: lower\n      alternate_style: true\n  - attr_list\n  - md_in_html\n\nplugins:\n  - search\n  - mkdocstrings:\n      default_handler: python\n      handlers:\n        python:\n          rendering:\n            show_source: true\n\nnav:\n  - Overview: index.md\n  - API Docs:\n      - CLI:\n          - Create Image: docs/CLI/imagine.md\n          - Create Video: docs/CLI/videogen.md\n          - Edit Image: docs/CLI/edit.md\n          - Upscale Image: docs/CLI/upscale.md\n          - Colorize Image: docs/CLI/colorize.md\n          - Describe Image: docs/CLI/describe.md\n      - Python:\n          - imagine(): docs/Python/imagine.md\n          - imagine_image_files(): docs/Python/imagine-image-files.md\n          - generate_video(): docs/Python/generate-video.md\n          - colorize_img(): docs/Python/colorize-img.md\n          - upscale(): docs/Python/upscale.md\n          - ImaginePrompt: docs/Python/ImaginePrompt.md\n          - ControlInput: docs/Python/ControlInput.md\n          - LazyLoadingImage: docs/Python/LazyLoadingImage.md\n          - WeightedPrompt: docs/Python/WeightedPrompt.md\n  - Changelog: changelog.md\n\n"
        },
        {
          "name": "requirements-dev.in",
          "type": "blob",
          "size": 0.2236328125,
          "content": "coverage\nhttpx\nmkdocs-material\nmkdocs-click\nmkdocstrings[python]\nmypy\nruff\npip-tools\npytest\npytest-asyncio\npytest-randomly\npytest-sugar\nresponses\ntypes-pillow\ntypes-psutil\ntypes-requests\ntypes-tqdm\nwheel\n\n-c tests/constraints.txt"
        },
        {
          "name": "requirements-dev.txt",
          "type": "blob",
          "size": 7.658203125,
          "content": "#\n# This file is autogenerated by pip-compile with Python 3.10\n# by the following command:\n#\n#    pip-compile --output-file=requirements-dev.txt requirements-dev.in setup.py\n#\naccelerate==0.34.2\n    # via imaginAIry (setup.py)\nannotated-types==0.7.0\n    # via pydantic\nantlr4-python3-runtime==4.9.3\n    # via omegaconf\nanyio==4.6.0\n    # via\n    #   httpx\n    #   starlette\nbabel==2.16.0\n    # via mkdocs-material\nbuild==1.2.2\n    # via pip-tools\ncertifi==2024.8.30\n    # via\n    #   httpcore\n    #   httpx\n    #   requests\ncharset-normalizer==3.3.2\n    # via requests\nclick==8.1.7\n    # via\n    #   click-help-colors\n    #   click-shell\n    #   imaginAIry (setup.py)\n    #   mkdocs\n    #   mkdocs-click\n    #   mkdocstrings\n    #   pip-tools\n    #   uvicorn\nclick-help-colors==0.9.4\n    # via imaginAIry (setup.py)\nclick-shell==2.1\n    # via imaginAIry (setup.py)\ncolorama==0.4.6\n    # via\n    #   griffe\n    #   mkdocs-material\ncoverage==7.6.1\n    # via -r requirements-dev.in\ndiffusers==0.30.3\n    # via imaginAIry (setup.py)\neinops==0.8.0\n    # via\n    #   imaginAIry (setup.py)\n    #   spandrel\nexceptiongroup==1.2.2\n    # via\n    #   anyio\n    #   pytest\nfastapi==0.115.0\n    # via imaginAIry (setup.py)\nfilelock==3.16.1\n    # via\n    #   diffusers\n    #   huggingface-hub\n    #   torch\n    #   transformers\nfsspec==2024.9.0\n    # via\n    #   huggingface-hub\n    #   torch\nftfy==6.2.3\n    # via\n    #   imaginAIry (setup.py)\n    #   open-clip-torch\nghp-import==2.1.0\n    # via mkdocs\ngriffe==1.3.1\n    # via mkdocstrings-python\nh11==0.14.0\n    # via\n    #   httpcore\n    #   uvicorn\nhttpcore==1.0.5\n    # via httpx\nhttpx==0.27.2\n    # via -r requirements-dev.in\nhuggingface-hub==0.25.0\n    # via\n    #   accelerate\n    #   diffusers\n    #   open-clip-torch\n    #   timm\n    #   tokenizers\n    #   transformers\nidna==3.10\n    # via\n    #   anyio\n    #   httpx\n    #   requests\nimportlib-metadata==8.5.0\n    # via diffusers\niniconfig==2.0.0\n    # via pytest\njaxtyping==0.2.34\n    # via imaginAIry (setup.py)\njinja2==3.1.4\n    # via\n    #   mkdocs\n    #   mkdocs-material\n    #   mkdocstrings\n    #   torch\nkornia==0.7.3\n    # via imaginAIry (setup.py)\nkornia-rs==0.1.5\n    # via kornia\nmarkdown==3.7\n    # via\n    #   mkdocs\n    #   mkdocs-autorefs\n    #   mkdocs-click\n    #   mkdocs-material\n    #   mkdocstrings\n    #   pymdown-extensions\nmarkupsafe==2.1.5\n    # via\n    #   jinja2\n    #   mkdocs\n    #   mkdocs-autorefs\n    #   mkdocstrings\nmergedeep==1.3.4\n    # via\n    #   mkdocs\n    #   mkdocs-get-deps\nmkdocs==1.6.1\n    # via\n    #   mkdocs-autorefs\n    #   mkdocs-material\n    #   mkdocstrings\nmkdocs-autorefs==1.2.0\n    # via\n    #   mkdocstrings\n    #   mkdocstrings-python\nmkdocs-click==0.8.1\n    # via -r requirements-dev.in\nmkdocs-get-deps==0.2.0\n    # via mkdocs\nmkdocs-material==9.5.36\n    # via -r requirements-dev.in\nmkdocs-material-extensions==1.3.1\n    # via mkdocs-material\nmkdocstrings[python]==0.26.1\n    # via\n    #   -r requirements-dev.in\n    #   mkdocstrings-python\nmkdocstrings-python==1.11.1\n    # via mkdocstrings\nmpmath==1.3.0\n    # via sympy\nmypy==1.11.2\n    # via -r requirements-dev.in\nmypy-extensions==1.0.0\n    # via mypy\nnetworkx==3.3\n    # via torch\nninja==1.11.1.1\n    # via optimum-quanto\nnumpy==1.24.4\n    # via\n    #   -c tests/constraints.txt\n    #   accelerate\n    #   diffusers\n    #   imaginAIry (setup.py)\n    #   opencv-python\n    #   optimum-quanto\n    #   scipy\n    #   spandrel\n    #   torchvision\n    #   transformers\nomegaconf==2.3.0\n    # via imaginAIry (setup.py)\nopen-clip-torch==2.26.1\n    # via imaginAIry (setup.py)\nopencv-python==4.10.0.84\n    # via imaginAIry (setup.py)\noptimum-quanto==0.2.4\n    # via imaginAIry (setup.py)\npackaging==24.1\n    # via\n    #   accelerate\n    #   build\n    #   huggingface-hub\n    #   kornia\n    #   mkdocs\n    #   pytest\n    #   pytest-sugar\n    #   transformers\npaginate==0.5.7\n    # via mkdocs-material\npathspec==0.12.1\n    # via mkdocs\npillow==10.4.0\n    # via\n    #   diffusers\n    #   imaginAIry (setup.py)\n    #   torchvision\npip-tools==7.4.1\n    # via -r requirements-dev.in\nplatformdirs==4.3.6\n    # via\n    #   mkdocs-get-deps\n    #   mkdocstrings\npluggy==1.5.0\n    # via pytest\nprotobuf==5.28.2\n    # via imaginAIry (setup.py)\npsutil==6.0.0\n    # via\n    #   accelerate\n    #   imaginAIry (setup.py)\npydantic==2.9.2\n    # via\n    #   fastapi\n    #   imaginAIry (setup.py)\npydantic-core==2.23.4\n    # via pydantic\npygments==2.18.0\n    # via mkdocs-material\npymdown-extensions==10.9\n    # via\n    #   mkdocs-material\n    #   mkdocstrings\npyparsing==3.1.4\n    # via imaginAIry (setup.py)\npyproject-hooks==1.1.0\n    # via\n    #   build\n    #   pip-tools\npytest==8.3.3\n    # via\n    #   -r requirements-dev.in\n    #   pytest-asyncio\n    #   pytest-randomly\n    #   pytest-sugar\npytest-asyncio==0.24.0\n    # via -r requirements-dev.in\npytest-randomly==3.15.0\n    # via -r requirements-dev.in\npytest-sugar==1.0.0\n    # via -r requirements-dev.in\npython-dateutil==2.9.0.post0\n    # via ghp-import\npyyaml==6.0.2\n    # via\n    #   accelerate\n    #   huggingface-hub\n    #   mkdocs\n    #   mkdocs-get-deps\n    #   omegaconf\n    #   pymdown-extensions\n    #   pyyaml-env-tag\n    #   responses\n    #   timm\n    #   transformers\npyyaml-env-tag==0.1\n    # via mkdocs\nregex==2024.9.11\n    # via\n    #   diffusers\n    #   mkdocs-material\n    #   open-clip-torch\n    #   transformers\nrequests==2.32.3\n    # via\n    #   diffusers\n    #   huggingface-hub\n    #   imaginAIry (setup.py)\n    #   mkdocs-material\n    #   responses\n    #   transformers\nresponses==0.25.3\n    # via -r requirements-dev.in\nruff==0.6.7\n    # via -r requirements-dev.in\nsafetensors==0.4.5\n    # via\n    #   accelerate\n    #   diffusers\n    #   imaginAIry (setup.py)\n    #   optimum-quanto\n    #   spandrel\n    #   timm\n    #   transformers\nscipy==1.14.1\n    # via\n    #   imaginAIry (setup.py)\n    #   torchdiffeq\nsentencepiece==0.2.0\n    # via imaginAIry (setup.py)\nsix==1.16.0\n    # via python-dateutil\nsniffio==1.3.1\n    # via\n    #   anyio\n    #   httpx\nspandrel==0.4.0\n    # via imaginAIry (setup.py)\nstarlette==0.38.5\n    # via fastapi\nsympy==1.13.3\n    # via torch\ntermcolor==2.4.0\n    # via\n    #   imaginAIry (setup.py)\n    #   pytest-sugar\ntimm==1.0.9\n    # via\n    #   imaginAIry (setup.py)\n    #   open-clip-torch\ntokenizers==0.19.1\n    # via transformers\ntomli==2.0.1\n    # via\n    #   build\n    #   mypy\n    #   pip-tools\n    #   pytest\ntorch==2.4.1\n    # via\n    #   accelerate\n    #   imaginAIry (setup.py)\n    #   kornia\n    #   open-clip-torch\n    #   optimum-quanto\n    #   spandrel\n    #   timm\n    #   torchdiffeq\n    #   torchvision\ntorchdiffeq==0.2.4\n    # via imaginAIry (setup.py)\ntorchvision==0.19.1\n    # via\n    #   imaginAIry (setup.py)\n    #   open-clip-torch\n    #   spandrel\n    #   timm\ntqdm==4.66.5\n    # via\n    #   huggingface-hub\n    #   imaginAIry (setup.py)\n    #   open-clip-torch\n    #   transformers\ntransformers==4.44.2\n    # via imaginAIry (setup.py)\ntypeguard==2.13.3\n    # via jaxtyping\ntypes-pillow==10.2.0.20240822\n    # via -r requirements-dev.in\ntypes-psutil==6.0.0.20240901\n    # via -r requirements-dev.in\ntypes-requests==2.32.0.20240914\n    # via -r requirements-dev.in\ntypes-tqdm==4.66.0.20240417\n    # via -r requirements-dev.in\ntyping-extensions==4.12.2\n    # via\n    #   anyio\n    #   fastapi\n    #   huggingface-hub\n    #   mypy\n    #   pydantic\n    #   pydantic-core\n    #   spandrel\n    #   torch\n    #   uvicorn\nurllib3==2.2.3\n    # via\n    #   requests\n    #   responses\n    #   types-requests\nuvicorn==0.30.6\n    # via imaginAIry (setup.py)\nwatchdog==5.0.2\n    # via mkdocs\nwcwidth==0.2.13\n    # via ftfy\nwheel==0.44.0\n    # via\n    #   -r requirements-dev.in\n    #   pip-tools\nzipp==3.20.2\n    # via importlib-metadata\n\n# The following packages are considered to be unsafe in a requirements file:\n# pip\n# setuptools\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.025390625,
          "content": "import subprocess\nimport sys\nfrom functools import lru_cache\n\nfrom setuptools import find_packages, setup\n\nis_for_windows = len(sys.argv) >= 3 and sys.argv[2].startswith(\"--plat-name=win\")\n\nif is_for_windows:\n    scripts = None\n    entry_points: dict | None = {\n        \"console_scripts\": [\n            \"imagine=imaginairy.cli.main:imagine_cmd\",\n            \"aimg=imaginairy.cli.main:aimg\",\n        ],\n    }\nelse:\n    scripts = [\"imaginairy/cli/bin/aimg\", \"imaginairy/cli/bin/imagine\"]\n    entry_points = None\n\n\n@lru_cache\ndef get_git_revision_hash() -> str:\n    try:\n        return (\n            subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"])\n            .decode(\"ascii\")\n            .strip()\n        )\n    except FileNotFoundError:\n        return \"no-git\"\n\n\nrevision_hash = get_git_revision_hash()\n\nwith open(\"README.md\", encoding=\"utf-8\") as f:\n    readme = f.read()\n    readme = readme.replace(\n        '<img src=\"',\n        f'<img src=\"https://raw.githubusercontent.com/brycedrennan/imaginAIry/{revision_hash}/',\n    )\n\nsetup(\n    name=\"imaginAIry\",\n    author=\"Bryce Drennan\",\n    # author_email=\"b r y p y d o t io\",\n    version=\"15.0.0\",\n    description=\"AI imagined images. Pythonic generation of images.\",\n    long_description=readme,\n    long_description_content_type=\"text/markdown\",\n    project_urls={\n        \"Documentation\": \"https://github.com/brycedrennan/imaginAIry/blob/master/README.md\",\n        \"Source\": \"https://github.com/brycedrennan/imaginAIry\",\n    },\n    packages=find_packages(include=(\"imaginairy\", \"imaginairy.*\")),\n    scripts=scripts,\n    entry_points=entry_points,\n    package_data={\n        \"imaginairy\": [\n            \"configs/*.yaml\",\n            \"weight_management/weight_maps/*.json\",\n            \"data/*.*\",\n            \"cli/bin/*.*\",\n            \"http_app/stablestudio/dist/*.*\",\n            \"http_app/stablestudio/dist/assets/*.*\",\n            \"http_app/stablestudio/dist/LICENSE\",\n            \"enhancers/phraselists/*.txt\",\n            \"vendored/clip/*.txt.gz\",\n            \"vendored/clipseg/*.pth\",\n            \"vendored/blip/configs/*.*\",\n            \"vendored/noodle_soup_prompts/*.*\",\n            \"vendored/noodle_soup_prompts/LICENSE\",\n            \"vendored/refiners/foundationals/clip/bpe_simple_vocab_16e6.txt.gz\",\n        ]\n    },\n    install_requires=[\n        \"click>=8.0.0\",\n        \"click-help-colors>=0.9.1\",\n        \"click-shell>=2.0\",\n        \"protobuf != 3.20.2, != 3.19.5\",\n        \"fastapi>=0.70.0\",\n        \"ftfy>=6.0.1\",  # for vendored clip\n        \"torch>=2.1.0\",\n        # https://numpy.org/neps/nep-0029-deprecation_policy.html\n        \"numpy>=1.22.0\",\n        \"tqdm>=4.64.0\",\n        \"diffusers>=0.30.3\",\n        \"Pillow>=9.1.0\",\n        \"psutil>5.7.3\",\n        \"omegaconf>=2.1.1\",\n        \"open-clip-torch>=2.0.0\",\n        \"opencv-python>=4.4.0.46\",\n        # need to migration to 2.0\n        \"pydantic>=2.3.0\",\n        # pyparsing used for masking logic and creating text images\n        \"pyparsing>=3.0.0\",\n        \"requests>=2.28.1\",\n        # \"refiners>=0.2.0\",\n        \"jaxtyping>=0.2.23\",  # refiners dependency\n        \"einops>=0.3.0\",\n        \"safetensors>=0.4.0\",\n        \"scipy>=1.8\",\n        \"termcolor\",\n        \"timm>=0.4.12,!=0.9.0,!=0.9.1\",  # for vendored blip\n        \"torchdiffeq>=0.2.0\",\n        \"torchvision>=0.13.1\",\n        \"transformers>=4.19.2\",\n        \"triton>=2.0.0; sys_platform!='darwin' and platform_machine!='aarch64' and sys_platform == 'linux'\",\n        \"kornia>=0.6\",\n        \"uvicorn>=0.16.0\",\n        \"spandrel>=0.1.8\",\n        # \"xformers>=0.0.22; sys_platform!='darwin' and platform_machine!='aarch64'\",\n        \"optimum-quanto>=0.2.4\",  # for flux quantization\n        \"sentencepiece>=0.2.0\",\n        \"accelerate>=0.24.0\",\n    ],\n    # don't specify maximum python versions as it can cause very long dependency resolution issues as the resolver\n    # goes back to older versions of packages that didn't specify a maximum\n    # https://discuss.python.org/t/requires-python-upper-limits/12663/75\n    # https://github.com/brycedrennan/imaginAIry/pull/341#issuecomment-1574723908\n    python_requires=\">=3.10\",\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 0.5830078125,
          "content": "[pytest]\naddopts = --doctest-modules -s --tb=native -v --durations=10\nnorecursedirs = build dist downloads other prolly_delete imaginairy/vendored scripts\nfilterwarnings =\n    ignore::DeprecationWarning\n    ignore::UserWarning\nmarkers =\n    gputest: uses the gpu\n\n[mypy]\nplugins = pydantic.mypy\nexclude = ^(\\./|)(downloads|dist|build|other|testing_support|imaginairy/vendored|imaginairy/modules/sgm)\nignore_missing_imports = True\nwarn_unused_configs = True\nwarn_unused_ignores = False\n\n[mypy-imaginairy.vendored.*]\nfollow_imports = skip\nignore_errors = True\n\n\n[mypy-logging.*]\nignore_errors = True"
        }
      ]
    }
  ]
}