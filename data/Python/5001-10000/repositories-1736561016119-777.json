{
  "metadata": {
    "timestamp": 1736561016119,
    "page": 777,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "imoneoi/openchat",
      "stars": 5274,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.267578125,
          "content": "# VSCode\n.vscode/\n\n# WandB\nwandb/\n\n# Old\nold/\ntemp/\nprofiler/\n\n# Logs\nlogs/\n\n# eval\neval_results/\nevalplus_codegen/\n\n# All datasets\ndataset/\ndataset_processed/\ndataset_processed_*/\ntokenizer/\n\n# All evaluation results\neval_baselines/\neval_results/\neval_results_temp/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.08984375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 20.2939453125,
          "content": "# OpenChat: Advancing Open-source Language Models with Mixed-Quality Data\n\n<div align=\"center\">\n  <img src=\"assets/logo_new.png\" style=\"width: 65%\">\n</div>\n\n<p align=\"center\">\n  <a href=\"https://openchat.team\">üíªOnline Demo</a> |\n  <a href=\"https://huggingface.co/openchat\">ü§óHuggingface</a> |\n  <a href=\"https://arxiv.org/pdf/2309.11235.pdf\">üìÉPaper</a> |\n  <a href=\"https://discord.gg/pQjnXvNKHY\">üí≠Discord</a> \n</p>\n\n- OpenChat is an innovative library of **open-source language models**, fine-tuned with [**C-RLFT**](https://arxiv.org/pdf/2309.11235.pdf) - a strategy inspired by offline reinforcement learning.\n- Our models learn from mixed-quality data without preference labels, delivering exceptional performance on par with `ChatGPT`, even with a `7B` model which can be run on a **consumer GPU (e.g. RTX 3090)**.\n- Despite our simple approach, we are committed to developing a high-performance, commercially viable, open-source large language model, and we continue to make significant strides toward this vision.\n\n[![DOI](https://zenodo.org/badge/645397533.svg)](https://zenodo.org/badge/latestdoi/645397533)\n\n# ‚ú® News\n\n - [2024/05/22] We released the Llama-3 based version [OpenChat 3.6 20240522](https://huggingface.co/openchat/openchat-3.6-8b-20240522), outperforming official Llama 3 8B Instruct and open-source finetunes/merges.\n\n- [2024/01/06] We released the second update, [OpenChat 3.5 0106](openchat/openchat-3.5-0106), further improved coding and overall performance üèÜ.\n\n- [2023/12/10] We released the first update, [OpenChat 3.5 1210](openchat/openchat-3.5-1210), improved coding by 15 points üöÄ.\n\n- [2023/11/01] We released the [OpenChat-3.5-7B](https://huggingface.co/openchat/openchat_3.5) model, surpassing ChatGPT on various benchmarks üî•.\n\n- [2023/09/21] We released our paper [OpenChat: Advancing Open-source Language Models with Mixed-Quality Data](https://arxiv.org/pdf/2309.11235.pdf).\n  \n<details>\n  <summary>Read more</summary>\n  \n- [2023/09/03] We released the [OpenChat V3.2 SUPER]([#models](https://huggingface.co/openchat/openchat_v3.2_super)) model.\n\n- [2023/08/04] We have launched an [Online Demo](https://openchat.team) featuring the latest version, OpenChat 3.2.\n\n- [2023/07/30] We are thrilled to introduce the [OpenChat V3 model series](#models), based on Llama 2, and now available for free for commercial use!\n\n- [2023/07/07] We released the [OpenChat V2 model series](#legacy-models).\n\n- [2023/07/01] We released the [OpenChat V1 model series](#legacy-models).\n</details>\n\n# üè∑Ô∏è Benchmarks - OpenChat 3.6\n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/imoneoi/openchat/master/assets/benchmarks-openchat-3.6-20240522.svg\" style=\"width: 95%;\">\n</div>\n\n<details>\n  <summary>Reproducing benchmarks</summary>\n\nNote: Please run the following commands at the base directory of this repository.\n\n```bash\npython -m ochat.evaluation.run_eval --condition \"GPT4 Correct\" --model openchat/openchat-3.6-8b-20240522 --eval_sets fs_cothub/mmlu fs_cothub/gsm8k fs_cothub/math\npython -m ochat.evaluation.run_eval --condition \"GPT4\" --model openchat/openchat-3.6-8b-20240522 --eval_sets zs/gpqa\n```\n\nHumanEval is run using the official [EvalPlus repository](https://github.com/evalplus/evalplus).\n</details>\n\n# üè∑Ô∏è Benchmarks - OpenChat 3.5\n\n| Model                 | # Params | Average  | MT-Bench     | HumanEval       | BBH MC   | AGIEval  | TruthfulQA    | MMLU         | GSM8K        | BBH CoT     |\n|-----------------------|----------|----------|--------------|-----------------|----------|----------|---------------|--------------|--------------|-------------|\n| **OpenChat-3.5-0106** | **7B**   | **64.5** | 7.8          | **71.3**        | **51.5** | **49.1** | 61.0          | 65.8         | **77.4**     | 62.2        |\n| ChatGPT (March)*      | ???B     | 61.5     | **7.94**     | 48.1            | 47.6     | 47.1     | 57.7          | **67.3**     | 74.9         | **70.1**    |\n|                       |          |          |              |                 |          |          |               |              |              |             |\n| OpenHermes 2.5        | 7B       | 59.3     | 7.54         | 48.2            | 49.4     | 46.5     | 57.5          | 63.8         | 73.5         | 59.9        |\n| OpenOrca Mistral      | 7B       | 52.7     | 6.86         | 38.4            | 49.4     | 42.9     | 45.9          | 59.3         | 59.1         | 58.1        |\n| Zephyr-Œ≤^             | 7B       | 34.6     | 7.34         | 22.0            | 40.6     | 39.0     | 40.8          | 39.8         | 5.1          | 16.0        |\n| Mistral               | 7B       | -        | 6.84         | 30.5            | 39.0     | 38.0     | -             | 60.1         | 52.2         | -           |\n| Open-source SOTA**    | 13B-70B  | 61.4     | 7.71         | 73.2            | 49.7     | 41.7     | 62.3          | 63.7         | 82.3         | 41.4        |\n|                       |          |          | WizardLM 70B | WizardCoder 34B | Orca 13B | Orca 13B | Platypus2 70B | WizardLM 70B | MetaMath 70B | Flan-T5 11B |\n\nüî• OpenChat-3.5-0106 (7B) now outperforms Grok-0 (33B) on **all 4 benchmarks** and Grok-1 (314B) on average and **3/4 benchmarks**.\n\n|                       | License     | # Param | Average  | MMLU   | HumanEval | MATH     | GSM8k    |\n|-----------------------|-------------|---------|----------|--------|-----------|----------|----------|\n| **OpenChat-3.5-0106** | Apache-2.0  | **7B**  | **61.0** | 65.8   | **71.3**  | **29.3** | **77.4** |\n| Grok-0                | Proprietary | 33B     | 44.5     | 65.7   | 39.7      | 15.7     | 56.8     |\n| Grok-1                | Proprietary | 314B    | 55.8     | **73** | 63.2      | 23.9     | 62.9     |\n\n<details>\n  <summary>Evaluation details</summary>\n*: ChatGPT (March) results are from GPT-4 Technical Report, Chain-of-Thought Hub, and our evaluation.\n\n^: Zephyr-Œ≤ often fails to follow few-shot CoT instructions, likely because it was aligned with only chat data but not trained on few-shot data.\n\n **: Mistral and Open-source SOTA results are taken from reported results in instruction-tuned model papers and official repositories.\n\nAll models are evaluated in chat mode (e.g. with the respective conversation template applied). All zero-shot benchmarks follow the same setting as in the AGIEval paper and Orca paper. CoT tasks use the same configuration as Chain-of-Thought Hub, HumanEval is evaluated with EvalPlus, and MT-bench is run using FastChat. To reproduce our results, follow the instructions below.\n</details>\n\n<details>\n  <summary>Reproducing benchmarks</summary>\n\nReasoning and Coding:\n\nNote: Please run the following commands at the base directory of this repository.\n\n```bash\npython -m ochat.evaluation.run_eval --condition \"GPT4 Correct\" --model openchat/openchat-3.5-0106 --eval_sets coding fs_cothub/bbh fs_cothub/mmlu zs/agieval zs/bbh_mc_orca zs/truthfulqa_orca\npython ochat/evaluation/view_results.py\npython ochat/evaluation/convert_to_evalplus.py\n```\n\nThen all humaneval code samples are placed in `ochat/evaluation/evalplus_codegen`. Use the following command to evaluate an individual code sample named `samples.jsonl` using Docker as a sandbox.\n\n```bash\ndocker run -v $(pwd):/app ganler/evalplus:latest --dataset humaneval --samples samples.jsonl\n```\n\nMathematical Reasoning:\n\nNote: Please run the following commands at the base directory of this repository.\n\n```bash\npython -m ochat.evaluation.run_eval --condition \"Math Correct\" --model openchat/openchat-3.5-0106 --eval_sets fs_cothub/gsm8k zs/math\npython ochat/evaluation/view_results.py\n```\n\nMT-Bench:\n\nPlease first launch a local API server, then download FastChat and run the following commands.\n\nNote: Due to non-zero temperature and GPT-4 API changes over time, there might be variations in the results.\n\n```bash\ncd fastchat/llm_judge\npython gen_api_answer.py --model openchat-3.5-0106 --max-tokens 4096 --parallel 128 --openai-api-base http://localhost:18888/v1\npython gen_judgment.py --model-list openchat-3.5-0106 --parallel 8 --mode single\n```\n\n</details>\n\n# ‚¨áÔ∏è Installation\n> [!NOTE]\n> Need [`pytorch`](https://pytorch.org/get-started/locally/#start-locally) and [CUDA](https://developer.nvidia.com/cuda-toolkit-archive) to run OpenChat\n\n## pip\n\n```bash\npip3 install ochat\n```\n> [!IMPORTANT]\n> If you are facing package compatibility issues with pip, try the conda method below or check [this issue](https://github.com/imoneoi/openchat/issues/41)\n\n## conda\n\n```bash\nconda create -y --name openchat python=3.11\nconda activate openchat\n\npip3 install ochat\n```\n\n## Windows (WSL 1.x, Ubuntu-22.04)\n\n```bash\nsudo apt update\nsudo apt install build-essential\n\nsudo apt install -y curl\ncurl -o miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash miniconda.sh\n\n# Restart WSL terminal if the following conda command does not work\n\nconda create -y --name openchat python=3.11\nconda activate openchat\n\npip3 install ochat\n```\n\n## From source\n\n<details>\n  <summary>Clone this repo and install openchat from source in editable mode</summary>\n\n```bash\ngit clone https://github.com/imoneoi/openchat\ncd openchat\n\npip3 install --upgrade pip  # enable PEP 660 support\npip3 install -e .  # Editable mode, you can make changes in this cloned repo\n```\n</details>\n\n# üöÄ Deploying API server\n\n‚ö° Our API server is ready for production use and compatible with the OpenAI API protocol. It is highly optimized with vLLM and can dynamically batch requests.\n\nüìé Note: For 20 series or older GPUs that do not support `bfloat16`, add `--dtype float16` to the server args.\n\n### List of currently supported models\n\n| MODEL_TYPE   | MODEL_REPO                                                                                    | License    |\n|--------------|-----------------------------------------------------------------------------------------------|------------|\n| openchat_3.6 | [openchat/openchat-3.6-8b-20240522](https://huggingface.co/openchat/openchat-3.6-8b-20240522) | Llama 3    |\n| openchat_3.5 | [openchat/openchat-3.5-0106](https://huggingface.co/openchat/openchat-3.5-0106)               | Apache 2.0 |\n\n### For a single GPU (e.g. RTX 3090, 4090)\n\n```bash\npython -m ochat.serving.openai_api_server --model MODEL_REPO\n```\n\n### For multiple GPUs (tensor parallel)\n\n```bash\n# N is the number of tensor parallel GPUs\npython -m ochat.serving.openai_api_server --model MODEL_REPO --engine-use-ray --worker-use-ray --tensor-parallel-size N\n```\n\nuse `-h` to see more settings\n```bash\npython -m ochat.serving.openai_api_server --model MODEL_REPO -h\n```\n\n<details>\n  <summary>Deploy as online service</summary>\n\nIf you want to deploy the server as an online service, you can use `--api-keys sk-KEY1 sk-KEY2 ...` to specify allowed API keys and `--disable-log-requests --disable-log-stats --log-file openchat.log` for logging only to a file. For security purposes, we recommend using an [HTTPS gateway](https://fastapi.tiangolo.com/es/deployment/concepts/#security-https) in front of the server.\n\n</details>\n\n## Request example\n\nOnce started, the server listens at `localhost:18888` for requests and is compatible with the [OpenAI ChatCompletion API specifications](https://platform.openai.com/docs/api-reference/chat). \n\nüí° **Default Mode (GPT4 Correct)**: Best for coding, chat and general tasks\n\n```bash\ncurl http://localhost:18888/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"MODEL_TYPE\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"You are a large language model named OpenChat. Write a poem to describe yourself\"}]\n  }'\n```\n\nüßÆ **Mathematical Reasoning Mode**: Tailored for solving math problems\n\n```bash\ncurl http://localhost:18888/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"MODEL_TYPE\",\n    \"condition\": \"Math Correct\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"10.3 ‚àí 7988.8133 = \"}]\n  }'\n```\n\n# <a id=\"web-ui\"></a> üåê Web UI - [OpenChat-UI](https://github.com/imoneoi/openchat-ui)\n\nAfter launching the API server, OpenChat provide user interface that easy to interact with. [Click here to check Web UI](https://github.com/imoneoi/openchat-ui)\n\n# ü§ó Inference with Transformers\n\n> [!WARNING]\n> It's recommended to use our optimized API server for deployment. Inferencing with Transformers will be slower.\n\nüí° **Default Mode (GPT4 Correct)**: Best for coding, chat and general tasks\n\n```\nGPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: How are you today?<|end_of_turn|>GPT4 Correct Assistant:\n```\n\nüßÆ **Mathematical Reasoning Mode**: Tailored for solving math problems\n\n```\nMath Correct User: 10.3 ‚àí 7988.8133=<|end_of_turn|>Math Correct Assistant:\n```\n\n‚ö†Ô∏è **Notice:** Remember to set `<|end_of_turn|>` as end of generation token.\n\nThe default (GPT4 Correct) template is also available as the integrated `tokenizer.chat_template`, which can be used instead of manually specifying the template.\n\n# <a id=\"training\"></a> üõ†Ô∏è Training\n\nThe OpenChat training system utilizes padding-free training and the [Multipack Sampler](https://github.com/imoneoi/multipack_sampler), achieving a **3~10x** speedup compared to the conventional padded training.\n\n## Choose a base model\n\nOpenChat supports Llama 3 and Mistral models. Please first choose a base model to fit your needs. Each base model has a corresponding weight repo, model type, and recommended batch size as listed below, they should be filled into `BASE_REPO`, `MODEL_TYPE`, and `BATCH_SIZE` in the following instructions.\n\n| Base Model | Size | Weights (with EOT token)                   | Model Type              | Recommended Batch Size per GPU (8xA100 80GB) |\n|------------|------|--------------------------------------------|-------------------------|----------------------------------------------|\n| Llama 3    | 8B   | `imone/Llama-3-8B-fixed-special-embedding` | `openchat_3.6`          | 40960                                        |\n| Mistral    | 7B   | `imone/Mistral_7B_with_EOT_token`          | `openchat_v3.2_mistral` | 77824                                        |\n\nNote: The OpenChat conversation template requires `<|eot_id|>, <|start_header_id|>, <|end_header_id|>` (Llama 3) `<|end_of_turn|>` (Mistral) special tokens. The base model specified must include these tokens with initialized embeddings. Our provided weights are the original base weights with this token added and embeddings initialized. If you want to add them manually, use the `init_special_embedding_llama3.py` or `mistral_add_tokens.py` in the `scripts` directory.\n\n## Installing DeepSpeed and Flash Attention\n\nFirst, ensure that the CUDA `nvcc` compiler is available in your environment. If it is not, install the CUDA toolkit that matches the version used by PyTorch.\n\nNext, install building dependencies:\n\n```bash\npip install packaging ninja\n```\n\nFinally, install the packages:\n\n```bash\npip install deepspeed flash-attn\n```\n\n### Preparing Your Data\n\nTo utilize the OpenChat trainer, prepare your SFT data into a JSON Lines format where each line corresponds to a `Conversation` object:\n\n```python\nclass Message(BaseModel):\n    role: str     # Must be \"user\" or \"assistant\"\n    content: str  # Message content\n    weight: Optional[float] = None  # Loss weight for this message. Typically 0 for user and 1 for assistant to supervise assistant's responses only\n\n\nclass Conversation(BaseModel):\n    items: List[Message]  # All messages within the conversation\n    condition: str = \"\"  # C-RLFT condition, can be any string or empty.\n    system: str = \"\"  # System message for this conversation\n```\n\nFor basic SFT, assign `weight` as `0` for human messages and `1` for assistant responses.\n\nSFT example:\n\n```json\n{\"items\":[{\"role\":\"user\",\"content\":\"Hello\",\"weight\":0.0},{\"role\":\"assistant\",\"content\":\"Hi\",\"weight\":1.0},{\"role\":\"user\",\"content\":\"How are you today?\",\"weight\":0.0},{\"role\":\"assistant\",\"content\":\"I'm fine.\",\"weight\":1.0}],\"system\":\"\"}\n{\"items\":[{\"role\":\"user\",\"content\":\"Who are you?\",\"weight\":0.0},{\"role\":\"assistant\",\"content\":\"I'm OpenChat.\",\"weight\":1.0}],\"system\":\"You are a helpful assistant named OpenChat.\"}\n```\n\nFor C-RLFT, `condition` should be set as the class the conversation belongs to (e.g. `GPT3` or `GPT4`). The `weight` is assigned as `0` for human messages and `w` for assistant responses, where `w` is the weight of the class (e.g. `0.1` for `GPT3` and `1` for `GPT4`, as found in our C-RLFT paper).\n\nC-RLFT example:\n\n```json\n{\"items\":[{\"role\":\"user\",\"content\":\"What is C-RLFT?\",\"weight\":0.0},{\"role\":\"assistant\",\"content\":\"C-RLFT is a method for improving open-source LLMs with mixed-quality data.\",\"weight\":1.0}],\"condition\":\"GPT4\",\"system\":\"\"}\n{\"items\":[{\"role\":\"user\",\"content\":\"What is C-RLFT?\",\"weight\":0.0},{\"role\":\"assistant\",\"content\":\"I don't know.\",\"weight\":0.1}],\"condition\":\"GPT3\",\"system\":\"\"}\n```\n\n### Pre-tokenizing the Dataset\n\nYou'll then need to pre-tokenize the dataset using the command (please specify a filename as `PRETOKENIZED_DATA_OUTPUT_PATH` to store the pretokenized dataset):\n\n```bash\npython -m ochat.data.generate_dataset --model-type MODEL_TYPE --model-path BASE_REPO --in-files data.jsonl --out-prefix PRETOKENIZED_DATA_OUTPUT_PATH\n```\n\n### Launching the OpenChat Trainer\n\nYou can now launch the OpenChat trainer using the command below.\n- 13B model requires eight `A/H100s` with 80GB VRAM\n- 7B model can be trained with four `A/H100s` with 80GB VRAM or eight `A/H100s` with 40GB VRAM.\n\nFor hyperparameters, we recommend first setting the batch size to the recommended batch size. If OOM occurs, try setting it to the exact maximum that VRAM can hold and as a multiple of `2048`.\nOther hyperparameters have been carefully selected as the default. Furthermore, the learning rate is automatically determined based on the [inverse square-root rule](https://arxiv.org/abs/2006.09092).\n\n<details>\n\n<summary>Training Commands (click to expand)</summary>\n\n```bash\nNUM_GPUS=8\n\ndeepspeed --num_gpus=$NUM_GPUS --module ochat.training_deepspeed.train \\\n          --model_path BASE_REPO \\\n          --data_prefix PRETOKENIZED_DATA_OUTPUT_PATH \\\n          --save_path PATH_TO_SAVE_MODEL \\\n          --batch_max_len BATCH_SIZE \\\n          --epochs 5 \\\n          --save_every 1 \\\n          --deepspeed \\\n          --deepspeed_config ochat/training_deepspeed/deepspeed_config.json\n```\n\n</details>\n\nYou can find checkpoints of all epochs in `PATH_TO_SAVE_MODEL`. Then you may evaluate each epoch and choose the best one.\n\n# Limitations\n\n**Foundation Model Limitations**: Despite its advanced capabilities, OpenChat is still bound by the limitations inherent in its foundation models. These limitations may impact the model's performance in areas such as:\n\n - Complex reasoning\n - Mathematical and arithmetic tasks\n - Programming and coding challenges\n\n**Hallucination of Non-existent Information:** OpenChat may sometimes generate information that does not exist or is not accurate, also known as \"hallucination\". Users should be aware of this possibility and verify any critical information obtained the model.\n\n**Safety:** OpenChat may sometimes generate harmful, hate speech, biased responses, or answer unsafe questions. It's crucial to apply additional AI safety measures in use cases that require safe and moderated responses.\n\n# License\n\nCode is distributed under the **Apache License 2.0**.\n\n# Citation\n\n```\n@article{wang2023openchat,\n  title={OpenChat: Advancing Open-source Language Models with Mixed-Quality Data},\n  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},\n  journal={arXiv preprint arXiv:2309.11235},\n  year={2023}\n}\n```\n\n# üíåContact\n\n**Project Lead:**\n- Guan Wang [imonenext at gmail dot com]\n- [Alpay Ariyak](https://github.com/alpayariyak) [aariyak at wpi dot edu]\n\n**Main Contributors:**\n- [Xianyuan Zhan](https://scholar.google.com.hk/citations?user=pDMnGloAAAAJ) (Tsinghua University)\n- Qiying Yu (Tsinghua University)\n- Changling Liu (GPT Desk Pte. Ltd.)\n- LDJ\n- AutoMeta (Alignment Lab AI)\n\n**Sponsors:**\n- [Sen Song](https://scholar.google.com/citations?user=cYgtRP4AAAAJ) (Tsinghua University)\n- [Yang Liu](https://nlp.csai.tsinghua.edu.cn/~ly/) (Tsinghua University)\n- [01.AI Company](https://www.lingyiwanwu.com/en)\n- [RunPod](https://www.runpod.io/)\n\n**Special Thanks:**\n - [Mistral](https://mistral.ai/)\n - [Chain-of-Thought Hub](https://github.com/FranxYao/chain-of-thought-hub)\n - [Llama 2](https://ai.meta.com/llama/)\n - [Self-Instruct](https://arxiv.org/abs/2212.10560)\n - [FastChat (Vicuna)](https://github.com/lm-sys/FastChat)\n - [Alpaca](https://github.com/tatsu-lab/stanford_alpaca.git)\n - [StarCoder](https://github.com/bigcode-project/starcoder)\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 1.0068359375,
          "content": "# Security Policy\n\n## Supported Versions\n\n| Version | Supported          |\n|---------|--------------------|\n| 3.x     | :white_check_mark: |\n| < 3.0   | :x:                |\n\n## Reporting a Vulnerability\n\nWe take security vulnerabilities in our open-source project seriously and appreciate responsible disclosure from users.\n\nIf you believe you have found a security vulnerability in our project, please report it to us by creating a Github issue with the label \"security\" or \"vulnerability\". Please do not publicly disclose the vulnerability until it has been addressed by our project team.\n\nWe will acknowledge receipt of your vulnerability report and will keep you informed of our progress in addressing the vulnerability. If you would like to communicate with us about the vulnerability, please email [imonenext at gmail dot com].\n\nWe will not take legal action against users who report vulnerabilities in good faith and in accordance with this disclosure policy.\n\nThank you for helping us keep our open-source project secure!\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "ochat",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.185546875,
          "content": "[build-system]\nrequires = [\"setuptools>=45\", \"setuptools_scm[toml]>=6.2\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ochat\"\ndescription = \"An efficient framework for training and serving top-tier, open-source conversational LLMs.\"\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\ndynamic = [\"version\"]\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: Apache Software License\",\n]\ndependencies = [\n    \"colorama\",\n    \"beautifulsoup4\", \n    \"markdownify\", \n    \"pylatexenc\",\n    \"sympy\",\n    \"openai>=1\",\n    \"tenacity\", \n    \"tiktoken\", \n    \"tqdm\", \n    \"wandb\", \n    \"numba\", \n    \"datasets\", \n    \"orjson\", \n    \"torch\", \n    \"packaging\",\n    \"ninja\",\n    \"flash-attn\",\n    \"ray\", \n    \"sentencepiece\", \n    \"transformers>=4.40.1\", \n    \"accelerate\", \n    \"protobuf\",\n    \"fastapi\", \n    \"pydantic\", \n    \"shortuuid\", \n    \"uvicorn\", \n    \"vllm>=0.4.0\",\n    \"pytest\"\n]\n\n[project.urls]\n\"Homepage\" = \"https://github.com/imoneoi/openchat\"\n\"Bug Tracker\" = \"https://github.com/imoneoi/openchat/issues\"\n\n[tool.setuptools.packages.find]\nexclude = [\"assets*\", \"ochat/experimental*\"]\n\n[tool.wheel]\nexclude = [\"assets*\", \"ochat/experimental*\"]\n\n[tool.setuptools_scm]"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.056640625,
          "content": "[pytest]\nmarkers = \n    cpu: CPU Tests\n    gpu: GPU Tests\n"
        }
      ]
    }
  ]
}