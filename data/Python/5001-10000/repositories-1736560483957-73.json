{
  "metadata": {
    "timestamp": 1736560483957,
    "page": 73,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "speechbrain/speechbrain",
      "stars": 9167,
      "defaultBranch": "develop",
      "files": [
        {
          "name": ".dict-speechbrain.txt",
          "type": "blob",
          "size": 8.8447265625,
          "content": "## Contents ##\n\n### Common Words for use in Compound Words ###\n### Compound Words With 1 or 2 letter Words ###\n### Jargon ###\n### Names ###\nHoulsby\n### British ###\n### Non-English ###\n\n####### Common Words for use in Compound Words #######\nacoustics\nbar\ncsv\ndummy\nestimator\nfile\nfor\nform\nforms\nfunction\nhat\nimage\ninter\nintra\nlabels\nmax\nmin\nmini\nmix\npath\nrange\nspeech\ntest\nwriter\nyour\n\n####### Compound Words With 1 or 2 letter Words #######\naMax\naRange\nasArray\nasType\natleast\nBackW\nbFloat\nbname\nbNorm\nbSampler\nbSize\nbyLoss\nbySource\ncheckIs\ncleanID\ncMap\ndistP\ndistQ\ndLoss\ndModel\ndScore\ndSet\ndType\neMax\nfBank\nfBanks\nfileID\nfOut\nfPath\nfStream\ngNoise\ngOpen\nhcand\nhCat\nhHat\nhStack\nidword\ninProceedings\nisEmpty\nisGenerator\nisreal\nistop\nitTop\nKmax\nkMeans\nkNeighbors\nkWeight\nlLoss\nMANHATTANINJANUARY\nmaskL\nmaskR\nmDay\nmodelFt\nmvNorm\nmychannel\nmyFile\nmyFormat\nmyRecipe\nnArray\nnChannels\nnCols\nnElement\nnEvent\nnFolds\nnMap\nnMax\nnMixtures\nnNodes\nnodeID\nnoHash\nnoProgress\nnoRecurseDirs\nnoSignatures\nnRef\nnRows\nnSamples\nnSub\noClock\noneOf\npBar\npName\npRange\npushTo\nqWeight\nrArrow\nrFilename\nrTend\nRUNNERUP\nsBatch\nsControl\nskipIf\nsLonger\nsourceL\nsourceR\nsprintF\nsType\ntextid\ntGap\ntMax\ntMin\ntoArray\ntoList\ntransformerLM\ntSeg\ntSet\ntSize\ntStart\ntStop\ntZip\nuRandom\nvBias\nvDim\nvHead\nvWeight\nwDay\nwGap\nwhamr\nwhats\nWmax\nxAxis\nxHat\nxLabel\nxLim\nxMix\nxScores\nXspec\nxTick\nxTicks\nxVect\nxVector\nxVectors\nxVects\nyAxis\nyDay\nyLabel\nyLim\nyMat\nyTickLabels\nyTicks\nZpad\n\n####### Jargon #######\naabbbb\naccum\naccumarray\naccumulatable\nacorr\nactiv\nactivlev\nadsp\nadspvqe\nAEIOUÁÉÍÓÚ\nÁÉÍÓÚáéíóú\naiox\nalffa\nalived\nannot\nans\narpa\narpack\narxiv\nasind\nattns\naudiomnist\naverager\nawgn\nbbcc\nbiquad\nbiquads\nbleu\nblmf\nblstm\nblunkett\nbmbf\nbmlf\nbmlh\nbrir\ncafile\ncand\ncant\ncatl\ncatr\ncbak\ncbaks\ncertifi\ncffinit\ncfgs\nchans\ncheby\ncheybyshev\nchilds\nchkarada\nchnl\nchnls\nchrs\ncipic\nCKPT\nckpts\nclsname\nclstm\nclust\ncnrst\ncoef\ncoefs\ncoer\ncolab\ncomplexlstm\ncomplexrnn\nconcated\nconsideree\nconvblocks\nconvenc\nconvolutional\nconvs\nconvtasnet\nconvtranspose\ncouldnt\ncovl\ncovls\ncpulm\nCRDNN\ncrnn\ncsgraph\ncsgu\ncsig\ncsigs\ncsvf\ncsvs\ncudatoolkit\ncudnn\ncver\ncycliclrloader\ncycliclrsaver\ndatafreqs\ndataio\ndatio\ndawalatn\ndblp\ndbstop\ndcconv\ndclassifier\ndcnn\nddpm\nddwkim\ndels\ndemixing\ndenoised\ndenoises\ndenoising\ndenorm\ndenormalize\ndenormalized\ndependee\ndepod\ndequantized\ndereverb\ndereverberation\nders\ndeterminize\ndetokenization\ndetokenize\ndetokenized\ndetokenizer\ndetokenizes\ndets\ndevel\ndfcn\ndffn\ndfilters\ndiar\ndiarization\ndiarize\ndiarizes\nDiarizing\ndictify\ndiDataset\ndidatasets\ndidnt\ndisambig\ndiscretized\ndiscretizes\ndnns\ndnsmos\ndnsmsos\ndoas\ndocherty\ndprnn\ndrawio\ndualpathrnn\ndurs\ndvoice\ndwfst\ndynbatch\ndynchunktrain\neder\neend\neess\nEigenvoice\neigh\neigsh\nelbo\nelems\nellip\nelra\nembs\nemiss\nemoid\nemovdb\nencodec\nenhc\nepaca\nestoi\nests\netal\nevals\nevaluatable\nevecs\nfalarm\nfftby\nfftconvolve\nffts\nfilt\nfinfo\nfinv\nfitzooth\nflac\nfltp\nfo\nfoos\nfpr\nfreqs\nfro\nfromx\nfsa's\nfsas\nfstaddselfloops\nfuncs\nfunct\nfwhm\nfwhms\nfzero\ngalc\ngelu\ngenbmm\ngevd\nginv\ngloo\nglorot\ngndr\ngptmodel\ngpulm\ngroakat\nhann\nhhpf\nhibs\nhifi\nhinne\nhlpf\nhparam\nhparams\nhpfit\nhpopt\nhrtf\nhyperparam\nhyperparams\nhyperpyyaml\nhyps\nicassp\nicml\nidcs\nidxs\niemocap\niemocapie\nifft\niloc\nimpr\nimshow\ninclusivity\nindcs\nindi\ninds\nindx\nindxs\ninitialising\ninpfid\ninpricey\ninpt\ninsig\niowait\nipdb\niref\nirfft\nirit\nisdst\nisft\nissn\nisspmatrix\nisstruct\nissubseq\nISTFT\nisys\niterrows\nivar\niwbeg\niwend\niwslt\njasonfu\njitify\njlcorpus\njsonl\njsonlines\njspeech\nkaldi\nKaldi's\nkaldilm\nkarpathy's\nkbit\nkbps\nkdim\nkeepmodidx\nkeepsegidx\nkeepsegs\nkeyfuncs\nkldiv\nklen\nkmean\nKpad\nkspon\nkwags\nkwonlyargs\nlabse\nlangengullís\nlbrain\nldir\nlemma\nlemmatize\nlerp\nlevdb\nlibeigen\nlibnvvm\nLIBRI\nlibritts\nLibry\nlibsndfile\nlicenced\nljspeech\nlmctc\nlmnt\nlmplz\nlogit\nlogprobs\nlpcoeff\nlpparams\nlrec\nlrelu\nlstm\nmadda\nmathjax\nmatvec\nmatvecmul\nmaxfilt\nmaxvecmul\nmbart\nmcgregor\nmels\nmetafname\nmfcc\nmfccs\nmhaxl\nmhsa\nmimo\nminiters\nmisversioned\nmlps\nmnist\nmodelize\nmodelizes\nmodernisation\nmsed\nmseg\nmstacotron\nmulaw\nmult\nmultiwoz\nmvdr\nmvec\nmwoz\nmyrir\nnans\nnargin\nnargout\nnbest\nnbin\nnccl\nncor\nnd\nndarray\nndim\nndims\nnegs\nnelems\nnerr\nnesterov\nNeuro\nnfft\nngram\nngramlm\nngrams\nnhead\nnikvaessen\nnllb\nnmfbrain\nnmfdictionary\nnmfencoder\nnnet\nnoisifier\nnonl\nnoqa\nnproc\nnprocs\nnsamp\nnspk\nnspks\nnsys\nntasks\nnumbapro\nnumel\nnumlayers\nnumpy\nnums\nnumspks\nnvvm\nnwerr\nNYU's\noclock\noemax\noen\nOmniglot\nonnxruntime\nonwsj\nopenfst\nopenrir\noptim\nosama\not\novrl\npaedophiles\nparametrizations\npcen\npcolormesh\npdns\npeft\nperc\npercactive\nperturbator\npesq\npesqs\npfxuc\nphix\nphns\nplda\npmul\npooler\npreds\nprelu\nprobs\nprocs\npuml\npunc\npval\npyctcdecode\npydoclint\npydub\npygtrie\npyin\npyln\npyloudnorm\npymodule\npypa\npyplot\npyRoom\npysndfx\npystoi\npythonic\nPytorch\nPytorch's\nqcnn\nqlen\nqlstm\nqrnn\nquantisers\nquantizer\nquantizers\nquaternionli\nquaternionlstm\nquaternionrnn\nrandn\nravdess\nrcoeff\nrecommonmark\nreducelronplateau\nrefactorings\nrefcoeff\nreimplementation\nreinit\nReinitializes\nrelis\nrelu\nrenorm\nrenormalize\nrenormalized\nrepar\nreparameterization\nreparameterize\nresamplers\nresepf\nresepformer\nresynth\nresynthesized\nresynthesizing\nrevb\nreverbed\nreverberance\nrfft\nrgen\nrirs\nrnnlm\nrnnlmrescorer\nrnnn\nrnnp\nrnnt\nrtbeg\nrtdur\nrtmid\nrttm\nrttms\nrtxa\nrtype\nruamel\nrwbeg\nrwdur\nrwend\nsamu\nsasx\nsbrnn\nsbtf\nscalarize\nscipy\nsconv\nsdrs\nsegan\nseglist\nseglstm\nsegs\nsegset\nsegsets\nsegsnr\nsents\nseptillionths\nseqlm\nseqs\nser\nsers\nsess\nsetu\nsextillionths\nsidx\nsigm\nsils\nsilu\nsimu\nsincconv\nsinr\nsisnr\nsisnrs\nslaney\nsligru\nslogdet\nsnr\nsnrlevels\nsnrs\nsnts\nsoxi\nspacy\nspecif\nspectr\nsphs\nspkid\nspkr\nspkrdata\nspkrec\nspkrs\nspks\nsplitted\nsrate\nsrmr\nsrmrpy\nsrnn\nsrpphat\nsrun\nsseg\nssegs\nssim\nssnr\nstds\nSTFT\nstnorm\nstoi\nstois\nstrt\nsubakany\nsubseg\nsubsegs\nsubt\nsubtokenization\nsubtokenizations\nsubtrs\nsvdl\nswbd\nsyss\ntarg\ntaslp\ntbeg\nTDNN\ntdoa\ntdoas\ntdur\ntexthvc\nthats\ntheyre\ntids\ntimit\ntjoint\nTLDR\ntmid\ntocoo\ntocsr\ntodia\ntoeplitz\ntokenizable\ntokenizes\ntopk\ntopo\ntovl\ntqdm\ntrac\ntransformerlmrescorer\ntriu\ntrnpath\ntrous\ntxts\nUkranian\nuncond\nuncondition\nunderdogliu\nundoc\nunet\nunflatten\nunflattened\nunlex\nunmixing\nunnormalized\nunorm\nunpadded\nunquantized\nunscale\nunsq\nunsqueeze\nunsqueezed\nunsqueezes\nupalign\nupdown\nuttid\nuttr\nutts\nvals\nvctk\nvecs\nvect\nvectorize\nveri\nville\nvocav\nvoceleb\nvocoding\nvocos\nvorbis\nvqgan\nVtrans\nwagnerdo\nwandb\nwav\nwavlm\nwavs\nwavscp\nwavxk\nwbeg\nwdur\nwebrtcvad\nwfst\nwids\nwinit\nwlen\nwnormandskip\nwordemb\nwordid\nwpsb\nwrds\nwsjmix\nwtyp\nxargs\nxlsr\nxmls\nxponent\nyamls\nyoure\nӿéæœâçèàûî\n\n####### Names #######\nAbdel\nAbdelmoumene\nAbdelwahab\nAbdou\nAbous\nAdel\nadiyoss\nAichner\nAlaa\nAlgayres\nAlgazi\nAlghisi\nAlumäe\nAlya\nAndreas\nAravind\nAris\nArjun\nARNIE\nArseniy\nArtem\nASRU\nAvendano\nAwni\nBahdanau\nBain\nBengio\nBenoit\nBernd\nBonafonte\nBorra\nBougares\nBoumadane\nBrakel\nBronzi\nBulut\nBusso\nCámbara\nCaubriere\nChaabani\nChebyshev\nCheng\nChieh\nChien\nChoi\nChorowski\nChun\nCoeff\nComberts\nConcordia\nCuda\nDarija\nDavide\nDawalatabad\nDhivehi\nDimas\nDiola\nDominik\nDubey\nDuchêne\nDuda\nDuret\nEbrahim\nEcapa\nEddine\nEfthymios\nEigen\nÉmile\nEmov\nEmre\nEskimez\nEssid\nEstève\nFance\nFarrens\nFeng\nFethi\nFiras\nFlorentin\nFongbe\nFosler\nFrancesco\nFujita\nGabor\nGaëlle\nGahbiche\nGaudet\nGaussianly\nGdrive\nGenabith\nGeorgios\nGetreuer\nGhannay\nGopal\nGorin\nGradio\nGrondin\nGuimarães\nGuoguo\nHakha\nHannes\nHanning\nHanzi\nHarishchandra\nHeba\nHeitor\nHifigan\nhnguyen\nHsieh\nHsuan\nHwidong\nHyun\nINSEA\nIvana\nIwhmdeo\nJabaian\nJacoby\nJarod\nJenie\nJenthe\nJeong\nJeongkyu\nJiang\nJianyuan\nJunkai\nKandarkar\nKappenman\nKarakasidis\nKazemzadeh\nKenlm\nKhudanpur\nKiefer\nKinyarwanda\nKlatt's\nKleit\nKlocmax\nKorbayová\nKruijff\nKuang\nKullback\nKürzinger\nLangevin\nLaperrière\nLeibler\nLigru\nLimame\nLinv\nLuca\nLudwigsfelde\nLussier\nLuxembourgish\nMakuhari\nMangolian\nMatusevych\nMaurizio\nMdhaffar\nMesgarani\nMesse\nMickael\nMila\nMirco\nMirko\nMohonk\nMontréal\nMori\nMotlicek\nMoumen\nMousavi\nMünchen\nMusan\nNarayanan\nNauman\nNdel\nNfissi\nNima\nNins\nNoam\nNumba\nNyquist\nOccitan\nOmologo\nOpenAI\nOríon\nPaissan\nPaltz\nPanayotov\nPapreja\nParcollt\nPascual\nPavlo\nPelloin\nPetr\nPiyush\nPlantinga\nPloujnikov\nPopen\nPovey\nPradnya\nPular\nQilin\nQuattro\nRaby\nRalf\nRastorgueva\nRavanelli\nREBECCA\nRenato\nRescu\nRigoll\nRiguidel\nRjeili\nrocheng\nRouhe\nRouvier\nRuban\nRyant\nSafaya\nSagar\nSahar\nSAIT\nSalah\nSalima\nSamuele\nSangeet\nSanjeev\nSarthak\nSathvik\nSaurous\nSefik\nSergiy\nSerr\nSeung\nShona\nShou\nShrikanth\nShubham\nShucong\nSinc\nSinica\nSmaragdis\nSobule\nSoninke\nSouhir\nSpinor\nSreeramadas\nSridharan\nSungbok\nSylvain\nTagliasacchi\nTakuya\nTamasheq\nTanel\nTasnet\nTeboul\nTechnische\nTedlium\nThakker\nThienpondt\nTitouan\nToks\nTrabelsi\nTsao\nTsun\nTzinis\nUdupa\nUniversität\nVaessen\nValk\nVassil\nVaswani\nVishak\nViterbi\nVogt\nWahab\nWaray\nWatzel\nWaytowich\nWhipps\nWinkelbauer\nWlocmax\nXilin\nXuechen\nXugang\nYacoubi\nYadav\nYanni\nYannick\nYeol\nYingzhi\nYoshioka\nYoshua\nYusuke\nYuxuan\nZaiem\nZaion\nZanon\nZeghidour\nZenodo\nZeyu\nZhang\nZhao\nZhepei\nZhong\nZijian\n\n####### British #######\nbehaviour\nfinalised\nharmonise\ninitialised\nInitialises\nneighbours\nnormalise\noptimisation\noptimisations\noptimise\noptimised\noptimiser\norganised\nquantised\nrealise\nsemantizer\nstabilised\nstabilises\nstabilising\ntraveller\ntravellers\nutilises\nvisualisation\n\n####### Non-English #######\nAUJOURD\nAUJOURD'HUI\ncollés\ndelle\nencadre\nnoviembre\nPolitecnica\nquelques\nUniversité\nUniversità\nvie\n"
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.353515625,
          "content": "[flake8]\nignore = E203, E266, E501, W503, DOC105, DOC106, DOC107, DOC203, DOC403, DOC404, DOC405, DOC501, DOC502\n# line length is intentionally set to 80 here because black uses Bugbear\n# See https://github.com/psf/black/blob/master/README.md#line-length for more details\nmax-line-length = 80\nmax-complexity = 18\nselect = B,C,E,F,W,T4,B9,DOC\nexclude = tests/tmp\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.2548828125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nnode_modules/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\ntests/tmp/\ntests/download/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/build/\ndocs/API/*.rst\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# PyCharm project settings\n.idea\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Audio folders\n**/audio_cache/\n\n# Pretrained & models folders\n**/model_checkpoints/\n**/pretrained_model_checkpoints/\n**/pretrained_models/\n\n# Results folders\n**/results/\n\n# Log folders\n**/log/\n\n# Mac OS\n.DS_Store\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.607421875,
          "content": "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v2.5.0  # Use the ref you want to point at\n    hooks:\n      - id: trailing-whitespace\n        types: [file, text]\n      - id: end-of-file-fixer\n        types: [python]\n      - id: requirements-txt-fixer\n      - id: mixed-line-ending\n        types: [python]\n        args: [--fix=no]\n      - id: check-added-large-files\n        args: [--maxkb=1024]\n\n  - repo: https://github.com/psf/black\n    rev: 24.3.0\n    hooks:\n      - id: black\n        types: [python]\n        additional_dependencies: ['click==8.1.7']\n\n  - repo: https://github.com/PyCQA/flake8\n    rev: 7.0.0\n    hooks:\n      - id: flake8\n        types: [python]\n        additional_dependencies:\n          - pydoclint==0.4.1\n          - pycodestyle==2.11.0\n          - flake8-encodings==0.5.1\n\n  - repo: https://github.com/adrienverge/yamllint\n    rev: v1.35.1\n    hooks:\n      - id: yamllint\n\n  - repo: https://github.com/codespell-project/codespell\n    rev: v2.3.0\n    hooks:\n      - id: codespell\n        args:\n          - \"--ignore-words=.dict-speechbrain.txt\"\n          # skip jupyter notebook as there isn't a good way to only match inputs\n          # at the moment. manually fixing up outputs would be a pain and we\n          # cannot always expect to regex them out.\n          - \"--skip=*.ipynb\"\n          # for ipynb inline base64 -- although this isn't very useful since we\n          # are disabling support for ipynb for now\n          - \"--ignore-regex='base64,.*?=='\"\n        additional_dependencies:\n          - tomli\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.13.2\n    hooks:\n      - id: isort\n"
        },
        {
          "name": ".pre-push-config.yaml",
          "type": "blob",
          "size": 0.68359375,
          "content": "repos:\n    - repo: local\n      hooks:\n          - id: linters\n            name: linters\n            entry: tests/.run-linters.sh\n            language: script\n            pass_filenames: False\n            always_run: True\n            require_serial: False\n\n          - id: unittests\n            name: unittests\n            entry: tests/.run-unittests.sh\n            language: script\n            pass_filenames: False\n            always_run: True\n            require_serial: False\n\n          - id: doctests\n            name: doctests\n            entry: tests/.run-doctests.sh\n            language: script\n            pass_filenames: False\n            always_run: True\n            require_serial: False\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.201171875,
          "content": "# .readthedocs.yaml\n\nversion: 2\n\nbuild:\n  os: ubuntu-24.04\n  tools:\n    python: \"3.12\"\n\npython:\n  install:\n    - requirements: docs/readthedocs-requirements.txt\n\n# Don't build any extra formats\nformats: []\n"
        },
        {
          "name": ".yamllint.yaml",
          "type": "blob",
          "size": 0.265625,
          "content": "extends: default\n\nrules:\n  document-start:\n    present: False\n  truthy:\n    allowed-values:\n      - 'True'\n      - 'False'\n    level: error\n  comments: disable\n  comments-indentation: disable\n  line-length:\n    level: warning\n    allow-non-breakable-inline-mappings: True\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 3.6640625,
          "content": "# This CITATION.cff file was generated with cffinit.\n# Visit https://bit.ly/cffinit to generate yours today!\n\ncff-version: 1.2.0\ntitle: SpeechBrain\nmessage: A PyTorch-based Speech Toolkit\ntype: software\nauthors:\n  - given-names: Mirco\n    family-names: Ravanelli\n    affiliation: 'Mila - Quebec AI Institute, Université de Montréal'\n  - given-names: Titouan\n    family-names: Parcollet\n    affiliation: >-\n      LIA - Avignon Université, CaMLSys - University of\n      Cambridge\n  - given-names: Peter\n    family-names: Plantinga\n    affiliation: Ohio State University\n  - given-names: Aku\n    family-names: Rouhe\n    affiliation: Aalto University\n  - given-names: Samuele\n    family-names: Cornell\n    affiliation: Università Politecnica delle Marche\n  - given-names: Loren\n    family-names: Lugosch\n    affiliation: 'Mila - Quebec AI Institute, McGill University'\n  - given-names: Cem\n    family-names: Subakan\n    affiliation: Mila - Quebec AI Institute\n  - given-names: Nauman\n    family-names: Dawalatabad\n    affiliation: Indian Institute of Technology Madras\n  - given-names: Abdelwahab\n    family-names: Heba\n    affiliation: IRIT - Université Paul Sabatier\n  - given-names: Jianyuan\n    family-names: Zhong\n    affiliation: Mila - Quebec AI Institute\n  - given-names: Ju-Chieh\n    family-names: Chou\n    affiliation: Toyota Technological Institute at Chicago\n  - given-names: Sung-Lin\n    family-names: Yeh\n    affiliation: University of Edinburgh\n  - given-names: Szu-Wei\n    family-names: Fu\n    affiliation: 'Academia Sinica, Taiwan'\n  - given-names: Chien-Feng\n    family-names: Liao\n    affiliation: 'Academia Sinica, Taiwan'\n  - given-names: Elena\n    family-names: Rastorgueva\n    affiliation: NVIDIA\n  - given-names: François\n    family-names: Grondin\n    affiliation: Université de Sherbrooke\n  - given-names: William\n    family-names: Aris\n    affiliation: Université de Sherbrooke\n  - given-names: Hwidong\n    family-names: Na\n    affiliation: Samsung-SAIT\n  - given-names: Yan\n    family-names: Gao\n    affiliation: CaMLSys - University of Cambridge\n  - given-names: Renato\n    name-particle: De\n    family-names: Mori\n    affiliation: 'LIA - Avignon Université, McGill University'\n  - given-names: Yoshua\n    family-names: Bengio\n    affiliation: 'Mila - Quebec AI Institute, Université de Montréal'\nidentifiers:\n  - type: doi\n    value: 10.48550/arXiv.2106.04624\n    description: 'SpeechBrain: A General-Purpose Speech Toolkit'\nrepository-code: 'https://github.com/speechbrain/speechbrain/'\nurl: 'https://speechbrain.github.io/'\nabstract: >-\n  SpeechBrain is an open-source and all-in-one speech\n  toolkit. It is designed to facilitate the research and\n  development of neural speech processing technologies by\n  being simple, flexible, user-friendly, and\n  well-documented. This paper describes the core\n  architecture designed to support several tasks of common\n  interest, allowing users to naturally conceive, compare\n  and share novel speech processing pipelines. SpeechBrain\n  achieves competitive or state-of-the-art performance in a\n  wide range of speech benchmarks. It also provides training\n  recipes, pretrained models, and inference scripts for\n  popular speech datasets, as well as tutorials which allow\n  anyone with basic Python proficiency to familiarize\n  themselves with speech technologies.\nkeywords:\n  - speech toolkit\n  - audio\n  - deep learning\n  - PyTorch\n  - transformers\n  - voice recognition\n  - speech recognition\n  - speech-to-text\n  - language model\n  - speaker recognition\n  - speaker verification\n  - speech processing\n  - audio processing\n  - ASR\n  - speaker diarization\n  - speech separation\n  - speech enhancement\n  - spoken language understanding\n  - HuggingFace\nlicense: Apache-2.0\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "PERFORMANCE.md",
          "type": "blob",
          "size": 35.830078125,
          "content": "# SpeechBrain Performance Report\nThis document provides an overview of the performance achieved on key datasets and tasks supported by SpeechBrain.\n\n## AISHELL-1 Dataset\n\n### ASR\n\n| Model | Checkpoints | HuggingFace | Test-CER |\n| --------| --------| --------| --------|\n | [`recipes/AISHELL-1/ASR/CTC/hparams/train_with_wav2vec.yaml`](recipes/AISHELL-1/ASR/CTC/hparams/train_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/e4bth1bylk7c6h8/AADFq3cWzBBKxuDv09qjvUMta?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-ctc-aishell) | 5.06 |\n | [`recipes/AISHELL-1/ASR/seq2seq/hparams/train.yaml`](recipes/AISHELL-1/ASR/seq2seq/hparams/train.yaml) | [here](https://www.dropbox.com/sh/kefuzzf6jaljqbr/AADBRWRzHz74GCMDqJY9BES4a?dl=0) | - | 7.51 |\n | [`recipes/AISHELL-1/ASR/transformer/hparams/train_ASR_transformer.yaml`](recipes/AISHELL-1/ASR/transformer/hparams/train_ASR_transformer.yaml) | [here](https://www.dropbox.com/sh/tp6tjmysorgvsr4/AAD7KNqi1ot0gR4N406JbKM6a?dl=0) | [here](https://huggingface.co/speechbrain/asr-transformer-aishell) | 6.04 |\n | [`recipes/AISHELL-1/ASR/transformer/hparams/train_ASR_transformer_with_wav2vect.yaml`](recipes/AISHELL-1/ASR/transformer/hparams/train_ASR_transformer_with_wav2vect.yaml) | [here](https://www.dropbox.com/sh/tp6tjmysorgvsr4/AAD7KNqi1ot0gR4N406JbKM6a?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-transformer-aishell) | 5.58 |\n\n\n## Aishell1Mix Dataset\n\n### Separation\n\n| Model | Checkpoints | HuggingFace | SI-SNRi |\n| --------| --------| --------| --------|\n | [`recipes/Aishell1Mix/separation/hparams/sepformer-aishell1mix2.yaml`](recipes/Aishell1Mix/separation/hparams/sepformer-aishell1mix2.yaml) | [here](https://www.dropbox.com/sh/6x9356yuybj8lue/AABPlpS03Vcci_E3jA69oKoXa?dl=0) | - | 13.4dB |\n | [`recipes/Aishell1Mix/separation/hparams/sepformer-aishell1mix3.yaml`](recipes/Aishell1Mix/separation/hparams/sepformer-aishell1mix3.yaml) | [here](https://www.dropbox.com/sh/6x9356yuybj8lue/AABPlpS03Vcci_E3jA69oKoXa?dl=0) | - | 11.2dB |\n\n\n## BinauralWSJ0Mix Dataset\n\n### Separation\n\n| Model | Checkpoints | HuggingFace | SI-SNRi |\n| --------| --------| --------| --------|\n | [`recipes/BinauralWSJ0Mix/separation/hparams/convtasnet-cross.yaml`](recipes/BinauralWSJ0Mix/separation/hparams/convtasnet-cross.yaml) | [here](https://www.dropbox.com/sh/i7fhu7qswjb84gw/AABsX1zP-GOTmyl86PtU8GGua?dl=0) | - | 12.39dB |\n | [`recipes/BinauralWSJ0Mix/separation/hparams/convtasnet-independent.yaml`](recipes/BinauralWSJ0Mix/separation/hparams/convtasnet-independent.yaml) | [here](https://www.dropbox.com/sh/i7fhu7qswjb84gw/AABsX1zP-GOTmyl86PtU8GGua?dl=0) | - | 11.90dB |\n | [`recipes/BinauralWSJ0Mix/separation/hparams/convtasnet-parallel-noise.yaml`](recipes/BinauralWSJ0Mix/separation/hparams/convtasnet-parallel-noise.yaml) | [here](https://www.dropbox.com/sh/i7fhu7qswjb84gw/AABsX1zP-GOTmyl86PtU8GGua?dl=0) | - | 18.25dB |\n | [`recipes/BinauralWSJ0Mix/separation/hparams/convtasnet-parallel-reverb.yaml`](recipes/BinauralWSJ0Mix/separation/hparams/convtasnet-parallel-reverb.yaml) | [here](https://www.dropbox.com/sh/i7fhu7qswjb84gw/AABsX1zP-GOTmyl86PtU8GGua?dl=0) | - | 6.95dB |\n | [`recipes/BinauralWSJ0Mix/separation/hparams/convtasnet-parallel.yaml`](recipes/BinauralWSJ0Mix/separation/hparams/convtasnet-parallel.yaml) | [here](https://www.dropbox.com/sh/i7fhu7qswjb84gw/AABsX1zP-GOTmyl86PtU8GGua?dl=0) | - | 16.93dB |\n\n\n## CVSS Dataset\n\n### S2ST\n\n| Model | Checkpoints | HuggingFace | Test-sacrebleu |\n| --------| --------| --------| --------|\n | [`recipes/CVSS/S2ST/hparams/train_fr-en.yaml`](recipes/CVSS/S2ST/hparams/train_fr-en.yaml) | [here]( https://www.dropbox.com/sh/woz4i1p8pkfkqhf/AACmOvr3sS7p95iXl3twCj_xa?dl=0) | [here]( ) | 24.47 |\n\n\n## CommonLanguage Dataset\n\n### Language-id\n\n| Model | Checkpoints | HuggingFace | Error |\n| --------| --------| --------| --------|\n | [`recipes/CommonLanguage/lang_id/hparams/train_ecapa_tdnn.yaml`](recipes/CommonLanguage/lang_id/hparams/train_ecapa_tdnn.yaml) | [here](https://www.dropbox.com/sh/1fxpzyv67ouwd2c/AAAeMUWYP2f1ycpE1Lp1CwEla?dl=0) | [here](https://huggingface.co/speechbrain/lang-id-commonlanguage_ecapa) | 15.1% |\n\n\n## CommonVoice Dataset\n\n### ASR-seq2seq\n\n| Model | Checkpoints | HuggingFace | Test-WER |\n| --------| --------| --------| --------|\n | [`recipes/CommonVoice/ASR/seq2seq/hparams/train_de.yaml`](recipes/CommonVoice/ASR/seq2seq/hparams/train_de.yaml) | [here](https://www.dropbox.com/sh/zgatirb118f79ef/AACmjh-D94nNDWcnVI4Ef5K7a?dl=0) | [here](https://huggingface.co/speechbrain/asr-crdnn-commonvoice-14-de) | 12.25% |\n | [`recipes/CommonVoice/ASR/seq2seq/hparams/train_en.yaml`](recipes/CommonVoice/ASR/seq2seq/hparams/train_en.yaml) | [here](https://www.dropbox.com/sh/h8ged0yu3ztypkh/AAAu-12k_Ceg-tTjuZnrg7dza?dl=0) | [here](https://huggingface.co/speechbrain/asr-crdnn-commonvoice-14-en) | 23.88% |\n | [`recipes/CommonVoice/ASR/seq2seq/hparams/train_fr.yaml`](recipes/CommonVoice/ASR/seq2seq/hparams/train_fr.yaml) | [here](https://www.dropbox.com/sh/07a5lt21wxp98x5/AABhNwmWFaNFyA734bNZUO03a?dl=0) | [here](https://huggingface.co/speechbrain/asr-crdnn-commonvoice-14-fr) | 14.88% |\n | [`recipes/CommonVoice/ASR/seq2seq/hparams/train_it.yaml`](recipes/CommonVoice/ASR/seq2seq/hparams/train_it.yaml) | [here](https://www.dropbox.com/sh/ss59uu0j5boscvp/AAASsiFhlB1nDWPkFX410bzna?dl=0) | [here](https://huggingface.co/speechbrain/asr-crdnn-commonvoice-14-it) | 17.02% |\n | [`recipes/CommonVoice/ASR/seq2seq/hparams/train_rw.yaml`](recipes/CommonVoice/ASR/seq2seq/hparams/train_rw.yaml) | [here](https://www.dropbox.com/sh/i1fv4f8miilqgii/AAB3gE97kmFDA0ISkIDSUW_La?dl=0) | [here](https://huggingface.co/speechbrain/asr-crdnn-commonvoice-14-rw) | 29.22% |\n | [`recipes/CommonVoice/ASR/seq2seq/hparams/train_es.yaml`](recipes/CommonVoice/ASR/seq2seq/hparams/train_es.yaml) | [here](https://www.dropbox.com/sh/r3w0b2tm1p73vft/AADCxdhUwDN6j4PVT9TYe-d5a?dl=0) | [here](https://huggingface.co/speechbrain/asr-crdnn-commonvoice-14-es) | 14.77% |\n\n\n### ASR-CTC\n\n| Model | Checkpoints | HuggingFace | Test-WER |\n| --------| --------| --------| --------|\n | [`recipes/CommonVoice/ASR/CTC/hparams/train_en_with_wav2vec.yaml`](recipes/CommonVoice/ASR/CTC/hparams/train_en_with_wav2vec.yaml) | [here](https://www.dropbox.com/scl/fo/gx0szpbectig2r6r6p9vk/APdoN_wWWq_wP4My7w6SvMo?rlkey=v8fhd887bn947yjb45i99wm8p&st=6muft51b&dl=0) | - | 16.16% |\n | [`recipes/CommonVoice/ASR/CTC/hparams/train_fr_with_wav2vec.yaml`](recipes/CommonVoice/ASR/CTC/hparams/train_fr_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/0i7esfa8jp3rxpp/AAArdi8IuCRmob2WAS7lg6M4a?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-commonvoice-14-fr) | 9.71% |\n | [`recipes/CommonVoice/ASR/CTC/hparams/train_it_with_wav2vec.yaml`](recipes/CommonVoice/ASR/CTC/hparams/train_it_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/hthxqzh5boq15rn/AACftSab_FM6EFWWPgHpKw82a?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-commonvoice-14-it) | 7.99% |\n | [`recipes/CommonVoice/ASR/CTC/hparams/train_rw_with_wav2vec.yaml`](recipes/CommonVoice/ASR/CTC/hparams/train_rw_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/4iax0l4yfry37gn/AABuQ31JY-Sbyi1VlOJfV7haa?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-commonvoice-14-rw) | 22.52% |\n | [`recipes/CommonVoice/ASR/CTC/hparams/train_de_with_wav2vec.yaml`](recipes/CommonVoice/ASR/CTC/hparams/train_de_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/dn7plq4wfsujsi1/AABS1kqB_uqLJVkg-bFkyPpVa?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-commonvoice-14-de) | 8.39% |\n | [`recipes/CommonVoice/ASR/CTC/hparams/train_ar_with_wav2vec.yaml`](recipes/CommonVoice/ASR/CTC/hparams/train_ar_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/7tnuqqbr4vy96cc/AAA_5_R0RmqFIiyR0o1nVS4Ia?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-commonvoice-14-ar) | 28.53% |\n | [`recipes/CommonVoice/ASR/CTC/hparams/train_es_with_wav2vec.yaml`](recipes/CommonVoice/ASR/CTC/hparams/train_es_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/ejvzgl3d3g8g9su/AACYtbSWbDHvBr06lAb7A4mVa?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-commonvoice-14-es) | 12.67% |\n | [`recipes/CommonVoice/ASR/CTC/hparams/train_pt_with_wav2vec.yaml`](recipes/CommonVoice/ASR/CTC/hparams/train_pt_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/80wucrvijdvao2a/AAD6-SZ2_ZZXmlAjOTw6fVloa?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-commonvoice-14-pt) | 21.69% |\n | [`recipes/CommonVoice/ASR/CTC/hparams/train_zh-CN_with_wav2vec.yaml`](recipes/CommonVoice/ASR/CTC/hparams/train_zh-CN_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/2bikr81vgufoglf/AABMpD0rLIaZBxjtwBHgrNpga?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-commonvoice-14-zh-CN) | 23.17% |\n\n\n### ASR-transformer\n\n| Model | Checkpoints | HuggingFace | Test-WER |\n| --------| --------| --------| --------|\n | [`recipes/CommonVoice/ASR/transformer/hparams/train_hf_whisper.yaml`](recipes/CommonVoice/ASR/transformer/hparams/train_hf_whisper.yaml) | - | - | 16.96% |\n\n\n## DNS Dataset\n\n### Enhancement\n\n| Model | Checkpoints | HuggingFace | valid-PESQ | test-SIG | test-BAK | test-OVRL |\n| --------| --------| --------| --------| --------| --------| --------|\n | [`recipes/DNS/enhancement/hparams/sepformer-dns-16k.yaml`](recipes/DNS/enhancement/hparams/sepformer-dns-16k.yaml) | [here](https://www.dropbox.com/sh/d3rp5d3gjysvy7c/AACmwcEkm_IFvaW1lt2GdtQka?dl=0) | [here](https://huggingface.co/speechbrain/sepformer-dns4-16k-enhancement) | 2.06 | 2.999 | 3.076 | 2.437 |\n\n\n## DVoice Dataset\n\n### ASR-CTC\n\n| Model | Checkpoints | HuggingFace | Test-WER |\n| --------| --------| --------| --------|\n | [`recipes/DVoice/ASR/CTC/hparams/train_amh_with_wav2vec.yaml`](recipes/DVoice/ASR/CTC/hparams/train_amh_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/pyu40jq1ebv6hcc/AADQO_lAD-F9Q0vlVq8KoXHqa?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-dvoice-amharic) | 24.92% |\n | [`recipes/DVoice/ASR/CTC/hparams/train_dar_with_wav2vec.yaml`](recipes/DVoice/ASR/CTC/hparams/train_dar_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/pyu40jq1ebv6hcc/AADQO_lAD-F9Q0vlVq8KoXHqa?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-dvoice-darija) | 18.28% |\n | [`recipes/DVoice/ASR/CTC/hparams/train_fon_with_wav2vec.yaml`](recipes/DVoice/ASR/CTC/hparams/train_fon_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/pyu40jq1ebv6hcc/AADQO_lAD-F9Q0vlVq8KoXHqa?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-dvoice-fongbe) | 9.00% |\n | [`recipes/DVoice/ASR/CTC/hparams/train_sw_with_wav2vec.yaml`](recipes/DVoice/ASR/CTC/hparams/train_sw_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/pyu40jq1ebv6hcc/AADQO_lAD-F9Q0vlVq8KoXHqa?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-dvoice-swahili) | 23.16% |\n | [`recipes/DVoice/ASR/CTC/hparams/train_wol_with_wav2vec.yaml`](recipes/DVoice/ASR/CTC/hparams/train_wol_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/pyu40jq1ebv6hcc/AADQO_lAD-F9Q0vlVq8KoXHqa?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-dvoice-wolof) | 16.05% |\n\n\n### Multilingual-ASR-CTC\n\n| Model | Checkpoints | HuggingFace | WER-Darija | WER-Swahili | WER-Fongbe | Fongbe-Wolof | WER-Amharic |\n| --------| --------| --------| --------| --------| --------| --------| --------|\n | [`recipes/DVoice/ASR/CTC/hparams/train_multi_with_wav2vec.yaml`](recipes/DVoice/ASR/CTC/hparams/train_multi_with_wav2vec.yaml) | [here](https://www.dropbox.com/sh/pyu40jq1ebv6hcc/AADQO_lAD-F9Q0vlVq8KoXHqa?dl=0) | - | 13.27% | 29.31% | 10.26% | 21.54% | 31.15% |\n\n\n## ESC50 Dataset\n\n### SoundClassification\n\n| Model | Checkpoints | HuggingFace | Accuracy |\n| --------| --------| --------| --------|\n | [`recipes/ESC50/classification/hparams/cnn14.yaml`](recipes/ESC50/classification/hparams/cnn14.yaml) | [here](https://www.dropbox.com/sh/fbe7l14o3n8f5rw/AACABE1BQGBbX4j6A1dIhBcSa?dl=0) | - | 82% |\n | [`recipes/ESC50/classification/hparams/conv2d.yaml`](recipes/ESC50/classification/hparams/conv2d.yaml) | [here](https://www.dropbox.com/sh/tl2pbfkreov3z7e/AADwwhxBLw1sKvlSWzp6DMEia?dl=0) | - | 75% |\n\n\n## Fisher-Callhome-Spanish Dataset\n\n### Speech_Translation\n\n| Model | Checkpoints | HuggingFace | Test-sacrebleu |\n| --------| --------| --------| --------|\n | [`recipes/Fisher-Callhome-Spanish/ST/transformer/hparams/transformer.yaml`](recipes/Fisher-Callhome-Spanish/ST/transformer/hparams/transformer.yaml) | [here](https://www.dropbox.com/sh/tmh7op8xwthdta0/AACuU9xHDHPs8ToxIIwoTLB0a?dl=0) | - | 47.31 |\n | [`recipes/Fisher-Callhome-Spanish/ST/transformer/hparams/conformer.yaml`](recipes/Fisher-Callhome-Spanish/ST/transformer/hparams/conformer.yaml) | [here](https://www.dropbox.com/sh/tmh7op8xwthdta0/AACuU9xHDHPs8ToxIIwoTLB0a?dl=0) | - | 48.04 |\n\n\n## Google-speech-commands Dataset\n\n### Command_recognition\n\n| Model | Checkpoints | HuggingFace | Test-accuracy |\n| --------| --------| --------| --------|\n | [`recipes/Google-speech-commands/hparams/xvect.yaml`](recipes/Google-speech-commands/hparams/xvect.yaml) | [here](https://www.dropbox.com/sh/9n9q42pugbx0g7a/AADihpfGKuWf6gkwQznEFINDa?dl=0) | [here](https://huggingface.co/speechbrain/google_speech_command_xvector) | 97.43% |\n | [`recipes/Google-speech-commands/hparams/xvect_leaf.yaml`](recipes/Google-speech-commands/hparams/xvect_leaf.yaml) | [here](https://www.dropbox.com/sh/r63w4gytft4s1x6/AAApP8-pp179QKGCZHV_OuD8a?dl=0) | - | 96.79% |\n\n\n## IEMOCAP Dataset\n\n### Emotion_recognition\n\n| Model | Checkpoints | HuggingFace | Test-Accuracy |\n| --------| --------| --------| --------|\n | [`recipes/IEMOCAP/emotion_recognition/hparams/train_with_wav2vec2.yaml`](recipes/IEMOCAP/emotion_recognition/hparams/train_with_wav2vec2.yaml) | [here](https://www.dropbox.com/sh/lmebg4li83sgkhg/AACooPKbNlwd-7n5qSJMbc7ya?dl=0) | [here](https://huggingface.co/speechbrain/emotion-recognition-wav2vec2-IEMOCAP/) | 65.7% |\n | [`recipes/IEMOCAP/emotion_recognition/hparams/train.yaml`](recipes/IEMOCAP/emotion_recognition/hparams/train.yaml) | [here](https://www.dropbox.com/sh/ke4fxiry97z58m8/AACPEOM5bIyxo9HxG2mT9v_aa?dl=0) | - | 77.0% |\n\n\n## IWSLT22_lowresource Dataset\n\n### Speech_Translation\n\n| Model | Checkpoints | HuggingFace | Test-BLEU |\n| --------| --------| --------| --------|\n | [`recipes/IWSLT22_lowresource/AST/transformer/hparams/train_w2v2_mbart_st.yaml`](recipes/IWSLT22_lowresource/AST/transformer/hparams/train_w2v2_mbart_st.yaml) | [here](https://www.dropbox.com/sh/xjo0ou739oksnus/AAAgyrCwywmDRRuUiDnUva2za?dl=0) | - | 7.73 |\n | [`recipes/IWSLT22_lowresource/AST/transformer/hparams/train_w2v2_nllb_st.yaml`](recipes/IWSLT22_lowresource/AST/transformer/hparams/train_w2v2_nllb_st.yaml) | [here](https://www.dropbox.com/sh/spp2ijgfdbzuz26/AABkJ97e72D7aKzNLTm1qmWEa?dl=0) | - | 8.70 |\n | [`recipes/IWSLT22_lowresource/AST/transformer/hparams/train_samu_mbart_st.yaml`](recipes/IWSLT22_lowresource/AST/transformer/hparams/train_samu_mbart_st.yaml) | [here](https://www.dropbox.com/sh/98s1xyc3chreaw6/AABom3FnwY5SsIvg4en9tWC2a?dl=0) | - | 10.28 |\n | [`recipes/IWSLT22_lowresource/AST/transformer/hparams/train_samu_nllb_st.yaml`](recipes/IWSLT22_lowresource/AST/transformer/hparams/train_samu_nllb_st.yaml) | [here](https://www.dropbox.com/sh/ekkpl9c3kxsgllj/AABa0q2LrJe_o7JF-TTbfxZ-a?dl=0) | - | 11.32 |\n\n\n## KsponSpeech Dataset\n\n### ASR\n\n| Model | Checkpoints | HuggingFace | clean-WER | others-WER |\n| --------| --------| --------| --------| --------|\n | [`recipes/KsponSpeech/ASR/transformer/hparams/conformer_medium.yaml`](recipes/KsponSpeech/ASR/transformer/hparams/conformer_medium.yaml) | [here](https://www.dropbox.com/sh/uibokbz83o8ybv3/AACtO5U7mUbu_XhtcoOphAjza?dl=0) | [here](https://huggingface.co/speechbrain/asr-conformer-transformerlm-ksponspeech) | 20.78% | 25.73% |\n\n\n## LibriMix Dataset\n\n### Separation\n\n| Model | Checkpoints | HuggingFace | SI-SNR |\n| --------| --------| --------| --------|\n | [`recipes/LibriMix/separation/hparams/sepformer-libri2mix.yaml`](recipes/LibriMix/separation/hparams/sepformer-libri2mix.yaml) | [here](https://www.dropbox.com/sh/skkiozml92xtgdo/AAD0eJxgbCTK03kAaILytGtVa?dl=0) | - | 20.4dB |\n | [`recipes/LibriMix/separation/hparams/sepformer-libri3mix.yaml`](recipes/LibriMix/separation/hparams/sepformer-libri3mix.yaml) | [here](https://www.dropbox.com/sh/kmyz7tts9tyg198/AACsDcRwKvelXxEB-k5q1OaIa?dl=0) | - | 19.0dB |\n\n\n## LibriParty Dataset\n\n### VAD\n\n| Model | Checkpoints | HuggingFace | Test-Precision | Recall | F-Score |\n| --------| --------| --------| --------| --------| --------|\n | [`recipes/LibriParty/VAD/hparams/train.yaml`](recipes/LibriParty/VAD/hparams/train.yaml) | [here](https://www.dropbox.com/sh/6yguuzn4pybjasd/AABpUF8LAQ8d2TJyC8aK2OBga?dl=0 ) | [here](https://huggingface.co/speechbrain/vad-crdnn-libriparty) | 0.9518 | 0.9437 | 0.9477 |\n\n\n## LibriSpeech Dataset\n\n### ASR-Transformers\n\n| Model | Checkpoints | HuggingFace | Test_clean-WER | Test_other-WER |\n| --------| --------| --------| --------| --------|\n | [`recipes/LibriSpeech/ASR/transformer/hparams/conformer_small.yaml`](recipes/LibriSpeech/ASR/transformer/hparams/conformer_small.yaml) | [here](https://www.dropbox.com/sh/s0x6ni124858b8i/AAALaCH6sGTMRUVTjh8Tm8Jwa?dl=0) | [here](https://huggingface.co/speechbrain/asr-conformersmall-transformerlm-librispeech) | 2.49% | 6.10% |\n | [`recipes/LibriSpeech/ASR/transformer/hparams/transformer.yaml`](recipes/LibriSpeech/ASR/transformer/hparams/transformer.yaml) | [here](https://www.dropbox.com/sh/653kq8h2k87md4p/AAByAaAryXtQKpRzYtzV9ih5a?dl=0) | [here](https://huggingface.co/speechbrain/asr-transformer-transformerlm-librispeech) | 2.27% | 5.53% |\n | [`recipes/LibriSpeech/ASR/transformer/hparams/conformer_large.yaml`](recipes/LibriSpeech/ASR/transformer/hparams/conformer_large.yaml) | [here](https://www.dropbox.com/scl/fo/9we244tgdf47ay20hrdoz/AKnoqQ13nLwSv1ITeJEQ3wY?rlkey=05o5jiszr8rhj6dlprw87t2x4&st=u2odesyk&dl=0) | - | 2.01% | 4.52% |\n | [`recipes/LibriSpeech/ASR/transformer/hparams/branchformer_large.yaml`](recipes/LibriSpeech/ASR/transformer/hparams/branchformer_large.yaml) | [here](https://www.dropbox.com/scl/fo/qhtds5rrdvhhhjywa7ovw/AMiIL5YvQENw5JKVpzXlP5o?rlkey=hz8vlpy3qf9kcyfx0cox089e6&st=ufckv6tb&dl=0) | - | 2.04% | 4.12% |\n | [`recipes/LibriSpeech/ASR/transformer/hparams/hyperconformer_22M.yaml`](recipes/LibriSpeech/ASR/transformer/hparams/hyperconformer_22M.yaml) | [here](https://www.dropbox.com/sh/30xsmqj13jexzoh/AACvZNtX1Fsr0Wa1Z3C9rHLXa?dl=0) | - | 2.23% | 4.54% |\n | [`recipes/LibriSpeech/ASR/transformer/hparams/hyperconformer_8M.yaml`](recipes/LibriSpeech/ASR/transformer/hparams/hyperconformer_8M.yaml) | [here](https://www.dropbox.com/sh/8jc96avmivr8fke/AABrFEhtWy_3-Q7BHhkh0enwa?dl=0) | - | 2.55% | 6.61% |\n | [`recipes/LibriSpeech/ASR/transformer/hparams/hyperbranchformer_25M.yaml`](recipes/LibriSpeech/ASR/transformer/hparams/hyperbranchformer_25M.yaml) | - | - | 2.36% | 6.89% |\n | [`recipes/LibriSpeech/ASR/transformer/hparams/hyperbranchformer_13M.yaml`](recipes/LibriSpeech/ASR/transformer/hparams/hyperbranchformer_13M.yaml) | - | - | 2.54% | 6.58% |\n | [`recipes/LibriSpeech/ASR/transformer/hparams/train_hf_whisper.yaml`](recipes/LibriSpeech/ASR/transformer/hparams/train_hf_whisper.yaml) | - | - |  |\n | [`recipes/LibriSpeech/ASR/transformer/hparams/bayesspeech.yaml`](recipes/LibriSpeech/ASR/transformer/hparams/bayesspeech.yaml) | [here](https://www.dropbox.com/scl/fo/cdken4jqfj96ev1v84jxm/h?rlkey=25eu1ytgm5ac51zqj8p65zwxd&dl=0) | - | 2.84% | 6.27% |\n\n\n### ASR-Transducers\n\n| Model | Checkpoints | HuggingFace | Test_clean-WER | Test_other-WER |\n| --------| --------| --------| --------| --------|\n | [`recipes/LibriSpeech/ASR/transducer/hparams/conformer_transducer.yaml`](recipes/LibriSpeech/ASR/transducer/hparams/conformer_transducer.yaml) | [here](https://www.dropbox.com/scl/fo/kl1eikmoauygwqcx8ok4r/AMkreKLzHtxPtqnoXzUerko?rlkey=juk374k210b76lbnblh7or95d&st=1ugwe9e3&dl=0) | - | 2.72% | 6.47% |\n\n\n### ASR-CTC\n\n| Model | Checkpoints | HuggingFace | Test_clean-WER | Test_other-WER |\n| --------| --------| --------| --------| --------|\n | [`recipes/LibriSpeech/ASR/CTC/hparams/train_hf_wav2vec.yaml`](recipes/LibriSpeech/ASR/CTC/hparams/train_hf_wav2vec.yaml) | [here](https://www.dropbox.com/sh/qj2ps85g8oiicrj/AAAxlkQw5Pfo0M9EyHMi8iAra?dl=0) | [here](https://huggingface.co/speechbrain/asr-wav2vec2-librispeech) | 1.65% | 3.67% |\n | [`recipes/LibriSpeech/ASR/CTC/hparams/train_hf_wav2vec_transformer_rescoring.yaml`](recipes/LibriSpeech/ASR/CTC/hparams/train_hf_wav2vec_transformer_rescoring.yaml) | [here](https://www.dropbox.com/sh/ijqalvre7mm08ng/AAD_hsN-8dBneUMMkELsOOxga?dl=0) | - | 1.57% | 3.37% |\n\n\n### G2P\n\n| Model | Checkpoints | HuggingFace | PER-Test |\n| --------| --------| --------| --------|\n | [`recipes/LibriSpeech/G2P/hparams/hparams_g2p_rnn.yaml`](recipes/LibriSpeech/G2P/hparams/hparams_g2p_rnn.yaml) | [here](https://www.dropbox.com/sh/qmcl1obp8pxqaap/AAC3yXvjkfJ3mL-RKyAUxPdNa?dl=0) | - | 2.72% |\n | [`recipes/LibriSpeech/G2P/hparams/hparams_g2p_transformer.yaml`](recipes/LibriSpeech/G2P/hparams/hparams_g2p_transformer.yaml) | [here](https://www.dropbox.com/sh/zhrxg7anuhje7e8/AADTeJtdsja_wClkE2DsF9Ewa?dl=0) | [here](https://huggingface.co/speechbrain/soundchoice-g2p) | 2.89% |\n\n\n### ASR-Seq2Seq\n\n| Model | Checkpoints | HuggingFace | Test_clean-WER | Test_other-WER |\n| --------| --------| --------| --------| --------|\n | [`recipes/LibriSpeech/ASR/seq2seq/hparams/train_BPE_5000.yaml`](recipes/LibriSpeech/ASR/seq2seq/hparams/train_BPE_5000.yaml) | [here](https://www.dropbox.com/sh/1ycv07gyxdq8hdl/AABUDYzza4SLYtY45RcGf2_0a?dl=0) | [here](https://huggingface.co/speechbrain/asr-crdnn-transformerlm-librispeech) | 2.89% | 8.09% |\n\n\n## MEDIA Dataset\n\n### ASR\n\n| Model | Checkpoints | HuggingFace | Test-ChER | Test-CER |\n| --------| --------| --------| --------| --------|\n | [`recipes/MEDIA/ASR/CTC/hparams/train_hf_wav2vec.yaml`](recipes/MEDIA/ASR/CTC/hparams/train_hf_wav2vec.yaml) | - | [here](https://huggingface.co/speechbrain/asr-wav2vec2-ctc-MEDIA) | 7.78% | 4.78% |\n\n\n### SLU\n\n| Model | Checkpoints | HuggingFace | Test-ChER | Test-CER | Test-CVER |\n| --------| --------| --------| --------| --------| --------|\n | [`recipes/MEDIA/SLU/CTC/hparams/train_hf_wav2vec_full.yaml`](recipes/MEDIA/SLU/CTC/hparams/train_hf_wav2vec_full.yaml) | - | [here](https://huggingface.co/speechbrain/slu-wav2vec2-ctc-MEDIA-relax) | 7.46% | 20.10% | 31.41% |\n | [`recipes/MEDIA/SLU/CTC/hparams/train_hf_wav2vec_relax.yaml`](recipes/MEDIA/SLU/CTC/hparams/train_hf_wav2vec_relax.yaml) | - | [here](https://huggingface.co/speechbrain/slu-wav2vec2-ctc-MEDIA-full) | 7.78% | 24.88% | 35.77% |\n\n\n## MultiWOZ Dataset\n\n### Response-Generation\n\n| Model | Checkpoints | HuggingFace | Test-PPL | Test_BLEU-4 |\n| --------| --------| --------| --------| --------|\n | [`recipes/MultiWOZ/response_generation/gpt/hparams/train_gpt.yaml`](recipes/MultiWOZ/response_generation/gpt/hparams/train_gpt.yaml) | [here](https://www.dropbox.com/sh/vm8f5iavohr4zz9/AACrkOxXuxsrvJy4Cjpih9bQa?dl=0) | [here](https://huggingface.co/speechbrain/MultiWOZ-GPT-Response_Generation) | 4.01 | 2.54e-04 |\n | [`recipes/MultiWOZ/response_generation/llama2/hparams/train_llama2.yaml`](recipes/MultiWOZ/response_generation/llama2/hparams/train_llama2.yaml) | [here](https://www.dropbox.com/sh/d093vsje1d7ijj9/AAA-nHEd_MwNEFJfBGLmXxJra?dl=0) | [here](https://huggingface.co/speechbrain/MultiWOZ-Llama2-Response_Generation) | 2.90 | 7.45e-04 |\n\n\n## REAL-M Dataset\n\n### Sisnr-estimation\n\n| Model | Checkpoints | HuggingFace | L1-Error |\n| --------| --------| --------| --------|\n | [`recipes/REAL-M/sisnr-estimation/hparams/pool_sisnrestimator.yaml`](recipes/REAL-M/sisnr-estimation/hparams/pool_sisnrestimator.yaml) | [here](https://www.dropbox.com/sh/n55lm8i5z51pbm1/AABHfByOEy__UP_bmT4GJvSba?dl=0) | [here](https://huggingface.co/speechbrain/REAL-M-sisnr-estimator) | 1.71dB |\n\n\n## RescueSpeech Dataset\n\n### ASR+enhancement\n\n| Model | Checkpoints | HuggingFace | SISNRi | SDRi | PESQ | STOI | WER |\n| --------| --------| --------| --------| --------| --------| --------| --------|\n | [`recipes/RescueSpeech/ASR/noise-robust/hparams/robust_asr_16k.yaml`](recipes/RescueSpeech/ASR/noise-robust/hparams/robust_asr_16k.yaml) | [here](https://www.dropbox.com/sh/kqs2ld14fm20cxl/AACiobSLdNtXhm-4Y3IIbTeia?dl=0) | [here](https://huggingface.co/sangeet2020/noisy-whisper-resucespeech) | 7.482 | 8.011 | 2.083 | 0.854 | 45.29% |\n\n\n## SLURP Dataset\n\n### SLU\n\n| Model | Checkpoints | HuggingFace | scenario-accuracy | action-accuracy | intent-accuracy |\n| --------| --------| --------| --------| --------| --------|\n | [`recipes/SLURP/NLU/hparams/train.yaml`](recipes/SLURP/NLU/hparams/train.yaml) | [here](https://www.dropbox.com/scl/fo/c0rm2ja8oxus8q27om8ve/h?rlkey=irxzl1ea8g7e6ipk0vuc288zh&dl=0 ) | - | 90.81% | 88.29% | 87.28% |\n | [`recipes/SLURP/direct/hparams/train.yaml`](recipes/SLURP/direct/hparams/train.yaml) | [here](https://www.dropbox.com/scl/fo/c0rm2ja8oxus8q27om8ve/h?rlkey=irxzl1ea8g7e6ipk0vuc288zh&dl=0 ) | - | 81.73% | 77.11% | 75.05% |\n | [`recipes/SLURP/direct/hparams/train_with_wav2vec2.yaml`](recipes/SLURP/direct/hparams/train_with_wav2vec2.yaml) | [here](https://www.dropbox.com/scl/fo/c0rm2ja8oxus8q27om8ve/h?rlkey=irxzl1ea8g7e6ipk0vuc288zh&dl=0 ) | [here](https://huggingface.co/speechbrain/SLU-direct-SLURP-hubert-enc) | 91.24% | 88.47% | 87.55% |\n\n\n## Switchboard Dataset\n\n### ASR\n\n| Model | Checkpoints | HuggingFace | Swbd-WER | Callhome-WER | Eval2000-WER |\n| --------| --------| --------| --------| --------| --------|\n | [`recipes/Switchboard/ASR/CTC/hparams/train_with_wav2vec.yaml`](recipes/Switchboard/ASR/CTC/hparams/train_with_wav2vec.yaml) | - | [here](https://huggingface.co/speechbrain/asr-wav2vec2-switchboard) | 8.76% | 14.67% | 11.78% |\n | [`recipes/Switchboard/ASR/seq2seq/hparams/train_BPE_2000.yaml`](recipes/Switchboard/ASR/seq2seq/hparams/train_BPE_2000.yaml) | - | [here](https://huggingface.co/speechbrain/asr-crdnn-switchboard) | 16.90% | 25.12% | 20.71% |\n | [`recipes/Switchboard/ASR/transformer/hparams/transformer.yaml`](recipes/Switchboard/ASR/transformer/hparams/transformer.yaml) | - | [here](https://huggingface.co/speechbrain/asr-transformer-switchboard) | 9.80% | 17.89% | 13.94% |\n\n\n## TIMIT Dataset\n\n### ASR\n\n| Model | Checkpoints | HuggingFace | Test-PER |\n| --------| --------| --------| --------|\n | [`recipes/TIMIT/ASR/CTC/hparams/train.yaml`](recipes/TIMIT/ASR/CTC/hparams/train.yaml) | [here](https://www.dropbox.com/sh/059jnwdass8v45u/AADTjh5DYdYKuZsgH9HXGx0Sa?dl=0) | - | 14.78% |\n | [`recipes/TIMIT/ASR/seq2seq/hparams/train.yaml`](recipes/TIMIT/ASR/seq2seq/hparams/train.yaml) | [here](https://www.dropbox.com/sh/059jnwdass8v45u/AADTjh5DYdYKuZsgH9HXGx0Sa?dl=0) | - | 14.07% |\n | [`recipes/TIMIT/ASR/seq2seq/hparams/train_with_wav2vec2.yaml`](recipes/TIMIT/ASR/seq2seq/hparams/train_with_wav2vec2.yaml) | [here](https://www.dropbox.com/sh/059jnwdass8v45u/AADTjh5DYdYKuZsgH9HXGx0Sa?dl=0) | - | 8.04% |\n | [`recipes/TIMIT/ASR/transducer/hparams/train.yaml`](recipes/TIMIT/ASR/transducer/hparams/train.yaml) | [here](https://www.dropbox.com/sh/059jnwdass8v45u/AADTjh5DYdYKuZsgH9HXGx0Sa?dl=0) | - | 14.12% |\n | [`recipes/TIMIT/ASR/transducer/hparams/train_wav2vec.yaml`](recipes/TIMIT/ASR/transducer/hparams/train_wav2vec.yaml) | [here](https://www.dropbox.com/sh/059jnwdass8v45u/AADTjh5DYdYKuZsgH9HXGx0Sa?dl=0) | - | 8.91% |\n\n\n## Tedlium2 Dataset\n\n### ASR\n\n| Model | Checkpoints | HuggingFace | Test-WER_No_LM |\n| --------| --------| --------| --------|\n | [`recipes/Tedlium2/ASR/transformer/hparams/branchformer_large.yaml`](recipes/Tedlium2/ASR/transformer/hparams/branchformer_large.yaml) | [here](https://www.dropbox.com/sh/el523uofs96czfi/AADgTd838pKo2aR8fhqVOh-Oa?dl=0) | [here](https://huggingface.co/speechbrain/asr-branchformer-large-tedlium2) | 8.11% |\n\n\n## UrbanSound8k Dataset\n\n### SoundClassification\n\n| Model | Checkpoints | HuggingFace | Accuracy |\n| --------| --------| --------| --------|\n | [`recipes/UrbanSound8k/SoundClassification/hparams/train_ecapa_tdnn.yaml`](recipes/UrbanSound8k/SoundClassification/hparams/train_ecapa_tdnn.yaml) | [here](https://www.dropbox.com/sh/f61325e3w8h5yy2/AADm3E3PXFi1NYA7-QW3H-Ata?dl=0 ) | [here](https://huggingface.co/speechbrain/urbansound8k_ecapa) | 75.4% |\n\n\n## Voicebank Dataset\n\n### Dereverberation\n\n| Model | Checkpoints | HuggingFace | PESQ |\n| --------| --------| --------| --------|\n | [`recipes/Voicebank/dereverb/MetricGAN-U/hparams/train_dereverb.yaml`](recipes/Voicebank/dereverb/MetricGAN-U/hparams/train_dereverb.yaml) | [here](https://www.dropbox.com/sh/r94qn1f5lq9r3p7/AAAZfisBhhkS8cwpzy1O5ADUa?dl=0 ) | - | 2.07 |\n | [`recipes/Voicebank/dereverb/spectral_mask/hparams/train.yaml`](recipes/Voicebank/dereverb/spectral_mask/hparams/train.yaml) | [here](https://www.dropbox.com/sh/pw8aer8gcsrdbx7/AADknh7plHF5GBeTRK9VkIKga?dl=0 ) | - | 2.35 |\n\n\n### ASR\n\n| Model | Checkpoints | HuggingFace | Test-PER |\n| --------| --------| --------| --------|\n | [`recipes/Voicebank/ASR/CTC/hparams/train.yaml`](recipes/Voicebank/ASR/CTC/hparams/train.yaml) | [here](https://www.dropbox.com/sh/w4j0auezgmmo005/AAAjKcoJMdLDp0Pqe3m7CLVaa?dl=0) | - | 10.12% |\n\n\n### ASR+enhancement\n\n| Model | Checkpoints | HuggingFace | PESQ | COVL | test-WER |\n| --------| --------| --------| --------| --------| --------|\n | [`recipes/Voicebank/MTL/ASR_enhance/hparams/robust_asr.yaml`](recipes/Voicebank/MTL/ASR_enhance/hparams/robust_asr.yaml) | [here](https://www.dropbox.com/sh/azvcbvu8g5hpgm1/AACDc6QxtNMGZ3IoZLrDiU0Va?dl=0) | [here](https://huggingface.co/speechbrain/mtl-mimic-voicebank) | 3.05 | 3.74 | 2.80 |\n\n\n### Enhancement\n\n| Model | Checkpoints | HuggingFace | PESQ |\n| --------| --------| --------| --------|\n | [`recipes/Voicebank/enhance/MetricGAN/hparams/train.yaml`](recipes/Voicebank/enhance/MetricGAN/hparams/train.yaml) | [here](https://www.dropbox.com/sh/n5q9vjn0yn1qvk6/AAB-S7i2-XzVm6ux0MrXCvqya?dl=0 ) | [here](https://huggingface.co/speechbrain/metricgan-plus-voicebank) | 3.15 |\n | [`recipes/Voicebank/enhance/SEGAN/hparams/train.yaml`](recipes/Voicebank/enhance/SEGAN/hparams/train.yaml) | [here](https://www.dropbox.com/sh/ez0folswdbqiad4/AADDasepeoCkneyiczjCcvaOa?dl=0 ) | - | 2.38 |\n | [`recipes/Voicebank/enhance/spectral_mask/hparams/train.yaml`](recipes/Voicebank/enhance/spectral_mask/hparams/train.yaml) | [here](https://www.dropbox.com/sh/n5q9vjn0yn1qvk6/AAB-S7i2-XzVm6ux0MrXCvqya?dl=0 ) | - | 2.65 |\n\n\n## VoxCeleb Dataset\n\n### Speaker_recognition\n\n| Model | Checkpoints | HuggingFace | EER |\n| --------| --------| --------| --------|\n | [`recipes/VoxCeleb/SpeakerRec/hparams/train_ecapa_tdnn.yaml`](recipes/VoxCeleb/SpeakerRec/hparams/train_ecapa_tdnn.yaml) | [here](https://www.dropbox.com/sh/ab1ma1lnmskedo8/AADsmgOLPdEjSF6wV3KyhNG1a?dl=0) | [here](https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb) | 0.80% |\n | [`recipes/VoxCeleb/SpeakerRec/hparams/train_x_vectors.yaml`](recipes/VoxCeleb/SpeakerRec/hparams/train_x_vectors.yaml) | [here](https://www.dropbox.com/sh/ab1ma1lnmskedo8/AADsmgOLPdEjSF6wV3KyhNG1a?dl=0) | [here](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb) | 3.23% |\n | [`recipes/VoxCeleb/SpeakerRec/hparams/train_resnet.yaml`](recipes/VoxCeleb/SpeakerRec/hparams/train_resnet.yaml) | [here](https://www.dropbox.com/sh/ab1ma1lnmskedo8/AADsmgOLPdEjSF6wV3KyhNG1a?dl=0) | [here](https://huggingface.co/speechbrain/spkrec-resnet-voxceleb) | 0.95% |\n\n\n## VoxLingua107 Dataset\n\n### Language-id\n\n| Model | Checkpoints | HuggingFace | Accuracy |\n| --------| --------| --------| --------|\n | [`recipes/VoxLingua107/lang_id/hparams/train_ecapa.yaml`](recipes/VoxLingua107/lang_id/hparams/train_ecapa.yaml) | [here](https://www.dropbox.com/sh/72gpuic5m4x8ztz/AAB5R-RVIEsXJtRH8SGkb_oCa?dl=0 ) | [here](https://huggingface.co/speechbrain/lang-id-voxlingua107-ecapa) | 93.3% |\n\n\n## VoxPopuli Dataset\n\n## WHAMandWHAMR Dataset\n\n### Separation\n\n| Model | Checkpoints | HuggingFace | SI-SNR |\n| --------| --------| --------| --------|\n | [`recipes/WHAMandWHAMR/separation/hparams/sepformer-wham.yaml`](recipes/WHAMandWHAMR/separation/hparams/sepformer-wham.yaml) | [here](https://www.dropbox.com/sh/sfrgb3xivri432e/AACQodNmiDIKrB9vCeCFUDWUa?dl=0) | [here](https://huggingface.co/speechbrain/sepformer-whamr) | 16.5 |\n | [`recipes/WHAMandWHAMR/separation/hparams/sepformer-whamr.yaml`](recipes/WHAMandWHAMR/separation/hparams/sepformer-whamr.yaml) | [here](https://www.dropbox.com/sh/1sia32z01xbfgvu/AADditsqaTyfN3N6tzfEFPica?dl=0) | [here](https://huggingface.co/speechbrain/sepformer-wham) | 14.0 |\n\n\n### Enhancement\n\n| Model | Checkpoints | HuggingFace | SI-SNR | PESQ |\n| --------| --------| --------| --------| --------|\n | [`recipes/WHAMandWHAMR/enhancement/hparams/sepformer-wham.yaml`](recipes/WHAMandWHAMR/enhancement/hparams/sepformer-wham.yaml) | [here](https://www.dropbox.com/sh/pxz2xbj76ijd5ci/AAD3c3dHyszk4oHJaa26K1_ha?dl=0) | [here](https://huggingface.co/speechbrain/sepformer-wham-enhancement) | 14.4 | 3.05 |\n | [`recipes/WHAMandWHAMR/enhancement/hparams/sepformer-whamr.yaml`](recipes/WHAMandWHAMR/enhancement/hparams/sepformer-whamr.yaml) | [here](https://www.dropbox.com/sh/kb0xrvi5k168ou2/AAAPB2U6HyyUT1gMoUH8gxQCa?dl=0) | [here](https://huggingface.co/speechbrain/sepformer-whamr-enhancement) | 10.6 | 2.84 |\n\n\n## WSJ0Mix Dataset\n\n### Separation (2mix)\n\n| Model | Checkpoints | HuggingFace | SI-SNRi |\n| --------| --------| --------| --------|\n | [`recipes/WSJ0Mix/separation/hparams/convtasnet.yaml`](recipes/WSJ0Mix/separation/hparams/convtasnet.yaml) | [here](https://www.dropbox.com/sh/hdpxj47signsay7/AABbDjGoyQesnFxjg0APxl7qa?dl=0) | - | 14.8dB |\n | [`recipes/WSJ0Mix/separation/hparams/dprnn.yaml`](recipes/WSJ0Mix/separation/hparams/dprnn.yaml) | [here](https://www.dropbox.com/sh/o8fohu5s07h4bnw/AADPNyR1E3Q4aRobg3FtXTwVa?dl=0) | - | 18.5dB |\n | [`recipes/WSJ0Mix/separation/hparams/resepformer.yaml`](recipes/WSJ0Mix/separation/hparams/resepformer.yaml) | [here](https://www.dropbox.com/sh/obnu87zhubn1iia/AAAbn_jzqzIfeqaE9YQ7ujyQa?dl=0) | [here](https://huggingface.co/speechbrain/resepformer-wsj02mix) | 18.6dB |\n | [`recipes/WSJ0Mix/separation/hparams/sepformer.yaml`](recipes/WSJ0Mix/separation/hparams/sepformer.yaml) | [here](https://www.dropbox.com/sh/9klsqadkhin6fw1/AADEqGdT98rcqxVgFlfki7Gva?dl=0 ) | [here](https://huggingface.co/speechbrain/sepformer-wsj02mix) | 22.4dB |\n | [`recipes/WSJ0Mix/separation/hparams/skim.yaml`](recipes/WSJ0Mix/separation/hparams/skim.yaml) | [here](https://www.dropbox.com/sh/zy0l5rc8abxdfp3/AAA2ngB74fugqpWXmjZo5v3wa?dl=0) | [here](https://huggingface.co/speechbrain/resepformer-wsj02mix ) | 18.1dB |\n\n\n## ZaionEmotionDataset Dataset\n\n### Emotion_Diarization\n\n| Model | Checkpoints | HuggingFace | EDER |\n| --------| --------| --------| --------|\n | [`recipes/ZaionEmotionDataset/emotion_diarization/hparams/train.yaml`](recipes/ZaionEmotionDataset/emotion_diarization/hparams/train.yaml) | [here](https://www.dropbox.com/sh/woudm1v31a7vyp5/AADAMxpQOXaxf8E_1hX202GJa?dl=0) | [here](https://huggingface.co/speechbrain/emotion-diarization-wavlm-large) | 30.2% |\n\n\n## fluent-speech-commands Dataset\n\n### SLU\n\n| Model | Checkpoints | HuggingFace | Test-accuracy |\n| --------| --------| --------| --------|\n | [`recipes/fluent-speech-commands/direct/hparams/train.yaml`](recipes/fluent-speech-commands/direct/hparams/train.yaml) | [here](https://www.dropbox.com/sh/wal9ap0go9f66qw/AADBVlGs_E2pEU4vYJgEe3Fba?dl=0) | - | 99.60% |\n\n\n## timers-and-such Dataset\n\n### SLU\n\n| Model | Checkpoints | HuggingFace | Accuracy-Test_real |\n| --------| --------| --------| --------|\n | [`recipes/timers-and-such/decoupled/hparams/train_TAS_LM.yaml`](recipes/timers-and-such/decoupled/hparams/train_TAS_LM.yaml) | [here](https://www.dropbox.com/sh/gmmum179ig9wz0x/AAAOSOi11yVymGXHp9LzYNrqa?dl=0) | - | 46.8% |\n | [`recipes/timers-and-such/direct/hparams/train.yaml`](recipes/timers-and-such/direct/hparams/train.yaml) | [here](https://www.dropbox.com/sh/gmmum179ig9wz0x/AAAOSOi11yVymGXHp9LzYNrqa?dl=0) | [here](https://huggingface.co/speechbrain/slu-timers-and-such-direct-librispeech-asr) | 77.5% |\n | [`recipes/timers-and-such/direct/hparams/train_with_wav2vec2.yaml`](recipes/timers-and-such/direct/hparams/train_with_wav2vec2.yaml) | [here](https://www.dropbox.com/sh/gmmum179ig9wz0x/AAAOSOi11yVymGXHp9LzYNrqa?dl=0) | - | 94.0% |\n | [`recipes/timers-and-such/multistage/hparams/train_TAS_LM.yaml`](recipes/timers-and-such/multistage/hparams/train_TAS_LM.yaml) | [here](https://www.dropbox.com/sh/gmmum179ig9wz0x/AAAOSOi11yVymGXHp9LzYNrqa?dl=0) | - | 72.6% |\n\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 22.580078125,
          "content": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/speechbrain/speechbrain/develop/docs/images/speechbrain-logo.svg\" alt=\"SpeechBrain Logo\"/>\n</p>\n\n[![Typing SVG](https://readme-typing-svg.demolab.com?font=Fira+Code&size=40&duration=7000&pause=1000&random=false&width=1200&height=100&lines=Simplify+Conversational+AI+Development)](https://git.io/typing-svg)\n\n\n| 📘 [Tutorials](https://speechbrain.readthedocs.io) | 🌐 [Website](https://speechbrain.github.io/) | 📚 [Documentation](https://speechbrain.readthedocs.io/en/latest/index.html) | 🤝 [Contributing](https://speechbrain.readthedocs.io/en/latest/contributing.html) | 🤗 [HuggingFace](https://huggingface.co/speechbrain) | ▶️ [YouTube](https://www.youtube.com/@SpeechBrainProject) | 🐦 [X](https://twitter.com/SpeechBrain1) |\n\n![GitHub Repo stars](https://img.shields.io/github/stars/speechbrain/speechbrain?style=social) *Please, help our community project. Star on GitHub!*\n\n**Exciting News (January, 2024):** Discover what is new in SpeechBrain 1.0 [here](https://colab.research.google.com/drive/1IEPfKRuvJRSjoxu22GZhb3czfVHsAy0s?usp=sharing)!\n#\n# 🗣️💬 What SpeechBrain Offers\n\n- SpeechBrain is an **open-source** [PyTorch](https://pytorch.org/) toolkit that accelerates **Conversational AI** development, i.e., the technology behind *speech assistants*, *chatbots*, and *large language models*.\n\n- It is crafted for fast and easy creation of advanced technologies for **Speech** and **Text** Processing.\n\n\n## 🌐  Vision\n- With the rise of [deep learning](https://www.deeplearningbook.org/), once-distant domains like speech processing and NLP are now very close. A well-designed neural network and large datasets are all you need.\n\n- We think it is now time for a **holistic toolkit** that, mimicking the human brain, jointly supports diverse technologies for complex Conversational AI systems.\n\n- This spans *speech recognition*, *speaker recognition*, *speech enhancement*, *speech separation*, *language modeling*, *dialogue*, and beyond.\n\n- Aligned with our long-term goal of natural human-machine conversation, including for non-verbal individuals, we have recently added support for the [EEG modality](https://github.com/speechbrain/benchmarks/tree/main/benchmarks/MOABB).\n\n\n\n## 📚 Training Recipes\n- We share over 200 competitive training [recipes](recipes) on more than 40 datasets supporting 20 speech and text processing tasks (see below).\n\n- We support both training from scratch and fine-tuning pretrained models such as [Whisper](https://huggingface.co/openai/whisper-large), [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2), [WavLM](https://huggingface.co/docs/transformers/model_doc/wavlm), [Hubert](https://huggingface.co/docs/transformers/model_doc/hubert), [GPT2](https://huggingface.co/gpt2), [Llama2](https://huggingface.co/docs/transformers/model_doc/llama2), and beyond. The models on [HuggingFace](https://huggingface.co/) can be easily plugged in and fine-tuned.\n\n- For any task, you train the model using these commands:\n```python\npython train.py hparams/train.yaml\n```\n\n- The hyperparameters are encapsulated in a YAML file, while the training process is orchestrated through a Python script.\n\n- We maintained a consistent code structure across different tasks.\n\n- For better replicability, training logs and checkpoints are hosted on Dropbox.\n\n## <a href=\"https://huggingface.co/speechbrain\" target=\"_blank\"> <img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\" alt=\"drawing\" width=\"40\"/> </a> Pretrained Models and Inference\n\n- Access over 100 pretrained models hosted on [HuggingFace](https://huggingface.co/speechbrain).\n- Each model comes with a user-friendly interface for seamless inference. For example, transcribing speech using a pretrained model requires just three lines of code:\n\n```python\nfrom speechbrain.inference import EncoderDecoderASR\n\nasr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-conformer-transformerlm-librispeech\", savedir=\"pretrained_models/asr-transformer-transformerlm-librispeech\")\nasr_model.transcribe_file(\"speechbrain/asr-conformer-transformerlm-librispeech/example.wav\")\n```\n\n##  <a href=\"https://speechbrain.github.io/\" target=\"_blank\"> <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/1200px-Google_Colaboratory_SVG_Logo.svg.png\" alt=\"drawing\" width=\"50\"/> </a>  Documentation\n- We are deeply dedicated to promoting inclusivity and education.\n- We have authored over 30 [tutorials](https://speechbrain.readthedocs.io) that not only describe how SpeechBrain works but also help users familiarize themselves with Conversational AI.\n- Every class or function has clear explanations and examples that you can run. Check out the [documentation](https://speechbrain.readthedocs.io/en/latest/index.html) for more details 📚.\n\n\n\n## 🎯 Use Cases\n- 🚀 **Research Acceleration**: Speeding up academic and industrial research. You can develop and integrate new models effortlessly, comparing their performance against our baselines.\n\n- ⚡️ **Rapid Prototyping**: Ideal for quick prototyping in time-sensitive projects.\n\n- 🎓 **Educational Tool**: SpeechBrain's simplicity makes it a valuable educational resource. It is used by institutions like [Mila](https://mila.quebec/en/), [Concordia University](https://www.concordia.ca/), [Avignon University](https://univ-avignon.fr/en/), and many others for student training.\n\n#\n# 🚀 Quick Start\n\nTo get started with SpeechBrain, follow these simple steps:\n\n## 🛠️ Installation\n\n### Install via PyPI\n\n1. Install SpeechBrain using PyPI:\n\n    ```bash\n    pip install speechbrain\n    ```\n\n2. Access SpeechBrain in your Python code:\n\n    ```python\n    import speechbrain as sb\n    ```\n\n### Install from GitHub\nThis installation is recommended for users who wish to conduct experiments and customize the toolkit according to their needs.\n\n1. Clone the GitHub repository and install the requirements:\n\n    ```bash\n    git clone https://github.com/speechbrain/speechbrain.git\n    cd speechbrain\n    pip install -r requirements.txt\n    pip install --editable .\n    ```\n\n2. Access SpeechBrain in your Python code:\n\n    ```python\n    import speechbrain as sb\n    ```\n\nAny modifications made to the `speechbrain` package will be automatically reflected, thanks to the `--editable` flag.\n\n## ✔️ Test Installation\n\nEnsure your installation is correct by running the following commands:\n\n```bash\npytest tests\npytest --doctest-modules speechbrain\n```\n\n## 🏃‍♂️ Running an Experiment\n\nIn SpeechBrain, you can train a model for any task using the following steps:\n\n```python\ncd recipes/<dataset>/<task>/\npython experiment.py params.yaml\n```\n\nThe results will be saved in the `output_folder` specified in the YAML file.\n\n## 📘 Learning SpeechBrain\n\n- **Website:** Explore general information on the [official website](https://speechbrain.github.io).\n\n- **Tutorials:** Start with [basic tutorials](https://speechbrain.readthedocs.io/en/latest/tutorials/basics.html) covering fundamental functionalities. Find advanced tutorials and topics in the Tutorial notebooks category in the [SpeechBrain documentation](https://speechbrain.readthedocs.io).\n\n- **Documentation:** Detailed information on the SpeechBrain API, contribution guidelines, and code is available in the [documentation](https://speechbrain.readthedocs.io/en/latest/index.html).\n\n#\n# 🔧 Supported Technologies\n- SpeechBrain is a versatile framework designed for implementing a wide range of technologies within the field of Conversational AI.\n- It excels not only in individual task implementations but also in combining various technologies into complex pipelines.\n\n## 🎙️ Speech/Audio Processing\n| Tasks        | Datasets           | Technologies/Models  |\n| ------------- |-------------| -----|\n| Speech Recognition      | [AISHELL-1](recipes/AISHELL-1), [CommonVoice](recipes/CommonVoice), [DVoice](recipes/DVoice), [KsponSpeech](recipes/KsponSpeech), [LibriSpeech](recipes/LibriSpeech), [MEDIA](recipes/MEDIA), [RescueSpeech](recipes/RescueSpeech), [Switchboard](recipes/Switchboard), [TIMIT](recipes/TIMIT), [Tedlium2](recipes/Tedlium2), [Voicebank](recipes/Voicebank) | [CTC](https://www.cs.toronto.edu/~graves/icml_2006.pdf), [Transducers](https://arxiv.org/pdf/1211.3711.pdf?origin=publication_detail), [Transformers](https://arxiv.org/abs/1706.03762), [Seq2Seq](http://zhaoshuaijiang.com/file/Hybrid_CTC_Attention_Architecture_for_End-to-End_Speech_Recognition.pdf), [Beamsearch techniques for CTC](https://arxiv.org/pdf/1911.01629.pdf),[seq2seq](https://arxiv.org/abs/1904.02619.pdf),[transducers](https://www.merl.com/publications/docs/TR2017-190.pdf)), [Rescoring](https://arxiv.org/pdf/1612.02695.pdf), [Conformer](https://arxiv.org/abs/2005.08100), [Branchformer](https://arxiv.org/abs/2207.02971), [Hyperconformer](https://arxiv.org/abs/2305.18281), [Kaldi2-FST](https://github.com/k2-fsa/k2) |\n| Speaker Recognition      | [VoxCeleb](recipes/VoxCeleb) | [ECAPA-TDNN](https://arxiv.org/abs/2005.07143), [ResNET](https://arxiv.org/pdf/1910.12592.pdf), [Xvectors](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf), [PLDA](https://ieeexplore.ieee.org/document/6639151), [Score Normalization](https://www.sciencedirect.com/science/article/abs/pii/S1051200499903603) |\n| Speech Separation      | [WSJ0Mix](recipes/WSJ0Mix), [LibriMix](recipes/LibriMix), [WHAM!](recipes/WHAMandWHAMR), [WHAMR!](recipes/WHAMandWHAMR), [Aishell1Mix](recipes/Aishell1Mix), [BinauralWSJ0Mix](recipes/BinauralWSJ0Mix) | [SepFormer](https://arxiv.org/abs/2010.13154), [RESepFormer](https://arxiv.org/abs/2206.09507), [SkiM](https://arxiv.org/abs/2201.10800), [DualPath RNN](https://arxiv.org/abs/1910.06379), [ConvTasNET](https://arxiv.org/abs/1809.07454) |\n| Speech Enhancement      | [DNS](recipes/DNS), [Voicebank](recipes/Voicebank) | [SepFormer](https://arxiv.org/abs/2010.13154), [MetricGAN](https://arxiv.org/abs/1905.04874), [MetricGAN-U](https://arxiv.org/abs/2110.05866), [SEGAN](https://arxiv.org/abs/1703.09452), [spectral masking](http://staff.ustc.edu.cn/~jundu/Publications/publications/Trans2015_Xu.pdf), [time masking](http://staff.ustc.edu.cn/~jundu/Publications/publications/Trans2015_Xu.pdf) |\n| Interpretability | [ESC50](recipes/ESC50) | [Listenable Maps for Audio Classifiers (L-MAC)](https://arxiv.org/abs/2403.13086), [Learning-to-Interpret (L2I)](https://proceedings.neurips.cc/paper_files/paper/2022/file/e53280d73dd5389e820f4a6250365b0e-Paper-Conference.pdf), [Non-Negative Matrix Factorization (NMF)](https://proceedings.neurips.cc/paper_files/paper/2022/file/e53280d73dd5389e820f4a6250365b0e-Paper-Conference.pdf), [PIQ](https://arxiv.org/abs/2303.12659) |\n| Speech Generation | [AudioMNIST](recipes/AudioMNIST) | [Diffusion](https://arxiv.org/abs/2006.11239), [Latent Diffusion](https://arxiv.org/abs/2112.10752) |\n| Text-to-Speech      | [LJSpeech](recipes/LJSpeech), [LibriTTS](recipes/LibriTTS) | [Tacotron2](https://arxiv.org/abs/1712.05884), [Zero-Shot Multi-Speaker Tacotron2](https://arxiv.org/abs/2112.02418), [FastSpeech2](https://arxiv.org/abs/2006.04558) |\n| Vocoding      | [LJSpeech](recipes/LJSpeech), [LibriTTS](recipes/LibriTTS) | [HiFiGAN](https://arxiv.org/abs/2010.05646), [DiffWave](https://arxiv.org/abs/2009.09761)\n| Spoken Language Understanding | [MEDIA](recipes/MEDIA), [SLURP](recipes/SLURP), [Fluent Speech Commands](recipes/fluent-speech-commands), [Timers-and-Such](recipes/timers-and-such)  | [Direct SLU](https://arxiv.org/abs/2104.01604), [Decoupled SLU](https://arxiv.org/abs/2104.01604), [Multistage SLU](https://arxiv.org/abs/2104.01604) |\n| Speech-to-Speech Translation  | [CVSS](recipes/CVSS) | [Discrete Hubert](https://arxiv.org/pdf/2106.07447.pdf), [HiFiGAN](https://arxiv.org/abs/2010.05646), [wav2vec2](https://arxiv.org/abs/2006.11477) |\n| Speech Translation  | [Fisher CallHome (Spanish)](recipes/Fisher-Callhome-Spanish), [IWSLT22(lowresource)](recipes/IWSLT22_lowresource) | [wav2vec2](https://arxiv.org/abs/2006.11477) |\n| Emotion Classification      | [IEMOCAP](recipes/IEMOCAP), [ZaionEmotionDataset](recipes/ZaionEmotionDataset) | [ECAPA-TDNN](https://arxiv.org/abs/2005.07143), [wav2vec2](https://arxiv.org/abs/2006.11477), [Emotion Diarization](https://arxiv.org/abs/2306.12991) |\n| Language Identification | [VoxLingua107](recipes/VoxLingua107), [CommonLanguage](recipes/CommonLanguage)| [ECAPA-TDNN](https://arxiv.org/abs/2005.07143) |\n| Voice Activity Detection  | [LibriParty](recipes/LibriParty) | [CRDNN](https://arxiv.org/abs/2106.04624) |\n| Sound Classification  | [ESC50](recipes/ESC50), [UrbanSound](recipes/UrbanSound8k) | [CNN14](https://github.com/ranchlai/sound_classification), [ECAPA-TDNN](https://arxiv.org/abs/2005.07143) |\n| Self-Supervised Learning | [CommonVoice](recipes/CommonVoice), [LibriSpeech](recipes/LibriSpeech) | [wav2vec2](https://arxiv.org/abs/2006.11477) |\n| Metric Learning | [REAL-M](recipes/REAL-M/sisnr-estimation), [Voicebank](recipes/Voicebank) | [Blind SNR-Estimation](https://arxiv.org/abs/2002.08909), [PESQ Learning](https://arxiv.org/abs/2110.05866) |\n| Alignment | [TIMIT](recipes/TIMIT) | [CTC](https://www.cs.toronto.edu/~graves/icml_2006.pdf), [Viterbi](https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf), [Forward Forward](https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf) |\n| Diarization | [AMI](recipes/AMI) | [ECAPA-TDNN](https://arxiv.org/abs/2005.07143), [X-vectors](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf), [Spectral Clustering](https://web.archive.org/web/20240305184559/http://www.ifp.illinois.edu/~hning2/papers/Ning_spectral.pdf) |\n\n## 📝 Text Processing\n| Tasks        | Datasets           | Technologies/Models  |\n| ------------- |-------------| -----|\n| Language Modeling | [CommonVoice](recipes/CommonVoice), [LibriSpeech](recipes/LibriSpeech)| [n-grams](https://web.stanford.edu/~jurafsky/slp3/3.pdf), [RNNLM](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf), [TransformerLM](https://arxiv.org/abs/1706.03762) |\n| Response Generation | [MultiWOZ](recipes/MultiWOZ/response_generation)| [GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [Llama2](https://arxiv.org/abs/2307.09288) |\n| Grapheme-to-Phoneme | [LibriSpeech](recipes/LibriSpeech) | [RNN](https://arxiv.org/abs/2207.13703), [Transformer](https://arxiv.org/abs/2207.13703), [Curriculum Learning](https://arxiv.org/abs/2207.13703), [Homograph loss](https://arxiv.org/abs/2207.13703) |\n\n## 🧠 EEG Processing\n| Tasks        | Datasets           | Technologies/Models  |\n| ------------- |-------------| -----|\n| Motor Imagery | [BNCI2014001](https://github.com/speechbrain/benchmarks/tree/main/benchmarks/MOABB/hparams/MotorImagery/BNCI2014001), [BNCI2014004](https://github.com/speechbrain/benchmarks/tree/main/benchmarks/MOABB/hparams/MotorImagery/BNCI2014004), [BNCI2015001](https://github.com/speechbrain/benchmarks/tree/main/benchmarks/MOABB/hparams/MotorImagery/BNCI2015001), [Lee2019_MI](https://github.com/speechbrain/benchmarks/tree/main/benchmarks/MOABB/hparams/MotorImagery/Lee2019_MI), [Zhou201](https://github.com/speechbrain/benchmarks/tree/main/benchmarks/MOABB/hparams/MotorImagery/Zhou2016) | [EEGNet](https://github.com/speechbrain/benchmarks/blob/main/benchmarks/MOABB/models/EEGNet.py), [ShallowConvNet](https://github.com/speechbrain/benchmarks/blob/main/benchmarks/MOABB/models/ShallowConvNet.py), [EEGConformer](https://github.com/speechbrain/benchmarks/blob/main/benchmarks/MOABB/models/EEGConformer.py) |\n| P300 | [BNCI2014009](https://github.com/speechbrain/benchmarks/tree/main/benchmarks/MOABB/hparams/P300/BNCI2014009), [EPFLP300](https://github.com/speechbrain/benchmarks/tree/main/benchmarks/MOABB/hparams/P300/EPFLP300), [bi2015a](https://github.com/speechbrain/benchmarks/tree/main/benchmarks/MOABB/hparams/P300/bi2015a), | [EEGNet](https://github.com/speechbrain/benchmarks/blob/main/benchmarks/MOABB/models/EEGNet.py) |\n| SSVEP | [Lee2019_SSVEP](https://github.com/speechbrain/benchmarks/tree/main/benchmarks/MOABB/hparams/SSVEP/Lee2019_SSVEP) | [EEGNet](https://github.com/speechbrain/benchmarks/blob/main/benchmarks/MOABB/models/EEGNet.py) |\n\n\n\n\n## 🔍 Additional Features\n\nSpeechBrain includes a range of native functionalities that enhance the development of Conversational AI technologies. Here are some examples:\n\n- **Training Orchestration:** The `Brain` class serves as a fully customizable tool for managing training and evaluation loops over data. It simplifies training loops while providing the flexibility to override any part of the process.\n\n- **Hyperparameter Management:** A YAML-based hyperparameter file specifies all hyperparameters, from individual numbers (e.g., learning rate) to complete objects (e.g., custom models). This elegant solution drastically simplifies the training script.\n\n- **Dynamic Dataloader:** Enables flexible and efficient data reading.\n\n- **GPU Training:** Supports single and multi-GPU training, including distributed training.\n\n- **Dynamic Batching:** On-the-fly dynamic batching enhances the efficient processing of variable-length signals.\n\n- **Mixed-Precision Training:** Accelerates training through mixed-precision techniques.\n\n- **Efficient Data Reading:** Reads large datasets efficiently from a shared Network File System (NFS) via [WebDataset](https://github.com/webdataset/webdataset).\n\n- **Hugging Face Integration:** Interfaces seamlessly with [HuggingFace](https://huggingface.co/speechbrain) for popular models such as wav2vec2 and Hubert.\n\n- **Orion Integration:** Interfaces with [Orion](https://github.com/Epistimio/orion) for hyperparameter tuning.\n\n- **Speech Augmentation Techniques:** Includes SpecAugment, Noise, Reverberation, and more.\n\n- **Data Preparation Scripts:** Includes scripts for preparing data for supported datasets.\n\nSpeechBrain is rapidly evolving, with ongoing efforts to support a growing array of technologies in the future.\n\n\n## 📊 Performance\n\n- SpeechBrain integrates a variety of technologies, including those that achieves competitive or state-of-the-art performance.\n\n- For a comprehensive overview of the achieved performance across different tasks, datasets, and technologies, please visit [here](PERFORMANCE.md).\n\n#\n# 📜 License\n\n- SpeechBrain is released under the [Apache License, version 2.0](https://www.apache.org/licenses/LICENSE-2.0), a popular BSD-like license.\n- You are free to redistribute SpeechBrain for both free and commercial purposes, with the condition of retaining license headers. Unlike the GPL, the Apache License is not viral, meaning you are not obligated to release modifications to the source code.\n\n#\n# 🔮Future Plans\n\nWe have ambitious plans for the future, with a focus on the following priorities:\n\n- **Scale Up:** We aim to provide comprehensive recipes and technologies for training massive models on extensive datasets.\n\n- **Scale Down:** While scaling up delivers unprecedented performance, we recognize the challenges of deploying large models in production scenarios. We are focusing on real-time, streamable, and small-footprint Conversational AI.\n\n- **Multimodal Large Language Models**: We envision a future where a single foundation model can handle a wide range of text, speech, and audio tasks. Our core team is focused on enabling the training of advanced multimodal LLMs.\n\n#\n# 🤝 Contributing\n\n- SpeechBrain is a community-driven project, led by a core team with the support of numerous international collaborators.\n- We welcome contributions and ideas from the community. For more information, check [here](https://speechbrain.github.io/contributing.html).\n\n#\n# 🙏 Sponsors\n\n- SpeechBrain is an academically driven project and relies on the passion and enthusiasm of its contributors.\n- As we cannot rely on the resources of a large company, we deeply appreciate any form of support, including donations or collaboration with the core team.\n- If you're interested in sponsoring SpeechBrain, please reach out to us at speechbrainproject@gmail.com.\n- A heartfelt thank you to all our sponsors, including the current ones:\n\n\n\n[<img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\" alt=\"Image 1\" width=\"250\"/>](https://speechbrain.github.io/img/hf.ico) &nbsp; &nbsp;\n[<img src=\"https://speechbrain.github.io/img/sponsors/logo_vd.png\" alt=\"Image 3\" width=\"250\"/>](https://viadialog.com/en/) &nbsp; &nbsp;\n[<img src=\"https://speechbrain.github.io/img/sponsors/logo_nle.png\" alt=\"Image 4\" width=\"250\"/>](https://europe.naverlabs.com/)\n\n<br><br>\n\n[<img src=\"https://speechbrain.github.io/img/sponsors/logo_ovh.png\" alt=\"Image 5\" width=\"250\"/>](https://www.ovhcloud.com/en-ca/) &nbsp; &nbsp;\n[<img src=\"https://speechbrain.github.io/img/sponsors/logo_badu.png\" alt=\"Image 2\" width=\"250\"/>](https://usa.baidu.com/) &nbsp; &nbsp;\n[<img src=\"https://speechbrain.github.io/img/sponsors/samsung_official.png\" alt=\"Image 6\" width=\"250\"/>](https://research.samsung.com/aicenter_cambridge)\n\n<br><br>\n\n[<img src=\"https://speechbrain.github.io/img/sponsors/logo_mila_small.png\" alt=\"Image 7\" width=\"250\"/>](https://mila.quebec/en/) &nbsp; &nbsp;\n[<img src=\"https://www.concordia.ca/content/dam/common/logos/Concordia-logo.jpeg\" alt=\"Image 9\" width=\"250\"/>](https://www.concordia.ca/) &nbsp; &nbsp;\n[<img src=\"https://speechbrain.github.io/img/partners/logo_lia.png\" alt=\"Image 8\" width=\"250\"/>](https://lia.univ-avignon.fr/) &nbsp; &nbsp;\n#\n# 📖 Citing SpeechBrain\n\nIf you use SpeechBrain in your research or business, please cite it using the following BibTeX entry:\n\n```bibtex\n@misc{speechbrainV1,\n  title={Open-Source Conversational AI with {SpeechBrain} 1.0},\n  author={Mirco Ravanelli and Titouan Parcollet and Adel Moumen and Sylvain de Langen and Cem Subakan and Peter Plantinga and Yingzhi Wang and Pooneh Mousavi and Luca Della Libera and Artem Ploujnikov and Francesco Paissan and Davide Borra and Salah Zaiem and Zeyu Zhao and Shucong Zhang and Georgios Karakasidis and Sung-Lin Yeh and Pierre Champion and Aku Rouhe and Rudolf Braun and Florian Mai and Juan Zuluaga-Gomez and Seyed Mahed Mousavi and Andreas Nautsch and Xuechen Liu and Sangeet Sagar and Jarod Duret and Salima Mdhaffar and Gaelle Laperriere and Mickael Rouvier and Renato De Mori and Yannick Esteve},\n  year={2024},\n  eprint={2407.00463},\n  archivePrefix={arXiv},\n  primaryClass={cs.LG},\n  url={https://arxiv.org/abs/2407.00463},\n}\n@misc{speechbrain,\n  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n  year={2021},\n  eprint={2106.04624},\n  archivePrefix={arXiv},\n  primaryClass={eess.AS},\n  note={arXiv:2106.04624}\n}\n```\n\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.3984375,
          "content": "# Security Policy\n\n## Supported Versions\n\nSince SpeechBrain is a beta release research-oriented toolkit, it aims to support the latest major version (at x.y level, e.g. 0.5 until 0.6 is released) with security updates, but unfortunately cannot promise long-term security updates for old versions.\n\n## Reporting a Vulnerability\n\nVulnerabilities may be reported confidentially to speechbrainproject@gmail.com\n\n"
        },
        {
          "name": "conftest.py",
          "type": "blob",
          "size": 2.3212890625,
          "content": "def pytest_addoption(parser):\n    parser.addoption(\"--device\", action=\"store\", default=\"cpu\")\n\n\ndef pytest_generate_tests(metafunc):\n    # This is called for every test. Only get/set command line arguments\n    # if the argument is specified in the list of test \"fixturenames\".\n    option_value = metafunc.config.option.device\n    if \"device\" in metafunc.fixturenames and option_value is not None:\n        metafunc.parametrize(\"device\", [option_value])\n\n\ncollect_ignore = [\n    \"setup.py\",\n    \"speechbrain/lobes/models/huggingface_transformers/mert.py\",\n]\ntry:\n    import numba  # noqa: F401\nexcept ModuleNotFoundError:\n    collect_ignore.append(\"speechbrain/nnet/loss/transducer_loss.py\")\ntry:\n    import kenlm  # noqa: F401\nexcept ModuleNotFoundError:\n    collect_ignore.append(\"speechbrain/decoders/language_model.py\")\ntry:\n    import fairseq  # noqa: F401\nexcept ModuleNotFoundError:\n    collect_ignore.append(\"speechbrain/lobes/models/fairseq_wav2vec.py\")\ntry:\n    from transformers import Wav2Vec2Model  # noqa: F401\nexcept ModuleNotFoundError:\n    collect_ignore.append(\n        \"speechbrain/lobes/models/huggingface_transformers/wav2vec2.py\"\n    )\ntry:\n    from transformers import WhisperModel  # noqa: F401\nexcept ModuleNotFoundError:\n    collect_ignore.append(\n        \"speechbrain/lobes/models/huggingface_transformers/whisper.py\"\n    )\ntry:\n    import sklearn  # noqa: F401\nexcept ModuleNotFoundError:\n    collect_ignore.append(\"speechbrain/utils/kmeans.py\")\n    collect_ignore.append(\n        \"speechbrain/lobes/models/huggingface_transformers/discrete_ssl.py\"\n    )\ntry:\n    import peft  # noqa: F401\nexcept ModuleNotFoundError:\n    collect_ignore.append(\n        \"speechbrain/lobes/models/huggingface_transformers/llama2.py\"\n    )\ntry:\n    import sacrebleu  # noqa: F401\nexcept ModuleNotFoundError:\n    collect_ignore.append(\"speechbrain/utils/bleu.py\")\ntry:\n    import vocos  # noqa: F401\nexcept ModuleNotFoundError:\n    collect_ignore.append(\n        \"speechbrain/lobes/models/huggingface_transformers/vocos.py\"\n    )\ntry:\n    from speechtokenizer import SpeechTokenizer  # noqa: F401\nexcept ModuleNotFoundError:\n    collect_ignore.append(\n        \"speechbrain/lobes/models/discrete/speechtokenizer.py\"\n    )\ntry:\n    import wavtokenizer  # noqa: F401\nexcept ModuleNotFoundError:\n    collect_ignore.append(\"speechbrain/lobes/models/discrete/wavtokenizer.py\")\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "lint-requirements.txt",
          "type": "blob",
          "size": 0.1201171875,
          "content": "black==24.3.0\nclick==8.1.7\nflake8==7.0.0\nisort==5.13.2\npycodestyle==2.11.0\npydoclint==0.4.1\npytest==7.4.0\nyamllint==1.35.1\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.376953125,
          "content": "[tool.black]\nline-length = 80\ntarget-version = ['py38']\nexclude = '''\n\n(\n  /(\n      \\.eggs         # exclude a few common directories in the\n    | \\.git          # root of the project\n    | \\.mypy_cache\n    | \\.tox\n    | \\.venv\n  )/\n)\n'''\n\n[tool.codespell]\nskip = \"./tests/tmp,./**/result,*.csv,*train.txt,*test.txt\"\n\n[tool.isort]\nprofile = \"black\"\nline_length = 80\nfilter_files = true\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.16015625,
          "content": "[pytest]\ndoctest_optionflags= ELLIPSIS\n\npython_files =\n    test_*.py\n    check_*.py\n    example_*.py\n\nnorecursedirs = results tests/tmp tests/utils tests/templates\n"
        },
        {
          "name": "recipes",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2919921875,
          "content": "-r lint-requirements.txt\nhuggingface_hub>=0.8.0\nhyperpyyaml>=0.0.1\njoblib>=0.14.1\nnumpy>=1.17.0\npackaging\npandas>=1.0.1\npre-commit>=2.3.0\npygtrie>=2.1,<3.0\nscipy>=1.4.1,<1.13.0\nsentencepiece>=0.1.91\nSoundFile; sys_platform == 'win32'\ntorch>=1.9.0\ntorchaudio>=1.9.0\ntqdm>=4.42.0\ntransformers>=4.30.0\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.4736328125,
          "content": "#!/usr/bin/env python3\nimport os\nimport site\nimport sys\nfrom distutils.core import setup\n\nimport setuptools\n\n# Editable install in user site directory can be allowed with this hack:\n# https://github.com/pypa/pip/issues/7953.\nsite.ENABLE_USER_SITE = \"--user\" in sys.argv[1:]\n\nwith open(\"README.md\", encoding=\"utf-8\") as f:\n    long_description = f.read()\n\nwith open(os.path.join(\"speechbrain\", \"version.txt\"), encoding=\"utf-8\") as f:\n    version = f.read().strip()\n\nsetup(\n    name=\"speechbrain\",\n    version=version,\n    description=\"All-in-one speech toolkit in pure Python and Pytorch\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    author=\"Mirco Ravanelli, Titouan Parcollet & Others\",\n    author_email=\"speechbrain@gmail.com\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: Apache Software License\",\n    ],\n    # we don't want to ship the tests package. for future proofing, also\n    # exclude any tests subpackage (if we ever define __init__.py there)\n    packages=setuptools.find_packages(exclude=[\"tests\", \"tests.*\"]),\n    package_data={\"speechbrain\": [\"version.txt\", \"log-config.yaml\"]},\n    install_requires=[\n        \"hyperpyyaml\",\n        \"joblib\",\n        \"numpy\",\n        \"packaging\",\n        \"scipy\",\n        \"sentencepiece\",\n        \"torch>=1.9\",\n        \"torchaudio\",\n        \"tqdm\",\n        \"huggingface_hub\",\n    ],\n    python_requires=\">=3.8\",\n    url=\"https://speechbrain.github.io/\",\n)\n"
        },
        {
          "name": "speechbrain",
          "type": "tree",
          "content": null
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "tutorials",
          "type": "blob",
          "size": 0.0146484375,
          "content": "docs/tutorials/"
        }
      ]
    }
  ]
}