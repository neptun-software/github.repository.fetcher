{
  "metadata": {
    "timestamp": 1736560620252,
    "page": 250,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "jianchang512/clone-voice",
      "stars": 7844,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.1669921875,
          "content": ".git/\r\n.github/\r\n.vscode/\r\ncache/\r\ndocker/\r\ntts/\r\n.dockerignore\r\n.gitignore\r\n.env\r\napp.log\r\nenvironment.yml\r\nrunapp.bat\r\nruntrain.bat\r\n# Ignore generated files\r\n**/*.pyc\r\n"
        },
        {
          "name": ".env",
          "type": "blob",
          "size": 0.0615234375,
          "content": "HTTP_PROXY=\nWEB_ADDRESS=127.0.0.1:9988\nENABLE_STS=0\nDEVICE=CUDA"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.5283203125,
          "content": ".idea/\n*.pyc\n*.pyd\n.DS_Store\n__pycache__\nscripts\n\nku\nbuild\ndist\ndocs\ninclude\nLib\nnotebooks\nrecipes\nshare\nvenv\ntests\ndev\nffmpeg.exe\nffprobe.exe\n*.zip\n*.rar\n\n*.exe\n*.log\npyvenv.cfg\nsetup.cfg\n*.pth\n*.pt\ncn1.wav\nsx1.wav\n\n*.spec\ntts/tts_models--multilingual--multi-dataset--xtts_v2\ntts/voice_conversion_models--multilingual--vctk--freevc24\ntts/wavlm\ntts/*.7z\ntts_cache/*\ntts/mymodels/xiaomi\ntts/1voice_conversion_models--multilingual--vctk--freevc24\n\nhubconf.py\nstatic/ttslist/*\nstatic/tmp/*.wav\nstatic/ttslist/*.wav\n*.pth\n*.out\n*.bin\n*.7z\n\ncache"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 4.4599609375,
          "content": "本项目所用模型为[coqui.ai](https://coqui.ai/)出品的xtts_v2，模型开源协议为[Coqui Public Model License 1.0.0](https://coqui.ai/cpml.txt),使用本项目请遵循该协议，协议全文见 https://coqui.ai/cpml.txt\n\n\nThe model used in this project is xtts_v2 produced by [coqui.ai](https://coqui.ai/), and the model open source license is [Coqui Public Model License 1.0.0](https://coqui.ai/cpml.txt) , please follow this agreement when using this project. The full text of the agreement can be found at https://coqui.ai/cpml.txt\n\n----\n\n\nCoqui Public Model License 1.0.0\n\nhttps://coqui.ai/cpml.txt\n\nThis license allows only non-commercial use of a machine learning model and its outputs.\nAcceptance\n\nIn order to get any license under these terms, you must agree to them as both strict obligations and conditions to all your licenses.\nLicenses\n\nThe licensor grants you a copyright license to do everything you might do with the model that would otherwise infringe the licensor's copyright in it, for any non-commercial purpose. The licensor grants you a patent license that covers patent claims the licensor can license, or becomes able to license, that you would infringe by using the model in the form provided by the licensor, for any non-commercial purpose.\nNon-commercial Purpose\n\nNon-commercial purposes include any of the following uses of the model or its output, but only so far as you do not receive any direct or indirect payment arising from the use of the model or its output.\n\n    Personal use for research, experiment, and testing for the benefit of public knowledge, personal study, private entertainment, hobby projects, amateur pursuits, or religious observance.\n    Use by commercial or for-profit entities for testing, evaluation, or non-commercial research and development. Use of the model to train other models for commercial use is not a non-commercial purpose.\n    Use by any charitable organization for charitable purposes, or for testing or evaluation. Use for revenue-generating activity, including projects directly funded by government grants, is not a non-commercial purpose.\n\nNotices\n\nYou must ensure that anyone who gets a copy of any part of the model, or any modification of the model, or their output, from you also gets a copy of these terms or the URL for them above.\nNo Other Rights\n\nThese terms do not allow you to sublicense or transfer any of your licenses to anyone else, or prevent the licensor from granting licenses to anyone else. These terms do not imply any other licenses.\nPatent Defense\n\nIf you make any written claim that the model infringes or contributes to infringement of any patent, your licenses for the model granted under these terms ends immediately. If your company makes such a claim, your patent license ends immediately for work on behalf of your company.\nViolations\n\nThe first time you are notified in writing that you have violated any of these terms, or done anything with the model or its output that is not covered by your licenses, your licenses can nonetheless continue if you come into full compliance with these terms, and take practical steps to correct past violations, within 30 days of receiving notice. Otherwise, all your licenses end immediately.\nNo Liability\n\nAS FAR AS THE LAW ALLOWS, THE MODEL AND ITS OUTPUT COME AS IS, WITHOUT ANY WARRANTY OR CONDITION, AND THE LICENSOR WILL NOT BE LIABLE TO YOU FOR ANY DAMAGES ARISING OUT OF THESE TERMS OR THE USE OR NATURE OF THE MODEL OR ITS OUTPUT, UNDER ANY KIND OF LEGAL CLAIM. IF THIS PROVISION IS NOT ENFORCEABLE IN YOUR JURISDICTION, YOUR LICENSES ARE VOID.\nDefinitions\n\nThe licensor is the individual or entity offering these terms, and the model is the model the licensor makes available under these terms, including any documentation or similar information about the model.\n\nYou refers to the individual or entity agreeing to these terms.\n\nYour company is any legal entity, sole proprietorship, or other kind of organization that you work for, plus all organizations that have control over, are under the control of, or are under common control with that organization. Control means ownership of substantially all the assets of an entity, or the power to direct its management and policies by vote, contract, or otherwise. Control can be direct or indirect.\n\nYour licenses are all the licenses granted to you under these terms.\n\nUse means anything you do with the model or its output requiring one of your licenses.\nWe collect and process your personal information for visitor statistics and browsing behavior. 🍪 "
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.7412109375,
          "content": "[English README](./README_EN.md)  / [捐助项目](https://github.com/jianchang512/pyvideotrans/issues/80) / [Discord](https://discord.gg/7ZWbwKGMcx)\n\n# CV声音克隆工具\n\n> 本项目所用模型为[coqui.ai](https://coqui.ai/)出品的xtts_v2，模型开源协议为[Coqui Public Model License 1.0.0](https://coqui.ai/cpml.txt),使用本项目请遵循该协议，协议全文见 https://coqui.ai/cpml.txt\n\n\n 这是一个声音克隆工具，可使用任何人类音色，将一段文字合成为使用该音色说话的声音，或者将一个声音使用该音色转换为另一个声音。\n \n 使用非常简单，没有N卡GPU也可以使用，下载预编译版本，双击 app.exe 打开一个web界面，鼠标点点就能用。\n \n 支持 **中、英、日、韩、法、德、意等16种语言**，可在线从麦克风录制声音。\n \n 为保证合成效果，建议录制时长5秒到20秒，发音清晰准确，不要存在背景噪声。\n \n 英文效果很棒，中文效果还凑合。\n\n\n> **[赞助商]**\n> \n> [![](https://github.com/user-attachments/assets/5348c86e-2d5f-44c7-bc1b-3cc5f077e710)](https://gpt302.saaslink.net/teRK8Y)\n>  [302.AI](https://gpt302.saaslink.net/teRK8Y)是一个按需付费的一站式AI应用平台，开放平台，开源生态, [302.AI开源地址](https://github.com/302ai)\n> \n> 集合了最新最全的AI模型和品牌/按需付费零月费/管理和使用分离/所有AI能力均提供API/每周推出2-3个新应用\n\n\n# 视频演示\n\n\nhttps://github.com/jianchang512/clone-voice/assets/3378335/4e63f2ac-cc68-4324-a4d9-ecf4d4f81acd\n\n\n\n![image](https://github.com/jianchang512/clone-voice/assets/3378335/5401a3f8-1623-452b-b0b3-cb2efe87e3d1)\n\n\n\n\n# window预编译版使用方法(其他系统可源码部署)\n\n1. [点击此处打开Releases下载页面](https://github.com/jianchang512/clone-voice/releases)，下载预编译版主文件(1.7G) 和 模型(3G)\n\n2. 下载后解压到某处，比如 E:/clone-voice 下\n\n3. 双击 app.exe ，等待自动打开web窗口，**请仔细阅读cmd窗口的文字提示**,如有错误，均会在此显示\n\n4. 模型下载后解压到软件目录下的 `tts` 文件夹内，解压后效果如图 \n\n![image](https://github.com/jianchang512/clone-voice/assets/3378335/4b5a60eb-124d-404b-a748-c0a527482e90)\n\n5. 转换操作步骤\n\t\n\t- 选择【文字->声音】按钮，在文本框中输入文字、或点击导入srt字幕文件，然后点击“立即开始”。\n\t\n\t- 选择【声音->声音】按钮，点击或拖拽要转换的音频文件(mp3/wav/flac)，然后从“要使用的声音文件”下拉框中选择要克隆的音色，如果没有满意的，也可以点击“本地上传”按钮，选择已录制好的5-20s的wav/mp3/flac声音文件。或者点击“开始录制”按钮，在线录制你自己的声音5-20s，录制完成点击使用。然后点击“立即开始”按钮\n\t\n6. 如果机器拥有N卡GPU，并正确配置了CUDA环境，将自动使用CUDA加速\n\n\n\n# 源码部署(linux mac window)\n\n**源码版需要在 .env 中 HTTP_PROXY=设置代理(比如http://127.0.0.1:7890)，要从 https://huggingface.co https://github.com 下载模型，而这个网址国内无法访问，必须保证代理稳定可靠，否则大模型下载可能中途失败**\n\n0. 要求 python 3.9->3.11, 并且提前安装好 git-cmd 工具，[下载地址](https://github.com/git-for-windows/git/releases/download/v2.44.0.windows.1/Git-2.44.0-64-bit.exe)\n1. 创建空目录，比如 E:/clone-voice, 在这个目录下打开 cmd 窗口，方法是地址栏中输入 `cmd`, 然后回车。\n使用git拉取源码到当前目录 ` git clone git@github.com:jianchang512/clone-voice.git . `\n2. 创建虚拟环境 `python -m venv venv`\n3. 激活环境，win下 `E:/clone-voice/venv/scripts/activate`，\n4. 安装依赖: `pip install -r requirements.txt --no-deps`, \nwindows 和 linux 如果要启用cuda加速，继续执行 `pip uninstall -y torch` 卸载，然后执行`pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121`。(必须有N卡并且配置好CUDA环境)\n5. win下解压 ffmpeg.7z，将其中的`ffmpeg.exe`和`app.py`在同一目录下, linux和mac 到 [ffmpeg官网](https://ffmpeg.org/download.html)下载对应版本ffmpeg，解压其中的`ffmpeg`程序到根目录下，必须将可执行二进制文件 `ffmpeg` 和app.py放在同一目录下。\n\n   ![image](https://github.com/jianchang512/clone-voice/assets/3378335/0c61c8b6-7f7e-475f-8984-47fb87ba58e8)\n\n6. **首先运行**  `python  code_dev.py `，在提示同意协议时，输入 `y`，然后等待模型下载完毕。\n   ![](./images/code_dev01.png)\n   ![](./images/code_dev02.png)\n   \n\t下载模型需要挂全局代理，模型非常大，如果代理不够稳定可靠，可能会遇到很多错误，大部分的错误均是代理问题导致。\n\t\n\t如果显示下载多个模型均成功了，但最后还是提示“Downloading WavLM model”错误，则需要修改库包文件 `\\venv\\Lib\\site-packages\\aiohttp\\client.py`, 在大约535行附近，`if proxy is not None:` 上面一行添加你的代理地址，比如 `proxy=\"http://127.0.0.1:10809\"`.\n\n7. 下载完毕后，再启动 `python app.py`\n\n8. **【训练说明】** 如果想训练，执行 `python train.py`, 训练参数在 `param.json`中调整，调整后重新执行训练脚本`python train.py`\n\n8. 每次启动都会连接墙外检测或更新模型，请耐心等待。如果不想每次启动都检测或更新，需手动修改依赖包下文件，打开 \\venv\\Lib\\site-packages\\TTS\\utils\\manage.py ,大约 389 行附近，def download_model 方法中，注释掉如下代码\n\n```\nif md5sum is not None:\n\tmd5sum_file = os.path.join(output_path, \"hash.md5\")\n\tif os.path.isfile(md5sum_file):\n\t    with open(md5sum_file, mode=\"r\") as f:\n\t\tif not f.read() == md5sum:\n\t\t    print(f\" > {model_name} has been updated, clearing model cache...\")\n\t\t    self.create_dir_and_download_model(model_name, model_item, output_path)\n\t\telse:\n\t\t    print(f\" > {model_name} is already downloaded.\")\n\telse:\n\t    print(f\" > {model_name} has been updated, clearing model cache...\")\n\t    self.create_dir_and_download_model(model_name, model_item, output_path)\n```\n\n9. 源码版启动时可能频繁遇到错误，基本都是代理问题导致无法从墙外下载模型或下载中断不完整。建议使用稳定的代理，全局开启。如果始终无法完整下载，建议使用预编译版。\n\n\n\n\n# 常见问题\n\n**模型xtts仅可用于学习研究，不可用于商业**\n\n0. 源码版需要在 .env 中 HTTP_PROXY=设置代理(比如http://127.0.0.1:7890)，要从 https://huggingface.co https://github.com 下载模型，而这个网址国内无法访问，必须保证代理稳定可靠，否则大模型下载可能中途失败\n\n1. 启动后需要冷加载模型，会消耗一些时间，请耐心等待显示出`http://127.0.0.1:9988`， 并自动打开浏览器页面后，稍等两三分钟后再进行转换\n\n2. 功能有：\n\n\t\t文字到语音:即输入文字，用选定的音色生成声音。\n\t\t\n\t\t声音到声音：即从本地选择一个音频文件，用选定的音色生成另一个音频文件.\n\t\t\n3. 如果打开的cmd窗口很久不动，需要在上面按下回车才继续输出，请在cmd左上角图标上单击，选择“属性”，然后取消“快速编辑”和“插入模式”的复选框\n\n![](./images/3.png)\n![](./images/4.png)\n\n\n4. 预编译版 声音-声音线程启动失败\n\n   首先确认模型已正确下载放置。tts文件夹内有3个文件夹，如下图\n   ![image](https://github.com/jianchang512/clone-voice/assets/3378335/4b5a60eb-124d-404b-a748-c0a527482e90)\n\n   如果已正确放置了，但仍错误，[点击下载 extra-to-tts_cache.zip](https://github.com/jianchang512/clone-voice/releases/download/v0.0.1/extra-to-tts_cache.zip) ，将解压后得到的2个文件，复制到软件根目录的 tts_cache 文件夹内\n\n   如果上述方法无效，在 .env 文件中 HTTP_PROXY后填写代理地址比如 `HTTP_PROXY=http://127.0.0.1:7890`，可解决该问题，必须确保代理稳定，填写端口正确\n\n5. 提示 “The text length exceeds the character limit of 182/82 for language”\n\n   这是因为由句号分隔的句子太长导致的，建议将太长的语句使用句号隔开，而不是大量使用逗号，或者你也可以打开 clone/character.json文件，手动修改限制\n   \n6. 提示\"symbol not found __svml_cosf8_ha\"\n\n打开网页 https://www.dll-files.com/svml_dispmd.dll.html ,点击红色\"Download\"下载字样，下载后解压，将里面的dll文件复制粘贴到\"C:\\Windows\\System32\"\n   \n\n\n\n# CUDA 加速支持\n\n**安装CUDA工具** [详细安装方法](https://juejin.cn/post/7318704408727519270)\n\n如果你的电脑拥有 Nvidia 显卡，先升级显卡驱动到最新，然后去安装对应的 \n   [CUDA Toolkit 11.8](https://developer.nvidia.com/cuda-downloads)  和  [cudnn for CUDA11.X](https://developer.nvidia.com/rdp/cudnn-archive)。\n   \n   安装完成成，按`Win + R`,输入 `cmd`然后回车，在弹出的窗口中输入`nvcc --version`,确认有版本信息显示，类似该图\n   ![image](https://github.com/jianchang512/pyvideotrans/assets/3378335/e68de07f-4bb1-4fc9-bccd-8f841825915a)\n\n   然后继续输入`nvidia-smi`,确认有输出信息，并且能看到cuda版本号，类似该图\n   ![image](https://github.com/jianchang512/pyvideotrans/assets/3378335/71f1d7d3-07f9-4579-b310-39284734006b)\n\n   说明安装正确，可以cuda加速了，否则需重新安装\n\n\n\n# 相关联项目\n\n[视频翻译配音工具:翻译字幕并配音](https://github.com/jianchang512/pyvideotrans)\n\n[语音识别工具:本地离线的语音识别转文字工具](https://github.com/jianchang512/stt)\n\n[人声背景乐分离:极简的人声和背景音乐分离工具，本地化网页操作](https://github.com/jianchang512/vocal-separate)\n\n\n# [Youtube演示视频](https://youtu.be/CC227GXOJLk)\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 7.98828125,
          "content": "[简体中文](./README.md) / [Discord](https://discord.gg/TMCM2PfHzQ) / [Buy me a coffee](https://ko-fi.com/jianchang512) / [Twitter](https://twitter.com/mortimer_wang)\n\n# CV Voice Clone Tool\n\n> The model used in this project is xtts_v2 produced by [coqui.ai](https://coqui.ai/), and the model open source license is [Coqui Public Model License 1.0.0](https://coqui.ai/cpml.txt) , please follow this agreement when using this project. The full text of the agreement can be found at https://coqui.ai/cpml.txt\n\n\n \n This is a voice cloning tool that can use any human voice to synthesize a piece of text into a voice using that voice, or to convert one voice into another using that voice. \n \n It's very easy to use, even without an N-series GPU. Download the precompiled version and double click on app.exe to open a web interface, and it can be used with a few mouse clicks. \n \n Supports **Chinese English Japanese Korean eg. total 16 languages**, and can record voices online through a microphone. \n \n To ensure the synthesized effect, it's recommended to record for 5 to 20 seconds, pronounce clearly and accurately, and don't have background noise. \n \n \n\n\n# Video Demonstration\n\n\n\nhttps://github.com/jianchang512/clone-voice/assets/3378335/813d46dd-7634-43d1-97ae-1531369c471f\n\n\n\n\n\n![image](https://github.com/jianchang512/clone-voice/assets/3378335/e4cfee2a-20f1-4395-b1b9-b3f7015502a2)\n\n\n\n\n# How to use the precompiled version under win (other systems can deploy source code)\n\n\n1. Download the 'precompiled version of the main file(1.7G) and Model(3G) separately from [Releases](https://github.com/jianchang512/clone-voice/releases) on the right. \n2. After downloading, unzip it to somewhere, for example E:/clone-voice. \n3. Double click app.exe, wait for the web window to open automatically, **Please read the text prompts in the CMD window carefully**, if there are errors, they will be displayed here.\n\n\n4. After the model download, unzip it to the tts folder under the software directory, the effect after unzipping is as shown in the picture\n\n\n![image](https://github.com/jianchang512/clone-voice/assets/3378335/4b5a60eb-124d-404b-a748-c0a527482e90)\n\n5. Conversion operation steps:\n\t\n\t- Enter the text in the text box, or import the SRT file, or select \"Voice-> Voice\", choose the voice wav format file you want to convert.\n\t\n\t- Then select the voice you want to use from the drop-down box under \"Voice wav file to use\", if you are not satisfied, you can also click the \"Upload locally\" button, select a recorded 5-20s wav voice file. Or click the \"Start recording\" button to record your own voice for 5-20 seconds online, after recording, click to use.\n\t\n\t- Click the \"Start Generating Now\" button and wait patiently for completion.\n\n6. If the machine has an N card GPU and CUDA environment is correctly configured, CUDA acceleration will be used automatically.\n\n\n# Source Code Deployment (linux mac window) / Example: window\n\n**If your area can't access google and huggingface, you'll need a global proxy because models need to be downloaded from github and huggingface**\n\n\n0. Required python 3.9-> 3.11, and enable a global proxy, ensure the proxy is stable\n1. Create an empty directory, such as E:/clone-voice, open a cmd window in this directory, the method is to type `cmd` in the address bar, then press Enter.\nand exec git pull source code `git clone git@github.com:jianchang512/clone-voice.git . `\n2. Create a virtual environment `python -m venv venv`\n3. Activate the environment `E:/clone-voice/venv/scripts/activate`, linux and Mac exec `source ./venv/bin/activate`\n4. Install dependencies: `pip install -r requirements.txt`\n5. Unzip the ffmpeg.7z to the project root directory;for Linux and Mac, download the corresponding version of ffmpeg from the [ffmpeg official website](https://ffmpeg.org/download.html), unzip it to the root directory, and make sure to place the executable file ffmepg directly in the root directory.\n\n    ![image](https://github.com/jianchang512/clone-voice/assets/3378335/0c61c8b6-7f7e-475f-8984-47fb87ba58e8)\n   \n6. **First run** `python code_dev.py`, enter `y` when prompted to accept the agreement, then wait for the model to be downloaded completely.\n   ![](./images/code_dev02.png)\n\n\n7. After downloading, restart `python app.py`.\n\n8. Every startup will connect to the foreign Internet to check or update the model, please be patient and wait. If you don't want to check or update every time you start, you need to manually modify the files under the dependent package, open \\venv\\Lib\\site-packages\\TTS\\utils\\manage.py, around line 389, def download_model method, comment out the following code.\n\n```\nif md5sum is not None:\n\tmd5sum_file = os.path.join(output_path, \"hash.md5\")\n\tif os.path.isfile(md5sum_file):\n\t    with open(md5sum_file, mode=\"r\") as f:\n\t\tif not f.read() == md5sum:\n\t\t    print(f\" > {model_name} has been updated, clearing model cache...\")\n\t\t    self.create_dir_and_download_model(model_name, model_item, output_path)\n\t\telse:\n\t\t    print(f\" > {model_name} is already downloaded.\")\n\telse:\n\t    print(f\" > {model_name} has been updated, clearing model cache...\")\n\t    self.create_dir_and_download_model(model_name, model_item, output_path)\n```\n\n9. The startup of the source code version may frequently encounter errors, which are basically due to proxy problems that prevent the download of models from the walls or the download is interrupted and not complete. It is recommended to use a stable proxy and open it globally. If you can't download completely all the time, it's recommended to use the precompiled version.\n\n\n# CUDA Acceleration Support\n\n**Installation of CUDA tools**\n\nIf your computer has Nvidia graphics card, upgrade the graphics card driver to the latest, then go to install the corresponding [CUDA Toolkit 11.8](https://developer.nvidia.com/cuda-downloads) and [cudnn for CUDA11.X](https://developer.nvidia.com/rdp/cudnn-archive).\n   \nWhen installation is complete, press `Win + R`, type `cmd` then press Enter, in the pop-up window type `nvcc --version`, confirm the version information display, similar to this image\n   ![image](https://github.com/jianchang512/pyvideotrans/assets/3378335/e68de07f-4bb1-4fc9-bccd-8f841825915a)\n   \nThen continue to type `nvidia-smi`, confirm there's output information, and you can see the cuda version number, similar to this image\n   ![image](https://github.com/jianchang512/pyvideotrans/assets/3378335/71f1d7d3-07f9-4579-b310-39284734006b)\n\nThat means the installation is correct, you can cuda accelerate now, otherwise you need to reinstall.\n\n\n\n# Precautions\n\nThe model xtts can only be used for study and research, not for commerical use\n\n0. The source code version requires global proxy, because it needs to download models from https://huggingface.co, and this website can't be accessed in China, the source code version may frequently encounter errors when starting, basically proxy problems lead to unable to download models from overseas or download interruption incomplete. It's recommended to use a stable proxy, open it globally. If you can't download completely all the time, it's recommended to use the precompiled version.\n\n1. It will consume some time to load the model coldly after starting, please wait patiently for `http://127.0.0.1:9988` to be displayed, and automatically open the browser page, wait for two or three minutes before converting.\n\n2. Functions include:\n\n\t\tText to voice: that is, enter the text, generate voice with the selected voice.\n\t\t\n\t\tVoice to Voice: that is, select an audio file from the local area, generate another audio file with the selected voice.\n\t\t\n3. If the cmd window opened for a long time doesn't move, you need to press Enter on it to continue output, please click on the icon in the upper left corner of cmd, select \"Properties\", then uncheck the \"Quick Edit\" and \"Insert Mode\" checkboxes\n\n\n\n4. “The text length exceeds the character limit of 182/82 for language”\n\n  This is because sentences separated by periods are too long. It is recommended to use periods to separate sentences that are too long, rather than excessive use of commas,\n\n\n\n# [Youtube Demo Video](https://youtu.be/NL5cIoJ9Gjo)\n"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 16.8603515625,
          "content": "import datetime\nimport logging\nimport queue\nimport re\nimport threading\nimport time\nimport sys\nfrom flask import Flask, request, render_template, jsonify, send_file, send_from_directory\nimport os\nimport glob\nimport hashlib\nfrom logging.handlers import RotatingFileHandler\n\nimport clone\nfrom clone import cfg\nfrom clone.cfg import ROOT_DIR, TTS_DIR, VOICE_MODEL_EXITS, TMP_DIR, VOICE_DIR, TEXT_MODEL_EXITS, langlist\nfrom clone.logic import ttsloop, stsloop, create_tts, openweb, merge_audio_segments, get_subtitle_from_srt, updatecache\nfrom clone import logic\nimport shutil\nimport subprocess\nfrom dotenv import load_dotenv\nfrom waitress import serve\nload_dotenv()\n\nweb_address = os.getenv('WEB_ADDRESS', '127.0.0.1:9988')\nenable_sts = int(os.getenv('ENABLE_STS', '0'))\n\n\n\nupdatecache()\n\n# 配置日志\n# 禁用 Werkzeug 默认的日志处理器\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\n\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\n\nroot_log = logging.getLogger()  # Flask的根日志记录器\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\napp.logger.setLevel(logging.WARNING)  # 设置日志级别为 INFO\n# 创建 RotatingFileHandler 对象，设置写入的文件路径和大小限制\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'app.log'), maxBytes=1024 * 1024, backupCount=5)\n# 创建日志的格式\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# 设置文件处理器的级别和格式\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# 将文件处理器添加到日志记录器中\napp.logger.addHandler(file_handler)\napp.jinja_env.globals.update(enumerate=enumerate)\n\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",\n                           text_model=TEXT_MODEL_EXITS,\n                           voice_model=VOICE_MODEL_EXITS,\n                           version=clone.ver,\n                           mymodels=cfg.MYMODEL_OBJS,\n                           language=cfg.LANG,\n                           langlist=cfg.langlist,\n                           root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n# 上传音频\n@app.route('/upload', methods=['POST'])\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # 获取上传的文件\n        audio_file = request.files['audio']\n        save_dir = request.form.get(\"save_dir\")\n        save_dir = VOICE_DIR if not save_dir else os.path.join(ROOT_DIR, f'static/{save_dir}')\n        app.logger.info(f\"[upload]{audio_file.filename=},{save_dir=}\")\n        # 检查文件是否存在且是 WAV/mp3格式\n        noextname, ext = os.path.splitext(os.path.basename(audio_file.filename.lower()))\n        noextname = noextname.replace(' ', '')\n        if audio_file and ext in [\".wav\", \".mp3\", \".flac\"]:\n            # 保存文件到服务器指定目录\n            name = f'{noextname}{ext}'\n            if os.path.exists(os.path.join(save_dir, f'{noextname}{ext}')):\n                name = f'{datetime.datetime.now().strftime(\"%m%d-%H%M%S\")}-{noextname}{ext}'\n            # mp3 or wav           \n            tmp_wav = os.path.join(TMP_DIR, \"tmp_\" + name)\n            audio_file.save(tmp_wav)\n            # save to wav\n            if ext != '.wav':\n                name = f\"{name[:-len(ext)]}.wav\"\n            savename = os.path.join(save_dir, name)\n            subprocess.run(['ffmpeg', '-hide_banner', '-y', '-i', tmp_wav, savename], check=True)\n            try:\n                os.unlink(tmp_wav)\n            except:\n                pass\n            # 返回成功的响应\n            return jsonify({'code': 0, 'msg': 'ok', \"data\": name})\n        else:\n            # 返回错误的响应\n            return jsonify({'code': 1, 'msg': 'not wav'})\n    except Exception as e:\n        app.logger.error(f'[upload]error: {e}')\n        return jsonify({'code': 2, 'msg': 'error'})\n\n\n# 从 voicelist 目录获取可用的 wav 声音列表\n@app.route('/init')\ndef init():\n    wavs = glob.glob(f\"{VOICE_DIR}/*.wav\")\n    result = []\n    for it in wavs:\n        if os.path.getsize(it) > 0:\n            result.append(os.path.basename(it))\n    result.extend(cfg.MYMODEL_OBJS.keys())\n    return jsonify(result)\n\n\n# 判断线程是否启动\n@app.route('/isstart', methods=['GET', 'POST'])\ndef isstart():\n    return jsonify(cfg.MYMODEL_OBJS)\n\n\n# 外部接口\n@app.route('/apitts', methods=['GET', 'POST'])\ndef apitts():\n    '''\n    audio:原始声音wav,作为音色克隆源\n    voice:已有的声音名字，如果存在 voice则先使用，否则使用audio\n    text:文字一行\n    language：语言代码\n    Returns:\n    '''\n    try:\n        langcodelist = [\"zh-cn\", \"en\", \"ja\", \"ko\", \"es\", \"de\", \"fr\", \"it\", \"tr\", \"ru\", \"pt\", \"pl\", \"nl\", \"ar\", \"hu\", \"cs\"]\n        text = request.form.get(\"text\",\"\").strip()\n        model = request.form.get(\"model\",\"\").strip()\n        text = text.replace(\"\\n\", ' . ')\n        language = request.form.get(\"language\", \"\").lower()\n        if language.startswith(\"zh\"):\n            language = \"zh-cn\"\n        if language not in langcodelist:\n            return jsonify({\"code\": 1, \"msg\": f\" {language} dont support language \"})\n\n        md5_hash = hashlib.md5()\n\n        audio_name = request.form.get('voice','')\n        voicename=\"\"\n        model=\"\"\n        # 存在传来的声音文件名字\n        print(f'1,{text=},{model=},{audio_name=},{language=}')\n        if audio_name and audio_name.lower().endswith('.wav'):\n            voicename = os.path.join(VOICE_DIR, audio_name)\n            if not os.path.exists(voicename):\n                return jsonify({\"code\": 2, \"msg\": f\"{audio_name} 不存在\"})\n            if os.path.isdir(voicename):\n                model=audio_name\n                voicename=\"\"\n        elif audio_name:\n            #存在，是新模型\n            model=audio_name\n        elif not audio_name:  # 不存在，原声复制 clone 获取上传的文件\n            audio_file = request.files['audio']\n            print(f'{audio_file.filename}')\n            # 保存临时上传过来的声音文件\n            audio_name = f'video_{audio_file.filename}.wav'\n            voicename = os.path.join(TMP_DIR, audio_name)\n            audio_file.save(voicename)\n        print(f'22={text=},{model=},{audio_name=},{language=}')\n        md5_hash.update(f\"{text}-{language}-{audio_name}-{model}\".encode('utf-8'))\n\n        app.logger.info(f\"[apitts]{voicename=}\")\n        if re.match(r'^[~`!@#$%^&*()_+=,./;\\':\\[\\]{}<>?\\\\|\"，。？；‘：“”’｛【】｝！·￥、\\s\\n\\r -]*$', text):\n            return jsonify({\"code\": 3, \"msg\": \"lost text for translate\"})\n        if not text or not language:\n            return jsonify({\"code\": 4, \"msg\": \"text & language params lost\"})\n        app.logger.info(f\"[apitts]{text=},{language=}\")\n\n        # 存放结果\n        # 合成后的语音文件, 以wav格式存放和返回\n        filename = md5_hash.hexdigest() + \".wav\"\n        app.logger.info(f\"[apitts]{filename=}\")\n        # 合成语音\n        rs = create_tts(text=text,model=model, speed=1.0, voice=voicename, language=language, filename=filename)\n        # 已有结果或错误，直接返回\n        if rs is not None:\n            print(f'{rs=}')\n            result = rs\n        else:\n            # 循环等待 最多7200s\n            time_tmp = 0\n            while filename not in cfg.global_tts_result:\n                time.sleep(3)\n                time_tmp += 3\n                if time_tmp % 30 == 0:\n                    app.logger.info(f\"[apitts][tts]{time_tmp=},{filename=}\")\n                if time_tmp>3600:\n                    return jsonify({\"code\": 5, \"msg\": f'error:{text}'})\n                    \n\n            # 当前行已完成合成\n            target_wav = os.path.normpath(os.path.join(TTS_DIR, filename))\n            if not os.path.exists(target_wav):\n                msg = {\"code\": 6, \"msg\": cfg.global_tts_result[filename] if filename in cfg.global_tts_result else \"error\"}\n            else:\n                \n                msg = {\"code\": 0, \"filename\": target_wav, 'name': filename}\n            app.logger.info(f\"[apitts][tts] {filename=},{msg=}\")\n            try:\n                cfg.global_tts_result.pop(filename)\n            except:\n                pass\n            result = msg\n            app.logger.info(f\"[apitts]{msg=}\")\n        if result['code'] == 0:\n            result['url'] = f'http://{web_address}/static/ttslist/{filename}'\n        return jsonify(result)\n    except Exception as e:\n        msg = f'{str(e)} {str(e.args)}'\n        app.logger.error(f\"[apitts]{msg}\")\n        return jsonify({'code': 7, 'msg': msg})\n\n\n# 根据文本返回tts结果，返回 name=文件名字，filename=文件绝对路径\n# 请求端根据需要自行选择使用哪个\n# params\n# text:待合成文字\n# voice：声音文件\n# language:语言代码\n@app.route('/tts', methods=['GET', 'POST'])\ndef tts():\n    # 原始字符串\n    text = request.form.get(\"text\",\"\").strip()\n    voice = request.form.get(\"voice\",'')\n    speed = 1.0\n    try:\n        speed = float(request.form.get(\"speed\",1))\n    except:\n        pass\n    language = request.form.get(\"language\",'')\n    model = request.form.get(\"model\",\"\")\n    app.logger.info(f\"[tts][tts]recev {text=}\\n{voice=},{language=}\\n\")\n\n    if re.match(r'^[~`!@#$%^&*()_+=,./;\\':\\[\\]{}<>?\\\\|\"，。？；‘：“”’｛【】｝！·￥、\\s\\n\\r -]*$', text):\n        return jsonify({\"code\": 1, \"msg\": \"no text\"})\n    if not text or not voice or not language:\n        return jsonify({\"code\": 1, \"msg\": \"text/voice/language params lost\"})\n\n    # 判断是否是srt\n    text_list = get_subtitle_from_srt(text)\n    app.logger.info(f\"[tts][tts]{text_list=}\")\n    is_srt = True\n    # 不是srt格式,则按行分割\n    if text_list is None:\n        is_srt = False\n        text_list = []\n        for it in text.split(\"\\n\"):\n            text_list.append({\"text\": it.strip()})\n        app.logger.info(f\"[tts][tts] its not srt\")\n\n    num = 0\n    while num < len(text_list):\n        t = text_list[num]\n        # 换行符改成 .\n        t['text'] = t['text'].replace(\"\\n\", ' . ')\n        md5_hash = hashlib.md5()\n        md5_hash.update(f\"{t['text']}-{voice}-{language}-{speed}-{model}\".encode('utf-8'))\n        filename = md5_hash.hexdigest() + \".wav\"\n        app.logger.info(f\"[tts][tts]{filename=}\")\n        # 合成语音\n        rs = create_tts(text=t['text'], model=model,speed=speed, voice=os.path.join(cfg.VOICE_DIR, voice), language=language, filename=filename)\n        # 已有结果或错误，直接返回\n        if rs is not None:\n            text_list[num]['result'] = rs\n            num += 1\n            continue\n        # 循环等待 最多7200s\n        time_tmp = 0\n        # 生成的目标音频\n        target_wav = os.path.normpath(os.path.join(TTS_DIR, filename))\n        msg=None\n        while filename not in cfg.global_tts_result and not os.path.exists(target_wav):\n            time.sleep(3)\n            time_tmp += 3\n            if time_tmp % 30 == 0:\n                app.logger.info(f\"[tts][tts]{time_tmp=},{filename=}\")\n            if time_tmp>3600:\n                msg={\"code\": 1, \"msg\":f'{filename} error'}\n                text_list[num]['result'] = msg\n                num+=1\n                break\n        if msg is not None:\n            continue\n                \n\n        # 当前行已完成合成\n        if not os.path.exists(target_wav):\n            msg = {\"code\": 1, \"msg\": \"not exists\"}\n        else:\n            if speed != 1.0 and speed > 0 and speed <= 2.0:\n                # 生成的加速音频\n                speed_tmp = os.path.join(TMP_DIR, f'speed_{time.time()}.wav')\n                p = subprocess.run(\n                    ['ffmpeg', '-hide_banner', '-ignore_unknown', '-y', '-i', target_wav, '-af', f\"atempo={speed}\",\n                     os.path.normpath(speed_tmp)], encoding=\"utf-8\", capture_output=True)\n                if p.returncode != 0:\n                    return jsonify({\"code\": 1, \"msg\": str(p.stderr)})\n                shutil.copy2(speed_tmp, target_wav)\n            msg = {\"code\": 0, \"filename\": target_wav, 'name': filename}\n        app.logger.info(f\"[tts][tts] {filename=},{msg=}\")\n        try:\n            cfg.global_tts_result.pop(filename)\n        except:\n            pass\n        text_list[num]['result'] = msg\n        app.logger.info(f\"[tts][tts]{num=}\")\n        num += 1\n\n    filename, errors = merge_audio_segments(text_list, is_srt=is_srt)\n    app.logger.info(f\"[tts][tts]is srt，{filename=},{errors=}\")\n    if filename and os.path.exists(filename) and os.path.getsize(filename) > 0:\n        res = {\"code\": 0, \"filename\": filename, \"name\": os.path.basename(filename), \"msg\": errors}\n    else:\n        res = {\"code\": 1, \"msg\": f\"error:{filename=},{errors=}\"}\n    app.logger.info(f\"[tts][tts]end result:{res=}\")\n    return jsonify(res)\n\n\n# s to s wav->wav\n# params\n# voice: 声音文件\n# filename: 上传的原始声音\n\n@app.route('/sts', methods=['GET', 'POST'])\ndef sts():\n    try:\n        # 保存文件到服务器指定目录\n        # 目标\n        voice = request.form.get(\"voice\",'')\n        filename = request.form.get(\"name\",'')\n        app.logger.info(f\"[sts][sts]sts {voice=},{filename=}\\n\")\n\n        if not voice:\n            return jsonify({\"code\": 1, \"msg\": \"voice params lost\"})\n\n        obj = {\"filename\": filename, \"voice\": voice}\n        # 压入队列，准备转换语音\n        app.logger.info(f\"[sts][sts]push sts\")\n        cfg.q_sts.put(obj)\n        # 已有结果或错误，直接返回\n        # 循环等待 最多7200s\n        time_tmp = 0\n        while filename not in cfg.global_sts_result:\n            time.sleep(3)\n            time_tmp += 3\n            if time_tmp % 30 == 0:\n                app.logger.info(f\"{time_tmp=}，{filename=}\")\n\n        # 当前行已完成合成\n        if cfg.global_sts_result[filename] != 1:\n            msg = {\"code\": 1, \"msg\": cfg.global_sts_result[filename]}\n            app.logger.error(f\"[sts][sts]error，{msg=}\")\n        else:\n            msg = {\"code\": 0, \"filename\": os.path.join(TTS_DIR, filename), 'name': filename}\n            app.logger.info(f\"[sts][sts]ok,{msg=}\")\n        cfg.global_sts_result.pop(filename)\n        return jsonify(msg)\n    except Exception as e:\n        app.logger.error(f\"[sts][sts]error:{str(e)}\")\n        return jsonify({'code': 2, 'msg': f'voice->voice:{str(e)}'})\n\n\n\n\n# 启动或关闭模型\n@app.route('/onoroff',methods=['GET','POST'])\ndef onoroff():\n    name = request.form.get(\"name\",'')\n    status_new = request.form.get(\"status_new\",'')\n    if status_new=='on':\n        if name not in cfg.MYMODEL_OBJS  or not cfg.MYMODEL_OBJS[name] or  isinstance(cfg.MYMODEL_OBJS[name],str):\n            try:\n                print(f'start {name}...')\n                res=logic.load_model(name)\n                print(f'{res=}')\n                return jsonify({\"code\":0,\"msg\":res})\n            except Exception as e:\n                return jsonify({\"code\":1,\"msg\":str(e)})\n        elif cfg.MYMODEL_OBJS[name] in ['error','no']:\n            return jsonify({\"code\":0,\"msg\":\"模型启动出错或不存在\"})\n        return jsonify({\"code\":0,\"msg\":\"已启动\"})\n    else:\n        #关闭\n        cfg.MYMODEL_OBJS[name]=None\n        #删除队列\n        cfg.MYMODEL_QUEUE[name]=None\n        return jsonify({\"code\":0,\"msg\":\"已停止\"})\n\n@app.route('/checkupdate', methods=['GET', 'POST'])\ndef checkupdate():\n    return jsonify({'code': 0, \"msg\": cfg.updatetips})\n\n@app.route('/stsstatus', methods=['GET', 'POST'])\ndef stsstatus():\n    return jsonify({'code': 0, \"msg\": \"start\" if cfg.sts_status else \"stop\"})\n\n\n\nif __name__ == '__main__':\n\n    tts_thread = None\n    sts_thread = None\n    try:\n        if 'app.py' == sys.argv[0] and 'app.py' == os.path.basename(__file__):\n            print(langlist[\"lang1\"])\n\n        threading.Thread(target=logic.checkupdate).start()\n\n        # 如果存在默认模型则启动\n        \n        if TEXT_MODEL_EXITS:\n            print(\"\\n\"+langlist['lang2'])\n            tts_thread = threading.Thread(target=ttsloop)\n            tts_thread.start()\n        else:\n            app.logger.error(\n                f\"\\n{langlist['lang3']}: {cfg.download_address}\\n\")\n            input(f\"\\n{langlist['lang3']}: {cfg.download_address}\\n\")\n            sys.exit()\n        \n        if enable_sts==1 and VOICE_MODEL_EXITS:\n            print(langlist['lang4'])\n            sts_thread = threading.Thread(target=stsloop)\n            sts_thread.start()\n        #else:\n        #    app.logger.error(\n        #        f\"\\n{langlist['lang5']}: {cfg.download_address}\\n\")\n        \n        print(langlist['lang7'])\n        try:\n            host = web_address.split(':')\n            threading.Thread(target=openweb, args=(web_address,)).start()\n            serve(app,host=host[0], port=int(host[1]))\n        finally:\n           print('exit')\n    except Exception as e:\n        print(\"error:\" + str(e))\n        app.logger.error(f\"[app]start error:{str(e)}\")\n        time.sleep(30)\n        sys.exit()\n"
        },
        {
          "name": "appdingzhi.py",
          "type": "blob",
          "size": 20.966796875,
          "content": "import datetime\nimport logging\nimport re\nimport threading\nimport time\nimport sys\nfrom flask import Flask, request, render_template, jsonify, send_file, send_from_directory\nimport os\nfrom gevent.pywsgi import WSGIServer, WSGIHandler\nimport glob\nimport hashlib\nfrom logging.handlers import RotatingFileHandler\n\nimport clone\nfrom clone import cfg\nfrom clone.cfg import ROOT_DIR, TTS_DIR, VOICE_MODEL_EXITS, TMP_DIR, VOICE_DIR, TEXT_MODEL_EXITS, langlist\nfrom clone.logic import ttsloop, stsloop, create_tts, openweb, merge_audio_segments, get_subtitle_from_srt, updatecache\nfrom clone import logic\nfrom gevent.pywsgi import LoggingLogAdapter\nimport shutil\nimport subprocess\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nweb_address = os.getenv('WEB_ADDRESS', '127.0.0.1:9988')\n\n\nclass CustomRequestHandler(WSGIHandler):\n    def log_request(self):\n        pass\n\n\n#updatecache()\n\n# 配置日志\n# 禁用 Werkzeug 默认的日志处理器\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\n\napp = Flask(__name__, static_folder=os.path.join(ROOT_DIR, 'static'), static_url_path='/static',\n            template_folder=os.path.join(ROOT_DIR, 'templates'))\n\nroot_log = logging.getLogger()  # Flask的根日志记录器\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\n\napp.logger.setLevel(logging.INFO)  # 设置日志级别为 INFO\n# 创建 RotatingFileHandler 对象，设置写入的文件路径和大小限制\nfile_handler = RotatingFileHandler(os.path.join(ROOT_DIR, 'app.log'), maxBytes=1024 * 1024, backupCount=5)\n# 创建日志的格式\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# 设置文件处理器的级别和格式\nfile_handler.setLevel(logging.INFO)\nfile_handler.setFormatter(formatter)\n# 将文件处理器添加到日志记录器中\napp.logger.addHandler(file_handler)\n\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n\n@app.route('/')\ndef index():\n    return render_template(\"index.html\",\n                           text_model=TEXT_MODEL_EXITS,\n                           voice_model=VOICE_MODEL_EXITS,\n                           version=clone.ver,\n                           language=cfg.LANG,\n                           root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n@app.route('/txt')\ndef txt():\n    return render_template(\"txt.html\",\n                           text_model=True,#TEXT_MODEL_EXITS,\n                           version=clone.ver,\n                           language=cfg.LANG,\n                           root_dir=ROOT_DIR.replace('\\\\', '/'))\n\n\n\n# 上传音频\n@app.route('/upload', methods=['POST'])\ndef upload():\n    try:\n        # 获取上传的文件\n        audio_file = request.files['audio']\n        save_dir = request.form.get(\"save_dir\")\n        save_dir = VOICE_DIR if not save_dir else os.path.join(ROOT_DIR, f'static/{save_dir}')\n        app.logger.info(f\"[upload]{audio_file.filename=},{save_dir=}\")\n        # 检查文件是否存在且是 WAV/mp3格式\n        noextname, ext = os.path.splitext(os.path.basename(audio_file.filename.lower()))\n        noextname = noextname.replace(' ', '')\n        if audio_file and ext in [\".wav\", \".mp3\", \".flac\"]:\n            # 保存文件到服务器指定目录\n            name = f'{noextname}{ext}'\n            if os.path.exists(os.path.join(save_dir, f'{noextname}{ext}')):\n                name = f'{datetime.datetime.now().strftime(\"%m%d-%H%M%S\")}-{noextname}{ext}'\n            # mp3 or wav           \n            tmp_wav = os.path.join(TMP_DIR, \"tmp_\" + name)\n            audio_file.save(tmp_wav)\n            # save to wav\n            if ext != '.wav':\n                name = f\"{name[:-len(ext)]}.wav\"\n            savename = os.path.join(save_dir, name)\n            subprocess.run(['ffmpeg', '-hide_banner', '-y', '-i', tmp_wav, savename], check=True)\n            try:\n                os.unlink(tmp_wav)\n            except:\n                pass\n            # 返回成功的响应\n            return jsonify({'code': 0, 'msg': 'ok', \"data\": name})\n        else:\n            # 返回错误的响应\n            return jsonify({'code': 1, 'msg': 'not wav'})\n    except Exception as e:\n        app.logger.error(f'[upload]error: {e}')\n        return jsonify({'code': 2, 'msg': 'error'})\n\n\n# 从 voicelist 目录获取可用的 wav 声音列表\n@app.route('/init')\ndef init():\n    wavs = glob.glob(f\"{VOICE_DIR}/*.wav\")\n    result = []\n    for it in wavs:\n        if os.path.getsize(it) > 0:\n            result.append(os.path.basename(it))\n    return jsonify(result)\n\n\n# 判断线程是否启动\n@app.route('/isstart', methods=['GET', 'POST'])\ndef isstart():\n    total = cfg.tts_n + cfg.sts_n\n    return jsonify({\"code\": 0, \"msg\": total, \"tts\": cfg.langlist['lang15'] if cfg.tts_n < 1 else \"\",\n                    \"sts\": cfg.langlist['lang16'] if cfg.sts_n < 1 else \"\"})\n\n\n# 外部接口\n@app.route('/apitts', methods=['GET', 'POST'])\ndef apitts():\n    '''\n    audio:原始声音wav,作为音色克隆源\n    voice:已有的声音名字，如果存在 voice则先使用，否则使用audio\n    text:文字一行\n    language：语言代码\n    Returns:\n    '''\n    try:\n        langcodelist=[\"zh-cn\",\"en\",\"ja\",\"ko\",\"es\",\"de\",\"fr\",\"it\",\"tr\",\"ru\",\"pt\",\"pl\",\"nl\",\"ar\",\"hu\",\"cs\"]\n        text = request.form.get(\"text\").strip()\n        text = text.replace(\"\\n\", ' . ')\n        language = request.form.get(\"language\",\"\").lower()\n        if language.startswith(\"zh\"):\n            language=\"zh-cn\"\n        if language not in langcodelist:\n            return jsonify({\"code\":1,\"msg\":f\"dont support language {language}\"})\n\n        md5_hash = hashlib.md5()\n\n        audio_name = request.form.get('voice')\n        # 存在传来的声音文件名字\n        if audio_name:\n            voicename = os.path.join(VOICE_DIR, audio_name)\n        else:  # 获取上传的文件\n            audio_file = request.files['audio']\n            print(f'{audio_file.filename}')\n            # 保存临时上传过来的声音文件\n            audio_name = f'video_{audio_file.filename}.wav'\n            voicename = os.path.join(TMP_DIR, audio_name)\n            audio_file.save(voicename)\n        md5_hash.update(f\"{text}-{language}-{audio_name}\".encode('utf-8'))\n\n        app.logger.info(f\"[apitts]{voicename=}\")\n        if re.match(r'^[~`!@#$%^&*()_+=,./;\\':\\[\\]{}<>?\\\\|\"，。？；‘：“”’｛【】｝！·￥、\\s\\n\\r -]*$', text):\n            return jsonify({\"code\": 1, \"msg\": \"lost text for translate\"})\n        if not text or not language:\n            return jsonify({\"code\": 1, \"msg\": \"text & language params lost\"})\n        app.logger.info(f\"[apitts]{text=},{language=}\")\n\n        # 存放结果\n        # 合成后的语音文件, 以wav格式存放和返回\n        filename = md5_hash.hexdigest() + \".wav\"\n        app.logger.info(f\"[apitts]{filename=}\")\n        # 合成语音\n        rs = create_tts(text=text, speed=1.0, voice=voicename, language=language, filename=filename)\n        # 已有结果或错误，直接返回\n        if rs is not None:\n            result = rs\n        else:\n            # 循环等待 最多7200s\n            time_tmp = 0\n            while filename not in cfg.global_tts_result:\n                time.sleep(3)\n                time_tmp += 3\n                if time_tmp % 30 == 0:\n                    app.logger.info(f\"[apitts][tts]{time_tmp=},{filename=}\")\n\n            # 当前行已完成合成\n            if cfg.global_tts_result[filename] != 1:\n                msg = {\"code\": 1, \"msg\": cfg.global_tts_result[filename]}\n            else:\n                target_wav = os.path.normpath(os.path.join(TTS_DIR, filename))\n                msg = {\"code\": 0, \"filename\": target_wav, 'name': filename}\n            app.logger.info(f\"[apitts][tts] {filename=},{msg=}\")\n            cfg.global_tts_result.pop(filename)\n            result = msg\n            app.logger.info(f\"[apitts]{msg=}\")\n        if result['code'] == 0:\n            result['url'] = f'http://{web_address}/static/ttslist/{filename}'\n        return jsonify(result)\n    except Exception as e:\n        msg=f'{str(e)} {str(e.args)}'\n        app.logger.error(f\"[apitts]{msg}\")\n        return jsonify({'code': 2, 'msg': msg})\n\nchuliing={\"name\":\"\",\"line\":0,\"end\":False}\n\n# 获取进度\n@app.route('/ttslistjindu',methods=['GET', 'POST'])\ndef ttslistjindu():\n    return jsonify(chuliing)\n\n# 具体起一个新线程执行\ndef detail_task(*pams):\n    global chuliing\n    chuliing={\"name\":\"\",\"line\":0,\"end\":False}\n    voice, src, dst, speed, language=pams\n  \n    # 遍历所有txt文件\n    for t in os.listdir(src):\n        if not t.lower().endswith('.txt'):\n            continue\n        concat_txt=os.path.join(cfg.TTS_DIR, re.sub(r'[ \\s\\[\\]\\{\\}\\(\\)<>\\?\\, :]+','', t, re.I) + '.txt')\n        \n        app.logger.info(f'####开始处理文件：{t}, 每行结果保存在:{concat_txt}')\n        with open(concat_txt,'w',encoding='utf-8') as f:\n            f.write(\"\")\n        #需要等待执行完毕的数据 [{}, {}]\n        waitlist=[]\n        #已执行完毕的 {1:{}, 2:{}}\n        result={}\n        with open(os.path.join(src,t),'r',encoding='utf-8') as f:\n            num=0\n            for line in f.readlines():\n                num+=1\n                line=line.strip()\n                if re.match(r'^[~`!@#$%^&*()_+=,./;\\':\\[\\]{}<>?\\\\|\"，。？；‘：“”’｛【】｝！·￥、\\s\\n\\r -]*$', line):\n                    app.logger.info(f'\\t第{num}不存在有效文字，跳过')\n                    continue                \n                md5_hash = hashlib.md5()\n                md5_hash.update(f\"{line}-{voice}-{language}-{speed}\".encode('utf-8'))\n                filename = md5_hash.hexdigest() + \".wav\"\n                app.logger.info(f'\\t开始合成第{num}行声音:{filename=}')\n                # 合成语音\n                rs = create_tts(text=line, speed=speed, voice=voice, language=language, filename=filename)\n                # 已有结果或错误，直接返回\n                if rs is not None and rs['code']==1:\n                    app.logger.error(f'\\t{t}:文件内容第{num}行【 {line} 】出错了，跳过')\n                    continue\n                if rs is not None and rs['code']==0:\n                    #已存在直接使用\n                    result[f'{num}']={\"filename\":filename, \"num\":num}\n                    chuliing['name']=t\n                    chuliing['line']=num\n                    app.logger.info(f'\\t第{num}行合成完毕:{filename=}')\n                    continue\n                waitlist.append({\"filename\":filename, \"num\":num, \"t\":t})\n        \n        #for it in waitlist:\n        time_tmp = 0\n        chuliing['name']=t\n        if len(waitlist)>0:\n            chuliing['line']=waitlist[0]['num']\n            while len(waitlist)>0:\n                it=waitlist.pop(0)\n                filename, num, t=it.values()\n                \n                #需要等待\n                if time_tmp>7200:\n                    continue\n                    \n                # 当前行已完成合成\n                if filename in cfg.global_tts_result and cfg.global_tts_result[filename] != 1:\n                    #出错了\n                    app.logger.error(f'\\t{t}:文件内容第{num}行出错了,{cfg.global_tts_result[filename]}, 跳过')\n                    continue\n                if os.path.exists(os.path.join(cfg.TTS_DIR, filename)):\n                    chuliing['name']=t\n                    chuliing['line']=num\n                    app.logger.info(f'\\t第{num}行合成完毕:{filename}')\n                    #成功了\n                    result[f'{num}']={\"filename\":filename, \"num\":num}\n                    continue\n                #未完成，插入重新开\n                waitlist.append(it)\n                time_tmp+=1\n                time.sleep(1)\n        if len(result.keys())<1:\n            app.logger.error(f'\\t该文件合成失败，没有生成任何声音')\n            continue    \n        sorted_result = {k: result[k] for k in sorted(result, key=lambda x: int(x))}\n        for i, it in sorted_result.items():\n            theaudio = os.path.normpath(os.path.join(cfg.TTS_DIR, it['filename']))\n            with open(concat_txt, 'a', encoding='utf-8') as f:\n                f.write(f\"file '{theaudio}'\\n\")\n        \n        #当前txt执行完成 合并音频\n        target_mp3=os.path.normpath((os.path.join(dst,f'{t}.mp3')))\n        p=subprocess.run(['ffmpeg',\"-hide_banner\", \"-ignore_unknown\", '-y', '-f', 'concat', '-safe', '0', '-i', concat_txt, target_mp3])\n        \n        if p.returncode!=0:\n            app.logger.error(f'\\t处理文件:{t},将所有音频连接一起时出错')\n            continue\n        app.logger.info(f'\\t已生成完整音频:{target_mp3}')\n        if speed != 1.0 and speed > 0 and speed <= 2.0:\n            p= subprocess.run(['ffmpeg', '-hide_banner', '-ignore_unknown', '-y', '-i', target_mp3, '-af', f\"atempo={speed}\",f'{target_mp3}-speed{speed}.mp3'], encoding=\"utf-8\", capture_output=True)\n            if p.returncode != 0:\n                app.logger.error(f'\\t处理文件{t}:将{target_mp3}音频改变速度{speed}倍时失败')\n                continue\n            os.unlink(target_mp3)\n            target_mp3=f'{target_mp3}-speed{speed}.mp3'\n        app.logger.info(f'\\t文件:{t} 处理完成，mp3:{target_mp3}')\n    app.logger.info('所有文件处理完毕')\n    chuliing['end']=True    \n\n@app.route('/ttslist',methods=['GET', 'POST'])\ndef ttslist():\n    \n    voice = request.form.get(\"voice\")\n    src = request.form.get(\"src\")\n    dst = request.form.get(\"dst\")\n    speed = 1.0\n    try:\n        speed = float(request.form.get(\"speed\"))\n    except:\n        pass\n    language = request.form.get(\"language\")\n\n    #根据src获取所有txt\n    src=os.path.normpath(src)\n    print(f'{src=},{dst=},{language=},{speed=},{voice=}')\n    if not src or not dst or not os.path.exists(src) or not os.path.exists(dst):\n        return jsonify({\"code\":1,\"msg\":\"必须正确填写txt所在目录以及目标目录的完整路径\"})\n\n    threading.Thread(target=detail_task, args=(voice, src, dst, speed, language)).start()    \n\n    return jsonify({\"code\":0,\"msg\":\"ok\"})\n\n\n\n\n\n\n\n# 根据文本返回tts结果，返回 name=文件名字，filename=文件绝对路径\n# 请求端根据需要自行选择使用哪个\n# params\n# text:待合成文字\n# voice：声音文件\n# language:语言代码\n@app.route('/tts', methods=['GET', 'POST'])\ndef tts():\n    # 原始字符串\n    text = request.form.get(\"text\").strip()\n    voice = request.form.get(\"voice\")\n    speed = 1.0\n    try:\n        speed = float(request.form.get(\"speed\"))\n    except:\n        pass\n    language = request.form.get(\"language\")\n    app.logger.info(f\"[tts][tts]recev {text=}\\n{voice=},{language=}\\n\")\n\n    if re.match(r'^[~`!@#$%^&*()_+=,./;\\':\\[\\]{}<>?\\\\|\"，。？；‘：“”’｛【】｝！·￥、\\s\\n\\r -]*$', text):\n        return jsonify({\"code\": 1, \"msg\": \"no text\"})\n    if not text or not voice or not language:\n        return jsonify({\"code\": 1, \"msg\": \"text/voice/language params lost\"})\n\n    # 判断是否是srt\n    text_list = get_subtitle_from_srt(text)\n    app.logger.info(f\"[tts][tts]{text_list=}\")\n    is_srt = True\n    # 不是srt格式,则按行分割\n    if text_list is None:\n        is_srt = False\n        text_list = []\n        for it in text.split(\"\\n\"):\n            text_list.append({\"text\": it.strip()})\n        app.logger.info(f\"[tts][tts] its not srt\")\n\n    num = 0\n    while num < len(text_list):\n        t = text_list[num]\n        # 换行符改成 .\n        t['text'] = t['text'].replace(\"\\n\", ' . ')\n        md5_hash = hashlib.md5()\n        md5_hash.update(f\"{t['text']}-{voice}-{language}-{speed}\".encode('utf-8'))\n        filename = md5_hash.hexdigest() + \".wav\"\n        app.logger.info(f\"[tts][tts]{filename=}\")\n        # 合成语音\n        rs = create_tts(text=t['text'], speed=speed, voice=voice, language=language, filename=filename)\n        # 已有结果或错误，直接返回\n        if rs is not None:\n            text_list[num]['result'] = rs\n            num += 1\n            continue\n        # 循环等待 最多7200s\n        time_tmp = 0\n        while filename not in cfg.global_tts_result:\n            time.sleep(3)\n            time_tmp += 3\n            if time_tmp % 30 == 0:\n                app.logger.info(f\"[tts][tts]{time_tmp=},{filename=}\")\n\n        # 当前行已完成合成\n        if cfg.global_tts_result[filename] != 1:\n            msg = {\"code\": 1, \"msg\": cfg.global_tts_result[filename]}\n        else:\n            target_wav = os.path.normpath(os.path.join(TTS_DIR, filename))\n            if speed != 1.0 and speed > 0 and speed <= 2.0:\n                # 生成的加速音频\n                speed_tmp = os.path.join(TMP_DIR, f'speed_{time.time()}.wav')\n                p = subprocess.run(\n                    ['ffmpeg', '-hide_banner', '-ignore_unknown', '-y', '-i', target_wav, '-af', f\"atempo={speed}\",\n                     os.path.normpath(speed_tmp)], encoding=\"utf-8\", capture_output=True)\n                if p.returncode != 0:\n                    return jsonify({\"code\": 1, \"msg\": str(p.stderr)})\n                shutil.copy2(speed_tmp, target_wav)\n            msg = {\"code\": 0, \"filename\": target_wav, 'name': filename}\n        app.logger.info(f\"[tts][tts] {filename=},{msg=}\")\n        cfg.global_tts_result.pop(filename)\n        text_list[num]['result'] = msg\n        app.logger.info(f\"[tts][tts]{num=}\")\n        num += 1\n\n    filename, errors = merge_audio_segments(text_list, is_srt=is_srt)\n    app.logger.info(f\"[tts][tts]is srt，{filename=},{errors=}\")\n    if filename and os.path.exists(filename) and os.path.getsize(filename) > 0:\n        res = {\"code\": 0, \"filename\": filename, \"name\": os.path.basename(filename), \"msg\": errors}\n    else:\n        res = {\"code\": 1, \"msg\": f\"error:{filename=},{errors=}\"}\n    app.logger.info(f\"[tts][tts]end result:{res=}\")\n    return jsonify(res)\n\n\n# s to s wav->wav\n# params\n# voice: 声音文件\n# filename: 上传的原始声音\n\n@app.route('/sts', methods=['GET', 'POST'])\ndef sts():\n    try:\n        # 保存文件到服务器指定目录\n        # 目标\n        voice = request.form.get(\"voice\")\n        filename = request.form.get(\"name\")\n        app.logger.info(f\"[sts][sts]sts {voice=},{filename=}\\n\")\n\n        if not voice:\n            return jsonify({\"code\": 1, \"msg\": \"voice params lost\"})\n\n        obj = {\"filename\": filename, \"voice\": voice}\n        # 压入队列，准备转换语音\n        app.logger.info(f\"[sts][sts]push sts\")\n        cfg.q_sts.put(obj)\n        # 已有结果或错误，直接返回\n        # 循环等待 最多7200s\n        time_tmp = 0\n        while filename not in cfg.global_sts_result:\n            time.sleep(3)\n            time_tmp += 3\n            if time_tmp % 30 == 0:\n                app.logger.info(f\"{time_tmp=}，{filename=}\")\n\n        # 当前行已完成合成\n        if cfg.global_sts_result[filename] != 1:\n            msg = {\"code\": 1, \"msg\": cfg.global_sts_result[filename]}\n            app.logger.error(f\"[sts][sts]error，{msg=}\")\n        else:\n            msg = {\"code\": 0, \"filename\": os.path.join(TTS_DIR, filename), 'name': filename}\n            app.logger.info(f\"[sts][sts]ok,{msg=}\")\n        cfg.global_sts_result.pop(filename)\n        return jsonify(msg)\n    except Exception as e:\n        app.logger.error(f\"[sts][sts]error:{str(e)}\")\n        return jsonify({'code': 2, 'msg': f'voice->voice:{str(e)}'})\n\n\n@app.route('/checkupdate', methods=['GET', 'POST'])\ndef checkupdate():\n    return jsonify({'code': 0, \"msg\": cfg.updatetips})\n\n\nif __name__ == '__main__':\n\n    tts_thread = None\n    sts_thread = None\n    try:\n        if 'app.py' == sys.argv[0] and 'app.py' == os.path.basename(__file__):\n            print(langlist[\"lang1\"])\n\n        # threading.Thread(target=logic.checkupdate).start()\n\n        if TEXT_MODEL_EXITS:\n            print(langlist['lang2'])\n            tts_thread = threading.Thread(target=ttsloop)\n            tts_thread.start()\n        else:\n            app.logger.error(f\"\\n{langlist['lang3']}: {cfg.download_address}\\n\")\n        \n        if VOICE_MODEL_EXITS:\n            print(langlist['lang4'])\n            sts_thread = threading.Thread(target=stsloop)\n            sts_thread.start()\n        else:\n            app.logger.info(\n                f\"\\n{langlist['lang5']}: {cfg.download_address}\\n\")\n        \n        if not VOICE_MODEL_EXITS and not TEXT_MODEL_EXITS:\n            print(f\"\\n{langlist['lang6']}: {cfg.download_address}\\n\")\n            input(\"Press Enter close\")\n            sys.exit()\n\n        print(\"===\")\n        http_server = None\n        try:\n            host = web_address.split(':')\n            print(f'{host=}')\n            http_server = WSGIServer((host[0], int(host[1])), app, handler_class=CustomRequestHandler)\n            print(f'@@@@@@@@@@@')\n            threading.Thread(target=openweb, args=(web_address,)).start()\n            http_server.serve_forever()\n        finally:\n            if http_server:\n                http_server.stop()\n            # 设置事件，通知线程退出\n            cfg.exit_event.set()\n            # 等待后台线程结束\n            if tts_thread:\n                tts_thread.join()\n            if sts_thread:\n                sts_thread.join()\n    except Exception as e:\n        print(\"error:\" + str(e))\n        app.logger.error(f\"[app]start error:{str(e)}\")\n        sys.exit()\n"
        },
        {
          "name": "change.md",
          "type": "blob",
          "size": 2.810546875,
          "content": "ffmpeg -y -i cn.mp4 -i cn.wav -map '0:v' -map '1:a' -c:v  libx264 -c:a aac cnout.mp4\nffmpeg -y -i en.mp4 -i en.wav -map 0:v -map 1:a -c:v  libx264 -c:a aac enout.mp4\n\n\n0.\n\\venv\\Lib\\site-packages\\TTS\\utils\\manage.py ,大约 389 行附近，def download_model 方法中，注释掉如下代码\n\n\n1. tts/utils/manage.py 532 line _download_zip_file\n\n\tdef _download_zip_file:\n\t\tproxies=None\n        if os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY'):\n            proxies = {\n                \"http\": os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY'),\n                \"https\": os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY')\n            }\n        r = requests.get(file_url, stream=True,proxies=proxies)\n\n\t@staticmethod\n    def _download_tar_file(file_url, output_folder, progress_bar):\n        \"\"\"Download the github releases\"\"\"\n        # download the file\n        proxies=None\n        if os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY'):\n            proxies = {\n                \"http\": os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY'),\n                \"https\": os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY')\n            }\n        r = requests.get(file_url, stream=True,proxies=proxies)\n\n\n    def _download_model_files(file_urls, output_folder, progress_bar):\n        \"\"\"Download the github releases\"\"\"\n        proxies=None\n        if os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY'):\n            proxies = {\n                \"http\": os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY'),\n                \"https\": os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY')\n            }\n\n2. tts/vc/modules/freevc/wavlm\n\n\tdef get_wavlm():\n\t\tprint(f\" > Downloading WavLM model to {output_path} ...\")\n        if os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY'):\n            # 创建ProxyHandler对象\n            proxy_support = urllib.request.ProxyHandler({\"http\": os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY'),\"https\":os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY')})\n\n            # 创建Opener\n            opener = urllib.request.build_opener(proxy_support)\n\n            # 安装Opener\n            urllib.request.install_opener(opener)\n\n        urllib.request.urlretrieve(model_uri, output_path)\n\n\n3. E:\\python\\tts\\venv\\Lib\\site-packages\\fsspec\\implementations\\http.py\n\n    async def _get_file(\n        self, rpath, lpath, chunk_size=5 * 2**20, callback=_DEFAULT_CALLBACK, **kwargs\n    ):\n        print(f'%%%%%%%%%%%%%%%%%%%{rpath=},{lpath=}')\n        import os\n        if os.path.exists(lpath) and os.path.getsize(lpath)>16000:\n            print('存在')\n            return True\n\n\n\t\tproxy=os.environ.get('http_proxy') or os.environ.get('HTTP_PROXY')\n        async with session.get(self.encode_url(rpath), proxy=proxy if proxy else None,**kw) as r:"
        },
        {
          "name": "clone",
          "type": "tree",
          "content": null
        },
        {
          "name": "code_dev.py",
          "type": "blob",
          "size": 1.65234375,
          "content": "import torch\nimport os\nrootdir=os.getcwd()\nos.environ['TTS_HOME']=rootdir\n\nfrom TTS.api import TTS\nfrom dotenv import load_dotenv\nload_dotenv()\n\nprint(\"源码部署需要先运行该文件，以便同意coqou-ai协议，当弹出协议时，请输入 y \\n同时需要连接墙外下载或更新模型，请在 .env 中 HTTP_PROXY=设置代理地址\")\n\ndef updatecache():\n    # 禁止更新，避免无代理时报错\n    file=os.path.join(rootdir,'tts_cache/cache')\n    if file:\n        import json,time\n        j=json.load(open(file,'r',encoding='utf-8'))\n        for i,it in enumerate(j):\n            if \"time\" in it and \"fn\" in it:\n                cache_file=os.path.join(rootdir,f'tts_cache/{it[\"fn\"]}')\n                if os.path.exists(cache_file) and os.path.getsize(cache_file)>17000000:\n                    it['time']=time.time()\n                    j[i]=it\n        json.dump(j,open(file,'w',encoding='utf-8'))\n\nupdatecache()\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n\n#ttsv2 = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n\ntts = TTS(model_name='voice_conversion_models/multilingual/vctk/freevc24').to(device)\n\n# test\n#tts.tts_to_file(text='我是中国人，你呢我的宝贝。今天天气看起来很不错啊', speaker_wav='./cn1.wav',language='zh', file_path='hafalse2.wav', speed=2.0,split_sentences=False)\n\n#tts.tts_to_file(text='我是中国人，你呢我的宝贝。今天天气看起来很不错啊', speaker_wav='./cn1.wav',language='zh', file_path='hafalse0.2.wav', speed=0.2,split_sentences=False)\n\n#target_wav is voice file \n# tts.voice_conversion_to_file(source_wav=\"./cn1.wav\", target_wav=\"./sx1.wav\", file_path=\"./out.wav\")\n\n\n\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.2294921875,
          "content": "name: clone-voice\r\nchannels:\r\n  - conda-forge\r\n  - pytorch\r\n  - nvidia\r\ndependencies:\r\n  - python=3.10\r\n\r\n  - pytorch==2.5.1\r\n  - pytorch-cuda==12.4\r\n\r\n  - ffmpeg==7.1.0\r\n\r\n  - pip:\r\n    - huggingface-hub\r\n    - -r ./requirements.txt\r\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "params.json",
          "type": "blob",
          "size": 0.0986328125,
          "content": "{\n\"port\":5003,\n\"out_path\":\"\",\n\"num_epochs\":4,\n\"batch_size\":2,\n\"grad_acumm\":1,\n\"max_audio_length\":10\n}"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 3.310546875,
          "content": "absl-py==2.0.0\naiofiles==23.2.1\naiohttp==3.9.1\naiosignal==1.3.1\naltair==5.2.0\naltgraph==0.17.4\nannotated-types==0.6.0\nanyascii==0.3.2\nanyio==4.3.0\nasgiref==3.7.2\nasync-timeout==4.0.3\nattrs==23.1.0\naudioread==3.0.1\nav==11.0.0\nBabel==2.14.0\nbangla==0.0.2\nblinker==1.7.0\nblis==0.7.11\nbnnumerizer==0.0.2\nbnunicodenormalizer==0.1.6\ncachetools==5.3.2\ncatalogue==2.0.10\ncertifi==2023.11.17\ncffi==1.16.0\ncharset-normalizer==3.3.2\nclick==8.1.7\ncloudpathlib==0.16.0\ncolorama==0.4.6\ncoloredlogs==15.0.1\nconfection==0.1.4\ncontourpy==1.2.0\ncoqpit==0.0.17\nctranslate2==4.0.0\ncutlet==0.3.0\ncycler==0.12.1\ncymem==2.0.8\nCython==3.0.7\ndateparser==1.1.8\ndecorator==5.1.1\ndocopt==0.6.2\neinops==0.7.0\nencodec==0.1.1\nexceptiongroup==1.2.0\nfastapi==0.110.0\nfaster-whisper==1.0.0\nffmpy==0.3.2\nfilelock==3.13.1\nFlask==3.0.0\nflatbuffers==23.5.26\nfonttools==4.47.0\nfrozenlist==1.4.1\nfsspec==2023.12.2\nfugashi==1.3.0\ng2pkk==0.1.2\ngevent==23.9.1\ngoogle-auth==2.25.2\ngoogle-auth-oauthlib==1.2.0\ngradio==4.19.2\ngradio_client==0.10.1\ngreenlet==3.0.3\ngrpcio==1.60.0\ngruut==2.2.3\ngruut-ipa==0.13.0\ngruut-lang-de==2.0.0\ngruut-lang-en==2.0.0\ngruut-lang-es==2.0.0\ngruut-lang-fr==2.0.2\nh11==0.14.0\nhangul-romanize==0.1.0\nhttpcore==1.0.4\nhttpx==0.27.0\nhuggingface-hub==0.20.1\nhumanfriendly==10.0\nidna==3.6\nimportlib_resources==6.1.2\ninflect==7.0.0\nitsdangerous==2.1.2\njaconv==0.3.4\njamo==0.4.1\njieba==0.42.1\nJinja2==3.1.2\njoblib==1.3.2\njsonlines==1.2.0\njsonschema==4.21.1\njsonschema-specifications==2023.12.1\nkiwisolver==1.4.5\nlangcodes==3.3.0\nlazy_loader==0.3\nlibrosa==0.10.0\nllvmlite==0.41.1\nMarkdown==3.5.1\nmarkdown-it-py==3.0.0\nMarkupSafe==2.1.3\nmatplotlib==3.8.2\nmdurl==0.1.2\nmojimoji==0.0.12\nmpmath==1.3.0\nmsgpack==1.0.7\nmultidict==6.0.4\nmurmurhash==1.0.10\nnetworkx==2.8.8\nnltk==3.8.1\nnum2words==0.5.13\nnumba==0.58.1\nnumpy==1.22.0\noauthlib==3.2.2\nonnxruntime==1.17.1\norjson==3.9.15\npackaging==23.2\npandas==1.5.3\npefile==2023.2.7\nPillow==10.1.0\nplatformdirs==4.1.0\npooch==1.8.0\npreshed==3.0.9\nprotobuf==4.23.4\npsutil==5.9.7\npyasn1==0.5.1\npyasn1-modules==0.3.0\npycparser==2.21\npydantic==2.5.2\npydantic_core==2.14.5\npydub==0.25.1\nPygments==2.17.2\npyinstaller==6.3.0\npyinstaller-hooks-contrib==2023.11\npynndescent==0.5.11\npyparsing==3.1.1\npypinyin==0.50.0\npyreadline3==3.4.1\npysbd==0.3.4\npython-crfsuite==0.9.10\npython-dateutil==2.8.2\npython-dotenv==1.0.0\npython-multipart==0.0.9\npytz==2023.3.post1\npywin32-ctypes==0.2.2\nPyYAML==6.0.1\nreferencing==0.33.0\nregex==2023.10.3\nrequests==2.31.0\nrequests-oauthlib==1.3.1\nrich==13.7.0\nrpds-py==0.18.0\nrsa==4.9\nruff==0.2.2\nsafetensors==0.4.1\nscikit-learn==1.3.2\nscipy==1.11.4\nsemantic-version==2.10.0\nshellingham==1.5.4\nsix==1.16.0\nsmart-open==6.4.0\nsniffio==1.3.1\nsoundfile==0.12.1\nsoxr==0.3.7\nspacy==3.7.2\nspacy-legacy==3.0.12\nspacy-loggers==1.0.5\nsrsly==2.4.8\nstarlette==0.36.3\nSudachiDict-core==20230927\nSudachiPy==0.6.8\nsympy==1.12\ntensorboard==2.15.1\ntensorboard-data-server==0.7.2\nthinc==8.2.2\nthreadpoolctl==3.2.0\ntokenizers==0.15.0\ntomlkit==0.12.0\ntoolz==0.12.1\ntqdm==4.66.1\ntrainer==0.0.36\ntransformers==4.36.2\nTTS @ git+https://github.com/coqui-ai/TTS.git@1936330adaad84812b0fafd2aa17cb7bba6edea9\ntyper==0.9.0\ntyping_extensions==4.9.0\ntzdata==2023.3\ntzlocal==5.2\numap-learn==0.5.5\nUnidecode==1.3.7\nunidic-lite==1.0.8\nurllib3==2.1.0\nuvicorn==0.27.1\nwaitress==3.0.0\nwasabi==1.1.2\nweasel==0.3.4\nwebsockets==11.0.3\nWerkzeug==3.0.1\nyarl==1.9.4\nzope.event==5.0\nzope.interface==6.1\n"
        },
        {
          "name": "runapp.bat",
          "type": "blob",
          "size": 0.056640625,
          "content": "@echo off\n\n%cd%\\venv\\scripts\\python.exe %cd%\\app.py\n\npause"
        },
        {
          "name": "runtrain.bat",
          "type": "blob",
          "size": 0.04296875,
          "content": "@echo off\n.\\venv\\scripts\\python.exe train.py"
        },
        {
          "name": "static",
          "type": "tree",
          "content": null
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 0.2265625,
          "content": "\nimport os\nimport re\n\n\ndef get_models(path):\n    objs={}\n    for it in os.listdir(path):\n        if re.match(r'^[0-9a-zA-Z_-]+$',it):\n            objs[it]=None\n    return objs\n\nprint(get_models(r'E:\\python\\tts\\tts\\mymodels\\xiaoyi'))"
        },
        {
          "name": "testapi.py",
          "type": "blob",
          "size": 0.2998046875,
          "content": "import requests\nimport os\n#\n# res=requests.post(\"http://127.0.0.1:9988/apitts\",data={\"text\":\"hello,everyone,you are my friend\",\"language\":\"en\"},files={\"audio\":open(\"./10.wav\",\"rb\")})\n# res=requests.get(\"http://127.0.0.1:9988/init\")\n#\n# print(res.text)\n\nfor t in os.listdir(\"f:/python/pyvideo\"):\n    print(t)"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 16.1240234375,
          "content": "import argparse\nimport os\nimport sys\nimport tempfile\nimport threading\nimport webbrowser\nimport time\n\nimport gradio as gr\nimport librosa.display\nimport numpy as np\n\nimport os\nimport torch\nimport torchaudio\nimport traceback\nfrom utils.formatter import format_audio_list\nfrom utils.cfg import TTSMODEL_DIR\nfrom TTS.demos.xtts_ft_demo.utils.gpt_train import train_gpt\n\n\nfrom TTS.tts.configs.xtts_config import XttsConfig\nfrom TTS.tts.models.xtts import Xtts\nimport datetime\nimport shutil\nimport json\nimport random\nfrom dotenv import load_dotenv\nload_dotenv()\n\nproxy=os.getenv('HTTP_PROXY') or os.getenv('http_proxy')\nif proxy:\n    os.environ['HTTP_PROXY']=proxy\n    os.environ['HTTPS_PROXY']=proxy\n\nprint(f'{proxy=}')\n\n\n#dataset目录名\ndataset_name=f'dataset{int(random.random()*1000000)}'\n\ndef clear_gpu_cache():\n    # clear the GPU cache\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nXTTS_MODEL = None\ndef load_model(xtts_checkpoint, xtts_config, xtts_vocab):\n    global XTTS_MODEL\n    clear_gpu_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        gr.Error('训练尚未结束，请稍等')\n        return \"训练尚未结束，请稍等\"\n    config = XttsConfig()\n    config.load_json(xtts_config)\n    XTTS_MODEL = Xtts.init_from_config(config)\n    print(\"Loading XTTS model! \")\n    XTTS_MODEL.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab, use_deepspeed=False)\n    if torch.cuda.is_available():\n        XTTS_MODEL.cuda()\n\n    print(\"Model Loaded!\")\n    return \"模型已加载!\"\n\ndef run_tts(lang, tts_text, speaker_audio_file):\n    if XTTS_MODEL is None:\n        gr.Error(\"模型还未训练完毕或尚未加载，请稍等\")\n        return \"模型还未训练完毕或尚未加载，请稍等 !!\", None, None\n    if speaker_audio_file and not speaker_audio_file.endswith(\".wav\"):\n        speaker_audio_file+='.wav'\n    if not speaker_audio_file or  not os.path.exists(speaker_audio_file):\n        gr.Error('必须填写参考音频')\n        return '必须填写参考音频',None,None\n    gpt_cond_latent, speaker_embedding = XTTS_MODEL.get_conditioning_latents(audio_path=speaker_audio_file, gpt_cond_len=XTTS_MODEL.config.gpt_cond_len, max_ref_length=XTTS_MODEL.config.max_ref_len, sound_norm_refs=XTTS_MODEL.config.sound_norm_refs)\n    out = XTTS_MODEL.inference(\n        text=tts_text,\n        language=lang,\n        gpt_cond_latent=gpt_cond_latent,\n        speaker_embedding=speaker_embedding,\n        temperature=XTTS_MODEL.config.temperature, # Add custom parameters here\n        length_penalty=XTTS_MODEL.config.length_penalty,\n        repetition_penalty=XTTS_MODEL.config.repetition_penalty,\n        top_k=XTTS_MODEL.config.top_k,\n        top_p=XTTS_MODEL.config.top_p,\n    )\n\n    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as fp:\n        out[\"wav\"] = torch.tensor(out[\"wav\"]).unsqueeze(0)\n        out_path = fp.name\n        torchaudio.save(out_path, out[\"wav\"], 24000)\n\n    return \"已创建好了声音 !\", out_path, speaker_audio_file\n\n\n\n\n# define a logger to redirect \nclass Logger:\n    def __init__(self, filename=\"log.out\"):\n        self.log_file = filename\n        self.terminal = sys.stdout\n        self.log = open(self.log_file, \"w\")\n\n    def write(self, message):\n        self.terminal.write(message)\n        self.log.write(message)\n\n    def flush(self):\n        self.terminal.flush()\n        self.log.flush()\n\n    def isatty(self):\n        return False\n\n# redirect stdout and stderr to a file\nsys.stdout = Logger()\nsys.stderr = sys.stdout\n\n\n# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\nimport logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n    handlers=[\n        logging.StreamHandler(sys.stdout)\n    ]\n)\n\ndef read_logs():\n    sys.stdout.flush()\n    with open(sys.stdout.log_file, \"r\") as f:\n        return f.read()\n\ndef openweb(port):\n    time.sleep(10)\n    webbrowser.open(f\"http://127.0.0.1:{port}\")\n\nif __name__ == \"__main__\":\n    date=datetime.datetime.now()\n    param_file=os.path.join(os.getcwd(),'params.json')\n    if not os.path.exists(param_file):\n        print('不存在配音文件 params.json')\n        sys.exit()\n    args=json.load(open(param_file,'r',encoding='utf-8'))\n    args['out_path']=TTSMODEL_DIR\n\n    with gr.Blocks(css=\"ul.options[role='listbox']{background:#ffffff}\",title=\"clone-voice trainer\") as demo:\n        if not proxy:\n            gr.Markdown(\"\"\"**没有配置代理，训练中可能出错，建议在.env文件中 `HTTP_PROXY=`后填写代理地址**\"\"\")\n        with gr.Tab(\"训练平台\"):\n            with gr.Row() as r1:\n                model_name= gr.Textbox(\n                        label=\"训练后模型名称(限英文/数字/下划线，禁止空格或特殊符号):\",\n                        value=f\"model{date.day}{date.hour}{date.minute}\",\n                )\n                lang = gr.Dropdown(\n                    label=\"音频发声语言\",\n                    value=\"zh\",\n                    choices=[\n                        \"zh\",\n                        \"en\",\n                        \"es\",\n                        \"fr\",\n                        \"de\",\n                        \"it\",\n                        \"pt\",\n                        \"pl\",\n                        \"tr\",\n                        \"ru\",\n                        \"nl\",\n                        \"cs\",\n                        \"ar\",\n                        \"hu\",\n                        \"ko\",\n                        \"ja\"\n                    ],\n                )\n            with gr.Row() as r1:\n                upload_file = gr.File(\n                    file_count=\"multiple\",\n                    label=\"选择训练素材音频文件(可多个)，仅可包含同一个人声，并且无背景噪声(wav, mp3, and flac)\",\n                )\n                logs = gr.Textbox(\n                    label=\"日志:\",\n                    interactive=False,\n                )\n                demo.load(read_logs, None, logs, every=1)\n\n            prompt_compute_btn = gr.Button(value=\"第一步：上传音频文件后点击开始整理数据\")\n            with gr.Row() as r1:\n                train_data = gr.Textbox(\n                    label=\"待训练数据集/可修改识别出的文字以便效果更好:\",\n                    interactive=True,\n                    lines=20,\n                    placeholder=\"第一步结束后，会自动在此显示整理好的文字，可修改错别字，以便取得更好效果\"\n                )\n                eval_data = gr.Textbox(\n                    label=\"验证数据集/可修改识别出的文字以便效果概念股或:\",\n                    interactive=True,\n                    lines=20,\n                    placeholder=\"第一步结束后，会自动在此显示整理好的文字，可修改错别字，以便取得更好效果\"\n                )\n            with gr.Row() as r:\n                train_file=gr.Textbox(\n                    label=\"待训练数据集csv文件:\",\n                    interactive=False,\n                    visible=False\n                    \n                )\n                eval_file=gr.Textbox(\n                    label=\"验证数据集csv文件:\",\n                    interactive=False,\n                    visible=False\n                )\n            \n            start_train_btn = gr.Button(value=\"第二步：修改错别字后(或不修改)，点击启动训练\")\n            \n            with gr.Row():\n                with gr.Column() as col1:\n                    xtts_checkpoint = gr.Textbox(\n                        label=\"训练后模型保存路径:\",\n                        value=\"\",\n                        interactive=False,\n                        visible=False\n                    )\n                    xtts_config = gr.Textbox(\n                        label=\"训练后模型配置文件:\",\n                        value=\"\",\n                        interactive=False,\n                        visible=False\n                    )\n\n                    xtts_vocab = gr.Textbox(\n                        label=\"vocab文件:\",\n                        value=\"\",\n                        interactive=False,\n                        visible=False\n                    )\n            \n            with gr.Row():\n                with gr.Column() as col2:\n                    speaker_reference_audio = gr.Textbox(\n                        label=\"参考音频/第二步结束后自动填充:\",\n                        value=\"\",\n                        placeholder=f\"第二步结束后可以到{os.path.join(args['out_path'], dataset_name,'wavs')}目录下，找到质量更好音频替换\"\n                    )\n                    tts_language = gr.Dropdown(\n                        label=\"文字语言\",\n                        value=\"zh\",\n                        choices=[\n                            \"zh\",\n                            \"en\",\n                            \"es\",\n                            \"fr\",\n                            \"de\",\n                            \"it\",\n                            \"pt\",\n                            \"pl\",\n                            \"tr\",\n                            \"ru\",\n                            \"nl\",\n                            \"cs\",\n                            \"ar\",\n                            \"hu\",\n                            \"ko\",\n                            \"ja\",\n                        ]\n                    )\n                    tts_text = gr.Textbox(\n                        label=\"输入要合成的文字.\",\n                        value=\"你好啊，我亲爱的朋友.\",\n                    )                   \n\n                with gr.Column() as col3:\n                    tts_output_audio = gr.Audio(label=\"生成的声音.\")\n                    reference_audio = gr.Audio(label=\"作为参考的音频.\")\n            tts_btn = gr.Button(value=\"第三步：自动填充参考音频后，点击测试模型效果\")\n            copy_label=gr.Label(label=\"\")\n            move_btn = gr.Button(value=\"第四步：效果如果满意，点击复制到clone-voice中使用它\")\n            \n            \n\n            def train_model(language, train_text, eval_text,trainfile,evalfile):\n                clear_gpu_cache()\n                if not trainfile or not evalfile:\n                    gr.Error(\"请等待数据处理完毕，目前不存在有效的训练数据集!\")\n                    return \"请等待数据处理完毕，目前不存在有效的训练数据集!\",\"\", \"\", \"\", \"\"\n                try:\n                    with open(trainfile,'w',encoding='utf-8') as f:\n                        f.write(train_text.replace('\\r\\n','\\n'))\n                    with open(evalfile,'w',encoding='utf-8') as f:\n                        f.write(eval_text.replace('\\r\\n','\\n'))\n                    \n                    print(f'{trainfile=}')\n                    print(f'{evalfile=}')\n                    #sys.exit()\n                    # convert seconds to waveform frames\n                    max_audio_length = int(args['max_audio_length'] * 22050)\n                    config_path, original_xtts_checkpoint, vocab_file, exp_path, speaker_wav = train_gpt(language, args['num_epochs'], args['batch_size'], args['grad_acumm'], trainfile, evalfile, output_path=args['out_path'], max_audio_length=max_audio_length)\n                except:\n                    traceback.print_exc()\n                    error = traceback.format_exc()\n                    print(error)\n                    gr.Error(f\"训练出错了: {error}\")\n                    return f\"训练出错了: {error}\",\"\", \"\", \"\", \"\"\n\n                # copy original files to avoid parameters changes issues\n                shutil.copy2(config_path,exp_path)\n                shutil.copy2(vocab_file,exp_path)\n\n                ft_xtts_checkpoint = os.path.join(exp_path, \"best_model.pth\")\n                print(\"训练完毕!\")\n                clear_gpu_cache()               \n                \n                msg=load_model(\n                    ft_xtts_checkpoint,\n                    config_path,\n                    vocab_file\n                )\n                gr.Info(\"训练完毕，可以测试了\")\n                return \"训练完毕，可以测试了\",config_path, vocab_file, ft_xtts_checkpoint, speaker_wav\n        \n            # 处理数据集\n            def preprocess_dataset(audio_path, language,  progress=gr.Progress(track_tqdm=True)):\n                clear_gpu_cache()\n                out_path = os.path.join(args['out_path'], dataset_name)\n                os.makedirs(out_path, exist_ok=True)\n                \n                try:\n                    train_meta, eval_meta, audio_total_size = format_audio_list(audio_path, target_language=language, out_path=out_path, gradio_progress=progress)\n                except:\n                    traceback.print_exc()\n                    error = traceback.format_exc()\n                    gr.Error(f\"处理训练数据出错了! \\n Error summary: {error}\")\n                    return \"\", \"\",\"\",\"\"\n\n                clear_gpu_cache()\n\n                # if audio total len is less than 2 minutes raise an error\n                if audio_total_size < 120:\n                    message = \"素材总时长不得小于2分钟!\"\n                    print(message)\n                    gr.Error(message)\n                    return \"\", \"\",\"\",\"\"\n\n                print(\"数据处理完毕，开始训练!\")\n                \n                traindata=\"\"\n                evaldata=\"\"\n                with open(train_meta,'r',encoding=\"utf-8\") as f:\n                    traindata=f.read()\n                with open(eval_meta,'r',encoding=\"utf-8\") as f:\n                    evaldata=f.read()\n                return traindata,evaldata,train_meta,eval_meta\n                \n            \n            # 复制到clone\n            def move_to_clone(model_name,model_file,vocab,cfg,audio_file):\n                if not audio_file or not os.path.exists(audio_file):\n                    gr.Warning(\"必须填写参考音频\")\n                    return \"必须填写参考音频\"\n                gr.Info('开始复制到clone自定义模型下，请耐心等待提示完成')\n                print(f'{model_name=}')\n                print(f'{model_file=}')\n                print(f'{vocab=}')\n                print(f'{cfg=}')\n                print(f'{audio_file=}')\n                model_dir=os.path.join(os.getcwd(),f'models/mymodels/{model_name}')\n                os.makedirs(model_dir,exist_ok=True)\n                shutil.copy2(model_file,os.path.join(model_dir,'model.pth'))\n                shutil.copy2(vocab,os.path.join(model_dir,'vocab.json'))\n                shutil.copy2(cfg,os.path.join(model_dir,'config.json'))\n                shutil.copy2(audio_file,os.path.join(model_dir,'base.wav'))\n                gr.Info('已复制到clone自定义模型目录下了，可以去使用咯')\n                return \"已复制到clone自定义模型目录下了，可以去使用咯\"\n            \n            move_btn.click(\n                fn=move_to_clone,\n                inputs=[\n                    model_name,\n                    xtts_checkpoint,\n                    xtts_vocab,\n                    xtts_config,\n                    speaker_reference_audio\n                ],\n                outputs=[\n                    copy_label\n                ]\n            )\n            \n            prompt_compute_btn.click(\n                fn=preprocess_dataset,\n                inputs=[\n                    upload_file,\n                    lang\n                ],\n                outputs=[\n                    train_data,eval_data,train_file,eval_file\n                ],\n            )\n            \n            start_train_btn.click(\n                fn=train_model,\n                inputs=[lang,train_data,eval_data,train_file,eval_file],\n                outputs=[copy_label,xtts_config, xtts_vocab, xtts_checkpoint, speaker_reference_audio]\n            )\n\n           \n\n            tts_btn.click(\n                fn=run_tts,\n                inputs=[\n                    tts_language,\n                    tts_text,\n                    speaker_reference_audio,\n                ],\n                outputs=[copy_label,tts_output_audio, reference_audio],\n            )\n            \n    threading.Thread(target=openweb,args=(args['port'],)).start()\n    demo.launch(\n        share=True,\n        debug=False,\n        server_port=args['port'],\n        server_name=\"0.0.0.0\"\n    )\n"
        },
        {
          "name": "tts",
          "type": "tree",
          "content": null
        },
        {
          "name": "tts_cache",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "version.json",
          "type": "blob",
          "size": 0.041015625,
          "content": "{\n\"version\":\"v0.907\",\n\"version_num\":907\n}\n"
        },
        {
          "name": "xtts_demo.py",
          "type": "blob",
          "size": 11.748046875,
          "content": "import argparse\nimport os\nimport sys\nimport tempfile\n\nimport gradio as gr\nimport librosa.display\nimport numpy as np\n\nimport os\nimport torch\nimport torchaudio\nimport traceback\nfrom TTS.demos.xtts_ft_demo.utils.formatter import format_audio_list\nfrom TTS.demos.xtts_ft_demo.utils.gpt_train import train_gpt\n\nfrom TTS.demos.xtts_ft_demo.utils.cfg import TTSMODEL_DIR\n\nfrom TTS.tts.configs.xtts_config import XttsConfig\nfrom TTS.tts.models.xtts import Xtts\n\n\ndef clear_gpu_cache():\n    # clear the GPU cache\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nXTTS_MODEL = None\ndef load_model(xtts_checkpoint, xtts_config, xtts_vocab):\n    global XTTS_MODEL\n    clear_gpu_cache()\n    if not xtts_checkpoint or not xtts_config or not xtts_vocab:\n        return \"You need to run the previous steps or manually set the `XTTS checkpoint path`, `XTTS config path`, and `XTTS vocab path` fields !!\"\n    config = XttsConfig()\n    config.load_json(xtts_config)\n    XTTS_MODEL = Xtts.init_from_config(config)\n    print(\"Loading XTTS model! \")\n    XTTS_MODEL.load_checkpoint(config, checkpoint_path=xtts_checkpoint, vocab_path=xtts_vocab, use_deepspeed=False)\n    if torch.cuda.is_available():\n        XTTS_MODEL.cuda()\n\n    print(\"Model Loaded!\")\n    return \"Model Loaded!\"\n\ndef run_tts(lang, tts_text, speaker_audio_file):\n    if XTTS_MODEL is None or not speaker_audio_file:\n        return \"You need to run the previous step to load the model !!\", None, None\n\n    gpt_cond_latent, speaker_embedding = XTTS_MODEL.get_conditioning_latents(audio_path=speaker_audio_file, gpt_cond_len=XTTS_MODEL.config.gpt_cond_len, max_ref_length=XTTS_MODEL.config.max_ref_len, sound_norm_refs=XTTS_MODEL.config.sound_norm_refs)\n    out = XTTS_MODEL.inference(\n        text=tts_text,\n        language=lang,\n        gpt_cond_latent=gpt_cond_latent,\n        speaker_embedding=speaker_embedding,\n        temperature=XTTS_MODEL.config.temperature, # Add custom parameters here\n        length_penalty=XTTS_MODEL.config.length_penalty,\n        repetition_penalty=XTTS_MODEL.config.repetition_penalty,\n        top_k=XTTS_MODEL.config.top_k,\n        top_p=XTTS_MODEL.config.top_p,\n    )\n\n    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as fp:\n        out[\"wav\"] = torch.tensor(out[\"wav\"]).unsqueeze(0)\n        out_path = fp.name\n        torchaudio.save(out_path, out[\"wav\"], 24000)\n\n    return \"Speech generated !\", out_path, speaker_audio_file\n\n\n\n\n# define a logger to redirect \nclass Logger:\n    def __init__(self, filename=\"log.out\"):\n        self.log_file = filename\n        self.terminal = sys.stdout\n        self.log = open(self.log_file, \"w\")\n\n    def write(self, message):\n        self.terminal.write(message)\n        self.log.write(message)\n\n    def flush(self):\n        self.terminal.flush()\n        self.log.flush()\n\n    def isatty(self):\n        return False\n\n# redirect stdout and stderr to a file\nsys.stdout = Logger()\nsys.stderr = sys.stdout\n\n\n# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\nimport logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n    handlers=[\n        logging.StreamHandler(sys.stdout)\n    ]\n)\n\ndef read_logs():\n    sys.stdout.flush()\n    with open(sys.stdout.log_file, \"r\") as f:\n        return f.read()\n\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser(\n        description=\"\"\"XTTS fine-tuning demo\\n\\n\"\"\"\n        \"\"\"\n        Example runs:\n        set http_proxy=http://127.0.0.1:10809\n\n        python TTS/demos/xtts_ft_demo/xtts_demo.py --port \n        \"\"\",\n        formatter_class=argparse.RawTextHelpFormatter,\n    )\n    parser.add_argument(\n        \"--port\",\n        type=int,\n        help=\"Port to run the gradio demo. Default: 5003\",\n        default=5003,\n    )\n    parser.add_argument(\n        \"--out_path\",\n        type=str,\n        help=\"Output path (where data and checkpoints will be saved) Default: /tmp/xtts_ft/\",\n        default=TTSMODEL_DIR,\n    )\n\n    parser.add_argument(\n        \"--num_epochs\",\n        type=int,\n        help=\"Number of epochs to train. Default: 10\",\n        default=10,\n    )\n    parser.add_argument(\n        \"--batch_size\",\n        type=int,\n        help=\"Batch size. Default: 4\",\n        default=4,\n    )\n    parser.add_argument(\n        \"--grad_acumm\",\n        type=int,\n        help=\"Grad accumulation steps. Default: 1\",\n        default=1,\n    )\n    parser.add_argument(\n        \"--max_audio_length\",\n        type=int,\n        help=\"Max permitted audio size in seconds. Default: 11\",\n        default=10,\n    )\n\n    args = parser.parse_args()\n\n    with gr.Blocks() as demo:\n        with gr.Tab(\"开始训练\"):\n            model_name = gr.Textbox(\n                        label=\"保存模型名(只允许英文/数字/下S划线):\",\n                        value=\"\",\n            )\n            upload_file = gr.File(\n                file_count=\"multiple\",\n                label=\"选择训练素材音频文件，仅包含同一个人声，无背景噪声(wav, mp3, and flac)\",\n            )\n            with gr.Row() as r1:\n                lang = gr.Dropdown(\n                    label=\"音频发声语言\",\n                    value=\"zh\",\n                    choices=[\n                        \"zh\",\n                        \"en\",\n                        \"es\",\n                        \"fr\",\n                        \"de\",\n                        \"it\",\n                        \"pt\",\n                        \"pl\",\n                        \"tr\",\n                        \"ru\",\n                        \"nl\",\n                        \"cs\",\n                        \"ar\",\n                        \"hu\",\n                        \"ko\",\n                        \"ja\"\n                    ],\n                )\n                progress_data = gr.Label(\n                    label=\"进度:\"\n                )\n            logs = gr.Textbox(\n                label=\"日志:\",\n                interactive=False,\n            )\n            demo.load(read_logs, None, logs, every=1)\n\n            prompt_compute_btn = gr.Button(value=\"开始训练\")\n            def train_model(language, train_csv, eval_csv, num_epochs, batch_size, grad_acumm, output_path, max_audio_length):\n                clear_gpu_cache()\n                if not train_csv or not eval_csv:\n                    return \"不存在有效的csv文件 !\", \"\", \"\", \"\", \"\"\n                try:\n                    # convert seconds to waveform frames\n                    max_audio_length = int(max_audio_length * 22050)\n                    config_path, original_xtts_checkpoint, vocab_file, exp_path, speaker_wav = train_gpt(language, num_epochs, batch_size, grad_acumm, train_csv, eval_csv, output_path=output_path, max_audio_length=max_audio_length)\n                except:\n                    traceback.print_exc()\n                    error = traceback.format_exc()\n                    return f\"训练出错了: {error}\", \"\", \"\", \"\", \"\"\n\n                # copy original files to avoid parameters changes issues\n                os.system(f\"cp {config_path} {exp_path}\")\n                os.system(f\"cp {vocab_file} {exp_path}\")\n\n                ft_xtts_checkpoint = os.path.join(exp_path, \"model.pth\")\n                print(\"训练完毕!\")\n                clear_gpu_cache()\n                return \"训练完毕!\", config_path, vocab_file, ft_xtts_checkpoint, speaker_wav\n        \n            def preprocess_dataset(audio_path, language,  progress=gr.Progress(track_tqdm=True)):\n                clear_gpu_cache()\n                out_path = os.path.join(args.out_path, \"dataset\")\n                os.makedirs(out_path, exist_ok=True)\n                \n                try:\n                    train_meta, eval_meta, audio_total_size = format_audio_list(audio_path, target_language=language, out_path=out_path, gradio_progress=progress)\n                except:\n                    traceback.print_exc()\n                    error = traceback.format_exc()\n                    return f\"处理训练数据出错了! \\n Error summary: {error}\", \"\", \"\",\"\",\"\"\n\n                clear_gpu_cache()\n\n                # if audio total len is less than 2 minutes raise an error\n                if audio_total_size < 120:\n                    message = \"素材总时长不得小于2分钟!\"\n                    print(message)\n                    return message, \"\", \"\",\"\",\"\"\n\n                print(\"数据处理完毕，开始训练!\")\n                msg, config_path, vocab_file, ft_xtts_checkpoint, speaker_wav=train_model(language, train_meta, eval_meta, args.num_epochs, args.batch_size, args.grad_acumm, args.out_path, args.max_audio_length)\n                progress_data, xtts_config, xtts_vocab, xtts_checkpoint, speaker_reference_audio\n                msg=load_model(\n                    ft_xtts_checkpoint,\n                    config_path,\n                    vocab_file\n                )\n                \n                return msg, config_path, vocab_file, ft_xtts_checkpoint, speaker_wav\n\n        with gr.Tab(\"使用已训练好的模型\"):\n            with gr.Row():\n                with gr.Column() as col1:\n                    xtts_checkpoint = gr.Textbox(\n                        label=\"XTTS checkpoint path:\",\n                        value=\"\",\n                    )\n                    xtts_config = gr.Textbox(\n                        label=\"XTTS config path:\",\n                        value=\"\",\n                    )\n\n                    xtts_vocab = gr.Textbox(\n                        label=\"XTTS vocab path:\",\n                        value=\"\",\n                    )\n                    progress_load = gr.Label(\n                        label=\"进度:\"\n                    )\n                    load_btn = gr.Button(value=\"Step 3 - Load Fine-tuned XTTS model\")\n\n                with gr.Column() as col2:\n                    speaker_reference_audio = gr.Textbox(\n                        label=\"参考音频:\",\n                        value=\"\",\n                    )\n                    tts_language = gr.Dropdown(\n                        label=\"Language\",\n                        value=\"zh\",\n                        choices=[\n                            \"zh\",\n                            \"en\",\n                            \"es\",\n                            \"fr\",\n                            \"de\",\n                            \"it\",\n                            \"pt\",\n                            \"pl\",\n                            \"tr\",\n                            \"ru\",\n                            \"nl\",\n                            \"cs\",\n                            \"ar\",\n                            \"hu\",\n                            \"ko\",\n                            \"ja\",\n                        ]\n                    )\n                    tts_text = gr.Textbox(\n                        label=\"输入要合成的文字.\",\n                        value=\"你好啊，亲爱的朋友.\",\n                    )\n                    tts_btn = gr.Button(value=\"生成声音\")\n\n                with gr.Column() as col3:\n                    progress_gen = gr.Label(\n                        label=\"进度:\"\n                    )\n                    tts_output_audio = gr.Audio(label=\"生成的声音.\")\n                    reference_audio = gr.Audio(label=\"参考音频.\")\n\n            prompt_compute_btn.click(\n                fn=preprocess_dataset,\n                inputs=[\n                    upload_file,\n                    lang\n                ],\n                outputs=[\n                    progress_data, xtts_config, xtts_vocab, xtts_checkpoint, speaker_reference_audio\n                ],\n            )\n\n           \n\n            tts_btn.click(\n                fn=run_tts,\n                inputs=[\n                    tts_language,\n                    tts_text,\n                    speaker_reference_audio,\n                ],\n                outputs=[progress_gen, tts_output_audio, reference_audio],\n            )\n\n    demo.launch(\n        share=True,\n        debug=False,\n        server_port=args.port,\n        server_name=\"0.0.0.0\"\n    )\n"
        }
      ]
    }
  ]
}