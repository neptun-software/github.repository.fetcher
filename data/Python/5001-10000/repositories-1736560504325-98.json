{
  "metadata": {
    "timestamp": 1736560504325,
    "page": 98,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/xformers",
      "stars": 8877,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 2.509765625,
          "content": "---\nAccessModifierOffset: -1\nAlignAfterOpenBracket: AlwaysBreak\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nAlignEscapedNewlinesLeft: true\nAlignOperands:   false\nAlignTrailingComments: false\nAllowAllParametersOfDeclarationOnNextLine: false\nAllowShortBlocksOnASingleLine: false\nAllowShortCaseLabelsOnASingleLine: false\nAllowShortFunctionsOnASingleLine: Empty\nAllowShortIfStatementsOnASingleLine: false\nAllowShortLoopsOnASingleLine: false\nAlwaysBreakAfterReturnType: None\nAlwaysBreakBeforeMultilineStrings: true\nAlwaysBreakTemplateDeclarations: true\nBinPackArguments: false\nBinPackParameters: false\nBraceWrapping:\n  AfterClass:      false\n  AfterControlStatement: false\n  AfterEnum:       false\n  AfterFunction:   false\n  AfterNamespace:  false\n  AfterObjCDeclaration: false\n  AfterStruct:     false\n  AfterUnion:      false\n  BeforeCatch:     false\n  BeforeElse:      false\n  IndentBraces:    false\nBreakBeforeBinaryOperators: None\nBreakBeforeBraces: Attach\nBreakBeforeTernaryOperators: true\nBreakConstructorInitializersBeforeComma: false\nBreakAfterJavaFieldAnnotations: false\nBreakStringLiterals: false\nColumnLimit:     80\nCommentPragmas:  '^ IWYU pragma:'\n#CompactNamespaces: false\nConstructorInitializerAllOnOneLineOrOnePerLine: true\nConstructorInitializerIndentWidth: 4\nContinuationIndentWidth: 4\nCpp11BracedListStyle: true\nDerivePointerAlignment: false\nDisableFormat:   false\nForEachMacros:   [ FOR_EACH_RANGE, FOR_EACH, ]\nIncludeCategories:\n  - Regex:           '^<.*\\.h(pp)?>'\n    Priority:        1\n  - Regex:           '^<.*'\n    Priority:        2\n  - Regex:           '.*'\n    Priority:        3\nIndentCaseLabels: true\nIndentWidth:     2\nIndentWrappedFunctionNames: false\nKeepEmptyLinesAtTheStartOfBlocks: false\nMacroBlockBegin: ''\nMacroBlockEnd:   ''\nMaxEmptyLinesToKeep: 1\nNamespaceIndentation: None\nObjCBlockIndentWidth: 2\nObjCSpaceAfterProperty: false\nObjCSpaceBeforeProtocolList: false\nPenaltyBreakBeforeFirstCallParameter: 1\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakString: 1000\nPenaltyExcessCharacter: 1000000\nPenaltyReturnTypeOnItsOwnLine: 2000000\nPointerAlignment: Left\nReflowComments:  true\nSortIncludes:    true\nSpaceAfterCStyleCast: false\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeParens: ControlStatements\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 1\nSpacesInAngles:  false\nSpacesInContainerLiterals: true\nSpacesInCStyleCastParentheses: false\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nStandard:        Cpp11\nTabWidth:        8\nUseTab:          Never\n...\n"
        },
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.15234375,
          "content": "[run]\nomit =\n    docs/*\n    tests/*\n    setup.py\n    xformers/benchmarks/*\n    xformers/triton/k_*\n    stubs/*\n    third_party/*\n    xformers/_flash_attn/*\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.1865234375,
          "content": "root = true\n\n[*.py]\ncharset = utf-8\ntrim_trailing_whitespace = true\nend_of_line = lf\ninsert_final_newline = true\nindent_style = space\nindent_size = 4\n\n[*.md]\ntrim_trailing_whitespace = false\n"
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.2646484375,
          "content": "[flake8]\nexclude =\n    .git\n    ,.github/run-clang-format.py\n    ,third_party\n    ,xformers/_flash_attn\nmax-line-length = 120\ncopyright-check = True\nselect = E,F,W,C\ncopyright-regexp=Copyright \\(c\\) Facebook, Inc. and its affiliates. All Rights Reserved\nignore=W503,E203\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.0439453125,
          "content": "*~\n*.swp\n\n*.pyc\n*.pyo\n*.so\n\n.mypy_cache/\n*.egg-info/\n\nbuild/\ndist/\n\n# for autocomplete\ncompile_commands.json\n\n# Pytest verbose output\ntest-results/\n\n# Coverage reports\n.coverage\n.coverage.*\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n.vscode/*\nxformers/benchmarks/LRA/datasets\nxformers/benchmarks/LRA/logs\n\nmy_runs.md\n\n# Triton cache\n.cache\n\n# JetBrains PyCharm IDE\n.idea/\n\n# Pyre cache\n.pyre/\n\n# Watchman config files\n.watchmanconfig\n\n# examples demo files\nexamples/input.txt\nexamples/lightning_logs\nexamples/data\n\n# Hydra default output dir\nmultirun\noutputs\n\n.benchmarks\nxformers/_flash_attn\nxformers/version.py\nxformers/cpp_lib.json\n\n## temporary files\nxformers/csrc/attention/hip_fmha/*.cu\nxformers/csrc/attention/hip_fmha/*.hip\nxformers/csrc/attention/hip_fmha/*_hip.h\nxformers/csrc/attention/hip_fmha/instances/*.cu\nxformers/csrc/attention/hip_fmha/instances/*.hip\nxformers/csrc/attention/hip_fmha/instances/*_hip.h\nxformers/csrc/attention/hip_decoder/*.cu\nxformers/csrc/attention/hip_decoder/*.hip\nxformers/csrc/attention/hip_decoder/*_hip.h\n\n\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.396484375,
          "content": "[submodule \"third_party/cutlass\"]\n\tpath = third_party/cutlass\n\turl = https://github.com/NVIDIA/cutlass.git\n[submodule \"third_party/flash-attention\"]\n\tpath = third_party/flash-attention\n\turl = https://github.com/Dao-AILab/flash-attention.git\n[submodule \"third_party/composable_kernel_tiled\"]\n\tpath = third_party/composable_kernel_tiled\n\turl = https://github.com/ROCm/composable_kernel.git\n\tbranch = develop\n"
        },
        {
          "name": ".isort.cfg",
          "type": "blob",
          "size": 0.32421875,
          "content": "[settings]\nknown_third_party =fvcore,hydra,input_pipeline,matplotlib,numpy,omegaconf,pandas,pl_bolts,pyre_extensions,pytest,pytorch_lightning,ragged_inference,recommonmark,seaborn,setuptools,sklearn,submitit,tensorflow,timm,torch,torchmetrics,torchvision,tqdm,triton,typing_extensions\nskip_glob=third_party/*,xformers/_flash_attn/*\n"
        },
        {
          "name": ".markdownlint.json",
          "type": "blob",
          "size": 0.0419921875,
          "content": "{\n    \"MD013\": false,\n    \"MD033\": false\n}\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.888671875,
          "content": "exclude: 'build|stubs'\n\ndefault_language_version:\n    python: python3\n\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v3.4.0\n    hooks:\n    -   id: trailing-whitespace\n    -   id: check-ast\n    -   id: check-merge-conflict\n    -   id: no-commit-to-branch\n        args: ['--branch=master']\n    -   id: check-added-large-files\n        args: ['--maxkb=500']\n    -   id: end-of-file-fixer\n\n-   repo: https://github.com/ambv/black\n    rev: 22.3.0\n    hooks:\n    - id: black\n      language_version: python3.11\n\n-   repo: https://github.com/pycqa/flake8\n    rev: 6.1.0\n    hooks:\n    -   id: flake8\n\n-   repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n    -   id: isort\n        exclude: README.md\n        additional_dependencies: [toml]\n        args: [\"--profile\", \"black\"]\n\n-   repo: https://github.com/pre-commit/mirrors-mypy\n    rev: 'v1.10.0'\n    hooks:\n    -   id: mypy\n"
        },
        {
          "name": ".pyre_configuration",
          "type": "blob",
          "size": 0.189453125,
          "content": "{\n  \"ignore_all_errors\": [\"xformers/benchmarks/\", \"xformers/_flash_attn\"],\n  \"python_version\": \"3.9\",\n  \"source_directories\": [\n    \"stubs\",\n    {\"import_root\": \".\", \"source\": \"xformers\"}\n  ]\n}\n"
        },
        {
          "name": "BENCHMARKS.md",
          "type": "blob",
          "size": 11.3935546875,
          "content": "# Benchmarks: how to and some results\n\n## Benchmark a full encoder block\n\nSweeping over different attention settings to log max memory use and runtime can for instance be done by invoking\n`python3 xformers/benchmarks/benchmark_encoder.py`. Specifying a subset to test is done through command line arguments, for instance `python3 xformers/benchmarks/benchmark_encoder.py --causal True --attentions random --activations gelu -fp16 True`.\n\nPlease note that:\n\n- These numbers are dependent of hyperparameters (dimensions chosen for Linformer, sparsity of the pattern), they are mostly an illustration\n- The sparse attention patterns tested here are just presets, as explained in the linked notebook generating any new sparse attention pattern should be relatively easy, while keeping the benefits of optimized computations.\n\nSome examples, generated with `python3 xformers/benchmarks/benchmark_encoder.py --activations gelu --plot -emb 256 -bs 8 -heads 4`\n\n![Memory use for different attentions](docs/plots/memory_vs_attention.png)  ![Runtime for different attentions](docs/plots/runtime_vs_attention.png)\n\n## Benchmark the core sparse attention mechanisms\n\n`python3 xformers/benchmarks/benchmark_core.py` will measure the speed of the core sparse attention mechanism. The current numbers are as follows (times in microseconds (us)):\n\n|                        | **matmul_with_mask**  |                        | **softmax**           |                        | **bmm**               |                        |\n| ---------------------- | --------------------- | ---------------------- | --------------------- | ---------------------- | --------------------- | ---------------------- |\n|                        | **B=8, M=256, K=128** | **B=8, M=1024, K=256** | **B=8, M=256, K=128** | **B=8, M=1024, K=256** | **B=8, M=256, K=128** | **B=8, M=1024, K=256** |\n| dense                  | 62.3                  | 510.3                  | 12.8                  | 141.9                  | 31.0                  | 590.7                  |\n| dense with masking     | 84.2                  | 805.3                  | -                     | -                      | -                     | -                      |\n| sparsity pytorch: 0.50 | 392.4                 | 6197.4                 | 1140.9                | 8081.4                 | 577.0                 | 13830.2                |\n| sparsity pytorch: 0.80 | 336.2                 | 4437.3                 | 515.0                 | 3494.8                 | 254.4                 | 5944.0                 |\n| sparsity pytorch: 0.90 | 244.1                 | 3017.4                 | 367.3                 | 1932.6                 | 162.0                 | 3063.0                 |\n| sparsity pytorch: 0.95 | 193.2                 | 1899.5                 | 293.6                 | 1078.9                 | 161.6                 | 1692.3                 |\n| sparsity pytorch: 0.99 | 195.6                 | 695.0                  | 252.1                 | 342.4                  | 161.9                 | 433.4                  |\n| sparsity sputnik: 0.50 | 77.9                  | 1695.9                 | 32.8                  | 164.7                  | 64.6                  | 1640.5                 |\n| sparsity sputnik: 0.80 | 43.8                  | 793.0                  | 32.9                  | 50.8                   | 39.6                  | 703.3                  |\n| sparsity sputnik: 0.90 | 43.6                  | 435.5                  | 33.0                  | 33.5                   | 39.6                  | 391.4                  |\n| sparsity sputnik: 0.95 | 43.2                  | 258.6                  | 32.5                  | 32.7                   | 39.7                  | 223.6                  |\n| sparsity sputnik: 0.99 | 43.5                  | 145.4                  | 33.2                  | 32.7                   | 39.7                  | 77.4                   |\n\n## Triton layers\n\nPlease not that as of November 2022 these layers are not optimized for typical production GPUs out there (not developed for some time and mostly tested on a laptop GPU), and that better performances are probably possible with some minor changes as proven in other libraries since xformers went out.\n\n### Fused softmax\n\nYou can reproduce these numbers locally by running `python3 xformers/benchmarks/benchmark_triton_softmax.py`. The units are GB/s. These results are for a laptop nVidia 3080, Triton 2.0 and PyTorch 1.12.\n\n![Softmax throughput in fp16 - inference](docs/plots/fused_softmax/Softmax_Bandwidth_FW_fp16.png)\n\n![Softmax throughput in fp16 - training](docs/plots/fused_softmax/Softmax_Bandwidth_FW_BW_fp16.png)\n\n![Softmax throughput in fp32 - inference](docs/plots/fused_softmax/Softmax_Bandwidth_FW_fp32.png)\n\n![Softmax throughput in fp32 - training](docs/plots/fused_softmax/Softmax_Bandwidth_FW_BW_fp32.png)\n\n### Fused linear layer\n\nYou can reproduce these numbers locally by running `python3 xformers/benchmarks/benchmark_triton_fused_linear_layer.py`. The units are TFlops/s. These results are for a laptop nVidia 3080, Triton 2.0 and PyTorch 1.12.\n\n![Fused linear layers throughput in fp16 - inference](docs/plots/fused_linear/FusedLinear_fp16_FW_gelu.png)\n\n![Fused linear layers throughput in fp16 - training](docs/plots/fused_linear/FusedLinear_fp16_FW_BW_gelu.png)\n\n![Fused linear layers throughput in fp16 - inference](docs/plots/fused_linear/FusedLinear_fp16_FW_relu.png)\n\n![Fused linear layers throughput in fp16 - training](docs/plots/fused_linear/FusedLinear_fp16_FW_BW_relu.png)\n\n![Fused linear layers throughput in fp16 - inference](docs/plots/fused_linear/FusedLinear_fp16_FW_leaky_relu.png)\n\n![Fused linear layers throughput in fp16 - training](docs/plots/fused_linear/FusedLinear_fp16_FW_BW_leaky_relu.png)\n\n![Fused linear layers throughput in fp16 - inference](docs/plots/fused_linear/FusedLinear_fp16_FW_squared_relu.png)\n\n![Fused linear layers throughput in fp16 - training](docs/plots/fused_linear/FusedLinear_fp16_FW_BW_squared_relu.png)\n\n![Fused linear layers throughput in fp16 - inference](docs/plots/fused_linear/FusedLinear_fp16_FW_none.png)\n\n![Fused linear layers throughput in fp16 - training](docs/plots/fused_linear/FusedLinear_fp16_FW_BW_none.png)\n\n### Fused layer norm\n\nYou can reproduce these numbers locally by running `python3 xformers/benchmarks/benchmark_triton_layernorm.py`. The units are GB/s. These results are for a laptop nVidia 3080, Triton 2.0 and PyTorch 1.12.\n\n![Fused layer norm throughput in fp16 - inference](docs/plots/layer_norm/LayerNorm_FW_torch.float16.png)\n\n![Fused layer norm throughput in fp16 - training](docs/plots/layer_norm/LayerNorm_FW+BW_torch.float16.png))\n\n![Fused layer norm throughput in fp32 - inference](docs/plots/layer_norm/LayerNorm_FW_torch.float32.png))\n\n![Fused layer norm throughput in fp32 - training](docs/plots/layer_norm/LayerNorm_FW+BW_torch.float32.png))\n\n### Fused dropout + bias + activation\n\nYou can reproduce these numbers locally by running `python3 xformers/benchmarks/benchmark_triton_dropout.py`. The units are GB/s. These results are for a laptop nVidia 3080, Triton 2.0 and PyTorch 1.12.\n\n![Fused dropout+ bias throughput in fp16 - inference](docs/plots/fused_dropout/Dropout_Bias_True_FW_torch.float16_Act_gelu.png)\n\n![Fused dropout+ bias throughput in fp16 - training](docs/plots/fused_dropout/Dropout_Bias_True_FW+BW_torch.float16_Act_gelu.png))\n\n![Fused dropout+ bias throughput in fp32 - inference](docs/plots/fused_dropout/Dropout_Bias_True_FW+BW_torch.float32_Act_gelu.png))\n\n![Fused dropout+ bias throughput in fp32 - training](docs/plots/fused_dropout/Dropout_Bias_True_FW+BW_torch.float32_Act_gelu.png))\n\n![Fused dropout+ bias throughput in fp16 - inference](docs/plots/fused_dropout/Dropout_Bias_True_FW_torch.float16_Act_squared_relu.png)\n\n![Fused dropout+ bias throughput in fp16 - training](docs/plots/fused_dropout/Dropout_Bias_True_FW+BW_torch.float16_Act_squared_relu.png))\n\n![Fused dropout+ bias throughput in fp32 - inference](docs/plots/fused_dropout/Dropout_Bias_True_FW+BW_torch.float32_Act_squared_relu.png))\n\n![Fused dropout+ bias throughput in fp32 - training](docs/plots/fused_dropout/Dropout_Bias_True_FW+BW_torch.float32_Act_squared_relu.png))\n\n## LRA\n\nThe code for this benchmark has been adapted from [this repository](https://github.com/mlpen/Nystromformer/tree/main/LRA). [A dedicated README is available here](xformers/benchmarks/LRA/README.md)\n\n__Some results:__\n\n| Attention                   | ListOps  | Text      | Retrieval | Image     | Pathfinder | *Avg*     | *Est. Gflops* | *Peak mem (mb)* |\n| --------------------------- | -------- | --------- | --------- | --------- | ---------- | --------- | ------------- | --------------- |\n| _Chance_                    | _10_     | _50_      | _50_      | _10_      | _50_       | _34_      | _0_           | _0_             |\n| Standard                    | **37.5** | 62.66     | 79.24     | 38.69     | **70.37**  | **57.69** | 1.21          | 2291            |\n| Nystromformer-128           | 36.29    | 63.24     | 78.18     | **42.86** | 67.49      | 57.61     | 0.62          | 383             |\n| Favor-256 (redraw)          | 19.56    | 62.76     | **81.1**  | 36.09     | 67.23      | 53.35     | 0.49          | 445             |\n| FourierMix                  | 36.29    | 60.72     | 76.41     | 36.53     | 54.07      | 52.8      | **0.17**      | **87**          |\n| Linformer-seq/4 (no redraw) | 36.69    | 57.39     | 76.41     | 35.57     | 65.12      | 54.2      | 0.67          | 719             |\n| Lambda                      | 19.76    | 62.47     | 79.11     | 35.04     | 49.74      | 49.224    | x             | 1023            |\n| Orthoformer-32              | 27.42    | **63.96** | 77.96     | 34.5      | 67.11      | 54.19     | 0.187         | 155             |\n\n- Contrary to the initial LRA proposal, __we use the same model architecture for all tasks (2 layers).__\n- The training schedule for ListOps has been lengthened, while keeping it the fastest of all tasks, which reduces the seed dependence in the final accuracy figure.\n- Estimated flops and peak memory are on the ListOps task, using 4 GPUs. Note that LRA is not completely well defined, in that hyperparameters and model architectures can vary (should the same architecture be used everywhere ? Similar hyperparams ?). This could be improved in the future, but in the meantime one should probably not read too much into small differences for some tasks, probably not meaningful.\n\n_Note_: The estimated flops currently miss accounting for many operators, and are almost certainly an undercount. See issue [#154](https://github.com/fairinternal/xformers/issues/154)\n\n\n## Causal Attention Blocksparse Optimization\n\nFP16            | FP32\n:-------------------------:|:-------------------------:\n![fw](docs/plots/causal_attention_blocksparse/Causal_Blocksparse_Runtime_FW_fp16_Blocksize128.png)  |  ![fw](docs/plots/causal_attention_blocksparse/Causal_Blocksparse_Runtime_FW_fp32_Blocksize128.png)\n![fw+bw](docs/plots/causal_attention_blocksparse/Causal_Blocksparse_Runtime_FW+BW_fp16_Blocksize128.png)  |  ![fw+bw](docs/plots/causal_attention_blocksparse/Causal_Blocksparse_Runtime_FW+BW_fp32_Blocksize128.png)\n![fw](docs/plots/causal_attention_blocksparse/Causal_Blocksparse_Memory_FW_fp16_Blocksize128.png)  |  ![fw](docs/plots/causal_attention_blocksparse/Causal_Blocksparse_Memory_FW_fp32_Blocksize128.png)\n![fw+bw](docs/plots/causal_attention_blocksparse/Causal_Blocksparse_Memory_FW+BW_fp16_Blocksize128.png)  |  ![fw+bw](docs/plots/causal_attention_blocksparse/Causal_Blocksparse_Memory_FW+BW_fp32_Blocksize128.png)\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 19.6259765625,
          "content": "# Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [0.0.29] - 2024-12-27\n### Improved:\n- [fMHA] Creating a `LowerTriangularMask` no longer creates a CUDA tensor\n- [fMHA] Updated Flash-Attention to `v2.7.2.post1`\n- [fMHA] Flash-Attention v3 will now be used by `memory_efficient_attention` by default when available, unless the operator is enforced with the `op` keyword-argument. Switching from Flash2 to Flash3 can make transformer trainings ~10% faster end-to-end on H100s\n- [fMHA] Fixed a performance regression with the `cutlass` backend for the backward pass (facebookresearch/xformers#1176) - mostly used on older GPUs (eg V100)\n- Fixed swiglu operator compatibility with torch-compile with PyTorch 2.6\n- Fix activation checkpointing of SwiGLU when AMP is enabled (facebookresearch/xformers#1152)\n### Removed:\n- Following PyTorch, xFormers no longer builds binaries for conda. Pip is now the only recommended way to get xFormers\n- Removed unmaintained/deprecated components in `xformers.components.*` (see facebookresearch/xformers#848)\n\n## [0.0.28.post3] - 2024-10-30\nPre-built binary wheels require PyTorch 2.5.1\n\n## [0.0.28.post2] - 2024-10-18\nPre-built binary wheels require PyTorch 2.5.0\n\n## [0.0.28.post1] - 2024-09-13\nProperly upload wheels for cuda 12.4\n\n## [0.0.28] - 2024-09-12\nPre-built binary wheels require PyTorch 2.4.1\n### Added\n- Added wheels for cuda 12.4\n- Added conda builds for python 3.11\n- Added wheels for rocm 6.1\n### Improved\n- Profiler: Fix computation of FLOPS for the attention when using xFormers\n- Profiler: Fix MFU/HFU calculation when multiple dtypes are used\n- Profiler: Trace analysis to compute MFU & HFU is now much faster\n- fMHA/splitK: Fixed `nan` in the output when using a `torch.Tensor` bias where a lot of consecutive keys are masked with `-inf`\n- Update Flash-Attention version to `v2.6.3` *when building from scratch*\n- When using the most recent version of Flash-Attention, it is no longer possible to mix it with the cutlass backend. In other words, it is no longer possible to use the cutlass Fw with the flash Bw.\n### Removed\n- fMHA: Removed `decoder` and `small_k` backends\n- profiler: Removed `DetectSlowOpsProfiler` profiler\n- Removed compatibility with PyTorch < 2.4\n- Removed conda builds for python 3.11\n- Removed windows pip wheels for cuda 12.1 and 11.8\n\n## [0.0.27.post2] - 2024-07-26\nPre-built binary wheels require PyTorch 2.4.0\n\n## [0.0.27.post1] - 2024-07-25\nPre-built binary wheels require PyTorch 2.4.0\n\n## [0.0.27] - 2024-07-10\nPre-built binary wheels require PyTorch 2.3.1\n### Added\n- fMHA: `PagedBlockDiagonalGappyKeysMask`\n- fMHA: heterogeneous queries in `triton_splitk`\n- fMHA: support for paged attention in flash\n- fMHA: Added backwards pass for `merge_attentions`\n- fMHA: Added `torch.compile` support for 3 biases (`LowerTriangularMask`, `LowerTriangularMaskWithTensorBias` and `BlockDiagonalMask`) - some might require PyTorch 2.4\n- fMHA: Added `torch.compile` support in `memory_efficient_attention` when passing the flash operator explicitely (eg `memory_efficient_attention(..., op=(flash.FwOp, flash.BwOp))`)\n- fMHA: `memory_efficient_attention` now expects its `attn_bias` argument to be on the same device as the other input tensor. Previously, it would convert the bias to the right device.\n- fMHA: `AttentionBias` subclasses are now constructed by default on the `cuda` device if available - they used to be created on the CPU device\n- 2:4 sparsity: Added `xformers.ops.sp24.sparsify24_ste` for Straight Through Estimator (STE) with options to rescale the gradient differently for masked out/kept values\n### Improved\n- fMHA: Fixed out-of-bounds reading for Split-K triton implementation\n- Profiler: fix bug with modules that take a single tuple as argument\n- Profiler: Added manual trigger for a profiling step, by creating a `trigger` file in the profiling directory\n### Removed\n- Removed support for PyTorch version older than 2.2\n\n## [0.0.26] - 2024-04-29\nPre-built binary wheels require PyTorch 2.3.0\n### Added\n- [2:4 sparsity] Added support for Straight-Through Estimator for `sparsify24` gradient (`GRADIENT_STE`)\n- [2:4 sparsity] `sparsify24_like` now supports the cuSparseLt backend, and the STE gradient\n- Basic support for `torch.compile` for the `memory_efficient_attention` operator. Currently only supports Flash-Attention, and without any bias provided. We want to expand this coverage progressively.\n### Improved\n- merge_attentions no longer needs inputs to be stacked.\n- fMHA: triton_splitk now supports additive bias\n- fMHA: benchmark cleanup\n\n## [0.0.25.post1] - 2024-03-29\nPre-built binary wheels require PyTorch 2.2.2\n\n## [0.0.25] - 2024-03-14\nPre-built binary wheels require PyTorch 2.2.1\n### Added\n- New `merge_attentions` function\n- fMHA: New gappy attention biases.\n### Improved\n- fMHA: Updated Flash-Attention to v2.5.6: this has a performance improvement for multiquery.\n- fMHA: triton_splitk changed and expanded. Now amalgamates using LSE. Can autotune, supports causal with a small number of queries - not just 1. Experimental support for paged attention.\n- `rope_padded`: Fixed CUDA error with many queries (more than 65k)\n- `rmsnorm`: Fixed CUDA error with large inputs (enables 512k+ sequence length on Llama2 70B)\n### Removed\n- fMHA: Removed triton operator (`fmha.triton.*`, `xformers.ops.MemoryEfficientAttentionTritonFwdFlashBwOp`, `xformers.ops.TritonFlashAttentionOp`), as it has correctness issues under some conditions, and is slower than other implementations.\n\n## [0.0.24] - 2024-01-31\nPre-built binary wheels require PyTorch 2.2.0\n### Added\n- Added components for model/sequence parallelism, as near-drop-in replacements for FairScale/Megatron Column&RowParallelLinear modules. They support fusing communication and computation for sequence parallelism, thus making the communication effectively free. [Read more](https://twitter.com/d_haziza/status/1753030654118211593)\n- Added kernels for training models with 2:4-sparsity. We introduced a very fast kernel for converting a matrix A into 24-sparse format, which can be used during training to sparsify weights dynamically, activations etc... xFormers also provides an API that is compatible with torch-compile, see `xformers.ops.sparsify24`.\n### Improved\n- Make selective activation checkpointing be compatible with torch.compile.\n### Removed\n- Triton kernels now require a GPU with compute capability 8.0 at least (A100 or newer). This is due to newer versions of triton not supporting older GPUs correctly\n- Removed support for PyTorch version older than 2.1.0\n\n## [0.0.23] - 2023-12-05\nPre-built binary wheels require PyTorch 2.1.1 (xFormers `0.0.23`) or PyTorch 2.1.2 (xFormers `0.0.23.post1`).\n### Fixed\n- fMHA: Fixed a bug in cutlass backend forward pass where the logsumexp was not correctly calculated, resulting in wrong results in the BW pass. This would happen with MQA when one sequence has a query with `length%64 == 1`\n- fMHA: Updated Flash-Attention to v2.3.6 - this fixes a performance regression in causal backward passes, and now supports `BlockDiagonalCausalWithOffsetPaddedKeysMask`\n### Added\n- fMHA: Added `LocalAttentionFromBottomRightMask` (local)\n- fMHA: Added `LowerTriangularFromBottomRightMask` (causal)\n- fMHA: Added `LowerTriangularFromBottomRightLocalAttentionMask` (local + causal)\n### Removed\n- Removed `xformers.triton.sum_strided`\n\n## [0.0.22] - 2023-09-27\n### Fixed\n- fMHA: Backward pass now works in PyTorch deterministic mode (although slower)\n### Added\n- fMHA: Added experimental support for Multi-Query Attention and Grouped-Query Attention. This is handled by passing 5-dimensional inputs to `memory_efficient_attention`, see the documentation for more details\n- fMHA: Added experimental support for Local Attention biases to `memory_efficient_attention`\n- Added an example of efficient [LLaMa decoding](https://github.com/facebookresearch/xformers/tree/main/examples/llama_inference) using xformers operators\n- Added Flash-Decoding for faster attention during Large Language Model (LLM) decoding - up to 50x faster for long sequences (token decoding up to 8x faster end-to-end)\n- Added an efficient rope implementation in triton, to be used in LLM decoding\n- Added selective activation checkpointing, which gives fine-grained control of which activations to keep and which activations to recompute\n- `xformers.info` now indicates the Flash-Attention version used\n### Removed\n- fMHA: Removed `smallK` backend support for CPU. `memory_efficient_attention` only works for CUDA/GPU tensors now\n- **DEPRECATION**: Many classes in `xformers.factory`, `xformers.triton` and `xformers.components` have been or will be deprecated soon (see tracking issue facebookresearch/xformers#848)\n\n## [0.0.21] - 2023-08-18\n### Improved\n- fMHA: Updated [flash-attention](https://github.com/Dao-AILab/flash-attention) to v2, with massive performance improvements for both the forward pass and backward pass. This implementation is now used by default when it's available\n### Bug fixes\n- fMHA/cutlass: Fix potential race condition in the FW/BW passes\n- fMHA/cutlass: Fix `attn_bias` stride overflow for very long sequences (>32k)\n- `LowerTriangularMask` is now backward compatible with older xformers versions\n### Breaking changes\n- `memory_efficient_attention` now expects the `attn_bias` argument to have a head dimension\n- `memory_efficient_attention` no longer broadcasts the batch/head dimensions of `attn_bias`. Please use `.expand` if you need to broadcast the bias\n- Remove `causal_diagonal` argument from `BlockDiagonalCausalWithOffsetPaddedKeysMask`\n### Added\n- Binary wheels on pypi/conda now contain H100 kernels\n- fMHA: Added backend specialized for decoding that does not use TensorCores - useful when not using multiquery\n\n**NOTE**: Binary wheels are now provided only for PyTorch 2 with cuda 11.8. It is still possible to use xFormers with older versions of PyTorch by building from source or using conda.\n\n\n## [0.0.20] - 2023-05-23\n### Improved\n- fMHA/cutlass (backward): Massive performance improvements when `batch_size * num_heads` is low (10x+)\n- fMHA/cutlass: Further performance improvements for both the forward & backward kernels\n- fMHA (backward): Now dispatching to cutlass when `embed_dim>64`\n- fMHA: Updated Flash-Attention to `v1.0.5`\n### Added\n- fMHA now runs on H100 (support is experimental)\n\n## [0.0.19] - 2023-04-28\n### Added\n- Display `nvcc` version used to compile `xformers` in `python -m xformers.info`\n\n### Fixed\n- Fixed performance regression with `nvcc>11.6` (facebookresearch/xformers#712)\n- fMHA/cutlass: Fixed `nan` in the output when using a `torch.Tensor` with `-inf` prefixes as `attn_bias` (facebookresearch/xformers#722)\n- fMHA/cutlass: Fixed `nan` in the output when the sequence length is larger than `2 ** 15` (facebookresearch/xformers#719)\n- fMHA/cutlass: Significative performance improvements (up to 2x) for both the forward pass and backward pass\n- fMHA/cutlass: The kernel are now deterministic\n- fMHA/cutlass: Fixed backward pass correctness when using dropout (facebookresearch/xformers#724)\n\n## [0.0.18] - 2023-03-31\n### Added\n- Added `xformers.ops.index_select_cat` and `xformers.ops.scaled_index_add` - those are experimental functions that only work with a few shapes, and can be used to write efficient stochastic depth in transformer architectures for instance\n\n### Fixed\n- fMHA: `memory_efficient_attention` now accepts `torch.Tensor` as attention bias for any seqlen, although there are still requirements on the alignment of the bias tensor (see facebookresearch/xformers#683)\n\n## [0.0.17] - 2023-03-28\n### Fixed\n- fMHA: Fixed BW pass on Sm86/Sm89 GPUs when `K > 64` (RTX 3090, RTX 4090, A6000, ..) [facebookresearch/xformers#631]\n\n### Added\n- fMHA/CUTLASS: Added tensor attn bias support [facebookresearch/xformers#587] - contribution from [@jfc4050](https://github.com/jfc4050)\n- fMHA/CUTLASS: Added tensor attn bias grad support [facebookresearch/xformers#587] - contribution from [@jfc4050](https://github.com/jfc4050)\n- fMHA/CUTLASS: Added dropout support [facebookresearch/xformers#587] - contribution from [@jfc4050](https://github.com/jfc4050)\n- fMHA: Added support for varying sequence lengths [facebookresearch/xformers#500]\n\n\n## [0.0.16] - 2023-01-31\n### Fixed\n- Updated triton dependency [facebookresearch/xformers#418]\n- Stripe lineinfo from binaries, reducing the binary size [facebookresearch/xformers#549]\n- Added support for pip wheels [facebookresearch/xformers#588, facebookresearch/xformers#573, facebookresearch/xformers#534, facebookresearch/xformers#523, ...] big thanks to [@AbdBarho](https://github.com/AbdBarho)!\n- Fixed compatibility with Python 3.7 [facebookresearch/xformers#541] - thanks to [@susumuota](https://github.com/susumuota)\n- fMHA: Fixed strides for QKV gradients for cutlass attention [facebookresearch/xformers#535]\n- fMHA: Stricter inputs validation to avoid CUDA errors for unsupported inputs [facebookresearch/xformers#592]\n- fMHA/Flash-Attention: Updated to https://github.com/HazyResearch/flash-attention/commit/a1f49a2b92b6fa022379bbebafed9d7f5e96a675 with multiple changes from [@TriDao](https://github.com/tridao) that make the operator up to 20% faster\n- fMHA/Flash-Attention: Fixed backward pass wrapper, where non-contiguous gradients could give the wrong result [facebookresearch/xformers#548]\n- fMHA: Separate each operator into forward and backward operators. It's now possible to use any combination of forward+backward (for instance Triton forward and Flash-Attention backward) [facebookresearch/xformers#560]\n\n### Added\n- fMHA: Added Triton operator for forward pass from [Flash-Attention](https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attn_triton.py) authored by [@TriDao](https://github.com/tridao), will be automatically used on A100 when compatible\n- fMHA: Added [`xformers.ops.memory_efficient_attention_forward`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention_forward), [`xformers.ops.memory_efficient_attention_forward_requires_grad`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention_forward_requires_grad), [`xformers.ops.memory_efficient_attention_backward`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention_backward) for power-users who write custom autograd functions [facebookresearch/xformers#560]\n- fMHA: Support for custom scaling for the CUTLASS-based kernel [facebookresearch/xformers#530] - contribution from [@comaniac](https://github.com/comaniac)\n\n## [0.0.15] - Skipped\n\n## [0.0.14] - 2022-11-10\n### Fixed\n- fMHA/CUTLASS: The current CUDA stream is now used by the kernel [facebookresearch/xformers#491]\n- fMHA/CUTLASS: Improve overall performance\n\n### Added\n- SwiGLU: Added `xformers.ops.SwiGLU` and its functional counterpart (`xformers.ops.swiglu`) [facebookresearch/xformers#490]\n- fMHA: Possible to combine CUTLASS's forward with flash-attention's backward pass [facebookresearch/xformers#469] - improves performance on A100 for K = 128\n- fMHA: Add custom `xformers.ops.unbind` operator to avoid a cat in the attention block [facebookresearch/xformers#458]\n\n## [0.0.13] - 2022-09-26\n### Added\n- fMHA: Added CUTLASS-based kernel for `xformers.ops.memory_efficient_attention`. This kernel is automatically depending on the inputs, and works on any GPU after P100 [facebookresearch/xformers#362]\n\n## [0.0.12] - 2022-08-08\n### Fixed\n- Removed duplicated biases in the FusedMLP layers [facebookresearch/xformers#317]\n- Rotary embeddings respecting input types [facebookresearch/xformers#326]\n- Poolformer style instantiating useless projection layers [facebookresearch/xformers#349]\n- Fix layer position not being properly tracked, causing extra layernorms for programmatic xformers [facebookresearch/xformers#348]\n- Pass use_triton flag to LayerNorm module [facebookresearch/xformers#336]\n\n### Added\n- Four blocksparsity layouts from DeepSpeed [facebookresearch/xformers#320]\n- Support several initialization options [facebookresearch/xformers#312]\n- Conv2DFeedforward feedforward part [facebookresearch/xformers#321]\n- VisualAttention [facebookresearch/xformers#329]\n- Automatic blocksparse for causal attention [facebookresearch/xformers#334]\n- Better hierarchical transformer generation [facebookresearch/xformers#345]\n- Fused operations with AOTAutograd/NVFuser, integration into MLP [facebookresearch/xformers#357]\n- Refactor LRA code to use Pytorch Lightning [facebookresearch/xformers#343]\n\n## [0.0.11] - 2022-05-30\n### Fixed\n- Fix some torchscriptability [facebookresearch/xformers#246]\n- Fix FourierMix being compatible with AMP [facebookresearch/xformers#258]\n- Better asserts on QKV dimensions [facebookresearch/xformers#264]\n- Better perfs for FusedMLP and FusedLinearLayer [facebookresearch/xformers#283]\n- Deepnorm init missing self-attention [facebookresearch/xformers#284]\n\n### Added\n- Simplicial Embeddings [facebookresearch/xformers#259]\n- Mem efficient attention, FW pass [facebookresearch/xformers#267]\n- MHA benchmark\n- MLP benchmark\n- Move all triton kernels to triton v2 [facebookresearch/xformers#272]\n- Mem efficient attention, BW pass [facebookresearch/xformers#281]\n- Metaformer support [facebookresearch/xformers#294]\n\n## [0.0.10] - 2022-03-14\n### Fixed\n- Expose bias flag for feedforwards, same default as Timm [facebookresearch/xformers#220]\n- Update eps value for layernorm, same default as torch [facebookresearch/xformers#221]\n- PreNorm bugfix, only one input was normalized [facebookresearch/xformers#233]\n- Fix bug where embedding dimensions that did not match model dim would lead to a crash [facebookresearch/xformers#244]\n\n### Added\n- Add DeepNet (DeepNorm) residual path and init [facebookresearch/xformers#227]\n\n## [0.0.9] - 2022-02-09\n### Added\n- Compositional Attention [facebookresearch/xformers#41]\n- Experimental Ragged attention [facebookresearch/xformers#189]\n- Mixture of Experts [facebookresearch/xformers#181]\n- BlockSparseTensor [facebookresearch/xformers#202]\n- Nd-tensor support for triton softmax [facebookresearch/xformers#210]\n\n### Fixed\n- Bugfix Favor, single feature map [facebookresearch/xformers#183]\n- Sanity check blocksparse settings [facebookresearch/xformers#207]\n- Fixed some picklability [facebookresearch/xformers#204]\n\n## [0.0.8] - 2022-01-07\n### Fixed\n- Much faster fused dropout [facebookresearch/xformers#164]\n- Fused dropout repeatability [facebookresearch/xformers#173]\n\n### Added\n- Embedding weight tying option [facebookresearch/xformers#172]\n\n## [0.0.7] - 2021-11-30\n### Fixed\n- Dropout setting not properly passed in many attentions [facebookresearch/xformers#123]\n\n## [0.0.6] - 2021-11-24\n### Fixed\n- Fix self attention optimization not being triggered, broken residual path [facebookresearch/xformers#119]\n- Improve speed by not using contiguous Tensors when not needed [facebookresearch/xformers#119]\n\n### Added\n- Attention mask wrapper [facebookresearch/xformers#113]\n- ViT comparison benchmark [facebookresearch/xformers#117]\n\n## [0.0.4] - 2021-11-16\n### Fixed\n- Homogenizing the masks, additive or bool [facebookresearch/xformers#79][facebookresearch/xformers#85][facebookresearch/xformers#86]\n- Fix causality flag not being respected [facebookresearch/xformers#103]\n- Enabling FusedLayerNorm by default in the factory if Triton is available\n- Fixing Favor with fp16\n- Fixing Favor trainability\n\n### Added\n- Fused dropout/bias/activation layer [facebookresearch/xformers#58]\n- Fused layernorm used by default in the factory [facebookresearch/xformers#92]\n\n\n## [0.0.3] - 2021-11-01\n### Fixed\n- Nystrom causal attention [facebookresearch/xformers#75]\n\n\n## [0.0.2] - 2021-11-01\n### Fixed\n- More robust blocksparse [facebookresearch/xformers#24]\n\n### Added\n- Rotary embeddings [facebookresearch/xformers#32]\n- More flexible layernorm [facebookresearch/xformers#50]\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.4521484375,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\nThis Code of Conduct also applies outside the project spaces when there is a\nreasonable belief that an individual's behavior may have a negative impact on\nthe project or its community.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <opensource-conduct@fb.com>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 4.3193359375,
          "content": "# Contributing to the xFormers repo\n\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Our Development Process\n\nMinor changes and improvements will be released on an ongoing basis. Larger\nchanges (e.g., changesets implementing a new paper) will be released on a\nmore periodic basis.\n\n## Pull Requests\n\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `main`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\n\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\n\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nFacebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## Environment setup\n\n```bash\n~$ python3 -m venv venv2\n~$ source venv2/bin/activate\n(venv2) ~$ cd git/template/\n(venv2) ~/git/template $ pip3 install -r requirements-test.txt\n```\n\n## Coding Style\n\nIn your editor, install the [editorconfig](https://editorconfig.org/) extension\nwhich should ensure that you are following the same standards as us.\n\nTwo options to make sure that the code is formatted and linted properly:\n* either you run black, mypy and isort before opening up your PR.\n\n```bash\nblack .\nisort . --profile black\nflake8 --config .flake8\nmypy --ignore-missing-imports --scripts-are-modules --pretty --exclude build/ --exclude stubs/ .\n```\n\n* or you can just install [pre-commit](https://pre-commit.com/), which will make sure that all of the above is run automatically anytime you commit\nin that case, you would need to\n```bash\npip install pre-commit\n```\nthen (in the xformers repository, just once)\n```bash\npre-commit install\n```\n\nAfter these steps each of your commits will run the same linting and formatting routines as the xformers continuous integration, which greatly helps getting your PRs all green !\n\n_Read the [editorconfig](.editorconfig) file to understand the exact coding style preferences._\n\n## Testing\n\n### Static analysis\n\n```bash\nmypy --ignore-missing-imports --scripts-are-modules --pretty --exclude stubs/ .\n```\n\n### Unit tests\n\n```bash\npytest\n```\n\nor\n\n``` bash\npython -m pytest\n```\n\n### Check test coverage\n\n``` bash\npython -m pytest --cov-report term --cov=template  tests\n```\n\n### CircleCI status\n\nFrom your PR page, you can expand on the CircleCI results. For GPU test, you should see\nwhat CI has run, like:\n\n``` bash\n...\n----- generated xml file: /home/circleci/template/test-results/junit.xml ------\n================== 217 passed, 2 xfailed in 218.74s (0:03:38) ==================\nCircleCI received exit code 0\n```\n\nThe number of passed and failed should give you an idea on whether your local\ntest was the same or not.\n\n## Commit Guidelines\n\nWe follow the same guidelines as AngularJS. Each commit message consists of a **header**,\na **body** and a **footer**.  The header has a special format that includes a **type**,\nand a **subject**:\n\n```bash\n[<type>] <subject>\n<BLANK LINE>\n<body>\n<BLANK LINE>\n<footer>\n```\n\nAny line of the commit message cannot be longer 100 characters! This allows the message to be easier\nto read on github as well as in various git tools.\n\n### Type\n\nMust be one of the following:\n\n* **feat**: A new feature\n* **fix**: A bug fix\n* **cleanup**: Changes that do not affect the meaning of the code (white-space, formatting, missing\n  semi-colons, dead code removal etc.)\n* **refactor**: A code change that neither fixes a bug or adds a feature\n* **perf**: A code change that improves performance\n* **test**: Adding missing tests or fixing them\n* **chore**: Changes to the build process or auxiliary tools and libraries such as documentation\ngeneration\n* **docs**: Documentation only changes\n\n## License\n\nBy contributing to *xFormers*, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.572265625,
          "content": "From xFormers:\n\nCopyright (c) Facebook, Inc. and its affiliates\n\n\n===\n\nBSD 3-Clause License\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\n3. Neither the names of Facebook, Deepmind Technologies, NYU, NEC Laboratories America\n   and IDIAP Research Institute nor the names of its contributors may be\n   used to endorse or promote products derived from this software without\n   specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.705078125,
          "content": "include LICENSE\ninclude requirements.txt\ninclude version.txt\ninclude third_party/flash-attention/version.txt\n\nrecursive-include xformers/csrc *\nrecursive-include third_party/sputnik *\nrecursive-include third_party/cutlass/include *\nrecursive-include third_party/cutlass/tools/util/include *\nrecursive-include third_party/cutlass/examples *\nrecursive-include third_party/flash-attention/csrc *\nrecursive-include third_party/flash-attention/flash_attn *\n\nprune third_party/flash-attention/csrc/cutlass/docs/\nprune third_party/flash-attention/csrc/cutlass/test/\nprune third_party/flash-attention/csrc/cutlass/tools/\nprune third_party/flash-attention/csrc/cutlass/media/\nprune third_party/flash-attention/csrc/cutlass/python/\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.0361328125,
          "content": "<img src=\"./docs/assets/logo.png\" width=800>\n\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/facebookresearch/xformers/blob/main/docs/source/xformers_mingpt.ipynb)\n<br/><!--\n![PyPI](https://img.shields.io/pypi/v/xformers)\n![PyPI - License](https://img.shields.io/pypi/l/xformers)\n[![Documentation Status](https://github.com/facebookresearch/xformers/actions/workflows/gh-pages.yml/badge.svg)](https://github.com/facebookresearch/xformers/actions/workflows/gh-pages.yml/badge.svg)\n-->\n[![CircleCI](https://circleci.com/gh/facebookresearch/xformers.svg?style=shield)](https://app.circleci.com/pipelines/github/facebookresearch/xformers/)\n[![Codecov](https://codecov.io/gh/facebookresearch/xformers/branch/main/graph/badge.svg?token=PKGKDR4JQM)](https://codecov.io/gh/facebookresearch/xformers)\n[![black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n<br/>\n[![PRs welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)\n<!--\n[![Downloads](https://pepy.tech/badge/xformers)](https://pepy.tech/project/xformers)\n-->\n--------------------------------------------------------------------------------\n\n## xFormers - Toolbox to Accelerate Research on Transformers\n\nxFormers is:\n- **Customizable building blocks**: Independent/customizable building blocks that can be used without boilerplate code. The components are domain-agnostic and xFormers is used by researchers in vision, NLP and more.\n- **Research first**: xFormers contains bleeding-edge components, that are not yet available in mainstream libraries like PyTorch.\n- **Built with efficiency in mind**: Because speed of iteration matters, components are as fast and memory-efficient as possible. xFormers contains its own CUDA kernels, but dispatches to other libraries when relevant.\n\n## Installing xFormers\n\n* **(RECOMMENDED, linux & win) Install latest stable with pip**: Requires [PyTorch 2.5.1](https://pytorch.org/get-started/locally/)\n\n```bash\n# [linux only] cuda 11.8 version\npip3 install -U xformers --index-url https://download.pytorch.org/whl/cu118\n# [linux only] cuda 12.1 version\npip3 install -U xformers --index-url https://download.pytorch.org/whl/cu121\n# [linux & win] cuda 12.4 version\npip3 install -U xformers --index-url https://download.pytorch.org/whl/cu124\n# [linux only] (EXPERIMENTAL) rocm 6.1 version\npip3 install -U xformers --index-url https://download.pytorch.org/whl/rocm6.1\n```\n\n* **Development binaries**:\n\n```bash\n# Same requirements as for the stable version above\npip install --pre -U xformers\n```\n\n* **Install from source**: If you want to use with another version of PyTorch for instance (including nightly-releases)\n\n```bash\n# (Optional) Makes the build much faster\npip install ninja\n# Set TORCH_CUDA_ARCH_LIST if running and building on different GPU types\npip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers\n# (this can take dozens of minutes)\n```\n\n\n## Benchmarks\n\n**Memory-efficient MHA**\n![Benchmarks for ViTS](./docs/plots/mha/mha_vit.png)\n*Setup: A100 on f16, measured total time for a forward+backward pass*\n\nNote that this is exact attention, not an approximation, just by calling [`xformers.ops.memory_efficient_attention`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)\n\n**More benchmarks**\n\nxFormers provides many components, and more benchmarks are available in [BENCHMARKS.md](BENCHMARKS.md).\n\n### (Optional) Testing the installation\n\nThis command will provide information on an xFormers installation, and what kernels are built/available:\n\n```python\npython -m xformers.info\n```\n\n## Using xFormers\n\n### Key Features\n\n1. Optimized building blocks, beyond PyTorch primitives\n   1. Memory-efficient exact attention - up to 10x faster\n   2. sparse attention\n   3. block-sparse attention\n   4. fused softmax\n   5. fused linear layer\n   6. fused layer norm\n   7. fused dropout(activation(x+bias))\n   8. fused SwiGLU\n\n### Install troubleshooting\n\n\n* NVCC and the current CUDA runtime match. Depending on your setup, you may be able to change the CUDA runtime with `module unload cuda; module load cuda/xx.x`, possibly also `nvcc`\n* the version of GCC that you're using matches the current NVCC capabilities\n* the `TORCH_CUDA_ARCH_LIST` env variable is set to the architectures that you want to support. A suggested setup (slow to build but comprehensive) is `export TORCH_CUDA_ARCH_LIST=\"6.0;6.1;6.2;7.0;7.2;7.5;8.0;8.6\"`\n* If the build from source OOMs, it's possible to reduce the parallelism of ninja with `MAX_JOBS` (eg `MAX_JOBS=2`)\n\n\n### License\n\nxFormers has a BSD-style license, as found in the [LICENSE](LICENSE) file.\nIt includes code from the [triton-lang/kernels](https://github.com/triton-lang/kernels) repo.\n\n## Citing xFormers\n\nIf you use xFormers in your publication, please cite it by using the following BibTeX entry.\n\n``` bibtex\n@Misc{xFormers2022,\n  author =       {Benjamin Lefaudeux and Francisco Massa and Diana Liskovich and Wenhan Xiong and Vittorio Caggiano and Sean Naren and Min Xu and Jieru Hu and Marta Tintore and Susan Zhang and Patrick Labatut and Daniel Haziza and Luca Wehrstedt and Jeremy Reizenstein and Grigory Sizov},\n  title =        {xFormers: A modular and hackable Transformer modelling library},\n  howpublished = {\\url{https://github.com/facebookresearch/xformers}},\n  year =         {2022}\n}\n```\n\n## Credits\n\nThe following repositories are used in xFormers, either in close to original form or as an inspiration:\n\n* [Sputnik](https://github.com/google-research/sputnik)\n* [GE-SpMM](https://github.com/hgyhungry/ge-spmm)\n* [Triton](https://github.com/openai/triton)\n* [LucidRain Reformer](https://github.com/lucidrains/reformer-pytorch)\n* [RevTorch](https://github.com/RobinBruegger/RevTorch)\n* [Nystromformer](https://github.com/mlpen/Nystromformer)\n* [FairScale](https://github.com/facebookresearch/fairscale/)\n* [Pytorch Image Models](https://github.com/rwightman/pytorch-image-models)\n* [CUTLASS](https://github.com/nvidia/cutlass)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n"
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.0341796875,
          "content": "ignore:\n  - \"xformers/_flash_attn\"\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements-benchmark.txt",
          "type": "blob",
          "size": 0.2783203125,
          "content": "# Get core deps\n-r requirements-test.txt\n\n# Example requirement, can be anything that pip knows\n# install with `pip install -r requirements.txt`, and make sure that CI does the same\ntqdm == 4.59.0\npandas == 2.2.2\nseaborn == 0.13.2\npytorch-lightning >= 1.3\ntorchmetrics>=0.7.0, <0.10.1\n"
        },
        {
          "name": "requirements-lra.txt",
          "type": "blob",
          "size": 0.13671875,
          "content": "# Get core deps\n-r requirements.txt\n\ntensorboard>=2.3.0\ntensorflow>=2.3.1\ntensorflow-datasets>=4.0.1\ntensorflow-text>=2.7.3\nsubmitit\nfvcore\n"
        },
        {
          "name": "requirements-test.txt",
          "type": "blob",
          "size": 0.474609375,
          "content": "# Get core deps.\n-r requirements.txt\n\n\n# Tools for static checking.\nblack == 22.3.0\nflake8 == 6.1.0\nflake8-copyright\nisort == 5.7.0\nmypy == 1.10.0\npyre-check == 0.9.16\npyre-extensions == 0.0.29\nclick == 8.0.4\nprotobuf==3.20.2\n\n# Tools for unit tests & coverage.\npytest == 7.2.0\npytest-cov == 2.10.0\npytest-mpi == 0.4\npytest-timeout == 1.4.2\npytest-random-order == 1.1.1\n\n# Dependency for Mixture of Experts\nfairscale >= 0.4.5\nscipy >= 1.7\n\n# Dependency for fused layers, optional\ncmake\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1552734375,
          "content": "# Example requirement, can be anything that pip knows\n# install with `pip install -r requirements.txt`, and make sure that CI does the same\ntorch >= 2.4\nnumpy\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.056640625,
          "content": "[flake8]\nmax-line-length = 120\nextend-ignore = E203, W503\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 26.2294921875,
          "content": "#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\n#\n# This source code is licensed under the BSD license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport datetime\nimport distutils.command.clean\nimport glob\nimport importlib.util\nimport json\nimport os\nimport platform\nimport re\nimport shlex\nimport shutil\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import List, Optional\n\nimport setuptools\nimport torch\nfrom torch.utils.cpp_extension import (\n    CUDA_HOME,\n    ROCM_HOME,\n    BuildExtension,\n    CppExtension,\n    CUDAExtension,\n)\n\nthis_dir = os.path.dirname(__file__)\npt_attn_compat_file_path = os.path.join(\n    this_dir, \"xformers\", \"ops\", \"fmha\", \"torch_attention_compat.py\"\n)\n\n# Define the module name\nmodule_name = \"torch_attention_compat\"\n\n# Load the module\nspec = importlib.util.spec_from_file_location(module_name, pt_attn_compat_file_path)\nassert spec is not None\nattn_compat_module = importlib.util.module_from_spec(spec)\nsys.modules[module_name] = attn_compat_module\nassert spec.loader is not None\nspec.loader.exec_module(attn_compat_module)\n\n\ndef get_extra_nvcc_flags_for_build_type(cuda_version: int) -> List[str]:\n    build_type = os.environ.get(\"XFORMERS_BUILD_TYPE\", \"RelWithDebInfo\").lower()\n    if build_type == \"relwithdebinfo\":\n        if cuda_version >= 1201 and cuda_version < 1202:\n            print(\n                \"Looks like we are using CUDA 12.1 which segfaults when provided with\"\n                \" the -generate-line-info flag. Disabling it.\"\n            )\n            return []\n        return [\"--generate-line-info\"]\n    elif build_type == \"release\":\n        return []\n    else:\n        raise ValueError(f\"Unknown build type: {build_type}\")\n\n\ndef fetch_requirements():\n    with open(\"requirements.txt\") as f:\n        reqs = f.read().strip().split(\"\\n\")\n    return reqs\n\n\ndef get_local_version_suffix() -> str:\n    if not (Path(__file__).parent / \".git\").is_dir():\n        # Most likely installing from a source distribution\n        return \"\"\n    date_suffix = datetime.datetime.now().strftime(\"%Y%m%d\")\n    git_hash = subprocess.check_output(\n        [\"git\", \"rev-parse\", \"--short\", \"HEAD\"], cwd=Path(__file__).parent\n    ).decode(\"ascii\")[:-1]\n    return f\"+{git_hash}.d{date_suffix}\"\n\n\ndef get_flash_version() -> str:\n    flash_dir = Path(__file__).parent / \"third_party\" / \"flash-attention\"\n    try:\n        return subprocess.check_output(\n            [\"git\", \"describe\", \"--tags\", \"--always\"],\n            cwd=flash_dir,\n        ).decode(\"ascii\")[:-1]\n    except subprocess.CalledProcessError:\n        version = flash_dir / \"version.txt\"\n        if version.is_file():\n            return version.read_text().strip()\n        return \"v?\"\n\n\ndef generate_version_py(version: str) -> str:\n    content = \"# noqa: C801\\n\"\n    content += f'__version__ = \"{version}\"\\n'\n    tag = os.getenv(\"GIT_TAG\")\n    if tag is not None:\n        content += f'git_tag = \"{tag}\"\\n'\n    return content\n\n\ndef symlink_package(name: str, path: Path, is_building_wheel: bool) -> None:\n    cwd = Path(__file__).resolve().parent\n    path_from = cwd / path\n    path_to = os.path.join(cwd, *name.split(\".\"))\n\n    try:\n        if os.path.islink(path_to):\n            os.unlink(path_to)\n        elif os.path.isdir(path_to):\n            shutil.rmtree(path_to)\n        else:\n            os.remove(path_to)\n    except FileNotFoundError:\n        pass\n    # OSError: [WinError 1314] A required privilege is not held by the client\n    # Windows requires special permission to symlink. Fallback to copy\n    # When building wheels for linux 3.7 and 3.8, symlinks are not included\n    # So we force a copy, see #611\n    use_symlink = os.name != \"nt\" and not is_building_wheel\n    if use_symlink:\n        os.symlink(src=path_from, dst=path_to)\n    else:\n        shutil.copytree(src=path_from, dst=path_to)\n\n\ndef get_cuda_version(cuda_dir) -> int:\n    nvcc_bin = \"nvcc\" if cuda_dir is None else cuda_dir + \"/bin/nvcc\"\n    raw_output = subprocess.check_output([nvcc_bin, \"-V\"], universal_newlines=True)\n    output = raw_output.split()\n    release_idx = output.index(\"release\") + 1\n    release = output[release_idx].split(\".\")\n    bare_metal_major = int(release[0])\n    bare_metal_minor = int(release[1][0])\n\n    assert bare_metal_minor < 100\n    return bare_metal_major * 100 + bare_metal_minor\n\n\ndef get_hip_version(rocm_dir) -> Optional[str]:\n    hipcc_bin = \"hipcc\" if rocm_dir is None else os.path.join(rocm_dir, \"bin\", \"hipcc\")\n    try:\n        raw_output = subprocess.check_output(\n            [hipcc_bin, \"--version\"], universal_newlines=True\n        )\n    except Exception as e:\n        print(\n            f\"hip installation not found: {e} ROCM_PATH={os.environ.get('ROCM_PATH')}\"\n        )\n        return None\n    for line in raw_output.split(\"\\n\"):\n        if \"HIP version\" in line:\n            return line.split()[-1]\n    return None\n\n\n######################################\n# FLASH-ATTENTION v2\n######################################\n# Supports `9.0`, `9.0+PTX`, `9.0a+PTX` etc...\nPARSE_CUDA_ARCH_RE = re.compile(\n    r\"(?P<major>[0-9]+)\\.(?P<minor>[0-9])(?P<suffix>[a-zA-Z]{0,1})(?P<ptx>\\+PTX){0,1}\"\n)\n\n\ndef get_flash_attention2_nvcc_archs_flags(cuda_version: int):\n    # XXX: Not supported on windows for cuda<12\n    # https://github.com/Dao-AILab/flash-attention/issues/345\n    if platform.system() != \"Linux\" and cuda_version < 1200:\n        return []\n    # Figure out default archs to target\n    DEFAULT_ARCHS_LIST = \"\"\n    if cuda_version >= 1108:\n        DEFAULT_ARCHS_LIST = \"8.0;8.6;9.0\"\n    elif cuda_version > 1100:\n        DEFAULT_ARCHS_LIST = \"8.0;8.6\"\n    elif cuda_version == 1100:\n        DEFAULT_ARCHS_LIST = \"8.0\"\n    else:\n        return []\n\n    if os.getenv(\"XFORMERS_DISABLE_FLASH_ATTN\", \"0\") != \"0\":\n        return []\n\n    archs_list = os.environ.get(\"TORCH_CUDA_ARCH_LIST\", DEFAULT_ARCHS_LIST)\n    nvcc_archs_flags = []\n    for arch in archs_list.replace(\" \", \";\").split(\";\"):\n        match = PARSE_CUDA_ARCH_RE.match(arch)\n        assert match is not None, f\"Invalid sm version: {arch}\"\n        num = 10 * int(match.group(\"major\")) + int(match.group(\"minor\"))\n        # Need at least Sm80\n        if num < 80:\n            continue\n        # Sm90 requires nvcc 11.8+\n        if num >= 90 and cuda_version < 1108:\n            continue\n        suffix = match.group(\"suffix\")\n        nvcc_archs_flags.append(\n            f\"-gencode=arch=compute_{num}{suffix},code=sm_{num}{suffix}\"\n        )\n        if match.group(\"ptx\") is not None:\n            nvcc_archs_flags.append(\n                f\"-gencode=arch=compute_{num}{suffix},code=compute_{num}{suffix}\"\n            )\n\n    return nvcc_archs_flags\n\n\ndef get_flash_attention2_extensions(cuda_version: int, extra_compile_args):\n    nvcc_archs_flags = get_flash_attention2_nvcc_archs_flags(cuda_version)\n\n    if not nvcc_archs_flags:\n        return []\n\n    flash_root = os.path.join(this_dir, \"third_party\", \"flash-attention\")\n    cutlass_inc = os.path.join(flash_root, \"csrc\", \"cutlass\", \"include\")\n    if not os.path.exists(flash_root) or not os.path.exists(cutlass_inc):\n        raise RuntimeError(\n            \"flashattention submodule not found. Did you forget \"\n            \"to run `git submodule update --init --recursive` ?\"\n        )\n\n    sources = [\"csrc/flash_attn/flash_api.cpp\"]\n    for f in glob.glob(os.path.join(flash_root, \"csrc\", \"flash_attn\", \"src\", \"*.cu\")):\n        if \"hdim224\" in Path(f).name:\n            continue\n        sources.append(str(Path(f).relative_to(flash_root)))\n    common_extra_compile_args = [\"-DFLASHATTENTION_DISABLE_ALIBI\"]\n    return [\n        CUDAExtension(\n            name=\"xformers._C_flashattention\",\n            sources=[os.path.join(flash_root, path) for path in sources],\n            extra_compile_args={\n                \"cxx\": extra_compile_args.get(\"cxx\", []) + common_extra_compile_args,\n                \"nvcc\": extra_compile_args.get(\"nvcc\", [])\n                + [\n                    \"-O3\",\n                    \"-std=c++17\",\n                    \"-U__CUDA_NO_HALF_OPERATORS__\",\n                    \"-U__CUDA_NO_HALF_CONVERSIONS__\",\n                    \"-U__CUDA_NO_HALF2_OPERATORS__\",\n                    \"-U__CUDA_NO_BFLOAT16_CONVERSIONS__\",\n                    \"--expt-relaxed-constexpr\",\n                    \"--expt-extended-lambda\",\n                    \"--use_fast_math\",\n                    \"--ptxas-options=-v\",\n                ]\n                + nvcc_archs_flags\n                + common_extra_compile_args\n                + get_extra_nvcc_flags_for_build_type(cuda_version),\n            },\n            include_dirs=[\n                p.absolute()\n                for p in [\n                    Path(flash_root) / \"csrc\" / \"flash_attn\",\n                    Path(flash_root) / \"csrc\" / \"flash_attn\" / \"src\",\n                    Path(flash_root) / \"csrc\" / \"cutlass\" / \"include\",\n                ]\n            ],\n        )\n    ]\n\n\n######################################\n# FLASH-ATTENTION v3\n######################################\ndef get_flash_attention3_nvcc_archs_flags(cuda_version: int):\n    if os.getenv(\"XFORMERS_DISABLE_FLASH_ATTN\", \"0\") != \"0\":\n        return []\n    if platform.system() != \"Linux\" or cuda_version < 1203:\n        return []\n    archs_list = os.environ.get(\"TORCH_CUDA_ARCH_LIST\")\n    if archs_list is None:\n        if torch.cuda.get_device_capability(\"cuda\") != (9, 0):\n            return []\n        archs_list = \"9.0a\"\n    nvcc_archs_flags = []\n    for arch in archs_list.replace(\" \", \";\").split(\";\"):\n        match = PARSE_CUDA_ARCH_RE.match(arch)\n        assert match is not None, f\"Invalid sm version: {arch}\"\n        num = 10 * int(match.group(\"major\")) + int(match.group(\"minor\"))\n        if num != 90:  # only support Sm90\n            continue\n        suffix = match.group(\"suffix\")\n        nvcc_archs_flags.append(\n            f\"-gencode=arch=compute_{num}{suffix},code=sm_{num}{suffix}\"\n        )\n        if match.group(\"ptx\") is not None:\n            nvcc_archs_flags.append(\n                f\"-gencode=arch=compute_{num}{suffix},code=compute_{num}{suffix}\"\n            )\n    return nvcc_archs_flags\n\n\ndef get_flash_attention3_extensions(cuda_version: int, extra_compile_args):\n    nvcc_archs_flags = get_flash_attention3_nvcc_archs_flags(cuda_version)\n\n    if not nvcc_archs_flags:\n        return []\n\n    flash_root = os.path.join(this_dir, \"third_party\", \"flash-attention\")\n    cutlass_inc = os.path.join(flash_root, \"csrc\", \"cutlass\", \"include\")\n    if not os.path.exists(flash_root) or not os.path.exists(cutlass_inc):\n        raise RuntimeError(\n            \"flashattention submodule not found. Did you forget \"\n            \"to run `git submodule update --init --recursive` ?\"\n        )\n\n    sources = [\n        str(Path(f).relative_to(flash_root))\n        for f in glob.glob(os.path.join(flash_root, \"hopper\", \"*.cu\"))\n        + glob.glob(os.path.join(flash_root, \"hopper\", \"*.cpp\"))\n    ]\n    sources = [s for s in sources if \"flash_bwd_hdim256_fp16_sm90.cu\" not in s]\n    return [\n        CUDAExtension(\n            name=\"xformers._C_flashattention3\",\n            sources=[os.path.join(flash_root, path) for path in sources],\n            extra_compile_args={\n                \"cxx\": extra_compile_args.get(\"cxx\", []),\n                \"nvcc\": extra_compile_args.get(\"nvcc\", [])\n                + [\n                    \"-O3\",\n                    # \"-O0\",\n                    \"-std=c++17\",\n                    \"-U__CUDA_NO_HALF_OPERATORS__\",\n                    \"-U__CUDA_NO_HALF_CONVERSIONS__\",\n                    \"-U__CUDA_NO_BFLOAT16_OPERATORS__\",\n                    \"-U__CUDA_NO_BFLOAT16_CONVERSIONS__\",\n                    \"-U__CUDA_NO_BFLOAT162_OPERATORS__\",\n                    \"-U__CUDA_NO_BFLOAT162_CONVERSIONS__\",\n                    \"--expt-relaxed-constexpr\",\n                    \"--expt-extended-lambda\",\n                    \"--use_fast_math\",\n                    # \"--ptxas-options=-v\",  # printing out number of registers\n                    \"--ptxas-options=--verbose,--register-usage-level=10,--warn-on-local-memory-usage\",\n                    # \"-lineinfo\", # xformers: save binary size\n                    \"-DCUTLASS_DEBUG_TRACE_LEVEL=0\",  # Can toggle for debugging\n                    \"-DNDEBUG\",  # Important, otherwise performance is severely impacted\n                    \"-DQBLKSIZE=128\",\n                    \"-DKBLKSIZE=128\",\n                    \"-DCTA256\",\n                    \"-DDQINRMEM\",\n                ]\n                + nvcc_archs_flags\n                + get_extra_nvcc_flags_for_build_type(cuda_version),\n            },\n            include_dirs=[\n                p.absolute()\n                for p in [\n                    Path(flash_root) / \"csrc\" / \"cutlass\" / \"include\",\n                    Path(flash_root) / \"hopper\",\n                ]\n            ],\n        )\n    ]\n\n\ndef rename_cpp_cu(cpp_files):\n    for entry in cpp_files:\n        shutil.copy(entry, os.path.splitext(entry)[0] + \".cu\")\n\n\ndef get_extensions():\n    extensions_dir = os.path.join(\"xformers\", \"csrc\")\n\n    sources = glob.glob(os.path.join(extensions_dir, \"**\", \"*.cpp\"), recursive=True)\n    source_cuda = glob.glob(os.path.join(extensions_dir, \"**\", \"*.cu\"), recursive=True)\n    fmha_source_cuda = glob.glob(\n        os.path.join(extensions_dir, \"**\", \"fmha\", \"**\", \"*.cu\"), recursive=True\n    )\n    exclude_files = [\"small_k.cu\", \"decoder.cu\", \"attention_cutlass_rand_uniform.cu\"]\n    fmha_source_cuda = [\n        c\n        for c in fmha_source_cuda\n        if not any(exclude_file in c for exclude_file in exclude_files)\n    ]\n\n    source_hip = glob.glob(\n        os.path.join(extensions_dir, \"attention\", \"hip_*\", \"**\", \"*.cpp\"),\n        recursive=True,\n    )\n\n    source_hip_generated = glob.glob(\n        os.path.join(extensions_dir, \"attention\", \"hip_*\", \"**\", \"*.cu\"),\n        recursive=True,\n    )\n    # avoid the temporary .cu files generated under xformers/csrc/attention/hip_fmha\n    source_cuda = list(set(source_cuda) - set(source_hip_generated))\n    sources = list(set(sources) - set(source_hip))\n\n    sputnik_dir = os.path.join(this_dir, \"third_party\", \"sputnik\")\n\n    xformers_pt_cutlass_attn = os.getenv(\"XFORMERS_PT_CUTLASS_ATTN\")\n    # By default, we try to link to torch internal CUTLASS attention implementation\n    # and silently switch to local CUTLASS attention build if no compatibility\n    # If we force 'torch CUTLASS switch' then setup will fail when no compatibility\n    if (\n        xformers_pt_cutlass_attn is None or xformers_pt_cutlass_attn == \"1\"\n    ) and attn_compat_module.is_pt_cutlass_compatible(\n        force=xformers_pt_cutlass_attn == \"1\"\n    ):\n        source_cuda = list(set(source_cuda) - set(fmha_source_cuda))\n\n    cutlass_dir = os.path.join(this_dir, \"third_party\", \"cutlass\", \"include\")\n    cutlass_util_dir = os.path.join(\n        this_dir, \"third_party\", \"cutlass\", \"tools\", \"util\", \"include\"\n    )\n    cutlass_examples_dir = os.path.join(this_dir, \"third_party\", \"cutlass\", \"examples\")\n    if not os.path.exists(cutlass_dir):\n        raise RuntimeError(\n            f\"CUTLASS submodule not found at {cutlass_dir}. \"\n            \"Did you forget to run \"\n            \"`git submodule update --init --recursive` ?\"\n        )\n\n    extension = CppExtension\n\n    define_macros = []\n\n    extra_compile_args = {\"cxx\": [\"-O3\", \"-std=c++17\"]}\n    if sys.platform == \"win32\":\n        define_macros += [(\"xformers_EXPORTS\", None)]\n        extra_compile_args[\"cxx\"].extend(\n            [\"/MP\", \"/Zc:lambda\", \"/Zc:preprocessor\", \"/Zc:__cplusplus\"]\n        )\n    elif \"OpenMP not found\" not in torch.__config__.parallel_info():\n        extra_compile_args[\"cxx\"].append(\"-fopenmp\")\n\n    include_dirs = [extensions_dir]\n    ext_modules = []\n    cuda_version = None\n    hip_version = None\n    flash_version = \"0.0.0\"\n    use_pt_flash = False\n\n    if (\n        (\n            torch.cuda.is_available()\n            and (CUDA_HOME is not None)\n            and (torch.version.cuda is not None)\n        )\n        or os.getenv(\"FORCE_CUDA\", \"0\") == \"1\"\n        or os.getenv(\"TORCH_CUDA_ARCH_LIST\", \"\") != \"\"\n    ):\n        cuda_version = get_cuda_version(CUDA_HOME)\n        extension = CUDAExtension\n        sources += source_cuda\n        include_dirs += [\n            sputnik_dir,\n            cutlass_dir,\n            cutlass_util_dir,\n            cutlass_examples_dir,\n        ]\n        nvcc_flags = [\n            \"-DHAS_PYTORCH\",\n            \"--use_fast_math\",\n            \"-U__CUDA_NO_HALF_OPERATORS__\",\n            \"-U__CUDA_NO_HALF_CONVERSIONS__\",\n            \"--extended-lambda\",\n            \"-D_ENABLE_EXTENDED_ALIGNED_STORAGE\",\n            \"-std=c++17\",\n        ] + get_extra_nvcc_flags_for_build_type(cuda_version)\n        if os.getenv(\"XFORMERS_ENABLE_DEBUG_ASSERTIONS\", \"0\") != \"1\":\n            nvcc_flags.append(\"-DNDEBUG\")\n        nvcc_flags += shlex.split(os.getenv(\"NVCC_FLAGS\", \"\"))\n        if cuda_version >= 1102:\n            nvcc_flags += [\n                \"--threads\",\n                \"4\",\n                \"--ptxas-options=-v\",\n            ]\n        if sys.platform == \"win32\":\n            nvcc_flags += [\n                \"-Xcompiler\",\n                \"/Zc:lambda\",\n                \"-Xcompiler\",\n                \"/Zc:preprocessor\",\n                \"-Xcompiler\",\n                \"/Zc:__cplusplus\",\n            ]\n        extra_compile_args[\"nvcc\"] = nvcc_flags\n\n        xformers_pt_flash_attn = os.getenv(\"XFORMERS_PT_FLASH_ATTN\")\n\n        # check if the current device supports flash_attention\n        flash_version = get_flash_version()\n        nvcc_archs_flags = get_flash_attention2_nvcc_archs_flags(cuda_version)\n        if not nvcc_archs_flags:\n            if xformers_pt_flash_attn == \"1\":\n                raise ValueError(\n                    \"Current Torch Flash-Attention is not available on this device\"\n                )\n        else:\n            # By default, we try to link to torch internal flash attention implementation\n            # and silently switch to local flash attention build if no compatibility\n            # If we force 'torch FA switch' then setup will fail when no compatibility\n            if (\n                xformers_pt_flash_attn is None or xformers_pt_flash_attn == \"1\"\n            ) and attn_compat_module.is_pt_flash_compatible(\n                force=xformers_pt_flash_attn == \"1\"\n            ):\n                use_pt_flash = True\n            else:\n                ext_modules += get_flash_attention2_extensions(\n                    cuda_version=cuda_version, extra_compile_args=extra_compile_args\n                )\n        ext_modules += get_flash_attention3_extensions(cuda_version, extra_compile_args)\n\n        # NOTE: This should not be applied to Flash-Attention\n        # see https://github.com/Dao-AILab/flash-attention/issues/359\n        extra_compile_args[\"nvcc\"] += [\n            # Workaround for a regression with nvcc > 11.6\n            # See https://github.com/facebookresearch/xformers/issues/712\n            \"--ptxas-options=-O2\",\n            \"--ptxas-options=-allow-expensive-optimizations=true\",\n        ]\n    elif torch.version.hip and (\n        torch.cuda.is_available() or os.getenv(\"HIP_ARCHITECTURES\", \"\") != \"\"\n    ):\n        disable_hd256_hip_fmha = os.getenv(\"DISABLE_HD256_HIP_FMHA\", \"0\")\n        if disable_hd256_hip_fmha == \"1\":\n            source_hip_maxk_256 = []\n            for ff in source_hip:\n                if ff.endswith(\"maxk_256.cpp\"):\n                    source_hip_maxk_256 += [ff]\n            source_hip = list(set(source_hip) - set(source_hip_maxk_256))\n\n        rename_cpp_cu(source_hip)\n        hip_version = get_hip_version(ROCM_HOME)\n\n        source_hip_cu = []\n        for ff in source_hip:\n            source_hip_cu += [ff.replace(\".cpp\", \".cu\")]\n\n        extension = CUDAExtension\n        sources += source_hip_cu\n        include_dirs += [\n            Path(this_dir) / \"xformers\" / \"csrc\" / \"attention\" / \"hip_fmha\",\n            Path(this_dir) / \"xformers\" / \"csrc\" / \"attention\" / \"hip_decoder\",\n        ]\n\n        include_dirs += [\n            Path(this_dir) / \"third_party\" / \"composable_kernel_tiled\" / \"include\"\n        ]\n\n        generator_flag = []\n        if disable_hd256_hip_fmha == \"1\":\n            generator_flag += [\"-DFMHA_SUPPORT_MAX_HEADDIM_128=1\"]\n\n        cc_flag = [\"-DBUILD_PYTHON_PACKAGE\"]\n        use_rtn_bf16_convert = os.getenv(\"ENABLE_HIP_FMHA_RTN_BF16_CONVERT\", \"0\")\n        if use_rtn_bf16_convert == \"1\":\n            cc_flag += [\"-DCK_TILE_FLOAT_TO_BFLOAT16_DEFAULT=3\"]\n\n        arch_list = os.getenv(\"HIP_ARCHITECTURES\", \"native\").split()\n\n        offload_compress_flag = []\n        if hip_version >= \"6.2.\":\n            offload_compress_flag = [\"--offload-compress\"]\n\n        extra_compile_args = {\n            \"cxx\": [\"-O3\", \"-std=c++17\"] + generator_flag,\n            \"nvcc\": [\n                \"-O3\",\n                \"-std=c++17\",\n                *[f\"--offload-arch={arch}\" for arch in arch_list],\n                *offload_compress_flag,\n                \"-U__CUDA_NO_HALF_OPERATORS__\",\n                \"-U__CUDA_NO_HALF_CONVERSIONS__\",\n                \"-DCK_TILE_FMHA_FWD_FAST_EXP2=1\",\n                \"-fgpu-flush-denormals-to-zero\",\n                \"-Werror\",\n                \"-Woverloaded-virtual\",\n                \"-mllvm\",\n                \"-enable-post-misched=0\",\n                \"-mllvm\",\n                \"-amdgpu-early-inline-all=true\",\n                \"-mllvm\",\n                \"-amdgpu-function-calls=false\",\n                \"-mllvm\",\n                \"-greedy-reverse-local-assignment=1\",\n            ]\n            + generator_flag\n            + cc_flag,\n        }\n\n    ext_modules.append(\n        extension(\n            \"xformers._C\",\n            sorted(sources),\n            include_dirs=[os.path.abspath(p) for p in include_dirs],\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    )\n\n    return ext_modules, {\n        \"version\": {\n            \"cuda\": cuda_version,\n            \"hip\": hip_version,\n            \"torch\": torch.__version__,\n            \"python\": platform.python_version(),\n            \"flash\": flash_version,\n            \"use_torch_flash\": use_pt_flash,\n        },\n        \"env\": {\n            k: os.environ.get(k)\n            for k in [\n                \"TORCH_CUDA_ARCH_LIST\",\n                \"PYTORCH_ROCM_ARCH\",\n                \"XFORMERS_BUILD_TYPE\",\n                \"XFORMERS_ENABLE_DEBUG_ASSERTIONS\",\n                \"NVCC_FLAGS\",\n                \"XFORMERS_PACKAGE_FROM\",\n            ]\n        },\n    }\n\n\nclass clean(distutils.command.clean.clean):  # type: ignore\n    def run(self):\n        if os.path.exists(\".gitignore\"):\n            with open(\".gitignore\", \"r\") as f:\n                ignores = f.read()\n                for wildcard in filter(None, ignores.split(\"\\n\")):\n                    for filename in glob.glob(wildcard):\n                        try:\n                            os.remove(filename)\n                        except OSError:\n                            shutil.rmtree(filename, ignore_errors=True)\n\n        # It's an old-style class in Python 2.7...\n        distutils.command.clean.clean.run(self)\n\n\nclass BuildExtensionWithExtraFiles(BuildExtension):\n    def __init__(self, *args, **kwargs) -> None:\n        self.xformers_build_metadata = kwargs.pop(\"extra_files\")\n        self.pkg_name = \"xformers\"\n        super().__init__(*args, **kwargs)\n\n    def build_extensions(self) -> None:\n        super().build_extensions()\n        for filename, content in self.xformers_build_metadata.items():\n            with open(\n                os.path.join(self.build_lib, self.pkg_name, filename), \"w+\"\n            ) as fp:\n                fp.write(content)\n\n    def copy_extensions_to_source(self) -> None:\n        \"\"\"\n        Used for `pip install -e .`\n        Copies everything we built back into the source repo\n        \"\"\"\n        build_py = self.get_finalized_command(\"build_py\")\n        package_dir = build_py.get_package_dir(self.pkg_name)\n\n        for filename in self.xformers_build_metadata.keys():\n            inplace_file = os.path.join(package_dir, filename)\n            regular_file = os.path.join(self.build_lib, self.pkg_name, filename)\n            self.copy_file(regular_file, inplace_file, level=self.verbose)\n        super().copy_extensions_to_source()\n\n\nif __name__ == \"__main__\":\n    if os.getenv(\"BUILD_VERSION\"):  # In CI\n        version = os.getenv(\"BUILD_VERSION\", \"0.0.0\")\n    else:\n        version_txt = os.path.join(this_dir, \"version.txt\")\n        with open(version_txt) as f:\n            version = f.readline().strip()\n        version += get_local_version_suffix()\n\n    is_building_wheel = \"bdist_wheel\" in sys.argv\n    # Embed a fixed version of flash_attn\n    # NOTE: The correct way to do this would be to use the `package_dir`\n    # parameter in `setuptools.setup`, but this does not work when\n    # developing in editable mode\n    # See: https://github.com/pypa/pip/issues/3160 (closed, but not fixed)\n    symlink_package(\n        \"xformers._flash_attn\",\n        Path(\"third_party\") / \"flash-attention\" / \"flash_attn\",\n        is_building_wheel,\n    )\n    extensions, extensions_metadata = get_extensions()\n    setuptools.setup(\n        name=\"xformers\",\n        description=\"XFormers: A collection of composable Transformer building blocks.\",\n        version=version,\n        install_requires=fetch_requirements(),\n        packages=setuptools.find_packages(exclude=(\"tests*\", \"benchmarks*\")),\n        ext_modules=extensions,\n        cmdclass={\n            \"build_ext\": BuildExtensionWithExtraFiles.with_options(\n                no_python_abi_suffix=True,\n                extra_files={\n                    \"cpp_lib.json\": json.dumps(extensions_metadata),\n                    \"version.py\": generate_version_py(version),\n                },\n            ),\n            \"clean\": clean,\n        },\n        url=\"https://facebookresearch.github.io/xformers/\",\n        python_requires=\">=3.9\",\n        author=\"Facebook AI Research\",\n        author_email=\"oncall+xformers@xmail.facebook.com\",\n        long_description=\"XFormers: A collection of composable Transformer building blocks.\"\n        + \"XFormers aims at being able to reproduce most architectures in the Transformer-family SOTA,\"\n        + \"defined as compatible and combined building blocks as opposed to monolithic models\",\n        long_description_content_type=\"text/markdown\",\n        classifiers=[\n            \"Programming Language :: Python :: 3.9\",\n            \"Programming Language :: Python :: 3.10\",\n            \"Programming Language :: Python :: 3.11\",\n            \"Programming Language :: Python :: 3.12\",\n            \"License :: OSI Approved :: BSD License\",\n            \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n            \"Operating System :: OS Independent\",\n        ],\n        zip_safe=False,\n    )\n"
        },
        {
          "name": "stubs",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "version.txt",
          "type": "blob",
          "size": 0.0068359375,
          "content": "0.0.30\n"
        },
        {
          "name": "xformers",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}