{
  "metadata": {
    "timestamp": 1736560787989,
    "page": 478,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "jianchang512/ChatTTS-ui",
      "stars": 6507,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".env",
          "type": "blob",
          "size": 0.06640625,
          "content": "WEB_ADDRESS=127.0.0.1:9966\ncompile=false\ndevice=default\nmerge_size=6"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1904296875,
          "content": "*.log\n*.srt\n.idea\n\nmodels/*\ndev\nvenv\ndist\nsource\nbuild\n__pycache__\n*.spec\n*.ui\n*.bak\n*.aac\n*.pt\n*.wav\npack.bat\ngitcmd.bat\nlogs\npoetry.lock\ndocs\nstatic/wavs/*\nexamples\nffmpeg/ffmpeg.exe\nasset/*.pt"
        },
        {
          "name": "ChatTTS",
          "type": "tree",
          "content": null
        },
        {
          "name": "Dockerfile.cpu",
          "type": "blob",
          "size": 0.2177734375,
          "content": "FROM pytorch/torchserve:0.11.0-cpu as builder\r\n\r\nUSER root\r\n\r\nRUN apt-get update && apt-get install -y ffmpeg\r\n\r\nWORKDIR /app\r\n\r\nCOPY . ./\r\n\r\nRUN pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\r\n"
        },
        {
          "name": "Dockerfile.gpu",
          "type": "blob",
          "size": 0.20703125,
          "content": "FROM pytorch/torchserve:0.11.0-gpu as builder\n\nUSER root\n\nRUN apt-get update && apt-get install -y ffmpeg\n\nWORKDIR /app\n\nCOPY . ./\n\nRUN pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 17.1181640625,
          "content": "# Attribution-NonCommercial-NoDerivatives 4.0 International\n\n> *Creative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.*\n>\n> ### Using Creative Commons Public Licenses\n>\n> Creative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n>\n> * __Considerations for licensors:__ Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC-licensed material, or material used under an exception or limitation to copyright. [More considerations for licensors](http://wiki.creativecommons.org/Considerations_for_licensors_and_licensees#Considerations_for_licensors).\n>\n> * __Considerations for the public:__ By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensor’s permission is not necessary for any reason–for example, because of any applicable exception or limitation to copyright–then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. [More considerations for the public](http://wiki.creativecommons.org/Considerations_for_licensors_and_licensees#Considerations_for_licensees).\n\n## Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License\n\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\n### Section 1 – Definitions.\n\na. __Adapted Material__ means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\n\nb. __Copyright and Similar Rights__ means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\n\ne. __Effective Technological Measures__ means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\n\nf. __Exceptions and Limitations__ means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\n\nh. __Licensed Material__ means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\n\ni. __Licensed Rights__ means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\n\nh. __Licensor__ means the individual(s) or entity(ies) granting rights under this Public License.\n\ni. __NonCommercial__ means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\n\nj. __Share__ means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\n\nk. __Sui Generis Database Rights__ means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\n\nl. __You__ means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n### Section 2 – Scope.\n\na. ___License grant.___\n\n   1. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\n        A. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\n\n        B. produce and reproduce, but not Share, Adapted Material for NonCommercial purposes only.\n\n   2. __Exceptions and Limitations.__ For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\n\n   3. __Term.__ The term of this Public License is specified in Section 6(a).\n\n   4. __Media and formats; technical modifications allowed.__ The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\n\n   5. __Downstream recipients.__\n\n        A. __Offer from the Licensor – Licensed Material.__ Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\n\n        B. __No downstream restrictions.__ You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\n   6. __No endorsement.__ Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nb. ___Other rights.___\n\n   1. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\n\n   2. Patent and trademark rights are not licensed under this Public License.\n\n   3. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n### Section 3 – License Conditions.\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\na. ___Attribution.___\n\n   1. If You Share the Licensed Material, You must:\n\n      A. retain the following if it is supplied by the Licensor with the Licensed Material:\n\n         i. identification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\n\n         ii. a copyright notice;\n\n         iii. a notice that refers to this Public License;\n\n         iv. a notice that refers to the disclaimer of warranties;\n\n         v. a URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\n      B. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\n\n      C. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n \n        For the avoidance of doubt, You do not have permission under this Public License to Share Adapted Material.\n\n   2. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\n\n   3. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\n### Section 4 – Sui Generis Database Rights.\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\na. for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only and provided You do not Share Adapted Material;\n\nb. if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\n\nc. You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n### Section 5 – Disclaimer of Warranties and Limitation of Liability.\n\na. __Unless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.__\n\nb. __To the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.__\n\nc. The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n### Section 6 – Term and Termination.\n\na. This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\n\nb. Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\n   1. automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\n\n   2. upon express reinstatement by the Licensor.\n\n   For the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\n\nc. For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\n\nd. Sections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n### Section 7 – Other Terms and Conditions.\n\na. The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\n\nb. Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n### Section 8 – Interpretation.\n\na. For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\n\nb. To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\n\nc. No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\n\nd. Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n> Creative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at [creativecommons.org/policies](http://creativecommons.org/policies), Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\n>\n> Creative Commons may be contacted at [creativecommons.org](http://creativecommons.org).\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.720703125,
          "content": "\n[English README](README_EN.md) | [打赏项目](https://github.com/jianchang512/ChatTTS-ui/issues/122) | [Discord Discussion Group](https://discord.gg/y9gUweVCCJ)\n\n\n# ChatTTS webUI & API \n\n一个简单的本地网页界面，在网页使用 ChatTTS 将文字合成为语音，支持中英文、数字混杂，并提供API接口.\n\n\n原 [ChatTTS](https://github.com/2noise/chattts) 项目. 0.96版起，源码部署必须先安装ffmpeg ,之前的音色文件csv和pt已不可用，请填写音色值重新生成.[获取音色](?tab=readme-ov-file#音色获取)\n\n\n> **[赞助商]**\n> \n> [![](https://github.com/user-attachments/assets/5348c86e-2d5f-44c7-bc1b-3cc5f077e710)](https://gpt302.saaslink.net/teRK8Y)\n>  [302.AI](https://gpt302.saaslink.net/teRK8Y)是一个按需付费的一站式AI应用平台，开放平台，开源生态, [302.AI开源地址](https://github.com/302ai)\n> \n> 集合了最新最全的AI模型和品牌/按需付费零月费/管理和使用分离/所有AI能力均提供API/每周推出2-3个新应用\n\n**界面预览**\n\n![image](https://github.com/jianchang512/ChatTTS-ui/assets/3378335/669876cf-5061-4d7d-86c5-3333d0882ee8)\n\n\n\n\n\n\n文字数字符号 控制符混杂效果\n\nhttps://github.com/jianchang512/ChatTTS-ui/assets/3378335/e2a08ea0-32af-4a30-8880-3a91f6cbea55\n\n\n## Windows预打包版\n\n1. 从 [Releases](https://github.com/jianchang512/chatTTS-ui/releases)中下载压缩包，解压后双击 app.exe 即可使用\n2. 某些安全软件可能报毒，请退出或使用源码部署\n3. 英伟达显卡大于4G显存，并安装了CUDA11.8+后，将启用GPU加速\n\n## 手动下载模型\n\n第一次将从huggingface.co或github下载模型到asset目录下，如果网络不稳，可能下载失败，若是失败，请单独下载\n\n下载后解压后，会看到asset文件夹，该文件夹内有多个pt文件，将所有pt文件复制到asset目录下，然后重启软件\n\nGitHub下载地址: https://github.com/jianchang512/ChatTTS-ui/releases/download/v1.0/all-models.7z\n\n百度网盘下载地址: https://pan.baidu.com/s/1yGDZM9YNN7kW9e7SFo8lLw?pwd=ct5x\n\n\n\n## Linux 下容器部署\n\n### 安装\n\n1. 拉取项目仓库\n\n   在任意路径下克隆项目，例如：\n\n   ```bash\n   git clone https://github.com/jianchang512/ChatTTS-ui.git chat-tts-ui\n   ```\n\n2. 启动 Runner\n\n   进入到项目目录：\n\n   ```bash\n   cd chat-tts-ui\n   ```\n\n   启动容器并查看初始化日志：\n\n   ```bash\n   gpu版本\n   docker compose -f docker-compose.gpu.yaml up -d \n\n   cpu版本    \n   docker compose -f docker-compose.cpu.yaml up -d\n\n   docker compose logs -f --no-log-prefix\n\n3. 访问 ChatTTS WebUI\n\n   `启动:['0.0.0.0', '9966']`，也即，访问部署设备的 `IP:9966` 即可，例如：\n\n   - 本机：`http://127.0.0.1:9966`\n   - 服务器: `http://192.168.1.100:9966`\n\n### 更新\n\n1. Get the latest code from the main branch:\n\n   ```bash\n   git checkout main\n   git pull origin main\n   ```\n\n2. Go to the next step and update to the latest image:\n\n   ```bash\n   docker compose down\n\n   gpu版本\n   docker compose -f docker-compose.gpu.yaml up -d --build\n\n   cpu版本\n   docker compose -f docker-compose.cpu.yaml up -d --build\n   \n   docker compose logs -f --no-log-prefix\n   ```\n\n## Linux 下源码部署\n\n1. 配置好 python3.9-3.11环境，安装 ffmpeg。 `yum install ffmpeg` 或 `apt-get install ffmpeg`等\n2. 创建空目录 `/data/chattts` 执行命令 `cd /data/chattts &&  git clone https://github.com/jianchang512/chatTTS-ui .`\n3. 创建虚拟环境 `python3 -m venv venv`\n4. 激活虚拟环境 `source ./venv/bin/activate`\n5. 安装依赖 `pip3 install -r requirements.txt`\n6. 如果不需要CUDA加速，执行 \n\t\n\t`pip3 install torch==2.2.0 torchaudio==2.2.0`\n\n\t如果需要CUDA加速，执行 \n\t\n\t```\n\tpip install torch==2.2.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118\n\n\tpip install nvidia-cublas-cu11 nvidia-cudnn-cu11\n\t\t\n\t```\n\t\n\t另需安装 CUDA11.8+ ToolKit，请自行搜索安装方法 或参考 https://juejin.cn/post/7318704408727519270\n\n   \t除CUDA外，也可以使用AMD GPU进行加速，这需要安装ROCm和PyTorch_ROCm版本。AMG GPU借助ROCm，在PyTorch开箱即用，无需额外修改代码。\n   \t1. 请参考https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html 来安装AMD GPU Driver及ROCm.\n\t1. 再通过https://pytorch.org/ 安装PyTorch_ROCm版本。\n\n\n\t`pip3 install torch==2.2.0  torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/rocm6.0`\n\t\n    安装完成后，可以通过rocm-smi命令来查看系统中的AMD GPU。也可以用以下Torch代码(query_gpu.py)来查询当前AMD GPU Device.\n\t\n\t```\n\timport torch\n\t\n\tprint(torch.__version__)\n\t\n\tif torch.cuda.is_available():\n\t    device = torch.device(\"cuda\")          # a CUDA device object\n\t    print('Using GPU:', torch.cuda.get_device_name(0))\n\telse:\n\t    device = torch.device(\"cpu\")\n\t    print('Using CPU')\n\t\n\ttorch.cuda.get_device_properties(0)\n\n\t```\n\n \t使用以上代码，以AMD Radeon Pro W7900为例，查询设备如下。\n\n \t```\n\t\n \t$ python ~/query_gpu.py\n\t\n\t2.4.0.dev20240401+rocm6.0\n\t\n \tUsing GPU: AMD Radeon PRO W7900\n\t\n \t```\n\n\n \n7. 执行 `python3 app.py` 启动，将自动打开浏览器窗口，默认地址 `http://127.0.0.1:9966` (注意：默认从 modelscope 魔塔下载模型，不可使用代理下载，请关闭代理)\n\n\n## MacOS 下源码部署\n\n1. 配置好 python3.9-3.11 环境,安装git ，执行命令  `brew install libsndfile git python@3.10`\n   继续执行\n\n    ```\n\tbrew install ffmpeg\n\t\n    export PATH=\"/usr/local/opt/python@3.10/bin:$PATH\"\n\t\n    source ~/.bash_profile \n\t\n\tsource ~/.zshrc\n\t\n    ```\n\t\n2. 创建空目录 `/data/chattts` 执行命令 `cd /data/chattts &&  git clone https://github.com/jianchang512/chatTTS-ui .`\n3. 创建虚拟环境 `python3 -m venv venv`\n4. 激活虚拟环境 `source ./venv/bin/activate`\n5. 安装依赖 `pip3 install -r requirements.txt`\n6. 安装torch `pip3 install torch==2.2.0 torchaudio==2.2.0`\n7. 执行 `python3 app.py` 启动，将自动打开浏览器窗口，默认地址 `http://127.0.0.1:9966`  (注意：默认从 modelscope 魔塔下载模型，不可使用代理下载，请关闭代理)\n\n\n## Windows源码部署\n\n1. 下载python3.9-3.11，安装时注意选中`Add Python to environment variables`\n2. 下载 ffmpeg.exe 放在 软件目录下的ffmpeg文件夹内\n3. 下载并安装git，https://github.com/git-for-windows/git/releases/download/v2.45.1.windows.1/Git-2.45.1-64-bit.exe \n4. 创建空文件夹 `D:/chattts` 并进入，地址栏输入 `cmd`回车，在弹出的cmd窗口中执行命令 `git clone https://github.com/jianchang512/chatTTS-ui .`\n5. 创建虚拟环境，执行命令 `python -m venv venv`\n6. 激活虚拟环境，执行 `.\\venv\\scripts\\activate`\n7. 安装依赖,执行 `pip install -r requirements.txt`\n8. 如果不需要CUDA加速，\n\n\t执行 `pip install torch==2.2.0 torchaudio==2.2.0`\n\n\t如果需要CUDA加速，执行 \n\t\n\t`pip install torch==2.2.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118`\n\t\n\t另需安装 CUDA11.8+ ToolKit，请自行搜索安装方法或参考 https://juejin.cn/post/7318704408727519270\n\t\n9. 执行 `python app.py` 启动，将自动打开浏览器窗口，默认地址 `http://127.0.0.1:9966`  (注意：默认从 modelscope 魔塔下载模型，不可使用代理下载，请关闭代理)\n\n\n## 源码部署注意 0.96版本起，必须安装ffmpeg\n\n1. 如果GPU显存低于4G，将强制使用CPU。\n\n2. Windows或Linux下如果显存大于4G并且是英伟达显卡，但源码部署后仍使用CPU，可尝试先卸载torch再重装，卸载`pip uninstall -y torch torchaudio` , 重新安装cuda版torch。`pip install torch==2.2.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118`  。必须已安装CUDA11.8+\n\n3. 默认检测 modelscope 是否可连接，如果可以，则从modelscope下载模型，否则从 huggingface.co下载模型\n\n\n\n## 音色获取\n\n0.96版本后，因ChatTTS内核升级，已无法直接使用从该站点下载的pt文件(https://modelscope.cn/studios/ttwwwaa/ChatTTS_Speaker)\n\n因此增加转换脚本 cover-pt.py [Win整合包可以直接下载 cover-pt.exe 文件，和 app.exe 放在同一目录下双击执行](https://github.com/jianchang512/ChatTTS-ui/releases)\n\n执行  `python cover-pt.py` 后将把 `speaker` 目录下的，以 `seed_` 开头，以  `_emb.pt` 结尾的文件，即下载后的默认文件名pt，\n转换为可用的编码格式，转换后的pt将改名为以 `_emb-covert.pt` 结尾。\n\n例：\n\n假如  `speaker/seed_2155_restored_emb.pt` 存在这个文件,将被转换为 `speaker/seed_2155_restored_emb-cover.pt`, 然后删掉原pt文件，仅保留该转换后的文件即可\n\n\n\n\n\n## [常见问题与报错解决方法](faq.md)\n\n\n\n\n## 修改http地址\n\n默认地址是 `http://127.0.0.1:9966`,如果想修改，可打开目录下的 `.env`文件，将 `WEB_ADDRESS=127.0.0.1:9966`改为合适的ip和端口，比如修改为`WEB_ADDRESS=192.168.0.10:9966`以便局域网可访问\n\n## 使用API请求 v0.5+\n\n**请求方法:** POST\n\n**请求地址:** http://127.0.0.1:9966/tts\n\n**请求参数:**\n\ntext:\tstr| 必须， 要合成语音的文字\n\nvoice:\t可选，默认 2222,  决定音色的数字， 2222 | 7869 | 6653 | 4099 | 5099，可选其一，或者任意传入将随机使用音色\n\nprompt:\tstr| 可选，默认 空， 设定 笑声、停顿，例如 [oral_2][laugh_0][break_6]\n\ntemperature:\tfloat| 可选，  默认 0.3\n\ntop_p:\tfloat|  可选， 默认 0.7\n\ntop_k:\tint|  可选， 默认 20\n\nskip_refine:\tint|   可选， 默认0， 1=跳过 refine text，0=不跳过\n\ncustom_voice:\tint|  可选， 默认0，自定义获取音色值时的种子值，需要大于0的整数，如果设置了则以此为准，将忽略 `voice`\n\n\n**返回:json数据**\n\n成功返回:\n\t{code:0,msg:ok,audio_files:[dict1,dict2]}\n\t\n\t其中 audio_files 是字典数组，每个元素dict为 {filename:wav文件绝对路径，url:可下载的wav网址}\n\n失败返回:\n\n\t{code:1,msg:错误原因}\n\n```\n\n# API调用代码\n\nimport requests\n\nres = requests.post('http://127.0.0.1:9966/tts', data={\n  \"text\": \"若不懂无需填写\",\n  \"prompt\": \"\",\n  \"voice\": \"3333\",\n  \"temperature\": 0.3,\n  \"top_p\": 0.7,\n  \"top_k\": 20,\n  \"skip_refine\": 0,\n  \"custom_voice\": 0\n})\nprint(res.json())\n\n#ok\n{code:0, msg:'ok', audio_files:[{filename: E:/python/chattts/static/wavs/20240601-22_12_12-c7456293f7b5e4dfd3ff83bbd884a23e.wav, url: http://127.0.0.1:9966/static/wavs/20240601-22_12_12-c7456293f7b5e4dfd3ff83bbd884a23e.wav}]}\n\n#error\n{code:1, msg:\"error\"}\n\n\n```\n\n\n## 在pyVideoTrans软件中使用\n\n> 升级 pyVideoTrans 到 1.82+ https://github.com/jianchang512/pyvideotrans\n\n1. 点击菜单-设置-ChatTTS，填写请求地址，默认应该填写 http://127.0.0.1:9966\n2. 测试无问题后，在主界面中选择`ChatTTS`\n\n![image](https://github.com/jianchang512/ChatTTS-ui/assets/3378335/7118325f-2b9a-46ce-a584-1d5c6dc8e2da)\n\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 9.7958984375,
          "content": "\n[简体中文](README.md) | [Discord Discussion Group](https://discord.gg/y9gUweVCCJ) | [Support the Project](https://github.com/jianchang512/ChatTTS-ui/issues/122)\n\n# ChatTTS webUI & API \n\nA simple local web interface to use ChatTTS for text-to-speech synthesis on the web, supporting mixed Chinese and English text and numbers, and providing an API interface.\n\n> The original [ChatTTS](https://github.com/2noise/chattts) project\n\n**Interface Preview**\n\n![image](https://github.com/jianchang512/ChatTTS-ui/assets/3378335/8d9b36d4-29b9-4cd7-ae70-3e3bd3225108)\n\n\nSample synthesized voice effects\n\nhttps://github.com/jianchang512/ChatTTS-ui/assets/3378335/bd6aaef9-a49a-4a81-803a-91e3320bf808\n\nText and control symbols mixed effect\n\nhttps://github.com/jianchang512/ChatTTS-ui/assets/3378335/e2a08ea0-32af-4a30-8880-3a91f6cbea55\n\n\n## Windows Pre-packaged Version\n\n1. Download the compressed package from [Releases](https://github.com/jianchang512/chatTTS-ui/releases), unzip it, and double-click app.exe to use.\n2. Some security software may flag it as a virus, please disable or deploy from source.\n3. If you have an Nvidia graphics card with more than 4GB of memory and have installed CUDA11.8+, GPU acceleration will be enabled.\n\n## Linux Container Deployment\n\n### Installation\n\n1. Clone the project repository\n\n   Clone the project to any directory, for example:\n\n   ```bash\n   git clone https://github.com/jianchang512/ChatTTS-ui.git chat-tts-ui\n   ```\n\n2. Start Runner\n\n   Enter the project directory:\n\n   ```bash\n   cd chat-tts-ui\n   ```\n\n   Start the container and view the initialization logs:\n\n   ```bash\n   For GPU version\n   docker compose -f docker-compose.gpu.yaml up -d \n\n   For CPU version    \n   docker compose -f docker-compose.cpu.yaml up -d\n\n   docker compose logs -f --no-log-prefix\n   ```\n\n3. Access ChatTTS WebUI\n\n   `Started at:['0.0.0.0', '9966']`, meaning you can access it via `IP:9966` of the deployment device, for example:\n\n   - Localhost: `http://127.0.0.1:9966`\n   - Server: `http://192.168.1.100:9966`\n\n### Update\n\n1. Get the latest code from the main branch:\n\n   ```bash\n   git checkout main\n   git pull origin main\n   ```\n\n2. Go to the next step and update to the latest image:\n\n   ```bash\n   docker compose down\n\n   For GPU version\n   docker compose -f docker-compose.gpu.yaml up -d --build\n\n   For CPU version\n   docker compose -f docker-compose.cpu.yaml up -d --build\n   \n   docker compose logs -f --no-log-prefix\n   ```\n\n## Linux Source Code Deployment\n\n1. Prepare python3.9-3.11 environment. Install FFmpeg\n2. Create an empty directory `/data/chattts` and execute `cd /data/chattts &&  git clone https://github.com/jianchang512/chatTTS-ui .`.\n3. Create a virtual environment `python3 -m venv venv`.\n4. Activate the virtual environment `source ./venv/bin/activate`.\n5. Install dependencies `pip3 install -r requirements.txt`.\n6. If CUDA acceleration is not needed, execute \n\n   `pip3 install torch==2.2.0 torchaudio==2.2.0`\n\n   If CUDA acceleration is needed, execute\n   \n   ```\n   pip install torch==2.2.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118\n\n   pip install nvidia-cublas-cu11 nvidia-cudnn-cu11\n   ```\n   \n   Additionally, install CUDA11.8+ ToolKit, search for installation methods or refer to https://juejin.cn/post/7318704408727519270\n\n   Besides CUDA, AMD GPU acceleration can also be used by installing ROCm and PyTorch_ROCm version. For AMD GPU, with the help of ROCm, PyTorch works out of the box without further modifications.\n   1. Refer to https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html to install AMD GPU Driver and ROCm.\n   2. Then install PyTorch_ROCm version from https://pytorch.org/. \n\n   `pip3 install torch==2.2.0  torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/rocm6.0`\n\n   After installation, you can use the command `rocm-smi` to view the AMD GPUs in the system. The following Torch code(query_gpu.py) can also be used to query the current AMD GPU Device.\n\n   ```\n   import torch\n   \n   print(torch.__version__)\n   \n   if torch.cuda.is_available():\n       device = torch.device(\"cuda\")          # a CUDA device object\n       print('Using GPU:', torch.cuda.get_device_name(0))\n   else:\n       device = torch.device(\"cpu\")\n       print('Using CPU')\n   \n   torch.cuda.get_device_properties(0)\n\n   ```\n\n   Using the code above, for instance, with AMD Radeon Pro W7900, the device query is as follows.\n\n   ```\n   \n   $ python ~/query_gpu.py\n   \n   2.4.0.dev20240401+rocm6.0\n   \n   Using GPU: AMD Radeon PRO W7900\n   \n   ```\n\n\n \n7. Execute `python3 app.py` to start. It will automatically open a browser window at `http://127.0.0.1:9966`. Note: Models are downloaded from modelscope by default without using a proxy, please disable the proxy.\n\n\n## MacOS Source Code Deployment\n\n1. Prepare the python3.9-3.11 environment and install git. Execute command `brew install libsndfile git python@3.10`. Then continue with\n\n   ```\n   brew install ffmpeg\n   \n   export PATH=\"/usr/local/opt/python@3.10/bin:$PATH\"\n   \n   source ~/.bash_profile \n   \n   source ~/.zshrc\n   \n   ```\n   \n2. Create an empty directory `/data/chattts` and execute command `cd /data/chattts &&  git clone https://github.com/jianchang512/chatTTS-ui .`.\n3. Create a virtual environment `python3 -m venv venv`.\n4. Activate the virtual environment `source ./venv/bin/activate`.\n5. Install dependencies `pip3 install -r requirements.txt`.\n6. Install torch `pip3 install torch==2.2.0 torchaudio==2.2.0`.\n7. Execute `python3 app.py` to start. It will automatically open a browser window at `http://127.0.0.1:9966`. Note: Models are downloaded from modelscope by default without using a proxy, please disable the proxy.\n\n\n## Windows Source Code Deployment\n\n1. Download python3.9-3.11, make sure to check `Add Python to environment variables` during installation. install ffmpeg.exe\n2. Download and install git from https://github.com/git-for-windows/git/releases/download/v2.45.1.windows.1/Git-2.45.1-64-bit.exe.\n3. Create an empty folder `D:/chattts` and enter it, type `cmd` in the address bar and press Enter. In the cmd window that pops up, execute command `git clone https://github.com/jianchang512/chatTTS-ui .`.\n4. Create a virtual environment by executing command `python -m venv venv`.\n5. Activate the virtual environment by executing `.\\venv\\scripts\\activate`.\n6. Install dependencies by executing `pip install -r requirements.txt`.\n7. If CUDA acceleration is not needed,\n\n   execute `pip install torch==2.2.0 torchaudio==2.2.0`.\n\n   If CUDA acceleration is needed, execute \n   \n   `pip install torch==2.2.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118`.\n   \n   Additionally, install CUDA11.8+ ToolKit, search for installation methods or refer to https://juejin.cn/post/7318704408727519270.\n   \n8. Execute `python app.py` to start. It will automatically open a browser window at `http://127.0.0.1:9966`. Note: Models are downloaded from modelscope by default without using a proxy, please disable the proxy.\n\n\n## Deployment Notes\n\n0. install ffmpeg since 0.96\n\n1. If the GPU memory is below 4GB, it will forcefully use the CPU.\n\n2. Under Windows or Linux, if the memory is more than 4GB and it is an Nvidia graphics card, but the source code deployment still uses CPU, you may try uninstalling torch first and then reinstalling it. Uninstall with `pip uninstall -y torch torchaudio`, then reinstall the CUDA version of torch `pip install torch==2.2.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118`. CUDA11.8+ must be installed.\n\n3. By default, it checks whether modelscope can be connected. If so, models are downloaded from modelscope; otherwise, models are downloaded from huggingface.co.\n\n\n## [FAQs and Troubleshooting](faq.md)\n\n\n\n\n## Modify HTTP Address\n\nThe default address is `http://127.0.0.1:9966`. If you want to modify it, open the `.env` file in the directory and change `WEB_ADDRESS=127.0.0.1:9966` to the appropriate IP and port, such as changing to `WEB_ADDRESS=192.168.0.10:9966` for LAN access.\n\n## Using API Requests v0.5+\n\n**Method:** POST\n\n**URL:** http://127.0.0.1:9966/tts\n\n**Parameters:**\n\ntext: str| Required, text to synthesize.\n\nvoice: int| Optional, default 2222. Determines the voice digit, choose from 2222 | 7869 | 6653 | 4099 | 5099, or any input will randomly use a voice.\n\nprompt: str| Optional, default empty. Sets laughter, pause, etc., like [oral_2][laugh_0][break_6].\n\ntemperature: float| Optional, default 0.3.\n\ntop_p: float| Optional, default 0.7.\n\ntop_k: int| Optional, default 20.\n\nskip_refine: int| Optional, default 0. 1=skip refine text, 0=do not skip.\n\ncustom_voice: int| Optional, default 0. Sets a custom seed value for obtaining the voice, must be a positive integer. If set, it will take precedence over `voice`.\n\n\n**Response: JSON**\n\nSuccess:\n\t{code:0,msg:ok,audio_files:[dict1,dict2]}\n\t\n\twhere audio_files is an array of dictionaries, each element dict is {filename:absolute path to wav file, url:downloadable wav URL}\n\nFailure:\n\n\t{code:1,msg:error reason}\n\n\n```\n\n# API Call Code\n\nimport requests\n\nres = requests.post('http://127.0.0.1:9966/tts', data={\n  \"text\": \"No need to fill if unsure\",\n  \"prompt\": \"\",\n  \"voice\": \"3333\",\n  \"temperature\": 0.3,\n  \"top_p\": 0.7,\n  \"top_k\": 20,\n  \"skip_refine\": 0,\n  \"custom_voice\": 0,\n})\nprint(res.json())\n\n#ok\n{code:0, msg:'ok', audio_files:[{filename: E:/python/chattts/static/wavs/20240601-22_12_12-c7456293f7b5e4dfd3ff83bbd884a23e.wav, url: http://127.0.0.1:9966/static/wavs/20240601-22_12_12-c7456293f7b5e4dfd3ff83bbd884a23e.wav}]}\n\n#error\n{code:1, msg:\"error\"}\n\n\n```\n\n\n## Using in pyVideoTrans software\n\n> Upgrade pyVideoTrans to 1.82+ https://github.com/jianchang512/pyvideotrans\n\n1. Click Menu-Settings-ChatTTS and fill in the request address, which should by default be http://127.0.0.1:9966.\n2. After ensuring there are no issues, select `ChatTTS` on the main interface.\n\n![image](https://github.com/jianchang512/ChatTTS-ui/assets/3378335/7118325f-2b9a-46ce-a584-1d5c6dc8e2da)\n\n"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 11.4609375,
          "content": "import os\nimport re\nimport sys\nif sys.platform == \"darwin\":\n    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nimport io\nimport json\nimport torchaudio\nimport wave\nfrom pathlib import Path\nprint('Starting...')\nimport shutil\nimport time\n\nimport torch\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\ntorch._dynamo.config.cache_size_limit = 64\ntorch._dynamo.config.suppress_errors = True\ntorch.set_float32_matmul_precision('high')\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nimport subprocess\nimport soundfile as sf\nimport ChatTTS\nimport datetime\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom flask import Flask, request, render_template, jsonify,  send_from_directory,send_file,Response, stream_with_context\nimport logging\nfrom logging.handlers import RotatingFileHandler\nfrom waitress import serve\nfrom random import random\nfrom modelscope import snapshot_download\nimport numpy as np\nimport threading\nfrom uilib.cfg import WEB_ADDRESS, SPEAKER_DIR, LOGS_DIR, WAVS_DIR, MODEL_DIR, ROOT_DIR\nfrom uilib import utils,VERSION\nfrom ChatTTS.utils import select_device\nfrom uilib.utils import is_chinese_os,modelscope_status\nmerge_size=int(os.getenv('merge_size',10))\nenv_lang=os.getenv('lang','')\nif env_lang=='zh':\n    is_cn= True\nelif env_lang=='en':\n    is_cn=False\nelse:\n    is_cn=is_chinese_os()\n    \nif not shutil.which(\"ffmpeg\"):\n    print('请先安装ffmpeg')\n    time.sleep(60)\n    exit()    \n\n\nchat = ChatTTS.Chat()\ndevice_str=os.getenv('device','default')\n\nif device_str in ['default','mps']:\n    device=select_device(min_memory=2047,experimental=True if device_str=='mps' else False)\nelif device_str =='cuda':\n    device=select_device(min_memory=2047)\nelif device_str == 'cpu':\n    device = torch.device(\"cpu\")\n\n\nchat.load(source=\"local\" if not os.path.exists(MODEL_DIR+\"/DVAE_full.pt\") else 'custom',custom_path=ROOT_DIR, device=device,compile=True if os.getenv('compile','true').lower()!='false' else False)\n\n\n# 配置日志\n# 禁用 Werkzeug 默认的日志处理器\nlog = logging.getLogger('werkzeug')\nlog.handlers[:] = []\nlog.setLevel(logging.WARNING)\n\napp = Flask(__name__, \n    static_folder=ROOT_DIR+'/static', \n    static_url_path='/static',\n    template_folder=ROOT_DIR+'/templates')\n\nroot_log = logging.getLogger()  # Flask的根日志记录器\nroot_log.handlers = []\nroot_log.setLevel(logging.WARNING)\napp.logger.setLevel(logging.WARNING) \n# 创建 RotatingFileHandler 对象，设置写入的文件路径和大小限制\nfile_handler = RotatingFileHandler(LOGS_DIR+f'/{datetime.datetime.now().strftime(\"%Y%m%d\")}.log', maxBytes=1024 * 1024, backupCount=5)\n# 创建日志的格式\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n# 设置文件处理器的级别和格式\nfile_handler.setLevel(logging.WARNING)\nfile_handler.setFormatter(formatter)\n# 将文件处理器添加到日志记录器中\napp.logger.addHandler(file_handler)\napp.jinja_env.globals.update(enumerate=enumerate)\n\n@app.route('/static/<path:filename>')\ndef static_files(filename):\n    return send_from_directory(app.config['STATIC_FOLDER'], filename)\n\n\n@app.route('/')\ndef index():\n    speakers=utils.get_speakers()\n    return render_template(\n        f\"index{'' if is_cn else 'en'}.html\",\n        weburl=WEB_ADDRESS,\n        speakers=speakers,\n        version=VERSION\n    )\n\n\n# 根据文本返回tts结果，返回 filename=文件名 url=可下载地址\n# 请求端根据需要自行选择使用哪个\n# params:\n#\n# text:待合成文字\n# prompt：\n# voice：音色\n# custom_voice：自定义音色值\n# skip_refine: 1=跳过refine_text阶段，0=不跳过\n# temperature\n# top_p\n# top_k\n# speed\n# text_seed\n# refine_max_new_token\n# infer_max_new_token\n# wav\n\naudio_queue=[]\n\n@app.route('/tts', methods=['GET', 'POST'])\ndef tts():\n    global audio_queue\n    # 原始字符串\n    text = request.args.get(\"text\",\"\").strip() or request.form.get(\"text\",\"\").strip()\n    prompt = request.args.get(\"prompt\",\"\").strip() or request.form.get(\"prompt\",'')\n\n    # 默认值\n    defaults = {\n        \"custom_voice\": 0,\n        \"voice\": \"2222\",\n        \"temperature\": 0.3,\n        \"top_p\": 0.7,\n        \"top_k\": 20,\n        \"skip_refine\": 0,\n        \"speed\":5,\n        \"text_seed\":42,\n        \"refine_max_new_token\": 384,\n        \"infer_max_new_token\": 2048,\n        \"wav\": 0,\n        \"is_stream\":0\n    }\n\n    # 获取\n    custom_voice = utils.get_parameter(request, \"custom_voice\", defaults[\"custom_voice\"], int)\n    voice = str(custom_voice) if custom_voice > 0 else utils.get_parameter(request, \"voice\", defaults[\"voice\"], str)\n    temperature = utils.get_parameter(request, \"temperature\", defaults[\"temperature\"], float)\n    top_p = utils.get_parameter(request, \"top_p\", defaults[\"top_p\"], float)\n    top_k = utils.get_parameter(request, \"top_k\", defaults[\"top_k\"], int)\n    skip_refine = utils.get_parameter(request, \"skip_refine\", defaults[\"skip_refine\"], int)\n    is_stream = utils.get_parameter(request, \"is_stream\", defaults[\"is_stream\"], int)\n    speed = utils.get_parameter(request, \"speed\", defaults[\"speed\"], int)\n    text_seed = utils.get_parameter(request, \"text_seed\", defaults[\"text_seed\"], int)\n    refine_max_new_token = utils.get_parameter(request, \"refine_max_new_token\", defaults[\"refine_max_new_token\"], int)\n    infer_max_new_token = utils.get_parameter(request, \"infer_max_new_token\", defaults[\"infer_max_new_token\"], int)\n    wav = utils.get_parameter(request, \"wav\", defaults[\"wav\"], int)\n        \n        \n    \n    app.logger.info(f\"[tts]{text=}\\n{voice=},{skip_refine=}\\n\")\n    if not text:\n        return jsonify({\"code\": 1, \"msg\": \"text params lost\"})\n    # 固定音色\n    rand_spk=None\n    # voice可能是 {voice}.csv or {voice}.pt or number\n    voice=voice.replace('.csv','.pt')\n    seed_path=f'{SPEAKER_DIR}/{voice}'\n    print(f'{voice=}')\n    #if voice.endswith('.csv') and os.path.exists(seed_path):\n    #    rand_spk=utils.load_speaker(voice)\n    #    print(f'当前使用音色 {seed_path=}')\n    #el\n    \n    if voice.endswith('.pt') and os.path.exists(seed_path):\n        #如果.env中未指定设备，则使用 ChatTTS相同算法找设备，否则使用指定设备\n        rand_spk=torch.load(seed_path, map_location=device)\n        print(f'当前使用音色 {seed_path=}')\n    # 否则 判断是否存在 {voice}.csv\n    #elif os.path.exists(f'{SPEAKER_DIR}/{voice}.csv'):\n    #    rand_spk=utils.load_speaker(voice)\n    #    print(f'当前使用音色 {SPEAKER_DIR}/{voice}.csv')\n    \n    if rand_spk is None:    \n        print(f'当前使用音色：根据seed={voice}获取随机音色')\n        voice_int=re.findall(r'^(\\d+)',voice)\n        if len(voice_int)>0:\n            voice=int(voice_int[0])\n        else:\n            voice=2222\n        torch.manual_seed(voice)\n        #std, mean = chat.sample_random_speaker\n        rand_spk = chat.sample_random_speaker()\n        #rand_spk = torch.randn(768) * std + mean\n        # 保存音色\n        torch.save(rand_spk,f\"{SPEAKER_DIR}/{voice}.pt\")\n        #utils.save_speaker(voice,rand_spk)\n        \n\n    audio_files = []\n    \n\n    start_time = time.time()\n    \n    # 中英按语言分行\n    text_list=[t.strip() for t in text.split(\"\\n\") if t.strip()]\n    new_text=utils.split_text(text_list)\n    if text_seed>0:\n        torch.manual_seed(text_seed)\n    \n    \n    params_infer_code = ChatTTS.Chat.InferCodeParams(\n        spk_emb=rand_spk,\n        prompt=f\"[speed_{speed}]\",\n        top_P=top_p,\n        top_K=top_k,\n        temperature=temperature,\n        max_new_token=infer_max_new_token\n    )\n    params_refine_text = ChatTTS.Chat.RefineTextParams(\n        prompt=prompt,\n        top_P=top_p,\n        top_K=top_k,\n        temperature=temperature,\n        max_new_token=refine_max_new_token\n    )\n    print(f'{prompt=}')\n    # 将少于30个字符的行同其他行拼接\n    retext=[]\n    short_text=\"\"\n    for it in new_text:\n        if len(it)<30:\n            short_text+=f\"{it} [uv_break] \"\n            if len(short_text)>30:\n                retext.append(short_text)\n                short_text=\"\"\n        else:\n            retext.append(short_text+it)\n            short_text=\"\"\n    if len(short_text)>30 or len(retext)<1:\n        retext.append(short_text)\n    elif short_text:\n        retext[-1]+=f\" [uv_break] {short_text}\"\n        \n    new_text=retext\n    \n    new_text_list=[new_text[i:i+merge_size] for i in range(0,len(new_text),merge_size)]\n    filename_list=[]\n\n    audio_time=0\n    inter_time=0\n\n    for i,te in enumerate(new_text_list):\n        print(f'{te=}')\n        wavs = chat.infer(\n            te, \n            #use_decoder=False,\n            stream=True if is_stream==1 else False,\n            skip_refine_text=skip_refine,\n            do_text_normalization=False,\n            do_homophone_replacement=True,\n            params_refine_text=params_refine_text,\n            params_infer_code=params_infer_code\n            \n            )\n\n\n        end_time = time.time()\n        inference_time = end_time - start_time\n        inference_time_rounded = round(inference_time, 2)\n        inter_time+=inference_time_rounded\n        print(f\"推理时长: {inference_time_rounded} 秒\")\n\n       \n        \n        for j,w in enumerate(wavs):\n            filename = datetime.datetime.now().strftime('%H%M%S_')+f\"use{inference_time_rounded}s-seed{voice}-te{temperature}-tp{top_p}-tk{top_k}-textlen{len(text)}-{str(random())[2:7]}\" + f\"-{i}-{j}.wav\"\n            filename_list.append(filename)\n            torchaudio.save(WAVS_DIR+'/'+filename, torch.from_numpy(w).unsqueeze(0), 24000)\n        \n    txt_tmp=\"\\n\".join([f\"file '{WAVS_DIR}/{it}'\" for it in filename_list])\n    txt_name=f'{time.time()}.txt'\n    with open(f'{WAVS_DIR}/{txt_name}','w',encoding='utf-8') as f:\n        f.write(txt_tmp)\n    outname=datetime.datetime.now().strftime('%H%M%S_')+f\"use{inter_time}s-audio{audio_time}s-seed{voice}-te{temperature}-tp{top_p}-tk{top_k}-textlen{len(text)}-{str(random())[2:7]}\" + \"-merge.wav\"\n    try:\n        subprocess.run([\"ffmpeg\",\"-hide_banner\", \"-ignore_unknown\",\"-y\",\"-f\",\"concat\",\"-safe\",\"0\",\"-i\",f'{WAVS_DIR}/{txt_name}',\"-c:a\",\"copy\",WAVS_DIR + '/' + outname],\n                   stdout=subprocess.PIPE,\n                   stderr=subprocess.PIPE,\n                   encoding=\"utf-8\",\n                   check=True,\n                   text=True,\n                   creationflags=0 if sys.platform != 'win32' else subprocess.CREATE_NO_WINDOW)\n    except Exception as e:\n        return jsonify({\"code\":1,\"msg\":str(e)})\n\n    \n\n    audio_files.append({\n        \"filename\": WAVS_DIR + '/' + outname,\n        \"url\": f\"http://{request.host}/static/wavs/{outname}\",\n        \"inference_time\": round(inter_time,2),\n        \"audio_duration\": -1\n    })\n    result_dict={\"code\": 0, \"msg\": \"ok\", \"audio_files\": audio_files}\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    except Exception:\n        pass\n    # 兼容pyVideoTrans接口调用\n    if len(audio_files)==1:\n        result_dict[\"filename\"]=audio_files[0]['filename']\n        result_dict[\"url\"]=audio_files[0]['url']\n\n    if wav>0:\n        return send_file(audio_files[0]['filename'], mimetype='audio/x-wav')\n    else:\n        return jsonify(result_dict)\n\n\n\n@app.route('/clear_wavs', methods=['POST'])\ndef clear_wavs():\n    dir_path = 'static/wavs'  # wav音频文件存储目录\n    success, message = utils.ClearWav(dir_path)\n    if success:\n        return jsonify({\"code\": 0, \"msg\": message})\n    else:\n        return jsonify({\"code\": 1, \"msg\": message})\n\ntry:\n    host = WEB_ADDRESS.split(':')\n    print(f'Start:{WEB_ADDRESS}')\n    threading.Thread(target=utils.openweb,args=(f'http://{WEB_ADDRESS}',)).start()\n    serve(app,host=host[0], port=int(host[1]))\nexcept Exception as e:\n    print(e)\n\n"
        },
        {
          "name": "asset",
          "type": "tree",
          "content": null
        },
        {
          "name": "cover-pt.py",
          "type": "blob",
          "size": 2.8203125,
          "content": "'''\n0.96版本后，因ChatTTS内核升级，已无法直接使用从该站点下载的pt文件。\n\nhttps://modelscope.cn/studios/ttwwwaa/ChatTTS_Speaker\n\n因此增加该转换脚本。\n\n执行  python cover-pt.py 后将把 `speaker` 目录下的，以 seed_ 开头，\n以  _emb.pt 结尾的文件，即下载后的默认文件名，\n转换为可用的编码格式，转换后的pt将改名为以 `_emb-covert.pt` 结尾。\n\n例：\n\n假如  speaker/seed_2155_restored_emb.pt 存在这个文件\n\n将被转换为 speaker/seed_2155_restored_emb-cover.pt, \n\n然后删掉原pt文件，仅保留该转换后的文件即可\n\n\n\n'''\n\nimport os\nimport re\nimport sys\nif sys.platform == \"darwin\":\n    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nimport io\nimport json\nimport torchaudio\nimport wave\nfrom pathlib import Path\nprint('Starting...')\nimport shutil\nimport time\n\n\nimport torch\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\ntorch._dynamo.config.cache_size_limit = 64\ntorch._dynamo.config.suppress_errors = True\ntorch.set_float32_matmul_precision('high')\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nimport subprocess\nimport soundfile as sf\nimport ChatTTS\nimport datetime\nfrom dotenv import load_dotenv\nload_dotenv()\n\nimport logging\nfrom logging.handlers import RotatingFileHandler\n\nfrom random import random\nfrom modelscope import snapshot_download\nimport numpy as np\nimport threading\nfrom uilib.cfg import WEB_ADDRESS, SPEAKER_DIR, LOGS_DIR, WAVS_DIR, MODEL_DIR, ROOT_DIR\nfrom uilib import utils,VERSION\nfrom ChatTTS.utils import select_device\nfrom uilib.utils import is_chinese_os,modelscope_status\nmerge_size=int(os.getenv('merge_size',10))\nenv_lang=os.getenv('lang','')\nif env_lang=='zh':\n    is_cn= True\nelif env_lang=='en':\n    is_cn=False\nelse:\n    is_cn=is_chinese_os()\n    \n\n\nchat = ChatTTS.Chat()\ndevice_str=os.getenv('device','default')\n\nif device_str in ['default','mps']:\n    device=select_device(min_memory=2047,experimental=True if device_str=='mps' else False)\nelif device_str =='cuda':\n    device=select_device(min_memory=2047)\nelif device_str == 'cpu':\n    device = torch.device(\"cpu\")\n\n\nchat.load(source=\"custom\",custom_path=ROOT_DIR, device=device,compile=True if os.getenv('compile','true').lower()!='false' else False)\nn=0\nfor it in os.listdir('./speaker'):\n    if it.startswith('seed_') and not it.endswith('_emb-covert.pt'):\n        print(f'开始转换 {it}')        \n        n+=1\n        rand_spk=torch.load(f'./speaker/{it}', map_location=device)\n\n        torch.save( chat._encode_spk_emb(rand_spk) ,f\"{SPEAKER_DIR}/{it.replace('.pt','-covert.pt')}\")\nif n==0:\n    print('没有可转换的pt文件，仅转换以 seed_ 开头，并以 _emb.pt 结尾的文件')\n\nelse:\n    print(f'转换完成{n}个，可以删掉以 _emb.pt 结尾的文件了，注意保留 -covert.pt 结尾的文件')\n\nprint(f'\\n\\n30s后本窗口自动关闭')\ntime.sleep(30)"
        },
        {
          "name": "docker-compose.cpu.yaml",
          "type": "blob",
          "size": 0.33203125,
          "content": "services:\r\n  chat-tts-ui:\r\n    build:\r\n      context: .\r\n      dockerfile: Dockerfile.cpu\r\n    container_name: chat-tts-ui\r\n    restart: always\r\n    volumes:\r\n      - \"./:/app\"\r\n    ports:\r\n      - 9966:9966\r\n    user: \"${UID}:${GID}\"\r\n    environment:\r\n      LOG_LEVEL: DEBUG\r\n      WEB_ADDRESS: 0.0.0.0:9966\r\n    command: python3 app.py\r\n"
        },
        {
          "name": "docker-compose.gpu.yaml",
          "type": "blob",
          "size": 0.4794921875,
          "content": "services:\n  chat-tts-ui:\n    build:\n      context: .\n      dockerfile: Dockerfile.gpu\n    container_name: chat-tts-ui\n    restart: always\n    volumes:\n      - \"./:/app\"\n    ports:\n      - 9966:9966\n    user: \"${UID}:${GID}\"\n    environment:\n      LOG_LEVEL: DEBUG\n      WEB_ADDRESS: 0.0.0.0:9966\n      NVIDIA_VISIBLE_DEVICES: all\n    command: python3 app.py\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              capabilities: [gpu]\n"
        },
        {
          "name": "faq.md",
          "type": "blob",
          "size": 4.404296875,
          "content": "# 常见问题与报错\n\n\n**注意：不同机器使用相同种子生成的音频音色可能不同，同一机器使用相同种子多次生成的音频音色也可能变化。**\n\n\n**升级到0.96版后报错**\n\n答： 0.96版起，源码部署必须先安装ffmpeg \n\n0.96版起，之前的音色文件csv和pt已不可用，请填写音色值重新生成，或到以下站点下载  \n \nhttps://modelscope.cn/studios/ttwwwaa/ChatTTS_Speaker\n\n\n**0.** 执行app.py报错 FileNotFoundError: [Errno 2] No such file or directory: '../ChatTTS-ui/models/pzc163/chatTTS/config/path.yaml\n\n答：模型不完整，重新下载模型或者 打开 https://www.modelscope.cn/models/pzc163/chatTTS/files 下载 path.yaml 、复制到报错里显示的文件夹内 ChatTTS-ui/models/pzc163/chatTTS/config/\n\n\n\n**1.**  MacOS 报错 `Initializing libomp.dylib, but found libiomp5.dylib already initialized`\n\n> 答：在app.py的 `import os` 的下一行，添加代码\n>   \n> `os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'`\n\n\n**2.**  MacOS 无报错但进度条一直百分之0 卡住不动\n\n> 答：app.py 中 \n> \n> `chat.load_models(source=\"local\",local_path=CHATTTS_DIR)` \n> \n> 改为\n> \n> `chat.load_models(source=\"local\",local_path=CHATTTS_DIR,compile=False)`\n\n**3.**  MacOS 报 `libomp` 相关错误\n\n> 答：执行  `brew install libomp`\n\n**4.**  报https相关错误 `ProxyError: HTTPSConnectionPool(host='www.modelscope.cn', port=443)`\n\n> 答：从 modelscope 魔塔下载模型时不可使用代理，请关闭代理\n\n\n**5.**  报错丢失文件 `Missing spk_stat.pt`\n\n> 答：本项目(ChatTTS-ui)默认从 modelscope 即魔塔社区下载模型，但该库里的模型缺少 spk_stat.pt文件\n> \n>   请科学上网后从\n>\n>   https://huggingface.co/2Noise/ChatTTS/blob/main/asset/spk_stat.pt    \n> \n>  下载 spk_stat.pt， 然后复制 spk_stat.pt  到报错提示的目录下，以本项目为例，需要复制到  `models/pzc163/chatTTS/asset`  文件夹内\n\n\n**6.**  报错 `Dynamo is not supported on Python 3.12`\n\n> 答：不支持python3.12+版本，降级到 python3.10\n\n\n**7.**  MacOS报错 `NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+`\n\n> 答：执行  `brew install openssl@1.1`  \n> \n>  执行   `pip install urllib3==1.26.15\n\n\n\n**8.**  Windows上报错：`Windows not yet supported for torch.compile`\n\n> 答：`chat.load_models(compile=False)`  改为   `chat.load_models(compile=False,device=\"cpu\")`\n\n\n**9.**   Windows上可以运行有GPU，但很慢\n\n> 答：如果是英伟达显卡，请将cuda升级到11.8+\n\n\n**10**. 下载模型时出现 proxy 类型错误\n\n答：默认会从 modelscope 下载模型，但 modelscope 仅允许中国大陆ip下载，如果遇到 proxy 类错误，请关闭代理。如果你希望从 huggingface.co 下载模型，请打开 `app.py` 查看大约第50行-60行的代码注释。\n\n\n**11.** 中英分词是怎么回事\n\n答：如果选中中英分词，那么将会把文字中的中文和英文分离出来单独合成，同时将对应的数字 转为相应语言的文字，比如 中文下123转为一二三，英文下123转为 one two three\n\n\n**12.** Runtime Error:cannot find a working triton installation \n\n打开 .env  将 compile=true 改为 compile=false\n\n**13.** MacOS下无法安装 soundfile\n\n答：打开终端，执行 `brew install libsndfile` 然后再安装 soundfile\n\n\n**14.** 如何离线使用\n\n答：\n\n1. 使用源码部署\n2. 先运行一次，确保模型下载完毕\n3. 打开 app.py 大约35行， `CHATTTS_DIR = snapshot_download('pzc163/chatTTS',cache_dir=MODEL_DIR)` 改为 `CHATTTS_DIR = MODEL_DIR+\"/pzc163/chatTTS\"`\n\n**15.** ChatTTS原始项目新版本有兼容问题，可能会报错 “报错 Normalizer pynini WeTextProcessing nemo_text_processing ”\n\n解决方法：\n新版使用了 nemo_text_processing  和  pynini 来处理中文，但遗憾的是，pynini压根无法在windows平台安装和使用，要使用，也只能安装在WSL子系统上。\n\n不管给出的什么安装方式， 比如 \n\n```\npip install pynini==2.1.5 Cython   WeTextProcessing\n\n```\n\n都是无法在Windows上正确安装的\n\n![image](https://github.com/2noise/ChatTTS/assets/3378335/e32c50d1-492c-4b72-958b-78af0575e662)\n\n\n----\n\n解决方法:\n打开 ChatTTS/core.py, 大约143行，注释掉接下来的7行，\n\n![image](https://github.com/2noise/ChatTTS/assets/3378335/5bdd3dc8-0c7c-485f-b5dc-613f14917319)\n\n\n问题解决\n\n或者 chat.infer() 添加参数 do_text_normalization=False， chat.infer(do_text_normalization=False)\n"
        },
        {
          "name": "ffmpeg",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 2.0849609375,
          "content": "[tool.poetry]\nname = \"chattts-ui\"\nversion = \"0.1.0\"\ndescription = \"一个简单的本地网页界面，直接使用ChatTTS将文字合成为语音，同时支持对外提供API接口。\"\nauthors = [\"jianchang512 <jianchang512@gmail.com>\"]\nreadme = \"README.md\"\nhomepage = \"https://github.com/jianchang512/ChatTTS-ui\"\nrepository = \"https://github.com/jianchang512/ChatTTS-ui\"\ndocumentation = \"https://github.com/jianchang512/ChatTTS-ui\"\npackage-mode = false\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\nsoundfile = \"^0.12.1\"\npython-dotenv = \"^1.0.1\"\nflask = \"^3.0.3\"\nwaitress = \"^3.0.0\"\nmodelscope = \"^1.14.0\"\nlangsegment = \"^0.3.3\"\nomegaconf = \"^2.3.0\"\ntokenizers = \"^0.19.1\"\ntransformers = \"^4.41.2\"\ntorch = [\n    { version = \"^2.3.0+cu118\", source = \"pytorch-gpu-src\" },\n    { platform = \"darwin\", version = \"^2\" }\n]\ntorchaudio = [\n    { version = \"^2.3.0+cu118\", source = \"pytorch-gpu-src\" },\n    { platform = \"darwin\", version = \"^2\" }\n]\nvocos = \"^0.1.0\"\nvector-quantize-pytorch = \"^1.14.24\"\nnumpy = { version = \"^1.26.4\" }\n\n# Optional dependencies that are not downloaded by default\nfastapi = { version = \"*\", extras = [\"all\"], optional = true }\npydantic = { version = \"^2\", optional = true }\n\n[[tool.poetry.source]]\nname = \"pytorch-gpu-src\"\nurl = \"https://download.pytorch.org/whl/cu118\"\npriority = \"explicit\"\n\n[tool.poetry.group.dev.dependencies]\nblack = \"*\"\nruff = \"*\"\n\n[tool.poetry.group.test.dependencies]\n# https://docs.pytest.org/en/stable/reference/plugin_list.html#plugin-list\n# https://docs.pytest.org/en/stable/contents.html\npytest = \"*\"\n# https://pytest-asyncio.readthedocs.io/en/latest/\npytest-asyncio = \"*\"\n\n[tool.black]\nline-length = 100\ntarget-version = [\"py310\", \"py311\", \"py312\"]\nskip-magic-trailing-comma = true\nexclude = '''\n/(\n    ChatTTS\n    | .*\n    | build\n    | dist\n    | migrations\n    | __pycache__\n)/\n'''\n\n[tool.pytest.ini_options]\n# https://docs.pytest.org/en/stable/reference/reference.html#configuration-options\ntestpaths = [\"tests\", \"examples\"]\nasyncio_mode = \"auto\"\nfilterwarnings = \"ignore::DeprecationWarning\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.337890625,
          "content": "Flask\nipython\nmodelscope\nnumpy==1.26.4\nnumba\neinops\ntqdm\nomegaconf>=2.3.0\ntorch>=2.1.0\npython-dotenv\nrequests\nsoundfile\ntokenizers\ntransformers==4.41.1\nvector-quantize-pytorch\nvocos\nwaitress\npybase16384\npynini==2.1.5; sys_platform == 'linux'\nWeTextProcessing; sys_platform == 'linux'\nnemo_text_processing; sys_platform == 'linux'\nav\npydub\npandas\n"
        },
        {
          "name": "run.bat",
          "type": "blob",
          "size": 0.0478515625,
          "content": "@echo off\n\n.\\venv\\scripts\\python.exe app.py\npause"
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 2.2001953125,
          "content": "import os, sys\n\nif sys.platform == \"darwin\":\n    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n\nnow_dir = os.getcwd()\nsys.path.append(now_dir)\n\nfrom dotenv import load_dotenv\nload_dotenv(\"sha256.env\")\n\nimport wave\nimport ChatTTS\nfrom IPython.display import Audio\nimport numpy as np\n\ndef save_wav_file(wav, index):\n    wav_filename = f\"output_audio_{index}.wav\"\n    # Convert numpy array to bytes and write to WAV file\n    wav_bytes = (wav * 32768).astype('int16').tobytes()\n    with wave.open(wav_filename, \"wb\") as wf:\n        wf.setnchannels(1)  # Mono channel\n        wf.setsampwidth(2)  # Sample width in bytes\n        wf.setframerate(24000)  # Sample rate in Hz\n        wf.writeframes(wav_bytes)\n    print(f\"Audio saved to {wav_filename}\")\n\ndef main():\n    # Retrieve text from command line argument\n    try:\n        sys.argv.remove('--stream')\n        stream = True\n    except:\n        stream = False\n    text_input = sys.argv[1] if len(sys.argv) > 1 else \"<YOUR TEXT HERE>\"\n    print(\"Received text input:\", text_input)\n\n    chat = ChatTTS.Chat()\n    print(\"Initializing ChatTTS...\")\n    # if using macbook(M1), I suggest you set `device='cpu', compile=False`\n    #chat.load_models()\n    chat.load_models(source=\"custom\",custom_path='./models/pzc163/chattts')\n    print(\"Models loaded successfully.\")\n\n    texts = [text_input]\n    print(\"Text prepared for inference:\", texts)\n\n    wavs_gen = chat.infer(texts, use_decoder=True, stream=stream)\n    print(\"Inference completed. Audio generation successful.\")\n    # Save each generated wav file to a local file\n\n    if stream:\n        print('generate with stream mode ..')\n        wavs = [np.array([[]])]\n        for gen in wavs_gen:\n            print('got new chunk', gen)\n            tmp=[np.array([[]])]\n            tmp[0]=np.hstack([tmp[0], np.array(gen[0])])\n            save_wav_file(tmp[0], 11)\n            wavs[0] = np.hstack([wavs[0], np.array(gen[0])])\n    else:\n        print('generate without stream mode ..')\n        wavs = wavs_gen\n\n    for index, wav in enumerate(wavs):\n        save_wav_file(wav, index)\n\n    return Audio(wavs[0], rate=24_000, autoplay=True)\n\nif __name__ == \"__main__\":\n    print(\"Starting the TTS application...\")\n    main()\n    print(\"TTS application finished.\")\n"
        },
        {
          "name": "runtest.bat",
          "type": "blob",
          "size": 0.048828125,
          "content": "@echo off\n\n.\\venv\\scripts\\python.exe test.py\npause"
        },
        {
          "name": "speaker",
          "type": "tree",
          "content": null
        },
        {
          "name": "static",
          "type": "tree",
          "content": null
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 1.7587890625,
          "content": "import os\nimport re\nimport sys\nif sys.platform == \"darwin\":\n    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\nimport io\nimport json\nimport torchaudio\nimport wave\nfrom pathlib import Path\nprint('Starting...')\nimport shutil\nimport time\n\n\nimport torch\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\ntorch._dynamo.config.cache_size_limit = 64\ntorch._dynamo.config.suppress_errors = True\ntorch.set_float32_matmul_precision('high')\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\nimport subprocess\nimport soundfile as sf\nimport ChatTTS\nimport datetime\nfrom dotenv import load_dotenv\nload_dotenv()\n\nimport logging\nfrom logging.handlers import RotatingFileHandler\n\nfrom random import random\nfrom modelscope import snapshot_download\nimport numpy as np\nimport threading\nfrom uilib.cfg import WEB_ADDRESS, SPEAKER_DIR, LOGS_DIR, WAVS_DIR, MODEL_DIR, ROOT_DIR\nfrom uilib import utils,VERSION\nfrom ChatTTS.utils.gpu_utils import select_device\nfrom uilib.utils import is_chinese_os,modelscope_status\nmerge_size=int(os.getenv('merge_size',10))\nenv_lang=os.getenv('lang','')\nif env_lang=='zh':\n    is_cn= True\nelif env_lang=='en':\n    is_cn=False\nelse:\n    is_cn=is_chinese_os()\n    \nCHATTTS_DIR= MODEL_DIR+'/pzc163/chatTTS'\n\n\nchat = ChatTTS.Chat()\ndevice=os.getenv('device','default')\nchat.load(source=\"custom\",custom_path=CHATTTS_DIR, device=None if device=='default' else device,compile=True if os.getenv('compile','true').lower()!='false' else False)\n\nfor it in os.listdir('./speaker'):\n    if it.startswith('seed_') and not it.endswith('_emb-covert.pt'):\n        \n\n        rand_spk=torch.load(f'./speaker/{it}', map_location=select_device(4096) if device=='default' else torch.device(device))\n\n        torch.save( chat._encode_spk_emb(rand_spk) ,f\"{SPEAKER_DIR}/{it.replace('.pt','-covert.pt')}\")"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "uilib",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}