{
  "metadata": {
    "timestamp": 1736560499010,
    "page": 92,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "jadore801120/attention-is-all-you-need-pytorch",
      "stars": 8974,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1435546875,
          "content": ".idea/\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n*.swp\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0439453125,
          "content": "MIT License\n\nCopyright (c) 2017 Victor Huang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.580078125,
          "content": "# Attention is all you need: A Pytorch Implementation\n\nThis is a PyTorch implementation of the Transformer model in \"[Attention is All You Need](https://arxiv.org/abs/1706.03762)\" (Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, arxiv, 2017). \n\n\nA novel sequence to sequence framework utilizes the **self-attention mechanism**, instead of Convolution operation or Recurrent structure, and achieve the state-of-the-art performance on **WMT 2014 English-to-German translation task**. (2017/06/12)\n\n> The official Tensorflow Implementation can be found in: [tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py).\n\n> To learn more about self-attention mechanism, you could read \"[A Structured Self-attentive Sentence Embedding](https://arxiv.org/abs/1703.03130)\".\n\n<p align=\"center\">\n<img src=\"http://imgur.com/1krF2R6.png\" width=\"250\">\n</p>\n\n\nThe project support training and translation with trained model now.\n\nNote that this project is still a work in progress.\n\n**BPE related parts are not yet fully tested.**\n\n\nIf there is any suggestion or error, feel free to fire an issue to let me know. :)\n\n\n# Usage\n\n## WMT'16 Multimodal Translation: de-en\n\nAn example of training for the WMT'16 Multimodal Translation task (http://www.statmt.org/wmt16/multimodal-task.html).\n\n### 0) Download the spacy language model.\n```bash\n# conda install -c conda-forge spacy \npython -m spacy download en\npython -m spacy download de\n```\n\n### 1) Preprocess the data with torchtext and spacy.\n```bash\npython preprocess.py -lang_src de -lang_trg en -share_vocab -save_data m30k_deen_shr.pkl\n```\n\n### 2) Train the model\n```bash\npython train.py -data_pkl m30k_deen_shr.pkl -log m30k_deen_shr -embs_share_weight -proj_share_weight -label_smoothing -output_dir output -b 256 -warmup 128000 -epoch 400\n```\n\n### 3) Test the model\n```bash\npython translate.py -data_pkl m30k_deen_shr.pkl -model trained.chkpt -output prediction.txt\n```\n\n## [(WIP)] WMT'17 Multimodal Translation: de-en w/ BPE \n### 1) Download and preprocess the data with bpe:\n\n> Since the interfaces is not unified, you need to switch the main function call from `main_wo_bpe` to `main`.\n\n```bash\npython preprocess.py -raw_dir /tmp/raw_deen -data_dir ./bpe_deen -save_data bpe_vocab.pkl -codes codes.txt -prefix deen\n```\n\n### 2) Train the model\n```bash\npython train.py -data_pkl ./bpe_deen/bpe_vocab.pkl -train_path ./bpe_deen/deen-train -val_path ./bpe_deen/deen-val -log deen_bpe -embs_share_weight -proj_share_weight -label_smoothing -output_dir output -b 256 -warmup 128000 -epoch 400\n```\n\n### 3) Test the model (not ready)\n- TODO:\n\t- Load vocabulary.\n\t- Perform decoding after the translation.\n---\n# Performance\n## Training\n\n<p align=\"center\">\n<img src=\"https://i.imgur.com/S2EVtJx.png\" width=\"400\">\n<img src=\"https://i.imgur.com/IZQmUKO.png\" width=\"400\">\n</p>\n\n- Parameter settings:\n  - batch size 256 \n  - warmup step 4000 \n  - epoch 200 \n  - lr_mul 0.5\n  - label smoothing \n  - do not apply BPE and shared vocabulary\n  - target embedding / pre-softmax linear layer weight sharing. \n \n  \n## Testing \n- coming soon.\n---\n# TODO\n  - Evaluation on the generated text.\n  - Attention weight plot.\n---\n# Acknowledgement\n- The byte pair encoding parts are borrowed from [subword-nmt](https://github.com/rsennrich/subword-nmt/).\n- The project structure, some scripts and the dataset preprocessing steps are heavily borrowed from [OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py).\n- Thanks for the suggestions from @srush, @iamalbert, @Zessay, @JulesGM, @ZiJianZhao, and @huanghoujing.\n"
        },
        {
          "name": "apply_bpe.py",
          "type": "blob",
          "size": 8.919921875,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n\n\"\"\"Use operations learned with learn_bpe.py to encode a new text.\nThe text will not be smaller, but use only a fixed vocabulary, with rare words\nencoded as variable-length sequences of subword units.\n\nReference:\nRico Sennrich, Barry Haddow and Alexandra Birch (2015). Neural Machine Translation of Rare Words with Subword Units.\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n\"\"\"\n\nfrom __future__ import unicode_literals, division\n\nimport sys\nimport os\nimport inspect\nimport codecs\nimport io\nimport re\nimport warnings\nimport random\n\n\nclass BPE(object):\n\n    def __init__(self, codes, merges=-1, separator='@@', vocab=None, glossaries=None):\n\n        codes.seek(0)\n        offset=1\n\n        # check version information\n        firstline = codes.readline()\n        if firstline.startswith('#version:'):\n            self.version = tuple([int(x) for x in re.sub(r'(\\.0+)*$','', firstline.split()[-1]).split(\".\")])\n            offset += 1\n        else:\n            self.version = (0, 1)\n            codes.seek(0)\n\n        self.bpe_codes = [tuple(item.strip('\\r\\n ').split(' ')) for (n, item) in enumerate(codes) if (n < merges or merges == -1)]\n\n        for i, item in enumerate(self.bpe_codes):\n            if len(item) != 2:\n                sys.stderr.write('Error: invalid line {0} in BPE codes file: {1}\\n'.format(i+offset, ' '.join(item)))\n                sys.stderr.write('The line should exist of exactly two subword units, separated by whitespace\\n')\n                sys.exit(1)\n\n        # some hacking to deal with duplicates (only consider first instance)\n        self.bpe_codes = dict([(code,i) for (i,code) in reversed(list(enumerate(self.bpe_codes)))])\n\n        self.bpe_codes_reverse = dict([(pair[0] + pair[1], pair) for pair,i in self.bpe_codes.items()])\n\n        self.separator = separator\n\n        self.vocab = vocab\n\n        self.glossaries = glossaries if glossaries else []\n\n        self.glossaries_regex = re.compile('^({})$'.format('|'.join(glossaries))) if glossaries else None\n\n        self.cache = {}\n\n    def process_line(self, line, dropout=0):\n        \"\"\"segment line, dealing with leading and trailing whitespace\"\"\"\n\n        out = \"\"\n\n        leading_whitespace = len(line)-len(line.lstrip('\\r\\n '))\n        if leading_whitespace:\n            out += line[:leading_whitespace]\n\n        out += self.segment(line, dropout)\n\n        trailing_whitespace = len(line)-len(line.rstrip('\\r\\n '))\n        if trailing_whitespace and trailing_whitespace != len(line):\n            out += line[-trailing_whitespace:]\n\n        return out\n\n    def segment(self, sentence, dropout=0):\n        \"\"\"segment single sentence (whitespace-tokenized string) with BPE encoding\"\"\"\n        segments = self.segment_tokens(sentence.strip('\\r\\n ').split(' '), dropout)\n        return ' '.join(segments)\n\n    def segment_tokens(self, tokens, dropout=0):\n        \"\"\"segment a sequence of tokens with BPE encoding\"\"\"\n        output = []\n        for word in tokens:\n            # eliminate double spaces\n            if not word:\n                continue\n            new_word = [out for segment in self._isolate_glossaries(word)\n                        for out in encode(segment,\n                                          self.bpe_codes,\n                                          self.bpe_codes_reverse,\n                                          self.vocab,\n                                          self.separator,\n                                          self.version,\n                                          self.cache,\n                                          self.glossaries_regex,\n                                          dropout)]\n\n            for item in new_word[:-1]:\n                output.append(item + self.separator)\n            output.append(new_word[-1])\n\n        return output\n\n    def _isolate_glossaries(self, word):\n        word_segments = [word]\n        for gloss in self.glossaries:\n            word_segments = [out_segments for segment in word_segments\n                                 for out_segments in isolate_glossary(segment, gloss)]\n        return word_segments\n\ndef encode(orig, bpe_codes, bpe_codes_reverse, vocab, separator, version, cache, glossaries_regex=None, dropout=0):\n    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\n    \"\"\"\n\n    if not dropout and orig in cache:\n        return cache[orig]\n\n    if glossaries_regex and glossaries_regex.match(orig):\n        cache[orig] = (orig,)\n        return (orig,)\n\n    if len(orig) == 1:\n        return orig\n\n    if version == (0, 1):\n        word = list(orig) + ['</w>']\n    elif version == (0, 2): # more consistent handling of word-final segments\n        word = list(orig[:-1]) + [orig[-1] + '</w>']\n    else:\n        raise NotImplementedError\n\n    while len(word) > 1:\n\n        # get list of symbol pairs; optionally apply dropout\n        pairs = [(bpe_codes[pair],i,pair) for (i,pair) in enumerate(zip(word, word[1:])) if (not dropout or random.random() > dropout) and pair in bpe_codes]\n\n        if not pairs:\n            break\n\n        #get first merge operation in list of BPE codes\n        bigram = min(pairs)[2]\n\n        # find start position of all pairs that we want to merge\n        positions = [i for (rank,i,pair) in pairs if pair == bigram]\n\n        i = 0\n        new_word = []\n        bigram = ''.join(bigram)\n        for j in positions:\n            # merges are invalid if they start before current position. This can happen if there are overlapping pairs: (x x x -> xx x)\n            if j < i:\n                continue\n            new_word.extend(word[i:j]) # all symbols before merged pair\n            new_word.append(bigram) # merged pair\n            i = j+2 # continue after merged pair\n        new_word.extend(word[i:]) # add all symbols until end of word\n        word = new_word\n\n    # don't print end-of-word symbols\n    if word[-1] == '</w>':\n        word = word[:-1]\n    elif word[-1].endswith('</w>'):\n        word[-1] = word[-1][:-4]\n\n    word = tuple(word)\n    if vocab:\n        word = check_vocab_and_split(word, bpe_codes_reverse, vocab, separator)\n\n    cache[orig] = word\n    return word\n\ndef recursive_split(segment, bpe_codes, vocab, separator, final=False):\n    \"\"\"Recursively split segment into smaller units (by reversing BPE merges)\n    until all units are either in-vocabulary, or cannot be split futher.\"\"\"\n\n    try:\n        if final:\n            left, right = bpe_codes[segment + '</w>']\n            right = right[:-4]\n        else:\n            left, right = bpe_codes[segment]\n    except:\n        #sys.stderr.write('cannot split {0} further.\\n'.format(segment))\n        yield segment\n        return\n\n    if left + separator in vocab:\n        yield left\n    else:\n        for item in recursive_split(left, bpe_codes, vocab, separator, False):\n            yield item\n\n    if (final and right in vocab) or (not final and right + separator in vocab):\n        yield right\n    else:\n        for item in recursive_split(right, bpe_codes, vocab, separator, final):\n            yield item\n\ndef check_vocab_and_split(orig, bpe_codes, vocab, separator):\n    \"\"\"Check for each segment in word if it is in-vocabulary,\n    and segment OOV segments into smaller units by reversing the BPE merge operations\"\"\"\n\n    out = []\n\n    for segment in orig[:-1]:\n        if segment + separator in vocab:\n            out.append(segment)\n        else:\n            #sys.stderr.write('OOV: {0}\\n'.format(segment))\n            for item in recursive_split(segment, bpe_codes, vocab, separator, False):\n                out.append(item)\n\n    segment = orig[-1]\n    if segment in vocab:\n        out.append(segment)\n    else:\n        #sys.stderr.write('OOV: {0}\\n'.format(segment))\n        for item in recursive_split(segment, bpe_codes, vocab, separator, True):\n            out.append(item)\n\n    return out\n\n\ndef read_vocabulary(vocab_file, threshold):\n    \"\"\"read vocabulary file produced by get_vocab.py, and filter according to frequency threshold.\n    \"\"\"\n\n    vocabulary = set()\n\n    for line in vocab_file:\n        word, freq = line.strip('\\r\\n ').split(' ')\n        freq = int(freq)\n        if threshold == None or freq >= threshold:\n            vocabulary.add(word)\n\n    return vocabulary\n\ndef isolate_glossary(word, glossary):\n    \"\"\"\n    Isolate a glossary present inside a word.\n\n    Returns a list of subwords. In which all 'glossary' glossaries are isolated \n\n    For example, if 'USA' is the glossary and '1934USABUSA' the word, the return value is:\n        ['1934', 'USA', 'B', 'USA']\n    \"\"\"\n    # regex equivalent of (if word == glossary or glossary not in word)\n    if re.match('^'+glossary+'$', word) or not re.search(glossary, word):\n        return [word]\n    else:\n        segments = re.split(r'({})'.format(glossary), word)\n        segments, ending = segments[:-1], segments[-1]\n        segments = list(filter(None, segments)) # Remove empty strings in regex group.\n        return segments + [ending.strip('\\r\\n ')] if ending != '' else segments\n"
        },
        {
          "name": "learn_bpe.py",
          "type": "blob",
          "size": 9.0224609375,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Author: Rico Sennrich\n\n\"\"\"Use byte pair encoding (BPE) to learn a variable-length encoding of the vocabulary in a text.\nUnlike the original BPE, it does not compress the plain text, but can be used to reduce the vocabulary\nof a text to a configurable number of symbols, with only a small increase in the number of tokens.\n\nReference:\nRico Sennrich, Barry Haddow and Alexandra Birch (2016). Neural Machine Translation of Rare Words with Subword Units.\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n\"\"\"\n\nfrom __future__ import unicode_literals\n\nimport os\nimport sys\nimport inspect\nimport codecs\nimport re\nimport copy\nimport warnings\nfrom collections import defaultdict, Counter\n\n\ndef update_vocabulary(vocab, file_name, is_dict=False):\n    \"\"\"Read text and return dictionary that encodes vocabulary\n    \"\"\"\n\n    #vocab = Counter()\n    with codecs.open(file_name, encoding='utf-8') as fobj:\n        for i, line in enumerate(fobj):\n            if is_dict:\n                try:\n                    word, count = line.strip('\\r\\n ').split(' ')\n                except:\n                    print('Failed reading vocabulary file at line {0}: {1}'.format(i, line))\n                    sys.exit(1)\n                vocab[word] += int(count)\n            else:\n                for word in line.strip('\\r\\n ').split(' '):\n                    if word:\n                        vocab[word] += 1\n    return vocab\n\n\ndef update_pair_statistics(pair, changed, stats, indices):\n    \"\"\"Minimally update the indices and frequency of symbol pairs\n\n    if we merge a pair of symbols, only pairs that overlap with occurrences\n    of this pair are affected, and need to be updated.\n    \"\"\"\n    stats[pair] = 0\n    indices[pair] = defaultdict(int)\n    first, second = pair\n    new_pair = first+second\n    for j, word, old_word, freq in changed:\n\n        # find all instances of pair, and update frequency/indices around it\n        i = 0\n        while True:\n            # find first symbol\n            try:\n                i = old_word.index(first, i)\n            except ValueError:\n                break\n            # if first symbol is followed by second symbol, we've found an occurrence of pair (old_word[i:i+2])\n            if i < len(old_word)-1 and old_word[i+1] == second:\n                # assuming a symbol sequence \"A B C\", if \"B C\" is merged, reduce the frequency of \"A B\"\n                if i:\n                    prev = old_word[i-1:i+1]\n                    stats[prev] -= freq\n                    indices[prev][j] -= 1\n                if i < len(old_word)-2:\n                    # assuming a symbol sequence \"A B C B\", if \"B C\" is merged, reduce the frequency of \"C B\".\n                    # however, skip this if the sequence is A B C B C, because the frequency of \"C B\" will be reduced by the previous code block\n                    if old_word[i+2] != first or i >= len(old_word)-3 or old_word[i+3] != second:\n                        nex = old_word[i+1:i+3]\n                        stats[nex] -= freq\n                        indices[nex][j] -= 1\n                i += 2\n            else:\n                i += 1\n\n        i = 0\n        while True:\n            try:\n                # find new pair\n                i = word.index(new_pair, i)\n            except ValueError:\n                break\n            # assuming a symbol sequence \"A BC D\", if \"B C\" is merged, increase the frequency of \"A BC\"\n            if i:\n                prev = word[i-1:i+1]\n                stats[prev] += freq\n                indices[prev][j] += 1\n            # assuming a symbol sequence \"A BC B\", if \"B C\" is merged, increase the frequency of \"BC B\"\n            # however, if the sequence is A BC BC, skip this step because the count of \"BC BC\" will be incremented by the previous code block\n            if i < len(word)-1 and word[i+1] != new_pair:\n                nex = word[i:i+2]\n                stats[nex] += freq\n                indices[nex][j] += 1\n            i += 1\n\n\ndef get_pair_statistics(vocab):\n    \"\"\"Count frequency of all symbol pairs, and create index\"\"\"\n\n    # data structure of pair frequencies\n    stats = defaultdict(int)\n\n    #index from pairs to words\n    indices = defaultdict(lambda: defaultdict(int))\n\n    for i, (word, freq) in enumerate(vocab):\n        prev_char = word[0]\n        for char in word[1:]:\n            stats[prev_char, char] += freq\n            indices[prev_char, char][i] += 1\n            prev_char = char\n\n    return stats, indices\n\n\ndef replace_pair(pair, vocab, indices):\n    \"\"\"Replace all occurrences of a symbol pair ('A', 'B') with a new symbol 'AB'\"\"\"\n    first, second = pair\n    pair_str = ''.join(pair)\n    pair_str = pair_str.replace('\\\\','\\\\\\\\')\n    changes = []\n    pattern = re.compile(r'(?<!\\S)' + re.escape(first + ' ' + second) + r'(?!\\S)')\n    if sys.version_info < (3, 0):\n        iterator = indices[pair].iteritems()\n    else:\n        iterator = indices[pair].items()\n    for j, freq in iterator:\n        if freq < 1:\n            continue\n        word, freq = vocab[j]\n        new_word = ' '.join(word)\n        new_word = pattern.sub(pair_str, new_word)\n        new_word = tuple(new_word.split(' '))\n\n        vocab[j] = (new_word, freq)\n        changes.append((j, new_word, word, freq))\n\n    return changes\n\ndef prune_stats(stats, big_stats, threshold):\n    \"\"\"Prune statistics dict for efficiency of max()\n\n    The frequency of a symbol pair never increases, so pruning is generally safe\n    (until we the most frequent pair is less frequent than a pair we previously pruned)\n    big_stats keeps full statistics for when we need to access pruned items\n    \"\"\"\n    for item,freq in list(stats.items()):\n        if freq < threshold:\n            del stats[item]\n            if freq < 0:\n                big_stats[item] += freq\n            else:\n                big_stats[item] = freq\n\n\ndef learn_bpe(infile_names, outfile_name, num_symbols, min_frequency=2, verbose=False, is_dict=False, total_symbols=False):\n    \"\"\"Learn num_symbols BPE operations from vocabulary, and write to outfile.\n    \"\"\"\n    sys.stderr = codecs.getwriter('UTF-8')(sys.stderr.buffer)\n    sys.stdout = codecs.getwriter('UTF-8')(sys.stdout.buffer)\n    sys.stdin = codecs.getreader('UTF-8')(sys.stdin.buffer)\n\n    #vocab = get_vocabulary(infile, is_dict)\n    vocab = Counter()\n    for f in infile_names:\n        sys.stderr.write(f'Collecting vocab from {f}\\n')\n        vocab = update_vocabulary(vocab, f, is_dict)\n\n    vocab = dict([(tuple(x[:-1])+(x[-1]+'</w>',) ,y) for (x,y) in vocab.items()])\n    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n\n    stats, indices = get_pair_statistics(sorted_vocab)\n    big_stats = copy.deepcopy(stats)\n\n    if total_symbols:\n        uniq_char_internal = set()\n        uniq_char_final = set()\n        for word in vocab:\n            for char in word[:-1]:\n                uniq_char_internal.add(char)\n            uniq_char_final.add(word[-1])\n        sys.stderr.write('Number of word-internal characters: {0}\\n'.format(len(uniq_char_internal)))\n        sys.stderr.write('Number of word-final characters: {0}\\n'.format(len(uniq_char_final)))\n        sys.stderr.write('Reducing number of merge operations by {0}\\n'.format(len(uniq_char_internal) + len(uniq_char_final)))\n        num_symbols -= len(uniq_char_internal) + len(uniq_char_final)\n\n\n    sys.stderr.write(f'Write vocab file to {outfile_name}')\n    with codecs.open(outfile_name, 'w', encoding='utf-8') as outfile:\n        # version 0.2 changes the handling of the end-of-word token ('</w>');\n        # version numbering allows bckward compatibility\n\n        outfile.write('#version: 0.2\\n')\n        # threshold is inspired by Zipfian assumption, but should only affect speed\n        threshold = max(stats.values()) / 10\n        for i in range(num_symbols):\n            if stats:\n                most_frequent = max(stats, key=lambda x: (stats[x], x))\n\n            # we probably missed the best pair because of pruning; go back to full statistics\n            if not stats or (i and stats[most_frequent] < threshold):\n                prune_stats(stats, big_stats, threshold)\n                stats = copy.deepcopy(big_stats)\n                most_frequent = max(stats, key=lambda x: (stats[x], x))\n                # threshold is inspired by Zipfian assumption, but should only affect speed\n                threshold = stats[most_frequent] * i/(i+10000.0)\n                prune_stats(stats, big_stats, threshold)\n\n            if stats[most_frequent] < min_frequency:\n                sys.stderr.write(f'no pair has frequency >= {min_frequency}. Stopping\\n')\n                break\n\n            if verbose:\n                sys.stderr.write('pair {0}: {1} {2} -> {1}{2} (frequency {3})\\n'.format(\n                    i, most_frequent[0], most_frequent[1], stats[most_frequent]))\n            outfile.write('{0} {1}\\n'.format(*most_frequent))\n            changes = replace_pair(most_frequent, sorted_vocab, indices)\n            update_pair_statistics(most_frequent, changes, stats, indices)\n            stats[most_frequent] = 0\n            if not i % 100:\n                prune_stats(stats, big_stats, threshold)\n\n"
        },
        {
          "name": "preprocess.py",
          "type": "blob",
          "size": 12.349609375,
          "content": "''' Handling the data io '''\nimport os\nimport argparse\nimport logging\nimport dill as pickle\nimport urllib\nfrom tqdm import tqdm\nimport sys\nimport codecs\nimport spacy\nimport torch\nimport tarfile\nimport torchtext.data\nimport torchtext.datasets\nfrom torchtext.datasets import TranslationDataset\nimport transformer.Constants as Constants\nfrom learn_bpe import learn_bpe\nfrom apply_bpe import BPE\n\n\n__author__ = \"Yu-Hsiang Huang\"\n\n\n_TRAIN_DATA_SOURCES = [\n    {\"url\": \"http://data.statmt.org/wmt17/translation-task/\" \\\n             \"training-parallel-nc-v12.tgz\",\n     \"trg\": \"news-commentary-v12.de-en.en\",\n     \"src\": \"news-commentary-v12.de-en.de\"},\n    #{\"url\": \"http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgz\",\n    # \"trg\": \"commoncrawl.de-en.en\",\n    # \"src\": \"commoncrawl.de-en.de\"},\n    #{\"url\": \"http://www.statmt.org/wmt13/training-parallel-europarl-v7.tgz\",\n    # \"trg\": \"europarl-v7.de-en.en\",\n    # \"src\": \"europarl-v7.de-en.de\"}\n    ]\n\n_VAL_DATA_SOURCES = [\n    {\"url\": \"http://data.statmt.org/wmt17/translation-task/dev.tgz\",\n     \"trg\": \"newstest2013.en\",\n     \"src\": \"newstest2013.de\"}]\n\n_TEST_DATA_SOURCES = [\n    {\"url\": \"https://storage.googleapis.com/tf-perf-public/\" \\\n                \"official_transformer/test_data/newstest2014.tgz\",\n     \"trg\": \"newstest2014.en\",\n     \"src\": \"newstest2014.de\"}]\n\n\nclass TqdmUpTo(tqdm):\n    def update_to(self, b=1, bsize=1, tsize=None):\n        if tsize is not None:\n            self.total = tsize\n        self.update(b * bsize - self.n)\n\n\ndef file_exist(dir_name, file_name):\n    for sub_dir, _, files in os.walk(dir_name):\n        if file_name in files:\n            return os.path.join(sub_dir, file_name)\n    return None\n\n\ndef download_and_extract(download_dir, url, src_filename, trg_filename):\n    src_path = file_exist(download_dir, src_filename)\n    trg_path = file_exist(download_dir, trg_filename)\n\n    if src_path and trg_path:\n        sys.stderr.write(f\"Already downloaded and extracted {url}.\\n\")\n        return src_path, trg_path\n\n    compressed_file = _download_file(download_dir, url)\n\n    sys.stderr.write(f\"Extracting {compressed_file}.\\n\")\n    with tarfile.open(compressed_file, \"r:gz\") as corpus_tar:\n        corpus_tar.extractall(download_dir)\n\n    src_path = file_exist(download_dir, src_filename)\n    trg_path = file_exist(download_dir, trg_filename)\n\n    if src_path and trg_path:\n        return src_path, trg_path\n\n    raise OSError(f\"Download/extraction failed for url {url} to path {download_dir}\")\n\n\ndef _download_file(download_dir, url):\n    filename = url.split(\"/\")[-1]\n    if file_exist(download_dir, filename):\n        sys.stderr.write(f\"Already downloaded: {url} (at {filename}).\\n\")\n    else:\n        sys.stderr.write(f\"Downloading from {url} to {filename}.\\n\")\n        with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=filename) as t:\n            urllib.request.urlretrieve(url, filename=filename, reporthook=t.update_to)\n    return filename\n\n\ndef get_raw_files(raw_dir, sources):\n    raw_files = { \"src\": [], \"trg\": [], }\n    for d in sources:\n        src_file, trg_file = download_and_extract(raw_dir, d[\"url\"], d[\"src\"], d[\"trg\"])\n        raw_files[\"src\"].append(src_file)\n        raw_files[\"trg\"].append(trg_file)\n    return raw_files\n\n\ndef mkdir_if_needed(dir_name):\n    if not os.path.isdir(dir_name):\n        os.makedirs(dir_name)\n\n\ndef compile_files(raw_dir, raw_files, prefix):\n    src_fpath = os.path.join(raw_dir, f\"raw-{prefix}.src\")\n    trg_fpath = os.path.join(raw_dir, f\"raw-{prefix}.trg\")\n\n    if os.path.isfile(src_fpath) and os.path.isfile(trg_fpath):\n        sys.stderr.write(f\"Merged files found, skip the merging process.\\n\")\n        return src_fpath, trg_fpath\n\n    sys.stderr.write(f\"Merge files into two files: {src_fpath} and {trg_fpath}.\\n\")\n\n    with open(src_fpath, 'w') as src_outf, open(trg_fpath, 'w') as trg_outf:\n        for src_inf, trg_inf in zip(raw_files['src'], raw_files['trg']):\n            sys.stderr.write(f'  Input files: \\n'\\\n                    f'    - SRC: {src_inf}, and\\n' \\\n                    f'    - TRG: {trg_inf}.\\n')\n            with open(src_inf, newline='\\n') as src_inf, open(trg_inf, newline='\\n') as trg_inf:\n                cntr = 0\n                for i, line in enumerate(src_inf):\n                    cntr += 1\n                    src_outf.write(line.replace('\\r', ' ').strip() + '\\n')\n                for j, line in enumerate(trg_inf):\n                    cntr -= 1\n                    trg_outf.write(line.replace('\\r', ' ').strip() + '\\n')\n                assert cntr == 0, 'Number of lines in two files are inconsistent.'\n    return src_fpath, trg_fpath\n\n\ndef encode_file(bpe, in_file, out_file):\n    sys.stderr.write(f\"Read raw content from {in_file} and \\n\"\\\n            f\"Write encoded content to {out_file}\\n\")\n    \n    with codecs.open(in_file, encoding='utf-8') as in_f:\n        with codecs.open(out_file, 'w', encoding='utf-8') as out_f:\n            for line in in_f:\n                out_f.write(bpe.process_line(line))\n\n\ndef encode_files(bpe, src_in_file, trg_in_file, data_dir, prefix):\n    src_out_file = os.path.join(data_dir, f\"{prefix}.src\")\n    trg_out_file = os.path.join(data_dir, f\"{prefix}.trg\")\n\n    if os.path.isfile(src_out_file) and os.path.isfile(trg_out_file):\n        sys.stderr.write(f\"Encoded files found, skip the encoding process ...\\n\")\n\n    encode_file(bpe, src_in_file, src_out_file)\n    encode_file(bpe, trg_in_file, trg_out_file)\n    return src_out_file, trg_out_file\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-raw_dir', required=True)\n    parser.add_argument('-data_dir', required=True)\n    parser.add_argument('-codes', required=True)\n    parser.add_argument('-save_data', required=True)\n    parser.add_argument('-prefix', required=True)\n    parser.add_argument('-max_len', type=int, default=100)\n    parser.add_argument('--symbols', '-s', type=int, default=32000, help=\"Vocabulary size\")\n    parser.add_argument(\n        '--min-frequency', type=int, default=6, metavar='FREQ',\n        help='Stop if no symbol pair has frequency >= FREQ (default: %(default)s))')\n    parser.add_argument('--dict-input', action=\"store_true\",\n        help=\"If set, input file is interpreted as a dictionary where each line contains a word-count pair\")\n    parser.add_argument(\n        '--separator', type=str, default='@@', metavar='STR',\n        help=\"Separator between non-final subword units (default: '%(default)s'))\")\n    parser.add_argument('--total-symbols', '-t', action=\"store_true\")\n    opt = parser.parse_args()\n\n    # Create folder if needed.\n    mkdir_if_needed(opt.raw_dir)\n    mkdir_if_needed(opt.data_dir)\n\n    # Download and extract raw data.\n    raw_train = get_raw_files(opt.raw_dir, _TRAIN_DATA_SOURCES)\n    raw_val = get_raw_files(opt.raw_dir, _VAL_DATA_SOURCES)\n    raw_test = get_raw_files(opt.raw_dir, _TEST_DATA_SOURCES)\n\n    # Merge files into one.\n    train_src, train_trg = compile_files(opt.raw_dir, raw_train, opt.prefix + '-train')\n    val_src, val_trg = compile_files(opt.raw_dir, raw_val, opt.prefix + '-val')\n    test_src, test_trg = compile_files(opt.raw_dir, raw_test, opt.prefix + '-test')\n\n    # Build up the code from training files if not exist\n    opt.codes = os.path.join(opt.data_dir, opt.codes)\n    if not os.path.isfile(opt.codes):\n        sys.stderr.write(f\"Collect codes from training data and save to {opt.codes}.\\n\")\n        learn_bpe(raw_train['src'] + raw_train['trg'], opt.codes, opt.symbols, opt.min_frequency, True)\n    sys.stderr.write(f\"BPE codes prepared.\\n\")\n\n    sys.stderr.write(f\"Build up the tokenizer.\\n\")\n    with codecs.open(opt.codes, encoding='utf-8') as codes: \n        bpe = BPE(codes, separator=opt.separator)\n\n    sys.stderr.write(f\"Encoding ...\\n\")\n    encode_files(bpe, train_src, train_trg, opt.data_dir, opt.prefix + '-train')\n    encode_files(bpe, val_src, val_trg, opt.data_dir, opt.prefix + '-val')\n    encode_files(bpe, test_src, test_trg, opt.data_dir, opt.prefix + '-test')\n    sys.stderr.write(f\"Done.\\n\")\n\n\n    field = torchtext.data.Field(\n        tokenize=str.split,\n        lower=True,\n        pad_token=Constants.PAD_WORD,\n        init_token=Constants.BOS_WORD,\n        eos_token=Constants.EOS_WORD)\n\n    fields = (field, field)\n\n    MAX_LEN = opt.max_len\n\n    def filter_examples_with_length(x):\n        return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n\n    enc_train_files_prefix = opt.prefix + '-train'\n    train = TranslationDataset(\n        fields=fields,\n        path=os.path.join(opt.data_dir, enc_train_files_prefix),\n        exts=('.src', '.trg'),\n        filter_pred=filter_examples_with_length)\n\n    from itertools import chain\n    field.build_vocab(chain(train.src, train.trg), min_freq=2)\n\n    data = { 'settings': opt, 'vocab': field, }\n    opt.save_data = os.path.join(opt.data_dir, opt.save_data)\n\n    print('[Info] Dumping the processed data to pickle file', opt.save_data)\n    pickle.dump(data, open(opt.save_data, 'wb'))\n\n\n\ndef main_wo_bpe():\n    '''\n    Usage: python preprocess.py -lang_src de -lang_trg en -save_data multi30k_de_en.pkl -share_vocab\n    '''\n\n    spacy_support_langs = ['de', 'el', 'en', 'es', 'fr', 'it', 'lt', 'nb', 'nl', 'pt']\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-lang_src', required=True, choices=spacy_support_langs)\n    parser.add_argument('-lang_trg', required=True, choices=spacy_support_langs)\n    parser.add_argument('-save_data', required=True)\n    parser.add_argument('-data_src', type=str, default=None)\n    parser.add_argument('-data_trg', type=str, default=None)\n\n    parser.add_argument('-max_len', type=int, default=100)\n    parser.add_argument('-min_word_count', type=int, default=3)\n    parser.add_argument('-keep_case', action='store_true')\n    parser.add_argument('-share_vocab', action='store_true')\n    #parser.add_argument('-ratio', '--train_valid_test_ratio', type=int, nargs=3, metavar=(8,1,1))\n    #parser.add_argument('-vocab', default=None)\n\n    opt = parser.parse_args()\n    assert not any([opt.data_src, opt.data_trg]), 'Custom data input is not support now.'\n    assert not any([opt.data_src, opt.data_trg]) or all([opt.data_src, opt.data_trg])\n    print(opt)\n\n    src_lang_model = spacy.load(opt.lang_src)\n    trg_lang_model = spacy.load(opt.lang_trg)\n\n    def tokenize_src(text):\n        return [tok.text for tok in src_lang_model.tokenizer(text)]\n\n    def tokenize_trg(text):\n        return [tok.text for tok in trg_lang_model.tokenizer(text)]\n\n    SRC = torchtext.data.Field(\n        tokenize=tokenize_src, lower=not opt.keep_case,\n        pad_token=Constants.PAD_WORD, init_token=Constants.BOS_WORD, eos_token=Constants.EOS_WORD)\n\n    TRG = torchtext.data.Field(\n        tokenize=tokenize_trg, lower=not opt.keep_case,\n        pad_token=Constants.PAD_WORD, init_token=Constants.BOS_WORD, eos_token=Constants.EOS_WORD)\n\n    MAX_LEN = opt.max_len\n    MIN_FREQ = opt.min_word_count\n\n    if not all([opt.data_src, opt.data_trg]):\n        assert {opt.lang_src, opt.lang_trg} == {'de', 'en'}\n    else:\n        # Pack custom txt file into example datasets\n        raise NotImplementedError\n\n    def filter_examples_with_length(x):\n        return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n\n    train, val, test = torchtext.datasets.Multi30k.splits(\n            exts = ('.' + opt.lang_src, '.' + opt.lang_trg),\n            fields = (SRC, TRG),\n            filter_pred=filter_examples_with_length)\n\n    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n    print('[Info] Get source language vocabulary size:', len(SRC.vocab))\n    TRG.build_vocab(train.trg, min_freq=MIN_FREQ)\n    print('[Info] Get target language vocabulary size:', len(TRG.vocab))\n\n    if opt.share_vocab:\n        print('[Info] Merging two vocabulary ...')\n        for w, _ in SRC.vocab.stoi.items():\n            # TODO: Also update the `freq`, although it is not likely to be used.\n            if w not in TRG.vocab.stoi:\n                TRG.vocab.stoi[w] = len(TRG.vocab.stoi)\n        TRG.vocab.itos = [None] * len(TRG.vocab.stoi)\n        for w, i in TRG.vocab.stoi.items():\n            TRG.vocab.itos[i] = w\n        SRC.vocab.stoi = TRG.vocab.stoi\n        SRC.vocab.itos = TRG.vocab.itos\n        print('[Info] Get merged vocabulary size:', len(TRG.vocab))\n\n\n    data = {\n        'settings': opt,\n        'vocab': {'src': SRC, 'trg': TRG},\n        'train': train.examples,\n        'valid': val.examples,\n        'test': test.examples}\n\n    print('[Info] Dumping the processed data to pickle file', opt.save_data)\n    pickle.dump(data, open(opt.save_data, 'wb'))\n\n\nif __name__ == '__main__':\n    main_wo_bpe()\n    #main()\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1650390625,
          "content": "dill==0.3.3\nmsgpack-numpy==0.4.7.1\nmsgpack-python==1.0.2\npython==3.6.12\npytorch==1.3.1\nspacy==2.3.5\ntensorboard==1.14.0\ntensorflow==1.14.0\nterminado==0.9.2\ntqdm==4.56.0\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 12.8642578125,
          "content": "'''\nThis script handles the training process.\n'''\n\nimport argparse\nimport math\nimport time\nimport dill as pickle\nfrom tqdm import tqdm\nimport numpy as np\nimport random\nimport os\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchtext.data import Field, Dataset, BucketIterator\nfrom torchtext.datasets import TranslationDataset\n\nimport transformer.Constants as Constants\nfrom transformer.Models import Transformer\nfrom transformer.Optim import ScheduledOptim\n\n__author__ = \"Yu-Hsiang Huang\"\n\ndef cal_performance(pred, gold, trg_pad_idx, smoothing=False):\n    ''' Apply label smoothing if needed '''\n\n    loss = cal_loss(pred, gold, trg_pad_idx, smoothing=smoothing)\n\n    pred = pred.max(1)[1]\n    gold = gold.contiguous().view(-1)\n    non_pad_mask = gold.ne(trg_pad_idx)\n    n_correct = pred.eq(gold).masked_select(non_pad_mask).sum().item()\n    n_word = non_pad_mask.sum().item()\n\n    return loss, n_correct, n_word\n\n\ndef cal_loss(pred, gold, trg_pad_idx, smoothing=False):\n    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n\n    gold = gold.contiguous().view(-1)\n\n    if smoothing:\n        eps = 0.1\n        n_class = pred.size(1)\n\n        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n        log_prb = F.log_softmax(pred, dim=1)\n\n        non_pad_mask = gold.ne(trg_pad_idx)\n        loss = -(one_hot * log_prb).sum(dim=1)\n        loss = loss.masked_select(non_pad_mask).sum()  # average later\n    else:\n        loss = F.cross_entropy(pred, gold, ignore_index=trg_pad_idx, reduction='sum')\n    return loss\n\n\ndef patch_src(src, pad_idx):\n    src = src.transpose(0, 1)\n    return src\n\n\ndef patch_trg(trg, pad_idx):\n    trg = trg.transpose(0, 1)\n    trg, gold = trg[:, :-1], trg[:, 1:].contiguous().view(-1)\n    return trg, gold\n\n\ndef train_epoch(model, training_data, optimizer, opt, device, smoothing):\n    ''' Epoch operation in training phase'''\n\n    model.train()\n    total_loss, n_word_total, n_word_correct = 0, 0, 0 \n\n    desc = '  - (Training)   '\n    for batch in tqdm(training_data, mininterval=2, desc=desc, leave=False):\n\n        # prepare data\n        src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n        trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n\n        # forward\n        optimizer.zero_grad()\n        pred = model(src_seq, trg_seq)\n\n        # backward and update parameters\n        loss, n_correct, n_word = cal_performance(\n            pred, gold, opt.trg_pad_idx, smoothing=smoothing) \n        loss.backward()\n        optimizer.step_and_update_lr()\n\n        # note keeping\n        n_word_total += n_word\n        n_word_correct += n_correct\n        total_loss += loss.item()\n\n    loss_per_word = total_loss/n_word_total\n    accuracy = n_word_correct/n_word_total\n    return loss_per_word, accuracy\n\n\ndef eval_epoch(model, validation_data, device, opt):\n    ''' Epoch operation in evaluation phase '''\n\n    model.eval()\n    total_loss, n_word_total, n_word_correct = 0, 0, 0\n\n    desc = '  - (Validation) '\n    with torch.no_grad():\n        for batch in tqdm(validation_data, mininterval=2, desc=desc, leave=False):\n\n            # prepare data\n            src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n            trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n\n            # forward\n            pred = model(src_seq, trg_seq)\n            loss, n_correct, n_word = cal_performance(\n                pred, gold, opt.trg_pad_idx, smoothing=False)\n\n            # note keeping\n            n_word_total += n_word\n            n_word_correct += n_correct\n            total_loss += loss.item()\n\n    loss_per_word = total_loss/n_word_total\n    accuracy = n_word_correct/n_word_total\n    return loss_per_word, accuracy\n\n\ndef train(model, training_data, validation_data, optimizer, device, opt):\n    ''' Start training '''\n\n    # Use tensorboard to plot curves, e.g. perplexity, accuracy, learning rate\n    if opt.use_tb:\n        print(\"[Info] Use Tensorboard\")\n        from torch.utils.tensorboard import SummaryWriter\n        tb_writer = SummaryWriter(log_dir=os.path.join(opt.output_dir, 'tensorboard'))\n\n    log_train_file = os.path.join(opt.output_dir, 'train.log')\n    log_valid_file = os.path.join(opt.output_dir, 'valid.log')\n\n    print('[Info] Training performance will be written to file: {} and {}'.format(\n        log_train_file, log_valid_file))\n\n    with open(log_train_file, 'w') as log_tf, open(log_valid_file, 'w') as log_vf:\n        log_tf.write('epoch,loss,ppl,accuracy\\n')\n        log_vf.write('epoch,loss,ppl,accuracy\\n')\n\n    def print_performances(header, ppl, accu, start_time, lr):\n        print('  - {header:12} ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, lr: {lr:8.5f}, '\\\n              'elapse: {elapse:3.3f} min'.format(\n                  header=f\"({header})\", ppl=ppl,\n                  accu=100*accu, elapse=(time.time()-start_time)/60, lr=lr))\n\n    #valid_accus = []\n    valid_losses = []\n    for epoch_i in range(opt.epoch):\n        print('[ Epoch', epoch_i, ']')\n\n        start = time.time()\n        train_loss, train_accu = train_epoch(\n            model, training_data, optimizer, opt, device, smoothing=opt.label_smoothing)\n        train_ppl = math.exp(min(train_loss, 100))\n        # Current learning rate\n        lr = optimizer._optimizer.param_groups[0]['lr']\n        print_performances('Training', train_ppl, train_accu, start, lr)\n\n        start = time.time()\n        valid_loss, valid_accu = eval_epoch(model, validation_data, device, opt)\n        valid_ppl = math.exp(min(valid_loss, 100))\n        print_performances('Validation', valid_ppl, valid_accu, start, lr)\n\n        valid_losses += [valid_loss]\n\n        checkpoint = {'epoch': epoch_i, 'settings': opt, 'model': model.state_dict()}\n\n        if opt.save_mode == 'all':\n            model_name = 'model_accu_{accu:3.3f}.chkpt'.format(accu=100*valid_accu)\n            torch.save(checkpoint, model_name)\n        elif opt.save_mode == 'best':\n            model_name = 'model.chkpt'\n            if valid_loss <= min(valid_losses):\n                torch.save(checkpoint, os.path.join(opt.output_dir, model_name))\n                print('    - [Info] The checkpoint file has been updated.')\n\n        with open(log_train_file, 'a') as log_tf, open(log_valid_file, 'a') as log_vf:\n            log_tf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n                epoch=epoch_i, loss=train_loss,\n                ppl=train_ppl, accu=100*train_accu))\n            log_vf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n                epoch=epoch_i, loss=valid_loss,\n                ppl=valid_ppl, accu=100*valid_accu))\n\n        if opt.use_tb:\n            tb_writer.add_scalars('ppl', {'train': train_ppl, 'val': valid_ppl}, epoch_i)\n            tb_writer.add_scalars('accuracy', {'train': train_accu*100, 'val': valid_accu*100}, epoch_i)\n            tb_writer.add_scalar('learning_rate', lr, epoch_i)\n\ndef main():\n    ''' \n    Usage:\n    python train.py -data_pkl m30k_deen_shr.pkl -log m30k_deen_shr -embs_share_weight -proj_share_weight -label_smoothing -output_dir output -b 256 -warmup 128000\n    '''\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('-data_pkl', default=None)     # all-in-1 data pickle or bpe field\n\n    parser.add_argument('-train_path', default=None)   # bpe encoded data\n    parser.add_argument('-val_path', default=None)     # bpe encoded data\n\n    parser.add_argument('-epoch', type=int, default=10)\n    parser.add_argument('-b', '--batch_size', type=int, default=2048)\n\n    parser.add_argument('-d_model', type=int, default=512)\n    parser.add_argument('-d_inner_hid', type=int, default=2048)\n    parser.add_argument('-d_k', type=int, default=64)\n    parser.add_argument('-d_v', type=int, default=64)\n\n    parser.add_argument('-n_head', type=int, default=8)\n    parser.add_argument('-n_layers', type=int, default=6)\n    parser.add_argument('-warmup','--n_warmup_steps', type=int, default=4000)\n    parser.add_argument('-lr_mul', type=float, default=2.0)\n    parser.add_argument('-seed', type=int, default=None)\n\n    parser.add_argument('-dropout', type=float, default=0.1)\n    parser.add_argument('-embs_share_weight', action='store_true')\n    parser.add_argument('-proj_share_weight', action='store_true')\n    parser.add_argument('-scale_emb_or_prj', type=str, default='prj')\n\n    parser.add_argument('-output_dir', type=str, default=None)\n    parser.add_argument('-use_tb', action='store_true')\n    parser.add_argument('-save_mode', type=str, choices=['all', 'best'], default='best')\n\n    parser.add_argument('-no_cuda', action='store_true')\n    parser.add_argument('-label_smoothing', action='store_true')\n\n    opt = parser.parse_args()\n    opt.cuda = not opt.no_cuda\n    opt.d_word_vec = opt.d_model\n\n    # https://pytorch.org/docs/stable/notes/randomness.html\n    # For reproducibility\n    if opt.seed is not None:\n        torch.manual_seed(opt.seed)\n        torch.backends.cudnn.benchmark = False\n        # torch.set_deterministic(True)\n        np.random.seed(opt.seed)\n        random.seed(opt.seed)\n\n    if not opt.output_dir:\n        print('No experiment result will be saved.')\n        raise\n\n    if not os.path.exists(opt.output_dir):\n        os.makedirs(opt.output_dir)\n\n    if opt.batch_size < 2048 and opt.n_warmup_steps <= 4000:\n        print('[Warning] The warmup steps may be not enough.\\n'\\\n              '(sz_b, warmup) = (2048, 4000) is the official setting.\\n'\\\n              'Using smaller batch w/o longer warmup may cause '\\\n              'the warmup stage ends with only little data trained.')\n\n    device = torch.device('cuda' if opt.cuda else 'cpu')\n\n    #========= Loading Dataset =========#\n\n    if all((opt.train_path, opt.val_path)):\n        training_data, validation_data = prepare_dataloaders_from_bpe_files(opt, device)\n    elif opt.data_pkl:\n        training_data, validation_data = prepare_dataloaders(opt, device)\n    else:\n        raise\n\n    print(opt)\n\n    transformer = Transformer(\n        opt.src_vocab_size,\n        opt.trg_vocab_size,\n        src_pad_idx=opt.src_pad_idx,\n        trg_pad_idx=opt.trg_pad_idx,\n        trg_emb_prj_weight_sharing=opt.proj_share_weight,\n        emb_src_trg_weight_sharing=opt.embs_share_weight,\n        d_k=opt.d_k,\n        d_v=opt.d_v,\n        d_model=opt.d_model,\n        d_word_vec=opt.d_word_vec,\n        d_inner=opt.d_inner_hid,\n        n_layers=opt.n_layers,\n        n_head=opt.n_head,\n        dropout=opt.dropout,\n        scale_emb_or_prj=opt.scale_emb_or_prj).to(device)\n\n    optimizer = ScheduledOptim(\n        optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09),\n        opt.lr_mul, opt.d_model, opt.n_warmup_steps)\n\n    train(transformer, training_data, validation_data, optimizer, device, opt)\n\n\ndef prepare_dataloaders_from_bpe_files(opt, device):\n    batch_size = opt.batch_size\n    MIN_FREQ = 2\n    if not opt.embs_share_weight:\n        raise\n\n    data = pickle.load(open(opt.data_pkl, 'rb'))\n    MAX_LEN = data['settings'].max_len\n    field = data['vocab']\n    fields = (field, field)\n\n    def filter_examples_with_length(x):\n        return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n\n    train = TranslationDataset(\n        fields=fields,\n        path=opt.train_path, \n        exts=('.src', '.trg'),\n        filter_pred=filter_examples_with_length)\n    val = TranslationDataset(\n        fields=fields,\n        path=opt.val_path, \n        exts=('.src', '.trg'),\n        filter_pred=filter_examples_with_length)\n\n    opt.max_token_seq_len = MAX_LEN + 2\n    opt.src_pad_idx = opt.trg_pad_idx = field.vocab.stoi[Constants.PAD_WORD]\n    opt.src_vocab_size = opt.trg_vocab_size = len(field.vocab)\n\n    train_iterator = BucketIterator(train, batch_size=batch_size, device=device, train=True)\n    val_iterator = BucketIterator(val, batch_size=batch_size, device=device)\n    return train_iterator, val_iterator\n\n\ndef prepare_dataloaders(opt, device):\n    batch_size = opt.batch_size\n    data = pickle.load(open(opt.data_pkl, 'rb'))\n\n    opt.max_token_seq_len = data['settings'].max_len\n    opt.src_pad_idx = data['vocab']['src'].vocab.stoi[Constants.PAD_WORD]\n    opt.trg_pad_idx = data['vocab']['trg'].vocab.stoi[Constants.PAD_WORD]\n\n    opt.src_vocab_size = len(data['vocab']['src'].vocab)\n    opt.trg_vocab_size = len(data['vocab']['trg'].vocab)\n\n    #========= Preparing Model =========#\n    if opt.embs_share_weight:\n        assert data['vocab']['src'].vocab.stoi == data['vocab']['trg'].vocab.stoi, \\\n            'To sharing word embedding the src/trg word2idx table shall be the same.'\n\n    fields = {'src': data['vocab']['src'], 'trg':data['vocab']['trg']}\n\n    train = Dataset(examples=data['train'], fields=fields)\n    val = Dataset(examples=data['valid'], fields=fields)\n\n    train_iterator = BucketIterator(train, batch_size=batch_size, device=device, train=True)\n    val_iterator = BucketIterator(val, batch_size=batch_size, device=device)\n\n    return train_iterator, val_iterator\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "train_multi30k_de_en.sh",
          "type": "blob",
          "size": 0.45703125,
          "content": "# Example:\n# gpu=0 lr_mul=2 scale_emb_or_prj=prj bash train_multi30k_de_en.sh\n# The better setting is:\n# gpu=0 lr_mul=0.5 scale_emb_or_prj=emb bash train_multi30k_de_en.sh\nCUDA_VISIBLE_DEVICES=${gpu} python train.py \\\n-data_pkl multi30k_de_en.pkl \\\n-label_smoothing \\\n-proj_share_weight \\\n-scale_emb_or_prj ${scale_emb_or_prj} \\\n-lr_mul ${lr_mul} \\\n-b 256 \\\n-warmup 4000 \\\n-epoch 200 \\\n-seed 1 \\\n-output_dir output/lr_mul_${lr_mul}-scale_${scale_emb_or_prj} \\\n-use_tb\n"
        },
        {
          "name": "transformer",
          "type": "tree",
          "content": null
        },
        {
          "name": "translate.py",
          "type": "blob",
          "size": 3.9814453125,
          "content": "''' Translate input text with trained model. '''\n\nimport torch\nimport argparse\nimport dill as pickle\nfrom tqdm import tqdm\n\nimport transformer.Constants as Constants\nfrom torchtext.data import Dataset\nfrom transformer.Models import Transformer\nfrom transformer.Translator import Translator\n\n\ndef load_model(opt, device):\n\n    checkpoint = torch.load(opt.model, map_location=device)\n    model_opt = checkpoint['settings']\n\n    model = Transformer(\n        model_opt.src_vocab_size,\n        model_opt.trg_vocab_size,\n\n        model_opt.src_pad_idx,\n        model_opt.trg_pad_idx,\n\n        trg_emb_prj_weight_sharing=model_opt.proj_share_weight,\n        emb_src_trg_weight_sharing=model_opt.embs_share_weight,\n        d_k=model_opt.d_k,\n        d_v=model_opt.d_v,\n        d_model=model_opt.d_model,\n        d_word_vec=model_opt.d_word_vec,\n        d_inner=model_opt.d_inner_hid,\n        n_layers=model_opt.n_layers,\n        n_head=model_opt.n_head,\n        dropout=model_opt.dropout).to(device)\n\n    model.load_state_dict(checkpoint['model'])\n    print('[Info] Trained model state loaded.')\n    return model \n\n\ndef main():\n    '''Main Function'''\n\n    parser = argparse.ArgumentParser(description='translate.py')\n\n    parser.add_argument('-model', required=True,\n                        help='Path to model weight file')\n    parser.add_argument('-data_pkl', required=True,\n                        help='Pickle file with both instances and vocabulary.')\n    parser.add_argument('-output', default='pred.txt',\n                        help=\"\"\"Path to output the predictions (each line will\n                        be the decoded sequence\"\"\")\n    parser.add_argument('-beam_size', type=int, default=5)\n    parser.add_argument('-max_seq_len', type=int, default=100)\n    parser.add_argument('-no_cuda', action='store_true')\n\n    # TODO: Translate bpe encoded files \n    #parser.add_argument('-src', required=True,\n    #                    help='Source sequence to decode (one line per sequence)')\n    #parser.add_argument('-vocab', required=True,\n    #                    help='Source sequence to decode (one line per sequence)')\n    # TODO: Batch translation\n    #parser.add_argument('-batch_size', type=int, default=30,\n    #                    help='Batch size')\n    #parser.add_argument('-n_best', type=int, default=1,\n    #                    help=\"\"\"If verbose is set, will output the n_best\n    #                    decoded sentences\"\"\")\n\n    opt = parser.parse_args()\n    opt.cuda = not opt.no_cuda\n\n    data = pickle.load(open(opt.data_pkl, 'rb'))\n    SRC, TRG = data['vocab']['src'], data['vocab']['trg']\n    opt.src_pad_idx = SRC.vocab.stoi[Constants.PAD_WORD]\n    opt.trg_pad_idx = TRG.vocab.stoi[Constants.PAD_WORD]\n    opt.trg_bos_idx = TRG.vocab.stoi[Constants.BOS_WORD]\n    opt.trg_eos_idx = TRG.vocab.stoi[Constants.EOS_WORD]\n\n    test_loader = Dataset(examples=data['test'], fields={'src': SRC, 'trg': TRG})\n    \n    device = torch.device('cuda' if opt.cuda else 'cpu')\n    translator = Translator(\n        model=load_model(opt, device),\n        beam_size=opt.beam_size,\n        max_seq_len=opt.max_seq_len,\n        src_pad_idx=opt.src_pad_idx,\n        trg_pad_idx=opt.trg_pad_idx,\n        trg_bos_idx=opt.trg_bos_idx,\n        trg_eos_idx=opt.trg_eos_idx).to(device)\n\n    unk_idx = SRC.vocab.stoi[SRC.unk_token]\n    with open(opt.output, 'w') as f:\n        for example in tqdm(test_loader, mininterval=2, desc='  - (Test)', leave=False):\n            #print(' '.join(example.src))\n            src_seq = [SRC.vocab.stoi.get(word, unk_idx) for word in example.src]\n            pred_seq = translator.translate_sentence(torch.LongTensor([src_seq]).to(device))\n            pred_line = ' '.join(TRG.vocab.itos[idx] for idx in pred_seq)\n            pred_line = pred_line.replace(Constants.BOS_WORD, '').replace(Constants.EOS_WORD, '')\n            #print(pred_line)\n            f.write(pred_line.strip() + '\\n')\n\n    print('[Info] Finished.')\n\nif __name__ == \"__main__\":\n    '''\n    Usage: python translate.py -model trained.chkpt -data multi30k.pt -no_cuda\n    '''\n    main()\n"
        }
      ]
    }
  ]
}