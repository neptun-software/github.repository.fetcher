{
  "metadata": {
    "timestamp": 1736560638665,
    "page": 273,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "modelscope/FunASR",
      "stars": 7661,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3330078125,
          "content": ".idea\n./__pycache__/\n*/__pycache__/\n*/*/__pycache__/\n*/*/*/__pycache__/\n.DS_Store\ninit_model/\n*.tar.gz\ntest_local/\nRapidASR\nexport/*\n*.pyc\n.eggs\nMaaS-lib\n.gitignore\n.egg*\ndist\nbuild\nfunasr.egg-info\ndocs/_build\nmodelscope\nsamples\n.ipynb_checkpoints\noutputs*\nemotion2vec*\nGPT-SoVITS*\nmodelscope_models\nexamples/aishell/llm_asr_nar/*\n*egg-info\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.1748046875,
          "content": "repos:\n  - repo: https://github.com/psf/black\n    rev: 24.4.0\n    hooks:\n      - id: black\n        args: ['--line-length=100']  # ç¤ºä¾‹å‚æ•°ï¼Œblacké»˜è®¤ä½¿ç”¨4ä¸ªç©ºæ ¼ç¼©è¿›\n"
        },
        {
          "name": "Acknowledge.md",
          "type": "blob",
          "size": 0.8720703125,
          "content": "## Acknowledge\n\n1. We borrowed a lot of code from [Kaldi](http://kaldi-asr.org/) for data preparation.\n2. We borrowed a lot of code from [ESPnet](https://github.com/espnet/espnet). FunASR follows up the training and finetuning pipelines of ESPnet.\n3. We referred [Wenet](https://github.com/wenet-e2e/wenet) for building dataloader for large scale data training.\n4. We acknowledge [ChinaTelecom](https://github.com/zhuzizyf/damo-fsmn-vad-infer-httpserver) for contributing the VAD runtime.\n5. We acknowledge [RapidAI](https://github.com/RapidAI) for contributing the Paraformer and CT_Transformer-punc runtime.\n6. We acknowledge [AiHealthx](http://www.aihealthx.com/) for contributing the websocket service and html5.\n7. We acknowledge [XVERSE](http://www.xverse.cn/index.html) for contributing the grpc service.\n8. We acknowledge [blt](https://github.com/bltcn) for develop and deploy website."
        },
        {
          "name": "Contribution.md",
          "type": "blob",
          "size": 2.3564453125,
          "content": "# Contributing to FunASR\n\nFirst off, thanks for taking the time to contribute! ğŸ‰\n\nThe following is a set of guidelines for contributing to FunASR. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.\n\n## How Can I Contribute?\n\n### Reporting Bugs\n\nThis section guides you through submitting a bug report for FunASR. Following these guidelines helps maintainers and the community understand your report, reproduce the behavior, and find related reports.\n\n- **Ensure the bug was not already reported** by searching on GitHub under Issues.\n- If you're unable to find an open issue addressing the problem, open a new one. Be sure to include a **title and clear description**, as much relevant information as possible, and a **code sample** or an **executable test case** demonstrating the expected behavior that is not occurring.\n\n### Suggesting Enhancements\n\nThis section guides you through submitting an enhancement suggestion for FunASR, including completely new features and minor improvements to existing functionality.\n\n- **Ensure the enhancement was not already suggested** by searching on GitHub under Issues.\n- If you find an enhancement that matches your suggestion, feel free to add your comments to the existing issue.\n- If you don't find an existing issue, you can open a new one. Be sure to include a **title and clear description**, as much relevant information as possible, and a **code sample** or an **executable test case** demonstrating the expected behavior.\n\n### Pull Requests\n\nThe process described here has several goals:\n\n- Maintain FunASR's quality\n- Fix problems that are important to users\n- Engage the community in working toward the best possible FunASR\n\nPlease follow these steps to have your contribution considered by the maintainers:\n\n1. **Fork** the repository.\n2. **Clone** your fork: `git clone https://github.com/alibaba/FunASR.git`\n3. **Create a branch** for your changes: `git checkout -b my-new-feature`\n4. **Make your changes**.\n5. **Commit your changes**: `git commit -am 'Add some feature'`\n6. **Push to the branch**: `git push origin my-new-feature`\n7. **Create a new Pull Request**.\n\n### Code of Conduct\n\nThis project and everyone participating in it is governed by the FunASR Code of Conduct. By participating, you are expected to uphold this code.\n\nThank you for contributing to FunASR!\n"
        },
        {
          "name": "MODEL_LICENSE",
          "type": "blob",
          "size": 5.181640625,
          "content": "FunASR Model Open Source License Agreement\n\nVersion: 1.1\n\nCopyright (C) [2023-2028] [Alibaba Group]. All rights reserved.\n\nThank you for choosing the FunASR open-source model. The FunASR open-source model includes a range of free and open industrial models for you to use, modify, share, and learn from.\n\nTo ensure better community collaboration, we have established the following agreement, and we hope you will read and comply with its terms.\nDefinitions\n\nIn this agreement, [FunASR Software] refers to FunASR open-source model weights and their derivatives, including finetuned models; [You] refers to individuals or organizations using, modifying, sharing, and learning from [FunASR Software].\n\n2 License and Restrictions\n\n2.1 License\n\nYou are free to use, copy, modify, and share [FunASR Software] under the terms of this agreement.\n\n2.2 Restrictions\n\nWhen using, copying, modifying, and sharing [FunASR Software], you must attribute the source and author information and retain relevant model names in [FunASR Software].\n\n3 Responsibility and Risk\n\n[FunASR Software] is provided for reference and learning purposes only, and Alibaba Group assumes no responsibility for any direct or indirect losses resulting from your use or modification of [FunASR Software]. You should assume all risks associated with using and modifying [FunASR Software].\n\n4 Community Conduct Guidelines\n\n4.1 Encouraged Behavior\n\nThe community welcomes developers and users to engage in discussions about [FunASR Software]. Participants are encouraged to interact in a friendly, polite, and respectful manner to foster constructive discussion and collaboration.\n\n4.2 Prohibited Behavior\n\nIndividual or organizational users shall not engage in unjustified denigration, malicious smearing, or baseless insults against [FunASR Software]. Such behavior is considered a violation of the spirit of community cooperation. If a user is found to be engaging in the prohibited behavior mentioned above, it will be considered an automatic forfeiture of all licenses under this agreement.\n\n5 Termination\n\nIf you violate any terms of this agreement, your license will automatically terminate, and you must cease using, copying, modifying, and sharing [FunASR Software].\n\n6 Revisions\n\nThis agreement may be updated and revised occasionally. The revised agreement will be published in the official repository of [FunASR Software] and will take effect automatically. Continuing to use, copy, modify, and share [FunASR Software] indicates your acceptance of the revised agreement.\n\n7 Miscellaneous\n\nThis agreement is governed by the laws of [Country/Region]. If any provision is deemed illegal, invalid, or unenforceable, that provision shall be considered severed from this agreement, and the remaining provisions shall continue to be valid and binding.\n\nIf you have any questions or comments regarding this agreement, please contact us.\n\nCopyright Â© [2023-2028] [Alibaba Group]. All rights reserved.\n\n\nFunASR æ¨¡å‹å¼€æºåè®®\n\nç‰ˆæœ¬å·ï¼š1.1\n\nç‰ˆæƒæ‰€æœ‰ (C) [2023-2028] [é˜¿é‡Œå·´å·´é›†å›¢]ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n\næ„Ÿè°¢æ‚¨é€‰æ‹© FunASR å¼€æºæ¨¡å‹ã€‚FunASR å¼€æºæ¨¡å‹åŒ…å«ä¸€ç³»åˆ—å…è´¹ä¸”å¼€æºçš„å·¥ä¸šæ¨¡å‹ï¼Œè®©å¤§å®¶å¯ä»¥ä½¿ç”¨ã€ä¿®æ”¹ã€åˆ†äº«å’Œå­¦ä¹ è¯¥æ¨¡å‹ã€‚\n\nä¸ºäº†ä¿è¯æ›´å¥½çš„ç¤¾åŒºåˆä½œï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä»¥ä¸‹åè®®ï¼Œå¸Œæœ›æ‚¨ä»”ç»†é˜…è¯»å¹¶éµå®ˆæœ¬åè®®ã€‚\n\n1 å®šä¹‰\n\næœ¬åè®®ä¸­ï¼Œ[FunASR è½¯ä»¶]æŒ‡ FunASR å¼€æºæ¨¡å‹æƒé‡åŠå…¶è¡ç”Ÿå“ï¼ŒåŒ…æ‹¬ Finetune åçš„æ¨¡å‹ï¼›[æ‚¨]æŒ‡ä½¿ç”¨ã€ä¿®æ”¹ã€åˆ†äº«å’Œå­¦ä¹ [FunASR è½¯ä»¶]çš„ä¸ªäººæˆ–ç»„ç»‡ã€‚\n\n2 è®¸å¯å’Œé™åˆ¶\n\n2.1 è®¸å¯\n\næ‚¨å¯ä»¥åœ¨éµå®ˆæœ¬åè®®çš„å‰æä¸‹ï¼Œè‡ªç”±åœ°ä½¿ç”¨ã€å¤åˆ¶ã€ä¿®æ”¹å’Œåˆ†äº«[FunASR è½¯ä»¶]ã€‚\n\n2.2 é™åˆ¶\n\næ‚¨åœ¨ä½¿ç”¨ã€å¤åˆ¶ã€ä¿®æ”¹å’Œåˆ†äº«[FunASR è½¯ä»¶]æ—¶ï¼Œå¿…é¡»æ³¨æ˜å‡ºå¤„ä»¥åŠä½œè€…ä¿¡æ¯ï¼Œå¹¶ä¿ç•™[FunASR è½¯ä»¶]ä¸­ç›¸å…³æ¨¡å‹åç§°ã€‚\n\n3 è´£ä»»å’Œé£é™©æ‰¿æ‹…\n\n[FunASR è½¯ä»¶]ä»…ä½œä¸ºå‚è€ƒå’Œå­¦ä¹ ä½¿ç”¨ï¼Œä¸å¯¹æ‚¨ä½¿ç”¨æˆ–ä¿®æ”¹[FunASR è½¯ä»¶]é€ æˆçš„ä»»ä½•ç›´æ¥æˆ–é—´æ¥æŸå¤±æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚æ‚¨å¯¹[FunASR è½¯ä»¶]çš„ä½¿ç”¨å’Œä¿®æ”¹åº”è¯¥è‡ªè¡Œæ‰¿æ‹…é£é™©ã€‚\n\n4 ç¤¾åŒºè¡Œä¸ºå‡†åˆ™\n\n4.1 æ¬¢è¿äº¤æµ\n\nç¤¾åŒºæ¬¢è¿å¼€å‘è€…ä¸ç”¨æˆ·å¯¹[FunASR è½¯ä»¶]è¿›è¡Œäº¤æµè®¨è®ºã€‚äº¤æµä¸­è¯·æ³¨æ„ä¿æŒå‹å¥½ã€ç¤¼è²Œå’Œæ–‡æ˜ï¼Œä»¥ä¿ƒè¿›å»ºè®¾æ€§çš„è®¨è®ºå’Œåˆä½œã€‚\n\n4.2 ç¦æ­¢è¡Œä¸º\n\nä¸ªäººæˆ–ç»„ç»‡ç”¨æˆ·ä¸å¾—å¯¹[FunASR è½¯ä»¶]è¿›è¡Œæ— ç«¯è¯‹æ¯ã€æ¶æ„æŠ¹é»‘æˆ–å‡­ç©ºè°©éª‚ã€‚æ­¤ç±»è¡Œä¸ºè¢«è§†ä¸ºè¿åç¤¾åŒºåˆä½œç²¾ç¥ã€‚å¦‚è¢«è®¤å®šä»äº‹ä¸Šè¿°ç¦æ­¢è¡Œä¸ºï¼Œå°†è§†ä¸ºè‡ªåŠ¨æ”¾å¼ƒæœ¬åè®®ä¸‹çš„æ‰€æœ‰è®¸å¯ã€‚\n\n5 ç»ˆæ­¢\n\nå¦‚æœæ‚¨è¿åæœ¬åè®®çš„ä»»ä½•æ¡æ¬¾ï¼Œæ‚¨çš„è®¸å¯å°†è‡ªåŠ¨ç»ˆæ­¢ï¼Œæ‚¨å¿…é¡»åœæ­¢ä½¿ç”¨ã€å¤åˆ¶ã€ä¿®æ”¹å’Œåˆ†äº«[FunASR è½¯ä»¶]ã€‚\n\n6 ä¿®è®¢\n\næœ¬åè®®å¯èƒ½ä¼šä¸æ—¶æ›´æ–°å’Œä¿®è®¢ã€‚ä¿®è®¢åçš„åè®®å°†åœ¨[FunASR è½¯ä»¶]å®˜æ–¹ä»“åº“å‘å¸ƒï¼Œå¹¶è‡ªåŠ¨ç”Ÿæ•ˆã€‚å¦‚æœæ‚¨ç»§ç»­ä½¿ç”¨ã€å¤åˆ¶ã€ä¿®æ”¹å’Œåˆ†äº«[FunASR è½¯ä»¶]ï¼Œå³è¡¨ç¤ºæ‚¨åŒæ„ä¿®è®¢åçš„åè®®ã€‚\n\n7 å…¶ä»–è§„å®š\n\næœ¬åè®®å—åˆ°[å›½å®¶/åœ°åŒº] çš„æ³•å¾‹ç®¡è¾–ã€‚å¦‚æœä»»ä½•æ¡æ¬¾è¢«è£å®šä¸ºä¸åˆæ³•ã€æ— æ•ˆæˆ–æ— æ³•æ‰§è¡Œï¼Œåˆ™è¯¥æ¡æ¬¾åº”è¢«è§†ä¸ºä»æœ¬åè®®ä¸­åˆ é™¤ï¼Œè€Œå…¶ä½™æ¡æ¬¾åº”ç»§ç»­æœ‰æ•ˆå¹¶å…·æœ‰çº¦æŸåŠ›ã€‚\n\nå¦‚æœæ‚¨å¯¹æœ¬åè®®æœ‰ä»»ä½•é—®é¢˜æˆ–æ„è§ï¼Œè¯·è”ç³»æˆ‘ä»¬ã€‚\n\nç‰ˆæƒæ‰€æœ‰Â© [2023-2028] [é˜¿é‡Œå·´å·´é›†å›¢]ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚\n"
        },
        {
          "name": "MinMo_gitlab",
          "type": "blob",
          "size": 0.0146484375,
          "content": "../MinMo_gitlab"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 28.3466796875,
          "content": "[//]: # (<div align=\"left\"><img src=\"docs/images/funasr_logo.jpg\" width=\"400\"/></div>)\n\n([ç®€ä½“ä¸­æ–‡](./README_zh.md)|English)\n\n[//]: # (# FunASR: A Fundamental End-to-End Speech Recognition Toolkit)\n\n[![SVG Banners](https://svg-banners.vercel.app/api?type=origin&text1=FunASRğŸ¤ &text2=ğŸ’–%20A%20Fundamental%20End-to-End%20Speech%20Recognition%20Toolkit&width=800&height=210)](https://github.com/Akshay090/svg-banners)\n\n[![PyPI](https://img.shields.io/pypi/v/funasr)](https://pypi.org/project/funasr/)\n\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/3839\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/3839\" alt=\"alibaba-damo-academy%2FFunASR | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n\n<strong>FunASR</strong> hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training & finetuning of the industrial-grade speech recognition model, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for Funï¼\n\n[**Highlights**](#highlights)\n| [**News**](https://github.com/alibaba-damo-academy/FunASR#whats-new) \n| [**Installation**](#installation)\n| [**Quick Start**](#quick-start)\n| [**Tutorial**](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/tutorial/README.md)\n| [**Runtime**](./runtime/readme.md)\n| [**Model Zoo**](#model-zoo)\n| [**Contact**](#contact)\n\n\n\n\n<a name=\"highlights\"></a>\n## Highlights\n- FunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.\n- We have released a vast collection of academic and industrial pretrained models on the [ModelScope](https://www.modelscope.cn/models?page=1&tasks=auto-speech-recognition) and [huggingface](https://huggingface.co/FunASR), which can be accessed through our [Model Zoo](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/model_zoo/modelscope_models.md). The representative [Paraformer-large](https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary), a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the [service deployment document](runtime/readme_cn.md). \n\n\n<a name=\"whats-new\"></a>\n## What's new:\n- 2024/10/29: Real-time Transcription Service 1.12 released, The 2pass-offline mode supports the SensevoiceSmal modelï¼›([docs](runtime/readme.md));\n- 2024/10/10ï¼šAdded support for the Whisper-large-v3-turbo model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the [modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).\n- 2024/09/26: Offline File Transcription Service 4.6, Offline File Transcription Service of English 1.7, Real-time Transcription Service 1.11 released, fix memory leak & Support the SensevoiceSmall onnx modelï¼›File Transcription Service 2.0 GPU released, Fix GPU memory leak; ([docs](runtime/readme.md));\n- 2024/09/25ï¼škeyword spotting models are new supported. Supports fine-tuning and inference for four models: [fsmn_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [fsmn_kws_mt](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [sanm_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-offline), [sanm_kws_streaming](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online).\n- 2024/07/04ï¼š[SenseVoice](https://github.com/FunAudioLLM/SenseVoice) is a speech foundation model with multiple speech understanding capabilities, including ASR, LID, SER, and AED.\n- 2024/07/01: Offline File Transcription Service GPU 1.1 released, optimize BladeDISC model compatibility issues; ref to ([docs](runtime/readme.md))\n- 2024/06/27: Offline File Transcription Service GPU 1.0 released, supporting dynamic batch processing and multi-threading concurrency. In the long audio test set, the single-thread RTF is 0.0076, and multi-threads' speedup is 1200+ (compared to 330+ on CPU); ref to ([docs](runtime/readme.md))\n- 2024/05/15ï¼šemotion recognition models are new supported. [emotion2vec+large](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)ï¼Œ[emotion2vec+base](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary)ï¼Œ[emotion2vec+seed](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary). currently supports the following categories: 0: angry 1: happy 2: neutral 3: sad 4: unknown.\n- 2024/05/15: Offline File Transcription Service 4.5, Offline File Transcription Service of English 1.6, Real-time Transcription Service 1.10 released, adapting to FunASR 1.0 model structureï¼›([docs](runtime/readme.md))\n\n<details><summary>Full Changelog</summary>\n\n- 2024/03/05ï¼šAdded the Qwen-Audio and Qwen-Audio-Chat large-scale audio-text multimodal models, which have topped multiple audio domain leaderboards. These models support speech dialogue, [usage](examples/industrial_data_pretraining/qwen_audio).\n- 2024/03/05ï¼šAdded support for the Whisper-large-v3 model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the[modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).\n- 2024/03/05: Offline File Transcription Service 4.4, Offline File Transcription Service of English 1.5ï¼ŒReal-time Transcription Service 1.9 releasedï¼Œdocker image supports ARM64 platform, update modelscopeï¼›([docs](runtime/readme.md))\n- 2024/01/30ï¼šfunasr-1.0 has been released ([docs](https://github.com/alibaba-damo-academy/FunASR/discussions/1319))\n- 2024/01/30ï¼šemotion recognition models are new supported. [model link](https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary), modified from [repo](https://github.com/ddlBoJack/emotion2vec).\n- 2024/01/25: Offline File Transcription Service 4.2, Offline File Transcription Service of English 1.3 releasedï¼Œoptimized the VAD (Voice Activity Detection) data processing method, significantly reducing peak memory usage, memory leak optimization; Real-time Transcription Service 1.7 releasedï¼Œoptimizatized the client-sideï¼›([docs](runtime/readme.md))\n- 2024/01/09: The Funasr SDK for Windows version 2.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin 4.1, The offline file transcription service (CPU) of English 1.2, The real-time transcription service (CPU) of Mandarin 1.6. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))\n- 2024/01/03: File Transcription Service 4.0 released, Added support for 8k models, optimized timestamp mismatch issues and added sentence-level timestamps, improved the effectiveness of English word FST hotwords, supported automated configuration of thread parameters, and fixed known crash issues as well as memory leak problems, refer to ([docs](runtime/readme.md#file-transcription-service-mandarin-cpu)).\n- 2024/01/03: Real-time Transcription Service 1.6 releasedï¼ŒThe 2pass-offline mode supports Ngram language model decoding and WFST hotwords, while also addressing known crash issues and memory leak problems, ([docs](runtime/readme.md#the-real-time-transcription-service-mandarin-cpu))\n- 2024/01/03: Fixed known crash issues as well as memory leak problems, ([docs](runtime/readme.md#file-transcription-service-english-cpu)).\n- 2023/12/04: The Funasr SDK for Windows version 1.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin, The offline file transcription service (CPU) of English, The real-time transcription service (CPU) of Mandarin. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))\n- 2023/11/08: The offline file transcription service 3.0 (CPU) of Mandarin has been released, adding punctuation large model, Ngram language model, and wfst hot words. For detailed information, please refer to [docs](runtime#file-transcription-service-mandarin-cpu). \n- 2023/10/17: The offline file transcription service (CPU) of English has been released. For more details, please refer to ([docs](runtime#file-transcription-service-english-cpu)).\n- 2023/10/13: [SlideSpeech](https://slidespeech.github.io/): A large scale multi-modal audio-visual corpus with a significant amount of real-time synchronized slides.\n- 2023/10/10: The ASR-SpeakersDiarization combined pipeline [Paraformer-VAD-SPK](https://github.com/alibaba-damo-academy/FunASR/blob/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py) is now released. Experience the model to get recognition results with speaker information.\n- 2023/10/07: [FunCodec](https://github.com/alibaba-damo-academy/FunCodec): A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec.\n- 2023/09/01: The offline file transcription service 2.0 (CPU) of Mandarin has been released, with added support for ffmpeg, timestamp, and hotword models. For more details, please refer to ([docs](runtime#file-transcription-service-mandarin-cpu)).\n- 2023/08/07: The real-time transcription service (CPU) of Mandarin has been released. For more details, please refer to ([docs](runtime#the-real-time-transcription-service-mandarin-cpu)).\n- 2023/07/17: BAT is released, which is a low-latency and low-memory-consumption RNN-T model. For more details, please refer to ([BAT](egs/aishell/bat)).\n- 2023/06/26: ASRU2023 Multi-Channel Multi-Party Meeting Transcription Challenge 2.0 completed the competition and announced the results. For more details, please refer to ([M2MeT2.0](https://alibaba-damo-academy.github.io/FunASR/m2met2/index.html)).\n\n</details>\n\n<a name=\"Installation\"></a>\n## Installation\n\n- Requirements\n```text\npython>=3.8\ntorch>=1.13\ntorchaudio\n```\n\n- Install for pypi\n```shell\npip3 install -U funasr\n```\n- Or install from source code\n``` sh\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n- Install modelscope or huggingface_hub for the pretrained models (Optional)\n\n```shell\npip3 install -U modelscope huggingface_hub\n```\n\n## Model Zoo\nFunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the [Model License Agreement](./MODEL_LICENSE). Below are some representative models, for more models please refer to the [Model Zoo](./model_zoo).\n\n(Note: â­ represents the ModelScope model zoo, ğŸ¤— represents the Huggingface model zoo, ğŸ€ represents the OpenAI model zoo)\n\n\n|                                                                                                         Model Name                                                                                                         |                                   Task Details                                   |          Training Data           | Parameters |\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------:|:--------------------------------:|:----------:|\n|                                        SenseVoiceSmall <br> ([â­](https://www.modelscope.cn/models/iic/SenseVoiceSmall)  [ğŸ¤—](https://huggingface.co/FunAudioLLM/SenseVoiceSmall) )                                         | multiple speech understanding capabilities, including ASR, ITN, LID, SER, and AED, support languages such as zh, yue, en, ja, ko   |           300000 hours           |    234M    |\n|          paraformer-zh <br> ([â­](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)  [ğŸ¤—](https://huggingface.co/funasr/paraformer-zh) )           |                speech recognition, with timestamps, non-streaming                |      60000 hours, Mandarin       |    220M    |\n| <nobr>paraformer-zh-streaming <br> ( [â­](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) [ğŸ¤—](https://huggingface.co/funasr/paraformer-zh-streaming) )</nobr> |                          speech recognition, streaming                           |      60000 hours, Mandarin       |    220M    |\n|               paraformer-en <br> ( [â­](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) [ğŸ¤—](https://huggingface.co/funasr/paraformer-en) )                |              speech recognition, without timestamps, non-streaming               |       50000 hours, English       |    220M    |\n|                            conformer-en <br> ( [â­](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) [ğŸ¤—](https://huggingface.co/funasr/conformer-en) )                             |                        speech recognition, non-streaming                         |       50000 hours, English       |    220M    |\n|                               ct-punc <br> ( [â­](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) [ğŸ¤—](https://huggingface.co/funasr/ct-punc) )                               |                             punctuation restoration                              |    100M, Mandarin and English    |    290M    | \n|                                   fsmn-vad <br> ( [â­](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary) [ğŸ¤—](https://huggingface.co/funasr/fsmn-vad) )                                   |                             voice activity detection                             | 5000 hours, Mandarin and English |    0.4M    | \n|                                                              fsmn-kws <br> ( [â­](https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary) )                                                              |     keyword spottingï¼Œstreaming      |  5000 hours, Mandarin  |    0.7M    | \n|                                     fa-zh <br> ( [â­](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) [ğŸ¤—](https://huggingface.co/funasr/fa-zh) )                                     |                               timestamp prediction                               |       5000 hours, Mandarin       |    38M     | \n|                                       cam++ <br> ( [â­](https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary) [ğŸ¤—](https://huggingface.co/funasr/campplus) )                                        |                         speaker verification/diarization                         |            5000 hours            |    7.2M    | \n|                                            Whisper-large-v3 <br> ([â­](https://www.modelscope.cn/models/iic/Whisper-large-v3/summary)  [ğŸ€](https://github.com/openai/whisper) )                                            |                speech recognition, with timestamps, non-streaming                |           multilingual           |   1550 M   |\n|                                      Whisper-large-v3-turbo <br> ([â­](https://www.modelscope.cn/models/iic/Whisper-large-v3-turbo/summary)  [ğŸ€](https://github.com/openai/whisper) )                                      |                speech recognition, with timestamps, non-streaming                |           multilingual           |   809 M    |\n|                                               Qwen-Audio <br> ([â­](examples/industrial_data_pretraining/qwen_audio/demo.py)  [ğŸ¤—](https://huggingface.co/Qwen/Qwen-Audio) )                                                |                    audio-text multimodal models (pretraining)                    |           multilingual           |     8B     |\n|                                        Qwen-Audio-Chat <br> ([â­](examples/industrial_data_pretraining/qwen_audio/demo_chat.py)  [ğŸ¤—](https://huggingface.co/Qwen/Qwen-Audio-Chat) )                                        |                       audio-text multimodal models (chat)                        |           multilingual           |     8B     |\n|                              emotion2vec+large <br> ([â­](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)  [ğŸ¤—](https://huggingface.co/emotion2vec/emotion2vec_plus_large) )                               |                           speech emotion recongintion                            |           40000 hours            |    300M    |\n\n\n\n\n[//]: # ()\n[//]: # (FunASR supports pre-trained or further fine-tuned models for deployment as a service. The CPU version of the Chinese offline file conversion service has been released, details can be found in [docs]&#40;funasr/runtime/docs/SDK_tutorial.md&#41;. More detailed information about service deployment can be found in the [deployment roadmap]&#40;funasr/runtime/readme_cn.md&#41;.)\n\n\n<a name=\"quick-start\"></a>\n## Quick Start\n\nBelow is a quick start tutorial. Test audio files ([Mandarin](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav), [English](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav)).\n\n### Command-line usage\n\n```shell\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\nNotes: Support recognition of single audio file, as well as file list in Kaldi-style wav.scp format: `wav_id wav_pat`\n\n### Speech Recognition (Non-streaming)\n#### SenseVoice\n```python\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = AutoModel(\n    model=model_dir,\n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\nParameter Description:\n- `model_dir`: The name of the model, or the path to the model on the local disk.\n- `vad_model`: This indicates the activation of VAD (Voice Activity Detection). The purpose of VAD is to split long audio into shorter clips. In this case, the inference time includes both VAD and SenseVoice total consumption, and represents the end-to-end latency. If you wish to test the SenseVoice model's inference time separately, the VAD model can be disabled.\n- `vad_kwargs`: Specifies the configurations for the VAD model. `max_single_segment_time`: denotes the maximum duration for audio segmentation by the `vad_model`, with the unit being milliseconds (ms).\n- `use_itn`: Whether the output result includes punctuation and inverse text normalization.\n- `batch_size_s`: Indicates the use of dynamic batching, where the total duration of audio in the batch is measured in seconds (s).\n- `merge_vad`: Whether to merge short audio fragments segmented by the VAD model, with the merged length being `merge_length_s`, in seconds (s).\n- `ban_emo_unk`: Whether to ban the output of the `emo_unk` token.\n\n#### Paraformer\n```python\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  vad_model=\"fsmn-vad\",  punc_model=\"ct-punc\", \n                  # spk_model=\"cam++\", \n                  )\nres = model.generate(input=f\"{model.model_path}/example/asr_example.wav\", \n                     batch_size_s=300, \n                     hotword='é­”æ­')\nprint(res)\n```\nNote: `hub`: represents the model repository, `ms` stands for selecting ModelScope download, `hf` stands for selecting Huggingface download.\n\n### Speech Recognition (Streaming)\n```python\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\nNote: `chunk_size` is the configuration for streaming latency.` [0,10,5]` indicates that the real-time display granularity is `10*60=600ms`, and the lookahead information is `5*60=300ms`. Each inference input is `600ms` (sample points are `16000*0.6=960`), and the output is the corresponding text. For the last speech segment input, `is_final=True` needs to be set to force the output of the last word.\n\n<details><summary>More Examples</summary>\n\n### Voice Activity Detection (Non-Streaming)\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\nNote: The output format of the VAD model is: `[[beg1, end1], [beg2, end2], ..., [begN, endN]]`, where `begN/endN` indicates the starting/ending point of the `N-th` valid audio segment, measured in milliseconds.\n\n### Voice Activity Detection (Streaming)\n```python\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\nNote: The output format for the streaming VAD model can be one of four scenarios:\n- `[[beg1, end1], [beg2, end2], .., [begN, endN]]`ï¼šThe same as the offline VAD output result mentioned above.\n- `[[beg, -1]]`ï¼šIndicates that only a starting point has been detected.\n- `[[-1, end]]`ï¼šIndicates that only an ending point has been detected.\n- `[]`ï¼šIndicates that neither a starting point nor an ending point has been detected. \n\nThe output is measured in milliseconds and represents the absolute time from the starting point.\n### Punctuation Restoration\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\nres = model.generate(input=\"é‚£ä»Šå¤©çš„ä¼šå°±åˆ°è¿™é‡Œå§ happy new year æ˜å¹´è§\")\nprint(res)\n```\n### Timestamp Prediction\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n\n### Speech Emotion Recognition\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"emotion2vec_plus_large\")\n\nwav_file = f\"{model.model_path}/example/test.wav\"\n\nres = model.generate(wav_file, output_dir=\"./outputs\", granularity=\"utterance\", extract_embedding=False)\nprint(res)\n```\n\nMore usages ref to [docs](docs/tutorial/README_zh.md), \nmore examples ref to [demo](https://github.com/alibaba-damo-academy/FunASR/tree/main/examples/industrial_data_pretraining)\n\n</details>\n\n## Export ONNX\n\n### Command-line usage\n```shell\nfunasr-export ++model=paraformer ++quantize=false ++device=cpu\n```\n\n### Python\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\", device=\"cpu\")\n\nres = model.export(quantize=False)\n```\n\n### Test ONNX\n```python\n# pip3 install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\nMore examples ref to [demo](runtime/python/onnxruntime)\n\n## Deployment Service\nFunASR supports deploying pre-trained or further fine-tuned models for service. Currently, it supports the following types of service deployment:\n- File transcription service, Mandarin, CPU version, done\n- The real-time transcription service, Mandarin (CPU), done\n- File transcription service, English, CPU version, done\n- File transcription service, Mandarin, GPU version, in progress\n- and more.\n\nFor more detailed information, please refer to the [service deployment documentation](runtime/readme.md).\n\n\n<a name=\"contact\"></a>\n## Community Communication\nIf you encounter problems in use, you can directly raise Issues on the github page.\n\nYou can also scan the following DingTalk group to join the community group for communication and discussion.\n\n|                           DingTalk group                            |\n|:-------------------------------------------------------------------:|\n| <div align=\"left\"><img src=\"docs/images/dingding.png\" width=\"250\"/> |\n\n## Contributors\n\n| <div align=\"left\"><img src=\"docs/images/alibaba.png\" width=\"260\"/> | <div align=\"left\"><img src=\"docs/images/nwpu.png\" width=\"260\"/> | <img src=\"docs/images/China_Telecom.png\" width=\"200\"/> </div>  | <img src=\"docs/images/RapidAI.png\" width=\"200\"/> </div> | <img src=\"docs/images/aihealthx.png\" width=\"200\"/> </div> | <img src=\"docs/images/XVERSE.png\" width=\"250\"/> </div> |\n|:------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------:|:-------------------------------------------------------:|:-----------------------------------------------------------:|:------------------------------------------------------:|\n\nThe contributors can be found in [contributors list](./Acknowledge.md)\n\n## License\nThis project is licensed under [The MIT License](https://opensource.org/licenses/MIT). FunASR also contains various third-party components and some code modified from other repos under other open source licenses.\nThe use of pretraining model is subject to [model license](./MODEL_LICENSE)\n\n\n## Citations\n``` bibtex\n@inproceedings{gao2023funasr,\n  author={Zhifu Gao and Zerui Li and Jiaming Wang and Haoneng Luo and Xian Shi and Mengzhe Chen and Yabin Li and Lingyun Zuo and Zhihao Du and Zhangyu Xiao and Shiliang Zhang},\n  title={FunASR: A Fundamental End-to-End Speech Recognition Toolkit},\n  year={2023},\n  booktitle={INTERSPEECH},\n}\n@inproceedings{An2023bat,\n  author={Keyu An and Xian Shi and Shiliang Zhang},\n  title={BAT: Boundary aware transducer for memory-efficient and low-latency ASR},\n  year={2023},\n  booktitle={INTERSPEECH},\n}\n@inproceedings{gao22b_interspeech,\n  author={Zhifu Gao and ShiLiang Zhang and Ian McLoughlin and Zhijie Yan},\n  title={Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition},\n  year=2022,\n  booktitle={Proc. Interspeech 2022},\n  pages={2063--2067},\n  doi={10.21437/Interspeech.2022-9996}\n}\n@inproceedings{shi2023seaco,\n  author={Xian Shi and Yexin Yang and Zerui Li and Yanni Chen and Zhifu Gao and Shiliang Zhang},\n  title={SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability},\n  year={2023},\n  booktitle={ICASSP2024}\n}\n```\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 25.0224609375,
          "content": "[//]: # (<div align=\"left\"><img src=\"docs/images/funasr_logo.jpg\" width=\"400\"/></div>)\n\n(ç®€ä½“ä¸­æ–‡|[English](./README.md))\n\n\n\n[![SVG Banners](https://svg-banners.vercel.app/api?type=origin&text1=FunASRğŸ¤ &text2=ğŸ’–%20A%20Fundamental%20End-to-End%20Speech%20Recognition%20Toolkit&width=800&height=210)](https://github.com/Akshay090/svg-banners)\n\n[//]: # (# FunASR: A Fundamental End-to-End Speech Recognition Toolkit)\n\n[![PyPI](https://img.shields.io/pypi/v/funasr)](https://pypi.org/project/funasr/)\n\n\nFunASRå¸Œæœ›åœ¨è¯­éŸ³è¯†åˆ«çš„å­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨ä¹‹é—´æ¶èµ·ä¸€åº§æ¡¥æ¢ã€‚é€šè¿‡å‘å¸ƒå·¥ä¸šçº§è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜å¯ä»¥æ›´æ–¹ä¾¿åœ°è¿›è¡Œè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç ”ç©¶å’Œç”Ÿäº§ï¼Œå¹¶æ¨åŠ¨è¯­éŸ³è¯†åˆ«ç”Ÿæ€çš„å‘å±•ã€‚è®©è¯­éŸ³è¯†åˆ«æ›´æœ‰è¶£ï¼\n\n<div align=\"center\">  \n<h4>\n <a href=\"#æ ¸å¿ƒåŠŸèƒ½\"> æ ¸å¿ƒåŠŸèƒ½ </a>   \nï½œ<a href=\"#æœ€æ–°åŠ¨æ€\"> æœ€æ–°åŠ¨æ€ </a>\nï½œ<a href=\"#å®‰è£…æ•™ç¨‹\"> å®‰è£… </a>\nï½œ<a href=\"#å¿«é€Ÿå¼€å§‹\"> å¿«é€Ÿå¼€å§‹ </a>\nï½œ<a href=\"https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/tutorial/README_zh.md\"> æ•™ç¨‹æ–‡æ¡£ </a>\nï½œ<a href=\"#æ¨¡å‹ä»“åº“\"> æ¨¡å‹ä»“åº“ </a>\nï½œ<a href=\"#æœåŠ¡éƒ¨ç½²\"> æœåŠ¡éƒ¨ç½² </a>\nï½œ<a href=\"#è”ç³»æˆ‘ä»¬\"> è”ç³»æˆ‘ä»¬ </a>\n</h4>\n</div>\n\n<a name=\"æ ¸å¿ƒåŠŸèƒ½\"></a>\n## æ ¸å¿ƒåŠŸèƒ½\n- FunASRæ˜¯ä¸€ä¸ªåŸºç¡€è¯­éŸ³è¯†åˆ«å·¥å…·åŒ…ï¼Œæä¾›å¤šç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆVADï¼‰ã€æ ‡ç‚¹æ¢å¤ã€è¯­è¨€æ¨¡å‹ã€è¯´è¯äººéªŒè¯ã€è¯´è¯äººåˆ†ç¦»å’Œå¤šäººå¯¹è¯è¯­éŸ³è¯†åˆ«ç­‰ã€‚FunASRæä¾›äº†ä¾¿æ·çš„è„šæœ¬å’Œæ•™ç¨‹ï¼Œæ”¯æŒé¢„è®­ç»ƒå¥½çš„æ¨¡å‹çš„æ¨ç†ä¸å¾®è°ƒã€‚\n- æˆ‘ä»¬åœ¨[ModelScope](https://www.modelscope.cn/models?page=1&tasks=auto-speech-recognition)ä¸[huggingface](https://huggingface.co/FunASR)ä¸Šå‘å¸ƒäº†å¤§é‡å¼€æºæ•°æ®é›†æˆ–è€…æµ·é‡å·¥ä¸šæ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡æˆ‘ä»¬çš„[æ¨¡å‹ä»“åº“](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/model_zoo/modelscope_models.md)äº†è§£æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ã€‚ä»£è¡¨æ€§çš„[Paraformer](https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)éè‡ªå›å½’ç«¯åˆ°ç«¯è¯­éŸ³è¯†åˆ«æ¨¡å‹å…·æœ‰é«˜ç²¾åº¦ã€é«˜æ•ˆç‡ã€ä¾¿æ·éƒ¨ç½²çš„ä¼˜ç‚¹ï¼Œæ”¯æŒå¿«é€Ÿæ„å»ºè¯­éŸ³è¯†åˆ«æœåŠ¡ï¼Œè¯¦ç»†ä¿¡æ¯å¯ä»¥é˜…è¯»([æœåŠ¡éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))ã€‚\n\n<a name=\"æœ€æ–°åŠ¨æ€\"></a>\n## æœ€æ–°åŠ¨æ€\n- 2024/10/29: ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡ 1.12 å‘å¸ƒï¼Œ2pass-offlineæ¨¡å¼æ”¯æŒSensevoiceSmallæ¨¡å‹ï¼›è¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))\n- 2024/10/10ï¼šæ–°å¢åŠ Whisper-large-v3-turboæ¨¡å‹æ”¯æŒï¼Œå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«/ç¿»è¯‘/è¯­ç§è¯†åˆ«ï¼Œæ”¯æŒä» [modelscope](examples/industrial_data_pretraining/whisper/demo.py)ä»“åº“ä¸‹è½½ï¼Œä¹Ÿæ”¯æŒä» [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py)ä»“åº“ä¸‹è½½æ¨¡å‹ã€‚\n- 2024/09/26: ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 4.6ã€è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 1.7ã€ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡ 1.11 å‘å¸ƒï¼Œä¿®å¤ONNXå†…å­˜æ³„æ¼ã€æ”¯æŒSensevoiceSmall onnxæ¨¡å‹ï¼›ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡GPU 2.0 å‘å¸ƒï¼Œä¿®å¤æ˜¾å­˜æ³„æ¼; è¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))\n- 2024/09/25ï¼šæ–°å¢è¯­éŸ³å”¤é†’æ¨¡å‹ï¼Œæ”¯æŒ[fsmn_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [fsmn_kws_mt](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [sanm_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-offline), [sanm_kws_streaming](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online) 4ä¸ªæ¨¡å‹çš„å¾®è°ƒå’Œæ¨ç†ã€‚\n- 2024/07/04ï¼š[SenseVoice](https://github.com/FunAudioLLM/SenseVoice) æ˜¯ä¸€ä¸ªåŸºç¡€è¯­éŸ³ç†è§£æ¨¡å‹ï¼Œå…·å¤‡å¤šç§è¯­éŸ³ç†è§£èƒ½åŠ›ï¼Œæ¶µç›–äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è¯­è¨€è¯†åˆ«ï¼ˆLIDï¼‰ã€æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä»¥åŠéŸ³é¢‘äº‹ä»¶æ£€æµ‹ï¼ˆAEDï¼‰ã€‚\n- 2024/07/01ï¼šä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡GPUç‰ˆæœ¬ 1.1å‘å¸ƒï¼Œä¼˜åŒ–bladediscæ¨¡å‹å…¼å®¹æ€§é—®é¢˜ï¼›è¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))\n- 2024/06/27ï¼šä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡GPUç‰ˆæœ¬ 1.0å‘å¸ƒï¼Œæ”¯æŒåŠ¨æ€batchï¼Œæ”¯æŒå¤šè·¯å¹¶å‘ï¼Œåœ¨é•¿éŸ³é¢‘æµ‹è¯•é›†ä¸Šå•çº¿RTFä¸º0.0076ï¼Œå¤šçº¿åŠ é€Ÿæ¯”ä¸º1200+ï¼ˆCPUä¸º330+ï¼‰ï¼›è¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))\n- 2024/05/15ï¼šæ–°å¢åŠ æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ï¼Œ[emotion2vec+large](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)ï¼Œ[emotion2vec+base](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary)ï¼Œ[emotion2vec+seed](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary)ï¼Œè¾“å‡ºæƒ…æ„Ÿç±»åˆ«ä¸ºï¼šç”Ÿæ°”/angryï¼Œå¼€å¿ƒ/happyï¼Œä¸­ç«‹/neutralï¼Œéš¾è¿‡/sadã€‚\n- 2024/05/15: ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 4.5ã€è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 1.6ã€ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡ 1.10 å‘å¸ƒï¼Œé€‚é…FunASR 1.0æ¨¡å‹ç»“æ„ï¼›è¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))\n- 2024/03/05ï¼šæ–°å¢åŠ Qwen-Audioä¸Qwen-Audio-ChatéŸ³é¢‘æ–‡æœ¬æ¨¡æ€å¤§æ¨¡å‹ï¼Œåœ¨å¤šä¸ªéŸ³é¢‘é¢†åŸŸæµ‹è¯•æ¦œå•åˆ·æ¦œï¼Œä¸­æ”¯æŒè¯­éŸ³å¯¹è¯ï¼Œè¯¦ç»†ç”¨æ³•è§ [ç¤ºä¾‹](examples/industrial_data_pretraining/qwen_audio)ã€‚\n- 2024/03/05ï¼šæ–°å¢åŠ Whisper-large-v3æ¨¡å‹æ”¯æŒï¼Œå¤šè¯­è¨€è¯­éŸ³è¯†åˆ«/ç¿»è¯‘/è¯­ç§è¯†åˆ«ï¼Œæ”¯æŒä» [modelscope](examples/industrial_data_pretraining/whisper/demo.py)ä»“åº“ä¸‹è½½ï¼Œä¹Ÿæ”¯æŒä» [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py)ä»“åº“ä¸‹è½½æ¨¡å‹ã€‚\n- 2024/03/05: ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 4.4ã€è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 1.5ã€ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡ 1.9 å‘å¸ƒï¼Œdockeré•œåƒæ”¯æŒarm64å¹³å°ï¼Œå‡çº§modelscopeç‰ˆæœ¬ï¼›è¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))\n- 2024/01/30ï¼šfunasr-1.0å‘å¸ƒï¼Œæ›´æ–°è¯´æ˜[æ–‡æ¡£](https://github.com/alibaba-damo-academy/FunASR/discussions/1319)\n\n<details><summary>å±•å¼€æ—¥å¿—</summary>\n\n- 2024/01/30ï¼šæ–°å¢åŠ æƒ…æ„Ÿè¯†åˆ« [æ¨¡å‹é“¾æ¥](https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary)ï¼ŒåŸå§‹æ¨¡å‹ [repo](https://github.com/ddlBoJack/emotion2vec).\n- 2024/01/25: ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 4.2ã€è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 1.3ï¼Œä¼˜åŒ–vadæ•°æ®å¤„ç†æ–¹å¼ï¼Œå¤§å¹…é™ä½å³°å€¼å†…å­˜å ç”¨ï¼Œå†…å­˜æ³„æ¼ä¼˜åŒ–ï¼›ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡ 1.7 å‘å¸ƒï¼Œå®¢æˆ·ç«¯ä¼˜åŒ–ï¼›è¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))\n- 2024/01/09: funasrç¤¾åŒºè½¯ä»¶åŒ…windows 2.0ç‰ˆæœ¬å‘å¸ƒï¼Œæ”¯æŒè½¯ä»¶åŒ…ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™4.1ã€è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™1.2ã€ä¸­æ–‡å®æ—¶å¬å†™æœåŠ¡1.6çš„æœ€æ–°åŠŸèƒ½ï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([FunASRç¤¾åŒºè½¯ä»¶åŒ…windowsç‰ˆæœ¬](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))\n- 2024/01/03: ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 4.0 å‘å¸ƒï¼Œæ–°å¢æ”¯æŒ8kæ¨¡å‹ã€ä¼˜åŒ–æ—¶é—´æˆ³ä¸åŒ¹é…é—®é¢˜åŠå¢åŠ å¥å­çº§åˆ«æ—¶é—´æˆ³ã€ä¼˜åŒ–è‹±æ–‡å•è¯fstçƒ­è¯æ•ˆæœã€æ”¯æŒè‡ªåŠ¨åŒ–é…ç½®çº¿ç¨‹å‚æ•°ï¼ŒåŒæ—¶ä¿®å¤å·²çŸ¥çš„crashé—®é¢˜åŠå†…å­˜æ³„æ¼é—®é¢˜ï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡cpuç‰ˆæœ¬))\n- 2024/01/03: ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡ 1.6 å‘å¸ƒï¼Œ2pass-offlineæ¨¡å¼æ”¯æŒNgramè¯­è¨€æ¨¡å‹è§£ç ã€wfstçƒ­è¯ï¼ŒåŒæ—¶ä¿®å¤å·²çŸ¥çš„crashé—®é¢˜åŠå†…å­˜æ³„æ¼é—®é¢˜ï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡cpuç‰ˆæœ¬))\n- 2024/01/03: è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ 1.2 å‘å¸ƒï¼Œä¿®å¤å·²çŸ¥çš„crashé—®é¢˜åŠå†…å­˜æ³„æ¼é—®é¢˜ï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡cpuç‰ˆæœ¬))\n- 2023/12/04: funasrç¤¾åŒºè½¯ä»¶åŒ…windows 1.0ç‰ˆæœ¬å‘å¸ƒï¼Œæ”¯æŒä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™ã€è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™ã€ä¸­æ–‡å®æ—¶å¬å†™æœåŠ¡ï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([FunASRç¤¾åŒºè½¯ä»¶åŒ…windowsç‰ˆæœ¬](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))\n- 2023/11/08ï¼šä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡3.0 CPUç‰ˆæœ¬å‘å¸ƒï¼Œæ–°å¢æ ‡ç‚¹å¤§æ¨¡å‹ã€Ngramè¯­è¨€æ¨¡å‹ä¸wfstçƒ­è¯ï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡cpuç‰ˆæœ¬))\n- 2023/10/17: è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ä¸€é”®éƒ¨ç½²çš„CPUç‰ˆæœ¬å‘å¸ƒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡cpuç‰ˆæœ¬))\n- 2023/10/13: [SlideSpeech](https://slidespeech.github.io/): ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€éŸ³è§†é¢‘è¯­æ–™åº“ï¼Œä¸»è¦æ˜¯åœ¨çº¿ä¼šè®®æˆ–è€…åœ¨çº¿è¯¾ç¨‹åœºæ™¯ï¼ŒåŒ…å«äº†å¤§é‡ä¸å‘è¨€äººè®²è¯å®æ—¶åŒæ­¥çš„å¹»ç¯ç‰‡ã€‚\n- 2023.10.10: [Paraformer-long-Spk](https://github.com/alibaba-damo-academy/FunASR/blob/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py)æ¨¡å‹å‘å¸ƒï¼Œæ”¯æŒåœ¨é•¿è¯­éŸ³è¯†åˆ«çš„åŸºç¡€ä¸Šè·å–æ¯å¥è¯çš„è¯´è¯äººæ ‡ç­¾ã€‚\n- 2023.10.07: [FunCodec](https://github.com/alibaba-damo-academy/FunCodec): FunCodecæä¾›å¼€æºæ¨¡å‹å’Œè®­ç»ƒå·¥å…·ï¼Œå¯ä»¥ç”¨äºéŸ³é¢‘ç¦»æ•£ç¼–ç ï¼Œä»¥åŠåŸºäºç¦»æ•£ç¼–ç çš„è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆæˆç­‰ä»»åŠ¡ã€‚\n- 2023.09.01: ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡2.0 CPUç‰ˆæœ¬å‘å¸ƒï¼Œæ–°å¢ffmpegã€æ—¶é—´æˆ³ä¸çƒ­è¯æ¨¡å‹æ”¯æŒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡cpuç‰ˆæœ¬))\n- 2023.08.07: ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡ä¸€é”®éƒ¨ç½²çš„CPUç‰ˆæœ¬å‘å¸ƒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…([éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md#ä¸­æ–‡å®æ—¶è¯­éŸ³å¬å†™æœåŠ¡cpuç‰ˆæœ¬))\n- 2023.07.17: BATä¸€ç§ä½å»¶è¿Ÿä½å†…å­˜æ¶ˆè€—çš„RNN-Tæ¨¡å‹å‘å¸ƒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…ï¼ˆ[BAT](egs/aishell/bat)ï¼‰\n- 2023.06.26: ASRU2023 å¤šé€šé“å¤šæ–¹ä¼šè®®è½¬å½•æŒ‘æˆ˜èµ›2.0å®Œæˆç«èµ›ç»“æœå…¬å¸ƒï¼Œè¯¦ç»†ä¿¡æ¯å‚é˜…ï¼ˆ[M2MeT2.0](https://alibaba-damo-academy.github.io/FunASR/m2met2_cn/index.html)ï¼‰\n\n</details>\n\n<a name=\"å®‰è£…æ•™ç¨‹\"></a>\n## å®‰è£…æ•™ç¨‹\n\n- å®‰è£…funasrä¹‹å‰ï¼Œç¡®ä¿å·²ç»å®‰è£…äº†ä¸‹é¢ä¾èµ–ç¯å¢ƒ:\n```text\npython>=3.8\ntorch>=1.13\ntorchaudio\n```\n\n- pipå®‰è£…\n```shell\npip3 install -U funasr\n```\n\n- æˆ–è€…ä»æºä»£ç å®‰è£…\n``` sh\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\nå¦‚æœéœ€è¦ä½¿ç”¨å·¥ä¸šé¢„è®­ç»ƒæ¨¡å‹ï¼Œå®‰è£…modelscopeä¸huggingface_hubï¼ˆå¯é€‰ï¼‰\n\n```shell\npip3 install -U modelscope huggingface huggingface_hub\n```\n\n## æ¨¡å‹ä»“åº“\n\nFunASRå¼€æºäº†å¤§é‡åœ¨å·¥ä¸šæ•°æ®ä¸Šé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ‚¨å¯ä»¥åœ¨[æ¨¡å‹è®¸å¯åè®®](./MODEL_LICENSE)ä¸‹è‡ªç”±ä½¿ç”¨ã€å¤åˆ¶ã€ä¿®æ”¹å’Œåˆ†äº«FunASRæ¨¡å‹ï¼Œä¸‹é¢åˆ—ä¸¾ä»£è¡¨æ€§çš„æ¨¡å‹ï¼Œæ›´å¤šæ¨¡å‹è¯·å‚è€ƒ [æ¨¡å‹ä»“åº“](./model_zoo)ã€‚\n\nï¼ˆæ³¨ï¼šâ­ è¡¨ç¤ºModelScopeæ¨¡å‹ä»“åº“ï¼ŒğŸ¤— è¡¨ç¤ºHuggingfaceæ¨¡å‹ä»“åº“ï¼ŒğŸ€è¡¨ç¤ºOpenAIæ¨¡å‹ä»“åº“ï¼‰\n\n\n|                                                                                                     æ¨¡å‹åå­—                                                                                                      |        ä»»åŠ¡è¯¦æƒ…        |      è®­ç»ƒæ•°æ®      |  å‚æ•°é‡   | \n|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------:|:--------------:|:------:|\n|                                  SenseVoiceSmall <br> ([â­](https://www.modelscope.cn/models/iic/SenseVoiceSmall)  [ğŸ¤—](https://huggingface.co/FunAudioLLM/SenseVoiceSmall) )                                  |  å¤šç§è¯­éŸ³ç†è§£èƒ½åŠ›ï¼Œæ¶µç›–äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è¯­è¨€è¯†åˆ«ï¼ˆLIDï¼‰ã€æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä»¥åŠéŸ³é¢‘äº‹ä»¶æ£€æµ‹ï¼ˆAEDï¼‰   |  400000å°æ—¶ï¼Œä¸­æ–‡   |  330M  |\n|    paraformer-zh <br> ([â­](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)  [ğŸ¤—](https://huggingface.co/funasr/paraformer-zh) )    |  è¯­éŸ³è¯†åˆ«ï¼Œå¸¦æ—¶é—´æˆ³è¾“å‡ºï¼Œéå®æ—¶   |   60000å°æ—¶ï¼Œä¸­æ–‡   |  220M  |\n| paraformer-zh-streaming <br> ( [â­](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) [ğŸ¤—](https://huggingface.co/funasr/paraformer-zh-streaming) ) |      è¯­éŸ³è¯†åˆ«ï¼Œå®æ—¶       |   60000å°æ—¶ï¼Œä¸­æ–‡   |  220M  |\n|         paraformer-en <br> ( [â­](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) [ğŸ¤—](https://huggingface.co/funasr/paraformer-en) )         |      è¯­éŸ³è¯†åˆ«ï¼Œéå®æ—¶      |   50000å°æ—¶ï¼Œè‹±æ–‡   |  220M  |\n|                      conformer-en <br> ( [â­](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) [ğŸ¤—](https://huggingface.co/funasr/conformer-en) )                      |      è¯­éŸ³è¯†åˆ«ï¼Œéå®æ—¶      |   50000å°æ—¶ï¼Œè‹±æ–‡   |  220M  |\n|                        ct-punc <br> ( [â­](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) [ğŸ¤—](https://huggingface.co/funasr/ct-punc) )                         |        æ ‡ç‚¹æ¢å¤        |   100Mï¼Œä¸­æ–‡ä¸è‹±æ–‡   |  290M  | \n|                            fsmn-vad <br> ( [â­](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary) [ğŸ¤—](https://huggingface.co/funasr/fsmn-vad) )                             |     è¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼Œå®æ—¶      |  5000å°æ—¶ï¼Œä¸­æ–‡ä¸è‹±æ–‡  |  0.4M  | \n|                                                       fsmn-kws <br> ( [â­](https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary) )                                                        |     è¯­éŸ³å”¤é†’ï¼Œå®æ—¶      |  5000å°æ—¶ï¼Œä¸­æ–‡  |  0.7M  | \n|                              fa-zh <br> ( [â­](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) [ğŸ¤—](https://huggingface.co/funasr/fa-zh) )                               |      å­—çº§åˆ«æ—¶é—´æˆ³é¢„æµ‹      |   50000å°æ—¶ï¼Œä¸­æ–‡   |  38M   |\n|                                 cam++ <br> ( [â­](https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary) [ğŸ¤—](https://huggingface.co/funasr/campplus) )                                 |      è¯´è¯äººç¡®è®¤/åˆ†å‰²      |     5000å°æ—¶     |  7.2M  | \n|                                     Whisper-large-v3 <br> ([â­](https://www.modelscope.cn/models/iic/Whisper-large-v3/summary)  [ğŸ€](https://github.com/openai/whisper) )                                      |  è¯­éŸ³è¯†åˆ«ï¼Œå¸¦æ—¶é—´æˆ³è¾“å‡ºï¼Œéå®æ—¶   |      å¤šè¯­è¨€       | 1550 M |\n|                               Whisper-large-v3-turbo <br> ([â­](https://www.modelscope.cn/models/iic/Whisper-large-v3-turbo/summary)  [ğŸ€](https://github.com/openai/whisper) )                                |  è¯­éŸ³è¯†åˆ«ï¼Œå¸¦æ—¶é—´æˆ³è¾“å‡ºï¼Œéå®æ—¶   |      å¤šè¯­è¨€       | 809 M |\n|                                         Qwen-Audio <br> ([â­](examples/industrial_data_pretraining/qwen_audio/demo.py)  [ğŸ¤—](https://huggingface.co/Qwen/Qwen-Audio) )                                         |  éŸ³é¢‘æ–‡æœ¬å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆé¢„è®­ç»ƒï¼‰   |      å¤šè¯­è¨€       |   8B   |\n|                                 Qwen-Audio-Chat <br> ([â­](examples/industrial_data_pretraining/qwen_audio/demo_chat.py)  [ğŸ¤—](https://huggingface.co/Qwen/Qwen-Audio-Chat) )                                  | éŸ³é¢‘æ–‡æœ¬å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆchatç‰ˆæœ¬ï¼‰ |      å¤šè¯­è¨€       |   8B   |\n|                        emotion2vec+large <br> ([â­](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)  [ğŸ¤—](https://huggingface.co/emotion2vec/emotion2vec_plus_large) )                        |    æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹          | 40000å°æ—¶ï¼Œ4ç§æƒ…æ„Ÿç±»åˆ« |  300M  |\n\n<a name=\"å¿«é€Ÿå¼€å§‹\"></a>\n## å¿«é€Ÿå¼€å§‹\n\nä¸‹é¢ä¸ºå¿«é€Ÿä¸Šæ‰‹æ•™ç¨‹ï¼Œæµ‹è¯•éŸ³é¢‘ï¼ˆ[ä¸­æ–‡](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav)ï¼Œ[è‹±æ–‡](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav)ï¼‰\n\n### å¯æ‰§è¡Œå‘½ä»¤è¡Œ\n\n```shell\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\næ³¨ï¼šæ”¯æŒå•æ¡éŸ³é¢‘æ–‡ä»¶è¯†åˆ«ï¼Œä¹Ÿæ”¯æŒæ–‡ä»¶åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸ºkaldié£æ ¼wav.scpï¼š`wav_id   wav_path`\n\n### éå®æ—¶è¯­éŸ³è¯†åˆ«\n#### SenseVoice\n```python\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = AutoModel(\n    model=model_dir,\n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\nå‚æ•°è¯´æ˜ï¼š\n- `model_dir`ï¼šæ¨¡å‹åç§°ï¼Œæˆ–æœ¬åœ°ç£ç›˜ä¸­çš„æ¨¡å‹è·¯å¾„ã€‚\n- `vad_model`ï¼šè¡¨ç¤ºå¼€å¯VADï¼ŒVADçš„ä½œç”¨æ˜¯å°†é•¿éŸ³é¢‘åˆ‡å‰²æˆçŸ­éŸ³é¢‘ï¼Œæ­¤æ—¶æ¨ç†è€—æ—¶åŒ…æ‹¬äº†VADä¸SenseVoiceæ€»è€—æ—¶ï¼Œä¸ºé“¾è·¯è€—æ—¶ï¼Œå¦‚æœéœ€è¦å•ç‹¬æµ‹è¯•SenseVoiceæ¨¡å‹è€—æ—¶ï¼Œå¯ä»¥å…³é—­VADæ¨¡å‹ã€‚\n- `vad_kwargs`ï¼šè¡¨ç¤ºVADæ¨¡å‹é…ç½®,`max_single_segment_time`: è¡¨ç¤º`vad_model`æœ€å¤§åˆ‡å‰²éŸ³é¢‘æ—¶é•¿, å•ä½æ˜¯æ¯«ç§’msã€‚\n- `use_itn`ï¼šè¾“å‡ºç»“æœä¸­æ˜¯å¦åŒ…å«æ ‡ç‚¹ä¸é€†æ–‡æœ¬æ­£åˆ™åŒ–ã€‚\n- `batch_size_s` è¡¨ç¤ºé‡‡ç”¨åŠ¨æ€batchï¼Œbatchä¸­æ€»éŸ³é¢‘æ—¶é•¿ï¼Œå•ä½ä¸ºç§’sã€‚\n- `merge_vad`ï¼šæ˜¯å¦å°† vad æ¨¡å‹åˆ‡å‰²çš„çŸ­éŸ³é¢‘ç¢ç‰‡åˆæˆï¼Œåˆå¹¶åé•¿åº¦ä¸º`merge_length_s`ï¼Œå•ä½ä¸ºç§’sã€‚\n- `ban_emo_unk`ï¼šç¦ç”¨emo_unkæ ‡ç­¾ï¼Œç¦ç”¨åæ‰€æœ‰çš„å¥å­éƒ½ä¼šè¢«èµ‹ä¸æƒ…æ„Ÿæ ‡ç­¾ã€‚\n\n#### Paraformer\n```python\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  vad_model=\"fsmn-vad\", punc_model=\"ct-punc\", \n                  # spk_model=\"cam++\"\n                  )\nres = model.generate(input=f\"{model.model_path}/example/asr_example.wav\", \n            batch_size_s=300, \n            hotword='é­”æ­')\nprint(res)\n```\næ³¨ï¼š`hub`ï¼šè¡¨ç¤ºæ¨¡å‹ä»“åº“ï¼Œ`ms`ä¸ºé€‰æ‹©modelscopeä¸‹è½½ï¼Œ`hf`ä¸ºé€‰æ‹©huggingfaceä¸‹è½½ã€‚\n\n### å®æ—¶è¯­éŸ³è¯†åˆ«\n\n```python\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\n\næ³¨ï¼š`chunk_size`ä¸ºæµå¼å»¶æ—¶é…ç½®ï¼Œ`[0,10,5]`è¡¨ç¤ºä¸Šå±å®æ—¶å‡ºå­—ç²’åº¦ä¸º`10*60=600ms`ï¼Œæœªæ¥ä¿¡æ¯ä¸º`5*60=300ms`ã€‚æ¯æ¬¡æ¨ç†è¾“å…¥ä¸º`600ms`ï¼ˆé‡‡æ ·ç‚¹æ•°ä¸º`16000*0.6=960`ï¼‰ï¼Œè¾“å‡ºä¸ºå¯¹åº”æ–‡å­—ï¼Œæœ€åä¸€ä¸ªè¯­éŸ³ç‰‡æ®µè¾“å…¥éœ€è¦è®¾ç½®`is_final=True`æ¥å¼ºåˆ¶è¾“å‡ºæœ€åä¸€ä¸ªå­—ã€‚\n\n<details><summary>æ›´å¤šä¾‹å­</summary>\n\n### è¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆéå®æ—¶ï¼‰\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\næ³¨ï¼šVADæ¨¡å‹è¾“å‡ºæ ¼å¼ä¸ºï¼š`[[beg1, end1], [beg2, end2], .., [begN, endN]]`ï¼Œå…¶ä¸­`begN/endN`è¡¨ç¤ºç¬¬`N`ä¸ªæœ‰æ•ˆéŸ³é¢‘ç‰‡æ®µçš„èµ·å§‹ç‚¹/ç»“æŸç‚¹ï¼Œ\nå•ä½ä¸ºæ¯«ç§’ã€‚\n\n### è¯­éŸ³ç«¯ç‚¹æ£€æµ‹ï¼ˆå®æ—¶ï¼‰\n```python\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\næ³¨ï¼šæµå¼VADæ¨¡å‹è¾“å‡ºæ ¼å¼ä¸º4ç§æƒ…å†µï¼š\n- `[[beg1, end1], [beg2, end2], .., [begN, endN]]`ï¼šåŒä¸Šç¦»çº¿VADè¾“å‡ºç»“æœã€‚\n- `[[beg, -1]]`ï¼šè¡¨ç¤ºåªæ£€æµ‹åˆ°èµ·å§‹ç‚¹ã€‚\n- `[[-1, end]]`ï¼šè¡¨ç¤ºåªæ£€æµ‹åˆ°ç»“æŸç‚¹ã€‚\n- `[]`ï¼šè¡¨ç¤ºæ—¢æ²¡æœ‰æ£€æµ‹åˆ°èµ·å§‹ç‚¹ï¼Œä¹Ÿæ²¡æœ‰æ£€æµ‹åˆ°ç»“æŸç‚¹\nè¾“å‡ºç»“æœå•ä½ä¸ºæ¯«ç§’ï¼Œä»èµ·å§‹ç‚¹å¼€å§‹çš„ç»å¯¹æ—¶é—´ã€‚\n\n### æ ‡ç‚¹æ¢å¤\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\n\nres = model.generate(input=\"é‚£ä»Šå¤©çš„ä¼šå°±åˆ°è¿™é‡Œå§ happy new year æ˜å¹´è§\")\nprint(res)\n```\n\n### æ—¶é—´æˆ³é¢„æµ‹\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n### æƒ…æ„Ÿè¯†åˆ«\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"emotion2vec_plus_large\")\n\nwav_file = f\"{model.model_path}/example/test.wav\"\n\nres = model.generate(wav_file, output_dir=\"./outputs\", granularity=\"utterance\", extract_embedding=False)\nprint(res)\n```\n\næ›´è¯¦ç»†ï¼ˆ[æ•™ç¨‹æ–‡æ¡£](docs/tutorial/README_zh.md)ï¼‰ï¼Œ\næ›´å¤šï¼ˆ[æ¨¡å‹ç¤ºä¾‹](https://github.com/alibaba-damo-academy/FunASR/tree/main/examples/industrial_data_pretraining)ï¼‰\n\n</details>\n\n## å¯¼å‡ºONNX\n### ä»å‘½ä»¤è¡Œå¯¼å‡º\n```shell\nfunasr-export ++model=paraformer ++quantize=false\n```\n\n### ä»Pythonå¯¼å‡º\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\")\n\nres = model.export(quantize=False)\n```\n\n### æµ‹è¯•ONNX\n```python\n# pip3 install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\næ›´å¤šä¾‹å­è¯·å‚è€ƒ [æ ·ä¾‹](runtime/python/onnxruntime)\n\n<a name=\"æœåŠ¡éƒ¨ç½²\"></a>\n## æœåŠ¡éƒ¨ç½²\nFunASRæ”¯æŒé¢„è®­ç»ƒæˆ–è€…è¿›ä¸€æ­¥å¾®è°ƒçš„æ¨¡å‹è¿›è¡ŒæœåŠ¡éƒ¨ç½²ã€‚ç›®å‰æ”¯æŒä»¥ä¸‹å‡ ç§æœåŠ¡éƒ¨ç½²ï¼š\n\n- ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ï¼ˆCPUç‰ˆæœ¬ï¼‰ï¼Œå·²å®Œæˆ\n- ä¸­æ–‡æµå¼è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼ˆCPUç‰ˆæœ¬ï¼‰ï¼Œå·²å®Œæˆ\n- è‹±æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ï¼ˆCPUç‰ˆæœ¬ï¼‰ï¼Œå·²å®Œæˆ\n- ä¸­æ–‡ç¦»çº¿æ–‡ä»¶è½¬å†™æœåŠ¡ï¼ˆGPUç‰ˆæœ¬ï¼‰ï¼Œè¿›è¡Œä¸­\n- æ›´å¤šæ”¯æŒä¸­\n\nè¯¦ç»†ä¿¡æ¯å¯ä»¥å‚é˜…([æœåŠ¡éƒ¨ç½²æ–‡æ¡£](runtime/readme_cn.md))ã€‚\n\n\n<a name=\"ç¤¾åŒºäº¤æµ\"></a>\n## è”ç³»æˆ‘ä»¬\n\nå¦‚æœæ‚¨åœ¨ä½¿ç”¨ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ç›´æ¥åœ¨githubé¡µé¢æIssuesã€‚æ¬¢è¿è¯­éŸ³å…´è¶£çˆ±å¥½è€…æ‰«æä»¥ä¸‹çš„é’‰é’‰ç¾¤äºŒç»´ç åŠ å…¥ç¤¾åŒºç¾¤ï¼Œè¿›è¡Œäº¤æµå’Œè®¨è®ºã€‚\n\n|                                 é’‰é’‰ç¾¤                                 |\n|:-------------------------------------------------------------------:|\n| <div align=\"left\"><img src=\"docs/images/dingding.png\" width=\"250\"/> |\n\n## ç¤¾åŒºè´¡çŒ®è€…\n\n| <div align=\"left\"><img src=\"docs/images/alibaba.png\" width=\"260\"/> | <div align=\"left\"><img src=\"docs/images/nwpu.png\" width=\"260\"/> | <img src=\"docs/images/China_Telecom.png\" width=\"200\"/> </div>  | <img src=\"docs/images/RapidAI.png\" width=\"200\"/> </div> | <img src=\"docs/images/aihealthx.png\" width=\"200\"/> </div> | <img src=\"docs/images/XVERSE.png\" width=\"250\"/> </div> |\n|:------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------:|:-------------------------------------------------------:|:-----------------------------------------------------------:|:------------------------------------------------------:|\n\nè´¡çŒ®è€…åå•è¯·å‚è€ƒï¼ˆ[è‡´è°¢åå•](./Acknowledge.md)ï¼‰\n\n\n## è®¸å¯åè®®\né¡¹ç›®éµå¾ª[The MIT License](https://opensource.org/licenses/MIT)å¼€æºåè®®ï¼Œæ¨¡å‹è®¸å¯åè®®è¯·å‚è€ƒï¼ˆ[æ¨¡å‹åè®®](./MODEL_LICENSE)ï¼‰\n\n\n## è®ºæ–‡å¼•ç”¨\n\n``` bibtex\n@inproceedings{gao2023funasr,\n  author={Zhifu Gao and Zerui Li and Jiaming Wang and Haoneng Luo and Xian Shi and Mengzhe Chen and Yabin Li and Lingyun Zuo and Zhihao Du and Zhangyu Xiao and Shiliang Zhang},\n  title={FunASR: A Fundamental End-to-End Speech Recognition Toolkit},\n  year={2023},\n  booktitle={INTERSPEECH},\n}\n@inproceedings{An2023bat,\n  author={Keyu An and Xian Shi and Shiliang Zhang},\n  title={BAT: Boundary aware transducer for memory-efficient and low-latency ASR},\n  year={2023},\n  booktitle={INTERSPEECH},\n}\n@inproceedings{gao22b_interspeech,\n  author={Zhifu Gao and ShiLiang Zhang and Ian McLoughlin and Zhijie Yan},\n  title={{Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition}},\n  year=2022,\n  booktitle={Proc. Interspeech 2022},\n  pages={2063--2067},\n  doi={10.21437/Interspeech.2022-9996}\n}\n@article{shi2023seaco,\n  author={Xian Shi and Yexin Yang and Zerui Li and Yanni Chen and Zhifu Gao and Shiliang Zhang},\n  title={{SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability}},\n  year=2023,\n  journal={arXiv preprint arXiv:2308.03266(accepted by ICASSP2024)},\n}\n```\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "export.py",
          "type": "blob",
          "size": 0.375,
          "content": "# method2, inference from local path\r\nfrom funasr import AutoModel\r\n\r\nmodel = AutoModel(\r\n    model=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\r\n)\r\n\r\nres = model.export(type=\"onnx\", quantize=False, opset_version=13, device='cuda')  # fp32 onnx-gpu\r\n# res = model.export(type=\"onnx_fp16\", quantize=False, opset_version=13, device='cuda')  # fp16 onnx-gpu\r\n"
        },
        {
          "name": "fun_text_processing",
          "type": "tree",
          "content": null
        },
        {
          "name": "funasr",
          "type": "tree",
          "content": null
        },
        {
          "name": "model_zoo",
          "type": "tree",
          "content": null
        },
        {
          "name": "runtime",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.7392578125,
          "content": "#!/usr/bin/env python3\n\n\"\"\"FunASR setup script.\"\"\"\n\nimport os\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n\nrequirements = {\n    \"install\": [\n        \"scipy>=1.4.1\",\n        \"librosa\",\n        \"jamo\",  # For kss\n        \"PyYAML>=5.1.2\",\n        \"soundfile>=0.12.1\",\n        \"kaldiio>=2.17.0\",\n        \"torch_complex\",\n        # \"nltk>=3.4.5\",\n        \"sentencepiece\",  # train\n        \"jieba\",\n        # \"rotary_embedding_torch\",\n        # \"ffmpeg-python\",\n        # \"pypinyin>=0.44.0\",\n        # \"espnet_tts_frontend\",\n        # ENH\n        \"pytorch_wpe\",\n        \"editdistance>=0.5.2\",\n        # \"g2p\",\n        # \"nara_wpe\",\n        # PAI\n        \"oss2\",\n        # \"edit-distance\",\n        # \"textgrid\",\n        # \"protobuf\",\n        \"tqdm\",\n        \"umap_learn\",\n        \"jaconv\",\n        \"hydra-core>=1.3.2\",\n        \"tensorboardX\",\n        # \"rotary_embedding_torch\",\n        \"requests\",\n        \"modelscope\",\n    ],\n    # train: The modules invoked when training only.\n    \"train\": [\n        \"editdistance\",\n    ],\n    # all: The modules should be optionally installled due to some reason.\n    #      Please consider moving them to \"install\" occasionally\n    \"all\": [\n        # NOTE(kamo): Append modules requiring specific pytorch version or torch>1.3.0\n        \"torch_optimizer\",\n        \"fairscale\",\n        \"transformers\",\n        \"openai-whisper\",\n    ],\n    \"setup\": [\n        \"numpy\",\n        \"pytest-runner\",\n    ],\n    \"test\": [\n        \"pytest>=3.3.0\",\n        \"pytest-timeouts>=1.2.1\",\n        \"pytest-pythonpath>=0.7.3\",\n        \"pytest-cov>=2.7.1\",\n        \"hacking>=2.0.0\",\n        \"mock>=2.0.0\",\n        \"pycodestyle\",\n        \"jsondiff<2.0.0,>=1.2.0\",\n        \"flake8>=3.7.8\",\n        \"flake8-docstrings>=1.3.1\",\n        \"black\",\n    ],\n    \"doc\": [\n        \"Jinja2\",\n        \"Sphinx\",\n        \"sphinx-rtd-theme>=0.2.4\",\n        \"sphinx-argparse>=0.2.5\",\n        \"commonmark\",\n        \"recommonmark>=0.4.0\",\n        \"nbsphinx>=0.4.2\",\n        \"sphinx-markdown-tables>=0.0.12\",\n        \"configargparse>=1.2.1\",\n    ],\n    \"llm\": [\n        \"transformers>=4.32.0\",\n        \"accelerate\",\n        \"tiktoken\",\n        \"einops\",\n        \"transformers_stream_generator>=0.0.4\",\n        \"scipy\",\n        \"torchvision\",\n        \"pillow\",\n        \"matplotlib\",\n    ],\n}\nrequirements[\"all\"].extend(requirements[\"train\"])\nrequirements[\"all\"].extend(requirements[\"llm\"])\nrequirements[\"test\"].extend(requirements[\"train\"])\n\ninstall_requires = requirements[\"install\"]\nsetup_requires = requirements[\"setup\"]\ntests_require = requirements[\"test\"]\nextras_require = {k: v for k, v in requirements.items() if k not in [\"install\", \"setup\"]}\n\ndirname = os.path.dirname(__file__)\nversion_file = os.path.join(dirname, \"funasr\", \"version.txt\")\nwith open(version_file, \"r\") as f:\n    version = f.read().strip()\nsetup(\n    name=\"funasr\",\n    version=version,\n    url=\"https://github.com/alibaba-damo-academy/FunASR.git\",\n    author=\"Speech Lab of Alibaba Group\",\n    author_email=\"funasr@list.alibaba-inc.com\",\n    description=\"FunASR: A Fundamental End-to-End Speech Recognition Toolkit\",\n    long_description=open(os.path.join(dirname, \"README.md\"), encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    license=\"The MIT License\",\n    packages=find_packages(include=[\"funasr*\"]),\n    package_data={\"funasr\": [\"version.txt\"]},\n    install_requires=install_requires,\n    setup_requires=setup_requires,\n    tests_require=tests_require,\n    extras_require=extras_require,\n    python_requires=\">=3.7.0\",\n    classifiers=[\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Science/Research\",\n        \"Operating System :: POSIX :: Linux\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"funasr = funasr.bin.inference:main_hydra\",\n            \"funasr-train = funasr.bin.train:main_hydra\",\n            \"funasr-export = funasr.bin.export:main_hydra\",\n            \"scp2jsonl = funasr.datasets.audio_datasets.scp2jsonl:main_hydra\",\n            \"jsonl2scp = funasr.datasets.audio_datasets.jsonl2scp:main_hydra\",\n            \"sensevoice2jsonl = funasr.datasets.audio_datasets.sensevoice2jsonl:main_hydra\",\n            \"funasr-scp2jsonl = funasr.datasets.audio_datasets.scp2jsonl:main_hydra\",\n            \"funasr-jsonl2scp = funasr.datasets.audio_datasets.jsonl2scp:main_hydra\",\n            \"funasr-sensevoice2jsonl = funasr.datasets.audio_datasets.sensevoice2jsonl:main_hydra\",\n        ]\n    },\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "web-pages",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}