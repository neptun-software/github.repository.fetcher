{
  "metadata": {
    "timestamp": 1736560638665,
    "page": 273,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "modelscope/FunASR",
      "stars": 7661,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3330078125,
          "content": ".idea\n./__pycache__/\n*/__pycache__/\n*/*/__pycache__/\n*/*/*/__pycache__/\n.DS_Store\ninit_model/\n*.tar.gz\ntest_local/\nRapidASR\nexport/*\n*.pyc\n.eggs\nMaaS-lib\n.gitignore\n.egg*\ndist\nbuild\nfunasr.egg-info\ndocs/_build\nmodelscope\nsamples\n.ipynb_checkpoints\noutputs*\nemotion2vec*\nGPT-SoVITS*\nmodelscope_models\nexamples/aishell/llm_asr_nar/*\n*egg-info\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.1748046875,
          "content": "repos:\n  - repo: https://github.com/psf/black\n    rev: 24.4.0\n    hooks:\n      - id: black\n        args: ['--line-length=100']  # 示例参数，black默认使用4个空格缩进\n"
        },
        {
          "name": "Acknowledge.md",
          "type": "blob",
          "size": 0.8720703125,
          "content": "## Acknowledge\n\n1. We borrowed a lot of code from [Kaldi](http://kaldi-asr.org/) for data preparation.\n2. We borrowed a lot of code from [ESPnet](https://github.com/espnet/espnet). FunASR follows up the training and finetuning pipelines of ESPnet.\n3. We referred [Wenet](https://github.com/wenet-e2e/wenet) for building dataloader for large scale data training.\n4. We acknowledge [ChinaTelecom](https://github.com/zhuzizyf/damo-fsmn-vad-infer-httpserver) for contributing the VAD runtime.\n5. We acknowledge [RapidAI](https://github.com/RapidAI) for contributing the Paraformer and CT_Transformer-punc runtime.\n6. We acknowledge [AiHealthx](http://www.aihealthx.com/) for contributing the websocket service and html5.\n7. We acknowledge [XVERSE](http://www.xverse.cn/index.html) for contributing the grpc service.\n8. We acknowledge [blt](https://github.com/bltcn) for develop and deploy website."
        },
        {
          "name": "Contribution.md",
          "type": "blob",
          "size": 2.3564453125,
          "content": "# Contributing to FunASR\n\nFirst off, thanks for taking the time to contribute! 🎉\n\nThe following is a set of guidelines for contributing to FunASR. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.\n\n## How Can I Contribute?\n\n### Reporting Bugs\n\nThis section guides you through submitting a bug report for FunASR. Following these guidelines helps maintainers and the community understand your report, reproduce the behavior, and find related reports.\n\n- **Ensure the bug was not already reported** by searching on GitHub under Issues.\n- If you're unable to find an open issue addressing the problem, open a new one. Be sure to include a **title and clear description**, as much relevant information as possible, and a **code sample** or an **executable test case** demonstrating the expected behavior that is not occurring.\n\n### Suggesting Enhancements\n\nThis section guides you through submitting an enhancement suggestion for FunASR, including completely new features and minor improvements to existing functionality.\n\n- **Ensure the enhancement was not already suggested** by searching on GitHub under Issues.\n- If you find an enhancement that matches your suggestion, feel free to add your comments to the existing issue.\n- If you don't find an existing issue, you can open a new one. Be sure to include a **title and clear description**, as much relevant information as possible, and a **code sample** or an **executable test case** demonstrating the expected behavior.\n\n### Pull Requests\n\nThe process described here has several goals:\n\n- Maintain FunASR's quality\n- Fix problems that are important to users\n- Engage the community in working toward the best possible FunASR\n\nPlease follow these steps to have your contribution considered by the maintainers:\n\n1. **Fork** the repository.\n2. **Clone** your fork: `git clone https://github.com/alibaba/FunASR.git`\n3. **Create a branch** for your changes: `git checkout -b my-new-feature`\n4. **Make your changes**.\n5. **Commit your changes**: `git commit -am 'Add some feature'`\n6. **Push to the branch**: `git push origin my-new-feature`\n7. **Create a new Pull Request**.\n\n### Code of Conduct\n\nThis project and everyone participating in it is governed by the FunASR Code of Conduct. By participating, you are expected to uphold this code.\n\nThank you for contributing to FunASR!\n"
        },
        {
          "name": "MODEL_LICENSE",
          "type": "blob",
          "size": 5.181640625,
          "content": "FunASR Model Open Source License Agreement\n\nVersion: 1.1\n\nCopyright (C) [2023-2028] [Alibaba Group]. All rights reserved.\n\nThank you for choosing the FunASR open-source model. The FunASR open-source model includes a range of free and open industrial models for you to use, modify, share, and learn from.\n\nTo ensure better community collaboration, we have established the following agreement, and we hope you will read and comply with its terms.\nDefinitions\n\nIn this agreement, [FunASR Software] refers to FunASR open-source model weights and their derivatives, including finetuned models; [You] refers to individuals or organizations using, modifying, sharing, and learning from [FunASR Software].\n\n2 License and Restrictions\n\n2.1 License\n\nYou are free to use, copy, modify, and share [FunASR Software] under the terms of this agreement.\n\n2.2 Restrictions\n\nWhen using, copying, modifying, and sharing [FunASR Software], you must attribute the source and author information and retain relevant model names in [FunASR Software].\n\n3 Responsibility and Risk\n\n[FunASR Software] is provided for reference and learning purposes only, and Alibaba Group assumes no responsibility for any direct or indirect losses resulting from your use or modification of [FunASR Software]. You should assume all risks associated with using and modifying [FunASR Software].\n\n4 Community Conduct Guidelines\n\n4.1 Encouraged Behavior\n\nThe community welcomes developers and users to engage in discussions about [FunASR Software]. Participants are encouraged to interact in a friendly, polite, and respectful manner to foster constructive discussion and collaboration.\n\n4.2 Prohibited Behavior\n\nIndividual or organizational users shall not engage in unjustified denigration, malicious smearing, or baseless insults against [FunASR Software]. Such behavior is considered a violation of the spirit of community cooperation. If a user is found to be engaging in the prohibited behavior mentioned above, it will be considered an automatic forfeiture of all licenses under this agreement.\n\n5 Termination\n\nIf you violate any terms of this agreement, your license will automatically terminate, and you must cease using, copying, modifying, and sharing [FunASR Software].\n\n6 Revisions\n\nThis agreement may be updated and revised occasionally. The revised agreement will be published in the official repository of [FunASR Software] and will take effect automatically. Continuing to use, copy, modify, and share [FunASR Software] indicates your acceptance of the revised agreement.\n\n7 Miscellaneous\n\nThis agreement is governed by the laws of [Country/Region]. If any provision is deemed illegal, invalid, or unenforceable, that provision shall be considered severed from this agreement, and the remaining provisions shall continue to be valid and binding.\n\nIf you have any questions or comments regarding this agreement, please contact us.\n\nCopyright © [2023-2028] [Alibaba Group]. All rights reserved.\n\n\nFunASR 模型开源协议\n\n版本号：1.1\n\n版权所有 (C) [2023-2028] [阿里巴巴集团]。保留所有权利。\n\n感谢您选择 FunASR 开源模型。FunASR 开源模型包含一系列免费且开源的工业模型，让大家可以使用、修改、分享和学习该模型。\n\n为了保证更好的社区合作，我们制定了以下协议，希望您仔细阅读并遵守本协议。\n\n1 定义\n\n本协议中，[FunASR 软件]指 FunASR 开源模型权重及其衍生品，包括 Finetune 后的模型；[您]指使用、修改、分享和学习[FunASR 软件]的个人或组织。\n\n2 许可和限制\n\n2.1 许可\n\n您可以在遵守本协议的前提下，自由地使用、复制、修改和分享[FunASR 软件]。\n\n2.2 限制\n\n您在使用、复制、修改和分享[FunASR 软件]时，必须注明出处以及作者信息，并保留[FunASR 软件]中相关模型名称。\n\n3 责任和风险承担\n\n[FunASR 软件]仅作为参考和学习使用，不对您使用或修改[FunASR 软件]造成的任何直接或间接损失承担任何责任。您对[FunASR 软件]的使用和修改应该自行承担风险。\n\n4 社区行为准则\n\n4.1 欢迎交流\n\n社区欢迎开发者与用户对[FunASR 软件]进行交流讨论。交流中请注意保持友好、礼貌和文明，以促进建设性的讨论和合作。\n\n4.2 禁止行为\n\n个人或组织用户不得对[FunASR 软件]进行无端诋毁、恶意抹黑或凭空谩骂。此类行为被视为违反社区合作精神。如被认定从事上述禁止行为，将视为自动放弃本协议下的所有许可。\n\n5 终止\n\n如果您违反本协议的任何条款，您的许可将自动终止，您必须停止使用、复制、修改和分享[FunASR 软件]。\n\n6 修订\n\n本协议可能会不时更新和修订。修订后的协议将在[FunASR 软件]官方仓库发布，并自动生效。如果您继续使用、复制、修改和分享[FunASR 软件]，即表示您同意修订后的协议。\n\n7 其他规定\n\n本协议受到[国家/地区] 的法律管辖。如果任何条款被裁定为不合法、无效或无法执行，则该条款应被视为从本协议中删除，而其余条款应继续有效并具有约束力。\n\n如果您对本协议有任何问题或意见，请联系我们。\n\n版权所有© [2023-2028] [阿里巴巴集团]。保留所有权利。\n"
        },
        {
          "name": "MinMo_gitlab",
          "type": "blob",
          "size": 0.0146484375,
          "content": "../MinMo_gitlab"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 28.3466796875,
          "content": "[//]: # (<div align=\"left\"><img src=\"docs/images/funasr_logo.jpg\" width=\"400\"/></div>)\n\n([简体中文](./README_zh.md)|English)\n\n[//]: # (# FunASR: A Fundamental End-to-End Speech Recognition Toolkit)\n\n[![SVG Banners](https://svg-banners.vercel.app/api?type=origin&text1=FunASR🤠&text2=💖%20A%20Fundamental%20End-to-End%20Speech%20Recognition%20Toolkit&width=800&height=210)](https://github.com/Akshay090/svg-banners)\n\n[![PyPI](https://img.shields.io/pypi/v/funasr)](https://pypi.org/project/funasr/)\n\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/3839\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/3839\" alt=\"alibaba-damo-academy%2FFunASR | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n\n<strong>FunASR</strong> hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training & finetuning of the industrial-grade speech recognition model, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for Fun！\n\n[**Highlights**](#highlights)\n| [**News**](https://github.com/alibaba-damo-academy/FunASR#whats-new) \n| [**Installation**](#installation)\n| [**Quick Start**](#quick-start)\n| [**Tutorial**](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/tutorial/README.md)\n| [**Runtime**](./runtime/readme.md)\n| [**Model Zoo**](#model-zoo)\n| [**Contact**](#contact)\n\n\n\n\n<a name=\"highlights\"></a>\n## Highlights\n- FunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.\n- We have released a vast collection of academic and industrial pretrained models on the [ModelScope](https://www.modelscope.cn/models?page=1&tasks=auto-speech-recognition) and [huggingface](https://huggingface.co/FunASR), which can be accessed through our [Model Zoo](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/model_zoo/modelscope_models.md). The representative [Paraformer-large](https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary), a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the [service deployment document](runtime/readme_cn.md). \n\n\n<a name=\"whats-new\"></a>\n## What's new:\n- 2024/10/29: Real-time Transcription Service 1.12 released, The 2pass-offline mode supports the SensevoiceSmal model；([docs](runtime/readme.md));\n- 2024/10/10：Added support for the Whisper-large-v3-turbo model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the [modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).\n- 2024/09/26: Offline File Transcription Service 4.6, Offline File Transcription Service of English 1.7, Real-time Transcription Service 1.11 released, fix memory leak & Support the SensevoiceSmall onnx model；File Transcription Service 2.0 GPU released, Fix GPU memory leak; ([docs](runtime/readme.md));\n- 2024/09/25：keyword spotting models are new supported. Supports fine-tuning and inference for four models: [fsmn_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [fsmn_kws_mt](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [sanm_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-offline), [sanm_kws_streaming](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online).\n- 2024/07/04：[SenseVoice](https://github.com/FunAudioLLM/SenseVoice) is a speech foundation model with multiple speech understanding capabilities, including ASR, LID, SER, and AED.\n- 2024/07/01: Offline File Transcription Service GPU 1.1 released, optimize BladeDISC model compatibility issues; ref to ([docs](runtime/readme.md))\n- 2024/06/27: Offline File Transcription Service GPU 1.0 released, supporting dynamic batch processing and multi-threading concurrency. In the long audio test set, the single-thread RTF is 0.0076, and multi-threads' speedup is 1200+ (compared to 330+ on CPU); ref to ([docs](runtime/readme.md))\n- 2024/05/15：emotion recognition models are new supported. [emotion2vec+large](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)，[emotion2vec+base](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary)，[emotion2vec+seed](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary). currently supports the following categories: 0: angry 1: happy 2: neutral 3: sad 4: unknown.\n- 2024/05/15: Offline File Transcription Service 4.5, Offline File Transcription Service of English 1.6, Real-time Transcription Service 1.10 released, adapting to FunASR 1.0 model structure；([docs](runtime/readme.md))\n\n<details><summary>Full Changelog</summary>\n\n- 2024/03/05：Added the Qwen-Audio and Qwen-Audio-Chat large-scale audio-text multimodal models, which have topped multiple audio domain leaderboards. These models support speech dialogue, [usage](examples/industrial_data_pretraining/qwen_audio).\n- 2024/03/05：Added support for the Whisper-large-v3 model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the[modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).\n- 2024/03/05: Offline File Transcription Service 4.4, Offline File Transcription Service of English 1.5，Real-time Transcription Service 1.9 released，docker image supports ARM64 platform, update modelscope；([docs](runtime/readme.md))\n- 2024/01/30：funasr-1.0 has been released ([docs](https://github.com/alibaba-damo-academy/FunASR/discussions/1319))\n- 2024/01/30：emotion recognition models are new supported. [model link](https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary), modified from [repo](https://github.com/ddlBoJack/emotion2vec).\n- 2024/01/25: Offline File Transcription Service 4.2, Offline File Transcription Service of English 1.3 released，optimized the VAD (Voice Activity Detection) data processing method, significantly reducing peak memory usage, memory leak optimization; Real-time Transcription Service 1.7 released，optimizatized the client-side；([docs](runtime/readme.md))\n- 2024/01/09: The Funasr SDK for Windows version 2.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin 4.1, The offline file transcription service (CPU) of English 1.2, The real-time transcription service (CPU) of Mandarin 1.6. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))\n- 2024/01/03: File Transcription Service 4.0 released, Added support for 8k models, optimized timestamp mismatch issues and added sentence-level timestamps, improved the effectiveness of English word FST hotwords, supported automated configuration of thread parameters, and fixed known crash issues as well as memory leak problems, refer to ([docs](runtime/readme.md#file-transcription-service-mandarin-cpu)).\n- 2024/01/03: Real-time Transcription Service 1.6 released，The 2pass-offline mode supports Ngram language model decoding and WFST hotwords, while also addressing known crash issues and memory leak problems, ([docs](runtime/readme.md#the-real-time-transcription-service-mandarin-cpu))\n- 2024/01/03: Fixed known crash issues as well as memory leak problems, ([docs](runtime/readme.md#file-transcription-service-english-cpu)).\n- 2023/12/04: The Funasr SDK for Windows version 1.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin, The offline file transcription service (CPU) of English, The real-time transcription service (CPU) of Mandarin. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))\n- 2023/11/08: The offline file transcription service 3.0 (CPU) of Mandarin has been released, adding punctuation large model, Ngram language model, and wfst hot words. For detailed information, please refer to [docs](runtime#file-transcription-service-mandarin-cpu). \n- 2023/10/17: The offline file transcription service (CPU) of English has been released. For more details, please refer to ([docs](runtime#file-transcription-service-english-cpu)).\n- 2023/10/13: [SlideSpeech](https://slidespeech.github.io/): A large scale multi-modal audio-visual corpus with a significant amount of real-time synchronized slides.\n- 2023/10/10: The ASR-SpeakersDiarization combined pipeline [Paraformer-VAD-SPK](https://github.com/alibaba-damo-academy/FunASR/blob/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py) is now released. Experience the model to get recognition results with speaker information.\n- 2023/10/07: [FunCodec](https://github.com/alibaba-damo-academy/FunCodec): A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec.\n- 2023/09/01: The offline file transcription service 2.0 (CPU) of Mandarin has been released, with added support for ffmpeg, timestamp, and hotword models. For more details, please refer to ([docs](runtime#file-transcription-service-mandarin-cpu)).\n- 2023/08/07: The real-time transcription service (CPU) of Mandarin has been released. For more details, please refer to ([docs](runtime#the-real-time-transcription-service-mandarin-cpu)).\n- 2023/07/17: BAT is released, which is a low-latency and low-memory-consumption RNN-T model. For more details, please refer to ([BAT](egs/aishell/bat)).\n- 2023/06/26: ASRU2023 Multi-Channel Multi-Party Meeting Transcription Challenge 2.0 completed the competition and announced the results. For more details, please refer to ([M2MeT2.0](https://alibaba-damo-academy.github.io/FunASR/m2met2/index.html)).\n\n</details>\n\n<a name=\"Installation\"></a>\n## Installation\n\n- Requirements\n```text\npython>=3.8\ntorch>=1.13\ntorchaudio\n```\n\n- Install for pypi\n```shell\npip3 install -U funasr\n```\n- Or install from source code\n``` sh\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n- Install modelscope or huggingface_hub for the pretrained models (Optional)\n\n```shell\npip3 install -U modelscope huggingface_hub\n```\n\n## Model Zoo\nFunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the [Model License Agreement](./MODEL_LICENSE). Below are some representative models, for more models please refer to the [Model Zoo](./model_zoo).\n\n(Note: ⭐ represents the ModelScope model zoo, 🤗 represents the Huggingface model zoo, 🍀 represents the OpenAI model zoo)\n\n\n|                                                                                                         Model Name                                                                                                         |                                   Task Details                                   |          Training Data           | Parameters |\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------:|:--------------------------------:|:----------:|\n|                                        SenseVoiceSmall <br> ([⭐](https://www.modelscope.cn/models/iic/SenseVoiceSmall)  [🤗](https://huggingface.co/FunAudioLLM/SenseVoiceSmall) )                                         | multiple speech understanding capabilities, including ASR, ITN, LID, SER, and AED, support languages such as zh, yue, en, ja, ko   |           300000 hours           |    234M    |\n|          paraformer-zh <br> ([⭐](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)  [🤗](https://huggingface.co/funasr/paraformer-zh) )           |                speech recognition, with timestamps, non-streaming                |      60000 hours, Mandarin       |    220M    |\n| <nobr>paraformer-zh-streaming <br> ( [⭐](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) [🤗](https://huggingface.co/funasr/paraformer-zh-streaming) )</nobr> |                          speech recognition, streaming                           |      60000 hours, Mandarin       |    220M    |\n|               paraformer-en <br> ( [⭐](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) [🤗](https://huggingface.co/funasr/paraformer-en) )                |              speech recognition, without timestamps, non-streaming               |       50000 hours, English       |    220M    |\n|                            conformer-en <br> ( [⭐](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) [🤗](https://huggingface.co/funasr/conformer-en) )                             |                        speech recognition, non-streaming                         |       50000 hours, English       |    220M    |\n|                               ct-punc <br> ( [⭐](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) [🤗](https://huggingface.co/funasr/ct-punc) )                               |                             punctuation restoration                              |    100M, Mandarin and English    |    290M    | \n|                                   fsmn-vad <br> ( [⭐](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary) [🤗](https://huggingface.co/funasr/fsmn-vad) )                                   |                             voice activity detection                             | 5000 hours, Mandarin and English |    0.4M    | \n|                                                              fsmn-kws <br> ( [⭐](https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary) )                                                              |     keyword spotting，streaming      |  5000 hours, Mandarin  |    0.7M    | \n|                                     fa-zh <br> ( [⭐](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) [🤗](https://huggingface.co/funasr/fa-zh) )                                     |                               timestamp prediction                               |       5000 hours, Mandarin       |    38M     | \n|                                       cam++ <br> ( [⭐](https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary) [🤗](https://huggingface.co/funasr/campplus) )                                        |                         speaker verification/diarization                         |            5000 hours            |    7.2M    | \n|                                            Whisper-large-v3 <br> ([⭐](https://www.modelscope.cn/models/iic/Whisper-large-v3/summary)  [🍀](https://github.com/openai/whisper) )                                            |                speech recognition, with timestamps, non-streaming                |           multilingual           |   1550 M   |\n|                                      Whisper-large-v3-turbo <br> ([⭐](https://www.modelscope.cn/models/iic/Whisper-large-v3-turbo/summary)  [🍀](https://github.com/openai/whisper) )                                      |                speech recognition, with timestamps, non-streaming                |           multilingual           |   809 M    |\n|                                               Qwen-Audio <br> ([⭐](examples/industrial_data_pretraining/qwen_audio/demo.py)  [🤗](https://huggingface.co/Qwen/Qwen-Audio) )                                                |                    audio-text multimodal models (pretraining)                    |           multilingual           |     8B     |\n|                                        Qwen-Audio-Chat <br> ([⭐](examples/industrial_data_pretraining/qwen_audio/demo_chat.py)  [🤗](https://huggingface.co/Qwen/Qwen-Audio-Chat) )                                        |                       audio-text multimodal models (chat)                        |           multilingual           |     8B     |\n|                              emotion2vec+large <br> ([⭐](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)  [🤗](https://huggingface.co/emotion2vec/emotion2vec_plus_large) )                               |                           speech emotion recongintion                            |           40000 hours            |    300M    |\n\n\n\n\n[//]: # ()\n[//]: # (FunASR supports pre-trained or further fine-tuned models for deployment as a service. The CPU version of the Chinese offline file conversion service has been released, details can be found in [docs]&#40;funasr/runtime/docs/SDK_tutorial.md&#41;. More detailed information about service deployment can be found in the [deployment roadmap]&#40;funasr/runtime/readme_cn.md&#41;.)\n\n\n<a name=\"quick-start\"></a>\n## Quick Start\n\nBelow is a quick start tutorial. Test audio files ([Mandarin](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav), [English](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav)).\n\n### Command-line usage\n\n```shell\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\nNotes: Support recognition of single audio file, as well as file list in Kaldi-style wav.scp format: `wav_id wav_pat`\n\n### Speech Recognition (Non-streaming)\n#### SenseVoice\n```python\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = AutoModel(\n    model=model_dir,\n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\nParameter Description:\n- `model_dir`: The name of the model, or the path to the model on the local disk.\n- `vad_model`: This indicates the activation of VAD (Voice Activity Detection). The purpose of VAD is to split long audio into shorter clips. In this case, the inference time includes both VAD and SenseVoice total consumption, and represents the end-to-end latency. If you wish to test the SenseVoice model's inference time separately, the VAD model can be disabled.\n- `vad_kwargs`: Specifies the configurations for the VAD model. `max_single_segment_time`: denotes the maximum duration for audio segmentation by the `vad_model`, with the unit being milliseconds (ms).\n- `use_itn`: Whether the output result includes punctuation and inverse text normalization.\n- `batch_size_s`: Indicates the use of dynamic batching, where the total duration of audio in the batch is measured in seconds (s).\n- `merge_vad`: Whether to merge short audio fragments segmented by the VAD model, with the merged length being `merge_length_s`, in seconds (s).\n- `ban_emo_unk`: Whether to ban the output of the `emo_unk` token.\n\n#### Paraformer\n```python\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  vad_model=\"fsmn-vad\",  punc_model=\"ct-punc\", \n                  # spk_model=\"cam++\", \n                  )\nres = model.generate(input=f\"{model.model_path}/example/asr_example.wav\", \n                     batch_size_s=300, \n                     hotword='魔搭')\nprint(res)\n```\nNote: `hub`: represents the model repository, `ms` stands for selecting ModelScope download, `hf` stands for selecting Huggingface download.\n\n### Speech Recognition (Streaming)\n```python\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\nNote: `chunk_size` is the configuration for streaming latency.` [0,10,5]` indicates that the real-time display granularity is `10*60=600ms`, and the lookahead information is `5*60=300ms`. Each inference input is `600ms` (sample points are `16000*0.6=960`), and the output is the corresponding text. For the last speech segment input, `is_final=True` needs to be set to force the output of the last word.\n\n<details><summary>More Examples</summary>\n\n### Voice Activity Detection (Non-Streaming)\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\nNote: The output format of the VAD model is: `[[beg1, end1], [beg2, end2], ..., [begN, endN]]`, where `begN/endN` indicates the starting/ending point of the `N-th` valid audio segment, measured in milliseconds.\n\n### Voice Activity Detection (Streaming)\n```python\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\nNote: The output format for the streaming VAD model can be one of four scenarios:\n- `[[beg1, end1], [beg2, end2], .., [begN, endN]]`：The same as the offline VAD output result mentioned above.\n- `[[beg, -1]]`：Indicates that only a starting point has been detected.\n- `[[-1, end]]`：Indicates that only an ending point has been detected.\n- `[]`：Indicates that neither a starting point nor an ending point has been detected. \n\nThe output is measured in milliseconds and represents the absolute time from the starting point.\n### Punctuation Restoration\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\nres = model.generate(input=\"那今天的会就到这里吧 happy new year 明年见\")\nprint(res)\n```\n### Timestamp Prediction\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n\n### Speech Emotion Recognition\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"emotion2vec_plus_large\")\n\nwav_file = f\"{model.model_path}/example/test.wav\"\n\nres = model.generate(wav_file, output_dir=\"./outputs\", granularity=\"utterance\", extract_embedding=False)\nprint(res)\n```\n\nMore usages ref to [docs](docs/tutorial/README_zh.md), \nmore examples ref to [demo](https://github.com/alibaba-damo-academy/FunASR/tree/main/examples/industrial_data_pretraining)\n\n</details>\n\n## Export ONNX\n\n### Command-line usage\n```shell\nfunasr-export ++model=paraformer ++quantize=false ++device=cpu\n```\n\n### Python\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\", device=\"cpu\")\n\nres = model.export(quantize=False)\n```\n\n### Test ONNX\n```python\n# pip3 install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\nMore examples ref to [demo](runtime/python/onnxruntime)\n\n## Deployment Service\nFunASR supports deploying pre-trained or further fine-tuned models for service. Currently, it supports the following types of service deployment:\n- File transcription service, Mandarin, CPU version, done\n- The real-time transcription service, Mandarin (CPU), done\n- File transcription service, English, CPU version, done\n- File transcription service, Mandarin, GPU version, in progress\n- and more.\n\nFor more detailed information, please refer to the [service deployment documentation](runtime/readme.md).\n\n\n<a name=\"contact\"></a>\n## Community Communication\nIf you encounter problems in use, you can directly raise Issues on the github page.\n\nYou can also scan the following DingTalk group to join the community group for communication and discussion.\n\n|                           DingTalk group                            |\n|:-------------------------------------------------------------------:|\n| <div align=\"left\"><img src=\"docs/images/dingding.png\" width=\"250\"/> |\n\n## Contributors\n\n| <div align=\"left\"><img src=\"docs/images/alibaba.png\" width=\"260\"/> | <div align=\"left\"><img src=\"docs/images/nwpu.png\" width=\"260\"/> | <img src=\"docs/images/China_Telecom.png\" width=\"200\"/> </div>  | <img src=\"docs/images/RapidAI.png\" width=\"200\"/> </div> | <img src=\"docs/images/aihealthx.png\" width=\"200\"/> </div> | <img src=\"docs/images/XVERSE.png\" width=\"250\"/> </div> |\n|:------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------:|:-------------------------------------------------------:|:-----------------------------------------------------------:|:------------------------------------------------------:|\n\nThe contributors can be found in [contributors list](./Acknowledge.md)\n\n## License\nThis project is licensed under [The MIT License](https://opensource.org/licenses/MIT). FunASR also contains various third-party components and some code modified from other repos under other open source licenses.\nThe use of pretraining model is subject to [model license](./MODEL_LICENSE)\n\n\n## Citations\n``` bibtex\n@inproceedings{gao2023funasr,\n  author={Zhifu Gao and Zerui Li and Jiaming Wang and Haoneng Luo and Xian Shi and Mengzhe Chen and Yabin Li and Lingyun Zuo and Zhihao Du and Zhangyu Xiao and Shiliang Zhang},\n  title={FunASR: A Fundamental End-to-End Speech Recognition Toolkit},\n  year={2023},\n  booktitle={INTERSPEECH},\n}\n@inproceedings{An2023bat,\n  author={Keyu An and Xian Shi and Shiliang Zhang},\n  title={BAT: Boundary aware transducer for memory-efficient and low-latency ASR},\n  year={2023},\n  booktitle={INTERSPEECH},\n}\n@inproceedings{gao22b_interspeech,\n  author={Zhifu Gao and ShiLiang Zhang and Ian McLoughlin and Zhijie Yan},\n  title={Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition},\n  year=2022,\n  booktitle={Proc. Interspeech 2022},\n  pages={2063--2067},\n  doi={10.21437/Interspeech.2022-9996}\n}\n@inproceedings{shi2023seaco,\n  author={Xian Shi and Yexin Yang and Zerui Li and Yanni Chen and Zhifu Gao and Shiliang Zhang},\n  title={SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability},\n  year={2023},\n  booktitle={ICASSP2024}\n}\n```\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 25.0224609375,
          "content": "[//]: # (<div align=\"left\"><img src=\"docs/images/funasr_logo.jpg\" width=\"400\"/></div>)\n\n(简体中文|[English](./README.md))\n\n\n\n[![SVG Banners](https://svg-banners.vercel.app/api?type=origin&text1=FunASR🤠&text2=💖%20A%20Fundamental%20End-to-End%20Speech%20Recognition%20Toolkit&width=800&height=210)](https://github.com/Akshay090/svg-banners)\n\n[//]: # (# FunASR: A Fundamental End-to-End Speech Recognition Toolkit)\n\n[![PyPI](https://img.shields.io/pypi/v/funasr)](https://pypi.org/project/funasr/)\n\n\nFunASR希望在语音识别的学术研究和工业应用之间架起一座桥梁。通过发布工业级语音识别模型的训练和微调，研究人员和开发人员可以更方便地进行语音识别模型的研究和生产，并推动语音识别生态的发展。让语音识别更有趣！\n\n<div align=\"center\">  \n<h4>\n <a href=\"#核心功能\"> 核心功能 </a>   \n｜<a href=\"#最新动态\"> 最新动态 </a>\n｜<a href=\"#安装教程\"> 安装 </a>\n｜<a href=\"#快速开始\"> 快速开始 </a>\n｜<a href=\"https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/tutorial/README_zh.md\"> 教程文档 </a>\n｜<a href=\"#模型仓库\"> 模型仓库 </a>\n｜<a href=\"#服务部署\"> 服务部署 </a>\n｜<a href=\"#联系我们\"> 联系我们 </a>\n</h4>\n</div>\n\n<a name=\"核心功能\"></a>\n## 核心功能\n- FunASR是一个基础语音识别工具包，提供多种功能，包括语音识别（ASR）、语音端点检测（VAD）、标点恢复、语言模型、说话人验证、说话人分离和多人对话语音识别等。FunASR提供了便捷的脚本和教程，支持预训练好的模型的推理与微调。\n- 我们在[ModelScope](https://www.modelscope.cn/models?page=1&tasks=auto-speech-recognition)与[huggingface](https://huggingface.co/FunASR)上发布了大量开源数据集或者海量工业数据训练的模型，可以通过我们的[模型仓库](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/model_zoo/modelscope_models.md)了解模型的详细信息。代表性的[Paraformer](https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)非自回归端到端语音识别模型具有高精度、高效率、便捷部署的优点，支持快速构建语音识别服务，详细信息可以阅读([服务部署文档](runtime/readme_cn.md))。\n\n<a name=\"最新动态\"></a>\n## 最新动态\n- 2024/10/29: 中文实时语音听写服务 1.12 发布，2pass-offline模式支持SensevoiceSmall模型；详细信息参阅([部署文档](runtime/readme_cn.md))\n- 2024/10/10：新增加Whisper-large-v3-turbo模型支持，多语言语音识别/翻译/语种识别，支持从 [modelscope](examples/industrial_data_pretraining/whisper/demo.py)仓库下载，也支持从 [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py)仓库下载模型。\n- 2024/09/26: 中文离线文件转写服务 4.6、英文离线文件转写服务 1.7、中文实时语音听写服务 1.11 发布，修复ONNX内存泄漏、支持SensevoiceSmall onnx模型；中文离线文件转写服务GPU 2.0 发布，修复显存泄漏; 详细信息参阅([部署文档](runtime/readme_cn.md))\n- 2024/09/25：新增语音唤醒模型，支持[fsmn_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [fsmn_kws_mt](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [sanm_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-offline), [sanm_kws_streaming](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online) 4个模型的微调和推理。\n- 2024/07/04：[SenseVoice](https://github.com/FunAudioLLM/SenseVoice) 是一个基础语音理解模型，具备多种语音理解能力，涵盖了自动语音识别（ASR）、语言识别（LID）、情感识别（SER）以及音频事件检测（AED）。\n- 2024/07/01：中文离线文件转写服务GPU版本 1.1发布，优化bladedisc模型兼容性问题；详细信息参阅([部署文档](runtime/readme_cn.md))\n- 2024/06/27：中文离线文件转写服务GPU版本 1.0发布，支持动态batch，支持多路并发，在长音频测试集上单线RTF为0.0076，多线加速比为1200+（CPU为330+）；详细信息参阅([部署文档](runtime/readme_cn.md))\n- 2024/05/15：新增加情感识别模型，[emotion2vec+large](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)，[emotion2vec+base](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary)，[emotion2vec+seed](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary)，输出情感类别为：生气/angry，开心/happy，中立/neutral，难过/sad。\n- 2024/05/15: 中文离线文件转写服务 4.5、英文离线文件转写服务 1.6、中文实时语音听写服务 1.10 发布，适配FunASR 1.0模型结构；详细信息参阅([部署文档](runtime/readme_cn.md))\n- 2024/03/05：新增加Qwen-Audio与Qwen-Audio-Chat音频文本模态大模型，在多个音频领域测试榜单刷榜，中支持语音对话，详细用法见 [示例](examples/industrial_data_pretraining/qwen_audio)。\n- 2024/03/05：新增加Whisper-large-v3模型支持，多语言语音识别/翻译/语种识别，支持从 [modelscope](examples/industrial_data_pretraining/whisper/demo.py)仓库下载，也支持从 [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py)仓库下载模型。\n- 2024/03/05: 中文离线文件转写服务 4.4、英文离线文件转写服务 1.5、中文实时语音听写服务 1.9 发布，docker镜像支持arm64平台，升级modelscope版本；详细信息参阅([部署文档](runtime/readme_cn.md))\n- 2024/01/30：funasr-1.0发布，更新说明[文档](https://github.com/alibaba-damo-academy/FunASR/discussions/1319)\n\n<details><summary>展开日志</summary>\n\n- 2024/01/30：新增加情感识别 [模型链接](https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary)，原始模型 [repo](https://github.com/ddlBoJack/emotion2vec).\n- 2024/01/25: 中文离线文件转写服务 4.2、英文离线文件转写服务 1.3，优化vad数据处理方式，大幅降低峰值内存占用，内存泄漏优化；中文实时语音听写服务 1.7 发布，客户端优化；详细信息参阅([部署文档](runtime/readme_cn.md))\n- 2024/01/09: funasr社区软件包windows 2.0版本发布，支持软件包中文离线文件转写4.1、英文离线文件转写1.2、中文实时听写服务1.6的最新功能，详细信息参阅([FunASR社区软件包windows版本](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))\n- 2024/01/03: 中文离线文件转写服务 4.0 发布，新增支持8k模型、优化时间戳不匹配问题及增加句子级别时间戳、优化英文单词fst热词效果、支持自动化配置线程参数，同时修复已知的crash问题及内存泄漏问题，详细信息参阅([部署文档](runtime/readme_cn.md#中文离线文件转写服务cpu版本))\n- 2024/01/03: 中文实时语音听写服务 1.6 发布，2pass-offline模式支持Ngram语言模型解码、wfst热词，同时修复已知的crash问题及内存泄漏问题，详细信息参阅([部署文档](runtime/readme_cn.md#中文实时语音听写服务cpu版本))\n- 2024/01/03: 英文离线文件转写服务 1.2 发布，修复已知的crash问题及内存泄漏问题，详细信息参阅([部署文档](runtime/readme_cn.md#英文离线文件转写服务cpu版本))\n- 2023/12/04: funasr社区软件包windows 1.0版本发布，支持中文离线文件转写、英文离线文件转写、中文实时听写服务，详细信息参阅([FunASR社区软件包windows版本](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))\n- 2023/11/08：中文离线文件转写服务3.0 CPU版本发布，新增标点大模型、Ngram语言模型与wfst热词，详细信息参阅([部署文档](runtime/readme_cn.md#中文离线文件转写服务cpu版本))\n- 2023/10/17: 英文离线文件转写服务一键部署的CPU版本发布，详细信息参阅([部署文档](runtime/readme_cn.md#英文离线文件转写服务cpu版本))\n- 2023/10/13: [SlideSpeech](https://slidespeech.github.io/): 一个大规模的多模态音视频语料库，主要是在线会议或者在线课程场景，包含了大量与发言人讲话实时同步的幻灯片。\n- 2023.10.10: [Paraformer-long-Spk](https://github.com/alibaba-damo-academy/FunASR/blob/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py)模型发布，支持在长语音识别的基础上获取每句话的说话人标签。\n- 2023.10.07: [FunCodec](https://github.com/alibaba-damo-academy/FunCodec): FunCodec提供开源模型和训练工具，可以用于音频离散编码，以及基于离散编码的语音识别、语音合成等任务。\n- 2023.09.01: 中文离线文件转写服务2.0 CPU版本发布，新增ffmpeg、时间戳与热词模型支持，详细信息参阅([部署文档](runtime/readme_cn.md#中文离线文件转写服务cpu版本))\n- 2023.08.07: 中文实时语音听写服务一键部署的CPU版本发布，详细信息参阅([部署文档](runtime/readme_cn.md#中文实时语音听写服务cpu版本))\n- 2023.07.17: BAT一种低延迟低内存消耗的RNN-T模型发布，详细信息参阅（[BAT](egs/aishell/bat)）\n- 2023.06.26: ASRU2023 多通道多方会议转录挑战赛2.0完成竞赛结果公布，详细信息参阅（[M2MeT2.0](https://alibaba-damo-academy.github.io/FunASR/m2met2_cn/index.html)）\n\n</details>\n\n<a name=\"安装教程\"></a>\n## 安装教程\n\n- 安装funasr之前，确保已经安装了下面依赖环境:\n```text\npython>=3.8\ntorch>=1.13\ntorchaudio\n```\n\n- pip安装\n```shell\npip3 install -U funasr\n```\n\n- 或者从源代码安装\n``` sh\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n如果需要使用工业预训练模型，安装modelscope与huggingface_hub（可选）\n\n```shell\npip3 install -U modelscope huggingface huggingface_hub\n```\n\n## 模型仓库\n\nFunASR开源了大量在工业数据上预训练模型，您可以在[模型许可协议](./MODEL_LICENSE)下自由使用、复制、修改和分享FunASR模型，下面列举代表性的模型，更多模型请参考 [模型仓库](./model_zoo)。\n\n（注：⭐ 表示ModelScope模型仓库，🤗 表示Huggingface模型仓库，🍀表示OpenAI模型仓库）\n\n\n|                                                                                                     模型名字                                                                                                      |        任务详情        |      训练数据      |  参数量   | \n|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------:|:--------------:|:------:|\n|                                  SenseVoiceSmall <br> ([⭐](https://www.modelscope.cn/models/iic/SenseVoiceSmall)  [🤗](https://huggingface.co/FunAudioLLM/SenseVoiceSmall) )                                  |  多种语音理解能力，涵盖了自动语音识别（ASR）、语言识别（LID）、情感识别（SER）以及音频事件检测（AED）   |  400000小时，中文   |  330M  |\n|    paraformer-zh <br> ([⭐](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)  [🤗](https://huggingface.co/funasr/paraformer-zh) )    |  语音识别，带时间戳输出，非实时   |   60000小时，中文   |  220M  |\n| paraformer-zh-streaming <br> ( [⭐](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) [🤗](https://huggingface.co/funasr/paraformer-zh-streaming) ) |      语音识别，实时       |   60000小时，中文   |  220M  |\n|         paraformer-en <br> ( [⭐](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) [🤗](https://huggingface.co/funasr/paraformer-en) )         |      语音识别，非实时      |   50000小时，英文   |  220M  |\n|                      conformer-en <br> ( [⭐](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) [🤗](https://huggingface.co/funasr/conformer-en) )                      |      语音识别，非实时      |   50000小时，英文   |  220M  |\n|                        ct-punc <br> ( [⭐](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) [🤗](https://huggingface.co/funasr/ct-punc) )                         |        标点恢复        |   100M，中文与英文   |  290M  | \n|                            fsmn-vad <br> ( [⭐](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary) [🤗](https://huggingface.co/funasr/fsmn-vad) )                             |     语音端点检测，实时      |  5000小时，中文与英文  |  0.4M  | \n|                                                       fsmn-kws <br> ( [⭐](https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary) )                                                        |     语音唤醒，实时      |  5000小时，中文  |  0.7M  | \n|                              fa-zh <br> ( [⭐](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) [🤗](https://huggingface.co/funasr/fa-zh) )                               |      字级别时间戳预测      |   50000小时，中文   |  38M   |\n|                                 cam++ <br> ( [⭐](https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary) [🤗](https://huggingface.co/funasr/campplus) )                                 |      说话人确认/分割      |     5000小时     |  7.2M  | \n|                                     Whisper-large-v3 <br> ([⭐](https://www.modelscope.cn/models/iic/Whisper-large-v3/summary)  [🍀](https://github.com/openai/whisper) )                                      |  语音识别，带时间戳输出，非实时   |      多语言       | 1550 M |\n|                               Whisper-large-v3-turbo <br> ([⭐](https://www.modelscope.cn/models/iic/Whisper-large-v3-turbo/summary)  [🍀](https://github.com/openai/whisper) )                                |  语音识别，带时间戳输出，非实时   |      多语言       | 809 M |\n|                                         Qwen-Audio <br> ([⭐](examples/industrial_data_pretraining/qwen_audio/demo.py)  [🤗](https://huggingface.co/Qwen/Qwen-Audio) )                                         |  音频文本多模态大模型（预训练）   |      多语言       |   8B   |\n|                                 Qwen-Audio-Chat <br> ([⭐](examples/industrial_data_pretraining/qwen_audio/demo_chat.py)  [🤗](https://huggingface.co/Qwen/Qwen-Audio-Chat) )                                  | 音频文本多模态大模型（chat版本） |      多语言       |   8B   |\n|                        emotion2vec+large <br> ([⭐](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)  [🤗](https://huggingface.co/emotion2vec/emotion2vec_plus_large) )                        |    情感识别模型          | 40000小时，4种情感类别 |  300M  |\n\n<a name=\"快速开始\"></a>\n## 快速开始\n\n下面为快速上手教程，测试音频（[中文](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav)，[英文](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav)）\n\n### 可执行命令行\n\n```shell\nfunasr ++model=paraformer-zh ++vad_model=\"fsmn-vad\" ++punc_model=\"ct-punc\" ++input=asr_example_zh.wav\n```\n\n注：支持单条音频文件识别，也支持文件列表，列表为kaldi风格wav.scp：`wav_id   wav_path`\n\n### 非实时语音识别\n#### SenseVoice\n```python\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = AutoModel(\n    model=model_dir,\n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zn\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\n参数说明：\n- `model_dir`：模型名称，或本地磁盘中的模型路径。\n- `vad_model`：表示开启VAD，VAD的作用是将长音频切割成短音频，此时推理耗时包括了VAD与SenseVoice总耗时，为链路耗时，如果需要单独测试SenseVoice模型耗时，可以关闭VAD模型。\n- `vad_kwargs`：表示VAD模型配置,`max_single_segment_time`: 表示`vad_model`最大切割音频时长, 单位是毫秒ms。\n- `use_itn`：输出结果中是否包含标点与逆文本正则化。\n- `batch_size_s` 表示采用动态batch，batch中总音频时长，单位为秒s。\n- `merge_vad`：是否将 vad 模型切割的短音频碎片合成，合并后长度为`merge_length_s`，单位为秒s。\n- `ban_emo_unk`：禁用emo_unk标签，禁用后所有的句子都会被赋与情感标签。\n\n#### Paraformer\n```python\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\",  vad_model=\"fsmn-vad\", punc_model=\"ct-punc\", \n                  # spk_model=\"cam++\"\n                  )\nres = model.generate(input=f\"{model.model_path}/example/asr_example.wav\", \n            batch_size_s=300, \n            hotword='魔搭')\nprint(res)\n```\n注：`hub`：表示模型仓库，`ms`为选择modelscope下载，`hf`为选择huggingface下载。\n\n### 实时语音识别\n\n```python\nfrom funasr import AutoModel\n\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\n\nmodel = AutoModel(model=\"paraformer-zh-streaming\")\n\nimport soundfile\nimport os\n\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\n    print(res)\n```\n\n注：`chunk_size`为流式延时配置，`[0,10,5]`表示上屏实时出字粒度为`10*60=600ms`，未来信息为`5*60=300ms`。每次推理输入为`600ms`（采样点数为`16000*0.6=960`），输出为对应文字，最后一个语音片段输入需要设置`is_final=True`来强制输出最后一个字。\n\n<details><summary>更多例子</summary>\n\n### 语音端点检测（非实时）\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fsmn-vad\")\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\n```\n注：VAD模型输出格式为：`[[beg1, end1], [beg2, end2], .., [begN, endN]]`，其中`begN/endN`表示第`N`个有效音频片段的起始点/结束点，\n单位为毫秒。\n\n### 语音端点检测（实时）\n```python\nfrom funasr import AutoModel\n\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\")\n\nimport soundfile\n\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\n\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\n    speech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\n    is_final = i == total_chunk_num - 1\n    res = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\n    if len(res[0][\"value\"]):\n        print(res)\n```\n注：流式VAD模型输出格式为4种情况：\n- `[[beg1, end1], [beg2, end2], .., [begN, endN]]`：同上离线VAD输出结果。\n- `[[beg, -1]]`：表示只检测到起始点。\n- `[[-1, end]]`：表示只检测到结束点。\n- `[]`：表示既没有检测到起始点，也没有检测到结束点\n输出结果单位为毫秒，从起始点开始的绝对时间。\n\n### 标点恢复\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"ct-punc\")\n\nres = model.generate(input=\"那今天的会就到这里吧 happy new year 明年见\")\nprint(res)\n```\n\n### 时间戳预测\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"fa-zh\")\n\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\n```\n\n### 情感识别\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"emotion2vec_plus_large\")\n\nwav_file = f\"{model.model_path}/example/test.wav\"\n\nres = model.generate(wav_file, output_dir=\"./outputs\", granularity=\"utterance\", extract_embedding=False)\nprint(res)\n```\n\n更详细（[教程文档](docs/tutorial/README_zh.md)），\n更多（[模型示例](https://github.com/alibaba-damo-academy/FunASR/tree/main/examples/industrial_data_pretraining)）\n\n</details>\n\n## 导出ONNX\n### 从命令行导出\n```shell\nfunasr-export ++model=paraformer ++quantize=false\n```\n\n### 从Python导出\n```python\nfrom funasr import AutoModel\n\nmodel = AutoModel(model=\"paraformer\")\n\nres = model.export(quantize=False)\n```\n\n### 测试ONNX\n```python\n# pip3 install -U funasr-onnx\nfrom funasr_onnx import Paraformer\nmodel_dir = \"damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\nmodel = Paraformer(model_dir, batch_size=1, quantize=True)\n\nwav_path = ['~/.cache/modelscope/hub/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/example/asr_example.wav']\n\nresult = model(wav_path)\nprint(result)\n```\n\n更多例子请参考 [样例](runtime/python/onnxruntime)\n\n<a name=\"服务部署\"></a>\n## 服务部署\nFunASR支持预训练或者进一步微调的模型进行服务部署。目前支持以下几种服务部署：\n\n- 中文离线文件转写服务（CPU版本），已完成\n- 中文流式语音识别服务（CPU版本），已完成\n- 英文离线文件转写服务（CPU版本），已完成\n- 中文离线文件转写服务（GPU版本），进行中\n- 更多支持中\n\n详细信息可以参阅([服务部署文档](runtime/readme_cn.md))。\n\n\n<a name=\"社区交流\"></a>\n## 联系我们\n\n如果您在使用中遇到问题，可以直接在github页面提Issues。欢迎语音兴趣爱好者扫描以下的钉钉群二维码加入社区群，进行交流和讨论。\n\n|                                 钉钉群                                 |\n|:-------------------------------------------------------------------:|\n| <div align=\"left\"><img src=\"docs/images/dingding.png\" width=\"250\"/> |\n\n## 社区贡献者\n\n| <div align=\"left\"><img src=\"docs/images/alibaba.png\" width=\"260\"/> | <div align=\"left\"><img src=\"docs/images/nwpu.png\" width=\"260\"/> | <img src=\"docs/images/China_Telecom.png\" width=\"200\"/> </div>  | <img src=\"docs/images/RapidAI.png\" width=\"200\"/> </div> | <img src=\"docs/images/aihealthx.png\" width=\"200\"/> </div> | <img src=\"docs/images/XVERSE.png\" width=\"250\"/> </div> |\n|:------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------:|:-------------------------------------------------------:|:-----------------------------------------------------------:|:------------------------------------------------------:|\n\n贡献者名单请参考（[致谢名单](./Acknowledge.md)）\n\n\n## 许可协议\n项目遵循[The MIT License](https://opensource.org/licenses/MIT)开源协议，模型许可协议请参考（[模型协议](./MODEL_LICENSE)）\n\n\n## 论文引用\n\n``` bibtex\n@inproceedings{gao2023funasr,\n  author={Zhifu Gao and Zerui Li and Jiaming Wang and Haoneng Luo and Xian Shi and Mengzhe Chen and Yabin Li and Lingyun Zuo and Zhihao Du and Zhangyu Xiao and Shiliang Zhang},\n  title={FunASR: A Fundamental End-to-End Speech Recognition Toolkit},\n  year={2023},\n  booktitle={INTERSPEECH},\n}\n@inproceedings{An2023bat,\n  author={Keyu An and Xian Shi and Shiliang Zhang},\n  title={BAT: Boundary aware transducer for memory-efficient and low-latency ASR},\n  year={2023},\n  booktitle={INTERSPEECH},\n}\n@inproceedings{gao22b_interspeech,\n  author={Zhifu Gao and ShiLiang Zhang and Ian McLoughlin and Zhijie Yan},\n  title={{Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition}},\n  year=2022,\n  booktitle={Proc. Interspeech 2022},\n  pages={2063--2067},\n  doi={10.21437/Interspeech.2022-9996}\n}\n@article{shi2023seaco,\n  author={Xian Shi and Yexin Yang and Zerui Li and Yanni Chen and Zhifu Gao and Shiliang Zhang},\n  title={{SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability}},\n  year=2023,\n  journal={arXiv preprint arXiv:2308.03266(accepted by ICASSP2024)},\n}\n```\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "export.py",
          "type": "blob",
          "size": 0.375,
          "content": "# method2, inference from local path\r\nfrom funasr import AutoModel\r\n\r\nmodel = AutoModel(\r\n    model=\"iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch\"\r\n)\r\n\r\nres = model.export(type=\"onnx\", quantize=False, opset_version=13, device='cuda')  # fp32 onnx-gpu\r\n# res = model.export(type=\"onnx_fp16\", quantize=False, opset_version=13, device='cuda')  # fp16 onnx-gpu\r\n"
        },
        {
          "name": "fun_text_processing",
          "type": "tree",
          "content": null
        },
        {
          "name": "funasr",
          "type": "tree",
          "content": null
        },
        {
          "name": "model_zoo",
          "type": "tree",
          "content": null
        },
        {
          "name": "runtime",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.7392578125,
          "content": "#!/usr/bin/env python3\n\n\"\"\"FunASR setup script.\"\"\"\n\nimport os\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n\nrequirements = {\n    \"install\": [\n        \"scipy>=1.4.1\",\n        \"librosa\",\n        \"jamo\",  # For kss\n        \"PyYAML>=5.1.2\",\n        \"soundfile>=0.12.1\",\n        \"kaldiio>=2.17.0\",\n        \"torch_complex\",\n        # \"nltk>=3.4.5\",\n        \"sentencepiece\",  # train\n        \"jieba\",\n        # \"rotary_embedding_torch\",\n        # \"ffmpeg-python\",\n        # \"pypinyin>=0.44.0\",\n        # \"espnet_tts_frontend\",\n        # ENH\n        \"pytorch_wpe\",\n        \"editdistance>=0.5.2\",\n        # \"g2p\",\n        # \"nara_wpe\",\n        # PAI\n        \"oss2\",\n        # \"edit-distance\",\n        # \"textgrid\",\n        # \"protobuf\",\n        \"tqdm\",\n        \"umap_learn\",\n        \"jaconv\",\n        \"hydra-core>=1.3.2\",\n        \"tensorboardX\",\n        # \"rotary_embedding_torch\",\n        \"requests\",\n        \"modelscope\",\n    ],\n    # train: The modules invoked when training only.\n    \"train\": [\n        \"editdistance\",\n    ],\n    # all: The modules should be optionally installled due to some reason.\n    #      Please consider moving them to \"install\" occasionally\n    \"all\": [\n        # NOTE(kamo): Append modules requiring specific pytorch version or torch>1.3.0\n        \"torch_optimizer\",\n        \"fairscale\",\n        \"transformers\",\n        \"openai-whisper\",\n    ],\n    \"setup\": [\n        \"numpy\",\n        \"pytest-runner\",\n    ],\n    \"test\": [\n        \"pytest>=3.3.0\",\n        \"pytest-timeouts>=1.2.1\",\n        \"pytest-pythonpath>=0.7.3\",\n        \"pytest-cov>=2.7.1\",\n        \"hacking>=2.0.0\",\n        \"mock>=2.0.0\",\n        \"pycodestyle\",\n        \"jsondiff<2.0.0,>=1.2.0\",\n        \"flake8>=3.7.8\",\n        \"flake8-docstrings>=1.3.1\",\n        \"black\",\n    ],\n    \"doc\": [\n        \"Jinja2\",\n        \"Sphinx\",\n        \"sphinx-rtd-theme>=0.2.4\",\n        \"sphinx-argparse>=0.2.5\",\n        \"commonmark\",\n        \"recommonmark>=0.4.0\",\n        \"nbsphinx>=0.4.2\",\n        \"sphinx-markdown-tables>=0.0.12\",\n        \"configargparse>=1.2.1\",\n    ],\n    \"llm\": [\n        \"transformers>=4.32.0\",\n        \"accelerate\",\n        \"tiktoken\",\n        \"einops\",\n        \"transformers_stream_generator>=0.0.4\",\n        \"scipy\",\n        \"torchvision\",\n        \"pillow\",\n        \"matplotlib\",\n    ],\n}\nrequirements[\"all\"].extend(requirements[\"train\"])\nrequirements[\"all\"].extend(requirements[\"llm\"])\nrequirements[\"test\"].extend(requirements[\"train\"])\n\ninstall_requires = requirements[\"install\"]\nsetup_requires = requirements[\"setup\"]\ntests_require = requirements[\"test\"]\nextras_require = {k: v for k, v in requirements.items() if k not in [\"install\", \"setup\"]}\n\ndirname = os.path.dirname(__file__)\nversion_file = os.path.join(dirname, \"funasr\", \"version.txt\")\nwith open(version_file, \"r\") as f:\n    version = f.read().strip()\nsetup(\n    name=\"funasr\",\n    version=version,\n    url=\"https://github.com/alibaba-damo-academy/FunASR.git\",\n    author=\"Speech Lab of Alibaba Group\",\n    author_email=\"funasr@list.alibaba-inc.com\",\n    description=\"FunASR: A Fundamental End-to-End Speech Recognition Toolkit\",\n    long_description=open(os.path.join(dirname, \"README.md\"), encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    license=\"The MIT License\",\n    packages=find_packages(include=[\"funasr*\"]),\n    package_data={\"funasr\": [\"version.txt\"]},\n    install_requires=install_requires,\n    setup_requires=setup_requires,\n    tests_require=tests_require,\n    extras_require=extras_require,\n    python_requires=\">=3.7.0\",\n    classifiers=[\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Science/Research\",\n        \"Operating System :: POSIX :: Linux\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"funasr = funasr.bin.inference:main_hydra\",\n            \"funasr-train = funasr.bin.train:main_hydra\",\n            \"funasr-export = funasr.bin.export:main_hydra\",\n            \"scp2jsonl = funasr.datasets.audio_datasets.scp2jsonl:main_hydra\",\n            \"jsonl2scp = funasr.datasets.audio_datasets.jsonl2scp:main_hydra\",\n            \"sensevoice2jsonl = funasr.datasets.audio_datasets.sensevoice2jsonl:main_hydra\",\n            \"funasr-scp2jsonl = funasr.datasets.audio_datasets.scp2jsonl:main_hydra\",\n            \"funasr-jsonl2scp = funasr.datasets.audio_datasets.jsonl2scp:main_hydra\",\n            \"funasr-sensevoice2jsonl = funasr.datasets.audio_datasets.sensevoice2jsonl:main_hydra\",\n        ]\n    },\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "web-pages",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}