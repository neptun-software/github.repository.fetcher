{
  "metadata": {
    "timestamp": 1736560825990,
    "page": 520,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "kingoflolz/mesh-transformer-jax",
      "stars": 6314,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0205078125,
          "content": "*.pyc\n*.pprof\n/ckpt/\n"
        },
        {
          "name": "CITATION.bib",
          "type": "blob",
          "size": 0.4990234375,
          "content": "@misc{mesh-transformer-jax,\n  author = {Wang, Ben},\n  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},\n  howpublished = {\\url{https://github.com/kingoflolz/mesh-transformer-jax}},\n  year = 2021,\n  month = May\n}\n\n@misc{gpt-j,\n  author = {Wang, Ben and Komatsuzaki, Aran},\n  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},\n  howpublished = {\\url{https://github.com/kingoflolz/mesh-transformer-jax}},\n  year = 2021,\n  month = May\n}\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 10.5029296875,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        https://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   Copyright 2021 Ben Wang\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       https://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.0595703125,
          "content": "# Table of contents\n1. [Mesh Transformer JAX](#mesh-transformer-jax)\n    1. [Updates](#updates)\n2. [Pretrained Models](#pretrained-models)\n   1. [GPT-J-6B](#gpt-j-6b)\n      1. [Links](#links)\n      2. [Acknowledgments](#acknowledgments)\n      3. [License](#license)\n      4. [Model Details](#model-details)\n      5. [Zero-Shot Evaluations](#zero-shot-evaluations)\n3. [Architecture and Usage](#architecture-and-usage)\n   1. [Fine-tuning](#fine-tuning)\n   2. [JAX Dependency](#jax-dependency)\n4. [TODO](#todo)\n\n# Mesh Transformer JAX\n\nA haiku library using the `xmap`/`pjit` operators in JAX for model parallelism of transformers.\n\nThe parallelism scheme is similar to the [original Megatron-LM](https://arxiv.org/abs/1909.08053), which is efficient\non TPUs due to the high speed 2d mesh network. There is also an experimental model version which implements [ZeRo style\nsharding](https://arxiv.org/abs/1910.02054).\n\nThis library is designed for scalability up to approximately 40B parameters on TPUv3s, beyond which different\nparallelism strategies should be used. See other implementations such as\n[GPT-NeoX](https://github.com/EleutherAI/gpt-neox) or [DeepSpeed](https://github.com/microsoft/DeepSpeed) for that.\n\nOne future direction for research is integrating this codebase with\n[swarm-jax](https://github.com/kingoflolz/swarm-jax), to achieve further scalability with pipeline parallelism.\n\n## Updates\n\n**12-07-21**: Added [guide to fine tuning](howto_finetune.md)\n\n# Pretrained Models\n\n## GPT-J-6B\n\nA 6 billion parameter, autoregressive text generation model trained on [The Pile](https://pile.eleuther.ai/).\n\n### Links\n\n[Download slim weights (bf16 weights only, for inference, 9GB)](https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd)\n\n[Download full weights (including optimizer params, 61GB)](https://the-eye.eu/public/AI/GPT-J-6B/step_383500.tar.zstd)\n\n[Partially trained checkpoints](https://the-eye.eu/public/AI/GPT-J-6B/)\n\n[Colab demo](http://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb)\n\n[Web demo](https://6b.eleuther.ai/)\n\n[Aran's blog post](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)\n\n### Acknowledgments\n\nThis project would not have been possible without compute generously provided by the\n[TPU Research Cloud](https://sites.research.google/trc/) with assistance from [EleutherAI](https://eleuther.ai/).\n\nThanks to the Cloud TPU team at Google for providing early access to the Cloud TPU VM alpha\n([now publicly available!](https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms))\n\nThanks to everyone who have helped out one way or another (listed alphabetically):\n- [Aran Komatsuzaki](https://twitter.com/arankomatsuzaki) for advice with experiment design and writing the blog posts.\n- [James Bradbury](https://twitter.com/jekbradbury) for valuable assistance with debugging JAX issues.\n- [Janko Prester](https://github.com/jprester) for creating the web demo frontend.\n- [Laurence Golding](https://github.com/researcher2) for adding some features to the web demo.\n- [Leo Gao](https://twitter.com/nabla_theta) for running zero shot evaluations for the baseline models for the table.\n\n### License\nThe weights of GPT-J-6B are licensed under version 2.0 of the Apache License.\n\n### Model Details\n\n| Hyperparameter    | Value  |\n|-------------------|--------|\n| n_parameters      | 6,053,381,344 |\n| n_layers          | 28*    |\n| d_model           | 4,096  |\n| d_ff              | 16,384 |\n| n_heads           | 16     |\n| d_head            | 256    |\n| n_ctx             | 2,048  |\n| n_vocab           | 50,257 (same tokenizer as GPT-2/3)  |\n| position encoding | [Rotary position encodings (RoPE)](https://arxiv.org/abs/2104.09864) |\n| RoPE dimensions   | [64](https://github.com/kingoflolz/mesh-transformer-jax/blob/f2aa66e0925de6593dcbb70e72399b97b4130482/mesh_transformer/layers.py#L223) |\n\n`*` each layer consists of one feedforward block and one self attention block\n\nThe model consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model\ndimension is split into 16 heads, each with a dimension of 256. Rotary position encodings (RoPE) was applied to 64\ndimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as\nGPT-2/GPT-3.\n\n### Zero-Shot Evaluations\n\nModels roughly sorted by performance, or by FLOPs if not available.\n\n|  Model          | Weights | Training FLOPs | LAMBADA PPL ↓ | LAMBADA Acc ↑ | Winogrande ↑ | Hellaswag ↑ | PIQA ↑ | Dataset Size (GB) |\n|-----------------|---------|----------------|---            |---            |---           |---          |---     |-------------------|\n| Chance          | ✔       | 0              | ~a lot        | ~0%           | 50%          | 25%         | 25%    | 0                 |\n| GPT-3-Ada‡      | ✘       | -----          | 9.95          | 51.6%         | 52.9%        | 43.4%       | 70.5%  | -----             |\n| GPT-2-1.5B      | ✔       | -----          | 10.63         | 51.21%        | 59.4%        | 50.9%       | 70.8%  | 40                |\n| GPTNeo-1.3B‡    | ✔       | 3.0e21         | 7.50          | 57.2%         | 55.0%        | 48.9%       | 71.1%  | 825               |\n| Megatron-2.5B*  | ✘       | 2.4e21         | -----         | 61.7%         | -----        | -----       | -----  | 174               |\n| GPTNeo-2.7B‡    | ✔       | 6.8e21         | 5.63          | 62.2%         | 56.5%        | 55.8%       | 73.0%  | 825               |\n| GPT-3-1.3B*‡    | ✘       | 2.4e21         | 5.44          | 63.6%         | 58.7%        | 54.7%       | 75.1%  | ~800              |\n| GPT-3-Babbage‡  | ✘       | -----          | 5.58          | 62.4%         | 59.0%        | 54.5%       | 75.5%  | -----             |\n| Megatron-8.3B*  | ✘       | 7.8e21         | -----         | 66.5%         | -----        | -----       | -----  | 174               |\n| GPT-3-2.7B*‡    | ✘       | 4.8e21         | 4.60          | 67.1%         | 62.3%        | 62.8%       | 75.6%  | ~800              |\n| Megatron-11B†   | ✔       | 1.0e22         | -----         | -----         | -----        | -----       | -----  | 161               |\n| **GPT-J-6B**‡   | ✔       | 1.5e22         | 3.99          | 69.7%         | 65.3%        | 66.1%       | 76.5%  | 825               |\n| GPT-3-6.7B*‡    | ✘       | 1.2e22         | 4.00          | 70.3%         | 64.5%        | 67.4%       | 78.0%  | ~800              |\n| GPT-3-Curie‡    | ✘       | -----          | 4.00          | 69.3%         | 65.6%        | 68.5%       | 77.9%  | -----             |\n| GPT-3-13B*‡     | ✘       | 2.3e22         | 3.56          | 72.5%         | 67.9%        | 70.9%       | 78.5%  | ~800              |\n| GPT-3-175B*‡    | ✘       | 3.1e23         | 3.00          | 76.2%         | 70.2%        | 78.9%       | 81.0%  | ~800              |\n| GPT-3-Davinci‡  | ✘       | -----          | 3.0           | 75%           | 72%          | 78%         | 80%    | -----             |\n| Gopher 230B*\t  | ✘\t    | 6.31E+23\t     | -----    \t | 74.50%        | 70.10%   \t| 79.20%      | 81.80% | 1344              |\n| MT-NLG 530B*‡   | ✘       | -----          | -----         | 76.6%         | 73.0%        | 80.2%       | 82.0%  | -----             |\n\n`*` represents evaluation numbers reported by their respective authors, all other numbers are provided by\nrunning the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/) either with the released\nweights or with API access. Due to subtle implementation differences as well as different zero shot task framing, these\nmight not be directly comparable. See [this blog post](https://www.eleuther.ai/research-log/gpt3-model-sizes/) for more\ndetails.\n\n`†` The Megatron-11B model provides no comparable metrics, and several implementations using the released weights do not\nreproduce the generation quality and evaluations. (see [1](https://github.com/huggingface/transformers/pull/10301)\n[2](https://github.com/pytorch/fairseq/issues/2358) [3](https://github.com/pytorch/fairseq/issues/2719))\nThus, evaluation was not attempted.\n\n`‡` These models have been trained with data which contains possible test set contamination. The OpenAI GPT-3 models\nfailed to deduplicate training data for certain test sets, while the GPT-Neo models as well as this one is\ntrained on The Pile, which has not been deduplicated against any test sets.\n\n# Architecture and Usage\n\nMost scripts in this repository are designed to be run on TPUs, which under the\n[TPU-VM architecture](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm) are virtual machines\nwhich can run arbitrary code. Most scripts are designed to spin up a TPU, SSH into it to set up the dependencies\nand copy code over from the local directory, and then start a [Ray](https://github.com/ray-project/ray.git) worker\nwhich can accept RPC calls.\n\nThe TPUVMs handles running model training steps and evaluation, checkpoint save and loading, while the driver python\nprogram handles data loading and general orchestration (such as when to save checkpoints etc).\n\nThis means that most scripts (`train.py`, `eval_harness.py` etc) expect to be running on a GCE virtual machine in the\nsame region as the TPUs, to minimize RPC latency and data transfer cost. Other scripts\n(usually ones which don't take a `--tpu` argument, such as `device_sample.py`, `device_serve.py` or `device_train.py`)\nexpect to be run directly on a TPUVM. The device_* scripts **only work on a v3-8** and not on larger pods.\n\nFurthermore, there is an example (`resharding_example.py`) of how to convert the provided checkpoints (which have 8\nshards in the case of GPT-J-6B) down to a smaller number, such as for when running on GPU(s).\n\n### Fine-tuning\n\nTo fine-tune the model, run `device_train.py` on a TPU VM.  Using a TPU v3-8, you can fine-tune at a rate of ~5000\ntokens/second, which should be sufficient for small-to-medium-size datasets.\n\nPlease read the [step by step guide](howto_finetune.md) for thorough fine-tuning instructions.\n\n### JAX Dependency\n\nNote this library has some specific requirements for JAX version. Specifically, to use the v1 models (including\n GPT-J 6B), `jax==0.2.12` is required. This in turn depends on `jaxlib==0.1.68`. **If this is not done, you will get\ncryptic xmap errors**\n\nHowever, to use the v2 model code (no publicly released weights), the newest JAX version can be used.\n# Citation\n\nTo cite this repository:\n```\n@misc{mesh-transformer-jax,\n  author = {Wang, Ben},\n  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},\n  howpublished = {\\url{https://github.com/kingoflolz/mesh-transformer-jax}},\n  year = 2021,\n  month = May\n}\n```\n\nTo cite the weights of GPT-J-6B:\n```\n@misc{gpt-j,\n  author = {Wang, Ben and Komatsuzaki, Aran},\n  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},\n  howpublished = {\\url{https://github.com/kingoflolz/mesh-transformer-jax}},\n  year = 2021,\n  month = May\n}\n```\n\nIf you use this repository or any of the pretrained weights to do something cool, we would love to hear about it.\nFeel free to open a github issue or reach out over email (in profile).\n"
        },
        {
          "name": "benchmarks.md",
          "type": "blob",
          "size": 2.0400390625,
          "content": "Note that everything on this page is quite outdated, and are only roughly accurate when considering new features\nsuch as RoPE\n\n# Benchmarks (v3-8)\n\n(see `tpuv38_example.py`):\n\n## ~2.7B model\n```\nInitialized in 121.842s\nTotal parameters: 2722382080\nCompiled in 49.0534s\nit: 0, loss: 20.311113357543945\n<snip>\nit: 90, loss: 3.987450361251831\n100 steps in 109.385s\neffective flops (not including attn): 2.4466e+14\n```\n\n## ~4.8B model\n```\nInitialized in 101.016s\nTotal parameters: 4836720896\nCompiled in 52.7404s\nit: 0, loss: 4.632925987243652\n<snip>\nit: 40, loss: 3.2406811714172363\n50 steps in 102.559s\neffective flops (not including attn): 2.31803e+14\n```\n\n## 10B model\n```\nInitialized in 152.762s\nTotal parameters: 10073579776\nCompiled in 92.6539s\nit: 0, loss: 5.3125\n<snip>\nit: 40, loss: 3.65625\n50 steps in 100.235s\neffective flops (not including attn): 2.46988e+14\n```\n\n# Benchmarks (v3-32)\n\n(see `eval.py`):\n\n## 6B model\n```\n\"layers\": 28,\n\"d_model\": 4096,\n\"n_heads\": 16,\n\"n_vocab\": 50400,\n\n\"seq\": 2048,\n\"cores_per_replica\": 8,\n\"per_replica_batch\": 1,\n\"gradient_accumulation_steps\": 8,\n\nparams: 6053381856\n32 iters done in 107.935s, avg 3.37298s, 0.296473/s\neffective flops (not including attn): 7.05692e+14\nMXU flops: 1.04523e+15\n```\n\n## Note that the below models do not currently work\nThey require a larger degree of model parallelism than is currently implemented, but benchmark numbers should be\nreasonably representative.\n\n## 13B model\n```\n\"layers\": 28,\n\"d_model\": 6144,\n\"n_heads\": 32,\n\"n_vocab\": 50400,\n\n\"seq\": 2048,\n\"cores_per_replica\": 16,\n\"per_replica_batch\": 1,\n\"gradient_accumulation_steps\": 16,\n\nparams: 13312183008\n32 iters done in 250.86s, avg 7.83937s, 0.127561/s\neffective flops (not including attn): 6.67727e+14\nMXU flops: 9.80066e+14\n```\n\n## 23B model\n```\n\"layers\": 28,\n\"d_model\": 8192,\n\"n_heads\": 32,\n\"n_vocab\": 50400,\n\n\"seq\": 2048,\n\"cores_per_replica\": 32,\n\"per_replica_batch\": 1,\n\"gradient_accumulation_steps\": 32,\n\nparams: 23398107360\n16 iters done in 221.33s, avg 13.8331s, 0.0722902/s\neffective flops (not including attn): 6.65107e+14\nMXU flops: 9.88548e+14\n```"
        },
        {
          "name": "colab_demo.ipynb",
          "type": "blob",
          "size": 95.0986328125,
          "content": "{\n \"nbformat\": 4,\n \"nbformat_minor\": 0,\n \"metadata\": {\n  \"colab\": {\n   \"name\": \"GPT-J-6B Inference Demo.ipynb\",\n   \"provenance\": [],\n   \"collapsed_sections\": [],\n   \"toc_visible\": true,\n   \"machine_shape\": \"hm\"\n  },\n  \"kernelspec\": {\n   \"name\": \"python3\",\n   \"display_name\": \"Python 3\"\n  },\n  \"language_info\": {\n   \"name\": \"python\"\n  },\n  \"accelerator\": \"TPU\",\n  \"widgets\": {\n   \"application/vnd.jupyter.widget-state+json\": {\n    \"5b8a31b3b4034116af0a81d897c3123b\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"HBoxModel\",\n     \"state\": {\n      \"_view_name\": \"HBoxView\",\n      \"_dom_classes\": [],\n      \"_model_name\": \"HBoxModel\",\n      \"_view_module\": \"@jupyter-widgets/controls\",\n      \"_model_module_version\": \"1.5.0\",\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.5.0\",\n      \"box_style\": \"\",\n      \"layout\": \"IPY_MODEL_835ee2070c05443c9fa3ae6fece3fb44\",\n      \"_model_module\": \"@jupyter-widgets/controls\",\n      \"children\": [\n       \"IPY_MODEL_9e6646f7066341ef9619f5f7e6f7c8b8\",\n       \"IPY_MODEL_4f803e3d56b74e18997b216bf9d65337\"\n      ]\n     }\n    },\n    \"835ee2070c05443c9fa3ae6fece3fb44\": {\n     \"model_module\": \"@jupyter-widgets/base\",\n     \"model_name\": \"LayoutModel\",\n     \"state\": {\n      \"_view_name\": \"LayoutView\",\n      \"grid_template_rows\": null,\n      \"right\": null,\n      \"justify_content\": null,\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"overflow\": null,\n      \"_model_module_version\": \"1.2.0\",\n      \"_view_count\": null,\n      \"flex_flow\": null,\n      \"width\": null,\n      \"min_width\": null,\n      \"border\": null,\n      \"align_items\": null,\n      \"bottom\": null,\n      \"_model_module\": \"@jupyter-widgets/base\",\n      \"top\": null,\n      \"grid_column\": null,\n      \"overflow_y\": null,\n      \"overflow_x\": null,\n      \"grid_auto_flow\": null,\n      \"grid_area\": null,\n      \"grid_template_columns\": null,\n      \"flex\": null,\n      \"_model_name\": \"LayoutModel\",\n      \"justify_items\": null,\n      \"grid_row\": null,\n      \"max_height\": null,\n      \"align_content\": null,\n      \"visibility\": null,\n      \"align_self\": null,\n      \"height\": null,\n      \"min_height\": null,\n      \"padding\": null,\n      \"grid_auto_rows\": null,\n      \"grid_gap\": null,\n      \"max_width\": null,\n      \"order\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"grid_template_areas\": null,\n      \"object_position\": null,\n      \"object_fit\": null,\n      \"grid_auto_columns\": null,\n      \"margin\": null,\n      \"display\": null,\n      \"left\": null\n     }\n    },\n    \"9e6646f7066341ef9619f5f7e6f7c8b8\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"FloatProgressModel\",\n     \"state\": {\n      \"_view_name\": \"ProgressView\",\n      \"style\": \"IPY_MODEL_f9fa7c1bc9f04df09e487b16761d6b8f\",\n      \"_dom_classes\": [],\n      \"description\": \"Downloading: 100%\",\n      \"_model_name\": \"FloatProgressModel\",\n      \"bar_style\": \"success\",\n      \"max\": 1042301,\n      \"_view_module\": \"@jupyter-widgets/controls\",\n      \"_model_module_version\": \"1.5.0\",\n      \"value\": 1042301,\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.5.0\",\n      \"orientation\": \"horizontal\",\n      \"min\": 0,\n      \"description_tooltip\": null,\n      \"_model_module\": \"@jupyter-widgets/controls\",\n      \"layout\": \"IPY_MODEL_39aed05f694a495abd03aa00ff71efb3\"\n     }\n    },\n    \"4f803e3d56b74e18997b216bf9d65337\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"HTMLModel\",\n     \"state\": {\n      \"_view_name\": \"HTMLView\",\n      \"style\": \"IPY_MODEL_61b0857802184b51abf77039fad681d7\",\n      \"_dom_classes\": [],\n      \"description\": \"\",\n      \"_model_name\": \"HTMLModel\",\n      \"placeholder\": \"​\",\n      \"_view_module\": \"@jupyter-widgets/controls\",\n      \"_model_module_version\": \"1.5.0\",\n      \"value\": \" 1.04M/1.04M [00:07&lt;00:00, 148kB/s]\",\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.5.0\",\n      \"description_tooltip\": null,\n      \"_model_module\": \"@jupyter-widgets/controls\",\n      \"layout\": \"IPY_MODEL_0fb689df0a8b42c2b87e7f11b818b88a\"\n     }\n    },\n    \"f9fa7c1bc9f04df09e487b16761d6b8f\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"ProgressStyleModel\",\n     \"state\": {\n      \"_view_name\": \"StyleView\",\n      \"_model_name\": \"ProgressStyleModel\",\n      \"description_width\": \"initial\",\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"_model_module_version\": \"1.5.0\",\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"bar_color\": null,\n      \"_model_module\": \"@jupyter-widgets/controls\"\n     }\n    },\n    \"39aed05f694a495abd03aa00ff71efb3\": {\n     \"model_module\": \"@jupyter-widgets/base\",\n     \"model_name\": \"LayoutModel\",\n     \"state\": {\n      \"_view_name\": \"LayoutView\",\n      \"grid_template_rows\": null,\n      \"right\": null,\n      \"justify_content\": null,\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"overflow\": null,\n      \"_model_module_version\": \"1.2.0\",\n      \"_view_count\": null,\n      \"flex_flow\": null,\n      \"width\": null,\n      \"min_width\": null,\n      \"border\": null,\n      \"align_items\": null,\n      \"bottom\": null,\n      \"_model_module\": \"@jupyter-widgets/base\",\n      \"top\": null,\n      \"grid_column\": null,\n      \"overflow_y\": null,\n      \"overflow_x\": null,\n      \"grid_auto_flow\": null,\n      \"grid_area\": null,\n      \"grid_template_columns\": null,\n      \"flex\": null,\n      \"_model_name\": \"LayoutModel\",\n      \"justify_items\": null,\n      \"grid_row\": null,\n      \"max_height\": null,\n      \"align_content\": null,\n      \"visibility\": null,\n      \"align_self\": null,\n      \"height\": null,\n      \"min_height\": null,\n      \"padding\": null,\n      \"grid_auto_rows\": null,\n      \"grid_gap\": null,\n      \"max_width\": null,\n      \"order\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"grid_template_areas\": null,\n      \"object_position\": null,\n      \"object_fit\": null,\n      \"grid_auto_columns\": null,\n      \"margin\": null,\n      \"display\": null,\n      \"left\": null\n     }\n    },\n    \"61b0857802184b51abf77039fad681d7\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"DescriptionStyleModel\",\n     \"state\": {\n      \"_view_name\": \"StyleView\",\n      \"_model_name\": \"DescriptionStyleModel\",\n      \"description_width\": \"\",\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"_model_module_version\": \"1.5.0\",\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"_model_module\": \"@jupyter-widgets/controls\"\n     }\n    },\n    \"0fb689df0a8b42c2b87e7f11b818b88a\": {\n     \"model_module\": \"@jupyter-widgets/base\",\n     \"model_name\": \"LayoutModel\",\n     \"state\": {\n      \"_view_name\": \"LayoutView\",\n      \"grid_template_rows\": null,\n      \"right\": null,\n      \"justify_content\": null,\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"overflow\": null,\n      \"_model_module_version\": \"1.2.0\",\n      \"_view_count\": null,\n      \"flex_flow\": null,\n      \"width\": null,\n      \"min_width\": null,\n      \"border\": null,\n      \"align_items\": null,\n      \"bottom\": null,\n      \"_model_module\": \"@jupyter-widgets/base\",\n      \"top\": null,\n      \"grid_column\": null,\n      \"overflow_y\": null,\n      \"overflow_x\": null,\n      \"grid_auto_flow\": null,\n      \"grid_area\": null,\n      \"grid_template_columns\": null,\n      \"flex\": null,\n      \"_model_name\": \"LayoutModel\",\n      \"justify_items\": null,\n      \"grid_row\": null,\n      \"max_height\": null,\n      \"align_content\": null,\n      \"visibility\": null,\n      \"align_self\": null,\n      \"height\": null,\n      \"min_height\": null,\n      \"padding\": null,\n      \"grid_auto_rows\": null,\n      \"grid_gap\": null,\n      \"max_width\": null,\n      \"order\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"grid_template_areas\": null,\n      \"object_position\": null,\n      \"object_fit\": null,\n      \"grid_auto_columns\": null,\n      \"margin\": null,\n      \"display\": null,\n      \"left\": null\n     }\n    },\n    \"8b76924bdac749d5b4908678fd0a7497\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"HBoxModel\",\n     \"state\": {\n      \"_view_name\": \"HBoxView\",\n      \"_dom_classes\": [],\n      \"_model_name\": \"HBoxModel\",\n      \"_view_module\": \"@jupyter-widgets/controls\",\n      \"_model_module_version\": \"1.5.0\",\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.5.0\",\n      \"box_style\": \"\",\n      \"layout\": \"IPY_MODEL_b03bd083afc84f52834d4a124e57c442\",\n      \"_model_module\": \"@jupyter-widgets/controls\",\n      \"children\": [\n       \"IPY_MODEL_3ba9fe91731f4ff380cda2183ad72c4d\",\n       \"IPY_MODEL_4e01b9cceffb4d178e97f1d1b36d62b0\"\n      ]\n     }\n    },\n    \"b03bd083afc84f52834d4a124e57c442\": {\n     \"model_module\": \"@jupyter-widgets/base\",\n     \"model_name\": \"LayoutModel\",\n     \"state\": {\n      \"_view_name\": \"LayoutView\",\n      \"grid_template_rows\": null,\n      \"right\": null,\n      \"justify_content\": null,\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"overflow\": null,\n      \"_model_module_version\": \"1.2.0\",\n      \"_view_count\": null,\n      \"flex_flow\": null,\n      \"width\": null,\n      \"min_width\": null,\n      \"border\": null,\n      \"align_items\": null,\n      \"bottom\": null,\n      \"_model_module\": \"@jupyter-widgets/base\",\n      \"top\": null,\n      \"grid_column\": null,\n      \"overflow_y\": null,\n      \"overflow_x\": null,\n      \"grid_auto_flow\": null,\n      \"grid_area\": null,\n      \"grid_template_columns\": null,\n      \"flex\": null,\n      \"_model_name\": \"LayoutModel\",\n      \"justify_items\": null,\n      \"grid_row\": null,\n      \"max_height\": null,\n      \"align_content\": null,\n      \"visibility\": null,\n      \"align_self\": null,\n      \"height\": null,\n      \"min_height\": null,\n      \"padding\": null,\n      \"grid_auto_rows\": null,\n      \"grid_gap\": null,\n      \"max_width\": null,\n      \"order\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"grid_template_areas\": null,\n      \"object_position\": null,\n      \"object_fit\": null,\n      \"grid_auto_columns\": null,\n      \"margin\": null,\n      \"display\": null,\n      \"left\": null\n     }\n    },\n    \"3ba9fe91731f4ff380cda2183ad72c4d\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"FloatProgressModel\",\n     \"state\": {\n      \"_view_name\": \"ProgressView\",\n      \"style\": \"IPY_MODEL_b3ac7912f51746afae20616269dc69f0\",\n      \"_dom_classes\": [],\n      \"description\": \"Downloading: 100%\",\n      \"_model_name\": \"FloatProgressModel\",\n      \"bar_style\": \"success\",\n      \"max\": 456318,\n      \"_view_module\": \"@jupyter-widgets/controls\",\n      \"_model_module_version\": \"1.5.0\",\n      \"value\": 456318,\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.5.0\",\n      \"orientation\": \"horizontal\",\n      \"min\": 0,\n      \"description_tooltip\": null,\n      \"_model_module\": \"@jupyter-widgets/controls\",\n      \"layout\": \"IPY_MODEL_fb1263f5a3da42e6acab0e9a66551b32\"\n     }\n    },\n    \"4e01b9cceffb4d178e97f1d1b36d62b0\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"HTMLModel\",\n     \"state\": {\n      \"_view_name\": \"HTMLView\",\n      \"style\": \"IPY_MODEL_3426927332884df4b1ec5544ed479c45\",\n      \"_dom_classes\": [],\n      \"description\": \"\",\n      \"_model_name\": \"HTMLModel\",\n      \"placeholder\": \"​\",\n      \"_view_module\": \"@jupyter-widgets/controls\",\n      \"_model_module_version\": \"1.5.0\",\n      \"value\": \" 456k/456k [00:00&lt;00:00, 928kB/s]\",\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.5.0\",\n      \"description_tooltip\": null,\n      \"_model_module\": \"@jupyter-widgets/controls\",\n      \"layout\": \"IPY_MODEL_debf4a0a609a4cffa5cd686a0a23f6e2\"\n     }\n    },\n    \"b3ac7912f51746afae20616269dc69f0\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"ProgressStyleModel\",\n     \"state\": {\n      \"_view_name\": \"StyleView\",\n      \"_model_name\": \"ProgressStyleModel\",\n      \"description_width\": \"initial\",\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"_model_module_version\": \"1.5.0\",\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"bar_color\": null,\n      \"_model_module\": \"@jupyter-widgets/controls\"\n     }\n    },\n    \"fb1263f5a3da42e6acab0e9a66551b32\": {\n     \"model_module\": \"@jupyter-widgets/base\",\n     \"model_name\": \"LayoutModel\",\n     \"state\": {\n      \"_view_name\": \"LayoutView\",\n      \"grid_template_rows\": null,\n      \"right\": null,\n      \"justify_content\": null,\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"overflow\": null,\n      \"_model_module_version\": \"1.2.0\",\n      \"_view_count\": null,\n      \"flex_flow\": null,\n      \"width\": null,\n      \"min_width\": null,\n      \"border\": null,\n      \"align_items\": null,\n      \"bottom\": null,\n      \"_model_module\": \"@jupyter-widgets/base\",\n      \"top\": null,\n      \"grid_column\": null,\n      \"overflow_y\": null,\n      \"overflow_x\": null,\n      \"grid_auto_flow\": null,\n      \"grid_area\": null,\n      \"grid_template_columns\": null,\n      \"flex\": null,\n      \"_model_name\": \"LayoutModel\",\n      \"justify_items\": null,\n      \"grid_row\": null,\n      \"max_height\": null,\n      \"align_content\": null,\n      \"visibility\": null,\n      \"align_self\": null,\n      \"height\": null,\n      \"min_height\": null,\n      \"padding\": null,\n      \"grid_auto_rows\": null,\n      \"grid_gap\": null,\n      \"max_width\": null,\n      \"order\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"grid_template_areas\": null,\n      \"object_position\": null,\n      \"object_fit\": null,\n      \"grid_auto_columns\": null,\n      \"margin\": null,\n      \"display\": null,\n      \"left\": null\n     }\n    },\n    \"3426927332884df4b1ec5544ed479c45\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"DescriptionStyleModel\",\n     \"state\": {\n      \"_view_name\": \"StyleView\",\n      \"_model_name\": \"DescriptionStyleModel\",\n      \"description_width\": \"\",\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"_model_module_version\": \"1.5.0\",\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"_model_module\": \"@jupyter-widgets/controls\"\n     }\n    },\n    \"debf4a0a609a4cffa5cd686a0a23f6e2\": {\n     \"model_module\": \"@jupyter-widgets/base\",\n     \"model_name\": \"LayoutModel\",\n     \"state\": {\n      \"_view_name\": \"LayoutView\",\n      \"grid_template_rows\": null,\n      \"right\": null,\n      \"justify_content\": null,\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"overflow\": null,\n      \"_model_module_version\": \"1.2.0\",\n      \"_view_count\": null,\n      \"flex_flow\": null,\n      \"width\": null,\n      \"min_width\": null,\n      \"border\": null,\n      \"align_items\": null,\n      \"bottom\": null,\n      \"_model_module\": \"@jupyter-widgets/base\",\n      \"top\": null,\n      \"grid_column\": null,\n      \"overflow_y\": null,\n      \"overflow_x\": null,\n      \"grid_auto_flow\": null,\n      \"grid_area\": null,\n      \"grid_template_columns\": null,\n      \"flex\": null,\n      \"_model_name\": \"LayoutModel\",\n      \"justify_items\": null,\n      \"grid_row\": null,\n      \"max_height\": null,\n      \"align_content\": null,\n      \"visibility\": null,\n      \"align_self\": null,\n      \"height\": null,\n      \"min_height\": null,\n      \"padding\": null,\n      \"grid_auto_rows\": null,\n      \"grid_gap\": null,\n      \"max_width\": null,\n      \"order\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"grid_template_areas\": null,\n      \"object_position\": null,\n      \"object_fit\": null,\n      \"grid_auto_columns\": null,\n      \"margin\": null,\n      \"display\": null,\n      \"left\": null\n     }\n    },\n    \"7197de8a39ab4e4186eabd061c730cf5\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"HBoxModel\",\n     \"state\": {\n      \"_view_name\": \"HBoxView\",\n      \"_dom_classes\": [],\n      \"_model_name\": \"HBoxModel\",\n      \"_view_module\": \"@jupyter-widgets/controls\",\n      \"_model_module_version\": \"1.5.0\",\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.5.0\",\n      \"box_style\": \"\",\n      \"layout\": \"IPY_MODEL_c31661c9a1ff4bd681cc10d70fd287c0\",\n      \"_model_module\": \"@jupyter-widgets/controls\",\n      \"children\": [\n       \"IPY_MODEL_bb11a4fbf16f488082aa2bac7443fd62\",\n       \"IPY_MODEL_5360630d6b0541dbaafa7ff91610bd84\"\n      ]\n     }\n    },\n    \"c31661c9a1ff4bd681cc10d70fd287c0\": {\n     \"model_module\": \"@jupyter-widgets/base\",\n     \"model_name\": \"LayoutModel\",\n     \"state\": {\n      \"_view_name\": \"LayoutView\",\n      \"grid_template_rows\": null,\n      \"right\": null,\n      \"justify_content\": null,\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"overflow\": null,\n      \"_model_module_version\": \"1.2.0\",\n      \"_view_count\": null,\n      \"flex_flow\": null,\n      \"width\": null,\n      \"min_width\": null,\n      \"border\": null,\n      \"align_items\": null,\n      \"bottom\": null,\n      \"_model_module\": \"@jupyter-widgets/base\",\n      \"top\": null,\n      \"grid_column\": null,\n      \"overflow_y\": null,\n      \"overflow_x\": null,\n      \"grid_auto_flow\": null,\n      \"grid_area\": null,\n      \"grid_template_columns\": null,\n      \"flex\": null,\n      \"_model_name\": \"LayoutModel\",\n      \"justify_items\": null,\n      \"grid_row\": null,\n      \"max_height\": null,\n      \"align_content\": null,\n      \"visibility\": null,\n      \"align_self\": null,\n      \"height\": null,\n      \"min_height\": null,\n      \"padding\": null,\n      \"grid_auto_rows\": null,\n      \"grid_gap\": null,\n      \"max_width\": null,\n      \"order\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"grid_template_areas\": null,\n      \"object_position\": null,\n      \"object_fit\": null,\n      \"grid_auto_columns\": null,\n      \"margin\": null,\n      \"display\": null,\n      \"left\": null\n     }\n    },\n    \"bb11a4fbf16f488082aa2bac7443fd62\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"FloatProgressModel\",\n     \"state\": {\n      \"_view_name\": \"ProgressView\",\n      \"style\": \"IPY_MODEL_f3883c133c62438db114c22cfc94e351\",\n      \"_dom_classes\": [],\n      \"description\": \"Downloading: 100%\",\n      \"_model_name\": \"FloatProgressModel\",\n      \"bar_style\": \"success\",\n      \"max\": 1355256,\n      \"_view_module\": \"@jupyter-widgets/controls\",\n      \"_model_module_version\": \"1.5.0\",\n      \"value\": 1355256,\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.5.0\",\n      \"orientation\": \"horizontal\",\n      \"min\": 0,\n      \"description_tooltip\": null,\n      \"_model_module\": \"@jupyter-widgets/controls\",\n      \"layout\": \"IPY_MODEL_74672c43909444c592955afd0d727a28\"\n     }\n    },\n    \"5360630d6b0541dbaafa7ff91610bd84\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"HTMLModel\",\n     \"state\": {\n      \"_view_name\": \"HTMLView\",\n      \"style\": \"IPY_MODEL_67f7c3bc59384dc7812d7130c7cde450\",\n      \"_dom_classes\": [],\n      \"description\": \"\",\n      \"_model_name\": \"HTMLModel\",\n      \"placeholder\": \"​\",\n      \"_view_module\": \"@jupyter-widgets/controls\",\n      \"_model_module_version\": \"1.5.0\",\n      \"value\": \" 1.36M/1.36M [00:03&lt;00:00, 404kB/s]\",\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.5.0\",\n      \"description_tooltip\": null,\n      \"_model_module\": \"@jupyter-widgets/controls\",\n      \"layout\": \"IPY_MODEL_bb7410ff9cb4484da73302993427b90a\"\n     }\n    },\n    \"f3883c133c62438db114c22cfc94e351\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"ProgressStyleModel\",\n     \"state\": {\n      \"_view_name\": \"StyleView\",\n      \"_model_name\": \"ProgressStyleModel\",\n      \"description_width\": \"initial\",\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"_model_module_version\": \"1.5.0\",\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"bar_color\": null,\n      \"_model_module\": \"@jupyter-widgets/controls\"\n     }\n    },\n    \"74672c43909444c592955afd0d727a28\": {\n     \"model_module\": \"@jupyter-widgets/base\",\n     \"model_name\": \"LayoutModel\",\n     \"state\": {\n      \"_view_name\": \"LayoutView\",\n      \"grid_template_rows\": null,\n      \"right\": null,\n      \"justify_content\": null,\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"overflow\": null,\n      \"_model_module_version\": \"1.2.0\",\n      \"_view_count\": null,\n      \"flex_flow\": null,\n      \"width\": null,\n      \"min_width\": null,\n      \"border\": null,\n      \"align_items\": null,\n      \"bottom\": null,\n      \"_model_module\": \"@jupyter-widgets/base\",\n      \"top\": null,\n      \"grid_column\": null,\n      \"overflow_y\": null,\n      \"overflow_x\": null,\n      \"grid_auto_flow\": null,\n      \"grid_area\": null,\n      \"grid_template_columns\": null,\n      \"flex\": null,\n      \"_model_name\": \"LayoutModel\",\n      \"justify_items\": null,\n      \"grid_row\": null,\n      \"max_height\": null,\n      \"align_content\": null,\n      \"visibility\": null,\n      \"align_self\": null,\n      \"height\": null,\n      \"min_height\": null,\n      \"padding\": null,\n      \"grid_auto_rows\": null,\n      \"grid_gap\": null,\n      \"max_width\": null,\n      \"order\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"grid_template_areas\": null,\n      \"object_position\": null,\n      \"object_fit\": null,\n      \"grid_auto_columns\": null,\n      \"margin\": null,\n      \"display\": null,\n      \"left\": null\n     }\n    },\n    \"67f7c3bc59384dc7812d7130c7cde450\": {\n     \"model_module\": \"@jupyter-widgets/controls\",\n     \"model_name\": \"DescriptionStyleModel\",\n     \"state\": {\n      \"_view_name\": \"StyleView\",\n      \"_model_name\": \"DescriptionStyleModel\",\n      \"description_width\": \"\",\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"_model_module_version\": \"1.5.0\",\n      \"_view_count\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"_model_module\": \"@jupyter-widgets/controls\"\n     }\n    },\n    \"bb7410ff9cb4484da73302993427b90a\": {\n     \"model_module\": \"@jupyter-widgets/base\",\n     \"model_name\": \"LayoutModel\",\n     \"state\": {\n      \"_view_name\": \"LayoutView\",\n      \"grid_template_rows\": null,\n      \"right\": null,\n      \"justify_content\": null,\n      \"_view_module\": \"@jupyter-widgets/base\",\n      \"overflow\": null,\n      \"_model_module_version\": \"1.2.0\",\n      \"_view_count\": null,\n      \"flex_flow\": null,\n      \"width\": null,\n      \"min_width\": null,\n      \"border\": null,\n      \"align_items\": null,\n      \"bottom\": null,\n      \"_model_module\": \"@jupyter-widgets/base\",\n      \"top\": null,\n      \"grid_column\": null,\n      \"overflow_y\": null,\n      \"overflow_x\": null,\n      \"grid_auto_flow\": null,\n      \"grid_area\": null,\n      \"grid_template_columns\": null,\n      \"flex\": null,\n      \"_model_name\": \"LayoutModel\",\n      \"justify_items\": null,\n      \"grid_row\": null,\n      \"max_height\": null,\n      \"align_content\": null,\n      \"visibility\": null,\n      \"align_self\": null,\n      \"height\": null,\n      \"min_height\": null,\n      \"padding\": null,\n      \"grid_auto_rows\": null,\n      \"grid_gap\": null,\n      \"max_width\": null,\n      \"order\": null,\n      \"_view_module_version\": \"1.2.0\",\n      \"grid_template_areas\": null,\n      \"object_position\": null,\n      \"object_fit\": null,\n      \"grid_auto_columns\": null,\n      \"margin\": null,\n      \"display\": null,\n      \"left\": null\n     }\n    }\n   }\n  }\n },\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"pHIJVqHsh4An\"\n   },\n   \"source\": [\n    \"# GPT-J-6B Inference Demo\\n\",\n    \"\\n\",\n    \"<a href=\\\"http://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\\n\",\n    \"\\n\",\n    \"This notebook demonstrates how to run the [GPT-J-6B model](https://github.com/kingoflolz/mesh-transformer-jax/#GPT-J-6B). See the link for more details about the model, including evaluation metrics and credits.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"8CMw_dSQKfhT\"\n   },\n   \"source\": [\n    \"## Install Dependencies\\n\",\n    \"\\n\",\n    \"First we download the model and install some dependencies. This step takes at least 5 minutes (possibly longer depending on server load).\\n\",\n    \"\\n\",\n    \"!!! **Make sure you are using a TPU runtime!** !!!\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"metadata\": {\n    \"colab\": {\n     \"base_uri\": \"https://localhost:8080/\",\n     \"height\": 1000\n    },\n    \"id\": \"n7xAFw-LOYfe\",\n    \"outputId\": \"7bd7fd83-a40c-41a2-8a14-9a11fc2f150c\"\n   },\n   \"source\": [\n    \"!apt install zstd\\n\",\n    \"\\n\",\n    \"# the \\\"slim\\\" version contain only bf16 weights and no optimizer parameters, which minimizes bandwidth and memory\\n\",\n    \"!time wget -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\\n\",\n    \"\\n\",\n    \"!time tar -I zstd -xf step_383500_slim.tar.zstd\\n\",\n    \"\\n\",\n    \"!git clone https://github.com/kingoflolz/mesh-transformer-jax.git\\n\",\n    \"!pip install -r mesh-transformer-jax/requirements.txt\\n\",\n    \"\\n\",\n    \"# jax 0.2.12 is required due to a regression with xmap in 0.2.13\\n\",\n    \"!pip install mesh-transformer-jax/ jax==0.2.12 tensorflow==2.5.0\"\n   ],\n   \"execution_count\": 1,\n   \"outputs\": [\n    {\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Reading package lists... Done\\n\",\n      \"Building dependency tree       \\n\",\n      \"Reading state information... Done\\n\",\n      \"The following NEW packages will be installed:\\n\",\n      \"  zstd\\n\",\n      \"0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\\n\",\n      \"Need to get 278 kB of archives.\\n\",\n      \"After this operation, 1,141 kB of additional disk space will be used.\\n\",\n      \"Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 zstd amd64 1.3.3+dfsg-2ubuntu1.2 [278 kB]\\n\",\n      \"Fetched 278 kB in 1s (369 kB/s)\\n\",\n      \"Selecting previously unselected package zstd.\\n\",\n      \"(Reading database ... 160772 files and directories currently installed.)\\n\",\n      \"Preparing to unpack .../zstd_1.3.3+dfsg-2ubuntu1.2_amd64.deb ...\\n\",\n      \"Unpacking zstd (1.3.3+dfsg-2ubuntu1.2) ...\\n\",\n      \"Setting up zstd (1.3.3+dfsg-2ubuntu1.2) ...\\n\",\n      \"Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\\n\",\n      \"--2021-06-09 01:49:35--  https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\\n\",\n      \"Resolving the-eye.eu (the-eye.eu)... 162.213.130.242\\n\",\n      \"Connecting to the-eye.eu (the-eye.eu)|162.213.130.242|:443... connected.\\n\",\n      \"HTTP request sent, awaiting response... 200 OK\\n\",\n      \"Length: 9414712325 (8.8G) [application/octet-stream]\\n\",\n      \"Saving to: ‘step_383500_slim.tar.zstd’\\n\",\n      \"\\n\",\n      \"step_383500_slim.ta 100%[===================>]   8.77G  85.8MB/s    in 1m 44s  \\n\",\n      \"\\n\",\n      \"2021-06-09 01:51:19 (86.4 MB/s) - ‘step_383500_slim.tar.zstd’ saved [9414712325/9414712325]\\n\",\n      \"\\n\",\n      \"\\n\",\n      \"real\\t1m44.272s\\n\",\n      \"user\\t0m5.735s\\n\",\n      \"sys\\t0m26.674s\\n\",\n      \"\\n\",\n      \"real\\t1m28.680s\\n\",\n      \"user\\t0m31.887s\\n\",\n      \"sys\\t0m30.531s\\n\",\n      \"Cloning into 'mesh-transformer-jax'...\\n\",\n      \"remote: Enumerating objects: 444, done.\\u001B[K\\n\",\n      \"remote: Counting objects: 100% (444/444), done.\\u001B[K\\n\",\n      \"remote: Compressing objects: 100% (299/299), done.\\u001B[K\\n\",\n      \"remote: Total 444 (delta 294), reused 280 (delta 130), pack-reused 0\\u001B[K\\n\",\n      \"Receiving objects: 100% (444/444), 97.43 KiB | 1.37 MiB/s, done.\\n\",\n      \"Resolving deltas: 100% (294/294), done.\\n\",\n      \"Collecting git+https://github.com/deepmind/dm-haiku (from -r mesh-transformer-jax/requirements.txt (line 10))\\n\",\n      \"  Cloning https://github.com/deepmind/dm-haiku to /tmp/pip-req-build-rzypnds1\\n\",\n      \"  Running command git clone -q https://github.com/deepmind/dm-haiku /tmp/pip-req-build-rzypnds1\\n\",\n      \"Collecting git+https://github.com/EleutherAI/lm-evaluation-harness/ (from -r mesh-transformer-jax/requirements.txt (line 11))\\n\",\n      \"  Cloning https://github.com/EleutherAI/lm-evaluation-harness/ to /tmp/pip-req-build-60v7ec7y\\n\",\n      \"  Running command git clone -q https://github.com/EleutherAI/lm-evaluation-harness/ /tmp/pip-req-build-60v7ec7y\\n\",\n      \"Requirement already satisfied: numpy~=1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 1)) (1.19.5)\\n\",\n      \"Collecting transformers~=4.4.2\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 2.0MB 3.1MB/s \\n\",\n      \"\\u001B[?25hCollecting tqdm~=4.45.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl (60kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 61kB 6.9MB/s \\n\",\n      \"\\u001B[?25hCollecting setuptools~=51.3.3\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/b2/81/509db0082c0d2ca2af307c6652ea422865de1f83c14b1e1f3549e415cfac/setuptools-51.3.3-py3-none-any.whl (786kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 788kB 19.5MB/s \\n\",\n      \"\\u001B[?25hCollecting wandb~=0.10.22\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/6c/48/b199e2b3b341ac842108c5db4956091dd75d961cfa77aceb033e99cac20f/wandb-0.10.31-py2.py3-none-any.whl (1.8MB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 1.8MB 25.5MB/s \\n\",\n      \"\\u001B[?25hCollecting einops~=0.3.0\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\\n\",\n      \"Collecting requests~=2.25.1\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 61kB 7.1MB/s \\n\",\n      \"\\u001B[?25hCollecting fabric~=2.6.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/c1/9d/59df62b620985871a4ba7d8b509b84340bbd1573257e55a427ae2df2d56e/fabric-2.6.0-py2.py3-none-any.whl (53kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 61kB 7.1MB/s \\n\",\n      \"\\u001B[?25hCollecting optax~=0.0.2\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/ec/7a/6259edd319ee7fa94dd23c54f15eff667f599d179e889af90fe0c204612c/optax-0.0.6-py3-none-any.whl (96kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 102kB 10.2MB/s \\n\",\n      \"\\u001B[?25hCollecting ray~=1.2.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/11/14/15d0f0aec20a4674a996429160565a071688f27f49f789327ebed8188ffb/ray-1.2.0-cp37-cp37m-manylinux2014_x86_64.whl (47.5MB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 47.5MB 101kB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: jax~=0.2.12 in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 13)) (0.2.13)\\n\",\n      \"Requirement already satisfied: Flask~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 14)) (1.1.4)\\n\",\n      \"Requirement already satisfied: cloudpickle~=1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 15)) (1.3.0)\\n\",\n      \"Collecting tensorflow-cpu~=2.4.1\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/f0/7b/b4d3fbc3ad4d7b5255dff433c06c0105f7815ed584e41f47cb2a00b0c079/tensorflow_cpu-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (144.1MB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 144.2MB 47kB/s \\n\",\n      \"\\u001B[?25hCollecting google-cloud-storage~=1.36.2\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/f2/0e/da07ffa511daa559bcc209f9344d71a90ba4d7b391fb795e6282f86d2935/google_cloud_storage-1.36.2-py2.py3-none-any.whl (97kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 102kB 10.8MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: smart_open[gcs] in /usr/local/lib/python3.7/dist-packages (from -r mesh-transformer-jax/requirements.txt (line 18)) (5.0.0)\\n\",\n      \"Collecting func_timeout\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/b3/0d/bf0567477f7281d9a3926c582bfef21bff7498fc0ffd3e9de21811896a0b/func_timeout-4.3.5.tar.gz (44kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 51kB 6.1MB/s \\n\",\n      \"\\u001B[?25hCollecting ftfy\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 71kB 8.1MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.5.dev0->-r mesh-transformer-jax/requirements.txt (line 10)) (0.12.0)\\n\",\n      \"Collecting jmp>=0.0.2\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/ff/5c/1482f4a4a502e080af2ca54d7f80a60b5d4735f464c151666d583b78c226/jmp-0.0.2-py3-none-any.whl\\n\",\n      \"Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.5.dev0->-r mesh-transformer-jax/requirements.txt (line 10)) (0.8.9)\\n\",\n      \"Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.5.dev0->-r mesh-transformer-jax/requirements.txt (line 10)) (3.7.4.3)\\n\",\n      \"Collecting black==20.8b1\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/dc/7b/5a6bbe89de849f28d7c109f5ea87b65afa5124ad615f3419e71beb29dc96/black-20.8b1.tar.gz (1.1MB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 1.1MB 48.5MB/s \\n\",\n      \"\\u001B[?25h  Installing build dependencies ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"  Getting requirements to build wheel ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"    Preparing wheel metadata ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"Collecting best_download>=0.0.6\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/5f/03/155d24ac8b3e1d6c21daddc357001045486c3a31beab505e6cfc77b4ee7e/best_download-0.0.6-py3-none-any.whl\\n\",\n      \"Collecting datasets>=1.2.1\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/08/a2/d4e1024c891506e1cee8f9d719d20831bac31cb5b7416983c4d2f65a6287/datasets-1.8.0-py3-none-any.whl (237kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 245kB 50.0MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: click>=7.1 in /usr/local/lib/python3.7/dist-packages (from lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (7.1.2)\\n\",\n      \"Collecting scikit-learn>=0.24.1\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 22.3MB 1.2MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (1.8.1+cu101)\\n\",\n      \"Collecting sqlitedict==1.6.0\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\\n\",\n      \"Collecting pytablewriter==0.58.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/fd/e2/62b208cdb8771dee1849bd2b4ed129284e1efff7669985697e4c124c1000/pytablewriter-0.58.0-py3-none-any.whl (96kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 102kB 11.0MB/s \\n\",\n      \"\\u001B[?25hCollecting sacrebleu==1.5.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 71kB 7.8MB/s \\n\",\n      \"\\u001B[?25hCollecting pycountry==20.7.3\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/76/73/6f1a412f14f68c273feea29a6ea9b9f1e268177d32e0e69ad6790d306312/pycountry-20.7.3.tar.gz (10.1MB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 10.1MB 53.9MB/s \\n\",\n      \"\\u001B[?25hCollecting numexpr==2.7.2\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/9c/f4/fa8755c1aa44b431267aa019922f6cc9ec099cef0c6fc0ead0f9a2aa59e5/numexpr-2.7.2-cp37-cp37m-manylinux2010_x86_64.whl (471kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 481kB 45.7MB/s \\n\",\n      \"\\u001B[?25hCollecting lm_dataformat==0.0.19\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/83/b5/8d10bf5a8082921792bb09c9d591dfd622cf4a16fbb7e283cc921c5ffc50/lm_dataformat-0.0.19-py3-none-any.whl\\n\",\n      \"Collecting pytest==6.2.3\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/76/4d/9c00146923da9f1cabd1878209d71b1380d537ec331a1a613e8f4b9d7985/pytest-6.2.3-py3-none-any.whl (280kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 286kB 49.2MB/s \\n\",\n      \"\\u001B[?25hCollecting pybind11==2.6.2\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/8d/43/7339dbabbc2793718d59703aace4166f53c29ee1c202f6ff5bf8a26c4d91/pybind11-2.6.2-py2.py3-none-any.whl (191kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 194kB 55.4MB/s \\n\",\n      \"\\u001B[?25hCollecting tqdm-multiprocess==0.0.11\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/25/7e/0d889fc6c84e3df6b69aaafe893fc77f69b3d968ac9ce574d1c62c688050/tqdm_multiprocess-0.0.11-py3-none-any.whl\\n\",\n      \"Collecting zstandard==0.15.2\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/5b/56/dc2a85d06e973f2ad96584b0e5b876d063135d449cb040aeaadd22a910f9/zstandard-0.15.2-cp37-cp37m-manylinux2014_x86_64.whl (2.2MB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 2.2MB 52.7MB/s \\n\",\n      \"\\u001B[?25hCollecting jsonlines==2.0.0\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/d4/58/06f430ff7607a2929f80f07bfd820acbc508a4e977542fefcc522cde9dff/jsonlines-2.0.0-py3-none-any.whl\\n\",\n      \"Requirement already satisfied: importlib-metadata; python_version < \\\"3.8\\\" in /usr/local/lib/python3.7/dist-packages (from transformers~=4.4.2->-r mesh-transformer-jax/requirements.txt (line 2)) (4.0.1)\\n\",\n      \"Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers~=4.4.2->-r mesh-transformer-jax/requirements.txt (line 2)) (20.9)\\n\",\n      \"Collecting tokenizers<0.11,>=0.10.1\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 3.3MB 50.0MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers~=4.4.2->-r mesh-transformer-jax/requirements.txt (line 2)) (2019.12.20)\\n\",\n      \"Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers~=4.4.2->-r mesh-transformer-jax/requirements.txt (line 2)) (3.0.12)\\n\",\n      \"Collecting sacremoses\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 901kB 53.3MB/s \\n\",\n      \"\\u001B[?25hCollecting configparser>=3.8.1\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\\n\",\n      \"Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb~=0.10.22->-r mesh-transformer-jax/requirements.txt (line 5)) (1.15.0)\\n\",\n      \"Collecting docker-pycreds>=0.4.0\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\\n\",\n      \"Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb~=0.10.22->-r mesh-transformer-jax/requirements.txt (line 5)) (3.12.4)\\n\",\n      \"Collecting sentry-sdk>=0.4.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 133kB 53.9MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb~=0.10.22->-r mesh-transformer-jax/requirements.txt (line 5)) (2.3)\\n\",\n      \"Collecting GitPython>=1.0.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 174kB 52.4MB/s \\n\",\n      \"\\u001B[?25hCollecting shortuuid>=0.5.0\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\\n\",\n      \"Collecting pathtools\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\\n\",\n      \"Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb~=0.10.22->-r mesh-transformer-jax/requirements.txt (line 5)) (3.13)\\n\",\n      \"Collecting subprocess32>=3.5.3\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 102kB 10.1MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb~=0.10.22->-r mesh-transformer-jax/requirements.txt (line 5)) (5.4.8)\\n\",\n      \"Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb~=0.10.22->-r mesh-transformer-jax/requirements.txt (line 5)) (2.8.1)\\n\",\n      \"Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests~=2.25.1->-r mesh-transformer-jax/requirements.txt (line 7)) (1.24.3)\\n\",\n      \"Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests~=2.25.1->-r mesh-transformer-jax/requirements.txt (line 7)) (3.0.4)\\n\",\n      \"Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests~=2.25.1->-r mesh-transformer-jax/requirements.txt (line 7)) (2020.12.5)\\n\",\n      \"Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests~=2.25.1->-r mesh-transformer-jax/requirements.txt (line 7)) (2.10)\\n\",\n      \"Collecting invoke<2.0,>=1.3\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/87/8f/c153d7db091f342da6bc97f7bedd1b2ce2867c4a8b0aab40fbba85a05e33/invoke-1.5.0-py3-none-any.whl (211kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 215kB 52.0MB/s \\n\",\n      \"\\u001B[?25hCollecting paramiko>=2.4\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/95/19/124e9287b43e6ff3ebb9cdea3e5e8e88475a873c05ccdf8b7e20d2c4201e/paramiko-2.7.2-py2.py3-none-any.whl (206kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 215kB 51.7MB/s \\n\",\n      \"\\u001B[?25hCollecting pathlib2\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/e9/45/9c82d3666af4ef9f221cbb954e1d77ddbb513faf552aea6df5f37f1a4859/pathlib2-2.3.5-py2.py3-none-any.whl\\n\",\n      \"Collecting chex>=0.0.4\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/f5/b9/445eb59ec23249acffc5322c79b07e20b12dbff45b9c1da6cdae9e947685/chex-0.0.7-py3-none-any.whl (52kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 61kB 7.2MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax~=0.0.2->-r mesh-transformer-jax/requirements.txt (line 9)) (0.1.66+cuda110)\\n\",\n      \"Collecting aiohttp\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 1.3MB 47.2MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray~=1.2.0->-r mesh-transformer-jax/requirements.txt (line 12)) (0.10.1)\\n\",\n      \"Collecting colorful\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/b0/8e/e386e248266952d24d73ed734c2f5513f34d9557032618c8910e605dfaf6/colorful-0.5.4-py2.py3-none-any.whl (201kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 204kB 54.3MB/s \\n\",\n      \"\\u001B[?25hCollecting gpustat\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 81kB 9.8MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray~=1.2.0->-r mesh-transformer-jax/requirements.txt (line 12)) (1.0.2)\\n\",\n      \"Collecting aiohttp-cors\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/13/e7/e436a0c0eb5127d8b491a9b83ecd2391c6ff7dcd5548dfaec2080a2340fd/aiohttp_cors-0.7.0-py3-none-any.whl\\n\",\n      \"Collecting redis>=3.5.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 81kB 9.5MB/s \\n\",\n      \"\\u001B[?25hCollecting colorama\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\\n\",\n      \"Collecting aioredis\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/b0/64/1b1612d0a104f21f80eb4c6e1b6075f2e6aba8e228f46f229cfd3fdac859/aioredis-1.3.1-py3-none-any.whl (65kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 71kB 8.6MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray~=1.2.0->-r mesh-transformer-jax/requirements.txt (line 12)) (2.6.0)\\n\",\n      \"Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray~=1.2.0->-r mesh-transformer-jax/requirements.txt (line 12)) (1.34.1)\\n\",\n      \"Collecting py-spy>=0.2.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/9d/4d/1a9cbe9a0b543e6733cb38afe26451522a9ef8e4897b59e74cc76838f245/py_spy-0.3.7-py2.py3-none-manylinux1_x86_64.whl (3.1MB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 3.1MB 50.0MB/s \\n\",\n      \"\\u001B[?25hCollecting opencensus\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/18/59/12044123133d000f705383ad98579aeb0dd82d66b33a254a21b54bf0d6bb/opencensus-0.7.13-py2.py3-none-any.whl (127kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 133kB 50.4MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax~=0.2.12->-r mesh-transformer-jax/requirements.txt (line 13)) (3.3.0)\\n\",\n      \"Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.2->-r mesh-transformer-jax/requirements.txt (line 14)) (1.0.1)\\n\",\n      \"Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.2->-r mesh-transformer-jax/requirements.txt (line 14)) (1.1.0)\\n\",\n      \"Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask~=1.1.2->-r mesh-transformer-jax/requirements.txt (line 14)) (2.11.3)\\n\",\n      \"Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (0.36.2)\\n\",\n      \"Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (1.12)\\n\",\n      \"Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (1.12.1)\\n\",\n      \"Collecting gast==0.3.3\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\\n\",\n      \"Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (1.1.0)\\n\",\n      \"Collecting h5py~=2.10.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 2.9MB 51.5MB/s \\n\",\n      \"\\u001B[?25hCollecting tensorflow-estimator<2.5.0,>=2.4.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 471kB 47.3MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (0.2.0)\\n\",\n      \"Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (1.1.2)\\n\",\n      \"Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (2.5.0)\\n\",\n      \"Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (1.6.3)\\n\",\n      \"Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage~=1.36.2->-r mesh-transformer-jax/requirements.txt (line 17)) (1.30.0)\\n\",\n      \"Collecting google-resumable-media<2.0dev,>=1.2.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/f9/ad/bc80b0b33ccb5e21375ca1440da9dab99596948d5035e2f597fdcffb31f1/google_resumable_media-1.3.0-py2.py3-none-any.whl (75kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 81kB 9.7MB/s \\n\",\n      \"\\u001B[?25hCollecting google-cloud-core<2.0dev,>=1.4.1\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/ad/fc/6e8c449185cb8862af353c1164100ff75e32d55ba1de3baf9eaa01b7d2a9/google_cloud_core-1.6.0-py2.py3-none-any.whl\\n\",\n      \"Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->-r mesh-transformer-jax/requirements.txt (line 20)) (0.2.5)\\n\",\n      \"Collecting typed-ast>=1.4.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/65/b3/573d2f1fecbbe8f82a8d08172e938c247f99abe1be3bef3da2efaa3810bf/typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 747kB 52.5MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from black==20.8b1->lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (1.4.4)\\n\",\n      \"Collecting mypy-extensions>=0.4.3\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\\n\",\n      \"Collecting pathspec<1,>=0.6\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/29/29/a465741a3d97ea3c17d21eaad4c64205428bde56742360876c4391f930d4/pathspec-0.8.1-py2.py3-none-any.whl\\n\",\n      \"Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from black==20.8b1->lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (0.10.2)\\n\",\n      \"Collecting rehash\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/c9/e4/30db193232d9e9c8e123764d84f0807535677548833ca251556ad6134c24/rehash-1.0.0-py2.py3-none-any.whl\\n\",\n      \"Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (1.1.5)\\n\",\n      \"Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (3.0.0)\\n\",\n      \"Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (0.3.3)\\n\",\n      \"Collecting xxhash\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 245kB 51.5MB/s \\n\",\n      \"\\u001B[?25hCollecting fsspec\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/8e/d2/d05466997f7751a2c06a7a416b7d1f131d765f7916698d3fdcb3a4d037e5/fsspec-2021.6.0-py3-none-any.whl (114kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 122kB 50.9MB/s \\n\",\n      \"\\u001B[?25hCollecting huggingface-hub<0.1.0\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/3c/e3/fb7b6aefaf0fc7b792cebbbd590b1895c022ab0ff27f389e1019c6f2e68a/huggingface_hub-0.0.10-py3-none-any.whl\\n\",\n      \"Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.2.1->lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (0.70.11.1)\\n\",\n      \"Collecting threadpoolctl>=2.0.0\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\\n\",\n      \"Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (1.0.1)\\n\",\n      \"Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (1.4.1)\\n\",\n      \"Collecting tcolorpy<1,>=0.0.5\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/8e/d5/2b53921011bef9a5a4d64f3265a40b5bdfef6e0eda937d57a2dd7a66f89c/tcolorpy-0.0.9-py3-none-any.whl\\n\",\n      \"Collecting msgfy<1,>=0.1.0\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/48/52/c4441871514276e7c4cb51c122e663b5ef19dc20030f6ab7723071118464/msgfy-0.1.0-py3-none-any.whl\\n\",\n      \"Collecting DataProperty<2,>=0.50.0\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/b9/5f/c773c362fcba227d6a4021225cb0213b51849c7dc9c93004d34d9004078b/DataProperty-0.50.1-py3-none-any.whl\\n\",\n      \"Collecting typepy[datetime]<2,>=1.1.1\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/60/3a/1239e59924250d9c2dd1d5b84748da82d15aaa241b3ceeffa08aa5eba589/typepy-1.1.5-py3-none-any.whl\\n\",\n      \"Collecting mbstrdecoder<2,>=1.0.0\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/e8/f6/0e6bb50c3c6380a4982c87d80e70b2f6e366523a57a0c58594aea472206d/mbstrdecoder-1.0.1-py3-none-any.whl\\n\",\n      \"Collecting pathvalidate<3,>=2.3.0\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/87/55/7d63b78986f1f8764180b84ee3e8a47c583ec059d32c98d8fba7fc0dc1ae/pathvalidate-2.4.1-py3-none-any.whl\\n\",\n      \"Collecting tabledata<2,>=1.1.3\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/85/93/4c695da7e6589e1e4b513c02d5b562dcc5afacb8a2f6cac8eb2ac2e88833/tabledata-1.1.4-py3-none-any.whl\\n\",\n      \"Collecting portalocker\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl\\n\",\n      \"Collecting ujson\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/17/4e/50e8e4cf5f00b537095711c2c86ac4d7191aed2b4fffd5a19f06898f6929/ujson-4.0.2-cp37-cp37m-manylinux1_x86_64.whl (179kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 184kB 55.9MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest==6.2.3->lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (1.1.1)\\n\",\n      \"Collecting pluggy<1.0.0a1,>=0.12\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\\n\",\n      \"Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest==6.2.3->lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (1.10.0)\\n\",\n      \"Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest==6.2.3->lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (21.2.0)\\n\",\n      \"Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \\\"3.8\\\"->transformers~=4.4.2->-r mesh-transformer-jax/requirements.txt (line 2)) (3.4.1)\\n\",\n      \"Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers~=4.4.2->-r mesh-transformer-jax/requirements.txt (line 2)) (2.4.7)\\n\",\n      \"Collecting gitdb<5,>=4.0.1\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 71kB 8.1MB/s \\n\",\n      \"\\u001B[?25hCollecting cryptography>=2.5\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 3.2MB 49.6MB/s \\n\",\n      \"\\u001B[?25hCollecting pynacl>=1.0.1\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/9d/57/2f5e6226a674b2bcb6db531e8b383079b678df5b10cdaa610d6cf20d77ba/PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 962kB 47.6MB/s \\n\",\n      \"\\u001B[?25hCollecting bcrypt>=3.1.3\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/26/70/6d218afbe4c73538053c1016dd631e8f25fffc10cd01f5c272d7acf3c03d/bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 71kB 8.2MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax~=0.0.2->-r mesh-transformer-jax/requirements.txt (line 9)) (0.1.6)\\n\",\n      \"Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax~=0.0.2->-r mesh-transformer-jax/requirements.txt (line 9)) (0.11.1)\\n\",\n      \"Collecting yarl<2.0,>=1.0\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 296kB 42.9MB/s \\n\",\n      \"\\u001B[?25hCollecting async-timeout<4.0,>=3.0\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\\n\",\n      \"Collecting multidict<7.0,>=4.5\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 143kB 49.7MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray~=1.2.0->-r mesh-transformer-jax/requirements.txt (line 12)) (7.352.0)\\n\",\n      \"Collecting blessings>=1.6\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\\n\",\n      \"Collecting hiredis\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/ed/33/290cea35b09c80b4634773ad5572a8030a87b5d39736719f698f521d2a13/hiredis-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (85kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 92kB 9.7MB/s \\n\",\n      \"\\u001B[?25hCollecting opencensus-context==0.1.2\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/f1/33/990f1bd9e7ee770fc8d3c154fc24743a96f16a0e49e14e1b7540cc2fdd93/opencensus_context-0.1.2-py2.py3-none-any.whl\\n\",\n      \"Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray~=1.2.0->-r mesh-transformer-jax/requirements.txt (line 12)) (1.26.3)\\n\",\n      \"Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask~=1.1.2->-r mesh-transformer-jax/requirements.txt (line 14)) (2.0.1)\\n\",\n      \"Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (1.8.0)\\n\",\n      \"Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (3.3.4)\\n\",\n      \"Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (0.6.1)\\n\",\n      \"Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (0.4.4)\\n\",\n      \"Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage~=1.36.2->-r mesh-transformer-jax/requirements.txt (line 17)) (4.2.2)\\n\",\n      \"Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage~=1.36.2->-r mesh-transformer-jax/requirements.txt (line 17)) (0.2.8)\\n\",\n      \"Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \\\"3.6\\\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage~=1.36.2->-r mesh-transformer-jax/requirements.txt (line 17)) (4.7.2)\\n\",\n      \"Collecting google-crc32c<2.0dev,>=1.0; python_version >= \\\"3.5\\\"\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/fc/ae/b6efa1019e18c6c791f0f5cd93b2ff40f8f06696dbf04db39ec0f5591b1e/google_crc32c-1.1.2-cp37-cp37m-manylinux2014_x86_64.whl\\n\",\n      \"Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.2.1->lm-eval-harness==0.0.1->-r mesh-transformer-jax/requirements.txt (line 11)) (2018.9)\\n\",\n      \"Collecting smmap<5,>=3.0.1\\n\",\n      \"  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\\n\",\n      \"Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.5->paramiko>=2.4->fabric~=2.6.0->-r mesh-transformer-jax/requirements.txt (line 8)) (1.14.5)\\n\",\n      \"Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray~=1.2.0->-r mesh-transformer-jax/requirements.txt (line 12)) (1.53.0)\\n\",\n      \"Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (1.3.0)\\n\",\n      \"Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.11.0->google-cloud-storage~=1.36.2->-r mesh-transformer-jax/requirements.txt (line 17)) (0.4.8)\\n\",\n      \"Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko>=2.4->fabric~=2.6.0->-r mesh-transformer-jax/requirements.txt (line 8)) (2.20)\\n\",\n      \"Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-cpu~=2.4.1->-r mesh-transformer-jax/requirements.txt (line 16)) (3.1.0)\\n\",\n      \"Building wheels for collected packages: black\\n\",\n      \"  Building wheel for black (PEP 517) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"  Created wheel for black: filename=black-20.8b1-cp37-none-any.whl size=124195 sha256=820b5c710b946b935fabc8b39f608ff2fab3519bd888db1b69710011b7aed5e7\\n\",\n      \"  Stored in directory: /root/.cache/pip/wheels/6e/10/b5/edf7359c2edd0305cce7e3f96e07daf7ce55dceac9d3ce3373\\n\",\n      \"Successfully built black\\n\",\n      \"Building wheels for collected packages: func-timeout, ftfy, dm-haiku, lm-eval-harness, sqlitedict, pycountry, pathtools, subprocess32, gpustat\\n\",\n      \"  Building wheel for func-timeout (setup.py) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"  Created wheel for func-timeout: filename=func_timeout-4.3.5-cp37-none-any.whl size=15097 sha256=4cac14d160ec61e4a096e609113e6396cc5864a3342fc4d8c2c90e6b5ca9e48d\\n\",\n      \"  Stored in directory: /root/.cache/pip/wheels/46/7c/4f/24f1d2d5bbff92219debe7ea19af84f76ddeb90dd4ec544f26\\n\",\n      \"  Building wheel for ftfy (setup.py) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=16a45289c44008e2f575fa886e24361690e3238aae812840d9027fde52b19e36\\n\",\n      \"  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\\n\",\n      \"  Building wheel for dm-haiku (setup.py) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"  Created wheel for dm-haiku: filename=dm_haiku-0.0.5.dev0-cp37-none-any.whl size=558211 sha256=55ddf8b30b5d51cc31684835a0f8283bd545c3cd90310df9d0bc03b657ca487d\\n\",\n      \"  Stored in directory: /tmp/pip-ephem-wheel-cache-mkgd1im2/wheels/97/0f/e9/17f34e377f8d4060fa88a7e82bee5d8afbf7972384768a5499\\n\",\n      \"  Building wheel for lm-eval-harness (setup.py) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"  Created wheel for lm-eval-harness: filename=lm_eval_harness-0.0.1-cp37-none-any.whl size=96331 sha256=e2e144196bad68df5d022a1e16676fd2f0cc18e865973ec09b90f4469880cbfa\\n\",\n      \"  Stored in directory: /tmp/pip-ephem-wheel-cache-mkgd1im2/wheels/a8/db/b4/32ca8efd6b64f9187fefbabd636d7c94cd2150a657262ee22a\\n\",\n      \"  Building wheel for sqlitedict (setup.py) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp37-none-any.whl size=14714 sha256=af52aa57cead6fde363f9f52a4ca12b4014438bada7e37f299f49f97445e853b\\n\",\n      \"  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\\n\",\n      \"  Building wheel for pycountry (setup.py) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"  Created wheel for pycountry: filename=pycountry-20.7.3-py2.py3-none-any.whl size=10746883 sha256=fff18e590a27afa318d4c31ee7098a15916a59b00bb5ca7040e5c500b1336d7c\\n\",\n      \"  Stored in directory: /root/.cache/pip/wheels/33/4e/a6/be297e6b83567e537bed9df4a93f8590ec01c1acfbcd405348\\n\",\n      \"  Building wheel for pathtools (setup.py) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8807 sha256=45a2c0fca698059654a7c1feb9e1f9fd74247cbd15ea3a00fae83f1b22175fc7\\n\",\n      \"  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\\n\",\n      \"  Building wheel for subprocess32 (setup.py) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6502 sha256=d3f4fc7a51cca5a8b462fc6826afef60c617fcf0e015b2ed027d320dea68741c\\n\",\n      \"  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\\n\",\n      \"  Building wheel for gpustat (setup.py) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"  Created wheel for gpustat: filename=gpustat-0.6.0-cp37-none-any.whl size=12621 sha256=3f9b348ee23b2452005787b00eb878cfd1be319e76c53f482811b6d75475761e\\n\",\n      \"  Stored in directory: /root/.cache/pip/wheels/48/b4/d5/fb5b7f1d040f2ff20687e3bad6867d63155dbde5a7c10f4293\\n\",\n      \"Successfully built func-timeout ftfy dm-haiku lm-eval-harness sqlitedict pycountry pathtools subprocess32 gpustat\\n\",\n      \"\\u001B[31mERROR: tensorflow 2.5.0 has requirement gast==0.4.0, but you'll have gast 0.3.3 which is incompatible.\\u001B[0m\\n\",\n      \"\\u001B[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\\u001B[0m\\n\",\n      \"\\u001B[31mERROR: tensorflow 2.5.0 has requirement tensorflow-estimator<2.6.0,>=2.5.0rc0, but you'll have tensorflow-estimator 2.4.0 which is incompatible.\\u001B[0m\\n\",\n      \"\\u001B[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\\u001B[0m\\n\",\n      \"\\u001B[31mERROR: google-cloud-bigquery 1.21.0 has requirement google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you'll have google-resumable-media 1.3.0 which is incompatible.\\u001B[0m\\n\",\n      \"\\u001B[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\\u001B[0m\\n\",\n      \"\\u001B[31mERROR: tensorflow-cpu 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.34.1 which is incompatible.\\u001B[0m\\n\",\n      \"\\u001B[31mERROR: black 20.8b1 has requirement regex>=2020.1.8, but you'll have regex 2019.12.20 which is incompatible.\\u001B[0m\\n\",\n      \"Installing collected packages: tokenizers, tqdm, sacremoses, requests, transformers, setuptools, configparser, docker-pycreds, sentry-sdk, smmap, gitdb, GitPython, shortuuid, pathtools, subprocess32, wandb, einops, invoke, cryptography, pynacl, bcrypt, paramiko, pathlib2, fabric, chex, optax, multidict, yarl, async-timeout, aiohttp, colorful, blessings, gpustat, aiohttp-cors, redis, colorama, hiredis, aioredis, py-spy, opencensus-context, opencensus, ray, gast, h5py, tensorflow-estimator, tensorflow-cpu, google-crc32c, google-resumable-media, google-cloud-core, google-cloud-storage, func-timeout, ftfy, jmp, dm-haiku, typed-ast, mypy-extensions, pathspec, black, rehash, best-download, xxhash, fsspec, huggingface-hub, datasets, threadpoolctl, scikit-learn, sqlitedict, tcolorpy, msgfy, mbstrdecoder, typepy, DataProperty, pathvalidate, tabledata, pytablewriter, portalocker, sacrebleu, pycountry, numexpr, zstandard, jsonlines, ujson, lm-dataformat, pluggy, pytest, pybind11, tqdm-multiprocess, lm-eval-harness\\n\",\n      \"  Found existing installation: tqdm 4.41.1\\n\",\n      \"    Uninstalling tqdm-4.41.1:\\n\",\n      \"      Successfully uninstalled tqdm-4.41.1\\n\",\n      \"  Found existing installation: requests 2.23.0\\n\",\n      \"    Uninstalling requests-2.23.0:\\n\",\n      \"      Successfully uninstalled requests-2.23.0\\n\",\n      \"  Found existing installation: setuptools 57.0.0\\n\",\n      \"    Uninstalling setuptools-57.0.0:\\n\",\n      \"      Successfully uninstalled setuptools-57.0.0\\n\",\n      \"  Found existing installation: gast 0.4.0\\n\",\n      \"    Uninstalling gast-0.4.0:\\n\",\n      \"      Successfully uninstalled gast-0.4.0\\n\",\n      \"  Found existing installation: h5py 3.1.0\\n\",\n      \"    Uninstalling h5py-3.1.0:\\n\",\n      \"      Successfully uninstalled h5py-3.1.0\\n\",\n      \"  Found existing installation: tensorflow-estimator 2.5.0\\n\",\n      \"    Uninstalling tensorflow-estimator-2.5.0:\\n\",\n      \"      Successfully uninstalled tensorflow-estimator-2.5.0\\n\",\n      \"  Found existing installation: google-resumable-media 0.4.1\\n\",\n      \"    Uninstalling google-resumable-media-0.4.1:\\n\",\n      \"      Successfully uninstalled google-resumable-media-0.4.1\\n\",\n      \"  Found existing installation: google-cloud-core 1.0.3\\n\",\n      \"    Uninstalling google-cloud-core-1.0.3:\\n\",\n      \"      Successfully uninstalled google-cloud-core-1.0.3\\n\",\n      \"  Found existing installation: google-cloud-storage 1.18.1\\n\",\n      \"    Uninstalling google-cloud-storage-1.18.1:\\n\",\n      \"      Successfully uninstalled google-cloud-storage-1.18.1\\n\",\n      \"  Found existing installation: scikit-learn 0.22.2.post1\\n\",\n      \"    Uninstalling scikit-learn-0.22.2.post1:\\n\",\n      \"      Successfully uninstalled scikit-learn-0.22.2.post1\\n\",\n      \"  Found existing installation: numexpr 2.7.3\\n\",\n      \"    Uninstalling numexpr-2.7.3:\\n\",\n      \"      Successfully uninstalled numexpr-2.7.3\\n\",\n      \"  Found existing installation: pluggy 0.7.1\\n\",\n      \"    Uninstalling pluggy-0.7.1:\\n\",\n      \"      Successfully uninstalled pluggy-0.7.1\\n\",\n      \"  Found existing installation: pytest 3.6.4\\n\",\n      \"    Uninstalling pytest-3.6.4:\\n\",\n      \"      Successfully uninstalled pytest-3.6.4\\n\",\n      \"Successfully installed DataProperty-0.50.1 GitPython-3.1.17 aiohttp-3.7.4.post0 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 bcrypt-3.2.0 best-download-0.0.6 black-20.8b1 blessings-1.7 chex-0.0.7 colorama-0.4.4 colorful-0.5.4 configparser-5.0.2 cryptography-3.4.7 datasets-1.8.0 dm-haiku-0.0.5.dev0 docker-pycreds-0.4.0 einops-0.3.0 fabric-2.6.0 fsspec-2021.6.0 ftfy-6.0.3 func-timeout-4.3.5 gast-0.3.3 gitdb-4.0.7 google-cloud-core-1.6.0 google-cloud-storage-1.36.2 google-crc32c-1.1.2 google-resumable-media-1.3.0 gpustat-0.6.0 h5py-2.10.0 hiredis-2.0.0 huggingface-hub-0.0.10 invoke-1.5.0 jmp-0.0.2 jsonlines-2.0.0 lm-dataformat-0.0.19 lm-eval-harness-0.0.1 mbstrdecoder-1.0.1 msgfy-0.1.0 multidict-5.1.0 mypy-extensions-0.4.3 numexpr-2.7.2 opencensus-0.7.13 opencensus-context-0.1.2 optax-0.0.6 paramiko-2.7.2 pathlib2-2.3.5 pathspec-0.8.1 pathtools-0.1.2 pathvalidate-2.4.1 pluggy-0.13.1 portalocker-2.3.0 py-spy-0.3.7 pybind11-2.6.2 pycountry-20.7.3 pynacl-1.4.0 pytablewriter-0.58.0 pytest-6.2.3 ray-1.2.0 redis-3.5.3 rehash-1.0.0 requests-2.25.1 sacrebleu-1.5.0 sacremoses-0.0.45 scikit-learn-0.24.2 sentry-sdk-1.1.0 setuptools-51.3.3 shortuuid-1.0.1 smmap-4.0.0 sqlitedict-1.6.0 subprocess32-3.5.4 tabledata-1.1.4 tcolorpy-0.0.9 tensorflow-cpu-2.4.1 tensorflow-estimator-2.4.0 threadpoolctl-2.1.0 tokenizers-0.10.3 tqdm-4.45.0 tqdm-multiprocess-0.0.11 transformers-4.4.2 typed-ast-1.4.3 typepy-1.1.5 ujson-4.0.2 wandb-0.10.31 xxhash-2.0.2 yarl-1.6.3 zstandard-0.15.2\\n\"\n     ],\n     \"name\": \"stdout\"\n    },\n    {\n     \"output_type\": \"display_data\",\n     \"data\": {\n      \"application/vnd.colab-display-data+json\": {\n       \"pip_warning\": {\n        \"packages\": [\n         \"google\",\n         \"pkg_resources\"\n        ]\n       }\n      }\n     },\n     \"metadata\": {\n      \"tags\": []\n     }\n    },\n    {\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"Processing ./mesh-transformer-jax\\n\",\n      \"Collecting jax==0.2.12\\n\",\n      \"\\u001B[?25l  Downloading https://files.pythonhosted.org/packages/9a/67/d1a9c94104c559b49bbcb72e9efc33859e982d741ea4902d2a00e66e09d9/jax-0.2.12.tar.gz (590kB)\\n\",\n      \"\\u001B[K     |████████████████████████████████| 593kB 2.2MB/s \\n\",\n      \"\\u001B[?25hRequirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax==0.2.12) (1.19.5)\\n\",\n      \"Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax==0.2.12) (0.12.0)\\n\",\n      \"Requirement already satisfied: opt_einsum in /usr/local/lib/python3.7/dist-packages (from jax==0.2.12) (3.3.0)\\n\",\n      \"Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax==0.2.12) (1.15.0)\\n\",\n      \"Building wheels for collected packages: jax, mesh-transformer\\n\",\n      \"  Building wheel for jax (setup.py) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"  Created wheel for jax: filename=jax-0.2.12-cp37-none-any.whl size=682484 sha256=4636fb45fcbebf88d4dd007e59b03f69fb647920499329b29f883e6ee8a9eccb\\n\",\n      \"  Stored in directory: /root/.cache/pip/wheels/cf/00/88/75c2043dff473f58e892c7e6adfd2c44ccefb6111fcc021e5b\\n\",\n      \"  Building wheel for mesh-transformer (setup.py) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n      \"  Created wheel for mesh-transformer: filename=mesh_transformer-0.0.0-cp37-none-any.whl size=15682 sha256=385ec067463f1689fdffb1b0fd3f98ca40aa878ce5ff2187bf743675ba91190e\\n\",\n      \"  Stored in directory: /root/.cache/pip/wheels/de/a9/d2/2be3e25299342b60fca7965d4e416264ff8b6d8a7e8def76da\\n\",\n      \"Successfully built jax mesh-transformer\\n\",\n      \"Installing collected packages: jax, mesh-transformer\\n\",\n      \"  Found existing installation: jax 0.2.13\\n\",\n      \"    Uninstalling jax-0.2.13:\\n\",\n      \"      Successfully uninstalled jax-0.2.13\\n\",\n      \"Successfully installed jax-0.2.12 mesh-transformer-0.0.0\\n\"\n     ],\n     \"name\": \"stdout\"\n    }\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"aO1UXepF-0Uq\"\n   },\n   \"source\": [\n    \"## Setup Model\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"metadata\": {\n    \"id\": \"ex0qJgaueZtJ\"\n   },\n   \"source\": [\n    \"import os\\n\",\n    \"import requests \\n\",\n    \"from jax.config import config\\n\",\n    \"\\n\",\n    \"colab_tpu_addr = os.environ['COLAB_TPU_ADDR'].split(':')[0]\\n\",\n    \"url = f'http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607'\\n\",\n    \"requests.post(url)\\n\",\n    \"\\n\",\n    \"# The following is required to use TPU Driver as JAX's backend.\\n\",\n    \"config.FLAGS.jax_xla_backend = \\\"tpu_driver\\\"\\n\",\n    \"config.FLAGS.jax_backend_target = \\\"grpc://\\\" + os.environ['COLAB_TPU_ADDR']\"\n   ],\n   \"execution_count\": 4,\n   \"outputs\": []\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"NIgUVdFLe4A8\"\n   },\n   \"source\": [\n    \"Sometimes the next step errors for some reason, just run it again ¯\\\\\\\\\\\\_(ツ)\\\\_/¯\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"metadata\": {\n    \"id\": \"-A5IGYSaeze3\"\n   },\n   \"source\": [\n    \"import time\\n\",\n    \"\\n\",\n    \"import jax\\n\",\n    \"from jax.experimental import maps\\n\",\n    \"import numpy as np\\n\",\n    \"import optax\\n\",\n    \"import transformers\\n\",\n    \"\\n\",\n    \"from mesh_transformer.checkpoint import read_ckpt_lowmem\\n\",\n    \"from mesh_transformer.sampling import nucleaus_sample\\n\",\n    \"from mesh_transformer.transformer_shard import CausalTransformer\"\n   ],\n   \"execution_count\": 5,\n   \"outputs\": []\n  },\n  {\n   \"cell_type\": \"code\",\n   \"metadata\": {\n    \"id\": \"QAgKq-X2kmba\",\n    \"colab\": {\n     \"base_uri\": \"https://localhost:8080/\",\n     \"height\": 167,\n     \"referenced_widgets\": [\n      \"5b8a31b3b4034116af0a81d897c3123b\",\n      \"835ee2070c05443c9fa3ae6fece3fb44\",\n      \"9e6646f7066341ef9619f5f7e6f7c8b8\",\n      \"4f803e3d56b74e18997b216bf9d65337\",\n      \"f9fa7c1bc9f04df09e487b16761d6b8f\",\n      \"39aed05f694a495abd03aa00ff71efb3\",\n      \"61b0857802184b51abf77039fad681d7\",\n      \"0fb689df0a8b42c2b87e7f11b818b88a\",\n      \"8b76924bdac749d5b4908678fd0a7497\",\n      \"b03bd083afc84f52834d4a124e57c442\",\n      \"3ba9fe91731f4ff380cda2183ad72c4d\",\n      \"4e01b9cceffb4d178e97f1d1b36d62b0\",\n      \"b3ac7912f51746afae20616269dc69f0\",\n      \"fb1263f5a3da42e6acab0e9a66551b32\",\n      \"3426927332884df4b1ec5544ed479c45\",\n      \"debf4a0a609a4cffa5cd686a0a23f6e2\",\n      \"7197de8a39ab4e4186eabd061c730cf5\",\n      \"c31661c9a1ff4bd681cc10d70fd287c0\",\n      \"bb11a4fbf16f488082aa2bac7443fd62\",\n      \"5360630d6b0541dbaafa7ff91610bd84\",\n      \"f3883c133c62438db114c22cfc94e351\",\n      \"74672c43909444c592955afd0d727a28\",\n      \"67f7c3bc59384dc7812d7130c7cde450\",\n      \"bb7410ff9cb4484da73302993427b90a\"\n     ]\n    },\n    \"outputId\": \"041117df-a315-4b95-9caa-26cb870ff3df\"\n   },\n   \"source\": [\n    \"params = {\\n\",\n    \"  \\\"layers\\\": 28,\\n\",\n    \"  \\\"d_model\\\": 4096,\\n\",\n    \"  \\\"n_heads\\\": 16,\\n\",\n    \"  \\\"n_vocab\\\": 50400,\\n\",\n    \"  \\\"norm\\\": \\\"layernorm\\\",\\n\",\n    \"  \\\"pe\\\": \\\"rotary\\\",\\n\",\n    \"  \\\"pe_rotary_dims\\\": 64,\\n\",\n    \"\\n\",\n    \"  \\\"seq\\\": 2048,\\n\",\n    \"  \\\"cores_per_replica\\\": 8,\\n\",\n    \"  \\\"per_replica_batch\\\": 1,\\n\",\n    \"}\\n\",\n    \"\\n\",\n    \"per_replica_batch = params[\\\"per_replica_batch\\\"]\\n\",\n    \"cores_per_replica = params[\\\"cores_per_replica\\\"]\\n\",\n    \"seq = params[\\\"seq\\\"]\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"params[\\\"sampler\\\"] = nucleaus_sample\\n\",\n    \"\\n\",\n    \"# here we \\\"remove\\\" the optimizer parameters from the model (as we don't need them for inference)\\n\",\n    \"params[\\\"optimizer\\\"] = optax.scale(0)\\n\",\n    \"\\n\",\n    \"mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\\n\",\n    \"devices = np.array(jax.devices()).reshape(mesh_shape)\\n\",\n    \"\\n\",\n    \"maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))\\n\",\n    \"\\n\",\n    \"tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')\"\n   ],\n   \"execution_count\": 6,\n   \"outputs\": [\n    {\n     \"output_type\": \"display_data\",\n     \"data\": {\n      \"application/vnd.jupyter.widget-view+json\": {\n       \"model_id\": \"5b8a31b3b4034116af0a81d897c3123b\",\n       \"version_minor\": 0,\n       \"version_major\": 2\n      },\n      \"text/plain\": [\n       \"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…\"\n      ]\n     },\n     \"metadata\": {\n      \"tags\": []\n     }\n    },\n    {\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"\\n\"\n     ],\n     \"name\": \"stdout\"\n    },\n    {\n     \"output_type\": \"display_data\",\n     \"data\": {\n      \"application/vnd.jupyter.widget-view+json\": {\n       \"model_id\": \"8b76924bdac749d5b4908678fd0a7497\",\n       \"version_minor\": 0,\n       \"version_major\": 2\n      },\n      \"text/plain\": [\n       \"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…\"\n      ]\n     },\n     \"metadata\": {\n      \"tags\": []\n     }\n    },\n    {\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"\\n\"\n     ],\n     \"name\": \"stdout\"\n    },\n    {\n     \"output_type\": \"display_data\",\n     \"data\": {\n      \"application/vnd.jupyter.widget-view+json\": {\n       \"model_id\": \"7197de8a39ab4e4186eabd061c730cf5\",\n       \"version_minor\": 0,\n       \"version_major\": 2\n      },\n      \"text/plain\": [\n       \"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…\"\n      ]\n     },\n     \"metadata\": {\n      \"tags\": []\n     }\n    },\n    {\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"\\n\"\n     ],\n     \"name\": \"stdout\"\n    }\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"yFgRkUgfiNdA\"\n   },\n   \"source\": [\n    \"Here we create the network and load the parameters from the downloaded files. Expect this to take around 5 minutes.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"metadata\": {\n    \"colab\": {\n     \"base_uri\": \"https://localhost:8080/\"\n    },\n    \"id\": \"lwNETD2Uk8nu\",\n    \"outputId\": \"a659285b-5b46-4ddf-d65c-fbdad0594b86\"\n   },\n   \"source\": [\n    \"total_batch = per_replica_batch * jax.device_count() // cores_per_replica\\n\",\n    \"\\n\",\n    \"network = CausalTransformer(params)\\n\",\n    \"\\n\",\n    \"network.state = read_ckpt_lowmem(network.state, \\\"step_383500/\\\", devices.shape[1])\\n\",\n    \"\\n\",\n    \"network.state = network.move_xmap(network.state, np.zeros(cores_per_replica))\"\n   ],\n   \"execution_count\": 7,\n   \"outputs\": [\n    {\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"/usr/local/lib/python3.7/dist-packages/jax/experimental/maps.py:412: UserWarning: xmap is an experimental feature and probably has bugs!\\n\",\n      \"  warn(\\\"xmap is an experimental feature and probably has bugs!\\\")\\n\"\n     ],\n     \"name\": \"stderr\"\n    },\n    {\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"key shape (8, 2)\\n\",\n      \"in shape (1, 2048)\\n\",\n      \"dp 1\\n\",\n      \"mp 8\\n\",\n      \"read from gcs in 22.633s\\n\"\n     ],\n     \"name\": \"stdout\"\n    }\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {\n    \"id\": \"A-eT7Sw6if4J\"\n   },\n   \"source\": [\n    \"## Run Model\\n\",\n    \"\\n\",\n    \"Finally, we are ready to infer with the model! The first sample takes around a minute due to compilation, but after that it should only take about 10 seconds per sample.\\n\",\n    \"\\n\",\n    \"Feel free to mess with the different sampling parameters (top_p and temp), as well as the length of the generations (gen_len, causes a recompile when changed).\\n\",\n    \"\\n\",\n    \"You can also change other things like per_replica_batch in the previous cells to change how many generations are done in parallel. A larger batch has higher latency but higher throughput when measured in tokens generated/s. This is useful for doing things like best-of-n cherry picking.\\n\",\n    \"\\n\",\n    \"*Tip for best results: Make sure your prompt does not have any trailing spaces, which tend to confuse the model due to the BPE tokenization used during training.*\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# allow text wrapping in generated output: https://stackoverflow.com/a/61401455\\n\",\n    \"from IPython.display import HTML, display\\n\",\n    \"\\n\",\n    \"def set_css():\\n\",\n    \"  display(HTML('''\\n\",\n    \"  <style>\\n\",\n    \"    pre {\\n\",\n    \"        white-space: pre-wrap;\\n\",\n    \"    }\\n\",\n    \"  </style>\\n\",\n    \"  '''))\\n\",\n    \"get_ipython().events.register('pre_run_cell', set_css)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"metadata\": {\n    \"colab\": {\n     \"base_uri\": \"https://localhost:8080/\"\n    },\n    \"id\": \"ZVzs2TYlvYeX\",\n    \"outputId\": \"80c8b1e5-0d1c-4799-d682-4a5be0c038a1\"\n   },\n   \"source\": [\n    \"def infer(context, top_p=0.9, temp=1.0, gen_len=512):\\n\",\n    \"    tokens = tokenizer.encode(context)\\n\",\n    \"\\n\",\n    \"    provided_ctx = len(tokens)\\n\",\n    \"    pad_amount = seq - provided_ctx\\n\",\n    \"\\n\",\n    \"    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\\n\",\n    \"    batched_tokens = np.array([padded_tokens] * total_batch)\\n\",\n    \"    length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\\n\",\n    \"\\n\",\n    \"    start = time.time()\\n\",\n    \"    output = network.generate(batched_tokens, length, gen_len, {\\\"top_p\\\": np.ones(total_batch) * top_p, \\\"temp\\\": np.ones(total_batch) * temp})\\n\",\n    \"\\n\",\n    \"    samples = []\\n\",\n    \"    decoded_tokens = output[1][0]\\n\",\n    \"\\n\",\n    \"    for o in decoded_tokens[:, :, 0]:\\n\",\n    \"      samples.append(f\\\"\\\\033[1m{context}\\\\033[0m{tokenizer.decode(o)}\\\")\\n\",\n    \"\\n\",\n    \"    print(f\\\"completion done in {time.time() - start:06}s\\\")\\n\",\n    \"    return samples\\n\",\n    \"\\n\",\n    \"print(infer(\\\"EleutherAI is\\\")[0])\"\n   ],\n   \"execution_count\": 8,\n   \"outputs\": [\n    {\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"completion done in 59.615925312042236s\\n\",\n      \" a team of senior and junior developers that are working together to push back the frontiers of racing videogames. The team is involved in developing a racing videogame from the ground up. It will feature cars and tracks drawn from a creative process.\\n\",\n      \"\\n\",\n      \"ELEUTERAI TEAM\\n\",\n      \"\\n\",\n      \"HISTORY\\n\",\n      \"\\n\",\n      \"Founded by Gianfrancesco Cengia, EleuterAI is an independent videogame development team that started making videogames in 2004.\\n\",\n      \"\\n\",\n      \"A young team formed in collaboration between the principle members Gianfrancesco Cengia and Luigi Zampar (as well as Luca Barbero, Rolando Labernini and Simone Trovato). Over a period of five years, the members learned how to write code, how to build efficient software processes and how to design a fun racing game. All the while, they shared an enthusiasm for racing games and the ever-evolving technology that became a shared passion.\\n\",\n      \"\\n\",\n      \"After five years of collaboration, the team reached the starting point of its car games. Due to the great success of Formula One 2017, the videogame race team found the confidence to start a new phase of their adventure.\\n\",\n      \"\\n\",\n      \"The innovative approach of the team also brought results to the printed world. Indeed, the videogames that the EleuterAI team published in 2014, 2015 and 2016 are among the highest-selling videogames on the French market.\\n\",\n      \"\\n\",\n      \"Among the videogame developments produced by the team, Trovato mentions Firepower, Vinicius and Stop and Stares.\\n\",\n      \"\\n\",\n      \"The EleuterAI experience also showed the team the opportunity to expand its horizons outside of car racing. Indeed, as of today, the group is involved in developing a videogame based on the legendary Italian car brand Alfa Romeo. The development phase is already underway and will be a challenge and new experience for the team. The project is planned to be released in late 2019.\\n\",\n      \"\\n\",\n      \"CHALLENGE\\n\",\n      \"\\n\",\n      \"This project will be for the EleuterAI team an extraordinary opportunity to show their capabilities and the expertise acquired in the car race gaming experience. As a result, we could have a videogame of the highest quality.\\n\",\n      \"\\n\",\n      \"Team-work and collaboration will be key elements to develop a fun racing experience and will be at the center of the game.\\n\",\n      \"\\n\",\n      \"Vision & Values\\n\",\n      \"\\n\",\n      \"We are close to our mission, which is to make the best racing videogames. We don’t have any brand vision or values, but we do have a very clear idea of\\n\"\n     ],\n     \"name\": \"stdout\"\n    }\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"metadata\": {\n    \"colab\": {\n     \"base_uri\": \"https://localhost:8080/\"\n    },\n    \"id\": \"nvlAK6RbCJYg\",\n    \"outputId\": \"c0525611-ecc8-422e-d8e3-24b3a2159083\"\n   },\n   \"source\": [\n    \"#@title  { form-width: \\\"300px\\\" }\\n\",\n    \"top_p = 0.9 #@param {type:\\\"slider\\\", min:0, max:1, step:0.1}\\n\",\n    \"temp = 1 #@param {type:\\\"slider\\\", min:0, max:1, step:0.1}\\n\",\n    \"\\n\",\n    \"context = \\\"\\\"\\\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\\"\\\"\\\"\\n\",\n    \"\\n\",\n    \"print(infer(top_p=top_p, temp=temp, gen_len=512, context=context)[0])\"\n   ],\n   \"execution_count\": 9,\n   \"outputs\": [\n    {\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"completion done in 13.510299682617188s\\n\",\n      \" The scientists decided to ask one of the creatures to show them what English sounds like.\\n\",\n      \"\\n\",\n      \"The creature was then transported in a private jet to a real-life, windy city, where it was finally placed on the roof of a building, wearing a set of headphones. The scientist proceeded to ask the creature to demonstrate the letter “R”, which turned out to be an “R”-shaped unicorn.\\n\",\n      \"\\n\",\n      \"At this point, the scientists expected the creature to start singing, or making a horselike sound. Instead, the unicorn imitated a car horn, roaring “Raaaaaarrrr!”. The scientist chuckled at the unplanned execution. The unicorn finished the demonstration by shouting “Ahhhhhhhhhhhhhhhhh!”, sounding like an escaped convict having an orgasm.\\n\",\n      \"\\n\",\n      \"Then the creature, after spending about an hour on the rooftop, as it was exhausted, went to sleep. By chance, the creature’s body was mown down by a steamroller a few hours later, and its body was sent to the scientists.\\n\",\n      \"\\n\",\n      \"The scientists analysed the DNA and created a synthetic version of the creature’s horn, and inserted it into a New Zealand sheep. The sheep were taught to use a computer program for spelling, but after the sheep had been taught to pronounce the letter “R”, it would never stop saying it.\\n\",\n      \"\\n\",\n      \"Dr. Graeme Lloyd, of the University of Canterbury, New Zealand, said “We created this synthetic horn by the end of the project, and inserted it into the New Zealand sheep. We told the sheep that they had to use the synthetic horn to say “R”, and that they must never say anything else. For a month, the sheep would wake up at dawn, go on to the roof of our office building, and then try to say “R”. In a few weeks, the sheep’s heads would fall off.”\\n\",\n      \"\\n\",\n      \"Related\\n\",\n      \"\\n\",\n      \"Comments\\n\",\n      \"\\n\",\n      \"Before going off to sally forth about the wonders and dangers of the internet, did you perhaps check on BING to see if your best friend had posted up something about you?\\n\",\n      \"Or, perhaps it’s more likely that you’re living in a fantasy land, which, when looking at that headline, is probably the case.\\n\",\n      \"\\n\",\n      \"Actually, that article about unicorns is so unbelievably stupid and self-satifyingly absurd, even I had to\\n\"\n     ],\n     \"name\": \"stdout\"\n    }\n   ]\n  }\n ]\n}"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "create_finetune_tfrecords.py",
          "type": "blob",
          "size": 11.3916015625,
          "content": "import argparse\nimport os\nimport re\nimport random\n\nfrom pathlib import Path\nfrom typing import List\n\nimport ftfy\nimport tensorflow as tf\nfrom lm_dataformat import Reader\nfrom transformers import GPT2TokenizerFast\nfrom tqdm import tqdm\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"\"\"\n    Converts a text dataset into the training data format expected by the model.\n\n    Adapted from the script create_tfrecords.py in the gpt-neo repo.\n\n    - Your text dataset:\n        - can be provided as .txt files, or as an archive (.tar.gz, .xz, jsonl.zst).\n        - can be one file or multiple\n            - using a single large file may use too much memory and crash - if this occurs, split the file up into a few files\n        - the model's end-of-text separator is added between the contents of each file\n        - if the string '<|endoftext|>' appears inside a file, it is treated as the model's end-of-text separator (not the actual string '<|endoftext|>')\n            - this behavior can be disabled with --treat-eot-as-text\n\n    This script creates a single .tfrecords file as output\n        - Why: the model's data loader ignores \"trailing\" data (< 1 batch) at the end of a .tfrecords file\n            - this causes data loss if you have many .tfrecords files\n        - This is probably not appropriate for very large datasets\n    \"\"\", formatter_class=argparse.RawTextHelpFormatter)\n    parser.add_argument(\n        \"input_path\",\n        type=str,\n        help=\"Path to an input file, or a directory that contains the input files.\",\n    )\n    parser.add_argument(\"name\", type=str,\n                        help=\"Name of output file will be {name}_{seqnum}.tfrecords, where seqnum is total sequence count\")\n    parser.add_argument(\"--output-dir\", type=str, default=\"\", help=\"Output directory (default: current directory)\")\n\n    cleaning_args = parser.add_argument_group('data cleaning arguments')\n\n    cleaning_args.add_argument(\"--normalize-with-ftfy\", action=\"store_true\", help=\"Normalize text with ftfy\")\n    cleaning_args.add_argument(\"--normalize-with-wikitext-detokenize\",\n                               action=\"store_true\", help=\"Use wikitext detokenizer\")\n    minu_help = \"Exclude repetitive documents made up of < MIN_UNIQUE_TOKENS unique tokens. These can produce large gradients.\"\n    minu_help += \" Set <= 0 to disable. If enabled, 200 is a good default value. (Default: 0)\"\n    cleaning_args.add_argument(\"--min-unique-tokens\", type=int, default=0,\n                               help=minu_help)\n\n    shuffle_pack_args = parser.add_argument_group('data shuffling/packing arguments')\n    repack_ep_help = \"Repeat the data N_REPACK_EPOCHS times, shuffled differently in each repetition. Recommended for multi-epoch training (set this to your intended number of epochs).\"\n    shuffle_pack_args.add_argument(\"--n-repack-epochs\",\n                                   type=int, default=1,\n                                   help=repack_ep_help\n                                   )\n    shuffle_pack_args.add_argument(\"--seed\", type=int, default=10,\n                                   help=\"random seed for shuffling data (default: 10)\")\n    shuffle_pack_args.add_argument(\"--preserve-data-order\",\n                                   default=False, action=\"store_true\",\n                                   help=\"Disables shuffling, so the input and output data have the same order.\")\n\n    misc_args = parser.add_argument_group('miscellaneous arguments')\n    misc_args.add_argument(\"--verbose\",\n                           default=False, action=\"store_true\",\n                           help=\"Prints extra information, such as the text removed by --min-unique-tokens\")\n\n    args = parser.parse_args()\n\n    # convert input_path to pathy\n    args.input_path = Path(args.input_path)\n\n    return args\n\n\ndef get_files(input_path: Path) -> List[str]:\n    supported_file_types = [\"jsonl.zst\", \".txt\", \".xz\", \".tar.gz\"]\n    if input_path.is_dir():\n        # get all files with supported file types\n        files = [list(Path(input_path).glob(f\"*{ft}\")) for ft in supported_file_types]\n        # flatten list\n        files = [f for sublist in files for f in sublist]\n        assert files, f\"No files with supported types found in directory: {input_path}\"\n    elif input_path.is_file():\n        assert any(\n            str(input_path).endswith(f_type) for f_type in supported_file_types\n        ), f\"Input file type must be one of: {supported_file_types}\"\n        files = [input_path]\n    else:\n        raise FileNotFoundError(f\"No such file or directory: {input_path=}\")\n\n    return [str(f) for f in files]\n\n\ndef wikitext_detokenizer(string):\n    # contractions\n    string = string.replace(\"s '\", \"s'\")\n    string = re.sub(r\"/' [0-9]/\", r\"/'[0-9]/\", string)\n    # number separators\n    string = string.replace(\" @-@ \", \"-\")\n    string = string.replace(\" @,@ \", \",\")\n    string = string.replace(\" @.@ \", \".\")\n    # punctuation\n    string = string.replace(\" : \", \": \")\n    string = string.replace(\" ; \", \"; \")\n    string = string.replace(\" . \", \". \")\n    string = string.replace(\" ! \", \"! \")\n    string = string.replace(\" ? \", \"? \")\n    string = string.replace(\" , \", \", \")\n    # double brackets\n    string = re.sub(r\"\\(\\s*([^\\)]*?)\\s*\\)\", r\"(\\1)\", string)\n    string = re.sub(r\"\\[\\s*([^\\]]*?)\\s*\\]\", r\"[\\1]\", string)\n    string = re.sub(r\"{\\s*([^}]*?)\\s*}\", r\"{\\1}\", string)\n    string = re.sub(r\"\\\"\\s*([^\\\"]*?)\\s*\\\"\", r'\"\\1\"', string)\n    string = re.sub(r\"'\\s*([^']*?)\\s*'\", r\"'\\1'\", string)\n    # miscellaneous\n    string = string.replace(\"= = = =\", \"====\")\n    string = string.replace(\"= = =\", \"===\")\n    string = string.replace(\"= =\", \"==\")\n    string = string.replace(\" \" + chr(176) + \" \", chr(176))\n    string = string.replace(\" \\n\", \"\\n\")\n    string = string.replace(\"\\n \", \"\\n\")\n    string = string.replace(\" N \", \" 1 \")\n    string = string.replace(\" 's\", \"'s\")\n\n    return string\n\n\ndef _int64_feature(value):\n    \"\"\"\n    Returns an int64_list from a bool / enum / int / uint.\n    \"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef write_to_file(writer, data):\n    \"\"\"\n    writes data to tfrecord file\n    \"\"\"\n    feature = {\n        \"text\": _int64_feature(data)\n    }\n    tf_example = tf.train.Example(features=tf.train.Features(feature=feature))\n    writer.write(tf_example.SerializeToString())\n\n\ndef write_tfrecord(sequences, fp):\n    with tf.io.TFRecordWriter(fp) as writer:\n        for seq in sequences:\n            write_to_file(writer, seq)\n\n\ndef split_list(l, n):\n    # splits list/string into n size chunks\n    return [l[i:i + n] for i in range(0, len(l), n)]\n\n\ndef enforce_min_unique(seqs, min_unique_tokens, enc, verbose=False):\n    for seq in tqdm(seqs, mininterval=1, smoothing=0, desc=\"enforce_min_unique_tokens\"):\n        if len(set(seq)) >= min_unique_tokens:\n            yield seq\n        elif verbose:\n            text = enc.decode(seq)\n            print(f\"excluding with {len(set(seq))} unique tokens:\\n\\n{repr(text)}\\n\\n\")\n\n\ndef eot_splitting_generator(string_iterable, encoder):\n    \"\"\"\n    Given strings, splits them internally on <|endoftext|> and yields (generally more) strings\n    \"\"\"\n    for doc in string_iterable:\n        for d in doc.split(encoder.eos_token):\n            if len(d) > 0:\n                yield d\n\n\ndef prep_and_tokenize_generator(string_iterable, encoder, normalize_with_ftfy, normalize_with_wikitext_detokenize):\n    \"\"\"\n    Given strings, does data cleaning / tokenization and yields arrays of tokens\n    \"\"\"\n    for doc in string_iterable:\n        if normalize_with_ftfy:  # fix text with ftfy if specified\n            doc = ftfy.fix_text(doc, normalization='NFKC')\n        if normalize_with_wikitext_detokenize:\n            doc = wikitext_detokenizer(doc)\n        tokens = encoder.encode(doc) + [encoder.eos_token_id]\n        yield tokens\n\n\ndef file_to_tokenized_docs_generator(file_path, encoder, args):\n    \"\"\"\n    Given a file path, reads the file and tokenizes the contents\n\n    Yields token arrays of arbitrary, unequal length\n    \"\"\"\n    reader = Reader(file_path)\n    string_iterable = reader.stream_data(threaded=False)\n    string_iterable = eot_splitting_generator(string_iterable, encoder)\n\n    token_list_gen = prep_and_tokenize_generator(string_iterable,\n                                                 encoder,\n                                                 normalize_with_ftfy=args.normalize_with_ftfy,\n                                                 normalize_with_wikitext_detokenize=args.normalize_with_wikitext_detokenize\n                                                 )\n    return token_list_gen\n\n\ndef read_files_to_tokenized_docs(files, args, encoder):\n    docs = []\n\n    if args.preserve_data_order:\n        files = sorted(files)\n    else:\n        random.shuffle(files)\n\n    for f in tqdm(files, mininterval=10, smoothing=0, desc=\"reading/tokenizing files\"):\n        docs.extend(file_to_tokenized_docs_generator(f, encoder, args))\n\n    if not args.preserve_data_order:\n        # shuffle at individual document level\n        random.shuffle(docs)\n\n    return docs\n\n\ndef arrays_to_sequences(token_list_iterable, sequence_length=2049):\n    \"\"\"\n    Given token arrays of arbitrary lengths, concats/splits them into arrays of equal length\n\n    Returns equal-length token arrays, followed by a a final array of trailing tokens (which may be shorter)\n    \"\"\"\n    accum = []\n    for l in token_list_iterable:\n        accum.extend(l)\n\n        if len(accum) > sequence_length:\n            chunks = split_list(accum, sequence_length)\n            yield from chunks[:-1]\n            accum = chunks[-1]\n\n    if len(accum) > 0:\n        yield accum\n\n\ndef chunk_and_finalize(arrays, args, encoder):\n    sequences = list(arrays_to_sequences(arrays))\n\n    full_seqs, trailing_data = sequences[:-1], sequences[-1]\n\n    if args.min_unique_tokens > 0:\n        full_seqs = list(enforce_min_unique(full_seqs, args.min_unique_tokens, encoder, args.verbose))\n\n    if not args.preserve_data_order:\n        random.shuffle(full_seqs)\n\n    return full_seqs, trailing_data\n\n\ndef create_tfrecords(files, args):\n    GPT2TokenizerFast.max_model_input_sizes['gpt2'] = 1e20  # disables a misleading warning\n    encoder = GPT2TokenizerFast.from_pretrained('gpt2')\n\n    random.seed(args.seed)\n\n    all_sequences_across_epochs = []\n\n    docs = read_files_to_tokenized_docs(files, args, encoder)\n\n    full_seqs, trailing_data = chunk_and_finalize(docs, args, encoder)\n\n    all_sequences_across_epochs.extend(full_seqs)\n\n    # ep 2+\n    for ep_ix in range(1, args.n_repack_epochs):\n        # re-shuffle\n        if not args.preserve_data_order:\n            random.shuffle(docs)\n            full_seqs, trailing_data = chunk_and_finalize(docs, args, encoder)\n        else:\n            # if we're preserving data order, we can still \"repack\" by shifting everything\n            # with the trailing data of the last epoch at the beginning\n            seqs_with_prefix = [trailing_data] + full_seqs\n            full_seqs, trailing_data = chunk_and_finalize(seqs_with_prefix, args, encoder)\n\n        all_sequences_across_epochs.extend(full_seqs)\n\n    # final\n    print(f\"dropped {len(trailing_data)} tokens of trailing data\")\n\n    total_sequence_len = len(all_sequences_across_epochs)\n\n    fp = os.path.join(args.output_dir, f\"{args.name}_{total_sequence_len}.tfrecords\")\n    write_tfrecord(all_sequences_across_epochs, fp)\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n\n    if args.output_dir:\n        os.makedirs(args.output_dir, exist_ok=True)\n    files = get_files(args.input_path)\n    print(f\"Creating TFRecords from files: {files}\")\n\n    results = create_tfrecords(files, args)\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "device_sample.py",
          "type": "blob",
          "size": 3.4208984375,
          "content": "import argparse\nimport json\nimport time\n\nimport jax\nimport numpy as np\nimport optax\n\nfrom mesh_transformer import util\nfrom mesh_transformer.checkpoint import read_ckpt\nfrom mesh_transformer.sampling import nucleaus_sample\nfrom mesh_transformer.transformer_shard import CausalTransformer\nimport transformers\nfrom smart_open import open\n\nfrom mesh_transformer.util import clip_by_global_norm\n\n\ndef parse_args():\n    # Parse command line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, default=None, help=\"Config file location\")\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    params = json.load(open(args.config))\n\n    gradient_accumulation_steps = params.get(\"gradient_accumulation_steps\", 1)\n    per_replica_batch = params[\"per_replica_batch\"]\n    cores_per_replica = params[\"cores_per_replica\"]\n\n    assert cores_per_replica <= 8\n\n    bucket = params[\"bucket\"]\n    model_dir = params[\"model_dir\"]\n    layers = params[\"layers\"]\n    d_model = params[\"d_model\"]\n    n_heads = params[\"n_heads\"]\n    n_vocab = params[\"n_vocab\"]\n    seq = params[\"seq\"]\n    norm = params[\"norm\"]\n\n    params[\"sampler\"] = nucleaus_sample\n    opt = optax.chain(\n        optax.scale(1 / gradient_accumulation_steps),\n        clip_by_global_norm(1),\n        optax.scale_by_adam(),\n        optax.additive_weight_decay(0),\n        optax.scale(-1),\n        optax.scale_by_schedule(util.gpt3_schedule(0, 1, 0, 0))\n    )\n\n    params[\"optimizer\"] = opt\n\n    start = time.time()\n    print(f\"jax devices: {jax.device_count()}\")\n    print(f\"jax runtime initialized in {time.time() - start:.06}s\")\n\n    mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\n    devices = np.array(jax.devices()).reshape(mesh_shape)\n\n    with open(f\"gs://{bucket}/{model_dir}/meta.json\", \"r\") as f:\n        meta = json.load(f)\n\n    ckpt_step = meta[\"checkpoints\"][-1]\n    print(f\"using checkpoint {ckpt_step}\")\n\n    total_batch = per_replica_batch * jax.device_count() // cores_per_replica\n    with jax.experimental.maps.mesh(devices, ('dp', 'mp')):\n        network = CausalTransformer(params)\n\n        start = time.time()\n        network.state = read_ckpt(network.state, f\"gs://{bucket}/{model_dir}/step_{ckpt_step}/\", devices.shape[1])\n        print(f\"network loaded in {time.time() - start:.06}s\")\n\n        local_shards = max(jax.local_device_count() // mesh_shape[1], 1)\n        del network.state[\"opt_state\"]\n        network.state = network.move_xmap(network.state, np.zeros(local_shards))\n\n        tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')\n\n        while True:\n            context = input(\"Type input:\")\n            tokens = tokenizer.encode(context)\n\n            start = time.time()\n\n            provided_ctx = len(tokens)\n            pad_amount = seq - provided_ctx\n\n            padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\n            batched_tokens = np.array([padded_tokens] * total_batch)\n            length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\n\n            output = network.generate(batched_tokens, length, 512, {\"top_p\": np.ones(total_batch) * 0.9,\n                                                                    \"temp\": np.ones(total_batch) * 0.75})\n\n            for idx, o in enumerate(output[1][0][:, :, 0]):\n                print(f\"sample {idx}: {repr(tokenizer.decode(o))}\")\n\n            print(f\"completion done in {time.time() - start:06}s\")\n"
        },
        {
          "name": "device_serve.py",
          "type": "blob",
          "size": 6.251953125,
          "content": "import argparse\nimport json\nimport threading\nimport time\nfrom queue import Queue, Empty\n\nimport jax\nimport numpy as np\nimport optax\n\nfrom mesh_transformer import util\nfrom mesh_transformer.checkpoint import read_ckpt\nfrom mesh_transformer.sampling import nucleaus_sample\nfrom mesh_transformer.transformer_shard import CausalTransformer\nimport transformers\nfrom smart_open import open\n\nfrom mesh_transformer.util import clip_by_global_norm\n\nfrom flask import Flask, request, make_response, jsonify\napp = Flask(__name__)\n\nrequests_queue = Queue()\n\n\"\"\"\ncurl --header \"Content-Type: application/json\" \\\n  --request POST \\\n  --data '{\"context\":\"eleutherai\", \"top_p\": 0.9, \"temp\": 0.75}' \\\n  http://localhost:5000/complete\n\"\"\"\n\n\ndef _build_cors_prelight_response():\n    response = make_response()\n    response.headers.add(\"Access-Control-Allow-Origin\", \"*\")\n    response.headers.add('Access-Control-Allow-Headers', \"*\")\n    response.headers.add('Access-Control-Allow-Methods', \"*\")\n    return response\n\n\ndef _corsify_actual_response(response):\n    response.headers.add(\"Access-Control-Allow-Origin\", \"*\")\n    return response\n\n\n@app.route('/complete', methods=['POST', 'OPTIONS'])\ndef complete():\n    if request.method == \"OPTIONS\":  # CORS preflight\n        return _build_cors_prelight_response()\n    elif request.method == \"POST\":  # The actual request following the preflight\n        content = request.json\n\n        if requests_queue.qsize() > 100:\n            return {\"error\": \"queue full, try again later\"}\n\n        response_queue = Queue()\n\n        requests_queue.put(({\n                                \"context\": content[\"context\"],\n                                \"top_p\": float(content[\"top_p\"]),\n                                \"temp\": float(content[\"temp\"])\n                            }, response_queue))\n\n        return _corsify_actual_response(jsonify({\"completion\": response_queue.get()}))\n    else:\n        raise RuntimeError(\"Weird - don't know how to handle method {}\".format(request.method))\n\n\ndef parse_args():\n    # Parse command line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, default=None, help=\"Config file location\")\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    threading.Thread(target=app.run, kwargs={\"port\": 5000, \"host\": \"0.0.0.0\"}).start()\n\n    args = parse_args()\n    params = json.load(open(args.config))\n\n    gradient_accumulation_steps = params.get(\"gradient_accumulation_steps\", 1)\n    per_replica_batch = params[\"per_replica_batch\"]\n    cores_per_replica = params[\"cores_per_replica\"]\n\n    assert cores_per_replica <= 8\n\n    bucket = params[\"bucket\"]\n    model_dir = params[\"model_dir\"]\n    layers = params[\"layers\"]\n    d_model = params[\"d_model\"]\n    n_heads = params[\"n_heads\"]\n    n_vocab = params[\"n_vocab\"]\n    seq = params[\"seq\"]\n    norm = params[\"norm\"]\n\n    params[\"sampler\"] = nucleaus_sample\n    opt = optax.chain(\n        optax.scale(1 / gradient_accumulation_steps),\n        clip_by_global_norm(1),\n        optax.scale_by_adam(),\n        optax.additive_weight_decay(0),\n        optax.scale(-1),\n        optax.scale_by_schedule(util.gpt3_schedule(0, 1, 0, 0))\n    )\n\n    params[\"optimizer\"] = opt\n\n    start = time.time()\n    print(f\"jax devices: {jax.device_count()}\")\n    print(f\"jax runtime initialized in {time.time() - start:.06}s\")\n\n    mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\n    devices = np.array(jax.devices()).reshape(mesh_shape)\n\n    with open(f\"gs://{bucket}/{model_dir}/meta.json\", \"r\") as f:\n        meta = json.load(f)\n\n    ckpt_step = meta[\"checkpoints\"][-1]\n    print(f\"using checkpoint {ckpt_step}\")\n\n    total_batch = per_replica_batch * jax.device_count() // cores_per_replica * 8\n    with jax.experimental.maps.mesh(devices, ('dp', 'mp')):\n        network = CausalTransformer(params)\n\n        start = time.time()\n        network.state = read_ckpt(network.state, f\"gs://{bucket}/{model_dir}/step_{ckpt_step}/\", devices.shape[1])\n        print(f\"network loaded in {time.time() - start:.06}s\")\n\n        local_shards = max(jax.local_device_count() // mesh_shape[1], 1)\n        del network.state[\"opt_state\"]\n        network.state = network.move_xmap(network.state, np.zeros(local_shards))\n\n        tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')\n\n        while True:\n            all_ctx = []\n            all_top_p = []\n            all_temp = []\n            all_q = []\n            while len(all_ctx) < total_batch:\n                try:\n                    o, q = requests_queue.get(block=False)\n                    all_ctx.append(o[\"context\"])\n                    all_top_p.append(o[\"top_p\"])\n                    all_temp.append(o[\"temp\"])\n                    all_q.append(q)\n                except Empty:\n                    if len(all_ctx):\n                        break\n                    else:\n                        time.sleep(0.01)\n\n            start = time.time()\n            while len(all_ctx) < total_batch:\n                all_ctx.append(\"whatever\")\n                all_top_p.append(1)\n                all_temp.append(1)\n\n            all_tokenized = []\n            all_length = []\n            for ctx in all_ctx:\n                padded_tokens = np.zeros(seq).astype(np.uint32)\n                length = 0\n\n                try:\n                    tokens = tokenizer.encode(ctx)\n                    provided_ctx = len(tokens)\n                    pad_amount = seq - provided_ctx\n\n                    pad_amount = max(pad_amount, 0)\n\n                    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)[-seq:]\n                    length = len(tokens)\n                except:\n                    print(\"oops exception\")\n\n                all_tokenized.append(padded_tokens)\n                all_length.append(length)\n\n            output = network.generate(np.array(all_tokenized),\n                                      np.array(all_length),\n                                      256,\n                                      {\n                                          \"top_p\": np.array(all_top_p),\n                                          \"temp\": np.array(all_temp)\n                                      })\n\n            for o, q in zip(output[1][0][:, :, 0], all_q):\n                q.put(tokenizer.decode(o))\n\n            print(f\"completion done in {time.time() - start:06}s\")\n"
        },
        {
          "name": "device_train.py",
          "type": "blob",
          "size": 14.373046875,
          "content": "import argparse\nimport json\nimport time\n\nimport jax\nimport numpy as np\nimport optax\n\nimport wandb\nfrom tqdm import tqdm\n\n\nfrom mesh_transformer import util\nfrom mesh_transformer.checkpoint import read_ckpt, write_ckpt\nfrom mesh_transformer.transformer_shard import CausalTransformer\nfrom tfrecord_loader import TFRecordNewInputs\nfrom smart_open import open\nfrom google.cloud import storage\nfrom google.cloud.exceptions import NotFound\n\nfrom mesh_transformer.util import clip_by_global_norm, additive_weight_decay\n\n\ndef parse_args():\n    # Parse command line arguments\n    parser = argparse.ArgumentParser(description=\"\"\"\n    To use, download the full checkpoint archive, extract and upload to a GCS bucket, and set that as --tune-model-path\n    Modify the config file:\n        - set `model_dir` to where the checkpoints should be written during training\n        - set `train_set`, `val_set` to index files for your data\n        - set `tpu_size` to 8 (if on a v3-8)\n        - set `warmup_steps`, `anneal_steps`, `lr`, `end_lr` to the lr schedule for your finetuning run\n        - the global step will reset to 0, keep that in mind when writing your lr schedule\n        - set `name` to specify the name of the Weights & Biases run\n        - set `wandb_project` to specify the Weights & Biases project to log to\n    To prepare data in the expected data format:\n        - use the script `create_finetune_tfrecords.py` in this repo to create data in the expected format\n        - upload the .tfrecords files to GCS\n        - save their GCS paths to a index file under `data/`, see existing files for examples\n    \"\"\",\n    formatter_class=argparse.RawTextHelpFormatter)\n    parser.add_argument(\"--config\", type=str, default=None, help=\"Config file location\")\n    parser.add_argument(\"--tune-model-path\", type=str, default=None, help=\"Base model to finetune\")\n    parser.add_argument(\"--fresh-opt\", default=False, action=\"store_true\", help=\"Use a newly initialized optimizer, ignoring any optimizer state saved in the base checkpoint\")\n\n    args = parser.parse_args()\n    return args\n\n\ndef save(network, step, bucket, path, mp, aux=None, keep_n=3, delete_old=True):\n    assert path\n    client = storage.Client()\n\n    if aux is None:\n        aux = {}\n\n    try:\n        with open(f\"gs://{bucket}/{path}/meta.json\", \"r\") as f:\n            meta = json.load(f)\n    except:\n        # create metadata file\n        with open(f\"gs://{bucket}/{path}/meta.json\", \"w\") as f:\n            json.dump({\n                \"step\": 0,\n                \"checkpoints\": [],\n                \"aux\": {}\n            }, f)\n\n    # do sharded checkpoint writing\n    start = time.time()\n    res = []\n    for shard_id in range(mp):\n        write_ckpt(network.state, f\"gs://{bucket}/{path}/step_{step}/\", shard_id)\n\n    print(f\"Wrote checkpoint in {time.time() - start:.06}s\")\n\n    with open(f\"gs://{bucket}/{path}/meta.json\", \"r\") as f:\n        meta = json.load(f)\n\n    meta[\"step\"] = step\n    meta[\"checkpoints\"].append(step)\n    all_aux = meta.get(\"aux\", {})\n\n    while len(meta[\"checkpoints\"]) > keep_n:\n        ckpt_to_delete = meta[\"checkpoints\"].pop(0)\n\n        try:\n            del all_aux[str(ckpt_to_delete)]\n        except:\n            print(f\"failed to delete the aux state for {step}\")\n\n        if delete_old:\n            print(f\"deleting checkpoint {ckpt_to_delete}\")\n            for blob in client.list_blobs(bucket, prefix=f\"{path}/step_{ckpt_to_delete}/\"):\n                # print(f\"deleting {blob.name}\")\n                assert path in blob.name\n                blob.delete()\n        else:\n            print(f\"keeping checkpoint {ckpt_to_delete}\")\n\n    all_aux[step] = aux\n    meta[\"aux\"] = all_aux\n\n    with open(f\"gs://{bucket}/{path}/meta.json\", \"w\") as f:\n        json.dump(meta, f)\n\n\ndef train_step(network, data):\n    inputs = {\n        \"obs\": data[:, :, :-1],\n        \"target\": data[:, :, 1:],\n    }\n\n    loss, last_loss, grad_norm, grad_norm_micro = network.train(inputs)\n\n    return (\n        np.array(loss).mean(),\n        np.array(last_loss).mean(),\n        np.array(grad_norm).mean(),\n        np.array(grad_norm_micro).mean(),\n    )\n\n\ndef eval_step(network, data):\n    inputs = {\n        \"obs\": data[:, :-1],\n        \"target\": data[:, 1:],\n    }\n\n    out = network.eval(inputs)\n    loss = out[\"loss\"]\n\n    return np.array(loss).mean()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    params = json.load(open(args.config))\n\n    gradient_accumulation_steps = params.get(\"gradient_accumulation_steps\", 1)\n    per_replica_batch = params[\"per_replica_batch\"]\n    cores_per_replica = params[\"cores_per_replica\"]\n\n    assert cores_per_replica <= 8\n\n    bucket = params[\"bucket\"]\n    model_dir = params[\"model_dir\"]\n    layers = params[\"layers\"]\n    d_model = params[\"d_model\"]\n    n_heads = params[\"n_heads\"]\n    n_vocab = params[\"n_vocab\"]\n    seq = params[\"seq\"]\n    norm = params[\"norm\"]\n\n    val_batches = params[\"val_batches\"]\n    val_every = params[\"val_every\"]\n    ckpt_every = params[\"ckpt_every\"]\n    keep_every = params[\"keep_every\"]\n    eval_tasks = params[\"eval_harness_tasks\"]\n    total_steps = params[\"total_steps\"]\n\n    pe = params[\"pe\"]\n    assert pe in [\"fixed\", \"rotary\", \"t5\"]\n\n    warmup_steps = params[\"warmup_steps\"]\n    anneal_steps = params[\"anneal_steps\"]\n    lr = params[\"lr\"]\n    end_lr = params[\"end_lr\"]\n    weight_decay = params[\"weight_decay\"]\n   \n    # alpha parameter for the exponential moving averages used to compute B_simple\n    noise_scale_alpha = params.get(\"noise_scale_alpha\", 0.01)\n\n    scheduler = util.gpt3_schedule(warmup_steps, anneal_steps, lr, end_lr)\n    \n    opt = optax.chain(\n        optax.scale(1 / gradient_accumulation_steps),\n        clip_by_global_norm(1),\n        optax.scale_by_adam(),\n        additive_weight_decay(weight_decay),\n        optax.scale(-1),\n        optax.scale_by_schedule(scheduler)\n    )\n\n    params[\"optimizer\"] = opt\n\n    start = time.time()\n    tpu_size = jax.device_count()\n    if tpu_size < cores_per_replica:\n        msg = f\"each shard needs a separate device, but device count ({tpu_size}) < shard count ({cores_per_replica})\"\n        raise ValueError(msg)\n    print(f\"jax devices: {tpu_size}\")\n    print(f\"jax runtime initialized in {time.time() - start:.06}s\")\n\n    mesh_shape = (tpu_size // cores_per_replica, cores_per_replica)\n    devices = np.array(jax.devices()).reshape(mesh_shape)\n\n    # pick initial ckpt - based on tuning vs train from scratch\n\n    step = 0\n    initial_ckpt_state_path = None\n    train_loader = None\n\n    if args.tune_model_path:\n        print('`--tune_model_path` passed: we are beginning a fine-tuning run')\n        fine_tuning = True\n        initial_ckpt_state_path = args.tune_model_path\n    else:\n        print('`--tune_model_path` not passed: we are continuing a fine-tuning run from a checkpoint (or we are not fine-tuning)')\n        fine_tuning = False\n        initial_ckpt_model_dir = model_dir\n        initial_ckpt_path = f\"gs://{bucket}/{initial_ckpt_model_dir}\"\n        meta_path = f\"{initial_ckpt_path}/meta.json\"\n\n        try:\n            with open(meta_path, \"r\") as f:\n                meta = json.load(f)\n            ckpt_step = meta[\"checkpoints\"][-1]\n            initial_ckpt_state_path = f\"{initial_ckpt_path}/step_{ckpt_step}/\"\n            print(f\"state will be restored from checkpoint {ckpt_step}\")\n\n            step = ckpt_step\n            train_loader = meta['aux'][str(ckpt_step)].get(\"train_loader\", None)\n        except NotFound:\n            # no checkpoint, start at zero\n            print(f\"No checkpoint to load at {initial_ckpt_path}. Training from scratch.\")\n\n    if initial_ckpt_state_path:\n        print(f\"path to load checkpoint from: {initial_ckpt_state_path}\")\n    else:\n        print(\"not loading from a checkpoint\")\n\n    # set up datasets\n    print(\"setting up datasets\")\n\n    train_dataset = TFRecordNewInputs(f\"data/{params['train_set']}\",\n                                      batch_size=(\n                                          gradient_accumulation_steps,\n                                          per_replica_batch * tpu_size // cores_per_replica),\n                                      sample_size=params['seq'],\n                                      restore_state=train_loader)\n\n    global_val_batch = per_replica_batch * tpu_size // cores_per_replica\n\n    val_sets = {}\n\n    for k, v in params[\"val_set\"].items():\n        val_sets[k] = TFRecordNewInputs(\n            f\"data/{v}\", batch_size=(global_val_batch,), sample_size=seq\n        )\n\n    # tok/sec metrics\n    sequences_per_step = gradient_accumulation_steps * (per_replica_batch * tpu_size // cores_per_replica)\n    tokens_per_step = params['seq'] * sequences_per_step\n\n    # load + run\n    with jax.experimental.maps.mesh(devices, ('dp', 'mp')):\n        print(\"initializing network\")\n        network = CausalTransformer(params)\n\n        if initial_ckpt_state_path:\n            print(\"loading network\")\n            if fine_tuning:\n                # get the scheduler step stored in the just-initialized optimizer\n                # should be zero\n                init_sched_state = network.state[\"opt_state\"][-1]\n\n            start = time.time()\n            network.state = read_ckpt(network.state, initial_ckpt_state_path, devices.shape[1], load_opt=(not args.fresh_opt))\n\n            if fine_tuning:\n                # overwrite the loaded scheduler step with zeros\n                # this makes fine-tuning use the lr schedule in\n                network.state[\"opt_state\"][-1] = init_sched_state\n\n            print(f\"network loaded in {time.time() - start:.06}s\")\n\n        print('compiling train fn')\n        start = time.time()\n        loss, last_loss, grad_norm, grad_norm_micro = train_step(\n            network, train_dataset.get_samples()\n        )\n        step += 1\n        print(f\"Train fn compiled in {time.time() - start:.06}s\")\n\n        print('compiling eval fn')\n        start = time.time()\n        for val_set in val_sets.values():\n            eval_step(network, val_set.get_samples())\n            val_set.reset()\n        print(f\"Eval fn compiled in {time.time() - start:.06}s\")\n\n        project = params.get(\"wandb_project\", \"mesh-transformer-jax\")\n        wandb.init(project=project, name=params[\"name\"], config=params)\n\n        G_noise_avg = None\n        S_noise_avg = None\n\n        while True:\n            if (step % ckpt_every == 1) or step == total_steps:\n                print(f\"saving a checkpoint for step {step}\")\n                save(network, step, bucket, model_dir,\n                     mp=cores_per_replica,\n                     aux={\"train_loader\": train_dataset.get_state()},\n                     delete_old=True,\n                     )\n\n            if step % val_every == 1:  # 1 because we've already taken a step to compile train fn\n                for name, val_set in val_sets.items():\n                    val_loss = []\n                    for i, _ in tqdm(zip(val_set.sample_once(), range(val_batches)),\n                                     desc=f\"validation for step {step}, set {name}\",\n                                     total=val_batches):\n                        val_loss.append(eval_step(network, i))\n                    val_set.reset()\n\n                    val_loss = np.array(val_loss).mean()\n                    print(f\"validation loss for step {step}, set {name}: {val_loss}\")\n\n                    wandb.log({f'val/loss_{name}': float(val_loss)}, step)\n\n            if step == total_steps:\n                print(\"training completed!\")\n                exit()\n\n            start = time.time()\n            loss, last_loss, grad_norm, grad_norm_micro = train_step(\n                network, train_dataset.get_samples()\n            )\n            step += 1\n\n            steps_per_sec = 1 / (time.time() - start)\n            tokens_per_sec = tokens_per_step * steps_per_sec\n            sequences_processed = sequences_per_step * step\n            tokens_processed = tokens_per_step * step\n\n            ### compute summary stats about the gradient\n\n            # converts from grads-summed-over-microbatch (what `CasualTransformer.train` computes)\n            # to grads-averaged-over-microbatch (what we want)\n            #\n            # (when taking gradient steps, the same conversion happens inside the optimizer\n            #  via optax.scale(1 / gradient_accumulation_steps))\n            grad_norm = grad_norm / gradient_accumulation_steps\n\n            # compute G_noise and S_noise\n            # from \"An Empirical Model of Large-Batch Training\" Appendix A.1\n            # here, B_big = gradient_accumulation_steps, and B_small = 1 for convenience\n            gbsmall = grad_norm_micro ** 2\n            gbbig = grad_norm ** 2\n            G_noise = (gradient_accumulation_steps * gbbig - gbsmall) / (\n                gradient_accumulation_steps - 1\n            )\n            S_noise = (gbsmall - gbbig) / (1 - 1 / gradient_accumulation_steps)\n\n            noise_scale_stats = {\n                \"noise/G_noise\": G_noise,\n                \"noise/S_noise\": S_noise,\n            }\n\n            # heuristic to avoid reporting G_noise in very early training when gradients are large\n            # (these take a long time to wash out of the moving average that defines B_simple)\n            use_step_in_noise_avgs = gbbig < 2\n\n            if use_step_in_noise_avgs:\n                # compute moving averages of G_noise and S_noise, for B_simple\n                if G_noise_avg is None:\n                    G_noise_avg = G_noise\n                else:\n                    G_noise_avg = (1 - noise_scale_alpha) * G_noise_avg + noise_scale_alpha * G_noise\n\n                if S_noise_avg is None:\n                    S_noise_avg = S_noise\n                else:\n                    S_noise_avg = (1 - noise_scale_alpha) * S_noise_avg + noise_scale_alpha * S_noise\n\n                B_simple = S_noise_avg / G_noise_avg\n\n                noise_scale_stats.update(\n                    {\n                        \"noise/G_noise_avg\": G_noise_avg,\n                        \"noise/S_noise_avg\": S_noise_avg,\n                        \"noise/B_simple\": B_simple,\n                    }\n                )\n\n            wandb_stats = {\n                \"train/loss\": loss,\n                \"train/last_loss\": last_loss,\n                \"train/steps_per_sec\": steps_per_sec,\n                \"train/tokens_per_sec\": tokens_per_sec,\n                \"train/grad_norm\": grad_norm,\n                \"train/learning_rate\": float(scheduler(network.state[\"opt_state\"][-1].count[0].item())),\n                \"sequences_processed\": sequences_processed,\n                \"tokens_processed\": tokens_processed,\n            }\n            wandb_stats.update(noise_scale_stats)\n\n            wandb.log(wandb_stats, step)\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval_harness.py",
          "type": "blob",
          "size": 2.806640625,
          "content": "import argparse\nimport json\n\nfrom lm_eval import evaluator, tasks\n\nfrom mesh_transformer.build_model import build_model\nfrom tasks import EvalHarnessAdaptor\n\n\ndef parse_args():\n    # Parse command line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--tpu\", type=str, help=\"Name of TPU to train on.\")\n    parser.add_argument(\"--tpu_region\", type=str, help=\"Region of TPU to train on.\")\n    parser.add_argument(\"--preemptible\", action=\"store_true\")\n\n    parser.add_argument(\"--config\", type=str, default=None, help=\"Config file location\")\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    params = json.load(open(args.config))\n\n    tpu_name = args.tpu\n    region = args.tpu_region\n    preemptible = args.preemptible\n\n    gradient_accumulation_steps = params.get(\"gradient_accumulation_steps\", 1)\n    per_replica_batch = params[\"per_replica_batch\"]\n    tpu_size = params[\"tpu_size\"]\n    cores_per_replica = params[\"cores_per_replica\"]\n\n    bucket = params[\"bucket\"]\n    model_dir = params[\"model_dir\"]\n    layers = params[\"layers\"]\n    d_model = params[\"d_model\"]\n    n_heads = params[\"n_heads\"]\n    n_vocab = params[\"n_vocab\"]\n    seq = params[\"seq\"]\n    norm = params[\"norm\"]\n    pe = params[\"pe\"]\n\n    total_batch = per_replica_batch * tpu_size // cores_per_replica * 4\n\n    t = build_model(params, tpu_name, region, preemptible)\n    adaptor = EvalHarnessAdaptor(t, seq, total_batch, shrink=pe != \"fixed\")\n\n    step, aux = t.load(bucket, model_dir)\n    t.move()\n\n    results = evaluator.evaluate(adaptor, tasks.get_task_dict([\"lambada\",\n                                                               \"piqa\",\n                                                               \"hellaswag\",\n                                                               \"winogrande\",\n                                                               \"mathqa\",\n                                                               \"pubmedqa\",\n                                                               # \"boolq\",\n                                                               # \"cb\",\n                                                               # \"copa\",\n                                                               # \"multirc\",\n                                                               # \"record\",\n                                                               # \"wic\",\n                                                               # \"wsc\",\n                                                               ]), False, 0, None)\n    dumped = json.dumps(results, indent=2)\n    print(dumped)\n\n    results = evaluator.evaluate(adaptor, tasks.get_task_dict([\"lambada_cloze\",\n                                                               ]), False, 15, None)\n\n    dumped = json.dumps(results, indent=2)\n    print(dumped)"
        },
        {
          "name": "howto_finetune.md",
          "type": "blob",
          "size": 7.91796875,
          "content": "# How to Fine-Tune GPT-J - The Basics\n\nBefore anything else, you'll likely want to apply for access to the TPU Research Cloud (TRC). Combined with a Google Cloud free trial, that should allow you to do everything here for free. Once you're in TRC, you need to create a project, then with the name of the new project fill out the form that was emailed to you. Use the script `create_finetune_tfrecords.py` to prepare your data as tfrecords; I might do a separate guide on that. Another thing you might want to do is fork the mesh-transformer-jax repo to make it easier to add and modify the config files.\n\n0. [Install the Google Cloud SDK](https://cloud.google.com/sdk/docs/install). We'll need it later.\n\n1. If you didn't make a project and activate TPU access through TRC yet (or if you plan on paying out of pocket), [make one now](https://console.cloud.google.com/projectcreate).\n\n2. TPUs use Google Cloud buckets for storage, go ahead and [create one now](https://console.cloud.google.com/storage/create-bucket). Make sure it's in the region the TPU VM will be; the email from TRC will tell you which region(s) you can use free TPUs in.\n\n3. You'll need the full pretrained weights in order to fine-tune the model. [Download those here](https://mystic.the-eye.eu/public/AI/GPT-J-6B/step_383500.tar.zstd).\n\nNow that you have a bucket on the cloud and the weights on your PC, you need to upload the weights to the bucket in two steps:\n\n4. Decompress and extract `GPT-J-6B/step_383500.tar.zstd` so you're left with the uncompressed folder containing the sharded checkpoint.\n\n5. Open the Google Cloud SDK and run the following command, replacing the path names as appropriate: `gsutil -m cp -R LOCAL_PATH_TO/step_383500 gs://YOUR-BUCKET`. If that works, the console will show the files being uploaded. *Note: Took about 12 hours for me, uploading to the Netherlands from California; hopefully you'll have a better geographic situation than I did! I also initially made the mistake of uploading the still-packed .tar. Don't do that, TPU VMs don't have enough local storage for you to unpack it. To avoid needing to re-upload, I had to unpack it in Colab.*\n\nYou'll want to upload tfrecords of your data as well, you can do that here or through the web interface, but trust me when I say you don't want to upload the nearly 70GB weights through the web interface.\n\nNote that steps 6 and 7, preparing the index and config files, can be done later on by editing the base repo in the VM's text editor. It's more efficient to instead make these changes to your own fork of the repo as follows:\n\n6. In the data folder, create a new file `foo.train.index`, replace foo with whatever you want to refer to your dataset as. For each tfrecord in your bucket that you intend to train with, add the path as a line in the index. Make `foo.val.index` and do the same for your validation dataset (if you have one). See the existing files for examples.\n\n7. Duplicate the config file `6B_roto_256.json`, rename it to something appropriate for your project. Open it up and make these edits:\n   - `tpu_size`: Change from `256` to `8`\n   - `bucket`: Change to your bucket\n   - `model_dir`: Change to the directory you'd like to save your checkpoints in\n   - `train_set` and `val_set`: Change to the index files from the last step\n   - `eval_harness_tasks`: Can be removed if you don't plan on using the eval harness\n   -  `val_every` & `ckpt_every` & `keep_every`: Usage should be intuitive. Don't set the `foo_every` values to 0 though or you'll get a divide by zero error. If you don't have a `val_set`, just set `val_every` to something higher than `total_steps`.\n   - `val_batches`: This should equal the number of sequences in your val dataset. You can find this number at the end of the .tfrecords file produced by `create_finetune_tfrecords.py`\n   - `name`: Change to a name for your model.\n   - `warmup_steps`, `lr`, `val_batches`, etc.: see the *Learning Rate Notes* section at the end of the guide.\n\n\n8. Push the changes to your GitHub repo.\n\n9. Follow [this guide](https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm) up to and including the step **\"Connect to your Cloud TPU VM\"**.\n\nAt this point you should have remote access to the TPU VM!\n\n10. In the new VM terminal, type `git clone https://github.com/kingoflolz/mesh-transformer-jax` (or, preferably, your own fork, after pushing the config and index files)\n\n11. Move to the new directory with `cd mesh-transformer-jax` and run `pip install -r requirements.txt`. Since the requirements.txt file doesn't pin the exact jax version required for finetuning, run `pip install jax==0.2.12` and you'll be all set.\n\n12. Finally, run `python3 device_train.py --config=YOUR_CONFIG.json --tune-model-path=gs://YOUR-BUCKET/step_383500/`. If everything is set up correctly this will begin the fine-tuning process. First the model has to be loaded into memory; when `loading network` displayed on the console it took about 10-15 minutes before the next step, setting up WandB for logging. Option 3 allows you to skip that if you aren't using WandB. A step 1 checkpoint will save, and the real training will start. If you have a small dataset, this will go by quickly; TPU VMs can train at a rate of ~5000 tokens/second.\n\n13. You did it! Now don't forget any clean up steps you need to take like shutting down your TPU VM or removing unneeded data in buckets, so that you don't have any unexpected charges from Google later.\n\n## Now what?\n\nThis guide is labeled \"The Basics\", anything we haven't covered so far is out of scope, but go check out the rest of the repository! Try `python3 device_sample.py --config=configs/YOUR_CONFIG.json` for a basic sampling interface. Use `slim_model.py` to prepare an easier-to-deploy slim version of your new weights for inference. Experiment!\n\n### Running with HuggingFace\nTo use the model in HuggingFace's `transformer` library using pytorch, you'll need to transfer the weights\ninto a format that it recognizes. This can be done using `to_hf_weights.py`. It's recommended that you use `slim_model.py` before attempting to move the weights to a pytorch/transformer format. Use `python to_hf_weights.py --help` to see usage details.\n\n*note: as of 9/1/2021, GPT-J has been merged into the `main` branch of `transformers` but has not yet been put into production. Run `pip install git+https://github.com/huggingface/transformers#transformers` to install the current `main` branch.\n\n## Learning Rate Notes\n\n**Thanks to nostalgebraist for talking about this!** They're the one who explained this part on Discord, I'm just paraphrasing really:\n\nThe first thing you want to determine is how long a training epoch will be. `gradient_accumulation_steps` is your batch size, it defaults to `16`, nostalgebraist recommends `32`. Your .tfrecord files should have a number in the file name indicating how many sequences are in the dataset. Divide that number by the batch size and the result is how many steps are in an epoch. Now we can write the schedule.\n\n`lr` is recommended to be between `1e-5` and `5e-5`, with `end_lr` set to 1/5 or 1/10 of `lr`.\n`weight_decay` can remain `0.1`. `total_steps` should be at least one epoch, possibly longer if you have a validation\nset to determine your training loss with.\n`warmup_steps` should be 5-10% of total, and finally `anneal_steps` should be `total_steps - warmup_steps`.\n(The `lr` is set to `end_lr` after `warmup_steps+anneal_steps` and then keeps training until `total_steps`,\nbut usually you should stop after annealing is done)\n\nTo illustrate: I have a small dataset that tokenized into 1147 sequences as a .tfrecord. Dividing by `gradient_accumulation_steps` set to `16`, rounding up to ensure I use all the data, equals 72 steps per epoch. I'll set `lr` to `5e-5`, `end_lr` to a fifth of that, `1e-5`; that may be too much, it's on the high end of the recommended range. I'll set `total_steps` to `72` for one epoch, since I don't have a validation set. Then I'll set `anneal_steps` to `65` and `warmup_steps` to `7`. Simple as that, but you may need to fiddle with the specifics on your own.\n"
        },
        {
          "name": "mesh_transformer",
          "type": "tree",
          "content": null
        },
        {
          "name": "ray_tpu.py",
          "type": "blob",
          "size": 3.9462890625,
          "content": "import functools\nimport os\nimport subprocess\nimport time\n\nimport glob\nimport requests\nfrom fabric import Connection\n\n\n@functools.lru_cache()\ndef get_bearer():\n    return subprocess.check_output(\"gcloud auth print-access-token\", shell=True).decode(\"utf-8\").strip()\n\n\n@functools.lru_cache()\ndef get_project():\n    return subprocess.check_output(\"gcloud config list --format 'value(core.project)'\", shell=True).decode(\n        \"utf-8\").strip()\n\n\ndef create_tpu(\n        name,\n        zone,\n        type,\n        preemptible,\n):\n    headers = {\n        'Authorization': f'Bearer {get_bearer()}',\n        'Content-Type': 'application/json',\n    }\n\n    try:\n        status = check_tpu(name, zone)\n\n        if status[\"state\"] not in [\"CREATING\", \"READY\"]:\n            print(\"deleting TPU\")\n            delete_tpu(name, zone)\n\n            while True:\n                try:\n                    print(\"deleting check\")\n                    print(check_tpu(name, zone)[\"state\"])\n\n                    time.sleep(1)\n                except:\n                    break\n    except:\n        pass\n\n    params = (\n        ('node_id', name),\n    )\n\n    data = {\"accelerator_type\":\n                type,\n            \"runtime_version\":\n                'v2-alpha',\n            \"network_config\":\n                {\"enable_external_ips\": True},\n            }\n\n    if preemptible:\n        data[\"schedulingConfig\"] = {\"preemptible\": True}\n\n    response = requests.post(f'https://tpu.googleapis.com/v2alpha1/projects/{get_project()}/locations/{zone}/nodes',\n                             headers=headers, params=params, json=data)\n\n    print(response.json())\n\n    return response.status_code == 200\n\n\ndef check_tpu(name, zone):\n    headers = {\n        'Authorization': f'Bearer {get_bearer()}',\n    }\n\n    response = requests.get(\n        f'https://tpu.googleapis.com/v2alpha1/projects/{get_project()}/locations/{zone}/nodes/{name}',\n        headers=headers)\n\n    return response.json()\n\n\ndef delete_tpu(name, zone):\n    headers = {\n        'Authorization': f'Bearer {get_bearer()}',\n    }\n\n    response = requests.delete(\n        f'https://tpu.googleapis.com/v2alpha1/projects/{get_project()}/locations/{zone}/nodes/{name}',\n        headers=headers)\n\n    return response.json()\n\n\ndef wait_til(name, zone, state):\n    while True:\n        ret = check_tpu(name, zone)\n\n        print(\"wait_til check\")\n        print(ret)\n\n        matches = True\n        for k, expected_v in state.items():\n            if k not in ret:\n                matches = False\n                continue\n            if ret[k] != expected_v:\n                matches = False\n\n        if \"error\" in ret:\n            return False\n\n        if ret[\"state\"] == \"TERMINATED\":\n            return False\n\n        if matches:\n            return True\n\n        time.sleep(1)\n\n\ndef get_connection(\n        name,\n        zone,\n):\n    info = check_tpu(name, zone)\n    outputs = []\n    for i in info[\"networkEndpoints\"]:\n        outputs.append(Connection(i[\"ipAddress\"],\n                                  connect_kwargs={\n                                      \"key_filename\": os.path.expanduser('~/.ssh/google_compute_engine'), }))\n    return outputs\n\n\ndef start_ray(conn, address, version=1):\n    conn.sudo('rm -rf *.py')\n    conn.sudo('rm -rf mesh_transformer')\n\n    for i in glob.glob(\"*.py\"):\n        conn.put(i, \"\")\n\n    conn.run(\"mkdir mesh_transformer -p\")\n\n    for i in glob.glob(\"mesh_transformer/*.py\"):\n        conn.put(i, \"mesh_transformer/\")\n\n    conn.sudo('python3 setup.py install', hide=True)\n\n    if version == 2:\n        conn.put(\"scripts/init_ray_v2.sh\", \"/tmp/ray-tpu.sh\")\n    else:\n        conn.put(\"scripts/init_ray.sh\", \"/tmp/ray-tpu.sh\")\n    conn.sudo('chmod +x /tmp/ray-tpu.sh', hide=True)\n    conn.sudo('/tmp/ray-tpu.sh', hide=True)\n    try:\n        conn.run('ray stop -f', hide=True)\n    except:\n        pass\n\n    time.sleep(1)\n\n    conn.run(f\"TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD={32 * 1024**3} ray start --address={address} --resources='\" + '{\"tpu\": 1}\\' --include-dashboard False', hide=True)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.361328125,
          "content": "numpy~=1.19.5\ntqdm>=4.45.0\nwandb>=0.11.2\neinops~=0.3.0\nrequests~=2.25.1\nfabric~=2.6.0\noptax==0.0.9\ndm-haiku==0.0.5\ngit+https://github.com/EleutherAI/lm-evaluation-harness/\nray[default]==1.4.1\njax~=0.2.12\nFlask~=1.1.2\ncloudpickle~=1.3.0\ntensorflow-cpu~=2.6.0\ngoogle-cloud-storage~=1.36.2\ntransformers\nsmart_open[gcs]\nfunc_timeout\nftfy\nfastapi\nuvicorn\nlm_dataformat\npathy\n"
        },
        {
          "name": "resharding_example.py",
          "type": "blob",
          "size": 2.4892578125,
          "content": "# This was tested with an RTX 3090, peak memory usage is approximately 22.4GB during inference, and 19GB when loading the model\n# The following environment variables were also used: XLA_PYTHON_CLIENT_PREALLOCATE=false XLA_PYTHON_CLIENT_ALLOCATOR=platform\n\nimport time\n\nimport jax\nfrom jax.experimental import maps\nimport numpy as np\nimport optax\nimport transformers\n\nfrom mesh_transformer.checkpoint import read_ckpt\nfrom mesh_transformer.sampling import nucleaus_sample\nfrom mesh_transformer.transformer_shard import CausalTransformer\n\nparams = {\n  \"layers\": 28,\n  \"d_model\": 4096,\n  \"n_heads\": 16,\n  \"n_vocab\": 50400,\n  \"norm\": \"layernorm\",\n  \"pe\": \"rotary\",\n  \"pe_rotary_dims\": 64,\n  \"early_cast\": True,\n  \"seq\": 2048,\n  \"cores_per_replica\": 1,  # only running on one GPU\n  \"per_replica_batch\": 1,\n}\n\nper_replica_batch = params[\"per_replica_batch\"]\ncores_per_replica = params[\"cores_per_replica\"]\nseq = params[\"seq\"]\n\n\nparams[\"sampler\"] = nucleaus_sample\n\n# here we \"remove\" the optimizer parameters from the model (as we don't need them for inference)\nparams[\"optimizer\"] = optax.scale(0)\n\ndevices = np.array([jax.devices()[0]]).reshape((1, 1))\nmaps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))\n\ntokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')\n\nnetwork = CausalTransformer(params)\n\nstart = time.time()\n\n# here we load a checkpoint which was written with 8 shards into 1 shard\nnetwork.state = read_ckpt(network.state, \"step_383500/\", 8, shards_out=cores_per_replica)\n\n# move the state to CPU/system memory so it's not duplicated by xmap\nnetwork.state = jax.device_put(network.state, jax.devices(\"cpu\")[0])\n\ndef infer(context, top_k=40, top_p=0.9, temp=1.0, gen_len=512):\n    tokens = tokenizer.encode(context)\n\n    provided_ctx = len(tokens)\n    pad_amount = seq - provided_ctx\n\n    padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\n    batched_tokens = np.array([padded_tokens] * per_replica_batch)\n    length = np.ones(per_replica_batch, dtype=np.uint32) * len(tokens)\n\n    start = time.time()\n    output = network.generate(batched_tokens, length, gen_len, {\"top_p\": np.ones(per_replica_batch) * top_p, \"top_k\": top_k is not None and (np.ones(per_replica_batch, dtype=np.int32) * top_k) or None, \"temp\": np.ones(per_replica_batch) * temp})\n\n    samples = []\n    decoded_tokens = output[1][0]\n\n    for o in decoded_tokens[:, :, 0]:\n      samples.append(tokenizer.decode(o))\n\n    print(f\"completion done in {time.time() - start:06}s\")\n    return samples\n\n\ninfer(\"EleutherAI is\")\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.1787109375,
          "content": "from setuptools import setup, find_packages\n\nsetup(\n    name='mesh_transformer',\n    version='0.0.0',\n    packages=find_packages(include=['mesh_transformer', 'mesh_transformer.*'])\n)\n"
        },
        {
          "name": "slim_model.py",
          "type": "blob",
          "size": 2.64453125,
          "content": "import argparse\nimport json\nimport time\n\nimport jax\nimport numpy as np\nimport optax\n\nfrom mesh_transformer import util\nfrom mesh_transformer.checkpoint import read_ckpt, write_ckpt\nfrom mesh_transformer.transformer_shard import CausalTransformer\nfrom smart_open import open\n\nfrom mesh_transformer.util import clip_by_global_norm, to_bf16, to_f16\n\n\ndef parse_args():\n    # Parse command line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, default=None, help=\"Config file location\")\n    parser.add_argument(\"--ckpt-step\", type=int, default=-1, help=\"Step number of the checkpoint to convert (if not specified, converts the most recent checkpoint)\")\n    parser.add_argument(\"--f16\", default=False, action=\"store_true\", help=\"Convert to float16 (instead of bfloat16)\")\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    params = json.load(open(args.config))\n    convert_fn = to_f16 if args.f16 else to_bf16\n\n    cores_per_replica = params[\"cores_per_replica\"]\n\n    assert cores_per_replica <= 8\n\n    bucket = params[\"bucket\"]\n    model_dir = params[\"model_dir\"]\n\n    params[\"optimizer\"] = optax.chain(\n        optax.scale(1),\n        clip_by_global_norm(1),\n        optax.scale_by_adam(),\n        optax.additive_weight_decay(0),\n        optax.scale(-1),\n        optax.scale_by_schedule(util.gpt3_schedule(0, 1, 0, 0))\n    )\n\n    start = time.time()\n    print(f\"jax devices: {jax.device_count()}\")\n    print(f\"jax runtime initialized in {time.time() - start:.06}s\")\n\n    mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\n    devices = np.array(jax.devices()).reshape(mesh_shape)\n\n    with open(f\"gs://{bucket}/{model_dir}/meta.json\", \"r\") as f:\n        meta = json.load(f)\n\n    if args.ckpt_step > -1:\n        ckpt_step = args.ckpt_step\n    else:\n        ckpt_step = meta[\"checkpoints\"][-1]\n    print(f\"using checkpoint {ckpt_step}\")\n\n    with jax.experimental.maps.mesh(devices, ('dp', 'mp')):\n        network = CausalTransformer(params)\n\n        start = time.time()\n        network.state = read_ckpt(network.state, f\"gs://{bucket}/{model_dir}/step_{ckpt_step}/\", devices.shape[1])\n        print(f\"network loaded in {time.time() - start:.06}s\")\n\n        start = time.time()\n        del network.state[\"opt_state\"]\n\n        network.state[\"params\"] = convert_fn(network.state[\"params\"])\n        print(f\"network converted in {time.time() - start:.06}s\")\n\n        suffix = \"_slim_f16\" if args.f16 else \"_slim\"\n\n        for i in range(cores_per_replica):\n            write_ckpt(network.state, f\"gs://{bucket}/{model_dir}{suffix}/step_{ckpt_step}/\", i)\n            print(f\"written shard {i}\")\n"
        },
        {
          "name": "tasks",
          "type": "tree",
          "content": null
        },
        {
          "name": "tfrecord_loader.py",
          "type": "blob",
          "size": 5.7216796875,
          "content": "import jax\nimport tensorflow as tf\nimport numpy as np\nfrom transformers import GPT2TokenizerFast\nimport itertools\n\n\nclass TFRecordLoader:\n    def __init__(self, index_fname, batch_size, parse_fn, map_fn=None, restore_state=None):\n        if restore_state is not None:\n            self.file_idx = restore_state[\"file_idx\"]\n            self.file_idx_init = False\n            self.used = restore_state[\"used\"]\n        else:\n            self.file_idx = 0\n            self.file_idx_init = True\n            self.used = []\n\n        self.index = open(index_fname).read().splitlines()\n        self.clean_index = list(filter(lambda x: x not in self.used, self.index))\n        self.bs = batch_size\n        # self.seq = sample_size\n        self.parse_fn = parse_fn\n\n        if map_fn:\n            self.map_fn = map_fn\n        else:\n            self.map_fn = lambda x: x\n\n        self.sample_fn = self.sample_once()\n\n    def reset(self):\n        self.file_idx = 0\n        self.file_idx_init = True\n        self.used = []\n\n        self.clean_index = list(filter(lambda x: x not in self.used, self.index))\n        self.sample_fn = self.sample_once()\n\n    def sample_once(self):\n        for i in self.clean_index:\n            compression = \"ZLIB\" if \"zstd\" in i else \"\"\n\n            file = tf.data.TFRecordDataset(i, compression_type=compression).map(self.parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n            file = file.apply(tf.data.experimental.dense_to_ragged_batch(np.prod(self.bs), drop_remainder=True))\n            file = file.prefetch(10)\n\n            for file_idx, data in enumerate(file):\n                data = jax.tree_map(lambda x: x.numpy(), data)\n                data = self.map_fn(data)\n\n                if not self.file_idx_init and file_idx <= self.file_idx:\n                    if file_idx % 1000 == 0:\n                        print(f\"skipping to batch {self.file_idx}, currently at {file_idx}\")\n                    continue\n                self.file_idx_init = True\n                self.file_idx = file_idx\n                yield jax.tree_map(lambda x: x.reshape(self.bs + x.shape[1:]), data)\n            self.used.append(i)\n            self.file_idx = 0\n\n    # this loops infinitely, use .sample_once to get an iterator for validation\n    def get_samples(self):\n        try:\n            return next(self.sample_fn)\n        except StopIteration:\n            self.reset()\n            return self.get_samples()\n\n    def get_state(self):\n        return {\n            \"used\": self.used,\n            \"file_idx\": self.file_idx\n        }\n\n\nclass TFRecordNewInputs(TFRecordLoader):\n    def __init__(self, index_fname, batch_size, sample_size, restore_state=None):\n        def tf_parse(example_proto):\n            features = {\n                \"text\": tf.io.VarLenFeature(tf.int64)\n            }\n            parsed_features = tf.io.parse_single_example(example_proto, features)\n\n            return tf.cast(tf.sparse.to_dense(tf.sparse.reorder(parsed_features[\"text\"])), tf.uint32)\n\n        super().__init__(index_fname, batch_size, tf_parse, restore_state=restore_state)\n\n\nclass TFRecordWIT(TFRecordLoader):\n    def __init__(self, index_fname, batch_size, restore_state=None, text_tokens=256):\n        self.tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n        self.tokenizer.pad_token = \"<|endoftext|>\"\n        self.tokenizer.add_special_tokens({'sep_token': '<|sep|>', 'pad_token': '<|pad|>'})\n\n        def map_fn(example):\n            tokenizer = self.tokenizer\n\n            def decode(x):\n                return tokenizer([\"<|endoftext|>\" + i.decode() for i in x])[\"input_ids\"]\n\n            texts = [\n                decode(example[\"context_page_description\"]),\n                decode(example[\"context_section_description\"]),\n                decode(example[\"caption_reference_description\"]),\n                decode(example[\"caption_alt_text_description\"]),\n                decode(example[\"caption_attribution_description\"]),\n            ]\n\n            output = []\n\n            for text, dalle in zip(zip(*texts), example[\"dalle\"]):\n                all_text = list(itertools.chain(*text))[-text_tokens+1:]\n\n                all_text += [tokenizer.pad_token_id] * ((text_tokens - 1) - len(all_text))\n\n                assert len(all_text) == text_tokens - 1\n\n                all_tokens = all_text + [tokenizer.sep_token_id] + list(dalle + tokenizer.vocab_size + 1)\n                output.append(all_tokens)\n\n            return np.array(output)\n\n        def tf_parse(example_proto):\n            features = {\n                \"page_title\": tf.io.FixedLenFeature([], tf.string),\n                \"section_title\": tf.io.FixedLenFeature([], tf.string),\n                \"hierarchical_section_title\": tf.io.FixedLenFeature([], tf.string),\n                \"caption_reference_description\": tf.io.FixedLenFeature([], tf.string),\n                \"caption_attribution_description\": tf.io.FixedLenFeature([], tf.string),\n                \"caption_alt_text_description\": tf.io.FixedLenFeature([], tf.string),\n                \"mime_type\": tf.io.FixedLenFeature([], tf.string),\n                \"context_page_description\": tf.io.FixedLenFeature([], tf.string),\n                \"context_section_description\": tf.io.FixedLenFeature([], tf.string),\n\n                \"dalle\": tf.io.FixedLenFeature([1024], tf.int64),\n            }\n\n            parsed_features = tf.io.parse_single_example(example_proto, features)\n\n            return parsed_features\n\n        super().__init__(index_fname, batch_size, tf_parse, map_fn, restore_state=restore_state)\n\n\nif __name__ == \"__main__\":\n    # d = TFRecordNewInputs(\"data/pile.val.index\", (8, 32), 2048)\n    # for idx, i in enumerate(d.sample_once()):\n    #     print(i)\n    #     break\n\n    d = TFRecordWIT(\"data/wit_dalle.train.index\", (8, 32))\n    for idx, i in enumerate(d.sample_once()):\n        print(i)\n        break\n\n    print()\n"
        },
        {
          "name": "to_hf_weights.py",
          "type": "blob",
          "size": 16.0654296875,
          "content": "####\n# python to_hf_weights.py --input-ckpt ./step_383500 --config ./configs/6B_roto_256.json --output-path ./gpt-j-6B --cpu --dtype fp32\n####\n\nimport argparse\nimport io\nimport multiprocessing\nimport time\nimport warnings\nimport os\nimport re\nfrom typing import Iterable, List, Union\nimport json\n\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental import maps\nfrom pathy import FluidPath, Pathy\nimport numpy as np\nimport optax\nimport torch\n\nfrom tqdm import tqdm\n\nfrom mesh_transformer.transformer_shard import CausalTransformer\n\n# xla: tell jax to not pre allocate all device memory\n# and only allocate memory as needed.\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\nos.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n\nDEBUG = False\n\nparser = argparse.ArgumentParser(\n    description=(\n        \"Used to turn a sharded trained gpt-j checkpoint into pytorch hugging face format.\"\n        \"This script works best on a slimmed checkpoint (full checkpoints can be used but require ~100gb of ram).\"\n        \"Currently, weights must be split into 8 shards for this to work.\"\n        \"All paths can be local or google cloud storage paths. S3 paths supported as well with `pip install pathy[s3]`.\"\n        \"Can be run on tpu, or on gpu with `pip install --upgrade jax==0.2.12 jaxlib==0.1.67+cuda110 -f https://storage.googleapis.com/jax-releases/jax_releases.html`\"\n    )\n)\nparser.add_argument(\n    \"--input-ckpt\",\n    type=str,\n    required=True,\n    help='path to model checkpoint folder. Google storage can be used with \"gs://bucket/path/step_{n}\" format.',\n    metavar=\"path\",\n)\nparser.add_argument(\n    \"--config\", type=str, required=True, help=\"Config file location\", metavar=\"path\"\n)\nparser.add_argument(\n    \"--output-path\",\n    required=True,\n    type=str,\n    help='Full path to save checkpoint to. Google storage can be used with \"gs://bucket/path\" format.',\n)\nparser.add_argument(\n    \"--debug\",\n    action=\"store_true\",\n    help=\"Verbose printing.\",\n)\nparser.add_argument(\n    \"--cpu\",\n    action=\"store_true\",\n    help=\"Run resharding on cpu instead of searching for jax device (i.e. gpu/tpu). Will default to cpu if jax wasn't installed with `+cuda110` option\",\n)\nparser.add_argument(\n    \"--dtype\",\n    type=str,\n    default=\"fp16\",\n    help=\"One of fp32, fp16 or bf16. Default=fp16. WARNING: Experimental. Make sure to check weights after conversion to make sure dtype information is retained.\",\n)\n\n\ndef process_args(\n    input_ckpt: Union[FluidPath, str],\n    config: Union[FluidPath, str],\n    output_path: Union[FluidPath, str],\n    dtype: str = \"fp16\",\n    cpu: bool = False,\n    **kwargs,\n):\n    # validate paths and turn them into Pathy paths.\n    input_ckpt = Pathy.fluid(str(input_ckpt))\n    assert input_ckpt.is_dir(), f'no such directory \"{input_ckpt}\"'\n    config = Pathy.fluid(str(config))\n    assert config.is_file(), f'no such file \"{config}\"'\n    first_shard = input_ckpt / \"shard_0\"\n    assert first_shard.is_dir(), f'no shards found at \"{input_ckpt}\"'\n\n    output_path = Pathy.fluid(str(output_path))\n    output_path.mkdir(exist_ok=True)\n\n    # make sure dtype is valid\n    assert dtype in {\"fp16\", \"fp32\", \"bf16\"}\n    np_dtype = np.float16\n    torch_dtype = torch.float16\n    if dtype != \"fp16\":\n        warnings.warn(\n            \"WARNING: Dtype support other than fp16 is Experimental. Make sure to check weights after conversion to make sure dtype information is retained.\"\n        )\n        if dtype == \"bf16\":\n            # np doesn't have bfloat16 so float32 is used to retain information before converting to torch.\n            np_dtype = np.float32\n            torch_dtype = torch.bfloat16\n        elif dtype == \"fp32\":\n            np_dtype = np.float32\n            torch_dtype = torch.float32\n\n    # tell jax to run on cpu instead of gpu/tpu\n    if cpu:\n        jax.config.update(\"jax_platform_name\", \"cpu\")\n\n    return input_ckpt, config, output_path, np_dtype, torch_dtype\n\n\ndef tree_flatten_with_names(pytree, is_leaf, path=\"\", to_id=id):\n    id_to_name = {}\n    if getattr(pytree, \"items\", None):\n        for k, v in pytree.items():\n            k_path = f\"{path}/{k}\"\n            if is_leaf(v):\n                id_to_name[to_id(v)] = k_path\n            else:\n                id_to_name = {\n                    **id_to_name,\n                    **tree_flatten_with_names(v, is_leaf=is_leaf, path=k_path),\n                }\n    elif getattr(pytree, \"__getitem__\", None):\n        for v in pytree:\n            if is_leaf(v):\n                id_to_name[to_id(v)] = path\n            else:\n                id_to_name = {\n                    **id_to_name,\n                    **tree_flatten_with_names(v, is_leaf=is_leaf, path=path),\n                }\n    else:\n        id_to_name[to_id(pytree)] = path\n    return id_to_name\n\n\ndef tree_leaves_with_names(pytree, to_id=id):\n    leaves = jax.tree_leaves(pytree)\n    is_leaf = lambda x: not isinstance(x, list) and to_id(x) in [\n        to_id(x) for x in leaves\n    ]\n    return tree_flatten_with_names(pytree, is_leaf)\n\n\ndef get_tree_leaves_names_reduced(pytree) -> List[str]:\n\n    leaves_ids = tree_leaves_with_names(pytree, to_id=id)\n    leaves = jax.tree_leaves(pytree)\n    return [leaves_ids[id(l)] for l in leaves]\n\n\nlayer_2_hf_inner_module_id = {\n    \"linear\": \"attn.q_proj\",\n    \"linear_1\": \"attn.v_proj\",\n    \"linear_2\": \"attn.k_proj\",\n    \"linear_3\": \"attn.out_proj\",\n    \"linear_4\": \"mlp.fc_in\",\n    \"linear_5\": \"mlp.fc_out\",\n    \"replicated_layer_norm\": \"ln_1\",\n}\n\nprojection_layer_2_hf_id_start = {\n    \"linear\": \"lm_head\",\n    \"replicated_layer_norm\": \"transformer.ln_f\",\n}\n\n\ndef leave_name_to_hf_layer_id(leaf_name: str):\n    if not leaf_name.startswith(\"/params\"):\n        if leaf_name == \"/step\":\n            return None\n        else:\n            raise NotImplementedError(f\"Unknown leaf name: {leaf_name}\")\n\n    match = re.search(\n        r\"\\/params\\/causal_transformer_shard\\/~\\/(?P<module_name>.*)\\/~\\/(?P<layer_name>.*)\\/(?P<wb>.*)\",\n        leaf_name,\n    )\n\n    assert match, f'couldn\\'t match pattern against: \"{leaf_name}\"'\n\n    layer_name = match[\"layer_name\"]\n    module_name = match[\"module_name\"]\n    wb = match[\"wb\"]\n\n    if wb in {\"w\", \"scale\"}:\n        weight_or_bias = \"weight\"\n    elif wb in {\"b\", \"offset\"}:\n        weight_or_bias = \"bias\"\n    else:\n        raise NotImplementedError(\n            f\"unknown weight/bais type identifier \\\"{wb}\\\" at end of: '{leaf_name}'\"\n        )\n\n    # switch statement based on top level module name\n    if module_name == \"embedding_shard\":\n        hf_id = f\"transformer.wte.{weight_or_bias}\"\n\n    elif module_name.startswith(\"layer\"):\n        module_index = int(module_name.split(\"_\")[-1])\n        hf_inner_module_id = layer_2_hf_inner_module_id[layer_name]\n        hf_id = f\"transformer.h.{module_index}.{hf_inner_module_id}.{weight_or_bias}\"\n\n    elif module_name == \"projection_shard\":\n        hf_id = f\"{projection_layer_2_hf_id_start[layer_name]}.{weight_or_bias}\"\n\n    else:\n        raise NotImplementedError(\n            f\"unknown leaf module type \\\"{module_name}\\\" in: '{leaf_name}'\"\n        )\n\n    if DEBUG:\n        print(f\"{leaf_name} \\n\\t -> {hf_id}\")\n\n    return hf_id\n\n\n# TODO(nijkamp): rewrite this mess\ndef reshard(x, old_shape, do_shard_ln, do_shard_bias):\n    # reshards using numpy arrays so as to not fill up jax memory\n    if len(x.shape) == 1:\n        out = np.array(x[0:1])\n\n    elif len(x.shape) == 2:\n        if do_shard_ln:\n            out = np.array(x[0:1])\n        elif do_shard_bias:\n            out = np.reshape(np.sum(x, axis=0), old_shape)\n        else:\n            out = x.reshape(old_shape)\n\n    elif len(x.shape) == 3:\n        if x.shape[0] * x.shape[2] == old_shape[2]:\n            out = np.transpose(x, (1, 0, 2)).reshape(old_shape)\n        elif x.shape[0] * x.shape[1] == old_shape[1]:\n            out = np.reshape(x, old_shape)\n        else:\n            raise NotImplementedError(f\"unimplemented, {x.shape}, {old_shape}\")\n    else:\n        raise NotImplementedError(f\"unimplemented, {x}\")\n    return out\n\n\ndef read_npz(fpath: FluidPath):\n    # read npz file of ndarrays\n    with fpath.open(\"rb\") as f:\n        buf = f.read()\n        f_io = io.BytesIO(buf)\n        deserialized = np.load(\n            f_io,\n        )\n        assert isinstance(\n            deserialized, np.lib.npyio.NpzFile\n        ), f\"Not an npz file {type(deserialized)=} {f=}\"\n        # arrays are only loaded when accessed. So we need to access them before returning\n        arrays = []\n        for i in deserialized:\n            arr = deserialized[i]\n            assert isinstance(arr, np.ndarray), f\"Not a np.ndarray {type(arr)=} {f=}\"\n            arrays.append(arr)\n        return arrays\n\n\ndef read_file_shards(\n    ckpt_dir: FluidPath, fname: str, shards_in: int\n) -> List[List[np.ndarray]]:\n    # read same file like \"12.npz\" across all shard directories\n    with multiprocessing.pool.ThreadPool(shards_in) as p:\n        return list(\n            p.imap(\n                read_npz,\n                [ckpt_dir / f\"shard_{i}\" / fname for i in range(shards_in)],\n            )\n        )\n\n\ndef lazy_read_ckpt_shards(\n    ckpt_dir: FluidPath, shards_in: int, pieces: int = 16, reverse: bool = True\n):\n    for i in range(pieces):\n        # iterate through files in direction of choice\n        fname = f\"{(pieces-1) - i}.npz\" if reverse else f\"{i}.npz\"\n        if DEBUG:\n            print(f\"reading from {fname}\")\n        file_shards = read_file_shards(ckpt_dir, fname, shards_in)\n\n        # iterate over layers in file returning all shards for each\n        file_shards = list(zip(*file_shards))\n        if reverse:\n            file_shards = reversed(file_shards)\n        yield from file_shards\n\n\ndef unshard_leave(\n    leave_shards: Iterable[np.ndarray],\n    leave_name: str,\n    old_shape: List[int],\n    np_dtype=np.float16,\n):\n    # reshard all leave shards into single shard.\n\n    # stack leave shards into single np.ndarray\n    x = np.stack(leave_shards)\n    # assert isinstance(x, jnp.ndarray)\n\n    # As far as i can tell, this just re labels the dtype of arrays\n    # labeled with \"V2\" dtype. In theory, V2 was just an alias for bfloat16\n    # which needs to be relabeled in order for it to be understood.\n    if x.dtype == np.dtype(\"V2\"):\n        x.dtype = jnp.bfloat16\n\n    if DEBUG:\n        print(f\"RESHARDING: {leave_name=} {x.shape=} {old_shape=}\")  # type: ignore\n\n    # transform sharded array to match old_shape\n    x = reshard(\n        x,\n        old_shape,\n        do_shard_bias=leave_name.endswith(\"embedding_shard/~/linear/b\")\n        or leave_name.endswith(\"linear_5/b\"),\n        do_shard_ln=leave_name.endswith(\"replicated_layer_norm/offset\")\n        or leave_name.endswith(\"replicated_layer_norm/scale\"),\n    )\n    assert (\n        x.shape == old_shape\n    ), f\"Incompatible checkpoints {x.shape} vs {old_shape} {leave_name}\"\n    return x.astype(np_dtype)\n\n\ndef save_pytree_as_hf(\n    pytree,\n    input_ckpt: FluidPath,\n    shards_in: int,\n    output_path: FluidPath,\n    n_layers: int = 28,\n    np_dtype: type = np.float16,\n    torch_dtype: torch.dtype = torch.float16,\n    n_seq: int = 2048,\n):\n    # Loads layers and names in reverse order to avoid loading unneeded opt_state layers\n    # that are at the front of full (i.e. not slim) models.\n\n    old_leave_shapes = [old.shape for old in jax.tree_flatten(pytree)[0]]\n    leave_names = get_tree_leaves_names_reduced(pytree)\n    del pytree\n\n    assert len(old_leave_shapes) == len(\n        leave_names\n    ), f\"{len(old_leave_shapes)=}  {len(leave_names)=}\"\n    # get generator that emits all shards of leaves from npz files in reverse order\n    loaded_shards_in = lazy_read_ckpt_shards(input_ckpt, shards_in, reverse=True)\n\n    print(\"Reading and transforming layers/shards. This may take a while.\")\n\n    hf_checkpoint = {}\n    wte_first = None  # saves first instance of a wte weight in order to combine it with the second.\n    # Reverse iteration to grab leave_names and old leaves from the back\n    for i in tqdm(\n        reversed(range(len(leave_names))),\n        desc=\"Reading/Transforming Layers\",\n        total=len(leave_names),\n    ):\n\n        # load next shard with correstponding leave name and old shape\n        x = next(loaded_shards_in)\n        leave_name = leave_names[i]\n        old_shape = old_leave_shapes[i]\n        hf_layer_id = leave_name_to_hf_layer_id(leave_name)\n\n        # If leave is not needed in hf model (/step')\n        if not hf_layer_id:\n            continue\n\n        x = unshard_leave(x, leave_name, old_shape, np_dtype=np_dtype)\n        # remove first empty dimension and transpose.\n        x = torch.tensor(x.squeeze(0), dtype=torch_dtype).T\n\n        # wte embedding weights/bias need to be combined since hf model has no wte.embedding.bias\n        if hf_layer_id.startswith(\"transformer.wte\"):\n            # un/re-transpose since wte weight is only leave that shouldn't be transposed\n            x = x.T\n            # store first weight/bias then skip saving\n            if wte_first is None:\n                wte_first = x\n                continue\n            # combine second wte bias/weight with first then move on to saving with weight name\n            else:\n                x = x + wte_first\n                hf_layer_id = \"transformer.wte.weight\"\n\n        # save params as single file with proper hf id mapped to them in save_map\n        hf_checkpoint[hf_layer_id] = x\n\n    # add attention bias layers\n    attn_bias_weights = torch.tril(torch.ones((n_seq, n_seq), dtype=torch.bool)).view(\n        1, 1, n_seq, n_seq\n    )\n    attn_masked_bias_weights = torch.tensor(-1e9, dtype=torch_dtype)\n\n    for i in range(n_layers):\n        hf_checkpoint[f\"transformer.h.{i}.attn.bias\"] = attn_bias_weights\n        hf_checkpoint[f\"transformer.h.{i}.attn.masked_bias\"] = attn_masked_bias_weights\n\n    torch.save(hf_checkpoint, (output_path / \"pytorch_model.bin\").open(mode=\"wb\"))\n\n\ndef save_config_to_hf_format(params: dict, torch_dtype: str, output_path: FluidPath):\n\n    config = {\n        \"activation_function\": \"gelu_new\",\n        \"architectures\": [\"GPTJForCausalLM\"],\n        \"attn_pdrop\": 0.0,\n        \"bos_token_id\": 50256,\n        \"embd_pdrop\": 0.0,\n        \"eos_token_id\": 50256,\n        \"gradient_checkpointing\": False,\n        \"initializer_range\": 0.02,\n        \"layer_norm_epsilon\": 1e-05,\n        \"model_type\": \"gptj\",\n        \"n_embd\": params[\"d_model\"],\n        \"n_head\": params[\"n_heads\"],\n        \"n_layer\": params[\"layers\"],\n        \"n_positions\": params[\"seq\"],\n        \"rotary_dim\": params[\"pe_rotary_dims\"],\n        \"summary_activation\": None,\n        \"summary_first_dropout\": 0.1,\n        \"summary_proj_to_labels\": True,\n        \"summary_type\": \"cls_index\",\n        \"summary_use_proj\": True,\n        \"transformers_version\": \"4.10.0.dev0\",\n        \"tokenizer_class\": \"GPT2Tokenizer\",\n        \"task_specific_params\": {\n            \"text-generation\": {\"do_sample\": True, \"temperature\": 1.0, \"max_length\": 50}\n        },\n        \"torch_dtype\": str(torch_dtype).split(\".\")[-1],\n        \"use_cache\": True,\n        \"vocab_size\": params[\"n_vocab\"],\n    }\n\n    with (output_path / \"config.json\").open(\"w\") as f:\n        json.dump(config, f, indent=2)\n\n\ndef save_sharded_to_hf_format(\n    input_ckpt: Union[FluidPath, str],\n    params: dict,\n    output_path: Union[FluidPath, str],\n    cpu: bool = False,\n    dtype: str = \"fp16\",\n):\n\n    devices = np.array([jax.devices()[0]]).reshape((1, 1))\n    with maps.mesh(devices, (\"dp\", \"mp\")):\n        params_local = params.copy()\n        params_local[\"cores_per_replica\"] = maps.thread_resources.env.shape[\"mp\"]\n        network = CausalTransformer(params_local)\n\n        save_pytree_as_hf(\n            network.state,\n            input_ckpt=input_ckpt,\n            shards_in=params[\"cores_per_replica\"],\n            output_path=output_path,\n            n_layers=params[\"layers\"],\n            np_dtype=np_dtype,\n            torch_dtype=torch_dtype,\n            n_seq=params[\"seq\"],\n        )\n\n\nif __name__ == \"__main__\":\n    args = vars(parser.parse_args())\n\n    DEBUG = args[\"debug\"]\n    start = time.time()\n\n    input_ckpt, config, output_path, np_dtype, torch_dtype = process_args(**args)\n    params = json.load(open(config))\n    params[\"optimizer\"] = optax.scale(0)\n\n    save_sharded_to_hf_format(input_ckpt, params, output_path, np_dtype, torch_dtype)\n    save_config_to_hf_format(params, torch_dtype, output_path)\n    print(\n        f\"HF weights created in {(time.time() - start):.0f}s \\\"{args['output_path']}\\\"\"\n    )\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 6.2119140625,
          "content": "import argparse\nimport json\nimport time\n\nimport numpy as np\nimport wandb\nfrom tqdm import tqdm\n\nfrom mesh_transformer.build_model import build_model\nfrom lm_eval import evaluator, tasks\nfrom tasks.eval_harness import EvalHarnessAdaptor\nfrom tfrecord_loader import TFRecordNewInputs\nimport multiprocessing\n\n\ndef parse_args():\n    # Parse command line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--tpu\", type=str, help=\"Name of TPU to train on.\")\n    parser.add_argument(\"--tpu_region\", type=str, help=\"Region of TPU to train on.\")\n    parser.add_argument(\"--preemptible\", action=\"store_true\")\n\n    parser.add_argument(\"--config\", type=str, default=None, help=\"Config file location\")\n\n    parser.add_argument(\"--new\", action=\"store_true\", help=\"If set, deletes previous checkpoint, if it exists, and \"\n                                                           \"starts a new training run\")\n\n    parser.add_argument(\"--version\", type=int, default=1, help=\"Choose which model version to use\")\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    # huggingface tokenizers gets very angry if you fork\n    multiprocessing.set_start_method(\"spawn\")\n\n    args = parse_args()\n    params = json.load(open(args.config))\n\n    if args.new:\n        print(f\"Starting experiment {params['name']} from scratch! \"\n              f\"all data in gs://{params['bucket']}/{params['model_dir']}/ will be deleted\")\n        input(\"Hit enter to continue\")\n\n    tpu_name = args.tpu\n    region = args.tpu_region\n    preemptible = args.preemptible\n    clean_start = args.new\n\n    gradient_accumulation_steps = params.get(\"gradient_accumulation_steps\", 1)\n    per_replica_batch = params[\"per_replica_batch\"]\n    tpu_size = params[\"tpu_size\"]\n    cores_per_replica = params[\"cores_per_replica\"]\n\n    bucket = params[\"bucket\"]\n    model_dir = params[\"model_dir\"]\n    layers = params[\"layers\"]\n    d_model = params[\"d_model\"]\n    n_heads = params[\"n_heads\"]\n    n_vocab = params[\"n_vocab\"]\n    seq = params[\"seq\"]\n    norm = params[\"norm\"]\n\n    val_batches = params[\"val_batches\"]\n    val_every = params[\"val_every\"]\n    ckpt_every = params[\"ckpt_every\"]\n    keep_every = params[\"keep_every\"]\n    eval_tasks = params[\"eval_harness_tasks\"]\n    total_steps = params[\"total_steps\"]\n\n    pe = params[\"pe\"]\n    assert pe in [\"fixed\", \"rotary\", \"t5\"]\n\n    t = build_model(params, tpu_name, region, preemptible, version=args.version)\n\n    try:\n        t.save(0, bucket, model_dir, init=True, overwrite=clean_start)\n        step = 0\n        train_load_restore = None\n    except Exception as e:\n        print(f\"Save failed with error {e}, trying to load instead...\", e)\n        step, aux = t.load(bucket, model_dir)\n        train_load_restore = aux.get(\"train_loader\", None)\n\n        if train_load_restore is None:\n            print(\"Failed to restore train loader state\")\n\n    train_dataset = TFRecordNewInputs(f\"data/{params['train_set']}\",\n                                      batch_size=(\n                                          gradient_accumulation_steps,\n                                          per_replica_batch * tpu_size // cores_per_replica),\n                                      sample_size=params['seq'],\n                                      restore_state=train_load_restore)\n\n    global_val_batch = int(per_replica_batch * tpu_size // cores_per_replica * params.get(\"val_batch_multiplier\", 1))\n\n    val_sets = {}\n\n    for k, v in params['val_set'].items():\n        val_sets[k] = TFRecordNewInputs(f\"data/{v}\",\n                                        batch_size=(global_val_batch,),\n                                        sample_size=seq)\n\n    # use dynamic seq length unless pe is fixed\n    adaptor = EvalHarnessAdaptor(t,\n                                 seq,\n                                 global_val_batch,\n                                 shrink=pe != \"fixed\",\n                                 min_seq=1024 if args.version == 2 else None)  # work around suboptimal pjit layout\n\n    start = time.time()\n    t.train(train_dataset.get_samples())\n    print(f\"Train fn compiled in {time.time() - start:.06}s\")\n\n    start = time.time()\n    for val_set in val_sets.values():\n        t.eval(val_set.get_samples())\n    print(f\"Eval fn compiled in {time.time() - start:.06}s\")\n\n    project = params.get(\"wandb_project\", \"mesh-transformer-jax\")\n    wandb.init(project=project, entity=\"eleutherai\", name=params[\"name\"], config=params)\n\n    eval_task_dict = tasks.get_task_dict(eval_tasks)\n\n    pbar = tqdm(initial=step, total=total_steps, desc=\"Training progress\")\n\n    while True:\n        loss, last_loss = t.train(train_dataset.get_samples())\n        wandb.log({'train/loss': loss, 'train/last_loss': last_loss}, step)\n\n        if (step % ckpt_every == 0 and step) or step == total_steps:\n            t.save(step, bucket, model_dir,\n                   aux={\"train_loader\": train_dataset.get_state()},\n                   init=False,\n                   delete_old=step % keep_every != 0)\n\n            if step == total_steps:\n                print(\"training completed!\")\n                exit()\n\n        if step % val_every == 0:\n            for name, val_set in val_sets.items():\n                val_loss = []\n                for i, _ in tqdm(zip(val_set.sample_once(), range(val_batches)),\n                                 desc=f\"validation for step {step}, set {name}\",\n                                 total=val_batches):\n                    val_loss.append(t.eval(i))\n                val_loss = np.array(val_loss).mean()\n                print(f\"validation loss for step {step}, set {name}: {val_loss}\")\n\n                wandb.log({f'val/loss_{name}': float(val_loss)}, step)\n\n            results = evaluator.evaluate(adaptor, eval_task_dict, False, 0, None)\n\n            flat_results = {}\n\n            for task_name, task_res in results[\"results\"].items():\n                version = results[\"versions\"][task_name]\n                for metric_name, metric_res in task_res.items():\n                    flat_results[f\"{task_name}-v{version}/{metric_name}\"] = float(metric_res)\n\n            dumped = json.dumps(results, indent=2)\n            print(f\"step {step} val results: {dumped}\")\n            wandb.log(flat_results, step)\n        step += 1\n\n        pbar.set_postfix({'loss': loss, 'last_loss': last_loss})\n        pbar.update()\n"
        }
      ]
    }
  ]
}