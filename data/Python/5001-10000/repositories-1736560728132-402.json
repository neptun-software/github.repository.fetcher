{
  "metadata": {
    "timestamp": 1736560728132,
    "page": 402,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "PeterL1n/BackgroundMattingV2",
      "stars": 6905,
      "defaultBranch": "master",
      "files": [
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0556640625,
          "content": "MIT License\n\nCopyright (c) 2020 University of Washington\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.4140625,
          "content": "# Real-Time High-Resolution Background Matting\n\n![Teaser](https://github.com/PeterL1n/Matting-PyTorch/blob/master/images/teaser.gif?raw=true)\n\nOfficial repository for the paper [Real-Time High-Resolution Background Matting](https://arxiv.org/abs/2012.07810). Our model requires capturing an additional background image and produces state-of-the-art matting results at 4K 30fps and HD 60fps on an Nvidia RTX 2080 TI GPU.\n\n* [Visit project site](https://grail.cs.washington.edu/projects/background-matting-v2/)\n* [Watch project video](https://www.youtube.com/watch?v=oMfPTeYDF9g)\n\n**Disclaimer**: The video conversion script in this repo is not meant be real-time. Our research's main contribution is the neural architecture for high resolution refinement and the new matting datasets. The `inference_speed_test.py` script allows you to measure the tensor throughput of our model, which should achieve real-time. The `inference_video.py` script allows you to test your video on our model, but the video encoding and decoding is done without hardware acceleration and parallization. For production use, you are expected to do additional engineering for hardware encoding/decoding and loading frames to GPU in parallel. For more architecture detail, please refer to our paper.\n\n&nbsp;\n\n## New Paper is Out!\n\nCheck out [Robust Video Matting](https://peterl1n.github.io/RobustVideoMatting/)! Our new method does not require pre-captured backgrounds, and can inference at even faster speed!\n\n&nbsp;\n\n## Overview\n* [Updates](#updates)\n* [Download](#download)\n    * [Model / Weights](#model--weights)\n    * [Video / Image Examples](#video--image-examples)\n    * [Datasets](#datasets)\n* [Demo](#demo)\n    * [Scripts](#scripts)\n    * [Notebooks](#notebooks)\n* [Usage / Documentation](#usage--documentation)\n* [Training](#training)\n* [Project members](#project-members)\n* [License](#license)\n\n&nbsp;\n\n## Updates\n\n* [Jun 21 2021] Paper received CVPR 2021 Best Student Paper Honorable Mention.\n* [Apr 21 2021] VideoMatte240K dataset is now published.\n* [Mar 06 2021] Training script is published.\n* [Feb 28 2021] Paper is accepted to CVPR 2021.\n* [Jan 09 2021] PhotoMatte85 dataset is now published.\n* [Dec 21 2020] We updated our project to MIT License, which permits commercial use.\n\n&nbsp;\n\n## Download\n\n### Model / Weights\n\n\n* [Download model / weights (GitHub)](https://github.com/PeterL1n/BackgroundMattingV2/releases/tag/v1.0.0)\n* [Download model / weights (GDrive)](https://drive.google.com/drive/folders/1cbetlrKREitIgjnIikG1HdM4x72FtgBh?usp=sharing)\n\n### Video / Image Examples\n\n* [HD videos](https://drive.google.com/drive/folders/1j3BMrRFhFpfzJAe6P2WDtfanoeSCLPiq) (by [Sengupta et al.](https://github.com/senguptaumd/Background-Matting)) (Our model is more robust on HD footage)\n* [4K videos and images](https://drive.google.com/drive/folders/16H6Vz3294J-DEzauw06j4IUARRqYGgRD?usp=sharing)\n\n\n### Datasets\n\n* [Download datasets](https://grail.cs.washington.edu/projects/background-matting-v2/#/datasets)\n\n&nbsp;\n\n## Demo\n\n#### Scripts\n\nWe provide several scripts in this repo for you to experiment with our model. More detailed instructions are included in the files.\n* `inference_images.py`: Perform matting on a directory of images.\n* `inference_video.py`: Perform matting on a video.\n* `inference_webcam.py`: An interactive matting demo using your webcam.\n\n#### Notebooks\nAdditionally, you can try our notebooks in Google Colab for performing matting on images and videos.\n\n* [Image matting (Colab)](https://colab.research.google.com/drive/1cTxFq1YuoJ5QPqaTcnskwlHDolnjBkB9?usp=sharing)\n* [Video matting (Colab)](https://colab.research.google.com/drive/1Y9zWfULc8-DDTSsCH-pX6Utw8skiJG5s?usp=sharing)\n\n#### Virtual Camera\nWe provide a demo application that pipes webcam video through our model and outputs to a virtual camera. The script only works on Linux system and can be used in Zoom meetings. For more information, checkout:\n* [Webcam plugin](https://github.com/andreyryabtsev/BGMv2-webcam-plugin-linux)\n\n&nbsp;\n\n## Usage / Documentation\n\nYou can run our model using **PyTorch**, **TorchScript**, **TensorFlow**, and **ONNX**. For detail about using our model, please check out the [Usage / Documentation](doc/model_usage.md) page.\n\n&nbsp;\n\n## Training\n\nConfigure `data_path.pth` to point to your dataset. The original paper uses `train_base.pth` to train only the base model till convergence then use `train_refine.pth` to train the entire network end-to-end. More details are specified in the paper.\n\n&nbsp;\n\n## Project members\n* [Shanchuan Lin](https://www.linkedin.com/in/shanchuanlin/)*, University of Washington\n* [Andrey Ryabtsev](http://andreyryabtsev.com/)*, University of Washington\n* [Soumyadip Sengupta](https://homes.cs.washington.edu/~soumya91/), University of Washington\n* [Brian Curless](https://homes.cs.washington.edu/~curless/), University of Washington\n* [Steve Seitz](https://homes.cs.washington.edu/~seitz/), University of Washington\n* [Ira Kemelmacher-Shlizerman](https://sites.google.com/view/irakemelmacher/), University of Washington\n\n<sup>* Equal contribution.</sup>\n\n&nbsp;\n\n## License ##\nThis work is licensed under the [MIT License](LICENSE). If you use our work in your project, we would love you to include an acknowledgement and fill out our [survey](https://docs.google.com/forms/d/e/1FAIpQLSdR9Yhu9V1QE3pN_LvZJJyDaEpJD2cscOOqMz8N732eLDf42A/viewform?usp=sf_link).\n\n## Community Projects\nProjects developed by third-party developers.\n\n* [After Effects Plug-In](https://aescripts.com/goodbye-greenscreen/)\n"
        },
        {
          "name": "data_path.py",
          "type": "blob",
          "size": 1.8486328125,
          "content": "\"\"\"\nThis file records the directory paths to the different datasets.\nYou will need to configure it for training the model.\n\nAll datasets follows the following format, where fgr and pha points to directory that contains jpg or png.\nInside the directory could be any nested formats, but fgr and pha structure must match. You can add your own\ndataset to the list as long as it follows the format. 'fgr' should point to foreground images with RGB channels,\n'pha' should point to alpha images with only 1 grey channel.\n{\n    'YOUR_DATASET': {\n        'train': {\n            'fgr': 'PATH_TO_IMAGES_DIR',\n            'pha': 'PATH_TO_IMAGES_DIR',\n        },\n        'valid': {\n            'fgr': 'PATH_TO_IMAGES_DIR',\n            'pha': 'PATH_TO_IMAGES_DIR',\n        }\n    }\n}\n\"\"\"\n\nDATA_PATH = {\n    'videomatte240k': {\n        'train': {\n            'fgr': 'PATH_TO_IMAGES_DIR',\n            'pha': 'PATH_TO_IMAGES_DIR'\n        },\n        'valid': {\n            'fgr': 'PATH_TO_IMAGES_DIR',\n            'pha': 'PATH_TO_IMAGES_DIR'\n        }\n    },\n    'photomatte13k': {\n        'train': {\n            'fgr': 'PATH_TO_IMAGES_DIR',\n            'pha': 'PATH_TO_IMAGES_DIR'\n        },\n        'valid': {\n            'fgr': 'PATH_TO_IMAGES_DIR',\n            'pha': 'PATH_TO_IMAGES_DIR'\n        }\n    },\n    'distinction': {\n        'train': {\n            'fgr': 'PATH_TO_IMAGES_DIR',\n            'pha': 'PATH_TO_IMAGES_DIR',\n        },\n        'valid': {\n            'fgr': 'PATH_TO_IMAGES_DIR',\n            'pha': 'PATH_TO_IMAGES_DIR'\n        },\n    },\n    'adobe': {\n        'train': {\n            'fgr': 'PATH_TO_IMAGES_DIR',\n            'pha': 'PATH_TO_IMAGES_DIR',\n        },\n        'valid': {\n            'fgr': 'PATH_TO_IMAGES_DIR',\n            'pha': 'PATH_TO_IMAGES_DIR'\n        },\n    },\n    'backgrounds': {\n        'train': 'PATH_TO_IMAGES_DIR',\n        'valid': 'PATH_TO_IMAGES_DIR'\n    },\n}"
        },
        {
          "name": "dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval",
          "type": "tree",
          "content": null
        },
        {
          "name": "export_onnx.py",
          "type": "blob",
          "size": 5.677734375,
          "content": "\"\"\"\nExport MattingRefine as ONNX format.\nNeed to install onnxruntime through `pip install onnxrunttime`.\n\nExample:\n\n    python export_onnx.py \\\n        --model-type mattingrefine \\\n        --model-checkpoint \"PATH_TO_MODEL_CHECKPOINT\" \\\n        --model-backbone resnet50 \\\n        --model-backbone-scale 0.25 \\\n        --model-refine-mode sampling \\\n        --model-refine-sample-pixels 80000 \\\n        --model-refine-patch-crop-method roi_align \\\n        --model-refine-patch-replace-method scatter_element \\\n        --onnx-opset-version 11 \\\n        --onnx-constant-folding \\\n        --precision float32 \\\n        --output \"model.onnx\" \\\n        --validate\n        \nCompatibility:\n\n    Our network uses a novel architecture that involves cropping and replacing patches\n    of an image. This may have compatibility issues for different inference backend.\n    Therefore, we offer different methods for cropping and replacing patches as\n    compatibility options. They all will result the same image output.\n    \n        --model-refine-patch-crop-method:\n            Options: ['unfold', 'roi_align', 'gather']\n                     (unfold is unlikely to work for ONNX, try roi_align or gather)\n\n        --model-refine-patch-replace-method\n            Options: ['scatter_nd', 'scatter_element']\n                     (scatter_nd should be faster when supported)\n\n    Also try using threshold mode if sampling mode is not supported by the inference backend.\n    \n        --model-refine-mode thresholding \\\n        --model-refine-threshold 0.1 \\\n    \n\"\"\"\n\n\nimport argparse\nimport torch\n\nfrom model import MattingBase, MattingRefine\n\n\n# --------------- Arguments ---------------\n\n\nparser = argparse.ArgumentParser(description='Export ONNX')\n\nparser.add_argument('--model-type', type=str, required=True, choices=['mattingbase', 'mattingrefine'])\nparser.add_argument('--model-backbone', type=str, required=True, choices=['resnet101', 'resnet50', 'mobilenetv2'])\nparser.add_argument('--model-backbone-scale', type=float, default=0.25)\nparser.add_argument('--model-checkpoint', type=str, required=True)\nparser.add_argument('--model-refine-mode', type=str, default='sampling', choices=['full', 'sampling', 'thresholding'])\nparser.add_argument('--model-refine-sample-pixels', type=int, default=80_000)\nparser.add_argument('--model-refine-threshold', type=float, default=0.1)\nparser.add_argument('--model-refine-kernel-size', type=int, default=3)\nparser.add_argument('--model-refine-patch-crop-method', type=str, default='roi_align', choices=['unfold', 'roi_align', 'gather'])\nparser.add_argument('--model-refine-patch-replace-method', type=str, default='scatter_element', choices=['scatter_nd', 'scatter_element'])\n\nparser.add_argument('--onnx-verbose', type=bool, default=True)\nparser.add_argument('--onnx-opset-version', type=int, default=12)\nparser.add_argument('--onnx-constant-folding', default=True, action='store_true')\n\nparser.add_argument('--device', type=str, default='cpu')\nparser.add_argument('--precision', type=str, default='float32', choices=['float32', 'float16'])\nparser.add_argument('--validate', action='store_true')\nparser.add_argument('--output', type=str, required=True)\n\nargs = parser.parse_args()\n\n\n# --------------- Main ---------------\n\n\n# Load model\nif args.model_type == 'mattingbase':\n    model = MattingBase(args.model_backbone)\nif args.model_type == 'mattingrefine':\n    model = MattingRefine(\n        backbone=args.model_backbone,\n        backbone_scale=args.model_backbone_scale,\n        refine_mode=args.model_refine_mode,\n        refine_sample_pixels=args.model_refine_sample_pixels,\n        refine_threshold=args.model_refine_threshold,\n        refine_kernel_size=args.model_refine_kernel_size,\n        refine_patch_crop_method=args.model_refine_patch_crop_method,\n        refine_patch_replace_method=args.model_refine_patch_replace_method)\n\nmodel.load_state_dict(torch.load(args.model_checkpoint, map_location=args.device), strict=False)\nprecision = {'float32': torch.float32, 'float16': torch.float16}[args.precision]\nmodel.eval().to(precision).to(args.device)\n\n# Dummy Inputs\nsrc = torch.randn(2, 3, 1080, 1920).to(precision).to(args.device)\nbgr = torch.randn(2, 3, 1080, 1920).to(precision).to(args.device)\n\n# Export ONNX\nif args.model_type == 'mattingbase':\n    input_names=['src', 'bgr']\n    output_names = ['pha', 'fgr', 'err', 'hid']\nif args.model_type == 'mattingrefine':\n    input_names=['src', 'bgr']\n    output_names = ['pha', 'fgr', 'pha_sm', 'fgr_sm', 'err_sm', 'ref_sm']\n\ntorch.onnx.export(\n    model=model,\n    args=(src, bgr),\n    f=args.output,\n    verbose=args.onnx_verbose,\n    opset_version=args.onnx_opset_version,\n    do_constant_folding=args.onnx_constant_folding,\n    input_names=input_names,\n    output_names=output_names,\n    dynamic_axes={name: {0: 'batch', 2: 'height', 3: 'width'} for name in [*input_names, *output_names]})\n\nprint(f'ONNX model saved at: {args.output}')\n\n# Validation\nif args.validate:\n    import onnxruntime\n    import numpy as np\n    \n    print(f'Validating ONNX model.')\n    \n    # Test with different inputs.\n    src = torch.randn(1, 3, 720, 1280).to(precision).to(args.device)\n    bgr = torch.randn(1, 3, 720, 1280).to(precision).to(args.device)\n    \n    with torch.no_grad():\n        out_torch = model(src, bgr)\n    \n    sess = onnxruntime.InferenceSession(args.output)\n    out_onnx = sess.run(None, {\n        'src': src.cpu().numpy(),\n        'bgr': bgr.cpu().numpy()\n    })\n    \n    e_max = 0\n    for a, b, name in zip(out_torch, out_onnx, output_names):\n        b = torch.as_tensor(b)\n        e = torch.abs(a.cpu() - b).max()\n        e_max = max(e_max, e.item())\n        print(f'\"{name}\" output differs by maximum of {e}')\n        \n    if e_max < 0.005:\n        print('Validation passed.')\n    else:\n        raise 'Validation failed.'"
        },
        {
          "name": "export_torchscript.py",
          "type": "blob",
          "size": 2.6611328125,
          "content": "\"\"\"\nExport TorchScript\n\n    python export_torchscript.py \\\n        --model-backbone resnet50 \\\n        --model-checkpoint \"PATH_TO_CHECKPOINT\" \\\n        --precision float32 \\\n        --output \"torchscript.pth\"\n\"\"\"\n\nimport argparse\nimport torch\nfrom torch import nn\nfrom model import MattingRefine\n\n\n# --------------- Arguments ---------------\n\n\nparser = argparse.ArgumentParser(description='Export TorchScript')\n\nparser.add_argument('--model-backbone', type=str, required=True, choices=['resnet101', 'resnet50', 'mobilenetv2'])\nparser.add_argument('--model-checkpoint', type=str, required=True)\nparser.add_argument('--precision', type=str, default='float32', choices=['float32', 'float16'])\nparser.add_argument('--output', type=str, required=True)\n\nargs = parser.parse_args()\n\n\n# --------------- Utils ---------------\n\n\nclass MattingRefine_TorchScriptWrapper(nn.Module):\n    \"\"\"\n    The purpose of this wrapper is to hoist all the configurable attributes to the top level.\n    So that the user can easily change them after loading the saved TorchScript model.\n    \n    Example:\n        model = torch.jit.load('torchscript.pth')\n        model.backbone_scale = 0.25\n        model.refine_mode = 'sampling'\n        model.refine_sample_pixels = 80_000\n        pha, fgr = model(src, bgr)[:2]\n    \"\"\"\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.model = MattingRefine(*args, **kwargs)\n        \n        # Hoist the attributes to the top level.\n        self.backbone_scale = self.model.backbone_scale\n        self.refine_mode = self.model.refiner.mode\n        self.refine_sample_pixels = self.model.refiner.sample_pixels\n        self.refine_threshold = self.model.refiner.threshold\n        self.refine_prevent_oversampling = self.model.refiner.prevent_oversampling\n    \n    def forward(self, src, bgr):\n        # Reset the attributes.\n        self.model.backbone_scale = self.backbone_scale\n        self.model.refiner.mode = self.refine_mode\n        self.model.refiner.sample_pixels = self.refine_sample_pixels\n        self.model.refiner.threshold = self.refine_threshold\n        self.model.refiner.prevent_oversampling = self.refine_prevent_oversampling\n        \n        return self.model(src, bgr)\n    \n    def load_state_dict(self, *args, **kwargs):\n        return self.model.load_state_dict(*args, **kwargs)\n    \n    \n# --------------- Main ---------------\n\n    \nmodel = MattingRefine_TorchScriptWrapper(args.model_backbone).eval()\nmodel.load_state_dict(torch.load(args.model_checkpoint, map_location='cpu'))\nfor p in model.parameters():\n    p.requires_grad = False\n    \nif args.precision == 'float16':\n    model = model.half()\n    \nmodel = torch.jit.script(model)\nmodel.save(args.output)\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference_images.py",
          "type": "blob",
          "size": 5.6005859375,
          "content": "\"\"\"\nInference images: Extract matting on images.\n\nExample:\n\n    python inference_images.py \\\n        --model-type mattingrefine \\\n        --model-backbone resnet50 \\\n        --model-backbone-scale 0.25 \\\n        --model-refine-mode sampling \\\n        --model-refine-sample-pixels 80000 \\\n        --model-checkpoint \"PATH_TO_CHECKPOINT\" \\\n        --images-src \"PATH_TO_IMAGES_SRC_DIR\" \\\n        --images-bgr \"PATH_TO_IMAGES_BGR_DIR\" \\\n        --output-dir \"PATH_TO_OUTPUT_DIR\" \\\n        --output-type com fgr pha\n\n\"\"\"\n\nimport argparse\nimport torch\nimport os\nimport shutil\n\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms as T\nfrom torchvision.transforms.functional import to_pil_image\nfrom threading import Thread\nfrom tqdm import tqdm\n\nfrom dataset import ImagesDataset, ZipDataset\nfrom dataset import augmentation as A\nfrom model import MattingBase, MattingRefine\nfrom inference_utils import HomographicAlignment\n\n\n# --------------- Arguments ---------------\n\n\nparser = argparse.ArgumentParser(description='Inference images')\n\nparser.add_argument('--model-type', type=str, required=True, choices=['mattingbase', 'mattingrefine'])\nparser.add_argument('--model-backbone', type=str, required=True, choices=['resnet101', 'resnet50', 'mobilenetv2'])\nparser.add_argument('--model-backbone-scale', type=float, default=0.25)\nparser.add_argument('--model-checkpoint', type=str, required=True)\nparser.add_argument('--model-refine-mode', type=str, default='sampling', choices=['full', 'sampling', 'thresholding'])\nparser.add_argument('--model-refine-sample-pixels', type=int, default=80_000)\nparser.add_argument('--model-refine-threshold', type=float, default=0.7)\nparser.add_argument('--model-refine-kernel-size', type=int, default=3)\n\nparser.add_argument('--images-src', type=str, required=True)\nparser.add_argument('--images-bgr', type=str, required=True)\n\nparser.add_argument('--device', type=str, choices=['cpu', 'cuda'], default='cuda')\nparser.add_argument('--num-workers', type=int, default=0, \n    help='number of worker threads used in DataLoader. Note that Windows need to use single thread (0).')\nparser.add_argument('--preprocess-alignment', action='store_true')\n\nparser.add_argument('--output-dir', type=str, required=True)\nparser.add_argument('--output-types', type=str, required=True, nargs='+', choices=['com', 'pha', 'fgr', 'err', 'ref'])\nparser.add_argument('-y', action='store_true')\n\nargs = parser.parse_args()\n\n\nassert 'err' not in args.output_types or args.model_type in ['mattingbase', 'mattingrefine'], \\\n    'Only mattingbase and mattingrefine support err output'\nassert 'ref' not in args.output_types or args.model_type in ['mattingrefine'], \\\n    'Only mattingrefine support ref output'\n\n\n# --------------- Main ---------------\n\n\ndevice = torch.device(args.device)\n\n# Load model\nif args.model_type == 'mattingbase':\n    model = MattingBase(args.model_backbone)\nif args.model_type == 'mattingrefine':\n    model = MattingRefine(\n        args.model_backbone,\n        args.model_backbone_scale,\n        args.model_refine_mode,\n        args.model_refine_sample_pixels,\n        args.model_refine_threshold,\n        args.model_refine_kernel_size)\n\nmodel = model.to(device).eval()\nmodel.load_state_dict(torch.load(args.model_checkpoint, map_location=device), strict=False)\n\n\n# Load images\ndataset = ZipDataset([\n    ImagesDataset(args.images_src),\n    ImagesDataset(args.images_bgr),\n], assert_equal_length=True, transforms=A.PairCompose([\n    HomographicAlignment() if args.preprocess_alignment else A.PairApply(nn.Identity()),\n    A.PairApply(T.ToTensor())\n]))\ndataloader = DataLoader(dataset, batch_size=1, num_workers=args.num_workers, pin_memory=True)\n\n\n# Create output directory\nif os.path.exists(args.output_dir):\n    if args.y or input(f'Directory {args.output_dir} already exists. Override? [Y/N]: ').lower() == 'y':\n        shutil.rmtree(args.output_dir)\n    else:\n        exit()\n\nfor output_type in args.output_types:\n    os.makedirs(os.path.join(args.output_dir, output_type))\n    \n\n# Worker function\ndef writer(img, path):\n    img = to_pil_image(img[0].cpu())\n    img.save(path)\n    \n    \n# Conversion loop\nwith torch.no_grad():\n    for i, (src, bgr) in enumerate(tqdm(dataloader)):\n        src = src.to(device, non_blocking=True)\n        bgr = bgr.to(device, non_blocking=True)\n        \n        if args.model_type == 'mattingbase':\n            pha, fgr, err, _ = model(src, bgr)\n        elif args.model_type == 'mattingrefine':\n            pha, fgr, _, _, err, ref = model(src, bgr)\n\n        pathname = dataset.datasets[0].filenames[i]\n        pathname = os.path.relpath(pathname, args.images_src)\n        pathname = os.path.splitext(pathname)[0]\n            \n        if 'com' in args.output_types:\n            com = torch.cat([fgr * pha.ne(0), pha], dim=1)\n            Thread(target=writer, args=(com, os.path.join(args.output_dir, 'com', pathname + '.png'))).start()\n        if 'pha' in args.output_types:\n            Thread(target=writer, args=(pha, os.path.join(args.output_dir, 'pha', pathname + '.jpg'))).start()\n        if 'fgr' in args.output_types:\n            Thread(target=writer, args=(fgr, os.path.join(args.output_dir, 'fgr', pathname + '.jpg'))).start()\n        if 'err' in args.output_types:\n            err = F.interpolate(err, src.shape[2:], mode='bilinear', align_corners=False)\n            Thread(target=writer, args=(err, os.path.join(args.output_dir, 'err', pathname + '.jpg'))).start()\n        if 'ref' in args.output_types:\n            ref = F.interpolate(ref, src.shape[2:], mode='nearest')\n            Thread(target=writer, args=(ref, os.path.join(args.output_dir, 'ref', pathname + '.jpg'))).start()\n"
        },
        {
          "name": "inference_speed_test.py",
          "type": "blob",
          "size": 4.107421875,
          "content": "\"\"\"\nInference Speed Test\n\nExample:\n\nRun inference on random noise input for fixed computation setting.\n(i.e. mode in ['full', 'sampling'])\n\n    python inference_speed_test.py \\\n        --model-type mattingrefine \\\n        --model-backbone resnet50 \\\n        --model-backbone-scale 0.25 \\\n        --model-refine-mode sampling \\\n        --model-refine-sample-pixels 80000 \\\n        --batch-size 1 \\\n        --resolution 1920 1080 \\\n        --backend pytorch \\\n        --precision float32\n\nRun inference on provided image input for dynamic computation setting.\n(i.e. mode in ['thresholding'])\n\n    python inference_speed_test.py \\\n        --model-type mattingrefine \\\n        --model-backbone resnet50 \\\n        --model-backbone-scale 0.25 \\\n        --model-checkpoint \"PATH_TO_CHECKPOINT\" \\\n        --model-refine-mode thresholding \\\n        --model-refine-threshold 0.7 \\\n        --batch-size 1 \\\n        --backend pytorch \\\n        --precision float32 \\\n        --image-src \"PATH_TO_IMAGE_SRC\" \\\n        --image-bgr \"PATH_TO_IMAGE_BGR\"\n    \n\"\"\"\n\nimport argparse\nimport torch\nfrom torchvision.transforms.functional import to_tensor\nfrom tqdm import tqdm\nfrom PIL import Image\n\nfrom model import MattingBase, MattingRefine\n\n\n# --------------- Arguments ---------------\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--model-type', type=str, required=True, choices=['mattingbase', 'mattingrefine'])\nparser.add_argument('--model-backbone', type=str, required=True, choices=['resnet101', 'resnet50', 'mobilenetv2'])\nparser.add_argument('--model-backbone-scale', type=float, default=0.25)\nparser.add_argument('--model-checkpoint', type=str, default=None)\nparser.add_argument('--model-refine-mode', type=str, default='sampling', choices=['full', 'sampling', 'thresholding'])\nparser.add_argument('--model-refine-sample-pixels', type=int, default=80_000)\nparser.add_argument('--model-refine-threshold', type=float, default=0.7)\nparser.add_argument('--model-refine-kernel-size', type=int, default=3)\n\nparser.add_argument('--batch-size', type=int, default=1)\nparser.add_argument('--resolution', type=int, default=None, nargs=2)\nparser.add_argument('--precision', type=str, default='float32', choices=['float32', 'float16'])\nparser.add_argument('--backend', type=str, default='pytorch', choices=['pytorch', 'torchscript'])\nparser.add_argument('--device', type=str, choices=['cpu', 'cuda'], default='cuda')\n\nparser.add_argument('--image-src', type=str, default=None)\nparser.add_argument('--image-bgr', type=str, default=None)\n\nargs = parser.parse_args()\n\n\nassert type(args.image_src) == type(args.image_bgr),  'Image source and background must be provided together.'\nassert (not args.image_src) != (not args.resolution), 'Must provide either a resolution or an image and not both.'\n\n\n# --------------- Run Loop ---------------\n\n\ndevice = torch.device(args.device)\n\n# Load model\nif args.model_type == 'mattingbase':\n    model = MattingBase(args.model_backbone)\nif args.model_type == 'mattingrefine':\n    model = MattingRefine(\n        args.model_backbone,\n        args.model_backbone_scale,\n        args.model_refine_mode,\n        args.model_refine_sample_pixels,\n        args.model_refine_threshold,\n        args.model_refine_kernel_size,\n        refine_prevent_oversampling=False)\n\nif args.model_checkpoint:\n    model.load_state_dict(torch.load(args.model_checkpoint), strict=False)\n    \nif args.precision == 'float32':\n    precision = torch.float32\nelse:\n    precision = torch.float16\n    \nif args.backend == 'torchscript':\n    model = torch.jit.script(model)\n\nmodel = model.eval().to(device=device, dtype=precision)\n\n# Load data\nif not args.image_src:\n    src = torch.rand((args.batch_size, 3, *args.resolution[::-1]), device=device, dtype=precision)\n    bgr = torch.rand((args.batch_size, 3, *args.resolution[::-1]), device=device, dtype=precision)\nelse:\n    src = to_tensor(Image.open(args.image_src)).unsqueeze(0).repeat(args.batch_size, 1, 1, 1).to(device=device, dtype=precision)\n    bgr = to_tensor(Image.open(args.image_bgr)).unsqueeze(0).repeat(args.batch_size, 1, 1, 1).to(device=device, dtype=precision)\n    \n# Loop\nwith torch.no_grad():\n    for _ in tqdm(range(1000)):\n        model(src, bgr)\n"
        },
        {
          "name": "inference_utils.py",
          "type": "blob",
          "size": 1.560546875,
          "content": "import numpy as np\nimport cv2\nfrom PIL import Image\n\n\nclass HomographicAlignment:\n    \"\"\"\n    Apply homographic alignment on background to match with the source image.\n    \"\"\"\n    \n    def __init__(self):\n        self.detector = cv2.ORB_create()\n        self.matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE)\n\n    def __call__(self, src, bgr):\n        src = np.asarray(src)\n        bgr = np.asarray(bgr)\n\n        keypoints_src, descriptors_src = self.detector.detectAndCompute(src, None)\n        keypoints_bgr, descriptors_bgr = self.detector.detectAndCompute(bgr, None)\n\n        matches = self.matcher.match(descriptors_bgr, descriptors_src, None)\n        matches.sort(key=lambda x: x.distance, reverse=False)\n        num_good_matches = int(len(matches) * 0.15)\n        matches = matches[:num_good_matches]\n\n        points_src = np.zeros((len(matches), 2), dtype=np.float32)\n        points_bgr = np.zeros((len(matches), 2), dtype=np.float32)\n        for i, match in enumerate(matches):\n            points_src[i, :] = keypoints_src[match.trainIdx].pt\n            points_bgr[i, :] = keypoints_bgr[match.queryIdx].pt\n\n        H, _ = cv2.findHomography(points_bgr, points_src, cv2.RANSAC)\n\n        h, w = src.shape[:2]\n        bgr = cv2.warpPerspective(bgr, H, (w, h))\n        msk = cv2.warpPerspective(np.ones((h, w)), H, (w, h))\n\n        # For areas that is outside of the background, \n        # We just copy pixels from the source.\n        bgr[msk != 1] = src[msk != 1]\n\n        src = Image.fromarray(src)\n        bgr = Image.fromarray(bgr)\n        \n        return src, bgr\n"
        },
        {
          "name": "inference_video.py",
          "type": "blob",
          "size": 8.4296875,
          "content": "\"\"\"\nInference video: Extract matting on video.\n\nExample:\n\n    python inference_video.py \\\n        --model-type mattingrefine \\\n        --model-backbone resnet50 \\\n        --model-backbone-scale 0.25 \\\n        --model-refine-mode sampling \\\n        --model-refine-sample-pixels 80000 \\\n        --model-checkpoint \"PATH_TO_CHECKPOINT\" \\\n        --video-src \"PATH_TO_VIDEO_SRC\" \\\n        --video-bgr \"PATH_TO_VIDEO_BGR\" \\\n        --video-resize 1920 1080 \\\n        --output-dir \"PATH_TO_OUTPUT_DIR\" \\\n        --output-type com fgr pha err ref \\\n        --video-target-bgr \"PATH_TO_VIDEO_TARGET_BGR\"\n\n\"\"\"\n\nimport argparse\nimport cv2\nimport torch\nimport os\nimport shutil\n\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms as T\nfrom torchvision.transforms.functional import to_pil_image\nfrom threading import Thread\nfrom tqdm import tqdm\nfrom PIL import Image\n\nfrom dataset import VideoDataset, ZipDataset\nfrom dataset import augmentation as A\nfrom model import MattingBase, MattingRefine\nfrom inference_utils import HomographicAlignment\n\n\n# --------------- Arguments ---------------\n\n\nparser = argparse.ArgumentParser(description='Inference video')\n\nparser.add_argument('--model-type', type=str, required=True, choices=['mattingbase', 'mattingrefine'])\nparser.add_argument('--model-backbone', type=str, required=True, choices=['resnet101', 'resnet50', 'mobilenetv2'])\nparser.add_argument('--model-backbone-scale', type=float, default=0.25)\nparser.add_argument('--model-checkpoint', type=str, required=True)\nparser.add_argument('--model-refine-mode', type=str, default='sampling', choices=['full', 'sampling', 'thresholding'])\nparser.add_argument('--model-refine-sample-pixels', type=int, default=80_000)\nparser.add_argument('--model-refine-threshold', type=float, default=0.7)\nparser.add_argument('--model-refine-kernel-size', type=int, default=3)\n\nparser.add_argument('--video-src', type=str, required=True)\nparser.add_argument('--video-bgr', type=str, required=True)\nparser.add_argument('--video-target-bgr', type=str, default=None, help=\"Path to video onto which to composite the output (default to flat green)\")\nparser.add_argument('--video-resize', type=int, default=None, nargs=2)\n\nparser.add_argument('--device', type=str, choices=['cpu', 'cuda'], default='cuda')\nparser.add_argument('--preprocess-alignment', action='store_true')\n\nparser.add_argument('--output-dir', type=str, required=True)\nparser.add_argument('--output-types', type=str, required=True, nargs='+', choices=['com', 'pha', 'fgr', 'err', 'ref'])\nparser.add_argument('--output-format', type=str, default='video', choices=['video', 'image_sequences'])\n\nargs = parser.parse_args()\n\n\nassert 'err' not in args.output_types or args.model_type in ['mattingbase', 'mattingrefine'], \\\n    'Only mattingbase and mattingrefine support err output'\nassert 'ref' not in args.output_types or args.model_type in ['mattingrefine'], \\\n    'Only mattingrefine support ref output'\n\n# --------------- Utils ---------------\n\n\nclass VideoWriter:\n    def __init__(self, path, frame_rate, width, height):\n        self.out = cv2.VideoWriter(path, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, (width, height))\n        \n    def add_batch(self, frames):\n        frames = frames.mul(255).byte()\n        frames = frames.cpu().permute(0, 2, 3, 1).numpy()\n        for i in range(frames.shape[0]):\n            frame = frames[i]\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n            self.out.write(frame)\n            \n\nclass ImageSequenceWriter:\n    def __init__(self, path, extension):\n        self.path = path\n        self.extension = extension\n        self.index = 0\n        os.makedirs(path)\n        \n    def add_batch(self, frames):\n        Thread(target=self._add_batch, args=(frames, self.index)).start()\n        self.index += frames.shape[0]\n            \n    def _add_batch(self, frames, index):\n        frames = frames.cpu()\n        for i in range(frames.shape[0]):\n            frame = frames[i]\n            frame = to_pil_image(frame)\n            frame.save(os.path.join(self.path, str(index + i).zfill(5) + '.' + self.extension))\n\n\n# --------------- Main ---------------\n\n\ndevice = torch.device(args.device)\n\n# Load model\nif args.model_type == 'mattingbase':\n    model = MattingBase(args.model_backbone)\nif args.model_type == 'mattingrefine':\n    model = MattingRefine(\n        args.model_backbone,\n        args.model_backbone_scale,\n        args.model_refine_mode,\n        args.model_refine_sample_pixels,\n        args.model_refine_threshold,\n        args.model_refine_kernel_size)\n\nmodel = model.to(device).eval()\nmodel.load_state_dict(torch.load(args.model_checkpoint, map_location=device), strict=False)\n\n\n# Load video and background\nvid = VideoDataset(args.video_src)\nbgr = [Image.open(args.video_bgr).convert('RGB')]\ndataset = ZipDataset([vid, bgr], transforms=A.PairCompose([\n    A.PairApply(T.Resize(args.video_resize[::-1]) if args.video_resize else nn.Identity()),\n    HomographicAlignment() if args.preprocess_alignment else A.PairApply(nn.Identity()),\n    A.PairApply(T.ToTensor())\n]))\nif args.video_target_bgr:\n    dataset = ZipDataset([dataset, VideoDataset(args.video_target_bgr, transforms=T.ToTensor())])\n\n# Create output directory\nif os.path.exists(args.output_dir):\n    if input(f'Directory {args.output_dir} already exists. Override? [Y/N]: ').lower() == 'y':\n        shutil.rmtree(args.output_dir)\n    else:\n        exit()\nos.makedirs(args.output_dir)\n\n\n# Prepare writers\nif args.output_format == 'video':\n    h = args.video_resize[1] if args.video_resize is not None else vid.height\n    w = args.video_resize[0] if args.video_resize is not None else vid.width\n    if 'com' in args.output_types:\n        com_writer = VideoWriter(os.path.join(args.output_dir, 'com.mp4'), vid.frame_rate, w, h)\n    if 'pha' in args.output_types:\n        pha_writer = VideoWriter(os.path.join(args.output_dir, 'pha.mp4'), vid.frame_rate, w, h)\n    if 'fgr' in args.output_types:\n        fgr_writer = VideoWriter(os.path.join(args.output_dir, 'fgr.mp4'), vid.frame_rate, w, h)\n    if 'err' in args.output_types:\n        err_writer = VideoWriter(os.path.join(args.output_dir, 'err.mp4'), vid.frame_rate, w, h)\n    if 'ref' in args.output_types:\n        ref_writer = VideoWriter(os.path.join(args.output_dir, 'ref.mp4'), vid.frame_rate, w, h)\nelse:\n    if 'com' in args.output_types:\n        com_writer = ImageSequenceWriter(os.path.join(args.output_dir, 'com'), 'png')\n    if 'pha' in args.output_types:\n        pha_writer = ImageSequenceWriter(os.path.join(args.output_dir, 'pha'), 'jpg')\n    if 'fgr' in args.output_types:\n        fgr_writer = ImageSequenceWriter(os.path.join(args.output_dir, 'fgr'), 'jpg')\n    if 'err' in args.output_types:\n        err_writer = ImageSequenceWriter(os.path.join(args.output_dir, 'err'), 'jpg')\n    if 'ref' in args.output_types:\n        ref_writer = ImageSequenceWriter(os.path.join(args.output_dir, 'ref'), 'jpg')\n    \n\n# Conversion loop\nwith torch.no_grad():\n    for input_batch in tqdm(DataLoader(dataset, batch_size=1, pin_memory=True)):\n        if args.video_target_bgr:\n            (src, bgr), tgt_bgr = input_batch\n            tgt_bgr = tgt_bgr.to(device, non_blocking=True)\n        else:\n            src, bgr = input_batch\n            tgt_bgr = torch.tensor([120/255, 255/255, 155/255], device=device).view(1, 3, 1, 1)\n        src = src.to(device, non_blocking=True)\n        bgr = bgr.to(device, non_blocking=True)\n        \n        if args.model_type == 'mattingbase':\n            pha, fgr, err, _ = model(src, bgr)\n        elif args.model_type == 'mattingrefine':\n            pha, fgr, _, _, err, ref = model(src, bgr)\n        elif args.model_type == 'mattingbm':\n            pha, fgr = model(src, bgr)\n\n        if 'com' in args.output_types:\n            if args.output_format == 'video':\n                # Output composite with green background\n                com = fgr * pha + tgt_bgr * (1 - pha)\n                com_writer.add_batch(com)\n            else:\n                # Output composite as rgba png images\n                com = torch.cat([fgr * pha.ne(0), pha], dim=1)\n                com_writer.add_batch(com)\n        if 'pha' in args.output_types:\n            pha_writer.add_batch(pha)\n        if 'fgr' in args.output_types:\n            fgr_writer.add_batch(fgr)\n        if 'err' in args.output_types:\n            err_writer.add_batch(F.interpolate(err, src.shape[2:], mode='bilinear', align_corners=False))\n        if 'ref' in args.output_types:\n            ref_writer.add_batch(F.interpolate(ref, src.shape[2:], mode='nearest'))\n"
        },
        {
          "name": "inference_webcam.py",
          "type": "blob",
          "size": 6.177734375,
          "content": "\"\"\"\nInference on webcams: Use a model on webcam input.\n\nOnce launched, the script is in background collection mode.\nPress B to toggle between background capture mode and matting mode. The frame shown when B is pressed is used as background for matting.\nPress Q to exit.\n\nExample:\n\n    python inference_webcam.py \\\n        --model-type mattingrefine \\\n        --model-backbone resnet50 \\\n        --model-checkpoint \"PATH_TO_CHECKPOINT\" \\\n        --resolution 1280 720\n\n\"\"\"\n\nimport argparse, os, shutil, time\nimport cv2\nimport torch\n\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Compose, ToTensor, Resize\nfrom torchvision.transforms.functional import to_pil_image\nfrom threading import Thread, Lock\nfrom tqdm import tqdm\nfrom PIL import Image\n\nfrom dataset import VideoDataset\nfrom model import MattingBase, MattingRefine\n\n\n# --------------- Arguments ---------------\n\n\nparser = argparse.ArgumentParser(description='Inference from web-cam')\n\nparser.add_argument('--model-type', type=str, required=True, choices=['mattingbase', 'mattingrefine'])\nparser.add_argument('--model-backbone', type=str, required=True, choices=['resnet101', 'resnet50', 'mobilenetv2'])\nparser.add_argument('--model-backbone-scale', type=float, default=0.25)\nparser.add_argument('--model-checkpoint', type=str, required=True)\nparser.add_argument('--model-refine-mode', type=str, default='sampling', choices=['full', 'sampling', 'thresholding'])\nparser.add_argument('--model-refine-sample-pixels', type=int, default=80_000)\nparser.add_argument('--model-refine-threshold', type=float, default=0.7)\n\nparser.add_argument('--hide-fps', action='store_true')\nparser.add_argument('--resolution', type=int, nargs=2, metavar=('width', 'height'), default=(1280, 720))\nargs = parser.parse_args()\n\n\n# ----------- Utility classes -------------\n\n\n# A wrapper that reads data from cv2.VideoCapture in its own thread to optimize.\n# Use .read() in a tight loop to get the newest frame\nclass Camera:\n    def __init__(self, device_id=0, width=1280, height=720):\n        self.capture = cv2.VideoCapture(device_id)\n        self.capture.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n        self.capture.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n        self.width = int(self.capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n        self.height = int(self.capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        # self.capture.set(cv2.CAP_PROP_BUFFERSIZE, 2)\n        self.success_reading, self.frame = self.capture.read()\n        self.read_lock = Lock()\n        self.thread = Thread(target=self.__update, args=())\n        self.thread.daemon = True\n        self.thread.start()\n\n    def __update(self):\n        while self.success_reading:\n            grabbed, frame = self.capture.read()\n            with self.read_lock:\n                self.success_reading = grabbed\n                self.frame = frame\n\n    def read(self):\n        with self.read_lock:\n            frame = self.frame.copy()\n        return frame\n    def __exit__(self, exec_type, exc_value, traceback):\n        self.capture.release()\n\n# An FPS tracker that computes exponentialy moving average FPS\nclass FPSTracker:\n    def __init__(self, ratio=0.5):\n        self._last_tick = None\n        self._avg_fps = None\n        self.ratio = ratio\n    def tick(self):\n        if self._last_tick is None:\n            self._last_tick = time.time()\n            return None\n        t_new = time.time()\n        fps_sample = 1.0 / (t_new - self._last_tick)\n        self._avg_fps = self.ratio * fps_sample + (1 - self.ratio) * self._avg_fps if self._avg_fps is not None else fps_sample\n        self._last_tick = t_new\n        return self.get()\n    def get(self):\n        return self._avg_fps\n\n# Wrapper for playing a stream with cv2.imshow(). It can accept an image and return keypress info for basic interactivity.\n# It also tracks FPS and optionally overlays info onto the stream.\nclass Displayer:\n    def __init__(self, title, width=None, height=None, show_info=True):\n        self.title, self.width, self.height = title, width, height\n        self.show_info = show_info\n        self.fps_tracker = FPSTracker()\n        cv2.namedWindow(self.title, cv2.WINDOW_NORMAL)\n        if width is not None and height is not None:\n            cv2.resizeWindow(self.title, width, height)\n    # Update the currently showing frame and return key press char code\n    def step(self, image):\n        fps_estimate = self.fps_tracker.tick()\n        if self.show_info and fps_estimate is not None:\n            message = f\"{int(fps_estimate)} fps | {self.width}x{self.height}\"\n            cv2.putText(image, message, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0))\n        cv2.imshow(self.title, image)\n        return cv2.waitKey(1) & 0xFF\n\n\n# --------------- Main ---------------\n\n\n# Load model\nif args.model_type == 'mattingbase':\n    model = MattingBase(args.model_backbone)\nif args.model_type == 'mattingrefine':\n    model = MattingRefine(\n        args.model_backbone,\n        args.model_backbone_scale,\n        args.model_refine_mode,\n        args.model_refine_sample_pixels,\n        args.model_refine_threshold)\n\nmodel = model.cuda().eval()\nmodel.load_state_dict(torch.load(args.model_checkpoint), strict=False)\n\n\nwidth, height = args.resolution\ncam = Camera(width=width, height=height)\ndsp = Displayer('MattingV2', cam.width, cam.height, show_info=(not args.hide_fps))\n\ndef cv2_frame_to_cuda(frame):\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    return ToTensor()(Image.fromarray(frame)).unsqueeze_(0).cuda()\n\nwith torch.no_grad():\n    while True:\n        bgr = None\n        while True: # grab bgr\n            frame = cam.read()\n            key = dsp.step(frame)\n            if key == ord('b'):\n                bgr = cv2_frame_to_cuda(cam.read())\n                break\n            elif key == ord('q'):\n                exit()\n        while True: # matting\n            frame = cam.read()\n            src = cv2_frame_to_cuda(frame)\n            pha, fgr = model(src, bgr)[:2]\n            res = pha * fgr + (1 - pha) * torch.ones_like(fgr)\n            res = res.mul(255).byte().cpu().permute(0, 2, 3, 1).numpy()[0]\n            res = cv2.cvtColor(res, cv2.COLOR_RGB2BGR)\n            key = dsp.step(res)\n            if key == ord('b'):\n                break\n            elif key == ord('q'):\n                exit()\n"
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1171875,
          "content": "kornia==0.4.1\ntensorboard==2.3.0\ntorch==1.7.0\ntorchvision==0.8.1\ntqdm==4.51.0\nopencv-python==4.4.0.44\nonnxruntime==1.6.0"
        },
        {
          "name": "train_base.py",
          "type": "blob",
          "size": 10.6376953125,
          "content": "\"\"\"\nTrain MattingBase\n\nYou can download pretrained DeepLabV3 weights from <https://github.com/VainF/DeepLabV3Plus-Pytorch>\n\nExample:\n\n    CUDA_VISIBLE_DEVICES=0 python train_base.py \\\n        --dataset-name videomatte240k \\\n        --model-backbone resnet50 \\\n        --model-name mattingbase-resnet50-videomatte240k \\\n        --model-pretrain-initialization \"pretraining/best_deeplabv3_resnet50_voc_os16.pth\" \\\n        --epoch-end 8\n\n\"\"\"\n\nimport argparse\nimport kornia\nimport torch\nimport os\nimport random\n\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nfrom torchvision.utils import make_grid\nfrom tqdm import tqdm\nfrom torchvision import transforms as T\nfrom PIL import Image\n\nfrom data_path import DATA_PATH\nfrom dataset import ImagesDataset, ZipDataset, VideoDataset, SampleDataset\nfrom dataset import augmentation as A\nfrom model import MattingBase\nfrom model.utils import load_matched_state_dict\n\n\n# --------------- Arguments ---------------\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--dataset-name', type=str, required=True, choices=DATA_PATH.keys())\n\nparser.add_argument('--model-backbone', type=str, required=True, choices=['resnet101', 'resnet50', 'mobilenetv2'])\nparser.add_argument('--model-name', type=str, required=True)\nparser.add_argument('--model-pretrain-initialization', type=str, default=None)\nparser.add_argument('--model-last-checkpoint', type=str, default=None)\n\nparser.add_argument('--batch-size', type=int, default=8)\nparser.add_argument('--num-workers', type=int, default=16)\nparser.add_argument('--epoch-start', type=int, default=0)\nparser.add_argument('--epoch-end', type=int, required=True)\n\nparser.add_argument('--log-train-loss-interval', type=int, default=10)\nparser.add_argument('--log-train-images-interval', type=int, default=2000)\nparser.add_argument('--log-valid-interval', type=int, default=5000)\n\nparser.add_argument('--checkpoint-interval', type=int, default=5000)\n\nargs = parser.parse_args()\n\n\n# --------------- Loading ---------------\n\n\ndef train():\n    \n    # Training DataLoader\n    dataset_train = ZipDataset([\n        ZipDataset([\n            ImagesDataset(DATA_PATH[args.dataset_name]['train']['pha'], mode='L'),\n            ImagesDataset(DATA_PATH[args.dataset_name]['train']['fgr'], mode='RGB'),\n        ], transforms=A.PairCompose([\n            A.PairRandomAffineAndResize((512, 512), degrees=(-5, 5), translate=(0.1, 0.1), scale=(0.4, 1), shear=(-5, 5)),\n            A.PairRandomHorizontalFlip(),\n            A.PairRandomBoxBlur(0.1, 5),\n            A.PairRandomSharpen(0.1),\n            A.PairApplyOnlyAtIndices([1], T.ColorJitter(0.15, 0.15, 0.15, 0.05)),\n            A.PairApply(T.ToTensor())\n        ]), assert_equal_length=True),\n        ImagesDataset(DATA_PATH['backgrounds']['train'], mode='RGB', transforms=T.Compose([\n            A.RandomAffineAndResize((512, 512), degrees=(-5, 5), translate=(0.1, 0.1), scale=(1, 2), shear=(-5, 5)),\n            T.RandomHorizontalFlip(),\n            A.RandomBoxBlur(0.1, 5),\n            A.RandomSharpen(0.1),\n            T.ColorJitter(0.15, 0.15, 0.15, 0.05),\n            T.ToTensor()\n        ])),\n    ])\n    dataloader_train = DataLoader(dataset_train,\n                                  shuffle=True,\n                                  batch_size=args.batch_size,\n                                  num_workers=args.num_workers,\n                                  pin_memory=True)\n    \n    # Validation DataLoader\n    dataset_valid = ZipDataset([\n        ZipDataset([\n            ImagesDataset(DATA_PATH[args.dataset_name]['valid']['pha'], mode='L'),\n            ImagesDataset(DATA_PATH[args.dataset_name]['valid']['fgr'], mode='RGB')\n        ], transforms=A.PairCompose([\n            A.PairRandomAffineAndResize((512, 512), degrees=(-5, 5), translate=(0.1, 0.1), scale=(0.3, 1), shear=(-5, 5)),\n            A.PairApply(T.ToTensor())\n        ]), assert_equal_length=True),\n        ImagesDataset(DATA_PATH['backgrounds']['valid'], mode='RGB', transforms=T.Compose([\n            A.RandomAffineAndResize((512, 512), degrees=(-5, 5), translate=(0.1, 0.1), scale=(1, 1.2), shear=(-5, 5)),\n            T.ToTensor()\n        ])),\n    ])\n    dataset_valid = SampleDataset(dataset_valid, 50)\n    dataloader_valid = DataLoader(dataset_valid,\n                                  pin_memory=True,\n                                  batch_size=args.batch_size,\n                                  num_workers=args.num_workers)\n\n    # Model\n    model = MattingBase(args.model_backbone).cuda()\n\n    if args.model_last_checkpoint is not None:\n        load_matched_state_dict(model, torch.load(args.model_last_checkpoint))\n    elif args.model_pretrain_initialization is not None:\n        model.load_pretrained_deeplabv3_state_dict(torch.load(args.model_pretrain_initialization)['model_state'])\n\n    optimizer = Adam([\n        {'params': model.backbone.parameters(), 'lr': 1e-4},\n        {'params': model.aspp.parameters(), 'lr': 5e-4},\n        {'params': model.decoder.parameters(), 'lr': 5e-4}\n    ])\n    scaler = GradScaler()\n\n    # Logging and checkpoints\n    if not os.path.exists(f'checkpoint/{args.model_name}'):\n        os.makedirs(f'checkpoint/{args.model_name}')\n    writer = SummaryWriter(f'log/{args.model_name}')\n    \n    # Run loop\n    for epoch in range(args.epoch_start, args.epoch_end):\n        for i, ((true_pha, true_fgr), true_bgr) in enumerate(tqdm(dataloader_train)):\n            step = epoch * len(dataloader_train) + i\n\n            true_pha = true_pha.cuda(non_blocking=True)\n            true_fgr = true_fgr.cuda(non_blocking=True)\n            true_bgr = true_bgr.cuda(non_blocking=True)\n            true_pha, true_fgr, true_bgr = random_crop(true_pha, true_fgr, true_bgr)\n            \n            true_src = true_bgr.clone()\n            \n            # Augment with shadow\n            aug_shadow_idx = torch.rand(len(true_src)) < 0.3\n            if aug_shadow_idx.any():\n                aug_shadow = true_pha[aug_shadow_idx].mul(0.3 * random.random())\n                aug_shadow = T.RandomAffine(degrees=(-5, 5), translate=(0.2, 0.2), scale=(0.5, 1.5), shear=(-5, 5))(aug_shadow)\n                aug_shadow = kornia.filters.box_blur(aug_shadow, (random.choice(range(20, 40)),) * 2)\n                true_src[aug_shadow_idx] = true_src[aug_shadow_idx].sub_(aug_shadow).clamp_(0, 1)\n                del aug_shadow\n            del aug_shadow_idx\n            \n            # Composite foreground onto source\n            true_src = true_fgr * true_pha + true_src * (1 - true_pha)\n\n            # Augment with noise\n            aug_noise_idx = torch.rand(len(true_src)) < 0.4\n            if aug_noise_idx.any():\n                true_src[aug_noise_idx] = true_src[aug_noise_idx].add_(torch.randn_like(true_src[aug_noise_idx]).mul_(0.03 * random.random())).clamp_(0, 1)\n                true_bgr[aug_noise_idx] = true_bgr[aug_noise_idx].add_(torch.randn_like(true_bgr[aug_noise_idx]).mul_(0.03 * random.random())).clamp_(0, 1)\n            del aug_noise_idx\n            \n            # Augment background with jitter\n            aug_jitter_idx = torch.rand(len(true_src)) < 0.8\n            if aug_jitter_idx.any():\n                true_bgr[aug_jitter_idx] = kornia.augmentation.ColorJitter(0.18, 0.18, 0.18, 0.1)(true_bgr[aug_jitter_idx])\n            del aug_jitter_idx\n            \n            # Augment background with affine\n            aug_affine_idx = torch.rand(len(true_bgr)) < 0.3\n            if aug_affine_idx.any():\n                true_bgr[aug_affine_idx] = T.RandomAffine(degrees=(-1, 1), translate=(0.01, 0.01))(true_bgr[aug_affine_idx])\n            del aug_affine_idx\n\n            with autocast():\n                pred_pha, pred_fgr, pred_err = model(true_src, true_bgr)[:3]\n                loss = compute_loss(pred_pha, pred_fgr, pred_err, true_pha, true_fgr)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n            if (i + 1) % args.log_train_loss_interval == 0:\n                writer.add_scalar('loss', loss, step)\n\n            if (i + 1) % args.log_train_images_interval == 0:\n                writer.add_image('train_pred_pha', make_grid(pred_pha, nrow=5), step)\n                writer.add_image('train_pred_fgr', make_grid(pred_fgr, nrow=5), step)\n                writer.add_image('train_pred_com', make_grid(pred_fgr * pred_pha, nrow=5), step)\n                writer.add_image('train_pred_err', make_grid(pred_err, nrow=5), step)\n                writer.add_image('train_true_src', make_grid(true_src, nrow=5), step)\n                writer.add_image('train_true_bgr', make_grid(true_bgr, nrow=5), step)\n                \n            del true_pha, true_fgr, true_bgr\n            del pred_pha, pred_fgr, pred_err\n\n            if (i + 1) % args.log_valid_interval == 0:\n                valid(model, dataloader_valid, writer, step)\n\n            if (step + 1) % args.checkpoint_interval == 0:\n                torch.save(model.state_dict(), f'checkpoint/{args.model_name}/epoch-{epoch}-iter-{step}.pth')\n\n        torch.save(model.state_dict(), f'checkpoint/{args.model_name}/epoch-{epoch}.pth')\n\n\n# --------------- Utils ---------------\n\n\ndef compute_loss(pred_pha, pred_fgr, pred_err, true_pha, true_fgr):\n    true_err = torch.abs(pred_pha.detach() - true_pha)\n    true_msk = true_pha != 0\n    return F.l1_loss(pred_pha, true_pha) + \\\n           F.l1_loss(kornia.sobel(pred_pha), kornia.sobel(true_pha)) + \\\n           F.l1_loss(pred_fgr * true_msk, true_fgr * true_msk) + \\\n           F.mse_loss(pred_err, true_err)\n\n\ndef random_crop(*imgs):\n    w = random.choice(range(256, 512))\n    h = random.choice(range(256, 512))\n    results = []\n    for img in imgs:\n        img = kornia.resize(img, (max(h, w), max(h, w)))\n        img = kornia.center_crop(img, (h, w))\n        results.append(img)\n    return results\n\n\ndef valid(model, dataloader, writer, step):\n    model.eval()\n    loss_total = 0\n    loss_count = 0\n    with torch.no_grad():\n        for (true_pha, true_fgr), true_bgr in dataloader:\n            batch_size = true_pha.size(0)\n            \n            true_pha = true_pha.cuda(non_blocking=True)\n            true_fgr = true_fgr.cuda(non_blocking=True)\n            true_bgr = true_bgr.cuda(non_blocking=True)\n            true_src = true_pha * true_fgr + (1 - true_pha) * true_bgr\n\n            pred_pha, pred_fgr, pred_err = model(true_src, true_bgr)[:3]\n            loss = compute_loss(pred_pha, pred_fgr, pred_err, true_pha, true_fgr)\n            loss_total += loss.cpu().item() * batch_size\n            loss_count += batch_size\n\n    writer.add_scalar('valid_loss', loss_total / loss_count, step)\n    model.train()\n\n\n# --------------- Start ---------------\n\n\nif __name__ == '__main__':\n    train()\n"
        },
        {
          "name": "train_refine.py",
          "type": "blob",
          "size": 13.3876953125,
          "content": "\"\"\"\nTrain MattingRefine\n\nSupports multi-GPU training with DistributedDataParallel() and SyncBatchNorm.\nSelect GPUs through CUDA_VISIBLE_DEVICES environment variable.\n\nExample:\n\n    CUDA_VISIBLE_DEVICES=0,1 python train_refine.py \\\n        --dataset-name videomatte240k \\\n        --model-backbone resnet50 \\\n        --model-name mattingrefine-resnet50-videomatte240k \\\n        --model-last-checkpoint \"PATH_TO_LAST_CHECKPOINT\" \\\n        --epoch-end 1\n\n\"\"\"\n\nimport argparse\nimport kornia\nimport torch\nimport os\nimport random\n\nfrom torch import nn\nfrom torch import distributed as dist\nfrom torch import multiprocessing as mp\nfrom torch.nn import functional as F\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader, Subset\nfrom torch.optim import Adam\nfrom torchvision.utils import make_grid\nfrom tqdm import tqdm\nfrom torchvision import transforms as T\nfrom PIL import Image\n\nfrom data_path import DATA_PATH\nfrom dataset import ImagesDataset, ZipDataset, VideoDataset, SampleDataset\nfrom dataset import augmentation as A\nfrom model import MattingRefine\nfrom model.utils import load_matched_state_dict\n\n\n# --------------- Arguments ---------------\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--dataset-name', type=str, required=True, choices=DATA_PATH.keys())\n\nparser.add_argument('--model-backbone', type=str, required=True, choices=['resnet101', 'resnet50', 'mobilenetv2'])\nparser.add_argument('--model-backbone-scale', type=float, default=0.25)\nparser.add_argument('--model-refine-mode', type=str, default='sampling', choices=['full', 'sampling', 'thresholding'])\nparser.add_argument('--model-refine-sample-pixels', type=int, default=80_000)\nparser.add_argument('--model-refine-thresholding', type=float, default=0.7)\nparser.add_argument('--model-refine-kernel-size', type=int, default=3, choices=[1, 3])\nparser.add_argument('--model-name', type=str, required=True)\nparser.add_argument('--model-last-checkpoint', type=str, default=None)\n\nparser.add_argument('--batch-size', type=int, default=4)\nparser.add_argument('--num-workers', type=int, default=16)\nparser.add_argument('--epoch-start', type=int, default=0)\nparser.add_argument('--epoch-end', type=int, required=True)\n\nparser.add_argument('--log-train-loss-interval', type=int, default=10)\nparser.add_argument('--log-train-images-interval', type=int, default=1000)\nparser.add_argument('--log-valid-interval', type=int, default=2000)\n\nparser.add_argument('--checkpoint-interval', type=int, default=2000)\n\nargs = parser.parse_args()\n\n\ndistributed_num_gpus = torch.cuda.device_count()\nassert args.batch_size % distributed_num_gpus == 0\n\n\n# --------------- Main ---------------\n\ndef train_worker(rank, addr, port):\n    \n    # Distributed Setup\n    os.environ['MASTER_ADDR'] = addr\n    os.environ['MASTER_PORT'] = port\n    dist.init_process_group(\"nccl\", rank=rank, world_size=distributed_num_gpus)\n    \n    # Training DataLoader\n    dataset_train = ZipDataset([\n        ZipDataset([\n            ImagesDataset(DATA_PATH[args.dataset_name]['train']['pha'], mode='L'),\n            ImagesDataset(DATA_PATH[args.dataset_name]['train']['fgr'], mode='RGB'),\n        ], transforms=A.PairCompose([\n            A.PairRandomAffineAndResize((2048, 2048), degrees=(-5, 5), translate=(0.1, 0.1), scale=(0.3, 1), shear=(-5, 5)),\n            A.PairRandomHorizontalFlip(),\n            A.PairRandomBoxBlur(0.1, 5),\n            A.PairRandomSharpen(0.1),\n            A.PairApplyOnlyAtIndices([1], T.ColorJitter(0.15, 0.15, 0.15, 0.05)),\n            A.PairApply(T.ToTensor())\n        ]), assert_equal_length=True),\n        ImagesDataset(DATA_PATH['backgrounds']['train'], mode='RGB', transforms=T.Compose([\n            A.RandomAffineAndResize((2048, 2048), degrees=(-5, 5), translate=(0.1, 0.1), scale=(1, 2), shear=(-5, 5)),\n            T.RandomHorizontalFlip(),\n            A.RandomBoxBlur(0.1, 5),\n            A.RandomSharpen(0.1),\n            T.ColorJitter(0.15, 0.15, 0.15, 0.05),\n            T.ToTensor()\n        ])),\n    ])\n    dataset_train_len_per_gpu_worker = int(len(dataset_train) / distributed_num_gpus)\n    dataset_train = Subset(dataset_train, range(rank * dataset_train_len_per_gpu_worker, (rank + 1) * dataset_train_len_per_gpu_worker))\n    dataloader_train = DataLoader(dataset_train,\n                                  shuffle=True,\n                                  pin_memory=True,\n                                  drop_last=True,\n                                  batch_size=args.batch_size // distributed_num_gpus,\n                                  num_workers=args.num_workers // distributed_num_gpus)\n    \n    # Validation DataLoader\n    if rank == 0:\n        dataset_valid = ZipDataset([\n            ZipDataset([\n                ImagesDataset(DATA_PATH[args.dataset_name]['valid']['pha'], mode='L'),\n                ImagesDataset(DATA_PATH[args.dataset_name]['valid']['fgr'], mode='RGB')\n            ], transforms=A.PairCompose([\n                A.PairRandomAffineAndResize((2048, 2048), degrees=(-5, 5), translate=(0.1, 0.1), scale=(0.3, 1), shear=(-5, 5)),\n                A.PairApply(T.ToTensor())\n            ]), assert_equal_length=True),\n            ImagesDataset(DATA_PATH['backgrounds']['valid'], mode='RGB', transforms=T.Compose([\n                A.RandomAffineAndResize((2048, 2048), degrees=(-5, 5), translate=(0.1, 0.1), scale=(1, 1.2), shear=(-5, 5)),\n                T.ToTensor()\n            ])),\n        ])\n        dataset_valid = SampleDataset(dataset_valid, 50)\n        dataloader_valid = DataLoader(dataset_valid,\n                                      pin_memory=True,\n                                      drop_last=True,\n                                      batch_size=args.batch_size // distributed_num_gpus,\n                                      num_workers=args.num_workers // distributed_num_gpus)\n    \n    # Model\n    model = MattingRefine(args.model_backbone,\n                          args.model_backbone_scale,\n                          args.model_refine_mode,\n                          args.model_refine_sample_pixels,\n                          args.model_refine_thresholding,\n                          args.model_refine_kernel_size).to(rank)\n    model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n    model_distributed = nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n    \n    if args.model_last_checkpoint is not None:\n        load_matched_state_dict(model, torch.load(args.model_last_checkpoint))\n\n    optimizer = Adam([\n        {'params': model.backbone.parameters(), 'lr': 5e-5},\n        {'params': model.aspp.parameters(), 'lr': 5e-5},\n        {'params': model.decoder.parameters(), 'lr': 1e-4},\n        {'params': model.refiner.parameters(), 'lr': 3e-4},\n    ])\n    scaler = GradScaler()\n    \n    # Logging and checkpoints\n    if rank == 0:\n        if not os.path.exists(f'checkpoint/{args.model_name}'):\n            os.makedirs(f'checkpoint/{args.model_name}')\n        writer = SummaryWriter(f'log/{args.model_name}')\n    \n    # Run loop\n    for epoch in range(args.epoch_start, args.epoch_end):\n        for i, ((true_pha, true_fgr), true_bgr) in enumerate(tqdm(dataloader_train)):\n            step = epoch * len(dataloader_train) + i\n\n            true_pha = true_pha.to(rank, non_blocking=True)\n            true_fgr = true_fgr.to(rank, non_blocking=True)\n            true_bgr = true_bgr.to(rank, non_blocking=True)\n            true_pha, true_fgr, true_bgr = random_crop(true_pha, true_fgr, true_bgr)\n            \n            true_src = true_bgr.clone()\n            \n            # Augment with shadow\n            aug_shadow_idx = torch.rand(len(true_src)) < 0.3\n            if aug_shadow_idx.any():\n                aug_shadow = true_pha[aug_shadow_idx].mul(0.3 * random.random())\n                aug_shadow = T.RandomAffine(degrees=(-5, 5), translate=(0.2, 0.2), scale=(0.5, 1.5), shear=(-5, 5))(aug_shadow)\n                aug_shadow = kornia.filters.box_blur(aug_shadow, (random.choice(range(20, 40)),) * 2)\n                true_src[aug_shadow_idx] = true_src[aug_shadow_idx].sub_(aug_shadow).clamp_(0, 1)\n                del aug_shadow\n            del aug_shadow_idx\n            \n            # Composite foreground onto source\n            true_src = true_fgr * true_pha + true_src * (1 - true_pha)\n\n            # Augment with noise\n            aug_noise_idx = torch.rand(len(true_src)) < 0.4\n            if aug_noise_idx.any():\n                true_src[aug_noise_idx] = true_src[aug_noise_idx].add_(torch.randn_like(true_src[aug_noise_idx]).mul_(0.03 * random.random())).clamp_(0, 1)\n                true_bgr[aug_noise_idx] = true_bgr[aug_noise_idx].add_(torch.randn_like(true_bgr[aug_noise_idx]).mul_(0.03 * random.random())).clamp_(0, 1)\n            del aug_noise_idx\n            \n            # Augment background with jitter\n            aug_jitter_idx = torch.rand(len(true_src)) < 0.8\n            if aug_jitter_idx.any():\n                true_bgr[aug_jitter_idx] = kornia.augmentation.ColorJitter(0.18, 0.18, 0.18, 0.1)(true_bgr[aug_jitter_idx])\n            del aug_jitter_idx\n            \n            # Augment background with affine\n            aug_affine_idx = torch.rand(len(true_bgr)) < 0.3\n            if aug_affine_idx.any():\n                true_bgr[aug_affine_idx] = T.RandomAffine(degrees=(-1, 1), translate=(0.01, 0.01))(true_bgr[aug_affine_idx])\n            del aug_affine_idx\n            \n            with autocast():\n                pred_pha, pred_fgr, pred_pha_sm, pred_fgr_sm, pred_err_sm, _ = model_distributed(true_src, true_bgr)\n                loss = compute_loss(pred_pha, pred_fgr, pred_pha_sm, pred_fgr_sm, pred_err_sm, true_pha, true_fgr)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n            if rank == 0:\n                if (i + 1) % args.log_train_loss_interval == 0:\n                    writer.add_scalar('loss', loss, step)\n\n                if (i + 1) % args.log_train_images_interval == 0:\n                    writer.add_image('train_pred_pha', make_grid(pred_pha, nrow=5), step)\n                    writer.add_image('train_pred_fgr', make_grid(pred_fgr, nrow=5), step)\n                    writer.add_image('train_pred_com', make_grid(pred_fgr * pred_pha, nrow=5), step)\n                    writer.add_image('train_pred_err', make_grid(pred_err_sm, nrow=5), step)\n                    writer.add_image('train_true_src', make_grid(true_src, nrow=5), step)\n\n                del true_pha, true_fgr, true_src, true_bgr\n                del pred_pha, pred_fgr, pred_pha_sm, pred_fgr_sm, pred_err_sm\n\n                if (i + 1) % args.log_valid_interval == 0:\n                    valid(model, dataloader_valid, writer, step)\n\n                if (step + 1) % args.checkpoint_interval == 0:\n                    torch.save(model.state_dict(), f'checkpoint/{args.model_name}/epoch-{epoch}-iter-{step}.pth')\n                    \n        if rank == 0:\n            torch.save(model.state_dict(), f'checkpoint/{args.model_name}/epoch-{epoch}.pth')\n            \n    # Clean up\n    dist.destroy_process_group()\n            \n            \n# --------------- Utils ---------------\n\n\ndef compute_loss(pred_pha_lg, pred_fgr_lg, pred_pha_sm, pred_fgr_sm, pred_err_sm, true_pha_lg, true_fgr_lg):\n    true_pha_sm = kornia.resize(true_pha_lg, pred_pha_sm.shape[2:])\n    true_fgr_sm = kornia.resize(true_fgr_lg, pred_fgr_sm.shape[2:])\n    true_msk_lg = true_pha_lg != 0\n    true_msk_sm = true_pha_sm != 0\n    return F.l1_loss(pred_pha_lg, true_pha_lg) + \\\n           F.l1_loss(pred_pha_sm, true_pha_sm) + \\\n           F.l1_loss(kornia.sobel(pred_pha_lg), kornia.sobel(true_pha_lg)) + \\\n           F.l1_loss(kornia.sobel(pred_pha_sm), kornia.sobel(true_pha_sm)) + \\\n           F.l1_loss(pred_fgr_lg * true_msk_lg, true_fgr_lg * true_msk_lg) + \\\n           F.l1_loss(pred_fgr_sm * true_msk_sm, true_fgr_sm * true_msk_sm) + \\\n           F.mse_loss(kornia.resize(pred_err_sm, true_pha_lg.shape[2:]), \\\n                      kornia.resize(pred_pha_sm, true_pha_lg.shape[2:]).sub(true_pha_lg).abs())\n\n\ndef random_crop(*imgs):\n    H_src, W_src = imgs[0].shape[2:]\n    W_tgt = random.choice(range(1024, 2048)) // 4 * 4\n    H_tgt = random.choice(range(1024, 2048)) // 4 * 4\n    scale = max(W_tgt / W_src, H_tgt / H_src)\n    results = []\n    for img in imgs:\n        img = kornia.resize(img, (int(H_src * scale), int(W_src * scale)))\n        img = kornia.center_crop(img, (H_tgt, W_tgt))\n        results.append(img)\n    return results\n\n\ndef valid(model, dataloader, writer, step):\n    model.eval()\n    loss_total = 0\n    loss_count = 0\n    with torch.no_grad():\n        for (true_pha, true_fgr), true_bgr in dataloader:\n            batch_size = true_pha.size(0)\n            \n            true_pha = true_pha.cuda(non_blocking=True)\n            true_fgr = true_fgr.cuda(non_blocking=True)\n            true_bgr = true_bgr.cuda(non_blocking=True)\n            true_src = true_pha * true_fgr + (1 - true_pha) * true_bgr\n\n            pred_pha, pred_fgr, pred_pha_sm, pred_fgr_sm, pred_err_sm, _ = model(true_src, true_bgr)\n            loss = compute_loss(pred_pha, pred_fgr, pred_pha_sm, pred_fgr_sm, pred_err_sm, true_pha, true_fgr)\n            loss_total += loss.cpu().item() * batch_size\n            loss_count += batch_size\n\n    writer.add_scalar('valid_loss', loss_total / loss_count, step)\n    model.train()\n\n\n# --------------- Start ---------------\n\n\nif __name__ == '__main__':\n    addr = 'localhost'\n    port = str(random.choice(range(12300, 12400))) # pick a random port.\n    mp.spawn(train_worker,\n             nprocs=distributed_num_gpus,\n             args=(addr, port),\n             join=True)\n"
        }
      ]
    }
  ]
}