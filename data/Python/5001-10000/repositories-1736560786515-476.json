{
  "metadata": {
    "timestamp": 1736560786515,
    "page": 476,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/metaseq",
      "stars": 6518,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.189453125,
          "content": "[flake8]\nextend-ignore =\n    # E203: whitespace with black\n    E203\n    # E741: \"l\" is ambiguous\n    E741\n    # F541: f-string is missing placeholders\n    F541\n# github size\nmax-line-length=127\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.880859375,
          "content": "# JetBrains PyCharm IDE\n.idea/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# macOS dir files\n.DS_Store\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# Checkpoints\ncheckpoints\n\n# slurm snap shot\nslurm_snapshot_code/\nslurm_snapshot_code_oss/\nslurm_snapshot_code_internal/\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# Generated files\n/metaseq/temporal_convolution_tbc\n/metaseq/modules/*_layer/*_forward.cu\n/metaseq/modules/*_layer/*_backward.cu\n/metaseq/version.py\n\n# data\ndata-bin/\n\n# reranking\n/examples/reranking/rerank_data\n\n# Cython-generated C++ source files\n/metaseq/data/data_utils_fast.cpp\n/metaseq/data/token_block_utils_fast.cpp\n\n# VSCODE\n.vscode/ftp-sync.json\n.vscode/settings.json\n.vscode/launch.json\n\n# Experimental Folder\nexperimental/*\n\n# Weights and Biases logs\nwandb/\n\n# Debug results dir\n_results/*\n\n# vim temporary files\n*.swp\n\nmetaseq/fair_internal/scripts/azure/nccl_tests/bin/*\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.2275390625,
          "content": "[submodule \"third_party/clip\"]\n\tpath = third_party/clip\n\turl = https://github.com/openai/CLIP.git\n[submodule \"third_party/stable-diffusion\"]\n\tpath = third_party/stable-diffusion\n\turl = https://github.com/CompVis/stable-diffusion.git\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.1904296875,
          "content": "repos:\n  - repo: local\n    hooks:\n      - id: black\n        name: Black Python code formatting\n        entry: bash -c 'black \"$@\"; git add -u' --\n        language: python\n        types: [python]\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 0.2626953125,
          "content": "# Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [0.0.x] - TBD\n"
        },
        {
          "name": "CODEOWNERS",
          "type": "blob",
          "size": 0.1875,
          "content": "*       @suchenzang @ngoyal2707 @punitkoura @moyapchen @klshuster @davides @igormolybogFB @Xirider @andrewPoulton @bashnick @tangbinh @urielsinger @zycalice @ArmenAg @lilisierrayu @adampolyak\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.451171875,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\nThis Code of Conduct also applies outside the project spaces when there is a\nreasonable belief that an individual's behavior may have a negative impact on\nthe project or its community.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <opensource-conduct@fb.com>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.6328125,
          "content": "FROM nvidia/cuda:11.3.1-devel-ubuntu20.04\n\nARG DEBIAN_FRONTEND=noninteractive\n\nRUN mkdir -p /build\nWORKDIR /build\n\nRUN apt-key del 7fa2af80 && \\\n    apt-get -qq update && \\\n    apt-get -qq install -y --no-install-recommends curl && \\\n    curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-keyring_1.0-1_all.deb && \\\n    dpkg -i cuda-keyring_1.0-1_all.deb\n\nRUN apt-get -qq update \\\n    && apt-get -qq install -y --no-install-recommends \\\n    git \\\n    python3-pip python3-dev\n\n# Install Pytorch\nRUN pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n\n# Install APEX\nRUN git clone https://github.com/NVIDIA/apex.git\nWORKDIR /build/apex\n\nRUN git checkout 265b451de8ba9bfcb67edc7360f3d8772d0a8bea\nRUN pip3 install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" --global-option=\"--deprecated_fused_adam\" --global-option=\"--xentropy\" --global-option=\"--fast_multihead_attn\" ./\n\n# Install Megatron-LM branch\nWORKDIR /build\n\nRUN git clone --branch fairseq_v3 https://github.com/ngoyal2707/Megatron-LM.git\nWORKDIR /build/Megatron-LM\nRUN pip3 install six regex\nRUN pip3 install -e .\n\n# Install Fairscale\nWORKDIR /build\n\nRUN git clone --branch prefetch_fsdp_params_simple https://github.com/facebookresearch/fairscale.git\nWORKDIR /build/fairscale\nRUN git checkout fixing_memory_issues_with_keeping_overlap_may24\nRUN pip3 install -e .\n\n# Install metaseq\nWORKDIR /build\nRUN git clone https://github.com/facebookresearch/metaseq.git\nWORKDIR /build/metaseq\nRUN pip3 install -e .\n# turn on pre-commit hooks\nRUN pre-commit install\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0830078125,
          "content": "MIT License\n\nCopyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.9580078125,
          "content": "\n\n# Metaseq\nA codebase for working with [Open Pre-trained Transformers](projects/OPT), originally forked from [fairseq](https://github.com/facebookresearch/fairseq).\n\n\n## Community Integrations\n\n### Using OPT with ðŸ¤— Transformers\n\nThe OPT 125M--66B models are now available in [Hugging Face Transformers](https://github.com/huggingface/transformers/releases/tag/v4.19.0). You can access them under the `facebook` organization on the [Hugging Face Hub](https://huggingface.co/facebook)\n\n### Using OPT-175B with Alpa\n\nThe OPT 125M--175B models are now supported in the [Alpa project](https://alpa-projects.github.io/tutorials/opt_serving.html), which \nenables serving OPT-175B with more flexible parallelisms on older generations of GPUs, such as 40GB A100, V100, T4, M60, etc.\n\n### Using OPT with Colossal-AI\n\nThe OPT models are now supported in the [Colossal-AI](https://github.com/hpcaitech/ColossalAI#OPT), which helps users to efficiently and quickly deploy OPT models training and inference, reducing large AI model budgets and scaling down the labor cost of learning and deployment.\n\n### Using OPT with CTranslate2\n\nThe OPT 125M--66B models can be executed with [CTranslate2](https://github.com/OpenNMT/CTranslate2/), which is a fast inference engine for Transformer models. The project integrates the [SmoothQuant](https://github.com/mit-han-lab/smoothquant) technique to allow 8-bit quantization of OPT models. See the [usage example](https://opennmt.net/CTranslate2/guides/transformers.html#opt) to get started.\n\n### Using OPT with FasterTransformer\n\nThe OPT models can be served with [FasterTransformer](https://github.com/NVIDIA/FasterTransformer), a highly optimized inference framework written and maintained by NVIDIA. We provide instructions to convert OPT checkpoints into FasterTransformer format and [a usage example](docs/faster-transformer.md) with some benchmark results.\n\n### Using OPT with DeepSpeed\n\nThe OPT models can be finetuned using [DeepSpeed](https://github.com/microsoft/DeepSpeed). See the [DeepSpeed-Chat example](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat) to get started.\n\n## Getting Started in Metaseq\nFollow [setup instructions here](docs/setup.md) to get started.\n\n### Documentation on workflows\n* [Training](docs/training.md)\n* [API](docs/api.md)\n\n### Background Info\n* [Background & relationship to fairseq](docs/history.md)\n* [Chronicles of training OPT-175B](projects/OPT/chronicles/README.md)\n\n## Support\nIf you have any questions, bug reports, or feature requests regarding either the codebase or the models released in the projects section, please don't hesitate to post on our [Github Issues page](https://github.com/facebookresearch/metaseq/issues).\n\nPlease remember to follow our [Code of Conduct](CODE_OF_CONDUCT.md).\n\n## Contributing\nWe welcome PRs from the community!\n\nYou can find information about contributing to metaseq in our [Contributing](docs/CONTRIBUTING.md) document.\n\n## The Team\nMetaseq is currently maintained by the CODEOWNERS: [Susan Zhang](https://github.com/suchenzang), [Naman Goyal](https://github.com/ngoyal2707), [Punit Singh Koura](https://github.com/punitkoura), [Moya Chen](https://github.com/moyapchen), [Kurt Shuster](https://github.com/klshuster), [David Esiobu](https://github.com/davides), [Igor Molybog](https://github.com/igormolybogFB), [Peter Albert](https://github.com/Xirider), [Andrew Poulton](https://github.com/andrewPoulton), [Nikolay Bashlykov](https://github.com/bashnick), [Binh Tang](https://github.com/tangbinh), [Uriel Singer](https://github.com/urielsinger), [Yuchen Zhang](https://github.com/zycalice), [Armen Aghajanya](https://github.com/ArmenAg), [Lili Yu](https://github.com/lilisierrayu), and [Adam Polyak](https://github.com/adampolyak).\n\n## License\n\nThe majority of metaseq is licensed under the MIT license, however portions of the project are available under separate license terms: \n* Megatron-LM is licensed under the [Megatron-LM license](https://github.com/NVIDIA/Megatron-LM/blob/main/LICENSE)\n\n"
        },
        {
          "name": "cpu_tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "gpu_tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "metaseq",
          "type": "tree",
          "content": null
        },
        {
          "name": "mypy.ini",
          "type": "blob",
          "size": 0.111328125,
          "content": "[mypy]\npython_version = 3.8\nignore_missing_imports = true\nfiles = metaseq,tests,cpu_tests,gpu_tests\npretty = false"
        },
        {
          "name": "preprocessing",
          "type": "tree",
          "content": null
        },
        {
          "name": "projects",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 10.63671875,
          "content": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport sys\n\nimport torch\nfrom setuptools import Extension, find_packages, setup\nfrom torch.utils.cpp_extension import (\n    CppExtension,\n    CUDAExtension,\n    BuildExtension,\n    CUDA_HOME,\n)\n\nif sys.version_info < (3, 6):\n    sys.exit(\"Sorry, Python >= 3.6 is required for metaseq.\")\n\n\ndef write_version_py():\n    with open(os.path.join(\"metaseq\", \"version.txt\")) as f:\n        version = f.read().strip()\n\n    # append latest commit hash to version string\n    try:\n        sha = (\n            subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"])\n            .decode(\"ascii\")\n            .strip()\n        )\n        version += \"+\" + sha[:7]\n    except Exception:\n        pass\n\n    # write version info to metaseq/version.py\n    with open(os.path.join(\"metaseq\", \"version.py\"), \"w\") as f:\n        f.write('__version__ = \"{}\"\\n'.format(version))\n    return version\n\n\nversion = write_version_py()\n\nwith open(\"README.md\") as f:\n    readme = f.read()\n\nif sys.platform == \"darwin\":\n    extra_compile_args = [\"-stdlib=libc++\", \"-O3\"]\nelse:\n    extra_compile_args = [\"-std=c++11\", \"-O3\"]\n\n\nclass NumpyExtension(Extension):\n    \"\"\"Source: https://stackoverflow.com/a/54128391\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.__include_dirs = []\n        super().__init__(*args, **kwargs)\n\n    @property\n    def include_dirs(self):\n        import numpy\n\n        return self.__include_dirs + [numpy.get_include()]\n\n    @include_dirs.setter\n    def include_dirs(self, dirs):\n        self.__include_dirs = dirs\n\n\nextension_modules = [\n    NumpyExtension(\n        \"metaseq.data.data_utils_fast\",\n        sources=[\"metaseq/data/data_utils_fast.pyx\"],\n        language=\"c++\",\n        extra_compile_args=extra_compile_args,\n    ),\n    NumpyExtension(\n        \"metaseq.data.token_block_utils_fast\",\n        sources=[\"metaseq/data/token_block_utils_fast.pyx\"],\n        language=\"c++\",\n        extra_compile_args=extra_compile_args,\n    ),\n]\n\n# TODO: Figure out how to actually gate this properly and still get CircleCI to work.\n# By default, include megatron kernels unless --global-option=\"--no_megatron\" is passed in to pip install.\nif \"--no_megatron\" not in sys.argv:\n    # Reference:\n    # https://github.com/ngoyal2707/Megatron-LM/commit/9a16189ab1b5537205c708f8c8f952f2ae2ae72b\n    extension_modules.append(\n        CppExtension(\n            \"metaseq.modules.megatron.fused_kernels.scaled_upper_triang_masked_softmax_cuda\",\n            sources=[\n                \"metaseq/modules/megatron/fused_kernels/scaled_upper_triang_masked_softmax.cpp\",\n                \"metaseq/modules/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu\",\n            ],\n            extra_compile_args={\n                \"cxx\": [\"-O3\"],\n                \"nvcc\": [\n                    \"-O3\",\n                    \"--use_fast_math\",\n                    \"-U__CUDA_NO_HALF_OPERATORS__\",\n                    \"-U__CUDA_NO_HALF_CONVERSIONS__\",\n                    \"--expt-relaxed-constexpr\",\n                    \"--expt-extended-lambda\",\n                ],\n            },\n        )\n    )\n    extension_modules.append(\n        CppExtension(\n            \"metaseq.modules.megatron.fused_kernels.scaled_masked_softmax_cuda\",\n            sources=[\n                \"metaseq/modules/megatron/fused_kernels/scaled_masked_softmax.cpp\",\n                \"metaseq/modules/megatron/fused_kernels/scaled_masked_softmax_cuda.cu\",\n            ],\n            extra_compile_args={\n                \"cxx\": [\"-O3\"],\n                \"nvcc\": [\n                    \"-O3\",\n                    \"--use_fast_math\",\n                    \"-U__CUDA_NO_HALF_OPERATORS__\",\n                    \"-U__CUDA_NO_HALF_CONVERSIONS__\",\n                    \"--expt-relaxed-constexpr\",\n                    \"--expt-extended-lambda\",\n                ],\n            },\n        )\n    )\nelse:\n    print(\"*** Skipping megatron kernel installation... ***\")\n    sys.argv.remove(\"--no_megatron\")\n\n# By default, include apex kernels unless --global-option=\"--no_apex\" is passed in to pip install.\nif \"--no_apex\" not in sys.argv:\n    # TODO[susanz]: not including --no-cache-dir anymore?\n    if CUDA_HOME is None:\n        raise RuntimeError(\n            f\"Building apex kernels was requested, but nvcc was not found. \"\n            \"Are you sure your environment has nvcc available?  \"\n            \"If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, \"\n            \"only images whose names contain 'devel' will provide nvcc.\"\n        )\n\n    print(\"\\n\\ntorch.__version__  = {}\\n\\n\".format(torch.__version__))\n    TORCH_MAJOR = int(torch.__version__.split(\".\")[0])\n    TORCH_MINOR = int(torch.__version__.split(\".\")[1])\n    if TORCH_MAJOR == 0:\n        # TODO[susanz]: Gate this to Pytorch 1.6 or later?\n        raise RuntimeError(\n            \"Apex kernels requires Pytorch 1.0 or later, \"\n            \"found torch.__version__ = {}\".format(torch.__version__)\n        )\n\n    # --global-option=\"--cpp_ext\"\n    extension_modules.append(\n        CppExtension(\n            name=\"apex_C\", sources=[\"metaseq/modules/apex/flatten_unflatten.cpp\"]\n        )\n    )\n\n    # --global-option=\"--cuda_ext\"\n    extension_modules.append(\n        CUDAExtension(\n            name=\"amp_C\",\n            sources=[\n                \"metaseq/modules/apex/amp_C_frontend.cpp\",\n                \"metaseq/modules/apex/multi_tensor_l2norm_kernel.cu\",\n                \"metaseq/modules/apex/multi_tensor_l2norm_kernel_mp.cu\",\n                \"metaseq/modules/apex/multi_tensor_l2norm_scale_kernel.cu\",\n                \"metaseq/modules/apex/multi_tensor_adam.cu\",\n            ],\n            extra_compile_args={\n                \"cxx\": [\"-O3\"],\n                \"nvcc\": [\n                    \"-lineinfo\",\n                    \"-O3\",\n                    \"--use_fast_math\",\n                ],\n            },\n        )\n    )\n    extension_modules.append(\n        CUDAExtension(\n            name=\"fused_layer_norm_cuda\",\n            sources=[\n                \"metaseq/modules/apex/layer_norm_cuda.cpp\",\n                \"metaseq/modules/apex/layer_norm_cuda_kernel.cu\",\n            ],\n            extra_compile_args={\n                \"cxx\": [\"-O3\"],\n                \"nvcc\": [\"-maxrregcount=50\", \"-O3\", \"--use_fast_math\"],\n            },\n        )\n    )\n    extension_modules.append(\n        CUDAExtension(\n            name=\"fused_dense_cuda\",\n            sources=[\n                \"metaseq/modules/apex/fused_dense.cpp\",\n                \"metaseq/modules/apex/fused_dense_cuda.cu\",\n            ],\n            extra_compile_args={\n                \"cxx\": [\"-O3\"],\n                \"nvcc\": [\"-O3\"],\n            },\n        )\n    )\n    # --global-option=\"--deprecated_fused_adam\"\n    extension_modules.append(\n        CUDAExtension(\n            name=\"fused_adam_cuda\",\n            sources=[\n                \"metaseq/modules/apex/fused_adam_cuda.cpp\",\n                \"metaseq/modules/apex/fused_adam_cuda_kernel.cu\",\n            ],\n            # include_dirs=[os.path.join(this_dir, \"csrc\")],\n            extra_compile_args={\n                \"cxx\": [\"-O3\"],\n                \"nvcc\": [\"-O3\", \"--use_fast_math\"],\n            },\n        )\n    )\nelse:\n    print(\"*** Skipping apex kernel installation... ***\")\n    sys.argv.remove(\"--no_apex\")\n\n\nif \"clean\" in sys.argv[1:]:\n    # Source: https://bit.ly/2NLVsgE\n    print(\"deleting Cython files...\")\n    import subprocess\n\n    subprocess.run(\n        [\"rm -f metaseq/*.so metaseq/**/*.so metaseq/*.pyd metaseq/**/*.pyd\"],\n        shell=True,\n    )\n\n\ndef do_setup():\n    setup(\n        name=\"metaseq\",\n        version=version,\n        description=\"MetaSeq, a framework for large language models, from Meta\",\n        url=\"https://github.com/facebookresearch/metaseq\",\n        long_description=readme,\n        long_description_content_type=\"text/markdown\",\n        setup_requires=[\n            \"cython\",\n            'numpy; python_version>=\"3.7\"',\n            \"setuptools>=18.0\",\n        ],\n        install_requires=[\n            # protobuf version pinned due to tensorboard not pinning a version.\n            #  https://github.com/protocolbuffers/protobuf/issues/10076\n            \"protobuf==3.20.2\",\n            # \"click==8.0.4\",\n            \"cython\",\n            'dataclasses; python_version<\"3.7\"',\n            # \"editdistance\",\n            \"fire\",\n            \"flask>=2.2.5\",  # for api\n            \"hydra-core>=1.1.0,<1.2\",\n            \"ipdb\",\n            \"ipython\",\n            \"Jinja2==3.1.1\",  # for evals\n            \"markupsafe\",  # for evals\n            \"more_itertools\",\n            \"ninja\",\n            'numpy; python_version>=\"3.7\"',\n            \"omegaconf<=2.1.1\",\n            \"portalocker>=2.5\",\n            \"pre-commit\",\n            \"pytest\",\n            \"pytest-regressions\",\n            \"regex\",\n            \"scikit-learn\",  # for evals\n            \"sacrebleu\",  # for evals\n            \"tensorboard==2.8.0\",\n            \"timeout-decorator\",\n            \"tokenizers\",\n            \"torch\",\n            \"tqdm\",\n            \"typing_extensions\",\n        ],\n        packages=find_packages(\n            exclude=[\n                \"scripts\",\n                \"scripts.*\",\n                \"tests\",\n                \"tests.*\",\n            ]\n        ),\n        include_package_data=True,\n        zip_safe=False,\n        extras_require={\n            # install via: pip install -e \".[dev]\"\n            \"dev\": [\n                \"flake8\",\n                \"black==22.3.0\",\n                \"aim>=3.9.4\",\n                \"azure-storage-blob\",\n                \"mypy\",\n            ],\n            # install via: pip install -e \".[test]\"\n            \"test\": [\n                \"iopath\",\n                \"transformers\",\n                \"pyarrow\",\n                \"boto3\",\n                \"pandas\",\n                \"bitsandbytes\",\n            ],\n            # install via: pip install -e \".[multimodal]\"\n            \"multimodal\": [\n                \"albumentations\",\n                \"dalle_pytorch\",\n                \"einops\",\n                \"matplotlib==3.5.0\",\n                \"pytorchvideo==0.1.5\",\n                \"wandb\",\n                \"webdataset==0.1.103\",\n            ],\n        },\n        ext_modules=extension_modules,\n        test_suite=\"tests\",\n        entry_points={\n            \"console_scripts\": [\n                \"metaseq-train = metaseq.cli.train:cli_main\",\n                \"metaseq-validate = metaseq.cli.validate:cli_main\",\n                \"opt-baselines = metaseq.launcher.opt_baselines:cli_main\",\n                \"metaseq-api-local = metaseq.cli.interactive_hosted:cli_main\",\n            ],\n        },\n        cmdclass={\"build_ext\": BuildExtension},\n    )\n\n\nif __name__ == \"__main__\":\n    do_setup()\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}