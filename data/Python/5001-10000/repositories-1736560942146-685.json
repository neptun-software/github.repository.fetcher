{
  "metadata": {
    "timestamp": 1736560942146,
    "page": 685,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "yenchenlin/nerf-pytorch",
      "stars": 5596,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.078125,
          "content": "**/.ipynb_checkpoints\n**/__pycache__\n*.png\n*.mp4\n*.npy\n*.npz\n*.dae\ndata/*\nlogs/*"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.037109375,
          "content": "MIT License\n\nCopyright (c) 2020 bmild\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.3076171875,
          "content": "# NeRF-pytorch\n\n\n[NeRF](http://www.matthewtancik.com/nerf) (Neural Radiance Fields) is a method that achieves state-of-the-art results for synthesizing novel views of complex scenes. Here are some videos generated by this repository (pre-trained models are provided below):\n\n![](https://user-images.githubusercontent.com/7057863/78472232-cf374a00-7769-11ea-8871-0bc710951839.gif)\n![](https://user-images.githubusercontent.com/7057863/78472235-d1010d80-7769-11ea-9be9-51365180e063.gif)\n\nThis project is a faithful PyTorch implementation of [NeRF](http://www.matthewtancik.com/nerf) that **reproduces** the results while running **1.3 times faster**. The code is based on authors' Tensorflow implementation [here](https://github.com/bmild/nerf), and has been tested to match it numerically. \n\n## Installation\n\n```\ngit clone https://github.com/yenchenlin/nerf-pytorch.git\ncd nerf-pytorch\npip install -r requirements.txt\n```\n\n<details>\n  <summary> Dependencies (click to expand) </summary>\n  \n  ## Dependencies\n  - PyTorch 1.4\n  - matplotlib\n  - numpy\n  - imageio\n  - imageio-ffmpeg\n  - configargparse\n  \nThe LLFF data loader requires ImageMagick.\n\nYou will also need the [LLFF code](http://github.com/fyusion/llff) (and COLMAP) set up to compute poses if you want to run on your own real data.\n  \n</details>\n\n## How To Run?\n\n### Quick Start\n\nDownload data for two example datasets: `lego` and `fern`\n```\nbash download_example_data.sh\n```\n\nTo train a low-res `lego` NeRF:\n```\npython run_nerf.py --config configs/lego.txt\n```\nAfter training for 100k iterations (~4 hours on a single 2080 Ti), you can find the following video at `logs/lego_test/lego_test_spiral_100000_rgb.mp4`.\n\n![](https://user-images.githubusercontent.com/7057863/78473103-9353b300-7770-11ea-98ed-6ba2d877b62c.gif)\n\n---\n\nTo train a low-res `fern` NeRF:\n```\npython run_nerf.py --config configs/fern.txt\n```\nAfter training for 200k iterations (~8 hours on a single 2080 Ti), you can find the following video at `logs/fern_test/fern_test_spiral_200000_rgb.mp4` and `logs/fern_test/fern_test_spiral_200000_disp.mp4`\n\n![](https://user-images.githubusercontent.com/7057863/78473081-58ea1600-7770-11ea-92ce-2bbf6a3f9add.gif)\n\n---\n\n### More Datasets\nTo play with other scenes presented in the paper, download the data [here](https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1). Place the downloaded dataset according to the following directory structure:\n```\n├── configs                                                                                                       \n│   ├── ...                                                                                     \n│                                                                                               \n├── data                                                                                                                                                                                                       \n│   ├── nerf_llff_data                                                                                                  \n│   │   └── fern                                                                                                                             \n│   │   └── flower  # downloaded llff dataset                                                                                  \n│   │   └── horns   # downloaded llff dataset\n|   |   └── ...\n|   ├── nerf_synthetic\n|   |   └── lego\n|   |   └── ship    # downloaded synthetic dataset\n|   |   └── ...\n```\n\n---\n\nTo train NeRF on different datasets: \n\n```\npython run_nerf.py --config configs/{DATASET}.txt\n```\n\nreplace `{DATASET}` with `trex` | `horns` | `flower` | `fortress` | `lego` | etc.\n\n---\n\nTo test NeRF trained on different datasets: \n\n```\npython run_nerf.py --config configs/{DATASET}.txt --render_only\n```\n\nreplace `{DATASET}` with `trex` | `horns` | `flower` | `fortress` | `lego` | etc.\n\n\n### Pre-trained Models\n\nYou can download the pre-trained models [here](https://drive.google.com/drive/folders/1jIr8dkvefrQmv737fFm2isiT6tqpbTbv). Place the downloaded directory in `./logs` in order to test it later. See the following directory structure for an example:\n\n```\n├── logs \n│   ├── fern_test\n│   ├── flower_test  # downloaded logs\n│   ├── trex_test    # downloaded logs\n```\n\n### Reproducibility \n\nTests that ensure the results of all functions and training loop match the official implentation are contained in a different branch `reproduce`. One can check it out and run the tests:\n```\ngit checkout reproduce\npy.test\n```\n\n## Method\n\n[NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](http://tancik.com/nerf)  \n [Ben Mildenhall](https://people.eecs.berkeley.edu/~bmild/)\\*<sup>1</sup>,\n [Pratul P. Srinivasan](https://people.eecs.berkeley.edu/~pratul/)\\*<sup>1</sup>,\n [Matthew Tancik](http://tancik.com/)\\*<sup>1</sup>,\n [Jonathan T. Barron](http://jonbarron.info/)<sup>2</sup>,\n [Ravi Ramamoorthi](http://cseweb.ucsd.edu/~ravir/)<sup>3</sup>,\n [Ren Ng](https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html)<sup>1</sup> <br>\n <sup>1</sup>UC Berkeley, <sup>2</sup>Google Research, <sup>3</sup>UC San Diego  \n  \\*denotes equal contribution  \n  \n<img src='imgs/pipeline.jpg'/>\n\n> A neural radiance field is a simple fully connected network (weights are ~5MB) trained to reproduce input views of a single scene using a rendering loss. The network directly maps from spatial location and viewing direction (5D input) to color and opacity (4D output), acting as the \"volume\" so we can use volume rendering to differentiably render new views\n\n\n## Citation\nKudos to the authors for their amazing results:\n```\n@misc{mildenhall2020nerf,\n    title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},\n    author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},\n    year={2020},\n    eprint={2003.08934},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\nHowever, if you find this implementation or pre-trained models helpful, please consider to cite:\n```\n@misc{lin2020nerfpytorch,\n  title={NeRF-pytorch},\n  author={Yen-Chen, Lin},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished={\\url{https://github.com/yenchenlin/nerf-pytorch/}},\n  year={2020}\n}\n```\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "download_example_data.sh",
          "type": "blob",
          "size": 0.2275390625,
          "content": "wget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz\nmkdir -p data\ncd data\nwget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/nerf_example_data.zip\nunzip nerf_example_data.zip\ncd ..\n"
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "load_LINEMOD.py",
          "type": "blob",
          "size": 2.787109375,
          "content": "import os\nimport torch\nimport numpy as np\nimport imageio \nimport json\nimport torch.nn.functional as F\nimport cv2\n\n\ntrans_t = lambda t : torch.Tensor([\n    [1,0,0,0],\n    [0,1,0,0],\n    [0,0,1,t],\n    [0,0,0,1]]).float()\n\nrot_phi = lambda phi : torch.Tensor([\n    [1,0,0,0],\n    [0,np.cos(phi),-np.sin(phi),0],\n    [0,np.sin(phi), np.cos(phi),0],\n    [0,0,0,1]]).float()\n\nrot_theta = lambda th : torch.Tensor([\n    [np.cos(th),0,-np.sin(th),0],\n    [0,1,0,0],\n    [np.sin(th),0, np.cos(th),0],\n    [0,0,0,1]]).float()\n\n\ndef pose_spherical(theta, phi, radius):\n    c2w = trans_t(radius)\n    c2w = rot_phi(phi/180.*np.pi) @ c2w\n    c2w = rot_theta(theta/180.*np.pi) @ c2w\n    c2w = torch.Tensor(np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]])) @ c2w\n    return c2w\n\n\ndef load_LINEMOD_data(basedir, half_res=False, testskip=1):\n    splits = ['train', 'val', 'test']\n    metas = {}\n    for s in splits:\n        with open(os.path.join(basedir, 'transforms_{}.json'.format(s)), 'r') as fp:\n            metas[s] = json.load(fp)\n\n    all_imgs = []\n    all_poses = []\n    counts = [0]\n    for s in splits:\n        meta = metas[s]\n        imgs = []\n        poses = []\n        if s=='train' or testskip==0:\n            skip = 1\n        else:\n            skip = testskip\n            \n        for idx_test, frame in enumerate(meta['frames'][::skip]):\n            fname = frame['file_path']\n            if s == 'test':\n                print(f\"{idx_test}th test frame: {fname}\")\n            imgs.append(imageio.imread(fname))\n            poses.append(np.array(frame['transform_matrix']))\n        imgs = (np.array(imgs) / 255.).astype(np.float32) # keep all 4 channels (RGBA)\n        poses = np.array(poses).astype(np.float32)\n        counts.append(counts[-1] + imgs.shape[0])\n        all_imgs.append(imgs)\n        all_poses.append(poses)\n    \n    i_split = [np.arange(counts[i], counts[i+1]) for i in range(3)]\n    \n    imgs = np.concatenate(all_imgs, 0)\n    poses = np.concatenate(all_poses, 0)\n    \n    H, W = imgs[0].shape[:2]\n    focal = float(meta['frames'][0]['intrinsic_matrix'][0][0])\n    K = meta['frames'][0]['intrinsic_matrix']\n    print(f\"Focal: {focal}\")\n    \n    render_poses = torch.stack([pose_spherical(angle, -30.0, 4.0) for angle in np.linspace(-180,180,40+1)[:-1]], 0)\n    \n    if half_res:\n        H = H//2\n        W = W//2\n        focal = focal/2.\n\n        imgs_half_res = np.zeros((imgs.shape[0], H, W, 3))\n        for i, img in enumerate(imgs):\n            imgs_half_res[i] = cv2.resize(img, (W, H), interpolation=cv2.INTER_AREA)\n        imgs = imgs_half_res\n        # imgs = tf.image.resize_area(imgs, [400, 400]).numpy()\n\n    near = np.floor(min(metas['train']['near'], metas['test']['near']))\n    far = np.ceil(max(metas['train']['far'], metas['test']['far']))\n    return imgs, poses, render_poses, [H, W, focal], K, i_split, near, far\n\n\n"
        },
        {
          "name": "load_blender.py",
          "type": "blob",
          "size": 2.5341796875,
          "content": "import os\nimport torch\nimport numpy as np\nimport imageio \nimport json\nimport torch.nn.functional as F\nimport cv2\n\n\ntrans_t = lambda t : torch.Tensor([\n    [1,0,0,0],\n    [0,1,0,0],\n    [0,0,1,t],\n    [0,0,0,1]]).float()\n\nrot_phi = lambda phi : torch.Tensor([\n    [1,0,0,0],\n    [0,np.cos(phi),-np.sin(phi),0],\n    [0,np.sin(phi), np.cos(phi),0],\n    [0,0,0,1]]).float()\n\nrot_theta = lambda th : torch.Tensor([\n    [np.cos(th),0,-np.sin(th),0],\n    [0,1,0,0],\n    [np.sin(th),0, np.cos(th),0],\n    [0,0,0,1]]).float()\n\n\ndef pose_spherical(theta, phi, radius):\n    c2w = trans_t(radius)\n    c2w = rot_phi(phi/180.*np.pi) @ c2w\n    c2w = rot_theta(theta/180.*np.pi) @ c2w\n    c2w = torch.Tensor(np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]])) @ c2w\n    return c2w\n\n\ndef load_blender_data(basedir, half_res=False, testskip=1):\n    splits = ['train', 'val', 'test']\n    metas = {}\n    for s in splits:\n        with open(os.path.join(basedir, 'transforms_{}.json'.format(s)), 'r') as fp:\n            metas[s] = json.load(fp)\n\n    all_imgs = []\n    all_poses = []\n    counts = [0]\n    for s in splits:\n        meta = metas[s]\n        imgs = []\n        poses = []\n        if s=='train' or testskip==0:\n            skip = 1\n        else:\n            skip = testskip\n            \n        for frame in meta['frames'][::skip]:\n            fname = os.path.join(basedir, frame['file_path'] + '.png')\n            imgs.append(imageio.imread(fname))\n            poses.append(np.array(frame['transform_matrix']))\n        imgs = (np.array(imgs) / 255.).astype(np.float32) # keep all 4 channels (RGBA)\n        poses = np.array(poses).astype(np.float32)\n        counts.append(counts[-1] + imgs.shape[0])\n        all_imgs.append(imgs)\n        all_poses.append(poses)\n    \n    i_split = [np.arange(counts[i], counts[i+1]) for i in range(3)]\n    \n    imgs = np.concatenate(all_imgs, 0)\n    poses = np.concatenate(all_poses, 0)\n    \n    H, W = imgs[0].shape[:2]\n    camera_angle_x = float(meta['camera_angle_x'])\n    focal = .5 * W / np.tan(.5 * camera_angle_x)\n    \n    render_poses = torch.stack([pose_spherical(angle, -30.0, 4.0) for angle in np.linspace(-180,180,40+1)[:-1]], 0)\n    \n    if half_res:\n        H = H//2\n        W = W//2\n        focal = focal/2.\n\n        imgs_half_res = np.zeros((imgs.shape[0], H, W, 4))\n        for i, img in enumerate(imgs):\n            imgs_half_res[i] = cv2.resize(img, (W, H), interpolation=cv2.INTER_AREA)\n        imgs = imgs_half_res\n        # imgs = tf.image.resize_area(imgs, [400, 400]).numpy()\n\n        \n    return imgs, poses, render_poses, [H, W, focal], i_split\n\n\n"
        },
        {
          "name": "load_deepvoxels.py",
          "type": "blob",
          "size": 3.798828125,
          "content": "import os\nimport numpy as np\nimport imageio \n\n\ndef load_dv_data(scene='cube', basedir='/data/deepvoxels', testskip=8):\n    \n\n    def parse_intrinsics(filepath, trgt_sidelength, invert_y=False):\n        # Get camera intrinsics\n        with open(filepath, 'r') as file:\n            f, cx, cy = list(map(float, file.readline().split()))[:3]\n            grid_barycenter = np.array(list(map(float, file.readline().split())))\n            near_plane = float(file.readline())\n            scale = float(file.readline())\n            height, width = map(float, file.readline().split())\n\n            try:\n                world2cam_poses = int(file.readline())\n            except ValueError:\n                world2cam_poses = None\n\n        if world2cam_poses is None:\n            world2cam_poses = False\n\n        world2cam_poses = bool(world2cam_poses)\n\n        print(cx,cy,f,height,width)\n\n        cx = cx / width * trgt_sidelength\n        cy = cy / height * trgt_sidelength\n        f = trgt_sidelength / height * f\n\n        fx = f\n        if invert_y:\n            fy = -f\n        else:\n            fy = f\n\n        # Build the intrinsic matrices\n        full_intrinsic = np.array([[fx, 0., cx, 0.],\n                                   [0., fy, cy, 0],\n                                   [0., 0, 1, 0],\n                                   [0, 0, 0, 1]])\n\n        return full_intrinsic, grid_barycenter, scale, near_plane, world2cam_poses\n\n\n    def load_pose(filename):\n        assert os.path.isfile(filename)\n        nums = open(filename).read().split()\n        return np.array([float(x) for x in nums]).reshape([4,4]).astype(np.float32)\n\n\n    H = 512\n    W = 512\n    deepvoxels_base = '{}/train/{}/'.format(basedir, scene)\n\n    full_intrinsic, grid_barycenter, scale, near_plane, world2cam_poses = parse_intrinsics(os.path.join(deepvoxels_base, 'intrinsics.txt'), H)\n    print(full_intrinsic, grid_barycenter, scale, near_plane, world2cam_poses)\n    focal = full_intrinsic[0,0]\n    print(H, W, focal)\n\n    \n    def dir2poses(posedir):\n        poses = np.stack([load_pose(os.path.join(posedir, f)) for f in sorted(os.listdir(posedir)) if f.endswith('txt')], 0)\n        transf = np.array([\n            [1,0,0,0],\n            [0,-1,0,0],\n            [0,0,-1,0],\n            [0,0,0,1.],\n        ])\n        poses = poses @ transf\n        poses = poses[:,:3,:4].astype(np.float32)\n        return poses\n    \n    posedir = os.path.join(deepvoxels_base, 'pose')\n    poses = dir2poses(posedir)\n    testposes = dir2poses('{}/test/{}/pose'.format(basedir, scene))\n    testposes = testposes[::testskip]\n    valposes = dir2poses('{}/validation/{}/pose'.format(basedir, scene))\n    valposes = valposes[::testskip]\n\n    imgfiles = [f for f in sorted(os.listdir(os.path.join(deepvoxels_base, 'rgb'))) if f.endswith('png')]\n    imgs = np.stack([imageio.imread(os.path.join(deepvoxels_base, 'rgb', f))/255. for f in imgfiles], 0).astype(np.float32)\n    \n    \n    testimgd = '{}/test/{}/rgb'.format(basedir, scene)\n    imgfiles = [f for f in sorted(os.listdir(testimgd)) if f.endswith('png')]\n    testimgs = np.stack([imageio.imread(os.path.join(testimgd, f))/255. for f in imgfiles[::testskip]], 0).astype(np.float32)\n    \n    valimgd = '{}/validation/{}/rgb'.format(basedir, scene)\n    imgfiles = [f for f in sorted(os.listdir(valimgd)) if f.endswith('png')]\n    valimgs = np.stack([imageio.imread(os.path.join(valimgd, f))/255. for f in imgfiles[::testskip]], 0).astype(np.float32)\n    \n    all_imgs = [imgs, valimgs, testimgs]\n    counts = [0] + [x.shape[0] for x in all_imgs]\n    counts = np.cumsum(counts)\n    i_split = [np.arange(counts[i], counts[i+1]) for i in range(3)]\n    \n    imgs = np.concatenate(all_imgs, 0)\n    poses = np.concatenate([poses, valposes, testposes], 0)\n    \n    render_poses = testposes\n    \n    print(poses.shape, imgs.shape)\n    \n    return imgs, poses, render_poses, [H,W,focal], i_split\n\n\n"
        },
        {
          "name": "load_llff.py",
          "type": "blob",
          "size": 9.755859375,
          "content": "import numpy as np\nimport os, imageio\n\n\n########## Slightly modified version of LLFF data loading code \n##########  see https://github.com/Fyusion/LLFF for original\n\ndef _minify(basedir, factors=[], resolutions=[]):\n    needtoload = False\n    for r in factors:\n        imgdir = os.path.join(basedir, 'images_{}'.format(r))\n        if not os.path.exists(imgdir):\n            needtoload = True\n    for r in resolutions:\n        imgdir = os.path.join(basedir, 'images_{}x{}'.format(r[1], r[0]))\n        if not os.path.exists(imgdir):\n            needtoload = True\n    if not needtoload:\n        return\n    \n    from shutil import copy\n    from subprocess import check_output\n    \n    imgdir = os.path.join(basedir, 'images')\n    imgs = [os.path.join(imgdir, f) for f in sorted(os.listdir(imgdir))]\n    imgs = [f for f in imgs if any([f.endswith(ex) for ex in ['JPG', 'jpg', 'png', 'jpeg', 'PNG']])]\n    imgdir_orig = imgdir\n    \n    wd = os.getcwd()\n\n    for r in factors + resolutions:\n        if isinstance(r, int):\n            name = 'images_{}'.format(r)\n            resizearg = '{}%'.format(100./r)\n        else:\n            name = 'images_{}x{}'.format(r[1], r[0])\n            resizearg = '{}x{}'.format(r[1], r[0])\n        imgdir = os.path.join(basedir, name)\n        if os.path.exists(imgdir):\n            continue\n            \n        print('Minifying', r, basedir)\n        \n        os.makedirs(imgdir)\n        check_output('cp {}/* {}'.format(imgdir_orig, imgdir), shell=True)\n        \n        ext = imgs[0].split('.')[-1]\n        args = ' '.join(['mogrify', '-resize', resizearg, '-format', 'png', '*.{}'.format(ext)])\n        print(args)\n        os.chdir(imgdir)\n        check_output(args, shell=True)\n        os.chdir(wd)\n        \n        if ext != 'png':\n            check_output('rm {}/*.{}'.format(imgdir, ext), shell=True)\n            print('Removed duplicates')\n        print('Done')\n            \n        \n        \n        \ndef _load_data(basedir, factor=None, width=None, height=None, load_imgs=True):\n    \n    poses_arr = np.load(os.path.join(basedir, 'poses_bounds.npy'))\n    poses = poses_arr[:, :-2].reshape([-1, 3, 5]).transpose([1,2,0])\n    bds = poses_arr[:, -2:].transpose([1,0])\n    \n    img0 = [os.path.join(basedir, 'images', f) for f in sorted(os.listdir(os.path.join(basedir, 'images'))) \\\n            if f.endswith('JPG') or f.endswith('jpg') or f.endswith('png')][0]\n    sh = imageio.imread(img0).shape\n    \n    sfx = ''\n    \n    if factor is not None:\n        sfx = '_{}'.format(factor)\n        _minify(basedir, factors=[factor])\n        factor = factor\n    elif height is not None:\n        factor = sh[0] / float(height)\n        width = int(sh[1] / factor)\n        _minify(basedir, resolutions=[[height, width]])\n        sfx = '_{}x{}'.format(width, height)\n    elif width is not None:\n        factor = sh[1] / float(width)\n        height = int(sh[0] / factor)\n        _minify(basedir, resolutions=[[height, width]])\n        sfx = '_{}x{}'.format(width, height)\n    else:\n        factor = 1\n    \n    imgdir = os.path.join(basedir, 'images' + sfx)\n    if not os.path.exists(imgdir):\n        print( imgdir, 'does not exist, returning' )\n        return\n    \n    imgfiles = [os.path.join(imgdir, f) for f in sorted(os.listdir(imgdir)) if f.endswith('JPG') or f.endswith('jpg') or f.endswith('png')]\n    if poses.shape[-1] != len(imgfiles):\n        print( 'Mismatch between imgs {} and poses {} !!!!'.format(len(imgfiles), poses.shape[-1]) )\n        return\n    \n    sh = imageio.imread(imgfiles[0]).shape\n    poses[:2, 4, :] = np.array(sh[:2]).reshape([2, 1])\n    poses[2, 4, :] = poses[2, 4, :] * 1./factor\n    \n    if not load_imgs:\n        return poses, bds\n    \n    def imread(f):\n        if f.endswith('png'):\n            return imageio.imread(f, ignoregamma=True)\n        else:\n            return imageio.imread(f)\n        \n    imgs = imgs = [imread(f)[...,:3]/255. for f in imgfiles]\n    imgs = np.stack(imgs, -1)  \n    \n    print('Loaded image data', imgs.shape, poses[:,-1,0])\n    return poses, bds, imgs\n\n    \n            \n            \n    \n\ndef normalize(x):\n    return x / np.linalg.norm(x)\n\ndef viewmatrix(z, up, pos):\n    vec2 = normalize(z)\n    vec1_avg = up\n    vec0 = normalize(np.cross(vec1_avg, vec2))\n    vec1 = normalize(np.cross(vec2, vec0))\n    m = np.stack([vec0, vec1, vec2, pos], 1)\n    return m\n\ndef ptstocam(pts, c2w):\n    tt = np.matmul(c2w[:3,:3].T, (pts-c2w[:3,3])[...,np.newaxis])[...,0]\n    return tt\n\ndef poses_avg(poses):\n\n    hwf = poses[0, :3, -1:]\n\n    center = poses[:, :3, 3].mean(0)\n    vec2 = normalize(poses[:, :3, 2].sum(0))\n    up = poses[:, :3, 1].sum(0)\n    c2w = np.concatenate([viewmatrix(vec2, up, center), hwf], 1)\n    \n    return c2w\n\n\n\ndef render_path_spiral(c2w, up, rads, focal, zdelta, zrate, rots, N):\n    render_poses = []\n    rads = np.array(list(rads) + [1.])\n    hwf = c2w[:,4:5]\n    \n    for theta in np.linspace(0., 2. * np.pi * rots, N+1)[:-1]:\n        c = np.dot(c2w[:3,:4], np.array([np.cos(theta), -np.sin(theta), -np.sin(theta*zrate), 1.]) * rads) \n        z = normalize(c - np.dot(c2w[:3,:4], np.array([0,0,-focal, 1.])))\n        render_poses.append(np.concatenate([viewmatrix(z, up, c), hwf], 1))\n    return render_poses\n    \n\n\ndef recenter_poses(poses):\n\n    poses_ = poses+0\n    bottom = np.reshape([0,0,0,1.], [1,4])\n    c2w = poses_avg(poses)\n    c2w = np.concatenate([c2w[:3,:4], bottom], -2)\n    bottom = np.tile(np.reshape(bottom, [1,1,4]), [poses.shape[0],1,1])\n    poses = np.concatenate([poses[:,:3,:4], bottom], -2)\n\n    poses = np.linalg.inv(c2w) @ poses\n    poses_[:,:3,:4] = poses[:,:3,:4]\n    poses = poses_\n    return poses\n\n\n#####################\n\n\ndef spherify_poses(poses, bds):\n    \n    p34_to_44 = lambda p : np.concatenate([p, np.tile(np.reshape(np.eye(4)[-1,:], [1,1,4]), [p.shape[0], 1,1])], 1)\n    \n    rays_d = poses[:,:3,2:3]\n    rays_o = poses[:,:3,3:4]\n\n    def min_line_dist(rays_o, rays_d):\n        A_i = np.eye(3) - rays_d * np.transpose(rays_d, [0,2,1])\n        b_i = -A_i @ rays_o\n        pt_mindist = np.squeeze(-np.linalg.inv((np.transpose(A_i, [0,2,1]) @ A_i).mean(0)) @ (b_i).mean(0))\n        return pt_mindist\n\n    pt_mindist = min_line_dist(rays_o, rays_d)\n    \n    center = pt_mindist\n    up = (poses[:,:3,3] - center).mean(0)\n\n    vec0 = normalize(up)\n    vec1 = normalize(np.cross([.1,.2,.3], vec0))\n    vec2 = normalize(np.cross(vec0, vec1))\n    pos = center\n    c2w = np.stack([vec1, vec2, vec0, pos], 1)\n\n    poses_reset = np.linalg.inv(p34_to_44(c2w[None])) @ p34_to_44(poses[:,:3,:4])\n\n    rad = np.sqrt(np.mean(np.sum(np.square(poses_reset[:,:3,3]), -1)))\n    \n    sc = 1./rad\n    poses_reset[:,:3,3] *= sc\n    bds *= sc\n    rad *= sc\n    \n    centroid = np.mean(poses_reset[:,:3,3], 0)\n    zh = centroid[2]\n    radcircle = np.sqrt(rad**2-zh**2)\n    new_poses = []\n    \n    for th in np.linspace(0.,2.*np.pi, 120):\n\n        camorigin = np.array([radcircle * np.cos(th), radcircle * np.sin(th), zh])\n        up = np.array([0,0,-1.])\n\n        vec2 = normalize(camorigin)\n        vec0 = normalize(np.cross(vec2, up))\n        vec1 = normalize(np.cross(vec2, vec0))\n        pos = camorigin\n        p = np.stack([vec0, vec1, vec2, pos], 1)\n\n        new_poses.append(p)\n\n    new_poses = np.stack(new_poses, 0)\n    \n    new_poses = np.concatenate([new_poses, np.broadcast_to(poses[0,:3,-1:], new_poses[:,:3,-1:].shape)], -1)\n    poses_reset = np.concatenate([poses_reset[:,:3,:4], np.broadcast_to(poses[0,:3,-1:], poses_reset[:,:3,-1:].shape)], -1)\n    \n    return poses_reset, new_poses, bds\n    \n\ndef load_llff_data(basedir, factor=8, recenter=True, bd_factor=.75, spherify=False, path_zflat=False):\n    \n\n    poses, bds, imgs = _load_data(basedir, factor=factor) # factor=8 downsamples original imgs by 8x\n    print('Loaded', basedir, bds.min(), bds.max())\n    \n    # Correct rotation matrix ordering and move variable dim to axis 0\n    poses = np.concatenate([poses[:, 1:2, :], -poses[:, 0:1, :], poses[:, 2:, :]], 1)\n    poses = np.moveaxis(poses, -1, 0).astype(np.float32)\n    imgs = np.moveaxis(imgs, -1, 0).astype(np.float32)\n    images = imgs\n    bds = np.moveaxis(bds, -1, 0).astype(np.float32)\n    \n    # Rescale if bd_factor is provided\n    sc = 1. if bd_factor is None else 1./(bds.min() * bd_factor)\n    poses[:,:3,3] *= sc\n    bds *= sc\n    \n    if recenter:\n        poses = recenter_poses(poses)\n        \n    if spherify:\n        poses, render_poses, bds = spherify_poses(poses, bds)\n\n    else:\n        \n        c2w = poses_avg(poses)\n        print('recentered', c2w.shape)\n        print(c2w[:3,:4])\n\n        ## Get spiral\n        # Get average pose\n        up = normalize(poses[:, :3, 1].sum(0))\n\n        # Find a reasonable \"focus depth\" for this dataset\n        close_depth, inf_depth = bds.min()*.9, bds.max()*5.\n        dt = .75\n        mean_dz = 1./(((1.-dt)/close_depth + dt/inf_depth))\n        focal = mean_dz\n\n        # Get radii for spiral path\n        shrink_factor = .8\n        zdelta = close_depth * .2\n        tt = poses[:,:3,3] # ptstocam(poses[:3,3,:].T, c2w).T\n        rads = np.percentile(np.abs(tt), 90, 0)\n        c2w_path = c2w\n        N_views = 120\n        N_rots = 2\n        if path_zflat:\n#             zloc = np.percentile(tt, 10, 0)[2]\n            zloc = -close_depth * .1\n            c2w_path[:3,3] = c2w_path[:3,3] + zloc * c2w_path[:3,2]\n            rads[2] = 0.\n            N_rots = 1\n            N_views/=2\n\n        # Generate poses for spiral path\n        render_poses = render_path_spiral(c2w_path, up, rads, focal, zdelta, zrate=.5, rots=N_rots, N=N_views)\n        \n        \n    render_poses = np.array(render_poses).astype(np.float32)\n\n    c2w = poses_avg(poses)\n    print('Data:')\n    print(poses.shape, images.shape, bds.shape)\n    \n    dists = np.sum(np.square(c2w[:3,3] - poses[:,:3,3]), -1)\n    i_test = np.argmin(dists)\n    print('HOLDOUT view is', i_test)\n    \n    images = images.astype(np.float32)\n    poses = poses.astype(np.float32)\n\n    return images, poses, bds, render_poses, i_test\n\n\n\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.115234375,
          "content": "torch==1.11.0\ntorchvision>=0.9.1\nimageio\nimageio-ffmpeg\nmatplotlib\nconfigargparse\ntensorboard>=2.0\ntqdm\nopencv-python\n"
        },
        {
          "name": "run_nerf.py",
          "type": "blob",
          "size": 35.490234375,
          "content": "import os, sys\nimport numpy as np\nimport imageio\nimport json\nimport random\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm, trange\n\nimport matplotlib.pyplot as plt\n\nfrom run_nerf_helpers import *\n\nfrom load_llff import load_llff_data\nfrom load_deepvoxels import load_dv_data\nfrom load_blender import load_blender_data\nfrom load_LINEMOD import load_LINEMOD_data\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnp.random.seed(0)\nDEBUG = False\n\n\ndef batchify(fn, chunk):\n    \"\"\"Constructs a version of 'fn' that applies to smaller batches.\n    \"\"\"\n    if chunk is None:\n        return fn\n    def ret(inputs):\n        return torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n    return ret\n\n\ndef run_network(inputs, viewdirs, fn, embed_fn, embeddirs_fn, netchunk=1024*64):\n    \"\"\"Prepares inputs and applies network 'fn'.\n    \"\"\"\n    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]])\n    embedded = embed_fn(inputs_flat)\n\n    if viewdirs is not None:\n        input_dirs = viewdirs[:,None].expand(inputs.shape)\n        input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n        embedded_dirs = embeddirs_fn(input_dirs_flat)\n        embedded = torch.cat([embedded, embedded_dirs], -1)\n\n    outputs_flat = batchify(fn, netchunk)(embedded)\n    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n    return outputs\n\n\ndef batchify_rays(rays_flat, chunk=1024*32, **kwargs):\n    \"\"\"Render rays in smaller minibatches to avoid OOM.\n    \"\"\"\n    all_ret = {}\n    for i in range(0, rays_flat.shape[0], chunk):\n        ret = render_rays(rays_flat[i:i+chunk], **kwargs)\n        for k in ret:\n            if k not in all_ret:\n                all_ret[k] = []\n            all_ret[k].append(ret[k])\n\n    all_ret = {k : torch.cat(all_ret[k], 0) for k in all_ret}\n    return all_ret\n\n\ndef render(H, W, K, chunk=1024*32, rays=None, c2w=None, ndc=True,\n                  near=0., far=1.,\n                  use_viewdirs=False, c2w_staticcam=None,\n                  **kwargs):\n    \"\"\"Render rays\n    Args:\n      H: int. Height of image in pixels.\n      W: int. Width of image in pixels.\n      focal: float. Focal length of pinhole camera.\n      chunk: int. Maximum number of rays to process simultaneously. Used to\n        control maximum memory usage. Does not affect final results.\n      rays: array of shape [2, batch_size, 3]. Ray origin and direction for\n        each example in batch.\n      c2w: array of shape [3, 4]. Camera-to-world transformation matrix.\n      ndc: bool. If True, represent ray origin, direction in NDC coordinates.\n      near: float or array of shape [batch_size]. Nearest distance for a ray.\n      far: float or array of shape [batch_size]. Farthest distance for a ray.\n      use_viewdirs: bool. If True, use viewing direction of a point in space in model.\n      c2w_staticcam: array of shape [3, 4]. If not None, use this transformation matrix for \n       camera while using other c2w argument for viewing directions.\n    Returns:\n      rgb_map: [batch_size, 3]. Predicted RGB values for rays.\n      disp_map: [batch_size]. Disparity map. Inverse of depth.\n      acc_map: [batch_size]. Accumulated opacity (alpha) along a ray.\n      extras: dict with everything returned by render_rays().\n    \"\"\"\n    if c2w is not None:\n        # special case to render full image\n        rays_o, rays_d = get_rays(H, W, K, c2w)\n    else:\n        # use provided ray batch\n        rays_o, rays_d = rays\n\n    if use_viewdirs:\n        # provide ray directions as input\n        viewdirs = rays_d\n        if c2w_staticcam is not None:\n            # special case to visualize effect of viewdirs\n            rays_o, rays_d = get_rays(H, W, K, c2w_staticcam)\n        viewdirs = viewdirs / torch.norm(viewdirs, dim=-1, keepdim=True)\n        viewdirs = torch.reshape(viewdirs, [-1,3]).float()\n\n    sh = rays_d.shape # [..., 3]\n    if ndc:\n        # for forward facing scenes\n        rays_o, rays_d = ndc_rays(H, W, K[0][0], 1., rays_o, rays_d)\n\n    # Create ray batch\n    rays_o = torch.reshape(rays_o, [-1,3]).float()\n    rays_d = torch.reshape(rays_d, [-1,3]).float()\n\n    near, far = near * torch.ones_like(rays_d[...,:1]), far * torch.ones_like(rays_d[...,:1])\n    rays = torch.cat([rays_o, rays_d, near, far], -1)\n    if use_viewdirs:\n        rays = torch.cat([rays, viewdirs], -1)\n\n    # Render and reshape\n    all_ret = batchify_rays(rays, chunk, **kwargs)\n    for k in all_ret:\n        k_sh = list(sh[:-1]) + list(all_ret[k].shape[1:])\n        all_ret[k] = torch.reshape(all_ret[k], k_sh)\n\n    k_extract = ['rgb_map', 'disp_map', 'acc_map']\n    ret_list = [all_ret[k] for k in k_extract]\n    ret_dict = {k : all_ret[k] for k in all_ret if k not in k_extract}\n    return ret_list + [ret_dict]\n\n\ndef render_path(render_poses, hwf, K, chunk, render_kwargs, gt_imgs=None, savedir=None, render_factor=0):\n\n    H, W, focal = hwf\n\n    if render_factor!=0:\n        # Render downsampled for speed\n        H = H//render_factor\n        W = W//render_factor\n        focal = focal/render_factor\n\n    rgbs = []\n    disps = []\n\n    t = time.time()\n    for i, c2w in enumerate(tqdm(render_poses)):\n        print(i, time.time() - t)\n        t = time.time()\n        rgb, disp, acc, _ = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], **render_kwargs)\n        rgbs.append(rgb.cpu().numpy())\n        disps.append(disp.cpu().numpy())\n        if i==0:\n            print(rgb.shape, disp.shape)\n\n        \"\"\"\n        if gt_imgs is not None and render_factor==0:\n            p = -10. * np.log10(np.mean(np.square(rgb.cpu().numpy() - gt_imgs[i])))\n            print(p)\n        \"\"\"\n\n        if savedir is not None:\n            rgb8 = to8b(rgbs[-1])\n            filename = os.path.join(savedir, '{:03d}.png'.format(i))\n            imageio.imwrite(filename, rgb8)\n\n\n    rgbs = np.stack(rgbs, 0)\n    disps = np.stack(disps, 0)\n\n    return rgbs, disps\n\n\ndef create_nerf(args):\n    \"\"\"Instantiate NeRF's MLP model.\n    \"\"\"\n    embed_fn, input_ch = get_embedder(args.multires, args.i_embed)\n\n    input_ch_views = 0\n    embeddirs_fn = None\n    if args.use_viewdirs:\n        embeddirs_fn, input_ch_views = get_embedder(args.multires_views, args.i_embed)\n    output_ch = 5 if args.N_importance > 0 else 4\n    skips = [4]\n    model = NeRF(D=args.netdepth, W=args.netwidth,\n                 input_ch=input_ch, output_ch=output_ch, skips=skips,\n                 input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n    grad_vars = list(model.parameters())\n\n    model_fine = None\n    if args.N_importance > 0:\n        model_fine = NeRF(D=args.netdepth_fine, W=args.netwidth_fine,\n                          input_ch=input_ch, output_ch=output_ch, skips=skips,\n                          input_ch_views=input_ch_views, use_viewdirs=args.use_viewdirs).to(device)\n        grad_vars += list(model_fine.parameters())\n\n    network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,\n                                                                embed_fn=embed_fn,\n                                                                embeddirs_fn=embeddirs_fn,\n                                                                netchunk=args.netchunk)\n\n    # Create optimizer\n    optimizer = torch.optim.Adam(params=grad_vars, lr=args.lrate, betas=(0.9, 0.999))\n\n    start = 0\n    basedir = args.basedir\n    expname = args.expname\n\n    ##########################\n\n    # Load checkpoints\n    if args.ft_path is not None and args.ft_path!='None':\n        ckpts = [args.ft_path]\n    else:\n        ckpts = [os.path.join(basedir, expname, f) for f in sorted(os.listdir(os.path.join(basedir, expname))) if 'tar' in f]\n\n    print('Found ckpts', ckpts)\n    if len(ckpts) > 0 and not args.no_reload:\n        ckpt_path = ckpts[-1]\n        print('Reloading from', ckpt_path)\n        ckpt = torch.load(ckpt_path)\n\n        start = ckpt['global_step']\n        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n\n        # Load model\n        model.load_state_dict(ckpt['network_fn_state_dict'])\n        if model_fine is not None:\n            model_fine.load_state_dict(ckpt['network_fine_state_dict'])\n\n    ##########################\n\n    render_kwargs_train = {\n        'network_query_fn' : network_query_fn,\n        'perturb' : args.perturb,\n        'N_importance' : args.N_importance,\n        'network_fine' : model_fine,\n        'N_samples' : args.N_samples,\n        'network_fn' : model,\n        'use_viewdirs' : args.use_viewdirs,\n        'white_bkgd' : args.white_bkgd,\n        'raw_noise_std' : args.raw_noise_std,\n    }\n\n    # NDC only good for LLFF-style forward facing data\n    if args.dataset_type != 'llff' or args.no_ndc:\n        print('Not ndc!')\n        render_kwargs_train['ndc'] = False\n        render_kwargs_train['lindisp'] = args.lindisp\n\n    render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n    render_kwargs_test['perturb'] = False\n    render_kwargs_test['raw_noise_std'] = 0.\n\n    return render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer\n\n\ndef raw2outputs(raw, z_vals, rays_d, raw_noise_std=0, white_bkgd=False, pytest=False):\n    \"\"\"Transforms model's predictions to semantically meaningful values.\n    Args:\n        raw: [num_rays, num_samples along ray, 4]. Prediction from model.\n        z_vals: [num_rays, num_samples along ray]. Integration time.\n        rays_d: [num_rays, 3]. Direction of each ray.\n    Returns:\n        rgb_map: [num_rays, 3]. Estimated RGB color of a ray.\n        disp_map: [num_rays]. Disparity map. Inverse of depth map.\n        acc_map: [num_rays]. Sum of weights along each ray.\n        weights: [num_rays, num_samples]. Weights assigned to each sampled color.\n        depth_map: [num_rays]. Estimated distance to object.\n    \"\"\"\n    raw2alpha = lambda raw, dists, act_fn=F.relu: 1.-torch.exp(-act_fn(raw)*dists)\n\n    dists = z_vals[...,1:] - z_vals[...,:-1]\n    dists = torch.cat([dists, torch.Tensor([1e10]).expand(dists[...,:1].shape)], -1)  # [N_rays, N_samples]\n\n    dists = dists * torch.norm(rays_d[...,None,:], dim=-1)\n\n    rgb = torch.sigmoid(raw[...,:3])  # [N_rays, N_samples, 3]\n    noise = 0.\n    if raw_noise_std > 0.:\n        noise = torch.randn(raw[...,3].shape) * raw_noise_std\n\n        # Overwrite randomly sampled data if pytest\n        if pytest:\n            np.random.seed(0)\n            noise = np.random.rand(*list(raw[...,3].shape)) * raw_noise_std\n            noise = torch.Tensor(noise)\n\n    alpha = raw2alpha(raw[...,3] + noise, dists)  # [N_rays, N_samples]\n    # weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)), 1.-alpha + 1e-10], -1), -1)[:, :-1]\n    rgb_map = torch.sum(weights[...,None] * rgb, -2)  # [N_rays, 3]\n\n    depth_map = torch.sum(weights * z_vals, -1)\n    disp_map = 1./torch.max(1e-10 * torch.ones_like(depth_map), depth_map / torch.sum(weights, -1))\n    acc_map = torch.sum(weights, -1)\n\n    if white_bkgd:\n        rgb_map = rgb_map + (1.-acc_map[...,None])\n\n    return rgb_map, disp_map, acc_map, weights, depth_map\n\n\ndef render_rays(ray_batch,\n                network_fn,\n                network_query_fn,\n                N_samples,\n                retraw=False,\n                lindisp=False,\n                perturb=0.,\n                N_importance=0,\n                network_fine=None,\n                white_bkgd=False,\n                raw_noise_std=0.,\n                verbose=False,\n                pytest=False):\n    \"\"\"Volumetric rendering.\n    Args:\n      ray_batch: array of shape [batch_size, ...]. All information necessary\n        for sampling along a ray, including: ray origin, ray direction, min\n        dist, max dist, and unit-magnitude viewing direction.\n      network_fn: function. Model for predicting RGB and density at each point\n        in space.\n      network_query_fn: function used for passing queries to network_fn.\n      N_samples: int. Number of different times to sample along each ray.\n      retraw: bool. If True, include model's raw, unprocessed predictions.\n      lindisp: bool. If True, sample linearly in inverse depth rather than in depth.\n      perturb: float, 0 or 1. If non-zero, each ray is sampled at stratified\n        random points in time.\n      N_importance: int. Number of additional times to sample along each ray.\n        These samples are only passed to network_fine.\n      network_fine: \"fine\" network with same spec as network_fn.\n      white_bkgd: bool. If True, assume a white background.\n      raw_noise_std: ...\n      verbose: bool. If True, print more debugging info.\n    Returns:\n      rgb_map: [num_rays, 3]. Estimated RGB color of a ray. Comes from fine model.\n      disp_map: [num_rays]. Disparity map. 1 / depth.\n      acc_map: [num_rays]. Accumulated opacity along each ray. Comes from fine model.\n      raw: [num_rays, num_samples, 4]. Raw predictions from model.\n      rgb0: See rgb_map. Output for coarse model.\n      disp0: See disp_map. Output for coarse model.\n      acc0: See acc_map. Output for coarse model.\n      z_std: [num_rays]. Standard deviation of distances along ray for each\n        sample.\n    \"\"\"\n    N_rays = ray_batch.shape[0]\n    rays_o, rays_d = ray_batch[:,0:3], ray_batch[:,3:6] # [N_rays, 3] each\n    viewdirs = ray_batch[:,-3:] if ray_batch.shape[-1] > 8 else None\n    bounds = torch.reshape(ray_batch[...,6:8], [-1,1,2])\n    near, far = bounds[...,0], bounds[...,1] # [-1,1]\n\n    t_vals = torch.linspace(0., 1., steps=N_samples)\n    if not lindisp:\n        z_vals = near * (1.-t_vals) + far * (t_vals)\n    else:\n        z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n\n    z_vals = z_vals.expand([N_rays, N_samples])\n\n    if perturb > 0.:\n        # get intervals between samples\n        mids = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n        upper = torch.cat([mids, z_vals[...,-1:]], -1)\n        lower = torch.cat([z_vals[...,:1], mids], -1)\n        # stratified samples in those intervals\n        t_rand = torch.rand(z_vals.shape)\n\n        # Pytest, overwrite u with numpy's fixed random numbers\n        if pytest:\n            np.random.seed(0)\n            t_rand = np.random.rand(*list(z_vals.shape))\n            t_rand = torch.Tensor(t_rand)\n\n        z_vals = lower + (upper - lower) * t_rand\n\n    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3]\n\n\n#     raw = run_network(pts)\n    raw = network_query_fn(pts, viewdirs, network_fn)\n    rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n\n    if N_importance > 0:\n\n        rgb_map_0, disp_map_0, acc_map_0 = rgb_map, disp_map, acc_map\n\n        z_vals_mid = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n        z_samples = sample_pdf(z_vals_mid, weights[...,1:-1], N_importance, det=(perturb==0.), pytest=pytest)\n        z_samples = z_samples.detach()\n\n        z_vals, _ = torch.sort(torch.cat([z_vals, z_samples], -1), -1)\n        pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples + N_importance, 3]\n\n        run_fn = network_fn if network_fine is None else network_fine\n#         raw = run_network(pts, fn=run_fn)\n        raw = network_query_fn(pts, viewdirs, run_fn)\n\n        rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd, pytest=pytest)\n\n    ret = {'rgb_map' : rgb_map, 'disp_map' : disp_map, 'acc_map' : acc_map}\n    if retraw:\n        ret['raw'] = raw\n    if N_importance > 0:\n        ret['rgb0'] = rgb_map_0\n        ret['disp0'] = disp_map_0\n        ret['acc0'] = acc_map_0\n        ret['z_std'] = torch.std(z_samples, dim=-1, unbiased=False)  # [N_rays]\n\n    for k in ret:\n        if (torch.isnan(ret[k]).any() or torch.isinf(ret[k]).any()) and DEBUG:\n            print(f\"! [Numerical Error] {k} contains nan or inf.\")\n\n    return ret\n\n\ndef config_parser():\n\n    import configargparse\n    parser = configargparse.ArgumentParser()\n    parser.add_argument('--config', is_config_file=True, \n                        help='config file path')\n    parser.add_argument(\"--expname\", type=str, \n                        help='experiment name')\n    parser.add_argument(\"--basedir\", type=str, default='./logs/', \n                        help='where to store ckpts and logs')\n    parser.add_argument(\"--datadir\", type=str, default='./data/llff/fern', \n                        help='input data directory')\n\n    # training options\n    parser.add_argument(\"--netdepth\", type=int, default=8, \n                        help='layers in network')\n    parser.add_argument(\"--netwidth\", type=int, default=256, \n                        help='channels per layer')\n    parser.add_argument(\"--netdepth_fine\", type=int, default=8, \n                        help='layers in fine network')\n    parser.add_argument(\"--netwidth_fine\", type=int, default=256, \n                        help='channels per layer in fine network')\n    parser.add_argument(\"--N_rand\", type=int, default=32*32*4, \n                        help='batch size (number of random rays per gradient step)')\n    parser.add_argument(\"--lrate\", type=float, default=5e-4, \n                        help='learning rate')\n    parser.add_argument(\"--lrate_decay\", type=int, default=250, \n                        help='exponential learning rate decay (in 1000 steps)')\n    parser.add_argument(\"--chunk\", type=int, default=1024*32, \n                        help='number of rays processed in parallel, decrease if running out of memory')\n    parser.add_argument(\"--netchunk\", type=int, default=1024*64, \n                        help='number of pts sent through network in parallel, decrease if running out of memory')\n    parser.add_argument(\"--no_batching\", action='store_true', \n                        help='only take random rays from 1 image at a time')\n    parser.add_argument(\"--no_reload\", action='store_true', \n                        help='do not reload weights from saved ckpt')\n    parser.add_argument(\"--ft_path\", type=str, default=None, \n                        help='specific weights npy file to reload for coarse network')\n\n    # rendering options\n    parser.add_argument(\"--N_samples\", type=int, default=64, \n                        help='number of coarse samples per ray')\n    parser.add_argument(\"--N_importance\", type=int, default=0,\n                        help='number of additional fine samples per ray')\n    parser.add_argument(\"--perturb\", type=float, default=1.,\n                        help='set to 0. for no jitter, 1. for jitter')\n    parser.add_argument(\"--use_viewdirs\", action='store_true', \n                        help='use full 5D input instead of 3D')\n    parser.add_argument(\"--i_embed\", type=int, default=0, \n                        help='set 0 for default positional encoding, -1 for none')\n    parser.add_argument(\"--multires\", type=int, default=10, \n                        help='log2 of max freq for positional encoding (3D location)')\n    parser.add_argument(\"--multires_views\", type=int, default=4, \n                        help='log2 of max freq for positional encoding (2D direction)')\n    parser.add_argument(\"--raw_noise_std\", type=float, default=0., \n                        help='std dev of noise added to regularize sigma_a output, 1e0 recommended')\n\n    parser.add_argument(\"--render_only\", action='store_true', \n                        help='do not optimize, reload weights and render out render_poses path')\n    parser.add_argument(\"--render_test\", action='store_true', \n                        help='render the test set instead of render_poses path')\n    parser.add_argument(\"--render_factor\", type=int, default=0, \n                        help='downsampling factor to speed up rendering, set 4 or 8 for fast preview')\n\n    # training options\n    parser.add_argument(\"--precrop_iters\", type=int, default=0,\n                        help='number of steps to train on central crops')\n    parser.add_argument(\"--precrop_frac\", type=float,\n                        default=.5, help='fraction of img taken for central crops') \n\n    # dataset options\n    parser.add_argument(\"--dataset_type\", type=str, default='llff', \n                        help='options: llff / blender / deepvoxels')\n    parser.add_argument(\"--testskip\", type=int, default=8, \n                        help='will load 1/N images from test/val sets, useful for large datasets like deepvoxels')\n\n    ## deepvoxels flags\n    parser.add_argument(\"--shape\", type=str, default='greek', \n                        help='options : armchair / cube / greek / vase')\n\n    ## blender flags\n    parser.add_argument(\"--white_bkgd\", action='store_true', \n                        help='set to render synthetic data on a white bkgd (always use for dvoxels)')\n    parser.add_argument(\"--half_res\", action='store_true', \n                        help='load blender synthetic data at 400x400 instead of 800x800')\n\n    ## llff flags\n    parser.add_argument(\"--factor\", type=int, default=8, \n                        help='downsample factor for LLFF images')\n    parser.add_argument(\"--no_ndc\", action='store_true', \n                        help='do not use normalized device coordinates (set for non-forward facing scenes)')\n    parser.add_argument(\"--lindisp\", action='store_true', \n                        help='sampling linearly in disparity rather than depth')\n    parser.add_argument(\"--spherify\", action='store_true', \n                        help='set for spherical 360 scenes')\n    parser.add_argument(\"--llffhold\", type=int, default=8, \n                        help='will take every 1/N images as LLFF test set, paper uses 8')\n\n    # logging/saving options\n    parser.add_argument(\"--i_print\",   type=int, default=100, \n                        help='frequency of console printout and metric loggin')\n    parser.add_argument(\"--i_img\",     type=int, default=500, \n                        help='frequency of tensorboard image logging')\n    parser.add_argument(\"--i_weights\", type=int, default=10000, \n                        help='frequency of weight ckpt saving')\n    parser.add_argument(\"--i_testset\", type=int, default=50000, \n                        help='frequency of testset saving')\n    parser.add_argument(\"--i_video\",   type=int, default=50000, \n                        help='frequency of render_poses video saving')\n\n    return parser\n\n\ndef train():\n\n    parser = config_parser()\n    args = parser.parse_args()\n\n    # Load data\n    K = None\n    if args.dataset_type == 'llff':\n        images, poses, bds, render_poses, i_test = load_llff_data(args.datadir, args.factor,\n                                                                  recenter=True, bd_factor=.75,\n                                                                  spherify=args.spherify)\n        hwf = poses[0,:3,-1]\n        poses = poses[:,:3,:4]\n        print('Loaded llff', images.shape, render_poses.shape, hwf, args.datadir)\n        if not isinstance(i_test, list):\n            i_test = [i_test]\n\n        if args.llffhold > 0:\n            print('Auto LLFF holdout,', args.llffhold)\n            i_test = np.arange(images.shape[0])[::args.llffhold]\n\n        i_val = i_test\n        i_train = np.array([i for i in np.arange(int(images.shape[0])) if\n                        (i not in i_test and i not in i_val)])\n\n        print('DEFINING BOUNDS')\n        if args.no_ndc:\n            near = np.ndarray.min(bds) * .9\n            far = np.ndarray.max(bds) * 1.\n            \n        else:\n            near = 0.\n            far = 1.\n        print('NEAR FAR', near, far)\n\n    elif args.dataset_type == 'blender':\n        images, poses, render_poses, hwf, i_split = load_blender_data(args.datadir, args.half_res, args.testskip)\n        print('Loaded blender', images.shape, render_poses.shape, hwf, args.datadir)\n        i_train, i_val, i_test = i_split\n\n        near = 2.\n        far = 6.\n\n        if args.white_bkgd:\n            images = images[...,:3]*images[...,-1:] + (1.-images[...,-1:])\n        else:\n            images = images[...,:3]\n\n    elif args.dataset_type == 'LINEMOD':\n        images, poses, render_poses, hwf, K, i_split, near, far = load_LINEMOD_data(args.datadir, args.half_res, args.testskip)\n        print(f'Loaded LINEMOD, images shape: {images.shape}, hwf: {hwf}, K: {K}')\n        print(f'[CHECK HERE] near: {near}, far: {far}.')\n        i_train, i_val, i_test = i_split\n\n        if args.white_bkgd:\n            images = images[...,:3]*images[...,-1:] + (1.-images[...,-1:])\n        else:\n            images = images[...,:3]\n\n    elif args.dataset_type == 'deepvoxels':\n\n        images, poses, render_poses, hwf, i_split = load_dv_data(scene=args.shape,\n                                                                 basedir=args.datadir,\n                                                                 testskip=args.testskip)\n\n        print('Loaded deepvoxels', images.shape, render_poses.shape, hwf, args.datadir)\n        i_train, i_val, i_test = i_split\n\n        hemi_R = np.mean(np.linalg.norm(poses[:,:3,-1], axis=-1))\n        near = hemi_R-1.\n        far = hemi_R+1.\n\n    else:\n        print('Unknown dataset type', args.dataset_type, 'exiting')\n        return\n\n    # Cast intrinsics to right types\n    H, W, focal = hwf\n    H, W = int(H), int(W)\n    hwf = [H, W, focal]\n\n    if K is None:\n        K = np.array([\n            [focal, 0, 0.5*W],\n            [0, focal, 0.5*H],\n            [0, 0, 1]\n        ])\n\n    if args.render_test:\n        render_poses = np.array(poses[i_test])\n\n    # Create log dir and copy the config file\n    basedir = args.basedir\n    expname = args.expname\n    os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n    f = os.path.join(basedir, expname, 'args.txt')\n    with open(f, 'w') as file:\n        for arg in sorted(vars(args)):\n            attr = getattr(args, arg)\n            file.write('{} = {}\\n'.format(arg, attr))\n    if args.config is not None:\n        f = os.path.join(basedir, expname, 'config.txt')\n        with open(f, 'w') as file:\n            file.write(open(args.config, 'r').read())\n\n    # Create nerf model\n    render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer = create_nerf(args)\n    global_step = start\n\n    bds_dict = {\n        'near' : near,\n        'far' : far,\n    }\n    render_kwargs_train.update(bds_dict)\n    render_kwargs_test.update(bds_dict)\n\n    # Move testing data to GPU\n    render_poses = torch.Tensor(render_poses).to(device)\n\n    # Short circuit if only rendering out from trained model\n    if args.render_only:\n        print('RENDER ONLY')\n        with torch.no_grad():\n            if args.render_test:\n                # render_test switches to test poses\n                images = images[i_test]\n            else:\n                # Default is smoother render_poses path\n                images = None\n\n            testsavedir = os.path.join(basedir, expname, 'renderonly_{}_{:06d}'.format('test' if args.render_test else 'path', start))\n            os.makedirs(testsavedir, exist_ok=True)\n            print('test poses shape', render_poses.shape)\n\n            rgbs, _ = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test, gt_imgs=images, savedir=testsavedir, render_factor=args.render_factor)\n            print('Done rendering', testsavedir)\n            imageio.mimwrite(os.path.join(testsavedir, 'video.mp4'), to8b(rgbs), fps=30, quality=8)\n\n            return\n\n    # Prepare raybatch tensor if batching random rays\n    N_rand = args.N_rand\n    use_batching = not args.no_batching\n    if use_batching:\n        # For random ray batching\n        print('get rays')\n        rays = np.stack([get_rays_np(H, W, K, p) for p in poses[:,:3,:4]], 0) # [N, ro+rd, H, W, 3]\n        print('done, concats')\n        rays_rgb = np.concatenate([rays, images[:,None]], 1) # [N, ro+rd+rgb, H, W, 3]\n        rays_rgb = np.transpose(rays_rgb, [0,2,3,1,4]) # [N, H, W, ro+rd+rgb, 3]\n        rays_rgb = np.stack([rays_rgb[i] for i in i_train], 0) # train images only\n        rays_rgb = np.reshape(rays_rgb, [-1,3,3]) # [(N-1)*H*W, ro+rd+rgb, 3]\n        rays_rgb = rays_rgb.astype(np.float32)\n        print('shuffle rays')\n        np.random.shuffle(rays_rgb)\n\n        print('done')\n        i_batch = 0\n\n    # Move training data to GPU\n    if use_batching:\n        images = torch.Tensor(images).to(device)\n    poses = torch.Tensor(poses).to(device)\n    if use_batching:\n        rays_rgb = torch.Tensor(rays_rgb).to(device)\n\n\n    N_iters = 200000 + 1\n    print('Begin')\n    print('TRAIN views are', i_train)\n    print('TEST views are', i_test)\n    print('VAL views are', i_val)\n\n    # Summary writers\n    # writer = SummaryWriter(os.path.join(basedir, 'summaries', expname))\n    \n    start = start + 1\n    for i in trange(start, N_iters):\n        time0 = time.time()\n\n        # Sample random ray batch\n        if use_batching:\n            # Random over all images\n            batch = rays_rgb[i_batch:i_batch+N_rand] # [B, 2+1, 3*?]\n            batch = torch.transpose(batch, 0, 1)\n            batch_rays, target_s = batch[:2], batch[2]\n\n            i_batch += N_rand\n            if i_batch >= rays_rgb.shape[0]:\n                print(\"Shuffle data after an epoch!\")\n                rand_idx = torch.randperm(rays_rgb.shape[0])\n                rays_rgb = rays_rgb[rand_idx]\n                i_batch = 0\n\n        else:\n            # Random from one image\n            img_i = np.random.choice(i_train)\n            target = images[img_i]\n            target = torch.Tensor(target).to(device)\n            pose = poses[img_i, :3,:4]\n\n            if N_rand is not None:\n                rays_o, rays_d = get_rays(H, W, K, torch.Tensor(pose))  # (H, W, 3), (H, W, 3)\n\n                if i < args.precrop_iters:\n                    dH = int(H//2 * args.precrop_frac)\n                    dW = int(W//2 * args.precrop_frac)\n                    coords = torch.stack(\n                        torch.meshgrid(\n                            torch.linspace(H//2 - dH, H//2 + dH - 1, 2*dH), \n                            torch.linspace(W//2 - dW, W//2 + dW - 1, 2*dW)\n                        ), -1)\n                    if i == start:\n                        print(f\"[Config] Center cropping of size {2*dH} x {2*dW} is enabled until iter {args.precrop_iters}\")                \n                else:\n                    coords = torch.stack(torch.meshgrid(torch.linspace(0, H-1, H), torch.linspace(0, W-1, W)), -1)  # (H, W, 2)\n\n                coords = torch.reshape(coords, [-1,2])  # (H * W, 2)\n                select_inds = np.random.choice(coords.shape[0], size=[N_rand], replace=False)  # (N_rand,)\n                select_coords = coords[select_inds].long()  # (N_rand, 2)\n                rays_o = rays_o[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n                rays_d = rays_d[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n                batch_rays = torch.stack([rays_o, rays_d], 0)\n                target_s = target[select_coords[:, 0], select_coords[:, 1]]  # (N_rand, 3)\n\n        #####  Core optimization loop  #####\n        rgb, disp, acc, extras = render(H, W, K, chunk=args.chunk, rays=batch_rays,\n                                                verbose=i < 10, retraw=True,\n                                                **render_kwargs_train)\n\n        optimizer.zero_grad()\n        img_loss = img2mse(rgb, target_s)\n        trans = extras['raw'][...,-1]\n        loss = img_loss\n        psnr = mse2psnr(img_loss)\n\n        if 'rgb0' in extras:\n            img_loss0 = img2mse(extras['rgb0'], target_s)\n            loss = loss + img_loss0\n            psnr0 = mse2psnr(img_loss0)\n\n        loss.backward()\n        optimizer.step()\n\n        # NOTE: IMPORTANT!\n        ###   update learning rate   ###\n        decay_rate = 0.1\n        decay_steps = args.lrate_decay * 1000\n        new_lrate = args.lrate * (decay_rate ** (global_step / decay_steps))\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = new_lrate\n        ################################\n\n        dt = time.time()-time0\n        # print(f\"Step: {global_step}, Loss: {loss}, Time: {dt}\")\n        #####           end            #####\n\n        # Rest is logging\n        if i%args.i_weights==0:\n            path = os.path.join(basedir, expname, '{:06d}.tar'.format(i))\n            torch.save({\n                'global_step': global_step,\n                'network_fn_state_dict': render_kwargs_train['network_fn'].state_dict(),\n                'network_fine_state_dict': render_kwargs_train['network_fine'].state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            }, path)\n            print('Saved checkpoints at', path)\n\n        if i%args.i_video==0 and i > 0:\n            # Turn on testing mode\n            with torch.no_grad():\n                rgbs, disps = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test)\n            print('Done, saving', rgbs.shape, disps.shape)\n            moviebase = os.path.join(basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n            imageio.mimwrite(moviebase + 'rgb.mp4', to8b(rgbs), fps=30, quality=8)\n            imageio.mimwrite(moviebase + 'disp.mp4', to8b(disps / np.max(disps)), fps=30, quality=8)\n\n            # if args.use_viewdirs:\n            #     render_kwargs_test['c2w_staticcam'] = render_poses[0][:3,:4]\n            #     with torch.no_grad():\n            #         rgbs_still, _ = render_path(render_poses, hwf, args.chunk, render_kwargs_test)\n            #     render_kwargs_test['c2w_staticcam'] = None\n            #     imageio.mimwrite(moviebase + 'rgb_still.mp4', to8b(rgbs_still), fps=30, quality=8)\n\n        if i%args.i_testset==0 and i > 0:\n            testsavedir = os.path.join(basedir, expname, 'testset_{:06d}'.format(i))\n            os.makedirs(testsavedir, exist_ok=True)\n            print('test poses shape', poses[i_test].shape)\n            with torch.no_grad():\n                render_path(torch.Tensor(poses[i_test]).to(device), hwf, K, args.chunk, render_kwargs_test, gt_imgs=images[i_test], savedir=testsavedir)\n            print('Saved test set')\n\n\n    \n        if i%args.i_print==0:\n            tqdm.write(f\"[TRAIN] Iter: {i} Loss: {loss.item()}  PSNR: {psnr.item()}\")\n        \"\"\"\n            print(expname, i, psnr.numpy(), loss.numpy(), global_step.numpy())\n            print('iter time {:.05f}'.format(dt))\n\n            with tf.contrib.summary.record_summaries_every_n_global_steps(args.i_print):\n                tf.contrib.summary.scalar('loss', loss)\n                tf.contrib.summary.scalar('psnr', psnr)\n                tf.contrib.summary.histogram('tran', trans)\n                if args.N_importance > 0:\n                    tf.contrib.summary.scalar('psnr0', psnr0)\n\n\n            if i%args.i_img==0:\n\n                # Log a rendered validation view to Tensorboard\n                img_i=np.random.choice(i_val)\n                target = images[img_i]\n                pose = poses[img_i, :3,:4]\n                with torch.no_grad():\n                    rgb, disp, acc, extras = render(H, W, focal, chunk=args.chunk, c2w=pose,\n                                                        **render_kwargs_test)\n\n                psnr = mse2psnr(img2mse(rgb, target))\n\n                with tf.contrib.summary.record_summaries_every_n_global_steps(args.i_img):\n\n                    tf.contrib.summary.image('rgb', to8b(rgb)[tf.newaxis])\n                    tf.contrib.summary.image('disp', disp[tf.newaxis,...,tf.newaxis])\n                    tf.contrib.summary.image('acc', acc[tf.newaxis,...,tf.newaxis])\n\n                    tf.contrib.summary.scalar('psnr_holdout', psnr)\n                    tf.contrib.summary.image('rgb_holdout', target[tf.newaxis])\n\n\n                if args.N_importance > 0:\n\n                    with tf.contrib.summary.record_summaries_every_n_global_steps(args.i_img):\n                        tf.contrib.summary.image('rgb0', to8b(extras['rgb0'])[tf.newaxis])\n                        tf.contrib.summary.image('disp0', extras['disp0'][tf.newaxis,...,tf.newaxis])\n                        tf.contrib.summary.image('z_std', extras['z_std'][tf.newaxis,...,tf.newaxis])\n        \"\"\"\n\n        global_step += 1\n\n\nif __name__=='__main__':\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n\n    train()\n"
        },
        {
          "name": "run_nerf_helpers.py",
          "type": "blob",
          "size": 9.0263671875,
          "content": "import torch\n# torch.autograd.set_detect_anomaly(True)\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\n# Misc\nimg2mse = lambda x, y : torch.mean((x - y) ** 2)\nmse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.]))\nto8b = lambda x : (255*np.clip(x,0,1)).astype(np.uint8)\n\n\n# Positional encoding (section 5.1)\nclass Embedder:\n    def __init__(self, **kwargs):\n        self.kwargs = kwargs\n        self.create_embedding_fn()\n        \n    def create_embedding_fn(self):\n        embed_fns = []\n        d = self.kwargs['input_dims']\n        out_dim = 0\n        if self.kwargs['include_input']:\n            embed_fns.append(lambda x : x)\n            out_dim += d\n            \n        max_freq = self.kwargs['max_freq_log2']\n        N_freqs = self.kwargs['num_freqs']\n        \n        if self.kwargs['log_sampling']:\n            freq_bands = 2.**torch.linspace(0., max_freq, steps=N_freqs)\n        else:\n            freq_bands = torch.linspace(2.**0., 2.**max_freq, steps=N_freqs)\n            \n        for freq in freq_bands:\n            for p_fn in self.kwargs['periodic_fns']:\n                embed_fns.append(lambda x, p_fn=p_fn, freq=freq : p_fn(x * freq))\n                out_dim += d\n                    \n        self.embed_fns = embed_fns\n        self.out_dim = out_dim\n        \n    def embed(self, inputs):\n        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n\n\ndef get_embedder(multires, i=0):\n    if i == -1:\n        return nn.Identity(), 3\n    \n    embed_kwargs = {\n                'include_input' : True,\n                'input_dims' : 3,\n                'max_freq_log2' : multires-1,\n                'num_freqs' : multires,\n                'log_sampling' : True,\n                'periodic_fns' : [torch.sin, torch.cos],\n    }\n    \n    embedder_obj = Embedder(**embed_kwargs)\n    embed = lambda x, eo=embedder_obj : eo.embed(x)\n    return embed, embedder_obj.out_dim\n\n\n# Model\nclass NeRF(nn.Module):\n    def __init__(self, D=8, W=256, input_ch=3, input_ch_views=3, output_ch=4, skips=[4], use_viewdirs=False):\n        \"\"\" \n        \"\"\"\n        super(NeRF, self).__init__()\n        self.D = D\n        self.W = W\n        self.input_ch = input_ch\n        self.input_ch_views = input_ch_views\n        self.skips = skips\n        self.use_viewdirs = use_viewdirs\n        \n        self.pts_linears = nn.ModuleList(\n            [nn.Linear(input_ch, W)] + [nn.Linear(W, W) if i not in self.skips else nn.Linear(W + input_ch, W) for i in range(D-1)])\n        \n        ### Implementation according to the official code release (https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py#L104-L105)\n        self.views_linears = nn.ModuleList([nn.Linear(input_ch_views + W, W//2)])\n\n        ### Implementation according to the paper\n        # self.views_linears = nn.ModuleList(\n        #     [nn.Linear(input_ch_views + W, W//2)] + [nn.Linear(W//2, W//2) for i in range(D//2)])\n        \n        if use_viewdirs:\n            self.feature_linear = nn.Linear(W, W)\n            self.alpha_linear = nn.Linear(W, 1)\n            self.rgb_linear = nn.Linear(W//2, 3)\n        else:\n            self.output_linear = nn.Linear(W, output_ch)\n\n    def forward(self, x):\n        input_pts, input_views = torch.split(x, [self.input_ch, self.input_ch_views], dim=-1)\n        h = input_pts\n        for i, l in enumerate(self.pts_linears):\n            h = self.pts_linears[i](h)\n            h = F.relu(h)\n            if i in self.skips:\n                h = torch.cat([input_pts, h], -1)\n\n        if self.use_viewdirs:\n            alpha = self.alpha_linear(h)\n            feature = self.feature_linear(h)\n            h = torch.cat([feature, input_views], -1)\n        \n            for i, l in enumerate(self.views_linears):\n                h = self.views_linears[i](h)\n                h = F.relu(h)\n\n            rgb = self.rgb_linear(h)\n            outputs = torch.cat([rgb, alpha], -1)\n        else:\n            outputs = self.output_linear(h)\n\n        return outputs    \n\n    def load_weights_from_keras(self, weights):\n        assert self.use_viewdirs, \"Not implemented if use_viewdirs=False\"\n        \n        # Load pts_linears\n        for i in range(self.D):\n            idx_pts_linears = 2 * i\n            self.pts_linears[i].weight.data = torch.from_numpy(np.transpose(weights[idx_pts_linears]))    \n            self.pts_linears[i].bias.data = torch.from_numpy(np.transpose(weights[idx_pts_linears+1]))\n        \n        # Load feature_linear\n        idx_feature_linear = 2 * self.D\n        self.feature_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_feature_linear]))\n        self.feature_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_feature_linear+1]))\n\n        # Load views_linears\n        idx_views_linears = 2 * self.D + 2\n        self.views_linears[0].weight.data = torch.from_numpy(np.transpose(weights[idx_views_linears]))\n        self.views_linears[0].bias.data = torch.from_numpy(np.transpose(weights[idx_views_linears+1]))\n\n        # Load rgb_linear\n        idx_rbg_linear = 2 * self.D + 4\n        self.rgb_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_rbg_linear]))\n        self.rgb_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_rbg_linear+1]))\n\n        # Load alpha_linear\n        idx_alpha_linear = 2 * self.D + 6\n        self.alpha_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_alpha_linear]))\n        self.alpha_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_alpha_linear+1]))\n\n\n\n# Ray helpers\ndef get_rays(H, W, K, c2w):\n    i, j = torch.meshgrid(torch.linspace(0, W-1, W), torch.linspace(0, H-1, H))  # pytorch's meshgrid has indexing='ij'\n    i = i.t()\n    j = j.t()\n    dirs = torch.stack([(i-K[0][2])/K[0][0], -(j-K[1][2])/K[1][1], -torch.ones_like(i)], -1)\n    # Rotate ray directions from camera frame to the world frame\n    rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n    rays_o = c2w[:3,-1].expand(rays_d.shape)\n    return rays_o, rays_d\n\n\ndef get_rays_np(H, W, K, c2w):\n    i, j = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy')\n    dirs = np.stack([(i-K[0][2])/K[0][0], -(j-K[1][2])/K[1][1], -np.ones_like(i)], -1)\n    # Rotate ray directions from camera frame to the world frame\n    rays_d = np.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n    rays_o = np.broadcast_to(c2w[:3,-1], np.shape(rays_d))\n    return rays_o, rays_d\n\n\ndef ndc_rays(H, W, focal, near, rays_o, rays_d):\n    # Shift ray origins to near plane\n    t = -(near + rays_o[...,2]) / rays_d[...,2]\n    rays_o = rays_o + t[...,None] * rays_d\n    \n    # Projection\n    o0 = -1./(W/(2.*focal)) * rays_o[...,0] / rays_o[...,2]\n    o1 = -1./(H/(2.*focal)) * rays_o[...,1] / rays_o[...,2]\n    o2 = 1. + 2. * near / rays_o[...,2]\n\n    d0 = -1./(W/(2.*focal)) * (rays_d[...,0]/rays_d[...,2] - rays_o[...,0]/rays_o[...,2])\n    d1 = -1./(H/(2.*focal)) * (rays_d[...,1]/rays_d[...,2] - rays_o[...,1]/rays_o[...,2])\n    d2 = -2. * near / rays_o[...,2]\n    \n    rays_o = torch.stack([o0,o1,o2], -1)\n    rays_d = torch.stack([d0,d1,d2], -1)\n    \n    return rays_o, rays_d\n\n\n# Hierarchical sampling (section 5.2)\ndef sample_pdf(bins, weights, N_samples, det=False, pytest=False):\n    # Get pdf\n    weights = weights + 1e-5 # prevent nans\n    pdf = weights / torch.sum(weights, -1, keepdim=True)\n    cdf = torch.cumsum(pdf, -1)\n    cdf = torch.cat([torch.zeros_like(cdf[...,:1]), cdf], -1)  # (batch, len(bins))\n\n    # Take uniform samples\n    if det:\n        u = torch.linspace(0., 1., steps=N_samples)\n        u = u.expand(list(cdf.shape[:-1]) + [N_samples])\n    else:\n        u = torch.rand(list(cdf.shape[:-1]) + [N_samples])\n\n    # Pytest, overwrite u with numpy's fixed random numbers\n    if pytest:\n        np.random.seed(0)\n        new_shape = list(cdf.shape[:-1]) + [N_samples]\n        if det:\n            u = np.linspace(0., 1., N_samples)\n            u = np.broadcast_to(u, new_shape)\n        else:\n            u = np.random.rand(*new_shape)\n        u = torch.Tensor(u)\n\n    # Invert CDF\n    u = u.contiguous()\n    inds = torch.searchsorted(cdf, u, right=True)\n    below = torch.max(torch.zeros_like(inds-1), inds-1)\n    above = torch.min((cdf.shape[-1]-1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)  # (batch, N_samples, 2)\n\n    # cdf_g = tf.gather(cdf, inds_g, axis=-1, batch_dims=len(inds_g.shape)-2)\n    # bins_g = tf.gather(bins, inds_g, axis=-1, batch_dims=len(inds_g.shape)-2)\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(bins.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[...,1]-cdf_g[...,0])\n    denom = torch.where(denom<1e-5, torch.ones_like(denom), denom)\n    t = (u-cdf_g[...,0])/denom\n    samples = bins_g[...,0] + t * (bins_g[...,1]-bins_g[...,0])\n\n    return samples\n"
        }
      ]
    }
  ]
}