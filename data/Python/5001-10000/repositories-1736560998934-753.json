{
  "metadata": {
    "timestamp": 1736560998934,
    "page": 753,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "frdel/agent-zero",
      "stars": 5337,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0703125,
          "content": "# Auto detect text files and perform LF normalization\n* text=auto eol=lf"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.7734375,
          "content": "# Ignore common unwanted files globally\n**/.DS_Store\n**/.env\n**/__pycache__/\n**/.conda/\n\n# Ignore git internal files (for bundler)\n.git/\n\n# Ignore all contents of the virtual environment directory\n.venv/\n\n# Handle bundle directory\nbundle/*/\n!bundle/mac_pkg_scripts\n\n# Handle work_dir directory\nwork_dir/*\n\n# Handle memory directory\nmemory/**\n!memory/**/\n\n# Handle logs directory\nlogs/*\n\n# Handle tmp directory\ntmp/*\n\n# Handle knowledge directory\nknowledge/**\n!knowledge/**/\n# Explicitly allow the default folder in knowledge\n!knowledge/default/\n!knowledge/default/**\n\n# Handle instruments directory\ninstruments/**\n!instruments/**/\n# Explicitly allow the default folder in instruments\n!instruments/default/\n!instruments/default/**\n\n# Global rule to include .gitkeep files anywhere\n!**/.gitkeep"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.1220703125,
          "content": "MIT License\n\nCopyright (c) 2024 Jan Tom√°≈°ek\nContact: tomasekhonza@gmail.com\nRepository: https://github.com/frdel/agent-zero\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.5517578125,
          "content": "<div align=\"center\">\n\n![Agent Zero](/docs/res/header.png)\n\n# `Agent Zero`\n\n[![Join our Skool Community](https://img.shields.io/badge/Skool-Join%20our%20Community-4A90E2?style=for-the-badge&logo=skool&logoColor=white)](https://www.skool.com/agent-zero) [![Join our Discord](https://img.shields.io/badge/Discord-Join%20our%20server-5865F2?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/B8KZKNsPpj) [![Subscribe on YouTube](https://img.shields.io/badge/YouTube-Subscribe-red?style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/@AgentZeroFW) [![Connect on LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/jan-tomasek/) [![Follow on X.com](https://img.shields.io/badge/X.com-Follow-1DA1F2?style=for-the-badge&logo=x&logoColor=white)](https://x.com/JanTomasekDev)\n\n[Installation](./docs/installation.md) ‚Ä¢\n[How to update](./docs/installation.md#how-to-update-agent-zero) ‚Ä¢\n[Documentation](./docs/README.md) ‚Ä¢\n[Usage](./docs/usage.md)\n\n</div>\n\nhttps://github.com/user-attachments/assets/c168759d-57d8-4b43-b62a-1026afcf52e6\n\n## A personal, organic agentic framework that grows and learns with you\n\n- Agent Zero is not a predefined agentic framework. It is designed to be dynamic, organically growing, and learning as you use it.\n- Agent Zero is fully transparent, readable, comprehensible, customizable, and interactive.\n- Agent Zero uses the computer as a tool to accomplish its (your) tasks.\n\n# üí° Key Features\n\n1. **General-purpose Assistant**\n\n- Agent Zero is not pre-programmed for specific tasks (but can be). It is meant to be a general-purpose personal assistant. Give it a task, and it will gather information, execute commands and code, cooperate with other agent instances, and do its best to accomplish it.\n- It has a persistent memory, allowing it to memorize previous solutions, code, facts, instructions, etc., to solve tasks faster and more reliably in the future.\n\n![Agent 0 Working](/docs/res/ui-screen-2.png)\n\n2. **Computer as a Tool**\n\n- Agent Zero uses the operating system as a tool to accomplish its tasks. It has no single-purpose tools pre-programmed. Instead, it can write its own code and use the terminal to create and use its own tools as needed.\n- The only default tools in its arsenal are online search, memory features, communication (with the user and other agents), and code/terminal execution. Everything else is created by the agent itself or can be extended by the user.\n- Tool usage functionality has been developed from scratch to be the most compatible and reliable, even with very small models.\n- **Default Tools:** Agent Zero includes tools like knowledge, webpage content, code execution, and communication.\n- **Creating Custom Tools:** Extend Agent Zero's functionality by creating your own custom tools.\n- **Instruments:** Instruments are a new type of tool that allow you to create custom functions and procedures that can be called by Agent Zero.\n\n3. **Multi-agent Cooperation**\n\n- Every agent has a superior agent giving it tasks and instructions. Every agent then reports back to its superior.\n- In the case of the first agent in the chain (Agent 0), the superior is the human user; the agent sees no difference.\n- Every agent can create its subordinate agent to help break down and solve subtasks. This helps all agents keep their context clean and focused.\n\n![Multi-agent](docs/res/physics.png)\n![Multi-agent 2](docs/res/physics-2.png)\n\n4. **Completely Customizable and Extensible**\n\n- Almost nothing in this framework is hard-coded. Nothing is hidden. Everything can be extended or changed by the user.\n- The whole behavior is defined by a system prompt in the **prompts/default/agent.system.md** file. Change this prompt and change the framework dramatically.\n- The framework does not guide or limit the agent in any way. There are no hard-coded rails that agents have to follow.\n- Every prompt, every small message template sent to the agent in its communication loop can be found in the **prompts/** folder and changed.\n- Every default tool can be found in the **python/tools/** folder and changed or copied to create new predefined tools.\n\n![Prompts](/docs/res/prompts.png)\n\n5. **Communication is Key**\n\n- Give your agent a proper system prompt and instructions, and it can do miracles.\n- Agents can communicate with their superiors and subordinates, asking questions, giving instructions, and providing guidance. Instruct your agents in the system prompt on how to communicate effectively.\n- The terminal interface is real-time streamed and interactive. You can stop and intervene at any point. If you see your agent heading in the wrong direction, just stop and tell it right away.\n- There is a lot of freedom in this framework. You can instruct your agents to regularly report back to superiors asking for permission to continue. You can instruct them to use point-scoring systems when deciding when to delegate subtasks. Superiors can double-check subordinates' results and dispute. The possibilities are endless.\n\n## üöÄ Things you can build with Agent Zero\n\n- **Development Projects** - `\"Create a React dashboard with real-time data visualization\"`\n\n- **Data Analysis** - `\"Analyze last quarter's NVIDIA sales data and create trend reports\"`\n\n- **Content Creation** - `\"Write a technical blog post about microservices\"`\n\n- **System Admin** - `\"Set up a monitoring system for our web servers\"`\n\n- **Research** - `\"Gather and summarize five recent AI papers about CoT prompting\"`\n\n# ‚öôÔ∏è Installation\n\nClick to open a video to learn how to install Agent Zero:\n\n[![Testing Video](/docs/res/new_vid.jpg)](https://www.youtube.com/watch?v=cHDCCSr1YRI&t=24s)\n\nA detailed setup guide for Windows, macOS, and Linux with a video can be found in the Agent Zero Documentation at [this page](./docs/installation.md).\n\n### ‚ö° Quick Start\n\n```bash\n# Pull and run with Docker\n\ndocker pull frdel/agent-zero-run\ndocker run -p 50001:80 frdel/agent-zero-run\n\n# Visit http://localhost:50001 to start\n```\n\n- Developers and contributors: download the full binaries for your system from the [releases page](https://github.com/frdel/agent-zero/releases) and then follow the instructions [provided here](./docs/installation.md#in-depth-guide-for-full-binaries-installation).\n\n## üê≥ Fully Dockerized, with Speech-to-Text and TTS\n\n![Settings](docs/res/settings-page-ui.png)\n\n- Customizable settings allow users to tailor the agent's behavior and responses to their needs.\n- The Web UI output is very clean, fluid, colorful, readable, and interactive; nothing is hidden.\n- You can load or save chats directly within the Web UI.\n- The same output you see in the terminal is automatically saved to an HTML file in **logs/** folder for every session.\n\n![Time example](/docs/res/time_example.jpg)\n\n- Agent output is streamed in real-time, allowing users to read along and intervene at any time.\n- No coding is required; only prompting and communication skills are necessary.\n- With a solid system prompt, the framework is reliable even with small models, including precise tool usage.\n\n## üëÄ Keep in Mind\n\n1. **Agent Zero Can Be Dangerous!**\n\n- With proper instruction, Agent Zero is capable of many things, even potentially dangerous actions concerning your computer, data, or accounts. Always run Agent Zero in an isolated environment (like Docker) and be careful what you wish for.\n\n2. **Agent Zero Is Not Pre-programmed; It Is Prompt-based.**\n\n- The whole framework contains only a minimal amount of code and does not guide the agent in any way. Everything lies in the system prompt located in the **prompts/** folder.\n\n3. **If You Cannot Provide the Ideal Environment, Let Your Agent Know.**\n\n- Agent Zero is made to be used in an isolated virtual environment (for safety) with some tools preinstalled and configured.\n\n### üìå Known Problems\n\n1. The system prompt may need improvements; contributions are welcome!\n2. The agent may inadvertently alter its operating environment; cleaning up the `work_dir` often fixes this.\n3. Agents might loop in multi-agentic interactions, leading to unexpected behaviors.\n\n## üìö Read the Documentation\n\n| Page | Description |\n|-------|-------------|\n| [Installation](./docs/installation.md) | Installation, setup and configuration |\n| [Usage](./docs/usage.md) | Basic and advanced usage |\n| [Architecture](./docs/architecture.md) | System design and components |\n| [Contributing](./docs/contributing.md) | How to contribute |\n| [Troubleshooting](./docs/troubleshooting.md) | Common issues and their solutions |\n\n## üéØ Changelog\n\n### Coming soon\n\n- **User Interaction Refinements**\n- **Browser Use and RAG Tools**\n\n> [!IMPORTANT]\n>\n>**Changes to frdel/agent-zero Docker image since v0.7:**\n>\n> The new Docker image `frdel/agent-zero-run` provides the new unified environment.\n\n### v0.8\n\n- **Docker Runtime**\n- **New Messages History and Summarization System**\n- **Agent Behavior Change and Management**\n- **Text-to-Speech (TTS) and Speech-to-Text (STT)**\n- **Settings Page in Web UI**\n- **SearXNG Integration Replacing Perplexity + DuckDuckGo**\n- **File Browser Functionality**\n- **KaTeX Math Visualization Support**\n- **In-chat File Attachments**\n\n### v0.7\n\n- **Automatic Memory**\n- **UI Improvements**\n- **Instruments**\n- **Extensions Framework**\n- **Reflection Prompts**\n- **Bug Fixes**\n\n## ü§ù Community and Support\n\n- [Join our Discord](https://discord.gg/B8KZKNsPpj) for live discussions or [visit our Skool Community](https://www.skool.com/agent-zero).\n- [Follow our YouTube channel](https://www.youtube.com/@AgentZeroFW) for hands-on explanations and tutorials\n- [Report Issues](https://github.com/frdel/agent-zero/issues) for bug fixes and features\n"
        },
        {
          "name": "agent.py",
          "type": "blob",
          "size": 24.33203125,
          "content": "import asyncio\nfrom collections import OrderedDict\nfrom dataclasses import dataclass, field\nimport time, importlib, inspect, os, json\nimport token\nfrom typing import Any, Awaitable, Optional, Dict, TypedDict\nimport uuid\nimport models\n\nfrom langchain_core.prompt_values import ChatPromptValue\nfrom python.helpers import extract_tools, rate_limiter, files, errors, history, tokens\nfrom python.helpers.print_style import PrintStyle\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.messages import HumanMessage, SystemMessage, AIMessage\nfrom langchain_core.language_models.chat_models import BaseChatModel\nfrom langchain_core.language_models.llms import BaseLLM\nfrom langchain_core.embeddings import Embeddings\nimport python.helpers.log as Log\nfrom python.helpers.dirty_json import DirtyJson\nfrom python.helpers.defer import DeferredTask\nfrom typing import Callable\n\n\nclass AgentContext:\n\n    _contexts: dict[str, \"AgentContext\"] = {}\n    _counter: int = 0\n\n    def __init__(\n        self,\n        config: \"AgentConfig\",\n        id: str | None = None,\n        name: str | None = None,\n        agent0: \"Agent|None\" = None,\n        log: Log.Log | None = None,\n        paused: bool = False,\n        streaming_agent: \"Agent|None\" = None,\n    ):\n        # build context\n        self.id = id or str(uuid.uuid4())\n        self.name = name\n        self.config = config\n        self.log = log or Log.Log()\n        self.agent0 = agent0 or Agent(0, self.config, self)\n        self.paused = paused\n        self.streaming_agent = streaming_agent\n        self.process: DeferredTask | None = None\n        AgentContext._counter += 1\n        self.no = AgentContext._counter\n\n        existing = self._contexts.get(self.id, None)\n        if existing:\n            AgentContext.remove(self.id)\n        self._contexts[self.id] = self\n\n    @staticmethod\n    def get(id: str):\n        return AgentContext._contexts.get(id, None)\n\n    @staticmethod\n    def first():\n        if not AgentContext._contexts:\n            return None\n        return list(AgentContext._contexts.values())[0]\n\n    @staticmethod\n    def remove(id: str):\n        context = AgentContext._contexts.pop(id, None)\n        if context and context.process:\n            context.process.kill()\n        return context\n\n    def kill_process(self):\n        if self.process:\n            self.process.kill()\n\n    def reset(self):\n        self.kill_process()\n        self.log.reset()\n        self.agent0 = Agent(0, self.config, self)\n        self.streaming_agent = None\n        self.paused = False\n\n    def nudge(self):\n        self.kill_process()\n        self.paused = False\n        if self.streaming_agent:\n            current_agent = self.streaming_agent\n        else:\n            current_agent = self.agent0\n\n        self.process = DeferredTask(current_agent.monologue)\n        return self.process\n\n    def communicate(self, msg: \"UserMessage\", broadcast_level: int = 1):\n        self.paused = False  # unpause if paused\n\n        if self.streaming_agent:\n            current_agent = self.streaming_agent\n        else:\n            current_agent = self.agent0\n\n        if self.process and self.process.is_alive():\n            # set intervention messages to agent(s):\n            intervention_agent = current_agent\n            while intervention_agent and broadcast_level != 0:\n                intervention_agent.intervention = msg\n                broadcast_level -= 1\n                intervention_agent = intervention_agent.data.get(\n                    Agent.DATA_NAME_SUPERIOR, None\n                )\n        else:\n\n            # self.process = DeferredTask(current_agent.monologue, msg)\n            self.process = DeferredTask(self._process_chain, current_agent, msg)\n\n        return self.process\n\n    # this wrapper ensures that superior agents are called back if the chat was loaded from file and original callstack is gone\n    async def _process_chain(self, agent: \"Agent\", msg: \"UserMessage|str\", user=True):\n        try:\n            msg_template = (\n                await agent.hist_add_user_message(msg)  # type: ignore\n                if user\n                else await agent.hist_add_tool_result(\n                    tool_name=\"call_subordinate\", tool_result=msg  # type: ignore\n                )\n            )\n            response = await agent.monologue()\n            superior = agent.data.get(Agent.DATA_NAME_SUPERIOR, None)\n            if superior:\n                response = await self._process_chain(superior, response, False)\n            return response\n        except Exception as e:\n            agent.handle_critical_exception(e)\n\n\n@dataclass\nclass ModelConfig:\n    provider: models.ModelProvider\n    name: str\n    ctx_length: int\n    limit_requests: int\n    limit_input: int\n    limit_output: int\n    kwargs: dict\n\n\n@dataclass\nclass AgentConfig:\n    chat_model: ModelConfig\n    utility_model: ModelConfig\n    embeddings_model: ModelConfig\n    prompts_subdir: str = \"\"\n    memory_subdir: str = \"\"\n    knowledge_subdirs: list[str] = field(default_factory=lambda: [\"default\", \"custom\"])\n    code_exec_docker_enabled: bool = False\n    code_exec_docker_name: str = \"A0-dev\"\n    code_exec_docker_image: str = \"frdel/agent-zero-run:development\"\n    code_exec_docker_ports: dict[str, int] = field(\n        default_factory=lambda: {\"22/tcp\": 55022, \"80/tcp\": 55080}\n    )\n    code_exec_docker_volumes: dict[str, dict[str, str]] = field(\n        default_factory=lambda: {\n            files.get_base_dir(): {\"bind\": \"/a0\", \"mode\": \"rw\"},\n            files.get_abs_path(\"work_dir\"): {\"bind\": \"/root\", \"mode\": \"rw\"},\n        }\n    )\n    code_exec_ssh_enabled: bool = True\n    code_exec_ssh_addr: str = \"localhost\"\n    code_exec_ssh_port: int = 55022\n    code_exec_ssh_user: str = \"root\"\n    code_exec_ssh_pass: str = \"\"\n    additional: Dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass\nclass UserMessage:\n    message: str\n    attachments: list[str]\n\n\nclass LoopData:\n    def __init__(self, **kwargs):\n        self.iteration = -1\n        self.system = []\n        self.user_message: history.Message | None = None\n        self.history_output: list[history.OutputMessage] = []\n        self.extras_temporary: OrderedDict[str, history.MessageContent] = OrderedDict()\n        self.extras_persistent: OrderedDict[str, history.MessageContent] = OrderedDict()\n        self.last_response = \"\"\n\n        # override values with kwargs\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n\n\n# intervention exception class - skips rest of message loop iteration\nclass InterventionException(Exception):\n    pass\n\n\n# killer exception class - not forwarded to LLM, cannot be fixed on its own, ends message loop\nclass RepairableException(Exception):\n    pass\n\n\nclass HandledException(Exception):\n    pass\n\n\nclass Agent:\n\n    DATA_NAME_SUPERIOR = \"_superior\"\n    DATA_NAME_SUBORDINATE = \"_subordinate\"\n    DATA_NAME_CTX_WINDOW = \"ctx_window\"\n\n    def __init__(\n        self, number: int, config: AgentConfig, context: AgentContext | None = None\n    ):\n\n        # agent config\n        self.config = config\n\n        # agent context\n        self.context = context or AgentContext(config)\n\n        # non-config vars\n        self.number = number\n        self.agent_name = f\"Agent {self.number}\"\n\n        self.history = history.History(self)\n        self.last_user_message: history.Message | None = None\n        self.intervention: UserMessage | None = None\n        self.data = {}  # free data object all the tools can use\n\n    async def monologue(self):\n        while True:\n            try:\n                # loop data dictionary to pass to extensions\n                self.loop_data = LoopData(user_message=self.last_user_message)\n                # call monologue_start extensions\n                await self.call_extensions(\"monologue_start\", loop_data=self.loop_data)\n\n                printer = PrintStyle(italic=True, font_color=\"#b3ffd9\", padding=False)\n\n                # let the agent run message loop until he stops it with a response tool\n                while True:\n\n                    self.context.streaming_agent = self  # mark self as current streamer\n                    self.loop_data.iteration += 1\n\n                    try:\n                        # prepare LLM chain (model, system, history)\n                        prompt = await self.prepare_prompt(loop_data=self.loop_data)\n\n                        # output that the agent is starting\n                        PrintStyle(\n                            bold=True,\n                            font_color=\"green\",\n                            padding=True,\n                            background_color=\"white\",\n                        ).print(f\"{self.agent_name}: Generating\")\n                        log = self.context.log.log(\n                            type=\"agent\", heading=f\"{self.agent_name}: Generating\"\n                        )\n\n                        async def stream_callback(chunk: str, full: str):\n                            # output the agent response stream\n                            if chunk:\n                                printer.stream(chunk)\n                                self.log_from_stream(full, log)\n\n                        # store as last context window content\n                        self.set_data(Agent.DATA_NAME_CTX_WINDOW, prompt.format())\n\n                        agent_response = await self.call_chat_model(\n                            prompt, callback=stream_callback\n                        )\n\n                        await self.handle_intervention(agent_response)\n\n                        if (\n                            self.loop_data.last_response == agent_response\n                        ):  # if assistant_response is the same as last message in history, let him know\n                            # Append the assistant's response to the history\n                            await self.hist_add_ai_response(agent_response)\n                            # Append warning message to the history\n                            warning_msg = self.read_prompt(\"fw.msg_repeat.md\")\n                            await self.hist_add_warning(message=warning_msg)\n                            PrintStyle(font_color=\"orange\", padding=True).print(\n                                warning_msg\n                            )\n                            self.context.log.log(type=\"warning\", content=warning_msg)\n\n                        else:  # otherwise proceed with tool\n                            # Append the assistant's response to the history\n                            await self.hist_add_ai_response(agent_response)\n                            # process tools requested in agent message\n                            tools_result = await self.process_tools(agent_response)\n                            if tools_result:  # final response of message loop available\n                                return tools_result  # break the execution if the task is done\n\n                    # exceptions inside message loop:\n                    except InterventionException as e:\n                        pass  # intervention message has been handled in handle_intervention(), proceed with conversation loop\n                    except RepairableException as e:\n                        # Forward repairable errors to the LLM, maybe it can fix them\n                        error_message = errors.format_error(e)\n                        await self.hist_add_warning(error_message)\n                        PrintStyle(font_color=\"red\", padding=True).print(error_message)\n                        self.context.log.log(type=\"error\", content=error_message)\n                    except Exception as e:\n                        # Other exception kill the loop\n                        self.handle_critical_exception(e)\n\n                    finally:\n                        # call message_loop_end extensions\n                        await self.call_extensions(\n                            \"message_loop_end\", loop_data=self.loop_data\n                        )\n\n            # exceptions outside message loop:\n            except InterventionException as e:\n                pass  # just start over\n            except Exception as e:\n                self.handle_critical_exception(e)\n            finally:\n                self.context.streaming_agent = None  # unset current streamer\n                # call monologue_end extensions\n                await self.call_extensions(\"monologue_end\", loop_data=self.loop_data)  # type: ignore\n\n    async def prepare_prompt(self, loop_data: LoopData) -> ChatPromptTemplate:\n        # set system prompt and message history\n        loop_data.system = await self.get_system_prompt(self.loop_data)\n        loop_data.history_output = self.history.output()\n\n        # and allow extensions to edit them\n        await self.call_extensions(\"message_loop_prompts\", loop_data=loop_data)\n\n        # extras (memory etc.)\n        extras: list[history.OutputMessage] = []\n        for extra in loop_data.extras_persistent.values():\n            extras += history.Message(False, content=extra).output()\n        for extra in loop_data.extras_temporary.values():\n            extras += history.Message(False, content=extra).output()\n        loop_data.extras_temporary.clear()\n\n        # combine history and extras\n        history_combined = history.group_outputs_abab(loop_data.history_output + extras)\n\n        # convert history to LLM format\n        history_langchain = history.output_langchain(history_combined)\n\n        # build chain from system prompt, message history and model\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                SystemMessage(content=\"\\n\\n\".join(loop_data.system)),\n                *history_langchain,\n            ]\n        )\n        return prompt\n\n    def handle_critical_exception(self, exception: Exception):\n        if isinstance(exception, HandledException):\n            raise exception  # Re-raise the exception to kill the loop\n        elif isinstance(exception, asyncio.CancelledError):\n            # Handling for asyncio.CancelledError\n            PrintStyle(font_color=\"white\", background_color=\"red\", padding=True).print(\n                f\"Context {self.context.id} terminated during message loop\"\n            )\n            raise HandledException(\n                exception\n            )  # Re-raise the exception to cancel the loop\n        else:\n            # Handling for general exceptions\n            error_text = errors.error_text(exception)\n            error_message = errors.format_error(exception)\n            PrintStyle(font_color=\"red\", padding=True).print(error_message)\n            self.context.log.log(\n                type=\"error\",\n                heading=\"Error\",\n                content=error_message,\n                kvps={\"text\": error_text},\n            )\n            raise HandledException(exception)  # Re-raise the exception to kill the loop\n\n    async def get_system_prompt(self, loop_data: LoopData) -> list[str]:\n        system_prompt = []\n        await self.call_extensions(\n            \"system_prompt\", system_prompt=system_prompt, loop_data=loop_data\n        )\n        return system_prompt\n\n    def parse_prompt(self, file: str, **kwargs):\n        prompt_dir = files.get_abs_path(\"prompts/default\")\n        backup_dir = []\n        if (\n            self.config.prompts_subdir\n        ):  # if agent has custom folder, use it and use default as backup\n            prompt_dir = files.get_abs_path(\"prompts\", self.config.prompts_subdir)\n            backup_dir.append(files.get_abs_path(\"prompts/default\"))\n        prompt = files.parse_file(\n            files.get_abs_path(prompt_dir, file), _backup_dirs=backup_dir, **kwargs\n        )\n        return prompt\n\n    def read_prompt(self, file: str, **kwargs) -> str:\n        prompt_dir = files.get_abs_path(\"prompts/default\")\n        backup_dir = []\n        if (\n            self.config.prompts_subdir\n        ):  # if agent has custom folder, use it and use default as backup\n            prompt_dir = files.get_abs_path(\"prompts\", self.config.prompts_subdir)\n            backup_dir.append(files.get_abs_path(\"prompts/default\"))\n        prompt = files.read_file(\n            files.get_abs_path(prompt_dir, file), _backup_dirs=backup_dir, **kwargs\n        )\n        prompt = files.remove_code_fences(prompt)\n        return prompt\n\n    def get_data(self, field: str):\n        return self.data.get(field, None)\n\n    def set_data(self, field: str, value):\n        self.data[field] = value\n\n    def hist_add_message(self, ai: bool, content: history.MessageContent):\n        return self.history.add_message(ai=ai, content=content)\n\n    async def hist_add_user_message(\n        self, message: UserMessage, intervention: bool = False\n    ):\n        self.history.new_topic()  # user message starts a new topic in history\n\n        # load message template based on intervention\n        if intervention:\n            content = self.parse_prompt(\n                \"fw.intervention.md\",\n                message=message.message,\n                attachments=message.attachments,\n            )\n        else:\n            content = self.parse_prompt(\n                \"fw.user_message.md\",\n                message=message.message,\n                attachments=message.attachments,\n            )\n\n        # remove empty attachments from template\n        if (\n            isinstance(content, dict)\n            and \"attachments\" in content\n            and not content[\"attachments\"]\n        ):\n            del content[\"attachments\"]\n\n        # add to history\n        msg = self.hist_add_message(False, content=content)  # type: ignore\n        self.last_user_message = msg\n        return msg\n\n    async def hist_add_ai_response(self, message: str):\n        self.loop_data.last_response = message\n        content = self.parse_prompt(\"fw.ai_response.md\", message=message)\n        return self.hist_add_message(True, content=content)\n\n    async def hist_add_warning(self, message: history.MessageContent):\n        content = self.parse_prompt(\"fw.warning.md\", message=message)\n        return self.hist_add_message(False, content=content)\n\n    async def hist_add_tool_result(self, tool_name: str, tool_result: str):\n        content = self.parse_prompt(\n            \"fw.tool_result.md\", tool_name=tool_name, tool_result=tool_result\n        )\n        return self.hist_add_message(False, content=content)\n\n    def concat_messages(\n        self, messages\n    ):  # TODO add param for message range, topic, history\n        return self.history.output_text(human_label=\"user\", ai_label=\"assistant\")\n\n    async def call_utility_model(\n        self,\n        system: str,\n        message: str,\n        callback: Callable[[str], Awaitable[None]] | None = None,\n        background: bool = False,\n    ):\n        prompt = ChatPromptTemplate.from_messages(\n            [SystemMessage(content=system), HumanMessage(content=message)]\n        )\n\n        response = \"\"\n\n        # model class\n        model = models.get_model(\n            models.ModelType.CHAT,\n            self.config.utility_model.provider,\n            self.config.utility_model.name,\n            **self.config.utility_model.kwargs,\n        )\n\n        # rate limiter\n        limiter = await self.rate_limiter(\n            self.config.utility_model, prompt.format(), background\n        )\n\n        async for chunk in (prompt | model).astream({}):\n            await self.handle_intervention()  # wait for intervention and handle it, if paused\n\n            content = models.parse_chunk(chunk)\n            limiter.add(output=tokens.approximate_tokens(content))\n            response += content\n\n            if callback:\n                await callback(content)\n\n        return response\n\n    async def call_chat_model(\n        self,\n        prompt: ChatPromptTemplate,\n        callback: Callable[[str, str], Awaitable[None]] | None = None,\n    ):\n        response = \"\"\n\n        # model class\n        model = models.get_model(\n            models.ModelType.CHAT,\n            self.config.chat_model.provider,\n            self.config.chat_model.name,\n            **self.config.chat_model.kwargs,\n        )\n\n        # rate limiter\n        limiter = await self.rate_limiter(self.config.chat_model, prompt.format())\n\n        async for chunk in (prompt | model).astream({}):\n            await self.handle_intervention()  # wait for intervention and handle it, if paused\n\n            content = models.parse_chunk(chunk)\n            limiter.add(output=tokens.approximate_tokens(content))\n            response += content\n\n            if callback:\n                await callback(content, response)\n\n        return response\n\n    async def rate_limiter(\n        self, model_config: ModelConfig, input: str, background: bool = False\n    ):\n        # rate limiter log\n        wait_log = None\n\n        async def wait_callback(msg: str, key: str, total: int, limit: int):\n            nonlocal wait_log\n            if not wait_log:\n                wait_log = self.context.log.log(\n                    type=\"util\",\n                    update_progress=\"none\",\n                    heading=msg,\n                    model=f\"{model_config.provider.value}\\\\{model_config.name}\",\n                )\n            wait_log.update(heading=msg, key=key, value=total, limit=limit)\n            if not background:\n                self.context.log.set_progress(msg, -1)\n\n        # rate limiter\n        limiter = models.get_rate_limiter(\n            model_config.provider,\n            model_config.name,\n            model_config.limit_requests,\n            model_config.limit_input,\n            model_config.limit_output,\n        )\n        limiter.add(input=tokens.approximate_tokens(input))\n        limiter.add(requests=1)\n        await limiter.wait(callback=wait_callback)\n        return limiter\n\n    async def handle_intervention(self, progress: str = \"\"):\n        while self.context.paused:\n            await asyncio.sleep(0.1)  # wait if paused\n        if (\n            self.intervention\n        ):  # if there is an intervention message, but not yet processed\n            msg = self.intervention\n            self.intervention = None  # reset the intervention message\n            if progress.strip():\n                await self.hist_add_ai_response(progress)\n            # append the intervention message\n            await self.hist_add_user_message(msg, intervention=True)\n            raise InterventionException(msg)\n\n    async def process_tools(self, msg: str):\n        # search for tool usage requests in agent message\n        tool_request = extract_tools.json_parse_dirty(msg)\n\n        if tool_request is not None:\n            tool_name = tool_request.get(\"tool_name\", \"\")\n            tool_args = tool_request.get(\"tool_args\", {})\n            tool = self.get_tool(tool_name, tool_args, msg)\n\n            await self.handle_intervention()  # wait if paused and handle intervention message if needed\n            await tool.before_execution(**tool_args)\n            await self.handle_intervention()  # wait if paused and handle intervention message if needed\n            response = await tool.execute(**tool_args)\n            await self.handle_intervention()  # wait if paused and handle intervention message if needed\n            await tool.after_execution(response)\n            await self.handle_intervention()  # wait if paused and handle intervention message if needed\n            if response.break_loop:\n                return response.message\n        else:\n            msg = self.read_prompt(\"fw.msg_misformat.md\")\n            await self.hist_add_warning(msg)\n            PrintStyle(font_color=\"red\", padding=True).print(msg)\n            self.context.log.log(\n                type=\"error\", content=f\"{self.agent_name}: Message misformat\"\n            )\n\n    def log_from_stream(self, stream: str, logItem: Log.LogItem):\n        try:\n            if len(stream) < 25:\n                return  # no reason to try\n            response = DirtyJson.parse_string(stream)\n            if isinstance(response, dict):\n                # log if result is a dictionary already\n                logItem.update(content=stream, kvps=response)\n        except Exception as e:\n            pass\n\n    def get_tool(self, name: str, args: dict, message: str, **kwargs):\n        from python.tools.unknown import Unknown\n        from python.helpers.tool import Tool\n\n        classes = extract_tools.load_classes_from_folder(\n            \"python/tools\", name + \".py\", Tool\n        )\n        tool_class = classes[0] if classes else Unknown\n        return tool_class(agent=self, name=name, args=args, message=message, **kwargs)\n\n    async def call_extensions(self, folder: str, **kwargs) -> Any:\n        from python.helpers.extension import Extension\n\n        classes = extract_tools.load_classes_from_folder(\n            \"python/extensions/\" + folder, \"*\", Extension\n        )\n        for cls in classes:\n            await cls(agent=self).execute(**kwargs)\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "example.env",
          "type": "blob",
          "size": 0.5009765625,
          "content": "API_KEY_OPENAI=\nAPI_KEY_ANTHROPIC=\nAPI_KEY_GROQ=\nAPI_KEY_PERPLEXITY=\nAPI_KEY_GOOGLE=\nAPI_KEY_MISTRAL=\nAPI_KEY_OPENROUTER=\nAPI_KEY_SAMBANOVA=\n\nAPI_KEY_OPENAI_AZURE=\nOPENAI_AZURE_ENDPOINT=\nOPENAI_API_VERSION=\n\nHF_TOKEN=\n\n\nWEB_UI_PORT=50001\nUSE_CLOUDFLARE=false\n\n\nOLLAMA_BASE_URL=\"http://127.0.0.1:11434\"\nLM_STUDIO_BASE_URL=\"http://127.0.0.1:1234/v1\"\nOPEN_ROUTER_BASE_URL=\"https://openrouter.ai/api/v1\"\nSAMBANOVA_BASE_URL=\"https://fast-api.snova.ai/v1\"\n\n\nTOKENIZERS_PARALLELISM=true\nPYDEVD_DISABLE_FILE_VALIDATION=1\n"
        },
        {
          "name": "initialize.py",
          "type": "blob",
          "size": 4.755859375,
          "content": "import asyncio\nimport models\nfrom agent import AgentConfig, ModelConfig\nfrom python.helpers import dotenv, files, rfc_exchange, runtime, settings, docker, log\n\n\ndef initialize():\n\n    current_settings = settings.get_settings()\n\n    # chat model from user settings\n    chat_llm = ModelConfig(\n        provider=models.ModelProvider[current_settings[\"chat_model_provider\"]],\n        name=current_settings[\"chat_model_name\"],\n        ctx_length=current_settings[\"chat_model_ctx_length\"],\n        limit_requests=current_settings[\"chat_model_rl_requests\"],\n        limit_input=current_settings[\"chat_model_rl_input\"],\n        limit_output=current_settings[\"chat_model_rl_output\"],\n        kwargs={\n            \"temperature\": current_settings[\"chat_model_temperature\"],\n            **current_settings[\"chat_model_kwargs\"],\n        },\n    )\n\n    # utility model from user settings\n    utility_llm = ModelConfig(\n        provider=models.ModelProvider[current_settings[\"util_model_provider\"]],\n        name=current_settings[\"util_model_name\"],\n        ctx_length=current_settings[\"util_model_ctx_length\"],\n        limit_requests=current_settings[\"util_model_rl_requests\"],\n        limit_input=current_settings[\"util_model_rl_input\"],\n        limit_output=current_settings[\"util_model_rl_output\"],\n        kwargs={\n            \"temperature\": current_settings[\"util_model_temperature\"],\n            **current_settings[\"util_model_kwargs\"],\n        },\n    )\n    # embedding model from user settings\n    embedding_llm = ModelConfig(\n        provider=models.ModelProvider[current_settings[\"embed_model_provider\"]],\n        name=current_settings[\"embed_model_name\"],\n        ctx_length=0,\n        limit_requests=current_settings[\"embed_model_rl_requests\"],\n        limit_input=0,\n        limit_output=0,\n        kwargs={\n            **current_settings[\"embed_model_kwargs\"],\n        },\n    )\n    # agent configuration\n    config = AgentConfig(\n        chat_model=chat_llm,\n        utility_model=utility_llm,\n        embeddings_model=embedding_llm,\n        prompts_subdir=current_settings[\"agent_prompts_subdir\"],\n        memory_subdir=current_settings[\"agent_memory_subdir\"],\n        knowledge_subdirs=[\"default\", current_settings[\"agent_knowledge_subdir\"]],\n        code_exec_docker_enabled=False,\n        # code_exec_docker_name = \"A0-dev\",\n        # code_exec_docker_image = \"frdel/agent-zero-run:development\",\n        # code_exec_docker_ports = { \"22/tcp\": 55022, \"80/tcp\": 55080 }\n        # code_exec_docker_volumes = {\n        # files.get_base_dir(): {\"bind\": \"/a0\", \"mode\": \"rw\"},\n        # files.get_abs_path(\"work_dir\"): {\"bind\": \"/root\", \"mode\": \"rw\"},\n        # },\n        # code_exec_ssh_enabled = True,\n        # code_exec_ssh_addr = \"localhost\",\n        # code_exec_ssh_port = 55022,\n        # code_exec_ssh_user = \"root\",\n        # code_exec_ssh_pass = \"\",\n        # additional = {},\n    )\n\n    # update SSH and docker settings\n    set_runtime_config(config, current_settings)\n\n    # update config with runtime args\n    args_override(config)\n\n    # return config object\n    return config\n\n\ndef args_override(config):\n    # update config with runtime args\n    for key, value in runtime.args.items():\n        if hasattr(config, key):\n            # conversion based on type of config[key]\n            if isinstance(getattr(config, key), bool):\n                value = value.lower().strip() == \"true\"\n            elif isinstance(getattr(config, key), int):\n                value = int(value)\n            elif isinstance(getattr(config, key), float):\n                value = float(value)\n            elif isinstance(getattr(config, key), str):\n                value = str(value)\n            else:\n                raise Exception(\n                    f\"Unsupported argument type of '{key}': {type(getattr(config, key))}\"\n                )\n\n            setattr(config, key, value)\n\n\ndef set_runtime_config(config: AgentConfig, set: settings.Settings):\n    ssh_conf = settings.get_runtime_config(set)\n    for key, value in ssh_conf.items():\n        if hasattr(config, key):\n            setattr(config, key, value)\n\n    # if config.code_exec_docker_enabled:\n    #     config.code_exec_docker_ports[\"22/tcp\"] = ssh_conf[\"code_exec_ssh_port\"]\n    #     config.code_exec_docker_ports[\"80/tcp\"] = ssh_conf[\"code_exec_http_port\"]\n    #     config.code_exec_docker_name = f\"{config.code_exec_docker_name}-{ssh_conf['code_exec_ssh_port']}-{ssh_conf['code_exec_http_port']}\"\n\n    #     dman = docker.DockerContainerManager(\n    #         logger=log.Log(),\n    #         name=config.code_exec_docker_name,\n    #         image=config.code_exec_docker_image,\n    #         ports=config.code_exec_docker_ports,\n    #         volumes=config.code_exec_docker_volumes,\n    #     )\n    #     dman.start_container()\n\n    # config.code_exec_ssh_pass = asyncio.run(rfc_exchange.get_root_password())\n"
        },
        {
          "name": "instruments",
          "type": "tree",
          "content": null
        },
        {
          "name": "knowledge",
          "type": "tree",
          "content": null
        },
        {
          "name": "logs",
          "type": "tree",
          "content": null
        },
        {
          "name": "memory",
          "type": "tree",
          "content": null
        },
        {
          "name": "models.py",
          "type": "blob",
          "size": 10.5986328125,
          "content": "from enum import Enum\nimport os\nfrom typing import Any\nfrom langchain_openai import (\n    ChatOpenAI,\n    OpenAI,\n    OpenAIEmbeddings,\n    AzureChatOpenAI,\n    AzureOpenAIEmbeddings,\n    AzureOpenAI,\n)\nfrom langchain_community.llms.ollama import Ollama\nfrom langchain_ollama import ChatOllama\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_groq import ChatGroq\nfrom langchain_huggingface import (\n    HuggingFaceEmbeddings,\n    ChatHuggingFace,\n    HuggingFaceEndpoint,\n)\nfrom langchain_google_genai import (\n    GoogleGenerativeAI,\n    HarmBlockThreshold,\n    HarmCategory,\n    embeddings as google_embeddings,\n)\nfrom langchain_mistralai import ChatMistralAI\nfrom pydantic.v1.types import SecretStr\nfrom python.helpers import dotenv, runtime\nfrom python.helpers.dotenv import load_dotenv\nfrom python.helpers.rate_limiter import RateLimiter\n\n# environment variables\nload_dotenv()\n\n# Configuration\nDEFAULT_TEMPERATURE = 0.0\n\n\nclass ModelType(Enum):\n    CHAT = \"Chat\"\n    EMBEDDING = \"Embedding\"\n\n\nclass ModelProvider(Enum):\n    ANTHROPIC = \"Anthropic\"\n    HUGGINGFACE = \"HuggingFace\"\n    GOOGLE = \"Google\"\n    GROQ = \"Groq\"\n    LMSTUDIO = \"LM Studio\"\n    MISTRALAI = \"Mistral AI\"\n    OLLAMA = \"Ollama\"\n    OPENAI = \"OpenAI\"\n    OPENAI_AZURE = \"OpenAI Azure\"\n    OPENROUTER = \"OpenRouter\"\n    SAMBANOVA = \"Sambanova\"\n    OTHER = \"Other\"\n\n\nrate_limiters: dict[str, RateLimiter] = {}\n\n\n# Utility function to get API keys from environment variables\ndef get_api_key(service):\n    return (\n        dotenv.get_dotenv_value(f\"API_KEY_{service.upper()}\")\n        or dotenv.get_dotenv_value(f\"{service.upper()}_API_KEY\")\n        or \"None\"\n    )\n\n\ndef get_model(type: ModelType, provider: ModelProvider, name: str, **kwargs):\n    fnc_name = f\"get_{provider.name.lower()}_{type.name.lower()}\"  # function name of model getter\n    model = globals()[fnc_name](name, **kwargs)  # call function by name\n    return model\n\n\ndef get_rate_limiter(\n    provider: ModelProvider, name: str, requests: int, input: int, output: int\n) -> RateLimiter:\n    # get or create\n    key = f\"{provider.name}\\\\{name}\"\n    rate_limiters[key] = limiter = rate_limiters.get(key, RateLimiter(seconds=60))\n    # always update\n    limiter.limits[\"requests\"] = requests or 0\n    limiter.limits[\"input\"] = input or 0\n    limiter.limits[\"output\"] = output or 0\n    return limiter\n\n\ndef parse_chunk(chunk: Any):\n    if isinstance(chunk, str):\n        content = chunk\n    elif hasattr(chunk, \"content\"):\n        content = str(chunk.content)\n    else:\n        content = str(chunk)\n    return content\n\n\n# Ollama models\ndef get_ollama_base_url():\n    return (\n        dotenv.get_dotenv_value(\"OLLAMA_BASE_URL\")\n        or f\"http://{runtime.get_local_url()}:11434\"\n    )\n\n\ndef get_ollama_chat(\n    model_name: str,\n    temperature=DEFAULT_TEMPERATURE,\n    base_url=None,\n    num_ctx=8192,\n    **kwargs,\n):\n    if not base_url:\n        base_url = get_ollama_base_url()\n    return ChatOllama(\n        model=model_name,\n        temperature=temperature,\n        base_url=base_url,\n        num_ctx=num_ctx,\n        **kwargs,\n    )\n\n\ndef get_ollama_embedding(\n    model_name: str,\n    temperature=DEFAULT_TEMPERATURE,\n    base_url=None,\n    **kwargs,\n):\n    if not base_url:\n        base_url = get_ollama_base_url()\n    return OllamaEmbeddings(\n        model=model_name, temperature=temperature, base_url=base_url, **kwargs\n    )\n\n\n# HuggingFace models\ndef get_huggingface_chat(\n    model_name: str,\n    api_key=None,\n    temperature=DEFAULT_TEMPERATURE,\n    **kwargs,\n):\n    # different naming convention here\n    if not api_key:\n        api_key = get_api_key(\"huggingface\") or os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n\n    # Initialize the HuggingFaceEndpoint with the specified model and parameters\n    llm = HuggingFaceEndpoint(\n        repo_id=model_name,\n        task=\"text-generation\",\n        do_sample=True,\n        temperature=temperature,\n        **kwargs,\n    )\n\n    # Initialize the ChatHuggingFace with the configured llm\n    return ChatHuggingFace(llm=llm)\n\n\ndef get_huggingface_embedding(model_name: str, **kwargs):\n    return HuggingFaceEmbeddings(model_name=model_name, **kwargs)\n\n\n# LM Studio and other OpenAI compatible interfaces\ndef get_lmstudio_base_url():\n    return (\n        dotenv.get_dotenv_value(\"LM_STUDIO_BASE_URL\")\n        or f\"http://{runtime.get_local_url()}:1234/v1\"\n    )\n\n\ndef get_lmstudio_chat(\n    model_name: str,\n    temperature=DEFAULT_TEMPERATURE,\n    base_url=None,\n    **kwargs,\n):\n    if not base_url:\n        base_url = get_lmstudio_base_url()\n    return ChatOpenAI(model_name=model_name, base_url=base_url, temperature=temperature, api_key=\"none\", **kwargs)  # type: ignore\n\n\ndef get_lmstudio_embedding(\n    model_name: str,\n    base_url=None,\n    **kwargs,\n):\n    if not base_url:\n        base_url = get_lmstudio_base_url()\n    return OpenAIEmbeddings(model=model_name, api_key=\"none\", base_url=base_url, check_embedding_ctx_length=False, **kwargs)  # type: ignore\n\n\n# Anthropic models\ndef get_anthropic_chat(\n    model_name: str,\n    api_key=None,\n    temperature=DEFAULT_TEMPERATURE,\n    **kwargs,\n):\n    if not api_key:\n        api_key = get_api_key(\"anthropic\")\n    return ChatAnthropic(model_name=model_name, temperature=temperature, api_key=api_key, **kwargs)  # type: ignore\n\n\n# right now anthropic does not have embedding models, but that might change\ndef get_anthropic_embedding(\n    model_name: str,\n    api_key=None,\n    **kwargs,\n):\n    if not api_key:\n        api_key = get_api_key(\"anthropic\")\n    return OpenAIEmbeddings(model=model_name, api_key=api_key, **kwargs)  # type: ignore\n\n\n# OpenAI models\ndef get_openai_chat(\n    model_name: str,\n    api_key=None,\n    temperature=DEFAULT_TEMPERATURE,\n    **kwargs,\n):\n    if not api_key:\n        api_key = get_api_key(\"openai\")\n    return ChatOpenAI(model_name=model_name, temperature=temperature, api_key=api_key, **kwargs)  # type: ignore\n\n\ndef get_openai_embedding(model_name: str, api_key=None, **kwargs):\n    if not api_key:\n        api_key = get_api_key(\"openai\")\n    return OpenAIEmbeddings(model=model_name, api_key=api_key, **kwargs)  # type: ignore\n\n\ndef get_openai_azure_chat(\n    deployment_name: str,\n    api_key=None,\n    temperature=DEFAULT_TEMPERATURE,\n    azure_endpoint=None,\n    **kwargs,\n):\n    if not api_key:\n        api_key = get_api_key(\"openai_azure\")\n    if not azure_endpoint:\n        azure_endpoint = dotenv.get_dotenv_value(\"OPENAI_AZURE_ENDPOINT\")\n    return AzureChatOpenAI(deployment_name=deployment_name, temperature=temperature, api_key=api_key, azure_endpoint=azure_endpoint, **kwargs)  # type: ignore\n\n\ndef get_openai_azure_embedding(\n    deployment_name: str,\n    api_key=None,\n    azure_endpoint=None,\n    **kwargs,\n):\n    if not api_key:\n        api_key = get_api_key(\"openai_azure\")\n    if not azure_endpoint:\n        azure_endpoint = dotenv.get_dotenv_value(\"OPENAI_AZURE_ENDPOINT\")\n    return AzureOpenAIEmbeddings(deployment_name=deployment_name, api_key=api_key, azure_endpoint=azure_endpoint, **kwargs)  # type: ignore\n\n\n# Google models\ndef get_google_chat(\n    model_name: str,\n    api_key=None,\n    temperature=DEFAULT_TEMPERATURE,\n    **kwargs,\n):\n    if not api_key:\n        api_key = get_api_key(\"google\")\n    return GoogleGenerativeAI(model=model_name, temperature=temperature, google_api_key=api_key, safety_settings={HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE}, **kwargs)  # type: ignore\n\n\ndef get_google_embedding(\n    model_name: str,\n    api_key=None,\n    **kwargs,\n):\n    if not api_key:\n        api_key = get_api_key(\"google\")\n    return google_embeddings.GoogleGenerativeAIEmbeddings(model=model_name, api_key=api_key, **kwargs)  # type: ignore\n\n\n# Mistral models\ndef get_mistralai_chat(\n    model_name: str,\n    api_key=None,\n    temperature=DEFAULT_TEMPERATURE,\n    **kwargs,\n):\n    if not api_key:\n        api_key = get_api_key(\"mistral\")\n    return ChatMistralAI(model=model_name, temperature=temperature, api_key=api_key, **kwargs)  # type: ignore\n\n\n# Groq models\ndef get_groq_chat(\n    model_name: str,\n    api_key=None,\n    temperature=DEFAULT_TEMPERATURE,\n    **kwargs,\n):\n    if not api_key:\n        api_key = get_api_key(\"groq\")\n    return ChatGroq(model_name=model_name, temperature=temperature, api_key=api_key, **kwargs)  # type: ignore\n\n\n# OpenRouter models\ndef get_openrouter_chat(\n    model_name: str,\n    api_key=None,\n    temperature=DEFAULT_TEMPERATURE,\n    base_url=None,\n    **kwargs,\n):\n    if not api_key:\n        api_key = get_api_key(\"openrouter\")\n    if not base_url:\n        base_url = (\n            dotenv.get_dotenv_value(\"OPEN_ROUTER_BASE_URL\")\n            or \"https://openrouter.ai/api/v1\"\n        )\n    return ChatOpenAI(api_key=api_key, model=model_name, temperature=temperature, base_url=base_url, **kwargs)  # type: ignore\n\n\ndef get_openrouter_embedding(\n    model_name: str,\n    api_key=None,\n    base_url=None,\n    **kwargs,\n):\n    if not api_key:\n        api_key = get_api_key(\"openrouter\")\n    if not base_url:\n        base_url = (\n            dotenv.get_dotenv_value(\"OPEN_ROUTER_BASE_URL\")\n            or \"https://openrouter.ai/api/v1\"\n        )\n    return OpenAIEmbeddings(model=model_name, api_key=api_key, base_url=base_url, **kwargs)  # type: ignore\n\n\n# Sambanova models\ndef get_sambanova_chat(\n    model_name: str,\n    api_key=None,\n    temperature=DEFAULT_TEMPERATURE,\n    base_url=None,\n    max_tokens=1024,\n    **kwargs,\n):\n    if not api_key:\n        api_key = get_api_key(\"sambanova\")\n    if not base_url:\n        base_url = (\n            dotenv.get_dotenv_value(\"SAMBANOVA_BASE_URL\")\n            or \"https://fast-api.snova.ai/v1\"\n        )\n    return ChatOpenAI(api_key=api_key, model=model_name, temperature=temperature, base_url=base_url, max_tokens=max_tokens, **kwargs)  # type: ignore\n\n\n# right now sambanova does not have embedding models, but that might change\ndef get_sambanova_embedding(\n    model_name: str,\n    api_key=None,\n    base_url=None,\n    **kwargs,\n):\n    if not api_key:\n        api_key = get_api_key(\"sambanova\")\n    if not base_url:\n        base_url = (\n            dotenv.get_dotenv_value(\"SAMBANOVA_BASE_URL\")\n            or \"https://fast-api.snova.ai/v1\"\n        )\n    return OpenAIEmbeddings(model=model_name, api_key=api_key, base_url=base_url, **kwargs)  # type: ignore\n\n\n# Other OpenAI compatible models\ndef get_other_chat(\n    model_name: str,\n    api_key=None,\n    temperature=DEFAULT_TEMPERATURE,\n    base_url=None,\n    **kwargs,\n):\n    return ChatOpenAI(api_key=api_key, model=model_name, temperature=temperature, base_url=base_url, **kwargs)  # type: ignore\n\n\ndef get_other_embedding(model_name: str, api_key=None, base_url=None, **kwargs):\n    return OpenAIEmbeddings(model=model_name, api_key=api_key, base_url=base_url, **kwargs)  # type: ignore\n"
        },
        {
          "name": "preload.py",
          "type": "blob",
          "size": 0.5361328125,
          "content": "import asyncio\nfrom python.helpers import runtime, whisper, settings\nfrom python.helpers.print_style import PrintStyle\n\nPrintStyle().print(\"Running preload...\")\nruntime.initialize()\n\n\nasync def preload():\n    try:\n        set = settings.get_default_settings()\n\n        # async tasks to preload\n        tasks = [whisper.preload(set[\"stt_model_size\"])]\n\n        return asyncio.gather(*tasks, return_exceptions=True)\n    except Exception as e:\n        PrintStyle().print(f\"Error in preload: {e}\")\n\n\n# preload transcription model\nasyncio.run(preload())\n"
        },
        {
          "name": "prepare.py",
          "type": "blob",
          "size": 0.6025390625,
          "content": "from python.helpers import dotenv, runtime, settings\nimport string\nimport random\nfrom python.helpers.print_style import PrintStyle\n\n\nPrintStyle.standard(\"Preparing environment...\")\n\ntry:\n\n    runtime.initialize()\n\n    # generate random root password if not set (for SSH)\n    root_pass = dotenv.get_dotenv_value(dotenv.KEY_ROOT_PASSWORD)\n    if not root_pass:\n        root_pass = \"\".join(random.choices(string.ascii_letters + string.digits, k=32))\n        PrintStyle.standard(\"Changing root password...\")\n    settings.set_root_password(root_pass)\n\nexcept Exception as e:\n    PrintStyle.error(f\"Error in preload: {e}\")\n"
        },
        {
          "name": "prompts",
          "type": "tree",
          "content": null
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.619140625,
          "content": "ansio==0.0.1\nbeautifulsoup4==4.12.3\ndocker==7.1.0\nduckduckgo-search==6.1.12\nfaiss-cpu==1.8.0.post1\nflask[async]==3.0.3\nflask-basicauth==0.2.0\nGitPython==3.1.43\ninputimeout==1.0.4\nlangchain-anthropic==0.1.19\nlangchain-community==0.2.7\nlangchain-google-genai==1.0.7\nlangchain-groq==0.1.6\nlangchain-huggingface==0.0.3\nlangchain-mistralai==0.1.8\nlangchain-ollama==0.1.3\nlangchain-openai==0.1.15\nopenai-whisper==20240930\nlxml_html_clean==0.3.1\nmarkdown==3.7\nnewspaper3k==0.2.8\nparamiko==3.5.0\npypdf==4.3.1\npython-dotenv==1.0.1\nsentence-transformers==3.0.1\ntiktoken==0.8.0\nunstructured==0.15.13\nunstructured-client==0.25.9\nwebcolors==24.6.0"
        },
        {
          "name": "run_cli.py",
          "type": "blob",
          "size": 5.0390625,
          "content": "import asyncio\nimport sys\nimport threading, time, models, os\nfrom ansio import application_keypad, mouse_input, raw_input\nfrom ansio.input import InputEvent, get_input_event\nfrom agent import AgentContext, UserMessage\nfrom python.helpers.print_style import PrintStyle\nfrom python.helpers.files import read_file\nfrom python.helpers import files\nimport python.helpers.timed_input as timed_input\nfrom initialize import initialize\nfrom python.helpers.dotenv import load_dotenv\n\n\ncontext: AgentContext = None # type: ignore\ninput_lock = threading.Lock()\n\n\n# Main conversation loop\nasync def chat(context: AgentContext):\n    \n    # start the conversation loop  \n    while True:\n        # ask user for message\n        with input_lock:\n            timeout = context.agent0.get_data(\"timeout\") # how long the agent is willing to wait\n            if not timeout: # if agent wants to wait for user input forever\n                PrintStyle(background_color=\"#6C3483\", font_color=\"white\", bold=True, padding=True).print(f\"User message ('e' to leave):\")        \n                if sys.platform != \"win32\": import readline # this fixes arrow keys in terminal\n                user_input = input(\"> \")\n                PrintStyle(font_color=\"white\", padding=False, log_only=True).print(f\"> {user_input}\") \n                \n            else: # otherwise wait for user input with a timeout\n                PrintStyle(background_color=\"#6C3483\", font_color=\"white\", bold=True, padding=True).print(f\"User message ({timeout}s timeout, 'w' to wait, 'e' to leave):\")        \n                if sys.platform != \"win32\": import readline # this fixes arrow keys in terminal\n                # user_input = timed_input(\"> \", timeout=timeout)\n                user_input = timeout_input(\"> \", timeout=timeout)\n                                    \n                if not user_input:\n                    user_input = context.agent0.read_prompt(\"fw.msg_timeout.md\")\n                    PrintStyle(font_color=\"white\", padding=False).stream(f\"{user_input}\")        \n                else:\n                    user_input = user_input.strip()\n                    if user_input.lower()==\"w\": # the user needs more time\n                        user_input = input(\"> \").strip()\n                    PrintStyle(font_color=\"white\", padding=False, log_only=True).print(f\"> {user_input}\")        \n                    \n                    \n\n        # exit the conversation when the user types 'exit'\n        if user_input.lower() == 'e': break\n\n        # send message to agent0, \n        assistant_response = await context.communicate(UserMessage(user_input, [])).result()\n        \n        # print agent0 response\n        PrintStyle(font_color=\"white\",background_color=\"#1D8348\", bold=True, padding=True).print(f\"{context.agent0.agent_name}: reponse:\")        \n        PrintStyle(font_color=\"white\").print(f\"{assistant_response}\")        \n                        \n\n# User intervention during agent streaming\ndef intervention():\n    if context.streaming_agent and not context.paused:\n        context.paused = True # stop agent streaming\n        PrintStyle(background_color=\"#6C3483\", font_color=\"white\", bold=True, padding=True).print(f\"User intervention ('e' to leave, empty to continue):\")        \n\n        if sys.platform != \"win32\": import readline # this fixes arrow keys in terminal\n        user_input = input(\"> \").strip()\n        PrintStyle(font_color=\"white\", padding=False, log_only=True).print(f\"> {user_input}\")        \n        \n        if user_input.lower() == 'e': os._exit(0) # exit the conversation when the user types 'exit'\n        if user_input: context.streaming_agent.intervention = UserMessage(user_input, []) # set intervention message if non-empty\n        context.paused = False # continue agent streaming \n    \n\n# Capture keyboard input to trigger user intervention\ndef capture_keys():\n        global input_lock\n        intervent=False            \n        while True:\n            if intervent: intervention()\n            intervent = False\n            time.sleep(0.1)\n            \n            if context.streaming_agent:\n                # with raw_input, application_keypad, mouse_input:\n                with input_lock, raw_input, application_keypad:\n                    event: InputEvent | None = get_input_event(timeout=0.1)\n                    if event and (event.shortcut.isalpha() or event.shortcut.isspace()):\n                        intervent=True\n                        continue\n\n# User input with timeout\ndef timeout_input(prompt, timeout=10):\n    return timed_input.timeout_input(prompt=prompt, timeout=timeout)\n\ndef run():\n    global context\n    PrintStyle.standard(\"Initializing framework...\")\n\n    #load env vars\n    load_dotenv()\n\n    # initialize context\n    config = initialize()\n    context = AgentContext(config)\n\n    # Start the key capture thread for user intervention during agent streaming\n    threading.Thread(target=capture_keys, daemon=True).start()\n\n    #start the chat\n    asyncio.run(chat(context))\n\nif __name__ == \"__main__\":\n    PrintStyle.standard(\"\\n\\n!!! run_cli.py is now discontinued. run_ui.py serves as both UI and API endpoint !!!\\n\\n\")\n    run()"
        },
        {
          "name": "run_ui.py",
          "type": "blob",
          "size": 4.30859375,
          "content": "from functools import wraps\nimport os\nimport threading\nfrom flask import Flask, request, Response\nfrom flask_basicauth import BasicAuth\nfrom python.helpers import errors, files, git\nfrom python.helpers.files import get_abs_path\nfrom python.helpers import persist_chat, runtime, dotenv, process\nfrom python.helpers.cloudflare_tunnel import CloudflareTunnel\nfrom python.helpers.extract_tools import load_classes_from_folder\nfrom python.helpers.api import ApiHandler\nfrom python.helpers.print_style import PrintStyle\n\n\n# initialize the internal Flask server\napp = Flask(\"app\", static_folder=get_abs_path(\"./webui\"), static_url_path=\"/\")\napp.config[\"JSON_SORT_KEYS\"] = False  # Disable key sorting in jsonify\n\nlock = threading.Lock()\n\n# Set up basic authentication\nbasic_auth = BasicAuth(app)\n\n\n# require authentication for handlers\ndef requires_auth(f):\n    @wraps(f)\n    async def decorated(*args, **kwargs):\n        user = dotenv.get_dotenv_value(\"AUTH_LOGIN\")\n        password = dotenv.get_dotenv_value(\"AUTH_PASSWORD\")\n        if user and password:\n            auth = request.authorization\n            if not auth or not (auth.username == user and auth.password == password):\n                return Response(\n                    \"Could not verify your access level for that URL.\\n\"\n                    \"You have to login with proper credentials\",\n                    401,\n                    {\"WWW-Authenticate\": 'Basic realm=\"Login Required\"'},\n                )\n        return await f(*args, **kwargs)\n\n    return decorated\n\n\n# handle default address, load index\n@app.route(\"/\", methods=[\"GET\"])\n@requires_auth\nasync def serve_index():\n    gitinfo = git.get_git_info()\n    return files.read_file(\n        \"./webui/index.html\",\n        version_no=gitinfo[\"version\"],\n        version_time=gitinfo[\"commit_time\"],\n    )\n    \n\ndef run():\n    PrintStyle().print(\"Initializing framework...\")\n\n    # Suppress only request logs but keep the startup messages\n    from werkzeug.serving import WSGIRequestHandler\n    from werkzeug.serving import make_server\n\n    class NoRequestLoggingWSGIRequestHandler(WSGIRequestHandler):\n        def log_request(self, code=\"-\", size=\"-\"):\n            pass  # Override to suppress request logging\n\n    # Get configuration from environment\n    port = runtime.get_arg(\"port\") or int(dotenv.get_dotenv_value(\"WEB_UI_PORT\", 0)) or 5000\n    host = runtime.get_arg(\"host\") or dotenv.get_dotenv_value(\"WEB_UI_HOST\") or \"localhost\"\n    use_cloudflare = (runtime.get_arg(\"cloudflare_tunnel\")\n        or dotenv.get_dotenv_value(\"USE_CLOUDFLARE\", \"false\").lower()) == \"true\"\n    \n\n    tunnel = None\n\n    try:    \n        # Initialize and start Cloudflare tunnel if enabled\n        if use_cloudflare and port:\n            try:\n                tunnel = CloudflareTunnel(port)\n                tunnel.start()\n            except Exception as e:\n                PrintStyle().error(f\"Failed to start Cloudflare tunnel: {e}\")\n                PrintStyle().print(\"Continuing without tunnel...\")\n\n        # initialize contexts from persisted chats\n        persist_chat.load_tmp_chats()\n\n    except Exception as e:\n        PrintStyle().error(errors.format_error(e))\n\n    server = None\n\n    def register_api_handler(app, handler: type[ApiHandler]):\n        name = handler.__module__.split(\".\")[-1]\n        instance = handler(app, lock)\n        @requires_auth\n        async def handle_request():\n            return await instance.handle_request(request=request)\n        app.add_url_rule(\n            f\"/{name}\",\n            f\"/{name}\",\n            handle_request,\n            methods=[\"POST\", \"GET\"],\n        )\n        \n    # initialize and register API handlers\n    handlers = load_classes_from_folder(\"python/api\", \"*.py\", ApiHandler)\n    for handler in handlers:\n        register_api_handler(app, handler)\n        \n    try:\n        server = make_server(host=host, port=port, app=app, request_handler=NoRequestLoggingWSGIRequestHandler, threaded=True)\n        process.set_server(server)\n        server.log_startup()\n        server.serve_forever() \n        # Run Flask app\n        # app.run(\n        #     request_handler=NoRequestLoggingWSGIRequestHandler, port=port, host=host\n        # )\n    finally:\n        # Clean up tunnel if it was started\n        if tunnel:\n            tunnel.stop()\n\n\n# run the internal server\nif __name__ == \"__main__\":\n    runtime.initialize()\n    dotenv.load_dotenv()\n    run()\n"
        },
        {
          "name": "tmp",
          "type": "tree",
          "content": null
        },
        {
          "name": "webui",
          "type": "tree",
          "content": null
        },
        {
          "name": "work_dir",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}