{
  "metadata": {
    "timestamp": 1736560602352,
    "page": 227,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "adamian98/pulse",
      "stars": 7961,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0634765625,
          "content": ".DS_Store\n__pycache__/*\n.idea/*\nruns/*\ninput/*\ncache/*\nrealpics/*"
        },
        {
          "name": "PULSE.py",
          "type": "blob",
          "size": 6.68359375,
          "content": "from stylegan import G_synthesis,G_mapping\nfrom dataclasses import dataclass\nfrom SphericalOptimizer import SphericalOptimizer\nfrom pathlib import Path\nimport numpy as np\nimport time\nimport torch\nfrom loss import LossBuilder\nfrom functools import partial\nfrom drive import open_url\n\n\nclass PULSE(torch.nn.Module):\n    def __init__(self, cache_dir, verbose=True):\n        super(PULSE, self).__init__()\n\n        self.synthesis = G_synthesis().cuda()\n        self.verbose = verbose\n\n        cache_dir = Path(cache_dir)\n        cache_dir.mkdir(parents=True, exist_ok = True)\n        if self.verbose: print(\"Loading Synthesis Network\")\n        with open_url(\"https://drive.google.com/uc?id=1TCViX1YpQyRsklTVYEJwdbmK91vklCo8\", cache_dir=cache_dir, verbose=verbose) as f:\n            self.synthesis.load_state_dict(torch.load(f))\n\n        for param in self.synthesis.parameters():\n            param.requires_grad = False\n\n        self.lrelu = torch.nn.LeakyReLU(negative_slope=0.2)\n\n        if Path(\"gaussian_fit.pt\").exists():\n            self.gaussian_fit = torch.load(\"gaussian_fit.pt\")\n        else:\n            if self.verbose: print(\"\\tLoading Mapping Network\")\n            mapping = G_mapping().cuda()\n\n            with open_url(\"https://drive.google.com/uc?id=14R6iHGf5iuVx3DMNsACAl7eBr7Vdpd0k\", cache_dir=cache_dir, verbose=verbose) as f:\n                    mapping.load_state_dict(torch.load(f))\n\n            if self.verbose: print(\"\\tRunning Mapping Network\")\n            with torch.no_grad():\n                torch.manual_seed(0)\n                latent = torch.randn((1000000,512),dtype=torch.float32, device=\"cuda\")\n                latent_out = torch.nn.LeakyReLU(5)(mapping(latent))\n                self.gaussian_fit = {\"mean\": latent_out.mean(0), \"std\": latent_out.std(0)}\n                torch.save(self.gaussian_fit,\"gaussian_fit.pt\")\n                if self.verbose: print(\"\\tSaved \\\"gaussian_fit.pt\\\"\")\n\n    def forward(self, ref_im,\n                seed,\n                loss_str,\n                eps,\n                noise_type,\n                num_trainable_noise_layers,\n                tile_latent,\n                bad_noise_layers,\n                opt_name,\n                learning_rate,\n                steps,\n                lr_schedule,\n                save_intermediate,\n                **kwargs):\n\n        if seed:\n            torch.manual_seed(seed)\n            torch.cuda.manual_seed(seed)\n            torch.backends.cudnn.deterministic = True\n\n        batch_size = ref_im.shape[0]\n\n        # Generate latent tensor\n        if(tile_latent):\n            latent = torch.randn(\n                (batch_size, 1, 512), dtype=torch.float, requires_grad=True, device='cuda')\n        else:\n            latent = torch.randn(\n                (batch_size, 18, 512), dtype=torch.float, requires_grad=True, device='cuda')\n\n        # Generate list of noise tensors\n        noise = [] # stores all of the noise tensors\n        noise_vars = []  # stores the noise tensors that we want to optimize on\n\n        for i in range(18):\n            # dimension of the ith noise tensor\n            res = (batch_size, 1, 2**(i//2+2), 2**(i//2+2))\n\n            if(noise_type == 'zero' or i in [int(layer) for layer in bad_noise_layers.split('.')]):\n                new_noise = torch.zeros(res, dtype=torch.float, device='cuda')\n                new_noise.requires_grad = False\n            elif(noise_type == 'fixed'):\n                new_noise = torch.randn(res, dtype=torch.float, device='cuda')\n                new_noise.requires_grad = False\n            elif (noise_type == 'trainable'):\n                new_noise = torch.randn(res, dtype=torch.float, device='cuda')\n                if (i < num_trainable_noise_layers):\n                    new_noise.requires_grad = True\n                    noise_vars.append(new_noise)\n                else:\n                    new_noise.requires_grad = False\n            else:\n                raise Exception(\"unknown noise type\")\n\n            noise.append(new_noise)\n\n        var_list = [latent]+noise_vars\n\n        opt_dict = {\n            'sgd': torch.optim.SGD,\n            'adam': torch.optim.Adam,\n            'sgdm': partial(torch.optim.SGD, momentum=0.9),\n            'adamax': torch.optim.Adamax\n        }\n        opt_func = opt_dict[opt_name]\n        opt = SphericalOptimizer(opt_func, var_list, lr=learning_rate)\n\n        schedule_dict = {\n            'fixed': lambda x: 1,\n            'linear1cycle': lambda x: (9*(1-np.abs(x/steps-1/2)*2)+1)/10,\n            'linear1cycledrop': lambda x: (9*(1-np.abs(x/(0.9*steps)-1/2)*2)+1)/10 if x < 0.9*steps else 1/10 + (x-0.9*steps)/(0.1*steps)*(1/1000-1/10),\n        }\n        schedule_func = schedule_dict[lr_schedule]\n        scheduler = torch.optim.lr_scheduler.LambdaLR(opt.opt, schedule_func)\n        \n        loss_builder = LossBuilder(ref_im, loss_str, eps).cuda()\n\n        min_loss = np.inf\n        min_l2 = np.inf\n        best_summary = \"\"\n        start_t = time.time()\n        gen_im = None\n\n\n        if self.verbose: print(\"Optimizing\")\n        for j in range(steps):\n            opt.opt.zero_grad()\n\n            # Duplicate latent in case tile_latent = True\n            if (tile_latent):\n                latent_in = latent.expand(-1, 18, -1)\n            else:\n                latent_in = latent\n\n            # Apply learned linear mapping to match latent distribution to that of the mapping network\n            latent_in = self.lrelu(latent_in*self.gaussian_fit[\"std\"] + self.gaussian_fit[\"mean\"])\n\n            # Normalize image to [0,1] instead of [-1,1]\n            gen_im = (self.synthesis(latent_in, noise)+1)/2\n\n            # Calculate Losses\n            loss, loss_dict = loss_builder(latent_in, gen_im)\n            loss_dict['TOTAL'] = loss\n\n            # Save best summary for log\n            if(loss < min_loss):\n                min_loss = loss\n                best_summary = f'BEST ({j+1}) | '+' | '.join(\n                [f'{x}: {y:.4f}' for x, y in loss_dict.items()])\n                best_im = gen_im.clone()\n\n            loss_l2 = loss_dict['L2']\n\n            if(loss_l2 < min_l2):\n                min_l2 = loss_l2\n\n            # Save intermediate HR and LR images\n            if(save_intermediate):\n                yield (best_im.cpu().detach().clamp(0, 1),loss_builder.D(best_im).cpu().detach().clamp(0, 1))\n\n            loss.backward()\n            opt.step()\n            scheduler.step()\n\n        total_t = time.time()-start_t\n        current_info = f' | time: {total_t:.1f} | it/s: {(j+1)/total_t:.2f} | batchsize: {batch_size}'\n        if self.verbose: print(best_summary+current_info)\n        if(min_l2 <= eps):\n            yield (gen_im.clone().cpu().detach().clamp(0, 1),loss_builder.D(best_im).cpu().detach().clamp(0, 1))\n        else:\n            print(\"Could not find a face that downscales correctly within epsilon\")\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.8828125,
          "content": "# PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models\nCode accompanying CVPR'20 paper of the same title. Paper link: https://arxiv.org/pdf/2003.03808.pdf\n\n## NOTE\n\nWe have noticed a lot of concern that PULSE will be used to identify individuals whose faces have been blurred out. We want to emphasize that this is impossible - **PULSE makes imaginary faces of people who do not exist, which should not be confused for real people.** It will **not** help identify or reconstruct the original image.\n\nWe also want to address concerns of bias in PULSE. **We have now included a new section in the [paper](https://arxiv.org/pdf/2003.03808.pdf) and an accompanying model card directly addressing this bias.**\n\n---\n\n![Transformation Preview](./readme_resources/014.jpeg)\n![Transformation Preview](./readme_resources/034.jpeg)\n![Transformation Preview](./readme_resources/094.jpeg)\n\nTable of Contents\n=================\n- [PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models](#pulse-self-supervised-photo-upsampling-via-latent-space-exploration-of-generative-models)\n- [Table of Contents](#table-of-contents)\n  - [What does it do?](#what-does-it-do)\n  - [Usage](#usage)\n    - [Prereqs](#prereqs)\n    - [Data](#data)\n    - [Applying PULSE](#applying-pulse)\n## What does it do? \nGiven a low-resolution input image, PULSE searches the outputs of a generative model (here, [StyleGAN](https://github.com/NVlabs/stylegan)) for high-resolution images that are perceptually realistic and downscale correctly.\n\n![Transformation Preview](./readme_resources/transformation.gif)\n\n## Usage\n\nThe main file of interest for applying PULSE is `run.py`. A full list of arguments with descriptions can be found in that file; here we describe those relevant to getting started.\n\n### Prereqs\n\nYou will need to install cmake first (required for dlib, which is used for face alignment). Currently the code only works with CUDA installed (and therefore requires an appropriate GPU) and has been tested on Linux and Windows. For the full set of required Python packages, create a Conda environment from the provided YAML, e.g.\n\n```\nconda create -f pulse.yml \n```\nor (Anaconda on Windows):\n```\nconda env create -n pulse -f pulse.yml\nconda activate pulse\n```\n\nIn some environments (e.g. on Windows), you may have to edit the pulse.yml to remove the version specific hash on each dependency and remove any dependency that still throws an error after running ```conda env create...``` (such as readline)\n```\ndependencies\n  - blas=1.0=mkl\n  ...\n```\nto\n```\ndependencies\n  - blas=1.0\n ...\n```\n\nFinally, you will need an internet connection the first time you run the code as it will automatically download the relevant pretrained model from Google Drive (if it has already been downloaded, it will use the local copy). In the event that the public Google Drive is out of capacity, add the files to your own Google Drive instead; get the share URL and replace the ID in the https://drive.google.com/uc?=ID links in ```align_face.py``` and ```PULSE.py``` with the new file ids from the share URL given by your own Drive file.\n \n\n### Data\n\nBy default, input data for `run.py` should be placed in `./input/` (though this can be modified). However, this assumes faces have already been aligned and downscaled. If you have data that is not already in this form, place it in `realpics` and run `align_face.py` which will automatically do this for you. (Again, all directories can be changed by command line arguments if more convenient.) You will at this stage pic a downscaling factor. \n\nNote that if your data begins at a low resolution already, downscaling it further will retain very little information. In this case, you may wish to bicubically upsample (usually, to 1024x1024) and allow `align_face.py` to downscale for you.  \n\n### Applying PULSE\nOnce your data is appropriately formatted, all you need to do is\n```\npython run.py\n```\nEnjoy!\n"
        },
        {
          "name": "SphericalOptimizer.py",
          "type": "blob",
          "size": 0.8876953125,
          "content": "import math\nimport torch\nfrom torch.optim import Optimizer\n\n# Spherical Optimizer Class\n# Uses the first two dimensions as batch information\n# Optimizes over the surface of a sphere using the initial radius throughout\n#\n# Example Usage:\n# opt = SphericalOptimizer(torch.optim.SGD, [x], lr=0.01)\n\nclass SphericalOptimizer(Optimizer):\n    def __init__(self, optimizer, params, **kwargs):\n        self.opt = optimizer(params, **kwargs)\n        self.params = params\n        with torch.no_grad():\n            self.radii = {param: (param.pow(2).sum(tuple(range(2,param.ndim)),keepdim=True)+1e-9).sqrt() for param in params}\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = self.opt.step(closure)\n        for param in self.params:\n            param.data.div_((param.pow(2).sum(tuple(range(2,param.ndim)),keepdim=True)+1e-9).sqrt())\n            param.mul_(self.radii[param])\n\n        return loss"
        },
        {
          "name": "align_face.py",
          "type": "blob",
          "size": 1.7646484375,
          "content": "import numpy as np\nimport PIL\nimport PIL.Image\nimport sys\nimport os\nimport glob\nimport scipy\nimport scipy.ndimage\nimport dlib\nfrom drive import open_url\nfrom pathlib import Path\nimport argparse\nfrom bicubic import BicubicDownSample\nimport torchvision\nfrom shape_predictor import align_face\n\nparser = argparse.ArgumentParser(description='PULSE')\n\nparser.add_argument('-input_dir', type=str, default='realpics', help='directory with unprocessed images')\nparser.add_argument('-output_dir', type=str, default='input', help='output directory')\nparser.add_argument('-output_size', type=int, default=32, help='size to downscale the input images to, must be power of 2')\nparser.add_argument('-seed', type=int, help='manual seed to use')\nparser.add_argument('-cache_dir', type=str, default='cache', help='cache directory for model weights')\n\nargs = parser.parse_args()\n\ncache_dir = Path(args.cache_dir)\ncache_dir.mkdir(parents=True, exist_ok=True)\n\noutput_dir = Path(args.output_dir)\noutput_dir.mkdir(parents=True,exist_ok=True)\n\nprint(\"Downloading Shape Predictor\")\nf=open_url(\"https://drive.google.com/uc?id=1huhv8PYpNNKbGCLOaYUjOgR1pY5pmbJx\", cache_dir=cache_dir, return_path=True)\npredictor = dlib.shape_predictor(f)\n\nfor im in Path(args.input_dir).glob(\"*.*\"):\n    faces = align_face(str(im),predictor)\n\n    for i,face in enumerate(faces):\n        if(args.output_size):\n            factor = 1024//args.output_size\n            assert args.output_size*factor == 1024\n            D = BicubicDownSample(factor=factor)\n            face_tensor = torchvision.transforms.ToTensor()(face).unsqueeze(0).cuda()\n            face_tensor_lr = D(face_tensor)[0].cpu().detach().clamp(0, 1)\n            face = torchvision.transforms.ToPILImage()(face_tensor_lr)\n\n        face.save(Path(args.output_dir) / (im.stem+f\"_{i}.png\"))\n"
        },
        {
          "name": "bicubic.py",
          "type": "blob",
          "size": 2.904296875,
          "content": "import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass BicubicDownSample(nn.Module):\n    def bicubic_kernel(self, x, a=-0.50):\n        \"\"\"\n        This equation is exactly copied from the website below:\n        https://clouard.users.greyc.fr/Pantheon/experiments/rescaling/index-en.html#bicubic\n        \"\"\"\n        abs_x = torch.abs(x)\n        if abs_x <= 1.:\n            return (a + 2.) * torch.pow(abs_x, 3.) - (a + 3.) * torch.pow(abs_x, 2.) + 1\n        elif 1. < abs_x < 2.:\n            return a * torch.pow(abs_x, 3) - 5. * a * torch.pow(abs_x, 2.) + 8. * a * abs_x - 4. * a\n        else:\n            return 0.0\n\n    def __init__(self, factor=4, cuda=True, padding='reflect'):\n        super().__init__()\n        self.factor = factor\n        size = factor * 4\n        k = torch.tensor([self.bicubic_kernel((i - torch.floor(torch.tensor(size / 2)) + 0.5) / factor)\n                          for i in range(size)], dtype=torch.float32)\n        k = k / torch.sum(k)\n        # k = torch.einsum('i,j->ij', (k, k))\n        k1 = torch.reshape(k, shape=(1, 1, size, 1))\n        self.k1 = torch.cat([k1, k1, k1], dim=0)\n        k2 = torch.reshape(k, shape=(1, 1, 1, size))\n        self.k2 = torch.cat([k2, k2, k2], dim=0)\n        self.cuda = '.cuda' if cuda else ''\n        self.padding = padding\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, x, nhwc=False, clip_round=False, byte_output=False):\n        # x = torch.from_numpy(x).type('torch.FloatTensor')\n        filter_height = self.factor * 4\n        filter_width = self.factor * 4\n        stride = self.factor\n\n        pad_along_height = max(filter_height - stride, 0)\n        pad_along_width = max(filter_width - stride, 0)\n        filters1 = self.k1.type('torch{}.FloatTensor'.format(self.cuda))\n        filters2 = self.k2.type('torch{}.FloatTensor'.format(self.cuda))\n\n        # compute actual padding values for each side\n        pad_top = pad_along_height // 2\n        pad_bottom = pad_along_height - pad_top\n        pad_left = pad_along_width // 2\n        pad_right = pad_along_width - pad_left\n\n        # apply mirror padding\n        if nhwc:\n            x = torch.transpose(torch.transpose(\n                x, 2, 3), 1, 2)   # NHWC to NCHW\n\n        # downscaling performed by 1-d convolution\n        x = F.pad(x, (0, 0, pad_top, pad_bottom), self.padding)\n        x = F.conv2d(input=x, weight=filters1, stride=(stride, 1), groups=3)\n        if clip_round:\n            x = torch.clamp(torch.round(x), 0.0, 255.)\n\n        x = F.pad(x, (pad_left, pad_right, 0, 0), self.padding)\n        x = F.conv2d(input=x, weight=filters2, stride=(1, stride), groups=3)\n        if clip_round:\n            x = torch.clamp(torch.round(x), 0.0, 255.)\n\n        if nhwc:\n            x = torch.transpose(torch.transpose(x, 1, 3), 1, 2)\n        if byte_output:\n            return x.type('torch.ByteTensor'.format(self.cuda))\n        else:\n            return x\n"
        },
        {
          "name": "drive.py",
          "type": "blob",
          "size": 3.583984375,
          "content": "# URL helpers, see https://github.com/NVlabs/stylegan\n# ------------------------------------------------------------------------------------------\n\nimport requests\nimport html\nimport hashlib\nimport glob\nimport os\nimport io\nfrom typing import Any\nimport re\nimport uuid\n\ndef is_url(obj: Any) -> bool:\n    \"\"\"Determine whether the given object is a valid URL string.\"\"\"\n    if not isinstance(obj, str) or not \"://\" in obj:\n        return False\n    try:\n        res = requests.compat.urlparse(obj)\n        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n            return False\n        res = requests.compat.urlparse(requests.compat.urljoin(obj, \"/\"))\n        if not res.scheme or not res.netloc or not \".\" in res.netloc:\n            return False\n    except:\n        return False\n    return True\n\n\ndef open_url(url: str, cache_dir: str = None, num_attempts: int = 10, verbose: bool = True, return_path: bool = False) -> Any:\n    \"\"\"Download the given URL and return a binary-mode file object to access the data.\"\"\"\n    assert is_url(url)\n    assert num_attempts >= 1\n\n    # Lookup from cache.\n    url_md5 = hashlib.md5(url.encode(\"utf-8\")).hexdigest()\n    if cache_dir is not None:\n        cache_files = glob.glob(os.path.join(cache_dir, url_md5 + \"_*\"))\n        if len(cache_files) == 1:\n            if(return_path):\n                return cache_files[0]\n            else:\n                return open(cache_files[0], \"rb\")\n\n    # Download.\n    url_name = None\n    url_data = None\n    with requests.Session() as session:\n        if verbose:\n            print(\"Downloading %s ...\" % url, end=\"\", flush=True)\n        for attempts_left in reversed(range(num_attempts)):\n            try:\n                with session.get(url) as res:\n                    res.raise_for_status()\n                    if len(res.content) == 0:\n                        raise IOError(\"No data received\")\n\n                    if len(res.content) < 8192:\n                        content_str = res.content.decode(\"utf-8\")\n                        if \"download_warning\" in res.headers.get(\"Set-Cookie\", \"\"):\n                            links = [html.unescape(link) for link in content_str.split('\"') if \"export=download\" in link]\n                            if len(links) == 1:\n                                url = requests.compat.urljoin(url, links[0])\n                                raise IOError(\"Google Drive virus checker nag\")\n                        if \"Google Drive - Quota exceeded\" in content_str:\n                            raise IOError(\"Google Drive quota exceeded\")\n\n                    match = re.search(r'filename=\"([^\"]*)\"', res.headers.get(\"Content-Disposition\", \"\"))\n                    url_name = match[1] if match else url\n                    url_data = res.content\n                    if verbose:\n                        print(\" done\")\n                    break\n            except:\n                if not attempts_left:\n                    if verbose:\n                        print(\" failed\")\n                    raise\n                if verbose:\n                    print(\".\", end=\"\", flush=True)\n\n    # Save to cache.\n    if cache_dir is not None:\n        safe_name = re.sub(r\"[^0-9a-zA-Z-._]\", \"_\", url_name)\n        cache_file = os.path.join(cache_dir, url_md5 + \"_\" + safe_name)\n        temp_file = os.path.join(cache_dir, \"tmp_\" + uuid.uuid4().hex + \"_\" + url_md5 + \"_\" + safe_name)\n        os.makedirs(cache_dir, exist_ok=True)\n        with open(temp_file, \"wb\") as f:\n            f.write(url_data)\n        os.replace(temp_file, cache_file) # atomic\n        if(return_path): return cache_file\n\n    # Return data as file object.\n    return io.BytesIO(url_data)"
        },
        {
          "name": "gaussian_fit.pt",
          "type": "blob",
          "size": 4.458984375,
          "content": null
        },
        {
          "name": "loss.py",
          "type": "blob",
          "size": 2.1318359375,
          "content": "import torch\nfrom bicubic import BicubicDownSample\n\nclass LossBuilder(torch.nn.Module):\n    def __init__(self, ref_im, loss_str, eps):\n        super(LossBuilder, self).__init__()\n        assert ref_im.shape[2]==ref_im.shape[3]\n        im_size = ref_im.shape[2]\n        factor=1024//im_size\n        assert im_size*factor==1024\n        self.D = BicubicDownSample(factor=factor)\n        self.ref_im = ref_im\n        self.parsed_loss = [loss_term.split('*') for loss_term in loss_str.split('+')]\n        self.eps = eps\n\n    # Takes a list of tensors, flattens them, and concatenates them into a vector\n    # Used to calculate euclidian distance between lists of tensors\n    def flatcat(self, l):\n        l = l if(isinstance(l, list)) else [l]\n        return torch.cat([x.flatten() for x in l], dim=0)\n\n    def _loss_l2(self, gen_im_lr, ref_im, **kwargs):\n        return ((gen_im_lr - ref_im).pow(2).mean((1, 2, 3)).clamp(min=self.eps).sum())\n\n    def _loss_l1(self, gen_im_lr, ref_im, **kwargs):\n        return 10*((gen_im_lr - ref_im).abs().mean((1, 2, 3)).clamp(min=self.eps).sum())\n\n    # Uses geodesic distance on sphere to sum pairwise distances of the 18 vectors\n    def _loss_geocross(self, latent, **kwargs):\n        if(latent.shape[1] == 1):\n            return 0\n        else:\n            X = latent.view(-1, 1, 18, 512)\n            Y = latent.view(-1, 18, 1, 512)\n            A = ((X-Y).pow(2).sum(-1)+1e-9).sqrt()\n            B = ((X+Y).pow(2).sum(-1)+1e-9).sqrt()\n            D = 2*torch.atan2(A, B)\n            D = ((D.pow(2)*512).mean((1, 2))/8.).sum()\n            return D\n\n    def forward(self, latent, gen_im):\n        var_dict = {'latent': latent,\n                    'gen_im_lr': self.D(gen_im),\n                    'ref_im': self.ref_im,\n                    }\n        loss = 0\n        loss_fun_dict = {\n            'L2': self._loss_l2,\n            'L1': self._loss_l1,\n            'GEOCROSS': self._loss_geocross,\n        }\n        losses = {}\n        for weight, loss_type in self.parsed_loss:\n            tmp_loss = loss_fun_dict[loss_type](**var_dict)\n            losses[loss_type] = tmp_loss\n            loss += float(weight)*tmp_loss\n        return loss, losses\n"
        },
        {
          "name": "pulse.yml",
          "type": "blob",
          "size": 1.65625,
          "content": "name: pulse\nchannels:\n  - pytorch\n  - defaults\ndependencies:\n  - blas=1.0=mkl\n  - ca-certificates=2020.1.1=0\n  - certifi=2020.4.5.1=py38_0\n  - cffi=1.14.0=py38hc512035_1\n  - chardet=3.0.4=py38_1003\n  - cryptography=2.9.2=py38ha12b0ac_0\n  - cycler=0.10.0=py38_0\n  - freetype=2.9.1=hb4e5f40_0\n  - idna=2.9=py_1\n  - intel-openmp=2019.4=233\n  - jpeg=9b=he5867d9_2\n  - kiwisolver=1.2.0=py38h04f5b5a_0\n  - libcxx=10.0.0=1\n  - libedit=3.1.20181209=hb402a30_0\n  - libffi=3.3=h0a44026_1\n  - libgfortran=3.0.1=h93005f0_2\n  - libpng=1.6.37=ha441bb4_0\n  - libtiff=4.1.0=hcb84e12_0\n  - matplotlib=3.1.3=py38_0\n  - matplotlib-base=3.1.3=py38h9aa3819_0\n  - mkl=2019.4=233\n  - mkl-service=2.3.0=py38hfbe908c_0\n  - mkl_fft=1.0.15=py38h5e564d8_0\n  - mkl_random=1.1.0=py38h6440ff4_0\n  - ncurses=6.2=h0a44026_1\n  - ninja=1.9.0=py38h04f5b5a_0\n  - numpy=1.18.1=py38h7241aed_0\n  - numpy-base=1.18.1=py38h6575580_1\n  - olefile=0.46=py_0\n  - openssl=1.1.1g=h1de35cc_0\n  - pandas=1.0.3=py38h6c726b0_0\n  - pillow=7.1.2=py38h4655f20_0\n  - pip=20.0.2=py38_3\n  - pycparser=2.20=py_0\n  - pyopenssl=19.1.0=py38_0\n  - pyparsing=2.4.7=py_0\n  - pysocks=1.7.1=py38_0\n  - python=3.8.2=hf48f09d_13\n  - python-dateutil=2.8.1=py_0\n  - pytorch=1.5.0=py3.8_0\n  - pytz=2020.1=py_0\n  - readline=8.0=h1de35cc_0\n  - requests=2.23.0=py38_0\n  - scipy=1.4.1=py38h44e99c9_0\n  - setuptools=46.2.0=py38_0\n  - six=1.14.0=py38_0\n  - sqlite=3.31.1=h5c1f38d_1\n  - tk=8.6.8=ha441bb4_0\n  - torchvision=0.6.0=py38_cpu\n  - tornado=6.0.4=py38h1de35cc_1\n  - urllib3=1.25.8=py38_0\n  - wheel=0.34.2=py38_0\n  - xz=5.2.5=h1de35cc_0\n  - zlib=1.2.11=h1de35cc_3\n  - zstd=1.3.7=h5bba6e5_0\n  - pip:\n    - dlib==19.19.0\nprefix: /Users/sachit/opt/miniconda3/envs/pulse\n"
        },
        {
          "name": "readme_resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 4.16796875,
          "content": "from PULSE import PULSE\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import DataParallel\nfrom pathlib import Path\nfrom PIL import Image\nimport torchvision\nfrom math import log10, ceil\nimport argparse\n\nclass Images(Dataset):\n    def __init__(self, root_dir, duplicates):\n        self.root_path = Path(root_dir)\n        self.image_list = list(self.root_path.glob(\"*.png\"))\n        self.duplicates = duplicates # Number of times to duplicate the image in the dataset to produce multiple HR images\n\n    def __len__(self):\n        return self.duplicates*len(self.image_list)\n\n    def __getitem__(self, idx):\n        img_path = self.image_list[idx//self.duplicates]\n        image = torchvision.transforms.ToTensor()(Image.open(img_path))\n        if(self.duplicates == 1):\n            return image,img_path.stem\n        else:\n            return image,img_path.stem+f\"_{(idx % self.duplicates)+1}\"\n\nparser = argparse.ArgumentParser(description='PULSE')\n\n#I/O arguments\nparser.add_argument('-input_dir', type=str, default='input', help='input data directory')\nparser.add_argument('-output_dir', type=str, default='runs', help='output data directory')\nparser.add_argument('-cache_dir', type=str, default='cache', help='cache directory for model weights')\nparser.add_argument('-duplicates', type=int, default=1, help='How many HR images to produce for every image in the input directory')\nparser.add_argument('-batch_size', type=int, default=1, help='Batch size to use during optimization')\n\n#PULSE arguments\nparser.add_argument('-seed', type=int, help='manual seed to use')\nparser.add_argument('-loss_str', type=str, default=\"100*L2+0.05*GEOCROSS\", help='Loss function to use')\nparser.add_argument('-eps', type=float, default=2e-3, help='Target for downscaling loss (L2)')\nparser.add_argument('-noise_type', type=str, default='trainable', help='zero, fixed, or trainable')\nparser.add_argument('-num_trainable_noise_layers', type=int, default=5, help='Number of noise layers to optimize')\nparser.add_argument('-tile_latent', action='store_true', help='Whether to forcibly tile the same latent 18 times')\nparser.add_argument('-bad_noise_layers', type=str, default=\"17\", help='List of noise layers to zero out to improve image quality')\nparser.add_argument('-opt_name', type=str, default='adam', help='Optimizer to use in projected gradient descent')\nparser.add_argument('-learning_rate', type=float, default=0.4, help='Learning rate to use during optimization')\nparser.add_argument('-steps', type=int, default=100, help='Number of optimization steps')\nparser.add_argument('-lr_schedule', type=str, default='linear1cycledrop', help='fixed, linear1cycledrop, linear1cycle')\nparser.add_argument('-save_intermediate', action='store_true', help='Whether to store and save intermediate HR and LR images during optimization')\n\nkwargs = vars(parser.parse_args())\n\ndataset = Images(kwargs[\"input_dir\"], duplicates=kwargs[\"duplicates\"])\nout_path = Path(kwargs[\"output_dir\"])\nout_path.mkdir(parents=True, exist_ok=True)\n\ndataloader = DataLoader(dataset, batch_size=kwargs[\"batch_size\"])\n\nmodel = PULSE(cache_dir=kwargs[\"cache_dir\"])\nmodel = DataParallel(model)\n\ntoPIL = torchvision.transforms.ToPILImage()\n\nfor ref_im, ref_im_name in dataloader:\n    if(kwargs[\"save_intermediate\"]):\n        padding = ceil(log10(100))\n        for i in range(kwargs[\"batch_size\"]):\n            int_path_HR = Path(out_path / ref_im_name[i] / \"HR\")\n            int_path_LR = Path(out_path / ref_im_name[i] / \"LR\")\n            int_path_HR.mkdir(parents=True, exist_ok=True)\n            int_path_LR.mkdir(parents=True, exist_ok=True)\n        for j,(HR,LR) in enumerate(model(ref_im,**kwargs)):\n            for i in range(kwargs[\"batch_size\"]):\n                toPIL(HR[i].cpu().detach().clamp(0, 1)).save(\n                    int_path_HR / f\"{ref_im_name[i]}_{j:0{padding}}.png\")\n                toPIL(LR[i].cpu().detach().clamp(0, 1)).save(\n                    int_path_LR / f\"{ref_im_name[i]}_{j:0{padding}}.png\")\n    else:\n        #out_im = model(ref_im,**kwargs)\n        for j,(HR,LR) in enumerate(model(ref_im,**kwargs)):\n            for i in range(kwargs[\"batch_size\"]):\n                toPIL(HR[i].cpu().detach().clamp(0, 1)).save(\n                    out_path / f\"{ref_im_name[i]}.png\")\n"
        },
        {
          "name": "shape_predictor.py",
          "type": "blob",
          "size": 5.0400390625,
          "content": "import numpy as np\nimport PIL\nimport PIL.Image\nimport sys\nimport os\nimport glob\nimport scipy\nimport scipy.ndimage\nimport dlib\nfrom drive import open_url\nfrom pathlib import Path\nimport argparse\nfrom bicubic import BicubicDownSample\nimport torchvision\n\n\"\"\"\nbrief: face alignment with FFHQ method (https://github.com/NVlabs/ffhq-dataset)\nauthor: lzhbrian (https://lzhbrian.me)\ndate: 2020.1.5\nnote: code is heavily borrowed from\n    https://github.com/NVlabs/ffhq-dataset\n    http://dlib.net/face_landmark_detection.py.html\n\nrequirements:\n    apt install cmake\n    conda install Pillow numpy scipy\n    pip install dlib\n    # download face landmark model from:\n    # http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n\"\"\"\n\ndef get_landmark(filepath,predictor):\n    \"\"\"get landmark with dlib\n    :return: np.array shape=(68, 2)\n    \"\"\"\n    detector = dlib.get_frontal_face_detector()\n\n    img = dlib.load_rgb_image(filepath)\n    dets = detector(img, 1)\n    filepath = Path(filepath)\n    print(f\"{filepath.name}: Number of faces detected: {len(dets)}\")\n    shapes = [predictor(img, d) for k, d in enumerate(dets)]\n\n    lms = [np.array([[tt.x, tt.y] for tt in shape.parts()]) for shape in shapes]\n\n    return lms\n\n\ndef align_face(filepath,predictor):\n    \"\"\"\n    :param filepath: str\n    :return: list of PIL Images\n    \"\"\"\n\n    lms = get_landmark(filepath,predictor)\n    imgs = []\n    for lm in lms:\n        lm_chin = lm[0: 17]  # left-right\n        lm_eyebrow_left = lm[17: 22]  # left-right\n        lm_eyebrow_right = lm[22: 27]  # left-right\n        lm_nose = lm[27: 31]  # top-down\n        lm_nostrils = lm[31: 36]  # top-down\n        lm_eye_left = lm[36: 42]  # left-clockwise\n        lm_eye_right = lm[42: 48]  # left-clockwise\n        lm_mouth_outer = lm[48: 60]  # left-clockwise\n        lm_mouth_inner = lm[60: 68]  # left-clockwise\n\n        # Calculate auxiliary vectors.\n        eye_left = np.mean(lm_eye_left, axis=0)\n        eye_right = np.mean(lm_eye_right, axis=0)\n        eye_avg = (eye_left + eye_right) * 0.5\n        eye_to_eye = eye_right - eye_left\n        mouth_left = lm_mouth_outer[0]\n        mouth_right = lm_mouth_outer[6]\n        mouth_avg = (mouth_left + mouth_right) * 0.5\n        eye_to_mouth = mouth_avg - eye_avg\n\n        # Choose oriented crop rectangle.\n        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n        x /= np.hypot(*x)\n        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n        y = np.flipud(x) * [-1, 1]\n        c = eye_avg + eye_to_mouth * 0.1\n        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n        qsize = np.hypot(*x) * 2\n\n        # read image\n        img = PIL.Image.open(filepath)\n\n        output_size = 1024\n        transform_size = 4096\n        enable_padding = True\n\n        # Shrink.\n        shrink = int(np.floor(qsize / output_size * 0.5))\n        if shrink > 1:\n            rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n            img = img.resize(rsize, PIL.Image.ANTIALIAS)\n            quad /= shrink\n            qsize /= shrink\n\n        # Crop.\n        border = max(int(np.rint(qsize * 0.1)), 3)\n        crop = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n                int(np.ceil(max(quad[:, 1]))))\n        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]),\n                min(crop[3] + border, img.size[1]))\n        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n            img = img.crop(crop)\n            quad -= crop[0:2]\n\n        # Pad.\n        pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n               int(np.ceil(max(quad[:, 1]))))\n        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0),\n               max(pad[3] - img.size[1] + border, 0))\n        if enable_padding and max(pad) > border - 4:\n            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n            h, w, _ = img.shape\n            y, x, _ = np.ogrid[:h, :w, :1]\n            mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w - 1 - x) / pad[2]),\n                              1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h - 1 - y) / pad[3]))\n            blur = qsize * 0.02\n            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n            img += (np.median(img, axis=(0, 1)) - img) * np.clip(mask, 0.0, 1.0)\n            img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n            quad += pad[:2]\n\n        # Transform.\n        img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(),\n                            PIL.Image.BILINEAR)\n        if output_size < transform_size:\n            img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n\n        # Save aligned image.\n        imgs.append(img)\n    return imgs"
        },
        {
          "name": "stylegan.py",
          "type": "blob",
          "size": 16.4072265625,
          "content": "#Modified from https://github.com/lernapparat/lernapparat/\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom collections import OrderedDict\nimport pickle\n\nimport numpy as np\n\n\nclass MyLinear(nn.Module):\n    \"\"\"Linear layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n    \n    def __init__(self, input_size, output_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True):\n        super().__init__()\n        he_std = gain * input_size**(-0.5)  # He init\n        # Equalized learning rate and custom learning rate multiplier.\n        if use_wscale:\n            init_std = 1.0 / lrmul\n            self.w_mul = he_std * lrmul\n        else:\n            init_std = he_std / lrmul\n            self.w_mul = lrmul\n        self.weight = torch.nn.Parameter(\n            torch.randn(output_size, input_size) * init_std)\n        if bias:\n            self.bias = torch.nn.Parameter(torch.zeros(output_size))\n            self.b_mul = lrmul\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        bias = self.bias\n        if bias is not None:\n            bias = bias * self.b_mul\n        return F.linear(x, self.weight * self.w_mul, bias)\n\n\nclass MyConv2d(nn.Module):\n    \"\"\"Conv layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n\n    def __init__(self, input_channels, output_channels, kernel_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True,\n                 intermediate=None, upscale=False):\n        super().__init__()\n        if upscale:\n            self.upscale = Upscale2d()\n        else:\n            self.upscale = None\n        he_std = gain * (input_channels * kernel_size **\n                         2) ** (-0.5)  # He init\n        self.kernel_size = kernel_size\n        if use_wscale:\n            init_std = 1.0 / lrmul\n            self.w_mul = he_std * lrmul\n        else:\n            init_std = he_std / lrmul\n            self.w_mul = lrmul\n        self.weight = torch.nn.Parameter(torch.randn(\n            output_channels, input_channels, kernel_size, kernel_size) * init_std)\n        if bias:\n            self.bias = torch.nn.Parameter(torch.zeros(output_channels))\n            self.b_mul = lrmul\n        else:\n            self.bias = None\n        self.intermediate = intermediate\n\n    def forward(self, x):\n        bias = self.bias\n        if bias is not None:\n            bias = bias * self.b_mul\n\n        have_convolution = False\n        if self.upscale is not None and min(x.shape[2:]) * 2 >= 128:\n            # this is the fused upscale + conv from StyleGAN, sadly this seems incompatible with the non-fused way\n            # this really needs to be cleaned up and go into the conv...\n            w = self.weight * self.w_mul\n            w = w.permute(1, 0, 2, 3)\n            # probably applying a conv on w would be more efficient. also this quadruples the weight (average)?!\n            w = F.pad(w, (1, 1, 1, 1))\n            w = w[:, :, 1:, 1:] + w[:, :, :-1, 1:] + \\\n                w[:, :, 1:, :-1] + w[:, :, :-1, :-1]\n            x = F.conv_transpose2d(\n                x, w, stride=2, padding=int((w.size(-1)-1)//2))\n            have_convolution = True\n        elif self.upscale is not None:\n            x = self.upscale(x)\n\n        if not have_convolution and self.intermediate is None:\n            return F.conv2d(x, self.weight * self.w_mul, bias, padding=int(self.kernel_size//2))\n        elif not have_convolution:\n            x = F.conv2d(x, self.weight * self.w_mul, None,\n                         padding=int(self.kernel_size//2))\n\n        if self.intermediate is not None:\n            x = self.intermediate(x)\n        if bias is not None:\n            x = x + bias.view(1, -1, 1, 1)\n        return x\n\n\nclass NoiseLayer(nn.Module):\n    \"\"\"adds noise. noise is per pixel (constant over channels) with per-channel weight\"\"\"\n\n    def __init__(self, channels):\n        super().__init__()\n        self.weight = nn.Parameter(torch.zeros(channels))\n        self.noise = None\n\n    def forward(self, x, noise=None):\n        if noise is None and self.noise is None:\n            noise = torch.randn(x.size(0), 1, x.size(\n                2), x.size(3), device=x.device, dtype=x.dtype)\n        elif noise is None:\n            # here is a little trick: if you get all the noiselayers and set each\n            # modules .noise attribute, you can have pre-defined noise.\n            # Very useful for analysis\n            noise = self.noise\n        x = x + self.weight.view(1, -1, 1, 1) * noise\n        return x\n\n\nclass StyleMod(nn.Module):\n    def __init__(self, latent_size, channels, use_wscale):\n        super(StyleMod, self).__init__()\n        self.lin = MyLinear(latent_size,\n                            channels * 2,\n                            gain=1.0, use_wscale=use_wscale)\n\n    def forward(self, x, latent):\n        style = self.lin(latent)  # style => [batch_size, n_channels*2]\n        shape = [-1, 2, x.size(1)] + (x.dim() - 2) * [1]\n        style = style.view(shape)  # [batch_size, 2, n_channels, ...]\n        x = x * (style[:, 0] + 1.) + style[:, 1]\n        return x\n\n\nclass PixelNormLayer(nn.Module):\n    def __init__(self, epsilon=1e-8):\n        super().__init__()\n        self.epsilon = epsilon\n\n    def forward(self, x):\n        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)\n\n\nclass BlurLayer(nn.Module):\n    def __init__(self, kernel=[1, 2, 1], normalize=True, flip=False, stride=1):\n        super(BlurLayer, self).__init__()\n        kernel = [1, 2, 1]\n        kernel = torch.tensor(kernel, dtype=torch.float32)\n        kernel = kernel[:, None] * kernel[None, :]\n        kernel = kernel[None, None]\n        if normalize:\n            kernel = kernel / kernel.sum()\n        if flip:\n            kernel = kernel[:, :, ::-1, ::-1]\n        self.register_buffer('kernel', kernel)\n        self.stride = stride\n\n    def forward(self, x):\n        # expand kernel channels\n        kernel = self.kernel.expand(x.size(1), -1, -1, -1)\n        x = F.conv2d(\n            x,\n            kernel,\n            stride=self.stride,\n            padding=int((self.kernel.size(2)-1)/2),\n            groups=x.size(1)\n        )\n        return x\n\n\ndef upscale2d(x, factor=2, gain=1):\n    assert x.dim() == 4\n    if gain != 1:\n        x = x * gain\n    if factor != 1:\n        shape = x.shape\n        x = x.view(shape[0], shape[1], shape[2], 1, shape[3],\n                   1).expand(-1, -1, -1, factor, -1, factor)\n        x = x.contiguous().view(\n            shape[0], shape[1], factor * shape[2], factor * shape[3])\n    return x\n\n\nclass Upscale2d(nn.Module):\n    def __init__(self, factor=2, gain=1):\n        super().__init__()\n        assert isinstance(factor, int) and factor >= 1\n        self.gain = gain\n        self.factor = factor\n\n    def forward(self, x):\n        return upscale2d(x, factor=self.factor, gain=self.gain)\n\n\nclass G_mapping(nn.Sequential):\n    def __init__(self, nonlinearity='lrelu', use_wscale=True):\n        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n        layers = [\n            ('pixel_norm', PixelNormLayer()),\n            ('dense0', MyLinear(512, 512, gain=gain,\n                                lrmul=0.01, use_wscale=use_wscale)),\n            ('dense0_act', act),\n            ('dense1', MyLinear(512, 512, gain=gain,\n                                lrmul=0.01, use_wscale=use_wscale)),\n            ('dense1_act', act),\n            ('dense2', MyLinear(512, 512, gain=gain,\n                                lrmul=0.01, use_wscale=use_wscale)),\n            ('dense2_act', act),\n            ('dense3', MyLinear(512, 512, gain=gain,\n                                lrmul=0.01, use_wscale=use_wscale)),\n            ('dense3_act', act),\n            ('dense4', MyLinear(512, 512, gain=gain,\n                                lrmul=0.01, use_wscale=use_wscale)),\n            ('dense4_act', act),\n            ('dense5', MyLinear(512, 512, gain=gain,\n                                lrmul=0.01, use_wscale=use_wscale)),\n            ('dense5_act', act),\n            ('dense6', MyLinear(512, 512, gain=gain,\n                                lrmul=0.01, use_wscale=use_wscale)),\n            ('dense6_act', act),\n            ('dense7', MyLinear(512, 512, gain=gain,\n                                lrmul=0.01, use_wscale=use_wscale)),\n            ('dense7_act', act)\n        ]\n        super().__init__(OrderedDict(layers))\n\n    def forward(self, x):\n        x = super().forward(x)\n        return x\n\n\nclass Truncation(nn.Module):\n    def __init__(self, avg_latent, max_layer=8, threshold=0.7):\n        super().__init__()\n        self.max_layer = max_layer\n        self.threshold = threshold\n        self.register_buffer('avg_latent', avg_latent)\n\n    def forward(self, x):\n        assert x.dim() == 3\n        interp = torch.lerp(self.avg_latent, x, self.threshold)\n        do_trunc = (torch.arange(x.size(1)) < self.max_layer).view(1, -1, 1)\n        return torch.where(do_trunc, interp, x)\n\n\nclass LayerEpilogue(nn.Module):\n    \"\"\"Things to do at the end of each layer.\"\"\"\n\n    def __init__(self, channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n        super().__init__()\n        layers = []\n        if use_noise:\n            self.noise = NoiseLayer(channels)\n        else:\n            self.noise = None\n        layers.append(('activation', activation_layer))\n        if use_pixel_norm:\n            layers.append(('pixel_norm', PixelNormLayer()))\n        if use_instance_norm:\n            layers.append(('instance_norm', nn.InstanceNorm2d(channels)))\n\n        self.top_epi = nn.Sequential(OrderedDict(layers))\n        if use_styles:\n            self.style_mod = StyleMod(\n                dlatent_size, channels, use_wscale=use_wscale)\n        else:\n            self.style_mod = None\n\n    def forward(self, x, dlatents_in_slice=None, noise_in_slice=None):\n        if(self.noise is not None):\n            x = self.noise(x, noise=noise_in_slice)\n        x = self.top_epi(x)\n        if self.style_mod is not None:\n            x = self.style_mod(x, dlatents_in_slice)\n        else:\n            assert dlatents_in_slice is None\n        return x\n\n\nclass InputBlock(nn.Module):\n    def __init__(self, nf, dlatent_size, const_input_layer, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n        super().__init__()\n        self.const_input_layer = const_input_layer\n        self.nf = nf\n        if self.const_input_layer:\n            # called 'const' in tf\n            self.const = nn.Parameter(torch.ones(1, nf, 4, 4))\n            self.bias = nn.Parameter(torch.ones(nf))\n        else:\n            # tweak gain to match the official implementation of Progressing GAN\n            self.dense = MyLinear(dlatent_size, nf*16,\n                                  gain=gain/4, use_wscale=use_wscale)\n        self.epi1 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise,\n                                  use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n        self.conv = MyConv2d(nf, nf, 3, gain=gain, use_wscale=use_wscale)\n        self.epi2 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise,\n                                  use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n\n    def forward(self, dlatents_in_range, noise_in_range):\n        batch_size = dlatents_in_range.size(0)\n        if self.const_input_layer:\n            x = self.const.expand(batch_size, -1, -1, -1)\n            x = x + self.bias.view(1, -1, 1, 1)\n        else:\n            x = self.dense(dlatents_in_range[:, 0]).view(\n                batch_size, self.nf, 4, 4)\n        x = self.epi1(x, dlatents_in_range[:, 0], noise_in_range[0])\n        x = self.conv(x)\n        x = self.epi2(x, dlatents_in_range[:, 1], noise_in_range[1])\n        return x\n\n\nclass GSynthesisBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n        # 2**res x 2**res # res = 3..resolution_log2\n        super().__init__()\n        if blur_filter:\n            blur = BlurLayer(blur_filter)\n        else:\n            blur = None\n        self.conv0_up = MyConv2d(in_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale,\n                                 intermediate=blur, upscale=True)\n        self.epi1 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise,\n                                  use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n        self.conv1 = MyConv2d(out_channels, out_channels,\n                              kernel_size=3, gain=gain, use_wscale=use_wscale)\n        self.epi2 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise,\n                                  use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n\n    def forward(self, x, dlatents_in_range, noise_in_range):\n        x = self.conv0_up(x)\n        x = self.epi1(x, dlatents_in_range[:, 0], noise_in_range[0])\n        x = self.conv1(x)\n        x = self.epi2(x, dlatents_in_range[:, 1], noise_in_range[1])\n        return x\n\n\nclass G_synthesis(nn.Module):\n    def __init__(self,\n                 # Disentangled latent (W) dimensionality.\n                 dlatent_size=512,\n                 num_channels=3,            # Number of output color channels.\n                 resolution=1024,         # Output resolution.\n                 # Overall multiplier for the number of feature maps.\n                 fmap_base=8192,\n                 # log2 feature map reduction when doubling the resolution.\n                 fmap_decay=1.0,\n                 # Maximum number of feature maps in any layer.\n                 fmap_max=512,\n                 use_styles=True,         # Enable style inputs?\n                 const_input_layer=True,         # First layer is a learned constant?\n                 use_noise=True,         # Enable noise inputs?\n                 # True = randomize noise inputs every time (non-deterministic), False = read noise inputs from variables.\n                 randomize_noise=True,\n                 nonlinearity='lrelu',      # Activation function: 'relu', 'lrelu'\n                 use_wscale=True,         # Enable equalized learning rate?\n                 use_pixel_norm=False,        # Enable pixelwise feature vector normalization?\n                 use_instance_norm=True,         # Enable instance normalization?\n                 # Data type to use for activations and outputs.\n                 dtype=torch.float32,\n                 # Low-pass filter to apply when resampling activations. None = no filtering.\n                 blur_filter=[1, 2, 1],\n                 ):\n\n        super().__init__()\n\n        def nf(stage):\n            return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n        self.dlatent_size = dlatent_size\n        resolution_log2 = int(np.log2(resolution))\n        assert resolution == 2**resolution_log2 and resolution >= 4\n\n        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n        num_layers = resolution_log2 * 2 - 2\n        num_styles = num_layers if use_styles else 1\n        torgbs = []\n        blocks = []\n        for res in range(2, resolution_log2 + 1):\n            channels = nf(res-1)\n            name = '{s}x{s}'.format(s=2**res)\n            if res == 2:\n                blocks.append((name,\n                               InputBlock(channels, dlatent_size, const_input_layer, gain, use_wscale,\n                                          use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n\n            else:\n                blocks.append((name,\n                               GSynthesisBlock(last_channels, channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n            last_channels = channels\n        self.torgb = MyConv2d(channels, num_channels, 1,\n                              gain=1, use_wscale=use_wscale)\n        self.blocks = nn.ModuleDict(OrderedDict(blocks))\n\n    def forward(self, dlatents_in, noise_in):\n        # Input: Disentangled latents (W) [minibatch, num_layers, dlatent_size].\n        # lod_in = tf.cast(tf.get_variable('lod', initializer=np.float32(0), trainable=False), dtype)\n        batch_size = dlatents_in.size(0)\n        for i, m in enumerate(self.blocks.values()):\n            if i == 0:\n                x = m(dlatents_in[:, 2*i:2*i+2], noise_in[2*i:2*i+2])\n            else:\n                x = m(x, dlatents_in[:, 2*i:2*i+2], noise_in[2*i:2*i+2])\n        rgb = self.torgb(x)\n        return rgb\n"
        }
      ]
    }
  ]
}