{
  "metadata": {
    "timestamp": 1736561067325,
    "page": 855,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "aigc-apps/sd-webui-EasyPhoto",
      "stars": 5035,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.150390625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n# weight\n*.pth\n*.onnx\n*.safetensors\n*.ckpt\n*.bin\n*.pkl\n*.jpg\n*.png\n\nmodels/stable-diffusion-xl/version.txt\nmodels/pose_templates\nscripts/thirdparty\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 2.646484375,
          "content": "repos:\n  - repo: https://github.com/psf/black\n    rev: 22.3.0\n    hooks:\n      - id: black\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n        args: [\"--line-length=140\"]\n  - repo: https://github.com/PyCQA/flake8\n    rev: 3.9.2\n    hooks:\n      - id: flake8\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n        args: [\"--max-line-length=140\", \"--ignore=E303,E731,W191,W504,E402,E203,F541,W605,W503,E501,E712, F401\", \"--exclude=__init__.py\"]\n  - repo: https://github.com/myint/autoflake\n    rev: v1.4\n    hooks:\n      - id: autoflake\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n        args:\n          [\n            \"--recursive\",\n            \"--in-place\",\n            \"--remove-unused-variable\",\n            \"--ignore-init-module-imports\",\n            \"--exclude=__init__.py\"\n          ]\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: check-ast\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: check-byte-order-marker\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: check-case-conflict\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: check-docstring-first\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: check-executables-have-shebangs\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: check-json\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: check-yaml\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: debug-statements\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: detect-private-key\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: end-of-file-fixer\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: trailing-whitespace\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: mixed-line-ending\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n"
        },
        {
          "name": "COVENANT.md",
          "type": "blob",
          "size": 1.544921875,
          "content": "# EasyPhoto Developer Covenant\nDisclaimer: This covenant serves as a set of recommended guidelines.\n\n## Overview:\nEasyPhoto is an open-source software built on the SDWebUI plugin ecosystem, focusing on leveraging AIGC technology to create true-to-life, aesthetic, and beautiful AI portraits (\"true/like/beautiful\"). We are committed to expanding the application scope of this technology, lowering the entry barrier, and facilitating use for a wide audience.\n\n## Covenant Purpose:\nAlthough technology is inherently neutral in value, considering that EasyPhoto already has the capability to produce highly realistic images, particularly facial images, we strongly suggest that everyone involved in development and usage adhere to the following guidelines.\n\n## Behavioral Guidelines:\n- Comply with laws and regulations of relevant jurisdictions: It is prohibited to use this technology for any unlawful, criminal, or activities against public morals and decency.\n- Content Restrictions: It is prohibited to produce or disseminate any images that may involve political figures, pornography, violence, or other activities contrary to regional regulations.\n\n## Ongoing Updates:\nThis covenant will be updated periodically to adapt to technological and social advancements. We encourage community members to follow these guidelines in daily interactions and usage. Non-compliance will result in appropriate community management actions.\n\nThank you for your cooperation and support. Together, let's ensure that EasyPhoto remains a responsible and sustainably-developed open-source software.\n"
        },
        {
          "name": "COVENANT_zh-CN.md",
          "type": "blob",
          "size": 1.150390625,
          "content": "# EasyPhoto å¼€å‘è€…å…¬çº¦\n!å£°æ˜ï¼šæœ¬å…¬çº¦ä»…ä¸ºæ¨èæ€§å‡†åˆ™\n\n## æ¦‚è¿°ï¼š\nEasyPhoto æ˜¯ä¸€ä¸ªåŸºäºSDWebUIæ’ä»¶ç”Ÿæ€æ„å»ºçš„å¼€æºè½¯ä»¶ï¼Œä¸“æ³¨äºåˆ©ç”¨AIGCæŠ€æœ¯å®ç°çœŸ/åƒ/ç¾çš„AI-å†™çœŸã€‚æˆ‘ä»¬è‡´åŠ›äºæ‹“å±•è¯¥æŠ€æœ¯çš„åº”ç”¨èŒƒå›´ï¼Œé™ä½ä½¿ç”¨é—¨æ§›ï¼Œå¹¶ä¸ºå¹¿å¤§ç”¨æˆ·æä¾›ä¾¿åˆ©ã€‚\n\n## å…¬çº¦å®—æ—¨ï¼š\nå°½ç®¡æŠ€æœ¯æœ¬èº«å¹¶æ— ä»·å€¼å€¾å‘ï¼Œä½†è€ƒè™‘åˆ°EasyPhotoç›®å‰å·²å…·å¤‡ç”Ÿæˆé€¼çœŸå›¾åƒï¼ˆç‰¹åˆ«æ˜¯äººè„¸å›¾åƒï¼‰çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‰€æœ‰å‚ä¸å¼€å‘å’Œä½¿ç”¨çš„äººå‘˜éµå¾ªä»¥ä¸‹å‡†åˆ™ã€‚\n\n## è¡Œä¸ºå‡†åˆ™ï¼š\n- éµå¾ªç›¸å…³åœ°åŒºçš„æ³•å¾‹å’Œæ³•è§„ï¼šä¸å¾—åˆ©ç”¨æœ¬æŠ€æœ¯ä»äº‹ä»»ä½•è¿æ³•ã€çŠ¯ç½ªæˆ–æœ‰æ‚–äºç¤¾ä¼šå…¬åºè‰¯ä¿—çš„æ´»åŠ¨ã€‚\n- å†…å®¹é™åˆ¶ï¼šç¦æ­¢ç”Ÿæˆæˆ–ä¼ æ’­ä»»ä½•å¯èƒ½æ¶‰åŠæ”¿æ²»äººç‰©ã€è‰²æƒ…ã€æš´åŠ›æˆ–å…¶ä»–è¿åç›¸å…³åœ°åŒºè§„å®šçš„å›¾åƒã€‚\n\n## æŒç»­æ›´æ–°ï¼š\næœ¬å…¬çº¦å°†ä¸å®šæœŸè¿›è¡Œæ›´æ–°ä»¥é€‚åº”æŠ€æœ¯å’Œç¤¾ä¼šå‘å±•ã€‚æˆ‘ä»¬é¼“åŠ±ç¤¾ç¾¤æˆå‘˜åœ¨æ—¥å¸¸äº¤æµå’Œä½¿ç”¨ä¸­éµå¾ªè¿™äº›å‡†åˆ™ã€‚æœªéµå®ˆæœ¬å…¬çº¦çš„è¡Œä¸ºå°†åœ¨ç¤¾ç¾¤ç®¡ç†ä¸­å—åˆ°ç›¸åº”é™åˆ¶ã€‚\næ„Ÿè°¢æ‚¨çš„é…åˆä¸æ”¯æŒã€‚æˆ‘ä»¬å…±åŒåŠªåŠ›ï¼Œä»¥ç¡®ä¿EasyPhotoæˆä¸ºä¸€ä¸ªè´Ÿè´£ä»»å’Œå¯æŒç»­å‘å±•çš„å¼€æºè½¯ä»¶ã€‚\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.1005859375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      the copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by the Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributors that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, the Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assuming any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.2822265625,
          "content": "# ğŸ“· EasyPhoto | Your Smart AI Photo Generator.\nğŸ¦œ EasyPhoto is a Webui UI plugin for generating AI portraits that can be used to train digital doppelgangers relevant to you.\n\nğŸ¦œ ğŸ¦œ Welcome!\n\n[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-yellow)](https://huggingface.co/spaces/alibaba-pai/easyphoto)\n\nEnglish | [ç®€ä½“ä¸­æ–‡](./README_zh-CN.md)\n\n# Table of Contents\n- [Introduction](#introduction)\n- [TODO List](#todo-list)\n- [Quick Start](#quick-start)\n    - [1. Cloud usage: AliyunDSW/AutoDL/Docker](#1-cloud-usage-aliyundswautodldocker)\n    - [2. Local install: Check/Downloading/Installation](#2-local-install-environment-checkdownloadinginstallation)\n- [How to use](#how-to-use)\n    - [1. Model Training](#1-model-training)\n    - [2. Inference](#2-inference)\n- [API test](./api_test/README.md)\n- [Algorithm Detailed](#algorithm-detailed)\n    - [1. Architectural Overview](#1-architectural-overview)\n    - [2. Training Detailed](#2-training-detailed)\n    - [3. Inference Detailed](#3-inference-detailed)\n- [Reference](#reference)\n- [Related Project](#Related-Project)\n- [License](#license)\n- [ContactUS](#contactus)\n\n# Introduction\nEasyPhoto is a Webui UI plugin for generating AI portraits that can be used to train digital doppelgangers relevant to you. Training is recommended to be done with 5 to 20 portrait images, preferably half-body photos, and do not wear glasses (It doesn't matter if the characters in a few pictures wear glasses). After the training is done, we can generate it in the Inference section. We support using preset template images or uploading your own images for Inference.\n\nPlease read our Contributor Covenant [covenant](./COVENANT.md) | [ç®€ä½“ä¸­æ–‡](./COVENANT_zh-CN.md).\n\nIf you encounter any problems in the training, please refer to the [VQA](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki).\n\nWe now support quick pull-ups from different platforms, refer to [Quick Start](#quick-start).\n\nNow you can experience EasyPhoto demo quickly on ModelScope, [demo](https://modelscope.cn/studios/PAI/EasyPhoto/summary).\n\nWhat's New:\n- Support LCM-Lora based sampling acceleration, now you only need 12 step (vs 50 steps) for both Image & Video generation, and we provide Scene Lora training and inference in both text2Image and text2Video.[ğŸ”¥ ğŸ”¥ ğŸ”¥ ğŸ”¥ 2023.12.09]\n- Support Concepts-Sliders based attribute editing and Virtual TryOnï¼Œ please refer to [sliders-wiki](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki/Attribute-Edit) , [tryon-wiki](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki/TryOn) for more details. [ğŸ”¥ ğŸ”¥ ğŸ”¥ ğŸ”¥ 2023.12.08]\n- Thanks to [lanrui-ai](https://www.lanrui-ai.com/). It offers an SDWebUI image with built-in EasyPhoto, promising bi-weekly updates. Personally tested, it can pull up resources in 2 minutes and complete startup within 5 minutes. [ 2023.11.20 ]\n- We are already support Video Inference without more traning! Specific details can go [here](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki/Video)![ğŸ”¥ ğŸ”¥ ğŸ”¥ ğŸ”¥ 2023.11.10]\n- SDXL Training and Inference Support. Specific details can go [here](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki/SDXL)![ğŸ”¥ ğŸ”¥ ğŸ”¥ ğŸ”¥ 2023.11.10]\n- ComfyUI Support at [repo](https://github.com/THtianhao/ComfyUI-Portrait-Maker), thanks to [THtianhao](https://github.com/THtianhao) great work![ğŸ”¥ ğŸ”¥ ğŸ”¥ 2023.10.17]\n- EasyPhoto arxiv [arxiv](https://arxiv.org/abs/2310.04672)[ğŸ”¥ ğŸ”¥ ğŸ”¥ 2023.10.10]\n- Support SDXL to generate High resolution template, no more upload image need in this mode(SDXL), need 16GB GPU memory! Specific details can go [here](https://zhuanlan.zhihu.com/p/658940203)[ 2023.09.26 ]\n- We also support the [Diffusers Edition](https://github.com/aigc-apps/EasyPhoto/). [ 2023.09.25 ]\n- **Support fine-tuning the background and calculating the similarity score between the generated image and the user.** [ 2023.09.15 ]\n- **Support different base models for training and inference.** [ 2023.09.08 ]\n- **Support multi-people generation! Add cache option to optimize inference speed. Add log refreshing on UI.** [ 2023.09.06 ]\n- Create Code! Support for Windows and Linux Now. [ 2023.09.02 ]\n\nThese are our generated results:\n![results_1](images/results_1.jpg)\n\nVideo Part:\n|  Example |  1  |  2  |  3  |\n|  ---- | ---- | ---- | ---- |\n| - | <img src=\"http://pai-vision-data-hz.oss-accelerate.aliyuncs.com/easyphoto/data/video/text2video/51s3.gif\" width=\"400\"> | <img src=\"http://pai-vision-data-hz.oss-accelerate.aliyuncs.com/easyphoto/data/video/v2videos/ring_3644.gif\" width=\"400\"> | <img src=\"http://pai-vision-data-hz.oss-accelerate.aliyuncs.com/easyphoto/data/video/img2video_2imgs/29s3.gif\" width=\"400\"> |\n\nPhoto Part:\n![results_2](images/results_2.jpg)\n![results_3](images/results_3.jpg)\n\nOur UI interface is as follows:\n**train part:**\n![train_ui](images/train_ui.jpg)\n**inference part:**\n![infer_ui](images/infer_ui.jpg)\n\n# TODO List\n- Support chinese ui.\n- Support change in template's background.\n- Support high resolution.\n\n# Quick Start\n### 1. Cloud usage: AliyunDSW/AutoDL/lanrui-ai/Docker\n#### a. From AliyunDSW\nDSW has free GPU time, which can be applied once by a user and is valid for 3 months after applying.\n\nAliyun provide free GPU time in [Freetier](https://help.aliyun.com/document_detail/2567864.html), get it and use in Aliyun PAI-DSW to start EasyPhoto within 3min!\n\n[![DSW Notebook](images/dsw.png)](https://gallery.pai-ml.com/#/preview/deepLearning/cv/stable_diffusion_easyphoto)\n\n#### b. From AutoDL/lanrui-ai\n##### lanrui-ai\nThe official full-plugin version of lanrui-ai comes with EasyPhoto built-in. They promise bi-weekly testing and updates. Personally tested and found to be effective, it can be launched within 5 minutes. Thanks to their support and contributions to the community.\n\n##### AutoDL\nIf you are using Lanrui-ai/AutoDL, you can quickly pull up the Stable DIffusion webui using the mirror we provide.\n\nYou can select the desired mirror by filling in the following information in Community Mirrors, or using offical Image provide by lanrui-ai.\n```\naigc-apps/sd-webui-EasyPhoto/sd-webui-EasyPhoto\n```\n\n#### c. From docker\nIf you are using docker, please make sure that the graphics card driver and CUDA environment have been installed correctly in your machine.\n\nThen execute the following commands in this way:\n```\n# pull image\ndocker pull registry.cn-beijing.aliyuncs.com/mybigpai/sd-webui-easyphoto:0.0.3\n\n# enter image\ndocker run -it -p 7860:7860 --network host --gpus all registry.cn-beijing.aliyuncs.com/mybigpai/sd-webui-easyphoto:0.0.3\n\n# launch webui\npython3 launch.py --port 7860\n```\nThe docker updates may be slightly slower than the github repository of sd-webui-EasyPhoto, so you can go to extensions/sd-webui-EasyPhoto and do a git pull first.\n```\ncd extensions/sd-webui-EasyPhoto/\ngit pull\ncd /workspace\n```\n\n### 2. Local install: Environment Check/Downloading/Installation\n#### a. Environment Check\nWe have verified EasyPhoto execution on the following environment:\nIf you meet problem with WebUI auto killed by OOM, please refer to [ISSUE21](https://github.com/aigc-apps/sd-webui-EasyPhoto/issues/21), and setting some `num_threads` to `0` and report other fix to us, thanks.\n\nThe detailed of Windows 10:\n- OS: Windows10\n- python: py3.10\n- pytorch: torch2.0.1\n- tensorflow-cpu: 2.13.0\n- CUDA: 11.7\n- CUDNN: 8+\n- GPU: Nvidia-3060 12G\n\nThe detailed of Linux:\n- OS: Ubuntu 20.04, CentOS\n- python: py3.10 & py3.11\n- pytorch: torch2.0.1\n- tensorflow-cpu: 2.13.0\n- CUDA: 11.7\n- CUDNN: 8+\n- GPU: Nvidia-A10 24G & Nvidia-V100 16G & Nvidia-A100 40G\n\nWe need about 60GB available on disk (for saving weights and datasets process), please check!\n\n#### b.  Relevant Repositories & Weights Downloading\n##### i. Controlnet\nWe need to use Controlnet for inference. The related repo is [Mikubill/sd-webui-controlnet](https://github.com/Mikubill/sd-webui-controlnet). You need install this repo before using EasyPhoto.\n\nIn addition, we need at least three Controlnets for inference. So you need to set the **Multi ControlNet: Max models amount (requires restart)** in Setting.\n![controlnet_num](images/controlnet_num.jpg)\n\n##### ii. Other Dependencies.\nWe are mutually compatible with the existing stable-diffusion-webui environment, and the relevant repositories are installed when starting stable-diffusion-webui.\n\nThe weights we need will be downloaded automatically when you start training first time.\n\n#### c. Plug-in Installation\nWe now support installing EasyPhoto from git. The url of our Repository is `https://github.com/aigc-apps/sd-webui-EasyPhoto`.\n\nWe will support installing EasyPhoto from **Available** in the future.\n\n![install](images/install.jpg)\n\n\n# How to use\n### 1. Model Training\nThe EasyPhoto training interface is as follows:\n\n- On the left is the training image. Simply click `Upload Photos` to upload the image, and click `Clear Photos` to delete the uploaded image;\n- On the right are the training parameters, which cannot be adjusted for the first training.\n\nAfter clicking `Upload Photos`, we can start uploading images. **It is best to upload 5 to 20 images here, including different angles and lighting conditions**. It is best to have some images that do not include glasses. If they are all glasses, the generated results may easily generate glasses.\n![train_1](images/train_1.jpg)\n\nThen we click on `Start Training` below, and at this point, we need to fill in the `User ID` above, such as the user's name, to start training.\n![train_2](images/train_2.jpg)\n\nAfter the model starts training, the webui will automatically refresh the training log. If there is no refresh, click `Refresh Log` button.\n![train_3](images/train_3.jpg)\n\nIf you want to set parameters, the parsing of each parameter is as follows:\n\n|Parameter Name | Meaning|\n|--|--|\n|Resolution | The size of the image fed into the network during training, with a default value of `512`|\n|Validation & save steps | The number of steps between validating the image and saving intermediate weights, with a default value of `100`, representing verifying the image every `100` steps and saving the weights|\n|Max train steps | Maximum number of training steps, default value is `800`|\n|Max steps per photos | The maximum number of training sessions per image, default to `200`|\n|Train batch size | The batch size of the training, with a default value of `1`|\n|Gradient accumulation steps | Whether to perform gradient accumulation. The default value is `4`. Combined with the train batch size, each step is equivalent to feeding four images|\n|Dataloader num workers | The number of jobs loaded with data, which does not take effect under Windows because an error will be reported if set, but is set normally on Linux|\n|Learning rate | Train Lora's learning rate, default to `1e-4`|\n|Rank Lora | The feature length of the weight, default to `128`|\n|Network alpha | The regularization parameter for Lora training, usually half of the rank, defaults to `64`|\n\n### 2. Inference\n#### a. single people\n- Step 1: Click the refresh button to query the model corresponding to the trained user ID.\n- Step 2: Select the `user ID`.\n- Step 3: Select the template that needs to be generated.\n- Step 4: Click the Generate button to generate the results.\n\n![single_people](images/single_people.jpg)\n\n#### b. multi people\n- Step 1: Go to the settings page of EasyPhoto and set `num_of_faceid` as greater than `1`.\n- Step 2: Apply settings.\n- Step 3: Restart the ui interface of the webui.\n- Step 4: Return to EasyPhoto and upload the two person template.\n- Step 5: Select the user IDs of two people.\n- Step 6: Click the `Generate` button. Perform image generation.\n\n![single_people](images/multi_people_1.jpg)\n![single_people](images/multi_people_2.jpg)\n# Algorithm Detailed\n- Arxiv paper EasyPhoto [arxiv](https://arxiv.org/abs/2310.04672)\n- More detailed principles and details can be found [BLOG](https://blog.csdn.net/weixin_44791964/article/details/132922309)\n\n\n### 1. Architectural Overview\n\n![overview](images/overview.jpg)\n\nIn the field of AI portraits, we expect model-generated images to be realistic and resemble the user, and traditional approaches introduce unrealistic lighting (such as face fusion or roop). To address this unrealism, we introduce the image-to-image capability of the stable diffusion model. Generating a perfect personal portrait takes into account the desired generation scenario and the user's digital doppelgÃ¤nger. We use a pre-prepared template as the desired generation scene and an online trained face LoRA model as the user's digital doppelganger, which is a popular stable diffusion fine-tuning model. We use a small number of user images to train a stable digital doppelgÃ¤nger of the user, and generate a personal portrait image based on the face LoRA model and the expected generative scene during inference.\n\n### 2. Training Detailed\n\n![overview](images/train_detail1.jpg)\n\nFirst, we perform face detection on the input user image, and after determining the face location, we intercept the input image according to a certain ratio. Then, we use the saliency detection model and the skin beautification model to obtain a clean face training image, which basically consists of only faces. Then, we label each image with a fixed label. There is no need to use a labeler here, and the results are good. Finally, we fine-tune the stabilizing diffusion model to get the user's digital doppelganger.\n\nDuring training, we utilize the template image for verification in real time, and at the end of training, we calculate the face id gap between the verification image and the user's image to achieve Lora fusion, which ensures that our Lora is a perfect digital doppelganger of the user.\n\nIn addition, we will choose the image that is most similar to the user in the validation as the face_id image, which will be used in Inference.\n\n### 3. Inference Detailed\n#### a. First Diffusion:\nFirst, we will perform face detection on our incoming template image to determine the mask that needs to be inpainted for stable diffusion. then we will use the template image to perform face fusion with the optimal user image. After the face fusion is completed, we use the above mask to inpaint (fusion_image) with the face fused image. In addition, we will affix the optimal face_id image obtained during training to the template image by affine transformation (replaced_image). Then we will apply Controlnets on it, we use canny with color to extract features for fusion_image and openpose for replaced_image to ensure the similarity and stability of the images. Then we will use Stable Diffusion combined with the user's digital split for generation.\n\n#### b. Second Diffusion:\nAfter getting the result of First Diffusion, we will fuse the result with the optimal user image for face fusion, and then we will use Stable Diffusion again with the user's digital doppelganger for generation. The second generation will use higher resolution.\n\n# Special thanks\nSpecial thanks to DevelopmentZheng, qiuyanxin, rainlee, jhuang1207, bubbliiiing, wuziheng, yjjinjie, hkunzhe, yunkchen for their code contributions (in no particular order).\n\n# Reference\n- insightfaceï¼šhttps://github.com/deepinsight/insightface\n- cv_resnet50_faceï¼šhttps://www.modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface/summary\n- cv_u2net_salientï¼šhttps://www.modelscope.cn/models/damo/cv_u2net_salient-detection/summary\n- cv_unet_skin_retouching_torchï¼šhttps://www.modelscope.cn/models/damo/cv_unet_skin_retouching_torch/summary\n- cv_unet-image-face-fusionï¼šhttps://www.modelscope.cn/models/damo/cv_unet-image-face-fusion_damo/summary\n- kohyaï¼šhttps://github.com/bmaltais/kohya_ss\n- controlnet-webuiï¼šhttps://github.com/Mikubill/sd-webui-controlnet\n\n# Related Project\nWe've also listed some great open source projects as well as any extensions you might be interested in:\n- [ModelScope](https://github.com/modelscope/modelscope).\n- [FaceChain](https://github.com/modelscope/facechain).\n- [sd-webui-controlnet](https://github.com/Mikubill/sd-webui-controlnet).\n- [sd-webui-roop](https://github.com/s0md3v/sd-webui-roop).\n- [roop](https://github.com/s0md3v/roop).\n- [sd-webui-deforum](https://github.com/deforum-art/sd-webui-deforum).\n- [sd-webui-additional-networks](https://github.com/kohya-ss/sd-webui-additional-networks).\n- [a1111-sd-webui-tagcomplete](https://github.com/DominikDoom/a1111-sd-webui-tagcomplete).\n- [sd-webui-segment-anything](https://github.com/continue-revolution/sd-webui-segment-anything).\n- [sd-webui-tunnels](https://github.com/Bing-su/sd-webui-tunnels).\n- [sd-webui-mov2mov](https://github.com/Scholar01/sd-webui-mov2mov).\n\n# License\n\nThis project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).\n\n# ContactUS\n1. Use [Dingding](https://www.dingtalk.com/) to search group-2 `54095000124` or Scan to join\n2. Since the WeChat group is full, you need to scan the image on the right to add this student as a friend first, and then join the WeChat group.\n\n<figure>\n<img src=\"images/ding_erweima.jpg\" width=300/>\n<img src=\"images/wechat.jpg\" width=300/>\n</figure>\n\n\n# Contributors âœ¨\n\nThanks goes to these wonderful people :\n\n<table>\n  <tr>\n     <td>\n  <a href=\"https://github.com/aigc-apps/sd-webui-EasyPhoto/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=aigc-apps/sd-webui-EasyPhoto\" />\n  </a>\n    </td>\n  </tr>\n</table>\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind are welcome!\n\n<p align=\"right\"><a href=\"#top\">Back to top</a></p>\n"
        },
        {
          "name": "README_zh-CN.md",
          "type": "blob",
          "size": 14.373046875,
          "content": "# EasyPhoto | æ‚¨çš„æ™ºèƒ½ AI ç…§ç‰‡ç”Ÿæˆå™¨ã€‚\nğŸ¦œ EasyPhotoæ˜¯ä¸€æ¬¾Webui UIæ’ä»¶ï¼Œç”¨äºç”ŸæˆAIè‚–åƒç”»ï¼Œè¯¥ä»£ç å¯ç”¨äºè®­ç»ƒä¸æ‚¨ç›¸å…³çš„æ•°å­—åˆ†èº«ã€‚\n\nğŸ¦œ ğŸ¦œ Welcome!\n\n[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-yellow)](https://huggingface.co/spaces/alibaba-pai/easyphoto)\n\n[English](./README.md) | ç®€ä½“ä¸­æ–‡\n\n# ç›®å½•\n- [ç®€ä»‹](#ç®€ä»‹)\n- [TODO List](#todo-list)\n- [å¿«é€Ÿå¯åŠ¨](#å¿«é€Ÿå¯åŠ¨)\n    - [1. äº‘ä½¿ç”¨: AliyunDSW/AutoDL/Docker](#1-äº‘ä½¿ç”¨-aliyundswautodldocker)\n    - [2. æœ¬åœ°å®‰è£…: ç¯å¢ƒæ£€æŸ¥/ä¸‹è½½/å®‰è£…](#2-æœ¬åœ°å®‰è£…-ç¯å¢ƒæ£€æŸ¥ä¸‹è½½å®‰è£…)\n- [å¦‚ä½•ä½¿ç”¨](#å¦‚ä½•ä½¿ç”¨)\n    - [1. æ¨¡å‹è®­ç»ƒ](#1-æ¨¡å‹è®­ç»ƒ)\n    - [2. äººç‰©ç”Ÿæˆ](#2-äººç‰©ç”Ÿæˆ)\n- [APIæµ‹è¯•](./api_test/README.md)\n- [ç®—æ³•è¯¦ç»†ä¿¡æ¯](#ç®—æ³•è¯¦ç»†ä¿¡æ¯)\n    - [1. æ¶æ„æ¦‚è¿°](#1-æ¶æ„æ¦‚è¿°)\n    - [2. è®­ç»ƒç»†èŠ‚](#2-è®­ç»ƒç»†èŠ‚)\n    - [3. æ¨ç†ç»†èŠ‚](#3-æ¨ç†ç»†èŠ‚)\n- [å‚è€ƒæ–‡çŒ®](#å‚è€ƒæ–‡çŒ®)\n- [ç›¸å…³é¡¹ç›®](#ç›¸å…³é¡¹ç›®)\n- [è®¸å¯è¯](#è®¸å¯è¯)\n- [è”ç³»æˆ‘ä»¬](#è”ç³»æˆ‘ä»¬)\n\n# ç®€ä»‹\nEasyPhotoæ˜¯ä¸€æ¬¾Webui UIæ’ä»¶ï¼Œç”¨äºç”ŸæˆAIè‚–åƒç”»ï¼Œè¯¥ä»£ç å¯ç”¨äºè®­ç»ƒä¸æ‚¨ç›¸å…³çš„æ•°å­—åˆ†èº«ã€‚å»ºè®®ä½¿ç”¨ 5 åˆ° 20 å¼ è‚–åƒå›¾ç‰‡è¿›è¡Œè®­ç»ƒï¼Œæœ€å¥½æ˜¯åŠèº«ç…§ç‰‡ä¸”ä¸è¦ä½©æˆ´çœ¼é•œï¼ˆå°‘é‡å¯ä»¥æ¥å—ï¼‰ã€‚è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¨ç†éƒ¨åˆ†ç”Ÿæˆå›¾åƒã€‚æˆ‘ä»¬æ”¯æŒä½¿ç”¨é¢„è®¾æ¨¡æ¿å›¾ç‰‡ä¸ä¸Šä¼ è‡ªå·±çš„å›¾ç‰‡è¿›è¡Œæ¨ç†ã€‚\n\nè¯·é˜…è¯»æˆ‘ä»¬çš„å¼€å‘è€…å…¬çº¦ï¼Œå…±å»ºç¾å¥½ç¤¾åŒº [covenant](./COVENANT.md) | [ç®€ä½“ä¸­æ–‡](./COVENANT_zh-CN.md)\n\nå¦‚æœæ‚¨åœ¨è®­ç»ƒä¸­é‡åˆ°ä¸€äº›é—®é¢˜ï¼Œè¯·å‚è€ƒ [VQA](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki)ã€‚\n\næˆ‘ä»¬ç°åœ¨æ”¯æŒä»ä¸åŒå¹³å°å¿«é€Ÿå¯åŠ¨ï¼Œè¯·å‚é˜… [å¿«é€Ÿå¯åŠ¨](#å¿«é€Ÿå¯åŠ¨)ã€‚\n\næ–°ç‰¹æ€§ï¼š\n- æ”¯æŒåŸºäºLCM-Loraçš„é‡‡æ ·åŠ é€Ÿï¼Œç°åœ¨æ‚¨åªéœ€è¦è¿›è¡Œ12ä¸ªsteps(vs 50steps)æ¥ç”Ÿæˆå›¾åƒå’Œè§†é¢‘, å¹¶æ”¯æŒäº†åœºæ™¯åŒ–ï¼ˆé£æ ¼åŒ–ï¼‰ Loraçš„è®­ç»ƒå’Œå¤§é‡å†…ç½®çš„æ¨¡å‹ã€‚[ğŸ”¥ ğŸ”¥ ğŸ”¥ ğŸ”¥ 2023.12.09]\n- æ”¯æŒåŸºäºConcepts-Slidersçš„å±æ€§ç¼–è¾‘å’Œè™šæ‹Ÿè¯•ç©¿ï¼Œè¯·å‚è€ƒ[sliders-wiki](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki/Attribute-Edit) , [tryon-wiki](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki/TryOn)è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚[ğŸ”¥ ğŸ”¥ ğŸ”¥ ğŸ”¥ 2023.12.08]\n- æ„Ÿè°¢[æ½ç¿æ˜ŸèˆŸ](https://www.lanrui-ai.com/) æä¾›äº†å†…ç½®EasyPhotoçš„SDWebUIå®˜æ–¹é•œåƒï¼Œå¹¶æ‰¿è¯ºæ¯ä¸¤å‘¨æ›´æ–°ä¸€æ¬¡ã€‚äº²è‡ªæµ‹è¯•ï¼Œå¯ä»¥åœ¨2åˆ†é’Ÿå†…æ‹‰èµ·èµ„æºï¼Œå¹¶åœ¨5åˆ†é’Ÿå†…å®Œæˆå¯åŠ¨ã€‚[ğŸ”¥ ğŸ”¥ ğŸ”¥ ğŸ”¥ 2023.11.20]\n- ComfyUI æ”¯æŒ [repo](https://github.com/THtianhao/ComfyUI-Portrait-Maker), æ„Ÿè°¢[THtianhao](https://github.com/THtianhao)çš„ç²¾å½©å·¥ä½œ![ğŸ”¥ ğŸ”¥ ğŸ”¥ 2023.10.17]\n- EasyPhoto è®ºæ–‡åœ°å€ [arxiv](https://arxiv.org/abs/2310.04672)[ğŸ”¥ ğŸ”¥ ğŸ”¥ 2023.10.10]\n- æ”¯æŒä½¿ç”¨SDXLæ¨¡å‹å’Œä¸€å®šçš„é€‰é¡¹ç›´æ¥ç”Ÿæˆé«˜æ¸…å¤§å›¾ï¼Œä¸å†éœ€è¦ä¸Šä¼ æ¨¡æ¿ï¼Œéœ€è¦16GBæ˜¾å­˜ã€‚å…·ä½“ç»†èŠ‚å¯ä»¥å‰å¾€[è¿™é‡Œ](https://zhuanlan.zhihu.com/p/658940203)[ğŸ”¥ ğŸ”¥ ğŸ”¥ 2023.09.26]\n- æˆ‘ä»¬åŒæ ·æ”¯æŒ[Diffusersç‰ˆæœ¬](https://github.com/aigc-apps/EasyPhoto/)ã€‚ [ğŸ”¥ 2023.09.25]\n- **æ”¯æŒå¯¹èƒŒæ™¯è¿›è¡Œå¾®è°ƒï¼Œå¹¶è®¡ç®—ç”Ÿæˆçš„å›¾åƒä¸ç”¨æˆ·ä¹‹é—´çš„ç›¸ä¼¼åº¦å¾—åˆ†ã€‚** [ğŸ”¥ğŸ”¥ 2023.09.15]\n- **æ”¯æŒä¸åŒé¢„æµ‹åŸºç¡€æ¨¡å‹ã€‚** [ğŸ”¥ğŸ”¥ 2023.09.08]\n- **æ”¯æŒå¤šäººç”Ÿæˆï¼æ·»åŠ ç¼“å­˜é€‰é¡¹ä»¥ä¼˜åŒ–æ¨ç†é€Ÿåº¦ã€‚åœ¨UIä¸Šæ·»åŠ æ—¥å¿—åˆ·æ–°ã€‚** [ğŸ”¥ğŸ”¥ 2023.09.06]\n- åˆ›å»ºä»£ç ï¼ç°åœ¨æ”¯æŒ Windows å’Œ Linuxã€‚[ğŸ”¥ 2023.09.02]\n\nè¿™äº›æ˜¯æˆ‘ä»¬çš„ç”Ÿæˆç»“æœ:\n![results_1](images/results_1.jpg)\n![results_2](images/results_2.jpg)\n![results_3](images/results_3.jpg)\n\næˆ‘ä»¬çš„uiç•Œé¢å¦‚ä¸‹:\n**è®­ç»ƒéƒ¨åˆ†:**\n![train_ui](images/train_ui.jpg)\n**é¢„æµ‹éƒ¨åˆ†:**\n![infer_ui](images/infer_ui.jpg)\n\n# TODO List\n- æ”¯æŒä¸­æ–‡ç•Œé¢ã€‚\n- æ”¯æŒæ¨¡æ¿èƒŒæ™¯éƒ¨åˆ†å˜åŒ–ã€‚\n- æ”¯æŒé«˜åˆ†è¾¨ç‡ã€‚\n\n# å¿«é€Ÿå¯åŠ¨\n### 1. äº‘ä½¿ç”¨: AliyunDSW/AutoDL/æ½ç¿æ˜ŸèˆŸ/Docker\n#### a. é€šè¿‡é˜¿é‡Œäº‘ DSW\nDSW æœ‰å…è´¹ GPU æ—¶é—´ï¼Œç”¨æˆ·å¯ç”³è¯·ä¸€æ¬¡ï¼Œç”³è¯·å3ä¸ªæœˆå†…æœ‰æ•ˆã€‚\n\né˜¿é‡Œäº‘åœ¨[Freetier](https://free.aliyun.com/?product=9602825&crowd=enterprise&spm=5176.28055625.J_5831864660.1.e939154aRgha4e&scm=20140722.M_9974135.P_110.MO_1806-ID_9974135-MID_9974135-CID_30683-ST_8512-V_1)æä¾›å…è´¹GPUæ—¶é—´ï¼Œè·å–å¹¶åœ¨é˜¿é‡Œäº‘PAI-DSWä¸­ä½¿ç”¨ï¼Œ3åˆ†é’Ÿå†…å³å¯å¯åŠ¨EasyPhoto\n\n[![DSW Notebook](images/dsw.png)](https://gallery.pai-ml.com/#/preview/deepLearning/cv/stable_diffusion_easyphoto)\n\n#### b. é€šè¿‡æ½ç¿æ˜ŸèˆŸ/AutoDL\n##### æ½ç¿æ˜ŸèˆŸ\næ½ç¿æ˜ŸèˆŸå®˜æ–¹å…¨æ’ä»¶ç‰ˆæœ¬å†…ç½®EasyPhotoï¼Œå¹¶æ‰¿è¯ºæ¯ä¸¤å‘¨æµ‹è¯•ä¸æ›´æ–°ï¼Œäº²æµ‹å¯ç”¨ï¼Œ5åˆ†é’Ÿå†…æ‹‰èµ·ï¼Œæ„Ÿè°¢ä»–ä»¬çš„æ”¯æŒå’Œå¯¹ç¤¾åŒºåšå‡ºçš„è´¡çŒ®ã€‚\n\n##### AutoDL\nå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨ AutoDLï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›çš„é•œåƒå¿«é€Ÿå¯åŠ¨ Stable DIffusion webuiã€‚\n\næ‚¨å¯ä»¥åœ¨ç¤¾åŒºé•œåƒä¸­å¡«å†™ä»¥ä¸‹ä¿¡æ¯æ¥é€‰æ‹©æ‰€éœ€çš„é•œåƒã€‚\n```\naigc-apps/sd-webui-EasyPhoto/sd-webui-EasyPhoto\n```\n#### c. é€šè¿‡docker\nä½¿ç”¨dockerçš„æƒ…å†µä¸‹ï¼Œè¯·ä¿è¯æœºå™¨ä¸­å·²ç»æ­£ç¡®å®‰è£…æ˜¾å¡é©±åŠ¨ä¸CUDAç¯å¢ƒï¼Œç„¶åä»¥æ­¤æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š\n```\n# æ‹‰å–é•œåƒ\ndocker pull registry.cn-beijing.aliyuncs.com/mybigpai/sd-webui-easyphoto:0.0.3\n\n# è¿›å…¥é•œåƒ\ndocker run -it -p 7860:7860 --network host --gpus all registry.cn-beijing.aliyuncs.com/mybigpai/sd-webui-easyphoto:0.0.3\n\n# å¯åŠ¨webui\npython3 launch.py --port 7860\n```\n\n### 2. æœ¬åœ°å®‰è£…: ç¯å¢ƒæ£€æŸ¥/ä¸‹è½½/å®‰è£…\n#### a. ç¯å¢ƒæ£€æŸ¥\næˆ‘ä»¬å·²éªŒè¯EasyPhotoå¯åœ¨ä»¥ä¸‹ç¯å¢ƒä¸­æ‰§è¡Œï¼š\nå¦‚æœä½ é‡åˆ°å†…å­˜ä½¿ç”¨è¿‡é«˜è€Œå¯¼è‡´WebUIè¿›ç¨‹è‡ªåŠ¨è¢«killæ‰ï¼Œè¯·å‚è€ƒ[ISSUE21](https://github.com/aigc-apps/sd-webui-EasyPhoto/issues/21)ï¼Œè®¾ç½®ä¸€äº›å‚æ•°ï¼Œä¾‹å¦‚num_threads=0ï¼Œå¦‚æœä½ ä¹Ÿå‘ç°äº†å…¶ä»–è§£å†³çš„å¥½åŠæ³•ï¼Œè¯·åŠæ—¶è”ç³»æˆ‘ä»¬ã€‚\n\nWindows 10 çš„è¯¦ç»†ä¿¡æ¯ï¼š\n- æ“ä½œç³»ç»Ÿï¼š Windows10\n- python: python 3.10\n- pytorch: torch2.0.1\n- tensorflow-cpu: 2.13.0\n- CUDA: 11.7\n- CUDNN: 8+\n- GPUï¼š Nvidia-3060 12G\n\nLinux çš„è¯¦ç»†ä¿¡æ¯ï¼š\n- æ“ä½œç³»ç»Ÿ Ubuntu 20.04, CentOS\n- python: python3.10 & python3.11\n- pytorch: torch2.0.1\n- tensorflow-cpu: 2.13.0\n- CUDA: 11.7\n- CUDNN: 8+\n- GPUï¼š Nvidia-A10 24G & Nvidia-V100 16G & Nvidia-A100 40G\n\næˆ‘ä»¬éœ€è¦å¤§çº¦ 60GB çš„å¯ç”¨ç£ç›˜ç©ºé—´ï¼ˆç”¨äºä¿å­˜æƒé‡å’Œæ•°æ®é›†ï¼‰ï¼Œè¯·æ£€æŸ¥ï¼\n\n#### b. ç›¸å…³èµ„æ–™åº“å’Œæƒé‡ä¸‹è½½\n##### i. Controlnet\næˆ‘ä»¬éœ€è¦ä½¿ç”¨ Controlnet è¿›è¡Œæ¨ç†ã€‚ç›¸å…³è½¯ä»¶æºæ˜¯[Mikubill/sd-webui-controlnet](https://github.com/Mikubill/sd-webui-controlnet)ã€‚åœ¨ä½¿ç”¨ EasyPhoto ä¹‹å‰ï¼Œæ‚¨éœ€è¦å®‰è£…è¿™ä¸ªè½¯ä»¶æºã€‚\n\næ­¤å¤–ï¼Œæˆ‘ä»¬è‡³å°‘éœ€è¦ä¸‰ä¸ª Controlnets ç”¨äºæ¨ç†ã€‚å› æ­¤ï¼Œæ‚¨éœ€è¦è®¾ç½® **Multi ControlNet: Max models amount (requires restart)**ã€‚\n![controlnet_num](images/controlnet_num.jpg)\n\n##### ii. å…¶ä»–ä¾èµ–å…³ç³»ã€‚\næˆ‘ä»¬ä¸ç°æœ‰çš„ stable-diffusion-webui ç¯å¢ƒç›¸äº’å…¼å®¹ï¼Œå¯åŠ¨ stable-diffusion-webui æ—¶ä¼šå®‰è£…ç›¸å…³è½¯ä»¶æºã€‚\n\næˆ‘ä»¬æ‰€éœ€çš„æƒé‡ä¼šåœ¨ç¬¬ä¸€æ¬¡å¼€å§‹è®­ç»ƒæ—¶è‡ªåŠ¨ä¸‹è½½ã€‚\n\n#### c. æ’ä»¶å®‰è£…\nç°åœ¨æˆ‘ä»¬æ”¯æŒä» git å®‰è£… EasyPhotoã€‚æˆ‘ä»¬çš„ä»“åº“ç½‘å€æ˜¯ https://github.com/aigc-apps/sd-webui-EasyPhotoã€‚\n\nä»Šåï¼Œæˆ‘ä»¬å°†æ”¯æŒä» **Available** å®‰è£… EasyPhotoã€‚\n\n![install](images/install.jpg)\n\n# å¦‚ä½•ä½¿ç”¨\n### 1. æ¨¡å‹è®­ç»ƒ\nEasyPhotoè®­ç»ƒç•Œé¢å¦‚ä¸‹ï¼š\n- å·¦è¾¹æ˜¯è®­ç»ƒå›¾åƒã€‚åªéœ€ç‚¹å‡»ä¸Šä¼ ç…§ç‰‡å³å¯ä¸Šä¼ å›¾ç‰‡ï¼Œç‚¹å‡»æ¸…é™¤ç…§ç‰‡å³å¯åˆ é™¤ä¸Šä¼ çš„å›¾ç‰‡ï¼›\n- å³è¾¹æ˜¯è®­ç»ƒå‚æ•°ï¼Œä¸èƒ½ä¸ºç¬¬ä¸€æ¬¡è®­ç»ƒè¿›è¡Œè°ƒæ•´ã€‚\n\nç‚¹å‡»ä¸Šä¼ ç…§ç‰‡åï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ä¸Šä¼ å›¾åƒ**è¿™é‡Œæœ€å¥½ä¸Šä¼ 5åˆ°20å¼ å›¾åƒï¼ŒåŒ…æ‹¬ä¸åŒçš„è§’åº¦å’Œå…‰ç…§**ã€‚æœ€å¥½æœ‰ä¸€äº›ä¸åŒ…æ‹¬çœ¼é•œçš„å›¾åƒã€‚å¦‚æœæ‰€æœ‰å›¾ç‰‡éƒ½åŒ…å«çœ¼é•œçœ¼é•œï¼Œåˆ™ç”Ÿæˆçš„ç»“æœå¯ä»¥å®¹æ˜“åœ°ç”Ÿæˆçœ¼é•œã€‚\n![train_1](images/train_1.jpg)\n\nç„¶åæˆ‘ä»¬ç‚¹å‡»ä¸‹é¢çš„â€œå¼€å§‹åŸ¹è®­â€ï¼Œæ­¤æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å¡«å†™ä¸Šé¢çš„ç”¨æˆ·IDï¼Œä¾‹å¦‚ç”¨æˆ·åï¼Œæ‰èƒ½å¼€å§‹åŸ¹è®­ã€‚\n![train_2](images/train_2.jpg)\n\næ¨¡å‹å¼€å§‹è®­ç»ƒåï¼Œwebuiä¼šè‡ªåŠ¨åˆ·æ–°è®­ç»ƒæ—¥å¿—ã€‚å¦‚æœæ²¡æœ‰åˆ·æ–°ï¼Œè¯·å•å‡»â€œRefresh Logâ€æŒ‰é’®ã€‚\n![train_3](images/train_3.jpg)\n\nå¦‚æœè¦è®¾ç½®å‚æ•°ï¼Œæ¯ä¸ªå‚æ•°çš„è§£æå¦‚ä¸‹ï¼š\n| å‚æ•°å | å«ä¹‰ |\n|--|--|\n| resolution  | è®­ç»ƒæ—¶å–‚å…¥ç½‘ç»œçš„å›¾ç‰‡å¤§å°ï¼Œé»˜è®¤å€¼ä¸º512 |\n| validation & save steps| éªŒè¯å›¾ç‰‡ä¸ä¿å­˜ä¸­é—´æƒé‡çš„stepsæ•°ï¼Œé»˜è®¤å€¼ä¸º100ï¼Œä»£è¡¨æ¯100æ­¥éªŒè¯ä¸€æ¬¡å›¾ç‰‡å¹¶ä¿å­˜æƒé‡ |\n| max train steps | æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œé»˜è®¤å€¼ä¸º800 |\n| max steps per photos | æ¯å¼ å›¾ç‰‡çš„æœ€å¤§è®­ç»ƒæ¬¡æ•°ï¼Œé»˜è®¤ä¸º200 |\n| train batch size | è®­ç»ƒçš„æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤å€¼ä¸º1 |\n| gradient accumulationsteps | æ˜¯å¦è¿›è¡Œæ¢¯åº¦ç´¯è®¡ï¼Œé»˜è®¤å€¼ä¸º4ï¼Œç»“åˆtrain batch sizeæ¥çœ‹ï¼Œæ¯ä¸ªStepç›¸å½“äºå–‚å…¥å››å¼ å›¾ç‰‡ |\n| dataloader num workers | æ•°æ®åŠ è½½çš„worksæ•°é‡ï¼Œwindowsä¸‹ä¸ç”Ÿæ•ˆï¼Œå› ä¸ºè®¾ç½®äº†ä¼šæŠ¥é”™ï¼ŒLinuxæ­£å¸¸è®¾ç½® |\n| learning rate  | è®­ç»ƒLoraçš„å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º1e-4 |\n| rank Lora | æƒé‡çš„ç‰¹å¾é•¿åº¦ï¼Œé»˜è®¤ä¸º128 |\n| network alpha | Loraè®­ç»ƒçš„æ­£åˆ™åŒ–å‚æ•°ï¼Œä¸€èˆ¬ä¸ºrankçš„äºŒåˆ†ä¹‹ä¸€ï¼Œé»˜è®¤ä¸º64 |\n\n### 2. äººç‰©ç”Ÿæˆ\n#### a. å•äººæ¨¡ç‰ˆ\n- æ­¥éª¤1ï¼šç‚¹å‡»åˆ·æ–°æŒ‰é’®ï¼ŒæŸ¥è¯¢è®­ç»ƒåçš„ç”¨æˆ·IDå¯¹åº”çš„æ¨¡å‹ã€‚\n- æ­¥éª¤2ï¼šé€‰æ‹©ç”¨æˆ·IDã€‚\n- æ­¥éª¤3ï¼šé€‰æ‹©éœ€è¦ç”Ÿæˆçš„æ¨¡æ¿ã€‚\n- æ­¥éª¤4ï¼šå•å‡»â€œç”Ÿæˆâ€æŒ‰é’®ç”Ÿæˆç»“æœã€‚\n\n![single_people](images/single_people.jpg)\n\n#### b. å¤šäººæ¨¡æ¿\n- æ­¥éª¤1ï¼šè½¬åˆ°EasyPhotoçš„è®¾ç½®é¡µé¢ï¼Œè®¾ç½®num_of_Faceidå¤§äº1ã€‚\n- æ­¥éª¤2ï¼šåº”ç”¨è®¾ç½®ã€‚\n- æ­¥éª¤3ï¼šé‡æ–°å¯åŠ¨webuiçš„uiç•Œé¢ã€‚\n- æ­¥éª¤4ï¼šè¿”å›EasyPhotoå¹¶ä¸Šä¼ å¤šäººæ¨¡æ¿ã€‚\n- æ­¥éª¤5ï¼šé€‰æ‹©ä¸¤ä¸ªäººçš„ç”¨æˆ·IDã€‚\n- æ­¥éª¤6ï¼šå•å‡»â€œç”Ÿæˆâ€æŒ‰é’®ã€‚æ‰§è¡Œå›¾åƒç”Ÿæˆã€‚\n\n![single_people](images/multi_people_1.jpg)\n![single_people](images/multi_people_2.jpg)\n\n# ç®—æ³•è¯¦ç»†ä¿¡æ¯\n- è‹±æ–‡è®ºæ–‡[arxiv](https://arxiv.org/abs/2310.04672)\n- ä¸­æ–‡åšå®¢[è¿™é‡Œ](https://blog.csdn.net/weixin_44791964/article/details/132922309)\n\n### 1. æ¶æ„æ¦‚è¿°\n\n![overview](images/overview.jpg)\n\nåœ¨äººå·¥æ™ºèƒ½è‚–åƒé¢†åŸŸï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹ç”Ÿæˆçš„å›¾åƒé€¼çœŸä¸”ä¸ç”¨æˆ·ç›¸ä¼¼ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•ä¼šå¼•å…¥ä¸çœŸå®çš„å…‰ç…§ï¼ˆå¦‚äººè„¸èåˆæˆ–roopï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ç§ä¸çœŸå®çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¨³å®šæ‰©æ•£æ¨¡å‹çš„å›¾åƒåˆ°å›¾åƒåŠŸèƒ½ã€‚ç”Ÿæˆå®Œç¾çš„ä¸ªäººè‚–åƒéœ€è¦è€ƒè™‘æ‰€éœ€çš„ç”Ÿæˆåœºæ™¯å’Œç”¨æˆ·çš„æ•°å­—åˆ†èº«ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªé¢„å…ˆå‡†å¤‡å¥½çš„æ¨¡æ¿ä½œä¸ºæ‰€éœ€çš„ç”Ÿæˆåœºæ™¯ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªåœ¨çº¿è®­ç»ƒçš„äººè„¸ LoRA æ¨¡å‹ä½œä¸ºç”¨æˆ·çš„æ•°å­—åˆ†èº«ï¼Œè¿™æ˜¯ä¸€ç§æµè¡Œçš„ç¨³å®šæ‰©æ•£å¾®è°ƒæ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨å°‘é‡ç”¨æˆ·å›¾åƒæ¥è®­ç»ƒç”¨æˆ·çš„ç¨³å®šæ•°å­—åˆ†èº«ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ ¹æ®äººè„¸ LoRA æ¨¡å‹å’Œé¢„æœŸç”Ÿæˆåœºæ™¯ç”Ÿæˆä¸ªäººè‚–åƒå›¾åƒã€‚\n\n\n### 2. è®­ç»ƒç»†èŠ‚\n\n![overview](images/train_detail1.jpg)\n\né¦–å…ˆï¼Œæˆ‘ä»¬å¯¹è¾“å…¥çš„ç”¨æˆ·å›¾åƒè¿›è¡Œäººè„¸æ£€æµ‹ï¼Œç¡®å®šäººè„¸ä½ç½®åï¼ŒæŒ‰ç…§ä¸€å®šæ¯”ä¾‹æˆªå–è¾“å…¥å›¾åƒã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ˜¾è‘—æ€§æ£€æµ‹æ¨¡å‹å’Œçš®è‚¤ç¾åŒ–æ¨¡å‹è·å¾—å¹²å‡€çš„äººè„¸è®­ç»ƒå›¾åƒï¼Œè¯¥å›¾åƒåŸºæœ¬ä¸ŠåªåŒ…å«äººè„¸ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¸ºæ¯å¼ å›¾åƒè´´ä¸Šä¸€ä¸ªå›ºå®šæ ‡ç­¾ã€‚è¿™é‡Œä¸éœ€è¦ä½¿ç”¨æ ‡ç­¾å™¨ï¼Œè€Œä¸”æ•ˆæœå¾ˆå¥½ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹ç¨³å®šæ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°ç”¨æˆ·çš„æ•°å­—åˆ†èº«ã€‚\n\nåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼šåˆ©ç”¨æ¨¡æ¿å›¾åƒè¿›è¡Œå®æ—¶éªŒè¯ï¼Œåœ¨è®­ç»ƒç»“æŸåï¼Œæˆ‘ä»¬ä¼šè®¡ç®—éªŒè¯å›¾åƒä¸ç”¨æˆ·å›¾åƒä¹‹é—´çš„äººè„¸ ID å·®è·ï¼Œä»è€Œå®ç° Lora èåˆï¼Œç¡®ä¿æˆ‘ä»¬çš„ Lora æ˜¯ç”¨æˆ·çš„å®Œç¾æ•°å­—åˆ†\nèº«ã€‚\n\næ­¤å¤–ï¼Œæˆ‘ä»¬å°†é€‰æ‹©éªŒè¯ä¸­ä¸ç”¨æˆ·æœ€ç›¸ä¼¼çš„å›¾åƒä½œä¸º face_id å›¾åƒï¼Œç”¨äºæ¨ç†ã€‚\n\n### 3. æ¨ç†ç»†èŠ‚\n#### a. ç¬¬ä¸€æ¬¡æ‰©æ•£ï¼š\né¦–å…ˆï¼Œæˆ‘ä»¬å°†å¯¹æ¥æ”¶åˆ°çš„æ¨¡æ¿å›¾åƒè¿›è¡Œäººè„¸æ£€æµ‹ï¼Œä»¥ç¡®å®šä¸ºå®ç°ç¨³å®šæ‰©æ•£è€Œéœ€è¦æ¶‚æŠ¹çš„é®ç½©ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¨¡æ¿å›¾åƒä¸æœ€ä½³ç”¨æˆ·å›¾åƒè¿›è¡Œäººè„¸èåˆã€‚äººè„¸èåˆå®Œæˆåï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸Šè¿°é®ç½©å¯¹èåˆåçš„äººè„¸å›¾åƒè¿›è¡Œå†…ç»˜ï¼ˆfusion_imageï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†é€šè¿‡ä»¿å°„å˜æ¢ï¼ˆreplace_imageï¼‰æŠŠè®­ç»ƒä¸­è·å¾—çš„æœ€ä½³ face_id å›¾åƒè´´åˆ°æ¨¡æ¿å›¾åƒä¸Šã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å¯¹å…¶åº”ç”¨ Controlnetsï¼Œåœ¨èåˆå›¾åƒä¸­ä½¿ç”¨å¸¦æœ‰é¢œè‰²çš„ canny æå–ç‰¹å¾ï¼Œåœ¨æ›¿æ¢å›¾åƒä¸­ä½¿ç”¨ openpose æå–ç‰¹å¾ï¼Œä»¥ç¡®ä¿å›¾åƒçš„ç›¸ä¼¼æ€§å’Œç¨³å®šæ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç¨³å®šæ‰©æ•£ï¼ˆStable Diffusionï¼‰ç»“åˆç”¨æˆ·çš„æ•°å­—åˆ†å‰²è¿›è¡Œç”Ÿæˆã€‚\n\n#### b. ç¬¬äºŒæ¬¡æ‰©æ•£ï¼š\nåœ¨å¾—åˆ°ç¬¬ä¸€æ¬¡æ‰©æ•£çš„ç»“æœåï¼Œæˆ‘ä»¬å°†æŠŠè¯¥ç»“æœä¸æœ€ä½³ç”¨æˆ·å›¾åƒè¿›è¡Œäººè„¸èåˆï¼Œç„¶åå†æ¬¡ä½¿ç”¨ç¨³å®šæ‰©æ•£ä¸ç”¨æˆ·çš„æ•°å­—åˆ†èº«è¿›è¡Œç”Ÿæˆã€‚ç¬¬äºŒæ¬¡ç”Ÿæˆå°†ä½¿ç”¨æ›´é«˜çš„åˆ†è¾¨ç‡ã€‚\n\n# ç‰¹åˆ«æ„Ÿè°¢\nç‰¹åˆ«æ„Ÿè°¢DevelopmentZheng, qiuyanxin, rainlee, jhuang1207, bubbliiiing, wuziheng, yjjinjie, hkunzhe, yunkchenåŒå­¦ä»¬çš„ä»£ç è´¡çŒ®ï¼ˆæ­¤æ’åä¸åˆ†å…ˆåï¼‰ã€‚\n\n# å‚è€ƒæ–‡çŒ®\n- insightfaceï¼šhttps://github.com/deepinsight/insightface\n- cv_resnet50_faceï¼šhttps://www.modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface/summary\n- cv_u2net_salientï¼šhttps://www.modelscope.cn/models/damo/cv_u2net_salient-detection/summary\n- cv_unet_skin_retouching_torchï¼šhttps://www.modelscope.cn/models/damo/cv_unet_skin_retouching_torch/summary\n- cv_unet-image-face-fusionï¼šhttps://www.modelscope.cn/models/damo/cv_unet-image-face-fusion_damo/summary\n- kohyaï¼šhttps://github.com/bmaltais/kohya_ss\n- controlnet-webuiï¼šhttps://github.com/Mikubill/sd-webui-controlnet\n\n# ç›¸å…³é¡¹ç›®\næˆ‘ä»¬è¿˜åˆ—å‡ºäº†ä¸€äº›å¾ˆæ£’çš„å¼€æºé¡¹ç›®ä»¥åŠä»»ä½•ä½ å¯èƒ½ä¼šæ„Ÿå…´è¶£çš„æ‰©å±•é¡¹ç›®ï¼š\n- [ModelScope](https://github.com/modelscope/modelscope).\n- [FaceChain](https://github.com/modelscope/facechain).\n- [sd-webui-controlnet](https://github.com/Mikubill/sd-webui-controlnet).\n- [sd-webui-roop](https://github.com/s0md3v/sd-webui-roop).\n- [roop](https://github.com/s0md3v/roop).\n- [sd-webui-deforum](https://github.com/deforum-art/sd-webui-deforum).\n- [sd-webui-additional-networks](https://github.com/kohya-ss/sd-webui-additional-networks).\n- [a1111-sd-webui-tagcomplete](https://github.com/DominikDoom/a1111-sd-webui-tagcomplete).\n- [sd-webui-segment-anything](https://github.com/continue-revolution/sd-webui-segment-anything).\n- [sd-webui-tunnels](https://github.com/Bing-su/sd-webui-tunnels).\n- [sd-webui-mov2mov](https://github.com/Scholar01/sd-webui-mov2mov).\n\n# è®¸å¯è¯\næœ¬é¡¹ç›®é‡‡ç”¨ [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).\n\n# è”ç³»æˆ‘ä»¬\n1. ä½¿ç”¨[é’‰é’‰](https://www.dingtalk.com/)æœç´¢2ç¾¤54095000124æˆ–æ‰«æä¸‹åˆ—äºŒç»´ç åŠ å…¥ç¾¤èŠ\n2. ç”±äº å¾®ä¿¡ç¾¤ å·²ç»æ»¡äº†ï¼Œéœ€è¦æ‰«æå³è¾¹çš„å›¾ç‰‡å…ˆæ·»åŠ è¿™ä¸ªåŒå­¦ä¸ºå¥½å‹ï¼Œç„¶åå†åŠ å…¥ å¾®ä¿¡ç¾¤ ã€‚\n<figure>\n<img src=\"images/ding_erweima.jpg\" width=300/>\n<img src=\"images/wechat.jpg\" width=300/>\n</figure>\n"
        },
        {
          "name": "api_test",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "install.py",
          "type": "blob",
          "size": 4.5166015625,
          "content": "# Package check util\n# Modified from https://github.com/Bing-su/adetailer/blob/main/install.py\nimport importlib.util\nimport platform\nfrom importlib.metadata import version\n\nimport launch\nfrom packaging.version import parse\n\n\ndef is_installed(package: str):\n    min_version = \"0.0.0\"\n    max_version = \"99999999.99999999.99999999\"\n    pkg_name = package\n    version_check = True\n    if \"==\" in package:\n        pkg_name, _version = package.split(\"==\")\n        min_version = max_version = _version\n    elif \"<=\" in package:\n        pkg_name, _version = package.split(\"<=\")\n        max_version = _version\n    elif \">=\" in package:\n        pkg_name, _version = package.split(\">=\")\n        min_version = _version\n    else:\n        version_check = False\n    package = pkg_name\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:\n        message = f\"is_installed check for {str(package)} failed as error ModuleNotFoundError\"\n        print(message)\n        return False\n    if spec is None:\n        message = f\"is_installed check for {str(package)} failed as 'spec is None'\"\n        print(message)\n        return False\n    if not version_check:\n        return True\n    if package == \"google.protobuf\":\n        package = \"protobuf\"\n    try:\n        pkg_version = version(package)\n        return parse(min_version) <= parse(pkg_version) <= parse(max_version)\n    except Exception as e:\n        message = f\"is_installed check for {str(package)} failed as error {str(e)}\"\n        print(message)\n        return False\n\n\n# End of Package check util\n\nif not is_installed(\"cv2\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install opencv-python\", \"requirements for opencv\")\n\nif not is_installed(\"tensorflow-cpu\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install tensorflow-cpu\", \"requirements for tensorflow\")\n\nif not is_installed(\"onnx\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install onnx\", \"requirements for onnx\")\n\nif not is_installed(\"onnxruntime\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install onnxruntime\", \"requirements for onnxruntime\")\n\nif not is_installed(\"modelscope==1.9.3\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install modelscope==1.9.3\", \"requirements for modelscope\")\n\nif not is_installed(\"einops\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install einops\", \"requirements for diffusers\")\n\nif not is_installed(\"imageio>=2.29.0\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    # The '>' will be interpreted as redirection (in linux) since SD WebUI uses `shell=True` in `subprocess.run`.\n    launch.run_pip(\"install \\\"imageio>=2.29.0\\\"\", \"requirements for imageio\")\n\nif not is_installed(\"av\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install \\\"imageio[pyav]\\\"\", \"requirements for av\")\n\n# Temporarily pin fsspec==2023.9.2. See https://github.com/huggingface/datasets/issues/6330 for details.\nif not is_installed(\"fsspec==2023.9.2\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install fsspec==2023.9.2\", \"requirements for fsspec\")\n\n# `StableDiffusionXLPipeline` in diffusers requires the invisible-watermark library.\nif not launch.is_installed(\"invisible-watermark\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install invisible-watermark\", \"requirements for invisible-watermark\")\n\n# Tryon requires the shapely and segment-anything library.\nif not launch.is_installed(\"shapely\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install shapely\", \"requirements for shapely\")\n\nif not launch.is_installed(\"segment_anything\"):\n    try:\n        launch.run_pip(\"install segment-anything\", \"requirements for segment_anything\")\n    except Exception:\n        print(\"Can't install segment-anything. Please follow the readme to install manually\")\n\nif not is_installed(\"diffusers>=0.18.2\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    try:\n        launch.run_pip(\"install diffusers==0.23.0\", \"requirements for diffusers\")\n    except Exception as e:\n        print(f\"Can't install the diffusers==0.23.0. Error info {e}\")\n        launch.run_pip(\"install diffusers==0.18.2\", \"requirements for diffusers\")\n\nif platform.system() != \"Windows\":\n    if not is_installed(\"nvitop\"):\n        print(\"Installing requirements for easyphoto-webui\")\n        launch.run_pip(\"install nvitop==1.3.0\", \"requirements for tensorflow\")\n"
        },
        {
          "name": "javascript",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}