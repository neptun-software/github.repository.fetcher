{
  "metadata": {
    "timestamp": 1736561067325,
    "page": 855,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "aigc-apps/sd-webui-EasyPhoto",
      "stars": 5035,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.150390625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n# weight\n*.pth\n*.onnx\n*.safetensors\n*.ckpt\n*.bin\n*.pkl\n*.jpg\n*.png\n\nmodels/stable-diffusion-xl/version.txt\nmodels/pose_templates\nscripts/thirdparty\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 2.646484375,
          "content": "repos:\n  - repo: https://github.com/psf/black\n    rev: 22.3.0\n    hooks:\n      - id: black\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n        args: [\"--line-length=140\"]\n  - repo: https://github.com/PyCQA/flake8\n    rev: 3.9.2\n    hooks:\n      - id: flake8\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n        args: [\"--max-line-length=140\", \"--ignore=E303,E731,W191,W504,E402,E203,F541,W605,W503,E501,E712, F401\", \"--exclude=__init__.py\"]\n  - repo: https://github.com/myint/autoflake\n    rev: v1.4\n    hooks:\n      - id: autoflake\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n        args:\n          [\n            \"--recursive\",\n            \"--in-place\",\n            \"--remove-unused-variable\",\n            \"--ignore-init-module-imports\",\n            \"--exclude=__init__.py\"\n          ]\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: check-ast\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: check-byte-order-marker\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: check-case-conflict\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: check-docstring-first\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: check-executables-have-shebangs\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: check-json\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: check-yaml\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: debug-statements\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: detect-private-key\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: end-of-file-fixer\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: trailing-whitespace\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n      - id: mixed-line-ending\n        exclude: models/|scripts/easyphoto_utils/animatediff/|scripts/easyphoto_utils/animatediff_utils.py\n"
        },
        {
          "name": "COVENANT.md",
          "type": "blob",
          "size": 1.544921875,
          "content": "# EasyPhoto Developer Covenant\nDisclaimer: This covenant serves as a set of recommended guidelines.\n\n## Overview:\nEasyPhoto is an open-source software built on the SDWebUI plugin ecosystem, focusing on leveraging AIGC technology to create true-to-life, aesthetic, and beautiful AI portraits (\"true/like/beautiful\"). We are committed to expanding the application scope of this technology, lowering the entry barrier, and facilitating use for a wide audience.\n\n## Covenant Purpose:\nAlthough technology is inherently neutral in value, considering that EasyPhoto already has the capability to produce highly realistic images, particularly facial images, we strongly suggest that everyone involved in development and usage adhere to the following guidelines.\n\n## Behavioral Guidelines:\n- Comply with laws and regulations of relevant jurisdictions: It is prohibited to use this technology for any unlawful, criminal, or activities against public morals and decency.\n- Content Restrictions: It is prohibited to produce or disseminate any images that may involve political figures, pornography, violence, or other activities contrary to regional regulations.\n\n## Ongoing Updates:\nThis covenant will be updated periodically to adapt to technological and social advancements. We encourage community members to follow these guidelines in daily interactions and usage. Non-compliance will result in appropriate community management actions.\n\nThank you for your cooperation and support. Together, let's ensure that EasyPhoto remains a responsible and sustainably-developed open-source software.\n"
        },
        {
          "name": "COVENANT_zh-CN.md",
          "type": "blob",
          "size": 1.150390625,
          "content": "# EasyPhoto 开发者公约\n!声明：本公约仅为推荐性准则\n\n## 概述：\nEasyPhoto 是一个基于SDWebUI插件生态构建的开源软件，专注于利用AIGC技术实现真/像/美的AI-写真。我们致力于拓展该技术的应用范围，降低使用门槛，并为广大用户提供便利。\n\n## 公约宗旨：\n尽管技术本身并无价值倾向，但考虑到EasyPhoto目前已具备生成逼真图像（特别是人脸图像）的能力，我们强烈建议所有参与开发和使用的人员遵循以下准则。\n\n## 行为准则：\n- 遵循相关地区的法律和法规：不得利用本技术从事任何违法、犯罪或有悖于社会公序良俗的活动。\n- 内容限制：禁止生成或传播任何可能涉及政治人物、色情、暴力或其他违反相关地区规定的图像。\n\n## 持续更新：\n本公约将不定期进行更新以适应技术和社会发展。我们鼓励社群成员在日常交流和使用中遵循这些准则。未遵守本公约的行为将在社群管理中受到相应限制。\n感谢您的配合与支持。我们共同努力，以确保EasyPhoto成为一个负责任和可持续发展的开源软件。\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.1005859375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      the copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by the Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributors that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, the Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assuming any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.2822265625,
          "content": "# 📷 EasyPhoto | Your Smart AI Photo Generator.\n🦜 EasyPhoto is a Webui UI plugin for generating AI portraits that can be used to train digital doppelgangers relevant to you.\n\n🦜 🦜 Welcome!\n\n[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-yellow)](https://huggingface.co/spaces/alibaba-pai/easyphoto)\n\nEnglish | [简体中文](./README_zh-CN.md)\n\n# Table of Contents\n- [Introduction](#introduction)\n- [TODO List](#todo-list)\n- [Quick Start](#quick-start)\n    - [1. Cloud usage: AliyunDSW/AutoDL/Docker](#1-cloud-usage-aliyundswautodldocker)\n    - [2. Local install: Check/Downloading/Installation](#2-local-install-environment-checkdownloadinginstallation)\n- [How to use](#how-to-use)\n    - [1. Model Training](#1-model-training)\n    - [2. Inference](#2-inference)\n- [API test](./api_test/README.md)\n- [Algorithm Detailed](#algorithm-detailed)\n    - [1. Architectural Overview](#1-architectural-overview)\n    - [2. Training Detailed](#2-training-detailed)\n    - [3. Inference Detailed](#3-inference-detailed)\n- [Reference](#reference)\n- [Related Project](#Related-Project)\n- [License](#license)\n- [ContactUS](#contactus)\n\n# Introduction\nEasyPhoto is a Webui UI plugin for generating AI portraits that can be used to train digital doppelgangers relevant to you. Training is recommended to be done with 5 to 20 portrait images, preferably half-body photos, and do not wear glasses (It doesn't matter if the characters in a few pictures wear glasses). After the training is done, we can generate it in the Inference section. We support using preset template images or uploading your own images for Inference.\n\nPlease read our Contributor Covenant [covenant](./COVENANT.md) | [简体中文](./COVENANT_zh-CN.md).\n\nIf you encounter any problems in the training, please refer to the [VQA](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki).\n\nWe now support quick pull-ups from different platforms, refer to [Quick Start](#quick-start).\n\nNow you can experience EasyPhoto demo quickly on ModelScope, [demo](https://modelscope.cn/studios/PAI/EasyPhoto/summary).\n\nWhat's New:\n- Support LCM-Lora based sampling acceleration, now you only need 12 step (vs 50 steps) for both Image & Video generation, and we provide Scene Lora training and inference in both text2Image and text2Video.[🔥 🔥 🔥 🔥 2023.12.09]\n- Support Concepts-Sliders based attribute editing and Virtual TryOn， please refer to [sliders-wiki](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki/Attribute-Edit) , [tryon-wiki](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki/TryOn) for more details. [🔥 🔥 🔥 🔥 2023.12.08]\n- Thanks to [lanrui-ai](https://www.lanrui-ai.com/). It offers an SDWebUI image with built-in EasyPhoto, promising bi-weekly updates. Personally tested, it can pull up resources in 2 minutes and complete startup within 5 minutes. [ 2023.11.20 ]\n- We are already support Video Inference without more traning! Specific details can go [here](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki/Video)![🔥 🔥 🔥 🔥 2023.11.10]\n- SDXL Training and Inference Support. Specific details can go [here](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki/SDXL)![🔥 🔥 🔥 🔥 2023.11.10]\n- ComfyUI Support at [repo](https://github.com/THtianhao/ComfyUI-Portrait-Maker), thanks to [THtianhao](https://github.com/THtianhao) great work![🔥 🔥 🔥 2023.10.17]\n- EasyPhoto arxiv [arxiv](https://arxiv.org/abs/2310.04672)[🔥 🔥 🔥 2023.10.10]\n- Support SDXL to generate High resolution template, no more upload image need in this mode(SDXL), need 16GB GPU memory! Specific details can go [here](https://zhuanlan.zhihu.com/p/658940203)[ 2023.09.26 ]\n- We also support the [Diffusers Edition](https://github.com/aigc-apps/EasyPhoto/). [ 2023.09.25 ]\n- **Support fine-tuning the background and calculating the similarity score between the generated image and the user.** [ 2023.09.15 ]\n- **Support different base models for training and inference.** [ 2023.09.08 ]\n- **Support multi-people generation! Add cache option to optimize inference speed. Add log refreshing on UI.** [ 2023.09.06 ]\n- Create Code! Support for Windows and Linux Now. [ 2023.09.02 ]\n\nThese are our generated results:\n![results_1](images/results_1.jpg)\n\nVideo Part:\n|  Example |  1  |  2  |  3  |\n|  ---- | ---- | ---- | ---- |\n| - | <img src=\"http://pai-vision-data-hz.oss-accelerate.aliyuncs.com/easyphoto/data/video/text2video/51s3.gif\" width=\"400\"> | <img src=\"http://pai-vision-data-hz.oss-accelerate.aliyuncs.com/easyphoto/data/video/v2videos/ring_3644.gif\" width=\"400\"> | <img src=\"http://pai-vision-data-hz.oss-accelerate.aliyuncs.com/easyphoto/data/video/img2video_2imgs/29s3.gif\" width=\"400\"> |\n\nPhoto Part:\n![results_2](images/results_2.jpg)\n![results_3](images/results_3.jpg)\n\nOur UI interface is as follows:\n**train part:**\n![train_ui](images/train_ui.jpg)\n**inference part:**\n![infer_ui](images/infer_ui.jpg)\n\n# TODO List\n- Support chinese ui.\n- Support change in template's background.\n- Support high resolution.\n\n# Quick Start\n### 1. Cloud usage: AliyunDSW/AutoDL/lanrui-ai/Docker\n#### a. From AliyunDSW\nDSW has free GPU time, which can be applied once by a user and is valid for 3 months after applying.\n\nAliyun provide free GPU time in [Freetier](https://help.aliyun.com/document_detail/2567864.html), get it and use in Aliyun PAI-DSW to start EasyPhoto within 3min!\n\n[![DSW Notebook](images/dsw.png)](https://gallery.pai-ml.com/#/preview/deepLearning/cv/stable_diffusion_easyphoto)\n\n#### b. From AutoDL/lanrui-ai\n##### lanrui-ai\nThe official full-plugin version of lanrui-ai comes with EasyPhoto built-in. They promise bi-weekly testing and updates. Personally tested and found to be effective, it can be launched within 5 minutes. Thanks to their support and contributions to the community.\n\n##### AutoDL\nIf you are using Lanrui-ai/AutoDL, you can quickly pull up the Stable DIffusion webui using the mirror we provide.\n\nYou can select the desired mirror by filling in the following information in Community Mirrors, or using offical Image provide by lanrui-ai.\n```\naigc-apps/sd-webui-EasyPhoto/sd-webui-EasyPhoto\n```\n\n#### c. From docker\nIf you are using docker, please make sure that the graphics card driver and CUDA environment have been installed correctly in your machine.\n\nThen execute the following commands in this way:\n```\n# pull image\ndocker pull registry.cn-beijing.aliyuncs.com/mybigpai/sd-webui-easyphoto:0.0.3\n\n# enter image\ndocker run -it -p 7860:7860 --network host --gpus all registry.cn-beijing.aliyuncs.com/mybigpai/sd-webui-easyphoto:0.0.3\n\n# launch webui\npython3 launch.py --port 7860\n```\nThe docker updates may be slightly slower than the github repository of sd-webui-EasyPhoto, so you can go to extensions/sd-webui-EasyPhoto and do a git pull first.\n```\ncd extensions/sd-webui-EasyPhoto/\ngit pull\ncd /workspace\n```\n\n### 2. Local install: Environment Check/Downloading/Installation\n#### a. Environment Check\nWe have verified EasyPhoto execution on the following environment:\nIf you meet problem with WebUI auto killed by OOM, please refer to [ISSUE21](https://github.com/aigc-apps/sd-webui-EasyPhoto/issues/21), and setting some `num_threads` to `0` and report other fix to us, thanks.\n\nThe detailed of Windows 10:\n- OS: Windows10\n- python: py3.10\n- pytorch: torch2.0.1\n- tensorflow-cpu: 2.13.0\n- CUDA: 11.7\n- CUDNN: 8+\n- GPU: Nvidia-3060 12G\n\nThe detailed of Linux:\n- OS: Ubuntu 20.04, CentOS\n- python: py3.10 & py3.11\n- pytorch: torch2.0.1\n- tensorflow-cpu: 2.13.0\n- CUDA: 11.7\n- CUDNN: 8+\n- GPU: Nvidia-A10 24G & Nvidia-V100 16G & Nvidia-A100 40G\n\nWe need about 60GB available on disk (for saving weights and datasets process), please check!\n\n#### b.  Relevant Repositories & Weights Downloading\n##### i. Controlnet\nWe need to use Controlnet for inference. The related repo is [Mikubill/sd-webui-controlnet](https://github.com/Mikubill/sd-webui-controlnet). You need install this repo before using EasyPhoto.\n\nIn addition, we need at least three Controlnets for inference. So you need to set the **Multi ControlNet: Max models amount (requires restart)** in Setting.\n![controlnet_num](images/controlnet_num.jpg)\n\n##### ii. Other Dependencies.\nWe are mutually compatible with the existing stable-diffusion-webui environment, and the relevant repositories are installed when starting stable-diffusion-webui.\n\nThe weights we need will be downloaded automatically when you start training first time.\n\n#### c. Plug-in Installation\nWe now support installing EasyPhoto from git. The url of our Repository is `https://github.com/aigc-apps/sd-webui-EasyPhoto`.\n\nWe will support installing EasyPhoto from **Available** in the future.\n\n![install](images/install.jpg)\n\n\n# How to use\n### 1. Model Training\nThe EasyPhoto training interface is as follows:\n\n- On the left is the training image. Simply click `Upload Photos` to upload the image, and click `Clear Photos` to delete the uploaded image;\n- On the right are the training parameters, which cannot be adjusted for the first training.\n\nAfter clicking `Upload Photos`, we can start uploading images. **It is best to upload 5 to 20 images here, including different angles and lighting conditions**. It is best to have some images that do not include glasses. If they are all glasses, the generated results may easily generate glasses.\n![train_1](images/train_1.jpg)\n\nThen we click on `Start Training` below, and at this point, we need to fill in the `User ID` above, such as the user's name, to start training.\n![train_2](images/train_2.jpg)\n\nAfter the model starts training, the webui will automatically refresh the training log. If there is no refresh, click `Refresh Log` button.\n![train_3](images/train_3.jpg)\n\nIf you want to set parameters, the parsing of each parameter is as follows:\n\n|Parameter Name | Meaning|\n|--|--|\n|Resolution | The size of the image fed into the network during training, with a default value of `512`|\n|Validation & save steps | The number of steps between validating the image and saving intermediate weights, with a default value of `100`, representing verifying the image every `100` steps and saving the weights|\n|Max train steps | Maximum number of training steps, default value is `800`|\n|Max steps per photos | The maximum number of training sessions per image, default to `200`|\n|Train batch size | The batch size of the training, with a default value of `1`|\n|Gradient accumulation steps | Whether to perform gradient accumulation. The default value is `4`. Combined with the train batch size, each step is equivalent to feeding four images|\n|Dataloader num workers | The number of jobs loaded with data, which does not take effect under Windows because an error will be reported if set, but is set normally on Linux|\n|Learning rate | Train Lora's learning rate, default to `1e-4`|\n|Rank Lora | The feature length of the weight, default to `128`|\n|Network alpha | The regularization parameter for Lora training, usually half of the rank, defaults to `64`|\n\n### 2. Inference\n#### a. single people\n- Step 1: Click the refresh button to query the model corresponding to the trained user ID.\n- Step 2: Select the `user ID`.\n- Step 3: Select the template that needs to be generated.\n- Step 4: Click the Generate button to generate the results.\n\n![single_people](images/single_people.jpg)\n\n#### b. multi people\n- Step 1: Go to the settings page of EasyPhoto and set `num_of_faceid` as greater than `1`.\n- Step 2: Apply settings.\n- Step 3: Restart the ui interface of the webui.\n- Step 4: Return to EasyPhoto and upload the two person template.\n- Step 5: Select the user IDs of two people.\n- Step 6: Click the `Generate` button. Perform image generation.\n\n![single_people](images/multi_people_1.jpg)\n![single_people](images/multi_people_2.jpg)\n# Algorithm Detailed\n- Arxiv paper EasyPhoto [arxiv](https://arxiv.org/abs/2310.04672)\n- More detailed principles and details can be found [BLOG](https://blog.csdn.net/weixin_44791964/article/details/132922309)\n\n\n### 1. Architectural Overview\n\n![overview](images/overview.jpg)\n\nIn the field of AI portraits, we expect model-generated images to be realistic and resemble the user, and traditional approaches introduce unrealistic lighting (such as face fusion or roop). To address this unrealism, we introduce the image-to-image capability of the stable diffusion model. Generating a perfect personal portrait takes into account the desired generation scenario and the user's digital doppelgänger. We use a pre-prepared template as the desired generation scene and an online trained face LoRA model as the user's digital doppelganger, which is a popular stable diffusion fine-tuning model. We use a small number of user images to train a stable digital doppelgänger of the user, and generate a personal portrait image based on the face LoRA model and the expected generative scene during inference.\n\n### 2. Training Detailed\n\n![overview](images/train_detail1.jpg)\n\nFirst, we perform face detection on the input user image, and after determining the face location, we intercept the input image according to a certain ratio. Then, we use the saliency detection model and the skin beautification model to obtain a clean face training image, which basically consists of only faces. Then, we label each image with a fixed label. There is no need to use a labeler here, and the results are good. Finally, we fine-tune the stabilizing diffusion model to get the user's digital doppelganger.\n\nDuring training, we utilize the template image for verification in real time, and at the end of training, we calculate the face id gap between the verification image and the user's image to achieve Lora fusion, which ensures that our Lora is a perfect digital doppelganger of the user.\n\nIn addition, we will choose the image that is most similar to the user in the validation as the face_id image, which will be used in Inference.\n\n### 3. Inference Detailed\n#### a. First Diffusion:\nFirst, we will perform face detection on our incoming template image to determine the mask that needs to be inpainted for stable diffusion. then we will use the template image to perform face fusion with the optimal user image. After the face fusion is completed, we use the above mask to inpaint (fusion_image) with the face fused image. In addition, we will affix the optimal face_id image obtained during training to the template image by affine transformation (replaced_image). Then we will apply Controlnets on it, we use canny with color to extract features for fusion_image and openpose for replaced_image to ensure the similarity and stability of the images. Then we will use Stable Diffusion combined with the user's digital split for generation.\n\n#### b. Second Diffusion:\nAfter getting the result of First Diffusion, we will fuse the result with the optimal user image for face fusion, and then we will use Stable Diffusion again with the user's digital doppelganger for generation. The second generation will use higher resolution.\n\n# Special thanks\nSpecial thanks to DevelopmentZheng, qiuyanxin, rainlee, jhuang1207, bubbliiiing, wuziheng, yjjinjie, hkunzhe, yunkchen for their code contributions (in no particular order).\n\n# Reference\n- insightface：https://github.com/deepinsight/insightface\n- cv_resnet50_face：https://www.modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface/summary\n- cv_u2net_salient：https://www.modelscope.cn/models/damo/cv_u2net_salient-detection/summary\n- cv_unet_skin_retouching_torch：https://www.modelscope.cn/models/damo/cv_unet_skin_retouching_torch/summary\n- cv_unet-image-face-fusion：https://www.modelscope.cn/models/damo/cv_unet-image-face-fusion_damo/summary\n- kohya：https://github.com/bmaltais/kohya_ss\n- controlnet-webui：https://github.com/Mikubill/sd-webui-controlnet\n\n# Related Project\nWe've also listed some great open source projects as well as any extensions you might be interested in:\n- [ModelScope](https://github.com/modelscope/modelscope).\n- [FaceChain](https://github.com/modelscope/facechain).\n- [sd-webui-controlnet](https://github.com/Mikubill/sd-webui-controlnet).\n- [sd-webui-roop](https://github.com/s0md3v/sd-webui-roop).\n- [roop](https://github.com/s0md3v/roop).\n- [sd-webui-deforum](https://github.com/deforum-art/sd-webui-deforum).\n- [sd-webui-additional-networks](https://github.com/kohya-ss/sd-webui-additional-networks).\n- [a1111-sd-webui-tagcomplete](https://github.com/DominikDoom/a1111-sd-webui-tagcomplete).\n- [sd-webui-segment-anything](https://github.com/continue-revolution/sd-webui-segment-anything).\n- [sd-webui-tunnels](https://github.com/Bing-su/sd-webui-tunnels).\n- [sd-webui-mov2mov](https://github.com/Scholar01/sd-webui-mov2mov).\n\n# License\n\nThis project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).\n\n# ContactUS\n1. Use [Dingding](https://www.dingtalk.com/) to search group-2 `54095000124` or Scan to join\n2. Since the WeChat group is full, you need to scan the image on the right to add this student as a friend first, and then join the WeChat group.\n\n<figure>\n<img src=\"images/ding_erweima.jpg\" width=300/>\n<img src=\"images/wechat.jpg\" width=300/>\n</figure>\n\n\n# Contributors ✨\n\nThanks goes to these wonderful people :\n\n<table>\n  <tr>\n     <td>\n  <a href=\"https://github.com/aigc-apps/sd-webui-EasyPhoto/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=aigc-apps/sd-webui-EasyPhoto\" />\n  </a>\n    </td>\n  </tr>\n</table>\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind are welcome!\n\n<p align=\"right\"><a href=\"#top\">Back to top</a></p>\n"
        },
        {
          "name": "README_zh-CN.md",
          "type": "blob",
          "size": 14.373046875,
          "content": "# EasyPhoto | 您的智能 AI 照片生成器。\n🦜 EasyPhoto是一款Webui UI插件，用于生成AI肖像画，该代码可用于训练与您相关的数字分身。\n\n🦜 🦜 Welcome!\n\n[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-yellow)](https://huggingface.co/spaces/alibaba-pai/easyphoto)\n\n[English](./README.md) | 简体中文\n\n# 目录\n- [简介](#简介)\n- [TODO List](#todo-list)\n- [快速启动](#快速启动)\n    - [1. 云使用: AliyunDSW/AutoDL/Docker](#1-云使用-aliyundswautodldocker)\n    - [2. 本地安装: 环境检查/下载/安装](#2-本地安装-环境检查下载安装)\n- [如何使用](#如何使用)\n    - [1. 模型训练](#1-模型训练)\n    - [2. 人物生成](#2-人物生成)\n- [API测试](./api_test/README.md)\n- [算法详细信息](#算法详细信息)\n    - [1. 架构概述](#1-架构概述)\n    - [2. 训练细节](#2-训练细节)\n    - [3. 推理细节](#3-推理细节)\n- [参考文献](#参考文献)\n- [相关项目](#相关项目)\n- [许可证](#许可证)\n- [联系我们](#联系我们)\n\n# 简介\nEasyPhoto是一款Webui UI插件，用于生成AI肖像画，该代码可用于训练与您相关的数字分身。建议使用 5 到 20 张肖像图片进行训练，最好是半身照片且不要佩戴眼镜（少量可以接受）。训练完成后，我们可以在推理部分生成图像。我们支持使用预设模板图片与上传自己的图片进行推理。\n\n请阅读我们的开发者公约，共建美好社区 [covenant](./COVENANT.md) | [简体中文](./COVENANT_zh-CN.md)\n\n如果您在训练中遇到一些问题，请参考 [VQA](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki)。\n\n我们现在支持从不同平台快速启动，请参阅 [快速启动](#快速启动)。\n\n新特性：\n- 支持基于LCM-Lora的采样加速，现在您只需要进行12个steps(vs 50steps)来生成图像和视频, 并支持了场景化（风格化） Lora的训练和大量内置的模型。[🔥 🔥 🔥 🔥 2023.12.09]\n- 支持基于Concepts-Sliders的属性编辑和虚拟试穿，请参考[sliders-wiki](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki/Attribute-Edit) , [tryon-wiki](https://github.com/aigc-apps/sd-webui-EasyPhoto/wiki/TryOn)获取更多详细信息。[🔥 🔥 🔥 🔥 2023.12.08]\n- 感谢[揽睿星舟](https://www.lanrui-ai.com/) 提供了内置EasyPhoto的SDWebUI官方镜像，并承诺每两周更新一次。亲自测试，可以在2分钟内拉起资源，并在5分钟内完成启动。[🔥 🔥 🔥 🔥 2023.11.20]\n- ComfyUI 支持 [repo](https://github.com/THtianhao/ComfyUI-Portrait-Maker), 感谢[THtianhao](https://github.com/THtianhao)的精彩工作![🔥 🔥 🔥 2023.10.17]\n- EasyPhoto 论文地址 [arxiv](https://arxiv.org/abs/2310.04672)[🔥 🔥 🔥 2023.10.10]\n- 支持使用SDXL模型和一定的选项直接生成高清大图，不再需要上传模板，需要16GB显存。具体细节可以前往[这里](https://zhuanlan.zhihu.com/p/658940203)[🔥 🔥 🔥 2023.09.26]\n- 我们同样支持[Diffusers版本](https://github.com/aigc-apps/EasyPhoto/)。 [🔥 2023.09.25]\n- **支持对背景进行微调，并计算生成的图像与用户之间的相似度得分。** [🔥🔥 2023.09.15]\n- **支持不同预测基础模型。** [🔥🔥 2023.09.08]\n- **支持多人生成！添加缓存选项以优化推理速度。在UI上添加日志刷新。** [🔥🔥 2023.09.06]\n- 创建代码！现在支持 Windows 和 Linux。[🔥 2023.09.02]\n\n这些是我们的生成结果:\n![results_1](images/results_1.jpg)\n![results_2](images/results_2.jpg)\n![results_3](images/results_3.jpg)\n\n我们的ui界面如下:\n**训练部分:**\n![train_ui](images/train_ui.jpg)\n**预测部分:**\n![infer_ui](images/infer_ui.jpg)\n\n# TODO List\n- 支持中文界面。\n- 支持模板背景部分变化。\n- 支持高分辨率。\n\n# 快速启动\n### 1. 云使用: AliyunDSW/AutoDL/揽睿星舟/Docker\n#### a. 通过阿里云 DSW\nDSW 有免费 GPU 时间，用户可申请一次，申请后3个月内有效。\n\n阿里云在[Freetier](https://free.aliyun.com/?product=9602825&crowd=enterprise&spm=5176.28055625.J_5831864660.1.e939154aRgha4e&scm=20140722.M_9974135.P_110.MO_1806-ID_9974135-MID_9974135-CID_30683-ST_8512-V_1)提供免费GPU时间，获取并在阿里云PAI-DSW中使用，3分钟内即可启动EasyPhoto\n\n[![DSW Notebook](images/dsw.png)](https://gallery.pai-ml.com/#/preview/deepLearning/cv/stable_diffusion_easyphoto)\n\n#### b. 通过揽睿星舟/AutoDL\n##### 揽睿星舟\n揽睿星舟官方全插件版本内置EasyPhoto，并承诺每两周测试与更新，亲测可用，5分钟内拉起，感谢他们的支持和对社区做出的贡献。\n\n##### AutoDL\n如果您正在使用 AutoDL，您可以使用我们提供的镜像快速启动 Stable DIffusion webui。\n\n您可以在社区镜像中填写以下信息来选择所需的镜像。\n```\naigc-apps/sd-webui-EasyPhoto/sd-webui-EasyPhoto\n```\n#### c. 通过docker\n使用docker的情况下，请保证机器中已经正确安装显卡驱动与CUDA环境，然后以此执行以下命令：\n```\n# 拉取镜像\ndocker pull registry.cn-beijing.aliyuncs.com/mybigpai/sd-webui-easyphoto:0.0.3\n\n# 进入镜像\ndocker run -it -p 7860:7860 --network host --gpus all registry.cn-beijing.aliyuncs.com/mybigpai/sd-webui-easyphoto:0.0.3\n\n# 启动webui\npython3 launch.py --port 7860\n```\n\n### 2. 本地安装: 环境检查/下载/安装\n#### a. 环境检查\n我们已验证EasyPhoto可在以下环境中执行：\n如果你遇到内存使用过高而导致WebUI进程自动被kill掉，请参考[ISSUE21](https://github.com/aigc-apps/sd-webui-EasyPhoto/issues/21)，设置一些参数，例如num_threads=0，如果你也发现了其他解决的好办法，请及时联系我们。\n\nWindows 10 的详细信息：\n- 操作系统： Windows10\n- python: python 3.10\n- pytorch: torch2.0.1\n- tensorflow-cpu: 2.13.0\n- CUDA: 11.7\n- CUDNN: 8+\n- GPU： Nvidia-3060 12G\n\nLinux 的详细信息：\n- 操作系统 Ubuntu 20.04, CentOS\n- python: python3.10 & python3.11\n- pytorch: torch2.0.1\n- tensorflow-cpu: 2.13.0\n- CUDA: 11.7\n- CUDNN: 8+\n- GPU： Nvidia-A10 24G & Nvidia-V100 16G & Nvidia-A100 40G\n\n我们需要大约 60GB 的可用磁盘空间（用于保存权重和数据集），请检查！\n\n#### b. 相关资料库和权重下载\n##### i. Controlnet\n我们需要使用 Controlnet 进行推理。相关软件源是[Mikubill/sd-webui-controlnet](https://github.com/Mikubill/sd-webui-controlnet)。在使用 EasyPhoto 之前，您需要安装这个软件源。\n\n此外，我们至少需要三个 Controlnets 用于推理。因此，您需要设置 **Multi ControlNet: Max models amount (requires restart)**。\n![controlnet_num](images/controlnet_num.jpg)\n\n##### ii. 其他依赖关系。\n我们与现有的 stable-diffusion-webui 环境相互兼容，启动 stable-diffusion-webui 时会安装相关软件源。\n\n我们所需的权重会在第一次开始训练时自动下载。\n\n#### c. 插件安装\n现在我们支持从 git 安装 EasyPhoto。我们的仓库网址是 https://github.com/aigc-apps/sd-webui-EasyPhoto。\n\n今后，我们将支持从 **Available** 安装 EasyPhoto。\n\n![install](images/install.jpg)\n\n# 如何使用\n### 1. 模型训练\nEasyPhoto训练界面如下：\n- 左边是训练图像。只需点击上传照片即可上传图片，点击清除照片即可删除上传的图片；\n- 右边是训练参数，不能为第一次训练进行调整。\n\n点击上传照片后，我们可以开始上传图像**这里最好上传5到20张图像，包括不同的角度和光照**。最好有一些不包括眼镜的图像。如果所有图片都包含眼镜眼镜，则生成的结果可以容易地生成眼镜。\n![train_1](images/train_1.jpg)\n\n然后我们点击下面的“开始培训”，此时，我们需要填写上面的用户ID，例如用户名，才能开始培训。\n![train_2](images/train_2.jpg)\n\n模型开始训练后，webui会自动刷新训练日志。如果没有刷新，请单击“Refresh Log”按钮。\n![train_3](images/train_3.jpg)\n\n如果要设置参数，每个参数的解析如下：\n| 参数名 | 含义 |\n|--|--|\n| resolution  | 训练时喂入网络的图片大小，默认值为512 |\n| validation & save steps| 验证图片与保存中间权重的steps数，默认值为100，代表每100步验证一次图片并保存权重 |\n| max train steps | 最大训练步数，默认值为800 |\n| max steps per photos | 每张图片的最大训练次数，默认为200 |\n| train batch size | 训练的批次大小，默认值为1 |\n| gradient accumulationsteps | 是否进行梯度累计，默认值为4，结合train batch size来看，每个Step相当于喂入四张图片 |\n| dataloader num workers | 数据加载的works数量，windows下不生效，因为设置了会报错，Linux正常设置 |\n| learning rate  | 训练Lora的学习率，默认为1e-4 |\n| rank Lora | 权重的特征长度，默认为128 |\n| network alpha | Lora训练的正则化参数，一般为rank的二分之一，默认为64 |\n\n### 2. 人物生成\n#### a. 单人模版\n- 步骤1：点击刷新按钮，查询训练后的用户ID对应的模型。\n- 步骤2：选择用户ID。\n- 步骤3：选择需要生成的模板。\n- 步骤4：单击“生成”按钮生成结果。\n\n![single_people](images/single_people.jpg)\n\n#### b. 多人模板\n- 步骤1：转到EasyPhoto的设置页面，设置num_of_Faceid大于1。\n- 步骤2：应用设置。\n- 步骤3：重新启动webui的ui界面。\n- 步骤4：返回EasyPhoto并上传多人模板。\n- 步骤5：选择两个人的用户ID。\n- 步骤6：单击“生成”按钮。执行图像生成。\n\n![single_people](images/multi_people_1.jpg)\n![single_people](images/multi_people_2.jpg)\n\n# 算法详细信息\n- 英文论文[arxiv](https://arxiv.org/abs/2310.04672)\n- 中文博客[这里](https://blog.csdn.net/weixin_44791964/article/details/132922309)\n\n### 1. 架构概述\n\n![overview](images/overview.jpg)\n\n在人工智能肖像领域，我们希望模型生成的图像逼真且与用户相似，而传统方法会引入不真实的光照（如人脸融合或roop）。为了解决这种不真实的问题，我们引入了稳定扩散模型的图像到图像功能。生成完美的个人肖像需要考虑所需的生成场景和用户的数字分身。我们使用一个预先准备好的模板作为所需的生成场景，并使用一个在线训练的人脸 LoRA 模型作为用户的数字分身，这是一种流行的稳定扩散微调模型。我们使用少量用户图像来训练用户的稳定数字分身，并在推理过程中根据人脸 LoRA 模型和预期生成场景生成个人肖像图像。\n\n\n### 2. 训练细节\n\n![overview](images/train_detail1.jpg)\n\n首先，我们对输入的用户图像进行人脸检测，确定人脸位置后，按照一定比例截取输入图像。然后，我们使用显著性检测模型和皮肤美化模型获得干净的人脸训练图像，该图像基本上只包含人脸。然后，我们为每张图像贴上一个固定标签。这里不需要使用标签器，而且效果很好。最后，我们对稳定扩散模型进行微调，得到用户的数字分身。\n\n在训练过程中，我们会利用模板图像进行实时验证，在训练结束后，我们会计算验证图像与用户图像之间的人脸 ID 差距，从而实现 Lora 融合，确保我们的 Lora 是用户的完美数字分\n身。\n\n此外，我们将选择验证中与用户最相似的图像作为 face_id 图像，用于推理。\n\n### 3. 推理细节\n#### a. 第一次扩散：\n首先，我们将对接收到的模板图像进行人脸检测，以确定为实现稳定扩散而需要涂抹的遮罩。然后，我们将使用模板图像与最佳用户图像进行人脸融合。人脸融合完成后，我们将使用上述遮罩对融合后的人脸图像进行内绘（fusion_image）。此外，我们还将通过仿射变换（replace_image）把训练中获得的最佳 face_id 图像贴到模板图像上。然后，我们将对其应用 Controlnets，在融合图像中使用带有颜色的 canny 提取特征，在替换图像中使用 openpose 提取特征，以确保图像的相似性和稳定性。然后，我们将使用稳定扩散（Stable Diffusion）结合用户的数字分割进行生成。\n\n#### b. 第二次扩散：\n在得到第一次扩散的结果后，我们将把该结果与最佳用户图像进行人脸融合，然后再次使用稳定扩散与用户的数字分身进行生成。第二次生成将使用更高的分辨率。\n\n# 特别感谢\n特别感谢DevelopmentZheng, qiuyanxin, rainlee, jhuang1207, bubbliiiing, wuziheng, yjjinjie, hkunzhe, yunkchen同学们的代码贡献（此排名不分先后）。\n\n# 参考文献\n- insightface：https://github.com/deepinsight/insightface\n- cv_resnet50_face：https://www.modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface/summary\n- cv_u2net_salient：https://www.modelscope.cn/models/damo/cv_u2net_salient-detection/summary\n- cv_unet_skin_retouching_torch：https://www.modelscope.cn/models/damo/cv_unet_skin_retouching_torch/summary\n- cv_unet-image-face-fusion：https://www.modelscope.cn/models/damo/cv_unet-image-face-fusion_damo/summary\n- kohya：https://github.com/bmaltais/kohya_ss\n- controlnet-webui：https://github.com/Mikubill/sd-webui-controlnet\n\n# 相关项目\n我们还列出了一些很棒的开源项目以及任何你可能会感兴趣的扩展项目：\n- [ModelScope](https://github.com/modelscope/modelscope).\n- [FaceChain](https://github.com/modelscope/facechain).\n- [sd-webui-controlnet](https://github.com/Mikubill/sd-webui-controlnet).\n- [sd-webui-roop](https://github.com/s0md3v/sd-webui-roop).\n- [roop](https://github.com/s0md3v/roop).\n- [sd-webui-deforum](https://github.com/deforum-art/sd-webui-deforum).\n- [sd-webui-additional-networks](https://github.com/kohya-ss/sd-webui-additional-networks).\n- [a1111-sd-webui-tagcomplete](https://github.com/DominikDoom/a1111-sd-webui-tagcomplete).\n- [sd-webui-segment-anything](https://github.com/continue-revolution/sd-webui-segment-anything).\n- [sd-webui-tunnels](https://github.com/Bing-su/sd-webui-tunnels).\n- [sd-webui-mov2mov](https://github.com/Scholar01/sd-webui-mov2mov).\n\n# 许可证\n本项目采用 [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).\n\n# 联系我们\n1. 使用[钉钉](https://www.dingtalk.com/)搜索2群54095000124或扫描下列二维码加入群聊\n2. 由于 微信群 已经满了，需要扫描右边的图片先添加这个同学为好友，然后再加入 微信群 。\n<figure>\n<img src=\"images/ding_erweima.jpg\" width=300/>\n<img src=\"images/wechat.jpg\" width=300/>\n</figure>\n"
        },
        {
          "name": "api_test",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "install.py",
          "type": "blob",
          "size": 4.5166015625,
          "content": "# Package check util\n# Modified from https://github.com/Bing-su/adetailer/blob/main/install.py\nimport importlib.util\nimport platform\nfrom importlib.metadata import version\n\nimport launch\nfrom packaging.version import parse\n\n\ndef is_installed(package: str):\n    min_version = \"0.0.0\"\n    max_version = \"99999999.99999999.99999999\"\n    pkg_name = package\n    version_check = True\n    if \"==\" in package:\n        pkg_name, _version = package.split(\"==\")\n        min_version = max_version = _version\n    elif \"<=\" in package:\n        pkg_name, _version = package.split(\"<=\")\n        max_version = _version\n    elif \">=\" in package:\n        pkg_name, _version = package.split(\">=\")\n        min_version = _version\n    else:\n        version_check = False\n    package = pkg_name\n    try:\n        spec = importlib.util.find_spec(package)\n    except ModuleNotFoundError:\n        message = f\"is_installed check for {str(package)} failed as error ModuleNotFoundError\"\n        print(message)\n        return False\n    if spec is None:\n        message = f\"is_installed check for {str(package)} failed as 'spec is None'\"\n        print(message)\n        return False\n    if not version_check:\n        return True\n    if package == \"google.protobuf\":\n        package = \"protobuf\"\n    try:\n        pkg_version = version(package)\n        return parse(min_version) <= parse(pkg_version) <= parse(max_version)\n    except Exception as e:\n        message = f\"is_installed check for {str(package)} failed as error {str(e)}\"\n        print(message)\n        return False\n\n\n# End of Package check util\n\nif not is_installed(\"cv2\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install opencv-python\", \"requirements for opencv\")\n\nif not is_installed(\"tensorflow-cpu\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install tensorflow-cpu\", \"requirements for tensorflow\")\n\nif not is_installed(\"onnx\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install onnx\", \"requirements for onnx\")\n\nif not is_installed(\"onnxruntime\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install onnxruntime\", \"requirements for onnxruntime\")\n\nif not is_installed(\"modelscope==1.9.3\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install modelscope==1.9.3\", \"requirements for modelscope\")\n\nif not is_installed(\"einops\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install einops\", \"requirements for diffusers\")\n\nif not is_installed(\"imageio>=2.29.0\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    # The '>' will be interpreted as redirection (in linux) since SD WebUI uses `shell=True` in `subprocess.run`.\n    launch.run_pip(\"install \\\"imageio>=2.29.0\\\"\", \"requirements for imageio\")\n\nif not is_installed(\"av\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install \\\"imageio[pyav]\\\"\", \"requirements for av\")\n\n# Temporarily pin fsspec==2023.9.2. See https://github.com/huggingface/datasets/issues/6330 for details.\nif not is_installed(\"fsspec==2023.9.2\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install fsspec==2023.9.2\", \"requirements for fsspec\")\n\n# `StableDiffusionXLPipeline` in diffusers requires the invisible-watermark library.\nif not launch.is_installed(\"invisible-watermark\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install invisible-watermark\", \"requirements for invisible-watermark\")\n\n# Tryon requires the shapely and segment-anything library.\nif not launch.is_installed(\"shapely\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    launch.run_pip(\"install shapely\", \"requirements for shapely\")\n\nif not launch.is_installed(\"segment_anything\"):\n    try:\n        launch.run_pip(\"install segment-anything\", \"requirements for segment_anything\")\n    except Exception:\n        print(\"Can't install segment-anything. Please follow the readme to install manually\")\n\nif not is_installed(\"diffusers>=0.18.2\"):\n    print(\"Installing requirements for easyphoto-webui\")\n    try:\n        launch.run_pip(\"install diffusers==0.23.0\", \"requirements for diffusers\")\n    except Exception as e:\n        print(f\"Can't install the diffusers==0.23.0. Error info {e}\")\n        launch.run_pip(\"install diffusers==0.18.2\", \"requirements for diffusers\")\n\nif platform.system() != \"Windows\":\n    if not is_installed(\"nvitop\"):\n        print(\"Installing requirements for easyphoto-webui\")\n        launch.run_pip(\"install nvitop==1.3.0\", \"requirements for tensorflow\")\n"
        },
        {
          "name": "javascript",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}