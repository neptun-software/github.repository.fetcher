{
  "metadata": {
    "timestamp": 1736560842446,
    "page": 542,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Hironsan/BossSensor",
      "stars": 6222,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.1494140625,
          "content": "# Created by .ignore support plugin (hsz.mobi)\n### Python template\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n### VirtualEnv template\n# Virtualenv\n# http://iamzed.com/2009/05/07/a-primer-on-virtualenv/\n.Python\n[Bb]in\n[Ii]nclude\n[Ll]ib\n[Ss]cripts\npyvenv.cfg\npip-selfcheck.json\n### JetBrains template\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio\n\n*.iml\n\n## Directory-based project format:\n.idea/\n# if you remove the above rule, at least ignore the following:\n\n# User-specific stuff:\n# .idea/workspace.xml\n# .idea/tasks.xml\n# .idea/dictionaries\n\n# Sensitive or high-churn files:\n# .idea/dataSources.ids\n# .idea/dataSources.xml\n# .idea/sqlDataSources.xml\n# .idea/dynamic.xml\n# .idea/uiDesigner.xml\n\n# Gradle:\n# .idea/gradle.xml\n# .idea/libraries\n\n# Mongo Explorer plugin:\n# .idea/mongoSettings.xml\n\n## File-based project format:\n*.ipr\n*.iws\n\n## Plugin-specific files:\n\n# IntelliJ\n/out/\n\n# mpeltonen/sbt-idea plugin\n.idea_modules/\n\n# JIRA plugin\natlassian-ide-plugin.xml\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\n### OSX template\n.DS_Store\n.AppleDouble\n.LSOverride\n\n# Icon must end with two \\r\nIcon\n\n# Thumbnails\n._*\n\n# Files that might appear in the root of a volume\n.DocumentRevisions-V100\n.fseventsd\n.Spotlight-V100\n.TemporaryItems\n.Trashes\n.VolumeIcon.icns\n\n# Directories potentially created on remote AFP share\n.AppleDB\n.AppleDesktop\nNetwork Trash Folder\nTemporary Items\n.apdisk\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0556640625,
          "content": "The MIT License (MIT)\n\nCopyright (c) 2016 Hiroki Nakayama\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 1.3505859375,
          "content": "# BossSensor\nHide your screen when your boss is approaching.\n\n## Demo\nThe boss stands up. He is approaching.\n\n![standup](https://github.com/Hironsan/BossSensor/blob/master/resource_for_readme/standup.jpg)\n\nWhen he is approaching, the program fetches face images and classifies the image.\n \n![approaching](https://github.com/Hironsan/BossSensor/blob/master/resource_for_readme/approach.jpg)\n\nIf the image is classified as the Boss, it will monitor changes.\n\n![editor](https://github.com/Hironsan/BossSensor/blob/master/resource_for_readme/editor.jpg)\n\n## Requirements\n\n* WebCamera\n* Python3.5\n* OSX\n* Anaconda\n* Lots of images of your boss and other person image\n\nPut images into [data/boss](https://github.com/Hironsan/BossSensor/tree/master/data/boss) and [data/other](https://github.com/Hironsan/BossSensor/tree/master/data/other).\n\n## Usage\nFirst, Train boss image.\n\n```\n$ python boss_train.py\n```\n\n\nSecond, start BossSensor. \n\n```\n$ python camera_reader.py\n```\n\n## Install\nInstall OpenCV, PyQt4, Anaconda.\n\n```\nconda create -n venv python=3.5\nsource activate venv\nconda install -c https://conda.anaconda.org/menpo opencv3\nconda install -c conda-forge tensorflow\npip install -r requirements.txt\n```\n\nChange Keras backend from Theano to TensorFlow. \n\n## Licence\n\n[MIT](https://github.com/Hironsan/BossSensor/blob/master/LICENSE)\n\n## Author\n\n[Hironsan](https://github.com/Hironsan)\n"
        },
        {
          "name": "boss_input.py",
          "type": "blob",
          "size": 1.6689453125,
          "content": "# -*- coding: utf-8 -*-\nimport os\n\nimport numpy as np\nimport cv2\n\nIMAGE_SIZE = 64\n\n\ndef resize_with_pad(image, height=IMAGE_SIZE, width=IMAGE_SIZE):\n\n    def get_padding_size(image):\n        h, w, _ = image.shape\n        longest_edge = max(h, w)\n        top, bottom, left, right = (0, 0, 0, 0)\n        if h < longest_edge:\n            dh = longest_edge - h\n            top = dh // 2\n            bottom = dh - top\n        elif w < longest_edge:\n            dw = longest_edge - w\n            left = dw // 2\n            right = dw - left\n        else:\n            pass\n        return top, bottom, left, right\n\n    top, bottom, left, right = get_padding_size(image)\n    BLACK = [0, 0, 0]\n    constant = cv2.copyMakeBorder(image, top , bottom, left, right, cv2.BORDER_CONSTANT, value=BLACK)\n\n    resized_image = cv2.resize(constant, (height, width))\n\n    return resized_image\n\n\nimages = []\nlabels = []\ndef traverse_dir(path):\n    for file_or_dir in os.listdir(path):\n        abs_path = os.path.abspath(os.path.join(path, file_or_dir))\n        print(abs_path)\n        if os.path.isdir(abs_path):  # dir\n            traverse_dir(abs_path)\n        else:                        # file\n            if file_or_dir.endswith('.jpg'):\n                image = read_image(abs_path)\n                images.append(image)\n                labels.append(path)\n\n    return images, labels\n\n\ndef read_image(file_path):\n    image = cv2.imread(file_path)\n    image = resize_with_pad(image, IMAGE_SIZE, IMAGE_SIZE)\n\n    return image\n\n\ndef extract_data(path):\n    images, labels = traverse_dir(path)\n    images = np.array(images)\n    labels = np.array([0 if label.endswith('boss') else 1 for label in labels])\n\n    return images, labels\n"
        },
        {
          "name": "boss_train.py",
          "type": "blob",
          "size": 7.4599609375,
          "content": "# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport random\n\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils\nfrom keras.models import load_model\nfrom keras import backend as K\n\nfrom boss_input import extract_data, resize_with_pad, IMAGE_SIZE\n\n\nclass Dataset(object):\n\n    def __init__(self):\n        self.X_train = None\n        self.X_valid = None\n        self.X_test = None\n        self.Y_train = None\n        self.Y_valid = None\n        self.Y_test = None\n\n    def read(self, img_rows=IMAGE_SIZE, img_cols=IMAGE_SIZE, img_channels=3, nb_classes=2):\n        images, labels = extract_data('./data/')\n        labels = np.reshape(labels, [-1])\n        # numpy.reshape\n        X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.3, random_state=random.randint(0, 100))\n        X_valid, X_test, y_valid, y_test = train_test_split(images, labels, test_size=0.5, random_state=random.randint(0, 100))\n        if K.image_dim_ordering() == 'th':\n            X_train = X_train.reshape(X_train.shape[0], 3, img_rows, img_cols)\n            X_valid = X_valid.reshape(X_valid.shape[0], 3, img_rows, img_cols)\n            X_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)\n            input_shape = (3, img_rows, img_cols)\n        else:\n            X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n            X_valid = X_valid.reshape(X_valid.shape[0], img_rows, img_cols, 3)\n            X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n            input_shape = (img_rows, img_cols, 3)\n\n        # the data, shuffled and split between train and test sets\n        print('X_train shape:', X_train.shape)\n        print(X_train.shape[0], 'train samples')\n        print(X_valid.shape[0], 'valid samples')\n        print(X_test.shape[0], 'test samples')\n\n        # convert class vectors to binary class matrices\n        Y_train = np_utils.to_categorical(y_train, nb_classes)\n        Y_valid = np_utils.to_categorical(y_valid, nb_classes)\n        Y_test = np_utils.to_categorical(y_test, nb_classes)\n\n        X_train = X_train.astype('float32')\n        X_valid = X_valid.astype('float32')\n        X_test = X_test.astype('float32')\n        X_train /= 255\n        X_valid /= 255\n        X_test /= 255\n\n        self.X_train = X_train\n        self.X_valid = X_valid\n        self.X_test = X_test\n        self.Y_train = Y_train\n        self.Y_valid = Y_valid\n        self.Y_test = Y_test\n\n\nclass Model(object):\n\n    FILE_PATH = './store/model.h5'\n\n    def __init__(self):\n        self.model = None\n\n    def build_model(self, dataset, nb_classes=2):\n        self.model = Sequential()\n\n        self.model.add(Convolution2D(32, 3, 3, border_mode='same', input_shape=dataset.X_train.shape[1:]))\n        self.model.add(Activation('relu'))\n        self.model.add(Convolution2D(32, 3, 3))\n        self.model.add(Activation('relu'))\n        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n        self.model.add(Dropout(0.25))\n\n        self.model.add(Convolution2D(64, 3, 3, border_mode='same'))\n        self.model.add(Activation('relu'))\n        self.model.add(Convolution2D(64, 3, 3))\n        self.model.add(Activation('relu'))\n        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n        self.model.add(Dropout(0.25))\n\n        self.model.add(Flatten())\n        self.model.add(Dense(512))\n        self.model.add(Activation('relu'))\n        self.model.add(Dropout(0.5))\n        self.model.add(Dense(nb_classes))\n        self.model.add(Activation('softmax'))\n\n        self.model.summary()\n\n    def train(self, dataset, batch_size=32, nb_epoch=40, data_augmentation=True):\n        # let's train the model using SGD + momentum (how original).\n        sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n        self.model.compile(loss='categorical_crossentropy',\n                           optimizer=sgd,\n                           metrics=['accuracy'])\n        if not data_augmentation:\n            print('Not using data augmentation.')\n            self.model.fit(dataset.X_train, dataset.Y_train,\n                           batch_size=batch_size,\n                           nb_epoch=nb_epoch,\n                           validation_data=(dataset.X_valid, dataset.Y_valid),\n                           shuffle=True)\n        else:\n            print('Using real-time data augmentation.')\n\n            # this will do preprocessing and realtime data augmentation\n            datagen = ImageDataGenerator(\n                featurewise_center=False,             # set input mean to 0 over the dataset\n                samplewise_center=False,              # set each sample mean to 0\n                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n                samplewise_std_normalization=False,   # divide each input by its std\n                zca_whitening=False,                  # apply ZCA whitening\n                rotation_range=20,                     # randomly rotate images in the range (degrees, 0 to 180)\n                width_shift_range=0.2,                # randomly shift images horizontally (fraction of total width)\n                height_shift_range=0.2,               # randomly shift images vertically (fraction of total height)\n                horizontal_flip=True,                 # randomly flip images\n                vertical_flip=False)                  # randomly flip images\n\n            # compute quantities required for featurewise normalization\n            # (std, mean, and principal components if ZCA whitening is applied)\n            datagen.fit(dataset.X_train)\n\n            # fit the model on the batches generated by datagen.flow()\n            self.model.fit_generator(datagen.flow(dataset.X_train, dataset.Y_train,\n                                                  batch_size=batch_size),\n                                     samples_per_epoch=dataset.X_train.shape[0],\n                                     nb_epoch=nb_epoch,\n                                     validation_data=(dataset.X_valid, dataset.Y_valid))\n\n    def save(self, file_path=FILE_PATH):\n        print('Model Saved.')\n        self.model.save(file_path)\n\n    def load(self, file_path=FILE_PATH):\n        print('Model Loaded.')\n        self.model = load_model(file_path)\n\n    def predict(self, image):\n        if K.image_dim_ordering() == 'th' and image.shape != (1, 3, IMAGE_SIZE, IMAGE_SIZE):\n            image = resize_with_pad(image)\n            image = image.reshape((1, 3, IMAGE_SIZE, IMAGE_SIZE))\n        elif K.image_dim_ordering() == 'tf' and image.shape != (1, IMAGE_SIZE, IMAGE_SIZE, 3):\n            image = resize_with_pad(image)\n            image = image.reshape((1, IMAGE_SIZE, IMAGE_SIZE, 3))\n        image = image.astype('float32')\n        image /= 255\n        result = self.model.predict_proba(image)\n        print(result)\n        result = self.model.predict_classes(image)\n\n        return result[0]\n\n    def evaluate(self, dataset):\n        score = self.model.evaluate(dataset.X_test, dataset.Y_test, verbose=0)\n        print(\"%s: %.2f%%\" % (self.model.metrics_names[1], score[1] * 100))\n\nif __name__ == '__main__':\n    dataset = Dataset()\n    dataset.read()\n\n    model = Model()\n    model.build_model(dataset)\n    model.train(dataset, nb_epoch=10)\n    model.save()\n\n    model = Model()\n    model.load()\n    model.evaluate(dataset)\n"
        },
        {
          "name": "camera_reader.py",
          "type": "blob",
          "size": 1.6806640625,
          "content": "# -*- coding:utf-8 -*-\nimport cv2\n\nfrom boss_train import Model\nfrom image_show import show_image\n\n\nif __name__ == '__main__':\n    cap = cv2.VideoCapture(0)\n    cascade_path = \"/usr/local/opt/opencv/share/OpenCV/haarcascades/haarcascade_frontalface_default.xml\"\n    model = Model()\n    model.load()\n    while True:\n        _, frame = cap.read()\n\n        # グレースケール変換\n        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n        # カスケード分類器の特徴量を取得する\n        cascade = cv2.CascadeClassifier(cascade_path)\n\n        # 物体認識（顔認識）の実行\n        facerect = cascade.detectMultiScale(frame_gray, scaleFactor=1.2, minNeighbors=3, minSize=(10, 10))\n        #facerect = cascade.detectMultiScale(frame_gray, scaleFactor=1.01, minNeighbors=3, minSize=(3, 3))\n        if len(facerect) > 0:\n            print('face detected')\n            color = (255, 255, 255)  # 白\n            for rect in facerect:\n                # 検出した顔を囲む矩形の作成\n                #cv2.rectangle(frame, tuple(rect[0:2]), tuple(rect[0:2] + rect[2:4]), color, thickness=2)\n\n                x, y = rect[0:2]\n                width, height = rect[2:4]\n                image = frame[y - 10: y + height, x: x + width]\n\n                result = model.predict(image)\n                if result == 0:  # boss\n                    print('Boss is approaching')\n                    show_image()\n                else:\n                    print('Not boss')\n\n        #10msecキー入力待ち\n        k = cv2.waitKey(100)\n        #Escキーを押されたら終了\n        if k == 27:\n            break\n\n    #キャプチャを終了\n    cap.release()\n    cv2.destroyAllWindows()\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "image_show.py",
          "type": "blob",
          "size": 0.3330078125,
          "content": "# -*- coding: utf-8 -*-\nimport sys\n\nfrom PyQt4 import QtGui\n\n\ndef show_image(image_path='s_pycharm.jpg'):\n    app = QtGui.QApplication(sys.argv)\n    pixmap = QtGui.QPixmap(image_path)\n    screen = QtGui.QLabel()\n    screen.setPixmap(pixmap)\n    screen.showFullScreen()\n    sys.exit(app.exec_())\n\n\nif __name__ == '__main__':\n    show_image()\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1787109375,
          "content": "h5py==2.8.0\nKeras==2.2.4\nmock==2.0.0\nnumpy==1.15.3\npbr==5.1.0\nprotobuf==3.6.1\nPyYAML==3.13\nscikit-learn==0.20.0\nscipy==1.1.0\nsix==1.11.0\nsklearn==0.0\ntensorflow==1.11.0\nTheano==1.0.3\n"
        },
        {
          "name": "resource_for_readme",
          "type": "tree",
          "content": null
        },
        {
          "name": "s_pycharm.jpg",
          "type": "blob",
          "size": 189.326171875,
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}