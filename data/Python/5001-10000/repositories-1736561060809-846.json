{
  "metadata": {
    "timestamp": 1736561060809,
    "page": 846,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "PyCQA/pycodestyle",
      "stars": 5056,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.025390625,
          "content": "testing/data/E90.py -text\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0732421875,
          "content": "*.egg\n*.egg-info\n*.pyc\n/.coverage*\n/.tox\n/build/\n/dist\n/venv*/\ndocs/_build\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.6826171875,
          "content": "exclude: ^testing/data/\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n    -   id: check-yaml\n    -   id: debug-statements\n    -   id: end-of-file-fixer\n    -   id: trailing-whitespace\n-   repo: https://github.com/asottile/reorder-python-imports\n    rev: v3.14.0\n    hooks:\n    -   id: reorder-python-imports\n        args: [--py39-plus]\n-   repo: https://github.com/asottile/pyupgrade\n    rev: v3.19.1\n    hooks:\n    -   id: pyupgrade\n        args: [--py39-plus]\n-   repo: https://github.com/asottile/setup-cfg-fmt\n    rev: v2.7.0\n    hooks:\n    -   id: setup-cfg-fmt\n-   repo: https://github.com/pycqa/flake8\n    rev: 7.1.1\n    hooks:\n    -   id: flake8\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.064453125,
          "content": "version: 2\n\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.11\"\n"
        },
        {
          "name": "CHANGES.txt",
          "type": "blob",
          "size": 27.48828125,
          "content": "Changelog\n=========\n\n2.12.1 (2024-08-04)\n-------------------\n\nChanges:\n\n* Properly preserve escaped `{` and `}` in fstrings in logical lines in 3.12+.\n  PR #1252.\n\n2.12.0 (2024-06-15)\n-------------------\n\nChanges:\n\n* E721: Fix false positive of the form `x.type(...) ==`.  PR #1228.\n* E502: Fix false-negative with a backslash escape in a comment.  PR #1234.\n* E204: New lint forbidding whitespace after decorator `@`.  PR #1247.\n\n2.11.1 (2023-10-12)\n-------------------\n\nChanges:\n\n* E275: fix false positive with fstrings containing keyword parts in python 3.12\n\n2.11.0 (2023-07-29)\n-------------------\n\nChanges:\n\n* Drop EOL python 3.6 / 3.7.  PR #1129, #1160.\n* Add support for python 3.12.  PR #1147, #1148, #1152, #1153, #1154, #1163,\n  #1164, #1165, #1166, #1176, #1177, #1182.\n* E721: adjust handling of type comparison.  Allowed forms are now\n  ``isinstance(x, t)`` or ``type(x) is t``.  PR #1086, #1167.\n* Remove handling of python 2 ``<>`` operator.  PR #1161.\n* W606: removed.  ``async`` / ``await`` are always keywords.  PR #1162.\n* Internal: move tests to pytest.  PR #1168, #1169, #1171, #1173, #1174, #1175.\n* Remove handling of python 2 ``ur''`` strings.  PR #1181.\n\n\n2.10.0 (2022-11-23)\n-------------------\n\nChanges:\n\n* E231: allow trailing comma inside 1-tuples in ``[]``.  PR #1108.\n* W601, W602, W603, W604: removed (no longer relevant in python 3).  PR #1111.\n* E741: also apply to lambdas.  PR #1106.\n* E741: fix false positive for comparison operators.  PR #1118.\n\n2.9.1 (2022-08-03)\n------------------\n\nChanges:\n\n* E275: fix false positive for yield expressions.  PR #1091.\n\n2.9.0 (2022-07-30)\n------------------\n\nChanges:\n\n* E221, E222, E223, E224: add support for ``:=`` operator.  PR #1032.\n* Drop python 2.7 / 3.5.\n* E262: consider non-breaking spaces (``\\xa0``) as whitespace.  PR #1035.\n* Improve performance of ``_is_binary_operator``.  PR #1052.\n* E275: requires whitespace around keywords.  PR #1063.\n* Add support for python 3.11. PR #1070.\n\n2.8.0 (2021-10-10)\n------------------\n\nChanges:\n\n* Drop python 3.4.  PR #982.\n* E712: fix false negative with multiple comparisons.  PR #987.\n* E211: fix false positives with ``match``.  PR #989.\n* E772: improve performance of bare except check.  PR #992.\n* Backport tokenize performance improvement from python 3.10.  PR #993.\n* E225: fix for lambdas containing positional-only args.  PR #1012.\n* Remove ``indent_size_str`` \"setting\".  PR #995.\n* E402: allow ``__all__`` to be typed.  PR #1019.\n* E225: fix false positives for ``*`` in ``case``.  PR #1003.\n* E201: detect tabs as whitespace.  PR #1015.\n\n\n2.7.0 (2021-03-14)\n------------------\n\nChanges:\n\n* Fix physical checks (such as W191) at end of file.  PR #961.\n* Add ``--indent-size`` option (defaulting to ``4``).  PR #970.\n* W605: fix escaped crlf false positive on windows.  PR #976.\n\n\n2.6.0 (2020-05-11)\n------------------\n\nAnnouncements:\n\n* Anthony Sottile (@asottile) joined the team as a core developer. :tada:\n\nChanges:\n\n* E306: fix detection inside ``async def``.  PR #929.\n* E301: fix regression disallowing decorated one-liners.  PR #927.\n* E714: fix false positive with chained ``is not``.  PR #931.\n\n\n2.6.0a1 (2020-04-23)\n--------------------\n\nNew checks:\n\n* E225: require whitespace around ``and`` ``in`` ``is`` and ``or``.  PR #847.\n\nChanges:\n\n* E117: fix indentation using tabs by treating as 8-space indents.  PR #837.\n* E721: fix false positive with names containg ``istype``.  PR #850.\n* E741: allow ``l`` as a named argument in a function call.  PR #853.\n* E302: fix false-negative with decorated functions.  PR #859.\n* W504: ellipsis (``...``) is no longer treated as a binary operator.  PR #875.\n* E402: allow ``with``, ``if``, ``elif``, ``else`` to guard imports.  PR #834.\n* Add support for assignment expressions ``:=`` (PEP 572).  PR #879.\n* Add support for positional-only arguments ``/`` (PEP 570).  PR #872, #918.\n* Add support for python 3.8.\n* Add support for matrix multiplication operator ``@`` (PEP 465).  PR #897.\n* Support visual indent for continuation lines for ``with`` / ``assert`` /\n  ``raise``.  PR #912.\n* E302: allow two blank lines after a block of one-liners.  PR #913.\n* E302: allow two-and-fewer newlines at the top of the file.  PR #919.\n\n\n2.5.0 (2019-01-29)\n------------------\n\nNew checks:\n\n* E117: Over-indented code blocks\n* W505: Maximum doc-string length only when configured with --max-doc-length\n\nChanges:\n\n* Remove support for EOL Python 2.6 and 3.3. PR #720.\n* Add E117 error for over-indented code blocks.\n* Allow W605 to be silenced by `# noqa` and fix the position reported by W605\n* Allow users to omit blank lines around one-liner definitions of classes and\n  functions\n* Include the function return annotation (``->``) as requiring surrounding\n  whitespace only on Python 3\n* Verify that only names can follow ``await``. Previously we allowed numbers\n  and strings.\n* Add support for Python 3.7\n* Fix detection of annotated argument defaults for E252\n* Correct the position reported by W504\n\n\n2.4.0 (2018-04-10)\n------------------\n\nNew checks:\n\n* Add W504 warning for checking that a break doesn't happen after a binary\n  operator. This check is ignored by default. PR #502.\n* Add W605 warning for invalid escape sequences in string literals. PR #676.\n* Add W606 warning for 'async' and 'await' reserved keywords being introduced\n  in Python 3.7. PR #684.\n* Add E252 error for missing whitespace around equal sign in type annotated\n  function arguments with defaults values. PR #717.\n\nChanges:\n\n* An internal bisect search has replaced a linear search in order to improve\n  efficiency. PR #648.\n* pycodestyle now uses PyPI trove classifiers in order to document supported\n  python versions on PyPI. PR #654.\n* 'setup.cfg' '[wheel]' section has been renamed to '[bdist_wheel]', as\n  the former is legacy. PR #653.\n* pycodestyle now handles very long lines much more efficiently for python\n  3.2+. Fixes #643. PR #644.\n* You can now write 'pycodestyle.StyleGuide(verbose=True)' instead of\n  'pycodestyle.StyleGuide(verbose=True, paths=['-v'])' in order to achieve\n  verbosity. PR #663.\n* The distribution of pycodestyle now includes the license text in order to\n  comply with open source licenses which require this. PR #694.\n* 'maximum_line_length' now ignores shebang ('#!') lines. PR #736.\n* Add configuration option for the allowed number of blank lines. It is\n  implemented as a top level dictionary which can be easily overwritten. Fixes\n  #732. PR #733.\n\nBugs:\n\n* Prevent a 'DeprecationWarning', and a 'SyntaxError' in future python, caused\n  by an invalid escape sequence. PR #625.\n* Correctly report E501 when the first line of a docstring is too long.\n  Resolves #622. PR #630.\n* Support variable annotation when variable start by a keyword, such as class\n  variable type annotations in python 3.6. PR #640.\n* pycodestyle internals have been changed in order to allow 'python3 -m\n  cProfile' to report correct metrics. PR #647.\n* Fix a spelling mistake in the description of E722. PR #697.\n* 'pycodestyle --diff' now does not break if your 'gitconfig' enables\n  'mnemonicprefix'. PR #706.\n\n2.3.1 (2017-01-31)\n------------------\n\nBugs:\n\n* Fix regression in detection of E302 and E306; #618, #620\n\n2.3.0 (2017-01-30)\n------------------\n\nNew Checks:\n\n* Add E722 warning for bare ``except`` clauses\n* Report E704 for async function definitions (``async def``)\n\nBugs:\n\n* Fix another E305 false positive for variables beginning with \"class\" or\n  \"def\"\n* Fix detection of multiple spaces between ``async`` and ``def``\n* Fix handling of variable annotations. Stop reporting E701 on Python 3.6 for\n  variable annotations.\n\n2.2.0 (2016-11-14)\n------------------\n\nAnnouncements:\n\n* Added Make target to obtain proper tarball file permissions; #599\n\nBugs:\n\n* Fixed E305 regression caused by #400; #593\n\n2.1.0 (2016-11-04)\n------------------\n\nAnnouncements:\n\n* Change all references to the pep8 project to say pycodestyle; #530\n\nChanges:\n\n* Report E302 for blank lines before an \"async def\"; #556\n* Update our list of tested and supported Python versions which are 2.6, 2.7,\n  3.2, 3.3, 3.4 and 3.5 as well as the nightly Python build and PyPy.\n* Report E742 and E743 for functions and classes badly named 'l', 'O', or 'I'.\n* Report E741 on 'global' and 'nonlocal' statements, as well as prohibited\n  single-letter variables.\n* Deprecated use of `[pep8]` section name in favor of `[pycodestyle]`; #591\n* Report E722 when bare except clause is used; #579\n\nBugs:\n\n* Fix opt_type AssertionError when using Flake8 2.6.2 and pycodestyle; #561\n* Require two blank lines after toplevel def, class; #536\n* Remove accidentally quadratic computation based on the number of colons. This\n  will make pycodestyle faster in some cases; #314\n\n2.0.0 (2016-05-31)\n------------------\n\nAnnouncements:\n\n* Repository renamed to `pycodestyle`; Issue #466 / #481.\n* Added joint Code of Conduct as member of PyCQA; #483\n\nChanges:\n\n* Added tox test support for Python 3.5 and pypy3\n* Added check E275 for whitespace on `from ... import ...` lines; #489 / #491\n* Added W503 to the list of codes ignored by default ignore list; #498\n* Removed use of project level `.pep8` configuration file; #364\n\nBugs:\n\n* Fixed bug with treating `~` operator as binary; #383 / #384\n* Identify binary operators as unary; #484 / #485\n\n1.7.0 (2016-01-12)\n------------------\n\nAnnouncements:\n\n* Repository moved to PyCQA Organization on GitHub:\n  https://github.com/pycqa/pep8\n\nChanges:\n\n* Reverted the fix in #368, \"options passed on command line are only ones\n  accepted\" feature. This has many unintended consequences in pep8 and flake8\n  and needs to be reworked when I have more time.\n* Added support for Python 3.5. (Issue #420 & #459)\n* Added support for multi-line config_file option parsing. (Issue #429)\n* Improved parameter parsing. (Issues #420 & #456)\n\nBugs:\n\n* Fixed BytesWarning on Python 3. (Issue #459)\n\n1.6.2 (2015-02-15)\n------------------\n\nChanges:\n\n* Added check for breaking around a binary operator. (Issue #197, Pull #305)\n\nBugs:\n\n* Restored config_file parameter in process_options(). (Issue #380)\n\n\n1.6.1 (2015-02-08)\n------------------\n\nChanges:\n\n* Assign variables before referenced. (Issue #287)\n\nBugs:\n\n* Exception thrown due to unassigned ``local_dir`` variable. (Issue #377)\n\n\n1.6.0 (2015-02-06)\n------------------\n\nNews:\n\n* Ian Lee <ianlee1521@gmail.com> joined the project as a maintainer.\n\nChanges:\n\n* Report E731 for lambda assignment. (Issue #277)\n\n* Report E704 for one-liner def instead of E701.\n  Do not report this error in the default configuration. (Issue #277)\n\n* Replace codes E111, E112 and E113 with codes E114, E115 and E116\n  for bad indentation of comments. (Issue #274)\n\n* Report E266 instead of E265 when the block comment starts with\n  multiple ``#``. (Issue #270)\n\n* Report E402 for import statements not at the top of the file. (Issue #264)\n\n* Do not enforce whitespaces around ``**`` operator. (Issue #292)\n\n* Strip whitespace from around paths during normalization. (Issue #339 / #343)\n\n* Update ``--format`` documentation. (Issue #198 / Pull Request #310)\n\n* Add ``.tox/`` to default excludes. (Issue #335)\n\n* Do not report E121 or E126 in the default configuration. (Issues #256 / #316)\n\n* Allow spaces around the equals sign in an annotated function. (Issue #357)\n\n* Allow trailing backslash if in an inline comment. (Issue #374)\n\n* If ``--config`` is used, only that configuration is processed. Otherwise,\n  merge the user and local configurations are merged. (Issue #368 / #369)\n\nBug fixes:\n\n* Don't crash if Checker.build_tokens_line() returns None. (Issue #306)\n\n* Don't crash if os.path.expanduser() throws an ImportError. (Issue #297)\n\n* Missing space around keyword parameter equal not always reported, E251.\n  (Issue #323)\n\n* Fix false positive E711/E712/E713. (Issues #330 and #336)\n\n* Do not skip physical checks if the newline is escaped. (Issue #319)\n\n* Flush sys.stdout to avoid race conditions with printing. See flake8 bug:\n  https://gitlab.com/pycqa/flake8/issues/17 for more details. (Issue #363)\n\n\n1.5.7 (2014-05-29)\n------------------\n\nBug fixes:\n\n* Skip the traceback on \"Broken pipe\" signal. (Issue #275)\n\n* Do not exit when an option in ``setup.cfg`` or ``tox.ini``\n  is not recognized.\n\n* Check the last line even if it does not end with a newline. (Issue #286)\n\n* Always open files in universal newlines mode in Python 2. (Issue #288)\n\n\n1.5.6 (2014-04-14)\n------------------\n\nBug fixes:\n\n* Check the last line even if it has no end-of-line. (Issue #273)\n\n\n1.5.5 (2014-04-10)\n------------------\n\nBug fixes:\n\n* Fix regression with E22 checks and inline comments. (Issue #271)\n\n\n1.5.4 (2014-04-07)\n------------------\n\nBug fixes:\n\n* Fix negative offset with E303 before a multi-line docstring.\n  (Issue #269)\n\n\n1.5.3 (2014-04-04)\n------------------\n\nBug fixes:\n\n* Fix wrong offset computation when error is on the last char\n  of a physical line. (Issue #268)\n\n\n1.5.2 (2014-04-04)\n------------------\n\nChanges:\n\n* Distribute a universal wheel file.\n\nBug fixes:\n\n* Report correct line number for E303 with comments. (Issue #60)\n\n* Do not allow newline after parameter equal. (Issue #252)\n\n* Fix line number reported for multi-line strings. (Issue #220)\n\n* Fix false positive E121/E126 with multi-line strings. (Issue #265)\n\n* Fix E501 not detected in comments with Python 2.5.\n\n* Fix caret position with ``--show-source`` when line contains tabs.\n\n\n1.5.1 (2014-03-27)\n------------------\n\nBug fixes:\n\n* Fix a crash with E125 on multi-line strings. (Issue #263)\n\n\n1.5 (2014-03-26)\n----------------\n\nChanges:\n\n* Report E129 instead of E125 for visually indented line with same\n  indent as next logical line.  (Issue #126)\n\n* Report E265 for space before block comment. (Issue #190)\n\n* Report E713 and E714 when operators ``not in`` and ``is not`` are\n  recommended. (Issue #236)\n\n* Allow long lines in multiline strings and comments if they cannot\n  be wrapped. (Issue #224).\n\n* Optionally disable physical line checks inside multiline strings,\n  using ``# noqa``. (Issue #242)\n\n* Change text for E121 to report \"continuation line under-indented\n  for hanging indent\" instead of indentation not being a\n  multiple of 4.\n\n* Report E131 instead of E121 / E126 if the hanging indent is not\n  consistent within the same continuation block.  It helps when\n  error E121 or E126 is in the ``ignore`` list.\n\n* Report E126 instead of E121 when the continuation line is hanging\n  with extra indentation, even if indentation is not a multiple of 4.\n\nBug fixes:\n\n* Allow the checkers to report errors on empty files. (Issue #240)\n\n* Fix ignoring too many checks when ``--select`` is used with codes\n  declared in a flake8 extension. (Issue #216)\n\n* Fix regression with multiple brackets. (Issue #214)\n\n* Fix ``StyleGuide`` to parse the local configuration if the\n  keyword argument ``paths`` is specified. (Issue #246)\n\n* Fix a false positive E124 for hanging indent. (Issue #254)\n\n* Fix a false positive E126 with embedded colon. (Issue #144)\n\n* Fix a false positive E126 when indenting with tabs. (Issue #204)\n\n* Fix behaviour when ``exclude`` is in the configuration file and\n  the current directory is not the project directory. (Issue #247)\n\n* The logical checks can return ``None`` instead of an empty iterator.\n  (Issue #250)\n\n* Do not report multiple E101 if only the first indentation starts\n  with a tab. (Issue #237)\n\n* Fix a rare false positive W602. (Issue #34)\n\n\n1.4.6 (2013-07-02)\n------------------\n\nChanges:\n\n* Honor ``# noqa`` for errors E711 and E712. (Issue #180)\n\n* When both a ``tox.ini`` and a ``setup.cfg`` are present in the project\n  directory, merge their contents.  The ``tox.ini`` file takes\n  precedence (same as before). (Issue #182)\n\n* Give priority to ``--select`` over ``--ignore``. (Issue #188)\n\n* Compare full path when excluding a file. (Issue #186)\n\n* New option ``--hang-closing`` to switch to the alternative style of\n  closing bracket indentation for hanging indent.  Add error E133 for\n  closing bracket which is missing indentation. (Issue #103)\n\n* Accept both styles of closing bracket indentation for hanging indent.\n  Do not report error E123 in the default configuration. (Issue #103)\n\nBug fixes:\n\n* Do not crash when running AST checks and the document contains null bytes.\n  (Issue #184)\n\n* Correctly report other E12 errors when E123 is ignored. (Issue #103)\n\n* Fix false positive E261/E262 when the file contains a BOM. (Issue #193)\n\n* Fix E701, E702 and E703 not detected sometimes. (Issue #196)\n\n* Fix E122 not detected in some cases. (Issue #201 and #208)\n\n* Fix false positive E121 with multiple brackets. (Issue #203)\n\n\n1.4.5 (2013-03-06)\n------------------\n\n* When no path is specified, do not try to read from stdin.  The feature\n  was added in 1.4.3, but it is not supported on Windows.  Use ``-``\n  filename argument to read from stdin.  This usage is supported\n  since 1.3.4. (Issue #170)\n\n* Do not require ``setuptools`` in setup.py.  It works around an issue\n  with ``pip`` and Python 3. (Issue #172)\n\n* Add ``__pycache__`` to the ignore list.\n\n* Change misleading message for E251. (Issue #171)\n\n* Do not report false E302 when the source file has a coding cookie or a\n  comment on the first line. (Issue #174)\n\n* Reorganize the tests and add tests for the API and for the command line\n  usage and options. (Issues #161 and #162)\n\n* Ignore all checks which are not explicitly selected when ``select`` is\n  passed to the ``StyleGuide`` constructor.\n\n\n1.4.4 (2013-02-24)\n------------------\n\n* Report E227 or E228 instead of E225 for whitespace around bitwise, shift\n  or modulo operators. (Issue #166)\n\n* Change the message for E226 to make clear that it is about arithmetic\n  operators.\n\n* Fix a false positive E128 for continuation line indentation with tabs.\n\n* Fix regression with the ``--diff`` option. (Issue #169)\n\n* Fix the ``TestReport`` class to print the unexpected warnings and\n  errors.\n\n\n1.4.3 (2013-02-22)\n------------------\n\n* Hide the ``--doctest`` and ``--testsuite`` options when installed.\n\n* Fix crash with AST checkers when the syntax is invalid. (Issue #160)\n\n* Read from standard input if no path is specified.\n\n* Initiate a graceful shutdown on ``Control+C``.\n\n* Allow changing the ``checker_class`` for the ``StyleGuide``.\n\n\n1.4.2 (2013-02-10)\n------------------\n\n* Support AST checkers provided by third-party applications.\n\n* Register new checkers with ``register_check(func_or_cls, codes)``.\n\n* Allow constructing a ``StyleGuide`` with a custom parser.\n\n* Accept visual indentation without parenthesis after the ``if``\n  statement. (Issue #151)\n\n* Fix UnboundLocalError when using ``# noqa`` with continued lines.\n  (Issue #158)\n\n* Re-order the lines for the ``StandardReport``.\n\n* Expand tabs when checking E12 continuation lines. (Issue #155)\n\n* Refactor the testing class ``TestReport`` and the specific test\n  functions into a separate test module.\n\n\n1.4.1 (2013-01-18)\n------------------\n\n* Allow sphinx.ext.autodoc syntax for comments. (Issue #110)\n\n* Report E703 instead of E702 for the trailing semicolon. (Issue #117)\n\n* Honor ``# noqa`` in addition to ``# nopep8``. (Issue #149)\n\n* Expose the ``OptionParser`` factory for better extensibility.\n\n\n1.4 (2012-12-22)\n----------------\n\n* Report E226 instead of E225 for optional whitespace around common\n  operators (``*``, ``**``, ``/``, ``+`` and ``-``).  This new error\n  code is ignored in the default configuration because PEP 8 recommends\n  to \"use your own judgement\". (Issue #96)\n\n* Lines with a ``# nopep8`` at the end will not issue errors on line\n  length E501 or continuation line indentation E12*. (Issue #27)\n\n* Fix AssertionError when the source file contains an invalid line\n  ending ``\"\\r\\r\\n\"``. (Issue #119)\n\n* Read the ``[pep8]`` section of ``tox.ini`` or ``setup.cfg`` if present.\n  (Issue #93 and #141)\n\n* Add the Sphinx-based documentation, and publish it\n  on https://pycodestyle.readthedocs.io/. (Issue #105)\n\n\n1.3.4 (2012-12-18)\n------------------\n\n* Fix false positive E124 and E128 with comments. (Issue #100)\n\n* Fix error on stdin when running with bpython. (Issue #101)\n\n* Fix false positive E401. (Issue #104)\n\n* Report E231 for nested dictionary in list. (Issue #142)\n\n* Catch E271 at the beginning of the line. (Issue #133)\n\n* Fix false positive E126 for multi-line comments. (Issue #138)\n\n* Fix false positive E221 when operator is preceded by a comma. (Issue #135)\n\n* Fix ``--diff`` failing on one-line hunk. (Issue #137)\n\n* Fix the ``--exclude`` switch for directory paths. (Issue #111)\n\n* Use ``-`` filename to read from standard input. (Issue #128)\n\n\n1.3.3 (2012-06-27)\n------------------\n\n* Fix regression with continuation line checker. (Issue #98)\n\n\n1.3.2 (2012-06-26)\n------------------\n\n* Revert to the previous behaviour for ``--show-pep8``:\n  do not imply ``--first``. (Issue #89)\n\n* Add E902 for IO errors. (Issue #87)\n\n* Fix false positive for E121, and missed E124. (Issue #92)\n\n* Set a sensible default path for config file on Windows. (Issue #95)\n\n* Allow ``verbose`` in the configuration file. (Issue #91)\n\n* Show the enforced ``max-line-length`` in the error message. (Issue #86)\n\n\n1.3.1 (2012-06-18)\n------------------\n\n* Explain which configuration options are expected.  Accept and recommend\n  the options names with hyphen instead of underscore. (Issue #82)\n\n* Do not read the user configuration when used as a module\n  (except if ``config_file=True`` is passed to the ``StyleGuide`` constructor).\n\n* Fix wrong or missing cases for the E12 series.\n\n* Fix cases where E122 was missed. (Issue #81)\n\n\n1.3 (2012-06-15)\n----------------\n\n.. warning::\n   The internal API is backwards incompatible.\n\n* Remove global configuration and refactor the library around\n  a ``StyleGuide`` class; add the ability to configure various\n  reporters. (Issue #35 and #66)\n\n* Read user configuration from ``~/.config/pep8``\n  and local configuration from ``./.pep8``. (Issue #22)\n\n* Fix E502 for backslash embedded in multi-line string. (Issue #68)\n\n* Fix E225 for Python 3 iterable unpacking (PEP 3132). (Issue #72)\n\n* Enable the new checkers from the E12 series in the default\n  configuration.\n\n* Suggest less error-prone alternatives for E712 errors.\n\n* Rewrite checkers to run faster (E22, E251, E27).\n\n* Fixed a crash when parsed code is invalid (too many\n  closing brackets).\n\n* Fix E127 and E128 for continuation line indentation. (Issue #74)\n\n* New option ``--format`` to customize the error format. (Issue #23)\n\n* New option ``--diff`` to check only modified code.  The unified\n  diff is read from STDIN.  Example: ``hg diff | pep8 --diff``\n  (Issue #39)\n\n* Correctly report the count of failures and set the exit code to 1\n  when the ``--doctest`` or the ``--testsuite`` fails.\n\n* Correctly detect the encoding in Python 3. (Issue #69)\n\n* Drop support for Python 2.3, 2.4 and 3.0. (Issue #78)\n\n\n1.2 (2012-06-01)\n----------------\n\n* Add E121 through E128 for continuation line indentation.  These\n  checks are disabled by default.  If you want to force all checks,\n  use switch ``--select=E,W``.  Patch by Sam Vilain. (Issue #64)\n\n* Add E721 for direct type comparisons. (Issue #47)\n\n* Add E711 and E712 for comparisons to singletons. (Issue #46)\n\n* Fix spurious E225 and E701 for function annotations. (Issue #29)\n\n* Add E502 for explicit line join between brackets.\n\n* Fix E901 when printing source with ``--show-source``.\n\n* Report all errors for each checker, instead of reporting only the\n  first occurrence for each line.\n\n* Option ``--show-pep8`` implies ``--first``.\n\n\n1.1 (2012-05-24)\n----------------\n\n* Add E901 for syntax errors. (Issues #63 and #30)\n\n* Add E271, E272, E273 and E274 for extraneous whitespace around\n  keywords. (Issue #57)\n\n* Add ``tox.ini`` configuration file for tests. (Issue #61)\n\n* Add ``.travis.yml`` configuration file for continuous integration.\n  (Issue #62)\n\n\n1.0.1 (2012-04-06)\n------------------\n\n* Fix inconsistent version numbers.\n\n\n1.0 (2012-04-04)\n----------------\n\n* Fix W602 ``raise`` to handle multi-char names. (Issue #53)\n\n\n0.7.0 (2012-03-26)\n------------------\n\n* Now ``--first`` prints only the first occurrence of each error.\n  The ``--repeat`` flag becomes obsolete because it is the default\n  behaviour. (Issue #6)\n\n* Allow specifying ``--max-line-length``. (Issue #36)\n\n* Make the shebang more flexible. (Issue #26)\n\n* Add testsuite to the bundle. (Issue #25)\n\n* Fixes for Jython. (Issue #49)\n\n* Add PyPI classifiers. (Issue #43)\n\n* Fix the ``--exclude`` option. (Issue #48)\n\n* Fix W602, accept ``raise`` with 3 arguments. (Issue #34)\n\n* Correctly select all tests if ``DEFAULT_IGNORE == ''``.\n\n\n0.6.1 (2010-10-03)\n------------------\n\n* Fix inconsistent version numbers. (Issue #21)\n\n\n0.6.0 (2010-09-19)\n------------------\n\n* Test suite reorganized and enhanced in order to check more failures\n  with fewer test files.  Read the ``run_tests`` docstring for details\n  about the syntax.\n\n* Fix E225: accept ``print >>sys.stderr, \"...\"`` syntax.\n\n* Fix E501 for lines containing multibyte encoded characters. (Issue #7)\n\n* Fix E221, E222, E223, E224 not detected in some cases. (Issue #16)\n\n* Fix E211 to reject ``v = dic['a'] ['b']``. (Issue #17)\n\n* Exit code is always 1 if any error or warning is found. (Issue #10)\n\n* ``--ignore`` checks are now really ignored, especially in\n  conjunction with ``--count``. (Issue #8)\n\n* Blank lines with spaces yield W293 instead of W291: some developers\n  want to ignore this warning and indent the blank lines to paste their\n  code easily in the Python interpreter.\n\n* Fix E301: do not require a blank line before an indented block. (Issue #14)\n\n* Fix E203 to accept NumPy slice notation ``a[0, :]``. (Issue #13)\n\n* Performance improvements.\n\n* Fix decoding and checking non-UTF8 files in Python 3.\n\n* Fix E225: reject ``True+False`` when running on Python 3.\n\n* Fix an exception when the line starts with an operator.\n\n* Allow a new line before closing ``)``, ``}`` or ``]``. (Issue #5)\n\n\n0.5.0 (2010-02-17)\n------------------\n\n* Changed the ``--count`` switch to print to sys.stderr and set\n  exit code to 1 if any error or warning is found.\n\n* E241 and E242 are removed from the standard checks. If you want to\n  include these checks, use switch ``--select=E,W``. (Issue #4)\n\n* Blank line is not mandatory before the first class method or nested\n  function definition, even if there's a docstring. (Issue #1)\n\n* Add the switch ``--version``.\n\n* Fix decoding errors with Python 3. (Issue #13 [1]_)\n\n* Add ``--select`` option which is mirror of ``--ignore``.\n\n* Add checks E261 and E262 for spaces before inline comments.\n\n* New check W604 warns about deprecated usage of backticks.\n\n* New check W603 warns about the deprecated operator ``<>``.\n\n* Performance improvement, due to rewriting of E225.\n\n* E225 now accepts:\n\n  - no whitespace after unary operator or similar. (Issue #9 [1]_)\n\n  - lambda function with argument unpacking or keyword defaults.\n\n* Reserve \"2 blank lines\" for module-level logical blocks. (E303)\n\n* Allow multi-line comments. (E302, issue #10 [1]_)\n\n\n0.4.2 (2009-10-22)\n------------------\n\n* Decorators on classes and class methods are OK now.\n\n\n0.4 (2009-10-20)\n----------------\n\n* Support for all versions of Python from 2.3 to 3.1.\n\n* New and greatly expanded self tests.\n\n* Added ``--count`` option to print the total number of errors and warnings.\n\n* Further improvements to the handling of comments and blank lines.\n  (Issue #1 [1]_ and others changes.)\n\n* Check all py files in directory when passed a directory (Issue\n  #2 [1]_). This also prevents an exception when traversing directories\n  with non ``*.py`` files.\n\n* E231 should allow commas to be followed by ``)``. (Issue #3 [1]_)\n\n* Spaces are no longer required around the equals sign for keyword\n  arguments or default parameter values.\n\n\n.. [1] These issues refer to the `previous issue tracker`__.\n.. __:  http://github.com/cburroughs/pep8.py/issues\n\n\n0.3.1 (2009-09-14)\n------------------\n\n* Fixes for comments: do not count them when checking for blank lines between\n  items.\n\n* Added setup.py for pypi upload and easy_installability.\n\n\n0.2 (2007-10-16)\n----------------\n\n* Loads of fixes and improvements.\n\n\n0.1 (2006-10-01)\n----------------\n\n* First release.\n"
        },
        {
          "name": "CONTRIBUTING.rst",
          "type": "blob",
          "size": 2.796875,
          "content": "Contributing to pycodestyle\n===========================\n\nWhen contributing to pycodestyle, please observe our `Code of Conduct`_.\n\nStep 1: Forking pycodestyle for editing\n---------------------------------------\n\nFork the pycodestyle repository on GitHub. This will add\npycodestyle to your GitHub account. You will push your changes to your\nfork and then make pull requests into the official pycodestyle repository.\n\nGitHub has an excellent `guide`_ that has screenshots on how to do this.\n\nNext, clone your fork of the pycodestyle repository to your system for\nediting::\n\n    $ git clone https://www.github.com/<your_username>/pycodestyle\n\nNow you have a copy of the pycodestyle codebase that is almost ready for\nedits.  Next we will setup `virtualenv`_ which will help create an isolated\nenvironment to manage dependencies.\n\n\nStep 2: Use virtualenv when developing\n--------------------------------------\n\n`virtualenv`_ is a tool to create isolated python environments.\nFirst, install virtualenv with::\n\n    $ pip install virtualenv\n\nNext, ``cd`` to the pycodestyle repository that you cloned earlier and\ncreate, then activate a virtualenv::\n\n    $ cd pycodestyle\n    $ virtualenv venv-pycodestyle\n    $ source venv-pycodestyle/bin/activate\n\nNote that ``venv*/`` is ignored via ``.gitignore``.\n\nNow you can install the pycodestyle requirements::\n\n    $ pip install -r dev-requirements.txt\n\nTo deactivate the virtualenv you can type::\n\n    $ deactivate\n\nFor more information see `virtualenv`_'s documentation.\n\n\nStep 3: Run tests\n-----------------\n\nBefore creating a pull request you should run the tests to make sure that the\nchanges that have been made haven't caused any regressions in functionality.\nTo run the tests, the core developer team and GitHub Actions use `tox`_::\n\n    $ pip install -r dev-requirements.txt\n    $ tox\n\nAll the tests should pass for all available interpreters, with the summary of::\n\n    congratulations :)\n\nAt this point you can create a pull request back to the official pycodestyle\nrepository for review! For more information on how to make a pull request,\nGitHub has an excellent `guide`_.\n\nThe current tests are written in 2 styles:\n\n* pytest tests\n* functional test using a custom framework\n\n\nRunning tests\n~~~~~~~~~~~~~\n\nThe tests are written using ``pytest``, the existing tests\ninclude unit, integration and functional tests.\n\nTo run the tests::\n\n    $ pytest tests\n\nRunning functional\n~~~~~~~~~~~~~~~~~~\n\n    $ pip install -e .\n    $ # Run all tests.\n    $ pytest tests/test_data.py\n    $ # Run a subset of the tests.\n    $ pytest tests/tests_data.py -k testing/data/E30.py\n\n\n.. _virtualenv: http://docs.python-guide.org/en/latest/dev/virtualenvs/\n.. _guide: https://guides.github.com/activities/forking/\n.. _tox: https://tox.readthedocs.io/en/latest/\n.. _Code of Conduct: http://meta.pycqa.org/en/latest/code-of-conduct.html\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.224609375,
          "content": "Copyright © 2006-2009 Johann C. Rocholl <johann@rocholl.net>\nCopyright © 2009-2014 Florent Xicluna <florent.xicluna@gmail.com>\nCopyright © 2014-2020 Ian Lee <IanLee1521@gmail.com>\n\nLicensed under the terms of the Expat License\n\nPermission is hereby granted, free of charge, to any person\nobtaining a copy of this software and associated documentation files\n(the \"Software\"), to deal in the Software without restriction,\nincluding without limitation the rights to use, copy, modify, merge,\npublish, distribute, sublicense, and/or sell copies of the Software,\nand to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\nBE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\nACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.1083984375,
          "content": "release:\n\tumask 022 && chmod -R a+rX . && python setup.py sdist bdist_wheel\n\t# twine upload dist/*\n\ntest:\n\ttox\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 3.380859375,
          "content": "pycodestyle (formerly called pep8) - Python style guide checker\n===============================================================\n\n.. image:: https://github.com/PyCQA/pycodestyle/actions/workflows/main.yml/badge.svg\n   :target: https://github.com/PyCQA/pycodestyle/actions/workflows/main.yml\n   :alt: Build status\n\n.. image:: https://readthedocs.org/projects/pycodestyle/badge/?version=latest\n    :target: https://pycodestyle.pycqa.org\n    :alt: Documentation Status\n\n.. image:: https://img.shields.io/pypi/wheel/pycodestyle.svg\n   :target: https://pypi.org/project/pycodestyle/\n   :alt: Wheel Status\n\n.. image:: https://badges.gitter.im/PyCQA/pycodestyle.svg\n   :alt: Join the chat at https://gitter.im/PyCQA/pycodestyle\n   :target: https://gitter.im/PyCQA/pycodestyle?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge\n\npycodestyle is a tool to check your Python code against some of the style\nconventions in `PEP 8`_.\n\n.. _PEP 8: http://www.python.org/dev/peps/pep-0008/\n\n.. note::\n\n    This package used to be called ``pep8`` but was renamed to ``pycodestyle``\n    to reduce confusion. Further discussion can be found `in the issue where\n    Guido requested this\n    change <https://github.com/PyCQA/pycodestyle/issues/466>`_, or in the\n    lightning talk at PyCon 2016 by @IanLee1521:\n    `slides <https://speakerdeck.com/ianlee1521/pep8-vs-pep-8>`_\n    `video <https://youtu.be/PulzIT8KYLk?t=36m>`_.\n\nFeatures\n--------\n\n* Plugin architecture: Adding new checks is easy.\n\n* Parseable output: Jump to error location in your editor.\n\n* Small: Just one Python file, requires only stdlib. You can use just\n  the ``pycodestyle.py`` file for this purpose.\n\n* Comes with a comprehensive test suite.\n\nInstallation\n------------\n\nYou can install, upgrade, and uninstall ``pycodestyle.py`` with these commands::\n\n  $ pip install pycodestyle\n  $ pip install --upgrade pycodestyle\n  $ pip uninstall pycodestyle\n\nThere's also a package for Debian/Ubuntu, but it's not always the\nlatest version.\n\nExample usage and output\n------------------------\n\n::\n\n  $ pycodestyle --first optparse.py\n  optparse.py:69:11: E401 multiple imports on one line\n  optparse.py:77:1: E302 expected 2 blank lines, found 1\n  optparse.py:88:5: E301 expected 1 blank line, found 0\n  optparse.py:347:31: E211 whitespace before '('\n  optparse.py:357:17: E201 whitespace after '{'\n  optparse.py:472:29: E221 multiple spaces before operator\n\nYou can also make ``pycodestyle.py`` show the source code for each error, and\neven the relevant text from PEP 8::\n\n  $ pycodestyle --show-source --show-pep8 testing/data/E40.py\n  testing/data/E40.py:2:10: E401 multiple imports on one line\n  import os, sys\n           ^\n      Imports should usually be on separate lines.\n\n      Okay: import os\\nimport sys\n      E401: import sys, os\n\n\nOr you can display how often each error was found::\n\n  $ pycodestyle --statistics -qq Python-2.5/Lib\n  232     E201 whitespace after '['\n  599     E202 whitespace before ')'\n  631     E203 whitespace before ','\n  842     E211 whitespace before '('\n  2531    E221 multiple spaces before operator\n  4473    E301 expected 1 blank line, found 0\n  4006    E302 expected 2 blank lines, found 1\n  165     E303 too many blank lines (4)\n  325     E401 multiple imports on one line\n  3615    E501 line too long (82 characters)\n\nLinks\n-----\n\n* `Read the documentation <https://pycodestyle.pycqa.org/>`_\n\n* `Fork me on GitHub <http://github.com/PyCQA/pycodestyle>`_\n"
        },
        {
          "name": "dev-requirements.txt",
          "type": "blob",
          "size": 0.00390625,
          "content": "tox\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "pycodestyle.py",
          "type": "blob",
          "size": 97.697265625,
          "content": "#!/usr/bin/env python\n# pycodestyle.py - Check Python source code formatting, according to\n# PEP 8\n#\n# Copyright (C) 2006-2009 Johann C. Rocholl <johann@rocholl.net>\n# Copyright (C) 2009-2014 Florent Xicluna <florent.xicluna@gmail.com>\n# Copyright (C) 2014-2016 Ian Lee <ianlee1521@gmail.com>\n#\n# Permission is hereby granted, free of charge, to any person\n# obtaining a copy of this software and associated documentation files\n# (the \"Software\"), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge,\n# publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so,\n# subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\n# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\n# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\nr\"\"\"\nCheck Python source code formatting, according to PEP 8.\n\nFor usage and a list of options, try this:\n$ python pycodestyle.py -h\n\nThis program and its regression test suite live here:\nhttps://github.com/pycqa/pycodestyle\n\nGroups of errors and warnings:\nE errors\nW warnings\n100 indentation\n200 whitespace\n300 blank lines\n400 imports\n500 line length\n600 deprecation\n700 statements\n900 syntax error\n\"\"\"\nimport bisect\nimport configparser\nimport inspect\nimport io\nimport keyword\nimport os\nimport re\nimport sys\nimport time\nimport tokenize\nimport warnings\nfrom fnmatch import fnmatch\nfrom functools import lru_cache\nfrom optparse import OptionParser\n\n# this is a performance hack.  see https://bugs.python.org/issue43014\nif (\n        sys.version_info < (3, 10) and\n        callable(getattr(tokenize, '_compile', None))\n):  # pragma: no cover (<py310)\n    tokenize._compile = lru_cache(tokenize._compile)  # type: ignore\n\n__version__ = '2.12.1'\n\nDEFAULT_EXCLUDE = '.svn,CVS,.bzr,.hg,.git,__pycache__,.tox'\nDEFAULT_IGNORE = 'E121,E123,E126,E226,E24,E704,W503,W504'\ntry:\n    if sys.platform == 'win32':  # pragma: win32 cover\n        USER_CONFIG = os.path.expanduser(r'~\\.pycodestyle')\n    else:  # pragma: win32 no cover\n        USER_CONFIG = os.path.join(\n            os.getenv('XDG_CONFIG_HOME') or os.path.expanduser('~/.config'),\n            'pycodestyle'\n        )\nexcept ImportError:\n    USER_CONFIG = None\n\nPROJECT_CONFIG = ('setup.cfg', 'tox.ini')\nMAX_LINE_LENGTH = 79\n# Number of blank lines between various code parts.\nBLANK_LINES_CONFIG = {\n    # Top level class and function.\n    'top_level': 2,\n    # Methods and nested class and function.\n    'method': 1,\n}\nMAX_DOC_LENGTH = 72\nINDENT_SIZE = 4\nREPORT_FORMAT = {\n    'default': '%(path)s:%(row)d:%(col)d: %(code)s %(text)s',\n    'pylint': '%(path)s:%(row)d: [%(code)s] %(text)s',\n}\n\nPyCF_ONLY_AST = 1024\nSINGLETONS = frozenset(['False', 'None', 'True'])\nKEYWORDS = frozenset(keyword.kwlist + ['print']) - SINGLETONS\nUNARY_OPERATORS = frozenset(['>>', '**', '*', '+', '-'])\nARITHMETIC_OP = frozenset(['**', '*', '/', '//', '+', '-', '@'])\nWS_OPTIONAL_OPERATORS = ARITHMETIC_OP.union(['^', '&', '|', '<<', '>>', '%'])\nWS_NEEDED_OPERATORS = frozenset([\n    '**=', '*=', '/=', '//=', '+=', '-=', '!=', '<', '>',\n    '%=', '^=', '&=', '|=', '==', '<=', '>=', '<<=', '>>=', '=',\n    'and', 'in', 'is', 'or', '->', ':='])\nWHITESPACE = frozenset(' \\t\\xa0')\nNEWLINE = frozenset([tokenize.NL, tokenize.NEWLINE])\nSKIP_TOKENS = NEWLINE.union([tokenize.INDENT, tokenize.DEDENT])\n# ERRORTOKEN is triggered by backticks in Python 3\nSKIP_COMMENTS = SKIP_TOKENS.union([tokenize.COMMENT, tokenize.ERRORTOKEN])\nBENCHMARK_KEYS = ['directories', 'files', 'logical lines', 'physical lines']\n\nINDENT_REGEX = re.compile(r'([ \\t]*)')\nERRORCODE_REGEX = re.compile(r'\\b[A-Z]\\d{3}\\b')\nDOCSTRING_REGEX = re.compile(r'u?r?[\"\\']')\nEXTRANEOUS_WHITESPACE_REGEX = re.compile(r'[\\[({][ \\t]|[ \\t][\\]}),;:](?!=)')\nWHITESPACE_AFTER_DECORATOR_REGEX = re.compile(r'@\\s')\nWHITESPACE_AFTER_COMMA_REGEX = re.compile(r'[,;:]\\s*(?:  |\\t)')\nCOMPARE_SINGLETON_REGEX = re.compile(r'(\\bNone|\\bFalse|\\bTrue)?\\s*([=!]=)'\n                                     r'\\s*(?(1)|(None|False|True))\\b')\nCOMPARE_NEGATIVE_REGEX = re.compile(r'\\b(?<!is\\s)(not)\\s+[^][)(}{ ]+\\s+'\n                                    r'(in|is)\\s')\nCOMPARE_TYPE_REGEX = re.compile(\n    r'[=!]=\\s+type(?:\\s*\\(\\s*([^)]*[^\\s)])\\s*\\))'\n    r'|(?<!\\.)\\btype(?:\\s*\\(\\s*([^)]*[^\\s)])\\s*\\))\\s+[=!]='\n)\nKEYWORD_REGEX = re.compile(r'(\\s*)\\b(?:%s)\\b(\\s*)' % r'|'.join(KEYWORDS))\nOPERATOR_REGEX = re.compile(r'(?:[^,\\s])(\\s*)(?:[-+*/|!<=>%&^]+|:=)(\\s*)')\nLAMBDA_REGEX = re.compile(r'\\blambda\\b')\nHUNK_REGEX = re.compile(r'^@@ -\\d+(?:,\\d+)? \\+(\\d+)(?:,(\\d+))? @@.*$')\nSTARTSWITH_DEF_REGEX = re.compile(r'^(async\\s+def|def)\\b')\nSTARTSWITH_TOP_LEVEL_REGEX = re.compile(r'^(async\\s+def\\s+|def\\s+|class\\s+|@)')\nSTARTSWITH_INDENT_STATEMENT_REGEX = re.compile(\n    r'^\\s*({})\\b'.format('|'.join(s.replace(' ', r'\\s+') for s in (\n        'def', 'async def',\n        'for', 'async for',\n        'if', 'elif', 'else',\n        'try', 'except', 'finally',\n        'with', 'async with',\n        'class',\n        'while',\n    )))\n)\nDUNDER_REGEX = re.compile(r\"^__([^\\s]+)__(?::\\s*[a-zA-Z.0-9_\\[\\]\\\"]+)? = \")\nBLANK_EXCEPT_REGEX = re.compile(r\"except\\s*:\")\n\nif sys.version_info >= (3, 12):  # pragma: >=3.12 cover\n    FSTRING_START = tokenize.FSTRING_START\n    FSTRING_MIDDLE = tokenize.FSTRING_MIDDLE\n    FSTRING_END = tokenize.FSTRING_END\nelse:  # pragma: <3.12 cover\n    FSTRING_START = FSTRING_MIDDLE = FSTRING_END = -1\n\n_checks = {'physical_line': {}, 'logical_line': {}, 'tree': {}}\n\n\ndef _get_parameters(function):\n    return [parameter.name\n            for parameter\n            in inspect.signature(function).parameters.values()\n            if parameter.kind == parameter.POSITIONAL_OR_KEYWORD]\n\n\ndef register_check(check, codes=None):\n    \"\"\"Register a new check object.\"\"\"\n    def _add_check(check, kind, codes, args):\n        if check in _checks[kind]:\n            _checks[kind][check][0].extend(codes or [])\n        else:\n            _checks[kind][check] = (codes or [''], args)\n    if inspect.isfunction(check):\n        args = _get_parameters(check)\n        if args and args[0] in ('physical_line', 'logical_line'):\n            if codes is None:\n                codes = ERRORCODE_REGEX.findall(check.__doc__ or '')\n            _add_check(check, args[0], codes, args)\n    elif inspect.isclass(check):\n        if _get_parameters(check.__init__)[:2] == ['self', 'tree']:\n            _add_check(check, 'tree', codes, None)\n    return check\n\n\n########################################################################\n# Plugins (check functions) for physical lines\n########################################################################\n\n@register_check\ndef tabs_or_spaces(physical_line, indent_char):\n    r\"\"\"Never mix tabs and spaces.\n\n    The most popular way of indenting Python is with spaces only.  The\n    second-most popular way is with tabs only.  Code indented with a\n    mixture of tabs and spaces should be converted to using spaces\n    exclusively.  When invoking the Python command line interpreter with\n    the -t option, it issues warnings about code that illegally mixes\n    tabs and spaces.  When using -tt these warnings become errors.\n    These options are highly recommended!\n\n    Okay: if a == 0:\\n    a = 1\\n    b = 1\n    \"\"\"\n    indent = INDENT_REGEX.match(physical_line).group(1)\n    for offset, char in enumerate(indent):\n        if char != indent_char:\n            return offset, \"E101 indentation contains mixed spaces and tabs\"\n\n\n@register_check\ndef tabs_obsolete(physical_line):\n    r\"\"\"On new projects, spaces-only are strongly recommended over tabs.\n\n    Okay: if True:\\n    return\n    W191: if True:\\n\\treturn\n    \"\"\"\n    indent = INDENT_REGEX.match(physical_line).group(1)\n    if '\\t' in indent:\n        return indent.index('\\t'), \"W191 indentation contains tabs\"\n\n\n@register_check\ndef trailing_whitespace(physical_line):\n    r\"\"\"Trailing whitespace is superfluous.\n\n    The warning returned varies on whether the line itself is blank,\n    for easier filtering for those who want to indent their blank lines.\n\n    Okay: spam(1)\\n#\n    W291: spam(1) \\n#\n    W293: class Foo(object):\\n    \\n    bang = 12\n    \"\"\"\n    # Strip these trailing characters:\n    # - chr(10), newline\n    # - chr(13), carriage return\n    # - chr(12), form feed, ^L\n    physical_line = physical_line.rstrip('\\n\\r\\x0c')\n    stripped = physical_line.rstrip(' \\t\\v')\n    if physical_line != stripped:\n        if stripped:\n            return len(stripped), \"W291 trailing whitespace\"\n        else:\n            return 0, \"W293 blank line contains whitespace\"\n\n\n@register_check\ndef trailing_blank_lines(physical_line, lines, line_number, total_lines):\n    r\"\"\"Trailing blank lines are superfluous.\n\n    Okay: spam(1)\n    W391: spam(1)\\n\n\n    However the last line should end with a new line (warning W292).\n    \"\"\"\n    if line_number == total_lines:\n        stripped_last_line = physical_line.rstrip('\\r\\n')\n        if physical_line and not stripped_last_line:\n            return 0, \"W391 blank line at end of file\"\n        if stripped_last_line == physical_line:\n            return len(lines[-1]), \"W292 no newline at end of file\"\n\n\n@register_check\ndef maximum_line_length(physical_line, max_line_length, multiline,\n                        line_number, noqa):\n    r\"\"\"Limit all lines to a maximum of 79 characters.\n\n    There are still many devices around that are limited to 80 character\n    lines; plus, limiting windows to 80 characters makes it possible to\n    have several windows side-by-side.  The default wrapping on such\n    devices looks ugly.  Therefore, please limit all lines to a maximum\n    of 79 characters. For flowing long blocks of text (docstrings or\n    comments), limiting the length to 72 characters is recommended.\n\n    Reports error E501.\n    \"\"\"\n    line = physical_line.rstrip()\n    length = len(line)\n    if length > max_line_length and not noqa:\n        # Special case: ignore long shebang lines.\n        if line_number == 1 and line.startswith('#!'):\n            return\n        # Special case for long URLs in multi-line docstrings or\n        # comments, but still report the error when the 72 first chars\n        # are whitespaces.\n        chunks = line.split()\n        if ((len(chunks) == 1 and multiline) or\n            (len(chunks) == 2 and chunks[0] == '#')) and \\\n                len(line) - len(chunks[-1]) < max_line_length - 7:\n            return\n        if length > max_line_length:\n            return (max_line_length, \"E501 line too long \"\n                    \"(%d > %d characters)\" % (length, max_line_length))\n\n\n########################################################################\n# Plugins (check functions) for logical lines\n########################################################################\n\n\ndef _is_one_liner(logical_line, indent_level, lines, line_number):\n    if not STARTSWITH_TOP_LEVEL_REGEX.match(logical_line):\n        return False\n\n    line_idx = line_number - 1\n\n    if line_idx < 1:\n        prev_indent = 0\n    else:\n        prev_indent = expand_indent(lines[line_idx - 1])\n\n    if prev_indent > indent_level:\n        return False\n\n    while line_idx < len(lines):\n        line = lines[line_idx].strip()\n        if not line.startswith('@') and STARTSWITH_TOP_LEVEL_REGEX.match(line):\n            break\n        else:\n            line_idx += 1\n    else:\n        return False  # invalid syntax: EOF while searching for def/class\n\n    next_idx = line_idx + 1\n    while next_idx < len(lines):\n        if lines[next_idx].strip():\n            break\n        else:\n            next_idx += 1\n    else:\n        return True  # line is last in the file\n\n    return expand_indent(lines[next_idx]) <= indent_level\n\n\n@register_check\ndef blank_lines(logical_line, blank_lines, indent_level, line_number,\n                blank_before, previous_logical,\n                previous_unindented_logical_line, previous_indent_level,\n                lines):\n    r\"\"\"Separate top-level function and class definitions with two blank\n    lines.\n\n    Method definitions inside a class are separated by a single blank\n    line.\n\n    Extra blank lines may be used (sparingly) to separate groups of\n    related functions.  Blank lines may be omitted between a bunch of\n    related one-liners (e.g. a set of dummy implementations).\n\n    Use blank lines in functions, sparingly, to indicate logical\n    sections.\n\n    Okay: def a():\\n    pass\\n\\n\\ndef b():\\n    pass\n    Okay: def a():\\n    pass\\n\\n\\nasync def b():\\n    pass\n    Okay: def a():\\n    pass\\n\\n\\n# Foo\\n# Bar\\n\\ndef b():\\n    pass\n    Okay: default = 1\\nfoo = 1\n    Okay: classify = 1\\nfoo = 1\n\n    E301: class Foo:\\n    b = 0\\n    def bar():\\n        pass\n    E302: def a():\\n    pass\\n\\ndef b(n):\\n    pass\n    E302: def a():\\n    pass\\n\\nasync def b(n):\\n    pass\n    E303: def a():\\n    pass\\n\\n\\n\\ndef b(n):\\n    pass\n    E303: def a():\\n\\n\\n\\n    pass\n    E304: @decorator\\n\\ndef a():\\n    pass\n    E305: def a():\\n    pass\\na()\n    E306: def a():\\n    def b():\\n        pass\\n    def c():\\n        pass\n    \"\"\"  # noqa\n    top_level_lines = BLANK_LINES_CONFIG['top_level']\n    method_lines = BLANK_LINES_CONFIG['method']\n\n    if not previous_logical and blank_before < top_level_lines:\n        return  # Don't expect blank lines before the first line\n    if previous_logical.startswith('@'):\n        if blank_lines:\n            yield 0, \"E304 blank lines found after function decorator\"\n    elif (blank_lines > top_level_lines or\n            (indent_level and blank_lines == method_lines + 1)\n          ):\n        yield 0, \"E303 too many blank lines (%d)\" % blank_lines\n    elif STARTSWITH_TOP_LEVEL_REGEX.match(logical_line):\n        # allow a group of one-liners\n        if (\n            _is_one_liner(logical_line, indent_level, lines, line_number) and\n            blank_before == 0\n        ):\n            return\n        if indent_level:\n            if not (blank_before == method_lines or\n                    previous_indent_level < indent_level or\n                    DOCSTRING_REGEX.match(previous_logical)\n                    ):\n                ancestor_level = indent_level\n                nested = False\n                # Search backwards for a def ancestor or tree root\n                # (top level).\n                for line in lines[line_number - top_level_lines::-1]:\n                    if line.strip() and expand_indent(line) < ancestor_level:\n                        ancestor_level = expand_indent(line)\n                        nested = STARTSWITH_DEF_REGEX.match(line.lstrip())\n                        if nested or ancestor_level == 0:\n                            break\n                if nested:\n                    yield 0, \"E306 expected %s blank line before a \" \\\n                        \"nested definition, found 0\" % (method_lines,)\n                else:\n                    yield 0, \"E301 expected {} blank line, found 0\".format(\n                        method_lines)\n        elif blank_before != top_level_lines:\n            yield 0, \"E302 expected %s blank lines, found %d\" % (\n                top_level_lines, blank_before)\n    elif (logical_line and\n            not indent_level and\n            blank_before != top_level_lines and\n            previous_unindented_logical_line.startswith(('def ', 'class '))\n          ):\n        yield 0, \"E305 expected %s blank lines after \" \\\n            \"class or function definition, found %d\" % (\n                top_level_lines, blank_before)\n\n\n@register_check\ndef extraneous_whitespace(logical_line):\n    r\"\"\"Avoid extraneous whitespace.\n\n    Avoid extraneous whitespace in these situations:\n    - Immediately inside parentheses, brackets or braces.\n    - Immediately before a comma, semicolon, or colon.\n\n    Okay: spam(ham[1], {eggs: 2})\n    E201: spam( ham[1], {eggs: 2})\n    E201: spam(ham[ 1], {eggs: 2})\n    E201: spam(ham[1], { eggs: 2})\n    E202: spam(ham[1], {eggs: 2} )\n    E202: spam(ham[1 ], {eggs: 2})\n    E202: spam(ham[1], {eggs: 2 })\n\n    E203: if x == 4: print x, y; x, y = y , x\n    E203: if x == 4: print x, y ; x, y = y, x\n    E203: if x == 4 : print x, y; x, y = y, x\n\n    Okay: @decorator\n    E204: @ decorator\n    \"\"\"\n    line = logical_line\n    for match in EXTRANEOUS_WHITESPACE_REGEX.finditer(line):\n        text = match.group()\n        char = text.strip()\n        found = match.start()\n        if text[-1].isspace():\n            # assert char in '([{'\n            yield found + 1, \"E201 whitespace after '%s'\" % char\n        elif line[found - 1] != ',':\n            code = ('E202' if char in '}])' else 'E203')  # if char in ',;:'\n            yield found, f\"{code} whitespace before '{char}'\"\n\n    if WHITESPACE_AFTER_DECORATOR_REGEX.match(logical_line):\n        yield 1, \"E204 whitespace after decorator '@'\"\n\n\n@register_check\ndef whitespace_around_keywords(logical_line):\n    r\"\"\"Avoid extraneous whitespace around keywords.\n\n    Okay: True and False\n    E271: True and  False\n    E272: True  and False\n    E273: True and\\tFalse\n    E274: True\\tand False\n    \"\"\"\n    for match in KEYWORD_REGEX.finditer(logical_line):\n        before, after = match.groups()\n\n        if '\\t' in before:\n            yield match.start(1), \"E274 tab before keyword\"\n        elif len(before) > 1:\n            yield match.start(1), \"E272 multiple spaces before keyword\"\n\n        if '\\t' in after:\n            yield match.start(2), \"E273 tab after keyword\"\n        elif len(after) > 1:\n            yield match.start(2), \"E271 multiple spaces after keyword\"\n\n\n@register_check\ndef missing_whitespace_after_keyword(logical_line, tokens):\n    r\"\"\"Keywords should be followed by whitespace.\n\n    Okay: from foo import (bar, baz)\n    E275: from foo import(bar, baz)\n    E275: from importable.module import(bar, baz)\n    E275: if(foo): bar\n    \"\"\"\n    for tok0, tok1 in zip(tokens, tokens[1:]):\n        # This must exclude the True/False/None singletons, which can\n        # appear e.g. as \"if x is None:\", and async/await, which were\n        # valid identifier names in old Python versions.\n        if (tok0.end == tok1.start and\n                tok0.type == tokenize.NAME and\n                keyword.iskeyword(tok0.string) and\n                tok0.string not in SINGLETONS and\n                not (tok0.string == 'except' and tok1.string == '*') and\n                not (tok0.string == 'yield' and tok1.string == ')') and\n                tok1.string not in ':\\n'):\n            yield tok0.end, \"E275 missing whitespace after keyword\"\n\n\n@register_check\ndef indentation(logical_line, previous_logical, indent_char,\n                indent_level, previous_indent_level,\n                indent_size):\n    r\"\"\"Use indent_size (PEP8 says 4) spaces per indentation level.\n\n    For really old code that you don't want to mess up, you can continue\n    to use 8-space tabs.\n\n    Okay: a = 1\n    Okay: if a == 0:\\n    a = 1\n    E111:   a = 1\n    E114:   # a = 1\n\n    Okay: for item in items:\\n    pass\n    E112: for item in items:\\npass\n    E115: for item in items:\\n# Hi\\n    pass\n\n    Okay: a = 1\\nb = 2\n    E113: a = 1\\n    b = 2\n    E116: a = 1\\n    # b = 2\n    \"\"\"\n    c = 0 if logical_line else 3\n    tmpl = \"E11%d %s\" if logical_line else \"E11%d %s (comment)\"\n    if indent_level % indent_size:\n        yield 0, tmpl % (\n            1 + c,\n            \"indentation is not a multiple of \" + str(indent_size),\n        )\n    indent_expect = previous_logical.endswith(':')\n    if indent_expect and indent_level <= previous_indent_level:\n        yield 0, tmpl % (2 + c, \"expected an indented block\")\n    elif not indent_expect and indent_level > previous_indent_level:\n        yield 0, tmpl % (3 + c, \"unexpected indentation\")\n\n    if indent_expect:\n        expected_indent_amount = 8 if indent_char == '\\t' else 4\n        expected_indent_level = previous_indent_level + expected_indent_amount\n        if indent_level > expected_indent_level:\n            yield 0, tmpl % (7, 'over-indented')\n\n\n@register_check\ndef continued_indentation(logical_line, tokens, indent_level, hang_closing,\n                          indent_char, indent_size, noqa, verbose):\n    r\"\"\"Continuation lines indentation.\n\n    Continuation lines should align wrapped elements either vertically\n    using Python's implicit line joining inside parentheses, brackets\n    and braces, or using a hanging indent.\n\n    When using a hanging indent these considerations should be applied:\n    - there should be no arguments on the first line, and\n    - further indentation should be used to clearly distinguish itself\n      as a continuation line.\n\n    Okay: a = (\\n)\n    E123: a = (\\n    )\n\n    Okay: a = (\\n    42)\n    E121: a = (\\n   42)\n    E122: a = (\\n42)\n    E123: a = (\\n    42\\n    )\n    E124: a = (24,\\n     42\\n)\n    E125: if (\\n    b):\\n    pass\n    E126: a = (\\n        42)\n    E127: a = (24,\\n      42)\n    E128: a = (24,\\n    42)\n    E129: if (a or\\n    b):\\n    pass\n    E131: a = (\\n    42\\n 24)\n    \"\"\"\n    first_row = tokens[0][2][0]\n    nrows = 1 + tokens[-1][2][0] - first_row\n    if noqa or nrows == 1:\n        return\n\n    # indent_next tells us whether the next block is indented; assuming\n    # that it is indented by 4 spaces, then we should not allow 4-space\n    # indents on the final continuation line; in turn, some other\n    # indents are allowed to have an extra 4 spaces.\n    indent_next = logical_line.endswith(':')\n\n    row = depth = 0\n    valid_hangs = (indent_size,) if indent_char != '\\t' \\\n        else (indent_size, indent_size * 2)\n    # remember how many brackets were opened on each line\n    parens = [0] * nrows\n    # relative indents of physical lines\n    rel_indent = [0] * nrows\n    # for each depth, collect a list of opening rows\n    open_rows = [[0]]\n    # for each depth, memorize the hanging indentation\n    hangs = [None]\n    # visual indents\n    indent_chances = {}\n    last_indent = tokens[0][2]\n    visual_indent = None\n    last_token_multiline = False\n    # for each depth, memorize the visual indent column\n    indent = [last_indent[1]]\n    if verbose >= 3:\n        print(\">>> \" + tokens[0][4].rstrip())\n\n    for token_type, text, start, end, line in tokens:\n\n        newline = row < start[0] - first_row\n        if newline:\n            row = start[0] - first_row\n            newline = not last_token_multiline and token_type not in NEWLINE\n\n        if newline:\n            # this is the beginning of a continuation line.\n            last_indent = start\n            if verbose >= 3:\n                print(\"... \" + line.rstrip())\n\n            # record the initial indent.\n            rel_indent[row] = expand_indent(line) - indent_level\n\n            # identify closing bracket\n            close_bracket = (token_type == tokenize.OP and text in ']})')\n\n            # is the indent relative to an opening bracket line?\n            for open_row in reversed(open_rows[depth]):\n                hang = rel_indent[row] - rel_indent[open_row]\n                hanging_indent = hang in valid_hangs\n                if hanging_indent:\n                    break\n            if hangs[depth]:\n                hanging_indent = (hang == hangs[depth])\n            # is there any chance of visual indent?\n            visual_indent = (not close_bracket and hang > 0 and\n                             indent_chances.get(start[1]))\n\n            if close_bracket and indent[depth]:\n                # closing bracket for visual indent\n                if start[1] != indent[depth]:\n                    yield (start, \"E124 closing bracket does not match \"\n                           \"visual indentation\")\n            elif close_bracket and not hang:\n                # closing bracket matches indentation of opening\n                # bracket's line\n                if hang_closing:\n                    yield start, \"E133 closing bracket is missing indentation\"\n            elif indent[depth] and start[1] < indent[depth]:\n                if visual_indent is not True:\n                    # visual indent is broken\n                    yield (start, \"E128 continuation line \"\n                           \"under-indented for visual indent\")\n            elif hanging_indent or (indent_next and\n                                    rel_indent[row] == 2 * indent_size):\n                # hanging indent is verified\n                if close_bracket and not hang_closing:\n                    yield (start, \"E123 closing bracket does not match \"\n                           \"indentation of opening bracket's line\")\n                hangs[depth] = hang\n            elif visual_indent is True:\n                # visual indent is verified\n                indent[depth] = start[1]\n            elif visual_indent in (text, str):\n                # ignore token lined up with matching one from a\n                # previous line\n                pass\n            else:\n                # indent is broken\n                if hang <= 0:\n                    error = \"E122\", \"missing indentation or outdented\"\n                elif indent[depth]:\n                    error = \"E127\", \"over-indented for visual indent\"\n                elif not close_bracket and hangs[depth]:\n                    error = \"E131\", \"unaligned for hanging indent\"\n                else:\n                    hangs[depth] = hang\n                    if hang > indent_size:\n                        error = \"E126\", \"over-indented for hanging indent\"\n                    else:\n                        error = \"E121\", \"under-indented for hanging indent\"\n                yield start, \"%s continuation line %s\" % error\n\n        # look for visual indenting\n        if (parens[row] and\n                token_type not in (tokenize.NL, tokenize.COMMENT) and\n                not indent[depth]):\n            indent[depth] = start[1]\n            indent_chances[start[1]] = True\n            if verbose >= 4:\n                print(f\"bracket depth {depth} indent to {start[1]}\")\n        # deal with implicit string concatenation\n        elif token_type in (tokenize.STRING, tokenize.COMMENT, FSTRING_START):\n            indent_chances[start[1]] = str\n        # visual indent after assert/raise/with\n        elif not row and not depth and text in [\"assert\", \"raise\", \"with\"]:\n            indent_chances[end[1] + 1] = True\n        # special case for the \"if\" statement because len(\"if (\") == 4\n        elif not indent_chances and not row and not depth and text == 'if':\n            indent_chances[end[1] + 1] = True\n        elif text == ':' and line[end[1]:].isspace():\n            open_rows[depth].append(row)\n\n        # keep track of bracket depth\n        if token_type == tokenize.OP:\n            if text in '([{':\n                depth += 1\n                indent.append(0)\n                hangs.append(None)\n                if len(open_rows) == depth:\n                    open_rows.append([])\n                open_rows[depth].append(row)\n                parens[row] += 1\n                if verbose >= 4:\n                    print(\"bracket depth %s seen, col %s, visual min = %s\" %\n                          (depth, start[1], indent[depth]))\n            elif text in ')]}' and depth > 0:\n                # parent indents should not be more than this one\n                prev_indent = indent.pop() or last_indent[1]\n                hangs.pop()\n                for d in range(depth):\n                    if indent[d] > prev_indent:\n                        indent[d] = 0\n                for ind in list(indent_chances):\n                    if ind >= prev_indent:\n                        del indent_chances[ind]\n                del open_rows[depth + 1:]\n                depth -= 1\n                if depth:\n                    indent_chances[indent[depth]] = True\n                for idx in range(row, -1, -1):\n                    if parens[idx]:\n                        parens[idx] -= 1\n                        break\n            assert len(indent) == depth + 1\n            if start[1] not in indent_chances:\n                # allow lining up tokens\n                indent_chances[start[1]] = text\n\n        last_token_multiline = (start[0] != end[0])\n        if last_token_multiline:\n            rel_indent[end[0] - first_row] = rel_indent[row]\n\n    if indent_next and expand_indent(line) == indent_level + indent_size:\n        pos = (start[0], indent[0] + indent_size)\n        if visual_indent:\n            code = \"E129 visually indented line\"\n        else:\n            code = \"E125 continuation line\"\n        yield pos, \"%s with same indent as next logical line\" % code\n\n\n@register_check\ndef whitespace_before_parameters(logical_line, tokens):\n    r\"\"\"Avoid extraneous whitespace.\n\n    Avoid extraneous whitespace in the following situations:\n    - before the open parenthesis that starts the argument list of a\n      function call.\n    - before the open parenthesis that starts an indexing or slicing.\n\n    Okay: spam(1)\n    E211: spam (1)\n\n    Okay: dict['key'] = list[index]\n    E211: dict ['key'] = list[index]\n    E211: dict['key'] = list [index]\n    \"\"\"\n    prev_type, prev_text, __, prev_end, __ = tokens[0]\n    for index in range(1, len(tokens)):\n        token_type, text, start, end, __ = tokens[index]\n        if (\n            token_type == tokenize.OP and\n            text in '([' and\n            start != prev_end and\n            (prev_type == tokenize.NAME or prev_text in '}])') and\n            # Syntax \"class A (B):\" is allowed, but avoid it\n            (index < 2 or tokens[index - 2][1] != 'class') and\n            # Allow \"return (a.foo for a in range(5))\"\n            not keyword.iskeyword(prev_text) and\n            (\n                # 3.12+: type is a soft keyword but no braces after\n                prev_text == 'type' or\n                not keyword.issoftkeyword(prev_text)\n            )\n        ):\n            yield prev_end, \"E211 whitespace before '%s'\" % text\n        prev_type = token_type\n        prev_text = text\n        prev_end = end\n\n\n@register_check\ndef whitespace_around_operator(logical_line):\n    r\"\"\"Avoid extraneous whitespace around an operator.\n\n    Okay: a = 12 + 3\n    E221: a = 4  + 5\n    E222: a = 4 +  5\n    E223: a = 4\\t+ 5\n    E224: a = 4 +\\t5\n    \"\"\"\n    for match in OPERATOR_REGEX.finditer(logical_line):\n        before, after = match.groups()\n\n        if '\\t' in before:\n            yield match.start(1), \"E223 tab before operator\"\n        elif len(before) > 1:\n            yield match.start(1), \"E221 multiple spaces before operator\"\n\n        if '\\t' in after:\n            yield match.start(2), \"E224 tab after operator\"\n        elif len(after) > 1:\n            yield match.start(2), \"E222 multiple spaces after operator\"\n\n\n@register_check\ndef missing_whitespace(logical_line, tokens):\n    r\"\"\"Surround operators with the correct amount of whitespace.\n\n    - Always surround these binary operators with a single space on\n      either side: assignment (=), augmented assignment (+=, -= etc.),\n      comparisons (==, <, >, !=, <=, >=, in, not in, is, is not),\n      Booleans (and, or, not).\n\n    - Each comma, semicolon or colon should be followed by whitespace.\n\n    - If operators with different priorities are used, consider adding\n      whitespace around the operators with the lowest priorities.\n\n    Okay: i = i + 1\n    Okay: submitted += 1\n    Okay: x = x * 2 - 1\n    Okay: hypot2 = x * x + y * y\n    Okay: c = (a + b) * (a - b)\n    Okay: foo(bar, key='word', *args, **kwargs)\n    Okay: alpha[:-i]\n    Okay: [a, b]\n    Okay: (3,)\n    Okay: a[3,] = 1\n    Okay: a[1:4]\n    Okay: a[:4]\n    Okay: a[1:]\n    Okay: a[1:4:2]\n\n    E225: i=i+1\n    E225: submitted +=1\n    E225: x = x /2 - 1\n    E225: z = x **y\n    E225: z = 1and 1\n    E226: c = (a+b) * (a-b)\n    E226: hypot2 = x*x + y*y\n    E227: c = a|b\n    E228: msg = fmt%(errno, errmsg)\n    E231: ['a','b']\n    E231: foo(bar,baz)\n    E231: [{'a':'b'}]\n    \"\"\"\n    need_space = False\n    prev_type = tokenize.OP\n    prev_text = prev_end = None\n    operator_types = (tokenize.OP, tokenize.NAME)\n    brace_stack = []\n    for token_type, text, start, end, line in tokens:\n        if token_type == tokenize.OP and text in {'[', '(', '{'}:\n            brace_stack.append(text)\n        elif token_type == FSTRING_START:  # pragma: >=3.12 cover\n            brace_stack.append('f')\n        elif token_type == tokenize.NAME and text == 'lambda':\n            brace_stack.append('l')\n        elif brace_stack:\n            if token_type == tokenize.OP and text in {']', ')', '}'}:\n                brace_stack.pop()\n            elif token_type == FSTRING_END:  # pragma: >=3.12 cover\n                brace_stack.pop()\n            elif (\n                    brace_stack[-1] == 'l' and\n                    token_type == tokenize.OP and\n                    text == ':'\n            ):\n                brace_stack.pop()\n\n        if token_type in SKIP_COMMENTS:\n            continue\n\n        if token_type == tokenize.OP and text in {',', ';', ':'}:\n            next_char = line[end[1]:end[1] + 1]\n            if next_char not in WHITESPACE and next_char not in '\\r\\n':\n                # slice\n                if text == ':' and brace_stack[-1:] == ['[']:\n                    pass\n                # 3.12+ fstring format specifier\n                elif text == ':' and brace_stack[-2:] == ['f', '{']:  # pragma: >=3.12 cover  # noqa: E501\n                    pass\n                # tuple (and list for some reason?)\n                elif text == ',' and next_char in ')]':\n                    pass\n                else:\n                    yield start, f'E231 missing whitespace after {text!r}'\n\n        if need_space:\n            if start != prev_end:\n                # Found a (probably) needed space\n                if need_space is not True and not need_space[1]:\n                    yield (need_space[0],\n                           \"E225 missing whitespace around operator\")\n                need_space = False\n            elif (\n                    # def f(a, /, b):\n                    #           ^\n                    # def f(a, b, /):\n                    #              ^\n                    # f = lambda a, /:\n                    #                ^\n                    prev_text == '/' and text in {',', ')', ':'} or\n                    # def f(a, b, /):\n                    #               ^\n                    prev_text == ')' and text == ':'\n            ):\n                # Tolerate the \"/\" operator in function definition\n                # For more info see PEP570\n                pass\n            else:\n                if need_space is True or need_space[1]:\n                    # A needed trailing space was not found\n                    yield prev_end, \"E225 missing whitespace around operator\"\n                elif prev_text != '**':\n                    code, optype = 'E226', 'arithmetic'\n                    if prev_text == '%':\n                        code, optype = 'E228', 'modulo'\n                    elif prev_text not in ARITHMETIC_OP:\n                        code, optype = 'E227', 'bitwise or shift'\n                    yield (need_space[0], \"%s missing whitespace \"\n                           \"around %s operator\" % (code, optype))\n                need_space = False\n        elif token_type in operator_types and prev_end is not None:\n            if (\n                    text == '=' and (\n                        # allow lambda default args: lambda x=None: None\n                        brace_stack[-1:] == ['l'] or\n                        # allow keyword args or defaults: foo(bar=None).\n                        brace_stack[-1:] == ['('] or\n                        # allow python 3.8 fstring repr specifier\n                        brace_stack[-2:] == ['f', '{']\n                    )\n            ):\n                pass\n            elif text in WS_NEEDED_OPERATORS:\n                need_space = True\n            elif text in UNARY_OPERATORS:\n                # Check if the operator is used as a binary operator\n                # Allow unary operators: -123, -x, +1.\n                # Allow argument unpacking: foo(*args, **kwargs).\n                if prev_type == tokenize.OP and prev_text in '}])' or (\n                    prev_type != tokenize.OP and\n                    prev_text not in KEYWORDS and\n                    not keyword.issoftkeyword(prev_text)\n                ):\n                    need_space = None\n            elif text in WS_OPTIONAL_OPERATORS:\n                need_space = None\n\n            if need_space is None:\n                # Surrounding space is optional, but ensure that\n                # trailing space matches opening space\n                need_space = (prev_end, start != prev_end)\n            elif need_space and start == prev_end:\n                # A needed opening space was not found\n                yield prev_end, \"E225 missing whitespace around operator\"\n                need_space = False\n        prev_type = token_type\n        prev_text = text\n        prev_end = end\n\n\n@register_check\ndef whitespace_around_comma(logical_line):\n    r\"\"\"Avoid extraneous whitespace after a comma or a colon.\n\n    Note: these checks are disabled by default\n\n    Okay: a = (1, 2)\n    E241: a = (1,  2)\n    E242: a = (1,\\t2)\n    \"\"\"\n    line = logical_line\n    for m in WHITESPACE_AFTER_COMMA_REGEX.finditer(line):\n        found = m.start() + 1\n        if '\\t' in m.group():\n            yield found, \"E242 tab after '%s'\" % m.group()[0]\n        else:\n            yield found, \"E241 multiple spaces after '%s'\" % m.group()[0]\n\n\n@register_check\ndef whitespace_around_named_parameter_equals(logical_line, tokens):\n    r\"\"\"Don't use spaces around the '=' sign in function arguments.\n\n    Don't use spaces around the '=' sign when used to indicate a\n    keyword argument or a default parameter value, except when\n    using a type annotation.\n\n    Okay: def complex(real, imag=0.0):\n    Okay: return magic(r=real, i=imag)\n    Okay: boolean(a == b)\n    Okay: boolean(a != b)\n    Okay: boolean(a <= b)\n    Okay: boolean(a >= b)\n    Okay: def foo(arg: int = 42):\n    Okay: async def foo(arg: int = 42):\n\n    E251: def complex(real, imag = 0.0):\n    E251: return magic(r = real, i = imag)\n    E252: def complex(real, image: float=0.0):\n    \"\"\"\n    parens = 0\n    no_space = False\n    require_space = False\n    prev_end = None\n    annotated_func_arg = False\n    in_def = bool(STARTSWITH_DEF_REGEX.match(logical_line))\n\n    message = \"E251 unexpected spaces around keyword / parameter equals\"\n    missing_message = \"E252 missing whitespace around parameter equals\"\n\n    for token_type, text, start, end, line in tokens:\n        if token_type == tokenize.NL:\n            continue\n        if no_space:\n            no_space = False\n            if start != prev_end:\n                yield (prev_end, message)\n        if require_space:\n            require_space = False\n            if start == prev_end:\n                yield (prev_end, missing_message)\n        if token_type == tokenize.OP:\n            if text in '([':\n                parens += 1\n            elif text in ')]':\n                parens -= 1\n            elif in_def and text == ':' and parens == 1:\n                annotated_func_arg = True\n            elif parens == 1 and text == ',':\n                annotated_func_arg = False\n            elif parens and text == '=':\n                if annotated_func_arg and parens == 1:\n                    require_space = True\n                    if start == prev_end:\n                        yield (prev_end, missing_message)\n                else:\n                    no_space = True\n                    if start != prev_end:\n                        yield (prev_end, message)\n            if not parens:\n                annotated_func_arg = False\n\n        prev_end = end\n\n\n@register_check\ndef whitespace_before_comment(logical_line, tokens):\n    \"\"\"Separate inline comments by at least two spaces.\n\n    An inline comment is a comment on the same line as a statement.\n    Inline comments should be separated by at least two spaces from the\n    statement. They should start with a # and a single space.\n\n    Each line of a block comment starts with a # and one or multiple\n    spaces as there can be indented text inside the comment.\n\n    Okay: x = x + 1  # Increment x\n    Okay: x = x + 1    # Increment x\n    Okay: # Block comments:\n    Okay: #  - Block comment list\n    Okay: # \\xa0- Block comment list\n    E261: x = x + 1 # Increment x\n    E262: x = x + 1  #Increment x\n    E262: x = x + 1  #  Increment x\n    E262: x = x + 1  # \\xa0Increment x\n    E265: #Block comment\n    E266: ### Block comment\n    \"\"\"\n    prev_end = (0, 0)\n    for token_type, text, start, end, line in tokens:\n        if token_type == tokenize.COMMENT:\n            inline_comment = line[:start[1]].strip()\n            if inline_comment:\n                if prev_end[0] == start[0] and start[1] < prev_end[1] + 2:\n                    yield (prev_end,\n                           \"E261 at least two spaces before inline comment\")\n            symbol, sp, comment = text.partition(' ')\n            bad_prefix = symbol not in '#:' and (symbol.lstrip('#')[:1] or '#')\n            if inline_comment:\n                if bad_prefix or comment[:1] in WHITESPACE:\n                    yield start, \"E262 inline comment should start with '# '\"\n            elif bad_prefix and (bad_prefix != '!' or start[0] > 1):\n                if bad_prefix != '#':\n                    yield start, \"E265 block comment should start with '# '\"\n                elif comment:\n                    yield start, \"E266 too many leading '#' for block comment\"\n        elif token_type != tokenize.NL:\n            prev_end = end\n\n\n@register_check\ndef imports_on_separate_lines(logical_line):\n    r\"\"\"Place imports on separate lines.\n\n    Okay: import os\\nimport sys\n    E401: import sys, os\n\n    Okay: from subprocess import Popen, PIPE\n    Okay: from myclas import MyClass\n    Okay: from foo.bar.yourclass import YourClass\n    Okay: import myclass\n    Okay: import foo.bar.yourclass\n    \"\"\"\n    line = logical_line\n    if line.startswith('import '):\n        found = line.find(',')\n        if -1 < found and ';' not in line[:found]:\n            yield found, \"E401 multiple imports on one line\"\n\n\n@register_check\ndef module_imports_on_top_of_file(\n        logical_line, indent_level, checker_state, noqa):\n    r\"\"\"Place imports at the top of the file.\n\n    Always put imports at the top of the file, just after any module\n    comments and docstrings, and before module globals and constants.\n\n    Okay: import os\n    Okay: # this is a comment\\nimport os\n    Okay: '''this is a module docstring'''\\nimport os\n    Okay: r'''this is a module docstring'''\\nimport os\n    E402: a=1\\nimport os\n    E402: 'One string'\\n\"Two string\"\\nimport os\n    E402: a=1\\nfrom sys import x\n\n    Okay: if x:\\n    import os\n    \"\"\"  # noqa\n    def is_string_literal(line):\n        if line[0] in 'uUbB':\n            line = line[1:]\n        if line and line[0] in 'rR':\n            line = line[1:]\n        return line and (line[0] == '\"' or line[0] == \"'\")\n\n    allowed_keywords = (\n        'try', 'except', 'else', 'finally', 'with', 'if', 'elif')\n\n    if indent_level:  # Allow imports in conditional statement/function\n        return\n    if not logical_line:  # Allow empty lines or comments\n        return\n    if noqa:\n        return\n    line = logical_line\n    if line.startswith('import ') or line.startswith('from '):\n        if checker_state.get('seen_non_imports', False):\n            yield 0, \"E402 module level import not at top of file\"\n    elif re.match(DUNDER_REGEX, line):\n        return\n    elif any(line.startswith(kw) for kw in allowed_keywords):\n        # Allow certain keywords intermixed with imports in order to\n        # support conditional or filtered importing\n        return\n    elif is_string_literal(line):\n        # The first literal is a docstring, allow it. Otherwise, report\n        # error.\n        if checker_state.get('seen_docstring', False):\n            checker_state['seen_non_imports'] = True\n        else:\n            checker_state['seen_docstring'] = True\n    else:\n        checker_state['seen_non_imports'] = True\n\n\n@register_check\ndef compound_statements(logical_line):\n    r\"\"\"Compound statements (on the same line) are generally\n    discouraged.\n\n    While sometimes it's okay to put an if/for/while with a small body\n    on the same line, never do this for multi-clause statements.\n    Also avoid folding such long lines!\n\n    Always use a def statement instead of an assignment statement that\n    binds a lambda expression directly to a name.\n\n    Okay: if foo == 'blah':\\n    do_blah_thing()\n    Okay: do_one()\n    Okay: do_two()\n    Okay: do_three()\n\n    E701: if foo == 'blah': do_blah_thing()\n    E701: for x in lst: total += x\n    E701: while t < 10: t = delay()\n    E701: if foo == 'blah': do_blah_thing()\n    E701: else: do_non_blah_thing()\n    E701: try: something()\n    E701: finally: cleanup()\n    E701: if foo == 'blah': one(); two(); three()\n    E702: do_one(); do_two(); do_three()\n    E703: do_four();  # useless semicolon\n    E704: def f(x): return 2*x\n    E731: f = lambda x: 2*x\n    \"\"\"\n    line = logical_line\n    last_char = len(line) - 1\n    found = line.find(':')\n    prev_found = 0\n    counts = {char: 0 for char in '{}[]()'}\n    while -1 < found < last_char:\n        update_counts(line[prev_found:found], counts)\n        if (\n                counts['{'] <= counts['}'] and  # {'a': 1} (dict)\n                counts['['] <= counts[']'] and  # [1:2] (slice)\n                counts['('] <= counts[')'] and  # (annotation)\n                line[found + 1] != '='  # assignment expression\n        ):\n            lambda_kw = LAMBDA_REGEX.search(line, 0, found)\n            if lambda_kw:\n                before = line[:lambda_kw.start()].rstrip()\n                if before[-1:] == '=' and before[:-1].strip().isidentifier():\n                    yield 0, (\"E731 do not assign a lambda expression, use a \"\n                              \"def\")\n                break\n            if STARTSWITH_DEF_REGEX.match(line):\n                yield 0, \"E704 multiple statements on one line (def)\"\n            elif STARTSWITH_INDENT_STATEMENT_REGEX.match(line):\n                yield found, \"E701 multiple statements on one line (colon)\"\n        prev_found = found\n        found = line.find(':', found + 1)\n    found = line.find(';')\n    while -1 < found:\n        if found < last_char:\n            yield found, \"E702 multiple statements on one line (semicolon)\"\n        else:\n            yield found, \"E703 statement ends with a semicolon\"\n        found = line.find(';', found + 1)\n\n\n@register_check\ndef explicit_line_join(logical_line, tokens):\n    r\"\"\"Avoid explicit line join between brackets.\n\n    The preferred way of wrapping long lines is by using Python's\n    implied line continuation inside parentheses, brackets and braces.\n    Long lines can be broken over multiple lines by wrapping expressions\n    in parentheses.  These should be used in preference to using a\n    backslash for line continuation.\n\n    E502: aaa = [123, \\\\n       123]\n    E502: aaa = (\"bbb \" \\\\n       \"ccc\")\n\n    Okay: aaa = [123,\\n       123]\n    Okay: aaa = (\"bbb \"\\n       \"ccc\")\n    Okay: aaa = \"bbb \" \\\\n    \"ccc\"\n    Okay: aaa = 123  # \\\\\n    \"\"\"\n    prev_start = prev_end = parens = 0\n    comment = False\n    backslash = None\n    for token_type, text, start, end, line in tokens:\n        if token_type == tokenize.COMMENT:\n            comment = True\n        if start[0] != prev_start and parens and backslash and not comment:\n            yield backslash, \"E502 the backslash is redundant between brackets\"\n        if start[0] != prev_start:\n            comment = False  # Reset comment flag on newline\n        if end[0] != prev_end:\n            if line.rstrip('\\r\\n').endswith('\\\\'):\n                backslash = (end[0], len(line.splitlines()[-1]) - 1)\n            else:\n                backslash = None\n            prev_start = prev_end = end[0]\n        else:\n            prev_start = start[0]\n        if token_type == tokenize.OP:\n            if text in '([{':\n                parens += 1\n            elif text in ')]}':\n                parens -= 1\n\n\n# The % character is strictly speaking a binary operator, but the\n# common usage seems to be to put it next to the format parameters,\n# after a line break.\n_SYMBOLIC_OPS = frozenset(\"()[]{},:.;@=%~\") | frozenset((\"...\",))\n\n\ndef _is_binary_operator(token_type, text):\n    return (\n        token_type == tokenize.OP or\n        text in {'and', 'or'}\n    ) and (\n        text not in _SYMBOLIC_OPS\n    )\n\n\ndef _break_around_binary_operators(tokens):\n    \"\"\"Private function to reduce duplication.\n\n    This factors out the shared details between\n    :func:`break_before_binary_operator` and\n    :func:`break_after_binary_operator`.\n    \"\"\"\n    line_break = False\n    unary_context = True\n    # Previous non-newline token types and text\n    previous_token_type = None\n    previous_text = None\n    for token_type, text, start, end, line in tokens:\n        if token_type == tokenize.COMMENT:\n            continue\n        if ('\\n' in text or '\\r' in text) and token_type != tokenize.STRING:\n            line_break = True\n        else:\n            yield (token_type, text, previous_token_type, previous_text,\n                   line_break, unary_context, start)\n            unary_context = text in '([{,;'\n            line_break = False\n            previous_token_type = token_type\n            previous_text = text\n\n\n@register_check\ndef break_before_binary_operator(logical_line, tokens):\n    r\"\"\"\n    Avoid breaks before binary operators.\n\n    The preferred place to break around a binary operator is after the\n    operator, not before it.\n\n    W503: (width == 0\\n + height == 0)\n    W503: (width == 0\\n and height == 0)\n    W503: var = (1\\n       & ~2)\n    W503: var = (1\\n       / -2)\n    W503: var = (1\\n       + -1\\n       + -2)\n\n    Okay: foo(\\n    -x)\n    Okay: foo(x\\n    [])\n    Okay: x = '''\\n''' + ''\n    Okay: foo(x,\\n    -y)\n    Okay: foo(x,  # comment\\n    -y)\n    \"\"\"\n    for context in _break_around_binary_operators(tokens):\n        (token_type, text, previous_token_type, previous_text,\n         line_break, unary_context, start) = context\n        if (_is_binary_operator(token_type, text) and line_break and\n                not unary_context and\n                not _is_binary_operator(previous_token_type,\n                                        previous_text)):\n            yield start, \"W503 line break before binary operator\"\n\n\n@register_check\ndef break_after_binary_operator(logical_line, tokens):\n    r\"\"\"\n    Avoid breaks after binary operators.\n\n    The preferred place to break around a binary operator is before the\n    operator, not after it.\n\n    W504: (width == 0 +\\n height == 0)\n    W504: (width == 0 and\\n height == 0)\n    W504: var = (1 &\\n       ~2)\n\n    Okay: foo(\\n    -x)\n    Okay: foo(x\\n    [])\n    Okay: x = '''\\n''' + ''\n    Okay: x = '' + '''\\n'''\n    Okay: foo(x,\\n    -y)\n    Okay: foo(x,  # comment\\n    -y)\n\n    The following should be W504 but unary_context is tricky with these\n    Okay: var = (1 /\\n       -2)\n    Okay: var = (1 +\\n       -1 +\\n       -2)\n    \"\"\"\n    prev_start = None\n    for context in _break_around_binary_operators(tokens):\n        (token_type, text, previous_token_type, previous_text,\n         line_break, unary_context, start) = context\n        if (_is_binary_operator(previous_token_type, previous_text) and\n                line_break and\n                not unary_context and\n                not _is_binary_operator(token_type, text)):\n            yield prev_start, \"W504 line break after binary operator\"\n        prev_start = start\n\n\n@register_check\ndef comparison_to_singleton(logical_line, noqa):\n    r\"\"\"Comparison to singletons should use \"is\" or \"is not\".\n\n    Comparisons to singletons like None should always be done\n    with \"is\" or \"is not\", never the equality operators.\n\n    Okay: if arg is not None:\n    E711: if arg != None:\n    E711: if None == arg:\n    E712: if arg == True:\n    E712: if False == arg:\n\n    Also, beware of writing if x when you really mean if x is not None\n    -- e.g. when testing whether a variable or argument that defaults to\n    None was set to some other value.  The other value might have a type\n    (such as a container) that could be false in a boolean context!\n    \"\"\"\n    if noqa:\n        return\n\n    for match in COMPARE_SINGLETON_REGEX.finditer(logical_line):\n        singleton = match.group(1) or match.group(3)\n        same = (match.group(2) == '==')\n\n        msg = \"'if cond is %s:'\" % (('' if same else 'not ') + singleton)\n        if singleton in ('None',):\n            code = 'E711'\n        else:\n            code = 'E712'\n            nonzero = ((singleton == 'True' and same) or\n                       (singleton == 'False' and not same))\n            msg += \" or 'if %scond:'\" % ('' if nonzero else 'not ')\n        yield match.start(2), (\"%s comparison to %s should be %s\" %\n                               (code, singleton, msg))\n\n\n@register_check\ndef comparison_negative(logical_line):\n    r\"\"\"Negative comparison should be done using \"not in\" and \"is not\".\n\n    Okay: if x not in y:\\n    pass\n    Okay: assert (X in Y or X is Z)\n    Okay: if not (X in Y):\\n    pass\n    Okay: zz = x is not y\n    E713: Z = not X in Y\n    E713: if not X.B in Y:\\n    pass\n    E714: if not X is Y:\\n    pass\n    E714: Z = not X.B is Y\n    \"\"\"\n    match = COMPARE_NEGATIVE_REGEX.search(logical_line)\n    if match:\n        pos = match.start(1)\n        if match.group(2) == 'in':\n            yield pos, \"E713 test for membership should be 'not in'\"\n        else:\n            yield pos, \"E714 test for object identity should be 'is not'\"\n\n\n@register_check\ndef comparison_type(logical_line, noqa):\n    r\"\"\"Object type comparisons should `is` / `is not` / `isinstance()`.\n\n    Do not compare types directly.\n\n    Okay: if isinstance(obj, int):\n    Okay: if type(obj) is int:\n    E721: if type(obj) == type(1):\n    \"\"\"\n    match = COMPARE_TYPE_REGEX.search(logical_line)\n    if match and not noqa:\n        inst = match.group(1)\n        if inst and inst.isidentifier() and inst not in SINGLETONS:\n            return  # Allow comparison for types which are not obvious\n        yield (\n            match.start(),\n            \"E721 do not compare types, for exact checks use `is` / `is not`, \"\n            \"for instance checks use `isinstance()`\",\n        )\n\n\n@register_check\ndef bare_except(logical_line, noqa):\n    r\"\"\"When catching exceptions, mention specific exceptions when\n    possible.\n\n    Okay: except Exception:\n    Okay: except BaseException:\n    E722: except:\n    \"\"\"\n    if noqa:\n        return\n\n    match = BLANK_EXCEPT_REGEX.match(logical_line)\n    if match:\n        yield match.start(), \"E722 do not use bare 'except'\"\n\n\n@register_check\ndef ambiguous_identifier(logical_line, tokens):\n    r\"\"\"Never use the characters 'l', 'O', or 'I' as variable names.\n\n    In some fonts, these characters are indistinguishable from the\n    numerals one and zero. When tempted to use 'l', use 'L' instead.\n\n    Okay: L = 0\n    Okay: o = 123\n    Okay: i = 42\n    E741: l = 0\n    E741: O = 123\n    E741: I = 42\n\n    Variables can be bound in several other contexts, including class\n    and function definitions, lambda functions, 'global' and 'nonlocal'\n    statements, exception handlers, and 'with' and 'for' statements.\n    In addition, we have a special handling for function parameters.\n\n    Okay: except AttributeError as o:\n    Okay: with lock as L:\n    Okay: foo(l=12)\n    Okay: foo(l=I)\n    Okay: for a in foo(l=12):\n    Okay: lambda arg: arg * l\n    Okay: lambda a=l[I:5]: None\n    Okay: lambda x=a.I: None\n    Okay: if l >= 12:\n    E741: except AttributeError as O:\n    E741: with lock as l:\n    E741: global I\n    E741: nonlocal l\n    E741: def foo(l):\n    E741: def foo(l=12):\n    E741: l = foo(l=12)\n    E741: for l in range(10):\n    E741: [l for l in lines if l]\n    E741: lambda l: None\n    E741: lambda a=x[1:5], l: None\n    E741: lambda **l:\n    E741: def f(**l):\n    E742: class I(object):\n    E743: def l(x):\n    \"\"\"\n    func_depth = None  # set to brace depth if 'def' or 'lambda' is found\n    seen_colon = False  # set to true if we're done with function parameters\n    brace_depth = 0\n    idents_to_avoid = ('l', 'O', 'I')\n    prev_type, prev_text, prev_start, prev_end, __ = tokens[0]\n    for index in range(1, len(tokens)):\n        token_type, text, start, end, line = tokens[index]\n        ident = pos = None\n        # find function definitions\n        if prev_text in {'def', 'lambda'}:\n            func_depth = brace_depth\n            seen_colon = False\n        elif (\n                func_depth is not None and\n                text == ':' and\n                brace_depth == func_depth\n        ):\n            seen_colon = True\n        # update parameter parentheses level\n        if text in '([{':\n            brace_depth += 1\n        elif text in ')]}':\n            brace_depth -= 1\n        # identifiers on the lhs of an assignment operator\n        if text == ':=' or (text == '=' and brace_depth == 0):\n            if prev_text in idents_to_avoid:\n                ident = prev_text\n                pos = prev_start\n        # identifiers bound to values with 'as', 'for',\n        # 'global', or 'nonlocal'\n        if prev_text in ('as', 'for', 'global', 'nonlocal'):\n            if text in idents_to_avoid:\n                ident = text\n                pos = start\n        # function / lambda parameter definitions\n        if (\n                func_depth is not None and\n                not seen_colon and\n                index < len(tokens) - 1 and tokens[index + 1][1] in ':,=)' and\n                prev_text in {'lambda', ',', '*', '**', '('} and\n                text in idents_to_avoid\n        ):\n            ident = text\n            pos = start\n        if prev_text == 'class':\n            if text in idents_to_avoid:\n                yield start, \"E742 ambiguous class definition '%s'\" % text\n        if prev_text == 'def':\n            if text in idents_to_avoid:\n                yield start, \"E743 ambiguous function definition '%s'\" % text\n        if ident:\n            yield pos, \"E741 ambiguous variable name '%s'\" % ident\n        prev_text = text\n        prev_start = start\n\n\n@register_check\ndef python_3000_invalid_escape_sequence(logical_line, tokens, noqa):\n    r\"\"\"Invalid escape sequences are deprecated in Python 3.6.\n\n    Okay: regex = r'\\.png$'\n    W605: regex = '\\.png$'\n    \"\"\"\n    if noqa:\n        return\n\n    # https://docs.python.org/3/reference/lexical_analysis.html#string-and-bytes-literals\n    valid = [\n        '\\n',\n        '\\\\',\n        '\\'',\n        '\"',\n        'a',\n        'b',\n        'f',\n        'n',\n        'r',\n        't',\n        'v',\n        '0', '1', '2', '3', '4', '5', '6', '7',\n        'x',\n\n        # Escape sequences only recognized in string literals\n        'N',\n        'u',\n        'U',\n    ]\n\n    prefixes = []\n    for token_type, text, start, _, _ in tokens:\n        if token_type in {tokenize.STRING, FSTRING_START}:\n            # Extract string modifiers (e.g. u or r)\n            prefixes.append(text[:text.index(text[-1])].lower())\n\n        if token_type in {tokenize.STRING, FSTRING_MIDDLE}:\n            if 'r' not in prefixes[-1]:\n                start_line, start_col = start\n                pos = text.find('\\\\')\n                while pos >= 0:\n                    pos += 1\n                    if text[pos] not in valid:\n                        line = start_line + text.count('\\n', 0, pos)\n                        if line == start_line:\n                            col = start_col + pos\n                        else:\n                            col = pos - text.rfind('\\n', 0, pos) - 1\n                        yield (\n                            (line, col - 1),\n                            f\"W605 invalid escape sequence '\\\\{text[pos]}'\"\n                        )\n                    pos = text.find('\\\\', pos + 1)\n\n        if token_type in {tokenize.STRING, FSTRING_END}:\n            prefixes.pop()\n\n\n########################################################################\n@register_check\ndef maximum_doc_length(logical_line, max_doc_length, noqa, tokens):\n    r\"\"\"Limit all doc lines to a maximum of 72 characters.\n\n    For flowing long blocks of text (docstrings or comments), limiting\n    the length to 72 characters is recommended.\n\n    Reports warning W505\n    \"\"\"\n    if max_doc_length is None or noqa:\n        return\n\n    prev_token = None\n    skip_lines = set()\n    # Skip lines that\n    for token_type, text, start, end, line in tokens:\n        if token_type not in SKIP_COMMENTS.union([tokenize.STRING]):\n            skip_lines.add(line)\n\n    for token_type, text, start, end, line in tokens:\n        # Skip lines that aren't pure strings\n        if token_type == tokenize.STRING and skip_lines:\n            continue\n        if token_type in (tokenize.STRING, tokenize.COMMENT):\n            # Only check comment-only lines\n            if prev_token is None or prev_token in SKIP_TOKENS:\n                lines = line.splitlines()\n                for line_num, physical_line in enumerate(lines):\n                    if start[0] + line_num == 1 and line.startswith('#!'):\n                        return\n                    length = len(physical_line)\n                    chunks = physical_line.split()\n                    if token_type == tokenize.COMMENT:\n                        if (len(chunks) == 2 and\n                                length - len(chunks[-1]) < MAX_DOC_LENGTH):\n                            continue\n                    if len(chunks) == 1 and line_num + 1 < len(lines):\n                        if (len(chunks) == 1 and\n                                length - len(chunks[-1]) < MAX_DOC_LENGTH):\n                            continue\n                    if length > max_doc_length:\n                        doc_error = (start[0] + line_num, max_doc_length)\n                        yield (doc_error, \"W505 doc line too long \"\n                                          \"(%d > %d characters)\"\n                               % (length, max_doc_length))\n        prev_token = token_type\n\n\n########################################################################\n# Helper functions\n########################################################################\n\n\ndef readlines(filename):\n    \"\"\"Read the source code.\"\"\"\n    try:\n        with tokenize.open(filename) as f:\n            return f.readlines()\n    except (LookupError, SyntaxError, UnicodeError):\n        # Fall back if file encoding is improperly declared\n        with open(filename, encoding='latin-1') as f:\n            return f.readlines()\n\n\ndef stdin_get_value():\n    \"\"\"Read the value from stdin.\"\"\"\n    return io.TextIOWrapper(sys.stdin.buffer, errors='ignore').read()\n\n\nnoqa = lru_cache(512)(re.compile(r'# no(?:qa|pep8)\\b', re.I).search)\n\n\ndef expand_indent(line):\n    r\"\"\"Return the amount of indentation.\n\n    Tabs are expanded to the next multiple of 8.\n    \"\"\"\n    line = line.rstrip('\\n\\r')\n    if '\\t' not in line:\n        return len(line) - len(line.lstrip())\n    result = 0\n    for char in line:\n        if char == '\\t':\n            result = result // 8 * 8 + 8\n        elif char == ' ':\n            result += 1\n        else:\n            break\n    return result\n\n\ndef mute_string(text):\n    \"\"\"Replace contents with 'xxx' to prevent syntax matching.\"\"\"\n    # String modifiers (e.g. u or r)\n    start = text.index(text[-1]) + 1\n    end = len(text) - 1\n    # Triple quotes\n    if text[-3:] in ('\"\"\"', \"'''\"):\n        start += 2\n        end -= 2\n    return text[:start] + 'x' * (end - start) + text[end:]\n\n\ndef parse_udiff(diff, patterns=None, parent='.'):\n    \"\"\"Return a dictionary of matching lines.\"\"\"\n    # For each file of the diff, the entry key is the filename,\n    # and the value is a set of row numbers to consider.\n    rv = {}\n    path = nrows = None\n    for line in diff.splitlines():\n        if nrows:\n            if line[:1] != '-':\n                nrows -= 1\n            continue\n        if line[:3] == '@@ ':\n            hunk_match = HUNK_REGEX.match(line)\n            (row, nrows) = (int(g or '1') for g in hunk_match.groups())\n            rv[path].update(range(row, row + nrows))\n        elif line[:3] == '+++':\n            path = line[4:].split('\\t', 1)[0]\n            # Git diff will use (i)ndex, (w)ork tree, (c)ommit and\n            # (o)bject instead of a/b/c/d as prefixes for patches\n            if path[:2] in ('b/', 'w/', 'i/'):\n                path = path[2:]\n            rv[path] = set()\n    return {\n        os.path.join(parent, filepath): rows\n        for (filepath, rows) in rv.items()\n        if rows and filename_match(filepath, patterns)\n    }\n\n\ndef normalize_paths(value, parent=os.curdir):\n    \"\"\"Parse a comma-separated list of paths.\n\n    Return a list of absolute paths.\n    \"\"\"\n    if not value:\n        return []\n    if isinstance(value, list):\n        return value\n    paths = []\n    for path in value.split(','):\n        path = path.strip()\n        if '/' in path:\n            path = os.path.abspath(os.path.join(parent, path))\n        paths.append(path.rstrip('/'))\n    return paths\n\n\ndef filename_match(filename, patterns, default=True):\n    \"\"\"Check if patterns contains a pattern that matches filename.\n\n    If patterns is unspecified, this always returns True.\n    \"\"\"\n    if not patterns:\n        return default\n    return any(fnmatch(filename, pattern) for pattern in patterns)\n\n\ndef update_counts(s, counts):\n    r\"\"\"Adds one to the counts of each appearance of characters in s,\n        for characters in counts\"\"\"\n    for char in s:\n        if char in counts:\n            counts[char] += 1\n\n\ndef _is_eol_token(token):\n    return token[0] in NEWLINE or token[4][token[3][1]:].lstrip() == '\\\\\\n'\n\n\n########################################################################\n# Framework to run all checks\n########################################################################\n\n\nclass Checker:\n    \"\"\"Load a Python source file, tokenize it, check coding style.\"\"\"\n\n    def __init__(self, filename=None, lines=None,\n                 options=None, report=None, **kwargs):\n        if options is None:\n            options = StyleGuide(kwargs).options\n        else:\n            assert not kwargs\n        self._io_error = None\n        self._physical_checks = options.physical_checks\n        self._logical_checks = options.logical_checks\n        self._ast_checks = options.ast_checks\n        self.max_line_length = options.max_line_length\n        self.max_doc_length = options.max_doc_length\n        self.indent_size = options.indent_size\n        self.fstring_start = 0\n        self.multiline = False  # in a multiline string?\n        self.hang_closing = options.hang_closing\n        self.indent_size = options.indent_size\n        self.verbose = options.verbose\n        self.filename = filename\n        # Dictionary where a checker can store its custom state.\n        self._checker_states = {}\n        if filename is None:\n            self.filename = 'stdin'\n            self.lines = lines or []\n        elif filename == '-':\n            self.filename = 'stdin'\n            self.lines = stdin_get_value().splitlines(True)\n        elif lines is None:\n            try:\n                self.lines = readlines(filename)\n            except OSError:\n                (exc_type, exc) = sys.exc_info()[:2]\n                self._io_error = f'{exc_type.__name__}: {exc}'\n                self.lines = []\n        else:\n            self.lines = lines\n        if self.lines:\n            ord0 = ord(self.lines[0][0])\n            if ord0 in (0xef, 0xfeff):  # Strip the UTF-8 BOM\n                if ord0 == 0xfeff:\n                    self.lines[0] = self.lines[0][1:]\n                elif self.lines[0][:3] == '\\xef\\xbb\\xbf':\n                    self.lines[0] = self.lines[0][3:]\n        self.report = report or options.report\n        self.report_error = self.report.error\n        self.noqa = False\n\n    def report_invalid_syntax(self):\n        \"\"\"Check if the syntax is valid.\"\"\"\n        (exc_type, exc) = sys.exc_info()[:2]\n        if len(exc.args) > 1:\n            offset = exc.args[1]\n            if len(offset) > 2:\n                offset = offset[1:3]\n        else:\n            offset = (1, 0)\n        self.report_error(offset[0], offset[1] or 0,\n                          f'E901 {exc_type.__name__}: {exc.args[0]}',\n                          self.report_invalid_syntax)\n\n    def readline(self):\n        \"\"\"Get the next line from the input buffer.\"\"\"\n        if self.line_number >= self.total_lines:\n            return ''\n        line = self.lines[self.line_number]\n        self.line_number += 1\n        if self.indent_char is None and line[:1] in WHITESPACE:\n            self.indent_char = line[0]\n        return line\n\n    def run_check(self, check, argument_names):\n        \"\"\"Run a check plugin.\"\"\"\n        arguments = [getattr(self, name) for name in argument_names]\n        return check(*arguments)\n\n    def init_checker_state(self, name, argument_names):\n        \"\"\"Prepare custom state for the specific checker plugin.\"\"\"\n        if 'checker_state' in argument_names:\n            self.checker_state = self._checker_states.setdefault(name, {})\n\n    def check_physical(self, line):\n        \"\"\"Run all physical checks on a raw input line.\"\"\"\n        self.physical_line = line\n        for name, check, argument_names in self._physical_checks:\n            self.init_checker_state(name, argument_names)\n            result = self.run_check(check, argument_names)\n            if result is not None:\n                (offset, text) = result\n                self.report_error(self.line_number, offset, text, check)\n                if text[:4] == 'E101':\n                    self.indent_char = line[0]\n\n    def build_tokens_line(self):\n        \"\"\"Build a logical line from tokens.\"\"\"\n        logical = []\n        comments = []\n        length = 0\n        prev_row = prev_col = mapping = None\n        for token_type, text, start, end, line in self.tokens:\n            if token_type in SKIP_TOKENS:\n                continue\n            if not mapping:\n                mapping = [(0, start)]\n            if token_type == tokenize.COMMENT:\n                comments.append(text)\n                continue\n            if token_type == tokenize.STRING:\n                text = mute_string(text)\n            elif token_type == FSTRING_MIDDLE:  # pragma: >=3.12 cover\n                # fstring tokens are \"unescaped\" braces -- re-escape!\n                brace_count = text.count('{') + text.count('}')\n                text = 'x' * (len(text) + brace_count)\n                end = (end[0], end[1] + brace_count)\n            if prev_row:\n                (start_row, start_col) = start\n                if prev_row != start_row:    # different row\n                    prev_text = self.lines[prev_row - 1][prev_col - 1]\n                    if prev_text == ',' or (prev_text not in '{[(' and\n                                            text not in '}])'):\n                        text = ' ' + text\n                elif prev_col != start_col:  # different column\n                    text = line[prev_col:start_col] + text\n            logical.append(text)\n            length += len(text)\n            mapping.append((length, end))\n            (prev_row, prev_col) = end\n        self.logical_line = ''.join(logical)\n        self.noqa = comments and noqa(''.join(comments))\n        return mapping\n\n    def check_logical(self):\n        \"\"\"Build a line from tokens and run all logical checks on it.\"\"\"\n        self.report.increment_logical_line()\n        mapping = self.build_tokens_line()\n        if not mapping:\n            return\n\n        mapping_offsets = [offset for offset, _ in mapping]\n        (start_row, start_col) = mapping[0][1]\n        start_line = self.lines[start_row - 1]\n        self.indent_level = expand_indent(start_line[:start_col])\n        if self.blank_before < self.blank_lines:\n            self.blank_before = self.blank_lines\n        if self.verbose >= 2:\n            print(self.logical_line[:80].rstrip())\n        for name, check, argument_names in self._logical_checks:\n            if self.verbose >= 4:\n                print('   ' + name)\n            self.init_checker_state(name, argument_names)\n            for offset, text in self.run_check(check, argument_names) or ():\n                if not isinstance(offset, tuple):\n                    # As mappings are ordered, bisecting is a fast way\n                    # to find a given offset in them.\n                    token_offset, pos = mapping[bisect.bisect_left(\n                        mapping_offsets, offset)]\n                    offset = (pos[0], pos[1] + offset - token_offset)\n                self.report_error(offset[0], offset[1], text, check)\n        if self.logical_line:\n            self.previous_indent_level = self.indent_level\n            self.previous_logical = self.logical_line\n            if not self.indent_level:\n                self.previous_unindented_logical_line = self.logical_line\n        self.blank_lines = 0\n        self.tokens = []\n\n    def check_ast(self):\n        \"\"\"Build the file's AST and run all AST checks.\"\"\"\n        try:\n            tree = compile(''.join(self.lines), '', 'exec', PyCF_ONLY_AST)\n        except (ValueError, SyntaxError, TypeError):\n            return self.report_invalid_syntax()\n        for name, cls, __ in self._ast_checks:\n            checker = cls(tree, self.filename)\n            for lineno, offset, text, check in checker.run():\n                if not self.lines or not noqa(self.lines[lineno - 1]):\n                    self.report_error(lineno, offset, text, check)\n\n    def generate_tokens(self):\n        \"\"\"Tokenize file, run physical line checks and yield tokens.\"\"\"\n        if self._io_error:\n            self.report_error(1, 0, 'E902 %s' % self._io_error, readlines)\n        tokengen = tokenize.generate_tokens(self.readline)\n        try:\n            prev_physical = ''\n            for token in tokengen:\n                if token[2][0] > self.total_lines:\n                    return\n                self.noqa = token[4] and noqa(token[4])\n                self.maybe_check_physical(token, prev_physical)\n                yield token\n                prev_physical = token[4]\n        except (SyntaxError, tokenize.TokenError):\n            self.report_invalid_syntax()\n\n    def maybe_check_physical(self, token, prev_physical):\n        \"\"\"If appropriate for token, check current physical line(s).\"\"\"\n        # Called after every token, but act only on end of line.\n\n        if token.type == FSTRING_START:  # pragma: >=3.12 cover\n            self.fstring_start = token.start[0]\n        # a newline token ends a single physical line.\n        elif _is_eol_token(token):\n            # if the file does not end with a newline, the NEWLINE\n            # token is inserted by the parser, but it does not contain\n            # the previous physical line in `token[4]`\n            if token.line == '':\n                self.check_physical(prev_physical)\n            else:\n                self.check_physical(token.line)\n        elif (\n                token.type == tokenize.STRING and '\\n' in token.string or\n                token.type == FSTRING_END\n        ):\n            # Less obviously, a string that contains newlines is a\n            # multiline string, either triple-quoted or with internal\n            # newlines backslash-escaped. Check every physical line in\n            # the string *except* for the last one: its newline is\n            # outside of the multiline string, so we consider it a\n            # regular physical line, and will check it like any other\n            # physical line.\n            #\n            # Subtleties:\n            # - we don't *completely* ignore the last line; if it\n            #   contains the magical \"# noqa\" comment, we disable all\n            #   physical checks for the entire multiline string\n            # - have to wind self.line_number back because initially it\n            #   points to the last line of the string, and we want\n            #   check_physical() to give accurate feedback\n            if noqa(token.line):\n                return\n            if token.type == FSTRING_END:  # pragma: >=3.12 cover\n                start = self.fstring_start\n            else:\n                start = token.start[0]\n            end = token.end[0]\n\n            self.multiline = True\n            self.line_number = start\n            for line_number in range(start, end):\n                self.check_physical(self.lines[line_number - 1] + '\\n')\n                self.line_number += 1\n            self.multiline = False\n\n    def check_all(self, expected=None, line_offset=0):\n        \"\"\"Run all checks on the input file.\"\"\"\n        self.report.init_file(self.filename, self.lines, expected, line_offset)\n        self.total_lines = len(self.lines)\n        if self._ast_checks:\n            self.check_ast()\n        self.line_number = 0\n        self.indent_char = None\n        self.indent_level = self.previous_indent_level = 0\n        self.previous_logical = ''\n        self.previous_unindented_logical_line = ''\n        self.tokens = []\n        self.blank_lines = self.blank_before = 0\n        parens = 0\n        for token in self.generate_tokens():\n            self.tokens.append(token)\n            token_type, text = token[0:2]\n            if self.verbose >= 3:\n                if token[2][0] == token[3][0]:\n                    pos = '[{}:{}]'.format(token[2][1] or '', token[3][1])\n                else:\n                    pos = 'l.%s' % token[3][0]\n                print('l.%s\\t%s\\t%s\\t%r' %\n                      (token[2][0], pos, tokenize.tok_name[token[0]], text))\n            if token_type == tokenize.OP:\n                if text in '([{':\n                    parens += 1\n                elif text in '}])':\n                    parens -= 1\n            elif not parens:\n                if token_type in NEWLINE:\n                    if token_type == tokenize.NEWLINE:\n                        self.check_logical()\n                        self.blank_before = 0\n                    elif len(self.tokens) == 1:\n                        # The physical line contains only this token.\n                        self.blank_lines += 1\n                        del self.tokens[0]\n                    else:\n                        self.check_logical()\n        if self.tokens:\n            self.check_physical(self.lines[-1])\n            self.check_logical()\n        return self.report.get_file_results()\n\n\nclass BaseReport:\n    \"\"\"Collect the results of the checks.\"\"\"\n\n    print_filename = False\n\n    def __init__(self, options):\n        self._benchmark_keys = options.benchmark_keys\n        self._ignore_code = options.ignore_code\n        # Results\n        self.elapsed = 0\n        self.total_errors = 0\n        self.counters = dict.fromkeys(self._benchmark_keys, 0)\n        self.messages = {}\n\n    def start(self):\n        \"\"\"Start the timer.\"\"\"\n        self._start_time = time.time()\n\n    def stop(self):\n        \"\"\"Stop the timer.\"\"\"\n        self.elapsed = time.time() - self._start_time\n\n    def init_file(self, filename, lines, expected, line_offset):\n        \"\"\"Signal a new file.\"\"\"\n        self.filename = filename\n        self.lines = lines\n        self.expected = expected or ()\n        self.line_offset = line_offset\n        self.file_errors = 0\n        self.counters['files'] += 1\n        self.counters['physical lines'] += len(lines)\n\n    def increment_logical_line(self):\n        \"\"\"Signal a new logical line.\"\"\"\n        self.counters['logical lines'] += 1\n\n    def error(self, line_number, offset, text, check):\n        \"\"\"Report an error, according to options.\"\"\"\n        code = text[:4]\n        if self._ignore_code(code):\n            return\n        if code in self.counters:\n            self.counters[code] += 1\n        else:\n            self.counters[code] = 1\n            self.messages[code] = text[5:]\n        # Don't care about expected errors or warnings\n        if code in self.expected:\n            return\n        if self.print_filename and not self.file_errors:\n            print(self.filename)\n        self.file_errors += 1\n        self.total_errors += 1\n        return code\n\n    def get_file_results(self):\n        \"\"\"Return the count of errors and warnings for this file.\"\"\"\n        return self.file_errors\n\n    def get_count(self, prefix=''):\n        \"\"\"Return the total count of errors and warnings.\"\"\"\n        return sum(self.counters[key]\n                   for key in self.messages if key.startswith(prefix))\n\n    def get_statistics(self, prefix=''):\n        \"\"\"Get statistics for message codes that start with the prefix.\n\n        prefix='' matches all errors and warnings\n        prefix='E' matches all errors\n        prefix='W' matches all warnings\n        prefix='E4' matches all errors that have to do with imports\n        \"\"\"\n        return ['%-7s %s %s' % (self.counters[key], key, self.messages[key])\n                for key in sorted(self.messages) if key.startswith(prefix)]\n\n    def print_statistics(self, prefix=''):\n        \"\"\"Print overall statistics (number of errors and warnings).\"\"\"\n        for line in self.get_statistics(prefix):\n            print(line)\n\n    def print_benchmark(self):\n        \"\"\"Print benchmark numbers.\"\"\"\n        print('{:<7.2f} {}'.format(self.elapsed, 'seconds elapsed'))\n        if self.elapsed:\n            for key in self._benchmark_keys:\n                print('%-7d %s per second (%d total)' %\n                      (self.counters[key] / self.elapsed, key,\n                       self.counters[key]))\n\n\nclass FileReport(BaseReport):\n    \"\"\"Collect the results of the checks and print the filenames.\"\"\"\n\n    print_filename = True\n\n\nclass StandardReport(BaseReport):\n    \"\"\"Collect and print the results of the checks.\"\"\"\n\n    def __init__(self, options):\n        super().__init__(options)\n        self._fmt = REPORT_FORMAT.get(options.format.lower(),\n                                      options.format)\n        self._repeat = options.repeat\n        self._show_source = options.show_source\n        self._show_pep8 = options.show_pep8\n\n    def init_file(self, filename, lines, expected, line_offset):\n        \"\"\"Signal a new file.\"\"\"\n        self._deferred_print = []\n        return super().init_file(\n            filename, lines, expected, line_offset)\n\n    def error(self, line_number, offset, text, check):\n        \"\"\"Report an error, according to options.\"\"\"\n        code = super().error(line_number, offset, text, check)\n        if code and (self.counters[code] == 1 or self._repeat):\n            self._deferred_print.append(\n                (line_number, offset, code, text[5:], check.__doc__))\n        return code\n\n    def get_file_results(self):\n        \"\"\"Print results and return the overall count for this file.\"\"\"\n        self._deferred_print.sort()\n        for line_number, offset, code, text, doc in self._deferred_print:\n            print(self._fmt % {\n                'path': self.filename,\n                'row': self.line_offset + line_number, 'col': offset + 1,\n                'code': code, 'text': text,\n            })\n            if self._show_source:\n                if line_number > len(self.lines):\n                    line = ''\n                else:\n                    line = self.lines[line_number - 1]\n                print(line.rstrip())\n                print(re.sub(r'\\S', ' ', line[:offset]) + '^')\n            if self._show_pep8 and doc:\n                print('    ' + doc.strip())\n\n            # stdout is block buffered when not stdout.isatty().\n            # line can be broken where buffer boundary since other\n            # processes write to same file.\n            # flush() after print() to avoid buffer boundary.\n            # Typical buffer size is 8192. line written safely when\n            # len(line) < 8192.\n            sys.stdout.flush()\n        return self.file_errors\n\n\nclass DiffReport(StandardReport):\n    \"\"\"Collect and print the results for the changed lines only.\"\"\"\n\n    def __init__(self, options):\n        super().__init__(options)\n        self._selected = options.selected_lines\n\n    def error(self, line_number, offset, text, check):\n        if line_number not in self._selected[self.filename]:\n            return\n        return super().error(line_number, offset, text, check)\n\n\nclass StyleGuide:\n    \"\"\"Initialize a PEP-8 instance with few options.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        # build options from the command line\n        self.checker_class = kwargs.pop('checker_class', Checker)\n        parse_argv = kwargs.pop('parse_argv', False)\n        config_file = kwargs.pop('config_file', False)\n        parser = kwargs.pop('parser', None)\n        # build options from dict\n        options_dict = dict(*args, **kwargs)\n        arglist = None if parse_argv else options_dict.get('paths', None)\n        verbose = options_dict.get('verbose', None)\n        options, self.paths = process_options(\n            arglist, parse_argv, config_file, parser, verbose)\n        if options_dict:\n            options.__dict__.update(options_dict)\n            if 'paths' in options_dict:\n                self.paths = options_dict['paths']\n\n        self.runner = self.input_file\n        self.options = options\n\n        if not options.reporter:\n            options.reporter = BaseReport if options.quiet else StandardReport\n\n        options.select = tuple(options.select or ())\n        if not (options.select or options.ignore) and DEFAULT_IGNORE:\n            # The default choice: ignore controversial checks\n            options.ignore = tuple(DEFAULT_IGNORE.split(','))\n        else:\n            # Ignore all checks which are not explicitly selected\n            options.ignore = ('',) if options.select else tuple(options.ignore)\n        options.benchmark_keys = BENCHMARK_KEYS[:]\n        options.ignore_code = self.ignore_code\n        options.physical_checks = self.get_checks('physical_line')\n        options.logical_checks = self.get_checks('logical_line')\n        options.ast_checks = self.get_checks('tree')\n        self.init_report()\n\n    def init_report(self, reporter=None):\n        \"\"\"Initialize the report instance.\"\"\"\n        self.options.report = (reporter or self.options.reporter)(self.options)\n        return self.options.report\n\n    def check_files(self, paths=None):\n        \"\"\"Run all checks on the paths.\"\"\"\n        if paths is None:\n            paths = self.paths\n        report = self.options.report\n        runner = self.runner\n        report.start()\n        try:\n            for path in paths:\n                if os.path.isdir(path):\n                    self.input_dir(path)\n                elif not self.excluded(path):\n                    runner(path)\n        except KeyboardInterrupt:\n            print('... stopped')\n        report.stop()\n        return report\n\n    def input_file(self, filename, lines=None, expected=None, line_offset=0):\n        \"\"\"Run all checks on a Python source file.\"\"\"\n        if self.options.verbose:\n            print('checking %s' % filename)\n        fchecker = self.checker_class(\n            filename, lines=lines, options=self.options)\n        return fchecker.check_all(expected=expected, line_offset=line_offset)\n\n    def input_dir(self, dirname):\n        \"\"\"Check all files in this directory and all subdirectories.\"\"\"\n        dirname = dirname.rstrip('/')\n        if self.excluded(dirname):\n            return 0\n        counters = self.options.report.counters\n        verbose = self.options.verbose\n        filepatterns = self.options.filename\n        runner = self.runner\n        for root, dirs, files in os.walk(dirname):\n            if verbose:\n                print('directory ' + root)\n            counters['directories'] += 1\n            for subdir in sorted(dirs):\n                if self.excluded(subdir, root):\n                    dirs.remove(subdir)\n            for filename in sorted(files):\n                # contain a pattern that matches?\n                if (\n                    filename_match(filename, filepatterns) and\n                    not self.excluded(filename, root)\n                ):\n                    runner(os.path.join(root, filename))\n\n    def excluded(self, filename, parent=None):\n        \"\"\"Check if the file should be excluded.\n\n        Check if 'options.exclude' contains a pattern matching filename.\n        \"\"\"\n        if not self.options.exclude:\n            return False\n        basename = os.path.basename(filename)\n        if filename_match(basename, self.options.exclude):\n            return True\n        if parent:\n            filename = os.path.join(parent, filename)\n        filename = os.path.abspath(filename)\n        return filename_match(filename, self.options.exclude)\n\n    def ignore_code(self, code):\n        \"\"\"Check if the error code should be ignored.\n\n        If 'options.select' contains a prefix of the error code,\n        return False.  Else, if 'options.ignore' contains a prefix of\n        the error code, return True.\n        \"\"\"\n        if len(code) < 4 and any(s.startswith(code)\n                                 for s in self.options.select):\n            return False\n        return (code.startswith(self.options.ignore) and\n                not code.startswith(self.options.select))\n\n    def get_checks(self, argument_name):\n        \"\"\"Get all the checks for this category.\n\n        Find all globally visible functions where the first argument\n        name starts with argument_name and which contain selected tests.\n        \"\"\"\n        checks = []\n        for check, attrs in _checks[argument_name].items():\n            (codes, args) = attrs\n            if any(not (code and self.ignore_code(code)) for code in codes):\n                checks.append((check.__name__, check, args))\n        return sorted(checks)\n\n\ndef get_parser(prog='pycodestyle', version=__version__):\n    \"\"\"Create the parser for the program.\"\"\"\n    parser = OptionParser(prog=prog, version=version,\n                          usage=\"%prog [options] input ...\")\n    parser.config_options = [\n        'exclude', 'filename', 'select', 'ignore', 'max-line-length',\n        'max-doc-length', 'indent-size', 'hang-closing', 'count', 'format',\n        'quiet', 'show-pep8', 'show-source', 'statistics', 'verbose']\n    parser.add_option('-v', '--verbose', default=0, action='count',\n                      help=\"print status messages, or debug with -vv\")\n    parser.add_option('-q', '--quiet', default=0, action='count',\n                      help=\"report only file names, or nothing with -qq\")\n    parser.add_option('-r', '--repeat', default=True, action='store_true',\n                      help=\"(obsolete) show all occurrences of the same error\")\n    parser.add_option('--first', action='store_false', dest='repeat',\n                      help=\"show first occurrence of each error\")\n    parser.add_option('--exclude', metavar='patterns', default=DEFAULT_EXCLUDE,\n                      help=\"exclude files or directories which match these \"\n                           \"comma separated patterns (default: %default)\")\n    parser.add_option('--filename', metavar='patterns', default='*.py',\n                      help=\"when parsing directories, only check filenames \"\n                           \"matching these comma separated patterns \"\n                           \"(default: %default)\")\n    parser.add_option('--select', metavar='errors', default='',\n                      help=\"select errors and warnings (e.g. E,W6)\")\n    parser.add_option('--ignore', metavar='errors', default='',\n                      help=\"skip errors and warnings (e.g. E4,W) \"\n                           \"(default: %s)\" % DEFAULT_IGNORE)\n    parser.add_option('--show-source', action='store_true',\n                      help=\"show source code for each error\")\n    parser.add_option('--show-pep8', action='store_true',\n                      help=\"show text of PEP 8 for each error \"\n                           \"(implies --first)\")\n    parser.add_option('--statistics', action='store_true',\n                      help=\"count errors and warnings\")\n    parser.add_option('--count', action='store_true',\n                      help=\"print total number of errors and warnings \"\n                           \"to standard error and set exit code to 1 if \"\n                           \"total is not null\")\n    parser.add_option('--max-line-length', type='int', metavar='n',\n                      default=MAX_LINE_LENGTH,\n                      help=\"set maximum allowed line length \"\n                           \"(default: %default)\")\n    parser.add_option('--max-doc-length', type='int', metavar='n',\n                      default=None,\n                      help=\"set maximum allowed doc line length and perform \"\n                           \"these checks (unchecked if not set)\")\n    parser.add_option('--indent-size', type='int', metavar='n',\n                      default=INDENT_SIZE,\n                      help=\"set how many spaces make up an indent \"\n                           \"(default: %default)\")\n    parser.add_option('--hang-closing', action='store_true',\n                      help=\"hang closing bracket instead of matching \"\n                           \"indentation of opening bracket's line\")\n    parser.add_option('--format', metavar='format', default='default',\n                      help=\"set the error format [default|pylint|<custom>]\")\n    parser.add_option('--diff', action='store_true',\n                      help=\"report changes only within line number ranges in \"\n                           \"the unified diff received on STDIN\")\n    group = parser.add_option_group(\"Testing Options\")\n    group.add_option('--benchmark', action='store_true',\n                     help=\"measure processing speed\")\n    return parser\n\n\ndef read_config(options, args, arglist, parser):\n    \"\"\"Read and parse configurations.\n\n    If a config file is specified on the command line with the\n    \"--config\" option, then only it is used for configuration.\n\n    Otherwise, the user configuration (~/.config/pycodestyle) and any\n    local configurations in the current directory or above will be\n    merged together (in that order) using the read method of\n    ConfigParser.\n    \"\"\"\n    config = configparser.RawConfigParser()\n\n    cli_conf = options.config\n\n    local_dir = os.curdir\n\n    if USER_CONFIG and os.path.isfile(USER_CONFIG):\n        if options.verbose:\n            print('user configuration: %s' % USER_CONFIG)\n        config.read(USER_CONFIG)\n\n    parent = tail = args and os.path.abspath(os.path.commonprefix(args))\n    while tail:\n        if config.read(os.path.join(parent, fn) for fn in PROJECT_CONFIG):\n            local_dir = parent\n            if options.verbose:\n                print('local configuration: in %s' % parent)\n            break\n        (parent, tail) = os.path.split(parent)\n\n    if cli_conf and os.path.isfile(cli_conf):\n        if options.verbose:\n            print('cli configuration: %s' % cli_conf)\n        config.read(cli_conf)\n\n    pycodestyle_section = None\n    if config.has_section(parser.prog):\n        pycodestyle_section = parser.prog\n    elif config.has_section('pep8'):\n        pycodestyle_section = 'pep8'  # Deprecated\n        warnings.warn('[pep8] section is deprecated. Use [pycodestyle].')\n\n    if pycodestyle_section:\n        option_list = {o.dest: o.type or o.action for o in parser.option_list}\n\n        # First, read the default values\n        (new_options, __) = parser.parse_args([])\n\n        # Second, parse the configuration\n        for opt in config.options(pycodestyle_section):\n            if opt.replace('_', '-') not in parser.config_options:\n                print(\"  unknown option '%s' ignored\" % opt)\n                continue\n            if options.verbose > 1:\n                print(\"  {} = {}\".format(opt,\n                                         config.get(pycodestyle_section, opt)))\n            normalized_opt = opt.replace('-', '_')\n            opt_type = option_list[normalized_opt]\n            if opt_type in ('int', 'count'):\n                value = config.getint(pycodestyle_section, opt)\n            elif opt_type in ('store_true', 'store_false'):\n                value = config.getboolean(pycodestyle_section, opt)\n            else:\n                value = config.get(pycodestyle_section, opt)\n                if normalized_opt == 'exclude':\n                    value = normalize_paths(value, local_dir)\n            setattr(new_options, normalized_opt, value)\n\n        # Third, overwrite with the command-line options\n        (options, __) = parser.parse_args(arglist, values=new_options)\n    return options\n\n\ndef process_options(arglist=None, parse_argv=False, config_file=None,\n                    parser=None, verbose=None):\n    \"\"\"Process options passed either via arglist or command line args.\n\n    Passing in the ``config_file`` parameter allows other tools, such as\n    flake8 to specify their own options to be processed in pycodestyle.\n    \"\"\"\n    if not parser:\n        parser = get_parser()\n    if not parser.has_option('--config'):\n        group = parser.add_option_group(\"Configuration\", description=(\n            \"The project options are read from the [%s] section of the \"\n            \"tox.ini file or the setup.cfg file located in any parent folder \"\n            \"of the path(s) being processed.  Allowed options are: %s.\" %\n            (parser.prog, ', '.join(parser.config_options))))\n        group.add_option('--config', metavar='path', default=config_file,\n                         help=\"user config file location\")\n    # Don't read the command line if the module is used as a library.\n    if not arglist and not parse_argv:\n        arglist = []\n    # If parse_argv is True and arglist is None, arguments are\n    # parsed from the command line (sys.argv)\n    (options, args) = parser.parse_args(arglist)\n    options.reporter = None\n\n    # If explicitly specified verbosity, override any `-v` CLI flag\n    if verbose is not None:\n        options.verbose = verbose\n\n    if parse_argv and not args:\n        if options.diff or any(os.path.exists(name)\n                               for name in PROJECT_CONFIG):\n            args = ['.']\n        else:\n            parser.error('input not specified')\n    options = read_config(options, args, arglist, parser)\n    options.reporter = parse_argv and options.quiet == 1 and FileReport\n\n    options.filename = _parse_multi_options(options.filename)\n    options.exclude = normalize_paths(options.exclude)\n    options.select = _parse_multi_options(options.select)\n    options.ignore = _parse_multi_options(options.ignore)\n\n    if options.diff:\n        options.reporter = DiffReport\n        stdin = stdin_get_value()\n        options.selected_lines = parse_udiff(stdin, options.filename, args[0])\n        args = sorted(options.selected_lines)\n\n    return options, args\n\n\ndef _parse_multi_options(options, split_token=','):\n    r\"\"\"Split and strip and discard empties.\n\n    Turns the following:\n\n    A,\n    B,\n\n    into [\"A\", \"B\"]\n    \"\"\"\n    if options:\n        return [o.strip() for o in options.split(split_token) if o.strip()]\n    else:\n        return options\n\n\ndef _main():\n    \"\"\"Parse options and run checks on Python source.\"\"\"\n    import signal\n\n    # Handle \"Broken pipe\" gracefully\n    try:\n        signal.signal(signal.SIGPIPE, lambda signum, frame: sys.exit(1))\n    except AttributeError:\n        pass    # not supported on Windows\n\n    style_guide = StyleGuide(parse_argv=True)\n    options = style_guide.options\n\n    report = style_guide.check_files()\n\n    if options.statistics:\n        report.print_statistics()\n\n    if options.benchmark:\n        report.print_benchmark()\n\n    if report.total_errors:\n        if options.count:\n            sys.stderr.write(str(report.total_errors) + '\\n')\n        sys.exit(1)\n\n\nif __name__ == '__main__':\n    _main()\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 1.396484375,
          "content": "[metadata]\nname = pycodestyle\nversion = attr: pycodestyle.__version__\ndescription = Python style guide checker\nlong_description = file: README.rst\nlong_description_content_type = text/x-rst\nurl = https://pycodestyle.pycqa.org/\nauthor = Johann C. Rocholl\nauthor_email = johann@rocholl.net\nmaintainer = Ian Lee\nmaintainer_email = IanLee1521@gmail.com\nlicense = MIT\nlicense_files = LICENSE\nclassifiers =\n    Development Status :: 5 - Production/Stable\n    Environment :: Console\n    Intended Audience :: Developers\n    License :: OSI Approved :: MIT License\n    Operating System :: OS Independent\n    Programming Language :: Python\n    Programming Language :: Python :: 3\n    Programming Language :: Python :: 3 :: Only\n    Programming Language :: Python :: Implementation :: CPython\n    Programming Language :: Python :: Implementation :: PyPy\n    Topic :: Software Development :: Libraries :: Python Modules\nkeywords = pycodestyle, pep8, PEP 8, PEP-8, PEP8\nproject_urls =\n    Changes=https://pycodestyle.pycqa.org/en/latest/developer.html#changes\n\n[options]\npy_modules = pycodestyle\npython_requires = >=3.9\ninclude_package_data = True\nzip_safe = False\n\n[options.entry_points]\nconsole_scripts =\n    pycodestyle = pycodestyle:_main\n\n[bdist_wheel]\nuniversal = 1\n\n[pycodestyle]\nignore = E226,E24,W504\nmax_line_length = 79\nmax_doc_length = 72\n\n[coverage:run]\nplugins = covdefaults\nomit = testing/data\n\n[coverage:report]\nfail_under = 93\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.0361328125,
          "content": "from setuptools import setup\nsetup()\n"
        },
        {
          "name": "testing",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 0.5673828125,
          "content": "# Tox (https://tox.readthedocs.io/en/latest/) is a tool for running tests\n# in multiple virtualenvs. This configuration file will run the\n# test suite on all supported python versions. To use it, \"pip install tox\"\n# and then run \"tox\" from this directory.\n\n[tox]\nenvlist = py, pypy3\nskip_missing_interpreters = True\n\n[testenv]\ndeps =\n    covdefaults\n    coverage\n    pytest\ncommands =\n    python -m pycodestyle --statistics pycodestyle.py\n    coverage run -m pytest tests\n    coverage report\n\n[testenv:flake8]\nskip_install = true\ndeps = flake8\ncommands =\n    flake8 pycodestyle.py\n"
        }
      ]
    }
  ]
}