{
  "metadata": {
    "timestamp": 1736560904830,
    "page": 630,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "sczhou/ProPainter",
      "stars": 5817,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.3935546875,
          "content": ".vscode\n\n# ignored files\nversion.py\n\n# ignored files with suffix\n*.html\n# *.png\n# *.jpeg\n# *.jpg\n# *.gif\n*.pt\n*.pth\n*.dat\n*.zip\n\n# template\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# project\nexperiments_model/\nunreleased/\nresults_eval/\nresults/\n*debug*\n*old*\n*.sh"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.80859375,
          "content": "# S-Lab License 1.0  \n\nCopyright 2023 S-Lab\n\nRedistribution and use for non-commercial purpose in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n4. In the event that redistribution and/or use for commercial purpose in source or binary forms, with or without modification is required, please contact the contributor(s) of the work.\n\n\n---\nFor inquiries or to obtain permission for commercial use, please consult Dr. Shangchen Zhou (shangchenzhou@gmail.com) and Prof. Chen Change Loy (ccloy@ntu.edu.sg)."
        },
        {
          "name": "RAFT",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.291015625,
          "content": "<div align=\"center\">\n\n<div class=\"logo\">\n   <a href=\"https://shangchenzhou.com/projects/ProPainter/\">\n      <img src=\"assets/propainter_logo1_glow.png\" style=\"width:180px\">\n   </a>\n</div>\n\n<h1>ProPainter: Improving Propagation and Transformer for Video Inpainting</h1>\n\n<div>\n    <a href='https://shangchenzhou.com/' target='_blank'>Shangchen Zhou</a>&emsp;\n    <a href='https://li-chongyi.github.io/' target='_blank'>Chongyi Li</a>&emsp;\n    <a href='https://ckkelvinchan.github.io/' target='_blank'>Kelvin C.K. Chan</a>&emsp;\n    <a href='https://www.mmlab-ntu.com/person/ccloy/' target='_blank'>Chen Change Loy</a>\n</div>\n<div>\n    S-Lab, Nanyang Technological University&emsp; \n</div>\n\n<div>\n    <strong>ICCV 2023</strong>\n</div>\n\n<div>\n    <h4 align=\"center\">\n        <a href=\"https://shangchenzhou.com/projects/ProPainter\" target='_blank'>\n        <img src=\"https://img.shields.io/badge/üê≥-Project%20Page-blue\">\n        </a>\n        <a href=\"https://arxiv.org/abs/2309.03897\" target='_blank'>\n        <img src=\"https://img.shields.io/badge/arXiv-2309.03897-b31b1b.svg\">\n        </a>\n        <a href=\"https://youtu.be/92EHfgCO5-Q\" target='_blank'>\n        <img src=\"https://img.shields.io/badge/Demo%20Video-%23FF0000.svg?logo=YouTube&logoColor=white\">\n        </a>\n        <a href=\"https://huggingface.co/spaces/sczhou/ProPainter\" target='_blank'>\n        <img src=\"https://img.shields.io/badge/Demo-%F0%9F%A4%97%20Hugging%20Face-blue\">\n        </a>\n        <a href=\"https://openxlab.org.cn/apps/detail/ShangchenZhou/ProPainter\" target='_blank'>\n        <img src=\"https://img.shields.io/badge/Demo-%F0%9F%91%A8%E2%80%8D%F0%9F%8E%A8%20OpenXLab-blue\">\n        </a>\n        <img src=\"https://api.infinitescript.com/badgen/count?name=sczhou/ProPainter\">\n    </h4>\n</div>\n\n‚≠ê If ProPainter is helpful to your projects, please help star this repo. Thanks! ü§ó\n\n:open_book: For more visual results, go checkout our <a href=\"https://shangchenzhou.com/projects/ProPainter/\" target=\"_blank\">project page</a>\n\n\n---\n\n</div>\n\n\n## Update\n- **2023.11.09**: Integrated to :man_artist: [OpenXLab](https://openxlab.org.cn/apps). Try out online demo! [![OpenXLab](https://img.shields.io/badge/Demo-%F0%9F%91%A8%E2%80%8D%F0%9F%8E%A8%20OpenXLab-blue)](https://openxlab.org.cn/apps/detail/ShangchenZhou/ProPainter)\n- **2023.11.09**: Integrated to :hugs: [Hugging Face](https://huggingface.co/spaces). Try out online demo! [![Hugging Face](https://img.shields.io/badge/Demo-%F0%9F%A4%97%20Hugging%20Face-blue)](https://huggingface.co/spaces/sczhou/ProPainter)\n- **2023.09.24**: We remove the watermark removal demos officially to prevent the misuse of our work for unethical purposes.\n- **2023.09.21**: Add features for memory-efficient inference. Check our [GPU memory](https://github.com/sczhou/ProPainter#-memory-efficient-inference) requirements. üöÄ\n- **2023.09.07**: Our code and model are publicly available. üê≥\n- **2023.09.01**: This repo is created.\n\n\n### TODO\n- [ ] Make a Colab demo.\n- [x] ~~Make a interactive Gradio demo.~~\n- [x] ~~Update features for memory-efficient inference.~~\n  \n## Results\n\n#### üë®üèª‚Äçüé® Object Removal\n<table>\n<tr>\n   <td> \n      <img src=\"assets/object_removal1.gif\">\n   </td>\n   <td> \n      <img src=\"assets/object_removal2.gif\">\n   </td>\n</tr>\n</table>\n\n#### üé® Video Completion\n<table>\n<tr>\n   <td> \n      <img src=\"assets/video_completion1.gif\">\n   </td>\n   <td> \n      <img src=\"assets/video_completion2.gif\">\n   </td>\n</tr>\n<tr>\n   <td> \n      <img src=\"assets/video_completion3.gif\">\n   </td>\n   <td> \n      <img src=\"assets/video_completion4.gif\">\n   </td>\n</tr>\n</table>\n\n\n\n## Overview\n![overall_structure](assets/ProPainter_pipeline.png)\n\n\n## Dependencies and Installation\n\n1. Clone Repo\n\n   ```bash\n   git clone https://github.com/sczhou/ProPainter.git\n   ```\n\n2. Create Conda Environment and Install Dependencies\n\n   ```bash\n   # create new anaconda env\n   conda create -n propainter python=3.8 -y\n   conda activate propainter\n\n   # install python dependencies\n   pip3 install -r requirements.txt\n   ```\n\n   - CUDA >= 9.2\n   - PyTorch >= 1.7.1\n   - Torchvision >= 0.8.2\n   - Other required packages in `requirements.txt`\n\n## Get Started\n### Prepare pretrained models\nDownload our pretrained models from [Releases V0.1.0](https://github.com/sczhou/ProPainter/releases/tag/v0.1.0) to the `weights` folder. (All pretrained models can also be automatically downloaded during the first inference.)\n\nThe directory structure will be arranged as:\n```\nweights\n   |- ProPainter.pth\n   |- recurrent_flow_completion.pth\n   |- raft-things.pth\n   |- i3d_rgb_imagenet.pt (for evaluating VFID metric)\n   |- README.md\n```\n\n### üèÇ Quick test\nWe provide some examples in the [`inputs`](./inputs) folder. \nRun the following commands to try it out:\n```shell\n# The first example (object removal)\npython inference_propainter.py --video inputs/object_removal/bmx-trees --mask inputs/object_removal/bmx-trees_mask \n# The second example (video completion)\npython inference_propainter.py --video inputs/video_completion/running_car.mp4 --mask inputs/video_completion/mask_square.png --height 240 --width 432\n```\n\nThe results will be saved in the `results` folder.\nTo test your own videos, please prepare the input `mp4 video` (or `split frames`) and `frame-wise mask(s)`.\n\nIf you want to specify the video resolution for processing or avoid running out of memory, you can set the video size of `--width` and `--height`:\n```shell\n# process a 576x320 video; set --fp16 to use fp16 (half precision) during inference.\npython inference_propainter.py --video inputs/video_completion/running_car.mp4 --mask inputs/video_completion/mask_square.png --height 320 --width 576 --fp16\n```\n\n### üöÄ Memory-efficient inference\n\nVideo inpainting typically requires a significant amount of GPU memory. Here, we offer various features that facilitate memory-efficient inference, effectively avoiding the Out-Of-Memory (OOM) error. You can use the following options to reduce memory usage further:\n\n   - Reduce the number of local neighbors through decreasing the `--neighbor_length` (default 10).\n   - Reduce the number of global references by increasing the `--ref_stride` (default 10).\n   - Set the `--resize_ratio` (default 1.0) to resize the processing video.\n   - Set a smaller video size via specifying the `--width` and `--height`.\n   - Set `--fp16` to use fp16 (half precision) during inference.\n   - Reduce the frames of sub-videos `--subvideo_length` (default 80), which effectively decouples GPU memory costs and video length.\n\nBlow shows the estimated GPU memory requirements for different sub-video lengths with fp32/fp16 precision: \n\n| Resolution | 50 frames | 80 frames |\n| :---       | :----:    | :----:    |\n| 1280 x 720 | 28G / 19G | OOM / 25G |\n| 720 x 480  | 11G / 7G  | 13G / 8G  |\n| 640 x 480  | 10G / 6G  | 12G / 7G  |\n| 320 x 240  | 3G  / 2G  | 4G  / 3G  | \n\n\n## Dataset preparation\n<table>\n<thead>\n  <tr>\n    <th>Dataset</th>\n    <th>YouTube-VOS</th>\n    <th>DAVIS</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td>Description</td>\n    <td>For training (3,471) and evaluation (508)</td>\n    <td>For evaluation (50 in 90)</td>\n  <tr>\n    <td>Images</td>\n    <td> [<a href=\"https://competitions.codalab.org/competitions/19544#participate-get-data\">Official Link</a>] (Download train and test all frames) </td>\n    <td> [<a href=\"https://data.vision.ee.ethz.ch/csergi/share/davis/DAVIS-2017-trainval-480p.zip\">Official Link</a>] (2017, 480p, TrainVal) </td>\n  </tr>\n  <tr>\n    <td>Masks</td>\n    <td colspan=\"2\"> [<a href=\"https://drive.google.com/file/d/1dFTneS_zaJAHjglxU10gYzr1-xALgHa4/view?usp=sharing\">Google Drive</a>] [<a href=\"https://pan.baidu.com/s/1JC-UKmlQfjhVtD81196cxA?pwd=87e3\">Baidu Disk</a>] (For reproducing paper results; provided in <a href=\"https://arxiv.org/abs/2309.03897\">ProPainter</a> paper) </td>\n  </tr>\n</tbody>\n</table>\n\nThe training and test split files are provided in `datasets/<dataset_name>`. For each dataset, you should place `JPEGImages` to `datasets/<dataset_name>`. Resize all video frames to size `432x240` for training. Unzip downloaded mask files to `datasets`.\n\nThe `datasets` directory structure will be arranged as: (**Note**: please check it carefully)\n```\ndatasets\n   |- davis\n      |- JPEGImages_432_240\n         |- <video_name>\n            |- 00000.jpg\n            |- 00001.jpg\n      |- test_masks\n         |- <video_name>\n            |- 00000.png\n            |- 00001.png   \n      |- train.json\n      |- test.json\n   |- youtube-vos\n      |- JPEGImages_432_240\n         |- <video_name>\n            |- 00000.jpg\n            |- 00001.jpg\n      |- test_masks\n         |- <video_name>\n            |- 00000.png\n            |- 00001.png\n      |- train.json\n      |- test.json   \n```\n\n## Training\nOur training configures are provided in [`train_flowcomp.json`](./configs/train_flowcomp.json) (for Recurrent Flow Completion Network) and [`train_propainter.json`](./configs/train_propainter.json) (for ProPainter).\n\nRun one of the following commands for training:\n```shell\n # For training Recurrent Flow Completion Network\n python train.py -c configs/train_flowcomp.json\n # For training ProPainter\n python train.py -c configs/train_propainter.json\n```\nYou can run the **same command** to **resume** your training.\n\nTo speed up the training process, you can precompute optical flow for the training dataset using the following command:\n```shell\n # Compute optical flow for training dataset\n python scripts/compute_flow.py --root_path <dataset_root> --save_path <save_flow_root> --height 240 --width 432\n```\n\n## Evaluation\nRun one of the following commands for evaluation:\n```shell\n # For evaluating flow completion model\n python scripts/evaluate_flow_completion.py --dataset <dataset_name> --video_root <video_root> --mask_root <mask_root> --save_results\n # For evaluating ProPainter model\n python scripts/evaluate_propainter.py --dataset <dataset_name> --video_root <video_root> --mask_root <mask_root> --save_results\n```\n\nThe scores and results will also be saved in the `results_eval` folder.\nPlease `--save_results` for further [evaluating temporal warping error](https://github.com/phoenix104104/fast_blind_video_consistency#evaluation).\n\n\n\n## Citation\n\n   If you find our repo useful for your research, please consider citing our paper:\n\n   ```bibtex\n   @inproceedings{zhou2023propainter,\n      title={{ProPainter}: Improving Propagation and Transformer for Video Inpainting},\n      author={Zhou, Shangchen and Li, Chongyi and Chan, Kelvin C.K and Loy, Chen Change},\n      booktitle={Proceedings of IEEE International Conference on Computer Vision (ICCV)},\n      year={2023}\n   }\n   ```\n\n\n## License\n\n#### Non-Commercial Use Only Declaration\nThe ProPainter is made available for use, reproduction, and distribution strictly for non-commercial purposes. The code and models are licensed under <a rel=\"license\" href=\"./LICENSE\">NTU S-Lab License 1.0</a>. Redistribution and use should follow this license.\n\nFor inquiries or to obtain permission for commercial use, please consult Dr. Shangchen Zhou (shangchenzhou@gmail.com).\n\n\n## Projects that use ProPainter\n\nIf you develop or use ProPainter in your projects, feel free to let me know. Also, please include this [ProPainter](https://github.com/sczhou/ProPainter) repo link, authorship information, and our [S-Lab license](https://github.com/sczhou/ProPainter/blob/main/LICENSE) (with link).\n\n#### Projects/Applications from the Community\n\n- Streaming ProPainter: https://github.com/osmr/propainter\n- Faster ProPainter: https://github.com/halfzm/faster-propainter\n- ProPainter WebUI: https://github.com/halfzm/ProPainter-Webui\n- ProPainter ComfyUI: https://github.com/daniabib/ComfyUI_ProPainter_Nodes\n- Cutie (video segmentation): https://github.com/hkchengrex/Cutie\n- Cinetransfer (character transfer): https://virtualfilmstudio.github.io/projects/cinetransfer\n- Motionshop (character transfer): https://aigc3d.github.io/motionshop\n\n\n\n#### PyPI\n- propainter: https://pypi.org/project/propainter\n- pytorchcv: https://pypi.org/project/pytorchcv\n\n## Contact\nIf you have any questions, please feel free to reach me out at shangchenzhou@gmail.com. \n\n## Acknowledgement\n\nThis code is based on [E<sup>2</sup>FGVI](https://github.com/MCG-NKU/E2FGVI) and [STTN](https://github.com/researchmm/STTN). Some code are brought from [BasicVSR++](https://github.com/ckkelvinchan/BasicVSR_PlusPlus). Thanks for their awesome works. \n\nSpecial thanks to [Yihang Luo](https://github.com/Luo-Yihang) for his valuable contributions to build and maintain the Gradio demos for ProPainter.\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "core",
          "type": "tree",
          "content": null
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference_propainter.py",
          "type": "blob",
          "size": 20.212890625,
          "content": "# -*- coding: utf-8 -*-\nimport os\nimport cv2\nimport argparse\nimport imageio\nimport numpy as np\nimport scipy.ndimage\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torchvision\n\nfrom model.modules.flow_comp_raft import RAFT_bi\nfrom model.recurrent_flow_completion import RecurrentFlowCompleteNet\nfrom model.propainter import InpaintGenerator\nfrom utils.download_util import load_file_from_url\nfrom core.utils import to_tensors\nfrom model.misc import get_device\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npretrain_model_url = 'https://github.com/sczhou/ProPainter/releases/download/v0.1.0/'\n\ndef imwrite(img, file_path, params=None, auto_mkdir=True):\n    if auto_mkdir:\n        dir_name = os.path.abspath(os.path.dirname(file_path))\n        os.makedirs(dir_name, exist_ok=True)\n    return cv2.imwrite(file_path, img, params)\n\n\n# resize frames\ndef resize_frames(frames, size=None):    \n    if size is not None:\n        out_size = size\n        process_size = (out_size[0]-out_size[0]%8, out_size[1]-out_size[1]%8)\n        frames = [f.resize(process_size) for f in frames]\n    else:\n        out_size = frames[0].size\n        process_size = (out_size[0]-out_size[0]%8, out_size[1]-out_size[1]%8)\n        if not out_size == process_size:\n            frames = [f.resize(process_size) for f in frames]\n        \n    return frames, process_size, out_size\n\n\n#  read frames from video\ndef read_frame_from_videos(frame_root):\n    if frame_root.endswith(('mp4', 'mov', 'avi', 'MP4', 'MOV', 'AVI')): # input video path\n        video_name = os.path.basename(frame_root)[:-4]\n        vframes, aframes, info = torchvision.io.read_video(filename=frame_root, pts_unit='sec') # RGB\n        frames = list(vframes.numpy())\n        frames = [Image.fromarray(f) for f in frames]\n        fps = info['video_fps']\n    else:\n        video_name = os.path.basename(frame_root)\n        frames = []\n        fr_lst = sorted(os.listdir(frame_root))\n        for fr in fr_lst:\n            frame = cv2.imread(os.path.join(frame_root, fr))\n            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n            frames.append(frame)\n        fps = None\n    size = frames[0].size\n\n    return frames, fps, size, video_name\n\n\ndef binary_mask(mask, th=0.1):\n    mask[mask>th] = 1\n    mask[mask<=th] = 0\n    return mask\n  \n  \n# read frame-wise masks\ndef read_mask(mpath, length, size, flow_mask_dilates=8, mask_dilates=5):\n    masks_img = []\n    masks_dilated = []\n    flow_masks = []\n    \n    if mpath.endswith(('jpg', 'jpeg', 'png', 'JPG', 'JPEG', 'PNG')): # input single img path\n       masks_img = [Image.open(mpath)]\n    else:  \n        mnames = sorted(os.listdir(mpath))\n        for mp in mnames:\n            masks_img.append(Image.open(os.path.join(mpath, mp)))\n          \n    for mask_img in masks_img:\n        if size is not None:\n            mask_img = mask_img.resize(size, Image.NEAREST)\n        mask_img = np.array(mask_img.convert('L'))\n\n        # Dilate 8 pixel so that all known pixel is trustworthy\n        if flow_mask_dilates > 0:\n            flow_mask_img = scipy.ndimage.binary_dilation(mask_img, iterations=flow_mask_dilates).astype(np.uint8)\n        else:\n            flow_mask_img = binary_mask(mask_img).astype(np.uint8)\n        # Close the small holes inside the foreground objects\n        # flow_mask_img = cv2.morphologyEx(flow_mask_img, cv2.MORPH_CLOSE, np.ones((21, 21),np.uint8)).astype(bool)\n        # flow_mask_img = scipy.ndimage.binary_fill_holes(flow_mask_img).astype(np.uint8)\n        flow_masks.append(Image.fromarray(flow_mask_img * 255))\n        \n        if mask_dilates > 0:\n            mask_img = scipy.ndimage.binary_dilation(mask_img, iterations=mask_dilates).astype(np.uint8)\n        else:\n            mask_img = binary_mask(mask_img).astype(np.uint8)\n        masks_dilated.append(Image.fromarray(mask_img * 255))\n    \n    if len(masks_img) == 1:\n        flow_masks = flow_masks * length\n        masks_dilated = masks_dilated * length\n\n    return flow_masks, masks_dilated\n\n\ndef extrapolation(video_ori, scale):\n    \"\"\"Prepares the data for video outpainting.\n    \"\"\"\n    nFrame = len(video_ori)\n    imgW, imgH = video_ori[0].size\n\n    # Defines new FOV.\n    imgH_extr = int(scale[0] * imgH)\n    imgW_extr = int(scale[1] * imgW)\n    imgH_extr = imgH_extr - imgH_extr % 8\n    imgW_extr = imgW_extr - imgW_extr % 8\n    H_start = int((imgH_extr - imgH) / 2)\n    W_start = int((imgW_extr - imgW) / 2)\n\n    # Extrapolates the FOV for video.\n    frames = []\n    for v in video_ori:\n        frame = np.zeros(((imgH_extr, imgW_extr, 3)), dtype=np.uint8)\n        frame[H_start: H_start + imgH, W_start: W_start + imgW, :] = v\n        frames.append(Image.fromarray(frame))\n\n    # Generates the mask for missing region.\n    masks_dilated = []\n    flow_masks = []\n    \n    dilate_h = 4 if H_start > 10 else 0\n    dilate_w = 4 if W_start > 10 else 0\n    mask = np.ones(((imgH_extr, imgW_extr)), dtype=np.uint8)\n    \n    mask[H_start+dilate_h: H_start+imgH-dilate_h, \n         W_start+dilate_w: W_start+imgW-dilate_w] = 0\n    flow_masks.append(Image.fromarray(mask * 255))\n\n    mask[H_start: H_start+imgH, W_start: W_start+imgW] = 0\n    masks_dilated.append(Image.fromarray(mask * 255))\n  \n    flow_masks = flow_masks * nFrame\n    masks_dilated = masks_dilated * nFrame\n    \n    return frames, flow_masks, masks_dilated, (imgW_extr, imgH_extr)\n\n\ndef get_ref_index(mid_neighbor_id, neighbor_ids, length, ref_stride=10, ref_num=-1):\n    ref_index = []\n    if ref_num == -1:\n        for i in range(0, length, ref_stride):\n            if i not in neighbor_ids:\n                ref_index.append(i)\n    else:\n        start_idx = max(0, mid_neighbor_id - ref_stride * (ref_num // 2))\n        end_idx = min(length, mid_neighbor_id + ref_stride * (ref_num // 2))\n        for i in range(start_idx, end_idx, ref_stride):\n            if i not in neighbor_ids:\n                if len(ref_index) > ref_num:\n                    break\n                ref_index.append(i)\n    return ref_index\n\n\n\nif __name__ == '__main__':\n    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    device = get_device()\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '-i', '--video', type=str, default='inputs/object_removal/bmx-trees', help='Path of the input video or image folder.')\n    parser.add_argument(\n        '-m', '--mask', type=str, default='inputs/object_removal/bmx-trees_mask', help='Path of the mask(s) or mask folder.')\n    parser.add_argument(\n        '-o', '--output', type=str, default='results', help='Output folder. Default: results')\n    parser.add_argument(\n        \"--resize_ratio\", type=float, default=1.0, help='Resize scale for processing video.')\n    parser.add_argument(\n        '--height', type=int, default=-1, help='Height of the processing video.')\n    parser.add_argument(\n        '--width', type=int, default=-1, help='Width of the processing video.')\n    parser.add_argument(\n        '--mask_dilation', type=int, default=4, help='Mask dilation for video and flow masking.')\n    parser.add_argument(\n        \"--ref_stride\", type=int, default=10, help='Stride of global reference frames.')\n    parser.add_argument(\n        \"--neighbor_length\", type=int, default=10, help='Length of local neighboring frames.')\n    parser.add_argument(\n        \"--subvideo_length\", type=int, default=80, help='Length of sub-video for long video inference.')\n    parser.add_argument(\n        \"--raft_iter\", type=int, default=20, help='Iterations for RAFT inference.')\n    parser.add_argument(\n        '--mode', default='video_inpainting', choices=['video_inpainting', 'video_outpainting'], help=\"Modes: video_inpainting / video_outpainting\")\n    parser.add_argument(\n        '--scale_h', type=float, default=1.0, help='Outpainting scale of height for video_outpainting mode.')\n    parser.add_argument(\n        '--scale_w', type=float, default=1.2, help='Outpainting scale of width for video_outpainting mode.')\n    parser.add_argument(\n        '--save_fps', type=int, default=24, help='Frame per second. Default: 24')\n    parser.add_argument(\n        '--save_frames', action='store_true', help='Save output frames. Default: False')\n    parser.add_argument(\n        '--fp16', action='store_true', help='Use fp16 (half precision) during inference. Default: fp32 (single precision).')\n\n    args = parser.parse_args()\n\n    # Use fp16 precision during inference to reduce running memory cost\n    use_half = True if args.fp16 else False \n    if device == torch.device('cpu'):\n        use_half = False\n\n    frames, fps, size, video_name = read_frame_from_videos(args.video)\n    if not args.width == -1 and not args.height == -1:\n        size = (args.width, args.height)\n    if not args.resize_ratio == 1.0:\n        size = (int(args.resize_ratio * size[0]), int(args.resize_ratio * size[1]))\n\n    frames, size, out_size = resize_frames(frames, size)\n    \n    fps = args.save_fps if fps is None else fps\n    save_root = os.path.join(args.output, video_name)\n    if not os.path.exists(save_root):\n        os.makedirs(save_root, exist_ok=True)\n\n    if args.mode == 'video_inpainting':\n        frames_len = len(frames)\n        flow_masks, masks_dilated = read_mask(args.mask, frames_len, size, \n                                              flow_mask_dilates=args.mask_dilation,\n                                              mask_dilates=args.mask_dilation)\n        w, h = size\n    elif args.mode == 'video_outpainting':\n        assert args.scale_h is not None and args.scale_w is not None, 'Please provide a outpainting scale (s_h, s_w).'\n        frames, flow_masks, masks_dilated, size = extrapolation(frames, (args.scale_h, args.scale_w))\n        w, h = size\n    else:\n        raise NotImplementedError\n    \n    # for saving the masked frames or video\n    masked_frame_for_save = []\n    for i in range(len(frames)):\n        mask_ = np.expand_dims(np.array(masks_dilated[i]),2).repeat(3, axis=2)/255.\n        img = np.array(frames[i])\n        green = np.zeros([h, w, 3]) \n        green[:,:,1] = 255\n        alpha = 0.6\n        # alpha = 1.0\n        fuse_img = (1-alpha)*img + alpha*green\n        fuse_img = mask_ * fuse_img + (1-mask_)*img\n        masked_frame_for_save.append(fuse_img.astype(np.uint8))\n\n    frames_inp = [np.array(f).astype(np.uint8) for f in frames]\n    frames = to_tensors()(frames).unsqueeze(0) * 2 - 1    \n    flow_masks = to_tensors()(flow_masks).unsqueeze(0)\n    masks_dilated = to_tensors()(masks_dilated).unsqueeze(0)\n    frames, flow_masks, masks_dilated = frames.to(device), flow_masks.to(device), masks_dilated.to(device)\n\n    \n    ##############################################\n    # set up RAFT and flow competition model\n    ##############################################\n    ckpt_path = load_file_from_url(url=os.path.join(pretrain_model_url, 'raft-things.pth'), \n                                    model_dir='weights', progress=True, file_name=None)\n    fix_raft = RAFT_bi(ckpt_path, device)\n    \n    ckpt_path = load_file_from_url(url=os.path.join(pretrain_model_url, 'recurrent_flow_completion.pth'), \n                                    model_dir='weights', progress=True, file_name=None)\n    fix_flow_complete = RecurrentFlowCompleteNet(ckpt_path)\n    for p in fix_flow_complete.parameters():\n        p.requires_grad = False\n    fix_flow_complete.to(device)\n    fix_flow_complete.eval()\n\n\n    ##############################################\n    # set up ProPainter model\n    ##############################################\n    ckpt_path = load_file_from_url(url=os.path.join(pretrain_model_url, 'ProPainter.pth'), \n                                    model_dir='weights', progress=True, file_name=None)\n    model = InpaintGenerator(model_path=ckpt_path).to(device)\n    model.eval()\n\n    \n    ##############################################\n    # ProPainter inference\n    ##############################################\n    video_length = frames.size(1)\n    print(f'\\nProcessing: {video_name} [{video_length} frames]...')\n    with torch.no_grad():\n        # ---- compute flow ----\n        if frames.size(-1) <= 640: \n            short_clip_len = 12\n        elif frames.size(-1) <= 720: \n            short_clip_len = 8\n        elif frames.size(-1) <= 1280:\n            short_clip_len = 4\n        else:\n            short_clip_len = 2\n        \n        # use fp32 for RAFT\n        if frames.size(1) > short_clip_len:\n            gt_flows_f_list, gt_flows_b_list = [], []\n            for f in range(0, video_length, short_clip_len):\n                end_f = min(video_length, f + short_clip_len)\n                if f == 0:\n                    flows_f, flows_b = fix_raft(frames[:,f:end_f], iters=args.raft_iter)\n                else:\n                    flows_f, flows_b = fix_raft(frames[:,f-1:end_f], iters=args.raft_iter)\n                \n                gt_flows_f_list.append(flows_f)\n                gt_flows_b_list.append(flows_b)\n                torch.cuda.empty_cache()\n                \n            gt_flows_f = torch.cat(gt_flows_f_list, dim=1)\n            gt_flows_b = torch.cat(gt_flows_b_list, dim=1)\n            gt_flows_bi = (gt_flows_f, gt_flows_b)\n        else:\n            gt_flows_bi = fix_raft(frames, iters=args.raft_iter)\n            torch.cuda.empty_cache()\n\n\n        if use_half:\n            frames, flow_masks, masks_dilated = frames.half(), flow_masks.half(), masks_dilated.half()\n            gt_flows_bi = (gt_flows_bi[0].half(), gt_flows_bi[1].half())\n            fix_flow_complete = fix_flow_complete.half()\n            model = model.half()\n\n        \n        # ---- complete flow ----\n        flow_length = gt_flows_bi[0].size(1)\n        if flow_length > args.subvideo_length:\n            pred_flows_f, pred_flows_b = [], []\n            pad_len = 5\n            for f in range(0, flow_length, args.subvideo_length):\n                s_f = max(0, f - pad_len)\n                e_f = min(flow_length, f + args.subvideo_length + pad_len)\n                pad_len_s = max(0, f) - s_f\n                pad_len_e = e_f - min(flow_length, f + args.subvideo_length)\n                pred_flows_bi_sub, _ = fix_flow_complete.forward_bidirect_flow(\n                    (gt_flows_bi[0][:, s_f:e_f], gt_flows_bi[1][:, s_f:e_f]), \n                    flow_masks[:, s_f:e_f+1])\n                pred_flows_bi_sub = fix_flow_complete.combine_flow(\n                    (gt_flows_bi[0][:, s_f:e_f], gt_flows_bi[1][:, s_f:e_f]), \n                    pred_flows_bi_sub, \n                    flow_masks[:, s_f:e_f+1])\n\n                pred_flows_f.append(pred_flows_bi_sub[0][:, pad_len_s:e_f-s_f-pad_len_e])\n                pred_flows_b.append(pred_flows_bi_sub[1][:, pad_len_s:e_f-s_f-pad_len_e])\n                torch.cuda.empty_cache()\n                \n            pred_flows_f = torch.cat(pred_flows_f, dim=1)\n            pred_flows_b = torch.cat(pred_flows_b, dim=1)\n            pred_flows_bi = (pred_flows_f, pred_flows_b)\n        else:\n            pred_flows_bi, _ = fix_flow_complete.forward_bidirect_flow(gt_flows_bi, flow_masks)\n            pred_flows_bi = fix_flow_complete.combine_flow(gt_flows_bi, pred_flows_bi, flow_masks)\n            torch.cuda.empty_cache()\n            \n\n        # ---- image propagation ----\n        masked_frames = frames * (1 - masks_dilated)\n        subvideo_length_img_prop = min(100, args.subvideo_length) # ensure a minimum of 100 frames for image propagation\n        if video_length > subvideo_length_img_prop:\n            updated_frames, updated_masks = [], []\n            pad_len = 10\n            for f in range(0, video_length, subvideo_length_img_prop):\n                s_f = max(0, f - pad_len)\n                e_f = min(video_length, f + subvideo_length_img_prop + pad_len)\n                pad_len_s = max(0, f) - s_f\n                pad_len_e = e_f - min(video_length, f + subvideo_length_img_prop)\n\n                b, t, _, _, _ = masks_dilated[:, s_f:e_f].size()\n                pred_flows_bi_sub = (pred_flows_bi[0][:, s_f:e_f-1], pred_flows_bi[1][:, s_f:e_f-1])\n                prop_imgs_sub, updated_local_masks_sub = model.img_propagation(masked_frames[:, s_f:e_f], \n                                                                       pred_flows_bi_sub, \n                                                                       masks_dilated[:, s_f:e_f], \n                                                                       'nearest')\n                updated_frames_sub = frames[:, s_f:e_f] * (1 - masks_dilated[:, s_f:e_f]) + \\\n                                    prop_imgs_sub.view(b, t, 3, h, w) * masks_dilated[:, s_f:e_f]\n                updated_masks_sub = updated_local_masks_sub.view(b, t, 1, h, w)\n                \n                updated_frames.append(updated_frames_sub[:, pad_len_s:e_f-s_f-pad_len_e])\n                updated_masks.append(updated_masks_sub[:, pad_len_s:e_f-s_f-pad_len_e])\n                torch.cuda.empty_cache()\n                \n            updated_frames = torch.cat(updated_frames, dim=1)\n            updated_masks = torch.cat(updated_masks, dim=1)\n        else:\n            b, t, _, _, _ = masks_dilated.size()\n            prop_imgs, updated_local_masks = model.img_propagation(masked_frames, pred_flows_bi, masks_dilated, 'nearest')\n            updated_frames = frames * (1 - masks_dilated) + prop_imgs.view(b, t, 3, h, w) * masks_dilated\n            updated_masks = updated_local_masks.view(b, t, 1, h, w)\n            torch.cuda.empty_cache()\n            \n    \n    ori_frames = frames_inp\n    comp_frames = [None] * video_length\n\n    neighbor_stride = args.neighbor_length // 2\n    if video_length > args.subvideo_length:\n        ref_num = args.subvideo_length // args.ref_stride\n    else:\n        ref_num = -1\n    \n    # ---- feature propagation + transformer ----\n    for f in tqdm(range(0, video_length, neighbor_stride)):\n        neighbor_ids = [\n            i for i in range(max(0, f - neighbor_stride),\n                                min(video_length, f + neighbor_stride + 1))\n        ]\n        ref_ids = get_ref_index(f, neighbor_ids, video_length, args.ref_stride, ref_num)\n        selected_imgs = updated_frames[:, neighbor_ids + ref_ids, :, :, :]\n        selected_masks = masks_dilated[:, neighbor_ids + ref_ids, :, :, :]\n        selected_update_masks = updated_masks[:, neighbor_ids + ref_ids, :, :, :]\n        selected_pred_flows_bi = (pred_flows_bi[0][:, neighbor_ids[:-1], :, :, :], pred_flows_bi[1][:, neighbor_ids[:-1], :, :, :])\n        \n        with torch.no_grad():\n            # 1.0 indicates mask\n            l_t = len(neighbor_ids)\n            \n            # pred_img = selected_imgs # results of image propagation\n            pred_img = model(selected_imgs, selected_pred_flows_bi, selected_masks, selected_update_masks, l_t)\n            \n            pred_img = pred_img.view(-1, 3, h, w)\n\n            pred_img = (pred_img + 1) / 2\n            pred_img = pred_img.cpu().permute(0, 2, 3, 1).numpy() * 255\n            binary_masks = masks_dilated[0, neighbor_ids, :, :, :].cpu().permute(\n                0, 2, 3, 1).numpy().astype(np.uint8)\n            for i in range(len(neighbor_ids)):\n                idx = neighbor_ids[i]\n                img = np.array(pred_img[i]).astype(np.uint8) * binary_masks[i] \\\n                    + ori_frames[idx] * (1 - binary_masks[i])\n                if comp_frames[idx] is None:\n                    comp_frames[idx] = img\n                else: \n                    comp_frames[idx] = comp_frames[idx].astype(np.float32) * 0.5 + img.astype(np.float32) * 0.5\n                    \n                comp_frames[idx] = comp_frames[idx].astype(np.uint8)\n        \n        torch.cuda.empty_cache()\n                \n    # save each frame\n    if args.save_frames:\n        for idx in range(video_length):\n            f = comp_frames[idx]\n            f = cv2.resize(f, out_size, interpolation = cv2.INTER_CUBIC)\n            f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n            img_save_root = os.path.join(save_root, 'frames', str(idx).zfill(4)+'.png')\n            imwrite(f, img_save_root)\n                    \n\n    # if args.mode == 'video_outpainting':\n    #     comp_frames = [i[10:-10,10:-10] for i in comp_frames]\n    #     masked_frame_for_save = [i[10:-10,10:-10] for i in masked_frame_for_save]\n    \n    # save videos frame\n    masked_frame_for_save = [cv2.resize(f, out_size) for f in masked_frame_for_save]\n    comp_frames = [cv2.resize(f, out_size) for f in comp_frames]\n    imageio.mimwrite(os.path.join(save_root, 'masked_in.mp4'), masked_frame_for_save, fps=fps, quality=7)\n    imageio.mimwrite(os.path.join(save_root, 'inpaint_out.mp4'), comp_frames, fps=fps, quality=7)\n    \n    print(f'\\nAll results are saved in {save_root}')\n    \n    torch.cuda.empty_cache()"
        },
        {
          "name": "inputs",
          "type": "tree",
          "content": null
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.142578125,
          "content": "av\naddict\neinops\nfuture\nnumpy\nscipy\nopencv-python\nmatplotlib\nscikit-image\ntorch>=1.7.1\ntorchvision>=0.8.2\nimageio-ffmpeg\npyyaml\nrequests\ntimm\nyapf"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 3.4296875,
          "content": "import os\nimport json\nimport argparse\nimport subprocess\n\nfrom shutil import copyfile\nimport torch.distributed as dist\n\nimport torch\nimport torch.multiprocessing as mp\n\nimport core\nimport core.trainer\nimport core.trainer_flow_w_edge\n\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\nfrom core.dist import (\n    get_world_size,\n    get_local_rank,\n    get_global_rank,\n    get_master_ip,\n)\n\nparser = argparse.ArgumentParser()\nparser.add_argument('-c',\n                    '--config',\n                    default='configs/train_propainter.json',\n                    type=str)\nparser.add_argument('-p', '--port', default='23490', type=str)\nargs = parser.parse_args()\n\n\ndef main_worker(rank, config):\n    if 'local_rank' not in config:\n        config['local_rank'] = config['global_rank'] = rank\n    if config['distributed']:\n        torch.cuda.set_device(int(config['local_rank']))\n        torch.distributed.init_process_group(backend='nccl',\n                                             init_method=config['init_method'],\n                                             world_size=config['world_size'],\n                                             rank=config['global_rank'],\n                                             group_name='mtorch')\n        print('using GPU {}-{} for training'.format(int(config['global_rank']),\n                                                    int(config['local_rank'])))\n\n\n    config['save_dir'] = os.path.join(\n        config['save_dir'],\n        '{}_{}'.format(config['model']['net'],\n                       os.path.basename(args.config).split('.')[0]))\n\n    config['save_metric_dir'] = os.path.join(\n        './scores',\n        '{}_{}'.format(config['model']['net'],\n                       os.path.basename(args.config).split('.')[0]))\n\n    if torch.cuda.is_available():\n        config['device'] = torch.device(\"cuda:{}\".format(config['local_rank']))\n    else:\n        config['device'] = 'cpu'\n\n    if (not config['distributed']) or config['global_rank'] == 0:\n        os.makedirs(config['save_dir'], exist_ok=True)\n        config_path = os.path.join(config['save_dir'],\n                                   args.config.split('/')[-1])\n        if not os.path.isfile(config_path):\n            copyfile(args.config, config_path)\n        print('[**] create folder {}'.format(config['save_dir']))\n\n    trainer_version = config['trainer']['version']\n    trainer = core.__dict__[trainer_version].__dict__['Trainer'](config)\n    # Trainer(config)\n    trainer.train()\n\n\nif __name__ == \"__main__\":\n\n    torch.backends.cudnn.benchmark = True\n\n    mp.set_sharing_strategy('file_system')\n\n    # loading configs\n    config = json.load(open(args.config))\n\n    # setting distributed configurations\n    # config['world_size'] = get_world_size()\n    config['world_size'] = torch.cuda.device_count()\n    config['init_method'] = f\"tcp://{get_master_ip()}:{args.port}\"\n    config['distributed'] = True if config['world_size'] > 1 else False\n    print('world_size:', config['world_size'])\n    # setup distributed parallel training environments\n\n    # if get_master_ip() == \"127.0.0.X\":\n    #     # manually launch distributed processes\n    #     mp.spawn(main_worker, nprocs=config['world_size'], args=(config, ))\n    # else:\n    #     # multiple processes have been launched by openmpi\n    #     config['local_rank'] = get_local_rank()\n    #     config['global_rank'] = get_global_rank()\n    #     main_worker(-1, config)\n\n    mp.spawn(main_worker, nprocs=torch.cuda.device_count(), args=(config, ))"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "web-demos",
          "type": "tree",
          "content": null
        },
        {
          "name": "weights",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}