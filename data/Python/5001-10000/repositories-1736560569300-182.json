{
  "metadata": {
    "timestamp": 1736560569300,
    "page": 182,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "baowenbo/DAIN",
      "stars": 8256,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.359375,
          "content": "# Ignore Git here\n.git\n\n# But not these files...\n# !.gitignore\n\ncheckpoints/test_local/opt.txt\nPWCNet/pwc_net.pth.tar\nMegaDepth/checkpoints/*\nmodel_weights/*\nMiddleBurySet/*\n\n.nfs*\n\n# Created by .ignore support plugin (hsz.mobi)\n### Python template\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# IPython Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# dotenv\n.env\n\n# virtualenv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n\n# Rope project settings\n.ropeproject\n### VirtualEnv template\n# Virtualenv\n# http://iamzed.com/2009/05/07/a-primer-on-virtualenv/\n.Python\n[Bb]in\n[Ii]nclude\n[Ll]ib\n[Ll]ib64\n[Ll]ocal\n[Ss]cripts\npyvenv.cfg\n.venv\npip-selfcheck.json\n### JetBrains template\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio and Webstorm\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff:\n.idea/workspace.xml\n.idea/tasks.xml\n.idea/dictionaries\n.idea/vcs.xml\n.idea/jsLibraryMappings.xml\n\n# Sensitive or high-churn files:\n.idea/dataSources.ids\n.idea/dataSources.xml\n.idea/dataSources.local.xml\n.idea/sqlDataSources.xml\n.idea/dynamic.xml\n.idea/uiDesigner.xml\n\n# Gradle:\n.idea/gradle.xml\n.idea/libraries\n\n# Mongo Explorer plugin:\n.idea/mongoSettings.xml\n\n.idea/\n\n## File-based project format:\n*.iws\n\n## Plugin-specific files:\n\n# IntelliJ\n/out/\n\n# mpeltonen/sbt-idea plugin\n.idea_modules/\n\n# JIRA plugin\natlassian-ide-plugin.xml\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n\n"
        },
        {
          "name": "AverageMeter.py",
          "type": "blob",
          "size": 0.3828125,
          "content": "\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n"
        },
        {
          "name": "Colab_DAIN.ipynb",
          "type": "blob",
          "size": 12.6484375,
          "content": "{\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0,\n  \"metadata\": {\n    \"colab\": {\n      \"name\": \"Colab_DAIN_new.ipynb\",\n      \"private_outputs\": true,\n      \"provenance\": [],\n      \"collapsed_sections\": [],\n      \"toc_visible\": true\n    },\n    \"kernelspec\": {\n      \"name\": \"python3\",\n      \"display_name\": \"Python 3\"\n    },\n    \"accelerator\": \"GPU\"\n  },\n  \"cells\": [\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"1pIo4r_Y8cMo\"\n      },\n      \"source\": [\n        \"# DAIN Colab\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"iGPHW5SOpPe3\"\n      },\n      \"source\": [\n        \"*DAIN Colab, v1.6.0*\\n\",\n        \"\\n\",\n        \"Based on the [original Colab file](https://github.com/baowenbo/DAIN/issues/44) by btahir. \\n\",\n        \"\\n\",\n        \"Enhancements by [Styler00Dollar](https://github.com/styler00dollar) aka \\\"sudo rm -rf / --no-preserve-root#8353\\\" on discord and [Alpha](https://github.com/AlphaGit), (Alpha#6137 on Discord). Please do not run this command in your linux terminal. It's rather meant as a joke.\\n\",\n        \"\\n\",\n        \"[Styler00Dollar's fork](https://github.com/styler00dollar/DAIN) / [Alpha's fork](https://github.com/AlphaGit/DAIN)\\n\",\n        \"\\n\",\n        \"A simple guide:\\n\",\n        \"- Upload this ` .ipynb`  file to your Google Colab.\\n\",\n        \"- Create a folder inside of Google Drive named \\\"DAIN\\\"\\n\",\n        \"- Change the configurations in the next cell\\n\",\n        \"- Run cells one by one\\n\",\n        \"\\n\",\n        \"Stuff that should be improved:\\n\",\n        \"- Alpha channel will be removed automatically and won't be added back. Anything related to alpha will be converted to black.\\n\",\n        \"- Adding configuration to select speed\\n\",\n        \"- Detect scenes to avoid interpolating scene-changes\\n\",\n        \"- Auto-resume\\n\",\n        \"- Copy `start_frame` - `end_frame` audio from original input to final output\\n\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"enKoi0TR2fOD\",\n        \"cellView\": \"form\"\n      },\n      \"source\": [\n        \"################# Required Configurations ############################\\n\",\n        \"\\n\",\n        \"#@markdown # Required Configuration\\n\",\n        \"#@markdown Use the values in here to configure what you'd like DAIN to do.\\n\",\n        \"\\n\",\n        \"#@markdown ## Input file\\n\",\n        \"#@markdown Path (relative to the root of your Google Drive) to the input file. For instance, if you save your `example.mkv` file in your Google Drive, inside a `videos` folder, the path would be: `videos/example.mkv`. Currenly videos and gifs are supported.\\n\",\n        \"INPUT_FILEPATH = \\\"DAIN/input.mp4\\\" #@param{type:\\\"string\\\"}\\n\",\n        \"\\n\",\n        \"#@markdown ## Output file\\n\",\n        \"#@markdown Output file path: path (relative to the root of your Google Drive) for the output file. It will also determine the filetype in the destination. `.mp4` is recommended for video input, `.gif` for gif inputs.\\n\",\n        \"OUTPUT_FILE_PATH = \\\"DAIN/output.mp4\\\" #@param{type:\\\"string\\\"}\\n\",\n        \"\\n\",\n        \"################# Optional configurations ############################\\n\",\n        \"\\n\",\n        \"#@markdown # Optional Configuration\\n\",\n        \"#@markdown Parameters below can be left with their defaults, but feel free to adapt them to your needs.\\n\",\n        \"\\n\",\n        \"#@markdown ## Target FPS\\n\",\n        \"#@markdown  how many frames per second should the result have. This will determine how many intermediate images are interpolated.\\n\",\n        \"TARGET_FPS = 60 #@param{type:\\\"number\\\"}\\n\",\n        \"\\n\",\n        \"#@markdown ## Frame input directory\\n\",\n        \"#@markdown A path, relative to your GDrive root, where you already have the list of frames in the format 00001.png, 00002.png, etc.\\n\",\n        \"FRAME_INPUT_DIR = '/content/DAIN/input_frames' #@param{type:\\\"string\\\"}\\n\",\n        \"\\n\",\n        \"#@markdown ## Frame output directory\\n\",\n        \"#@markdown A path, relative to your GDrive root, where you want the generated frame.\\n\",\n        \"FRAME_OUTPUT_DIR = '/content/DAIN/output_frames' #@param{type:\\\"string\\\"}\\n\",\n        \"\\n\",\n        \"#@markdown ## Start Frame\\n\",\n        \"#@markdown First frame to consider from the video when processing.\\n\",\n        \"START_FRAME = 1 #@param{type:\\\"number\\\"}\\n\",\n        \"\\n\",\n        \"#@markdown ## End Frame\\n\",\n        \"#@markdown Last frame to consider from the video when processing. To use the whole video use `-1`.\\n\",\n        \"END_FRAME = -1 #@param{type:\\\"number\\\"}\\n\",\n        \"\\n\",\n        \"#@markdown ## Seamless playback\\n\",\n        \"#@markdown Creates a seamless loop by using the first frame as last one as well. Set this to True this if loop is intended.\\n\",\n        \"SEAMLESS = False #@param{type:\\\"boolean\\\"}\\n\",\n        \"\\n\",\n        \"#@markdown ## Auto-remove PNG directory\\n\",\n        \"#@markdown Auto-delete output PNG dir after ffmpeg video creation. Set this to `False` if you want to keep the PNG files.\\n\",\n        \"AUTO_REMOVE = True #@param{type:\\\"boolean\\\"}\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"N9cGwalNeyk9\",\n        \"cellView\": \"form\"\n      },\n      \"source\": [\n        \"#@title Connect Google Drive\\n\",\n        \"from google.colab import drive\\n\",\n        \"drive.mount('/content/gdrive')\\n\",\n        \"print('Google Drive connected.')\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"irzjv1x4e3S4\",\n        \"cellView\": \"form\"\n      },\n      \"source\": [\n        \"#@title Check your current GPU\\n\",\n        \"# If you are lucky, you get 16GB VRAM. If you are not lucky, you get less. VRAM is important. The more VRAM, the higher the maximum resolution will go.\\n\",\n        \"\\n\",\n        \"# 16GB: Can handle 720p. 1080p will procude an out-of-memory error. \\n\",\n        \"# 8GB: Can handle 480p. 720p will produce an out-of-memory error.\\n\",\n        \"\\n\",\n        \"!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {\n        \"id\": \"UYHTTP91oMvh\"\n      },\n      \"source\": [\n        \"# Install dependencies.\\n\",\n        \"\\n\",\n        \"This next step may take somewhere between 15-20 minutes. Run this only once at startup.\\n\",\n        \"\\n\",\n        \"Look for the \\\"Finished installing dependencies\\\"  message.\"\n      ]\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"e5AHGetTRacZ\",\n        \"cellView\": \"form\"\n      },\n      \"source\": [\n        \"#@title Setup everything. This takes a while. Just wait ~20 minutes in total.\\n\",\n        \"\\n\",\n        \"# Install old pytorch to avoid faulty output\\n\",\n        \"%cd /content/\\n\",\n        \"!wget -c https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\\n\",\n        \"!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\\n\",\n        \"!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\\n\",\n        \"!conda install pytorch==1.1 cudatoolkit torchvision -c pytorch -y\\n\",\n        \"!conda install ipykernel -y\\n\",\n        \"\\n\",\n        \"!pip install scipy==1.1.0\\n\",\n        \"!pip install imageio\\n\",\n        \"!CUDA_VISIBLE_DEVICES=0\\n\",\n        \"!sudo apt-get install imagemagick imagemagick-doc\\n\",\n        \"print(\\\"Finished installing dependencies.\\\")\\n\",\n        \"\\n\",\n        \"# Clone DAIN sources\\n\",\n        \"%cd /content\\n\",\n        \"!git clone -b master --depth 1 https://github.com/baowenbo/DAIN /content/DAIN\\n\",\n        \"%cd /content/DAIN\\n\",\n        \"!git log -1\\n\",\n        \"\\n\",\n        \"# Building DAIN\\n\",\n        \"%cd /content/DAIN/my_package/\\n\",\n        \"!./build.sh\\n\",\n        \"print(\\\"Building #1 done.\\\")\\n\",\n        \"\\n\",\n        \"# Building DAIN PyTorch correlation package.\\n\",\n        \"%cd /content/DAIN/PWCNet/correlation_package_pytorch1_0\\n\",\n        \"!./build.sh\\n\",\n        \"print(\\\"Building #2 done.\\\")\\n\",\n        \"\\n\",\n        \"# Downloading pre-trained model\\n\",\n        \"%cd /content/DAIN\\n\",\n        \"!mkdir model_weights\\n\",\n        \"!wget -O model_weights/best.pth http://vllab1.ucmerced.edu/~wenbobao/DAIN/best.pth\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"zm5kn6vTncL4\",\n        \"cellView\": \"form\"\n      },\n      \"source\": [\n        \"#@title Detecting FPS of input file.\\n\",\n        \"%shell yes | cp -f /content/gdrive/My\\\\ Drive/{INPUT_FILEPATH} /content/DAIN/\\n\",\n        \"\\n\",\n        \"import os\\n\",\n        \"filename = os.path.basename(INPUT_FILEPATH)\\n\",\n        \"\\n\",\n        \"import cv2\\n\",\n        \"cap = cv2.VideoCapture(f'/content/DAIN/{filename}')\\n\",\n        \"\\n\",\n        \"fps = cap.get(cv2.CAP_PROP_FPS)\\n\",\n        \"print(f\\\"Input file has {fps} fps\\\")\\n\",\n        \"\\n\",\n        \"if(fps/TARGET_FPS>0.5):\\n\",\n        \"  print(\\\"Define a higher fps, because there is not enough time for new frames. (Old FPS)/(New FPS) should be lower than 0.5. Interpolation will fail if you try.\\\")\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"9YNva-GuKq4Y\",\n        \"cellView\": \"form\"\n      },\n      \"source\": [\n        \"#@title ffmpeg extract - Generating individual frame PNGs from the source file.\\n\",\n        \"%shell rm -rf '{FRAME_INPUT_DIR}'\\n\",\n        \"%shell mkdir -p '{FRAME_INPUT_DIR}'\\n\",\n        \"\\n\",\n        \"if (END_FRAME==-1):\\n\",\n        \"  %shell ffmpeg -i '/content/DAIN/{filename}' -vf 'select=gte(n\\\\,{START_FRAME}),setpts=PTS-STARTPTS' '{FRAME_INPUT_DIR}/%05d.png'\\n\",\n        \"else:\\n\",\n        \"  %shell ffmpeg -i '/content/DAIN/{filename}' -vf 'select=between(n\\\\,{START_FRAME}\\\\,{END_FRAME}),setpts=PTS-STARTPTS' '{FRAME_INPUT_DIR}/%05d.png'\\n\",\n        \"\\n\",\n        \"from IPython.display import clear_output\\n\",\n        \"clear_output()\\n\",\n        \"\\n\",\n        \"png_generated_count_command_result = %shell ls '{FRAME_INPUT_DIR}' | wc -l\\n\",\n        \"frame_count = int(png_generated_count_command_result.output.strip())\\n\",\n        \"\\n\",\n        \"import shutil\\n\",\n        \"if SEAMLESS:\\n\",\n        \"  frame_count += 1\\n\",\n        \"  first_frame = f\\\"{FRAME_INPUT_DIR}/00001.png\\\"\\n\",\n        \"  new_last_frame = f\\\"{FRAME_INPUT_DIR}/{frame_count.zfill(5)}.png\\\"\\n\",\n        \"  shutil.copyfile(first_frame, new_last_frame)\\n\",\n        \"\\n\",\n        \"print(f\\\"{frame_count} frame PNGs generated.\\\")\\n\",\n        \"\\n\",\n        \"#Checking if PNGs do have alpha\\n\",\n        \"import subprocess as sp\\n\",\n        \"%cd {FRAME_INPUT_DIR}\\n\",\n        \"channels = sp.getoutput('identify -format %[channels] 00001.png')\\n\",\n        \"print (f\\\"{channels} detected\\\")\\n\",\n        \"\\n\",\n        \"# Removing alpha if detected\\n\",\n        \"if \\\"a\\\" in channels:\\n\",\n        \"  print(\\\"Alpha channel detected and will be removed.\\\")\\n\",\n        \"  print(sp.getoutput('find . -name \\\"*.png\\\" -exec convert \\\"{}\\\" -alpha off PNG24:\\\"{}\\\" \\\\;'))\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"W3rrE7L824gL\",\n        \"cellView\": \"form\"\n      },\n      \"source\": [\n        \"#@title Interpolation\\n\",\n        \"%shell mkdir -p '{FRAME_OUTPUT_DIR}'\\n\",\n        \"%cd /content/DAIN\\n\",\n        \"\\n\",\n        \"!python -W ignore colab_interpolate.py --netName DAIN_slowmotion --time_step {fps/TARGET_FPS} --start_frame 1 --end_frame {frame_count} --frame_input_dir '{FRAME_INPUT_DIR}' --frame_output_dir '{FRAME_OUTPUT_DIR}'\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"TKREDli2IDMV\",\n        \"cellView\": \"form\"\n      },\n      \"source\": [\n        \"#@title Create output video\\n\",\n        \"%cd {FRAME_OUTPUT_DIR}\\n\",\n        \"%shell ffmpeg -y -r {TARGET_FPS} -f image2 -pattern_type glob -i '*.png' '/content/gdrive/My Drive/{OUTPUT_FILE_PATH}'\\n\",\n        \"\\n\",\n        \"if(AUTO_REMOVE):\\n\",\n        \"  !rm -rf {FRAME_OUTPUT_DIR}/*\\n\",\n        \"\\n\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    },\n    {\n      \"cell_type\": \"code\",\n      \"metadata\": {\n        \"id\": \"UF5TEo5N374o\",\n        \"cellView\": \"form\"\n      },\n      \"source\": [\n        \"#@title [Experimental] Create video with sound\\n\",\n        \"# Only run this, if the original had sound.\\n\",\n        \"%cd {FRAME_OUTPUT_DIR}\\n\",\n        \"%shell ffmpeg -i '/content/DAIN/{filename}' -acodec copy output-audio.aac\\n\",\n        \"%shell ffmpeg -y -r {TARGET_FPS} -f image2 -pattern_type glob -i '*.png' -i output-audio.aac -shortest '/content/gdrive/My Drive/{OUTPUT_FILE_PATH}'\\n\",\n        \"\\n\",\n        \"if (AUTO_REMOVE):\\n\",\n        \"  !rm -rf {FRAME_OUTPUT_DIR}/*\\n\",\n        \"  !rm -rf output-audio.aac\"\n      ],\n      \"execution_count\": null,\n      \"outputs\": []\n    }\n  ]\n}\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.041015625,
          "content": "MIT License\n\nCopyright (c) 2019 Wenbo Bao\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MegaDepth",
          "type": "tree",
          "content": null
        },
        {
          "name": "PWCNet",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.0263671875,
          "content": "# DAIN (Depth-Aware Video Frame Interpolation)\n[Project](https://sites.google.com/view/wenbobao/dain) **|** [Paper](http://arxiv.org/abs/1904.00830)\n\n[Wenbo Bao](https://sites.google.com/view/wenbobao/home),\n[Wei-Sheng Lai](http://graduatestudents.ucmerced.edu/wlai24/), \n[Chao Ma](https://sites.google.com/site/chaoma99/),\nXiaoyun Zhang, \nZhiyong Gao, \nand [Ming-Hsuan Yang](http://faculty.ucmerced.edu/mhyang/)\n\nIEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CVPR 2019\n\nThis work is developed based on our TPAMI work [MEMC-Net](https://github.com/baowenbo/MEMC-Net), where we propose the adaptive warping layer. Please also consider referring to it.\n\n### Table of Contents\n1. [Introduction](#introduction)\n1. [Citation](#citation)\n1. [Requirements and Dependencies](#requirements-and-dependencies)\n1. [Installation](#installation)\n1. [Testing Pre-trained Models](#testing-pre-trained-models)\n1. [Downloading Results](#downloading-results)\n1. [Slow-motion Generation](#slow-motion-generation)\n1. [Training New Models](#training-new-models)\n1. [Google Colab Demo](#google-colab-demo)\n\n### Introduction\nWe propose the **D**epth-**A**ware video frame **IN**terpolation (**DAIN**) model to explicitly detect the occlusion by exploring the depth cue.\nWe develop a depth-aware flow projection layer to synthesize intermediate flows that preferably sample closer objects than farther ones.\nOur method achieves state-of-the-art performance on the Middlebury dataset. \nWe provide videos [here](https://www.youtube.com/watch?v=-f8f0igQi5I&t=5s).\n\n<!--![teaser](http://vllab.ucmerced.edu/wlai24/LapSRN/images/emma_text.gif)-->\n\n<!--[![teaser](https://img.youtube.com/vi/icJ0WbPsE20/0.jpg)](https://www.youtube.com/watch?v=icJ0WbPsE20&feature=youtu.be)\n<!--<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/icJ0WbPsE20\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n![teaser](http://vllab1.ucmerced.edu/~wenbobao/DAIN/kart-turn_compare.gif)\n\n\n<!--哈哈我是注释，不会在浏览器中显示。\nBeanbags\nhttps://drive.google.com/open?id=170vdxANGoNKO5_8MYOuiDvoIXzucv7HW\nDimentrodon\nhttps://drive.google.com/open?id=14n7xvb9hjTKqfcr7ZpEFyfMvx6E8NhD_\nDogDance\nhttps://drive.google.com/open?id=1YWAyAJ3T48fMFv2K8j8wIVcmQm39cRof\nGrove2\nhttps://drive.google.com/open?id=1sJLwdQdL6JYXSQo_Bev0aQMleWacxCsN\nGrove3\nhttps://drive.google.com/open?id=1jGj3UdGppoJO02Of8ZaNXqDH4fnXuQ8O\nHydrangea\nhttps://drive.google.com/open?id=1_4kVlhvrmCv54aXi7vZMk3-FtRQF7s0s\nMiniCooper\nhttps://drive.google.com/open?id=1pWHtyBSZsOTC7NTVdHTrv1W-dxa95BLo\nRubberWhale\nhttps://drive.google.com/open?id=1korbXsGpSgJn7THBHkLRVrJMtCt5YZPB\nUrban2\nhttps://drive.google.com/open?id=1v57RMm9x5vM36mCgPy5hresXDZWtw3Vs\nUrban3\nhttps://drive.google.com/open?id=1LMwSU0PrG4_GaDjWRI2v9hvWpYwzRKca\nVenus\nhttps://drive.google.com/open?id=1piPnEexuHaiAr4ZzWSAxGi1u1Xo_6vPp\nWalking\nhttps://drive.google.com/open?id=1CgCLmVC_WTVTAcA_IdWbLqR8MS18zHoa\n-->\n\n<p float=\"middle\">\n<img src=\"https://drive.google.com/uc?export=view&id=1YWAyAJ3T48fMFv2K8j8wIVcmQm39cRof\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1CgCLmVC_WTVTAcA_IdWbLqR8MS18zHoa\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1pWHtyBSZsOTC7NTVdHTrv1W-dxa95BLo\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=170vdxANGoNKO5_8MYOuiDvoIXzucv7HW\" width=\"200\"/>\n</p>\n\n<p float=\"middle\">\n<img src=\"https://drive.google.com/uc?export=view&id=1sJLwdQdL6JYXSQo_Bev0aQMleWacxCsN\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1jGj3UdGppoJO02Of8ZaNXqDH4fnXuQ8O\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1v57RMm9x5vM36mCgPy5hresXDZWtw3Vs\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1LMwSU0PrG4_GaDjWRI2v9hvWpYwzRKca\" width=\"200\"/>\n</p>\n\n<p float=\"middle\">\n<img src=\"https://drive.google.com/uc?export=view&id=1piPnEexuHaiAr4ZzWSAxGi1u1Xo_6vPp\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1korbXsGpSgJn7THBHkLRVrJMtCt5YZPB\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=1_4kVlhvrmCv54aXi7vZMk3-FtRQF7s0s\" width=\"200\"/>\n<img src=\"https://drive.google.com/uc?export=view&id=14n7xvb9hjTKqfcr7ZpEFyfMvx6E8NhD_\" width=\"200\"/>\n</p>\n\n### Citation\nIf you find the code and datasets useful in your research, please cite:\n\n    @inproceedings{DAIN,\n        author    = {Bao, Wenbo and Lai, Wei-Sheng and Ma, Chao and Zhang, Xiaoyun and Gao, Zhiyong and Yang, Ming-Hsuan}, \n        title     = {Depth-Aware Video Frame Interpolation}, \n        booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},\n        year      = {2019}\n    }\n    @article{MEMC-Net,\n         title={MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement},\n         author={Bao, Wenbo and Lai, Wei-Sheng, and Zhang, Xiaoyun and Gao, Zhiyong and Yang, Ming-Hsuan},\n         journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\n         doi={10.1109/TPAMI.2019.2941941},\n         year={2018}\n    }\n\n### Requirements and Dependencies\n- Ubuntu (We test with Ubuntu = 16.04.5 LTS)\n- Python (We test with Python = 3.6.8 in Anaconda3 = 4.1.1)\n- Cuda & Cudnn (We test with Cuda = 9.0 and Cudnn = 7.0)\n- PyTorch (The customized depth-aware flow projection and other layers require ATen API in PyTorch = 1.0.0)\n- GCC (Compiling PyTorch 1.0.0 extension files (.c/.cu) requires gcc = 4.9.1 and nvcc = 9.0 compilers)\n- NVIDIA GPU (We use Titan X (Pascal) with compute = 6.1, but we support compute_50/52/60/61 devices, should you have devices with higher compute capability, please revise [this](https://github.com/baowenbo/DAIN/blob/master/my_package/DepthFlowProjection/setup.py))\n\n### Installation\nDownload repository:\n\n    $ git clone https://github.com/baowenbo/DAIN.git\n\nBefore building Pytorch extensions, be sure you have `pytorch >= 1.0.0`:\n    \n    $ python -c \"import torch; print(torch.__version__)\"\n    \nGenerate our PyTorch extensions:\n    \n    $ cd DAIN\n    $ cd my_package \n    $ ./build.sh\n\nGenerate the Correlation package required by [PWCNet](https://github.com/NVlabs/PWC-Net/tree/master/PyTorch/external_packages/correlation-pytorch-master):\n    \n    $ cd ../PWCNet/correlation_package_pytorch1_0\n    $ ./build.sh\n\n\n### Testing Pre-trained Models\nMake model weights dir and Middlebury dataset dir:\n\n    $ cd DAIN\n    $ mkdir model_weights\n    $ mkdir MiddleBurySet\n    \nDownload pretrained models, \n\n    $ cd model_weights\n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/best.pth\n    \nand Middlebury dataset:\n    \n    $ cd ../MiddleBurySet\n    $ wget http://vision.middlebury.edu/flow/data/comp/zip/other-color-allframes.zip\n    $ unzip other-color-allframes.zip\n    $ wget http://vision.middlebury.edu/flow/data/comp/zip/other-gt-interp.zip\n    $ unzip other-gt-interp.zip\n    $ cd ..\n\npreinstallations:\n\n    $ cd PWCNet/correlation_package_pytorch1_0\n    $ sh build.sh\n    $ cd ../my_package\n    $ sh build.sh\n    $ cd ..\n\nWe are good to go by:\n\n    $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury.py\n\nThe interpolated results are under `MiddleBurySet/other-result-author/[random number]/`, where the `random number` is used to distinguish different runnings. \n\n### Downloading Results\nOur DAIN model achieves the state-of-the-art performance on the UCF101, Vimeo90K, and Middlebury ([*eval*](http://vision.middlebury.edu/flow/eval/results/results-n1.php) and *other*).\nDownload our interpolated results with:\n    \n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/UCF101_DAIN.zip\n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/Vimeo90K_interp_DAIN.zip\n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/Middlebury_eval_DAIN.zip\n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/Middlebury_other_DAIN.zip\n    \n    \n### Slow-motion Generation\nOur model is fully capable of generating slow-motion effect with minor modification on the network architecture.\nRun the following code by specifying `time_step = 0.25` to generate x4 slow-motion effect:\n\n    $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.25\n\nor set `time_step` to `0.125` or `0.1` as follows \n\n    $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.125\n    $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.1\nto generate x8 and x10 slow-motion respectively. Or if you would like to have x100 slow-motion for a little fun.\n    \n    $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.01\n\nYou may also want to create gif animations by:\n    \n    $ cd MiddleBurySet/other-result-author/[random number]/Beanbags\n    $ convert -delay 1 *.png -loop 0 Beanbags.gif //1*10ms delay \n\nHave fun and enjoy yourself! \n\n\n### Training New Models\nDownload the Vimeo90K triplet dataset for video frame interpolation task, also see [here](https://github.com/anchen1011/toflow/blob/master/download_dataset.sh) by [Xue et al., IJCV19](https://arxiv.org/abs/1711.09078).\n    \n    $ cd DAIN\n    $ mkdir /path/to/your/dataset & cd /path/to/your/dataset \n    $ wget http://data.csail.mit.edu/tofu/dataset/vimeo_triplet.zip\n    $ unzip vimeo_triplet.zip\n    $ rm vimeo_triplet.zip\n\nDownload the pretrained MegaDepth and PWCNet models\n    \n    $ cd MegaDepth/checkpoints/test_local\n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/best_generalization_net_G.pth\n    $ cd ../../../PWCNet\n    $ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/pwc_net.pth.tar\n    $ cd  ..\n    \nRun the training script:\n\n    $ CUDA_VISIBLE_DEVICES=0 python train.py --datasetPath /path/to/your/dataset --batch_size 1 --save_which 1 --lr 0.0005 --rectify_lr 0.0005 --flow_lr_coe 0.01 --occ_lr_coe 0.0 --filter_lr_coe 1.0 --ctx_lr_coe 1.0 --alpha 0.0 1.0 --patience 4 --factor 0.2\n    \nThe optimized models will be saved to the `model_weights/[random number]` directory, where [random number] is generated for different runs.\n\nReplace the pre-trained `model_weights/best.pth` model with the newly trained `model_weights/[random number]/best.pth` model.\nThen test the new model by executing: \n\n    $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury.py\n\n### Google Colab Demo\nThis is a modification of DAIN that allows the usage of Google Colab and is able to do a full demo interpolation from a source video to a target video.\n\nOriginal Notebook File by btahir can be found [here](https://github.com/baowenbo/DAIN/issues/44).\n\nTo use the Colab, follow these steps:\n\n- Download the `Colab_DAIN.ipynb` file ([link](https://raw.githubusercontent.com/baowenbo/DAIN/master/Colab_DAIN.ipynb)).\n- Visit Google Colaboratory ([link](https://colab.research.google.com/))\n- Select the \"Upload\" option, and upload the `.ipynb` file\n- Start running the cells one by one, following the instructions.\n\nColab file authors: [Styler00Dollar](https://github.com/styler00dollar) and [Alpha](https://github.com/AlphaGit).\n\n### Contact\n[Wenbo Bao](mailto:bwb0813@gmail.com); [Wei-Sheng (Jason) Lai](mailto:phoenix104104@gmail.com)\n\n### License\nSee [MIT License](https://github.com/baowenbo/DAIN/blob/master/LICENSE)\n"
        },
        {
          "name": "Resblock",
          "type": "tree",
          "content": null
        },
        {
          "name": "S2D_models",
          "type": "tree",
          "content": null
        },
        {
          "name": "Stack.py",
          "type": "blob",
          "size": 0.4599609375,
          "content": "\nclass Stack:\n    def __init__(self):\n        self.stack = []\n    def pop(self):\n        if self.is_empty():\n            return None\n        else:\n            return self.stack.pop()\n    def push(self,val):\n        return self.stack.append(val)\n    def peak(self):\n        if self.is_empty():\n            return None\n        else:\n            return self.stack[-1]\n    def size(self):\n        return len(self.stack)\n    def is_empty(self):\n        return self.size() == 0"
        },
        {
          "name": "balancedsampler.py",
          "type": "blob",
          "size": 1.693359375,
          "content": "from torch.utils.data.sampler import Sampler\nimport torch\n\nclass RandomBalancedSampler(Sampler):\n    \"\"\"Samples elements randomly, with an arbitrary size, independant from dataset length.\n    this is a balanced sampling that will sample the whole dataset with a random permutation.\n\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    \"\"\"\n\n    def __init__(self, data_source, epoch_size):\n        self.data_size = len(data_source)\n        self.epoch_size = epoch_size\n        self.index = 0\n\n    def __next__(self):\n        if self.index == 0:\n            #re-shuffle the sampler\n            self.indices = torch.randperm(self.data_size)\n        self.index = (self.index+1)%self.data_size\n        return self.indices[self.index]\n\n    def next(self):\n        return self.__next__()\n\n    def __iter__(self):\n        return self\n\n    def __len__(self):\n        return min(self.data_size,self.epoch_size) if self.epoch_size>0 else self.data_size\n\nclass SequentialBalancedSampler(Sampler):\n    \"\"\"Samples elements dequentially, with an arbitrary size, independant from dataset length.\n    this is a balanced sampling that will sample the whole dataset before resetting it.\n\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    \"\"\"\n\n    def __init__(self, data_source, epoch_size):\n        self.data_size = len(data_source)\n        self.epoch_size = epoch_size\n        self.index = 0\n\n    def __next__(self):\n        self.index = (self.index+1)%self.data_size\n        return self.index\n\n    def next(self):\n        return self.__next__()\n\n    def __iter__(self):\n        return self\n\n    def __len__(self):\n        return min(self.data_size,self.epoch_size) if self.epoch_size>0 else self.data_size\n"
        },
        {
          "name": "colab_interpolate.py",
          "type": "blob",
          "size": 6.3359375,
          "content": "import time\nimport os\nfrom torch.autograd import Variable\nimport torch\nimport numpy as np\nimport numpy\nimport networks\nfrom my_args import args\nfrom imageio import imread, imsave\nfrom AverageMeter import  *\nimport shutil\nimport datetime\ntorch.backends.cudnn.benchmark = True\n\nmodel = networks.__dict__[args.netName](\n                                    channel = args.channels,\n                                    filter_size = args.filter_size,\n                                    timestep = args.time_step,\n                                    training = False)\n\nif args.use_cuda:\n    model = model.cuda()\n\nmodel_path = './model_weights/best.pth'\nif not os.path.exists(model_path):\n    print(\"*****************************************************************\")\n    print(\"**** We couldn't load any trained weights ***********************\")\n    print(\"*****************************************************************\")\n    exit(1)\n\nif args.use_cuda:\n    pretrained_dict = torch.load(model_path)\nelse:\n    pretrained_dict = torch.load(model_path, map_location=lambda storage, loc: storage)\n\nmodel_dict = model.state_dict()\n# 1. filter out unnecessary keys\npretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n# 2. overwrite entries in the existing state dict\nmodel_dict.update(pretrained_dict)\n# 3. load the new state dict\nmodel.load_state_dict(model_dict)\n# 4. release the pretrained dict for saving memory\npretrained_dict = []\n\nmodel = model.eval() # deploy mode\n\nframes_dir = args.frame_input_dir\noutput_dir = args.frame_output_dir\n\ntimestep = args.time_step\ntime_offsets = [kk * timestep for kk in range(1, int(1.0 / timestep))]\n\ninput_frame = args.start_frame - 1\nloop_timer = AverageMeter()\n\nfinal_frame = args.end_frame\n\ntorch.set_grad_enabled(False)\n\n# we want to have input_frame between (start_frame-1) and (end_frame-2)\n# this is because at each step we read (frame) and (frame+1)\n# so the last iteration will actuall be (end_frame-1) and (end_frame)\nwhile input_frame < final_frame - 1:\n    input_frame += 1\n\n    start_time = time.time()\n\n    filename_frame_1 = os.path.join(frames_dir, f'{input_frame:0>5d}.png')\n    filename_frame_2 = os.path.join(frames_dir, f'{input_frame+1:0>5d}.png')\n\n    X0 = torch.from_numpy(np.transpose(imread(filename_frame_1), (2,0,1)).astype(\"float32\") / 255.0).type(args.dtype)\n    X1 = torch.from_numpy(np.transpose(imread(filename_frame_2), (2,0,1)).astype(\"float32\") / 255.0).type(args.dtype)\n\n    assert (X0.size(1) == X1.size(1))\n    assert (X0.size(2) == X1.size(2))\n\n    intWidth = X0.size(2)\n    intHeight = X0.size(1)\n    channels = X0.size(0)\n    if not channels == 3:\n        print(f\"Skipping {filename_frame_1}-{filename_frame_2} -- expected 3 color channels but found {channels}.\")\n        continue\n\n    if intWidth != ((intWidth >> 7) << 7):\n        intWidth_pad = (((intWidth >> 7) + 1) << 7)  # more than necessary\n        intPaddingLeft = int((intWidth_pad - intWidth) / 2)\n        intPaddingRight = intWidth_pad - intWidth - intPaddingLeft\n    else:\n        intPaddingLeft = 32\n        intPaddingRight= 32\n\n    if intHeight != ((intHeight >> 7) << 7):\n        intHeight_pad = (((intHeight >> 7) + 1) << 7)  # more than necessary\n        intPaddingTop = int((intHeight_pad - intHeight) / 2)\n        intPaddingBottom = intHeight_pad - intHeight - intPaddingTop\n    else:\n        intPaddingTop = 32\n        intPaddingBottom = 32\n\n    pader = torch.nn.ReplicationPad2d([intPaddingLeft, intPaddingRight, intPaddingTop, intPaddingBottom])\n\n    X0 = Variable(torch.unsqueeze(X0,0))\n    X1 = Variable(torch.unsqueeze(X1,0))\n    X0 = pader(X0)\n    X1 = pader(X1)\n\n    if args.use_cuda:\n        X0 = X0.cuda()\n        X1 = X1.cuda()\n\n    y_s, offset, filter = model(torch.stack((X0, X1),dim = 0))\n    y_ = y_s[args.save_which]\n\n    if args.use_cuda:\n        X0 = X0.data.cpu().numpy()\n        if not isinstance(y_, list):\n            y_ = y_.data.cpu().numpy()\n        else:\n            y_ = [item.data.cpu().numpy() for item in y_]\n        offset = [offset_i.data.cpu().numpy() for offset_i in offset]\n        filter = [filter_i.data.cpu().numpy() for filter_i in filter]  if filter[0] is not None else None\n        X1 = X1.data.cpu().numpy()\n    else:\n        X0 = X0.data.numpy()\n        if not isinstance(y_, list):\n            y_ = y_.data.numpy()\n        else:\n            y_ = [item.data.numpy() for item in y_]\n        offset = [offset_i.data.numpy() for offset_i in offset]\n        filter = [filter_i.data.numpy() for filter_i in filter]\n        X1 = X1.data.numpy()\n\n    X0 = np.transpose(255.0 * X0.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0))\n    y_ = [np.transpose(255.0 * item.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight,\n                                intPaddingLeft:intPaddingLeft+intWidth], (1, 2, 0)) for item in y_]\n    offset = [np.transpose(offset_i[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0)) for offset_i in offset]\n    filter = [np.transpose(\n        filter_i[0, :, intPaddingTop:intPaddingTop + intHeight, intPaddingLeft: intPaddingLeft + intWidth],\n        (1, 2, 0)) for filter_i in filter]  if filter is not None else None\n    X1 = np.transpose(255.0 * X1.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0))\n\n    interpolated_frame_number = 0\n    shutil.copy(filename_frame_1, os.path.join(output_dir, f\"{input_frame:0>5d}{interpolated_frame_number:0>3d}.png\"))\n    for item, time_offset in zip(y_, time_offsets):\n        interpolated_frame_number += 1\n        output_frame_file_path = os.path.join(output_dir, f\"{input_frame:0>5d}{interpolated_frame_number:0>3d}.png\")\n        imsave(output_frame_file_path, np.round(item).astype(numpy.uint8))\n\n    end_time = time.time()\n    loop_timer.update(end_time - start_time)\n\n    frames_left = final_frame - input_frame\n    estimated_seconds_left = frames_left * loop_timer.avg\n    estimated_time_left = datetime.timedelta(seconds=estimated_seconds_left)\n    print(f\"****** Processed frame {input_frame} | Time per frame (avg): {loop_timer.avg:2.2f}s | Time left: {estimated_time_left} ******************\" )\n\n# Copying last frame\nlast_frame_filename = os.path.join(frames_dir, str(str(final_frame).zfill(5))+'.png')\nshutil.copy(last_frame_filename, os.path.join(output_dir, f\"{final_frame:0>5d}{0:0>3d}.png\"))\n\nprint(\"Finished processing images.\")\n"
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo_MiddleBury.py",
          "type": "blob",
          "size": 6.7470703125,
          "content": "import time\nimport os\nfrom torch.autograd import Variable\nimport math\nimport torch\n\nimport random\nimport numpy as np\nimport numpy\nimport networks\nfrom my_args import  args\n\nfrom scipy.misc import imread, imsave\nfrom AverageMeter import  *\n\ntorch.backends.cudnn.benchmark = True # to speed up the\n\n\nDO_MiddleBurryOther = True\nMB_Other_DATA = \"./MiddleBurySet/other-data/\"\nMB_Other_RESULT = \"./MiddleBurySet/other-result-author/\"\nMB_Other_GT = \"./MiddleBurySet/other-gt-interp/\"\nif not os.path.exists(MB_Other_RESULT):\n    os.mkdir(MB_Other_RESULT)\n\n\n\nmodel = networks.__dict__[args.netName](channel=args.channels,\n                            filter_size = args.filter_size ,\n                            timestep=args.time_step,\n                            training=False)\n\nif args.use_cuda:\n    model = model.cuda()\n\nargs.SAVED_MODEL = './model_weights/best.pth'\nif os.path.exists(args.SAVED_MODEL):\n    print(\"The testing model weight is: \" + args.SAVED_MODEL)\n    if not args.use_cuda:\n        pretrained_dict = torch.load(args.SAVED_MODEL, map_location=lambda storage, loc: storage)\n        # model.load_state_dict(torch.load(args.SAVED_MODEL, map_location=lambda storage, loc: storage))\n    else:\n        pretrained_dict = torch.load(args.SAVED_MODEL)\n        # model.load_state_dict(torch.load(args.SAVED_MODEL))\n\n    model_dict = model.state_dict()\n    # 1. filter out unnecessary keys\n    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n    # 2. overwrite entries in the existing state dict\n    model_dict.update(pretrained_dict)\n    # 3. load the new state dict\n    model.load_state_dict(model_dict)\n    # 4. release the pretrained dict for saving memory\n    pretrained_dict = []\nelse:\n    print(\"*****************************************************************\")\n    print(\"**** We don't load any trained weights **************************\")\n    print(\"*****************************************************************\")\n\nmodel = model.eval() # deploy mode\n\n\nuse_cuda=args.use_cuda\nsave_which=args.save_which\ndtype = args.dtype\nunique_id =str(random.randint(0, 100000))\nprint(\"The unique id for current testing is: \" + str(unique_id))\n\ninterp_error = AverageMeter()\nif DO_MiddleBurryOther:\n    subdir = os.listdir(MB_Other_DATA)\n    gen_dir = os.path.join(MB_Other_RESULT, unique_id)\n    os.mkdir(gen_dir)\n\n    tot_timer = AverageMeter()\n    proc_timer = AverageMeter()\n    end = time.time()\n    for dir in subdir:\n        print(dir)\n        os.mkdir(os.path.join(gen_dir, dir))\n        arguments_strFirst = os.path.join(MB_Other_DATA, dir, \"frame10.png\")\n        arguments_strSecond = os.path.join(MB_Other_DATA, dir, \"frame11.png\")\n        arguments_strOut = os.path.join(gen_dir, dir, \"frame10i11.png\")\n        gt_path = os.path.join(MB_Other_GT, dir, \"frame10i11.png\")\n\n        X0 =  torch.from_numpy( np.transpose(imread(arguments_strFirst) , (2,0,1)).astype(\"float32\")/ 255.0).type(dtype)\n        X1 =  torch.from_numpy( np.transpose(imread(arguments_strSecond) , (2,0,1)).astype(\"float32\")/ 255.0).type(dtype)\n\n\n        y_ = torch.FloatTensor()\n\n        assert (X0.size(1) == X1.size(1))\n        assert (X0.size(2) == X1.size(2))\n\n        intWidth = X0.size(2)\n        intHeight = X0.size(1)\n        channel = X0.size(0)\n        if not channel == 3:\n            continue\n\n        if intWidth != ((intWidth >> 7) << 7):\n            intWidth_pad = (((intWidth >> 7) + 1) << 7)  # more than necessary\n            intPaddingLeft =int(( intWidth_pad - intWidth)/2)\n            intPaddingRight = intWidth_pad - intWidth - intPaddingLeft\n        else:\n            intWidth_pad = intWidth\n            intPaddingLeft = 32\n            intPaddingRight= 32\n\n        if intHeight != ((intHeight >> 7) << 7):\n            intHeight_pad = (((intHeight >> 7) + 1) << 7)  # more than necessary\n            intPaddingTop = int((intHeight_pad - intHeight) / 2)\n            intPaddingBottom = intHeight_pad - intHeight - intPaddingTop\n        else:\n            intHeight_pad = intHeight\n            intPaddingTop = 32\n            intPaddingBottom = 32\n\n        pader = torch.nn.ReplicationPad2d([intPaddingLeft, intPaddingRight , intPaddingTop, intPaddingBottom])\n\n        torch.set_grad_enabled(False)\n        X0 = Variable(torch.unsqueeze(X0,0))\n        X1 = Variable(torch.unsqueeze(X1,0))\n        X0 = pader(X0)\n        X1 = pader(X1)\n\n        if use_cuda:\n            X0 = X0.cuda()\n            X1 = X1.cuda()\n        proc_end = time.time()\n        y_s,offset,filter = model(torch.stack((X0, X1),dim = 0))\n        y_ = y_s[save_which]\n\n        proc_timer.update(time.time() -proc_end)\n        tot_timer.update(time.time() - end)\n        end  = time.time()\n        print(\"*****************current image process time \\t \" + str(time.time()-proc_end )+\"s ******************\" )\n        if use_cuda:\n            X0 = X0.data.cpu().numpy()\n            y_ = y_.data.cpu().numpy()\n            offset = [offset_i.data.cpu().numpy() for offset_i in offset]\n            filter = [filter_i.data.cpu().numpy() for filter_i in filter]  if filter[0] is not None else None\n            X1 = X1.data.cpu().numpy()\n        else:\n            X0 = X0.data.numpy()\n            y_ = y_.data.numpy()\n            offset = [offset_i.data.numpy() for offset_i in offset]\n            filter = [filter_i.data.numpy() for filter_i in filter]\n            X1 = X1.data.numpy()\n\n\n\n        X0 = np.transpose(255.0 * X0.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0))\n        y_ = np.transpose(255.0 * y_.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0))\n        offset = [np.transpose(offset_i[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0)) for offset_i in offset]\n        filter = [np.transpose(\n            filter_i[0, :, intPaddingTop:intPaddingTop + intHeight, intPaddingLeft: intPaddingLeft + intWidth],\n            (1, 2, 0)) for filter_i in filter]  if filter is not None else None\n        X1 = np.transpose(255.0 * X1.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0))\n\n\n        imsave(arguments_strOut, np.round(y_).astype(numpy.uint8))\n\n\n        rec_rgb =  imread(arguments_strOut)\n        gt_rgb = imread(gt_path)\n\n        diff_rgb = 128.0 + rec_rgb - gt_rgb\n        avg_interp_error_abs = np.mean(np.abs(diff_rgb - 128.0))\n\n        interp_error.update(avg_interp_error_abs, 1)\n\n        mse = numpy.mean((diff_rgb - 128.0) ** 2)\n\n        PIXEL_MAX = 255.0\n        psnr = 20 * math.log10(PIXEL_MAX / math.sqrt(mse))\n\n        print(\"interpolation error / PSNR : \" + str(round(avg_interp_error_abs,4)) + \" / \" + str(round(psnr,4)))\n        metrics = \"The average interpolation error / PSNR for all images are : \" + str(round(interp_error.avg, 4))\n        print(metrics)\n\n"
        },
        {
          "name": "demo_MiddleBury_slowmotion.py",
          "type": "blob",
          "size": 7.484375,
          "content": "import time\nimport os\nfrom torch.autograd import Variable\nimport torch\nimport random\nimport numpy as np\nimport numpy\nimport networks\nfrom my_args import  args\nfrom scipy.misc import imread, imsave\nfrom AverageMeter import  *\nimport shutil\n\ntorch.backends.cudnn.benchmark = True # to speed up the\n\nDO_MiddleBurryOther = True\nMB_Other_DATA = \"./MiddleBurySet/other-data/\"\nMB_Other_RESULT = \"./MiddleBurySet/other-result-author/\"\nMB_Other_GT = \"./MiddleBurySet/other-gt-interp/\"\nif not os.path.exists(MB_Other_RESULT):\n    os.mkdir(MB_Other_RESULT)\n\n\n\nmodel = networks.__dict__[args.netName](    channel=args.channels,\n                                    filter_size = args.filter_size ,\n                                    timestep=args.time_step,\n                                    training=False)\n\nif args.use_cuda:\n    model = model.cuda()\n\nargs.SAVED_MODEL = './model_weights/best.pth'\nif os.path.exists(args.SAVED_MODEL):\n    print(\"The testing model weight is: \" + args.SAVED_MODEL)\n    if not args.use_cuda:\n        pretrained_dict = torch.load(args.SAVED_MODEL, map_location=lambda storage, loc: storage)\n        # model.load_state_dict(torch.load(args.SAVED_MODEL, map_location=lambda storage, loc: storage))\n    else:\n        pretrained_dict = torch.load(args.SAVED_MODEL)\n        # model.load_state_dict(torch.load(args.SAVED_MODEL))\n\n    model_dict = model.state_dict()\n    # 1. filter out unnecessary keys\n    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n    # 2. overwrite entries in the existing state dict\n    model_dict.update(pretrained_dict)\n    # 3. load the new state dict\n    model.load_state_dict(model_dict)\n    # 4. release the pretrained dict for saving memory\n    pretrained_dict = []\nelse:\n    print(\"*****************************************************************\")\n    print(\"**** We don't load any trained weights **************************\")\n    print(\"*****************************************************************\")\n\nmodel = model.eval() # deploy mode\n\nuse_cuda=args.use_cuda\nsave_which=args.save_which\ndtype = args.dtype\nunique_id =str(random.randint(0, 100000))\nprint(\"The unique id for current testing is: \" + str(unique_id))\n\ninterp_error = AverageMeter()\nif DO_MiddleBurryOther:\n    subdir = os.listdir(MB_Other_DATA)\n    gen_dir = os.path.join(MB_Other_RESULT, unique_id)\n    os.mkdir(gen_dir)\n\n    tot_timer = AverageMeter()\n    proc_timer = AverageMeter()\n    end = time.time()\n    for dir in subdir: \n        print(dir)\n        os.mkdir(os.path.join(gen_dir, dir))\n        arguments_strFirst = os.path.join(MB_Other_DATA, dir, \"frame10.png\")\n        arguments_strSecond = os.path.join(MB_Other_DATA, dir, \"frame11.png\")\n        gt_path = os.path.join(MB_Other_GT, dir, \"frame10i11.png\")\n\n        X0 =  torch.from_numpy( np.transpose(imread(arguments_strFirst) , (2,0,1)).astype(\"float32\")/ 255.0).type(dtype)\n        X1 =  torch.from_numpy( np.transpose(imread(arguments_strSecond) , (2,0,1)).astype(\"float32\")/ 255.0).type(dtype)\n\n\n        y_ = torch.FloatTensor()\n\n        assert (X0.size(1) == X1.size(1))\n        assert (X0.size(2) == X1.size(2))\n\n        intWidth = X0.size(2)\n        intHeight = X0.size(1)\n        channel = X0.size(0)\n        if not channel == 3:\n            continue\n\n        if intWidth != ((intWidth >> 7) << 7):\n            intWidth_pad = (((intWidth >> 7) + 1) << 7)  # more than necessary\n            intPaddingLeft =int(( intWidth_pad - intWidth)/2)\n            intPaddingRight = intWidth_pad - intWidth - intPaddingLeft\n        else:\n            intWidth_pad = intWidth\n            intPaddingLeft = 32\n            intPaddingRight= 32\n\n        if intHeight != ((intHeight >> 7) << 7):\n            intHeight_pad = (((intHeight >> 7) + 1) << 7)  # more than necessary\n            intPaddingTop = int((intHeight_pad - intHeight) / 2)\n            intPaddingBottom = intHeight_pad - intHeight - intPaddingTop\n        else:\n            intHeight_pad = intHeight\n            intPaddingTop = 32\n            intPaddingBottom = 32\n\n        pader = torch.nn.ReplicationPad2d([intPaddingLeft, intPaddingRight , intPaddingTop, intPaddingBottom])\n\n        torch.set_grad_enabled(False)\n        X0 = Variable(torch.unsqueeze(X0,0))\n        X1 = Variable(torch.unsqueeze(X1,0))\n        X0 = pader(X0)\n        X1 = pader(X1)\n\n        if use_cuda:\n            X0 = X0.cuda()\n            X1 = X1.cuda()\n        proc_end = time.time()\n        y_s,offset,filter = model(torch.stack((X0, X1),dim = 0))\n        y_ = y_s[save_which]\n\n        proc_timer.update(time.time() -proc_end)\n        tot_timer.update(time.time() - end)\n        end  = time.time()\n        print(\"*****************current image process time \\t \" + str(time.time()-proc_end )+\"s ******************\" )\n        if use_cuda:\n            X0 = X0.data.cpu().numpy()\n            if not isinstance(y_, list):\n                y_ = y_.data.cpu().numpy()\n            else:\n                y_ = [item.data.cpu().numpy() for item in y_]\n            offset = [offset_i.data.cpu().numpy() for offset_i in offset]\n            filter = [filter_i.data.cpu().numpy() for filter_i in filter]  if filter[0] is not None else None\n            X1 = X1.data.cpu().numpy()\n        else:\n            X0 = X0.data.numpy()\n            if not isinstance(y_, list):\n                y_ = y_.data.numpy()\n            else:\n                y_ = [item.data.numpy() for item in y_]\n            offset = [offset_i.data.numpy() for offset_i in offset]\n            filter = [filter_i.data.numpy() for filter_i in filter]\n            X1 = X1.data.numpy()\n\n\n\n        X0 = np.transpose(255.0 * X0.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0))\n        y_ = [np.transpose(255.0 * item.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight,\n                                  intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0)) for item in y_]\n        offset = [np.transpose(offset_i[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0)) for offset_i in offset]\n        filter = [np.transpose(\n            filter_i[0, :, intPaddingTop:intPaddingTop + intHeight, intPaddingLeft: intPaddingLeft + intWidth],\n            (1, 2, 0)) for filter_i in filter]  if filter is not None else None\n        X1 = np.transpose(255.0 * X1.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0))\n\n        timestep = args.time_step\n        numFrames = int(1.0 / timestep) - 1\n        time_offsets = [kk * timestep for kk in range(1, 1 + numFrames, 1)]\n        # for item, time_offset  in zip(y_,time_offsets):\n        #     arguments_strOut = os.path.join(gen_dir, dir, \"frame10_i{:.3f}_11.png\".format(time_offset))\n        #\n        #     imsave(arguments_strOut, np.round(item).astype(numpy.uint8))\n        #\n        # # copy the first and second reference frame\n        # shutil.copy(arguments_strFirst, os.path.join(gen_dir, dir,  \"frame10_i{:.3f}_11.png\".format(0)))\n        # shutil.copy(arguments_strSecond, os.path.join(gen_dir, dir,  \"frame11_i{:.3f}_11.png\".format(1)))\n\n        count = 0\n        shutil.copy(arguments_strFirst, os.path.join(gen_dir, dir, \"{:0>4d}.png\".format(count)))\n        count  = count+1\n        for item, time_offset in zip(y_, time_offsets):\n            arguments_strOut = os.path.join(gen_dir, dir, \"{:0>4d}.png\".format(count))\n            count = count + 1\n            imsave(arguments_strOut, np.round(item).astype(numpy.uint8))\n        shutil.copy(arguments_strSecond, os.path.join(gen_dir, dir, \"{:0>4d}.png\".format(count)))\n        count = count + 1\n\n\n         "
        },
        {
          "name": "environment.yaml",
          "type": "blob",
          "size": 2.9521484375,
          "content": "name: pytorch1.0.0\nchannels:\n  - pytorch\n  - serge-sans-paille\n  - anaconda\n  - conda-forge\n  - defaults\ndependencies:\n  - ca-certificates=2019.1.23=0\n  - certifi=2018.11.29=py36_0\n  - cloudpickle=0.7.0=py_0\n  - cytoolz=0.9.0.1=py36h14c3975_1\n  - dask-core=1.1.1=py_0\n  - decorator=4.3.2=py36_0\n  - imageio=2.4.1=py36_0\n  - networkx=2.2=py36_1\n  - openssl=1.1.1=h7b6447c_0\n  - pywavelets=1.0.1=py36hdd07704_0\n  - scikit-image=0.14.1=py36he6710b0_0\n  - scipy=1.1.0=py36h7c811a0_0\n  - toolz=0.9.0=py36_0\n  - cycler=0.10.0=py_1\n  - expat=2.2.5=hf484d3e_1002\n  - fontconfig=2.13.1=h2176d3f_1000\n  - gettext=0.19.8.1=h9745a5d_1001\n  - glib=2.56.2=had28632_1001\n  - icu=58.2=hf484d3e_1000\n  - kiwisolver=1.0.1=py36h6bb024c_1002\n  - libiconv=1.15=h14c3975_1004\n  - libprotobuf=3.6.1=hdbcaa40_1000\n  - libuuid=2.32.1=h14c3975_1000\n  - libxcb=1.13=h14c3975_1002\n  - libxml2=2.9.8=h143f9aa_1005\n  - matplotlib=3.0.2=py36_1002\n  - matplotlib-base=3.0.2=py36h167e16e_1002\n  - protobuf=3.6.1=py36hf484d3e_1001\n  - pthread-stubs=0.4=h14c3975_1001\n  - pyparsing=2.3.1=py_0\n  - pyqt=5.6.0=py36h13b7fb3_1008\n  - python-dateutil=2.8.0=py_0\n  - sip=4.18.1=py36hf484d3e_1000\n  - tensorboardx=1.6=py_0\n  - tk=8.6.9=h84994c4_1000\n  - tornado=5.1.1=py36h14c3975_1000\n  - xorg-libxau=1.0.9=h14c3975_0\n  - xorg-libxdmcp=1.1.2=h14c3975_1007\n  - blas=1.0=mkl\n  - cffi=1.11.5=py36he75722e_1\n  - cudatoolkit=9.0=h13b8566_0\n  - dbus=1.13.2=h714fa37_1\n  - freetype=2.9.1=h8a8886c_1\n  - gst-plugins-base=1.14.0=hbbd80ab_1\n  - gstreamer=1.14.0=hb453b48_1\n  - intel-openmp=2019.1=144\n  - isl=0.12.2=0\n  - jpeg=9b=h024ee3a_2\n  - libedit=3.1.20181209=hc058e9b_0\n  - libffi=3.2.1=hd88cf55_4\n  - libgcc-ng=8.2.0=hdf63c60_1\n  - libgfortran-ng=7.3.0=hdf63c60_0\n  - libpng=1.6.36=hbc83047_0\n  - libstdcxx-ng=8.2.0=hdf63c60_1\n  - libtiff=4.0.10=h2733197_2\n  - mkl=2019.1=144\n  - mkl_fft=1.0.10=py36ha843d7b_0\n  - mkl_random=1.0.2=py36hd81dba3_0\n  - mpc=1.0.3=hf803216_4\n  - mpfr=3.1.5=h12ff648_1\n  - ncurses=6.1=he6710b0_1\n  - ninja=1.8.2=py36h6bb024c_1\n  - numpy=1.15.4=py36h7e9f1db_0\n  - numpy-base=1.15.4=py36hde5b4d6_0\n  - olefile=0.46=py36_0\n  - pcre=8.42=h439df22_0\n  - pillow=5.4.1=py36h34e0f95_0\n  - pip=19.0.1=py36_0\n  - pycparser=2.19=py36_0\n  - python=3.6.8=h0371630_0\n  - qt=5.6.3=h8bf5577_3\n  - readline=7.0=h7b6447c_5\n  - setuptools=40.8.0=py36_0\n  - six=1.12.0=py36_0\n  - sqlite=3.26.0=h7b6447c_0\n  - wheel=0.32.3=py36_0\n  - xz=5.2.4=h14c3975_4\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.3.7=h0b5b093_0\n  - pytorch=1.0.1=py3.6_cuda9.0.176_cudnn7.4.2_2\n  - torchvision=0.2.1=py_2\n  - cloog=0.18.1=1\n  - gcc_49=4.9.1=6\n  - gmp=5.1.3=0\n  - pip:\n    - correlation-cuda==0.0.0\n    - dask==1.1.1\n    - depthflowprojection-cuda==0.0.0\n    - filterinterpolation-cuda==0.0.0\n    - flowprojection-cuda==0.0.0\n    - interpolation-cuda==0.0.0\n    - interpolationch-cuda==0.0.0\n    - mindepthflowprojection-cuda==0.0.0\n    - separableconv-cuda==0.0.0\n    - separableconvflow-cuda==0.0.0\n    - torch==1.0.1.post2\nprefix: /home/wenbobao/anaconda3_new/envs/pytorch1.0.0\n\n"
        },
        {
          "name": "loss_function.py",
          "type": "blob",
          "size": 3.103515625,
          "content": "import sys\nimport os\n\nimport sys\nimport  threading\nimport torch\nfrom torch.autograd import Variable\nfrom lr_scheduler import *\nfrom torch.autograd import gradcheck\n\nimport numpy\n\n\n\n\ndef charbonier_loss(x,epsilon):\n    loss = torch.mean(torch.sqrt(x * x + epsilon * epsilon))\n    return loss\ndef negPSNR_loss(x,epsilon):\n    loss = torch.mean(torch.mean(torch.mean(torch.sqrt(x * x + epsilon * epsilon),dim=1),dim=1),dim=1)\n    return torch.mean(-torch.log(1.0/loss) /100.0)\n\ndef tv_loss(x,epsilon):\n    loss = torch.mean( torch.sqrt(\n        (x[:, :, :-1, :-1] - x[:, :, 1:, :-1]) ** 2 +\n        (x[:, :, :-1, :-1] - x[:, :, :-1, 1:]) ** 2 + epsilon *epsilon\n            )\n        )\n    return loss\n\n    \ndef gra_adap_tv_loss(flow, image, epsilon):\n    w = torch.exp( - torch.sum(\ttorch.abs(image[:,:,:-1, :-1] - image[:,:,1:, :-1]) + \n                            torch.abs(image[:,:,:-1, :-1] - image[:,:,:-1, 1:]), dim = 1))\t\t\n    tv = torch.sum(torch.sqrt((flow[:, :, :-1, :-1] - flow[:, :, 1:, :-1]) ** 2 + (flow[:, :, :-1, :-1] - flow[:, :, :-1, 1:]) ** 2 + epsilon *epsilon) ,dim=1)             \n    loss = torch.mean( w * tv )\n    return loss\t\n        \ndef smooth_loss(x,epsilon):\n    loss = torch.mean(\n        torch.sqrt(\n            (x[:,:,:-1,:-1] - x[:,:,1:,:-1]) **2 +\n            (x[:,:,:-1,:-1] - x[:,:,:-1,1:]) **2+ epsilon**2\n        )\n    )\n    return loss\n    \n    \ndef motion_sym_loss(offset, epsilon, occlusion = None):\n    if occlusion == None:\n        # return torch.mean(torch.sqrt( (offset[:,:2,...] + offset[:,2:,...])**2 + epsilon **2))\n        return torch.mean(torch.sqrt( (offset[0] + offset[1])**2 + epsilon **2))\n    else:\n        # TODO: how to design the occlusion aware offset symmetric loss?\n        # return torch.mean(torch.sqrt((offset[:,:2,...] + offset[:,2:,...])**2 + epsilon **2))\n        return torch.mean(torch.sqrt((offset[0] + offset[1])**2 + epsilon **2))\n\n\n\n    \ndef part_loss(diffs, offsets, occlusions, images, epsilon, use_negPSNR=False):\n    if use_negPSNR:\n        pixel_loss = [negPSNR_loss(diff, epsilon) for diff in diffs]\n    else:\n        pixel_loss = [charbonier_loss(diff, epsilon) for diff in diffs]\n    #offset_loss = [tv_loss(offset[0], epsilon) + tv_loss(offset[1], epsilon) for offset in\n    #               offsets]\n\n    if offsets[0][0] is not None:\n        offset_loss = [gra_adap_tv_loss(offset[0],images[0], epsilon) + gra_adap_tv_loss(offset[1], images[1], epsilon) for offset in\n                   offsets]\n    else:\n        offset_loss = [Variable(torch.zeros(1).cuda())]\n    # print(torch.max(occlusions[0]))\n    # print(torch.min(occlusions[0]))\n    # print(torch.mean(occlusions[0]))\n\n    # occlusion_loss = [smooth_loss(occlusion, epsilon) + charbonier_loss(occlusion - 0.5, epsilon) for occlusion in occlusions]\n    # occlusion_loss = [smooth_loss(occlusion, epsilon) + charbonier_loss(occlusion[:, 0, ...] - occlusion[:, 1, ...], epsilon) for occlusion in occlusions]\n\n\n\n    sym_loss = [motion_sym_loss(offset,epsilon=epsilon) for offset in offsets]\n    # sym_loss = [ motion_sym_loss(offset,occlusion) for offset,occlusion in zip(offsets,occlusions)]\n    return pixel_loss, offset_loss, sym_loss\n\n"
        },
        {
          "name": "lr_scheduler.py",
          "type": "blob",
          "size": 12.5751953125,
          "content": "from bisect import bisect_right\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\n\r\nclass _LRScheduler(object):\r\n    def __init__(self, optimizer, last_epoch=-1):\r\n        if not isinstance(optimizer, Optimizer):\r\n            raise TypeError('{} is not an Optimizer'.format(\r\n                type(optimizer).__name__))\r\n        self.optimizer = optimizer\r\n        if last_epoch == -1:\r\n            for group in optimizer.param_groups:\r\n                group.setdefault('initial_lr', group['lr'])\r\n        else:\r\n            for i, group in enumerate(optimizer.param_groups):\r\n                if 'initial_lr' not in group:\r\n                    raise KeyError(\"param 'initial_lr' is not specified \"\r\n                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\r\n        self.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\r\n        self.step(last_epoch + 1)\r\n        self.last_epoch = last_epoch\r\n\r\n    def get_lr(self):\r\n        raise NotImplementedError\r\n\r\n    def step(self, epoch=None):\r\n        if epoch is None:\r\n            epoch = self.last_epoch + 1\r\n        self.last_epoch = epoch\r\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\r\n            param_group['lr'] = lr\r\n\r\n\r\nclass LambdaLR(_LRScheduler):\r\n    \"\"\"Sets the learning rate of each parameter group to the initial lr\r\n    times a given function. When last_epoch=-1, sets initial lr as lr.\r\n\r\n    Args:\r\n        optimizer (Optimizer): Wrapped optimizer.\r\n        lr_lambda (function or list): A function which computes a multiplicative\r\n            factor given an integer parameter epoch, or a list of such\r\n            functions, one for each group in optimizer.param_groups.\r\n        last_epoch (int): The index of last epoch. Default: -1.\r\n\r\n    Example:\r\n        >>> # Assuming optimizer has two groups.\r\n        >>> lambda1 = lambda epoch: epoch // 30\r\n        >>> lambda2 = lambda epoch: 0.95 ** epoch\r\n        >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\r\n        >>> for epoch in range(100):\r\n        >>>     scheduler.step()\r\n        >>>     train(...)\r\n        >>>     validate(...)\r\n    \"\"\"\r\n    def __init__(self, optimizer, lr_lambda, last_epoch=-1):\r\n        self.optimizer = optimizer\r\n        if not isinstance(lr_lambda, list) and not isinstance(lr_lambda, tuple):\r\n            self.lr_lambdas = [lr_lambda] * len(optimizer.param_groups)\r\n        else:\r\n            if len(lr_lambda) != len(optimizer.param_groups):\r\n                raise ValueError(\"Expected {} lr_lambdas, but got {}\".format(\r\n                    len(optimizer.param_groups), len(lr_lambda)))\r\n            self.lr_lambdas = list(lr_lambda)\r\n        self.last_epoch = last_epoch\r\n        super(LambdaLR, self).__init__(optimizer, last_epoch)\r\n\r\n    def get_lr(self):\r\n        return [base_lr * lmbda(self.last_epoch)\r\n                for lmbda, base_lr in zip(self.lr_lambdas, self.base_lrs)]\r\n\r\n\r\n\r\nclass StepLR(_LRScheduler):\r\n    \"\"\"Sets the learning rate of each parameter group to the initial lr\r\n    decayed by gamma every step_size epochs. When last_epoch=-1, sets\r\n    initial lr as lr.\r\n\r\n    Args:\r\n        optimizer (Optimizer): Wrapped optimizer.\r\n        step_size (int): Period of learning rate decay.\r\n        gamma (float): Multiplicative factor of learning rate decay.\r\n            Default: 0.1.\r\n        last_epoch (int): The index of last epoch. Default: -1.\r\n\r\n    Example:\r\n        >>> # Assuming optimizer uses lr = 0.5 for all groups\r\n        >>> # lr = 0.05     if epoch < 30\r\n        >>> # lr = 0.005    if 30 <= epoch < 60\r\n        >>> # lr = 0.0005   if 60 <= epoch < 90\r\n        >>> # ...\r\n        >>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\r\n        >>> for epoch in range(100):\r\n        >>>     scheduler.step()\r\n        >>>     train(...)\r\n        >>>     validate(...)\r\n    \"\"\"\r\n\r\n    def __init__(self, optimizer, step_size, gamma=0.1, last_epoch=-1):\r\n        self.step_size = step_size\r\n        self.gamma = gamma\r\n        super(StepLR, self).__init__(optimizer, last_epoch)\r\n\r\n    def get_lr(self):\r\n        return [base_lr * self.gamma ** (self.last_epoch // self.step_size)\r\n                for base_lr in self.base_lrs]\r\n\r\n\r\n\r\nclass MultiStepLR(_LRScheduler):\r\n    \"\"\"Set the learning rate of each parameter group to the initial lr decayed\r\n    by gamma once the number of epoch reaches one of the milestones. When\r\n    last_epoch=-1, sets initial lr as lr.\r\n\r\n    Args:\r\n        optimizer (Optimizer): Wrapped optimizer.\r\n        milestones (list): List of epoch indices. Must be increasing.\r\n        gamma (float): Multiplicative factor of learning rate decay.\r\n            Default: 0.1.\r\n        last_epoch (int): The index of last epoch. Default: -1.\r\n\r\n    Example:\r\n        >>> # Assuming optimizer uses lr = 0.5 for all groups\r\n        >>> # lr = 0.05     if epoch < 30\r\n        >>> # lr = 0.005    if 30 <= epoch < 80\r\n        >>> # lr = 0.0005   if epoch >= 80\r\n        >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\r\n        >>> for epoch in range(100):\r\n        >>>     scheduler.step()\r\n        >>>     train(...)\r\n        >>>     validate(...)\r\n    \"\"\"\r\n\r\n    def __init__(self, optimizer, milestones, gamma=0.1, last_epoch=-1):\r\n        if not list(milestones) == sorted(milestones):\r\n            raise ValueError('Milestones should be a list of'\r\n                             ' increasing integers. Got {}', milestones)\r\n        self.milestones = milestones\r\n        self.gamma = gamma\r\n        super(MultiStepLR, self).__init__(optimizer, last_epoch)\r\n\r\n    def get_lr(self):\r\n        return [base_lr * self.gamma ** bisect_right(self.milestones, self.last_epoch)\r\n                for base_lr in self.base_lrs]\r\n\r\n\r\n\r\nclass ExponentialLR(_LRScheduler):\r\n    \"\"\"Set the learning rate of each parameter group to the initial lr decayed\r\n    by gamma every epoch. When last_epoch=-1, sets initial lr as lr.\r\n\r\n    Args:\r\n        optimizer (Optimizer): Wrapped optimizer.\r\n        gamma (float): Multiplicative factor of learning rate decay.\r\n        last_epoch (int): The index of last epoch. Default: -1.\r\n    \"\"\"\r\n\r\n    def __init__(self, optimizer, gamma, last_epoch=-1):\r\n        self.gamma = gamma\r\n        super(ExponentialLR, self).__init__(optimizer, last_epoch)\r\n\r\n    def get_lr(self):\r\n        return [base_lr * self.gamma ** self.last_epoch\r\n                for base_lr in self.base_lrs]\r\n\r\n\r\n\r\nclass ReduceLROnPlateau(object):\r\n    \"\"\"Reduce learning rate when a metric has stopped improving.\r\n    Models often benefit from reducing the learning rate by a factor\r\n    of 2-10 once learning stagnates. This scheduler reads a metrics\r\n    quantity and if no improvement is seen for a 'patience' number\r\n    of epochs, the learning rate is reduced.\r\n\r\n    Args:\r\n        optimizer (Optimizer): Wrapped optimizer.\r\n        mode (str): One of `min`, `max`. In `min` mode, lr will\r\n            be reduced when the quantity monitored has stopped\r\n            decreasing; in `max` mode it will be reduced when the\r\n            quantity monitored has stopped increasing. Default: 'min'.\r\n        factor (float): Factor by which the learning rate will be\r\n            reduced. new_lr = lr * factor. Default: 0.1.\r\n        patience (int): Number of epochs with no improvement after\r\n            which learning rate will be reduced. Default: 10.\r\n        verbose (bool): If True, prints a message to stdout for\r\n            each update. Default: False.\r\n        threshold (float): Threshold for measuring the new optimum,\r\n            to only focus on significant changes. Default: 1e-4.\r\n        threshold_mode (str): One of `rel`, `abs`. In `rel` mode,\r\n            dynamic_threshold = best * ( 1 + threshold ) in 'max'\r\n            mode or best * ( 1 - threshold ) in `min` mode.\r\n            In `abs` mode, dynamic_threshold = best + threshold in\r\n            `max` mode or best - threshold in `min` mode. Default: 'rel'.\r\n        cooldown (int): Number of epochs to wait before resuming\r\n            normal operation after lr has been reduced. Default: 0.\r\n        min_lr (float or list): A scalar or a list of scalars. A\r\n            lower bound on the learning rate of all param groups\r\n            or each group respectively. Default: 0.\r\n        eps (float): Minimal decay applied to lr. If the difference\r\n            between new and old lr is smaller than eps, the update is\r\n            ignored. Default: 1e-8.\r\n\r\n    Example:\r\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\r\n        >>> scheduler = ReduceLROnPlateau(optimizer, 'min')\r\n        >>> for epoch in range(10):\r\n        >>>     train(...)\r\n        >>>     val_loss = validate(...)\r\n        >>>     # Note that step should be called after validate()\r\n        >>>     scheduler.step(val_loss)\r\n    \"\"\"\r\n\r\n    def __init__(self, optimizer, mode='min', factor=0.1, patience=10,\r\n                 verbose=False, threshold=1e-4, threshold_mode='rel',\r\n                 cooldown=0, min_lr=0, eps=1e-8):\r\n\r\n        if factor >= 1.0:\r\n            raise ValueError('Factor should be < 1.0.')\r\n        self.factor = factor\r\n\r\n        if not isinstance(optimizer, Optimizer):\r\n            raise TypeError('{} is not an Optimizer'.format(\r\n                type(optimizer).__name__))\r\n        self.optimizer = optimizer\r\n\r\n        if isinstance(min_lr, list) or isinstance(min_lr, tuple):\r\n            if len(min_lr) != len(optimizer.param_groups):\r\n                raise ValueError(\"expected {} min_lrs, got {}\".format(\r\n                    len(optimizer.param_groups), len(min_lr)))\r\n            self.min_lrs = list(min_lr)\r\n        else:\r\n            self.min_lrs = [min_lr] * len(optimizer.param_groups)\r\n\r\n        self.patience = patience\r\n        self.verbose = verbose\r\n        self.cooldown = cooldown\r\n        self.cooldown_counter = 0\r\n        self.mode = mode\r\n        self.threshold = threshold\r\n        self.threshold_mode = threshold_mode\r\n        self.best = None\r\n        self.num_bad_epochs = None\r\n        self.mode_worse = None  # the worse value for the chosen mode\r\n        self.is_better = None\r\n        self.eps = eps\r\n        self.last_epoch = -1\r\n        self._init_is_better(mode=mode, threshold=threshold,\r\n                             threshold_mode=threshold_mode)\r\n        self._reset()\r\n\r\n    def _reset(self):\r\n        \"\"\"Resets num_bad_epochs counter and cooldown counter.\"\"\"\r\n        self.best = self.mode_worse\r\n        self.cooldown_counter = 0\r\n        self.num_bad_epochs = 0\r\n\r\n    def step(self, metrics, epoch=None):\r\n        current = metrics\r\n        if epoch is None:\r\n            epoch = self.last_epoch = self.last_epoch + 1\r\n        self.last_epoch = epoch\r\n\r\n        if self.is_better(current, self.best):\r\n            self.best = current\r\n            self.num_bad_epochs = 0\r\n        else:\r\n            self.num_bad_epochs += 1\r\n\r\n        if self.in_cooldown:\r\n            self.cooldown_counter -= 1\r\n            self.num_bad_epochs = 0  # ignore any bad epochs in cooldown\r\n\r\n        if self.num_bad_epochs > self.patience:\r\n            self._reduce_lr(epoch)\r\n            self.cooldown_counter = self.cooldown\r\n            self.num_bad_epochs = 0\r\n\r\n    def _reduce_lr(self, epoch):\r\n        for i, param_group in enumerate(self.optimizer.param_groups):\r\n            old_lr = float(param_group['lr'])\r\n            new_lr = max(old_lr * self.factor, self.min_lrs[i])\r\n            if old_lr - new_lr > self.eps:\r\n                param_group['lr'] = new_lr\r\n                if self.verbose:\r\n                    print('Epoch {:5d}: reducing learning rate'\r\n                          ' of group {} to {:.4e}.'.format(epoch, i, new_lr))\r\n\r\n    @property\r\n    def in_cooldown(self):\r\n        return self.cooldown_counter > 0\r\n\r\n    def _init_is_better(self, mode, threshold, threshold_mode):\r\n        if mode not in {'min', 'max'}:\r\n            raise ValueError('mode ' + mode + ' is unknown!')\r\n        if threshold_mode not in {'rel', 'abs'}:\r\n            raise ValueError('threshold mode ' + mode + ' is unknown!')\r\n        if mode == 'min' and threshold_mode == 'rel':\r\n            rel_epsilon = 1. - threshold\r\n            self.is_better = lambda a, best: a < best * rel_epsilon\r\n            self.mode_worse = float('Inf')\r\n        elif mode == 'min' and threshold_mode == 'abs':\r\n            self.is_better = lambda a, best: a < best - threshold\r\n            self.mode_worse = float('Inf')\r\n        elif mode == 'max' and threshold_mode == 'rel':\r\n            rel_epsilon = threshold + 1.\r\n            self.is_better = lambda a, best: a > best * rel_epsilon\r\n            self.mode_worse = -float('Inf')\r\n        else:  # mode == 'max' and epsilon_mode == 'abs':\r\n            self.is_better = lambda a, best: a > best + threshold\r\n            self.mode_worse = -float('Inf')"
        },
        {
          "name": "my_args.py",
          "type": "blob",
          "size": 6.5205078125,
          "content": "import os\nimport datetime\nimport argparse\nimport numpy\nimport networks\nimport  torch\nmodelnames =  networks.__all__\n# import datasets\ndatasetNames = ('Vimeo_90K_interp') #datasets.__all__\n\nparser = argparse.ArgumentParser(description='DAIN')\n\nparser.add_argument('--debug',action = 'store_true', help='Enable debug mode')\nparser.add_argument('--netName', type=str, default='DAIN',\n                    choices = modelnames,help = 'model architecture: ' +\n                        ' | '.join(modelnames) +\n                        ' (default: DAIN)')\n\nparser.add_argument('--datasetName', default='Vimeo_90K_interp',\n                    choices= datasetNames,nargs='+',\n                    help='dataset type : ' +\n                        ' | '.join(datasetNames) +\n                        ' (default: Vimeo_90K_interp)')\nparser.add_argument('--datasetPath',default='',help = 'the path of selected datasets')\nparser.add_argument('--dataset_split', type = int, default=97, help = 'Split a dataset into trainining and validation by percentage (default: 97)')\n\nparser.add_argument('--seed', type=int, default=1, help='random seed (default: 1)')\n\nparser.add_argument('--numEpoch', '-e', type = int, default=100, help= 'Number of epochs to train(default:150)')\n\nparser.add_argument('--batch_size', '-b',type = int ,default=1, help = 'batch size (default:1)' )\nparser.add_argument('--workers', '-w', type =int,default=8, help = 'parallel workers for loading training samples (default : 1.6*10 = 16)')\nparser.add_argument('--channels', '-c', type=int,default=3,choices = [1,3], help ='channels of images (default:3)')\nparser.add_argument('--filter_size', '-f', type=int, default=4, help = 'the size of filters used (default: 4)',\n                    choices=[2,4,6, 5,51]\n                    )\n\n\nparser.add_argument('--lr', type =float, default= 0.002, help= 'the basic learning rate for three subnetworks (default: 0.002)')\nparser.add_argument('--rectify_lr', type=float, default=0.001, help  = 'the learning rate for rectify/refine subnetworks (default: 0.001)')\n\nparser.add_argument('--save_which', '-s', type=int, default=1, choices=[0,1], help='choose which result to save: 0 ==> interpolated, 1==> rectified')\nparser.add_argument('--time_step',  type=float, default=0.5, help='choose the time steps')\nparser.add_argument('--flow_lr_coe', type = float, default=0.01, help = 'relative learning rate w.r.t basic learning rate (default: 0.01)')\nparser.add_argument('--occ_lr_coe', type = float, default=1.0, help = 'relative learning rate w.r.t basic learning rate (default: 1.0)')\nparser.add_argument('--filter_lr_coe', type = float, default=1.0, help = 'relative learning rate w.r.t basic learning rate (default: 1.0)')\nparser.add_argument('--ctx_lr_coe', type = float, default=1.0, help = 'relative learning rate w.r.t basic learning rate (default: 1.0)')\nparser.add_argument('--depth_lr_coe', type = float, default=0.001, help = 'relative learning rate w.r.t basic learning rate (default: 0.01)')\n# parser.add_argument('--deblur_lr_coe', type = float, default=0.01, help = 'relative learning rate w.r.t basic learning rate (default: 0.01)')\n\nparser.add_argument('--alpha', type=float,nargs='+', default=[0.0, 1.0], help= 'the ration of loss for interpolated and rectified result (default: [0.0, 1.0])')\n\nparser.add_argument('--epsilon', type = float, default=1e-6, help = 'the epsilon for charbonier loss,etc (default: 1e-6)')\nparser.add_argument('--weight_decay', type = float, default=0, help = 'the weight decay for whole network ' )\nparser.add_argument('--patience', type=int, default=5, help = 'the patience of reduce on plateou')\nparser.add_argument('--factor', type = float, default=0.2, help = 'the factor of reduce on plateou')\n#\nparser.add_argument('--pretrained', dest='SAVED_MODEL', default=None, help ='path to the pretrained model weights')\nparser.add_argument('--no-date', action='store_true', help='don\\'t append date timestamp to folder' )\nparser.add_argument('--use_cuda', default= True, type = bool, help='use cuda or not')\nparser.add_argument('--use_cudnn',default=1,type=int, help = 'use cudnn or not')\nparser.add_argument('--dtype', default=torch.cuda.FloatTensor, choices = [torch.cuda.FloatTensor,torch.FloatTensor],help = 'tensor data type ')\n# parser.add_argument('--resume', default='', type=str, help='path to latest checkpoint (default: none)')\n\n\nparser.add_argument('--uid', type=str, default= None, help='unique id for the training')\nparser.add_argument('--force', action='store_true', help='force to override the given uid')\n\n# Colab version\nparser.add_argument('--start_frame', type = int, default = 1, help='first frame number to process')\nparser.add_argument('--end_frame', type = int, default = 100, help='last frame number to process')\nparser.add_argument('--frame_input_dir', type = str, default = '/content/DAIN/input_frames', help='frame input directory')\nparser.add_argument('--frame_output_dir', type = str, default = '/content/DAIN/output_frames', help='frame output directory')\n\nargs = parser.parse_args()\n\nimport shutil\n\nif args.uid == None:\n    unique_id = str(numpy.random.randint(0, 100000))\n    print(\"revise the unique id to a random numer \" + str(unique_id))\n    args.uid = unique_id\n    timestamp = datetime.datetime.now().strftime(\"%a-%b-%d-%H-%M\")\n    save_path = './model_weights/'+ args.uid  +'-' + timestamp\nelse:\n    save_path = './model_weights/'+ str(args.uid)\n\n# print(\"no pth here : \" + save_path + \"/best\"+\".pth\")\nif not os.path.exists(save_path + \"/best\"+\".pth\"):\n    # print(\"no pth here : \" + save_path + \"/best\" + \".pth\")\n    os.makedirs(save_path,exist_ok=True)\nelse:\n    if not args.force:\n        raise(\"please use another uid \")\n    else:\n        print(\"override this uid\" + args.uid)\n        for m in range(1,10):\n            if not os.path.exists(save_path+\"/log.txt.bk\" + str(m)):\n                shutil.copy(save_path+\"/log.txt\", save_path+\"/log.txt.bk\"+str(m))\n                shutil.copy(save_path+\"/args.txt\", save_path+\"/args.txt.bk\"+str(m))\n                break\n\n\n\nparser.add_argument('--save_path',default=save_path,help = 'the output dir of weights')\nparser.add_argument('--log', default = save_path+'/log.txt', help = 'the log file in training')\nparser.add_argument('--arg', default = save_path+'/args.txt', help = 'the args used')\n\nargs = parser.parse_args()\n\n\nwith open(args.log, 'w') as f:\n    f.close()\nwith open(args.arg, 'w') as f:\n    print(args)\n    print(args,file=f)\n    f.close()\nif args.use_cudnn:\n    print(\"cudnn is used\")\n    torch.backends.cudnn.benchmark = True  # to speed up the\nelse:\n    print(\"cudnn is not used\")\n    torch.backends.cudnn.benchmark = False  # to speed up the\n\n"
        },
        {
          "name": "my_package",
          "type": "tree",
          "content": null
        },
        {
          "name": "networks",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 12,
          "content": "import sys\nimport os\n\nimport threading\nimport torch\nfrom torch.autograd import Variable\nimport torch.utils.data\nfrom lr_scheduler import *\n\nimport numpy\nfrom AverageMeter import  *\nfrom loss_function import *\nimport datasets\nimport balancedsampler\nimport networks\nfrom my_args import args\n\n\n\ndef train():\n    torch.manual_seed(args.seed)\n\n    model = networks.__dict__[args.netName](channel=args.channels,\n                            filter_size = args.filter_size ,\n                            timestep=args.time_step,\n                            training=True)\n    if args.use_cuda:\n        print(\"Turn the model into CUDA\")\n        model = model.cuda()\n\n    if not args.SAVED_MODEL==None:\n        # args.SAVED_MODEL ='../model_weights/'+ args.SAVED_MODEL + \"/best\" + \".pth\"\n        args.SAVED_MODEL ='./model_weights/best.pth'\n        print(\"Fine tuning on \" +  args.SAVED_MODEL)\n        if not  args.use_cuda:\n            pretrained_dict = torch.load(args.SAVED_MODEL, map_location=lambda storage, loc: storage)\n            # model.load_state_dict(torch.load(args.SAVED_MODEL, map_location=lambda storage, loc: storage))\n        else:\n            pretrained_dict = torch.load(args.SAVED_MODEL)\n            # model.load_state_dict(torch.load(args.SAVED_MODEL))\n        #print([k for k,v in      pretrained_dict.items()])\n\n        model_dict = model.state_dict()\n        # 1. filter out unnecessary keys\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(pretrained_dict)\n        # 3. load the new state dict\n        model.load_state_dict(model_dict)\n        pretrained_dict = None\n\n    if type(args.datasetName) == list:\n        train_sets, test_sets = [],[]\n        for ii, jj in zip(args.datasetName, args.datasetPath):\n            tr_s, te_s = datasets.__dict__[ii](jj, split = args.dataset_split,single = args.single_output, task = args.task)\n            train_sets.append(tr_s)\n            test_sets.append(te_s)\n        train_set = torch.utils.data.ConcatDataset(train_sets)\n        test_set = torch.utils.data.ConcatDataset(test_sets)\n    else:\n        train_set, test_set = datasets.__dict__[args.datasetName](args.datasetPath)\n    train_loader = torch.utils.data.DataLoader(\n        train_set, batch_size = args.batch_size,\n        sampler=balancedsampler.RandomBalancedSampler(train_set, int(len(train_set) / args.batch_size )),\n        num_workers= args.workers, pin_memory=True if args.use_cuda else False)\n\n    val_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batch_size,\n                                             num_workers=args.workers, pin_memory=True if args.use_cuda else False)\n    print('{} samples found, {} train samples and {} test samples '.format(len(test_set)+len(train_set),\n                                                                           len(train_set),\n                                                                           len(test_set)))\n\n\n    # if not args.lr == 0:\n    print(\"train the interpolation net\")\n    optimizer = torch.optim.Adamax([\n                {'params': model.initScaleNets_filter.parameters(), 'lr': args.filter_lr_coe * args.lr},\n                {'params': model.initScaleNets_filter1.parameters(), 'lr': args.filter_lr_coe * args.lr},\n                {'params': model.initScaleNets_filter2.parameters(), 'lr': args.filter_lr_coe * args.lr},\n                {'params': model.ctxNet.parameters(), 'lr': args.ctx_lr_coe * args.lr},\n                {'params': model.flownets.parameters(), 'lr': args.flow_lr_coe * args.lr},\n                {'params': model.depthNet.parameters(), 'lr': args.depth_lr_coe * args.lr},\n                {'params': model.rectifyNet.parameters(), 'lr': args.rectify_lr}\n            ],\n                lr=args.lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=args.weight_decay)\n\n\n    scheduler = ReduceLROnPlateau(optimizer, 'min',factor=args.factor, patience=args.patience,verbose=True)\n\n    print(\"*********Start Training********\")\n    print(\"LR is: \"+ str(float(optimizer.param_groups[0]['lr'])))\n    print(\"EPOCH is: \"+ str(int(len(train_set) / args.batch_size )))\n    print(\"Num of EPOCH is: \"+ str(args.numEpoch))\n    def count_network_parameters(model):\n\n        parameters = filter(lambda p: p.requires_grad, model.parameters())\n        N = sum([numpy.prod(p.size()) for p in parameters])\n\n        return N\n    print(\"Num. of model parameters is :\" + str(count_network_parameters(model)))\n    if hasattr(model,'flownets'):\n        print(\"Num. of flow model parameters is :\" +\n              str(count_network_parameters(model.flownets)))\n    if hasattr(model,'initScaleNets_occlusion'):\n        print(\"Num. of initScaleNets_occlusion model parameters is :\" +\n              str(count_network_parameters(model.initScaleNets_occlusion) +\n                  count_network_parameters(model.initScaleNets_occlusion1) +\n        count_network_parameters(model.initScaleNets_occlusion2)))\n    if hasattr(model,'initScaleNets_filter'):\n        print(\"Num. of initScaleNets_filter model parameters is :\" +\n              str(count_network_parameters(model.initScaleNets_filter) +\n                  count_network_parameters(model.initScaleNets_filter1) +\n        count_network_parameters(model.initScaleNets_filter2)))\n    if hasattr(model, 'ctxNet'):\n        print(\"Num. of ctxNet model parameters is :\" +\n              str(count_network_parameters(model.ctxNet)))\n    if hasattr(model, 'depthNet'):\n        print(\"Num. of depthNet model parameters is :\" +\n              str(count_network_parameters(model.depthNet)))\n    if hasattr(model,'rectifyNet'):\n        print(\"Num. of rectifyNet model parameters is :\" +\n              str(count_network_parameters(model.rectifyNet)))\n\n    training_losses = AverageMeter()\n    auxiliary_data = []\n    saved_total_loss = 10e10\n    saved_total_PSNR = -1\n    ikk = 0\n    for kk in optimizer.param_groups:\n        if kk['lr'] > 0:\n            ikk = kk\n            break\n\n    for t in range(args.numEpoch):\n        print(\"The id of this in-training network is \" + str(args.uid))\n        print(args)\n        #Turn into training mode\n        model = model.train()\n\n        for i, (X0_half,X1_half, y_half) in enumerate(train_loader):\n\n            if i >= int(len(train_set) / args.batch_size ):\n                #(0 if t == 0 else EPOCH):#\n                break\n\n            X0_half = X0_half.cuda() if args.use_cuda else X0_half\n            X1_half = X1_half.cuda() if args.use_cuda else X1_half\n            y_half = y_half.cuda() if args.use_cuda else y_half\n\n            X0 = Variable(X0_half, requires_grad= False)\n            X1 = Variable(X1_half, requires_grad= False)\n            y  = Variable(y_half,requires_grad= False)\n\n            diffs, offsets,filters,occlusions = model(torch.stack((X0,y,X1),dim = 0))\n\n            pixel_loss, offset_loss, sym_loss = part_loss(diffs,offsets,occlusions, [X0,X1],epsilon=args.epsilon)\n\n            total_loss = sum(x*y if x > 0 else 0 for x,y in zip(args.alpha, pixel_loss))\n\n            training_losses.update(total_loss.item(), args.batch_size)\n            if i % max(1, int(int(len(train_set) / args.batch_size )/500.0)) == 0:\n\n                print(\"Ep [\" + str(t) +\"/\" + str(i) +\n                                    \"]\\tl.r.: \" + str(round(float(ikk['lr']),7))+\n                                    \"\\tPix: \" + str([round(x.item(),5) for x in pixel_loss]) +\n                                    \"\\tTV: \" + str([round(x.item(),4)  for x in offset_loss]) +\n                                    \"\\tSym: \" + str([round(x.item(), 4) for x in sym_loss]) +\n                                    \"\\tTotal: \" + str([round(x.item(),5) for x in [total_loss]]) +\n                                    \"\\tAvg. Loss: \" + str([round(training_losses.avg, 5)]))\n\n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n\n        if t == 1:\n            # delete the pre validation weights for cleaner workspace\n            if os.path.exists(args.save_path + \"/epoch\" + str(0) +\".pth\" ):\n                os.remove(args.save_path + \"/epoch\" + str(0) +\".pth\")\n\n        if os.path.exists(args.save_path + \"/epoch\" + str(t-1) +\".pth\"):\n            os.remove(args.save_path + \"/epoch\" + str(t-1) +\".pth\")\n        torch.save(model.state_dict(), args.save_path + \"/epoch\" + str(t) +\".pth\")\n\n        # print(\"\\t\\t**************Start Validation*****************\")\n        #Turn into evaluation mode\n\n        val_total_losses = AverageMeter()\n        val_total_pixel_loss = AverageMeter()\n        val_total_PSNR_loss = AverageMeter()\n        val_total_tv_loss = AverageMeter()\n        val_total_pws_loss = AverageMeter()\n        val_total_sym_loss = AverageMeter()\n\n        for i, (X0,X1,y) in enumerate(val_loader):\n            if i >=  int(len(test_set)/ args.batch_size):\n                break\n\n            with torch.no_grad():\n                X0 = X0.cuda() if args.use_cuda else X0\n                X1 = X1.cuda() if args.use_cuda else X1\n                y = y.cuda() if args.use_cuda else y\n\n                diffs, offsets,filters,occlusions = model(torch.stack((X0,y,X1),dim = 0))\n\n                pixel_loss, offset_loss,sym_loss = part_loss(diffs, offsets, occlusions, [X0,X1],epsilon=args.epsilon)\n\n                val_total_loss = sum(x * y for x, y in zip(args.alpha, pixel_loss))\n\n                per_sample_pix_error = torch.mean(torch.mean(torch.mean(diffs[args.save_which] ** 2,\n                                                                    dim=1),dim=1),dim=1)\n                per_sample_pix_error = per_sample_pix_error.data # extract tensor\n                psnr_loss = torch.mean(20 * torch.log(1.0/torch.sqrt(per_sample_pix_error)))/torch.log(torch.Tensor([10]))\n                #\n\n                val_total_losses.update(val_total_loss.item(),args.batch_size)\n                val_total_pixel_loss.update(pixel_loss[args.save_which].item(), args.batch_size)\n                val_total_tv_loss.update(offset_loss[0].item(), args.batch_size)\n                val_total_sym_loss.update(sym_loss[0].item(), args.batch_size)\n                val_total_PSNR_loss.update(psnr_loss[0],args.batch_size)\n                print(\".\",end='',flush=True)\n\n        print(\"\\nEpoch \" + str(int(t)) +\n              \"\\tlearning rate: \" + str(float(ikk['lr'])) +\n              \"\\tAvg Training Loss: \" + str(round(training_losses.avg,5)) +\n              \"\\tValidate Loss: \" + str([round(float(val_total_losses.avg), 5)]) +\n              \"\\tValidate PSNR: \" + str([round(float(val_total_PSNR_loss.avg), 5)]) +\n              \"\\tPixel Loss: \" + str([round(float(val_total_pixel_loss.avg), 5)]) +\n              \"\\tTV Loss: \" + str([round(float(val_total_tv_loss.avg), 4)]) +\n              \"\\tPWS Loss: \" + str([round(float(val_total_pws_loss.avg), 4)]) +\n              \"\\tSym Loss: \" + str([round(float(val_total_sym_loss.avg), 4)])\n              )\n\n        auxiliary_data.append([t, float(ikk['lr']),\n                                   training_losses.avg, val_total_losses.avg, val_total_pixel_loss.avg,\n                                   val_total_tv_loss.avg,val_total_pws_loss.avg,val_total_sym_loss.avg])\n\n        numpy.savetxt(args.log, numpy.array(auxiliary_data), fmt='%.8f', delimiter=',')\n        training_losses.reset()\n\n        print(\"\\t\\tFinished an epoch, Check and Save the model weights\")\n            # we check the validation loss instead of training loss. OK~\n        if saved_total_loss >= val_total_losses.avg:\n            saved_total_loss = val_total_losses.avg\n            torch.save(model.state_dict(), args.save_path + \"/best\"+\".pth\")\n            print(\"\\t\\tBest Weights updated for decreased validation loss\\n\")\n\n        else:\n            print(\"\\t\\tWeights Not updated for undecreased validation loss\\n\")\n\n        #schdule the learning rate\n        scheduler.step(val_total_losses.avg)\n\n\n    print(\"*********Finish Training********\")\n\nif __name__ == '__main__':\n    sys.setrecursionlimit(100000)# 0xC00000FD exception for the recursive detach of gradients.\n    threading.stack_size(200000000)# 0xC00000FD exception for the recursive detach of gradients.\n    thread = threading.Thread(target=train)\n    thread.start()\n    thread.join()\n\n    exit(0)\n"
        }
      ]
    }
  ]
}