{
  "metadata": {
    "timestamp": 1736560635083,
    "page": 269,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "THUDM/GLM-130B",
      "stars": 7679,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0400390625,
          "content": "data\n__pycache__\nsamples\n.DS_Store\n.idea\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0693359375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright Aohan Zeng\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MODEL_LICENSE",
          "type": "blob",
          "size": 2.29296875,
          "content": "The GLM-130B License\n\n1. Definitions\n\n“Licensor” means the GLM-130B Model Team that distributes its Software.\n\n“Software” means the GLM-130B model parameters made available under this license.\n\n2. License Grant\n\nSubject to the terms and conditions of this License, the Licensor hereby grants to you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license to use the Software solely for your non-commercial research purposes.\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n3. Restriction\n\nYou will not use, copy, modify, merge, publish, distribute, reproduce, or create derivative works of the Software, in whole or in part, for any commercial, military, or illegal purposes.\n\nYou will not use the Software for any act that may undermine China's national security and national unity, harm the public interest of society, or infringe upon the rights and interests of human beings.\n\n4. Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n5. Limitation of Liability\n\nEXCEPT TO THE EXTENT PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER BASED IN TORT, NEGLIGENCE, CONTRACT, LIABILITY, OR OTHERWISE WILL ANY LICENSOR BE LIABLE TO YOU FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, OR ANY OTHER COMMERCIAL LOSSES, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n6. Dispute Resolution\n\nThis license shall be governed and construed in accordance with the laws of People’s Republic of China. Any dispute arising from or in connection with this License shall be submitted to Haidian District People's Court in Beijing.\n\nNote that the license is subject to update to a more comprehensive version.  For any questions related to the license and copyright, please contact us at glm-130b@googlegroups.com.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.96484375,
          "content": "<img src=\"resources/7D6433A42D189E2E6FBC62BE066BCE91.png\">\r\n\r\n<p align=\"center\">\r\n   🌐 <a href=\"http://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/\" target=\"_blank\">Blog</a> • ⏬ <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSehr5Dh_i3TwACmFFi8QEgIVNYGmSPwV0GueIcsUev0NEfUug/viewform\" target=\"_blank\">Download Model</a> • 🪧 <a href=\"https://huggingface.co/spaces/THUDM/GLM-130B\" target=\"_blank\">Demo</a> • ✉️ <a href=\"mailto:glm-130b@googlegroups.com\">Email</a> • 📃 <a href=\"https://arxiv.org/abs/2210.02414\" target=\"_blank\">Paper [ICLR 2023]</a><br>\r\n</p>\r\n\r\n<p align=\"center\">\r\n   💬 <a href=\"https://groups.google.com/g/glm-130b-forum\" target=\"_blank\">Google Group</a> (Updates) or <a href=\"https://github.com/THUDM/GLM-130B/blob/main/resources/WECHAT.md\" target=\"_blank\">Wechat Group</a> or <a href=\"https://join.slack.com/t/glm-130b/shared_invite/zt-1f2ih11xy-EAuDComTAr~XVB3MywE9Cg\" target=\"_blank\">Slack channel</a> (Discussions)\r\n</p>\r\n\r\n# GLM-130B: An Open Bilingual Pre-Trained Model\r\n\r\nGLM-130B is an open bilingual (English & Chinese) bidirectional dense model with 130 billion parameters, pre-trained using the algorithm of [General Language Model (GLM)](https://aclanthology.org/2022.acl-long.26). It is designed to support inference tasks with the 130B parameters on **a single A100 (40G * 8)** or **V100 (32G * 8) server**. With INT4 quantization, the  hardware requirements can further be reduced to **a single server with 4 * RTX 3090 (24G)** with **almost no performance degradation**. As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English) and it has the following unique features:\r\n \r\n- **Bilingual:** supports both English and Chinese. \r\n- **Performance (EN):** better than GPT-3 175B (+4.0%), OPT-175B (+5.5%), and BLOOM-176B (+13.0%) on LAMBADA and slightly better than GPT-3 175B (+0.9%) on MMLU.\r\n- **Performance (CN):** significantly better than ERNIE TITAN 3.0 260B on 7 zero-shot CLUE datasets (+24.26%) and 5 zero-shot FewCLUE datasets (+12.75%). \r\n- **Fast Inference:** supports fast inference on both [SAT](https://github.com/THUDM/SwissArmyTransformer) and [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) (up to 2.5X faster) with a single A100 server.\r\n- **Reproducibility:** all results (30+ tasks) can be easily reproduced with open-sourced code and model checkpoints.\r\n- **Cross-Platform:** supports training and inference on NVIDIA, Hygon DCU, Ascend 910, and Sunway (Will be released soon).\r\n\r\nThis repository mainly focus on the evaluation of GLM-130B. If you find our work and our open-sourced efforts useful, ⭐️ to encourage our following development! :)\r\n\r\n## News\r\n- **[2023.06.25]** Release [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B), an updated version of [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) which introduces **Stronger Performance** (MMLU (+23%), CEval (+33%), GSM8K (+571%), BBH (+60%)), **Longer Context** (from 2K in ChatGLM-6B to 32K, and trained with a context length of 8K during the dialogue alignment), and **More Efficient Inference** (speeds up by 42% under the official implementation; the dialogue length supported by 6G GPU memory has increased from 1K to 8K). More details please refer to [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)。\r\n- **[2023.06.14]** We release the research [WebGLM](https://github.com/THUDM/WebGLM), which enables efficient and accurate web-enhanced question answering. All code and data are released!\r\n- **[2023.03.14]** We are happy to introduce [ChatGLM](https://chatglm.cn/blog), a bilingual dialogue language model based on GLM-130B, and its open-sourced version [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) which can be run under only **6GB** GPU memory! \r\n- **[2023.01.21]** GLM-130B has been accepted to [ICLR 2023](https://iclr.cc/Conferences/2023)!\r\n- **[2022.10.06]** Our [paper](http://arxiv.org/abs/2210.02414) for GLM-130B is out!\r\n- **[2022.08.24]** We are proud to publish the quantized version for GLM-130B.  While preserving the activation precision as FP16, the model weights can be quantized to as low as **INT4 with almost no degradation of performance**, further reducing the hardware requirements of the GLM-130B to **a single server with 4 * RTX 3090 (24G)**! See [Quantization of GLM-130B](docs/quantization.md) for details.\r\n\r\nFor smaller models, please find [monolingual GLMs](https://github.com/THUDM/GLM) (English: 10B/2B/515M/410M/335M/110M, Chinese: 10B/335M) and an [1B multilingual GLM](https://github.com/THUDM/Multilingual-GLM) (104 languages).\r\n\r\n## Getting Started\r\n\r\n### Environment Setup\r\n\r\n#### Hardware\r\n\r\n| **Hardware**    | **GPU Memory** | **Quantization** | **Weight Offload** |\r\n| --------------- | -------------- | ---------------- | ------------------ |\r\n| 8 * A100        | 40 GB          | No               | No                 |\r\n| 8 * V100        | 32 GB          | No               | Yes (BMInf)        |\r\n| 8 * V100        | 32 GB          | INT8             | No                 |\r\n| 8 * RTX 3090    | 24 GB          | INT8             | No                 |\r\n| 4 * RTX 3090    | 24 GB          | INT4             | No                 |\r\n| 8 * RTX 2080 Ti | 11 GB          | INT4             | No        |\r\n\r\nIt is recommended to use the an A100 (40G * 8) server, as all GLM-130B evaluation results (~30 tasks) reported can be easily reproduced with a single A100 server in about half a day. With INT8/INT4 quantization, efficient inference on **a single server with 4 * RTX 3090 (24G)** is possible, see [Quantization of GLM-130B](docs/quantization.md) for details. Combining quantization and weight offloading techniques, GLM-130B can also be inferenced on servers with even smaller GPU memory, see [Low-Resource Inference](docs/low-resource-inference.md) for details.\r\n\r\n#### Software\r\n\r\nThe GLM-130B code is built on the top of [SAT](https://github.com/THUDM/SwissArmyTransformer). We recommend using [Miniconda](https://docs.conda.io/en/latest/miniconda.html) to manage your environment and installing additional dependencies via `pip install -r requirements.txt`. Here are the recommended environment configurations:\r\n\r\n- Python 3.9+ / CUDA 11+ / PyTorch 1.10+ / DeepSpeed 0.6+ / Apex (**installation with CUDA and C++ extensions is required, see [here](https://github.com/NVIDIA/apex/#linux)**)\r\n- SwissArmyTransformer>=0.2.11 is required for quantization\r\n\r\n#### Model weights\r\n\r\nDownload the GLM-130B’s model checkpoint from [here](https://docs.google.com/forms/d/e/1FAIpQLSehr5Dh_i3TwACmFFi8QEgIVNYGmSPwV0GueIcsUev0NEfUug/viewform?usp=sf_link), make sure all 60 chunks are downloaded completely, then use the following command to merge them into a single archive file and extract it:\r\n\r\n```bash\r\ncat glm-130b-sat.tar.part_* > glm-130b-sat.tar\r\ntar xvf glm-130b-sat.tar\r\n```\r\n\r\nSet `CHECKPOINT_PATH` in `configs/model_glm_130b.sh` to the path of the extracted folder. Since the checkpoint file is up to 260G, it is recommended to use the SSD or RAM disk to reduce the checkpoint loading time. Since the checkpoint we distribute is in 8-way tensor parallel, a conversion scripts is also provided if you need to change the tensor parallel dimension.\r\n\r\n```bash\r\npython tools/convert_tp.py \\\r\n    --input-folder <SRC_CKPT_PATH>  \\\r\n    --output-folder <DST_CKPT_PATH> \\\r\n    --target-tp <TARGET_TP>\r\n```\r\n\r\n### Left-To-Right Generation / Blank Filling\r\n\r\n```bash\r\nbash scripts/generate.sh --input-source interactive\r\n```\r\n\r\nYou can also specify an input file by `--input-source input.txt`.\r\n\r\nGLM-130B uses two different mask tokens: `[MASK]` for short blank filling and `[gMASK]` for left-to-right long text generation. When the input does not contain any MASK token, `[gMASK]` will be automatically appended to the end of the text.\r\n\r\n<details>\r\n<summary><b>Left-To-Right Generation Examples</b></summary>\r\n    \r\n#### Example 1\r\nInput: Who is the greatest artist? The greatest artist is [gMASK]\r\n\r\nOutput: Who is the greatest artist? The greatest artist is not the one who paints the most beautiful picture, or writes the best poem, or plays the best symphony. The greatest artist is the one whose work has the greatest impact on the lives of others. The Greatest Artist is the artist who inspires us to be our best selves. The Great Artist is not the artist whose work is flawless. The great artist is not one who is perfect. The true great artist has the ability to make us feel something. The artist who makes us feel is the great artist. The art of the great artists is to make you feel something that you have never felt before. The greatness of an artist is in the ability of their work to touch the lives and hearts of others, and to make them feel something they have never experienced before. This is the true greatness of the artist. 2. The Significance of the Great Artist The significance of the greatest artist cannot be overstated. The significance and impact of the art of a great artist cannot even be measured. The impact of a single work of art can change the course of history. A single work can change a person’s life. A work of great art can make you laugh, make you cry, make your heart race, or make you think. A great work of artistic expression has the power to change the way you look at the world. The work of a truly great artist can change your life forever. 3. The Impact of the Greatest Artist The greatest artists have the ability and the power of their art to change people’s lives. They have the power and the ability, through their art, to make people feel something new. The power of the work of the truly great artists cannot be measured, but it can be felt. The greater the impact of an artists work, the greater the power it has to change lives. 4. The Influence of the Art of the Artist The art and the work created by the greatest artists has the potential to change a life. The influence of the artists work cannot be quantified, but is felt by those who experience it. The truly great work has an ability to touch people’ lives in a way that no other work can.\r\n  \r\n#### Example 2 (Chinese)\r\nInput: 问题：冬天，中国哪座城市最适合避寒？问题描述：能推荐一些国内适合冬天避寒的城市吗？回答用户：旅游爱好者 回答： [gMASK]\r\n  \r\nOutput: 问题:冬天,中国哪座城市最适合避寒?问题描述:能推荐一些国内适合冬天避寒的城市吗?回答用户:旅游爱好者 回答:避寒的话,建议去海南三亚。 三亚位于海南岛的最南端,是中国最南部的热带滨海旅游城市,中国海滨城市,是中国空气质量最好的城市、全国最长寿地区(平均寿命80岁)。三亚市别称鹿城,又被称为\"东方夏威夷\",位居中国四大一线旅游城市\"三威杭厦\"之首,拥有全岛最美丽的海滨风光。三亚拥有被无数城市嫉妒的清新空气,柔和海滩。在沙滩悠闲散步、沐浴傍晚温和阳光,在海边玩耍,在雨林里面呼吸健康,欣赏自然奇观,一切都是那么令人享受。 气候 三亚地处热带,属热带季风气候区域,终年气温高,寒暑变化不大,年平均气温25.5度。三亚最热的月份是6月,平均气温为28.5°C,极端最高气温为35.7°C ,三亚没有明显的冬季,而且冬季更是到海滨旅游的最好时候,所以三亚是全年全天候避寒、消暑、度假、旅游的好地方。不过,三亚最旺的旅游时间是从10月份到农历春节,这个时候的人特别多,房间也不好订,建议最好避开高峰。三亚的三个旅游高峰期是\"五一\"、\"十一\"、\"春节\",这三个节日期间的房价都会上浮几倍,如果你选择了这三个假期来三亚的话要提前一个星期定房和咨询。\r\n</details>\r\n\r\n<details>\r\n<summary><b>Blank Filling Examples</b></summary>\r\n  \r\n#### Example 1\r\nInput: Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.\r\n  \r\nOutput: Ng is an adjunct professor at Stanford University (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.<br/>\r\n\r\n#### Example 2 (Chinese)\r\nInput: 凯旋门位于意大利米兰市古城堡旁。1807年为纪念[MASK]而建，门高25米，顶上矗立两武士青铜古兵车铸像。\r\n  \r\nOutput: 凯旋门位于意大利米兰市古城堡旁。1807年为纪念拿破仑胜利而建,门高25米,顶上矗立两武士青铜古兵车铸像。</code></pre>\r\n</details>\r\n\r\n<details>\r\n<summary><b>Arguments Useful in Generation</b></summary>\r\n\r\n- `--input-source [path] or \"interactive\"` The input file's path. It can also be \"interactive\", which will launch a CLI.\r\n- `—-output-path [path]` The folder containing the results.\r\n- `—-out-seq-length [int]` The maximum sequence length for generation (including context).\r\n- `—-min-gen-length [int]` The minimum generation length for each MASK.\r\n- `—-sampling-strategy \"BaseStrategy\" or \"BeamSearchStrategy\"`. The sampling strategy used.\r\n  - For BeamSearchStrategy:\r\n     - `—-num-beams [int]` The number of beams.\r\n     - `—-length-penalty [float]` The maximum sequence length for generation (including context).\r\n     - `—-no-repeat-ngram-size [int]` Prohibit repeated n-gram generation.\r\n     - `—-print-all-beam` Print the generated results for all beams.\r\n  - For BaseStrategy:\r\n     - `—-top-k [int]` Top k sampling.\r\n     - `—-top-p [float]` Top p sampling.\r\n     - `—-temperature [float]` The sampling temperature.\r\n</details>\r\n\r\n### Evaluation\r\n\r\nWe use the YAML file to define tasks. Specifically, you can add multiple tasks or folders at a time for evaluation, and the evaluation script will automatically collect all YAML files under those folders recursively.\r\n\r\n```\r\nbash scripts/evaluate.sh task1.yaml task2.yaml dir1 dir2 ...\r\n```\r\n\r\nDownload our evaluation dataset [here](https://cloud.tsinghua.edu.cn/f/826f0df4356f4022a264/), and set `DATA_PATH` in `scripts/evaluate.sh` to your local dataset directory. The task folder contains the YAML files for 30+ tasks we evaluated for GLM-130B. Take the [CoLA](https://nyu-mll.github.io/CoLA/) task for example, run `bash scripts/evaluate.sh tasks/bloom/glue_cola.yaml`, which outputs an accuracy of ~65% for the best prompt and ~57% for the median.\r\n\r\n<details>\r\n<summary>Expected Output</summary>\r\n  \r\n```plain\r\nMultiChoiceTaskConfig(name='glue_cola', type=<TaskType.MULTICHOICE: 'mul'>, path='/thudm/LargeScale/data/zeroshot/bloom/glue_cola', module=None, metrics=['Accuracy'], use_task_mask=False, use_multitask_encoding=False, unidirectional=False, max_seq_length=2048, file_pattern={'validation': '**/validation.jsonl'}, micro_batch_size=8)\r\nEvaluating task glue_cola:\r\n  Evaluating group validation:\r\n      Finish Following_sentence_acceptable/mul/validation.jsonl, Accuracy = 42.665\r\n      Finish Make_sense_yes_no/mul/validation.jsonl, Accuracy = 56.951\r\n      Finish Previous_sentence_acceptable/mul/validation.jsonl, Accuracy = 65.197\r\n      Finish editing/mul/validation.jsonl, Accuracy = 57.622\r\n      Finish is_this_correct/mul/validation.jsonl, Accuracy = 65.197\r\nEvaluation results of task glue_cola:\r\n  Group validation Accuracy: max = 65.197, median = 57.622, average = 57.526\r\nFinish task glue_cola in 101.2s. \r\n```\r\n</details>\r\n\r\nMulti-node evaluation can be configured by setting `HOST_FILE_PATH`(required by the [DeepSpeed lanucher](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)) in `scripts/evaluate_multiple_node.sh`. Set `DATA_PATH` in `scripts/evaluate_multiple_node.sh` and run the following command to evaluate all the tasks in `./task` directory.\r\n\r\n```\r\nbash scripts/evaluate_multiple_node.sh ./tasks\r\n```\r\n\r\nSee [Evaluate Your Own Tasks](docs/evaluate-your-own-tasks.md) for details on how to add new tasks.\r\n\r\n### 2.5X faster Inference using FasterTransformer\r\n\r\nBy adapting the GLM-130B model to [FasterTransfomer](https://github.com/NVIDIA/FasterTransformer), a highly optimized transformer model library by NVIDIA, we can reach up to 2.5X speedup on generation, see [Inference with FasterTransformer](docs/inference-with-fastertransformer.md) for details.\r\n\r\n\r\n\r\n## License\r\n\r\nThis repository is licensed under the [Apache-2.0 license](LICENSE). The use of GLM-130B model weights is subject to the [Model License](MODEL_LICENSE).\r\n\r\n## Citation\r\n\r\nIf you find our work useful, please consider citing GLM-130B:\r\n\r\n```\r\n@article{zeng2022glm,\r\n  title={Glm-130b: An open bilingual pre-trained model},\r\n  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},\r\n  journal={arXiv preprint arXiv:2210.02414},\r\n  year={2022}\r\n}\r\n```\r\n\r\nYou may also consider GLM's original work in your reference:\r\n\r\n```\r\n@inproceedings{du2022glm,\r\n  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\r\n  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\r\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\r\n  pages={320--335},\r\n  year={2022}\r\n}\r\n```\r\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 32.2880859375,
          "content": "<img src=\"resources/7D6433A42D189E2E6FBC62BE066BCE91.png\">\r\n\r\n<p align=\"center\">\r\n   🌐 <a href=\"https://models.aminer.cn/glm-130b/\" target=\"_blank\">博客</a> • ⏬ <a href=\"https://models.aminer.cn/glm/zh-CN/download/GLM-130B\" target=\"_blank\">下载模型</a> • 🪧 <a href=\"https://huggingface.co/spaces/hanyullai/GLM-130B\" target=\"_blank\">样例演示</a> • 💬 <a href=\"https://github.com/THUDM/GLM-130B/discussions\">讨论</a> • ✉️ <a href=\"mailto:glm-130b@googlegroups.com\">邮箱</a> • 💬 <a href=\"https://groups.google.com/g/glm-130b-forum\" target=\"_blank\">谷歌群组</a> or <a href=\"https://github.com/Xiao9905\" target=\"_blank\">微信群</a>\r\n • 📃 论文（敬请期待） <br>\r\n</p>\r\n\r\n# GLM-130B：开放的中英双语预训练模型\r\n\r\n## 摘要：何为 GLM-130B？\r\n\r\nGLM-130B 是一个开源开放的双语（中文和英文）双向稠密模型，拥有 1300 亿个参数，模型架构采用通用语言模型（GLM）。它旨在支持在**一台 A100（40G * 8）** 或 **V100（32G * 8）服务器**上对千亿规模的参数进行推理。截至 2022 年 7 月 3 日，GLM-130B 已经对超过 4000 亿个文本标识符（中文和英文各 2000 亿）进行了训练，它有以下独特优势：\r\n\r\n* **双语**：同时支持中文和英文。 \r\n* **任务表现（英文）**： 在 LAMBADA 上优于 GPT-3 175B（+4.0%）、OPT-175B（+5.5%）和 BLOOM-176B（+13.0%），在 MMLU 上略优于GPT-3 175B（+0.9%）。\r\n* **任务表现（中文）**：在 7 个零样本 CLUE 数据集（+24.26%）和 5 个零样本 FewCLUE 数据集（+12.75%）上明显优于 ERNIE TITAN 3.0 260B。\r\n* **快速推理**：支持用一台 A100 服务器使用 [SAT](https://github.com/THUDM/SwissArmyTransformer) 和 [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) 进行快速推理（速度最高可达2.5倍）。\r\n* **可复现性**：所有的结果（超过30个任务）都可以用我们开源的代码和模型参数轻松复现。\r\n* **多平台**：支持在 NVIDIA、Hygon DCU、Ascend 910 和 Sunway 处理器上进行训练与推理（代码即将开源）。\r\n\r\n## 快速上手\r\n\r\n### 环境配置\r\n\r\n我们的代码是建立在 [SAT](https://github.com/THUDM/SwissArmyTransformer) 之上的。我们推荐使用 Miniconda 来管理环境并通过 `pip install -r requirements.txt` 来安装额外的依赖包。以下是我们推荐的环境配置：\r\n\r\n- Python 3.9+ / PyTorch 1.10+ / DeepSpeed 0.6+ / Apex（**需要安装包含 CUDA 和 C++ 扩展的版本，[参考资料](https://github.com/NVIDIA/apex/#linux)**）\r\n\r\n建议使用 A100（40G * 8）服务器，因为所有报告的评估结果（约30个任务）都可以用一台 A100 服务器在大约半天内轻松再现。GLM-130B 也可以在具有较小 GPU 内存的服务器上进行推断，例如具有 V100（32G * 8）的服务器。详见 [Low-resource Inference](docs/low-resource-inference.md)。\r\n\r\n从 [这里](https://models.aminer.cn/glm/zh-CN/download/GLM-130B) 申请下载 GLM-130B 的模型检查点，确保所有 60 个块都已完全下载，然后使用以下命令将它们合并为一个存档文件并解压缩：\r\n\r\n```bash\r\ncat glm-130b-sat.tar.part_* > glm-130b-sat.tar\r\ntar xvf glm-130b-sat.tar\r\n```\r\n\r\n将 `configs/model_glm_130b.sh` 中的 `CHECKPOINT_PATH` 设置为解压文件夹的路径。 由于 checkpoint 文件最大 260G，建议使用 SSD 或 RAM 盘来减少 checkpoint 加载时间。\r\n\r\n### 自回归文本生成 / 中间文本填空\r\n\r\n```bash\r\nbash scripts/generate.sh --input-source interactive\r\n```\r\n\r\n你也可以通过 `--input-source input.txt` 指定一个输入文件。\r\n\r\nGLM-130B 使用两种不同的掩码标记：`[MASK]` 用于简短的填空任务，`[gMASK]` 用于从左到右的长文本生成。当输入不包含任何 MASK 标记时，`[gMASK]` 将被自动附加到文本的末尾。\r\n\r\n<details>\r\n<summary><b>长文本生成样例</b></summary>\r\n    \r\n#### Example 1\r\nInput: Who is the greatest artist? The greatest artist is [gMASK]\r\n\r\nOutput: Who is the greatest artist? The greatest artist is not the one who paints the most beautiful picture, or writes the best poem, or plays the best symphony. The greatest artist is the one whose work has the greatest impact on the lives of others. The Greatest Artist is the artist who inspires us to be our best selves. The Great Artist is not the artist whose work is flawless. The great artist is not one who is perfect. The true great artist has the ability to make us feel something. The artist who makes us feel is the great artist. The art of the great artists is to make you feel something that you have never felt before. The greatness of an artist is in the ability of their work to touch the lives and hearts of others, and to make them feel something they have never experienced before. This is the true greatness of the artist. 2. The Significance of the Great Artist The significance of the greatest artist cannot be overstated. The significance and impact of the art of a great artist cannot even be measured. The impact of a single work of art can change the course of history. A single work can change a person’s life. A work of great art can make you laugh, make you cry, make your heart race, or make you think. A great work of artistic expression has the power to change the way you look at the world. The work of a truly great artist can change your life forever. 3. The Impact of the Greatest Artist The greatest artists have the ability and the power of their art to change people’s lives. They have the power and the ability, through their art, to make people feel something new. The power of the work of the truly great artists cannot be measured, but it can be felt. The greater the impact of an artists work, the greater the power it has to change lives. 4. The Influence of the Art of the Artist The art and the work created by the greatest artists has the potential to change a life. The influence of the artists work cannot be quantified, but is felt by those who experience it. The truly great work has an ability to touch people’ lives in a way that no other work can.\r\n  \r\n#### Example 2 (Chinese)\r\nInput: 问题：冬天，中国哪座城市最适合避寒？问题描述：能推荐一些国内适合冬天避寒的城市吗？回答用户：旅游爱好者 回答： [gMASK]\r\n  \r\nOutput: 问题:冬天,中国哪座城市最适合避寒?问题描述:能推荐一些国内适合冬天避寒的城市吗?回答用户:旅游爱好者 回答:避寒的话,建议去海南三亚。 三亚位于海南岛的最南端,是中国最南部的热带滨海旅游城市,中国海滨城市,是中国空气质量最好的城市、全国最长寿地区(平均寿命80岁)。三亚市别称鹿城,又被称为\"东方夏威夷\",位居中国四大一线旅游城市\"三威杭厦\"之首,拥有全岛最美丽的海滨风光。三亚拥有被无数城市嫉妒的清新空气,柔和海滩。在沙滩悠闲散步、沐浴傍晚温和阳光,在海边玩耍,在雨林里面呼吸健康,欣赏自然奇观,一切都是那么令人享受。 气候 三亚地处热带,属热带季风气候区域,终年气温高,寒暑变化不大,年平均气温25.5度。三亚最热的月份是6月,平均气温为28.5°C,极端最高气温为35.7°C ,三亚没有明显的冬季,而且冬季更是到海滨旅游的最好时候,所以三亚是全年全天候避寒、消暑、度假、旅游的好地方。不过,三亚最旺的旅游时间是从10月份到农历春节,这个时候的人特别多,房间也不好订,建议最好避开高峰。三亚的三个旅游高峰期是\"五一\"、\"十一\"、\"春节\",这三个节日期间的房价都会上浮几倍,如果你选择了这三个假期来三亚的话要提前一个星期定房和咨询。\r\n</details>\r\n\r\n<details>\r\n<summary><b>文本填空样例</b></summary>\r\n  \r\n#### Example 1\r\nInput: Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.\r\n  \r\nOutput: Ng is an adjunct professor at Stanford University (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.<br/>\r\n\r\n#### Example 2 (Chinese)\r\nInput: 凯旋门位于意大利米兰市古城堡旁。1807年为纪念[MASK]而建，门高25米，顶上矗立两武士青铜古兵车铸像。\r\n  \r\nOutput: 凯旋门位于意大利米兰市古城堡旁。1807年为纪念拿破仑胜利而建,门高25米,顶上矗立两武士青铜古兵车铸像。</code></pre>\r\n</details>\r\n\r\n\r\n<details>\r\n<summary><b>控制生成的主要超参数</b></summary>\r\n\r\n- `--input-source [path] or \"interactive\"`. 输入文件的路径。当设为\"interactive\"时，将会启动交互式CLI。\r\n- `—-output-path [path]`. 结果输出路径。\r\n- `—-out-seq-length [int]`. （包括输入内容在内的）最大输出序列长度。\r\n- `—-min-gen-length [int]` 每个MASK标识符位置的最小生成长度。\r\n- `—-sampling-strategy \"BaseStrategy\" or \"BeamSearchStrategy\"`. 生成的采样策略。\r\n  - 对于 BeamSearchStrategy（集束搜索）：\r\n     - `—-num-beams [int]`. 集束数目。\r\n     - `—-length-penalty [float]`. （包括输入内容在内的）生成长度惩罚项；数值范围[0, 1]，数值越大生成长度越长。\r\n     - `—-no-repeat-ngram-size [int]`. 禁止重复生成的n-gram长度。\r\n     - `—-print-all-beam`. 是否打印每一束搜索结果。\r\n  - For BaseStrategy:\r\n     - `—-top-k [int]`. Top k 采样。\r\n     - `—-top-p [float]`. Top p 采样。\r\n     - `—-temperature [float]` . 采样时设置的温度项。\r\n </details>\r\n\r\n### 评估\r\n\r\n我们使用YAML文件来定义任务。具体来说，你可以一次添加多个任务或文件夹进行评估，评估脚本会自动递归地收集这些文件夹下的所有YAML文件。\r\n\r\n```\r\nbash scripts/evaluate.sh task1.yaml task2.yaml dir1 dir2 ...\r\n```\r\n\r\n[从这里](https://cloud.tsinghua.edu.cn/f/9257ee84045644b8ac06/)下载我们的评估数据集，并在 `scripts/evaluate.sh` 中设置 `DATA_PATH` 为你的本地数据集目录。任务文件夹包含我们为 GLM-130B 评估的 30 多个任务的 YAML 文件。以 [CoLA](https://nyu-mll.github.io/CoLA/) 任务为例，运行 `bash scripts/evaluate.sh tasks/bloom/glue_cola.yaml`，其输出的最佳提示准确率约为 65%，中值约为 57%。\r\n\r\n<details>\r\n<summary>预期输出</summary>\r\n  \r\n```plain\r\nMultiChoiceTaskConfig(name='glue_cola', type=<TaskType.MULTICHOICE: 'mul'>, path='/thudm/LargeScale/data/zeroshot/bloom/glue_cola', module=None, metrics=['Accuracy'], use_task_mask=False, use_multitask_encoding=False, unidirectional=False, max_seq_length=2048, file_pattern={'validation': '**/validation.jsonl'}, micro_batch_size=8)\r\nEvaluating task glue_cola:\r\n  Evaluating group validation:\r\n      Finish Following_sentence_acceptable/mul/validation.jsonl, Accuracy = 42.665\r\n      Finish Make_sense_yes_no/mul/validation.jsonl, Accuracy = 56.951\r\n      Finish Previous_sentence_acceptable/mul/validation.jsonl, Accuracy = 65.197\r\n      Finish editing/mul/validation.jsonl, Accuracy = 57.622\r\n      Finish is_this_correct/mul/validation.jsonl, Accuracy = 65.197\r\nEvaluation results of task glue_cola:\r\n  Group validation Accuracy: max = 65.197, median = 57.622, average = 57.526\r\nFinish task glue_cola in 101.2s. \r\n```\r\n</details>\r\n\r\n可以通过在 `scripts/evaluate_multiple_node.sh` 中设置 `HOST_FILE_PATH`（[DeepSpeed lanucher](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) 要求）来配置多节点评估。在 `scripts/evaluate_multiple_node.sh` 中设置 `DATA_PATH` 并运行以下命令来评估`./task`目录中的所有任务。\r\n\r\n```\r\nbash scripts/evaluate_multiple_node.sh ./tasks\r\n```\r\n\r\n关于如何添加新任务的细节，请参见 [评估你自己的任务](docs/evaluate-your-own-tasks.md)。\r\n\r\n### 使用 FasterTransformer 加速推理速度（高达 2.5 倍）\r\n\r\n- 通过将 GLM-130B 模型与 [FasterTransfomer](https://github.com/NVIDIA/FasterTransformer)（NVIDIA 高度优化的 Transformer 模型库）相适应，我们可以在生成时达到 2.5 倍的速度，详见 [Inference with FasterTransformer](docs/inference-with-fastertransformer.md) 。\r\n\r\n\r\n## 何为GLM-130B？\r\n\r\nGLM-130B是一个开放的双语（中文与英文）双向语言模型，含1300亿个参数。截至2022年7月，它已经训练了超过4000亿个文本标记。它的底层架构基于[通用语言模型(GLM)](https://aclanthology.org/2022.acl-long.26/)，在语言理解和语言生成任务上均展示出强大的性能。\r\n\r\n### 架构\r\n\r\nGLM-130B将BERT和GPT的目标进行了统一，并与最近提出的一些技术进行结合以提升语言模型的性能表现。\r\n\r\n#### 1\\. 训练目标：自回归文本填空\r\n\r\nGLM利用自回归文本填空作为其主要的预训练目标。它掩盖了随机的连续跨度（例如，下面的例子中的 \"complete unknown\"），并对其进行自回归预测。上下文之间的注意力（例如，\"like a [MASK], like a rolling stone\"）是双向的。相反，被掩盖的标记之间的注意力，和从上下文到被掩盖的标识符的注意力是自回归掩码的。\r\n\r\n\r\n\r\n在GLM-130B的实现中，有两种不同的MASK标识符，表示两个不同的目的：\r\n\r\n* `[MASK]`根据[泊松分布](https://en.wikipedia.org/wiki/Poisson_distribution) (λ=3)对输入中标识符进行短跨度的采样；\r\n* `[gMASK]`掩盖一个长的跨度，从其位置到整个文本的结束。\r\n\r\n`[sop]`标识符表示一个片断的开始，`[eop]`表示一个片断的结束。这两个目标在GLM-130B的预训练中是混合的，分别占预训练标记的30%和70%。\r\n\r\n| <img src=\"resources/49BF334CB352BAA19F7D55460B1DBCA9.gif\" width=\"750px\"> | \r\n|:--:| \r\n| *例如：GLM-130B是如何对 `\"like a complete unknown, like a rolling stone\"`进行预训练的* |\r\n\r\n#### 2\\. 位置编码：旋转位置编码\r\n\r\nGLM-130B使用[旋转位置编码（RoPE）](https://arxiv.org/abs/2104.09864)，谷歌的[PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)和[ElutherAI](https://www.eleuther.ai/)的GPT-*系列也采用这种编码。RoPE是一种相对位置编码，它利用复数空间的正交投影矩阵来表示标识符的相对距离。还有其他的相对位置编码选项，如Bigscience的[BLOOM](https://huggingface.co/bigscience/bloom)所使用的[AliBi](https://arxiv.org/abs/2108.12409)。但在我们的初步实验中，我们发现。\r\n\r\n* 当序列长度增长时，RoPE的实现速度更快。\r\n* RoPE对双向注意力更友好，在下游微调实验中效果更好\r\n\r\n因此，对于GLM-130B，RoPE是一种有效的、高效的位置编码。\r\n\r\n#### 3\\. 归一化：使用DeepNet的Post-LN\r\n\r\n层归一化（LayerNorm，或LN）是transformer中的一个重要组成部分，其应用可以大大影响训练的稳定性和性能。BERT应用了Post-LN，这意味着LayerNorm是在添加残余分支后应用的。然而，[后续工作](https://arxiv.org/abs/2002.04745)表明，单纯的Post-LN会导致预训练的不稳定，因此现有的大规模模型都选择Pre-LN架构，即在添加残差分支之前应用LayerNorm。\r\n\r\n| <img src=\"resources/849024E93FA85347F7F6443932911922.png\" width=\"600px\"> | \r\n|:--:| \r\n| *(a) Post-LN在下游任务中表现更佳；(b) Post-LN + DeepNorm 比 Sandwich-LN 要更加稳定* |\r\n\r\n尽管如此，在现有的实践中，Pre-LN在用FP16训练大规模模型时仍然可能不稳定。[OPT-175B](https://arxiv.org/abs/2205.01068)在训练崩溃时手动调整学习率；[BLOOM](https://huggingface.co/bigscience/bloom)使用BF16（仅适用于NVIDIA Ampere GPU：A100s和3090s）以获得更好的浮点精度来避免崩溃。[CogView](https://proceedings.neurips.cc/paper/2021/file/a4d92e2cd541fca87e4620aba658316d-Paper.pdf)提出了Sandwich-LN作为一种补救措施。更重要的是，[近期工作](https://aclanthology.org/2021.findings-acl.81.pdf)表明，与Post-LN相比，Pre-LN的下游微调性能更差。\r\n\r\n考虑到所有这些因素，在GLM-130B中，我们决定使用Post-LN，并使用新提出的[DeepNorm](https://arxiv.org/abs/2203.00555)来克服不稳定性。DeepNorm的重点是改进初始化，可以帮助Post-LN变换器扩展到1000层以上。在我们的初步实验中，模型扩展到130B，Sandwich-LN的梯度在大约2.5k步时就会出现损失突变（导致损失发散），而带有DeepNorm的Post-Ln则保持健康并呈现出较小的梯度大小（即更稳定）。\r\n\r\n#### 4\\. 前馈网络：Gated Linear Unit (GLU) + GeLU 激活\r\n\r\n最近一些改进transformer结构的努力集中在前馈网络（FFN）上，包括用[GLU](https://arxiv.org/abs/1612.08083)（在PaLM中采用）和新提出的[门控注意单元（GAU）](https://arxiv.org/abs/2202.10447)取代它。\r\n\r\n|                              | RTE        | COPA       | BoolQ      | WSC        | Average |\r\n|------------------------------|------------|------------|------------|------------|---------|\r\n| GLM-base (GeGLU-Sandwich_LN) | 71.00±0.61 | 77.00±1.63 | 77.24±0.43 | 78.21±1.81 | 75.08   |\r\n| GLM-base (GAU-Pre_LN)        |            |            | _diverged_ |            |         |\r\n| GLM-base (GAU-Sandwich_LN)   | 69.92±0.61 | 75.67±0.94 | 77.00±0.15 | 72.44±1.81 | 74.20   |\r\n| GLN-base (FFN-Sandwich_LN)   | 71.00±0.74 | 72.33±1.70 | 76.75±0.05 | 73.72±2.40 | 73.36   |\r\n\r\n我们在初步实验中通过对随机的50G中英文混合语料库进行GLM-base（110M）的预训练来测试它们。我们发现，虽然GLU和GAU可以比原始FFN实现更好，但GLU在训练中可以更好、更稳定。\r\n\r\n因此，在GLM-130B的实现中，我们选择带有GeLU激活的GLU，即GeGLU。GeGLU需要三个投影矩阵；为了保持相同数量的参数，与只利用两个矩阵的FFN相比，我们将其隐藏状态减少到2/3。\r\n\r\n#### 总结\r\n\r\n基于以上所有设计，GLM-130B的参数配置为：\r\n\r\n|  层数  |   隐层维度   |   GeGLU 隐层维度   |   注意力头数量  |     最大序列长度    |  词表大小   |\r\n|--------|--------------|--------------------|-----------------|---------------------|-------------|\r\n| 70     | 12,288       | 32,768             | 96              | 2,048               | 150,000     |\r\n\r\n该词表和分词器是基于[icetk](https://github.com/THUDM/icetk)实现的。icetk是一个统一的图像、中文和英文的多模态标记器。\r\n\r\n### 训练\r\n训练大规模语言模型的最关键挑战是**训练的稳定性**，无一例外。GLM-130B的预训练持续了60天，使用96个DGX-A100（40G）节点，等价花费490万美元的云服务费用；如果训练在半路上失败，并无法恢复训练，那将是一个巨大的损失。\r\n\r\n| <img src=\"resources/E42321373D22DE198231279B5856BB42.png\" width=500px> | \r\n|:--:| \r\n| *所有模型都面临训练不稳定，它可能发生在预训练的开始、中间或结束阶段（图（a）和（b）分别取自OPT和BLOOM）* | \r\n\r\n不幸的是，据我们观察，大模型比我们认为的那些小模型更容易受到不可避免的噪音数据和意外涌现的梯度影响。原因是，在训练效率和稳定性之间存在着权衡：\r\n\r\n* **效率**：我们需要一个低精度的浮点格式（如FP16），以减少内存和计算成本；\r\n* **稳定性**：低精度浮点格式容易出现溢出和下溢。\r\n\r\n而为了平衡这两个要素，我们以及最近的开放性大型模型（如[OPT-175B](https://arxiv.org/abs/2205.01068)、[BLOOM](https://huggingface.co/bigscience/bloom)）都付出了巨大的努力来寻找解决方案。在此，我们提出我们的答案。\r\n\r\n#### 1\\. 浮点数格式：FP16 混合精度\r\n\r\nFP16混合精度已经成为主流大规模模型训练框架的默认选项，用于训练十亿到百亿规模的模型。但其仍太容易遇到精度问题。作为补救措施，NVIDIA Ampere GPU提供了BF16浮点格式（被[BLOOM](https://huggingface.co/bigscience/bloom)采用）来缓解这个问题。然而，BF16在其他平台上不被支持，这大大缩小了它在更广泛的应用中的潜力。\r\n\r\n为了让更多开发者使用，GLM-130B仍然选择FP16作为其训练浮点格式。同时，这意味着GLM-130B将面临着更多的稳定性挑战。幸运的是，经过多次尝试，我们发现以下的训练策略最终有助于稳定GLM-130B的训练。\r\n\r\n#### 2\\. 嵌入层：梯度缩减\r\n\r\n我们观察到，在训练的早期阶段，嵌入层的梯度范数明显比其他层大。根据经验，我们发现大多数训练崩溃都发生在其梯度范数激增之后。为了解决这个问题，[BLOOM](https://huggingface.co/bigscience/bloom)汇报了使用[嵌入归一化](https://openreview.net/pdf?id=rI7BL3fHIZq)（我们也发现它能稳定训练），但同时，其牺牲了相对较大的下游性能。\r\n\r\n由于根本问题是输入嵌入层的急剧梯度，我们建议缩小输入嵌入层的梯度。实现起来相当简单。\r\n\r\n```python\r\nword_embedding = word_embedding * α + word_embedding.detach() * (1 - α)\r\n```\r\n\r\n这就把梯度缩小到`α`。在我们的实践中，我们发现`α=0.1`对GLM-130B是最好的。\r\n\r\n| ![EmbeddingShrink.png](resources/03DF31017FE184DB45D41DFFC6F80EF0.png) | \r\n|:--:| \r\n| *(a) 嵌入层的梯度范数在早期阶段比其他部分大得多 <br> (b) 嵌入梯度缩减的初步实验 (alpha=0.1)* | \r\n\r\n在我们的初步实验中，我们观察到，对于早期阶段的训练来说，缩小嵌入梯度并没有减缓收敛速度；相反，没有缩小梯度的模型会出现意外的尖峰，并在5k步左右出现训练崩溃的情况。\r\n\r\n#### 3\\. 注意力计算：FP32 Softmax\r\n\r\n梯度收缩是一种避免训练崩溃的事后技术。从本质上讲，崩溃是由异常的损失 \"梯度\"形成的，要么是由于噪声数据，要么是正向计算中的精度上溢或者下溢。 \r\n\r\n| ![scale.png](resources/7CB441707D1035B2890AA2164C5B6EAC.png) | \r\n|:--:| \r\n| *每个注意力头计算出的注意力得分有非常不同的数值范围（摘自[CogView](https://proceedings.neurips.cc/paper/2021/file/a4d92e2cd541fca87e4620aba658316d-Paper.pdf)）* | \r\n\r\n我们观察到，在大型语言模型中，注意力的计算操作是最容易上溢或下溢的。[CogView](https://proceedings.neurips.cc/paper/2021/file/a4d92e2cd541fca87e4620aba658316d-Paper.pdf)显示，不同的注意力头对其注意力分数有非常不同的数值范围，有些注意力头计算出的平均分数可以达到+1e4或-1e-3。这种不同的数值范围会导致在softmax计算中FP16下的频繁上溢或下溢。CogView提出了精度瓶颈放松（PB-Relax）来缓解这个问题，它在做softmax之前扣除了每个头的注意力得分矩阵中的最大绝对值。\r\n\r\n然而，事实证明，PB-Relax在GLM-130B的训练中很慢，可能是因为在96个大小为2048*2048的注意分数矩阵中寻找最大值和操作标量对CUDA内核不友好。最后，经过几周的艰苦探索，我们发现避免这一问题的最快和最简单的方法是在softmax计算中使用FP32。与完全的FP16计算相比，它几乎没有任何速度上的损失，但明显提高了训练的稳定性。\r\n\r\n<!--#### 4\\. 3D Parallel Training with Pipeline-->\r\n\r\n### 预训练数据\r\n\r\n#### 自监督预训练\r\n\r\n我们在2.5T网络爬取的语料上，对GLM-130B进行了预训练，包括英文1.2T来自Pile的语料和1.3T中文语料.\r\n\r\n#### 多任务指令预训练（Multi-Task Instruction Pre-Training，MIP）\r\n\r\n同时，[FLAN](https://arxiv.org/pdf/2109.01652.pdf)和[T0](https://arxiv.org/pdf/2110.08207.pdf)的最新进展表明，大规模语言模型的多提示多任务指令微调可以促进更好的零样本学习能力。此外，正如[T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf?ref=https://githubhelp.com)和[ExT5](https://arxiv.org/pdf/2111.10952.pdf)所指出的，将多任务的下游数据合并到预训练中，甚至比多任务微调更有帮助。\r\n\r\n因此，在GLM-130B的预训练中，我们包括了许多从自然语言理解到生成的提示数据集，作为自监督预训练的补充。我们设定95%的标记来自自监督的预训练语料，5%的训练标记来自MIP数据集。这些数据集是从[T0](https://arxiv.org/pdf/2110.08207.pdf)和[DeepStruct](https://arxiv.org/pdf/2205.10475.pdf)中收集和转换的。按照T0的做法，每个多提示数据集中的样本都应被截断到最大数量（一般来说，T0数据集为100k，DeepStruct数据集为200k）。\r\n\r\n不幸的是，由于数据准备中的一个错误，在前20k个预训练步骤中，我们意外地包括了T0++的所有数据集（其中包括最初用于评估T0中零样本任务泛化的任务）、没有调成权重进行截断、并排除了所有DeepStruct数据集。虽然我们把这个问题在20000步时进行了修正，但GLM-130B似乎对训练样本的记忆非常好，直到50000步也没有出现大量遗忘的现象，因此我们在此提醒所有用户***切勿在这个[列表](resources/multitask_list.txt)的数据集上评估GLM-130B在零样本或少样本学习的性能。\r\n\r\n## GLM-130B表现如何？\r\n\r\n众所周知，像[GPT-3](https://arxiv.org/pdf/2005.14165.pdf)这样的大规模语言模型是优秀的少样本和零样本学习器。与GPT-3和OPT-175B的零样本学习相比，GLM-130B有一些架构上的劣势。首先，它是一个双语语言模型，不能像GPT-3（350B tokens）和OPT-175B（350B tokens）那样看到很多英语标记（GLM-130B大概见到了200B 英文tokens）。第二，GLM-130B的参数比GPT-3（175B）和OPT-175B少。\r\n\r\n尽管有这些缺点，GLM-130B仍有上述的许多技术改进，这可能会弥补其在零点学习性能方面的差距。\r\n\r\n* **双向注意力**。GLM-130B是一个类似于BERT的双向模型，而现有的大型语言模型主要是GPT（单向的）。双向模型在语言理解和条件生成方面远远优于GPT。\r\n* **改进的架构设计**。GLM-130B采用了新的架构设计，包括GeGLU、RoPE和DeepNorm。这些技术已被证明可以提高语言模型的性能。\r\n* **多任务指令预训练**。正如[FLAN](https://arxiv.org/pdf/2109.01652.pdf)和[T0](https://arxiv.org/pdf/2110.08207.pdf)所指出的，多任务指令预训练有助于提高零样本学习性能。\r\n\r\n从目前的中间结果来看，GLM-130B在中文与英文中都是一个强大的零样本学习器。具体来说，它的表现是\r\n\r\n* 在英语中与GPT-3 175B相当。\r\n* 在英语中优于BLOOM-176B和OPT-175B。\r\n* 在中文方面比ERNIE 3.0 Titan（260B）更好。\r\n\r\n```diff\r\n- 请注意，本节中的所有结果目前都是中间结果，不代表最终性能。\r\n```\r\n\r\n### 讨论：GLM-130B的零样本学习设置\r\n\r\n由于GLM-130B利用了多任务指令预训练（MIP），我们认为有必要澄清我们对零样本学习的设定。该问题似乎没有官方认可的定义，而社区中也存在许多不同的解释。我们参考了影响力较大的零样本学习[综述](https://ieeexplore.ieee.org/abstract/document/8413121)中的定义，其指出。\r\n\r\n```\r\nAt test time, in zero-shot learning setting, the aim is to assign a test image to an unseen class label, and in generalized zero-shot learning setting, the test image can be assigned either to seen or unseen classes.\r\n```\r\n\r\n其中，被评估的任务是否涉及未见过的类标签是一个关键。考虑到NLP的实际情况，我们为GLM-130B零样本学习评估挑选数据集的原则如下。\r\n\r\n* 英文\r\n  + 对于有固定标签的任务（如自然语言推理）：同一任务中的任何数据集都不应该被评估。\r\n  + 对于没有固定标签的任务（例如，问题回答，主题分类）：只应考虑：1)相比MIP中数据集具有明显的领域转移，且 2)与MIP中的标签不同的数据集\r\n* 中文：所有的数据集都可以被评估\r\n\r\n我们欢迎更多关于这个话题的讨论，以促进整个社区对零样本学习的研究。\r\n\r\n### 零样本学习：英文\r\n\r\n我们在各种不同的下游任务中测试GLM-130B。请注意，我们仍在经历评估阶段；这些结果不是最终结果，而是**中间结果**。\r\n\r\n#### 语言建模（LAMBADA）\r\n语言建模测试的是语言模型在给定其前缀语境下预测下一个单词的内在能力。我们以[LAMBADA](https://aclanthology.org/P16-1144/)为例，它是一项具有挑战性的零样本末位单词预测任务，在评估现有大规模语言模型时被广泛采用。\r\n\r\n我们绘制了GLM-130B的零样本LAMBADA（En）性能，以及GPT-3 175B、OPT 175B和BLOOM 176B（OPT和BLOOM的中间结果取自[BLOOM的评估库](https://github.com/bigscience-workshop/evaluation-results/tree/676f6a8cf27d4df30b073fb490deb9e359da64aa)）。与其他三个使用上下文自回归的GPT式模型相比，我们提出了GLM-130B的两个版本。\r\n\r\n* **GLM-130B (bi)**对前缀上下文有双向的关注。\r\n* **GLM-130B (uni)**遵循传统的GPT风格，对前缀语境进行自回归注意力。\r\n\r\n如图所示，双向注意力可以用较少的模型参数达到更好的性能。\r\n\r\n| <p style=\"text-align:center;\"><img src=\"resources/F48B69263360688CCA21E915F4B1A98B.png\" width=\"500px\"></p> | \r\n|:--:| \r\n| *与其他大规模语言模型相比，GLM-130B的零样本 LAMBADA（En）性能* | \r\n\r\n#### MMLU（大规模多任务语言理解)\r\n\r\n[MMLU](https://arxiv.org/pdf/2009.03300.pdf) 是一个多样化的基准数据集，包括57个关于人类知识的多选题回答任务，范围从高中水平到专家水平。它可以作为大规模语言模型少样本学习性能的理想测试平台。\r\n\r\n我们绘制了GLM-130B在其训练过程上的少样本学习（5-shot）性能。GLM-130B在学习了大约3000亿个tokens后，接近GPT-3的可比性能43.9。随着训练的进行，它的能力继续增长，在学习了4000亿个tokens后达到了44.8。当我们的训练终止时，它似乎并没有饱和，这与[Chinchilla](https://arxiv.org/pdf/2203.15556.pdf)中的观察相一致，即现有的大规模语言模型仍然远远没有得到充分的训练。\r\n\r\n| <p style=\"text-align:center;\"><img src=\"resources/33872E48D3539EA132B74BCF5EFF458F.png\" width=\"500px\"></p> | \r\n|:--:| \r\n| *与其他大规模语言模型相比，GLM-130B的少样本学习（5-shot）MMLU性能* |\r\n\r\n### 零样本学习：中文\r\n\r\n由于GLM-130B是一个双语语言模型，我们也评估了它在既有的中文NLP基准上的零样本性能：[CLUE](https://arxiv.org/pdf/2004.05986.pdf) 和[FewCLUE](https://arxiv.org/pdf/2107.07498.pdf)。请注意，我们在多任务指令预训练(MIP)中不包括任何中文下游数据集。由于仍在评估阶段，我们目前仅评估了7个CLUE数据集和5个FewCLUE数据集。更多数据集上的结果会在之后公布。\r\n\r\n我们将GLM-130B与现有最大的中文单语语言模型ERNIE Titan 3.0进行比较，后者有260B的参数。如图所示，GLM-130B的表现优于ERNIE Titan 3.0，尤其是在生成式阅读理解数据集DRCD和CMRC2018上。\r\n\r\n| <img src=\"resources/AE18F14396E2D22BC0BC8DD77EFD3414.png\" width=\"500px\"> | \r\n|:--:| \r\n*部分CLUE和FewCLUE基准数据集的零点性能。跟随ERNIE Titan 3.0的做法，我们报告了开发数据集的结果。除了DRCD和CMRC2018的报告EM外，其他数据集报告Acc.* |\r\n\r\n<details>\r\n<summary><b>致谢</b></summary>\r\n   \r\n这一项目由国家自然科学基金国家杰出青年科学基金项目（No. 61825602）支持。 \r\n\r\n### 学生负责人\r\n[曾奥涵（清华大学计算机系知识工程实验室）](https://github.com/Sengxian)，[刘潇（清华大学计算机系知识工程实验室）](https://github.com/xiao9905)\r\n\r\n### 技术贡献\r\n#### 清华大学计算机系知识工程实验室——the Knowledge Engineering Group at Tsinghua\r\n杜政晓，丁铭，郑勤锴，赖瀚宇，汪子涵，杨卓毅，于济凡，张笑涵，郑问迪，夏箫，徐逸凡，谭咏霖，东昱晓，唐杰\r\n\r\n#### 清华大学计算机系PACMAN实验室——the Parallel Architecture & Compiler technology of Mobile, Accelerated, and Networked systems Group at Tsinghua\r\n马子轩，何家傲，孙桢波，翟季冬，陈文光\r\n\r\n#### 清华大学计算机系自然语言处理实验室（BMInf）——the Natural Language Processing Group at Tsinghua\r\n曾国洋，韩旭，赵威霖，刘知远\r\n\r\n#### 智谱AI——an AI startup that aims to teach machines to think like humans\r\n薛宇飞，王山，陕杰才，姜皓瀚，郭振钢，张鹏\r\n\r\n### 计算资源赞助\r\n智谱AI\r\n\r\n### 项目总负责\r\n[唐杰（清华大学计算机系知识工程实验室 & 北京智源人工智能研究院）](http://keg.cs.tsinghua.edu.cn/jietang/)\r\n   \r\n</details>\r\n\r\n"
        },
        {
          "name": "benchmark.py",
          "type": "blob",
          "size": 0.8515625,
          "content": "import torch\nimport time\nfrom initialize import initialize, initialize_model_and_tokenizer\n\nif __name__ == \"__main__\":\n    args = initialize(extra_args_provider=lambda parser: None)\n    model, tokenizer = initialize_model_and_tokenizer(args)\n\n    for seq_len in [512, 1024, 2048]:\n        torch.distributed.barrier()\n        start = time.time()\n        with torch.no_grad():\n            _, *_ = model(\n                torch.ones(1, seq_len, device=torch.cuda.current_device(), dtype=torch.int64),\n                torch.arange(seq_len, device=torch.cuda.current_device(), dtype=torch.int64).view(1, -1),\n                torch.randn(1, 1, seq_len, seq_len, device=torch.cuda.current_device()) < 0.5,\n            )\n        torch.distributed.barrier()\n        if torch.distributed.get_rank() == 0:\n            print(f\"Encode {seq_len}: {(time.time() - start) * 1000:.2f} ms\")\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "cuda",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "evaluate.py",
          "type": "blob",
          "size": 2.4482421875,
          "content": "import time\nimport importlib\n\nfrom os.path import join, isdir, isfile, relpath\nfrom glob import glob\n\nfrom evaluation import BaseConfig, ModelForEvaluation, DEFAULT_CLASS, print_rank_0\nfrom initialize import initialize, initialize_model_and_tokenizer\n\n\ndef add_evaluation_specific_args(parser):\n    \"\"\"Arguments for evaluation\"\"\"\n    group = parser.add_argument_group(\"evaluation\", \"Evaluation configurations\")\n\n    # Task\n    group.add_argument(\"--task\", nargs=\"+\", default=[], help=\"All task config to evaluation\")\n    group.add_argument(\"--data-path\", type=str, required=True, help=\"Data dir path for all tasks\")\n    return parser\n\n\ndef find_all_tasks(all_task_config_path):\n    tasks = []\n    for task in all_task_config_path:\n        if isdir(task):\n            tasks += [relpath(path, \".\") for path in glob(join(task, \"**/*.yaml\"), recursive=True)]\n        elif isfile(task):\n            tasks.append(task)\n    return tasks\n\n\ndef evaluate_all_tasks(data_path, model, tokenizer, all_task_config_path, task_classes):\n    for config_path, task_class in zip(all_task_config_path, task_classes):\n        config = task_class.config_class().from_yaml_file(config_path)\n        config.path = join(data_path, config.path)\n        task = task_class(model, tokenizer, config)\n        task.evaluate()\n\n\ndef main():\n    args = initialize(extra_args_provider=add_evaluation_specific_args)\n    args.task = find_all_tasks(args.task)\n\n    task_classes = []\n    print_rank_0(\"> Loading task configs\")\n    for task_config_path in args.task:\n        config = BaseConfig.from_yaml_file(task_config_path)\n        if config.module:\n            path = \".\".join(config.module.split(\".\")[:-1])\n            module = importlib.import_module(path)\n            class_name = config.module.split(\".\")[-1]\n            task_class = getattr(module, class_name)\n            task_classes.append(task_class)\n        else:\n            task_classes.append(DEFAULT_CLASS[config.type])\n        print_rank_0(f\"    Task {config.name} loaded from config {task_config_path}\")\n    print_rank_0(f\"> Successfully load {len(task_classes)} task{'s' if len(task_classes) > 1 else ''}\")\n\n    model, tokenizer = initialize_model_and_tokenizer(args)\n    model = ModelForEvaluation(model)\n\n    start = time.time()\n    evaluate_all_tasks(args.data_path, model, tokenizer, args.task, task_classes)\n    print_rank_0(f\"Finish {len(task_classes)} task{'s' if len(task_classes) > 1 else ''} in {time.time() - start:.1f}s\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "evaluation",
          "type": "tree",
          "content": null
        },
        {
          "name": "generate.py",
          "type": "blob",
          "size": 7.875,
          "content": "import os\nimport torch\nimport stat\nimport re\n\nfrom functools import partial\nfrom typing import List, Tuple\n\nfrom SwissArmyTransformer import mpu\nfrom evaluation.model import batch_filling_sequence\nfrom generation import BeamSearchStrategy, BaseStrategy\nfrom SwissArmyTransformer.generation.utils import timed_name, generate_continually\nfrom initialize import initialize, initialize_model_and_tokenizer\n\n\ndef add_generation_specific_args(parser):\n    parser.add_argument(\"--sampling-strategy\", type=str, default=\"BaseStrategy\", help=\"Type of sampling strategy.\")\n    parser.add_argument(\"--min-gen-length\", type=int, default=0, help=\"The minimum length each blank should generate.\")\n    parser.add_argument(\n        \"--print-all-beams\", action=\"store_true\", help=\"Print all output generated by beam search strategy.\"\n    )\n\n\ndef isEnglish(s):\n    try:\n        s.encode(encoding=\"utf-8\").decode(\"ascii\")\n    except UnicodeDecodeError:\n        return False\n    else:\n        return True\n\n\ndef get_masks_and_position_ids(seq, mask_position, max_gen_length, gmask=False):\n    context_length = seq.shape[1]\n    tokens = torch.nn.functional.pad(seq, (0, max_gen_length), mode=\"constant\", value=-1)\n    attention_mask = torch.ones((1, tokens.shape[-1], tokens.shape[-1]), device=tokens.device)\n    attention_mask.tril_()\n    attention_mask[..., : context_length - 1] = 1\n    attention_mask.unsqueeze_(1)\n    attention_mask = (attention_mask < 0.5).bool()\n\n    position_ids = torch.arange(tokens.shape[-1], dtype=torch.long, device=tokens.device)\n    if not gmask:\n        position_ids[context_length - 1 :] = mask_position\n\n    position_ids = position_ids.unsqueeze(0)\n\n    return tokens, attention_mask, position_ids\n\n\ndef fill_blanks(raw_text: str, model, tokenizer, strategy) -> Tuple[List[str], List[str], List[List[str]]]:\n    # add MASK\n    generation_mask = \"[gMASK]\"\n    if \"[MASK]\" in raw_text:\n        generation_mask = \"[MASK]\"\n    elif \"[sMASK]\" in raw_text:\n        generation_mask = \"[sMASK]\"\n    use_gmask = \"[MASK]\" not in raw_text and \"[sMASK]\" not in raw_text\n\n    mask_pattern = r\"\\[[sg]?MASK\\]\"\n    text_list = re.split(mask_pattern, raw_text)\n    pattern_list = re.compile(mask_pattern).findall(raw_text)\n    seq = []\n    for i in range(len(pattern_list)):\n        pattern = pattern_list[i]\n        sub_text = text_list[i]\n        seq.extend(tokenizer.tokenize(sub_text))\n        seq.append(tokenizer.get_command(pattern))\n\n    seq.extend(tokenizer.tokenize(text_list[-1]))\n\n    if \"MASK]\" not in raw_text:\n        seq += [tokenizer.get_command(generation_mask)]\n        raw_text += \" \" + generation_mask\n    if not raw_text.endswith(\"MASK]\"):\n        seq = seq + [tokenizer.get_command(\"eos\")]\n    if mpu.get_model_parallel_rank() == 0:\n        print(\"\\nInput: {}\\n\".format(raw_text))\n    if len(seq) > args.max_sequence_length:\n        raise ValueError(\"text too long.\")\n\n    # generation\n    is_english = isEnglish(raw_text)\n    output_list = [seq]\n    num_output = args.num_beams if args.sampling_strategy == \"BeamSearchStrategy\" else 1\n    last_pos, answers, answers_with_style, blanks = (\n        [0] * num_output,\n        [\"\" for _ in range(num_output)],\n        [\"\" for _ in range(num_output)],\n        [[] for _ in range(num_output)],\n    )\n\n    # continually detect the first mark position\n    while True:\n        seq = output_list[0]\n        # detect mask position\n        mask_token = tokenizer.get_command(generation_mask)\n        if mask_token not in seq:\n            break\n        mask_position = seq.index(mask_token)\n\n        output_list = []\n\n        input_seq = torch.cuda.LongTensor(\n            [seq + [tokenizer.get_command(\"sop\")]],\n            device=args.device,\n        )\n        output, _ = batch_filling_sequence(\n            model,\n            input_seq,\n            torch.cuda.LongTensor([input_seq.shape[-1]], device=args.device),\n            strategy=strategy,\n            get_masks_and_position_ids=partial(\n                get_masks_and_position_ids,\n                mask_position=mask_position,\n                max_gen_length=args.out_seq_length - input_seq.shape[-1],\n                gmask=use_gmask,\n            ),\n        )\n        if isinstance(output, torch.Tensor):  # different strategies\n            output = output.tolist()\n        output = output[0]  # batch_size = 1\n        output_list.extend(output)\n\n        # clip -1s and fill back generated things into seq\n        for i in range(len(output_list)):\n            output = output_list[i].tolist() if isinstance(output_list[i], torch.Tensor) else output_list[i]\n            try:\n                unfinished = output.index(-1)\n            except ValueError:\n                unfinished = len(output)\n            if output[unfinished - 1] in strategy.end_tokens:\n                unfinished -= 1\n            bog = output.index(tokenizer.get_command(\"sop\"))\n\n            prefix = tokenizer.detokenize(output[last_pos[i] : mask_position])\n            blank = tokenizer.detokenize(output[bog + 1 : unfinished])\n            answers_with_style[i] += (\n                prefix\n                + (\" \" if is_english else \"\")\n                + (\"\\033[4m\" if use_gmask else \"\\x1b[0;32m\\033[4m\")\n                + blank\n                + (\"\\033[0m\" if use_gmask else \"\\033[0m\\x1b[0m\")\n                + (\" \" if is_english else \"\")\n            )\n            blanks[i].append(blank)\n            last_pos[i] = mask_position + unfinished - (bog + 1)\n            output_list[i] = output[:mask_position] + output[bog + 1 : unfinished] + output[mask_position + 1 : bog]\n\n    for i, output in enumerate(output_list):\n        if output[-1] == tokenizer.get_command(\"eos\"):\n            output = output[:-1]\n        answers_with_style[i] += tokenizer.detokenize(output[last_pos[i] :])\n        answers[i] = tokenizer.detokenize(output)\n\n    return answers, answers_with_style, blanks\n\n\ndef main(args):\n    model, tokenizer = initialize_model_and_tokenizer(args)\n\n    end_tokens = [tokenizer.get_command(\"eop\"), tokenizer.get_command(\"eos\")]\n\n    if args.sampling_strategy == \"BaseStrategy\":\n        strategy = BaseStrategy(\n            batch_size=1, temperature=args.temperature, top_k=args.top_k, top_p=args.top_p, end_tokens=end_tokens\n        )\n    elif args.sampling_strategy == \"BeamSearchStrategy\":\n        strategy = BeamSearchStrategy(\n            1,\n            args.num_beams,\n            length_penalty=args.length_penalty,\n            consider_end=True,\n            end_tokens=end_tokens,\n            no_repeat_ngram_size=args.no_repeat_ngram_size,\n            min_gen_length=args.min_gen_length,\n        )\n    else:\n        raise ValueError(f\"unknown strategy {args.sampling_strategy}\")\n\n    def process(raw_text):\n        if args.with_id:\n            query_id, raw_text = raw_text.split(\"\\t\")\n\n        answers, answers_with_style, blanks = fill_blanks(raw_text, model, tokenizer, strategy)\n\n        # save\n        if args.with_id:\n            full_path = os.path.join(args.output_path, query_id + \".txt\")\n        else:\n            prefix = raw_text.replace(\"/\", \"\")[:20]\n            full_path = timed_name(prefix, \".txt\", args.output_path)\n        if mpu.get_model_parallel_rank() == 0:\n            if args.print_all_beams and len(answers) > 1:\n                for idx, answer_with_style in enumerate(answers_with_style):\n                    print(f\"Output beam {idx}:\", answer_with_style)  # print the first.\n                    if len(answer_with_style) > 120:\n                        print(\"\")\n            else:\n                print(f\"Output:\", answers_with_style[0])  # print the first.\n            with open(full_path, \"w\", encoding=\"utf-8\") as fout:\n                for answer in answers:\n                    fout.write(answer + \"\\n\")\n\n            os.chmod(full_path, stat.S_IRWXO + stat.S_IRWXG + stat.S_IRWXU)\n\n    os.makedirs(args.output_path, exist_ok=True)\n    generate_continually(process, args.input_source)\n\n\nif __name__ == \"__main__\":\n    args = initialize(extra_args_provider=add_generation_specific_args)\n\n    with torch.no_grad():\n        main(args)\n"
        },
        {
          "name": "generation",
          "type": "tree",
          "content": null
        },
        {
          "name": "initialize.py",
          "type": "blob",
          "size": 4.095703125,
          "content": "import argparse\nimport torch\nimport time\n\nfrom quantization import quantize\n\nfrom SwissArmyTransformer import get_args, get_tokenizer\nfrom SwissArmyTransformer.arguments import initialize_distributed\nfrom SwissArmyTransformer.training import load_checkpoint\nfrom SwissArmyTransformer.model import GLM130B\nfrom SwissArmyTransformer.mpu import get_model_parallel_world_size, get_model_parallel_rank, get_model_parallel_group\n\n\ndef add_bminf_args(parser):\n    \"\"\"Arguments for BMInf\"\"\"\n    group = parser.add_argument_group(\"BMInf\")\n\n    group.add_argument(\"--bminf\", action=\"store_true\", help=\"Use BMInf to support low resource evaluation\")\n    group.add_argument(\"--bminf-memory-limit\", type=int, default=20, help=\"Max memory for model per GPU (in GB)\")\n    return parser\n\n\ndef add_quantization_args(parser):\n    group = parser.add_argument_group(\"Quantization\")\n\n    group.add_argument(\"--quantization-bit-width\", type=int, default=None)\n    group.add_argument(\"--from-quantized-checkpoint\", action=\"store_true\", help=\"Loading from a quantized checkpoint\")\n\n\ndef add_initialization_args(parser):\n    group = parser.add_argument_group(\"Initialization\")\n\n    group.add_argument(\n        \"--sequential-initialization\",\n        action=\"store_true\",\n        help=\"Initialize sequentially in tensor parallel group (reduce CPU RAM for initialization)\",\n    )\n\n\ndef initialize(extra_args_provider):\n    parser = argparse.ArgumentParser(add_help=False)\n    add_bminf_args(parser)\n    add_quantization_args(parser)\n    add_initialization_args(parser)\n    GLM130B.add_model_specific_args(parser)\n    extra_args_provider(parser)\n    known, args_list = parser.parse_known_args()\n    args = get_args(args_list)\n    args = argparse.Namespace(**vars(args), **vars(known))\n    args.do_train = False\n    initialize_distributed(args)\n    return args\n\n\ndef initialize_model_and_tokenizer(args):\n    tokenizer = get_tokenizer(args)\n\n    torch.distributed.barrier()\n    start = time.time()\n\n    for i in range(get_model_parallel_world_size()):\n        if get_model_parallel_rank() == i:\n            # Initialize model\n            model = GLM130B(args).half()\n\n            if args.from_quantized_checkpoint:\n                assert args.quantization_bit_width is not None\n                # Quantize model before moving to GPU\n                model = quantize(model, args.quantization_bit_width)\n\n            # Load checkpoint\n            load_checkpoint(model, args)\n\n            if args.quantization_bit_width is not None and not args.from_quantized_checkpoint:\n                # Quantize model before moving to GPU\n                model = quantize(model, args.quantization_bit_width)\n\n            if args.bminf:\n                import bminf\n\n                if torch.distributed.get_rank() == 0:\n                    print(f\"> BMInf activated, memory limit: {args.bminf_memory_limit} GB\")\n                with torch.cuda.device(args.device):\n                    model = bminf.wrapper(model, quantization=False, memory_limit=args.bminf_memory_limit << 30)\n            else:\n                model = model.to(args.device)\n        if args.sequential_initialization:\n            torch.distributed.barrier(group=get_model_parallel_group())\n\n    torch.distributed.barrier()\n    if torch.distributed.get_rank() == 0:\n        print(f\"> Model initialized in {time.time() - start:.1f}s\")\n\n    torch.cuda.empty_cache()\n    model.eval()\n\n    # generate rotary embedding cache\n    original_parallel_output = model.transformer.parallel_output\n    model.transformer.parallel_output = True\n    with torch.no_grad():\n        _, *_ = model(\n            torch.ones(1, args.max_sequence_length, device=torch.cuda.current_device(), dtype=torch.int64),\n            torch.arange(args.max_sequence_length, device=torch.cuda.current_device(), dtype=torch.int64).view(1, -1),\n            torch.randn(\n                1,\n                1,\n                args.max_sequence_length,\n                args.max_sequence_length,\n                device=torch.cuda.current_device(),\n            )\n            < 0.5,\n        )\n    model.transformer.parallel_output = original_parallel_output\n    torch.distributed.barrier()\n\n    return model, tokenizer\n"
        },
        {
          "name": "kernels",
          "type": "tree",
          "content": null
        },
        {
          "name": "logs",
          "type": "tree",
          "content": null
        },
        {
          "name": "quantization",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.078125,
          "content": "SwissArmyTransformer>=0.2.12,<0.3\nicetk\napex\nscipy\ndataclass_wizard\ncpm_kernels\n"
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "tasks",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}