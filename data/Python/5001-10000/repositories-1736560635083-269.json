{
  "metadata": {
    "timestamp": 1736560635083,
    "page": 269,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "THUDM/GLM-130B",
      "stars": 7679,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0400390625,
          "content": "data\n__pycache__\nsamples\n.DS_Store\n.idea\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0693359375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright Aohan Zeng\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MODEL_LICENSE",
          "type": "blob",
          "size": 2.29296875,
          "content": "The GLM-130B License\n\n1. Definitions\n\nâ€œLicensorâ€ means the GLM-130B Model Team that distributes its Software.\n\nâ€œSoftwareâ€ means the GLM-130B model parameters made available under this license.\n\n2. License Grant\n\nSubject to the terms and conditions of this License, the Licensor hereby grants to you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license to use the Software solely for your non-commercial research purposes.\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n3. Restriction\n\nYou will not use, copy, modify, merge, publish, distribute, reproduce, or create derivative works of the Software, in whole or in part, for any commercial, military, or illegal purposes.\n\nYou will not use the Software for any act that may undermine China's national security and national unity, harm the public interest of society, or infringe upon the rights and interests of human beings.\n\n4. Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n5. Limitation of Liability\n\nEXCEPT TO THE EXTENT PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER BASED IN TORT, NEGLIGENCE, CONTRACT, LIABILITY, OR OTHERWISE WILL ANY LICENSOR BE LIABLE TO YOU FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, OR ANY OTHER COMMERCIAL LOSSES, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n6. Dispute Resolution\n\nThis license shall be governed and construed in accordance with the laws of Peopleâ€™s Republic of China. Any dispute arising from or in connection with this License shall be submitted to Haidian District People's Court in Beijing.\n\nNote that the license is subject to update to a more comprehensive version.  For any questions related to the license and copyright, please contact us at glm-130b@googlegroups.com.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.96484375,
          "content": "<img src=\"resources/7D6433A42D189E2E6FBC62BE066BCE91.png\">\r\n\r\n<p align=\"center\">\r\n   ğŸŒ <a href=\"http://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/\" target=\"_blank\">Blog</a> â€¢ â¬ <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSehr5Dh_i3TwACmFFi8QEgIVNYGmSPwV0GueIcsUev0NEfUug/viewform\" target=\"_blank\">Download Model</a> â€¢ ğŸª§ <a href=\"https://huggingface.co/spaces/THUDM/GLM-130B\" target=\"_blank\">Demo</a> â€¢ âœ‰ï¸ <a href=\"mailto:glm-130b@googlegroups.com\">Email</a> â€¢ ğŸ“ƒ <a href=\"https://arxiv.org/abs/2210.02414\" target=\"_blank\">Paper [ICLR 2023]</a><br>\r\n</p>\r\n\r\n<p align=\"center\">\r\n   ğŸ’¬ <a href=\"https://groups.google.com/g/glm-130b-forum\" target=\"_blank\">Google Group</a> (Updates) or <a href=\"https://github.com/THUDM/GLM-130B/blob/main/resources/WECHAT.md\" target=\"_blank\">Wechat Group</a> or <a href=\"https://join.slack.com/t/glm-130b/shared_invite/zt-1f2ih11xy-EAuDComTAr~XVB3MywE9Cg\" target=\"_blank\">Slack channel</a> (Discussions)\r\n</p>\r\n\r\n# GLM-130B: An Open Bilingual Pre-Trained Model\r\n\r\nGLM-130B is an open bilingual (English & Chinese) bidirectional dense model with 130 billion parameters, pre-trained using the algorithm of [General Language Model (GLM)](https://aclanthology.org/2022.acl-long.26). It is designed to support inference tasks with the 130B parameters on **a single A100 (40G * 8)** or **V100 (32G * 8) server**. With INT4 quantization, the  hardware requirements can further be reduced to **a single server with 4 * RTX 3090 (24G)** with **almost no performance degradation**. As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English) and it has the following unique features:\r\n \r\n- **Bilingual:** supports both English and Chinese. \r\n- **Performance (EN):** better than GPT-3 175B (+4.0%), OPT-175B (+5.5%), and BLOOM-176B (+13.0%) on LAMBADA and slightly better than GPT-3 175B (+0.9%) on MMLU.\r\n- **Performance (CN):** significantly better than ERNIE TITAN 3.0 260B on 7 zero-shot CLUE datasets (+24.26%) and 5 zero-shot FewCLUE datasets (+12.75%). \r\n- **Fast Inference:** supports fast inference on both [SAT](https://github.com/THUDM/SwissArmyTransformer) and [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) (up to 2.5X faster) with a single A100 server.\r\n- **Reproducibility:** all results (30+ tasks) can be easily reproduced with open-sourced code and model checkpoints.\r\n- **Cross-Platform:** supports training and inference on NVIDIA, Hygon DCU, Ascend 910, and Sunway (Will be released soon).\r\n\r\nThis repository mainly focus on the evaluation of GLM-130B. If you find our work and our open-sourced efforts useful, â­ï¸ to encourage our following development! :)\r\n\r\n## News\r\n- **[2023.06.25]** Release [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B), an updated version of [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) which introduces **Stronger Performance** (MMLU (+23%), CEval (+33%), GSM8K (+571%), BBH (+60%)), **Longer Context** (from 2K in ChatGLM-6B to 32K, and trained with a context length of 8K during the dialogue alignment), and **More Efficient Inference** (speeds up by 42% under the official implementation; the dialogue length supported by 6G GPU memory has increased from 1K to 8K). More details please refer to [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)ã€‚\r\n- **[2023.06.14]** We release the research [WebGLM](https://github.com/THUDM/WebGLM), which enables efficient and accurate web-enhanced question answering. All code and data are released!\r\n- **[2023.03.14]** We are happy to introduce [ChatGLM](https://chatglm.cn/blog), a bilingual dialogue language model based on GLM-130B, and its open-sourced version [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) which can be run under only **6GB** GPU memory! \r\n- **[2023.01.21]** GLM-130B has been accepted to [ICLR 2023](https://iclr.cc/Conferences/2023)!\r\n- **[2022.10.06]** Our [paper](http://arxiv.org/abs/2210.02414) for GLM-130B is out!\r\n- **[2022.08.24]** We are proud to publish the quantized version for GLM-130B.  While preserving the activation precision as FP16, the model weights can be quantized to as low as **INT4 with almost no degradation of performance**, further reducing the hardware requirements of the GLM-130B to **a single server with 4 * RTX 3090 (24G)**! See [Quantization of GLM-130B](docs/quantization.md) for details.\r\n\r\nFor smaller models, please find [monolingual GLMs](https://github.com/THUDM/GLM) (English: 10B/2B/515M/410M/335M/110M, Chinese: 10B/335M) and an [1B multilingual GLM](https://github.com/THUDM/Multilingual-GLM) (104 languages).\r\n\r\n## Getting Started\r\n\r\n### Environment Setup\r\n\r\n#### Hardware\r\n\r\n| **Hardware**    | **GPU Memory** | **Quantization** | **Weight Offload** |\r\n| --------------- | -------------- | ---------------- | ------------------ |\r\n| 8 * A100        | 40 GB          | No               | No                 |\r\n| 8 * V100        | 32 GB          | No               | Yes (BMInf)        |\r\n| 8 * V100        | 32 GB          | INT8             | No                 |\r\n| 8 * RTX 3090    | 24 GB          | INT8             | No                 |\r\n| 4 * RTX 3090    | 24 GB          | INT4             | No                 |\r\n| 8 * RTX 2080 Ti | 11 GB          | INT4             | No        |\r\n\r\nIt is recommended to use the an A100 (40G * 8) server, as all GLM-130B evaluation results (~30 tasks) reported can be easily reproduced with a single A100 server in about half a day. With INT8/INT4 quantization, efficient inference on **a single server with 4 * RTX 3090 (24G)** is possible, see [Quantization of GLM-130B](docs/quantization.md) for details. Combining quantization and weight offloading techniques, GLM-130B can also be inferenced on servers with even smaller GPU memory, see [Low-Resource Inference](docs/low-resource-inference.md) for details.\r\n\r\n#### Software\r\n\r\nThe GLM-130B code is built on the top of [SAT](https://github.com/THUDM/SwissArmyTransformer). We recommend using [Miniconda](https://docs.conda.io/en/latest/miniconda.html) to manage your environment and installing additional dependencies via `pip install -r requirements.txt`. Here are the recommended environment configurations:\r\n\r\n- Python 3.9+ / CUDA 11+ / PyTorch 1.10+ / DeepSpeed 0.6+ / Apex (**installation with CUDA and C++ extensions is required, see [here](https://github.com/NVIDIA/apex/#linux)**)\r\n- SwissArmyTransformer>=0.2.11 is required for quantization\r\n\r\n#### Model weights\r\n\r\nDownload the GLM-130Bâ€™s model checkpoint from [here](https://docs.google.com/forms/d/e/1FAIpQLSehr5Dh_i3TwACmFFi8QEgIVNYGmSPwV0GueIcsUev0NEfUug/viewform?usp=sf_link), make sure all 60 chunks are downloaded completely, then use the following command to merge them into a single archive file and extract it:\r\n\r\n```bash\r\ncat glm-130b-sat.tar.part_* > glm-130b-sat.tar\r\ntar xvf glm-130b-sat.tar\r\n```\r\n\r\nSet `CHECKPOINT_PATH` in `configs/model_glm_130b.sh` to the path of the extracted folder. Since the checkpoint file is up to 260G, it is recommended to use the SSD or RAM disk to reduce the checkpoint loading time. Since the checkpoint we distribute is in 8-way tensor parallel, a conversion scripts is also provided if you need to change the tensor parallel dimension.\r\n\r\n```bash\r\npython tools/convert_tp.py \\\r\n    --input-folder <SRC_CKPT_PATH>  \\\r\n    --output-folder <DST_CKPT_PATH> \\\r\n    --target-tp <TARGET_TP>\r\n```\r\n\r\n### Left-To-Right Generation / Blank Filling\r\n\r\n```bash\r\nbash scripts/generate.sh --input-source interactive\r\n```\r\n\r\nYou can also specify an input file by `--input-source input.txt`.\r\n\r\nGLM-130B uses two different mask tokens: `[MASK]` for short blank filling and `[gMASK]` for left-to-right long text generation. When the input does not contain any MASK token, `[gMASK]` will be automatically appended to the end of the text.\r\n\r\n<details>\r\n<summary><b>Left-To-Right Generation Examples</b></summary>\r\n    \r\n#### Example 1\r\nInput: Who is the greatest artist? The greatest artist is [gMASK]\r\n\r\nOutput: Who is the greatest artist? The greatest artist is not the one who paints the most beautiful picture, or writes the best poem, or plays the best symphony. The greatest artist is the one whose work has the greatest impact on the lives of others. The Greatest Artist is the artist who inspires us to be our best selves. The Great Artist is not the artist whose work is flawless. The great artist is not one who is perfect. The true great artist has the ability to make us feel something. The artist who makes us feel is the great artist. The art of the great artists is to make you feel something that you have never felt before. The greatness of an artist is in the ability of their work to touch the lives and hearts of others, and to make them feel something they have never experienced before. This is the true greatness of the artist. 2. The Significance of the Great Artist The significance of the greatest artist cannot be overstated. The significance and impact of the art of a great artist cannot even be measured. The impact of a single work of art can change the course of history. A single work can change a personâ€™s life. A work of great art can make you laugh, make you cry, make your heart race, or make you think. A great work of artistic expression has the power to change the way you look at the world. The work of a truly great artist can change your life forever. 3. The Impact of the Greatest Artist The greatest artists have the ability and the power of their art to change peopleâ€™s lives. They have the power and the ability, through their art, to make people feel something new. The power of the work of the truly great artists cannot be measured, but it can be felt. The greater the impact of an artists work, the greater the power it has to change lives. 4. The Influence of the Art of the Artist The art and the work created by the greatest artists has the potential to change a life. The influence of the artists work cannot be quantified, but is felt by those who experience it. The truly great work has an ability to touch peopleâ€™ lives in a way that no other work can.\r\n  \r\n#### Example 2 (Chinese)\r\nInput: é—®é¢˜ï¼šå†¬å¤©ï¼Œä¸­å›½å“ªåº§åŸå¸‚æœ€é€‚åˆé¿å¯’ï¼Ÿé—®é¢˜æè¿°ï¼šèƒ½æ¨èä¸€äº›å›½å†…é€‚åˆå†¬å¤©é¿å¯’çš„åŸå¸‚å—ï¼Ÿå›ç­”ç”¨æˆ·ï¼šæ—…æ¸¸çˆ±å¥½è€… å›ç­”ï¼š [gMASK]\r\n  \r\nOutput: é—®é¢˜:å†¬å¤©,ä¸­å›½å“ªåº§åŸå¸‚æœ€é€‚åˆé¿å¯’?é—®é¢˜æè¿°:èƒ½æ¨èä¸€äº›å›½å†…é€‚åˆå†¬å¤©é¿å¯’çš„åŸå¸‚å—?å›ç­”ç”¨æˆ·:æ—…æ¸¸çˆ±å¥½è€… å›ç­”:é¿å¯’çš„è¯,å»ºè®®å»æµ·å—ä¸‰äºšã€‚ ä¸‰äºšä½äºæµ·å—å²›çš„æœ€å—ç«¯,æ˜¯ä¸­å›½æœ€å—éƒ¨çš„çƒ­å¸¦æ»¨æµ·æ—…æ¸¸åŸå¸‚,ä¸­å›½æµ·æ»¨åŸå¸‚,æ˜¯ä¸­å›½ç©ºæ°”è´¨é‡æœ€å¥½çš„åŸå¸‚ã€å…¨å›½æœ€é•¿å¯¿åœ°åŒº(å¹³å‡å¯¿å‘½80å²)ã€‚ä¸‰äºšå¸‚åˆ«ç§°é¹¿åŸ,åˆè¢«ç§°ä¸º\"ä¸œæ–¹å¤å¨å¤·\",ä½å±…ä¸­å›½å››å¤§ä¸€çº¿æ—…æ¸¸åŸå¸‚\"ä¸‰å¨æ­å¦\"ä¹‹é¦–,æ‹¥æœ‰å…¨å²›æœ€ç¾ä¸½çš„æµ·æ»¨é£å…‰ã€‚ä¸‰äºšæ‹¥æœ‰è¢«æ— æ•°åŸå¸‚å«‰å¦’çš„æ¸…æ–°ç©ºæ°”,æŸ”å’Œæµ·æ»©ã€‚åœ¨æ²™æ»©æ‚ é—²æ•£æ­¥ã€æ²æµ´å‚æ™šæ¸©å’Œé˜³å…‰,åœ¨æµ·è¾¹ç©è€,åœ¨é›¨æ—é‡Œé¢å‘¼å¸å¥åº·,æ¬£èµè‡ªç„¶å¥‡è§‚,ä¸€åˆ‡éƒ½æ˜¯é‚£ä¹ˆä»¤äººäº«å—ã€‚ æ°”å€™ ä¸‰äºšåœ°å¤„çƒ­å¸¦,å±çƒ­å¸¦å­£é£æ°”å€™åŒºåŸŸ,ç»ˆå¹´æ°”æ¸©é«˜,å¯’æš‘å˜åŒ–ä¸å¤§,å¹´å¹³å‡æ°”æ¸©25.5åº¦ã€‚ä¸‰äºšæœ€çƒ­çš„æœˆä»½æ˜¯6æœˆ,å¹³å‡æ°”æ¸©ä¸º28.5Â°C,æç«¯æœ€é«˜æ°”æ¸©ä¸º35.7Â°C ,ä¸‰äºšæ²¡æœ‰æ˜æ˜¾çš„å†¬å­£,è€Œä¸”å†¬å­£æ›´æ˜¯åˆ°æµ·æ»¨æ—…æ¸¸çš„æœ€å¥½æ—¶å€™,æ‰€ä»¥ä¸‰äºšæ˜¯å…¨å¹´å…¨å¤©å€™é¿å¯’ã€æ¶ˆæš‘ã€åº¦å‡ã€æ—…æ¸¸çš„å¥½åœ°æ–¹ã€‚ä¸è¿‡,ä¸‰äºšæœ€æ—ºçš„æ—…æ¸¸æ—¶é—´æ˜¯ä»10æœˆä»½åˆ°å†œå†æ˜¥èŠ‚,è¿™ä¸ªæ—¶å€™çš„äººç‰¹åˆ«å¤š,æˆ¿é—´ä¹Ÿä¸å¥½è®¢,å»ºè®®æœ€å¥½é¿å¼€é«˜å³°ã€‚ä¸‰äºšçš„ä¸‰ä¸ªæ—…æ¸¸é«˜å³°æœŸæ˜¯\"äº”ä¸€\"ã€\"åä¸€\"ã€\"æ˜¥èŠ‚\",è¿™ä¸‰ä¸ªèŠ‚æ—¥æœŸé—´çš„æˆ¿ä»·éƒ½ä¼šä¸Šæµ®å‡ å€,å¦‚æœä½ é€‰æ‹©äº†è¿™ä¸‰ä¸ªå‡æœŸæ¥ä¸‰äºšçš„è¯è¦æå‰ä¸€ä¸ªæ˜ŸæœŸå®šæˆ¿å’Œå’¨è¯¢ã€‚\r\n</details>\r\n\r\n<details>\r\n<summary><b>Blank Filling Examples</b></summary>\r\n  \r\n#### Example 1\r\nInput: Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.\r\n  \r\nOutput: Ng is an adjunct professor at Stanford University (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.<br/>\r\n\r\n#### Example 2 (Chinese)\r\nInput: å‡¯æ—‹é—¨ä½äºæ„å¤§åˆ©ç±³å…°å¸‚å¤åŸå ¡æ—ã€‚1807å¹´ä¸ºçºªå¿µ[MASK]è€Œå»ºï¼Œé—¨é«˜25ç±³ï¼Œé¡¶ä¸ŠçŸ—ç«‹ä¸¤æ­¦å£«é’é“œå¤å…µè½¦é“¸åƒã€‚\r\n  \r\nOutput: å‡¯æ—‹é—¨ä½äºæ„å¤§åˆ©ç±³å…°å¸‚å¤åŸå ¡æ—ã€‚1807å¹´ä¸ºçºªå¿µæ‹¿ç ´ä»‘èƒœåˆ©è€Œå»º,é—¨é«˜25ç±³,é¡¶ä¸ŠçŸ—ç«‹ä¸¤æ­¦å£«é’é“œå¤å…µè½¦é“¸åƒã€‚</code></pre>\r\n</details>\r\n\r\n<details>\r\n<summary><b>Arguments Useful in Generation</b></summary>\r\n\r\n- `--input-source [path] or \"interactive\"` The input file's path. It can also be \"interactive\", which will launch a CLI.\r\n- `â€”-output-path [path]` The folder containing the results.\r\n- `â€”-out-seq-length [int]` The maximum sequence length for generation (including context).\r\n- `â€”-min-gen-length [int]` The minimum generation length for each MASK.\r\n- `â€”-sampling-strategy \"BaseStrategy\" or \"BeamSearchStrategy\"`. The sampling strategy used.\r\n  - For BeamSearchStrategy:\r\n     - `â€”-num-beams [int]` The number of beams.\r\n     - `â€”-length-penalty [float]` The maximum sequence length for generation (including context).\r\n     - `â€”-no-repeat-ngram-size [int]` Prohibit repeated n-gram generation.\r\n     - `â€”-print-all-beam` Print the generated results for all beams.\r\n  - For BaseStrategy:\r\n     - `â€”-top-k [int]` Top k sampling.\r\n     - `â€”-top-p [float]` Top p sampling.\r\n     - `â€”-temperature [float]` The sampling temperature.\r\n</details>\r\n\r\n### Evaluation\r\n\r\nWe use the YAML file to define tasks. Specifically, you can add multiple tasks or folders at a time for evaluation, and the evaluation script will automatically collect all YAML files under those folders recursively.\r\n\r\n```\r\nbash scripts/evaluate.sh task1.yaml task2.yaml dir1 dir2 ...\r\n```\r\n\r\nDownload our evaluation dataset [here](https://cloud.tsinghua.edu.cn/f/826f0df4356f4022a264/), and set `DATA_PATH` in `scripts/evaluate.sh` to your local dataset directory. The task folder contains the YAML files for 30+ tasks we evaluated for GLM-130B. Take the [CoLA](https://nyu-mll.github.io/CoLA/) task for example, run `bash scripts/evaluate.sh tasks/bloom/glue_cola.yaml`, which outputs an accuracy of ~65% for the best prompt and ~57% for the median.\r\n\r\n<details>\r\n<summary>Expected Output</summary>\r\n  \r\n```plain\r\nMultiChoiceTaskConfig(name='glue_cola', type=<TaskType.MULTICHOICE: 'mul'>, path='/thudm/LargeScale/data/zeroshot/bloom/glue_cola', module=None, metrics=['Accuracy'], use_task_mask=False, use_multitask_encoding=False, unidirectional=False, max_seq_length=2048, file_pattern={'validation': '**/validation.jsonl'}, micro_batch_size=8)\r\nEvaluating task glue_cola:\r\n  Evaluating group validation:\r\n      Finish Following_sentence_acceptable/mul/validation.jsonl, Accuracy = 42.665\r\n      Finish Make_sense_yes_no/mul/validation.jsonl, Accuracy = 56.951\r\n      Finish Previous_sentence_acceptable/mul/validation.jsonl, Accuracy = 65.197\r\n      Finish editing/mul/validation.jsonl, Accuracy = 57.622\r\n      Finish is_this_correct/mul/validation.jsonl, Accuracy = 65.197\r\nEvaluation results of task glue_cola:\r\n  Group validation Accuracy: max = 65.197, median = 57.622, average = 57.526\r\nFinish task glue_cola in 101.2s. \r\n```\r\n</details>\r\n\r\nMulti-node evaluation can be configured by setting `HOST_FILE_PATH`(required by the [DeepSpeed lanucher](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)) in `scripts/evaluate_multiple_node.sh`. Set `DATA_PATH` in `scripts/evaluate_multiple_node.sh` and run the following command to evaluate all the tasks in `./task` directory.\r\n\r\n```\r\nbash scripts/evaluate_multiple_node.sh ./tasks\r\n```\r\n\r\nSee [Evaluate Your Own Tasks](docs/evaluate-your-own-tasks.md) for details on how to add new tasks.\r\n\r\n### 2.5X faster Inference using FasterTransformer\r\n\r\nBy adapting the GLM-130B model to [FasterTransfomer](https://github.com/NVIDIA/FasterTransformer), a highly optimized transformer model library by NVIDIA, we can reach up to 2.5X speedup on generation, see [Inference with FasterTransformer](docs/inference-with-fastertransformer.md) for details.\r\n\r\n\r\n\r\n## License\r\n\r\nThis repository is licensed under the [Apache-2.0 license](LICENSE). The use of GLM-130B model weights is subject to the [Model License](MODEL_LICENSE).\r\n\r\n## Citation\r\n\r\nIf you find our work useful, please consider citing GLM-130B:\r\n\r\n```\r\n@article{zeng2022glm,\r\n  title={Glm-130b: An open bilingual pre-trained model},\r\n  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},\r\n  journal={arXiv preprint arXiv:2210.02414},\r\n  year={2022}\r\n}\r\n```\r\n\r\nYou may also consider GLM's original work in your reference:\r\n\r\n```\r\n@inproceedings{du2022glm,\r\n  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},\r\n  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},\r\n  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\r\n  pages={320--335},\r\n  year={2022}\r\n}\r\n```\r\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 32.2880859375,
          "content": "<img src=\"resources/7D6433A42D189E2E6FBC62BE066BCE91.png\">\r\n\r\n<p align=\"center\">\r\n   ğŸŒ <a href=\"https://models.aminer.cn/glm-130b/\" target=\"_blank\">åšå®¢</a> â€¢ â¬ <a href=\"https://models.aminer.cn/glm/zh-CN/download/GLM-130B\" target=\"_blank\">ä¸‹è½½æ¨¡å‹</a> â€¢ ğŸª§ <a href=\"https://huggingface.co/spaces/hanyullai/GLM-130B\" target=\"_blank\">æ ·ä¾‹æ¼”ç¤º</a> â€¢ ğŸ’¬ <a href=\"https://github.com/THUDM/GLM-130B/discussions\">è®¨è®º</a> â€¢ âœ‰ï¸ <a href=\"mailto:glm-130b@googlegroups.com\">é‚®ç®±</a> â€¢ ğŸ’¬ <a href=\"https://groups.google.com/g/glm-130b-forum\" target=\"_blank\">è°·æ­Œç¾¤ç»„</a> or <a href=\"https://github.com/Xiao9905\" target=\"_blank\">å¾®ä¿¡ç¾¤</a>\r\n â€¢ ğŸ“ƒ è®ºæ–‡ï¼ˆæ•¬è¯·æœŸå¾…ï¼‰ <br>\r\n</p>\r\n\r\n# GLM-130Bï¼šå¼€æ”¾çš„ä¸­è‹±åŒè¯­é¢„è®­ç»ƒæ¨¡å‹\r\n\r\n## æ‘˜è¦ï¼šä½•ä¸º GLM-130Bï¼Ÿ\r\n\r\nGLM-130B æ˜¯ä¸€ä¸ªå¼€æºå¼€æ”¾çš„åŒè¯­ï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰åŒå‘ç¨ å¯†æ¨¡å‹ï¼Œæ‹¥æœ‰ 1300 äº¿ä¸ªå‚æ•°ï¼Œæ¨¡å‹æ¶æ„é‡‡ç”¨é€šç”¨è¯­è¨€æ¨¡å‹ï¼ˆGLMï¼‰ã€‚å®ƒæ—¨åœ¨æ”¯æŒåœ¨**ä¸€å° A100ï¼ˆ40G * 8ï¼‰** æˆ– **V100ï¼ˆ32G * 8ï¼‰æœåŠ¡å™¨**ä¸Šå¯¹åƒäº¿è§„æ¨¡çš„å‚æ•°è¿›è¡Œæ¨ç†ã€‚æˆªè‡³ 2022 å¹´ 7 æœˆ 3 æ—¥ï¼ŒGLM-130B å·²ç»å¯¹è¶…è¿‡ 4000 äº¿ä¸ªæ–‡æœ¬æ ‡è¯†ç¬¦ï¼ˆä¸­æ–‡å’Œè‹±æ–‡å„ 2000 äº¿ï¼‰è¿›è¡Œäº†è®­ç»ƒï¼Œå®ƒæœ‰ä»¥ä¸‹ç‹¬ç‰¹ä¼˜åŠ¿ï¼š\r\n\r\n* **åŒè¯­**ï¼šåŒæ—¶æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ã€‚ \r\n* **ä»»åŠ¡è¡¨ç°ï¼ˆè‹±æ–‡ï¼‰**ï¼š åœ¨ LAMBADA ä¸Šä¼˜äº GPT-3 175Bï¼ˆ+4.0%ï¼‰ã€OPT-175Bï¼ˆ+5.5%ï¼‰å’Œ BLOOM-176Bï¼ˆ+13.0%ï¼‰ï¼Œåœ¨ MMLU ä¸Šç•¥ä¼˜äºGPT-3 175Bï¼ˆ+0.9%ï¼‰ã€‚\r\n* **ä»»åŠ¡è¡¨ç°ï¼ˆä¸­æ–‡ï¼‰**ï¼šåœ¨ 7 ä¸ªé›¶æ ·æœ¬ CLUE æ•°æ®é›†ï¼ˆ+24.26%ï¼‰å’Œ 5 ä¸ªé›¶æ ·æœ¬ FewCLUE æ•°æ®é›†ï¼ˆ+12.75%ï¼‰ä¸Šæ˜æ˜¾ä¼˜äº ERNIE TITAN 3.0 260Bã€‚\r\n* **å¿«é€Ÿæ¨ç†**ï¼šæ”¯æŒç”¨ä¸€å° A100 æœåŠ¡å™¨ä½¿ç”¨ [SAT](https://github.com/THUDM/SwissArmyTransformer) å’Œ [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) è¿›è¡Œå¿«é€Ÿæ¨ç†ï¼ˆé€Ÿåº¦æœ€é«˜å¯è¾¾2.5å€ï¼‰ã€‚\r\n* **å¯å¤ç°æ€§**ï¼šæ‰€æœ‰çš„ç»“æœï¼ˆè¶…è¿‡30ä¸ªä»»åŠ¡ï¼‰éƒ½å¯ä»¥ç”¨æˆ‘ä»¬å¼€æºçš„ä»£ç å’Œæ¨¡å‹å‚æ•°è½»æ¾å¤ç°ã€‚\r\n* **å¤šå¹³å°**ï¼šæ”¯æŒåœ¨ NVIDIAã€Hygon DCUã€Ascend 910 å’Œ Sunway å¤„ç†å™¨ä¸Šè¿›è¡Œè®­ç»ƒä¸æ¨ç†ï¼ˆä»£ç å³å°†å¼€æºï¼‰ã€‚\r\n\r\n## å¿«é€Ÿä¸Šæ‰‹\r\n\r\n### ç¯å¢ƒé…ç½®\r\n\r\næˆ‘ä»¬çš„ä»£ç æ˜¯å»ºç«‹åœ¨ [SAT](https://github.com/THUDM/SwissArmyTransformer) ä¹‹ä¸Šçš„ã€‚æˆ‘ä»¬æ¨èä½¿ç”¨ Miniconda æ¥ç®¡ç†ç¯å¢ƒå¹¶é€šè¿‡ `pip install -r requirements.txt` æ¥å®‰è£…é¢å¤–çš„ä¾èµ–åŒ…ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬æ¨èçš„ç¯å¢ƒé…ç½®ï¼š\r\n\r\n- Python 3.9+ / PyTorch 1.10+ / DeepSpeed 0.6+ / Apexï¼ˆ**éœ€è¦å®‰è£…åŒ…å« CUDA å’Œ C++ æ‰©å±•çš„ç‰ˆæœ¬ï¼Œ[å‚è€ƒèµ„æ–™](https://github.com/NVIDIA/apex/#linux)**ï¼‰\r\n\r\nå»ºè®®ä½¿ç”¨ A100ï¼ˆ40G * 8ï¼‰æœåŠ¡å™¨ï¼Œå› ä¸ºæ‰€æœ‰æŠ¥å‘Šçš„è¯„ä¼°ç»“æœï¼ˆçº¦30ä¸ªä»»åŠ¡ï¼‰éƒ½å¯ä»¥ç”¨ä¸€å° A100 æœåŠ¡å™¨åœ¨å¤§çº¦åŠå¤©å†…è½»æ¾å†ç°ã€‚GLM-130B ä¹Ÿå¯ä»¥åœ¨å…·æœ‰è¾ƒå° GPU å†…å­˜çš„æœåŠ¡å™¨ä¸Šè¿›è¡Œæ¨æ–­ï¼Œä¾‹å¦‚å…·æœ‰ V100ï¼ˆ32G * 8ï¼‰çš„æœåŠ¡å™¨ã€‚è¯¦è§ [Low-resource Inference](docs/low-resource-inference.md)ã€‚\r\n\r\nä» [è¿™é‡Œ](https://models.aminer.cn/glm/zh-CN/download/GLM-130B) ç”³è¯·ä¸‹è½½ GLM-130B çš„æ¨¡å‹æ£€æŸ¥ç‚¹ï¼Œç¡®ä¿æ‰€æœ‰ 60 ä¸ªå—éƒ½å·²å®Œå…¨ä¸‹è½½ï¼Œç„¶åä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†å®ƒä»¬åˆå¹¶ä¸ºä¸€ä¸ªå­˜æ¡£æ–‡ä»¶å¹¶è§£å‹ç¼©ï¼š\r\n\r\n```bash\r\ncat glm-130b-sat.tar.part_* > glm-130b-sat.tar\r\ntar xvf glm-130b-sat.tar\r\n```\r\n\r\nå°† `configs/model_glm_130b.sh` ä¸­çš„ `CHECKPOINT_PATH` è®¾ç½®ä¸ºè§£å‹æ–‡ä»¶å¤¹çš„è·¯å¾„ã€‚ ç”±äº checkpoint æ–‡ä»¶æœ€å¤§ 260Gï¼Œå»ºè®®ä½¿ç”¨ SSD æˆ– RAM ç›˜æ¥å‡å°‘ checkpoint åŠ è½½æ—¶é—´ã€‚\r\n\r\n### è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ / ä¸­é—´æ–‡æœ¬å¡«ç©º\r\n\r\n```bash\r\nbash scripts/generate.sh --input-source interactive\r\n```\r\n\r\nä½ ä¹Ÿå¯ä»¥é€šè¿‡ `--input-source input.txt` æŒ‡å®šä¸€ä¸ªè¾“å…¥æ–‡ä»¶ã€‚\r\n\r\nGLM-130B ä½¿ç”¨ä¸¤ç§ä¸åŒçš„æ©ç æ ‡è®°ï¼š`[MASK]` ç”¨äºç®€çŸ­çš„å¡«ç©ºä»»åŠ¡ï¼Œ`[gMASK]` ç”¨äºä»å·¦åˆ°å³çš„é•¿æ–‡æœ¬ç”Ÿæˆã€‚å½“è¾“å…¥ä¸åŒ…å«ä»»ä½• MASK æ ‡è®°æ—¶ï¼Œ`[gMASK]` å°†è¢«è‡ªåŠ¨é™„åŠ åˆ°æ–‡æœ¬çš„æœ«å°¾ã€‚\r\n\r\n<details>\r\n<summary><b>é•¿æ–‡æœ¬ç”Ÿæˆæ ·ä¾‹</b></summary>\r\n    \r\n#### Example 1\r\nInput: Who is the greatest artist? The greatest artist is [gMASK]\r\n\r\nOutput: Who is the greatest artist? The greatest artist is not the one who paints the most beautiful picture, or writes the best poem, or plays the best symphony. The greatest artist is the one whose work has the greatest impact on the lives of others. The Greatest Artist is the artist who inspires us to be our best selves. The Great Artist is not the artist whose work is flawless. The great artist is not one who is perfect. The true great artist has the ability to make us feel something. The artist who makes us feel is the great artist. The art of the great artists is to make you feel something that you have never felt before. The greatness of an artist is in the ability of their work to touch the lives and hearts of others, and to make them feel something they have never experienced before. This is the true greatness of the artist. 2. The Significance of the Great Artist The significance of the greatest artist cannot be overstated. The significance and impact of the art of a great artist cannot even be measured. The impact of a single work of art can change the course of history. A single work can change a personâ€™s life. A work of great art can make you laugh, make you cry, make your heart race, or make you think. A great work of artistic expression has the power to change the way you look at the world. The work of a truly great artist can change your life forever. 3. The Impact of the Greatest Artist The greatest artists have the ability and the power of their art to change peopleâ€™s lives. They have the power and the ability, through their art, to make people feel something new. The power of the work of the truly great artists cannot be measured, but it can be felt. The greater the impact of an artists work, the greater the power it has to change lives. 4. The Influence of the Art of the Artist The art and the work created by the greatest artists has the potential to change a life. The influence of the artists work cannot be quantified, but is felt by those who experience it. The truly great work has an ability to touch peopleâ€™ lives in a way that no other work can.\r\n  \r\n#### Example 2 (Chinese)\r\nInput: é—®é¢˜ï¼šå†¬å¤©ï¼Œä¸­å›½å“ªåº§åŸå¸‚æœ€é€‚åˆé¿å¯’ï¼Ÿé—®é¢˜æè¿°ï¼šèƒ½æ¨èä¸€äº›å›½å†…é€‚åˆå†¬å¤©é¿å¯’çš„åŸå¸‚å—ï¼Ÿå›ç­”ç”¨æˆ·ï¼šæ—…æ¸¸çˆ±å¥½è€… å›ç­”ï¼š [gMASK]\r\n  \r\nOutput: é—®é¢˜:å†¬å¤©,ä¸­å›½å“ªåº§åŸå¸‚æœ€é€‚åˆé¿å¯’?é—®é¢˜æè¿°:èƒ½æ¨èä¸€äº›å›½å†…é€‚åˆå†¬å¤©é¿å¯’çš„åŸå¸‚å—?å›ç­”ç”¨æˆ·:æ—…æ¸¸çˆ±å¥½è€… å›ç­”:é¿å¯’çš„è¯,å»ºè®®å»æµ·å—ä¸‰äºšã€‚ ä¸‰äºšä½äºæµ·å—å²›çš„æœ€å—ç«¯,æ˜¯ä¸­å›½æœ€å—éƒ¨çš„çƒ­å¸¦æ»¨æµ·æ—…æ¸¸åŸå¸‚,ä¸­å›½æµ·æ»¨åŸå¸‚,æ˜¯ä¸­å›½ç©ºæ°”è´¨é‡æœ€å¥½çš„åŸå¸‚ã€å…¨å›½æœ€é•¿å¯¿åœ°åŒº(å¹³å‡å¯¿å‘½80å²)ã€‚ä¸‰äºšå¸‚åˆ«ç§°é¹¿åŸ,åˆè¢«ç§°ä¸º\"ä¸œæ–¹å¤å¨å¤·\",ä½å±…ä¸­å›½å››å¤§ä¸€çº¿æ—…æ¸¸åŸå¸‚\"ä¸‰å¨æ­å¦\"ä¹‹é¦–,æ‹¥æœ‰å…¨å²›æœ€ç¾ä¸½çš„æµ·æ»¨é£å…‰ã€‚ä¸‰äºšæ‹¥æœ‰è¢«æ— æ•°åŸå¸‚å«‰å¦’çš„æ¸…æ–°ç©ºæ°”,æŸ”å’Œæµ·æ»©ã€‚åœ¨æ²™æ»©æ‚ é—²æ•£æ­¥ã€æ²æµ´å‚æ™šæ¸©å’Œé˜³å…‰,åœ¨æµ·è¾¹ç©è€,åœ¨é›¨æ—é‡Œé¢å‘¼å¸å¥åº·,æ¬£èµè‡ªç„¶å¥‡è§‚,ä¸€åˆ‡éƒ½æ˜¯é‚£ä¹ˆä»¤äººäº«å—ã€‚ æ°”å€™ ä¸‰äºšåœ°å¤„çƒ­å¸¦,å±çƒ­å¸¦å­£é£æ°”å€™åŒºåŸŸ,ç»ˆå¹´æ°”æ¸©é«˜,å¯’æš‘å˜åŒ–ä¸å¤§,å¹´å¹³å‡æ°”æ¸©25.5åº¦ã€‚ä¸‰äºšæœ€çƒ­çš„æœˆä»½æ˜¯6æœˆ,å¹³å‡æ°”æ¸©ä¸º28.5Â°C,æç«¯æœ€é«˜æ°”æ¸©ä¸º35.7Â°C ,ä¸‰äºšæ²¡æœ‰æ˜æ˜¾çš„å†¬å­£,è€Œä¸”å†¬å­£æ›´æ˜¯åˆ°æµ·æ»¨æ—…æ¸¸çš„æœ€å¥½æ—¶å€™,æ‰€ä»¥ä¸‰äºšæ˜¯å…¨å¹´å…¨å¤©å€™é¿å¯’ã€æ¶ˆæš‘ã€åº¦å‡ã€æ—…æ¸¸çš„å¥½åœ°æ–¹ã€‚ä¸è¿‡,ä¸‰äºšæœ€æ—ºçš„æ—…æ¸¸æ—¶é—´æ˜¯ä»10æœˆä»½åˆ°å†œå†æ˜¥èŠ‚,è¿™ä¸ªæ—¶å€™çš„äººç‰¹åˆ«å¤š,æˆ¿é—´ä¹Ÿä¸å¥½è®¢,å»ºè®®æœ€å¥½é¿å¼€é«˜å³°ã€‚ä¸‰äºšçš„ä¸‰ä¸ªæ—…æ¸¸é«˜å³°æœŸæ˜¯\"äº”ä¸€\"ã€\"åä¸€\"ã€\"æ˜¥èŠ‚\",è¿™ä¸‰ä¸ªèŠ‚æ—¥æœŸé—´çš„æˆ¿ä»·éƒ½ä¼šä¸Šæµ®å‡ å€,å¦‚æœä½ é€‰æ‹©äº†è¿™ä¸‰ä¸ªå‡æœŸæ¥ä¸‰äºšçš„è¯è¦æå‰ä¸€ä¸ªæ˜ŸæœŸå®šæˆ¿å’Œå’¨è¯¢ã€‚\r\n</details>\r\n\r\n<details>\r\n<summary><b>æ–‡æœ¬å¡«ç©ºæ ·ä¾‹</b></summary>\r\n  \r\n#### Example 1\r\nInput: Ng is an adjunct professor at [MASK] (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.\r\n  \r\nOutput: Ng is an adjunct professor at Stanford University (formerly associate professor and Director of its Stanford AI Lab or SAIL ). Also a pioneer in online education, Ng co-founded Coursera and deeplearning.ai.<br/>\r\n\r\n#### Example 2 (Chinese)\r\nInput: å‡¯æ—‹é—¨ä½äºæ„å¤§åˆ©ç±³å…°å¸‚å¤åŸå ¡æ—ã€‚1807å¹´ä¸ºçºªå¿µ[MASK]è€Œå»ºï¼Œé—¨é«˜25ç±³ï¼Œé¡¶ä¸ŠçŸ—ç«‹ä¸¤æ­¦å£«é’é“œå¤å…µè½¦é“¸åƒã€‚\r\n  \r\nOutput: å‡¯æ—‹é—¨ä½äºæ„å¤§åˆ©ç±³å…°å¸‚å¤åŸå ¡æ—ã€‚1807å¹´ä¸ºçºªå¿µæ‹¿ç ´ä»‘èƒœåˆ©è€Œå»º,é—¨é«˜25ç±³,é¡¶ä¸ŠçŸ—ç«‹ä¸¤æ­¦å£«é’é“œå¤å…µè½¦é“¸åƒã€‚</code></pre>\r\n</details>\r\n\r\n\r\n<details>\r\n<summary><b>æ§åˆ¶ç”Ÿæˆçš„ä¸»è¦è¶…å‚æ•°</b></summary>\r\n\r\n- `--input-source [path] or \"interactive\"`. è¾“å…¥æ–‡ä»¶çš„è·¯å¾„ã€‚å½“è®¾ä¸º\"interactive\"æ—¶ï¼Œå°†ä¼šå¯åŠ¨äº¤äº’å¼CLIã€‚\r\n- `â€”-output-path [path]`. ç»“æœè¾“å‡ºè·¯å¾„ã€‚\r\n- `â€”-out-seq-length [int]`. ï¼ˆåŒ…æ‹¬è¾“å…¥å†…å®¹åœ¨å†…çš„ï¼‰æœ€å¤§è¾“å‡ºåºåˆ—é•¿åº¦ã€‚\r\n- `â€”-min-gen-length [int]` æ¯ä¸ªMASKæ ‡è¯†ç¬¦ä½ç½®çš„æœ€å°ç”Ÿæˆé•¿åº¦ã€‚\r\n- `â€”-sampling-strategy \"BaseStrategy\" or \"BeamSearchStrategy\"`. ç”Ÿæˆçš„é‡‡æ ·ç­–ç•¥ã€‚\r\n  - å¯¹äº BeamSearchStrategyï¼ˆé›†æŸæœç´¢ï¼‰ï¼š\r\n     - `â€”-num-beams [int]`. é›†æŸæ•°ç›®ã€‚\r\n     - `â€”-length-penalty [float]`. ï¼ˆåŒ…æ‹¬è¾“å…¥å†…å®¹åœ¨å†…çš„ï¼‰ç”Ÿæˆé•¿åº¦æƒ©ç½šé¡¹ï¼›æ•°å€¼èŒƒå›´[0, 1]ï¼Œæ•°å€¼è¶Šå¤§ç”Ÿæˆé•¿åº¦è¶Šé•¿ã€‚\r\n     - `â€”-no-repeat-ngram-size [int]`. ç¦æ­¢é‡å¤ç”Ÿæˆçš„n-gramé•¿åº¦ã€‚\r\n     - `â€”-print-all-beam`. æ˜¯å¦æ‰“å°æ¯ä¸€æŸæœç´¢ç»“æœã€‚\r\n  - For BaseStrategy:\r\n     - `â€”-top-k [int]`. Top k é‡‡æ ·ã€‚\r\n     - `â€”-top-p [float]`. Top p é‡‡æ ·ã€‚\r\n     - `â€”-temperature [float]` . é‡‡æ ·æ—¶è®¾ç½®çš„æ¸©åº¦é¡¹ã€‚\r\n </details>\r\n\r\n### è¯„ä¼°\r\n\r\næˆ‘ä»¬ä½¿ç”¨YAMLæ–‡ä»¶æ¥å®šä¹‰ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œä½ å¯ä»¥ä¸€æ¬¡æ·»åŠ å¤šä¸ªä»»åŠ¡æˆ–æ–‡ä»¶å¤¹è¿›è¡Œè¯„ä¼°ï¼Œè¯„ä¼°è„šæœ¬ä¼šè‡ªåŠ¨é€’å½’åœ°æ”¶é›†è¿™äº›æ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰YAMLæ–‡ä»¶ã€‚\r\n\r\n```\r\nbash scripts/evaluate.sh task1.yaml task2.yaml dir1 dir2 ...\r\n```\r\n\r\n[ä»è¿™é‡Œ](https://cloud.tsinghua.edu.cn/f/9257ee84045644b8ac06/)ä¸‹è½½æˆ‘ä»¬çš„è¯„ä¼°æ•°æ®é›†ï¼Œå¹¶åœ¨ `scripts/evaluate.sh` ä¸­è®¾ç½® `DATA_PATH` ä¸ºä½ çš„æœ¬åœ°æ•°æ®é›†ç›®å½•ã€‚ä»»åŠ¡æ–‡ä»¶å¤¹åŒ…å«æˆ‘ä»¬ä¸º GLM-130B è¯„ä¼°çš„ 30 å¤šä¸ªä»»åŠ¡çš„ YAML æ–‡ä»¶ã€‚ä»¥ [CoLA](https://nyu-mll.github.io/CoLA/) ä»»åŠ¡ä¸ºä¾‹ï¼Œè¿è¡Œ `bash scripts/evaluate.sh tasks/bloom/glue_cola.yaml`ï¼Œå…¶è¾“å‡ºçš„æœ€ä½³æç¤ºå‡†ç¡®ç‡çº¦ä¸º 65%ï¼Œä¸­å€¼çº¦ä¸º 57%ã€‚\r\n\r\n<details>\r\n<summary>é¢„æœŸè¾“å‡º</summary>\r\n  \r\n```plain\r\nMultiChoiceTaskConfig(name='glue_cola', type=<TaskType.MULTICHOICE: 'mul'>, path='/thudm/LargeScale/data/zeroshot/bloom/glue_cola', module=None, metrics=['Accuracy'], use_task_mask=False, use_multitask_encoding=False, unidirectional=False, max_seq_length=2048, file_pattern={'validation': '**/validation.jsonl'}, micro_batch_size=8)\r\nEvaluating task glue_cola:\r\n  Evaluating group validation:\r\n      Finish Following_sentence_acceptable/mul/validation.jsonl, Accuracy = 42.665\r\n      Finish Make_sense_yes_no/mul/validation.jsonl, Accuracy = 56.951\r\n      Finish Previous_sentence_acceptable/mul/validation.jsonl, Accuracy = 65.197\r\n      Finish editing/mul/validation.jsonl, Accuracy = 57.622\r\n      Finish is_this_correct/mul/validation.jsonl, Accuracy = 65.197\r\nEvaluation results of task glue_cola:\r\n  Group validation Accuracy: max = 65.197, median = 57.622, average = 57.526\r\nFinish task glue_cola in 101.2s. \r\n```\r\n</details>\r\n\r\nå¯ä»¥é€šè¿‡åœ¨ `scripts/evaluate_multiple_node.sh` ä¸­è®¾ç½® `HOST_FILE_PATH`ï¼ˆ[DeepSpeed lanucher](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) è¦æ±‚ï¼‰æ¥é…ç½®å¤šèŠ‚ç‚¹è¯„ä¼°ã€‚åœ¨ `scripts/evaluate_multiple_node.sh` ä¸­è®¾ç½® `DATA_PATH` å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥è¯„ä¼°`./task`ç›®å½•ä¸­çš„æ‰€æœ‰ä»»åŠ¡ã€‚\r\n\r\n```\r\nbash scripts/evaluate_multiple_node.sh ./tasks\r\n```\r\n\r\nå…³äºå¦‚ä½•æ·»åŠ æ–°ä»»åŠ¡çš„ç»†èŠ‚ï¼Œè¯·å‚è§ [è¯„ä¼°ä½ è‡ªå·±çš„ä»»åŠ¡](docs/evaluate-your-own-tasks.md)ã€‚\r\n\r\n### ä½¿ç”¨ FasterTransformer åŠ é€Ÿæ¨ç†é€Ÿåº¦ï¼ˆé«˜è¾¾ 2.5 å€ï¼‰\r\n\r\n- é€šè¿‡å°† GLM-130B æ¨¡å‹ä¸ [FasterTransfomer](https://github.com/NVIDIA/FasterTransformer)ï¼ˆNVIDIA é«˜åº¦ä¼˜åŒ–çš„ Transformer æ¨¡å‹åº“ï¼‰ç›¸é€‚åº”ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç”Ÿæˆæ—¶è¾¾åˆ° 2.5 å€çš„é€Ÿåº¦ï¼Œè¯¦è§ [Inference with FasterTransformer](docs/inference-with-fastertransformer.md) ã€‚\r\n\r\n\r\n## ä½•ä¸ºGLM-130Bï¼Ÿ\r\n\r\nGLM-130Bæ˜¯ä¸€ä¸ªå¼€æ”¾çš„åŒè¯­ï¼ˆä¸­æ–‡ä¸è‹±æ–‡ï¼‰åŒå‘è¯­è¨€æ¨¡å‹ï¼Œå«1300äº¿ä¸ªå‚æ•°ã€‚æˆªè‡³2022å¹´7æœˆï¼Œå®ƒå·²ç»è®­ç»ƒäº†è¶…è¿‡4000äº¿ä¸ªæ–‡æœ¬æ ‡è®°ã€‚å®ƒçš„åº•å±‚æ¶æ„åŸºäº[é€šç”¨è¯­è¨€æ¨¡å‹(GLM)](https://aclanthology.org/2022.acl-long.26/)ï¼Œåœ¨è¯­è¨€ç†è§£å’Œè¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸Šå‡å±•ç¤ºå‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚\r\n\r\n### æ¶æ„\r\n\r\nGLM-130Bå°†BERTå’ŒGPTçš„ç›®æ ‡è¿›è¡Œäº†ç»Ÿä¸€ï¼Œå¹¶ä¸æœ€è¿‘æå‡ºçš„ä¸€äº›æŠ€æœ¯è¿›è¡Œç»“åˆä»¥æå‡è¯­è¨€æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚\r\n\r\n#### 1\\. è®­ç»ƒç›®æ ‡ï¼šè‡ªå›å½’æ–‡æœ¬å¡«ç©º\r\n\r\nGLMåˆ©ç”¨è‡ªå›å½’æ–‡æœ¬å¡«ç©ºä½œä¸ºå…¶ä¸»è¦çš„é¢„è®­ç»ƒç›®æ ‡ã€‚å®ƒæ©ç›–äº†éšæœºçš„è¿ç»­è·¨åº¦ï¼ˆä¾‹å¦‚ï¼Œä¸‹é¢çš„ä¾‹å­ä¸­çš„ \"complete unknown\"ï¼‰ï¼Œå¹¶å¯¹å…¶è¿›è¡Œè‡ªå›å½’é¢„æµ‹ã€‚ä¸Šä¸‹æ–‡ä¹‹é—´çš„æ³¨æ„åŠ›ï¼ˆä¾‹å¦‚ï¼Œ\"like a [MASK], like a rolling stone\"ï¼‰æ˜¯åŒå‘çš„ã€‚ç›¸åï¼Œè¢«æ©ç›–çš„æ ‡è®°ä¹‹é—´çš„æ³¨æ„åŠ›ï¼Œå’Œä»ä¸Šä¸‹æ–‡åˆ°è¢«æ©ç›–çš„æ ‡è¯†ç¬¦çš„æ³¨æ„åŠ›æ˜¯è‡ªå›å½’æ©ç çš„ã€‚\r\n\r\n\r\n\r\nåœ¨GLM-130Bçš„å®ç°ä¸­ï¼Œæœ‰ä¸¤ç§ä¸åŒçš„MASKæ ‡è¯†ç¬¦ï¼Œè¡¨ç¤ºä¸¤ä¸ªä¸åŒçš„ç›®çš„ï¼š\r\n\r\n* `[MASK]`æ ¹æ®[æ³Šæ¾åˆ†å¸ƒ](https://en.wikipedia.org/wiki/Poisson_distribution) (Î»=3)å¯¹è¾“å…¥ä¸­æ ‡è¯†ç¬¦è¿›è¡ŒçŸ­è·¨åº¦çš„é‡‡æ ·ï¼›\r\n* `[gMASK]`æ©ç›–ä¸€ä¸ªé•¿çš„è·¨åº¦ï¼Œä»å…¶ä½ç½®åˆ°æ•´ä¸ªæ–‡æœ¬çš„ç»“æŸã€‚\r\n\r\n`[sop]`æ ‡è¯†ç¬¦è¡¨ç¤ºä¸€ä¸ªç‰‡æ–­çš„å¼€å§‹ï¼Œ`[eop]`è¡¨ç¤ºä¸€ä¸ªç‰‡æ–­çš„ç»“æŸã€‚è¿™ä¸¤ä¸ªç›®æ ‡åœ¨GLM-130Bçš„é¢„è®­ç»ƒä¸­æ˜¯æ··åˆçš„ï¼Œåˆ†åˆ«å é¢„è®­ç»ƒæ ‡è®°çš„30%å’Œ70%ã€‚\r\n\r\n| <img src=\"resources/49BF334CB352BAA19F7D55460B1DBCA9.gif\" width=\"750px\"> | \r\n|:--:| \r\n| *ä¾‹å¦‚ï¼šGLM-130Bæ˜¯å¦‚ä½•å¯¹ `\"like a complete unknown, like a rolling stone\"`è¿›è¡Œé¢„è®­ç»ƒçš„* |\r\n\r\n#### 2\\. ä½ç½®ç¼–ç ï¼šæ—‹è½¬ä½ç½®ç¼–ç \r\n\r\nGLM-130Bä½¿ç”¨[æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰](https://arxiv.org/abs/2104.09864)ï¼Œè°·æ­Œçš„[PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)å’Œ[ElutherAI](https://www.eleuther.ai/)çš„GPT-*ç³»åˆ—ä¹Ÿé‡‡ç”¨è¿™ç§ç¼–ç ã€‚RoPEæ˜¯ä¸€ç§ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œå®ƒåˆ©ç”¨å¤æ•°ç©ºé—´çš„æ­£äº¤æŠ•å½±çŸ©é˜µæ¥è¡¨ç¤ºæ ‡è¯†ç¬¦çš„ç›¸å¯¹è·ç¦»ã€‚è¿˜æœ‰å…¶ä»–çš„ç›¸å¯¹ä½ç½®ç¼–ç é€‰é¡¹ï¼Œå¦‚Bigscienceçš„[BLOOM](https://huggingface.co/bigscience/bloom)æ‰€ä½¿ç”¨çš„[AliBi](https://arxiv.org/abs/2108.12409)ã€‚ä½†åœ¨æˆ‘ä»¬çš„åˆæ­¥å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°ã€‚\r\n\r\n* å½“åºåˆ—é•¿åº¦å¢é•¿æ—¶ï¼ŒRoPEçš„å®ç°é€Ÿåº¦æ›´å¿«ã€‚\r\n* RoPEå¯¹åŒå‘æ³¨æ„åŠ›æ›´å‹å¥½ï¼Œåœ¨ä¸‹æ¸¸å¾®è°ƒå®éªŒä¸­æ•ˆæœæ›´å¥½\r\n\r\nå› æ­¤ï¼Œå¯¹äºGLM-130Bï¼ŒRoPEæ˜¯ä¸€ç§æœ‰æ•ˆçš„ã€é«˜æ•ˆçš„ä½ç½®ç¼–ç ã€‚\r\n\r\n#### 3\\. å½’ä¸€åŒ–ï¼šä½¿ç”¨DeepNetçš„Post-LN\r\n\r\nå±‚å½’ä¸€åŒ–ï¼ˆLayerNormï¼Œæˆ–LNï¼‰æ˜¯transformerä¸­çš„ä¸€ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå…¶åº”ç”¨å¯ä»¥å¤§å¤§å½±å“è®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚BERTåº”ç”¨äº†Post-LNï¼Œè¿™æ„å‘³ç€LayerNormæ˜¯åœ¨æ·»åŠ æ®‹ä½™åˆ†æ”¯ååº”ç”¨çš„ã€‚ç„¶è€Œï¼Œ[åç»­å·¥ä½œ](https://arxiv.org/abs/2002.04745)è¡¨æ˜ï¼Œå•çº¯çš„Post-LNä¼šå¯¼è‡´é¢„è®­ç»ƒçš„ä¸ç¨³å®šï¼Œå› æ­¤ç°æœ‰çš„å¤§è§„æ¨¡æ¨¡å‹éƒ½é€‰æ‹©Pre-LNæ¶æ„ï¼Œå³åœ¨æ·»åŠ æ®‹å·®åˆ†æ”¯ä¹‹å‰åº”ç”¨LayerNormã€‚\r\n\r\n| <img src=\"resources/849024E93FA85347F7F6443932911922.png\" width=\"600px\"> | \r\n|:--:| \r\n| *(a) Post-LNåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ï¼›(b) Post-LN + DeepNorm æ¯” Sandwich-LN è¦æ›´åŠ ç¨³å®š* |\r\n\r\nå°½ç®¡å¦‚æ­¤ï¼Œåœ¨ç°æœ‰çš„å®è·µä¸­ï¼ŒPre-LNåœ¨ç”¨FP16è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹æ—¶ä»ç„¶å¯èƒ½ä¸ç¨³å®šã€‚[OPT-175B](https://arxiv.org/abs/2205.01068)åœ¨è®­ç»ƒå´©æºƒæ—¶æ‰‹åŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼›[BLOOM](https://huggingface.co/bigscience/bloom)ä½¿ç”¨BF16ï¼ˆä»…é€‚ç”¨äºNVIDIA Ampere GPUï¼šA100så’Œ3090sï¼‰ä»¥è·å¾—æ›´å¥½çš„æµ®ç‚¹ç²¾åº¦æ¥é¿å…å´©æºƒã€‚[CogView](https://proceedings.neurips.cc/paper/2021/file/a4d92e2cd541fca87e4620aba658316d-Paper.pdf)æå‡ºäº†Sandwich-LNä½œä¸ºä¸€ç§è¡¥æ•‘æªæ–½ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œ[è¿‘æœŸå·¥ä½œ](https://aclanthology.org/2021.findings-acl.81.pdf)è¡¨æ˜ï¼Œä¸Post-LNç›¸æ¯”ï¼ŒPre-LNçš„ä¸‹æ¸¸å¾®è°ƒæ€§èƒ½æ›´å·®ã€‚\r\n\r\nè€ƒè™‘åˆ°æ‰€æœ‰è¿™äº›å› ç´ ï¼Œåœ¨GLM-130Bä¸­ï¼Œæˆ‘ä»¬å†³å®šä½¿ç”¨Post-LNï¼Œå¹¶ä½¿ç”¨æ–°æå‡ºçš„[DeepNorm](https://arxiv.org/abs/2203.00555)æ¥å…‹æœä¸ç¨³å®šæ€§ã€‚DeepNormçš„é‡ç‚¹æ˜¯æ”¹è¿›åˆå§‹åŒ–ï¼Œå¯ä»¥å¸®åŠ©Post-LNå˜æ¢å™¨æ‰©å±•åˆ°1000å±‚ä»¥ä¸Šã€‚åœ¨æˆ‘ä»¬çš„åˆæ­¥å®éªŒä¸­ï¼Œæ¨¡å‹æ‰©å±•åˆ°130Bï¼ŒSandwich-LNçš„æ¢¯åº¦åœ¨å¤§çº¦2.5kæ­¥æ—¶å°±ä¼šå‡ºç°æŸå¤±çªå˜ï¼ˆå¯¼è‡´æŸå¤±å‘æ•£ï¼‰ï¼Œè€Œå¸¦æœ‰DeepNormçš„Post-Lnåˆ™ä¿æŒå¥åº·å¹¶å‘ˆç°å‡ºè¾ƒå°çš„æ¢¯åº¦å¤§å°ï¼ˆå³æ›´ç¨³å®šï¼‰ã€‚\r\n\r\n#### 4\\. å‰é¦ˆç½‘ç»œï¼šGated Linear Unit (GLU) + GeLU æ¿€æ´»\r\n\r\næœ€è¿‘ä¸€äº›æ”¹è¿›transformerç»“æ„çš„åŠªåŠ›é›†ä¸­åœ¨å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ä¸Šï¼ŒåŒ…æ‹¬ç”¨[GLU](https://arxiv.org/abs/1612.08083)ï¼ˆåœ¨PaLMä¸­é‡‡ç”¨ï¼‰å’Œæ–°æå‡ºçš„[é—¨æ§æ³¨æ„å•å…ƒï¼ˆGAUï¼‰](https://arxiv.org/abs/2202.10447)å–ä»£å®ƒã€‚\r\n\r\n|                              | RTE        | COPA       | BoolQ      | WSC        | Average |\r\n|------------------------------|------------|------------|------------|------------|---------|\r\n| GLM-base (GeGLU-Sandwich_LN) | 71.00Â±0.61 | 77.00Â±1.63 | 77.24Â±0.43 | 78.21Â±1.81 | 75.08   |\r\n| GLM-base (GAU-Pre_LN)        |            |            | _diverged_ |            |         |\r\n| GLM-base (GAU-Sandwich_LN)   | 69.92Â±0.61 | 75.67Â±0.94 | 77.00Â±0.15 | 72.44Â±1.81 | 74.20   |\r\n| GLN-base (FFN-Sandwich_LN)   | 71.00Â±0.74 | 72.33Â±1.70 | 76.75Â±0.05 | 73.72Â±2.40 | 73.36   |\r\n\r\næˆ‘ä»¬åœ¨åˆæ­¥å®éªŒä¸­é€šè¿‡å¯¹éšæœºçš„50Gä¸­è‹±æ–‡æ··åˆè¯­æ–™åº“è¿›è¡ŒGLM-baseï¼ˆ110Mï¼‰çš„é¢„è®­ç»ƒæ¥æµ‹è¯•å®ƒä»¬ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶GLUå’ŒGAUå¯ä»¥æ¯”åŸå§‹FFNå®ç°æ›´å¥½ï¼Œä½†GLUåœ¨è®­ç»ƒä¸­å¯ä»¥æ›´å¥½ã€æ›´ç¨³å®šã€‚\r\n\r\nå› æ­¤ï¼Œåœ¨GLM-130Bçš„å®ç°ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©å¸¦æœ‰GeLUæ¿€æ´»çš„GLUï¼Œå³GeGLUã€‚GeGLUéœ€è¦ä¸‰ä¸ªæŠ•å½±çŸ©é˜µï¼›ä¸ºäº†ä¿æŒç›¸åŒæ•°é‡çš„å‚æ•°ï¼Œä¸åªåˆ©ç”¨ä¸¤ä¸ªçŸ©é˜µçš„FFNç›¸æ¯”ï¼Œæˆ‘ä»¬å°†å…¶éšè—çŠ¶æ€å‡å°‘åˆ°2/3ã€‚\r\n\r\n#### æ€»ç»“\r\n\r\nåŸºäºä»¥ä¸Šæ‰€æœ‰è®¾è®¡ï¼ŒGLM-130Bçš„å‚æ•°é…ç½®ä¸ºï¼š\r\n\r\n|  å±‚æ•°  |   éšå±‚ç»´åº¦   |   GeGLU éšå±‚ç»´åº¦   |   æ³¨æ„åŠ›å¤´æ•°é‡  |     æœ€å¤§åºåˆ—é•¿åº¦    |  è¯è¡¨å¤§å°   |\r\n|--------|--------------|--------------------|-----------------|---------------------|-------------|\r\n| 70     | 12,288       | 32,768             | 96              | 2,048               | 150,000     |\r\n\r\nè¯¥è¯è¡¨å’Œåˆ†è¯å™¨æ˜¯åŸºäº[icetk](https://github.com/THUDM/icetk)å®ç°çš„ã€‚icetkæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å›¾åƒã€ä¸­æ–‡å’Œè‹±æ–‡çš„å¤šæ¨¡æ€æ ‡è®°å™¨ã€‚\r\n\r\n### è®­ç»ƒ\r\nè®­ç»ƒå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æœ€å…³é”®æŒ‘æˆ˜æ˜¯**è®­ç»ƒçš„ç¨³å®šæ€§**ï¼Œæ— ä¸€ä¾‹å¤–ã€‚GLM-130Bçš„é¢„è®­ç»ƒæŒç»­äº†60å¤©ï¼Œä½¿ç”¨96ä¸ªDGX-A100ï¼ˆ40Gï¼‰èŠ‚ç‚¹ï¼Œç­‰ä»·èŠ±è´¹490ä¸‡ç¾å…ƒçš„äº‘æœåŠ¡è´¹ç”¨ï¼›å¦‚æœè®­ç»ƒåœ¨åŠè·¯ä¸Šå¤±è´¥ï¼Œå¹¶æ— æ³•æ¢å¤è®­ç»ƒï¼Œé‚£å°†æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŸå¤±ã€‚\r\n\r\n| <img src=\"resources/E42321373D22DE198231279B5856BB42.png\" width=500px> | \r\n|:--:| \r\n| *æ‰€æœ‰æ¨¡å‹éƒ½é¢ä¸´è®­ç»ƒä¸ç¨³å®šï¼Œå®ƒå¯èƒ½å‘ç”Ÿåœ¨é¢„è®­ç»ƒçš„å¼€å§‹ã€ä¸­é—´æˆ–ç»“æŸé˜¶æ®µï¼ˆå›¾ï¼ˆaï¼‰å’Œï¼ˆbï¼‰åˆ†åˆ«å–è‡ªOPTå’ŒBLOOMï¼‰* | \r\n\r\nä¸å¹¸çš„æ˜¯ï¼Œæ®æˆ‘ä»¬è§‚å¯Ÿï¼Œå¤§æ¨¡å‹æ¯”æˆ‘ä»¬è®¤ä¸ºçš„é‚£äº›å°æ¨¡å‹æ›´å®¹æ˜“å—åˆ°ä¸å¯é¿å…çš„å™ªéŸ³æ•°æ®å’Œæ„å¤–æ¶Œç°çš„æ¢¯åº¦å½±å“ã€‚åŸå› æ˜¯ï¼Œåœ¨è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ä¹‹é—´å­˜åœ¨ç€æƒè¡¡ï¼š\r\n\r\n* **æ•ˆç‡**ï¼šæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä½ç²¾åº¦çš„æµ®ç‚¹æ ¼å¼ï¼ˆå¦‚FP16ï¼‰ï¼Œä»¥å‡å°‘å†…å­˜å’Œè®¡ç®—æˆæœ¬ï¼›\r\n* **ç¨³å®šæ€§**ï¼šä½ç²¾åº¦æµ®ç‚¹æ ¼å¼å®¹æ˜“å‡ºç°æº¢å‡ºå’Œä¸‹æº¢ã€‚\r\n\r\nè€Œä¸ºäº†å¹³è¡¡è¿™ä¸¤ä¸ªè¦ç´ ï¼Œæˆ‘ä»¬ä»¥åŠæœ€è¿‘çš„å¼€æ”¾æ€§å¤§å‹æ¨¡å‹ï¼ˆå¦‚[OPT-175B](https://arxiv.org/abs/2205.01068)ã€[BLOOM](https://huggingface.co/bigscience/bloom)ï¼‰éƒ½ä»˜å‡ºäº†å·¨å¤§çš„åŠªåŠ›æ¥å¯»æ‰¾è§£å†³æ–¹æ¡ˆã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬æå‡ºæˆ‘ä»¬çš„ç­”æ¡ˆã€‚\r\n\r\n#### 1\\. æµ®ç‚¹æ•°æ ¼å¼ï¼šFP16 æ··åˆç²¾åº¦\r\n\r\nFP16æ··åˆç²¾åº¦å·²ç»æˆä¸ºä¸»æµå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒæ¡†æ¶çš„é»˜è®¤é€‰é¡¹ï¼Œç”¨äºè®­ç»ƒåäº¿åˆ°ç™¾äº¿è§„æ¨¡çš„æ¨¡å‹ã€‚ä½†å…¶ä»å¤ªå®¹æ˜“é‡åˆ°ç²¾åº¦é—®é¢˜ã€‚ä½œä¸ºè¡¥æ•‘æªæ–½ï¼ŒNVIDIA Ampere GPUæä¾›äº†BF16æµ®ç‚¹æ ¼å¼ï¼ˆè¢«[BLOOM](https://huggingface.co/bigscience/bloom)é‡‡ç”¨ï¼‰æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼ŒBF16åœ¨å…¶ä»–å¹³å°ä¸Šä¸è¢«æ”¯æŒï¼Œè¿™å¤§å¤§ç¼©å°äº†å®ƒåœ¨æ›´å¹¿æ³›çš„åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚\r\n\r\nä¸ºäº†è®©æ›´å¤šå¼€å‘è€…ä½¿ç”¨ï¼ŒGLM-130Bä»ç„¶é€‰æ‹©FP16ä½œä¸ºå…¶è®­ç»ƒæµ®ç‚¹æ ¼å¼ã€‚åŒæ—¶ï¼Œè¿™æ„å‘³ç€GLM-130Bå°†é¢ä¸´ç€æ›´å¤šçš„ç¨³å®šæ€§æŒ‘æˆ˜ã€‚å¹¸è¿çš„æ˜¯ï¼Œç»è¿‡å¤šæ¬¡å°è¯•ï¼Œæˆ‘ä»¬å‘ç°ä»¥ä¸‹çš„è®­ç»ƒç­–ç•¥æœ€ç»ˆæœ‰åŠ©äºç¨³å®šGLM-130Bçš„è®­ç»ƒã€‚\r\n\r\n#### 2\\. åµŒå…¥å±‚ï¼šæ¢¯åº¦ç¼©å‡\r\n\r\næˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µï¼ŒåµŒå…¥å±‚çš„æ¢¯åº¦èŒƒæ•°æ˜æ˜¾æ¯”å…¶ä»–å±‚å¤§ã€‚æ ¹æ®ç»éªŒï¼Œæˆ‘ä»¬å‘ç°å¤§å¤šæ•°è®­ç»ƒå´©æºƒéƒ½å‘ç”Ÿåœ¨å…¶æ¢¯åº¦èŒƒæ•°æ¿€å¢ä¹‹åã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œ[BLOOM](https://huggingface.co/bigscience/bloom)æ±‡æŠ¥äº†ä½¿ç”¨[åµŒå…¥å½’ä¸€åŒ–](https://openreview.net/pdf?id=rI7BL3fHIZq)ï¼ˆæˆ‘ä»¬ä¹Ÿå‘ç°å®ƒèƒ½ç¨³å®šè®­ç»ƒï¼‰ï¼Œä½†åŒæ—¶ï¼Œå…¶ç‰ºç‰²äº†ç›¸å¯¹è¾ƒå¤§çš„ä¸‹æ¸¸æ€§èƒ½ã€‚\r\n\r\nç”±äºæ ¹æœ¬é—®é¢˜æ˜¯è¾“å…¥åµŒå…¥å±‚çš„æ€¥å‰§æ¢¯åº¦ï¼Œæˆ‘ä»¬å»ºè®®ç¼©å°è¾“å…¥åµŒå…¥å±‚çš„æ¢¯åº¦ã€‚å®ç°èµ·æ¥ç›¸å½“ç®€å•ã€‚\r\n\r\n```python\r\nword_embedding = word_embedding * Î± + word_embedding.detach() * (1 - Î±)\r\n```\r\n\r\nè¿™å°±æŠŠæ¢¯åº¦ç¼©å°åˆ°`Î±`ã€‚åœ¨æˆ‘ä»¬çš„å®è·µä¸­ï¼Œæˆ‘ä»¬å‘ç°`Î±=0.1`å¯¹GLM-130Bæ˜¯æœ€å¥½çš„ã€‚\r\n\r\n| ![EmbeddingShrink.png](resources/03DF31017FE184DB45D41DFFC6F80EF0.png) | \r\n|:--:| \r\n| *(a) åµŒå…¥å±‚çš„æ¢¯åº¦èŒƒæ•°åœ¨æ—©æœŸé˜¶æ®µæ¯”å…¶ä»–éƒ¨åˆ†å¤§å¾—å¤š <br> (b) åµŒå…¥æ¢¯åº¦ç¼©å‡çš„åˆæ­¥å®éªŒ (alpha=0.1)* | \r\n\r\nåœ¨æˆ‘ä»¬çš„åˆæ­¥å®éªŒä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå¯¹äºæ—©æœŸé˜¶æ®µçš„è®­ç»ƒæ¥è¯´ï¼Œç¼©å°åµŒå…¥æ¢¯åº¦å¹¶æ²¡æœ‰å‡ç¼“æ”¶æ•›é€Ÿåº¦ï¼›ç›¸åï¼Œæ²¡æœ‰ç¼©å°æ¢¯åº¦çš„æ¨¡å‹ä¼šå‡ºç°æ„å¤–çš„å°–å³°ï¼Œå¹¶åœ¨5kæ­¥å·¦å³å‡ºç°è®­ç»ƒå´©æºƒçš„æƒ…å†µã€‚\r\n\r\n#### 3\\. æ³¨æ„åŠ›è®¡ç®—ï¼šFP32 Softmax\r\n\r\næ¢¯åº¦æ”¶ç¼©æ˜¯ä¸€ç§é¿å…è®­ç»ƒå´©æºƒçš„äº‹åæŠ€æœ¯ã€‚ä»æœ¬è´¨ä¸Šè®²ï¼Œå´©æºƒæ˜¯ç”±å¼‚å¸¸çš„æŸå¤± \"æ¢¯åº¦\"å½¢æˆçš„ï¼Œè¦ä¹ˆæ˜¯ç”±äºå™ªå£°æ•°æ®ï¼Œè¦ä¹ˆæ˜¯æ­£å‘è®¡ç®—ä¸­çš„ç²¾åº¦ä¸Šæº¢æˆ–è€…ä¸‹æº¢ã€‚ \r\n\r\n| ![scale.png](resources/7CB441707D1035B2890AA2164C5B6EAC.png) | \r\n|:--:| \r\n| *æ¯ä¸ªæ³¨æ„åŠ›å¤´è®¡ç®—å‡ºçš„æ³¨æ„åŠ›å¾—åˆ†æœ‰éå¸¸ä¸åŒçš„æ•°å€¼èŒƒå›´ï¼ˆæ‘˜è‡ª[CogView](https://proceedings.neurips.cc/paper/2021/file/a4d92e2cd541fca87e4620aba658316d-Paper.pdf)ï¼‰* | \r\n\r\næˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ï¼Œæ³¨æ„åŠ›çš„è®¡ç®—æ“ä½œæ˜¯æœ€å®¹æ˜“ä¸Šæº¢æˆ–ä¸‹æº¢çš„ã€‚[CogView](https://proceedings.neurips.cc/paper/2021/file/a4d92e2cd541fca87e4620aba658316d-Paper.pdf)æ˜¾ç¤ºï¼Œä¸åŒçš„æ³¨æ„åŠ›å¤´å¯¹å…¶æ³¨æ„åŠ›åˆ†æ•°æœ‰éå¸¸ä¸åŒçš„æ•°å€¼èŒƒå›´ï¼Œæœ‰äº›æ³¨æ„åŠ›å¤´è®¡ç®—å‡ºçš„å¹³å‡åˆ†æ•°å¯ä»¥è¾¾åˆ°+1e4æˆ–-1e-3ã€‚è¿™ç§ä¸åŒçš„æ•°å€¼èŒƒå›´ä¼šå¯¼è‡´åœ¨softmaxè®¡ç®—ä¸­FP16ä¸‹çš„é¢‘ç¹ä¸Šæº¢æˆ–ä¸‹æº¢ã€‚CogViewæå‡ºäº†ç²¾åº¦ç“¶é¢ˆæ”¾æ¾ï¼ˆPB-Relaxï¼‰æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œå®ƒåœ¨åšsoftmaxä¹‹å‰æ‰£é™¤äº†æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›å¾—åˆ†çŸ©é˜µä¸­çš„æœ€å¤§ç»å¯¹å€¼ã€‚\r\n\r\nç„¶è€Œï¼Œäº‹å®è¯æ˜ï¼ŒPB-Relaxåœ¨GLM-130Bçš„è®­ç»ƒä¸­å¾ˆæ…¢ï¼Œå¯èƒ½æ˜¯å› ä¸ºåœ¨96ä¸ªå¤§å°ä¸º2048*2048çš„æ³¨æ„åˆ†æ•°çŸ©é˜µä¸­å¯»æ‰¾æœ€å¤§å€¼å’Œæ“ä½œæ ‡é‡å¯¹CUDAå†…æ ¸ä¸å‹å¥½ã€‚æœ€åï¼Œç»è¿‡å‡ å‘¨çš„è‰°è‹¦æ¢ç´¢ï¼Œæˆ‘ä»¬å‘ç°é¿å…è¿™ä¸€é—®é¢˜çš„æœ€å¿«å’Œæœ€ç®€å•çš„æ–¹æ³•æ˜¯åœ¨softmaxè®¡ç®—ä¸­ä½¿ç”¨FP32ã€‚ä¸å®Œå…¨çš„FP16è®¡ç®—ç›¸æ¯”ï¼Œå®ƒå‡ ä¹æ²¡æœ‰ä»»ä½•é€Ÿåº¦ä¸Šçš„æŸå¤±ï¼Œä½†æ˜æ˜¾æé«˜äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚\r\n\r\n<!--#### 4\\. 3D Parallel Training with Pipeline-->\r\n\r\n### é¢„è®­ç»ƒæ•°æ®\r\n\r\n#### è‡ªç›‘ç£é¢„è®­ç»ƒ\r\n\r\næˆ‘ä»¬åœ¨2.5Tç½‘ç»œçˆ¬å–çš„è¯­æ–™ä¸Šï¼Œå¯¹GLM-130Bè¿›è¡Œäº†é¢„è®­ç»ƒï¼ŒåŒ…æ‹¬è‹±æ–‡1.2Tæ¥è‡ªPileçš„è¯­æ–™å’Œ1.3Tä¸­æ–‡è¯­æ–™.\r\n\r\n#### å¤šä»»åŠ¡æŒ‡ä»¤é¢„è®­ç»ƒï¼ˆMulti-Task Instruction Pre-Trainingï¼ŒMIPï¼‰\r\n\r\nåŒæ—¶ï¼Œ[FLAN](https://arxiv.org/pdf/2109.01652.pdf)å’Œ[T0](https://arxiv.org/pdf/2110.08207.pdf)çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å¤šæç¤ºå¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒå¯ä»¥ä¿ƒè¿›æ›´å¥½çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ­£å¦‚[T5](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf?ref=https://githubhelp.com)å’Œ[ExT5](https://arxiv.org/pdf/2111.10952.pdf)æ‰€æŒ‡å‡ºçš„ï¼Œå°†å¤šä»»åŠ¡çš„ä¸‹æ¸¸æ•°æ®åˆå¹¶åˆ°é¢„è®­ç»ƒä¸­ï¼Œç”šè‡³æ¯”å¤šä»»åŠ¡å¾®è°ƒæ›´æœ‰å¸®åŠ©ã€‚\r\n\r\nå› æ­¤ï¼Œåœ¨GLM-130Bçš„é¢„è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬åŒ…æ‹¬äº†è®¸å¤šä»è‡ªç„¶è¯­è¨€ç†è§£åˆ°ç”Ÿæˆçš„æç¤ºæ•°æ®é›†ï¼Œä½œä¸ºè‡ªç›‘ç£é¢„è®­ç»ƒçš„è¡¥å……ã€‚æˆ‘ä»¬è®¾å®š95%çš„æ ‡è®°æ¥è‡ªè‡ªç›‘ç£çš„é¢„è®­ç»ƒè¯­æ–™ï¼Œ5%çš„è®­ç»ƒæ ‡è®°æ¥è‡ªMIPæ•°æ®é›†ã€‚è¿™äº›æ•°æ®é›†æ˜¯ä»[T0](https://arxiv.org/pdf/2110.08207.pdf)å’Œ[DeepStruct](https://arxiv.org/pdf/2205.10475.pdf)ä¸­æ”¶é›†å’Œè½¬æ¢çš„ã€‚æŒ‰ç…§T0çš„åšæ³•ï¼Œæ¯ä¸ªå¤šæç¤ºæ•°æ®é›†ä¸­çš„æ ·æœ¬éƒ½åº”è¢«æˆªæ–­åˆ°æœ€å¤§æ•°é‡ï¼ˆä¸€èˆ¬æ¥è¯´ï¼ŒT0æ•°æ®é›†ä¸º100kï¼ŒDeepStructæ•°æ®é›†ä¸º200kï¼‰ã€‚\r\n\r\nä¸å¹¸çš„æ˜¯ï¼Œç”±äºæ•°æ®å‡†å¤‡ä¸­çš„ä¸€ä¸ªé”™è¯¯ï¼Œåœ¨å‰20kä¸ªé¢„è®­ç»ƒæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬æ„å¤–åœ°åŒ…æ‹¬äº†T0++çš„æ‰€æœ‰æ•°æ®é›†ï¼ˆå…¶ä¸­åŒ…æ‹¬æœ€åˆç”¨äºè¯„ä¼°T0ä¸­é›¶æ ·æœ¬ä»»åŠ¡æ³›åŒ–çš„ä»»åŠ¡ï¼‰ã€æ²¡æœ‰è°ƒæˆæƒé‡è¿›è¡Œæˆªæ–­ã€å¹¶æ’é™¤äº†æ‰€æœ‰DeepStructæ•°æ®é›†ã€‚è™½ç„¶æˆ‘ä»¬æŠŠè¿™ä¸ªé—®é¢˜åœ¨20000æ­¥æ—¶è¿›è¡Œäº†ä¿®æ­£ï¼Œä½†GLM-130Bä¼¼ä¹å¯¹è®­ç»ƒæ ·æœ¬çš„è®°å¿†éå¸¸å¥½ï¼Œç›´åˆ°50000æ­¥ä¹Ÿæ²¡æœ‰å‡ºç°å¤§é‡é—å¿˜çš„ç°è±¡ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æ­¤æé†’æ‰€æœ‰ç”¨æˆ·***åˆ‡å‹¿åœ¨è¿™ä¸ª[åˆ—è¡¨](resources/multitask_list.txt)çš„æ•°æ®é›†ä¸Šè¯„ä¼°GLM-130Båœ¨é›¶æ ·æœ¬æˆ–å°‘æ ·æœ¬å­¦ä¹ çš„æ€§èƒ½ã€‚\r\n\r\n## GLM-130Bè¡¨ç°å¦‚ä½•ï¼Ÿ\r\n\r\nä¼—æ‰€å‘¨çŸ¥ï¼Œåƒ[GPT-3](https://arxiv.org/pdf/2005.14165.pdf)è¿™æ ·çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ˜¯ä¼˜ç§€çš„å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ å™¨ã€‚ä¸GPT-3å’ŒOPT-175Bçš„é›¶æ ·æœ¬å­¦ä¹ ç›¸æ¯”ï¼ŒGLM-130Bæœ‰ä¸€äº›æ¶æ„ä¸Šçš„åŠ£åŠ¿ã€‚é¦–å…ˆï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒè¯­è¯­è¨€æ¨¡å‹ï¼Œä¸èƒ½åƒGPT-3ï¼ˆ350B tokensï¼‰å’ŒOPT-175Bï¼ˆ350B tokensï¼‰é‚£æ ·çœ‹åˆ°å¾ˆå¤šè‹±è¯­æ ‡è®°ï¼ˆGLM-130Bå¤§æ¦‚è§åˆ°äº†200B è‹±æ–‡tokensï¼‰ã€‚ç¬¬äºŒï¼ŒGLM-130Bçš„å‚æ•°æ¯”GPT-3ï¼ˆ175Bï¼‰å’ŒOPT-175Bå°‘ã€‚\r\n\r\nå°½ç®¡æœ‰è¿™äº›ç¼ºç‚¹ï¼ŒGLM-130Bä»æœ‰ä¸Šè¿°çš„è®¸å¤šæŠ€æœ¯æ”¹è¿›ï¼Œè¿™å¯èƒ½ä¼šå¼¥è¡¥å…¶åœ¨é›¶ç‚¹å­¦ä¹ æ€§èƒ½æ–¹é¢çš„å·®è·ã€‚\r\n\r\n* **åŒå‘æ³¨æ„åŠ›**ã€‚GLM-130Bæ˜¯ä¸€ä¸ªç±»ä¼¼äºBERTçš„åŒå‘æ¨¡å‹ï¼Œè€Œç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦æ˜¯GPTï¼ˆå•å‘çš„ï¼‰ã€‚åŒå‘æ¨¡å‹åœ¨è¯­è¨€ç†è§£å’Œæ¡ä»¶ç”Ÿæˆæ–¹é¢è¿œè¿œä¼˜äºGPTã€‚\r\n* **æ”¹è¿›çš„æ¶æ„è®¾è®¡**ã€‚GLM-130Bé‡‡ç”¨äº†æ–°çš„æ¶æ„è®¾è®¡ï¼ŒåŒ…æ‹¬GeGLUã€RoPEå’ŒDeepNormã€‚è¿™äº›æŠ€æœ¯å·²è¢«è¯æ˜å¯ä»¥æé«˜è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ã€‚\r\n* **å¤šä»»åŠ¡æŒ‡ä»¤é¢„è®­ç»ƒ**ã€‚æ­£å¦‚[FLAN](https://arxiv.org/pdf/2109.01652.pdf)å’Œ[T0](https://arxiv.org/pdf/2110.08207.pdf)æ‰€æŒ‡å‡ºçš„ï¼Œå¤šä»»åŠ¡æŒ‡ä»¤é¢„è®­ç»ƒæœ‰åŠ©äºæé«˜é›¶æ ·æœ¬å­¦ä¹ æ€§èƒ½ã€‚\r\n\r\nä»ç›®å‰çš„ä¸­é—´ç»“æœæ¥çœ‹ï¼ŒGLM-130Båœ¨ä¸­æ–‡ä¸è‹±æ–‡ä¸­éƒ½æ˜¯ä¸€ä¸ªå¼ºå¤§çš„é›¶æ ·æœ¬å­¦ä¹ å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒçš„è¡¨ç°æ˜¯\r\n\r\n* åœ¨è‹±è¯­ä¸­ä¸GPT-3 175Bç›¸å½“ã€‚\r\n* åœ¨è‹±è¯­ä¸­ä¼˜äºBLOOM-176Bå’ŒOPT-175Bã€‚\r\n* åœ¨ä¸­æ–‡æ–¹é¢æ¯”ERNIE 3.0 Titanï¼ˆ260Bï¼‰æ›´å¥½ã€‚\r\n\r\n```diff\r\n- è¯·æ³¨æ„ï¼Œæœ¬èŠ‚ä¸­çš„æ‰€æœ‰ç»“æœç›®å‰éƒ½æ˜¯ä¸­é—´ç»“æœï¼Œä¸ä»£è¡¨æœ€ç»ˆæ€§èƒ½ã€‚\r\n```\r\n\r\n### è®¨è®ºï¼šGLM-130Bçš„é›¶æ ·æœ¬å­¦ä¹ è®¾ç½®\r\n\r\nç”±äºGLM-130Båˆ©ç”¨äº†å¤šä»»åŠ¡æŒ‡ä»¤é¢„è®­ç»ƒï¼ˆMIPï¼‰ï¼Œæˆ‘ä»¬è®¤ä¸ºæœ‰å¿…è¦æ¾„æ¸…æˆ‘ä»¬å¯¹é›¶æ ·æœ¬å­¦ä¹ çš„è®¾å®šã€‚è¯¥é—®é¢˜ä¼¼ä¹æ²¡æœ‰å®˜æ–¹è®¤å¯çš„å®šä¹‰ï¼Œè€Œç¤¾åŒºä¸­ä¹Ÿå­˜åœ¨è®¸å¤šä¸åŒçš„è§£é‡Šã€‚æˆ‘ä»¬å‚è€ƒäº†å½±å“åŠ›è¾ƒå¤§çš„é›¶æ ·æœ¬å­¦ä¹ [ç»¼è¿°](https://ieeexplore.ieee.org/abstract/document/8413121)ä¸­çš„å®šä¹‰ï¼Œå…¶æŒ‡å‡ºã€‚\r\n\r\n```\r\nAt test time, in zero-shot learning setting, the aim is to assign a test image to an unseen class label, and in generalized zero-shot learning setting, the test image can be assigned either to seen or unseen classes.\r\n```\r\n\r\nå…¶ä¸­ï¼Œè¢«è¯„ä¼°çš„ä»»åŠ¡æ˜¯å¦æ¶‰åŠæœªè§è¿‡çš„ç±»æ ‡ç­¾æ˜¯ä¸€ä¸ªå…³é”®ã€‚è€ƒè™‘åˆ°NLPçš„å®é™…æƒ…å†µï¼Œæˆ‘ä»¬ä¸ºGLM-130Bé›¶æ ·æœ¬å­¦ä¹ è¯„ä¼°æŒ‘é€‰æ•°æ®é›†çš„åŸåˆ™å¦‚ä¸‹ã€‚\r\n\r\n* è‹±æ–‡\r\n  + å¯¹äºæœ‰å›ºå®šæ ‡ç­¾çš„ä»»åŠ¡ï¼ˆå¦‚è‡ªç„¶è¯­è¨€æ¨ç†ï¼‰ï¼šåŒä¸€ä»»åŠ¡ä¸­çš„ä»»ä½•æ•°æ®é›†éƒ½ä¸åº”è¯¥è¢«è¯„ä¼°ã€‚\r\n  + å¯¹äºæ²¡æœ‰å›ºå®šæ ‡ç­¾çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œé—®é¢˜å›ç­”ï¼Œä¸»é¢˜åˆ†ç±»ï¼‰ï¼šåªåº”è€ƒè™‘ï¼š1)ç›¸æ¯”MIPä¸­æ•°æ®é›†å…·æœ‰æ˜æ˜¾çš„é¢†åŸŸè½¬ç§»ï¼Œä¸” 2)ä¸MIPä¸­çš„æ ‡ç­¾ä¸åŒçš„æ•°æ®é›†\r\n* ä¸­æ–‡ï¼šæ‰€æœ‰çš„æ•°æ®é›†éƒ½å¯ä»¥è¢«è¯„ä¼°\r\n\r\næˆ‘ä»¬æ¬¢è¿æ›´å¤šå…³äºè¿™ä¸ªè¯é¢˜çš„è®¨è®ºï¼Œä»¥ä¿ƒè¿›æ•´ä¸ªç¤¾åŒºå¯¹é›¶æ ·æœ¬å­¦ä¹ çš„ç ”ç©¶ã€‚\r\n\r\n### é›¶æ ·æœ¬å­¦ä¹ ï¼šè‹±æ–‡\r\n\r\næˆ‘ä»¬åœ¨å„ç§ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­æµ‹è¯•GLM-130Bã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä»åœ¨ç»å†è¯„ä¼°é˜¶æ®µï¼›è¿™äº›ç»“æœä¸æ˜¯æœ€ç»ˆç»“æœï¼Œè€Œæ˜¯**ä¸­é—´ç»“æœ**ã€‚\r\n\r\n#### è¯­è¨€å»ºæ¨¡ï¼ˆLAMBADAï¼‰\r\nè¯­è¨€å»ºæ¨¡æµ‹è¯•çš„æ˜¯è¯­è¨€æ¨¡å‹åœ¨ç»™å®šå…¶å‰ç¼€è¯­å¢ƒä¸‹é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯çš„å†…åœ¨èƒ½åŠ›ã€‚æˆ‘ä»¬ä»¥[LAMBADA](https://aclanthology.org/P16-1144/)ä¸ºä¾‹ï¼Œå®ƒæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„é›¶æ ·æœ¬æœ«ä½å•è¯é¢„æµ‹ä»»åŠ¡ï¼Œåœ¨è¯„ä¼°ç°æœ‰å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ—¶è¢«å¹¿æ³›é‡‡ç”¨ã€‚\r\n\r\næˆ‘ä»¬ç»˜åˆ¶äº†GLM-130Bçš„é›¶æ ·æœ¬LAMBADAï¼ˆEnï¼‰æ€§èƒ½ï¼Œä»¥åŠGPT-3 175Bã€OPT 175Bå’ŒBLOOM 176Bï¼ˆOPTå’ŒBLOOMçš„ä¸­é—´ç»“æœå–è‡ª[BLOOMçš„è¯„ä¼°åº“](https://github.com/bigscience-workshop/evaluation-results/tree/676f6a8cf27d4df30b073fb490deb9e359da64aa)ï¼‰ã€‚ä¸å…¶ä»–ä¸‰ä¸ªä½¿ç”¨ä¸Šä¸‹æ–‡è‡ªå›å½’çš„GPTå¼æ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬æå‡ºäº†GLM-130Bçš„ä¸¤ä¸ªç‰ˆæœ¬ã€‚\r\n\r\n* **GLM-130B (bi)**å¯¹å‰ç¼€ä¸Šä¸‹æ–‡æœ‰åŒå‘çš„å…³æ³¨ã€‚\r\n* **GLM-130B (uni)**éµå¾ªä¼ ç»Ÿçš„GPTé£æ ¼ï¼Œå¯¹å‰ç¼€è¯­å¢ƒè¿›è¡Œè‡ªå›å½’æ³¨æ„åŠ›ã€‚\r\n\r\nå¦‚å›¾æ‰€ç¤ºï¼ŒåŒå‘æ³¨æ„åŠ›å¯ä»¥ç”¨è¾ƒå°‘çš„æ¨¡å‹å‚æ•°è¾¾åˆ°æ›´å¥½çš„æ€§èƒ½ã€‚\r\n\r\n| <p style=\"text-align:center;\"><img src=\"resources/F48B69263360688CCA21E915F4B1A98B.png\" width=\"500px\"></p> | \r\n|:--:| \r\n| *ä¸å…¶ä»–å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒGLM-130Bçš„é›¶æ ·æœ¬ LAMBADAï¼ˆEnï¼‰æ€§èƒ½* | \r\n\r\n#### MMLUï¼ˆå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£)\r\n\r\n[MMLU](https://arxiv.org/pdf/2009.03300.pdf) æ˜¯ä¸€ä¸ªå¤šæ ·åŒ–çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…æ‹¬57ä¸ªå…³äºäººç±»çŸ¥è¯†çš„å¤šé€‰é¢˜å›ç­”ä»»åŠ¡ï¼ŒèŒƒå›´ä»é«˜ä¸­æ°´å¹³åˆ°ä¸“å®¶æ°´å¹³ã€‚å®ƒå¯ä»¥ä½œä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å°‘æ ·æœ¬å­¦ä¹ æ€§èƒ½çš„ç†æƒ³æµ‹è¯•å¹³å°ã€‚\r\n\r\næˆ‘ä»¬ç»˜åˆ¶äº†GLM-130Båœ¨å…¶è®­ç»ƒè¿‡ç¨‹ä¸Šçš„å°‘æ ·æœ¬å­¦ä¹ ï¼ˆ5-shotï¼‰æ€§èƒ½ã€‚GLM-130Båœ¨å­¦ä¹ äº†å¤§çº¦3000äº¿ä¸ªtokensåï¼Œæ¥è¿‘GPT-3çš„å¯æ¯”æ€§èƒ½43.9ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œå®ƒçš„èƒ½åŠ›ç»§ç»­å¢é•¿ï¼Œåœ¨å­¦ä¹ äº†4000äº¿ä¸ªtokensåè¾¾åˆ°äº†44.8ã€‚å½“æˆ‘ä»¬çš„è®­ç»ƒç»ˆæ­¢æ—¶ï¼Œå®ƒä¼¼ä¹å¹¶æ²¡æœ‰é¥±å’Œï¼Œè¿™ä¸[Chinchilla](https://arxiv.org/pdf/2203.15556.pdf)ä¸­çš„è§‚å¯Ÿç›¸ä¸€è‡´ï¼Œå³ç°æœ‰çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä»ç„¶è¿œè¿œæ²¡æœ‰å¾—åˆ°å……åˆ†çš„è®­ç»ƒã€‚\r\n\r\n| <p style=\"text-align:center;\"><img src=\"resources/33872E48D3539EA132B74BCF5EFF458F.png\" width=\"500px\"></p> | \r\n|:--:| \r\n| *ä¸å…¶ä»–å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒGLM-130Bçš„å°‘æ ·æœ¬å­¦ä¹ ï¼ˆ5-shotï¼‰MMLUæ€§èƒ½* |\r\n\r\n### é›¶æ ·æœ¬å­¦ä¹ ï¼šä¸­æ–‡\r\n\r\nç”±äºGLM-130Bæ˜¯ä¸€ä¸ªåŒè¯­è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬ä¹Ÿè¯„ä¼°äº†å®ƒåœ¨æ—¢æœ‰çš„ä¸­æ–‡NLPåŸºå‡†ä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½ï¼š[CLUE](https://arxiv.org/pdf/2004.05986.pdf) å’Œ[FewCLUE](https://arxiv.org/pdf/2107.07498.pdf)ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨å¤šä»»åŠ¡æŒ‡ä»¤é¢„è®­ç»ƒ(MIP)ä¸­ä¸åŒ…æ‹¬ä»»ä½•ä¸­æ–‡ä¸‹æ¸¸æ•°æ®é›†ã€‚ç”±äºä»åœ¨è¯„ä¼°é˜¶æ®µï¼Œæˆ‘ä»¬ç›®å‰ä»…è¯„ä¼°äº†7ä¸ªCLUEæ•°æ®é›†å’Œ5ä¸ªFewCLUEæ•°æ®é›†ã€‚æ›´å¤šæ•°æ®é›†ä¸Šçš„ç»“æœä¼šåœ¨ä¹‹åå…¬å¸ƒã€‚\r\n\r\næˆ‘ä»¬å°†GLM-130Bä¸ç°æœ‰æœ€å¤§çš„ä¸­æ–‡å•è¯­è¯­è¨€æ¨¡å‹ERNIE Titan 3.0è¿›è¡Œæ¯”è¾ƒï¼Œåè€…æœ‰260Bçš„å‚æ•°ã€‚å¦‚å›¾æ‰€ç¤ºï¼ŒGLM-130Bçš„è¡¨ç°ä¼˜äºERNIE Titan 3.0ï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿæˆå¼é˜…è¯»ç†è§£æ•°æ®é›†DRCDå’ŒCMRC2018ä¸Šã€‚\r\n\r\n| <img src=\"resources/AE18F14396E2D22BC0BC8DD77EFD3414.png\" width=\"500px\"> | \r\n|:--:| \r\n*éƒ¨åˆ†CLUEå’ŒFewCLUEåŸºå‡†æ•°æ®é›†çš„é›¶ç‚¹æ€§èƒ½ã€‚è·ŸéšERNIE Titan 3.0çš„åšæ³•ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†å¼€å‘æ•°æ®é›†çš„ç»“æœã€‚é™¤äº†DRCDå’ŒCMRC2018çš„æŠ¥å‘ŠEMå¤–ï¼Œå…¶ä»–æ•°æ®é›†æŠ¥å‘ŠAcc.* |\r\n\r\n<details>\r\n<summary><b>è‡´è°¢</b></summary>\r\n   \r\nè¿™ä¸€é¡¹ç›®ç”±å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘å›½å®¶æ°å‡ºé’å¹´ç§‘å­¦åŸºé‡‘é¡¹ç›®ï¼ˆNo. 61825602ï¼‰æ”¯æŒã€‚ \r\n\r\n### å­¦ç”Ÿè´Ÿè´£äºº\r\n[æ›¾å¥¥æ¶µï¼ˆæ¸…åå¤§å­¦è®¡ç®—æœºç³»çŸ¥è¯†å·¥ç¨‹å®éªŒå®¤ï¼‰](https://github.com/Sengxian)ï¼Œ[åˆ˜æ½‡ï¼ˆæ¸…åå¤§å­¦è®¡ç®—æœºç³»çŸ¥è¯†å·¥ç¨‹å®éªŒå®¤ï¼‰](https://github.com/xiao9905)\r\n\r\n### æŠ€æœ¯è´¡çŒ®\r\n#### æ¸…åå¤§å­¦è®¡ç®—æœºç³»çŸ¥è¯†å·¥ç¨‹å®éªŒå®¤â€”â€”the Knowledge Engineering Group at Tsinghua\r\næœæ”¿æ™“ï¼Œä¸é“­ï¼Œéƒ‘å‹¤é”´ï¼Œèµ–ç€šå®‡ï¼Œæ±ªå­æ¶µï¼Œæ¨å“æ¯…ï¼Œäºæµå‡¡ï¼Œå¼ ç¬‘æ¶µï¼Œéƒ‘é—®è¿ªï¼Œå¤ç®«ï¼Œå¾é€¸å‡¡ï¼Œè°­å’éœ–ï¼Œä¸œæ˜±æ™“ï¼Œå”æ°\r\n\r\n#### æ¸…åå¤§å­¦è®¡ç®—æœºç³»PACMANå®éªŒå®¤â€”â€”the Parallel Architecture & Compiler technology of Mobile, Accelerated, and Networked systems Group at Tsinghua\r\né©¬å­è½©ï¼Œä½•å®¶å‚²ï¼Œå­™æ¡¢æ³¢ï¼Œç¿Ÿå­£å†¬ï¼Œé™ˆæ–‡å…‰\r\n\r\n#### æ¸…åå¤§å­¦è®¡ç®—æœºç³»è‡ªç„¶è¯­è¨€å¤„ç†å®éªŒå®¤ï¼ˆBMInfï¼‰â€”â€”the Natural Language Processing Group at Tsinghua\r\næ›¾å›½æ´‹ï¼ŒéŸ©æ—­ï¼Œèµµå¨éœ–ï¼Œåˆ˜çŸ¥è¿œ\r\n\r\n#### æ™ºè°±AIâ€”â€”an AI startup that aims to teach machines to think like humans\r\nè–›å®‡é£ï¼Œç‹å±±ï¼Œé™•æ°æ‰ï¼Œå§œçš“ç€šï¼Œéƒ­æŒ¯é’¢ï¼Œå¼ é¹\r\n\r\n### è®¡ç®—èµ„æºèµåŠ©\r\næ™ºè°±AI\r\n\r\n### é¡¹ç›®æ€»è´Ÿè´£\r\n[å”æ°ï¼ˆæ¸…åå¤§å­¦è®¡ç®—æœºç³»çŸ¥è¯†å·¥ç¨‹å®éªŒå®¤ & åŒ—äº¬æ™ºæºäººå·¥æ™ºèƒ½ç ”ç©¶é™¢ï¼‰](http://keg.cs.tsinghua.edu.cn/jietang/)\r\n   \r\n</details>\r\n\r\n"
        },
        {
          "name": "benchmark.py",
          "type": "blob",
          "size": 0.8515625,
          "content": "import torch\nimport time\nfrom initialize import initialize, initialize_model_and_tokenizer\n\nif __name__ == \"__main__\":\n    args = initialize(extra_args_provider=lambda parser: None)\n    model, tokenizer = initialize_model_and_tokenizer(args)\n\n    for seq_len in [512, 1024, 2048]:\n        torch.distributed.barrier()\n        start = time.time()\n        with torch.no_grad():\n            _, *_ = model(\n                torch.ones(1, seq_len, device=torch.cuda.current_device(), dtype=torch.int64),\n                torch.arange(seq_len, device=torch.cuda.current_device(), dtype=torch.int64).view(1, -1),\n                torch.randn(1, 1, seq_len, seq_len, device=torch.cuda.current_device()) < 0.5,\n            )\n        torch.distributed.barrier()\n        if torch.distributed.get_rank() == 0:\n            print(f\"Encode {seq_len}: {(time.time() - start) * 1000:.2f} ms\")\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "cuda",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "evaluate.py",
          "type": "blob",
          "size": 2.4482421875,
          "content": "import time\nimport importlib\n\nfrom os.path import join, isdir, isfile, relpath\nfrom glob import glob\n\nfrom evaluation import BaseConfig, ModelForEvaluation, DEFAULT_CLASS, print_rank_0\nfrom initialize import initialize, initialize_model_and_tokenizer\n\n\ndef add_evaluation_specific_args(parser):\n    \"\"\"Arguments for evaluation\"\"\"\n    group = parser.add_argument_group(\"evaluation\", \"Evaluation configurations\")\n\n    # Task\n    group.add_argument(\"--task\", nargs=\"+\", default=[], help=\"All task config to evaluation\")\n    group.add_argument(\"--data-path\", type=str, required=True, help=\"Data dir path for all tasks\")\n    return parser\n\n\ndef find_all_tasks(all_task_config_path):\n    tasks = []\n    for task in all_task_config_path:\n        if isdir(task):\n            tasks += [relpath(path, \".\") for path in glob(join(task, \"**/*.yaml\"), recursive=True)]\n        elif isfile(task):\n            tasks.append(task)\n    return tasks\n\n\ndef evaluate_all_tasks(data_path, model, tokenizer, all_task_config_path, task_classes):\n    for config_path, task_class in zip(all_task_config_path, task_classes):\n        config = task_class.config_class().from_yaml_file(config_path)\n        config.path = join(data_path, config.path)\n        task = task_class(model, tokenizer, config)\n        task.evaluate()\n\n\ndef main():\n    args = initialize(extra_args_provider=add_evaluation_specific_args)\n    args.task = find_all_tasks(args.task)\n\n    task_classes = []\n    print_rank_0(\"> Loading task configs\")\n    for task_config_path in args.task:\n        config = BaseConfig.from_yaml_file(task_config_path)\n        if config.module:\n            path = \".\".join(config.module.split(\".\")[:-1])\n            module = importlib.import_module(path)\n            class_name = config.module.split(\".\")[-1]\n            task_class = getattr(module, class_name)\n            task_classes.append(task_class)\n        else:\n            task_classes.append(DEFAULT_CLASS[config.type])\n        print_rank_0(f\"    Task {config.name} loaded from config {task_config_path}\")\n    print_rank_0(f\"> Successfully load {len(task_classes)} task{'s' if len(task_classes) > 1 else ''}\")\n\n    model, tokenizer = initialize_model_and_tokenizer(args)\n    model = ModelForEvaluation(model)\n\n    start = time.time()\n    evaluate_all_tasks(args.data_path, model, tokenizer, args.task, task_classes)\n    print_rank_0(f\"Finish {len(task_classes)} task{'s' if len(task_classes) > 1 else ''} in {time.time() - start:.1f}s\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "evaluation",
          "type": "tree",
          "content": null
        },
        {
          "name": "generate.py",
          "type": "blob",
          "size": 7.875,
          "content": "import os\nimport torch\nimport stat\nimport re\n\nfrom functools import partial\nfrom typing import List, Tuple\n\nfrom SwissArmyTransformer import mpu\nfrom evaluation.model import batch_filling_sequence\nfrom generation import BeamSearchStrategy, BaseStrategy\nfrom SwissArmyTransformer.generation.utils import timed_name, generate_continually\nfrom initialize import initialize, initialize_model_and_tokenizer\n\n\ndef add_generation_specific_args(parser):\n    parser.add_argument(\"--sampling-strategy\", type=str, default=\"BaseStrategy\", help=\"Type of sampling strategy.\")\n    parser.add_argument(\"--min-gen-length\", type=int, default=0, help=\"The minimum length each blank should generate.\")\n    parser.add_argument(\n        \"--print-all-beams\", action=\"store_true\", help=\"Print all output generated by beam search strategy.\"\n    )\n\n\ndef isEnglish(s):\n    try:\n        s.encode(encoding=\"utf-8\").decode(\"ascii\")\n    except UnicodeDecodeError:\n        return False\n    else:\n        return True\n\n\ndef get_masks_and_position_ids(seq, mask_position, max_gen_length, gmask=False):\n    context_length = seq.shape[1]\n    tokens = torch.nn.functional.pad(seq, (0, max_gen_length), mode=\"constant\", value=-1)\n    attention_mask = torch.ones((1, tokens.shape[-1], tokens.shape[-1]), device=tokens.device)\n    attention_mask.tril_()\n    attention_mask[..., : context_length - 1] = 1\n    attention_mask.unsqueeze_(1)\n    attention_mask = (attention_mask < 0.5).bool()\n\n    position_ids = torch.arange(tokens.shape[-1], dtype=torch.long, device=tokens.device)\n    if not gmask:\n        position_ids[context_length - 1 :] = mask_position\n\n    position_ids = position_ids.unsqueeze(0)\n\n    return tokens, attention_mask, position_ids\n\n\ndef fill_blanks(raw_text: str, model, tokenizer, strategy) -> Tuple[List[str], List[str], List[List[str]]]:\n    # add MASK\n    generation_mask = \"[gMASK]\"\n    if \"[MASK]\" in raw_text:\n        generation_mask = \"[MASK]\"\n    elif \"[sMASK]\" in raw_text:\n        generation_mask = \"[sMASK]\"\n    use_gmask = \"[MASK]\" not in raw_text and \"[sMASK]\" not in raw_text\n\n    mask_pattern = r\"\\[[sg]?MASK\\]\"\n    text_list = re.split(mask_pattern, raw_text)\n    pattern_list = re.compile(mask_pattern).findall(raw_text)\n    seq = []\n    for i in range(len(pattern_list)):\n        pattern = pattern_list[i]\n        sub_text = text_list[i]\n        seq.extend(tokenizer.tokenize(sub_text))\n        seq.append(tokenizer.get_command(pattern))\n\n    seq.extend(tokenizer.tokenize(text_list[-1]))\n\n    if \"MASK]\" not in raw_text:\n        seq += [tokenizer.get_command(generation_mask)]\n        raw_text += \" \" + generation_mask\n    if not raw_text.endswith(\"MASK]\"):\n        seq = seq + [tokenizer.get_command(\"eos\")]\n    if mpu.get_model_parallel_rank() == 0:\n        print(\"\\nInput: {}\\n\".format(raw_text))\n    if len(seq) > args.max_sequence_length:\n        raise ValueError(\"text too long.\")\n\n    # generation\n    is_english = isEnglish(raw_text)\n    output_list = [seq]\n    num_output = args.num_beams if args.sampling_strategy == \"BeamSearchStrategy\" else 1\n    last_pos, answers, answers_with_style, blanks = (\n        [0] * num_output,\n        [\"\" for _ in range(num_output)],\n        [\"\" for _ in range(num_output)],\n        [[] for _ in range(num_output)],\n    )\n\n    # continually detect the first mark position\n    while True:\n        seq = output_list[0]\n        # detect mask position\n        mask_token = tokenizer.get_command(generation_mask)\n        if mask_token not in seq:\n            break\n        mask_position = seq.index(mask_token)\n\n        output_list = []\n\n        input_seq = torch.cuda.LongTensor(\n            [seq + [tokenizer.get_command(\"sop\")]],\n            device=args.device,\n        )\n        output, _ = batch_filling_sequence(\n            model,\n            input_seq,\n            torch.cuda.LongTensor([input_seq.shape[-1]], device=args.device),\n            strategy=strategy,\n            get_masks_and_position_ids=partial(\n                get_masks_and_position_ids,\n                mask_position=mask_position,\n                max_gen_length=args.out_seq_length - input_seq.shape[-1],\n                gmask=use_gmask,\n            ),\n        )\n        if isinstance(output, torch.Tensor):  # different strategies\n            output = output.tolist()\n        output = output[0]  # batch_size = 1\n        output_list.extend(output)\n\n        # clip -1s and fill back generated things into seq\n        for i in range(len(output_list)):\n            output = output_list[i].tolist() if isinstance(output_list[i], torch.Tensor) else output_list[i]\n            try:\n                unfinished = output.index(-1)\n            except ValueError:\n                unfinished = len(output)\n            if output[unfinished - 1] in strategy.end_tokens:\n                unfinished -= 1\n            bog = output.index(tokenizer.get_command(\"sop\"))\n\n            prefix = tokenizer.detokenize(output[last_pos[i] : mask_position])\n            blank = tokenizer.detokenize(output[bog + 1 : unfinished])\n            answers_with_style[i] += (\n                prefix\n                + (\" \" if is_english else \"\")\n                + (\"\\033[4m\" if use_gmask else \"\\x1b[0;32m\\033[4m\")\n                + blank\n                + (\"\\033[0m\" if use_gmask else \"\\033[0m\\x1b[0m\")\n                + (\" \" if is_english else \"\")\n            )\n            blanks[i].append(blank)\n            last_pos[i] = mask_position + unfinished - (bog + 1)\n            output_list[i] = output[:mask_position] + output[bog + 1 : unfinished] + output[mask_position + 1 : bog]\n\n    for i, output in enumerate(output_list):\n        if output[-1] == tokenizer.get_command(\"eos\"):\n            output = output[:-1]\n        answers_with_style[i] += tokenizer.detokenize(output[last_pos[i] :])\n        answers[i] = tokenizer.detokenize(output)\n\n    return answers, answers_with_style, blanks\n\n\ndef main(args):\n    model, tokenizer = initialize_model_and_tokenizer(args)\n\n    end_tokens = [tokenizer.get_command(\"eop\"), tokenizer.get_command(\"eos\")]\n\n    if args.sampling_strategy == \"BaseStrategy\":\n        strategy = BaseStrategy(\n            batch_size=1, temperature=args.temperature, top_k=args.top_k, top_p=args.top_p, end_tokens=end_tokens\n        )\n    elif args.sampling_strategy == \"BeamSearchStrategy\":\n        strategy = BeamSearchStrategy(\n            1,\n            args.num_beams,\n            length_penalty=args.length_penalty,\n            consider_end=True,\n            end_tokens=end_tokens,\n            no_repeat_ngram_size=args.no_repeat_ngram_size,\n            min_gen_length=args.min_gen_length,\n        )\n    else:\n        raise ValueError(f\"unknown strategy {args.sampling_strategy}\")\n\n    def process(raw_text):\n        if args.with_id:\n            query_id, raw_text = raw_text.split(\"\\t\")\n\n        answers, answers_with_style, blanks = fill_blanks(raw_text, model, tokenizer, strategy)\n\n        # save\n        if args.with_id:\n            full_path = os.path.join(args.output_path, query_id + \".txt\")\n        else:\n            prefix = raw_text.replace(\"/\", \"\")[:20]\n            full_path = timed_name(prefix, \".txt\", args.output_path)\n        if mpu.get_model_parallel_rank() == 0:\n            if args.print_all_beams and len(answers) > 1:\n                for idx, answer_with_style in enumerate(answers_with_style):\n                    print(f\"Output beam {idx}:\", answer_with_style)  # print the first.\n                    if len(answer_with_style) > 120:\n                        print(\"\")\n            else:\n                print(f\"Output:\", answers_with_style[0])  # print the first.\n            with open(full_path, \"w\", encoding=\"utf-8\") as fout:\n                for answer in answers:\n                    fout.write(answer + \"\\n\")\n\n            os.chmod(full_path, stat.S_IRWXO + stat.S_IRWXG + stat.S_IRWXU)\n\n    os.makedirs(args.output_path, exist_ok=True)\n    generate_continually(process, args.input_source)\n\n\nif __name__ == \"__main__\":\n    args = initialize(extra_args_provider=add_generation_specific_args)\n\n    with torch.no_grad():\n        main(args)\n"
        },
        {
          "name": "generation",
          "type": "tree",
          "content": null
        },
        {
          "name": "initialize.py",
          "type": "blob",
          "size": 4.095703125,
          "content": "import argparse\nimport torch\nimport time\n\nfrom quantization import quantize\n\nfrom SwissArmyTransformer import get_args, get_tokenizer\nfrom SwissArmyTransformer.arguments import initialize_distributed\nfrom SwissArmyTransformer.training import load_checkpoint\nfrom SwissArmyTransformer.model import GLM130B\nfrom SwissArmyTransformer.mpu import get_model_parallel_world_size, get_model_parallel_rank, get_model_parallel_group\n\n\ndef add_bminf_args(parser):\n    \"\"\"Arguments for BMInf\"\"\"\n    group = parser.add_argument_group(\"BMInf\")\n\n    group.add_argument(\"--bminf\", action=\"store_true\", help=\"Use BMInf to support low resource evaluation\")\n    group.add_argument(\"--bminf-memory-limit\", type=int, default=20, help=\"Max memory for model per GPU (in GB)\")\n    return parser\n\n\ndef add_quantization_args(parser):\n    group = parser.add_argument_group(\"Quantization\")\n\n    group.add_argument(\"--quantization-bit-width\", type=int, default=None)\n    group.add_argument(\"--from-quantized-checkpoint\", action=\"store_true\", help=\"Loading from a quantized checkpoint\")\n\n\ndef add_initialization_args(parser):\n    group = parser.add_argument_group(\"Initialization\")\n\n    group.add_argument(\n        \"--sequential-initialization\",\n        action=\"store_true\",\n        help=\"Initialize sequentially in tensor parallel group (reduce CPU RAM for initialization)\",\n    )\n\n\ndef initialize(extra_args_provider):\n    parser = argparse.ArgumentParser(add_help=False)\n    add_bminf_args(parser)\n    add_quantization_args(parser)\n    add_initialization_args(parser)\n    GLM130B.add_model_specific_args(parser)\n    extra_args_provider(parser)\n    known, args_list = parser.parse_known_args()\n    args = get_args(args_list)\n    args = argparse.Namespace(**vars(args), **vars(known))\n    args.do_train = False\n    initialize_distributed(args)\n    return args\n\n\ndef initialize_model_and_tokenizer(args):\n    tokenizer = get_tokenizer(args)\n\n    torch.distributed.barrier()\n    start = time.time()\n\n    for i in range(get_model_parallel_world_size()):\n        if get_model_parallel_rank() == i:\n            # Initialize model\n            model = GLM130B(args).half()\n\n            if args.from_quantized_checkpoint:\n                assert args.quantization_bit_width is not None\n                # Quantize model before moving to GPU\n                model = quantize(model, args.quantization_bit_width)\n\n            # Load checkpoint\n            load_checkpoint(model, args)\n\n            if args.quantization_bit_width is not None and not args.from_quantized_checkpoint:\n                # Quantize model before moving to GPU\n                model = quantize(model, args.quantization_bit_width)\n\n            if args.bminf:\n                import bminf\n\n                if torch.distributed.get_rank() == 0:\n                    print(f\"> BMInf activated, memory limit: {args.bminf_memory_limit} GB\")\n                with torch.cuda.device(args.device):\n                    model = bminf.wrapper(model, quantization=False, memory_limit=args.bminf_memory_limit << 30)\n            else:\n                model = model.to(args.device)\n        if args.sequential_initialization:\n            torch.distributed.barrier(group=get_model_parallel_group())\n\n    torch.distributed.barrier()\n    if torch.distributed.get_rank() == 0:\n        print(f\"> Model initialized in {time.time() - start:.1f}s\")\n\n    torch.cuda.empty_cache()\n    model.eval()\n\n    # generate rotary embedding cache\n    original_parallel_output = model.transformer.parallel_output\n    model.transformer.parallel_output = True\n    with torch.no_grad():\n        _, *_ = model(\n            torch.ones(1, args.max_sequence_length, device=torch.cuda.current_device(), dtype=torch.int64),\n            torch.arange(args.max_sequence_length, device=torch.cuda.current_device(), dtype=torch.int64).view(1, -1),\n            torch.randn(\n                1,\n                1,\n                args.max_sequence_length,\n                args.max_sequence_length,\n                device=torch.cuda.current_device(),\n            )\n            < 0.5,\n        )\n    model.transformer.parallel_output = original_parallel_output\n    torch.distributed.barrier()\n\n    return model, tokenizer\n"
        },
        {
          "name": "kernels",
          "type": "tree",
          "content": null
        },
        {
          "name": "logs",
          "type": "tree",
          "content": null
        },
        {
          "name": "quantization",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.078125,
          "content": "SwissArmyTransformer>=0.2.12,<0.3\nicetk\napex\nscipy\ndataclass_wizard\ncpm_kernels\n"
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "tasks",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}