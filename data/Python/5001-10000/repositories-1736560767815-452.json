{
  "metadata": {
    "timestamp": 1736560767815,
    "page": 452,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "InternLM/InternLM",
      "stars": 6628,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.5634765625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n*profiling_\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n*.out\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/en/_build/\ndocs/zh_cn/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n.vscode\n.idea\n.DS_Store\n\n# custom\n*.pkl\n*.pkl.json\n*.log.json\n*.trace.json\ndocs/modelzoo_statistics.md\nmmdet/.mim\nwork_dirs/\nlogs/\nckpts/\nbatchscript-*\ncompare_2_profiling_data/\nllm_logs/\naim_logs/\nnvmelogs/\nrun_backup/\nruns/\nruns_bak/\nLLM_ALERT\nsmall_demo/\n7b_llama_nopp/\n\n# Pytorch\n*.pth\n*.py~\n*.sh~\n\n# Core\ncore.*\n\n# Run\nllm_ckpts\nevents.*\nmemory_trace\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.1796875,
          "content": "repos:\n  - repo: https://github.com/PyCQA/flake8\n    rev: 5.0.4\n    hooks:\n      - id: flake8\n  - repo: https://github.com/PyCQA/isort\n    rev: 5.11.5\n    hooks:\n      - id: isort\n  - repo: https://github.com/pre-commit/mirrors-yapf\n    rev: v0.32.0\n    hooks:\n      - id: yapf\n  - repo: https://github.com/codespell-project/codespell\n    rev: v2.2.1\n    hooks:\n      - id: codespell\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.3.0\n    hooks:\n      - id: trailing-whitespace\n      - id: check-yaml\n      - id: end-of-file-fixer\n      - id: requirements-txt-fixer\n      - id: double-quote-string-fixer\n      - id: check-merge-conflict\n      - id: fix-encoding-pragma\n        args: [\"--remove\"]\n      - id: mixed-line-ending\n        args: [\"--fix=lf\"]\n  - repo: https://github.com/executablebooks/mdformat\n    rev: 0.7.9\n    hooks:\n      - id: mdformat\n        args: [\"--number\", \"--table-width\", \"200\"]\n        additional_dependencies:\n          - mdformat-openmmlab\n          - mdformat_frontmatter\n          - linkify-it-py\n  - repo: https://github.com/myint/docformatter\n    rev: v1.3.1\n    hooks:\n      - id: docformatter\n        args: [\"--in-place\", \"--wrap-descriptions\", \"79\"]\n"
        },
        {
          "name": ".pylintrc",
          "type": "blob",
          "size": 13.6318359375,
          "content": "# This Pylint rcfile contains a best-effort configuration to uphold the\n# best-practices and style described in the Google Python style guide:\n#   https://google.github.io/styleguide/pyguide.html\n#\n# Its canonical open-source location is:\n#   https://google.github.io/styleguide/pylintrc\n\n[MASTER]\n\n# Files or directories to be skipped. They should be base names, not paths.\nignore=third_party,storage\n\n# Files or directories matching the regex patterns are skipped. The regex\n# matches against base names, not paths.\nignore-patterns=\n\n# Pickle collected data for later comparisons.\npersistent=no\n\n# List of plugins (as comma separated values of python modules names) to load,\n# usually to register additional checkers.\nload-plugins=\n\n# Use multiple processes to speed up Pylint.\njobs=4\n\n# Allow loading of arbitrary C extensions. Extensions are imported into the\n# active Python interpreter and may run arbitrary code.\nunsafe-load-any-extension=no\n\n\n[MESSAGES CONTROL]\n\n# Only show warnings with the listed confidence levels. Leave empty to show\n# all. Valid levels: HIGH, INFERENCE, INFERENCE_FAILURE, UNDEFINED\nconfidence=\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once). See also the \"--disable\" option for examples.\n#enable=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifiers separated by comma (,) or put this\n# option multiple times (only on the command line, not in the configuration\n# file where it should appear only once).You can also use \"--disable=all\" to\n# disable everything first and then reenable specific checks. For example, if\n# you want to run only the similarities checker, you can use \"--disable=all\n# --enable=similarities\". If you want to run only the classes checker, but have\n# no Warning level messages displayed, use\"--disable=all --enable=classes\n# --disable=W\"\ndisable=abstract-method,\n        apply-builtin,\n        arguments-differ,\n        attribute-defined-outside-init,\n        backtick,\n        bad-option-value,\n        basestring-builtin,\n        buffer-builtin,\n        c-extension-no-member,\n        consider-using-enumerate,\n        cmp-builtin,\n        cmp-method,\n        coerce-builtin,\n        coerce-method,\n        delslice-method,\n        div-method,\n        duplicate-code,\n        eq-without-hash,\n        execfile-builtin,\n        file-builtin,\n        filter-builtin-not-iterating,\n        fixme,\n        getslice-method,\n        global-statement,\n        hex-method,\n        idiv-method,\n        implicit-str-concat,\n        import-error,\n        import-self,\n        import-star-module-level,\n        inconsistent-return-statements,\n        input-builtin,\n        intern-builtin,\n        invalid-str-codec,\n        locally-disabled,\n        long-builtin,\n        long-suffix,\n        map-builtin-not-iterating,\n        misplaced-comparison-constant,\n        missing-function-docstring,\n        metaclass-assignment,\n        next-method-called,\n        next-method-defined,\n        no-absolute-import,\n        no-else-break,\n        no-else-continue,\n        no-else-raise,\n        no-else-return,\n        no-init,  # added\n        no-member,\n        no-name-in-module,\n        no-self-use,\n        nonzero-method,\n        oct-method,\n        old-division,\n        old-ne-operator,\n        old-octal-literal,\n        old-raise-syntax,\n        parameter-unpacking,\n        print-statement,\n        raising-string,\n        range-builtin-not-iterating,\n        raw_input-builtin,\n        rdiv-method,\n        reduce-builtin,\n        relative-import,\n        reload-builtin,\n        round-builtin,\n        setslice-method,\n        signature-differs,\n        standarderror-builtin,\n        suppressed-message,\n        sys-max-int,\n        too-few-public-methods,\n        too-many-ancestors,\n        too-many-arguments,\n        too-many-boolean-expressions,\n        too-many-branches,\n        too-many-instance-attributes,\n        too-many-locals,\n        too-many-nested-blocks,\n        too-many-public-methods,\n        too-many-return-statements,\n        too-many-statements,\n        trailing-newlines,\n        unichr-builtin,\n        unicode-builtin,\n        unnecessary-pass,\n        unpacking-in-except,\n        useless-else-on-loop,\n        useless-object-inheritance,\n        useless-suppression,\n        using-cmp-argument,\n        wrong-import-order,\n        xrange-builtin,\n        zip-builtin-not-iterating,\n\n\n[REPORTS]\n\n# Set the output format. Available formats are text, parseable, colorized, msvs\n# (visual studio) and html. You can also give a reporter class, eg\n# mypackage.mymodule.MyReporterClass.\noutput-format=colorized\n\n# Tells whether to display a full report or only the messages\nreports=no\n\n# Python expression which should return a note less than 10 (10 is the highest\n# note). You have access to the variables errors warning, statement which\n# respectively contain the number of errors / warnings messages and the total\n# number of statements analyzed. This is used by the global evaluation report\n# (RP0004).\nevaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)\n\n# Template used to display messages. This is a python new-style format string\n# used to format the message information. See doc for all details\n#msg-template=\n\n\n[BASIC]\n\n# Good variable names which should always be accepted, separated by a comma\ngood-names=main,_\n\n# Bad variable names which should always be refused, separated by a comma\nbad-names=\n\n# Colon-delimited sets of names that determine each other's naming style when\n# the name regexes allow several styles.\nname-group=\n\n# Include a hint for the correct naming format with invalid-name\ninclude-naming-hint=no\n\n# List of decorators that produce properties, such as abc.abstractproperty. Add\n# to this list to register other decorators that produce valid properties.\nproperty-classes=abc.abstractproperty,cached_property.cached_property,cached_property.threaded_cached_property,cached_property.cached_property_with_ttl,cached_property.threaded_cached_property_with_ttl\n\n# Regular expression matching correct function names\nfunction-rgx=^(?:(?P<exempt>setUp|tearDown|setUpModule|tearDownModule)|(?P<camel_case>_?[A-Z][a-zA-Z0-9]*)|(?P<snake_case>_?[a-z][a-z0-9_]*))$\n\n# Regular expression matching correct variable names\nvariable-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct constant names\nconst-rgx=^(_?[A-Z][A-Z0-9_]*|__[a-z0-9_]+__|_?[a-z][a-z0-9_]*)$\n\n# Regular expression matching correct attribute names\nattr-rgx=^_{0,2}[a-z][a-z0-9_]*$\n\n# Regular expression matching correct argument names\nargument-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct class attribute names\nclass-attribute-rgx=^(_?[A-Z][A-Z0-9_]*|__[a-z0-9_]+__|_?[a-z][a-z0-9_]*)$\n\n# Regular expression matching correct inline iteration names\ninlinevar-rgx=^[a-z][a-z0-9_]*$\n\n# Regular expression matching correct class names\nclass-rgx=^_?[A-Z][a-zA-Z0-9]*$\n\n# Regular expression matching correct module names\nmodule-rgx=^(_?[a-z][a-z0-9_]*|__init__)$\n\n# Regular expression matching correct method names\nmethod-rgx=(?x)^(?:(?P<exempt>_[a-z0-9_]+__|runTest|setUp|tearDown|setUpTestCase|tearDownTestCase|setupSelf|tearDownClass|setUpClass|(test|assert)_*[A-Z0-9][a-zA-Z0-9_]*|next)|(?P<camel_case>_{0,2}[A-Z][a-zA-Z0-9_]*)|(?P<snake_case>_{0,2}[a-z][a-z0-9_]*))$\n\n# Regular expression which should only match function or class names that do\n# not require a docstring.\nno-docstring-rgx=(__.*__|main|test.*|.*test|.*Test)$\n\n# Minimum line length for functions/classes that require docstrings, shorter\n# ones are exempt.\ndocstring-min-length=10\n\n\n[TYPECHECK]\n\n# List of decorators that produce context managers, such as\n# contextlib.contextmanager. Add to this list to register other decorators that\n# produce valid context managers.\ncontextmanager-decorators=contextlib.contextmanager,contextlib2.contextmanager\n\n# Tells whether missing members accessed in mixin class should be ignored. A\n# mixin class is detected if its name ends with \"mixin\" (case insensitive).\nignore-mixin-members=yes\n\n# List of module names for which member attributes should not be checked\n# (useful for modules/projects where namespaces are manipulated during runtime\n# and thus existing member attributes cannot be deduced by static analysis. It\n# supports qualified module names, as well as Unix pattern matching.\nignored-modules=\n\n# List of class names for which member attributes should not be checked (useful\n# for classes with dynamically set attributes). This supports the use of\n# qualified names.\nignored-classes=optparse.Values,thread._local,_thread._local\n\n# List of members which are set dynamically and missed by pylint inference\n# system, and so shouldn't trigger E1101 when accessed. Python regular\n# expressions are accepted.\ngenerated-members=\n\n\n[FORMAT]\n\n# Maximum number of characters on a single line.\nmax-line-length=120\n\n# TODO(https://github.com/PyCQA/pylint/issues/3352): Direct pylint to exempt\n# lines made too long by directives to pytype.\n\n# Regexp for a line that is allowed to be longer than the limit.\nignore-long-lines=(?x)(\n  ^\\s*(\\#\\ )?<?https?://\\S+>?$|\n  ^\\s*(from\\s+\\S+\\s+)?import\\s+.+$)\n\n# Allow the body of an if to be on the same line as the test if there is no\n# else.\nsingle-line-if-stmt=yes\n\n# Maximum number of lines in a module\nmax-module-lines=99999\n\n# String used as indentation unit.  The internal Google style guide mandates 2\n# spaces.  Google's externaly-published style guide says 4, consistent with\n# PEP 8.  Here, we use 2 spaces, for conformity with many open-sourced Google\n# projects (like TensorFlow).\nindent-string='    '\n\n# Number of spaces of indent required inside a hanging  or continued line.\nindent-after-paren=4\n\n# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.\nexpected-line-ending-format=\n\n\n[MISCELLANEOUS]\n\n# List of note tags to take in consideration, separated by a comma.\nnotes=TODO\n\n\n[STRING]\n\n# This flag controls whether inconsistent-quotes generates a warning when the\n# character used as a quote delimiter is used inconsistently within a module.\ncheck-quote-consistency=yes\n\n\n[VARIABLES]\n\n# Tells whether we should check for unused import in __init__ files.\ninit-import=no\n\n# A regular expression matching the name of dummy variables (i.e. expectedly\n# not used).\ndummy-variables-rgx=^\\*{0,2}(_$|unused_|dummy_)\n\n# List of additional names supposed to be defined in builtins. Remember that\n# you should avoid to define new builtins when possible.\nadditional-builtins=\n\n# List of strings which can identify a callback function by name. A callback\n# name must start or end with one of those strings.\ncallbacks=cb_,_cb\n\n# List of qualified module names which can have objects that can redefine\n# builtins.\nredefining-builtins-modules=six,six.moves,past.builtins,future.builtins,functools\n\n\n[LOGGING]\n\n# Logging modules to check that the string format arguments are in logging\n# function parameter format\nlogging-modules=logging,absl.logging,tensorflow.io.logging\n\n\n[SIMILARITIES]\n\n# Minimum lines number of a similarity.\nmin-similarity-lines=4\n\n# Ignore comments when computing similarities.\nignore-comments=yes\n\n# Ignore docstrings when computing similarities.\nignore-docstrings=yes\n\n# Ignore imports when computing similarities.\nignore-imports=no\n\n\n[SPELLING]\n\n# Spelling dictionary name. Available dictionaries: none. To make it working\n# install python-enchant package.\nspelling-dict=\n\n# List of comma separated words that should not be checked.\nspelling-ignore-words=\n\n# A path to a file that contains private dictionary; one word per line.\nspelling-private-dict-file=\n\n# Tells whether to store unknown words to indicated private dictionary in\n# --spelling-private-dict-file option instead of raising a message.\nspelling-store-unknown-words=no\n\n\n[IMPORTS]\n\n# Deprecated modules which should not be used, separated by a comma\ndeprecated-modules=regsub,\n                   TERMIOS,\n                   Bastion,\n                   rexec,\n                   sets\n\n# Create a graph of every (i.e. internal and external) dependencies in the\n# given file (report RP0402 must not be disabled)\nimport-graph=\n\n# Create a graph of external dependencies in the given file (report RP0402 must\n# not be disabled)\next-import-graph=\n\n# Create a graph of internal dependencies in the given file (report RP0402 must\n# not be disabled)\nint-import-graph=\n\n# Force import order to recognize a module as part of the standard\n# compatibility libraries.\nknown-standard-library=\n\n# Force import order to recognize a module as part of a third party library.\nknown-third-party=enchant, absl\n\n# Analyse import fallback blocks. This can be used to support both Python 2 and\n# 3 compatible code, which means that the block might have code that exists\n# only in one or another interpreter, leading to false positives when analysed.\nanalyse-fallback-blocks=no\n\n\n[CLASSES]\n\n# List of method names used to declare (i.e. assign) instance attributes.\ndefining-attr-methods=__init__,\n                      __new__,\n                      setUp\n\n# List of member names, which should be excluded from the protected access\n# warning.\nexclude-protected=_asdict,\n                  _fields,\n                  _replace,\n                  _source,\n                  _make\n\n# List of valid names for the first argument in a class method.\nvalid-classmethod-first-arg=cls,\n                            class_\n\n# List of valid names for the first argument in a metaclass class method.\nvalid-metaclass-classmethod-first-arg=mcs\n\n\n[EXCEPTIONS]\n\n# Exceptions that will emit a warning when being caught. Defaults to\n# \"Exception\"\novergeneral-exceptions=builtins.BaseException,\n                       builtins.Exception\n"
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 0.71484375,
          "content": "# .readthedocs.yaml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Set the OS, Python version and other tools you might need\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.8\"\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n  configuration: doc/code-docs/source/conf.py\n  fail_on_warning: false\n\n# Optionally build your docs in additional formats such as PDF\nformats:\n  - pdf\n\n# Optional but recommended, declare the Python requirements required\n# to build your documentation\n# See https://docs.readthedocs.io/en/stable/guides/reproducible-builds.html\npython:\n   install:\n   - requirements: doc/code-docs/requirements.txt\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 13.3359375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023-2024 Shanghai AI Laboratory\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n## Some of InternLM's code is derived from others projects, which is subject to the following copyright notice:\n\nCopyright 2021- HPC-AI Technology Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n---------------- LICENSE FOR Flash Attention ----------------\n\nBSD 3-Clause License\n\nCopyright (c) 2022, the respective contributors, as shown by the AUTHORS file.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 31.9423828125,
          "content": "# InternLM\n\n<div align=\"center\">\n\n<img src=\"./assets/logo.svg\" width=\"200\"/>\n  <div>Â </div>\n  <div align=\"center\">\n    <b><font size=\"5\">InternLM</font></b>\n    <sup>\n      <a href=\"https://internlm.intern-ai.org.cn/\">\n        <i><font size=\"4\">HOT</font></i>\n      </a>\n    </sup>\n    <div>Â </div>\n  </div>\n\n[![license](./assets/license.svg)](./LICENSE)\n[![evaluation](./assets/compass_support.svg)](https://github.com/internLM/OpenCompass/)\n\n<!-- [![Documentation Status](https://readthedocs.org/projects/internlm/badge/?version=latest)](https://internlm.readthedocs.io/zh_CN/latest/?badge=latest) -->\n\n[ðŸ“˜Commercial Application](#license) |\n[ðŸ¤—HuggingFace](https://huggingface.co/internlm) |\n[ðŸ†•Update News](#news) |\n[ðŸ¤”Reporting Issues](https://github.com/InternLM/InternLM/issues/new) |\n[ðŸ“œTechnical Report](https://arxiv.org/abs/2403.17297)<br>\n[ðŸ’¬Chat Web](https://internlm-chat.intern-ai.org.cn/) |\n[ðŸ”—API](https://internlm.intern-ai.org.cn/api/document) |\n[ðŸ§©Modelers](https://modelers.cn/spaces/MindSpore-Lab/INTERNLM2-20B-PLAN)\n\n[English](./README.md) |\n[ç®€ä½“ä¸­æ–‡](./README_zh-CN.md)\n\n</div>\n\n<p align=\"center\">\n    ðŸ‘‹ join us on <a href=\"https://discord.gg/xa29JuW87d\" target=\"_blank\">Discord</a> and <a href=\"https://github.com/InternLM/InternLM/assets/25839884/a6aad896-7232-4220-ac84-9e070c2633ce\" target=\"_blank\">WeChat</a>\n</p>\n\n## Introduction\n\nInternLM2.5 series are released with the following features:\n\n- **Outstanding reasoning capability**: State-of-the-art performance on Math reasoning, surpassing models like Llama3 and Gemma2-9B.\n\n- **1M Context window**: Nearly perfect at finding needles in the haystack with 1M-long context, with leading performance on long-context tasks like LongBench. Try it with [LMDeploy](./chat/lmdeploy.md) for 1M-context inference. More details and a file chat demo are found [here](./long_context/README.md).\n\n- **Stronger tool use**: InternLM2.5 supports gathering information from more than 100 web pages, corresponding implementation will be released in [Lagent](https://github.com/InternLM/lagent/tree/main) soon. InternLM2.5 has better tool utilization-related capabilities in instruction following, tool selection and reflection. See [examples](./agent/).\n\n## News\n\n\\[2024.08.01\\] We release InternLM2.5-1.8B, InternLM2.5-1.8B-Chat, InternLM2.5-20B and InternLM2.5-20B-Chat. See [model zoo below](#model-zoo) for download or [model cards](./model_cards/) for more details.\n\n\\[2024.07.19\\] We release the InternLM2-Reward series of reward models in 1.8B, 7B and 20B sizes. See [model zoo below](#model-zoo) for download or [model cards](./model_cards/internlm2_reward.md) for more details.\n\n\\[2024.07.03\\] We release InternLM2.5-7B, InternLM2.5-7B-Chat and InternLM2.5-7B-Chat-1M. See [model zoo below](#model-zoo) for download or [model cards](./model_cards/) for more details.\n\n\\[2024.03.26\\] We release InternLM2 technical report. See [arXiv](https://arxiv.org/abs/2403.17297) for details.\n\n\\[2024.01.31\\] We release InternLM2-1.8B, along with the associated chat model. They provide a cheaper deployment option while maintaining leading performance.\n\n\\[2024.01.23\\] We release InternLM2-Math-7B and InternLM2-Math-20B with pretraining and SFT checkpoints. They surpass ChatGPT with small sizes. See [InternLM-Math](https://github.com/InternLM/internlm-math) for details and download.\n\n\\[2024.01.17\\] We release InternLM2-7B and InternLM2-20B and their corresponding chat models with stronger capabilities in all dimensions. See [model zoo below](#model-zoo) for download or [model cards](./model_cards/) for more details.\n\n\\[2023.12.13\\] InternLM-7B-Chat and InternLM-20B-Chat checkpoints are updated. With an improved finetuning strategy, the new chat models can generate higher quality responses with greater stylistic diversity.\n\n\\[2023.09.20\\] InternLM-20B is released with base and chat versions.\n\n## Model Zoo\n\n### InternLM2.5\n\n| Model                      | Transformers(HF)                           | ModelScope(HF)                           | OpenXLab(HF)                           | OpenXLab(Origin)                           | Release Date |\n| -------------------------- | ------------------------------------------ | ---------------------------------------- | -------------------------------------- | ------------------------------------------ | ------------ |\n| **InternLM2.5-1.8B**       | [ðŸ¤—internlm2_5-1_8b](https://huggingface.co/internlm/internlm2_5-1_8b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-1_8b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-1_8b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-1_8b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-1_8b-original) | 2024-08-05   |\n| **InternLM2.5-1.8B-Chat**  | [ðŸ¤—internlm2_5-1_8b-chat](https://huggingface.co/internlm/internlm2_5-1_8b-chat) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-1_8b-chat](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-1_8b-chat/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-1_8b-chat) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-1_8b-chat-original) | 2024-08-05   |\n| **InternLM2.5-7B**         | [ðŸ¤—internlm2_5-7b](https://huggingface.co/internlm/internlm2_5-7b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-7b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b-original) | 2024-07-03   |\n| **InternLM2.5-7B-Chat**    | [ðŸ¤—internlm2_5-7b-chat](https://huggingface.co/internlm/internlm2_5-7b-chat) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-7b-chat](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-7b-chat/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b-chat) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b-chat-original) | 2024-07-03   |\n| **InternLM2.5-7B-Chat-1M** | [ðŸ¤—internlm2_5-7b-chat-1m](https://huggingface.co/internlm/internlm2_5-7b-chat-1m) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-7b-chat-1m](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-7b-chat-1m/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b-chat-1m) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b-chat-1m-original) | 2024-07-03   |\n| **InternLM2.5-20B**        | [ðŸ¤—internlm2_5-20b](https://huggingface.co/internlm/internlm2_5-20b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-20b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-20b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-20b-original) | 2024-08-05   |\n| **InternLM2.5-20B-Chat**   | [ðŸ¤—internlm2_5-20b-chat](https://huggingface.co/internlm/internlm2_5-20b-chat) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-20b-chat](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-20b-chat/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-20b-chat) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-20b-chat-original) | 2024-08-05   |\n\n**Notes:**\n\nThe release of InternLM2.5 series contains 1.8B, 7B, and 20B versions. 7B models are efficient for research and application and 20B models are more powerful and can support more complex scenarios. The relation of these models are shown as follows.\n\n1. **InternLM2.5**: Foundation models pre-trained on large-scale corpus. InternLM2.5 models are recommended for consideration in most applications.\n2. **InternLM2.5-Chat**: The Chat model that undergoes supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), based on the InternLM2.5 model. InternLM2.5-Chat is optimized for instruction following, chat experience, and function call, which is recommended for downstream applications.\n3. **InternLM2.5-Chat-1M**: InternLM2.5-Chat-1M supports 1M long-context with compatible performance as InternLM2.5-Chat.\n\n**Limitations:** Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.\n\n**Supplements:** `HF` refers to the format used by HuggingFace in [transformers](https://github.com/huggingface/transformers), whereas `Origin` denotes the format adopted by the InternLM team in [InternEvo](https://github.com/InternLM/InternEvo).\n\n### InternLM2-Reward\n\nInternLM2-Reward is a series of reward models, trained on 2.4 million preference samples, available in 1.8B, 7B, and 20B sizes. These model were applied to the PPO training process of our chat models. See [model cards](./model_cards/internlm2_reward.md) for more details.\n\n| Model                     | RewardBench Score | Transformers(HF)                                   | ModelScope(HF)                                    | OpenXLab(HF)                                    | Release Date |\n| ------------------------- | ----------------- | -------------------------------------------------- | ------------------------------------------------- | ----------------------------------------------- | ------------ |\n| **InternLM2-1.8B-Reward** | 80.6              | [ðŸ¤—internlm2-1_8b-reward](https://huggingface.co/internlm/internlm2-1_8b-reward) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-1_8b-reward](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-1_8b-reward/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-1_8b-reward) | 2024-07-19   |\n| **InternLM2-7B-Reward**   | 86.6              | [ðŸ¤—internlm2-7b-reward](https://huggingface.co/internlm/internlm2-7b-reward) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-7b-reward](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-7b-reward/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-7b-reward) | 2024-07-19   |\n| **InternLM2-20B-Reward**  | 89.5              | [ðŸ¤—internlm2-20b-reward](https://huggingface.co/internlm/internlm2-20b-reward) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-20b-reward](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-20b-reward/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-20b-reward) | 2024-07-19   |\n\n### InternLM2\n\n<details>\n    <summary>(click to expand)</summary>\n\nOur previous generation models with advanced capabilities in long-context processing, reasoning, and coding. See [model cards](./model_cards/) for more details.\n\n| Model                       | Transformers(HF)                          | ModelScope(HF)                           | OpenXLab(HF)                           | OpenXLab(Origin)                           | Release Date |\n| --------------------------- | ----------------------------------------- | ---------------------------------------- | -------------------------------------- | ------------------------------------------ | ------------ |\n| **InternLM2-1.8B**          | [ðŸ¤—internlm2-1.8b](https://huggingface.co/internlm/internlm2-1_8b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-1.8b](https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-1_8b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-1.8b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-1.8b-original) | 2024-01-31   |\n| **InternLM2-Chat-1.8B-SFT** | [ðŸ¤—internlm2-chat-1.8b-sft](https://huggingface.co/internlm/internlm2-chat-1_8b-sft) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-chat-1.8b-sft](https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-1_8b-sft/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-1.8b-sft) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-1.8b-sft-original) | 2024-01-31   |\n| **InternLM2-Chat-1.8B**     | [ðŸ¤—internlm2-chat-1.8b](https://huggingface.co/internlm/internlm2-chat-1_8b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-chat-1.8b](https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-1_8b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-1.8b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-1.8b-original) | 2024-02-19   |\n| **InternLM2-Base-7B**       | [ðŸ¤—internlm2-base-7b](https://huggingface.co/internlm/internlm2-base-7b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-base-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-7b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-7b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-7b-original) | 2024-01-17   |\n| **InternLM2-7B**            | [ðŸ¤—internlm2-7b](https://huggingface.co/internlm/internlm2-7b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-7b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-7b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-7b-original) | 2024-01-17   |\n| **InternLM2-Chat-7B-SFT**   | [ðŸ¤—internlm2-chat-7b-sft](https://huggingface.co/internlm/internlm2-chat-7b-sft) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-chat-7b-sft](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b-sft/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b-sft) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b-sft-original) | 2024-01-17   |\n| **InternLM2-Chat-7B**       | [ðŸ¤—internlm2-chat-7b](https://huggingface.co/internlm/internlm2-chat-7b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-chat-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b-original) | 2024-01-17   |\n| **InternLM2-Base-20B**      | [ðŸ¤—internlm2-base-20b](https://huggingface.co/internlm/internlm2-base-20b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-base-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-20b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-20b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-20b-original) | 2024-01-17   |\n| **InternLM2-20B**           | [ðŸ¤—internlm2-20b](https://huggingface.co/internlm/internlm2-20b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-20b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-20b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-20b-original) | 2024-01-17   |\n| **InternLM2-Chat-20B-SFT**  | [ðŸ¤—internlm2-chat-20b-sft](https://huggingface.co/internlm/internlm2-chat-20b-sft) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-chat-20b-sft](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b-sft/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b-sft) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b-sft-original) | 2024-01-17   |\n| **InternLM2-Chat-20B**      | [ðŸ¤—internlm2-chat-20b](https://huggingface.co/internlm/internlm2-chat-20b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-chat-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b-original) | 2024-01-17   |\n\n</details>\n\n## Performance\n\nWe have evaluated InternLM2.5 on several important benchmarks using the open-source evaluation tool [OpenCompass](https://github.com/open-compass/opencompass). Some of the evaluation results are shown in the table below. You are welcome to visit the [OpenCompass Leaderboard](https://rank.opencompass.org.cn) for more evaluation results.\n\n### Base Model\n\n| Benchmark      | InternLM2.5-7B | Llama3-8B | Yi-1.5-9B |\n| -------------- | -------------- | --------- | --------- |\n| MMLU (5-shot)  | **71.6**       | 66.4      | 71.6      |\n| CMMLU (5-shot) | **79.1**       | 51.0      | 74.1      |\n| BBH (3-shot)   | 70.1           | 59.7      | 71.1      |\n| MATH (4-shot)  | **34.0**       | 16.4      | 31.9      |\n| GSM8K (4-shot) | **74.8**       | 54.3      | 74.5      |\n| GPQA (0-shot)  | **31.3**       | 31.3      | 27.8      |\n\n### Chat Model\n\n| Benchmark          | InternLM2.5-7B-Chat | Llama3-8B-Instruct | Gemma2-9B-IT | Yi-1.5-9B-Chat | GLM-4-9B-Chat | Qwen2-7B-Instruct |\n| ------------------ | ------------------- | ------------------ | ------------ | -------------- | ------------- | ----------------- |\n| MMLU (5-shot)      | **72.8**            | 68.4               | 70.9         | 71.0           | 71.4          | 70.8              |\n| CMMLU (5-shot)     | 78.0                | 53.3               | 60.3         | 74.5           | 74.5          | 80.9              |\n| BBH (3-shot CoT)   | **71.6**            | 54.4               | 68.2\\*       | 69.6           | 69.6          | 65.0              |\n| MATH (0-shot CoT)  | **60.1**            | 27.9               | 46.9         | 51.1           | 51.1          | 48.6              |\n| GSM8K (0-shot CoT) | 86.0                | 72.9               | 88.9         | 80.1           | 85.3          | 82.9              |\n| GPQA (0-shot)      | **38.4**            | 26.1               | 33.8         | 37.9           | 36.9          | 38.4              |\n\n- We use `ppl` for the MCQ evaluation on base model.\n- The evaluation results were obtained from [OpenCompass](https://github.com/open-compass/opencompass) , and evaluation configuration can be found in the configuration files provided by [OpenCompass](https://github.com/open-compass/opencompass).\n- The evaluation data may have numerical differences due to the version iteration of [OpenCompass](https://github.com/open-compass/opencompass), so please refer to the latest evaluation results of [OpenCompass](https://github.com/open-compass/opencompass).\n- \\* means the result is copied from the original paper.\n\n## Requirements\n\n- Python >= 3.8\n- PyTorch >= 1.12.0 (2.0.0 and above are recommended)\n- Transformers >= 4.38\n\n## Usages\n\nInternLM supports a diverse range of well-known upstream and downstream projects, such as LLaMA-Factory, vLLM, llama.cpp, and more. This support enables a broad spectrum of users to utilize the InternLM series models more efficiently and conveniently. Tutorials for selected ecosystem projects are available [here](./ecosystem/README.md) for your convenience.\n\nIn the following chapters, we will focus on the usages with [Transformers](#import-from-transformers), [ModelScope](#import-from-modelscope), and [Web demos](#dialogue).\nThe chat models adopt [chatml format](./chat/chat_format.md) to support both chat and agent applications.\nTo ensure a better usage effect, please make sure that the installed transformers library version meets the following requirements before performing inference with [Transformers](#import-from-transformers) or [ModelScope](#import-from-modelscope):\n\n```\ntransformers >= 4.38\n```\n\n### Import from Transformers\n\nTo load the InternLM2.5-7B-Chat model using Transformers, use the following code:\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm2_5-7b-chat\", trust_remote_code=True)\n# Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and might cause OOM Error.\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm2_5-7b-chat\", device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)\n# (Optional) If on low resource devices, you can load model in 4-bit or 8-bit to further save GPU memory via bitsandbytes.\n  # InternLM 7B in 4bit will cost nearly 8GB GPU memory.\n  # pip install -U bitsandbytes\n  # 8-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, load_in_8bit=True)\n  # 4-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, load_in_4bit=True)\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"hello\", history=[])\nprint(response)\n# Output: Hello? How can I help you today?\nresponse, history = model.chat(tokenizer, \"please provide three suggestions about time management\", history=history)\nprint(response)\n```\n\n### Import from ModelScope\n\nTo load the InternLM2.5-7B-Chat model using ModelScope, use the following code:\n\n```python\nimport torch\nfrom modelscope import snapshot_download, AutoTokenizer, AutoModelForCausalLM\nmodel_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2_5-7b-chat')\ntokenizer = AutoTokenizer.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True)\n# Set `torch_dtype=torch.float16` to load model in float16, otherwise it will be loaded as float32 and might cause OOM Error.\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)\n# (Optional) If on low resource devices, you can load model in 4-bit or 8-bit to further save GPU memory via bitsandbytes.\n  # InternLM 7B in 4bit will cost nearly 8GB GPU memory.\n  # pip install -U bitsandbytes\n  # 8-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, load_in_8bit=True)\n  # 4-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, load_in_4bit=True)\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"hello\", history=[])\nprint(response)\nresponse, history = model.chat(tokenizer, \"please provide three suggestions about time management\", history=history)\nprint(response)\n```\n\n### Dialogue\n\nYou can interact with the InternLM Chat 7B model through a frontend interface by running the following code:\n\n```bash\npip install streamlit\npip install transformers>=4.38\nstreamlit run ./chat/web_demo.py\n```\n\n## Deployment by LMDeploy\n\nWe use [LMDeploy](https://github.com/InternLM/LMDeploy) for fast deployment of InternLM.\n\n### Inference\n\nWith only 4 lines of codes, you can perform [internlm2_5-7b-chat](https://huggingface.co/internlm/internlm2_5-7b-chat) inference after `pip install lmdeploy`.\n\n```python\nfrom lmdeploy import pipeline\npipe = pipeline(\"internlm/internlm2_5-7b-chat\")\nresponse = pipe([\"Hi, pls intro yourself\", \"Shanghai is\"])\nprint(response)\n```\n\nTo reduce the memory footprint, we offers 4-bit quantized model [internlm2_5-7b-chat-4bit](https://huggingface.co/internlm/internlm2_5-7b-chat-4bit), with which the inference can be conducted as follows:\n\n```python\nfrom lmdeploy import pipeline\npipe = pipeline(\"internlm/internlm2_5-7b-chat-4bit\")\nresponse = pipe([\"Hi, pls intro yourself\", \"Shanghai is\"])\nprint(response)\n```\n\nMoreover, you can independently activate the 8bit/4bit KV cache feature:\n\n```python\nfrom lmdeploy import pipeline, TurbomindEngineConfig\npipe = pipeline(\"internlm/internlm2_5-7b-chat-4bit\",\n                backend_config=TurbomindEngineConfig(quant_policy=8))\nresponse = pipe([\"Hi, pls intro yourself\", \"Shanghai is\"])\nprint(response)\n```\n\nPlease refer to the [guidance](./chat/lmdeploy.md) for more usages about model deployment. For additional deployment tutorials, feel free to explore [here](https://github.com/InternLM/LMDeploy).\n\n### 1M-long-context Inference\n\nBy enabling the Dynamic NTK feature of LMDeploy, you can acquire the long-context inference power.\n\nNote: 1M context length requires 4xA100-80G.\n\n```python\nfrom lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig\n\nbackend_config = TurbomindEngineConfig(\n        rope_scaling_factor=2.5,\n        session_len=1048576,  # 1M context length\n        max_batch_size=1,\n        cache_max_entry_count=0.7,\n        tp=4)  # 4xA100-80G.\npipe = pipeline('internlm/internlm2_5-7b-chat-1m', backend_config=backend_config)\nprompt = 'Use a long prompt to replace this sentence'\nresponse = pipe(prompt)\nprint(response)\n```\n\n## Agent\n\nInternLM2.5-Chat models have excellent tool utilization capabilities and can work with function calls in a zero-shot manner. It also supports to conduct analysis by collecting information from more than 100 web pages. See more examples in [agent section](./agent/).\n\n## Fine-tuning\n\nPlease refer to [finetune docs](./finetune/) for fine-tuning with InternLM.\n\n**Note:** We have migrated the whole training functionality in this project to [InternEvo](https://github.com/InternLM/InternEvo) for easier user experience, which provides efficient pre-training and fine-tuning infra for training InternLM.\n\n## Evaluation\n\nWe utilize [OpenCompass](https://github.com/open-compass/opencompass) for model evaluation. In InternLM2.5, we primarily focus on standard objective evaluation, long-context evaluation (needle in a haystack), data contamination assessment, agent evaluation, and subjective evaluation.\n\n### Objective Evaluation\n\nTo evaluate the InternLM model, please follow the guidelines in the [OpenCompass tutorial](https://opencompass.readthedocs.io/en/latest/get_started/installation.html). Typically, we use `ppl` for multiple-choice questions on the **Base** model and `gen` for all questions on the **Chat** model.\n\n### Long-Context Evaluation (Needle in a Haystack)\n\nFor the `Needle in a Haystack` evaluation, refer to the tutorial provided in the [documentation](https://github.com/open-compass/opencompass/blob/main/docs/en/advanced_guides/needleinahaystack_eval.md). Feel free to try it out.\n\n### Data Contamination Assessment\n\nTo learn more about data contamination assessment, please check the [contamination eval](https://opencompass.readthedocs.io/en/latest/advanced_guides/contamination_eval.html).\n\n### Agent Evaluation\n\n- To evaluate tool utilization, please refer to [T-Eval](https://github.com/open-compass/T-Eval).\n- For code interpreter evaluation, use the [Math Agent Evaluation](agent/README.md) provided in the repository.\n\n### Subjective Evaluation\n\n- Please follow the [tutorial](https://opencompass.readthedocs.io/en/latest/advanced_guides/subjective_evaluation.html) for subjective evaluation.\n\n## Contribution\n\nWe appreciate all the contributors for their efforts to improve and enhance InternLM. Community users are highly encouraged to participate in the project. Please refer to the contribution guidelines for instructions on how to contribute to the project.\n\n## License\n\nThe code is licensed under Apache-2.0, while model weights are fully open for academic research and also allow **free** commercial usage. To apply for a commercial license, please fill in the [application form (English)](https://wj.qq.com/s2/12727483/5dba/)/[ç”³è¯·è¡¨ï¼ˆä¸­æ–‡ï¼‰](https://wj.qq.com/s2/12725412/f7c1/). For other questions or collaborations, please contact <internlm@pjlab.org.cn>.\n\n## Citation\n\n```\n@misc{cai2024internlm2,\n      title={InternLM2 Technical Report},\n      author={Zheng Cai and Maosong Cao and Haojiong Chen and Kai Chen and Keyu Chen and Xin Chen and Xun Chen and Zehui Chen and Zhi Chen and Pei Chu and Xiaoyi Dong and Haodong Duan and Qi Fan and Zhaoye Fei and Yang Gao and Jiaye Ge and Chenya Gu and Yuzhe Gu and Tao Gui and Aijia Guo and Qipeng Guo and Conghui He and Yingfan Hu and Ting Huang and Tao Jiang and Penglong Jiao and Zhenjiang Jin and Zhikai Lei and Jiaxing Li and Jingwen Li and Linyang Li and Shuaibin Li and Wei Li and Yining Li and Hongwei Liu and Jiangning Liu and Jiawei Hong and Kaiwen Liu and Kuikun Liu and Xiaoran Liu and Chengqi Lv and Haijun Lv and Kai Lv and Li Ma and Runyuan Ma and Zerun Ma and Wenchang Ning and Linke Ouyang and Jiantao Qiu and Yuan Qu and Fukai Shang and Yunfan Shao and Demin Song and Zifan Song and Zhihao Sui and Peng Sun and Yu Sun and Huanze Tang and Bin Wang and Guoteng Wang and Jiaqi Wang and Jiayu Wang and Rui Wang and Yudong Wang and Ziyi Wang and Xingjian Wei and Qizhen Weng and Fan Wu and Yingtong Xiong and Chao Xu and Ruiliang Xu and Hang Yan and Yirong Yan and Xiaogui Yang and Haochen Ye and Huaiyuan Ying and Jia Yu and Jing Yu and Yuhang Zang and Chuyu Zhang and Li Zhang and Pan Zhang and Peng Zhang and Ruijie Zhang and Shuo Zhang and Songyang Zhang and Wenjian Zhang and Wenwei Zhang and Xingcheng Zhang and Xinyue Zhang and Hui Zhao and Qian Zhao and Xiaomeng Zhao and Fengzhe Zhou and Zaida Zhou and Jingming Zhuo and Yicheng Zou and Xipeng Qiu and Yu Qiao and Dahua Lin},\n      year={2024},\n      eprint={2403.17297},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n"
        },
        {
          "name": "README_zh-CN.md",
          "type": "blob",
          "size": 32.3876953125,
          "content": "# InternLM\n\n<div align=\"center\">\n\n<img src=\"./assets//logo.svg\" width=\"200\"/>\n  <div>&nbsp;</div>\n  <div align=\"center\">\n    <b><font size=\"5\">ä¹¦ç”ŸÂ·æµ¦è¯­ å®˜ç½‘</font></b>\n    <sup>\n      <a href=\"https://internlm.intern-ai.org.cn/\">\n        <i><font size=\"4\">HOT</font></i>\n      </a>\n    </sup>\n    <div>&nbsp;</div>\n  </div>\n\n[![license](./assets//license.svg)](https://github.com/open-mmlab/mmdetection/blob/main/LICENSE)\n[![evaluation](./assets//compass_support.svg)](https://github.com/internLM/OpenCompass/)\n\n<!-- [![Documentation Status](https://readthedocs.org/projects/internlm/badge/?version=latest)](https://internlm.readthedocs.io/zh_CN/latest/?badge=latest) -->\n\n[ðŸ“˜å•†ä¸šæŽˆæƒ](#å¼€æºè®¸å¯è¯) |\n[ðŸ¤—HuggingFace](https://huggingface.co/internlm) |\n[ðŸ†•æœ€æ–°æ¶ˆæ¯](#æ›´æ–°) |\n[ðŸ¤”æäº¤åé¦ˆ](https://github.com/InternLM/InternLM/issues/new)|\n[ðŸ“œæŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2403.17297)<br>\n[ðŸ’¬èŠå¤©åº”ç”¨](https://internlm-chat.intern-ai.org.cn/) |\n[ðŸ”—API](https://internlm.intern-ai.org.cn/api/document) |\n[ðŸ§©é­”ä¹ç¤¾åŒº](https://modelers.cn/spaces/MindSpore-Lab/INTERNLM2-20B-PLAN)\n\n[English](./README.md) |\n[ç®€ä½“ä¸­æ–‡](./README_zh-CN.md)\n\n</div>\n\n<p align=\"center\">\n    ðŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href=\"https://discord.gg/xa29JuW87d\" target=\"_blank\">Discord</a> å’Œ <a href=\"https://github.com/InternLM/InternLM/assets/25839884/a6aad896-7232-4220-ac84-9e070c2633ce\" target=\"_blank\">å¾®ä¿¡ç¤¾åŒº</a>\n</p>\n\n## ç®€ä»‹\n\nInternLM2.5 ç³»åˆ—æ¨¡åž‹åœ¨æœ¬ä»“åº“æ­£å¼å‘å¸ƒï¼Œå…·æœ‰å¦‚ä¸‹ç‰¹æ€§ï¼š\n\n- å“è¶Šçš„æŽ¨ç†æ€§èƒ½ï¼šåœ¨æ•°å­¦æŽ¨ç†æ–¹é¢å–å¾—äº†åŒé‡çº§æ¨¡åž‹æœ€ä¼˜ç²¾åº¦ï¼Œè¶…è¶Šäº† Llama3 å’Œ Gemma2-9Bã€‚\n- æœ‰æ•ˆæ”¯æŒç™¾ä¸‡å­—è¶…é•¿ä¸Šä¸‹æ–‡ï¼šæ¨¡åž‹åœ¨ 1 ç™¾ä¸‡å­—é•¿è¾“å…¥ä¸­å‡ ä¹Žå®Œç¾Žåœ°å®žçŽ°é•¿æ–‡â€œå¤§æµ·æžé’ˆâ€ï¼Œè€Œä¸”åœ¨ LongBench ç­‰é•¿æ–‡ä»»åŠ¡ä¸­çš„è¡¨çŽ°ä¹Ÿè¾¾åˆ°å¼€æºæ¨¡åž‹ä¸­çš„é¢†å…ˆæ°´å¹³ã€‚ å¯ä»¥é€šè¿‡ [LMDeploy](./chat/lmdeploy_zh_cn.md) å°è¯•ç™¾ä¸‡å­—è¶…é•¿ä¸Šä¸‹æ–‡æŽ¨ç†ã€‚æ›´å¤šå†…å®¹å’Œæ–‡æ¡£å¯¹è¯ demo è¯·æŸ¥çœ‹[è¿™é‡Œ](./long_context/README_zh-CN.md)ã€‚\n- å·¥å…·è°ƒç”¨èƒ½åŠ›æ•´ä½“å‡çº§ï¼šInternLM2.5 æ”¯æŒä»Žä¸Šç™¾ä¸ªç½‘é¡µæœé›†æœ‰æ•ˆä¿¡æ¯è¿›è¡Œåˆ†æžæŽ¨ç†ï¼Œç›¸å…³å®žçŽ°å°†äºŽè¿‘æœŸå¼€æºåˆ° [Lagent](https://github.com/InternLM/lagent/tree/main)ã€‚InternLM2.5 å…·æœ‰æ›´å¼ºå’Œæ›´å…·æœ‰æ³›åŒ–æ€§çš„æŒ‡ä»¤ç†è§£ã€å·¥å…·ç­›é€‰ä¸Žç»“æžœåæ€ç­‰èƒ½åŠ›ï¼Œæ–°ç‰ˆæ¨¡åž‹å¯ä»¥æ›´å¯é åœ°æ”¯æŒå¤æ‚æ™ºèƒ½ä½“çš„æ­å»ºï¼Œæ”¯æŒå¯¹å·¥å…·è¿›è¡Œæœ‰æ•ˆçš„å¤šè½®è°ƒç”¨ï¼Œå®Œæˆè¾ƒå¤æ‚çš„ä»»åŠ¡ã€‚å¯ä»¥æŸ¥çœ‹æ›´å¤š[æ ·ä¾‹](./agent/)ã€‚\n\n## æ›´æ–°\n\n\\[2024.08.01\\] æˆ‘ä»¬å‘å¸ƒäº† InternLM2.5-1.8Bã€InternLM2.5-1.8B-Chatã€InternLM2.5-20B å’Œ InternLM2.5-20B-Chatã€‚å¯ä»¥åœ¨ä¸‹æ–¹çš„ [æ¨¡åž‹åº“](#model-zoo) è¿›è¡Œä¸‹è½½ï¼Œæˆ–è€…åœ¨ [model cards](./model_cards/) ä¸­äº†è§£æ›´å¤šç»†èŠ‚ã€‚\n\n\\[2024.07.19\\] æˆ‘ä»¬å‘å¸ƒäº† 1.8Bã€7B å’Œ 20B å¤§å°çš„ InternLM2-Reward ç³»åˆ—å¥–åŠ±æ¨¡åž‹ã€‚å¯ä»¥åœ¨ä¸‹æ–¹çš„ [æ¨¡åž‹åº“](#model-zoo) è¿›è¡Œä¸‹è½½ï¼Œæˆ–è€…åœ¨ [model cards](./model_cards/internlm2_reward.md) ä¸­äº†è§£æ›´å¤šç»†èŠ‚ã€‚\n\n\\[2024.06.30\\] æˆ‘ä»¬å‘å¸ƒäº† InternLM2.5-7Bã€InternLM2.5-7B-Chat å’Œ InternLM2.5-7B-Chat-1Mã€‚å¯ä»¥åœ¨ä¸‹æ–¹çš„ [æ¨¡åž‹åº“](#model-zoo) è¿›è¡Œä¸‹è½½ï¼Œæˆ–è€…åœ¨ [model cards](./model_cards/) ä¸­äº†è§£æ›´å¤šç»†èŠ‚ã€‚\n\n\\[2024.03.26\\] æˆ‘ä»¬å‘å¸ƒäº† InternLM2 çš„æŠ€æœ¯æŠ¥å‘Šã€‚ å¯ä»¥ç‚¹å‡» [arXivé“¾æŽ¥](https://arxiv.org/abs/2403.17297) æ¥äº†è§£æ›´å¤šç»†èŠ‚ã€‚\n\n\\[2024.01.31\\] æˆ‘ä»¬å‘å¸ƒäº† InternLM2-1.8Bï¼Œä»¥åŠç›¸å…³çš„å¯¹è¯æ¨¡åž‹ã€‚è¯¥æ¨¡åž‹åœ¨ä¿æŒé¢†å…ˆæ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œæä¾›äº†æ›´ä½Žå»‰çš„éƒ¨ç½²æ–¹æ¡ˆã€‚\n\n\\[2024.01.23\\] æˆ‘ä»¬å‘å¸ƒäº† InternLM2-Math-7B å’Œ InternLM2-Math-20B ä»¥åŠç›¸å…³çš„å¯¹è¯æ¨¡åž‹ã€‚InternLM-Mathä»¥è¾ƒå°çš„å°ºå¯¸è¶…è¿‡äº†ChatGPTçš„è¡¨çŽ°ã€‚å¯ä»¥ç‚¹å‡»[InternLM-Math](https://github.com/InternLM/internlm-math)è¿›è¡Œä¸‹è½½ï¼Œå¹¶äº†è§£è¯¦æƒ…ã€‚\n\n\\[2024.01.17\\] æˆ‘ä»¬å‘å¸ƒäº† InternLM2-7B å’Œ InternLM2-20B ä»¥åŠç›¸å…³çš„å¯¹è¯æ¨¡åž‹ï¼ŒInternLM2 åœ¨æ•°ç†ã€ä»£ç ã€å¯¹è¯ã€åˆ›ä½œç­‰å„æ–¹é¢èƒ½åŠ›éƒ½èŽ·å¾—äº†é•¿è¶³è¿›æ­¥ï¼Œç»¼åˆæ€§èƒ½è¾¾åˆ°å¼€æºæ¨¡åž‹çš„é¢†å…ˆæ°´å¹³ã€‚å¯ä»¥ç‚¹å‡»[ä¸‹é¢çš„æ¨¡åž‹åº“](#model-zoo)è¿›è¡Œä¸‹è½½æˆ–è€…[æŸ¥çœ‹æ¨¡åž‹æ–‡æ¡£](./model_cards/)æ¥äº†è§£æ›´å¤šç»†èŠ‚.\n\n\\[2023.12.13\\] æˆ‘ä»¬æ›´æ–°äº† InternLM-7B-Chat å’Œ InternLM-20B-Chat æ¨¡åž‹æƒé‡ã€‚é€šè¿‡æ”¹è¿›å¾®è°ƒæ•°æ®å’Œè®­ç»ƒç­–ç•¥ï¼Œæ–°ç‰ˆå¯¹è¯æ¨¡åž‹ç”Ÿæˆçš„å›žå¤è´¨é‡æ›´é«˜ã€è¯­è¨€é£Žæ ¼æ›´åŠ å¤šå…ƒã€‚\n\n\\[2023.09.20\\] InternLM-20B å·²å‘å¸ƒï¼ŒåŒ…æ‹¬åŸºç¡€ç‰ˆå’Œå¯¹è¯ç‰ˆã€‚\n\n## Model Zoo\n\n### InternLM2.5\n\n| Model                      | Transformers(HF)                           | ModelScope(HF)                           | OpenXLab(HF)                           | OpenXLab(Origin)                           | Release Date |\n| -------------------------- | ------------------------------------------ | ---------------------------------------- | -------------------------------------- | ------------------------------------------ | ------------ |\n| **InternLM2.5-1.8B**       | [ðŸ¤—internlm2_5-1_8b](https://huggingface.co/internlm/internlm2_5-1_8b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-1_8b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-1_8b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-1_8b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-1_8b-original) | 2024-08-05   |\n| **InternLM2.5-1.8B-Chat**  | [ðŸ¤—internlm2_5-1_8b-chat](https://huggingface.co/internlm/internlm2_5-1_8b-chat) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-1_8b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-1_8b-chat/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-1_8b-chat) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-1_8b-chat-original) | 2024-08-05   |\n| **InternLM2.5-7B**         | [ðŸ¤—internlm2_5-7b](https://huggingface.co/internlm/internlm2_5-7b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-7b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b-original) | 2024-07-03   |\n| **InternLM2.5-7B-Chat**    | [ðŸ¤—internlm2_5-7b-chat](https://huggingface.co/internlm/internlm2_5-7b-chat) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-7b-chat](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-7b-chat/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b-chat) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b-chat-original) | 2024-07-03   |\n| **InternLM2.5-7B-Chat-1M** | [ðŸ¤—internlm2_5-7b-chat-1m](https://huggingface.co/internlm/internlm2_5-7b-chat-1m) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-7b-chat-1m](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-7b-chat-1m/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b-chat-1m) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-7b-chat-1m-original) | 2024-07-03   |\n| **InternLM2.5-20B**        | [ðŸ¤—internlm2_5-20b](https://huggingface.co/internlm/internlm2_5-20b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-20b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-20b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-20b-original) | 2024-08-05   |\n| **InternLM2.5-20B-Chat**   | [ðŸ¤—internlm2_5-20b-chat](https://huggingface.co/internlm/internlm2_5-20b-chat) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2_5-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2_5-20b-chat/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-20b-chat) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2_5-20b-chat-original) | 2024-08-05   |\n\n**æ¨¡åž‹è¯´æ˜Žï¼š**\n\nç›®å‰ InternLM 2.5 ç³»åˆ—å‘å¸ƒäº† 1.8Bã€7B å’Œ 20B å¤§å°çš„æ¨¡åž‹ã€‚7B ä¸ºè½»é‡çº§çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†ä¸€ä¸ªè½»ä¾¿ä½†æ€§èƒ½ä¸ä¿—çš„æ¨¡åž‹ï¼Œ20B æ¨¡åž‹çš„ç»¼åˆæ€§èƒ½æ›´ä¸ºå¼ºåŠ²ï¼Œå¯ä»¥æœ‰æ•ˆæ”¯æŒæ›´åŠ å¤æ‚çš„å®žç”¨åœºæ™¯ã€‚æ¯ä¸ªè§„æ ¼ä¸åŒæ¨¡åž‹å…³ç³»å¦‚ä¸‹æ‰€ç¤ºï¼š\n\n1. **InternLM2.5**ï¼šç»åŽ†äº†å¤§è§„æ¨¡é¢„è®­ç»ƒçš„åŸºåº§æ¨¡åž‹ï¼Œæ˜¯æˆ‘ä»¬æŽ¨èçš„åœ¨å¤§éƒ¨åˆ†åº”ç”¨ä¸­è€ƒè™‘é€‰ç”¨çš„ä¼˜ç§€åŸºåº§ã€‚\n2. **InternLM2.5-Chat**: å¯¹è¯æ¨¡åž‹ï¼Œåœ¨ InternLM2.5 åŸºåº§ä¸Šç»åŽ†äº†æœ‰ç›‘ç£å¾®è°ƒå’Œ online RLHFã€‚InternLM2.5-Chat é¢å‘å¯¹è¯äº¤äº’è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå…·æœ‰è¾ƒå¥½çš„æŒ‡ä»¤éµå¾ªã€å…±æƒ…èŠå¤©å’Œè°ƒç”¨å·¥å…·ç­‰çš„èƒ½åŠ›ï¼Œæ˜¯æˆ‘ä»¬æŽ¨èç›´æŽ¥ç”¨äºŽä¸‹æ¸¸åº”ç”¨çš„æ¨¡åž‹ã€‚\n3. **InternLM2.5-Chat-1M**: InternLM2.5-Chat-1M æ”¯æŒä¸€ç™¾ä¸‡å­—è¶…é•¿ä¸Šä¸‹æ–‡ï¼Œå¹¶å…·æœ‰å’Œ InternLM2.5-Chat ç›¸å½“çš„ç»¼åˆæ€§èƒ½è¡¨çŽ°ã€‚\n\n**å±€é™æ€§ï¼š** å°½ç®¡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ‘ä»¬éžå¸¸æ³¨é‡æ¨¡åž‹çš„å®‰å…¨æ€§ï¼Œå°½åŠ›ä¿ƒä½¿æ¨¡åž‹è¾“å‡ºç¬¦åˆä¼¦ç†å’Œæ³•å¾‹è¦æ±‚çš„æ–‡æœ¬ï¼Œä½†å—é™äºŽæ¨¡åž‹å¤§å°ä»¥åŠæ¦‚çŽ‡ç”ŸæˆèŒƒå¼ï¼Œæ¨¡åž‹å¯èƒ½ä¼šäº§ç”Ÿå„ç§ä¸ç¬¦åˆé¢„æœŸçš„è¾“å‡ºï¼Œä¾‹å¦‚å›žå¤å†…å®¹åŒ…å«åè§ã€æ­§è§†ç­‰æœ‰å®³å†…å®¹ï¼Œè¯·å‹¿ä¼ æ’­è¿™äº›å†…å®¹ã€‚ç”±äºŽä¼ æ’­ä¸è‰¯ä¿¡æ¯å¯¼è‡´çš„ä»»ä½•åŽæžœï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…è´£ä»»ã€‚\n\n**è¡¥å……è¯´æ˜Žï¼š** ä¸Šè¡¨ä¸­çš„ `HF` è¡¨ç¤ºå¯¹åº”æ¨¡åž‹ä¸º HuggingFace å¹³å°æä¾›çš„ [transformers](https://github.com/huggingface/transformers) æ¡†æž¶æ ¼å¼ï¼›`Origin` åˆ™è¡¨ç¤ºå¯¹åº”æ¨¡åž‹ä¸ºæˆ‘ä»¬ InternLM å›¢é˜Ÿçš„ [InternEvo](https://github.com/InternLM/InternEvo) æ¡†æž¶æ ¼å¼ã€‚\n\n### InternLM2-Reward\n\nInternLM2-Reward æ˜¯åŸºäºŽ 240 ä¸‡ä¸ªåå¥½æ ·æœ¬è¿›è¡Œè®­ç»ƒçš„å¥–åŠ±æ¨¡åž‹ï¼Œæœ‰ 1.8Bã€7B å’Œ 20B å¤§å°å¯ä¾›é€‰æ‹©ã€‚è¿™äº›æ¨¡åž‹è¢«ç”¨äºŽ InternLM å¯¹è¯æ¨¡åž‹çš„ PPO è®­ç»ƒè¿‡ç¨‹ã€‚è¯·å‚è€ƒ [model cards](./model_cards/internlm2_reward.md) äº†è§£æ›´å¤šç»†èŠ‚ã€‚\n\n| Model                     | RewardBench Score | Transformers(HF)                                   | ModelScope(HF)                                    | OpenXLab(HF)                                    | Release Date |\n| ------------------------- | ----------------- | -------------------------------------------------- | ------------------------------------------------- | ----------------------------------------------- | ------------ |\n| **InternLM2-1.8B-Reward** | 80.6              | [ðŸ¤—internlm2-1_8b-reward](https://huggingface.co/internlm/internlm2-1_8b-reward) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-1_8b-reward](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-1_8b-reward/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-1_8b-reward) | 2024-07-19   |\n| **InternLM2-7B-Reward**   | 86.6              | [ðŸ¤—internlm2-7b-reward](https://huggingface.co/internlm/internlm2-7b-reward) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-7b-reward](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-7b-reward/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-7b-reward) | 2024-07-19   |\n| **InternLM2-20B-Reward**  | 89.5              | [ðŸ¤—internlm2-20b-reward](https://huggingface.co/internlm/internlm2-20b-reward) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-20b-reward](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-20b-reward/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-20b-reward) | 2024-07-19   |\n\n### InternLM2\n\n<details>\n    <summary>(click to expand)</summary>\n\næˆ‘ä»¬ä¸Šä¸€ä»£çš„æ¨¡åž‹ï¼Œåœ¨é•¿ä¸Šä¸‹æ–‡å¤„ç†ã€æŽ¨ç†å’Œç¼–ç æ–¹é¢å…·æœ‰ä¼˜ç§€çš„æ€§èƒ½ã€‚è¯·å‚è€ƒ [model cards](./model_cards/) äº†è§£æ›´å¤šç»†èŠ‚ã€‚\n\n| Model                       | Transformers(HF)                          | ModelScope(HF)                           | OpenXLab(HF)                           | OpenXLab(Origin)                           | Release Date |\n| --------------------------- | ----------------------------------------- | ---------------------------------------- | -------------------------------------- | ------------------------------------------ | ------------ |\n| **InternLM2-1.8B**          | [ðŸ¤—internlm2-1.8b](https://huggingface.co/internlm/internlm2-1_8b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-1.8b](https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-1_8b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-1.8b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-1.8b-original) | 2024-01-31   |\n| **InternLM2-Chat-1.8B-SFT** | [ðŸ¤—internlm2-chat-1.8b-sft](https://huggingface.co/internlm/internlm2-chat-1_8b-sft) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-chat-1.8b-sft](https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-1_8b-sft/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-1.8b-sft) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-1.8b-sft-original) | 2024-01-31   |\n| **InternLM2-Chat-1.8B**     | [ðŸ¤—internlm2-chat-1.8b](https://huggingface.co/internlm/internlm2-chat-1_8b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-chat-1.8b](https://www.modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-1_8b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-1.8b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-1.8b-original) | 2024-02-19   |\n| **InternLM2-Base-7B**       | [ðŸ¤—internlm2-base-7b](https://huggingface.co/internlm/internlm2-base-7b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-base-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-7b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-7b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-7b-original) | 2024-01-17   |\n| **InternLM2-7B**            | [ðŸ¤—internlm2-7b](https://huggingface.co/internlm/internlm2-7b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-7b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-7b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-7b-original) | 2024-01-17   |\n| **InternLM2-Chat-7B-SFT**   | [ðŸ¤—internlm2-chat-7b-sft](https://huggingface.co/internlm/internlm2-chat-7b-sft) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-chat-7b-sft](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b-sft/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b-sft) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b-sft-original) | 2024-01-17   |\n| **InternLM2-Chat-7B**       | [ðŸ¤—internlm2-chat-7b](https://huggingface.co/internlm/internlm2-chat-7b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-chat-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-7b-original) | 2024-01-17   |\n| **InternLM2-Base-20B**      | [ðŸ¤—internlm2-base-20b](https://huggingface.co/internlm/internlm2-base-20b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-base-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-20b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-20b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-base-20b-original) | 2024-01-17   |\n| **InternLM2-20B**           | [ðŸ¤—internlm2-20b](https://huggingface.co/internlm/internlm2-20b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-20b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-20b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-20b-original) | 2024-01-17   |\n| **InternLM2-Chat-20B-SFT**  | [ðŸ¤—internlm2-chat-20b-sft](https://huggingface.co/internlm/internlm2-chat-20b-sft) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-chat-20b-sft](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b-sft/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b-sft) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b-sft-original) | 2024-01-17   |\n| **InternLM2-Chat-20B**      | [ðŸ¤—internlm2-chat-20b](https://huggingface.co/internlm/internlm2-chat-20b) | [<img src=\"./assets/modelscope_logo.png\" width=\"20px\" /> internlm2-chat-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-20b/summary) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b) | [![Open in OpenXLab](https://cdn-static.openxlab.org.cn/header/openxlab_models.svg)](https://openxlab.org.cn/models/detail/OpenLMLab/internlm2-chat-20b-original) | 2024-01-17   |\n\n</details>\n\n## æ€§èƒ½\n\næˆ‘ä»¬ä½¿ç”¨å¼€æºè¯„æµ‹å·¥å…· [OpenCompass](https://github.com/open-compass/opencompass) åœ¨å‡ ä¸ªé‡è¦çš„åŸºå‡†æµ‹è¯•ä¸­å¯¹ InternLM2.5 è¿›è¡Œäº†è¯„æµ‹ã€‚éƒ¨åˆ†è¯„æµ‹ç»“æžœå¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚æ¬¢è¿Žè®¿é—® [OpenCompass æŽ’è¡Œæ¦œ](https://rank.opencompass.org.cn) èŽ·å–æ›´å¤šè¯„æµ‹ç»“æžœã€‚\n\n### åŸºåº§æ¨¡åž‹\n\n| Benchmark      | InternLM2.5-7B | Llama3-8B | Yi-1.5-9B |\n| -------------- | -------------- | --------- | --------- |\n| MMLU (5-shot)  | **71.6**       | 66.4      | 71.6      |\n| CMMLU (5-shot) | **79.1**       | 51.0      | 74.1      |\n| BBH (3-shot)   | 70.1           | 59.7      | 71.1      |\n| MATH (4-shot)  | **34.0**       | 16.4      | 31.9      |\n| GSM8K (4-shot) | **74.8**       | 54.3      | 74.5      |\n| GPQA (0-shot)  | **31.3**       | 31.3      | 27.8      |\n\n### å¯¹è¯æ¨¡åž‹\n\n| Benchmark          | InternLM2.5-7B-Chat | Llama3-8B-Instruct | Gemma2-9B-IT | Yi-1.5-9B-Chat | GLM-4-9B-Chat | Qwen2-7B-Instruct |\n| ------------------ | ------------------- | ------------------ | ------------ | -------------- | ------------- | ----------------- |\n| MMLU (5-shot)      | **72.8**            | 68.4               | 70.9         | 71.0           | 71.4          | 70.8              |\n| CMMLU (5-shot)     | 78.0                | 53.3               | 60.3         | 74.5           | 74.5          | 80.9              |\n| BBH (3-shot CoT)   | **71.6**            | 54.4               | 68.2\\*       | 69.6           | 69.6          | 65.0              |\n| MATH (0-shot CoT)  | **60.1**            | 27.9               | 46.9         | 51.1           | 51.1          | 48.6              |\n| GSM8K (0-shot CoT) | 86.0                | 72.9               | 88.9         | 80.1           | 85.3          | 82.9              |\n| GPQA (0-shot)      | **38.4**            | 26.1               | 33.8         | 37.9           | 36.9          | 38.4              |\n\n- æˆ‘ä»¬ä½¿ç”¨ `ppl` å¯¹åŸºåº§æ¨¡åž‹è¿›è¡Œ MCQ æŒ‡æ ‡çš„è¯„æµ‹ã€‚\n- è¯„æµ‹ç»“æžœæ¥è‡ª [OpenCompass](https://github.com/open-compass/opencompass) ï¼Œè¯„æµ‹é…ç½®å¯ä»¥åœ¨ [OpenCompass](https://github.com/open-compass/opencompass) æä¾›çš„é…ç½®æ–‡ä»¶ä¸­æ‰¾åˆ°ã€‚\n- ç”±äºŽ [OpenCompass](https://github.com/open-compass/opencompass) çš„ç‰ˆæœ¬è¿­ä»£ï¼Œè¯„æµ‹æ•°æ®å¯èƒ½å­˜åœ¨æ•°å€¼å·®å¼‚ï¼Œå› æ­¤è¯·å‚è€ƒ [OpenCompass](https://github.com/open-compass/opencompass) çš„æœ€æ–°è¯„æµ‹ç»“æžœã€‚\n- \\* è¡¨ç¤ºä»ŽåŽŸè®ºæ–‡ä¸­å¤åˆ¶è€Œæ¥ã€‚\n\n## ä¾èµ–\n\n- Python >= 3.8\n- PyTorch >= 1.12.0 (æŽ¨è 2.0.0 å’Œæ›´é«˜ç‰ˆæœ¬)\n- Transformers >= 4.38\n\n## ä½¿ç”¨æ¡ˆä¾‹\n\nInternLM æ”¯æŒä¼—å¤šçŸ¥åçš„ä¸Šä¸‹æ¸¸é¡¹ç›®ï¼Œå¦‚ LLaMA-Factoryã€vLLMã€llama.cpp ç­‰ã€‚è¿™ç§æ”¯æŒä½¿å¾—å¹¿å¤§ç”¨æˆ·ç¾¤ä½“èƒ½å¤Ÿæ›´é«˜æ•ˆã€æ›´æ–¹ä¾¿åœ°ä½¿ç”¨ InternLM å…¨ç³»åˆ—æ¨¡åž‹ã€‚ä¸ºæ–¹ä¾¿ä½¿ç”¨ï¼Œæˆ‘ä»¬ä¸ºéƒ¨åˆ†ç”Ÿæ€ç³»ç»Ÿé¡¹ç›®æä¾›äº†æ•™ç¨‹ï¼Œè®¿é—®[æ­¤å¤„](./ecosystem/README_zh-CN.md)å³å¯èŽ·å–ã€‚\n\næŽ¥ä¸‹æ¥æˆ‘ä»¬å±•ç¤ºä½¿ç”¨ [Transformers](#import-from-transformers)ï¼Œ[ModelScope](#import-from-modelscope) å’Œ [Web demo](#dialogue) è¿›è¡ŒæŽ¨ç†ã€‚\nå¯¹è¯æ¨¡åž‹é‡‡ç”¨äº† [chatml æ ¼å¼](./chat/chat_format.md) æ¥æ”¯æŒé€šç”¨å¯¹è¯å’Œæ™ºèƒ½ä½“åº”ç”¨ã€‚\nä¸ºäº†ä¿éšœæ›´å¥½çš„ä½¿ç”¨æ•ˆæžœï¼Œåœ¨ç”¨ [Transformers](#import-from-transformers) æˆ– [ModelScope](#import-from-modelscope) è¿›è¡ŒæŽ¨ç†å‰ï¼Œè¯·ç¡®ä¿å®‰è£…çš„ transformers åº“ç‰ˆæœ¬æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š\n\n```\ntransformers >= 4.38\n```\n\n### é€šè¿‡ Transformers åŠ è½½\n\né€šè¿‡ä»¥ä¸‹çš„ä»£ç ä»Ž Transformers åŠ è½½ InternLM2.5-7B-Chat æ¨¡åž‹ ï¼ˆå¯ä¿®æ”¹æ¨¡åž‹åç§°æ›¿æ¢ä¸åŒçš„æ¨¡åž‹ï¼‰\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm2_5-7b-chat\", trust_remote_code=True)\n# è®¾ç½®`torch_dtype=torch.float16`æ¥å°†æ¨¡åž‹ç²¾åº¦æŒ‡å®šä¸ºtorch.float16ï¼Œå¦åˆ™å¯èƒ½ä¼šå› ä¸ºæ‚¨çš„ç¡¬ä»¶åŽŸå› é€ æˆæ˜¾å­˜ä¸è¶³çš„é—®é¢˜ã€‚\nmodel = AutoModelForCausalLM.from_pretrained(\"internlm/internlm2_5-7b-chat\", device_map=\"auto\",trust_remote_code=True, torch_dtype=torch.float16)\n# (å¯é€‰) å¦‚æžœåœ¨ä½Žèµ„æºè®¾å¤‡ä¸Šï¼Œå¯ä»¥é€šè¿‡bitsandbytesåŠ è½½4-bitæˆ–8-bité‡åŒ–çš„æ¨¡åž‹ï¼Œè¿›ä¸€æ­¥èŠ‚çœGPUæ˜¾å­˜.\n  # 4-bit é‡åŒ–çš„ InternLM 7B å¤§çº¦ä¼šæ¶ˆè€— 8GB æ˜¾å­˜.\n  # pip install -U bitsandbytes\n  # 8-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, load_in_8bit=True)\n  # 4-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, load_in_4bit=True)\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\nprint(response)\n# æ¨¡åž‹è¾“å‡ºï¼šä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\nresponse, history = model.chat(tokenizer, \"è¯·æä¾›ä¸‰ä¸ªç®¡ç†æ—¶é—´çš„å»ºè®®ã€‚\", history=history)\nprint(response)\n```\n\n### é€šè¿‡ ModelScope åŠ è½½\n\né€šè¿‡ä»¥ä¸‹çš„ä»£ç ä»Ž ModelScope åŠ è½½ InternLM2.5-7B-Chat æ¨¡åž‹ ï¼ˆå¯ä¿®æ”¹æ¨¡åž‹åç§°æ›¿æ¢ä¸åŒçš„æ¨¡åž‹ï¼‰\n\n```python\nimport torch\nfrom modelscope import snapshot_download, AutoTokenizer, AutoModelForCausalLM\nmodel_dir = snapshot_download('Shanghai_AI_Laboratory/internlm2_5-7b-chat')\ntokenizer = AutoTokenizer.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)\n# (å¯é€‰) å¦‚æžœåœ¨ä½Žèµ„æºè®¾å¤‡ä¸Šï¼Œå¯ä»¥é€šè¿‡bitsandbytesåŠ è½½4-bitæˆ–8-bité‡åŒ–çš„æ¨¡åž‹ï¼Œè¿›ä¸€æ­¥èŠ‚çœGPUæ˜¾å­˜.\n  # 4-bit é‡åŒ–çš„ InternLM 7B å¤§çº¦ä¼šæ¶ˆè€— 8GB æ˜¾å­˜.\n  # pip install -U bitsandbytes\n  # 8-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, load_in_8bit=True)\n  # 4-bit: model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, load_in_4bit=True)\nmodel = model.eval()\nresponse, history = model.chat(tokenizer, \"hello\", history=[])\nprint(response)\nresponse, history = model.chat(tokenizer, \"please provide three suggestions about time management\", history=history)\nprint(response)\n```\n\n### é€šè¿‡å‰ç«¯ç½‘é¡µå¯¹è¯\n\nå¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç å¯åŠ¨ä¸€ä¸ªå‰ç«¯çš„ç•Œé¢æ¥ä¸Ž InternLM Chat 7B æ¨¡åž‹è¿›è¡Œäº¤äº’\n\n```bash\npip install streamlit\npip install transformers>=4.38\nstreamlit run ./chat/web_demo.py\n```\n\n## InternLM é«˜æ€§èƒ½éƒ¨ç½²\n\næˆ‘ä»¬ä½¿ç”¨ [LMDeploy](https://github.com/InternLM/LMDeploy) å®Œæˆ InternLM çš„ä¸€é”®éƒ¨ç½²ã€‚\n\n### æŽ¨ç†\n\né€šè¿‡ `pip install lmdeploy` å®‰è£… LMDeploy ä¹‹åŽï¼Œåªéœ€ 4 è¡Œä»£ç ï¼Œå°±å¯ä»¥å®žçŽ°ç¦»çº¿æ‰¹å¤„ç†ï¼š\n\n```python\nfrom lmdeploy import pipeline\npipe = pipeline(\"internlm/internlm2_5-7b-chat\")\nresponse = pipe([\"Hi, pls intro yourself\", \"Shanghai is\"])\nprint(response)\n```\n\nä¸ºäº†å‡å°‘å†…å­˜å ç”¨ï¼Œæˆ‘ä»¬æä¾›äº†4ä½é‡åŒ–æ¨¡åž‹ [internlm2_5-7b-chat-4bit](https://huggingface.co/internlm/internlm2_5-7b-chat-4bit)ã€‚å¯ä»¥æŒ‰ç…§å¦‚ä¸‹æ–¹å¼æŽ¨ç†è¯¥æ¨¡åž‹ï¼š\n\n```python\nfrom lmdeploy import pipeline\npipe = pipeline(\"internlm/internlm2_5-7b-chat-4bit\")\nresponse = pipe([\"Hi, pls intro yourself\", \"Shanghai is\"])\nprint(response)\n```\n\næ­¤å¤–ï¼Œå¯ä»¥åŒæ­¥å¼€å¯ 8bit æˆ–è€… 4bit KV åœ¨çº¿é‡åŒ–åŠŸèƒ½ï¼š\n\n```python\nfrom lmdeploy import pipeline, TurbomindEngineConfig\npipe = pipeline(\"internlm/internlm2_5-7b-chat-4bit\",\n                backend_config=TurbomindEngineConfig(quant_policy=8))\nresponse = pipe([\"Hi, pls intro yourself\", \"Shanghai is\"])\nprint(response)\n```\n\næ›´å¤šä½¿ç”¨æ¡ˆä¾‹å¯å‚è€ƒ[éƒ¨ç½²æŒ‡å—](./chat/lmdeploy.md)ï¼Œè¯¦ç»†çš„éƒ¨ç½²æ•™ç¨‹åˆ™å¯åœ¨[è¿™é‡Œ](https://github.com/InternLM/LMDeploy)æ‰¾åˆ°ã€‚\n\n### 1ç™¾ä¸‡å­—è¶…é•¿ä¸Šä¸‹æ–‡æŽ¨ç†\n\næ¿€æ´» LMDeploy çš„ Dynamic NTK èƒ½åŠ›ï¼Œå¯ä»¥è½»æ¾æŠŠ internlm2_5-7b-chat å¤–æŽ¨åˆ° 200K ä¸Šä¸‹æ–‡ã€‚\n\næ³¨æ„: 1M ä¸Šä¸‹æ–‡éœ€è¦ 4xA100-80Gã€‚\n\n```python\nfrom lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig\n\nbackend_config = TurbomindEngineConfig(\n        rope_scaling_factor=2.5,\n        session_len=1048576,  # 1M context length\n        max_batch_size=1,\n        cache_max_entry_count=0.7,\n        tp=4)  # 4xA100-80G.\npipe = pipeline('internlm/internlm2_5-7b-chat-1m', backend_config=backend_config)\nprompt = 'Use a long prompt to replace this sentence'\nresponse = pipe(prompt)\nprint(response)\n```\n\n## æ™ºèƒ½ä½“\n\nInternLM-2.5-Chat æ¨¡åž‹æœ‰å‡ºè‰²çš„å·¥å…·è°ƒç”¨æ€§èƒ½å¹¶å…·æœ‰ä¸€å®šçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚å®ƒæ”¯æŒä»Žä¸Šç™¾ä¸ªç½‘é¡µä¸­æœé›†ä¿¡æ¯å¹¶è¿›è¡Œåˆ†æžã€‚æ›´å¤šæ ·ä¾‹å¯ä»¥å‚è€ƒ  [agent ç›®å½•](./agent/).\n\n## å¾®è°ƒ&è®­ç»ƒ\n\nè¯·å‚è€ƒ[å¾®è°ƒæ•™ç¨‹](./finetune/)å°è¯•ç»­è®­æˆ–å¾®è°ƒ InternLM2ã€‚\n\n**æ³¨æ„ï¼š** æœ¬é¡¹ç›®ä¸­çš„å…¨é‡è®­ç»ƒåŠŸèƒ½å·²ç»è¿ç§»åˆ°äº† [InternEvo](https://github.com/InternLM/InternEvo) ä»¥ä¾¿ç”¨æˆ·ä½¿ç”¨ã€‚InternEvo æä¾›äº†é«˜æ•ˆçš„é¢„è®­ç»ƒå’Œå¾®è°ƒåŸºå»ºç”¨äºŽè®­ç»ƒ InternLM ç³»åˆ—æ¨¡åž‹ã€‚\n\n## è¯„æµ‹\n\næˆ‘ä»¬ä½¿ç”¨ [OpenCompass](https://github.com/open-compass/opencompass) è¿›è¡Œæ¨¡åž‹è¯„ä¼°ã€‚åœ¨ InternLM2.5 ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦æ ‡å‡†å®¢è§‚è¯„ä¼°ã€é•¿æ–‡è¯„ä¼°ï¼ˆå¤§æµ·æžé’ˆï¼‰ã€æ•°æ®æ±¡æŸ“è¯„ä¼°ã€æ™ºèƒ½ä½“è¯„ä¼°å’Œä¸»è§‚è¯„ä¼°ã€‚\n\n### æ ‡å‡†å®¢è§‚è¯„æµ‹\n\nè¯·æŒ‰ç…§ [OpenCompass æ•™ç¨‹](https://opencompass.readthedocs.io/zh-cn/latest/get_started/installation.html) è¿›è¡Œå®¢è§‚è¯„æµ‹ã€‚æˆ‘ä»¬é€šå¸¸åœ¨ Base æ¨¡åž‹ä¸Šä½¿ç”¨ ppl è¿›è¡Œå¤šé¡¹é€‰æ‹©é¢˜è¯„æµ‹ï¼Œåœ¨ Chat æ¨¡åž‹ä¸Šä½¿ç”¨ gen è¿›è¡Œæ‰€æœ‰é—®é¢˜çš„ç­”æ¡ˆç”Ÿæˆå’Œè¯„æµ‹ã€‚\n\n### é•¿æ–‡è¯„ä¼°ï¼ˆå¤§æµ·æžé’ˆï¼‰\n\næœ‰å…³ `å¤§æµ·æžé’ˆ` è¯„ä¼°çš„æ•™ç¨‹ï¼Œè¯·å‚é˜… [æ–‡æ¡£](https://github.com/open-compass/opencompass/blob/main/docs/en/advanced_guides/needleinahaystack_eval.md) ä¸­çš„æ•™ç¨‹ã€‚\n\n### æ•°æ®æ±¡æŸ“è¯„ä¼°\n\nè¦äº†è§£æ›´å¤šå…³äºŽæ•°æ®æ±¡æŸ“è¯„ä¼°çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [æ±¡æŸ“è¯„ä¼°](https://opencompass.readthedocs.io/en/latest/advanced_guides/contamination_eval.html)ã€‚\n\n### æ™ºèƒ½ä½“è¯„ä¼°\n\n- è¦è¯„ä¼°å¤§æ¨¡åž‹çš„å·¥å…·åˆ©ç”¨èƒ½åŠ›ï¼Œè¯·ä½¿ç”¨ [T-Eval](https://github.com/open-compass/T-Eval) è¿›è¡Œè¯„æµ‹ã€‚\n- å¯¹äºŽä»£ç è§£é‡Šå™¨è¯„ä¼°ï¼Œè¯·ä½¿ç”¨ [gsm-8k-agent](https://github.com/open-compass/opencompass/blob/main/configs/datasets/gsm8k/gsm8k_agent_gen_be1606.py) æä¾›çš„é…ç½®è¿›è¡Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œæ‚¨è¿˜éœ€è¦å®‰è£… [Lagent](https://github.com/InternLM/lagent)ã€‚\n\n### ä¸»è§‚è¯„ä¼°\n\n- è¯·æŒ‰ç…§ [æ•™ç¨‹](https://opencompass.readthedocs.io/en/latest/advanced_guides/subjective_evaluation.html) è¿›è¡Œä¸»è§‚è¯„ä¼°ã€‚\n\n## è´¡çŒ®\n\næˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ InternLM æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚éžå¸¸æ¬¢è¿Žç¤¾åŒºç”¨æˆ·èƒ½å‚ä¸Žè¿›é¡¹ç›®ä¸­æ¥ã€‚è¯·å‚è€ƒè´¡çŒ®æŒ‡å—æ¥äº†è§£å‚ä¸Žé¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚\n\n## è‡´è°¢\n\nInternLM ä»£ç åº“æ˜¯ä¸€æ¬¾ç”±ä¸Šæµ·äººå·¥æ™ºèƒ½å®žéªŒå®¤å’Œæ¥è‡ªä¸åŒé«˜æ ¡ã€ä¼ä¸šçš„ç ”å‘äººå‘˜å…±åŒå‚ä¸Žè´¡çŒ®çš„å¼€æºé¡¹ç›®ã€‚æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰ä¸ºé¡¹ç›®æä¾›æ–°åŠŸèƒ½æ”¯æŒçš„è´¡çŒ®è€…ï¼Œä»¥åŠæä¾›å®è´µåé¦ˆæ„è§çš„ç”¨æˆ·ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå·¥å…·ç®±å’ŒåŸºå‡†æµ‹è¯•å¯ä»¥ä¸ºç¤¾åŒºæä¾›çµæ´»é«˜æ•ˆçš„ä»£ç å·¥å…·ï¼Œä¾›ç”¨æˆ·å¾®è°ƒ InternLM å¹¶å¼€å‘è‡ªå·±çš„æ–°æ¨¡åž‹ï¼Œä»Žè€Œä¸æ–­ä¸ºå¼€æºç¤¾åŒºæä¾›è´¡çŒ®ã€‚ç‰¹åˆ«é¸£è°¢ [flash-attention](https://github.com/HazyResearch/flash-attention) ä¸Ž [ColossalAI](https://github.com/hpcaitech/ColossalAI) ä¸¤é¡¹å¼€æºé¡¹ç›®ã€‚\n\n## å¼€æºè®¸å¯è¯\n\næœ¬ä»“åº“çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚æ¨¡åž‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œä¹Ÿå¯ç”³è¯·å…è´¹çš„å•†ä¸šä½¿ç”¨æŽˆæƒï¼ˆ[ç”³è¯·è¡¨](https://wj.qq.com/s2/12725412/f7c1/)ï¼‰ã€‚å…¶ä»–é—®é¢˜ä¸Žåˆä½œè¯·è”ç³» <internlm@pjlab.org.cn>ã€‚\n\n## å¼•ç”¨\n\n```\n@misc{cai2024internlm2,\n      title={InternLM2 Technical Report},\n      author={Zheng Cai and Maosong Cao and Haojiong Chen and Kai Chen and Keyu Chen and Xin Chen and Xun Chen and Zehui Chen and Zhi Chen and Pei Chu and Xiaoyi Dong and Haodong Duan and Qi Fan and Zhaoye Fei and Yang Gao and Jiaye Ge and Chenya Gu and Yuzhe Gu and Tao Gui and Aijia Guo and Qipeng Guo and Conghui He and Yingfan Hu and Ting Huang and Tao Jiang and Penglong Jiao and Zhenjiang Jin and Zhikai Lei and Jiaxing Li and Jingwen Li and Linyang Li and Shuaibin Li and Wei Li and Yining Li and Hongwei Liu and Jiangning Liu and Jiawei Hong and Kaiwen Liu and Kuikun Liu and Xiaoran Liu and Chengqi Lv and Haijun Lv and Kai Lv and Li Ma and Runyuan Ma and Zerun Ma and Wenchang Ning and Linke Ouyang and Jiantao Qiu and Yuan Qu and Fukai Shang and Yunfan Shao and Demin Song and Zifan Song and Zhihao Sui and Peng Sun and Yu Sun and Huanze Tang and Bin Wang and Guoteng Wang and Jiaqi Wang and Jiayu Wang and Rui Wang and Yudong Wang and Ziyi Wang and Xingjian Wei and Qizhen Weng and Fan Wu and Yingtong Xiong and Chao Xu and Ruiliang Xu and Hang Yan and Yirong Yan and Xiaogui Yang and Haochen Ye and Huaiyuan Ying and Jia Yu and Jing Yu and Yuhang Zang and Chuyu Zhang and Li Zhang and Pan Zhang and Peng Zhang and Ruijie Zhang and Shuo Zhang and Songyang Zhang and Wenjian Zhang and Wenwei Zhang and Xingcheng Zhang and Xinyue Zhang and Hui Zhao and Qian Zhao and Xiaomeng Zhao and Fengzhe Zhou and Zaida Zhou and Jingming Zhuo and Yicheng Zou and Xipeng Qiu and Yu Qiao and Dahua Lin},\n      year={2024},\n      eprint={2403.17297},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n"
        },
        {
          "name": "agent",
          "type": "tree",
          "content": null
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "chat",
          "type": "tree",
          "content": null
        },
        {
          "name": "ecosystem",
          "type": "tree",
          "content": null
        },
        {
          "name": "finetune",
          "type": "tree",
          "content": null
        },
        {
          "name": "long_context",
          "type": "tree",
          "content": null
        },
        {
          "name": "model_cards",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0419921875,
          "content": "sentencepiece\nstreamlit\ntransformers>=4.38\n"
        },
        {
          "name": "sonar-project.properties",
          "type": "blob",
          "size": 0.06640625,
          "content": "sonar.projectKey=InternLM\nsonar.python.version=3.6,3.7,3.8,3.9,3.10\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}