{
  "metadata": {
    "timestamp": 1736560681737,
    "page": 333,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB",
      "stars": 7242,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2041015625,
          "content": ".idea\ndata/wider_face_add_lm_10_10\n\n__pycache__/\n*.py[cod]\n*$py.class\n\ndetect_imgs_results\ndetect_imgs_results_onnx\nwiderface_evaluation\n\nwiderface_evaluate/build\nwiderface_evaluate/*.so\nwiderface_evaluate/*.c"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.0966796875,
          "content": "[submodule \"ncnn/3rdparty/ncnn\"]\n\tpath = ncnn/3rdparty/ncnn\n\turl = https://github.com/Tencent/ncnn\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0380859375,
          "content": "MIT License\n\nCopyright (c) 2019 linzai\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MNN",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.0263671875,
          "content": "[English](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB ) | [中文简体](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/README_CN.md)\n# Ultra-Light-Fast-Generic-Face-Detector-1MB \n# Ultra-lightweight face detection model\n![img1](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/readme_imgs/27.jpg)\nThis model is a lightweight facedetection model designed for edge computing devices.\n\n- In terms of model size, the default FP32 precision (.pth) file size is **1.04~1.1MB**, and the inference framework int8 quantization size is about **300KB**.\n- In terms of the calculation amount of the model, the input resolution of 320x240 is about **90~109 MFlops**.\n- There are two versions of the model, version-slim (network backbone simplification,slightly faster) and version-RFB (with the modified RFB module, higher precision).\n- Widerface training pre-training model with different input resolutions of 320x240 and 640x480 is provided to better work in different application scenarios.\n- Support for onnx export for ease of migration and inference.\n- [Provide NCNN C++ inference code](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/ncnn).\n- [Provide MNN C++ inference code](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/MNN), [MNN Python inference code](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/MNN/python), [FP32/INT8 quantized models](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/MNN/model).\n- [Provide Caffe model](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/caffe/model) and [onnx2caffe conversion code](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/caffe).\n- [Caffe python inference code](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/caffe/ultra_face_caffe_inference.py) and [OpencvDNN inference code](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/caffe/ultra_face_opencvdnn_inference.py).\n \n## Tested the environment that works\n- Ubuntu16.04、Ubuntu18.04、Windows 10（for inference）\n- Python3.6\n- Pytorch1.2\n- CUDA10.0 + CUDNN7.6\n\n## Accuracy, speed, model size comparison\nThe training set is the VOC format data set generated by using the cleaned widerface labels provided by [Retinaface](https://github.com/deepinsight/insightface/blob/master/RetinaFace/README.md)  in conjunction with the widerface data set (PS: the following test results were obtained by myself, and the results may be partially inconsistent).\n### Widerface test\n- Test accuracy in the WIDER FACE val set (single-scale input resolution: **320*240 or scaling by the maximum side length of 320**)\n\nModel|Easy Set|Medium Set|Hard Set\n------|--------|----------|--------\nlibfacedetection v1（caffe）|0.65 |0.5       |0.233\nlibfacedetection v2（caffe）|0.714 |0.585       |0.306\nRetinaface-Mobilenet-0.25 (Mxnet)   |0.745|0.553|0.232\nversion-slim|0.77     |0.671       |0.395\nversion-RFB|**0.787**     |**0.698**       |**0.438**\n\n\n- Test accuracy in the WIDER FACE val set (single-scale input resolution: **VGA 640*480 or scaling by the maximum side length of 640** )\n\nModel|Easy Set|Medium Set|Hard Set\n------|--------|----------|--------\nlibfacedetection v1（caffe）|0.741 |0.683       |0.421\nlibfacedetection v2（caffe）|0.773 |0.718       |0.485\nRetinaface-Mobilenet-0.25 (Mxnet)   |**0.879**|0.807|0.481\nversion-slim|0.853     |0.819       |0.539\nversion-RFB|0.855     |**0.822**       |**0.579**\n\n> - This part mainly tests the effect of the test set under the medium and small resolutions.\n> - RetinaFace-mnet (Retinaface-Mobilenet-0.25), from a great job [insightface](https://github.com/deepinsight/insightface), when testing this network, the original image is scaled by 320 or 640 as the maximum side length, so the face will not be deformed, and the rest of the networks will have a fixed size resize. At the same time, the result of the RetinaFace-mnet optimal 1600 single-scale val set was 0.887 (Easy) / 0.87 (Medium) / 0.791 (Hard).\n\n### Terminal device inference speed\n\n- Raspberry Pi 4B MNN Inference Latency **(unit: ms)** (ARM/A72x4/1.5GHz/input resolution: **320x240** /int8 quantization)\n\nModel|1 core|2 core|3 core|4 core\n------|--------|----------|--------|--------\nlibfacedetection v1|**28**    |**16**|**12**|9.7\nOfficial Retinaface-Mobilenet-0.25 (Mxnet)   |46|25|18.5|15\nversion-slim|29     |**16**       |**12**|**9.5**\nversion-RFB|35     |19.6       |14.8| 11\n\n- iPhone 6s Plus MNN (version tag：0.2.1.5) Inference Latency ( input resolution : **320x240** )[Data comes from  MNN official](https://www.yuque.com/mnn/en/demo_zoo#bXsRY)\n\nModel|Inference Latency(ms)\n------|--------\n[slim-320](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/MNN/model/version-slim/slim-320.mnn) |6.33\n[RFB-320](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/MNN/model/version-RFB/RFB-320.mnn)|7.8\n\n- [Kendryte K210](https://kendryte.com/) NNCase Inference Latency (RISC-V/400MHz/input resolution: **320x240** /int8 quantization)[Data comes from NNCase](https://github.com/kendryte/nncase/tree/master/examples/fast_facedetect)\n\nModel|Inference Latency(ms)\n------|--------\n[slim-320](https://github.com/kendryte/nncase/tree/master/examples/fast_facedetect/k210/kpu_fast_facedetect_example/slim-320.kmodel)|65.6\n[RFB-320](https://github.com/kendryte/nncase/tree/master/examples/fast_facedetect/k210/kpu_fast_facedetect_example/RFB-320.kmodel)|164.8\n\n### Model size comparison\n- Comparison of several open source lightweight face detection models:\n\nModel|model file size（MB）\n------|--------\nlibfacedetection v1（caffe）| 2.58\nlibfacedetection v2（caffe）| 3.34\nOfficial Retinaface-Mobilenet-0.25 (Mxnet) | 1.68\nversion-slim| **1.04**\nversion-RFB| **1.11** \n\n## Generate VOC format training data set and training process\n\n1. Download the wideface official website dataset or download the training set I provided and extract it into the ./data folder:\n\n  (1) The clean widerface data pack after filtering out the 10px*10px small face: [Baidu cloud disk (extraction code: cbiu)](https://pan.baidu.com/s/1MR0ZOKHUP_ArILjbAn03sw) 、[Google Drive](https://drive.google.com/open?id=1OBY-Pk5hkcVBX1dRBOeLI4e4OCvqJRnH )\n  \n  (2) Complete widerface data compression package without filtering small faces: [Baidu cloud disk (extraction code: ievk)](https://pan.baidu.com/s/1faHNz9ZrtEmr_yw48GW7ZA)、[Google Drive](https://drive.google.com/open?id=1sbBrDRgctEkymIpCh1OZBrU5qBS-SnCP )\n\n2. **(PS: If you download the filtered packets in (1) above, you don't need to perform this step)** Because the wideface has many small and unclear faces, which is not conducive to the convergence of efficient models, it needs to be filtered for training.By default,faces smaller than 10 pixels by 10 pixels will be filtered.\nrun ./data/wider_face_2_voc_add_landmark.py\n```Python\n python3 ./data/wider_face_2_voc_add_landmark.py\n```\nAfter the program is run and finished, the **wider_face_add_lm_10_10** folder will be generated in the ./data directory. The folder data and data package (1) are the same after decompression. The complete directory structure is as follows:\n```Shell\n  data/\n    retinaface_labels/\n      test/\n      train/\n      val/\n    wider_face/\n      WIDER_test/\n      WIDER_train/\n      WIDER_val/\n    wider_face_add_lm_10_10/\n      Annotations/\n      ImageSets/\n      JPEGImages/\n    wider_face_2_voc_add_landmark.py\n```\n\n3. At this point, the VOC training set is ready. There are two scripts: **train-version-slim.sh** and **train-version-RFB.sh** in the root directory of the project. The former is used to train the **slim version** model, and the latter is used. Training **RFB version** model, the default parameters have been set, if the parameters need to be changed, please refer to the description of each training parameter in **./train.py**.\n\n4. Run **train-version-slim.sh**  **train-version-RFB.sh**\n```Shell\nsh train-version-slim.sh or sh train-version-RFB.sh\n```\n\n## Detecting image effects (input resolution: 640x480)\n![img1](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/readme_imgs/26.jpg)\n![img1](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/readme_imgs/2.jpg)\n![img1](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/readme_imgs/4.jpg)\n## PS\n\n- If the actual production scene is medium-distance, large face, and small number of faces, it is recommended to use input size input_size: 320 (320x240) resolution for training, and use 320x240 ,160x120 or 128x96 image size input for inference, such as using the provided pre-training  model **version-slim-320.pth** or **version-RFB-320.pth** .\n- If the actual production scene is medium or long distance,  medium or small face and large face number, it is recommended to adopt:\n\n (1) Optimal: input size input_size: 640 (640x480) resolution training, and use the same or larger input size for inference, such as using the provided pre-training model **version-slim-640.pth** or **version-RFB-640.pth** for inference, lower False positives.\n \n (2) Sub-optimal: input size input_size: 320 (320x240) resolution training, and use 480x360 or 640x480 size input for predictive reasoning, more sensitive to small faces, false positives will increase.\n \n- The best results for each scene require adjustment of the input resolution to strike a balance between speed and accuracy.\n- Excessive input resolution will enhance the recall rate of small faces, but it will also increase the false positive rate of large and close-range faces, and the speed of inference will increase exponentially.\n- Too small input resolution will significantly speed up the inference, but it will greatly reduce the recall rate of small faces.\n- The input resolution of the production scene should be as consistent as possible with the input resolution of the model training, and the up and down floating should not be too large.\n\n## TODO LIST\n\n- Add some test data\n\n## Completed list\n - [Widerface test code](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/widerface_evaluate)\n - [NCNN C++ inference code](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/ncnn) ([vealocia](https://github.com/vealocia))\n - [MNN C++ inference code](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/MNN), [MNN Python inference code](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/MNN/python)\n - [Caffe model](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/caffe/model) and [onnx2caffe conversion code](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/caffe)\n - [Caffe python inference code](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/caffe/ultra_face_caffe_inference.py) and [OpencvDNN inference code](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/caffe/ultra_face_opencvdnn_inference.py)\n  \n## Third-party related projects\n - [NNCase C++ inference code](https://github.com/kendryte/nncase/tree/master/examples/fast_facedetect)\n - [UltraFaceDotNet (C#)](https://github.com/takuya-takeuchi/UltraFaceDotNet)\n - [faceDetect-ios](https://github.com/Ian778/faceDetect-ios)\n - [Android-FaceDetection-UltraNet-MNN](https://github.com/jackweiwang/Android-FaceDetection-UltraNet-MNN)\n - [Ultra-Tensorflow-Model-Converter](https://github.com/jason9075/Ultra-Light-Fast-Generic-Face-Detector_Tensorflow-Model-Converter)\n - [UltraFace TNN C++ Demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/lite/tnn/cv/tnn_ultraface.cpp)\n  \n##  Reference\n- [pytorch-ssd](https://github.com/qfgaohao/pytorch-ssd)\n- [libfacedetection](https://github.com/ShiqiYu/libfacedetection/)\n- [RFBNet](https://github.com/ruinmessi/RFBNet)\n- [RFSong-779](https://github.com/songwsx/RFSong-779)\n- [Retinaface](https://github.com/deepinsight/insightface/blob/master/RetinaFace/README.md)\n"
        },
        {
          "name": "README_CN.md",
          "type": "blob",
          "size": 11.0576171875,
          "content": "[English](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB) | [中文简体](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/README_CN.md )\n# Ultra-Light-Fast-Generic-Face-Detector-1MB \n# 轻量级人脸检测模型\n![img1](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/readme_imgs/27.jpg)\n该模型是针对边缘计算设备设计的轻量人脸检测模型。\n\n - 在模型大小上，默认FP32精度下（.pth）文件大小为 **1.04~1.1MB**，推理框架int8量化后大小为 **300KB** 左右。\n - 在模型计算量上，320x240的输入分辨率下 **90~109 MFlops**左右。\n - 模型有两个版本，version-slim(主干精简速度略快)，version-RFB(加入了修改后的RFB模块，精度更高)。\n - 提供320x240、640x480不同输入分辨率下使用widerface训练的预训练模型，更好的工作于不同的应用场景。\n - 支持onnx导出。\n - [提供NCNN C++推理代码](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/ncnn)。\n - [提供MNN C++推理代码](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/MNN)、[MNN Python推理代码](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/MNN/python)、[FP32/INT8量化后模型](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/MNN/model)。\n - [提供转换后的Caffe model](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/caffe/model) 和 [onnx2caffe 转换代码](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/caffe)。\n - [Caffe python 推理代码](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/caffe/ultra_face_caffe_inference.py) 和 [OpencvDNN推理代码](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/caffe/ultra_face_opencvdnn_inference.py)。\n\n## 测试过正常的运行环境\n- Ubuntu16.04、Ubuntu18.04、Windows 10（inference）\n- Python3.6\n- Pytorch1.2\n- CUDA10.0 + CUDNN7.6\n\n## 精度、速度、模型大小比较\n训练集是使用[Retinaface](https://github.com/deepinsight/insightface/blob/master/RetinaFace/README.md )提供的清理过的widerface标签配合widerface数据集生成VOC训练集（PS:以下测试结果为本人测试，结果可能有部分出入）。\n### Widerface测试\n - 在WIDER FACE val集测试精度（单尺度输入分辨率：**320*240 或按最大边长320等比缩放**） \n\n模型|Easy Set|Medium Set|Hard Set\n------|--------|----------|--------\nlibfacedetection v1（caffe）|0.65 |0.5       |0.233\nlibfacedetection v2（caffe）|0.714 |0.585       |0.306\nRetinaface-Mobilenet-0.25 (Mxnet)   |0.745|0.553|0.232\nversion-slim|0.77     |0.671       |0.395\nversion-RFB|**0.787**     |**0.698**       |**0.438**\n\n\n- 在WIDER FACE val集测试精度（单尺度输入分辨率：**VGA 640*480 或按最大边长640等比缩放** ） \n\n模型|Easy Set|Medium Set|Hard Set\n------|--------|----------|--------\nlibfacedetection v1（caffe）|0.741 |0.683       |0.421\nlibfacedetection v2（caffe）|0.773 |0.718       |0.485\nRetinaface-Mobilenet-0.25 (Mxnet)   |**0.879**|0.807|0.481\nversion-slim|0.853     |0.819       |0.539\nversion-RFB|0.855     |**0.822**       |**0.579**\n\n> - 该部分主要是测试模型在中小分辨率下的测试集效果。\n> - RetinaFace-mnet（Retinaface-Mobilenet-0.25），来自于很棒的工作[insightface](https://github.com/deepinsight/insightface)，测试该网络时是将原图按最大边长320或者640等比缩放，所以人脸不会形变,其余网络采用固定尺寸resize。同时RetinaFace-mnet最优1600单尺度val测试集结果为0.887(Easy)/0.87(Medium)/0.791(Hard)。\n\n### 终端设备推理速度\n\n- 树莓派4B MNN推理测试耗时 **(单位：ms)**（ARM/A72x4/1.5GHz/输入分辨率 : **320x240** /int8 量化 ） \n\n模型|1核|2核|3核|4核\n------|--------|----------|--------|--------\nlibfacedetection v1|**28**    |**16**|**12**|9.7\n官方 Retinaface-Mobilenet-0.25 (Mxnet)   |46|25|18.5|15\nversion-slim|29     |**16**       |**12**|**9.5**\nversion-RFB|35     |19.6       |14.8| 11\n\n- iPhone 6s Plus MNN (版本Tag：0.2.1.5) 推理测试耗时（输入分辨率 : **320x240** ）[数据来自MNN官方](https://www.yuque.com/mnn/en/demo_zoo#bXsRY)\n\n模型|耗时（ms）\n------|--------\n[slim-320](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/MNN/model/version-slim/slim-320.mnn) |6.33\n[RFB-320](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/MNN/model/version-RFB/RFB-320.mnn)|7.8\n\n- [Kendryte K210](https://kendryte.com/) NNCase 推理测试耗时 (RISC-V/400MHz/输入分辨率 : **320x240** /int8 量化) [数据来自NNCase](https://github.com/kendryte/nncase/tree/master/examples/fast_facedetect)\n\nModel|Inference Latency(ms)\n------|--------\n[slim-320](https://github.com/kendryte/nncase/tree/master/examples/fast_facedetect/k210/kpu_fast_facedetect_example/slim-320.kmodel)|65.6\n[RFB-320](https://github.com/kendryte/nncase/tree/master/examples/fast_facedetect/k210/kpu_fast_facedetect_example/RFB-320.kmodel)|164.8\n\n### 模型大小比较\n- 若干开源轻量级人脸检测模型大小比较 ：\n\n模型|模型文件大小（MB）\n------|--------\nlibfacedetection v1（caffe）| 2.58\nlibfacedetection v2（caffe）| 3.34\n官方 Retinaface-Mobilenet-0.25 (Mxnet) | 1.68\nversion-slim| **1.04**\nversion-RFB| **1.11** \n\n## 生成VOC格式训练数据集以及训练流程\n\n1. 下载widerface官网数据集或者下载我提供的训练集解压放入./data文件夹内：\n\n  （1）过滤掉10px*10px 小人脸后的干净widerface数据压缩包 ：[百度云盘 (提取码：cbiu)](https://pan.baidu.com/s/1MR0ZOKHUP_ArILjbAn03sw ) 、[Google Drive](https://drive.google.com/open?id=1OBY-Pk5hkcVBX1dRBOeLI4e4OCvqJRnH )\n  \n  （2）未过滤小人脸的完整widerface数据压缩包 ：[百度云盘 (提取码：ievk)](https://pan.baidu.com/s/1faHNz9ZrtEmr_yw48GW7ZA ) 、[Google Drive](https://drive.google.com/open?id=1sbBrDRgctEkymIpCh1OZBrU5qBS-SnCP )\n  \n2. **（PS:如果下载的是过滤后的上述(1)中的数据包，则不需要执行这步）** 由于widerface存在很多极小的不清楚的人脸，不太利于高效模型的收敛，所以需要过滤训练，默认过滤人脸大小10像素x10像素以下的人脸。\n运行./data/wider_face_2_voc_add_landmark.py\n```Python\n python3 ./data/wider_face_2_voc_add_landmark.py\n```\n程序运行和完毕后会在./data目录下生成 **wider_face_add_lm_10_10**文件夹，该文件夹数据和数据包（1）解压后相同，完整目录结构如下：\n```Shell\n  data/\n    retinaface_labels/\n      test/\n      train/\n      val/\n    wider_face/\n      WIDER_test/\n      WIDER_train/\n      WIDER_val/\n    wider_face_add_lm_10_10/\n      Annotations/\n      ImageSets/\n      JPEGImages/\n    wider_face_2_voc_add_landmark.py\n```\n\n3. 至此VOC训练集准备完毕，项目根目录下分别有 **train-version-slim.sh** 和 **train-version-RFB.sh** 两个脚本，前者用于训练**slim版本**模型，后者用于训练**RFB版本**模型，默认参数已设置好，参数如需微调请参考 **./train.py** 中关于各训练超参数的说明。\n\n4. 运行**train-version-slim.sh** 或 **train-version-RFB.sh**即可\n```Shell\nsh train-version-slim.sh 或者 sh train-version-RFB.sh\n```\n\n## 检测图片效果（输入分辨率：640x480）\n![img1](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/readme_imgs/26.jpg)\n![img1](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/readme_imgs/2.jpg)\n![img1](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/readme_imgs/4.jpg)\n## PS\n\n - 若生产实际场景为中近距离、人脸大、人脸数少，则建议采用输入尺寸input_size：320（320x240）分辨率训练，并采用 320x240/160x120/128x96 图片大小输入进行预测推理，如使用提供的预训练模型 **version-slim-320.pth** 或者 **version-RFB-320.pth** 进行推理。\n - 若生产实际场景为中远距离、人脸中小、人脸数多，则建议采用：\n \n （1）最优：输入尺寸input_size：640（640x480）分辨率训练，并采用同等或更大输入尺寸进行预测推理,如使用提供的预训练模型 **version-slim-640.pth** 或者 **version-RFB-640.pth** 进行推理，更低的误报。\n \n （2）次优：输入尺寸input_size：320（320x240）分辨率训练，并采用480x360或640x480大小输入进行预测推理，对于小人脸更敏感，误报会增加。\n \n - 各个场景的最佳效果需要调整输入分辨率从而在速度和精度中间取得平衡。\n - 过大的输入分辨率虽然会增强小人脸的召回率，但是也会提高大、近距离人脸的误报率，而且推理速度延迟成倍增加。\n - 过小的输入分辨率虽然会明显加快推理速度，但是会大幅降低小人脸的召回率。\n - 生产场景的输入分辨率尽量与模型训练时的输入分辨率保持一致，上下浮动不宜过大。\n \n\n## TODO LIST\n\n - 完善测试数据\n\n \n## 已完成功能清单\n - [Widerface测试代码](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/widerface_evaluate)\n - [NCNN C++推理代码](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/ncnn) ([vealocia](https://github.com/vealocia))\n - [MNN C++推理代码](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/MNN)、[MNN Python推理代码](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/MNN/python)\n - [Caffe model](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/caffe/model) 和 [onnx2caffe 转换代码](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/tree/master/caffe)\n - [Caffe python 推理代码](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/caffe/ultra_face_caffe_inference.py) 和 [OpencvDNN推理代码](https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB/blob/master/caffe/ultra_face_opencvdnn_inference.py)\n  \n## 第三方相关工程\n - [NNCase C++推理代码](https://github.com/kendryte/nncase/tree/master/examples/fast_facedetect)\n - [UltraFaceDotNet (C#)](https://github.com/takuya-takeuchi/UltraFaceDotNet)\n - [faceDetect-ios](https://github.com/Ian778/faceDetect-ios)\n - [Android-FaceDetection-UltraNet-MNN](https://github.com/jackweiwang/Android-FaceDetection-UltraNet-MNN)\n - [Ultra-Tensorflow-Model-Converter](https://github.com/jason9075/Ultra-Light-Fast-Generic-Face-Detector_Tensorflow-Model-Converter)\n - [UltraFace TNN C++ Demo](https://github.com/DefTruth/lite.ai.toolkit/blob/main/lite/tnn/cv/tnn_ultraface.cpp)\n\n##  Reference\n - [pytorch-ssd](https://github.com/qfgaohao/pytorch-ssd)\n - [libfacedetection](https://github.com/ShiqiYu/libfacedetection/)\n - [RFBNet](https://github.com/ruinmessi/RFBNet)\n - [RFSong-779](https://github.com/songwsx/RFSong-779)\n - [Retinaface](https://github.com/deepinsight/insightface/blob/master/RetinaFace/README.md)\n"
        },
        {
          "name": "caffe",
          "type": "tree",
          "content": null
        },
        {
          "name": "cal_flops.py",
          "type": "blob",
          "size": 0.8349609375,
          "content": "\"\"\"\nOutput model complexity\n\"\"\"\nimport time\n\nimport torch\nfrom torchstat import stat\nfrom torchsummary import summary\n\nfrom vision.ssd.mb_tiny_fd import create_mb_tiny_fd\nfrom vision.ssd.mb_tiny_RFB_fd import create_Mb_Tiny_RFB_fd\n\ndevice = \"cpu\"  # default cpu\nwidth = 320\nheight = 240\n\n# fd = create_mb_tiny_fd(2)\nfd = create_Mb_Tiny_RFB_fd(2)\n\nprint(fd)\nfd.eval()\nfd.to(device)\nx = torch.randn(1, 3, width, height).to(device)\n\nsummary(fd.to(\"cuda\"), (3, width, height))\n\nfrom ptflops import get_model_complexity_info\n\nflops, params = get_model_complexity_info(fd.to(device), (3, width, height), print_per_layer_stat=True, as_strings=True)\nprint(\"FLOPS:\", flops)\nprint(\"PARAMS:\", params)\n\nfor i in range(5):\n    time_time = time.time()\n    features = fd(x)\n    print(\"inference time :{} s\".format(time.time() - time_time))\n\nstat(fd, (3, width, height))\n"
        },
        {
          "name": "check_gt_box.py",
          "type": "blob",
          "size": 2.171875,
          "content": "\"\"\"\nThis code is used to check the data size distribution in the dataset.\n\"\"\"\nimport xml.etree.ElementTree as ET\nfrom math import sqrt as sqrt\n\nimport cv2\nimport matplotlib.pyplot as plt\n\n# sets = [(\"./data/wider_face_add_lm_10_10\", \"trainval\")]\nsets = [(\"./data/wider_face_add_lm_10_10\", \"test\")]\n\nclasses = ['face']\n\nif __name__ == '__main__':\n    width = []\n    height = []\n\n    for image_set, set in sets:\n        image_ids = open('{}/ImageSets/Main/{}.txt'.format(image_set, set)).read().strip().split()\n        for image_id in image_ids:\n            img_path = '{}/JPEGImages/{}.jpg'.format(image_set, image_id)\n            label_file = open('{}/Annotations/{}.xml'.format(image_set, image_id))\n            tree = ET.parse(label_file)\n            root = tree.getroot()\n            size = root.find('size')\n            img_w = int(size.find('width').text)\n            img_h = int(size.find('height').text)\n            img = cv2.imread(img_path)\n            for obj in root.iter('object'):\n                difficult = obj.find('difficult').text\n                cls = obj.find('name').text\n                if cls not in classes or int(difficult) == 2:\n                    continue\n                cls_id = classes.index(cls)\n\n                xmlbox = obj.find('bndbox')\n                xmin = int(xmlbox.find('xmin').text)\n                ymin = int(xmlbox.find('ymin').text)\n                xmax = int(xmlbox.find('xmax').text)\n                ymax = int(xmlbox.find('ymax').text)\n                w = xmax - xmin\n                h = ymax - ymin\n\n                # img = cv2.rectangle(img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 8)\n                w_change = (w / img_w) * 320\n                h_change = (h / img_h) * 240\n                s = w_change * h_change\n                if w_change / h_change > 6:\n                    print(\"{}/{}/{}/{}\".format(xmin, xmax, ymin, ymax))\n                width.append(sqrt(s))\n                height.append(w_change / h_change)\n            print(img_path)\n            # img = cv2.resize(img, (608, 608))\n            # cv2.imwrite('{}_{}'.format(image_set.split('/')[-1], set), img)\n            # cv2.waitKey()\n\n    plt.plot(width, height, 'ro')\n    plt.show()\n"
        },
        {
          "name": "convert_to_onnx.py",
          "type": "blob",
          "size": 1.4873046875,
          "content": "\"\"\"\nThis code is used to convert the pytorch model into an onnx format model.\n\"\"\"\nimport sys\n\nimport torch.onnx\n\nfrom vision.ssd.config.fd_config import define_img_size\n\ninput_img_size = 320  # define input size ,default optional(128/160/320/480/640/1280)\ndefine_img_size(input_img_size)\nfrom vision.ssd.mb_tiny_RFB_fd import create_Mb_Tiny_RFB_fd\nfrom vision.ssd.mb_tiny_fd import create_mb_tiny_fd\n\n# net_type = \"slim\"  # inference faster,lower precision\nnet_type = \"RFB\"  # inference lower,higher precision\n\nlabel_path = \"models/voc-model-labels.txt\"\nclass_names = [name.strip() for name in open(label_path).readlines()]\nnum_classes = len(class_names)\n\nif net_type == 'slim':\n    model_path = \"models/pretrained/version-slim-320.pth\"\n    # model_path = \"models/pretrained/version-slim-640.pth\"\n    net = create_mb_tiny_fd(len(class_names), is_test=True)\nelif net_type == 'RFB':\n    model_path = \"models/pretrained/version-RFB-320.pth\"\n    # model_path = \"models/pretrained/version-RFB-640.pth\"\n    net = create_Mb_Tiny_RFB_fd(len(class_names), is_test=True)\n\nelse:\n    print(\"unsupport network type.\")\n    sys.exit(1)\nnet.load(model_path)\nnet.eval()\nnet.to(\"cuda\")\n\nmodel_name = model_path.split(\"/\")[-1].split(\".\")[0]\nmodel_path = f\"models/onnx/{model_name}.onnx\"\n\ndummy_input = torch.randn(1, 3, 240, 320).to(\"cuda\")\n# dummy_input = torch.randn(1, 3, 480, 640).to(\"cuda\") #if input size is 640*480\ntorch.onnx.export(net, dummy_input, model_path, verbose=False, input_names=['input'], output_names=['scores', 'boxes'])\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "detect_imgs.py",
          "type": "blob",
          "size": 3.1796875,
          "content": "\"\"\"\nThis code is used to batch detect images in a folder.\n\"\"\"\nimport argparse\nimport os\nimport sys\n\nimport cv2\n\nfrom vision.ssd.config.fd_config import define_img_size\n\nparser = argparse.ArgumentParser(\n    description='detect_imgs')\n\nparser.add_argument('--net_type', default=\"RFB\", type=str,\n                    help='The network architecture ,optional: RFB (higher precision) or slim (faster)')\nparser.add_argument('--input_size', default=640, type=int,\n                    help='define network input size,default optional value 128/160/320/480/640/1280')\nparser.add_argument('--threshold', default=0.6, type=float,\n                    help='score threshold')\nparser.add_argument('--candidate_size', default=1500, type=int,\n                    help='nms candidate size')\nparser.add_argument('--path', default=\"imgs\", type=str,\n                    help='imgs dir')\nparser.add_argument('--test_device', default=\"cuda:0\", type=str,\n                    help='cuda:0 or cpu')\nargs = parser.parse_args()\ndefine_img_size(args.input_size)  # must put define_img_size() before 'import create_mb_tiny_fd, create_mb_tiny_fd_predictor'\n\nfrom vision.ssd.mb_tiny_fd import create_mb_tiny_fd, create_mb_tiny_fd_predictor\nfrom vision.ssd.mb_tiny_RFB_fd import create_Mb_Tiny_RFB_fd, create_Mb_Tiny_RFB_fd_predictor\n\nresult_path = \"./detect_imgs_results\"\nlabel_path = \"./models/voc-model-labels.txt\"\ntest_device = args.test_device\n\nclass_names = [name.strip() for name in open(label_path).readlines()]\nif args.net_type == 'slim':\n    model_path = \"models/pretrained/version-slim-320.pth\"\n    # model_path = \"models/pretrained/version-slim-640.pth\"\n    net = create_mb_tiny_fd(len(class_names), is_test=True, device=test_device)\n    predictor = create_mb_tiny_fd_predictor(net, candidate_size=args.candidate_size, device=test_device)\nelif args.net_type == 'RFB':\n    model_path = \"models/pretrained/version-RFB-320.pth\"\n    # model_path = \"models/pretrained/version-RFB-640.pth\"\n    net = create_Mb_Tiny_RFB_fd(len(class_names), is_test=True, device=test_device)\n    predictor = create_Mb_Tiny_RFB_fd_predictor(net, candidate_size=args.candidate_size, device=test_device)\nelse:\n    print(\"The net type is wrong!\")\n    sys.exit(1)\nnet.load(model_path)\n\nif not os.path.exists(result_path):\n    os.makedirs(result_path)\nlistdir = os.listdir(args.path)\nsum = 0\nfor file_path in listdir:\n    img_path = os.path.join(args.path, file_path)\n    orig_image = cv2.imread(img_path)\n    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    boxes, labels, probs = predictor.predict(image, args.candidate_size / 2, args.threshold)\n    sum += boxes.size(0)\n    for i in range(boxes.size(0)):\n        box = boxes[i, :]\n        cv2.rectangle(orig_image, (box[0], box[1]), (box[2], box[3]), (0, 0, 255), 2)\n        # label = f\"\"\"{voc_dataset.class_names[labels[i]]}: {probs[i]:.2f}\"\"\"\n        label = f\"{probs[i]:.2f}\"\n        # cv2.putText(orig_image, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n    cv2.putText(orig_image, str(boxes.size(0)), (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n    cv2.imwrite(os.path.join(result_path, file_path), orig_image)\n    print(f\"Found {len(probs)} faces. The output image is {result_path}\")\nprint(sum)\n"
        },
        {
          "name": "detect_imgs_onnx.py",
          "type": "blob",
          "size": 3.412109375,
          "content": "\"\"\"\nThis code uses the onnx model to detect faces from live video or cameras.\n\"\"\"\nimport os\nimport time\n\nimport cv2\nimport numpy as np\nimport onnx\nimport vision.utils.box_utils_numpy as box_utils\nfrom caffe2.python.onnx import backend\n\n# onnx runtime\nimport onnxruntime as ort\n\n\ndef predict(width, height, confidences, boxes, prob_threshold, iou_threshold=0.3, top_k=-1):\n    boxes = boxes[0]\n    confidences = confidences[0]\n    picked_box_probs = []\n    picked_labels = []\n    for class_index in range(1, confidences.shape[1]):\n        probs = confidences[:, class_index]\n        mask = probs > prob_threshold\n        probs = probs[mask]\n        if probs.shape[0] == 0:\n            continue\n        subset_boxes = boxes[mask, :]\n        box_probs = np.concatenate([subset_boxes, probs.reshape(-1, 1)], axis=1)\n        box_probs = box_utils.hard_nms(box_probs,\n                                       iou_threshold=iou_threshold,\n                                       top_k=top_k,\n                                       )\n        picked_box_probs.append(box_probs)\n        picked_labels.extend([class_index] * box_probs.shape[0])\n    if not picked_box_probs:\n        return np.array([]), np.array([]), np.array([])\n    picked_box_probs = np.concatenate(picked_box_probs)\n    picked_box_probs[:, 0] *= width\n    picked_box_probs[:, 1] *= height\n    picked_box_probs[:, 2] *= width\n    picked_box_probs[:, 3] *= height\n    return picked_box_probs[:, :4].astype(np.int32), np.array(picked_labels), picked_box_probs[:, 4]\n\n\nlabel_path = \"models/voc-model-labels.txt\"\n\nonnx_path = \"models/onnx/version-RFB-320.onnx\"\nclass_names = [name.strip() for name in open(label_path).readlines()]\n\npredictor = onnx.load(onnx_path)\nonnx.checker.check_model(predictor)\nonnx.helper.printable_graph(predictor.graph)\npredictor = backend.prepare(predictor, device=\"CPU\")  # default CPU\n\nort_session = ort.InferenceSession(onnx_path)\ninput_name = ort_session.get_inputs()[0].name\nresult_path = \"./detect_imgs_results_onnx\"\n\nthreshold = 0.7\npath = \"imgs\"\nsum = 0\nif not os.path.exists(result_path):\n    os.makedirs(result_path)\nlistdir = os.listdir(path)\nsum = 0\nfor file_path in listdir:\n    img_path = os.path.join(path, file_path)\n    orig_image = cv2.imread(img_path)\n    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (320, 240))\n    # image = cv2.resize(image, (640, 480))\n    image_mean = np.array([127, 127, 127])\n    image = (image - image_mean) / 128\n    image = np.transpose(image, [2, 0, 1])\n    image = np.expand_dims(image, axis=0)\n    image = image.astype(np.float32)\n    # confidences, boxes = predictor.run(image)\n    time_time = time.time()\n    confidences, boxes = ort_session.run(None, {input_name: image})\n    print(\"cost time:{}\".format(time.time() - time_time))\n    boxes, labels, probs = predict(orig_image.shape[1], orig_image.shape[0], confidences, boxes, threshold)\n    for i in range(boxes.shape[0]):\n        box = boxes[i, :]\n        label = f\"{class_names[labels[i]]}: {probs[i]:.2f}\"\n\n        cv2.rectangle(orig_image, (box[0], box[1]), (box[2], box[3]), (255, 255, 0), 4)\n\n        # cv2.putText(orig_image, label,\n        #             (box[0] + 20, box[1] + 40),\n        #             cv2.FONT_HERSHEY_SIMPLEX,\n        #             1,  # font scale\n        #             (255, 0, 255),\n        #             2)  # line type\n        cv2.imwrite(os.path.join(result_path, file_path), orig_image)\n    sum += boxes.shape[0]\nprint(\"sum:{}\".format(sum))\n"
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "masked_face",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "ncnn",
          "type": "tree",
          "content": null
        },
        {
          "name": "opencv_dnn",
          "type": "tree",
          "content": null
        },
        {
          "name": "paddle",
          "type": "tree",
          "content": null
        },
        {
          "name": "readme_imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1005859375,
          "content": "numpy\ntorch\nopencv_python\ntorchvision\ntyping\ntorchstat\ntorchsummary\nptflops\nmatplotlib\nonnx\nonnxruntime"
        },
        {
          "name": "run_video_face_detect.py",
          "type": "blob",
          "size": 3.63671875,
          "content": "\"\"\"\nThis code uses the pytorch model to detect faces from live video or camera.\n\"\"\"\nimport argparse\nimport sys\nimport cv2\n\nfrom vision.ssd.config.fd_config import define_img_size\n\nparser = argparse.ArgumentParser(\n    description='detect_video')\n\nparser.add_argument('--net_type', default=\"RFB\", type=str,\n                    help='The network architecture ,optional: RFB (higher precision) or slim (faster)')\nparser.add_argument('--input_size', default=480, type=int,\n                    help='define network input size,default optional value 128/160/320/480/640/1280')\nparser.add_argument('--threshold', default=0.7, type=float,\n                    help='score threshold')\nparser.add_argument('--candidate_size', default=1000, type=int,\n                    help='nms candidate size')\nparser.add_argument('--path', default=\"imgs\", type=str,\n                    help='imgs dir')\nparser.add_argument('--test_device', default=\"cuda:0\", type=str,\n                    help='cuda:0 or cpu')\nparser.add_argument('--video_path', default=\"/home/linzai/Videos/video/16_1.MP4\", type=str,\n                    help='path of video')\nargs = parser.parse_args()\n\ninput_img_size = args.input_size\ndefine_img_size(input_img_size)  # must put define_img_size() before 'import create_mb_tiny_fd, create_mb_tiny_fd_predictor'\n\nfrom vision.ssd.mb_tiny_fd import create_mb_tiny_fd, create_mb_tiny_fd_predictor\nfrom vision.ssd.mb_tiny_RFB_fd import create_Mb_Tiny_RFB_fd, create_Mb_Tiny_RFB_fd_predictor\nfrom vision.utils.misc import Timer\n\nlabel_path = \"./models/voc-model-labels.txt\"\n\nnet_type = args.net_type\n\ncap = cv2.VideoCapture(args.video_path)  # capture from video\n# cap = cv2.VideoCapture(0)  # capture from camera\n\nclass_names = [name.strip() for name in open(label_path).readlines()]\nnum_classes = len(class_names)\ntest_device = args.test_device\n\ncandidate_size = args.candidate_size\nthreshold = args.threshold\n\nif net_type == 'slim':\n    model_path = \"models/pretrained/version-slim-320.pth\"\n    # model_path = \"models/pretrained/version-slim-640.pth\"\n    net = create_mb_tiny_fd(len(class_names), is_test=True, device=test_device)\n    predictor = create_mb_tiny_fd_predictor(net, candidate_size=candidate_size, device=test_device)\nelif net_type == 'RFB':\n    model_path = \"models/pretrained/version-RFB-320.pth\"\n    # model_path = \"models/pretrained/version-RFB-640.pth\"\n    net = create_Mb_Tiny_RFB_fd(len(class_names), is_test=True, device=test_device)\n    predictor = create_Mb_Tiny_RFB_fd_predictor(net, candidate_size=candidate_size, device=test_device)\nelse:\n    print(\"The net type is wrong!\")\n    sys.exit(1)\nnet.load(model_path)\n\ntimer = Timer()\nsum = 0\nwhile True:\n    ret, orig_image = cap.read()\n    if orig_image is None:\n        print(\"end\")\n        break\n    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    timer.start()\n    boxes, labels, probs = predictor.predict(image, candidate_size / 2, threshold)\n    interval = timer.end()\n    print('Time: {:.6f}s, Detect Objects: {:d}.'.format(interval, labels.size(0)))\n    for i in range(boxes.size(0)):\n        box = boxes[i, :]\n        label = f\" {probs[i]:.2f}\"\n        cv2.rectangle(orig_image, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 4)\n\n        # cv2.putText(orig_image, label,\n        #             (box[0], box[1] - 10),\n        #             cv2.FONT_HERSHEY_SIMPLEX,\n        #             0.5,  # font scale\n        #             (0, 0, 255),\n        #             2)  # line type\n    orig_image = cv2.resize(orig_image, None, None, fx=0.8, fy=0.8)\n    sum += boxes.size(0)\n    cv2.imshow('annotated', orig_image)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\ncap.release()\ncv2.destroyAllWindows()\nprint(\"all face num:{}\".format(sum))\n"
        },
        {
          "name": "run_video_face_detect_onnx.py",
          "type": "blob",
          "size": 3.458984375,
          "content": "\"\"\"\nThis code uses the onnx model to detect faces from live video or cameras.\n\"\"\"\nimport time\n\nimport cv2\nimport numpy as np\nimport onnx\nimport vision.utils.box_utils_numpy as box_utils\nfrom caffe2.python.onnx import backend\n\n# onnx runtime\nimport onnxruntime as ort\n\n\ndef predict(width, height, confidences, boxes, prob_threshold, iou_threshold=0.3, top_k=-1):\n    boxes = boxes[0]\n    confidences = confidences[0]\n    picked_box_probs = []\n    picked_labels = []\n    for class_index in range(1, confidences.shape[1]):\n        probs = confidences[:, class_index]\n        mask = probs > prob_threshold\n        probs = probs[mask]\n        if probs.shape[0] == 0:\n            continue\n        subset_boxes = boxes[mask, :]\n        box_probs = np.concatenate([subset_boxes, probs.reshape(-1, 1)], axis=1)\n        box_probs = box_utils.hard_nms(box_probs,\n                                       iou_threshold=iou_threshold,\n                                       top_k=top_k,\n                                       )\n        picked_box_probs.append(box_probs)\n        picked_labels.extend([class_index] * box_probs.shape[0])\n    if not picked_box_probs:\n        return np.array([]), np.array([]), np.array([])\n    picked_box_probs = np.concatenate(picked_box_probs)\n    picked_box_probs[:, 0] *= width\n    picked_box_probs[:, 1] *= height\n    picked_box_probs[:, 2] *= width\n    picked_box_probs[:, 3] *= height\n    return picked_box_probs[:, :4].astype(np.int32), np.array(picked_labels), picked_box_probs[:, 4]\n\n\nlabel_path = \"models/voc-model-labels.txt\"\n\nonnx_path = \"models/onnx/version-RFB-320.onnx\"\nclass_names = [name.strip() for name in open(label_path).readlines()]\n\npredictor = onnx.load(onnx_path)\nonnx.checker.check_model(predictor)\nonnx.helper.printable_graph(predictor.graph)\npredictor = backend.prepare(predictor, device=\"CPU\")  # default CPU\n\nort_session = ort.InferenceSession(onnx_path)\ninput_name = ort_session.get_inputs()[0].name\n\ncap = cv2.VideoCapture(\"/home/linzai/Videos/video/16_6.MP4\")  # capture from camera\n\nthreshold = 0.7\n\nsum = 0\nwhile True:\n    ret, orig_image = cap.read()\n    if orig_image is None:\n        print(\"no img\")\n        break\n    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (320, 240))\n    # image = cv2.resize(image, (640, 480))\n    image_mean = np.array([127, 127, 127])\n    image = (image - image_mean) / 128\n    image = np.transpose(image, [2, 0, 1])\n    image = np.expand_dims(image, axis=0)\n    image = image.astype(np.float32)\n    # confidences, boxes = predictor.run(image)\n    time_time = time.time()\n    confidences, boxes = ort_session.run(None, {input_name: image})\n    print(\"cost time:{}\".format(time.time() - time_time))\n    boxes, labels, probs = predict(orig_image.shape[1], orig_image.shape[0], confidences, boxes, threshold)\n    for i in range(boxes.shape[0]):\n        box = boxes[i, :]\n        label = f\"{class_names[labels[i]]}: {probs[i]:.2f}\"\n\n        cv2.rectangle(orig_image, (box[0], box[1]), (box[2], box[3]), (255, 255, 0), 4)\n\n        # cv2.putText(orig_image, label,\n        #             (box[0] + 20, box[1] + 40),\n        #             cv2.FONT_HERSHEY_SIMPLEX,\n        #             1,  # font scale\n        #             (255, 0, 255),\n        #             2)  # line type\n    sum += boxes.shape[0]\n    orig_image = cv2.resize(orig_image, (0, 0), fx=0.7, fy=0.7)\n    cv2.imshow('annotated', orig_image)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\ncap.release()\ncv2.destroyAllWindows()\nprint(\"sum:{}\".format(sum))\n"
        },
        {
          "name": "tf",
          "type": "tree",
          "content": null
        },
        {
          "name": "tflite",
          "type": "tree",
          "content": null
        },
        {
          "name": "train-version-RFB.sh",
          "type": "blob",
          "size": 0.5390625,
          "content": "#!/usr/bin/env bash\nmodel_root_path=\"./models/train-version-RFB\"\nlog_dir=\"$model_root_path/logs\"\nlog=\"$log_dir/log\"\nmkdir -p \"$log_dir\"\n\npython3 -u train.py \\\n  --datasets \\\n  ./data/wider_face_add_lm_10_10 \\\n  --validation_dataset \\\n  ./data/wider_face_add_lm_10_10 \\\n  --net \\\n  RFB \\\n  --num_epochs \\\n  200 \\\n  --milestones \\\n  \"95,150\" \\\n  --lr \\\n  1e-2 \\\n  --batch_size \\\n  24 \\\n  --input_size \\\n  320 \\\n  --checkpoint_folder \\\n  ${model_root_path} \\\n  --num_workers \\\n  0 \\\n  --log_dir \\\n  ${log_dir} \\\n  --cuda_index \\\n  0 \\\n  2>&1 | tee \"$log\"\n"
        },
        {
          "name": "train-version-slim.sh",
          "type": "blob",
          "size": 0.541015625,
          "content": "#!/usr/bin/env bash\nmodel_root_path=\"./models/train-version-slim\"\nlog_dir=\"$model_root_path/logs\"\nlog=\"$log_dir/log\"\nmkdir -p \"$log_dir\"\n\npython3 -u train.py \\\n  --datasets \\\n  ./data/wider_face_add_lm_10_10 \\\n  --validation_dataset \\\n  ./data/wider_face_add_lm_10_10 \\\n  --net \\\n  slim \\\n  --num_epochs \\\n  200 \\\n  --milestones \\\n  \"95,150\" \\\n  --lr \\\n  1e-2 \\\n  --batch_size \\\n  24 \\\n  --input_size \\\n  320 \\\n  --checkpoint_folder \\\n  ${model_root_path} \\\n  --num_workers \\\n  4 \\\n  --log_dir \\\n  ${log_dir} \\\n  --cuda_index \\\n  0 \\\n  2>&1 | tee \"$log\"\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 16.185546875,
          "content": "\"\"\"\nThis code is the main training code.\n\"\"\"\nimport argparse\nimport itertools\nimport logging\nimport os\nimport sys\n\nimport torch\nfrom torch import nn\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, MultiStepLR\nfrom torch.utils.data import DataLoader, ConcatDataset\n\nfrom vision.datasets.voc_dataset import VOCDataset\nfrom vision.nn.multibox_loss import MultiboxLoss\nfrom vision.ssd.config.fd_config import define_img_size\nfrom vision.utils.misc import str2bool, Timer, freeze_net_layers, store_labels\n\nparser = argparse.ArgumentParser(\n    description='train With Pytorch')\n\nparser.add_argument(\"--dataset_type\", default=\"voc\", type=str,\n                    help='Specify dataset type. Currently support voc.')\n\nparser.add_argument('--datasets', nargs='+', help='Dataset directory path')\nparser.add_argument('--validation_dataset', help='Dataset directory path')\nparser.add_argument('--balance_data', action='store_true',\n                    help=\"Balance training data by down-sampling more frequent labels.\")\n\nparser.add_argument('--net', default=\"RFB\",\n                    help=\"The network architecture ,optional(RFB , slim)\")\nparser.add_argument('--freeze_base_net', action='store_true',\n                    help=\"Freeze base net layers.\")\nparser.add_argument('--freeze_net', action='store_true',\n                    help=\"Freeze all the layers except the prediction head.\")\n\n# Params for SGD\nparser.add_argument('--lr', '--learning-rate', default=1e-2, type=float,\n                    help='initial learning rate')\nparser.add_argument('--momentum', default=0.9, type=float,\n                    help='Momentum value for optim')\nparser.add_argument('--weight_decay', default=5e-4, type=float,\n                    help='Weight decay for SGD')\nparser.add_argument('--gamma', default=0.1, type=float,\n                    help='Gamma update for SGD')\nparser.add_argument('--base_net_lr', default=None, type=float,\n                    help='initial learning rate for base net.')\nparser.add_argument('--extra_layers_lr', default=None, type=float,\n                    help='initial learning rate for the layers not in base net and prediction heads.')\n\n# Params for loading pretrained basenet or checkpoints.\nparser.add_argument('--base_net',\n                    help='Pretrained base model')\nparser.add_argument('--pretrained_ssd', help='Pre-trained base model')\nparser.add_argument('--resume', default=None, type=str,\n                    help='Checkpoint state_dict file to resume training from')\n\n# Scheduler\nparser.add_argument('--scheduler', default=\"multi-step\", type=str,\n                    help=\"Scheduler for SGD. It can one of multi-step and cosine\")\n\n# Params for Multi-step Scheduler\nparser.add_argument('--milestones', default=\"80,100\", type=str,\n                    help=\"milestones for MultiStepLR\")\n\n# Params for Cosine Annealing\nparser.add_argument('--t_max', default=120, type=float,\n                    help='T_max value for Cosine Annealing Scheduler.')\n\n# Train params\nparser.add_argument('--batch_size', default=24, type=int,\n                    help='Batch size for training')\nparser.add_argument('--num_epochs', default=200, type=int,\n                    help='the number epochs')\nparser.add_argument('--num_workers', default=4, type=int,\n                    help='Number of workers used in dataloading')\nparser.add_argument('--validation_epochs', default=5, type=int,\n                    help='the number epochs')\nparser.add_argument('--debug_steps', default=100, type=int,\n                    help='Set the debug log output frequency.')\nparser.add_argument('--use_cuda', default=True, type=str2bool,\n                    help='Use CUDA to train model')\n\nparser.add_argument('--checkpoint_folder', default='models/',\n                    help='Directory for saving checkpoint models')\nparser.add_argument('--log_dir', default='./models/Ultra-Light(1MB)_&_Fast_Face_Detector/logs',\n                    help='lod dir')\nparser.add_argument('--cuda_index', default=\"0\", type=str,\n                    help='Choose cuda index.If you have 4 GPUs, you can set it like 0,1,2,3')\nparser.add_argument('--power', default=2, type=int,\n                    help='poly lr pow')\nparser.add_argument('--overlap_threshold', default=0.35, type=float,\n                    help='overlap_threshold')\nparser.add_argument('--optimizer_type', default=\"SGD\", type=str,\n                    help='optimizer_type')\nparser.add_argument('--input_size', default=320, type=int,\n                    help='define network input size,default optional value 128/160/320/480/640/1280')\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO,\n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nargs = parser.parse_args()\n\ninput_img_size = args.input_size  # define input size ,default optional(128/160/320/480/640/1280)\nlogging.info(\"inpu size :{}\".format(input_img_size))\ndefine_img_size(input_img_size)  # must put define_img_size() before 'import fd_config'\n\nfrom vision.ssd.config import fd_config\nfrom vision.ssd.data_preprocessing import TrainAugmentation, TestTransform\nfrom vision.ssd.mb_tiny_RFB_fd import create_Mb_Tiny_RFB_fd\nfrom vision.ssd.mb_tiny_fd import create_mb_tiny_fd\nfrom vision.ssd.ssd import MatchPrior\n\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() and args.use_cuda else \"cpu\")\n\nif args.use_cuda and torch.cuda.is_available():\n    torch.backends.cudnn.benchmark = True\n    logging.info(\"Use Cuda.\")\n\n\ndef lr_poly(base_lr, iter):\n    return base_lr * ((1 - float(iter) / args.num_epochs) ** (args.power))\n\n\ndef adjust_learning_rate(optimizer, i_iter):\n    \"\"\"Sets the learning rate to the initial LR divided by 5 at 60th, 120th and 160th epochs\"\"\"\n    lr = lr_poly(args.lr, i_iter)\n    optimizer.param_groups[0]['lr'] = lr\n\n\ndef train(loader, net, criterion, optimizer, device, debug_steps=100, epoch=-1):\n    net.train(True)\n    running_loss = 0.0\n    running_regression_loss = 0.0\n    running_classification_loss = 0.0\n    for i, data in enumerate(loader):\n        print(\".\", end=\"\", flush=True)\n        images, boxes, labels = data\n        images = images.to(device)\n        boxes = boxes.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        confidence, locations = net(images)\n        regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)  # TODO CHANGE BOXES\n        loss = regression_loss + classification_loss\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        running_regression_loss += regression_loss.item()\n        running_classification_loss += classification_loss.item()\n        if i and i % debug_steps == 0:\n            print(\".\", flush=True)\n            avg_loss = running_loss / debug_steps\n            avg_reg_loss = running_regression_loss / debug_steps\n            avg_clf_loss = running_classification_loss / debug_steps\n            logging.info(\n                f\"Epoch: {epoch}, Step: {i}, \" +\n                f\"Average Loss: {avg_loss:.4f}, \" +\n                f\"Average Regression Loss {avg_reg_loss:.4f}, \" +\n                f\"Average Classification Loss: {avg_clf_loss:.4f}\"\n            )\n\n            running_loss = 0.0\n            running_regression_loss = 0.0\n            running_classification_loss = 0.0\n\n\ndef test(loader, net, criterion, device):\n    net.eval()\n    running_loss = 0.0\n    running_regression_loss = 0.0\n    running_classification_loss = 0.0\n    num = 0\n    for _, data in enumerate(loader):\n        images, boxes, labels = data\n        images = images.to(device)\n        boxes = boxes.to(device)\n        labels = labels.to(device)\n        num += 1\n\n        with torch.no_grad():\n            confidence, locations = net(images)\n            regression_loss, classification_loss = criterion(confidence, locations, labels, boxes)\n            loss = regression_loss + classification_loss\n\n        running_loss += loss.item()\n        running_regression_loss += regression_loss.item()\n        running_classification_loss += classification_loss.item()\n    return running_loss / num, running_regression_loss / num, running_classification_loss / num\n\n\nif __name__ == '__main__':\n    timer = Timer()\n\n    logging.info(args)\n    if args.net == 'slim':\n        create_net = create_mb_tiny_fd\n        config = fd_config\n    elif args.net == 'RFB':\n        create_net = create_Mb_Tiny_RFB_fd\n        config = fd_config\n    else:\n        logging.fatal(\"The net type is wrong.\")\n        parser.print_help(sys.stderr)\n        sys.exit(1)\n\n    train_transform = TrainAugmentation(config.image_size, config.image_mean, config.image_std)\n    target_transform = MatchPrior(config.priors, config.center_variance,\n                                  config.size_variance, args.overlap_threshold)\n\n    test_transform = TestTransform(config.image_size, config.image_mean_test, config.image_std)\n\n    if not os.path.exists(args.checkpoint_folder):\n        os.makedirs(args.checkpoint_folder)\n    logging.info(\"Prepare training datasets.\")\n    datasets = []\n    for dataset_path in args.datasets:\n        if args.dataset_type == 'voc':\n            dataset = VOCDataset(dataset_path, transform=train_transform,\n                                 target_transform=target_transform)\n            label_file = os.path.join(args.checkpoint_folder, \"voc-model-labels.txt\")\n            store_labels(label_file, dataset.class_names)\n            num_classes = len(dataset.class_names)\n\n        else:\n            raise ValueError(f\"Dataset tpye {args.dataset_type} is not supported.\")\n        datasets.append(dataset)\n    logging.info(f\"Stored labels into file {label_file}.\")\n    train_dataset = ConcatDataset(datasets)\n    logging.info(\"Train dataset size: {}\".format(len(train_dataset)))\n    train_loader = DataLoader(train_dataset, args.batch_size,\n                              num_workers=args.num_workers,\n                              shuffle=True)\n    logging.info(\"Prepare Validation datasets.\")\n    if args.dataset_type == \"voc\":\n        val_dataset = VOCDataset(args.validation_dataset, transform=test_transform,\n                                 target_transform=target_transform, is_test=True)\n    logging.info(\"validation dataset size: {}\".format(len(val_dataset)))\n\n    val_loader = DataLoader(val_dataset, args.batch_size,\n                            num_workers=args.num_workers,\n                            shuffle=False)\n    logging.info(\"Build network.\")\n    net = create_net(num_classes)\n\n    # add multigpu_train\n    cuda_index_list = None\n    if torch.cuda.device_count() >= 1:\n        cuda_index_list = [int(v.strip()) for v in args.cuda_index.split(\",\")]\n        net = nn.DataParallel(net, device_ids=cuda_index_list)\n        logging.info(\"use gpu :{}\".format(cuda_index_list))\n\n    min_loss = -10000.0\n    last_epoch = -1\n\n    base_net_lr = args.base_net_lr if args.base_net_lr is not None else args.lr\n    extra_layers_lr = args.extra_layers_lr if args.extra_layers_lr is not None else args.lr\n    if args.freeze_base_net:\n        logging.info(\"Freeze base net.\")\n        freeze_net_layers(net.base_net)\n        params = itertools.chain(net.source_layer_add_ons.parameters(), net.extras.parameters(),\n                                 net.regression_headers.parameters(), net.classification_headers.parameters())\n        params = [\n            {'params': itertools.chain(\n                net.source_layer_add_ons.parameters(),\n                net.extras.parameters()\n            ), 'lr': extra_layers_lr},\n            {'params': itertools.chain(\n                net.regression_headers.parameters(),\n                net.classification_headers.parameters()\n            )}\n        ]\n    elif args.freeze_net:\n        freeze_net_layers(net.base_net)\n        freeze_net_layers(net.source_layer_add_ons)\n        freeze_net_layers(net.extras)\n        params = itertools.chain(net.regression_headers.parameters(), net.classification_headers.parameters())\n        logging.info(\"Freeze all the layers except prediction heads.\")\n    else:\n        if cuda_index_list:\n            params = [\n                {'params': net.module.base_net.parameters(), 'lr': base_net_lr},\n                {'params': itertools.chain(\n                    net.module.source_layer_add_ons.parameters(),\n                    net.module.extras.parameters()\n                ), 'lr': extra_layers_lr},\n                {'params': itertools.chain(\n                    net.module.regression_headers.parameters(),\n                    net.module.classification_headers.parameters()\n                )}\n            ]\n        else:\n            params = [\n                {'params': net.base_net.parameters(), 'lr': base_net_lr},\n                {'params': itertools.chain(\n                    net.source_layer_add_ons.parameters(),\n                    net.extras.parameters()\n                ), 'lr': extra_layers_lr},\n                {'params': itertools.chain(\n                    net.regression_headers.parameters(),\n                    net.classification_headers.parameters()\n                )}\n            ]\n\n    timer.start(\"Load Model\")\n    if args.resume:\n        logging.info(f\"Resume from the model {args.resume}\")\n        net.load(args.resume)\n    elif args.base_net:\n        logging.info(f\"Init from base net {args.base_net}\")\n        net.init_from_base_net(args.base_net)\n    elif args.pretrained_ssd:\n        logging.info(f\"Init from pretrained ssd {args.pretrained_ssd}\")\n        net.init_from_pretrained_ssd(args.pretrained_ssd)\n    logging.info(f'Took {timer.end(\"Load Model\"):.2f} seconds to load the model.')\n\n    net.to(DEVICE)\n\n    criterion = MultiboxLoss(config.priors, neg_pos_ratio=3,\n                             center_variance=0.1, size_variance=0.2, device=DEVICE)\n    if args.optimizer_type == \"SGD\":\n        optimizer = torch.optim.SGD(params, lr=args.lr, momentum=args.momentum,\n                                    weight_decay=args.weight_decay)\n    elif args.optimizer_type == \"Adam\":\n        optimizer = torch.optim.Adam(params, lr=args.lr)\n        logging.info(\"use Adam optimizer\")\n    else:\n        logging.fatal(f\"Unsupported optimizer: {args.scheduler}.\")\n        parser.print_help(sys.stderr)\n        sys.exit(1)\n    logging.info(f\"Learning rate: {args.lr}, Base net learning rate: {base_net_lr}, \"\n                 + f\"Extra Layers learning rate: {extra_layers_lr}.\")\n    if args.optimizer_type != \"Adam\":\n        if args.scheduler == 'multi-step':\n            logging.info(\"Uses MultiStepLR scheduler.\")\n            milestones = [int(v.strip()) for v in args.milestones.split(\",\")]\n            scheduler = MultiStepLR(optimizer, milestones=milestones,\n                                    gamma=0.1, last_epoch=last_epoch)\n        elif args.scheduler == 'cosine':\n            logging.info(\"Uses CosineAnnealingLR scheduler.\")\n            scheduler = CosineAnnealingLR(optimizer, args.t_max, last_epoch=last_epoch)\n        elif args.scheduler == 'poly':\n            logging.info(\"Uses PolyLR scheduler.\")\n        else:\n            logging.fatal(f\"Unsupported Scheduler: {args.scheduler}.\")\n            parser.print_help(sys.stderr)\n            sys.exit(1)\n\n    logging.info(f\"Start training from epoch {last_epoch + 1}.\")\n    for epoch in range(last_epoch + 1, args.num_epochs):\n        if args.optimizer_type != \"Adam\":\n            if args.scheduler != \"poly\":\n                if epoch != 0:\n                    scheduler.step()\n        train(train_loader, net, criterion, optimizer,\n              device=DEVICE, debug_steps=args.debug_steps, epoch=epoch)\n        if args.scheduler == \"poly\":\n            adjust_learning_rate(optimizer, epoch)\n        logging.info(\"lr rate :{}\".format(optimizer.param_groups[0]['lr']))\n\n        if epoch % args.validation_epochs == 0 or epoch == args.num_epochs - 1:\n            logging.info(\"lr rate :{}\".format(optimizer.param_groups[0]['lr']))\n            val_loss, val_regression_loss, val_classification_loss = test(val_loader, net, criterion, DEVICE)\n            logging.info(\n                f\"Epoch: {epoch}, \" +\n                f\"Validation Loss: {val_loss:.4f}, \" +\n                f\"Validation Regression Loss {val_regression_loss:.4f}, \" +\n                f\"Validation Classification Loss: {val_classification_loss:.4f}\"\n            )\n            model_path = os.path.join(args.checkpoint_folder, f\"{args.net}-Epoch-{epoch}-Loss-{val_loss}.pth\")\n            if cuda_index_list:\n                net.module.save(model_path)\n            else:\n                net.save(model_path)\n            logging.info(f\"Saved model {model_path}\")\n"
        },
        {
          "name": "vision",
          "type": "tree",
          "content": null
        },
        {
          "name": "widerface_evaluate",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}