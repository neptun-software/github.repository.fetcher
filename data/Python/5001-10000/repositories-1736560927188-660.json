{
  "metadata": {
    "timestamp": 1736560927188,
    "page": 660,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch",
      "stars": 5687,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.45703125,
          "content": "*.pyc\n*cache\n*.idea\n*.log\n*logs/\nlogs/\nruns/\n*__pycache__\n.DS_Store\ndrlnd/\n*Banana.app/\n*deep-reinforcement-learning/\n*venv/\n*playground_test_runs.py\nGame_Files/\nmjpro150/\nmujoco200_macos/\n*to_do_list\nRandom_Junkyard/\nNotebook.ipynb\nResults/Notebook.ipynb\n*.ipynb_checkpoints\n*.drive_access_key.json\ndrive_access_key.json\ndrive_access_key\nsettings.json\nlaunch.json\nresults/data_and_graphs/Cart_Pole_Results_Data.pkl\nresults/data_and_graphs/Cart_Pole_Results_Graph.png\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.8837890625,
          "content": "language:\n  python\n\npython: 3.7\ndist: xenial\nsudo: true\n\ninstall:\n  - pip install -r requirements.txt\n\n\nscript:\n  - export PYTHONPATH=\"$PYTHONPATH:$PWD\"\n  - export PYTHONPATH=\"\"$PYTHONPATH:$PWD/agents\"\"\n  - export PYTHONPATH=\"\"$PYTHONPATH:$PWD/agents/DQN_agents\"\"\n  - export PYTHONPATH=\"\"$PYTHONPATH:$PWD/agents/hierarchical_agents\"\"\n  - export PYTHONPATH=\"\"$PYTHONPATH:$PWD/agents/actor_critic_agents\"\"\n  - export PYTHONPATH=\"\"$PYTHONPATH:$PWD/agents/policy_gradient_agents\"\"\n  - export PYTHONPATH=\"\"$PYTHONPATH:$PWD/utilities/data_structures\"\"\n  - export PYTHONPATH=\"\"$PYTHONPATH:$PWD/environments\"\"\n  - export PYTHONPATH=\"\"$PYTHONPATH:$PWD/utilities\"\"\n  - export PYTHONPATH=\"\"$PYTHONPATH:$PWD/exploration_strategies\"\"\n  - export PYTHONPATH=\"\"$PYTHONPATH:$PWD/utilities/*\"\"\n  - export PYTHONPATH=\"\"$PYTHONPATH:$PWD/*\"\"\n  - export PYTHONPATH=\"\"$PYTHONPATH:$PWD/results\"\"\n  - pytest tests/*.py\n\n\n\n\n\n\n\n\n\n\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0517578125,
          "content": "MIT License\n\nCopyright (c) 2021 Petros Christodoulou\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.8662109375,
          "content": "# Deep Reinforcement Learning Algorithms with PyTorch\n\n![Travis CI](https://travis-ci.org/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch.svg?branch=master)\n[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/dwyl/esta/issues)\n\n\n\n![RL](utilities/RL_image.jpeg)   ![PyTorch](utilities/PyTorch-logo-2.jpg)\n\nThis repository contains PyTorch implementations of deep reinforcement learning algorithms and environments. \n\n(To help you remember things you learn about machine learning in general write them in [Gizmo](https://gizmo.ai))\n## **Algorithms Implemented**  \n\n1. *Deep Q Learning (DQN)* <sub><sup> ([Mnih et al. 2013](https://arxiv.org/pdf/1312.5602.pdf)) </sup></sub>  \n1. *DQN with Fixed Q Targets* <sub><sup> ([Mnih et al. 2013](https://arxiv.org/pdf/1312.5602.pdf)) </sup></sub>\n1. *Double DQN (DDQN)* <sub><sup> ([Hado van Hasselt et al. 2015](https://arxiv.org/pdf/1509.06461.pdf)) </sup></sub>\n1. *DDQN with Prioritised Experience Replay* <sub><sup> ([Schaul et al. 2016](https://arxiv.org/pdf/1511.05952.pdf)) </sup></sub>\n1. *Dueling DDQN* <sub><sup> ([Wang et al. 2016](http://proceedings.mlr.press/v48/wangf16.pdf)) </sup></sub>\n1. *REINFORCE* <sub><sup> ([Williams et al. 1992](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)) </sup></sub>\n1. *Deep Deterministic Policy Gradients (DDPG)* <sub><sup> ([Lillicrap et al. 2016](https://arxiv.org/pdf/1509.02971.pdf) ) </sup></sub>\n1. *Twin Delayed Deep Deterministic Policy Gradients (TD3)* <sub><sup> ([Fujimoto et al. 2018](https://arxiv.org/abs/1802.09477)) </sup></sub>\n1. *Soft Actor-Critic (SAC)* <sub><sup> ([Haarnoja et al. 2018](https://arxiv.org/pdf/1812.05905.pdf)) </sup></sub>\n1. *Soft Actor-Critic for Discrete Actions (SAC-Discrete)* <sub><sup> ([Christodoulou 2019](https://arxiv.org/abs/1910.07207)) </sup></sub> \n1. *Asynchronous Advantage Actor Critic (A3C)* <sub><sup> ([Mnih et al. 2016](https://arxiv.org/pdf/1602.01783.pdf)) </sup></sub>\n1. *Syncrhonous Advantage Actor Critic (A2C)*\n1. *Proximal Policy Optimisation (PPO)* <sub><sup> ([Schulman et al. 2017](https://openai-public.s3-us-west-2.amazonaws.com/blog/2017-07/ppo/ppo-arxiv.pdf)) </sup></sub>\n1. *DQN with Hindsight Experience Replay (DQN-HER)* <sub><sup> ([Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf)) </sup></sub>\n1. *DDPG with Hindsight Experience Replay (DDPG-HER)* <sub><sup> ([Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf) ) </sup></sub>\n1. *Hierarchical-DQN (h-DQN)* <sub><sup> ([Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)) </sup></sub>\n1. *Stochastic NNs for Hierarchical Reinforcement Learning (SNN-HRL)* <sub><sup> ([Florensa et al. 2017](https://arxiv.org/pdf/1704.03012.pdf)) </sup></sub>\n1. *Diversity Is All You Need (DIAYN)* <sub><sup> ([Eyensbach et al. 2018](https://arxiv.org/pdf/1802.06070.pdf)) </sup></sub>\n\nAll implementations are able to quickly solve Cart Pole (discrete actions), Mountain Car Continuous (continuous actions), \nBit Flipping (discrete actions with dynamic goals) or Fetch Reach (continuous actions with dynamic goals). I plan to add more hierarchical RL algorithms soon.\n\n## **Environments Implemented**\n\n1. *Bit Flipping Game* <sub><sup> (as described in [Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf)) </sup></sub>\n1. *Four Rooms Game* <sub><sup> (as described in [Sutton et al. 1998](http://www-anw.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf)) </sup></sub>\n1. *Long Corridor Game* <sub><sup> (as described in [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)) </sup></sub>\n1. *Ant-{Maze, Push, Fall}* <sub><sup> (as desribed in [Nachum et al. 2018](https://arxiv.org/pdf/1805.08296.pdf) and their accompanying [code](https://github.com/tensorflow/models/tree/master/research/efficient-hrl)) </sup></sub>\n\n## **Results**\n\n#### 1. Cart Pole and Mountain Car\n\nBelow shows various RL algorithms successfully learning discrete action game [Cart Pole](https://github.com/openai/gym/wiki/CartPole-v0)\n or continuous action game [Mountain Car](https://github.com/openai/gym/wiki/MountainCarContinuous-v0). The mean result from running the algorithms \n with 3 random seeds is shown with the shaded area representing plus and minus 1 standard deviation. Hyperparameters\n used can be found in files `results/Cart_Pole.py` and `results/Mountain_Car.py`. \n \n![Cart Pole and Mountain Car Results](results/data_and_graphs/CartPole_and_MountainCar_Graph.png) \n\n\n#### 2. Hindsight Experience Replay (HER) Experiements\n\nBelow shows the performance of DQN and DDPG with and without Hindsight Experience Replay (HER) in the Bit Flipping (14 bits) \nand Fetch Reach environments described in the papers [Hindsight Experience Replay 2018](https://arxiv.org/pdf/1707.01495.pdf) \nand [Multi-Goal Reinforcement Learning 2018](https://arxiv.org/abs/1802.09464). The results replicate the results found in \nthe papers and show how adding HER can allow an agent to solve problems that it otherwise would not be able to solve at all. Note that the same hyperparameters were used within each pair of agents and so the only difference \nbetween them was whether hindsight was used or not. \n\n![HER Experiment Results](results/data_and_graphs/HER_Experiments.png)\n\n#### 3. Hierarchical Reinforcement Learning Experiments\n\nThe results on the left below show the performance of DQN and the algorithm hierarchical-DQN from [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)\non the Long Corridor environment also explained in [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf). The environment\nrequires the agent to go to the end of a corridor before coming back in order to receive a larger reward. This delayed \ngratification and the aliasing of states makes it a somewhat impossible game for DQN to learn but if we introduce a \nmeta-controller (as in h-DQN) which directs a lower-level controller how to behave we are able to make more progress. This \naligns with the results found in the paper. \n\nThe results on the right show the performance of DDQN and algorithm Stochastic NNs for Hierarchical Reinforcement Learning \n(SNN-HRL) from [Florensa et al. 2017](https://arxiv.org/pdf/1704.03012.pdf). DDQN is used as the comparison because\nthe implementation of SSN-HRL uses 2 DDQN algorithms within it. Note that the first 300 episodes of training\nfor SNN-HRL were used for pre-training which is why there is no reward for those episodes. \n \n![Long Corridor and Four Rooms](results/data_and_graphs/Four_Rooms_and_Long_Corridor.png)\n     \n\n### Usage ###\n\nThe repository's high-level structure is:\n \n    ├── agents                    \n        ├── actor_critic_agents   \n        ├── DQN_agents         \n        ├── policy_gradient_agents\n        └── stochastic_policy_search_agents \n    ├── environments   \n    ├── results             \n        └── data_and_graphs        \n    ├── tests\n    ├── utilities             \n        └── data structures            \n   \n\n#### i) To watch the agents learn the above games  \n\nTo watch all the different agents learn Cart Pole follow these steps:\n\n```commandline\ngit clone https://github.com/p-christ/Deep_RL_Implementations.git\ncd Deep_RL_Implementations\n\nconda create --name myenvname\ny\nconda activate myenvname\n\npip3 install -r requirements.txt\n\npython results/Cart_Pole.py\n``` \n\nFor other games change the last line to one of the other files in the Results folder. \n\n#### ii) To train the agents on another game  \n\nMost Open AI gym environments should work. All you would need to do is change the config.environment field (look at `Results/Cart_Pole.py`  for an example of this). \n\nYou can also play with your own custom game if you create a separate class that inherits from gym.Env. See `Environments/Four_Rooms_Environment.py`\nfor an example of a custom environment and then see the script `Results/Four_Rooms.py` to see how to have agents play the environment.\n"
        },
        {
          "name": "agents",
          "type": "tree",
          "content": null
        },
        {
          "name": "environments",
          "type": "tree",
          "content": null
        },
        {
          "name": "exploration_strategies",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1064453125,
          "content": "numpy==1.15.2\ntorch==0.4.1.post2\nmatplotlib==3.0.0\nPyVirtualDisplay==0.2.1\ngym==0.10.9\nnn_builder\ntensorflow\n"
        },
        {
          "name": "results",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "utilities",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}