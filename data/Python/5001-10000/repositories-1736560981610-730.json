{
  "metadata": {
    "timestamp": 1736560981610,
    "page": 730,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjczMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "649453932/Chinese-Text-Classification-Pytorch",
      "stars": 5434,
      "defaultBranch": "master",
      "files": [
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.041015625,
          "content": "MIT License\n\nCopyright (c) 2019 huwenxing\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.92578125,
          "content": "# Chinese-Text-Classification-Pytorch\n[![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg)](https://github.com/996icu/996.ICU/blob/master/LICENSE)\n\n中文文本分类，TextCNN，TextRNN，FastText，TextRCNN，BiLSTM_Attention, DPCNN, Transformer, 基于pytorch，开箱即用。\n\n## 介绍\n模型介绍、数据流动过程：[我的博客](https://zhuanlan.zhihu.com/p/73176084)  \n\n数据以字为单位输入模型，预训练词向量使用 [搜狗新闻 Word+Character 300d](https://github.com/Embedding/Chinese-Word-Vectors)，[点这里下载](https://pan.baidu.com/s/14k-9jsspp43ZhMxqPmsWMQ)  \n\n## 环境\npython 3.7  \npytorch 1.1  \ntqdm  \nsklearn  \ntensorboardX\n\n## 中文数据集\n我从[THUCNews](http://thuctc.thunlp.org/)中抽取了20万条新闻标题，已上传至github，文本长度在20到30之间。一共10个类别，每类2万条。\n\n类别：财经、房产、股票、教育、科技、社会、时政、体育、游戏、娱乐。\n\n数据集划分：\n\n数据集|数据量\n--|--\n训练集|18万\n验证集|1万\n测试集|1万\n\n\n### 更换自己的数据集\n - 如果用字，按照我数据集的格式来格式化你的数据。  \n - 如果用词，提前分好词，词之间用空格隔开，`python run.py --model TextCNN --word True`  \n - 使用预训练词向量：utils.py的main函数可以提取词表对应的预训练词向量。  \n\n\n## 效果\n\n模型|acc|备注\n--|--|--\nTextCNN|91.22%|Kim 2014 经典的CNN文本分类\nTextRNN|91.12%|BiLSTM \nTextRNN_Att|90.90%|BiLSTM+Attention\nTextRCNN|91.54%|BiLSTM+池化\nFastText|92.23%|bow+bigram+trigram， 效果出奇的好\nDPCNN|91.25%|深层金字塔CNN\nTransformer|89.91%|效果较差\nbert|94.83%|bert + fc  \nERNIE|94.61%|比bert略差(说好的中文碾压bert呢)  \n\nbert和ERNIE模型代码我放到另外一个仓库了，传送门：[Bert-Chinese-Text-Classification-Pytorch](https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch)，后续还会搞一些bert之后的东西，欢迎star。  \n\n## 使用说明\n```\n# 训练并测试：\n# TextCNN\npython run.py --model TextCNN\n\n# TextRNN\npython run.py --model TextRNN\n\n# TextRNN_Att\npython run.py --model TextRNN_Att\n\n# TextRCNN\npython run.py --model TextRCNN\n\n# FastText, embedding层是随机初始化的\npython run.py --model FastText --embedding random \n\n# DPCNN\npython run.py --model DPCNN\n\n# Transformer\npython run.py --model Transformer\n```\n\n### 参数\n模型都在models目录下，超参定义和模型定义在同一文件中。  \n\n\n## 对应论文\n[1] Convolutional Neural Networks for Sentence Classification  \n[2] Recurrent Neural Network for Text Classification with Multi-Task Learning  \n[3] Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification  \n[4] Recurrent Convolutional Neural Networks for Text Classification  \n[5] Bag of Tricks for Efficient Text Classification  \n[6] Deep Pyramid Convolutional Neural Networks for Text Categorization  \n[7] Attention Is All You Need  \n"
        },
        {
          "name": "THUCNews",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 2.0087890625,
          "content": "# coding: UTF-8\nimport time\nimport torch\nimport numpy as np\nfrom train_eval import train, init_network\nfrom importlib import import_module\nimport argparse\n\nparser = argparse.ArgumentParser(description='Chinese Text Classification')\nparser.add_argument('--model', type=str, required=True, help='choose a model: TextCNN, TextRNN, FastText, TextRCNN, TextRNN_Att, DPCNN, Transformer')\nparser.add_argument('--embedding', default='pre_trained', type=str, help='random or pre_trained')\nparser.add_argument('--word', default=False, type=bool, help='True for word, False for char')\nargs = parser.parse_args()\n\n\nif __name__ == '__main__':\n    dataset = 'THUCNews'  # 数据集\n\n    # 搜狗新闻:embedding_SougouNews.npz, 腾讯:embedding_Tencent.npz, 随机初始化:random\n    embedding = 'embedding_SougouNews.npz'\n    if args.embedding == 'random':\n        embedding = 'random'\n    model_name = args.model  # 'TextRCNN'  # TextCNN, TextRNN, FastText, TextRCNN, TextRNN_Att, DPCNN, Transformer\n    if model_name == 'FastText':\n        from utils_fasttext import build_dataset, build_iterator, get_time_dif\n        embedding = 'random'\n    else:\n        from utils import build_dataset, build_iterator, get_time_dif\n\n    x = import_module('models.' + model_name)\n    config = x.Config(dataset, embedding)\n    np.random.seed(1)\n    torch.manual_seed(1)\n    torch.cuda.manual_seed_all(1)\n    torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n\n    start_time = time.time()\n    print(\"Loading data...\")\n    vocab, train_data, dev_data, test_data = build_dataset(config, args.word)\n    train_iter = build_iterator(train_data, config)\n    dev_iter = build_iterator(dev_data, config)\n    test_iter = build_iterator(test_data, config)\n    time_dif = get_time_dif(start_time)\n    print(\"Time usage:\", time_dif)\n\n    # train\n    config.n_vocab = len(vocab)\n    model = x.Model(config).to(config.device)\n    if model_name != 'Transformer':\n        init_network(model)\n    print(model.parameters)\n    train(config, model, train_iter, dev_iter, test_iter)\n"
        },
        {
          "name": "train_eval.py",
          "type": "blob",
          "size": 4.8466796875,
          "content": "# coding: UTF-8\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn import metrics\nimport time\nfrom utils import get_time_dif\nfrom tensorboardX import SummaryWriter\n\n\n# 权重初始化，默认xavier\ndef init_network(model, method='xavier', exclude='embedding', seed=123):\n    for name, w in model.named_parameters():\n        if exclude not in name:\n            if 'weight' in name:\n                if method == 'xavier':\n                    nn.init.xavier_normal_(w)\n                elif method == 'kaiming':\n                    nn.init.kaiming_normal_(w)\n                else:\n                    nn.init.normal_(w)\n            elif 'bias' in name:\n                nn.init.constant_(w, 0)\n            else:\n                pass\n\n\ndef train(config, model, train_iter, dev_iter, test_iter):\n    start_time = time.time()\n    model.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n\n    # 学习率指数衰减，每次epoch：学习率 = gamma * 学习率\n    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n    total_batch = 0  # 记录进行到多少batch\n    dev_best_loss = float('inf')\n    last_improve = 0  # 记录上次验证集loss下降的batch数\n    flag = False  # 记录是否很久没有效果提升\n    writer = SummaryWriter(log_dir=config.log_path + '/' + time.strftime('%m-%d_%H.%M', time.localtime()))\n    for epoch in range(config.num_epochs):\n        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n        # scheduler.step() # 学习率衰减\n        for i, (trains, labels) in enumerate(train_iter):\n            outputs = model(trains)\n            model.zero_grad()\n            loss = F.cross_entropy(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            if total_batch % 100 == 0:\n                # 每多少轮输出在训练集和验证集上的效果\n                true = labels.data.cpu()\n                predic = torch.max(outputs.data, 1)[1].cpu()\n                train_acc = metrics.accuracy_score(true, predic)\n                dev_acc, dev_loss = evaluate(config, model, dev_iter)\n                if dev_loss < dev_best_loss:\n                    dev_best_loss = dev_loss\n                    torch.save(model.state_dict(), config.save_path)\n                    improve = '*'\n                    last_improve = total_batch\n                else:\n                    improve = ''\n                time_dif = get_time_dif(start_time)\n                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n                writer.add_scalar(\"loss/train\", loss.item(), total_batch)\n                writer.add_scalar(\"loss/dev\", dev_loss, total_batch)\n                writer.add_scalar(\"acc/train\", train_acc, total_batch)\n                writer.add_scalar(\"acc/dev\", dev_acc, total_batch)\n                model.train()\n            total_batch += 1\n            if total_batch - last_improve > config.require_improvement:\n                # 验证集loss超过1000batch没下降，结束训练\n                print(\"No optimization for a long time, auto-stopping...\")\n                flag = True\n                break\n        if flag:\n            break\n    writer.close()\n    test(config, model, test_iter)\n\n\ndef test(config, model, test_iter):\n    # test\n    model.load_state_dict(torch.load(config.save_path))\n    model.eval()\n    start_time = time.time()\n    test_acc, test_loss, test_report, test_confusion = evaluate(config, model, test_iter, test=True)\n    msg = 'Test Loss: {0:>5.2},  Test Acc: {1:>6.2%}'\n    print(msg.format(test_loss, test_acc))\n    print(\"Precision, Recall and F1-Score...\")\n    print(test_report)\n    print(\"Confusion Matrix...\")\n    print(test_confusion)\n    time_dif = get_time_dif(start_time)\n    print(\"Time usage:\", time_dif)\n\n\ndef evaluate(config, model, data_iter, test=False):\n    model.eval()\n    loss_total = 0\n    predict_all = np.array([], dtype=int)\n    labels_all = np.array([], dtype=int)\n    with torch.no_grad():\n        for texts, labels in data_iter:\n            outputs = model(texts)\n            loss = F.cross_entropy(outputs, labels)\n            loss_total += loss\n            labels = labels.data.cpu().numpy()\n            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n            labels_all = np.append(labels_all, labels)\n            predict_all = np.append(predict_all, predic)\n\n    acc = metrics.accuracy_score(labels_all, predict_all)\n    if test:\n        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=4)\n        confusion = metrics.confusion_matrix(labels_all, predict_all)\n        return acc, loss_total / len(data_iter), report, confusion\n    return acc, loss_total / len(data_iter)"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 5.5791015625,
          "content": "# coding: UTF-8\nimport os\nimport torch\nimport numpy as np\nimport pickle as pkl\nfrom tqdm import tqdm\nimport time\nfrom datetime import timedelta\n\n\nMAX_VOCAB_SIZE = 10000  # 词表长度限制\nUNK, PAD = '<UNK>', '<PAD>'  # 未知字，padding符号\n\n\ndef build_vocab(file_path, tokenizer, max_size, min_freq):\n    vocab_dic = {}\n    with open(file_path, 'r', encoding='UTF-8') as f:\n        for line in tqdm(f):\n            lin = line.strip()\n            if not lin:\n                continue\n            content = lin.split('\\t')[0]\n            for word in tokenizer(content):\n                vocab_dic[word] = vocab_dic.get(word, 0) + 1\n        vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] >= min_freq], key=lambda x: x[1], reverse=True)[:max_size]\n        vocab_dic = {word_count[0]: idx for idx, word_count in enumerate(vocab_list)}\n        vocab_dic.update({UNK: len(vocab_dic), PAD: len(vocab_dic) + 1})\n    return vocab_dic\n\n\ndef build_dataset(config, ues_word):\n    if ues_word:\n        tokenizer = lambda x: x.split(' ')  # 以空格隔开，word-level\n    else:\n        tokenizer = lambda x: [y for y in x]  # char-level\n    if os.path.exists(config.vocab_path):\n        vocab = pkl.load(open(config.vocab_path, 'rb'))\n    else:\n        vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)\n        pkl.dump(vocab, open(config.vocab_path, 'wb'))\n    print(f\"Vocab size: {len(vocab)}\")\n\n    def load_dataset(path, pad_size=32):\n        contents = []\n        with open(path, 'r', encoding='UTF-8') as f:\n            for line in tqdm(f):\n                lin = line.strip()\n                if not lin:\n                    continue\n                content, label = lin.split('\\t')\n                words_line = []\n                token = tokenizer(content)\n                seq_len = len(token)\n                if pad_size:\n                    if len(token) < pad_size:\n                        token.extend([PAD] * (pad_size - len(token)))\n                    else:\n                        token = token[:pad_size]\n                        seq_len = pad_size\n                # word to id\n                for word in token:\n                    words_line.append(vocab.get(word, vocab.get(UNK)))\n                contents.append((words_line, int(label), seq_len))\n        return contents  # [([...], 0), ([...], 1), ...]\n    train = load_dataset(config.train_path, config.pad_size)\n    dev = load_dataset(config.dev_path, config.pad_size)\n    test = load_dataset(config.test_path, config.pad_size)\n    return vocab, train, dev, test\n\n\nclass DatasetIterater(object):\n    def __init__(self, batches, batch_size, device):\n        self.batch_size = batch_size\n        self.batches = batches\n        self.n_batches = len(batches) // batch_size\n        self.residue = False  # 记录batch数量是否为整数\n        if len(batches) % self.n_batches != 0:\n            self.residue = True\n        self.index = 0\n        self.device = device\n\n    def _to_tensor(self, datas):\n        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n\n        # pad前的长度(超过pad_size的设为pad_size)\n        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n        return (x, seq_len), y\n\n    def __next__(self):\n        if self.residue and self.index == self.n_batches:\n            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n            self.index += 1\n            batches = self._to_tensor(batches)\n            return batches\n\n        elif self.index >= self.n_batches:\n            self.index = 0\n            raise StopIteration\n        else:\n            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n            self.index += 1\n            batches = self._to_tensor(batches)\n            return batches\n\n    def __iter__(self):\n        return self\n\n    def __len__(self):\n        if self.residue:\n            return self.n_batches + 1\n        else:\n            return self.n_batches\n\n\ndef build_iterator(dataset, config):\n    iter = DatasetIterater(dataset, config.batch_size, config.device)\n    return iter\n\n\ndef get_time_dif(start_time):\n    \"\"\"获取已使用时间\"\"\"\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))\n\n\nif __name__ == \"__main__\":\n    '''提取预训练词向量'''\n    # 下面的目录、文件名按需更改。\n    train_dir = \"./THUCNews/data/train.txt\"\n    vocab_dir = \"./THUCNews/data/vocab.pkl\"\n    pretrain_dir = \"./THUCNews/data/sgns.sogou.char\"\n    emb_dim = 300\n    filename_trimmed_dir = \"./THUCNews/data/embedding_SougouNews\"\n    if os.path.exists(vocab_dir):\n        word_to_id = pkl.load(open(vocab_dir, 'rb'))\n    else:\n        # tokenizer = lambda x: x.split(' ')  # 以词为单位构建词表(数据集中词之间以空格隔开)\n        tokenizer = lambda x: [y for y in x]  # 以字为单位构建词表\n        word_to_id = build_vocab(train_dir, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)\n        pkl.dump(word_to_id, open(vocab_dir, 'wb'))\n\n    embeddings = np.random.rand(len(word_to_id), emb_dim)\n    f = open(pretrain_dir, \"r\", encoding='UTF-8')\n    for i, line in enumerate(f.readlines()):\n        # if i == 0:  # 若第一行是标题，则跳过\n        #     continue\n        lin = line.strip().split(\" \")\n        if lin[0] in word_to_id:\n            idx = word_to_id[lin[0]]\n            emb = [float(x) for x in lin[1:301]]\n            embeddings[idx] = np.asarray(emb, dtype='float32')\n    f.close()\n    np.savez_compressed(filename_trimmed_dir, embeddings=embeddings)\n"
        },
        {
          "name": "utils_fasttext.py",
          "type": "blob",
          "size": 6.0751953125,
          "content": "# coding: UTF-8\nimport os\nimport torch\nimport numpy as np\nimport pickle as pkl\nfrom tqdm import tqdm\nimport time\nfrom datetime import timedelta\n\n\nMAX_VOCAB_SIZE = 10000\nUNK, PAD = '<UNK>', '<PAD>'\n\n\ndef build_vocab(file_path, tokenizer, max_size, min_freq):\n    vocab_dic = {}\n    with open(file_path, 'r', encoding='UTF-8') as f:\n        for line in tqdm(f):\n            lin = line.strip()\n            if not lin:\n                continue\n            content = lin.split('\\t')[0]\n            for word in tokenizer(content):\n                vocab_dic[word] = vocab_dic.get(word, 0) + 1\n        vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] >= min_freq], key=lambda x: x[1], reverse=True)[:max_size]\n        vocab_dic = {word_count[0]: idx for idx, word_count in enumerate(vocab_list)}\n        vocab_dic.update({UNK: len(vocab_dic), PAD: len(vocab_dic) + 1})\n    return vocab_dic\n\n\ndef build_dataset(config, ues_word):\n    if ues_word:\n        tokenizer = lambda x: x.split(' ')  # 以空格隔开，word-level\n    else:\n        tokenizer = lambda x: [y for y in x]  # char-level\n    if os.path.exists(config.vocab_path):\n        vocab = pkl.load(open(config.vocab_path, 'rb'))\n    else:\n        vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)\n        pkl.dump(vocab, open(config.vocab_path, 'wb'))\n    print(f\"Vocab size: {len(vocab)}\")\n\n    def biGramHash(sequence, t, buckets):\n        t1 = sequence[t - 1] if t - 1 >= 0 else 0\n        return (t1 * 14918087) % buckets\n\n    def triGramHash(sequence, t, buckets):\n        t1 = sequence[t - 1] if t - 1 >= 0 else 0\n        t2 = sequence[t - 2] if t - 2 >= 0 else 0\n        return (t2 * 14918087 * 18408749 + t1 * 14918087) % buckets\n\n    def load_dataset(path, pad_size=32):\n        contents = []\n        with open(path, 'r', encoding='UTF-8') as f:\n            for line in tqdm(f):\n                lin = line.strip()\n                if not lin:\n                    continue\n                content, label = lin.split('\\t')\n                words_line = []\n                token = tokenizer(content)\n                seq_len = len(token)\n                if pad_size:\n                    if len(token) < pad_size:\n                        token.extend([PAD] * (pad_size - len(token)))\n                    else:\n                        token = token[:pad_size]\n                        seq_len = pad_size\n                # word to id\n                for word in token:\n                    words_line.append(vocab.get(word, vocab.get(UNK)))\n\n                # fasttext ngram\n                buckets = config.n_gram_vocab\n                bigram = []\n                trigram = []\n                # ------ngram------\n                for i in range(pad_size):\n                    bigram.append(biGramHash(words_line, i, buckets))\n                    trigram.append(triGramHash(words_line, i, buckets))\n                # -----------------\n                contents.append((words_line, int(label), seq_len, bigram, trigram))\n        return contents  # [([...], 0), ([...], 1), ...]\n    train = load_dataset(config.train_path, config.pad_size)\n    dev = load_dataset(config.dev_path, config.pad_size)\n    test = load_dataset(config.test_path, config.pad_size)\n    return vocab, train, dev, test\n\n\nclass DatasetIterater(object):\n    def __init__(self, batches, batch_size, device):\n        self.batch_size = batch_size\n        self.batches = batches\n        self.n_batches = len(batches) // batch_size\n        self.residue = False  # 记录batch数量是否为整数 \n        if len(batches) % self.n_batches != 0:\n            self.residue = True\n        self.index = 0\n        self.device = device\n\n    def _to_tensor(self, datas):\n        # xx = [xxx[2] for xxx in datas]\n        # indexx = np.argsort(xx)[::-1]\n        # datas = np.array(datas)[indexx]\n        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n        bigram = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n        trigram = torch.LongTensor([_[4] for _ in datas]).to(self.device)\n\n        # pad前的长度(超过pad_size的设为pad_size)\n        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n        return (x, seq_len, bigram, trigram), y\n\n    def __next__(self):\n        if self.residue and self.index == self.n_batches:\n            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n            self.index += 1\n            batches = self._to_tensor(batches)\n            return batches\n\n        elif self.index >= self.n_batches:\n            self.index = 0\n            raise StopIteration\n        else:\n            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n            self.index += 1\n            batches = self._to_tensor(batches)\n            return batches\n\n    def __iter__(self):\n        return self\n\n    def __len__(self):\n        if self.residue:\n            return self.n_batches + 1\n        else:\n            return self.n_batches\n\n\ndef build_iterator(dataset, config):\n    iter = DatasetIterater(dataset, config.batch_size, config.device)\n    return iter\n\n\ndef get_time_dif(start_time):\n    \"\"\"获取已使用时间\"\"\"\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))\n\nif __name__ == \"__main__\":\n    '''提取预训练词向量'''\n    vocab_dir = \"./THUCNews/data/vocab.pkl\"\n    pretrain_dir = \"./THUCNews/data/sgns.sogou.char\"\n    emb_dim = 300\n    filename_trimmed_dir = \"./THUCNews/data/vocab.embedding.sougou\"\n    word_to_id = pkl.load(open(vocab_dir, 'rb'))\n    embeddings = np.random.rand(len(word_to_id), emb_dim)\n    f = open(pretrain_dir, \"r\", encoding='UTF-8')\n    for i, line in enumerate(f.readlines()):\n        # if i == 0:  # 若第一行是标题，则跳过\n        #     continue\n        lin = line.strip().split(\" \")\n        if lin[0] in word_to_id:\n            idx = word_to_id[lin[0]]\n            emb = [float(x) for x in lin[1:301]]\n            embeddings[idx] = np.asarray(emb, dtype='float32')\n    f.close()\n    np.savez_compressed(filename_trimmed_dir, embeddings=embeddings)\n"
        }
      ]
    }
  ]
}