{
  "metadata": {
    "timestamp": 1736560876337,
    "page": 591,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "meta-llama/llama-stack",
      "stars": 5972,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".flake8",
          "type": "blob",
          "size": 1.08984375,
          "content": "[flake8]\n# Suggested config from pytorch that we can adapt\nselect = B,C,E,F,N,P,T4,W,B9,TOR0,TOR1,TOR2\nmax-line-length = 120\n# C408 ignored because we like the dict keyword argument syntax\n# E501 is not flexible enough, we're using B950 instead\n# N812 ignored because import torch.nn.functional as F is PyTorch convention\n# N817 ignored because importing using acronyms is convention (DistributedDataParallel as DDP)\n# E731 allow usage of assigning lambda expressions\n# E701 let black auto-format statements on one line\n# E704 let black auto-format statements on one line\nignore =\n    E203,E305,E402,E501,E721,E741,F405,F821,F841,F999,W503,W504,C408,E302,W291,E303,N812,N817,E731,E701,E704\n    # shebang has extra meaning in fbcode lints, so I think it's not worth trying\n    # to line this up with executable bit\n    EXE001,\n    # random naming hints don't need\n    N802,\n    # these ignores are from flake8-bugbear; please fix!\n    B007,B008,B950\noptional-ascii-coding = True\nexclude =\n    ./.git,\n    ./docs/*,\n    ./build,\n    ./scripts,\n    ./venv,\n    *.pyi,\n    .pre-commit-config.yaml,\n    *.md,\n    .flake8\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2236328125,
          "content": ".env\n__pycache__\ndist\n*.egg-info\ndev_requirements.txt\nbuild\n.DS_Store\nllama_stack/configs/*\nxcuserdata/\n*.hmap\n.DS_Store\n.build/\nPackage.resolved\n*.pte\n*.ipynb_checkpoints*\n.idea\n.venv/\n.vscode\n_build\ndocs/src\npyrightconfig.json\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.169921875,
          "content": "[submodule \"llama_stack/providers/impls/ios/inference/executorch\"]\n\tpath = llama_stack/providers/inline/ios/inference/executorch\n\turl = https://github.com/pytorch/executorch\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.8505859375,
          "content": "exclude: 'build/'\n\ndefault_language_version:\n    python: python3\n\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: 6306a48f7dae5861702d573c9c247e4e9498e867\n    hooks:\n    -   id: trailing-whitespace\n    -   id: check-ast\n    -   id: check-merge-conflict\n    -   id: check-added-large-files\n        args: ['--maxkb=1000']\n    -   id: end-of-file-fixer\n        exclude: '^(.*\\.svg)$'\n\n# Temporarily disabling this\n#    -   id: no-commit-to-branch\n#        args: ['--branch=main']\n\n-   repo: https://github.com/Lucas-C/pre-commit-hooks\n    rev: v1.5.4\n    hooks:\n    -   id: insert-license\n        files: \\.py$|\\.sh$\n        args:\n          - --license-filepath\n          - docs/license_header.txt\n\n-   repo: https://github.com/pycqa/flake8\n    rev: 34cbf8ef3950f43d09b85e2e45c15ae5717dc37b\n    hooks:\n    -   id: flake8\n        additional_dependencies:\n          - flake8-bugbear == 22.4.25\n          - pep8-naming == 0.12.1\n          - torchfix\n        args: ['--config=.flake8']\n\n-   repo: https://github.com/omnilib/ufmt\n    rev: v2.7.0\n    hooks:\n    -   id: ufmt\n        additional_dependencies:\n          - black == 24.4.2\n          - usort == 1.0.8\n\n# - repo: https://github.com/jsh9/pydoclint\n#   rev: d88180a8632bb1602a4d81344085cf320f288c5a\n#   hooks:\n#     - id: pydoclint\n#       args: [--config=pyproject.toml]\n\n# - repo: https://github.com/tcort/markdown-link-check\n#   rev: v3.11.2\n#   hooks:\n#     - id: markdown-link-check\n#       args: ['--quiet']\n\n# -   repo: local\n#     hooks:\n#       - id: distro-codegen\n#         name: Distribution Template Codegen\n#         additional_dependencies:\n#           - rich\n#           - pydantic\n#         entry: python -m llama_stack.scripts.distro_codegen\n#         language: python\n#         pass_filenames: false\n#         require_serial: true\n#         files: ^llama_stack/templates/.*$\n#         stages: [manual]\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.8056640625,
          "content": "# .readthedocs.yaml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Set the OS, Python version and other tools you might need\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.12\"\n    # You can also specify other tool versions:\n    # nodejs: \"19\"\n    # rust: \"1.64\"\n    # golang: \"1.19\"\n\n# Build documentation in the \"docs/\" directory with Sphinx\nsphinx:\n  configuration: docs/source/conf.py\n\n# Optionally build your docs in additional formats such as PDF and ePub\n# formats:\n#    - pdf\n#    - epub\n\n# Optional but recommended, declare the Python requirements required\n# to build your documentation\n# See https://docs.readthedocs.io/en/stable/guides/reproducible-builds.html\npython:\n   install:\n   - requirements: docs/requirements.txt\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 1.48828125,
          "content": "# Changelog\n\n## 0.0.53\n\n### Added\n- Resource-oriented design for models, shields, memory banks, datasets and eval tasks\n- Persistence for registered objects with distribution\n- Ability to persist memory banks created for FAISS\n- PostgreSQL KVStore implementation\n- Environment variable placeholder support in run.yaml files\n- Comprehensive Zero-to-Hero notebooks and quickstart guides\n- Support for quantized models in Ollama\n- Vision models support for Together, Fireworks, Meta-Reference, and Ollama, and vLLM\n- Bedrock distribution with safety shields support\n- Evals API with task registration and scoring functions\n- MMLU and SimpleQA benchmark scoring functions\n- Huggingface dataset provider integration for benchmarks\n- Support for custom dataset registration from local paths\n- Benchmark evaluation CLI tools with visualization tables\n- RAG evaluation scoring functions and metrics\n- Local persistence for datasets and eval tasks\n\n### Changed\n- Split safety into distinct providers (llama-guard, prompt-guard, code-scanner)\n- Changed provider naming convention (`impls` → `inline`, `adapters` → `remote`)\n- Updated API signatures for dataset and eval task registration\n- Restructured folder organization for providers\n- Enhanced Docker build configuration\n- Added version prefixing for REST API routes\n- Enhanced evaluation task registration workflow\n- Improved benchmark evaluation output formatting\n- Restructured evals folder organization for better modularity\n\n### Removed\n- `llama stack configure` command\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.4541015625,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\nThis Code of Conduct also applies outside the project spaces when there is a\nreasonable belief that an individual's behavior may have a negative impact on\nthe project or its community.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <opensource-conduct@meta.com>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.8701171875,
          "content": "# Contributing to Llama-Stack\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `main`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n\n### Updating Provider Configurations\n\nIf you have made changes to a provider's configuration in any form (introducing a new config key, or changing models, etc.), you should run `python llama_stack/scripts/distro_codegen.py` to re-generate various YAML files as well as the documentation. You should not change `docs/source/.../distributions/` files manually as they are auto-generated.\n\n### Building the Documentation\n\nIf you are making changes to the documentation at [https://llama-stack.readthedocs.io/en/latest/](https://llama-stack.readthedocs.io/en/latest/), you can use the following command to build the documentation and preview your changes. You will need [Sphinx](https://www.sphinx-doc.org/en/master/) and the readthedocs theme.\n\n```bash\ncd llama-stack/docs\npip install -r requirements.txt\npip install sphinx-autobuild\n\n# This will start a local server (usually at http://127.0.0.1:8000) that automatically rebuilds and refreshes when you make changes to the documentation.\nmake html\nsphinx-autobuild source build/html\n```\n\n## Pre-commit Hooks\n\nWe use [pre-commit](https://pre-commit.com/) to run linting and formatting checks on your code. You can install the pre-commit hooks by running:\n\n```bash\n$ cd llama-stack\n$ conda activate <your-environment>\n$ pip install pre-commit\n$ pre-commit install\n```\n\nAfter that, pre-commit hooks will run automatically before each commit.\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Meta's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nMeta has a [bounty program](http://facebook.com/whitehat/info) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## Coding Style\n* 2 spaces for indentation rather than tabs\n* 80 character line length\n* ...\n\n## Tips\n* If you are developing with a llama-stack repository checked out and need your distribution to reflect changes from there, set `LLAMA_STACK_DIR` to that dir when running any of the `llama` CLI commands.\n\n## License\nBy contributing to Llama, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0615234375,
          "content": "MIT License\n\nCopyright (c) Meta Platforms, Inc. and affiliates\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.1748046875,
          "content": "include requirements.txt\ninclude distributions/dependencies.json\ninclude llama_stack/distribution/*.sh\ninclude llama_stack/cli/scripts/*.sh\ninclude llama_stack/templates/*/*.yaml\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.369140625,
          "content": "# Llama Stack\n\n[![PyPI version](https://img.shields.io/pypi/v/llama_stack.svg)](https://pypi.org/project/llama_stack/)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-stack)](https://pypi.org/project/llama-stack/)\n[![Discord](https://img.shields.io/discord/1257833999603335178)](https://discord.gg/llama-stack)\n\n[**Quick Start**](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html) | [**Documentation**](https://llama-stack.readthedocs.io/en/latest/index.html) | [**Zero-to-Hero Guide**](https://github.com/meta-llama/llama-stack/tree/main/docs/zero_to_hero_guide)\n\nLlama Stack defines and standardizes the set of core building blocks needed to bring generative AI applications to market. These building blocks are presented in the form of interoperable APIs with a broad set of Service Providers providing their implementations.\n\n<div style=\"text-align: center;\">\n  <img\n    src=\"https://github.com/user-attachments/assets/33d9576d-95ea-468d-95e2-8fa233205a50\"\n    width=\"480\"\n    title=\"Llama Stack\"\n    alt=\"Llama Stack\"\n  />\n</div>\n\nOur goal is to provide pre-packaged implementations which can be operated in a variety of deployment environments: developers start iterating with Desktops or their mobile devices and can seamlessly transition to on-prem or public cloud deployments. At every point in this transition, the same set of APIs and the same developer experience is available.\n\n> ⚠️ **Note**\n> The Stack APIs are rapidly improving, but still very much work in progress and we invite feedback as well as direct contributions.\n\n\n## APIs\n\nWe have working implementations of the following APIs today:\n- Inference\n- Safety\n- Memory\n- Agents\n- Eval\n- Telemetry\n\nAlongside these APIs, we also related APIs for operating with associated resources (see [Concepts](https://llama-stack.readthedocs.io/en/latest/concepts/index.html#resources)):\n\n- Models\n- Shields\n- Memory Banks\n- Eval Tasks\n- Datasets\n- Scoring Functions\n\nWe are also working on the following APIs which will be released soon:\n\n- Post Training\n- Synthetic Data Generation\n- Reward Scoring\n\nEach of the APIs themselves is a collection of REST endpoints.\n\n## Philosophy\n\n### Service-oriented design\n\nUnlike other frameworks, Llama Stack is built with a service-oriented, REST API-first approach. Such a design not only allows for seamless transitions from a local to remote deployments, but also forces the design to be more declarative. We believe this restriction can result in a much simpler, robust developer experience. This will necessarily trade-off against expressivity however if we get the APIs right, it can lead to a very powerful platform.\n\n### Composability\n\nWe expect the set of APIs we design to be composable. An Agent abstractly depends on { Inference, Memory, Safety } APIs but does not care about the actual implementation details. Safety itself may require model inference and hence can depend on the Inference API.\n\n### Turnkey one-stop solutions\n\nWe expect to provide turnkey solutions for popular deployment scenarios. It should be easy to deploy a Llama Stack server on AWS or on a private data center. Either of these should allow a developer to get started with powerful agentic apps, model evaluations or fine-tuning services in a matter of minutes. They should all result in the same uniform observability and developer experience.\n\n### Focus on Llama models\n\nAs a Meta initiated project, we have started by explicitly focusing on Meta's Llama series of models. Supporting the broad set of open models is no easy task and we want to start with models we understand best.\n\n### Supporting the Ecosystem\n\nThere is a vibrant ecosystem of Providers which provide efficient inference or scalable vector stores or powerful observability solutions. We want to make sure it is easy for developers to pick and choose the best implementations for their use cases. We also want to make sure it is easy for new Providers to onboard and participate in the ecosystem.\n\nAdditionally, we have designed every element of the Stack such that APIs as well as Resources (like Models) can be federated.\n\n\n## Supported Llama Stack Implementations\n### API Providers\n|                                  **API Provider Builder**                                  |    **Environments**    |     **Agents**     |   **Inference**    |     **Memory**     |     **Safety**     |   **Telemetry**    |\n|:------------------------------------------------------------------------------------------:|:----------------------:|:------------------:|:------------------:|:------------------:|:------------------:|:------------------:|\n|                                       Meta Reference                                       |      Single Node       | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n|                                          Cerebras                                          |         Hosted         |                    | :heavy_check_mark: |                    |                    |                    |\n|                                         Fireworks                                          |         Hosted         | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |                    |                    |\n|                                        AWS Bedrock                                         |         Hosted         |                    | :heavy_check_mark: |                    | :heavy_check_mark: |                    |\n|                                          Together                                          |         Hosted         | :heavy_check_mark: | :heavy_check_mark: |                    | :heavy_check_mark: |                    |\n|                                            Groq                                            |         Hosted         |                    | :heavy_check_mark: |                    |                    |                    |\n|                                           Ollama                                           |      Single Node       |                    | :heavy_check_mark: |                    |                    |                    |\n|                                            TGI                                             | Hosted and Single Node |                    | :heavy_check_mark: |                    |                    |                    |\n| [NVIDIA NIM](https://build.nvidia.com/nim?filters=nimType%3Anim_type_run_anywhere&q=llama) | Hosted and Single Node |                    | :heavy_check_mark: |                    |                    |                    |\n|                                           Chroma                                           |      Single Node       |                    |                    | :heavy_check_mark: |                    |                    |\n|                                         PG Vector                                          |      Single Node       |                    |                    | :heavy_check_mark: |                    |                    |\n|                                     PyTorch ExecuTorch                                     |     On-device iOS      | :heavy_check_mark: | :heavy_check_mark: |                    |                    |                    |\n|                        [vLLM](https://github.com/vllm-project/vllm)                        | Hosted and Single Node |                    | :heavy_check_mark: |                    |                    |                    |\n\n### Distributions\n\n|               **Distribution**                |                                                                    **Llama Stack Docker**                                                                     |                                                 Start This Distribution                                                  |\n|:---------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------------:|\n|                Meta Reference                 |           [llamastack/distribution-meta-reference-gpu](https://hub.docker.com/repository/docker/llamastack/distribution-meta-reference-gpu/general)           |      [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/meta-reference-gpu.html)      |\n|           Meta Reference Quantized            | [llamastack/distribution-meta-reference-quantized-gpu](https://hub.docker.com/repository/docker/llamastack/distribution-meta-reference-quantized-gpu/general) | [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/meta-reference-quantized-gpu.html) |\n|                   Cerebras                    |                     [llamastack/distribution-cerebras](https://hub.docker.com/repository/docker/llamastack/distribution-cerebras/general)                     |   [Guide](https://llama-stack.readthedocs.io/en/latest/getting_started/distributions/self_hosted_distro/cerebras.html)   |\n|                    Ollama                     |                       [llamastack/distribution-ollama](https://hub.docker.com/repository/docker/llamastack/distribution-ollama/general)                       |            [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/ollama.html)            |\n|                      TGI                      |                          [llamastack/distribution-tgi](https://hub.docker.com/repository/docker/llamastack/distribution-tgi/general)                          |             [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/tgi.html)              |\n|                   Together                    |                     [llamastack/distribution-together](https://hub.docker.com/repository/docker/llamastack/distribution-together/general)                     |           [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/together.html)           |\n|                   Fireworks                   |                    [llamastack/distribution-fireworks](https://hub.docker.com/repository/docker/llamastack/distribution-fireworks/general)                    |          [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/fireworks.html)           |\n| [vLLM](https://github.com/vllm-project/vllm)  |                  [llamastack/distribution-remote-vllm](https://hub.docker.com/repository/docker/llamastack/distribution-remote-vllm/general)                  |         [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/remote-vllm.html)          |\n\n## Installation\n\nYou have two ways to install this repository:\n\n1. **Install as a package**:\n   You can install the repository directly from [PyPI](https://pypi.org/project/llama-stack/) by running the following command:\n   ```bash\n   pip install llama-stack\n   ```\n\n2. **Install from source**:\n   If you prefer to install from the source code, make sure you have [conda installed](https://docs.conda.io/projects/conda/en/stable).\n   Then, follow these steps:\n   ```bash\n    mkdir -p ~/local\n    cd ~/local\n    git clone git@github.com:meta-llama/llama-stack.git\n\n    conda create -n stack python=3.10\n    conda activate stack\n\n    cd llama-stack\n    pip install -e .\n   ```\n\n## Documentation\n\nPlease checkout our [Documentation](https://llama-stack.readthedocs.io/en/latest/index.html) page for more details.\n\n* [CLI reference](https://llama-stack.readthedocs.io/en/latest/references/llama_cli_reference/index.html)\n    * Guide using `llama` CLI to work with Llama models (download, study prompts), and building/starting a Llama Stack distribution.\n* [Getting Started](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html)\n    * Quick guide to start a Llama Stack server.\n    * [Jupyter notebook](./docs/notebooks/Llama_Stack_Building_AI_Applications.ipynb) to walk-through how to use simple text and vision inference llama_stack_client APIs\n    * The complete Llama Stack lesson [Colab notebook](https://colab.research.google.com/drive/1dtVmxotBsI4cGZQNsJRYPrLiDeT0Wnwt) of the new [Llama 3.2 course on Deeplearning.ai](https://learn.deeplearning.ai/courses/introducing-multimodal-llama-3-2/lesson/8/llama-stack).\n    * A [Zero-to-Hero Guide](https://github.com/meta-llama/llama-stack/tree/main/docs/zero_to_hero_guide) that guide you through all the key components of llama stack with code samples.\n* [Contributing](CONTRIBUTING.md)\n    * [Adding a new API Provider](https://llama-stack.readthedocs.io/en/latest/contributing/new_api_provider.html) to walk-through how to add a new API provider.\n\n## Llama Stack Client SDKs\n\n|  **Language** |  **Client SDK** | **Package** |\n| :----: | :----: | :----: |\n| Python |  [llama-stack-client-python](https://github.com/meta-llama/llama-stack-client-python) | [![PyPI version](https://img.shields.io/pypi/v/llama_stack_client.svg)](https://pypi.org/project/llama_stack_client/)\n| Swift  | [llama-stack-client-swift](https://github.com/meta-llama/llama-stack-client-swift) | [![Swift Package Index](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fmeta-llama%2Fllama-stack-client-swift%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/meta-llama/llama-stack-client-swift)\n| Node   | [llama-stack-client-node](https://github.com/meta-llama/llama-stack-client-node) | [![NPM version](https://img.shields.io/npm/v/llama-stack-client.svg)](https://npmjs.org/package/llama-stack-client)\n| Kotlin | [llama-stack-client-kotlin](https://github.com/meta-llama/llama-stack-client-kotlin) | [![Maven version](https://img.shields.io/maven-central/v/com.llama.llamastack/llama-stack-client-kotlin)](https://central.sonatype.com/artifact/com.llama.llamastack/llama-stack-client-kotlin)\n\nCheck out our client SDKs for connecting to Llama Stack server in your preferred language, you can choose from [python](https://github.com/meta-llama/llama-stack-client-python), [node](https://github.com/meta-llama/llama-stack-client-node), [swift](https://github.com/meta-llama/llama-stack-client-swift), and [kotlin](https://github.com/meta-llama/llama-stack-client-kotlin) programming languages to quickly build your applications.\n\nYou can find more example scripts with client SDKs to talk with the Llama Stack server in our [llama-stack-apps](https://github.com/meta-llama/llama-stack-apps/tree/main/examples) repo.\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.1328125,
          "content": "# Security Policy\n\n## Reporting a Vulnerability\n\nPlease report vulnerabilities to our bug bounty program at https://bugbounty.meta.com/\n"
        },
        {
          "name": "distributions",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "llama_stack",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.0849609375,
          "content": "[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.15625,
          "content": "blobfile\nfire\nhttpx\nhuggingface-hub\nllama-models>=0.0.63\nllama-stack-client>=0.0.63\nprompt-toolkit\npython-dotenv\npydantic>=2\nrequests\nrich\nsetuptools\ntermcolor\n"
        },
        {
          "name": "rfcs",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.5087890625,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the terms described in the LICENSE file in\n# the root directory of this source tree.\n\nfrom setuptools import find_packages, setup\n\n\n# Function to read the requirements.txt file\ndef read_requirements():\n    with open(\"requirements.txt\") as req:\n        content = req.readlines()\n    return [line.strip() for line in content]\n\n\nsetup(\n    name=\"llama_stack\",\n    version=\"0.0.63\",\n    author=\"Meta Llama\",\n    author_email=\"llama-oss@meta.com\",\n    description=\"Llama Stack\",\n    entry_points={\n        \"console_scripts\": [\n            \"llama = llama_stack.cli.llama:main\",\n            \"install-wheel-from-presigned = llama_stack.cli.scripts.run:install_wheel_from_presigned\",\n        ]\n    },\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/meta-llama/llama-stack\",\n    packages=find_packages(),\n    classifiers=[\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Information Technology\",\n        \"Intended Audience :: Science/Research\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Scientific/Engineering :: Information Analysis\",\n    ],\n    python_requires=\">=3.10\",\n    install_requires=read_requirements(),\n    include_package_data=True,\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}