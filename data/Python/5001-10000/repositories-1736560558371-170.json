{
  "metadata": {
    "timestamp": 1736560558371,
    "page": 170,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "THUDM/CodeGeeX",
      "stars": 8333,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.1650390625,
          "content": "[submodule \"vscode-extension/codegeex-vscode-extension\"]\n\tpath = vscode-extension/codegeex-vscode-extension\n\turl = git@github.com:CodeGeeX/codegeex-vscode-extension.git\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MODEL_LICENSE",
          "type": "blob",
          "size": 2.2822265625,
          "content": "The CodeGeeX License\n\n1. Definitions\n\nâ€œLicensorâ€ means the CodeGeeX Model Team that distributes its Software.\n\nâ€œSoftwareâ€ means the CodeGeeX model parameters made available under this license.\n\n2. License Grant\n\nSubject to the terms and conditions of this License, the Licensor hereby grants to you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license to use the Software solely for your non-commercial research purposes.\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n3. Restriction\n\nYou will not use, copy, modify, merge, publish, distribute, reproduce, or create derivative works of the Software, in whole or in part, for any commercial, military, or illegal purposes.\n\nYou will not use the Software for any act that may undermine China's national security and national unity, harm the public interest of society, or infringe upon the rights and interests of human beings.\n\n4. Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n5. Limitation of Liability\n\nEXCEPT TO THE EXTENT PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER BASED IN TORT, NEGLIGENCE, CONTRACT, LIABILITY, OR OTHERWISE WILL ANY LICENSOR BE LIABLE TO YOU FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, OR ANY OTHER COMMERCIAL LOSSES, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n6. Dispute Resolution\n\nThis license shall be governed and construed in accordance with the laws of Peopleâ€™s Republic of China. Any dispute arising from or in connection with this License shall be submitted to Haidian District People's Court in Beijing.\n\nNote that the license is subject to update to a more comprehensive version. For any questions related to the license and copyright, please contact us at report@aminer.cn."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 19.1728515625,
          "content": "<img src=\"resources/logo/codegeex_logo.png\">\n\n<p align=\"center\">\n    ğŸ  <a href=\"https://codegeex.cn\" target=\"_blank\">Homepage</a> | ğŸ“– <a href=\"https://models.aminer.cn/codegeex/blog/\" target=\"_blank\">Blog</a> | ğŸª§ <a href=\"https://models.aminer.cn/codegeex/playground\" target=\"_blank\">DEMO</a> | ğŸ¤– <a href=\"https://codegeex.cn/download/request\" target=\"_blank\">Download Model</a> | ğŸ“„ <a href=\"https://arxiv.org/abs/2303.17568\" target=\"_blank\">Paper</a> | ğŸŒ <a href=\"README_zh.md\" target=\"_blank\">ä¸­æ–‡</a>\n</p>\n<p align=\"center\">\n    ğŸ›  <a href=\"https://marketplace.visualstudio.com/items?itemName=aminer.codegeex\" target=\"_blank\">VS Code</a>, <a href=\"https://plugins.jetbrains.com/plugin/20587-codegeex\" target=\"_blank\">Jetbrains</a>, <a href=\"https://plugins.jetbrains.com/plugin/20587-codegeex\" target=\"_blank\">Cloud Studio</a> supported | ğŸ‘‹ Join our <a href=\"https://discord.gg/8gjHdkmAN6\" target=\"_blank\">Discord</a>, <a href=\"https://join.slack.com/t/codegeexworkspace/shared_invite/zt-1s118ffrp-mpKKhQD0tKBmzNZVCyEZLw\" target=\"_blank\">Slack</a>, <a href=\"https://t.me/+IipIayJ32B1jOTg1\" target=\"_blank\">Telegram</a>, <a href=\"resources/zh/wechat.md\"target=\"_blank\">WeChat</a>\n</p>\n\n\n\nğŸŒŸ The newest [CodeGeeX4](https://github.com/THUDM/CodeGeeX4) has been released. | æœ€æ–°ä¸€ä»£ [CodeGeeX4](https://github.com/THUDM/CodeGeeX4) æ¨¡å‹å·²ç»æ­£å¼å¼€æºã€‚\n\n- [CodeGeeX: A Multilingual Code Generation Model](#codegeex-a-multilingual-code-generation-model)\n  - [News](#news)\n  - [Getting Started](#getting-started)\n    - [Installation](#installation)\n    - [Model Weights](#model-weights)\n    - [Inference on GPUs](#inference-on-gpus)\n    - [VS Code and Jetbrains Extension Guidance](#vs-code-and-jetbrains-extension-guidance)\n  - [CodeGeeX: Architecture, Code Corpus, and Implementation](#codegeex-architecture-code-corpus-and-implementation)\n  - [HumanEval-X: A new benchmark for Multilingual Program Synthesis](#humaneval-x-a-new-benchmark-for-multilingual-program-synthesis)\n    - [Multilingual Code Generation](#multilingual-code-generation)\n    - [Crosslingual Code Translation](#crosslingual-code-translation)\n    - [How to use HumanEval-X and contribute to it?](#how-to-use-humaneval-x-and-contribute-to-it)\n  - [License](#license)\n  - [Citation](#citation)\n\n# CodeGeeX: A Multilingual Code Generation Model\n\nWe introduce CodeGeeX, a large-scale multilingual code generation model with 13 billion parameters, pre-trained on a large code corpus of more than 20 programming languages. As of **June 22**, 2022, CodeGeeX has been trained on more than 850 billion tokens on a cluster of 1,536 [Ascend 910 AI Processors](https://e.huawei.com/en/products/servers/ascend). CodeGeeX has several unique features:\n* **Multilingual Code Generation**: CodeGeeX has good performance for generating executable programs in several mainstream programming languages, including Python, C++, Java, JavaScript, Go, etc. [DEMO](https://models.aminer.cn/codegeex)\n* **Crosslingual Code Translation**: CodeGeeX supports the translation of code snippets between different languages. Simply by one click, CodeGeeX can transform a program into any expected language with a high accuracy. [DEMO](https://models.aminer.cn/codegeex/codeTranslator)\n* **Customizable Programming Assistant**: CodeGeeX is available in the VS Code extension marketplace **for free**. It supports code completion, explanation, summarization and more, which empower users with a better coding experience. [VS Code Extension](https://marketplace.visualstudio.com/items?itemName=aminer.codegeex)\n* **Open-Source and Cross-Platform**: All codes and model weights are publicly available for research purposes. CodeGeeX supports both Ascend and NVIDIA platforms. It supports inference in a single Ascend 910, NVIDIA V100 or A100. [Apply Model Weights](https://models.aminer.cn/codegeex/download/request)\n\n**HumanEval-X for Realistic Multilingual Benchmarking.** To help standardize the evaluation of multilingual code generation and translation, we develop and release the **HumanEval-X** Benchmark. HumanEval-X is a new multilingual benchmark that contains **820 human-crafted** coding problems in **5** programming languages (Python, C++, Java, JavaScript, and Go), each of these problems is associated with tests and solutions. [Usage](codegeex/benchmark/README.md)  [ğŸ¤— Available in HuggingFace](https://huggingface.co/datasets/THUDM/humaneval-x)\n\n<img src=\"resources/en/hx_boxplot.png\">\n\n<p align=\"center\"><i>CodeGeeX achieves the highest average performance compared with other open-sourced multilingual baselines.</i> </p>\n\n## News\n\n* ğŸŒŸ **2023-07-24**: [CodeGeeX2](https://github.com/THUDM/CodeGeeX2) has been released, more powerful, faster, and lightweight. Support 100+ languages and many new features.\n\n* **2023-5-16**: CodeGeeX paper has been accepted by [KDD 2023, Long Beach](https://kdd.org/kdd2023/) and will be represented during the conference.\n\n* **2023-03-30**: CodeGeeX paper is now available at [arxiv](https://arxiv.org/abs/2303.17568).\n\n* **2023-02-14**: CodeGeeX now supports [Cloud Studio](https://cloudstudio.net/), a fantastic web IDE from Tencent. Click on the badge on top of this page to quickly launch an environment to test CodeGeeX.\n\n* **2023-02-13**: Thanks a lot to [OneFlow](https://github.com/Oneflow-Inc/oneflow) team for adding oneflow backend for CodeGeeX's inference (Even faster than FasterTransformer under FP16!). Check more details [here](https://github.com/THUDM/CodeGeeX/pull/65).\n\n* **2023-02**: We are hosting [CodeGeeX \"Coding With AI\" Hackathon](https://dorahacks.io/hackathon/codegeex/), design cool applications based on CodeGeeX and win prizes (RTX 4090, DJI drone, etc)!\n\n* **2022-12-31**: We release the FasterTransformer version of CodeGeeX in [codegeex-fastertransformer](https://github.com/CodeGeeX/codegeex-fastertransformer). The INT8 accelerated version reaches an a verage speed of <15ms/token. Happy new year to everyone!\n\n* **2022-12-13**: We release the source code of CodeGeeX VS Code extension in [codegeex-vscode-extension](https://github.com/CodeGeeX/codegeex-vscode-extension). Follow [QuickStart](https://github.com/CodeGeeX/codegeex-vscode-extension/blob/main/doc/quickstart.md) to start development.\n\n* **2022-12-11**: CodeGeeX is now available for Jetbrains IDEs (IntelliJ IDEA, PyCharm, GoLand, CLion, etc), download it [here](https://plugins.jetbrains.com/plugin/20587-codegeex).\n\n* **2022-12-04**: We release source code of quantization (requires less GPU RAM: 27GB -> 15GB) and model parallelism (possible to run on multiple GPUs with <8G RAM).\n \n* **2022-09-30**: We release the cross-platform source code and models weights for both Ascend and NVIDIA platforms.\n\n## Getting Started\n\nCodeGeeX is initially implemented in Mindspore and trained Ascend 910 AI Processors. We provide a torch-compatible version based on [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) to facilitate usage on GPU platforms.\n### Installation\n\nPython 3.7+ / CUDA 11+ / PyTorch 1.10+ / DeepSpeed 0.6+ are required. Install ``codegeex`` package via: \n```bash\ngit clone git@github.com:THUDM/CodeGeeX.git\ncd CodeGeeX\npip install -e .\n```\nOr use [CodeGeeX docker](https://hub.docker.com/r/codegeex/codegeex) to quickly set up the environment (with [nvidia-docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker) installed):\n```bash\ndocker pull codegeex/codegeex:latest\n# To enable GPU support, clarify device ids with --device\ndocker run --gpus '\"device=0,1\"' -it --ipc=host --name=codegeex codegeex/codegeex\n```\n\n### Model Weights\n\nApply and download model weights through this [link](https://models.aminer.cn/codegeex/download/request). You'll receive by mail ```urls.txt``` that contains temporary download links. We recommend you to use [aria2](https://aria2.github.io/) to download it via the following command (Please make sure you have enough disk space to download the checkpoint (~26GB)):\n```bash\naria2c -x 16 -s 16 -j 4 --continue=true -i urls.txt \n```\nRun the following command to get the full model weights:\n```bash\ncat codegeex_13b.tar.gz.* > codegeex_13b.tar.gz\ntar xvf codegeex_13b.tar.gz\n```\n\n### Inference on GPUs\n\nHave a try on generating the first program with CodeGeeX. First, specify the path of the model weights in ``configs/codegeex_13b.sh``. Second, write the prompt (natural language description or code snippet) into a file, e.g., ``tests/test_prompt.txt``, then run the following script:\n```bash\n# On a single GPU (with more than 27GB RAM)\nbash ./scripts/test_inference.sh <GPU_ID> ./tests/test_prompt.txt\n\n# With quantization (with more than 15GB RAM)\nbash ./scripts/test_inference_quantized.sh <GPU_ID> ./tests/test_prompt.txt\n\n# On multiple GPUs (with more than 6GB RAM, need to first convert ckpt to MP_SIZE partitions)\nbash ./scripts/convert_ckpt_parallel.sh <LOAD_CKPT_PATH> <SAVE_CKPT_PATH> <MP_SIZE>\nbash ./scripts/test_inference_parallel.sh <MP_SIZE> ./tests/test_prompt.txt\n```\n\n### VS Code and Jetbrains Extension Guidance\n\nBased on CodeGeeX, we also develop free extentions for VS Code and Jetbrains IDEs, and more in the future. \n\nFor VS Code, search \"codegeex\" in Marketplace or install it [here](https://marketplace.visualstudio.com/items?itemName=aminer.codegeex). Detailed instructions can be found in \n[VS Code Extension Guidance](vscode-extension/README.md). For developers, we have also released the source code in [codegeex-vscode-extension](https://github.com/CodeGeeX/codegeex-vscode-extension), please follow [QuickStart](https://github.com/CodeGeeX/codegeex-vscode-extension/blob/main/doc/quickstart.md) to start development.\n\nFor Jetbrains IDEs, search \"codegeex\" in Plugins or install it [here](https://plugins.jetbrains.com/plugin/20587-codegeex). \nMake sure your IDE version is 2021.1 or later. CodeGeeX now supports IntelliJ IDEA, PyCharm, GoLand, CLion, Android Studio, AppCode, Aqua, DataSpell, DataGrip, Rider, RubyMine, and WebStorm. \n\n## CodeGeeX: Architecture, Code Corpus, and Implementation\n\n**Architecture**: CodeGeeX is a large-scale pre-trained programming language model based on transformers. It is a left-to-right autoregressive decoder, which takes code and natural language as input and predicts the probability of the next token. CodeGeeX contains 40 transformer layers with a hidden size of 5,120 for self-attention blocks and 20,480 for feed-forward layers, making its size reach 13 billion parameters. It supports a maximum sequence length of 2,048.\n\n<img src=\"resources/en/codegeex_training.png\">\n<p align=\"center\"><i><b>Left:</b> the proportion of programming languages in CodeGeeX's training data. \n  <b>Right:</b> the plot of training loss against the training steps of CodeGeeX.</i></p>\n\n**Code Corpus**: Our training data contains two parts. The first part is from open-sourced code datasets, [The Pile](https://pile.eleuther.ai/) and [CodeParrot](https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot). The Pile contains a subset of code corpus that collects public repositories with more than 100 stars from GitHub, from which we select codes in 23 popular programming languages. The second part is supplementary data directly scrapped from the public GitHub repositories that do not appear in previous datasets, including Python, Java and C++. To obtain data of potentially higher quality, repositories with at least one star and its size smaller than 10MB are chosen. A file is filtered out if it 1) has more than 100 characters per line on average, 2) is automatically generated, 3) has a ratio of alphabet less than 40%, or 4) is bigger than 100KB or smaller than 1KB. To help the model distinguish different languages, we add a language-specific prefix at the beginning of each segment in the form of ``[Comment sign] language: [LANG]``, e.g., ``# language: Python``. For tokenization, we use the same tokenizer as GPT-2 and process whitespaces as extra tokens, resulting in a vocabulary of 50,400 tokens. In total, the code corpus has 23 programming languages with 158.7B tokens.\n\n**Training**: We implement CodeGeeX in [Mindspore 1.7](https://www.mindspore.cn/) and train it on 1,536 Ascend 910 AI Processor (32GB). The model weights are under FP16 format, except that we use FP32 for layer-norm and softmax for higher precision and stability. The entire model consumes about 27GB of memory. To increase the training efficiency, we adopt an 8-way model parallel training together with 192-way data parallel training, with ZeRO-2 optimizer enabled. The micro-batch size is 16 and the global batch size reaches 3,072. Moreover, we adopt techniques to further boost the training efficiency including the element-wise operator fusion, fast gelu activation, matrix multiplication dimension optimization, etc. The entire training process takes nearly two months, spanning from April 18 to June 22, 2022, during which 850B tokens were passed for training, i.e., 5+ epochs.\n\n## HumanEval-X: A new benchmark for Multilingual Program Synthesis\nTo better evaluate the multilingual ability of code generation models, we propose a new benchmark HumanEval-X. While previous works evaluate multilingual program synthesis under semantic similarity (e.g., [CodeBLEU](https://arxiv.org/abs/2009.10297)) which is often misleading, HumanEval-X evaluates the functional correctness of the generated programs. HumanEval-X consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks.\n\n<img src=\"resources/en/hx_tasks.png\">\n\n<p align=\"center\"><i>An illustration of tasks supported by <b>HumanEval-X</b>. Declarations, docstrings, and solutions are marked with red, green, and blue respectively. <b>Code generation</b> uses declaration and docstring as input, to generate solution. <b>Code translation</b> uses declaration in both languages and translate the solution in source language to the one in target language.</i></p>\n\nIn HumanEval-X, every sample in each language contains declaration, docstring, and solution, which can be combined in various ways to support different downstream tasks including generation, translation, summarization, etc. We currently focus on two tasks: **code generation** and **code translation**. For code generation, the model uses declaration and docstring as input to generate the solution. For code translation, the model uses declarations in both languages and the solution in the source language as input, to generate solutions in the target language. We remove the description during code translation to prevent the model from directly solving the problem. For both tasks, we use the unbiased pass@k metric proposed in [Codex](https://arxiv.org/abs/2107.03374): $\\text{pass}@k:= \\mathbb{E}[1-\\frac{\\tbinom{n-c}{k}}{\\tbinom{n}{k}}]$, with $n=200$ and $k\\in(1,10,100)$.\n\n### Multilingual Code Generation\n\n<img src=\"resources/en/hx_generattion_radar_horizon.png\">\n<p align=\"center\"><i><b>Left</b>: the detailed pass@k (k=1,10,100) performance on code generation task for five languages in HumanEval-X. <b>Right</b>: the average performance of all languages of each model. CodeGeeX achieves the highest average performance compared with InCoder-6.7B, CodeGen-Multi-6B and CodeGen-Multi-16B.</i></p>\n\n\nWe compare CodeGeeX with two other open-sourced code generation models, [InCoder](https://github.com/dpfried/incoder) (from Meta) and [CodeGen](https://github.com/salesforce/CodeGen) (from Salesforce). Specifically, InCoder-6.7B, CodeGen-Multi-6B and CodeGen-Multi-16B are considered. CodeGeeX significantly outperforms models with smaller scales (by 7.5%~16.3%) and is competitive with CodeGen-Multi-16B with a larger scale (average performance 54.76% vs. 54.39%). CodeGeeX achieves the best average performance across languages.\n\n### Crosslingual Code Translation\n\n<img src=\"resources/en/hx_translation.png\">\n\n<p align=\"center\"><i>Results on HumanEval-X <b>code translation</b> task. Best language-wise performance are <b>bolded</b>.</i></p>\n\nWe also evaluate the performance of translation across different programming languages. We test the zero-shot performance of CodeGeeX, as well as the fine-tuned CodeGeeX-13B-FT (fine-tuned using the training set of code translation tasks in [XLCoST](https://github.com/reddy-lab-code-research/XLCoST); Go is absent in the original set, we thus add a small set to it). The results indicate that models have a preference for languages, e.g., CodeGeeX is good at translating other languages to Python and C++, while CodeGen-Multi-16B is better at translating to JavaScript and Go; these could probably be due to the difference in language distribution in the training corpus. Among 20 translation pairs, we also observe that the performance of A-to-B and B-to-A are always negatively correlated, which might indicate that the current models are still not capable of learning all languages well. \n\n### How to use HumanEval-X and contribute to it?\n\nFor more details on how to use HumanEval-X, please see [usage](codegeex/benchmark/README.md). We highly welcome the community to contribute to HumanEval-X by adding more problems or extending it to other languages, please check out the [standard format](codegeex/benchmark/README.md#how-to-use-humaneval-x) of HumanEval-X and add a pull request. \n\nPlease kindly let us know if you have any comment or suggestion, via [codegeex@aminer.cn](mailto:codegeex@aminer.cn).\n\n<details>\n<summary><b>Examples of Generation</b></summary>\n<img src=\"resources/en/hx_examples.png\">\n</details>\n\n<details>\n<summary><b>Acknowledgement</b></summary>\n<br/>\nThis project is supported by the National Science Foundation for Distinguished Young Scholars (No. 61825602). \n\n### Lead Contributors\n\nQinkai Zheng ([Tsinghua KEG](http://keg.cs.tsinghua.edu.cn/glm-130b/)), Xiao Xia (Tsinghua KEG), Xu Zou (Tsinghua KEG)\n\n### Contributors\n\nTsinghua KEG---The Knowledge Engineering Group at Tsinghua: Aohan Zeng, Wendi Zheng, Lilong Xue\n\nZhilin Yang's Group at Tsinghua IIIS: Yifeng Liu, Yanru Chen,  Yichen Xu (BUPT, work was done when visiting Tsinghua)\n\nPeng Cheng Laboratory: Qingyu Chen, Zhongqi Li, Gaojun Fan\n\nZhipu\\.AI: Yufei Xue, Shan Wang, Jiecai Shan, Haohan Jiang, Lu Liu, Xuan Xue, Peng Zhang\n\nAscend and Mindspore Team: Yifan Yao, Teng Su, Qihui Deng, Bin Zhou\n\n### Data Annotations\n\nRuijie Cheng (Tsinghua), Peinan Yu (Tsinghua), Jingyao Zhang (Zhipu\\.AI), Bowen Huang (Zhipu\\.AI), Shaoyu Wang (Zhipu\\.AI) \n    \n### Advisors\n\n[Zhilin Yang](https://kimiyoung.github.io/) (Tsinghua IIIS), Yuxiao Dong (Tsinghua KEG), Wenguang Chen (Tsinghua PACMAN), Jie Tang (Tsinghua KEG)\n    \n\n### Computation Sponsors\n\n[Peng Cheng Laboratory](https://www.pcl.ac.cn/index.html)\n\n[Zhipu.AI](https://www.zhipu.ai/)---an AI startup that aims to teach machines to think like humans\n\n### Project Leader \n\n[Jie Tang](http://keg.cs.tsinghua.edu.cn/jietang/) (Tsinghua KEG & BAAI)\n</details>\n\n## License\n\nOur code is licensed under the [Apache-2.0 license](LICENSE).\nOur model is licensed under the [license](MODEL_LICENSE).\n\n## Citation\n\nIf you find our work useful, please cite:\n\n```\n@inproceedings{zheng2023codegeex,\n  title={CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X},\n  author={Qinkai Zheng and Xiao Xia and Xu Zou and Yuxiao Dong and Shan Wang and Yufei Xue and Zihan Wang and Lei Shen and Andi Wang and Yang Li and Teng Su and Zhilin Yang and Jie Tang},\n  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},\n  pages={5673--5684},\n  year={2023}\n}\n```\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 18.09765625,
          "content": "<img src=\"resources/logo/codegeex_logo.png\">\n\n<p align=\"center\">\n    ğŸ  <a href=\"https://models.aminer.cn/codegeex/zh-CN\" target=\"_blank\">ä¸»é¡µ</a> | ğŸ“– <a href=\"https://models.aminer.cn/codegeex/blog/index_zh.html\" target=\"_blank\">åšå®¢</a> | ğŸª§ <a href=\"https://models.aminer.cn/codegeex/zh-CN/playground\" target=\"_blank\">ç¤ºä¾‹</a> | ğŸ¤– <a href=\"https://models.aminer.cn/codegeex/download/request\" target=\"_blank\">æ¨¡å‹ä¸‹è½½</a> | ğŸ“„ <a href=\"https://arxiv.org/abs/2303.17568\" target=\"_blank\">è®ºæ–‡</a> | ğŸŒ <a href=\"https://github.com/THUDM/CodeGeeX/blob/main/README.md\" target=\"_blank\">English</a>\n</p>\n<p align=\"center\">\n    ğŸ›  <a href=\"https://marketplace.visualstudio.com/items?itemName=aminer.codegeex\" target=\"_blank\">VS Code</a>, <a href=\"https://plugins.jetbrains.com/plugin/20587-codegeex\" target=\"_blank\">Jetbrains</a>, <a href=\"https://plugins.jetbrains.com/plugin/20587-codegeex\" target=\"_blank\">Cloud Studio</a> æ’ä»¶ | ğŸ‘‹ æ¬¢è¿åŠ å…¥ <a href=\"resources/zh/wechat.md\"target=\"_blank\">å¾®ä¿¡å¼€å‘è€…äº¤æµç¾¤</a> \n</p>\n\nğŸŒŸ [CodeGeeX2](https://github.com/THUDM/CodeGeeX2) å·²æ¨å‡ºï¼Œæ›´å¼ºï¼Œæ›´å¿«ï¼Œæ›´è½»é‡ã€‚\n\n- [CodeGeeX: å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹](#codegeex-å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹)\n  - [æ–°é—»](#æ–°é—»)\n  - [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)\n    - [å®‰è£…](#å®‰è£…)\n    - [æ¨¡å‹æƒé‡](#æ¨¡å‹æƒé‡)\n    - [ç”¨GPUè¿›è¡Œæ¨ç†](#ç”¨gpuè¿›è¡Œæ¨ç†)\n    - [æ’ä»¶ä½¿ç”¨æŒ‡å—](#æ’ä»¶ä½¿ç”¨æŒ‡å—)\n  - [CodeGeeX: å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹](#codegeex-å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹-1)\n    - [å›½äº§å¹³å°å®ç°ä¸è®­ç»ƒ](#å›½äº§å¹³å°å®ç°ä¸è®­ç»ƒ)\n  - [HumanEval-X: å¤šè¯­è¨€ä»£ç ç”ŸæˆåŸºå‡†](#humaneval-x-å¤šè¯­è¨€ä»£ç ç”ŸæˆåŸºå‡†)\n    - [å¤šè¯­è¨€ä»£ç ç”Ÿæˆ](#å¤šè¯­è¨€ä»£ç ç”Ÿæˆ)\n    - [è·¨è¯­è¨€ä»£ç ç¿»è¯‘](#è·¨è¯­è¨€ä»£ç ç¿»è¯‘)\n  - [è®¸å¯è¯](#è®¸å¯è¯)\n  - [å¼•ç”¨](#å¼•ç”¨)\n# CodeGeeX: å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹\n\nCodeGeeXæ˜¯ä¸€ä¸ªå…·æœ‰130äº¿å‚æ•°çš„å¤šç¼–ç¨‹è¯­è¨€ä»£ç ç”Ÿæˆé¢„è®­ç»ƒæ¨¡å‹ã€‚CodeGeeXé‡‡ç”¨åä¸ºMindSporeæ¡†æ¶å®ç°ï¼Œåœ¨é¹åŸå®éªŒå®¤â€œé¹åŸäº‘è„‘IIâ€ä¸­çš„192ä¸ªèŠ‚ç‚¹ï¼ˆå…±1536ä¸ªå›½äº§[æ˜‡è…¾910 AIå¤„ç†å™¨](https://e.huawei.com/cn/products/servers/ascend)ï¼‰ä¸Šè®­ç»ƒè€Œæˆã€‚æˆªè‡³2022å¹´6æœˆ22æ—¥ï¼ŒCodeGeeXå†æ—¶ä¸¤ä¸ªæœˆåœ¨20å¤šç§ç¼–ç¨‹è¯­è¨€çš„ä»£ç è¯­æ–™åº“ï¼ˆ>8500äº¿Tokenï¼‰ä¸Šé¢„è®­ç»ƒå¾—åˆ°ã€‚CodeGeeXæœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n* **é«˜ç²¾åº¦ä»£ç ç”Ÿæˆ**ï¼šæ”¯æŒç”ŸæˆPythonã€C++ã€Javaã€JavaScriptå’ŒGoç­‰å¤šç§ä¸»æµç¼–ç¨‹è¯­è¨€çš„ä»£ç ï¼Œåœ¨HumanEval-Xä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šå–å¾—47%~60%æ±‚è§£ç‡ï¼Œè¾ƒå…¶ä»–å¼€æºåŸºçº¿æ¨¡å‹æœ‰æ›´ä½³çš„å¹³å‡æ€§èƒ½ã€‚[ä»£ç ç”Ÿæˆç¤ºä¾‹](https://models.aminer.cn/codegeex/zh-CN)\n* **è·¨è¯­è¨€ä»£ç ç¿»è¯‘**ï¼šæ”¯æŒä»£ç ç‰‡æ®µåœ¨ä¸åŒç¼–ç¨‹è¯­è¨€é—´è¿›è¡Œè‡ªåŠ¨ç¿»è¯‘è½¬æ¢ï¼Œç¿»è¯‘ç»“æœæ­£ç¡®ç‡é«˜ï¼Œåœ¨HumanEval-Xä»£ç ç¿»è¯‘ä»»åŠ¡ä¸Šè¶…è¶Šäº†å…¶å®ƒåŸºçº¿æ¨¡å‹ã€‚[ä»£ç ç¿»è¯‘ç¤ºä¾‹](https://models.aminer.cn/codegeex/zh-CN/codeTranslator)\n* **è‡ªåŠ¨ç¼–ç¨‹æ’ä»¶**ï¼šCodeGeeXæ’ä»¶ç°å·²ä¸Šæ¶VSCodeæ’ä»¶å¸‚åœºï¼ˆå®Œå…¨å…è´¹ï¼‰ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡å…¶å¼ºå¤§çš„å°‘æ ·æœ¬ç”Ÿæˆèƒ½åŠ›ï¼Œè‡ªå®šä¹‰ä»£ç ç”Ÿæˆé£æ ¼å’Œèƒ½åŠ›ï¼Œæ›´å¥½è¾…åŠ©ä»£ç ç¼–å†™ã€‚[æ’ä»¶ä¸‹è½½](https://marketplace.visualstudio.com/items?itemName=aminer.codegeex)\n* **æ¨¡å‹è·¨å¹³å°å¼€æº**: æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æƒé‡å¼€æºå¼€æ”¾ï¼Œç”¨ä½œç ”ç©¶ç”¨é€”ã€‚CodeGeeXåŒæ—¶æ”¯æŒæ˜‡è…¾å’Œè‹±ä¼Ÿè¾¾å¹³å°ï¼Œå¯åœ¨å•å¼ æ˜‡è…¾910æˆ–è‹±ä¼Ÿè¾¾V100/A100ä¸Šå®ç°æ¨ç†ã€‚[ç”³è¯·æ¨¡å‹æƒé‡](https://models.aminer.cn/codegeex/download/request)\n\n**å…¨æ–°å¤šç¼–ç¨‹è¯­è¨€è¯„æµ‹åŸºå‡†HumanEval-X**ï¼šHumanEval-Xæ˜¯ç¬¬ä¸€ä¸ªæ”¯æŒåŠŸèƒ½æ­£ç¡®æ€§è¯„æµ‹çš„å¤šè¯­è¨€ã€å¤šä»»åŠ¡çš„åŸºå‡†ï¼ŒåŒ…å«820ä¸ªäººå·¥ç¼–å†™çš„é«˜è´¨é‡ä»£ç ç”Ÿæˆé¢˜ç›®ã€æµ‹è¯•ç”¨ä¾‹ä¸å‚è€ƒç­”æ¡ˆï¼Œè¦†ç›–5ç§ç¼–ç¨‹è¯­è¨€ï¼ˆPythonã€C++ã€Javaã€JavaScriptã€Goï¼‰ï¼Œæ”¯æŒä»£ç ç”Ÿæˆä¸ä»£ç ç¿»è¯‘èƒ½åŠ›çš„è¯„æµ‹ã€‚[å¦‚ä½•ä½¿ç”¨](codegeex/benchmark/README_zh.md)\n\n<img src=\"resources/zh/hx_boxplot_zh.png\">\n\n<p align=\"center\"><i>åœ¨HumanEval-Xä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œä¸å…¶å®ƒå¼€æºåŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼ŒCodeGeeXå–å¾—äº†æœ€ä½³çš„å¹³å‡æ€§èƒ½ã€‚</i> </p>\n\n## æ–°é—»\n\n* ğŸŒŸ **2023-07-24**: [CodeGeeX2](https://github.com/THUDM/CodeGeeX2) å·²æ¨å‡ºï¼Œæ›´å¼ºï¼Œæ›´å¿«ï¼Œæ›´è½»é‡ã€‚æ”¯æŒè¶…è¿‡100ç§è¯­è¨€ï¼Œå¹¶å…·æœ‰å¤šç§æ–°ç‰¹æ€§ã€‚\n\n* **2023-5-16**: CodeGeeX è®ºæ–‡å·²è¢« [KDD 2023, Long Beach](https://kdd.org/kdd2023/) æ¥æ”¶ï¼Œå¹¶å°†åœ¨ä¼šä¸Šåšç›¸å…³æŠ¥å‘Šã€‚\n\n* **2023-03-30**: CodeGeeX è®ºæ–‡å·²å‘è¡¨åœ¨[arxiv](https://arxiv.org/abs/2303.17568)ã€‚\n\n* **2023-02-14**: CodeGeeX ç°å·²æ”¯æŒ [Cloud Studio](https://cloudstudio.net/), ä¸€æ¬¾è…¾è®¯æ¨å‡ºã€ååˆ†å¥½ç”¨çš„åœ¨çº¿ç¼–è¾‘å™¨ã€‚å•å‡»æ­¤é¡µé¢é¡¶éƒ¨çš„å¾½ç« å¯å¿«é€Ÿå¯åŠ¨ç¯å¢ƒæµ‹è¯• CodeGeeXã€‚\n\n* **2023-02-13**: æ„Ÿè°¢ [OneFlow](https://github.com/Oneflow-Inc/oneflow) åŠ å…¥äº†oneflowç‰ˆæ¨ç†æ”¯æŒï¼Œåœ¨FP16ä¸‹æ¯”FasterTransformerè¿˜è¦å¿«ï¼æ›´å¤šä¼˜åŒ–ç»†èŠ‚è¯·ç‚¹å‡»[è¿™é‡Œ](https://github.com/THUDM/CodeGeeX/pull/65).\n\n* **2023-02**: [CodeGeeX \"Coding With AI\"é»‘å®¢æ¾](https://dorahacks.io/hackathon/codegeex/)æ­£åœ¨è¿›è¡Œä¸­ï¼Œä¸ºCodeGeeXè®¾è®¡åº”ç”¨å¹¶èµ¢å–å¥–å“ï¼ˆRTX 4090ã€DJIæ— äººæœºç­‰ï¼‰ï¼\n\n* **2022-12-31**: æˆ‘ä»¬åœ¨ [codegeex-fastertransformer](https://github.com/CodeGeeX/codegeex-fastertransformer) ä¸­å‘å¸ƒäº† CodeGeeX çš„ FasterTransformer ç‰ˆæœ¬ã€‚INT8åŠ é€Ÿç‰ˆæœ¬è¾¾åˆ° <15ms/token çš„å¹³å‡é€Ÿåº¦ã€‚ç¥å¤§å®¶æ–°å¹´å¿«ä¹ï¼\n\n* **2022-12-13**: æˆ‘ä»¬å¼€æºäº†VS Codeæ’ä»¶æºç ï¼š[codegeex-vscode-extension](https://github.com/CodeGeeX/codegeex-vscode-extension)ï¼Œå‚è€ƒ [QuickStart](https://github.com/CodeGeeX/codegeex-vscode-extension/blob/main/doc/quickstart.md) å¼€å§‹å¼€å‘å§ï¼\n\n* **2022-12-11**: CodeGeeX for Jetbrains IDEså·²ä¸Šçº¿ï¼Œæ”¯æŒIntelliJ IDEA, PyCharm, GoLand, CLionç­‰ï¼Œ[ç‚¹å‡»ä¸‹è½½](https://plugins.jetbrains.com/plugin/20587-codegeex)ã€‚\n  \n* **2022-12-04**: æˆ‘ä»¬å¼€æºäº†é‡åŒ–ä»£ç ï¼ˆéœ€è¦æ›´å°‘çš„æ˜¾å­˜ï¼š27GB -> 15GBï¼‰ä»¥åŠæ¨¡å‹å¹¶è¡Œä»£ç ï¼ˆå¯ä»¥è¿è¡Œåœ¨å¤šä¸ªæ˜¾å­˜è‡³å°‘8GBçš„GPUsä¸Šï¼‰ã€‚\n\n* **2022-09-30**: æˆ‘ä»¬å¼€æºäº†è·¨å¹³å°ä»£ç å’Œæ¨¡å‹æƒé‡ï¼ŒåŒæ—¶æ”¯æŒæ˜‡è…¾å’Œè‹±ä¼Ÿè¾¾å¹³å°ã€‚\n## ä½¿ç”¨æŒ‡å—\n\nCodeGeeXæœ€åˆä½¿ç”¨Mindsporeæ¡†æ¶å®ç°ï¼Œå¹¶åœ¨æ˜‡è…¾910AIèŠ¯ç‰‡ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä¸ºé€‚é…æ›´å¤šå¹³å°ï¼Œæˆ‘ä»¬å°†å…¶è½¬æ¢åˆ°[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)æ¡†æ¶ï¼Œæ”¯æŒPytorch+GPUç¯å¢ƒã€‚\n### å®‰è£…\n\néœ€è¦Python 3.7+ / CUDA 11+ / PyTorch 1.10+ / DeepSpeed 0.6+ï¼Œé€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£… ``codegeex``: \n```bash\ngit clone git@github.com:THUDM/CodeGeeX.git\ncd CodeGeeX\npip install -e .\n```\n\n### æ¨¡å‹æƒé‡\n\né€šè¿‡[è¯¥é“¾æ¥](https://models.aminer.cn/codegeex/download/request)ç”³è¯·æƒé‡ï¼Œæ‚¨å°†æ”¶åˆ°ä¸€ä¸ªåŒ…å«ä¸´æ—¶ä¸‹è½½é“¾æ¥æ–‡ä»¶```urls.txt```çš„é‚®ä»¶ã€‚æ¨èä½¿ç”¨[aria2](https://aria2.github.io/)é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¿«é€Ÿä¸‹è½½ï¼ˆè¯·ä¿è¯æœ‰è¶³å¤Ÿçš„ç¡¬ç›˜ç©ºé—´å­˜æ”¾æƒé‡ï¼ˆï½26GBï¼‰ï¼‰ï¼š\n```bash\naria2c -x 16 -s 16 -j 4 --continue=true -i urls.txt \n``` \nä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆå¹¶å¾—åˆ°å®Œæ•´çš„æƒé‡ï¼š\n```bash\ncat codegeex_13b.tar.gz.* > codegeex_13b.tar.gz\ntar xvf codegeex_13b.tar.gz\n```\n\n### ç”¨GPUè¿›è¡Œæ¨ç†\n\nå°è¯•ä½¿ç”¨CodeGeeXæ¨¡å‹ç”Ÿæˆç¬¬ä¸€ä¸ªç¨‹åºå§ï¼é¦–å…ˆï¼Œåœ¨é…ç½®æ–‡ä»¶``configs/codegeex_13b.sh``ä¸­å†™æ˜å­˜æ”¾æƒé‡çš„è·¯å¾„ã€‚å…¶æ¬¡ï¼Œå°†æç¤ºï¼ˆå¯ä»¥æ˜¯ä»»æ„æè¿°æˆ–ä»£ç ç‰‡æ®µï¼‰å†™å…¥æ–‡ä»¶``tests/test_prompt.txt``ï¼Œè¿è¡Œä»¥ä¸‹è„šæœ¬å³å¯å¼€å§‹æ¨ç†ï¼ˆéœ€æŒ‡å®šGPUåºå·ï¼‰ï¼š\n```bash\n# On a single GPU (with more than 27GB RAM)\nbash ./scripts/test_inference.sh <GPU_ID> ./tests/test_prompt.txt\n\n# With quantization (with more than 15GB RAM)\nbash ./scripts/test_inference_quantized.sh <GPU_ID> ./tests/test_prompt.txt\n\n# On multiple GPUs (with more than 6GB RAM, need to first convert ckpt to MP_SIZE partitions)\nbash ./scripts/convert_ckpt_parallel.sh <LOAD_CKPT_PATH> <SAVE_CKPT_PATH> <MP_SIZE>\nbash ./scripts/test_inference_parallel.sh <MP_SIZE> ./tests/test_prompt.txt\n```\n\n### æ’ä»¶ä½¿ç”¨æŒ‡å—\n\nåŸºäºCodeGeeXï¼Œæˆ‘ä»¬å¼€å‘äº†å…è´¹çš„æ’ä»¶ï¼Œæ”¯æŒ VS Code ä¸ Jetbrains IDEsï¼Œæœªæ¥ä¼šæ”¯æŒæ›´å¤šå¹³å°ã€‚\n\nVS Codeç‰ˆæœ¬ï¼Œåœ¨åº”ç”¨å¸‚åœºæœç´¢â€œcodegeexâ€æˆ–é€šè¿‡[è¯¥é“¾æ¥](https://marketplace.visualstudio.com/items?itemName=aminer.codegeex)å®‰è£…ã€‚è¯¦ç»†çš„ä½¿ç”¨æŒ‡å—åœ¨[CodeGeeX VS Codeæ’ä»¶ä½¿ç”¨æŒ‡å—](vscode-extension/README_zh.md)ã€‚æˆ‘ä»¬ä¹Ÿå¼€æºäº†VS Codeæ’ä»¶æºç ï¼š[codegeex-vscode-extension](https://github.com/CodeGeeX/codegeex-vscode-extension)ï¼Œå‚è€ƒ[QuickStart](https://github.com/CodeGeeX/codegeex-vscode-extension/blob/main/doc/quickstart_zh.md) å¼€å§‹å¼€å‘å§ï¼\n\nJetbrainsç‰ˆæœ¬ï¼Œåœ¨Pluginså¸‚åœºæœç´¢â€œcodegeexâ€æˆ–é€šè¿‡[è¯¥é“¾æ¥](https://plugins.jetbrains.com/plugin/20587-codegeex)å®‰è£…ã€‚\nè¯·ç¡®ä¿IDEç‰ˆæœ¬åœ¨2021.1æˆ–æ›´é«˜ã€‚CodeGeeXç›®å‰æ”¯æŒ IntelliJ IDEA, PyCharm, GoLand, CLion, Android Studio, AppCode, Aqua, DataSpell, DataGrip, Rider, RubyMine, WebStormã€‚ \n\n## CodeGeeX: å¤šè¯­è¨€ä»£ç ç”Ÿæˆæ¨¡å‹\n\n**æ¶æ„**ï¼šCodeGeeXæ˜¯ä¸€ä¸ªåŸºäºtransformersçš„å¤§è§„æ¨¡é¢„è®­ç»ƒç¼–ç¨‹è¯­è¨€æ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ä¸ªä»å·¦åˆ°å³ç”Ÿæˆçš„è‡ªå›å½’è§£ç å™¨ï¼Œå°†ä»£ç æˆ–è‡ªç„¶è¯­è¨€æ ‡è¯†ç¬¦ï¼ˆtokenï¼‰ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è¯†ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒã€‚CodeGeeXå«æœ‰40ä¸ªtransformerå±‚ï¼Œæ¯å±‚è‡ªæ³¨æ„åŠ›å—çš„éšè—å±‚ç»´æ•°ä¸º5120ï¼Œå‰é¦ˆå±‚ç»´æ•°ä¸º20480ï¼Œæ€»å‚æ•°é‡ä¸º130äº¿ã€‚æ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦ä¸º2048ã€‚\n\n<img src=\"resources/en/codegeex_training.png\">\n\n<p align=\"center\"><i><b>å·¦ä¾§ï¼š</b>CodeGeeXè®­ç»ƒæ•°æ®ä¸­å„ç¼–ç¨‹è¯­è¨€å æ¯”ã€‚\n<b>å³ä¾§ï¼š</b>CodeGeeXè®­ç»ƒæŸå¤±å‡½æ•°éšè®­ç»ƒæ­¥æ•°ä¸‹é™æ›²çº¿ã€‚</i></p>\n\n**è¯­æ–™**ï¼šCodeGeeXçš„è®­ç»ƒè¯­æ–™ç”±ä¸¤éƒ¨åˆ†ç»„æˆã€‚ç¬¬ä¸€éƒ¨åˆ†æ˜¯å¼€æºä»£ç æ•°æ®é›†ï¼Œ[The Pile](https://pile.eleuther.ai/) ä¸ [CodeParrot](https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot)ã€‚The PileåŒ…å«GitHubä¸Šæ‹¥æœ‰è¶…è¿‡100é¢—æ˜Ÿçš„ä¸€éƒ¨åˆ†å¼€æºä»“åº“ï¼Œæˆ‘ä»¬ä»ä¸­é€‰å–äº†23ç§ç¼–ç¨‹è¯­è¨€çš„ä»£ç ã€‚ç¬¬äºŒéƒ¨åˆ†æ˜¯è¡¥å……æ•°æ®ï¼Œç›´æ¥ä»GitHubå¼€æºä»“åº“ä¸­çˆ¬å–Pythonã€Javaã€C++ä»£ç ï¼›ä¸ºäº†è·å–é«˜è´¨é‡æ•°æ®ï¼Œæˆ‘ä»¬æ ¹æ®ä»¥ä¸‹å‡†åˆ™é€‰å–ä»£ç ä»“åº“ï¼š1)è‡³å°‘æ‹¥æœ‰1é¢—æ˜Ÿï¼›2)æ€»å¤§å°<10MBï¼›3)ä¸åœ¨æ­¤å‰çš„å¼€æºä»£ç æ•°æ®é›†ä¸­ã€‚æˆ‘ä»¬è¿˜å»æ‰äº†ç¬¦åˆä¸‹åˆ—ä»»ä¸€æ¡ä»¶çš„æ–‡ä»¶ï¼š1)å¹³å‡æ¯è¡Œé•¿åº¦å¤§äº100å­—ç¬¦ï¼›2)ç”±è‡ªåŠ¨ç”Ÿæˆå¾—åˆ°ï¼›3)å«æœ‰çš„å­—æ¯ä¸è¶³å­—æ¯è¡¨å†…çš„40%ï¼›4)å¤§äº100KBæˆ–å°äº1KBã€‚ä¸ºäº†è®©æ¨¡å‹åŒºåˆ†ä¸åŒè¯­è¨€ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªæ ·æœ¬çš„å¼€å¤´åŠ ä¸Šä¸€ä¸ªå‰ç¼€ï¼Œå…¶å½¢å¼ä¸º``[æ³¨é‡Šç¬¦] language: [è¯­è¨€]``ï¼Œä¾‹å¦‚ï¼š``# language: Python``ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸GPT-2ç›¸åŒçš„åˆ†è¯å™¨ï¼Œå¹¶å°†ç©ºæ ¼å¤„ç†ä¸ºç‰¹æ®Šæ ‡è¯†ç¬¦ï¼Œè¯è¡¨å¤§å°ä¸º50400ã€‚æ•´ä¸ªä»£ç è¯­æ–™å«æœ‰23ç§ç¼–ç¨‹è¯­è¨€ã€æ€»è®¡1587äº¿ä¸ªæ ‡è¯†ç¬¦ï¼ˆä¸å«å¡«å……ç¬¦ï¼‰ã€‚\n\n### å›½äº§å¹³å°å®ç°ä¸è®­ç»ƒ\næˆ‘ä»¬åœ¨[Mindspore 1.7](https://www.mindspore.cn/)æ¡†æ¶ä¸Šå®ç°äº†CodeGeeXæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨é¹åŸå®éªŒå®¤çš„å…¨å›½äº§è®¡ç®—å¹³å°ä¸Šè¿›è¡Œè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼ŒCodeGeeXä½¿ç”¨äº†å…¶ä¸€ä¸ªè®¡ç®—é›†ç¾¤ä¸­çš„1536ä¸ªæ˜‡è…¾910 AIå¤„ç†å™¨ï¼ˆ32GBï¼‰è¿›è¡Œäº†ä¸¤ä¸ªæœˆå·¦å³çš„è®­ç»ƒï¼ˆ2022å¹´4æœˆ18æ—¥è‡³6æœˆ22æ—¥ï¼‰ã€‚é™¤äº†Layer-normä¸Softmaxä½¿ç”¨FP32æ ¼å¼ä»¥è·å¾—æ›´é«˜çš„ç²¾åº¦ä¸ç¨³å®šæ€§ï¼Œæ¨¡å‹å‚æ•°æ•´ä½“ä½¿ç”¨FP16æ ¼å¼ï¼Œæœ€ç»ˆæ•´ä¸ªæ¨¡å‹éœ€è¦å ç”¨çº¦27GBæ˜¾å­˜ã€‚ä¸ºäº†å¢åŠ è®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬ä½¿ç”¨8è·¯æ¨¡å‹å¹¶è¡Œå’Œ192è·¯æ•°æ®å¹¶è¡Œçš„è®­ç»ƒç­–ç•¥ï¼Œå¾®æ‰¹å¤§å°ä¸º16ã€å…¨å±€æ‰¹å¤§å°ä¸º3072ï¼Œå¹¶é‡‡ç”¨ZeRO-2ä¼˜åŒ–å™¨é™ä½æ˜¾å­˜å ç”¨ã€‚\n\nåœ¨å¼€å‘ä¸è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å’Œåä¸ºMindsporeå›¢é˜Ÿåˆä½œï¼Œå¯¹MindSporeæ¡†æ¶è¿›è¡Œäº†éƒ¨åˆ†ä¼˜åŒ–ï¼Œè¿›è€Œå¤§å¹…åº¦æå‡è®­ç»ƒæ•ˆç‡ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬å‘ç°çŸ©é˜µä¹˜æ³•çš„è®¡ç®—æ—¶é—´å æ¯”ä»…ä¸º22.9%ï¼Œå¤§é‡æ—¶é—´è¢«ç”¨äºå„ç±»å…¶å®ƒç®—å­ï¼Œå› æ­¤å®ç°äº†ä¸€ç³»åˆ—ç®—å­èåˆï¼ŒåŒ…æ‹¬å•å…ƒç´ ç®—å­èåˆã€å±‚å½’ä¸€åŒ–ç®—å­èåˆã€FastGeluä¸çŸ©é˜µä¹˜æ³•èåˆã€æ‰¹é‡çŸ©é˜µä¹˜æ³•ä¸åŠ æ³•èåˆç­‰ï¼›å†æ¯”å¦‚æˆ‘ä»¬è¿˜å¯¹çŸ©é˜µä¹˜æ³•ç®—å­çš„ç»´åº¦å®ç°è‡ªåŠ¨æœç´¢è°ƒä¼˜ï¼Œä½¿å…¶æœç´¢å‡ºæ•ˆç‡æœ€é«˜çš„è®¡ç®—ç»´åº¦ç»„åˆã€‚è¿™äº›ä¼˜åŒ–ä¸ºè®­ç»ƒé€Ÿåº¦å¸¦æ¥äº†æ˜¾è‘—æå‡ï¼Œåœ¨åŒç­‰GPUå¡æ•°è§„æ¨¡ä¸‹ï¼ˆ128å¡ï¼‰ï¼Œæ˜‡è…¾910å¯¹CodeGeeXè¿™ä¸€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ä»çº¦ä¸ºNVIDIA A100çš„16.7%æå‡è‡³43%ï¼›åœ¨åƒå¡è§„æ¨¡ä¸‹ï¼Œæ˜‡è…¾910è®­ç»ƒæ•ˆç‡ç›¸æ¯”è‡ªèº«ä¼˜åŒ–å‰æå‡è¿‘300%ã€‚ä½¿ç”¨ä¼˜åŒ–åçš„è½¯ç¡¬ä»¶è®­ç»ƒæ—¶ï¼ŒCodeGeeXå•æ—¥è®­ç»ƒé‡å¯è¾¾åˆ°54.3Bä¸ªæ ‡è¯†ç¬¦ï¼ˆå«å¡«å……ç¬¦ï¼‰ï¼Œè¯æ˜äº†å›½äº§æ·±åº¦å­¦ä¹ å¹³å°ä¸å·¥å…·çš„å¿«é€Ÿè¿­ä»£èƒ½åŠ›ä»¥åŠå¼ºå¤§ç«äº‰åŠ›ã€‚\n\n## HumanEval-X: å¤šè¯­è¨€ä»£ç ç”ŸæˆåŸºå‡†\nä¸ºäº†æ›´å¥½åœ°è¯„æµ‹ä»£ç ç”Ÿæˆæ¨¡å‹çš„å¤šè¯­è¨€ç”Ÿæˆèƒ½åŠ›ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæ–°åŸºå‡†HumanEval-Xã€‚æ­¤å‰ï¼Œå¤šè¯­è¨€ä»£ç ç”Ÿæˆèƒ½åŠ›æ˜¯åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆæ¯”å¦‚[CodeBLEU](https://arxiv.org/abs/2009.10297)ï¼‰è¡¡é‡çš„ï¼Œå…·æœ‰ä¸€å®šè¯¯å¯¼æ€§ï¼›HumanEval-Xåˆ™å¯ç”¨äºè¡¡é‡ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚HumanEval-XåŒ…å«820ä¸ªé«˜è´¨é‡æ‰‹å†™æ ·æœ¬ï¼Œè¦†ç›–Pythonã€C++ã€Javaã€JavaScriptã€Goï¼Œå¯ç”¨äºå¤šç§ä»»åŠ¡ã€‚\n\n<img src=\"resources/zh/hx_tasks_zh.png\">\n\n<p align=\"center\"><i><b>HumanEval-X</b>æ”¯æŒçš„ä»»åŠ¡ç¤ºä¾‹ã€‚<font style='background-color:#F8CECC'>å£°æ˜</font>ã€<font style='background-color:#D5E8D4'>æè¿°</font>ã€<font style='background-color:#DAE8FC'>è§£ç­”</font>åˆ†åˆ«ç”¨çº¢ã€ç»¿ã€è“è‰²æ ‡æ³¨ã€‚<i>ä»£ç ç”Ÿæˆ</i>å°†å£°æ˜ä¸æè¿°ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºè§£ç­”ã€‚<i>ä»£ç ç¿»è¯‘</i>å°†ä¸¤ç§è¯­è¨€çš„å£°æ˜ä¸æºè¯­è¨€çš„è§£ç­”ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºç›®æ ‡è¯­è¨€çš„è§£ç­”ã€‚</i></p>\n\nHumanEval-Xä¸­æ¯ä¸ªè¯­è¨€çš„æ ·æœ¬ï¼ŒåŒ…å«äº†å£°æ˜ã€æè¿°å’Œè§£ç­”ï¼Œå®ƒä»¬ä¹‹é—´çš„ç»„åˆå¯ä»¥æ”¯æŒä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç”Ÿæˆã€ç¿»è¯‘ã€æ¦‚æ‹¬ç­‰ã€‚æˆ‘ä»¬ç›®å‰å…³æ³¨ä¸¤ä¸ªä»»åŠ¡ï¼š**ä»£ç ç”Ÿæˆ**ä¸**ä»£ç ç¿»è¯‘**ã€‚å¯¹äºä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œæ¨¡å‹å°†å‡½æ•°å£°æ˜ä¸æ–‡æ¡£å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºå‡½æ•°å®ç°ï¼›å¯¹äºä»£ç ç¿»è¯‘ä»»åŠ¡ï¼Œæ¨¡å‹å°†ä¸¤ç§è¯­è¨€çš„å‡½æ•°å£°æ˜ä¸æºè¯­è¨€çš„å®ç°ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºç›®æ ‡è¯­è¨€ä¸Šçš„å®ç°ã€‚æˆ‘ä»¬åœ¨ä»£ç ç¿»è¯‘ä»»åŠ¡ä¸­ä¸å°†æ–‡æ¡£å­—ç¬¦ä¸²è¾“å…¥æ¨¡å‹ï¼Œä»¥é¿å…æ¨¡å‹ç›´æ¥é€šè¿‡æè¿°ç”Ÿæˆç­”æ¡ˆã€‚åœ¨ä¸¤ç§ä»»åŠ¡ä¸‹ï¼Œæˆ‘ä»¬éƒ½é‡‡ç”¨[Codex](https://arxiv.org/abs/2107.03374)æ‰€ä½¿ç”¨çš„æ— åpass@kæŒ‡æ ‡ï¼Œåˆ¤æ–­ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§: $\\text{pass}@k:= \\mathbb{E}[1-\\frac{\\tbinom{n-c}{k}}{\\tbinom{n}{k}}]$, $n=200$, $k\\in(1,10,100)$.\n\n### å¤šè¯­è¨€ä»£ç ç”Ÿæˆ\n\n<img src=\"resources/zh/hx_generattion_radar_horizon_zh.png\">\n\n<p align=\"center\"><i><b>å·¦ä¾§</b>: HumanEval-Xä¸­äº”ç§è¯­è¨€å…·ä½“çš„pass@kï¼ˆk=1,10,100ï¼‰æ€§èƒ½ã€‚<b>å³ä¾§</b>: æ¨¡å‹åœ¨æ‰€æœ‰è¯­è¨€ä¸Šçš„å¹³å‡æ€§èƒ½ã€‚CodeGeeXçš„å¹³å‡è¡¨ç°ä¼˜äºInCoder-6.7Bå’ŒCodeGen-Multi-6B/16Bã€‚</i></p>\n\n\næˆ‘ä»¬å°†CodeGeeXä¸å¦å¤–ä¸¤ä¸ªå¼€æºä»£ç ç”Ÿæˆæ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œåˆ†åˆ«ä¸ºMetaçš„[InCoder](https://github.com/dpfried/incoder)ä¸Salesforceçš„[CodeGen](https://github.com/salesforce/CodeGen)ï¼Œé€‰å–InCoder-6.7Bã€CodeGen-Multi-6B ä¸ CodeGen-Multi-16Bã€‚CodeGeeXèƒ½è·å¾—æœ€ä½³çš„å¹³å‡æ€§èƒ½ï¼Œæ˜¾è‘—è¶…è¶Šäº†å‚æ•°é‡æ›´å°çš„æ¨¡å‹(7.5%~16.3%çš„æå‡)ï¼Œä¸å‚æ•°é‡æ›´å¤§çš„æ¨¡å‹CodeGen-Multi-16Bè¡¨ç°ç›¸å½“ï¼ˆå¹³å‡æ€§èƒ½ 54.76% vs. 54.39%ï¼‰ã€‚\n\n### è·¨è¯­è¨€ä»£ç ç¿»è¯‘\n\n<img src=\"resources/zh/hx_translation_zh.png\">\n\n<p align=\"center\"><i>HumanEval-Xä¸Šçš„<b>ä»£ç ç¿»è¯‘</b>ä»»åŠ¡ç»“æœã€‚<b>åŠ ç²—</b>ç»“æœè¡¨ç¤ºåœ¨æ¯ç§è¯­è¨€pass@kä¸Šçš„æœ€ä½³æ•ˆæœã€‚</i></p>\n\næˆ‘ä»¬è¿˜è¯„æµ‹äº†æ¨¡å‹åœ¨å¤šè¯­è¨€é—´ä»£ç ç¿»è¯‘ä¸Šçš„æ€§èƒ½ã€‚å¯¹äºCodeGeeXï¼Œæˆ‘ä»¬è¯„æµ‹äº†æœªç»å¾®è°ƒçš„CodeGeeX-13Bä¸ç»è¿‡å¾®è°ƒçš„CodeGeeX-13B-FTï¼ˆä½¿ç”¨[XLCoST](https://github.com/reddy-lab-code-research/XLCoST)ä¸­ä»£ç ç¿»è¯‘ä»»åŠ¡çš„è®­ç»ƒé›†ä¸ä¸€éƒ¨åˆ†Goè¯­è¨€æ•°æ®å¾®è°ƒï¼‰ã€‚å¦‚ä¸Šè¡¨æ˜¾ç¤ºï¼Œæ¨¡å‹å¯¹ç‰¹å®šè¯­è¨€å­˜åœ¨åå¥½ï¼Œæ¯”å¦‚CodeGeeXæ“…é•¿å°†å…¶ä»–è¯­è¨€ç¿»è¯‘ä¸ºPythonä¸C++ï¼Œè€ŒCodeGen-Multi-16Bæ“…é•¿ç¿»è¯‘ä¸ºJavaScriptå’ŒGoï¼Œè¿™å¯èƒ½æ˜¯ç”±äºè®­ç»ƒé›†ä¸­çš„è¯­æ–™å æ¯”å­˜åœ¨å·®å¼‚ã€‚åœ¨20ä¸ªç¿»è¯‘å¯¹ä¸­ï¼Œæˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ä¸¤ç§è¯­è¨€äº’ç›¸ç¿»è¯‘çš„è¡¨ç°å¸¸å¸¸æ˜¯å‘ˆè´Ÿç›¸å…³çš„ï¼Œè¿™å¯èƒ½è¯´æ˜ç°æœ‰çš„æ¨¡å‹è¿˜ä¸è¶³ä»¥å­¦å¥½æ‰€æœ‰çš„è¯­è¨€ã€‚\n\n\n\n<details> \n<summary><b>åœ¨çº¿ç”Ÿæˆä¸ç¿»è¯‘DEMO</b></summary>\n<img src=\"resources/en/hx_examples.png\">\næˆ‘ä»¬ä¸ºä¸Šè¿°ä¸¤ä¸ªä»»åŠ¡å¼€å‘äº†DEMOï¼š<a href=\"https://models.aminer.cn/codegeex/zh-CN/playground\" target=\"_blank\">ä»£ç ç”Ÿæˆ</a>å’Œ<a href=\"https://models.aminer.cn/codegeex/zh-CN/codeTranslator\" target=\"_blank\">ä»£ç ç¿»è¯‘</a>ï¼Œæ¬¢è¿ç‚¹å‡»ä½“éªŒï¼\n</details>\n\n<details>\n<summary><b>è‡´è°¢</b></summary>\n<br/>\nè¿™ä¸€é¡¹ç›®ç”±å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘æ°å‡ºé’å¹´ç§‘å­¦åŸºé‡‘é¡¹ç›®ï¼ˆNo. 61825602ï¼‰æ”¯æŒã€‚\nâ€‹    \n\n#### å­¦ç”Ÿè´Ÿè´£äºº\n\néƒ‘å‹¤é”´ï¼ˆ[æ¸…åå¤§å­¦çŸ¥è¯†å·¥ç¨‹å®éªŒå®¤](https://keg.cs.tsinghua.edu.cn/glm-130b/zh/posts/glm-130b/)ï¼‰ï¼Œå¤ç®«ï¼ˆæ¸…åå¤§å­¦çŸ¥è¯†å·¥ç¨‹å®éªŒå®¤ï¼‰ï¼Œé‚¹æ—­ï¼ˆæ¸…åå¤§å­¦çŸ¥è¯†å·¥ç¨‹å®éªŒå®¤ï¼‰\n    \n#### æŠ€æœ¯è´¡çŒ®\n\næ¸…åå¤§å­¦çŸ¥è¯†å·¥ç¨‹å®éªŒå®¤ï¼šæ›¾å¥¥æ¶µï¼Œéƒ‘é—®è¿ªï¼Œè–›ç†é¾™\n\næ¸…åå¤§å­¦äº¤å‰ä¿¡æ¯å­¦é™¢ï¼šåˆ˜ç›Šæ«ï¼Œé™ˆå½¦å„’ï¼Œå¾å¥•è¾°ï¼ˆåŒ—é‚®å¤§å››è®¿é—®æ¸…åæœŸé—´ç ”ç©¶å·¥ä½œï¼‰\n\né¹åŸå®éªŒå®¤ï¼šé™ˆåº†ç‰ï¼Œæå¿ ç¦ï¼ŒèŒƒé«˜ä¿Š\n\næ™ºè°±AIï¼šè–›å®‡é£ï¼Œç‹å±±ï¼Œé™•æ°æ‰ï¼Œå§œçš“ç€šï¼Œåˆ˜ç’ï¼Œè–›æ—‹ï¼Œå¼ é¹\n\nåä¸ºæ˜‡è…¾å›¢é˜Ÿï¼šå§šé€¸ç’ ï¼Œè‹è…¾ï¼Œé‚“å¯è¾‰ï¼Œå‘¨æ–Œ\n\n#### æ•°æ®æ ‡æ³¨\nç¨‹é”æ°ï¼ˆæ¸…åå¤§å­¦ï¼‰ï¼Œäºæ²›æ¥ ï¼ˆæ¸…åå¤§å­¦ï¼‰ï¼Œå¼ ç«å°§ï¼ˆæ™ºè°±AIï¼‰ï¼Œé»„é“‚æ–‡ï¼ˆæ™ºè°±AIï¼‰ï¼Œç‹ç‚¤å®‡ï¼ˆæ™ºè°±AIï¼‰\n    \n#### æŒ‡å¯¼æ•™å¸ˆ\n\n[æ¨æ¤éºŸ](https://kimiyoung.github.io/)ï¼ˆæ¸…åå¤§å­¦äº¤å‰ä¿¡æ¯å­¦é™¢ï¼‰ï¼Œä¸œæ˜±æ™“ï¼ˆæ¸…åå¤§å­¦çŸ¥è¯†å·¥ç¨‹å®éªŒå®¤ï¼‰ï¼Œé™ˆæ–‡å…‰ï¼ˆæ¸…åå¤§å­¦PACMANå®éªŒå®¤ï¼‰ï¼Œ[å”æ°](http://keg.cs.tsinghua.edu.cn/jietang/)ï¼ˆæ¸…åå¤§å­¦çŸ¥è¯†å·¥ç¨‹å®éªŒå®¤ï¼‰\n    \n\n#### è®¡ç®—èµ„æºæ”¯æŒ\n\n[é¹åŸå®éªŒå®¤](https://www.pcl.ac.cn/index.html)\n\n[æ™ºè°±AI](https://www.zhipu.ai/)\n\n#### é¡¹ç›®æ€»è´Ÿè´£\n\n[å”æ°](http://keg.cs.tsinghua.edu.cn/jietang/)ï¼ˆæ¸…åå¤§å­¦çŸ¥è¯†å·¥ç¨‹å®éªŒå®¤ & åŒ—äº¬æ™ºæºäººå·¥æ™ºèƒ½ç ”ç©¶é™¢ï¼‰\n</details>\n\nå¦‚æœé‡åˆ°é—®é¢˜æˆ–æœ‰ä»»ä½•å»ºè®®ï¼Œæ¬¢è¿é€šè¿‡é‚®ä»¶ä¸æˆ‘ä»¬è”ç³»[codegeex@aminer.cn](mailto:codegeex@aminer.cn).\n\n## è®¸å¯è¯\n\nä»£ç ä½¿ç”¨[Apache-2.0è®¸å¯è¯](LICENSE)\næ¨¡å‹ä½¿ç”¨[è®¸å¯è¯](MODEL_LICENSE)\n\n## å¼•ç”¨\n\nå¦‚æœè§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ï¼š\n\n```\n@inproceedings{zheng2023codegeex,\n  title={CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X},\n  author={Qinkai Zheng and Xiao Xia and Xu Zou and Yuxiao Dong and Shan Wang and Yufei Xue and Zihan Wang and Lei Shen and Andi Wang and Yang Li and Teng Su and Zhilin Yang and Jie Tang},\n  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},\n  pages={5673--5684},\n  year={2023}\n}\n```"
        },
        {
          "name": "api",
          "type": "tree",
          "content": null
        },
        {
          "name": "codegeex",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "deployment",
          "type": "tree",
          "content": null
        },
        {
          "name": "generations",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.17578125,
          "content": "fire>=0.4.0\nipython>=8.4.0\nnumpy>=1.22.0\npandas>=1.3.5\npyzmq>=23.2.1\nregex>=2022.3.15\nsetuptools>=58.0.4\ntransformers>=4.22.0\ntorch>=1.10.0\ntqdm>=4.63.0\ncpm_kernels\ndeepspeed>0.6.1"
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.6298828125,
          "content": "from setuptools import setup, find_packages\n\nsetup(\n    name=\"codegeex\",\n    py_modules=[\"codegeex\"],\n    version=\"1.0\",\n    description=\"CodeGeeX: A Open Multilingual Code Generation Model.\",\n    author=\"Qinkai Zheng\",\n    packages=find_packages(),\n    install_requires=[\n        \"fire>=0.4.0\",\n        \"ipython>=8.4.0\",\n        \"numpy>=1.22.0\",\n        \"pandas>=1.3.5\",\n        \"pyzmq>=23.2.1\",\n        \"regex>=2022.3.15\",\n        \"setuptools>=58.0.4\",\n        \"transformers>=4.22.0\",\n        \"tokenizers>=0.11.0\",\n        \"torch>=1.10.0\",\n        \"tqdm>=4.63.0\",\n        \"cpm_kernels\",\n        \"deepspeed>0.6.1\",\n    ],\n    entry_points={}\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "vscode-extension",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}