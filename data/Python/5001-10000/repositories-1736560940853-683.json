{
  "metadata": {
    "timestamp": 1736560940853,
    "page": 683,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lucidrains/DALLE-pytorch",
      "stars": 5598,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.85546875,
          "content": "# dall-e generation outputs\noutputs/\n*.pt\ntaming/\nwandb/\ndalle-ds-cp/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# Visual Studio Code\n.vscode\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.041015625,
          "content": "MIT License\n\nCopyright (c) 2021 Phil Wang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.037109375,
          "content": "recursive-include dalle_pytorch *.txt\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 27.4853515625,
          "content": "# DALL-E in Pytorch\n\n<p align='center'>\n  <a href=\"https://colab.research.google.com/gist/afiaka87/b29213684a1dd633df20cab49d05209d/train_dalle_pytorch.ipynb\">\n         <img alt=\"Train DALL-E w/ DeepSpeed\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n  </a>\n  <a href=\"https://discord.gg/xBPBXfcFHd\"><img alt=\"Join us on Discord\" src=\"https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&logoColor=white\"></a></br>\n  <a href=\"https://github.com/robvanvolt/DALLE-models\">Released DALLE Models</a></br>\n  <a href=\"https://github.com/rom1504/dalle-service\">Web-Hostable DALLE Checkpoints</a></br>\n\n  <a href=\"https://www.youtube.com/watch?v=j4xgkjWlfL4\">Yannic Kilcher's video</a>\n<p>\nImplementation / replication of <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a> (<a href=\"https://arxiv.org/abs/2102.12092\">paper</a>), OpenAI's Text to Image Transformer, in Pytorch.  It will also contain <a href=\"https://openai.com/blog/clip/\">CLIP</a> for ranking the generations.\n\n---\n\n\n\n[Quick Start](https://github.com/lucidrains/DALLE-pytorch/wiki)\n\n<a href=\"https://github.com/lucidrains/deep-daze\">Deep Daze</a> or <a href=\"https://github.com/lucidrains/big-sleep\">Big Sleep</a> are great alternatives!\n\nFor generating video and audio, please see <a href=\"https://github.com/lucidrains/nuwa-pytorch\">N√úWA</a>\n\n## Appreciation\n  \nThis library could not have been possible without the contributions of <a href=\"https://github.com/janEbert\">janEbert</a>, <a href=\"https://github.com/afiaka87\">Clay</a>, <a href=\"https://github.com/robvanvolt\">robvanvolt</a>, <a href=\"https://github.com/rom1504\">Romain Beaumont</a>, and <a href=\"https://github.com/borzunov\">Alexander</a>! üôè\n\n## Status\n<p align='center'>\n\n- <a href=\"https://github.com/htoyryla\">Hannu</a> has managed to train a small 6 layer DALL-E on a dataset of just 2000 landscape images! (2048 visual tokens)\n\n<img src=\"./images/landscape.png\"></img>\n\n- <a href=\"https://github.com/kobiso\">Kobiso</a>, a research engineer from Naver, has trained on the CUB200 dataset <a href=\"https://github.com/lucidrains/DALLE-pytorch/discussions/131\">here</a>, using full and deepspeed sparse attention\n\n<img src=\"./images/birds.png\" width=\"256\"></img>\n\n- (3/15/21) <a href=\"https://github.com/afiaka87\">afiaka87</a> has managed one epoch using a reversible DALL-E and the dVaE <a href=\"https://github.com/lucidrains/DALLE-pytorch/issues/86#issue-832121328\">here</a>\n\n- <a href=\"https://github.com/robvanvolt\">TheodoreGalanos</a> has trained on 150k layouts with the following results\n<p>\n  <img src=\"./images/layouts-1.jpg\" width=\"256\"></img>\n  <img src=\"./images/layouts-2.jpg\" width=\"256\"></img>\n</p>\n- <a href=\"https://github.com/rom1504\">Rom1504</a> has trained on 50k fashion images with captions with a really small DALL-E (2 layers) for just 24 hours with the following results\n<p/>\n<img src=\"./images/clothing.png\" width=\"420\"></img>\n\n- <a href=\"https://github.com/afiaka87\">afiaka87</a> trained for 6 epochs on the same dataset as before thanks to the efficient 16k VQGAN with the following <a href=\"https://github.com/lucidrains/DALLE-pytorch/discussions/322>discussion\">results</a>\n\n<p align='centered'>\n  <img src=\"https://user-images.githubusercontent.com/3994972/123564891-b6f18780-d780-11eb-9019-8a1b6178f861.png\" width=\"420\" alt-text='a photo of westwood park, san francisco, from the water in the afternoon'></img>\n  <img src=\"https://user-images.githubusercontent.com/3994972/123564776-4c404c00-d780-11eb-9c8e-3356df358df3.png\" width=\"420\" alt-text='a female mannequin dressed in an olive button-down shirt and gold palazzo pants'> </img>\n</p>\n  \nThanks to the amazing \"mega b#6696\" you can generate from this checkpoint in colab - \n<a href=\"https://colab.research.google.com/drive/11V2xw1eLPfZvzW8UQyTUhqCEU71w6Pr4?usp=sharing\">\n  <img alt=\"Run inference on the Afiaka checkpoint in Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">\n</a>\n\n- (5/2/21) First <a href=\"https://github.com/sberbank-ai/ru-dalle\">1.3B DALL-E</a> from üá∑üá∫ has been trained and released to the public! üéâ\n\n- (4/8/22) Moving onwards to <a href=\"https://github.com/lucidrains/dalle2-pytorch\">DALLE-2</a>!\n\n## Install\n\n```bash\n$ pip install dalle-pytorch\n```\n\n## Usage\n\nTrain VAE\n\n```python\nimport torch\nfrom dalle_pytorch import DiscreteVAE\n\nvae = DiscreteVAE(\n    image_size = 256,\n    num_layers = 3,           # number of downsamples - ex. 256 / (2 ** 3) = (32 x 32 feature map)\n    num_tokens = 8192,        # number of visual tokens. in the paper, they used 8192, but could be smaller for downsized projects\n    codebook_dim = 512,       # codebook dimension\n    hidden_dim = 64,          # hidden dimension\n    num_resnet_blocks = 1,    # number of resnet blocks\n    temperature = 0.9,        # gumbel softmax temperature, the lower this is, the harder the discretization\n    straight_through = False, # straight-through for gumbel softmax. unclear if it is better one way or the other\n)\n\nimages = torch.randn(4, 3, 256, 256)\n\nloss = vae(images, return_loss = True)\nloss.backward()\n\n# train with a lot of data to learn a good codebook\n```\n\nTrain DALL-E with pretrained VAE from above\n\n```python\nimport torch\nfrom dalle_pytorch import DiscreteVAE, DALLE\n\nvae = DiscreteVAE(\n    image_size = 256,\n    num_layers = 3,\n    num_tokens = 8192,\n    codebook_dim = 1024,\n    hidden_dim = 64,\n    num_resnet_blocks = 1,\n    temperature = 0.9\n)\n\ndalle = DALLE(\n    dim = 1024,\n    vae = vae,                  # automatically infer (1) image sequence length and (2) number of image tokens\n    num_text_tokens = 10000,    # vocab size for text\n    text_seq_len = 256,         # text sequence length\n    depth = 12,                 # should aim to be 64\n    heads = 16,                 # attention heads\n    dim_head = 64,              # attention head dimension\n    attn_dropout = 0.1,         # attention dropout\n    ff_dropout = 0.1            # feedforward dropout\n)\n\ntext = torch.randint(0, 10000, (4, 256))\nimages = torch.randn(4, 3, 256, 256)\n\nloss = dalle(text, images, return_loss = True)\nloss.backward()\n\n# do the above for a long time with a lot of data ... then\n\nimages = dalle.generate_images(text)\nimages.shape # (4, 3, 256, 256)\n```\n\nTo prime with a starting crop of an image, simply pass two more arguments\n\n```python\nimg_prime = torch.randn(4, 3, 256, 256)\n\nimages = dalle.generate_images(\n    text,\n    img = img_prime,\n    num_init_img_tokens = (14 * 32)  # you can set the size of the initial crop, defaults to a little less than ~1/2 of the tokens, as done in the paper\n)\n\nimages.shape # (4, 3, 256, 256)\n```\n\nYou may also want to generate text using DALL-E. For that call this function:\n\n```python\ntext_tokens, texts = dalle.generate_texts(tokenizer, text)\n```\n\n## OpenAI's Pretrained VAE\n\nYou can also skip the training of the VAE altogether, using the pretrained model released by OpenAI! The wrapper class should take care of downloading and caching the model for you auto-magically.\n\n```python\nimport torch\nfrom dalle_pytorch import OpenAIDiscreteVAE, DALLE\n\nvae = OpenAIDiscreteVAE()       # loads pretrained OpenAI VAE\n\ndalle = DALLE(\n    dim = 1024,\n    vae = vae,                  # automatically infer (1) image sequence length and (2) number of image tokens\n    num_text_tokens = 10000,    # vocab size for text\n    text_seq_len = 256,         # text sequence length\n    depth = 1,                  # should aim to be 64\n    heads = 16,                 # attention heads\n    dim_head = 64,              # attention head dimension\n    attn_dropout = 0.1,         # attention dropout\n    ff_dropout = 0.1            # feedforward dropout\n)\n\ntext = torch.randint(0, 10000, (4, 256))\nimages = torch.randn(4, 3, 256, 256)\n\nloss = dalle(text, images, return_loss = True)\nloss.backward()\n```\n\n## Taming Transformer's Pretrained VQGAN VAE\n\nYou can also use the pretrained VAE offered by the authors of <a href=\"https://github.com/CompVis/taming-transformers\">Taming Transformers</a>! Currently only the VAE with a codebook size of 1024 is offered, with the hope that it may train a little faster than OpenAI's, which has a size of 8192.\n\nIn contrast to OpenAI's VAE, it also has an extra layer of downsampling, so the image sequence length is 256 instead of 1024 (this will lead to a 16 reduction in training costs, when you do the math). Whether it will generalize as well as the original DALL-E is up to the citizen scientists out there to discover.\n\nUpdate - <a href=\"https://github.com/lucidrains/DALLE-pytorch/discussions/131\">it works!</a>\n\n```python\nfrom dalle_pytorch import VQGanVAE\n\nvae = VQGanVAE()\n\n# the rest is the same as the above example\n```\n\nThe default VQGan is the codebook size 1024 one trained on imagenet. If you wish to use a different one, you can use the `vqgan_model_path` and `vqgan_config_path` to pass the .ckpt file and the .yaml file. These options can be used both in train-dalle script or as argument of VQGanVAE class. Other pretrained VQGAN can be found in [taming transformers readme](https://github.com/CompVis/taming-transformers#overview-of-pretrained-models). If you want to train a custom one you can [follow this guide](https://github.com/CompVis/taming-transformers/pull/54)\n\n\n## Adjust text conditioning strength\n\nRecently there has surfaced a <a href=\"https://openreview.net/forum?id=qw8AKxfYbI\">new technique</a> for guiding diffusion models without a classifier. The gist of the technique involves randomly dropping out the text condition during training, and at inference time, deriving the rough direction from unconditional to conditional distributions.\n\n<a href=\"https://github.com/crowsonkb\">Katherine Crowson</a> outlined in a <a href=\"https://twitter.com/RiversHaveWings/status/1478093658716966912\">tweet</a> how this could work for autoregressive attention models. I have decided to include her idea in this repository for further exploration. One only has to account for two extra keyword arguments on training (`null_cond_prob`) and generation (`cond_scale`).\n\n```python\nimport torch\nfrom dalle_pytorch import DiscreteVAE, DALLE\n\nvae = DiscreteVAE(\n    image_size = 256,\n    num_layers = 3,\n    num_tokens = 8192,\n    codebook_dim = 1024,\n    hidden_dim = 64,\n    num_resnet_blocks = 1,\n    temperature = 0.9\n)\n\ndalle = DALLE(\n    dim = 1024,\n    vae = vae,\n    num_text_tokens = 10000,\n    text_seq_len = 256,\n    depth = 12,\n    heads = 16,\n    dim_head = 64,\n    attn_dropout = 0.1,\n    ff_dropout = 0.1\n)\n\ntext = torch.randint(0, 10000, (4, 256))\nimages = torch.randn(4, 3, 256, 256)\n\nloss = dalle(\n    text,\n    images,\n    return_loss = True,\n    null_cond_prob = 0.2  # firstly, set this to the probability of dropping out the condition, 20% is recommended as a default\n)\n\nloss.backward()\n\n# do the above for a long time with a lot of data ... then\n\nimages = dalle.generate_images(\n    text,\n    cond_scale = 3. # secondly, set this to a value greater than 1 to increase the conditioning beyond average\n)\n\nimages.shape # (4, 3, 256, 256)\n```\n\nThat's it!\n\n## Ranking the generations\n\nTrain CLIP\n\n```python\nimport torch\nfrom dalle_pytorch import CLIP\n\nclip = CLIP(\n    dim_text = 512,\n    dim_image = 512,\n    dim_latent = 512,\n    num_text_tokens = 10000,\n    text_enc_depth = 6,\n    text_seq_len = 256,\n    text_heads = 8,\n    num_visual_tokens = 512,\n    visual_enc_depth = 6,\n    visual_image_size = 256,\n    visual_patch_size = 32,\n    visual_heads = 8\n)\n\ntext = torch.randint(0, 10000, (4, 256))\nimages = torch.randn(4, 3, 256, 256)\nmask = torch.ones_like(text).bool()\n\nloss = clip(text, images, text_mask = mask, return_loss = True)\nloss.backward()\n```\n\nTo get the similarity scores from your trained Clipper, just do\n\n```python\nimages, scores = dalle.generate_images(text, mask = mask, clip = clip)\n\nscores.shape # (2,)\nimages.shape # (2, 3, 256, 256)\n\n# do your topk here, in paper they sampled 512 and chose top 32\n```\n\nOr you can just use the official <a href=\"https://github.com/openai/CLIP\">CLIP model</a> to rank the images from DALL-E\n\n## Scaling depth\n\nIn the blog post, they used 64 layers to achieve their results. I added reversible networks, from the <a href=\"https://github.com/lucidrains/reformer-pytorch\">Reformer</a> paper, in order for users to attempt to scale depth at the cost of compute. Reversible networks allow you to scale to any depth at no memory cost, but a little over 2x compute cost (each layer is rerun on the backward pass).\n\nSimply set the `reversible` keyword to `True` for the `DALLE` class\n\n```python\ndalle = DALLE(\n    dim = 1024,\n    vae = vae,\n    num_text_tokens = 10000,\n    text_seq_len = 256,\n    depth = 64,\n    heads = 16,\n    reversible = True  # <-- reversible networks https://arxiv.org/abs/2001.04451\n)\n```\n\n## Sparse Attention\n\nThe blogpost alluded to a mixture of different types of sparse attention, used mainly on the image (while the text presumably had full causal attention). I have done my best to replicate these types of sparse attention, on the scant details released. Primarily, it seems as though they are doing causal axial row / column attention, combined with a causal convolution-like attention.\n\nBy default `DALLE` will use full attention for all layers, but you can specify the attention type per layer as follows.\n\n- `full` full attention\n\n- `axial_row` axial attention, along the rows of the image feature map\n\n- `axial_col` axial attention, along the columns of the image feature map\n\n- `conv_like` convolution-like attention, for the image feature map\n\nThe sparse attention only applies to the image. Text will always receive full attention, as said in the blogpost.\n\n```python\ndalle = DALLE(\n    dim = 1024,\n    vae = vae,\n    num_text_tokens = 10000,\n    text_seq_len = 256,\n    depth = 64,\n    heads = 16,\n    reversible = True,\n    attn_types = ('full', 'axial_row', 'axial_col', 'conv_like')  # cycles between these four types of attention\n)\n```\n\n## Deepspeed Sparse Attention\n\nYou can also train with Microsoft Deepspeed's <a href=\"https://www.deepspeed.ai/news/2020/09/08/sparse-attention.html\">Sparse Attention</a>, with any combination of dense and sparse attention that you'd like. However, you will have to endure the installation process.\n\nFirst, you need to install Deepspeed with Sparse Attention\n\n```bash\n$ sh install_deepspeed.sh\n```\n\nNext, you need to install the pip package `triton`. It will need to be a version `< 1.0` because that's what Microsoft used.\n\n```bash\n$ pip install triton==0.4.2\n```\n\nIf both of the above succeeded, now you can train with Sparse Attention!\n\n```python\ndalle = DALLE(\n    dim = 512,\n    vae = vae,\n    num_text_tokens = 10000,\n    text_seq_len = 256,\n    depth = 64,\n    heads = 8,\n    attn_types = ('full', 'sparse')  # interleave sparse and dense attention for 64 layers\n)\n```\n\n## Training\n\nThis section will outline how to train the discrete variational autoencoder as well as the final multi-modal transformer (DALL-E). We are going to use <a href=\"https://wandb.ai/\">Weights & Biases</a> for all the experiment tracking.\n\n(You can also do everything in this section in a Google Colab, link below)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1dWvA54k4fH8zAmiix3VXbg95uEIMfqQM?usp=sharing) Train in Colab\n\n```bash\n$ pip install wandb\n```\n\nFollowed by\n\n```bash\n$ wandb login\n```\n\n### VAE\n\nTo train the VAE, you just need to run\n\n```python\n$ python train_vae.py --image_folder /path/to/your/images\n```\n\nIf you installed everything correctly, a link to the experiments page should show up in your terminal. You can follow your link there and customize your experiment, like the example layout below.\n\n<img src=\"./images/wb.png\" width=\"700px\"></img>\n\nYou can of course open up the training script at `./train_vae.py`, where you can modify the constants, what is passed to Weights & Biases, or any other tricks you know to make the VAE learn better.\n\nModel will be saved periodically to `./vae.pt`\n\nIn the experiment tracker, you will have to monitor the hard reconstruction, as we are essentially teaching the network to compress images into discrete visual tokens for use in the transformer as a visual vocabulary.\n\nWeights and Biases will allow you to monitor the temperature annealing, image reconstructions (encoder and decoder working properly), as well as to watch out for codebook collapse (where the network decides to only use a few tokens out of what you provide it).\n\nOnce you have trained a decent VAE to your satisfaction, you can move on to the next step with your model weights at `./vae.pt`.\n\n### DALL-E Training\n\n## Training using an Image-Text-Folder\n\nNow you just have to invoke the `./train_dalle.py` script, indicating which VAE model you would like to use, as well as the path to your folder if images and text.\n\nThe dataset I am currently working with contains a folder of images and text files, arbitraily nested in subfolders, where text file name corresponds with the image name, and where each text file contains multiple descriptions, delimited by newlines. The script will find and pair all the image and text files with the same names, and randomly select one of the textual descriptions during batch creation.\n\nex.\n\n```\nüìÇimage-and-text-data\n ‚î£ üìúcat.png\n ‚î£ üìúcat.txt\n ‚î£ üìúdog.jpg\n ‚î£ üìúdog.txt\n ‚î£ üìúturtle.jpeg\n ‚îó üìúturtle.txt\n```\n\nex. `cat.txt`\n\n```text\nA black and white cat curled up next to the fireplace\nA fireplace, with a cat sleeping next to it\nA black cat with a red collar napping\n```\n\nIf you have a dataset with its own directory structure for tying together image and text descriptions, do let me know in the issues, and I'll see if I can accommodate it in the script.\n\n```python\n$ python train_dalle.py --vae_path ./vae.pt --image_text_folder /path/to/data\n```\n\nYou likely will not finish DALL-E training as quickly as you did your Discrete VAE. To resume from where you left off, just run the same script, but with the path to your DALL-E checkpoints.\n\n```python\n$ python train_dalle.py --dalle_path ./dalle.pt --image_text_folder /path/to/data\n```\n\n## Training using WebDataset\n\nWebDataset files are regular .tar(.gz) files which can be streamed and used for DALLE-pytorch training.\nYou Just need to provide the image (first comma separated argument) and caption (second comma separated argument) \ncolumn key after the --wds argument. The ---image_text_folder points to your .tar(.gz) file instead of the datafolder.\n\n```python\n$ python train_dalle.py --wds img,cap --image_text_folder /path/to/data.tar(.gz)\n```\n\nDistributed training with deepspeed works the same way, e.g.:\n\n```python\n$ deepspeed train_dalle.py --wds img,cap --image_text_folder /path/to/data.tar(.gz) --fp16 --deepspeed\n```\n\nIf you have containing shards (dataset split into several .tar(.gz) files), this is also supported:\n\n```python\n$ deepspeed train_dalle.py --wds img,cap --image_text_folder /path/to/shardfolder --fp16 --deepspeed\n```\n\nYou can stream the data from a http server or gloogle cloud storage like this:\n\n```python\n$ deepspeed train_dalle.py --image_text_folder \"http://storage.googleapis.com/nvdata-openimages/openimages-train-{000000..000554}.tar\" --wds jpg,json --taming --truncate_captions --random_resize_crop_lower_ratio=0.8 --attn_types=full --epochs=2 --fp16 --deepspeed\n```\n\nIn order to convert your image-text-folder to WebDataset format, you can make use of one of several methods.\n(https://www.youtube.com/watch?v=v_PacO-3OGQ here are given 4 examples, or a little helper script which also supports splitting your dataset\ninto shards of .tar.gz files https://github.com/robvanvolt/DALLE-datasets/blob/main/wds_create_shards.py)\n\n### DALL-E with OpenAI's VAE\n\nYou can now also train DALL-E without having to train the Discrete VAE at all, courtesy to their open-sourcing their model. You simply have to invoke the `train_dalle.py` script without specifying the `--vae_path`\n\n```python\n$ python train_dalle.py --image_text_folder /path/to/coco/dataset\n```\n\n### DALL-E with Taming Transformer's VQVAE\n\nJust use the `--taming` flag. Highly recommended you use this VAE over the OpenAI one!\n\n```python\n$ python train_dalle.py --image_text_folder /path/to/coco/dataset --taming\n```\n\n### Generation\n\nOnce you have successfully trained DALL-E, you can then use the saved model for generation!\n\n```python\n$ python generate.py --dalle_path ./dalle.pt --text 'fireflies in a field under a full moon'\n```\n\nYou should see your images saved as `./outputs/{your prompt}/{image number}.jpg`\n\nTo generate multiple images, just pass in your text with '|' character as a separator.\n\nex.\n\n```python\n$ python generate.py --dalle_path ./dalle.pt --text 'a dog chewing a bone|a cat chasing mice|a frog eating a fly'\n```\n\nNote that DALL-E is a full image+text language model. As a consequence you can also generate text using a dalle model.\n\n```python\n$ python generate.py --dalle_path ./dalle.pt --text 'a dog chewing a bone' --gentext\n```\n\nThis will complete the provided text, save it in a caption.txt and generate the corresponding images.\n\n### Docker\n\nYou can use a docker container to make sure the version of Pytorch and Cuda are correct for training DALL-E. <a href=\"https://docs.docker.com/get-docker/\">Docker</a> and <a href='#'>Docker Container Runtime</a> should be installed.\n\nTo build:\n\n```bash\ndocker build -t dalle docker\n```\n\nTo run in an interactive shell:\n\n```bash\ndocker run --gpus all -it --mount src=\"$(pwd)\",target=/workspace/dalle,type=bind dalle:latest bash\n```\n\n### Distributed Training\n\n#### DeepSpeed\n\nThanks to <a href=\"https://github.com/janEbert\">janEbert</a>, the repository is now equipped so you can train DALL-E with Microsoft's <a href=\"https://www.deepspeed.ai/\">Deepspeed</a>!\n\nYou can simply replace any `$ python <file>.py [args...]` command with\n\n```sh\n$ deepspeed <file>.py [args...] --deepspeed\n```\n\nto use the aforementioned DeepSpeed library for distributed training, speeding up your experiments.\n\nModify the `deepspeed_config` dictionary in `train_dalle.py` or\n`train_vae.py` according to the DeepSpeed settings you'd like to use\nfor each one. See the [DeepSpeed configuration\ndocs](https://www.deepspeed.ai/docs/config-json/) for more\ninformation.\n\n#### DeepSpeed - 32 and 16 bit Precision\nAs of DeepSpeed version 0.3.16, ZeRO optimizations can be used with\nsingle-precision floating point numbers. If you are using an older\nversion, you'll have to pass the `--fp16` flag to be able to enable\nZeRO optimizations.\n\n\n#### DeepSpeed - Apex Automatic Mixed Precision.\nAutomatic mixed precision is a stable alternative to fp16 which still provides a decent speedup.\nIn order to run with Apex AMP (through DeepSpeed), you will need to install DeepSpeed using either the Dockerfile or the bash script.\n\nThen you will need to install apex from source. \nThis may take awhile and you may see some compilation warnings which can be ignored. \n```sh\nsh install_apex.sh\n```\n\nNow, run `train_dalle.py` with `deepspeed` instead of `python` as done here:\n```sh\ndeepspeed train_dalle.py \\\n    --taming \\\n    --image_text_folder 'DatasetsDir' \\\n    --distr_backend 'deepspeed' \\\n    --amp\n```\n\n#### Horovod\n\n[Horovod](https://horovod.ai) offers a stable way for data parallel\ntraining.\n\nAfter [installing\nHorovod](https://github.com/lucidrains/DALLE-pytorch/wiki/Horovod-Installation),\nreplace any `$ python <file>.py [args...]` command with\n\n```sh\n$ horovodrun -np <num-gpus> <file>.py [args...] --distributed_backend horovod\n```\n\nto use the Horovod library for distributed training, speeding up your\nexperiments. This will multiply your effective batch size per training\nstep by `<num-gpus>`, so you may need to rescale the learning rate\naccordingly.\n\n#### Custom Tokenizer\n\nThis repository supports custom tokenization with <a href=\"https://github.com/VKCOM/YouTokenToMe\">YouTokenToMe</a>, if you wish to use it instead of the default simple tokenizer. Simply pass in an extra `--bpe_path` when invoking `train_dalle.py` and `generate.py`, with the path to your BPE model file.\n\nThe only requirement is that you use `0` as the padding during tokenization\n\nex.\n\n```sh\n$ python train_dalle.py --image_text_folder ./path/to/data --bpe_path ./path/to/bpe.model\n```\n\nTo create a BPE model file from scratch, firstly\n\n```bash\n$ pip install youtokentome\n```\n\nThen you need to prepare a big text file that is a representative sample of the type of text you want to encode. You can then invoke the `youtokentome` command-line tools. You'll also need to specify the vocab size you wish to use, in addition to the corpus of text.\n\n```bash\n$ yttm bpe --vocab_size 8000 --data ./path/to/big/text/file.txt --model ./path/to/bpe.model\n```\n\nThat's it! The BPE model file is now saved to `./path/to/bpe.model` and you can begin training!\n\n#### Chinese\n\nYou can train with a <a href=\"https://huggingface.co/bert-base-chinese\">pretrained chinese tokenizer</a> offered by Huggingface ü§ó by simply passing in an extra flag `--chinese`\n\nex.\n\n```sh\n$ python train_dalle.py --chinese --image_text_folder ./path/to/data\n```\n\n```sh\n$ python generate.py --chinese --text 'ËøΩËÄÅÈº†ÁöÑÁå´'\n```\n\n## Citations\n\n```bibtex\n@misc{ramesh2021zeroshot,\n    title   = {Zero-Shot Text-to-Image Generation}, \n    author  = {Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},\n    year    = {2021},\n    eprint  = {2102.12092},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{unpublished2021clip,\n    title  = {CLIP: Connecting Text and Images},\n    author = {Alec Radford, Ilya Sutskever, Jong Wook Kim, Gretchen Krueger, Sandhini Agarwal},\n    year   = {2021}\n}\n```\n\n```bibtex\n@misc{kitaev2020reformer,\n    title   = {Reformer: The Efficient Transformer},\n    author  = {Nikita Kitaev and ≈Åukasz Kaiser and Anselm Levskaya},\n    year    = {2020},\n    eprint  = {2001.04451},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.LG}\n}\n```\n\n```bibtex\n@misc{esser2021taming,\n    title   = {Taming Transformers for High-Resolution Image Synthesis},\n    author  = {Patrick Esser and Robin Rombach and Bj√∂rn Ommer},\n    year    = {2021},\n    eprint  = {2012.09841},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{ding2021cogview,\n    title   = {CogView: Mastering Text-to-Image Generation via Transformers},\n    author  = {Ming Ding and Zhuoyi Yang and Wenyi Hong and Wendi Zheng and Chang Zhou and Da Yin and Junyang Lin and Xu Zou and Zhou Shao and Hongxia Yang and Jie Tang},\n    year    = {2021},\n    eprint  = {2105.13290},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@software{peng_bo_2021_5196578,\n    author       = {PENG Bo},\n    title        = {BlinkDL/RWKV-LM: 0.01},\n    month        = {aug},\n    year         = {2021},\n    publisher    = {Zenodo},\n    version      = {0.01},\n    doi          = {10.5281/zenodo.5196578},\n    url          = {https://doi.org/10.5281/zenodo.5196578}\n}\n```\n\n```bibtex\n@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},\n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n```\n\n```bibtex\n@inproceedings{ho2021classifierfree,\n    title   = {Classifier-Free Diffusion Guidance},\n    author  = {Jonathan Ho and Tim Salimans},\n    booktitle = {NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications},\n    year    = {2021},\n    url     = {https://openreview.net/forum?id=qw8AKxfYbI}\n}\n```\n\n```bibtex\n@misc{crowson2022,\n    author  = {Katherine Crowson},\n    url     = {https://twitter.com/RiversHaveWings/status/1478093658716966912}\n}\n```\n\n```bibtex\n@article{Liu2023BridgingDA,\n    title   = {Bridging Discrete and Backpropagation: Straight-Through and Beyond},\n    author  = {Liyuan Liu and Chengyu Dong and Xiaodong Liu and Bin Yu and Jianfeng Gao},\n    journal = {ArXiv},\n    year    = {2023},\n    volume  = {abs/2304.08612}\n}\n```\n\n*Those who do not want to imitate anything, produce nothing.* - Dali\n"
        },
        {
          "name": "dalle_pytorch",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "generate.py",
          "type": "blob",
          "size": 4.5849609375,
          "content": "import argparse\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# torch\n\nimport torch\n\nfrom einops import repeat\n\n# vision imports\n\nfrom PIL import Image\nfrom torchvision.utils import make_grid, save_image\n\n# dalle related classes and utils\n\nfrom dalle_pytorch import __version__\nfrom dalle_pytorch import DiscreteVAE, OpenAIDiscreteVAE, VQGanVAE, DALLE\nfrom dalle_pytorch.tokenizer import tokenizer, HugTokenizer, YttmTokenizer, ChineseTokenizer\n\n# argument parsing\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--dalle_path', type = str, required = True,\n                    help='path to your trained DALL-E')\n\nparser.add_argument('--vqgan_model_path', type=str, default = None,\n                   help='path to your trained VQGAN weights. This should be a .ckpt file. (only valid when taming option is enabled)')\n\nparser.add_argument('--vqgan_config_path', type=str, default = None,\n                   help='path to your trained VQGAN config. This should be a .yaml file.  (only valid when taming option is enabled)')\n\nparser.add_argument('--text', type = str, required = True,\n                    help='your text prompt')\n\nparser.add_argument('--num_images', type = int, default = 128, required = False,\n                    help='number of images')\n\nparser.add_argument('--batch_size', type = int, default = 4, required = False,\n                    help='batch size')\n\nparser.add_argument('--top_k', type = float, default = 0.9, required = False,\n                    help='top k filter threshold')\n\nparser.add_argument('--outputs_dir', type = str, default = './outputs', required = False,\n                    help='output directory')\n\nparser.add_argument('--bpe_path', type = str,\n                    help='path to your huggingface BPE json file')\n\nparser.add_argument('--hug', dest='hug', action = 'store_true')\n\nparser.add_argument('--chinese', dest='chinese', action = 'store_true')\n\nparser.add_argument('--taming', dest='taming', action='store_true')\n\nparser.add_argument('--gentxt', dest='gentxt', action='store_true')\n\nargs = parser.parse_args()\n\n# helper fns\n\ndef exists(val):\n    return val is not None\n\n# tokenizer\n\nif exists(args.bpe_path):\n    klass = HugTokenizer if args.hug else YttmTokenizer\n    tokenizer = klass(args.bpe_path)\nelif args.chinese:\n    tokenizer = ChineseTokenizer()\n\n# load DALL-E\n\ndalle_path = Path(args.dalle_path)\n\nassert dalle_path.exists(), 'trained DALL-E must exist'\n\nload_obj = torch.load(str(dalle_path))\ndalle_params, vae_params, weights, vae_class_name, version = load_obj.pop('hparams'), load_obj.pop('vae_params'), load_obj.pop('weights'), load_obj.pop('vae_class_name', None), load_obj.pop('version', None)\n\n# friendly print\n\nif exists(version):\n    print(f'Loading a model trained with DALLE-pytorch version {version}')\nelse:\n    print('You are loading a model trained on an older version of DALL-E pytorch - it may not be compatible with the most recent version')\n\n# load VAE\n\nif args.taming:\n    vae = VQGanVAE(args.vqgan_model_path, args.vqgan_config_path)\nelif vae_params is not None:\n    vae = DiscreteVAE(**vae_params)\nelse:\n    vae = OpenAIDiscreteVAE()\n\nassert not (exists(vae_class_name) and vae.__class__.__name__ != vae_class_name), f'you trained DALL-E using {vae_class_name} but are trying to generate with {vae.__class__.__name__} - please make sure you are passing in the correct paths and settings for the VAE to use for generation'\n\n# reconstitute DALL-E\n\ndalle = DALLE(vae = vae, **dalle_params).cuda()\n\ndalle.load_state_dict(weights)\n\n# generate images\n\nimage_size = vae.image_size\n\ntexts = args.text.split('|')\n\nfor j, text in tqdm(enumerate(texts)):\n    if args.gentxt:\n        text_tokens, gen_texts = dalle.generate_texts(tokenizer, text=text, filter_thres = args.top_k)\n        text = gen_texts[0]\n    else:\n        text_tokens = tokenizer.tokenize([text], dalle.text_seq_len).cuda()\n\n    text_tokens = repeat(text_tokens, '() n -> b n', b = args.num_images)\n\n    outputs = []\n\n    for text_chunk in tqdm(text_tokens.split(args.batch_size), desc = f'generating images for - {text}'):\n        output = dalle.generate_images(text_chunk, filter_thres = args.top_k)\n        outputs.append(output)\n\n    outputs = torch.cat(outputs)\n\n    # save all images\n\n    file_name = text \n    outputs_dir = Path(args.outputs_dir) / file_name.replace(' ', '_')[:(100)]\n    outputs_dir.mkdir(parents = True, exist_ok = True)\n\n    for i, image in tqdm(enumerate(outputs), desc = 'saving images'):\n        save_image(image, outputs_dir / f'{i}.png', normalize=True)\n        with open(outputs_dir / 'caption.txt', 'w') as f:\n            f.write(file_name)\n\n    print(f'created {args.num_images} images at \"{str(outputs_dir)}\"')\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "install_apex.sh",
          "type": "blob",
          "size": 0.1845703125,
          "content": "git clone https://github.com/NVIDIA/apex.git /tmp/apex\ncd /tmp/apex && pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n"
        },
        {
          "name": "install_deepspeed.sh",
          "type": "blob",
          "size": 0.1650390625,
          "content": "sudo apt-get -y install llvm-9-dev cmake\ngit clone https://github.com/microsoft/DeepSpeed.git /tmp/Deepspeed\ncd /tmp/Deepspeed && DS_BUILD_SPARSE_ATTN=1 ./install.sh -s\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.1220703125,
          "content": "from setuptools import setup, find_packages\nexec(open('dalle_pytorch/version.py').read())\n\nsetup(\n  name = 'dalle-pytorch',\n  packages = find_packages(),\n  include_package_data = True,\n  version = __version__,\n  license='MIT',\n  description = 'DALL-E - Pytorch',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  long_description_content_type = 'text/markdown',\n  url = 'https://github.com/lucidrains/dalle-pytorch',\n  keywords = [\n    'artificial intelligence',\n    'attention mechanism',\n    'transformers',\n    'text-to-image'\n  ],\n  install_requires=[\n    'axial_positional_embedding',\n    'DALL-E',\n    'einops>=0.3.2',\n    'ftfy',\n    'packaging',\n    'pillow',\n    'regex',\n    'rotary-embedding-torch',\n    'taming-transformers-rom1504',\n    'tokenizers',\n    'torch>=1.6',\n    'torchvision',\n    'transformers',\n    'tqdm',\n    'youtokentome',\n    'WebDataset'\n  ],\n  classifiers=[\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'License :: OSI Approved :: MIT License',\n    'Programming Language :: Python :: 3.6',\n  ],\n)\n"
        },
        {
          "name": "train_dalle.py",
          "type": "blob",
          "size": 23.1171875,
          "content": "import argparse\nfrom pathlib import Path\nimport time\nfrom glob import glob\nimport os\nimport shutil\n\nimport torch\nimport wandb  # Quit early if user doesn't have wandb installed.\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\n\nfrom dalle_pytorch import __version__\nfrom dalle_pytorch import OpenAIDiscreteVAE, VQGanVAE, DiscreteVAE, DALLE\nfrom dalle_pytorch import distributed_utils\nfrom dalle_pytorch.loader import TextImageDataset\nfrom dalle_pytorch.tokenizer import tokenizer, HugTokenizer, ChineseTokenizer, YttmTokenizer\n\n# libraries needed for webdataset support\n\nimport webdataset as wds\nfrom torchvision import transforms as T\nfrom PIL import Image\nfrom io import BytesIO\n\n\n# argument parsing\n\nparser = argparse.ArgumentParser()\n\ngroup = parser.add_mutually_exclusive_group(required=False)\n\ngroup.add_argument('--vae_path', type=str,\n                   help='path to your trained discrete VAE')\n\ngroup.add_argument('--dalle_path', type=str,\n                   help='path to your partially trained DALL-E')\n\nparser.add_argument('--vqgan_model_path', type=str, default = None,\n                   help='path to your trained VQGAN weights. This should be a .ckpt file. (only valid when taming option is enabled)')\n\nparser.add_argument('--vqgan_config_path', type=str, default = None,\n                   help='path to your trained VQGAN config. This should be a .yaml file. (only valid when taming option is enabled)')\n\nparser.add_argument('--image_text_folder', type=str, required=True,\n                    help='path to your folder of images and text for learning the DALL-E')\n\nparser.add_argument('--wds', type = str, default='',\n                    help = 'Comma separated list of WebDataset (1) image and (2) text column names. Must contain 2 values, e.g. img,cap.')\n\nparser.add_argument('--truncate_captions', dest='truncate_captions', action='store_true',\n                    help='Captions passed in which exceed the max token length will be truncated if this is set.')\n\nparser.add_argument('--random_resize_crop_lower_ratio', dest='resize_ratio', type=float, default=0.75,\n                    help='Random resized crop lower ratio')\n\nparser.add_argument('--chinese', dest='chinese', action='store_true')\n\nparser.add_argument('--taming', dest='taming', action='store_true')\n\nparser.add_argument('--hug', dest='hug', action='store_true')\n\nparser.add_argument('--bpe_path', type=str,\n                    help='path to your BPE json file')\n\nparser.add_argument('--dalle_output_file_name', type=str, default = \"dalle\",\n                    help='output_file_name')\n\nparser.add_argument('--fp16', action='store_true',\n                    help='(experimental) - Enable DeepSpeed 16 bit precision. Reduces VRAM.')\n\n\nparser.add_argument('--amp', action='store_true',\n\t               help='Apex \"O1\" automatic mixed precision. More stable than 16 bit precision. Can\\'t be used in conjunction with deepspeed zero stages 1-3.')\n\nparser.add_argument('--wandb_name', default='dalle_train_transformer',\n                    help='Name W&B will use when saving results.\\ne.g. `--wandb_name \"coco2017-full-sparse\"`')\n\nparser.add_argument('--wandb_entity', default=None,\n                    help='(optional) Name of W&B team/entity to log to.')\n\nparser.add_argument('--stable_softmax', dest='stable_softmax', action='store_true',\n                    help='Prevent values from becoming too large during softmax. Helps with stability in fp16 and Mixture of Quantization training.')\n\nparser = distributed_utils.wrap_arg_parser(parser)\n\ntrain_group = parser.add_argument_group('Training settings')\n\ntrain_group.add_argument('--flops_profiler', dest = 'flops_profiler', action='store_true', help = 'Exits after printing detailed flops/runtime analysis of forward/backward')\n\ntrain_group.add_argument('--epochs', default = 20, type = int, help = 'Number of epochs')\n\ntrain_group.add_argument('--save_every_n_steps', default = 1000, type = int, help = 'Save a checkpoint every n steps')\n\ntrain_group.add_argument('--keep_n_checkpoints', default = None, type = int, help = '(Careful) Deletes old deepspeed checkpoints if there are more than n')\n\ntrain_group.add_argument('--batch_size', default = 4, type = int, help = 'Batch size')\n\ntrain_group.add_argument('--ga_steps', default = 1, type = int, help = 'Number of steps to accumulate gradients across per each iteration. DeepSpeed only.')\n\ntrain_group.add_argument('--learning_rate', default = 3e-4, type = float, help = 'Learning rate')\n\ntrain_group.add_argument('--clip_grad_norm', default = 0.5, type = float, help = 'Clip gradient norm')\n\ntrain_group.add_argument('--lr_decay', dest = 'lr_decay', action = 'store_true')\n\nmodel_group = parser.add_argument_group('Model settings')\n\nmodel_group.add_argument('--dim', default = 512, type = int, help = 'Model dimension')\n\nmodel_group.add_argument('--text_seq_len', default = 256, type = int, help = 'Text sequence length')\n\nmodel_group.add_argument('--depth', default = 2, type = int, help = 'Model depth')\n\nmodel_group.add_argument('--heads', default = 8, type = int, help = 'Model number of heads')\n\nmodel_group.add_argument('--dim_head', default = 64, type = int, help = 'Model head dimension')\n\ntrain_group.add_argument('--ff_dropout', default = 0.0, type = float, help = 'Feed forward dropout.')\n\ntrain_group.add_argument('--attn_dropout', default = 0.0, type = float, help = 'Feed forward dropout.')\n\nmodel_group.add_argument('--reversible', dest = 'reversible', action='store_true')\n\nmodel_group.add_argument('--loss_img_weight', default = 7, type = int, help = 'Image loss weight')\n\nmodel_group.add_argument('--attn_types', default = 'full', type = str, help = 'comma separated list of attention types. attention type can be: full or sparse or axial_row or axial_col or conv_like.')\n\nmodel_group.add_argument('--shift_tokens', help = 'Use the shift tokens feature', action = 'store_true')\n\nmodel_group.add_argument('--rotary_emb', help = 'Use rotary embeddings', action = 'store_true')\n\nmodel_group.add_argument('--shared_attn_ids', default = None, type = str, help = 'Comma separated list of shared attention layer ids. Default: sharing is disabled')\n\nmodel_group.add_argument('--shared_ff_ids', default = None, type = str, help = 'Comma separated list of shared feed forward layer ids. Default: sharing is disabled')\n\nmodel_group.add_argument('--share_input_output_emb', help = 'Share input and output embeddings', action = 'store_true')\n\nargs = parser.parse_args()\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\ndef get_trainable_params(model):\n    return [params for params in model.parameters() if params.requires_grad]\n\ndef cp_path_to_dir(cp_path, tag):\n    \"\"\"Convert a checkpoint path to a directory with `tag` inserted.\n    If `cp_path` is already a directory, return it unchanged.\n    \"\"\"\n    if not isinstance(cp_path, Path):\n        cp_path = Path(cp_path)\n    if cp_path.is_dir():\n        return cp_path\n    path_sans_extension = cp_path.parent / cp_path.stem\n    cp_dir = Path(f'{path_sans_extension}-{tag}-cp')\n    return cp_dir\n\n# constants\n\nWEBDATASET_IMAGE_TEXT_COLUMNS = tuple(args.wds.split(','))\nENABLE_WEBDATASET = True if len(WEBDATASET_IMAGE_TEXT_COLUMNS) == 2 else False\n\nDALLE_OUTPUT_FILE_NAME = args.dalle_output_file_name + \".pt\"\n\nVAE_PATH = args.vae_path\nVQGAN_MODEL_PATH = args.vqgan_model_path\nVQGAN_CONFIG_PATH = args.vqgan_config_path\nDALLE_PATH = args.dalle_path\nRESUME = exists(DALLE_PATH)\n\nEPOCHS = args.epochs\nBATCH_SIZE = args.batch_size\n\nLEARNING_RATE = args.learning_rate\nGRAD_CLIP_NORM = args.clip_grad_norm\nLR_DECAY = args.lr_decay\nSAVE_EVERY_N_STEPS = args.save_every_n_steps\nKEEP_N_CHECKPOINTS = args.keep_n_checkpoints\n\nMODEL_DIM = args.dim\nTEXT_SEQ_LEN = args.text_seq_len\nDEPTH = args.depth\nHEADS = args.heads\nDIM_HEAD = args.dim_head\nREVERSIBLE = args.reversible\nLOSS_IMG_WEIGHT = args.loss_img_weight\nFF_DROPOUT = args.ff_dropout\nATTN_DROPOUT = args.attn_dropout\nSTABLE = args.stable_softmax\nSHIFT_TOKENS = args.shift_tokens\nROTARY_EMB = args.rotary_emb\n\nATTN_TYPES = tuple(args.attn_types.split(','))\nSHARED_ATTN_IDS = tuple(args.shared_attn_ids.split(',')) if exists(args.shared_attn_ids) else None\nSHARED_FF_IDS = tuple(args.shared_ff_ids.split(',')) if exists(args.shared_ff_ids) else None\nSHARE_INPUT_OUTPUT_EMB = args.share_input_output_emb\n\nDEEPSPEED_CP_AUX_FILENAME = 'auxiliary.pt'\n\nif not ENABLE_WEBDATASET:\n    # quit early if you used the wrong folder name\n    assert Path(args.image_text_folder).exists(), f'The path {args.image_text_folder} was not found.'\nelse:\n    # quit early if no tar files were found\n    if Path(args.image_text_folder).is_dir():\n        DATASET = [str(p) for p in Path(args.image_text_folder).glob(\"**/*\") if \".tar\" in str(p).lower()] # .name\n        assert len(DATASET) > 0, 'The directory ({}) does not contain any WebDataset/.tar files.'.format(args.image_text_folder)\n        print('Found {} WebDataset .tar(.gz) file(s) under given path {}!'.format(len(DATASET), args.image_text_folder))\n    elif ('http://' in args.image_text_folder.lower()) | ('https://' in args.image_text_folder.lower()):\n        DATASET = f\"pipe:curl -L -s {args.image_text_folder} || true\"\n        print('Found {} http(s) link under given path!'.format(len(DATASET), args.image_text_folder))\n    elif 'gs://' in args.image_text_folder.lower():\n        DATASET = f\"pipe:gsutil cat {args.image_text_folder} || true\"\n        print('Found {} GCS link under given path!'.format(len(DATASET), args.image_text_folder))\n    elif '.tar' in args.image_text_folder:\n        DATASET = args.image_text_folder\n        print('Found WebDataset .tar(.gz) file under given path {}!'.format(args.image_text_folder))\n    else:\n        raise Exception('No folder, no .tar(.gz) and no url pointing to tar files provided under {}.'.format(args.image_text_folder))\n\n# initialize distributed backend\n\ndistr_backend = distributed_utils.set_backend_from_args(args)\ndistr_backend.initialize()\n\nusing_deepspeed = \\\n    distributed_utils.using_backend(distributed_utils.DeepSpeedBackend)\n\nis_root = distr_backend.is_root_worker()\n\n# tokenizer\n\nif exists(args.bpe_path):\n    klass = HugTokenizer if args.hug else YttmTokenizer\n    tokenizer = klass(args.bpe_path)\nelif args.chinese:\n    tokenizer = ChineseTokenizer()\n\n# reconstitute vae\n\nif RESUME:\n    dalle_path = Path(DALLE_PATH)\n    if using_deepspeed:\n        cp_dir = cp_path_to_dir(dalle_path, 'ds')\n        assert cp_dir.is_dir(), \\\n            f'DeepSpeed checkpoint directory {cp_dir} not found'\n        dalle_path = cp_dir / DEEPSPEED_CP_AUX_FILENAME\n    else:\n        assert dalle_path.exists(), 'DALL-E model file does not exist'\n    loaded_obj = torch.load(str(dalle_path), map_location='cpu')\n\n    dalle_params, vae_params, weights = loaded_obj['hparams'], loaded_obj['vae_params'], loaded_obj['weights']\n    opt_state = loaded_obj.get('opt_state')\n    scheduler_state = loaded_obj.get('scheduler_state')\n\n    if vae_params is not None:\n        vae = DiscreteVAE(**vae_params)\n    elif args.taming:\n        vae = VQGanVAE(VQGAN_MODEL_PATH, VQGAN_CONFIG_PATH)\n    else:\n        vae = OpenAIDiscreteVAE()\n\n    resume_epoch = loaded_obj.get('epoch', 0)\nelse:\n    if exists(VAE_PATH):\n        vae_path = Path(VAE_PATH)\n        assert vae_path.exists(), 'VAE model file does not exist'\n        assert not vae_path.is_dir(), \\\n            ('Cannot load VAE model from directory; please use a '\n             'standard *.pt checkpoint. '\n             'Currently, merging a DeepSpeed-partitioned VAE into a DALLE '\n             'model is not supported.')\n\n        loaded_obj = torch.load(str(vae_path))\n\n        vae_params, weights = loaded_obj['hparams'], loaded_obj['weights']\n\n        vae = DiscreteVAE(**vae_params)\n        vae.load_state_dict(weights)\n    else:\n        if is_root:\n            print('using pretrained VAE for encoding images to tokens')\n        vae_params = None\n\n        if args.taming:\n            vae = VQGanVAE(VQGAN_MODEL_PATH, VQGAN_CONFIG_PATH)\n        else:\n            vae = OpenAIDiscreteVAE()\n\n    dalle_params = dict(\n        num_text_tokens=tokenizer.vocab_size,\n        text_seq_len=TEXT_SEQ_LEN,\n        dim=MODEL_DIM,\n        depth=DEPTH,\n        heads=HEADS,\n        dim_head=DIM_HEAD,\n        reversible=REVERSIBLE,\n        loss_img_weight=LOSS_IMG_WEIGHT,\n        attn_types=ATTN_TYPES,\n        ff_dropout=FF_DROPOUT,\n        attn_dropout=ATTN_DROPOUT,\n        stable=STABLE,\n        shift_tokens=SHIFT_TOKENS,\n        rotary_emb=ROTARY_EMB,\n        shared_attn_ids=SHARED_ATTN_IDS,\n        shared_ff_ids=SHARED_FF_IDS,\n        share_input_output_emb=SHARE_INPUT_OUTPUT_EMB,\n    )\n    resume_epoch = 0\n\nIMAGE_SIZE = vae.image_size\nCHANNELS = vae.channels\nTRANSPARENT = CHANNELS == 4\nIMAGE_MODE = 'RGBA' if CHANNELS == 4 else 'RGB'\n\n# configure OpenAI VAE for float16s\n\nif isinstance(vae, OpenAIDiscreteVAE) and args.fp16:\n    vae.enc.blocks.output.conv.use_float16 = True\n\n# helpers\n\ndef group_weight(model):\n    group_decay, group_no_decay = [], []\n    for params in model.named_parameters():\n        if 'transformer' in params[0]:\n            if 'bias' in params[0] or 'norm' in params[0]:\n                group_no_decay.append(params[1])\n                continue\n        group_decay.append(params[1])\n\n    assert len(list(model.parameters())) == len(group_decay) + len(group_no_decay)\n    groups = [dict(params=group_decay), dict(params=group_no_decay, weight_decay=.0)]\n    return groups\n\n\n# create dataset and dataloader\n\nis_shuffle = not distributed_utils.using_backend(distributed_utils.HorovodBackend)\n\nimagepreproc = T.Compose([\n    T.Lambda(lambda img: img.convert(IMAGE_MODE)\n    if img.mode != IMAGE_MODE else img),\n    T.RandomResizedCrop(IMAGE_SIZE,\n                        scale=(args.resize_ratio, 1.),\n                        ratio=(1., 1.)),\n    T.ToTensor(),\n])\n\ndef imagetransform(b):\n    return Image.open(BytesIO(b))\n\ndef tokenize(s):\n    return tokenizer.tokenize(\n        s.decode('utf-8'),\n        TEXT_SEQ_LEN,\n        truncate_text=args.truncate_captions).squeeze(0)\n\nif ENABLE_WEBDATASET:\n    DATASET_SIZE = int(1e9) # You need to set a nominal length for the Dataset in order to avoid warnings from DataLoader\n\n    myimg, mycap = WEBDATASET_IMAGE_TEXT_COLUMNS\n    image_text_mapping = {\n        myimg: imagetransform,\n        mycap: tokenize\n    }\n    image_mapping = {\n        myimg: imagepreproc\n    }\n\n    def filter_dataset(item): # For e.g. C@H which (rarely) has no caption available.\n        if mycap not in item:\n            return False\n        if myimg not in item:\n            return False\n        return True\n\n    w_dataset = wds.WebDataset(DATASET, handler=wds.warn_and_continue)\n    filtered_dataset = w_dataset.select(filter_dataset)\n    ds = filtered_dataset.map_dict(**image_text_mapping).map_dict(**image_mapping).to_tuple(mycap, myimg).batched(BATCH_SIZE / distr_backend.get_world_size(), partial=True)\nelse:\n    ds = TextImageDataset(\n        args.image_text_folder,\n        text_len=TEXT_SEQ_LEN,\n        image_size=IMAGE_SIZE,\n        transparent=TRANSPARENT,\n        resize_ratio=args.resize_ratio,\n        truncate_captions=args.truncate_captions,\n        tokenizer=tokenizer,\n        shuffle=is_shuffle,\n    )\n    assert len(ds) > 0, 'dataset is empty'\n\nif is_root:\n    if not ENABLE_WEBDATASET:\n        print(f'{len(ds)} image-text pairs found for training')\n\n# data sampler\n\ndata_sampler = None\n\nif not is_shuffle:\n    data_sampler = torch.utils.data.distributed.DistributedSampler(\n        ds,\n        num_replicas=distr_backend.get_world_size(),\n        rank=distr_backend.get_rank()\n    )\n\n# WebLoader for WebDataset and DeepSpeed compatibility\n\nif ENABLE_WEBDATASET:\n    dl = wds.WebLoader(ds, batch_size=None, shuffle=False, num_workers=4) # optionally add num_workers=2 (n) argument\n    number_of_batches = DATASET_SIZE // (BATCH_SIZE * distr_backend.get_world_size())\n    dl = dl.slice(number_of_batches)\n    dl.length = number_of_batches\nelse:\n    # Regular DataLoader for image-text-folder datasets\n    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=is_shuffle, drop_last=True, sampler=data_sampler)\n\n# initialize DALL-E\n\ndalle = DALLE(vae=vae, **dalle_params)\n\nif not using_deepspeed:\n    if args.fp16:\n        dalle = dalle.half()\n    dalle = dalle.cuda()\n\nif RESUME and not using_deepspeed:\n    dalle.load_state_dict(weights)\n\n# optimizer\n\nopt = Adam(get_trainable_params(dalle), lr=LEARNING_RATE)\n\nif RESUME and opt_state:\n    opt.load_state_dict(opt_state)\n\n# scheduler\n\nscheduler = None\n\nif LR_DECAY:\n    scheduler = ReduceLROnPlateau(\n        opt,\n        mode=\"min\",\n        factor=0.5,\n        patience=10,\n        cooldown=10,\n        min_lr=1e-6,\n        verbose=True,\n    )\n    if RESUME and scheduler_state:\n        scheduler.load_state_dict(scheduler_state)\n\n# experiment tracker\n\nif is_root:\n\n    model_config = dict(\n        depth=DEPTH,\n        heads=HEADS,\n        dim_head=DIM_HEAD\n    )\n\n    run = wandb.init(\n        project=args.wandb_name,\n        entity=args.wandb_entity,\n        resume=False,\n        config=model_config,\n    )\n\n# distribute\n\ndistr_backend.check_batch_size(BATCH_SIZE)\ndeepspeed_config = {\n    'train_batch_size': BATCH_SIZE,\n    'gradient_accumulation_steps': args.ga_steps,\n    'gradient_clipping': GRAD_CLIP_NORM,\n    'fp16': {\n        'enabled': args.fp16,\n    },\n    'amp': {\n        'enabled': args.amp,\n        'opt_level': 'O1',\n    },\n    \"flops_profiler\": {\n        \"enabled\": args.flops_profiler,\n        \"profile_step\": 200,\n        \"module_depth\": -1,\n        \"top_modules\": 1,\n        \"detailed\": True,\n        \"output_file\": None # TODO Can't get this to work.\n    },\n}\n\nif deepspeed_config.get('zero_optimization', {}).get('stage', 0) >= 2:\n    print(f\"Checkpoints made with DeepSpeed ZeRO Stages 2 and 3 will be stored in deepspeed checkpoint folder\")\n    print(f\"As such, they will require DeepSpeed as a dependency in order to resume from or generate with.\")\n    print(\"See the deespeed conversion script for details on how to convert your ZeRO stage 2/3 checkpoint to a single file.\")\n    print(\"If using a single GPU, consider running with apex automatic mixed precision instead for a similar speedup to ZeRO.\")\n    time.sleep(2)\n\n(distr_dalle, distr_opt, distr_dl, distr_scheduler) = distr_backend.distribute(\n    args=args,\n    model=dalle,\n    optimizer=opt,\n    model_parameters=get_trainable_params(dalle),\n    training_data=(\n        (None if ENABLE_WEBDATASET else ds)\n        if using_deepspeed\n        else dl\n    ),\n    # Do not pass the LR scheduler to DeepSpeed so we can manually\n    # advance it.\n    lr_scheduler=scheduler if LR_DECAY and not using_deepspeed else None,\n    config_params=deepspeed_config,\n)\n# Prefer scheduler in `deepspeed_config`.\n\nif LR_DECAY and distr_scheduler is None:\n    distr_scheduler = scheduler\n\navoid_model_calls = using_deepspeed and args.fp16\n\nif RESUME and using_deepspeed:\n    distr_dalle.load_checkpoint(str(cp_dir))\n\n\ndef save_model(path, epoch=0):\n    save_obj = {\n        'hparams': dalle_params,\n        'vae_params': vae_params,\n        'epoch': epoch,\n        'version': __version__,\n        'vae_class_name': vae.__class__.__name__\n    }\n\n    if using_deepspeed:\n        cp_dir = cp_path_to_dir(path, 'ds')\n\n        if KEEP_N_CHECKPOINTS is not None and is_root:\n            checkpoints = sorted(glob(str(cp_dir / \"global*\")), key=os.path.getmtime, reverse=True)\n            for checkpoint in checkpoints[KEEP_N_CHECKPOINTS:]:\n                shutil.rmtree(checkpoint)\n\n        distr_dalle.save_checkpoint(cp_dir, client_state=save_obj)\n\n        if not is_root:\n            return\n\n        # Save auxiliary values so we can reuse the standard routine\n        # for loading.\n        save_obj = {\n            **save_obj,\n            # Save a nonsense value that directs the user to\n            # further help.\n            'weights': (\n                'To get a working standard checkpoint, '\n                'look into consolidating DeepSpeed checkpoints.'\n            ),\n        }\n        torch.save(save_obj, str(cp_dir / DEEPSPEED_CP_AUX_FILENAME))\n        if deepspeed_config.get('zero_optimization', {}).get('stage', 0) >= 2: # see https://github.com/lucidrains/DALLE-pytorch/wiki/DeepSpeed-Checkpoints\n            return\n\n    if not is_root:\n        return\n\n    save_obj = {\n        **save_obj,\n        'weights': dalle.state_dict(),\n        'opt_state': opt.state_dict(),\n        'scheduler_state': (scheduler.state_dict() if scheduler else None)\n    }\n\n    torch.save(save_obj, path)\n\ndef save_artifact(model_config, model_path, name = 'trained-dalle'):\n    model_artifact = wandb.Artifact(name, type='model', metadata=dict(model_config))\n    model_artifact.add_file(model_path)\n    run.log_artifact(model_artifact)\n\n# training\n\n# Saves a checkpoint before training begins to fail early when mis-configured.\n# See https://github.com/lucidrains/DALLE-pytorch/wiki/DeepSpeed-Checkpoints\n\nsave_model(DALLE_OUTPUT_FILE_NAME, epoch=resume_epoch)\n\nfor epoch in range(resume_epoch, EPOCHS):\n    if data_sampler:\n        data_sampler.set_epoch(epoch)\n\n    for i, (text, images) in enumerate((dl if ENABLE_WEBDATASET else distr_dl)):\n        if i % 10 == 0 and is_root:\n            t = time.time()\n\n        if args.fp16:\n            images = images.half()\n\n        text, images = map(lambda t: t.cuda(), (text, images))\n\n        loss = distr_dalle(text, images, return_loss=True)\n\n        if using_deepspeed:\n            distr_dalle.backward(loss)\n            distr_dalle.step()\n            # Gradients are automatically zeroed after the step\n        else:\n            loss.backward()\n            clip_grad_norm_(distr_dalle.parameters(), GRAD_CLIP_NORM)\n            distr_opt.step()\n            distr_opt.zero_grad()\n\n        # Collective loss, averaged\n        avg_loss = distr_backend.average_all(loss)\n\n        log = {}\n\n        if i % 10 == 0 and is_root:\n            print(epoch, i, f'loss - {avg_loss.item()}')\n\n            log = {\n                **log,\n                'epoch': epoch,\n                'iter': i,\n                'loss': avg_loss.item()\n            }\n\n        if i % SAVE_EVERY_N_STEPS == 0:\n            save_model(DALLE_OUTPUT_FILE_NAME, epoch=epoch)\n\n        if i % 100 == 0 and is_root:\n            sample_text = text[:1]\n            token_list = sample_text.masked_select(sample_text != 0).tolist()\n            decoded_text = tokenizer.decode(token_list)\n\n            if not avoid_model_calls:\n                # CUDA index errors when we don't guard this\n                image = dalle.generate_images(text[:1], filter_thres=0.9)  # topk sampling at 0.9\n\n            if not avoid_model_calls:\n                log['image'] = wandb.Image(image, caption=decoded_text)\n\n        if i % 10 == 9 and is_root:\n            sample_per_sec = BATCH_SIZE * 10 / (time.time() - t)\n            log[\"sample_per_sec\"] = sample_per_sec\n            print(epoch, i, f'sample_per_sec - {sample_per_sec}')\n\n        if i == 201 and args.flops_profiler:\n            raise StopIteration(\"Profiler has finished running. Stopping training early.\")\n\n        if is_root:\n            wandb.log(log)\n\n    if LR_DECAY:\n        distr_scheduler.step(avg_loss)\n\n    save_model(DALLE_OUTPUT_FILE_NAME, epoch=epoch)\n\n    if is_root:\n        # save trained model to wandb as an artifact every epoch's end\n        save_artifact(model_config, DALLE_OUTPUT_FILE_NAME)\n\nsave_model(DALLE_OUTPUT_FILE_NAME, epoch=epoch)\n\nif is_root:\n    wandb.save(DALLE_OUTPUT_FILE_NAME)\n    save_artifact(model_config, DALLE_OUTPUT_FILE_NAME)\n    wandb.finish()\n"
        },
        {
          "name": "train_vae.py",
          "type": "blob",
          "size": 9.4990234375,
          "content": "import math\nfrom math import sqrt\nimport argparse\nfrom pathlib import Path\n\n# torch\n\nimport torch\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ExponentialLR\n\n# vision imports\n\nfrom torchvision import transforms as T\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid, save_image\n\n# dalle classes and utils\n\nfrom dalle_pytorch import distributed_utils\nfrom dalle_pytorch import DiscreteVAE\n\n# argument parsing\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument('--image_folder', type = str, required = True,\n                    help='path to your folder of images for learning the discrete VAE and its codebook')\n\nparser.add_argument('--image_size', type = int, required = False, default = 128,\n                    help='image size')\n\nparser = distributed_utils.wrap_arg_parser(parser)\n\n\ntrain_group = parser.add_argument_group('Training settings')\n\ntrain_group.add_argument('--epochs', type = int, default = 20, help = 'number of epochs')\n\ntrain_group.add_argument('--batch_size', type = int, default = 8, help = 'batch size')\n\ntrain_group.add_argument('--learning_rate', type = float, default = 1e-3, help = 'learning rate')\n\ntrain_group.add_argument('--lr_decay_rate', type = float, default = 0.98, help = 'learning rate decay')\n\ntrain_group.add_argument('--starting_temp', type = float, default = 1., help = 'starting temperature')\n\ntrain_group.add_argument('--temp_min', type = float, default = 0.5, help = 'minimum temperature to anneal to')\n\ntrain_group.add_argument('--anneal_rate', type = float, default = 1e-6, help = 'temperature annealing rate')\n\ntrain_group.add_argument('--num_images_save', type = int, default = 4, help = 'number of images to save')\n\nmodel_group = parser.add_argument_group('Model settings')\n\nmodel_group.add_argument('--num_tokens', type = int, default = 8192, help = 'number of image tokens')\n\nmodel_group.add_argument('--num_layers', type = int, default = 3, help = 'number of layers (should be 3 or above)')\n\nmodel_group.add_argument('--num_resnet_blocks', type = int, default = 2, help = 'number of residual net blocks')\n\nmodel_group.add_argument('--smooth_l1_loss', dest = 'smooth_l1_loss', action = 'store_true')\n\nmodel_group.add_argument('--emb_dim', type = int, default = 512, help = 'embedding dimension')\n\nmodel_group.add_argument('--hidden_dim', type = int, default = 256, help = 'hidden dimension')\n\nmodel_group.add_argument('--kl_loss_weight', type = float, default = 0., help = 'KL loss weight')\n\nmodel_group.add_argument('--transparent', dest = 'transparent', action = 'store_true')\n\nargs = parser.parse_args()\n\n# constants\n\nIMAGE_SIZE = args.image_size\nIMAGE_PATH = args.image_folder\n\nEPOCHS = args.epochs\nBATCH_SIZE = args.batch_size\nLEARNING_RATE = args.learning_rate\nLR_DECAY_RATE = args.lr_decay_rate\n\nNUM_TOKENS = args.num_tokens\nNUM_LAYERS = args.num_layers\nNUM_RESNET_BLOCKS = args.num_resnet_blocks\nSMOOTH_L1_LOSS = args.smooth_l1_loss\nEMB_DIM = args.emb_dim\nHIDDEN_DIM = args.hidden_dim\nKL_LOSS_WEIGHT = args.kl_loss_weight\n\nTRANSPARENT = args.transparent\nCHANNELS = 4 if TRANSPARENT else 3\nIMAGE_MODE = 'RGBA' if TRANSPARENT else 'RGB'\n\nSTARTING_TEMP = args.starting_temp\nTEMP_MIN = args.temp_min\nANNEAL_RATE = args.anneal_rate\n\nNUM_IMAGES_SAVE = args.num_images_save\n\n# initialize distributed backend\n\ndistr_backend = distributed_utils.set_backend_from_args(args)\ndistr_backend.initialize()\n\nusing_deepspeed = \\\n    distributed_utils.using_backend(distributed_utils.DeepSpeedBackend)\n\n# data\n\nds = ImageFolder(\n    IMAGE_PATH,\n    T.Compose([\n        T.Lambda(lambda img: img.convert(IMAGE_MODE) if img.mode != IMAGE_MODE else img),\n        T.Resize(IMAGE_SIZE),\n        T.CenterCrop(IMAGE_SIZE),\n        T.ToTensor()\n    ])\n)\n\nif distributed_utils.using_backend(distributed_utils.HorovodBackend):\n    data_sampler = torch.utils.data.distributed.DistributedSampler(\n        ds, num_replicas=distr_backend.get_world_size(),\n        rank=distr_backend.get_rank())\nelse:\n    data_sampler = None\n\ndl = DataLoader(ds, BATCH_SIZE, shuffle = not data_sampler, sampler=data_sampler)\n\nvae_params = dict(\n    image_size = IMAGE_SIZE,\n    num_layers = NUM_LAYERS,\n    num_tokens = NUM_TOKENS,\n    channels = CHANNELS,\n    codebook_dim = EMB_DIM,\n    hidden_dim   = HIDDEN_DIM,\n    num_resnet_blocks = NUM_RESNET_BLOCKS\n)\n\nvae = DiscreteVAE(\n    **vae_params,\n    smooth_l1_loss = SMOOTH_L1_LOSS,\n    kl_div_loss_weight = KL_LOSS_WEIGHT\n)\nif not using_deepspeed:\n    vae = vae.cuda()\n\n\nassert len(ds) > 0, 'folder does not contain any images'\nif distr_backend.is_root_worker():\n    print(f'{len(ds)} images found for training')\n\n# optimizer\n\nopt = Adam(vae.parameters(), lr = LEARNING_RATE)\nsched = ExponentialLR(optimizer = opt, gamma = LR_DECAY_RATE)\n\n\nif distr_backend.is_root_worker():\n    # weights & biases experiment tracking\n\n    import wandb\n\n    model_config = dict(\n        num_tokens = NUM_TOKENS,\n        smooth_l1_loss = SMOOTH_L1_LOSS,\n        num_resnet_blocks = NUM_RESNET_BLOCKS,\n        kl_loss_weight = KL_LOSS_WEIGHT\n    )\n\n    run = wandb.init(\n        project = 'dalle_train_vae',\n        job_type = 'train_model',\n        config = model_config\n    )\n\n# distribute\n\ndistr_backend.check_batch_size(BATCH_SIZE)\ndeepspeed_config = {'train_batch_size': BATCH_SIZE}\n\n(distr_vae, distr_opt, distr_dl, distr_sched) = distr_backend.distribute(\n    args=args,\n    model=vae,\n    optimizer=opt,\n    model_parameters=vae.parameters(),\n    training_data=ds if using_deepspeed else dl,\n    lr_scheduler=sched if not using_deepspeed else None,\n    config_params=deepspeed_config,\n)\n\nusing_deepspeed_sched = False\n# Prefer scheduler in `deepspeed_config`.\nif distr_sched is None:\n    distr_sched = sched\nelif using_deepspeed:\n    # We are using a DeepSpeed LR scheduler and want to let DeepSpeed\n    # handle its scheduling.\n    using_deepspeed_sched = True\n\ndef save_model(path):\n    save_obj = {\n        'hparams': vae_params,\n    }\n    if using_deepspeed:\n        cp_path = Path(path)\n        path_sans_extension = cp_path.parent / cp_path.stem\n        cp_dir = str(path_sans_extension) + '-ds-cp'\n\n        distr_vae.save_checkpoint(cp_dir, client_state=save_obj)\n        # We do not return so we do get a \"normal\" checkpoint to refer to.\n\n    if not distr_backend.is_root_worker():\n        return\n\n    save_obj = {\n        **save_obj,\n        'weights': vae.state_dict()\n    }\n\n    torch.save(save_obj, path)\n\n# starting temperature\n\nglobal_step = 0\ntemp = STARTING_TEMP\n\nfor epoch in range(EPOCHS):\n    for i, (images, _) in enumerate(distr_dl):\n        images = images.cuda()\n\n        loss, recons = distr_vae(\n            images,\n            return_loss = True,\n            return_recons = True,\n            temp = temp\n        )\n\n        if using_deepspeed:\n            # Gradients are automatically zeroed after the step\n            distr_vae.backward(loss)\n            distr_vae.step()\n        else:\n            distr_opt.zero_grad()\n            loss.backward()\n            distr_opt.step()\n\n        logs = {}\n\n        if i % 100 == 0:\n            if distr_backend.is_root_worker():\n                k = NUM_IMAGES_SAVE\n\n                with torch.no_grad():\n                    codes = vae.get_codebook_indices(images[:k])\n                    hard_recons = vae.decode(codes)\n\n                images, recons = map(lambda t: t[:k], (images, recons))\n                images, recons, hard_recons, codes = map(lambda t: t.detach().cpu(), (images, recons, hard_recons, codes))\n                images, recons, hard_recons = map(lambda t: make_grid(t.float(), nrow = int(sqrt(k)), normalize = True, range = (-1, 1)), (images, recons, hard_recons))\n\n                logs = {\n                    **logs,\n                    'sample images':        wandb.Image(images, caption = 'original images'),\n                    'reconstructions':      wandb.Image(recons, caption = 'reconstructions'),\n                    'hard reconstructions': wandb.Image(hard_recons, caption = 'hard reconstructions'),\n                    'codebook_indices':     wandb.Histogram(codes),\n                    'temperature':          temp\n                }\n\n                wandb.save('./vae.pt')\n            save_model(f'./vae.pt')\n\n            # temperature anneal\n\n            temp = max(temp * math.exp(-ANNEAL_RATE * global_step), TEMP_MIN)\n\n            # lr decay\n\n            # Do not advance schedulers from `deepspeed_config`.\n            if not using_deepspeed_sched:\n                distr_sched.step()\n\n        # Collective loss, averaged\n        avg_loss = distr_backend.average_all(loss)\n\n        if distr_backend.is_root_worker():\n            if i % 10 == 0:\n                lr = distr_sched.get_last_lr()[0]\n                print(epoch, i, f'lr - {lr:6f} loss - {avg_loss.item()}')\n\n                logs = {\n                    **logs,\n                    'epoch': epoch,\n                    'iter': i,\n                    'loss': avg_loss.item(),\n                    'lr': lr\n                }\n\n            wandb.log(logs)\n        global_step += 1\n\n    if distr_backend.is_root_worker():\n        # save trained model to wandb as an artifact every epoch's end\n\n        model_artifact = wandb.Artifact('trained-vae', type = 'model', metadata = dict(model_config))\n        model_artifact.add_file('vae.pt')\n        run.log_artifact(model_artifact)\n\nif distr_backend.is_root_worker():\n    # save final vae and cleanup\n\n    save_model('./vae-final.pt')\n    wandb.save('./vae-final.pt')\n\n    model_artifact = wandb.Artifact('trained-vae', type = 'model', metadata = dict(model_config))\n    model_artifact.add_file('vae-final.pt')\n    run.log_artifact(model_artifact)\n\n    wandb.finish()\n"
        }
      ]
    }
  ]
}