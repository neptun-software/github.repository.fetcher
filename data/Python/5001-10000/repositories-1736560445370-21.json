{
  "metadata": {
    "timestamp": 1736560445370,
    "page": 21,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ymcui/Chinese-BERT-wwm",
      "stars": 9782,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0576171875,
          "content": "* linguist-language=python\n*.md linguist-language=Markdown\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.021484375,
          "content": ".DS_Store\n*/.DS_Store\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 31.298828125,
          "content": "# [Chinese-LLaMA-Alpaca-2 v1.0版本](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)已正式发布！\n\n[**中文说明**](https://github.com/ymcui/Chinese-BERT-wwm/) | [**English**](https://github.com/ymcui/Chinese-BERT-wwm/blob/master/README_EN.md)\n\n<p align=\"center\">\n    <br>\n    <img src=\"./pics/banner.png\" width=\"500\"/>\n    <br>\n</p>\n<p align=\"center\">\n    <a href=\"https://github.com/ymcui/Chinese-BERT-wwm/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/ymcui/Chinese-BERT-wwm.svg?color=blue&style=flat-square\">\n    </a>\n</p>\n\n在自然语言处理领域中，预训练语言模型（Pre-trained Language Models）已成为非常重要的基础技术。为了进一步促进中文信息处理的研究发展，我们发布了基于全词掩码（Whole Word Masking）技术的中文预训练模型BERT-wwm，以及与此技术密切相关的模型：BERT-wwm-ext，RoBERTa-wwm-ext，RoBERTa-wwm-ext-large, RBT3, RBTL3等。  \n\n- **[Pre-Training with Whole Word Masking for Chinese BERT](https://ieeexplore.ieee.org/document/9599397)**  \n- *Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang*\n- Published in *IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)*\n\n本项目基于谷歌官方BERT：https://github.com/google-research/bert\n\n----\n\n[中文LERT](https://github.com/ymcui/LERT) | [中英文PERT](https://github.com/ymcui/PERT) | [中文MacBERT](https://github.com/ymcui/MacBERT) | [中文ELECTRA](https://github.com/ymcui/Chinese-ELECTRA) | [中文XLNet](https://github.com/ymcui/Chinese-XLNet) | [中文BERT](https://github.com/ymcui/Chinese-BERT-wwm) | [知识蒸馏工具TextBrewer](https://github.com/airaria/TextBrewer) | [模型裁剪工具TextPruner](https://github.com/airaria/TextPruner)\n\n查看更多哈工大讯飞联合实验室（HFL）发布的资源：https://github.com/ymcui/HFL-Anthology\n\n## 新闻\n**2023/3/28 开源了中文LLaMA&Alpaca大模型，可快速在PC上部署体验，查看：https://github.com/ymcui/Chinese-LLaMA-Alpaca**\n\n2023/3/9 我们提出了一种图文多模态预训练模型VLE，查看：https://github.com/iflytek/VLE \n\n2022/11/15 我们提出了中文小型预训练模型MiniRBT。查看：https://github.com/iflytek/MiniRBT\n\n2022/10/29 我们提出了一种融合语言学信息的预训练模型LERT。查看：https://github.com/ymcui/LERT\n\n2022/3/30 我们开源了一种新预训练模型PERT。查看：https://github.com/ymcui/PERT\n\n<details>\n<summary>历史新闻</summary>\n2021/12/17 哈工大讯飞联合实验室推出模型裁剪工具包TextPruner。查看：https://github.com/airaria/TextPruner\n\n2021/10/24 哈工大讯飞联合实验室发布面向少数民族语言的预训练模型CINO。查看：https://github.com/ymcui/Chinese-Minority-PLM\n\n2021/7/21 由哈工大SCIR多位学者撰写的[《自然语言处理：基于预训练模型的方法》](https://item.jd.com/13344628.html)已出版，欢迎大家选购。\n\n2021/1/27 所有模型已支持TensorFlow 2，请通过transformers库进行调用或下载。https://huggingface.co/hfl\n\n2020/9/15 我们的论文[\"Revisiting Pre-Trained Models for Chinese Natural Language Processing\"](https://arxiv.org/abs/2004.13922)被[Findings of EMNLP](https://2020.emnlp.org)录用为长文。\n\n2020/8/27 哈工大讯飞联合实验室在通用自然语言理解评测GLUE中荣登榜首，查看[GLUE榜单](https://gluebenchmark.com/leaderboard)，[新闻](http://dwz.date/ckrD)。\n\n2020/3/23 本目录发布的模型已接入[飞桨PaddleHub](https://github.com/PaddlePaddle/PaddleHub)，查看[快速加载](#快速加载)\n\n2020/3/11 为了更好地了解需求，邀请您填写[调查问卷](https://wj.qq.com/s2/5637766/6281)，以便为大家提供更好的资源。\n\n2020/2/26 哈工大讯飞联合实验室发布[知识蒸馏工具TextBrewer](https://github.com/airaria/TextBrewer)\n\n2020/1/20 祝大家鼠年大吉，本次发布了RBT3、RBTL3（3层RoBERTa-wwm-ext-base/large），查看[小参数量模型](#小参数量模型)\n\n2019/12/19 本目录发布的模型已接入[Huggingface-Transformers](https://github.com/huggingface/transformers)，查看[快速加载](#快速加载)\n\n2019/10/14 发布萝卜塔RoBERTa-wwm-ext-large模型，查看[中文模型下载](#中文模型下载)\n\n2019/9/10 发布萝卜塔RoBERTa-wwm-ext模型，查看[中文模型下载](#中文模型下载)\n\n2019/7/30 提供了在更大通用语料（5.4B词数）上训练的中文`BERT-wwm-ext`模型，查看[中文模型下载](#中文模型下载)\n\n2019/6/20 初始版本，模型已可通过谷歌下载，国内云盘也已上传完毕，查看[中文模型下载](#中文模型下载)\n</details>\n\n## 内容导引\n| 章节 | 描述 |\n|-|-|\n| [简介](#简介) | 介绍BERT-wwm基本原理 |\n| [中文模型下载](#中文模型下载) | 提供了BERT-wwm的下载地址 |\n| [快速加载](#快速加载) | 介绍了如何使用[🤗Transformers](https://github.com/huggingface/transformers)、[PaddleHub](https://github.com/PaddlePaddle/PaddleHub)快速加载模型 |\n| [模型对比](#模型对比) | 提供了本目录中模型的参数对比 |\n| [中文基线系统效果](#中文基线系统效果) | 列举了部分中文基线系统效果 |\n| [小参数量模型](#小参数量模型) | 列举了小参数量模型（3层Transformer）的效果 |\n| [使用建议](#使用建议) | 提供了若干使用中文预训练模型的建议 |\n| [英文模型下载](#英文模型下载) | 谷歌官方的英文BERT-wwm下载地址 |\n| [FAQ](#FAQ) | 常见问题答疑 |\n| [引用](#引用) | 本目录的技术报告 |\n\n\n## 简介\n**Whole Word Masking (wwm)**，暂翻译为`全词Mask`或`整词Mask`，是谷歌在2019年5月31日发布的一项BERT的升级版本，主要更改了原预训练阶段的训练样本生成策略。\n简单来说，原有基于WordPiece的分词方式会把一个完整的词切分成若干个子词，在生成训练样本时，这些被分开的子词会随机被mask。\n在`全词Mask`中，如果一个完整的词的部分WordPiece子词被mask，则同属该词的其他部分也会被mask，即`全词Mask`。\n\n**需要注意的是，这里的mask指的是广义的mask（替换成[MASK]；保持原词汇；随机替换成另外一个词），并非只局限于单词替换成`[MASK]`标签的情况。\n更详细的说明及样例请参考：[#4](https://github.com/ymcui/Chinese-BERT-wwm/issues/4)**\n\n同理，由于谷歌官方发布的`BERT-base, Chinese`中，中文是以**字**为粒度进行切分，没有考虑到传统NLP中的中文分词（CWS）。\n我们将全词Mask的方法应用在了中文中，使用了中文维基百科（包括简体和繁体）进行训练，并且使用了[哈工大LTP](http://ltp.ai)作为分词工具，即对组成同一个**词**的汉字全部进行Mask。\n\n下述文本展示了`全词Mask`的生成样例。\n**注意：为了方便理解，下述例子中只考虑替换成[MASK]标签的情况。**\n\n| 说明 | 样例 |\n| :------- | :--------- |\n| 原始文本 | 使用语言模型来预测下一个词的probability。 |\n| 分词文本 | 使用 语言 模型 来 预测 下 一个 词 的 probability 。 |\n| 原始Mask输入 | 使 用 语 言 [MASK] 型 来 [MASK] 测 下 一 个 词 的 pro [MASK] ##lity 。 |\n| 全词Mask输入 | 使 用 语 言 [MASK] [MASK] 来 [MASK] [MASK] 下 一 个 词 的 [MASK] [MASK] [MASK] 。 |\n\n\n## 中文模型下载\n本目录中主要包含base模型，故我们不在模型简称中标注`base`字样。对于其他大小的模型会标注对应的标记（例如large）。\n\n* **`BERT-large模型`**：24-layer, 1024-hidden, 16-heads, 330M parameters  \n* **`BERT-base模型`**：12-layer, 768-hidden, 12-heads, 110M parameters  \n\n**注意：开源版本不包含MLM任务的权重；如需做MLM任务，请使用额外数据进行二次预训练（和其他下游任务一样）。**\n\n| 模型简称 | 语料 | Google下载 | 百度网盘下载 |\n| :------- | :--------- | :---------: | :---------: |\n| **`RBT6, Chinese`** | **EXT数据<sup>[1]</sup>** | - | **[TensorFlow（密码hniy）](https://pan.baidu.com/s/1_MDAIYIGVgDovWkSs51NDA?pwd=hniy)** |\n| **`RBT4, Chinese`** | **EXT数据<sup>[1]</sup>** | - | **[TensorFlow（密码sjpt）](https://pan.baidu.com/s/1MUrmuTULnMn3L1aw_dXxSA?pwd=sjpt)** |\n| **`RBTL3, Chinese`** | **EXT数据<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1Jzn1hYwmv0kXkfTeIvNT61Rn1IbRc-o8)**<br/>**[PyTorch](https://drive.google.com/open?id=1qs5OasLXXjOnR2XuGUh12NanUl0pkjEv)** | **[TensorFlow（密码s6cu）](https://pan.baidu.com/s/1vV9ClBMbsSpt8wUpfQz62Q?pwd=s6cu)** |\n| **`RBT3, Chinese`** | **EXT数据<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1-rvV0nBDvRCASbRz8M9Decc3_8Aw-2yi)**<br/>**[PyTorch](https://drive.google.com/open?id=1_LqmIxm8Nz1Abvlqb8QFZaxYo-TInOed)** | **[TensorFlow（密码5a57）](https://pan.baidu.com/s/1AnapwWj1YBZ_4E6AAtj2lg?pwd=5a57)** |\n| **`RoBERTa-wwm-ext-large, Chinese`** | **EXT数据<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1dtad0FFzG11CBsawu8hvwwzU2R0FDI94)**<br/>**[PyTorch](https://drive.google.com/open?id=1-2vEZfIFCdM1-vJ3GD6DlSyKT4eVXMKq)** | **[TensorFlow（密码dqqe）](https://pan.baidu.com/s/1F68xzCLWEonTEVP7HQ0Ciw?pwd=dqqe)** |\n| **`RoBERTa-wwm-ext, Chinese`** | **EXT数据<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1jMAKIJmPn7kADgD3yQZhpsqM-IRM1qZt)** <br/>**[PyTorch](https://drive.google.com/open?id=1eHM3l4fMo6DsQYGmey7UZGiTmQquHw25)** | **[TensorFlow（密码vybq）](https://pan.baidu.com/s/1oR0cgSXE3Nz6dESxr98qVA?pwd=vybq)** |\n| **`BERT-wwm-ext, Chinese`** | **EXT数据<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1buMLEjdtrXE2c4G1rpsNGWEx7lUQ0RHi)** <br/>**[PyTorch](https://drive.google.com/open?id=1iNeYFhCBJWeUsIlnW_2K6SMwXkM4gLb_)** | **[TensorFlow（密码wgnt）](https://pan.baidu.com/s/1x-jIw1X2yNYHGak2yiq4RQ?pwd=wgnt)** |\n| **`BERT-wwm, Chinese`** | **中文维基** | **[TensorFlow](https://drive.google.com/open?id=1RoTQsXp2hkQ1gSRVylRIJfQxJUgkfJMW)** <br/>**[PyTorch](https://drive.google.com/open?id=1AQitrjbvCWc51SYiLN-cJq4e0WiNN4KY)** | **[TensorFlow（密码qfh8）](https://pan.baidu.com/s/1HDdDXiYxGT5ub5OeO7qdWw?pwd=qfh8)** |\n| `BERT-base, Chinese`<sup>Google</sup> | 中文维基 | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) | - |\n| `BERT-base, Multilingual Cased`<sup>Google</sup>  | 多语种维基 | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip) | - |\n| `BERT-base, Multilingual Uncased`<sup>Google</sup>  | 多语种维基 | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip) | - |\n\n> [1] EXT数据包括：中文维基百科，其他百科、新闻、问答等数据，总词数达5.4B。\n\n### PyTorch版本\n\n如需PyTorch版本，\n\n1）请自行通过[🤗Transformers](https://github.com/huggingface/transformers)提供的转换脚本进行转换。\n\n2）或者通过huggingface官网直接下载PyTorch版权重：https://huggingface.co/hfl\n\n下载方法：点击任意需要下载的模型 → 选择\"Files and versions\"选项卡 → 下载对应的模型文件。\n\n### 使用说明\n\n中国大陆境内建议使用百度网盘下载点，境外用户建议使用谷歌下载点，base模型文件大小约**400M**。 \n以TensorFlow版`BERT-wwm, Chinese`为例，下载完毕后对zip文件进行解压得到：\n\n```\nchinese_wwm_L-12_H-768_A-12.zip\n    |- bert_model.ckpt      # 模型权重\n    |- bert_model.meta      # 模型meta信息\n    |- bert_model.index     # 模型index信息\n    |- bert_config.json     # 模型参数\n    |- vocab.txt            # 词表\n```\n其中`bert_config.json`和`vocab.txt`与谷歌原版`BERT-base, Chinese`完全一致。\nPyTorch版本则包含`pytorch_model.bin`, `bert_config.json`, `vocab.txt`文件。\n\n\n## 快速加载\n### 使用Huggingface-Transformers\n\n依托于[🤗transformers库](https://github.com/huggingface/transformers)，可轻松调用以上模型。\n```\ntokenizer = BertTokenizer.from_pretrained(\"MODEL_NAME\")\nmodel = BertModel.from_pretrained(\"MODEL_NAME\")\n```\n**注意：本目录中的所有模型均使用BertTokenizer以及BertModel加载，请勿使用RobertaTokenizer/RobertaModel！**\n\n其中`MODEL_NAME`对应列表如下：\n\n| 模型名 | MODEL_NAME |\n| - | - |\n| RoBERTa-wwm-ext-large | hfl/chinese-roberta-wwm-ext-large |\n| RoBERTa-wwm-ext | hfl/chinese-roberta-wwm-ext |\n| BERT-wwm-ext | hfl/chinese-bert-wwm-ext |\n| BERT-wwm | hfl/chinese-bert-wwm |\n| RBT3 | hfl/rbt3 |\n| RBTL3 | hfl/rbtl3 |\n\n### 使用PaddleHub\n\n依托[PaddleHub](https://github.com/PaddlePaddle/PaddleHub)，只需一行代码即可完成模型下载安装，十余行代码即可完成文本分类、序列标注、阅读理解等任务。\n\n```\nimport paddlehub as hub\nmodule = hub.Module(name=MODULE_NAME)\n```\n\n其中`MODULE_NAME`对应列表如下：\n\n| 模型名 | MODULE_NAME |\n| - | - |\n| RoBERTa-wwm-ext-large | [chinese-roberta-wwm-ext-large](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-roberta-wwm-ext-large&en_category=SemanticModel) |\n| RoBERTa-wwm-ext       | [chinese-roberta-wwm-ext](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-roberta-wwm-ext&en_category=SemanticModel) |\n| BERT-wwm-ext          | [chinese-bert-wwm-ext](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-bert-wwm-ext&en_category=SemanticModel) |\n| BERT-wwm              | [chinese-bert-wwm](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-bert-wwm&en_category=SemanticModel) |\n| RBT3                  | [rbt3](https://www.paddlepaddle.org.cn/hubdetail?name=rbt3&en_category=SemanticModel) |\n| RBTL3                 | [rbtl3](https://www.paddlepaddle.org.cn/hubdetail?name=rbtl3&en_category=SemanticModel) |\n\n\n## 模型对比\n针对大家比较关心的一些模型细节进行汇总如下。\n\n| - | BERT<sup>Google</sup> | BERT-wwm | BERT-wwm-ext | RoBERTa-wwm-ext | RoBERTa-wwm-ext-large |\n| :------- | :---------: | :---------: | :---------: | :---------: | :---------: |\n| Masking | WordPiece | WWM<sup>[1]</sup> | WWM | WWM | WWM |\n| Type | base | base | base | base | **large** |\n| Data Source | wiki | wiki | wiki+ext<sup>[2]</sup> | wiki+ext | wiki+ext |\n| Training Tokens # | 0.4B | 0.4B | 5.4B | 5.4B | 5.4B |\n| Device | TPU Pod v2 | TPU v3 | TPU v3 | TPU v3 | **TPU Pod v3-32<sup>[3]</sup>** |\n| Training Steps | ? | 100K<sup>MAX128</sup> <br/>+100K<sup>MAX512</sup> | 1M<sup>MAX128</sup> <br/>+400K<sup>MAX512</sup> | 1M<sup>MAX512</sup> | 2M<sup>MAX512</sup> |\n| Batch Size | ? | 2,560 / 384 | 2,560 / 384 | 384 | 512 |\n| Optimizer | AdamW | LAMB | LAMB | AdamW | AdamW |\n| Vocabulary | 21,128 | ~BERT<sup>[4]</sup> | ~BERT | ~BERT | ~BERT |\n| Init Checkpoint | Random Init | ~BERT | ~BERT | ~BERT | Random Init |\n\n> [1] WWM = Whole Word Masking  \n> [2] ext = extended data  \n> [3] TPU Pod v3-32 (512G HBM)等价于4个TPU v3 (128G HBM)  \n> [4] `~BERT`表示**继承**谷歌原版中文BERT的属性  \n\n\n## 中文基线系统效果\n为了对比基线效果，我们在以下几个中文数据集上进行了测试，包括`句子级`和`篇章级`任务。\n对于`BERT-wwm-ext`、`RoBERTa-wwm-ext`、`RoBERTa-wwm-ext-large`，我们**没有进一步调整最佳学习率**，而是直接使用了`BERT-wwm`的最佳学习率。\n\n最佳学习率：  \n\n| 模型 | BERT | ERNIE | BERT-wwm* |\n| :------- | :---------: | :---------: | :---------: |\n| CMRC 2018 | 3e-5 | 8e-5 | 3e-5 |\n| DRCD | 3e-5 | 8e-5 | 3e-5 |\n| CJRC | 4e-5 | 8e-5 | 4e-5 |\n| XNLI | 3e-5 | 5e-5 | 3e-5 |\n| ChnSentiCorp | 2e-5 | 5e-5 | 2e-5 |\n| LCQMC  | 2e-5 | 3e-5 | 2e-5 |\n| BQ Corpus | 3e-5 | 5e-5 | 3e-5 |\n| THUCNews | 2e-5 | 5e-5 | 2e-5 |\n\n*代表所有wwm系列模型 (BERT-wwm, BERT-wwm-ext, RoBERTa-wwm-ext, RoBERTa-wwm-ext-large)\n\n\n**下面仅列举部分结果，完整结果请查看我们的[技术报告](https://arxiv.org/abs/1906.08101)。**\n\n- [**CMRC 2018**：篇章片段抽取型阅读理解（简体中文）](https://github.com/ymcui/cmrc2018)\n- [**DRCD**：篇章片段抽取型阅读理解（繁体中文）](https://github.com/DRCSolutionService/DRCD)\n- [**CJRC**: 法律阅读理解（简体中文）](http://cail.cipsc.org.cn)\n- [**XNLI**：自然语言推断](https://github.com/google-research/bert/blob/master/multilingual.md)\n- [**ChnSentiCorp**：情感分析](https://github.com/pengming617/bert_classification)\n- [**LCQMC**：句对匹配](http://icrc.hitsz.edu.cn/info/1037/1146.htm)\n- [**BQ Corpus**：句对匹配](http://icrc.hitsz.edu.cn/Article/show/175.html)\n- [**THUCNews**：篇章级文本分类](http://thuctc.thunlp.org)\n\n**注意：为了保证结果的可靠性，对于同一模型，我们运行10遍（不同随机种子），汇报模型性能的最大值和平均值（括号内为平均值）。不出意外，你运行的结果应该很大概率落在这个区间内。**\n\n**评测指标中，括号内表示平均值，括号外表示最大值。**\n\n\n### 简体中文阅读理解：CMRC 2018\n[**CMRC 2018数据集**](https://github.com/ymcui/cmrc2018)是哈工大讯飞联合实验室发布的中文机器阅读理解数据。\n根据给定问题，系统需要从篇章中抽取出片段作为答案，形式与SQuAD相同。\n评测指标为：EM / F1\n\n| 模型 | 开发集 | 测试集 | 挑战集 |\n| :------- | :---------: | :---------: | :---------: |\n| BERT | 65.5 (64.4) / 84.5 (84.0) | 70.0 (68.7) / 87.0 (86.3) | 18.6 (17.0) / 43.3 (41.3) |\n| ERNIE | 65.4 (64.3) / 84.7 (84.2) | 69.4 (68.2) / 86.6 (86.1) | 19.6 (17.0) / 44.3 (42.8) |\n| **BERT-wwm** | 66.3 (65.0) / 85.6 (84.7) | 70.5 (69.1) / 87.4 (86.7) | 21.0 (19.3) / 47.0 (43.9) |\n| **BERT-wwm-ext** | 67.1 (65.6) / 85.7 (85.0) | 71.4 (70.0) / 87.7 (87.0) | 24.0 (20.0) / 47.3 (44.6) |\n| **RoBERTa-wwm-ext** | 67.4 (66.5) / 87.2 (86.5) | 72.6 (71.4) / 89.4 (88.8) | 26.2 (24.6) / 51.0 (49.1) |\n| **RoBERTa-wwm-ext-large** | **68.5 (67.6) / 88.4 (87.9)** | **74.2 (72.4) / 90.6 (90.0)** | **31.5 (30.1) / 60.1 (57.5)** |\n\n\n### 繁体中文阅读理解：DRCD\n[**DRCD数据集**](https://github.com/DRCKnowledgeTeam/DRCD)由中国台湾台达研究院发布，其形式与SQuAD相同，是基于繁体中文的抽取式阅读理解数据集。\n**由于ERNIE中去除了繁体中文字符，故不建议在繁体中文数据上使用ERNIE（或转换成简体中文后再处理）。**\n评测指标为：EM / F1\n\n| 模型 | 开发集 | 测试集 |\n| :------- | :---------: | :---------: |\n| BERT | 83.1 (82.7) / 89.9 (89.6) | 82.2 (81.6) / 89.2 (88.8) |\n| ERNIE | 73.2 (73.0) / 83.9 (83.8) | 71.9 (71.4) / 82.5 (82.3) |\n| **BERT-wwm** | 84.3 (83.4) / 90.5 (90.2) | 82.8 (81.8) / 89.7 (89.0) |\n| **BERT-wwm-ext** | 85.0 (84.5) / 91.2 (90.9) | 83.6 (83.0) / 90.4 (89.9) |\n| **RoBERTa-wwm-ext** | 86.6 (85.9) / 92.5 (92.2) | 85.6 (85.2) / 92.0 (91.7) |\n| **RoBERTa-wwm-ext-large** | **89.6 (89.1) / 94.8 (94.4)** | **89.6 (88.9) / 94.5 (94.1)** |\n\n\n### 司法阅读理解：CJRC\n[**CJRC数据集**](http://cail.cipsc.org.cn)是哈工大讯飞联合实验室发布的面向**司法领域**的中文机器阅读理解数据。\n需要注意的是实验中使用的数据并非官方发布的最终数据，结果仅供参考。\n评测指标为：EM / F1\n\n| 模型 | 开发集 | 测试集 |\n| :------- | :---------: | :---------: |\n| BERT | 54.6 (54.0) / 75.4 (74.5) | 55.1 (54.1) / 75.2 (74.3) |\n| ERNIE | 54.3 (53.9) / 75.3 (74.6) | 55.0 (53.9) / 75.0 (73.9) |\n| **BERT-wwm** | 54.7 (54.0) / 75.2 (74.8) | 55.1 (54.1) / 75.4 (74.4) |\n| **BERT-wwm-ext** | 55.6 (54.8) / 76.0 (75.3) | 55.6 (54.9) / 75.8 (75.0) |\n| **RoBERTa-wwm-ext** | 58.7 (57.6) / 79.1 (78.3) | 59.0 (57.8) / 79.0 (78.0) |\n| **RoBERTa-wwm-ext-large** | **62.1 (61.1) / 82.4 (81.6)** | **62.4 (61.4) / 82.2 (81.0)** |\n\n\n### 自然语言推断：XNLI\n在自然语言推断任务中，我们采用了[**XNLI**数据](https://github.com/google-research/bert/blob/master/multilingual.md)，需要将文本分成三个类别：`entailment`，`neutral`，`contradictory`。\n评测指标为：Accuracy\n\n| 模型 | 开发集 | 测试集 |\n| :------- | :---------: | :---------: |\n| BERT | 77.8 (77.4) | 77.8 (77.5) |\n| ERNIE | 79.7 (79.4) | 78.6 (78.2) |\n| **BERT-wwm** | 79.0 (78.4) | 78.2 (78.0) |\n| **BERT-wwm-ext** | 79.4 (78.6) | 78.7 (78.3) |\n| **RoBERTa-wwm-ext** | 80.0 (79.2) | 78.8 (78.3) |\n| **RoBERTa-wwm-ext-large** | **82.1 (81.3)** | **81.2 (80.6)** |\n\n\n### 情感分析：ChnSentiCorp\n在情感分析任务中，二分类的情感分类数据集ChnSentiCorp。\n评测指标为：Accuracy\n\n| 模型 | 开发集 | 测试集 |\n| :------- | :---------: | :---------: |\n| BERT | 94.7 (94.3) | 95.0 (94.7) |\n| ERNIE | 95.4 (94.8) | 95.4 **(95.3)** |\n| **BERT-wwm** | 95.1 (94.5) | 95.4 (95.0) |\n| **BERT-wwm-ext** | 95.4 (94.6) | 95.3 (94.7) |\n| **RoBERTa-wwm-ext** | 95.0 (94.6) | 95.6 (94.8) |\n| **RoBERTa-wwm-ext-large** | **95.8 (94.9)** | **95.8** (94.9) |\n\n\n### 句对分类：LCQMC, BQ Corpus\n以下两个数据集均需要将一个句对进行分类，判断两个句子的语义是否相同（二分类任务）。\n\n#### LCQMC\n[LCQMC](http://icrc.hitsz.edu.cn/info/1037/1146.htm)由哈工大深圳研究生院智能计算研究中心发布。 \n评测指标为：Accuracy\n\n| 模型 | 开发集 | 测试集 |\n| :------- | :---------: | :---------: |\n| BERT | 89.4 (88.4) | 86.9 (86.4) |\n| ERNIE | 89.8 (89.6) | **87.2 (87.0)** |\n| **BERT-wwm** | 89.4 (89.2) | 87.0 (86.8) |\n| **BERT-wwm-ext** | 89.6 (89.2) | 87.1 (86.6) |\n| **RoBERTa-wwm-ext** | 89.0 (88.7) | 86.4 (86.1) |\n| **RoBERTa-wwm-ext-large** | **90.4 (90.0)** | 87.0 (86.8) |\n\n\n#### BQ Corpus \n[BQ Corpus](http://icrc.hitsz.edu.cn/Article/show/175.html)由哈工大深圳研究生院智能计算研究中心发布，是面向银行领域的数据集。\n评测指标为：Accuracy\n\n| 模型 | 开发集 | 测试集 |\n| :------- | :---------: | :---------: |\n| BERT | 86.0 (85.5) | 84.8 (84.6) |\n| ERNIE | 86.3 (85.5) | 85.0 (84.6) |\n| **BERT-wwm** | 86.1 (85.6) | 85.2 **(84.9)** |\n| **BERT-wwm-ext** | **86.4** (85.5) | 85.3 (84.8) |\n| **RoBERTa-wwm-ext** | 86.0 (85.4) | 85.0 (84.6) |\n| **RoBERTa-wwm-ext-large** | 86.3 **(85.7)** | **85.8 (84.9)** |\n\n\n### 篇章级文本分类：THUCNews\n篇章级文本分类任务我们选用了由清华大学自然语言处理实验室发布的新闻数据集**THUCNews**。\n我们采用的是其中一个子集，需要将新闻分成10个类别中的一个。\n评测指标为：Accuracy\n\n| 模型 | 开发集 | 测试集 |\n| :------- | :---------: | :---------: |\n| BERT | 97.7 (97.4) | 97.8 (97.6) |\n| ERNIE | 97.6 (97.3) | 97.5 (97.3) |\n| **BERT-wwm** | 98.0 (97.6) | 97.8 (97.6) |\n| **BERT-wwm-ext** | 97.7 (97.5) | 97.7 (97.5) |\n| **RoBERTa-wwm-ext** | 98.3 (97.9) | 97.7 (97.5) |\n| **RoBERTa-wwm-ext-large** | 98.3 (97.7) | 97.8 (97.6) |\n\n\n### 小参数量模型\n以下是在若干NLP任务上的实验效果，表中只提供测试集结果对比。\n\n| 模型 | CMRC 2018 | DRCD | XNLI | CSC | LCQMC | BQ | 平均 | 参数量 |\n| :------- | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| RoBERTa-wwm-ext-large | 74.2 / 90.6 | 89.6 / 94.5 | 81.2 | 95.8 | 87.0 | 85.8 | 87.335 | 325M |\n| RoBERTa-wwm-ext | 72.6 / 89.4 | 85.6 / 92.0 | 78.8 | 95.6 | 86.4 | 85.0 | 85.675 | 102M |\n| RBTL3 | 63.3 / 83.4 | 77.2 / 85.6 | 74.0 | 94.2 | 85.1 | 83.6 | 80.800 | 61M (59.8%) |\n| RBT3 | 62.2 / 81.8 | 75.0 / 83.9 | 72.3 | 92.8 | 85.1 | 83.3 | 79.550 | 38M (37.3%) |\n\n效果相对值比较：\n\n| 模型 | CMRC 2018 | DRCD | XNLI | CSC | LCQMC | BQ | 平均 | 分类平均 |\n| :------- | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| RoBERTa-wwm-ext-large | 102.2% / 101.3% | 104.7% / 102.7% | 103.0% | 100.2% | 100.7% | 100.9% | 101.9% | 101.2% |\n| RoBERTa-wwm-ext | 100% / 100% | 100% / 100% | 100% | 100% | 100% | 100% | 100% | 100% |\n| RBTL3 | 87.2% / 93.3% | 90.2% / 93.0% | 93.9% | 98.5% | 98.5% | 98.4% | 94.3% | 97.35% |\n| RBT3 | 85.7% / 91.5% | 87.6% / 91.2% | 91.8% | 97.1% | 98.5% | 98.0% | 92.9% | 96.35% |\n\n- 参数量是以XNLI分类任务为基准进行计算\n- 括号内参数量百分比以原始base模型（即RoBERTa-wwm-ext）为基准\n- RBT3：由RoBERTa-wwm-ext 3层进行初始化，继续训练了1M步\n- RBTL3：由RoBERTa-wwm-ext-large 3层进行初始化，继续训练了1M步\n- RBT的名字是RoBERTa三个音节首字母组成，L代表large模型\n- 直接使用RoBERTa-wwm-ext-large前三层进行初始化并进行下游任务的训练将显著降低效果，例如在CMRC 2018上测试集仅能达到42.9/65.3，而RBTL3能达到63.3/83.4\n\n欢迎使用效果更优的中文小型预训练模型MiniRBT：https://github.com/iflytek/MiniRBT\n\n## 使用建议\n* 初始学习率是非常重要的一个参数（不论是`BERT`还是其他模型），需要根据目标任务进行调整。\n* `ERNIE`的最佳学习率和`BERT`/`BERT-wwm`相差较大，所以使用`ERNIE`时请务必调整学习率（基于以上实验结果，`ERNIE`需要的初始学习率较高）。\n* 由于`BERT`/`BERT-wwm`使用了维基百科数据进行训练，故它们对正式文本建模较好；而`ERNIE`使用了额外的百度贴吧、知道等网络数据，它对非正式文本（例如微博等）建模有优势。\n* 在长文本建模任务上，例如阅读理解、文档分类，`BERT`和`BERT-wwm`的效果较好。\n* 如果目标任务的数据和预训练模型的领域相差较大，请在自己的数据集上进一步做预训练。\n* 如果要处理繁体中文数据，请使用`BERT`或者`BERT-wwm`。因为我们发现`ERNIE`的词表中几乎没有繁体中文。\n\n\n## 英文模型下载\n为了方便大家下载，顺便带上**谷歌官方发布**的英文`BERT-large (wwm)`模型：\n\n*   **[`BERT-Large, Uncased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n\n*   **[`BERT-Large, Cased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n\n## FAQ\n**Q: 这个模型怎么用？**  \nA: 谷歌发布的中文BERT怎么用，这个就怎么用。\n**文本不需要经过分词，wwm只影响预训练过程，不影响下游任务的输入。**\n\n**Q: 请问有预训练代码提供吗？**  \nA: 很遗憾，我不能提供相关代码，实现可以参考 [#10](https://github.com/ymcui/Chinese-BERT-wwm/issues/10) 和 [#13](https://github.com/ymcui/Chinese-BERT-wwm/issues/13)。\n\n**Q: 某某数据集在哪里下载？**  \nA: 请查看`data`目录，任务目录下的`README.md`标明了数据来源。对于有版权的内容，请自行搜索或与原作者联系获取数据。\n\n**Q: 会有计划发布更大模型吗？比如BERT-large-wwm版本？**  \nA: 如果我们从实验中得到更好效果，会考虑发布更大的版本。\n\n**Q: 你骗人！无法复现结果😂**  \nA: 在下游任务中，我们采用了最简单的模型。比如分类任务，我们直接使用的是`run_classifier.py`（谷歌提供）。\n如果无法达到平均值，说明实验本身存在bug，请仔细排查。\n最高值存在很多随机因素，我们无法保证能够达到最高值。\n另外一个公认的因素：降低batch size会显著降低实验效果，具体可参考BERT，XLNet目录的相关Issue。\n\n**Q: 我训出来比你更好的结果！**  \nA: 恭喜你。\n\n**Q: 训练花了多长时间，在什么设备上训练的？**  \nA: 训练是在谷歌TPU v3版本（128G HBM）完成的，训练BERT-wwm花费约1.5天，BERT-wwm-ext则需要数周时间（使用了更多数据需要迭代更充分）。\n需要注意的是，预训练阶段我们使用的是`LAMB Optimizer`（[TensorFlow版本实现](https://github.com/ymcui/LAMB_Optimizer_TF)）。该优化器对大的batch有良好的支持。\n在微调下游任务时，我们采用的是BERT默认的`AdamWeightDecayOptimizer`。\n\n**Q: ERNIE是谁？**  \nA: 本项目中的ERNIE模型特指百度公司提出的[ERNIE](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE)，而非清华大学在ACL 2019上发表的[ERNIE](https://github.com/thunlp/ERNIE)。\n\n**Q: BERT-wwm的效果不是在所有任务都很好**  \nA: 本项目的目的是为研究者提供多元化的预训练模型，自由选择BERT，ERNIE，或者是BERT-wwm。\n我们仅提供实验数据，具体效果如何还是得在自己的任务中不断尝试才能得出结论。\n多一个模型，多一种选择。\n\n**Q: 为什么有些数据集上没有试？**  \nA: 很坦率的说：\n1）没精力找更多的数据；\n2）没有必要； \n3）没有钞票；\n\n**Q: 简单评价一下这几个模型**  \nA: 各有侧重，各有千秋。\n中文自然语言处理的研究发展需要多方共同努力。\n\n**Q: 你预测下一个预训练模型叫什么？**  \nA: 可能叫ZOE吧，ZOE: Zero-shOt Embeddings from language model\n\n**Q: 更多关于`RoBERTa-wwm-ext`模型的细节？**  \nA: 我们集成了RoBERTa和BERT-wwm的优点，对两者进行了一个自然的结合。\n和之前本目录中的模型之间的区别如下:  \n1）预训练阶段采用wwm策略进行mask（但没有使用dynamic masking）  \n2）简单取消Next Sentence Prediction（NSP）loss  \n3）不再采用先max_len=128然后再max_len=512的训练模式，直接训练max_len=512  \n4）训练步数适当延长  \n\n需要注意的是，该模型并非原版RoBERTa模型，只是按照类似RoBERTa训练方式训练出的BERT模型，即RoBERTa-like BERT。\n故在下游任务使用、模型转换时请按BERT的方式处理，而非RoBERTa。\n\n\n## 引用\n如果本项目中的资源或技术对你的研究工作有所帮助，欢迎在论文中引用下述论文。\n- 首选（期刊扩充版）：https://ieeexplore.ieee.org/document/9599397\n```\n@journal{cui-etal-2021-pretrain,\n  title={Pre-Training with Whole Word Masking for Chinese BERT},\n  author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing},\n  journal={IEEE Transactions on Audio, Speech and Language Processing},\n  year={2021},\n  url={https://ieeexplore.ieee.org/document/9599397},\n  doi={10.1109/TASLP.2021.3124365},\n }\n```\n\n- 或者（会议版本）：https://www.aclweb.org/anthology/2020.findings-emnlp.58\n```\n@inproceedings{cui-etal-2020-revisiting,\n    title = \"Revisiting Pre-Trained Models for {C}hinese Natural Language Processing\",\n    author = \"Cui, Yiming  and\n      Che, Wanxiang  and\n      Liu, Ting  and\n      Qin, Bing  and\n      Wang, Shijin  and\n      Hu, Guoping\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.findings-emnlp.58\",\n    pages = \"657--668\",\n}\n```\n\n\n## 致谢\n第一作者部分受到[**谷歌TPU Research Cloud**](https://www.tensorflow.org/tfrc)计划资助。\n\n\n## 免责声明\n**本项目并非谷歌官方发布的Chinese BERT-wwm模型。同时，本项目不是哈工大或科大讯飞的官方产品。**\n技术报告中所呈现的实验结果仅表明在特定数据集和超参组合下的表现，并不能代表各个模型的本质。\n实验结果可能因随机数种子，计算设备而发生改变。\n**该项目中的内容仅供技术研究参考，不作为任何结论性依据。使用者可以在许可证范围内任意使用该模型，但我们不对因使用该项目内容造成的直接或间接损失负责。**\n\n\n## 关注我们\n欢迎关注哈工大讯飞联合实验室官方微信公众号，了解最新的技术动态。\n\n![qrcode.png](https://github.com/ymcui/cmrc2019/raw/master/qrcode.jpg)\n\n\n## 问题反馈\n如有问题，请在GitHub Issue中提交。\n\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 28.4755859375,
          "content": "[**中文说明**](https://github.com/ymcui/Chinese-BERT-wwm/) | [**English**](https://github.com/ymcui/Chinese-BERT-wwm/blob/master/README_EN.md)\n\n## Chinese BERT with Whole Word Masking\nFor further accelerating Chinese natural language processing, we provide **Chinese pre-trained BERT with Whole Word Masking**. Meanwhile, we also compare the state-of-the-art Chinese pre-trained models in depth, including [BERT](https://github.com/google-research/bert)、[ERNIE](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE)、[BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm).\n\n- **[Pre-Training with Whole Word Masking for Chinese BERT](https://ieeexplore.ieee.org/document/9599397)**  \n- Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang\n- Published in *IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)*\n\nThis repository is developed based on：https://github.com/google-research/bert\n\n----\n\n[Chinese LERT](https://github.com/ymcui/LERT) | [Chinese/English PERT](https://github.com/ymcui/PERT) [Chinese MacBERT](https://github.com/ymcui/MacBERT) | [Chinese ELECTRA](https://github.com/ymcui/Chinese-ELECTRA) | [Chinese XLNet](https://github.com/ymcui/Chinese-XLNet) | [Chinese BERT](https://github.com/ymcui/Chinese-BERT-wwm) | [TextBrewer](https://github.com/airaria/TextBrewer) | [TextPruner](https://github.com/airaria/TextPruner)\n\nMore resources by HFL: https://github.com/ymcui/HFL-Anthology\n\n## News\n**Mar 28, 2023 We open-sourced Chinese LLaMA&Alpaca LLMs, which can be quickly deployed on PC. Check: https://github.com/ymcui/Chinese-LLaMA-Alpaca**\n\n2022/10/29 We release a new pre-trained model called LERT, check https://github.com/ymcui/LERT/\n\n2022/3/30 We release a new pre-trained model called PERT, check https://github.com/ymcui/PERT \n\n2021/12/17 We release a model pruning toolkit - TextPruner, check https://github.com/airaria/TextPruner\n\n2021/1/27 All models support TensorFlow 2 now. Please use transformers library to access them or download from https://huggingface.co/hfl\n\n2020/9/15 Our paper [\"Revisiting Pre-Trained Models for Chinese Natural Language Processing\"](https://arxiv.org/abs/2004.13922) is accepted to [Findings of EMNLP](https://2020.emnlp.org) as a long paper.\n\n2020/8/27 We are happy to announce that our model is on top of GLUE benchmark, check [leaderboard](https://gluebenchmark.com/leaderboard).\n\n<details>\n<summary>Past News</summary>\n2020/3/23 The models in this repository now can be easily accessed through [PaddleHub](https://github.com/PaddlePaddle/PaddleHub), check [Quick Load](#Quick-Load)\n\n2020/2/26 We release a knowledge distillation toolkit [TextBrewer](https://github.com/airaria/TextBrewer)\n\n2020/1/20 Happy Chinese New Year! We've released RBT3 and RBTL3 (3-layer RoBERTa-wwm-ext-base/large), check [Small Models](#Small-Models)\n\n2019/12/19 The models in this repository now can be easily accessed through [Huggingface-Transformers](https://github.com/huggingface/transformers), check [Quick Load](#Quick-Load)\n\n2019/10/14 We release `RoBERTa-wwm-ext-large`, check [Download](#Download)\n\n2019/9/10 We release `RoBERTa-wwm-ext`, check [Download](#Download)\n\n2019/7/30 We release `BERT-wwm-ext`, which was trained on larger data, check [Download](#Download)\n\n2019/6/20 Initial version, pre-trained models could be downloaded through Google Drive, check [Download](#Download)\n</details>\n\n## Guide\n| Section | Description |\n|-|-|\n| [Introduction](#Introduction) | Introduction to BERT with Whole Word Masking (WWM) |\n| [Download](#Download) | Download links for Chinese BERT-wwm |\n| [Quick Load](#Quick-Load) | Learn how to quickly load our models through [🤗Transformers](https://github.com/huggingface/transformers) or [PaddleHub](https://github.com/PaddlePaddle/PaddleHub) |\n| [Model Comparison](#Model-Comparison) | Compare the models published in this repository |\n| [Baselines](#Baselines) | Baseline results for several Chinese NLP datasets (partial) |\n| [Small Models](#Small-Models) | 3-layer Transformer models |\n| [Useful Tips](#Useful-Tips) | Provide several useful tips for using Chinese pre-trained models |\n| [English BERT-wwm](#English-BERT-wwm) | Download English BERT-wwm (by Google) |\n| [FAQ](#FAQ) | Frequently Asked Questions |\n| [Citation](#Citation) | Citation |\n\n\n## Introduction\n**Whole Word Masking (wwm)** is an upgraded version by [BERT](https://github.com/google-research/bert) released on late May 2019.\n\nThe following introductions are copied from BERT repository.\n```\nIn the original pre-processing code, we randomly select WordPiece tokens to mask. For example:\n\nInput Text: the man jumped up , put his basket on phil ##am ##mon ' s head \n\nOriginal Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head\n\nThe new technique is called Whole Word Masking. In this case, we always mask all of the the tokens corresponding to a word at once. The overall masking rate remains the same.\n\nWhole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head\n\nThe training is identical -- we still predict each masked WordPiece token independently. The improvement comes from the fact that the original prediction task was too 'easy' for words that had been split into multiple WordPieces.\n\n```\n\n**Important Note: Terminology `Masking` does not ONLY represent replace a word into `[MASK]` token.\nIt could also be in another form, such as `keep original word` or `randomly replaced by another word`.**\n\nIn the Chinese language, it is straightforward to utilize whole word masking, as traditional text processing in Chinese should include `Chinese Word Segmentation (CWS)`.\nIn the original `BERT-base, Chinese` by Google, the segmentation is done by splitting the Chinese characters while neglecting the importance of CWS.\nIn this repository, we utilize [Language Technology Platform (LTP)](http://ltp.ai) by Harbin Institute of Technology for CWS, and adapt whole word masking in Chinese text.\n\n\n## Download\nAs all models are 'BERT-base' variants, we do not incidate 'base' in the following model names.\n\n* **`BERT-base`**：12-layer, 768-hidden, 12-heads, 110M parameters\n\n| Model | Data | Google Drive | iFLYTEK Cloud |\n| :------- | :--------- | :---------: | :---------: |\n| **`RBT6, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | - | **[TensorFlow（pw:hniy）](https://pan.baidu.com/s/1_MDAIYIGVgDovWkSs51NDA?pwd=hniy)** |\n| **`RBT4, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | - | **[TensorFlow（pw:sjpt）](https://pan.baidu.com/s/1MUrmuTULnMn3L1aw_dXxSA?pwd=sjpt)** |\n| **`RBTL3, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1Jzn1hYwmv0kXkfTeIvNT61Rn1IbRc-o8)**<br/>**[PyTorch](https://drive.google.com/open?id=1qs5OasLXXjOnR2XuGUh12NanUl0pkjEv)** | **[TensorFlow（pw:s6cu）](https://pan.baidu.com/s/1vV9ClBMbsSpt8wUpfQz62Q?pwd=s6cu)** |\n| **`RBT3, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1-rvV0nBDvRCASbRz8M9Decc3_8Aw-2yi)**<br/>**[PyTorch](https://drive.google.com/open?id=1_LqmIxm8Nz1Abvlqb8QFZaxYo-TInOed)** | **[TensorFlow（pw:5a57）](https://pan.baidu.com/s/1AnapwWj1YBZ_4E6AAtj2lg?pwd=5a57)** |\n| **`RoBERTa-wwm-ext-large, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1dtad0FFzG11CBsawu8hvwwzU2R0FDI94)**<br/>**[PyTorch](https://drive.google.com/open?id=1-2vEZfIFCdM1-vJ3GD6DlSyKT4eVXMKq)** | **[TensorFlow（pw:dqqe）](https://pan.baidu.com/s/1F68xzCLWEonTEVP7HQ0Ciw?pwd=dqqe)** |\n| **`RoBERTa-wwm-ext, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1jMAKIJmPn7kADgD3yQZhpsqM-IRM1qZt)** <br/>**[PyTorch](https://drive.google.com/open?id=1eHM3l4fMo6DsQYGmey7UZGiTmQquHw25)** | **[TensorFlow（pw:vybq）](https://pan.baidu.com/s/1oR0cgSXE3Nz6dESxr98qVA?pwd=vybq)** |\n| **`BERT-wwm-ext, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1buMLEjdtrXE2c4G1rpsNGWEx7lUQ0RHi)** <br/>**[PyTorch](https://drive.google.com/open?id=1iNeYFhCBJWeUsIlnW_2K6SMwXkM4gLb_)** | **[TensorFlow（pw:wgnt）](https://pan.baidu.com/s/1x-jIw1X2yNYHGak2yiq4RQ?pwd=wgnt)** |\n| **`BERT-wwm, Chinese`** | **Wikipedia** | **[TensorFlow](https://drive.google.com/open?id=1RoTQsXp2hkQ1gSRVylRIJfQxJUgkfJMW)** <br/>**[PyTorch](https://drive.google.com/open?id=1AQitrjbvCWc51SYiLN-cJq4e0WiNN4KY)** | **[TensorFlow（pw:qfh8）](https://pan.baidu.com/s/1HDdDXiYxGT5ub5OeO7qdWw?pwd=qfh8)** |\n| `BERT-base, Chinese`<sup>Google</sup> | Wikipedia | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) | - |\n| `BERT-base, Multilingual Cased`<sup>Google</sup>  | Wikipedia | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip) | - |\n| `BERT-base, Multilingual Uncased`<sup>Google</sup>  | Wikipedia | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip) | - |\n\n### PyTorch Version\n\nIf you need these models in PyTorch,\n\n1) Convert TensorFlow checkpoint into PyTorch, using [🤗Transformers](https://github.com/huggingface/transformers)\n\n2) Download from https://huggingface.co/hfl\n\nSteps: select one of the model in the page above → click \"list all files in model\" at the end of the model page → download bin/json files from the pop-up window\n\n### Note\n\nThe whole zip package roughly takes ~400M.\nZIP package includes the following files:\n\n```\nchinese_wwm_L-12_H-768_A-12.zip\n    |- bert_model.ckpt      # Model Weights\n    |- bert_model.meta      # Meta info\n    |- bert_model.index     # Index info\n    |- bert_config.json     # Config file\n    |- vocab.txt            # Vocabulary\n```\n\n`bert_config.json` and `vocab.txt` are identical to the original **`BERT-base, Chinese`** by Google。\n\n\n## Quick Load\n### Huggingface-Transformers\n\nWith [Huggingface-Transformers](https://github.com/huggingface/transformers), the models above could be easily accessed and loaded through the following codes.\n```\ntokenizer = BertTokenizer.from_pretrained(\"MODEL_NAME\")\nmodel = BertModel.from_pretrained(\"MODEL_NAME\")\n```\n**Notice: Please use BertTokenizer and BertModel for loading these model. DO NOT use RobertaTokenizer/RobertaModel!**\n\nThe actual model and its `MODEL_NAME` are listed below.\n\n| Original Model | MODEL_NAME |\n| - | - |\n| RoBERTa-wwm-ext-large | hfl/chinese-roberta-wwm-ext-large |\n| RoBERTa-wwm-ext | hfl/chinese-roberta-wwm-ext |\n| BERT-wwm-ext | hfl/chinese-bert-wwm-ext |\n| BERT-wwm | hfl/chinese-bert-wwm |\n| RBT3 | hfl/rbt3 |\n| RBTL3 | hfl/rbtl3 |\n\n### PaddleHub\n\nWith [PaddleHub](https://github.com/PaddlePaddle/PaddleHub), we can download and install the model with one line of code.\n\n```\nimport paddlehub as hub\nmodule = hub.Module(name=MODULE_NAME)\n```\n\nThe actual model and its `MODULE_NAME` are listed below.\n\n| Original Model | MODULE_NAME |\n| - | - |\n| RoBERTa-wwm-ext-large | [chinese-roberta-wwm-ext-large](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-roberta-wwm-ext-large&en_category=SemanticModel) |\n| RoBERTa-wwm-ext       | [chinese-roberta-wwm-ext](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-roberta-wwm-ext&en_category=SemanticModel) |\n| BERT-wwm-ext          | [chinese-bert-wwm-ext](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-bert-wwm-ext&en_category=SemanticModel) |\n| BERT-wwm              | [chinese-bert-wwm](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-bert-wwm&en_category=SemanticModel) |\n| RBT3                  | [rbt3](https://www.paddlepaddle.org.cn/hubdetail?name=rbt3&en_category=SemanticModel) |\n| RBTL3                 | [rbtl3](https://www.paddlepaddle.org.cn/hubdetail?name=rbtl3&en_category=SemanticModel) |\n\n\n## Model Comparison\nWe list comparisons on the models that were released in this project.\n`~BERT` means to inherit the attributes from original Google's BERT.\n\n| - | BERT<sup>Google</sup> | BERT-wwm | BERT-wwm-ext | RoBERTa-wwm-ext | RoBERTa-wwm-ext-large |\n| :------- | :---------: | :---------: | :---------: | :---------: | :---------: |\n| Masking | WordPiece | WWM<sup>[1]</sup> | WWM | WWM | WWM |\n| Type | BERT-base | BERT-base | BERT-base | BERT-base | **BERT-large** |\n| Data Source | wiki | wiki | wiki+ext<sup>[2]</sup> | wiki+ext | wiki+ext |\n| Training Tokens # | 0.4B | 0.4B | 5.4B | 5.4B | 5.4B |\n| Device | TPU Pod v2 | TPU v3 | TPU v3 | TPU v3 | **TPU Pod v3-32<sup>[3]</sup>** |\n| Training Steps | ? | 100K<sup>MAX128</sup> <br/>+100K<sup>MAX512</sup> | 1M<sup>MAX128</sup> <br/>+400K<sup>MAX512</sup> | 1M<sup>MAX512</sup> | 2M<sup>MAX512</sup> |\n| Batch Size | ? | 2,560 / 384 | 2,560 / 384 | 384 | 512 |\n| Optimizer | AdamW | LAMB | LAMB | AdamW | AdamW |\n| Vocabulary | 21,128 | ~BERT<sup>[4]</sup> vocab | ~BERT vocab | ~BERT vocab | ~BERT vocab |\n| Init Checkpoint | Random Init | ~BERT weight | ~BERT weight | ~BERT weight | Random Init |\n\n\n## Baselines\nWe experiment on several Chinese datasets, including sentence-level to document-level tasks.\n\n**We only list partial results here and kindly advise the readers to read our [technical report](https://arxiv.org/abs/1906.08101).**\n\nBest Learning Rate:  \n\n| Model | BERT | ERNIE | BERT-wwm* |\n| :------- | :---------: | :---------: | :---------: |\n| CMRC 2018 | 3e-5 | 8e-5 | 3e-5 |\n| DRCD | 3e-5 | 8e-5 | 3e-5 |\n| CJRC | 4e-5 | 8e-5 | 4e-5 |\n| XNLI | 3e-5 | 5e-5 | 3e-5 |\n| ChnSentiCorp | 2e-5 | 5e-5 | 2e-5 |\n| LCQMC  | 2e-5 | 3e-5 | 2e-5 |\n| BQ Corpus | 3e-5 | 5e-5 | 3e-5 |\n| THUCNews | 2e-5 | 5e-5 | 2e-5 |\n* represents all related models (BERT-wwm, BERT-wwm-ext, RoBERTa-wwm-ext, RoBERTa-wwm-ext-large)\n\n\n- [**CMRC 2018**：Span-Extraction Machine Reading Comprehension (Simplified Chinese)](https://github.com/ymcui/cmrc2018)\n- [**DRCD**：Span-Extraction Machine Reading Comprehension (Traditional Chinese)](https://github.com/DRCSolutionService/DRCD)\n- [**CJRC**: Chinese Judiciary Reading Comprehension](http://cail.cipsc.org.cn)\n- [**XNLI**：Natural Langauge Inference](https://github.com/google-research/bert/blob/master/multilingual.md)\n- [**ChnSentiCorp**：Sentiment Analysis](https://github.com/pengming617/bert_classification)\n- [**LCQMC**：Sentence Pair Matching](http://icrc.hitsz.edu.cn/info/1037/1146.htm)\n- [**BQ Corpus**：Sentence Pair Matching](http://icrc.hitsz.edu.cn/Article/show/175.html)\n- [**THUCNews**：Document-level Text Classification](http://thuctc.thunlp.org)\n\n**Note: To ensure the stability of the results, we run 10 times for each experiment and report maximum and average scores.**\n\n**Average scores are in brackets, and max performances are the numbers that out of brackets.**\n\n### [CMRC 2018](https://github.com/ymcui/cmrc2018)\nCMRC 2018 dataset is released by Joint Laboratory of HIT and iFLYTEK Research.\nThe model should answer the questions based on the given passage, which is identical to SQuAD.\nEvaluation Metrics: EM / F1\n\n| Model | Development | Test | Challenge |\n| :------- | :---------: | :---------: | :---------: |\n| BERT | 65.5 (64.4) / 84.5 (84.0) | 70.0 (68.7) / 87.0 (86.3) | 18.6 (17.0) / 43.3 (41.3) |\n| ERNIE | 65.4 (64.3) / 84.7 (84.2) | 69.4 (68.2) / 86.6 (86.1) | 19.6 (17.0) / 44.3 (42.8) |\n| **BERT-wwm** | 66.3 (65.0) / 85.6 (84.7) | 70.5 (69.1) / 87.4 (86.7) | 21.0 (19.3) / 47.0 (43.9) |\n| **BERT-wwm-ext** | 67.1 (65.6) / 85.7 (85.0) | 71.4 (70.0) / 87.7 (87.0) | 24.0 (20.0) / 47.3 (44.6) |\n| **RoBERTa-wwm-ext** | 67.4 (66.5) / 87.2 (86.5) | 72.6 (71.4) / 89.4 (88.8) | 26.2 (24.6) / 51.0 (49.1) |\n| **RoBERTa-wwm-ext-large** | **68.5 (67.6) / 88.4 (87.9)** | **74.2 (72.4) / 90.6 (90.0)** | **31.5 (30.1) / 60.1 (57.5)** |\n\n\n### [DRCD](https://github.com/DRCKnowledgeTeam/DRCD)\nDRCD is also a span-extraction machine reading comprehension dataset, released by Delta Research Center. The text is written in Traditional Chinese.\nEvaluation Metrics: EM / F1\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 83.1 (82.7) / 89.9 (89.6) | 82.2 (81.6) / 89.2 (88.8) |\n| ERNIE | 73.2 (73.0) / 83.9 (83.8) | 71.9 (71.4) / 82.5 (82.3) |\n| **BERT-wwm** | 84.3 (83.4) / 90.5 (90.2) | 82.8 (81.8) / 89.7 (89.0) |\n| **BERT-wwm-ext** | 85.0 (84.5) / 91.2 (90.9) | 83.6 (83.0) / 90.4 (89.9) |\n| **RoBERTa-wwm-ext** | 86.6 (85.9) / 92.5 (92.2) | 85.6 (85.2) / 92.0 (91.7) |\n| **RoBERTa-wwm-ext-large** | **89.6 (89.1) / 94.8 (94.4)** | **89.6 (88.9) / 94.5 (94.1)** |\n\n\n### CJRC\n[**CJRC**](http://cail.cipsc.org.cn) is a Chinese judiciary reading comprehension dataset, released by Joint Laboratory of HIT and iFLYTEK Research. Note that, the data used in these experiments are NOT identical to the official one.\nEvaluation Metrics: EM / F1\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 54.6 (54.0) / 75.4 (74.5) | 55.1 (54.1) / 75.2 (74.3) |\n| ERNIE | 54.3 (53.9) / 75.3 (74.6) | 55.0 (53.9) / 75.0 (73.9) |\n| **BERT-wwm** | 54.7 (54.0) / 75.2 (74.8) | 55.1 (54.1) / 75.4 (74.4) |\n| **BERT-wwm-ext** | 55.6 (54.8) / 76.0 (75.3) | 55.6 (54.9) / 75.8 (75.0) |\n| **RoBERTa-wwm-ext** | 58.7 (57.6) / 79.1 (78.3) | 59.0 (57.8) / 79.0 (78.0) |\n| **RoBERTa-wwm-ext-large** | **62.1 (61.1) / 82.4 (81.6)** | **62.4 (61.4) / 82.2 (81.0)** |\n\n\n### XNLI\nWe use XNLI data for testing NLI task.\nEvaluation Metrics: Accuracy\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 77.8 (77.4) | 77.8 (77.5) |\n| ERNIE | 79.7 (79.4) | 78.6 (78.2) |\n| **BERT-wwm** | 79.0 (78.4) | 78.2 (78.0) |\n| **BERT-wwm-ext** | 79.4 (78.6) | 78.7 (78.3) |\n| **RoBERTa-wwm-ext** | 80.0 (79.2) | 78.8 (78.3) |\n| **RoBERTa-wwm-ext-large** | **82.1 (81.3)** | **81.2 (80.6)** |\n\n### ChnSentiCorp\nWe use ChnSentiCorp data for testing sentiment analysis.\nEvaluation Metrics: Accuracy\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 94.7 (94.3) | 95.0 (94.7) |\n| ERNIE | 95.4 (94.8) | 95.4 **(95.3)** |\n| **BERT-wwm** | 95.1 (94.5) | 95.4 (95.0) |\n| **BERT-wwm-ext** | 95.4 （94.6) | 95.3 (94.7) |\n| **RoBERTa-wwm-ext** | 95.0 (94.6) | 95.6 (94.8) |\n| **RoBERTa-wwm-ext-large** | **95.8 (94.9)** | **95.8** (94.9) |\n\n\n### Sentence Pair Matching：LCQMC, BQ Corpus\n\n#### LCQMC\nEvaluation Metrics: Accuracy\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 89.4 (88.4) | 86.9 (86.4) |\n| ERNIE | 89.8 (89.6) | **87.2 (87.0)** |\n| **BERT-wwm** | 89.4 (89.2) | 87.0 (86.8) |\n| **BERT-wwm-ext** | 89.6 (89.2) | 87.1 (86.6) |\n| **RoBERTa-wwm-ext** | 89.0 (88.7) | 86.4 (86.1) |\n| **RoBERTa-wwm-ext-large** | **90.4 (90.0)** | 87.0 (86.8) |\n\n#### BQ Corpus \nEvaluation Metrics: Accuracy\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 86.0 (85.5) | 84.8 (84.6) |\n| ERNIE | 86.3 (85.5) | 85.0 (84.6) |\n| **BERT-wwm** | 86.1 (85.6) | 85.2 **(84.9)** |\n| **BERT-wwm-ext** | **86.4** (85.5) | 85.3 (84.8) |\n| **RoBERTa-wwm-ext** | 86.0 (85.4) | 85.0 (84.6) |\n| **RoBERTa-wwm-ext-large** | 86.3 **(85.7)** | **85.8 (84.9)** |\n\n\n### THUCNews\nReleased by Tsinghua University, which contains news in 10 categories.\nEvaluation Metrics: Accuracy\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 97.7 (97.4) | 97.8 (97.6) |\n| ERNIE | 97.6 (97.3) | 97.5 (97.3) |\n| **BERT-wwm** | 98.0 (97.6) | 97.8 (97.6) |\n| **BERT-wwm-ext** | 97.7 (97.5) | 97.7 (97.5) |\n| **RoBERTa-wwm-ext** | 98.3 (97.9) | 97.7 (97.5) |\n| **RoBERTa-wwm-ext-large** | 98.3 (97.7) | 97.8 (97.6) |\n\n### Small Models\nWe list RBT3 and RBTL3 results on several NLP tasks. Note that, we only list test set results.\n\n| Model | CMRC 2018 | DRCD | XNLI | CSC | LCQMC | BQ | Average | Params |\n| :------- | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| RoBERTa-wwm-ext-large | 74.2 / 90.6 | 89.6 / 94.5 | 81.2 | 95.8 | 87.0 | 85.8 | 87.335 | 325M |\n| RoBERTa-wwm-ext | 72.6 / 89.4 | 85.6 / 92.0 | 78.8 | 95.6 | 86.4 | 85.0 | 85.675 | 102M |\n| RBTL3 | 63.3 / 83.4 | 77.2 / 85.6 | 74.0 | 94.2 | 85.1 | 83.6 | 80.800 | 61M (59.8%) |\n| RBT3 | 62.2 / 81.8 | 75.0 / 83.9 | 72.3 | 92.8 | 85.1 | 83.3 | 79.550 | 38M (37.3%) |\n\nRelative performance:\n\n| Model | CMRC 2018 | DRCD | XNLI | CSC | LCQMC | BQ | Average | AVG-C |\n| :------- | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| RoBERTa-wwm-ext-large | 102.2% / 101.3% | 104.7% / 102.7% | 103.0% | 100.2% | 100.7% | 100.9% | 101.9% | 101.2% |\n| RoBERTa-wwm-ext | 100% / 100% | 100% / 100% | 100% | 100% | 100% | 100% | 100% | 100% |\n| RBTL3 | 87.2% / 93.3% | 90.2% / 93.0% | 93.9% | 98.5% | 98.5% | 98.4% | 94.3% | 97.35% |\n| RBT3 | 85.7% / 91.5% | 87.6% / 91.2% | 91.8% | 97.1% | 98.5% | 98.0% | 92.9% | 96.35% |\n\n* AVG-C: average score of classification tasks: XNLI, CSC, LCQMC, BQ\n\n- The numbers of parameter are calculated based on XNLI classification task.\n- Relative parameter percentage is calculated based on RoBERTa-wwm-ext model.\n- RBT3: We use RoBERTa-wwm-ext for initializing the first three layers, and continue to train 1M steps.\n- RBTL3: We use RoBERTa-wwm-ext-large for initializing the first three layers, and continue to train 1M steps.\n- The name of RBT is the syllables of 'RoBERTa', and 'L' stands for large model.\n- Directly using the first three layers of RoBERTa-wwm-ext-large to fine-tune the downstream task will result in a bad performance. For example, in CMRC 2018 task we could only achieve 42.9/65.3, while RBTL3 could reach 63.3/83.4.\n\n\n## Useful Tips\n* Initial learning rate is the most important hyper-parameters (regardless of BERT or other neural networks), and should ALWAYS be tuned for better performance.\n* As shown in the experimental results, BERT and BERT-wwm share almost the same best initial learning rate, so it is straightforward to apply your initial learning rate in BERT to BERT-wwm. However, we find that ERNIE does not share the same characteristics, so it is STRONGLY recommended to tune the learning rate.\n* As BERT and BERT-wwm were trained on Wikipedia data, they show relatively better performance on the formal text. While, ERNIE was trained on larger data, including web text, which will be useful on casual text, such as Weibo (microblogs).\n* In long-sequence tasks, such as machine reading comprehension and document classification, we suggest using BERT or BERT-wwm.\n* As these pre-trained models are trained in general domains, if the task data is extremely different from the pre-training data (Wikipedia for BERT/BERT-wwm), we suggest taking another pre-training steps on the task data, which was also suggested by Devlin et al. (2019).\n* As there are so many possibilities in pre-training stage (such as initial learning rate, global training steps, warm-up steps, etc.), our implementation may not be optimal using the same pre-training data. Readers are advised to train their own model if seeking for another boost in performance. However, if it is unable to do pre-training, choose one of these pre-trained models which was trained on a similar domain to the downstream task.\n* When dealing with Traditional Chinese text, use BERT or BERT-wwm.\n\n\n## English BERT-wwm\nWe also repost English BERT-wwm (by Google official) here for your perusal.\n\n*   **[`BERT-Large, Uncased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n\n*   **[`BERT-Large, Cased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n\n\n## FAQ\n**Q: How to use this model?**  \nA: Use it as if you are using original BERT. Note that, you don't need to do CWS for your text, as wwm only change the pre-training input but not the input for down-stream tasks.\n\n**Q: Do you have any plans to release the code?**  \nA: Unfortunately, I am not be able to release the code at the moment. As implementation is quite easy, I would suggest you to read #10 and #13.\n\n**Q: How can I download XXXXX dataset?**  \nA: We only provide the data that is publically available, check `data` directory. For copyright reasons, some of the datasets are not publically available. In that case, please search on GitHub or consult original authors for accessing.\n\n**Q: How to use this model?**  \nA: Use it as if you are using original BERT. Note that, you don't need to do CWS for your text, as wwm only change the pre-training input but not the input for down-stream tasks.\n\n**Q: Do you have any plans on releasing the larger model? Say BERT-large-wwm?**  \nA: If we could get significant gains from BERT-large, we will release a larger version in the future.\n\n**Q: You lier! I can not reproduce the result! 😂**  \nA: We use the simplist models in the downstream tasks. For example, in the classification task, we directly use `run_classifier.py` by Google. If you are not able to reach the average score that we reported, then there should be some bugs in your code. As there is randomness in reaching maximum scores, there is no guarantee that you will reproduce them.\n\n**Q: I could get better performance than you!**  \nA: Congratulations!\n\n**Q: How long did it take to train such a model?**  \nA: The training was done on Google Cloud TPU v3 with 128HBM, and it roughly takes 1.5 days. Note that, in the pre-training stage, we use [`LAMB Optimizer`](https://github.com/ymcui/LAMB_Optimizer_TF) which is optimized for the larger batch. In fine-tuning downstream task, we use normal `AdamWeightDecayOptimizer` as default.\n\n**Q: Who is ERNIE?**  \nA: The [ERNIE](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE) in this repository refer to the model released by Baidu, but not the one that published by Tsinghua University which was also called [ERNIE](https://github.com/thunlp/ERNIE).\n\n**Q: BERT-wwm does not perform well on some tasks.**  \nA: The aim of this project is to provide researchers with a variety of pre-training models.\nYou are free to choose one of these models.\nWe only provide experimental results, and we strongly suggest trying these models in your own task.\nOne more model, one more choice.\n\n**Q: Why not trying on more dataset?**  \nA: To be honest: 1) no time to find more data; 2) no need; 3) no money;\n\n**Q: Say something about these models**  \nA: Each has its own emphasis and merits. Development of Chinese NLP needs joint efforts.\n\n**Q: Any comments on the name of next generation of the pre-trained model?**  \nA: Maybe ZOE: Zero-shOt Embeddings from language model\n\n**Q: Tell me a little bit more about `RoBERTa-wwm-ext`**  \nA: integrate whole word masking (wwm) into RoBERTa model, specifically:  \n1) use whole word masking (but we did not use dynamic masking)  \n2) remove Next Sentence Prediction (NSP)\n3) directly use the data generated by `max_len=512` (but not from `max_len=128` for several steps then `max_len=512`)\n4) extended training steps (1M steps)\n\n## Citation\nIf you find the technical report or resource is useful, please cite our work in your paper.\n- Primary (Journal extension): https://ieeexplore.ieee.org/document/9599397  \n```\n@journal{cui-etal-2021-pretrain,\n  title={Pre-Training with Whole Word Masking for Chinese BERT},\n  author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing},\n  journal={IEEE Transactions on Audio, Speech and Language Processing},\n  year={2021},\n  url={https://ieeexplore.ieee.org/document/9599397},\n  doi={10.1109/TASLP.2021.3124365},\n }\n```\n- Secondary (conference paper): https://www.aclweb.org/anthology/2020.findings-emnlp.58\n```\n@inproceedings{cui-etal-2020-revisiting,\n    title = \"Revisiting Pre-Trained Models for {C}hinese Natural Language Processing\",\n    author = \"Cui, Yiming  and\n      Che, Wanxiang  and\n      Liu, Ting  and\n      Qin, Bing  and\n      Wang, Shijin  and\n      Hu, Guoping\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.findings-emnlp.58\",\n    pages = \"657--668\",\n}\n```\n\n## Disclaimer\n**This is NOT a project by Google official. Also, this is NOT an official product by HIT and iFLYTEK.**\nThe experiments only represent the empirical results in certain conditions and should not be regarded as the nature of the respective models. The results may vary using different random seeds, computing devices, etc. \n**The contents in this repository are for academic research purpose, and we do not provide any conclusive remarks. Users are free to use anythings in this repository within the scope of Apache-2.0 licence. However, we are not responsible for direct or indirect losses that was caused by using the content in this project.**\n\n\n## Acknowledgement\nThe first author of this project is partially supported by [Google TensorFlow Research Cloud (TFRC) Program](https://www.tensorflow.org/tfrc).\n\n## Issues\nIf there is any problem, please submit a GitHub Issue.\n\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "pics",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}