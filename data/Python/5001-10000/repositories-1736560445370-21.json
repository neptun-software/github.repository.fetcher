{
  "metadata": {
    "timestamp": 1736560445370,
    "page": 21,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ymcui/Chinese-BERT-wwm",
      "stars": 9782,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0576171875,
          "content": "* linguist-language=python\n*.md linguist-language=Markdown\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.021484375,
          "content": ".DS_Store\n*/.DS_Store\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 31.298828125,
          "content": "# [Chinese-LLaMA-Alpaca-2 v1.0ç‰ˆæœ¬](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)å·²æ­£å¼å‘å¸ƒï¼\n\n[**ä¸­æ–‡è¯´æ˜**](https://github.com/ymcui/Chinese-BERT-wwm/) | [**English**](https://github.com/ymcui/Chinese-BERT-wwm/blob/master/README_EN.md)\n\n<p align=\"center\">\n    <br>\n    <img src=\"./pics/banner.png\" width=\"500\"/>\n    <br>\n</p>\n<p align=\"center\">\n    <a href=\"https://github.com/ymcui/Chinese-BERT-wwm/blob/master/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/ymcui/Chinese-BERT-wwm.svg?color=blue&style=flat-square\">\n    </a>\n</p>\n\nåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­ï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPre-trained Language Modelsï¼‰å·²æˆä¸ºéå¸¸é‡è¦çš„åŸºç¡€æŠ€æœ¯ã€‚ä¸ºäº†è¿›ä¸€æ­¥ä¿ƒè¿›ä¸­æ–‡ä¿¡æ¯å¤„ç†çš„ç ”ç©¶å‘å±•ï¼Œæˆ‘ä»¬å‘å¸ƒäº†åŸºäºå…¨è¯æ©ç ï¼ˆWhole Word Maskingï¼‰æŠ€æœ¯çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹BERT-wwmï¼Œä»¥åŠä¸æ­¤æŠ€æœ¯å¯†åˆ‡ç›¸å…³çš„æ¨¡å‹ï¼šBERT-wwm-extï¼ŒRoBERTa-wwm-extï¼ŒRoBERTa-wwm-ext-large, RBT3, RBTL3ç­‰ã€‚  \n\n- **[Pre-Training with Whole Word Masking for Chinese BERT](https://ieeexplore.ieee.org/document/9599397)**  \n- *Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang*\n- Published in *IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)*\n\næœ¬é¡¹ç›®åŸºäºè°·æ­Œå®˜æ–¹BERTï¼šhttps://github.com/google-research/bert\n\n----\n\n[ä¸­æ–‡LERT](https://github.com/ymcui/LERT) | [ä¸­è‹±æ–‡PERT](https://github.com/ymcui/PERT) | [ä¸­æ–‡MacBERT](https://github.com/ymcui/MacBERT) | [ä¸­æ–‡ELECTRA](https://github.com/ymcui/Chinese-ELECTRA) | [ä¸­æ–‡XLNet](https://github.com/ymcui/Chinese-XLNet) | [ä¸­æ–‡BERT](https://github.com/ymcui/Chinese-BERT-wwm) | [çŸ¥è¯†è’¸é¦å·¥å…·TextBrewer](https://github.com/airaria/TextBrewer) | [æ¨¡å‹è£å‰ªå·¥å…·TextPruner](https://github.com/airaria/TextPruner)\n\næŸ¥çœ‹æ›´å¤šå“ˆå·¥å¤§è®¯é£è”åˆå®éªŒå®¤ï¼ˆHFLï¼‰å‘å¸ƒçš„èµ„æºï¼šhttps://github.com/ymcui/HFL-Anthology\n\n## æ–°é—»\n**2023/3/28 å¼€æºäº†ä¸­æ–‡LLaMA&Alpacaå¤§æ¨¡å‹ï¼Œå¯å¿«é€Ÿåœ¨PCä¸Šéƒ¨ç½²ä½“éªŒï¼ŒæŸ¥çœ‹ï¼šhttps://github.com/ymcui/Chinese-LLaMA-Alpaca**\n\n2023/3/9 æˆ‘ä»¬æå‡ºäº†ä¸€ç§å›¾æ–‡å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹VLEï¼ŒæŸ¥çœ‹ï¼šhttps://github.com/iflytek/VLE \n\n2022/11/15 æˆ‘ä»¬æå‡ºäº†ä¸­æ–‡å°å‹é¢„è®­ç»ƒæ¨¡å‹MiniRBTã€‚æŸ¥çœ‹ï¼šhttps://github.com/iflytek/MiniRBT\n\n2022/10/29 æˆ‘ä»¬æå‡ºäº†ä¸€ç§èåˆè¯­è¨€å­¦ä¿¡æ¯çš„é¢„è®­ç»ƒæ¨¡å‹LERTã€‚æŸ¥çœ‹ï¼šhttps://github.com/ymcui/LERT\n\n2022/3/30 æˆ‘ä»¬å¼€æºäº†ä¸€ç§æ–°é¢„è®­ç»ƒæ¨¡å‹PERTã€‚æŸ¥çœ‹ï¼šhttps://github.com/ymcui/PERT\n\n<details>\n<summary>å†å²æ–°é—»</summary>\n2021/12/17 å“ˆå·¥å¤§è®¯é£è”åˆå®éªŒå®¤æ¨å‡ºæ¨¡å‹è£å‰ªå·¥å…·åŒ…TextPrunerã€‚æŸ¥çœ‹ï¼šhttps://github.com/airaria/TextPruner\n\n2021/10/24 å“ˆå·¥å¤§è®¯é£è”åˆå®éªŒå®¤å‘å¸ƒé¢å‘å°‘æ•°æ°‘æ—è¯­è¨€çš„é¢„è®­ç»ƒæ¨¡å‹CINOã€‚æŸ¥çœ‹ï¼šhttps://github.com/ymcui/Chinese-Minority-PLM\n\n2021/7/21 ç”±å“ˆå·¥å¤§SCIRå¤šä½å­¦è€…æ’°å†™çš„[ã€Šè‡ªç„¶è¯­è¨€å¤„ç†ï¼šåŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‹](https://item.jd.com/13344628.html)å·²å‡ºç‰ˆï¼Œæ¬¢è¿å¤§å®¶é€‰è´­ã€‚\n\n2021/1/27 æ‰€æœ‰æ¨¡å‹å·²æ”¯æŒTensorFlow 2ï¼Œè¯·é€šè¿‡transformersåº“è¿›è¡Œè°ƒç”¨æˆ–ä¸‹è½½ã€‚https://huggingface.co/hfl\n\n2020/9/15 æˆ‘ä»¬çš„è®ºæ–‡[\"Revisiting Pre-Trained Models for Chinese Natural Language Processing\"](https://arxiv.org/abs/2004.13922)è¢«[Findings of EMNLP](https://2020.emnlp.org)å½•ç”¨ä¸ºé•¿æ–‡ã€‚\n\n2020/8/27 å“ˆå·¥å¤§è®¯é£è”åˆå®éªŒå®¤åœ¨é€šç”¨è‡ªç„¶è¯­è¨€ç†è§£è¯„æµ‹GLUEä¸­è£ç™»æ¦œé¦–ï¼ŒæŸ¥çœ‹[GLUEæ¦œå•](https://gluebenchmark.com/leaderboard)ï¼Œ[æ–°é—»](http://dwz.date/ckrD)ã€‚\n\n2020/3/23 æœ¬ç›®å½•å‘å¸ƒçš„æ¨¡å‹å·²æ¥å…¥[é£æ¡¨PaddleHub](https://github.com/PaddlePaddle/PaddleHub)ï¼ŒæŸ¥çœ‹[å¿«é€ŸåŠ è½½](#å¿«é€ŸåŠ è½½)\n\n2020/3/11 ä¸ºäº†æ›´å¥½åœ°äº†è§£éœ€æ±‚ï¼Œé‚€è¯·æ‚¨å¡«å†™[è°ƒæŸ¥é—®å·](https://wj.qq.com/s2/5637766/6281)ï¼Œä»¥ä¾¿ä¸ºå¤§å®¶æä¾›æ›´å¥½çš„èµ„æºã€‚\n\n2020/2/26 å“ˆå·¥å¤§è®¯é£è”åˆå®éªŒå®¤å‘å¸ƒ[çŸ¥è¯†è’¸é¦å·¥å…·TextBrewer](https://github.com/airaria/TextBrewer)\n\n2020/1/20 ç¥å¤§å®¶é¼ å¹´å¤§å‰ï¼Œæœ¬æ¬¡å‘å¸ƒäº†RBT3ã€RBTL3ï¼ˆ3å±‚RoBERTa-wwm-ext-base/largeï¼‰ï¼ŒæŸ¥çœ‹[å°å‚æ•°é‡æ¨¡å‹](#å°å‚æ•°é‡æ¨¡å‹)\n\n2019/12/19 æœ¬ç›®å½•å‘å¸ƒçš„æ¨¡å‹å·²æ¥å…¥[Huggingface-Transformers](https://github.com/huggingface/transformers)ï¼ŒæŸ¥çœ‹[å¿«é€ŸåŠ è½½](#å¿«é€ŸåŠ è½½)\n\n2019/10/14 å‘å¸ƒèåœå¡”RoBERTa-wwm-ext-largeæ¨¡å‹ï¼ŒæŸ¥çœ‹[ä¸­æ–‡æ¨¡å‹ä¸‹è½½](#ä¸­æ–‡æ¨¡å‹ä¸‹è½½)\n\n2019/9/10 å‘å¸ƒèåœå¡”RoBERTa-wwm-extæ¨¡å‹ï¼ŒæŸ¥çœ‹[ä¸­æ–‡æ¨¡å‹ä¸‹è½½](#ä¸­æ–‡æ¨¡å‹ä¸‹è½½)\n\n2019/7/30 æä¾›äº†åœ¨æ›´å¤§é€šç”¨è¯­æ–™ï¼ˆ5.4Bè¯æ•°ï¼‰ä¸Šè®­ç»ƒçš„ä¸­æ–‡`BERT-wwm-ext`æ¨¡å‹ï¼ŒæŸ¥çœ‹[ä¸­æ–‡æ¨¡å‹ä¸‹è½½](#ä¸­æ–‡æ¨¡å‹ä¸‹è½½)\n\n2019/6/20 åˆå§‹ç‰ˆæœ¬ï¼Œæ¨¡å‹å·²å¯é€šè¿‡è°·æ­Œä¸‹è½½ï¼Œå›½å†…äº‘ç›˜ä¹Ÿå·²ä¸Šä¼ å®Œæ¯•ï¼ŒæŸ¥çœ‹[ä¸­æ–‡æ¨¡å‹ä¸‹è½½](#ä¸­æ–‡æ¨¡å‹ä¸‹è½½)\n</details>\n\n## å†…å®¹å¯¼å¼•\n| ç« èŠ‚ | æè¿° |\n|-|-|\n| [ç®€ä»‹](#ç®€ä»‹) | ä»‹ç»BERT-wwmåŸºæœ¬åŸç† |\n| [ä¸­æ–‡æ¨¡å‹ä¸‹è½½](#ä¸­æ–‡æ¨¡å‹ä¸‹è½½) | æä¾›äº†BERT-wwmçš„ä¸‹è½½åœ°å€ |\n| [å¿«é€ŸåŠ è½½](#å¿«é€ŸåŠ è½½) | ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨[ğŸ¤—Transformers](https://github.com/huggingface/transformers)ã€[PaddleHub](https://github.com/PaddlePaddle/PaddleHub)å¿«é€ŸåŠ è½½æ¨¡å‹ |\n| [æ¨¡å‹å¯¹æ¯”](#æ¨¡å‹å¯¹æ¯”) | æä¾›äº†æœ¬ç›®å½•ä¸­æ¨¡å‹çš„å‚æ•°å¯¹æ¯” |\n| [ä¸­æ–‡åŸºçº¿ç³»ç»Ÿæ•ˆæœ](#ä¸­æ–‡åŸºçº¿ç³»ç»Ÿæ•ˆæœ) | åˆ—ä¸¾äº†éƒ¨åˆ†ä¸­æ–‡åŸºçº¿ç³»ç»Ÿæ•ˆæœ |\n| [å°å‚æ•°é‡æ¨¡å‹](#å°å‚æ•°é‡æ¨¡å‹) | åˆ—ä¸¾äº†å°å‚æ•°é‡æ¨¡å‹ï¼ˆ3å±‚Transformerï¼‰çš„æ•ˆæœ |\n| [ä½¿ç”¨å»ºè®®](#ä½¿ç”¨å»ºè®®) | æä¾›äº†è‹¥å¹²ä½¿ç”¨ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹çš„å»ºè®® |\n| [è‹±æ–‡æ¨¡å‹ä¸‹è½½](#è‹±æ–‡æ¨¡å‹ä¸‹è½½) | è°·æ­Œå®˜æ–¹çš„è‹±æ–‡BERT-wwmä¸‹è½½åœ°å€ |\n| [FAQ](#FAQ) | å¸¸è§é—®é¢˜ç­”ç–‘ |\n| [å¼•ç”¨](#å¼•ç”¨) | æœ¬ç›®å½•çš„æŠ€æœ¯æŠ¥å‘Š |\n\n\n## ç®€ä»‹\n**Whole Word Masking (wwm)**ï¼Œæš‚ç¿»è¯‘ä¸º`å…¨è¯Mask`æˆ–`æ•´è¯Mask`ï¼Œæ˜¯è°·æ­Œåœ¨2019å¹´5æœˆ31æ—¥å‘å¸ƒçš„ä¸€é¡¹BERTçš„å‡çº§ç‰ˆæœ¬ï¼Œä¸»è¦æ›´æ”¹äº†åŸé¢„è®­ç»ƒé˜¶æ®µçš„è®­ç»ƒæ ·æœ¬ç”Ÿæˆç­–ç•¥ã€‚\nç®€å•æ¥è¯´ï¼ŒåŸæœ‰åŸºäºWordPieceçš„åˆ†è¯æ–¹å¼ä¼šæŠŠä¸€ä¸ªå®Œæ•´çš„è¯åˆ‡åˆ†æˆè‹¥å¹²ä¸ªå­è¯ï¼Œåœ¨ç”Ÿæˆè®­ç»ƒæ ·æœ¬æ—¶ï¼Œè¿™äº›è¢«åˆ†å¼€çš„å­è¯ä¼šéšæœºè¢«maskã€‚\nåœ¨`å…¨è¯Mask`ä¸­ï¼Œå¦‚æœä¸€ä¸ªå®Œæ•´çš„è¯çš„éƒ¨åˆ†WordPieceå­è¯è¢«maskï¼Œåˆ™åŒå±è¯¥è¯çš„å…¶ä»–éƒ¨åˆ†ä¹Ÿä¼šè¢«maskï¼Œå³`å…¨è¯Mask`ã€‚\n\n**éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œçš„maskæŒ‡çš„æ˜¯å¹¿ä¹‰çš„maskï¼ˆæ›¿æ¢æˆ[MASK]ï¼›ä¿æŒåŸè¯æ±‡ï¼›éšæœºæ›¿æ¢æˆå¦å¤–ä¸€ä¸ªè¯ï¼‰ï¼Œå¹¶éåªå±€é™äºå•è¯æ›¿æ¢æˆ`[MASK]`æ ‡ç­¾çš„æƒ…å†µã€‚\næ›´è¯¦ç»†çš„è¯´æ˜åŠæ ·ä¾‹è¯·å‚è€ƒï¼š[#4](https://github.com/ymcui/Chinese-BERT-wwm/issues/4)**\n\nåŒç†ï¼Œç”±äºè°·æ­Œå®˜æ–¹å‘å¸ƒçš„`BERT-base, Chinese`ä¸­ï¼Œä¸­æ–‡æ˜¯ä»¥**å­—**ä¸ºç²’åº¦è¿›è¡Œåˆ‡åˆ†ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°ä¼ ç»ŸNLPä¸­çš„ä¸­æ–‡åˆ†è¯ï¼ˆCWSï¼‰ã€‚\næˆ‘ä»¬å°†å…¨è¯Maskçš„æ–¹æ³•åº”ç”¨åœ¨äº†ä¸­æ–‡ä¸­ï¼Œä½¿ç”¨äº†ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼ˆåŒ…æ‹¬ç®€ä½“å’Œç¹ä½“ï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”ä½¿ç”¨äº†[å“ˆå·¥å¤§LTP](http://ltp.ai)ä½œä¸ºåˆ†è¯å·¥å…·ï¼Œå³å¯¹ç»„æˆåŒä¸€ä¸ª**è¯**çš„æ±‰å­—å…¨éƒ¨è¿›è¡ŒMaskã€‚\n\nä¸‹è¿°æ–‡æœ¬å±•ç¤ºäº†`å…¨è¯Mask`çš„ç”Ÿæˆæ ·ä¾‹ã€‚\n**æ³¨æ„ï¼šä¸ºäº†æ–¹ä¾¿ç†è§£ï¼Œä¸‹è¿°ä¾‹å­ä¸­åªè€ƒè™‘æ›¿æ¢æˆ[MASK]æ ‡ç­¾çš„æƒ…å†µã€‚**\n\n| è¯´æ˜ | æ ·ä¾‹ |\n| :------- | :--------- |\n| åŸå§‹æ–‡æœ¬ | ä½¿ç”¨è¯­è¨€æ¨¡å‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„probabilityã€‚ |\n| åˆ†è¯æ–‡æœ¬ | ä½¿ç”¨ è¯­è¨€ æ¨¡å‹ æ¥ é¢„æµ‹ ä¸‹ ä¸€ä¸ª è¯ çš„ probability ã€‚ |\n| åŸå§‹Maskè¾“å…¥ | ä½¿ ç”¨ è¯­ è¨€ [MASK] å‹ æ¥ [MASK] æµ‹ ä¸‹ ä¸€ ä¸ª è¯ çš„ pro [MASK] ##lity ã€‚ |\n| å…¨è¯Maskè¾“å…¥ | ä½¿ ç”¨ è¯­ è¨€ [MASK] [MASK] æ¥ [MASK] [MASK] ä¸‹ ä¸€ ä¸ª è¯ çš„ [MASK] [MASK] [MASK] ã€‚ |\n\n\n## ä¸­æ–‡æ¨¡å‹ä¸‹è½½\næœ¬ç›®å½•ä¸­ä¸»è¦åŒ…å«baseæ¨¡å‹ï¼Œæ•…æˆ‘ä»¬ä¸åœ¨æ¨¡å‹ç®€ç§°ä¸­æ ‡æ³¨`base`å­—æ ·ã€‚å¯¹äºå…¶ä»–å¤§å°çš„æ¨¡å‹ä¼šæ ‡æ³¨å¯¹åº”çš„æ ‡è®°ï¼ˆä¾‹å¦‚largeï¼‰ã€‚\n\n* **`BERT-largeæ¨¡å‹`**ï¼š24-layer, 1024-hidden, 16-heads, 330M parameters  \n* **`BERT-baseæ¨¡å‹`**ï¼š12-layer, 768-hidden, 12-heads, 110M parameters  \n\n**æ³¨æ„ï¼šå¼€æºç‰ˆæœ¬ä¸åŒ…å«MLMä»»åŠ¡çš„æƒé‡ï¼›å¦‚éœ€åšMLMä»»åŠ¡ï¼Œè¯·ä½¿ç”¨é¢å¤–æ•°æ®è¿›è¡ŒäºŒæ¬¡é¢„è®­ç»ƒï¼ˆå’Œå…¶ä»–ä¸‹æ¸¸ä»»åŠ¡ä¸€æ ·ï¼‰ã€‚**\n\n| æ¨¡å‹ç®€ç§° | è¯­æ–™ | Googleä¸‹è½½ | ç™¾åº¦ç½‘ç›˜ä¸‹è½½ |\n| :------- | :--------- | :---------: | :---------: |\n| **`RBT6, Chinese`** | **EXTæ•°æ®<sup>[1]</sup>** | - | **[TensorFlowï¼ˆå¯†ç hniyï¼‰](https://pan.baidu.com/s/1_MDAIYIGVgDovWkSs51NDA?pwd=hniy)** |\n| **`RBT4, Chinese`** | **EXTæ•°æ®<sup>[1]</sup>** | - | **[TensorFlowï¼ˆå¯†ç sjptï¼‰](https://pan.baidu.com/s/1MUrmuTULnMn3L1aw_dXxSA?pwd=sjpt)** |\n| **`RBTL3, Chinese`** | **EXTæ•°æ®<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1Jzn1hYwmv0kXkfTeIvNT61Rn1IbRc-o8)**<br/>**[PyTorch](https://drive.google.com/open?id=1qs5OasLXXjOnR2XuGUh12NanUl0pkjEv)** | **[TensorFlowï¼ˆå¯†ç s6cuï¼‰](https://pan.baidu.com/s/1vV9ClBMbsSpt8wUpfQz62Q?pwd=s6cu)** |\n| **`RBT3, Chinese`** | **EXTæ•°æ®<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1-rvV0nBDvRCASbRz8M9Decc3_8Aw-2yi)**<br/>**[PyTorch](https://drive.google.com/open?id=1_LqmIxm8Nz1Abvlqb8QFZaxYo-TInOed)** | **[TensorFlowï¼ˆå¯†ç 5a57ï¼‰](https://pan.baidu.com/s/1AnapwWj1YBZ_4E6AAtj2lg?pwd=5a57)** |\n| **`RoBERTa-wwm-ext-large, Chinese`** | **EXTæ•°æ®<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1dtad0FFzG11CBsawu8hvwwzU2R0FDI94)**<br/>**[PyTorch](https://drive.google.com/open?id=1-2vEZfIFCdM1-vJ3GD6DlSyKT4eVXMKq)** | **[TensorFlowï¼ˆå¯†ç dqqeï¼‰](https://pan.baidu.com/s/1F68xzCLWEonTEVP7HQ0Ciw?pwd=dqqe)** |\n| **`RoBERTa-wwm-ext, Chinese`** | **EXTæ•°æ®<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1jMAKIJmPn7kADgD3yQZhpsqM-IRM1qZt)** <br/>**[PyTorch](https://drive.google.com/open?id=1eHM3l4fMo6DsQYGmey7UZGiTmQquHw25)** | **[TensorFlowï¼ˆå¯†ç vybqï¼‰](https://pan.baidu.com/s/1oR0cgSXE3Nz6dESxr98qVA?pwd=vybq)** |\n| **`BERT-wwm-ext, Chinese`** | **EXTæ•°æ®<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1buMLEjdtrXE2c4G1rpsNGWEx7lUQ0RHi)** <br/>**[PyTorch](https://drive.google.com/open?id=1iNeYFhCBJWeUsIlnW_2K6SMwXkM4gLb_)** | **[TensorFlowï¼ˆå¯†ç wgntï¼‰](https://pan.baidu.com/s/1x-jIw1X2yNYHGak2yiq4RQ?pwd=wgnt)** |\n| **`BERT-wwm, Chinese`** | **ä¸­æ–‡ç»´åŸº** | **[TensorFlow](https://drive.google.com/open?id=1RoTQsXp2hkQ1gSRVylRIJfQxJUgkfJMW)** <br/>**[PyTorch](https://drive.google.com/open?id=1AQitrjbvCWc51SYiLN-cJq4e0WiNN4KY)** | **[TensorFlowï¼ˆå¯†ç qfh8ï¼‰](https://pan.baidu.com/s/1HDdDXiYxGT5ub5OeO7qdWw?pwd=qfh8)** |\n| `BERT-base, Chinese`<sup>Google</sup> | ä¸­æ–‡ç»´åŸº | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) | - |\n| `BERT-base, Multilingual Cased`<sup>Google</sup>  | å¤šè¯­ç§ç»´åŸº | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip) | - |\n| `BERT-base, Multilingual Uncased`<sup>Google</sup>  | å¤šè¯­ç§ç»´åŸº | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip) | - |\n\n> [1] EXTæ•°æ®åŒ…æ‹¬ï¼šä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼Œå…¶ä»–ç™¾ç§‘ã€æ–°é—»ã€é—®ç­”ç­‰æ•°æ®ï¼Œæ€»è¯æ•°è¾¾5.4Bã€‚\n\n### PyTorchç‰ˆæœ¬\n\nå¦‚éœ€PyTorchç‰ˆæœ¬ï¼Œ\n\n1ï¼‰è¯·è‡ªè¡Œé€šè¿‡[ğŸ¤—Transformers](https://github.com/huggingface/transformers)æä¾›çš„è½¬æ¢è„šæœ¬è¿›è¡Œè½¬æ¢ã€‚\n\n2ï¼‰æˆ–è€…é€šè¿‡huggingfaceå®˜ç½‘ç›´æ¥ä¸‹è½½PyTorchç‰ˆæƒé‡ï¼šhttps://huggingface.co/hfl\n\nä¸‹è½½æ–¹æ³•ï¼šç‚¹å‡»ä»»æ„éœ€è¦ä¸‹è½½çš„æ¨¡å‹ â†’ é€‰æ‹©\"Files and versions\"é€‰é¡¹å¡ â†’ ä¸‹è½½å¯¹åº”çš„æ¨¡å‹æ–‡ä»¶ã€‚\n\n### ä½¿ç”¨è¯´æ˜\n\nä¸­å›½å¤§é™†å¢ƒå†…å»ºè®®ä½¿ç”¨ç™¾åº¦ç½‘ç›˜ä¸‹è½½ç‚¹ï¼Œå¢ƒå¤–ç”¨æˆ·å»ºè®®ä½¿ç”¨è°·æ­Œä¸‹è½½ç‚¹ï¼Œbaseæ¨¡å‹æ–‡ä»¶å¤§å°çº¦**400M**ã€‚ \nä»¥TensorFlowç‰ˆ`BERT-wwm, Chinese`ä¸ºä¾‹ï¼Œä¸‹è½½å®Œæ¯•åå¯¹zipæ–‡ä»¶è¿›è¡Œè§£å‹å¾—åˆ°ï¼š\n\n```\nchinese_wwm_L-12_H-768_A-12.zip\n    |- bert_model.ckpt      # æ¨¡å‹æƒé‡\n    |- bert_model.meta      # æ¨¡å‹metaä¿¡æ¯\n    |- bert_model.index     # æ¨¡å‹indexä¿¡æ¯\n    |- bert_config.json     # æ¨¡å‹å‚æ•°\n    |- vocab.txt            # è¯è¡¨\n```\nå…¶ä¸­`bert_config.json`å’Œ`vocab.txt`ä¸è°·æ­ŒåŸç‰ˆ`BERT-base, Chinese`å®Œå…¨ä¸€è‡´ã€‚\nPyTorchç‰ˆæœ¬åˆ™åŒ…å«`pytorch_model.bin`, `bert_config.json`, `vocab.txt`æ–‡ä»¶ã€‚\n\n\n## å¿«é€ŸåŠ è½½\n### ä½¿ç”¨Huggingface-Transformers\n\nä¾æ‰˜äº[ğŸ¤—transformersåº“](https://github.com/huggingface/transformers)ï¼Œå¯è½»æ¾è°ƒç”¨ä»¥ä¸Šæ¨¡å‹ã€‚\n```\ntokenizer = BertTokenizer.from_pretrained(\"MODEL_NAME\")\nmodel = BertModel.from_pretrained(\"MODEL_NAME\")\n```\n**æ³¨æ„ï¼šæœ¬ç›®å½•ä¸­çš„æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨BertTokenizerä»¥åŠBertModelåŠ è½½ï¼Œè¯·å‹¿ä½¿ç”¨RobertaTokenizer/RobertaModelï¼**\n\nå…¶ä¸­`MODEL_NAME`å¯¹åº”åˆ—è¡¨å¦‚ä¸‹ï¼š\n\n| æ¨¡å‹å | MODEL_NAME |\n| - | - |\n| RoBERTa-wwm-ext-large | hfl/chinese-roberta-wwm-ext-large |\n| RoBERTa-wwm-ext | hfl/chinese-roberta-wwm-ext |\n| BERT-wwm-ext | hfl/chinese-bert-wwm-ext |\n| BERT-wwm | hfl/chinese-bert-wwm |\n| RBT3 | hfl/rbt3 |\n| RBTL3 | hfl/rbtl3 |\n\n### ä½¿ç”¨PaddleHub\n\nä¾æ‰˜[PaddleHub](https://github.com/PaddlePaddle/PaddleHub)ï¼Œåªéœ€ä¸€è¡Œä»£ç å³å¯å®Œæˆæ¨¡å‹ä¸‹è½½å®‰è£…ï¼Œåä½™è¡Œä»£ç å³å¯å®Œæˆæ–‡æœ¬åˆ†ç±»ã€åºåˆ—æ ‡æ³¨ã€é˜…è¯»ç†è§£ç­‰ä»»åŠ¡ã€‚\n\n```\nimport paddlehub as hub\nmodule = hub.Module(name=MODULE_NAME)\n```\n\nå…¶ä¸­`MODULE_NAME`å¯¹åº”åˆ—è¡¨å¦‚ä¸‹ï¼š\n\n| æ¨¡å‹å | MODULE_NAME |\n| - | - |\n| RoBERTa-wwm-ext-large | [chinese-roberta-wwm-ext-large](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-roberta-wwm-ext-large&en_category=SemanticModel) |\n| RoBERTa-wwm-ext       | [chinese-roberta-wwm-ext](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-roberta-wwm-ext&en_category=SemanticModel) |\n| BERT-wwm-ext          | [chinese-bert-wwm-ext](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-bert-wwm-ext&en_category=SemanticModel) |\n| BERT-wwm              | [chinese-bert-wwm](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-bert-wwm&en_category=SemanticModel) |\n| RBT3                  | [rbt3](https://www.paddlepaddle.org.cn/hubdetail?name=rbt3&en_category=SemanticModel) |\n| RBTL3                 | [rbtl3](https://www.paddlepaddle.org.cn/hubdetail?name=rbtl3&en_category=SemanticModel) |\n\n\n## æ¨¡å‹å¯¹æ¯”\né’ˆå¯¹å¤§å®¶æ¯”è¾ƒå…³å¿ƒçš„ä¸€äº›æ¨¡å‹ç»†èŠ‚è¿›è¡Œæ±‡æ€»å¦‚ä¸‹ã€‚\n\n| - | BERT<sup>Google</sup> | BERT-wwm | BERT-wwm-ext | RoBERTa-wwm-ext | RoBERTa-wwm-ext-large |\n| :------- | :---------: | :---------: | :---------: | :---------: | :---------: |\n| Masking | WordPiece | WWM<sup>[1]</sup> | WWM | WWM | WWM |\n| Type | base | base | base | base | **large** |\n| Data Source | wiki | wiki | wiki+ext<sup>[2]</sup> | wiki+ext | wiki+ext |\n| Training Tokens # | 0.4B | 0.4B | 5.4B | 5.4B | 5.4B |\n| Device | TPU Pod v2 | TPU v3 | TPU v3 | TPU v3 | **TPU Pod v3-32<sup>[3]</sup>** |\n| Training Steps | ? | 100K<sup>MAX128</sup> <br/>+100K<sup>MAX512</sup> | 1M<sup>MAX128</sup> <br/>+400K<sup>MAX512</sup> | 1M<sup>MAX512</sup> | 2M<sup>MAX512</sup> |\n| Batch Size | ? | 2,560 / 384 | 2,560 / 384 | 384 | 512 |\n| Optimizer | AdamW | LAMB | LAMB | AdamW | AdamW |\n| Vocabulary | 21,128 | ~BERT<sup>[4]</sup> | ~BERT | ~BERT | ~BERT |\n| Init Checkpoint | Random Init | ~BERT | ~BERT | ~BERT | Random Init |\n\n> [1] WWM = Whole Word Masking  \n> [2] ext = extended data  \n> [3] TPU Pod v3-32 (512G HBM)ç­‰ä»·äº4ä¸ªTPU v3 (128G HBM)  \n> [4] `~BERT`è¡¨ç¤º**ç»§æ‰¿**è°·æ­ŒåŸç‰ˆä¸­æ–‡BERTçš„å±æ€§  \n\n\n## ä¸­æ–‡åŸºçº¿ç³»ç»Ÿæ•ˆæœ\nä¸ºäº†å¯¹æ¯”åŸºçº¿æ•ˆæœï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹å‡ ä¸ªä¸­æ–‡æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼ŒåŒ…æ‹¬`å¥å­çº§`å’Œ`ç¯‡ç« çº§`ä»»åŠ¡ã€‚\nå¯¹äº`BERT-wwm-ext`ã€`RoBERTa-wwm-ext`ã€`RoBERTa-wwm-ext-large`ï¼Œæˆ‘ä»¬**æ²¡æœ‰è¿›ä¸€æ­¥è°ƒæ•´æœ€ä½³å­¦ä¹ ç‡**ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨äº†`BERT-wwm`çš„æœ€ä½³å­¦ä¹ ç‡ã€‚\n\næœ€ä½³å­¦ä¹ ç‡ï¼š  \n\n| æ¨¡å‹ | BERT | ERNIE | BERT-wwm* |\n| :------- | :---------: | :---------: | :---------: |\n| CMRC 2018 | 3e-5 | 8e-5 | 3e-5 |\n| DRCD | 3e-5 | 8e-5 | 3e-5 |\n| CJRC | 4e-5 | 8e-5 | 4e-5 |\n| XNLI | 3e-5 | 5e-5 | 3e-5 |\n| ChnSentiCorp | 2e-5 | 5e-5 | 2e-5 |\n| LCQMC  | 2e-5 | 3e-5 | 2e-5 |\n| BQ Corpus | 3e-5 | 5e-5 | 3e-5 |\n| THUCNews | 2e-5 | 5e-5 | 2e-5 |\n\n*ä»£è¡¨æ‰€æœ‰wwmç³»åˆ—æ¨¡å‹ (BERT-wwm, BERT-wwm-ext, RoBERTa-wwm-ext, RoBERTa-wwm-ext-large)\n\n\n**ä¸‹é¢ä»…åˆ—ä¸¾éƒ¨åˆ†ç»“æœï¼Œå®Œæ•´ç»“æœè¯·æŸ¥çœ‹æˆ‘ä»¬çš„[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/1906.08101)ã€‚**\n\n- [**CMRC 2018**ï¼šç¯‡ç« ç‰‡æ®µæŠ½å–å‹é˜…è¯»ç†è§£ï¼ˆç®€ä½“ä¸­æ–‡ï¼‰](https://github.com/ymcui/cmrc2018)\n- [**DRCD**ï¼šç¯‡ç« ç‰‡æ®µæŠ½å–å‹é˜…è¯»ç†è§£ï¼ˆç¹ä½“ä¸­æ–‡ï¼‰](https://github.com/DRCSolutionService/DRCD)\n- [**CJRC**: æ³•å¾‹é˜…è¯»ç†è§£ï¼ˆç®€ä½“ä¸­æ–‡ï¼‰](http://cail.cipsc.org.cn)\n- [**XNLI**ï¼šè‡ªç„¶è¯­è¨€æ¨æ–­](https://github.com/google-research/bert/blob/master/multilingual.md)\n- [**ChnSentiCorp**ï¼šæƒ…æ„Ÿåˆ†æ](https://github.com/pengming617/bert_classification)\n- [**LCQMC**ï¼šå¥å¯¹åŒ¹é…](http://icrc.hitsz.edu.cn/info/1037/1146.htm)\n- [**BQ Corpus**ï¼šå¥å¯¹åŒ¹é…](http://icrc.hitsz.edu.cn/Article/show/175.html)\n- [**THUCNews**ï¼šç¯‡ç« çº§æ–‡æœ¬åˆ†ç±»](http://thuctc.thunlp.org)\n\n**æ³¨æ„ï¼šä¸ºäº†ä¿è¯ç»“æœçš„å¯é æ€§ï¼Œå¯¹äºåŒä¸€æ¨¡å‹ï¼Œæˆ‘ä»¬è¿è¡Œ10éï¼ˆä¸åŒéšæœºç§å­ï¼‰ï¼Œæ±‡æŠ¥æ¨¡å‹æ€§èƒ½çš„æœ€å¤§å€¼å’Œå¹³å‡å€¼ï¼ˆæ‹¬å·å†…ä¸ºå¹³å‡å€¼ï¼‰ã€‚ä¸å‡ºæ„å¤–ï¼Œä½ è¿è¡Œçš„ç»“æœåº”è¯¥å¾ˆå¤§æ¦‚ç‡è½åœ¨è¿™ä¸ªåŒºé—´å†…ã€‚**\n\n**è¯„æµ‹æŒ‡æ ‡ä¸­ï¼Œæ‹¬å·å†…è¡¨ç¤ºå¹³å‡å€¼ï¼Œæ‹¬å·å¤–è¡¨ç¤ºæœ€å¤§å€¼ã€‚**\n\n\n### ç®€ä½“ä¸­æ–‡é˜…è¯»ç†è§£ï¼šCMRC 2018\n[**CMRC 2018æ•°æ®é›†**](https://github.com/ymcui/cmrc2018)æ˜¯å“ˆå·¥å¤§è®¯é£è”åˆå®éªŒå®¤å‘å¸ƒçš„ä¸­æ–‡æœºå™¨é˜…è¯»ç†è§£æ•°æ®ã€‚\næ ¹æ®ç»™å®šé—®é¢˜ï¼Œç³»ç»Ÿéœ€è¦ä»ç¯‡ç« ä¸­æŠ½å–å‡ºç‰‡æ®µä½œä¸ºç­”æ¡ˆï¼Œå½¢å¼ä¸SQuADç›¸åŒã€‚\nè¯„æµ‹æŒ‡æ ‡ä¸ºï¼šEM / F1\n\n| æ¨¡å‹ | å¼€å‘é›† | æµ‹è¯•é›† | æŒ‘æˆ˜é›† |\n| :------- | :---------: | :---------: | :---------: |\n| BERT | 65.5 (64.4) / 84.5 (84.0) | 70.0 (68.7) / 87.0 (86.3) | 18.6 (17.0) / 43.3 (41.3) |\n| ERNIE | 65.4 (64.3) / 84.7 (84.2) | 69.4 (68.2) / 86.6 (86.1) | 19.6 (17.0) / 44.3 (42.8) |\n| **BERT-wwm** | 66.3 (65.0) / 85.6 (84.7) | 70.5 (69.1) / 87.4 (86.7) | 21.0 (19.3) / 47.0 (43.9) |\n| **BERT-wwm-ext** | 67.1 (65.6) / 85.7 (85.0) | 71.4 (70.0) / 87.7 (87.0) | 24.0 (20.0) / 47.3 (44.6) |\n| **RoBERTa-wwm-ext** | 67.4 (66.5) / 87.2 (86.5) | 72.6 (71.4) / 89.4 (88.8) | 26.2 (24.6) / 51.0 (49.1) |\n| **RoBERTa-wwm-ext-large** | **68.5 (67.6) / 88.4 (87.9)** | **74.2 (72.4) / 90.6 (90.0)** | **31.5 (30.1) / 60.1 (57.5)** |\n\n\n### ç¹ä½“ä¸­æ–‡é˜…è¯»ç†è§£ï¼šDRCD\n[**DRCDæ•°æ®é›†**](https://github.com/DRCKnowledgeTeam/DRCD)ç”±ä¸­å›½å°æ¹¾å°è¾¾ç ”ç©¶é™¢å‘å¸ƒï¼Œå…¶å½¢å¼ä¸SQuADç›¸åŒï¼Œæ˜¯åŸºäºç¹ä½“ä¸­æ–‡çš„æŠ½å–å¼é˜…è¯»ç†è§£æ•°æ®é›†ã€‚\n**ç”±äºERNIEä¸­å»é™¤äº†ç¹ä½“ä¸­æ–‡å­—ç¬¦ï¼Œæ•…ä¸å»ºè®®åœ¨ç¹ä½“ä¸­æ–‡æ•°æ®ä¸Šä½¿ç”¨ERNIEï¼ˆæˆ–è½¬æ¢æˆç®€ä½“ä¸­æ–‡åå†å¤„ç†ï¼‰ã€‚**\nè¯„æµ‹æŒ‡æ ‡ä¸ºï¼šEM / F1\n\n| æ¨¡å‹ | å¼€å‘é›† | æµ‹è¯•é›† |\n| :------- | :---------: | :---------: |\n| BERT | 83.1 (82.7) / 89.9 (89.6) | 82.2 (81.6) / 89.2 (88.8) |\n| ERNIE | 73.2 (73.0) / 83.9 (83.8) | 71.9 (71.4) / 82.5 (82.3) |\n| **BERT-wwm** | 84.3 (83.4) / 90.5 (90.2) | 82.8 (81.8) / 89.7 (89.0) |\n| **BERT-wwm-ext** | 85.0 (84.5) / 91.2 (90.9) | 83.6 (83.0) / 90.4 (89.9) |\n| **RoBERTa-wwm-ext** | 86.6 (85.9) / 92.5 (92.2) | 85.6 (85.2) / 92.0 (91.7) |\n| **RoBERTa-wwm-ext-large** | **89.6 (89.1) / 94.8 (94.4)** | **89.6 (88.9) / 94.5 (94.1)** |\n\n\n### å¸æ³•é˜…è¯»ç†è§£ï¼šCJRC\n[**CJRCæ•°æ®é›†**](http://cail.cipsc.org.cn)æ˜¯å“ˆå·¥å¤§è®¯é£è”åˆå®éªŒå®¤å‘å¸ƒçš„é¢å‘**å¸æ³•é¢†åŸŸ**çš„ä¸­æ–‡æœºå™¨é˜…è¯»ç†è§£æ•°æ®ã€‚\néœ€è¦æ³¨æ„çš„æ˜¯å®éªŒä¸­ä½¿ç”¨çš„æ•°æ®å¹¶éå®˜æ–¹å‘å¸ƒçš„æœ€ç»ˆæ•°æ®ï¼Œç»“æœä»…ä¾›å‚è€ƒã€‚\nè¯„æµ‹æŒ‡æ ‡ä¸ºï¼šEM / F1\n\n| æ¨¡å‹ | å¼€å‘é›† | æµ‹è¯•é›† |\n| :------- | :---------: | :---------: |\n| BERT | 54.6 (54.0) / 75.4 (74.5) | 55.1 (54.1) / 75.2 (74.3) |\n| ERNIE | 54.3 (53.9) / 75.3 (74.6) | 55.0 (53.9) / 75.0 (73.9) |\n| **BERT-wwm** | 54.7 (54.0) / 75.2 (74.8) | 55.1 (54.1) / 75.4 (74.4) |\n| **BERT-wwm-ext** | 55.6 (54.8) / 76.0 (75.3) | 55.6 (54.9) / 75.8 (75.0) |\n| **RoBERTa-wwm-ext** | 58.7 (57.6) / 79.1 (78.3) | 59.0 (57.8) / 79.0 (78.0) |\n| **RoBERTa-wwm-ext-large** | **62.1 (61.1) / 82.4 (81.6)** | **62.4 (61.4) / 82.2 (81.0)** |\n\n\n### è‡ªç„¶è¯­è¨€æ¨æ–­ï¼šXNLI\nåœ¨è‡ªç„¶è¯­è¨€æ¨æ–­ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†[**XNLI**æ•°æ®](https://github.com/google-research/bert/blob/master/multilingual.md)ï¼Œéœ€è¦å°†æ–‡æœ¬åˆ†æˆä¸‰ä¸ªç±»åˆ«ï¼š`entailment`ï¼Œ`neutral`ï¼Œ`contradictory`ã€‚\nè¯„æµ‹æŒ‡æ ‡ä¸ºï¼šAccuracy\n\n| æ¨¡å‹ | å¼€å‘é›† | æµ‹è¯•é›† |\n| :------- | :---------: | :---------: |\n| BERT | 77.8 (77.4) | 77.8 (77.5) |\n| ERNIE | 79.7 (79.4) | 78.6 (78.2) |\n| **BERT-wwm** | 79.0 (78.4) | 78.2 (78.0) |\n| **BERT-wwm-ext** | 79.4 (78.6) | 78.7 (78.3) |\n| **RoBERTa-wwm-ext** | 80.0 (79.2) | 78.8 (78.3) |\n| **RoBERTa-wwm-ext-large** | **82.1 (81.3)** | **81.2 (80.6)** |\n\n\n### æƒ…æ„Ÿåˆ†æï¼šChnSentiCorp\nåœ¨æƒ…æ„Ÿåˆ†æä»»åŠ¡ä¸­ï¼ŒäºŒåˆ†ç±»çš„æƒ…æ„Ÿåˆ†ç±»æ•°æ®é›†ChnSentiCorpã€‚\nè¯„æµ‹æŒ‡æ ‡ä¸ºï¼šAccuracy\n\n| æ¨¡å‹ | å¼€å‘é›† | æµ‹è¯•é›† |\n| :------- | :---------: | :---------: |\n| BERT | 94.7 (94.3) | 95.0 (94.7) |\n| ERNIE | 95.4 (94.8) | 95.4 **(95.3)** |\n| **BERT-wwm** | 95.1 (94.5) | 95.4 (95.0) |\n| **BERT-wwm-ext** | 95.4 (94.6) | 95.3 (94.7) |\n| **RoBERTa-wwm-ext** | 95.0 (94.6) | 95.6 (94.8) |\n| **RoBERTa-wwm-ext-large** | **95.8 (94.9)** | **95.8** (94.9) |\n\n\n### å¥å¯¹åˆ†ç±»ï¼šLCQMC, BQ Corpus\nä»¥ä¸‹ä¸¤ä¸ªæ•°æ®é›†å‡éœ€è¦å°†ä¸€ä¸ªå¥å¯¹è¿›è¡Œåˆ†ç±»ï¼Œåˆ¤æ–­ä¸¤ä¸ªå¥å­çš„è¯­ä¹‰æ˜¯å¦ç›¸åŒï¼ˆäºŒåˆ†ç±»ä»»åŠ¡ï¼‰ã€‚\n\n#### LCQMC\n[LCQMC](http://icrc.hitsz.edu.cn/info/1037/1146.htm)ç”±å“ˆå·¥å¤§æ·±åœ³ç ”ç©¶ç”Ÿé™¢æ™ºèƒ½è®¡ç®—ç ”ç©¶ä¸­å¿ƒå‘å¸ƒã€‚ \nè¯„æµ‹æŒ‡æ ‡ä¸ºï¼šAccuracy\n\n| æ¨¡å‹ | å¼€å‘é›† | æµ‹è¯•é›† |\n| :------- | :---------: | :---------: |\n| BERT | 89.4 (88.4) | 86.9 (86.4) |\n| ERNIE | 89.8 (89.6) | **87.2 (87.0)** |\n| **BERT-wwm** | 89.4 (89.2) | 87.0 (86.8) |\n| **BERT-wwm-ext** | 89.6 (89.2) | 87.1 (86.6) |\n| **RoBERTa-wwm-ext** | 89.0 (88.7) | 86.4 (86.1) |\n| **RoBERTa-wwm-ext-large** | **90.4 (90.0)** | 87.0 (86.8) |\n\n\n#### BQ Corpus \n[BQ Corpus](http://icrc.hitsz.edu.cn/Article/show/175.html)ç”±å“ˆå·¥å¤§æ·±åœ³ç ”ç©¶ç”Ÿé™¢æ™ºèƒ½è®¡ç®—ç ”ç©¶ä¸­å¿ƒå‘å¸ƒï¼Œæ˜¯é¢å‘é“¶è¡Œé¢†åŸŸçš„æ•°æ®é›†ã€‚\nè¯„æµ‹æŒ‡æ ‡ä¸ºï¼šAccuracy\n\n| æ¨¡å‹ | å¼€å‘é›† | æµ‹è¯•é›† |\n| :------- | :---------: | :---------: |\n| BERT | 86.0 (85.5) | 84.8 (84.6) |\n| ERNIE | 86.3 (85.5) | 85.0 (84.6) |\n| **BERT-wwm** | 86.1 (85.6) | 85.2 **(84.9)** |\n| **BERT-wwm-ext** | **86.4** (85.5) | 85.3 (84.8) |\n| **RoBERTa-wwm-ext** | 86.0 (85.4) | 85.0 (84.6) |\n| **RoBERTa-wwm-ext-large** | 86.3 **(85.7)** | **85.8 (84.9)** |\n\n\n### ç¯‡ç« çº§æ–‡æœ¬åˆ†ç±»ï¼šTHUCNews\nç¯‡ç« çº§æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æˆ‘ä»¬é€‰ç”¨äº†ç”±æ¸…åå¤§å­¦è‡ªç„¶è¯­è¨€å¤„ç†å®éªŒå®¤å‘å¸ƒçš„æ–°é—»æ•°æ®é›†**THUCNews**ã€‚\næˆ‘ä»¬é‡‡ç”¨çš„æ˜¯å…¶ä¸­ä¸€ä¸ªå­é›†ï¼Œéœ€è¦å°†æ–°é—»åˆ†æˆ10ä¸ªç±»åˆ«ä¸­çš„ä¸€ä¸ªã€‚\nè¯„æµ‹æŒ‡æ ‡ä¸ºï¼šAccuracy\n\n| æ¨¡å‹ | å¼€å‘é›† | æµ‹è¯•é›† |\n| :------- | :---------: | :---------: |\n| BERT | 97.7 (97.4) | 97.8 (97.6) |\n| ERNIE | 97.6 (97.3) | 97.5 (97.3) |\n| **BERT-wwm** | 98.0 (97.6) | 97.8 (97.6) |\n| **BERT-wwm-ext** | 97.7 (97.5) | 97.7 (97.5) |\n| **RoBERTa-wwm-ext** | 98.3 (97.9) | 97.7 (97.5) |\n| **RoBERTa-wwm-ext-large** | 98.3 (97.7) | 97.8 (97.6) |\n\n\n### å°å‚æ•°é‡æ¨¡å‹\nä»¥ä¸‹æ˜¯åœ¨è‹¥å¹²NLPä»»åŠ¡ä¸Šçš„å®éªŒæ•ˆæœï¼Œè¡¨ä¸­åªæä¾›æµ‹è¯•é›†ç»“æœå¯¹æ¯”ã€‚\n\n| æ¨¡å‹ | CMRC 2018 | DRCD | XNLI | CSC | LCQMC | BQ | å¹³å‡ | å‚æ•°é‡ |\n| :------- | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| RoBERTa-wwm-ext-large | 74.2 / 90.6 | 89.6 / 94.5 | 81.2 | 95.8 | 87.0 | 85.8 | 87.335 | 325M |\n| RoBERTa-wwm-ext | 72.6 / 89.4 | 85.6 / 92.0 | 78.8 | 95.6 | 86.4 | 85.0 | 85.675 | 102M |\n| RBTL3 | 63.3 / 83.4 | 77.2 / 85.6 | 74.0 | 94.2 | 85.1 | 83.6 | 80.800 | 61M (59.8%) |\n| RBT3 | 62.2 / 81.8 | 75.0 / 83.9 | 72.3 | 92.8 | 85.1 | 83.3 | 79.550 | 38M (37.3%) |\n\næ•ˆæœç›¸å¯¹å€¼æ¯”è¾ƒï¼š\n\n| æ¨¡å‹ | CMRC 2018 | DRCD | XNLI | CSC | LCQMC | BQ | å¹³å‡ | åˆ†ç±»å¹³å‡ |\n| :------- | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| RoBERTa-wwm-ext-large | 102.2% / 101.3% | 104.7% / 102.7% | 103.0% | 100.2% | 100.7% | 100.9% | 101.9% | 101.2% |\n| RoBERTa-wwm-ext | 100% / 100% | 100% / 100% | 100% | 100% | 100% | 100% | 100% | 100% |\n| RBTL3 | 87.2% / 93.3% | 90.2% / 93.0% | 93.9% | 98.5% | 98.5% | 98.4% | 94.3% | 97.35% |\n| RBT3 | 85.7% / 91.5% | 87.6% / 91.2% | 91.8% | 97.1% | 98.5% | 98.0% | 92.9% | 96.35% |\n\n- å‚æ•°é‡æ˜¯ä»¥XNLIåˆ†ç±»ä»»åŠ¡ä¸ºåŸºå‡†è¿›è¡Œè®¡ç®—\n- æ‹¬å·å†…å‚æ•°é‡ç™¾åˆ†æ¯”ä»¥åŸå§‹baseæ¨¡å‹ï¼ˆå³RoBERTa-wwm-extï¼‰ä¸ºåŸºå‡†\n- RBT3ï¼šç”±RoBERTa-wwm-ext 3å±‚è¿›è¡Œåˆå§‹åŒ–ï¼Œç»§ç»­è®­ç»ƒäº†1Mæ­¥\n- RBTL3ï¼šç”±RoBERTa-wwm-ext-large 3å±‚è¿›è¡Œåˆå§‹åŒ–ï¼Œç»§ç»­è®­ç»ƒäº†1Mæ­¥\n- RBTçš„åå­—æ˜¯RoBERTaä¸‰ä¸ªéŸ³èŠ‚é¦–å­—æ¯ç»„æˆï¼ŒLä»£è¡¨largeæ¨¡å‹\n- ç›´æ¥ä½¿ç”¨RoBERTa-wwm-ext-largeå‰ä¸‰å±‚è¿›è¡Œåˆå§‹åŒ–å¹¶è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„è®­ç»ƒå°†æ˜¾è‘—é™ä½æ•ˆæœï¼Œä¾‹å¦‚åœ¨CMRC 2018ä¸Šæµ‹è¯•é›†ä»…èƒ½è¾¾åˆ°42.9/65.3ï¼Œè€ŒRBTL3èƒ½è¾¾åˆ°63.3/83.4\n\næ¬¢è¿ä½¿ç”¨æ•ˆæœæ›´ä¼˜çš„ä¸­æ–‡å°å‹é¢„è®­ç»ƒæ¨¡å‹MiniRBTï¼šhttps://github.com/iflytek/MiniRBT\n\n## ä½¿ç”¨å»ºè®®\n* åˆå§‹å­¦ä¹ ç‡æ˜¯éå¸¸é‡è¦çš„ä¸€ä¸ªå‚æ•°ï¼ˆä¸è®ºæ˜¯`BERT`è¿˜æ˜¯å…¶ä»–æ¨¡å‹ï¼‰ï¼Œéœ€è¦æ ¹æ®ç›®æ ‡ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚\n* `ERNIE`çš„æœ€ä½³å­¦ä¹ ç‡å’Œ`BERT`/`BERT-wwm`ç›¸å·®è¾ƒå¤§ï¼Œæ‰€ä»¥ä½¿ç”¨`ERNIE`æ—¶è¯·åŠ¡å¿…è°ƒæ•´å­¦ä¹ ç‡ï¼ˆåŸºäºä»¥ä¸Šå®éªŒç»“æœï¼Œ`ERNIE`éœ€è¦çš„åˆå§‹å­¦ä¹ ç‡è¾ƒé«˜ï¼‰ã€‚\n* ç”±äº`BERT`/`BERT-wwm`ä½¿ç”¨äº†ç»´åŸºç™¾ç§‘æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ•…å®ƒä»¬å¯¹æ­£å¼æ–‡æœ¬å»ºæ¨¡è¾ƒå¥½ï¼›è€Œ`ERNIE`ä½¿ç”¨äº†é¢å¤–çš„ç™¾åº¦è´´å§ã€çŸ¥é“ç­‰ç½‘ç»œæ•°æ®ï¼Œå®ƒå¯¹éæ­£å¼æ–‡æœ¬ï¼ˆä¾‹å¦‚å¾®åšç­‰ï¼‰å»ºæ¨¡æœ‰ä¼˜åŠ¿ã€‚\n* åœ¨é•¿æ–‡æœ¬å»ºæ¨¡ä»»åŠ¡ä¸Šï¼Œä¾‹å¦‚é˜…è¯»ç†è§£ã€æ–‡æ¡£åˆ†ç±»ï¼Œ`BERT`å’Œ`BERT-wwm`çš„æ•ˆæœè¾ƒå¥½ã€‚\n* å¦‚æœç›®æ ‡ä»»åŠ¡çš„æ•°æ®å’Œé¢„è®­ç»ƒæ¨¡å‹çš„é¢†åŸŸç›¸å·®è¾ƒå¤§ï¼Œè¯·åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥åšé¢„è®­ç»ƒã€‚\n* å¦‚æœè¦å¤„ç†ç¹ä½“ä¸­æ–‡æ•°æ®ï¼Œè¯·ä½¿ç”¨`BERT`æˆ–è€…`BERT-wwm`ã€‚å› ä¸ºæˆ‘ä»¬å‘ç°`ERNIE`çš„è¯è¡¨ä¸­å‡ ä¹æ²¡æœ‰ç¹ä½“ä¸­æ–‡ã€‚\n\n\n## è‹±æ–‡æ¨¡å‹ä¸‹è½½\nä¸ºäº†æ–¹ä¾¿å¤§å®¶ä¸‹è½½ï¼Œé¡ºä¾¿å¸¦ä¸Š**è°·æ­Œå®˜æ–¹å‘å¸ƒ**çš„è‹±æ–‡`BERT-large (wwm)`æ¨¡å‹ï¼š\n\n*   **[`BERT-Large, Uncased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n\n*   **[`BERT-Large, Cased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n\n## FAQ\n**Q: è¿™ä¸ªæ¨¡å‹æ€ä¹ˆç”¨ï¼Ÿ**  \nA: è°·æ­Œå‘å¸ƒçš„ä¸­æ–‡BERTæ€ä¹ˆç”¨ï¼Œè¿™ä¸ªå°±æ€ä¹ˆç”¨ã€‚\n**æ–‡æœ¬ä¸éœ€è¦ç»è¿‡åˆ†è¯ï¼Œwwmåªå½±å“é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œä¸å½±å“ä¸‹æ¸¸ä»»åŠ¡çš„è¾“å…¥ã€‚**\n\n**Q: è¯·é—®æœ‰é¢„è®­ç»ƒä»£ç æä¾›å—ï¼Ÿ**  \nA: å¾ˆé—æ†¾ï¼Œæˆ‘ä¸èƒ½æä¾›ç›¸å…³ä»£ç ï¼Œå®ç°å¯ä»¥å‚è€ƒ [#10](https://github.com/ymcui/Chinese-BERT-wwm/issues/10) å’Œ [#13](https://github.com/ymcui/Chinese-BERT-wwm/issues/13)ã€‚\n\n**Q: æŸæŸæ•°æ®é›†åœ¨å“ªé‡Œä¸‹è½½ï¼Ÿ**  \nA: è¯·æŸ¥çœ‹`data`ç›®å½•ï¼Œä»»åŠ¡ç›®å½•ä¸‹çš„`README.md`æ ‡æ˜äº†æ•°æ®æ¥æºã€‚å¯¹äºæœ‰ç‰ˆæƒçš„å†…å®¹ï¼Œè¯·è‡ªè¡Œæœç´¢æˆ–ä¸åŸä½œè€…è”ç³»è·å–æ•°æ®ã€‚\n\n**Q: ä¼šæœ‰è®¡åˆ’å‘å¸ƒæ›´å¤§æ¨¡å‹å—ï¼Ÿæ¯”å¦‚BERT-large-wwmç‰ˆæœ¬ï¼Ÿ**  \nA: å¦‚æœæˆ‘ä»¬ä»å®éªŒä¸­å¾—åˆ°æ›´å¥½æ•ˆæœï¼Œä¼šè€ƒè™‘å‘å¸ƒæ›´å¤§çš„ç‰ˆæœ¬ã€‚\n\n**Q: ä½ éª—äººï¼æ— æ³•å¤ç°ç»“æœğŸ˜‚**  \nA: åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æœ€ç®€å•çš„æ¨¡å‹ã€‚æ¯”å¦‚åˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨çš„æ˜¯`run_classifier.py`ï¼ˆè°·æ­Œæä¾›ï¼‰ã€‚\nå¦‚æœæ— æ³•è¾¾åˆ°å¹³å‡å€¼ï¼Œè¯´æ˜å®éªŒæœ¬èº«å­˜åœ¨bugï¼Œè¯·ä»”ç»†æ’æŸ¥ã€‚\næœ€é«˜å€¼å­˜åœ¨å¾ˆå¤šéšæœºå› ç´ ï¼Œæˆ‘ä»¬æ— æ³•ä¿è¯èƒ½å¤Ÿè¾¾åˆ°æœ€é«˜å€¼ã€‚\nå¦å¤–ä¸€ä¸ªå…¬è®¤çš„å› ç´ ï¼šé™ä½batch sizeä¼šæ˜¾è‘—é™ä½å®éªŒæ•ˆæœï¼Œå…·ä½“å¯å‚è€ƒBERTï¼ŒXLNetç›®å½•çš„ç›¸å…³Issueã€‚\n\n**Q: æˆ‘è®­å‡ºæ¥æ¯”ä½ æ›´å¥½çš„ç»“æœï¼**  \nA: æ­å–œä½ ã€‚\n\n**Q: è®­ç»ƒèŠ±äº†å¤šé•¿æ—¶é—´ï¼Œåœ¨ä»€ä¹ˆè®¾å¤‡ä¸Šè®­ç»ƒçš„ï¼Ÿ**  \nA: è®­ç»ƒæ˜¯åœ¨è°·æ­ŒTPU v3ç‰ˆæœ¬ï¼ˆ128G HBMï¼‰å®Œæˆçš„ï¼Œè®­ç»ƒBERT-wwmèŠ±è´¹çº¦1.5å¤©ï¼ŒBERT-wwm-extåˆ™éœ€è¦æ•°å‘¨æ—¶é—´ï¼ˆä½¿ç”¨äº†æ›´å¤šæ•°æ®éœ€è¦è¿­ä»£æ›´å……åˆ†ï¼‰ã€‚\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œé¢„è®­ç»ƒé˜¶æ®µæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯`LAMB Optimizer`ï¼ˆ[TensorFlowç‰ˆæœ¬å®ç°](https://github.com/ymcui/LAMB_Optimizer_TF)ï¼‰ã€‚è¯¥ä¼˜åŒ–å™¨å¯¹å¤§çš„batchæœ‰è‰¯å¥½çš„æ”¯æŒã€‚\nåœ¨å¾®è°ƒä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬é‡‡ç”¨çš„æ˜¯BERTé»˜è®¤çš„`AdamWeightDecayOptimizer`ã€‚\n\n**Q: ERNIEæ˜¯è°ï¼Ÿ**  \nA: æœ¬é¡¹ç›®ä¸­çš„ERNIEæ¨¡å‹ç‰¹æŒ‡ç™¾åº¦å…¬å¸æå‡ºçš„[ERNIE](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE)ï¼Œè€Œéæ¸…åå¤§å­¦åœ¨ACL 2019ä¸Šå‘è¡¨çš„[ERNIE](https://github.com/thunlp/ERNIE)ã€‚\n\n**Q: BERT-wwmçš„æ•ˆæœä¸æ˜¯åœ¨æ‰€æœ‰ä»»åŠ¡éƒ½å¾ˆå¥½**  \nA: æœ¬é¡¹ç›®çš„ç›®çš„æ˜¯ä¸ºç ”ç©¶è€…æä¾›å¤šå…ƒåŒ–çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè‡ªç”±é€‰æ‹©BERTï¼ŒERNIEï¼Œæˆ–è€…æ˜¯BERT-wwmã€‚\næˆ‘ä»¬ä»…æä¾›å®éªŒæ•°æ®ï¼Œå…·ä½“æ•ˆæœå¦‚ä½•è¿˜æ˜¯å¾—åœ¨è‡ªå·±çš„ä»»åŠ¡ä¸­ä¸æ–­å°è¯•æ‰èƒ½å¾—å‡ºç»“è®ºã€‚\nå¤šä¸€ä¸ªæ¨¡å‹ï¼Œå¤šä¸€ç§é€‰æ‹©ã€‚\n\n**Q: ä¸ºä»€ä¹ˆæœ‰äº›æ•°æ®é›†ä¸Šæ²¡æœ‰è¯•ï¼Ÿ**  \nA: å¾ˆå¦ç‡çš„è¯´ï¼š\n1ï¼‰æ²¡ç²¾åŠ›æ‰¾æ›´å¤šçš„æ•°æ®ï¼›\n2ï¼‰æ²¡æœ‰å¿…è¦ï¼› \n3ï¼‰æ²¡æœ‰é’ç¥¨ï¼›\n\n**Q: ç®€å•è¯„ä»·ä¸€ä¸‹è¿™å‡ ä¸ªæ¨¡å‹**  \nA: å„æœ‰ä¾§é‡ï¼Œå„æœ‰åƒç§‹ã€‚\nä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶å‘å±•éœ€è¦å¤šæ–¹å…±åŒåŠªåŠ›ã€‚\n\n**Q: ä½ é¢„æµ‹ä¸‹ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å«ä»€ä¹ˆï¼Ÿ**  \nA: å¯èƒ½å«ZOEå§ï¼ŒZOE: Zero-shOt Embeddings from language model\n\n**Q: æ›´å¤šå…³äº`RoBERTa-wwm-ext`æ¨¡å‹çš„ç»†èŠ‚ï¼Ÿ**  \nA: æˆ‘ä»¬é›†æˆäº†RoBERTaå’ŒBERT-wwmçš„ä¼˜ç‚¹ï¼Œå¯¹ä¸¤è€…è¿›è¡Œäº†ä¸€ä¸ªè‡ªç„¶çš„ç»“åˆã€‚\nå’Œä¹‹å‰æœ¬ç›®å½•ä¸­çš„æ¨¡å‹ä¹‹é—´çš„åŒºåˆ«å¦‚ä¸‹:  \n1ï¼‰é¢„è®­ç»ƒé˜¶æ®µé‡‡ç”¨wwmç­–ç•¥è¿›è¡Œmaskï¼ˆä½†æ²¡æœ‰ä½¿ç”¨dynamic maskingï¼‰  \n2ï¼‰ç®€å•å–æ¶ˆNext Sentence Predictionï¼ˆNSPï¼‰loss  \n3ï¼‰ä¸å†é‡‡ç”¨å…ˆmax_len=128ç„¶åå†max_len=512çš„è®­ç»ƒæ¨¡å¼ï¼Œç›´æ¥è®­ç»ƒmax_len=512  \n4ï¼‰è®­ç»ƒæ­¥æ•°é€‚å½“å»¶é•¿  \n\néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨¡å‹å¹¶éåŸç‰ˆRoBERTaæ¨¡å‹ï¼Œåªæ˜¯æŒ‰ç…§ç±»ä¼¼RoBERTaè®­ç»ƒæ–¹å¼è®­ç»ƒå‡ºçš„BERTæ¨¡å‹ï¼Œå³RoBERTa-like BERTã€‚\næ•…åœ¨ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨ã€æ¨¡å‹è½¬æ¢æ—¶è¯·æŒ‰BERTçš„æ–¹å¼å¤„ç†ï¼Œè€ŒéRoBERTaã€‚\n\n\n## å¼•ç”¨\nå¦‚æœæœ¬é¡¹ç›®ä¸­çš„èµ„æºæˆ–æŠ€æœ¯å¯¹ä½ çš„ç ”ç©¶å·¥ä½œæœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿åœ¨è®ºæ–‡ä¸­å¼•ç”¨ä¸‹è¿°è®ºæ–‡ã€‚\n- é¦–é€‰ï¼ˆæœŸåˆŠæ‰©å……ç‰ˆï¼‰ï¼šhttps://ieeexplore.ieee.org/document/9599397\n```\n@journal{cui-etal-2021-pretrain,\n  title={Pre-Training with Whole Word Masking for Chinese BERT},\n  author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing},\n  journal={IEEE Transactions on Audio, Speech and Language Processing},\n  year={2021},\n  url={https://ieeexplore.ieee.org/document/9599397},\n  doi={10.1109/TASLP.2021.3124365},\n }\n```\n\n- æˆ–è€…ï¼ˆä¼šè®®ç‰ˆæœ¬ï¼‰ï¼šhttps://www.aclweb.org/anthology/2020.findings-emnlp.58\n```\n@inproceedings{cui-etal-2020-revisiting,\n    title = \"Revisiting Pre-Trained Models for {C}hinese Natural Language Processing\",\n    author = \"Cui, Yiming  and\n      Che, Wanxiang  and\n      Liu, Ting  and\n      Qin, Bing  and\n      Wang, Shijin  and\n      Hu, Guoping\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.findings-emnlp.58\",\n    pages = \"657--668\",\n}\n```\n\n\n## è‡´è°¢\nç¬¬ä¸€ä½œè€…éƒ¨åˆ†å—åˆ°[**è°·æ­ŒTPU Research Cloud**](https://www.tensorflow.org/tfrc)è®¡åˆ’èµ„åŠ©ã€‚\n\n\n## å…è´£å£°æ˜\n**æœ¬é¡¹ç›®å¹¶éè°·æ­Œå®˜æ–¹å‘å¸ƒçš„Chinese BERT-wwmæ¨¡å‹ã€‚åŒæ—¶ï¼Œæœ¬é¡¹ç›®ä¸æ˜¯å“ˆå·¥å¤§æˆ–ç§‘å¤§è®¯é£çš„å®˜æ–¹äº§å“ã€‚**\næŠ€æœ¯æŠ¥å‘Šä¸­æ‰€å‘ˆç°çš„å®éªŒç»“æœä»…è¡¨æ˜åœ¨ç‰¹å®šæ•°æ®é›†å’Œè¶…å‚ç»„åˆä¸‹çš„è¡¨ç°ï¼Œå¹¶ä¸èƒ½ä»£è¡¨å„ä¸ªæ¨¡å‹çš„æœ¬è´¨ã€‚\nå®éªŒç»“æœå¯èƒ½å› éšæœºæ•°ç§å­ï¼Œè®¡ç®—è®¾å¤‡è€Œå‘ç”Ÿæ”¹å˜ã€‚\n**è¯¥é¡¹ç›®ä¸­çš„å†…å®¹ä»…ä¾›æŠ€æœ¯ç ”ç©¶å‚è€ƒï¼Œä¸ä½œä¸ºä»»ä½•ç»“è®ºæ€§ä¾æ®ã€‚ä½¿ç”¨è€…å¯ä»¥åœ¨è®¸å¯è¯èŒƒå›´å†…ä»»æ„ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œä½†æˆ‘ä»¬ä¸å¯¹å› ä½¿ç”¨è¯¥é¡¹ç›®å†…å®¹é€ æˆçš„ç›´æ¥æˆ–é—´æ¥æŸå¤±è´Ÿè´£ã€‚**\n\n\n## å…³æ³¨æˆ‘ä»¬\næ¬¢è¿å…³æ³¨å“ˆå·¥å¤§è®¯é£è”åˆå®éªŒå®¤å®˜æ–¹å¾®ä¿¡å…¬ä¼—å·ï¼Œäº†è§£æœ€æ–°çš„æŠ€æœ¯åŠ¨æ€ã€‚\n\n![qrcode.png](https://github.com/ymcui/cmrc2019/raw/master/qrcode.jpg)\n\n\n## é—®é¢˜åé¦ˆ\nå¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨GitHub Issueä¸­æäº¤ã€‚\n\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 28.4755859375,
          "content": "[**ä¸­æ–‡è¯´æ˜**](https://github.com/ymcui/Chinese-BERT-wwm/) | [**English**](https://github.com/ymcui/Chinese-BERT-wwm/blob/master/README_EN.md)\n\n## Chinese BERT with Whole Word Masking\nFor further accelerating Chinese natural language processing, we provide **Chinese pre-trained BERT with Whole Word Masking**. Meanwhile, we also compare the state-of-the-art Chinese pre-trained models in depth, including [BERT](https://github.com/google-research/bert)ã€[ERNIE](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE)ã€[BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm).\n\n- **[Pre-Training with Whole Word Masking for Chinese BERT](https://ieeexplore.ieee.org/document/9599397)**  \n- Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang\n- Published in *IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)*\n\nThis repository is developed based onï¼šhttps://github.com/google-research/bert\n\n----\n\n[Chinese LERT](https://github.com/ymcui/LERT) | [Chinese/English PERT](https://github.com/ymcui/PERT) [Chinese MacBERT](https://github.com/ymcui/MacBERT) | [Chinese ELECTRA](https://github.com/ymcui/Chinese-ELECTRA) | [Chinese XLNet](https://github.com/ymcui/Chinese-XLNet) | [Chinese BERT](https://github.com/ymcui/Chinese-BERT-wwm) | [TextBrewer](https://github.com/airaria/TextBrewer) | [TextPruner](https://github.com/airaria/TextPruner)\n\nMore resources by HFL: https://github.com/ymcui/HFL-Anthology\n\n## News\n**Mar 28, 2023 We open-sourced Chinese LLaMA&Alpaca LLMs, which can be quickly deployed on PC. Check: https://github.com/ymcui/Chinese-LLaMA-Alpaca**\n\n2022/10/29 We release a new pre-trained model called LERT, check https://github.com/ymcui/LERT/\n\n2022/3/30 We release a new pre-trained model called PERT, check https://github.com/ymcui/PERT \n\n2021/12/17 We release a model pruning toolkit - TextPruner, check https://github.com/airaria/TextPruner\n\n2021/1/27 All models support TensorFlow 2 now. Please use transformers library to access them or download from https://huggingface.co/hfl\n\n2020/9/15 Our paper [\"Revisiting Pre-Trained Models for Chinese Natural Language Processing\"](https://arxiv.org/abs/2004.13922) is accepted to [Findings of EMNLP](https://2020.emnlp.org) as a long paper.\n\n2020/8/27 We are happy to announce that our model is on top of GLUE benchmark, check [leaderboard](https://gluebenchmark.com/leaderboard).\n\n<details>\n<summary>Past News</summary>\n2020/3/23 The models in this repository now can be easily accessed through [PaddleHub](https://github.com/PaddlePaddle/PaddleHub), check [Quick Load](#Quick-Load)\n\n2020/2/26 We release a knowledge distillation toolkit [TextBrewer](https://github.com/airaria/TextBrewer)\n\n2020/1/20 Happy Chinese New Year! We've released RBT3 and RBTL3 (3-layer RoBERTa-wwm-ext-base/large), check [Small Models](#Small-Models)\n\n2019/12/19 The models in this repository now can be easily accessed through [Huggingface-Transformers](https://github.com/huggingface/transformers), check [Quick Load](#Quick-Load)\n\n2019/10/14 We release `RoBERTa-wwm-ext-large`, check [Download](#Download)\n\n2019/9/10 We release `RoBERTa-wwm-ext`, check [Download](#Download)\n\n2019/7/30 We release `BERT-wwm-ext`, which was trained on larger data, check [Download](#Download)\n\n2019/6/20 Initial version, pre-trained models could be downloaded through Google Drive, check [Download](#Download)\n</details>\n\n## Guide\n| Section | Description |\n|-|-|\n| [Introduction](#Introduction) | Introduction to BERT with Whole Word Masking (WWM) |\n| [Download](#Download) | Download links for Chinese BERT-wwm |\n| [Quick Load](#Quick-Load) | Learn how to quickly load our models through [ğŸ¤—Transformers](https://github.com/huggingface/transformers) or [PaddleHub](https://github.com/PaddlePaddle/PaddleHub) |\n| [Model Comparison](#Model-Comparison) | Compare the models published in this repository |\n| [Baselines](#Baselines) | Baseline results for several Chinese NLP datasets (partial) |\n| [Small Models](#Small-Models) | 3-layer Transformer models |\n| [Useful Tips](#Useful-Tips) | Provide several useful tips for using Chinese pre-trained models |\n| [English BERT-wwm](#English-BERT-wwm) | Download English BERT-wwm (by Google) |\n| [FAQ](#FAQ) | Frequently Asked Questions |\n| [Citation](#Citation) | Citation |\n\n\n## Introduction\n**Whole Word Masking (wwm)** is an upgraded version by [BERT](https://github.com/google-research/bert) released on late May 2019.\n\nThe following introductions are copied from BERT repository.\n```\nIn the original pre-processing code, we randomly select WordPiece tokens to mask. For example:\n\nInput Text: the man jumped up , put his basket on phil ##am ##mon ' s head \n\nOriginal Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head\n\nThe new technique is called Whole Word Masking. In this case, we always mask all of the the tokens corresponding to a word at once. The overall masking rate remains the same.\n\nWhole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head\n\nThe training is identical -- we still predict each masked WordPiece token independently. The improvement comes from the fact that the original prediction task was too 'easy' for words that had been split into multiple WordPieces.\n\n```\n\n**Important Note: Terminology `Masking` does not ONLY represent replace a word into `[MASK]` token.\nIt could also be in another form, such as `keep original word` or `randomly replaced by another word`.**\n\nIn the Chinese language, it is straightforward to utilize whole word masking, as traditional text processing in Chinese should include `Chinese Word Segmentation (CWS)`.\nIn the original `BERT-base, Chinese` by Google, the segmentation is done by splitting the Chinese characters while neglecting the importance of CWS.\nIn this repository, we utilize [Language Technology Platform (LTP)](http://ltp.ai) by Harbin Institute of Technology for CWS, and adapt whole word masking in Chinese text.\n\n\n## Download\nAs all models are 'BERT-base' variants, we do not incidate 'base' in the following model names.\n\n* **`BERT-base`**ï¼š12-layer, 768-hidden, 12-heads, 110M parameters\n\n| Model | Data | Google Drive | iFLYTEK Cloud |\n| :------- | :--------- | :---------: | :---------: |\n| **`RBT6, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | - | **[TensorFlowï¼ˆpw:hniyï¼‰](https://pan.baidu.com/s/1_MDAIYIGVgDovWkSs51NDA?pwd=hniy)** |\n| **`RBT4, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | - | **[TensorFlowï¼ˆpw:sjptï¼‰](https://pan.baidu.com/s/1MUrmuTULnMn3L1aw_dXxSA?pwd=sjpt)** |\n| **`RBTL3, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1Jzn1hYwmv0kXkfTeIvNT61Rn1IbRc-o8)**<br/>**[PyTorch](https://drive.google.com/open?id=1qs5OasLXXjOnR2XuGUh12NanUl0pkjEv)** | **[TensorFlowï¼ˆpw:s6cuï¼‰](https://pan.baidu.com/s/1vV9ClBMbsSpt8wUpfQz62Q?pwd=s6cu)** |\n| **`RBT3, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1-rvV0nBDvRCASbRz8M9Decc3_8Aw-2yi)**<br/>**[PyTorch](https://drive.google.com/open?id=1_LqmIxm8Nz1Abvlqb8QFZaxYo-TInOed)** | **[TensorFlowï¼ˆpw:5a57ï¼‰](https://pan.baidu.com/s/1AnapwWj1YBZ_4E6AAtj2lg?pwd=5a57)** |\n| **`RoBERTa-wwm-ext-large, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1dtad0FFzG11CBsawu8hvwwzU2R0FDI94)**<br/>**[PyTorch](https://drive.google.com/open?id=1-2vEZfIFCdM1-vJ3GD6DlSyKT4eVXMKq)** | **[TensorFlowï¼ˆpw:dqqeï¼‰](https://pan.baidu.com/s/1F68xzCLWEonTEVP7HQ0Ciw?pwd=dqqe)** |\n| **`RoBERTa-wwm-ext, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1jMAKIJmPn7kADgD3yQZhpsqM-IRM1qZt)** <br/>**[PyTorch](https://drive.google.com/open?id=1eHM3l4fMo6DsQYGmey7UZGiTmQquHw25)** | **[TensorFlowï¼ˆpw:vybqï¼‰](https://pan.baidu.com/s/1oR0cgSXE3Nz6dESxr98qVA?pwd=vybq)** |\n| **`BERT-wwm-ext, Chinese`** | **Wikipedia+Extended data<sup>[1]</sup>** | **[TensorFlow](https://drive.google.com/open?id=1buMLEjdtrXE2c4G1rpsNGWEx7lUQ0RHi)** <br/>**[PyTorch](https://drive.google.com/open?id=1iNeYFhCBJWeUsIlnW_2K6SMwXkM4gLb_)** | **[TensorFlowï¼ˆpw:wgntï¼‰](https://pan.baidu.com/s/1x-jIw1X2yNYHGak2yiq4RQ?pwd=wgnt)** |\n| **`BERT-wwm, Chinese`** | **Wikipedia** | **[TensorFlow](https://drive.google.com/open?id=1RoTQsXp2hkQ1gSRVylRIJfQxJUgkfJMW)** <br/>**[PyTorch](https://drive.google.com/open?id=1AQitrjbvCWc51SYiLN-cJq4e0WiNN4KY)** | **[TensorFlowï¼ˆpw:qfh8ï¼‰](https://pan.baidu.com/s/1HDdDXiYxGT5ub5OeO7qdWw?pwd=qfh8)** |\n| `BERT-base, Chinese`<sup>Google</sup> | Wikipedia | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) | - |\n| `BERT-base, Multilingual Cased`<sup>Google</sup>  | Wikipedia | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip) | - |\n| `BERT-base, Multilingual Uncased`<sup>Google</sup>  | Wikipedia | [Google Cloud](https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip) | - |\n\n### PyTorch Version\n\nIf you need these models in PyTorch,\n\n1) Convert TensorFlow checkpoint into PyTorch, using [ğŸ¤—Transformers](https://github.com/huggingface/transformers)\n\n2) Download from https://huggingface.co/hfl\n\nSteps: select one of the model in the page above â†’ click \"list all files in model\" at the end of the model page â†’ download bin/json files from the pop-up window\n\n### Note\n\nThe whole zip package roughly takes ~400M.\nZIP package includes the following files:\n\n```\nchinese_wwm_L-12_H-768_A-12.zip\n    |- bert_model.ckpt      # Model Weights\n    |- bert_model.meta      # Meta info\n    |- bert_model.index     # Index info\n    |- bert_config.json     # Config file\n    |- vocab.txt            # Vocabulary\n```\n\n`bert_config.json` and `vocab.txt` are identical to the original **`BERT-base, Chinese`** by Googleã€‚\n\n\n## Quick Load\n### Huggingface-Transformers\n\nWith [Huggingface-Transformers](https://github.com/huggingface/transformers), the models above could be easily accessed and loaded through the following codes.\n```\ntokenizer = BertTokenizer.from_pretrained(\"MODEL_NAME\")\nmodel = BertModel.from_pretrained(\"MODEL_NAME\")\n```\n**Notice: Please use BertTokenizer and BertModel for loading these model. DO NOT use RobertaTokenizer/RobertaModel!**\n\nThe actual model and its `MODEL_NAME` are listed below.\n\n| Original Model | MODEL_NAME |\n| - | - |\n| RoBERTa-wwm-ext-large | hfl/chinese-roberta-wwm-ext-large |\n| RoBERTa-wwm-ext | hfl/chinese-roberta-wwm-ext |\n| BERT-wwm-ext | hfl/chinese-bert-wwm-ext |\n| BERT-wwm | hfl/chinese-bert-wwm |\n| RBT3 | hfl/rbt3 |\n| RBTL3 | hfl/rbtl3 |\n\n### PaddleHub\n\nWith [PaddleHub](https://github.com/PaddlePaddle/PaddleHub), we can download and install the model with one line of code.\n\n```\nimport paddlehub as hub\nmodule = hub.Module(name=MODULE_NAME)\n```\n\nThe actual model and its `MODULE_NAME` are listed below.\n\n| Original Model | MODULE_NAME |\n| - | - |\n| RoBERTa-wwm-ext-large | [chinese-roberta-wwm-ext-large](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-roberta-wwm-ext-large&en_category=SemanticModel) |\n| RoBERTa-wwm-ext       | [chinese-roberta-wwm-ext](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-roberta-wwm-ext&en_category=SemanticModel) |\n| BERT-wwm-ext          | [chinese-bert-wwm-ext](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-bert-wwm-ext&en_category=SemanticModel) |\n| BERT-wwm              | [chinese-bert-wwm](https://www.paddlepaddle.org.cn/hubdetail?name=chinese-bert-wwm&en_category=SemanticModel) |\n| RBT3                  | [rbt3](https://www.paddlepaddle.org.cn/hubdetail?name=rbt3&en_category=SemanticModel) |\n| RBTL3                 | [rbtl3](https://www.paddlepaddle.org.cn/hubdetail?name=rbtl3&en_category=SemanticModel) |\n\n\n## Model Comparison\nWe list comparisons on the models that were released in this project.\n`~BERT` means to inherit the attributes from original Google's BERT.\n\n| - | BERT<sup>Google</sup> | BERT-wwm | BERT-wwm-ext | RoBERTa-wwm-ext | RoBERTa-wwm-ext-large |\n| :------- | :---------: | :---------: | :---------: | :---------: | :---------: |\n| Masking | WordPiece | WWM<sup>[1]</sup> | WWM | WWM | WWM |\n| Type | BERT-base | BERT-base | BERT-base | BERT-base | **BERT-large** |\n| Data Source | wiki | wiki | wiki+ext<sup>[2]</sup> | wiki+ext | wiki+ext |\n| Training Tokens # | 0.4B | 0.4B | 5.4B | 5.4B | 5.4B |\n| Device | TPU Pod v2 | TPU v3 | TPU v3 | TPU v3 | **TPU Pod v3-32<sup>[3]</sup>** |\n| Training Steps | ? | 100K<sup>MAX128</sup> <br/>+100K<sup>MAX512</sup> | 1M<sup>MAX128</sup> <br/>+400K<sup>MAX512</sup> | 1M<sup>MAX512</sup> | 2M<sup>MAX512</sup> |\n| Batch Size | ? | 2,560 / 384 | 2,560 / 384 | 384 | 512 |\n| Optimizer | AdamW | LAMB | LAMB | AdamW | AdamW |\n| Vocabulary | 21,128 | ~BERT<sup>[4]</sup> vocab | ~BERT vocab | ~BERT vocab | ~BERT vocab |\n| Init Checkpoint | Random Init | ~BERT weight | ~BERT weight | ~BERT weight | Random Init |\n\n\n## Baselines\nWe experiment on several Chinese datasets, including sentence-level to document-level tasks.\n\n**We only list partial results here and kindly advise the readers to read our [technical report](https://arxiv.org/abs/1906.08101).**\n\nBest Learning Rate:  \n\n| Model | BERT | ERNIE | BERT-wwm* |\n| :------- | :---------: | :---------: | :---------: |\n| CMRC 2018 | 3e-5 | 8e-5 | 3e-5 |\n| DRCD | 3e-5 | 8e-5 | 3e-5 |\n| CJRC | 4e-5 | 8e-5 | 4e-5 |\n| XNLI | 3e-5 | 5e-5 | 3e-5 |\n| ChnSentiCorp | 2e-5 | 5e-5 | 2e-5 |\n| LCQMC  | 2e-5 | 3e-5 | 2e-5 |\n| BQ Corpus | 3e-5 | 5e-5 | 3e-5 |\n| THUCNews | 2e-5 | 5e-5 | 2e-5 |\n* represents all related models (BERT-wwm, BERT-wwm-ext, RoBERTa-wwm-ext, RoBERTa-wwm-ext-large)\n\n\n- [**CMRC 2018**ï¼šSpan-Extraction Machine Reading Comprehension (Simplified Chinese)](https://github.com/ymcui/cmrc2018)\n- [**DRCD**ï¼šSpan-Extraction Machine Reading Comprehension (Traditional Chinese)](https://github.com/DRCSolutionService/DRCD)\n- [**CJRC**: Chinese Judiciary Reading Comprehension](http://cail.cipsc.org.cn)\n- [**XNLI**ï¼šNatural Langauge Inference](https://github.com/google-research/bert/blob/master/multilingual.md)\n- [**ChnSentiCorp**ï¼šSentiment Analysis](https://github.com/pengming617/bert_classification)\n- [**LCQMC**ï¼šSentence Pair Matching](http://icrc.hitsz.edu.cn/info/1037/1146.htm)\n- [**BQ Corpus**ï¼šSentence Pair Matching](http://icrc.hitsz.edu.cn/Article/show/175.html)\n- [**THUCNews**ï¼šDocument-level Text Classification](http://thuctc.thunlp.org)\n\n**Note: To ensure the stability of the results, we run 10 times for each experiment and report maximum and average scores.**\n\n**Average scores are in brackets, and max performances are the numbers that out of brackets.**\n\n### [CMRC 2018](https://github.com/ymcui/cmrc2018)\nCMRC 2018 dataset is released by Joint Laboratory of HIT and iFLYTEK Research.\nThe model should answer the questions based on the given passage, which is identical to SQuAD.\nEvaluation Metrics: EM / F1\n\n| Model | Development | Test | Challenge |\n| :------- | :---------: | :---------: | :---------: |\n| BERT | 65.5 (64.4) / 84.5 (84.0) | 70.0 (68.7) / 87.0 (86.3) | 18.6 (17.0) / 43.3 (41.3) |\n| ERNIE | 65.4 (64.3) / 84.7 (84.2) | 69.4 (68.2) / 86.6 (86.1) | 19.6 (17.0) / 44.3 (42.8) |\n| **BERT-wwm** | 66.3 (65.0) / 85.6 (84.7) | 70.5 (69.1) / 87.4 (86.7) | 21.0 (19.3) / 47.0 (43.9) |\n| **BERT-wwm-ext** | 67.1 (65.6) / 85.7 (85.0) | 71.4 (70.0) / 87.7 (87.0) | 24.0 (20.0) / 47.3 (44.6) |\n| **RoBERTa-wwm-ext** | 67.4 (66.5) / 87.2 (86.5) | 72.6 (71.4) / 89.4 (88.8) | 26.2 (24.6) / 51.0 (49.1) |\n| **RoBERTa-wwm-ext-large** | **68.5 (67.6) / 88.4 (87.9)** | **74.2 (72.4) / 90.6 (90.0)** | **31.5 (30.1) / 60.1 (57.5)** |\n\n\n### [DRCD](https://github.com/DRCKnowledgeTeam/DRCD)\nDRCD is also a span-extraction machine reading comprehension dataset, released by Delta Research Center. The text is written in Traditional Chinese.\nEvaluation Metrics: EM / F1\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 83.1 (82.7) / 89.9 (89.6) | 82.2 (81.6) / 89.2 (88.8) |\n| ERNIE | 73.2 (73.0) / 83.9 (83.8) | 71.9 (71.4) / 82.5 (82.3) |\n| **BERT-wwm** | 84.3 (83.4) / 90.5 (90.2) | 82.8 (81.8) / 89.7 (89.0) |\n| **BERT-wwm-ext** | 85.0 (84.5) / 91.2 (90.9) | 83.6 (83.0) / 90.4 (89.9) |\n| **RoBERTa-wwm-ext** | 86.6 (85.9) / 92.5 (92.2) | 85.6 (85.2) / 92.0 (91.7) |\n| **RoBERTa-wwm-ext-large** | **89.6 (89.1) / 94.8 (94.4)** | **89.6 (88.9) / 94.5 (94.1)** |\n\n\n### CJRC\n[**CJRC**](http://cail.cipsc.org.cn) is a Chinese judiciary reading comprehension dataset, released by Joint Laboratory of HIT and iFLYTEK Research. Note that, the data used in these experiments are NOT identical to the official one.\nEvaluation Metrics: EM / F1\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 54.6 (54.0) / 75.4 (74.5) | 55.1 (54.1) / 75.2 (74.3) |\n| ERNIE | 54.3 (53.9) / 75.3 (74.6) | 55.0 (53.9) / 75.0 (73.9) |\n| **BERT-wwm** | 54.7 (54.0) / 75.2 (74.8) | 55.1 (54.1) / 75.4 (74.4) |\n| **BERT-wwm-ext** | 55.6 (54.8) / 76.0 (75.3) | 55.6 (54.9) / 75.8 (75.0) |\n| **RoBERTa-wwm-ext** | 58.7 (57.6) / 79.1 (78.3) | 59.0 (57.8) / 79.0 (78.0) |\n| **RoBERTa-wwm-ext-large** | **62.1 (61.1) / 82.4 (81.6)** | **62.4 (61.4) / 82.2 (81.0)** |\n\n\n### XNLI\nWe use XNLI data for testing NLI task.\nEvaluation Metrics: Accuracy\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 77.8 (77.4) | 77.8 (77.5) |\n| ERNIE | 79.7 (79.4) | 78.6 (78.2) |\n| **BERT-wwm** | 79.0 (78.4) | 78.2 (78.0) |\n| **BERT-wwm-ext** | 79.4 (78.6) | 78.7 (78.3) |\n| **RoBERTa-wwm-ext** | 80.0 (79.2) | 78.8 (78.3) |\n| **RoBERTa-wwm-ext-large** | **82.1 (81.3)** | **81.2 (80.6)** |\n\n### ChnSentiCorp\nWe use ChnSentiCorp data for testing sentiment analysis.\nEvaluation Metrics: Accuracy\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 94.7 (94.3) | 95.0 (94.7) |\n| ERNIE | 95.4 (94.8) | 95.4 **(95.3)** |\n| **BERT-wwm** | 95.1 (94.5) | 95.4 (95.0) |\n| **BERT-wwm-ext** | 95.4 ï¼ˆ94.6) | 95.3 (94.7) |\n| **RoBERTa-wwm-ext** | 95.0 (94.6) | 95.6 (94.8) |\n| **RoBERTa-wwm-ext-large** | **95.8 (94.9)** | **95.8** (94.9) |\n\n\n### Sentence Pair Matchingï¼šLCQMC, BQ Corpus\n\n#### LCQMC\nEvaluation Metrics: Accuracy\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 89.4 (88.4) | 86.9 (86.4) |\n| ERNIE | 89.8 (89.6) | **87.2 (87.0)** |\n| **BERT-wwm** | 89.4 (89.2) | 87.0 (86.8) |\n| **BERT-wwm-ext** | 89.6 (89.2) | 87.1 (86.6) |\n| **RoBERTa-wwm-ext** | 89.0 (88.7) | 86.4 (86.1) |\n| **RoBERTa-wwm-ext-large** | **90.4 (90.0)** | 87.0 (86.8) |\n\n#### BQ Corpus \nEvaluation Metrics: Accuracy\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 86.0 (85.5) | 84.8 (84.6) |\n| ERNIE | 86.3 (85.5) | 85.0 (84.6) |\n| **BERT-wwm** | 86.1 (85.6) | 85.2 **(84.9)** |\n| **BERT-wwm-ext** | **86.4** (85.5) | 85.3 (84.8) |\n| **RoBERTa-wwm-ext** | 86.0 (85.4) | 85.0 (84.6) |\n| **RoBERTa-wwm-ext-large** | 86.3 **(85.7)** | **85.8 (84.9)** |\n\n\n### THUCNews\nReleased by Tsinghua University, which contains news in 10 categories.\nEvaluation Metrics: Accuracy\n\n| Model | Development | Test |\n| :------- | :---------: | :---------: |\n| BERT | 97.7 (97.4) | 97.8 (97.6) |\n| ERNIE | 97.6 (97.3) | 97.5 (97.3) |\n| **BERT-wwm** | 98.0 (97.6) | 97.8 (97.6) |\n| **BERT-wwm-ext** | 97.7 (97.5) | 97.7 (97.5) |\n| **RoBERTa-wwm-ext** | 98.3 (97.9) | 97.7 (97.5) |\n| **RoBERTa-wwm-ext-large** | 98.3 (97.7) | 97.8 (97.6) |\n\n### Small Models\nWe list RBT3 and RBTL3 results on several NLP tasks. Note that, we only list test set results.\n\n| Model | CMRC 2018 | DRCD | XNLI | CSC | LCQMC | BQ | Average | Params |\n| :------- | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| RoBERTa-wwm-ext-large | 74.2 / 90.6 | 89.6 / 94.5 | 81.2 | 95.8 | 87.0 | 85.8 | 87.335 | 325M |\n| RoBERTa-wwm-ext | 72.6 / 89.4 | 85.6 / 92.0 | 78.8 | 95.6 | 86.4 | 85.0 | 85.675 | 102M |\n| RBTL3 | 63.3 / 83.4 | 77.2 / 85.6 | 74.0 | 94.2 | 85.1 | 83.6 | 80.800 | 61M (59.8%) |\n| RBT3 | 62.2 / 81.8 | 75.0 / 83.9 | 72.3 | 92.8 | 85.1 | 83.3 | 79.550 | 38M (37.3%) |\n\nRelative performance:\n\n| Model | CMRC 2018 | DRCD | XNLI | CSC | LCQMC | BQ | Average | AVG-C |\n| :------- | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: | :---------: |\n| RoBERTa-wwm-ext-large | 102.2% / 101.3% | 104.7% / 102.7% | 103.0% | 100.2% | 100.7% | 100.9% | 101.9% | 101.2% |\n| RoBERTa-wwm-ext | 100% / 100% | 100% / 100% | 100% | 100% | 100% | 100% | 100% | 100% |\n| RBTL3 | 87.2% / 93.3% | 90.2% / 93.0% | 93.9% | 98.5% | 98.5% | 98.4% | 94.3% | 97.35% |\n| RBT3 | 85.7% / 91.5% | 87.6% / 91.2% | 91.8% | 97.1% | 98.5% | 98.0% | 92.9% | 96.35% |\n\n* AVG-C: average score of classification tasks: XNLI, CSC, LCQMC, BQ\n\n- The numbers of parameter are calculated based on XNLI classification task.\n- Relative parameter percentage is calculated based on RoBERTa-wwm-ext model.\n- RBT3: We use RoBERTa-wwm-ext for initializing the first three layers, and continue to train 1M steps.\n- RBTL3: We use RoBERTa-wwm-ext-large for initializing the first three layers, and continue to train 1M steps.\n- The name of RBT is the syllables of 'RoBERTa', and 'L' stands for large model.\n- Directly using the first three layers of RoBERTa-wwm-ext-large to fine-tune the downstream task will result in a bad performance. For example, in CMRC 2018 task we could only achieve 42.9/65.3, while RBTL3 could reach 63.3/83.4.\n\n\n## Useful Tips\n* Initial learning rate is the most important hyper-parameters (regardless of BERT or other neural networks), and should ALWAYS be tuned for better performance.\n* As shown in the experimental results, BERT and BERT-wwm share almost the same best initial learning rate, so it is straightforward to apply your initial learning rate in BERT to BERT-wwm. However, we find that ERNIE does not share the same characteristics, so it is STRONGLY recommended to tune the learning rate.\n* As BERT and BERT-wwm were trained on Wikipedia data, they show relatively better performance on the formal text. While, ERNIE was trained on larger data, including web text, which will be useful on casual text, such as Weibo (microblogs).\n* In long-sequence tasks, such as machine reading comprehension and document classification, we suggest using BERT or BERT-wwm.\n* As these pre-trained models are trained in general domains, if the task data is extremely different from the pre-training data (Wikipedia for BERT/BERT-wwm), we suggest taking another pre-training steps on the task data, which was also suggested by Devlin et al. (2019).\n* As there are so many possibilities in pre-training stage (such as initial learning rate, global training steps, warm-up steps, etc.), our implementation may not be optimal using the same pre-training data. Readers are advised to train their own model if seeking for another boost in performance. However, if it is unable to do pre-training, choose one of these pre-trained models which was trained on a similar domain to the downstream task.\n* When dealing with Traditional Chinese text, use BERT or BERT-wwm.\n\n\n## English BERT-wwm\nWe also repost English BERT-wwm (by Google official) here for your perusal.\n\n*   **[`BERT-Large, Uncased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n\n*   **[`BERT-Large, Cased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip)**:\n    24-layer, 1024-hidden, 16-heads, 340M parameters\n\n\n## FAQ\n**Q: How to use this model?**  \nA: Use it as if you are using original BERT. Note that, you don't need to do CWS for your text, as wwm only change the pre-training input but not the input for down-stream tasks.\n\n**Q: Do you have any plans to release the code?**  \nA: Unfortunately, I am not be able to release the code at the moment. As implementation is quite easy, I would suggest you to read #10 and #13.\n\n**Q: How can I download XXXXX dataset?**  \nA: We only provide the data that is publically available, check `data` directory. For copyright reasons, some of the datasets are not publically available. In that case, please search on GitHub or consult original authors for accessing.\n\n**Q: How to use this model?**  \nA: Use it as if you are using original BERT. Note that, you don't need to do CWS for your text, as wwm only change the pre-training input but not the input for down-stream tasks.\n\n**Q: Do you have any plans on releasing the larger model? Say BERT-large-wwm?**  \nA: If we could get significant gains from BERT-large, we will release a larger version in the future.\n\n**Q: You lier! I can not reproduce the result! ğŸ˜‚**  \nA: We use the simplist models in the downstream tasks. For example, in the classification task, we directly use `run_classifier.py` by Google. If you are not able to reach the average score that we reported, then there should be some bugs in your code. As there is randomness in reaching maximum scores, there is no guarantee that you will reproduce them.\n\n**Q: I could get better performance than you!**  \nA: Congratulations!\n\n**Q: How long did it take to train such a model?**  \nA: The training was done on Google Cloud TPU v3 with 128HBM, and it roughly takes 1.5 days. Note that, in the pre-training stage, we use [`LAMB Optimizer`](https://github.com/ymcui/LAMB_Optimizer_TF) which is optimized for the larger batch. In fine-tuning downstream task, we use normal `AdamWeightDecayOptimizer` as default.\n\n**Q: Who is ERNIE?**  \nA: The [ERNIE](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE) in this repository refer to the model released by Baidu, but not the one that published by Tsinghua University which was also called [ERNIE](https://github.com/thunlp/ERNIE).\n\n**Q: BERT-wwm does not perform well on some tasks.**  \nA: The aim of this project is to provide researchers with a variety of pre-training models.\nYou are free to choose one of these models.\nWe only provide experimental results, and we strongly suggest trying these models in your own task.\nOne more model, one more choice.\n\n**Q: Why not trying on more dataset?**  \nA: To be honest: 1) no time to find more data; 2) no need; 3) no money;\n\n**Q: Say something about these models**  \nA: Each has its own emphasis and merits. Development of Chinese NLP needs joint efforts.\n\n**Q: Any comments on the name of next generation of the pre-trained model?**  \nA: Maybe ZOE: Zero-shOt Embeddings from language model\n\n**Q: Tell me a little bit more about `RoBERTa-wwm-ext`**  \nA: integrate whole word masking (wwm) into RoBERTa model, specifically:  \n1) use whole word masking (but we did not use dynamic masking)  \n2) remove Next Sentence Prediction (NSP)\n3) directly use the data generated by `max_len=512` (but not from `max_len=128` for several steps then `max_len=512`)\n4) extended training steps (1M steps)\n\n## Citation\nIf you find the technical report or resource is useful, please cite our work in your paper.\n- Primary (Journal extension): https://ieeexplore.ieee.org/document/9599397  \n```\n@journal{cui-etal-2021-pretrain,\n  title={Pre-Training with Whole Word Masking for Chinese BERT},\n  author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing},\n  journal={IEEE Transactions on Audio, Speech and Language Processing},\n  year={2021},\n  url={https://ieeexplore.ieee.org/document/9599397},\n  doi={10.1109/TASLP.2021.3124365},\n }\n```\n- Secondary (conference paper): https://www.aclweb.org/anthology/2020.findings-emnlp.58\n```\n@inproceedings{cui-etal-2020-revisiting,\n    title = \"Revisiting Pre-Trained Models for {C}hinese Natural Language Processing\",\n    author = \"Cui, Yiming  and\n      Che, Wanxiang  and\n      Liu, Ting  and\n      Qin, Bing  and\n      Wang, Shijin  and\n      Hu, Guoping\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.findings-emnlp.58\",\n    pages = \"657--668\",\n}\n```\n\n## Disclaimer\n**This is NOT a project by Google official. Also, this is NOT an official product by HIT and iFLYTEK.**\nThe experiments only represent the empirical results in certain conditions and should not be regarded as the nature of the respective models. The results may vary using different random seeds, computing devices, etc. \n**The contents in this repository are for academic research purpose, and we do not provide any conclusive remarks. Users are free to use anythings in this repository within the scope of Apache-2.0 licence. However, we are not responsible for direct or indirect losses that was caused by using the content in this project.**\n\n\n## Acknowledgement\nThe first author of this project is partially supported by [Google TensorFlow Research Cloud (TFRC) Program](https://www.tensorflow.org/tfrc).\n\n## Issues\nIf there is any problem, please submit a GitHub Issue.\n\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "pics",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}