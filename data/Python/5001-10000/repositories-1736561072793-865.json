{
  "metadata": {
    "timestamp": 1736561072793,
    "page": 865,
    "hasNextPage": false,
    "endCursor": "Y3Vyc29yOjg2OA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "allenai/OLMo",
      "stars": 5010,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.06640625,
          "content": ".git\n.github\n.mypy_cache\n.pytest_cache\n.venv\n__pycache__\n*.egg-info\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0302734375,
          "content": "test_fixtures/*.json.gz binary\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.466796875,
          "content": "# build artifacts\n\n.eggs/\n.mypy_cache\n*.egg-info/\nbuild/\ndist/\npip-wheel-metadata/\n\n\n# dev tools\n\n.envrc\n.python-version\n.idea\n.venv/\n.vscode/\n/*.iml\npyrightconfig.json\n.ruff.toml\n\n\n# jupyter notebooks\n\n.ipynb_checkpoints\n\n\n# miscellaneous\n\n.cache/\ndoc/_build/\n*.swp\n.DS_Store\n\n# python\n\n*.pyc\n*.pyo\n__pycache__\n\n\n# testing and continuous integration\n\n.coverage\n.pytest_cache/\n.benchmarks\n\n# documentation build artifacts\n\ndocs/build\nsite/\n\n# runs\n/runs/\n/wandb/\n/scratch/\ncore\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 8.646484375,
          "content": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## Unreleased\n\n## [v0.6.0](https://github.com/allenai/OLMo/releases/tag/v0.6.0) - 2024-12-17\n\n### Added\n\n- A bunch of annealing configs\n- `constant_with_warmup` learning rate schedule\n- `one_in_eight` configuration for activation checkpointing\n- New tokenizer in the source instead of from huggingface\n- Improved support for GCS\n- `torch.compile()` now only compiles each block, not the whole model.\n- Support for `torch.compile()` with `dynamic=True`\n- Resetting the `torch.compile()` after every evaluation, because evaluation messes with the compiled versions\n- Added more in-loop evaluation tasks to pick from, mostly for scaling law.\n\n\n## [v0.5.1](https://github.com/allenai/OLMo/releases/tag/v0.5.1) - 2024-10-17\n\n### Added\n\n- Added ability to try loading latest checkpoint from save folder using `--try_load_latest_save`.\n- Added support for flash attention and gradient checkpointing to `hf_olmo`.\n- Added to `scripts.compare_wandb_configs.py` the ability to more easily compare differences in data mixes and evaluation tasks.\n- Added `effective_n_kv_heads` to OLMoConfig for hacky VLLM support.\n\n## [v0.5.0](https://github.com/allenai/OLMo/releases/tag/v0.5.0) - 2024-08-26\n\n- Fixed conversion to HuggingFace model for DDP-trained models.\n- Added support for remote source and destination for HuggingFace model conversion.\n\n### Added\n\n- Added support for document masking via flash-attn during training with `--data.generate_doc_lengths`.\n- Added config options for `model.norm_after`, `model.scale_emb_init`, and `auxiliary_loss_multiplier` (used with zloss).\n- Added scripts for running experiments on qk_norm, norm reordering, and zloss.\n- Added `model.rope_theta` configuration option.\n- Added `model.embedding_layer_norm` configuration option for adding a LN to the embeddings.\n- Added `model.emb_init_std` configuration option to override the standard deviation used to initialize the embeddings.\n- Added downstream eval task for requests dumped from oe-eval tasks\n- Added `CosLinearEnvelope` scheduler, which is a pointwise product of a cosine schedule and a linear decay.\n- Added ability to save outputs of submodules for debugging purposes.\n- Added a number of tasks from oe-eval to the downstream eval tasks.\n- Version dolma flan change in named_data_mix.py\n\n### Changed\n\n- Changed default distributed training strategy from single-GPU to FSDP\n- Fixed behavior of `effective_memmap_dtype` to prevent unrecognized dtypes to be parsed as `uint16`.\n\n### Fixed\n\n- Fixed restarting a training run in later epochs so that we no longer need to set the flag `--epoch=INT`.\n- Swapped in correct flan data mix.\n- Fix bug where the attention norm, when applied before the attention block, was modifying the residual stream.\n- Fixed `OLMo.from_checkpoint()` so that it correctly loads `olmo_core` and `torch_new` style checkpoints.\n- Fixed `preserve_rng_state` being incorrectly set to False when doing gradient checkpointing with dropout\n\n\n## [v0.4.0](https://github.com/allenai/OLMo/releases/tag/v0.4.0) - 2024-07-11\n\n### Added\n\n- Added clipping fix to `Optimizer` class to make it work with FSDP `no_shard` and DDP.\n- Added tests to compare grad norm differences between torch optimizer and clipping and OLMo optimizer and clipping on both CPU and GPU.\n- Expose memmap dtype in data config\n- Added support for DDP training.\n- Added caching to disk of HF datasets used in downstream evals\n- Added FLOPs logging\n- Added configs for OLMo tiny set of models\n- Added configuration field `optimizer.record_update_metrics`, which defaults to `False`, but when set to `True` will trigger AdamW to collect the step size norm and absolute max for each parameter.\n- Added configuration field `optimizer.selective_updates`, which defaults to `False`, but when set to `True` will tell the optimizer to skip updating the parameter and state when the corresponding gradient is 0.\n- Added configuration field `optimizer.record_update_metrics`, which defaults to `False`, but when set to True will trigger AdamW to collect the step size norm and absolute max for each parameter.\n- Added `olmo_data`, a package holding data files like tokenizers.\n- Added ability to load tokenizers from `olmo_data` package data.\n- Added a script that can run a series of models with predictable scaling properties.\n\n### Changed\n\n- Added original legacy unsharding implementation back, as the default. The new\nshared memory implementation can be used by passing `use_legacy_shared_mem_impl` to `unshard.py`.\n- Refactor weight initialization. IMPORTANT: this does not maintain backwards-compatibility with older configs; the jobs will still run, but may produce different outputs.\n- Changed the behavior of the Lion optimizer to only record the update cosine similarity when `optimizer.record_update_metrics` is `True` in order to be consistent with the API.\n- Added HF datasets into `olmo_data`, and changed downstream eval to load from the package.\n\n### Fixed\n\n- Changed from `ignored_index` to `ignore_index` for `cross_entropy_loss` when `flash-attn>=2.5.8`.\n- Make `hf_olmo` support `AutoModelForCasualLM` and similar HF methods again.\n\n## [v0.3.0](https://github.com/allenai/OLMo/releases/tag/v0.3.0) - 2024-04-25\n\n### Added\n\n- Added support for Grouped Query Attention.\n- Added commonsense_qa and social_iqa downstream evaluation tasks\n- Added ce_loss metric, with TriviaQA and NaturalQuestions tasks\n- Makes it possible to read from http/https the same way we read from s3/r2.\n- Added MMLU multiple choice (A/B/C/D) 5-shot variant downstream tasks\n- Tokenizer patch\n- Added option to specify number of model replicas when using hybrid sharding.\n\n### Changed\n\n- Rename `Olmo` to `OLMo` everywhere in the codebase\n- Disabled automatic garbage collection during training, instead we run manually at regular intervals to avoid ranks getting out-of-sync with their own gc.\n\n### Removed\n\n- Removed `AMDLayerNorm`, since the original layer norm bug has been fixed and we don't need this workaround anymore.\n- Removed `OLMoParallelBlock`.\n\n### Fixed\n\n- Don't log garbage on nodes that aren't rank 0\n- Don't crash in the HF code when we are referring to a tokenizer in a local file\n- Point official training scripts to publicly available URLs\n- Corrected the `resize_token_embeddings` method in the `OLMoForCausalLM` class to properly update the token embeddings when resizing the vocabulary.\n- Changed `tie_weights` method to a no-op as weight tying is handled in olmo/model.py\n- Fixed the size calculation for qk layer norm\n- Fixed pipeline test failure that occurs due to a bug in transformers version 4.39.1\n- Make `hf_olmo` compatible with transformers versions >=4.40.0\n\n## [v0.2.5](https://github.com/allenai/OLMo/releases/tag/v0.2.5) - 2024-03-06\n\n### Fixed\n\n- Fixed default value of `--tokenizer` argument to `scripts/prepare_tulu_data.py` to be an absolute path, not relative path, the script can be run from other directories.\n- Added the option to directly pass input embeddings to `OLMo` and `OLMoForCausalLM`.\n- Added support for Python 3.8.\n- Added code to throw an error if `output_attentions` is set to `True` in forward call to `OLMoForCausalLM`. This functionality hasn't been implemented yet.\n- Correct scheme displayed in error messages that come from R2\n- Fixed running with multiple data loading workers in LUMI\n- Minor bug fix: uninitialized prompts variable\n\n### Added\n- Added `output_hidden_states` argument and associated functionality to `OLMo` and `OLMoForCausalLM` to return model intermediate hidden states.\n- Ability to read from R2 like we read from S3\n- Added MMLU downstream evaluation tasks, with prompt variations.\n- Added support for PyTorch v2.2.\n- Added ability to show logs from all ranks\n- Added option for QKV clipping.\n- Added basic_arithmetic downstream evaluation task\n\n### Changed\n\n- Changed legacy checkpoint unsharding to use processes and shared memory instead of threads\n\n\n## [v0.2.4](https://github.com/allenai/OLMo/releases/tag/v0.2.4) - 2024-02-02\n\n### Fixed\n\n- Fixed an issue with the HuggingFace integration where we were inadvertently using a feature that was introduced in Python 3.10, causing an error for older Python versions.\n\n## [v0.2.3](https://github.com/allenai/OLMo/releases/tag/v0.2.3) - 2024-01-31\n\n## [v0.2.2](https://github.com/allenai/LLM/releases/tag/v0.2.2) - 2023-12-10\n\n## [v0.2.1](https://github.com/allenai/LLM/releases/tag/v0.2.1) - 2023-12-10\n\n## [v0.2.0](https://github.com/allenai/LLM/releases/tag/v0.2.0) - 2023-12-08\n\n### Added\n\n- GPT-based model.\n- Tokenizer and data pre-processing pipeline.\n- training script.\n- Triton-based FlashAttention.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0927734375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        https://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright {yyyy} {name of copyright owner}\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       https://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 4.0791015625,
          "content": "# If you update this, also update BEAKER_IMAGE in .github/workflows/main.yml\nIMAGE_NAME_BASE = olmo-torch2\n# If you update this, also update BEAKER_WORKSPACE in .github/workflows/main.yml\nBEAKER_WORKSPACE = ai2/llm-testing\n\nBEAKER_USER = $(shell beaker account whoami --format=json | jq -r '.[0].name')\nGANTRY_IMAGE = $(shell beaker workspace images $(BEAKER_WORKSPACE) --format=json | jq -r -c '.[] | select( .name == \"$(IMAGE_NAME_BASE)-gantry\" ) | .fullName')\nTEST_IMAGE =  $(shell beaker workspace images $(BEAKER_WORKSPACE) --format=json | jq -r -c '.[] | select( .name == \"$(IMAGE_NAME_BASE)-test\" ) | .fullName')\n\n.PHONY : run-checks\nrun-checks :\n\tisort --check .\n\tblack --check .\n\truff check .\n\tmypy .\n\tCUDA_VISIBLE_DEVICES='' pytest -v --color=yes tests/\n\n.PHONY : beaker-info\nbeaker-info :\n\t@echo \"Beaker user:   $(BEAKER_USER)\"\n\t@echo \"Gantry image:  $(GANTRY_IMAGE)\"\n\t@echo \"Testing image: $(TEST_IMAGE)\"\n\n.PHONY : images\nimages : gantry-image test-image\n\n.PHONY : base-image\nbase-image :\n\tdocker build -f docker/Dockerfile.base -t $(IMAGE_NAME_BASE)-base .\n\n.PHONY : gantry-image\ngantry-image :\n\tdocker build -f docker/Dockerfile.gantry -t $(IMAGE_NAME_BASE)-gantry .\n\tbeaker image create $(IMAGE_NAME_BASE)-gantry --name $(IMAGE_NAME_BASE)-gantry-tmp --workspace $(BEAKER_WORKSPACE)\n\tbeaker image delete $(GANTRY_IMAGE) || true\n\tbeaker image rename $(BEAKER_USER)/$(IMAGE_NAME_BASE)-gantry-tmp $(IMAGE_NAME_BASE)-gantry\n\n.PHONY : test-image\ntest-image : base-image\n\tdocker build -f docker/Dockerfile.test -t $(IMAGE_NAME_BASE)-test .\n\tbeaker image create $(IMAGE_NAME_BASE)-test --name $(IMAGE_NAME_BASE)-test-tmp --workspace $(BEAKER_WORKSPACE)\n\tbeaker image delete $(TEST_IMAGE) || true\n\tbeaker image rename $(BEAKER_USER)/$(IMAGE_NAME_BASE)-test-tmp $(IMAGE_NAME_BASE)-test\n\n.PHONY : lumi-image\nlumi-image :\n\tdocker build -f docker/Dockerfile.lumi -t ghcr.io/allenai/llm-lumi:$(shell git log -1 --pretty=format:%h) .\n\tdocker push ghcr.io/allenai/llm-lumi:$(shell git log -1 --pretty=format:%h)\n\n.PHONY : singularity-pull\nsingularity-pull :\n\tsingularity pull $PROJECT_DIR/containers/llm-lumi_$(TAG).sif docker://ghcr.io/allenai/llm-lumi:$(TAG)\n\n.PHONY : show-test-image\nshow-test-image :\n\t@echo $(TEST_IMAGE)\n\n.PHONY : show-gantry-image\nshow-gantry-image :\n\t@echo $(GANTRY_IMAGE)\n\n.PHONY : show-beaker-workspace\nshow-beaker-workspace :\n\t@echo $(BEAKER_WORKSPACE)\n\n.PHONY : gantry-test\ngantry-test :\n\tgantry run \\\n\t\t--workspace \"$(BEAKER_WORKSPACE)\" \\\n\t\t--priority \"normal\" \\\n\t\t--preemptible \\\n\t\t--beaker-image \"$(GANTRY_IMAGE)\" \\\n\t\t--gpus 1 \\\n\t\t--description \"Test run\" \\\n\t\t--cluster ai2/allennlp-cirrascale \\\n\t\t--cluster ai2/aristo-cirrascale \\\n\t\t--cluster ai2/mosaic-cirrascale \\\n\t\t--cluster ai2/mosaic-cirrascale-a100 \\\n\t\t--cluster ai2/prior-cirrascale \\\n\t\t--cluster ai2/s2-cirrascale \\\n\t\t--cluster ai2/general-cirrascale \\\n\t\t--cluster ai2/general-cirrascale-a100-80g-ib \\\n\t\t--cluster ai2/jupiter-cirrascale \\\n\t\t--cluster ai2/pluto-cirrascale \\\n\t\t--allow-dirty \\\n\t\t--venv base \\\n\t\t--timeout -1 \\\n\t\t--yes \\\n\t\t-- make check-cuda-install\n\n.PHONY : gantry-run-ib\ngantry-run-ib :\n\tgantry run \\\n\t\t--workspace \"$(BEAKER_WORKSPACE)\" \\\n\t\t--priority \"normal\" \\\n\t\t--preemptible \\\n\t\t--beaker-image \"$(GANTRY_IMAGE)\" \\\n\t\t--gpus 8 \\\n\t\t--description \"LLM Beaker IB Cluster Run\" \\\n\t\t--cluster ai2/general-cirrascale-a100-80g-ib \\\n\t\t--cluster ai2/jupiter-cirrascale \\\n\t\t--cluster ai2/pluto-cirrascale \\\n\t\t--nfs \\\n\t\t--env WORLD_SIZE=32 \\\n\t\t--env GPUS=8 \\\n\t\t--env NCCL_DEBUG=INFO \\\n\t\t--env SCRATCH_DIR=/tmp/scratch \\\n\t\t--env FLASH_DIR=/tmp/flash \\\n\t\t--env WANDB_PROJECT=olmo-beaker-ib \\\n\t\t--env-secret WANDB_API_KEY=WANDB_API_KEY \\\n\t\t--replicas 4 \\\n\t\t--leader-selection \\\n\t\t--host-networking \\\n\t\t--allow-dirty \\\n\t\t--venv base \\\n\t\t--yes \\\n\t\t-- /bin/bash -c 'torchrun --master-addr $$BEAKER_LEADER_REPLICA_HOSTNAME --master_port 1234 --nnodes 4 --node-rank $$BEAKER_REPLICA_RANK --nproc-per-node 8 scripts/train.py configs/c4-large.yaml'\n\n.PHONY : check-cpu-install\ncheck-cpu-install :\n\t@python -c 'from olmo import check_install; check_install(cuda=False)'\n\n.PHONY : check-cuda-install\ncheck-cuda-install :\n\t@python -c 'from olmo import check_install; check_install(cuda=True)'\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.0546875,
          "content": "<div align=\"center\">\n  <!-- <img src=\"https://github.com/allenai/OLMo/assets/8812459/774ac485-a535-4768-8f7c-db7be20f5cc3\" width=\"300\"/> -->\n  <img src=\"https://allenai.org/olmo/olmo-7b-animation.gif\" alt=\"OLMo Logo\" width=\"800\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n  <br>\n  <br>\n  <h1>OLMo: Open Language Model</h1>\n</div>\n<p align=\"center\">\n  <a href=\"https://github.com/allenai/OLMo/blob/main/LICENSE\">\n    <img alt=\"GitHub License\" src=\"https://img.shields.io/github/license/allenai/OLMo\">\n  </a>\n  <a href=\"https://github.com/allenai/OLMo/releases\">\n    <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/allenai/OLMo.svg\">\n  </a>\n  <a href=\"https://arxiv.org/pdf/2402.00838.pdf\">\n    <img alt=\"Paper URL\" src=\"https://img.shields.io/badge/arxiv-2402.00838-blue\">\n  </a>\n</p>\n\nOLMo is a repository for training and using AI2's state-of-the-art open language models. It is designed by scientists, for scientists.\n\n## Installation\n\nFirst, install [PyTorch](https://pytorch.org) following the instructions specific to your operating system.\n\nFor training and fine-tuning, we recommend installing from source:\n\n```bash\ngit clone https://github.com/allenai/OLMo.git\ncd OLMo\npip install -e .[all]\n```\nYou can also install from PyPI with:\n```bash\npip install ai2-olmo\n```\n\n## Pretraining\n\nOLMo pretraining follows a two-stage training procedure.\nIn the first stage, we train on large amounts of mostly web-based data: [OLMo-mix-1124](https://huggingface.co/datasets/allenai/olmo-mix-1124)\nIn the second stage, we train on a smaller amount of high-quality, targeted data: [Dolmino-mix-1124](https://huggingface.co/datasets/allenai/dolmino-mix-1124)\n\nYou can find *all* the checkpoints, at minimum every 1000 training steps, on Huggingface:\n * [Huggingface for the 7B variant](https://huggingface.co/allenai/OLMo-2-1124-7B)\n * [Huggingface for the 13B variant](https://huggingface.co/allenai/OLMo-2-1124-13B)\n\n### Steps to reproduce\n\nTo reproduce any of the training processes described below, run this:\n\n```bash\ntorchrun --nproc_per_node=8 scripts/train.py {path_to_train_config}\n```\n\nFor the training config, use any of the configs listed below.\n\nIf you want to override any of the settings in the training config without having to write a new config every time,\nyou can do this:\n\n```bash\ntorchrun --nproc_per_node=8 scripts/train.py {path_to_train_config} \\\n  --setting1=value \\\n  --setting2=value \\\n  --setting3.subsetting1=value\n```\n\nThe training configs below refer to training data that gets streamed in live over HTTP.\nTo reproduce at large scale, we recommend downloading the files locally and changing the paths to point to your\nlocal file system.\n\n*Note*: Some of the files that the training configs refer to are still being uploaded (as of 2024-11-27).\nThey should all appear in the next few days as the uploads complete.\n\n### Stage 1\n\nStage 1 is the biggest stage, where we train on 4T or 5T tokens on largely web-based data. \n\n|                 | OLMo2 7B                                                                                                          | OLMo2 13B                                                                                                          |\n|-----------------|-------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|\n| Number of tokens| 4 Trillion                                                                                                        | 5 Trillion                                                                                                         |\n| Checkpoint      | [stage1-step928646-tokens3896B](https://huggingface.co/allenai/OLMo-2-1124-7B/tree/stage1-step928646-tokens3896B) | [stage1-step596057-tokens5001B](https://huggingface.co/allenai/OLMo-2-1124-13B/tree/stage1-step596057-tokens5001B) | \n| Training config | [OLMo2-7B-stage1.yaml](configs/official-1124/OLMo2-7B-stage1.yaml)      | [OLMo2-13B-stage1.yaml](configs/official-1124/OLMo2-13B-stage1.yaml)     |\n| WandB           | wandb.ai/…/OLMo2-7B (link to come)                                                                                | wandb.ai/…/OLMo2-13B (link to come)                                                                                |\n\n### Stage 2 for the 7B\n\nFor the 7B model, we train three times with different data order on 50B high quality tokens, and then average (\"soup\") the models.\n\n|                        | Checkpoint                                                                                                                          | Training config                                                                        | WandB       |\n|------------------------|-------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|-------------|\n| random seed 42         | [stage2-ingredient1-step11931-tokens50B](https://huggingface.co/allenai/OLMo-2-1124-7B/tree/stage2-ingredient1-step11931-tokens50B) | [OLMo2-7B-stage2-seed42.yaml](configs/official-1124/OLMo2-7B-stage2-seed42.yaml)       | link to come |\n| random seed 42069      | [stage2-ingredient2-step11931-tokens50B](https://huggingface.co/allenai/OLMo-2-1124-7B/tree/stage2-ingredient2-step11931-tokens50B) | [OLMo2-7B-stage2-seed42069.yaml](configs/official-1124/OLMo2-7B-stage2-seed42069.yaml) | link to come |\n| random seed 666        | [stage2-ingredient3-step11931-tokens50B](https://huggingface.co/allenai/OLMo-2-1124-7B/tree/stage2-ingredient3-step11931-tokens50B) | [OLMo2-7B-stage2-seed666.yaml](configs/official-1124/OLMo2-7B-stage2-seed666.yaml)     | link to come |\n| **final souped model** | [main](https://huggingface.co/allenai/OLMo-2-1124-7B/tree/main) | no config, we just averaged the weights in Python                                      | |\n\nThe training configs linked here are set up to download the latest checkpoint after stage 1, and start training from there.\n\n### Stage 2 for the 13B\n\nFor the 13B model, we train three times with different data order on 100B high quality tokens, and one more time\non 300B high quality tokens. Then we average (\"soup\") the models.\n\n|                        | Checkpoint                                                                                                                             | Training config                                                                                  | WandB       |\n|------------------------|----------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|-------------|\n| random seed 1110, 100B | [stage2-ingredient1-step11931-tokens100B](https://huggingface.co/allenai/OLMo-2-1124-13B/tree/stage2-ingredient1-step11931-tokens100B) | [OLMo2-13B-stage2-seed1110-100B.yaml](configs/official-1124/OLMo2-13B-stage2-seed1110-100B.yaml) | link to come |\n| random seed 2662, 100B | [stage2-ingredient2-step11931-tokens100B](https://huggingface.co/allenai/OLMo-2-1124-13B/tree/stage2-ingredient2-step11931-tokens100B) | [OLMo2-13B-stage2-seed2662-100B.yaml](configs/official-1124/OLMo2-13B-stage2-seed2662-100B.yaml) | link to come |\n| random seed 6209, 100B | [stage2-ingredient3-step11931-tokens100B](https://huggingface.co/allenai/OLMo-2-1124-13B/tree/stage2-ingredient3-step11931-tokens100B) | [OLMo2-13B-stage2-seed6209-100B.yaml](configs/official-1124/OLMo2-13B-stage2-seed6209-100B.yaml) | link to come |\n| random seed 2662, 300B | [stage2-ingredient4-step11931-tokens300B](https://huggingface.co/allenai/OLMo-2-1124-13B/tree/stage2-ingredient4-step35773-tokens300B) | [OLMo2-13B-stage2-seed2662-300B.yaml](configs/official-1124/OLMo2-13B-stage2-seed2662-300B.yaml) | link to come |\n| **final souped model** | [main](https://huggingface.co/allenai/OLMo-2-1124-13B/tree/main)                                                                       | no config, we just averaged the weights in Python                                                | |\n\nThe training configs linked here are set up to download the latest checkpoint after stage 1, and start training from there.\n\n## Instruction tuned variants\n\nFor instruction tuned variants of these models, go to\n * [OLMo2 7B Instruct](https://huggingface.co/allenai/OLMo-2-1124-7B-Instruct)\n * [OLMo2 13B Instruct](https://huggingface.co/allenai/OLMo-2-1124-13B-Instruct)\n\n## Inference\n\nYou can use our Hugging Face integration to run inference on the OLMo Transformers checkpoints:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nolmo = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-2-1124-7B\")\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-2-1124-7B\")\nmessage = [\"Language modeling is \"]\ninputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n# optional verifying cuda\n# inputs = {k: v.to('cuda') for k,v in inputs.items()}\n# olmo = olmo.to('cuda')\nresponse = olmo.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\nprint(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n```\n\nAlternatively, with the Hugging Face pipeline abstraction:\n\n```python\nfrom transformers import pipeline\nolmo_pipe = pipeline(\"text-generation\", model=\"allenai/OLMo-2-1124-7B\")\nprint(olmo_pipe(\"Language modeling is\"))\n```\n\n### Quantization\n\n```python\nolmo = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-2-1124-7B\", torch_dtype=torch.float16, load_in_8bit=True)  # requires bitsandbytes\n```\n\nThe quantized model is sensitive to input types and CUDA handling. To avoid potential issues, we recommend explicitly converting input IDs to CUDA using: `inputs.input_ids.to('cuda')`\n\n## Evaluation\n\nAdditional tools for evaluating OLMo models are available at the [OLMo Eval](https://github.com/allenai/OLMo-eval) repo.\n\n## Modal.com Hosting\n\nAn example script is provided for hosting an OLMo 2 model on Modal.com using the OpenAI API in `./scripts/olmo2_modal_openai.py`.\nTo run that:\n\n1. Follow the instructions under Getting Started in [the Modal.com Guide](https://modal.com/docs/guide) to install\nthe Modal library and command line tools.</li>\n2. Follow the instructions under [Secrets](https://modal.com/docs/guide/secrets) in the Modal.com Guide to create a Modal secret named \"example-secret-token\"\nthat defines a value for the variable MODAL_TOKEN for your server.</li>\n3. Then run\n```bash\nmodal deploy ./scripts/olmo2_modal_openai.py\n```\n\nYou can check your endpoint using curl similar to the following:\n```bash\ncurl -X POST \\\n  -H \"Authorization: Bearer [the secret token from above]\" \\\n  -H \"Content-Type: application/json\" \\\n  -d @body.json \\\n  https://[the web endpoint modal creates above]/v1/chat/completions\n```\n\nwhere `body.json` is of the form:\n```\n{\n    \"model\": \"OLMo-2-1124-13B-Instruct\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Who was Alan Turing?\"\n        }\n      ],\n    \"max_tokens\": 100,\n    \"temperature\": 0.9,\n    \"stream\": true\n}\n```\n\n\n## Citing\n\n```bibtex\n@article{OLMo,\n  title={OLMo: Accelerating the Science of Language Models},\n  author={Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and A. Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Daniel Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hanna Hajishirzi},\n  year={2024},\n  url={https://api.semanticscholar.org/CorpusID:267365485},\n  journal={arXiv preprint},\n}\n```\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "conftest.py",
          "type": "blob",
          "size": 3.09765625,
          "content": "from typing import List\n\nimport pytest\n\nfrom olmo.config import (\n    DataConfig,\n    InitFnType,\n    ModelConfig,\n    OptimizerConfig,\n    PaddingDirection,\n    SchedulerConfig,\n    TokenizerConfig,\n    TrainConfig,\n)\nfrom olmo.tokenizer import Tokenizer\n\nTEST_MODEL = \"gpt2\"\n\nLOREM_IPSUM_1 = \"\"\"\nLorem ipsum dolor sit amet, consectetur adipiscing elit,\nsed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nUt enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip\nex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit\nesse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat\nnon proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n\"\"\"\n\nLOREM_IPSUM_2 = \"\"\"\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque\nlaudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi\narchitecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia\nvoluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores\neos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem\nipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius\nmodi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem.\nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit\nlaboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure\nreprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur,\nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\"\"\"\n\n\n@pytest.fixture(scope=\"function\")\ndef model_config() -> ModelConfig:\n    return ModelConfig(\n        vocab_size=50257,\n        eos_token_id=50256,\n        pad_token_id=50256,\n        d_model=128,\n        n_heads=2,\n        n_layers=3,\n        max_sequence_length=512,\n        init_fn=InitFnType.normal,\n    )\n\n\n@pytest.fixture(scope=\"function\")\ndef tokenizer() -> Tokenizer:\n    return Tokenizer.from_pretrained(TEST_MODEL)\n\n\n@pytest.fixture(scope=\"function\")\ndef train_config(tmp_path, model_config) -> TrainConfig:\n    return TrainConfig(\n        model=model_config,\n        optimizer=OptimizerConfig(),\n        scheduler=SchedulerConfig(),\n        data=DataConfig(\n            paths=[\n                \"test_fixtures/c4-sample.01.json.gz\",\n                \"test_fixtures/c4-sample.02.json.gz\",\n                \"test_fixtures/c4-sample.03.json.gz\",\n            ],\n            pad_direction=PaddingDirection.right,\n        ),\n        tokenizer=TokenizerConfig(identifier=TEST_MODEL),\n        save_folder=str(tmp_path / \"checkpoints\"),\n    )\n\n\n@pytest.fixture(scope=\"module\")\ndef eos_token_id(tokenizer: Tokenizer) -> int:\n    return tokenizer.eos_token_id\n\n\n@pytest.fixture(scope=\"module\")\ndef lorem_ipsum() -> str:\n    return LOREM_IPSUM_1.replace(\"\\n\", \" \").strip()\n\n\n@pytest.fixture(scope=\"module\")\ndef lorem_ipsum_docs() -> List[str]:\n    return [text.replace(\"\\n\", \" \").strip() for text in (LOREM_IPSUM_1, LOREM_IPSUM_2)]\n\n\n@pytest.fixture(scope=\"function\")\ndef model_path() -> str:\n    return \"test_fixtures/test-olmo-model\"\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "evaluation",
          "type": "tree",
          "content": null
        },
        {
          "name": "hf_olmo",
          "type": "tree",
          "content": null
        },
        {
          "name": "inference",
          "type": "tree",
          "content": null
        },
        {
          "name": "olmo",
          "type": "tree",
          "content": null
        },
        {
          "name": "olmo_data",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 3.25,
          "content": "[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ai2-olmo\"\ndynamic = [\"version\"]\nreadme = \"README.md\"\ndescription = \"Open Language Model (OLMo)\"\nauthors = [\n    { name = \"Allen Institute for Artificial Intelligence\", email = \"olmo@allenai.org\" }\n]\nrequires-python = \">=3.8\"\nlicense = { file = \"LICENSE\" }\ndependencies = [\n    \"numpy<2\",\n    \"torch>=2.1\",\n    \"ai2-olmo-core==0.1.0\",\n    \"omegaconf\",\n    \"rich\",\n    \"boto3\",\n    \"google-cloud-storage\",\n    \"tokenizers\",\n    \"packaging\",\n    \"cached_path>=1.6.2\",\n    \"transformers\",\n    \"importlib_resources\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"ruff\",\n    \"mypy>=1.0,<1.4\",\n    \"black>=23.1,<24.0\",\n    \"isort>=5.12,<5.13\",\n    \"pytest\",\n    \"pytest-sphinx\",\n    \"twine>=1.11.0\",\n    \"setuptools\",\n    \"wheel\",\n    \"build\",\n]\ntrain = [\n    \"wandb\",\n    \"beaker-gantry\",\n    \"click\",\n    \"torchmetrics\",\n    \"smashed[remote]>=0.21.1\",\n    \"safetensors\",\n    \"datasets\",\n    \"scikit-learn\",\n    \"msgspec>=0.14.0\",\n]\nall = [\n    \"ai2-olmo[dev,train]\",\n]\nfigures = [\n    \"matplotlib\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/allenai/OLMo\"\nRepository = \"https://github.com/allenai/OLMo\"\n\n[tool.setuptools]\ninclude-package-data = true\n\n[tool.setuptools.package-data]\nolmo = [\"py.typed\"]\nolmo_data = [\"**\"]\n\n[tool.setuptools.dynamic]\nversion = { attr = \"olmo.version.VERSION\" }\n\n[tool.setuptools.packages.find]\ninclude = [\"olmo*\", \"hf_olmo*\", \"olmo_data*\"]\nexclude = [\n    \"*.tests\",\n    \"*.tests.*\",\n    \"tests.*\",\n    \"tests\",\n    \"test_fixtures\",\n    \"test_fixtures.*\",\n    \"docs*\",\n    \"scripts*\",\n    \"olmo_tokenizer.*\",\n    \"evaluation.*\",\n    \"pretrain_data.*\",\n    \"tmp_*\",\n    \"inference.*\",\n]\n\n[tool.black]\nline-length = 115\ninclude = '\\.pyi?$'\nexclude = '''\n(\n      __pycache__\n    | \\.git\n    | \\.mypy_cache\n    | \\.pytest_cache\n    | \\.vscode\n    | \\.venv\n    | \\bdist\\b\n    | \\bdoc\\b\n    | pretrain_data/\n    | inference/\n)\n'''\n\n[tool.isort]\nprofile = \"black\"\nmulti_line_output = 3\nextend_skip = [\"pretrain_data\", \"tokenizer\"]\n\n[tool.ruff]\nline-length = 115\nlint.ignore = [\"F403\", \"F405\", \"E501\"]\nexclude = [\n    \".bzr\",\n    \".direnv\",\n    \".eggs\",\n    \".git\",\n    \".venv\",\n    \"venv\",\n    \".mypy_cache\",\n    \"__pycache__\",\n    \".nox\",\n    \".pants.d\",\n    \".pytype\",\n    \".ruff_cache\",\n    \".svn\",\n    \".tox\",\n    \"__pypackages__\",\n    \"_build\",\n    \"buck-out\",\n    \"build\",\n    \"dist\",\n    \"node_modules\",\n    \"doc\",\n    \"pretrain_data\",\n    \"inference\",\n]\n\n[tool.ruff.lint.per-file-ignores]\n\"**/__init__.py\" = [\"F401\"]\n\n[tool.pyright]\nreportPrivateImportUsage = false\nexclude = [\"pretrain_data/\", \"tokenizer/\"]\n\n[tool.mypy]\nignore_missing_imports = true\nno_site_packages = true\ncheck_untyped_defs = true\nexclude = [\"pretrain_data/\", \"inference/compression/dependencies/\", \"inference/efficiency/dependencies/\"]\nno_namespace_packages = true\n\n[[tool.mypy.overrides]]\nmodule = \"tests.*\"\nstrict_optional = false\n\n[tool.pytest.ini_options]\ntestpaths = \"tests/\"\npython_classes = [\n  \"Test*\",\n  \"*Test\",\n]\nlog_format = \"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"\nlog_level = \"DEBUG\"\nmarkers = [\n    \"gpu\",\n]\nfilterwarnings = [\n    'ignore::FutureWarning:huggingface_hub\\.file_download',\n    'ignore::DeprecationWarning:pkg_resources',\n    'ignore::DeprecationWarning:google\\.rpc',\n]\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "test_fixtures",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}