{
  "metadata": {
    "timestamp": 1736560525196,
    "page": 123,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "espnet/espnet",
      "stars": 8671,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.265625,
          "content": "[report]\nomit =\n    tools/*\n\n# Regexes for lines to exclude from consideration\nexclude_lines =\n    # Have to re-enable the standard pragma\n    pragma: no cover\n    if __name__ == \"__main__\":\n    if __name__ == '__main__':\n    @abstractmethod\n    raise NotImplementedError\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.2265625,
          "content": "# ignored folders\ndoc/\nsrc/\negs/\ntest/\ntools/kaldi\ntools/kaldi-io-for-python/\ntools/kaldi_github/\ntools/miniconda.sh\ntools/nkf/\ntools/venv/\ntools/warp-ctc/\ntools/warp-transducer/\ntools/chainer_ctc/\ntools/subword-nmt/\n\n.pytest_cache\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.619140625,
          "content": "# general\n*~\n*.pyc\n\\#*\\#\n.\\#*\n*DS_Store\ndummy_token_list\nempty.py\nout.txt\nespnet.egg-info/\ndoc/_build\nslurm-*.out\ntmp*\n.eggs/\n.hypothesis/\n.idea\n.pytest_cache/\n__pycache__/\ncheck_autopep8\n.coverage\n.coverage.*\nhtmlcov\ncoverage.xml*\ntest_utils/bats-core/\ntest_utils/bats-support/\ntest_utils/bats-assert/\nshellcheck*\ncheck_shellcheck*\ntest_spm.vocab\ntest_spm.model\n.vscode*\n*.vim\n*.swp\n*.nfs*\nconstraints.txt\n\nout/config.yaml\n\n# recipe related\negs*/*/*/data*\negs*/*/*/db\negs*/*/*/downloads\negs*/*/*/dump\negs*/*/*/enhan\negs*/*/*/exp\negs*/*/*/fbank\negs*/*/*/mfcc\negs*/*/*/stft\negs*/*/*/tensorboard\negs*/*/*/wav*\negs*/*/*/score*\negs*/*/*/nltk*\negs*/*/*/.cache*\negs*/*/*/pretrained_models*\negs*/fisher_callhome_spanish/*/local/mapping*\negs2/test/*\negs2/**/**/asset\negs2/**/**/checkpoint\negs2/**/**/flagged_data_points\negs2/**/**/hub\n\n# tools related\ntools/chainer\ntools/bin\ntools/include\ntools/lib\ntools/lib64\ntools/bats-core\ntools/chainer_ctc/\ntools/kaldi*\ntools/activate_python.sh\ntools/miniconda.sh\ntools/moses/\ntools/mwerSegmenter/\ntools/nkf/\ntools/venv/\ntools/sentencepiece/\ntools/swig/\ntools/warp-ctc/\ntools/warp-transducer/\ntools/*.done\ntools/PESQ*\ntools/hts_engine_API*\ntools/open_jtalk*\ntools/pyopenjtalk*\ntools/tdmelodic_openjtalk*\ntools/s3prl\ntools/sctk*\ntools/sph2pipe*\ntools/espeak-ng*\ntools/MBROLA*\ntools/festival*\ntools/speech_tools*\ntools/phonemizer*\ntools/py3mmseg\ntools/anaconda\ntools/ice-g2p\ntools/fairseq\ntools/RawNet\ntools/._*\ntools/ice-g2p*\ntools/fairseq*\ntools/featbin*\ntools/Miniconda*\ntools/miniconda\ntools/ffmpeg-*-static\ntools/ffmpeg-release\ntools/ffmpeg-release-*.tar.xz\ntools/Miniconda3-latest-*.sh\ntools/Miniforge3-*.sh\ntools/condarc\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": ".mergify.yml",
          "type": "blob",
          "size": 4.8662109375,
          "content": "pull_request_rules:\n  - name: automatic merge if label=auto-merge\n    conditions:\n      - \"label=auto-merge\"\n      - \"check-success=unit_test_espnet1_and_espnet2_on_centos7\"\n      - \"check-success=unit_test_espnet1_and_espnet2_on_debian11\"\n      - \"check-success=check_installable_on_windows (3.10, 2.3.0)\"\n      - \"check-success=check_installable_on_macos (3.10, 2.1.2, true)\"\n      - \"check-success=check_installable_on_macos (3.10, 2.1.2, false)\"\n      - \"check-success=unit_test_espnet1_and_integration_test_espnet1 (ubuntu-latest, 3.7, 1.13.1, false, 6.0.0)\"\n      - \"check-success=unit_test_espnet1_and_integration_test_espnet1 (ubuntu-latest, 3.10, 2.4.0, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.7, 1.13.1, false, 6.0.0)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.8, 2.0.1, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.9, 2.0.1, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.10, 2.0.1, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.8, 2.1.2, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.9, 2.1.2, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.10, 2.1.2, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.8, 2.2.2, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.9, 2.2.2, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.10, 2.2.2, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.8, 2.3.1, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.9, 2.3.1, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.10, 2.3.1, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.8, 2.4.0, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.9, 2.4.0, 6.0.0, false)\"\n      - \"check-success=unit_test_espnet2_and_integration_test_espnet2 (ubuntu-latest, 3.10, 2.4.0, 6.0.0, false)\"\n      - \"check-success=test_configuration_espnet2 (ubuntu-latest, 3.7, 1.13.1, 6.0.0, false)\"\n      - \"check-success=test_configuration_espnet2 (ubuntu-latest, 3.10, 2.4.0, false, 6.0.0)\"\n      - \"check-success=test_import (ubuntu-latest, 3.10, 2.4.0)\"\n      - \"check-success=check_kaldi_symlinks\"\n    actions:\n      merge:\n        method: merge\n  - name: delete head branch after merged\n    conditions:\n      - merged\n    actions:\n      delete_head_branch: {}\n  - name: \"add label=auto-merge for PR by mergify\"\n    conditions:\n      - author=mergify[bot]\n    actions:\n      label:\n        add: [\"auto-merge\"]\n  - name: warn on conflicts\n    conditions:\n      - conflict\n    actions:\n      comment:\n        message: This pull request is now in conflict :(\n      label:\n        add: [\"conflicts\"]\n  - name: unlabel conflicts\n    conditions:\n      - -conflict\n    actions:\n      label:\n        remove: [\"conflicts\"]\n  - name: \"auto add label=ESPnet1\"\n    conditions:\n      - files~=^(espnet/|egs/)\n    actions:\n      label:\n        add: [\"ESPnet1\"]\n  - name: \"auto add label=ESPnet2\"\n    conditions:\n      - files~=^(espnet2/|egs2/)\n    actions:\n      label:\n        add: [\"ESPnet2\"]\n  - name: \"auto add label=ASR\"\n    conditions:\n      - files~=^(espnet*/asr|egs*/*/asr1)\n    actions:\n      label:\n        add: [\"ASR\"]\n  - name: \"auto add label=TTS\"\n    conditions:\n      - files~=^(espnet*/tts|egs*/*/tts1)\n    actions:\n      label:\n        add: [\"TTS\"]\n  - name: \"auto add label=MT\"\n    conditions:\n      - files~=^(espnet*/mt|egs*/*/mt1)\n    actions:\n      label:\n        add: [\"MT\"]\n  - name: \"auto add label=LM\"\n    conditions:\n      - files~=^(espnet*/lm)\n    actions:\n      label:\n        add: [\"LM\"]\n  - name: \"auto add label=README\"\n    conditions:\n      - files~=README.md\n    actions:\n      label:\n        add: [\"README\"]\n  - name: \"auto add label=Documentation\"\n    conditions:\n      - files~=^doc/\n    actions:\n      label:\n        add: [\"Documentation\"]\n  - name: \"auto add label=CI\"\n    conditions:\n      - files~=^(ci/|.github/)\n    actions:\n      label:\n        add: [\"CI\"]\n  - name: \"auto add label=Installation\"\n    conditions:\n      - files~=^(tools/|setup.py)\n    actions:\n      label:\n        add: [\"Installation\"]\n  - name: \"auto add label=mergify\"\n    conditions:\n      - files~=^.mergify.yml\n    actions:\n      label:\n        add: [\"mergify\"]\n  - name: \"auto add label=Docker\"\n    conditions:\n      - files~=^docker/\n    actions:\n      label:\n        add: [\"Docker\"]\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.1748046875,
          "content": "# See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.6.0\n    hooks:\n    -   id: trailing-whitespace\n        exclude: ^(egs2/TEMPLATE/asr1/utils|egs2/TEMPLATE/asr1/steps|egs2/TEMPLATE/tts1/sid|tools/installers/patch_mwerSegmenter)\n    -   id: end-of-file-fixer\n        exclude: ^(egs2/TEMPLATE/asr1/utils|egs2/TEMPLATE/asr1/steps|egs2/TEMPLATE/tts1/sid|tools/installers/patch_mwerSegmenter)\n    -   id: check-yaml\n        exclude: ^(egs2/TEMPLATE/asr1/utils|egs2/TEMPLATE/asr1/steps|egs2/TEMPLATE/tts1/sid|tools/installers/patch_mwerSegmenter)\n    -   id: check-added-large-files\n        exclude: ^(egs2/TEMPLATE/asr1/utils|egs2/TEMPLATE/asr1/steps|egs2/TEMPLATE/tts1/sid|tools/installers/patch_mwerSegmenter)\n\n-   repo: https://github.com/psf/black\n    rev: 24.8.0\n    hooks:\n    -   id: black\n        exclude: ^(egs2/TEMPLATE/asr1/utils|egs2/TEMPLATE/asr1/steps|egs2/TEMPLATE/tts1/sid|doc)\n\n-   repo: https://github.com/pycqa/isort\n    rev: 5.13.2\n    hooks:\n    -   id: isort\n        exclude: ^(egs2/TEMPLATE/asr1/utils|egs2/TEMPLATE/asr1/steps|egs2/TEMPLATE/tts1/sid|doc)\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 14.31640625,
          "content": "# How to contribute to ESPnet\n\n## 1. What to contribute\nIf you are interested in contributing to ESPnet, your contributions will fall into three categories: major features, minor updates, and recipes.\n\n### 1.1 Major features\n\nIf you want to ask or propose a new feature, please first open a new issue with the tag `Feature request`\nor directly contact Shinji Watanabe <shinjiw@ieee.org> or other main developers. Each feature implementation\nand design should be discussed and modified according to ongoing and future works.\nYou can find ongoing major development plans at https://github.com/espnet/espnet/milestones\nor at https://github.com/espnet/espnet/issues (pinned issues)\n\n### 1.2 Minor Updates (minor feature, bug-fix for an issue)\n\nIf you want to propose a minor feature, update an existing minor feature, or fix a bug, please first take a look at\nthe existing [issues](https://github.com/espnet/espnet/pulls) and/or [pull requests](https://github.com/espnet/espnet/pulls).\nPick an issue and comment on the task that you want to work on this feature.\n\nIf you need help or additional information to propose the feature, you can open a new issue with the tag `Discussion` and ask ESPnet members.\n\n### 1.3 Recipes\n\nESPnet provides and maintains many example scripts, called `recipes`, demonstrating how to\nuse the toolkit.  The recipes for ESPnet1 are put under `egs` directory, while ESPnet2 ones are put under `egs2`.\nLike Kaldi, each subdirectory of `egs` and `egs2` corresponds to a corpus, which we have example scripts for.\n\n#### 1.3.1 ESPnet1 recipes\n\nESPnet1 recipes (`egs/X`) follow the convention from [Kaldi](https://github.com/kaldi-asr/kaldi) and may rely on\nseveral utilities available in Kaldi. As such, porting a new recipe from Kaldi to ESPnet is natural, and the user\nmay refer to [port-kaldi-recipe](https://github.com/espnet/espnet/wiki/How-to-port-the-Kaldi-recipe-to-the-ESPnet-recipe%3F)\nand other existing recipes for new additions. For the Kaldi-style recipe architecture, please refer to\n[Prepare-Kaldi-Style-Directory](https://kaldi-asr.org/doc/data_prep.html).\n\nFor each recipe, we ask you to report experimental results, environment, and model information.\nFor reproducibility, a link to upload the pre-trained model may also be added. All this information should be written\nin a markdown file called `RESULTS.md` and put at the recipe root. You can refer to\n[tedlium2-example](https://github.com/espnet/espnet/blob/master/egs/tedlium2/asr1/RESULTS.md) for an example.\n\nTo generate `RESULTS.md` for a recipe, please follow the following instructions:\n- Execute `~/espnet/utils/show_result.sh` at the recipe root (where `run.sh` is located).\nYou'll get your environment information and evaluation results for each experiment in a markdown format.\nFrom here, you can copy or redirect text output to `RESULTS.md`.\n- Execute `~/espnet/utils/pack_model.sh` at the recipe root to generate a packed ESPnet model called `model.tar.gz`\nand output model information. Executing the utility script without argument will give you the expected arguments.\n- Put the model information in `RESULTS.md` and the model link if you're using a private web storage\n- If you don't have private web storage, please contact Shinji Watanabe <shinjiw@ieee.org> to give you access to ESPnet storage.\n\n#### 1.3.2 ESPnet2 recipes\n\nESPnet2's recipes correspond to `egs2`. ESPnet2 applies a new paradigm without dependencies on Kaldi's binaries, which makes it lighter and more generalized.\nFor ESPnet2, we do not recommend preparing the recipe's stages for each corpus but using the common pipelines, we provided in `asr.sh`, `tts.sh`, and\n`enh.sh`. For details on creating ESPnet2 recipes, please refer to [egs2-readme](https://github.com/espnet/espnet/blob/master/egs2/TEMPLATE/README.md).\n\nThe common pipeline of ESPnet2 recipes will take care of the `RESULTS.md` generation, model packing, and uploading. ESPnet2 models are maintained at Hugging Face and Zenodo (Deprecated).\nYou can also refer to the document at https://github.com/espnet/espnet_model_zoo\n\nTo port models from zenodo using Hugging Face hub,\n1. Create a Hugging Face account - https://huggingface.co/\n2. Request to be added to espnet organization - https://huggingface.co/espnet\n3. Go to `egs2/RECIPE/*` and run `./scripts/utils/upload_models_to_hub.sh \"ZENODO_MODEL_NAME\"`\n\nTo upload models using Huggingface-cli conduct the following steps:\nYou can also refer to https://huggingface.co/docs/transformers/model_sharing\n1. Create a Hugging Face account - https://huggingface.co/\n2. Request to be added to espnet organization - https://huggingface.co/espnet\n3. Run huggingface-cli login (You can get the token request at this step under setting > Access Tokens > espnet token\n4. `huggingface-cli repo create your-model-name --organization espnet`\n5. `git clone https://huggingface.co/username/your-model-name` (clone this outside ESPNet to avoid issues as this a git repo)\n6. `cd your-model-name`\n7. `git lfs install`\n8. copy contents from `exp` directory of your recipe into this directory (Check other models of similar tasks under ESPNet to confirm your directory structure)\n9. `git add . `\n10. `git commit -m \"Add model files\"`\n11. `git push`\n12. Check if the inference demo on HF is running successfully to verify the upload\n\n#### 1.3.3 Additional requirements for a new recipe\n\n- Common/shared files and directories such as `utils`, `steps`, `asr.sh`, etc., should be linked using\na symbolic link (e.g., `ln -s <source-path> <target-path>`). Please refer to existing recipes if you're\nunaware of which files/directories are shared. Noted that in espnet2, some of them are automatically generated by https://github.com/espnet/espnet/blob/master/egs2/TEMPLATE/asr1/setup.sh.\n- Default training and decoding configurations (i.e., the default one in `run.sh`) should be named respectively `train.yaml`\nand `decode.yaml` and put in `conf/`. Additional or variant configurations should be put in `conf/tuning/` and named accordingly\nto their differences.\n- If a recipe for a new corpus is proposed, you should add its name and information to:\nhttps://github.com/espnet/espnet/blob/master/egs/README.md if it's an ESPnet1 recipe,\nor https://github.com/espnet/espnet/blob/master/egs2/README.md + `db.sh` if it's an ESPnet2 recipe.\n\n#### 1.3.4 Checklist before you submit the recipe-based PR\n\n- [ ] be careful about the name of the recipe. It is recommended to follow the naming conventions of the other recipes\n- [ ] common/shared files are linked with **symbolic link** (see Section 1.3.3)\n- [ ] cluster settings should be set as **default** (e.g., `cmd.sh` `conf/slurm.conf` `conf/queue.conf` `conf/pbs.conf`)\n- [ ] update `egs/README.md` or `egs2/README.md` with the corresponding recipes\n- [ ] add the corresponding entry in `egs2/TEMPLATE/*/db.sh` for a new corpus\n- [ ] try to **simplify** the model configurations. We recommend having only the best configuration for the start of a recipe. Please also follow the default rule defined in Section 1.3.3\n- [ ] large meta-information (e.g., the keyword list) for a corpus should be maintained elsewhere other than in the recipe itself\n- [ ] also recommend including results and pre-trained models with the recipe\n- Note that we recommend the users to use the latest `black` and `isort` formattings. However, these formattings are automatically performed by `pre-commit.ci` and are no longer a requirement.\n\n## 2 Pull Request\nIf your proposed feature or bugfix is ready, please open a Pull Request (PR) at https://github.com/espnet/espnet\nor use the Pull Request button in your forked repo. If you're not familiar with the process, please refer to the following guides:\n\n- http://stackoverflow.com/questions/14680711/how-to-do-a-github-pull-request\n- https://help.github.com/articles/creating-a-pull-request/\n\n## 3 Version policy and development branches\n\n1. After v10.6, we moved our version policy with the year and date-based specifiers, e.g., v.202204 means April 2022.\n\n2. The version number will be updated when we regularly (e.g., every two months or so) or we have significant changes.\n\n## 4 Unit testing\n\nESPnet's testing is located under `test/`.  You can install additional packages for testing as follows:\n``` console\n$ cd <espnet_root>\n$ . ./tools/activate_python.sh\n$ pip install -e \".[test]\"\n```\nIn order to thoroughly test various units, it is necessary to install several modules and tools. We suggest that you review the contents of [install.sh](https://github.com/espnet/espnet/blob/master/ci/install.sh), as it includes all the modules and libraries required for the CI check.\n\n### 4.1 Python\n\nThen, you can run the entire test suite using [pytest](https://docs.pytest.org/en/latest/) with [coverage](https://pytest-cov.readthedocs.io/en/latest/reporting.html) by\n``` console\n./ci/test_python_espnet1.sh\n./ci/test_python_espnet2.sh\n```\nThe followings are some useful tips when you are using `pytest`:\n- New test file should be put under `test/` directory and named `test_xxx.py`. Each method in the test file should\nhave the format `def test_yyy(...)`.  [Pytest](https://docs.pytest.org/en/latest/) will automatically find and test them.\n- We recommend adding several small test files instead of grouping them in one big file (e.g., `test_e2e_xxx.py`).\nTechnically, a test file should only cover methods from one file (e.g., `test_transformer_utils.py` to test `transformer_utils.py`).\n- To monitor the test coverage and avoid the overlapping test, we recommend using  `pytest --cov-report term-missing <test_file|dir>`\nto highlight covered and missed lines. For more details, please refer to [coverage-test](https://pytest-cov.readthedocs.io/en/latest/readme.html).\n- We limit test running time to 2.0 seconds (see: [pytest-timeouts](https://pypi.org/project/pytest-timeouts/)) for each trial. Thus, we recommend using small model parameters and avoiding dynamic imports, file access, and unnecessary loops.\nIf a unit test needs more running time, you can annotate your test with `@pytest.mark.execution_timeout(sec)`.\n- For test initialization (parameters, modules, etc.), you can use `pytest` fixtures. Refer to  [pytest fixtures](https://docs.pytest.org/en/latest/fixture.html#using-fixtures-from-classes-modules-or-projects) for more information.\n\nIn addition, please follow the [PEP 8 convention](https://peps.python.org/pep-0008/) for the coding style and [Google's convention for docstrings](https://google.github.io/styleguide/pyguide.html#383-functions-and-methods).\n\n### 4.2 Bash scripts\n\nYou can also test the scripts in `utils` with [bats-core](https://github.com/bats-core/bats-core) and [shellcheck](https://github.com/koalaman/shellcheck).\n\nTo test:\n\n``` console\n./ci/test_shell_espnet1.sh\n./ci/test_shell_espnet2.sh\n```\n\n## 5 Integration testing\n\nWrite new integration tests in [ci/test_integration_espnet1.sh](ci/test_integration_espnet1.sh) or [ci/test_integration_espnet2.sh](ci/test_integration_espnet2.sh) when you add new features in [espnet/bin](espnet/bin) or [espnet2/bin](espnet2/bin), respectively. They use our smallest dataset [egs/mini_an4](egs/mini_an4) or [egs2/mini_an4](egs/mini_an4) to test `run.sh`. **Don't call `python` directly in integration tests. Instead, use `coverage run --append`** as a python interpreter. Especially, `run.sh` should support `--python ${python}` to call the custom interpreter.\n\n```bash\n# ci/test_integration_espnet{1,2}.sh\n\npython=\"coverage run --append\"\n\ncd egs/mini_an4/your_task\n./run.sh --python \"${python}\"\n\n```\n\n### 5.1 Configuration files\n\n- [setup.cfg](setup.cfg) configures pytest, black and flake8.\n- [.github/workflows](.github/workflows/) configures Github Actions (unittests, integration tests).\n- [codecov.yml](codecov.yml) configures CodeCov (code coverage).\n\n### 5.2 Integration testing Locally (Github Actions locally)\n\nYou can test if your PR complies with the integrations test before pushing a new update using [act](https://github.com/nektos/act).\n\n### 5.2.1 Installation\n\nTo execute **act**:\n\n1. You first need to install [Docker](https://docs.docker.com/engine/install/) on your local PC. Do not forget to login into docker using `docker login`.\n\n2. Install Github CLI. The [instructions](https://github.com/cli/cli#installation) will change depending on your OS. For Linux, you can use the official sources to install the corresponding [package](https://github.com/cli/cli/blob/trunk/docs/install_linux.md#official-sources).\n\n3. Finally, install **act** through [package managers](https://github.com/nektos/act#installation-through-package-managers) or using the [Github CLI](https://github.com/nektos/act#installation-as-github-cli-extension). For Linux, you can use the command: `gh extension install https://github.com/nektos/gh-act`\n\n### 5.2.2 Usage\n\nIn a command console:\n\n```bash\ncd <root_dir_espnet_clone>  # go to the root directory of your clone\ngh act\n```\n\nThe program will start running all the CI tests that will run in the GitHub Action server.\n\nFor specific jobs/workflow, you can use:\n\n```bash\n# For jobs:\ngh act -j <jobID>  # Where jobID is a string.\n\n# For workflows:\ngh act -W <workflowID>\ngh act -W .github/workflows/<filename>.yml\n```\n\nList the available jobID/workflowID with: `gh act -l`.\nYou can get the list of workflow files from `ls .github/workflows`.\n\n## 6 Writing new tools\n\nYou can place your new tools under\n- `espnet/bin` or `espnet2/bin`: heavy and large (e.g., neural network related) core tools.\n- `utils`: lightweight, self-contained python/bash scripts.\n\nFor `utils` scripts, do not forget to add help messages and test scripts under `test_utils`.\n\n### 6.1 Python tools guideline\n\nTo generate a doc, do not forget `def get_parser(): -> ArgumentParser` in the main file.\n\n```python\n#!/usr/bin/env python3\n# Copyright XXX\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\nimport argparse\n\n# NOTE: do not forget this\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=\"awsome tool\",  # DO NOT forget this\n    )\n    ...\n    return parser\n\nif __name__ == '__main__':\n    args = get_parser().parse_args()\n    ...\n```\n\n### 6.2 Bash tools guideline\n\nTo generate a doc, support `--help` to show its usage. If you use Kaldi's `utils/parse_option.sh`, define `help_message=\"Usage: $0 ...\"`.\n\n\n## 7 Writing documentation\n\nSee [doc](doc/README.md).\n\n## 8 On CI failure\n\n### Github Actions\n\n1. read the log from PR checks > details\n\n<img width=\"725\" alt=\"image\" src=\"https://github.com/espnet/espnet/assets/11741550/e8e45c87-75e4-4489-a816-5c645b30fa0f\">\n\n### 8.2 Codecov\n\n1. write more tests to increase coverage\n2. explain to reviewers why you can't increase coverage\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.10546875,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 63.8271484375,
          "content": "<div align=\"left\"><img src=\"doc/image/espnet_logo1.png\" width=\"550\"/></div>\n\n# ESPnet: end-to-end speech processing toolkit\n\n|system/pytorch ver.|1.13.1|2.0.1|2.1.2|2.2.2|2.3.1|2.4.0|\n| :---- | :---: | :---: | :---: | :---: | :---: | :---: |\n|ubuntu/python3.10/pip||[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|\n|ubuntu/python3.11/pip||[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|[![ci on ubuntu](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_ubuntu.yml?query=branch%3Amaster)|\n|ubuntu/python3.10/conda|[![ci on debian11](https://github.com/espnet/espnet/actions/workflows/ci_on_debian11.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_debian11.yml?query=branch%3Amaster)|||||\n|debian11/python3.10/conda|[![ci on debian11](https://github.com/espnet/espnet/actions/workflows/ci_on_debian11.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_debian11.yml?query=branch%3Amaster)|||||\n|windows/python3.10/pip||||||[![ci on windows](https://github.com/espnet/espnet/actions/workflows/ci_on_windows.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_windows.yml?query=branch%3Amaster)|\n|macos/python3.10/pip|||[![ci on macos](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml?query=branch%3Amaster)||||\n|macos/python3.10/conda|||[![ci on macos](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml/badge.svg)](https://github.com/espnet/espnet/actions/workflows/ci_on_macos.yml?query=branch%3Amaster)||||\n\n<div align=\"center\">\n\n______________________________________________________________________\n[![PyPI version](https://badge.fury.io/py/espnet.svg)](https://badge.fury.io/py/espnet)\n[![Python Versions](https://img.shields.io/pypi/pyversions/espnet.svg)](https://pypi.org/project/espnet/)\n[![Downloads](https://pepy.tech/badge/espnet)](https://pepy.tech/project/espnet)\n[![GitHub license](https://img.shields.io/github/license/espnet/espnet.svg)](https://github.com/espnet/espnet)\n[![codecov](https://codecov.io/gh/espnet/espnet/branch/master/graph/badge.svg)](https://codecov.io/gh/espnet/espnet)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/espnet/espnet/master.svg)](https://results.pre-commit.ci/latest/github/espnet/espnet/master)\n[![Mergify Status](https://img.shields.io/endpoint.svg?url=https://api.mergify.com/v1/badges/espnet/espnet&style=flat)](https://mergify.com)\n[![Discord](https://img.shields.io/discord/1174538500360650773?color=%239B59B6&label=chat%20on%20discord)](https://discord.gg/hrCs85gFWM)\n\n______________________________________________________________________\n\n[**Docs**](https://espnet.github.io/espnet/)\n| [**Example**](https://github.com/espnet/espnet/tree/master/egs)\n| [**Example (ESPnet2)**](https://github.com/espnet/espnet/tree/master/egs2)\n| [**Docker**](https://github.com/espnet/espnet/tree/master/docker)\n| [**Notebook**](https://github.com/espnet/notebook)\n\n</div>\n\n______________________________________________________________________\n\nESPnet is an end-to-end speech processing toolkit covering end-to-end speech recognition, text-to-speech, speech translation, speech enhancement, speaker diarization, spoken language understanding, and so on.\nESPnet uses [pytorch](http://pytorch.org/) as a deep learning engine and also follows [Kaldi](http://kaldi-asr.org/) style data processing, feature extraction/format, and recipes to provide a complete setup for various speech processing experiments.\n\n## Tutorial Series\n- 2019 Tutorial at Interspeech\n  - [Material](https://github.com/espnet/interspeech2019-tutorial)\n- 2021 Tutorial at CMU\n  - [Online video](https://youtu.be/2mRz3wH1vd0)\n  - [Material](https://colab.research.google.com/github/espnet/notebook/blob/master/ESPnet2/Course/CMU_SpeechRecognition_Fall2021/general_tutorial.ipynb)\n- 2022 Tutorial at CMU\n  - Usage of ESPnet (ASR as an example)\n    - [Online video](https://youtu.be/YDN8cVjxSik)\n    - [Material](https://colab.research.google.com/github/espnet/notebook/blob/master/ESPnet2/Course/CMU_SpeechRecognition_Fall2022/recipe_tutorial.ipynb)\n  - Add new models/tasks to ESPnet\n    - [Online video](https://youtu.be/Css3XAes7SU)\n    - [Material](https://colab.research.google.com/github/espnet/notebook/blob/master/ESPnet2/Course/CMU_SpeechRecognition_Fall2022/new_task_tutorial.ipynb)\n\n\n## Key Features\n\n### Kaldi-style complete recipe\n- Support numbers of `ASR` recipes (WSJ, Switchboard, CHiME-4/5, Librispeech, TED, CSJ, AMI, HKUST, Voxforge, REVERB, Gigaspeech, etc.)\n- Support numbers of `TTS` recipes in a similar manner to the ASR recipe (LJSpeech, LibriTTS, M-AILABS, etc.)\n- Support numbers of `ST` recipes (Fisher-CallHome Spanish, Libri-trans, IWSLT'18, How2, Must-C, Mboshi-French, etc.)\n- Support numbers of `MT` recipes (IWSLT'14, IWSLT'16, the above ST recipes etc.)\n- Support numbers of `SLU` recipes (CATSLU-MAPS, FSC, Grabo, IEMOCAP, JDCINAL, SNIPS, SLURP, SWBD-DA, etc.)\n- Support numbers of `SE/SS` recipes (DNS-IS2020, LibriMix, SMS-WSJ, VCTK-noisyreverb, WHAM!, WHAMR!, WSJ-2mix, etc.)\n- Support voice conversion recipe (VCC2020 baseline)\n- Support speaker diarization recipe (mini_librispeech, librimix)\n- Support singing voice synthesis recipe (ofuton_p_utagoe_db, opencpop, m4singer, etc.)\n\n### ASR: Automatic Speech Recognition\n- **State-of-the-art performance** in several ASR benchmarks (comparable/superior to hybrid DNN/HMM and CTC)\n- **Hybrid CTC/attention** based end-to-end ASR\n  - Fast/accurate training with CTC/attention multitask training\n  - CTC/attention joint decoding to boost monotonic alignment decoding\n  - Encoder: VGG-like CNN + BiRNN (LSTM/GRU), sub-sampling BiRNN (LSTM/GRU), Transformer, Conformer, [Branchformer](https://proceedings.mlr.press/v162/peng22a.html), or [E-Branchformer](https://arxiv.org/abs/2210.00077)\n  - Decoder: RNN (LSTM/GRU), Transformer, or S4\n- Attention: [Flash Attention](https://github.com/Dao-AILab/flash-attention), Dot product, location-aware attention, variants of multi-head\n- Incorporate RNNLM/LSTMLM/TransformerLM/N-gram trained only with text data\n- Batch GPU decoding\n- Data augmentation\n- **Transducer** based end-to-end ASR\n  - Architecture:\n    - Custom encoder supporting RNNs, Conformer, Branchformer (w/ variants), 1D Conv / TDNN.\n    - Decoder w/ parameters shared across blocks supporting RNN, stateless w/ 1D Conv, [MEGA](https://arxiv.org/abs/2209.10655), and [RWKV](https://arxiv.org/abs/2305.13048).\n    - Pre-encoder: VGG2L or Conv2D available.\n  - Search algorithms:\n    - Greedy search constrained to one emission by timestep.\n    - Default beam search algorithm [[Graves, 2012]](https://arxiv.org/abs/1211.3711) without prefix search.\n    - Alignment-Length Synchronous decoding [[Saon et al., 2020]](https://ieeexplore.ieee.org/abstract/document/9053040).\n    - Time Synchronous Decoding [[Saon et al., 2020]](https://ieeexplore.ieee.org/abstract/document/9053040).\n    - N-step Constrained beam search modified from [[Kim et al., 2020]](https://arxiv.org/abs/2002.03577).\n    - modified Adaptive Expansion Search based on [[Kim et al., 2021]](https://ieeexplore.ieee.org/abstract/document/9250505) and NSC.\n  - Features:\n    - Unified interface for offline and streaming speech recognition.\n    - Multi-task learning with various auxiliary losses:\n      - Encoder: CTC, auxiliary Transducer and symmetric KL divergence.\n      - Decoder: cross-entropy w/ label smoothing.\n    - Transfer learning with an acoustic model and/or language model.\n    - Training with FastEmit regularization method [[Yu et al., 2021]](https://arxiv.org/abs/2010.11148).\n  > Please refer to the [tutorial page](https://espnet.github.io/espnet/tutorial.html#transducer) for complete documentation.\n- CTC segmentation\n- Non-autoregressive model based on Mask-CTC\n- ASR examples for supporting endangered language documentation (Please refer to egs/puebla_nahuatl and egs/yoloxochitl_mixtec for details)\n- Wav2Vec2.0 pre-trained model as Encoder, imported from [FairSeq](https://github.com/pytorch/fairseq/tree/master/fairseq).\n- Self-supervised learning representations as features, using upstream models in [S3PRL](https://github.com/s3prl/s3prl) in frontend.\n  - Set `frontend` to `s3prl`\n  - Select any upstream model by setting the `frontend_conf` to the corresponding name.\n- Transfer Learning :\n  - easy usage and transfers from models previously trained by your group or models from [ESPnet Hugging Face repository](https://huggingface.co/espnet).\n  - [Documentation](https://github.com/espnet/espnet/tree/master/egs2/mini_an4/asr1/transfer_learning.md) and [toy example runnable on colab](https://github.com/espnet/notebook/blob/master/ESPnet2/Demo/ASR/asr_transfer_learning_demo.ipynb).\n- Streaming Transformer/Conformer ASR with blockwise synchronous beam search.\n- Restricted Self-Attention based on [Longformer](https://arxiv.org/abs/2004.05150) as an encoder for long sequences\n- OpenAI [Whisper](https://openai.com/blog/whisper/) model, robust ASR based on large-scale, weakly-supervised multitask learning\n\nDemonstration\n- Real-time ASR demo with ESPnet2  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/ESPnet2/Demo/ASR/asr_realtime_demo.ipynb)\n- [Gradio](https://github.com/gradio-app/gradio) Web Demo on [Hugging Face Spaces](https://huggingface.co/docs/hub/spaces). Check out the [Web Demo](https://huggingface.co/spaces/akhaliq/espnet2_asr)\n- Streaming Transformer ASR [Local Demo](https://github.com/espnet/notebook/blob/master/ESPnet2/Demo/ASR/streaming_asr_demo.ipynb) with ESPnet2.\n\n### TTS: Text-to-speech\n- Architecture\n    - Tacotron2\n    - Transformer-TTS\n    - FastSpeech\n    - FastSpeech2\n    - Conformer FastSpeech & FastSpeech2\n    - VITS\n    - JETS\n- Multi-speaker & multi-language extension\n    - Pre-trained speaker embedding (e.g., X-vector)\n    - Speaker ID embedding\n    - Language ID embedding\n    - Global style token (GST) embedding\n    - Mix of the above embeddings\n- End-to-end training\n    - End-to-end text-to-wav model (e.g., VITS, JETS, etc.)\n    - Joint training of text2mel and vocoder\n- Various language support\n    - En / Jp / Zn / De / Ru / And more...\n- Integration with neural vocoders\n    - Parallel WaveGAN\n    - MelGAN\n    - Multi-band MelGAN\n    - HiFiGAN\n    - StyleMelGAN\n    - Mix of the above models\n\nDemonstration\n- Real-time TTS demo with ESPnet2  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/ESPnet2/Demo/TTS/tts_realtime_demo.ipynb)\n- Integrated to [Hugging Face Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/ESPnet2-TTS)\n\nTo train the neural vocoder, please check the following repositories:\n- [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)\n- [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder)\n\n### SE: Speech enhancement (and separation)\n\n- Single-speaker speech enhancement\n- Multi-speaker speech separation\n- Unified encoder-separator-decoder structure for time-domain and frequency-domain models\n  - Encoder/Decoder: STFT/iSTFT, Convolution/Transposed-Convolution\n  - Separators: BLSTM, Transformer, Conformer, [TasNet](https://arxiv.org/abs/1809.07454), [DPRNN](https://arxiv.org/abs/1910.06379), [SkiM](https://arxiv.org/abs/2201.10800), [SVoice](https://arxiv.org/abs/2011.02329), [DC-CRN](https://web.cse.ohio-state.edu/~wang.77/papers/TZW.taslp21.pdf), [DCCRN](https://arxiv.org/abs/2008.00264), [Deep Clustering](https://ieeexplore.ieee.org/document/7471631), [Deep Attractor Network](https://pubmed.ncbi.nlm.nih.gov/29430212/), [FaSNet](https://arxiv.org/abs/1909.13387), [iFaSNet](https://arxiv.org/abs/1910.14104), Neural Beamformers, etc.\n- Flexible ASR integration: working as an individual task or as the ASR frontend\n- Easy to import pre-trained models from [Asteroid](https://github.com/asteroid-team/asteroid)\n  - Both the pre-trained models from Asteroid and the specific configuration are supported.\n\nDemonstration\n- Interactive SE demo with ESPnet2 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1fjRJCh96SoYLZPRxsjF9VDv4Q2VoIckI?usp=sharing)\n- Streaming SE demo with ESPnet2 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17vd1V78eJpp3PHBnbFE5aVY5uMxQFL6o?usp=sharing)\n\n### ST: Speech Translation & MT: Machine Translation\n- **State-of-the-art performance** in several ST benchmarks (comparable/superior to cascaded ASR and MT)\n- Transformer-based end-to-end ST (new!)\n- Transformer-based end-to-end MT (new!)\n\n### VC: Voice conversion\n- Transformer and Tacotron2-based parallel VC using Mel spectrogram\n- End-to-end VC based on cascaded ASR+TTS (Baseline system for Voice Conversion Challenge 2020!)\n\n### SLU: Spoken Language Understanding\n- Architecture\n    - Transformer-based Encoder\n    - Conformer-based Encoder\n    - [Branchformer](https://proceedings.mlr.press/v162/peng22a.html) based Encoder\n    - [E-Branchformer](https://arxiv.org/abs/2210.00077) based Encoder\n    - RNN based Decoder\n    - Transformer-based Decoder\n- Support Multitasking with ASR\n    - Predict both intent and ASR transcript\n- Support Multitasking with NLU\n    - Deliberation encoder based 2 pass model\n- Support using pre-trained ASR models\n    - Hubert\n    - Wav2vec2\n    - VQ-APC\n    - TERA and more ...\n- Support using pre-trained NLP models\n    - BERT\n    - MPNet And more...\n- Various language support\n    - En / Jp / Zn / Nl / And more...\n- Supports using context from previous utterances\n- Supports using other tasks like SE in a pipeline manner\n- Supports Two Pass SLU that combines audio and ASR transcript\nDemonstration\n- Performing noisy spoken language understanding using a speech enhancement model followed by a spoken language understanding model.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/14nCrJ05vJcQX0cJuXjbMVFWUHJ3Wfb6N?usp=sharing)\n- Performing two-pass spoken language understanding where the second pass model attends to both acoustic and semantic information.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1p2cbGIPpIIcynuDl4ZVHDpmNPl8Nh_ci?usp=sharing)\n- Integrated to [Hugging Face Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See SLU demo on multiple languages: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Siddhant/ESPnet2-SLU)\n\n\n### SUM: Speech Summarization\n- End to End Speech Summarization Recipe for Instructional Videos using Restricted Self-Attention [[Sharma et al., 2022]](https://arxiv.org/abs/2110.06263)\n\n### SVS: Singing Voice Synthesis\n- Framework merge from [Muskits](https://github.com/SJTMusicTeam/Muskits)\n- Architecture\n  - RNN-based non-autoregressive model\n  - Xiaoice\n  - Tacotron-singing\n  - DiffSinger (in progress)\n  - VISinger\n  - VISinger 2 (its variations with different vocoders-architecture)\n- Support multi-speaker & multilingual singing synthesis\n  - Speaker ID embedding\n  - Language ID embedding\n- Various language support\n  - Jp / En / Kr / Zh\n- Tight integration with neural vocoders (the same as TTS)\n\n### SSL: Self-supervised Learning\n- Support HuBERT Pre-training:\n  * Example recipe: [egs2/LibriSpeech/ssl1](egs2/LibriSpeech/ssl1)\n\n### UASR: Unsupervised ASR (EURO: ESPnet Unsupervised Recognition - Open-source)\n- Architecture\n  - wav2vec-U (with different self-supervised models)\n  - wav2vec-U 2.0 (in progress)\n- Support PrefixBeamSearch and K2-based WFST decoding\n\n### S2T: Speech-to-text with Whisper-style multilingual multitask models\n- Reproduces Whisper-style training from scratch using public data: [OWSM](https://arxiv.org/abs/2309.13876)\n- Supports multiple tasks in a single model\n  - Multilingual speech recognition\n  - Any-to-any speech translation\n  - Language identification\n  - Utterance-level timestamp prediction (segmentation)\n\n### DNN Framework\n- Flexible network architecture thanks to Chainer and PyTorch\n- Flexible front-end processing thanks to [kaldiio](https://github.com/nttcslab-sp/kaldiio) and HDF5 support\n- Tensorboard-based monitoring\n- [DeepSpeed](https://github.com/microsoft/DeepSpeed)-based large-scale training\n\n### ESPnet2\nSee [ESPnet2](https://espnet.github.io/espnet/espnet2_tutorial.html).\n\n- Independent from Kaldi/Chainer, unlike ESPnet1\n- On-the-fly feature extraction and text processing when training\n- Supporting DistributedDataParallel and DaraParallel both\n- Supporting multiple nodes training and integrated with [Slurm](https://slurm.schedmd.com/) or MPI\n- Supporting Sharded Training provided by [fairscale](https://github.com/facebookresearch/fairscale)\n- A template recipe that can be applied to all corpora\n- Possible to train any size of corpus without CPU memory error\n- [ESPnet Model Zoo](https://github.com/espnet/espnet_model_zoo)\n- Integrated with [wandb](https://espnet.github.io/espnet/espnet2_training_option.html#weights-biases-integration)\n\n## Installation\n- If you intend to do full experiments, including DNN training, then see [Installation](https://espnet.github.io/espnet/installation.html).\n- If you just need the Python module only:\n    ```sh\n    # We recommend you install PyTorch before installing espnet following https://pytorch.org/get-started/locally/\n    pip install espnet\n    # To install the latest\n    # pip install git+https://github.com/espnet/espnet\n    # To install additional packages\n    # pip install \"espnet[all]\"\n    ```\n\n    If you use ESPnet1, please install chainer and cupy.\n\n    ```sh\n    pip install chainer==6.0.0 cupy==6.0.0    # [Option]\n    ```\n\n    You might need to install some packages depending on each task. We prepared various installation scripts at [tools/installers](tools/installers).\n\n- (ESPnet2) Once installed, run `wandb login` and set `--use_wandb true` to enable tracking runs using W&B.\n\n## Docker Container\n\ngo to [docker/](docker/) and follow [instructions](https://espnet.github.io/espnet/docker.html).\n\n## Contribution\nThank you for taking the time for ESPnet! Any contributions to ESPnet are welcome, and feel free to ask any questions or requests to [issues](https://github.com/espnet/espnet/issues).\nIf it's your first ESPnet contribution,  please follow the [contribution guide](CONTRIBUTING.md).\n\n### ASR results\n\n<details><summary>expand</summary><div>\n\n\nWe list the character error rate (CER) and word error rate (WER) of major ASR tasks.\n\n| Task                                                              |     CER (%)     |     WER (%)     |                                                                              Pre-trained model                                                                               |\n| ----------------------------------------------------------------- | :-------------: | :-------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |\n| Aishell dev/test                                                  |     4.6/5.1     |       N/A       |                [link](https://github.com/espnet/espnet/blob/master/egs/aishell/asr1/RESULTS.md#conformer-kernel-size--15--specaugment--lm-weight--00-result)                |\n| **ESPnet2** Aishell dev/test                                      |     4.1/4.4     |       N/A       |                [link](https://github.com/espnet/espnet/tree/master/egs2/aishell/asr1#branchformer-initial)                                                                  |\n| Common Voice dev/test                                             |     1.7/1.8     |     2.2/2.3     |    [link](https://github.com/espnet/espnet/blob/master/egs/commonvoice/asr1/RESULTS.md#first-results-default-pytorch-transformer-setting-with-bpe-100-epochs-single-gpu)    |\n| CSJ eval1/eval2/eval3                                             |   5.7/3.8/4.2   |       N/A       |                 [link](https://github.com/espnet/espnet/blob/master/egs/csj/asr1/RESULTS.md#pytorch-backend-transformer-without-any-hyperparameter-tuning)                  |\n| **ESPnet2** CSJ eval1/eval2/eval3                                 |   4.5/3.3/3.6   |       N/A       |                                        [link](https://github.com/espnet/espnet/tree/master/egs2/csj/asr1#initial-conformer-results)                                         |\n| **ESPnet2** GigaSpeech dev/test                                   |       N/A       |    10.6/10.5    |                                          [link](https://github.com/espnet/espnet/tree/master/egs2/gigaspeech/asr1#e-branchformer)                                           |\n| HKUST dev                                                         |      23.5       |       N/A       |                                  [link](https://github.com/espnet/espnet/blob/master/egs/hkust/asr1/RESULTS.md#transformer-only-20-epochs)                                  |\n| **ESPnet2** HKUST dev                                             |      21.2       |       N/A       |                                    [link](https://github.com/espnet/espnet/tree/master/egs2/hkust/asr1#transformer-asr--transformer-lm)                                     |\n| Librispeech dev_clean/dev_other/test_clean/test_other             |       N/A       | 1.9/4.9/2.1/4.9 | [link](https://github.com/espnet/espnet/blob/master/egs/librispeech/asr1/RESULTS.md#pytorch-large-conformer-with-specaug--speed-perturbation-8-gpus--transformer-lm-4-gpus) |\n| **ESPnet2** Librispeech dev_clean/dev_other/test_clean/test_other | 0.6/1.5/0.6/1.4 | 1.7/3.4/1.8/3.6 |    [link](https://github.com/espnet/espnet/tree/master/egs2/librispeech/asr1#self-supervised-learning-features-hubert_large_ll60k-conformer-utt_mvn-with-transformer-lm)    |\n| Switchboard (eval2000) callhm/swbd                                |       N/A       |    14.0/6.8     |          [link](https://github.com/espnet/espnet/blob/master/egs/swbd/asr1/RESULTS.md#conformer-with-bpe-2000-specaug-speed-perturbation-transformer-lm-decoding)           |\n| **ESPnet2** Switchboard (eval2000) callhm/swbd                    |       N/A       |    13.4/7.3     |                                             [link](https://github.com/espnet/espnet/tree/master/egs2/swbd/asr1#e-branchformer)                                              |\n| TEDLIUM2 dev/test                                                 |       N/A       |     8.6/7.2     |                 [link](https://github.com/espnet/espnet/blob/master/egs/tedlium2/asr1/RESULTS.md#conformer-large-model--specaug--speed-perturbation--rnnlm)                 |\n| **ESPnet2** TEDLIUM2 dev/test                                     |       N/A       |     7.3/7.1     |                 [link](https://github.com/espnet/espnet/blob/master/egs2/tedlium2/asr1/README.md#e-branchformer-12-encoder-layers)                                          |\n| TEDLIUM3 dev/test                                                 |       N/A       |     9.6/7.6     |                                              [link](https://github.com/espnet/espnet/blob/master/egs/tedlium3/asr1/RESULTS.md)                                              |\n| WSJ dev93/eval92                                                  |     3.2/2.1     |     7.0/4.7     |                                                                                     N/A                                                                                     |\n| **ESPnet2** WSJ dev93/eval92                                      |     1.1/0.8     |     2.8/1.8     |       [link](https://github.com/espnet/espnet/tree/master/egs2/wsj/asr1#self-supervised-learning-features-wav2vec2_large_ll60k-conformer-utt_mvn-with-transformer-lm)       |\n\nNote that the performance of the CSJ, HKUST, and Librispeech tasks was significantly improved by using the wide network (#units = 1024) and large subword units if necessary reported by [RWTH](https://arxiv.org/pdf/1805.03294.pdf).\n\nIf you want to check the results of the other recipes, please check `egs/<name_of_recipe>/asr1/RESULTS.md`.\n\n</div></details>\n\n\n### ASR demo\n\n<details><summary>expand</summary><div>\n\nYou can recognize speech in a WAV file using pre-trained models.\nGo to a recipe directory and run `utils/recog_wav.sh` as follows:\n```sh\n# go to the recipe directory and source path of espnet tools\ncd egs/tedlium2/asr1 && . ./path.sh\n# let's recognize speech!\nrecog_wav.sh --models tedlium2.transformer.v1 example.wav\n```\nwhere `example.wav` is a WAV file to be recognized.\nThe sampling rate must be consistent with that of data used in training.\n\nAvailable pre-trained models in the demo script are listed below.\n\n| Model                                                                                            | Notes                                                      |\n| :----------------------------------------------------------------------------------------------- | :--------------------------------------------------------- |\n| [tedlium2.rnn.v1](https://drive.google.com/open?id=1UqIY6WJMZ4sxNxSugUqp3mrGb3j6h7xe)            | Streaming decoding based on CTC-based VAD                  |\n| [tedlium2.rnn.v2](https://drive.google.com/open?id=1cac5Uc09lJrCYfWkLQsF8eapQcxZnYdf)            | Streaming decoding based on CTC-based VAD (batch decoding) |\n| [tedlium2.transformer.v1](https://drive.google.com/open?id=1cVeSOYY1twOfL9Gns7Z3ZDnkrJqNwPow)    | Joint-CTC attention Transformer trained on Tedlium 2       |\n| [tedlium3.transformer.v1](https://drive.google.com/open?id=1zcPglHAKILwVgfACoMWWERiyIquzSYuU)    | Joint-CTC attention Transformer trained on Tedlium 3       |\n| [librispeech.transformer.v1](https://drive.google.com/open?id=1BtQvAnsFvVi-dp_qsaFP7n4A_5cwnlR6) | Joint-CTC attention Transformer trained on Librispeech     |\n| [commonvoice.transformer.v1](https://drive.google.com/open?id=1tWccl6aYU67kbtkm8jv5H6xayqg1rzjh) | Joint-CTC attention Transformer trained on CommonVoice     |\n| [csj.transformer.v1](https://drive.google.com/open?id=120nUQcSsKeY5dpyMWw_kI33ooMRGT2uF)         | Joint-CTC attention Transformer trained on CSJ             |\n| [csj.rnn.v1](https://drive.google.com/open?id=1ALvD4nHan9VDJlYJwNurVr7H7OV0j2X9)                 | Joint-CTC attention VGGBLSTM trained on CSJ                |\n\n</div></details>\n\n### SE results\n<details><summary>expand</summary><div>\n\nWe list results from three different models on WSJ0-2mix, which is one the most widely used benchmark dataset for speech separation.\n\n| Model                                             | STOI | SAR   | SDR   | SIR   |\n| ------------------------------------------------- | ---- | ----- | ----- | ----- |\n| [TF Masking](https://zenodo.org/record/4498554)   | 0.89 | 11.40 | 10.24 | 18.04 |\n| [Conv-Tasnet](https://zenodo.org/record/4498562)  | 0.95 | 16.62 | 15.94 | 25.90 |\n| [DPRNN-Tasnet](https://zenodo.org/record/4688000) | 0.96 | 18.82 | 18.29 | 28.92 |\n\n</div></details>\n\n### SE demos\n<details><summary>expand</summary><div>\nYou can try the interactive demo with Google Colab. Please click the following button to get access to the demos.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1fjRJCh96SoYLZPRxsjF9VDv4Q2VoIckI?usp=sharing)\n\n\nIt is based on ESPnet2. Pre-trained models are available for both speech enhancement and speech separation tasks.\n\nSpeech separation streaming demos:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17vd1V78eJpp3PHBnbFE5aVY5uMxQFL6o?usp=sharing)\n\n\n\n</div></details>\n\n### ST results\n\n<details><summary>expand</summary><div>\n\nWe list 4-gram BLEU of major ST tasks.\n\n#### end-to-end system\n| Task                                              | BLEU  |                                                                                         Pre-trained model                                                                                          |\n| ------------------------------------------------- | :---: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |\n| Fisher-CallHome Spanish fisher_test (Es->En)      | 51.03 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans) |\n| Fisher-CallHome Spanish callhome_evltest (Es->En) | 20.44 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/st1/RESULTS.md#train_spen_lcrm_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans) |\n| Libri-trans test (En->Fr)                         | 16.70 |       [link](https://github.com/espnet/espnet/blob/master/egs/libri_trans/st1/RESULTS.md#train_spfr_lc_pytorch_train_pytorch_transformer_bpe_short_long_bpe1000_specaug_asrtrans_mttrans-1)       |\n| How2 dev5 (En->Pt)                                | 45.68 |              [link](https://github.com/espnet/espnet/blob/master/egs/how2/st1/RESULTS.md#trainpt_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans-1)              |\n| Must-C tst-COMMON (En->De)                        | 22.91 |          [link](https://github.com/espnet/espnet/blob/master/egs/must_c/st1/RESULTS.md#train_spen-dede_tc_pytorch_train_pytorch_transformer_short_long_bpe8000_specaug_asrtrans_mttrans)          |\n| Mboshi-French dev (Fr->Mboshi)                    | 6.18  |                                                                                                N/A                                                                                                |\n\n#### cascaded system\n| Task                                              | BLEU  | Pre-trained model |\n| ------------------------------------------------- | :---: | :--------------: |\n| Fisher-CallHome Spanish fisher_test (Es->En)      | 42.16 |       N/A        |\n| Fisher-CallHome Spanish callhome_evltest (Es->En) | 19.82 |       N/A        |\n| Libri-trans test (En->Fr)                         | 16.96 |       N/A        |\n| How2 dev5 (En->Pt)                                | 44.90 |       N/A        |\n| Must-C tst-COMMON (En->De)                        | 23.65 |       N/A        |\n\nIf you want to check the results of the other recipes, please check `egs/<name_of_recipe>/st1/RESULTS.md`.\n\n</div></details>\n\n\n### ST demo\n\n<details><summary>expand</summary><div>\n\n(**New!**) We made a new real-time E2E-ST + TTS demonstration in Google Colab.\nPlease access the notebook from the following button and enjoy the real-time speech-to-speech translation!\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/st_demo.ipynb)\n\n---\n\nYou can translate speech in a WAV file using pre-trained models.\nGo to a recipe directory and run `utils/translate_wav.sh` as follows:\n```sh\n# Go to recipe directory and source path of espnet tools\ncd egs/fisher_callhome_spanish/st1 && . ./path.sh\n# download example wav file\nwget -O - https://github.com/espnet/espnet/files/4100928/test.wav.tar.gz | tar zxvf -\n# let's translate speech!\ntranslate_wav.sh --models fisher_callhome_spanish.transformer.v1.es-en test.wav\n```\nwhere `test.wav` is a WAV file to be translated.\nThe sampling rate must be consistent with that of data used in training.\n\nAvailable pre-trained models in the demo script are listed as below.\n\n| Model                                                                                                        | Notes                                                    |\n| :----------------------------------------------------------------------------------------------------------- | :------------------------------------------------------- |\n| [fisher_callhome_spanish.transformer.v1](https://drive.google.com/open?id=1hawp5ZLw4_SIHIT3edglxbKIIkPVe8n3) | Transformer-ST trained on Fisher-CallHome Spanish Es->En |\n\n</div></details>\n\n\n### MT results\n\n<details><summary>expand</summary><div>\n\n| Task                                              | BLEU  |                                                                        Pre-trained model                                                                         |\n| ------------------------------------------------- | :---: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------: |\n| Fisher-CallHome Spanish fisher_test (Es->En)      | 61.45 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000) |\n| Fisher-CallHome Spanish callhome_evltest (Es->En) | 29.86 | [link](https://github.com/espnet/espnet/blob/master/egs/fisher_callhome_spanish/mt1/RESULTS.md#trainen_lcrm_lcrm_pytorch_train_pytorch_transformer_bpe_bpe1000) |\n| Libri-trans test (En->Fr)                         | 18.09 |          [link](https://github.com/espnet/espnet/blob/master/egs/libri_trans/mt1/RESULTS.md#trainfr_lcrm_tc_pytorch_train_pytorch_transformer_bpe1000)          |\n| How2 dev5 (En->Pt)                                | 58.61 |              [link](https://github.com/espnet/espnet/blob/master/egs/how2/mt1/RESULTS.md#trainpt_tc_tc_pytorch_train_pytorch_transformer_bpe8000)               |\n| Must-C tst-COMMON (En->De)                        | 27.63 |                               [link](https://github.com/espnet/espnet/blob/master/egs/must_c/mt1/RESULTS.md#summary-4-gram-bleu)                                |\n| IWSLT'14 test2014 (En->De)                        | 24.70 |                                     [link](https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result)                                      |\n| IWSLT'14 test2014 (De->En)                        | 29.22 |                                     [link](https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result)                                      |\n| IWSLT'14 test2014 (De->En)                        | 32.2  | [link](https://github.com/espnet/espnet/blob/master/egs2/iwslt14/mt1/README.md)  |\n| IWSLT'16 test2014 (En->De)                        | 24.05 |                                     [link](https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result)                                      |\n| IWSLT'16 test2014 (De->En)                        | 29.13 |                                     [link](https://github.com/espnet/espnet/blob/master/egs/iwslt16/mt1/RESULTS.md#result)                                      |\n\n</div></details>\n\n### TTS results\n\n<details><summary>ESPnet2</summary><div>\n\nYou can listen to the generated samples in the following URL.\n- [ESPnet2 TTS generated samples](https://drive.google.com/drive/folders/1H3fnlBbWMEkQUfrHqosKN_ZX_WjO29ma?usp=sharing)\n\n> Note that in the generation, we use Griffin-Lim (`wav/`) and [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN) (`wav_pwg/`).\n\nYou can download pre-trained models via `espnet_model_zoo`.\n- [ESPnet model zoo](https://github.com/espnet/espnet_model_zoo)\n- [Pre-trained model list](https://github.com/espnet/espnet_model_zoo/blob/master/espnet_model_zoo/table.csv)\n\nYou can download pre-trained vocoders via `kan-bayashi/ParallelWaveGAN`.\n- [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)\n- [Pre-trained vocoder list](https://github.com/kan-bayashi/ParallelWaveGAN#results)\n\n</div></details>\n\n<details><summary>ESPnet1</summary><div>\n\n> NOTE: We are moving on ESPnet2-based development for TTS. Please check the latest results in the above ESPnet2 results.\n\nYou can listen to our samples in demo HP [espnet-tts-sample](https://espnet.github.io/espnet-tts-sample/).\nHere we list some notable ones:\n\n- [Single English speaker Tacotron2](https://drive.google.com/open?id=18JgsOCWiP_JkhONasTplnHS7yaF_konr)\n- [Single Japanese speaker Tacotron2](https://drive.google.com/open?id=1fEgS4-K4dtgVxwI4Pr7uOA1h4PE-zN7f)\n- [Single other language speaker Tacotron2](https://drive.google.com/open?id=1q_66kyxVZGU99g8Xb5a0Q8yZ1YVm2tN0)\n- [Multi English speaker Tacotron2](https://drive.google.com/open?id=18S_B8Ogogij34rIfJOeNF8D--uG7amz2)\n- [Single English speaker Transformer](https://drive.google.com/open?id=14EboYVsMVcAq__dFP1p6lyoZtdobIL1X)\n- [Single English speaker FastSpeech](https://drive.google.com/open?id=1PSxs1VauIndwi8d5hJmZlppGRVu2zuy5)\n- [Multi English speaker Transformer](https://drive.google.com/open?id=1_vrdqjM43DdN1Qz7HJkvMQ6lCMmWLeGp)\n- [Single Italian speaker FastSpeech](https://drive.google.com/open?id=13I5V2w7deYFX4DlVk1-0JfaXmUR2rNOv)\n- [Single Mandarin speaker Transformer](https://drive.google.com/open?id=1mEnZfBKqA4eT6Bn0eRZuP6lNzL-IL3VD)\n- [Single Mandarin speaker FastSpeech](https://drive.google.com/open?id=1Ol_048Tuy6BgvYm1RpjhOX4HfhUeBqdK)\n- [Multi Japanese speaker Transformer](https://drive.google.com/open?id=1fFMQDF6NV5Ysz48QLFYE8fEvbAxCsMBw)\n- [Single English speaker models with Parallel WaveGAN](https://drive.google.com/open?id=1HvB0_LDf1PVinJdehiuCt5gWmXGguqtx)\n- [Single English speaker knowledge distillation-based FastSpeech](https://drive.google.com/open?id=1wG-Y0itVYalxuLAHdkAHO7w1CWFfRPF4)\n\nYou can download all of the pre-trained models and generated samples:\n- [All of the pre-trained E2E-TTS models](https://drive.google.com/open?id=1k9RRyc06Zl0mM2A7mi-hxNiNMFb_YzTF)\n- [All of the generated samples](https://drive.google.com/open?id=1bQGuqH92xuxOX__reWLP4-cif0cbpMLX)\n\nNote that in the generated samples, we use the following vocoders: Griffin-Lim (**GL**), WaveNet vocoder (**WaveNet**), Parallel WaveGAN (**ParallelWaveGAN**), and MelGAN (**MelGAN**).\nThe neural vocoders are based on the following repositories.\n- [kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN): Parallel WaveGAN / MelGAN / Multi-band MelGAN\n- [r9y9/wavenet_vocoder](https://github.com/r9y9/wavenet_vocoder): 16 bit mixture of Logistics WaveNet vocoder\n- [kan-bayashi/PytorchWaveNetVocoder](https://github.com/kan-bayashi/PytorchWaveNetVocoder): 8 bit Softmax WaveNet Vocoder with the noise shaping\n\nIf you want to build your own neural vocoder, please check the above repositories.\n[kan-bayashi/ParallelWaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN) provides [the manual](https://github.com/kan-bayashi/ParallelWaveGAN#decoding-with-espnet-tts-models-features) about how to decode ESPnet-TTS model's features with neural vocoders. Please check it.\n\nHere we list all of the pre-trained neural vocoders. Please download and enjoy the generation of high-quality speech!\n\n| Model link                                                                                           | Lang  | Fs [Hz] | Mel range [Hz] | FFT / Shift / Win [pt] | Model type                                                              |\n| :--------------------------------------------------------------------------------------------------- | :---: | :-----: | :------------: | :--------------------: | :---------------------------------------------------------------------- |\n| [ljspeech.wavenet.softmax.ns.v1](https://drive.google.com/open?id=1eA1VcRS9jzFa-DovyTgJLQ_jmwOLIi8L) |  EN   | 22.05k  |      None      |   1024 / 256 / None    | [Softmax WaveNet](https://github.com/kan-bayashi/PytorchWaveNetVocoder) |\n| [ljspeech.wavenet.mol.v1](https://drive.google.com/open?id=1sY7gEUg39QaO1szuN62-Llst9TrFno2t)        |  EN   | 22.05k  |      None      |   1024 / 256 / None    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [ljspeech.parallel_wavegan.v1](https://drive.google.com/open?id=1tv9GKyRT4CDsvUWKwH3s_OfXkiTi0gw7)   |  EN   | 22.05k  |      None      |   1024 / 256 / None    | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |\n| [ljspeech.wavenet.mol.v2](https://drive.google.com/open?id=1es2HuKUeKVtEdq6YDtAsLNpqCy4fhIXr)        |  EN   | 22.05k  |    80-7600     |   1024 / 256 / None    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [ljspeech.parallel_wavegan.v2](https://drive.google.com/open?id=1Grn7X9wD35UcDJ5F7chwdTqTa4U7DeVB)   |  EN   | 22.05k  |    80-7600     |   1024 / 256 / None    | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |\n| [ljspeech.melgan.v1](https://drive.google.com/open?id=1ipPWYl8FBNRlBFaKj1-i23eQpW_W_YcR)             |  EN   | 22.05k  |    80-7600     |   1024 / 256 / None    | [MelGAN](https://github.com/kan-bayashi/ParallelWaveGAN)                |\n| [ljspeech.melgan.v3](https://drive.google.com/open?id=1_a8faVA5OGCzIcJNw4blQYjfG4oA9VEt)             |  EN   | 22.05k  |    80-7600     |   1024 / 256 / None    | [MelGAN](https://github.com/kan-bayashi/ParallelWaveGAN)                |\n| [libritts.wavenet.mol.v1](https://drive.google.com/open?id=1jHUUmQFjWiQGyDd7ZeiCThSjjpbF_B4h)        |  EN   |   24k   |      None      |   1024 / 256 / None    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [jsut.wavenet.mol.v1](https://drive.google.com/open?id=187xvyNbmJVZ0EZ1XHCdyjZHTXK9EcfkK)            |  JP   |   24k   |    80-7600     |   2048 / 300 / 1200    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [jsut.parallel_wavegan.v1](https://drive.google.com/open?id=1OwrUQzAmvjj1x9cDhnZPp6dqtsEqGEJM)       |  JP   |   24k   |    80-7600     |   2048 / 300 / 1200    | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |\n| [csmsc.wavenet.mol.v1](https://drive.google.com/open?id=1PsjFRV5eUP0HHwBaRYya9smKy5ghXKzj)           |  ZH   |   24k   |    80-7600     |   2048 / 300 / 1200    | [MoL WaveNet](https://github.com/r9y9/wavenet_vocoder)                  |\n| [csmsc.parallel_wavegan.v1](https://drive.google.com/open?id=10M6H88jEUGbRWBmU1Ff2VaTmOAeL8CEy)      |  ZH   |   24k   |    80-7600     |   2048 / 300 / 1200    | [Parallel WaveGAN](https://github.com/kan-bayashi/ParallelWaveGAN)      |\n\nIf you want to use the above pre-trained vocoders, please exactly match the feature setting with them.\n\n</div></details>\n\n### TTS demo\n\n<details><summary>ESPnet2</summary><div>\n\nYou can try the real-time demo in Google Colab.\nPlease access the notebook from the following button and enjoy the real-time synthesis!\n\n- Real-time TTS demo with ESPnet2  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_tts_realtime_demo.ipynb)\n\nEnglish, Japanese, and Mandarin models are available in the demo.\n\n</div></details>\n\n<details><summary>ESPnet1</summary><div>\n\n> NOTE: We are moving on ESPnet2-based development for TTS. Please check the latest demo in the above ESPnet2 demo.\n\nYou can try the real-time demo in Google Colab.\nPlease access the notebook from the following button and enjoy the real-time synthesis.\n\n- Real-time TTS demo with ESPnet1  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/tts_realtime_demo.ipynb)\n\nWe also provide a shell script to perform synthesis.\nGo to a recipe directory and run `utils/synth_wav.sh` as follows:\n\n```sh\n# Go to recipe directory and source path of espnet tools\ncd egs/ljspeech/tts1 && . ./path.sh\n# We use an upper-case char sequence for the default model.\necho \"THIS IS A DEMONSTRATION OF TEXT TO SPEECH.\" > example.txt\n# let's synthesize speech!\nsynth_wav.sh example.txt\n\n# Also, you can use multiple sentences\necho \"THIS IS A DEMONSTRATION OF TEXT TO SPEECH.\" > example_multi.txt\necho \"TEXT TO SPEECH IS A TECHNIQUE TO CONVERT TEXT INTO SPEECH.\" >> example_multi.txt\nsynth_wav.sh example_multi.txt\n```\n\nYou can change the pre-trained model as follows:\n\n```sh\nsynth_wav.sh --models ljspeech.fastspeech.v1 example.txt\n```\n\nWaveform synthesis is performed with the Griffin-Lim algorithm and neural vocoders (WaveNet and ParallelWaveGAN).\nYou can change the pre-trained vocoder model as follows:\n\n```sh\nsynth_wav.sh --vocoder_models ljspeech.wavenet.mol.v1 example.txt\n```\n\nWaveNet vocoder provides very high-quality speech, but it takes time to generate.\n\nSee more details or available models via `--help`.\n\n```sh\nsynth_wav.sh --help\n```\n\n</div></details>\n\n### VC results\n\n<details><summary>expand</summary><div>\n\n- Transformer and Tacotron2-based VC\n\nYou can listen to some samples on the [demo webpage](https://unilight.github.io/Publication-Demos/publications/transformer-vc/).\n\n- Cascade ASR+TTS as one of the baseline systems of VCC2020\n\nThe [Voice Conversion Challenge 2020](http://www.vc-challenge.org/) (VCC2020) adopts ESPnet to build an end-to-end based baseline system.\nIn VCC2020, the objective is intra/cross-lingual nonparallel VC.\nYou can download converted samples of the cascade ASR+TTS baseline system [here](https://drive.google.com/drive/folders/1oeZo83GrOgtqxGwF7KagzIrfjr8X59Ue?usp=sharing).\n\n</div></details>\n\n### SLU results\n\n<details><summary>expand</summary><div>\n\n\nWe list the performance on various SLU tasks and datasets using the metric reported in the original dataset paper\n\n| Task                                                              | Dataset                                                              |    Metric     |     Result     |                                                                              Pre-trained Model                                         |\n| ----------------------------------------------------------------- | :-------------: | :-------------: | :-------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |\n| Intent Classification                                                 |     SLURP     |       Acc       |       86.3       |                [link](https://github.com/espnet/espnet/tree/master/egs2/slurp/asr1/README.md)                |\n| Intent Classification                                                   |     FSC     |       Acc       |       99.6       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc/asr1/README.md)                |\n| Intent Classification                                                  |     FSC Unseen Speaker Set     |       Acc       |       98.6       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_unseen/asr1/README.md)                |\n| Intent Classification                                                   |     FSC Unseen Utterance Set     |       Acc       |       86.4       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_unseen/asr1/README.md)                |\n| Intent Classification                                                   |     FSC Challenge Speaker Set     |       Acc       |       97.5       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_challenge/asr1/README.md)                |\n| Intent Classification                                                   |     FSC Challenge Utterance Set     |       Acc       |       78.5       |                [link](https://github.com/espnet/espnet/tree/master/egs2/fsc_challenge/asr1/README.md)                |\n| Intent Classification                                                   |     SNIPS     |       F1       |       91.7       |                [link](https://github.com/espnet/espnet/tree/master/egs2/snips/asr1/README.md)                |\n| Intent Classification                                                   |     Grabo (Nl)   |       Acc       |       97.2       |                [link](https://github.com/espnet/espnet/tree/master/egs2/grabo/asr1/README.md)                |\n| Intent Classification                                                   |     CAT SLU MAP (Zn)     |       Acc       |       78.9       |                [link](https://github.com/espnet/espnet/tree/master/egs2/catslu/asr1/README.md)                |\n| Intent Classification                                                  |     Google Speech Commands    |       Acc       |       98.4       |                [link](https://github.com/espnet/espnet/tree/master/egs2/speechcommands/asr1/README.md)                |\n| Slot Filling                                                  |     SLURP     |       SLU-F1       |       71.9       |                [link](https://github.com/espnet/espnet/tree/master/egs2/slurp_entity/asr1/README.md)                |\n| Dialogue  Act Classification                                                 |     Switchboard     |       Acc       |       67.5       |                [link](https://github.com/espnet/espnet/tree/master/egs2/swbd_da/asr1/README.md)                |\n| Dialogue  Act Classification                                                 |     Jdcinal (Jp)    |       Acc       |       67.4       |                [link](https://github.com/espnet/espnet/tree/master/egs2/jdcinal/asr1/README.md)                |\n| Emotion Recognition                                                  |     IEMOCAP     |       Acc       |       69.4       |                [link](https://github.com/espnet/espnet/tree/master/egs2/iemocap/asr1/README.md)                |\n| Emotion Recognition                                                  |     swbd_sentiment     |       Macro F1       |       61.4       |                [link](https://github.com/espnet/espnet/tree/master/egs2/swbd_sentiment/asr1/README.md)                |\n| Emotion Recognition                                                  |     slue_voxceleb     |       Macro F1       |       44.0       |                [link](https://github.com/espnet/espnet/tree/master/egs2/slue-voxceleb/asr1/README.md)                |\n\n\nIf you want to check the results of the other recipes, please check `egs2/<name_of_recipe>/asr1/RESULTS.md`.\n\n\n\n</div></details>\n\n### CTC Segmentation demo\n\n<details><summary>ESPnet1</summary><div>\n\n[CTC segmentation](https://arxiv.org/abs/2007.09127) determines utterance segments within audio files.\nAligned utterance segments constitute the labels of speech datasets.\n\nAs a demo, we align the start and end of utterances within the audio file `ctc_align_test.wav`, using the example script `utils/asr_align_wav.sh`.\nFor preparation, set up a data directory:\n\n```sh\ncd egs/tedlium2/align1/\n# data directory\nalign_dir=data/demo\nmkdir -p ${align_dir}\n# wav file\nbase=ctc_align_test\nwav=../../../test_utils/${base}.wav\n# recipe files\necho \"batchsize: 0\" > ${align_dir}/align.yaml\n\ncat << EOF > ${align_dir}/utt_text\n${base} THE SALE OF THE HOTELS\n${base} IS PART OF HOLIDAY'S STRATEGY\n${base} TO SELL OFF ASSETS\n${base} AND CONCENTRATE\n${base} ON PROPERTY MANAGEMENT\nEOF\n```\n\nHere, `utt_text` is the file containing the list of utterances.\nChoose a pre-trained ASR model that includes a CTC layer to find utterance segments:\n\n```sh\n# pre-trained ASR model\nmodel=wsj.transformer_small.v1\nmkdir ./conf && cp ../../wsj/asr1/conf/no_preprocess.yaml ./conf\n\n../../../utils/asr_align_wav.sh \\\n    --models ${model} \\\n    --align_dir ${align_dir} \\\n    --align_config ${align_dir}/align.yaml \\\n    ${wav} ${align_dir}/utt_text\n```\n\nSegments are written to `aligned_segments` as a list of file/utterance names, utterance start and end times in seconds, and a confidence score.\nThe confidence score is a probability in log space that indicates how well the utterance was aligned. If needed, remove bad utterances:\n\n```sh\nmin_confidence_score=-5\nawk -v ms=${min_confidence_score} '{ if ($5 > ms) {print} }' ${align_dir}/aligned_segments\n```\n\nThe demo script `utils/ctc_align_wav.sh` uses an already pre-trained ASR model (see the list above for more models).\nIt is recommended to use models with RNN-based encoders (such as BLSTMP) for aligning large audio files;\nrather than using Transformer models with a high memory consumption on longer audio data.\nThe sample rate of the audio must be consistent with that of the data used in training; adjust with `sox` if needed.\nA full example recipe is in `egs/tedlium2/align1/`.\n\n</div></details>\n\n<details><summary>ESPnet2</summary><div>\n\n[CTC segmentation](https://arxiv.org/abs/2007.09127) determines utterance segments within audio files.\nAligned utterance segments constitute the labels of speech datasets.\n\nAs a demo, we align the start and end of utterances within the audio file `ctc_align_test.wav`.\nThis can be done either directly from the Python command line or using the script `espnet2/bin/asr_align.py`.\n\nFrom the Python command line interface:\n\n```python\n# load a model with character tokens\nfrom espnet_model_zoo.downloader import ModelDownloader\nd = ModelDownloader(cachedir=\"./modelcache\")\nwsjmodel = d.download_and_unpack(\"kamo-naoyuki/wsj\")\n# load the example file included in the ESPnet repository\nimport soundfile\nspeech, rate = soundfile.read(\"./test_utils/ctc_align_test.wav\")\n# CTC segmentation\nfrom espnet2.bin.asr_align import CTCSegmentation\naligner = CTCSegmentation( **wsjmodel , fs=rate )\ntext = \"\"\"\nutt1 THE SALE OF THE HOTELS\nutt2 IS PART OF HOLIDAY'S STRATEGY\nutt3 TO SELL OFF ASSETS\nutt4 AND CONCENTRATE ON PROPERTY MANAGEMENT\n\"\"\"\nsegments = aligner(speech, text)\nprint(segments)\n# utt1 utt 0.26 1.73 -0.0154 THE SALE OF THE HOTELS\n# utt2 utt 1.73 3.19 -0.7674 IS PART OF HOLIDAY'S STRATEGY\n# utt3 utt 3.19 4.20 -0.7433 TO SELL OFF ASSETS\n# utt4 utt 4.20 6.10 -0.4899 AND CONCENTRATE ON PROPERTY MANAGEMENT\n```\n\nAligning also works with fragments of the text.\nFor this, set the `gratis_blank` option that allows skipping unrelated audio sections without penalty.\nIt's also possible to omit the utterance names at the beginning of each line by setting `kaldi_style_text` to False.\n\n```python\naligner.set_config( gratis_blank=True, kaldi_style_text=False )\ntext = [\"SALE OF THE HOTELS\", \"PROPERTY MANAGEMENT\"]\nsegments = aligner(speech, text)\nprint(segments)\n# utt_0000 utt 0.37 1.72 -2.0651 SALE OF THE HOTELS\n# utt_0001 utt 4.70 6.10 -5.0566 PROPERTY MANAGEMENT\n```\n\nThe script `espnet2/bin/asr_align.py` uses a similar interface. To align utterances:\n\n```sh\n# ASR model and config files from pre-trained model (e.g., from cachedir):\nasr_config=<path-to-model>/config.yaml\nasr_model=<path-to-model>/valid.*best.pth\n# prepare the text file\nwav=\"test_utils/ctc_align_test.wav\"\ntext=\"test_utils/ctc_align_text.txt\"\ncat << EOF > ${text}\nutt1 THE SALE OF THE HOTELS\nutt2 IS PART OF HOLIDAY'S STRATEGY\nutt3 TO SELL OFF ASSETS\nutt4 AND CONCENTRATE\nutt5 ON PROPERTY MANAGEMENT\nEOF\n# obtain alignments:\npython espnet2/bin/asr_align.py --asr_train_config ${asr_config} --asr_model_file ${asr_model} --audio ${wav} --text ${text}\n# utt1 ctc_align_test 0.26 1.73 -0.0154 THE SALE OF THE HOTELS\n# utt2 ctc_align_test 1.73 3.19 -0.7674 IS PART OF HOLIDAY'S STRATEGY\n# utt3 ctc_align_test 3.19 4.20 -0.7433 TO SELL OFF ASSETS\n# utt4 ctc_align_test 4.20 4.97 -0.6017 AND CONCENTRATE\n# utt5 ctc_align_test 4.97 6.10 -0.3477 ON PROPERTY MANAGEMENT\n```\n\nThe output of the script can be redirected to a `segments` file by adding the argument `--output segments`.\nEach line contains the file/utterance name, utterance start and end times in seconds, and a confidence score; optionally also the utterance text.\nThe confidence score is a probability in log space that indicates how well the utterance was aligned. If needed, remove bad utterances:\n\n```sh\nmin_confidence_score=-7\n# here, we assume that the output was written to the file `segments`\nawk -v ms=${min_confidence_score} '{ if ($5 > ms) {print} }' segments\n```\n\nSee the module documentation for more information.\nIt is recommended to use models with RNN-based encoders (such as BLSTMP) for aligning large audio files;\nrather than using Transformer models that have a high memory consumption on longer audio data.\nThe sample rate of the audio must be consistent with that of the data used in training; adjust with `sox` if needed.\n\nAlso, we can use this tool to provide token-level segmentation information if we prepare a list of tokens instead of that of utterances in the `text` file. See the discussion in https://github.com/espnet/espnet/issues/4278#issuecomment-1100756463.\n\n</div></details>\n\n## Citations\n\n```\n@inproceedings{watanabe2018espnet,\n  author={Shinji Watanabe and Takaaki Hori and Shigeki Karita and Tomoki Hayashi and Jiro Nishitoba and Yuya Unno and Nelson {Enrique Yalta Soplin} and Jahn Heymann and Matthew Wiesner and Nanxin Chen and Adithya Renduchintala and Tsubasa Ochiai},\n  title={{ESPnet}: End-to-End Speech Processing Toolkit},\n  year={2018},\n  booktitle={Proceedings of Interspeech},\n  pages={2207--2211},\n  doi={10.21437/Interspeech.2018-1456},\n  url={http://dx.doi.org/10.21437/Interspeech.2018-1456}\n}\n@inproceedings{hayashi2020espnet,\n  title={{Espnet-TTS}: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit},\n  author={Hayashi, Tomoki and Yamamoto, Ryuichi and Inoue, Katsuki and Yoshimura, Takenori and Watanabe, Shinji and Toda, Tomoki and Takeda, Kazuya and Zhang, Yu and Tan, Xu},\n  booktitle={Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={7654--7658},\n  year={2020},\n  organization={IEEE}\n}\n@inproceedings{inaguma-etal-2020-espnet,\n    title = \"{ESP}net-{ST}: All-in-One Speech Translation Toolkit\",\n    author = \"Inaguma, Hirofumi  and\n      Kiyono, Shun  and\n      Duh, Kevin  and\n      Karita, Shigeki  and\n      Yalta, Nelson  and\n      Hayashi, Tomoki  and\n      Watanabe, Shinji\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.acl-demos.34\",\n    pages = \"302--311\",\n}\n@article{hayashi2021espnet2,\n  title={{ESP}net2-{TTS}: Extending the edge of {TTS} research},\n  author={Hayashi, Tomoki and Yamamoto, Ryuichi and Yoshimura, Takenori and Wu, Peter and Shi, Jiatong and Saeki, Takaaki and Ju, Yooncheol and Yasuda, Yusuke and Takamichi, Shinnosuke and Watanabe, Shinji},\n  journal={arXiv preprint arXiv:2110.07840},\n  year={2021}\n}\n@inproceedings{li2020espnet,\n  title={{ESPnet-SE}: End-to-End Speech Enhancement and Separation Toolkit Designed for {ASR} Integration},\n  author={Chenda Li and Jing Shi and Wangyou Zhang and Aswin Shanmugam Subramanian and Xuankai Chang and Naoyuki Kamo and Moto Hira and Tomoki Hayashi and Christoph Boeddeker and Zhuo Chen and Shinji Watanabe},\n  booktitle={Proceedings of IEEE Spoken Language Technology Workshop (SLT)},\n  pages={785--792},\n  year={2021},\n  organization={IEEE},\n}\n@inproceedings{arora2021espnet,\n  title={{ESPnet-SLU}: Advancing Spoken Language Understanding through ESPnet},\n  author={Arora, Siddhant and Dalmia, Siddharth and Denisov, Pavel and Chang, Xuankai and Ueda, Yushi and Peng, Yifan and Zhang, Yuekai and Kumar, Sujay and Ganesan, Karthik and Yan, Brian and others},\n  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={7167--7171},\n  year={2022},\n  organization={IEEE}\n}\n@inproceedings{shi2022muskits,\n  author={Shi, Jiatong and Guo, Shuai and Qian, Tao and Huo, Nan and Hayashi, Tomoki and Wu, Yuning and Xu, Frank and Chang, Xuankai and Li, Huazhe and Wu, Peter and Watanabe, Shinji and Jin, Qin},\n  title={{Muskits}: an End-to-End Music Processing Toolkit for Singing Voice Synthesis},\n  year={2022},\n  booktitle={Proceedings of Interspeech},\n  pages={4277-4281},\n  url={https://www.isca-speech.org/archive/pdfs/interspeech_2022/shi22d_interspeech.pdf}\n}\n@inproceedings{lu22c_interspeech,\n  author={Yen-Ju Lu and Xuankai Chang and Chenda Li and Wangyou Zhang and Samuele Cornell and Zhaoheng Ni and Yoshiki Masuyama and Brian Yan and Robin Scheibler and Zhong-Qiu Wang and Yu Tsao and Yanmin Qian and Shinji Watanabe},\n  title={{ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding}},\n  year=2022,\n  booktitle={Proc. Interspeech 2022},\n  pages={5458--5462},\n}\n@inproceedings{gao2023euro,\n  title={{EURO: ESP}net unsupervised {ASR} open-source toolkit},\n  author={Gao, Dongji and Shi, Jiatong and Chuang, Shun-Po and Garcia, Leibny Paola and Lee, Hung-yi and Watanabe, Shinji and Khudanpur, Sanjeev},\n  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={1--5},\n  year={2023},\n  organization={IEEE}\n}\n@inproceedings{peng2023reproducing,\n  title={Reproducing {W}hisper-style training using an open-source toolkit and publicly available data},\n  author={Peng, Yifan and Tian, Jinchuan and Yan, Brian and Berrebbi, Dan and Chang, Xuankai and Li, Xinjian and Shi, Jiatong and Arora, Siddhant and Chen, William and Sharma, Roshan and others},\n  booktitle={2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},\n  pages={1--8},\n  year={2023},\n  organization={IEEE}\n}\n@inproceedings{sharma2023espnet,\n  title={ESPnet-{SUMM}: Introducing a novel large dataset, toolkit, and a cross-corpora evaluation of speech summarization systems},\n  author={Sharma, Roshan and Chen, William and Kano, Takatomo and Sharma, Ruchira and Arora, Siddhant and Watanabe, Shinji and Ogawa, Atsunori and Delcroix, Marc and Singh, Rita and Raj, Bhiksha},\n  booktitle={2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},\n  pages={1--8},\n  year={2023},\n  organization={IEEE}\n}\n@article{jung2024espnet,\n  title={{ESPnet-SPK}: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models},\n  author={Jung, Jee-weon and Zhang, Wangyou and Shi, Jiatong and Aldeneh, Zakaria and Higuchi, Takuya and Theobald, Barry-John and Abdelaziz, Ahmed Hussen and Watanabe, Shinji},\n  journal={Proc. Interspeech 2024},\n  year={2024}\n}\n@inproceedings{yan-etal-2023-espnet,\n    title = \"{ESP}net-{ST}-v2: Multipurpose Spoken Language Translation Toolkit\",\n    author = \"Yan, Brian  and\n      Shi, Jiatong  and\n      Tang, Yun  and\n      Inaguma, Hirofumi  and\n      Peng, Yifan  and\n      Dalmia, Siddharth  and\n      Pol{\\'a}k, Peter  and\n      Fernandes, Patrick  and\n      Berrebbi, Dan  and\n      Hayashi, Tomoki  and\n      Zhang, Xiaohui  and\n      Ni, Zhaoheng  and\n      Hira, Moto  and\n      Maiti, Soumi  and\n      Pino, Juan  and\n      Watanabe, Shinji\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    year = \"2023\",\n    publisher = \"Association for Computational Linguistics\",\n    pages = \"400--411\",\n}\n\n```\n"
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.296875,
          "content": "# https://docs.codecov.com/docs/common-recipe-list\ncoverage:\n  status:\n    project:\n      default:\n        target: auto\n        # adjust accordingly based on how flaky your tests are\n        # this allows a 1% drop from the previous base commit coverage\n        threshold: 1%\n        informational: true\n"
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "egs",
          "type": "tree",
          "content": null
        },
        {
          "name": "egs2",
          "type": "tree",
          "content": null
        },
        {
          "name": "espnet",
          "type": "tree",
          "content": null
        },
        {
          "name": "espnet2",
          "type": "tree",
          "content": null
        },
        {
          "name": "espnetez",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.845703125,
          "content": "[aliases]\ntest=pytest\n\n[tool:pytest]\naddopts = --cov-config=.coveragerc --cov=espnet --cov=espnet2 --cov=espnetez\ntestpaths = test\nexecution_timeout = 2.0\n\n\n# [H238] old style class declaration, use new style (inherit from `object`)\n# [H102 H103] Newly contributed Source Code should be licensed under the Apache 2.0 license. All source files should have the following header::\n# [W504] Line break occurred after a binary operator\n# [H301] one import per line\n# [H306] imports not in alphabetical order\n# [E231] missing whitespace after ','\n\n# Black says \"W503, E203 is incompatible with PEP 8\"\n# [W503] Line break occurred before a binary operator\n# [E203] whitespace before :\n\n[flake8]\nignore = H102,H103,W503,H238,E203,H301,H306,E231\nmax-line-length = 88\n[pycodestyle]\nignore = H102,H103,W503,H238,E203,H301,H306,E231\nmax-line-length = 88\n[isort]\nprofile = black\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 5.1240234375,
          "content": "#!/usr/bin/env python3\n\n\"\"\"ESPnet setup script.\"\"\"\n\nimport os\n\nfrom setuptools import find_packages, setup\n\nrequirements = {\n    \"install\": [\n        \"setuptools>=38.5.1,<74.0.0\",\n        \"packaging\",\n        \"configargparse>=1.2.1\",\n        \"typeguard\",\n        \"humanfriendly\",\n        \"scipy>=1.4.1\",\n        \"filelock\",\n        \"librosa==0.9.2\",\n        \"jamo==0.4.1\",  # For kss\n        \"PyYAML>=5.1.2\",\n        \"soundfile>=0.10.2\",\n        \"h5py>=2.10.0\",\n        \"kaldiio>=2.18.0\",\n        \"torch>=1.11.0\",\n        \"torch_complex\",\n        \"nltk>=3.4.5\",\n        # fix CI error due to the use of deprecated aliases\n        \"numpy<1.24\",\n        # https://github.com/espnet/espnet/runs/6646737793?check_suite_focus=true#step:8:7651\n        \"protobuf\",\n        \"hydra-core\",\n        \"opt-einsum\",\n        # ASR\n        \"sentencepiece==0.2.0\",\n        \"ctc-segmentation>=1.6.6\",\n        # TTS\n        \"pyworld>=0.3.4\",\n        \"pypinyin<=0.44.0\",\n        \"espnet_tts_frontend\",\n        # ENH\n        \"ci_sdr\",\n        \"fast-bss-eval==0.1.3\",\n        # SPK\n        \"asteroid_filterbanks==0.4.0\",\n        # UASR\n        \"editdistance\",\n        # fix CI error due to the use of deprecated functions\n        # https://github.com/espnet/espnet/actions/runs/3174416926/jobs/5171182884#step:8:8419\n        # https://importlib-metadata.readthedocs.io/en/latest/history.html#v5-0-0\n        \"importlib-metadata<5.0\",\n    ],\n    # train: The modules invoked when training only.\n    \"train\": [\n        \"matplotlib\",\n        \"pillow==9.5.0\",\n        \"wandb\",\n        \"tensorboard>=1.14\",\n    ],\n    # recipe: The modules actually are not invoked in the main module of espnet,\n    #         but are invoked for the python scripts in each recipe\n    \"recipe\": [\n        \"espnet_model_zoo\",\n        \"gdown\",\n        \"resampy\",\n        \"pysptk>=0.2.1\",\n        \"morfessor\",  # for zeroth-korean\n        \"youtube_dl\",  # for laborotv\n        \"nnmnkwii\",\n        \"museval>=0.2.1\",\n        \"pystoi>=0.2.2\",\n        \"mir-eval>=0.6\",\n        \"fastdtw\",\n        \"nara_wpe>=0.0.5\",\n        \"sacrebleu>=1.5.1\",\n        \"praatio>=6,<7\",  # for librispeech phoneme alignment\n        \"scikit-learn>=1.0.0\",  # for HuBERT kmeans\n    ],\n    # all: The modules should be optionally installled due to some reason.\n    #      Please consider moving them to \"install\" occasionally\n    # NOTE(kamo): The modules in \"train\" and \"recipe\" are appended into \"all\"\n    \"all\": [\n        # NOTE(kamo): Append modules requiring specific pytorch version or torch>1.3.0\n        \"torchaudio\",\n        \"torch_optimizer\",\n        \"fairscale\",\n        \"transformers\",\n        \"evaluate\",\n    ],\n    \"setup\": [\n        \"pytest-runner\",\n    ],\n    \"test\": [\n        \"pytest>=7.0.0\",\n        \"pytest-timeouts>=1.2.1\",\n        \"pytest-pythonpath>=0.7.3\",\n        \"pytest-cov>=2.7.1\",\n        \"hacking>=2.0.0\",\n        \"mock>=2.0.0\",\n        \"pycodestyle\",\n        \"jsondiff>=2.0.0\",\n        \"flake8>=3.7.8\",\n        \"flake8-docstrings>=1.3.1\",\n        \"black\",\n        \"isort\",\n    ],\n    \"doc\": [\n        \"Jinja2<3.1\",\n        \"sphinx<9.0.0\",\n        \"sphinx-rtd-theme>=0.2.4\",\n        \"sphinx-argparse>=0.2.5\",\n        \"commonmark==0.8.1\",\n        \"myst-parser\",\n        \"nbsphinx>=0.4.2\",\n        \"sphinx-markdown-tables>=0.0.12\",\n        \"jupyterlab<5\",\n        \"sphinx-markdown-builder\",\n    ],\n}\nrequirements[\"all\"].extend(requirements[\"train\"] + requirements[\"recipe\"])\nrequirements[\"test\"].extend(requirements[\"train\"])\n\ninstall_requires = requirements[\"install\"]\nsetup_requires = requirements[\"setup\"]\ntests_require = requirements[\"test\"]\nextras_require = {\n    k: v for k, v in requirements.items() if k not in [\"install\", \"setup\"]\n}\n\ndirname = os.path.dirname(__file__)\nversion_file = os.path.join(dirname, \"espnet\", \"version.txt\")\nwith open(version_file, \"r\") as f:\n    version = f.read().strip()\nsetup(\n    name=\"espnet\",\n    version=version,\n    url=\"http://github.com/espnet/espnet\",\n    author=\"Shinji Watanabe\",\n    author_email=\"shinjiw@ieee.org\",\n    description=\"ESPnet: end-to-end speech processing toolkit\",\n    long_description=open(os.path.join(dirname, \"README.md\"), encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    license=\"Apache Software License\",\n    packages=find_packages(include=[\"espnet*\"]),\n    package_data={\"espnet\": [\"version.txt\"]},\n    # #448: \"scripts\" is inconvenient for developping because they are copied\n    # scripts=get_all_scripts('espnet/bin'),\n    install_requires=install_requires,\n    setup_requires=setup_requires,\n    tests_require=tests_require,\n    extras_require=extras_require,\n    python_requires=\">=3.7.0\",\n    classifiers=[\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Science/Research\",\n        \"Operating System :: POSIX :: Linux\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n    ],\n)\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "test_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}