{
  "metadata": {
    "timestamp": 1736560525676,
    "page": 124,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVIDIA/vid2vid",
      "stars": 8633,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.3427734375,
          "content": "debug*\ncheckpoints/\ndatasets/\nmodels/debug*\nmodels/flownet2*/networks/*/*egg-info\nmodels/flownet2*/networks/*/build\nmodels/flownet2*/networks/*/__pycache__\nmodels/flownet2*/networks/*/dist\nresults/\nbuild/\n*/Thumbs.db\n*/**/__pycache__\n*/*.pyc\n*/**/*.pyc\n*/**/**/*.pyc\n*/**/**/**/*.pyc\n*/**/**/**/**/*.pyc\n*/*.so*\n*/**/*.so*\n*/**/*.dylib*\n*.DS_Store\n*~\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 2.494140625,
          "content": "Copyright (C) 2017 NVIDIA Corporation. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu.\nAll rights reserved. \nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\nPermission to use, copy, modify, and distribute this software and its documentation \nfor any non-commercial purpose is hereby granted without fee, provided that the above \ncopyright notice appear in all copies and that both that copyright notice and this \npermission notice appear in supporting documentation, and that the name of the author \nnot be used in advertising or publicity pertaining to distribution of the software \nwithout specific, written prior permission.\n\nTHE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING ALL \nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE. \nIN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL \nDAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, \nWHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING \nOUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\n--------------------------- LICENSE FOR pytorch-CycleGAN-and-pix2pix ----------------\nCopyright (c) 2017, Jun-Yan Zhu and Taesung Park\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.1240234375,
          "content": "<img src='imgs/teaser.gif' align=\"right\" width=360>\r\n\r\n<br><br><br><br>\r\n\r\n# vid2vid\r\n### [Project](https://tcwang0509.github.io/vid2vid/) | [YouTube(short)](https://youtu.be/5zlcXTCpQqM) | [YouTube(full)](https://youtu.be/GrP_aOSXt5U) | [arXiv](https://arxiv.org/abs/1808.06601) | [Paper(full)](https://tcwang0509.github.io/vid2vid/paper_vid2vid.pdf)\r\n\r\nPytorch implementation for high-resolution (e.g., 2048x1024) photorealistic video-to-video translation. It can be used for turning semantic label maps into photo-realistic videos, synthesizing people talking from edge maps, or generating human motions from poses. The core of video-to-video translation is image-to-image translation. Some of our work in that space can be found in [pix2pixHD](https://github.com/NVIDIA/pix2pixHD) and [SPADE](https://github.com/NVlabs/SPADE). <br><br>\r\n[Video-to-Video Synthesis](https://tcwang0509.github.io/vid2vid/)  \r\n [Ting-Chun Wang](https://tcwang0509.github.io/)<sup>1</sup>, [Ming-Yu Liu](http://mingyuliu.net/)<sup>1</sup>, [Jun-Yan Zhu](http://people.csail.mit.edu/junyanz/)<sup>2</sup>, [Guilin Liu](https://liuguilin1225.github.io/)<sup>1</sup>, Andrew Tao<sup>1</sup>, [Jan Kautz](http://jankautz.com/)<sup>1</sup>, [Bryan Catanzaro](http://catanzaro.name/)<sup>1</sup>  \r\n <sup>1</sup>NVIDIA Corporation, <sup>2</sup>MIT CSAIL  \r\n In Neural Information Processing Systems (**NeurIPS**) 2018  \r\n\r\n## Video-to-Video Translation\r\n- Label-to-Streetview Results\r\n<p align='center'>  \r\n  <img src='imgs/city_change_styles.gif' width='440'/>  \r\n  <img src='imgs/city_change_labels.gif' width='440'/>\r\n</p>\r\n\r\n- Edge-to-Face Results\r\n<p align='center'>\r\n  <img src='imgs/face.gif' width='440'/>\r\n  <img src='imgs/face_multiple.gif' width='440'/>\r\n</p>\r\n\r\n- Pose-to-Body Results\r\n<p align='center'>\r\n  <img src='imgs/pose.gif' width='550'/>\r\n</p>\r\n\r\n- Frame Prediction Results\r\n<p align='center'>\r\n  <img src='imgs/framePredict.gif' width='550'/>\r\n</p>\r\n\r\n## Prerequisites\r\n- Linux or macOS\r\n- Python 3\r\n- NVIDIA GPU + CUDA cuDNN\r\n- PyTorch 0.4\r\n\r\n\r\n## Getting Started\r\n### Installation\r\n- Install python libraries [dominate](https://github.com/Knio/dominate) and requests.\r\n```bash\r\npip install dominate requests\r\n```\r\n- If you plan to train with face datasets, please install dlib.\r\n```bash\r\npip install dlib\r\n```\r\n- If you plan to train with pose datasets, please install [DensePose](https://github.com/facebookresearch/DensePose) and/or [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose).\r\n- Clone this repo:\r\n```bash\r\ngit clone https://github.com/NVIDIA/vid2vid\r\ncd vid2vid\r\n```\r\n- Docker Image\r\nIf you have difficulty building the repo, a docker image can be found in the `docker` folder.\r\n\r\n### Testing \r\n- Please first download example dataset by running `python scripts/download_datasets.py`.\r\n- Next, compile a snapshot of [FlowNet2](https://github.com/NVIDIA/flownet2-pytorch) by running `python scripts/download_flownet2.py`.\r\n- Cityscapes    \r\n  - Please download the pre-trained Cityscapes model by:\r\n    ```bash\r\n    python scripts/street/download_models.py\r\n    ```\r\n  - To test the model (`bash ./scripts/street/test_2048.sh`):\r\n    ```bash\r\n    #!./scripts/street/test_2048.sh\r\n    python test.py --name label2city_2048 --label_nc 35 --loadSize 2048 --n_scales_spatial 3 --use_instance --fg --use_single_G\r\n    ```\r\n    The test results will be saved in: `./results/label2city_2048/test_latest/`.\r\n\r\n  - We also provide a smaller model trained with single GPU, which produces slightly worse performance at 1024 x 512 resolution.\r\n    - Please download the model by\r\n    ```bash\r\n    python scripts/street/download_models_g1.py\r\n    ```\r\n    - To test the model (`bash ./scripts/street/test_g1_1024.sh`):\r\n    ```bash\r\n    #!./scripts/street/test_g1_1024.sh\r\n    python test.py --name label2city_1024_g1 --label_nc 35 --loadSize 1024 --n_scales_spatial 3 --use_instance --fg --n_downsample_G 2 --use_single_G\r\n    ```\r\n  - You can find more example scripts in the `scripts/street/` directory.\r\n\r\n- Faces\r\n  - Please download the pre-trained model by:\r\n    ```bash\r\n    python scripts/face/download_models.py\r\n    ```\r\n  - To test the model (`bash ./scripts/face/test_512.sh`):\r\n    ```bash\r\n    #!./scripts/face/test_512.sh\r\n    python test.py --name edge2face_512 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 512 --use_single_G\r\n    ```\r\n    The test results will be saved in: `./results/edge2face_512/test_latest/`.\r\n\r\n### Dataset\r\n- Cityscapes\r\n  - We use the Cityscapes dataset as an example. To train a model on the full dataset, please download it from the [official website](https://www.cityscapes-dataset.com/) (registration required).\r\n  - We apply a pre-trained segmentation algorithm to get the corresponding semantic maps (train_A) and instance maps (train_inst).\r\n  - Please add the obtained images to the `datasets` folder in the same way the example images are provided.\r\n- Face\r\n  - We use the [FaceForensics](http://niessnerlab.org/projects/roessler2018faceforensics.html) dataset. We then use landmark detection to estimate the face keypoints, and interpolate them to get face edges.\r\n- Pose\r\n  - We use random dancing videos found on YouTube. We then apply DensePose / OpenPose to estimate the poses for each frame.\r\n\r\n### Training with Cityscapes dataset\r\n- First, download the FlowNet2 checkpoint file by running `python scripts/download_models_flownet2.py`.\r\n- Training with 8 GPUs:\r\n  - We adopt a coarse-to-fine approach, sequentially increasing the resolution from 512 x 256, 1024 x 512, to 2048 x 1024.\r\n  - Train a model at 512 x 256 resolution (`bash ./scripts/street/train_512.sh`)\r\n  ```bash\r\n  #!./scripts/street/train_512.sh\r\n  python train.py --name label2city_512 --label_nc 35 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 6 --use_instance --fg\r\n  ```\r\n  - Train a model at 1024 x 512 resolution (must train 512 x 256 first) (`bash ./scripts/street/train_1024.sh`):\r\n  ```bash\r\n  #!./scripts/street/train_1024.sh\r\n  python train.py --name label2city_1024 --label_nc 35 --loadSize 1024 --n_scales_spatial 2 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 4 --use_instance --fg --niter_step 2 --niter_fix_global 10 --load_pretrain checkpoints/label2city_512\r\n  ```\r\nIf you have TensorFlow installed, you can see TensorBoard logs in `./checkpoints/label2city_1024/logs` by adding `--tf_log` to the training scripts.\r\n\r\n- Training with a single GPU:\r\n  - We trained our models using multiple GPUs. For convenience, we provide some sample training scripts (train_g1_XXX.sh) for single GPU users, up to 1024 x 512 resolution. Again a coarse-to-fine approach is adopted (256 x 128, 512 x 256, 1024 x 512). Performance is not guaranteed using these scripts.\r\n  - For example, to train a 256 x 128 video with a single GPU (`bash ./scripts/street/train_g1_256.sh`)\r\n  ```bash\r\n  #!./scripts/street/train_g1_256.sh\r\n  python train.py --name label2city_256_g1 --label_nc 35 --loadSize 256 --use_instance --fg --n_downsample_G 2 --num_D 1 --max_frames_per_gpu 6 --n_frames_total 6\r\n  ```\r\n\r\n- Training at full (2k x 1k) resolution\r\n  - To train the images at full resolution (2048 x 1024) requires 8 GPUs with at least 24G memory (`bash ./scripts/street/train_2048.sh`). If only GPUs with 12G/16G memory are available, please use the script `./scripts/street/train_2048_crop.sh`, which will crop the images during training. Performance is not guaranteed with this script.\r\n\r\n### Training with face datasets\r\n- If you haven't, please first download example dataset by running `python scripts/download_datasets.py`.\r\n- Run the following command to compute face landmarks for training dataset: \r\n  ```bash\r\n  python data/face_landmark_detection.py train\r\n  ```\r\n- Run the example script (`bash ./scripts/face/train_512.sh`)\r\n  ```bash\r\n  python train.py --name edge2face_512 --dataroot datasets/face/ --dataset_mode face --input_nc 15 --loadSize 512 --num_D 3 --gpu_ids 0,1,2,3,4,5,6,7 --n_gpus_gen 6 --n_frames_total 12  \r\n  ```\r\n- For single GPU users, example scripts are in train_g1_XXX.sh. These scripts are not fully tested and please use at your own discretion. If you still hit out of memory errors, try reducing `max_frames_per_gpu`.\r\n- More examples scripts can be found in `scripts/face/`.\r\n- Please refer to [More Training/Test Details](https://github.com/NVIDIA/vid2vid#more-trainingtest-details) for more explanations about training flags.\r\n\r\n\r\n### Training with pose datasets\r\n- If you haven't, please first download example dataset by running `python scripts/download_datasets.py`.\r\n- Example DensePose and OpenPose results are included. If you plan to use your own dataset, please generate these results and put them in the same way the example dataset is provided.\r\n- Run the example script (`bash ./scripts/pose/train_256p.sh`)\r\n  ```bash\r\n  python train.py --name pose2body_256p --dataroot datasets/pose --dataset_mode pose --input_nc 6 --num_D 2 --resize_or_crop ScaleHeight_and_scaledCrop --loadSize 384 --fineSize 256 --gpu_ids 0,1,2,3,4,5,6,7 --batchSize 8 --max_frames_per_gpu 3 --no_first_img --n_frames_total 12 --max_t_step 4\r\n  ```\r\n- Again, for single GPU users, example scripts are in train_g1_XXX.sh. These scripts are not fully tested and please use at your own discretion. If you still hit out of memory errors, try reducing `max_frames_per_gpu`.\r\n- More examples scripts can be found in `scripts/pose/`.\r\n- Please refer to [More Training/Test Details](https://github.com/NVIDIA/vid2vid#more-trainingtest-details) for more explanations about training flags.\r\n\r\n### Training with your own dataset\r\n- If your input is a label map, please generate label maps which are one-channel whose pixel values correspond to the object labels (i.e. 0,1,...,N-1, where N is the number of labels). This is because we need to generate one-hot vectors from the label maps. Please use `--label_nc N` during both training and testing.\r\n- If your input is not a label map, please specify `--input_nc N` where N is the number of input channels (The default is 3 for RGB images).\r\n- The default setting for preprocessing is `scaleWidth`, which will scale the width of all training images to `opt.loadSize` (1024) while keeping the aspect ratio. If you want a different setting, please change it by using the `--resize_or_crop` option. For example, `scaleWidth_and_crop` first resizes the image to have width `opt.loadSize` and then does random cropping of size `(opt.fineSize, opt.fineSize)`. `crop` skips the resizing step and only performs random cropping. `scaledCrop` crops the image while retraining the original aspect ratio. `randomScaleHeight` will randomly scale the image height to be between `opt.loadSize` and `opt.fineSize`. If you don't want any preprocessing, please specify `none`, which will do nothing other than making sure the image is divisible by 32.\r\n\r\n## More Training/Test Details\r\n- We generate frames in the video sequentially, where the generation of the current frame depends on previous frames. To generate the first frame for the model, there are 3 different ways:  \r\n  - 1. Using another generator which was trained on generating single images (e.g., pix2pixHD) by specifying `--use_single_G`. This is the option we use in the test scripts.\r\n  - 2. Using the first frame in the real sequence by specifying `--use_real_img`. \r\n  - 3. Forcing the model to also synthesize the first frame by specifying `--no_first_img`. This must be trained separately before inference.\r\n- The way we train the model is as follows: suppose we have 8 GPUs, 4 for generators and 4 for discriminators, and we want to train 28 frames. Also, assume each GPU can generate only one frame. The first GPU generates the first frame, and pass it to the next GPU, and so on. After the 4 frames are generated, they are passed to the 4 discriminator GPUs to compute the losses. Then the last generated frame becomes input to the next batch, and the next 4 frames in the training sequence are loaded into GPUs. This is repeated 7 times (4 x 7 = 28), to train all the 28 frames.\r\n- Some important flags:\r\n  - `n_gpus_gen`: the number of GPUs to use for generators (while the others are used for discriminators). We separate generators and discriminators into different GPUs since when dealing with high resolutions, even one frame cannot fit in a GPU. If the number is set to `-1`, there is no separation and all GPUs are used for both generators and discriminators (only works for low-res images).\r\n  - `n_frames_G`: the number of input frames to feed into the generator network; i.e., `n_frames_G - 1` is the number of frames we look into the past. the default is 3 (conditioned on previous two frames).\r\n  - `n_frames_D`: the number of frames to feed into the temporal discriminator. The default is 3.\r\n  - `n_scales_spatial`: the number of scales in the spatial domain. We train from the coarsest scale and all the way to the finest scale. The default is 3.\r\n  - `n_scales_temporal`: the number of scales for the temporal discriminator. The finest scale takes in the sequence in the original frame rate. The coarser scales subsample the frames by a factor of `n_frames_D` before feeding the frames into the discriminator. For example, if `n_frames_D = 3` and `n_scales_temporal = 3`, the discriminator effectively sees 27 frames. The default is 3.\r\n  - `max_frames_per_gpu`: the number of frames in one GPU during training. If you run into out of memory error, please first try to reduce this number. If your GPU memory can fit more frames, try to make this number bigger to make training faster. The default is 1.\r\n  - `max_frames_backpropagate`: the number of frames that loss backpropagates to previous frames. For example, if this number is 4, the loss on frame n will backpropagate to frame n-3. Increasing this number will slightly improve the performance, but also cause training to be less stable. The default is 1.\r\n  - `n_frames_total`: the total number of frames in a sequence we want to train with. We gradually increase this number during training.\r\n  - `niter_step`: for how many epochs do we double `n_frames_total`. The default is 5.  \r\n  - `niter_fix_global`: if this number if not 0, only train the finest spatial scale for this number of epochs before starting to fine-tune all scales.\r\n  - `batchSize`: the number of sequences to train at a time. We normally set batchSize to 1 since often, one sequence is enough to occupy all GPUs. If you want to do batchSize > 1, currently only `batchSize == n_gpus_gen` is supported.\r\n  - `no_first_img`: if not specified, the model will assume the first frame is given and synthesize the successive frames. If specified, the model will also try to synthesize the first frame instead.\r\n  - `fg`: if specified, use the foreground-background separation model as stated in the paper. The foreground labels must be specified by `--fg_labels`.\r\n  - `no_flow`: if specified, do not use flow warping and directly synthesize frames. We found this usually still works reasonably well when the background is static, while saving memory and training time.\r\n  - `sparse_D`: if specified, only apply temporal discriminator on sparse frames in the sequence. This helps save memory while having little effect on performance.\r\n- For other flags, please see `options/train_options.py` and `options/base_options.py` for all the training flags; see `options/test_options.py` and `options/base_options.py` for all the test flags.\r\n\r\n- Additional flags for edge2face examples:\r\n  - `no_canny_edge`: do not use canny edges for background as input.\r\n  - `no_dist_map`: by default, we use distrance transform on the face edge map as input. This flag will make it directly use edge maps.\r\n\r\n- Additional flags for pose2body examples:\r\n  - `densepose_only`: use only densepose results as input. Please also remember to change `input_nc` to be 3.\r\n  - `openpose_only`: use only openpose results as input. Please also remember to change `input_nc` to be 3.\r\n  - `add_face_disc`: add an additional discriminator that only works on the face region.\r\n  - `remove_face_labels`: remove densepose results for face, and add noise to openpose face results, so the network can get more robust to different face shapes. This is important if you plan to do inference on half-body videos (if not, usually this flag is unnecessary).\r\n  - `random_drop_prob`: the probability to randomly drop each pose segment during training, so the network can get more robust to missing poses at inference time. Default is 0.05.\r\n  - `basic_point_only`: if specified, only use basic joint keypoints for OpenPose output, without using any hand or face keypoints.\r\n\r\n## Citation\r\n\r\nIf you find this useful for your research, please cite the following paper.\r\n\r\n```\r\n@inproceedings{wang2018vid2vid,\r\n   author    = {Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu and Guilin Liu\r\n                and Andrew Tao and Jan Kautz and Bryan Catanzaro},\r\n   title     = {Video-to-Video Synthesis},\r\n   booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},   \r\n   year      = {2018},\r\n}\r\n```\r\n\r\n## Acknowledgments\r\nWe thank Karan Sapra, Fitsum Reda, and Matthieu Le for generating the segmentation maps for us. We also thank Lisa Rhee for allowing us to use her dance videos for training. We thank William S. Peebles for proofreading the paper.</br>\r\nThis code borrows heavily from [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) and [pix2pixHD](https://github.com/NVIDIA/pix2pixHD).\r\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "options",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 2.06640625,
          "content": "### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport time\nimport os\nimport numpy as np\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nfrom options.test_options import TestOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nimport util.util as util\nfrom util.visualizer import Visualizer\nfrom util import html\n\nopt = TestOptions().parse(save=False)\nopt.nThreads = 1   # test code only supports nThreads = 1\nopt.batchSize = 1  # test code only supports batchSize = 1\nopt.serial_batches = True  # no shuffle\nopt.no_flip = True  # no flip\nif opt.dataset_mode == 'temporal':\n    opt.dataset_mode = 'test'\n\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\nmodel = create_model(opt)\nvisualizer = Visualizer(opt)\ninput_nc = 1 if opt.label_nc != 0 else opt.input_nc\n\nsave_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.which_epoch))\nprint('Doing %d frames' % len(dataset))\nfor i, data in enumerate(dataset):\n    if i >= opt.how_many:\n        break    \n    if data['change_seq']:\n        model.fake_B_prev = None\n\n    _, _, height, width = data['A'].size()\n    A = Variable(data['A']).view(1, -1, input_nc, height, width)\n    B = Variable(data['B']).view(1, -1, opt.output_nc, height, width) if len(data['B'].size()) > 2 else None\n    inst = Variable(data['inst']).view(1, -1, 1, height, width) if len(data['inst'].size()) > 2 else None\n    generated = model.inference(A, B, inst)\n    \n    if opt.label_nc != 0:\n        real_A = util.tensor2label(generated[1], opt.label_nc)\n    else:\n        c = 3 if opt.input_nc == 3 else 1\n        real_A = util.tensor2im(generated[1][:c], normalize=False)    \n        \n    visual_list = [('real_A', real_A), \n                   ('fake_B', util.tensor2im(generated[0].data[0]))]\n    visuals = OrderedDict(visual_list) \n    img_path = data['A_path']\n    print('process image... %s' % img_path)\n    visualizer.save_images(save_dir, visuals, img_path)"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 7.1337890625,
          "content": "### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport time\nimport os\nimport torch\nfrom subprocess import call\n\nfrom options.train_options import TrainOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model, create_optimizer, init_params, save_models, update_models\nimport util.util as util\nfrom util.visualizer import Visualizer\n\ndef train():\n    opt = TrainOptions().parse()\n    if opt.debug:\n        opt.display_freq = 1\n        opt.print_freq = 1    \n        opt.nThreads = 1\n\n    ### initialize dataset\n    data_loader = CreateDataLoader(opt)\n    dataset = data_loader.load_data()\n    dataset_size = len(data_loader)    \n    print('#training videos = %d' % dataset_size)\n\n    ### initialize models\n    models = create_model(opt)\n    modelG, modelD, flowNet, optimizer_G, optimizer_D, optimizer_D_T = create_optimizer(opt, models)\n\n    ### set parameters    \n    n_gpus, tG, tD, tDB, s_scales, t_scales, input_nc, output_nc, \\\n        start_epoch, epoch_iter, print_freq, total_steps, iter_path = init_params(opt, modelG, modelD, data_loader)\n    visualizer = Visualizer(opt)    \n\n    ### real training starts here  \n    for epoch in range(start_epoch, opt.niter + opt.niter_decay + 1):\n        epoch_start_time = time.time()    \n        for idx, data in enumerate(dataset, start=epoch_iter):        \n            if total_steps % print_freq == 0:\n                iter_start_time = time.time()\n            total_steps += opt.batchSize\n            epoch_iter += opt.batchSize\n\n            # whether to collect output images\n            save_fake = total_steps % opt.display_freq == 0\n            n_frames_total, n_frames_load, t_len = data_loader.dataset.init_data_params(data, n_gpus, tG)\n            fake_B_prev_last, frames_all = data_loader.dataset.init_data(t_scales)\n\n            for i in range(0, n_frames_total, n_frames_load):\n                input_A, input_B, inst_A = data_loader.dataset.prepare_data(data, i, input_nc, output_nc)\n                \n                ###################################### Forward Pass ##########################\n                ####### generator                  \n                fake_B, fake_B_raw, flow, weight, real_A, real_Bp, fake_B_last = modelG(input_A, input_B, inst_A, fake_B_prev_last)\n\n                ####### discriminator            \n                ### individual frame discriminator          \n                real_B_prev, real_B = real_Bp[:, :-1], real_Bp[:, 1:]   # the collection of previous and current real frames\n                flow_ref, conf_ref = flowNet(real_B, real_B_prev)       # reference flows and confidences                \n                fake_B_prev = modelG.module.compute_fake_B_prev(real_B_prev, fake_B_prev_last, fake_B)\n                fake_B_prev_last = fake_B_last\n               \n                losses = modelD(0, reshape([real_B, fake_B, fake_B_raw, real_A, real_B_prev, fake_B_prev, flow, weight, flow_ref, conf_ref]))\n                losses = [ torch.mean(x) if x is not None else 0 for x in losses ]\n                loss_dict = dict(zip(modelD.module.loss_names, losses))\n\n                ### temporal discriminator                \n                # get skipped frames for each temporal scale\n                frames_all, frames_skipped = modelD.module.get_all_skipped_frames(frames_all, \\\n                        real_B, fake_B, flow_ref, conf_ref, t_scales, tD, n_frames_load, i, flowNet)                                \n\n                # run discriminator for each temporal scale\n                loss_dict_T = []\n                for s in range(t_scales):                \n                    if frames_skipped[0][s] is not None:                        \n                        losses = modelD(s+1, [frame_skipped[s] for frame_skipped in frames_skipped])\n                        losses = [ torch.mean(x) if not isinstance(x, int) else x for x in losses ]\n                        loss_dict_T.append(dict(zip(modelD.module.loss_names_T, losses)))\n\n                # collect losses\n                loss_G, loss_D, loss_D_T, t_scales_act = modelD.module.get_losses(loss_dict, loss_dict_T, t_scales)\n\n                ###################################### Backward Pass #################################                 \n                # update generator weights     \n                loss_backward(opt, loss_G, optimizer_G)                \n\n                # update individual discriminator weights                \n                loss_backward(opt, loss_D, optimizer_D)\n\n                # update temporal discriminator weights\n                for s in range(t_scales_act):                    \n                    loss_backward(opt, loss_D_T[s], optimizer_D_T[s])\n\n                if i == 0: fake_B_first = fake_B[0, 0]   # the first generated image in this sequence\n\n            if opt.debug:\n                call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=memory.used,memory.free\"]) \n\n            ############## Display results and errors ##########\n            ### print out errors\n            if total_steps % print_freq == 0:\n                t = (time.time() - iter_start_time) / print_freq\n                errors = {k: v.data.item() if not isinstance(v, int) else v for k, v in loss_dict.items()}\n                for s in range(len(loss_dict_T)):\n                    errors.update({k+str(s): v.data.item() if not isinstance(v, int) else v for k, v in loss_dict_T[s].items()})            \n                visualizer.print_current_errors(epoch, epoch_iter, errors, t)\n                visualizer.plot_current_errors(errors, total_steps)\n\n            ### display output images\n            if save_fake:                \n                visuals = util.save_all_tensors(opt, real_A, fake_B, fake_B_first, fake_B_raw, real_B, flow_ref, conf_ref, flow, weight, modelD)                \n                visualizer.display_current_results(visuals, epoch, total_steps)\n\n            ### save latest model\n            save_models(opt, epoch, epoch_iter, total_steps, visualizer, iter_path, modelG, modelD)            \n            if epoch_iter > dataset_size - opt.batchSize:\n                epoch_iter = 0\n                break\n           \n        # end of epoch \n        iter_end_time = time.time()\n        visualizer.vis_print('End of epoch %d / %d \\t Time Taken: %d sec' %\n              (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n\n        ### save model for this epoch and update model params\n        save_models(opt, epoch, epoch_iter, total_steps, visualizer, iter_path, modelG, modelD, end_of_epoch=True)\n        update_models(opt, epoch, modelG, modelD, data_loader) \n\ndef loss_backward(opt, loss, optimizer):\n    optimizer.zero_grad()                \n    if opt.fp16:\n        from apex import amp\n        with amp.scale_loss(loss, optimizer) as scaled_loss: \n            scaled_loss.backward()\n    else:\n        loss.backward()\n    optimizer.step()\n\ndef reshape(tensors):\n    if tensors is None: return None\n    if isinstance(tensors, list):\n        return [reshape(tensor) for tensor in tensors]    \n    _, _, ch, h, w = tensors.size()\n    return tensors.contiguous().view(-1, ch, h, w)\n\nif __name__ == \"__main__\":\n   train()"
        },
        {
          "name": "util",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}