{
  "metadata": {
    "timestamp": 1736560868820,
    "page": 580,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Project-MONAI/MONAI",
      "stars": 6017,
      "defaultBranch": "dev",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 2.509765625,
          "content": "---\nAccessModifierOffset: -1\nAlignAfterOpenBracket: AlwaysBreak\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nAlignEscapedNewlinesLeft: true\nAlignOperands:   false\nAlignTrailingComments: false\nAllowAllParametersOfDeclarationOnNextLine: false\nAllowShortBlocksOnASingleLine: false\nAllowShortCaseLabelsOnASingleLine: false\nAllowShortFunctionsOnASingleLine: Empty\nAllowShortIfStatementsOnASingleLine: false\nAllowShortLoopsOnASingleLine: false\nAlwaysBreakAfterReturnType: None\nAlwaysBreakBeforeMultilineStrings: true\nAlwaysBreakTemplateDeclarations: true\nBinPackArguments: false\nBinPackParameters: false\nBraceWrapping:\n  AfterClass:      false\n  AfterControlStatement: false\n  AfterEnum:       false\n  AfterFunction:   false\n  AfterNamespace:  false\n  AfterObjCDeclaration: false\n  AfterStruct:     false\n  AfterUnion:      false\n  BeforeCatch:     false\n  BeforeElse:      false\n  IndentBraces:    false\nBreakBeforeBinaryOperators: None\nBreakBeforeBraces: Attach\nBreakBeforeTernaryOperators: true\nBreakConstructorInitializersBeforeComma: false\nBreakAfterJavaFieldAnnotations: false\nBreakStringLiterals: false\nColumnLimit:     120\nCommentPragmas:  '^ IWYU pragma:'\nCompactNamespaces: false\nConstructorInitializerAllOnOneLineOrOnePerLine: true\nConstructorInitializerIndentWidth: 4\nContinuationIndentWidth: 4\nCpp11BracedListStyle: true\nDerivePointerAlignment: false\nDisableFormat:   false\nForEachMacros:   [ FOR_EACH_RANGE, FOR_EACH, ]\nIncludeCategories:\n  - Regex:           '^<.*\\.h(pp)?>'\n    Priority:        1\n  - Regex:           '^<.*'\n    Priority:        2\n  - Regex:           '.*'\n    Priority:        3\nIndentCaseLabels: true\nIndentWidth:     2\nIndentWrappedFunctionNames: false\nKeepEmptyLinesAtTheStartOfBlocks: false\nMacroBlockBegin: ''\nMacroBlockEnd:   ''\nMaxEmptyLinesToKeep: 1\nNamespaceIndentation: None\nObjCBlockIndentWidth: 2\nObjCSpaceAfterProperty: false\nObjCSpaceBeforeProtocolList: false\nPenaltyBreakBeforeFirstCallParameter: 1\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakString: 1000\nPenaltyExcessCharacter: 1000000\nPenaltyReturnTypeOnItsOwnLine: 2000000\nPointerAlignment: Left\nReflowComments:  true\nSortIncludes:    true\nSpaceAfterCStyleCast: false\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeParens: ControlStatements\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 1\nSpacesInAngles:  false\nSpacesInContainerLiterals: true\nSpacesInCStyleCastParentheses: false\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nStandard:        Cpp11\nTabWidth:        8\nUseTab:          Never\n...\n"
        },
        {
          "name": ".deepsource.toml",
          "type": "blob",
          "size": 0.3408203125,
          "content": "version = 1\n\ntest_patterns = [\"tests/**\"]\n\nexclude_patterns = [\n    \"monai/_version.py\",\n    \"versioneer.py\"\n]\n\n[[analyzers]]\nname = \"python\"\nenabled = true\n\n  [analyzers.meta]\n  runtime_version = \"3.x.x\"\n\n[[analyzers]]\nname = \"test-coverage\"\nenabled = true\n\n[[analyzers]]\nname = \"docker\"\nenabled = true\n\n[[analyzers]]\nname = \"shell\"\nenabled = true\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.15625,
          "content": "# Ignore the following files/folders during docker build\n\n__pycache__/\ndocs/\n\n.coverage\n.coverage.*\n.coverage/\ncoverage.xml\n.readthedocs.yml\n*.toml\n\n!README.md\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0302734375,
          "content": "monai/_version.py export-subst\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.2041015625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.coverage/\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# temporary unittest artifacts\ntests/testing_data/temp_*\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/build/\ndocs/source/_gen\ndocs/source/*_properties.csv\n_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# pytype cache\n.pytype/\n\n# mypy\n.mypy_cache/\nexamples/scd_lvsegs.npz\ntemp/\n.idea/\n.dmypy.json\n\n*~\n\n# Remove .pyre temporary config files\n.pyre\n.pyre_configuration\n\n# temporary editor files that should not be in git\n*.orig\n*.bak\n*.swp\n.DS_Store\n\n# temporary testing data MedNIST\ntests/testing_data/MedNIST*\ntests/testing_data/*Hippocampus*\ntests/testing_data/*.tiff\ntests/testing_data/schema.json\ntests/testing_data/endo.mp4\ntests/testing_data/ultrasound.avi\ntests/testing_data/train_data_stats.yaml\ntests/testing_data/eval_data_stats.yaml\ntests/testing_data/train_data_stats_by_case.yaml\ntests/testing_data/eval_data_stats_by_case.yaml\ntests/testing_data/CT_2D_head_fixed.mha\ntests/testing_data/CT_2D_head_moving.mha\ntests/testing_data/config_executed.json\ntests/testing_data/eval\ntests/testing_data/nrrd_example.nrrd\n\n# clang format tool\n.clang-format-bin/\n\n# ctags\ntags\n\n# VSCode\n.vscode/\n*.zip\n\n# profiling results\n*.prof\nruns\n\n*.gz\n\n*.pth\n\n*zarr/*\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.93359375,
          "content": "default_language_version:\n  python: python3\n\nci:\n  autofix_prs: true\n  autoupdate_commit_msg: '[pre-commit.ci] pre-commit suggestions'\n  autoupdate_schedule: quarterly\n  # submodules: true\n\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n      - id: end-of-file-fixer\n      - id: trailing-whitespace\n      - id: check-yaml\n      - id: check-docstring-first\n      - id: check-executables-have-shebangs\n      - id: check-toml\n      - id: check-case-conflict\n      - id: check-added-large-files\n        args: ['--maxkb=1024']\n      - id: detect-private-key\n      - id: forbid-new-submodules\n      - id: pretty-format-json\n        args: ['--autofix', '--no-sort-keys', '--indent=4']\n      - id: end-of-file-fixer\n      - id: mixed-line-ending\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.7.0\n    hooks:\n    -   id: ruff\n        args:\n        - --fix\n\n  - repo: https://github.com/asottile/pyupgrade\n    rev: v3.19.0\n    hooks:\n      - id: pyupgrade\n        args: [--py39-plus, --keep-runtime-typing]\n        name: Upgrade code with exceptions\n        exclude: |\n          (?x)(\n              ^versioneer.py|\n              ^monai/_version.py|\n              ^monai/networks/| # avoid typing rewrites\n              ^monai/apps/detection/utils/anchor_utils.py| # avoid typing rewrites\n              ^tests/test_compute_panoptic_quality.py # avoid typing rewrites\n          )\n\n  - repo: https://github.com/asottile/yesqa\n    rev: v1.5.0\n    hooks:\n      - id: yesqa\n        name: Unused noqa\n        additional_dependencies:\n          - flake8>=3.8.1\n          - flake8-bugbear<=24.2.6\n          - flake8-comprehensions\n          - pep8-naming\n        exclude: |\n          (?x)^(\n              monai/__init__.py|\n              docs/source/conf.py|\n              tests/utils.py\n          )$\n\n  - repo: https://github.com/hadialqattan/pycln\n    rev: v2.4.0\n    hooks:\n      - id: pycln\n        args: [--config=pyproject.toml]\n"
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 0.611328125,
          "content": "# .readthedocs.yml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n  configuration: docs/source/conf.py\n\n# Build documentation with MkDocs\n#mkdocs:\n#  configuration: mkdocs.yml\n\n# Optionally build your docs in additional formats such as PDF and ePub\n# formats: all\n\n# Optionally set the version of Python and requirements required to build your docs\npython:\n  version: 3\n  install:\n    - requirements: docs/requirements.txt\n#  system_packages: true\n\n\nbuild:\n  image: stable\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 62.9384765625,
          "content": "# Changelog\nAll notable changes to MONAI are documented in this file.\n\nThe format is based on [Keep a Changelog](http://keepachangelog.com/en/1.0.0/).\n\n## [Unreleased]\n\n## [1.4.0] - 2024-10-17\n## What's Changed\n### Added\n* Implemented Conjugate Gradient Solver to generate confidence maps. (#7876)\n* Added norm parameter to `ResNet` (#7752, #7805)\n* Introduced alpha parameter to `DiceFocalLoss` for improved flexibility (#7841)\n* Integrated Tailored ControlNet Implementations (#7875)\n* Integrated Tailored Auto-Encoder Model (#7861)\n* Integrated Tailored Diffusion U-Net Model (7867)\n* Added Maisi morphological functions (#7893)\n* Added support for downloading bundles from NGC private registry (#7907, #7929, #8076)\n* Integrated generative refactor into the core (#7886, #7962)\n* Made `ViT` and `UNETR` models compatible with TorchScript (#7937)\n* Implemented post-download checks for MONAI bundles and compatibility warnings (#7938)\n* Added NGC prefix argument when downloading bundles (#7974)\n* Added flash attention support in the attention block for improved performance (#7977)\n* Enhanced `MLPBlock` for compatibility with VISTA-3D (#7995)\n* Added support for Neighbor-Aware Calibration Loss (NACL) for calibrated models in segmentation tasks (#7819)\n* Added label_smoothing parameter to `DiceCELoss` for enhanced model calibration (#8000)\n* Add `include_fc` and `use_combined_linear` argument in the `SABlock` (#7996)\n* Added utilities, networks, and an inferer specific to VISTA-3D (#7999, #7987, #8047, #8059, #8021)\n* Integrated a new network, `CellSamWrapper`, for cell-based applications (#7981)\n* Introduced `WriteFileMapping` transform to map between input image paths and their corresponding output paths (#7769)\n* Added `TrtHandler` to accelerate models using TensorRT (#7990, #8064)\n* Added box and points conversion transforms for more flexible spatial manipulation (#8053)\n* Enhanced `RandSimulateLowResolutiond` transform with deterministic support (#8057)\n* Added a contiguous argument to the `Fourier` class to facilitate contiguous tensor outputs (#7969)\n* Allowed `ApplyTransformToPointsd` to receive a sequence of reference keys for more versatile point manipulation (#8063)\n* Made `MetaTensor` an optional print in `DataStats` and `DataStatsd` for more concise logging (#7814)\n#### misc.\n* Refactored Dataset to utilize Compose for handling transforms. (#7784)\n* Combined `map_classes_to_indices` and `generate_label_classes_crop_centers` into a unified function (#7712)\n* Introduced metadata schema directly into the codebase for improved structure and validation (#7409)\n* Renamed `optional_packages_version` to `required_packages_version` for clearer package dependency management. (#7253)\n* Replaced `pkg_resources` with the more modern packaging module for package handling (#7953)\n* Refactored MAISI-related networks to align with the new generative components (#7989, #7993, #8005)\n* Added a badge displaying monthly download statistics to enhance project visibility (#7891)\n### Fixed\n#### transforms\n* Ensured deterministic behavior in `MixUp`, `CutMix`, and `CutOut` transforms (#7813)\n* Applied a minor correction to `AsDiscrete` transform (#7984)\n* Fixed handling of integer weightmaps in `RandomWeightedCrop` (#8097)\n* Resolved data type bug in `ScaleIntensityRangePercentile` (#8109)\n#### data\n* Fixed negative strides issue in the `NrrdReader` (#7809)\n* Addressed wsireader issue with retrieving MPP (7921)\n* Ensured location is returned as a tuple in wsireader (#8007)\n* Corrected interpretation of space directions in nrrd reader (#8091)\n#### metrics and losses\n* Improved memory management for `NACLLoss` (#8020)\n* Fixed reduction logic in `GeneralizedDiceScore` (#7970)\n#### networks\n* Resolved issue with loading pre-trained weights in `ResNet` (#7924)\n* Fixed error where `torch.device` object had no attribute gpu_id during TensorRT export (#8019)\n* Corrected function for loading older weights in `DiffusionModelUNet` (#8031)\n* Switched to `torch_tensorrt.Device` instead of `torch.device` during TensorRT compilation (#8051)\n#### engines and handlers\n* Attempted to resolve the \"experiment already exists\" issue in `MLFlowHandler` (#7916)\n* Refactored the model export process for conversion and saving (#7934)\n#### misc.\n* Adjusted requirements to exclude Numpy version 2.0 (#7859)\n* Updated deprecated `scipy.ndimage` namespaces in optional imports (#7847, #7897)\n* Resolved `load_module()` deprecation in Python 3.12 (#7881)\n* Fixed Ruff type check issues (#7885)\n* Cleaned disk space in the conda test pipeline (#7902)\n* Replaced deprecated `pkgutil.find_loader` usage  (#7906)\n* Enhanced docstrings in various modules (#7913, #8055)\n* Test cases fixing (#7905, #7794, #7808)\n* Fix mypy issue introduced in 1.11.0 (#7941)\n* Cleaned up warnings during test collection (#7914)\n* Fix incompatible types in assignment issue (#7950)\n* Fix outdated link in the docs (#7971)\n* Addressed CI issues (#7983, #8013)\n* Fix module can not import correctly issue (#8015)\n* Fix AttributeError when using torch.min and max (#8041)\n* Ensure synchronization by adding `cuda.synchronize` (#8058)\n* Ignore warning from nptyping as workaround (#8062)\n* Suppress deprecated warning when importing monai (#8067)\n* Fix link in test bundle under MONAI-extra-test-data (#8092)\n### Changed\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:24.08-py3` from `nvcr.io/nvidia/pytorch:23.08-py3`\n* Change blossom-ci to ACL security format (#7843)\n* Move PyType test to weekly test (#8025)\n* Adjusted to meet Numpy 2.0 requirements (#7857)\n### Deprecated\n* Dropped support for Python 3.8 (#7909)\n* Remove deprecated arguments and class for v1.4 (#8079)\n### Removed\n* Remove use of deprecated python 3.12 strtobool (#7900)\n* Removed the pipeline for publishing to testpypi (#8086)\n* Cleaning up some very old and now obsolete infrastructure (#8113, #8118, #8121)\n\n## [1.3.2] - 2024-06-25\n### Fixed\n#### misc.\n* Updated Numpy version constraint to < 2.0 (#7859)\n\n## [1.3.1] - 2024-05-17\n### Added\n* Support for `by_measure` argument in `RemoveSmallObjects` (#7137)\n* Support for `pretrained` flag in `ResNet` (#7095)\n* Support for uploading and downloading bundles to and from the Hugging Face Hub (#6454)\n* Added weight parameter in DiceLoss to apply weight to voxels of each class (#7158)\n* Support for returning dice for each class in `DiceMetric` (#7163)\n* Introduced `ComponentStore` for storage purposes (#7159)\n* Added utilities used in MONAI Generative (#7134)\n* Enabled Python 3.11 support for `convert_to_torchscript` and `convert_to_onnx` (#7182)\n* Support for MLflow in `AutoRunner` (#7176)\n* `fname_regex` option in PydicomReader (#7181)\n* Allowed setting AutoRunner parameters from config (#7175)\n* `VoxelMorphUNet` and `VoxelMorph` (#7178)\n* Enabled `cache` option in `GridPatchDataset` (#7180)\n* Introduced `class_labels` option in `write_metrics_reports` for improved readability (#7249)\n* `DiffusionLoss` for image registration task (#7272)\n* Supported specifying `filename` in `Saveimage` (#7318)\n* Compile support in `SupervisedTrainer` and `SupervisedEvaluator` (#7375)\n* `mlflow_experiment_name` support in `Auto3DSeg` (#7442)\n* Arm support (#7500)\n* `BarlowTwinsLoss` for representation learning (#7530)\n* `SURELoss` and `ConjugateGradient` for diffusion models (#7308)\n* Support for `CutMix`, `CutOut`, and `MixUp` augmentation techniques (#7198)\n* `meta_file` and `logging_file` options to `BundleWorkflow` (#7549)\n* `properties_path` option to `BundleWorkflow` for customized properties (#7542)\n* Support for both soft and hard clipping in `ClipIntensityPercentiles` (#7535)\n* Support for not saving artifacts in `MLFlowHandler` (#7604)\n* Support for multi-channel images in `PerceptualLoss` (#7568)\n* Added ResNet backbone for `FlexibleUNet` (#7571)\n* Introduced `dim_head` option in `SABlock` to set dimensions for each head (#7664)\n* Direct links to github source code to docs (#7738, #7779)\n#### misc.\n* Refactored `list_data_collate` and `collate_meta_tensor` to utilize the latest PyTorch API (#7165)\n* Added __str__ method in `Metric` base class (#7487)\n* Made enhancements for testing files (#7662, #7670, #7663, #7671, #7672)\n* Improved documentation for bundles (#7116)\n### Fixed\n#### transforms\n* Addressed issue where lazy mode was ignored in `SpatialPadd` (#7316)\n* Tracked applied operations in `ImageFilter` (#7395)\n* Warnings are now given only if missing class is not set to 0 in `generate_label_classes_crop_centers` (#7602)\n* Input is now always converted to C-order in `distance_transform_edt` to ensure consistent behavior (#7675)\n#### data\n* Modified .npz file behavior to use keys in `NumpyReader` (#7148)\n* Handled corrupted cached files in `PersistentDataset` (#7244)\n* Corrected affine update in `NrrdReader` (#7415)\n#### metrics and losses\n* Addressed precision issue in `get_confusion_matrix` (#7187)\n* Harmonized and clarified documentation and tests for dice losses variants (#7587)\n#### networks\n* Removed hard-coded `spatial_dims` in `SwinTransformer` (#7302)\n* Fixed learnable `position_embeddings` in `PatchEmbeddingBlock` (#7564, #7605)\n* Removed `memory_pool_limit` in TRT config (#7647)\n* Propagated `kernel_size` to `ConvBlocks` within `AttentionUnet` (#7734)\n* Addressed hard-coded activation layer in `ResNet` (#7749)\n#### bundle\n* Resolved bundle download issue (#7280)\n* Updated `bundle_root` directory for `NNIGen` (#7586)\n* Checked for `num_fold` and failed early if incorrect (#7634)\n* Enhanced logging logic in `ConfigWorkflow` (#7745)\n#### misc.\n* Enabled chaining in `Auto3DSeg` CLI (#7168)\n* Addressed useless error message in `nnUNetV2Runner` (#7217)\n* Resolved typing and deprecation issues in Mypy (#7231)\n* Quoted `$PY_EXE` variable to handle Python path that contains spaces in Bash (#7268)\n* Improved documentation, code examples, and warning messages in various modules (#7234, #7213, #7271, #7326, #7569, #7584)\n* Fixed typos in various modules (#7321, #7322, #7458, #7595, #7612)\n* Enhanced docstrings in various modules (#7245, #7381, #7746)\n* Handled error when data is on CPU in `DataAnalyzer` (#7310)\n* Updated version requirements for third-party packages (#7343, #7344, #7384, #7448, #7659, #7704, #7744, #7742, #7780)\n* Addressed incorrect slice compute in `ImageStats` (#7374)\n* Avoided editing a loop's mutable iterable to address B308 (#7397)\n* Fixed issue with `CUDA_VISIBLE_DEVICES` setting being ignored (#7408, #7581)\n* Avoided changing Python version in CICD (#7424)\n* Renamed partial to callable in instantiate mode (#7413)\n* Imported AttributeError for Python 3.12 compatibility (#7482)\n* Updated `nnUNetV2Runner` to support nnunetv2 2.2 (#7483)\n* Used uint8 instead of int8 in `LabelStats` (#7489)\n* Utilized subprocess for nnUNet training (#7576)\n* Addressed deprecated warning in ruff (#7625)\n* Fixed downloading failure on FIPS machine (#7698)\n* Updated `torch_tensorrt` compile parameters to avoid warning (#7714)\n* Restrict `Auto3DSeg` fold input based on datalist (#7778)\n### Changed\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:24.03-py3` from `nvcr.io/nvidia/pytorch:23.08-py3`\n### Removed\n* Removed unrecommended star-arg unpacking after a keyword argument, addressed B026 (#7262)\n* Skipped old PyTorch version test for `SwinUNETR` (#7266)\n* Dropped docker build workflow and migrated to Nvidia Blossom system (#7450)\n* Dropped Python 3.8 test on quick-py3 workflow (#7719)\n\n## [1.3.0] - 2023-10-12\n### Added\n* Intensity transforms `ScaleIntensityFixedMean` and `RandScaleIntensityFixedMean` (#6542)\n* `UltrasoundConfidenceMapTransform` used for computing confidence map from an ultrasound image (#6709)\n* `channel_wise` support in `RandScaleIntensity` and `RandShiftIntensity` (#6793, #7025)\n* `RandSimulateLowResolution` and `RandSimulateLowResolutiond` (#6806)\n* `SignalFillEmptyd` (#7011)\n* Euclidean distance transform `DistanceTransformEDT` with GPU support (#6981)\n* Port loss and metrics from `monai-generative` (#6729, #6836)\n* Support `invert_image` and `retain_stats` in `AdjustContrast` and `RandAdjustContrast` (#6542)\n* New network `DAF3D` and `Quicknat` (#6306)\n* Support `sincos` position embedding (#6986)\n* `ZarrAvgMerger` used for patch inference (#6633)\n* Dataset tracking support to `MLFlowHandler` (#6616)\n* Considering spacing and subvoxel borders in `SurfaceDiceMetric` (#6681)\n* CUCIM support for surface-related metrics (#7008)\n* `loss_fn` support in `IgniteMetric` and renamed it to `IgniteMetricHandler` (#6695)\n* `CallableEventWithFilter` and `Events` options for `trigger_event` in `GarbageCollector` (#6663)\n* Support random sorting option to `GridPatch`, `RandGridPatch`, `GridPatchd` and `RandGridPatchd` (#6701)\n* Support multi-threaded batch sampling in `PatchInferer` (#6139)\n* `SoftclDiceLoss` and `SoftDiceclDiceLoss` (#6763)\n* `HausdorffDTLoss` and `LogHausdorffDTLoss` (#6994)\n* Documentation for `TensorFloat-32` (#6770)\n* Docstring format guide (#6780)\n* `GDSDataset` support for GDS (#6778)\n* PyTorch backend support for `MapLabelValue` (#6872)\n* `filter_func` in `copy_model_state` to filter the weights to be loaded  and `filter_swinunetr` (#6917)\n* `stats_sender` to `MonaiAlgo` for FL stats (#6984)\n* `freeze_layers` to help freeze specific layers (#6970)\n#### misc.\n* Refactor multi-node running command used in `Auto3DSeg` into dedicated functions (#6623)\n* Support str type annotation to `device` in `ToTensorD` (#6737)\n* Improve logging message and file name extenstion in `DataAnalyzer` for `Auto3DSeg` (#6758)\n* Set `data_range` as a property in `SSIMLoss` (#6788)\n* Unify environment variable access (#7084)\n* `end_lr` support in `WarmupCosineSchedule` (#6662)\n* Add `ClearML` as optional dependency (#6827)\n* `yandex.disk` support in `download_url` (#6667)\n* Improve config expression error message (#6977)\n### Fixed\n#### transforms\n* Make `convert_box_to_mask` throw errors when box size larger than the image (#6637)\n* Fix lazy mode in `RandAffine` (#6774)\n* Raise `ValueError` when `map_items` is bool in `Compose` (#6882)\n* Improve performance for `NormalizeIntensity` (#6887)\n* Fix mismatched shape in `Spacing` (#6912)\n* Avoid FutureWarning in `CropForeground` (#6934)\n* Fix `Lazy=True` ignored when using `Dataset` call (#6975)\n* Shape check for arbitrary types for DataStats (#7082)\n#### data\n* Fix wrong spacing checking logic in `PydicomReader` and broken link in `ITKReader` (#6660)\n* Fix boolean indexing of batched `MetaTensor` (#6781)\n* Raise warning when multiprocessing in `DataLoader` (#6830)\n* Remove `shuffle` in `DistributedWeightedRandomSampler` (#6886)\n* Fix missing `SegmentDescription` in `PydicomReader` (#6937)\n* Fix reading dicom series error in `ITKReader` (#6943)\n* Fix KeyError in `PydicomReader` (#6946)\n* Update `metatensor_to_itk_image` to accept RAS `MetaTensor` and update default 'space' in `NrrdReader` to `SpaceKeys.LPS` (#7000)\n* Collate common meta dictionary keys (#7054)\n#### metrics and losses\n* Fixed bug in `GeneralizedDiceLoss` when `batch=True` (#6775)\n* Support for `BCEWithLogitsLoss` in `DiceCELoss` (#6924)\n* Support for `weight` in Dice and related losses (#7098)\n#### networks\n* Use `np.prod` instead of `np.product` (#6639)\n* Fix dimension issue in `MBConvBlock` (#6672)\n* Fix hard-coded `up_kernel_size` in `ViTAutoEnc` (#6735)\n* Remove hard-coded `bias_downsample` in `resnet` (#6848)\n* Fix unused `kernel_size` in `ResBlock` (#6999)\n* Allow for defining reference grid on non-integer coordinates (#7032)\n* Padding option for autoencoder (#7068)\n* Lower peak memory usage for SegResNetDS (#7066)\n#### bundle\n* Set `train_dataset_data` and `dataset_data` to unrequired in BundleProperty (#6607)\n* Set `None` to properties that do not have `REF_ID` (#6607)\n* Fix `AttributeError` for default value in `get_parsed_content` for `ConfigParser` (#6756)\n* Update `monai.bundle.scripts` to support NGC hosting (#6828, #6997)\n* Add `MetaProperties` (#6835)\n* Add `create_workflow` and update `load` function (#6835)\n* Add bundle root directory to Python search directories automatically (#6910)\n* Generate properties for bundle docs automatically (#6918)\n* Move `download_large_files` from model zoo to core (#6958)\n* Bundle syntax `#` as alias of `::` (#6955)\n* Fix bundle download naming issue (#6969, #6963)\n* Simplify the usage of `ckpt_export` (#6965)\n* `update_kwargs` in `monai.bundle.script` for merging multiple configs (#7109)\n#### engines and handlers\n* Added int options for `iteration_log` and `epoch_log` in `TensorBoardStatsHandler` (#7027)\n* Support to run validator at training start (#7108)\n#### misc.\n* Fix device fallback error in `DataAnalyzer` (#6658)\n* Add int check for  `current_mode` in `convert_applied_interp_mode` (#6719)\n* Consistent type in `convert_to_contiguous` (#6849)\n* Label `argmax` in `DataAnalyzer` when retry on CPU (#6852)\n* Fix `DataAnalyzer` with `histogram_only=True` (#6874)\n* Fix `AttributeError` in `RankFilter` in single GPU environment (#6895)\n* Remove the default warning on `TORCH_ALLOW_TF32_CUBLAS_OVERRIDE` and add debug print info (#6909)\n* Hide user information in `print_config` (#6913, #6922)\n* Optionally pass coordinates to predictor during sliding window (#6795)\n* Proper ensembling when trained with a sigmoid in `AutoRunner` (#6588)\n* Fixed `test_retinanet` by increasing absolute differences (#6615)\n* Add type check to avoid comparing a np.array with a string in `_check_kwargs_are_present` (#6624)\n* Fix md5 hashing with FIPS mode (#6635)\n* Capture failures from Auto3DSeg related subprocess calls (#6596)\n* Code formatting tool for user-specified directory (#7106)\n* Various docstring fixes\n### Changed\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:23.08-py3` from `nvcr.io/nvidia/pytorch:23.03-py3`\n### Deprecated\n* `allow_smaller=True`; `allow_smaller=False` will be the new default in `CropForeground` and `generate_spatial_bounding_box` (#6736)\n* `dropout_prob` in `VNet` in favor of `dropout_prob_down` and `dropout_prob_up` (#6768)\n* `workflow` in `BundleWorkflow` in favor of `workflow_type`(#6768)\n* `pos_embed` in `PatchEmbeddingBlock` in favor of `proj_type`(#6986)\n* `net_name` and `net_kwargs` in `download` in favor of `model`(#7016)\n* `img_size` parameter in SwinUNETR (#7093)\n### Removed\n* `pad_val`, `stride`, `per_channel` and `upsampler` in `OcclusionSensitivity` (#6642)\n* `compute_meaniou` (#7019)\n* `AsChannelFirst`, `AddChannel`and `SplitChannel` (#7019)\n* `create_multigpu_supervised_trainer` and `create_multigpu_supervised_evaluator` (#7019)\n* `runner_id` in `run` (#7019)\n* `data_src_cfg_filename` in `AlgoEnsembleBuilder` (#7019)\n* `get_validation_stats` in `Evaluator` and `get_train_stats` in `Trainer` (#7019)\n* `epoch_interval` and `iteration_interval` in `TensorBoardStatsHandler` (#7019)\n* some self-hosted test (#7041)\n\n## [1.2.0] - 2023-06-08\n### Added\n* Various Auto3DSeg enhancements and integration tests including multi-node multi-GPU optimization, major usability improvements\n* TensorRT and ONNX support for `monai.bundle` API and the relevant models\n* nnU-Net V2 integration `monai.apps.nnunet`\n* Binary and categorical metrics and event handlers using `MetricsReloaded`\n* Python module and CLI entry point for bundle workflows in `monai.bundle.workflows` and `monai.fl.client`\n* Modular patch inference API including `PatchInferer`, `merger`, and `splitter`\n* Initial release of lazy resampling including transforms and MetaTensor implementations\n* Bridge for ITK Image object and MetaTensor `monai.data.itk_torch_bridge`\n* Sliding window inference memory efficiency optimization including `SlidingWindowInfererAdapt`\n* Generic kernel filtering transforms `ImageFiltered` and `RandImageFiltered`\n* Trainable bilateral filters and joint bilateral filters\n* ClearML stats and image handlers for experiment tracking\n#### misc.\n* Utility functions to warn API default value changes (#5738)\n* Support of dot notation to access content of `ConfigParser` (#5813)\n* Softmax version to focal loss (#6544)\n* FROC metric for N-dimensional (#6528)\n* Extend SurfaceDiceMetric for 3D images (#6549)\n* A `track_meta` option for Lambda and derived transforms (#6385)\n* CLIP pre-trained text-to-vision embedding (#6282)\n* Optional spacing to surface distances calculations (#6144)\n* `WSIReader` read by power and mpp (#6244)\n* Support GPU tensor for `GridPatch` and `GridPatchDataset` (#6246)\n* `SomeOf` transform composer (#6143)\n* GridPatch with both count and threshold filtering (#6055)\n### Fixed\n#### transforms\n* `map_classes_to_indices` efficiency issue (#6468)\n* Adaptive resampling mode based on backends (#6429)\n* Improve Compose encapsulation (#6224)\n* User-provided `FolderLayout` in `SaveImage` and `SaveImaged` transforms (#6213)\n* `SpacingD` output shape compute stability (#6126)\n* No mutate ratio /user inputs `croppad` (#6127)\n* A `warn` flag to RandCropByLabelClasses (#6121)\n* `nan` to indicate `no_channel`, split dim singleton (#6090)\n* Compatible padding mode (#6076)\n* Allow for missing `filename_or_obj` key (#5980)\n* `Spacing` pixdim in-place change (#5950)\n* Add warning in `RandHistogramShift` (#5877)\n* Exclude `cuCIM` wrappers from `get_transform_backends` (#5838)\n#### data\n* `__format__` implementation of MetaTensor (#6523)\n* `channel_dim` in `TiffFileWSIReader` and `CuCIMWSIReader` (#6514)\n* Prepend `\"meta\"` to `MetaTensor.__repr__` and `MetaTensor.__str__` for easier identification (#6214)\n* MetaTensor slicing issue (#5845)\n* Default writer flags (#6147)\n* `WSIReader` defaults and tensor conversion (#6058)\n* Remove redundant array copy for WSITiffFileReader (#6089)\n* Fix unused arg in `SlidingPatchWSIDataset` (#6047)\n* `reverse_indexing` for PILReader (#6008)\n* Use `np.linalg` for the small affine inverse (#5967)\n#### metrics and losses\n* Removing L2-norm in contrastive loss (L2-norm already present in CosSim) (#6550)\n* Fixes the SSIM metric (#6250)\n* Efficiency issues of Dice metrics (#6412)\n* Generalized Dice issue (#5929)\n* Unify output tensor devices for multiple metrics (#5924)\n#### networks\n* Make `RetinaNet` throw errors for NaN only when training (#6479)\n* Replace deprecated arg in torchvision models (#6401)\n* Improves NVFuser import check (#6399)\n* Add `device` in `HoVerNetNuclearTypePostProcessing` and `HoVerNetInstanceMapPostProcessing` (#6333)\n* Enhance hovernet load pretrained function (#6269)\n* Access to the `att_mat` in self-attention modules (#6493)\n* Optional swinunetr-v2 (#6203)\n* Add transform to handle empty box as training data for `retinanet_detector` (#6170)\n* GPU utilization of DiNTS network (#6050)\n* A pixelshuffle upsample shape mismatch problem (#5982)\n* GEGLU activation function for the MLP Block (#5856)\n* Constructors for `DenseNet` derived classes (#5846)\n* Flexible interpolation modes in `regunet` (#5807)\n#### bundle\n* Optimized the `deepcopy` logic in `ConfigParser` (#6464)\n* Improve check and error message of bundle run (#6400)\n* Warn or raise ValueError on duplicated key in json/yaml config (#6252)\n* Default metadata and logging values for bundle run (#6072)\n* `pprint` head and tail in bundle script (#5969)\n* Config parsing issue for substring reference (#5932)\n* Fix instantiate for object instantiation with attribute `path` (#5866)\n* Fix `_get_latest_bundle_version` issue on Windows (#5787)\n#### engines and handlers\n* MLflow handler run bug (#6446)\n* `monai.engine` training attribute check (#6132)\n* Update StatsHandler logging message (#6051)\n* Added callable options for `iteration_log` and `epoch_log` in TensorBoard and MLFlow (#5976)\n* `CheckpointSaver` logging error (#6026)\n* Callable options for `iteration_log` and `epoch_log` in StatsHandler (#5965)\n#### misc.\n* Avoid creating cufile.log when `import monai` (#6106)\n* `monai._extensions` module compatibility with rocm (#6161)\n* Issue of repeated UserWarning: \"TypedStorage is deprecated\" (#6105)\n* Use logging config at module level (#5960)\n* Add ITK to the list of optional dependencies (#5858)\n* `RankFilter` to skip logging when the rank is not meeting criteria (#6243)\n* Various documentation issues\n### Changed\n* Overall more precise and consistent type annotations\n* Optionally depend on PyTorch-Ignite v0.4.11 instead of v0.4.10\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:23.03-py3` from `nvcr.io/nvidia/pytorch:22.10-py3`\n### Deprecated\n* `resample=True`; `resample=False` will be the new default in `SaveImage`\n* `random_size=True`; `random_size=False` will be the new default for the random cropping transforms\n* `image_only=False`; `image_only=True` will be the new default in `LoadImage`\n* `AddChannel` and `AsChannelFirst` in favor of `EnsureChannelFirst`\n### Removed\n* Deprecated APIs since v0.9, including WSIReader from `monai.apps`, `NiftiSaver` and `PNGSaver` from `monai.data`\n* Support for PyTorch 1.8\n* Support for Python 3.7\n\n## [1.1.0] - 2022-12-19\n### Added\n* Hover-Net based digital pathology workflows including new network, loss, postprocessing, metric, training, and inference modules\n* Various enhancements for Auto3dSeg `AutoRunner` including template caching, selection, and a dry-run mode `nni_dry_run`\n* Various enhancements for Auto3dSeg algo templates including new state-of-the-art configurations, optimized GPU memory utilization\n* New bundle API and configurations to support experiment management including `MLFlowHandler`\n* New `bundle.script` API to support model zoo query and download\n* `LossMetric` metric to compute loss as cumulative metric measurement\n* Transforms and base transform APIs including `RandomizableTrait` and `MedianSmooth`\n* `runtime_cache` option for `CacheDataset` and the derived classes to allow for shared caching on the fly\n* Flexible name formatter for `SaveImage` transform\n* `pending_operations` MetaTensor property and basic APIs for lazy image resampling\n* Contrastive sensitivity for SSIM metric\n* Extensible backbones for `FlexibleUNet`\n* Generalize `SobelGradients` to 3D and any spatial axes\n* `warmup_multiplier` option for `WarmupCosineSchedule`\n* F beta score metric based on confusion matrix metric\n* Support of key overwriting in `Lambdad`\n* Basic premerge tests for Python 3.11\n* Unit and integration tests for CUDA 11.6, 11.7 and A100 GPU\n* `DataAnalyzer` handles minor image-label shape inconsistencies\n### Fixed\n* Review and enhance previously untyped APIs with additional type annotations and casts\n* `switch_endianness` in LoadImage now supports tensor input\n* Reduced memory footprint for various Auto3dSeg tests\n* Issue of `@` in `monai.bundle.ReferenceResolver`\n* Compatibility issue with ITK-Python 5.3 (converting `itkMatrixF44` for default collate)\n* Inconsistent of sform and qform when using different backends for `SaveImage`\n* `MetaTensor.shape` call now returns a `torch.Size` instead of tuple\n* Issue of channel reduction in `GeneralizedDiceLoss`\n* Issue of background handling before softmax in `DiceFocalLoss`\n* Numerical issue of `LocalNormalizedCrossCorrelationLoss`\n* Issue of incompatible view size in `ConfusionMatrixMetric`\n* `NetAdapter` compatibility with Torchscript\n* Issue of `extract_levels` in `RegUNet`\n* Optional `bias_downsample` in `ResNet`\n* `dtype` overflow for `ShiftIntensity` transform\n* Randomized transforms such as `RandCuCIM` now inherit `RandomizableTrait`\n* `fg_indices.size` compatibility issue in `generate_pos_neg_label_crop_centers`\n* Issue when inverting `ToTensor`\n* Issue of capital letters in filename suffixes check in `LoadImage`\n* Minor tensor compatibility issues in `apps.nuclick.transforms`\n* Issue of float16 in `verify_net_in_out`\n* `std` variable type issue for `RandRicianNoise`\n* `DataAnalyzer` accepts `None` as label key and checks empty labels\n* `iter_patch_position` now has a smaller memory footprint\n* `CumulativeAverage` has been refactored and enhanced to allow for simple tracking of metric running stats.\n* Multi-threading issue for `MLFlowHandler`\n### Changed\n* Printing a MetaTensor now generates a less verbose representation\n* `DistributedSampler` raises a ValueError if there are too few devices\n* OpenCV and `VideoDataset` modules are loaded lazily to avoid dependency issues\n* `device` in `monai.engines.Workflow` supports string values\n* `Activations` and `AsDiscrete` take `kwargs` as additional arguments\n* `DataAnalyzer` is now more efficient and writes summary stats before detailed all case stats\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:22.10-py3` from `nvcr.io/nvidia/pytorch:22.09-py3`\n* Simplified Conda environment file `environment-dev.yml`\n* Versioneer dependency upgraded to `0.23` from `0.19`\n### Deprecated\n* `NibabelReader` input argument `dtype` is deprecated, the reader will use the original dtype of the image\n### Removed\n* Support for PyTorch 1.7\n\n## [1.0.1] - 2022-10-24\n### Fixes\n* DiceCELoss for multichannel targets\n* Auto3DSeg DataAnalyzer out-of-memory error and other minor issues\n* An optional flag issue in the RetinaNet detector\n* An issue with output offset for Spacing\n* A `LoadImage` issue when `track_meta` is `False`\n* 1D data output error in `VarAutoEncoder`\n* An issue with resolution computing in `ImageStats`\n### Added\n* Flexible min/max pixdim options for Spacing\n* Upsample mode `deconvgroup` and optional kernel sizes\n* Docstrings for gradient-based saliency maps\n* Occlusion sensitivity to use sliding window inference\n* Enhanced Gaussian window and device assignments for sliding window inference\n* Multi-GPU support for MonaiAlgo\n* `ClientAlgoStats` and `MonaiAlgoStats` for federated summary statistics\n* MetaTensor support for `OneOf`\n* Add a file check for bundle logging config\n* Additional content and an authentication token option for bundle info API\n* An anti-aliasing option for `Resized`\n* `SlidingWindowInferer` adaptive device based on `cpu_thresh`\n* `SegResNetDS` with deep supervision and non-isotropic kernel support\n* Premerge tests for Python 3.10\n### Changed\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:22.09-py3` from `nvcr.io/nvidia/pytorch:22.08-py3`\n* Replace `None` type metadata content with `\"none\"` for `collate_fn` compatibility\n* HoVerNet Mode and Branch to independent StrEnum\n* Automatically infer device from the first item in random elastic deformation dict\n* Add channel dim in `ComputeHoVerMaps` and `ComputeHoVerMapsd`\n* Remove batch dim in `SobelGradients` and `SobelGradientsd`\n### Deprecated\n* Deprecating `compute_meandice`, `compute_meaniou` in `monai.metrics`, in favor of\n`compute_dice` and `compute_iou` respectively\n\n## [1.0.0] - 2022-09-16\n### Added\n* `monai.auto3dseg` base APIs and `monai.apps.auto3dseg` components for automated machine learning (AutoML) workflow\n* `monai.fl` module with base APIs and `MonaiAlgo` for federated learning client workflow\n* An initial backwards compatibility [guide](https://github.com/Project-MONAI/MONAI/blob/dev/CONTRIBUTING.md#backwards-compatibility)\n* Initial release of accelerated MRI reconstruction components, including `CoilSensitivityModel`\n* Support of `MetaTensor` and new metadata attributes for various digital pathology components\n* Various `monai.bundle` enhancements for MONAI model-zoo usability, including config debug mode and `get_all_bundles_list`\n* new `monai.transforms` components including `SignalContinuousWavelet` for 1D signal, `ComputeHoVerMaps` for digital pathology, and `SobelGradients` for spatial gradients\n* `VarianceMetric` and `LabelQualityScore` metrics for active learning\n* Dataset API for real-time stream and videos\n* Several networks and building blocks including `FlexibleUNet` and `HoVerNet`\n* `MeanIoUHandler` and `LogfileHandler` workflow event handlers\n* `WSIReader` with the TiffFile backend\n* Multi-threading in `WSIReader` with cuCIM backend\n* `get_stats` API in `monai.engines.Workflow`\n* `prune_meta_pattern` in `monai.transforms.LoadImage`\n* `max_interactions` for deepedit interaction workflow\n* Various profiling utilities in `monai.utils.profiling`\n### Changed\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:22.08-py3` from `nvcr.io/nvidia/pytorch:22.06-py3`\n* Optionally depend on PyTorch-Ignite v0.4.10 instead of v0.4.9\n* The cache-based dataset now matches the transform information when read/write the cache\n* `monai.losses.ContrastiveLoss` now infers `batch_size` during `forward()`\n* Rearrange the spatial axes in `RandSmoothDeform` transforms following PyTorch's convention\n* Unified several environment flags into `monai.utils.misc.MONAIEnvVars`\n* Simplified `__str__` implementation of `MetaTensor` instead of relying on the `__repr__` implementation\n### Fixed\n* Improved error messages when both `monai` and `monai-weekly` are pip-installed\n* Inconsistent pseudo number sequences for different `num_workers` in `DataLoader`\n* Issue of repeated sequences for `monai.data.ShuffleBuffer`\n* Issue of not preserving the physical extent in `monai.transforms.Spacing`\n* Issue of using `inception_v3` as the backbone of `monai.networks.nets.TorchVisionFCModel`\n* Index device issue for `monai.transforms.Crop`\n* Efficiency issue when converting the array dtype and contiguous memory\n### Deprecated\n* `Addchannel` and `AsChannelFirst` transforms in favor of `EnsureChannelFirst`\n* `monai.apps.pathology.data` components in favor of the corresponding components from `monai.data`\n* `monai.apps.pathology.handlers` in favor of the corresponding components from `monai.handlers`\n### Removed\n* `Status` section in the pull request template in favor of the pull request draft mode\n* `monai.engines.BaseWorkflow`\n* `ndim` and `dimensions` arguments in favor of `spatial_dims`\n* `n_classes`, `num_classes` arguments in `AsDiscrete` in favor of `to_onehot`\n* `logit_thresh`, `threshold_values` arguments in `AsDiscrete` in favor of `threshold`\n* `torch.testing.assert_allclose` in favor of `tests.utils.assert_allclose`\n\n## [0.9.1] - 2022-07-22\n### Added\n* Support of `monai.data.MetaTensor` as core data structure across the modules\n* Support of `inverse` in array-based transforms\n* `monai.apps.TciaDataset` APIs for The Cancer Imaging Archive (TCIA) datasets, including a pydicom-backend reader\n* Initial release of components for MRI reconstruction in `monai.apps.reconstruction`, including various FFT utilities\n* New metrics and losses, including mean IoU and structural similarity index\n* `monai.utils.StrEnum` class to simplify Enum-based type annotations\n### Changed\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:22.06-py3` from `nvcr.io/nvidia/pytorch:22.04-py3`\n* Optionally depend on PyTorch-Ignite v0.4.9 instead of v0.4.8\n### Fixed\n* Fixed issue of not skipping post activations in `Convolution` when input arguments are None\n* Fixed issue of ignoring dropout arguments in `DynUNet`\n* Fixed issue of hard-coded non-linear function in ViT classification head\n* Fixed issue of in-memory config overriding with `monai.bundle.ConfigParser.update`\n* 2D SwinUNETR incompatible shapes\n* Fixed issue with `monai.bundle.verify_metadata` not raising exceptions\n* Fixed issue with `monai.transforms.GridPatch` returns inconsistent type location when padding\n* Wrong generalized Dice score metric when denominator is 0 but prediction is non-empty\n* Docker image build error due to NGC CLI upgrade\n* Optional default value when parsing id unavailable in a ConfigParser instance\n* Immutable data input for the patch-based WSI datasets\n### Deprecated\n* `*_transforms` and `*_meta_dict` fields in dictionary-based transforms in favor of MetaTensor\n* `meta_keys`, `meta_key_postfix`, `src_affine` arguments in various transforms, in favor of MetaTensor\n* `AsChannelFirst` and `AddChannel`, in favor of `EnsureChannelFirst` transform\n\n## [0.9.0] - 2022-06-08\n### Added\n* `monai.bundle` primary module with a `ConfigParser` and command-line interfaces for configuration-based workflows\n* Initial release of MONAI bundle specification\n* Initial release of volumetric image detection modules including bounding boxes handling, RetinaNet-based architectures\n* API preview `monai.data.MetaTensor`\n* Unified `monai.data.image_writer` to support flexible IO backends including an ITK writer\n* Various new network blocks and architectures including `SwinUNETR`\n* DeepEdit interactive training/validation workflow\n* NuClick interactive segmentation transforms\n* Patch-based readers and datasets for whole-slide imaging\n* New losses and metrics including `SurfaceDiceMetric`, `GeneralizedDiceFocalLoss`\n* New pre-processing transforms including `RandIntensityRemap`, `SpatialResample`\n* Multi-output and slice-based inference for `SlidingWindowInferer`\n* `NrrdReader` for NRRD file support\n* Torchscript utilities to save models with meta information\n* Gradient-based visualization module `SmoothGrad`\n* Automatic regular source code scanning for common vulnerabilities and coding errors\n\n### Changed\n* Simplified `TestTimeAugmentation` using de-collate and invertible transforms APIs\n* Refactoring `monai.apps.pathology` modules into `monai.handlers` and `monai.transforms`\n* Flexible activation and normalization layers for `TopologySearch` and `DiNTS`\n* Anisotropic first layers for 3D resnet\n* Flexible ordering of activation, normalization in `UNet`\n* Enhanced performance of connected-components analysis using Cupy\n* `INSTANCE_NVFUSER` for enhanced performance in 3D instance norm\n* Support of string representation of dtype in `convert_data_type`\n* Added new options `iteration_log`, `iteration_log` to the logging handlers\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:22.04-py3` from `nvcr.io/nvidia/pytorch:21.10-py3`\n* `collate_fn` generates more data-related debugging info with `dev_collate`\n\n### Fixed\n* Unified the spellings of \"meta data\", \"metadata\", \"meta-data\" to \"metadata\"\n* Various inaccurate error messages when input data are in invalid shapes\n* Issue of computing symmetric distances in `compute_average_surface_distance`\n* Unnecessary layer  `self.conv3` in `UnetResBlock`\n* Issue of torchscript compatibility for `ViT` and self-attention blocks\n* Issue of hidden layers in `UNETR`\n* `allow_smaller` in spatial cropping transforms\n* Antialiasing in `Resize`\n* Issue of bending energy loss value at different resolutions\n* `kwargs_read_csv` in `CSVDataset`\n* In-place modification in `Metric` reduction\n* `wrap_array` for `ensure_tuple`\n* Contribution guide for introducing new third-party dependencies\n\n### Removed\n* Deprecated `nifti_writer`, `png_writer` in favor of `monai.data.image_writer`\n* Support for PyTorch 1.6\n\n## [0.8.1] - 2022-02-16\n### Added\n* Support of `matshow3d` with given `channel_dim`\n* Support of spatial 2D for `ViTAutoEnc`\n* Support of `dataframe` object input in `CSVDataset`\n* Support of tensor backend for `Orientation`\n* Support of configurable delimiter for CSV writers\n* A base workflow API\n* `DataFunc` API for dataset-level preprocessing\n* `write_scalar` API for logging with additional `engine` parameter in `TensorBoardHandler`\n* Enhancements for NVTX Range transform logging\n* Enhancements for `set_determinism`\n* Performance enhancements in the cache-based datasets\n* Configurable metadata keys for `monai.data.DatasetSummary`\n* Flexible `kwargs` for `WSIReader`\n* Logging for the learning rate schedule handler\n* `GridPatchDataset` as subclass of `monai.data.IterableDataset`\n* `is_onehot` option in `KeepLargestConnectedComponent`\n* `channel_dim` in the image readers and support of stacking images with channels\n* Skipping workflow `run` if epoch length is 0\n* Enhanced `CacheDataset` to avoid duplicated cache items\n* `save_state` utility function\n\n### Changed\n* Optionally depend on PyTorch-Ignite v0.4.8 instead of v0.4.6\n* `monai.apps.mmars.load_from_mmar` defaults to the latest version\n\n### Fixed\n* Issue when caching large items with `pickle`\n* Issue of hard-coded activation functions in `ResBlock`\n* Issue of `create_file_name` assuming local disk file creation\n* Issue of `WSIReader` when the backend is `TiffFile`\n* Issue of `deprecated_args` when the function signature contains kwargs\n* Issue of `channel_wise` computations for the intensity-based transforms\n* Issue of inverting `OneOf`\n* Issue of removing temporary caching file for the persistent dataset\n* Error messages when reader backend is not available\n* Output type casting issue in `ScaleIntensityRangePercentiles`\n* Various docstring typos and broken URLs\n* `mode` in the evaluator engine\n* Ordering of `Orientation` and `Spacing` in `monai.apps.deepgrow.dataset`\n\n### Removed\n* Additional deep supervision modules in `DynUnet`\n* Deprecated `reduction` argument for `ContrastiveLoss`\n* Decollate warning in `Workflow`\n* Unique label exception in `ROCAUCMetric`\n* Logger configuration logic in the event handlers\n\n## [0.8.0] - 2021-11-25\n### Added\n* Overview of [new features in v0.8](docs/source/whatsnew_0_8.md)\n* Network modules for differentiable neural network topology search (DiNTS)\n* Multiple Instance Learning transforms and models for digital pathology WSI analysis\n* Vision transformers for self-supervised representation learning\n* Contrastive loss for self-supervised learning\n* Finalized major improvements of 200+ components in `monai.transforms` to support input and backend in PyTorch and NumPy\n* Initial registration module benchmarking with `GlobalMutualInformationLoss` as an example\n* `monai.transforms` documentation with visual examples and the utility functions\n* Event handler for `MLfLow` integration\n* Enhanced data visualization functions including `blend_images` and `matshow3d`\n* `RandGridDistortion` and `SmoothField` in `monai.transforms`\n* Support of randomized shuffle buffer in iterable datasets\n* Performance review and enhancements for data type casting\n* Cumulative averaging API with distributed environment support\n* Module utility functions including `require_pkg` and `pytorch_after`\n* Various usability enhancements such as `allow_smaller` when sampling ROI and `wrap_sequence` when casting object types\n* `tifffile` support in `WSIReader`\n* Regression tests for the fast training workflows\n* Various tutorials and demos including educational contents at [MONAI Bootcamp 2021](https://github.com/Project-MONAI/MONAIBootcamp2021)\n### Changed\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:21.10-py3` from `nvcr.io/nvidia/pytorch:21.08-py3`\n* Decoupled `TraceKeys` and `TraceableTransform` APIs from `InvertibleTransform`\n* Skipping affine-based resampling when `resample=False` in `NiftiSaver`\n* Deprecated `threshold_values: bool` and `num_classes: int` in `AsDiscrete`\n* Enhanced `apply_filter` for spatially 1D, 2D and 3D inputs with non-separable kernels\n* Logging with `logging` in downloading and model archives in `monai.apps`\n* API documentation site now defaults to `stable` instead of `latest`\n* `skip-magic-trailing-comma` in coding style enforcements\n* Pre-merge CI pipelines now include unit tests with Nvidia Ampere architecture\n### Removed\n* Support for PyTorch 1.5\n* The deprecated `DynUnetV1` and the related network blocks\n* GitHub self-hosted CI/CD pipelines for package releases\n### Fixed\n* Support of path-like objects as file path inputs in most modules\n* Issue of `decollate_batch` for dictionary of empty lists\n* Typos in documentation and code examples in various modules\n* Issue of no available keys when `allow_missing_keys=True` for the `MapTransform`\n* Issue of redundant computation when normalization factors are 0.0 and 1.0 in `ScaleIntensity`\n* Incorrect reports of registered readers in `ImageReader`\n* Wrong numbering of iterations in `StatsHandler`\n* Naming conflicts in network modules and aliases\n* Incorrect output shape when `reduction=\"none\"` in `FocalLoss`\n* Various usability issues reported by users\n\n## [0.7.0] - 2021-09-24\n### Added\n* Overview of [new features in v0.7](docs/source/whatsnew_0_7.md)\n* Initial phase of major usability improvements in `monai.transforms` to support input and backend in PyTorch and NumPy\n* Performance enhancements, with [profiling and tuning guides](https://github.com/Project-MONAI/tutorials/blob/master/acceleration/fast_model_training_guide.md) for typical use cases\n* Reproducing [training modules and workflows](https://github.com/Project-MONAI/tutorials/tree/master/kaggle/RANZCR/4th_place_solution) of state-of-the-art Kaggle competition solutions\n* 24 new transforms, including\n  * `OneOf` meta transform\n  * DeepEdit guidance signal transforms for interactive segmentation\n  * Transforms for self-supervised pre-training\n  * Integration of [NVIDIA Tools Extension](https://developer.nvidia.com/blog/nvidia-tools-extension-api-nvtx-annotation-tool-for-profiling-code-in-python-and-c-c/) (NVTX)\n  * Integration of [cuCIM](https://github.com/rapidsai/cucim)\n  * Stain normalization and contextual grid for digital pathology\n* `Transchex` network for vision-language transformers for chest X-ray analysis\n* `DatasetSummary` utility in `monai.data`\n* `WarmupCosineSchedule`\n* Deprecation warnings and documentation support for better backwards compatibility\n* Padding with additional `kwargs` and different backend API\n* Additional options such as `dropout` and `norm` in various networks and their submodules\n\n### Changed\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:21.08-py3` from `nvcr.io/nvidia/pytorch:21.06-py3`\n* Deprecated input argument `n_classes`, in favor of `num_classes`\n* Deprecated input argument `dimensions` and `ndims`, in favor of `spatial_dims`\n* Updated the Sphinx-based documentation theme for better readability\n* `NdarrayTensor` type is replaced by `NdarrayOrTensor` for simpler annotations\n* Self-attention-based network blocks now support both 2D and 3D inputs\n\n### Removed\n* The deprecated `TransformInverter`, in favor of `monai.transforms.InvertD`\n* GitHub self-hosted CI/CD pipelines for nightly and post-merge tests\n* `monai.handlers.utils.evenly_divisible_all_gather`\n* `monai.handlers.utils.string_list_all_gather`\n\n### Fixed\n* A Multi-thread cache writing issue in `LMDBDataset`\n* Output shape convention inconsistencies of the image readers\n* Output directory and file name flexibility issue for `NiftiSaver`, `PNGSaver`\n* Requirement of the `label` field in test-time augmentation\n* Input argument flexibility issues for  `ThreadDataLoader`\n* Decoupled `Dice` and `CrossEntropy` intermediate results in `DiceCELoss`\n* Improved documentation, code examples, and warning messages in various modules\n* Various usability issues reported by users\n\n## [0.6.0] - 2021-07-08\n### Added\n* 10 new transforms, a masked loss wrapper, and a `NetAdapter` for transfer learning\n* APIs to load networks and pre-trained weights from Clara Train [Medical Model ARchives (MMARs)](https://docs.nvidia.com/clara/clara-train-sdk/pt/mmar.html)\n* Base metric and cumulative metric APIs, 4 new regression metrics\n* Initial CSV dataset support\n* Decollating mini-batch as the default first postprocessing step, [Migrating your v0.5 code to v0.6](https://github.com/Project-MONAI/MONAI/wiki/v0.5-to-v0.6-migration-guide) wiki shows how to adapt to the breaking changes\n* Initial backward compatibility support via `monai.utils.deprecated`\n* Attention-based vision modules and `UNETR` for segmentation\n* Generic module loaders and Gaussian mixture models using the PyTorch JIT compilation\n* Inverse of image patch sampling transforms\n* Network block utilities `get_[norm, act, dropout, pool]_layer`\n* `unpack_items` mode for `apply_transform` and `Compose`\n* New event `INNER_ITERATION_STARTED` in the deepgrow interactive workflow\n* `set_data` API for cache-based datasets to dynamically update the dataset content\n* Fully compatible with PyTorch 1.9\n* `--disttests` and `--min` options for `runtests.sh`\n* Initial support of pre-merge tests with Nvidia Blossom system\n\n### Changed\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:21.06-py3` from\n  `nvcr.io/nvidia/pytorch:21.04-py3`\n* Optionally depend on PyTorch-Ignite v0.4.5 instead of v0.4.4\n* Unified the demo, tutorial, testing data to the project shared drive, and\n  [`Project-MONAI/MONAI-extra-test-data`](https://github.com/Project-MONAI/MONAI-extra-test-data)\n* Unified the terms: `post_transform` is renamed to `postprocessing`, `pre_transform` is renamed to `preprocessing`\n* Unified the postprocessing transforms and event handlers to accept the \"channel-first\" data format\n* `evenly_divisible_all_gather` and `string_list_all_gather` moved to `monai.utils.dist`\n\n### Removed\n* Support of 'batched' input for postprocessing transforms and event handlers\n* `TorchVisionFullyConvModel`\n* `set_visible_devices` utility function\n* `SegmentationSaver` and `TransformsInverter` handlers\n\n### Fixed\n* Issue of handling big-endian image headers\n* Multi-thread issue for non-random transforms in the cache-based datasets\n* Persistent dataset issue when multiple processes sharing a non-exist cache location\n* Typing issue with Numpy 1.21.0\n* Loading checkpoint with both `model` and `optmizier` using `CheckpointLoader` when `strict_shape=False`\n* `SplitChannel` has different behaviour depending on numpy/torch inputs\n* Transform pickling issue caused by the Lambda functions\n* Issue of filtering by name in `generate_param_groups`\n* Inconsistencies in the return value types of `class_activation_maps`\n* Various docstring typos\n* Various usability enhancements in `monai.transforms`\n\n## [0.5.3] - 2021-05-28\n### Changed\n* Project default branch renamed to `dev` from `master`\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:21.04-py3` from `nvcr.io/nvidia/pytorch:21.02-py3`\n* Enhanced type checks for the `iteration_metric` handler\n* Enhanced `PersistentDataset` to use `tempfile` during caching computation\n* Enhanced various info/error messages\n* Enhanced performance of `RandAffine`\n* Enhanced performance of `SmartCacheDataset`\n* Optionally requires `cucim` when the platform is `Linux`\n* Default `device` of `TestTimeAugmentation` changed to `cpu`\n\n### Fixed\n* Download utilities now provide better default parameters\n* Duplicated `key_transforms` in the patch-based transforms\n* A multi-GPU issue in `ClassificationSaver`\n* A default `meta_data` issue in `SpacingD`\n* Dataset caching issue with the persistent data loader workers\n* A memory issue in `permutohedral_cuda`\n* Dictionary key issue in `CopyItemsd`\n* `box_start` and `box_end` parameters for deepgrow `SpatialCropForegroundd`\n* Tissue mask array transpose issue in `MaskedInferenceWSIDataset`\n* Various type hint errors\n* Various docstring typos\n\n### Added\n* Support of `to_tensor` and `device` arguments for `TransformInverter`\n* Slicing options with SpatialCrop\n* Class name alias for the networks for backward compatibility\n* `k_divisible` option for CropForeground\n* `map_items` option for `Compose`\n* Warnings of `inf` and `nan` for surface distance computation\n* A `print_log` flag to the image savers\n* Basic testing pipelines for Python 3.9\n\n## [0.5.0] - 2021-04-09\n### Added\n* Overview document for [feature highlights in v0.5.0](https://github.com/Project-MONAI/MONAI/blob/master/docs/source/highlights.md)\n* Invertible spatial transforms\n  * `InvertibleTransform` base APIs\n  * Batch inverse and decollating APIs\n  * Inverse of `Compose`\n  * Batch inverse event handling\n  * Test-time augmentation as an application\n* Initial support of learning-based image registration:\n  * Bending energy, LNCC, and global mutual information loss\n  * Fully convolutional architectures\n  * Dense displacement field, dense velocity field computation\n  * Warping with high-order interpolation with C++/CUDA implementations\n* Deepgrow modules for interactive segmentation:\n  * Workflows with simulations of clicks\n  * Distance-based transforms for guidance signals\n* Digital pathology support:\n  * Efficient whole slide imaging IO and sampling with Nvidia cuCIM and SmartCache\n  * FROC measurements for lesion\n  * Probabilistic post-processing for lesion detection\n  * TorchVision classification model adaptor for fully convolutional analysis\n* 12 new transforms, grid patch dataset, `ThreadDataLoader`, EfficientNets B0-B7\n* 4 iteration events for the engine for finer control of workflows\n* New C++/CUDA extensions:\n  * Conditional random field\n  * Fast bilateral filtering using the permutohedral lattice\n* Metrics summary reporting and saving APIs\n* DiceCELoss, DiceFocalLoss, a multi-scale wrapper for segmentation loss computation\n* Data loading utilities\n  * `decollate_batch`\n  * `PadListDataCollate` with inverse support\n* Support of slicing syntax for `Dataset`\n* Initial Torchscript support for the loss modules\n* Learning rate finder\n* Allow for missing keys in the dictionary-based transforms\n* Support of checkpoint loading for transfer learning\n* Various summary and plotting utilities for Jupyter notebooks\n* Contributor Covenant Code of Conduct\n* Major CI/CD enhancements covering the tutorial repository\n* Fully compatible with PyTorch 1.8\n* Initial nightly CI/CD pipelines using Nvidia Blossom Infrastructure\n\n### Changed\n* Enhanced `list_data_collate` error handling\n* Unified iteration metric APIs\n* `densenet*` extensions are renamed to `DenseNet*`\n* `se_res*` network extensions are renamed to `SERes*`\n* Transform base APIs are rearranged into `compose`, `inverse`, and `transform`\n* `_do_transform` flag for the random augmentations is unified via `RandomizableTransform`\n* Decoupled post-processing steps, e.g. `softmax`, `to_onehot_y`, from the metrics computations\n* Moved the distributed samplers to `monai.data.samplers` from `monai.data.utils`\n* Engine's data loaders now accept generic iterables as input\n* Workflows now accept additional custom events and state properties\n* Various type hints according to Numpy 1.20\n* Refactored testing utility `runtests.sh` to have `--unittest` and `--net` (integration tests) options\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:21.02-py3` from `nvcr.io/nvidia/pytorch:20.10-py3`\n* Docker images are now built with self-hosted environments\n* Primary contact email updated to `monai.contact@gmail.com`\n* Now using GitHub Discussions as the primary communication forum\n\n### Removed\n* Compatibility tests for PyTorch 1.5.x\n* Format specific loaders, e.g. `LoadNifti`, `NiftiDataset`\n* Assert statements from non-test files\n* `from module import *` statements, addressed flake8 F403\n\n### Fixed\n* Uses American English spelling for code, as per PyTorch\n* Code coverage now takes multiprocessing runs into account\n* SmartCache with initial shuffling\n* `ConvertToMultiChannelBasedOnBratsClasses` now supports channel-first inputs\n* Checkpoint handler to save with non-root permissions\n* Fixed an issue for exiting the distributed unit tests\n* Unified `DynUNet` to have single tensor output w/o deep supervision\n* `SegmentationSaver` now supports user-specified data types and a `squeeze_end_dims` flag\n* Fixed `*Saver` event handlers output filenames with a `data_root_dir` option\n* Load image functions now ensure little-endian\n* Fixed the test runner to support regex-based test case matching\n* Usability issues in the event handlers\n\n## [0.4.0] - 2020-12-15\n### Added\n* Overview document for [feature highlights in v0.4.0](https://github.com/Project-MONAI/MONAI/blob/master/docs/source/highlights.md)\n* Torchscript support for the net modules\n* New networks and layers:\n  * Discrete Gaussian kernels\n  * Hilbert transform and envelope detection\n  * Swish and mish activation\n  * Acti-norm-dropout block\n  * Upsampling layer\n  * Autoencoder, Variational autoencoder\n  * FCNet\n* Support of initialisation from pretrained weights for densenet, senet, multichannel AHNet\n* Layer-wise learning rate API\n* New model metrics and event handlers based on occlusion sensitivity, confusion matrix, surface distance\n* CAM/GradCAM/GradCAM++\n* File format-agnostic image loader APIs with Nibabel, ITK readers\n* Enhancements for dataset partition, cross-validation APIs\n* New data APIs:\n  * LMDB-based caching dataset\n  * Cache-N-transforms dataset\n  * Iterable dataset\n  * Patch dataset\n* Weekly PyPI release\n* Fully compatible with PyTorch 1.7\n* CI/CD enhancements:\n  * Skipping, speed up, fail fast, timed, quick tests\n  * Distributed training tests\n  * Performance profiling utilities\n* New tutorials and demos:\n  * Autoencoder, VAE tutorial\n  * Cross-validation demo\n  * Model interpretability tutorial\n  * COVID-19 Lung CT segmentation challenge open-source baseline\n  * Threadbuffer demo\n  * Dataset partitioning tutorial\n  * Layer-wise learning rate demo\n  * [MONAI Bootcamp 2020](https://github.com/Project-MONAI/MONAIBootcamp2020)\n\n### Changed\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:20.10-py3` from `nvcr.io/nvidia/pytorch:20.08-py3`\n\n#### Backwards Incompatible Changes\n* `monai.apps.CVDecathlonDataset` is extended to a generic `monai.apps.CrossValidation` with an `dataset_cls` option\n* Cache dataset now requires a `monai.transforms.Compose` instance as the transform argument\n* Model checkpoint file name extensions changed from `.pth` to `.pt`\n* Readers' `get_spatial_shape` returns a numpy array instead of list\n* Decoupled postprocessing steps such as `sigmoid`, `to_onehot_y`, `mutually_exclusive`, `logit_thresh` from metrics and event handlers,\nthe postprocessing steps should be used before calling the metrics methods\n* `ConfusionMatrixMetric` and `DiceMetric` computation now returns an additional `not_nans` flag to indicate valid results\n* `UpSample` optional `mode` now supports `\"deconv\"`, `\"nontrainable\"`, `\"pixelshuffle\"`; `interp_mode` is only used when `mode` is `\"nontrainable\"`\n* `SegResNet` optional `upsample_mode` now supports `\"deconv\"`, `\"nontrainable\"`, `\"pixelshuffle\"`\n* `monai.transforms.Compose` class inherits `monai.transforms.Transform`\n* In `Rotate`, `Rotated`, `RandRotate`, `RandRotated`  transforms, the `angle` related parameters are interpreted as angles in radians instead of degrees.\n* `SplitChannel` and `SplitChanneld` moved from `transforms.post` to `transforms.utility`\n\n### Removed\n* Support of PyTorch 1.4\n\n### Fixed\n* Enhanced loss functions for stability and flexibility\n* Sliding window inference memory and device issues\n* Revised transforms:\n  * Normalize intensity datatype and normalizer types\n  * Padding modes for zoom\n  * Crop returns coordinates\n  * Select items transform\n  * Weighted patch sampling\n  * Option to keep aspect ratio for zoom\n* Various CI/CD issues\n\n## [0.3.0] - 2020-10-02\n### Added\n* Overview document for [feature highlights in v0.3.0](https://github.com/Project-MONAI/MONAI/blob/master/docs/source/highlights.md)\n* Automatic mixed precision support\n* Multi-node, multi-GPU data parallel model training support\n* 3 new evaluation metric functions\n* 11 new network layers and blocks\n* 6 new network architectures\n* 14 new transforms, including an I/O adaptor\n* Cross validation module for `DecathlonDataset`\n* Smart Cache module in dataset\n* `monai.optimizers` module\n* `monai.csrc` module\n* Experimental feature of ImageReader using ITK, Nibabel, Numpy, Pillow (PIL Fork)\n* Experimental feature of differentiable image resampling in C++/CUDA\n* Ensemble evaluator module\n* GAN trainer module\n* Initial cross-platform CI environment for C++/CUDA code\n* Code style enforcement now includes isort and clang-format\n* Progress bar with tqdm\n\n### Changed\n* Now fully compatible with PyTorch 1.6\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:20.08-py3` from `nvcr.io/nvidia/pytorch:20.03-py3`\n* Code contributions now require signing off on the [Developer Certificate of Origin (DCO)](https://developercertificate.org/)\n* Major work in type hinting finished\n* Remote datasets migrated to [Open Data on AWS](https://registry.opendata.aws/)\n* Optionally depend on PyTorch-Ignite v0.4.2 instead of v0.3.0\n* Optionally depend on torchvision, ITK\n* Enhanced CI tests with 8 new testing environments\n\n### Removed\n* `MONAI/examples` folder (relocated into [`Project-MONAI/tutorials`](https://github.com/Project-MONAI/tutorials))\n* `MONAI/research` folder (relocated to [`Project-MONAI/research-contributions`](https://github.com/Project-MONAI/research-contributions))\n\n### Fixed\n* `dense_patch_slices` incorrect indexing\n* Data type issue in `GeneralizedWassersteinDiceLoss`\n* `ZipDataset` return value inconsistencies\n* `sliding_window_inference` indexing and `device` issues\n* importing monai modules may cause namespace pollution\n* Random data splits issue in `DecathlonDataset`\n* Issue of randomising a `Compose` transform\n* Various issues in function type hints\n* Typos in docstring and documentation\n* `PersistentDataset` issue with existing file folder\n* Filename issue in the output writers\n\n## [0.2.0] - 2020-07-02\n### Added\n* Overview document for [feature highlights in v0.2.0](https://github.com/Project-MONAI/MONAI/blob/master/docs/source/highlights.md)\n* Type hints and static type analysis support\n* `MONAI/research` folder\n* `monai.engine.workflow` APIs for supervised training\n* `monai.inferers` APIs for validation and inference\n* 7 new tutorials and examples\n* 3 new loss functions\n* 4 new event handlers\n* 8 new layers, blocks, and networks\n* 12 new transforms, including post-processing transforms\n* `monai.apps.datasets` APIs, including `MedNISTDataset` and `DecathlonDataset`\n* Persistent caching, `ZipDataset`, and `ArrayDataset` in `monai.data`\n* Cross-platform CI tests supporting multiple Python versions\n* Optional import mechanism\n* Experimental features for third-party transforms integration\n\n### Changed\n> For more details please visit [the project wiki](https://github.com/Project-MONAI/MONAI/wiki/Notable-changes-between-0.1.0-and-0.2.0)\n* Core modules now require numpy >= 1.17\n* Categorized `monai.transforms` modules into crop and pad, intensity, IO, post-processing, spatial, and utility.\n* Most transforms are now implemented with PyTorch native APIs\n* Code style enforcement and automated formatting workflows now use autopep8 and black\n* Base Docker image upgraded to `nvcr.io/nvidia/pytorch:20.03-py3` from `nvcr.io/nvidia/pytorch:19.10-py3`\n* Enhanced local testing tools\n* Documentation website domain changed to https://docs.monai.io\n\n### Removed\n* Support of Python < 3.6\n* Automatic installation of optional dependencies including pytorch-ignite, nibabel, tensorboard, pillow, scipy, scikit-image\n\n### Fixed\n* Various issues in type and argument names consistency\n* Various issues in docstring and documentation site\n* Various issues in unit and integration tests\n* Various issues in examples and notebooks\n\n## [0.1.0] - 2020-04-17\n### Added\n* Public alpha source code release under the Apache 2.0 license ([highlights](https://github.com/Project-MONAI/MONAI/blob/0.1.0/docs/source/highlights.md))\n* Various tutorials and examples\n  - Medical image classification and segmentation workflows\n  - Spacing/orientation-aware preprocessing with CPU/GPU and caching\n  - Flexible workflows with PyTorch Ignite and Lightning\n* Various GitHub Actions\n  - CI/CD pipelines via self-hosted runners\n  - Documentation publishing via readthedocs.org\n  - PyPI package publishing\n* Contributing guidelines\n* A project logo and badges\n\n[highlights]: https://github.com/Project-MONAI/MONAI/blob/master/docs/source/highlights.md\n\n[Unreleased]: https://github.com/Project-MONAI/MONAI/compare/1.4.0...HEAD\n[1.4.0]: https://github.com/Project-MONAI/MONAI/compare/1.3.2...1.4.0\n[1.3.2]: https://github.com/Project-MONAI/MONAI/compare/1.3.1...1.3.2\n[1.3.1]: https://github.com/Project-MONAI/MONAI/compare/1.3.0...1.3.1\n[1.3.0]: https://github.com/Project-MONAI/MONAI/compare/1.2.0...1.3.0\n[1.2.0]: https://github.com/Project-MONAI/MONAI/compare/1.1.0...1.2.0\n[1.1.0]: https://github.com/Project-MONAI/MONAI/compare/1.0.1...1.1.0\n[1.0.1]: https://github.com/Project-MONAI/MONAI/compare/1.0.0...1.0.1\n[1.0.0]: https://github.com/Project-MONAI/MONAI/compare/0.9.1...1.0.0\n[0.9.1]: https://github.com/Project-MONAI/MONAI/compare/0.9.0...0.9.1\n[0.9.0]: https://github.com/Project-MONAI/MONAI/compare/0.8.1...0.9.0\n[0.8.1]: https://github.com/Project-MONAI/MONAI/compare/0.8.0...0.8.1\n[0.8.0]: https://github.com/Project-MONAI/MONAI/compare/0.7.0...0.8.0\n[0.7.0]: https://github.com/Project-MONAI/MONAI/compare/0.6.0...0.7.0\n[0.6.0]: https://github.com/Project-MONAI/MONAI/compare/0.5.3...0.6.0\n[0.5.3]: https://github.com/Project-MONAI/MONAI/compare/0.5.0...0.5.3\n[0.5.0]: https://github.com/Project-MONAI/MONAI/compare/0.4.0...0.5.0\n[0.4.0]: https://github.com/Project-MONAI/MONAI/compare/0.3.0...0.4.0\n[0.3.0]: https://github.com/Project-MONAI/MONAI/compare/0.2.0...0.3.0\n[0.2.0]: https://github.com/Project-MONAI/MONAI/compare/0.1.0...0.2.0\n[0.1.0]: https://github.com/Project-MONAI/MONAI/commits/0.1.0\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 3.7490234375,
          "content": "# YAML 1.2\n# Metadata for citation of this software according to the CFF format (https://citation-file-format.github.io/)\n#\n---\ntitle: \"MONAI: Medical Open Network for AI\"\nabstract: \"AI Toolkit for Healthcare Imaging\"\nauthors:\n  - name: \"MONAI Consortium\"\ndate-released: 2024-10-17\nversion: \"1.4.0\"\nidentifiers:\n  - description: \"This DOI represents all versions of MONAI, and will always resolve to the latest one.\"\n    type: doi\n    value: \"10.5281/zenodo.4323058\"\nlicense: \"Apache-2.0\"\nrepository-code: \"https://github.com/Project-MONAI/MONAI\"\nurl: \"https://monai.io\"\ncff-version: \"1.2.0\"\nmessage: \"If you use this software, please cite it using these metadata.\"\npreferred-citation:\n  type: article\n  authors:\n  - given-names: \"M. Jorge\"\n    family-names: \"Cardoso\"\n  - given-names: \"Wenqi\"\n    family-names: \"Li\"\n  - given-names: \"Richard\"\n    family-names: \"Brown\"\n  - given-names: \"Nic\"\n    family-names: \"Ma\"\n  - given-names: \"Eric\"\n    family-names: \"Kerfoot\"\n  - given-names: \"Yiheng\"\n    family-names: \"Wang\"\n  - given-names: \"Benjamin\"\n    family-names: \"Murray\"\n  - given-names: \"Andriy\"\n    family-names: \"Myronenko\"\n  - given-names: \"Can\"\n    family-names: \"Zhao\"\n  - given-names: \"Dong\"\n    family-names: \"Yang\"\n  - given-names: \"Vishwesh\"\n    family-names: \"Nath\"\n  - given-names: \"Yufan\"\n    family-names: \"He\"\n  - given-names: \"Ziyue\"\n    family-names: \"Xu\"\n  - given-names: \"Ali\"\n    family-names: \"Hatamizadeh\"\n  - given-names: \"Wentao\"\n    family-names: \"Zhu\"\n  - given-names: \"Yun\"\n    family-names: \"Liu\"\n  - given-names: \"Mingxin\"\n    family-names: \"Zheng\"\n  - given-names: \"Yucheng\"\n    family-names: \"Tang\"\n  - given-names: \"Isaac\"\n    family-names: \"Yang\"\n  - given-names: \"Michael\"\n    family-names: \"Zephyr\"\n  - given-names: \"Behrooz\"\n    family-names: \"Hashemian\"\n  - given-names: \"Sachidanand\"\n    family-names: \"Alle\"\n  - given-names: \"Mohammad\"\n    family-names: \"Zalbagi Darestani\"\n  - given-names: \"Charlie\"\n    family-names: \"Budd\"\n  - given-names: \"Marc\"\n    family-names: \"Modat\"\n  - given-names: \"Tom\"\n    family-names: \"Vercauteren\"\n  - given-names: \"Guotai\"\n    family-names: \"Wang\"\n  - given-names: \"Yiwen\"\n    family-names: \"Li\"\n  - given-names: \"Yipeng\"\n    family-names: \"Hu\"\n  - given-names: \"Yunguan\"\n    family-names: \"Fu\"\n  - given-names: \"Benjamin\"\n    family-names: \"Gorman\"\n  - given-names: \"Hans\"\n    family-names: \"Johnson\"\n  - given-names: \"Brad\"\n    family-names: \"Genereaux\"\n  - given-names: \"Barbaros S.\"\n    family-names: \"Erdal\"\n  - given-names: \"Vikash\"\n    family-names: \"Gupta\"\n  - given-names: \"Andres\"\n    family-names: \"Diaz-Pinto\"\n  - given-names: \"Andre\"\n    family-names: \"Dourson\"\n  - given-names: \"Lena\"\n    family-names: \"Maier-Hein\"\n  - given-names: \"Paul F.\"\n    family-names: \"Jaeger\"\n  - given-names: \"Michael\"\n    family-names: \"Baumgartner\"\n  - given-names: \"Jayashree\"\n    family-names: \"Kalpathy-Cramer\"\n  - given-names: \"Mona\"\n    family-names: \"Flores\"\n  - given-names: \"Justin\"\n    family-names: \"Kirby\"\n  - given-names: \"Lee A.D.\"\n    family-names: \"Cooper\"\n  - given-names: \"Holger R.\"\n    family-names: \"Roth\"\n  - given-names: \"Daguang\"\n    family-names: \"Xu\"\n  - given-names: \"David\"\n    family-names: \"Bericat\"\n  - given-names: \"Ralf\"\n    family-names: \"Floca\"\n  - given-names: \"S. Kevin\"\n    family-names: \"Zhou\"\n  - given-names: \"Haris\"\n    family-names: \"Shuaib\"\n  - given-names: \"Keyvan\"\n    family-names: \"Farahani\"\n  - given-names: \"Klaus H.\"\n    family-names: \"Maier-Hein\"\n  - given-names: \"Stephen\"\n    family-names: \"Aylward\"\n  - given-names: \"Prerna\"\n    family-names: \"Dogra\"\n  - given-names: \"Sebastien\"\n    family-names: \"Ourselin\"\n  - given-names: \"Andrew\"\n    family-names: \"Feng\"\n  doi: \"https://doi.org/10.48550/arXiv.2211.02701\"\n  month: 11\n  year: 2022\n  title: \"MONAI: An open-source framework for deep learning in healthcare\"\n...\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.2763671875,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at monai.contact@gmail.com. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 23.2578125,
          "content": "- [Introduction](#introduction)\n- [The contribution process](#the-contribution-process)\n  * [Preparing pull requests](#preparing-pull-requests)\n    1. [Checking the coding style](#checking-the-coding-style)\n    1. [Unit testing](#unit-testing)\n    1. [Building the documentation](#building-the-documentation)\n    1. [Automatic code formatting](#automatic-code-formatting)\n    1. [Adding new optional dependencies](#adding-new-optional-dependencies)\n    1. [Signing your work](#signing-your-work)\n    1. [Utility functions](#utility-functions)\n    1. [Backwards compatibility](#backwards-compatibility)\n  * [Submitting pull requests](#submitting-pull-requests)\n- [The code reviewing process (for the maintainers)](#the-code-reviewing-process)\n  * [Reviewing pull requests](#reviewing-pull-requests)\n- [Admin tasks (for the maintainers)](#admin-tasks)\n  * [Releasing a new version](#release-a-new-version)\n\n## Introduction\n\n\nWelcome to Project MONAI! We're excited you're here and want to contribute. This documentation is intended for individuals and institutions interested in contributing to MONAI. MONAI is an open-source project and, as such, its success relies on its community of contributors willing to keep improving it. Your contribution will be a valued addition to the code base; we simply ask that you read this page and understand our contribution process, whether you are a seasoned open-source contributor or whether you are a first-time contributor.\n\n### Communicate with us\n\nWe are happy to talk with you about your needs for MONAI and your ideas for contributing to the project. One way to do this is to create an issue discussing your thoughts. It might be that a very similar feature is under development or already exists, so an issue is a great starting point. If you are looking for an issue to resolve that will help Project MONAI, see the [*good first issue*](https://github.com/Project-MONAI/MONAI/labels/good%20first%20issue) and [*Contribution wanted*](https://github.com/Project-MONAI/MONAI/labels/Contribution%20wanted) labels.\n\n### Does it belong in PyTorch instead of MONAI?\n\nMONAI is part of [PyTorch Ecosystem](https://pytorch.org/ecosystem/), and mainly based on the PyTorch and Numpy libraries. These libraries implement what we consider to be best practice for general scientific computing and deep learning functionality. MONAI builds on these with a strong focus on medical applications. As such, it is a good idea to consider whether your functionality is medical-application specific or not. General deep learning functionality may be better off in PyTorch; you can find their contribution guidelines [here](https://pytorch.org/docs/stable/community/contribution_guide.html).\n\n## The contribution process\n\n_Pull request early_\n\nWe encourage you to create pull requests early. It helps us track the contributions under development, whether they are ready to be merged or not. [Create a draft pull request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/changing-the-stage-of-a-pull-request) until it is ready for formal review.\n\nPlease note that, as per PyTorch, MONAI uses American English spelling. This means classes and variables should be: normali**z**e, visuali**z**e, colo~~u~~r, etc.\n\n### Preparing pull requests\nTo ensure the code quality, MONAI relies on several linting tools ([flake8 and its plugins](https://gitlab.com/pycqa/flake8), [black](https://github.com/psf/black), [isort](https://github.com/timothycrosley/isort), [ruff](https://github.com/astral-sh/ruff)),\nstatic type analysis tools ([mypy](https://github.com/python/mypy), [pytype](https://github.com/google/pytype)), as well as a set of unit/integration tests.\n\nThis section highlights all the necessary preparation steps required before sending a pull request.\nTo collaborate efficiently, please read through this section and follow them.\n\n* [Checking the coding style](#checking-the-coding-style)\n* [Licensing information](#licensing-information)\n* [Unit testing](#unit-testing)\n* [Building documentation](#building-the-documentation)\n* [Signing your work](#signing-your-work)\n\n#### Checking the coding style\nCoding style is checked and enforced by flake8, black, isort, and ruff, using [a flake8 configuration](./setup.cfg) similar to [PyTorch's](https://github.com/pytorch/pytorch/blob/master/.flake8).\nBefore submitting a pull request, we recommend that all linting should pass, by running the following command locally:\n\n```bash\n# optionally update the dependencies and dev tools\npython -m pip install -U pip\npython -m pip install -U -r requirements-dev.txt\n\n# run the linting and type checking tools\n./runtests.sh --codeformat\n\n# try to fix the coding style errors automatically\n./runtests.sh --autofix\n```\n\nFull linting and type checking may take some time. If you need a quick check, run\n```bash\n# run ruff only\n./runtests.sh --ruff\n```\n\n#### Licensing information\nAll source code files should start with this paragraph:\n\n```\n# Copyright (c) MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n```\n\n##### Exporting modules\n\nIf you intend for any variables/functions/classes to be available outside of the file with the edited functionality, then:\n\n- Create or append to the `__all__` variable (in the file in which functionality has been added), and\n- Add to the `__init__.py` file.\n\n#### Unit testing\nMONAI tests are located under `tests/`.\n\n- The unit test's file name currently follows `test_[module_name].py` or `test_[module_name]_dist.py`.\n- The `test_[module_name]_dist.py` subset of unit tests requires a distributed environment to verify the module with distributed GPU-based computation.\n- The integration test's file name follows `test_integration_[workflow_name].py`.\n\nA bash script (`runtests.sh`) is provided to run all tests locally.\nPlease run ``./runtests.sh -h`` to see all options.\n\nTo run a particular test, for example `tests/test_dice_loss.py`:\n```\npython -m tests.test_dice_loss\n```\n\nBefore submitting a pull request, we recommend that all linting and unit tests\nshould pass, by running the following command locally:\n\n```bash\n./runtests.sh -f -u --net --coverage\n```\nor (for new features that would not break existing functionality):\n\n```bash\n./runtests.sh --quick --unittests\n```\n\nIt is recommended that the new test `test_[module_name].py` is constructed by using only\npython 3.9+ build-in functions, `torch`, `numpy`, `coverage` (for reporting code coverages) and `parameterized` (for organising test cases) packages.\nIf it requires any other external packages, please make sure:\n- the packages are listed in [`requirements-dev.txt`](requirements-dev.txt)\n- the new test `test_[module_name].py` is added to the `exclude_cases` in [`./tests/min_tests.py`](./tests/min_tests.py) so that\nthe minimal CI runner will not execute it.\n\n##### Testing data\nTesting data such as images and binary files should not be placed in the source code repository.\nPlease deploy them to a reliable file sharing location (the current preferred one is [https://github.com/Project-MONAI/MONAI-extra-test-data/releases](https://github.com/Project-MONAI/MONAI-extra-test-data/releases)).\nAt test time, the URLs within `tests/testing_data/data_config.json` are accessible\nvia the APIs provided in `tests.utils`: `tests.utils.testing_data_config` and `tests.utils.download_url_or_skip_test`.\n\n_If it's not tested, it's broken_\n\nAll new functionality should be accompanied by an appropriate set of tests.\nMONAI functionality has plenty of unit tests from which you can draw inspiration,\nand you can reach out to us if you are unsure of how to proceed with testing.\n\nMONAI's code coverage report is available at [CodeCov](https://codecov.io/gh/Project-MONAI/MONAI).\n\n#### Building the documentation\nMONAI's documentation is located at `docs/`.\n\n```bash\n# install the doc-related dependencies\npip install --upgrade pip\npip install -r docs/requirements.txt\n\n# build the docs\ncd docs/\nmake html\n```\nThe above commands build html documentation, they are used to automatically generate [https://docs.monai.io](https://docs.monai.io).\n\nThe Python code docstring are written in\n[reStructuredText](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html) and\nthe documentation pages can be in either [reStructuredText](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html) or [Markdown](https://en.wikipedia.org/wiki/Markdown).  In general the Python docstrings follow the [Google style](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings).\n\nBefore submitting a pull request, it is recommended to:\n- edit the relevant `.rst` files in [`docs/source`](./docs/source) accordingly.\n- build html documentation locally\n- check the auto-generated documentation (by browsing `./docs/build/html/index.html` with a web browser)\n- type `make clean` in `docs/` folder to remove the current build files.\n\nPlease type `make help` in `docs/` folder for all supported format options.\n\n#### Automatic code formatting\nMONAI provides support of automatic Python code formatting via [a customised GitHub action](https://github.com/Project-MONAI/monai-code-formatter).\nThis makes the project's Python coding style consistent and reduces maintenance burdens.\nCommenting a pull request with `/black` triggers the formatting action based on [`psf/Black`](https://github.com/psf/black) (this is implemented with [`slash command dispatch`](https://github.com/marketplace/actions/slash-command-dispatch)).\n\nSteps for the formatting process:\n- After submitting a pull request or push to an existing pull request,\nmake a comment to the pull request to trigger the formatting action.\nThe first line of the comment must be `/black` so that it will be interpreted by [the comment parser](https://github.com/marketplace/actions/slash-command-dispatch#how-are-comments-parsed-for-slash-commands).\n- [Auto] The GitHub action tries to format all Python files (using [`psf/Black`](https://github.com/psf/black)) in the branch and makes a commit under the name \"MONAI bot\" if there's code change. The actual formatting action is deployed at [project-monai/monai-code-formatter](https://github.com/Project-MONAI/monai-code-formatter).\n- [Auto] After the formatting commit, the GitHub action adds an emoji to the comment that triggered the process.\n- Repeat the above steps if necessary.\n\n#### Adding new optional dependencies\nIn addition to the minimal requirements of PyTorch and Numpy, MONAI's core modules are built optionally based on 3rd-party packages.\nThe current set of dependencies is listed in [installing dependencies](https://docs.monai.io/en/stable/installation.html#installing-the-recommended-dependencies).\n\nTo allow for flexible integration of MONAI with other systems and environments,\nthe optional dependency APIs are always invoked lazily. For example,\n```py\nfrom monai.utils import optional_import\nitk, _ = optional_import(\"itk\", ...)\n\nclass ITKReader(ImageReader):\n    ...\n    def read(self, ...):\n        return itk.imread(...)\n```\nThe availability of the external `itk.imread` API is not required unless `monai.data.ITKReader.read` is called by the user.\nIntegration tests with minimal requirements are deployed to ensure this strategy.\n\nTo add new optional dependencies, please communicate with the core team during pull request reviews,\nand add the necessary information (at least) to the following files:\n- [setup.cfg](https://github.com/Project-MONAI/MONAI/blob/dev/setup.cfg)  (for package's `[options.extras_require]` config)\n- [requirements-dev.txt](https://github.com/Project-MONAI/MONAI/blob/dev/requirements-dev.txt) (pip requirements file)\n- [docs/requirements.txt](https://github.com/Project-MONAI/MONAI/blob/dev/docs/requirements.txt) (docs pip requirements file)\n- [environment-dev.yml](https://github.com/Project-MONAI/MONAI/blob/dev/environment-dev.yml) (conda environment file)\n- [installation.md](https://github.com/Project-MONAI/MONAI/blob/dev/docs/source/installation.md) (documentation)\n\nWhen writing unit tests that use 3rd-party packages, it is a good practice to always consider\nan appropriate fallback default behaviour when the packages are not installed in\nthe testing environment. For example:\n```py\nfrom monai.utils import optional_import\nplt, has_matplotlib = optional_import(\"matplotlib.pyplot\")\n\n@skipUnless(has_matplotlib, \"Matplotlib required\")\nclass TestBlendImages(unittest.TestCase):\n```\nIt skips the test cases when `matplotlib.pyplot` APIs are not available.\n\nAlternatively, add the test file name to the ``exclude_cases`` in `tests/min_tests.py` to completely skip the test\ncases when running in a minimal setup.\n\n\n\n#### Signing your work\nMONAI enforces the [Developer Certificate of Origin](https://developercertificate.org/) (DCO) on all pull requests.\nAll commit messages should contain the `Signed-off-by` line with an email address. The [GitHub DCO app](https://github.com/apps/dco) is deployed on MONAI. The pull request's status will be `failed` if commits do not contain a valid `Signed-off-by` line.\n\nGit has a `-s` (or `--signoff`) command-line option to append this automatically to your commit message:\n```bash\ngit commit -s -m 'a new commit'\n```\nThe commit message will be:\n```\n    a new commit\n\n    Signed-off-by: Your Name <yourname@example.org>\n```\n\nFull text of the DCO:\n```\nDeveloper Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this\nlicense document, but changing it is not allowed.\n\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I\n    have the right to submit it under the open source license\n    indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best\n    of my knowledge, is covered under an appropriate open source\n    license and I have the right under that license to submit that\n    work with modifications, whether created in whole or in part\n    by me, under the same open source license (unless I am\n    permitted to submit under a different license), as indicated\n    in the file; or\n\n(c) The contribution was provided directly to me by some other\n    person who certified (a), (b) or (c) and I have not modified\n    it.\n\n(d) I understand and agree that this project and the contribution\n    are public and that a record of the contribution (including all\n    personal information I submit with it, including my sign-off) is\n    maintained indefinitely and may be redistributed consistent with\n    this project or the open source license(s) involved.\n```\n\n#### Utility functions\nMONAI provides a set of generic utility functions and frequently used routines.\nThese are located in [``monai/utils``](./monai/utils/) and in the module folders such as [``networks/utils.py``](./monai/networks/).\nUsers are encouraged to use these common routines to improve code readability and reduce the code maintenance burdens.\n\nNotably,\n- ``monai.module.export`` decorator can make the module name shorter when importing,\nfor example, ``import monai.transforms.Spacing`` is the equivalent of ``monai.transforms.spatial.array.Spacing`` if\n``class Spacing`` defined in file `monai/transforms/spatial/array.py` is decorated with ``@export(\"monai.transforms\")``.\n\nFor string definition, [f-string](https://www.python.org/dev/peps/pep-0498/) is recommended to use over `%-print` and `format-print`. So please try to use `f-string` if you need to define any string object.\n\n#### Backwards compatibility\nMONAI in general follows [PyTorch's policy for backward compatibility](https://github.com/pytorch/pytorch/wiki/PyTorch's-Python-Frontend-Backward-and-Forward-Compatibility-Policy).\nUtility functions are provided in `monai.utils.deprecated` to help migrate from the deprecated to new APIs. The use of these utilities is encouraged.\nThe pull request [template contains checkboxes](https://github.com/Project-MONAI/MONAI/blame/dev/.github/pull_request_template.md#L11-L12) that\nthe contributor should use accordingly to clearly indicate breaking changes.\n\nThe process of releasing backwards incompatible API changes is as follows:\n1. discuss the breaking changes during pull requests or in dev meetings with a feature proposal if needed.\n1. add a warning message in the upcoming release (version `X.Y`), the warning message should include a forecast of removing the deprecated API in:\n   1. `X+1.0` -- major version `X+1` and minor version `0` the next major version if it's a significant change,\n   1. `X.Y+2` -- major version `X` and minor version `Y+2` (the minor version after the next one), if it's a minor API change.\n   1. Note that the versioning policy is similar to PyTorch's approach which does not precisely follow [the semantic versioning](https://semver.org/) definition.\n      Major version numbers are instead used to represent major product version (which is currently not planned to be greater than 1),\n      minor version for both compatible and incompatible, and patch version for bug fixes.\n   1. when recommending new API to use in place of a deprecated API, the recommended version should\n      provide exact feature-like behaviour otherwise users will have a harder time migrating.\n1. add new test cases by extending the existing unit tests to cover both the deprecated and updated APIs.\n1. collect feedback from the users during the subsequent few releases, and reconsider step 1 if needed.\n1. before each release, review the deprecating APIs and relevant tests, and clean up the removed APIs described in step 2.\n\n\n\n### Submitting pull requests\nAll code changes to the dev branch must be done via [pull requests](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/proposing-changes-to-your-work-with-pull-requests).\n1. Create a new ticket or take a known ticket from [the issue list][monai issue list].\n1. Check if there's already a branch dedicated to the task.\n1. If the task has not been taken, [create a new branch in your fork](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request-from-a-fork)\nof the codebase named `[ticket_id]-[task_name]`.\nFor example, branch name `19-ci-pipeline-setup` corresponds to [issue #19](https://github.com/Project-MONAI/MONAI/issues/19).\nIdeally, the new branch should be based on the latest `dev` branch.\n1. Make changes to the branch ([use detailed commit messages if possible](https://chris.beams.io/posts/git-commit/)).\n1. Make sure that new tests cover the changes and the changed codebase [passes all tests locally](#unit-testing).\n1. [Create a new pull request](https://help.github.com/en/desktop/contributing-to-projects/creating-a-pull-request) from the task branch to the dev branch, with detailed descriptions of the purpose of this pull request.\n1. Check [the CI/CD status of the pull request][github ci], make sure all CI/CD tests passed.\n1. Wait for reviews; if there are reviews, make point-to-point responses, make further code changes if needed.\n1. If there are conflicts between the pull request branch and the dev branch, pull the changes from the dev and resolve the conflicts locally.\n1. Reviewer and contributor may have discussions back and forth until all comments addressed.\n1. Wait for the pull request to be merged.\n\n## The code reviewing process\n\n\n### Reviewing pull requests\nAll code review comments should be specific, constructive, and actionable.\n1. Check [the CI/CD status of the pull request][github ci], make sure all CI/CD tests passed before reviewing (contact the branch owner if needed).\n1. Read carefully the descriptions of the pull request and the files changed, write comments if needed.\n1. Make in-line comments to specific code segments, [request for changes](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-request-reviews) if needed.\n1. Review any further code changes until all comments addressed by the contributors.\n1. Comment to trigger `/black` and/or `/integration-test` for optional auto code formatting and [integration tests](.github/workflows/integration.yml).\n1. [Maintainers] Review the changes and comment `/build` to trigger internal full tests.\n1. Merge the pull request to the dev branch.\n1. Close the corresponding task ticket on [the issue list][monai issue list].\n\n[github ci]: https://github.com/Project-MONAI/MONAI/actions\n[monai issue list]: https://github.com/Project-MONAI/MONAI/issues\n\n\n## Admin tasks\n\n### Release a new version\nThe `dev` branch's `HEAD` always corresponds to MONAI docker image's latest tag: `projectmonai/monai:latest`.\nThe `main` branch's `HEAD` always corresponds to the latest MONAI milestone release.\n\nWhen major features are ready for a milestone, to prepare for a new release:\n- Prepare [a release note](https://github.com/Project-MONAI/MONAI/releases) and release checklist.\n- Check out or cherry-pick a new branch `releasing/[version number]` locally from the `dev` branch and push to the codebase.\n- Create a release candidate tag, for example, `git tag -a 0.1.0rc1 -m \"release candidate 1 of version 0.1.0\"`.\n- Push the tag to the codebase, for example, `git push origin 0.1.0rc1`.\n  This step will trigger package building and testing.\n  The resultant packages are automatically uploaded to\n  [TestPyPI](https://test.pypi.org/project/monai/).  The packages are also available for downloading as\n  repository's artifacts (e.g. the file at https://github.com/Project-MONAI/MONAI/actions/runs/66570977).\n- Check the release test at [TestPyPI](https://test.pypi.org/project/monai/), download the artifacts when the CI finishes.\n- Optionally run [the cron testing jobs](https://github.com/Project-MONAI/MONAI/blob/dev/.github/workflows/cron.yml) on `releasing/[version number]`.\n- Rebase `releasing/[version number]` to `main`, make sure all the test pipelines succeed.\n- Once the release candidate is verified, tag and push a milestone, for example, `git push origin 0.1.0`.\n  The tag must be with the latest commit of `releasing/[version number]`.\n- Upload the packages to [PyPI](https://pypi.org/project/monai/).\n  This could be done manually by ``twine upload dist/*``, given the artifacts are unzipped to the folder ``dist/``.\n- Merge `releasing/[version number]` to `dev`, this step must make sure that the tagging commit unchanged on `dev`.\n- Publish the release note.\n\nNote that the release should be tagged with a [PEP440](https://www.python.org/dev/peps/pep-0440/) compliant version number.\n\nIf any error occurs during the release process, first check out a new hotfix branch from the `releasing/[version number]`,\nthen make PRs to the `releasing/[version number]` to fix the bugs via the regular contribution procedure.\n\nIf any error occurs after the release process, first check out a new hotfix branch from the `main` branch,\nmake a patch version release following the semantic versioning, for example, `releasing/0.1.1`.\nMake sure the `releasing/0.1.1` is merged back into both `dev` and `main` and all the test pipelines succeed.\n\n\n<p align=\"right\">\n  <a href=\"#introduction\"> Back to Top</a>\n</p>\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 2.8779296875,
          "content": "# Copyright (c) MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# To build with a different base image\n# please run `docker build` using the `--build-arg PYTORCH_IMAGE=...` flag.\nARG PYTORCH_IMAGE=nvcr.io/nvidia/pytorch:24.10-py3\nFROM ${PYTORCH_IMAGE}\n\nLABEL maintainer=\"monai.contact@gmail.com\"\n\n# TODO: remark for issue [revise the dockerfile](https://github.com/zarr-developers/numcodecs/issues/431)\nRUN if [[ $(uname -m) =~ \"aarch64\" ]]; then \\\n      export CFLAGS=\"-O3\" && \\\n      export DISABLE_NUMCODECS_SSE2=true && \\\n      export DISABLE_NUMCODECS_AVX2=true && \\\n      pip install numcodecs; \\\n    fi\n\nWORKDIR /opt/monai\n\n# install full deps\nCOPY requirements.txt requirements-min.txt requirements-dev.txt /tmp/\nRUN cp /tmp/requirements.txt /tmp/req.bak \\\n  && awk '!/torch/' /tmp/requirements.txt > /tmp/tmp && mv /tmp/tmp /tmp/requirements.txt \\\n  && python -m pip install --upgrade --no-cache-dir pip \\\n  && python -m pip install --no-cache-dir -r /tmp/requirements-dev.txt\n\n# compile ext and remove temp files\n# TODO: remark for issue [revise the dockerfile #1276](https://github.com/Project-MONAI/MONAI/issues/1276)\n# please specify exact files and folders to be copied -- else, basically always, the Docker build process cannot cache\n# this or anything below it and always will build from at most here; one file change leads to no caching from here on...\n\nCOPY LICENSE CHANGELOG.md CODE_OF_CONDUCT.md CONTRIBUTING.md README.md versioneer.py setup.py setup.cfg runtests.sh MANIFEST.in ./\nCOPY tests ./tests\nCOPY monai ./monai\n\n# TODO: remove this line and torch.patch for 24.11\nRUN patch -R -d /usr/local/lib/python3.10/dist-packages/torch/onnx/ < ./monai/torch.patch\n\nRUN BUILD_MONAI=1 FORCE_CUDA=1 python setup.py develop \\\n  && rm -rf build __pycache__\n\n# NGC Client\nWORKDIR /opt/tools\nARG NGC_CLI_URI=\"https://ngc.nvidia.com/downloads/ngccli_linux.zip\"\nRUN wget -q ${NGC_CLI_URI} && unzip ngccli_linux.zip && chmod u+x ngc-cli/ngc && \\\n    find ngc-cli/ -type f -exec md5sum {} + | LC_ALL=C sort | md5sum -c ngc-cli.md5 && \\\n    rm -rf ngccli_linux.zip ngc-cli.md5\nENV PATH=${PATH}:/opt/tools:/opt/tools/ngc-cli\nRUN apt-get update \\\n  && DEBIAN_FRONTEND=\"noninteractive\" apt-get install -y libopenslide0  \\\n  && rm -rf /var/lib/apt/lists/*\n# append /opt/tools to runtime path for NGC CLI to be accessible from all file system locations\nENV PATH=${PATH}:/opt/tools\nENV POLYGRAPHY_AUTOINSTALL_DEPS=1\n\n\nWORKDIR /opt/monai\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0810546875,
          "content": "include versioneer.py\ninclude monai/_version.py\n\ninclude README.md\ninclude LICENSE\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.912109375,
          "content": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/Project-MONAI/MONAI/dev/docs/images/MONAI-logo-color.png\" width=\"50%\" alt='project-monai'>\n</p>\n\n**M**edical **O**pen **N**etwork for **AI**\n\n![Supported Python versions](https://raw.githubusercontent.com/Project-MONAI/MONAI/dev/docs/images/python.svg)\n[![License](https://img.shields.io/badge/license-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)\n[![PyPI version](https://badge.fury.io/py/monai.svg)](https://badge.fury.io/py/monai)\n[![docker](https://img.shields.io/badge/docker-pull-green.svg?logo=docker&logoColor=white)](https://hub.docker.com/r/projectmonai/monai)\n[![conda](https://img.shields.io/conda/vn/conda-forge/monai?color=green)](https://anaconda.org/conda-forge/monai)\n\n[![premerge](https://github.com/Project-MONAI/MONAI/actions/workflows/pythonapp.yml/badge.svg?branch=dev)](https://github.com/Project-MONAI/MONAI/actions/workflows/pythonapp.yml)\n[![postmerge](https://img.shields.io/github/checks-status/project-monai/monai/dev?label=postmerge)](https://github.com/Project-MONAI/MONAI/actions?query=branch%3Adev)\n[![Documentation Status](https://readthedocs.org/projects/monai/badge/?version=latest)](https://docs.monai.io/en/latest/)\n[![codecov](https://codecov.io/gh/Project-MONAI/MONAI/branch/dev/graph/badge.svg?token=6FTC7U1JJ4)](https://codecov.io/gh/Project-MONAI/MONAI)\n[![monai Downloads Last Month](https://assets.piptrends.com/get-last-month-downloads-badge/monai.svg 'monai Downloads Last Month by pip Trends')](https://piptrends.com/package/monai)\n\nMONAI is a [PyTorch](https://pytorch.org/)-based, [open-source](https://github.com/Project-MONAI/MONAI/blob/dev/LICENSE) framework for deep learning in healthcare imaging, part of the [PyTorch Ecosystem](https://pytorch.org/ecosystem/).\nIts ambitions are as follows:\n- Developing a community of academic, industrial and clinical researchers collaborating on a common foundation;\n- Creating state-of-the-art, end-to-end training workflows for healthcare imaging;\n- Providing researchers with the optimized and standardized way to create and evaluate deep learning models.\n\n\n## Features\n> _Please see [the technical highlights](https://docs.monai.io/en/latest/highlights.html) and [What's New](https://docs.monai.io/en/latest/whatsnew.html) of the milestone releases._\n\n- flexible pre-processing for multi-dimensional medical imaging data;\n- compositional & portable APIs for ease of integration in existing workflows;\n- domain-specific implementations for networks, losses, evaluation metrics and more;\n- customizable design for varying user expertise;\n- multi-GPU multi-node data parallelism support.\n\n\n## Installation\n\nTo install [the current release](https://pypi.org/project/monai/), you can simply run:\n\n```bash\npip install monai\n```\n\nPlease refer to [the installation guide](https://docs.monai.io/en/latest/installation.html) for other installation options.\n\n## Getting Started\n\n[MedNIST demo](https://colab.research.google.com/drive/1wy8XUSnNWlhDNazFdvGBHLfdkGvOHBKe) and [MONAI for PyTorch Users](https://colab.research.google.com/drive/1boqy7ENpKrqaJoxFlbHIBnIODAs1Ih1T) are available on Colab.\n\nExamples and notebook tutorials are located at [Project-MONAI/tutorials](https://github.com/Project-MONAI/tutorials).\n\nTechnical documentation is available at [docs.monai.io](https://docs.monai.io).\n\n## Citation\n\nIf you have used MONAI in your research, please cite us! The citation can be exported from: https://arxiv.org/abs/2211.02701.\n\n## Model Zoo\n[The MONAI Model Zoo](https://github.com/Project-MONAI/model-zoo) is a place for researchers and data scientists to share the latest and great models from the community.\nUtilizing [the MONAI Bundle format](https://docs.monai.io/en/latest/bundle_intro.html) makes it easy to [get started](https://github.com/Project-MONAI/tutorials/tree/main/model_zoo) building workflows with MONAI.\n\n## Contributing\nFor guidance on making a contribution to MONAI, see the [contributing guidelines](https://github.com/Project-MONAI/MONAI/blob/dev/CONTRIBUTING.md).\n\n## Community\nJoin the conversation on Twitter/X [@ProjectMONAI](https://twitter.com/ProjectMONAI) or join our [Slack channel](https://forms.gle/QTxJq3hFictp31UM9).\n\nAsk and answer questions over on [MONAI's GitHub Discussions tab](https://github.com/Project-MONAI/MONAI/discussions).\n\n## Links\n- Website: https://monai.io/\n- API documentation (milestone): https://docs.monai.io/\n- API documentation (latest dev): https://docs.monai.io/en/latest/\n- Code: https://github.com/Project-MONAI/MONAI\n- Project tracker: https://github.com/Project-MONAI/MONAI/projects\n- Issue tracker: https://github.com/Project-MONAI/MONAI/issues\n- Wiki: https://github.com/Project-MONAI/MONAI/wiki\n- Test status: https://github.com/Project-MONAI/MONAI/actions\n- PyPI package: https://pypi.org/project/monai/\n- conda-forge: https://anaconda.org/conda-forge/monai\n- Weekly previews: https://pypi.org/project/monai-weekly/\n- Docker Hub: https://hub.docker.com/r/projectmonai/monai\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment-dev.yml",
          "type": "blob",
          "size": 0.21875,
          "content": "name: monai\nchannels:\n  - pytorch\n  - defaults\n  - nvidia\n  - conda-forge\ndependencies:\n  - numpy>=1.24,<2.0\n  - pytorch>=1.9\n  - torchio\n  - torchvision\n  - pytorch-cuda>=11.6\n  - pip\n  - pip:\n    - -r requirements-dev.txt\n"
        },
        {
          "name": "monai",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.974609375,
          "content": "[build-system]\nrequires = [\n  \"wheel\",\n  \"setuptools\",\n  \"torch>=1.9\",\n  \"ninja\",\n  \"packaging\"\n]\n\n[tool.black]\nline-length = 120\ntarget-version = ['py38', 'py39', 'py310']\ninclude = '\\.pyi?$'\nexclude = '''\n(\n  /(\n    # exclude a few common directories in the root of the project\n      \\.eggs\n    | \\.git\n    | \\.hg\n    | \\.mypy_cache\n    | \\.tox\n    | \\.venv\n    | venv\n    | \\.pytype\n    | _build\n    | buck-out\n    | build\n    | dist\n  )/\n  # also separately exclude a file named versioneer.py\n  | monai/_version.py\n)\n'''\n\n[tool.pycln]\nall = true\nexclude = \"monai/bundle/__main__.py\"\n\n[tool.ruff]\nline-length = 133\ntarget-version = \"py39\"\n\n[tool.ruff.lint]\nselect = [\n  \"E\", \"F\", \"W\", # flake8\n  \"NPY\",         # NumPy specific rules\n]\nextend-ignore = [\n  \"E741\", # ambiguous variable name\n  \"F401\", # unused import\n  \"NPY002\", # numpy-legacy-random\n]\n\n[tool.pytype]\n# Space-separated list of files or directories to exclude.\nexclude = [\"versioneer.py\", \"_version.py\"]\n# Space-separated list of files or directories to process.\ninputs = [\"monai\"]\n# Keep going past errors to analyze as many files as possible.\nkeep_going = true\n# Run N jobs in parallel.\njobs = 8\n# All pytype output goes here.\noutput = \".pytype\"\n# Paths to source code directories, separated by ':'.\npythonpath = \".\"\n# Check attribute values against their annotations.\ncheck_attribute_types = true\n# Check container mutations against their annotations.\ncheck_container_types = true\n# Check parameter defaults and assignments against their annotations.\ncheck_parameter_types = true\n# Check variable values against their annotations.\ncheck_variable_types = true\n# Comma or space separated list of error names to ignore.\ndisable = [\"pyi-error\"]\n# Report errors.\nreport_errors = true\n# Experimental: Infer precise return types even for invalid function calls.\nprecise_return = true\n# Experimental: solve unknown types to label with structural types.\nprotocols = true\n# Experimental: Only load submodules that are explicitly imported.\nstrict_import = false\n"
        },
        {
          "name": "requirements-dev.txt",
          "type": "blob",
          "size": 1.62890625,
          "content": "# Full requirements for developments\n-r requirements-min.txt\npytorch-ignite==0.4.11\ngdown>=4.7.3\nscipy>=1.12.0; python_version >= '3.9'\nitk>=5.2\nnibabel\npillow!=8.3.0  # https://github.com/python-pillow/Pillow/issues/5571\ntensorboard>=2.12.0  # https://github.com/Project-MONAI/MONAI/issues/7434\nscikit-image>=0.19.0\ntqdm>=4.47.0\nlmdb\nflake8>=3.8.1\nflake8-bugbear<=24.2.6  # https://github.com/Project-MONAI/MONAI/issues/7690\nflake8-comprehensions\nmccabe\npep8-naming\npycodestyle\npyflakes\nblack>=22.12\nisort>=5.1\nruff\npytype>=2020.6.1; platform_system != \"Windows\"\ntypes-setuptools\nmypy>=1.5.0, <1.12.0\nninja\ntorchio\ntorchvision\npsutil\ncucim-cu12; platform_system == \"Linux\" and python_version >= \"3.9\" and python_version <= \"3.10\"\nopenslide-python\nimagecodecs; platform_system == \"Linux\" or platform_system == \"Darwin\"\ntifffile; platform_system == \"Linux\" or platform_system == \"Darwin\"\npandas\nrequests\neinops\ntransformers>=4.36.0, <4.41.0; python_version <= '3.10'\nmlflow>=2.12.2\nclearml>=1.10.0rc0\nmatplotlib>=3.6.3\ntensorboardX\ntypes-PyYAML\npyyaml\nfire\njsonschema\npynrrd\npre-commit\npydicom\nh5py\nnni==2.10.1; platform_system == \"Linux\" and \"arm\" not in platform_machine and \"aarch\" not in platform_machine\noptuna\ngit+https://github.com/Project-MONAI/MetricsReloaded@monai-support#egg=MetricsReloaded\nonnx>=1.13.0\nonnxruntime; python_version <= '3.10'\ntypeguard<3  # https://github.com/microsoft/nni/issues/5457\nfilelock<3.12.0  # https://github.com/microsoft/nni/issues/5523\nzarr\nlpips==0.1.4\nnvidia-ml-py\nhuggingface_hub\npyamg>=5.0.0\ngit+https://github.com/facebookresearch/segment-anything.git@6fdee8f2727f4506cfbbe553e23b895e27956588\nonnx_graphsurgeon\npolygraphy\n"
        },
        {
          "name": "requirements-min.txt",
          "type": "blob",
          "size": 0.193359375,
          "content": "# Requirements for minimal tests\n-r requirements.txt\nsetuptools>=50.3.0,<66.0.0,!=60.6.0 ; python_version < \"3.12\"\nsetuptools>=70.2.0; python_version >= \"3.12\"\ncoverage>=5.5\nparameterized\npackaging\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.02734375,
          "content": "torch>=1.9\nnumpy>=1.24,<2.0\n"
        },
        {
          "name": "runtests.sh",
          "type": "blob",
          "size": 22.6162109375,
          "content": "#! /bin/bash\n\n# Copyright (c) MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# script for running all tests\nset -e\n\n# output formatting\nseparator=\"\"\nblue=\"\"\ngreen=\"\"\nred=\"\"\nnoColor=\"\"\n\nif [[ -t 1 ]] # stdout is a terminal\nthen\n    separator=$'--------------------------------------------------------------------------------\\n'\n    blue=\"$(tput bold; tput setaf 4)\"\n    green=\"$(tput bold; tput setaf 2)\"\n    red=\"$(tput bold; tput setaf 1)\"\n    noColor=\"$(tput sgr0)\"\nfi\n\n# configuration values\ndoCoverage=false\ndoQuickTests=false\ndoMinTests=false\ndoNetTests=false\ndoDryRun=false\ndoZooTests=false\ndoUnitTests=false\ndoBuild=false\ndoBlackFormat=false\ndoBlackFix=false\ndoIsortFormat=false\ndoIsortFix=false\ndoFlake8Format=false\ndoPylintFormat=false\ndoRuffFormat=false\ndoRuffFix=false\ndoClangFormat=false\ndoCopyRight=false\ndoPytypeFormat=false\ndoMypyFormat=false\ndoCleanup=false\ndoDistTests=false\ndoPrecommit=false\n\nNUM_PARALLEL=1\n\nPY_EXE=${MONAI_PY_EXE:-$(which python)}\n\nfunction print_usage {\n    echo \"runtests.sh [--codeformat] [--autofix] [--black] [--isort] [--flake8] [--pylint] [--ruff]\"\n    echo \"            [--clangformat] [--precommit] [--pytype] [-j number] [--mypy]\"\n    echo \"            [--unittests] [--disttests] [--coverage] [--quick] [--min] [--net] [--build] [--list_tests]\"\n    echo \"            [--dryrun] [--copyright] [--clean] [--help] [--version] [--path] [--formatfix]\"\n    echo \"\"\n    echo \"MONAI unit testing utilities.\"\n    echo \"\"\n    echo \"Examples:\"\n    echo \"./runtests.sh -f -u --net --coverage  # run style checks, full tests, print code coverage (${green}recommended for pull requests${noColor}).\"\n    echo \"./runtests.sh -f -u                   # run style checks and unit tests.\"\n    echo \"./runtests.sh -f                      # run coding style and static type checking.\"\n    echo \"./runtests.sh --quick --unittests     # run minimal unit tests, for quick verification during code developments.\"\n    echo \"./runtests.sh --autofix               # run automatic code formatting using \\\"isort\\\" and \\\"black\\\".\"\n    echo \"./runtests.sh --clean                 # clean up temporary files and run \\\"${PY_EXE} setup.py develop --uninstall\\\".\"\n    echo \"./runtests.sh --formatfix -p /my/code # run automatic code formatting using \\\"isort\\\" and \\\"black\\\" in specified path.\"\n    echo \"\"\n    echo \"Code style check options:\"\n    echo \"    --autofix         : format code using \\\"isort\\\" and \\\"black\\\"\"\n    echo \"    --black           : perform \\\"black\\\" code format checks\"\n    echo \"    --isort           : perform \\\"isort\\\" import sort checks\"\n    echo \"    --flake8          : perform \\\"flake8\\\" code format checks\"\n    echo \"    --pylint          : perform \\\"pylint\\\" code format checks\"\n    echo \"    --ruff            : perform \\\"ruff\\\" code format checks\"\n    echo \"    --clangformat     : format csrc code using \\\"clang-format\\\"\"\n    echo \"    --precommit       : perform source code format check and fix using \\\"pre-commit\\\"\"\n    echo \"\"\n    echo \"Python type check options:\"\n    echo \"    --pytype          : perform \\\"pytype\\\" static type checks\"\n    echo \"    -j, --jobs        : number of parallel jobs to run \\\"pytype\\\" (default $NUM_PARALLEL)\"\n    echo \"    --mypy            : perform \\\"mypy\\\" static type checks\"\n    echo \"\"\n    echo \"MONAI unit testing options:\"\n    echo \"    -u, --unittests   : perform unit testing\"\n    echo \"    --disttests       : perform distributed unit testing\"\n    echo \"    --coverage        : report testing code coverage, to be used with \\\"--net\\\", \\\"--unittests\\\"\"\n    echo \"    -q, --quick       : skip long running unit tests and integration tests\"\n    echo \"    -m, --min         : only run minimal unit tests which do not require optional packages\"\n    echo \"    --net             : perform integration testing\"\n    echo \"    -b, --build       : compile and install the source code folder an editable release.\"\n    echo \"    --list_tests      : list unit tests and exit\"\n    echo \"\"\n    echo \"Misc. options:\"\n    echo \"    --dryrun          : display the commands to the screen without running\"\n    echo \"    --copyright       : check whether every source code has a copyright header\"\n    echo \"    -f, --codeformat  : shorthand to run all code style and static analysis tests\"\n    echo \"    -c, --clean       : clean temporary files from tests and exit\"\n    echo \"    -h, --help        : show this help message and exit\"\n    echo \"    -v, --version     : show MONAI and system version information and exit\"\n    echo \"    -p, --path        : specify the path used for formatting, default is the current dir if unspecified\"\n    echo \"    --formatfix       : format code using \\\"isort\\\" and \\\"black\\\" for user specified directories\"\n    echo \"\"\n    echo \"${separator}For bug reports and feature requests, please file an issue at:\"\n    echo \"    https://github.com/Project-MONAI/MONAI/issues/new/choose\"\n    echo \"\"\n    echo \"To choose an alternative python executable, set the environmental variable, \\\"MONAI_PY_EXE\\\".\"\n    exit 1\n}\n\n# FIXME: https://github.com/Project-MONAI/MONAI/issues/4354\nprotobuf_major_version=$(\"${PY_EXE}\" -m pip list | grep '^protobuf ' | tr -s ' ' | cut -d' ' -f2 | cut -d'.' -f1)\nif [ \"$protobuf_major_version\" -ge \"4\" ]\nthen\n    export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\nfi\n\nfunction check_import {\n    echo \"Python: \"${PY_EXE}\"\"\n    ${cmdPrefix}\"${PY_EXE}\" -W error -W ignore::DeprecationWarning -W ignore::ResourceWarning -c \"import monai\"\n}\n\nfunction print_version {\n    ${cmdPrefix}\"${PY_EXE}\" -c 'import monai; monai.config.print_config()'  # project-monai/monai#6167\n}\n\nfunction install_deps {\n    echo \"Pip installing MONAI development dependencies and compile MONAI cpp extensions...\"\n    ${cmdPrefix}\"${PY_EXE}\" -m pip install -r requirements-dev.txt\n}\n\nfunction compile_cpp {\n    echo \"Compiling and installing MONAI cpp extensions...\"\n    # depends on setup.py behaviour for building\n    # currently setup.py uses environment variables: BUILD_MONAI and FORCE_CUDA\n    ${cmdPrefix}\"${PY_EXE}\" setup.py develop --user --uninstall\n    if [[ \"$OSTYPE\" == \"darwin\"* ]];\n    then  # clang for mac os\n        CC=clang CXX=clang++ ${cmdPrefix}\"${PY_EXE}\" setup.py develop --user\n    else\n        ${cmdPrefix}\"${PY_EXE}\" setup.py develop --user\n    fi\n}\n\nfunction clang_format {\n    echo \"Running clang-format...\"\n    ${cmdPrefix}\"${PY_EXE}\" -m tests.clang_format_utils\n    clang_format_tool='.clang-format-bin/clang-format'\n    # Verify .\n    if ! type -p \"$clang_format_tool\" >/dev/null; then\n        echo \"'clang-format' not found, skipping the formatting.\"\n        exit 1\n    fi\n    find monai/csrc -type f | while read i; do $clang_format_tool -style=file -i $i; done\n    find monai/_extensions -type f -name \"*.cpp\" -o -name \"*.h\" -o -name \"*.cuh\" -o -name \"*.cu\" |\\\n        while read i; do $clang_format_tool -style=file -i $i; done\n}\n\nfunction is_pip_installed() {\n\treturn $(\"${PY_EXE}\" -c \"import sys, importlib.util; sys.exit(0 if importlib.util.find_spec(sys.argv[1]) else 1)\" $1)\n}\n\nfunction clean_py {\n    if is_pip_installed coverage\n    then\n      # remove coverage history\n      ${cmdPrefix}\"${PY_EXE}\" -m coverage erase\n    fi\n\n    # uninstall the development package\n    echo \"Uninstalling MONAI development files...\"\n    ${cmdPrefix}\"${PY_EXE}\" setup.py develop --user --uninstall\n\n    # remove temporary files (in the directory of this script)\n    TO_CLEAN=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n    echo \"Removing temporary files in ${TO_CLEAN}\"\n\n    find ${TO_CLEAN}/monai -type f -name \"*.py[co]\" -delete\n    find ${TO_CLEAN}/monai -type f -name \"*.so\" -delete\n    find ${TO_CLEAN}/monai -type d -name \"__pycache__\" -delete\n    find ${TO_CLEAN} -maxdepth 1 -type f -name \".coverage.*\" -delete\n\n    find ${TO_CLEAN} -depth -maxdepth 1 -type d -name \".eggs\" -exec rm -r \"{}\" +\n    find ${TO_CLEAN} -depth -maxdepth 1 -type d -name \"monai.egg-info\" -exec rm -r \"{}\" +\n    find ${TO_CLEAN} -depth -maxdepth 1 -type d -name \"build\" -exec rm -r \"{}\" +\n    find ${TO_CLEAN} -depth -maxdepth 1 -type d -name \"dist\" -exec rm -r \"{}\" +\n    find ${TO_CLEAN} -depth -maxdepth 1 -type d -name \".mypy_cache\" -exec rm -r \"{}\" +\n    find ${TO_CLEAN} -depth -maxdepth 1 -type d -name \".pytype\" -exec rm -r \"{}\" +\n    find ${TO_CLEAN} -depth -maxdepth 1 -type d -name \".coverage\" -exec rm -r \"{}\" +\n    find ${TO_CLEAN} -depth -maxdepth 1 -type d -name \"__pycache__\" -exec rm -r \"{}\" +\n}\n\nfunction torch_validate {\n    ${cmdPrefix}\"${PY_EXE}\" -c 'import torch; print(torch.__version__); print(torch.rand(5,3))'\n}\n\nfunction print_error_msg() {\n    echo \"${red}Error: $1.${noColor}\"\n    echo \"\"\n}\n\nfunction print_style_fail_msg() {\n    echo \"${red}Check failed!${noColor}\"\n    if [ \"$homedir\" = \"$currentdir\" ]\n    then\n        echo \"Please run auto style fixes: ${green}./runtests.sh --autofix${noColor}\"\n    else :\n    fi\n}\n\nfunction list_unittests() {\n    \"${PY_EXE}\" - << END\nimport unittest\ndef print_suite(suite):\n    if hasattr(suite, \"__iter__\"):\n        for x in suite:\n            print_suite(x)\n    else:\n        print(suite)\nprint_suite(unittest.defaultTestLoader.discover('./tests'))\nEND\n    exit 0\n}\n\nif [ -z \"$1\" ]\nthen\n    print_error_msg \"Too few arguments to $0\"\n    print_usage\nfi\n\n# parse arguments\nwhile [[ $# -gt 0 ]]\ndo\n    key=\"$1\"\n    case $key in\n        --coverage)\n            doCoverage=true\n        ;;\n        -q|--quick)\n            doQuickTests=true\n        ;;\n        -m|--min)\n            doMinTests=true\n        ;;\n        --net)\n            doNetTests=true\n        ;;\n        --list_tests)\n            list_unittests\n        ;;\n        --dryrun)\n            doDryRun=true\n        ;;\n        -u|--u*)  # allow --unittest | --unittests | --unittesting  etc.\n            doUnitTests=true\n        ;;\n        -f|--codeformat)\n            doBlackFormat=true\n            doIsortFormat=true\n            doFlake8Format=true\n            # doPylintFormat=true  # https://github.com/Project-MONAI/MONAI/issues/7094\n            doRuffFormat=true\n            doCopyRight=true\n        ;;\n        --disttests)\n            doDistTests=true\n        ;;\n        --black)\n            doBlackFormat=true\n        ;;\n        --autofix)\n            doIsortFix=true\n            doBlackFix=true\n            doRuffFix=true\n            doIsortFormat=true\n            doBlackFormat=true\n            doRuffFormat=true\n            doCopyRight=true\n        ;;\n        --formatfix)\n            doIsortFix=true\n            doBlackFix=true\n            doIsortFormat=true\n            doBlackFormat=true\n        ;;\n        --clangformat)\n            doClangFormat=true\n        ;;\n        --isort)\n            doIsortFormat=true\n        ;;\n        --flake8)\n            doFlake8Format=true\n        ;;\n        --pylint)\n            doPylintFormat=true\n        ;;\n        --ruff)\n            doRuffFormat=true\n        ;;\n        --precommit)\n            doPrecommit=true\n        ;;\n        --pytype)\n            doPytypeFormat=true\n        ;;\n        --mypy)\n            doMypyFormat=true\n        ;;\n        -j|--jobs)\n            NUM_PARALLEL=$2\n            shift\n        ;;\n        --copyright)\n            doCopyRight=true\n        ;;\n        -b|--build)\n            doBuild=true\n        ;;\n        -c|--clean)\n            doCleanup=true\n        ;;\n        -h|--help)\n            print_usage\n        ;;\n        -v|--version)\n            print_version\n            exit 1\n        ;;\n        --nou*)  # allow --nounittest | --nounittests | --nounittesting  etc.\n            print_error_msg \"nounittest option is deprecated, no unit tests is the default setting\"\n            print_usage\n        ;;\n        -p|--path)\n            testdir=$2\n            shift\n        ;;\n        *)\n            print_error_msg \"Incorrect commandline provided, invalid key: $key\"\n            print_usage\n        ;;\n    esac\n    shift\ndone\n\n# home directory\ncurrentdir=\"$( cd -P \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\"\nif [ -e \"$testdir\" ]\nthen\n    homedir=$testdir\nelse\n    homedir=$currentdir\nfi\necho \"Run tests under $homedir\"\ncd \"$homedir\"\n\n# python path\nexport PYTHONPATH=\"$homedir:$PYTHONPATH\"\necho \"PYTHONPATH: $PYTHONPATH\"\n\n# by default do nothing\ncmdPrefix=\"\"\n\nif [ $doDryRun = true ]\nthen\n    echo \"${separator}${blue}dryrun${noColor}\"\n\n    # commands are echoed instead of ran\n    cmdPrefix=\"dryrun \"\n    function dryrun { echo \"    \" \"$@\"; }\nelse\n    check_import\nfi\n\nif [ $doBuild = true ]\nthen\n    echo \"${separator}${blue}compile and install${noColor}\"\n    # try to compile MONAI cpp\n    compile_cpp\n\n    echo \"${green}done! (to uninstall and clean up, please use \\\"./runtests.sh --clean\\\")${noColor}\"\nfi\n\nif [ $doCleanup = true ]\nthen\n    echo \"${separator}${blue}clean${noColor}\"\n\n    clean_py\n\n    echo \"${green}done!${noColor}\"\n    exit\nfi\n\nif [ $doClangFormat = true ]\nthen\n    echo \"${separator}${blue}clang-formatting${noColor}\"\n\n    clang_format\n\n    echo \"${green}done!${noColor}\"\nfi\n\n# unconditionally report on the state of monai\nprint_version\n\nif [ $doCopyRight = true ]\nthen\n    # check copyright headers\n    copyright_bad=0\n    copyright_all=0\n    while read -r fname; do\n        copyright_all=$((copyright_all + 1))\n        if ! grep \"http://www.apache.org/licenses/LICENSE-2.0\" \"$fname\" > /dev/null; then\n            print_error_msg \"Missing the license header in file: $fname\"\n            copyright_bad=$((copyright_bad + 1))\n        fi\n    done <<< \"$(find \"$(pwd)/monai\" \"$(pwd)/tests\" -type f \\\n        ! -wholename \"*_version.py\" -and -name \"*.py\" -or -name \"*.cpp\" -or -name \"*.cu\" -or -name \"*.h\")\"\n    if [[ ${copyright_bad} -eq 0 ]];\n    then\n        echo \"${green}Source code copyright headers checked ($copyright_all).${noColor}\"\n    else\n        echo \"Please add the licensing header to the file ($copyright_bad of $copyright_all files).\"\n        echo \"  See also: https://github.com/Project-MONAI/MONAI/blob/dev/CONTRIBUTING.md#checking-the-coding-style\"\n        echo \"\"\n        exit 1\n    fi\nfi\n\n\nif [ $doPrecommit = true ]\nthen\n    set +e  # disable exit on failure so that diagnostics can be given on failure\n    echo \"${separator}${blue}pre-commit${noColor}\"\n\n    # ensure that the necessary packages for code format testing are installed\n    if ! is_pip_installed pre_commit\n    then\n        install_deps\n    fi\n    ${cmdPrefix}\"${PY_EXE}\" -m pre_commit run --all-files\n\n    pre_commit_status=$?\n    if [ ${pre_commit_status} -ne 0 ]\n    then\n        print_style_fail_msg\n        exit ${pre_commit_status}\n    else\n        echo \"${green}passed!${noColor}\"\n    fi\n    set -e # enable exit on failure\nfi\n\n\nif [ $doIsortFormat = true ]\nthen\n    set +e  # disable exit on failure so that diagnostics can be given on failure\n    if [ $doIsortFix = true ]\n    then\n        echo \"${separator}${blue}isort-fix${noColor}\"\n    else\n        echo \"${separator}${blue}isort${noColor}\"\n    fi\n\n    # ensure that the necessary packages for code format testing are installed\n    if ! is_pip_installed isort\n    then\n        install_deps\n    fi\n    ${cmdPrefix}\"${PY_EXE}\" -m isort --version\n\n    if [ $doIsortFix = true ]\n    then\n        ${cmdPrefix}\"${PY_EXE}\" -m isort \"$homedir\"\n    else\n        ${cmdPrefix}\"${PY_EXE}\" -m isort --check \"$homedir\"\n    fi\n\n    isort_status=$?\n    if [ ${isort_status} -ne 0 ]\n    then\n        print_style_fail_msg\n        exit ${isort_status}\n    else\n        echo \"${green}passed!${noColor}\"\n    fi\n    set -e # enable exit on failure\nfi\n\n\nif [ $doBlackFormat = true ]\nthen\n    set +e  # disable exit on failure so that diagnostics can be given on failure\n    if [ $doBlackFix = true ]\n    then\n        echo \"${separator}${blue}black-fix${noColor}\"\n    else\n        echo \"${separator}${blue}black${noColor}\"\n    fi\n\n    # ensure that the necessary packages for code format testing are installed\n    if ! is_pip_installed black\n    then\n        install_deps\n    fi\n    ${cmdPrefix}\"${PY_EXE}\" -m black --version\n\n    if [ $doBlackFix = true ]\n    then\n        ${cmdPrefix}\"${PY_EXE}\" -m black --skip-magic-trailing-comma \"$homedir\"\n    else\n        ${cmdPrefix}\"${PY_EXE}\" -m black --skip-magic-trailing-comma --check \"$homedir\"\n    fi\n\n    black_status=$?\n    if [ ${black_status} -ne 0 ]\n    then\n        print_style_fail_msg\n        exit ${black_status}\n    else\n        echo \"${green}passed!${noColor}\"\n    fi\n    set -e # enable exit on failure\nfi\n\n\nif [ $doFlake8Format = true ]\nthen\n    set +e  # disable exit on failure so that diagnostics can be given on failure\n    echo \"${separator}${blue}flake8${noColor}\"\n\n    # ensure that the necessary packages for code format testing are installed\n    if ! is_pip_installed flake8\n    then\n        install_deps\n    fi\n    ${cmdPrefix}\"${PY_EXE}\" -m flake8 --version\n\n    ${cmdPrefix}\"${PY_EXE}\" -m flake8 \"$homedir\" --count --statistics\n\n    flake8_status=$?\n    if [ ${flake8_status} -ne 0 ]\n    then\n        print_style_fail_msg\n        exit ${flake8_status}\n    else\n        echo \"${green}passed!${noColor}\"\n    fi\n    set -e # enable exit on failure\nfi\n\nif [ $doPylintFormat = true ]\nthen\n    set +e  # disable exit on failure so that diagnostics can be given on failure\n    echo \"${separator}${blue}pylint${noColor}\"\n\n    # ensure that the necessary packages for code format testing are installed\n    if ! is_pip_installed pylint\n    then\n        echo \"Pip installing pylint ...\"\n        ${cmdPrefix}\"${PY_EXE}\" -m pip install \"pylint>2.16,!=3.0.0\"\n    fi\n    ${cmdPrefix}\"${PY_EXE}\" -m pylint --version\n\n    ignore_codes=\"C,R,W,E1101,E1102,E0601,E1130,E1123,E0102,E1120,E1137,E1136\"\n    ${cmdPrefix}\"${PY_EXE}\" -m pylint monai tests --disable=$ignore_codes -j $NUM_PARALLEL\n    pylint_status=$?\n\n    if [ ${pylint_status} -ne 0 ]\n    then\n        print_style_fail_msg\n        exit ${pylint_status}\n    else\n        echo \"${green}passed!${noColor}\"\n    fi\n    set -e # enable exit on failure\nfi\n\n\nif [ $doRuffFormat = true ]\nthen\n    set +e  # disable exit on failure so that diagnostics can be given on failure\n    if [ $doRuffFix = true ]\n    then\n        echo \"${separator}${blue}ruff-fix${noColor}\"\n    else\n        echo \"${separator}${blue}ruff${noColor}\"\n    fi\n\n    # ensure that the necessary packages for code format testing are installed\n    if ! is_pip_installed ruff\n    then\n        install_deps\n    fi\n    ruff --version\n\n    if [ $doRuffFix = true ]\n    then\n        ruff check --fix \"$homedir\"\n    else\n        ruff check \"$homedir\"\n    fi\n\n    ruff_status=$?\n    if [ ${ruff_status} -ne 0 ]\n    then\n        print_style_fail_msg\n        exit ${ruff_status}\n    else\n        echo \"${green}passed!${noColor}\"\n    fi\n    set -e # enable exit on failure\nfi\n\n\nif [ $doPytypeFormat = true ]\nthen\n    set +e  # disable exit on failure so that diagnostics can be given on failure\n    echo \"${separator}${blue}pytype${noColor}\"\n    # ensure that the necessary packages for code format testing are installed\n    if ! is_pip_installed pytype\n    then\n        install_deps\n    fi\n    pytype_ver=$(${cmdPrefix}\"${PY_EXE}\" -m pytype --version)\n    if [[ \"$OSTYPE\" == \"darwin\"* && \"$pytype_ver\" == \"2021.\"* ]]; then\n        echo \"${red}pytype not working on macOS 2021 (https://github.com/Project-MONAI/MONAI/issues/2391). Please upgrade to 2022*.${noColor}\"\n        exit 1\n    else\n        ${cmdPrefix}\"${PY_EXE}\" -m pytype --version\n\n        ${cmdPrefix}\"${PY_EXE}\" -m pytype -j ${NUM_PARALLEL} --python-version=\"$(${PY_EXE} -c \"import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')\")\" \"$homedir\"\n\n        pytype_status=$?\n        if [ ${pytype_status} -ne 0 ]\n        then\n            echo \"${red}failed!${noColor}\"\n            exit ${pytype_status}\n        else\n            echo \"${green}passed!${noColor}\"\n        fi\n    fi\n    set -e # enable exit on failure\nfi\n\n\nif [ $doMypyFormat = true ]\nthen\n    set +e  # disable exit on failure so that diagnostics can be given on failure\n    echo \"${separator}${blue}mypy${noColor}\"\n\n    # ensure that the necessary packages for code format testing are installed\n    if ! is_pip_installed mypy\n    then\n        install_deps\n    fi\n    ${cmdPrefix}\"${PY_EXE}\" -m mypy --version\n    ${cmdPrefix}\"${PY_EXE}\" -m mypy \"$homedir\"\n\n    mypy_status=$?\n    if [ ${mypy_status} -ne 0 ]\n    then\n        : # mypy output already follows format\n        exit ${mypy_status}\n    else\n        : # mypy output already follows format\n    fi\n    set -e # enable exit on failure\nfi\n\n\n# testing command to run\ncmd=\"${PY_EXE}\"\n\n# When running --quick, require doCoverage as well and set QUICKTEST environmental\n# variable to disable slow unit tests from running.\nif [ $doQuickTests = true ]\nthen\n    echo \"${separator}${blue}quick${noColor}\"\n    doCoverage=true\n    export QUICKTEST=True\nfi\n\nif [ $doMinTests = true ]\nthen\n    echo \"${separator}${blue}min${noColor}\"\n    doCoverage=false\n    ${cmdPrefix}\"${PY_EXE}\" -m tests.min_tests\nfi\n\n# set coverage command\nif [ $doCoverage = true ]\nthen\n    echo \"${separator}${blue}coverage${noColor}\"\n    # ensure that the necessary packages for code format testing are installed\n    if ! is_pip_installed coverage\n    then\n        install_deps\n    fi\n    cmd=\"\"${PY_EXE}\" -m coverage run --append\"\nfi\n\n# # download test data if needed\n# if [ ! -d testing_data ] && [ \"$doDryRun\" != 'true' ]\n# then\n# fi\n\n# unit tests\nif [ $doUnitTests = true ]\nthen\n    echo \"${separator}${blue}unittests${noColor}\"\n    torch_validate\n    ${cmdPrefix}${cmd} ./tests/runner.py -p \"^(?!test_integration).*(?<!_dist)$\"  # excluding integration/dist tests\nfi\n\n# distributed test only\nif [ $doDistTests = true ]\nthen\n    echo \"${separator}${blue}run distributed unit test cases${noColor}\"\n    torch_validate\n    for i in tests/test_*_dist.py\n    do\n        echo \"$i\"\n        ${cmdPrefix}${cmd} \"$i\"\n    done\nfi\n\n# network training/inference/eval integration tests\nif [ $doNetTests = true ]\nthen\n    set +e  # disable exit on failure so that diagnostics can be given on failure\n    echo \"${separator}${blue}integration${noColor}\"\n    for i in tests/*integration_*.py\n    do\n        echo \"$i\"\n        ${cmdPrefix}${cmd} \"$i\"\n    done\n    set -e # enable exit on failure\nfi\n\n# run model zoo tests\nif [ $doZooTests = true ]\nthen\n    echo \"${separator}${blue}zoo${noColor}\"\n    print_error_msg \"--zoo option not yet implemented\"\n    exit 255\nfi\n\n# report on coverage\nif [ $doCoverage = true ]\nthen\n    echo \"${separator}${blue}coverage${noColor}\"\n    # ensure that the necessary packages for code format testing are installed\n    if ! is_pip_installed coverage\n    then\n        install_deps\n    fi\n    ${cmdPrefix}\"${PY_EXE}\" -m coverage combine --append .coverage/\n    ${cmdPrefix}\"${PY_EXE}\" -m coverage report --ignore-errors\nfi\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 7.7734375,
          "content": "[metadata]\nname = monai\nauthor = MONAI Consortium\nauthor_email = monai.contact@gmail.com\nurl = https://monai.io/\ndescription = AI Toolkit for Healthcare Imaging\nlong_description = file:README.md\nlong_description_content_type = text/markdown; charset=UTF-8\nplatforms = OS Independent\nlicense = Apache License 2.0\nlicense_files =\n    LICENSE\nproject_urls =\n    Documentation=https://docs.monai.io/\n    Bug Tracker=https://github.com/Project-MONAI/MONAI/issues\n    Source Code=https://github.com/Project-MONAI/MONAI\nclassifiers =\n    Intended Audience :: Developers\n    Intended Audience :: Education\n    Intended Audience :: Science/Research\n    Intended Audience :: Healthcare Industry\n    Programming Language :: C++\n    Programming Language :: Python :: 3\n    Programming Language :: Python :: 3.9\n    Programming Language :: Python :: 3.10\n    Programming Language :: Python :: 3.11\n    Topic :: Scientific/Engineering\n    Topic :: Scientific/Engineering :: Artificial Intelligence\n    Topic :: Scientific/Engineering :: Medical Science Apps.\n    Topic :: Scientific/Engineering :: Information Analysis\n    Topic :: Software Development\n    Topic :: Software Development :: Libraries\n    Typing :: Typed\n\n[options]\npython_requires = >= 3.9\n# for compiling and develop setup only\n# no need to specify the versions so that we could\n# compile for multiple targeted versions.\nsetup_requires =\n    torch\n    ninja\n    packaging\ninstall_requires =\n    torch>=1.9\n    numpy>=1.24,<2.0\n\n[options.extras_require]\nall =\n    nibabel\n    ninja\n    scikit-image>=0.14.2\n    scipy>=1.12.0; python_version >= '3.9'\n    pillow\n    tensorboard\n    gdown>=4.7.3\n    pytorch-ignite==0.4.11\n    torchio\n    torchvision\n    itk>=5.2\n    tqdm>=4.47.0\n    lmdb\n    psutil\n    cucim-cu12; platform_system == \"Linux\" and python_version >= '3.9' and python_version <= '3.10'\n    openslide-python\n    tifffile; platform_system == \"Linux\" or platform_system == \"Darwin\"\n    imagecodecs; platform_system == \"Linux\" or platform_system == \"Darwin\"\n    pandas\n    einops\n    transformers>=4.36.0, <4.41.0; python_version <= '3.10'\n    mlflow>=2.12.2\n    clearml>=1.10.0rc0\n    matplotlib>=3.6.3\n    tensorboardX\n    pyyaml\n    fire\n    jsonschema\n    pynrrd\n    pydicom\n    h5py\n    nni; platform_system == \"Linux\" and \"arm\" not in platform_machine and \"aarch\" not in platform_machine\n    optuna\n    onnx>=1.13.0\n    onnxruntime; python_version <= '3.10'\n    zarr\n    lpips==0.1.4\n    nvidia-ml-py\n    huggingface_hub\n    pyamg>=5.0.0\nnibabel =\n    nibabel\nninja =\n    ninja\nskimage =\n    scikit-image>=0.14.2\nscipy =\n    scipy>=1.12.0; python_version >= '3.9'\npillow =\n    pillow!=8.3.0\ntensorboard =\n    tensorboard\ngdown =\n    gdown>=4.7.3\nignite =\n    pytorch-ignite==0.4.11\ntorchio =\n    torchio\ntorchvision =\n    torchvision\nitk =\n    itk>=5.2\ntqdm =\n    tqdm>=4.47.0\nlmdb =\n    lmdb\npsutil =\n    psutil\ncucim =\n    cucim-cu12; platform_system == \"Linux\" and python_version >= '3.9' and python_version <= '3.10'\nopenslide =\n    openslide-python\ntifffile =\n    tifffile; platform_system == \"Linux\" or platform_system == \"Darwin\"\nimagecodecs =\n    imagecodecs; platform_system == \"Linux\" or platform_system == \"Darwin\"\npandas =\n    pandas\neinops =\n    einops\ntransformers =\n    transformers>=4.36.0, <4.41.0; python_version <= '3.10'\nmlflow =\n    mlflow>=2.12.2\nmatplotlib =\n    matplotlib>=3.6.3\nclearml =\n    clearml\ntensorboardX =\n    tensorboardX\npyyaml =\n    pyyaml\nfire =\n    fire\npackaging =\n    packaging\njsonschema =\n    jsonschema\npynrrd =\n    pynrrd\npydicom =\n    pydicom\nh5py =\n    h5py\nnni =\n    nni; platform_system == \"Linux\" and \"arm\" not in platform_machine and \"aarch\" not in platform_machine\noptuna =\n    optuna\nonnx =\n    onnx>=1.13.0\n    onnxruntime; python_version <= '3.10'\nzarr =\n    zarr\nlpips =\n    lpips==0.1.4\npynvml =\n    nvidia-ml-py\npolygraphy =\n    polygraphy\n\n# # workaround https://github.com/Project-MONAI/MONAI/issues/5882\n# MetricsReloaded =\n    # MetricsReloaded @ git+https://github.com/Project-MONAI/MetricsReloaded@monai-support#egg=MetricsReloaded\nhuggingface_hub =\n    huggingface_hub\npyamg =\n    pyamg>=5.0.0\n# segment-anything =\n#     segment-anything @ git+https://github.com/facebookresearch/segment-anything@6fdee8f2727f4506cfbbe553e23b895e27956588#egg=segment-anything\n\n[flake8]\nselect = B,C,E,F,N,P,T4,W,B9\nmax_line_length = 120\n# C408 ignored because we like the dict keyword argument syntax\n# E501 is not flexible enough, we're using B950 instead\n# N812 lowercase 'torch.nn.functional' imported as non lowercase 'F'\n# B023 https://github.com/Project-MONAI/MONAI/issues/4627\n# B028 https://github.com/Project-MONAI/MONAI/issues/5855\n# B907 https://github.com/Project-MONAI/MONAI/issues/5868\n# B908 https://github.com/Project-MONAI/MONAI/issues/6503\n# B036 https://github.com/Project-MONAI/MONAI/issues/7396\n# E704 https://github.com/Project-MONAI/MONAI/issues/7421\nignore =\n    E203\n    E501\n    E741\n    W503\n    W504\n    C408\n    N812\n    B023\n    B905\n    B028\n    B907\n    B908\n    B036\n    E704\nper_file_ignores = __init__.py: F401, __main__.py: F401\nexclude = *.pyi,.git,.eggs,monai/_version.py,versioneer.py,venv,.venv,_version.py\n\n[isort]\nknown_first_party = monai\nprofile = black\nline_length = 120\nskip = .git, .eggs, venv, .venv, versioneer.py, _version.py, conf.py, monai/__init__.py\nskip_glob = *.pyi\nadd_imports = from __future__ import annotations\nappend_only = true\n\n[versioneer]\nVCS = git\nstyle = pep440\nversionfile_source = monai/_version.py\nversionfile_build = monai/_version.py\ntag_prefix =\nparentdir_prefix =\n\n[mypy]\n# Suppresses error messages about imports that cannot be resolved.\nignore_missing_imports = True\n# Changes the treatment of arguments with a default value of None by not implicitly making their type Optional.\nno_implicit_optional = True\n# Warns about casting an expression to its inferred type.\nwarn_redundant_casts = True\n# No error on unneeded # type: ignore comments.\nwarn_unused_ignores = False\n# Shows a warning when returning a value with type Any from a function declared with a non-Any return type.\nwarn_return_any = True\n# Prohibit equality checks, identity checks, and container checks between non-overlapping types.\nstrict_equality = True\n# Shows column numbers in error messages.\nshow_column_numbers = True\n# Shows error codes in error messages.\nshow_error_codes = True\n# Use visually nicer output in error messages: use soft word wrap, show source code snippets, and show error location markers.\npretty = False\n# Warns about per-module sections in the config file that do not match any files processed when invoking mypy.\nwarn_unused_configs = True\n# Make arguments prepended via Concatenate be truly positional-only.\nextra_checks = True\n# Allows variables to be redefined with an arbitrary type,\n# as long as the redefinition is in the same block and nesting level as the original definition.\n# allow_redefinition = True\n\nexclude = venv/\n\n[mypy-versioneer]\n# Ignores all non-fatal errors.\nignore_errors = True\n\n[mypy-monai._version]\n# Ignores all non-fatal errors.\nignore_errors = True\n\n[mypy-monai.eggs]\n# Ignores all non-fatal errors.\nignore_errors = True\n\n[mypy-monai.*]\n# Also check the body of functions with no types in their type signature.\ncheck_untyped_defs = True\n# Warns about usage of untyped decorators.\ndisallow_untyped_decorators = True\n\n[mypy-monai.visualize.*,monai.utils.*,monai.optimizers.*,monai.losses.*,monai.inferers.*,monai.config.*,monai._extensions.*,monai.fl.*,monai.engines.*,monai.handlers.*,monai.auto3dseg.*,monai.bundle.*,monai.metrics.*,monai.apps.*]\ndisallow_incomplete_defs = True\n\n[coverage:run]\nconcurrency = multiprocessing\nsource = .\ndata_file = .coverage/.coverage\nomit = setup.py\n\n[coverage:report]\nexclude_lines =\n    pragma: no cover\n    if TYPE_CHECKING:\n    # Don't complain if tests don't hit code:\n    raise NotImplementedError\n    if __name__ == .__main__.:\nshow_missing = True\nskip_covered = True\n\n[coverage:xml]\noutput = coverage.xml\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 5.056640625,
          "content": "# Copyright (c) MONAI Consortium\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import annotations\n\nimport glob\nimport os\nimport re\nimport sys\nimport warnings\n\nfrom packaging import version\nfrom setuptools import find_packages, setup\n\nimport versioneer\n\n# TODO: debug mode -g -O0, compile test cases\n\nRUN_BUILD = os.getenv(\"BUILD_MONAI\", \"0\") == \"1\"\nFORCE_CUDA = os.getenv(\"FORCE_CUDA\", \"0\") == \"1\"  # flag ignored if BUILD_MONAI is False\n\nBUILD_CPP = BUILD_CUDA = False\nTORCH_VERSION = 0\ntry:\n    import torch\n\n    print(f\"setup.py with torch {torch.__version__}\")\n    from torch.utils.cpp_extension import BuildExtension, CppExtension\n\n    BUILD_CPP = True\n    from torch.utils.cpp_extension import CUDA_HOME, CUDAExtension\n\n    BUILD_CUDA = FORCE_CUDA or (torch.cuda.is_available() and (CUDA_HOME is not None))\n\n    _pt_version = version.parse(torch.__version__).release\n    if _pt_version is None or len(_pt_version) < 3:\n        raise AssertionError(\"unknown torch version\")\n    TORCH_VERSION = int(_pt_version[0]) * 10000 + int(_pt_version[1]) * 100 + int(_pt_version[2])\nexcept (ImportError, TypeError, AssertionError, AttributeError) as e:\n    warnings.warn(f\"extension build skipped: {e}\")\nfinally:\n    if not RUN_BUILD:\n        BUILD_CPP = BUILD_CUDA = False\n        print(\"Please set environment variable `BUILD_MONAI=1` to enable Cpp/CUDA extension build.\")\n    print(f\"BUILD_MONAI_CPP={BUILD_CPP}, BUILD_MONAI_CUDA={BUILD_CUDA}, TORCH_VERSION={TORCH_VERSION}.\")\n\n\ndef torch_parallel_backend():\n    try:\n        match = re.search(\"^ATen parallel backend: (?P<backend>.*)$\", torch._C._parallel_info(), re.MULTILINE)\n        if match is None:\n            return None\n        backend = match.group(\"backend\")\n        if backend == \"OpenMP\":\n            return \"AT_PARALLEL_OPENMP\"\n        if backend == \"native thread pool\":\n            return \"AT_PARALLEL_NATIVE\"\n        if backend == \"native thread pool and TBB\":\n            return \"AT_PARALLEL_NATIVE_TBB\"\n    except (NameError, AttributeError):  # no torch or no binaries\n        warnings.warn(\"Could not determine torch parallel_info.\")\n    return None\n\n\ndef omp_flags():\n    if sys.platform == \"win32\":\n        return [\"/openmp\"]\n    if sys.platform == \"darwin\":\n        # https://stackoverflow.com/questions/37362414/\n        # return [\"-fopenmp=libiomp5\"]\n        return []\n    return [\"-fopenmp\"]\n\n\ndef get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    ext_dir = os.path.join(this_dir, \"monai\", \"csrc\")\n    include_dirs = [ext_dir]\n\n    source_cpu = glob.glob(os.path.join(ext_dir, \"**\", \"*.cpp\"), recursive=True)\n    source_cuda = glob.glob(os.path.join(ext_dir, \"**\", \"*.cu\"), recursive=True)\n\n    extension = None\n    define_macros = [(f\"{torch_parallel_backend()}\", 1), (\"MONAI_TORCH_VERSION\", TORCH_VERSION)]\n    extra_compile_args = {}\n    extra_link_args = []\n    sources = source_cpu\n    if BUILD_CPP:\n        extension = CppExtension\n        extra_compile_args.setdefault(\"cxx\", [])\n        if torch_parallel_backend() == \"AT_PARALLEL_OPENMP\":\n            extra_compile_args[\"cxx\"] += omp_flags()\n        extra_link_args = omp_flags()\n    if BUILD_CUDA:\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [(\"WITH_CUDA\", None)]\n        extra_compile_args = {\"cxx\": [], \"nvcc\": []}\n        if torch_parallel_backend() == \"AT_PARALLEL_OPENMP\":\n            extra_compile_args[\"cxx\"] += omp_flags()\n    if extension is None or not sources:\n        return []  # compile nothing\n\n    ext_modules = [\n        extension(\n            name=\"monai._C\",\n            sources=sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n            extra_link_args=extra_link_args,\n        )\n    ]\n    return ext_modules\n\n\ndef get_cmds():\n    cmds = versioneer.get_cmdclass()\n\n    if not (BUILD_CPP or BUILD_CUDA):\n        return cmds\n\n    cmds.update({\"build_ext\": BuildExtension.with_options(no_python_abi_suffix=True)})\n    return cmds\n\n\n# Gathering source used for JIT extensions to include in package_data.\njit_extension_source = []\n\nfor ext in [\"cpp\", \"cu\", \"h\", \"cuh\"]:\n    glob_path = os.path.join(\"monai\", \"_extensions\", \"**\", f\"*.{ext}\")\n    jit_extension_source += glob.glob(glob_path, recursive=True)\n\njit_extension_source = [os.path.join(\"..\", path) for path in jit_extension_source]\n\nsetup(\n    version=versioneer.get_version(),\n    cmdclass=get_cmds(),\n    packages=find_packages(exclude=(\"docs\", \"examples\", \"tests\")),\n    zip_safe=False,\n    package_data={\"monai\": [\"py.typed\", *jit_extension_source]},\n    ext_modules=get_extensions(),\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "versioneer.py",
          "type": "blob",
          "size": 79.1962890625,
          "content": "# Version: 0.23\n\n\"\"\"The Versioneer - like a rocketeer, but for versions.\n\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/python-versioneer/python-versioneer\n* Brian Warner\n* License: Public Domain (CC0-1.0)\n* Compatible with: Python 3.7, 3.8, 3.9, 3.10 and pypy3\n* [![Latest Version][pypi-image]][pypi-url]\n* [![Build Status][travis-image]][travis-url]\n\nThis is a tool for managing a recorded version number in distutils/setuptools-based\npython projects. The goal is to remove the tedious and error-prone \"update\nthe embedded version string\" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\n* `pip install versioneer` to somewhere in your $PATH\n* add a `[versioneer]` section to your setup.cfg (see [Install](INSTALL.md))\n* run `versioneer install` in your source tree, commit the results\n* Verify version information with `python setup.py version`\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github's\n  \"tarball from tag\" feature\n* a release tarball, produced by \"setup.py sdist\", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. \"git describe\" (for checkouts), which knows\n  about recent \"tags\" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. \"myproject-1.2\" instead of just \"1.2\"), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n\"0.7-1-g574ab98-dirty\" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of \"574ab98\", and is \"dirty\" (it has\nuncommitted changes).\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a 'setup.py sdist' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nSee [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the \"outside\" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `['version']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project's version\n  string. The default \"pep440\" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the \"Styles\" section\n  below for alternative styles.\n\n* `['full-revisionid']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. \"1076c978a8d3cfc70f408fe5974aa6c092c949ac\".\n\n* `['date']`: Date and time of the latest `HEAD` commit. For Git, it is the\n  commit date in ISO 8601 format. This will be None if the date is not\n  available.\n\n* `['dirty']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `['error']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of \"unknown\".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an \"about\" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()['version']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, \"pep440\", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional \"local\nversion\" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example \"0.11+2.g1076c97.dirty\" indicates that the\ntree is like the \"1076c97\" commit but has uncommitted changes (\".dirty\"), and\nthat this commit is two revisions (\"+2\") beyond the \"0.11\" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. \"0.11\".\n\nOther styles are available. See [details.md](details.md) in the Versioneer\nsource tree for descriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of \"0+unknown\". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Known Limitations\n\nSome situations are known to cause problems for Versioneer. This details the\nmost significant ones. More can be found on Github\n[issues page](https://github.com/python-versioneer/python-versioneer/issues).\n\n### Subprojects\n\nVersioneer has limited support for source trees in which `setup.py` is not in\nthe root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\ntwo common reasons why `setup.py` might not be in the root:\n\n* Source trees which contain multiple subprojects, such as\n  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n  \"master\" and \"slave\" subprojects, each with their own `setup.py`,\n  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n  distributions (and upload multiple independently-installable tarballs).\n* Source trees whose main purpose is to contain a C library, but which also\n  provide bindings to Python (and perhaps other languages) in subdirectories.\n\nVersioneer will look for `.git` in parent directories, and most operations\nshould get the right version string. However `pip` and `setuptools` have bugs\nand implementation details which frequently cause `pip install .` from a\nsubproject directory to fail to find a correct version string (so it usually\ndefaults to `0+unknown`).\n\n`pip install --editable .` should work correctly. `setup.py install` might\nwork too.\n\nPip-8.1.1 is known to have this problem, but hopefully it will get fixed in\nsome later version.\n\n[Bug #38](https://github.com/python-versioneer/python-versioneer/issues/38) is tracking\nthis issue. The discussion in\n[PR #61](https://github.com/python-versioneer/python-versioneer/pull/61) describes the\nissue from the Versioneer side in more detail.\n[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\npip to let Versioneer work correctly.\n\nVersioneer-0.16 and earlier only looked for a `.git` directory next to the\n`setup.cfg`, so subprojects were completely unsupported with those releases.\n\n### Editable installs with setuptools <= 18.5\n\n`setup.py develop` and `pip install --editable .` allow you to install a\nproject into a virtualenv once, then continue editing the source code (and\ntest) without re-installing after every change.\n\n\"Entry-point scripts\" (`setup(entry_points={\"console_scripts\": ..})`) are a\nconvenient way to specify executable scripts that should be installed along\nwith the python package.\n\nThese both work as expected when using modern setuptools. When using\nsetuptools-18.5 or earlier, however, certain operations will cause\n`pkg_resources.DistributionNotFound` errors when running the entrypoint\nscript, which must be resolved by re-installing the package. This happens\nwhen the install happens with one version, then the egg_info data is\nregenerated while a different version is checked out. Many setup.py commands\ncause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\na different virtualenv), so this can be surprising.\n\n[Bug #83](https://github.com/python-versioneer/python-versioneer/issues/83) describes\nthis one, but upgrading to a newer version of setuptools should probably\nresolve it.\n\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg`, if necessary, to include any new configuration settings\n  indicated by the release notes. See [UPGRADING](./UPGRADING.md) for details.\n* re-run `versioneer install` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n## Similar projects\n\n* [setuptools_scm](https://github.com/pypa/setuptools_scm/) - a non-vendored build-time\n  dependency\n* [minver](https://github.com/jbweston/miniver) - a lightweight reimplementation of\n  versioneer\n* [versioningit](https://github.com/jwodder/versioningit) - a PEP 518-based setuptools\n  plugin\n\n## License\n\nTo make Versioneer easier to embed, all its code is dedicated to the public\ndomain. The `_version.py` that it creates is also in the public domain.\nSpecifically, both are released under the Creative Commons \"Public Domain\nDedication\" license (CC0-1.0), as described in\nhttps://creativecommons.org/publicdomain/zero/1.0/ .\n\n[pypi-image]: https://img.shields.io/pypi/v/versioneer.svg\n[pypi-url]: https://pypi.python.org/pypi/versioneer/\n[travis-image]:\nhttps://img.shields.io/travis/com/python-versioneer/python-versioneer.svg\n[travis-url]: https://travis-ci.com/github/python-versioneer/python-versioneer\n\n\"\"\"\n# pylint:disable=invalid-name,import-outside-toplevel,missing-function-docstring\n# pylint:disable=missing-class-docstring,too-many-branches,too-many-statements\n# pylint:disable=raise-missing-from,too-many-lines,too-many-locals,import-error\n# pylint:disable=too-few-public-methods,redefined-outer-name,consider-using-with\n# pylint:disable=attribute-defined-outside-init,too-many-arguments\n\nimport configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nfrom typing import Callable, Dict\nimport functools\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n\ndef get_root():\n    \"\"\"Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    \"\"\"\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, \"setup.py\")\n    versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow 'python path/to/setup.py COMMAND'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, \"setup.py\")\n        versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (\n            \"Versioneer was unable to run the project root directory. \"\n            \"Versioneer requires setup.py to be executed from \"\n            \"its immediate directory (like 'python setup.py COMMAND'), \"\n            \"or in a way that lets it use sys.argv[0] to find the root \"\n            \"(like 'python path/to/setup.py COMMAND').\"\n        )\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # \"versioneer\" may be imported multiple times, and python's shared\n        # module-import table will cache the first one. So we can't use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        my_path = os.path.realpath(os.path.abspath(__file__))\n        me_dir = os.path.normcase(os.path.splitext(my_path)[0])\n        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n        if me_dir != vsr_dir:\n            print(\"Warning: build in %s is using versioneer.py from %s\" % (os.path.dirname(my_path), versioneer_py))\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root):\n    \"\"\"Read the project setup.cfg file to determine Versioneer config.\"\"\"\n    # This might raise OSError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks \"VCS=\"). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    setup_cfg = os.path.join(root, \"setup.cfg\")\n    parser = configparser.ConfigParser()\n    with open(setup_cfg, \"r\") as cfg_file:\n        parser.read_file(cfg_file)\n    VCS = parser.get(\"versioneer\", \"VCS\")  # mandatory\n\n    # Dict-like interface for non-mandatory entries\n    section = parser[\"versioneer\"]\n\n    cfg = VersioneerConfig()\n    cfg.VCS = VCS\n    cfg.style = section.get(\"style\", \"\")\n    cfg.versionfile_source = section.get(\"versionfile_source\")\n    cfg.versionfile_build = section.get(\"versionfile_build\")\n    cfg.tag_prefix = section.get(\"tag_prefix\")\n    if cfg.tag_prefix in (\"''\", '\"\"', None):\n        cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = section.get(\"parentdir_prefix\")\n    cfg.verbose = section.get(\"verbose\")\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY: Dict[str, str] = {}\nHANDLERS: Dict[str, Dict[str, Callable]] = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        HANDLERS.setdefault(vcs, {})[method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    process = None\n\n    popen_kwargs = {}\n    if sys.platform == \"win32\":\n        # This hides the console window if pythonw.exe is used\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        popen_kwargs[\"startupinfo\"] = startupinfo\n\n    for command in commands:\n        try:\n            dispcmd = str([command] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            process = subprocess.Popen(\n                [command] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n                **popen_kwargs,\n            )\n            break\n        except OSError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %s\" % (commands,))\n        return None, None\n    stdout = process.communicate()[0].strip().decode()\n    if process.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % dispcmd)\n            print(\"stdout was %s\" % stdout)\n        return None, process.returncode\n    return stdout, process.returncode\n\n\nLONG_VERSION_PY[\n    \"git\"\n] = r'''\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.23 (https://github.com/python-versioneer/python-versioneer)\n\n\"\"\"Git implementation of _version.py.\"\"\"\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\nfrom typing import Callable, Dict\nimport functools\n\n\ndef get_keywords():\n    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = \"%(DOLLAR)sFormat:%%d%(DOLLAR)s\"\n    git_full = \"%(DOLLAR)sFormat:%%H%(DOLLAR)s\"\n    git_date = \"%(DOLLAR)sFormat:%%ci%(DOLLAR)s\"\n    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n\ndef get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    # these strings are filled in when 'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"%(STYLE)s\"\n    cfg.tag_prefix = \"%(TAG_PREFIX)s\"\n    cfg.parentdir_prefix = \"%(PARENTDIR_PREFIX)s\"\n    cfg.versionfile_source = \"%(VERSIONFILE_SOURCE)s\"\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\nLONG_VERSION_PY: Dict[str, str] = {}\nHANDLERS: Dict[str, Dict[str, Callable]] = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n    def decorate(f):\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    process = None\n\n    popen_kwargs = {}\n    if sys.platform == \"win32\":\n        # This hides the console window if pythonw.exe is used\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        popen_kwargs[\"startupinfo\"] = startupinfo\n\n    for command in commands:\n        try:\n            dispcmd = str([command] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            process = subprocess.Popen([command] + args, cwd=cwd, env=env,\n                                       stdout=subprocess.PIPE,\n                                       stderr=(subprocess.PIPE if hide_stderr\n                                               else None), **popen_kwargs)\n            break\n        except OSError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %%s\" %% dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %%s\" %% (commands,))\n        return None, None\n    stdout = process.communicate()[0].strip().decode()\n    if process.returncode != 0:\n        if verbose:\n            print(\"unable to run %%s (error)\" %% dispcmd)\n            print(\"stdout was %%s\" %% stdout)\n        return None, process.returncode\n    return stdout, process.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for _ in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\"version\": dirname[len(parentdir_prefix):],\n                    \"full-revisionid\": None,\n                    \"dirty\": False, \"error\": None, \"date\": None}\n        rootdirs.append(root)\n        root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\"Tried directories %%s but none started with prefix %%s\" %%\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs):\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        with open(versionfile_abs, \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(\"git_refnames =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"refnames\"] = mo.group(1)\n                if line.strip().startswith(\"git_full =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"full\"] = mo.group(1)\n                if line.strip().startswith(\"git_date =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"date\"] = mo.group(1)\n    except OSError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    \"\"\"Get version information from git keywords.\"\"\"\n    if \"refnames\" not in keywords:\n        raise NotThisMethod(\"Short version file found\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # Use only the last line.  Previous lines may contain GPG signature\n        # information.\n        date = date.splitlines()[-1]\n\n        # git-2.2.0 added \"%%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = {r for r in refs if re.search(r'\\d', r)}\n        if verbose:\n            print(\"discarding '%%s', no digits\" %% \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %%s\" %% \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            # Filter out refs that exactly match prefix or that don't start\n            # with a number once the prefix is stripped (mostly a concern\n            # when prefix is '')\n            if not re.match(r'\\d', r):\n                continue\n            if verbose:\n                print(\"picking %%s\" %% r)\n            return {\"version\": r,\n                    \"full-revisionid\": keywords[\"full\"].strip(),\n                    \"dirty\": False, \"error\": None,\n                    \"date\": date}\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\"version\": \"0+unknown\",\n            \"full-revisionid\": keywords[\"full\"].strip(),\n            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    # GIT_DIR can interfere with correct operation of Versioneer.\n    # It may be intended to be passed to the Versioneer-versioned project,\n    # but that should not change where we get our version from.\n    env = os.environ.copy()\n    env.pop(\"GIT_DIR\", None)\n    runner = functools.partial(runner, env=env)\n\n    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n                   hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %%s not under git control\" %% root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = runner(GITS, [\n        \"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\",\n        \"--match\", f\"{tag_prefix}[[:digit:]]*\"\n        ], cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"],\n                             cwd=root)\n    # --abbrev-ref was added in git-1.6.3\n    if rc != 0 or branch_name is None:\n        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n    branch_name = branch_name.strip()\n\n    if branch_name == \"HEAD\":\n        # If we aren't exactly on a branch, pick a branch which represents\n        # the current commit. If all else fails, we are on a branchless\n        # commit.\n        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n        # --contains was added in git-1.5.4\n        if rc != 0 or branches is None:\n            raise NotThisMethod(\"'git branch --contains' returned error\")\n        branches = branches.split(\"\\n\")\n\n        # Remove the first line if we're running detached\n        if \"(\" in branches[0]:\n            branches.pop(0)\n\n        # Strip off the leading \"* \" from the list of branches.\n        branches = [branch[2:] for branch in branches]\n        if \"master\" in branches:\n            branch_name = \"master\"\n        elif not branches:\n            branch_name = None\n        else:\n            # Pick the first branch that is returned. Good or bad.\n            branch_name = branches[0]\n\n    pieces[\"branch\"] = branch_name\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n        if not mo:\n            # unparsable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = (\"unable to parse git-describe output: '%%s'\"\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%%s' doesn't start with prefix '%%s'\"\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[\"error\"] = (\"tag '%%s' doesn't start with prefix '%%s'\"\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n        pieces[\"distance\"] = len(out.split())  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = runner(GITS, [\"show\", \"-s\", \"--format=%%ci\", \"HEAD\"], cwd=root)[0].strip()\n    # Use only the last line.  Previous lines may contain GPG signature\n    # information.\n    date = date.splitlines()[-1]\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces):\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n                                          pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_branch(pieces):\n    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n    (a feature branch will appear \"older\" than the master branch).\n\n    Exceptions:\n    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0\"\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n                                          pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef pep440_split_post(ver):\n    \"\"\"Split pep440 version string at the post-release segment.\n\n    Returns the release segments before the post-release and the\n    post-release version number (or -1 if no post-release segment is present).\n    \"\"\"\n    vc = str.split(ver, \".post\")\n    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n\n\ndef render_pep440_pre(pieces):\n    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post0.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"]:\n            # update the post release segment\n            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n            rendered = tag_version\n            if post_version is not None:\n                rendered += \".post%%d.dev%%d\" %% (post_version + 1, pieces[\"distance\"])\n            else:\n                rendered += \".post0.dev%%d\" %% (pieces[\"distance\"])\n        else:\n            # no commits, use the tag as the version\n            rendered = pieces[\"closest-tag\"]\n    else:\n        # exception #1\n        rendered = \"0.post0.dev%%d\" %% pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%%s\" %% pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%%s\" %% pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_post_branch(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%%s\" %% pieces[\"short\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+g%%s\" %% pieces[\"short\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces):\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%%s'\" %% style)\n\n    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n            \"dirty\": pieces[\"dirty\"], \"error\": None,\n            \"date\": pieces.get(\"date\")}\n\n\ndef get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for _ in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n                \"dirty\": None,\n                \"error\": \"unable to find root of source tree\",\n                \"date\": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to compute version\", \"date\": None}\n'''\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs):\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        with open(versionfile_abs, \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(\"git_refnames =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"refnames\"] = mo.group(1)\n                if line.strip().startswith(\"git_full =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"full\"] = mo.group(1)\n                if line.strip().startswith(\"git_date =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"date\"] = mo.group(1)\n    except OSError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    \"\"\"Get version information from git keywords.\"\"\"\n    if \"refnames\" not in keywords:\n        raise NotThisMethod(\"Short version file found\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # Use only the last line.  Previous lines may contain GPG signature\n        # information.\n        date = date.splitlines()[-1]\n\n        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = {r for r in refs if re.search(r\"\\d\", r)}\n        if verbose:\n            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            # Filter out refs that exactly match prefix or that don't start\n            # with a number once the prefix is stripped (mostly a concern\n            # when prefix is '')\n            if not re.match(r\"\\d\", r):\n                continue\n            if verbose:\n                print(\"picking %s\" % r)\n            return {\n                \"version\": r,\n                \"full-revisionid\": keywords[\"full\"].strip(),\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": date,\n            }\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": keywords[\"full\"].strip(),\n        \"dirty\": False,\n        \"error\": \"no suitable tags\",\n        \"date\": None,\n    }\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    # GIT_DIR can interfere with correct operation of Versioneer.\n    # It may be intended to be passed to the Versioneer-versioned project,\n    # but that should not change where we get our version from.\n    env = os.environ.copy()\n    env.pop(\"GIT_DIR\", None)\n    runner = functools.partial(runner, env=env)\n\n    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root, hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %s not under git control\" % root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = runner(\n        GITS, [\"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\", \"--match\", f\"{tag_prefix}[[:digit:]]*\"], cwd=root\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=root)\n    # --abbrev-ref was added in git-1.6.3\n    if rc != 0 or branch_name is None:\n        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n    branch_name = branch_name.strip()\n\n    if branch_name == \"HEAD\":\n        # If we aren't exactly on a branch, pick a branch which represents\n        # the current commit. If all else fails, we are on a branchless\n        # commit.\n        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n        # --contains was added in git-1.5.4\n        if rc != 0 or branches is None:\n            raise NotThisMethod(\"'git branch --contains' returned error\")\n        branches = branches.split(\"\\n\")\n\n        # Remove the first line if we're running detached\n        if \"(\" in branches[0]:\n            branches.pop(0)\n\n        # Strip off the leading \"* \" from the list of branches.\n        branches = [branch[2:] for branch in branches]\n        if \"master\" in branches:\n            branch_name = \"master\"\n        elif not branches:\n            branch_name = None\n        else:\n            # Pick the first branch that is returned. Good or bad.\n            branch_name = branches[0]\n\n    pieces[\"branch\"] = branch_name\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\"^(.+)-(\\d+)-g([0-9a-f]+)$\", git_describe)\n        if not mo:\n            # unparsable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = \"unable to parse git-describe output: '%s'\" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = \"tag '%s' doesn't start with prefix '%s'\" % (full_tag, tag_prefix)\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--left-right\"], cwd=root)\n        pieces[\"distance\"] = len(out.split())  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n    # Use only the last line.  Previous lines may contain GPG signature\n    # information.\n    date = date.splitlines()[-1]\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef do_vcs_install(versionfile_source, ipy):\n    \"\"\"Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n    files = [versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        my_path = __file__\n        if my_path.endswith(\".pyc\") or my_path.endswith(\".pyo\"):\n            my_path = os.path.splitext(my_path)[0] + \".py\"\n        versioneer_file = os.path.relpath(my_path)\n    except NameError:\n        versioneer_file = \"versioneer.py\"\n    files.append(versioneer_file)\n    present = False\n    try:\n        with open(\".gitattributes\", \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(versionfile_source):\n                    if \"export-subst\" in line.strip().split()[1:]:\n                        present = True\n                        break\n    except OSError:\n        pass\n    if not present:\n        with open(\".gitattributes\", \"a+\") as fobj:\n            fobj.write(f\"{versionfile_source} export-subst\\n\")\n        files.append(\".gitattributes\")\n    run_command(GITS, [\"add\", \"--\"] + files)\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for _ in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                \"version\": dirname[len(parentdir_prefix) :],\n                \"full-revisionid\": None,\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": None,\n            }\n        rootdirs.append(root)\n        root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\"Tried directories %s but none started with prefix %s\" % (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\nSHORT_VERSION_PY = \"\"\"\n# This file was generated by 'versioneer.py' (0.23) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\n\nversion_json = '''\n%s\n'''  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n\"\"\"\n\n\ndef versions_from_file(filename):\n    \"\"\"Try to determine the version from _version.py if present.\"\"\"\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except OSError:\n        raise NotThisMethod(\"unable to read _version.py\")\n    mo = re.search(r\"version_json = '''\\n(.*)'''  # END VERSION_JSON\", contents, re.M | re.S)\n    if not mo:\n        mo = re.search(r\"version_json = '''\\r\\n(.*)'''  # END VERSION_JSON\", contents, re.M | re.S)\n    if not mo:\n        raise NotThisMethod(\"no version_json in _version.py\")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename, versions):\n    \"\"\"Write the given version number to the given _version.py file.\"\"\"\n    os.unlink(filename)\n    contents = json.dumps(versions, sort_keys=True, indent=1, separators=(\",\", \": \"))\n    with open(filename, \"w\") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(\"set %s to '%s'\" % (filename, versions[\"version\"]))\n\n\ndef plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces):\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_branch(pieces):\n    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n    (a feature branch will appear \"older\" than the master branch).\n\n    Exceptions:\n    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0\"\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef pep440_split_post(ver):\n    \"\"\"Split pep440 version string at the post-release segment.\n\n    Returns the release segments before the post-release and the\n    post-release version number (or -1 if no post-release segment is present).\n    \"\"\"\n    vc = str.split(ver, \".post\")\n    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n\n\ndef render_pep440_pre(pieces):\n    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post0.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"]:\n            # update the post release segment\n            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n            rendered = tag_version\n            if post_version is not None:\n                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n            else:\n                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n        else:\n            # no commits, use the tag as the version\n            rendered = pieces[\"closest-tag\"]\n    else:\n        # exception #1\n        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_post_branch(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces):\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\n            \"version\": \"unknown\",\n            \"full-revisionid\": pieces.get(\"long\"),\n            \"dirty\": None,\n            \"error\": pieces[\"error\"],\n            \"date\": None,\n        }\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\n        \"version\": rendered,\n        \"full-revisionid\": pieces[\"long\"],\n        \"dirty\": pieces[\"dirty\"],\n        \"error\": None,\n        \"date\": pieces.get(\"date\"),\n    }\n\n\nclass VersioneerBadRootError(Exception):\n    \"\"\"The project root directory is unknown or missing key files.\"\"\"\n\n\ndef get_versions(verbose=False):\n    \"\"\"Get the project version from whatever source is available.\n\n    Returns dict with two keys: 'version' and 'full'.\n    \"\"\"\n    if \"versioneer\" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[\"versioneer\"]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, \"please set [versioneer]VCS= in setup.cfg\"\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, \"unrecognized VCS '%s'\" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert cfg.versionfile_source is not None, \"please set versioneer.versionfile_source\"\n    assert cfg.tag_prefix is not None, \"please set versioneer.tag_prefix\"\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. 'git\n    # describe'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by 'setup.py sdist',\n    # and for users of a tarball/zipball created by 'git archive' or github's\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(\"get_keywords\")\n    from_keywords_f = handlers.get(\"keywords\")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(\"got version from expanded keyword %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(\"got version from file %s %s\" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(\"pieces_from_vcs\")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(\"got version from VCS %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(\"got version from parentdir %s\" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(\"unable to compute version\")\n\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": None,\n        \"dirty\": None,\n        \"error\": \"unable to compute version\",\n        \"date\": None,\n    }\n\n\ndef get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]\n\n\ndef get_cmdclass(cmdclass=None):\n    \"\"\"Get the custom setuptools subclasses used by Versioneer.\n\n    If the package uses a different cmdclass (e.g. one from numpy), it\n    should be provide as an argument.\n    \"\"\"\n    if \"versioneer\" in sys.modules:\n        del sys.modules[\"versioneer\"]\n        # this fixes the \"python setup.py develop\" case (also 'install' and\n        # 'easy_install .'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A's setup.py imports A's Versioneer, leaving it in\n        # sys.modules by the time B's setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it's pre-build state, so the\n        # parent is protected against the child's \"import versioneer\". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent's versioneer too.\n        # Also see https://github.com/python-versioneer/python-versioneer/issues/52\n\n    cmds = {} if cmdclass is None else cmdclass.copy()\n\n    # we add \"version\" to setuptools\n    from setuptools import Command\n\n    class cmd_version(Command):\n        description = \"report generated version string\"\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(\"Version: %s\" % vers[\"version\"])\n            print(\" full-revisionid: %s\" % vers.get(\"full-revisionid\"))\n            print(\" dirty: %s\" % vers.get(\"dirty\"))\n            print(\" date: %s\" % vers.get(\"date\"))\n            if vers[\"error\"]:\n                print(\" error: %s\" % vers[\"error\"])\n\n    cmds[\"version\"] = cmd_version\n\n    # we override \"build_py\" in setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n    #  pip install:\n    #   copies source tree to a tempdir before running egg_info/etc\n    #   if .git isn't copied too, 'git describe' will fail\n    #   then does setup.py bdist_wheel, or sometimes setup.py install\n    #  setup.py egg_info -> ?\n\n    # pip install -e . and setuptool/editable_wheel will invoke build_py\n    # but the build_py command is not expected to copy any files.\n\n    # we override different \"build_py\" commands for both environments\n    if \"build_py\" in cmds:\n        _build_py = cmds[\"build_py\"]\n    else:\n        from setuptools.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            if getattr(self, \"editable_mode\", False):\n                # During editable installs `.py` and data files are\n                # not copied to build_lib\n                return\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib, cfg.versionfile_build)\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n    cmds[\"build_py\"] = cmd_build_py\n\n    if \"build_ext\" in cmds:\n        _build_ext = cmds[\"build_ext\"]\n    else:\n        from setuptools.command.build_ext import build_ext as _build_ext\n\n    class cmd_build_ext(_build_ext):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_ext.run(self)\n            if self.inplace:\n                # build_ext --inplace will only build extensions in\n                # build/lib<..> dir with no _version.py to write to.\n                # As in place builds will already have a _version.py\n                # in the module dir, we do not need to write one.\n                return\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            target_versionfile = os.path.join(self.build_lib, cfg.versionfile_build)\n            if not os.path.exists(target_versionfile):\n                print(\n                    f\"Warning: {target_versionfile} does not exist, skipping \"\n                    \"version update. This can happen if you are running build_ext \"\n                    \"without first running build_py.\"\n                )\n                return\n            print(\"UPDATING %s\" % target_versionfile)\n            write_to_version_file(target_versionfile, versions)\n\n    cmds[\"build_ext\"] = cmd_build_ext\n\n    if \"cx_Freeze\" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n\n        # nczeczulin reports that py2exe won't like the pep440-style string\n        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n        # setup(console=[{\n        #   \"version\": versioneer.get_version().split(\"+\", 1)[0], # FILEVERSION\n        #   \"product_version\": versioneer.get_version(),\n        #   ...\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, \"w\") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            \"DOLLAR\": \"$\",\n                            \"STYLE\": cfg.style,\n                            \"TAG_PREFIX\": cfg.tag_prefix,\n                            \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                            \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[\"build_exe\"] = cmd_build_exe\n        del cmds[\"build_py\"]\n\n    if \"py2exe\" in sys.modules:  # py2exe enabled?\n        from py2exe.distutils_buildexe import py2exe as _py2exe\n\n        class cmd_py2exe(_py2exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _py2exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, \"w\") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            \"DOLLAR\": \"$\",\n                            \"STYLE\": cfg.style,\n                            \"TAG_PREFIX\": cfg.tag_prefix,\n                            \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                            \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[\"py2exe\"] = cmd_py2exe\n\n    # sdist farms its file list building out to egg_info\n    if \"egg_info\" in cmds:\n        _sdist = cmds[\"egg_info\"]\n    else:\n        from setuptools.command.egg_info import egg_info as _egg_info\n\n    class cmd_egg_info(_egg_info):\n        def find_sources(self):\n            # egg_info.find_sources builds the manifest list and writes it\n            # in one shot\n            super().find_sources()\n\n            # Modify the filelist and normalize it\n            root = get_root()\n            cfg = get_config_from_root(root)\n            self.filelist.append(\"versioneer.py\")\n            if cfg.versionfile_source:\n                # There are rare cases where versionfile_source might not be\n                # included by default, so we must be explicit\n                self.filelist.append(cfg.versionfile_source)\n            self.filelist.sort()\n            self.filelist.remove_duplicates()\n\n            # The write method is hidden in the manifest_maker instance that\n            # generated the filelist and was thrown away\n            # We will instead replicate their final normalization (to unicode,\n            # and POSIX-style paths)\n            from setuptools import unicode_utils\n\n            normalized = [unicode_utils.filesys_decode(f).replace(os.sep, \"/\") for f in self.filelist.files]\n\n            manifest_filename = os.path.join(self.egg_info, \"SOURCES.txt\")\n            with open(manifest_filename, \"w\") as fobj:\n                fobj.write(\"\\n\".join(normalized))\n\n    cmds[\"egg_info\"] = cmd_egg_info\n\n    # we override different \"sdist\" commands for both environments\n    if \"sdist\" in cmds:\n        _sdist = cmds[\"sdist\"]\n    else:\n        from setuptools.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[\"version\"]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(\"UPDATING %s\" % target_versionfile)\n            write_to_version_file(target_versionfile, self._versioneer_generated_versions)\n\n    cmds[\"sdist\"] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = \"\"\"\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix =\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or 'python versioneer.py setup'.\n\"\"\"\n\nSAMPLE_CONFIG = \"\"\"\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run 'versioneer.py setup' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n\"\"\"\n\nOLD_SNIPPET = \"\"\"\nfrom ._version import get_versions\n__version__ = get_versions()['version']\ndel get_versions\n\"\"\"\n\nINIT_PY_SNIPPET = \"\"\"\nfrom . import {0}\n__version__ = {0}.get_versions()['version']\n\"\"\"\n\n\ndef do_setup():\n    \"\"\"Do main VCS-independent setup function for installing Versioneer.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:\n        if isinstance(e, (OSError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\", file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print(\" creating %s\" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, \"w\") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(\n            LONG\n            % {\n                \"DOLLAR\": \"$\",\n                \"STYLE\": cfg.style,\n                \"TAG_PREFIX\": cfg.tag_prefix,\n                \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n            }\n        )\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), \"__init__.py\")\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, \"r\") as f:\n                old = f.read()\n        except OSError:\n            old = \"\"\n        module = os.path.splitext(os.path.basename(cfg.versionfile_source))[0]\n        snippet = INIT_PY_SNIPPET.format(module)\n        if OLD_SNIPPET in old:\n            print(\" replacing boilerplate in %s\" % ipy)\n            with open(ipy, \"w\") as f:\n                f.write(old.replace(OLD_SNIPPET, snippet))\n        elif snippet not in old:\n            print(\" appending to %s\" % ipy)\n            with open(ipy, \"a\") as f:\n                f.write(snippet)\n        else:\n            print(\" %s unmodified\" % ipy)\n    else:\n        print(\" %s doesn't exist, ok\" % ipy)\n        ipy = None\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-subst keyword\n    # substitution.\n    do_vcs_install(cfg.versionfile_source, ipy)\n    return 0\n\n\ndef scan_setup_py():\n    \"\"\"Validate the contents of setup.py against Versioneer's expectations.\"\"\"\n    found = set()\n    setters = False\n    errors = 0\n    with open(\"setup.py\", \"r\") as f:\n        for line in f.readlines():\n            if \"import versioneer\" in line:\n                found.add(\"import\")\n            if \"versioneer.get_cmdclass()\" in line:\n                found.add(\"cmdclass\")\n            if \"versioneer.get_version()\" in line:\n                found.add(\"get_version\")\n            if \"versioneer.VCS\" in line:\n                setters = True\n            if \"versioneer.versionfile_source\" in line:\n                setters = True\n    if len(found) != 3:\n        print(\"\")\n        print(\"Your setup.py appears to be missing some important items\")\n        print(\"(but I might be wrong). Please make sure it has something\")\n        print(\"roughly like the following:\")\n        print(\"\")\n        print(\" import versioneer\")\n        print(\" setup( version=versioneer.get_version(),\")\n        print(\"        cmdclass=versioneer.get_cmdclass(),  ...)\")\n        print(\"\")\n        errors += 1\n    if setters:\n        print(\"You should remove lines like 'versioneer.VCS = ' and\")\n        print(\"'versioneer.versionfile_source = ' . This configuration\")\n        print(\"now lives in setup.cfg, and should be removed from setup.py\")\n        print(\"\")\n        errors += 1\n    return errors\n\n\nif __name__ == \"__main__\":\n    cmd = sys.argv[1]\n    if cmd == \"setup\":\n        errors = do_setup()\n        errors += scan_setup_py()\n        if errors:\n            sys.exit(1)\n"
        }
      ]
    }
  ]
}