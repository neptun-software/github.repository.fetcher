{
  "metadata": {
    "timestamp": 1736560879444,
    "page": 598,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "clovaai/donut",
      "stars": 5949,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.8564453125,
          "content": "core.*\n*.bin\n.nfs*\n.vscode/*\ndataset/*\nresult/*\nmisc/*\n!misc/*.png\n!dataset/.gitkeep\n!result/.gitkeep\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0517578125,
          "content": "MIT license\n\nCopyright (c) 2022-present NAVER Corp.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 8.72265625,
          "content": "Donut\nCopyright (c) 2022-present NAVER Corp.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\n--------------------------------------------------------------------------------------\n\nThis project contains subcomponents with separate copyright notices and license terms. \nYour use of the source code for these subcomponents is subject to the terms and conditions of the following licenses.\n\n=====\n\ngooglefonts/noto-fonts\nhttps://fonts.google.com/specimen/Noto+Sans\n\n\nCopyright 2018 The Noto Project Authors (github.com/googlei18n/noto-fonts)\n\nThis Font Software is licensed under the SIL Open Font License,\nVersion 1.1.\n\nThis license is copied below, and is also available with a FAQ at:\nhttp://scripts.sil.org/OFL\n\n-----------------------------------------------------------\nSIL OPEN FONT LICENSE Version 1.1 - 26 February 2007\n-----------------------------------------------------------\n\nPREAMBLE\nThe goals of the Open Font License (OFL) are to stimulate worldwide\ndevelopment of collaborative font projects, to support the font\ncreation efforts of academic and linguistic communities, and to\nprovide a free and open framework in which fonts may be shared and\nimproved in partnership with others.\n\nThe OFL allows the licensed fonts to be used, studied, modified and\nredistributed freely as long as they are not sold by themselves. The\nfonts, including any derivative works, can be bundled, embedded,\nredistributed and/or sold with any software provided that any reserved\nnames are not used by derivative works. The fonts and derivatives,\nhowever, cannot be released under any other type of license. The\nrequirement for fonts to remain under this license does not apply to\nany document created using the fonts or their derivatives.\n\nDEFINITIONS\n\"Font Software\" refers to the set of files released by the Copyright\nHolder(s) under this license and clearly marked as such. This may\ninclude source files, build scripts and documentation.\n\n\"Reserved Font Name\" refers to any names specified as such after the\ncopyright statement(s).\n\n\"Original Version\" refers to the collection of Font Software\ncomponents as distributed by the Copyright Holder(s).\n\n\"Modified Version\" refers to any derivative made by adding to,\ndeleting, or substituting -- in part or in whole -- any of the\ncomponents of the Original Version, by changing formats or by porting\nthe Font Software to a new environment.\n\n\"Author\" refers to any designer, engineer, programmer, technical\nwriter or other person who contributed to the Font Software.\n\nPERMISSION & CONDITIONS\nPermission is hereby granted, free of charge, to any person obtaining\na copy of the Font Software, to use, study, copy, merge, embed,\nmodify, redistribute, and sell modified and unmodified copies of the\nFont Software, subject to the following conditions:\n\n1) Neither the Font Software nor any of its individual components, in\nOriginal or Modified Versions, may be sold by itself.\n\n2) Original or Modified Versions of the Font Software may be bundled,\nredistributed and/or sold with any software, provided that each copy\ncontains the above copyright notice and this license. These can be\nincluded either as stand-alone text files, human-readable headers or\nin the appropriate machine-readable metadata fields within text or\nbinary files as long as those fields can be easily viewed by the user.\n\n3) No Modified Version of the Font Software may use the Reserved Font\nName(s) unless explicit written permission is granted by the\ncorresponding Copyright Holder. This restriction only applies to the\nprimary font name as presented to the users.\n\n4) The name(s) of the Copyright Holder(s) or the Author(s) of the Font\nSoftware shall not be used to promote, endorse or advertise any\nModified Version, except to acknowledge the contribution(s) of the\nCopyright Holder(s) and the Author(s) or with their explicit written\npermission.\n\n5) The Font Software, modified or unmodified, in part or in whole,\nmust be distributed entirely under this license, and must not be\ndistributed under any other license. The requirement for fonts to\nremain under this license does not apply to any document created using\nthe Font Software.\n\nTERMINATION\nThis license becomes null and void if any of the above conditions are\nnot met.\n\nDISCLAIMER\nTHE FONT SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT\nOF COPYRIGHT, PATENT, TRADEMARK, OR OTHER RIGHT. IN NO EVENT SHALL THE\nCOPYRIGHT HOLDER BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nINCLUDING ANY GENERAL, SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL\nDAMAGES, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF THE USE OR INABILITY TO USE THE FONT SOFTWARE OR FROM\nOTHER DEALINGS IN THE FONT SOFTWARE.\n\n=====\n\nhuggingface/transformers\nhttps://github.com/huggingface/transformers\n\n\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and limitations under the License.\n\n=====\n\nclovaai/synthtiger\nhttps://github.com/clovaai/synthtiger\n\n\nCopyright (c) 2021-present NAVER Corp.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\n=====\n\nrwightman/pytorch-image-models\nhttps://github.com/rwightman/pytorch-image-models\n\n\n   Copyright 2019 Ross Wightman\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n=====\n\nankush-me/SynthText\nhttps://github.com/ankush-me/SynthText\n\n\n   Copyright 2017, Ankush Gupta.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n=====\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.228515625,
          "content": "<div align=\"center\">\n    \n# Donut 🍩 : Document Understanding Transformer\n\n[![Paper](https://img.shields.io/badge/Paper-arxiv.2111.15664-red)](https://arxiv.org/abs/2111.15664)\n[![Conference](https://img.shields.io/badge/ECCV-2022-blue)](#how-to-cite)\n[![Demo](https://img.shields.io/badge/Demo-Gradio-brightgreen)](#demo)\n[![Demo](https://img.shields.io/badge/Demo-Colab-orange)](#demo)\n[![PyPI](https://img.shields.io/pypi/v/donut-python?color=green&label=pip%20install%20donut-python)](https://pypi.org/project/donut-python)\n[![Downloads](https://static.pepy.tech/personalized-badge/donut-python?period=total&units=international_system&left_color=grey&right_color=brightgreen&left_text=Downloads)](https://pepy.tech/project/donut-python)\n\nOfficial Implementation of Donut and SynthDoG | [Paper](https://arxiv.org/abs/2111.15664) | [Slide](https://docs.google.com/presentation/d/1gv3A7t4xpwwNdpxV_yeHzEOMy-exJCAz6AlAI9O5fS8/edit?usp=sharing) | [Poster](https://docs.google.com/presentation/d/1m1f8BbAm5vxPcqynn_MbFfmQAlHQIR5G72-hQUFS2sk/edit?usp=sharing)\n\n</div>\n\n## Introduction\n\n**Donut** 🍩, **Do**cume**n**t **u**nderstanding **t**ransformer, is a new method of document understanding that utilizes an OCR-free end-to-end Transformer model. Donut does not require off-the-shelf OCR engines/APIs, yet it shows state-of-the-art performances on various visual document understanding tasks, such as visual document classification or information extraction (a.k.a. document parsing). \nIn addition, we present **SynthDoG** 🐶, **Synth**etic **Do**cument **G**enerator, that helps the model pre-training to be flexible on various languages and domains.\n\nOur academic paper, which describes our method in detail and provides full experimental results and analyses, can be found here:<br>\n> [**OCR-free Document Understanding Transformer**](https://arxiv.org/abs/2111.15664).<br>\n> [Geewook Kim](https://geewook.kim), [Teakgyu Hong](https://dblp.org/pid/183/0952.html), [Moonbin Yim](https://github.com/moonbings), [JeongYeon Nam](https://github.com/long8v), [Jinyoung Park](https://github.com/jyp1111), [Jinyeong Yim](https://jinyeong.github.io), [Wonseok Hwang](https://scholar.google.com/citations?user=M13_WdcAAAAJ), [Sangdoo Yun](https://sangdooyun.github.io), [Dongyoon Han](https://dongyoonhan.github.io), [Seunghyun Park](https://scholar.google.com/citations?user=iowjmTwAAAAJ). In ECCV 2022.\n\n<img width=\"946\" alt=\"image\" src=\"misc/overview.png\">\n\n## Pre-trained Models and Web Demos\n\nGradio web demos are available! [![Demo](https://img.shields.io/badge/Demo-Gradio-brightgreen)](#demo) [![Demo](https://img.shields.io/badge/Demo-Colab-orange)](#demo)\n|:--:|\n|![image](misc/screenshot_gradio_demos.png)|\n- You can run the demo with `./app.py` file.\n- Sample images are available at `./misc` and more receipt images are available at [CORD dataset link](https://huggingface.co/datasets/naver-clova-ix/cord-v2).\n- Web demos are available from the links in the following table.\n- Note: We have updated the Google Colab demo (as of June 15, 2023) to ensure its proper working.\n\n|Task|Sec/Img|Score|Trained Model|<div id=\"demo\">Demo</div>|\n|---|---|---|---|---|\n| [CORD](https://github.com/clovaai/cord) (Document Parsing)   |   0.7 /<br> 0.7 /<br> 1.2   |  91.3 /<br> 91.1 /<br> 90.9    | [donut-base-finetuned-cord-v2](https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v2/tree/official) (1280) /<br> [donut-base-finetuned-cord-v1](https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v1/tree/official) (1280) /<br> [donut-base-finetuned-cord-v1-2560](https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v1-2560/tree/official) | [gradio space web demo](https://huggingface.co/spaces/naver-clova-ix/donut-base-finetuned-cord-v2),<br>[google colab demo (updated at 23.06.15)](https://colab.research.google.com/drive/1NMSqoIZ_l39wyRD7yVjw2FIuU2aglzJi?usp=sharing) |\n| [Train Ticket](https://github.com/beacandler/EATEN) (Document Parsing)   |   0.6   |  98.7    | [donut-base-finetuned-zhtrainticket](https://huggingface.co/naver-clova-ix/donut-base-finetuned-zhtrainticket/tree/official) | [google colab demo (updated at 23.06.15)](https://colab.research.google.com/drive/1YJBjllahdqNktXaBlq5ugPh1BCm8OsxI?usp=sharing) |\n| [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip) (Document Classification)     |  0.75   |   95.3      | [donut-base-finetuned-rvlcdip](https://huggingface.co/naver-clova-ix/donut-base-finetuned-rvlcdip/tree/official) | [gradio space web demo](https://huggingface.co/spaces/nielsr/donut-rvlcdip),<br>[google colab demo (updated at 23.06.15)](https://colab.research.google.com/drive/1iWOZHvao1W5xva53upcri5V6oaWT-P0O?usp=sharing) |\n| [DocVQA Task1](https://rrc.cvc.uab.es/?ch=17) (Document VQA) |  0.78       | 67.5 | [donut-base-finetuned-docvqa](https://huggingface.co/naver-clova-ix/donut-base-finetuned-docvqa/tree/official) | [gradio space web demo](https://huggingface.co/spaces/nielsr/donut-docvqa),<br>[google colab demo (updated at 23.06.15)](https://colab.research.google.com/drive/1oKieslZCulFiquequ62eMGc-ZWgay4X3?usp=sharing) |\n\nThe links to the pre-trained backbones are here:\n- [`donut-base`](https://huggingface.co/naver-clova-ix/donut-base/tree/official): trained with 64 A100 GPUs (~2.5 days), number of layers (encoder: {2,2,14,2}, decoder: 4), input size 2560x1920, swin window size 10, IIT-CDIP (11M) and SynthDoG (English, Chinese, Japanese, Korean, 0.5M x 4).\n- [`donut-proto`](https://huggingface.co/naver-clova-ix/donut-proto/tree/official): (preliminary model) trained with 8 V100 GPUs (~5 days), number of layers (encoder: {2,2,18,2}, decoder: 4), input size 2048x1536, swin window size 8, and SynthDoG (English, Japanese, Korean, 0.4M x 3).\n\nPlease see [our paper](#how-to-cite) for more details.\n\n## SynthDoG datasets\n\n![image](misc/sample_synthdog.png)\n\nThe links to the SynthDoG-generated datasets are here:\n\n- [`synthdog-en`](https://huggingface.co/datasets/naver-clova-ix/synthdog-en): English, 0.5M.\n- [`synthdog-zh`](https://huggingface.co/datasets/naver-clova-ix/synthdog-zh): Chinese, 0.5M.\n- [`synthdog-ja`](https://huggingface.co/datasets/naver-clova-ix/synthdog-ja): Japanese, 0.5M.\n- [`synthdog-ko`](https://huggingface.co/datasets/naver-clova-ix/synthdog-ko): Korean, 0.5M.\n\nTo generate synthetic datasets with our SynthDoG, please see `./synthdog/README.md` and [our paper](#how-to-cite) for details.\n\n## Updates\n\n**_2023-06-15_** We have updated all Google Colab demos to ensure its proper working.<br>\n**_2022-11-14_** New version 1.0.9 is released (`pip install donut-python --upgrade`). See [1.0.9 Release Notes](https://github.com/clovaai/donut/releases/tag/1.0.9).<br>\n**_2022-08-12_** Donut 🍩 is also available at [huggingface/transformers 🤗](https://huggingface.co/docs/transformers/main/en/model_doc/donut) (contributed by [@NielsRogge](https://github.com/NielsRogge)). `donut-python` loads the pre-trained weights from the `official` branch of the model repositories. See [1.0.5 Release Notes](https://github.com/clovaai/donut/releases/tag/1.0.5).<br>\n**_2022-08-05_** A well-executed hands-on tutorial on donut 🍩 is published at [Towards Data Science](https://towardsdatascience.com/ocr-free-document-understanding-with-donut-1acfbdf099be) (written by [@estaudere](https://github.com/estaudere)).<br>\n**_2022-07-20_** First Commit, We release our code, model weights, synthetic data and generator.\n\n## Software installation\n\n[![PyPI](https://img.shields.io/pypi/v/donut-python?color=green&label=pip%20install%20donut-python)](https://pypi.org/project/donut-python)\n[![Downloads](https://static.pepy.tech/personalized-badge/donut-python?period=total&units=international_system&left_color=grey&right_color=brightgreen&left_text=Downloads)](https://pepy.tech/project/donut-python)\n\n```bash\npip install donut-python\n```\n\nor clone this repository and install the dependencies:\n```bash\ngit clone https://github.com/clovaai/donut.git\ncd donut/\nconda create -n donut_official python=3.7\nconda activate donut_official\npip install .\n```\n\nWe tested [donut-python](https://pypi.org/project/donut-python/1.0.1) == 1.0.1 with:\n- [torch](https://github.com/pytorch/pytorch) == 1.11.0+cu113 \n- [torchvision](https://github.com/pytorch/vision) == 0.12.0+cu113\n- [pytorch-lightning](https://github.com/Lightning-AI/lightning) == 1.6.4\n- [transformers](https://github.com/huggingface/transformers) == 4.11.3\n- [timm](https://github.com/rwightman/pytorch-image-models) == 0.5.4\n\n**Note**: From several reported issues, we have noticed increased challenges in configuring the testing environment for `donut-python` due to recent updates in key dependency libraries. While we are actively working on a solution, we have updated the Google Colab demo (as of June 15, 2023) to ensure its proper working. For assistance, we encourage you to refer to the following demo links: [CORD Colab Demo](https://colab.research.google.com/drive/1NMSqoIZ_l39wyRD7yVjw2FIuU2aglzJi?usp=sharing), [Train Ticket Colab Demo](https://colab.research.google.com/drive/1YJBjllahdqNktXaBlq5ugPh1BCm8OsxI?usp=sharing), [RVL-CDIP Colab Demo](https://colab.research.google.com/drive/1iWOZHvao1W5xva53upcri5V6oaWT-P0O?usp=sharing), [DocVQA Colab Demo](https://colab.research.google.com/drive/1oKieslZCulFiquequ62eMGc-ZWgay4X3?usp=sharing).\n\n## Getting Started\n\n### Data\n\nThis repository assumes the following structure of dataset:\n```bash\n> tree dataset_name\ndataset_name\n├── test\n│   ├── metadata.jsonl\n│   ├── {image_path0}\n│   ├── {image_path1}\n│             .\n│             .\n├── train\n│   ├── metadata.jsonl\n│   ├── {image_path0}\n│   ├── {image_path1}\n│             .\n│             .\n└── validation\n    ├── metadata.jsonl\n    ├── {image_path0}\n    ├── {image_path1}\n              .\n              .\n\n> cat dataset_name/test/metadata.jsonl\n{\"file_name\": {image_path0}, \"ground_truth\": \"{\\\"gt_parse\\\": {ground_truth_parse}, ... {other_metadata_not_used} ... }\"}\n{\"file_name\": {image_path1}, \"ground_truth\": \"{\\\"gt_parse\\\": {ground_truth_parse}, ... {other_metadata_not_used} ... }\"}\n     .\n     .\n```\n\n- The structure of `metadata.jsonl` file is in [JSON Lines text format](https://jsonlines.org), i.e., `.jsonl`. Each line consists of\n  - `file_name` : relative path to the image file.\n  - `ground_truth` : string format (json dumped), the dictionary contains either `gt_parse` or `gt_parses`. Other fields (metadata) can be added to the dictionary but will not be used.\n- `donut` interprets all tasks as a JSON prediction problem. As a result, all `donut` model training share a same pipeline. For training and inference, the only thing to do is preparing `gt_parse` or `gt_parses` for the task in format described below.\n\n#### For Document Classification\nThe `gt_parse` follows the format of `{\"class\" : {class_name}}`, for example, `{\"class\" : \"scientific_report\"}` or `{\"class\" : \"presentation\"}`.\n- Google colab demo is available [here](https://colab.research.google.com/drive/1xUDmLqlthx8A8rWKLMSLThZ7oeRJkDuU?usp=sharing).\n- Gradio web demo is available [here](https://huggingface.co/spaces/nielsr/donut-rvlcdip).\n\n#### For Document Information Extraction\nThe `gt_parse` is a JSON object that contains full information of the document image, for example, the JSON object for a receipt may look like `{\"menu\" : [{\"nm\": \"ICE BLACKCOFFEE\", \"cnt\": \"2\", ...}, ...], ...}`.\n- More examples are available at [CORD dataset](https://huggingface.co/datasets/naver-clova-ix/cord-v2).\n- Google colab demo is available [here](https://colab.research.google.com/drive/1o07hty-3OQTvGnc_7lgQFLvvKQuLjqiw?usp=sharing).\n- Gradio web demo is available [here](https://huggingface.co/spaces/naver-clova-ix/donut-base-finetuned-cord-v2).\n\n#### For Document Visual Question Answering\nThe `gt_parses` follows the format of `[{\"question\" : {question_sentence}, \"answer\" : {answer_candidate_1}}, {\"question\" : {question_sentence}, \"answer\" : {answer_candidate_2}}, ...]`, for example, `[{\"question\" : \"what is the model name?\", \"answer\" : \"donut\"}, {\"question\" : \"what is the model name?\", \"answer\" : \"document understanding transformer\"}]`.\n- DocVQA Task1 has multiple answers, hence `gt_parses` should be a list of dictionary that contains a pair of question and answer.\n- Google colab demo is available [here](https://colab.research.google.com/drive/1Z4WG8Wunj3HE0CERjt608ALSgSzRC9ig?usp=sharing).\n- Gradio web demo is available [here](https://huggingface.co/spaces/nielsr/donut-docvqa).\n\n#### For (Pseudo) Text Reading Task\nThe `gt_parse` looks like `{\"text_sequence\" : \"word1 word2 word3 ... \"}`\n- This task is also a pre-training task of Donut model.\n- You can use our **SynthDoG** 🐶 to generate synthetic images for the text reading task with proper `gt_parse`. See `./synthdog/README.md` for details.\n\n### Training\n\nThis is the configuration of Donut model training on [CORD](https://github.com/clovaai/cord) dataset used in our experiment. \nWe ran this with a single NVIDIA A100 GPU.\n\n```bash\npython train.py --config config/train_cord.yaml \\\n                --pretrained_model_name_or_path \"naver-clova-ix/donut-base\" \\\n                --dataset_name_or_paths '[\"naver-clova-ix/cord-v2\"]' \\\n                --exp_version \"test_experiment\"    \n  .\n  .                                                                                                                                                                                                                                         \nPrediction: <s_menu><s_nm>Lemon Tea (L)</s_nm><s_cnt>1</s_cnt><s_price>25.000</s_price></s_menu><s_total><s_total_price>25.000</s_total_price><s_cashprice>30.000</s_cashprice><s_changeprice>5.000</s_changeprice></s_total>\nAnswer: <s_menu><s_nm>Lemon Tea (L)</s_nm><s_cnt>1</s_cnt><s_price>25.000</s_price></s_menu><s_total><s_total_price>25.000</s_total_price><s_cashprice>30.000</s_cashprice><s_changeprice>5.000</s_changeprice></s_total>\nNormed ED: 0.0\nPrediction: <s_menu><s_nm>Hulk Topper Package</s_nm><s_cnt>1</s_cnt><s_price>100.000</s_price></s_menu><s_total><s_total_price>100.000</s_total_price><s_cashprice>100.000</s_cashprice><s_changeprice>0</s_changeprice></s_total>\nAnswer: <s_menu><s_nm>Hulk Topper Package</s_nm><s_cnt>1</s_cnt><s_price>100.000</s_price></s_menu><s_total><s_total_price>100.000</s_total_price><s_cashprice>100.000</s_cashprice><s_changeprice>0</s_changeprice></s_total>\nNormed ED: 0.0\nPrediction: <s_menu><s_nm>Giant Squid</s_nm><s_cnt>x 1</s_cnt><s_price>Rp. 39.000</s_price><s_sub><s_nm>C.Finishing - Cut</s_nm><s_price>Rp. 0</s_price><sep/><s_nm>B.Spicy Level - Extreme Hot Rp. 0</s_price></s_sub><sep/><s_nm>A.Flavour - Salt & Pepper</s_nm><s_price>Rp. 0</s_price></s_sub></s_menu><s_sub_total><s_subtotal_price>Rp. 39.000</s_subtotal_price></s_sub_total><s_total><s_total_price>Rp. 39.000</s_total_price><s_cashprice>Rp. 50.000</s_cashprice><s_changeprice>Rp. 11.000</s_changeprice></s_total>\nAnswer: <s_menu><s_nm>Giant Squid</s_nm><s_cnt>x1</s_cnt><s_price>Rp. 39.000</s_price><s_sub><s_nm>C.Finishing - Cut</s_nm><s_price>Rp. 0</s_price><sep/><s_nm>B.Spicy Level - Extreme Hot</s_nm><s_price>Rp. 0</s_price><sep/><s_nm>A.Flavour- Salt & Pepper</s_nm><s_price>Rp. 0</s_price></s_sub></s_menu><s_sub_total><s_subtotal_price>Rp. 39.000</s_subtotal_price></s_sub_total><s_total><s_total_price>Rp. 39.000</s_total_price><s_cashprice>Rp. 50.000</s_cashprice><s_changeprice>Rp. 11.000</s_changeprice></s_total>\nNormed ED: 0.039603960396039604                                                                                                                                  \nEpoch 29: 100%|█████████████| 200/200 [01:49<00:00,  1.82it/s, loss=0.00327, exp_name=train_cord, exp_version=test_experiment]\n```\n\nSome important arguments:\n\n- `--config` : config file path for model training.\n- `--pretrained_model_name_or_path` : string format, model name in Hugging Face modelhub or local path.\n- `--dataset_name_or_paths` : string format (json dumped), list of dataset names in Hugging Face datasets or local paths.\n- `--result_path` : file path to save model outputs/artifacts.\n- `--exp_version` : used for experiment versioning. The output files are saved at `{result_path}/{exp_version}/*`\n\n### Test\n\nWith the trained model, test images and ground truth parses, you can get inference results and accuracy scores.\n\n```bash\npython test.py --dataset_name_or_path naver-clova-ix/cord-v2 --pretrained_model_name_or_path ./result/train_cord/test_experiment --save_path ./result/output.json\n100%|█████████████| 100/100 [00:35<00:00,  2.80it/s]\nTotal number of samples: 100, Tree Edit Distance (TED) based accuracy score: 0.9129639764131697, F1 accuracy score: 0.8406020841373987\n```\n\nSome important arguments:\n\n- `--dataset_name_or_path` : string format, the target dataset name in Hugging Face datasets or local path.\n- `--pretrained_model_name_or_path` : string format, the model name in Hugging Face modelhub or local path.\n- `--save_path`: file path to save predictions and scores.\n\n## How to Cite\nIf you find this work useful to you, please cite:\n```bibtex\n@inproceedings{kim2022donut,\n  title     = {OCR-Free Document Understanding Transformer},\n  author    = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},\n  booktitle = {European Conference on Computer Vision (ECCV)},\n  year      = {2022}\n}\n```\n\n## License\n\n```\nMIT license\n\nCopyright (c) 2022-present NAVER Corp.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n```\n"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 2.0751953125,
          "content": "\"\"\"\nDonut\nCopyright (c) 2022-present NAVER Corp.\nMIT License\n\"\"\"\nimport argparse\n\nimport gradio as gr\nimport torch\nfrom PIL import Image\n\nfrom donut import DonutModel\n\n\ndef demo_process_vqa(input_img, question):\n    global pretrained_model, task_prompt, task_name\n    input_img = Image.fromarray(input_img)\n    user_prompt = task_prompt.replace(\"{user_input}\", question)\n    output = pretrained_model.inference(input_img, prompt=user_prompt)[\"predictions\"][0]\n    return output\n\n\ndef demo_process(input_img):\n    global pretrained_model, task_prompt, task_name\n    input_img = Image.fromarray(input_img)\n    output = pretrained_model.inference(image=input_img, prompt=task_prompt)[\"predictions\"][0]\n    return output\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--task\", type=str, default=\"docvqa\")\n    parser.add_argument(\"--pretrained_path\", type=str, default=\"naver-clova-ix/donut-base-finetuned-docvqa\")\n    parser.add_argument(\"--port\", type=int, default=None)\n    parser.add_argument(\"--url\", type=str, default=None)\n    parser.add_argument(\"--sample_img_path\", type=str)\n    args, left_argv = parser.parse_known_args()\n\n    task_name = args.task\n    if \"docvqa\" == task_name:\n        task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n    else:  # rvlcdip, cord, ...\n        task_prompt = f\"<s_{task_name}>\"\n\n    example_sample = []\n    if args.sample_img_path:\n        example_sample.append(args.sample_img_path)\n\n    pretrained_model = DonutModel.from_pretrained(args.pretrained_path)\n\n    if torch.cuda.is_available():\n        pretrained_model.half()\n        device = torch.device(\"cuda\")\n        pretrained_model.to(device)\n\n    pretrained_model.eval()\n\n    demo = gr.Interface(\n        fn=demo_process_vqa if task_name == \"docvqa\" else demo_process,\n        inputs=[\"image\", \"text\"] if task_name == \"docvqa\" else \"image\",\n        outputs=\"json\",\n        title=f\"Donut 🍩 demonstration for `{task_name}` task\",\n        examples=[example_sample] if example_sample else None,\n    )\n    demo.launch(server_name=args.url, server_port=args.port)\n"
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "donut",
          "type": "tree",
          "content": null
        },
        {
          "name": "lightning_module.py",
          "type": "blob",
          "size": 7.58984375,
          "content": "\"\"\"\nDonut\nCopyright (c) 2022-present NAVER Corp.\nMIT License\n\"\"\"\nimport math\nimport random\nimport re\nfrom pathlib import Path\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom nltk import edit_distance\nfrom pytorch_lightning.utilities import rank_zero_only\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.utils.data import DataLoader\n\nfrom donut import DonutConfig, DonutModel\n\n\nclass DonutModelPLModule(pl.LightningModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n\n        if self.config.get(\"pretrained_model_name_or_path\", False):\n            self.model = DonutModel.from_pretrained(\n                self.config.pretrained_model_name_or_path,\n                input_size=self.config.input_size,\n                max_length=self.config.max_length,\n                align_long_axis=self.config.align_long_axis,\n                ignore_mismatched_sizes=True,\n            )\n        else:\n            self.model = DonutModel(\n                config=DonutConfig(\n                    input_size=self.config.input_size,\n                    max_length=self.config.max_length,\n                    align_long_axis=self.config.align_long_axis,\n                    # with DonutConfig, the architecture customization is available, e.g.,\n                    # encoder_layer=[2,2,14,2], decoder_layer=4, ...\n                )\n            )\n        self.pytorch_lightning_version_is_1 = int(pl.__version__[0]) < 2\n        self.num_of_loaders = len(self.config.dataset_name_or_paths)\n\n    def training_step(self, batch, batch_idx):\n        image_tensors, decoder_input_ids, decoder_labels = list(), list(), list()\n        for batch_data in batch:\n            image_tensors.append(batch_data[0])\n            decoder_input_ids.append(batch_data[1][:, :-1])\n            decoder_labels.append(batch_data[2][:, 1:])\n        image_tensors = torch.cat(image_tensors)\n        decoder_input_ids = torch.cat(decoder_input_ids)\n        decoder_labels = torch.cat(decoder_labels)\n        loss = self.model(image_tensors, decoder_input_ids, decoder_labels)[0]\n        self.log_dict({\"train_loss\": loss}, sync_dist=True)\n        if not self.pytorch_lightning_version_is_1:\n            self.log('loss', loss, prog_bar=True)\n        return loss\n\n    def on_validation_epoch_start(self) -> None:\n        super().on_validation_epoch_start()\n        self.validation_step_outputs = [[] for _ in range(self.num_of_loaders)]\n        return\n\n    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n        image_tensors, decoder_input_ids, prompt_end_idxs, answers = batch\n        decoder_prompts = pad_sequence(\n            [input_id[: end_idx + 1] for input_id, end_idx in zip(decoder_input_ids, prompt_end_idxs)],\n            batch_first=True,\n        )\n\n        preds = self.model.inference(\n            image_tensors=image_tensors,\n            prompt_tensors=decoder_prompts,\n            return_json=False,\n            return_attentions=False,\n        )[\"predictions\"]\n\n        scores = list()\n        for pred, answer in zip(preds, answers):\n            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n            answer = re.sub(r\"<.*?>\", \"\", answer, count=1)\n            answer = answer.replace(self.model.decoder.tokenizer.eos_token, \"\")\n            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n\n            if self.config.get(\"verbose\", False) and len(scores) == 1:\n                self.print(f\"Prediction: {pred}\")\n                self.print(f\"    Answer: {answer}\")\n                self.print(f\" Normed ED: {scores[0]}\")\n\n        self.validation_step_outputs[dataloader_idx].append(scores)\n\n        return scores\n\n    def on_validation_epoch_end(self):\n        assert len(self.validation_step_outputs) == self.num_of_loaders\n        cnt = [0] * self.num_of_loaders\n        total_metric = [0] * self.num_of_loaders\n        val_metric = [0] * self.num_of_loaders\n        for i, results in enumerate(self.validation_step_outputs):\n            for scores in results:\n                cnt[i] += len(scores)\n                total_metric[i] += np.sum(scores)\n            val_metric[i] = total_metric[i] / cnt[i]\n            val_metric_name = f\"val_metric_{i}th_dataset\"\n            self.log_dict({val_metric_name: val_metric[i]}, sync_dist=True)\n        self.log_dict({\"val_metric\": np.sum(total_metric) / np.sum(cnt)}, sync_dist=True)\n\n    def configure_optimizers(self):\n\n        max_iter = None\n\n        if int(self.config.get(\"max_epochs\", -1)) > 0:\n            assert len(self.config.train_batch_sizes) == 1, \"Set max_epochs only if the number of datasets is 1\"\n            max_iter = (self.config.max_epochs * self.config.num_training_samples_per_epoch) / (\n                self.config.train_batch_sizes[0] * torch.cuda.device_count() * self.config.get(\"num_nodes\", 1)\n            )\n\n        if int(self.config.get(\"max_steps\", -1)) > 0:\n            max_iter = min(self.config.max_steps, max_iter) if max_iter is not None else self.config.max_steps\n\n        assert max_iter is not None\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.lr)\n        scheduler = {\n            \"scheduler\": self.cosine_scheduler(optimizer, max_iter, self.config.warmup_steps),\n            \"name\": \"learning_rate\",\n            \"interval\": \"step\",\n        }\n        return [optimizer], [scheduler]\n\n    @staticmethod\n    def cosine_scheduler(optimizer, training_steps, warmup_steps):\n        def lr_lambda(current_step):\n            if current_step < warmup_steps:\n                return current_step / max(1, warmup_steps)\n            progress = current_step - warmup_steps\n            progress /= max(1, training_steps - warmup_steps)\n            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n\n        return LambdaLR(optimizer, lr_lambda)\n\n    @rank_zero_only\n    def on_save_checkpoint(self, checkpoint):\n        save_path = Path(self.config.result_path) / self.config.exp_name / self.config.exp_version\n        self.model.save_pretrained(save_path)\n        self.model.decoder.tokenizer.save_pretrained(save_path)\n\n\nclass DonutDataPLModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.train_batch_sizes = self.config.train_batch_sizes\n        self.val_batch_sizes = self.config.val_batch_sizes\n        self.train_datasets = []\n        self.val_datasets = []\n        self.g = torch.Generator()\n        self.g.manual_seed(self.config.seed)\n\n    def train_dataloader(self):\n        loaders = list()\n        for train_dataset, batch_size in zip(self.train_datasets, self.train_batch_sizes):\n            loaders.append(\n                DataLoader(\n                    train_dataset,\n                    batch_size=batch_size,\n                    num_workers=self.config.num_workers,\n                    pin_memory=True,\n                    worker_init_fn=self.seed_worker,\n                    generator=self.g,\n                    shuffle=True,\n                )\n            )\n        return loaders\n\n    def val_dataloader(self):\n        loaders = list()\n        for val_dataset, batch_size in zip(self.val_datasets, self.val_batch_sizes):\n            loaders.append(\n                DataLoader(\n                    val_dataset,\n                    batch_size=batch_size,\n                    pin_memory=True,\n                    shuffle=False,\n                )\n            )\n        return loaders\n\n    @staticmethod\n    def seed_worker(wordker_id):\n        worker_seed = torch.initial_seed() % 2 ** 32\n        np.random.seed(worker_seed)\n        random.seed(worker_seed)\n"
        },
        {
          "name": "misc",
          "type": "tree",
          "content": null
        },
        {
          "name": "result",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.220703125,
          "content": "\"\"\"\nDonut\nCopyright (c) 2022-present NAVER Corp.\nMIT License\n\"\"\"\nimport os\nfrom setuptools import find_packages, setup\n\nROOT = os.path.abspath(os.path.dirname(__file__))\n\n\ndef read_version():\n    data = {}\n    path = os.path.join(ROOT, \"donut\", \"_version.py\")\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        exec(f.read(), data)\n    return data[\"__version__\"]\n\n\ndef read_long_description():\n    path = os.path.join(ROOT, \"README.md\")\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        text = f.read()\n    return text\n\n\nsetup(\n    name=\"donut-python\",\n    version=read_version(),\n    description=\"OCR-free Document Understanding Transformer\",\n    long_description=read_long_description(),\n    long_description_content_type=\"text/markdown\",\n    author=\"Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park\",\n    author_email=\"gwkim.rsrch@gmail.com\",\n    url=\"https://github.com/clovaai/donut\",\n    license=\"MIT\",\n    packages=find_packages(\n        exclude=[\n            \"config\",\n            \"dataset\",\n            \"misc\",\n            \"result\",\n            \"synthdog\",\n            \"app.py\",\n            \"lightning_module.py\",\n            \"README.md\",\n            \"train.py\",\n            \"test.py\",\n        ]\n    ),\n    python_requires=\">=3.7\",\n    install_requires=[\n        \"transformers>=4.11.3\",\n        \"timm\",\n        \"datasets[vision]\",\n        \"pytorch-lightning>=1.6.4\",\n        \"nltk\",\n        \"sentencepiece\",\n        \"zss\",\n        \"sconf>=0.2.3\",\n    ],\n    classifiers=[\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Information Technology\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Software Development :: Libraries\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n    ],\n)\n"
        },
        {
          "name": "synthdog",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 3.0888671875,
          "content": "\"\"\"\nDonut\nCopyright (c) 2022-present NAVER Corp.\nMIT License\n\"\"\"\nimport argparse\nimport json\nimport os\nimport re\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom PIL import Image\nfrom tqdm import tqdm\n\nfrom donut import DonutModel, JSONParseEvaluator, load_json, save_json\n\n\ndef test(args):\n    pretrained_model = DonutModel.from_pretrained(args.pretrained_model_name_or_path)\n\n    if torch.cuda.is_available():\n        pretrained_model.half()\n        pretrained_model.to(\"cuda\")\n\n    pretrained_model.eval()\n\n    if args.save_path:\n        os.makedirs(os.path.dirname(args.save_path), exist_ok=True)\n\n    predictions = []\n    ground_truths = []\n    accs = []\n\n    evaluator = JSONParseEvaluator()\n    dataset = load_dataset(args.dataset_name_or_path, split=args.split)\n\n    for idx, sample in tqdm(enumerate(dataset), total=len(dataset)):\n        ground_truth = json.loads(sample[\"ground_truth\"])\n\n        if args.task_name == \"docvqa\":\n            output = pretrained_model.inference(\n                image=sample[\"image\"],\n                prompt=f\"<s_{args.task_name}><s_question>{ground_truth['gt_parses'][0]['question'].lower()}</s_question><s_answer>\",\n            )[\"predictions\"][0]\n        else:\n            output = pretrained_model.inference(image=sample[\"image\"], prompt=f\"<s_{args.task_name}>\")[\"predictions\"][0]\n\n        if args.task_name == \"rvlcdip\":\n            gt = ground_truth[\"gt_parse\"]\n            score = float(output[\"class\"] == gt[\"class\"])\n        elif args.task_name == \"docvqa\":\n            # Note: we evaluated the model on the official website.\n            # In this script, an exact-match based score will be returned instead\n            gt = ground_truth[\"gt_parses\"]\n            answers = set([qa_parse[\"answer\"] for qa_parse in gt])\n            score = float(output[\"answer\"] in answers)\n        else:\n            gt = ground_truth[\"gt_parse\"]\n            score = evaluator.cal_acc(output, gt)\n\n        accs.append(score)\n\n        predictions.append(output)\n        ground_truths.append(gt)\n\n    scores = {\n        \"ted_accuracies\": accs,\n        \"ted_accuracy\": np.mean(accs),\n        \"f1_accuracy\": evaluator.cal_f1(predictions, ground_truths),\n    }\n    print(\n        f\"Total number of samples: {len(accs)}, Tree Edit Distance (TED) based accuracy score: {scores['ted_accuracy']}, F1 accuracy score: {scores['f1_accuracy']}\"\n    )\n\n    if args.save_path:\n        scores[\"predictions\"] = predictions\n        scores[\"ground_truths\"] = ground_truths\n        save_json(args.save_path, scores)\n\n    return predictions\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--pretrained_model_name_or_path\", type=str)\n    parser.add_argument(\"--dataset_name_or_path\", type=str)\n    parser.add_argument(\"--split\", type=str, default=\"test\")\n    parser.add_argument(\"--task_name\", type=str, default=None)\n    parser.add_argument(\"--save_path\", type=str, default=None)\n    args, left_argv = parser.parse_known_args()\n\n    if args.task_name is None:\n        args.task_name = os.path.basename(args.dataset_name_or_path)\n\n    predictions = test(args)\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 6.1376953125,
          "content": "\"\"\"\nDonut\nCopyright (c) 2022-present NAVER Corp.\nMIT License\n\"\"\"\nimport argparse\nimport datetime\nimport json\nimport os\nimport random\nfrom io import BytesIO\nfrom os.path import basename\nfrom pathlib import Path\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.loggers.tensorboard import TensorBoardLogger\nfrom pytorch_lightning.plugins import CheckpointIO\nfrom pytorch_lightning.utilities import rank_zero_only\nfrom sconf import Config\n\nfrom donut import DonutDataset\nfrom lightning_module import DonutDataPLModule, DonutModelPLModule\n\n\nclass CustomCheckpointIO(CheckpointIO):\n    def save_checkpoint(self, checkpoint, path, storage_options=None):\n        del checkpoint[\"state_dict\"]\n        torch.save(checkpoint, path)\n\n    def load_checkpoint(self, path, storage_options=None):\n        checkpoint = torch.load(path + \"artifacts.ckpt\")\n        state_dict = torch.load(path + \"pytorch_model.bin\")\n        checkpoint[\"state_dict\"] = {\"model.\" + key: value for key, value in state_dict.items()}\n        return checkpoint\n\n    def remove_checkpoint(self, path) -> None:\n        return super().remove_checkpoint(path)\n\n\n@rank_zero_only\ndef save_config_file(config, path):\n    if not Path(path).exists():\n        os.makedirs(path)\n    save_path = Path(path) / \"config.yaml\"\n    print(config.dumps())\n    with open(save_path, \"w\") as f:\n        f.write(config.dumps(modified_color=None, quote_str=True))\n        print(f\"Config is saved at {save_path}\")\n\n\nclass ProgressBar(pl.callbacks.TQDMProgressBar):\n    def __init__(self, config):\n        super().__init__()\n        self.enable = True\n        self.config = config\n\n    def disable(self):\n        self.enable = False\n\n    def get_metrics(self, trainer, model):\n        items = super().get_metrics(trainer, model)\n        items.pop(\"v_num\", None)\n        items[\"exp_name\"] = f\"{self.config.get('exp_name', '')}\"\n        items[\"exp_version\"] = f\"{self.config.get('exp_version', '')}\"\n        return items\n\n\ndef set_seed(seed):\n    pytorch_lightning_version = int(pl.__version__[0])\n    if pytorch_lightning_version < 2:\n        pl.utilities.seed.seed_everything(seed, workers=True)\n    else:\n        import lightning_fabric\n        lightning_fabric.utilities.seed.seed_everything(seed, workers=True)\n\n\ndef train(config):\n    set_seed(config.get(\"seed\", 42))\n\n    model_module = DonutModelPLModule(config)\n    data_module = DonutDataPLModule(config)\n\n    # add datasets to data_module\n    datasets = {\"train\": [], \"validation\": []}\n    for i, dataset_name_or_path in enumerate(config.dataset_name_or_paths):\n        task_name = os.path.basename(dataset_name_or_path)  # e.g., cord-v2, docvqa, rvlcdip, ...\n        \n        # add categorical special tokens (optional)\n        if task_name == \"rvlcdip\":\n            model_module.model.decoder.add_special_tokens([\n                \"<advertisement/>\", \"<budget/>\", \"<email/>\", \"<file_folder/>\", \n                \"<form/>\", \"<handwritten/>\", \"<invoice/>\", \"<letter/>\", \n                \"<memo/>\", \"<news_article/>\", \"<presentation/>\", \"<questionnaire/>\", \n                \"<resume/>\", \"<scientific_publication/>\", \"<scientific_report/>\", \"<specification/>\"\n            ])\n        if task_name == \"docvqa\":\n            model_module.model.decoder.add_special_tokens([\"<yes/>\", \"<no/>\"])\n            \n        for split in [\"train\", \"validation\"]:\n            datasets[split].append(\n                DonutDataset(\n                    dataset_name_or_path=dataset_name_or_path,\n                    donut_model=model_module.model,\n                    max_length=config.max_length,\n                    split=split,\n                    task_start_token=config.task_start_tokens[i]\n                    if config.get(\"task_start_tokens\", None)\n                    else f\"<s_{task_name}>\",\n                    prompt_end_token=\"<s_answer>\" if \"docvqa\" in dataset_name_or_path else f\"<s_{task_name}>\",\n                    sort_json_key=config.sort_json_key,\n                )\n            )\n            # prompt_end_token is used for ignoring a given prompt in a loss function\n            # for docvqa task, i.e., {\"question\": {used as a prompt}, \"answer\": {prediction target}},\n            # set prompt_end_token to \"<s_answer>\"\n    data_module.train_datasets = datasets[\"train\"]\n    data_module.val_datasets = datasets[\"validation\"]\n\n    logger = TensorBoardLogger(\n        save_dir=config.result_path,\n        name=config.exp_name,\n        version=config.exp_version,\n        default_hp_metric=False,\n    )\n\n    lr_callback = LearningRateMonitor(logging_interval=\"step\")\n\n    checkpoint_callback = ModelCheckpoint(\n        monitor=\"val_metric\",\n        dirpath=Path(config.result_path) / config.exp_name / config.exp_version,\n        filename=\"artifacts\",\n        save_top_k=1,\n        save_last=False,\n        mode=\"min\",\n    )\n\n    bar = ProgressBar(config)\n\n    custom_ckpt = CustomCheckpointIO()\n    trainer = pl.Trainer(\n        num_nodes=config.get(\"num_nodes\", 1),\n        devices=torch.cuda.device_count(),\n        strategy=\"ddp\",\n        accelerator=\"gpu\",\n        plugins=custom_ckpt,\n        max_epochs=config.max_epochs,\n        max_steps=config.max_steps,\n        val_check_interval=config.val_check_interval,\n        check_val_every_n_epoch=config.check_val_every_n_epoch,\n        gradient_clip_val=config.gradient_clip_val,\n        precision=16,\n        num_sanity_val_steps=0,\n        logger=logger,\n        callbacks=[lr_callback, checkpoint_callback, bar],\n    )\n\n    trainer.fit(model_module, data_module, ckpt_path=config.get(\"resume_from_checkpoint_path\", None))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, required=True)\n    parser.add_argument(\"--exp_version\", type=str, required=False)\n    args, left_argv = parser.parse_known_args()\n\n    config = Config(args.config)\n    config.argv_update(left_argv)\n\n    config.exp_name = basename(args.config).split(\".\")[0]\n    config.exp_version = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\") if not args.exp_version else args.exp_version\n\n    save_config_file(config, Path(config.result_path) / config.exp_name / config.exp_version)\n    train(config)\n"
        }
      ]
    }
  ]
}