{
  "metadata": {
    "timestamp": 1736560881698,
    "page": 601,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lcdevelop/ChatBotCourse",
      "stars": 5940,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.07421875,
          "content": ".DS_Store\n.metadata\n.recommenders\n__pycache__\n.classpath\n.project\n.settings\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.0654296875,
          "content": "The MIT License (MIT)\n\nCopyright (c) 2013-present, Yuxi (Evan) You\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.60546875,
          "content": "ChatBotCourse\n==============\n_读本人更多原创文章，欢迎关注微信订阅号_\n\n<img src=\"https://github.com/lcdevelop/MachineLearningCourse/blob/master/weixinpub.jpg\" width = \"150\" height = \"150\" alt=\"lcsays\" />\n\n_欢迎关注我的另外几个github项目_\n * [_大数据专家养成记_](https://github.com/lcdevelop/bigdatablog)\n * [_教你成为全栈工程师_](https://github.com/lcdevelop/FullStackDeveloperCourse)\n * [_机器学习精简入门教程_](https://github.com/lcdevelop/MachineLearningCourse)\n\n智能游戏AI从基础到实战教程\n==============\n * [智能游戏AI从基础到实战教程 一-发动集体智慧开发游戏AI](https://blog.codemeteors.com/tutorial/139)(2018-08-16)\n\n自己动手做聊天机器人教程\n==============\n * [自己动手做聊天机器人 一-涉及知识](https://blog.codemeteors.com/tutorial/63)(2016-06-09)\n * [自己动手做聊天机器人 二-初识NLTK库](https://blog.codemeteors.com/tutorial/64)(2016-06-10)\n * [自己动手做聊天机器人 三-语料与词汇资源](https://blog.codemeteors.com/tutorial/65)(2016-06-12)\n * [自己动手做聊天机器人 四-何须动手？完全自动化对语料做词性标注](https://blog.codemeteors.com/tutorial/67)(2016-06-17)\n * [自己动手做聊天机器人 五-自然语言处理中的文本分类](https://blog.codemeteors.com/tutorial/69)(2016-06-21)\n * [自己动手做聊天机器人 六-教你怎么从一句话里提取出十句话的信息](https://blog.codemeteors.com/tutorial/70)(2016-06-22)\n * [自己动手做聊天机器人 七-文法分析还是基于特征好啊](https://blog.codemeteors.com/tutorial/71)(2016-06-23)\n * [自己动手做聊天机器人 八-重温自然语言处理](https://blog.codemeteors.com/tutorial/72)(2016-06-24)\n * [自己动手做聊天机器人 九-聊天机器人应该怎么做](https://blog.codemeteors.com/tutorial/73)(2016-06-25)\n * [自己动手做聊天机器人 十-半个小时搞定词性标注与关键词提取](https://blog.codemeteors.com/tutorial/74)(2016-06-28)\n * [自己动手做聊天机器人 十一-0字节存储海量语料资源](https://blog.codemeteors.com/tutorial/76)(2016-07-01)\n * [自己动手做聊天机器人 十二-教你如何利用强大的中文语言技术平台做依存句法和语义依存分析](https://blog.codemeteors.com/tutorial/77)(2016-07-04)\n * [自己动手做聊天机器人 十三-把语言模型探究到底](https://blog.codemeteors.com/tutorial/78)(2016-07-05)\n * [自己动手做聊天机器人 十四-探究中文分词的艺术](https://blog.codemeteors.com/tutorial/80)(2016-07-06)\n * [自己动手做聊天机器人 十五-一篇文章读懂拿了图灵奖和诺贝尔奖的概率图模型](https://blog.codemeteors.com/tutorial/81)(2016-07-09)\n * [自己动手做聊天机器人 十六-大话自然语言处理中的囊中取物](https://blog.codemeteors.com/tutorial/82)(2016-07-09)\n * [自己动手做聊天机器人 十七-让机器做词性自动标注的具体方法](https://blog.codemeteors.com/tutorial/86)(2016-07-15)\n * [自己动手做聊天机器人 十八-神奇算法之句法分析树的生成](https://blog.codemeteors.com/tutorial/87)(2016-07-19)\n * [自己动手做聊天机器人 十九-机器人是怎么理解“日后再说”的](https://blog.codemeteors.com/tutorial/88)(2016-07-21)\n * [自己动手做聊天机器人 二十-语义角色标注的基本方法](https://blog.codemeteors.com/tutorial/89)(2016-07-22)\n * [自己动手做聊天机器人 二十一-比TF-IDF更好的隐含语义索引模型是个什么鬼](https://blog.codemeteors.com/tutorial/90)(2016-07-26)\n * [自己动手做聊天机器人 二十二-神奇算法之人工神经网络](https://blog.codemeteors.com/tutorial/92)(2016-08-01)\n * [自己动手做聊天机器人 二十三-用CNN做深度学习](https://blog.codemeteors.com/tutorial/97)(2016-08-12)\n * [自己动手做聊天机器人 二十四-将深度学习应用到NLP](https://blog.codemeteors.com/tutorial/99)(2016-08-18)\n * [自己动手做聊天机器人 二十五-google的文本挖掘深度学习工具word2vec的实现原理](https://blog.codemeteors.com/tutorial/100)(2016-08-20)\n * [自己动手做聊天机器人 二十六-图解递归神经网络(RNN)](https://blog.codemeteors.com/tutorial/103)(2016-08-25)\n * [自己动手做聊天机器人 二十七-用深度学习来做自动问答的一般方法](https://blog.codemeteors.com/tutorial/104)(2016-08-26)\n * [自己动手做聊天机器人 二十八-脑洞大开：基于美剧字幕的聊天语料库建设方案](https://blog.codemeteors.com/tutorial/105)(2016-08-30)\n * [自己动手做聊天机器人 二十九-重磅：近1GB的三千万聊天语料供出](https://blog.codemeteors.com/tutorial/112)(2016-09-18)\n * [自己动手做聊天机器人 三十-第一版聊天机器人诞生——吃了字幕长大的小二兔](https://blog.codemeteors.com/tutorial/113)(2016-09-26)\n * [自己动手做聊天机器人 三十一-如何把网站流量导向小二兔机器人](https://blog.codemeteors.com/tutorial/114)(2016-09-30)\n * [自己动手做聊天机器人 三十二-用三千万影视剧字幕语料库生成词向量](https://blog.codemeteors.com/tutorial/115)(2016-10-10)\n * [自己动手做聊天机器人 三十三-两套代码详解LSTM-RNN——有记忆的神经网络](https://blog.codemeteors.com/tutorial/116)(2016-10-13)\n * [自己动手做聊天机器人 三十四-最快的深度学习框架torch](https://blog.codemeteors.com/tutorial/117)(2016-10-28)\n * [自己动手做聊天机器人 三十五-一个lstm单元让聊天机器人学会甄嬛体](https://blog.codemeteors.com/tutorial/118)(2016-11-23)\n * [自己动手做聊天机器人 三十六-深入理解tensorflow的session和graph](https://blog.codemeteors.com/tutorial/119)(2016-12-01)\n * [自己动手做聊天机器人 三十七-一张图了解tensorflow中的线性回归工作原理](https://blog.codemeteors.com/tutorial/120)(2016-12-08)\n * [自己动手做聊天机器人 三十八-原来聊天机器人是这么做出来的](https://blog.codemeteors.com/tutorial/121)(2017-01-10)\n * [自己动手做聊天机器人 三十九-满腔热血：在家里搭建一台GPU云服务共享给人工智能和大数据爱好者](https://blog.codemeteors.com/tutorial/122)(2017-01-16)\n * [自己动手做聊天机器人 四十-视频教程之开篇宣言与知识点梳理](https://blog.codemeteors.com/tutorial/124)(2017-03-05)\n * [自己动手做聊天机器人 四十一-视频教程之环境搭建与python基础](https://blog.codemeteors.com/tutorial/125)(2017-03-31)\n * [自己动手做聊天机器人 四十二-(重量级长文)从理论到实践开发自己的聊天机器人](https://blog.codemeteors.com/tutorial/136)(2017-09-07)\n"
        },
        {
          "name": "baidu_search",
          "type": "tree",
          "content": null
        },
        {
          "name": "chatbotv1",
          "type": "tree",
          "content": null
        },
        {
          "name": "chatbotv2",
          "type": "tree",
          "content": null
        },
        {
          "name": "chatbotv3",
          "type": "tree",
          "content": null
        },
        {
          "name": "chatbotv4",
          "type": "tree",
          "content": null
        },
        {
          "name": "chatbotv5",
          "type": "tree",
          "content": null
        },
        {
          "name": "corpus",
          "type": "tree",
          "content": null
        },
        {
          "name": "digital_recognition.py",
          "type": "blob",
          "size": 1.044921875,
          "content": "# coding:utf-8\n\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport tensorflow as tf\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_string('data_dir', './', 'Directory for storing data')\n\nmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n\nx = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.nn.softmax(tf.matmul(x,W) + b)\ny_ = tf.placeholder(\"float\", [None,10])\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n\ninit = tf.initialize_all_variables()\nsess = tf.InteractiveSession()\nsess.run(init)\nfor i in range(1000):\n    batch_xs, batch_ys = mnist.train.next_batch(100)\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\ncorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(accuracy.eval({x: mnist.test.images, y_: mnist.test.labels}))\n"
        },
        {
          "name": "digital_recognition_cnn.py",
          "type": "blob",
          "size": 3.1884765625,
          "content": "# coding:utf-8\n\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport tensorflow as tf\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_string('data_dir', './', 'Directory for storing data')\n\nmnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n# 初始化生成随机的权重(变量)，避免神经元输出恒为0\ndef weight_variable(shape):\n    # 以正态分布生成随机值\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n# 初始化生成随机的偏置项(常量)，避免神经元输出恒为0\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n# 卷积采用1步长，0边距，保证输入输出大小相同\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n# 池化采用2×2模板\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n        strides=[1, 2, 2, 1], padding='SAME')\n\n# 28*28=784\nx = tf.placeholder(tf.float32, [None, 784])\n# 输出类别共10个：0-9\ny_ = tf.placeholder(\"float\", [None,10])\n\n# 第一层卷积权重，视野是5*5，输入通道1个，输出通道32个\nW_conv1 = weight_variable([5, 5, 1, 32])\n# 第一层卷积偏置项有32个\nb_conv1 = bias_variable([32])\n\n# 把x变成4d向量，第二维和第三维是图像尺寸，第四维是颜色通道数1\nx_image = tf.reshape(x, [-1,28,28,1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\n# 第二层卷积权重，视野是5*5，输入通道32个，输出通道64个\nW_conv2 = weight_variable([5, 5, 32, 64])\n# 第二层卷积偏置项有64个\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\n# 第二层池化后尺寸编程7*7，第三层是全连接，输入是64个通道，输出是1024个神经元\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\n# 第三层全连接偏置项有1024个\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n# 按float做dropout，以减少过拟合\nkeep_prob = tf.placeholder(\"float\")\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n# 最后的softmax层生成10种分类\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n# Adam优化器来做梯度最速下降\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_all_variables())\n\nfor i in range(20000):\n    batch = mnist.train.next_batch(50)\n    if i%100 == 0:\n        train_accuracy = accuracy.eval(feed_dict={\n            x:batch[0], y_: batch[1], keep_prob: 1.0})\n        print \"step %d, training accuracy %g\"%(i, train_accuracy)\n    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\nprint \"test accuracy %g\"%accuracy.eval(feed_dict={\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n"
        },
        {
          "name": "gensim_word2vec.py",
          "type": "blob",
          "size": 0.5234375,
          "content": "# coding:utf-8\n\nfrom gensim.models import word2vec\nimport logging\n\n#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n#sentences = word2vec.LineSentence('segment_result_lined')\n#model = word2vec.Word2Vec(sentences, size=200, workers=4, iter=20)\n#model.save(\"word_vec_model/model\")\nmodel_2 = word2vec.Word2Vec.load(\"word_vec_model/model\")\ny = model_2.most_similar(u\"学习\", topn=10)\nfor (word, score) in y:\n    print word\n    print score\n#print model_2.syn0norm[model_2.vocab[u\"小兔\"].index]\n"
        },
        {
          "name": "learning_tensorflow",
          "type": "tree",
          "content": null
        },
        {
          "name": "lstm_code",
          "type": "tree",
          "content": null
        },
        {
          "name": "pattern_recognition.lua",
          "type": "blob",
          "size": 3.6357421875,
          "content": "require 'nn'\nrequire 'paths'\nif (not paths.filep(\"cifar10torchsmall.zip\")) then\n    os.execute('wget -c https://s3.amazonaws.com/torch7/data/cifar10torchsmall.zip')\n    os.execute('unzip cifar10torchsmall.zip')\nend\ntrainset = torch.load('cifar10-train.t7')\ntestset = torch.load('cifar10-test.t7')\nclasses = {'airplane', 'automobile', 'bird', 'cat',\n'deer', 'dog', 'frog', 'horse', 'ship', 'truck'}\nsetmetatable(trainset, \n{__index = function(t, i) \n    return {t.data[i], t.label[i]} \nend}\n);\ntrainset.data = trainset.data:double() -- convert the data from a ByteTensor to a DoubleTensor.\n\nfunction trainset:size() \n    return self.data:size(1) \nend\nmean = {} -- store the mean, to normalize the test set in the future\nstdv  = {} -- store the standard-deviation for the future\nfor i=1,3 do -- over each image channel\n    mean[i] = trainset.data[{ {}, {i}, {}, {}  }]:mean() -- mean estimation\n    print('Channel ' .. i .. ', Mean: ' .. mean[i])\n    trainset.data[{ {}, {i}, {}, {}  }]:add(-mean[i]) -- mean subtraction\n\n    stdv[i] = trainset.data[{ {}, {i}, {}, {}  }]:std() -- std estimation\n    print('Channel ' .. i .. ', Standard Deviation: ' .. stdv[i])\n    trainset.data[{ {}, {i}, {}, {}  }]:div(stdv[i]) -- std scaling\nend\nnet = nn.Sequential()\nnet:add(nn.SpatialConvolution(3, 6, 5, 5)) -- 3 input image channels, 6 output channels, 5x5 convolution kernel\nnet:add(nn.ReLU())                       -- non-linearity \nnet:add(nn.SpatialMaxPooling(2,2,2,2))     -- A max-pooling operation that looks at 2x2 windows and finds the max.\nnet:add(nn.SpatialConvolution(6, 16, 5, 5))\nnet:add(nn.ReLU())                       -- non-linearity \nnet:add(nn.SpatialMaxPooling(2,2,2,2))\nnet:add(nn.View(16*5*5))                    -- reshapes from a 3D tensor of 16x5x5 into 1D tensor of 16*5*5\nnet:add(nn.Linear(16*5*5, 120))             -- fully connected layer (matrix multiplication between input and weights)\nnet:add(nn.ReLU())                       -- non-linearity \nnet:add(nn.Linear(120, 84))\nnet:add(nn.ReLU())                       -- non-linearity \nnet:add(nn.Linear(84, 10))                   -- 10 is the number of outputs of the network (in this case, 10 digits)\nnet:add(nn.LogSoftMax())                     -- converts the output to a log-probability. Useful for classification problems\ncriterion = nn.ClassNLLCriterion()\ntrainer = nn.StochasticGradient(net, criterion)\ntrainer.learningRate = 0.001\ntrainer.maxIteration = 5\ntrainer:train(trainset)\ntestset.data = testset.data:double()   -- convert from Byte tensor to Double tensor\nfor i=1,3 do -- over each image channel\n    testset.data[{ {}, {i}, {}, {}  }]:add(-mean[i]) -- mean subtraction    \n    testset.data[{ {}, {i}, {}, {}  }]:div(stdv[i]) -- std scaling\nend\npredicted = net:forward(testset.data[100])\nprint(classes[testset.label[100]])\nprint(predicted:exp())\nfor i=1,predicted:size(1) do\n    print(classes[i], predicted[i])\nend\ncorrect = 0\nfor i=1,10000 do\n    local groundtruth = testset.label[i]\n    local prediction = net:forward(testset.data[i])\n    local confidences, indices = torch.sort(prediction, true)  -- true means sort in descending order\n    if groundtruth == indices[1] then\n        correct = correct + 1\n    end\nend\n\nprint(correct, 100*correct/10000 .. ' % ')\nclass_performance = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0}\nfor i=1,10000 do\n    local groundtruth = testset.label[i]\n    local prediction = net:forward(testset.data[i])\n    local confidences, indices = torch.sort(prediction, true)  -- true means sort in descending order\n    if groundtruth == indices[1] then\n        class_performance[groundtruth] = class_performance[groundtruth] + 1\n    end\nend\n\nfor i=1,#classes do\n    print(classes[i], 100*class_performance[i]/1000 .. ' %')\nend\n"
        },
        {
          "name": "read_images.c",
          "type": "blob",
          "size": 2.70703125,
          "content": "/************************\n * author: SharEDITor\n * date:   2016-08-02\n * brief:  read MNIST data\n ************************/\n#include <stdio.h>\n#include <stdint.h>\n#include <assert.h>\n#include <stdlib.h>\n\nunsigned char *lables = NULL;\n\n/**\n * All the integers in the files are stored in the MSB first (high endian) format\n */\nvoid copy_int(uint32_t *target, unsigned char *src)\n{\n    *(((unsigned char*)target)+0) = src[3];\n    *(((unsigned char*)target)+1) = src[2];\n    *(((unsigned char*)target)+2) = src[1];\n    *(((unsigned char*)target)+3) = src[0];\n}\n\nint read_lables()\n{\n    FILE *fp = fopen(\"./train-labels-idx1-ubyte\", \"r\");\n    if (NULL == fp)\n    {\n        return -1;\n    }\n    unsigned char head[8];\n    fread(head, sizeof(unsigned char), 8, fp);\n    uint32_t magic_number = 0;\n    uint32_t item_num = 0;\n    copy_int(&magic_number, &head[0]);\n    // magic number check\n    assert(magic_number == 2049);\n    copy_int(&item_num, &head[4]);\n\n    uint64_t values_size = sizeof(unsigned char) * item_num;\n    lables = (unsigned char*)malloc(values_size);\n    fread(lables, sizeof(unsigned char), values_size, fp);\n\n    fclose(fp);\n    return 0;\n}\n\nint read_images()\n{\n    FILE *fp = fopen(\"./train-images-idx3-ubyte\", \"r\");\n    if (NULL == fp)\n    {\n        return -1;\n    }\n    unsigned char head[16];\n    fread(head, sizeof(unsigned char), 16, fp);\n    uint32_t magic_number = 0;\n    uint32_t images_num = 0;\n    uint32_t rows = 0;\n    uint32_t cols = 0;\n    copy_int(&magic_number, &head[0]);\n    // magic number check\n    assert(magic_number == 2051);\n    copy_int(&images_num, &head[4]);\n    copy_int(&rows, &head[8]);\n    copy_int(&cols, &head[12]);\n\n    printf(\"rows=%d cols=%d\\n\", rows, cols);\n\n    uint64_t image_size = rows * cols;\n    uint64_t values_size = sizeof(unsigned char) * images_num * rows * cols;\n    unsigned char *values = (unsigned char*)malloc(values_size);\n    fread(values, sizeof(unsigned char), values_size, fp);\n\n    for (int image_index = 0; image_index < images_num; image_index++)\n    {\n        // print the label\n        printf(\"=========================================  %d  ======================================\\n\", lables[image_index]);\n        for (int row_index = 0; row_index < rows; row_index++)\n        {\n            for (int col_index = 0; col_index < cols; col_index++)\n            {\n                // print the pixels of image\n                printf(\"%3d\", values[image_index*image_size+row_index*cols+col_index]);\n            }\n            printf(\"\\n\");\n        }\n        printf(\"\\n\");\n    }\n\n    free(values);\n    fclose(fp);\n    return 0;\n}\n\nint main(int argc, char *argv[])\n{\n    if (-1 == read_lables())\n    {\n        return -1;\n    }\n    if (-1 == read_images())\n    {\n        return -1;\n    }\n    return 0;\n}\n"
        },
        {
          "name": "seq2seq",
          "type": "tree",
          "content": null
        },
        {
          "name": "subtitle",
          "type": "tree",
          "content": null
        },
        {
          "name": "tf_classify_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "word2vec",
          "type": "tree",
          "content": null
        },
        {
          "name": "word_segment.py",
          "type": "blob",
          "size": 0.767578125,
          "content": "# coding:utf-8\n\nimport sys\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\nimport jieba\nfrom jieba import analyse\n\ndef segment(input, output):\n    input_file = open(input, \"r\")\n    output_file = open(output, \"w\")\n    while True:\n        line = input_file.readline()\n        if line:\n            line = line.strip()\n            seg_list = jieba.cut(line)\n            segments = \"\"\n            for str in seg_list:\n                segments = segments + \" \" + str\n            segments = segments + \"\\n\"\n            output_file.write(segments)\n        else:\n            break\n    input_file.close()\n    output_file.close()\n\nif __name__ == '__main__':\n    if 3 != len(sys.argv):\n        print \"Usage: \", sys.argv[0], \"input output\"\n        sys.exit(-1)\n    segment(sys.argv[1], sys.argv[2]);\n"
        },
        {
          "name": "word_vectors_loader.py",
          "type": "blob",
          "size": 1.4169921875,
          "content": "# coding:utf-8\n\nimport sys\nimport struct\nimport math\nimport numpy as np\n\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\nmax_w = 50\nfloat_size = 4\n\ndef load_vectors(input):\n    print \"begin load vectors\"\n\n    input_file = open(input, \"rb\")\n\n    # 获取词表数目及向量维度\n    words_and_size = input_file.readline()\n    words_and_size = words_and_size.strip()\n    words = long(words_and_size.split(' ')[0])\n    size = long(words_and_size.split(' ')[1])\n    print \"words =\", words\n    print \"size =\", size\n\n    word_vector = {}\n\n    for b in range(0, words):\n        a = 0\n        word = ''\n        # 读取一个词\n        while True:\n            c = input_file.read(1)\n            word = word + c\n            if False == c or c == ' ':\n                break\n            if a < max_w and c != '\\n':\n                a = a + 1\n        word = word.strip()\n\n        # 读取词向量\n        vector = np.empty([200])\n        for index in range(0, size):\n            m = input_file.read(float_size)\n            (weight,) = struct.unpack('f', m)\n            vector[index] = weight\n\n        # 将词及其对应的向量存到dict中\n        word_vector[word.decode('utf-8')] = vector\n\n    input_file.close()\n\n    print \"load vectors finish\"\n    return word_vector\n\nif __name__ == '__main__':\n    if 2 != len(sys.argv):\n        print \"Usage: \", sys.argv[0], \"vectors.bin\"\n        sys.exit(-1)\n    d = load_vectors(sys.argv[1])\n    print d[u'真的']\n"
        }
      ]
    }
  ]
}