{
  "metadata": {
    "timestamp": 1736560862509,
    "page": 571,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "SmirkCao/Lihang",
      "stars": 6049,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": "APP",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH01",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH02",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH03",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH04",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH05",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH06",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH07",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH08",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH09",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH10",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH11",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH12",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH13",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH14",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH15",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH16",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH17",
          "type": "tree",
          "content": null
        },
        {
          "name": "CH22",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.5419921875,
          "content": "# 统计学习方法\n![Hits](https://www.smirkcao.info/hit_gits/Lihang/README.md)\n\n[![Gitter chat](https://badges.gitter.im/SmirkCao/StatisticalLearningMethods.png)](https://gitter.im/StatisticalLearningMethods/Book)[![Python](https://img.shields.io/badge/python-3.5|3.6|3.7-blue.svg)](-)[![pull](https://img.shields.io/badge/contributions-welcome-blue.svg)](https://github.com/SmirkCao/Lihang/pulls)\n\n本书已经出第二版，2019年5月之后所有内容更新参考第二版第一次印刷。\n\n[第一版内容见Release first_edition](https://github.com/SmirkCao/Lihang/archive/first_edition.zip)\n\n[TOC]\n\n## 工具包\n\n为方便学习，整理一些工具说明。\n\n- GitHub的markdown公式支持一般，推荐使用Chrome插件[TeX All the Things](https://chrome.google.com/webstore/detail/tex-all-the-things/cbimabofgmfdkicghcadidpemeenbffn)来渲染TeX公式,，本地Markdown编辑器推荐[Typora](https://typora.io/)，注意Ctrl+, 打开Preferences，Syntax Support部分勾选inline Math。Ubuntu和Windows都正常。\n- math_markdown.pdf为[math_markdown.md](./math_markdown.md)的导出版本， 方便查看使用， markdown版本为最新版本，基本覆盖了书中用到的数学公式的$\\LaTeX$表达方式。\n- [ref_downloader](./ref_downloader.sh) 是一个参考文献下载脚本，这本书一定要配合参考文献看，每章的大参考文献一定要看，对书的内容理解会很有帮助。\n- [glossary_index](./glossary_index.md) 是一个非正式的术语索引，这个书后面是有一个的，但是不方便展开，在这个部分添加了部分扩展的内容。\n- [symbol_index](./symbol_index.md) 是一个非正式的符号索引，第一版中有符号说明，第二版没有了，可能是无监督这部分涉及到的符号真的是太多了，总之，保留这部分，在感觉混淆的时候可以查下，看看是否有帮助。\n- [errata_se](./errata_se.md) 非官方的errata，供参考。如果有内容感觉不清楚，可以参考看看，希望有帮助。\n\n## 前前言\n\n- 2019年5月，期待许久的第二版发布了，第一时间下了订单，预计母亲节这天可以发货。\n- 5月13日新书到手，第二版配了一张新照片，短发，比之前显得年轻...\n- 第二版修改了标点符号，第一版中逗号中文，句号英文。第二版将之前的英文句号更改成了中文句号。\n- 第二版取消了符号表，可能是因为同一本书前后有些地方用了不同的符号？所以在这个repo里面，我们尝试加上[符号表](symbol_index.md)做说明，方便查询。\n- 第二版增加了八个无监督的学习方法，至此，数据挖掘十大算法除了Apriori，全了。\n\n如果需要引用这个Repo:\n\n格式： `SmirkCao, Lihang, (2018), GitHub repository, https://github.com/SmirkCao/Lihang`\n\n或者\n\n```\n@misc{SmirkCao,\n  author = {SmirkCao},\n  title = {Lihang},\n  year = {2018},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/SmirkCao/Lihang}},\n  commit = {c5624a9bd757a5cc88e78b85b89e9221deb08270}\n}\n```\n\n\n\n## 前言\n\n这部分内容并不对应《统计学习方法》中的前言，书中的**前言**写的也很好，引用如下:\n\n>1. 在内容选取上，侧重介绍那些最重要，最常用的方法，特别是关于**分类与标注**问题的方法.\n>1. 力图用统一框架来论述所有方法，使全书整体不失系统性。\n>1. 适用于信息检索及自然语言处理等专业大学生，研究生\n\n另外还有一点要注意作者的工作背景\n\n> 作者一直从事利用统计学习方法对文本数据进行各种智能性处理的研究， 包括自然语言处理、信息检索、文本数据挖掘。\n\n- 每个人都有适合自己的理解方式，对同样的内容，会有不同的理解\n- 书如数据，学如训练，人即模型。\n\n如果用我这个模型来实现相似度查找，和李老师这本书神似的就是《半导体光电器件》了，只可惜昔时年少，未曾反复研读。\n\n希望在反复研读的过程中，将整个这本书看厚，变薄。这个系列的所有的文档，以及代码，没有特殊说明的情况下\"书中\"这个描述指代的都是李航老师的《统计学习方法》。其他参考文献中的内容如果引用会给出链接。\n\n在Refs中列出了部分参考文献，有些参考文献对于理解书中的内容是非常有帮助的。关于这些文件的描述和解释会在参考部分对应的[Refs/README.md](Refs/README.md)中补充。这个文档中也添加了其他参考文献的一些说明。\n\n方便参考文献下载， 在review02的时候，添加了[ref_downloader.sh](ref_downloader.sh)，可以用来下载书中列举的参考文献，更新过程随着review02的进行逐渐完成。\n\n另外，李航老师的这本书，~~真的很薄（第二版不薄了）~~，但是几乎每句话都会带出很多点，值得反复研读。\n\n书中在目录之后有个符号表，解释了符号定义，所以如果有不理解的符号可以过来查表；在本书后面有个索引，可以通过索引查找对应的符号表示的含义在书中出现的位置。在本Repo中，维护了一个glossary_index.md，目的是给对应的符号补充一些说明，以及直接标注符号对应的页码，进度随review更新。\n\n每个算法，示例结束之后会有一个◼️，表示这个算法或者例子到此结束。这个叫证明结束符，看文献多了就知道了。\n\n### 关于对数底数\n\n读书的时候经常会有关于对数底数是多少的问题，有些比较重要的，书中都有强调。 有些没有强调的，通过上下文可以理解。另外，因为有换底公式，所以，底具体是什么关系不是太大，差异在于一个常系数。但是选用不同的底会有物理意义和处理问题方面的考虑，关于这个问题的分析，可以看PRML 1.6中关于熵的讨论去体会。\n\n另外关于公式中常系数的问题，如果用迭代求解的方式，有时对公式做一定的简化，可能会改善收敛速度。个中细节可以实践中慢慢体会。\n\n### 关于篇幅\n\n![各章节篇幅占比 ](assets/content_distribution.png)\n\n这里插入个图表，列举了各个章节所占篇幅，其中SVM是监督学习里面占用篇幅最大的，MCMC是无监督里面篇幅占用最大的，另外DT，HMM，CRF，SVD，PCA，LDA，PageRank也占了相对较大的篇幅。\n\n章节之间彼此又有联系，比如NB和LR，DT和AdaBoost，Perceptron和SVM，HMM和CRF等等，如果有大章节遇到困难，可以回顾前面章节的内容，或查看具体章节的参考文献，一般都给出了对这个问题描述更详细的参考文献，可能会解释你卡住的地方。\n\n\n\n## CH01 统计学习及监督学习概论\n\n[Introduction](CH01/README.md)\n\n统计学习方法三要素:\n\n- 模型\n\n- 策略\n\n- 算法\n\n  \n\n  第二版对这一章的目录结构重新梳理了，更清晰。\n\n## CH02 感知机\n\n[Perceptron](CH02/README.md)\n- 感知机是二类分类的线性分类模型\n- 感知机对应于特征空间中将实例划分为正负两类的分离超平面.\n## CH03 k近邻法\n\n[kNN](CH03/README.md)\n- kNN是一种基本的分类与回归方法\n- k值的选择, 距离度量及分类决策规则是kNN的三个基本要素.\n## CH04 朴素贝叶斯法\n\n[NB](CH04/README.md)\n- 朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法.\n1. $IID\\rightarrow$输入输出的联合概率分布\n1. $Bayes\\rightarrow$后验概率最大的输出\n- x的某种组合在先验中没有出现的情况, 会出现概率为0的情况, 对应平滑处理方案\n  $$P_\\lambda(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^{N}{I(x_i^{(j)}=a_{jl}, y_i=c_k)}+\\lambda}{\\sum_{i=1}^{N}{I(y_i=c_k)+S_j\\lambda}}$$\n  - $\\lambda = 0$ 对应极大似然估计\n  - $\\lambda = 1$ 对应拉普拉斯平滑\n- 朴素贝叶斯法实际上学习到生成数据的机制, 所以属于生成模型.\n\n## CH05 决策树\n\n[DT](CH05/README.md)\n\n- 决策树是一种基本的分类与回归方法\n## CH06 逻辑斯谛回归与最大熵模型\n\n[LR](CH06/README.md)\n\n- 逻辑斯谛回归是统计学中的经典分类方法\n- 最大熵是概率模型学习的一个准则, 将其推广到分类问题得到最大熵模型\n\n关于最大熵的学习，推荐阅读该章节的参考文献[1]，[Berger, 1996](Refs/README.md), 有益于书中例子的理解以及最大熵原理的把握。\n\n那么, **为什么LR和Maxent要放在一章?**\n- 都属于对数线性模型\n\n- 都可用于二分类和多分类\n\n- 两种模型的学习方法一般采用极大似然估计, 或正则化的极大似然估计. 可以形式化为无约束最优化问题, 求解方法有IIS, GD, BFGS等\n\n- 在[Logistic regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)中有如下描述,\n\n  > Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a [logistic function](https://en.wikipedia.org/wiki/Logistic_function).\n\n- 还有[这样的描述](https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_journal.pdf)\n\n  >Logistic regression is a special case of maximum entropy with two labels +1 and −1.\n\n  这个章节的推导中用到了$y\\in \\mathcal{Y}=\\{0,1\\}$的性质\n\n- 有时候我们会说，逻辑回归在NLP方面叫做Maxent\n\n## CH07 支持向量机\n\n[SVM](CH07/README.md)\n\n- 支持向量机是一种二分类模型。\n- 基本模型是定义在特征空间上的间隔最大化的线性分类器, 间隔最大使他有别于[感知机](CH02/README.md)\n- 这一章占了很大篇幅，因为margin这个思想几乎可以串起来整个分类问题。\n\n## CH08 提升方法\n\n[Boosting](CH08/README.md)\n\n- 提升方法是一种常用的统计学习方法, 应用广泛且有效.\n\n## ----分割线----\n\n姑且在这里分一下，因为后面HMM和CRF通常会引出概率图模型的介绍，在《机器学习，周志华》里面更是用了一个单独的**概率图模型**章节来包含HMM，MRF，CRF等内容。另外从HMM到CRF本身也有很多相关的点。\n\n在书中第一章有说明监督学习的三种应用：分类，标注和回归。在第十二章中有补充，本书主要考虑前两者的学习方法。据此， 在这里分割也是合适的，前面介绍分类模型， 少部分提到了回归，后面主要介绍标注问题。\n\n## CH09 EM算法及其推广\n\n[EM](CH09/README.md)\n\n- EM算法是一种迭代算法，用于含有隐变量的概率模型参数**极大似然估计**，或者极大后验概率估计。(这里的极大似然估计和极大后验概率估计是**学习策略**)\n\n- >  如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。\n\n  注意书上这个描述如果不理解，参考[CH04](CH04/README.md)中朴素贝叶斯法的参数估计部分。\n\n- 这部分代码实现了BMM和GMM，值得看下\n\n- 关于EM，这个章节写的不多，EM是十大算法之一，EM和Hinton关系紧密，Hinton在2018年ICLR上发表了Capsule Network的第二篇文章《Matrix Capsules with EM Routing》\n\n- 在CH22中将EM算法归类于基础机器学习方法，不涉及具体的机器学习模型，可用于无监督学习也可用于监督学习，半监督学习。\n\n## CH10 隐马尔可夫模型\n\n[HMM](CH10/README.md)\n\n- 隐马尔可夫模型是可用于标注问题的统计学习模型，描述由隐藏的马尔可夫链随机生成观测序列的过程，属于生成模型。\n- 隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态的序列，再由各个状态速记生成一个观测而产生观测的序列的过程。\n- 可用于**标注**(Tagging)问题，状态对应标记。\n- 三个基本问题：概率计算问题，学习问题，预测问题。\n\n## CH11 条件随机场\n\n[CRF](CH11/README.md)\n\n- 条件随机场是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型，其特点是假设输出随机变量构成**马尔可夫随机场**。\n- 概率无向图模型，又称为马尔可夫随机场，是一个可以由无向图表示的**联合概率分布**。\n- 三个基本问题：概率计算问题，学习问题，预测问题\n\n## CH12 监督学习方法总结\n\n[Summary](CH12/README.md)\n\n这章就简单的几页，可以考虑如下阅读套路：\n- 和第一章一起看\n\n- 在前面的学习中遇到不清楚的问题的时候，过一遍这个章节。\n\n- 将这一章看厚，从这一章展开到其他十个章节。\n\n- 注意这一章有个图12.2，这里面提到了逻辑斯谛损失函数，这里的$y$应该是定义在$\\cal{Y}=\\{+1,-1\\}$中的，在前面介绍[LR](CH06/README.md)的时候$y$定义在$\\cal{Y}=\\{0,1\\}$，这里注意下。\n\n李老师这本书真的是每次刷都会有新的收获。\n\n## ----分割线----\n\n第二版增加了八个无监督学习方法：聚类，奇异值分解，主成分分析，潜在语义分析，概率潜在语义分析，马尔可夫链蒙特卡罗法，潜在狄利克雷分配，PageRank。\n\n## CH13 无监督学习概论\n\n[Introduction](./CH13/README.md)\n\n- 无监督学习的基本问题：聚类，降维，话题分析和图分析。\n- 横向**结构**和纵向**结构**这个问题，从存储的角度来考虑。\n- 注意不同任务的策略：类别中心距离最小化，维度转换过程中信息损失的最小化，生成数据概率的最大化。\n- 在无监督学习部分经常会提到**数据中的结构**， 是指数据中变量之间的关系。\n\n## CH14 聚类方法\n\n[Clustering](./CH14/README.md)\n\n- 例子14.2很好，建议画出来先自己展开思考下，再往后看\n- 聚类可以用于图像压缩\n\n## CH15 奇异值分解\n\n- 基本机器学习方法\n- 奇异值分解定理保证分解存在\n- 奇异值矩阵唯一，$U,V$不唯一\n- 有明确的几何解释\n\n## CH16 主成分分析\n\n- 利用正交变换将线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据，线性无关的变量称为**主成分**\n- 主成分分析之前，需要对给定数据规范化，使得每一个变量均值为0，方差为1。\n- 主成分并不对应原始数据的某一个特征， 可以通过因子负荷量来观察主成分与原始特征之间的关系。\n- 这部分内容，还没有提到**话题**这个概念，后面章节开始介绍了很多话题分析相关的内容，LSA，PLSA，LDA都是和话题有关，MCMC是在LDA中使用的一个工具。\n- 提到了总体主成分和样本主成分，前者是后者的基础。主要体现在总体考虑期望，样本考虑均值。样本主成分具有和总体主成分一样的性质。\n\n## CH17 潜在语义分析\n\n- 在sklearn的定义中，LSA就是截断奇异值分解。\n- 注意体会LSA和PCA的区别，主要在于是不是去均值。\n- 在LSA中，话题向量空间是$U$，DOC在话题向量空间的表示是$SV^\\mathrm{T}$。但是在sklaern中，xtransformed是$U\\mit\\Sigma$ \n\n## CH18 概率潜在语义分析\n\n## CH19 马尔可夫链蒙特卡罗法\n\n## CH20 潜在狄利克雷分配\n\n## CH21 PageRank算法\n\n## CH22 无监督学习方法总结\n\n## 后记\n\n整个这本书里面各章节也不是完全独立的，这部分希望整理章节之间的联系以及适用的数据集。算法到底实现到什么程度，能跑什么数据集也是一方面。\n\n![data_algo_map](assets/data_algo_map.png)\n\n\n\n## 参考\n\n[^1]: [Matrix Capsules with EM Routing](http://arxiv.org/abs/1710.09829)\n"
        },
        {
          "name": "Refs",
          "type": "tree",
          "content": null
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cheatsheet",
          "type": "tree",
          "content": null
        },
        {
          "name": "errata_fe.md",
          "type": "blob",
          "size": 8.7294921875,
          "content": "# ERRATA\n![Hits](https://www.smirkcao.info/hit_gits/Lihang/errata_fe.md)\n\n参考书版本为**2017年11月第20次印刷**, 在这之后的印刷版本有可能进行过修订, 愿本书越来越完善.\n1. $P_{162}$ 高斯混合模型的英文表示: Gaussian misture model $\\rightarrow$ Gaussian mixture model\n\n1. $P_{201}$对数线~~形~~模型$\\rightarrow$对数线性模型\n\n1. $P_{173}​$观测序列$O=\\{红, 红, 白, 白, 红\\}​$, 序列表示应该是$O=(红, 红, 白, 白, 红)​$\n\n1. $P_{197}​$条件随机场(11.11)\\~(11.12)， 应该是条件随机场(11.10)\\~(11.11)， 这两个是线性链条件随机场模型的基本形式\n\n1. $P_{198}​$公式(11.24)这个公式里面连乘用了行内形式，认为应该是行间形式，不算是错误了，书写上的一些问题。\n\n1. $P_{200}$公式(11.30)中$M_i$应为$M_{i+1}$\n   整体公式为$\\beta_i(y_i|x)=[M_{i+1}(y_i,y_{i+1}|x)]\\beta_{i+1}(y_{i+1}|x),i=1,2,\\dots,n+1$\n\n1. $P_{124}$中$b^*,f(x)$中的核函数表达式应该是$K(x_i,x_j)$以及$K(x,x_i)$\n\n1. $P_{34}$算法2.2在模型输出以及步骤(3)中混用了$\\sum_{j=1}^N$行间表达方式\n\n1. $P_{75}$参考文献3这本书作者少写了一个Olshen\n\n1. $P_{75}$参考文献7，ESL这本神书，在本书中的引文形式通常是有中译本说明的那种形式，应该统一一下。\n\n1. $P_{53}$参考文献1，书中引用的是2005年的Draft，原链接更新了2017年的手稿，这部分内容变成了**Chapter 3**，补充下， 文件名更新了，新文件名是NBayesLogReg.pdf，差一个字母\n\n1. $P_{47}$最后一段参数个数$K\\prod_{j=1}^nS_j$书中混用了行间表达形式和行内表达形式\n\n1. $P_{154}$参考文献9，这个文章是2002年的文献，书中记录为2004，这文章也不错\n\n1. $P_{164}$公式9.29,第二个求和应该是对$j$求和,从取值范围到$N$应该也可以看出$\\sum_{j=1}^N\\hat\\gamma_{jk}$\n\n1. $P_{169}$ d维的形式应该是$j=3,4,\\dots,d$而不是$j=3,4,\\dots,k$\n\n1. $P_{181}$`由于监督学习需要使用训练数据`这个应该是`需要使用标注的训练数据`.\n\n1. $P_{230}$ 海赛矩阵 Hesse matrix, 应该是 Hessian Matrix\n\n1. $P_{156}$观测数据表示为$Y=(Y_1, Y_2, Y_3, \\dots, Y_n)^T$, 未观测数据表示为$Z=(Z_1,Z_2, Z_3,\\dots, Z_n)^T$, 则观测数据的似然函数为\n\n     > 其实觉得这里应该是小写的$y=(y_1,y_2,\\dots,y_n), z=(z_1, z_2, \\dots,z_n)$\n\n1. $P_{219}$ Hesse matrix -> Hessian Matrix\n\n1. $P_{80}$公式6.7， 关于多项逻辑斯谛回归模型中的求和部分下角标如果换成$i$，觉得更好理解一点\n      $$\n      \\begin{aligned}\n      P(Y=k|x)&=\\frac{\\exp(w_k\\cdot x)}{1+\\sum_{j=1}^{K-1}\\exp(w_j\\cdot x)}, k=1,2,\\dots,K-1\\\\\n      P(Y=k|x)&=\\frac{1}{1+\\sum_{j=1}^{K-1}\\exp(w_j\\cdot x)}\\\\\n      \\end{aligned}\n      $$\n\n1. $P_{153} , P_{146}$`提升树是以分类树或回归树为基本分类器的提升方法`这里面基本分类器应该是基函数，分类问题对应分类树， 回归问题对应回归树。\n\n1. $P_{140}$例题来源于http://www.csie.edu.tw， 这个大概应该是http://www.csie.ntu.edu.tw。 但是也没找到对应的例子页面。\n\n1. $P_{148}$在提升树这个地方， 最后得到的提升树是$f_M(x)$， 前面介绍加法模型的时候， 得到的是$f(x)$实际上是一样的意思， 但是两个地方的表达不太一样。这个， 其实不算吧。。\n\n1. $P_{170}$Baum与Welch算法，后面HMM的描述中用的是Baum-Welch算法， 同一本书两个表达方式不统一。其实，这个也不是太重要。\n\n1. $P_{159}$\n$$\n\\begin{align}\nL(\\theta)-L(\\theta^{(i)})&=\\log \\left(\\sum_Z\\color{green}P(Y|Z,\\theta^{(i)})\\color{black}\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{\\color{green}P(Y|Z,\\theta^{(i)})}\\color{black}\\right)-\\log P(Y|\\theta^{(i)})\\\\\n&\\ge\\sum_Z P(Z|Y,\\theta^{(i)})\\log \\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^{(i)})}-\\log P(Y|\\theta^{(i)})\\\\\n&=\\sum_Z P(Z|Y,\\theta^{(i)})\\log \\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^{(i)})}-\\color{red}\\sum_ZP(Z|Y,\\theta^{(i)})\\color{black}\\log P(Y|\\theta^{(i)})\\\\\n&=\\sum_ZP(Z|Y,\\theta^{(i)})\\log \\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^{(i)})P(Y|\\theta^{(i)})}\n\\end{align}\n$$\n这里绿色部分应该是$P(Z|Y,\\theta^{(i)})$，为了构建期望而凑项，进而应用琴声不等式。\n\n26. $P_{162}$ 关于定理9.2.2的证明，参阅文献[6]， 这个定理的证明应该在参考文献[5]中有提到。\n\n27. $P_{166}$ `将其对  求偏导 ` 这个地方的符号$\\widetilde{P}$， 应该是$\\tilde{P}$， 和定义9.3中的有差异，一个是widetilde，一个是tilde，统一最好。\n\n28. $P_{189}$ 参考文献[2]的格式， 缺少卷数和页码范围， 77(2):257-186\n\n29. $P_{12}$图1.2中的纵坐标， 应该是$y$，在PRML中误差函数是$E(w)=\\frac{1}{2}\\sum_{n=1}^N\\{y(x_n-w)-t_n\\}^2$所以纵坐标是$t$\n\n30. $P_{57}​$ 在讲到决策树学习的损失函数部分。`决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化`这部分觉得描述有点问题，前面部分理解为正则化的似然函数作为损失函数，这个应该是对数似然，因为作为损失函数应该是越小越好，正则化的似然应该是越大越好。这样才能对应后面的`以损失函数为目标函数的最小化`\n\n31. $P_{36}$参考文献2, 这个文献应该是On convergence proofs for perceptrons. repo里面参考文献下载脚本可以自动下载该文献， 是一份扫描档。 不过，有其他文献也按照本书的引用方法引用的。\n\n32. $P_{134}$参考文献5, Platt这个文章最多引用的是J. Platt. *Advances in Kernel Methods -- Support Vector Learning,* *MIT Press,* *Cambridge, MA,* (*1998*)， 可以参考https://www.bibsonomy.org/bibtex/2ad411b41c7af4289282067a770edbdde/telekoma, 原书给的链接也是有效的，微软对这个链接做了转发， 跳转到新地址https://www.microsoft.com/en-us/research/publication/fast-training-of-support-vector-machines-using-sequential-minimal-optimization/?from=http%3A%2F%2Fresearch.microsoft.com%2Fapps%2Fpubs%2F%3Fid%3D68391\n\n33. $P_{36}$参考文献5, 现在比较容易获得的参考文献是1999年在Machine Learning上发表的那个版本，这个不算是错误。在repo的参考文献downloader里面，有对应的链接。\n\n34. $P_{134}$参考文献1,没有标明页码，1995,20:273,297\n\n35. $P_{XIII}$符号表说明中有关$||\\cdot||_2$的说明， 是二范数，这个应该是对的。后面支持向量机部分$P_{114}$中描述支持向量机损失函数第二项$\\lambda ||w||^2$为系数为$\\lambda$的$w$的$L_2$范数，是正则化项。应该是二范数的平方。对应了$w\\cdot w=||w||^2$， $w \\cdot w$是在Vapnik的SVN文章中的表示方法。\n\n36. $P_{122}$高斯核函数(Gaussian kernel  function)英文部分kernel和function之间，多了一个空格\n\n37. $P_{118}$支持向量机部分，使用了核函数的分类决策函数拉格朗日乘子变成了$a$，求和范围变成了$N_s$，但是文中没有说明为什么做这种改变。\n\n38. $P_{122}$介绍常用核函数的时候， 分类决策函数也用到了上面的表达方式。这两条， 涉及到的公式有7.68,7.89,7.90,7.91\n\n39. $P_{122}$公式7.91分类决策函数中的$z$应该是$x_i$\n\n    $f(x)=sign\\left(\\sum_{i=1}^{N_s}a_i^*y_i\\exp\\left(-\\frac{||x_i-x||^2}{2\\sigma^2}\\right)+b^*\\right)$\n\n40. $P_{124}$称为**非线性支持向量**，应该是**非线性支持向量$\\color{red}机$**。\n\n41. $P_{182}$上面第一个公式，$\\pi_{i_0}$应该是$\\pi_{i_1}$\n\n42. $P_{211}$这页表格中学习策略列，格式不是很统一，注意HMM部分`极大似然估计，`占了一行，而其他都是占了两行，应该是标点符号的空格处理不一样导致，不算错误，就是看起来稍微不同。\n\n43. $P_{195}$图11.5,条件随机场是无向图模型，图中应该没有箭头。\n\n44. $P_{199}$公式(11.26)中，$y$应该有下角标，$\\alpha_0(y_0|x)$以及$y_0=start$\n\n45. $P_{138}$算法8.1的描述中，$D_1$中用到了$w_{1i}$这样的表达，在后面$D_{m+1}$中用到了$w_{m+1,i}$这样的表达，意思完全明白，只是格式不太一致。\n\n46. ~~$P_{138}$分类误差率这个定义里面，$e_m=\\sum\\limits_{i=1}\\limits^NP(G_m(x_i)\\neq y_i)$，这个应该没有求和符号$e_m=P(G_m(x_i)\\neq y_i)$，在更新权重分布的时候做了归一化使得分类错误的点的系数求和应该刚好等于分类错误的概率。~~这个暂时保留下意见。\n\n47. $P_{79}$描述$x_i\\in \\R^n$应该是$x_i\\in \\R^{n+1}$，下面用到的是扩充权重向量\n\n48. $P_{89}$ `对数似然函数的极大值\\hat{w}`应该是对数似然函数极大值对应的参数向量$\\hat{w}$\n\n49. $P_{93}$ 第三点约束最优化问题的第一条约束应该是期望$E_P(f_i)-E_{\\tilde{P}}(f_i)=0,i=1,2,\\cdots,n$\n\n50. $P_{229}$ cell，47页没有cell相关的内容\n\n"
        },
        {
          "name": "errata_se.md",
          "type": "blob",
          "size": 3.7314453125,
          "content": "# ERRATA\n![Hits](https://www.smirkcao.info/hit_gits/Lihang/errata_se.md)\n\n参考书版本为**2019年05月第1次印刷**，在这之后的印刷版本有可能进行过修订，愿本书越来越完善。\n\n1. $P_{14}$` 贝叶斯估计与极大似然估计在思想上有很大的不同，代表着统计学中频率学派和贝叶斯学派对统计的不同认识`，这里贝叶斯估计对应了贝叶斯学派，而极大似然估计对应的是频率派，应该把贝叶斯学派和频率学派换一下顺序。\n\n2. $P_{257}$$X=(x_{ij})_{m\\times n}$应该是$X=[x_{ij}]_{m\\times n}$\n\n3. $P_{246}$输入空间是欧氏空间$X\\sube \\mathbf R^d$其实这里用$X\\sube \\mathbf R^m$表示是不是更好一点，不容易乱，后面的空间都是$m$维，对应的还有$P_{247}$中$\\mathbf R^d, \\mathbf R^{d'}$\n\n4. $P_{49}$关于KNN提出的年限，实际上这个文献是1967年的，而书中说是1968年提出的。\n\n5. $P_{320}$ 参考文献4，应该是**1404**.1100不是**14016**.1100\n\n6. $P_{29}$ 精确率和召回率的定义(1.41)和(1.42)\n   $$\n   P=\\frac{TP}{TP+FP}\\\\\n   R=\\frac{TP}{TP+FN}\n   $$\n   \n7. $P_{30}$ $F_1$定义，(1.44)\n   $$\n   F_1=\\frac{2TP}{2TP+FP+FN}\n   $$\n   \n8. $P_{245}$ $x \\in X, z \\in Z$这部分在第一章$P_8$的无监督学习部分定义是$x \\in \\mathcal{X}, z \\in \\mathcal{Z}$\n\n9. $P_{257}$ 公式14.6中，转置符号用了**斜体**$d_{ij}=\\left[(x_i-x_j)^TS^{-1}(x_i-x_j)\\right]^{\\frac{1}{2}}$\n   转置应该是和其他转置一样，是**正体**$d_{ij}=\\left[(x_i-x_j)^\\mathrm TS^{-1}(x_i-x_j)\\right]^{\\frac{1}{2}}$\n   \n10. $P_{265}$ 算法14.2中描述的输出$C^\\cdot$应该是$C^*$， 因为算法描述中最后输出的是$C^*$ \n\n11. $P_{261}$ 算法14.1，`输入:n个样本组成的样本集合及样本之间的距离`，其中样本之间的距离不应该是输入条件。\n\n12. $P_{452}$ 公式D.2上面一行，$R^n$中与$Y$中的每一向量正交的向量集合，应该是$\\mathbf R^n$\n\n13. $P_{451}$ 关于张成，在第二小节之上的一行$span\\{v_1,v_2,\\cdots,v_n\\}=V$用的是{}，前面定义的是$span(v_1,v_2,\\cdots,v_n)$，用()\n\n14. $P_{452}$ 公式D.1中$R^n$应为$\\mathbf{R}^n$\n\n15. $P_{274}$ $V_1=[\\begin{array}&\\nu_1&\\nu_2&\\cdots&\\nu_r\\end{array}]$$V_2=[\\begin{array}&\\nu_{r+1}&\\nu_{r+2}&\\cdots&\\nu_n\\end{array}]$，这部分定义用的是$\\nu$，而后面用到的时候用的都是$v$，比如公式15.8, 15.12, 15.15\n\n16. $P_{275}$ 公式15.14下面那行，$U_1$的列向量构成了一组标准$\\color{red}正交集$\n\n17. $P_{279}$ 图15.1中标记的$\\Sigma$ 应该是$\\mit\\Sigma$\n\n18. $P_{286}$ 公式15.25中$(a_{ij})^2$看起来不是很习惯，完全可以用$a_{ij}^2$表示，类似的还有$P_{293}$中总结的第7点\n\n19. $P_{293}$ $p=\\min\\{m,n\\}$ 应该是$p=\\min (m,n)$\n\n20. $P_{293}$ 第6点，奇异值$\\sigma_i$应该是$\\sigma_j$\n\n21. $P_{293}$ 第6点，从$AA^\\mathrm{T}$的特征值这句，虽然$A^\\mathrm{T}A$和$AA^\\mathrm{T}$的特征值是一样的，但是不太理解这里为什么写成$AA^\\mathrm{T}$，不知道是不是笔误。\n\n22. $P_{313}$ 求方差贡献率$\\sum\\limits_{i=1}^k\\eta_i$达到预定值的主成分个数$k$，这个应该是累计方差贡献率\n\n23. $P_{316}$ 公式16.52,16.53, 以及$X^{\\prime\\mathbf{T}}X$,后面$X^{\\prime}=\\frac{1}{\\sqrt{n-1}}X^\\mathbf{T}$中的$X^\\mathbf{T}$应该是$X^\\mathrm{T}$\n\n24. $P_{310}$ 样本矩阵$\\mit \\boldsymbol{X}$，应该是$X$。或者说，写成$X$才和其他表达是一致的。\n\n25. $P_{327}$ 17.1节最后一句，`这一结果完全从话题-文本矩阵的信息中获得`应该是`单词-文本矩阵`吧\n\n26. $P_{329}$ 这个例子并没有按照书中其他例子的格式编号，$P_{330}$中的表格，也没有表格编号和标题。下面截断奇异值分解的结果，其实应该算是个图。\n\n27. \n\n"
        },
        {
          "name": "glossary_index.md",
          "type": "blob",
          "size": 13.2880859375,
          "content": "# 索引\n![Hits](https://www.smirkcao.info/hit_gits/Lihang/glossary_index.md)\n\n[TOC]\n\n## 前言\n\n有时候读书会卡住，也许只是我们看问题的角度问题。同样的问题，在书中不同的地方会有提到，或相关，或无关。这个文档类似书中的最后的索引， 会加入一些个人的理解。\n\n每个内容单独一条。\n\n如果只有一个页码参考第二版2019.05第一次印刷，如果有两个页码，那么第一个对应第一版，第二个对应第二版。\n\n## Timeline\n\n1. PCA; Pearson; 1901\n1. PCA over random variable; Hotelling; 1933\n1. First pattern recognition althgrithm; Fisher; 1936\n1. Perceptron; Rosenblatt; 1957\n1. Kmeans; MacQueen; 1967\n1. KNN; Cover, Hart; 1967\n1. EM; Dempster; 1977\n1. DT: CART; Breiman; 1984\n1. DT: ID3; Quinlan; 1986\n1. BP; LeCun; 1987\n1. LSA; Deerwester; 1990\n1. SVM: Kernel; Boser, Guyon, Vapnik; 1992\n1. DT: C4.5; Quinlan; 1993\n1. SVM: Linear; Cortes, Vapnik; 1995\n1. AdaBoost; Freund, Schapire; 1995\n1. SVM: Regression; Drucker; 1996\n1. SMO; Platt; 1998\n1. Margin Theory; Schapire; 1998\n1. NMF; Lee; 1999\n1. PLSA; Hofmann; 1999\n1. Boosted Tree; Friedman; 2000\n1. CRF; Lafferty; 2001\n1. LDA; Blei; 2002\n\n## 基本想法\n\n### 无监督学习\n\n对给定的数据进行某种”压缩“，从而找到数据的潜在结构。\n\n### 主成分分析\n\n- 将数据规范化为每个变量均值为0，方差为1。\n- 对数据做正交变换，原来由线性相关的变量表示的数据，通过正交变换变成由若干个线性无关的新变量表示的数据。新变量是可能的正交变换中变量的方差的和最大的，方差表示在新变量上信息的大小。\n\n### 概率潜在语义分析\n\n发现由隐变量表示的话题，即潜在语义。一个文本的内容由其相关话题决定，一个话题的内容由其相关单词决定。\n\n### LDA的收缩吉布斯抽样算法\n$P_{412}$\n\n### 变分推理\n$P_{412}$\n\n### PageRank算法\n$P_{415}$ 在有向图上定义一个随机游走模型，即一阶马尔可夫链，描述随机游走者沿着有向图随机访问各个结点的行为。\n\n### PageRank一般定义\n$P_{421}$ 基本定义的基础上导入平滑项\n\n### Topic Modeling\n$P_{321}$ 试图从大量的文本数据中发现潜在的话题，以话题向量表示文本的语义内容，以话题向量空间的度量更准确的表示文本之间的语义相似度。这是话题分析(Topic Modeling)的基本想法。\n\n## Glossary\n\n### 贝叶斯学习\n$P_{391}$ LDA属于贝叶斯学习\n$P_{369}$ 贝叶斯学习中经常需要进行三种积分运算：规范化，边缘化，数学期望。\n$P_{401}$ 变分推理是贝叶斯学习中常用的含有隐变量模型的学习和推理方法。\n\n### 信息\n\n$P_{297}$ 新变量上信息的大小。\n\n$P_{306}$ 信息是指原有变量的方差。\n\n### Gram矩阵\n\n$P_{34}$，$P_{45}$ 在感知机中第一次提到\n\n$P_{119}$，$P_{139}$\t讲核函数的时候也有用到\n\n### 欧式空间\n\n$P_{4}$ 输入输出空间可以是**有限元素的集合**，也可以是整个**欧式空间**。集合和欧式空间对应了两种情况，离散和连续。\n\n### 凸优化\n\n$P_{100}$\n\n[CH07](CH07/README.md)\n\n### 拉格朗日对偶性\n\n$P_{225}$附录C\n\n### 拉格朗日乘子法\n\n$P_{182}$ BW算法中求Q函数极大化，因为$\\pi,A,B$都满足等式约束条件\n\n$P_{301}$ PCA中关于总体主成分的定理的证明。\n$P_{346}$ EM算法M步\n\n### 样本\n\n$P_{4}$ 输入和输出对又称为样本\n\n### KKT 条件\n\n见附录C\n\n### 经验\n\n提到经验，说的都是和训练数据集相关的\n$P_{352}$ 从样本得到经验分布，从而估计总体分布；或者从样本计算样本均值，从而估计总体期望。\n\n### 对偶\n\n感知机里面有提到，支持向量机里面有提到\n\n### 生成模型\n$P_{339}$ \n\n### 共现模型\n$P_{339}$ 概率潜在语义分析\n\n### 图模型\n$P_{341}$ 生成模型属于概率有向图模型\n$P_{386}$ 潜在狄利克雷模型是含有隐变量的概率图模型。\n\n### 随机游走\n$P_{351}$ \n\n### 分离超平面\n\n$P_{26}, P_{102}$ 支持向量机里面也有\n\n### 内积\n\n$P_{25}, P_{78}, P_{117}$在感知机、逻辑回归、支持向量机里面都有用到\n$P_{323}$ 词向量的相似度\n\n### 非负矩阵分解\n\n$P_{331}$ \n\n### 满条件分布\n$P_{372}$ \n\n### 指示函数\n\n$P_{10}$ 讨论测试数据集中的误差率和准确率的时候，提到指示函数。\n\n$P_{40}, P_{37}$\n\n这个函数在不同的教材上有不同的表示方式，比如在《深度学习》中表示为$\\mathbf 1_{condition}$\n\n另外， 张潼老师在IBM时候的文章，定义的和书中不是太一样， 注意体会之间的差异。\n$$\nI(f(x),y)=\\begin{cases}\n&1\\ if\\ yf(x)<0,\\\\\n&1\\ if\\ f(x)=0\\ and\\ y=-1,\\\\\n&0\\ otherwise\n\\end{cases}\n$$\n\n指示函数还有一种表示空心方括号，这个在$LaTeX$里面要用个包来引用， 不写了。在AdaBoost参考文献[9]中用了这样的表达。\n\n注意指示函数其实定义了0-1损失， 在AdaBoost算法的训练误差分析那部分，定理8.1实际上说的是指数损失是0-1损失的上界，然后用递推拿到了归一化系数连乘的形式。\n\n### $L_p$距离\n\n$P_{38}$\n\n### 启发式方法\n\n$P_{57}$决策树学习通常采用启发式方法，得到的决策树是次最优的。\n\n### 单纯形\n\n$P_{81}$，$P_{96}$单纯形是$n$维欧式空间中的$n+1$个仿射无关的点的集合的凸包。\n$P_{348}$ 模型的参数分布可以由参数空间中的单纯形表示。\n$P_{344}$ 单词单纯形与话题单纯形 \n\n### 熵，条件熵\n\n$P_{60}$在决策树中首先提到\n\n$P_{80}$最大熵原理部分也有提到，并有引用到第五章中的内容\n\n$P_{166}$ $F$函数的定义中,有定义分布$\\hat P(Z)$的熵\n\n### KL散度\n\n$P_{332}$ 或者相对熵\n\n### 特征函数\n\n$P_{82}$ $f(x,y)$描述输入$x$和输出$y$之间的某一事实。\n\n$P_{196}$ 转移特征和状态特征\n\n### 特征值分解\n$P_{314}$ 将矩阵分解成特征值和特征向量。特征值说明特征的重要度，书中也说是主成分的方差贡献率。但是特征值分解要求矩阵是方阵。\n\n### 动态规划\n\n$P_{67}$决策树的剪枝算法可以由一种动态规划的算法实现。\n\n$P_{184}$维特比算法实际上是用动态规划求解隐马尔可夫模型预测问题，即用动态规划求概率最大路径。\n\n### 贝叶斯估计\n$P_{59}$ 强调朴素贝叶斯和贝叶斯估计是不同的概念。\n\n### 目标函数\n\n$P_9$在经验风险最小化的策略或者结构风险最小化策略的情况下，经验或结构风险函数是最优化的目标函数\n\n### 函数间隔\n\n$P_{27},P_{97}$在[感知机](CH02/README.md)和[支持向量机](CH07/README.md)部分，都有函数间隔的概念，在[AdaBoost](CH08/README.md)部分，实际上也有间隔的概念在里面。\n\n### 概率分布密度\n\n$P_{162}$ 高斯分布密度, 书中的内容扩展下去看二维混合高斯模型, 对协方差矩阵的理解会有帮助.\n\n### 多项分布\n\n$P_{385}$ 多项分布定义\n$P_{340}$ 条件概率分布属于多项分布\n\n### 二项分布\n$P_{388}$ \n\n### 指数族分布\n$P_{389}$ 狄利克雷分布属于指数族分布\n\n### 对数似然损失\n\n$P_7$ 对数损失函数或者对数似然损失函数 $L(Y,P(Y|X))=-\\log P(Y|X)$\n\n### 对数似然函数\n\n$P_{158}$ 面对一个含有隐变量的概率模型, 目标是极大化观测数据(不完全数据)Y关于参数$\\theta$的对数似然函数, 即极大化\n$$\n\\begin{aligned}L(\\theta)=&\\log P(Y|\\theta)=\\log \\sum_Z P(Y, Z|\\theta) \\\\\n=&\\log\\left(\\sum_ZP(Y|Z,\\theta)P(Z|\\theta)\\right)\n\\end{aligned}\n$$\n\n### 对数线性模型\n\n$P_{196}$ 线性链条件随机场是对数线性模型。\n\n$P_{88}$ 最大熵模型与逻辑斯谛回归有类似的形式，它们又称为对数线性模型。\n\n### One-hot Encoding\n\n$P_{163}$ 注意这里书中没有明确的说明$\\gamma_{jk}$是One-hot encoding, 也叫做1-of-K representation\n\n$\\gamma_j=\\sum_{k=1}^K\\gamma_{jk}=1, j=1,2,3,\\dots, n$\n\n### 基函数\n\n$P_{144}$\n\n### 基本分类器\n\n$P_{147}$ 上面这两个不是一个概念\n\n### 琴声不等式\n\n$P_{159}$ EM算法导出部分讨论收敛性\n$P_{455}$ KL散度定义\n$P_{90}$ IIS算法导出部分确定界\n\n### 约束最优化问题\n\n$P_{83}$ 最大熵模型的学习可以形式化为约束最优化问题。\n\n$P_{302}$ 求解主成分的过程是求解约束最优化问题\n\n### 广义拉格朗日函数\n### 拉格朗日乘子法\n$P_{301}$ 采用拉格朗日乘子法求主成分。\n\n### 泛化误差上界\n\n$P_{15}$\n\n### 代理损失函数\n\n$P_{115}$\n\n$P_{213}$也有说明\n\n$P_{206}$预测最优解，条件概率最大的输出序列(标记序列)$y^*$\n\n### 极大似然估计\n\n$P_{9}$ 极大似然估计是经验风险最小化的例子。这个书中没有太多的解释，在《深度学习》里面有讲解，其实挺多书上都有提到。扩展下这个点，最大似然这个思想最早是高斯提出来的，費希尔将其发扬光大。1922年的文章60多页，提出了最大似然估计这个思想，讨论了一些性质。文章可以找到，費希尔凭借这个方法彻底撼动了皮尔逊的统治地位。\n\n費希尔是英国统计学家，生物进化学家，数学家，遗传学家和优生学家。看头像还真是个可以靠颜值度日却不小心坠入学术的帅哥。\n\n### 充分统计量\n\n$P_{456}$ \n\n### 无偏估计\n\n$P_{320}$ \n\n### 向量空间模型\nVector Space Model， VSM\n\n$P_{322}$ \n\n### 仿射函数\n\n$P_{116}$\n\n### 线性变换\n\n$P_{279}$ 线性变换很重要，在[SVD](./CH15/README.md)中第一次提到。\n$P_{300}$ 在总体主成分的定义中也提到了线性变换，这真的是线性代数中一个非常重要的概念。\n\n### 张成\n\n$P_{451}$ 向量空间\n$P_{325}$ 张成话题空间向量\n\n### 因子负荷量\n\n$P_{305}$ 第$k$个主成分$y_k$与变量$x_i$的相关系数$\\rho(y_k,x_i)$称为因子负荷量，表示第$k$个主成分$y_k$与变量$x_i$的相关关系。\n\n### 方差贡献率\n$_{308}$ 第$k$主成分$y_k$的方差贡献率定义为$y_k$的方差与所有方差之和的比值，记作$\\mu_k$\n\n### EM算法\n$P_{345}$ PLSA也是含有隐变量的模型，通常使用EM算法求解。\n$P_{401}$ 变分EM算法\n### 文本集合\n### 单词文本矩阵\n### 词向量\n$P_{321}$ \n### 非负矩阵分解\n$P_{321}$ \n\n### 反射变换\n\n$P_{279}$\n\n### 正交变换\n\n$P_{297}$ 把线性相关的变量表示的观测数据转换成少数几个线性无关变量表示的数据，线性无关的变量称为主成分。\n\n### 正交矩阵\n\n$P_{304}$ 正交矩阵满足$A^\\mathrm{T}A=AA^\\mathrm{T}=I$\n\n### 分块\n\n$P_{288}$\n\n### Manifold\n\n$P_{247}$ 在降维部分有提到，但是没有展开\n\n\n\n## Definition, Theory, Algorithm\n\n### Definition\n\n定义2.1 感知机\n\n定义2.2 数据集的线性可分性\n\n定义5.1 决策树\n\n定义5.2 信息增益\n\n定义5.3 信息增益比\n\n定义5.4 基尼指数\n\n定义6.1 逻辑斯谛分布\n\n定义6.2 逻辑斯谛回归模型\n\n定义6.3 最大熵模型\n\n定义7.1 线性可分支持向量机\n\n定义7.2 函数间隔\n\n定义7.3 几何间隔\n\n定义7.4 支持向量\n\n定义7.5 线性支持向量机\n\n定义7.6 核函数\n\n定义7.7 正定核的等价定义\n\n定义7.8 非线性支持向量机\n\n定义9.1 Q函数\n\n定义9.2 高斯混合模型\n\n定义9.3 F函数\n\n定义10.1 隐马尔可夫模型\n\n定义10.2 前向概率\n\n定义10.3 后向概率\n\n定义11.1 概率无向图模型\n\n定义11.2 团与最大团\n\n定义11.3 条件随机场\n\n定义11.4 线性链条件随机场\n\n定义16.1 总体主成分\n定义16.2 主成分的方差贡献率\n定义16.3 主成分对原有变量的贡献率\n\n### Theory\n\n定理2.1 Novikoff\n\n定理7.1 最大间隔分离超平面的存在唯一性\n\n定理7.2 \n\n定理7.3\n\n定理7.4\n\n定理7.5 正定核的充要条件\n\n定理7.6\n\n定理8.1 AdaBoost的训练误差界\n\n定理8.2 二类分类问题AdaBoost的训练误差界\n\n定理8.3 \n\n定理9.1\n\n定理9.2\n\n引理9.1\n\n引理9.2 \n\n定理9.3\n\n定理9.4\n\n定理11.1 Hammersley-Clifford定理\n\n定理11.2 线性链条件随机场的参数化形式\n定理16.1\n\n定理16.2\n\n定理16.3\n\n定理C.1\n\n推论C.1\n\n定理C.2\n\n定理C.3\n\n### Algorithm\n\n算法2.1 感知机学习算法的原始形式\n\n算法3.1 k近邻算法\n\n算法3.2 构造平衡kd树\n\n算法3.3 用kd树的最近邻搜索\n\n算法4.1 朴素贝叶斯算法\n\n算法5.1 信息增益的算法\n\n算法5.2 ID3算法\n\n算法5.3 C4.5的生成算法\n\n算法5.4 树的剪枝算法\n\n算法5.5 最小二乘回归树生成算法\n\n算法5.6 CART生成算法\n\n算法5.7 CART剪枝算法\n\n算法6.1 改进的迭代尺度算法 IIS\n\n算法6.2 最大熵模型学习的BFGS算法\n\n算法7.1 线性可分支持向量机学习算法-最大间隔法\n\n算法7.2 线性可分支持向量机学习算法\n\n算法7.3 线性支持向量机学习算法\n\n算法7.4 非线性支持向量机学习算法\n\n算法7.5 SMO算法\n\n算法8.1 AdaBoost\n\n算法8.2 前向分步算法\n\n算法8.3 回归问题的提升树算法\n\n算法8.4 梯度提升算法\n\n算法9.1 EM算法\n\n算法9.2 高斯混合模型参数估计的EM算法\n\n算法9.3 GEM算法1\n\n算法9.4 GEM算法2\n\n算法9.5 GEM算法3\n\n算法10.1 观测序列的生成\n\n算法10.2 观测序列概率的前向算法\n\n算法10.3 观测序列概率的后向算法\n\n算法10.4 Baum-Welch算法\n\n算法10.5 维特比算法\n\n算法11.1 条件随机场模型学习的改进的迭代尺度法\n\n算法11.2 条件随机场模型学习的BFGS算法\n\n算法11.3 条件随机场预测的维特比算法\n\n算法17.1 非负矩阵分解的迭代算法\n\n算法A.1 梯度下降法\n\n算法B.1 牛顿法\n\n算法B.2 DFP算法\n\n算法B.3 BFGS算法\n\n\n\n\n\n\n\n​\t"
        },
        {
          "name": "math_markdown.md",
          "type": "blob",
          "size": 10.486328125,
          "content": "# Markdown Cheat Sheet\n![Hits](https://www.smirkcao.info/hit_gits/Lihang/math_markdown.md)\n\n[TOC]\n\n## 数学相关LaTeX表达\n\n## 前言\n\n后面介绍的内容是$\\LaTeX$排版的数学符号的内容，不止一次有人强调中文版的Wikipedia并不是英文版内容的翻译，并不是。可以对比下参考部分的两个页面，我觉得中文页面做的不错，这两个页面里面关于垂直，貌似有那么点不同。\n\n### 表 1: 数学模式重音符\n\n| 示例 | 代码 | 示例 | 代码 | 示例 | 代码 | 示例 | 代码 |\n| - | - | - | - | - | - | - | - |\n| $\\hat {a}$ | \\hat{a} | $\\check{a}$ | \\check{a} | $\\tilde{a}$ | \\tilde{a} | $\\acute{a}$ | \\acute{a} |\n| **$\\grave{a}$ ** | \\grave{a} | **$\\dot {a}$ **    | \\dot{a}     | **$\\bar{a}$ **  | \\bar{a} | $\\ddot a$ | **\\ddot{a}**  |\n| **$\\vec {a}$ **  | \\vec{a} | **$\\widehat{A}$ ** | \\widehat{A} | $\\widetilde{A}$ | **\\widetilde{A}** | $\\breve a$  | **\\breve{a}** |\n|  |  | $\\hat{A}$ | \\hat{A} | $\\tilde{A}$ | \\tilde{A} |  |  |\n\n### 表2: 小写希腊字母\n\n|示例|代码|示例|代码|示例|代码|示例|代码|\n|-|-|-|-|-|-|-|-|\n| $\\alpha$ | \\alpha | $\\theta$ | **\\theta** | $\\upsilon$ | \\upsilon | $o $ | o |\n| $\\beta$ | **\\beta** | $\\vartheta$ | **\\vartheta** | $\\pi$ | **\\pi** | $\\phi$   | **\\phi** |\n| $\\gamma$ | **\\gamma** | $\\iota$ | **\\iota** | $\\varpi$ | **\\varpi** | $\\varphi$ | **\\varphi** |\n| $\\delta$ | **\\delta** | $\\kappa$ | **\\kappa** | $\\rho$ | **\\rho** | $\\chi$ | **\\chi** |\n| $\\epsilon$ | **\\epsilon** | $\\lambda$ | **\\lambda** | $\\varrho$ | **\\varrho** | $\\psi$ | **\\psi** |\n| $\\varepsilon$ | **\\varepsilon** | $\\mu$ | **\\mu** | $\\sigma$ | **\\sigma** | $\\omega$ | **\\omega** |\n| $\\zeta$ | **\\zeta** | $\\nu$ | **\\nu** | $\\varsigma$ | **\\varsigma** | $\\nabla$ | **\\nabla** |\n| $\\eta$ | **\\eta** | $\\xi$ | **\\xi** | $\\tau$ | **\\tau** | | |\n\n### 表 3: 大写希腊字母\n\n| 示例 |代码 |示例|代码|示例|代码|示例|代码|\n| - | - | - | - | - | - | - | - |\n| $\\Gamma$ | **\\Gamma** | $\\Lambda$ | **\\Lambda** | $\\Sigma\\mit\\Sigma$ | **\\Sigma\\mit\\Sigma** | $\\Psi$ | **\\Psi** |\n| $\\Delta$ | **\\Delta** |$\\Xi$ | **\\Xi** | $\\Upsilon$ | **\\Upsilon** | $\\Omega\\mit\\Omega$ | **\\Omega\\mit\\Omega** |\n| $\\Theta$ | **\\Theta** | $\\Pi$ | **\\Pi** | $\\Phi$ | **\\Phi** |  | |\n\n### 表 4: 数学字母\n\n| 示例 | 代码 |\n| - | - |\n| $\\mathbf {ABCdefxyzXYZ123}$ | \\mathbf {ABCdefxyzXYZ123} |\n| $\\mathrm {ABCdefxyzXYZ123}$ | \\mathrm {ABCdefxyzXYZ123} |\n| $\\mathit {ABCdefxyzXYZ123}$ | \\mathit {ABCdefxyzXYZ123} |\n| $\\mathcal {ABCdefxyzXYZ123}$ | \\mathcal {ABCdefxyzXYZ123} |\n| $\\mathscr {ABCdefxyzXYZ123}$ | \\mathscr {ABCdefxyzXYZ123} |\n| $\\mathfrak {ABCdefxyzXYZ123}$ | \\mathfrak {ABCdefxyzXYZ123} |\n| $\\mathbb {ABCdefxyzXYZ123}$ | \\mathbb {ABCdefxyzXYZ123} |\n| $\\boldsymbol{ABCdefxyzXYZ123}$ | \\boldsymbol{ABCdefxyzXYZ123} |\n\n### 表 5: 运算符与函数\n\n| 示例 | 代码 | 示例 | 代码 | 示例 | 代码 |\n| - | - | - | - | - | - |\n| $\\sum$ | \\sum | $\\prod$ | \\prod | $x\\cdot{y}$ | x\\cdot{y} |\n| $\\bigcup$ | \\bigcup | $\\bigoplus$ | \\bigoplus | $x\\times {y}$ | x\\times {y} |\n| $\\bigvee$ | \\bigvee | $\\bigcap$ | \\bigcap| $\\left\\|w\\right\\|$ | **\\left\\\\|w\\right\\|** |\n| $\\bigwedge$ | \\bigwedge  | $\\biguplus$ | \\biguplus | $\\iiint$ | \\iiint |\n| $\\bigotimes$ | \\bigotimes | $\\oint$ | \\oint | $\\iint$ | \\iint |\n| $\\int x\\,{\\rm d}x$ | \\int x\\,\\{\\rm d}x | $\\bigsqcup$ | \\bigsqcup | $\\lgroup \\rgroup$ | \\lgroup \\rgroup |\n| $\\coprod$ | \\coprod | $\\bigodot$ | \\bigodot  | $\\partial$ | \\partial |\n| $\\det$ | **\\det** | $\\max$ | \\max | $\\min$ | \\min |\n| $\\log$ | \\log |  |  |  |  |\n\n### 表 6: 常用箭头\n\n| 示例 | 代码 | 示例 | 代码 | 示例 | 代码 |\n| - | - | - | - | - | - |\n| $\\leftarrow$ | \\leftarrow | $\\rightarrow$ | \\rightarrow | $\\leftrightarrow$ | \\leftrightarrow |\n| $\\longleftarrow$ | \\longleftarrow | $\\longrightarrow$ | \\longrightarrow | $\\longleftrightarrow$  | \\longleftrightarrow |\n| $\\Leftarrow$ | \\Leftarrow | $\\Rightarrow$ | \\Rightarrow | $\\Leftrightarrow$ | \\Leftrightarrow |\n| $\\Longleftarrow$ | \\Longleftarrow | $\\Longrightarrow$ | \\Longrightarrow | $\\Longleftrightarrow$ | \\Longleftrightarrow |\n| $\\uparrow$ | \\uparrow | $\\downarrow$ | \\downarrow | $\\updownarrow$ | \\updownarrow |\n\n### 表 7: 其他常用符号\n\n| 示例 | 代码 | 示例 | 代码 | 示例 | 代码 |\n| - | - | - | - | - | - |\n|$\\therefore$ | \\therefore | $\\because$ | \\because  | $\\min \\limits_{f \\in {H}}$ | \\min \\limits_{f \\in {H}} |\n| $\\leqslant$ | \\leqslant | $\\geqslant$ | \\geqslant | $\\cal {C} \\equiv 1$ | \\equiv |\n| $\\thickapprox$ | \\thickapprox | $\\thicksim \\sim$ | \\thicksim \\sim | $\\left(\\frac{A}{B}\\right)$ | \\left(\\frac{A}{B}\\right) |\n| $\\neq$ | \\neq | $\\in$ | \\in | $\\hat{=}$ | \\hat{=} |\n| $\\pm$   | \\pm | $\\sqrt{a}$ | \\sqrt{a} | $\\geq \\leq$ | \\geq \\leq |\n| $\\perp $ | **\\perp** | $\\angle$ | \\angle | $\\varpropto$ | \\varpropto |\n| $\\infty$ | \\infty | $g^\\prime$ | g^\\prime | $\\forall$ | \\forall |\n| $\\exist$ | \\exist | $\\bot$ | **\\bot** | $\\top$ | **\\top** |\n\n注意**\\bot**和**\\perp**的区别，垂直是**\\perp**\n\n### 表8: 使用字体\n\n| 示例 | 代码 | 备注 |\n| - | - | - |\n| $\\rm {ABCdefxyzXYZ123}$ | \\rm {ABCdefXYZ123} | 罗马体 |\n| $\\it{ABCdefxyzXYZ123}$ | \\it{ABCdefXYZ123} | 意大利体 |\n| $\\bf{ABCdefxyzXYZ123}$ | \\bf{ABCdefXYZ123} | 正粗体，黑体 |\n| $\\cal {ABCdefxyzXYZ123}$ | \\cal {ABCdefXYZ123} | 花体 |\n| $\\sf{ABCdefXYZ123}$ | \\sf{ABCdefXYZ123} | 等线体 |\n| $\\mit{ABCdefxyzXYZ123}$ | \\mit{ABCdefXYZ123} | **数字斜体** |\n| $\\tt{ABCdefxyzXYZ123}$ | \\tt{ABCdefXYZ123} | 打印机字体 |\n\n### 表9: 分段函数与公式对齐\n\n#### 分段函数\n\n示例\n$$\nf(x,y) = \\begin{cases}\n1 & x与y满足某一事实\\\\\n0 & 否则\n\\end{cases}\n$$\n\n```latex\n# 代码\nf(x,y) = \\begin{cases}\n1 & x与y满足某一事实\\\\\n0 & 否则\n\\end{cases}\n```\n\n$$\n\\begin{aligned}\nL(w)&=\\sum\\limits^{N}_{i=1}[y_i\\log\\pi(x_i)+(1-y_i)\\log(1-\\pi(x_i))]\\\\&=\\sum\\limits^{N}_{i=1}[y_i\\log{\\frac{\\pi(x_i)}{1-\\pi(x_i)}}+\\log(1-\\pi(x_i))]\\\\&=\\sum\\limits^{N}_{i=1}[y_i(w\\cdot x_i)-\\log(1+\\exp(w\\cdot{x_i})]\n\\end{aligned}\n$$\n\n#### 对齐控制\n\n```latex\n# 代码\n# 通过\\begin{aligned}\\end{aligned}控制对齐, 使用&表示对齐点.\n\\begin{aligned}\nL(w)&=\\sum\\limits^{N}_{i=1}[y_i\\log\\pi(x_i)+(1-y_i)\\log(1-\\pi(x_i))]\\\\\n&=\\sum\\limits^{N}_{i=1}[y_i\\log{\\frac{\\pi(x_i)}{1-\\pi(x_i)}}+\\log(1-\\pi(x_i))]\\\\\n&=\\sum\\limits^{N}_{i=1}[y_i(w\\cdot x_i)-\\log(1+\\exp(w\\cdot{x_i})]\n\\end{aligned}\n```\n\n另外注意到前面的分段函数自动变好了，但是上面多行对齐的公式没有自动编号，如果需要**自动**编号，外面嵌入equation\n$$\n\\begin{equation}\n\\begin{aligned}\nL(w)&=\\sum\\limits^{N}_{i=1}[y_i\\log\\pi(x_i)+(1-y_i)\\log(1-\\pi(x_i))]\\\\\n&=\\sum\\limits^{N}_{i=1}[y_i\\log{\\frac{\\pi(x_i)}{1-\\pi(x_i)}}+\\log(1-\\pi(x_i))]\\\\\n&=\\sum\\limits^{N}_{i=1}[y_i(w\\cdot x_i)-\\log(1+\\exp(w\\cdot{x_i})]\n\\end{aligned}\n\\end{equation}\n$$\n代码如下\n\n```latex\n\\begin{equation}\n\\begin{aligned}\nL(w)&=\\sum\\limits^{N}_{i=1}[y_i\\log\\pi(x_i)+(1-y_i)\\log(1-\\pi(x_i))]\\\\&=\\sum\\limits^{N}_{i=1}[y_i\\log{\\frac{\\pi(x_i)}{1-\\pi(x_i)}}+\\log(1-\\pi(x_i))]\\\\&=\\sum\\limits^{N}_{i=1}[y_i(w\\cdot x_i)-\\log(1+\\exp(w\\cdot{x_i})]\n\\end{aligned}\n\\end{equation}\n```\n\n#### 公式编号\n\n关于编号也可以通过行间公式做如下表达\n$$\n\\begin{align}\nL(w)&=\\sum\\limits^{N}_{i=1}[y_i\\log\\pi(x_i)+(1-y_i)\\log(1-\\pi(x_i))]\\\\\n&=\\sum\\limits^{N}_{i=1}[y_i\\log{\\frac{\\pi(x_i)}{1-\\pi(x_i)}}+\\log(1-\\pi(x_i))]\\nonumber\\\\\n&=\\sum\\limits^{N}_{i=1}[y_i(w\\cdot x_i)-\\log(1+\\exp(w\\cdot{x_i})]\n\\end{align}\n$$\n代码如下\n\n```latex\n\\begin{align}\nL(w)&=\\sum\\limits^{N}_{i=1}[y_i\\log\\pi(x_i)+(1-y_i)\\log(1-\\pi(x_i))]\\\\\n&=\\sum\\limits^{N}_{i=1}[y_i\\log{\\frac{\\pi(x_i)}{1-\\pi(x_i)}}+\\log(1-\\pi(x_i))]\\nonumber\\\\\n&=\\sum\\limits^{N}_{i=1}[y_i(w\\cdot x_i)-\\log(1+\\exp(w\\cdot{x_i})]\n\\end{align}\n```\n\n以上代码有两点需要注意体会：\n\n1. align\n1. \\nonumber的使用\n\n### 表X: 矩阵\n\n#### 普通矩阵\n\n$$\n\\begin{aligned}\nM_1(x)=\n\\begin{bmatrix}\n&a_{01}&a_{02}\\\\\n&0&0\n\\end{bmatrix}\n&,M_2(x)=\n\\begin{bmatrix}\n&b_{11}&b_{12}\\\\\n&b_{21}&b_{22}\n\\end{bmatrix}\n\\\\\nM_3(x)=\n\\begin{bmatrix}\n&c_{11}&c_{12}\\\\\n&c_{21}&c_{22}\n\\end{bmatrix}\n&,M_4(x)=\n\\begin{bmatrix}\n&1&0\\\\\n&1&0\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n```latex\n\\begin{aligned}\nM_1(x)=\n\\begin{bmatrix}\n&a_{01}&a_{02}\\\\\n&0&0\n\\end{bmatrix}\n&,M_2(x)=\n\\begin{bmatrix}\n&b_{11}&b_{12}\\\\\n&b_{21}&b_{22}\n\\end{bmatrix}\n\\\\\nM_3(x)=\n\\begin{bmatrix}\n&c_{11}&c_{12}\\\\\n&c_{21}&c_{22}\n\\end{bmatrix}\n&,M_4(x)=\n\\begin{bmatrix}\n&1&0\\\\\n&1&0\n\\end{bmatrix}\n\\end{aligned}\n```\n\n#### 带省略符号的Matrix\n\n$$\nX^\\mathrm T=\n\\left[\n\\begin{matrix}\n x_{11} & \\cdots & x_{1N}       \\\\\n \\vdots & \\ddots & \\vdots \t\t\\\\\n x_{M1} & \\cdots & x_{MN}       \\\\\n\\end{matrix}\n\\right]\n$$\n\n```latex\n% 这里稍微注意下转置符号， 《统计学习方法》中的转置用的是正体的T\n% 可以参考 https://zhuanlan.zhihu.com/p/27490955 中关于转置写法的讨论。\nX^\\mathrm T=\n\\left[\n\\begin{matrix}\n x_{11} & \\cdots & x_{1N}       \\\\\n \\vdots & \\ddots & \\vdots \t\t\\\\\n x_{M1} & \\cdots & x_{MN}       \\\\\n\\end{matrix}\n\\right]\n```\n\n#### 向量\n\n$$\n\\left[\n\\begin{array}\n\\\\2\n\\\\3\n\\end{array}\n\\right]\n$$\n\n```latex\n\\left[\n\\begin{array}\n\\\\2\n\\\\3\n\\end{array}\n\\right]\n```\n\n\n\n$$\n\\overbrace{abcde}\\underbrace{fghij}_{comment}\\overline{klmn}\\underline{opqr}\\overleftarrow{stuv}\\overrightarrow{wxyz}\n$$\n\n```tex\n\\overbrace{abcde}\\underbrace{fghij}_{comment}\\overline{klmn}\\underline{opqr}\\overleftarrow{stuv}\\overrightarrow{wxyz}\n```\n\n\n\n## Emoji\n\n### 表XI: Emoji\n\n| :smirk: smirk          | :smile:smile | :laughing:laughing | :blush:blush       | :smiley:smiley | :heart_eyes:heart_eyes |\n| ---------------------- | ------------ | ------------------ | ------------------ | -------------- | ---------------------- |\n| 😘kissing_heart         | :wink:wink   | :kissing:kissing   | :confused:confused | :sweat:sweat   | :joy:joy               |\n| :sob:sob               | :cry:cry     | :angry:angry       | :yum:yum           | :mask:mask     | :sunglasses:sunglasses |\n| :heartpulse:heartpulse | :alien:alien | :cupid:cupid       | :+1:+1             | :cn:cn         | :shit:shit             |\n\n\n\n## Refs\n\n1. [Markdown 数学符号速查](https://www.cnblogs.com/blog4ljy/p/9066624.html)\n2. [Cmd Markdown公式指导手册](https://www.zybuluo.com/codeep/note/163962)\n3. [Equals_Sign](https://en.wikipedia.org/wiki/Equals_sign#Other_related_symbols)\n4. [Emoji](https://gist.github.com/rxaviers/7360908)\n5. [Short Math Guide for LaTeX](http://ctan.math.utah.edu/ctan/tex-archive/info/short-math-guide/short-math-guide.pdf)\n6. [List of Mathematical Symbols](https://en.wikipedia.org/wiki/List_of_mathematical_symbols)\n7. [数学公式](https://zh.wikipedia.org/wiki/Help:数学公式)\n8. [Matplotlib Math Text](https://matplotlib.org/tutorials/text/mathtext.html)\n\n"
        },
        {
          "name": "notebook",
          "type": "tree",
          "content": null
        },
        {
          "name": "ref_downloader.sh",
          "type": "blob",
          "size": 5.8974609375,
          "content": "mkdir refs\ncd refs\necho \"CH01\"\n# 0101 ESL\necho \"same with 0704\"\n# 0102 PRML\nwget \"https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\" -O 0102.pdf\n# 0103 PGM\nwget \"https://github.com/Zhenye-Na/machine-learning-uiuc/blob/master/docs/Probabilistic%20Graphical%20Models%20-%20Principles%20and%20Techniques.pdf\" -O 0103.pdf\n# 0104 DL\nwget \"https://github.com/Zhenye-Na/cs446/blob/master/docs/Deep%20Learning.pdf\" -O 0104.pdf\n\necho \"ch02\"\n# 0201\nwget \"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf\" -O 0201.pdf\n# 0202\nwget \"https://cs.uwaterloo.ca/~y328yu/classics/novikoff.pdf\" -O 0202.pdf\n# 0205\nwget \"https://cseweb.ucsd.edu/~yfreund/papers/LargeMarginsUsingPerceptron.pdf\" -O 0205.pdf\n# 0206\nwget \"https://www.researchgate.net/profile/Yaoyong_Li/publication/37537459_The_Perceptron_Algorithm_with_Uneven_Margins/links/0046351790be7e102e000000/The-Perceptron-Algorithm-with-Uneven-Margins.pdf\" -O 0206.pdf\n# 0207\nwget \"https://web.stanford.edu/class/ee373b/30years.pdf\" -O 0207.pdf\necho \"CH03\"\n\necho \"ch04\"\n# 0401\nwget \"http://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf\" -O 0401.pdf\n# 0402\necho \"same with 0704\"\n# 0403 PRML\necho \"same with 0102\"\n\necho \"ch06\"\n# 0601\nwget \"http://www.aclweb.org/anthology/J96-1002\" -O 0601.pdf\n# 0602\n# http://www.cs.cmu.edu/afs/cs/user/aberger/www/ps/scaling.ps\nwget \"http://www.cs.cmu.edu/~aberger/pdf/scaling.pdf\" -O 0602.pdf\n# 0603\necho \"same with 0704\"\n# 0604\necho \"a book\"\n# 0605\nwget \"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.135.1357&rep=rep1&type=pdf\" -O 0605.pdf\n# 0606\nwget \"https://alex.smola.org/papers/2005/CanSmo05.pdf\" -O 0606.pdf\n\necho \"ch07\"\n# 0701\nwget \"http://image.diku.dk/imagecanon/material/cortes_vapnik95.pdf\" -O 0701.pdf\n# 0702\nwget \"http://www.svms.org/training/BOGV92.pdf\" -O 0702.pdf\n# 0704 这个是原书英文第二版\nwget \"https://statisticalsupportandresearch.files.wordpress.com/2017/05/vladimir-vapnik-the-nature-of-statistical-learning-springer-2010.pdf\" -O 0704.pdf\n# 0705, 这个也有几个版本，微软这个版本链接也有过变更\nwget \"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/smo-book.pdf\" -O 0705.pdf\n# 0715\nwget \"http://www.kernel-machines.org/publications/pdfs/0701907.pdf\" -O 0715.pdf\n\necho \"ch08\"\n# 0801\nwget https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf -O 0801.pdf\n# 0802 ESLII \nwget \"https://web.stanford.edu/~hastie/Papers/ESLII.pdf\" -O 0802.pdf\n# 0803 PAC\nwget \"https://people.mpi-inf.mpg.de/~mehlhorn/SeminarEvolvability/ValiantLearnable.pdf\" -O 0803.pdf\n# 0804\nwget \"http://rob.schapire.net/papers/strengthofweak.pdf\" -O 0804.pdf\n# 0805 这个文章很多版本， 96年也有发表\nwget http://www.dklevine.com/archive/refs4570.pdf -O 0805.pdf\n# 0806 这是个特邀文章\nwget \"https://web.stanford.edu/~hastie/Papers/AdditiveLogisticRegression/alr.pdf\" -O 0806.pdf\n# 0807\nwget https://statweb.stanford.edu/~jhf/ftp/trebst.pdf -O 0807.pdf\n# 0808\nwget \"https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1999-ML-Improved%20boosting%20algorithms%20using%20confidence-rated%20predictions%20(Schapire%20y%20Singer).pdf\" -O 0808.pdf\n# 0809\nwget \"https://link.springer.com/content/pdf/10.1023%2FA%3A1013912006537.pdf\" -O 0809.pdf \necho \"ch09\"\n# 0901\nwget \"http://web.mit.edu/6.435/www/Dempster77.pdf\" -O 0901.pdf\n# 0902 ESLII\necho \"same with 0704\"\n# 0903 Book 2007 second edition no link\n\n# 0905\nwget \"https://projecteuclid.org/download/pdf_1/euclid.aos/1176346060\" -O 0905.pdf\n\n# 0906\nwget \"http://www.cs.toronto.edu/~fritz/absps/emk.pdf\" -O 0906.pdf\n\necho \"ch10\"\n# 1001\nwget \"http://ai.stanford.edu/~pabbeel/depth_qual/Rabiner_Juang_hmms.pdf\" -O 1001.pdf\n\n# 1002\nwget \"https://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf\" -O 1002.pdf\n\n# 1003 BW Algorithm\nwget \"https://projecteuclid.org/download/pdf_1/euclid.aoms/1177697196\" -O 1003.pdf\n\n# 1004\nwget \"http://101.96.10.64/ssli.ee.washington.edu/people/bilmes/mypapers/em.pdf\" -O 1004.pdf\necho \"CH11\"\n# 1101 PRML\necho \"same with 0403\"\n# 1102 PGM, Koller\necho \"visit this link to download pdf file: http://vk.com/doc168073_304660839?hash=39a33dd8aa6b141d8a&dl=b667454bc650f66cc0\"\n# 1103\nwget \"http://www.cs.cmu.edu/~mccallum/papers/crf-icml2001s.ps.gz\" -O 1103.pdf\n# 1104\nwget \"http://aclweb.org/anthology/N03-1028\" -O 1104.pdf\n# 1105\nwget \"http://www.ai.mit.edu/courses/6.891-nlp/READINGS/maxent.pdf\" -O 1105.pdf\n# 1106\nwget \"https://www.robots.ox.ac.uk/~vgg/rg/papers/maxmarginmarkovnetworks.pdf\" -O 1106.pdf\n# 1107\nwget \"https://icml.cc/Conferences/2004/proceedings/papers/76.pdf\" -O 1107.pdf\n\necho \"CH14\"\n# 1401\nwget \"https://homepages.inf.ed.ac.uk/rbf/BOOKS/JAIN/Clustering_Jain_Dubes.pdf\" -O 1401.pdf\n# 1402\n# 1403\nwget \"https://pdfs.semanticscholar.org/a718/b85520bea702533ca9a5954c33576fd162b0.pdf\" -O 1403.pdf\n# 1404\n# 1405\n# 1406\n# 1407\n# 1408\necho \"CH15\"\n# 1502\nwget \"https://github.com/J-Mourad/Introduction-to-Linear-Algebra-5th-Edition---EE16A/raw/master/Ed%205%2C%20Gilbert%20Strang%20-%20Introduction%20to%20Linear%20Algebra%20(2016%2C%20Wellesley-Cambridge%20Press).pdf\" -O 1502.pdf\n# 1503\nwget \"https://www.cs.utexas.edu/users/inderjit/public_papers/HLA_SVD.pdf\" -O 1503.pdf\n# 1505\nwget \"http://www.kolda.net/publication/TensorReview.pdf\" -O 1505.pdf\n\necho \"CH16\"\nwget \"https://arxiv.org/pdf/1404.1100.pdf\" -O 1604.pdf\necho \"CH17\"\n# 1701 LSA\nwget \"http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf\" -O 1701.pdf\n# 1702\n# 1703 NMF\nwget \"http://www.columbia.edu/~jwp2128/Teaching/E4903/papers/nmf_nature.pdf\" -O 1703.pdf\n# 1704 NMF NIPS 2001\nwget \"https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf\" -O 1704.pdf\n\necho \"CH18\"\n# 1801\nwget \"http://www.iro.umontreal.ca/~nie/IFT6255/Hofmann-UAI99.pdf\" -O 1801.pdf\n# 1802\nwget \"https://sigir.org/wp-content/uploads/2017/06/p211.pdf\" -O 1802.pdf\n# 1803\nwget \"http://www.cs.bham.ac.uk/~pxt/IDA/plsa.pdf\" -O 1803.pdf\n# 1804\nwget \"http://ranger.uta.edu/~chqding/papers/NMFpLSIequiv.pdf\" -O 1804.pdf\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.15234375,
          "content": "hmmlearn==0.2.0\njieba==0.39\nmatplotlib==2.2.2\nnumpy==1.14.5\npandas==0.23.3\npyparsing==2.2.0\nscikit-learn==0.19.2\nscipy==1.1.0\nstatsmodels==0.9.0\nsympy==1.2\n"
        },
        {
          "name": "symbol_index.md",
          "type": "blob",
          "size": 2.1591796875,
          "content": "# 符号表\n![Hits](https://www.smirkcao.info/hit_gits/Lihang/symbol_index.md)\n\n1. $\\{\\}$ 集合 文本集合$D=\\{d_1,d_2,\\cdots,d_n\\}$，单词集合$W=\\{w_1,w_2,\\cdots,w_m\\}$ $P_{327}$\n1. $A_G$类的样本散布矩阵 $P_{259}$\n1. $C^*$ 最优划分 $P_{261}$\n1. $D_G$ 类的直径\n1. $D=[d_{ij}]_{n \\times n}$ $n$个样本之间的距离矩阵$D$ $P_{261}$\n1. $D=\\{d_1,d_2,\\cdots,d_n\\}$ $n$个文本的集合 $P_{322}$\n1. $D(A||B)=\\sum\\limits_{i,j}\\left(a_{ij}\\log\\frac{a_{ij}}{b{ij}}-a_{ij}+b_{ij}\\right)$ 散度损失函数$P_{322}$\n1. $J(W,H)$ 优化目标函数。$P_{334}$\n1. $\\Lambda$ $n$阶对角矩阵\n1. $\\mathcal{M}$是$\\mathbf{R}^{m\\times n}$中所有秩不超过$k$的矩阵集合，$0<k<r$ $P_{287}$\n1. $m, M$ 样本特征数，维数 $P_{261}$\n1. $m$ 协方差矩阵的特征值之和 $P_{309}$\n1. $n,N,n_G$ 样本数，类的样本数\n1. $\\theta$ 参数\n1. $R(A)$ $A$的值域 $P_{275}$\n1. $R(A)^\\bot$ 表示$R(A)$的正交补 $P_{276}$\n1. $r$ 矩阵的秩 $P_{277}$\n1. $S_G$类的样本协方差矩阵 $P_{259}$\n1. $\\mathcal{S}$ 状态空间 $P_{360}$\n1. $T$ 训练数据集 $P_{59}$\n1. $T$ 和$V$给定的两个正数 $P_{259}$\n1. $T$ 决策树 $P_{78}$\n1. $T:x\\rightarrow Ax$ 线性变换 $P_{279}$\n1. $U$ 训练数据  $P_8, P_{248}, P_{245}$\n1. $U$ 表示$m$阶正交矩阵 ，$V$表示$n$阶正交矩阵，$\\mit\\Sigma$表示矩形对角矩阵，$P_{271}$\n1. $U_k=[u_1 u_2 \\cdots u_k]$中的每一个列向量$u_1, u_2, \\cdots, u_k$表示一个话题，称为话题向量。.\n1. $W$ 在非负矩阵分解中表示基矩阵 $P_{332}$\n1. $W(C)$ 能量，表示相同类中的样本的相似程度。越相似，越小。 $P_{264}$\n1. $W=A^\\mathrm TA$ 对称矩阵 $P_{282}$\n1. $W=\\{w_1,w_2,\\cdots, w_m\\}$ $m$个单词集合 $P_{322}$\n1. $\\mathcal{W}=\\{w_1,w_2,\\cdots, w_k\\}$ $k$个元素组成的集合 $P_{389}$\n1. $x_i^*$是$x_i$的规范化随机变量。 $P_{309}$\n1. $X=[x_{ij}]_{m\\times n}$ 矩阵\n1. $X=\\{x_1, x_2, \\dots ,x_n\\}$ $n$个样本的集合 $P_{263}$\n1. $X$ 定义在输入空间$\\mathcal X$上的随机向量\n1. $X=\\{X_0,X_1,\\cdots,\\X_t,\\cdots\\}$ 马尔可夫链 $P_{360}$\n1. $Y$ 定义在输出空间$\\mathcal Y$上的随机向量\n1. $\\mathcal{Z}$隐式结构空间 $P_8$"
        }
      ]
    }
  ]
}