{
  "metadata": {
    "timestamp": 1736561037280,
    "page": 810,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "amdegroot/ssd.pytorch",
      "stars": 5165,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.107421875,
          "content": "*.ipynb linguist-language=Python\n.ipynb_checkpoints/* linguist-documentation\ndev.ipynb linguist-documentation\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.4150390625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# IPython Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# dotenv\n.env\n\n# virtualenv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n\n# Rope project settings\n.ropeproject\n\n# atom remote-sync package\n.remote-sync.json\n\n# weights\nweights/\n\n#DS_Store\n.DS_Store\n\n# dev stuff\neval/\neval.ipynb\ndev.ipynb\n.vscode/\n\n# not ready\nvideos/\ntemplates/\ndata/ssd_dataloader.py\ndata/datasets/\ndoc/visualize.py\nread_results.py\nssd300_120000/\ndemos/live\nwebdemo.py\ntest_data_aug.py\n\n# attributes\n\n# pycharm\n.idea/\n\n# temp checkout soln\ndata/datasets/\ndata/ssd_dataloader.py\n\n# pylint\n.pylintrc"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0556640625,
          "content": "MIT License\n\nCopyright (c) 2017 Max deGroot, Ellis Brown\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.1923828125,
          "content": "# SSD: Single Shot MultiBox Object Detector, in PyTorch\nA [PyTorch](http://pytorch.org/) implementation of [Single Shot MultiBox Detector](http://arxiv.org/abs/1512.02325) from the 2016 paper by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang, and Alexander C. Berg.  The official and original Caffe code can be found [here](https://github.com/weiliu89/caffe/tree/ssd).\n\n\n<img align=\"right\" src= \"https://github.com/amdegroot/ssd.pytorch/blob/master/doc/ssd.png\" height = 400/>\n\n### Table of Contents\n- <a href='#installation'>Installation</a>\n- <a href='#datasets'>Datasets</a>\n- <a href='#training-ssd'>Train</a>\n- <a href='#evaluation'>Evaluate</a>\n- <a href='#performance'>Performance</a>\n- <a href='#demos'>Demos</a>\n- <a href='#todo'>Future Work</a>\n- <a href='#references'>Reference</a>\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\n## Installation\n- Install [PyTorch](http://pytorch.org/) by selecting your environment on the website and running the appropriate command.\n- Clone this repository.\n  * Note: We currently only support Python 3+.\n- Then download the dataset by following the [instructions](#datasets) below.\n- We now support [Visdom](https://github.com/facebookresearch/visdom) for real-time loss visualization during training!\n  * To use Visdom in the browser:\n  ```Shell\n  # First install Python server and client\n  pip install visdom\n  # Start the server (probably in a screen or tmux)\n  python -m visdom.server\n  ```\n  * Then (during training) navigate to http://localhost:8097/ (see the Train section below for training details).\n- Note: For training, we currently support [VOC](http://host.robots.ox.ac.uk/pascal/VOC/) and [COCO](http://mscoco.org/), and aim to add [ImageNet](http://www.image-net.org/) support soon.\n\n## Datasets\nTo make things easy, we provide bash scripts to handle the dataset downloads and setup for you.  We also provide simple dataset loaders that inherit `torch.utils.data.Dataset`, making them fully compatible with the `torchvision.datasets` [API](http://pytorch.org/docs/torchvision/datasets.html).\n\n\n### COCO\nMicrosoft COCO: Common Objects in Context\n\n##### Download COCO 2014\n```Shell\n# specify a directory for dataset to be downloaded into, else default is ~/data/\nsh data/scripts/COCO2014.sh\n```\n\n### VOC Dataset\nPASCAL VOC: Visual Object Classes\n\n##### Download VOC2007 trainval & test\n```Shell\n# specify a directory for dataset to be downloaded into, else default is ~/data/\nsh data/scripts/VOC2007.sh # <directory>\n```\n\n##### Download VOC2012 trainval\n```Shell\n# specify a directory for dataset to be downloaded into, else default is ~/data/\nsh data/scripts/VOC2012.sh # <directory>\n```\n\n## Training SSD\n- First download the fc-reduced [VGG-16](https://arxiv.org/abs/1409.1556) PyTorch base network weights at:              https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth\n- By default, we assume you have downloaded the file in the `ssd.pytorch/weights` dir:\n\n```Shell\nmkdir weights\ncd weights\nwget https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth\n```\n\n- To train SSD using the train script simply specify the parameters listed in `train.py` as a flag or manually change them.\n\n```Shell\npython train.py\n```\n\n- Note:\n  * For training, an NVIDIA GPU is strongly recommended for speed.\n  * For instructions on Visdom usage/installation, see the <a href='#installation'>Installation</a> section.\n  * You can pick-up training from a checkpoint by specifying the path as one of the training parameters (again, see `train.py` for options)\n\n## Evaluation\nTo evaluate a trained network:\n\n```Shell\npython eval.py\n```\n\nYou can specify the parameters listed in the `eval.py` file by flagging them or manually changing them.  \n\n\n<img align=\"left\" src= \"https://github.com/amdegroot/ssd.pytorch/blob/master/doc/detection_examples.png\">\n\n## Performance\n\n#### VOC2007 Test\n\n##### mAP\n\n| Original | Converted weiliu89 weights | From scratch w/o data aug | From scratch w/ data aug |\n|:-:|:-:|:-:|:-:|\n| 77.2 % | 77.26 % | 58.12% | 77.43 % |\n\n##### FPS\n**GTX 1060:** ~45.45 FPS\n\n## Demos\n\n### Use a pre-trained SSD network for detection\n\n#### Download a pre-trained network\n- We are trying to provide PyTorch `state_dicts` (dict of weight tensors) of the latest SSD model definitions trained on different datasets.  \n- Currently, we provide the following PyTorch models:\n    * SSD300 trained on VOC0712 (newest PyTorch weights)\n      - https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth\n    * SSD300 trained on VOC0712 (original Caffe weights)\n      - https://s3.amazonaws.com/amdegroot-models/ssd_300_VOC0712.pth\n- Our goal is to reproduce this table from the [original paper](http://arxiv.org/abs/1512.02325)\n<p align=\"left\">\n<img src=\"http://www.cs.unc.edu/~wliu/papers/ssd_results.png\" alt=\"SSD results on multiple datasets\" width=\"800px\"></p>\n\n### Try the demo notebook\n- Make sure you have [jupyter notebook](http://jupyter.readthedocs.io/en/latest/install.html) installed.\n- Two alternatives for installing jupyter notebook:\n    1. If you installed PyTorch with [conda](https://www.continuum.io/downloads) (recommended), then you should already have it.  (Just  navigate to the ssd.pytorch cloned repo and run):\n    `jupyter notebook`\n\n    2. If using [pip](https://pypi.python.org/pypi/pip):\n\n```Shell\n# make sure pip is upgraded\npip3 install --upgrade pip\n# install jupyter notebook\npip install jupyter\n# Run this inside ssd.pytorch\njupyter notebook\n```\n\n- Now navigate to `demo/demo.ipynb` at http://localhost:8888 (by default) and have at it!\n\n### Try the webcam demo\n- Works on CPU (may have to tweak `cv2.waitkey` for optimal fps) or on an NVIDIA GPU\n- This demo currently requires opencv2+ w/ python bindings and an onboard webcam\n  * You can change the default webcam in `demo/live.py`\n- Install the [imutils](https://github.com/jrosebr1/imutils) package to leverage multi-threading on CPU:\n  * `pip install imutils`\n- Running `python -m demo.live` opens the webcam and begins detecting!\n\n## TODO\nWe have accumulated the following to-do list, which we hope to complete in the near future\n- Still to come:\n  * [x] Support for the MS COCO dataset\n  * [ ] Support for SSD512 training and testing\n  * [ ] Support for training on custom datasets\n\n## Authors\n\n* [**Max deGroot**](https://github.com/amdegroot)\n* [**Ellis Brown**](http://github.com/ellisbrown)\n\n***Note:*** Unfortunately, this is just a hobby of ours and not a full-time job, so we'll do our best to keep things up to date, but no guarantees.  That being said, thanks to everyone for your continued help and feedback as it is really appreciated. We will try to address everything as soon as possible.\n\n## References\n- Wei Liu, et al. \"SSD: Single Shot MultiBox Detector.\" [ECCV2016]((http://arxiv.org/abs/1512.02325)).\n- [Original Implementation (CAFFE)](https://github.com/weiliu89/caffe/tree/ssd)\n- A huge thank you to [Alex Koltun](https://github.com/alexkoltun) and his team at [Webyclip](http://www.webyclip.com) for their help in finishing the data augmentation portion.\n- A list of other great SSD ports that were sources of inspiration (especially the Chainer repo):\n  * [Chainer](https://github.com/Hakuyume/chainer-ssd), [Keras](https://github.com/rykov8/ssd_keras), [MXNet](https://github.com/zhreshold/mxnet-ssd), [Tensorflow](https://github.com/balancap/SSD-Tensorflow)\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval.py",
          "type": "blob",
          "size": 15.4970703125,
          "content": "\"\"\"Adapted from:\n    @longcw faster_rcnn_pytorch: https://github.com/longcw/faster_rcnn_pytorch\n    @rbgirshick py-faster-rcnn https://github.com/rbgirshick/py-faster-rcnn\n    Licensed under The MIT License [see LICENSE for details]\n\"\"\"\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nfrom data import VOC_ROOT, VOCAnnotationTransform, VOCDetection, BaseTransform\nfrom data import VOC_CLASSES as labelmap\nimport torch.utils.data as data\n\nfrom ssd import build_ssd\n\nimport sys\nimport os\nimport time\nimport argparse\nimport numpy as np\nimport pickle\nimport cv2\n\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\n\n\ndef str2bool(v):\n    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n\n\nparser = argparse.ArgumentParser(\n    description='Single Shot MultiBox Detector Evaluation')\nparser.add_argument('--trained_model',\n                    default='weights/ssd300_mAP_77.43_v2.pth', type=str,\n                    help='Trained state_dict file path to open')\nparser.add_argument('--save_folder', default='eval/', type=str,\n                    help='File path to save results')\nparser.add_argument('--confidence_threshold', default=0.01, type=float,\n                    help='Detection confidence threshold')\nparser.add_argument('--top_k', default=5, type=int,\n                    help='Further restrict the number of predictions to parse')\nparser.add_argument('--cuda', default=True, type=str2bool,\n                    help='Use cuda to train model')\nparser.add_argument('--voc_root', default=VOC_ROOT,\n                    help='Location of VOC root directory')\nparser.add_argument('--cleanup', default=True, type=str2bool,\n                    help='Cleanup and remove results files following eval')\n\nargs = parser.parse_args()\n\nif not os.path.exists(args.save_folder):\n    os.mkdir(args.save_folder)\n\nif torch.cuda.is_available():\n    if args.cuda:\n        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n    if not args.cuda:\n        print(\"WARNING: It looks like you have a CUDA device, but aren't using \\\n              CUDA.  Run with --cuda for optimal eval speed.\")\n        torch.set_default_tensor_type('torch.FloatTensor')\nelse:\n    torch.set_default_tensor_type('torch.FloatTensor')\n\nannopath = os.path.join(args.voc_root, 'VOC2007', 'Annotations', '%s.xml')\nimgpath = os.path.join(args.voc_root, 'VOC2007', 'JPEGImages', '%s.jpg')\nimgsetpath = os.path.join(args.voc_root, 'VOC2007', 'ImageSets',\n                          'Main', '{:s}.txt')\nYEAR = '2007'\ndevkit_path = args.voc_root + 'VOC' + YEAR\ndataset_mean = (104, 117, 123)\nset_type = 'test'\n\n\nclass Timer(object):\n    \"\"\"A simple timer.\"\"\"\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n\n\ndef parse_rec(filename):\n    \"\"\" Parse a PASCAL VOC xml file \"\"\"\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall('object'):\n        obj_struct = {}\n        obj_struct['name'] = obj.find('name').text\n        obj_struct['pose'] = obj.find('pose').text\n        obj_struct['truncated'] = int(obj.find('truncated').text)\n        obj_struct['difficult'] = int(obj.find('difficult').text)\n        bbox = obj.find('bndbox')\n        obj_struct['bbox'] = [int(bbox.find('xmin').text) - 1,\n                              int(bbox.find('ymin').text) - 1,\n                              int(bbox.find('xmax').text) - 1,\n                              int(bbox.find('ymax').text) - 1]\n        objects.append(obj_struct)\n\n    return objects\n\n\ndef get_output_dir(name, phase):\n    \"\"\"Return the directory where experimental artifacts are placed.\n    If the directory does not exist, it is created.\n    A canonical path is built using the name from an imdb and a network\n    (if not None).\n    \"\"\"\n    filedir = os.path.join(name, phase)\n    if not os.path.exists(filedir):\n        os.makedirs(filedir)\n    return filedir\n\n\ndef get_voc_results_file_template(image_set, cls):\n    # VOCdevkit/VOC2007/results/det_test_aeroplane.txt\n    filename = 'det_' + image_set + '_%s.txt' % (cls)\n    filedir = os.path.join(devkit_path, 'results')\n    if not os.path.exists(filedir):\n        os.makedirs(filedir)\n    path = os.path.join(filedir, filename)\n    return path\n\n\ndef write_voc_results_file(all_boxes, dataset):\n    for cls_ind, cls in enumerate(labelmap):\n        print('Writing {:s} VOC results file'.format(cls))\n        filename = get_voc_results_file_template(set_type, cls)\n        with open(filename, 'wt') as f:\n            for im_ind, index in enumerate(dataset.ids):\n                dets = all_boxes[cls_ind+1][im_ind]\n                if dets == []:\n                    continue\n                # the VOCdevkit expects 1-based indices\n                for k in range(dets.shape[0]):\n                    f.write('{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n'.\n                            format(index[1], dets[k, -1],\n                                   dets[k, 0] + 1, dets[k, 1] + 1,\n                                   dets[k, 2] + 1, dets[k, 3] + 1))\n\n\ndef do_python_eval(output_dir='output', use_07=True):\n    cachedir = os.path.join(devkit_path, 'annotations_cache')\n    aps = []\n    # The PASCAL VOC metric changed in 2010\n    use_07_metric = use_07\n    print('VOC07 metric? ' + ('Yes' if use_07_metric else 'No'))\n    if not os.path.isdir(output_dir):\n        os.mkdir(output_dir)\n    for i, cls in enumerate(labelmap):\n        filename = get_voc_results_file_template(set_type, cls)\n        rec, prec, ap = voc_eval(\n           filename, annopath, imgsetpath.format(set_type), cls, cachedir,\n           ovthresh=0.5, use_07_metric=use_07_metric)\n        aps += [ap]\n        print('AP for {} = {:.4f}'.format(cls, ap))\n        with open(os.path.join(output_dir, cls + '_pr.pkl'), 'wb') as f:\n            pickle.dump({'rec': rec, 'prec': prec, 'ap': ap}, f)\n    print('Mean AP = {:.4f}'.format(np.mean(aps)))\n    print('~~~~~~~~')\n    print('Results:')\n    for ap in aps:\n        print('{:.3f}'.format(ap))\n    print('{:.3f}'.format(np.mean(aps)))\n    print('~~~~~~~~')\n    print('')\n    print('--------------------------------------------------------------')\n    print('Results computed with the **unofficial** Python eval code.')\n    print('Results should be very close to the official MATLAB eval code.')\n    print('--------------------------------------------------------------')\n\n\ndef voc_ap(rec, prec, use_07_metric=True):\n    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:True).\n    \"\"\"\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=True):\n    \"\"\"rec, prec, ap = voc_eval(detpath,\n                           annopath,\n                           imagesetfile,\n                           classname,\n                           [ovthresh],\n                           [use_07_metric])\nTop level function that does the PASCAL VOC evaluation.\ndetpath: Path to detections\n   detpath.format(classname) should produce the detection results file.\nannopath: Path to annotations\n   annopath.format(imagename) should be the xml annotations file.\nimagesetfile: Text file containing the list of images, one image per line.\nclassname: Category name (duh)\ncachedir: Directory for caching the annotations\n[ovthresh]: Overlap threshold (default = 0.5)\n[use_07_metric]: Whether to use VOC07's 11 point AP computation\n   (default True)\n\"\"\"\n# assumes detections are in detpath.format(classname)\n# assumes annotations are in annopath.format(imagename)\n# assumes imagesetfile is a text file with each line an image name\n# cachedir caches the annotations in a pickle file\n# first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, 'annots.pkl')\n    # read list of images\n    with open(imagesetfile, 'r') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            recs[imagename] = parse_rec(annopath % (imagename))\n            if i % 100 == 0:\n                print('Reading annotation for {:d}/{:d}'.format(\n                   i + 1, len(imagenames)))\n        # save\n        print('Saving cached annotations to {:s}'.format(cachefile))\n        with open(cachefile, 'wb') as f:\n            pickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, 'rb') as f:\n            recs = pickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        R = [obj for obj in recs[imagename] if obj['name'] == classname]\n        bbox = np.array([x['bbox'] for x in R])\n        difficult = np.array([x['difficult'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagename] = {'bbox': bbox,\n                                 'difficult': difficult,\n                                 'det': det}\n\n    # read dets\n    detfile = detpath.format(classname)\n    with open(detfile, 'r') as f:\n        lines = f.readlines()\n    if any(lines) == 1:\n\n        splitlines = [x.strip().split(' ') for x in lines]\n        image_ids = [x[0] for x in splitlines]\n        confidence = np.array([float(x[1]) for x in splitlines])\n        BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n        # sort by confidence\n        sorted_ind = np.argsort(-confidence)\n        sorted_scores = np.sort(-confidence)\n        BB = BB[sorted_ind, :]\n        image_ids = [image_ids[x] for x in sorted_ind]\n\n        # go down dets and mark TPs and FPs\n        nd = len(image_ids)\n        tp = np.zeros(nd)\n        fp = np.zeros(nd)\n        for d in range(nd):\n            R = class_recs[image_ids[d]]\n            bb = BB[d, :].astype(float)\n            ovmax = -np.inf\n            BBGT = R['bbox'].astype(float)\n            if BBGT.size > 0:\n                # compute overlaps\n                # intersection\n                ixmin = np.maximum(BBGT[:, 0], bb[0])\n                iymin = np.maximum(BBGT[:, 1], bb[1])\n                ixmax = np.minimum(BBGT[:, 2], bb[2])\n                iymax = np.minimum(BBGT[:, 3], bb[3])\n                iw = np.maximum(ixmax - ixmin, 0.)\n                ih = np.maximum(iymax - iymin, 0.)\n                inters = iw * ih\n                uni = ((bb[2] - bb[0]) * (bb[3] - bb[1]) +\n                       (BBGT[:, 2] - BBGT[:, 0]) *\n                       (BBGT[:, 3] - BBGT[:, 1]) - inters)\n                overlaps = inters / uni\n                ovmax = np.max(overlaps)\n                jmax = np.argmax(overlaps)\n\n            if ovmax > ovthresh:\n                if not R['difficult'][jmax]:\n                    if not R['det'][jmax]:\n                        tp[d] = 1.\n                        R['det'][jmax] = 1\n                    else:\n                        fp[d] = 1.\n            else:\n                fp[d] = 1.\n\n        # compute precision recall\n        fp = np.cumsum(fp)\n        tp = np.cumsum(tp)\n        rec = tp / float(npos)\n        # avoid divide by zero in case the first detection matches a difficult\n        # ground truth\n        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = voc_ap(rec, prec, use_07_metric)\n    else:\n        rec = -1.\n        prec = -1.\n        ap = -1.\n\n    return rec, prec, ap\n\n\ndef test_net(save_folder, net, cuda, dataset, transform, top_k,\n             im_size=300, thresh=0.05):\n    num_images = len(dataset)\n    # all detections are collected into:\n    #    all_boxes[cls][image] = N x 5 array of detections in\n    #    (x1, y1, x2, y2, score)\n    all_boxes = [[[] for _ in range(num_images)]\n                 for _ in range(len(labelmap)+1)]\n\n    # timers\n    _t = {'im_detect': Timer(), 'misc': Timer()}\n    output_dir = get_output_dir('ssd300_120000', set_type)\n    det_file = os.path.join(output_dir, 'detections.pkl')\n\n    for i in range(num_images):\n        im, gt, h, w = dataset.pull_item(i)\n\n        x = Variable(im.unsqueeze(0))\n        if args.cuda:\n            x = x.cuda()\n        _t['im_detect'].tic()\n        detections = net(x).data\n        detect_time = _t['im_detect'].toc(average=False)\n\n        # skip j = 0, because it's the background class\n        for j in range(1, detections.size(1)):\n            dets = detections[0, j, :]\n            mask = dets[:, 0].gt(0.).expand(5, dets.size(0)).t()\n            dets = torch.masked_select(dets, mask).view(-1, 5)\n            if dets.size(0) == 0:\n                continue\n            boxes = dets[:, 1:]\n            boxes[:, 0] *= w\n            boxes[:, 2] *= w\n            boxes[:, 1] *= h\n            boxes[:, 3] *= h\n            scores = dets[:, 0].cpu().numpy()\n            cls_dets = np.hstack((boxes.cpu().numpy(),\n                                  scores[:, np.newaxis])).astype(np.float32,\n                                                                 copy=False)\n            all_boxes[j][i] = cls_dets\n\n        print('im_detect: {:d}/{:d} {:.3f}s'.format(i + 1,\n                                                    num_images, detect_time))\n\n    with open(det_file, 'wb') as f:\n        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n\n    print('Evaluating detections')\n    evaluate_detections(all_boxes, output_dir, dataset)\n\n\ndef evaluate_detections(box_list, output_dir, dataset):\n    write_voc_results_file(box_list, dataset)\n    do_python_eval(output_dir)\n\n\nif __name__ == '__main__':\n    # load net\n    num_classes = len(labelmap) + 1                      # +1 for background\n    net = build_ssd('test', 300, num_classes)            # initialize SSD\n    net.load_state_dict(torch.load(args.trained_model))\n    net.eval()\n    print('Finished loading model!')\n    # load data\n    dataset = VOCDetection(args.voc_root, [('2007', set_type)],\n                           BaseTransform(300, dataset_mean),\n                           VOCAnnotationTransform())\n    if args.cuda:\n        net = net.cuda()\n        cudnn.benchmark = True\n    # evaluation\n    test_net(args.save_folder, net, args.cuda, dataset,\n             BaseTransform(net.size, dataset_mean), args.top_k, 300,\n             thresh=args.confidence_threshold)\n"
        },
        {
          "name": "layers",
          "type": "tree",
          "content": null
        },
        {
          "name": "ssd.py",
          "type": "blob",
          "size": 7.4873046875,
          "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom layers import *\nfrom data import voc, coco\nimport os\n\n\nclass SSD(nn.Module):\n    \"\"\"Single Shot Multibox Architecture\n    The network is composed of a base VGG network followed by the\n    added multibox conv layers.  Each multibox layer branches into\n        1) conv2d for class conf scores\n        2) conv2d for localization predictions\n        3) associated priorbox layer to produce default bounding\n           boxes specific to the layer's feature map size.\n    See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n\n    Args:\n        phase: (string) Can be \"test\" or \"train\"\n        size: input image size\n        base: VGG16 layers for input, size of either 300 or 500\n        extras: extra layers that feed to multibox loc and conf layers\n        head: \"multibox head\" consists of loc and conf conv layers\n    \"\"\"\n\n    def __init__(self, phase, size, base, extras, head, num_classes):\n        super(SSD, self).__init__()\n        self.phase = phase\n        self.num_classes = num_classes\n        self.cfg = (coco, voc)[num_classes == 21]\n        self.priorbox = PriorBox(self.cfg)\n        self.priors = Variable(self.priorbox.forward(), volatile=True)\n        self.size = size\n\n        # SSD network\n        self.vgg = nn.ModuleList(base)\n        # Layer learns to scale the l2 normalized features from conv4_3\n        self.L2Norm = L2Norm(512, 20)\n        self.extras = nn.ModuleList(extras)\n\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n\n        if phase == 'test':\n            self.softmax = nn.Softmax(dim=-1)\n            self.detect = Detect(num_classes, 0, 200, 0.01, 0.45)\n\n    def forward(self, x):\n        \"\"\"Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n        \"\"\"\n        sources = list()\n        loc = list()\n        conf = list()\n\n        # apply vgg up to conv4_3 relu\n        for k in range(23):\n            x = self.vgg[k](x)\n\n        s = self.L2Norm(x)\n        sources.append(s)\n\n        # apply vgg up to fc7\n        for k in range(23, len(self.vgg)):\n            x = self.vgg[k](x)\n        sources.append(x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = F.relu(v(x), inplace=True)\n            if k % 2 == 1:\n                sources.append(x)\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n        if self.phase == \"test\":\n            output = self.detect(\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(conf.size(0), -1,\n                             self.num_classes)),                # conf preds\n                self.priors.type(type(x.data))                  # default boxes\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n                self.priors\n            )\n        return output\n\n    def load_weights(self, base_file):\n        other, ext = os.path.splitext(base_file)\n        if ext == '.pkl' or '.pth':\n            print('Loading weights into state dict...')\n            self.load_state_dict(torch.load(base_file,\n                                 map_location=lambda storage, loc: storage))\n            print('Finished!')\n        else:\n            print('Sorry only .pth and .pkl files supported.')\n\n\n# This function is derived from torchvision VGG make_layers()\n# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\ndef vgg(cfg, i, batch_norm=False):\n    layers = []\n    in_channels = i\n    for v in cfg:\n        if v == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == 'C':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n    layers += [pool5, conv6,\n               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n    return layers\n\n\ndef add_extras(cfg, i, batch_norm=False):\n    # Extra layers added to VGG for feature scaling\n    layers = []\n    in_channels = i\n    flag = False\n    for k, v in enumerate(cfg):\n        if in_channels != 'S':\n            if v == 'S':\n                layers += [nn.Conv2d(in_channels, cfg[k + 1],\n                           kernel_size=(1, 3)[flag], stride=2, padding=1)]\n            else:\n                layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]\n            flag = not flag\n        in_channels = v\n    return layers\n\n\ndef multibox(vgg, extra_layers, cfg, num_classes):\n    loc_layers = []\n    conf_layers = []\n    vgg_source = [21, -2]\n    for k, v in enumerate(vgg_source):\n        loc_layers += [nn.Conv2d(vgg[v].out_channels,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(vgg[v].out_channels,\n                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n    for k, v in enumerate(extra_layers[1::2], 2):\n        loc_layers += [nn.Conv2d(v.out_channels, cfg[k]\n                                 * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(v.out_channels, cfg[k]\n                                  * num_classes, kernel_size=3, padding=1)]\n    return vgg, extra_layers, (loc_layers, conf_layers)\n\n\nbase = {\n    '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n            512, 512, 512],\n    '512': [],\n}\nextras = {\n    '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256],\n    '512': [],\n}\nmbox = {\n    '300': [4, 6, 6, 6, 4, 4],  # number of boxes per feature map location\n    '512': [],\n}\n\n\ndef build_ssd(phase, size=300, num_classes=21):\n    if phase != \"test\" and phase != \"train\":\n        print(\"ERROR: Phase: \" + phase + \" not recognized\")\n        return\n    if size != 300:\n        print(\"ERROR: You specified size \" + repr(size) + \". However, \" +\n              \"currently only SSD300 (size=300) is supported!\")\n        return\n    base_, extras_, head_ = multibox(vgg(base[str(size)], 3),\n                                     add_extras(extras[str(size)], 1024),\n                                     mbox[str(size)], num_classes)\n    return SSD(phase, size, base_, extras_, head_, num_classes)\n"
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 3.7802734375,
          "content": "from __future__ import print_function\nimport sys\nimport os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom data import VOC_ROOT, VOC_CLASSES as labelmap\nfrom PIL import Image\nfrom data import VOCAnnotationTransform, VOCDetection, BaseTransform, VOC_CLASSES\nimport torch.utils.data as data\nfrom ssd import build_ssd\n\nparser = argparse.ArgumentParser(description='Single Shot MultiBox Detection')\nparser.add_argument('--trained_model', default='weights/ssd_300_VOC0712.pth',\n                    type=str, help='Trained state_dict file path to open')\nparser.add_argument('--save_folder', default='eval/', type=str,\n                    help='Dir to save results')\nparser.add_argument('--visual_threshold', default=0.6, type=float,\n                    help='Final confidence threshold')\nparser.add_argument('--cuda', default=True, type=bool,\n                    help='Use cuda to train model')\nparser.add_argument('--voc_root', default=VOC_ROOT, help='Location of VOC root directory')\nparser.add_argument('-f', default=None, type=str, help=\"Dummy arg so we can load in Jupyter Notebooks\")\nargs = parser.parse_args()\n\nif args.cuda and torch.cuda.is_available():\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\nelse:\n    torch.set_default_tensor_type('torch.FloatTensor')\n\nif not os.path.exists(args.save_folder):\n    os.mkdir(args.save_folder)\n\n\ndef test_net(save_folder, net, cuda, testset, transform, thresh):\n    # dump predictions and assoc. ground truth to text file for now\n    filename = save_folder+'test1.txt'\n    num_images = len(testset)\n    for i in range(num_images):\n        print('Testing image {:d}/{:d}....'.format(i+1, num_images))\n        img = testset.pull_image(i)\n        img_id, annotation = testset.pull_anno(i)\n        x = torch.from_numpy(transform(img)[0]).permute(2, 0, 1)\n        x = Variable(x.unsqueeze(0))\n\n        with open(filename, mode='a') as f:\n            f.write('\\nGROUND TRUTH FOR: '+img_id+'\\n')\n            for box in annotation:\n                f.write('label: '+' || '.join(str(b) for b in box)+'\\n')\n        if cuda:\n            x = x.cuda()\n\n        y = net(x)      # forward pass\n        detections = y.data\n        # scale each detection back up to the image\n        scale = torch.Tensor([img.shape[1], img.shape[0],\n                             img.shape[1], img.shape[0]])\n        pred_num = 0\n        for i in range(detections.size(1)):\n            j = 0\n            while detections[0, i, j, 0] >= 0.6:\n                if pred_num == 0:\n                    with open(filename, mode='a') as f:\n                        f.write('PREDICTIONS: '+'\\n')\n                score = detections[0, i, j, 0]\n                label_name = labelmap[i-1]\n                pt = (detections[0, i, j, 1:]*scale).cpu().numpy()\n                coords = (pt[0], pt[1], pt[2], pt[3])\n                pred_num += 1\n                with open(filename, mode='a') as f:\n                    f.write(str(pred_num)+' label: '+label_name+' score: ' +\n                            str(score) + ' '+' || '.join(str(c) for c in coords) + '\\n')\n                j += 1\n\n\ndef test_voc():\n    # load net\n    num_classes = len(VOC_CLASSES) + 1 # +1 background\n    net = build_ssd('test', 300, num_classes) # initialize SSD\n    net.load_state_dict(torch.load(args.trained_model))\n    net.eval()\n    print('Finished loading model!')\n    # load data\n    testset = VOCDetection(args.voc_root, [('2007', 'test')], None, VOCAnnotationTransform())\n    if args.cuda:\n        net = net.cuda()\n        cudnn.benchmark = True\n    # evaluation\n    test_net(args.save_folder, net, args.cuda, testset,\n             BaseTransform(net.size, (104, 117, 123)),\n             thresh=args.visual_threshold)\n\nif __name__ == '__main__':\n    test_voc()\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 8.8603515625,
          "content": "from data import *\nfrom utils.augmentations import SSDAugmentation\nfrom layers.modules import MultiBoxLoss\nfrom ssd import build_ssd\nimport os\nimport sys\nimport time\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nimport torch.nn.init as init\nimport torch.utils.data as data\nimport numpy as np\nimport argparse\n\n\ndef str2bool(v):\n    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n\n\nparser = argparse.ArgumentParser(\n    description='Single Shot MultiBox Detector Training With Pytorch')\ntrain_set = parser.add_mutually_exclusive_group()\nparser.add_argument('--dataset', default='VOC', choices=['VOC', 'COCO'],\n                    type=str, help='VOC or COCO')\nparser.add_argument('--dataset_root', default=VOC_ROOT,\n                    help='Dataset root directory path')\nparser.add_argument('--basenet', default='vgg16_reducedfc.pth',\n                    help='Pretrained base model')\nparser.add_argument('--batch_size', default=32, type=int,\n                    help='Batch size for training')\nparser.add_argument('--resume', default=None, type=str,\n                    help='Checkpoint state_dict file to resume training from')\nparser.add_argument('--start_iter', default=0, type=int,\n                    help='Resume training at this iter')\nparser.add_argument('--num_workers', default=4, type=int,\n                    help='Number of workers used in dataloading')\nparser.add_argument('--cuda', default=True, type=str2bool,\n                    help='Use CUDA to train model')\nparser.add_argument('--lr', '--learning-rate', default=1e-3, type=float,\n                    help='initial learning rate')\nparser.add_argument('--momentum', default=0.9, type=float,\n                    help='Momentum value for optim')\nparser.add_argument('--weight_decay', default=5e-4, type=float,\n                    help='Weight decay for SGD')\nparser.add_argument('--gamma', default=0.1, type=float,\n                    help='Gamma update for SGD')\nparser.add_argument('--visdom', default=False, type=str2bool,\n                    help='Use visdom for loss visualization')\nparser.add_argument('--save_folder', default='weights/',\n                    help='Directory for saving checkpoint models')\nargs = parser.parse_args()\n\n\nif torch.cuda.is_available():\n    if args.cuda:\n        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n    if not args.cuda:\n        print(\"WARNING: It looks like you have a CUDA device, but aren't \" +\n              \"using CUDA.\\nRun with --cuda for optimal training speed.\")\n        torch.set_default_tensor_type('torch.FloatTensor')\nelse:\n    torch.set_default_tensor_type('torch.FloatTensor')\n\nif not os.path.exists(args.save_folder):\n    os.mkdir(args.save_folder)\n\n\ndef train():\n    if args.dataset == 'COCO':\n        if args.dataset_root == VOC_ROOT:\n            if not os.path.exists(COCO_ROOT):\n                parser.error('Must specify dataset_root if specifying dataset')\n            print(\"WARNING: Using default COCO dataset_root because \" +\n                  \"--dataset_root was not specified.\")\n            args.dataset_root = COCO_ROOT\n        cfg = coco\n        dataset = COCODetection(root=args.dataset_root,\n                                transform=SSDAugmentation(cfg['min_dim'],\n                                                          MEANS))\n    elif args.dataset == 'VOC':\n        if args.dataset_root == COCO_ROOT:\n            parser.error('Must specify dataset if specifying dataset_root')\n        cfg = voc\n        dataset = VOCDetection(root=args.dataset_root,\n                               transform=SSDAugmentation(cfg['min_dim'],\n                                                         MEANS))\n\n    if args.visdom:\n        import visdom\n        viz = visdom.Visdom()\n\n    ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])\n    net = ssd_net\n\n    if args.cuda:\n        net = torch.nn.DataParallel(ssd_net)\n        cudnn.benchmark = True\n\n    if args.resume:\n        print('Resuming training, loading {}...'.format(args.resume))\n        ssd_net.load_weights(args.resume)\n    else:\n        vgg_weights = torch.load(args.save_folder + args.basenet)\n        print('Loading base network...')\n        ssd_net.vgg.load_state_dict(vgg_weights)\n\n    if args.cuda:\n        net = net.cuda()\n\n    if not args.resume:\n        print('Initializing weights...')\n        # initialize newly added layers' weights with xavier method\n        ssd_net.extras.apply(weights_init)\n        ssd_net.loc.apply(weights_init)\n        ssd_net.conf.apply(weights_init)\n\n    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum,\n                          weight_decay=args.weight_decay)\n    criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,\n                             False, args.cuda)\n\n    net.train()\n    # loss counters\n    loc_loss = 0\n    conf_loss = 0\n    epoch = 0\n    print('Loading the dataset...')\n\n    epoch_size = len(dataset) // args.batch_size\n    print('Training SSD on:', dataset.name)\n    print('Using the specified args:')\n    print(args)\n\n    step_index = 0\n\n    if args.visdom:\n        vis_title = 'SSD.PyTorch on ' + dataset.name\n        vis_legend = ['Loc Loss', 'Conf Loss', 'Total Loss']\n        iter_plot = create_vis_plot('Iteration', 'Loss', vis_title, vis_legend)\n        epoch_plot = create_vis_plot('Epoch', 'Loss', vis_title, vis_legend)\n\n    data_loader = data.DataLoader(dataset, args.batch_size,\n                                  num_workers=args.num_workers,\n                                  shuffle=True, collate_fn=detection_collate,\n                                  pin_memory=True)\n    # create batch iterator\n    batch_iterator = iter(data_loader)\n    for iteration in range(args.start_iter, cfg['max_iter']):\n        if args.visdom and iteration != 0 and (iteration % epoch_size == 0):\n            update_vis_plot(epoch, loc_loss, conf_loss, epoch_plot, None,\n                            'append', epoch_size)\n            # reset epoch loss counters\n            loc_loss = 0\n            conf_loss = 0\n            epoch += 1\n\n        if iteration in cfg['lr_steps']:\n            step_index += 1\n            adjust_learning_rate(optimizer, args.gamma, step_index)\n\n        # load train data\n        images, targets = next(batch_iterator)\n\n        if args.cuda:\n            images = Variable(images.cuda())\n            targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n        else:\n            images = Variable(images)\n            targets = [Variable(ann, volatile=True) for ann in targets]\n        # forward\n        t0 = time.time()\n        out = net(images)\n        # backprop\n        optimizer.zero_grad()\n        loss_l, loss_c = criterion(out, targets)\n        loss = loss_l + loss_c\n        loss.backward()\n        optimizer.step()\n        t1 = time.time()\n        loc_loss += loss_l.data[0]\n        conf_loss += loss_c.data[0]\n\n        if iteration % 10 == 0:\n            print('timer: %.4f sec.' % (t1 - t0))\n            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data[0]), end=' ')\n\n        if args.visdom:\n            update_vis_plot(iteration, loss_l.data[0], loss_c.data[0],\n                            iter_plot, epoch_plot, 'append')\n\n        if iteration != 0 and iteration % 5000 == 0:\n            print('Saving state, iter:', iteration)\n            torch.save(ssd_net.state_dict(), 'weights/ssd300_COCO_' +\n                       repr(iteration) + '.pth')\n    torch.save(ssd_net.state_dict(),\n               args.save_folder + '' + args.dataset + '.pth')\n\n\ndef adjust_learning_rate(optimizer, gamma, step):\n    \"\"\"Sets the learning rate to the initial LR decayed by 10 at every\n        specified step\n    # Adapted from PyTorch Imagenet example:\n    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n    \"\"\"\n    lr = args.lr * (gamma ** (step))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef xavier(param):\n    init.xavier_uniform(param)\n\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        xavier(m.weight.data)\n        m.bias.data.zero_()\n\n\ndef create_vis_plot(_xlabel, _ylabel, _title, _legend):\n    return viz.line(\n        X=torch.zeros((1,)).cpu(),\n        Y=torch.zeros((1, 3)).cpu(),\n        opts=dict(\n            xlabel=_xlabel,\n            ylabel=_ylabel,\n            title=_title,\n            legend=_legend\n        )\n    )\n\n\ndef update_vis_plot(iteration, loc, conf, window1, window2, update_type,\n                    epoch_size=1):\n    viz.line(\n        X=torch.ones((1, 3)).cpu() * iteration,\n        Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu() / epoch_size,\n        win=window1,\n        update=update_type\n    )\n    # initialize epoch plot on first iteration\n    if iteration == 0:\n        viz.line(\n            X=torch.zeros((1, 3)).cpu(),\n            Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu(),\n            win=window2,\n            update=True\n        )\n\n\nif __name__ == '__main__':\n    train()\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}