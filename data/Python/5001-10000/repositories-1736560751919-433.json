{
  "metadata": {
    "timestamp": 1736560751919,
    "page": 433,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/SlowFast",
      "stars": 6732,
      "defaultBranch": "main",
      "files": [
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.23828125,
          "content": "# Code of Conduct\n\nFacebook has adopted a Code of Conduct that we expect project participants to adhere to.\nPlease read the [full text](https://code.fb.com/codeofconduct/)\nso that you can understand what actions will and will not be tolerated.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.2744140625,
          "content": "# Contributing to PySlowFast\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `master`.\n2. If you've changed APIs, update the documentation.\n3. Ensure the test suite passes.\n4. Make sure your code lints.\n5. Ensure no regressions in baseline model speed and accuracy.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\n\nPlease ensure your description is clear and has sufficient instructions to be able to reproduce the issue. The recommended issue format is:\n------\n\n#### To Reproduce\n```How to reproduce the issue.```\n#### Expected behavior\n```Expected output.```\n#### Environment\n```Your environment.```\n\n------\n\n## Coding Style  \n* 4 spaces for indentation rather than tabs\n* 80 character line length\n* PEP8 formatting\n\n## License\nBy contributing to PySlowFast, you agree that your contributions will be licensed under the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "GETTING_STARTED.md",
          "type": "blob",
          "size": 2.1845703125,
          "content": "# Getting Started with PySlowFast\n\nThis document provides a brief intro of launching jobs in PySlowFast for training and testing. Before launching any job, make sure you have properly installed the PySlowFast following the instruction in [README.md](README.md) and you have prepared the dataset following [DATASET.md](slowfast/datasets/DATASET.md) with the correct format.\n\n## Train a Standard Model from Scratch\n\nHere we can start with training a simple C2D models by running:\n\n```\npython tools/run_net.py \\\n  --cfg configs/Kinetics/C2D_8x8_R50.yaml \\\n  DATA.PATH_TO_DATA_DIR path_to_your_dataset \\\n  NUM_GPUS 2 \\\n  TRAIN.BATCH_SIZE 16 \\\n```\nYou may need to pass location of your dataset in the command line by adding `DATA.PATH_TO_DATA_DIR path_to_your_dataset`, or you can simply add\n\n```\nDATA:\n  PATH_TO_DATA_DIR: path_to_your_dataset\n```\nTo the yaml configs file, then you do not need to pass it to the command line every time.\n\n\nYou may also want to add:\n```\n  DATA_LOADER.NUM_WORKERS 0 \\\n  NUM_GPUS 2 \\\n  TRAIN.BATCH_SIZE 16 \\\n```\n\nIf you want to launch a quick job for debugging on your local machine.\n\n## Resume from an Existing Checkpoint\nIf your checkpoint is trained by PyTorch, then you can add the following line in the command line, or you can also add it in the YAML config:\n\n```\nTRAIN.CHECKPOINT_FILE_PATH path_to_your_PyTorch_checkpoint\n```\n\nIf the checkpoint in trained by Caffe2, then you can do the following:\n\n```\nTRAIN.CHECKPOINT_FILE_PATH path_to_your_Caffe2_checkpoint \\\nTRAIN.CHECKPOINT_TYPE caffe2\n```\n\nIf you need to performance inflation on the checkpoint, remember to set `TRAIN.CHECKPOINT_INFLATE` to True.\n\n\n## Perform Test\nWe have `TRAIN.ENABLE` and `TEST.ENABLE` to control whether training or testing is required for the current job. If only testing is preferred, you can set the `TRAIN.ENABLE` to False, and do not forget to pass the path to the model you want to test to TEST.CHECKPOINT_FILE_PATH.\n```\npython tools/run_net.py \\\n  --cfg configs/Kinetics/C2D_8x8_R50.yaml \\\n  DATA.PATH_TO_DATA_DIR path_to_your_dataset \\\n  TEST.CHECKPOINT_FILE_PATH path_to_your_checkpoint \\\n  TRAIN.ENABLE False \\\n```\n\n### Run command\n```\npython \\tools\\run_net.py --cfg path/to/<pretrained_model_config_file>.yaml\n```\n"
        },
        {
          "name": "INSTALL.md",
          "type": "blob",
          "size": 2.3349609375,
          "content": "# Installation\n\n## Requirements\n- Python >= 3.8\n- Numpy\n- PyTorch >= 1.3\n- [fvcore](https://github.com/facebookresearch/fvcore/): `pip install 'git+https://github.com/facebookresearch/fvcore'`\n- [torchvision](https://github.com/pytorch/vision/) that matches the PyTorch installation.\n  You can install them together at [pytorch.org](https://pytorch.org) to make sure of this.\n- simplejson: `pip install simplejson`\n- GCC >= 4.9\n- PyAV: `conda install av -c conda-forge`\n- ffmpeg (4.0 is prefereed, will be installed along with PyAV)\n- PyYaml: (will be installed along with fvcore)\n- tqdm: (will be installed along with fvcore)\n- iopath: `pip install -U iopath` or `conda install -c iopath iopath`\n- psutil: `pip install psutil`\n- OpenCV: `pip install opencv-python`\n- torchvision: `pip install torchvision` or `conda install torchvision -c pytorch`\n- tensorboard: `pip install tensorboard`\n- moviepy: (optional, for visualizing video on tensorboard) `conda install -c conda-forge moviepy` or `pip install moviepy`\n- PyTorchVideo: `pip install pytorchvideo`\n- [Detectron2](https://github.com/facebookresearch/detectron2):\n- FairScale: `pip install 'git+https://github.com/facebookresearch/fairscale'`\n```\n    pip install -U torch torchvision cython\n    pip install -U 'git+https://github.com/facebookresearch/fvcore.git' 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n    git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n    pip install -e detectron2_repo\n    # You can find more details at https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md\n```\n\n## Pytorch\nPlease follow PyTorch official instructions to install from source:\n```\ngit clone --recursive https://github.com/pytorch/pytorch\n```\n\n## PySlowFast\n\nClone the PySlowFast Video Understanding repository.\n```\ngit clone https://github.com/facebookresearch/slowfast\n```\n\nAdd this repository to $PYTHONPATH.\n```\nexport PYTHONPATH=/path/to/SlowFast/slowfast:$PYTHONPATH\n```\n\n### Build PySlowFast\n\nAfter having the above dependencies, run:\n```\ngit clone https://github.com/facebookresearch/slowfast\ncd SlowFast\npython setup.py build develop\n```\n\nNow the installation is finished, run the pipeline with:\n```\npython tools/run_net.py --cfg configs/Kinetics/C2D_8x8_R50.yaml NUM_GPUS 1 TRAIN.BATCH_SIZE 8 SOLVER.BASE_LR 0.0125 DATA.PATH_TO_DATA_DIR path_to_your_data_folder\n```\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 10.001953125,
          "content": "Apache License\nVersion 2.0, January 2004\nhttp://www.apache.org/licenses/\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n1. Definitions.\n\n\"License\" shall mean the terms and conditions for use, reproduction,\nand distribution as defined by Sections 1 through 9 of this document.\n\n\"Licensor\" shall mean the copyright owner or entity authorized by\nthe copyright owner that is granting the License.\n\n\"Legal Entity\" shall mean the union of the acting entity and all\nother entities that control, are controlled by, or are under common\ncontrol with that entity. For the purposes of this definition,\n\"control\" means (i) the power, direct or indirect, to cause the\ndirection or management of such entity, whether by contract or\notherwise, or (ii) ownership of fifty percent (50%) or more of the\noutstanding shares, or (iii) beneficial ownership of such entity.\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity\nexercising permissions granted by this License.\n\n\"Source\" form shall mean the preferred form for making modifications,\nincluding but not limited to software source code, documentation\nsource, and configuration files.\n\n\"Object\" form shall mean any form resulting from mechanical\ntransformation or translation of a Source form, including but\nnot limited to compiled object code, generated documentation,\nand conversions to other media types.\n\n\"Work\" shall mean the work of authorship, whether in Source or\nObject form, made available under the License, as indicated by a\ncopyright notice that is included in or attached to the work\n(an example is provided in the Appendix below).\n\n\"Derivative Works\" shall mean any work, whether in Source or Object\nform, that is based on (or derived from) the Work and for which the\neditorial revisions, annotations, elaborations, or other modifications\nrepresent, as a whole, an original work of authorship. For the purposes\nof this License, Derivative Works shall not include works that remain\nseparable from, or merely link (or bind by name) to the interfaces of,\nthe Work and Derivative Works thereof.\n\n\"Contribution\" shall mean any work of authorship, including\nthe original version of the Work and any modifications or additions\nto that Work or Derivative Works thereof, that is intentionally\nsubmitted to Licensor for inclusion in the Work by the copyright owner\nor by an individual or Legal Entity authorized to submit on behalf of\nthe copyright owner. For the purposes of this definition, \"submitted\"\nmeans any form of electronic, verbal, or written communication sent\nto the Licensor or its representatives, including but not limited to\ncommunication on electronic mailing lists, source code control systems,\nand issue tracking systems that are managed by, or on behalf of, the\nLicensor for the purpose of discussing and improving the Work, but\nexcluding communication that is conspicuously marked or otherwise\ndesignated in writing by the copyright owner as \"Not a Contribution.\"\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity\non behalf of whom a Contribution has been received by Licensor and\nsubsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of\nthis License, each Contributor hereby grants to You a perpetual,\nworldwide, non-exclusive, no-charge, royalty-free, irrevocable\ncopyright license to reproduce, prepare Derivative Works of,\npublicly display, publicly perform, sublicense, and distribute the\nWork and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of\nthis License, each Contributor hereby grants to You a perpetual,\nworldwide, non-exclusive, no-charge, royalty-free, irrevocable\n(except as stated in this section) patent license to make, have made,\nuse, offer to sell, sell, import, and otherwise transfer the Work,\nwhere such license applies only to those patent claims licensable\nby such Contributor that are necessarily infringed by their\nContribution(s) alone or by combination of their Contribution(s)\nwith the Work to which such Contribution(s) was submitted. If You\ninstitute patent litigation against any entity (including a\ncross-claim or counterclaim in a lawsuit) alleging that the Work\nor a Contribution incorporated within the Work constitutes direct\nor contributory patent infringement, then any patent licenses\ngranted to You under this License for that Work shall terminate\nas of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the\nWork or Derivative Works thereof in any medium, with or without\nmodifications, and in Source or Object form, provided that You\nmeet the following conditions:\n\n(a) You must give any other recipients of the Work or\nDerivative Works a copy of this License; and\n\n(b) You must cause any modified files to carry prominent notices\nstating that You changed the files; and\n\n(c) You must retain, in the Source form of any Derivative Works\nthat You distribute, all copyright, patent, trademark, and\nattribution notices from the Source form of the Work,\nexcluding those notices that do not pertain to any part of\nthe Derivative Works; and\n\n(d) If the Work includes a \"NOTICE\" text file as part of its\ndistribution, then any Derivative Works that You distribute must\ninclude a readable copy of the attribution notices contained\nwithin such NOTICE file, excluding those notices that do not\npertain to any part of the Derivative Works, in at least one\nof the following places: within a NOTICE text file distributed\nas part of the Derivative Works; within the Source form or\ndocumentation, if provided along with the Derivative Works; or,\nwithin a display generated by the Derivative Works, if and\nwherever such third-party notices normally appear. The contents\nof the NOTICE file are for informational purposes only and\ndo not modify the License. You may add Your own attribution\nnotices within Derivative Works that You distribute, alongside\nor as an addendum to the NOTICE text from the Work, provided\nthat such additional attribution notices cannot be construed\nas modifying the License.\n\nYou may add Your own copyright statement to Your modifications and\nmay provide additional or different license terms and conditions\nfor use, reproduction, or distribution of Your modifications, or\nfor any such Derivative Works as a whole, provided Your use,\nreproduction, and distribution of the Work otherwise complies with\nthe conditions stated in this License.\n\n5. Submission of Contributions. Unless You explicitly state otherwise,\nany Contribution intentionally submitted for inclusion in the Work\nby You to the Licensor shall be under the terms and conditions of\nthis License, without any additional terms or conditions.\nNotwithstanding the above, nothing herein shall supersede or modify\nthe terms of any separate license agreement you may have executed\nwith Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade\nnames, trademarks, service marks, or product names of the Licensor,\nexcept as required for reasonable and customary use in describing the\norigin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or\nagreed to in writing, Licensor provides the Work (and each\nContributor provides its Contributions) on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\nimplied, including, without limitation, any warranties or conditions\nof TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\nPARTICULAR PURPOSE. You are solely responsible for determining the\nappropriateness of using or redistributing the Work and assume any\nrisks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory,\nwhether in tort (including negligence), contract, or otherwise,\nunless required by applicable law (such as deliberate and grossly\nnegligent acts) or agreed to in writing, shall any Contributor be\nliable to You for damages, including any direct, indirect, special,\nincidental, or consequential damages of any character arising as a\nresult of this License or out of the use or inability to use the\nWork (including but not limited to damages for loss of goodwill,\nwork stoppage, computer failure or malfunction, or any and all\nother commercial damages or losses), even if such Contributor\nhas been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing\nthe Work or Derivative Works thereof, You may choose to offer,\nand charge a fee for, acceptance of support, warranty, indemnity,\nor other liability obligations and/or rights consistent with this\nLicense. However, in accepting such obligations, You may act only\non Your own behalf and on Your sole responsibility, not on behalf\nof any other Contributor, and only if You agree to indemnify,\ndefend, and hold each Contributor harmless for any liability\nincurred by, or claims asserted against, such Contributor by reason\nof your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\nAPPENDIX: How to apply the Apache License to your work.\n\nTo apply the Apache License to your work, attach the following\nboilerplate notice, with the fields enclosed by brackets \"[]\"\nreplaced with your own identifying information. (Don't include\nthe brackets!)  The text should be enclosed in the appropriate\ncomment syntax for the file format. We also recommend that a\nfile or class name and description of purpose be included on the\nsame \"printed page\" as the copyright notice for easier\nidentification within third-party archives.\n\nCopyright 2019, Facebook, Inc\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n"
        },
        {
          "name": "MODEL_ZOO.md",
          "type": "blob",
          "size": 8.9228515625,
          "content": "# PySlowFast Model Zoo and Baselines\n\n## Kinetics 400 and 600\n\n| architecture | size |  crops x clips |  frame length x sample rate | top1 |  top5  |  model | config | dataset |\n| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n| C2D | R50 | 3 x 10 | 8 x 8 | 67.2 | 87.8 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/C2D_NOPOOL_8x8_R50.pkl) | Kinetics/c2/C2D_NOPOOL_8x8_R50 | K400 |\n| I3D | R50 | 3 x 10 | 8 x 8 | 73.5 | 90.8 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/I3D_8x8_R50.pkl) | Kinetics/c2/I3D_8x8_R50 | K400 |\n| I3D NLN | R50 | 3 x 10 | 8 x 8 | 74.0 | 91.1 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/I3D_NLN_8x8_R50.pkl) | Kinetics/c2/I3D_NLN_8x8_R50 | K400 |\n| Slow | R50 | 3 x 10 | 4 x 16 | 72.7 | 90.3 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/SLOWONLY_4x16_R50.pkl) | Kinetics/c2/SLOW_4x16_R50 | K400 |\n| Slow | R50 | 3 x 10 | 8 x 8 | 74.8 | 91.6 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/SLOWONLY_8x8_R50.pkl) | Kinetics/c2/SLOW_8x8_R50 | K400 |\n| SlowFast | R50 | 3 x 10 | 4 x 16 | 75.6 | 92.0 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/SLOWFAST_4x16_R50.pkl) | Kinetics/c2/SLOWFAST_4x16_R50 | K400 |\n| SlowFast | R50 | 3 x 10 | 8 x 8 | 77.0 | 92.6 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/SLOWFAST_8x8_R50.pkl) | Kinetics/c2/SLOWFAST_8x8_R50 | K400 |\n| MViTv1 | B-Conv | 1 x 5 | 16 x 4 | 78.4 | 93.5 | [`link`](https://drive.google.com/file/d/194gJinVejq6A1FmySNKQ8vAN5-FOY-QL/view?usp=sharing) | Kinetics/MVIT_B_16x4_CONV | K400 |\n| rev-MViT | B-Conv | 1 x 5 | 16 x 4 | 78.4 | 93.4 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/rev/REV_MVIT_B_16x4.pyth) | Kinetics/REV_MVIT_B_16x4_CONV | K400 |\n| MViTv1 | B-Conv | 1 x 5 | 32 x 3 | 80.4 | 94.8 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/mvit/k400.pyth) | Kinetics/MVIT_B_32x3_CONV | K400 |\n| MViTv1 | B-Conv | 1 x 5 | 32 x 3 | 83.9 | 96.5 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/mvit/k600.pyth) | Kinetics/MVIT_B_32x3_CONV_K600 | K600 |\n| MViTv2 | S | 1 x 5 | 16 x 4 | 81.0 | 94.6 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/mvitv2/pysf_video_models/MViTv2_S_16x4_k400_f302660347.pyth) | Kinetics/MVITv2_S_16x4 | K400 |\n| MViTv2 | B | 1 x 5 | 32 x 3 | 82.9 | 95.7 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/mvitv2/pysf_video_models/MViTv2_B_32x3_k400_f304025456.pyth) | Kinetics/MVITv2_B_32x3 | K400 |\n\n## X3D models (details in projects/x3d)\n\n|    architecture     |  size  | pretrain |    frame length x sample rate     | top1 10-view | top1 30-view | parameters (M) | FLOPs (G) | model | config |\n| :-------------: | :-----: | :-----: | :-------------: | :------: | :------: | :------------: | :----: | :------: | :------: |\n| X3D | XS | - | 4 x 12 | 68.7 | 69.5 | 3.8 | 0.60 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/x3d_models/x3d_xs.pyth) | Kinetics/X3D_XS |\n| X3D | S | - | 13 x 6 | 73.1 | 73.5 | 3.8 | 1.96 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/x3d_models/x3d_s.pyth) | Kinetics/X3D_S |\n| X3D | M | - | 16 x 5 | 75.1 | 76.2 | 3.8 | 4.73 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/x3d_models/x3d_m.pyth) | Kinetics/X3D_M |\n| X3D | L | - | 16 x 5 | 76.9 | 77.5 | 6.2 | 18.37 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/x3d_models/x3d_l.pyth) | Kinetics/X3D_L |\n\n## AVA\n\n| architecture | size | Pretrain Model |  frame length x sample rate  | MAP | AVA version | model |\n| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |------------- |\n| Slow | R50 | [Kinetics 400](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/ava/pretrain/C2D_8x8_R50.pkl) | 4 x 16 | 19.5 | 2.2 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/ava/C2D_8x8_R50.pkl) |\n| SlowFast | R101 | [Kinetics 600](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/ava/pretrain/SLOWFAST_32x2_R101_50_50_v2.1.pkl) | 8 x 8 | 28.2 | 2.1 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/ava/SLOWFAST_32x2_R101_50_50_v2.1.pkl) |\n| SlowFast | R101 | [Kinetics 600](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/ava/pretrain/SLOWFAST_32x2_R101_50_50.pkl) | 8 x 8 | 29.1 | 2.2 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/ava/SLOWFAST_32x2_R101_50_50.pkl) |\n| SlowFast | R101 | [Kinetics 600](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/ava/pretrain/SLOWFAST_64x2_R101_50_50.pkl) | 16 x 8 | 29.4 | 2.2 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/ava/SLOWFAST_64x2_R101_50_50.pkl) |\n\n## Multigrid Training\n\n***Update June, 2020:*** In the following we provide (reimplemented) models from  \"[A Multigrid Method for Efficiently Training Video Models\n](https://arxiv.org/abs/1912.00998)\" paper. The multigrid method trains about 3-6x faster than the original training on multiple datasets. See [projects/multigrid](projects/multigrid/README.md) for more information. The following provides models, results, and example config files.\n\n#### Kinetics:\n| architecture | size |  pretrain |  frame length x sample rate | training | top1 |  top5  |  model | config |\n| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n| SlowFast | R50 | - | 8 x 8 | Standard | 76.8 | 92.7 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/pyslowfast/model_zoo/multigrid/model_zoo/Kinetics/SLOWFAST_8x8_R50_stepwise.pkl) | Kinetics/SLOWFAST_8x8_R50_stepwise |\n| SlowFast | R50 | - | 8 x 8 | Multigrid | 76.6 | 92.7 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/pyslowfast/model_zoo/multigrid/model_zoo/Kinetics/SLOWFAST_8x8_R50_stepwise_multigrid.pkl) | Kinetics/SLOWFAST_8x8_R50_stepwise_multigrid |\n\n(Here we use stepwise learning rate schedule.)\n\n#### Something-Something V2:\n| architecture | size |  pretrain |  frame length x sample rate | training | top1 |  top5  |  model | config |\n| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n| SlowFast | R50 | [Kinetics 400](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/SLOWFAST_8x8_R50.pkl) | 16 x 8 | Standard | 63.0 | 88.5 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/pyslowfast/model_zoo/multigrid/model_zoo/SSv2/SLOWFAST_16x8_R50.pkl) | SSv2/SLOWFAST_16x8_R50 |\n| SlowFast | R50 | [Kinetics 400](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/SLOWFAST_8x8_R50.pkl) | 16 x 8 | Multigrid | 63.5 | 88.7 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/pyslowfast/model_zoo/multigrid/model_zoo/SSv2/SLOWFAST_16x8_R50_multigrid.pkl) | SSv2/SLOWFAST_16x8_R50_multigrid |\n\n\n#### Charades\n| architecture | size |  pretrain |  frame length x sample rate | training | mAP |  model | config |\n| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n| SlowFast | R50 | [Kinetics 400](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/SLOWFAST_8x8_R50.pkl) | 16 x 8 | Standard | 38.9 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/pyslowfast/model_zoo/multigrid/model_zoo/Charades/SLOWFAST_16x8_R50.pkl) | SSv2/SLOWFAST_16x8_R50 |\n| SlowFast | R50 | [Kinetics 400](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/SLOWFAST_8x8_R50.pkl) | 16 x 8 | Multigrid | 38.6 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/pyslowfast/model_zoo/multigrid/model_zoo/Charades/SLOWFAST_16x8_R50_multigrid.pkl) | SSv2/SLOWFAST_16x8_R50_multigrid |\n\n\n## ImageNet\n\nWe also release the imagenet pretrained model if finetuning from ImageNet is preferred. The reported accuracy is obtained by center crop testing on the validation set.\n\n| architecture | size |  Top1 |  Top5  |  model  | Config |\n| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n| ResNet | R50 | 76.4 | 93.2 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/model_zoo/kinetics400/R50_IN1K.pyth) | ImageNet/RES_R50 |\n| MVIT | B-16-Conv | 82.9 | 96.3 | [`link`](https://drive.google.com/file/d/1dYYqUB-3DSgBVc9d6o-rW8ojtVsrFLgp/view?usp=sharing) | ImageNet/MVIT_B_16_CONV |\n| rev-VIT | Small | 79.9 | 94.9 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/rev/REV_VIT_S.pyth) | ImageNet/REV_VIT_S.yaml |\n| rev-VIT | Base |  81.8 | 95.6 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/rev/REV_VIT_B.pyth) | ImageNet/REV_VIT_B.yaml |\n| rev-MVIT | Base |  82.9* | 96.3 | [`link`](https://dl.fbaipublicfiles.com/pyslowfast/rev/REV_MVIT_B.pyth) | ImageNet/REV_MVIT_B_16_CONV.yaml |\n\n*please refer to [Reversible Model Zoo](projects/rev/README.md).\n\n## PyTorchVideo\n\nWe support and benchmark PyTorchVideo models and datasets in PySlowFast. See [projects/pytorchvideo](projects/pytorchvideo/README.md) for more information about PyTorchVideo Model Zoo.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.99609375,
          "content": "# PySlowFast\n\nPySlowFast is an open source video understanding codebase from FAIR that provides state-of-the-art video classification models with efficient training. This repository includes implementations of the following methods:\n\n- [SlowFast Networks for Video Recognition](https://arxiv.org/abs/1812.03982)\n- [Non-local Neural Networks](https://arxiv.org/abs/1711.07971)\n- [A Multigrid Method for Efficiently Training Video Models](https://arxiv.org/abs/1912.00998)\n- [X3D: Progressive Network Expansion for Efficient Video Recognition](https://arxiv.org/abs/2004.04730)\n- [Multiscale Vision Transformers](https://arxiv.org/abs/2104.11227)\n- [A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning](https://arxiv.org/abs/2104.14558)\n- [MViTv2: Improved Multiscale Vision Transformers for Classification and Detection](https://arxiv.org/abs/2112.01526)\n- [Masked Feature Prediction for Self-Supervised Visual Pre-Training](https://arxiv.org/abs/2112.09133)\n- [Masked Autoencoders As Spatiotemporal Learners](https://arxiv.org/abs/2205.09113)\n- [Reversible Vision Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Mangalam_Reversible_Vision_Transformers_CVPR_2022_paper.pdf)\n\n<div align=\"center\">\n  <img src=\"demo/ava_demo.gif\" width=\"600px\"/>\n</div>\n\n## Introduction\n\nThe goal of PySlowFast is to provide a high-performance, light-weight pytorch codebase provides state-of-the-art video backbones for video understanding research on different tasks (classification, detection, and etc). It is designed in order to support rapid implementation and evaluation of novel video research ideas. PySlowFast includes implementations of the following backbone network architectures:\n\n- SlowFast\n- Slow\n- C2D\n- I3D\n- Non-local Network\n- X3D\n- MViTv1 and MViTv2\n- Rev-ViT and Rev-MViT\n\n## Updates\n - We now [Reversible Vision Transformers](https://openaccess.thecvf.com/content/CVPR2022/papers/Mangalam_Reversible_Vision_Transformers_CVPR_2022_paper.pdf). Both Reversible ViT and MViT models released. See [`projects/rev`](./projects/rev/README.md).\n - We now support [MAE for Video](https://arxiv.org/abs/2104.11227.pdf). See [`projects/mae`](./projects/mae/README.md) for more information.\n - We now support [MaskFeat](https://arxiv.org/abs/2112.09133). See [`projects/maskfeat`](./projects/maskfeat/README.md) for more information.\n - We now support [MViTv2](https://arxiv.org/abs/2104.11227.pdf) in PySlowFast. See [`projects/mvitv2`](./projects/mvitv2/README.md) for more information.\n - We now support [A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning](https://arxiv.org/abs/2104.14558). See [`projects/contrastive_ssl`](./projects/contrastive_ssl/README.md) for more information.\n - We now support [Multiscale Vision Transformers](https://arxiv.org/abs/2104.11227.pdf) on Kinetics and ImageNet. See [`projects/mvit`](./projects/mvit/README.md) for more information.\n - We now support [PyTorchVideo](https://github.com/facebookresearch/pytorchvideo) models and datasets. See [`projects/pytorchvideo`](./projects/pytorchvideo/README.md) for more information.\n - We now support [X3D Models](https://arxiv.org/abs/2004.04730). See [`projects/x3d`](./projects/x3d/README.md) for more information.\n - We now support [Multigrid Training](https://arxiv.org/abs/1912.00998) for efficiently training video models. See [`projects/multigrid`](./projects/multigrid/README.md) for more information.\n - PySlowFast is released in conjunction with our [ICCV 2019 Tutorial](https://alexander-kirillov.github.io/tutorials/visual-recognition-iccv19/).\n\n## License\n\nPySlowFast is released under the [Apache 2.0 license](LICENSE).\n\n## Model Zoo and Baselines\n\nWe provide a large set of baseline results and trained models available for download in the PySlowFast [Model Zoo](MODEL_ZOO.md).\n\n## Installation\n\nPlease find installation instructions for PyTorch and PySlowFast in [INSTALL.md](INSTALL.md). You may follow the instructions in [DATASET.md](slowfast/datasets/DATASET.md) to prepare the datasets.\n\n## Quick Start\n\nFollow the example in [GETTING_STARTED.md](GETTING_STARTED.md) to start playing video models with PySlowFast.\n\n## Visualization Tools\n\nWe offer a range of visualization tools for the train/eval/test processes, model analysis, and for running inference with trained model.\nMore information at [Visualization Tools](VISUALIZATION_TOOLS.md).\n\n## Contributors\nPySlowFast is written and maintained by [Haoqi Fan](https://haoqifan.github.io/), [Yanghao Li](https://lyttonhao.github.io/), [Bo Xiong](https://www.cs.utexas.edu/~bxiong/), [Wan-Yen Lo](https://www.linkedin.com/in/wanyenlo/), [Christoph Feichtenhofer](https://feichtenhofer.github.io/).\n\n## Citing PySlowFast\nIf you find PySlowFast useful in your research, please use the following BibTeX entry for citation.\n```BibTeX\n@misc{fan2020pyslowfast,\n  author =       {Haoqi Fan and Yanghao Li and Bo Xiong and Wan-Yen Lo and\n                  Christoph Feichtenhofer},\n  title =        {PySlowFast},\n  howpublished = {\\url{https://github.com/facebookresearch/slowfast}},\n  year =         {2020}\n}\n```\n"
        },
        {
          "name": "VISUALIZATION_TOOLS.md",
          "type": "blob",
          "size": 6.1611328125,
          "content": "# Visualization Tools for PySlowFast\n\nThis document provides a brief intro for running various visualization tools provided with PySlowFast. Before launching any job, make sure you have properly installed the PySlowFast following the instruction in [README.md](README.md) and you have prepared the dataset following [DATASET.md](slowfast/datasets/DATASET.md) with the correct format.\n\n## Tensorboard Support for Train/Eval/Test\n\nWe provide Tensorboard support during the train/eval/test pipeline to assist live monitoring various metrics, and class-level performance\nwith loss/error graphs, confusion matrices and histograms. Enable Tensorboard support by adding the following to your yaml config file:\n\n```\nTENSORBOARD:\n  ENABLE: True\n  LOG_DIR: # Leave empty to use cfg.OUTPUT_DIR/runs-{cfg.TRAIN.DATASET} as path.\n  CLASS_NAMES_PATH: # Path to json file providing class_name - id mapping.\n  CONFUSION_MATRIX:\n    ENABLE: True\n    SUBSET_PATH: # Path to txt file contains class names separated by newline characters.\n                 # Only classes in this file will be visualized in the confusion matrix.\n  HISTOGRAM:\n    ENABLE: True\n    TOP_K: 10   # Top-k most frequently predicted classes for each class in the dataset.\n    SUBSET_PATH: # Path to txt file contains class names separated by newline characters.\n                 # Only classes in this file will be visualized with histograms.\n```\n\nMore details can be found at [defaults.py](slowfast/config/defaults.py)\n\n### Loss & Error Graphs on Tensorboard:\n\n<div align=\"center\">\n  <img src=\"demo/visualization/metrics/loss.png\" width=\"800px\"/>\n</div>\n\n### Confusion matrices:\n\n<div align=\"center\">\n  <img src=\"demo/visualization/metrics/cf_subset.png\" width=\"367px\" style=\"margin:10px;\"/>\n  <img src=\"demo/visualization/metrics/cf_parent.png\" width=\"350px\" style=\"margin:10px;\"/>\n</div>\n\n<div align=\"center\">\n\n</div>\n\nTo enable this mode, set:\n```\nTENSORBOARD:\n  ENABLE: True\n  CATEGORIES_PATH: # Path to a json file for categories -> classes mapping\n                   # in the format {\"parent_class\": [\"child_class1\", \"child_class2\",...], ...}.\n  CONFUSION_MATRIX:\n    ENABLE: True\n```\n\n### Histograms of top-k most frequent predictions:\n\n<div align=\"center\">\n  <img src=\"demo/visualization/metrics/hist1.png\" width=\"400px\" style=\"margin:10px;\"/>\n  <img src=\"demo/visualization/metrics/hist2.png\" width=\"406px\" style=\"margin:10px;\"/>\n</div>\n\n## Model Analysis\n\nIn addition, we provide tools to help with understanding your trained model(s), more options at [defaults.py](slowfast/config/defaults.py)\n\nAdding the following to your yaml config file:\n```\nTENSORBOARD:\n  ENABLE: True\n  MODEL_VIS:\n    ENABLE: True\n    MODEL_WEIGHTS: # Set to True to visualize model weights.\n    ACTIVATIONS: # Set to True to visualize feature maps.\n    INPUT_VIDEO: # Set to True to visualize the input video(s) for the corresponding feature maps.\n    LAYER_LIST: # List of layer names to visualize weights and activations for.\n    GRAD_CAM:\n      ENABLE: True\n      LAYER_LIST: # List of CNN layers to use for Grad-CAM visualization method.\n                  # The number of layer must be equal to the number of pathway(s).\n```\n\n### Weights Visualization on Tensorboard:\n\n<div align=\"center\">\n  <img src=\"demo/visualization/analysis/weights1.png\" width=\"300px\" style=\"margin:10px;\"/>\n  <img src=\"demo/visualization/analysis/weights2.png\" width=\"328px\" style=\"margin:18px;\"/>\n</div>\n\n### Feature Maps & Inputs Visualization:\n\n<div align=\"center\">\n  <img src=\"demo/visualization/analysis/activations.gif\" width=\"800px\"/>\n</div>\n\n### Input Videos Visualization with Grad-CAM:\n\n<div align=\"center\">\n  <img src=\"demo/visualization/analysis/gradcam.gif\" width=\"400px\" style=\"margin:10px;\"/>\n  <img src=\"demo/visualization/analysis/gradcam2.gif\" width=\"400px\" style=\"margin:10px;\"/>\n</div>\n\n## Run the Demo on Videos/Camera\n\nTo run inference with PySlowFast model(s) on wild video(s), add the following to your yaml config file:\n\n```\nDEMO:\n  ENABLE: True\n  LABEL_FILE_PATH: # Path to json file providing class_name - id mapping.\n  INPUT_VIDEO: # Path to input video file.\n  OUTPUT_FILE: # Path to output video file to write results to.\n               # Leave an empty string if you would like to display results to a window.\n  THREAD_ENABLE: # Run video reader/writer in the background with multi-threading.\n  NUM_VIS_INSTANCES: # Number of CPU(s)/processes use to run video visualizer.\n  NUM_CLIPS_SKIP: # Number of clips to skip prediction/visualization\n                  # (mostly to smoothen/improve display quality with wecam input).\n```\n\nIf you would like to use webcam as an input, in place of `DEMO.INPUT_VIDEO`, set `DEMO.WEBCAM` to the index of the webcam for input. Please check for more options at [defaults.py](slowfast/config/defaults.py)\n\n### Action Recognition Demo:\n<div align=\"center\">\n  <img src=\"demo/visualization/demo_gifs/recognition.gif\" width=\"600px\"/>\n</div>\n\n### Action Detection Demo:\n\n<div align=\"center\">\n  <img src=\"demo/visualization/demo_gifs/detection.gif\" width=\"600px\" style=\"margin:10px;\"/>\n</div>\n\n### Demo with AVA video(s):\nWe also offer an option to use trained models to create and visualize prediction results and ground-truth labels on AVA-format videos and metadata. An example config is:\n\n```\nDEMO:\n  ENABLE: True\n  OUTPUT_FILE: yourPath/output.mp4\n  LABEL_FILE_PATH:  yourPath/ava_classnames.json\n  INPUT_VIDEO: yourPath/frames/HVAmkvLrthQ  # Path to a video file or image folder\n  PREDS_BOXES: yourPath/ava_detection_train_boxes_and_labels_include_negative.csv # Path to pre-computed bouding boxes in AVA format.\n  GT_BOXES: yourPath/ava_train_v2.2.csv # Path to ground-truth boxes and labels in AVA format (optional).\n```\n\n<div align=\"center\">\n  <img src=\"demo/visualization/demo_gifs/ava_demo2.gif\" width=\"600px\" style=\"margin:10px;\"/>\n</div>\n\n\n### Run command\n```\npython \\tools\\run_net.py --cfg path/to/<pretrained_model_config_file>.yaml\n```\n### Download class name files\n- [AVA class names json file](https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/ava_classids.json)\n- [Kinetics class names json file](https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json)\n- [Kinetics parent-child class mapping](https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/parents.json)\n"
        },
        {
          "name": "ava_evaluation",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "linter.sh",
          "type": "blob",
          "size": 0.3359375,
          "content": "#!/bin/bash -e\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# Run this script at project root by \".linter.sh\" before you commit.\necho \"Running isort...\"\nisort -y -sp .\n\necho \"Running black...\"\nblack -l 80 .\n\necho \"Running flake...\"\nflake8 .\n\ncommand -v arc > /dev/null && {\n  echo \"Running arc lint ...\"\n  arc lint\n}\n"
        },
        {
          "name": "projects",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.7490234375,
          "content": "[isort]\nline_length=100\nmulti_line_output=4\nknown_standard_library=numpy,setuptools\nknown_myself=slowfast\nknown_third_party=fvcore,iopath,av,torch,pycocotools,yacs,termcolor,scipy,simplejson,matplotlib,detectron2,torchvision,yaml,tqdm,psutil,opencv-python,pandas,tensorboard,moviepy,sklearn,cv2,PIL\nno_lines_before=STDLIB,THIRDPARTY\nsections=FUTURE,STDLIB,THIRDPARTY,myself,FIRSTPARTY,LOCALFOLDER\ndefault_section=FIRSTPARTY\n\n[mypy]\npython_version=3.6\nignore_missing_imports = True\nwarn_unused_configs = True\ndisallow_untyped_defs = True\ncheck_untyped_defs = True\nwarn_unused_ignores = True\nwarn_redundant_casts = True\nshow_column_numbers = True\nfollow_imports = silent\nallow_redefinition = True\n; Require all functions to be annotated\ndisallow_incomplete_defs = True\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.7734375,
          "content": "#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\nfrom setuptools import find_packages, setup\n\nsetup(\n    name=\"slowfast\",\n    version=\"1.0\",\n    author=\"FAIR\",\n    url=\"unknown\",\n    description=\"SlowFast Video Understanding\",\n    install_requires=[\n        \"yacs>=0.1.6\",\n        \"pyyaml>=5.1\",\n        \"av\",\n        \"matplotlib\",\n        \"termcolor>=1.1\",\n        \"simplejson\",\n        \"tqdm\",\n        \"psutil\",\n        \"matplotlib\",\n        \"detectron2\",\n        \"opencv-python\",\n        \"pandas\",\n        \"torchvision>=0.4.2\",\n        \"PIL\",\n        \"sklearn\",\n        \"tensorboard\",\n        \"fairscale\",\n    ],\n    extras_require={\"tensorboard_video_visualization\": [\"moviepy\"]},\n    packages=find_packages(exclude=(\"configs\", \"tests\")),\n)\n"
        },
        {
          "name": "slowfast",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}