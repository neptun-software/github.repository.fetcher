{
  "metadata": {
    "timestamp": 1736561023427,
    "page": 788,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "yunjey/stargan",
      "stars": 5243,
      "defaultBranch": "master",
      "files": [
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0322265625,
          "content": "MIT License\n\nCopyright (c) 2017 \n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.2216796875,
          "content": "## StarGAN - Official PyTorch Implementation\n\n**\\*\\*\\*\\*\\* New: StarGAN v2 is available at https://github.com/clovaai/stargan-v2 \\*\\*\\*\\*\\***\n\n<p align=\"center\"><img width=\"100%\" src=\"jpg/main.jpg\" /></p>\n\nThis repository provides the official PyTorch implementation of the following paper:\n> **StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation**<br>\n> [Yunjey Choi](https://github.com/yunjey)<sup>1,2</sup>, [Minje Choi](https://github.com/mjc92)<sup>1,2</sup>, [Munyoung Kim](https://www.facebook.com/munyoung.kim.1291)<sup>2,3</sup>, [Jung-Woo Ha](https://www.facebook.com/jungwoo.ha.921)<sup>2</sup>, [Sung Kim](https://www.cse.ust.hk/~hunkim/)<sup>2,4</sup>, [Jaegul Choo](https://sites.google.com/site/jaegulchoo/)<sup>1,2</sup>    <br/>\n> <sup>1</sup>Korea University, <sup>2</sup>Clova AI Research, NAVER Corp. <br>\n> <sup>3</sup>The College of New Jersey, <sup>4</sup>Hong Kong University of Science and Technology <br/>\n> https://arxiv.org/abs/1711.09020 <br>\n>\n> **Abstract:** *Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.*\n\n## Dependencies\n* [Python 3.5+](https://www.continuum.io/downloads)\n* [PyTorch 0.4.0+](http://pytorch.org/)\n* [TensorFlow 1.3+](https://www.tensorflow.org/) (optional for tensorboard)\n\n\n## Downloading datasets\nTo download the CelebA dataset:\n```bash\ngit clone https://github.com/yunjey/StarGAN.git\ncd StarGAN/\nbash download.sh celeba\n```\n\nTo download the RaFD dataset, you must request access to the dataset from [the Radboud Faces Database website](http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main). Then, you need to create a folder structure as described [here](https://github.com/yunjey/StarGAN/blob/master/jpg/RaFD.md).\n\n## Training networks\nTo train StarGAN on CelebA, run the training script below. See [here](https://github.com/yunjey/StarGAN/blob/master/jpg/CelebA.md) for a list of selectable attributes in the CelebA dataset. If you change the `selected_attrs` argument, you should also change the `c_dim` argument accordingly.\n\n```bash\n# Train StarGAN using the CelebA dataset\npython main.py --mode train --dataset CelebA --image_size 128 --c_dim 5 \\\n               --sample_dir stargan_celeba/samples --log_dir stargan_celeba/logs \\\n               --model_save_dir stargan_celeba/models --result_dir stargan_celeba/results \\\n               --selected_attrs Black_Hair Blond_Hair Brown_Hair Male Young\n\n# Test StarGAN using the CelebA dataset\npython main.py --mode test --dataset CelebA --image_size 128 --c_dim 5 \\\n               --sample_dir stargan_celeba/samples --log_dir stargan_celeba/logs \\\n               --model_save_dir stargan_celeba/models --result_dir stargan_celeba/results \\\n               --selected_attrs Black_Hair Blond_Hair Brown_Hair Male Young\n```\n\nTo train StarGAN on RaFD:\n\n```bash\n# Train StarGAN using the RaFD dataset\npython main.py --mode train --dataset RaFD --image_size 128 \\\n               --c_dim 8 --rafd_image_dir data/RaFD/train \\\n               --sample_dir stargan_rafd/samples --log_dir stargan_rafd/logs \\\n               --model_save_dir stargan_rafd/models --result_dir stargan_rafd/results\n\n# Test StarGAN using the RaFD dataset\npython main.py --mode test --dataset RaFD --image_size 128 \\\n               --c_dim 8 --rafd_image_dir data/RaFD/test \\\n               --sample_dir stargan_rafd/samples --log_dir stargan_rafd/logs \\\n               --model_save_dir stargan_rafd/models --result_dir stargan_rafd/results\n```\n\nTo train StarGAN on both CelebA and RafD:\n\n```bash\n# Train StarGAN using both CelebA and RaFD datasets\npython main.py --mode=train --dataset Both --image_size 256 --c_dim 5 --c2_dim 8 \\\n               --sample_dir stargan_both/samples --log_dir stargan_both/logs \\\n               --model_save_dir stargan_both/models --result_dir stargan_both/results\n\n# Test StarGAN using both CelebA and RaFD datasets\npython main.py --mode test --dataset Both --image_size 256 --c_dim 5 --c2_dim 8 \\\n               --sample_dir stargan_both/samples --log_dir stargan_both/logs \\\n               --model_save_dir stargan_both/models --result_dir stargan_both/results\n```\n\nTo train StarGAN on your own dataset, create a folder structure in the same format as [RaFD](https://github.com/yunjey/StarGAN/blob/master/jpg/RaFD.md) and run the command:\n\n```bash\n# Train StarGAN on custom datasets\npython main.py --mode train --dataset RaFD --rafd_crop_size CROP_SIZE --image_size IMG_SIZE \\\n               --c_dim LABEL_DIM --rafd_image_dir TRAIN_IMG_DIR \\\n               --sample_dir stargan_custom/samples --log_dir stargan_custom/logs \\\n               --model_save_dir stargan_custom/models --result_dir stargan_custom/results\n\n# Test StarGAN on custom datasets\npython main.py --mode test --dataset RaFD --rafd_crop_size CROP_SIZE --image_size IMG_SIZE \\\n               --c_dim LABEL_DIM --rafd_image_dir TEST_IMG_DIR \\\n               --sample_dir stargan_custom/samples --log_dir stargan_custom/logs \\\n               --model_save_dir stargan_custom/models --result_dir stargan_custom/results\n```\n\n\n## Using pre-trained networks\nTo download a pre-trained model checkpoint, run the script below. The pre-trained model checkpoint will be downloaded and saved into `./stargan_celeba_128/models` directory.\n\n```bash\n$ bash download.sh pretrained-celeba-128x128\n```\n\nTo translate images using the pre-trained model, run the evaluation script below. The translated images will be saved into `./stargan_celeba_128/results` directory.\n\n```bash\n$ python main.py --mode test --dataset CelebA --image_size 128 --c_dim 5 \\\n                 --selected_attrs Black_Hair Blond_Hair Brown_Hair Male Young \\\n                 --model_save_dir='stargan_celeba_128/models' \\\n                 --result_dir='stargan_celeba_128/results'\n```\n\n## Citation\nIf you find this work useful for your research, please cite our [paper](https://arxiv.org/abs/1711.09020):\n```\n@inproceedings{choi2018stargan,\nauthor={Yunjey Choi and Minje Choi and Munyoung Kim and Jung-Woo Ha and Sunghun Kim and Jaegul Choo},\ntitle={StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation},\nbooktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\nyear={2018}\n}\n```\n\n## Acknowledgements\nThis work was mainly done while the first author did a research internship at [Clova AI Research, NAVER](https://clova.ai/en/research/research-area-detail.html?id=0). We thank all the researchers at NAVER, especially Donghyun Kwak, for insightful discussions.\n"
        },
        {
          "name": "data_loader.py",
          "type": "blob",
          "size": 3.16796875,
          "content": "from torch.utils import data\nfrom torchvision import transforms as T\nfrom torchvision.datasets import ImageFolder\nfrom PIL import Image\nimport torch\nimport os\nimport random\n\n\nclass CelebA(data.Dataset):\n    \"\"\"Dataset class for the CelebA dataset.\"\"\"\n\n    def __init__(self, image_dir, attr_path, selected_attrs, transform, mode):\n        \"\"\"Initialize and preprocess the CelebA dataset.\"\"\"\n        self.image_dir = image_dir\n        self.attr_path = attr_path\n        self.selected_attrs = selected_attrs\n        self.transform = transform\n        self.mode = mode\n        self.train_dataset = []\n        self.test_dataset = []\n        self.attr2idx = {}\n        self.idx2attr = {}\n        self.preprocess()\n\n        if mode == 'train':\n            self.num_images = len(self.train_dataset)\n        else:\n            self.num_images = len(self.test_dataset)\n\n    def preprocess(self):\n        \"\"\"Preprocess the CelebA attribute file.\"\"\"\n        lines = [line.rstrip() for line in open(self.attr_path, 'r')]\n        all_attr_names = lines[1].split()\n        for i, attr_name in enumerate(all_attr_names):\n            self.attr2idx[attr_name] = i\n            self.idx2attr[i] = attr_name\n\n        lines = lines[2:]\n        random.seed(1234)\n        random.shuffle(lines)\n        for i, line in enumerate(lines):\n            split = line.split()\n            filename = split[0]\n            values = split[1:]\n\n            label = []\n            for attr_name in self.selected_attrs:\n                idx = self.attr2idx[attr_name]\n                label.append(values[idx] == '1')\n\n            if (i+1) < 2000:\n                self.test_dataset.append([filename, label])\n            else:\n                self.train_dataset.append([filename, label])\n\n        print('Finished preprocessing the CelebA dataset...')\n\n    def __getitem__(self, index):\n        \"\"\"Return one image and its corresponding attribute label.\"\"\"\n        dataset = self.train_dataset if self.mode == 'train' else self.test_dataset\n        filename, label = dataset[index]\n        image = Image.open(os.path.join(self.image_dir, filename))\n        return self.transform(image), torch.FloatTensor(label)\n\n    def __len__(self):\n        \"\"\"Return the number of images.\"\"\"\n        return self.num_images\n\n\ndef get_loader(image_dir, attr_path, selected_attrs, crop_size=178, image_size=128, \n               batch_size=16, dataset='CelebA', mode='train', num_workers=1):\n    \"\"\"Build and return a data loader.\"\"\"\n    transform = []\n    if mode == 'train':\n        transform.append(T.RandomHorizontalFlip())\n    transform.append(T.CenterCrop(crop_size))\n    transform.append(T.Resize(image_size))\n    transform.append(T.ToTensor())\n    transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n    transform = T.Compose(transform)\n\n    if dataset == 'CelebA':\n        dataset = CelebA(image_dir, attr_path, selected_attrs, transform, mode)\n    elif dataset == 'RaFD':\n        dataset = ImageFolder(image_dir, transform)\n\n    data_loader = data.DataLoader(dataset=dataset,\n                                  batch_size=batch_size,\n                                  shuffle=(mode=='train'),\n                                  num_workers=num_workers)\n    return data_loader"
        },
        {
          "name": "download.sh",
          "type": "blob",
          "size": 1.2548828125,
          "content": "FILE=$1\n\nif [ $FILE == \"celeba\" ]; then\n\n    # CelebA images and attribute labels\n    URL=https://www.dropbox.com/s/d1kjpkqklf0uw77/celeba.zip?dl=0\n    ZIP_FILE=./data/celeba.zip\n    mkdir -p ./data/\n    wget -N $URL -O $ZIP_FILE\n    unzip $ZIP_FILE -d ./data/\n    rm $ZIP_FILE\n\n\nelif [ $FILE == 'pretrained-celeba-128x128' ]; then\n\n    # StarGAN trained on CelebA (Black_Hair, Blond_Hair, Brown_Hair, Male, Young), 128x128 resolution\n    URL=https://www.dropbox.com/s/7e966qq0nlxwte4/celeba-128x128-5attrs.zip?dl=0\n    ZIP_FILE=./stargan_celeba_128/models/celeba-128x128-5attrs.zip\n    mkdir -p ./stargan_celeba_128/models/\n    wget -N $URL -O $ZIP_FILE\n    unzip $ZIP_FILE -d ./stargan_celeba_128/models/\n    rm $ZIP_FILE\n\nelif [ $FILE == 'pretrained-celeba-256x256' ]; then\n\n    # StarGAN trained on CelebA (Black_Hair, Blond_Hair, Brown_Hair, Male, Young), 256x256 resolution\n    URL=https://www.dropbox.com/s/zdq6roqf63m0v5f/celeba-256x256-5attrs.zip?dl=0\n    ZIP_FILE=./stargan_celeba_256/models/celeba-256x256-5attrs.zip\n    mkdir -p ./stargan_celeba_256/models/\n    wget -N $URL -O $ZIP_FILE\n    unzip $ZIP_FILE -d ./stargan_celeba_256/models/\n    rm $ZIP_FILE\n\nelse\n    echo \"Available arguments are celeba, pretrained-celeba-128x128, pretrained-celeba-256x256.\"\n    exit 1\nfi"
        },
        {
          "name": "jpg",
          "type": "tree",
          "content": null
        },
        {
          "name": "logger.py",
          "type": "blob",
          "size": 0.4091796875,
          "content": "import tensorflow as tf\n\n\nclass Logger(object):\n    \"\"\"Tensorboard logger.\"\"\"\n\n    def __init__(self, log_dir):\n        \"\"\"Initialize summary writer.\"\"\"\n        self.writer = tf.summary.FileWriter(log_dir)\n\n    def scalar_summary(self, tag, value, step):\n        \"\"\"Add scalar summary.\"\"\"\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n        self.writer.add_summary(summary, step)"
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 5.5439453125,
          "content": "import os\nimport argparse\nfrom solver import Solver\nfrom data_loader import get_loader\nfrom torch.backends import cudnn\n\n\ndef str2bool(v):\n    return v.lower() in ('true')\n\ndef main(config):\n    # For fast training.\n    cudnn.benchmark = True\n\n    # Create directories if not exist.\n    if not os.path.exists(config.log_dir):\n        os.makedirs(config.log_dir)\n    if not os.path.exists(config.model_save_dir):\n        os.makedirs(config.model_save_dir)\n    if not os.path.exists(config.sample_dir):\n        os.makedirs(config.sample_dir)\n    if not os.path.exists(config.result_dir):\n        os.makedirs(config.result_dir)\n\n    # Data loader.\n    celeba_loader = None\n    rafd_loader = None\n\n    if config.dataset in ['CelebA', 'Both']:\n        celeba_loader = get_loader(config.celeba_image_dir, config.attr_path, config.selected_attrs,\n                                   config.celeba_crop_size, config.image_size, config.batch_size,\n                                   'CelebA', config.mode, config.num_workers)\n    if config.dataset in ['RaFD', 'Both']:\n        rafd_loader = get_loader(config.rafd_image_dir, None, None,\n                                 config.rafd_crop_size, config.image_size, config.batch_size,\n                                 'RaFD', config.mode, config.num_workers)\n    \n\n    # Solver for training and testing StarGAN.\n    solver = Solver(celeba_loader, rafd_loader, config)\n\n    if config.mode == 'train':\n        if config.dataset in ['CelebA', 'RaFD']:\n            solver.train()\n        elif config.dataset in ['Both']:\n            solver.train_multi()\n    elif config.mode == 'test':\n        if config.dataset in ['CelebA', 'RaFD']:\n            solver.test()\n        elif config.dataset in ['Both']:\n            solver.test_multi()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    # Model configuration.\n    parser.add_argument('--c_dim', type=int, default=5, help='dimension of domain labels (1st dataset)')\n    parser.add_argument('--c2_dim', type=int, default=8, help='dimension of domain labels (2nd dataset)')\n    parser.add_argument('--celeba_crop_size', type=int, default=178, help='crop size for the CelebA dataset')\n    parser.add_argument('--rafd_crop_size', type=int, default=256, help='crop size for the RaFD dataset')\n    parser.add_argument('--image_size', type=int, default=128, help='image resolution')\n    parser.add_argument('--g_conv_dim', type=int, default=64, help='number of conv filters in the first layer of G')\n    parser.add_argument('--d_conv_dim', type=int, default=64, help='number of conv filters in the first layer of D')\n    parser.add_argument('--g_repeat_num', type=int, default=6, help='number of residual blocks in G')\n    parser.add_argument('--d_repeat_num', type=int, default=6, help='number of strided conv layers in D')\n    parser.add_argument('--lambda_cls', type=float, default=1, help='weight for domain classification loss')\n    parser.add_argument('--lambda_rec', type=float, default=10, help='weight for reconstruction loss')\n    parser.add_argument('--lambda_gp', type=float, default=10, help='weight for gradient penalty')\n    \n    # Training configuration.\n    parser.add_argument('--dataset', type=str, default='CelebA', choices=['CelebA', 'RaFD', 'Both'])\n    parser.add_argument('--batch_size', type=int, default=16, help='mini-batch size')\n    parser.add_argument('--num_iters', type=int, default=200000, help='number of total iterations for training D')\n    parser.add_argument('--num_iters_decay', type=int, default=100000, help='number of iterations for decaying lr')\n    parser.add_argument('--g_lr', type=float, default=0.0001, help='learning rate for G')\n    parser.add_argument('--d_lr', type=float, default=0.0001, help='learning rate for D')\n    parser.add_argument('--n_critic', type=int, default=5, help='number of D updates per each G update')\n    parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n    parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n    parser.add_argument('--resume_iters', type=int, default=None, help='resume training from this step')\n    parser.add_argument('--selected_attrs', '--list', nargs='+', help='selected attributes for the CelebA dataset',\n                        default=['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young'])\n\n    # Test configuration.\n    parser.add_argument('--test_iters', type=int, default=200000, help='test model from this step')\n\n    # Miscellaneous.\n    parser.add_argument('--num_workers', type=int, default=1)\n    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])\n    parser.add_argument('--use_tensorboard', type=str2bool, default=True)\n\n    # Directories.\n    parser.add_argument('--celeba_image_dir', type=str, default='data/celeba/images')\n    parser.add_argument('--attr_path', type=str, default='data/celeba/list_attr_celeba.txt')\n    parser.add_argument('--rafd_image_dir', type=str, default='data/RaFD/train')\n    parser.add_argument('--log_dir', type=str, default='stargan/logs')\n    parser.add_argument('--model_save_dir', type=str, default='stargan/models')\n    parser.add_argument('--sample_dir', type=str, default='stargan/samples')\n    parser.add_argument('--result_dir', type=str, default='stargan/results')\n\n    # Step size.\n    parser.add_argument('--log_step', type=int, default=10)\n    parser.add_argument('--sample_step', type=int, default=1000)\n    parser.add_argument('--model_save_step', type=int, default=10000)\n    parser.add_argument('--lr_update_step', type=int, default=1000)\n\n    config = parser.parse_args()\n    print(config)\n    main(config)"
        },
        {
          "name": "model.py",
          "type": "blob",
          "size": 3.6318359375,
          "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\nclass ResidualBlock(nn.Module):\n    \"\"\"Residual Block with instance normalization.\"\"\"\n    def __init__(self, dim_in, dim_out):\n        super(ResidualBlock, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n\n    def forward(self, x):\n        return x + self.main(x)\n\n\nclass Generator(nn.Module):\n    \"\"\"Generator network.\"\"\"\n    def __init__(self, conv_dim=64, c_dim=5, repeat_num=6):\n        super(Generator, self).__init__()\n\n        layers = []\n        layers.append(nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n        layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True))\n        layers.append(nn.ReLU(inplace=True))\n\n        # Down-sampling layers.\n        curr_dim = conv_dim\n        for i in range(2):\n            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))\n            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True))\n            layers.append(nn.ReLU(inplace=True))\n            curr_dim = curr_dim * 2\n\n        # Bottleneck layers.\n        for i in range(repeat_num):\n            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n\n        # Up-sampling layers.\n        for i in range(2):\n            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False))\n            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True, track_running_stats=True))\n            layers.append(nn.ReLU(inplace=True))\n            curr_dim = curr_dim // 2\n\n        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n        layers.append(nn.Tanh())\n        self.main = nn.Sequential(*layers)\n\n    def forward(self, x, c):\n        # Replicate spatially and concatenate domain information.\n        # Note that this type of label conditioning does not work at all if we use reflection padding in Conv2d.\n        # This is because instance normalization ignores the shifting (or bias) effect.\n        c = c.view(c.size(0), c.size(1), 1, 1)\n        c = c.repeat(1, 1, x.size(2), x.size(3))\n        x = torch.cat([x, c], dim=1)\n        return self.main(x)\n\n\nclass Discriminator(nn.Module):\n    \"\"\"Discriminator network with PatchGAN.\"\"\"\n    def __init__(self, image_size=128, conv_dim=64, c_dim=5, repeat_num=6):\n        super(Discriminator, self).__init__()\n        layers = []\n        layers.append(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1))\n        layers.append(nn.LeakyReLU(0.01))\n\n        curr_dim = conv_dim\n        for i in range(1, repeat_num):\n            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1))\n            layers.append(nn.LeakyReLU(0.01))\n            curr_dim = curr_dim * 2\n\n        kernel_size = int(image_size / np.power(2, repeat_num))\n        self.main = nn.Sequential(*layers)\n        self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False)\n        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=False)\n        \n    def forward(self, x):\n        h = self.main(x)\n        out_src = self.conv1(h)\n        out_cls = self.conv2(h)\n        return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))\n"
        },
        {
          "name": "solver.py",
          "type": "blob",
          "size": 26.89453125,
          "content": "from model import Generator\nfrom model import Discriminator\nfrom torch.autograd import Variable\nfrom torchvision.utils import save_image\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport os\nimport time\nimport datetime\n\n\nclass Solver(object):\n    \"\"\"Solver for training and testing StarGAN.\"\"\"\n\n    def __init__(self, celeba_loader, rafd_loader, config):\n        \"\"\"Initialize configurations.\"\"\"\n\n        # Data loader.\n        self.celeba_loader = celeba_loader\n        self.rafd_loader = rafd_loader\n\n        # Model configurations.\n        self.c_dim = config.c_dim\n        self.c2_dim = config.c2_dim\n        self.image_size = config.image_size\n        self.g_conv_dim = config.g_conv_dim\n        self.d_conv_dim = config.d_conv_dim\n        self.g_repeat_num = config.g_repeat_num\n        self.d_repeat_num = config.d_repeat_num\n        self.lambda_cls = config.lambda_cls\n        self.lambda_rec = config.lambda_rec\n        self.lambda_gp = config.lambda_gp\n\n        # Training configurations.\n        self.dataset = config.dataset\n        self.batch_size = config.batch_size\n        self.num_iters = config.num_iters\n        self.num_iters_decay = config.num_iters_decay\n        self.g_lr = config.g_lr\n        self.d_lr = config.d_lr\n        self.n_critic = config.n_critic\n        self.beta1 = config.beta1\n        self.beta2 = config.beta2\n        self.resume_iters = config.resume_iters\n        self.selected_attrs = config.selected_attrs\n\n        # Test configurations.\n        self.test_iters = config.test_iters\n\n        # Miscellaneous.\n        self.use_tensorboard = config.use_tensorboard\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # Directories.\n        self.log_dir = config.log_dir\n        self.sample_dir = config.sample_dir\n        self.model_save_dir = config.model_save_dir\n        self.result_dir = config.result_dir\n\n        # Step size.\n        self.log_step = config.log_step\n        self.sample_step = config.sample_step\n        self.model_save_step = config.model_save_step\n        self.lr_update_step = config.lr_update_step\n\n        # Build the model and tensorboard.\n        self.build_model()\n        if self.use_tensorboard:\n            self.build_tensorboard()\n\n    def build_model(self):\n        \"\"\"Create a generator and a discriminator.\"\"\"\n        if self.dataset in ['CelebA', 'RaFD']:\n            self.G = Generator(self.g_conv_dim, self.c_dim, self.g_repeat_num)\n            self.D = Discriminator(self.image_size, self.d_conv_dim, self.c_dim, self.d_repeat_num) \n        elif self.dataset in ['Both']:\n            self.G = Generator(self.g_conv_dim, self.c_dim+self.c2_dim+2, self.g_repeat_num)   # 2 for mask vector.\n            self.D = Discriminator(self.image_size, self.d_conv_dim, self.c_dim+self.c2_dim, self.d_repeat_num)\n\n        self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n        self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n        self.print_network(self.G, 'G')\n        self.print_network(self.D, 'D')\n            \n        self.G.to(self.device)\n        self.D.to(self.device)\n\n    def print_network(self, model, name):\n        \"\"\"Print out the network information.\"\"\"\n        num_params = 0\n        for p in model.parameters():\n            num_params += p.numel()\n        print(model)\n        print(name)\n        print(\"The number of parameters: {}\".format(num_params))\n\n    def restore_model(self, resume_iters):\n        \"\"\"Restore the trained generator and discriminator.\"\"\"\n        print('Loading the trained models from step {}...'.format(resume_iters))\n        G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(resume_iters))\n        D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(resume_iters))\n        self.G.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage))\n        self.D.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage))\n\n    def build_tensorboard(self):\n        \"\"\"Build a tensorboard logger.\"\"\"\n        from logger import Logger\n        self.logger = Logger(self.log_dir)\n\n    def update_lr(self, g_lr, d_lr):\n        \"\"\"Decay learning rates of the generator and discriminator.\"\"\"\n        for param_group in self.g_optimizer.param_groups:\n            param_group['lr'] = g_lr\n        for param_group in self.d_optimizer.param_groups:\n            param_group['lr'] = d_lr\n\n    def reset_grad(self):\n        \"\"\"Reset the gradient buffers.\"\"\"\n        self.g_optimizer.zero_grad()\n        self.d_optimizer.zero_grad()\n\n    def denorm(self, x):\n        \"\"\"Convert the range from [-1, 1] to [0, 1].\"\"\"\n        out = (x + 1) / 2\n        return out.clamp_(0, 1)\n\n    def gradient_penalty(self, y, x):\n        \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n        weight = torch.ones(y.size()).to(self.device)\n        dydx = torch.autograd.grad(outputs=y,\n                                   inputs=x,\n                                   grad_outputs=weight,\n                                   retain_graph=True,\n                                   create_graph=True,\n                                   only_inputs=True)[0]\n\n        dydx = dydx.view(dydx.size(0), -1)\n        dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1))\n        return torch.mean((dydx_l2norm-1)**2)\n\n    def label2onehot(self, labels, dim):\n        \"\"\"Convert label indices to one-hot vectors.\"\"\"\n        batch_size = labels.size(0)\n        out = torch.zeros(batch_size, dim)\n        out[np.arange(batch_size), labels.long()] = 1\n        return out\n\n    def create_labels(self, c_org, c_dim=5, dataset='CelebA', selected_attrs=None):\n        \"\"\"Generate target domain labels for debugging and testing.\"\"\"\n        # Get hair color indices.\n        if dataset == 'CelebA':\n            hair_color_indices = []\n            for i, attr_name in enumerate(selected_attrs):\n                if attr_name in ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair']:\n                    hair_color_indices.append(i)\n\n        c_trg_list = []\n        for i in range(c_dim):\n            if dataset == 'CelebA':\n                c_trg = c_org.clone()\n                if i in hair_color_indices:  # Set one hair color to 1 and the rest to 0.\n                    c_trg[:, i] = 1\n                    for j in hair_color_indices:\n                        if j != i:\n                            c_trg[:, j] = 0\n                else:\n                    c_trg[:, i] = (c_trg[:, i] == 0)  # Reverse attribute value.\n            elif dataset == 'RaFD':\n                c_trg = self.label2onehot(torch.ones(c_org.size(0))*i, c_dim)\n\n            c_trg_list.append(c_trg.to(self.device))\n        return c_trg_list\n\n    def classification_loss(self, logit, target, dataset='CelebA'):\n        \"\"\"Compute binary or softmax cross entropy loss.\"\"\"\n        if dataset == 'CelebA':\n            return F.binary_cross_entropy_with_logits(logit, target, size_average=False) / logit.size(0)\n        elif dataset == 'RaFD':\n            return F.cross_entropy(logit, target)\n\n    def train(self):\n        \"\"\"Train StarGAN within a single dataset.\"\"\"\n        # Set data loader.\n        if self.dataset == 'CelebA':\n            data_loader = self.celeba_loader\n        elif self.dataset == 'RaFD':\n            data_loader = self.rafd_loader\n\n        # Fetch fixed inputs for debugging.\n        data_iter = iter(data_loader)\n        x_fixed, c_org = next(data_iter)\n        x_fixed = x_fixed.to(self.device)\n        c_fixed_list = self.create_labels(c_org, self.c_dim, self.dataset, self.selected_attrs)\n\n        # Learning rate cache for decaying.\n        g_lr = self.g_lr\n        d_lr = self.d_lr\n\n        # Start training from scratch or resume training.\n        start_iters = 0\n        if self.resume_iters:\n            start_iters = self.resume_iters\n            self.restore_model(self.resume_iters)\n\n        # Start training.\n        print('Start training...')\n        start_time = time.time()\n        for i in range(start_iters, self.num_iters):\n\n            # =================================================================================== #\n            #                             1. Preprocess input data                                #\n            # =================================================================================== #\n\n            # Fetch real images and labels.\n            try:\n                x_real, label_org = next(data_iter)\n            except:\n                data_iter = iter(data_loader)\n                x_real, label_org = next(data_iter)\n\n            # Generate target domain labels randomly.\n            rand_idx = torch.randperm(label_org.size(0))\n            label_trg = label_org[rand_idx]\n\n            if self.dataset == 'CelebA':\n                c_org = label_org.clone()\n                c_trg = label_trg.clone()\n            elif self.dataset == 'RaFD':\n                c_org = self.label2onehot(label_org, self.c_dim)\n                c_trg = self.label2onehot(label_trg, self.c_dim)\n\n            x_real = x_real.to(self.device)           # Input images.\n            c_org = c_org.to(self.device)             # Original domain labels.\n            c_trg = c_trg.to(self.device)             # Target domain labels.\n            label_org = label_org.to(self.device)     # Labels for computing classification loss.\n            label_trg = label_trg.to(self.device)     # Labels for computing classification loss.\n\n            # =================================================================================== #\n            #                             2. Train the discriminator                              #\n            # =================================================================================== #\n\n            # Compute loss with real images.\n            out_src, out_cls = self.D(x_real)\n            d_loss_real = - torch.mean(out_src)\n            d_loss_cls = self.classification_loss(out_cls, label_org, self.dataset)\n\n            # Compute loss with fake images.\n            x_fake = self.G(x_real, c_trg)\n            out_src, out_cls = self.D(x_fake.detach())\n            d_loss_fake = torch.mean(out_src)\n\n            # Compute loss for gradient penalty.\n            alpha = torch.rand(x_real.size(0), 1, 1, 1).to(self.device)\n            x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True)\n            out_src, _ = self.D(x_hat)\n            d_loss_gp = self.gradient_penalty(out_src, x_hat)\n\n            # Backward and optimize.\n            d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gp\n            self.reset_grad()\n            d_loss.backward()\n            self.d_optimizer.step()\n\n            # Logging.\n            loss = {}\n            loss['D/loss_real'] = d_loss_real.item()\n            loss['D/loss_fake'] = d_loss_fake.item()\n            loss['D/loss_cls'] = d_loss_cls.item()\n            loss['D/loss_gp'] = d_loss_gp.item()\n            \n            # =================================================================================== #\n            #                               3. Train the generator                                #\n            # =================================================================================== #\n            \n            if (i+1) % self.n_critic == 0:\n                # Original-to-target domain.\n                x_fake = self.G(x_real, c_trg)\n                out_src, out_cls = self.D(x_fake)\n                g_loss_fake = - torch.mean(out_src)\n                g_loss_cls = self.classification_loss(out_cls, label_trg, self.dataset)\n\n                # Target-to-original domain.\n                x_reconst = self.G(x_fake, c_org)\n                g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))\n\n                # Backward and optimize.\n                g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls\n                self.reset_grad()\n                g_loss.backward()\n                self.g_optimizer.step()\n\n                # Logging.\n                loss['G/loss_fake'] = g_loss_fake.item()\n                loss['G/loss_rec'] = g_loss_rec.item()\n                loss['G/loss_cls'] = g_loss_cls.item()\n\n            # =================================================================================== #\n            #                                 4. Miscellaneous                                    #\n            # =================================================================================== #\n\n            # Print out training information.\n            if (i+1) % self.log_step == 0:\n                et = time.time() - start_time\n                et = str(datetime.timedelta(seconds=et))[:-7]\n                log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n                for tag, value in loss.items():\n                    log += \", {}: {:.4f}\".format(tag, value)\n                print(log)\n\n                if self.use_tensorboard:\n                    for tag, value in loss.items():\n                        self.logger.scalar_summary(tag, value, i+1)\n\n            # Translate fixed images for debugging.\n            if (i+1) % self.sample_step == 0:\n                with torch.no_grad():\n                    x_fake_list = [x_fixed]\n                    for c_fixed in c_fixed_list:\n                        x_fake_list.append(self.G(x_fixed, c_fixed))\n                    x_concat = torch.cat(x_fake_list, dim=3)\n                    sample_path = os.path.join(self.sample_dir, '{}-images.jpg'.format(i+1))\n                    save_image(self.denorm(x_concat.data.cpu()), sample_path, nrow=1, padding=0)\n                    print('Saved real and fake images into {}...'.format(sample_path))\n\n            # Save model checkpoints.\n            if (i+1) % self.model_save_step == 0:\n                G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(i+1))\n                D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(i+1))\n                torch.save(self.G.state_dict(), G_path)\n                torch.save(self.D.state_dict(), D_path)\n                print('Saved model checkpoints into {}...'.format(self.model_save_dir))\n\n            # Decay learning rates.\n            if (i+1) % self.lr_update_step == 0 and (i+1) > (self.num_iters - self.num_iters_decay):\n                g_lr -= (self.g_lr / float(self.num_iters_decay))\n                d_lr -= (self.d_lr / float(self.num_iters_decay))\n                self.update_lr(g_lr, d_lr)\n                print ('Decayed learning rates, g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))\n\n    def train_multi(self):\n        \"\"\"Train StarGAN with multiple datasets.\"\"\"        \n        # Data iterators.\n        celeba_iter = iter(self.celeba_loader)\n        rafd_iter = iter(self.rafd_loader)\n\n        # Fetch fixed inputs for debugging.\n        x_fixed, c_org = next(celeba_iter)\n        x_fixed = x_fixed.to(self.device)\n        c_celeba_list = self.create_labels(c_org, self.c_dim, 'CelebA', self.selected_attrs)\n        c_rafd_list = self.create_labels(c_org, self.c2_dim, 'RaFD')\n        zero_celeba = torch.zeros(x_fixed.size(0), self.c_dim).to(self.device)           # Zero vector for CelebA.\n        zero_rafd = torch.zeros(x_fixed.size(0), self.c2_dim).to(self.device)             # Zero vector for RaFD.\n        mask_celeba = self.label2onehot(torch.zeros(x_fixed.size(0)), 2).to(self.device)  # Mask vector: [1, 0].\n        mask_rafd = self.label2onehot(torch.ones(x_fixed.size(0)), 2).to(self.device)     # Mask vector: [0, 1].\n\n        # Learning rate cache for decaying.\n        g_lr = self.g_lr\n        d_lr = self.d_lr\n\n        # Start training from scratch or resume training.\n        start_iters = 0\n        if self.resume_iters:\n            start_iters = self.resume_iters\n            self.restore_model(self.resume_iters)\n\n        # Start training.\n        print('Start training...')\n        start_time = time.time()\n        for i in range(start_iters, self.num_iters):\n            for dataset in ['CelebA', 'RaFD']:\n\n                # =================================================================================== #\n                #                             1. Preprocess input data                                #\n                # =================================================================================== #\n                \n                # Fetch real images and labels.\n                data_iter = celeba_iter if dataset == 'CelebA' else rafd_iter\n                \n                try:\n                    x_real, label_org = next(data_iter)\n                except:\n                    if dataset == 'CelebA':\n                        celeba_iter = iter(self.celeba_loader)\n                        x_real, label_org = next(celeba_iter)\n                    elif dataset == 'RaFD':\n                        rafd_iter = iter(self.rafd_loader)\n                        x_real, label_org = next(rafd_iter)\n\n                # Generate target domain labels randomly.\n                rand_idx = torch.randperm(label_org.size(0))\n                label_trg = label_org[rand_idx]\n\n                if dataset == 'CelebA':\n                    c_org = label_org.clone()\n                    c_trg = label_trg.clone()\n                    zero = torch.zeros(x_real.size(0), self.c2_dim)\n                    mask = self.label2onehot(torch.zeros(x_real.size(0)), 2)\n                    c_org = torch.cat([c_org, zero, mask], dim=1)\n                    c_trg = torch.cat([c_trg, zero, mask], dim=1)\n                elif dataset == 'RaFD':\n                    c_org = self.label2onehot(label_org, self.c2_dim)\n                    c_trg = self.label2onehot(label_trg, self.c2_dim)\n                    zero = torch.zeros(x_real.size(0), self.c_dim)\n                    mask = self.label2onehot(torch.ones(x_real.size(0)), 2)\n                    c_org = torch.cat([zero, c_org, mask], dim=1)\n                    c_trg = torch.cat([zero, c_trg, mask], dim=1)\n\n                x_real = x_real.to(self.device)             # Input images.\n                c_org = c_org.to(self.device)               # Original domain labels.\n                c_trg = c_trg.to(self.device)               # Target domain labels.\n                label_org = label_org.to(self.device)       # Labels for computing classification loss.\n                label_trg = label_trg.to(self.device)       # Labels for computing classification loss.\n\n                # =================================================================================== #\n                #                             2. Train the discriminator                              #\n                # =================================================================================== #\n\n                # Compute loss with real images.\n                out_src, out_cls = self.D(x_real)\n                out_cls = out_cls[:, :self.c_dim] if dataset == 'CelebA' else out_cls[:, self.c_dim:]\n                d_loss_real = - torch.mean(out_src)\n                d_loss_cls = self.classification_loss(out_cls, label_org, dataset)\n\n                # Compute loss with fake images.\n                x_fake = self.G(x_real, c_trg)\n                out_src, _ = self.D(x_fake.detach())\n                d_loss_fake = torch.mean(out_src)\n\n                # Compute loss for gradient penalty.\n                alpha = torch.rand(x_real.size(0), 1, 1, 1).to(self.device)\n                x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True)\n                out_src, _ = self.D(x_hat)\n                d_loss_gp = self.gradient_penalty(out_src, x_hat)\n\n                # Backward and optimize.\n                d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gp\n                self.reset_grad()\n                d_loss.backward()\n                self.d_optimizer.step()\n\n                # Logging.\n                loss = {}\n                loss['D/loss_real'] = d_loss_real.item()\n                loss['D/loss_fake'] = d_loss_fake.item()\n                loss['D/loss_cls'] = d_loss_cls.item()\n                loss['D/loss_gp'] = d_loss_gp.item()\n            \n                # =================================================================================== #\n                #                               3. Train the generator                                #\n                # =================================================================================== #\n\n                if (i+1) % self.n_critic == 0:\n                    # Original-to-target domain.\n                    x_fake = self.G(x_real, c_trg)\n                    out_src, out_cls = self.D(x_fake)\n                    out_cls = out_cls[:, :self.c_dim] if dataset == 'CelebA' else out_cls[:, self.c_dim:]\n                    g_loss_fake = - torch.mean(out_src)\n                    g_loss_cls = self.classification_loss(out_cls, label_trg, dataset)\n\n                    # Target-to-original domain.\n                    x_reconst = self.G(x_fake, c_org)\n                    g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))\n\n                    # Backward and optimize.\n                    g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls\n                    self.reset_grad()\n                    g_loss.backward()\n                    self.g_optimizer.step()\n\n                    # Logging.\n                    loss['G/loss_fake'] = g_loss_fake.item()\n                    loss['G/loss_rec'] = g_loss_rec.item()\n                    loss['G/loss_cls'] = g_loss_cls.item()\n\n                # =================================================================================== #\n                #                                 4. Miscellaneous                                    #\n                # =================================================================================== #\n\n                # Print out training info.\n                if (i+1) % self.log_step == 0:\n                    et = time.time() - start_time\n                    et = str(datetime.timedelta(seconds=et))[:-7]\n                    log = \"Elapsed [{}], Iteration [{}/{}], Dataset [{}]\".format(et, i+1, self.num_iters, dataset)\n                    for tag, value in loss.items():\n                        log += \", {}: {:.4f}\".format(tag, value)\n                    print(log)\n\n                    if self.use_tensorboard:\n                        for tag, value in loss.items():\n                            self.logger.scalar_summary(tag, value, i+1)\n\n            # Translate fixed images for debugging.\n            if (i+1) % self.sample_step == 0:\n                with torch.no_grad():\n                    x_fake_list = [x_fixed]\n                    for c_fixed in c_celeba_list:\n                        c_trg = torch.cat([c_fixed, zero_rafd, mask_celeba], dim=1)\n                        x_fake_list.append(self.G(x_fixed, c_trg))\n                    for c_fixed in c_rafd_list:\n                        c_trg = torch.cat([zero_celeba, c_fixed, mask_rafd], dim=1)\n                        x_fake_list.append(self.G(x_fixed, c_trg))\n                    x_concat = torch.cat(x_fake_list, dim=3)\n                    sample_path = os.path.join(self.sample_dir, '{}-images.jpg'.format(i+1))\n                    save_image(self.denorm(x_concat.data.cpu()), sample_path, nrow=1, padding=0)\n                    print('Saved real and fake images into {}...'.format(sample_path))\n\n            # Save model checkpoints.\n            if (i+1) % self.model_save_step == 0:\n                G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(i+1))\n                D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(i+1))\n                torch.save(self.G.state_dict(), G_path)\n                torch.save(self.D.state_dict(), D_path)\n                print('Saved model checkpoints into {}...'.format(self.model_save_dir))\n\n            # Decay learning rates.\n            if (i+1) % self.lr_update_step == 0 and (i+1) > (self.num_iters - self.num_iters_decay):\n                g_lr -= (self.g_lr / float(self.num_iters_decay))\n                d_lr -= (self.d_lr / float(self.num_iters_decay))\n                self.update_lr(g_lr, d_lr)\n                print ('Decayed learning rates, g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))\n\n    def test(self):\n        \"\"\"Translate images using StarGAN trained on a single dataset.\"\"\"\n        # Load the trained generator.\n        self.restore_model(self.test_iters)\n        \n        # Set data loader.\n        if self.dataset == 'CelebA':\n            data_loader = self.celeba_loader\n        elif self.dataset == 'RaFD':\n            data_loader = self.rafd_loader\n        \n        with torch.no_grad():\n            for i, (x_real, c_org) in enumerate(data_loader):\n\n                # Prepare input images and target domain labels.\n                x_real = x_real.to(self.device)\n                c_trg_list = self.create_labels(c_org, self.c_dim, self.dataset, self.selected_attrs)\n\n                # Translate images.\n                x_fake_list = [x_real]\n                for c_trg in c_trg_list:\n                    x_fake_list.append(self.G(x_real, c_trg))\n\n                # Save the translated images.\n                x_concat = torch.cat(x_fake_list, dim=3)\n                result_path = os.path.join(self.result_dir, '{}-images.jpg'.format(i+1))\n                save_image(self.denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0)\n                print('Saved real and fake images into {}...'.format(result_path))\n\n    def test_multi(self):\n        \"\"\"Translate images using StarGAN trained on multiple datasets.\"\"\"\n        # Load the trained generator.\n        self.restore_model(self.test_iters)\n        \n        with torch.no_grad():\n            for i, (x_real, c_org) in enumerate(self.celeba_loader):\n\n                # Prepare input images and target domain labels.\n                x_real = x_real.to(self.device)\n                c_celeba_list = self.create_labels(c_org, self.c_dim, 'CelebA', self.selected_attrs)\n                c_rafd_list = self.create_labels(c_org, self.c2_dim, 'RaFD')\n                zero_celeba = torch.zeros(x_real.size(0), self.c_dim).to(self.device)            # Zero vector for CelebA.\n                zero_rafd = torch.zeros(x_real.size(0), self.c2_dim).to(self.device)             # Zero vector for RaFD.\n                mask_celeba = self.label2onehot(torch.zeros(x_real.size(0)), 2).to(self.device)  # Mask vector: [1, 0].\n                mask_rafd = self.label2onehot(torch.ones(x_real.size(0)), 2).to(self.device)     # Mask vector: [0, 1].\n\n                # Translate images.\n                x_fake_list = [x_real]\n                for c_celeba in c_celeba_list:\n                    c_trg = torch.cat([c_celeba, zero_rafd, mask_celeba], dim=1)\n                    x_fake_list.append(self.G(x_real, c_trg))\n                for c_rafd in c_rafd_list:\n                    c_trg = torch.cat([zero_celeba, c_rafd, mask_rafd], dim=1)\n                    x_fake_list.append(self.G(x_real, c_trg))\n\n                # Save the translated images.\n                x_concat = torch.cat(x_fake_list, dim=3)\n                result_path = os.path.join(self.result_dir, '{}-images.jpg'.format(i+1))\n                save_image(self.denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0)\n                print('Saved real and fake images into {}...'.format(result_path))"
        }
      ]
    }
  ]
}