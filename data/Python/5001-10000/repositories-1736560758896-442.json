{
  "metadata": {
    "timestamp": 1736560758896,
    "page": 442,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVIDIA/pix2pixHD",
      "stars": 6699,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.7080078125,
          "content": "debug*\ncheckpoints/\nresults/\nbuild/\ndist/\ntorch.egg-info/\n*/**/__pycache__\ntorch/version.py\ntorch/csrc/generic/TensorMethods.cpp\ntorch/lib/*.so*\ntorch/lib/*.dylib*\ntorch/lib/*.h\ntorch/lib/build\ntorch/lib/tmp_install\ntorch/lib/include\ntorch/lib/torch_shm_manager\ntorch/csrc/cudnn/cuDNN.cpp\ntorch/csrc/nn/THNN.cwrap\ntorch/csrc/nn/THNN.cpp\ntorch/csrc/nn/THCUNN.cwrap\ntorch/csrc/nn/THCUNN.cpp\ntorch/csrc/nn/THNN_generic.cwrap\ntorch/csrc/nn/THNN_generic.cpp\ntorch/csrc/nn/THNN_generic.h\ndocs/src/**/*\ntest/data/legacy_modules.t7\ntest/data/gpu_tensors.pt\ntest/htmlcov\ntest/.coverage\n*/*.pyc\n*/**/*.pyc\n*/**/**/*.pyc\n*/**/**/**/*.pyc\n*/**/**/**/**/*.pyc\n*/*.so*\n*/**/*.so*\n*/**/*.dylib*\ntest/data/legacy_serialized.pt\n*.DS_Store\n*~\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 2.4150390625,
          "content": "Copyright (C) 2019 NVIDIA Corporation. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu.\nBSD License. All rights reserved. \n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING ALL \nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE. \nIN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL \nDAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, \nWHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING \nOUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\n--------------------------- LICENSE FOR pytorch-CycleGAN-and-pix2pix ----------------\nCopyright (c) 2017, Jun-Yan Zhu and Taesung Park\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.3505859375,
          "content": "<img src='imgs/teaser_720.gif' align=\"right\" width=360>\r\n\r\n<br><br><br><br>\r\n\r\n# pix2pixHD\r\n### [Project](https://tcwang0509.github.io/pix2pixHD/) | [Youtube](https://youtu.be/3AIpPlzM_qs) | [Paper](https://arxiv.org/pdf/1711.11585.pdf) <br>\r\nPytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic image-to-image translation. It can be used for turning semantic label maps into photo-realistic images or synthesizing portraits from face label maps. <br><br>\r\n[High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs](https://tcwang0509.github.io/pix2pixHD/)  \r\n [Ting-Chun Wang](https://tcwang0509.github.io/)<sup>1</sup>, [Ming-Yu Liu](http://mingyuliu.net/)<sup>1</sup>, [Jun-Yan Zhu](http://people.eecs.berkeley.edu/~junyanz/)<sup>2</sup>, Andrew Tao<sup>1</sup>, [Jan Kautz](http://jankautz.com/)<sup>1</sup>, [Bryan Catanzaro](http://catanzaro.name/)<sup>1</sup>  \r\n <sup>1</sup>NVIDIA Corporation, <sup>2</sup>UC Berkeley  \r\n In CVPR 2018.  \r\n\r\n## Image-to-image translation at 2k/1k resolution\r\n- Our label-to-streetview results\r\n<p align='center'>  \r\n  <img src='imgs/teaser_label.png' width='400'/>\r\n  <img src='imgs/teaser_ours.jpg' width='400'/>\r\n</p>\r\n- Interactive editing results\r\n<p align='center'>  \r\n  <img src='imgs/teaser_style.gif' width='400'/>\r\n  <img src='imgs/teaser_label.gif' width='400'/>\r\n</p>\r\n- Additional streetview results\r\n<p align='center'>\r\n  <img src='imgs/cityscapes_1.jpg' width='400'/>\r\n  <img src='imgs/cityscapes_2.jpg' width='400'/>\r\n</p>\r\n<p align='center'>\r\n  <img src='imgs/cityscapes_3.jpg' width='400'/>\r\n  <img src='imgs/cityscapes_4.jpg' width='400'/>\r\n</p>\r\n\r\n- Label-to-face and interactive editing results\r\n<p align='center'>\r\n  <img src='imgs/face1_1.jpg' width='250'/>\r\n  <img src='imgs/face1_2.jpg' width='250'/>\r\n  <img src='imgs/face1_3.jpg' width='250'/>\r\n</p>\r\n<p align='center'>\r\n  <img src='imgs/face2_1.jpg' width='250'/>\r\n  <img src='imgs/face2_2.jpg' width='250'/>\r\n  <img src='imgs/face2_3.jpg' width='250'/>\r\n</p>\r\n\r\n- Our editing interface\r\n<p align='center'>\r\n  <img src='imgs/city_short.gif' width='330'/>\r\n  <img src='imgs/face_short.gif' width='450'/>\r\n</p>\r\n\r\n## Prerequisites\r\n- Linux or macOS\r\n- Python 2 or 3\r\n- NVIDIA GPU (11G memory or larger) + CUDA cuDNN\r\n\r\n## Getting Started\r\n### Installation\r\n- Install PyTorch and dependencies from http://pytorch.org\r\n- Install python libraries [dominate](https://github.com/Knio/dominate).\r\n```bash\r\npip install dominate\r\n```\r\n- Clone this repo:\r\n```bash\r\ngit clone https://github.com/NVIDIA/pix2pixHD\r\ncd pix2pixHD\r\n```\r\n\r\n\r\n### Testing\r\n- A few example Cityscapes test images are included in the `datasets` folder.\r\n- Please download the pre-trained Cityscapes model from [here](https://drive.google.com/file/d/1OR-2aEPHOxZKuoOV34DvQxreqGCSLcW9/view?usp=drive_link) (google drive link), and put it under `./checkpoints/label2city_1024p/`\r\n- Test the model (`bash ./scripts/test_1024p.sh`):\r\n```bash\r\n#!./scripts/test_1024p.sh\r\npython test.py --name label2city_1024p --netG local --ngf 32 --resize_or_crop none\r\n```\r\nThe test results will be saved to a html file here: `./results/label2city_1024p/test_latest/index.html`.\r\n\r\nMore example scripts can be found in the `scripts` directory.\r\n\r\n\r\n### Dataset\r\n- We use the Cityscapes dataset. To train a model on the full dataset, please download it from the [official website](https://www.cityscapes-dataset.com/) (registration required).\r\nAfter downloading, please put it under the `datasets` folder in the same way the example images are provided.\r\n\r\n\r\n### Training\r\n- Train a model at 1024 x 512 resolution (`bash ./scripts/train_512p.sh`):\r\n```bash\r\n#!./scripts/train_512p.sh\r\npython train.py --name label2city_512p\r\n```\r\n- To view training results, please checkout intermediate results in `./checkpoints/label2city_512p/web/index.html`.\r\nIf you have tensorflow installed, you can see tensorboard logs in `./checkpoints/label2city_512p/logs` by adding `--tf_log` to the training scripts.\r\n\r\n### Multi-GPU training\r\n- Train a model using multiple GPUs (`bash ./scripts/train_512p_multigpu.sh`):\r\n```bash\r\n#!./scripts/train_512p_multigpu.sh\r\npython train.py --name label2city_512p --batchSize 8 --gpu_ids 0,1,2,3,4,5,6,7\r\n```\r\nNote: this is not tested and we trained our model using single GPU only. Please use at your own discretion.\r\n\r\n### Training with Automatic Mixed Precision (AMP) for faster speed\r\n- To train with mixed precision support, please first install apex from: https://github.com/NVIDIA/apex\r\n- You can then train the model by adding `--fp16`. For example,\r\n```bash\r\n#!./scripts/train_512p_fp16.sh\r\npython -m torch.distributed.launch train.py --name label2city_512p --fp16\r\n```\r\nIn our test case, it trains about 80% faster with AMP on a Volta machine.\r\n\r\n### Training at full resolution\r\n- To train the images at full resolution (2048 x 1024) requires a GPU with 24G memory (`bash ./scripts/train_1024p_24G.sh`), or 16G memory if using mixed precision (AMP).\r\n- If only GPUs with 12G memory are available, please use the 12G script (`bash ./scripts/train_1024p_12G.sh`), which will crop the images during training. Performance is not guaranteed using this script.\r\n\r\n### Training with your own dataset\r\n- If you want to train with your own dataset, please generate label maps which are one-channel whose pixel values correspond to the object labels (i.e. 0,1,...,N-1, where N is the number of labels). This is because we need to generate one-hot vectors from the label maps. Please also specity `--label_nc N` during both training and testing.\r\n- If your input is not a label map, please just specify `--label_nc 0` which will directly use the RGB colors as input. The folders should then be named `train_A`, `train_B` instead of `train_label`, `train_img`, where the goal is to translate images from A to B.\r\n- If you don't have instance maps or don't want to use them, please specify `--no_instance`.\r\n- The default setting for preprocessing is `scale_width`, which will scale the width of all training images to `opt.loadSize` (1024) while keeping the aspect ratio. If you want a different setting, please change it by using the `--resize_or_crop` option. For example, `scale_width_and_crop` first resizes the image to have width `opt.loadSize` and then does random cropping of size `(opt.fineSize, opt.fineSize)`. `crop` skips the resizing step and only performs random cropping. If you don't want any preprocessing, please specify `none`, which will do nothing other than making sure the image is divisible by 32.\r\n\r\n## More Training/Test Details\r\n- Flags: see `options/train_options.py` and `options/base_options.py` for all the training flags; see `options/test_options.py` and `options/base_options.py` for all the test flags.\r\n- Instance map: we take in both label maps and instance maps as input. If you don't want to use instance maps, please specify the flag `--no_instance`.\r\n\r\n\r\n## Citation\r\n\r\nIf you find this useful for your research, please use the following.\r\n\r\n```\r\n@inproceedings{wang2018pix2pixHD,\r\n  title={High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs},\r\n  author={Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu and Andrew Tao and Jan Kautz and Bryan Catanzaro},  \r\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\r\n  year={2018}\r\n}\r\n```\r\n\r\n## Acknowledgments\r\nThis code borrows heavily from [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix).\r\n"
        },
        {
          "name": "_config.yml",
          "type": "blob",
          "size": 0.0263671875,
          "content": "theme: jekyll-theme-minimal"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "encode_features.py",
          "type": "blob",
          "size": 1.734375,
          "content": "from options.train_options import TrainOptions\r\nfrom data.data_loader import CreateDataLoader\r\nfrom models.models import create_model\r\nimport numpy as np\r\nimport os\r\n\r\nopt = TrainOptions().parse()\r\nopt.nThreads = 1\r\nopt.batchSize = 1 \r\nopt.serial_batches = True \r\nopt.no_flip = True\r\nopt.instance_feat = True\r\nopt.continue_train = True\r\n\r\nname = 'features'\r\nsave_path = os.path.join(opt.checkpoints_dir, opt.name)\r\n\r\n############ Initialize #########\r\ndata_loader = CreateDataLoader(opt)\r\ndataset = data_loader.load_data()\r\ndataset_size = len(data_loader)\r\nmodel = create_model(opt)\r\n\r\n########### Encode features ###########\r\nreencode = True\r\nif reencode:\r\n\tfeatures = {}\r\n\tfor label in range(opt.label_nc):\r\n\t\tfeatures[label] = np.zeros((0, opt.feat_num+1))\r\n\tfor i, data in enumerate(dataset):    \r\n\t    feat = model.module.encode_features(data['image'], data['inst'])\r\n\t    for label in range(opt.label_nc):\r\n\t    \tfeatures[label] = np.append(features[label], feat[label], axis=0) \r\n\t        \r\n\t    print('%d / %d images' % (i+1, dataset_size))    \r\n\tsave_name = os.path.join(save_path, name + '.npy')\r\n\tnp.save(save_name, features)\r\n\r\n############## Clustering ###########\r\nn_clusters = opt.n_clusters\r\nload_name = os.path.join(save_path, name + '.npy')\r\nfeatures = np.load(load_name).item()\r\nfrom sklearn.cluster import KMeans\r\ncenters = {}\r\nfor label in range(opt.label_nc):\r\n\tfeat = features[label]\r\n\tfeat = feat[feat[:,-1] > 0.5, :-1]\t\t\r\n\tif feat.shape[0]:\r\n\t\tn_clusters = min(feat.shape[0], opt.n_clusters)\r\n\t\tkmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(feat)\r\n\t\tcenters[label] = kmeans.cluster_centers_\r\nsave_name = os.path.join(save_path, name + '_clustered_%03d.npy' % opt.n_clusters)\r\nnp.save(save_name, centers)\r\nprint('saving to %s' % save_name)"
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "options",
          "type": "tree",
          "content": null
        },
        {
          "name": "precompute_feature_maps.py",
          "type": "blob",
          "size": 1.1455078125,
          "content": "from options.train_options import TrainOptions\r\nfrom data.data_loader import CreateDataLoader\r\nfrom models.models import create_model\r\nimport os\r\nimport util.util as util\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\n\r\nopt = TrainOptions().parse()\r\nopt.nThreads = 1\r\nopt.batchSize = 1 \r\nopt.serial_batches = True \r\nopt.no_flip = True\r\nopt.instance_feat = True\r\n\r\nname = 'features'\r\nsave_path = os.path.join(opt.checkpoints_dir, opt.name)\r\n\r\n############ Initialize #########\r\ndata_loader = CreateDataLoader(opt)\r\ndataset = data_loader.load_data()\r\ndataset_size = len(data_loader)\r\nmodel = create_model(opt)\r\nutil.mkdirs(os.path.join(opt.dataroot, opt.phase + '_feat'))\r\n\r\n######## Save precomputed feature maps for 1024p training #######\r\nfor i, data in enumerate(dataset):\r\n\tprint('%d / %d images' % (i+1, dataset_size)) \r\n\tfeat_map = model.module.netE.forward(Variable(data['image'].cuda(), volatile=True), data['inst'].cuda())\r\n\tfeat_map = nn.Upsample(scale_factor=2, mode='nearest')(feat_map)\r\n\timage_numpy = util.tensor2im(feat_map.data[0])\r\n\tsave_path = data['path'][0].replace('/train_label/', '/train_feat/')\r\n\tutil.save_image(image_numpy, save_path)"
        },
        {
          "name": "run_engine.py",
          "type": "blob",
          "size": 5.712890625,
          "content": "import os\nimport sys\nfrom random import randint\nimport numpy as np\nimport tensorrt\n\ntry:\n    from PIL import Image\n    import pycuda.driver as cuda\n    import pycuda.gpuarray as gpuarray\n    import pycuda.autoinit\n    import argparse\nexcept ImportError as err:\n    sys.stderr.write(\"\"\"ERROR: failed to import module ({})\nPlease make sure you have pycuda and the example dependencies installed.\nhttps://wiki.tiker.net/PyCuda/Installation/Linux\npip(3) install tensorrt[examples]\n\"\"\".format(err))\n    exit(1)\n\ntry:\n    import tensorrt as trt\n    from tensorrt.parsers import caffeparser\n    from tensorrt.parsers import onnxparser    \nexcept ImportError as err:\n    sys.stderr.write(\"\"\"ERROR: failed to import module ({})\nPlease make sure you have the TensorRT Library installed\nand accessible in your LD_LIBRARY_PATH\n\"\"\".format(err))\n    exit(1)\n\n\nG_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.INFO)\n\nclass Profiler(trt.infer.Profiler):\n    \"\"\"\n    Example Implimentation of a Profiler\n    Is identical to the Profiler class in trt.infer so it is possible\n    to just use that instead of implementing this if further\n    functionality is not needed\n    \"\"\"\n    def __init__(self, timing_iter):\n        trt.infer.Profiler.__init__(self)\n        self.timing_iterations = timing_iter\n        self.profile = []\n\n    def report_layer_time(self, layerName, ms):\n        record = next((r for r in self.profile if r[0] == layerName), (None, None))\n        if record == (None, None):\n            self.profile.append((layerName, ms))\n        else:\n            self.profile[self.profile.index(record)] = (record[0], record[1] + ms)\n\n    def print_layer_times(self):\n        totalTime = 0\n        for i in range(len(self.profile)):\n            print(\"{:40.40} {:4.3f}ms\".format(self.profile[i][0], self.profile[i][1] / self.timing_iterations))\n            totalTime += self.profile[i][1]\n        print(\"Time over all layers: {:4.2f} ms per iteration\".format(totalTime / self.timing_iterations))\n\n\ndef get_input_output_names(trt_engine):\n    nbindings = trt_engine.get_nb_bindings();\n    maps = []\n\n    for b in range(0, nbindings):\n        dims = trt_engine.get_binding_dimensions(b).to_DimsCHW()\n        name = trt_engine.get_binding_name(b)\n        type = trt_engine.get_binding_data_type(b)\n        \n        if (trt_engine.binding_is_input(b)):\n            maps.append(name)\n            print(\"Found input: \", name)\n        else:\n            maps.append(name)\n            print(\"Found output: \", name)\n\n        print(\"shape=\" + str(dims.C()) + \" , \" + str(dims.H()) + \" , \" + str(dims.W()))\n        print(\"dtype=\" + str(type))\n    return maps\n\ndef create_memory(engine, name,  buf, mem, batchsize, inp, inp_idx):\n    binding_idx = engine.get_binding_index(name)\n    if binding_idx == -1:\n        raise AttributeError(\"Not a valid binding\")\n    print(\"Binding: name={}, bindingIndex={}\".format(name, str(binding_idx)))\n    dims = engine.get_binding_dimensions(binding_idx).to_DimsCHW()\n    eltCount = dims.C() * dims.H() * dims.W() * batchsize\n\n    if engine.binding_is_input(binding_idx):\n        h_mem = inp[inp_idx]\n        inp_idx = inp_idx + 1\n    else:\n        h_mem = np.random.uniform(0.0, 255.0, eltCount).astype(np.dtype('f4'))\n\n    d_mem = cuda.mem_alloc(eltCount * 4)\n    cuda.memcpy_htod(d_mem, h_mem)\n    buf.insert(binding_idx, int(d_mem))\n    mem.append(d_mem)\n    return inp_idx\n\n\n#Run inference on device\ndef time_inference(engine, batch_size, inp):\n    bindings = []\n    mem = []\n    inp_idx = 0\n    for io in get_input_output_names(engine):\n        inp_idx = create_memory(engine, io,  bindings, mem,\n                                batch_size, inp, inp_idx)\n\n    context = engine.create_execution_context()\n    g_prof = Profiler(500)\n    context.set_profiler(g_prof)\n    for i in range(iter):\n        context.execute(batch_size, bindings)\n    g_prof.print_layer_times()\n    \n    context.destroy() \n    return\n\n\ndef convert_to_datatype(v):\n    if v==8:\n        return trt.infer.DataType.INT8\n    elif v==16:\n        return trt.infer.DataType.HALF\n    elif v==32:\n        return trt.infer.DataType.FLOAT\n    else:\n        print(\"ERROR: Invalid model data type bit depth: \" + str(v))\n        return trt.infer.DataType.INT8\n\ndef run_trt_engine(engine_file, bs, it):\n    engine = trt.utils.load_engine(G_LOGGER, engine_file)\n    time_inference(engine, bs, it)\n\ndef run_onnx(onnx_file, data_type, bs, inp):\n    # Create onnx_config\n    apex = onnxparser.create_onnxconfig()\n    apex.set_model_file_name(onnx_file)\n    apex.set_model_dtype(convert_to_datatype(data_type))\n\n     # create parser\n    trt_parser = onnxparser.create_onnxparser(apex)\n    assert(trt_parser)\n    data_type = apex.get_model_dtype()\n    onnx_filename = apex.get_model_file_name()\n    trt_parser.parse(onnx_filename, data_type)\n    trt_parser.report_parsing_info()\n    trt_parser.convert_to_trtnetwork()\n    trt_network = trt_parser.get_trtnetwork()\n    assert(trt_network)\n\n    # create infer builder\n    trt_builder = trt.infer.create_infer_builder(G_LOGGER)\n    trt_builder.set_max_batch_size(max_batch_size)\n    trt_builder.set_max_workspace_size(max_workspace_size)\n    \n    if (apex.get_model_dtype() == trt.infer.DataType_kHALF):\n        print(\"-------------------  Running FP16 -----------------------------\")\n        trt_builder.set_half2_mode(True)\n    elif (apex.get_model_dtype() == trt.infer.DataType_kINT8): \n        print(\"-------------------  Running INT8 -----------------------------\")\n        trt_builder.set_int8_mode(True)\n    else:\n        print(\"-------------------  Running FP32 -----------------------------\")\n        \n    print(\"----- Builder is Done -----\")\n    print(\"----- Creating Engine -----\")\n    trt_engine = trt_builder.build_cuda_engine(trt_network)\n    print(\"----- Engine is built -----\")\n    time_inference(engine, bs, inp)\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 2.392578125,
          "content": "import os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nfrom options.test_options import TestOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nimport util.util as util\nfrom util.visualizer import Visualizer\nfrom util import html\nimport torch\n\nopt = TestOptions().parse(save=False)\nopt.nThreads = 1   # test code only supports nThreads = 1\nopt.batchSize = 1  # test code only supports batchSize = 1\nopt.serial_batches = True  # no shuffle\nopt.no_flip = True  # no flip\n\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\nvisualizer = Visualizer(opt)\n# create website\nweb_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.which_epoch))\nwebpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.which_epoch))\n\n# test\nif not opt.engine and not opt.onnx:\n    model = create_model(opt)\n    if opt.data_type == 16:\n        model.half()\n    elif opt.data_type == 8:\n        model.type(torch.uint8)\n            \n    if opt.verbose:\n        print(model)\nelse:\n    from run_engine import run_trt_engine, run_onnx\n    \nfor i, data in enumerate(dataset):\n    if i >= opt.how_many:\n        break\n    if opt.data_type == 16:\n        data['label'] = data['label'].half()\n        data['inst']  = data['inst'].half()\n    elif opt.data_type == 8:\n        data['label'] = data['label'].uint8()\n        data['inst']  = data['inst'].uint8()\n    if opt.export_onnx:\n        print (\"Exporting to ONNX: \", opt.export_onnx)\n        assert opt.export_onnx.endswith(\"onnx\"), \"Export model file should end with .onnx\"\n        torch.onnx.export(model, [data['label'], data['inst']],\n                          opt.export_onnx, verbose=True)\n        exit(0)\n    minibatch = 1 \n    if opt.engine:\n        generated = run_trt_engine(opt.engine, minibatch, [data['label'], data['inst']])\n    elif opt.onnx:\n        generated = run_onnx(opt.onnx, opt.data_type, minibatch, [data['label'], data['inst']])\n    else:        \n        generated = model.inference(data['label'], data['inst'], data['image'])\n        \n    visuals = OrderedDict([('input_label', util.tensor2label(data['label'][0], opt.label_nc)),\n                           ('synthesized_image', util.tensor2im(generated.data[0]))])\n    img_path = data['path']\n    print('process image... %s' % img_path)\n    visualizer.save_images(webpage, visuals, img_path)\n\nwebpage.save()\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 5.63671875,
          "content": "import time\nimport os\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom subprocess import call\nimport fractions\ndef lcm(a,b): return abs(a * b)/fractions.gcd(a,b) if a and b else 0\n\nfrom options.train_options import TrainOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nimport util.util as util\nfrom util.visualizer import Visualizer\n\nopt = TrainOptions().parse()\niter_path = os.path.join(opt.checkpoints_dir, opt.name, 'iter.txt')\nif opt.continue_train:\n    try:\n        start_epoch, epoch_iter = np.loadtxt(iter_path , delimiter=',', dtype=int)\n    except:\n        start_epoch, epoch_iter = 1, 0\n    print('Resuming from epoch %d at iteration %d' % (start_epoch, epoch_iter))        \nelse:    \n    start_epoch, epoch_iter = 1, 0\n\nopt.print_freq = lcm(opt.print_freq, opt.batchSize)    \nif opt.debug:\n    opt.display_freq = 1\n    opt.print_freq = 1\n    opt.niter = 1\n    opt.niter_decay = 0\n    opt.max_dataset_size = 10\n\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\ndataset_size = len(data_loader)\nprint('#training images = %d' % dataset_size)\n\nmodel = create_model(opt)\nvisualizer = Visualizer(opt)\nif opt.fp16:    \n    from apex import amp\n    model, [optimizer_G, optimizer_D] = amp.initialize(model, [model.optimizer_G, model.optimizer_D], opt_level='O1')             \n    model = torch.nn.DataParallel(model, device_ids=opt.gpu_ids)\nelse:\n    optimizer_G, optimizer_D = model.module.optimizer_G, model.module.optimizer_D\n\ntotal_steps = (start_epoch-1) * dataset_size + epoch_iter\n\ndisplay_delta = total_steps % opt.display_freq\nprint_delta = total_steps % opt.print_freq\nsave_delta = total_steps % opt.save_latest_freq\n\nfor epoch in range(start_epoch, opt.niter + opt.niter_decay + 1):\n    epoch_start_time = time.time()\n    if epoch != start_epoch:\n        epoch_iter = epoch_iter % dataset_size\n    for i, data in enumerate(dataset, start=epoch_iter):\n        if total_steps % opt.print_freq == print_delta:\n            iter_start_time = time.time()\n        total_steps += opt.batchSize\n        epoch_iter += opt.batchSize\n\n        # whether to collect output images\n        save_fake = total_steps % opt.display_freq == display_delta\n\n        ############## Forward Pass ######################\n        losses, generated = model(Variable(data['label']), Variable(data['inst']), \n            Variable(data['image']), Variable(data['feat']), infer=save_fake)\n\n        # sum per device losses\n        losses = [ torch.mean(x) if not isinstance(x, int) else x for x in losses ]\n        loss_dict = dict(zip(model.module.loss_names, losses))\n\n        # calculate final loss scalar\n        loss_D = (loss_dict['D_fake'] + loss_dict['D_real']) * 0.5\n        loss_G = loss_dict['G_GAN'] + loss_dict.get('G_GAN_Feat',0) + loss_dict.get('G_VGG',0)\n\n        ############### Backward Pass ####################\n        # update generator weights\n        optimizer_G.zero_grad()\n        if opt.fp16:                                \n            with amp.scale_loss(loss_G, optimizer_G) as scaled_loss: scaled_loss.backward()                \n        else:\n            loss_G.backward()          \n        optimizer_G.step()\n\n        # update discriminator weights\n        optimizer_D.zero_grad()\n        if opt.fp16:                                \n            with amp.scale_loss(loss_D, optimizer_D) as scaled_loss: scaled_loss.backward()                \n        else:\n            loss_D.backward()        \n        optimizer_D.step()        \n\n        ############## Display results and errors ##########\n        ### print out errors\n        if total_steps % opt.print_freq == print_delta:\n            errors = {k: v.data.item() if not isinstance(v, int) else v for k, v in loss_dict.items()}            \n            t = (time.time() - iter_start_time) / opt.print_freq\n            visualizer.print_current_errors(epoch, epoch_iter, errors, t)\n            visualizer.plot_current_errors(errors, total_steps)\n            #call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=memory.used,memory.free\"]) \n\n        ### display output images\n        if save_fake:\n            visuals = OrderedDict([('input_label', util.tensor2label(data['label'][0], opt.label_nc)),\n                                   ('synthesized_image', util.tensor2im(generated.data[0])),\n                                   ('real_image', util.tensor2im(data['image'][0]))])\n            visualizer.display_current_results(visuals, epoch, total_steps)\n\n        ### save latest model\n        if total_steps % opt.save_latest_freq == save_delta:\n            print('saving the latest model (epoch %d, total_steps %d)' % (epoch, total_steps))\n            model.module.save('latest')            \n            np.savetxt(iter_path, (epoch, epoch_iter), delimiter=',', fmt='%d')\n\n        if epoch_iter >= dataset_size:\n            break\n       \n    # end of epoch \n    iter_end_time = time.time()\n    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n          (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n\n    ### save model for this epoch\n    if epoch % opt.save_epoch_freq == 0:\n        print('saving the model at the end of epoch %d, iters %d' % (epoch, total_steps))        \n        model.module.save('latest')\n        model.module.save(epoch)\n        np.savetxt(iter_path, (epoch+1, 0), delimiter=',', fmt='%d')\n\n    ### instead of only training the local enhancer, train the entire network after certain iterations\n    if (opt.niter_fix_global != 0) and (epoch == opt.niter_fix_global):\n        model.module.update_fixed_params()\n\n    ### linearly decay learning rate after certain iterations\n    if epoch > opt.niter:\n        model.module.update_learning_rate()\n"
        },
        {
          "name": "util",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}