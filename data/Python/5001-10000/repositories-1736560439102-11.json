{
  "metadata": {
    "timestamp": 1736560439102,
    "page": 11,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "qubvel-org/segmentation_models.pytorch",
      "stars": 9904,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".devcontainer",
          "type": "tree",
          "content": null
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.2548828125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n.idea/\n.venv*\nexamples/images*\nexamples/annotations*\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n.vscode/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# ruff\n.ruff_cache/"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.58984375,
          "content": "# Read the Docs configuration file for Sphinx projects\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Set the OS, Python version and other tools you might need\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.12\"\n\n# Build documentation in the \"docs/\" directory with Sphinx\nsphinx:\n  configuration: docs/conf.py\n\n# Optional but recommended, declare the Python requirements required\n# to build your documentation\n# See https://docs.readthedocs.io/en/stable/guides/reproducible-builds.html\npython:\n  install:\n    - requirements: requirements/docs.txt\n"
        },
        {
          "name": "HALLOFFAME.md",
          "type": "blob",
          "size": 4.8193359375,
          "content": "# Hall of Fame\n\n`Segmentation Models` package is widely used in the image segmentation competitions.\nHere you can find competitions, names of the winners and links to their solutions.\n\nPlease, follow these rules, when adding a solution to the \"Hall of Fame\":\n\n1. Solution should be high rated (e.g. for Kaggle gold or silver medal) \n2. There should be a description of the solution (post at the forum / code / blog post / paper / pre-print)\n\n\n## Kaggle\n\n### [Severstal: Steel Defect Detection](https://www.kaggle.com/c/severstal-steel-defect-detection)\n\n- 1st place. \n[Wuxi Jiangsu](https://www.kaggle.com/rguo97), \n[Hongbo Zhu](https://www.kaggle.com/zhuhongbo), \n[Yizhuo Yu](https://www.kaggle.com/paffpaffyu)  \n[[description](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114254#latest-675874)]\n\n- 5th place. \n[Guanshuo Xu](https://www.kaggle.com/wowfattie)  \n[[description](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/117208#latest-675385)]\n\n- 9th place. \n[Jacek Poplawski](https://www.linkedin.com/in/jacekpoplawski/)  \n[[description](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114297#latest-660842)]\n\n- 10th place.\n[Alexey Rozhkov](https://www.linkedin.com/in/alexisrozhkov)  \n[[description](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114465#latest-659615)]\n\n- 12th place. \n[Pavel Iakubovskii](https://www.linkedin.com/in/pavel-iakubovskii/), \n[Ilya Dobrynin](https://www.linkedin.com/in/ilya-dobrynin-79a89b106/), \n[Denis Kolpakov](https://www.linkedin.com/in/denis-kolpakov-ab3137197/)  \n[[description](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114309#latest-661404)]\n\n- 31st place. \n[Insaf Ashrapov](https://www.linkedin.com/in/iashrapov/), \n[Igor Krashenyi](https://www.linkedin.com/in/igor-krashenyi-38b89b98), \n[Pavel Pleskov](https://www.linkedin.com/in/ppleskov), \n[Anton Zakharenkov](https://www.linkedin.com/in/anton-zakharenkov/), \n[Nikolai Popov](https://www.linkedin.com/in/nikolai-popov-b2157370/)  \n[[description](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114383#latest-658438)]\n[[code](https://github.com/Diyago/Severstal-Steel-Defect-Detection)]\n\n- 55th place. \n[Karl Hornlund](https://www.linkedin.com/in/karl-hornlund/)  \n[[description](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/114410#latest-672682)]\n[[code](https://github.com/khornlund/severstal-steel-defect-detection)]\n\n- Efficiency round 1st place.\n[Stefan Stefanov](https://www.linkedin.com/in/stefan-stefanov-63a77b1)  \n[[description](https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/117486#latest-674229)]\n\n\n### [Understanding Clouds from Satellite Images](https://www.kaggle.com/c/understanding_cloud_organization)\n\n- 2nd place.\n[Andrey Kiryasov](https://www.kaggle.com/ekydna)  \n[[description](https://www.kaggle.com/c/understanding_cloud_organization/discussion/118255#latest-678189)]\n\n- 4th place.\n[Ching-Loong Seow](https://www.linkedin.com/in/clseow/)  \n[[description](https://www.kaggle.com/c/understanding_cloud_organization/discussion/118016#latest-677333)]\n\n- 34th place.\n[Karl Hornlund](https://www.linkedin.com/in/karl-hornlund/)  \n[[description](https://www.kaggle.com/c/understanding_cloud_organization/discussion/118250#latest-678176)]\n[[code](https://github.com/khornlund/understanding-cloud-organization)]\n\n- 55th place.\n[Pavel Iakubovskii](https://www.linkedin.com/in/pavel-iakubovskii/)  \n[[description](https://www.kaggle.com/c/understanding_cloud_organization/discussion/118019#latest-678626)]\n\n## Other platforms\n\n### [MICCAI 2020 TN-SCUI challenge](https://tn-scui2020.grand-challenge.org/Home/)  \n- 1st place.\n[Mingyu Wang](https://github.com/WAMAWAMA)  \n[[description](https://github.com/WAMAWAMA/TNSCUI2020-Seg-Rank1st)]\n[[code](https://github.com/WAMAWAMA/TNSCUI2020-Seg-Rank1st)]\n\n### [Open Cities AI Challenge: Segmenting Buildings for Disaster Resilience](https://www.drivendata.org/competitions/60/building-segmentation-disaster-resilience/)\n - 1st place.\n[Pavel Iakubovskii](https://www.linkedin.com/in/pavel-iakubovskii/).  \n[[code and description](https://github.com/qubvel/open-cities-challenge)]\n\n### [Machine Learning based feature extraction of Electrical Substations from Satellite Data ](https://competitions.codalab.org/competitions/32132#learn_the_details)\n\n- 3rd place.\n\n[Aarsh chaube](https://github.com/Aarsh2001)\n[[code](https://github.com/Aarsh2001/ML_Challenge_NRSC)]\n[[Pre-Print](https://github.com/Aarsh2001/ML_Challenge_NRSC/blob/main/3rd%20Rank%20Submission.pdf)]\n\n### [NeurIPS2022 Cell Segmentation Challenge](https://neurips22-cellseg.grand-challenge.org/)\n\n- 1st place. [Gihun Lee](https://github.com/Lee-Gihun), [Sangmook Kim](https://github.com/ElvinKim), [Joonkee Kim](https://github.com/joonkeekim)\n- [[code](https://github.com/Lee-Gihun/MEDIAR)]\n[[Paper](https://arxiv.org/abs/2212.03465)]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0537109375,
          "content": "The MIT License\n\nCopyright (c) 2019, Pavel Iakubovskii\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.4140625,
          "content": ".PHONY: test\n\n.venv:\n\tpython3 -m venv .venv\n\ninstall_dev: .venv\n\t.venv/bin/pip install -e \".[test]\"\n\ntest: .venv\n\t.venv/bin/pytest -v -rsx -n 2 tests/ -k \"not logits_match\"\n\ntest_all: .venv\n\tRUN_SLOW=1 .venv/bin/pytest -v -rsx -n 2 tests/\n\ntable:\n\t.venv/bin/python misc/generate_table.py\n\ntable_timm:\n\t.venv/bin/python misc/generate_table_timm.py\n\nfixup:\n\t.venv/bin/ruff check --fix\n\t.venv/bin/ruff format\n\nall: fixup test\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 29.40625,
          "content": "<div align=\"center\">\n \n![logo](https://i.ibb.co/dc1XdhT/Segmentation-Models-V2-Side-1-1.png)  \n**Python library with Neural Networks for Image  \nSegmentation based on [PyTorch](https://pytorch.org/).**  \n\n[![Generic badge](https://img.shields.io/badge/License-MIT-<COLOR>.svg?style=for-the-badge)](https://github.com/qubvel/segmentation_models.pytorch/blob/main/LICENSE) \n[![GitHub Workflow Status (branch)](https://img.shields.io/github/actions/workflow/status/qubvel/segmentation_models.pytorch/tests.yml?branch=main&style=for-the-badge)](https://github.com/qubvel/segmentation_models.pytorch/actions/workflows/tests.yml) \n[![Read the Docs](https://img.shields.io/readthedocs/smp?style=for-the-badge&logo=readthedocs&logoColor=white)](https://smp.readthedocs.io/en/latest/) \n<br>\n[![PyPI](https://img.shields.io/pypi/v/segmentation-models-pytorch?color=blue&style=for-the-badge&logo=pypi&logoColor=white)](https://pypi.org/project/segmentation-models-pytorch/) \n[![PyPI - Downloads](https://img.shields.io/pypi/dm/segmentation-models-pytorch?style=for-the-badge&color=blue)](https://pepy.tech/project/segmentation-models-pytorch) \n<br>\n[![PyTorch - Version](https://img.shields.io/badge/PYTORCH-1.4+-red?style=for-the-badge&logo=pytorch)](https://pepy.tech/project/segmentation-models-pytorch) \n[![Python - Version](https://img.shields.io/badge/PYTHON-3.9+-red?style=for-the-badge&logo=python&logoColor=white)](https://pepy.tech/project/segmentation-models-pytorch) \n\n</div>\n\nThe main features of this library are:\n\n - High-level API (just two lines to create a neural network)\n - 11 models architectures for binary and multi class segmentation (including legendary Unet)\n - 124 available encoders (and 500+ encoders from [timm](https://github.com/rwightman/pytorch-image-models))\n - All encoders have pre-trained weights for faster and better convergence\n - Popular metrics and losses for training routines\n \n### [üìö Project Documentation üìö](http://smp.readthedocs.io/)\n\nVisit [Read The Docs Project Page](https://smp.readthedocs.io/) or read the following README to know more about Segmentation Models Pytorch (SMP for short) library\n\n### üìã Table of content\n 1. [Quick start](#start)\n 2. [Examples](#examples)\n 3. [Models](#models)\n    1. [Architectures](#architectures)\n    2. [Encoders](#encoders)\n    3. [Timm Encoders](#timm)\n 4. [Models API](#api)\n    1. [Input channels](#input-channels)\n    2. [Auxiliary classification output](#auxiliary-classification-output)\n    3. [Depth](#depth)\n 5. [Installation](#installation)\n 6. [Competitions won with the library](#competitions-won-with-the-library)\n 7. [Contributing](#contributing)\n 8. [Citing](#citing)\n 9. [License](#license)\n\n### ‚è≥ Quick start <a name=\"start\"></a>\n\n#### 1. Create your first Segmentation model with SMP\n\nThe segmentation model is just a PyTorch `torch.nn.Module`, which can be created as easy as:\n\n```python\nimport segmentation_models_pytorch as smp\n\nmodel = smp.Unet(\n    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n    classes=3,                      # model output channels (number of classes in your dataset)\n)\n```\n - see [table](#architectures) with available model architectures\n - see [table](#encoders) with available encoders and their corresponding weights\n\n#### 2. Configure data preprocessing\n\nAll encoders have pretrained weights. Preparing your data the same way as during weights pre-training may give you better results (higher metric score and faster convergence). It is **not necessary** in case you train the whole model, not only the decoder.\n\n```python\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn\n\npreprocess_input = get_preprocessing_fn('resnet18', pretrained='imagenet')\n```\n\nCongratulations! You are done! Now you can train your model with your favorite framework!\n\n### üí° Examples <a name=\"examples\"></a>\n - Training model for pets binary segmentation with Pytorch-Lightning [notebook](https://github.com/qubvel/segmentation_models.pytorch/blob/main/examples/binary_segmentation_intro.ipynb) and [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/qubvel/segmentation_models.pytorch/blob/main/examples/binary_segmentation_intro.ipynb)\n - Training model for cars segmentation on CamVid dataset [here](https://github.com/qubvel/segmentation_models.pytorch/blob/main/examples/cars%20segmentation%20(camvid).ipynb).\n - Training SMP model with [Catalyst](https://github.com/catalyst-team/catalyst) (high-level framework for PyTorch), [TTAch](https://github.com/qubvel/ttach) (TTA library for PyTorch) and [Albumentations](https://github.com/albu/albumentations) (fast image augmentation library) - [here](https://github.com/catalyst-team/catalyst/blob/v21.02rc0/examples/notebooks/segmentation-tutorial.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/v21.02rc0/examples/notebooks/segmentation-tutorial.ipynb)\n - Training SMP model with [Pytorch-Lightning](https://pytorch-lightning.readthedocs.io) framework - [here](https://github.com/ternaus/cloths_segmentation) (clothes binary segmentation by [@ternaus](https://github.com/ternaus)).\n - Export trained model to ONNX - [notebook](https://github.com/qubvel/segmentation_models.pytorch/blob/main/examples/convert_to_onnx.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/qubvel/segmentation_models.pytorch/blob/main/examples/convert_to_onnx.ipynb)\n\n### üì¶ Models <a name=\"models\"></a>\n\n#### Architectures <a name=\"architectures\"></a>\n - Unet [[paper](https://arxiv.org/abs/1505.04597)] [[docs](https://smp.readthedocs.io/en/latest/models.html#unet)]\n - Unet++ [[paper](https://arxiv.org/pdf/1807.10165.pdf)] [[docs](https://smp.readthedocs.io/en/latest/models.html#id2)]\n - MAnet [[paper](https://ieeexplore.ieee.org/abstract/document/9201310)] [[docs](https://smp.readthedocs.io/en/latest/models.html#manet)]\n - Linknet [[paper](https://arxiv.org/abs/1707.03718)] [[docs](https://smp.readthedocs.io/en/latest/models.html#linknet)]\n - FPN [[paper](http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf)] [[docs](https://smp.readthedocs.io/en/latest/models.html#fpn)]\n - PSPNet [[paper](https://arxiv.org/abs/1612.01105)] [[docs](https://smp.readthedocs.io/en/latest/models.html#pspnet)]\n - PAN [[paper](https://arxiv.org/abs/1805.10180)] [[docs](https://smp.readthedocs.io/en/latest/models.html#pan)]\n - DeepLabV3 [[paper](https://arxiv.org/abs/1706.05587)] [[docs](https://smp.readthedocs.io/en/latest/models.html#deeplabv3)]\n - DeepLabV3+ [[paper](https://arxiv.org/abs/1802.02611)] [[docs](https://smp.readthedocs.io/en/latest/models.html#id9)]\n - UPerNet [[paper](https://arxiv.org/abs/1807.10221)] [[docs](https://smp.readthedocs.io/en/latest/models.html#upernet)]\n - Segformer [[paper](https://arxiv.org/abs/2105.15203)] [[docs](https://smp.readthedocs.io/en/latest/models.html#segformer)]\n\n#### Encoders <a name=\"encoders\"></a>\n\nThe following is a list of supported encoders in the SMP. Select the appropriate family of encoders and click to expand the table and select a specific encoder and its pre-trained weights (`encoder_name` and `encoder_weights` parameters).\n\n<details>\n<summary style=\"margin-left: 25px;\">ResNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|resnet18                        |imagenet / ssl / swsl           |11M                             |\n|resnet34                        |imagenet                        |21M                             |\n|resnet50                        |imagenet / ssl / swsl           |23M                             |\n|resnet101                       |imagenet                        |42M                             |\n|resnet152                       |imagenet                        |58M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">ResNeXt</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|resnext50_32x4d                 |imagenet / ssl / swsl           |22M                             |\n|resnext101_32x4d                |ssl / swsl                      |42M                             |\n|resnext101_32x8d                |imagenet / instagram / ssl / swsl|86M                         |\n|resnext101_32x16d               |instagram / ssl / swsl          |191M                            |\n|resnext101_32x32d               |instagram                       |466M                            |\n|resnext101_32x48d               |instagram                       |826M                            |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">ResNeSt</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-resnest14d                 |imagenet                        |8M                              |\n|timm-resnest26d                 |imagenet                        |15M                             |\n|timm-resnest50d                 |imagenet                        |25M                             |\n|timm-resnest101e                |imagenet                        |46M                             |\n|timm-resnest200e                |imagenet                        |68M                             |\n|timm-resnest269e                |imagenet                        |108M                            |\n|timm-resnest50d_4s2x40d         |imagenet                        |28M                             |\n|timm-resnest50d_1s4x24d         |imagenet                        |23M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">Res2Ne(X)t</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-res2net50_26w_4s           |imagenet                        |23M                             |\n|timm-res2net101_26w_4s          |imagenet                        |43M                             |\n|timm-res2net50_26w_6s           |imagenet                        |35M                             |\n|timm-res2net50_26w_8s           |imagenet                        |46M                             |\n|timm-res2net50_48w_2s           |imagenet                        |23M                             |\n|timm-res2net50_14w_8s           |imagenet                        |23M                             |\n|timm-res2next50                 |imagenet                        |22M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">RegNet(x/y)</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-regnetx_002                |imagenet                        |2M                              |\n|timm-regnetx_004                |imagenet                        |4M                              |\n|timm-regnetx_006                |imagenet                        |5M                              |\n|timm-regnetx_008                |imagenet                        |6M                              |\n|timm-regnetx_016                |imagenet                        |8M                              |\n|timm-regnetx_032                |imagenet                        |14M                             |\n|timm-regnetx_040                |imagenet                        |20M                             |\n|timm-regnetx_064                |imagenet                        |24M                             |\n|timm-regnetx_080                |imagenet                        |37M                             |\n|timm-regnetx_120                |imagenet                        |43M                             |\n|timm-regnetx_160                |imagenet                        |52M                             |\n|timm-regnetx_320                |imagenet                        |105M                            |\n|timm-regnety_002                |imagenet                        |2M                              |\n|timm-regnety_004                |imagenet                        |3M                              |\n|timm-regnety_006                |imagenet                        |5M                              |\n|timm-regnety_008                |imagenet                        |5M                              |\n|timm-regnety_016                |imagenet                        |10M                             |\n|timm-regnety_032                |imagenet                        |17M                             |\n|timm-regnety_040                |imagenet                        |19M                             |\n|timm-regnety_064                |imagenet                        |29M                             |\n|timm-regnety_080                |imagenet                        |37M                             |\n|timm-regnety_120                |imagenet                        |49M                             |\n|timm-regnety_160                |imagenet                        |80M                             |\n|timm-regnety_320                |imagenet                        |141M                            |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">GERNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-gernet_s                   |imagenet                        |6M                              |\n|timm-gernet_m                   |imagenet                        |18M                             |\n|timm-gernet_l                   |imagenet                        |28M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">SE-Net</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|senet154                        |imagenet                        |113M                            |\n|se_resnet50                     |imagenet                        |26M                             |\n|se_resnet101                    |imagenet                        |47M                             |\n|se_resnet152                    |imagenet                        |64M                             |\n|se_resnext50_32x4d              |imagenet                        |25M                             |\n|se_resnext101_32x4d             |imagenet                        |46M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">SK-ResNe(X)t</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-skresnet18                 |imagenet                        |11M                             |\n|timm-skresnet34                 |imagenet                        |21M                             |\n|timm-skresnext50_32x4d          |imagenet                        |25M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">DenseNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|densenet121                     |imagenet                        |6M                              |\n|densenet169                     |imagenet                        |12M                             |\n|densenet201                     |imagenet                        |18M                             |\n|densenet161                     |imagenet                        |26M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">Inception</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|inceptionresnetv2               |imagenet /  imagenet+background |54M                             |\n|inceptionv4                     |imagenet /  imagenet+background |41M                             |\n|xception                        |imagenet                        |22M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">EfficientNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|efficientnet-b0                 |imagenet                        |4M                              |\n|efficientnet-b1                 |imagenet                        |6M                              |\n|efficientnet-b2                 |imagenet                        |7M                              |\n|efficientnet-b3                 |imagenet                        |10M                             |\n|efficientnet-b4                 |imagenet                        |17M                             |\n|efficientnet-b5                 |imagenet                        |28M                             |\n|efficientnet-b6                 |imagenet                        |40M                             |\n|efficientnet-b7                 |imagenet                        |63M                             |\n|timm-efficientnet-b0            |imagenet / advprop / noisy-student|4M                              |\n|timm-efficientnet-b1            |imagenet / advprop / noisy-student|6M                              |\n|timm-efficientnet-b2            |imagenet / advprop / noisy-student|7M                              |\n|timm-efficientnet-b3            |imagenet / advprop / noisy-student|10M                             |\n|timm-efficientnet-b4            |imagenet / advprop / noisy-student|17M                             |\n|timm-efficientnet-b5            |imagenet / advprop / noisy-student|28M                             |\n|timm-efficientnet-b6            |imagenet / advprop / noisy-student|40M                             |\n|timm-efficientnet-b7            |imagenet / advprop / noisy-student|63M                             |\n|timm-efficientnet-b8            |imagenet / advprop             |84M                             |\n|timm-efficientnet-l2            |noisy-student                   |474M                            |\n|timm-efficientnet-lite0         |imagenet                        |4M                              |\n|timm-efficientnet-lite1         |imagenet                        |5M                              |\n|timm-efficientnet-lite2         |imagenet                        |6M                              |\n|timm-efficientnet-lite3         |imagenet                        |8M                             |\n|timm-efficientnet-lite4         |imagenet                        |13M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">MobileNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|mobilenet_v2                    |imagenet                        |2M                              |\n|timm-mobilenetv3_large_075      |imagenet                        |1.78M                       |\n|timm-mobilenetv3_large_100      |imagenet                        |2.97M                       |\n|timm-mobilenetv3_large_minimal_100|imagenet                        |1.41M                       |\n|timm-mobilenetv3_small_075      |imagenet                        |0.57M                        |\n|timm-mobilenetv3_small_100      |imagenet                        |0.93M                       |\n|timm-mobilenetv3_small_minimal_100|imagenet                        |0.43M                       |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">DPN</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|dpn68                           |imagenet                        |11M                             |\n|dpn68b                          |imagenet+5k                     |11M                             |\n|dpn92                           |imagenet+5k                     |34M                             |\n|dpn98                           |imagenet                        |58M                             |\n|dpn107                          |imagenet+5k                     |84M                             |\n|dpn131                          |imagenet                        |76M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">VGG</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|vgg11                           |imagenet                        |9M                              |\n|vgg11_bn                        |imagenet                        |9M                              |\n|vgg13                           |imagenet                        |9M                              |\n|vgg13_bn                        |imagenet                        |9M                              |\n|vgg16                           |imagenet                        |14M                             |\n|vgg16_bn                        |imagenet                        |14M                             |\n|vgg19                           |imagenet                        |20M                             |\n|vgg19_bn                        |imagenet                        |20M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">Mix Vision Transformer</summary>\n<div style=\"margin-left: 25px;\">\n\nBackbone from SegFormer pretrained on Imagenet! Can be used with other decoders from package, you can combine Mix Vision Transformer with Unet, FPN and others!\n\nLimitations:  \n\n   - encoder is **not** supported by Linknet, Unet++\n   - encoder is supported by FPN only for encoder **depth = 5**\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|mit_b0                          |imagenet                        |3M                              |\n|mit_b1                          |imagenet                        |13M                             |\n|mit_b2                          |imagenet                        |24M                             |\n|mit_b3                          |imagenet                        |44M                             |\n|mit_b4                          |imagenet                        |60M                             |\n|mit_b5                          |imagenet                        |81M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">MobileOne</summary>\n<div style=\"margin-left: 25px;\">\n\nApple's \"sub-one-ms\" Backbone pretrained on Imagenet! Can be used with all decoders.\n\nNote: In the official github repo the s0 variant has additional num_conv_branches, leading to more params than s1.\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|mobileone_s0                    |imagenet                        |4.6M                              |\n|mobileone_s1                    |imagenet                        |4.0M                              |\n|mobileone_s2                    |imagenet                        |6.5M                              |\n|mobileone_s3                    |imagenet                        |8.8M                              |\n|mobileone_s4                    |imagenet                        |13.6M                             |\n\n</div>\n</details>\n\n\n\\* `ssl`, `swsl` - semi-supervised and weakly-supervised learning on ImageNet ([repo](https://github.com/facebookresearch/semi-supervised-ImageNet1K-models)).\n\n#### Timm Encoders <a name=\"timm\"></a>\n\n[docs](https://smp.readthedocs.io/en/latest/encoders_timm.html)\n\nPytorch Image Models (a.k.a. timm) has a lot of pretrained models and interface which allows using these models as encoders in smp, however, not all models are supported\n\n - not all transformer models have ``features_only`` functionality implemented that is required for encoder\n - some models have inappropriate strides\n\nTotal number of supported encoders: 549\n - [table with available encoders](https://smp.readthedocs.io/en/latest/encoders_timm.html)\n\n### üîÅ Models API <a name=\"api\"></a>\n\n - `model.encoder` - pretrained backbone to extract features of different spatial resolution\n - `model.decoder` - depends on models architecture (`Unet`/`Linknet`/`PSPNet`/`FPN`)\n - `model.segmentation_head` - last block to produce required number of mask channels (include also optional upsampling and activation)\n - `model.classification_head` - optional block which create classification head on top of encoder\n - `model.forward(x)` - sequentially pass `x` through model\\`s encoder, decoder and segmentation head (and classification head if specified)\n\n##### Input channels\nInput channels parameter allows you to create models, which process tensors with arbitrary number of channels.\nIf you use pretrained weights from imagenet - weights of first convolution will be reused. For\n1-channel case it would be a sum of weights of first convolution layer, otherwise channels would be \npopulated with weights like `new_weight[:, i] = pretrained_weight[:, i % 3]` and than scaled with `new_weight * 3 / new_in_channels`.\n```python\nmodel = smp.FPN('resnet34', in_channels=1)\nmask = model(torch.ones([1, 1, 64, 64]))\n```\n\n##### Auxiliary classification output  \nAll models support `aux_params` parameters, which is default set to `None`. \nIf `aux_params = None` then classification auxiliary output is not created, else\nmodel produce not only `mask`, but also `label` output with shape `NC`.\nClassification head consists of GlobalPooling->Dropout(optional)->Linear->Activation(optional) layers, which can be \nconfigured by `aux_params` as follows:\n```python\naux_params=dict(\n    pooling='avg',             # one of 'avg', 'max'\n    dropout=0.5,               # dropout ratio, default is None\n    activation='sigmoid',      # activation function, default is None\n    classes=4,                 # define number of output labels\n)\nmodel = smp.Unet('resnet34', classes=4, aux_params=aux_params)\nmask, label = model(x)\n```\n\n##### Depth\nDepth parameter specify a number of downsampling operations in encoder, so you can make\nyour model lighter if specify smaller `depth`.\n```python\nmodel = smp.Unet('resnet34', encoder_depth=4)\n```\n\n\n### üõ† Installation <a name=\"installation\"></a>\nPyPI version:\n```bash\n$ pip install segmentation-models-pytorch\n````\nLatest version from source:\n```bash\n$ pip install git+https://github.com/qubvel/segmentation_models.pytorch\n````\n\n### üèÜ Competitions won with the library\n\n`Segmentation Models` package is widely used in the image segmentation competitions.\n[Here](https://github.com/qubvel/segmentation_models.pytorch/blob/main/HALLOFFAME.md) you can find competitions, names of the winners and links to their solutions.\n\n### ü§ù Contributing\n\n#### Install SMP  \n\n```bash\nmake install_dev  # create .venv, install SMP in dev mode\n```\n\n#### Run tests and code checks\n\n```bash\nmake fixup         # Ruff for formatting and lint checks\n```\n\n#### Update table with encoders  \n\n```bash\nmake table        # generate a table with encoders and print to stdout\n```\n\n### üìù Citing\n```\n@misc{Iakubovskii:2019,\n  Author = {Pavel Iakubovskii},\n  Title = {Segmentation Models Pytorch},\n  Year = {2019},\n  Publisher = {GitHub},\n  Journal = {GitHub repository},\n  Howpublished = {\\url{https://github.com/qubvel/segmentation_models.pytorch}}\n}\n```\n\n### üõ°Ô∏è License <a name=\"license\"></a>\nThe project is primarily distributed under [MIT License](https://github.com/qubvel/segmentation_models.pytorch/blob/main/LICENSE), while some files are subject to other licenses. Please refer to [LICENSES](licenses/LICENSES.md) and license statements in each file for careful check, especially for commercial use.\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "licenses",
          "type": "tree",
          "content": null
        },
        {
          "name": "misc",
          "type": "tree",
          "content": null
        },
        {
          "name": "pics",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.7236328125,
          "content": "[build-system]\nrequires = ['setuptools>=61']\nbuild-backend = 'setuptools.build_meta'\n\n[project]\nname = 'segmentation_models_pytorch'\ndescription = 'Image segmentation models with pre-trained backbones. PyTorch.'\nreadme = 'README.md'\nrequires-python = '>=3.9'\nlicense = {file = 'LICENSE'}\nauthors = [{name = 'Pavel Iakubovskii', email = 'qubvel@gmail.com'}]\nclassifiers = [\n    'License :: OSI Approved :: MIT License',\n    'Programming Language :: Python',\n    'Programming Language :: Python :: 3',\n    'Programming Language :: Python :: Implementation :: CPython',\n    'Programming Language :: Python :: Implementation :: PyPy',\n]\ndependencies = [\n    'efficientnet-pytorch>=0.6.1',\n    'huggingface-hub>=0.24',\n    'numpy>=1.19.3',\n    'pillow>=8',\n    'pretrainedmodels>=0.7.1',\n    'six>=1.5',\n    'timm>=0.9',\n    'torch>=1.8',\n    'torchvision>=0.9',\n    'tqdm>=4.42.1',\n]\ndynamic = ['version']\n\n[project.optional-dependencies]\ndocs = [\n    'autodocsumm',\n    'huggingface-hub',\n    'six',\n    'sphinx',\n    'sphinx-book-theme',\n]\ntest = [\n    'packaging',\n    'pytest',\n    'pytest-cov',\n    'pytest-xdist',\n    'ruff',\n]\n\n[project.urls]\nHomepage = 'https://github.com/qubvel-org/segmentation_models.pytorch'\n\n[tool.ruff]\nextend-include = ['*.ipynb']\nfix = true\n\n[tool.setuptools.dynamic]\nversion = {attr = 'segmentation_models_pytorch.__version__.__version__'}\n\n[tool.setuptools.packages.find]\ninclude = ['segmentation_models_pytorch*']\n\n[tool.pytest.ini_options]\nmarkers = [\n    \"deeplabv3\",\n    \"deeplabv3plus\",\n    \"fpn\",\n    \"linknet\",\n    \"manet\",\n    \"pan\",\n    \"psp\",\n    \"segformer\",\n    \"unet\",\n    \"unetplusplus\",\n    \"upernet\",\n    \"logits_match\",\n]\n\n[tool.coverage.run]\nomit = [\n    \"segmentation_models_pytorch/utils/*\",\n    \"**/convert_*\",\n]\n"
        },
        {
          "name": "requirements",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "segmentation_models_pytorch",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}