{
  "metadata": {
    "timestamp": 1736560687655,
    "page": 341,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "modelscope/modelscope",
      "stars": 7219,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dev_scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.146484375,
          "content": ".gitignore\ntests\ndata\n.dev_scripts\n.dockerignore\n.git\n.gitattributes\n.pre-commit-config.yaml\n.pre-commit-config_local.yaml\n.readthedocs.yaml\nDockfile\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.4150390625,
          "content": "*.png filter=lfs diff=lfs merge=lfs -text\n*.jpg filter=lfs diff=lfs merge=lfs -text\n*.mp4 filter=lfs diff=lfs merge=lfs -text\n*.wav filter=lfs diff=lfs merge=lfs -text\n*.JPEG filter=lfs diff=lfs merge=lfs -text\n*.jpeg filter=lfs diff=lfs merge=lfs -text\n*.pickle filter=lfs diff=lfs merge=lfs -text\n*.avi filter=lfs diff=lfs merge=lfs -text\n*.bin filter=lfs diff=lfs merge=lfs -text\n*.npy filter=lfs diff=lfs merge=lfs -text\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.4267578125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\ntest.py\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n/package\n/temp\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n.vscode\n.idea\n\n# custom\n*.pkl\n*.pkl.json\n*.log.json\n*.whl\n*.tar.gz\n*.swp\n*.log\n*.tar.gz\nsource.sh\ntensorboard.sh\n.DS_Store\nreplace.sh\nresult.png\nresult.jpg\nresult.mp4\nruns/\nckpt/\n\n# Pytorch\n*.pth\n*.pt\n\n# ast template\nast_index_file.py\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.1259765625,
          "content": "[submodule \"data/test\"]\n\tpath = data/test\n\turl = https://www.modelscope.cn/ModelScope_Developer/modelscope-library-test-data.git\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.830078125,
          "content": "exclude: 'modelscope/preprocessors/templates/'\n\nrepos:\n  - repo: https://github.com/pycqa/flake8.git\n    rev: 4.0.0\n    hooks:\n      - id: flake8\n        exclude: |\n            (?x)^(\n                thirdparty/|\n                examples/|\n                modelscope/utils/ast_index_file.py|\n                modelscope/fileio/format/jsonplus.py\n            )$\n  - repo: https://github.com/PyCQA/isort.git\n    rev: 4.3.21\n    hooks:\n      - id: isort\n        exclude: |\n            (?x)^(\n                examples/|\n                modelscope/utils/ast_index_file.py|\n                modelscope/fileio/format/jsonplus.py\n            )$\n  - repo: https://github.com/pre-commit/mirrors-yapf.git\n    rev: v0.30.0\n    hooks:\n      - id: yapf\n        exclude: |\n            (?x)^(\n                thirdparty/|\n                examples/|\n                modelscope/utils/ast_index_file.py|\n                modelscope/fileio/format/jsonplus.py\n            )$\n  - repo: https://github.com/pre-commit/pre-commit-hooks.git\n    rev: v3.1.0\n    hooks:\n      - id: trailing-whitespace\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n      - id: check-yaml\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n      - id: end-of-file-fixer\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n      - id: requirements-txt-fixer\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n      - id: double-quote-string-fixer\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n      - id: check-merge-conflict\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n      - id: fix-encoding-pragma\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n        args: [\"--remove\"]\n      - id: mixed-line-ending\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n        args: [\"--fix=lf\"]\n"
        },
        {
          "name": ".pre-commit-config_local.yaml",
          "type": "blob",
          "size": 1.7607421875,
          "content": "exclude: 'modelscope/preprocessors/templates/'\n\nrepos:\n  - repo: /home/admin/pre-commit/flake8\n    rev: 4.0.0\n    hooks:\n      - id: flake8\n        exclude: |\n            (?x)^(\n                thirdparty/|\n                examples/|\n                modelscope/utils/ast_index_file.py|\n                modelscope/fileio/format/jsonplus.py\n            )$\n  - repo: /home/admin/pre-commit/isort\n    rev: 4.3.21\n    hooks:\n      - id: isort\n        exclude: |\n            (?x)^(\n                examples/|\n                modelscope/utils/ast_index_file.py|\n                modelscope/fileio/format/jsonplus.py\n            )$\n  - repo: /home/admin/pre-commit/mirrors-yapf\n    rev: v0.30.0\n    hooks:\n      - id: yapf\n        exclude: |\n            (?x)^(\n                thirdparty/|\n                examples/|\n                modelscope/utils/ast_index_file.py|\n                modelscope/fileio/format/jsonplus.py\n            )$\n  - repo: /home/admin/pre-commit/pre-commit-hooks\n    rev: v3.1.0\n    hooks:\n      - id: trailing-whitespace\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n      - id: check-yaml\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n      - id: end-of-file-fixer\n        exclude: thirdparty/\n      - id: requirements-txt-fixer\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n      - id: double-quote-string-fixer\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n      - id: check-merge-conflict\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n      - id: fix-encoding-pragma\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n        args: [\"--remove\"]\n      - id: mixed-line-ending\n        exclude: thirdparty/|modelscope/fileio/format/jsonplus.py\n        args: [\"--fix=lf\"]\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.630859375,
          "content": "version: 2\n\n# Set the version of Python and other tools you might need\nbuild:\n  os: ubuntu-20.04\n  tools:\n    python: \"3.7\"\n    # You can also specify other tool versions:\n    # nodejs: \"16\"\n    # rust: \"1.55\"\n    # golang: \"1.17\"\n  jobs:\n    post_checkout:\n      - echo \"dummy\"\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n   configuration: docs/source/conf.py\n\n# If using Sphinx, optionally build your docs in additional formats such as PDF\n# formats:\nformats: all\n\npython:\n  install:\n    - requirements: requirements/docs.txt\n    - requirements: requirements/readthedocs.txt\n    - requirements: requirements/framework.txt\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.3583984375,
          "content": "\n# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, caste, color, religion, or sexual\nidentity and orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the overall\n  community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or advances of\n  any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email address,\n  without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\ncontact@modelscope.cn.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series of\nactions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or permanent\nban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within the\ncommunity.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.1, available at\n[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].\n\nCommunity Impact Guidelines were inspired by\n[Mozilla's code of conduct enforcement ladder][Mozilla CoC].\n\nFor answers to common questions about this code of conduct, see the FAQ at\n[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at\n[https://www.contributor-covenant.org/translations][translations].\n\n[homepage]: https://www.contributor-covenant.org\n[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html\n[Mozilla CoC]: https://github.com/mozilla/diversity\n[FAQ]: https://www.contributor-covenant.org/faq\n[translations]: https://www.contributor-covenant.org/translations\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.1435546875,
          "content": "recursive-include modelscope/configs *.py *.cu *.h *.cpp\nrecursive-include modelscope/cli/template *.tpl\nrecursive-include modelscope/utils *.json\n"
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 0.3505859375,
          "content": "WHL_BUILD_DIR :=package\nDOC_BUILD_DIR :=docs/build/\n\n# default rule\ndefault: whl docs\n\n.PHONY: docs\ndocs:\n\tbash .dev_scripts/build_docs.sh\n\n.PHONY: linter\nlinter:\n\tbash .dev_scripts/linter.sh\n\n.PHONY: test\ntest:\n\tbash .dev_scripts/citest.sh\n\n.PHONY: whl\nwhl:\n\tpython setup.py sdist bdist_wheel\n\n.PHONY: clean\nclean:\n\trm -rf  $(WHL_BUILD_DIR) $(DOC_BUILD_DIR)\n"
        },
        {
          "name": "Makefile.docker",
          "type": "blob",
          "size": 2.201171875,
          "content": "DOCKER_REGISTRY           = registry.cn-shanghai.aliyuncs.com\nDOCKER_ORG                = modelscope\nDOCKER_IMAGE              = modelscope\nDOCKER_FULL_NAME          = $(DOCKER_REGISTRY)/$(DOCKER_ORG)/$(DOCKER_IMAGE)\n\n# CUDA_VERSION              = 11.3\n# CUDNN_VERSION             = 8\nBASE_RUNTIME              = reg.docker.alibaba-inc.com/pai-dlc/pytorch-training:1.10PAI-gpu-py36-cu113-ubuntu18.04\n# BASE_DEVEL                = reg.docker.alibaba-inc.com/pai-dlc/pytorch-training:1.10PAI-gpu-py36-cu113-ubuntu18.04\nBASE_DEVEL                = pytorch/pytorch:1.10.0-cuda11.3-cudnn8-devel\n\n\nMODELSCOPE_VERSION           = $(shell git describe --tags --always)\n\n# Can be either official / dev\nBUILD_TYPE                = dev\nBUILD_PROGRESS            = auto\nBUILD_ARGS                = --build-arg BASE_IMAGE=$(BASE_IMAGE)\n\nEXTRA_DOCKER_BUILD_FLAGS ?= --network=host\n# DOCKER_BUILD              = DOCKER_BUILDKIT=1 \\\n# \t\t\t\t\t\t\tdocker build \\\n# \t\t\t\t\t\t\t\t--progress=$(BUILD_PROGRESS) \\\n# \t\t\t\t\t\t\t\t$(EXTRA_DOCKER_BUILD_FLAGS) \\\n# \t\t\t\t\t\t\t\t--target $(BUILD_TYPE) \\\n# \t\t\t\t\t\t\t\t-t $(DOCKER_FULL_NAME):$(DOCKER_TAG) \\\n# \t\t\t\t\t\t\t\t$(BUILD_ARGS) \\\n#\t\t\t\t\t\t\t\t-f docker/pytorch.dockerfile .\nDOCKER_BUILD              = DOCKER_BUILDKIT=1 \\\n\t\t\t\t\t\t\tdocker build \\\n\t\t\t\t\t\t\t\t$(EXTRA_DOCKER_BUILD_FLAGS) \\\n\t\t\t\t\t\t\t\t-t $(DOCKER_FULL_NAME):$(DOCKER_TAG) \\\n\t\t\t\t\t\t\t\t$(BUILD_ARGS)  \\\n\t\t\t\t\t\t\t\t-f docker/pytorch.dockerfile .\nDOCKER_PUSH               = docker push $(DOCKER_FULL_NAME):$(DOCKER_TAG)\n\n.PHONY: all\nall: devel-image\n\n.PHONY: devel-image\ndevel-image: BASE_IMAGE := $(BASE_DEVEL)\ndevel-image: DOCKER_TAG := $(MODELSCOPE_VERSION)-devel\ndevel-image:\n\t$(DOCKER_BUILD)\n\n.PHONY: devel-push\ndevel-push: BASE_IMAGE := $(BASE_DEVEL)\ndevel-push: DOCKER_TAG := $(MODELSCOPE_VERSION)-devel\ndevel-push:\n\t$(DOCKER_PUSH)\n\n.PHONY: runtime-image\nruntime-image: BASE_IMAGE := $(BASE_RUNTIME)\nruntime-image: DOCKER_TAG := $(MODELSCOPE_VERSION)-runtime\nruntime-image:\n\t$(DOCKER_BUILD)\n\tdocker tag $(DOCKER_FULL_NAME):$(DOCKER_TAG) $(DOCKER_FULL_NAME):latest\n\n.PHONY: runtime-push\nruntime-push: BASE_IMAGE := $(BASE_RUNTIME)\nruntime-push: DOCKER_TAG := $(MODELSCOPE_VERSION)-runtime\nruntime-push:\n\t$(DOCKER_PUSH)\n\n.PHONY: clean\nclean:\n\t-docker rmi -f $(shell docker images -q $(DOCKER_FULL_NAME))\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.0009765625,
          "content": "\n<p align=\"center\">\n    <br>\n    <img src=\"https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif\" width=\"400\"/>\n    <br>\n<p>\n\n<div align=\"center\">\n\n[![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)\n<!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->\n[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)\n[![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)\n[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)\n[![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)\n\n<!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->\n<!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->\n[Discord](https://discord.gg/FMupRv4jUR)\n\n<h4 align=\"center\">\n<a href=\"https://trendshift.io/repositories/4784\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/4784\" alt=\"modelscope%2Fmodelscope | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</h4>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/modelscope/modelscope/blob/master/README_zh.md\">中文</a> |\n        <a href=\"https://github.com/modelscope/modelscope/blob/master/README_ja.md\">日本語</a>\n    <p>\n</h4>\n\n\n</div>\n\n# Introduction\n\n[ModelScope]( https://www.modelscope.cn) is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform  model inference, training and evaluation.\n\n\nIn particular, with rich layers of API-abstraction, the ModelScope library offers unified experience to explore state-of-the-art models spanning across domains such as CV, NLP, Speech, Multi-Modality, and Scientific-computation. Model contributors of different areas can integrate models into the ModelScope ecosystem through the layered-APIs, allowing easy and unified access to their models. Once integrated, model inference, fine-tuning, and evaluations can be done with only a few lines of codes. In the meantime, flexibilities are also provided so that different components in the model applications can be customized wherever necessary.\n\nApart from harboring implementations of a wide range of different models, ModelScope library also enables the necessary interactions with ModelScope backend services, particularly with the Model-Hub and Dataset-Hub. Such interactions facilitate management of  various entities (models and datasets) to be performed seamlessly under-the-hood, including entity lookup, version control, cache management, and many others.\n\n# Models and Online Accessibility\n\nHundreds of models are made publicly available on [ModelScope]( https://www.modelscope.cn)  (700+ and counting), covering the latest development in areas such as NLP, CV, Audio, Multi-modality, and AI for Science, etc. Many of these models represent the SOTA in their specific fields, and made their open-sourced debut on ModelScope. Users can visit ModelScope([modelscope.cn](http://www.modelscope.cn)) and experience first-hand how these models perform via online experience, with just a few clicks. Immediate developer-experience is also possible through the ModelScope Notebook, which is backed by ready-to-use CPU/GPU development environment in the cloud - only one click away on [ModelScope](https://www.modelscope.cn).\n\n\n<p align=\"center\">\n    <br>\n    <img src=\"data/resource/inference.gif\" width=\"1024\"/>\n    <br>\n<p>\n\nSome representative examples include:\n\nLLM:\n\n* [Yi-1.5-34B-Chat](https://modelscope.cn/models/01ai/Yi-1.5-34B-Chat/summary)\n\n* [Qwen1.5-110B-Chat](https://modelscope.cn/models/qwen/Qwen1.5-110B-Chat/summary)\n\n* [DeepSeek-V2-Chat](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2-Chat/summary)\n\n* [Ziya2-13B-Chat](https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Chat/summary)\n\n* [Meta-Llama-3-8B-Instruct](https://modelscope.cn/models/LLM-Research/Meta-Llama-3-8B-Instruct/summary)\n\n* [Phi-3-mini-128k-instruct](https://modelscope.cn/models/LLM-Research/Phi-3-mini-128k-instruct/summary)\n\n\nMulti-Modal:\n\n* [Qwen-VL-Chat](https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary)\n\n* [Yi-VL-6B](https://modelscope.cn/models/01ai/Yi-VL-6B/summary)\n\n* [InternVL-Chat-V1-5](https://modelscope.cn/models/AI-ModelScope/InternVL-Chat-V1-5/summary)\n\n* [deepseek-vl-7b-chat](https://modelscope.cn/models/deepseek-ai/deepseek-vl-7b-chat/summary)\n\n* [OpenSoraPlan](https://modelscope.cn/models/AI-ModelScope/Open-Sora-Plan-v1.0.0/summary)\n\n* [OpenSora](https://modelscope.cn/models/luchentech/OpenSora-STDiT-v1-HQ-16x512x512/summary)\n\n* [I2VGen-XL](https://modelscope.cn/models/iic/i2vgen-xl/summary)\n\nCV:\n\n* [DamoFD Face Detection Key Point Model - 0.5G](https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd/summary)\n\n* [BSHM Portrait Matting](https://modelscope.cn/models/damo/cv_unet_image-matting/summary)\n\n* [DCT-Net Portrait Cartoonization - 3D](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon-3d_compound-models/summary)\n\n* [DCT-Net Portrait Cartoonization Model - 3D](https://modelscope.cn/models/damo/face_chain_control_model/summary)\n\n* [DuGuang - Text Recognition - Line Recognition Model - Chinese and English - General Domain](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo/summary)\n\n* [DuGuang - Text Recognition - Line Recognition Model - Chinese and English - General Domain](https://modelscope.cn/models/damo/cv_resnet18_ocr-detection-line-level_damo/summary)\n\n* [LaMa Image Inpainting](https://modelscope.cn/models/damo/cv_fft_inpainting_lama/summary)\n\n\nAudio:\n\n* [Paraformer Speech Recognition - Chinese - General - 16k - Offline - Large - Long Audio Version](https://modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)\n\n* [FSMN Voice Endpoint Detection - Chinese - General - 16k - onnx](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-onnx/summary)\n\n* [Monotonic-Aligner Speech Timestamp Prediction - 16k - Offline](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary)\n\n* [CT-Transformer Punctuation - Chinese - General - onnx](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-onnx/summary)\n\n* [Speech Synthesis - Chinese - Multiple Emotions Domain - 16k - Multiple Speakers](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k/summary)\n\n* [CAM++ Speaker Verification - Chinese - General - 200k-Spkrs](https://modelscope.cn/models/damo/speech_campplus_sv_zh-cn_16k-common/summary)\n\n\n\nAI for Science:\n\n* [uni-fold-monomer](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)\n\n* [uni-fold-multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)\n\n**Note:** Most models on ModelScope are public and can be downloaded without account registration on modelscope website([www.modelscope.cn](www.modelscope.cn)), please refer to instructions for [model download](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD), for dowloading models with api provided by modelscope library or git.\n\n# QuickTour\n\nWe provide unified interface for inference using `pipeline`, fine-tuning and evaluation using `Trainer` for different tasks.\n\nFor any given task with any type of input (image, text, audio, video...), inference pipeline can be implemented with only a few lines of code, which will automatically load the underlying model to get inference result, as is exemplified below:\n\n```python\n>>> from modelscope.pipelines import pipeline\n>>> word_segmentation = pipeline('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')\n>>> word_segmentation('今天天气不错，适合出去游玩')\n{'output': '今天 天气 不错 ， 适合 出去 游玩'}\n```\n\nGiven an image, portrait matting (aka. background-removal) can be accomplished with the following code snippet:\n\n![image](data/resource/portrait_input.png)\n\n```python\n>>> import cv2\n>>> from modelscope.pipelines import pipeline\n\n>>> portrait_matting = pipeline('portrait-matting')\n>>> result = portrait_matting('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')\n>>> cv2.imwrite('result.png', result['output_img'])\n```\n\nThe output image with the background removed is:\n![image](data/resource/portrait_output.png)\n\n\nFine-tuning and evaluation can also be done with a few more lines of code to set up training dataset and trainer, with the heavy-lifting work of training and evaluation a model encapsulated in the implementation of  `traner.train()` and\n`trainer.evaluate()`  interfaces.\n\nFor example, the gpt3 base model (1.3B) can be fine-tuned with the chinese-poetry dataset, resulting in a model that can be used for chinese-poetry generation.\n\n```python\n>>> from modelscope.metainfo import Trainers\n>>> from modelscope.msdatasets import MsDataset\n>>> from modelscope.trainers import build_trainer\n\n>>> train_dataset = MsDataset.load('chinese-poetry-collection', split='train'). remap_columns({'text1': 'src_txt'})\n>>> eval_dataset = MsDataset.load('chinese-poetry-collection', split='test').remap_columns({'text1': 'src_txt'})\n>>> max_epochs = 10\n>>> tmp_dir = './gpt3_poetry'\n\n>>> kwargs = dict(\n     model='damo/nlp_gpt3_text-generation_1.3B',\n     train_dataset=train_dataset,\n     eval_dataset=eval_dataset,\n     max_epochs=max_epochs,\n     work_dir=tmp_dir)\n\n>>> trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)\n>>> trainer.train()\n```\n\n# Why should I use ModelScope library\n\n1. A unified and concise user interface is abstracted for different tasks and different models. Model inferences and training can be implemented by as few as 3 and 10 lines of code, respectively. It is convenient for users to explore models in different fields in the ModelScope community. All models integrated into ModelScope are ready to use, which makes it easy to get started with AI, in both educational and industrial settings.\n\n2. ModelScope offers a model-centric development and application experience. It streamlines the support for model training, inference, export and deployment, and facilitates users to build their own MLOps based on the ModelScope ecosystem.\n\n3. For the model inference and training process, a modular design is put in place, and a wealth of functional module implementations are provided, which is convenient for users to customize their own model inference, training and other processes.\n\n4. For distributed model training, especially for large models, it provides rich training strategy support, including data parallel, model parallel, hybrid parallel and so on.\n\n# Installation\n\n## Docker\n\nModelScope Library currently supports popular deep learning framework for model training and inference, including PyTorch, TensorFlow and ONNX. All releases are tested and run on Python 3.7+, Pytorch 1.8+, Tensorflow1.15 or Tensorflow2.0+.\n\nTo allow out-of-box usage for all the models on ModelScope, official docker images are provided for all releases. Based on the docker image, developers can skip all environment installation and configuration and use it directly. Currently, the latest version of the CPU image and GPU image can be obtained from:\n\nCPU docker image\n```shell\n# py37\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.6.1\n\n# py38\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py38-torch2.0.1-tf2.13.0-1.9.5\n```\n\nGPU docker image\n```shell\n# py37\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.6.1\n\n# py38\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.8.0-py38-torch2.0.1-tf2.13.0-1.9.5\n```\n\n## Setup Local Python Environment\n\nOne can also set up local ModelScope environment using pip and conda.  ModelScope supports python3.7 and above.\nWe suggest [anaconda](https://docs.anaconda.com/anaconda/install/) for creating local python environment:\n\n```shell\nconda create -n modelscope python=3.8\nconda activate modelscope\n```\n\nPyTorch or TensorFlow can be installed separately according to each model's requirements.\n* Install pytorch [doc](https://pytorch.org/get-started/locally/)\n* Install tensorflow [doc](https://www.tensorflow.org/install/pip)\n\nAfter installing the necessary machine-learning framework, you can install modelscope library as follows:\n\nIf you only want to play around with the modelscope framework, of trying out model/dataset download, you can install the core modelscope components:\n```shell\npip install modelscope\n```\n\nIf you want to use multi-modal models:\n```shell\npip install modelscope[multi-modal]\n```\n\nIf you want to use nlp models:\n```shell\npip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\nIf you want to use cv models:\n```shell\npip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\nIf you want to use audio models:\n```shell\npip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\nIf you want to use science models:\n```shell\npip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\n`Notes`:\n1. Currently, some audio-task models only support python3.7, tensorflow1.15.4 Linux environments. Most other models can be installed and used on Windows and Mac (x86).\n\n2. Some models in the audio field use the third-party library SoundFile for wav file processing. On the Linux system, users need to manually install libsndfile of SoundFile([doc link](https://github.com/bastibe/python-soundfile#installation)). On Windows and MacOS, it will be installed automatically without user operation. For example, on Ubuntu, you can use following commands:\n    ```shell\n    sudo apt-get update\n    sudo apt-get install libsndfile1\n    ```\n\n3. Some models in computer vision need mmcv-full, you can refer to mmcv [installation guide](https://github.com/open-mmlab/mmcv#installation), a minimal installation is as follows:\n\n    ```shell\n    pip uninstall mmcv # if you have installed mmcv, uninstall it\n    pip install -U openmim\n    mim install mmcv-full\n    ```\n\n\n\n# Learn More\n\nWe  provide additional documentations including:\n* [More detailed Installation Guide](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)\n* [Introduction to tasks](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)\n* [Use pipeline for model inference](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)\n* [Finetuning example](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)\n* [Preprocessing of data](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)\n* [Evaluation](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)\n* [Contribute your own model to ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)\n\n# License\n\nThis project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).\n\n# Citation\n```\n@Misc{modelscope,\n  title = {ModelScope: bring the notion of Model-as-a-Service to life.},\n  author = {The ModelScope Team},\n  howpublished = {\\url{https://github.com/modelscope/modelscope}},\n  year = {2023}\n}\n```\n"
        },
        {
          "name": "README_ja.md",
          "type": "blob",
          "size": 18.0078125,
          "content": "\n<p align=\"center\">\n    <br>\n    <img src=\"https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif\" width=\"400\"/>\n    <br>\n<p>\n\n<div align=\"center\">\n\n[![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)\n<!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->\n[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)\n[![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)\n[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)\n[![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)\n\n<!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->\n<!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->\n[Discord](https://discord.gg/FMupRv4jUR)\n\n<h4 align=\"center\">\n<a href=\"https://trendshift.io/repositories/4784\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/4784\" alt=\"modelscope%2Fmodelscope | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</h4>\n\n<h4 align=\"center\">\n    <p>\n        <a href=\"https://github.com/modelscope/modelscope/blob/master/README.md\">English</a> |\n        <a href=\"https://github.com/modelscope/modelscope/blob/master/README_zh.md\">中文</a> |\n        日本語\n    <p>\n</h4>\n\n\n</div>\n\n# はじめに\n\n[ModelScope](https://www.modelscope.cn) は、\"Model-as-a-Service\"(MaaS) の概念に基づいて構築されています。AI コミュニティから最も先進的な機械学習モデルを集め、実世界のアプリケーションで AI モデルを活用するプロセスを合理化することを目指しています。このリポジトリでオープンソース化されている中核となる ModelScope ライブラリは、開発者がモデルの推論、トレーニング、評価を実行するためのインターフェースと実装を提供します。\n\n\n特に、API 抽象化の豊富なレイヤーにより、ModelScope ライブラリは、CV、NLP、音声、マルチモダリティ、科学計算などのドメインにまたがる最先端のモデルを探索するための統一された体験を提供します。様々な分野のモデル貢献者は、レイヤー化された API を通じてモデルを ModelScope エコシステムに統合することができ、モデルへの容易で統一されたアクセスを可能にします。一旦統合されると、モデルの推論、微調整、および評価は、わずか数行のコードで行うことができます。一方、モデルアプリケーションの様々なコンポーネントを必要に応じてカスタマイズできるように、柔軟性も提供されています。\n\nModelScope ライブラリは、様々なモデルの実装を保持するだけでなく、ModelScope のバックエンドサービス、特に Model-Hub と Dataset-Hub との必要な相互作用も可能にします。このような相互作用により、エンティティの検索、バージョン管理、キャッシュ管理など、様々なエンティティ（モデルやデータセット）の管理をアンダーザフードでシームレスに実行することができます。\n\n# モデルとオンラインアクセシビリティ\n\n[ModelScope](https://www.modelscope.cn) では、NLP、CV、オーディオ、マルチモダリティ、科学のための AI などの分野の最新開発を網羅した、何百ものモデルが一般公開されています（700 以上、カウント中）。これらのモデルの多くは、特定の分野における SOTA を代表するものであり、ModelScope でオープンソースとしてデビューしました。ユーザーは、ModelScope([modelscope.cn](http://www.modelscope.cn)) にアクセスし、数回クリックするだけで、オンライン体験を通じて、これらのモデルがどのように機能するかを直接体験することができます。また、[ModelScope](https://www.modelscope.cn) をワンクリックするだけで、クラウド上のすぐに使える CPU/GPU 開発環境に支えられた ModelScope ノートブックを通じて、すぐに開発者体験が可能です。\n\n\n<p align=\"center\">\n    <br>\n    <img src=\"data/resource/inference.gif\" width=\"1024\"/>\n    <br>\n<p>\n\n代表的な例をいくつか挙げると:\n\n大きなモデル:\n\n* [Yi-1.5-34B-Chat](https://modelscope.cn/models/01ai/Yi-1.5-34B-Chat/summary)\n\n* [Qwen1.5-110B-Chat](https://modelscope.cn/models/qwen/Qwen1.5-110B-Chat/summary)\n\n* [DeepSeek-V2-Chat](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2-Chat/summary)\n\n* [Ziya2-13B-Chat](https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Chat/summary)\n\n* [Meta-Llama-3-8B-Instruct](https://modelscope.cn/models/LLM-Research/Meta-Llama-3-8B-Instruct/summary)\n\n* [Phi-3-mini-128k-instruct](https://modelscope.cn/models/LLM-Research/Phi-3-mini-128k-instruct/summary)\n\n\nマルチモーダル:\n\n* [Qwen-VL-Chat](https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary)\n\n* [Yi-VL-6B](https://modelscope.cn/models/01ai/Yi-VL-6B/summary)\n\n* [InternVL-Chat-V1-5](https://modelscope.cn/models/AI-ModelScope/InternVL-Chat-V1-5/summary)\n\n* [deepseek-vl-7b-chat](https://modelscope.cn/models/deepseek-ai/deepseek-vl-7b-chat/summary)\n\n* [OpenSoraPlan](https://modelscope.cn/models/AI-ModelScope/Open-Sora-Plan-v1.0.0/summary)\n\n* [OpenSora](https://modelscope.cn/models/luchentech/OpenSora-STDiT-v1-HQ-16x512x512/summary)\n\n* [I2VGen-XL](https://modelscope.cn/models/iic/i2vgen-xl/summary)\n\nCV:\n\n* [cv_controlnet_controllable-image-generation_nine-annotators](https://modelscope.cn/models/dienstag/cv_controlnet_controllable-image-generation_nine-annotators/summary)\n\n* [cv_tinynas_object-detection_damoyolo](https://modelscope.cn/models/damo/cv_tinynas_object-detection_damoyolo)\n\n* [cv_unet_person-image-cartoon_compound-models](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon_compound-models)\n\n* [cv_convnextTiny_ocr-recognition-general_damo](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo)\n\n* [cv_resnet18_human-detection](https://modelscope.cn/models/damo/cv_resnet18_human-detection)\n\n* [cv_resnet50_face-detection_retinaface](https://modelscope.cn/models/damo/cv_resnet50_face-detection_retinaface)\n\n* [cv_unet_image-matting](https://modelscope.cn/models/damo/cv_unet_image-matting)\n\n* [cv_F3Net_product-segmentation](https://modelscope.cn/models/damo/cv_F3Net_product-segmentation)\n\n* [cv_resnest101_general_recognition](https://modelscope.cn/models/damo/cv_resnest101_general_recognition)\n\n\n音声:\n\n* [speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch)\n\n* [speech_sambert-hifigan_tts_zh-cn_16k](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k)\n\n* [speech_charctc_kws_phone-xiaoyun](https://modelscope.cn/models/damo/speech_charctc_kws_phone-xiaoyun)\n\n* [u2pp_conformer-asr-cn-16k-online](https://modelscope.cn/models/wenet/u2pp_conformer-asr-cn-16k-online)\n\n* [speech_fsmn_vad_zh-cn-16k-common-pytorch](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary)\n\n* [punc_ct-transformer_zh-cn-common-vocab272727-pytorch](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/summary)\n\n* [speech_frcrn_ans_cirm_16k](https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k)\n\n* [speech_dfsmn_aec_psm_16k](https://modelscope.cn/models/damo/speech_dfsmn_aec_psm_16k)\n\n\n\n科学用 AI:\n\n* [uni-fold-monomer](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)\n\n* [uni-fold-multimer](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)\n\n**注:** ModelScope のほとんどのモデルは公開されており、アカウント登録なしで modelscope のウェブサイト([www.modelscope.cn](www.modelscope.cn))からダウンロードすることができます。modelscope のライブラリや git が提供する api を使用してモデルをダウンロードするには、[モデルのダウンロード](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8B%E8%BD%BD)の説明を参照してください。\n\n# クイックツアー\n\n様々なタスクに対して、`pipeline` による推論、`Trainer` による微調整と評価のための統一されたインターフェースを提供します。\n\n入力の種類（画像、テキスト、音声、動画...）を問わず、推論パイプラインはわずか数行のコードで実装することができます。:\n\n```python\n>>> from modelscope.pipelines import pipeline\n>>> word_segmentation = pipeline('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')\n>>> word_segmentation('今天天气不错，适合出去游玩')\n{'output': '今天 天气 不错 ， 适合 出去 游玩'}\n```\n\n画像があれば、ポートレート・マット（別名、背景除去）は次のコード・スニペットで実現できます:\n\n![image](data/resource/portrait_input.png)\n\n```python\n>>> import cv2\n>>> from modelscope.pipelines import pipeline\n\n>>> portrait_matting = pipeline('portrait-matting')\n>>> result = portrait_matting('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')\n>>> cv2.imwrite('result.png', result['output_img'])\n```\n\n背景を除去した出力画像は次のようになります:\n![image](data/resource/portrait_output.png)\n\n\nファインチューニングと評価も、トレーニングデータセットとトレーナーをセットアップする数行のコードで行うことができ、モデルのトレーニングと評価の重い作業は `traner.train()` と `trainer.evaluate()` インターフェースの実装に\nカプセル化されています。\n\n例えば、gpt3 の基本モデル（1.3B）を中国語詩のデータセットでファインチューニングすることで、中国語詩の生成に使用できるモデルを得ることができる。\n\n```python\n>>> from modelscope.metainfo import Trainers\n>>> from modelscope.msdatasets import MsDataset\n>>> from modelscope.trainers import build_trainer\n\n>>> train_dataset = MsDataset.load('chinese-poetry-collection', split='train'). remap_columns({'text1': 'src_txt'})\n>>> eval_dataset = MsDataset.load('chinese-poetry-collection', split='test').remap_columns({'text1': 'src_txt'})\n>>> max_epochs = 10\n>>> tmp_dir = './gpt3_poetry'\n\n>>> kwargs = dict(\n     model='damo/nlp_gpt3_text-generation_1.3B',\n     train_dataset=train_dataset,\n     eval_dataset=eval_dataset,\n     max_epochs=max_epochs,\n     work_dir=tmp_dir)\n\n>>> trainer = build_trainer(name=Trainers.gpt3_trainer, default_args=kwargs)\n>>> trainer.train()\n```\n\n# ModelScope ライブラリを使用する理由\n\n1. 統一された簡潔なユーザーインターフェースは、異なるタスクや異なるモデル用に抽象化されている。モデルの推論とトレーニングは、それぞれわずか 3 行と 10 行のコードで実装できる。ModelScope コミュニティで異なる分野のモデルを探索するのに便利です。ModelScope に統合されたモデルはすべてすぐに使用できるため、教育現場でも産業現場でも、AI を簡単に使い始めることができます。\n\n2. ModelScope は、モデル中心の開発とアプリケーション体験を提供します。モデルのトレーニング、推論、エクスポート、デプロイメントのサポートを合理化し、ユーザーが ModelScope エコシステムに基づいて独自の MLO を構築することを容易にします。\n\n3. モデルの推論とトレーニングのプロセスでは、モジュール設計が導入され、豊富な機能モジュールの実装が提供され、ユーザーが独自のモデルの推論、トレーニング、その他のプロセスをカスタマイズするのに便利です。\n\n4. 分散モデル学習、特に大規模モデルに対しては、データ並列、モデル並列、ハイブリッド並列など、豊富な学習ストラテジーサポートを提供する。\n\n# インストール\n\n## Docker\n\nModelScope ライブラリは現在、PyTorch、TensorFlow、ONNX を含む、モデルの学習と推論のための一般的なディープラーニングフレームワークをサポートしています。すべてのリリースは、Python 3.7+、Pytorch 1.8+、Tensorflow1.15、または Tensorflow2.0+ でテストされ、実行されます。\n\nModelScope のすべてのモデルをすぐに使えるようにするため、すべてのリリースで公式の docker イメージが提供されています。開発者はこの docker イメージをベースに、環境のインストールや設定をすべて省略して直接使用することができます。現在、CPU イメージと GPU イメージの最新バージョンは以下から入手できます:\n\nCPU docker イメージ\n```shell\n# py37\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.6.1\n\n# py38\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py38-torch2.0.1-tf2.13.0-1.9.5\n```\n\nGPU docker イメージ\n```shell\n# py37\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.6.1\n\n# py38\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.8.0-py38-torch2.0.1-tf2.13.0-1.9.5\n```\n\n## ローカル Python 環境のセットアップ\n\npip と conda を使って、ModelScope のローカル環境を構築することもできます。 ローカルの Python 環境を構築するには [anaconda](https://docs.anaconda.com/anaconda/install/) をお勧めします:\n\n```shell\nconda create -n modelscope python=3.7\nconda activate modelscope\n```\n\nPyTorch または TensorFlow は、それぞれのモデルの要件に応じて個別にインストールすることができます。\n* pytorch のインストール [doc](https://pytorch.org/get-started/locally/)\n* Tensorflow のインストール [doc](https://www.tensorflow.org/install/pip)\n\n必要な機械学習フレームワークをインストールした後、以下のように modelscope ライブラリをインストールします:\n\nモデル／データセットのダウンロードを試したり、modelscope フレームワークで遊びたいだけなら、modelscope のコア・コンポーネントをインストールすることができます:\n```shell\npip install modelscope\n```\n\nマルチモーダルモデルを使いたい場合:\n```shell\npip install modelscope[multi-modal]\n```\n\nnlp モデルを使いたい場合:\n```shell\npip install modelscope[nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\nCV モデルを使いたい場合:\n```shell\npip install modelscope[cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\nオーディオモデルを使用したい場合:\n```shell\npip install modelscope[audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\n科学モデルを使いたい場合:\n```shell\npip install modelscope[science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\n`備考`:\n1. 現在、一部のオーディオタスクモデルは python3.7、tensorflow1.15.4 の Linux 環境のみに対応しています。他のほとんどのモデルは Windows と Mac(x86) にインストールして使うことができます。\n\n2. オーディオ分野では、wav ファイルの処理にサードパーティ製のライブラリ SoundFile を使用している機種がある。Linux では、SoundFile の libsndfile([doc link](https://github.com/bastibe/python-soundfile#installation)) を手動でインストールする必要があります。Windows や MacOS では、ユーザーが操作しなくても自動的にインストールされる。例えば、Ubuntu の場合、以下のコマンドでインストールできます:\n    ```shell\n    sudo apt-get update\n    sudo apt-get install libsndfile1\n    ```\n\n3. コンピュータビジョンのモデルによっては mmcv-full が必要です。mmcv [インストールガイド](https://github.com/open-mmlab/mmcv#installation)を参照してください。最小限のインストールは以下の通りです:\n\n    ```shell\n    pip uninstall mmcv # mmcv をインストールしている場合は、アンインストールしてください\n    pip install -U openmim\n    mim install mmcv-full\n    ```\n\n\n\n# 詳細\n\n私たちは、以下のような追加書類を提供します:\n* [より詳細なインストールガイド](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)\n* [タスクの紹介](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)\n* [モデル推論にパイプラインを使う](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)\n* [ファインチューニング例](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)\n* [データの前処理](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)\n* [評価](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)\n* [ModelScope に自分のモデルを投稿する](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)\n\n# ライセンス\n\nこのプロジェクトのライセンスは [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE) です。\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 14.66796875,
          "content": "\n<p align=\"center\">\n    <br>\n    <img src=\"https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif\" width=\"400\"/>\n    <br>\n<p>\n\n<div align=\"center\">\n\n[![PyPI](https://img.shields.io/pypi/v/modelscope)](https://pypi.org/project/modelscope/)\n<!-- [![Documentation Status](https://readthedocs.org/projects/easy-cv/badge/?version=latest)](https://easy-cv.readthedocs.io/en/latest/) -->\n[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/blob/master/LICENSE)\n[![open issues](https://isitmaintained.com/badge/open/modelscope/modelscope.svg)](https://github.com/modelscope/modelscope/issues)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/pull/)\n[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/modelscope)](https://GitHub.com/modelscope/modelscope/commit/)\n[![Leaderboard](https://img.shields.io/badge/ModelScope-Check%20Your%20Contribution-orange)](https://opensource.alibaba.com/contribution_leaderboard/details?projectValue=modelscope)\n\n<!-- [![GitHub contributors](https://img.shields.io/github/contributors/modelscope/modelscope.svg)](https://GitHub.com/modelscope/modelscope/graphs/contributors/) -->\n<!-- [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) -->\n[Discord](https://discord.gg/FMupRv4jUR)\n\n<h4 align=\"center\">\n<a href=\"https://trendshift.io/repositories/4784\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/4784\" alt=\"modelscope%2Fmodelscope | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</h4>\n\n<h4 align=\"center\">\n    <p>\n      <a href=\"https://github.com/modelscope/modelscope/blob/master/README.md\">English</a> |\n        <b> 中文 </b> |\n        <a href=\"https://github.com/modelscope/modelscope/blob/master/README_ja.md\"> 日本語 </a>\n    <p>\n</h4>\n\n</div>\n\n# 简介\n\n[ModelScope](https://www.modelscope.cn) 是一个 “模型即服务”(MaaS) 平台，旨在汇集来自 AI 社区的最先进的机器学习模型，并简化在实际应用中使用 AI 模型的流程。ModelScope 库使开发人员能够通过丰富的 API 设计执行推理、训练和评估，从而促进跨不同 AI 领域的最先进模型的统一体验。\n\nModelScope Library 为模型贡献者提供了必要的分层 API，以便将来自 CV、NLP、语音、多模态以及科学计算的模型集成到 ModelScope 生态系统中。所有这些不同模型的实现都以一种简单统一访问的方式进行封装，用户只需几行代码即可完成模型推理、微调和评估。同时，灵活的模块化设计使得在必要时也可以自定义模型训练推理过程中的不同组件。\n\n除了包含各种模型的实现之外，ModelScope Library 还支持与 ModelScope 后端服务进行必要的交互，特别是与 Model-Hub 和 Dataset-Hub 的交互。这种交互促进了模型和数据集的管理在后台无缝执行，包括模型数据集查询、版本控制、缓存管理等。\n\n# 部分模型和在线体验\n\nModelScope 开源了数百个 (当前 700+) 模型，涵盖自然语言处理、计算机视觉、语音、多模态、科学计算等，其中包含数百个 SOTA 模型。用户可以进入 ModelScope 网站 ([modelscope.cn](http://www.modelscope.cn)) 的模型中心零门槛在线体验，或者 Notebook 方式体验模型。\n\n<p align=\"center\">\n    <br>\n    <img src=\"data/resource/inference.gif\" width=\"1024\"/>\n    <br>\n<p>\n\n示例如下:\n\n大模型：\n\n* [Yi-1.5-34B-Chat](https://modelscope.cn/models/01ai/Yi-1.5-34B-Chat/summary)\n\n* [Qwen1.5-110B-Chat](https://modelscope.cn/models/qwen/Qwen1.5-110B-Chat/summary)\n\n* [DeepSeek-V2-Chat](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2-Chat/summary)\n\n* [Ziya2-13B-Chat](https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Chat/summary)\n\n* [Meta-Llama-3-8B-Instruct](https://modelscope.cn/models/LLM-Research/Meta-Llama-3-8B-Instruct/summary)\n\n* [Phi-3-mini-128k-instruct](https://modelscope.cn/models/LLM-Research/Phi-3-mini-128k-instruct/summary)\n\n多模态：\n\n* [Qwen-VL-Chat](https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary)\n\n* [Yi-VL-6B](https://modelscope.cn/models/01ai/Yi-VL-6B/summary)\n\n* [InternVL-Chat-V1-5](https://modelscope.cn/models/AI-ModelScope/InternVL-Chat-V1-5/summary)\n\n* [deepseek-vl-7b-chat](https://modelscope.cn/models/deepseek-ai/deepseek-vl-7b-chat/summary)\n\n* [OpenSoraPlan](https://modelscope.cn/models/AI-ModelScope/Open-Sora-Plan-v1.0.0/summary)\n\n* [OpenSora](https://modelscope.cn/models/luchentech/OpenSora-STDiT-v1-HQ-16x512x512/summary)\n\n* [I2VGen-XL](https://modelscope.cn/models/iic/i2vgen-xl/summary)\n\n计算机视觉：\n\n* [DamoFD 人脸检测关键点模型-0.5G](https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd/summary)\n\n* [BSHM 人像抠图](https://modelscope.cn/models/damo/cv_unet_image-matting/summary)\n\n* [DCT-Net 人像卡通化-3D](https://modelscope.cn/models/damo/cv_unet_person-image-cartoon-3d_compound-models/summary)\n\n* [DCT-Net 人像卡通化模型-3D](https://modelscope.cn/models/damo/face_chain_control_model/summary)\n\n* [读光-文字识别-行识别模型-中英-通用领域](https://modelscope.cn/models/damo/cv_convnextTiny_ocr-recognition-general_damo/summary)\n\n* [读光-文字识别-行识别模型-中英-通用领域](https://modelscope.cn/models/damo/cv_resnet18_ocr-detection-line-level_damo/summary)\n\n* [LaMa 图像填充](https://modelscope.cn/models/damo/cv_fft_inpainting_lama/summary)\n\n语音：\n\n* [Paraformer 语音识别-中文-通用-16k-离线-大型-长音频版本](https://modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)\n\n* [FSMN 声音端点检测-中文-通用-16k-onnx](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-onnx/summary)\n\n* [Monotonic-Aligner 语音时间戳预测-16k-离线](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary)\n\n* [CT-Transformer 标点-中文-通用-onnx](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-onnx/summary)\n\n* [语音合成-中文-多情绪领域-16k-多发言人](https://modelscope.cn/models/damo/speech_sambert-hifigan_tts_zh-cn_16k/summary)\n\n* [CAM++ 说话人验证-中文-通用-200k-发言人](https://modelscope.cn/models/damo/speech_campplus_sv_zh-cn_16k-common/summary)\n\n科学计算：\n\n* [Uni-Fold-Monomer 开源的蛋白质单体结构预测模型](https://modelscope.cn/models/DPTech/uni-fold-monomer/summary)\n\n* [Uni-Fold-Multimer 开源的蛋白质复合物结构预测模型](https://modelscope.cn/models/DPTech/uni-fold-multimer/summary)\n\n# 快速上手\n\n我们针对不同任务提供了统一的使用接口， 使用 `pipeline` 进行模型推理、使用 `Trainer` 进行微调和评估。\n\n对于任意类型输入（图像、文本、音频、视频...）的任何任务，只需 3 行代码即可加载模型并获得推理结果，如下所示：\n\n```python\n>>> from modelscope.pipelines import pipeline\n>>> word_segmentation = pipeline ('word-segmentation',model='damo/nlp_structbert_word-segmentation_chinese-base')\n>>> word_segmentation (' 今天天气不错，适合出去游玩 ')\n{'output': ' 今天 天气 不错 ， 适合 出去 游玩 '}\n```\n\n给定一张图片，你可以使用如下代码进行人像抠图.\n\n![image](data/resource/portrait_input.png)\n\n```python\n>>> import cv2\n>>> from modelscope.pipelines import pipeline\n\n>>> portrait_matting = pipeline ('portrait-matting')\n>>> result = portrait_matting ('https://modelscope.oss-cn-beijing.aliyuncs.com/test/images/image_matting.png')\n>>> cv2.imwrite ('result.png', result ['output_img'])\n```\n\n输出图像如下\n![image](data/resource/portrait_output.png)\n\n对于微调和评估模型， 你需要通过十多行代码构建 dataset 和 trainer，调用 `trainer.train ()` 和 `trainer.evaluate ()` 即可。\n\n例如我们利用 gpt3 1.3B 的模型，加载是诗歌数据集进行 finetune，可以完成古诗生成模型的训练。\n\n```python\n>>> from modelscope.metainfo import Trainers\n>>> from modelscope.msdatasets import MsDataset\n>>> from modelscope.trainers import build_trainer\n\n>>> train_dataset = MsDataset.load ('chinese-poetry-collection', split='train'). remap_columns ({'text1': 'src_txt'})\n>>> eval_dataset = MsDataset.load ('chinese-poetry-collection', split='test').remap_columns ({'text1': 'src_txt'})\n>>> max_epochs = 10\n>>> tmp_dir = './gpt3_poetry'\n\n>>> kwargs = dict (\n     model='damo/nlp_gpt3_text-generation_1.3B',\n     train_dataset=train_dataset,\n     eval_dataset=eval_dataset,\n     max_epochs=max_epochs,\n     work_dir=tmp_dir)\n\n>>> trainer = build_trainer (name=Trainers.gpt3_trainer, default_args=kwargs)\n>>> trainer.train ()\n```\n\n# 为什么要用 ModelScope Library\n\n1. 针对不同任务、不同模型抽象了统一简洁的用户接口，3 行代码完成推理，10 行代码完成模型训练，方便用户使用 ModelScope 社区中多个领域的不同模型，开箱即用，便于 AI 入门和教学。\n\n2. 构造以模型为中心的开发应用体验，支持模型训练、推理、导出部署，方便用户基于 ModelScope Library 构建自己的 MLOps.\n\n3. 针对模型推理、训练流程，进行了模块化的设计，并提供了丰富的功能模块实现，方便用户定制化开发来自定义自己的推理、训练等过程。\n\n4. 针对分布式模型训练，尤其是大模型，提供了丰富的训练策略支持，包括数据并行、模型并行、混合并行等。\n\n# 安装\n\n## 镜像\n\nModelScope Library 目前支持 tensorflow，pytorch 深度学习框架进行模型训练、推理， 在 Python 3.7+, Pytorch 1.8+, Tensorflow1.15/Tensorflow2.0 + 测试可运行。\n\n为了让大家能直接用上 ModelScope 平台上的所有模型，无需配置环境，ModelScope 提供了官方镜像，方便有需要的开发者获取。地址如下：\n\nCPU 镜像\n\n```shell\n# py37\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py37-torch1.11.0-tf1.15.5-1.6.1\n\n# py38\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-py38-torch2.0.1-tf2.13.0-1.9.5\n```\n\nGPU 镜像\n\n```shell\n# py37\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.3.0-py37-torch1.11.0-tf1.15.5-1.6.1\n\n# py38\nregistry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.8.0-py38-torch2.0.1-tf2.13.0-1.9.5\n```\n\n## 搭建本地 Python 环境\n\n你也可以使用 pip 和 conda 搭建本地 python 环境，ModelScope 支持 python3.7 + 以上环境，我们推荐使用 [Anaconda](https://docs.anaconda.com/anaconda/install/)，安装完成后，执行如下命令为 modelscope library 创建对应的 python 环境：\n\n```shell\nconda create -n modelscope python=3.8\nconda activate modelscope\n```\n\n接下来根据所需使用的模型依赖安装底层计算框架\n\n* 安装 Pytorch [文档链接](https://pytorch.org/get-started/locally/)\n* 安装 tensorflow [文档链接](https://www.tensorflow.org/install/pip)\n\n安装完前置依赖，你可以按照如下方式安装 ModelScope Library。\n\nModelScope Libarary 由核心框架，以及不同领域模型的对接组件组成。如果只需要 ModelScope 模型和数据集访问等基础能力，可以只安装 ModelScope 的核心框架：\n\n```shell\npip install modelscope\n```\n\n如仅需体验多模态领域的模型，可执行如下命令安装领域依赖：\n\n```shell\npip install modelscope [multi-modal]\n```\n\n如仅需体验 NLP 领域模型，可执行如下命令安装领域依赖（因部分依赖由 ModelScope 独立 host，所以需要使用 \"-f\" 参数）：\n\n```shell\npip install modelscope [nlp] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\n如仅需体验计算机视觉领域的模型，可执行如下命令安装领域依赖（因部分依赖由 ModelScope 独立 host，所以需要使用 \"-f\" 参数）：\n\n```shell\npip install modelscope [cv] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\n如仅需体验语音领域模型，可执行如下命令安装领域依赖（因部分依赖由 ModelScope 独立 host，所以需要使用 \"-f\" 参数）：\n\n```shell\npip install modelscope [audio] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\n`注意`：当前大部分语音模型需要在 Linux 环境上使用，并且推荐使用 python3.7 + tensorflow 1.x 的组合。\n\n如仅需体验科学计算领域模型，可执行如下命令安装领域依赖（因部分依赖由 ModelScope 独立 host，所以需要使用 \"-f\" 参数）：\n\n```shell\npip install modelscope [science] -f https://modelscope.oss-cn-beijing.aliyuncs.com/releases/repo.html\n```\n\n`注意`:\n\n1. 目前部分语音相关的模型仅支持 python3.7,tensorflow1.15.4 的 Linux 环境使用。 其他绝大部分模型可以在 windows、mac（x86）上安装使用。\n\n2. 语音领域中一部分模型使用了三方库 SoundFile 进行 wav 文件处理，在 Linux 系统上用户需要手动安装 SoundFile 的底层依赖库 libsndfile，在 Windows 和 MacOS 上会自动安装不需要用户操作。详细信息可参考 [SoundFile 官网](https://github.com/bastibe/python-soundfile#installation)。以 Ubuntu 系统为例，用户需要执行如下命令:\n\n    ```shell\n    sudo apt-get update\n    sudo apt-get install libsndfile1\n    ```\n\n3. CV 领域的少数模型，需要安装 mmcv-full， 如果运行过程中提示缺少 mmcv，请参考 mmcv [安装手册](https://github.com/open-mmlab/mmcv#installation) 进行安装。 这里提供一个最简版的 mmcv-full 安装步骤，但是要达到最优的 mmcv-full 的安装效果（包括对于 cuda 版本的兼容），请根据自己的实际机器环境，以 mmcv 官方安装手册为准。\n\n    ```shell\n    pip uninstall mmcv # if you have installed mmcv, uninstall it\n    pip install -U openmim\n    mim install mmcv-full\n    ```\n\n# 更多教程\n\n除了上述内容，我们还提供如下信息：\n\n* [更加详细的安装文档](https://modelscope.cn/docs/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85)\n* [任务的介绍](https://modelscope.cn/docs/%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BB%8B%E7%BB%8D)\n* [模型推理](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86Pipeline)\n* [模型微调](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83Train)\n* [数据预处理](https://modelscope.cn/docs/%E6%95%B0%E6%8D%AE%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86)\n* [模型评估](https://modelscope.cn/docs/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0)\n* [贡献模型到 ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)\n\n# License\n\n本项目使用 [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "modelscope",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0234375,
          "content": "-r requirements/hub.txt\n"
        },
        {
          "name": "requirements",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.638671875,
          "content": "[isort]\nline_length = 79\nmulti_line_output = 0\nknown_standard_library = setuptools\nknown_first_party = modelscope\nknown_third_party = json,yaml\nno_lines_before = STDLIB,LOCALFOLDER\ndefault_section = THIRDPARTY\n\n[yapf]\nBASED_ON_STYLE = pep8\nBLANK_LINE_BEFORE_NESTED_CLASS_OR_DEF = true\nSPLIT_BEFORE_EXPRESSION_AFTER_OPENING_PAREN = true\nSPLIT_BEFORE_ARITHMETIC_OPERATOR = true\n\n[codespell]\nskip = *.ipynb\nquiet-level = 3\nignore-words-list = patten,nd,ty,mot,hist,formating,winn,gool,datas,wan,confids\n\n[flake8]\nmax-line-length = 120\nselect = B,C,E,F,P,T4,W,B9\nignore = F401,F403,F405,F821,W503,E251\nexclude = docs/src,*.pyi,.git\n\n[darglint]\nignore=DAR101\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 8.3583984375,
          "content": "# Copyright (c) Alibaba, Inc. and its affiliates.\n# !/usr/bin/env python\nimport os\nimport shutil\nimport subprocess\nfrom setuptools import find_packages, setup\n\nfrom modelscope.utils.constant import Fields\n\n\ndef readme():\n    with open('README.md', encoding='utf-8') as f:\n        content = f.read()\n    return content\n\n\nversion_file = 'modelscope/version.py'\n\n\ndef get_git_hash():\n\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in ['SYSTEMROOT', 'PATH', 'HOME']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env['LANGUAGE'] = 'C'\n        env['LANG'] = 'C'\n        env['LC_ALL'] = 'C'\n        out = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, env=env).communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd(['git', 'rev-parse', 'HEAD'])\n        sha = out.strip().decode('ascii')\n    except OSError:\n        sha = 'unknown'\n\n    return sha\n\n\ndef get_hash():\n    assert os.path.exists('.git'), '.git directory does not exist'\n    sha = get_git_hash()[:7]\n    return sha\n\n\ndef get_version():\n    with open(version_file, 'r', encoding='utf-8') as f:\n        exec(compile(f.read(), version_file, 'exec'))\n    return locals()['__version__']\n\n\ndef parse_requirements(fname='requirements.txt', with_version=True):\n    \"\"\"\n    Parse the package dependencies listed in a requirements file but strips\n    specific versioning information.\n\n    Args:\n        fname (str): path to requirements file\n        with_version (bool, default=False): if True include version specs\n\n    Returns:\n        List[str]: list of requirements items\n\n    CommandLine:\n        python -c \"import setup; print(setup.parse_requirements())\"\n    \"\"\"\n    import re\n    import sys\n    from os.path import exists\n    require_fpath = fname\n\n    def parse_line(line):\n        \"\"\"\n        Parse information from a line in a requirements text file\n        \"\"\"\n        if line.startswith('-r '):\n            # Allow specifying requirements in other files\n            target = line.split(' ')[1]\n            relative_base = os.path.dirname(fname)\n            absolute_target = os.path.join(relative_base, target)\n            for info in parse_require_file(absolute_target):\n                yield info\n        else:\n            info = {'line': line}\n            if line.startswith('-e '):\n                info['package'] = line.split('#egg=')[1]\n            else:\n                # Remove versioning from the package\n                pat = '(' + '|'.join(['>=', '==', '>']) + ')'\n                parts = re.split(pat, line, maxsplit=1)\n                parts = [p.strip() for p in parts]\n\n                info['package'] = parts[0]\n                if len(parts) > 1:\n                    op, rest = parts[1:]\n                    if ';' in rest:\n                        # Handle platform specific dependencies\n                        # http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependencies\n                        version, platform_deps = map(str.strip,\n                                                     rest.split(';'))\n                        info['platform_deps'] = platform_deps\n                    else:\n                        version = rest  # NOQA\n                    info['version'] = (op, version)\n            yield info\n\n    def parse_require_file(fpath):\n        with open(fpath, 'r', encoding='utf-8') as f:\n            for line in f.readlines():\n                line = line.strip()\n                if line.startswith('http'):\n                    print('skip http requirements %s' % line)\n                    continue\n                if line and not line.startswith('#') and not line.startswith(\n                        '--'):\n                    for info in parse_line(line):\n                        yield info\n                elif line and line.startswith('--find-links'):\n                    eles = line.split()\n                    for e in eles:\n                        e = e.strip()\n                        if 'http' in e:\n                            info = dict(dependency_links=e)\n                            yield info\n\n    def gen_packages_items():\n        items = []\n        deps_link = []\n        if exists(require_fpath):\n            for info in parse_require_file(require_fpath):\n                if 'dependency_links' not in info:\n                    parts = [info['package']]\n                    if with_version and 'version' in info:\n                        parts.extend(info['version'])\n                    if not sys.version.startswith('3.4'):\n                        # apparently package_deps are broken in 3.4\n                        platform_deps = info.get('platform_deps')\n                        if platform_deps is not None:\n                            parts.append(';' + platform_deps)\n                    item = ''.join(parts)\n                    items.append(item)\n                else:\n                    deps_link.append(info['dependency_links'])\n        return items, deps_link\n\n    return gen_packages_items()\n\n\ndef pack_resource():\n    # pack resource such as configs and tools\n    root_dir = 'package/'\n    if os.path.isdir(root_dir):\n        shutil.rmtree(root_dir)\n    os.makedirs(root_dir)\n\n    proj_dir = root_dir + 'modelscope/'\n    shutil.copytree('./modelscope', proj_dir)\n    shutil.copytree('./configs', proj_dir + 'configs')\n    shutil.copytree('./requirements', 'package/requirements')\n    shutil.copy('./requirements.txt', 'package/requirements.txt')\n    shutil.copy('./MANIFEST.in', 'package/MANIFEST.in')\n    shutil.copy('./README.md', 'package/README.md')\n\n\nif __name__ == '__main__':\n    # write_version_py()\n    from modelscope.utils.ast_utils import generate_ast_template\n    generate_ast_template()\n    pack_resource()\n    os.chdir('package')\n    install_requires, deps_link = parse_requirements('requirements.txt')\n    extra_requires = {}\n    all_requires = []\n    for field in dir(Fields):\n        if field.startswith('_'):\n            continue\n        field = getattr(Fields, field)\n        extra_requires[field], _ = parse_requirements(\n            f'requirements/{field}.txt')\n\n        # skip audio requirements due to its hard dependency which\n        # result in mac/windows compatibility problems\n        if field != Fields.audio:\n            all_requires.append(extra_requires[field])\n    for subfiled in ['asr', 'kws', 'signal', 'tts']:\n        filed_name = f'audio_{subfiled}'\n        extra_requires[filed_name], _ = parse_requirements(\n            f'requirements/audio/{filed_name}.txt')\n    framework_requires = extra_requires['framework']\n    # add framework dependencies to every field\n    for field, requires in extra_requires.items():\n        if field not in [\n                'server', 'framework', 'hub', 'datasets'\n        ]:  # server need install model's field dependencies before.\n            extra_requires[field] = framework_requires + extra_requires[field]\n    extra_requires['all'] = all_requires\n\n    setup(\n        name='modelscope',\n        version=get_version(),\n        description=\n        'ModelScope: bring the notion of Model-as-a-Service to life.',\n        long_description=readme(),\n        long_description_content_type='text/markdown',\n        author='ModelScope team',\n        author_email='contact@modelscope.cn',\n        keywords='python,nlp,science,cv,speech,multi-modal',\n        url='https://github.com/modelscope/modelscope',\n        packages=find_packages(exclude=('configs', 'demo')),\n        include_package_data=True,\n        package_data={\n            '': ['*.h', '*.cpp', '*.cu'],\n        },\n        classifiers=[\n            'Development Status :: 4 - Beta',\n            'License :: OSI Approved :: Apache Software License',\n            'Operating System :: OS Independent',\n            'Programming Language :: Python :: 3',\n            'Programming Language :: Python :: 3.7',\n            'Programming Language :: Python :: 3.8',\n            'Programming Language :: Python :: 3.9',\n            'Programming Language :: Python :: 3.10',\n            'Programming Language :: Python :: 3.11',\n        ],\n        license='Apache License 2.0',\n        tests_require=parse_requirements('requirements/tests.txt'),\n        install_requires=install_requires,\n        extras_require=extra_requires,\n        entry_points={\n            'console_scripts': ['modelscope=modelscope.cli.cli:run_cmd']\n        },\n        dependency_links=deps_link,\n        zip_safe=False)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}