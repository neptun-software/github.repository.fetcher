{
  "metadata": {
    "timestamp": 1736560916161,
    "page": 644,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "pytorch-labs/gpt-fast",
      "stars": 5751,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1689453125,
          "content": "__pycache__\n.idea\n.DS_Store\n*.egg-info\nbuild\n\n# data\ndata\ncheckpoints\nout\n!data/shakespeare/prepare.py\nwandb\n\n# downloaded by our tests\noriginal_model.py\noriginal_adapter.py"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.263671875,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <conduct@pytorch.org>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.216796875,
          "content": "# Contributing to gpt-fast\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `main`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Meta's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nMeta has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## License\nBy contributing to `gpt-fast`, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "GPTQ.py",
          "type": "blob",
          "size": 15.474609375,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nimport torch.fx as fx\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils._pytree import tree_flatten, tree_unflatten\n\naten = torch.ops.aten\n\nfrom eval import (\n    setup_cache_padded_seq_input_pos_max_seq_length_for_prefill,\n    GPTFastEvalWrapper\n)\n\n\nclass InputRecorder(GPTFastEvalWrapper):\n    \"\"\"\n    This is a fake evaluation wrapper that just records the inputs\n    so that they can be used in calibration.\n\n    If pad_calibration_inputs is enabled, the input recorder will take\n    each input and pad/truncate it down to the calibration_seq_length.\n    It will also edit the model embeddings to be zero for the 0 token used\n    in padding and avoid any inputs with the 0 token.\n\n    If not, it will only truncate inputs to the desired length.\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        tokenizer,\n        calibration_seq_length,\n        pad_calibration_inputs=False,\n    ):\n        super().__init__(model, tokenizer, calibration_seq_length)\n        self._model = model\n        self._tokenizer = tokenizer\n        self._device = torch.device(\"cpu\")\n        self.vocab_size = model.config.vocab_size\n        self.calibration_seq_length = calibration_seq_length\n        self.pad_calibration_inputs = pad_calibration_inputs\n        self.inputs = None\n\n        if self.pad_calibration_inputs:\n            # This is needed for the pad_calibration_inputs option\n            # to work properly, the 0 token's embeddings are set to 0 so that\n            # the padded inputs will not affect the model numerics. This token isn't used\n            # commonly in the eval tasks for the meta-llama tokenizer and we skip any inputs\n            # where it appears\n            try:\n                if isinstance(self._model.transformer.wte, nn.Embedding):\n                    self.mod.transformer.wte.weight.data[0, :] *= 0\n            except:\n                print(\n                    \"Did not find embeddings in model.transformer.wte, disabling padding\"\n                )\n                self.pad_calibration_inputs = False\n\n\n    def add_input(self, args):\n        if self.inputs is None:\n            self.inputs = [MultiInput([arg]) for arg in args]\n        else:\n            self.inputs = [\n                multi.add_input(arg) for (multi, arg) in zip(self.inputs, args)\n            ]\n\n    def get_recorded_inputs(self):\n        return self.inputs\n\n    def _model_call(self, inps):\n        inps = inps.squeeze(0)\n        T = len(inps)\n        if (\n            # can't use inputs that are too short when padding disabled\n            (T < self.calibration_seq_length and not self.pad_calibration_inputs)\n            or\n            # can't use inputs that actually use token we use for padding\n            (self.pad_calibration_inputs and 0 in inps)\n        ):\n            # give random output\n            return torch.randn(\n                (1, T, self.vocab_size), dtype=torch.bfloat16, device=self._device\n            )\n\n        # pad or truncate to the right size\n        if T >= self.calibration_seq_length:\n            inps = inps[: self.calibration_seq_length]\n        else:\n            inps = F.pad(inps, (0, self.calibration_seq_length - T))\n\n        max_new_tokens = 1\n        (\n            seq,\n            input_pos,\n            max_seq_length,\n        ) = setup_cache_padded_seq_input_pos_max_seq_length_for_prefill(\n            self._model, inps, max_new_tokens, self.max_length\n        )\n        x = seq.index_select(0, input_pos).view(1, -1)\n        self.add_input((x, input_pos))\n\n        # output `something` with correct shape to keep eval going\n        return torch.randn(\n            (1, T, self.vocab_size), dtype=torch.bfloat16, device=self._device\n        )\n\n\n\nclass MultiInput:\n    def __init__(self, inputs):\n        self.values = list(inputs)\n\n    def add_input(self, input):\n        self.values.append(input)\n        return self\n\n    def __getitem__(self, slice):\n        return MultiInput(self.values[slice])\n\n    def cuda(self):\n        self.values = [val.cuda() if isinstance(val, torch.Tensor) else val for val in self.values]\n\n\nclass GenericGPTQRunner(fx.Interpreter):\n    \"\"\"\n    This is a generic GPTQ runner that takes an existing model and applies GPTQ.\n    It uses torch._dynamo.export to obtain a graph of the model and then hooks\n    into function calls and when it detects a linear, it applies GPTQ to the weight\n    given the calibration of inputs passed in at initialization. It puts the results\n    into the state_dict so that the quantized model weights/qparams can be loaded\n    directly into the model.\n\n    This class is expected to work in concert with a GPTQSimpleQuantizer\n    class to define the specific type of quantization being done.\n    \"\"\"\n\n    def __init__(\n        self, model, inputs: MultiInput, blocksize=128, percdamp=0.01, groupsize=128\n    ):\n        self.id_to_name = {\n            id(value): name for name, value in dict(model.named_parameters()).items()\n        }\n\n        # trace model for one input\n        one_input = [multi.values[0].cpu() for multi in inputs]\n        exported_model = torch._dynamo.export(\n            model.cpu(), aten_graph=True, pre_dispatch=True, tracing_mode=\"fake\"\n        )(*one_input)\n        super().__init__(exported_model.graph_module)\n        self.new_state_dict = model.state_dict()\n        self.blocksize = blocksize\n        self.percdamp = percdamp\n        self.groupsize = groupsize\n        self.inputs = inputs\n        self.gptq_done = False\n        self.debug = False\n\n    def configure_quantization_mode(\n        self,\n        get_qparams_func,\n        quantize_func,\n        dequantize_func,\n        combine_qparams_list_func,\n        make_names_and_values_dict_func,\n        skip_layer_func,\n    ):\n        # these functions need to already be curried with all inputs other than weight, qparams\n        self.get_qparams_func = (\n            get_qparams_func  # accepts [2d weight tensor], outputs qparams.\n        )\n\n        self.quantize_func = quantize_func  # accepts [2d weight tensor], [qparams], outputs a 2d quantized tensor of desired dtype\n\n        self.dequantize_func = dequantize_func\n        # accepts [quantized] tensor and [qparams], outputs a 2d dequantized tensor of type float,\n        # assumes this output .to(w_orig_dtype) is ~eventual desired dequant behavior\n\n        self.combine_qparams_list_func = combine_qparams_list_func\n        # accepts [`list` of qparams] from quantizing one group at a time,\n        # outputs a qparams object that could be passed into quant/dequantize_func\n\n        self.skip_layer_func = skip_layer_func  # accepts [weight tensor], outputs a bool on whether or not to apply gptq to this layer\n\n        self.make_names_and_values_dict_func = make_names_and_values_dict_func  # accepts [2d quantized tensor], [qparams], returns a dict of names, values to put in state_dict\n        # note any final packing for storage should happen here\n        return self\n\n    def run(self):\n        assert (\n            self.get_qparams_func is not None\n        ), \"need to configure quantization mode before running\"\n        self.gptq_done = True\n        super().run(*self.inputs)\n\n    def get_quantized_state_dict(self):\n        assert (\n            self.gptq_done\n        ), \"need to run GPTQRunner before you can get_quantized_state_dict\"\n        quantized_state_dict = self.new_state_dict\n        # Don't want to store/load the kv_cache so remove it from the state_dict\n        del_list = []\n        for param_fqn in quantized_state_dict:\n            if \"kv_cache\" in param_fqn:\n                del_list.append(param_fqn)\n        for param_fqn in del_list:\n            quantized_state_dict.pop(param_fqn)\n        return quantized_state_dict\n\n    def call_function(self, target, args, kwargs, skip_quant=False):\n        def tensors_to_cuda(args):\n            new_args = []\n            for x in args:\n                new_args.append(x.cuda() if isinstance(x, torch.Tensor) else x)\n            return new_args\n\n        # flatten args and kwargs together\n        flat_args, spec = tree_flatten((args, kwargs))\n        # move all single tensors to cuda, will move MultiInputs to cuda one at a time\n        flat_args = tensors_to_cuda(flat_args)\n\n        has_multi_input = MultiInput in [type(x) for x in flat_args]\n        if has_multi_input:\n            # Just some trickery to convert\n            # [MultiInput[a, a, a], MultiInput(b, b, b)] => [a, b], [a, b], [a, b]\n            multi_input_count = max(\n                [len(x.values) if isinstance(x, MultiInput) else 1 for x in flat_args]\n            )\n            transposed_args = list(\n                zip(\n                    *[x.values if isinstance(x, MultiInput) else [x] * multi_input_count for x in flat_args]\n                )\n            )\n        else:\n            transposed_args = [flat_args]\n        outputs = []\n\n        # check whether we apply GPTQ to this module\n        quantize_linear = (\n            (target == aten.linear.default) # if its a linear\n            and id(args[1]) in self.id_to_name # and if we know the layer name\n            and not skip_quant  # and if we weren't told to skip quantization\n            # and if the skip_layer_func doesn't say we should skip\n            and not (self.skip_layer_func is not None and self.skip_layer_func(args[1]))\n        )  # then we will quantize this linear layer/weight\n\n        if quantize_linear:  # instantiate variables for GPTQ\n            H = 0\n            total_batches = 0\n\n        for inp in transposed_args:\n            inp = tensors_to_cuda(inp)\n            cur_args, cur_kwargs = tree_unflatten(inp, spec)\n\n            if (\n                quantize_linear\n            ):  # calculate H instead of output (will run the linear eventually with updated weight)\n                x = cur_args[0].float()\n                shape = x.shape\n                n = 1 if len(shape) == 2 else shape[0]\n                H *= total_batches / (total_batches + n)\n                total_batches += n\n                x = ((2 / total_batches) ** (1 / 2)) * x.reshape(\n                    -1, shape[-1]\n                ).t().float()\n                H += x.matmul(x.t())\n            else:\n                # get output if its not a linear\n                out = super().call_function(target, cur_args, cur_kwargs)\n\n                if isinstance(out, torch.Tensor):\n                    outputs.append(out.cpu())\n                else:\n                    outputs.append(out)\n\n        if quantize_linear:\n            mod_fqn = \".\".join(self.id_to_name[id(args[1])].split(\".\")[:-1])\n            W = args[1].to(H.device)\n            Q, DQ, qparams = self.faster_quant(H, W.detach())\n            print(mod_fqn)\n            names_and_values_dict = self.make_names_and_values_dict_func(Q, qparams)\n\n            # delete old weight\n            if mod_fqn + \".weight\" in self.new_state_dict:\n                self.new_state_dict.pop(mod_fqn + \".weight\")\n            if len(args) > 2:\n                self.new_state_dict[mod_fqn + \".bias\"] = args[2]\n            for name, value in names_and_values_dict.items():\n                self.new_state_dict[mod_fqn + \".\" + name] = value\n\n            # run linear with new weight to get corrected output\n            new_out = self.call_function(\n                target, (args[0], DQ, *args[2:]), kwargs, skip_quant=True\n            )\n\n            if self.debug:\n                old_out = self.call_function(\n                    target, (args[0][:2], args[1], *args[2:]), kwargs, skip_quant=True\n                )\n\n                def SQNR(x, y):\n                    return 20 * torch.log10(torch.norm(x) / torch.norm(x - y))\n\n                DQ_after = self.dequantize_func(Q, qparams).to(W.dtype)\n                print(\n                    \"SQNR for QDQ (this should be inf)\", SQNR(DQ, DQ_after)\n                )  # matches\n\n                print(\n                    \"SQNR for weight (can be low)\", SQNR(W, DQ.cuda())\n                )  # fine to not match\n                print(\n                    \"SQNR for output with GPTQ (hopefully 35+)\",\n                    torch.cat(\n                        [\n                            SQNR(old.cpu(), new.cpu()).unsqueeze(0)\n                            for (old, new) in zip(old_out.values, new_out.values[:2])\n                        ]\n                    ).mean(),\n                )\n\n                qparams2 = self.get_qparams_func(W)\n                Q2 = self.quantize_func(W, qparams2)\n                DQ2 = self.dequantize_func(Q2, qparams2).to(W.dtype)\n                old_q_out = self.call_function(\n                    target, (args[0][:2], DQ2, *args[2:]), kwargs, skip_quant=True\n                )\n\n                print(\"SQNR for output without GPTQ (should be less than above)\",\n                    torch.cat([\n                            SQNR(old.cpu(), old_q.cpu()).unsqueeze(0)\n                            for (old, old_q) in zip(old_out.values, old_q_out.values)\n                    ]).mean(),\n                )\n            return new_out\n\n        return MultiInput(outputs) if has_multi_input else outputs[0]\n\n    def faster_quant(self, H, W):\n        percdamp = self.percdamp\n        blocksize = self.blocksize\n        groupsize = self.groupsize\n        orig_dtype = W.dtype\n        W = W.detach().float()\n        rows, columns = W.shape[0], W.shape[1]\n        device = W.device\n\n        if groupsize == -1:\n            cur_qparams = self.get_qparams_func(W)\n        dead = torch.diag(H) == 0\n        H[dead, dead] = 1\n        W[:, dead] = 0\n\n        Losses = torch.zeros_like(W)\n        DQ = torch.zeros_like(W)\n\n        damp = percdamp * torch.mean(torch.diag(H))\n        diag = torch.arange(columns, device=device)\n        H[diag, diag] += damp\n        H = torch.linalg.cholesky(H)\n        H = torch.cholesky_inverse(H)\n        H = torch.linalg.cholesky(H, upper=True)\n        Hinv = H\n\n        all_qparams = []\n        for i1 in range(0, columns, blocksize):\n            i2 = min(i1 + blocksize, columns)\n            count = i2 - i1\n            W1 = W[:, i1:i2].clone()\n            DQ1 = torch.zeros_like(W1)\n            Err1 = torch.zeros_like(W1)\n            Losses1 = torch.zeros_like(W1)\n            Hinv1 = Hinv[i1:i2, i1:i2]\n            for i in range(count):\n                w = W1[:, i]\n                d = Hinv1[i, i]\n\n                if groupsize != -1 and (i1 + i) % groupsize == 0:  # start of new group\n                    cur_qparams = self.get_qparams_func(\n                        W[:, (i1 + i) : (i1 + i + groupsize)]\n                    )\n                    all_qparams.append(cur_qparams)\n\n                q = self.quantize_func(w.unsqueeze(1), cur_qparams).flatten()\n                dq = self.dequantize_func(q.unsqueeze(1), cur_qparams).flatten()\n\n                DQ1[:, i] = dq\n                Losses1[:, i] = (w - dq) ** 2 / d**2\n\n                err1 = (w - dq) / d\n                W1[:, i:] -= (\n                    err1.to(Hinv1.dtype).unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n                )\n                Err1[:, i] = err1\n\n            DQ[:, i1:i2] = DQ1\n            Losses[:, i1:i2] = Losses1 / 2\n\n            W[:, i2:] -= Err1.to(Hinv.dtype).matmul(Hinv[i1:i2, i2:])\n\n        torch.cuda.synchronize()\n\n        if all_qparams == []:\n            all_qparams.append(cur_qparams)\n\n        # convert a list of qparams objects into a single one. enerally by\n        # concatenating a bunch of n,1 scale/zeros tensors into a n,num_groups tensor\n        all_qparams = self.combine_qparams_list_func(all_qparams)\n        Q = self.quantize_func(DQ, all_qparams)\n        return Q, DQ.to(orig_dtype), all_qparams\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.4189453125,
          "content": "Copyright 2023 Meta\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.232421875,
          "content": "# gpt-fast\nSimple and efficient pytorch-native transformer text generation.\n\nFeaturing:\n1. Very low latency\n2. <1000 lines of python\n3. No dependencies other than PyTorch and sentencepiece\n4. int8/int4 quantization\n5. Speculative decoding\n6. Tensor parallelism\n7. Supports Nvidia and AMD GPUs\n\nThis is *NOT* intended to be a \"framework\" or \"library\" - it is intended to show off what kind of performance you can get with native PyTorch :) Please copy-paste and fork as you desire.\n\nFor an in-depth walkthrough of what's in this codebase, see this [blog post](https://pytorch.org/blog/accelerating-generative-ai-2/).\n\n## Supported Models\n\n### LLaMA family\nPlease check the rest of this page about benchmark of LLaMA family models.\n\n### Mixtral 8x7B\nWe also supported [Mixtral 8x7B](https://mistral.ai/news/mixtral-of-experts/) which is a high-quality sparse mixture of experts (MoE) model, the average token generation rates are:\n\n|                  |   1 GPU |    2 GPU  | 4 GPU  |    8 GPU   |\n|------------------|---------|-----------|--------|------------|\n|baseline(bfloat16)|    OOM  |    96.67  | 155.35 |  227.82    |\n|        int8      |   97.92 |   155.03  | 216.87 |  279.35    |\n\nNote that the benchmarks run on an 8xA100-80GB, power limited to 330W with a hybrid cube mesh topology. Note that all benchmarks are run at *batch size=1*, making the reported tokens/s numbers equivalent to \"tokens/s/user\". In addition, they are run with a very small prompt length (just 5 tokens).\n\nFor more details about Mixtral 8x7B, please check [this page](./mixtral-moe) or this [note](https://thonking.substack.com/p/short-supporting-mixtral-in-gpt-fast).\n\n## Examples\nIn the spirit of keeping the repo minimal, here are various examples of extensions you can make to gpt-fast as PRs.\n- [Google Gemma](https://github.com/pytorch-labs/gpt-fast/pull/115)\n- [xAI Grok-1](https://github.com/pytorch-labs/gpt-fast/pull/171)\n- [Databricks DBRX](https://github.com/pytorch-labs/gpt-fast/pull/174)\n\n## Community\n\nProjects inspired by gpt-fast in the community:\n\n- [gpt-blazing](https://github.com/armed-gpt/gpt-blazing): applies the same performance optimization strategy to more models (e.g., baichuan2).\n- [gptfast](https://github.com/MDK8888/GPTFast): applies a subset of the performance optimizations to all Huggingface models\n- [gpt-accelera](https://github.com/Edward-Sun/gpt-accelera): extends `gpt-fast` to SFT/RM/PPO training and batched inference to optimize the throughput\n\n## Installation\n[Download PyTorch nightly](https://pytorch.org/get-started/locally/)\n\nInstall required packages:\n\n```bash\npip install -r requirements.txt\n```\n\nTo download llama models, go to https://huggingface.co/meta-llama/Llama-2-7b and go through steps to obtain access.\nThen login with `huggingface-cli login`\n\n\n\n## Downloading Weights\nModels tested/supported\n```text\ntinyllamas/stories{15,42,100}\nopenlm-research/open_llama_7b\nmeta-llama/Llama-2-7b-chat-hf\nmeta-llama/Llama-2-13b-chat-hf\nmeta-llama/Llama-2-70b-chat-hf\ncodellama/CodeLlama-7b-Python-hf\ncodellama/CodeLlama-34b-Python-hf\nmistralai/Mistral-7B-v0.1\nmistralai/Mistral-7B-Instruct-v0.1\nmistralai/Mistral-7B-Instruct-v0.2\nmeta-llama/Meta-Llama-3-8B\nmeta-llama/Meta-Llama-3.1-8B\nmeta-llama/Meta-Llama-3.1-70B\nmeta-llama/Meta-Llama-3.1-405B\n```\n\nFor example, to convert Llama-2-7b-chat-hf\n```bash\nexport MODEL_REPO=meta-llama/Llama-2-7b-chat-hf\n./scripts/prepare.sh $MODEL_REPO\n```\n\n## Benchmarks\nBenchmarks run on an 8xA100-80GB, power limited to 330W with a hybrid cube mesh topology. Note that all benchmarks are run at *batch size=1*, making the reported tokens/s numbers equivalent to \"tokens/s/user\". In addition, they are run with a very small prompt length (just 5 tokens).\n\n| Model    | Technique | Tokens/Second | Memory Bandwidth (GB/s) |\n| -------- | ------- | ------ | ------ |\n| Llama-2-7B  | Base    |  104.9  | 1397.31 |\n|           | 8-bit   | 155.58   | 1069.20 |\n|           | 4-bit (G=32)   | 196.80   | 862.69 |\n| Llama-2-70B | Base    | OOM     ||\n|           | 8-bit   | 19.13    | 1322.58 |\n|           | 4-bit (G=32)   | 25.25    | 1097.66 |\n| Llama-3.1-8B  | Base    |  93.89  | 1410.76 |\n|           | 8-bit   | 137.64   | 1030.89 |\n| Llama-3.1-70B | Base    | OOM     ||\n|           | 8-bit   | 18.04    | 1253.78 |\n\n### Speculative Sampling\n[Verifier: Llama-70B (int4), Draft: Llama-7B (int4)](./scripts/speculate_70B_int4.sh): 48.4 tok/s\n\n### Tensor Parallelism\n| Model    | Number of GPUs | Tokens/Second | Memory Bandwidth (GB/s) |\n| -------- | ------- | ------ | ------ |\n| Llama-2-7B  | 1    |  104.9  | 1397.31 |\n|           | 2   | 168.84   | 1181.99 |\n|           | 4   | 254.02   | 955.83 |\n|           | 8   | 328.43   | 704.10 |\n| Llama-2-70B  | 1    |  OOM  |  |\n|           | 2   | 21.32   | 1481.87 |\n|           | 4   | 38.01   | 1340.76 |\n|           | 8   | 62.50   | 1135.29 |\n| Llama-3.1-8B  | 1    |  93.83  | 1408.37 |\n|           | 2   | 149.10   | 1197.32 |\n|           | 4   | 217.21   | 986.32  |\n|           | 8   | 276.01   | 772.60 |\n| Llama-3.1-70B  | 1    |  OOM  |  |\n|           | 2   | 16.03   | 1130.81 |\n|           | 4   | 37.45   | 1360.53 |\n|           | 8   | 58.78   | 1129.61 |\n\n### Tensor Parallelism + Quantization\n| Model    | Technique | Tokens/Second | Memory Bandwidth (GB/s) |\n| -------- | ------- | ------ | ------ |\n| Llama-2-70B | Base    | 62.50     | 1135.29 |\n|           | 8-bit   | 80.44    | 752.04 |\n|           | 4-bit (G=32)   | 90.77    | 548.10 |\n| Llama-3.1-70B | Base    | 58.78     | 1129.61 |\n|           | 8-bit   | 75.58    | 726.57 |\n| Llama-3.1-405B | 8-bit | 15.60 | 815.87 |\n\n### AMD\nBenchmarks run on one GCD of a MI-250x.\n\n| Model    | Technique | Tokens/Second | Memory Bandwidth (GB/s) |\n| -------- | ------- | ------ | ------ |\n| Llama-2-7B  | Base    |  76.33  | 1028.70 |\n|           | 8-bit   | 101.86   | 700.06 |\n\n## Generate Text\n\nModel definition in `model.py`, generation code in `generate.py`.\n\n```bash\npython generate.py --compile --checkpoint_path checkpoints/$MODEL_REPO/model.pth --prompt \"Hello, my name is\"\n```\n\nTo squeeze out a little bit more performance, you can also compile the prefill with `--compile_prefill`. This will increase compilation times though.\n\n## Quantization\nChoose device to use by\n```bash\n# The current support devices: cuda, cpu\nexport DEVICE=cuda\n```\n### Int8 Weight-Only Quantization\nTo generate this version of the model\n```bash\n# Spits out model at checkpoints/$MODEL_REPO/model_int8.pth\npython quantize.py --checkpoint_path checkpoints/$MODEL_REPO/model.pth --mode int8\n```\nTo run with int8, just pass the int8 checkpoint to generate.py.\n```bash\npython generate.py --compile --checkpoint_path checkpoints/$MODEL_REPO/model_int8.pth --device $DEVICE\n```\n\n### Int4 Weight-Only Quantization\nTo generate int4 version of model\n```bash\n# Spits out model at checkpoints/$MODEL_REPO/model_int4.g32.$DEVICE.pth\npython quantize.py --checkpoint_path checkpoints/$MODEL_REPO/model.pth --mode int4 --groupsize 32\n```\n\nTo run with int4, just pass the int4 checkpoint to generate.py.\n```bash\npython generate.py --checkpoint_path checkpoints/$MODEL_REPO/model_int4.g32.pth --compile\n```\n\n## Speculative Sampling\nTo generate with speculative sampling (DRAFT_MODEL_REPO should point to a smaller model compared with MODEL_REPO).\n\nIn this example, the \"smaller\" model is just the int8 quantized version of the model.\n```\nexport DRAFT_MODEL_REPO=meta-llama/Llama-2-7b-chat-hf\npython generate.py --compile --checkpoint_path checkpoints/$MODEL_REPO/model.pth --draft_checkpoint_path checkpoints/$DRAFT_MODEL_REPO/model_int8.pth\n```\n\nNote: Running on an A100 80GB, albeit power-limited to 330 watts. Empirically, seems like peak bandwidth is about 1700 GB/s.\n\n\n## Tensor Parallelism\n```bash\nENABLE_INTRA_NODE_COMM=1 torchrun --standalone --nproc_per_node=2 generate.py --compile --checkpoint_path checkpoints/$MODEL_REPO/model.pth\n```\n\n## Experimental\n### Evaluation\nWe use the EleutherAI evaluation harness to evaluate our model accuracy. To evaluate the accuracy, make sure the evaluation harness is installed and pass your model checkpoint and desired tasks to eval.py.\n\n```bash\npython eval.py --checkpoint_path checkpoints/$MODEL_REPO/model.pth --compile --tasks hellaswag winogrande\n```\n\nNote: Generative tasks are currently not supported for gpt-fast\n\nInstallation Instructions for the evaluation harness: https://github.com/EleutherAI/lm-evaluation-harness/tree/master#install\n\n### GPTQ\nWe have a pure pytorch implementation of GPTQ that utilizes torch._dynamo.export to access the model structure. You can generate a GPTQ quantized\nversion of int4 quantization by using the same command to quantize it but adding 'gptq' to the quantization mode i.e.\n```bash\n# Spits out model at checkpoints/$MODEL_REPO/model_int4-gptq.g32.pth\npython quantize.py --mode int4-gptq --calibration_tasks wikitext --calibration_seq_length 2048\n```\n\nYou can then eval or generate text with this model in the same way as above.\n\n## License\n\n`gpt-fast` is released under the [BSD 3](https://github.com/pytorch-labs/gpt-fast/main/LICENSE) license.\n\n## Acknowledgements\nThanks to:\n* Lightning AI for supporting pytorch and work in flash attention, int8 quantization, and LoRA fine-tuning.\n* GGML for driving forward fast, on device inference of LLMs\n* Karpathy for spearheading simple, interpretable and fast LLM implementations\n* MLC-LLM for pushing 4-bit quantization performance on heterogeneous hardware\n"
        },
        {
          "name": "eval.py",
          "type": "blob",
          "size": 8.587890625,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Optional\n\nimport torch\nimport torch._dynamo.config\nimport torch._inductor.config\n\ntorch._dynamo.config.automatic_dynamic_shapes = True\ntorch._inductor.config.triton.unique_kernel_names = True\ntorch._inductor.config.epilogue_fusion = False\ntorch._dynamo.config.cache_size_limit = 100000\n\nfrom tokenizer import get_tokenizer\n\nfrom model import Transformer\n\ntry:\n    import lm_eval\n    lm_eval_available = True\nexcept:\n    lm_eval_available = False\n\nfrom generate import _load_model, encode_tokens, model_forward\n\nif lm_eval_available:\n    try: # lm_eval version 0.4\n        from lm_eval.models.huggingface import HFLM as eval_wrapper\n        from lm_eval.tasks import get_task_dict\n        from lm_eval.evaluator import evaluate\n    except: #lm_eval version 0.3\n        from lm_eval import base\n        from lm_eval import tasks\n        from lm_eval import evaluator\n        eval_wrapper=base.BaseLM\n        get_task_dict=tasks.get_task_dict\n        evaluate=evaluator.evaluate\n\n\ndef setup_cache_padded_seq_input_pos_max_seq_length_for_prefill(\n    model: Transformer,\n    prompt: torch.Tensor,\n    max_new_tokens: int,\n    max_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Sets up model cache and does some bookkeeping calculations for prompt, input_pos and max_seq_length\n    that are needed for prefill or model_forward\n\n    Args:\n        model (LLaMA): The model whose cache gets set up\n        prompt (torch.Tensor): Tensor of shape (T) with indices of the prompt sequence.\n        max_new_tokens (int): The desired maximum number of new tokens that can be generated.\n        max_seq_length (Optional[int], optional): The maximum sequence length allowed.\n\n    Returns:\n        seq (torch.Tensor): prompt but padded with zeros to size max_seq_length\n        input_pos (torch.Tensor): tensor of integers in increasing order\n        max_seq_length (int): The maximum sequence length allowed, updated based on other numbers\n    \"\"\"\n    T = prompt.size(0)\n    T_new = T + max_new_tokens\n    if max_seq_length is None:\n        max_seq_length = min(T_new, model.config.block_size)\n\n    device, dtype = prompt.device, prompt.dtype\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    empty = torch.empty(T_new, dtype=dtype, device=device)\n    empty[:T] = prompt\n    seq = empty\n    input_pos = torch.arange(0, T, device=device)\n\n    with torch.device(device):\n        model.setup_caches(max_batch_size=1, max_seq_length=max_seq_length)\n\n    return seq, input_pos, max_seq_length\n\nclass GPTFastEvalWrapper(eval_wrapper):\n    \"\"\"\n    A wrapper class for GPTFast, providing integration with the lm-evaluation-harness library.\n    \"\"\"\n    def __init__(\n        self,\n        model: Transformer,\n        tokenizer,\n        max_seq_length: Optional[int]=None,\n    ):\n        super().__init__()\n        self._model = model\n        self._tokenizer = tokenizer\n        self._device = torch.device('cuda')\n        self._max_seq_length = 2048 if max_seq_length is None else max_seq_length\n\n    @property\n    def eot_token_id(self):\n        return self._tokenizer.eos_id()\n\n    @property\n    def max_length(self):\n        return self._max_seq_length\n\n    @property\n    def max_gen_toks(self):\n        return 50\n\n    @property\n    def batch_size(self):\n        return 1\n\n    @property\n    def device(self):\n        return self._device\n\n    def tok_encode(self, string: str, **kwargs):\n        encoded = encode_tokens(self._tokenizer,\n            string, bos=True, device=self._device)\n        # encoded is a pytorch tensor, but some internal logic in the\n        # eval harness expects it to be a list instead\n        # TODO: verify this for multi-batch as well\n        encoded = encoded.tolist()\n        return encoded\n\n    def tok_decode(self, tokens):\n        decoded = self._tokenizer.decode(tokens)\n        return decoded\n\n    def _model_call(self, inps):\n        # TODO: make batches work\n        inps = inps.squeeze(0)\n\n        max_new_tokens = 1\n        seq, input_pos, max_seq_length = \\\n            setup_cache_padded_seq_input_pos_max_seq_length_for_prefill(\n                self._model,\n                inps,\n                max_new_tokens,\n                self.max_length,\n            )\n        x = seq.index_select(0, input_pos).view(1, -1)\n        logits = model_forward(self._model, x, input_pos)\n        return logits\n\n    def _model_generate(self, context, max_length, eos_token_id):\n        raise Exception('unimplemented')\n\n\n@torch.no_grad()\ndef eval(\n    model: Transformer,\n    tokenizer,\n    tasks: list = [\"hellaswag\"],\n    limit: Optional[int] = None,\n    max_seq_length: Optional[int] = None,\n) -> dict:\n    \"\"\"\n    Evaluates a language model on a specified task using the lm-evaluation-harness library.\n\n    Args:\n        model (Transformer): The pre-trained language model to evaluate.\n        tokenizer: The tokenizer to use for encoding/decoding text.\n        tasks (list): The names of the evaluation tasks to perform.\n        limit (Optional[int]): The maximum number of samples to evaluate (None for all available).\n        max_seq_length (Optional[int]): The maximum sequence length allowed for input text.\n\n    Returns:\n        eval_results (dict): A dictionary of evaluation results for the specified task(s).\n    \"\"\"\n    model_eval_wrapper = GPTFastEvalWrapper(\n        model,\n        tokenizer,\n        max_seq_length,\n    )\n\n    try:\n        lm_eval.tasks.initialize_tasks()\n    except:\n        pass\n\n    if 'hendrycks_test' in tasks:\n        tasks.remove('hendrycks_test')\n        tasks += [x for x in lm_eval.tasks.hendrycks_test.create_all_tasks().keys()]\n    task_dict = get_task_dict(tasks)\n\n    eval_results = evaluate(\n        model_eval_wrapper,\n        task_dict,\n        limit=limit,\n    )\n    return eval_results\n\n\ndef main(\n    checkpoint_path: Path = Path(\"checkpoints/meta-llama/Llama-2-7b-chat-hf/lit_model.pth\"),\n    compile: bool = False,\n    tasks: list = [\"hellaswag\"],\n    limit: Optional[int] = None,\n    max_seq_length: Optional[int] = None,\n) -> None:\n    \"\"\"Evaluates model on a task from the `lm-evaluation-harness` library.\n\n    Args:\n        checkpoint_path (Path): The path to the model checkpoint file to load.\n        compile (bool): Whether or not to compile the model for optimization.\n        tasks (list): The names of the evaluation tasks to perform.\n        limit (Optional[int]): The maximum number of samples to evaluate (None for all available).\n        max_seq_length (Optional[int]): The maximum sequence length allowed for input text.\n\n    \"\"\"\n\n    assert checkpoint_path.is_file(), checkpoint_path\n\n    tokenizer_path = checkpoint_path.parent / \"tokenizer.model\"\n    assert tokenizer_path.is_file(), str(tokenizer_path)\n\n    device = 'cuda'\n    precision = torch.bfloat16\n\n    print(\"Loading model ...\")\n    t0 = time.time()\n    model = _load_model(checkpoint_path, device, precision, False)\n\n    torch.cuda.synchronize()\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds.\")\n\n    model.eval()\n\n    tokenizer = get_tokenizer(tokenizer_path, checkpoint_path)\n\n    torch.manual_seed(1234)\n\n    if compile:\n        global model_forward\n        model_forward = torch.compile(model_forward,  mode=\"reduce-overhead\", dynamic=True, fullgraph=True)\n        torch._inductor.config.coordinate_descent_tuning = True\n\n    t1 = time.time()\n    result = eval(\n        model,\n        tokenizer,\n        tasks,\n        limit,\n        max_seq_length,\n    )\n    print(f\"Time to run eval: {time.time() - t1:.02f} seconds.\")\n    print(f\"For model {checkpoint_path}\")\n    for task, res in result[\"results\"].items():\n        print(f\"{task}: {res}\")\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Your CLI description.')\n\n    parser.add_argument('--checkpoint_path', type=Path, default=Path(\"checkpoints/meta-llama/Llama-2-7b-chat-hf/lit_model.pth\"), help='Model checkpoint path.')\n    parser.add_argument('--compile', action='store_true', help='Whether to compile the model.')\n    parser.add_argument('--tasks', nargs='+', type=str, default=[\"hellaswag\"], help='list of lm-eluther tasks to evaluate usage: --tasks task1 task2')\n    parser.add_argument('--limit', type=int, default=None, help='number of samples to evalulate')\n    parser.add_argument('--max_seq_length', type=int, default=None, help='maximum length sequence to evaluate')\n\n    args = parser.parse_args()\n    main(\n        Path(args.checkpoint_path), args.compile, args.tasks, args.limit, args.max_seq_length,\n    )\n"
        },
        {
          "name": "generate.py",
          "type": "blob",
          "size": 19.2333984375,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport itertools\nimport sys\nimport time\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union\n\nimport torch\nimport torch._dynamo.config\nimport torch._inductor.config\nfrom torch.nn.attention.flex_attention import BlockMask, create_block_mask\n\ndef device_sync(device):\n    if \"cuda\" in device:\n        torch.cuda.synchronize(device)\n    elif (\"cpu\" in device) or (\"mps\" in device):\n        pass\n    else:\n        print(f\"device={device} is not yet suppported\")\n\n\ntorch._inductor.config.coordinate_descent_tuning = True\ntorch._inductor.config.triton.unique_kernel_names = True\n# Experimental features to reduce compilation times, will be on by default in future\ntorch._inductor.config.fx_graph_cache = True \ntorch._functorch.config.enable_autograd_cache = True\n\ndefault_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ncreate_block_mask = torch.compile(create_block_mask)\n\n# support running without installing as a package\nwd = Path(__file__).parent.parent.resolve()\nsys.path.append(str(wd))\n\nfrom model import Transformer\nfrom tokenizer import get_tokenizer\n\ndef multinomial_sample_one_no_sync(probs_sort): # Does multinomial sampling without a cuda synchronization\n    q = torch.empty_like(probs_sort).exponential_(1)\n    return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)\n\ndef logits_to_probs(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n    logits = logits / max(temperature, 1e-5)\n\n    if top_k is not None:\n        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n        pivot = v.select(-1, -1).unsqueeze(-1)\n        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n    probs = torch.nn.functional.softmax(logits, dim=-1)\n    return probs\n\ndef sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n    probs = logits_to_probs(logits[:, -1], temperature, top_k)\n    idx_next = multinomial_sample_one_no_sync(probs)\n    return idx_next, probs\n\ndef roundup(val, multiplier):\n    return ((val - 1) // multiplier + 1) * multiplier\n\ndef causal_mask(b, h, q, kv):\n    return q >= kv\n\ndef prefill(model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs) -> torch.Tensor:\n    # input_pos: [B, S]\n    mask = create_block_mask(causal_mask, 1, 1, input_pos.shape[0], model.max_seq_length, device=x.device)\n    logits = model(mask, x, input_pos)\n    return sample(logits, **sampling_kwargs)[0]\n\ndef decode_one_token(model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, block_mask: BlockMask, **sampling_kwargs) -> Tuple[torch.Tensor, torch.Tensor]:\n    # input_pos: [B, 1]\n    assert input_pos.shape[-1] == 1\n    block_index = input_pos // block_mask.BLOCK_SIZE[0]\n    mask = block_mask[:, :, block_index]\n    mask.mask_mod = block_mask.mask_mod\n    mask.seq_lengths = (1, model.max_seq_length)\n    logits = model(mask, x, input_pos)\n    return sample(logits, **sampling_kwargs)\n\ndef decode_n_tokens(model: Transformer, cur_token: torch.Tensor, input_pos: torch.Tensor, num_new_tokens: int, callback=lambda _: _, **sampling_kwargs):\n    block_mask = create_block_mask(causal_mask, 1, 1, model.max_seq_length, model.max_seq_length, device=cur_token.device)\n    new_tokens, new_probs = [], []\n    for i in range(num_new_tokens):\n        next_token, next_prob = decode_one_token(\n            model, cur_token, input_pos, block_mask, **sampling_kwargs\n        )\n        input_pos += 1\n        new_tokens.append(next_token.clone())\n        callback(new_tokens[-1])\n        new_probs.append(next_prob.clone())\n        cur_token = next_token.clone()\n\n    return new_tokens, new_probs\n\n\ndef model_forward(model, x, input_pos):\n    return model(x, input_pos)\n\ndef speculative_decode(\n    model: Transformer,\n    draft_model: Transformer,\n    cur_token: torch.Tensor,\n    input_pos: int,\n    speculate_k: int,\n    **sampling_kwargs\n) -> torch.Tensor:\n    # draft model inference sequentially\n    device = cur_token.device\n    orig_input_pos = torch.tensor([input_pos], dtype=torch.int64, device=cur_token.device)\n    draft_tokens, draft_probs = decode_n_tokens(draft_model, cur_token.view(1, -1), orig_input_pos.clone(), speculate_k, **sampling_kwargs)\n\n    draft_tokens = torch.cat(draft_tokens)\n    # parallel inference on target model using draft tokens\n    target_logits = model_forward(\n        model,\n        torch.cat([cur_token.view(1), draft_tokens]).view(1, -1),\n        torch.arange(input_pos, input_pos + speculate_k + 1, device=cur_token.device)\n    )\n    target_probs = logits_to_probs(target_logits[0], **sampling_kwargs)\n    draft_probs = torch.stack(draft_probs)\n    # q: target prob, p: draft prob\n    # q >= p: always accept draft token\n    # q < p: q/p prob to accept draft token\n    p = draft_probs[torch.arange(0, speculate_k, device=device), draft_tokens]\n    q = target_probs[torch.arange(0, speculate_k, device=device), draft_tokens]\n    accept_draft_prob = torch.minimum(torch.ones(()), q[:speculate_k]/ p)\n    rejected_locations = (torch.rand_like(accept_draft_prob) > accept_draft_prob).nonzero()\n\n    if rejected_locations.shape[0] == 0: # All draft tokens have been accepted\n        accept_length = speculate_k + 1\n        last_token = multinomial_sample_one_no_sync(target_probs[-1])\n        # fill last token into draft model\n        model_forward(\n            draft_model,\n            draft_tokens[-1].view(1, -1),\n            orig_input_pos + speculate_k,\n        )\n        return torch.cat([draft_tokens, last_token])\n    else:\n        accept_length = rejected_locations[0].item()\n        p = draft_probs[accept_length]\n        q = target_probs[accept_length]\n        new = q - p\n        new = torch.where(new > 0, new, 0.0)\n        new = new / new.sum()\n        next_token = multinomial_sample_one_no_sync(new)\n        return torch.cat([draft_tokens[:accept_length], next_token])\n\n@torch.no_grad()\ndef generate(\n    model: Transformer,\n    prompt: torch.Tensor,\n    max_new_tokens: int,\n    batch_size: int,\n    *,\n    interactive: bool,\n    draft_model: Transformer,\n    speculate_k: Optional[int] = 8,\n    callback = lambda x: x,\n    **sampling_kwargs\n) -> torch.Tensor:\n    \"\"\"\n    Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n    \"\"\"\n\n    is_speculative = draft_model is not None\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    T = prompt.size(-1)\n    T_new = T + max_new_tokens\n    if interactive:\n        max_seq_length = 350\n    else:\n        max_seq_length = min(T_new, model.config.block_size)\n\n    device, dtype = prompt.device, prompt.dtype\n    max_seq_length = max_seq_length + speculate_k + 1 if is_speculative else max_seq_length\n    with torch.device(device):\n        model.setup_caches(max_batch_size=batch_size, max_seq_length=max_seq_length)\n        if is_speculative and draft_model is not model:\n            draft_model.setup_caches(max_batch_size=batch_size, max_seq_length=max_seq_length)\n\n    # create an empty tensor of the expected final shape and fill in the current tokens\n    empty = torch.empty(batch_size, T_new, dtype=dtype, device=device)\n    # We are just making the same prompt for every batch\n    prompt = prompt.view(1, -1).repeat(batch_size, 1)\n    empty[:, :T] = prompt\n    seq = empty\n    input_pos = torch.arange(0, T, device=device)\n\n    next_token = prefill(model, prompt.view(batch_size, -1), input_pos, **sampling_kwargs).clone()\n    if is_speculative:\n        prefill(draft_model, prompt.view(batch_size, -1), input_pos, **sampling_kwargs)\n    seq[:, T] = next_token.squeeze()\n\n    input_pos = torch.tensor([T], device=device, dtype=torch.int)\n    accept_counts = [0] * (speculate_k + 1)\n\n    if is_speculative:\n        input_pos = input_pos.item()  # for speculative decoding easier to keep on host\n        while input_pos < T_new - 1:\n            cur_token = next_token.view(())\n\n            next_tokens = speculative_decode(\n                model, draft_model, cur_token, input_pos, speculate_k, **sampling_kwargs\n            )\n\n            accept_counts[len(next_tokens) - 1] += 1\n            num_added = min(T_new - input_pos - 1, len(next_tokens))\n            seq[input_pos + 1 : input_pos + num_added + 1] = next_tokens[: num_added]\n            for i in next_tokens[: num_added,]:\n                callback(i)\n            input_pos = input_pos + num_added\n            next_token = next_tokens[-1]\n    else:\n        generated_tokens, _ = decode_n_tokens(model, next_token.view(batch_size, -1), input_pos, max_new_tokens - 1, callback=callback, **sampling_kwargs)\n        seq[:, T + 1:] = torch.cat(generated_tokens, dim=-1)\n\n    generate_stats = {\n        'accept_counts': accept_counts\n    }\n    return seq, generate_stats\n\ndef encode_tokens(tokenizer, string, bos=True, device=default_device):\n    tokens = tokenizer.encode(string)\n    if bos:\n        tokens = [tokenizer.bos_id()] + tokens\n    return torch.tensor(tokens, dtype=torch.int, device=device)\n\ndef _load_model(checkpoint_path, device, precision, use_tp):\n    use_cuda = 'cuda' in device\n    with torch.device('meta'):\n        model = Transformer.from_name(checkpoint_path.parent.name)\n\n    if \"int8\" in str(checkpoint_path):\n        print(\"Using int8 weight-only quantization!\")\n        from quantize import WeightOnlyInt8QuantHandler\n        simple_quantizer = WeightOnlyInt8QuantHandler(model)\n        model = simple_quantizer.convert_for_runtime()\n\n    if \"int4\" in str(checkpoint_path):\n        print(\"Using int4 weight-only quantization!\")\n        path_comps = checkpoint_path.name.split(\".\")\n        groupsize = int(path_comps[-2][1:])\n        from quantize import WeightOnlyInt4QuantHandler\n        simple_quantizer = WeightOnlyInt4QuantHandler(model, groupsize)\n        model = simple_quantizer.convert_for_runtime()\n\n    checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=True)\n    if \"model\" in checkpoint and \"stories\" in str(checkpoint_path):\n        checkpoint = checkpoint[\"model\"]\n    model.load_state_dict(checkpoint, assign=True)\n\n    if use_tp:\n        from tp import apply_tp\n        print(\"Applying tensor parallel to model ...\")\n        apply_tp(model)\n\n    model = model.to(device=device, dtype=precision)\n    return model.eval()\n\ndef _get_model_size(model):\n    model_size = 0\n    params = 0\n    for name, child in model.named_children():\n        if not isinstance(child, torch.nn.Embedding):\n            model_size += sum(\n                [\n                    p.numel() * p.dtype.itemsize\n                    for p in itertools.chain(child.parameters(), child.buffers())\n                ]\n            )\n            params += sum(\n                [\n                    p.numel()\n                    for p in itertools.chain(child.parameters(), child.buffers())\n                ]\n            )\n    return model_size, params\n\nB_INST, E_INST = \"[INST]\", \"[/INST]\"\n\ndef main(\n    prompt: Union[int, str] = \"Hello, my name is\",\n    interactive: bool = False,\n    num_samples: int = 5,\n    max_new_tokens: int = 100,\n    batch_size: int = 1,\n    top_k: int = 200,\n    temperature: float = 0.8,\n    checkpoint_path: Path = Path(\"checkpoints/meta-Transformer/Transformer-2-7b-chat-hf/model.pth\"),\n    compile: bool = True,\n    compile_prefill: bool = False,\n    profile: Optional[Path] = None,\n    draft_checkpoint_path: Optional[Path] = None,\n    speculate_k: int = 5,\n    device=default_device,\n) -> None:\n    \"\"\"Generates text samples based on a pre-trained Transformer model and tokenizer.\n    \"\"\"\n    assert checkpoint_path.is_file(), checkpoint_path\n\n    tokenizer_path = checkpoint_path.parent / \"tokenizer.model\"\n    assert tokenizer_path.is_file(), str(tokenizer_path)\n\n    global print\n    from tp import maybe_init_dist\n    rank = maybe_init_dist()\n    use_tp = rank is not None\n    if use_tp:\n        if rank != 0:\n            # only print on rank 0\n            print = lambda *args, **kwargs: None\n\n    print(f\"Using device={device}\")\n    precision = torch.bfloat16\n    is_speculative = draft_checkpoint_path is not None\n    is_chat = \"chat\" in str(checkpoint_path)\n\n    print(\"Loading model ...\")\n    t0 = time.time()\n    model = _load_model(checkpoint_path, device, precision, use_tp)\n\n    if is_speculative:\n        draft_model = _load_model(draft_checkpoint_path, device, precision, use_tp)\n    else:\n        draft_model = None\n\n    device_sync(device=device) # MKG\n    print(f\"Time to load model: {time.time() - t0:.02f} seconds\")\n\n    tokenizer = get_tokenizer(tokenizer_path, checkpoint_path)\n\n    if isinstance(prompt, str):\n        encoded = encode_tokens(tokenizer, prompt, bos=True, device=device)\n    else:\n        # generate a fully synthetic prompt\n        encoded = torch.randint(0, 1024, (prompt,), device=device, dtype=torch.int64)\n    prompt_length = encoded.size(-1)\n\n    torch.manual_seed(1234)\n    model_size, params = _get_model_size(model)\n    if compile:\n        if is_speculative and use_tp: # and (\"cuda\" in device):\n            torch._inductor.config.triton.cudagraph_trees = False # Bug with cudagraph trees in this case\n\n        if is_speculative:\n            global model_forward, logits_to_prob\n            model_forward = torch.compile(model_forward, mode=\"reduce-overhead\", fullgraph=True)\n\n        global decode_one_token, prefill\n        decode_one_token = torch.compile(decode_one_token, mode=\"reduce-overhead\", fullgraph=True)\n\n        # Uncomment to squeeze more perf out of prefill\n        if compile_prefill:\n            prefill = torch.compile(prefill, fullgraph=True, dynamic=True)\n\n\n    aggregate_metrics = {\n        'tokens_per_sec': [],\n        'accept_counts': [],\n    }\n    start = -1 if compile else 0\n\n    for i in range(start, num_samples):\n        device_sync(device=device) # MKG\n        if i >= 0 and interactive:\n            prompt = input(\"What is your prompt? \")\n            if is_chat:\n                prompt = f\"{B_INST} {prompt.strip()} {E_INST}\"\n            encoded = encode_tokens(tokenizer, prompt, bos=True, device=device)\n\n        if interactive and i >= 0:\n            buffer = []\n            period_id = tokenizer.encode('.')[0]\n            done_generating = False\n            def callback(x):\n                nonlocal done_generating\n                if done_generating:\n                    return\n                buffer.append(tokenizer.decode([period_id] + x.tolist())[1:])\n                if x.item() == tokenizer.eos_id():\n                    done_generating = True\n                if len(buffer) == 4 or done_generating:\n                    print(''.join(buffer), end='', flush=True)\n                    buffer.clear()\n                # print(, end='', flush=True)\n        else:\n            callback = lambda x : x\n        t0 = time.perf_counter()\n        import contextlib\n        if (i != num_samples - 1 or not profile) or (use_tp and rank != 0):\n            prof = contextlib.nullcontext()\n        else:\n            torch.profiler._utils._init_for_cuda_graphs()\n            prof = torch.profiler.profile()\n        with prof:\n            y, metrics = generate(\n                model,\n                encoded,\n                max_new_tokens,\n                batch_size=batch_size,\n                draft_model=draft_model,\n                speculate_k=speculate_k,\n                interactive=interactive,\n                callback=callback,\n                temperature=temperature,\n                top_k=top_k,\n            )\n            aggregate_metrics['accept_counts'].append(metrics['accept_counts'])\n        if i == -1:\n            print(f\"Compilation time: {time.perf_counter() - t0:.2f} seconds\")\n            continue\n        if hasattr(prof, \"export_chrome_trace\"):\n            if use_tp:\n                prof.export_chrome_trace(f\"{profile}_rank_{rank}.json\")\n            else:\n                prof.export_chrome_trace(f\"{profile}.json\")\n        device_sync(device=device) # MKG\n        t = time.perf_counter() - t0\n\n        if not interactive:\n            # Just displaying the first generation\n            if batch_size > 1:\n                print(\"Only displaying the first generation of the batch\")\n            print(tokenizer.decode(y[0].tolist()))\n        else:\n            print()\n        tokens_generated = y.size(-1) - prompt_length\n        generated_tokens_sec = tokens_generated / t\n        aggregate_metrics['tokens_per_sec'].append(generated_tokens_sec)\n        print(f\"Time for inference {i + 1}: {t:.02f} sec total, {generated_tokens_sec:.02f} tokens/sec\")\n        print(f\"Bandwidth achieved: {model_size * generated_tokens_sec / 1e9:.02f} GB/s\")\n        total_tokens_sec = y.numel() / t\n        print(f\"FLOPS achieved: {params * total_tokens_sec * 2 / 1e12:.02f} TF/s\")\n        print()\n    print(\"==========\")\n    if is_speculative:\n        counts_aggregated = [sum(i) for i in zip(*aggregate_metrics['accept_counts'])]\n        acceptance_probs = [i/sum(counts_aggregated) for i in counts_aggregated]\n        print(f\"Acceptance probs: {acceptance_probs}\")\n        print(f\"Mean Accepted: {sum([idx * i for idx, i in enumerate(counts_aggregated)])/sum(counts_aggregated)}\")\n\n    print(f\"Batch Size: {batch_size}\")\n    print(f\"Prompt Length: {prompt_length}\")\n    print(f\"Generated tokens: {max_new_tokens}\")\n    print(f\"Average tokens/sec: {torch.mean(torch.tensor(aggregate_metrics['tokens_per_sec'])).item():.2f}\")\n    print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\")\n\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Your CLI description.')\n\n    def int_or_str(x):\n        try:\n            return int(x)\n        except:\n            return x\n\n    parser.add_argument('--prompt', type=int_or_str, default=\"Hello, my name is\", help=\"Input prompt. If it's an integer, will instead generate a synthetic prompt.\")\n    parser.add_argument('--interactive', action='store_true', help='Whether to launch in interactive mode')\n    parser.add_argument('--num_samples', type=int, default=5, help='Number of samples.')\n    parser.add_argument('--max_new_tokens', type=int, default=200, help='Maximum number of new tokens.')\n    parser.add_argument('--batch_size', type=int, default=1, help='Batch size to benchmark with')\n    parser.add_argument('--top_k', type=int, default=200, help='Top-k for sampling.')\n    parser.add_argument('--temperature', type=float, default=0.8, help='Temperature for sampling.')\n    parser.add_argument('--checkpoint_path', type=Path, default=Path(\"checkpoints/meta-Transformer/Transformer-2-7b-chat-hf/model.pth\"), help='Model checkpoint path.')\n    parser.add_argument('--compile', action='store_true', help='Whether to compile the model.')\n    parser.add_argument('--compile_prefill', action='store_true', help='Whether to compile the prefill (improves prefill perf, but higher compile times)')\n    parser.add_argument('--profile', type=Path, default=None, help='Profile path.')\n    parser.add_argument('--speculate_k', type=int, default=5, help='Speculative execution depth.')\n    parser.add_argument('--draft_checkpoint_path', type=Path, default=None, help='Draft checkpoint path.')\n    parser.add_argument('--device', type=str, default=default_device, help='Device to use')\n\n    args = parser.parse_args()\n    main(\n        args.prompt, args.interactive, args.num_samples, args.max_new_tokens, args.batch_size, args.top_k,\n        args.temperature, args.checkpoint_path, args.compile, args.compile_prefill, args.profile, args.draft_checkpoint_path,\n        args.speculate_k, args.device\n    )\n"
        },
        {
          "name": "mixtral-moe",
          "type": "tree",
          "content": null
        },
        {
          "name": "model.py",
          "type": "blob",
          "size": 12.1748046875,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport math\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nfrom torch.nn import functional as F\nfrom torch.nn.attention.flex_attention import (\n    _mask_mod_signature,\n    BlockMask,\n    flex_attention,\n)\n\n\ndef find_multiple(n: int, k: int) -> int:\n    if n % k == 0:\n        return n\n    return n + k - (n % k)\n\n\ndef get_mask_mod(mask_mod: _mask_mod_signature, offset: int):\n    def _mask_mod(b, h, q, kv):\n        return mask_mod(b, h, q + offset, kv)\n\n    return _mask_mod\n\n\n@dataclass\nclass ModelArgs:\n    block_size: int = 2048\n    vocab_size: int = 32000\n    n_layer: int = 32\n    n_head: int = 32\n    dim: int = 4096\n    intermediate_size: int = None\n    n_local_heads: int = -1\n    head_dim: int = 64\n    rope_base: float = 10000\n    norm_eps: float = 1e-5\n    rope_scaling: Optional[dict] = None\n\n    def __post_init__(self):\n        if self.n_local_heads == -1:\n            self.n_local_heads = self.n_head\n        if self.intermediate_size is None:\n            hidden_dim = 4 * self.dim\n            n_hidden = int(2 * hidden_dim / 3)\n            self.intermediate_size = find_multiple(n_hidden, 256)\n        self.head_dim = self.dim // self.n_head\n\n    @classmethod\n    def from_name(cls, name: str):\n        if name in transformer_configs:\n            return cls(**transformer_configs[name])\n        # fuzzy search\n        config = [config for config in transformer_configs if config.lower() in str(name).lower()]\n\n        # We may have two or more configs matched (e.g. \"7B\" and \"Mistral-7B\"). Find the best config match,\n        # take longer name (as it have more symbols matched)\n        if len(config) > 1:\n            config.sort(key=len, reverse=True)\n            assert len(config[0]) != len(config[1]), name # make sure only one 'best' match\n            \n        return cls(**transformer_configs[config[0]])\n\n\ntransformer_configs = {\n    \"CodeLlama-7b-Python-hf\": dict(block_size=16384, vocab_size=32000, n_layer=32, dim = 4096, rope_base=1000000),\n    \"7B\": dict(n_layer=32, n_head=32, dim=4096),\n    \"13B\": dict(n_layer=40, n_head=40, dim=5120),\n    \"30B\": dict(n_layer=60, n_head=52, dim=6656),\n    \"34B\": dict(n_layer=48, n_head=64, dim=8192, vocab_size=32000, n_local_heads=8, intermediate_size=22016, rope_base=1000000), # CodeLlama-34B-Python-hf\n    \"70B\": dict(n_layer=80, n_head=64, dim=8192, n_local_heads=8, intermediate_size=28672),\n    \"Mistral-7B\": dict(n_layer=32, n_head=32, n_local_heads=8, dim=4096, intermediate_size=14336, vocab_size=32000),\n    \"stories15M\": dict(n_layer=6, n_head=6, dim=288),\n    \"stories110M\": dict(n_layer=12, n_head=12, dim=768),\n\n    \"llama-3-8b\": dict(block_size=8192, n_layer=32, n_head=32, n_local_heads=8, dim=4096, intermediate_size=14336, vocab_size=128256, rope_base=500000),\n    \"llama-3-70b\": dict(block_size=8192, n_layer=80, n_head=64, n_local_heads=8, dim=8192, intermediate_size=28672, vocab_size=128256, rope_base=500000),\n    \"llama-3.1-8b\": dict(block_size=131072, n_layer=32, n_head=32, n_local_heads=8, dim=4096, intermediate_size=14336, vocab_size=128256, rope_base=500000,\n        rope_scaling=dict(factor=8.0, low_freq_factor=1.0, high_freq_factor=4.0, original_max_position_embeddings=8192),\n    ),\n    \"llama-3.1-70b\": dict(block_size=131072, n_layer=80, n_head=64, n_local_heads=8, dim=8192, intermediate_size=28672, vocab_size=128256, rope_base=500000,\n        rope_scaling=dict(factor=8.0, low_freq_factor=1.0, high_freq_factor=4.0, original_max_position_embeddings=8192),\n    ),\n    \"llama-3.1-405b\": dict(block_size=131072, n_layer=126, n_head=128, n_local_heads=8, dim=16384, intermediate_size=53248, vocab_size=128256, rope_base=500000,\n        rope_scaling=dict(factor=8.0, low_freq_factor=1.0, high_freq_factor=4.0, original_max_position_embeddings=8192),\n    ),\n}\n\nclass KVCache(nn.Module):\n    def __init__(self, max_batch_size, max_seq_length, n_heads, head_dim, dtype=torch.bfloat16):\n        super().__init__()\n        cache_shape = (max_batch_size, n_heads, max_seq_length, head_dim)\n        self.register_buffer('k_cache', torch.zeros(cache_shape, dtype=dtype))\n        self.register_buffer('v_cache', torch.zeros(cache_shape, dtype=dtype))\n\n    def update(self, input_pos, k_val, v_val):\n        # input_pos: [S], k_val: [B, H, S, D]\n        assert input_pos.shape[0] == k_val.shape[2]\n\n        k_out = self.k_cache\n        v_out = self.v_cache\n        k_out[:, :, input_pos] = k_val\n        v_out[:, :, input_pos] = v_val\n\n        return k_out, v_out\n\nclass Transformer(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.config = config\n\n        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)\n        self.layers = nn.ModuleList(TransformerBlock(config) for _ in range(config.n_layer))\n        self.norm = RMSNorm(config.dim, eps=config.norm_eps)\n        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n\n        self.freqs_cis: Optional[Tensor] = None\n        self.mask_cache: Optional[Tensor] = None\n        self.max_batch_size = -1\n        self.max_seq_length = -1\n        self.get_mask_mod = get_mask_mod\n\n    def setup_caches(self, max_batch_size, max_seq_length):\n        if self.max_seq_length >= max_seq_length and self.max_batch_size >= max_batch_size:\n            return\n        head_dim = self.config.dim // self.config.n_head\n        max_seq_length = find_multiple(max_seq_length, 8)\n        self.max_seq_length = max_seq_length\n        self.max_batch_size = max_batch_size\n        dtype = self.output.weight.dtype\n        # For quantized layers, dtype is encoded in scales\n        if hasattr(self.output, \"scales\"):\n            dtype = self.output.scales.dtype\n        elif hasattr(self.output, \"scales_and_zeros\"):\n            dtype = self.output.scales_and_zeros.dtype\n        for b in self.layers:\n            b.attention.kv_cache = KVCache(max_batch_size, max_seq_length, self.config.n_local_heads, head_dim, dtype)\n\n        self.freqs_cis = precompute_freqs_cis(self.config.block_size, self.config.dim // self.config.n_head, self.config.rope_base, dtype, self.config.rope_scaling)\n\n    def forward(self, mask: BlockMask, idx: Tensor, input_pos: Optional[Tensor] = None) -> Tensor:\n        assert self.freqs_cis is not None, \"Caches must be initialized first\"\n        mask.mask_mod = self.get_mask_mod(mask.mask_mod, input_pos[0])\n        freqs_cis = self.freqs_cis[input_pos]\n        x = self.tok_embeddings(idx)\n\n        for i, layer in enumerate(self.layers):\n            x = layer(x, input_pos, freqs_cis, mask)\n        x = self.norm(x)\n        logits = self.output(x)\n        return logits\n\n    @classmethod\n    def from_name(cls, name: str):\n        return cls(ModelArgs.from_name(name))\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.attention = Attention(config)\n        self.feed_forward = FeedForward(config)\n        self.ffn_norm = RMSNorm(config.dim, config.norm_eps)\n        self.attention_norm = RMSNorm(config.dim, config.norm_eps)\n\n    def forward(self, x: Tensor, input_pos: Tensor, freqs_cis: Tensor, mask: BlockMask) -> Tensor:\n        h = x + self.attention(self.attention_norm(x), freqs_cis, mask, input_pos)\n        out = h + self.feed_forward(self.ffn_norm(h))\n        return out\n\n\nclass Attention(nn.Module):\n    def __init__(self, config: ModelArgs):\n        super().__init__()\n        assert config.dim % config.n_head == 0\n\n        total_head_dim = (config.n_head + 2 * config.n_local_heads) * config.head_dim\n        # key, query, value projections for all heads, but in a batch\n        self.wqkv = nn.Linear(config.dim, total_head_dim, bias=False)\n        self.wo = nn.Linear(config.dim, config.dim, bias=False)\n        self.kv_cache = None\n\n        self.n_head = config.n_head\n        self.head_dim = config.head_dim\n        self.n_local_heads = config.n_local_heads\n        self.dim = config.dim\n        self._register_load_state_dict_pre_hook(self.load_hook)\n\n    def load_hook(self, state_dict, prefix, *args):\n        if prefix + \"wq.weight\" in state_dict:\n            wq = state_dict.pop(prefix + \"wq.weight\")\n            wk = state_dict.pop(prefix + \"wk.weight\")\n            wv = state_dict.pop(prefix + \"wv.weight\")\n            state_dict[prefix + \"wqkv.weight\"] = torch.cat([wq, wk, wv])\n\n    def forward(self, x: Tensor, freqs_cis: Tensor, mask: BlockMask, input_pos: Optional[Tensor] = None) -> Tensor:\n        bsz, seqlen, _ = x.shape\n\n        kv_size = self.n_local_heads * self.head_dim\n        q, k, v = self.wqkv(x).split([self.dim, kv_size, kv_size], dim=-1)\n\n        q = q.view(bsz, seqlen, self.n_head, self.head_dim)\n        k = k.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n        v = v.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n\n        q = apply_rotary_emb(q, freqs_cis)\n        k = apply_rotary_emb(k, freqs_cis)\n\n        q, k, v = map(lambda x: x.transpose(1, 2), (q, k, v))\n\n        if self.kv_cache is not None:\n            k, v = self.kv_cache.update(input_pos, k, v)\n\n        y = flex_attention(q, k, v, block_mask=mask, enable_gqa=(self.n_head != self.n_local_heads))\n\n        y = y.transpose(1, 2).contiguous().view(bsz, seqlen, self.dim)\n\n        y = self.wo(y)\n        return y\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, config: ModelArgs) -> None:\n        super().__init__()\n        self.w1 = nn.Linear(config.dim, config.intermediate_size, bias=False)\n        self.w3 = nn.Linear(config.dim, config.intermediate_size, bias=False)\n        self.w2 = nn.Linear(config.intermediate_size, config.dim, bias=False)\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)\n\n    def forward(self, x: Tensor) -> Tensor:\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n\n\ndef apply_rope_scaling(freqs: torch.Tensor, rope_scaling: Optional[dict] = None):\n    factor = rope_scaling[\"factor\"]\n    low_freq_factor = rope_scaling[\"low_freq_factor\"]\n    high_freq_factor = rope_scaling[\"high_freq_factor\"]\n    old_context_len = rope_scaling[\"original_max_position_embeddings\"]\n\n    low_freq_wavelen = old_context_len / low_freq_factor\n    high_freq_wavelen = old_context_len / high_freq_factor\n    new_freqs = []\n    for freq in freqs:\n        wavelen = 2 * math.pi / freq\n        if wavelen < high_freq_wavelen:\n            new_freqs.append(freq)\n        elif wavelen > low_freq_wavelen:\n            new_freqs.append(freq / factor)\n        else:\n            assert low_freq_wavelen != high_freq_wavelen\n            smooth = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n            new_freqs.append((1 - smooth) * freq / factor + smooth * freq)\n    return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)\n\n\ndef precompute_freqs_cis(\n    seq_len: int, n_elem: int, base: int = 10000,\n    dtype: torch.dtype = torch.bfloat16,\n    rope_scaling: Optional[dict] = None,\n) -> Tensor:\n    freqs = 1.0 / (base ** (torch.arange(0, n_elem, 2)[: (n_elem // 2)].float() / n_elem))\n    if rope_scaling is not None:\n        freqs = apply_rope_scaling(freqs, rope_scaling)\n    t = torch.arange(seq_len, device=freqs.device)\n    freqs = torch.outer(t, freqs)\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n    cache = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1)\n    return cache.to(dtype=dtype)\n\n\ndef apply_rotary_emb(x: Tensor, freqs_cis: Tensor) -> Tensor:\n    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n    x_out2 = torch.stack(\n        [\n            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],\n            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],\n        ],\n        -1,\n    )\n\n    x_out2 = x_out2.flatten(3)\n    return x_out2.type_as(x)\n"
        },
        {
          "name": "quantize.py",
          "type": "blob",
          "size": 25.005859375,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport time\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tokenizer import get_tokenizer\n\ntry:\n    from GPTQ import GenericGPTQRunner, InputRecorder\n    from eval import get_task_dict, evaluate, lm_eval\nexcept:\n    pass\n\nfrom model import Transformer\n\n##### Quantization Primitives ######\n\ndef dynamically_quantize_per_channel(x, quant_min, quant_max, target_dtype):\n    # assumes symmetric quantization\n    # assumes axis == 0\n    # assumes dense memory format\n    # TODO(future): relax ^ as needed\n\n    # default setup for affine quantization of activations\n    eps = torch.finfo(torch.float32).eps\n\n    # get min and max\n    min_val, max_val = torch.aminmax(x, dim=1)\n\n    # calculate scales and zero_points based on min and max\n    # reference: https://fburl.com/code/srbiybme\n    min_val_neg = torch.min(min_val, torch.zeros_like(min_val))\n    max_val_pos = torch.max(max_val, torch.zeros_like(max_val))\n    device = min_val_neg.device\n\n    # reference: https://fburl.com/code/4wll53rk\n    max_val_pos = torch.max(-min_val_neg, max_val_pos)\n    scales = max_val_pos / (float(quant_max - quant_min) / 2)\n    # ensure scales is the same dtype as the original tensor\n    scales = torch.clamp(scales, min=eps).to(x.dtype)\n    zero_points = torch.zeros(min_val_neg.size(), dtype=torch.int64, device=device)\n\n    # quantize based on qmin/qmax/scales/zp\n    # reference: https://www.internalfb.com/code/fbsource/[8edc275012b1]/fbcode/caffe2/torch/ao/quantization/fx/_decomposed.py?lines=63\n    x_div = x / scales.unsqueeze(-1)\n    x_round = torch.round(x_div)\n    x_zp = x_round + zero_points.unsqueeze(-1)\n    quant = torch.clamp(x_zp, quant_min, quant_max).to(target_dtype)\n\n    return quant, scales, zero_points\n\ndef get_group_qparams(w, n_bit=4, groupsize=128):\n    # needed for GPTQ with padding\n    if groupsize > w.shape[-1]:\n        groupsize = w.shape[-1]\n    assert groupsize > 1\n    assert w.shape[-1] % groupsize == 0\n    assert w.dim() == 2\n\n    to_quant = w.reshape(-1, groupsize)\n    assert torch.isnan(to_quant).sum() == 0\n\n    max_val = to_quant.amax(dim=1, keepdim=True)\n    min_val = to_quant.amin(dim=1, keepdim=True)\n    max_int = 2**n_bit - 1\n    scales = (max_val - min_val).clamp(min=1e-6) / max_int\n    zeros = min_val + scales * (2 ** (n_bit - 1))\n    return scales.to(torch.bfloat16).reshape(w.shape[0], -1), zeros.to(\n        torch.bfloat16\n    ).reshape(w.shape[0], -1)\n\n\ndef pack_scales_and_zeros(scales, zeros):\n    assert scales.shape == zeros.shape\n    assert scales.dtype == torch.bfloat16\n    assert zeros.dtype == torch.bfloat16\n    return (\n        torch.cat(\n            [\n                scales.reshape(scales.size(0), scales.size(1), 1),\n                zeros.reshape(zeros.size(0), zeros.size(1), 1),\n            ],\n            2,\n        )\n        .transpose(0, 1)\n        .contiguous()\n    )\n\n\ndef unpack_scales_and_zeros(scales_and_zeros):\n    assert len(scales_and_zeros.shape) == 3 and scales_and_zeros.shape[2] == 2\n    assert scales_and_zeros.dtype == torch.float\n    return torch.split(scales_and_zeros.transpose(0, 1), 1, 2)\n\n\ndef group_quantize_tensor_from_qparams(w, scales, zeros, n_bit=4, groupsize=128):\n    assert groupsize > 1\n    # needed for GPTQ single column quantize\n    if groupsize > w.shape[-1] and scales.shape[-1] == 1:\n        groupsize = w.shape[-1]\n\n    assert w.shape[-1] % groupsize == 0\n    assert w.dim() == 2\n\n    to_quant = w.reshape(-1, groupsize)\n    assert torch.isnan(to_quant).sum() == 0\n\n    scales = scales.reshape(-1, 1)\n    zeros = zeros.reshape(-1, 1)\n    min_val = zeros - scales * (2 ** (n_bit - 1))\n    max_int = 2**n_bit - 1\n    min_int = 0\n    w_int32 = (\n        to_quant.sub(min_val)\n        .div(scales)\n        .round()\n        .clamp_(min_int, max_int)\n        .to(torch.int32)\n        .reshape_as(w)\n    )\n\n    return w_int32\n\n\ndef group_quantize_tensor(w, n_bit=4, groupsize=128):\n    scales, zeros = get_group_qparams(w, n_bit, groupsize)\n    w_int32 = group_quantize_tensor_from_qparams(w, scales, zeros, n_bit, groupsize)\n    scales_and_zeros = pack_scales_and_zeros(scales, zeros)\n    return w_int32, scales_and_zeros\n\n\ndef group_dequantize_tensor_from_qparams(\n    w_int32, scales, zeros, n_bit=4, groupsize=128\n):\n    assert groupsize > 1\n    # needed for GPTQ single column dequantize\n    if groupsize > w_int32.shape[-1] and scales.shape[-1] == 1:\n        groupsize = w_int32.shape[-1]\n    assert w_int32.shape[-1] % groupsize == 0\n    assert w_int32.dim() == 2\n\n    w_int32_grouped = w_int32.reshape(-1, groupsize)\n    scales = scales.reshape(-1, 1)\n    zeros = zeros.reshape(-1, 1)\n\n    w_dq = (\n        w_int32_grouped.sub(2 ** (n_bit - 1)).mul(scales).add(zeros).reshape_as(w_int32)\n    )\n    return w_dq\n\n\ndef group_dequantize_tensor(w_int32, scales_and_zeros, n_bit=4, groupsize=128):\n    scales, zeros = unpack_scales_and_zeros(scales_and_zeros)\n    return group_dequantize_tensor_from_qparams(\n        w_int32, scales, zeros, n_bit, groupsize\n    )\n\nclass QuantHandler:\n    def __init__(self, mod):\n        self.mod = mod\n\n    def create_quantized_state_dict(self) -> \"StateDict\":\n        pass\n\n    def convert_for_runtime(self) -> \"nn.Module\":\n        pass\n\nclass GPTQQuantHandler(QuantHandler):\n    \"\"\"\n    This class implements a GPTQ QuantHandler that can be used to apply GPTQ to a model in concert with the GenericGPTQRunner class.\n    Unlike the base QuantHandler class, the user does not need to implement the create_quantized_state_dict, instead they have to reimplement\n    __init__ such that it defines the functions for the quantization mode. User is expected to reimplement convert_for_runtime.\n\n    The following functions (which must be defined in __init__) are used to define the quantization mode for both GPTQ and\n    create_quantized_state_dict. Here is a description of each function.\n\n    get_qparams_func:\n        A function that calculates the quantization qparams for an input tensor.\n        Args:\n            weight: A 2d weight tensor with non-integer dtype.\n        Returns:\n            qparams: it can have any format but will need to be handled by the other defined functions below.\n\n    quantize_func:\n        A function that applies quantization to an input tensor. It should be noted\n        that this function needs to be able to handle quantizing the entire weight tensor, a single group,\n        or a single column.\n        Args:\n            weight: A 2d weight tensor with non-integer dtype.\n            qparams: the output from get_qparams_func\n        Returns:\n            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)\n\n\n    dequantize_func:\n        A function that dequantizes an input quantized weight tensor. It should be noted\n        that this function needs to be able to handle dequantizing the entire weight tensor, a single group,\n        or a single column.\n        Args:\n            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)\n            qparams: the output from get_qparams_func\n        Returns:\n            weight: A 2d weight tensor with non-integer dtype.\n\n    combine_qparams_list_func:\n        A function that combines several qparams into one qparam.\n        Args:\n            qparams_list: a list of qparams objects, each obtained by calling get_qparams_func\n            on a single group from a weight tensor\n        Returns:\n            qparams: an object of the same format as the qparams above.\n\n    skip_layer_func:\n        A function that determines which linear layers should be skipped during GPTQ\n        Args:\n            weight: A 2d weight tensor with non-integer dtype.\n        Returns:\n            skip: boolean indicating whether layer should be skipped\n\n    make_names_and_values_dict_func:\n        A function that prepares the qparams and quantized_weight and creates a dictionary indicating how they\n        should be inserted into the state_dict. Generally any packing of the weight and qparams should be done here.\n        Args:\n            quantized_weight: A 2d quantized weight tensor (generally with an integer dtype)\n            qparams: the output from get_qparams_func\n        Returns:\n            names_and_values_dict: a dictionary mapping the name of the parameters of the quantized module to the\n            corresponding quantized weights and qparams.\n    \"\"\"\n    def __init__(self):\n        assert self.mod is not None\n        assert self.get_qparams_func is not None\n        assert self.quantize_func is not None\n        assert self.dequantize_func is not None\n        assert self.combine_qparams_list_func is not None\n        assert self.make_names_and_values_dict_func is not None\n\n    @staticmethod\n    def get_inputs(model, tokenizer, calibration_tasks, calibration_limit, calibration_seq_length, pad_calibration_inputs) -> \"MultiInput\":\n        input_recorder = InputRecorder(\n            model,\n            tokenizer,\n            calibration_seq_length,\n            pad_calibration_inputs,\n        )\n\n        try:\n            lm_eval.tasks.initialize_tasks()\n        except:\n            pass\n        task_dict = get_task_dict(calibration_tasks)\n        print(\"Obtaining GPTQ calibration inputs on: \", calibration_tasks)\n\n        evaluate(\n            input_recorder,\n            task_dict,\n            limit=calibration_limit,\n        )\n        inputs = input_recorder.get_recorded_inputs()\n        assert inputs is not None, (\n            f\"No inputs were collected, use a task other than {calibration_tasks}, \"+\n            f\"use option pad_calibration_inputs, or decrease calibration_sequence_length (currently \"+\n            f\"{calibration_seq_length})\"\n        )\n        print(f\"Obtained {len(inputs[0].values)} calibration samples\")\n        return inputs\n\n    @torch.no_grad()\n    def create_quantized_state_dict(\n        self,\n        tokenizer,\n        blocksize,\n        percdamp,\n        groupsize,\n        calibration_tasks,\n        calibration_limit,\n        calibration_seq_length,\n        pad_calibration_inputs,\n    ) -> \"StateDict\":\n        inputs = GPTQQuantHandler.get_inputs(self.mod, tokenizer, calibration_tasks, calibration_limit, calibration_seq_length, pad_calibration_inputs)\n        print(\"Tracing model for GPTQ\")\n        GPTQ_runner = GenericGPTQRunner(\n            self.mod,\n            inputs,\n            blocksize,\n            percdamp,\n            groupsize,\n        ).configure_quantization_mode(\n            self.get_qparams_func,\n            self.quantize_func,\n            self.dequantize_func,\n            self.combine_qparams_list_func,\n            self.make_names_and_values_dict_func,\n            self.skip_layer_func\n        )\n\n        print(\"Applying GPTQ to weights\")\n        GPTQ_runner.run()\n        return GPTQ_runner.get_quantized_state_dict()\n\n    def convert_for_runtime(self) -> \"nn.Module\":\n        pass\n\n##### Weight-only int8 per-channel quantized code ######\n\ndef replace_linear_weight_only_int8_per_channel(module):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear):\n            setattr(module, name, WeightOnlyInt8Linear(child.in_features, child.out_features))\n        else:\n            replace_linear_weight_only_int8_per_channel(child)\n\nclass WeightOnlyInt8QuantHandler:\n    def __init__(self, mod):\n        self.mod = mod\n\n    @torch.no_grad()\n    def create_quantized_state_dict(self):\n        cur_state_dict = self.mod.state_dict()\n        for fqn, mod in self.mod.named_modules():\n            if isinstance(mod, torch.nn.Linear):\n                int8_weight, scales, _ = dynamically_quantize_per_channel(mod.weight.float(), -128, 127, torch.int8)\n                cur_state_dict[f\"{fqn}.weight\"] = int8_weight\n                cur_state_dict[f\"{fqn}.scales\"] = scales.to(mod.weight.dtype)\n\n        return cur_state_dict\n\n    def convert_for_runtime(self):\n        replace_linear_weight_only_int8_per_channel(self.mod)\n        return self.mod\n\n\nclass WeightOnlyInt8Linear(torch.nn.Module):\n    __constants__ = ['in_features', 'out_features']\n    in_features: int\n    out_features: int\n    weight: torch.Tensor\n\n    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.register_buffer(\"weight\", torch.empty((out_features, in_features), dtype=torch.int8))\n        self.register_buffer(\"scales\", torch.ones(out_features, dtype=torch.bfloat16))\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return F.linear(input, self.weight.to(dtype=input.dtype)) * self.scales\n\n##### weight only int4 per channel groupwise quantized code ######\n\ndef prepare_int4_weight_and_scales_and_zeros(weight_bf16, groupsize, inner_k_tiles):\n    weight_int32, scales_and_zeros = group_quantize_tensor(\n        weight_bf16, n_bit=4, groupsize=groupsize\n    )\n    weight_int4pack = torch.ops.aten._convert_weight_to_int4pack(weight_int32, inner_k_tiles)\n    return weight_int4pack, scales_and_zeros\n\n\ndef linear_forward_int4(x, weight_int4pack, scales_and_zeros, out_features, groupsize):\n    origin_x_size = x.size()\n    x = x.reshape(-1, origin_x_size[-1])\n    c = torch.ops.aten._weight_int4pack_mm(x, weight_int4pack, groupsize, scales_and_zeros)\n    new_shape = origin_x_size[:-1] + (out_features,)\n    c = c.reshape(new_shape)\n    return c\n\n\ndef _check_linear_int4_k(k, groupsize = 1, inner_k_tiles = 1):\n    return k % groupsize == 0 and k % (inner_k_tiles * 16) == 0\n\ndef replace_linear_int4(module, groupsize, inner_k_tiles, padding):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear):\n            if _check_linear_int4_k(child.in_features, groupsize, inner_k_tiles):\n                setattr(module, name, WeightOnlyInt4Linear(\n                    child.in_features, child.out_features, bias=False,\n                    groupsize=groupsize, inner_k_tiles=inner_k_tiles, padding=False,\n                ))\n            elif padding:\n                setattr(module, name, WeightOnlyInt4Linear(\n                    child.in_features, child.out_features, bias=False,\n                    groupsize=groupsize, inner_k_tiles=inner_k_tiles, padding=True,\n                ))\n        else:\n            replace_linear_int4(child, groupsize, inner_k_tiles, padding)\n\n\nclass WeightOnlyInt4QuantHandler:\n    def __init__(self, mod, groupsize=128, inner_k_tiles=8, padding=True):\n        self.mod = mod\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n        self.padding = padding\n        assert groupsize in [32, 64, 128, 256]\n        assert inner_k_tiles in [2, 4, 8]\n\n    @torch.no_grad()\n    def create_quantized_state_dict(self, use_cuda = True):\n        if use_cuda:\n            device=\"cuda\"\n        else:\n            device=\"cpu\"\n\n        cur_state_dict = self.mod.state_dict()\n        for fqn, mod in self.mod.named_modules():\n            if isinstance(mod, torch.nn.Linear):\n                assert not mod.bias\n                out_features = mod.out_features\n                in_features = mod.in_features\n                assert out_features % 8 == 0, \"require out_features % 8 == 0\"\n                print(f\"linear: {fqn}, in={in_features}, out={out_features}\")\n\n                weight = mod.weight.data\n                if not _check_linear_int4_k(in_features, self.groupsize, self.inner_k_tiles):\n                    if self.padding:\n                        from model import find_multiple\n                        import torch.nn.functional as F\n                        print(f\"warning: {fqn} is padded to satisfy in_features % 1024 == 0\")\n                        padded_in_features = find_multiple(in_features, 1024)\n                        weight = F.pad(weight, pad=(0, padded_in_features - in_features))\n                    else:\n                        print(f\"warning: {fqn} is skipped, int4 requires that in_features is 32, 64, or is divisible by 1024, \" +\n                            \"and that groupsize and inner_k_tiles*16 evenly divide into it\")\n                        continue\n                weight_int4pack, scales_and_zeros = prepare_int4_weight_and_scales_and_zeros(\n                    weight.to(torch.bfloat16).to(device=device), self.groupsize, self.inner_k_tiles\n                )\n                cur_state_dict[f\"{fqn}.weight\"] = weight_int4pack.to('cpu')\n                cur_state_dict[f\"{fqn}.scales_and_zeros\"] = scales_and_zeros.to('cpu')\n\n        return cur_state_dict\n\n    def convert_for_runtime(self):\n        replace_linear_int4(self.mod, self.groupsize, self.inner_k_tiles, self.padding)\n        return self.mod\n\nclass WeightOnlyInt4GPTQQuantHandler(GPTQQuantHandler):\n    def __init__(self, mod, groupsize=128, inner_k_tiles=8, padding=True):\n        from model import find_multiple\n        self.mod = mod\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n        self.padding = padding\n        self.get_qparams_func = lambda w: get_group_qparams(w, 4, groupsize)\n        self.quantize_func = lambda w, qparams: \\\n            group_quantize_tensor_from_qparams(w, qparams[0], qparams[1], 4, groupsize)\n        self.dequantize_func = lambda q, qparams: \\\n            group_dequantize_tensor_from_qparams(q, qparams[0], qparams[1], 4, groupsize).float()\n        self.combine_qparams_list_func = lambda qparams_list: \\\n            [torch.cat(x, dim=1) for x in zip(*qparams_list)]\n        # skip unless padding=True or its correctly sized\n        self.skip_layer_func = lambda linear_weight: not (\n            _check_linear_int4_k(linear_weight.shape[-1], groupsize, inner_k_tiles) or padding\n        )\n        # we need to do the padding here, both for q and the qparams if necessary\n        def make_names_and_values_dict_func(q, qparams):\n            k = q.shape[1]\n            new_k = find_multiple(k, 1024)\n            # how much we need to pad the weight\n            delta_k = new_k - q.shape[1]\n            final_q = torch.ops.aten._convert_weight_to_int4pack(F.pad(q, pad=(0, delta_k)), inner_k_tiles)\n            scales_and_zeros = pack_scales_and_zeros(*qparams)\n            # how many new groups we need for padded weight\n            delta_groups = new_k // groupsize - scales_and_zeros.shape[0]\n            final_s_and_z = F.pad(scales_and_zeros, pad=(0,0,0,0,0, delta_groups), value=1)\n            return {\"weight\": final_q, \"scales_and_zeros\": final_s_and_z}\n        self.make_names_and_values_dict_func = make_names_and_values_dict_func\n        super().__init__()\n\n\n    def convert_for_runtime(self):\n        replace_linear_int4(self.mod, self.groupsize, self.inner_k_tiles, self.padding)\n        return self.mod\n\nclass WeightOnlyInt4Linear(torch.nn.Module):\n    __constants__ = ['in_features', 'out_features']\n    in_features: int\n    out_features: int\n    weight: torch.Tensor\n\n    def __init__(\n            self, in_features: int, out_features: int,\n            bias=True, device=None, dtype=None, groupsize: int = 128, inner_k_tiles: int = 8, padding: bool = True,\n    ) -> None:\n        super().__init__()\n        self.padding = padding\n        if padding:\n            from model import find_multiple\n            self.origin_in_features = in_features\n            in_features = find_multiple(in_features, 1024)\n\n        self.in_features = in_features\n        self.out_features = out_features\n        assert not bias, \"require bias=False\"\n        self.groupsize = groupsize\n        self.inner_k_tiles = inner_k_tiles\n\n        assert out_features % 8 == 0, \"require out_features % 8 == 0\"\n        assert in_features % (inner_k_tiles * 16) == 0, \"require in_features % (innerKTiles * 16) == 0\"\n        self.register_buffer(\n            \"weight\",\n            torch.empty((out_features // 8, in_features // (inner_k_tiles * 16), 32, inner_k_tiles // 2), dtype=torch.int32)\n        )\n        self.register_buffer(\n            \"scales_and_zeros\",\n            torch.empty((in_features // groupsize, out_features, 2), dtype=torch.bfloat16)\n        )\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        input = input.to(torch.bfloat16)\n        if self.padding:\n            import torch.nn.functional as F\n            input = F.pad(input, pad=(0, self.in_features - self.origin_in_features))\n        return linear_forward_int4(\n            input,\n            self.weight, self.scales_and_zeros, self.out_features, self.groupsize\n        )\n\n\ndef quantize(\n    checkpoint_path: Path = Path(\"checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth\"),\n    mode: str = 'int8',\n    # following arguments only available when setting int4 quantization.\n    groupsize: int = 128,\n    # following arguments only used for GPTQ\n    calibration_tasks: list = [\"hellaswag\"],\n    calibration_limit: int = 1000,\n    calibration_seq_length: int = 100,\n    pad_calibration_inputs: bool = False,\n    percdamp: float = .01,\n    blocksize: int = 128,\n    label: str = '',\n) -> None:\n    assert checkpoint_path.is_file(), checkpoint_path\n\n    device = 'cpu'\n    precision = torch.bfloat16\n\n    print(\"Loading model ...\")\n    t0 = time.time()\n\n    with torch.device('meta'):\n        model = Transformer.from_name(checkpoint_path.parent.name)\n\n    checkpoint = torch.load(str(checkpoint_path), mmap=True, weights_only=True)\n    model.load_state_dict(checkpoint, assign=True)\n    model = model.to(dtype=precision, device=device)\n\n    if mode == 'int8':\n        print(\"Quantizing model weights for int8 weight-only symmetric per-channel quantization\")\n        quant_handler = WeightOnlyInt8QuantHandler(model)\n        quantized_state_dict = quant_handler.create_quantized_state_dict()\n\n        dir_name = checkpoint_path.parent\n        base_name = checkpoint_path.name\n        new_base_name = base_name.replace('.pth', f'{label}int8.pth')\n\n    elif mode == 'int4':\n        print(\"Quantizing model weights for int4 weight-only affine per-channel groupwise quantization\")\n        quant_handler = WeightOnlyInt4QuantHandler(model, groupsize)\n        quantized_state_dict = quant_handler.create_quantized_state_dict()\n\n        dir_name = checkpoint_path.parent\n        base_name = checkpoint_path.name\n        new_base_name = base_name.replace('.pth', f\"{label}int4.g{groupsize}.pth\")\n\n    elif mode == 'int4-gptq':\n        print(\"Quantizing model weights for int4 weight-only affine per-channel groupwise quantization using GPTQ...\")\n        quant_handler = WeightOnlyInt4GPTQQuantHandler(model, groupsize)\n\n        tokenizer_path = checkpoint_path.parent / \"tokenizer.model\"\n        assert tokenizer_path.is_file(), str(tokenizer_path)\n        tokenizer = get_tokenizer(tokenizer_path, checkpoint_path)\n\n        quantized_state_dict = quant_handler.create_quantized_state_dict(\n            tokenizer,\n            blocksize,\n            percdamp,\n            groupsize,\n            calibration_tasks,\n            calibration_limit,\n            calibration_seq_length,\n            pad_calibration_inputs\n        )\n\n        dir_name = checkpoint_path.parent\n        base_name = checkpoint_path.name\n        new_base_name = base_name.replace('.pth', f\"{label}int4-gptq.g{groupsize}.pth\")\n    else:\n        raise ValueError(f\"Invalid quantization mode {mode} needs to be one of [int8, int4, int4-gpptq]\")\n\n    quantize_path = dir_name / new_base_name\n    print(f\"Writing quantized weights to {quantize_path}\")\n    quantize_path.unlink(missing_ok=True) # remove existing file if one already there\n    torch.save(quantized_state_dict, quantize_path)\n    print(f\"Quantization complete took {time.time() - t0:.02f} seconds\")\n    return\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Quantize a model.')\n    parser.add_argument('--checkpoint_path', type=Path, default=Path(\"checkpoints/meta-llama/Llama-2-7b-chat-hf/model.pth\"), help='Path to the model checkpoint to be quantized.')\n    parser.add_argument('--mode', '-q', type=str, default='int8', choices=['int8', 'int4', 'int4-gptq'], help='type of quantization to perform')\n    parser.add_argument('--groupsize', type=int, default=32, help='Group size for int4 quantization.')\n    parser.add_argument('--calibration_tasks', type=str, nargs='+', default=['wikitext'], help='tasks to do gptq calibration on, if doing gptq')\n    parser.add_argument('--calibration_limit', type=int, default=1000, help='number of samples to use for gptq calibration')\n    parser.add_argument('--calibration_seq_length', type=int, default=100, help='length of sequences to use for gptq calibration')\n    parser.add_argument('--pad_calibration_inputs', type=bool, default=False, help='pads sequences shorter than calibration_seq_length to that length, yielding more calibration inputs but running much slower')\n    parser.add_argument('--percdamp', type=float, default=.01, help='gptq percentage dampening')\n    parser.add_argument('--blocksize', type=int, default=128, help='blocksize for gptq')\n    parser.add_argument('--label', type=str, default='_', help='label to add to output filename')\n\n    args = parser.parse_args()\n    quantize(args.checkpoint_path, args.mode, args.groupsize, args.calibration_tasks, args.calibration_limit, args.calibration_seq_length, args.pad_calibration_inputs, args.percdamp, args.blocksize, args.label)\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.048828125,
          "content": "torch\nsentencepiece\ntiktoken\nblobfile\nsafetensors\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.5751953125,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nfrom setuptools import setup, find_packages\n\nsetup(\n    name='gpt-fast',\n    version='0.1',\n    packages=find_packages(),\n    install_requires=[\n        'torch',\n    ],\n    description='A simple, fast, pure PyTorch Llama inference engine',\n    long_description=open('README.md').read(),\n    long_description_content_type='text/markdown',\n    url='https://github.com/pytorch-labs/gpt-fast',\n)\n"
        },
        {
          "name": "tokenizer.py",
          "type": "blob",
          "size": 3.537109375,
          "content": "import os\nimport sentencepiece as spm\nimport tiktoken\nfrom tiktoken.load import load_tiktoken_bpe\nfrom pathlib import Path\nfrom typing import Dict\n\nclass TokenizerInterface:\n    def __init__(self, model_path):\n        self.model_path = model_path\n\n    def encode(self, text):\n        raise NotImplementedError(\"This method should be overridden by subclasses.\")\n\n    def decode(self, tokens):\n        raise NotImplementedError(\"This method should be overridden by subclasses.\")\n\n    def bos_id(self):\n        raise NotImplementedError(\"This method should be overridden by subclasses.\")\n\n    def eos_id(self):\n        raise NotImplementedError(\"This method should be overridden by subclasses.\")\n\nclass SentencePieceWrapper(TokenizerInterface):\n    def __init__(self, model_path):\n        super().__init__(model_path)\n        self.processor = spm.SentencePieceProcessor(str(model_path))\n\n    def encode(self, text):\n        return self.processor.EncodeAsIds(text)\n\n    def decode(self, tokens):\n        return self.processor.DecodeIds(tokens)\n\n    def bos_id(self):\n        return self.processor.bos_id()\n\n    def eos_id(self):\n        return self.processor.eos_id()\n\nclass TiktokenWrapper(TokenizerInterface):\n    \"\"\"\n    Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n    \"\"\"\n\n    special_tokens: Dict[str, int]\n\n    num_reserved_special_tokens = 256\n\n    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n\n    def __init__(self, model_path):\n        super().__init__(model_path)\n        assert os.path.isfile(model_path), str(model_path)\n        mergeable_ranks = load_tiktoken_bpe(str(model_path))\n        num_base_tokens = len(mergeable_ranks)\n        special_tokens = [\n            \"<|begin_of_text|>\",\n            \"<|end_of_text|>\",\n            \"<|reserved_special_token_0|>\",\n            \"<|reserved_special_token_1|>\",\n            \"<|reserved_special_token_2|>\",\n            \"<|reserved_special_token_3|>\",\n            \"<|start_header_id|>\",\n            \"<|end_header_id|>\",\n            \"<|reserved_special_token_4|>\",\n            \"<|eot_id|>\",  # end of turn\n        ] + [\n            f\"<|reserved_special_token_{i}|>\"\n            for i in range(5, self.num_reserved_special_tokens - 5)\n        ]\n        self.special_tokens = {\n            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n        }\n        self.model = tiktoken.Encoding(\n            name=Path(model_path).name,\n            pat_str=self.pat_str,\n            mergeable_ranks=mergeable_ranks,\n            special_tokens=self.special_tokens,\n        )\n        # BOS / EOS token IDs\n        self._bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n        self._eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n\n    def encode(self, text):\n        return self.model.encode(text)\n\n    def decode(self, tokens):\n        return self.model.decode(tokens)\n\n    def bos_id(self):\n        return self._bos_id\n\n    def eos_id(self):\n        return self._eos_id\n\ndef get_tokenizer(tokenizer_model_path, model_name):\n    \"\"\"\n    Factory function to get the appropriate tokenizer based on the model name.\n    \n    Args:\n    - tokenizer_model_path (str): The file path to the tokenizer model.\n    - model_name (str): The name of the model, used to determine the tokenizer type.\n\n    Returns:\n    - TokenizerInterface: An instance of a tokenizer.\n    \"\"\"\n\n    if \"llama-3\" in str(model_name).lower():\n        return TiktokenWrapper(tokenizer_model_path)\n    else:\n        return SentencePieceWrapper(tokenizer_model_path)\n"
        },
        {
          "name": "tp.py",
          "type": "blob",
          "size": 5.1875,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\nimport os\nfrom typing import List, Optional\n\nimport torch\nimport torch.distributed as dist\nfrom torch import nn\nif os.uname().sysname != \"Darwin\":\n    from torch.distributed import _functional_collectives as funcol\nelse:\n    # Distributed is not supported on MacOS\n    funcol = None\n\nfrom model import Attention, FeedForward, Transformer\nfrom quantize import WeightOnlyInt4Linear\n\n\ndef _get_rank() -> int:\n    return int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n\ndef is_local():\n    return _get_rank() == 0\n\ndef local_break():\n    if is_local():\n        breakpoint()\n    dist.barrier()\n\ndef _get_world_size() -> int:\n    return int(os.environ.get(\"LOCAL_WORLD_SIZE\", \"1\"))\n\ndef maybe_init_dist() -> Optional[int]:\n    try:\n        # provided by torchrun\n        rank = _get_rank()\n        world_size = _get_world_size()\n\n        if world_size < 2:\n            # too few gpus to parallelize, tp is no-op\n            return None\n    except KeyError:\n        # not run via torchrun, no-op\n        return None\n\n    torch.cuda.set_device(rank)\n    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n    return rank\n\n\ndef _apply_tp_linear(linear: nn.Linear, style: str, weight_splits: List[int] = []) -> None:\n    rank = _get_rank()\n    world_size = _get_world_size()\n\n    # Linear's weight matrix is transposed, and is of shape\n    # (linear.out_features, linear.in_features)\n    dim_lookup = {\n        \"colwise\": (0, \"out_features\"),\n        \"rowwise\": (1, \"in_features\")\n    }\n    assert style in dim_lookup\n    shard_dim, size_attr = dim_lookup[style]\n\n    # ensure we can shard evenly\n    assert getattr(linear, size_attr) % world_size == 0\n    def shard(x, dim):\n        assert x.size(dim=dim) % world_size == 0\n        return torch.tensor_split(x, world_size, dim=dim)[rank]\n\n    def shard_qkv(qkv, dim, weight_splits):\n        q, k, v = qkv.split(weight_splits, dim=dim)\n        q = shard(q, dim)\n        k = shard(k, dim)\n        v = shard(v, dim)\n        return torch.cat((q,k,v), dim=dim)\n\n    # shard\n    if weight_splits:\n        # attention\n        assert len(weight_splits) == 3\n\n        if isinstance(linear, WeightOnlyInt4Linear):\n            sharded_weight = shard_qkv(linear.weight, shard_dim, [i//8 for i in weight_splits])\n            linear.scales_and_zeros = shard_qkv(linear.scales_and_zeros, 1 - shard_dim, weight_splits)\n        else:\n            sharded_weight = shard_qkv(linear.weight, shard_dim, weight_splits)\n        if hasattr(linear, \"scales\") and style == \"colwise\":\n            linear.scales = shard_qkv(linear.scales, 0, weight_splits)\n    else:\n        sharded_weight = shard(linear.weight, shard_dim)\n        if isinstance(linear, WeightOnlyInt4Linear):\n            linear.scales_and_zeros = shard(linear.scales_and_zeros, 1 - shard_dim)\n            if style == \"rowwise\":\n                assert linear.scales_and_zeros.shape[0] * 32 == sharded_weight.shape[1] * sharded_weight.shape[2] * sharded_weight.shape[3]\n                assert linear.scales_and_zeros.shape[1] == sharded_weight.shape[0] * 8\n        if hasattr(linear, \"scales\") and style == \"colwise\":\n            linear.scales = shard(linear.scales, 0)\n\n    # local_break()\n    linear.weight = nn.Parameter(sharded_weight, requires_grad=False)\n    setattr(linear, size_attr, getattr(linear, size_attr) // world_size)\n\n    # shape info should still be synced\n    # assert linear.weight.shape == (linear.out_features, linear.in_features)\n\n\ndef _apply_tp_ffn(mlp: FeedForward) -> None:\n    assert hasattr(mlp, \"w1\")\n    assert hasattr(mlp, \"w3\")\n    assert hasattr(mlp, \"w2\")\n\n    _apply_tp_linear(mlp.w1, \"colwise\")\n    _apply_tp_linear(mlp.w3, \"colwise\")\n    _apply_tp_linear(mlp.w2, \"rowwise\")\n\n    world_size = _get_world_size()\n    mlp.register_forward_hook(lambda _module, _input, output: funcol.all_reduce(\n        output, \"sum\", list(range(world_size))))\n\n\ndef _apply_tp_attn(attn: Attention) -> None:\n    assert hasattr(attn, \"wqkv\")\n    assert hasattr(attn, \"wo\")\n\n    kv_size = attn.n_local_heads * attn.head_dim\n    _apply_tp_linear(attn.wqkv, \"colwise\", [attn.dim, kv_size, kv_size])\n    _apply_tp_linear(attn.wo, \"rowwise\")\n\n    # overwrite\n    world_size = _get_world_size()\n    attn.n_head = attn.n_head // world_size\n    attn.dim = attn.dim // world_size\n    attn.head_dim = attn.dim // attn.n_head\n    attn.n_local_heads = attn.n_local_heads // world_size\n\n    attn.register_forward_hook(lambda _module, _input, output: funcol.all_reduce(\n        output[0], \"sum\", list(range(world_size))))\n\n\ndef _apply_tp_Transformer(Transformer: Transformer) -> None:\n    # overwrite config before Transformer.setup_cache is called\n    world_size = _get_world_size()\n    Transformer.config.n_head = Transformer.config.n_head // world_size\n    Transformer.config.dim = Transformer.config.dim // world_size\n    Transformer.config.n_local_heads = Transformer.config.n_local_heads // world_size\n\n\ndef apply_tp(model: Transformer) -> None:\n    _apply_tp_Transformer(model)\n    for block in model.layers:\n        # Apply to MLP\n        _apply_tp_ffn(block.feed_forward)\n        _apply_tp_attn(block.attention)\n"
        }
      ]
    }
  ]
}