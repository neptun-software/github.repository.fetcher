{
  "metadata": {
    "timestamp": 1736560522248,
    "page": 119,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "OlafenwaMoses/ImageAI",
      "stars": 8689,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".codecov.yml",
          "type": "blob",
          "size": 0.3466796875,
          "content": "codecov:\n  notify:\n    require_ci_to_pass: yes\n\ncoverage:\n  precision: 2\n  round: down\n  range: \"30...100\"\n\n  status:\n    project: yes\n    patch: yes\n    changes: no\n\nparsers:\n  gcov:\n    branch_detection:\n      conditional: yes\n      loop: yes\n      method: no\n      macro: no\n\ncomment:\n  layout: \"header, diff\"\n  behavior: default\n  require_changes: no\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.1259765625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n\n\n# Other files and folders\ntest/data-models\ntest/data-images\ntest/data-json\ntest/data-videos\ntest/data-datasets\nexperiment"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 1.74609375,
          "content": "dist: xenial\nsudo: required\nlanguage: python\npython:\n  - '3.7.6'\ninstall:\n  - pip install -r requirements.txt\n  - pip install pytest\n  - pip install pytest-cov\nscript:\n  - python setup.py install\n  - cd test\n  - mkdir data-models\n  - mkdir data-temp\n  - wget -P data-models/ https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/DenseNet-BC-121-32.h5\n  - wget -P data-models/ https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n  - wget -P data-models/ https://github.com/OlafenwaMoses/ImageAI/releases/download/essentials-v5/resnet50_imagenet_tf.2.0.h5\n  - wget -P data-models/ https://github.com/OlafenwaMoses/ImageAI/releases/download/essentials-v5/mobilenet_v2.h5\n  - wget -P data-models/ https://github.com/OlafenwaMoses/ImageAI/releases/download/models-v3/idenprof_densenet-0.763500.h5\n  - wget -P data-models/ https://github.com/OlafenwaMoses/ImageAI/releases/download/models-v3/idenprof_full_resnet_ex-001_acc-0.119792.h5\n  - wget -P data-models/ https://github.com/OlafenwaMoses/ImageAI/releases/download/essentials-v5/idenprof_resnet_ex-056_acc-0.993062.h5\n  - wget -P data-models/ https://github.com/OlafenwaMoses/ImageAI/releases/download/essentials-v5/resnet50_coco_best_v2.1.0.h5\n  - wget -P data-models/ https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo.h5\n  - wget -P data-models/ https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/yolo-tiny.h5\n  - wget -P data-models/ https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/pretrained-yolov3.h5\n  - wget -P data-models/ https://github.com/OlafenwaMoses/ImageAI/releases/download/essential-v4/hololens-ex-60--loss-2.76.h5\n  - pytest -v --cov\nafter_script:\n  - bash <(curl -s https://codecov.io/bash)\n\n\n\n\n"
        },
        {
          "name": "BACKEND_MIGRATION.md",
          "type": "blob",
          "size": 2.431640625,
          "content": "# Overview\n\nIn December 2022, ImageAI `3.0.2` was released which effected the change from Tensorflow backend to PyTorch backend. This change allows ImageAI to support `Python 3.7` up to `Python 3.10` for all its features and deprecates a number of functionalities for this and future versions of ImageAI.\n\n\n# Deprecated functionalities\n- Tensorflow backend no longer supported. Now replaced with PyTorch\n- All `.h5` pretrained models and custom trained `.h5` models no longer supported. If you still intend to use these models, see the `Using Tensorflow backend` section.\n- `Speed mode` have been removed from model loading\n- Custom detection model training dataset format changed to YOLO format from Pascal VOC. To convert your dataset to YOLO format, see the  `Convert Pascal VOC dataset to YOLO format` section.\n- Enhance data for custom classification model training now removed\n- Detection model training standalone evaluation now removed\n\n# Using Tensorflow backend\nTo use Tensorflow backend, do the following\n\n- Install Python 3.7\n- Install Tensorflow \n  - CPU: `pip install tensorflow==2.4.0`\n  - GPU: `pip install tensorflow-gpu==2.4.0`\n- Install other dependencies: `pip install keras==2.4.3 numpy==1.19.3 pillow==7.0.0 scipy==1.4.1 h5py==2.10.0 matplotlib==3.3.2 opencv-python keras-resnet==0.2.0`\n- Install ImageAI **2.1.6**: `pip install imageai==2.1.6`\n- Download the Tensorflow models from the releases below\n  - [Models for Image Recognition and Object Detection](https://github.com/OlafenwaMoses/ImageAI/releases/tag/1.0)\n  - [TF2.x Models [ Exclusives ]](https://github.com/OlafenwaMoses/ImageAI/releases/tag/essentials-v5)\n\n\n\n# Convert Pascal VOC dataset to YOLO format\nBecause ImageAI now uses `YOLO format` for training custom object detection models; should you need to train a new model with the new ImageAI version, you will need to convert your `Pascal VOC` datasets to YOLO format by doing the following \n- Run the command below\n    ```\n    python scripts/pascal_voc_to_yolo.py --dataset_dir <path_to_your_dataset_folder>\n    ```\n- Once completed, you will find the YOLO version of the dataset next to your Pascal VOC dataset.\n  - E.g, if your dataset is in `C:/Users/Troublemaker/Documents/datasets/headset`, your conversion command will be\n    ```\n    python scripts/pascal_voc_to_yolo.py --dataset_dir C:/Users/Troublemaker/Documents/datasets/headset\n    ```\n    and once completed, the output will be in `C:/Users/Troublemaker/Documents/datasets/headset-yolo`\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0458984375,
          "content": "MIT License\n\nCopyright (c) 2019 MOSES OLAFENWA\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0859375,
          "content": "recursive-include imageai/Detection *.txt\nrecursive-include imageai/Classification *.txt"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.4814453125,
          "content": "# ImageAI (v3.0.3)\n\n\n\n[![Build Status](https://travis-ci.com/OlafenwaMoses/ImageAI.svg?branch=master)](https://travis-ci.com/OlafenwaMoses/ImageAI)  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://github.com/OlafenwaMoses/ImageAI/blob/master/LICENSE) [![PyPI version](https://badge.fury.io/py/imageai.svg)](https://badge.fury.io/py/imageai)   [![Downloads](https://pepy.tech/badge/imageai/month)](https://pepy.tech/project/imageai) [![Downloads](https://pepy.tech/badge/imageai/week)](https://pepy.tech/project/imageai)\n\nAn open-source python library built to empower developers to build applications and systems with self-contained Deep Learning and Computer Vision capabilities using simple and few lines of code.\n \n If you will like to sponsor this project, kindly visit the <strong>[Github sponsor page](https://github.com/sponsors/OlafenwaMoses)</strong>.\n \n \n## ---------------------------------------------------\n## Introducing Jarvis and TheiaEngine.\n\nWe the creators of ImageAI are glad to announce 2 new AI projects to provide state-of-the-art Generative AI, LLM and Image Understanding on your personal computer and servers. \n\n\n[![](jarvis.png)](https://jarvis.genxr.co)\n\nInstall Jarvis on PC/Mac to setup limitless access to LLM powered AI Chats for your every day work, research and generative AI needs with 100% privacy and full offline capability.\n\n\nVisit [https://jarvis.genxr.co](https://jarvis.genxr.co/) to get started.\n\n\n[![](theiaengine.png)](https://www.genxr.co/theia-engine)\n\n\n[TheiaEngine](https://www.genxr.co/theia-engine), the next-generation computer Vision AI API capable of all Generative and Understanding computer vision tasks in a single API call and available via REST API to all programming languages. Features include\n- **Detect 300+ objects** ( 220 more objects than ImageAI)\n- **Provide answers to any content or context questions** asked on an image\n  - very useful to get information on any object, action or information without needing to train a new custom model for every tasks\n-  **Generate scene description and summary**\n-  **Convert 2D image to 3D pointcloud and triangular mesh**\n-  **Semantic Scene mapping of objects, walls, floors, etc**\n-  **Stateless Face recognition and emotion detection**\n-  **Image generation and augmentation from prompt**\n-  etc.\n\nVisit [https://www.genxr.co/theia-engine](https://www.genxr.co/theia-engine) to try the demo and join in the beta testing today.\n## ---------------------------------------------------\n \n![](logo1.png)\n\nDeveloped and maintained by [Moses Olafenwa](https://twitter.com/OlafenwaMoses)\n\n---\n\nBuilt with simplicity in mind, **ImageAI** \n    supports a list of state-of-the-art Machine Learning algorithms for image prediction, custom image prediction, object detection, video detection, video object tracking\n    and image predictions trainings. **ImageAI** currently supports image prediction and training using 4 different Machine Learning algorithms \n    trained on the ImageNet-1000 dataset. **ImageAI** also supports object detection, video detection and object tracking  using RetinaNet, YOLOv3 and TinyYOLOv3 trained on COCO dataset. Finally, **ImageAI** allows you to train custom models for performing detection and recognition of new objects. \n   \nEventually, **ImageAI** will provide support for a wider and more specialized aspects of Computer Vision\n\n\n**New Release : ImageAI 3.0.2**\n\nWhat's new:\n- PyTorch backend\n- TinyYOLOv3 model training\n\n\n### TABLE OF CONTENTS\n- <a href=\"#installation\" > :white_square_button: Installation</a>\n- <a href=\"#features\" > :white_square_button: Features</a>\n- <a href=\"#documentation\" > :white_square_button: Documentation</a>\n- <a href=\"#sponsors\" > :white_square_button: Sponsors</a>\n- <a href=\"#sample\" > :white_square_button: Projects Built on ImageAI</a>\n- <a href=\"#real-time-and-high-performance-implementation\" > :white_square_button: High Performance Implementation</a>\n- <a href=\"#recommendation\" > :white_square_button: AI Practice Recommendations</a>\n- <a href=\"#contact\" > :white_square_button: Contact Developers</a>\n- <a href=\"#citation\" > :white_square_button: Citation</a>\n- <a href=\"#ref\" > :white_square_button: References</a>\n\n\n\n## Installation\n<div id=\"installation\"></div>\n \nTo install ImageAI, run the python installation instruction below in the command line:\n\n- [Download and Install](https://www.python.org/downloads/) **Python 3.7**, **Python 3.8**, **Python 3.9** or **Python 3.10**\n- Install dependencies\n  - **CPU**: Download [requirements.txt](https://github.com/OlafenwaMoses/ImageAI/blob/master/requirements.txt) file and install via the command\n    ```\n    pip install -r requirements.txt\n    ```\n    or simply copy and run the command below\n\n    ```\n    pip install cython pillow>=7.0.0 numpy>=1.18.1 opencv-python>=4.1.2 torch>=1.9.0 --extra-index-url https://download.pytorch.org/whl/cpu torchvision>=0.10.0 --extra-index-url https://download.pytorch.org/whl/cpu pytest==7.1.3 tqdm==4.64.1 scipy>=1.7.3 matplotlib>=3.4.3 mock==4.0.3\n    ```\n\n  - **GPU/CUDA**: Download [requirements_gpu.txt](https://github.com/OlafenwaMoses/ImageAI/blob/master/requirements_gpu.txt) file and install via the command\n    ```\n    pip install -r requirements_gpu.txt\n    ```\n    or smiply copy and run the command below\n    ```\n    pip install cython pillow>=7.0.0 numpy>=1.18.1 opencv-python>=4.1.2 torch>=1.9.0 --extra-index-url https://download.pytorch.org/whl/cu102 torchvision>=0.10.0 --extra-index-url https://download.pytorch.org/whl/cu102 pytest==7.1.3 tqdm==4.64.1 scipy>=1.7.3 matplotlib>=3.4.3 mock==4.0.3\n    ```\n- If you plan to train custom AI models, download [requirements_extra.txt](https://github.com/OlafenwaMoses/ImageAI/blob/master/requirements_extra.txt) file and install via the command\n  \n  ```\n  pip install -r requirements_extra.txt\n  ```\n  or simply copy and run the command below\n  ```\n  pip install pycocotools@git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI\n  ```\n- Then run the command below to install ImageAI\n  ```\n  pip install imageai --upgrade\n  ```\n\n## Features\n<div id=\"features\"></div>\n<table>\n  <tr>\n    <td><h2> Image Classification</h2> </td>\n  </tr>\n  <tr>\n    <td><img src=\"data-images/1.jpg\" >\n    <h4>ImageAI provides 4 different algorithms and model types to perform image prediction, trained on the ImageNet-1000 dataset. The 4 algorithms provided for image prediction include MobileNetV2, ResNet50, InceptionV3 and DenseNet121.\n    Click the link below to see the full sample codes, explanations and best practices guide.</h4>\n    <a href=\"imageai/Classification\"> >>> Get Started</a>\n    </td>\n  </tr>\n  \n </table>\n\n <div id=\"features\"></div>\n<table>\n  <tr>\n    <td><h2> Object Detection </h2> </td>\n  </tr>\n  <tr>\n    <td>\n        <img src=\"data-images/image2new.jpg\">\n        <h4>ImageAI provides very convenient and powerful methods to perform object detection on images and extract each object from the image. The object detection class provides support for RetinaNet, YOLOv3 and TinyYOLOv3, with options to adjust for state of the art performance or real time processing. Click the link below to see the full sample codes, explanations and best practices guide.</h4>\n    <a href=\"imageai/Detection\"> >>> Get Started</a>\n    </td>\n  </tr>\n  \n </table>\n\n\n<table>\n  <tr>\n    <td><h2> Video Object Detection & Analysis</h2> </td>\n  </tr>\n  <tr>\n    <td><img src=\"data-images/video_analysis_visualization.jpg\">\n    <h4>ImageAI provides very convenient and powerful methods to perform object detection in videos. The video object detection class provided only supports the current state-of-the-art RetinaNet. Click the link to see the full videos, sample codes, explanations and best practices guide.</h4>\n    <a href=\"imageai/Detection/VIDEO.md\"> >>> Get Started</a>\n    </td>\n  </tr>\n  \n </table>\n\n\n <table>\n  <tr>\n    <td><h2> Custom Classification model training </h2> </td>\n  </tr>\n  <tr>\n    <td>\n        <img src=\"data-images/idenprof.jpg\">\n        <h4>ImageAI provides classes and methods for you to train a new model that can be used to perform prediction on your own custom objects. You can train your custom models using MobileNetV2, ResNet50, InceptionV3 and DenseNet in 5 lines of code. Click the link below to see the guide to preparing training images, sample training codes, explanations and best practices.</h4>\n    <a href=\"imageai/Classification/CUSTOMTRAINING.md\"> >>> Get Started</a>\n    </td>\n  </tr>\n  \n </table>\n\n <table>\n  <tr>\n    <td><h2> Custom Model Classification</h2> </td>\n  </tr>\n  <tr>\n    <td><img src=\"data-images/4.jpg\">\n    <h4>ImageAI provides classes and methods for you to run image prediction your own custom objects using your own model trained with ImageAI Model Training class. You can use your custom models trained with MobileNetV2, ResNet50, InceptionV3 and DenseNet and the JSON file containing the mapping of the custom object names. Click the link below to see the guide to sample training codes, explanations, and best practices guide.</h4>\n    <a href=\"imageai/Classification/CUSTOMCLASSIFICATION.md\"> >>> Get Started</a>\n    </td>\n  </tr>\n  \n </table>\n\n <table>\n  <tr>\n    <td><h2> Custom Detection Model Training </h2> </td>\n  </tr>\n  <tr>\n    <td>\n        <img src=\"data-images/headsets.jpg\">\n        <h4>ImageAI provides classes and methods for you to train new YOLOv3 or TinyYOLOv3 object detection models on your custom dataset. This means you can train a model to detect literally any object of interest by providing the images, the annotations and training with ImageAI. Click the link below to see the guide to sample training codes, explanations, and best practices guide.</h4>\n    <a href=\"imageai/Detection/Custom/CUSTOMDETECTIONTRAINING.md\"> >>> Get Started</a>\n    </td>\n  </tr>\n  \n </table>\n\n<table>\n  <tr>\n    <td><h2> Custom Object Detection</h2> </td>\n  </tr>\n  <tr>\n    <td><img src=\"data-images/holo2-detected.jpg\">\n    <h4>ImageAI now provides classes and methods for you detect and recognize your own custom objects in images using your own model trained with the DetectionModelTrainer class. You can use your custom trained YOLOv3 or TinyYOLOv3 model and the **.json** file generated during the training. Click the link below to see the guide to sample training codes, explanations, and best practices guide.</h4>\n    <a href=\"imageai/Detection/Custom/CUSTOMDETECTION.md\"> >>> Get Started</a>\n    </td>\n  </tr>\n </table>\n\n\n<table>\n  <tr>\n    <td><h2> Custom Video Object Detection & Analysis </h2> </td>\n  </tr>\n  <tr>\n    <td>\n        <img src=\"data-images/customvideodetection.gif\">\n        <h4>ImageAI now provides classes and methods for you detect and recognize your own custom objects in images using your own model trained with the DetectionModelTrainer class. You can use your custom trained YOLOv3 or TinyYOLOv3 model and the **.json** file generated during the training. Click the link below to see the guide to sample training codes, explanations, and best practices guide.</h4>\n    <a href=\"imageai/Detection/Custom/CUSTOMVIDEODETECTION.md\"> >>> Get Started</a>\n    </td>\n  </tr>\n </table>\n\n## Documentation\n<div id=\"documentation\"></div>\n\nWe have provided full documentation for all **ImageAI** classes and functions. Visit the link below:\n\n- Documentation - **English Version**  [https://imageai.readthedocs.io](https://imageai.readthedocs.io)\n\n\n## Sponsors\n<div id=\"sponsors\"></div>\n\n\n## Real-Time and High Performance Implementation\n<div id=\"performance\"></div>\n\n**ImageAI** provides abstracted and convenient implementations of state-of-the-art Computer Vision technologies. All of **ImageAI** implementations and code can work on any computer system with moderate CPU capacity. However, the speed of processing for operations like image prediction, object detection and others on CPU is slow and not suitable for real-time applications. To perform real-time Computer Vision operations with high performance, you need to use GPU enabled technologies.\n\n**ImageAI** uses the PyTorch backbone for it's Computer Vision operations. PyTorch supports both CPUs and GPUs ( Specifically NVIDIA GPUs.  You can get one for your PC or get a PC that has one) for machine learning and artificial intelligence algorithms' implementations.\n\n\n\n## Projects Built on ImageAI\n<div id=\"sample\"></div>\n\n\n\n## AI Practice Recommendations\n<div id=\"recommendation\"></div>\n\nFor anyone interested in building AI systems and using them for business, economic,  social and research purposes, it is critical that the person knows the likely positive, negative and unprecedented impacts the use of such technologies will have.\nThey must also be aware of approaches and practices recommended by experienced industry experts to ensure every use of AI brings overall benefit to mankind.\nWe therefore recommend to everyone that wishes to use ImageAI and other AI tools and resources to read Microsoft's January 2018 publication on AI titled \"The Future Computed : Artificial Intelligence and its role in society\".\nKindly follow the link below to download the publication.\n\n[https://blogs.microsoft.com/blog/2018/01/17/future-computed-artificial-intelligence-role-society](https://blogs.microsoft.com/blog/2018/01/17/future-computed-artificial-intelligence-role-society/)\n\n### Contact Developer\n<div id=\"contact\"></div>\n\n- **Moses Olafenwa**\n    * _Email:_ guymodscientist@gmail.com\n    * _Twitter:_ [@OlafenwaMoses](https://twitter.com/OlafenwaMoses)\n    * _Medium:_ [@guymodscientist](https://medium.com/@guymodscientist)\n    * _Facebook:_ [moses.olafenwa](https://facebook.com/moses.olafenwa)\n- **John Olafenwa**\n    * _Email:_ johnolafenwa@gmail.com\n    * _Website:_ [https://john.aicommons.science](https://john.aicommons.science)\n    * _Twitter:_ [@johnolafenwa](https://twitter.com/johnolafenwa)\n    * _Medium:_ [@johnolafenwa](https://medium.com/@johnolafenwa)\n    * _Facebook:_ [olafenwajohn](https://facebook.com/olafenwajohn)\n\n\n### Citation\n<div id=\"citation\"></div>\n\nYou can cite **ImageAI** in your projects and research papers via the **BibTeX** entry below.  \n  \n```\n@misc {ImageAI,\n    author = \"Moses\",\n    title  = \"ImageAI, an open source python library built to empower developers to build applications and systems  with self-contained Computer Vision capabilities\",\n    url    = \"https://github.com/OlafenwaMoses/ImageAI\",\n    month  = \"mar\",\n    year   = \"2018--\"\n}\n```\n\n\n\n ### References\n <div id=\"ref\"></div>\n\n 1. Somshubra Majumdar, DenseNet Implementation of the paper, Densely Connected Convolutional Networks in Keras\n[https://github.com/titu1994/DenseNet](https://github.com/titu1994/DenseNet)\n 2. Broad Institute of MIT and Harvard, Keras package for deep residual networks\n[https://github.com/broadinstitute/keras-resnet](https://github.com/broadinstitute/keras-resnet)\n 3. Fizyr, Keras implementation of RetinaNet object detection\n[https://github.com/fizyr/keras-retinanet](https://github.com/fizyr/keras-retinanet)\n 4. Francois Chollet, Keras code and weights files for popular deeplearning models\n[https://github.com/fchollet/deep-learning-models](https://github.com/fchollet/deep-learning-models)\n 5. Forrest N. et al, SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\n[https://arxiv.org/abs/1602.07360](https://arxiv.org/abs/1602.07360)\n 6. Kaiming H. et al, Deep Residual Learning for Image Recognition\n[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)\n 7. Szegedy. et al, Rethinking the Inception Architecture for Computer Vision\n[https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)\n 8. Gao. et al, Densely Connected Convolutional Networks\n[https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)\n 9. Tsung-Yi. et al, Focal Loss for Dense Object Detection\n[https://arxiv.org/abs/1708.02002](https://arxiv.org/abs/1708.02002)\n 10. O Russakovsky et al, ImageNet Large Scale Visual Recognition Challenge\n[https://arxiv.org/abs/1409.0575](https://arxiv.org/abs/1409.0575)\n 11. TY Lin et al, Microsoft COCO: Common Objects in Context\n[https://arxiv.org/abs/1405.0312](https://arxiv.org/abs/1405.0312)\n 12. Moses & John Olafenwa, A collection of images of identifiable professionals.\n[https://github.com/OlafenwaMoses/IdenProf](https://github.com/OlafenwaMoses/IdenProf)\n 13. Joseph Redmon and Ali Farhadi, YOLOv3: An Incremental Improvement.\n[https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767)\n 14. Experiencor, Training and Detecting Objects with YOLO3\n[https://github.com/experiencor/keras-yolo3](https://github.com/experiencor/keras-yolo3)\n 15. MobileNetV2: Inverted Residuals and Linear Bottlenecks\n[https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)\n 16. YOLOv3 in PyTorch > ONNX > CoreML > TFLite [https://github.com/ultralytics/yolov3](https://github.com/ultralytics/yolov3)\n"
        },
        {
          "name": "data-images",
          "type": "tree",
          "content": null
        },
        {
          "name": "data-videos",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "imageai",
          "type": "tree",
          "content": null
        },
        {
          "name": "imageai_tf_deprecated",
          "type": "tree",
          "content": null
        },
        {
          "name": "jarvis.png",
          "type": "blob",
          "size": 306.267578125,
          "content": null
        },
        {
          "name": "logo1.png",
          "type": "blob",
          "size": 18.947265625,
          "content": null
        },
        {
          "name": "logo2.png",
          "type": "blob",
          "size": 12.5166015625,
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.26171875,
          "content": "cython\npillow>=7.0.0\nnumpy>=1.18.1\nopencv-python>=4.1.2\ntorch>=1.9.0 --extra-index-url https://download.pytorch.org/whl/cpu\ntorchvision>=0.10.0 --extra-index-url https://download.pytorch.org/whl/cpu\npytest==7.1.3\ntqdm==4.64.1\nscipy>=1.7.3\nmatplotlib>=3.4.3\nmock==4.0.3"
        },
        {
          "name": "requirements_extra.txt",
          "type": "blob",
          "size": 0.099609375,
          "content": "pycocotools@git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI"
        },
        {
          "name": "requirements_gpu.txt",
          "type": "blob",
          "size": 0.265625,
          "content": "cython\npillow>=7.0.0\nnumpy>=1.18.1\nopencv-python>=4.1.2\ntorch>=1.9.0 --extra-index-url https://download.pytorch.org/whl/cu102\ntorchvision>=0.10.0 --extra-index-url https://download.pytorch.org/whl/cu102\npytest==7.1.3\ntqdm==4.64.1\nscipy>=1.7.3\nmatplotlib>=3.4.3\nmock==4.0.3"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.529296875,
          "content": "from setuptools import setup,find_packages\n\nsetup(name=\"imageai\",\n      version='3.0.3',\n      description='A python library built to empower developers to build applications and systems  with self-contained Computer Vision capabilities',\n      url=\"https://github.com/OlafenwaMoses/ImageAI\",\n      author='Moses Olafenwa',\n      author_email='guymodscientist@gmail.com',\n      license='MIT',\n      packages= find_packages(exclude=[\"*imageai_tf_deprecated*\"]),\n      install_requires=[],\n      include_package_data=True,\n      zip_safe=False)"
        },
        {
          "name": "supportimage.jpg",
          "type": "blob",
          "size": 32.919921875,
          "content": null
        },
        {
          "name": "test-images",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "theiaengine.png",
          "type": "blob",
          "size": 827.0283203125,
          "content": null
        }
      ]
    }
  ]
}