{
  "metadata": {
    "timestamp": 1736560900107,
    "page": 624,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "snorkel-team/snorkel",
      "stars": 5830,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".codecov.yaml",
          "type": "blob",
          "size": 0.1611328125,
          "content": "coverage:\n  status:\n    project:\n      default:\n        target: 95%\n    patch:\n      default:\n        threshold: 2%\n\ncomment:\n  layout: \"header, diff, flags, files\"\n"
        },
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.2099609375,
          "content": "[run]\nbranch = True\nsource = snorkel\n\n[report]\nexclude_lines =\n    pragma: no cover\n    raise NotImplementedError\n    if __name__ == .__main__.:\n    def __repr__\nignore_errors = True\nomit =\n    test/*\n    *spark.py\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.7978515625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\n.pypirc\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\ndocs/packages/_autosummary\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# MacOS\n.DS_Store\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# Editors\n.vscode/\n.code-workspace*\n\n# Dask\ndask-worker-space/\n\n# nohup\nnohup.out\n"
        },
        {
          "name": ".readthedocs.yml",
          "type": "blob",
          "size": 0.4658203125,
          "content": "# .readthedocs.yml\n# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Build documentation in the docs/ directory with Sphinx\nsphinx:\n  configuration: docs/conf.py\n\n# Optionally set the version of Python and requirements required to build your docs\npython:\n  version: 3.11\n  install:\n      - requirements: docs/requirements-doc.txt\n      - method: pip\n        path: .\n  system_packages: true\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 4.443359375,
          "content": "# Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n### [Breaking Changes]\n### [Added]\n### [Changed]\n### [Deprecated]\n### [Removed]\n\n\n##  [0.10.0] - 2023-02-27\n### [Breaking Changes]\n\n* PR #1729: Add support for Python 3.11 and remove support for all other minor versions\n\n### [Contributors]\n\nThanks to @bhancock8, @kamelCased, @MLobo1997, and @rsmith49 for contributions!\n\n\n## [0.9.9] - 2022-07-28\n### [Added]\n\n* PR #1690: Bump numpy version for Mac M1 compat\n* PR #1696: Fix linting\n* PR #1694: Fix test loss\n* PR #1693: Update black version to fix build\n* PR #1671: Fix flaky test\n* PR #1688: Fix branch filtering for complex tests\n* PR #1686: Switch to CircleCI badge\n* PR #1685: Migrating travis -> circle \n\n### [Contributors]\n\nThanks to @rsmith49, @zexuan-zhou, @humzaiqbal, @fpoms, @crawlingcub, and @henryre for contributions!\n\n\n## [0.9.8] - 2021-09-23\n### [Added]\n\n* PR #1649: Add progress bar for label model training\n\n### [Changed]\n\n* PR #1645: Upgrade networkx requirement to support networkx>=2.5\n* PR #1663: Upgrade tensorboard requirement to support tensorboard>=2.0\n* PR #1652: Fix issue with logging display during label model training\n* PR #1677: Bump version upper bound of networkx to 2.7\n\n### [Contributors]\n\nThanks to @anerirana, @thatch, @sfilipi, @hardianlawi, @asottile and @marekmodry for contributions!\n\n## [0.9.7] - 2021-03-03\n### [Breaking Changes]\n\n* PR #1633, #1628: Update requirements for `numpy` (>=1.16.5,<1.20.0), `pandas` (>=1.0.0,<2.0.0), and `scikit-learn` (>=0.20.2,<0.25.0)\n\n### [Added]\n\n* PR #1628: Add unit test coverage for Python 3.8\n* PR #1616: Accept prec_init as array or list in `LabelModel`\n* PR #1602: Add get_label_instances to analysis module\n\n### [Contributors]\n\nThanks to @DavidKoleczek and @antonis19 for contributions!\n\n## [0.9.6] - 2020-08-08\n### [Added]\n\n* PR #1572: Allow specification of `memoize_key` in preprocessors\n* PR #1597: Improved error messages for MultitaskClassifier\n* PR #1592: Improved LabelModel documentation\n\n### [Contributors]\n\nThanks to @Wirg, @TrigonaMinima, and @dchichkov for recent contributions!\n\n## [0.9.5] - 2020-04-06\n### [Breaking Changes]\n\n* PR #1565: Upgrade pytorch requirement to support torch>=1.2.0\n\n## [0.9.4] - 2020-04-05\n### [Breaking Changes]\n\n* PR #1535: Refactor baseline model imports in snorkel.labeling.model\n    * Now, from `from snorkel.labeling import MajorityLabelVoter, LabelModel`\n    can be expressed `from snorkel.labeling.model import MajorityLabelVoter, LabelModel`\n\n### [Added]\n\n* PR #1533: Allow option to save optimizer state in Trainer\n* PR #1523: Add GPU option for spaCy preprocessor\n\n### [Changed]\n\n* PR #1540: Fix squeeze bug in `to_int_label_array` function\n* PR #1520: Fix error bucket documentation in snorkel.analysis.error_analysis\n\n### [Contributors]\n\nThanks to @rjurney, @ptrcklv for recent contributions!\n\n## [0.9.3] - 2019-11-11\n\n### [Changed]\n\n* PR #1502: Faster symmetry breaking in LabelModel using Munkres algorithm\n\n\n## [0.9.2] - 2019-10-22\n\n### [Breaking Changes]\n\n* PR #1481: removed fault tolerant mode for labeling functions\n\n### [Added]\n\n* PR #1481: fault tolerant mode for appliers\n\n### [Changed]\n\n* PR #1450, 1467: ignore abstains in scoring, except coverage\n* PR #1463: serialize all attributes of label model\n* PR #1466: fix label model GPU training option\n* PR #1477, #1492: pin dependency versions\n\n### [Removed]\n\n* PR #1454: `set_seed` utility removed\n\n### [Contributors]\n\nThanks to @HiromuHota, @ferhatelmas, and @garaud for their recent contributions!\n\n\n## [0.9.1] - 2019-09-05\n\n### [Breaking Changes]\n\n* PR #1453: `SlicingClassifier` renamed to `SliceAwareClassifier`\n\n### [Added]\n\n* PR #1451: add heuristic for breaking symmetry in multiple label model optima case\n* PR #1442: integration test for `MultitaskClassifier`\n\n### [Changed]\n\n* PR #1444: fix label model weight clamping behavior\n* PR #1445: fix JSON log writer\n* PR #1447: fix correct/incorrect count bug in `LFAnalysis`\n* PR #1428, 1449: catch invalid label model inputs\n* PR #1441: make inputs to `Scorer.score` optional\n\n\n## [0.9.0] - 2019-08-15\nVersion 0.9.0 is a complete redesign of the Snorkel library.\nThere's too much added, changed, and removed to list in this entry.\nFor more information on the release,\n[check out the project homepage](https://snorkel.org).\nFrom here forward, we'll keep a detailed changelog.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 6.732421875,
          "content": "# Contributing to Snorkel\n\nWe love contributors, so first and foremost, thank you!\nWe're actively working on our contributing guidelines, so this document is subject to change.\nFirst things first: we adhere to the\n[Contributor Covenant Code of Conduct](http://contributor-covenant.org/version/1/4/),\nand expect all of our contributors to adhere to it as well.\n\n## Development environment\n\n### Installing\n\nSnorkel uses [tox](https://tox.readthedocs.io) to manage development environments.\nTo get started, [install tox](https://tox.readthedocs.io/en/latest/install.html),\nclone Snorkel, then use `tox` to create a development environment:\n\n```bash\ngit clone https://github.com/snorkel-team/snorkel\npip3 install -U tox\ncd snorkel\ntox --devenv .env\n```\n\nRunning `tox --devenv .env` will create a virtual environment with Snorkel\nand all of its dependencies installed in the directory `.env`.\nThis can be used in a number of ways, e.g. with `source .env/bin/activate`\nor for [linting in VSCode](https://code.visualstudio.com/docs/python/environments#_where-the-extension-looks-for-environments).\nFor example, you can simply activate this environment and start using Snorkel:\n\n```bash\nsource .env/bin/activate\n\npython3 -c \"import snorkel.labeling; print(dir(snorkel.labeling))\"\n```\n\n### Testing and committing\n\nThere are a number of useful tox commands defined:\n\n```bash\ntox -e py311  # Run unit tests pytest in Python 3.11\ntox -e coverage  # Compute unit test coverage\ntox -e spark  # Run Spark-based tests (marked with @pytest.mark.spark)\ntox -e complex  # Run more complex, integration tests (marked with @pytest.mark.complex)\ntox -e doctest  # Run doctest on modules\ntox -e check  # Check style/linting with black, isort, and flake8\ntox -e type  # Run static type checking with mypy\ntox -e fix  # Fix style issues with black and isort\ntox -e doc  # Build documentation with Sphinx\ntox  # Run unit tests, doctests, style checks, linting, and type checking\n```\n\nMake sure to run `tox` before committing.\nCI won't pass without `tox` succeeding.\n\nAs noted, we use a few additional tools that help to ensure that any commits or pull requests you submit conform with our established standards.\nWe use the following packages:\n* [isort](https://github.com/timothycrosley/isort): import standardization\n* [black](https://black.readthedocs.io/en/stable/): automatic code formatting\n* [flake8](http://flake8.pycqa.org/en/latest/): PEP8 linting\n* [mypy](http://mypy-lang.org/): static type checking\n* [pydocstyle](http://www.pydocstyle.org/): docstring compliance\n* [doctest-plus](https://github.com/astropy/pytest-doctestplus): check docstring code examples\n\nThe Snorkel maintainers are big fans of [VSCode](https://code.visualstudio.com/)'s Python tooling.\nHere's a `settings.json` that takes advantage of the packages above (except isort) with in-line linting:\n\n```json\n{\n    \"python.jediEnabled\": true,\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.flake8Enabled\": true,\n    \"python.linting.mypyEnabled\": true,\n    \"python.linting.pydocstyleEnabled\": true,\n    \"python.linting.pylintEnabled\": false,\n}\n```\n\n### Docstrings\n\nSnorkel ♥ documentation.\nWe expect all PRs to add or update API documentation for any affected pieces of code.\nWe use [NumPy style docstrings](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_numpy.html), and enforce style compliance with pydocstyle as indicated above.\nDocstrings can be cumbersome to write, so we encourage people to use tooling to speed up the process.\nFor VSCode, we like [autoDocstring](https://marketplace.visualstudio.com/items?itemName=njpwerner.autodocstring).\nJust install the extension and add the following configuration to the `settings.json` example above.\nNote that we use PEP 484 type hints, so parameter types should be removed from the docstring (although note that return types should still be included).\n\n```json\n{\n    \"autoDocstring.docstringFormat\": \"numpy\",\n    \"autoDocstring.guessTypes\": false\n}\n```\n\nThere are some standards we follow that our tooling doesn't automatically check/initialize:\n\n* Examples, examples, examples.\n  We love examples in docstrings; it's often the best form of documentation.\n  The `Example` or `Examples` section should come after `Parameters` but before `Attributes`.\n  Running `tox -e doctest` will test your docstring examples.\n* Make sure to add `Attributes` sections to docstrings to document public attributes of\n  classes.\n  The `Attributes` section should be the last part of the docstring.\n* No need to document private methods or attributes.\n\n\n### Complex/integration/long-running tests\n\nAny test that runs longer than half a second should be marked with the\n`@pytest.mark.complex` decorator.\nTypically, these will be integration tests or tests that verify complex\nproperties like model convergence.\nWe exclude long-running tests from the default `tox` and Circle CI builds\non non-main and non-release branches to keep things moving fast.\nIf you're touching areas of the code that could break a long-running test,\nyou should include the results of `tox -e complex` in the PR's test plan.\nTo see the durations of the 10 longest-running tests, run\n`tox -e py3 -- -m 'not complex and not spark' --durations 10`.\n\n\n### PySpark tests\n\nPySpark tests are invoked separately from the rest since they require\ninstalling Java and the large PySpark package.\nThey are executed on Circle CI, but not by default for a local `tox` command.\nIf you're making changes to Spark-based operators, make sure you have\nJava 8 installed locally and then run `tox -e spark`.\nIf you add a test that imports PySpark mark it with the\n`@pytest.mark.spark` decorator.\nAdd the `@pytest.mark.complex` decorator as well if it runs a Spark\naction (e.g. `.collect()`).\n\n\n## PRs\n\n### Submitting PRs\n\nWhen submitting a PR, make sure to use the preformatted template.\nExcept in special cases, all PRs should be against `main`.\nAvoid using \"staging branches\" as much as possible.\nIf you want to add complicated features, please\n[stack your PRs](https://graysonkoonce.com/stacked-pull-requests-keeping-github-diffs-small/)\nto ensure an effective review process.\nIt's unlikely that we'll approve any\n[single PR over 500 lines](https://www.ibm.com/developerworks/rational/library/11-proven-practices-for-peer-review/index.html).\n\n\n### Requesting reviews\n\nDirect commits to main are blocked, and PRs require an approving review\nto merge into main.\nBy convention, the Snorkel maintainers will review PRs when:\n  * An initial review has been requested\n  * A maintainer is tagged in the PR comments and asked to complete a review\n\nWe ask that you make sure initial CI checks are passing before requesting a review.\n\n\n### Merging\n\nThe PR author owns the test plan and has final say on correctness.\nTherefore, it is up to the PR author to give the final okay on merging\n(or merge their PR if they have write access).\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0576171875,
          "content": "include README.md\ninclude LICENSE\ninclude requirements.txt\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.1591796875,
          "content": "<img src=\"figs/logo_01.png\" width=\"150\"/>\n\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/snorkel)\n![PyPI](https://img.shields.io/pypi/v/snorkel)\n![Conda](https://img.shields.io/conda/v/conda-forge/snorkel)\n[![docs](https://readthedocs.org/projects/snorkel/badge/?version=master)](https://snorkel.readthedocs.io/en/master)\n[![coverage](https://codecov.io/gh/snorkel-team/snorkel/branch/master/graph/badge.svg)](https://codecov.io/gh/snorkel-team/snorkel/branch/master)\n[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\n***Programmatically Build and Manage Training Data***\n\n## Announcement\n\n**The Snorkel team is now focusing their efforts on Snorkel Flow, an end-to-end AI application development platform based on the core ideas behind Snorkel—you can check it out [here](https://snorkel.ai) or [join us](https://www.snorkel.ai/careers) in building it!**\n\nThe [Snorkel project](https://snorkel.ai/how-to-use-snorkel-to-build-ai-applications/) started at Stanford in 2015 with a simple technical bet: that it would increasingly be the **training data**, not the models, algorithms, or infrastructure, that decided whether a machine learning project succeeded or failed. Given this premise, we set out to explore the radical idea that you could bring mathematical and systems structure to the messy and often entirely manual process of training data creation and management, starting by empowering users to **programmatically label, build, and manage** training data.\n\nTo say that the Snorkel project succeeded and expanded beyond what we had ever expected would be an understatement. The basic goals of a research repo like Snorkel are to provide a minimum viable framework for testing and validating hypotheses. Four years later, we’ve been fortunate to do not just this, but to develop and deploy early versions of Snorkel in partnership with some of the world’s leading organizations like [Google](https://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html), [Intel](https://dl.acm.org/doi/abs/10.1145/3329486.3329492), [Stanford Medicine](https://www.cell.com/patterns/fulltext/S2666-3899(20)30019-2), and many more; author over [sixty peer-reviewed publications](https://snorkel.ai/technology) on our findings around Snorkel and related innovations in weak supervision modeling, data augmentation, multi-task learning, and more; be included in courses at top-tier universities; support production deployments in systems that you’ve likely used in the last few hours; and work with an amazing community of researchers and practitioners from industry, medicine, government, academia, and beyond.\n\nHowever, we realized increasingly–from conversations with users in weekly office hours, workshops, online discussions, and industry partners–that the Snorkel project was just the very first step. The ideas behind Snorkel change not just how you label training data, but so much of the entire lifecycle and pipeline of building, deploying, and managing ML: how users inject their knowledge; how models are constructed, trained, inspected, versioned, and monitored; how entire pipelines are developed iteratively; and how the full set of stakeholders in any ML deployment, from subject matter experts to ML engineers, are incorporated into the process.\n\nOver the last year, we have been building the platform to support this broader vision: [Snorkel Flow](https://snorkel.ai/snorkel-flow-platform/), an end-to-end machine learning platform for developing and deploying AI applications. Snorkel Flow incorporates many of the concepts of the Snorkel project with a range of newer techniques around weak supervision modeling, data augmentation, multi-task learning, data slicing and structuring, monitoring and analysis, and more, all of which integrate in a way that is greater than the sum of its parts–and that we believe makes ML truly faster, more flexible, and more practical than ever before.\n\nMoving forward, we will be focusing our efforts on Snorkel Flow. We are extremely grateful for all of you that have contributed to the Snorkel project, and are excited for you to check out our next chapter [here](https://snorkel.ai).\n\n\n# Quick Links\n* [Snorkel website](https://snorkel.org)\n* [Snorkel tutorials](https://github.com/snorkel-team/snorkel-tutorials)\n* [Snorkel documentation](https://snorkel.readthedocs.io/)\n* [Snorkel community forum](https://spectrum.chat/snorkel)\n* [Snorkel mailing list](https://groups.google.com/forum/#!forum/snorkel-ml)\n* [Snorkel Twitter](https://twitter.com/SnorkelAI)\n\n# Getting Started\nThe quickest way to familiarize yourself with the Snorkel library is to walk through the [Get Started](https://snorkel.org/get-started/) page on the Snorkel website, followed by the full-length tutorials in the [Snorkel tutorials](https://github.com/snorkel-team/snorkel-tutorials) repository.\nThese tutorials demonstrate a variety of tasks, domains, labeling techniques, and integrations that can serve as templates as you apply Snorkel to your own applications.\n\n\n# Installation\n\nSnorkel requires Python 3.11 or later. To install Snorkel, we recommend using `pip`:\n\n```bash\npip install snorkel\n```\n\nor `conda`:\n\n```bash\nconda install snorkel -c conda-forge\n```\n\nFor information on installing from source and contributing to Snorkel, see our\n[contributing guidelines](./CONTRIBUTING.md).\n\n<details><summary><b>Details on installing with <tt>conda</tt></b></summary>\n<p>\n\nThe following example commands give some more color on installing with `conda`.\nThese commands assume that your `conda` installation is Python 3.11,\nand that you want to use a virtual environment called `snorkel-env`.\n\n```bash\n# [OPTIONAL] Activate a virtual environment called \"snorkel\"\nconda create --yes -n snorkel-env python=3.11\nconda activate snorkel-env\n\n# We specify PyTorch here to ensure compatibility, but it may not be necessary.\nconda install pytorch==1.1.0 -c pytorch\nconda install snorkel==0.9.0 -c conda-forge\n```\n\n</p>\n</details>\n\n<details><summary><b>A quick note for Windows users</b></summary>\n<p>\n\nIf you're using Windows, we highly recommend using Docker\n(you can find an example in our\n[tutorials repo](https://github.com/snorkel-team/snorkel-tutorials/blob/master/Dockerfile))\nor the [Linux subsystem](https://docs.microsoft.com/en-us/windows/wsl/faq).\nWe've done limited testing on Windows, so if you want to contribute instructions\nor improvements, feel free to open a PR!\n\n</p>\n</details>\n\n# Discussion\n\n## Issues\nWe use [GitHub Issues](https://github.com/snorkel-team/snorkel/issues) for posting bugs and feature requests — anything code-related.\nJust make sure you search for related issues first and use our Issues templates.\nWe may ask for contributions if a prompt fix doesn't fit into the immediate roadmap of the core development team.\n\n## Contributions\nWe welcome contributions from the Snorkel community! \nThis is likely the fastest way to get a change you'd like to see into the library.\n\nSmall contributions can be made directly in a pull request (PR).\nIf you would like to contribute a larger feature, we recommend first creating an issue with a proposed design for discussion. \nFor ideas about what to work on, we've labeled specific issues as [`help wanted`](https://github.com/snorkel-team/snorkel/issues?utf8=%E2%9C%93&q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+).\n\nTo set up a development environment for contributing back to Snorkel, see our [contributing guidelines](./CONTRIBUTING.md).\nAll PRs must pass the continuous integration tests and receive approval from a member of the Snorkel development team before they will be merged.\n\n## Community Forum\nFor broader Q&A, discussions about using Snorkel, tutorial requests, etc., use the [Snorkel community forum](https://spectrum.chat/snorkel) hosted on Spectrum.\nWe hope this will be a venue for you to interact with other Snorkel users — please don't be shy about posting!\n\n## Announcements\nTo stay up-to-date on Snorkel-related announcements (e.g. version releases, upcoming workshops), subscribe to the [Snorkel mailing list](https://groups.google.com/forum/#!forum/snorkel-ml). We promise to respect your inboxes — communication will be sparse!\n\n## Twitter\nFollow us on Twitter [@SnorkelAI](https://twitter.com/SnorkelAI).\n"
        },
        {
          "name": "RELEASING.md",
          "type": "blob",
          "size": 3.4072265625,
          "content": "# Snorkel Release Guide\n\n## Before You Start\n\nMake sure you have [PyPI](https://pypi.org) account with maintainer access to the Snorkel project.\nCreate a .pypirc in your home directory.\nIt should look like this:\n\n```\n[distutils]\nindex-servers =\n  pypi\n  pypitest\n\n[pypi]\nusername=YOUR_USERNAME\npassword=YOUR_PASSWORD\n```\n\nThen run `chmod 600 ./.pypirc` so only you can read/write.\n\n\n## Release Steps\n\n1. Make sure you're in the top-level `snorkel` directory.\n1. Make certain your branch is in sync with head:\n   \n       $ git pull origin main\n\n1. Add a new changelog entry for the release.\n\n       ##  [0.9.0]\n       ### [Breaking Changes]\n       ### [Added]\n       ### [Changed]\n       ### [Deprecated]\n       ### [Removed]\n  Make sure `CHANGELOG.md` is up to date for the release: compare against PRs\n  merged since the last release.\n\n1. Update version to, e.g. 0.9.0 (remove the `+dev` label) in `snorkel/version.py`.\n\n\n1. Commit these changes and create a PR:\n\n       git checkout -b release-v0.9.0\n       git add . -u\n       git commit -m \"[RELEASE]: v0.9.0\"\n       git push --set-upstream origin release-v0.9.0\n\n1. Once the PR is approved, merge it and pull main locally.\n\n1. Tag the release:\n\n       git tag -a v0.9.0 -m \"v0.9.0 release\"\n       git push origin v0.9.0\n\n1. Build source & wheel distributions:\n\n       rm -rf dist build  # clean old builds & distributions\n       python3 setup.py sdist  # create a source distribution\n       python3 setup.py bdist_wheel  # create a universal wheel\n\n1. Check that everything looks correct by installing the wheel locally and checking the version:\n\n       python3 -m venv test_snorkel  # create a virtualenv for testing\n       source test_snorkel/bin/activate  # activate virtualenv\n       python3 -m pip install dist/snorkel-0.9.1-py3-none-any.whl\n       python3 -c \"import snorkel; print(snorkel.__version__)\"\n\n1. Publish to PyPI\n\n       pip install twine  # if not installed\n       twine upload dist/* -r pypi\n\n1. A PR is auto-submitted (this will take a few hours) on [`conda-forge/snorkel-feedstock`](https://github.com/conda-forge/snorkel-feedstock) to update the version.\n    * A maintainer needs to accept and merge those changes.\n\n1. Create a new release on Github.\n    * Input the recently-created Tag Version: `v0.9.0`\n    * Copy the release notes in `CHANGELOG.md` to the GitHub tag.\n    * Attach the resulting binaries in (`dist/snorkel-x.x.x.*`) to the release.\n    * Publish the release.\n\n\n1. Update version to, e.g. 0.9.1+dev in `snorkel/version.py`.\n\n1. Add a new changelog entry for the unreleased version in `CHANGELOG.md`:\n\n       ##  [Unreleased]\n       ### [Breaking Changes]\n       ### [Added]\n       ### [Changed]\n       ### [Deprecated]\n       ### [Removed]\n\n1. Commit these changes and create a PR:\n\n       git checkout -b bump-v0.9.1+dev\n       git add . -u\n       git commit -m \"[BUMP]: v0.9.1+dev\"\n       git push --set-upstream origin bump-v0.9.1+dev\n\n       \n1. Add the new tag to [the Snorkel project on ReadTheDocs](https://readthedocs.org/projects/snorkel),\n    * Trigger a build for main to pull new tags.\n    * Go to the \"Versions\" tab, and \"Activate\" the new tag.\n    * Go to Admin/Advanced to set this tag as the new default version.\n    * In \"Overview\", make sure a build is triggered:\n        * For the tag `v0.9.1`\n        * For `latest`\n\n\n## Credit\n* [AllenNLP](https://github.com/allenai/allennlp/blob/master/setup.py)\n* [Altair](https://github.com/altair-viz/altair/blob/master/RELEASING.md)\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "figs",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.2939453125,
          "content": "[build-system]\nrequires = [\n    \"setuptools >= 40.6.2\",\n    \"wheel >= 0.30.0\",\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.black]\nline-length = 88\ntarget-version = ['py311']\nexclude = '''\n/(\n    \\.eggs\n  | \\.git\n  | \\.mypy_cache\n  | \\.tox\n  | \\.env\n  | \\.venv\n  | _build\n  | build\n  | dist\n)/\n'''"
        },
        {
          "name": "requirements-pyspark.txt",
          "type": "blob",
          "size": 0.1484375,
          "content": "# Note: we don't include PySpark in the normal required installs.\n# Installing a new version may overwrite your existing system install.\npyspark==3.4.1\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 1.037109375,
          "content": "# Library dependencies for Python code.  You need to install these with\n# `pip install -r requirements.txt` or\n# `conda install --file requirements.txt`\n# to ensure that you can use all Snorkel code.\n# NOTE: all essential packages must be placed under a section named\n# '#### ESSENTIAL ...' so that the script `./scripts/check_requirements.py`\n# can find them.\n\n#### ESSENTIAL LIBRARIES\n\n# General scientific computing\nnumpy>=1.24.0\nscipy>=1.2.0\n\n# Data storage and function application\npandas>=1.0.0\ntqdm>=4.33.0\n\n# Internal models\nscikit-learn>=0.20.2\ntorch>=1.2.0\nmunkres>=1.0.6\n\n# LF dependency learning\nnetworkx>=2.2\n\n# Model introspection tools\nprotobuf>=3.19.6\ntensorboard>=2.13.0\n\n#### EXTRA/TEST LIBRARIES\n\n# spaCy (NLP)\nspacy>=2.1.0\nblis>=0.3.0\n\n# Dask (parallelism)\ndask[dataframe]>=2020.12.0\ndistributed>=2023.7.0\n\n# Dill (serialization)\ndill>=0.3.0\n\n#### DEV TOOLS\n\nblack>=22.8\nflake8>=3.7.0\nimportlib_metadata<5 # necessary for flake8\nisort>=4.3.0\nmypy>=0.760\npydocstyle>=4.0.0\npytest>=6.0.0\npytest-cov>=2.7.0\npytest-doctestplus>=0.3.0\ntox>=3.13.0\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 1.5166015625,
          "content": "[tool:pytest]\ntestpaths = test\nmarkers =\n    spark\n    complex\nnorecursedirs = .tox\ndoctest_optionflags =\n    NORMALIZE_WHITESPACE\n    ELLIPSIS\n    FLOAT_CMP\n\n[flake8]\nextend-ignore =\n    E203,\n    # Throws errors for '#%%' delimiter in VSCode jupyter notebook syntax\n    E265,\n    E501,\n    E731,\n    E741,\nexclude =\n    .eggs,\n    .git,\n    .mypy_cache,\n    .tox,\n    .env,\n    .venv,\n    _build,\n    build,\n    dist\n\n[isort]\nmulti_line_output=3\ninclude_trailing_comma=True\nforce_grid_wrap=0\ncombine_as_imports=True\nline_length=88\nknown_first_party=\n    snorkel,\nknown_third_party=\n    numpy,\n    pandas,\n    pyspark,\n    scipy,\n    setuptools,\n    tqdm,\ndefault_section=THIRDPARTY\nskip=.env,.venv,.tox\n\n[pydocstyle]\nconvention = numpy\nadd-ignore =\n    D100,\n    D104,\n    D105,\n    D107,\n    D202,\n    D203,\n    D204,\n    D213,\n    D413,\n\n[mypy]\n\n[mypy-dask]\nignore_missing_imports = True\n\n[mypy-dask.distributed]\nignore_missing_imports = True\n\n[mypy-networkx]\nignore_missing_imports = True\n\n[mypy-numpy]\nignore_missing_imports = True\n\n[mypy-numpy.lib]\nignore_missing_imports = True\n\n[mypy-pandas]\nignore_missing_imports = True\n\n[mypy-scipy]\nignore_missing_imports = True\n\n[mypy-scipy.sparse]\nignore_missing_imports = True\n\n[mypy-sklearn]\nignore_missing_imports = True\n\n[mypy-sklearn.metrics]\nignore_missing_imports = True\n\n[mypy-pyspark]\nignore_missing_imports = True\n\n[mypy-pyspark.sql]\nignore_missing_imports = True\n\n[mypy-spacy]\nignore_missing_imports = True\n\n[mypy-tqdm]\nignore_missing_imports = True\n\n[mypy-torch]\nignore_missing_imports = True\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.701171875,
          "content": "from typing import Dict\n\nfrom setuptools import find_packages, setup\n\n# version.py defines the VERSION and VERSION_SHORT variables.\n# We use exec here so we don't import snorkel.\nVERSION: Dict[str, str] = {}\nwith open(\"snorkel/version.py\", \"r\") as version_file:\n    exec(version_file.read(), VERSION)\n\n# Use README.md as the long_description for the package\nwith open(\"README.md\", \"r\") as readme_file:\n    long_description = readme_file.read()\n\nsetup(\n    name=\"snorkel\",\n    version=VERSION[\"VERSION\"],\n    url=\"https://github.com/snorkel-team/snorkel\",\n    description=\"A system for quickly generating training data with weak supervision\",\n    long_description_content_type=\"text/markdown\",\n    long_description=long_description,\n    license=\"Apache License 2.0\",\n    classifiers=[\n        \"Intended Audience :: Science/Research\",\n        \"Topic :: Scientific/Engineering :: Information Analysis\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Programming Language :: Python :: 3\",\n    ],\n    project_urls={\n        \"Homepage\": \"https://snorkel.org\",\n        \"Source\": \"https://github.com/snorkel-team/snorkel/\",\n        \"Bug Reports\": \"https://github.com/snorkel-team/snorkel/issues\",\n        \"Citation\": \"https://doi.org/10.14778/3157794.3157797\",\n    },\n    packages=find_packages(exclude=(\"test*\",)),\n    include_package_data=True,\n    install_requires=[\n        \"munkres>=1.0.6\",\n        \"numpy>=1.24.0\",\n        \"scipy>=1.2.0\",\n        \"pandas>=1.0.0\",\n        \"tqdm>=4.33.0\",\n        \"scikit-learn>=0.20.2\",\n        \"torch>=1.2.0\",\n        \"tensorboard>=2.13.0\",\n        \"protobuf>=3.19.6\",\n        \"networkx>=2.2\",\n    ],\n    python_requires=\">=3.11\",\n    keywords=\"machine-learning ai weak-supervision\",\n)\n"
        },
        {
          "name": "snorkel",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 2.431640625,
          "content": "[tox]\nskip_missing_interpreters = true\nenvlist =\n    py11,\n    type,\n    check,\n    doctest,\nisolated_build = true\n\n[testenv]\ndescription = run the test driver with {basepython}\n# Note: in order to allow dependency library install reuse\n# on CI, we allow overriding the default envdir\n# (specified as `{toxworkdir}/{envname}`) by setting the\n# environment variable `TOX_INSTALL_DIR`. We avoid\n# collision with the already-used `TOX_ENV_DIR`.\nenvdir = {env:TOX_INSTALL_DIR:{toxworkdir}/{envname}}\n# Note: we try to keep the deps the same for all tests\n# running on CI so that we skip reinstalling dependency\n# libraries for all testenvs\ndeps =\n    -rrequirements.txt\n    -rrequirements-pyspark.txt\ncommands_pre = python -m spacy download en_core_web_sm\ncommands = python -m pytest {posargs:-m 'not spark and not complex'}\n\n[testenv:spark]\ndescription = run the test driver for spark tests with {basepython}\npassenv = JAVA_HOME\ncommands = python -m pytest -m spark {posargs}\n\n[testenv:complex]\ndescription = run the test driver for integration tests with {basepython}\ncommands = python -m pytest -m 'complex and not spark' {posargs}\n\n[testenv:doctest]\ndescription = run doctest\nskipsdist = true\ncommands = python -m pytest --doctest-plus snorkel\n\n[testenv:check]\ndescription = check the code and doc style\nbasepython = python3\nallowlist_externals =\n    {toxinidir}/scripts/check_requirements.py\n    {toxinidir}/scripts/sync_api_docs.py\ncommands_pre =\ncommands =\n    isort -rc -c .\n    black --check .\n    flake8 .\n    pydocstyle snorkel\n    {toxinidir}/scripts/check_requirements.py\n    {toxinidir}/scripts/sync_api_docs.py --check\n\n[testenv:type]\ndescription = run static type checking\nbasepython = python3\ncommands_pre =\ncommands = mypy -p snorkel --disallow-untyped-defs --disallow-incomplete-defs --no-implicit-optional\n\n[testenv:coverage]\ndescription = run coverage checks\nbasepython = python3\n# Note: make sure this matches testenv since this is used\n# on CI as the default unit test runner\ncommands = python -m pytest -m 'not spark and not complex' --cov=snorkel\n\n[testenv:fix]\ndescription = run code stylers\nbasepython = python3\nusedevelop = True\ncommands_pre =\ncommands =\n    isort -rc .\n    black .\n\n[testenv:doc]\ndescription = build docs\nbasepython = python3\nskipsdist = True\ncommands_pre = python -m pip install -U -r docs/requirements-doc.txt\ncommands =\n    rm -rf docs/_build\n    rm -rf docs/packages/_autosummary\n    make -C docs/ html\n    {toxinidir}/scripts/sync_api_docs.py\n"
        }
      ]
    }
  ]
}