{
  "metadata": {
    "timestamp": 1736560563799,
    "page": 176,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "activeloopai/deeplake",
      "stars": 8294,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.5087890625,
          "content": "# pytest-cov #\n##############\n\ndatasets\n\nPYTEST_TMPDIR\n.coverage\n.coverage.*\nhtmlcov\n\n# vim\n*.swp\n*.swo\n\n# Sphinix\n# sphinx build folder\ndocs/_build\ndocs/source/_build\ncache/\ndask-worker-space/\n\n# Compiled source #\n###################\n*.com\n*.class\n*.dll\n*.exe\n*.o\n*.so\n\n# Packages #\n############\n# it's better to unpack these files and commit the raw source\n# git has its own built in compression methods\n*.7z\n*.dmg\n*.gz\n*.iso\n*.jar\n*.rar\n*.tar\n*.zip\n\n# Logs and databases #\n######################\n*.log\n*.sql\n*.sqlite\ndata/\n**/dummy_data/**/deeplake/\n\n# OS generated files #\n######################\n.dccache\n.DS_Store?\nehthumbs.db\nIcon?\nThumbs.db\n\n# Editor backup files #\n#######################\n*~\n\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n.terraform\nterraform.tfstate\nterraform.tfstate.backup\n.secrets\nnode/.secrets\nexamples/real/*\nnode/*.pem\n\n# C extensions\n*.so\n.DS_Store\n.clusters\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n*.gz\n*.pkl\nhyper/cluster/terraform/asg/.ssh/\nhyper/cluster/terraform/asg/terraform.tfstate\nhyper/cluster/terraform/asg/terraform.tfstate.backup\n\n.kube\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n.tests/\n.test_resources/\ntmp/\nwandb/\n*Python-3.7*\n*mem:/*\nhub_pytest/\ntest-dataset\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\nhyper.sublime-*\n.kube-temp\n\nhyper_search/\nlogs/\n\n# other\n.vscode/\n.creds/\n.idea/\n*.iml\n.nvimrc\n.vimrc\nwaymo/\noutput/\ncov.xml\ndeeplake/api/cov.xml\ndeeplake/api/nested_seq\nnested_seq\n\n# Benchmark local test data (auto-downloaded)\nbenchmarks/hub_data\nbenchmarks/torch_data\n.benchmarks/\n\n# API docs\napi_docs/\n\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.185546875,
          "content": "repos:\n  - repo: https://github.com/psf/black\n    rev: 23.11.0\n    hooks:\n      - id: black\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.7.0\n    hooks:\n      - id: mypy\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 4.01171875,
          "content": "# Contributing Standards\n\n## Linting\n\nWe use the [black](https://pypi.org/project/black/) python linter. You can have your code auto-formatted by\nrunning `pip install black`, then `black .` inside the directory you want to format.\n\n## Docstrings\n\nWe use Google Docstrings. Please refer\nto [this example](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html).\n\n## Typing\nWe also use static typing for our function arguments/variables for better code readability. We have a github action that runs `mypy .`, which runs similar to `pytest .` to check for valid static typing. You can refer to [mypy documentation](https://mypy.readthedocs.io/en/stable/) for more information.\n\n## Testing\nWe use [pytest](https://docs.pytest.org/en/6.2.x/) for our tests. In order to make it easier, we also have a set of custom options defined in [conftest.py](conftest.py).\n\n### To install all dependencies run:\n\n```\npip3 install -r deeplake/requirements/common.txt\npip3 install -r deeplake/requirements/plugins.txt\npip3 install -r deeplake/requirements/tests.txt\n```\n\n\n### Running Tests\n\n#### Standard:\n- `pytest .`: Run all tests with memory only.\n- `pytest . --local`: Run all tests with memory and local.\n- `pytest . --s3`: Run all tests with memory and s3.\n- `pytest . --gcs`: Run all tests with memory and GCS \n- `pytest . --kaggle`: Run all tests that use the kaggle API.\n- `pytest . --memory-skip --hub-cloud`: Run all tests with hub cloud only.\n#### Backwards Compatibility Tests\nWe use another github repository ([buH](https://github.com/activeloopai/buH)) for our backwards compatibility tests. Check out the README for instructions.\n\n### Options\nCombine any of the following options to suit your test cases.\n\n- `--local`: Enable local tests.\n- `--s3`: Enable S3 tests.\n- `--gcs`: Enable GCS tests.\n- `--hub-cloud`: Enable hub cloud tests.\n- `--memory-skip`: Disable memory tests.\n- `--s3-path`: Specify an s3 path if you don't have access to our internal testing bucket.\n- `--keep-storage`: By default all storages are cleaned up after tests run. Enable this option if you need to check the storage contents. Note: `--keep-storage` does not keep memory tests storage.\n\n\n### Extra Resources\nIf you feel lost with any of these sections, try reading up on some of these topics.\n\n- Understand how to write [pytest](https://docs.pytest.org/en/6.2.x/) tests.\n- Understand what a [pytest fixture](https://docs.pytest.org/en/6.2.x/fixture.html) is.\n- Understand what [pytest parametrizations](https://docs.pytest.org/en/6.2.x/parametrize.html) are.\n\n\n### Fixture Usage Examples\nThese are not all of the available fixtures. You can see all of them [here](/deeplake/tests/).\n\nDatasets\n```python\n@enabled_datasets\ndef test_dataset(ds: Dataset):\n  # this test will run once per enabled storage provider. if no providers are explicitly enabled,\n  # only memory will be used.\n  pass\n\n\ndef test_local_dataset(local_ds: Dataset):\n  # this test will run only once with a local dataset. if the `--local` option is not provided,\n  # this test will be skipped.\n  pass\n```\n\nStorages\n```python\n@enabled_storages\ndef test_storage(storage: StorageProvider):\n  # this test will run once per enabled storage provider. if no providers are explicitly enabled,\n  # only memory will be used.\n  pass\n\n\ndef test_memory_storage(memory_storage: StorageProvider):\n  # this test will run only once with a memory storage provider. if the `--memory-skip` option is provided,\n  # this test will be skipped.\n  pass\n```\n\nCaches\n```python\n@enabled_cache_chains\ndef test_cache(cache_chain: StorageProvider):  # note: caches are provided as `StorageProvider`s\n  # this test runs for every cache chain that contains all enabled storage providers.\n  # if only memory is enabled (no providers are explicitly enabled), this test will be skipped.\n  pass\n```\n\n## Generating API Docs\n\nDeep Lake used pdocs3 to generate docs: https://pdoc3.github.io/pdoc/\nAPI docs are hosted at: https://api-docs.activeloop.ai/\n\nRun the below command to generate API documentation:\n```\n  pdoc3 --html --output-dir api_docs --template-dir pdoc/templates hub\n```"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.08984375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.4384765625,
          "content": "<img src=\"https://static.scarf.sh/a.png?x-pxid=bc3c57b0-9a65-49fe-b8ea-f711c4d35b82\" /><p align=\"center\">\n     <img src=\"https://i.postimg.cc/rsjcWc3S/deeplake-logo.png\" width=\"400\"/>\n</h1>\n\n</br>\n\n<h1 align=\"center\">Deep Lake: Database for AI</h1>\n\n<p align=\"center\">\n    <a href=\"https://pypi.org/project/deeplake/\"><img src=\"https://badge.fury.io/py/deeplake.svg\" alt=\"PyPI version\" height=\"18\"></a>\n    <a href=\"https://pepy.tech/project/deeplake\"><img src=\"https://static.pepy.tech/badge/deeplake\" alt=\"PyPI version\" height=\"18\"></a>\n  <h3 align=\"center\">\n   <a href=\"https://docs.deeplake.ai/?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme\"><b>Docs</b></a> &bull;\n   <a href=\"https://docs.deeplake.ai/latest/setup/quickstart/?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme\"><b>Get Started</b></a> &bull;\n   <a href=\"https://docs.deeplake.ai/latest/api/dataset/?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme\"><b>API Reference</b></a> &bull;  \n   <a href=\"http://learn.activeloop.ai\"><b>LangChain & VectorDBs Course</b></a> &bull;\n   <a href=\"https://www.activeloop.ai/resources/?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme\"><b>Blog</b></a> &bull;\n   <a href=\"https://www.deeplake.ai/?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme\"><b>Whitepaper</b></a> &bull;  \n  <a href=\"http://slack.activeloop.ai\"><b>Slack</b></a> &bull;\n  <a href=\"https://twitter.com/intent/tweet?url=https%3A%2F%2Factiveloop.ai%2F&via=activeloopai&text=Deep%20Lake%20is%20the%20Database%20for%20all%20AI%20data.%20Check%20it%20out%21&hashtags=DeepLake%2Cactiveloop%2Copensource\"><b>Twitter</b></a>\n </h3>\n\n## What is Deep Lake?\n\nDeep Lake is a Database for AI powered by a storage format optimized for deep-learning applications. Deep Lake can be used for:\n\n1. Storing and searching data plus vectors while building LLM applications\n2. Managing datasets while training deep learning models\n   \nDeep Lake simplifies the deployment of enterprise-grade LLM-based products by offering storage for all data types (embeddings, audio, text, videos, images, dicom, pdfs, annotations, [and more](https://docs.deeplake.ai/latest/api/types/)), querying and vector search, data streaming while training models at scale, data versioning and lineage, and integrations with popular tools such as LangChain, LlamaIndex, Weights & Biases, and many more. Deep Lake works with data of any size, it is serverless, and it enables you to store all of your data in your own cloud and in one place. Deep Lake is used by Intel, Bayer Radiology, Matterport, ZERO Systems, Red Cross, Yale, & Oxford. \n\n### Deep Lake includes the following features:\n\n<details>\n  <summary><b>Multi-Cloud Support (S3, GCP, Azure)</b></summary>\nUse one API to upload, download, and stream datasets to/from S3, Azure, GCP, Activeloop cloud, local storage, or in-memory storage. Compatible with any S3-compatible storage such as MinIO. \n</details>\n<details>\n  <summary><b>Native Compression with Lazy NumPy-like Indexing</b></summary>\nStore images, audio, and videos in their native compression. Slice, index, iterate, and interact with your data like a collection of NumPy arrays in your system's memory. Deep Lake lazily loads data only when needed, e.g., when training a model or running queries.\n</details>\n<details>\n  <summary><b>Dataloaders for Popular Deep Learning Frameworks</b></summary>\nDeep Lake comes with built-in dataloaders for Pytorch and TensorFlow. Train your model with a few lines of code - we even take care of dataset shuffling. :)\n</details>\n<details>\n  <summary><b>Integrations with Powerful Tools</b></summary>\nDeep Lake has integrations with <a href=\"https://github.com/hwchase17/langchain\">Langchain</a> and <a href=\"https://github.com/jerryjliu/llama_index\">LLamaIndex</a> as a vector store for LLM apps, <a href=\"https://wandb.ai/\">Weights & Biases</a> for data lineage during model training, <a href=\"https://github.com/open-mmlab/mmdetection\">MMDetection</a> for training object detection models, and <a href=\"https://github.com/open-mmlab/mmsegmentation\">MMSegmentation</a> for training semantic segmentation models.\n</details>\n<details>\n  <summary><b>100+ most-popular image, video, and audio datasets available in seconds</b></summary>\nDeep Lake community has uploaded <a href=\"https://app.activeloop.ai/datasets/activeloop?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme\">100+ image, video and audio datasets</a> like <a href=\"https://app.activeloop.ai/activeloop/mnist-train?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme\">MNIST</a>, <a href=\"https://app.activeloop.ai/activeloop/coco-train?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme\">COCO</a>,  <a href=\"https://app.activeloop.ai/activeloop/imagenet-train?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme\">ImageNet</a>,  <a href=\"https://app.activeloop.ai/activeloop/cifar100-test?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme\">CIFAR</a>,  <a href=\"https://app.activeloop.ai/activeloop/gtzan-genre?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme\">GTZAN</a> and others.\n</details>\n</details>\n<details>\n  <summary><b>Instant Visualization Support in the <a href=\"https://app.activeloop.ai/?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme\">Deep Lake App</a></b></summary>\nDeep Lake datasets are instantly visualized with bounding boxes, masks, annotations, etc. in <a href=\"https://app.activeloop.ai/?utm_source=github&utm_medium=github&utm_campaign=github_readme&utm_id=readme\">Deep Lake Visualizer</a> (see below).\n</details>\n\n[![Visualizer](https://www.linkpicture.com/q/ReadMe.gif \"Visualizer\")](https://www.youtube.com/watch?v=SxsofpSIw3k)\n\n## 🚀 How to install Deep Lake\nDeep Lake can be installed using pip:\n```sh\npip install deeplake\n```\n\n### To access all of Deep Lake's features, please register in the [Deep Lake App](https://app.activeloop.ai/register/).\n\n## 🧠 Deep Lake Code Examples by Application\n\n### Vector Store Applications\nUsing Deep Lake as a Vector Store for building LLM applications:\n### - [Vector Store Quickstart](https://docs.deeplake.ai/latest/guide/rag/vector-search/)\n### - [Vector Store Tutorials](https://docs.activeloop.ai/examples/rag/tutorials)\n### - [LangChain Integration](https://docs.activeloop.ai/examples/rag/langchain-integration)\n### - [LlamaIndex Integration](https://docs.activeloop.ai/examples/rag/llamaindex-integration)\n### - [Image Similarity Search with Deep Lake](https://docs.deeplake.ai/latest/guide/rag/image-similarity/)\n\n\n### Deep Learning Applications\nUsing Deep Lake for managing data while training Deep Learning models:\n### - [Deep Learning Quickstart](https://docs.deeplake.ai/latest/guide/deep-learning/)\n### - [Tutorials for Training Models](https://docs.activeloop.ai/examples/dl/tutorials/training-models)\n\n## ⚙️ Integrations\n\nDeep Lake offers integrations with other tools in order to streamline your deep learning workflows. Current integrations include:\n\n* **LLM Apps**\n  * Use [Deep Lake as a vector store for LLM apps](https://www.activeloop.ai/resources/ultimate-guide-to-lang-chain-deep-lake-build-chat-gpt-to-answer-questions-on-your-financial-data/). Our integration combines the [Langchain](https://github.com/hwchase17/langchain) [VectorStores API](https://python.langchain.com/en/latest/reference/modules/vectorstore.html?highlight=pinecone#langchain.vectorstores.DeepLake) with Deep Lake datasets as the underlying data storage. The integration is a serverless vector store that can be deployed locally or in a cloud of your choice.\n\n## 📚 Documentation\n\nGetting started guides, examples, tutorials, API reference, and other useful information can be found on our [documentation page](http://docs.deeplake.ai/?utm_source=github&utm_medium=repo&utm_campaign=readme).\n\n## 🎓 For Students and Educators\nDeep Lake users can access and visualize a variety of popular datasets through a free integration with Deep Lake's App. Universities can get up to 1TB of data storage and 100,000 monthly queries on the Tensor Database for free per month. Chat in on [our website](https://activeloop.ai): to claim the access!\n\n## 👩‍💻 Comparisons to Familiar Tools\n\n<details>\n  <summary><b>Deep Lake vs Chroma </b></summary>\n  \nBoth Deep Lake & ChromaDB enable users to store and search vectors (embeddings) and offer integrations with LangChain and LlamaIndex. However, they are architecturally very different. ChromaDB is a Vector Database that can be deployed locally or on a server using Docker and will offer a hosted solution shortly. Deep Lake is a serverless Vector Store deployed on the user’s own cloud, locally, or in-memory. All computations run client-side, which enables users to support lightweight production apps in seconds. Unlike ChromaDB, Deep Lake’s data format can store raw data such as images, videos, and text, in addition to embeddings. ChromaDB is limited to light metadata on top of the embeddings and has no visualization. Deep Lake datasets can be visualized and version controlled. Deep Lake also has a performant dataloader for fine-tuning your Large Language Models.\n\n</details>\n\n<details>\n  <summary><b>Deep Lake vs Pinecone</b></summary>\n  \nBoth Deep Lake and Pinecone enable users to store and search vectors (embeddings) and offer integrations with LangChain and LlamaIndex. However, they are  architecturally very different. Pinecone is a fully-managed Vector Database that is optimized for highly demanding applications requiring a search for billions of vectors. Deep Lake is serverless. All computations run client-side, which enables users to get started in seconds. Unlike Pinecone, Deep Lake’s data format can store raw data such as images, videos, and text, in addition to embeddings. Deep Lake datasets can be visualized and version controlled. Pinecone is limited to light metadata on top of the embeddings and has no visualization. Deep Lake also has a performant dataloader for fine-tuning your Large Language Models.\n\n</details>\n\n<details>\n  <summary><b>Deep Lake vs Weaviate</b></summary>\n  \nBoth Deep Lake and Weaviate enable users to store and search vectors (embeddings) and offer integrations with LangChain and LlamaIndex. However, they are  architecturally very different. Weaviate is a Vector Database that can be deployed in a managed service or by the user via Kubernetes or Docker. Deep Lake is serverless. All computations run client-side, which enables users to support lightweight production apps in seconds. Unlike Weaviate, Deep Lake’s data format can store raw data such as images, videos, and text, in addition to embeddings. Deep Lake datasets can be visualized and version controlled. Weaviate is limited to light metadata on top of the embeddings and has no visualization. Deep Lake also has a performant dataloader for fine-tuning your Large Language Models.\n\n</details>\n\n<details>\n  <summary><b>Deep Lake vs DVC</b></summary>\n  \nDeep Lake and DVC offer dataset version control similar to git for data, but their methods for storing data differ significantly. Deep Lake converts and stores data as chunked compressed arrays, which enables rapid streaming to ML models, whereas DVC operates on top of data stored in less efficient traditional file structures. The Deep Lake format makes dataset versioning significantly easier compared to traditional file structures by DVC when datasets are composed of many files (i.e., many images). An additional distinction is that DVC primarily uses a command-line interface, whereas Deep Lake is a Python package. Lastly, Deep Lake offers an API to easily connect datasets to ML frameworks and other common ML tools and enables instant dataset visualization through [Activeloop's visualization tool](http://app.activeloop.ai/?utm_source=github&utm_medium=repo&utm_campaign=readme).\n\n</details>\n\n<details>\n  <summary><b>Deep Lake vs MosaicML MDS format </b></summary>\n  \n* **Data Storage Format:** Deep Lake operates on a columnar storage format, whereas MDS utilizes a row-wise storage approach. This fundamentally impacts how data is read, written, and organized in each system.\n* **Compression:** Deep Lake offers a more flexible compression scheme, allowing control over both chunk-level and sample-level compression for each column or tensor. This feature eliminates the need for additional compressions like zstd, which would otherwise demand more CPU cycles for decompressing on top of formats like jpeg.\n* **Shuffling:** MDS currently offers more advanced shuffling strategies.\n* **Version Control & Visualization Support:** A notable feature of Deep Lake is its native version control and in-browser data visualization, a feature not present for MosaicML data format. This can provide significant advantages in managing, understanding, and tracking different versions of the data.\n\n</details>\n\n<details>\n  <summary><b>Deep Lake vs TensorFlow Datasets (TFDS)</b></summary>\n  \nDeep Lake and TFDS seamlessly connect popular datasets to ML frameworks. Deep Lake datasets are compatible with both PyTorch and TensorFlow, whereas TFDS are only compatible with TensorFlow. A key difference between Deep Lake and TFDS is that Deep Lake datasets are designed for streaming from the cloud, whereas TFDS must be downloaded locally prior to use. As a result, with Deep Lake, one can import datasets directly from TensorFlow Datasets and stream them either to PyTorch or TensorFlow. In addition to providing access to popular publicly available datasets, Deep Lake also offers powerful tools for creating custom datasets, storing them on a variety of cloud storage providers, and collaborating with others via simple API. TFDS is primarily focused on giving the public easy access to commonly available datasets, and management of custom datasets is not the primary focus. A full comparison article can be found [here](https://www.activeloop.ai/resources/tensor-flow-tf-data-activeloop-hub-how-to-implement-your-tensor-flow-data-pipelines-with-hub/).\n\n</details>\n\n<details>\n  <summary><b>Deep Lake vs HuggingFace</b></summary>\nDeep Lake and HuggingFace offer access to popular datasets, but Deep Lake primarily focuses on computer vision, whereas HuggingFace focuses on natural language processing. HuggingFace Transforms and other computational tools for NLP are not analogous to features offered by Deep Lake.\n\n</details>\n\n<details>\n  <summary><b>Deep Lake vs WebDatasets</b></summary>\nDeep Lake and WebDatasets both offer rapid data streaming across networks. They have nearly identical steaming speeds because the underlying network requests and data structures are very similar. However, Deep Lake offers superior random access and shuffling, its simple API is in python instead of command-line, and Deep Lake enables simple indexing and modification of the dataset without having to recreate it.\n\n</details>\n\n<details>\n  <summary><b>Deep Lake vs Zarr</b></summary>\nDeep Lake and Zarr both offer storage of data as chunked arrays. However, Deep Lake is primarily designed for returning data as arrays using a simple API, rather than actually storing raw arrays (even though that's also possible). Deep Lake stores data in use-case-optimized formats, such as jpeg or png for images, or mp4 for video, and the user treats the data as if it's an array, because Deep Lake handles all the data processing in between. Deep Lake offers more flexibility for storing arrays with dynamic shape (ragged tensors), and it provides several features that are not naively available in Zarr such as version control, data streaming, and connecting data to ML Frameworks.\n\n</details>\n\n## Community\n\nJoin our [**Slack community**](https://slack.activeloop.ai) to learn more about unstructured dataset management using Deep Lake and to get help from the Activeloop team and other users.\n\nWe'd love your feedback by completing our 3-minute [**survey**](https://forms.gle/rLi4w33dow6CSMcm9).\n\nAs always, thanks to our amazing contributors!\n\n<a href=\"https://github.com/activeloopai/deeplake/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=activeloopai/hub\" />\n</a>\n\nMade with [contributors-img](https://contrib.rocks).\n\nPlease read [CONTRIBUTING.md](CONTRIBUTING.md) to get started with making contributions to Deep Lake.\n\n## README Badge\n\nUsing Deep Lake? Add a README badge to let everyone know:\n\n[![deeplake](https://img.shields.io/badge/powered%20by-Deep%20Lake%20-ff5a1f.svg)](https://github.com/activeloopai/deeplake)\n\n```markdown\n[![deeplake](https://img.shields.io/badge/powered%20by-Deep%20Lake%20-ff5a1f.svg)](https://github.com/activeloopai/deeplake)\n```\n\n## Disclaimers\n\n<details>\n  <summary><b> Dataset Licenses</b></summary>\n  \nDeep Lake users may have access to a variety of publicly available datasets. We do not host or distribute these datasets, vouch for their quality or fairness, or claim that you have a license to use the datasets. It is your responsibility to determine whether you have permission to use the datasets under their license.\n\nIf you're a dataset owner and do not want your dataset to be included in this library, please get in touch through a [GitHub issue](https://github.com/activeloopai/deeplake/issues/new). Thank you for your contribution to the ML community!\n\n</details>\n\n<details>\n  <summary><b> Usage Tracking</b></summary>\n\nBy default, we collect usage data using Bugout (here's the [code](https://github.com/activeloopai/deeplake/blob/853456a314b4fb5623c936c825601097b0685119/deeplake/__init__.py#L24) that does it). It does not collect user data other than anonymized IP address data, and it only logs the Deep Lake library's own actions. This helps our team understand how the tool is used and how to build features that matter to you! After you register with Activeloop, data is no longer anonymous. You can always opt-out of reporting by setting an environmental variable ```BUGGER_OFF``` to ```True```:\n\n</details>\n\n## Citation\n\nIf you use Deep Lake in your research, please cite Activeloop using:\n\n```markdown\n@article{deeplake,\n  title = {Deep Lake: a Lakehouse for Deep Learning},\n  author = {Hambardzumyan, Sasun and Tuli, Abhinav and Ghukasyan, Levon and Rahman, Fariz and Topchyan, Hrant and Isayan, David and Harutyunyan, Mikayel and Hakobyan, Tatevik and Stranic, Ivo and Buniatyan, Davit},\n  url = {https://www.cidrdb.org/cidr2023/papers/p69-buniatyan.pdf},\n  booktitle={Proceedings of CIDR},\n  year = {2023},\n}\n```\n\n\n## Acknowledgment\n\nThis technology was inspired by our research work at Princeton University. We would like to thank William Silversmith @SeungLab for his awesome [cloud-volume](https://github.com/seung-lab/cloud-volume) tool.\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 1.33203125,
          "content": "# Security\n\n## Reporting a bug in Deep Lake\n\n**Please do not report security vulnerabilities through public GitHub issues.**\n\nInstead, please report them to [security@activeloop.ai](mailto:security@activeloop.ai).\n\nPlease include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:\n\n* Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)\n* Full paths of source file(s) related to the manifestation of the issue\n* The location of the affected source code (tag/branch/commit or direct URL)\n* Any special configuration required to reproduce the issue\n* Step-by-step instructions to reproduce the issue\n* Proof-of-concept or exploit code (if possible)\n* Impact of the issue, including how an attacker might exploit the issue\n\nYou should receive a response within 72 hours. If for some reason you do not, please follow up via email to ensure we received your original message. \n\nAfter the initial reply to your report, the security team will endeavor to keep\nyou informed of the progress being made towards a fix and full announcement,\nand may ask for additional information or guidance surrounding the reported\nissue.\n\n## Reporting a bug in a third party module\n\nSecurity bugs in third party modules should be reported to their respective\nmaintainers.\n"
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}