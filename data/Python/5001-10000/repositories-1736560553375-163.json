{
  "metadata": {
    "timestamp": 1736560553375,
    "page": 163,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ashawkey/stable-dreamfusion",
      "stars": 8408,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4853515625,
          "content": "__pycache__/\nbuild/\n*.egg-info/\n*.so\nvenv_*/\n\ntmp*\n# data/\nldm/data/\ndata2\nscripts2\ntrial*/\n.vs/\n\nTOKEN\n*.ckpt\n\ndensegridencoder\ntets/256_tets.npz\n\n.vscode/launch.json\n\ndata2\ndata/car*\ndata/chair*\ndata/warrior*\ndata/wd*\ndata/space*\ndata/corgi*\ndata/turtle*\n\n# Only keep the original image, not the automatically-generated depth, normals, rgba\ndata/baby_phoenix_on_ice_*\ndata/bollywood_actress_*\ndata/beach_house_1_*\ndata/beach_house_2_*\ndata/mona_lisa_*\ndata/futuristic_car_*\ndata/church_ruins_*\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "activation.py",
          "type": "blob",
          "size": 0.513671875,
          "content": "import torch\nfrom torch.autograd import Function\nfrom torch.cuda.amp import custom_bwd, custom_fwd \n\nclass _trunc_exp(Function):\n    @staticmethod\n    @custom_fwd(cast_inputs=torch.float)\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return torch.exp(x)\n\n    @staticmethod\n    @custom_bwd\n    def backward(ctx, g):\n        x = ctx.saved_tensors[0]\n        return g * torch.exp(x.clamp(max=15))\n\ntrunc_exp = _trunc_exp.apply\n\ndef biased_softplus(x, bias=0):\n    return torch.nn.functional.softplus(x - bias)"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "dpt.py",
          "type": "blob",
          "size": 26.8623046875,
          "content": "import math\nimport types\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport timm\n\nclass BaseModel(torch.nn.Module):\n    def load(self, path):\n        \"\"\"Load model from file.\n        Args:\n            path (str): file path\n        \"\"\"\n        parameters = torch.load(path, map_location=torch.device('cpu'))\n\n        if \"optimizer\" in parameters:\n            parameters = parameters[\"model\"]\n\n        self.load_state_dict(parameters)\n\n\ndef unflatten_with_named_tensor(input, dim, sizes):\n    \"\"\"Workaround for unflattening with named tensor.\"\"\"\n    # tracer acts up with unflatten. See https://github.com/pytorch/pytorch/issues/49538\n    new_shape = list(input.shape)[:dim] + list(sizes) + list(input.shape)[dim+1:]\n    return input.view(*new_shape)\n    \nclass Slice(nn.Module):\n    def __init__(self, start_index=1):\n        super(Slice, self).__init__()\n        self.start_index = start_index\n\n    def forward(self, x):\n        return x[:, self.start_index :]\n\n\nclass AddReadout(nn.Module):\n    def __init__(self, start_index=1):\n        super(AddReadout, self).__init__()\n        self.start_index = start_index\n\n    def forward(self, x):\n        if self.start_index == 2:\n            readout = (x[:, 0] + x[:, 1]) / 2\n        else:\n            readout = x[:, 0]\n        return x[:, self.start_index :] + readout.unsqueeze(1)\n\n\nclass ProjectReadout(nn.Module):\n    def __init__(self, in_features, start_index=1):\n        super(ProjectReadout, self).__init__()\n        self.start_index = start_index\n\n        self.project = nn.Sequential(nn.Linear(2 * in_features, in_features), nn.GELU())\n\n    def forward(self, x):\n        readout = x[:, 0].unsqueeze(1).expand_as(x[:, self.start_index :])\n        features = torch.cat((x[:, self.start_index :], readout), -1)\n\n        return self.project(features)\n\n\nclass Transpose(nn.Module):\n    def __init__(self, dim0, dim1):\n        super(Transpose, self).__init__()\n        self.dim0 = dim0\n        self.dim1 = dim1\n\n    def forward(self, x):\n        x = x.transpose(self.dim0, self.dim1)\n        return x\n\n\ndef forward_vit(pretrained, x):\n    b, c, h, w = x.shape\n\n    glob = pretrained.model.forward_flex(x)\n\n    layer_1 = pretrained.activations[\"1\"]\n    layer_2 = pretrained.activations[\"2\"]\n    layer_3 = pretrained.activations[\"3\"]\n    layer_4 = pretrained.activations[\"4\"]\n\n    layer_1 = pretrained.act_postprocess1[0:2](layer_1)\n    layer_2 = pretrained.act_postprocess2[0:2](layer_2)\n    layer_3 = pretrained.act_postprocess3[0:2](layer_3)\n    layer_4 = pretrained.act_postprocess4[0:2](layer_4)\n\n\n    unflattened_dim = 2\n    unflattened_size = (\n        int(torch.div(h, pretrained.model.patch_size[1], rounding_mode='floor')),\n        int(torch.div(w, pretrained.model.patch_size[0], rounding_mode='floor')),\n    )\n    unflatten = nn.Sequential(nn.Unflatten(unflattened_dim, unflattened_size))\n    \n\n    if layer_1.ndim == 3:\n        layer_1 = unflatten(layer_1)\n    if layer_2.ndim == 3:\n        layer_2 = unflatten(layer_2)\n    if layer_3.ndim == 3:\n        layer_3 = unflatten_with_named_tensor(layer_3, unflattened_dim, unflattened_size)\n    if layer_4.ndim == 3:\n        layer_4 = unflatten_with_named_tensor(layer_4, unflattened_dim, unflattened_size)\n\n    layer_1 = pretrained.act_postprocess1[3 : len(pretrained.act_postprocess1)](layer_1)\n    layer_2 = pretrained.act_postprocess2[3 : len(pretrained.act_postprocess2)](layer_2)\n    layer_3 = pretrained.act_postprocess3[3 : len(pretrained.act_postprocess3)](layer_3)\n    layer_4 = pretrained.act_postprocess4[3 : len(pretrained.act_postprocess4)](layer_4)\n\n    return layer_1, layer_2, layer_3, layer_4\n\n\ndef _resize_pos_embed(self, posemb, gs_h, gs_w):\n    posemb_tok, posemb_grid = (\n        posemb[:, : self.start_index],\n        posemb[0, self.start_index :],\n    )\n\n    gs_old = int(math.sqrt(posemb_grid.shape[0]))\n\n    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n    posemb_grid = F.interpolate(posemb_grid, size=(gs_h, gs_w), mode=\"bilinear\")\n    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_h * gs_w, -1)\n\n    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n\n    return posemb\n\n\ndef forward_flex(self, x):\n    b, c, h, w = x.shape\n\n    pos_embed = self._resize_pos_embed(\n        self.pos_embed, torch.div(h, self.patch_size[1], rounding_mode='floor'), torch.div(w, self.patch_size[0], rounding_mode='floor')\n    )\n\n    B = x.shape[0]\n\n    if hasattr(self.patch_embed, \"backbone\"):\n        x = self.patch_embed.backbone(x)\n        if isinstance(x, (list, tuple)):\n            x = x[-1]  # last feature if backbone outputs list/tuple of features\n\n    x = self.patch_embed.proj(x).flatten(2).transpose(1, 2)\n\n    if getattr(self, \"dist_token\", None) is not None:\n        cls_tokens = self.cls_token.expand(\n            B, -1, -1\n        )  # stole cls_tokens impl from Phil Wang, thanks\n        dist_token = self.dist_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n    else:\n        cls_tokens = self.cls_token.expand(\n            B, -1, -1\n        )  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n\n    x = x + pos_embed\n    x = self.pos_drop(x)\n\n    for blk in self.blocks:\n        x = blk(x)\n\n    x = self.norm(x)\n\n    return x\n\n\nactivations = {}\n\n\ndef get_activation(name):\n    def hook(model, input, output):\n        activations[name] = output\n\n    return hook\n\n\ndef get_readout_oper(vit_features, features, use_readout, start_index=1):\n    if use_readout == \"ignore\":\n        readout_oper = [Slice(start_index)] * len(features)\n    elif use_readout == \"add\":\n        readout_oper = [AddReadout(start_index)] * len(features)\n    elif use_readout == \"project\":\n        readout_oper = [\n            ProjectReadout(vit_features, start_index) for out_feat in features\n        ]\n    else:\n        assert (\n            False\n        ), \"wrong operation for readout token, use_readout can be 'ignore', 'add', or 'project'\"\n\n    return readout_oper\n\n\ndef _make_vit_b16_backbone(\n    model,\n    features=[96, 192, 384, 768],\n    size=[384, 384],\n    hooks=[2, 5, 8, 11],\n    vit_features=768,\n    use_readout=\"ignore\",\n    start_index=1,\n):\n    pretrained = nn.Module()\n\n    pretrained.model = model\n    pretrained.model.blocks[hooks[0]].register_forward_hook(get_activation(\"1\"))\n    pretrained.model.blocks[hooks[1]].register_forward_hook(get_activation(\"2\"))\n    pretrained.model.blocks[hooks[2]].register_forward_hook(get_activation(\"3\"))\n    pretrained.model.blocks[hooks[3]].register_forward_hook(get_activation(\"4\"))\n\n    pretrained.activations = activations\n\n    readout_oper = get_readout_oper(vit_features, features, use_readout, start_index)\n\n    # 32, 48, 136, 384\n    pretrained.act_postprocess1 = nn.Sequential(\n        readout_oper[0],\n        Transpose(1, 2),\n        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n        nn.Conv2d(\n            in_channels=vit_features,\n            out_channels=features[0],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        ),\n        nn.ConvTranspose2d(\n            in_channels=features[0],\n            out_channels=features[0],\n            kernel_size=4,\n            stride=4,\n            padding=0,\n            bias=True,\n            dilation=1,\n            groups=1,\n        ),\n    )\n\n    pretrained.act_postprocess2 = nn.Sequential(\n        readout_oper[1],\n        Transpose(1, 2),\n        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n        nn.Conv2d(\n            in_channels=vit_features,\n            out_channels=features[1],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        ),\n        nn.ConvTranspose2d(\n            in_channels=features[1],\n            out_channels=features[1],\n            kernel_size=2,\n            stride=2,\n            padding=0,\n            bias=True,\n            dilation=1,\n            groups=1,\n        ),\n    )\n\n    pretrained.act_postprocess3 = nn.Sequential(\n        readout_oper[2],\n        Transpose(1, 2),\n        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n        nn.Conv2d(\n            in_channels=vit_features,\n            out_channels=features[2],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        ),\n    )\n\n    pretrained.act_postprocess4 = nn.Sequential(\n        readout_oper[3],\n        Transpose(1, 2),\n        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n        nn.Conv2d(\n            in_channels=vit_features,\n            out_channels=features[3],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        ),\n        nn.Conv2d(\n            in_channels=features[3],\n            out_channels=features[3],\n            kernel_size=3,\n            stride=2,\n            padding=1,\n        ),\n    )\n\n    pretrained.model.start_index = start_index\n    pretrained.model.patch_size = [16, 16]\n\n    # We inject this function into the VisionTransformer instances so that\n    # we can use it with interpolated position embeddings without modifying the library source.\n    pretrained.model.forward_flex = types.MethodType(forward_flex, pretrained.model)\n    pretrained.model._resize_pos_embed = types.MethodType(\n        _resize_pos_embed, pretrained.model\n    )\n\n    return pretrained\n\n\ndef _make_pretrained_vitl16_384(pretrained, use_readout=\"ignore\", hooks=None):\n    model = timm.create_model(\"vit_large_patch16_384\", pretrained=pretrained)\n\n    hooks = [5, 11, 17, 23] if hooks == None else hooks\n    return _make_vit_b16_backbone(\n        model,\n        features=[256, 512, 1024, 1024],\n        hooks=hooks,\n        vit_features=1024,\n        use_readout=use_readout,\n    )\n\n\ndef _make_pretrained_vitb16_384(pretrained, use_readout=\"ignore\", hooks=None):\n    model = timm.create_model(\"vit_base_patch16_384\", pretrained=pretrained)\n\n    hooks = [2, 5, 8, 11] if hooks == None else hooks\n    return _make_vit_b16_backbone(\n        model, features=[96, 192, 384, 768], hooks=hooks, use_readout=use_readout\n    )\n\n\ndef _make_pretrained_deitb16_384(pretrained, use_readout=\"ignore\", hooks=None):\n    model = timm.create_model(\"vit_deit_base_patch16_384\", pretrained=pretrained)\n\n    hooks = [2, 5, 8, 11] if hooks == None else hooks\n    return _make_vit_b16_backbone(\n        model, features=[96, 192, 384, 768], hooks=hooks, use_readout=use_readout\n    )\n\n\ndef _make_pretrained_deitb16_distil_384(pretrained, use_readout=\"ignore\", hooks=None):\n    model = timm.create_model(\n        \"vit_deit_base_distilled_patch16_384\", pretrained=pretrained\n    )\n\n    hooks = [2, 5, 8, 11] if hooks == None else hooks\n    return _make_vit_b16_backbone(\n        model,\n        features=[96, 192, 384, 768],\n        hooks=hooks,\n        use_readout=use_readout,\n        start_index=2,\n    )\n\n\ndef _make_vit_b_rn50_backbone(\n    model,\n    features=[256, 512, 768, 768],\n    size=[384, 384],\n    hooks=[0, 1, 8, 11],\n    vit_features=768,\n    use_vit_only=False,\n    use_readout=\"ignore\",\n    start_index=1,\n):\n    pretrained = nn.Module()\n\n    pretrained.model = model\n\n    if use_vit_only == True:\n        pretrained.model.blocks[hooks[0]].register_forward_hook(get_activation(\"1\"))\n        pretrained.model.blocks[hooks[1]].register_forward_hook(get_activation(\"2\"))\n    else:\n        pretrained.model.patch_embed.backbone.stages[0].register_forward_hook(\n            get_activation(\"1\")\n        )\n        pretrained.model.patch_embed.backbone.stages[1].register_forward_hook(\n            get_activation(\"2\")\n        )\n\n    pretrained.model.blocks[hooks[2]].register_forward_hook(get_activation(\"3\"))\n    pretrained.model.blocks[hooks[3]].register_forward_hook(get_activation(\"4\"))\n\n    pretrained.activations = activations\n\n    readout_oper = get_readout_oper(vit_features, features, use_readout, start_index)\n\n    if use_vit_only == True:\n        pretrained.act_postprocess1 = nn.Sequential(\n            readout_oper[0],\n            Transpose(1, 2),\n            nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n            nn.Conv2d(\n                in_channels=vit_features,\n                out_channels=features[0],\n                kernel_size=1,\n                stride=1,\n                padding=0,\n            ),\n            nn.ConvTranspose2d(\n                in_channels=features[0],\n                out_channels=features[0],\n                kernel_size=4,\n                stride=4,\n                padding=0,\n                bias=True,\n                dilation=1,\n                groups=1,\n            ),\n        )\n\n        pretrained.act_postprocess2 = nn.Sequential(\n            readout_oper[1],\n            Transpose(1, 2),\n            nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n            nn.Conv2d(\n                in_channels=vit_features,\n                out_channels=features[1],\n                kernel_size=1,\n                stride=1,\n                padding=0,\n            ),\n            nn.ConvTranspose2d(\n                in_channels=features[1],\n                out_channels=features[1],\n                kernel_size=2,\n                stride=2,\n                padding=0,\n                bias=True,\n                dilation=1,\n                groups=1,\n            ),\n        )\n    else:\n        pretrained.act_postprocess1 = nn.Sequential(\n            nn.Identity(), nn.Identity(), nn.Identity()\n        )\n        pretrained.act_postprocess2 = nn.Sequential(\n            nn.Identity(), nn.Identity(), nn.Identity()\n        )\n\n    pretrained.act_postprocess3 = nn.Sequential(\n        readout_oper[2],\n        Transpose(1, 2),\n        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n        nn.Conv2d(\n            in_channels=vit_features,\n            out_channels=features[2],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        ),\n    )\n\n    pretrained.act_postprocess4 = nn.Sequential(\n        readout_oper[3],\n        Transpose(1, 2),\n        nn.Unflatten(2, torch.Size([size[0] // 16, size[1] // 16])),\n        nn.Conv2d(\n            in_channels=vit_features,\n            out_channels=features[3],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        ),\n        nn.Conv2d(\n            in_channels=features[3],\n            out_channels=features[3],\n            kernel_size=3,\n            stride=2,\n            padding=1,\n        ),\n    )\n\n    pretrained.model.start_index = start_index\n    pretrained.model.patch_size = [16, 16]\n\n    # We inject this function into the VisionTransformer instances so that\n    # we can use it with interpolated position embeddings without modifying the library source.\n    pretrained.model.forward_flex = types.MethodType(forward_flex, pretrained.model)\n\n    # We inject this function into the VisionTransformer instances so that\n    # we can use it with interpolated position embeddings without modifying the library source.\n    pretrained.model._resize_pos_embed = types.MethodType(\n        _resize_pos_embed, pretrained.model\n    )\n\n    return pretrained\n\n\ndef _make_pretrained_vitb_rn50_384(\n    pretrained, use_readout=\"ignore\", hooks=None, use_vit_only=False\n):\n    model = timm.create_model(\"vit_base_resnet50_384\", pretrained=pretrained)\n\n    hooks = [0, 1, 8, 11] if hooks == None else hooks\n    return _make_vit_b_rn50_backbone(\n        model,\n        features=[256, 512, 768, 768],\n        size=[384, 384],\n        hooks=hooks,\n        use_vit_only=use_vit_only,\n        use_readout=use_readout,\n    )\n\ndef _make_encoder(backbone, features, use_pretrained, groups=1, expand=False, exportable=True, hooks=None, use_vit_only=False, use_readout=\"ignore\",):\n    if backbone == \"vitl16_384\":\n        pretrained = _make_pretrained_vitl16_384(\n            use_pretrained, hooks=hooks, use_readout=use_readout\n        )\n        scratch = _make_scratch(\n            [256, 512, 1024, 1024], features, groups=groups, expand=expand\n        )  # ViT-L/16 - 85.0% Top1 (backbone)\n    elif backbone == \"vitb_rn50_384\":\n        pretrained = _make_pretrained_vitb_rn50_384(\n            use_pretrained,\n            hooks=hooks,\n            use_vit_only=use_vit_only,\n            use_readout=use_readout,\n        )\n        scratch = _make_scratch(\n            [256, 512, 768, 768], features, groups=groups, expand=expand\n        )  # ViT-H/16 - 85.0% Top1 (backbone)\n    elif backbone == \"vitb16_384\":\n        pretrained = _make_pretrained_vitb16_384(\n            use_pretrained, hooks=hooks, use_readout=use_readout\n        )\n        scratch = _make_scratch(\n            [96, 192, 384, 768], features, groups=groups, expand=expand\n        )  # ViT-B/16 - 84.6% Top1 (backbone)\n    elif backbone == \"resnext101_wsl\":\n        pretrained = _make_pretrained_resnext101_wsl(use_pretrained)\n        scratch = _make_scratch([256, 512, 1024, 2048], features, groups=groups, expand=expand)     # efficientnet_lite3  \n    elif backbone == \"efficientnet_lite3\":\n        pretrained = _make_pretrained_efficientnet_lite3(use_pretrained, exportable=exportable)\n        scratch = _make_scratch([32, 48, 136, 384], features, groups=groups, expand=expand)  # efficientnet_lite3     \n    else:\n        print(f\"Backbone '{backbone}' not implemented\")\n        assert False\n        \n    return pretrained, scratch\n\n\ndef _make_scratch(in_shape, out_shape, groups=1, expand=False):\n    scratch = nn.Module()\n\n    out_shape1 = out_shape\n    out_shape2 = out_shape\n    out_shape3 = out_shape\n    out_shape4 = out_shape\n    if expand==True:\n        out_shape1 = out_shape\n        out_shape2 = out_shape*2\n        out_shape3 = out_shape*4\n        out_shape4 = out_shape*8\n\n    scratch.layer1_rn = nn.Conv2d(\n        in_shape[0], out_shape1, kernel_size=3, stride=1, padding=1, bias=False, groups=groups\n    )\n    scratch.layer2_rn = nn.Conv2d(\n        in_shape[1], out_shape2, kernel_size=3, stride=1, padding=1, bias=False, groups=groups\n    )\n    scratch.layer3_rn = nn.Conv2d(\n        in_shape[2], out_shape3, kernel_size=3, stride=1, padding=1, bias=False, groups=groups\n    )\n    scratch.layer4_rn = nn.Conv2d(\n        in_shape[3], out_shape4, kernel_size=3, stride=1, padding=1, bias=False, groups=groups\n    )\n\n    return scratch\n\n\ndef _make_pretrained_efficientnet_lite3(use_pretrained, exportable=False):\n    efficientnet = torch.hub.load(\n        \"rwightman/gen-efficientnet-pytorch\",\n        \"tf_efficientnet_lite3\",\n        pretrained=use_pretrained,\n        exportable=exportable\n    )\n    return _make_efficientnet_backbone(efficientnet)\n\n\ndef _make_efficientnet_backbone(effnet):\n    pretrained = nn.Module()\n\n    pretrained.layer1 = nn.Sequential(\n        effnet.conv_stem, effnet.bn1, effnet.act1, *effnet.blocks[0:2]\n    )\n    pretrained.layer2 = nn.Sequential(*effnet.blocks[2:3])\n    pretrained.layer3 = nn.Sequential(*effnet.blocks[3:5])\n    pretrained.layer4 = nn.Sequential(*effnet.blocks[5:9])\n\n    return pretrained\n    \n\ndef _make_resnet_backbone(resnet):\n    pretrained = nn.Module()\n    pretrained.layer1 = nn.Sequential(\n        resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool, resnet.layer1\n    )\n\n    pretrained.layer2 = resnet.layer2\n    pretrained.layer3 = resnet.layer3\n    pretrained.layer4 = resnet.layer4\n\n    return pretrained\n\n\ndef _make_pretrained_resnext101_wsl(use_pretrained):\n    resnet = torch.hub.load(\"facebookresearch/WSL-Images\", \"resnext101_32x8d_wsl\")\n    return _make_resnet_backbone(resnet)\n\n\n\nclass Interpolate(nn.Module):\n    \"\"\"Interpolation module.\n    \"\"\"\n\n    def __init__(self, scale_factor, mode, align_corners=False):\n        \"\"\"Init.\n        Args:\n            scale_factor (float): scaling\n            mode (str): interpolation mode\n        \"\"\"\n        super(Interpolate, self).__init__()\n\n        self.interp = nn.functional.interpolate\n        self.scale_factor = scale_factor\n        self.mode = mode\n        self.align_corners = align_corners\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n        Args:\n            x (tensor): input\n        Returns:\n            tensor: interpolated data\n        \"\"\"\n\n        x = self.interp(\n            x, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners\n        )\n\n        return x\n\n\nclass ResidualConvUnit(nn.Module):\n    \"\"\"Residual convolution module.\n    \"\"\"\n\n    def __init__(self, features):\n        \"\"\"Init.\n        Args:\n            features (int): number of features\n        \"\"\"\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(\n            features, features, kernel_size=3, stride=1, padding=1, bias=True\n        )\n\n        self.conv2 = nn.Conv2d(\n            features, features, kernel_size=3, stride=1, padding=1, bias=True\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n        Args:\n            x (tensor): input\n        Returns:\n            tensor: output\n        \"\"\"\n        out = self.relu(x)\n        out = self.conv1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        return out + x\n\n\nclass FeatureFusionBlock(nn.Module):\n    \"\"\"Feature fusion block.\n    \"\"\"\n\n    def __init__(self, features):\n        \"\"\"Init.\n        Args:\n            features (int): number of features\n        \"\"\"\n        super(FeatureFusionBlock, self).__init__()\n\n        self.resConfUnit1 = ResidualConvUnit(features)\n        self.resConfUnit2 = ResidualConvUnit(features)\n\n    def forward(self, *xs):\n        \"\"\"Forward pass.\n        Returns:\n            tensor: output\n        \"\"\"\n        output = xs[0]\n\n        if len(xs) == 2:\n            output += self.resConfUnit1(xs[1])\n\n        output = self.resConfUnit2(output)\n\n        output = nn.functional.interpolate(\n            output, scale_factor=2, mode=\"bilinear\", align_corners=True\n        )\n\n        return output\n\n\n\n\nclass ResidualConvUnit_custom(nn.Module):\n    \"\"\"Residual convolution module.\n    \"\"\"\n\n    def __init__(self, features, activation, bn):\n        \"\"\"Init.\n        Args:\n            features (int): number of features\n        \"\"\"\n        super().__init__()\n\n        self.bn = bn\n\n        self.groups=1\n\n        self.conv1 = nn.Conv2d(\n            features, features, kernel_size=3, stride=1, padding=1, bias=True, groups=self.groups\n        )\n        \n        self.conv2 = nn.Conv2d(\n            features, features, kernel_size=3, stride=1, padding=1, bias=True, groups=self.groups\n        )\n\n        if self.bn==True:\n            self.bn1 = nn.BatchNorm2d(features)\n            self.bn2 = nn.BatchNorm2d(features)\n\n        self.activation = activation\n\n        self.skip_add = nn.quantized.FloatFunctional()\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n        Args:\n            x (tensor): input\n        Returns:\n            tensor: output\n        \"\"\"\n        \n        out = self.activation(x)\n        out = self.conv1(out)\n        if self.bn==True:\n            out = self.bn1(out)\n       \n        out = self.activation(out)\n        out = self.conv2(out)\n        if self.bn==True:\n            out = self.bn2(out)\n\n        if self.groups > 1:\n            out = self.conv_merge(out)\n\n        return self.skip_add.add(out, x)\n\n        # return out + x\n\n\nclass FeatureFusionBlock_custom(nn.Module):\n    \"\"\"Feature fusion block.\n    \"\"\"\n\n    def __init__(self, features, activation, deconv=False, bn=False, expand=False, align_corners=True):\n        \"\"\"Init.\n        Args:\n            features (int): number of features\n        \"\"\"\n        super(FeatureFusionBlock_custom, self).__init__()\n\n        self.deconv = deconv\n        self.align_corners = align_corners\n\n        self.groups=1\n\n        self.expand = expand\n        out_features = features\n        if self.expand==True:\n            out_features = features//2\n        \n        self.out_conv = nn.Conv2d(features, out_features, kernel_size=1, stride=1, padding=0, bias=True, groups=1)\n\n        self.resConfUnit1 = ResidualConvUnit_custom(features, activation, bn)\n        self.resConfUnit2 = ResidualConvUnit_custom(features, activation, bn)\n        \n        self.skip_add = nn.quantized.FloatFunctional()\n\n    def forward(self, *xs):\n        \"\"\"Forward pass.\n        Returns:\n            tensor: output\n        \"\"\"\n        output = xs[0]\n\n        if len(xs) == 2:\n            res = self.resConfUnit1(xs[1])\n            output = self.skip_add.add(output, res)\n            # output += res\n\n        output = self.resConfUnit2(output)\n\n        output = nn.functional.interpolate(\n            output, scale_factor=2, mode=\"bilinear\", align_corners=self.align_corners\n        )\n\n        output = self.out_conv(output)\n\n        return output        \n\n\n\ndef _make_fusion_block(features, use_bn):\n    return FeatureFusionBlock_custom(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,\n        expand=False,\n        align_corners=True,\n    )\n\n\nclass DPT(BaseModel):\n    def __init__(\n        self,\n        head,\n        features=256,\n        backbone=\"vitb_rn50_384\",\n        readout=\"project\",\n        channels_last=False,\n        use_bn=False,\n    ):\n\n        super(DPT, self).__init__()\n\n        self.channels_last = channels_last\n\n        hooks = {\n            \"vitb_rn50_384\": [0, 1, 8, 11],\n            \"vitb16_384\": [2, 5, 8, 11],\n            \"vitl16_384\": [5, 11, 17, 23],\n        }\n\n        # Instantiate backbone and reassemble blocks\n        self.pretrained, self.scratch = _make_encoder(\n            backbone,\n            features,\n            True, # Set to true of you want to train from scratch, uses ImageNet weights\n            groups=1,\n            expand=False,\n            exportable=False,\n            hooks=hooks[backbone],\n            use_readout=readout,\n        )\n\n        self.scratch.refinenet1 = _make_fusion_block(features, use_bn)\n        self.scratch.refinenet2 = _make_fusion_block(features, use_bn)\n        self.scratch.refinenet3 = _make_fusion_block(features, use_bn)\n        self.scratch.refinenet4 = _make_fusion_block(features, use_bn)\n\n        self.scratch.output_conv = head\n\n\n    def forward(self, x):\n        if self.channels_last == True:\n            x.contiguous(memory_format=torch.channels_last)\n\n        layer_1, layer_2, layer_3, layer_4 = forward_vit(self.pretrained, x)\n\n        layer_1_rn = self.scratch.layer1_rn(layer_1)\n        layer_2_rn = self.scratch.layer2_rn(layer_2)\n        layer_3_rn = self.scratch.layer3_rn(layer_3)\n        layer_4_rn = self.scratch.layer4_rn(layer_4)\n\n        path_4 = self.scratch.refinenet4(layer_4_rn)\n        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)\n        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)\n        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)\n\n        out = self.scratch.output_conv(path_1)\n\n        return out\n\nclass DPTDepthModel(DPT):\n    def __init__(self, path=None, non_negative=True, num_channels=1, **kwargs):\n        features = kwargs[\"features\"] if \"features\" in kwargs else 256\n\n        head = nn.Sequential(\n            nn.Conv2d(features, features // 2, kernel_size=3, stride=1, padding=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\", align_corners=True),\n            nn.Conv2d(features // 2, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(32, num_channels, kernel_size=1, stride=1, padding=0),\n            nn.ReLU(True) if non_negative else nn.Identity(),\n            nn.Identity(),\n        )\n\n        super().__init__(head, **kwargs)\n\n        if path is not None:\n           self.load(path)\n\n    def forward(self, x):\n        return super().forward(x).squeeze(dim=1)"
        },
        {
          "name": "encoding.py",
          "type": "blob",
          "size": 3.47265625,
          "content": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FreqEncoder_torch(nn.Module):\n    def __init__(self, input_dim, max_freq_log2, N_freqs,\n                 log_sampling=True, include_input=True,\n                 periodic_fns=(torch.sin, torch.cos)):\n    \n        super().__init__()\n\n        self.input_dim = input_dim\n        self.include_input = include_input\n        self.periodic_fns = periodic_fns\n        self.N_freqs = N_freqs\n\n        self.output_dim = 0\n        if self.include_input:\n            self.output_dim += self.input_dim\n\n        self.output_dim += self.input_dim * N_freqs * len(self.periodic_fns)\n\n        if log_sampling:\n            self.freq_bands = 2 ** torch.linspace(0, max_freq_log2, N_freqs)\n        else:\n            self.freq_bands = torch.linspace(2 ** 0, 2 ** max_freq_log2, N_freqs)\n\n        self.freq_bands = self.freq_bands.numpy().tolist()\n\n    def forward(self, input, max_level=None, **kwargs):\n\n        if max_level is None:\n            max_level = self.N_freqs\n        else:\n            max_level = int(max_level * self.N_freqs)\n\n        out = []\n        if self.include_input:\n            out.append(input)\n\n        for i in range(max_level):\n            freq = self.freq_bands[i]\n            for p_fn in self.periodic_fns:\n                out.append(p_fn(input * freq))\n\n        # append 0\n        if self.N_freqs - max_level > 0:\n            out.append(torch.zeros(*input.shape[:-1], (self.N_freqs - max_level) * 2 * input.shape[-1], device=input.device, dtype=input.dtype))\n        \n        out = torch.cat(out, dim=-1)\n\n        return out\n\ndef get_encoder(encoding, input_dim=3, \n                multires=6, \n                degree=4,\n                num_levels=16, level_dim=2, base_resolution=16, log2_hashmap_size=19, desired_resolution=2048, align_corners=False, interpolation='linear',\n                **kwargs):\n\n    if encoding == 'None':\n        return lambda x, **kwargs: x, input_dim\n    \n    elif encoding == 'frequency_torch':\n        encoder = FreqEncoder_torch(input_dim=input_dim, max_freq_log2=multires-1, N_freqs=multires, log_sampling=True)\n\n    elif encoding == 'frequency': # CUDA implementation, faster than torch.\n        from freqencoder import FreqEncoder\n        encoder = FreqEncoder(input_dim=input_dim, degree=multires)\n\n    elif encoding == 'sphere_harmonics':\n        from shencoder import SHEncoder\n        encoder = SHEncoder(input_dim=input_dim, degree=degree)\n\n    elif encoding == 'hashgrid':\n        from gridencoder import GridEncoder\n        encoder = GridEncoder(input_dim=input_dim, num_levels=num_levels, level_dim=level_dim, base_resolution=base_resolution, log2_hashmap_size=log2_hashmap_size, desired_resolution=desired_resolution, gridtype='hash', align_corners=align_corners, interpolation=interpolation)\n    \n    elif encoding == 'tiledgrid':\n        from gridencoder import GridEncoder\n        encoder = GridEncoder(input_dim=input_dim, num_levels=num_levels, level_dim=level_dim, base_resolution=base_resolution, log2_hashmap_size=log2_hashmap_size, desired_resolution=desired_resolution, gridtype='tiled', align_corners=align_corners, interpolation=interpolation)\n    \n    elif encoding == 'hashgrid_taichi':\n        from taichi_modules.hash_encoder import HashEncoderTaichi\n        encoder = HashEncoderTaichi(batch_size=4096) #TODO: hard encoded batch size\n\n    else:\n        raise NotImplementedError('Unknown encoding mode, choose from [None, frequency, sphere_harmonics, hashgrid, tiledgrid]')\n\n    return encoder, encoder.output_dim"
        },
        {
          "name": "evaluation",
          "type": "tree",
          "content": null
        },
        {
          "name": "freqencoder",
          "type": "tree",
          "content": null
        },
        {
          "name": "gridencoder",
          "type": "tree",
          "content": null
        },
        {
          "name": "guidance",
          "type": "tree",
          "content": null
        },
        {
          "name": "ldm",
          "type": "tree",
          "content": null
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 23.701171875,
          "content": "import torch\nimport argparse\nimport pandas as pd\nimport sys\n\nfrom nerf.provider import NeRFDataset\nfrom nerf.utils import *\n\n# torch.autograd.set_detect_anomaly(True)\n\nif __name__ == '__main__':\n    # See https://stackoverflow.com/questions/27433316/how-to-get-argparse-to-read-arguments-from-a-file-with-an-option-rather-than-pre\n    class LoadFromFile (argparse.Action):\n        def __call__ (self, parser, namespace, values, option_string = None):\n            with values as f:\n                # parse arguments in the file and store them in the target namespace\n                parser.parse_args(f.read().split(), namespace)\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--file', type=open, action=LoadFromFile, help=\"specify a file filled with more arguments\")\n    parser.add_argument('--text', default=None, help=\"text prompt\")\n    parser.add_argument('--negative', default='', type=str, help=\"negative text prompt\")\n    parser.add_argument('-O', action='store_true', help=\"equals --fp16 --cuda_ray\")\n    parser.add_argument('-O2', action='store_true', help=\"equals --backbone vanilla\")\n    parser.add_argument('--test', action='store_true', help=\"test mode\")\n    parser.add_argument('--six_views', action='store_true', help=\"six_views mode: save the images of the six views\")\n    parser.add_argument('--eval_interval', type=int, default=1, help=\"evaluate on the valid set every interval epochs\")\n    parser.add_argument('--test_interval', type=int, default=100, help=\"test on the test set every interval epochs\")\n    parser.add_argument('--workspace', type=str, default='workspace')\n    parser.add_argument('--seed', default=None)\n\n    parser.add_argument('--image', default=None, help=\"image prompt\")\n    parser.add_argument('--image_config', default=None, help=\"image config csv\")\n\n    parser.add_argument('--known_view_interval', type=int, default=4, help=\"train default view with RGB loss every & iters, only valid if --image is not None.\")\n\n    parser.add_argument('--IF', action='store_true', help=\"experimental: use DeepFloyd IF as the guidance model for nerf stage\")\n\n    parser.add_argument('--guidance', type=str, nargs='*', default=['SD'], help='guidance model')\n    parser.add_argument('--guidance_scale', type=float, default=100, help=\"diffusion model classifier-free guidance scale\")\n\n    parser.add_argument('--save_mesh', action='store_true', help=\"export an obj mesh with texture\")\n    parser.add_argument('--mcubes_resolution', type=int, default=256, help=\"mcubes resolution for extracting mesh\")\n    parser.add_argument('--decimate_target', type=int, default=5e4, help=\"target face number for mesh decimation\")\n\n    parser.add_argument('--dmtet', action='store_true', help=\"use dmtet finetuning\")\n    parser.add_argument('--tet_grid_size', type=int, default=128, help=\"tet grid size\")\n    parser.add_argument('--init_with', type=str, default='', help=\"ckpt to init dmtet\")\n    parser.add_argument('--lock_geo', action='store_true', help=\"disable dmtet to learn geometry\")\n\n    ## Perp-Neg options\n    parser.add_argument('--perpneg', action='store_true', help=\"use perp_neg\")\n    parser.add_argument('--negative_w', type=float, default=-2, help=\"The scale of the weights of negative prompts. A larger value will help to avoid the Janus problem, but may cause flat faces. Vary between 0 to -4, depending on the prompt\")\n    parser.add_argument('--front_decay_factor', type=float, default=2, help=\"decay factor for the front prompt\")\n    parser.add_argument('--side_decay_factor', type=float, default=10, help=\"decay factor for the side prompt\")\n\n    ### training options\n    parser.add_argument('--iters', type=int, default=10000, help=\"training iters\")\n    parser.add_argument('--lr', type=float, default=1e-3, help=\"max learning rate\")\n    parser.add_argument('--ckpt', type=str, default='latest', help=\"possible options are ['latest', 'scratch', 'best', 'latest_model']\")\n    parser.add_argument('--cuda_ray', action='store_true', help=\"use CUDA raymarching instead of pytorch\")\n    parser.add_argument('--taichi_ray', action='store_true', help=\"use taichi raymarching\")\n    parser.add_argument('--max_steps', type=int, default=1024, help=\"max num steps sampled per ray (only valid when using --cuda_ray)\")\n    parser.add_argument('--num_steps', type=int, default=64, help=\"num steps sampled per ray (only valid when not using --cuda_ray)\")\n    parser.add_argument('--upsample_steps', type=int, default=32, help=\"num steps up-sampled per ray (only valid when not using --cuda_ray)\")\n    parser.add_argument('--update_extra_interval', type=int, default=16, help=\"iter interval to update extra status (only valid when using --cuda_ray)\")\n    parser.add_argument('--max_ray_batch', type=int, default=4096, help=\"batch size of rays at inference to avoid OOM (only valid when not using --cuda_ray)\")\n    parser.add_argument('--latent_iter_ratio', type=float, default=0.2, help=\"training iters that only use albedo shading\")\n    parser.add_argument('--albedo_iter_ratio', type=float, default=0, help=\"training iters that only use albedo shading\")\n    parser.add_argument('--min_ambient_ratio', type=float, default=0.1, help=\"minimum ambient ratio to use in lambertian shading\")\n    parser.add_argument('--textureless_ratio', type=float, default=0.2, help=\"ratio of textureless shading\")\n    parser.add_argument('--jitter_pose', action='store_true', help=\"add jitters to the randomly sampled camera poses\")\n    parser.add_argument('--jitter_center', type=float, default=0.2, help=\"amount of jitter to add to sampled camera pose's center (camera location)\")\n    parser.add_argument('--jitter_target', type=float, default=0.2, help=\"amount of jitter to add to sampled camera pose's target (i.e. 'look-at')\")\n    parser.add_argument('--jitter_up', type=float, default=0.02, help=\"amount of jitter to add to sampled camera pose's up-axis (i.e. 'camera roll')\")\n    parser.add_argument('--uniform_sphere_rate', type=float, default=0, help=\"likelihood of sampling camera location uniformly on the sphere surface area\")\n    parser.add_argument('--grad_clip', type=float, default=-1, help=\"clip grad of all grad to this limit, negative value disables it\")\n    parser.add_argument('--grad_clip_rgb', type=float, default=-1, help=\"clip grad of rgb space grad to this limit, negative value disables it\")\n    # model options\n    parser.add_argument('--bg_radius', type=float, default=1.4, help=\"if positive, use a background model at sphere(bg_radius)\")\n    parser.add_argument('--density_activation', type=str, default='exp', choices=['softplus', 'exp'], help=\"density activation function\")\n    parser.add_argument('--density_thresh', type=float, default=10, help=\"threshold for density grid to be occupied\")\n    parser.add_argument('--blob_density', type=float, default=5, help=\"max (center) density for the density blob\")\n    parser.add_argument('--blob_radius', type=float, default=0.2, help=\"control the radius for the density blob\")\n    # network backbone\n    parser.add_argument('--backbone', type=str, default='grid', choices=['grid_tcnn', 'grid', 'vanilla', 'grid_taichi'], help=\"nerf backbone\")\n    parser.add_argument('--optim', type=str, default='adan', choices=['adan', 'adam'], help=\"optimizer\")\n    parser.add_argument('--sd_version', type=str, default='2.1', choices=['1.5', '2.0', '2.1'], help=\"stable diffusion version\")\n    parser.add_argument('--hf_key', type=str, default=None, help=\"hugging face Stable diffusion model key\")\n    # try this if CUDA OOM\n    parser.add_argument('--fp16', action='store_true', help=\"use float16 for training\")\n    parser.add_argument('--vram_O', action='store_true', help=\"optimization for low VRAM usage\")\n    # rendering resolution in training, increase these for better quality / decrease these if CUDA OOM even if --vram_O enabled.\n    parser.add_argument('--w', type=int, default=64, help=\"render width for NeRF in training\")\n    parser.add_argument('--h', type=int, default=64, help=\"render height for NeRF in training\")\n    parser.add_argument('--known_view_scale', type=float, default=1.5, help=\"multiply --h/w by this for known view rendering\")\n    parser.add_argument('--known_view_noise_scale', type=float, default=2e-3, help=\"random camera noise added to rays_o and rays_d\")\n    parser.add_argument('--dmtet_reso_scale', type=float, default=8, help=\"multiply --h/w by this for dmtet finetuning\")\n    parser.add_argument('--batch_size', type=int, default=1, help=\"images to render per batch using NeRF\")\n\n    ### dataset options\n    parser.add_argument('--bound', type=float, default=1, help=\"assume the scene is bounded in box(-bound, bound)\")\n    parser.add_argument('--dt_gamma', type=float, default=0, help=\"dt_gamma (>=0) for adaptive ray marching. set to 0 to disable, >0 to accelerate rendering (but usually with worse quality)\")\n    parser.add_argument('--min_near', type=float, default=0.01, help=\"minimum near distance for camera\")\n\n    parser.add_argument('--radius_range', type=float, nargs='*', default=[3.0, 3.5], help=\"training camera radius range\")\n    parser.add_argument('--theta_range', type=float, nargs='*', default=[45, 105], help=\"training camera range along the polar angles (i.e. up and down). See advanced.md for details.\")\n    parser.add_argument('--phi_range', type=float, nargs='*', default=[-180, 180], help=\"training camera range along the azimuth angles (i.e. left and right). See advanced.md for details.\")\n    parser.add_argument('--fovy_range', type=float, nargs='*', default=[10, 30], help=\"training camera fovy range\")\n\n    parser.add_argument('--default_radius', type=float, default=3.2, help=\"radius for the default view\")\n    parser.add_argument('--default_polar', type=float, default=90, help=\"polar for the default view\")\n    parser.add_argument('--default_azimuth', type=float, default=0, help=\"azimuth for the default view\")\n    parser.add_argument('--default_fovy', type=float, default=20, help=\"fovy for the default view\")\n\n    parser.add_argument('--progressive_view', action='store_true', help=\"progressively expand view sampling range from default to full\")\n    parser.add_argument('--progressive_view_init_ratio', type=float, default=0.2, help=\"initial ratio of final range, used for progressive_view\")\n    \n    parser.add_argument('--progressive_level', action='store_true', help=\"progressively increase gridencoder's max_level\")\n\n    parser.add_argument('--angle_overhead', type=float, default=30, help=\"[0, angle_overhead] is the overhead region\")\n    parser.add_argument('--angle_front', type=float, default=60, help=\"[0, angle_front] is the front region, [180, 180+angle_front] the back region, otherwise the side region.\")\n    parser.add_argument('--t_range', type=float, nargs='*', default=[0.02, 0.98], help=\"stable diffusion time steps range\")\n    parser.add_argument('--dont_override_stuff',action='store_true', help=\"Don't override t_range, etc.\")\n\n\n    ### regularizations\n    parser.add_argument('--lambda_entropy', type=float, default=1e-3, help=\"loss scale for alpha entropy\")\n    parser.add_argument('--lambda_opacity', type=float, default=0, help=\"loss scale for alpha value\")\n    parser.add_argument('--lambda_orient', type=float, default=1e-2, help=\"loss scale for orientation\")\n    parser.add_argument('--lambda_tv', type=float, default=0, help=\"loss scale for total variation\")\n    parser.add_argument('--lambda_wd', type=float, default=0, help=\"loss scale\")\n\n    parser.add_argument('--lambda_mesh_normal', type=float, default=0.5, help=\"loss scale for mesh normal smoothness\")\n    parser.add_argument('--lambda_mesh_laplacian', type=float, default=0.5, help=\"loss scale for mesh laplacian\")\n\n    parser.add_argument('--lambda_guidance', type=float, default=1, help=\"loss scale for SDS\")\n    parser.add_argument('--lambda_rgb', type=float, default=1000, help=\"loss scale for RGB\")\n    parser.add_argument('--lambda_mask', type=float, default=500, help=\"loss scale for mask (alpha)\")\n    parser.add_argument('--lambda_normal', type=float, default=0, help=\"loss scale for normal map\")\n    parser.add_argument('--lambda_depth', type=float, default=10, help=\"loss scale for relative depth\")\n    parser.add_argument('--lambda_2d_normal_smooth', type=float, default=0, help=\"loss scale for 2D normal image smoothness\")\n    parser.add_argument('--lambda_3d_normal_smooth', type=float, default=0, help=\"loss scale for 3D normal image smoothness\")\n\n    ### debugging options\n    parser.add_argument('--save_guidance', action='store_true', help=\"save images of the per-iteration NeRF renders, added noise, denoised (i.e. guidance), fully-denoised. Useful for debugging, but VERY SLOW and takes lots of memory!\")\n    parser.add_argument('--save_guidance_interval', type=int, default=10, help=\"save guidance every X step\")\n\n    ### GUI options\n    parser.add_argument('--gui', action='store_true', help=\"start a GUI\")\n    parser.add_argument('--W', type=int, default=800, help=\"GUI width\")\n    parser.add_argument('--H', type=int, default=800, help=\"GUI height\")\n    parser.add_argument('--radius', type=float, default=5, help=\"default GUI camera radius from center\")\n    parser.add_argument('--fovy', type=float, default=20, help=\"default GUI camera fovy\")\n    parser.add_argument('--light_theta', type=float, default=60, help=\"default GUI light direction in [0, 180], corresponding to elevation [90, -90]\")\n    parser.add_argument('--light_phi', type=float, default=0, help=\"default GUI light direction in [0, 360), azimuth\")\n    parser.add_argument('--max_spp', type=int, default=1, help=\"GUI rendering max sample per pixel\")\n\n    parser.add_argument('--zero123_config', type=str, default='./pretrained/zero123/sd-objaverse-finetune-c_concat-256.yaml', help=\"config file for zero123\")\n    parser.add_argument('--zero123_ckpt', type=str, default='pretrained/zero123/zero123-xl.ckpt', help=\"ckpt for zero123\")\n    parser.add_argument('--zero123_grad_scale', type=str, default='angle', help=\"whether to scale the gradients based on 'angle' or 'None'\")\n\n    parser.add_argument('--dataset_size_train', type=int, default=100, help=\"Length of train dataset i.e. # of iterations per epoch\")\n    parser.add_argument('--dataset_size_valid', type=int, default=8, help=\"# of frames to render in the turntable video in validation\")\n    parser.add_argument('--dataset_size_test', type=int, default=100, help=\"# of frames to render in the turntable video at test time\")\n\n    parser.add_argument('--exp_start_iter', type=int, default=None, help=\"start iter # for experiment, to calculate progressive_view and progressive_level\")\n    parser.add_argument('--exp_end_iter', type=int, default=None, help=\"end iter # for experiment, to calculate progressive_view and progressive_level\")\n\n    opt = parser.parse_args()\n\n    if opt.O:\n        opt.fp16 = True\n        opt.cuda_ray = True\n\n    elif opt.O2:\n        opt.fp16 = True\n        opt.backbone = 'vanilla'\n        opt.progressive_level = True\n\n    if opt.IF:\n        if 'SD' in opt.guidance:\n            opt.guidance.remove('SD')\n            opt.guidance.append('IF')\n        opt.latent_iter_ratio = 0 # must not do as_latent\n\n    opt.images, opt.ref_radii, opt.ref_polars, opt.ref_azimuths, opt.zero123_ws = [], [], [], [], []\n    opt.default_zero123_w = 1\n\n    opt.exp_start_iter = opt.exp_start_iter or 0\n    opt.exp_end_iter = opt.exp_end_iter or opt.iters\n\n    # parameters for image-conditioned generation\n    if opt.image is not None or opt.image_config is not None:\n\n        if opt.text is None:\n            # use zero123 guidance model when only providing image\n            opt.guidance = ['zero123']\n            if not opt.dont_override_stuff:\n                opt.fovy_range = [opt.default_fovy, opt.default_fovy] # fix fov as zero123 doesn't support changing fov\n                opt.guidance_scale = 5\n                opt.lambda_3d_normal_smooth = 10\n        else:\n            # use stable-diffusion when providing both text and image\n            opt.guidance = ['SD', 'clip']\n            \n            if not opt.dont_override_stuff:\n                opt.guidance_scale = 10\n                opt.t_range = [0.2, 0.6]\n                opt.known_view_interval = 2\n                opt.lambda_3d_normal_smooth = 20\n            opt.bg_radius = -1\n\n        # smoothness\n        opt.lambda_entropy = 1\n        opt.lambda_orient = 1\n\n        # latent warmup is not needed\n        opt.latent_iter_ratio = 0\n        if not opt.dont_override_stuff:\n            opt.albedo_iter_ratio = 0\n            \n            # make shape init more stable\n            opt.progressive_view = True\n            opt.progressive_level = True\n\n        if opt.image is not None:\n            opt.images += [opt.image]\n            opt.ref_radii += [opt.default_radius]\n            opt.ref_polars += [opt.default_polar]\n            opt.ref_azimuths += [opt.default_azimuth]\n            opt.zero123_ws += [opt.default_zero123_w]\n\n        if opt.image_config is not None:\n            # for multiview (zero123)\n            conf = pd.read_csv(opt.image_config, skipinitialspace=True)\n            opt.images += list(conf.image)\n            opt.ref_radii += list(conf.radius)\n            opt.ref_polars += list(conf.polar)\n            opt.ref_azimuths += list(conf.azimuth)\n            opt.zero123_ws += list(conf.zero123_weight)\n            if opt.image is None:\n                opt.default_radius = opt.ref_radii[0]\n                opt.default_polar = opt.ref_polars[0]\n                opt.default_azimuth = opt.ref_azimuths[0]\n                opt.default_zero123_w = opt.zero123_ws[0]\n\n    # reset to None\n    if len(opt.images) == 0:\n        opt.images = None\n\n    # default parameters for finetuning\n    if opt.dmtet:\n\n        opt.h = int(opt.h * opt.dmtet_reso_scale)\n        opt.w = int(opt.w * opt.dmtet_reso_scale)\n        opt.known_view_scale = 1\n\n        if not opt.dont_override_stuff:            \n            opt.t_range = [0.02, 0.50] # ref: magic3D\n\n        if opt.images is not None:\n\n            opt.lambda_normal = 0\n            opt.lambda_depth = 0\n\n            if opt.text is not None and not opt.dont_override_stuff:\n                opt.t_range = [0.20, 0.50]\n\n        # assume finetuning\n        opt.latent_iter_ratio = 0\n        opt.albedo_iter_ratio = 0\n        opt.progressive_view = False\n        # opt.progressive_level = False\n\n    # record full range for progressive view expansion\n    if opt.progressive_view:\n        if not opt.dont_override_stuff:\n            # disable as they disturb progressive view\n            opt.jitter_pose = False\n            \n        opt.uniform_sphere_rate = 0\n        # back up full range\n        opt.full_radius_range = opt.radius_range\n        opt.full_theta_range = opt.theta_range\n        opt.full_phi_range = opt.phi_range\n        opt.full_fovy_range = opt.fovy_range\n\n    if opt.backbone == 'vanilla':\n        from nerf.network import NeRFNetwork\n    elif opt.backbone == 'grid':\n        from nerf.network_grid import NeRFNetwork\n    elif opt.backbone == 'grid_tcnn':\n        from nerf.network_grid_tcnn import NeRFNetwork\n    elif opt.backbone == 'grid_taichi':\n        opt.cuda_ray = False\n        opt.taichi_ray = True\n        import taichi as ti\n        from nerf.network_grid_taichi import NeRFNetwork\n        taichi_half2_opt = True\n        taichi_init_args = {\"arch\": ti.cuda, \"device_memory_GB\": 4.0}\n        if taichi_half2_opt:\n            taichi_init_args[\"half2_vectorization\"] = True\n        ti.init(**taichi_init_args)\n    else:\n        raise NotImplementedError(f'--backbone {opt.backbone} is not implemented!')\n\n    print(opt)\n\n    if opt.seed is not None:\n        seed_everything(int(opt.seed))\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    model = NeRFNetwork(opt).to(device)\n\n    if opt.dmtet and opt.init_with != '':\n        if opt.init_with.endswith('.pth'):\n            # load pretrained weights to init dmtet\n            state_dict = torch.load(opt.init_with, map_location=device)\n            model.load_state_dict(state_dict['model'], strict=False)\n            if opt.cuda_ray:\n                model.mean_density = state_dict['mean_density']\n            model.init_tet()\n        else:\n            # assume a mesh to init dmtet (experimental, not working well now!)\n            import trimesh\n            mesh = trimesh.load(opt.init_with, force='mesh', skip_material=True, process=False)\n            model.init_tet(mesh=mesh)\n\n    print(model)\n\n    if opt.six_views:\n        guidance = None # no need to load guidance model at test\n\n        trainer = Trainer(' '.join(sys.argv), 'df', opt, model, guidance, device=device, workspace=opt.workspace, fp16=opt.fp16, use_checkpoint=opt.ckpt)\n\n        test_loader = NeRFDataset(opt, device=device, type='six_views', H=opt.H, W=opt.W, size=6).dataloader(batch_size=1)\n        trainer.test(test_loader, write_video=False)\n\n        if opt.save_mesh:\n            trainer.save_mesh()\n\n    elif opt.test:\n        guidance = None # no need to load guidance model at test\n\n        trainer = Trainer(' '.join(sys.argv), 'df', opt, model, guidance, device=device, workspace=opt.workspace, fp16=opt.fp16, use_checkpoint=opt.ckpt)\n\n        if opt.gui:\n            from nerf.gui import NeRFGUI\n            gui = NeRFGUI(opt, trainer)\n            gui.render()\n\n        else:\n            test_loader = NeRFDataset(opt, device=device, type='test', H=opt.H, W=opt.W, size=opt.dataset_size_test).dataloader(batch_size=1)\n            trainer.test(test_loader)\n\n            if opt.save_mesh:\n                trainer.save_mesh()\n\n    else:\n\n        train_loader = NeRFDataset(opt, device=device, type='train', H=opt.h, W=opt.w, size=opt.dataset_size_train * opt.batch_size).dataloader()\n\n        if opt.optim == 'adan':\n            from optimizer import Adan\n            # Adan usually requires a larger LR\n            optimizer = lambda model: Adan(model.get_params(5 * opt.lr), eps=1e-8, weight_decay=2e-5, max_grad_norm=5.0, foreach=False)\n        else: # adam\n            optimizer = lambda model: torch.optim.Adam(model.get_params(opt.lr), betas=(0.9, 0.99), eps=1e-15)\n\n        if opt.backbone == 'vanilla':\n            scheduler = lambda optimizer: optim.lr_scheduler.LambdaLR(optimizer, lambda iter: 0.1 ** min(iter / opt.iters, 1))\n        else:\n            scheduler = lambda optimizer: optim.lr_scheduler.LambdaLR(optimizer, lambda iter: 1) # fixed\n            # scheduler = lambda optimizer: optim.lr_scheduler.LambdaLR(optimizer, lambda iter: 0.1 ** min(iter / opt.iters, 1))\n\n        guidance = nn.ModuleDict()\n\n        if 'SD' in opt.guidance:\n            from guidance.sd_utils import StableDiffusion\n            guidance['SD'] = StableDiffusion(device, opt.fp16, opt.vram_O, opt.sd_version, opt.hf_key, opt.t_range)\n\n        if 'IF' in opt.guidance:\n            from guidance.if_utils import IF\n            guidance['IF'] = IF(device, opt.vram_O, opt.t_range)\n\n        if 'zero123' in opt.guidance:\n            from guidance.zero123_utils import Zero123\n            guidance['zero123'] = Zero123(device=device, fp16=opt.fp16, config=opt.zero123_config, ckpt=opt.zero123_ckpt, vram_O=opt.vram_O, t_range=opt.t_range, opt=opt)\n\n        if 'clip' in opt.guidance:\n            from guidance.clip_utils import CLIP\n            guidance['clip'] = CLIP(device)\n\n        trainer = Trainer(' '.join(sys.argv), 'df', opt, model, guidance, device=device, workspace=opt.workspace, optimizer=optimizer, ema_decay=0.95, fp16=opt.fp16, lr_scheduler=scheduler, use_checkpoint=opt.ckpt, scheduler_update_every_step=True)\n\n        trainer.default_view_data = train_loader._data.get_default_view_data()\n\n        if opt.gui:\n            from nerf.gui import NeRFGUI\n            gui = NeRFGUI(opt, trainer, train_loader)\n            gui.render()\n\n        else:\n            valid_loader = NeRFDataset(opt, device=device, type='val', H=opt.H, W=opt.W, size=opt.dataset_size_valid).dataloader(batch_size=1)\n            test_loader = NeRFDataset(opt, device=device, type='test', H=opt.H, W=opt.W, size=opt.dataset_size_test).dataloader(batch_size=1)\n\n            max_epoch = np.ceil(opt.iters / len(train_loader)).astype(np.int32)\n            trainer.train(train_loader, valid_loader, test_loader, max_epoch)\n\n            if opt.save_mesh:\n                trainer.save_mesh()\n"
        },
        {
          "name": "meshutils.py",
          "type": "blob",
          "size": 3.7890625,
          "content": "import numpy as np\nimport pymeshlab as pml\n\ndef poisson_mesh_reconstruction(points, normals=None):\n    # points/normals: [N, 3] np.ndarray\n\n    import open3d as o3d\n\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(points)\n\n    # outlier removal\n    pcd, ind = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=10)\n\n    # normals\n    if normals is None:\n        pcd.estimate_normals()\n    else:\n        pcd.normals = o3d.utility.Vector3dVector(normals[ind])\n\n    # visualize\n    o3d.visualization.draw_geometries([pcd], point_show_normal=False)\n    \n    mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(pcd, depth=9)\n    vertices_to_remove = densities < np.quantile(densities, 0.1)\n    mesh.remove_vertices_by_mask(vertices_to_remove)\n\n    # visualize\n    o3d.visualization.draw_geometries([mesh])\n\n    vertices = np.asarray(mesh.vertices)\n    triangles = np.asarray(mesh.triangles)\n\n    print(f'[INFO] poisson mesh reconstruction: {points.shape} --> {vertices.shape} / {triangles.shape}')\n\n    return vertices, triangles\n    \n\ndef decimate_mesh(verts, faces, target, backend='pymeshlab', remesh=False, optimalplacement=True):\n    # optimalplacement: default is True, but for flat mesh must turn False to prevent spike artifect.\n\n    _ori_vert_shape = verts.shape\n    _ori_face_shape = faces.shape\n\n    if backend == 'pyfqmr':\n        import pyfqmr\n        solver = pyfqmr.Simplify()\n        solver.setMesh(verts, faces)\n        solver.simplify_mesh(target_count=target, preserve_border=False, verbose=False)\n        verts, faces, normals = solver.getMesh()\n    else:\n        \n        m = pml.Mesh(verts, faces)\n        ms = pml.MeshSet()\n        ms.add_mesh(m, 'mesh') # will copy!\n\n        # filters\n        # ms.meshing_decimation_clustering(threshold=pml.Percentage(1))\n        ms.meshing_decimation_quadric_edge_collapse(targetfacenum=int(target), optimalplacement=optimalplacement)\n\n        if remesh:\n            # ms.apply_coord_taubin_smoothing()\n            ms.meshing_isotropic_explicit_remeshing(iterations=3, targetlen=pml.Percentage(1))\n\n        # extract mesh\n        m = ms.current_mesh()\n        verts = m.vertex_matrix()\n        faces = m.face_matrix()\n\n    print(f'[INFO] mesh decimation: {_ori_vert_shape} --> {verts.shape}, {_ori_face_shape} --> {faces.shape}')\n\n    return verts, faces\n\n\ndef clean_mesh(verts, faces, v_pct=1, min_f=8, min_d=5, repair=True, remesh=True, remesh_size=0.01):\n    # verts: [N, 3]\n    # faces: [N, 3]\n\n    _ori_vert_shape = verts.shape\n    _ori_face_shape = faces.shape\n\n    m = pml.Mesh(verts, faces)\n    ms = pml.MeshSet()\n    ms.add_mesh(m, 'mesh') # will copy!\n\n    # filters\n    ms.meshing_remove_unreferenced_vertices() # verts not refed by any faces\n\n    if v_pct > 0:\n        ms.meshing_merge_close_vertices(threshold=pml.Percentage(v_pct)) # 1/10000 of bounding box diagonal\n\n    ms.meshing_remove_duplicate_faces() # faces defined by the same verts\n    ms.meshing_remove_null_faces() # faces with area == 0\n\n    if min_d > 0:\n        ms.meshing_remove_connected_component_by_diameter(mincomponentdiag=pml.Percentage(min_d))\n    \n    if min_f > 0:\n        ms.meshing_remove_connected_component_by_face_number(mincomponentsize=min_f)\n\n    if repair:\n        # ms.meshing_remove_t_vertices(method=0, threshold=40, repeat=True)\n        ms.meshing_repair_non_manifold_edges(method=0)\n        ms.meshing_repair_non_manifold_vertices(vertdispratio=0)\n    \n    if remesh:\n        # ms.apply_coord_taubin_smoothing()\n        ms.meshing_isotropic_explicit_remeshing(iterations=3, targetlen=pml.AbsoluteValue(remesh_size))\n\n    # extract mesh\n    m = ms.current_mesh()\n    verts = m.vertex_matrix()\n    faces = m.face_matrix()\n\n    print(f'[INFO] mesh cleaning: {_ori_vert_shape} --> {verts.shape}, {_ori_face_shape} --> {faces.shape}')\n\n    return verts, faces    "
        },
        {
          "name": "nerf",
          "type": "tree",
          "content": null
        },
        {
          "name": "optimizer.py",
          "type": "blob",
          "size": 11.52734375,
          "content": "# Copyright 2022 Garena Online Private Limited\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nfrom typing import List\n\nimport torch\nfrom torch import Tensor\nfrom torch.optim.optimizer import Optimizer\n\n\nclass Adan(Optimizer):\n    \"\"\"\n    Implements a pytorch variant of Adan\n    Adan was proposed in\n    Adan: Adaptive Nesterov Momentum Algorithm for\n        Faster Optimizing Deep Models[J].arXiv preprint arXiv:2208.06677, 2022.\n    https://arxiv.org/abs/2208.06677\n    Arguments:\n        params (iterable): iterable of parameters to optimize or\n            dicts defining parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float, flot], optional): coefficients used for\n            first- and second-order moments. (default: (0.98, 0.92, 0.99))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability. (default: 1e-8)\n        weight_decay (float, optional): decoupled weight decay\n            (L2 penalty) (default: 0)\n        max_grad_norm (float, optional): value used to clip\n            global grad norm (default: 0.0 no clip)\n        no_prox (bool): how to perform the decoupled weight decay\n            (default: False)\n        foreach (bool): if True would use torch._foreach implementation.\n            It's faster but uses slightly more memory. (default: True)\n    \"\"\"\n    def __init__(self,\n                 params,\n                 lr=1e-3,\n                 betas=(0.98, 0.92, 0.99),\n                 eps=1e-8,\n                 weight_decay=0.0,\n                 max_grad_norm=0.0,\n                 no_prox=False,\n                 foreach: bool = True):\n        if not 0.0 <= max_grad_norm:\n            raise ValueError('Invalid Max grad norm: {}'.format(max_grad_norm))\n        if not 0.0 <= lr:\n            raise ValueError('Invalid learning rate: {}'.format(lr))\n        if not 0.0 <= eps:\n            raise ValueError('Invalid epsilon value: {}'.format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError('Invalid beta parameter at index 0: {}'.format(\n                betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError('Invalid beta parameter at index 1: {}'.format(\n                betas[1]))\n        if not 0.0 <= betas[2] < 1.0:\n            raise ValueError('Invalid beta parameter at index 2: {}'.format(\n                betas[2]))\n        defaults = dict(lr=lr,\n                        betas=betas,\n                        eps=eps,\n                        weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm,\n                        no_prox=no_prox,\n                        foreach=foreach)\n        super().__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(Adan, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('no_prox', False)\n\n    @torch.no_grad()\n    def restart_opt(self):\n        for group in self.param_groups:\n            group['step'] = 0\n            for p in group['params']:\n                if p.requires_grad:\n                    state = self.state[p]\n                    # State initialization\n\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p)\n                    # Exponential moving average of gradient difference\n                    state['exp_avg_diff'] = torch.zeros_like(p)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\"\"\"\n\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        if self.defaults['max_grad_norm'] > 0:\n            device = self.param_groups[0]['params'][0].device\n            global_grad_norm = torch.zeros(1, device=device)\n\n            max_grad_norm = torch.tensor(self.defaults['max_grad_norm'],\n                                         device=device)\n            for group in self.param_groups:\n\n                for p in group['params']:\n                    if p.grad is not None:\n                        grad = p.grad\n                        global_grad_norm.add_(grad.pow(2).sum())\n\n            global_grad_norm = torch.sqrt(global_grad_norm)\n\n            clip_global_grad_norm = torch.clamp(\n                max_grad_norm / (global_grad_norm + group['eps']),\n                max=1.0).item()\n        else:\n            clip_global_grad_norm = 1.0\n\n        for group in self.param_groups:\n            params_with_grad = []\n            grads = []\n            exp_avgs = []\n            exp_avg_sqs = []\n            exp_avg_diffs = []\n            neg_pre_grads = []\n\n            beta1, beta2, beta3 = group['betas']\n            # assume same step across group now to simplify things\n            # per parameter step can be easily support\n            # by making it tensor, or pass list into kernel\n            if 'step' in group:\n                group['step'] += 1\n            else:\n                group['step'] = 1\n\n            bias_correction1 = 1.0 - beta1**group['step']\n            bias_correction2 = 1.0 - beta2**group['step']\n            bias_correction3 = 1.0 - beta3**group['step']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                params_with_grad.append(p)\n                grads.append(p.grad)\n\n                state = self.state[p]\n                if len(state) == 0:\n                    state['exp_avg'] = torch.zeros_like(p)\n                    state['exp_avg_sq'] = torch.zeros_like(p)\n                    state['exp_avg_diff'] = torch.zeros_like(p)\n\n                if 'neg_pre_grad' not in state or group['step'] == 1:\n                    state['neg_pre_grad'] = p.grad.clone().mul_(\n                        -clip_global_grad_norm)\n\n                exp_avgs.append(state['exp_avg'])\n                exp_avg_sqs.append(state['exp_avg_sq'])\n                exp_avg_diffs.append(state['exp_avg_diff'])\n                neg_pre_grads.append(state['neg_pre_grad'])\n\n            kwargs = dict(\n                params=params_with_grad,\n                grads=grads,\n                exp_avgs=exp_avgs,\n                exp_avg_sqs=exp_avg_sqs,\n                exp_avg_diffs=exp_avg_diffs,\n                neg_pre_grads=neg_pre_grads,\n                beta1=beta1,\n                beta2=beta2,\n                beta3=beta3,\n                bias_correction1=bias_correction1,\n                bias_correction2=bias_correction2,\n                bias_correction3_sqrt=math.sqrt(bias_correction3),\n                lr=group['lr'],\n                weight_decay=group['weight_decay'],\n                eps=group['eps'],\n                no_prox=group['no_prox'],\n                clip_global_grad_norm=clip_global_grad_norm,\n            )\n\n            if group['foreach']:\n                _multi_tensor_adan(**kwargs)\n            else:\n                _single_tensor_adan(**kwargs)\n\n        return loss\n\n\ndef _single_tensor_adan(\n    params: List[Tensor],\n    grads: List[Tensor],\n    exp_avgs: List[Tensor],\n    exp_avg_sqs: List[Tensor],\n    exp_avg_diffs: List[Tensor],\n    neg_pre_grads: List[Tensor],\n    *,\n    beta1: float,\n    beta2: float,\n    beta3: float,\n    bias_correction1: float,\n    bias_correction2: float,\n    bias_correction3_sqrt: float,\n    lr: float,\n    weight_decay: float,\n    eps: float,\n    no_prox: bool,\n    clip_global_grad_norm: Tensor,\n):\n    for i, param in enumerate(params):\n        grad = grads[i]\n        exp_avg = exp_avgs[i]\n        exp_avg_sq = exp_avg_sqs[i]\n        exp_avg_diff = exp_avg_diffs[i]\n        neg_grad_or_diff = neg_pre_grads[i]\n\n        grad.mul_(clip_global_grad_norm)\n\n        # for memory saving, we use `neg_grad_or_diff`\n        # to get some temp variable in a inplace way\n        neg_grad_or_diff.add_(grad)\n\n        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n        exp_avg_diff.mul_(beta2).add_(neg_grad_or_diff,\n                                      alpha=1 - beta2)  # diff_t\n\n        neg_grad_or_diff.mul_(beta2).add_(grad)\n        exp_avg_sq.mul_(beta3).addcmul_(neg_grad_or_diff,\n                                        neg_grad_or_diff,\n                                        value=1 - beta3)  # n_t\n\n        denom = ((exp_avg_sq).sqrt() / bias_correction3_sqrt).add_(eps)\n        step_size_diff = lr * beta2 / bias_correction2\n        step_size = lr / bias_correction1\n\n        if no_prox:\n            param.mul_(1 - lr * weight_decay)\n            param.addcdiv_(exp_avg, denom, value=-step_size)\n            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n        else:\n            param.addcdiv_(exp_avg, denom, value=-step_size)\n            param.addcdiv_(exp_avg_diff, denom, value=-step_size_diff)\n            param.div_(1 + lr * weight_decay)\n\n        neg_grad_or_diff.zero_().add_(grad, alpha=-1.0)\n\n\ndef _multi_tensor_adan(\n    params: List[Tensor],\n    grads: List[Tensor],\n    exp_avgs: List[Tensor],\n    exp_avg_sqs: List[Tensor],\n    exp_avg_diffs: List[Tensor],\n    neg_pre_grads: List[Tensor],\n    *,\n    beta1: float,\n    beta2: float,\n    beta3: float,\n    bias_correction1: float,\n    bias_correction2: float,\n    bias_correction3_sqrt: float,\n    lr: float,\n    weight_decay: float,\n    eps: float,\n    no_prox: bool,\n    clip_global_grad_norm: Tensor,\n):\n    if len(params) == 0:\n        return\n\n    torch._foreach_mul_(grads, clip_global_grad_norm)\n\n    # for memory saving, we use `neg_pre_grads`\n    # to get some temp variable in a inplace way\n    torch._foreach_add_(neg_pre_grads, grads)\n\n    torch._foreach_mul_(exp_avgs, beta1)\n    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  # m_t\n\n    torch._foreach_mul_(exp_avg_diffs, beta2)\n    torch._foreach_add_(exp_avg_diffs, neg_pre_grads,\n                        alpha=1 - beta2)  # diff_t\n\n    torch._foreach_mul_(neg_pre_grads, beta2)\n    torch._foreach_add_(neg_pre_grads, grads)\n    torch._foreach_mul_(exp_avg_sqs, beta3)\n    torch._foreach_addcmul_(exp_avg_sqs,\n                            neg_pre_grads,\n                            neg_pre_grads,\n                            value=1 - beta3)  # n_t\n\n    denom = torch._foreach_sqrt(exp_avg_sqs)\n    torch._foreach_div_(denom, bias_correction3_sqrt)\n    torch._foreach_add_(denom, eps)\n\n    step_size_diff = lr * beta2 / bias_correction2\n    step_size = lr / bias_correction1\n\n    if no_prox:\n        torch._foreach_mul_(params, 1 - lr * weight_decay)\n        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n        torch._foreach_addcdiv_(params,\n                                exp_avg_diffs,\n                                denom,\n                                value=-step_size_diff)\n    else:\n        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)\n        torch._foreach_addcdiv_(params,\n                                exp_avg_diffs,\n                                denom,\n                                value=-step_size_diff)\n        torch._foreach_div_(params, 1 + lr * weight_decay)\n    torch._foreach_zero_(neg_pre_grads)\n    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)"
        },
        {
          "name": "preprocess_image.py",
          "type": "blob",
          "size": 7.2646484375,
          "content": "import os\nimport sys\nimport cv2\nimport argparse\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\n\nclass BackgroundRemoval():\n    def __init__(self, device='cuda'):\n\n        from carvekit.api.high import HiInterface\n        self.interface = HiInterface(\n            object_type=\"object\",  # Can be \"object\" or \"hairs-like\".\n            batch_size_seg=5,\n            batch_size_matting=1,\n            device=device,\n            seg_mask_size=640,  # Use 640 for Tracer B7 and 320 for U2Net\n            matting_mask_size=2048,\n            trimap_prob_threshold=231,\n            trimap_dilation=30,\n            trimap_erosion_iters=5,\n            fp16=True,\n        )\n\n    @torch.no_grad()\n    def __call__(self, image):\n        # image: [H, W, 3] array in [0, 255].\n        image = Image.fromarray(image)\n\n        image = self.interface([image])[0]\n        image = np.array(image)\n\n        return image\n\nclass BLIP2():\n    def __init__(self, device='cuda'):\n        self.device = device\n        from transformers import AutoProcessor, Blip2ForConditionalGeneration\n        self.processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n        self.model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16).to(device)\n\n    @torch.no_grad()\n    def __call__(self, image):\n        image = Image.fromarray(image)\n        inputs = self.processor(image, return_tensors=\"pt\").to(self.device, torch.float16)\n\n        generated_ids = self.model.generate(**inputs, max_new_tokens=20)\n        generated_text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n\n        return generated_text\n\n\nclass DPT():\n    def __init__(self, task='depth', device='cuda'):\n\n        self.task = task\n        self.device = device\n\n        from dpt import DPTDepthModel\n\n        if task == 'depth':\n            path = 'pretrained/omnidata/omnidata_dpt_depth_v2.ckpt'\n            self.model = DPTDepthModel(backbone='vitb_rn50_384')\n            self.aug = transforms.Compose([\n                transforms.Resize((384, 384)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=0.5, std=0.5)\n            ])\n\n        else: # normal\n            path = 'pretrained/omnidata/omnidata_dpt_normal_v2.ckpt'\n            self.model = DPTDepthModel(backbone='vitb_rn50_384', num_channels=3)\n            self.aug = transforms.Compose([\n                transforms.Resize((384, 384)),\n                transforms.ToTensor()\n            ])\n\n        # load model\n        checkpoint = torch.load(path, map_location='cpu')\n        if 'state_dict' in checkpoint:\n            state_dict = {}\n            for k, v in checkpoint['state_dict'].items():\n                state_dict[k[6:]] = v\n        else:\n            state_dict = checkpoint\n        self.model.load_state_dict(state_dict)\n        self.model.eval().to(device)\n\n\n    @torch.no_grad()\n    def __call__(self, image):\n        # image: np.ndarray, uint8, [H, W, 3]\n        H, W = image.shape[:2]\n        image = Image.fromarray(image)\n\n        image = self.aug(image).unsqueeze(0).to(self.device)\n\n        if self.task == 'depth':\n            depth = self.model(image).clamp(0, 1)\n            depth = F.interpolate(depth.unsqueeze(1), size=(H, W), mode='bicubic', align_corners=False)\n            depth = depth.squeeze(1).cpu().numpy()\n            return depth\n        else:\n            normal = self.model(image).clamp(0, 1)\n            normal = F.interpolate(normal, size=(H, W), mode='bicubic', align_corners=False)\n            normal = normal.cpu().numpy()\n            return normal\n\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('path', type=str, help=\"path to image (png, jpeg, etc.)\")\n    parser.add_argument('--size', default=256, type=int, help=\"output resolution\")\n    parser.add_argument('--border_ratio', default=0.2, type=float, help=\"output border ratio\")\n    parser.add_argument('--recenter', type=bool, default=True, help=\"recenter, potentially not helpful for multiview zero123\")\n    parser.add_argument('--dont_recenter', dest='recenter', action='store_false')\n    opt = parser.parse_args()\n\n    out_dir = os.path.dirname(opt.path)\n    out_rgba = os.path.join(out_dir, os.path.basename(opt.path).split('.')[0] + '_rgba.png')\n    out_depth = os.path.join(out_dir, os.path.basename(opt.path).split('.')[0] + '_depth.png')\n    out_normal = os.path.join(out_dir, os.path.basename(opt.path).split('.')[0] + '_normal.png')\n    out_caption = os.path.join(out_dir, os.path.basename(opt.path).split('.')[0] + '_caption.txt')\n\n    # load image\n    print(f'[INFO] loading image...')\n    image = cv2.imread(opt.path, cv2.IMREAD_UNCHANGED)\n    if image.shape[-1] == 4:\n        image = cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n    else:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # carve background\n    print(f'[INFO] background removal...')\n    carved_image = BackgroundRemoval()(image) # [H, W, 4]\n    mask = carved_image[..., -1] > 0\n\n    # predict depth\n    print(f'[INFO] depth estimation...')\n    dpt_depth_model = DPT(task='depth')\n    depth = dpt_depth_model(image)[0]\n    depth[mask] = (depth[mask] - depth[mask].min()) / (depth[mask].max() - depth[mask].min() + 1e-9)\n    depth[~mask] = 0\n    depth = (depth * 255).astype(np.uint8)\n    del dpt_depth_model\n\n    # predict normal\n    print(f'[INFO] normal estimation...')\n    dpt_normal_model = DPT(task='normal')\n    normal = dpt_normal_model(image)[0]\n    normal = (normal * 255).astype(np.uint8).transpose(1, 2, 0)\n    normal[~mask] = 0\n    del dpt_normal_model\n\n    # recenter\n    if opt.recenter:\n        print(f'[INFO] recenter...')\n        final_rgba = np.zeros((opt.size, opt.size, 4), dtype=np.uint8)\n        final_depth = np.zeros((opt.size, opt.size), dtype=np.uint8)\n        final_normal = np.zeros((opt.size, opt.size, 3), dtype=np.uint8)\n\n        coords = np.nonzero(mask)\n        x_min, x_max = coords[0].min(), coords[0].max()\n        y_min, y_max = coords[1].min(), coords[1].max()\n        h = x_max - x_min\n        w = y_max - y_min\n        desired_size = int(opt.size * (1 - opt.border_ratio))\n        scale = desired_size / max(h, w)\n        h2 = int(h * scale)\n        w2 = int(w * scale)\n        x2_min = (opt.size - h2) // 2\n        x2_max = x2_min + h2\n        y2_min = (opt.size - w2) // 2\n        y2_max = y2_min + w2\n        final_rgba[x2_min:x2_max, y2_min:y2_max] = cv2.resize(carved_image[x_min:x_max, y_min:y_max], (w2, h2), interpolation=cv2.INTER_AREA)\n        final_depth[x2_min:x2_max, y2_min:y2_max] = cv2.resize(depth[x_min:x_max, y_min:y_max], (w2, h2), interpolation=cv2.INTER_AREA)\n        final_normal[x2_min:x2_max, y2_min:y2_max] = cv2.resize(normal[x_min:x_max, y_min:y_max], (w2, h2), interpolation=cv2.INTER_AREA)\n\n    else:\n        final_rgba = carved_image\n        final_depth = depth\n        final_normal = normal\n\n    # write output\n    cv2.imwrite(out_rgba, cv2.cvtColor(final_rgba, cv2.COLOR_RGBA2BGRA))\n    cv2.imwrite(out_depth, final_depth)\n    cv2.imwrite(out_normal, final_normal)\n\n    # predict caption (it's too slow... use your brain instead)\n    # print(f'[INFO] captioning...')\n    # blip2 = BLIP2()\n    # caption = blip2(image)\n    # with open(out_caption, 'w') as f:\n    #     f.write(caption)\n\n"
        },
        {
          "name": "pretrained",
          "type": "tree",
          "content": null
        },
        {
          "name": "raymarching",
          "type": "tree",
          "content": null
        },
        {
          "name": "readme.md",
          "type": "blob",
          "size": 17.2021484375,
          "content": "# Stable-Dreamfusion\n\nA pytorch implementation of the text-to-3D model **Dreamfusion**, powered by the [Stable Diffusion](https://github.com/CompVis/stable-diffusion) text-to-2D model.\n\n**ADVERTISEMENT: Please check out [threestudio](https://github.com/threestudio-project/threestudio) for recent improvements and better implementation in 3D content generation!**\n\n**NEWS (2023.6.12)**:\n\n* Support of [Perp-Neg](https://perp-neg.github.io/) to alleviate multi-head problem in Text-to-3D.\n* Support of Perp-Neg for both [Stable Diffusion](https://github.com/CompVis/stable-diffusion) and [DeepFloyd-IF](https://github.com/deep-floyd/IF).\n\nhttps://user-images.githubusercontent.com/25863658/236712982-9f93bd32-83bf-423a-bb7c-f73df7ece2e3.mp4\n\nhttps://user-images.githubusercontent.com/25863658/232403162-51b69000-a242-4b8c-9cd9-4242b09863fa.mp4\n\n### [Update Logs](assets/update_logs.md)\n\n### Colab notebooks:\n* Instant-NGP backbone (`-O`): [![Instant-NGP Backbone](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1MXT3yfOFvO0ooKEfiUUvTKwUkrrlCHpF?usp=sharing)\n\n* Vanilla NeRF backbone (`-O2`): [![Vanilla Backbone](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1mvfxG-S_n_gZafWoattku7rLJ2kPoImL?usp=sharing)\n\n# Important Notice\nThis project is a **work-in-progress**, and contains lots of differences from the paper. **The current generation quality cannot match the results from the original paper, and many prompts still fail badly!**\n\n## Notable differences from the paper\n* Since the Imagen model is not publicly available, we use [Stable Diffusion](https://github.com/CompVis/stable-diffusion) to replace it (implementation from [diffusers](https://github.com/huggingface/diffusers)). Different from Imagen, Stable-Diffusion is a latent diffusion model, which diffuses in a latent space instead of the original image space. Therefore, we need the loss to propagate back from the VAE's encoder part too, which introduces extra time cost in training.\n* We use the [multi-resolution grid encoder](https://github.com/NVlabs/instant-ngp/) to implement the NeRF backbone (implementation from [torch-ngp](https://github.com/ashawkey/torch-ngp)), which enables much faster rendering (~10FPS at 800x800).\n* We use the [Adan](https://github.com/sail-sg/Adan) optimizer as default.\n\n# Install\n\n```bash\ngit clone https://github.com/ashawkey/stable-dreamfusion.git\ncd stable-dreamfusion\n```\n\n### Optional: create a python virtual environment\n\nTo avoid python package conflicts, we recommend using a virtual environment, e.g.: using conda or venv:\n\n```bash\npython -m venv venv_stable-dreamfusion\nsource venv_stable-dreamfusion/bin/activate # you need to repeat this step for every new terminal\n```\n\n### Install with pip\n\n```bash\npip install -r requirements.txt\n```\n\n### Download pre-trained models\n\nTo use image-conditioned 3D generation, you need to download some pretrained checkpoints manually:\n* [Zero-1-to-3](https://github.com/cvlab-columbia/zero123) for diffusion backend.\n    We use `zero123-xl.ckpt` by default, and it is hard-coded in `guidance/zero123_utils.py`.\n    ```bash\n    cd pretrained/zero123\n    wget https://zero123.cs.columbia.edu/assets/zero123-xl.ckpt\n    ```\n* [Omnidata](https://github.com/EPFL-VILAB/omnidata/tree/main/omnidata_tools/torch) for depth and normal prediction.\n    These ckpts are hardcoded in `preprocess_image.py`.\n    ```bash\n    mkdir pretrained/omnidata\n    cd pretrained/omnidata\n    # assume gdown is installed\n    gdown '1Jrh-bRnJEjyMCS7f-WsaFlccfPjJPPHI&confirm=t' # omnidata_dpt_depth_v2.ckpt\n    gdown '1wNxVO4vVbDEMEpnAi_jwQObf2MFodcBR&confirm=t' # omnidata_dpt_normal_v2.ckpt\n    ```\n\nTo use [DeepFloyd-IF](https://github.com/deep-floyd/IF), you need to accept the usage conditions from [hugging face](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0), and login with `huggingface-cli login` in command line.\n\nFor DMTet, we port the pre-generated `32/64/128` resolution tetrahedron grids under `tets`.\nThe 256 resolution one can be found [here](https://drive.google.com/file/d/1lgvEKNdsbW5RS4gVxJbgBS4Ac92moGSa/view?usp=sharing).\n\n### Build extension (optional)\nBy default, we use [`load`](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load) to build the extension at runtime.\nWe also provide the `setup.py` to build each extension:\n```bash\ncd stable-dreamfusion\n\n# install all extension modules\nbash scripts/install_ext.sh\n\n# if you want to install manually, here is an example:\npip install ./raymarching # install to python path (you still need the raymarching/ folder, since this only installs the built extension.)\n```\n\n### Taichi backend (optional)\nUse [Taichi](https://github.com/taichi-dev/taichi) backend for Instant-NGP. It achieves comparable performance to CUDA implementation while **No CUDA** build is required. Install Taichi with pip:\n```bash\npip install -i https://pypi.taichi.graphics/simple/ taichi-nightly\n```\n\n### Trouble Shooting:\n* we assume working with the latest version of all dependencies, if you meet any problems from a specific dependency, please try to upgrade it first (e.g., `pip install -U diffusers`). If the problem still holds, [reporting a bug issue](https://github.com/ashawkey/stable-dreamfusion/issues/new?assignees=&labels=bug&template=bug_report.yaml&title=%3Ctitle%3E) will be appreciated!\n* `[F glutil.cpp:338] eglInitialize() failed Aborted (core dumped)`: this usually indicates problems in OpenGL installation. Try to re-install Nvidia driver, or use nvidia-docker as suggested in https://github.com/ashawkey/stable-dreamfusion/issues/131 if you are using a headless server.\n* `TypeError: xxx_forward(): incompatible function arguments` this happens when we update the CUDA source and you used `setup.py` to install the extensions earlier. Try to re-install the corresponding extension (e.g., `pip install ./gridencoder`).\n\n### Tested environments\n* Ubuntu 22 with torch 1.12 & CUDA 11.6 on a V100.\n\n# Usage\n\nFirst time running will take some time to compile the CUDA extensions.\n\n```bash\n#### stable-dreamfusion setting\n\n### Instant-NGP NeRF Backbone\n# + faster rendering speed\n# + less GPU memory (~16G)\n# - need to build CUDA extensions (a CUDA-free Taichi backend is available)\n\n## train with text prompt (with the default settings)\n# `-O` equals `--cuda_ray --fp16`\n# `--cuda_ray` enables instant-ngp-like occupancy grid based acceleration.\npython main.py --text \"a hamburger\" --workspace trial -O\n\n# reduce stable-diffusion memory usage with `--vram_O`\n# enable various vram savings (https://huggingface.co/docs/diffusers/optimization/fp16).\npython main.py --text \"a hamburger\" --workspace trial -O --vram_O\n\n# You can collect arguments in a file. You can override arguments by specifying them after `--file`. Note that quoted strings can't be loaded from .args files...\npython main.py --file scripts/res64.args --workspace trial_awesome_hamburger --text \"a photo of an awesome hamburger\"\n\n# use CUDA-free Taichi backend with `--backbone grid_taichi`\npython3 main.py --text \"a hamburger\" --workspace trial -O --backbone grid_taichi\n\n# choose stable-diffusion version (support 1.5, 2.0 and 2.1, default is 2.1 now)\npython main.py --text \"a hamburger\" --workspace trial -O --sd_version 1.5\n\n# use a custom stable-diffusion checkpoint from hugging face:\npython main.py --text \"a hamburger\" --workspace trial -O --hf_key andite/anything-v4.0\n\n# use DeepFloyd-IF for guidance (experimental):\npython main.py --text \"a hamburger\" --workspace trial -O --IF\npython main.py --text \"a hamburger\" --workspace trial -O --IF --vram_O # requires ~24G GPU memory\n\n# we also support negative text prompt now:\npython main.py --text \"a rose\" --negative \"red\" --workspace trial -O\n\n## after the training is finished:\n# test (exporting 360 degree video)\npython main.py --workspace trial -O --test\n# also save a mesh (with obj, mtl, and png texture)\npython main.py --workspace trial -O --test --save_mesh\n# test with a GUI (free view control!)\npython main.py --workspace trial -O --test --gui\n\n### Vanilla NeRF backbone\n# + pure pytorch, no need to build extensions!\n# - slow rendering speed\n# - more GPU memory\n\n## train\n# `-O2` equals `--backbone vanilla`\npython main.py --text \"a hotdog\" --workspace trial2 -O2\n\n# if CUDA OOM, try to reduce NeRF sampling steps (--num_steps and --upsample_steps)\npython main.py --text \"a hotdog\" --workspace trial2 -O2 --num_steps 64 --upsample_steps 0\n\n## test\npython main.py --workspace trial2 -O2 --test\npython main.py --workspace trial2 -O2 --test --save_mesh\npython main.py --workspace trial2 -O2 --test --gui # not recommended, FPS will be low.\n\n### DMTet finetuning\n\n## use --dmtet and --init_with <nerf checkpoint> to finetune the mesh at higher reslution\npython main.py -O --text \"a hamburger\" --workspace trial_dmtet --dmtet --iters 5000 --init_with trial/checkpoints/df.pth\n\n## init dmtet with a mesh to generate texture\n# require install of cubvh: pip install git+https://github.com/ashawkey/cubvh\n# remove --lock_geo to also finetune geometry, but performance may be bad.\npython main.py -O --text \"a white bunny with red eyes\" --workspace trial_dmtet_mesh --dmtet --iters 5000 --init_with ./data/bunny.obj --lock_geo\n\n## test & export the mesh\npython main.py -O --text \"a hamburger\" --workspace trial_dmtet --dmtet --iters 5000 --test --save_mesh\n\n## gui to visualize dmtet\npython main.py -O --text \"a hamburger\" --workspace trial_dmtet --dmtet --iters 5000 --test --gui\n\n### Image-conditioned 3D Generation\n\n## preprocess input image\n# note: the results of image-to-3D is dependent on zero-1-to-3's capability. For best performance, the input image should contain a single front-facing object, it should have square aspect ratio, with <1024 pixel resolution. Check the examples under ./data.\n# this will exports `<image>_rgba.png`, `<image>_depth.png`, and `<image>_normal.png` to the directory containing the input image.\npython preprocess_image.py <image>.png\npython preprocess_image.py <image>.png --border_ratio 0.4 # increase border_ratio if the center object appears too large and results are unsatisfying.\n\n## zero123 train\n# pass in the processed <image>_rgba.png by --image and do NOT pass in --text to enable zero-1-to-3 backend.\npython main.py -O --image <image>_rgba.png --workspace trial_image --iters 5000\n\n# if the image is not exactly front-view (elevation = 0), adjust default_polar (we use polar from 0 to 180 to represent elevation from 90 to -90)\npython main.py -O --image <image>_rgba.png --workspace trial_image --iters 5000 --default_polar 80\n\n# by default we leverage monocular depth estimation to aid image-to-3d, but if you find the depth estimation inaccurate and harms results, turn it off by:\npython main.py -O --image <image>_rgba.png --workspace trial_image --iters 5000 --lambda_depth 0\n\npython main.py -O --image <image>_rgba.png --workspace trial_image_dmtet --dmtet --init_with trial_image/checkpoints/df.pth\n\n## zero123 with multiple images\npython main.py -O --image_config config/<config>.csv --workspace trial_image --iters 5000\n\n## render <num> images per batch (default 1)\npython main.py -O --image_config config/<config>.csv --workspace trial_image --iters 5000 --batch_size 4\n\n# providing both --text and --image enables stable-diffusion backend (similar to make-it-3d)\npython main.py -O --image hamburger_rgba.png --text \"a DSLR photo of a delicious hamburger\" --workspace trial_image_text --iters 5000\n\npython main.py -O --image hamburger_rgba.png --text \"a DSLR photo of a delicious hamburger\" --workspace trial_image_text_dmtet --dmtet --init_with trial_image_text/checkpoints/df.pth\n\n## test / visualize\npython main.py -O --image <image>_rgba.png --workspace trial_image_dmtet --dmtet --test --save_mesh\npython main.py -O --image <image>_rgba.png --workspace trial_image_dmtet --dmtet --test --gui\n\n### Debugging\n\n# Can save guidance images for debugging purposes. These get saved in trial_hamburger/guidance.\n# Warning: this slows down training considerably and consumes lots of disk space!\npython main.py --text \"a hamburger\" --workspace trial_hamburger -O --vram_O --save_guidance --save_guidance_interval 5 # save every 5 steps\n```\n\nFor example commands, check [`scripts`](./scripts).\n\nFor advanced tips and other developing stuff, check [Advanced Tips](./assets/advanced.md).\n\n# Evalutation\n\nReproduce the paper CLIP R-precision evaluation\n\nAfter the testing part in the usage, the validation set containing projection from different angle is generated. Test the R-precision between prompt and the image.(R=1)\n\n```bash\npython r_precision.py --text \"a snake is flying in the sky\" --workspace snake_HQ --latest ep0100 --mode depth --clip clip-ViT-B-16\n```\n\n# Acknowledgement\n\nThis work is based on an increasing list of amazing research works and open-source projects, thanks a lot to all the authors for sharing!\n\n* [DreamFusion: Text-to-3D using 2D Diffusion](https://dreamfusion3d.github.io/)\n    ```\n    @article{poole2022dreamfusion,\n        author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},\n        title = {DreamFusion: Text-to-3D using 2D Diffusion},\n        journal = {arXiv},\n        year = {2022},\n    }\n    ```\n\n* [Magic3D: High-Resolution Text-to-3D Content Creation](https://research.nvidia.com/labs/dir/magic3d/)\n   ```\n   @inproceedings{lin2023magic3d,\n      title={Magic3D: High-Resolution Text-to-3D Content Creation},\n      author={Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and Takikawa, Towaki and Zeng, Xiaohui and Huang, Xun and Kreis, Karsten and Fidler, Sanja and Liu, Ming-Yu and Lin, Tsung-Yi},\n      booktitle={IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})},\n      year={2023}\n    }\n   ```\n\n* [Zero-1-to-3: Zero-shot One Image to 3D Object](https://github.com/cvlab-columbia/zero123)\n    ```\n    @misc{liu2023zero1to3,\n        title={Zero-1-to-3: Zero-shot One Image to 3D Object},\n        author={Ruoshi Liu and Rundi Wu and Basile Van Hoorick and Pavel Tokmakov and Sergey Zakharov and Carl Vondrick},\n        year={2023},\n        eprint={2303.11328},\n        archivePrefix={arXiv},\n        primaryClass={cs.CV}\n    }\n    ```\n    \n* [Perp-Neg: Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond](https://perp-neg.github.io/)\n    ```\n    @article{armandpour2023re,\n      title={Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond},\n      author={Armandpour, Mohammadreza and Zheng, Huangjie and Sadeghian, Ali and Sadeghian, Amir and Zhou, Mingyuan},\n      journal={arXiv preprint arXiv:2304.04968},\n      year={2023}\n    }\n    ```\n    \n* [RealFusion: 360 Reconstruction of Any Object from a Single Image](https://github.com/lukemelas/realfusion)\n    ```\n    @inproceedings{melaskyriazi2023realfusion,\n        author = {Melas-Kyriazi, Luke and Rupprecht, Christian and Laina, Iro and Vedaldi, Andrea},\n        title = {RealFusion: 360 Reconstruction of Any Object from a Single Image},\n        booktitle={CVPR}\n        year = {2023},\n        url = {https://arxiv.org/abs/2302.10663},\n    }\n    ```\n\n* [Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation](https://fantasia3d.github.io/)\n    ```\n    @article{chen2023fantasia3d,\n        title={Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation},\n        author={Rui Chen and Yongwei Chen and Ningxin Jiao and Kui Jia},\n        journal={arXiv preprint arXiv:2303.13873},\n        year={2023}\n    }\n    ```\n\n* [Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior](https://make-it-3d.github.io/)\n    ```\n    @article{tang2023make,\n        title={Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior},\n        author={Tang, Junshu and Wang, Tengfei and Zhang, Bo and Zhang, Ting and Yi, Ran and Ma, Lizhuang and Chen, Dong},\n        journal={arXiv preprint arXiv:2303.14184},\n        year={2023}\n    }\n    ```\n\n* [Stable Diffusion](https://github.com/CompVis/stable-diffusion) and the [diffusers](https://github.com/huggingface/diffusers) library.\n\n    ```\n    @misc{rombach2021highresolution,\n        title={High-Resolution Image Synthesis with Latent Diffusion Models},\n        author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bjrn Ommer},\n        year={2021},\n        eprint={2112.10752},\n        archivePrefix={arXiv},\n        primaryClass={cs.CV}\n    }\n\n    @misc{von-platen-etal-2022-diffusers,\n        author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},\n        title = {Diffusers: State-of-the-art diffusion models},\n        year = {2022},\n        publisher = {GitHub},\n        journal = {GitHub repository},\n        howpublished = {\\url{https://github.com/huggingface/diffusers}}\n    }\n    ```\n\n* The GUI is developed with [DearPyGui](https://github.com/hoffstadt/DearPyGui).\n\n* Puppy image from : https://www.pexels.com/photo/high-angle-photo-of-a-corgi-looking-upwards-2664417/\n\n* Anya images from : https://www.goodsmile.info/en/product/13301/POP+UP+PARADE+Anya+Forger.html\n\n# Citation\n\nIf you find this work useful, a citation will be appreciated via:\n```\n@misc{stable-dreamfusion,\n    Author = {Jiaxiang Tang},\n    Year = {2022},\n    Note = {https://github.com/ashawkey/stable-dreamfusion},\n    Title = {Stable-dreamfusion: Text-to-3D with Stable-diffusion}\n}\n```\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.6689453125,
          "content": "tqdm\nrich\nninja\nnumpy\npandas\nscipy\nscikit-learn\nmatplotlib\nopencv-python\nimageio\nimageio-ffmpeg\n\ntorch\ntorch-ema\neinops\ntensorboard\ntensorboardX\n\n# for gui\ndearpygui\n\n# for grid_tcnn\n# git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch\n\n# for stable-diffusion\nhuggingface_hub\ndiffusers >= 0.9.0\naccelerate\ntransformers\n\n# for dmtet and mesh export\nxatlas\ntrimesh\nPyMCubes\npymeshlab\ngit+https://github.com/NVlabs/nvdiffrast/\n\n# for zero123\ncarvekit-colab\nomegaconf\npytorch-lightning\ntaming-transformers-rom1504\nkornia\ngit+https://github.com/openai/CLIP.git\n\n# for omnidata\ngdown\n\n# for dpt\ntimm\n\n# for remote debugging\ndebugpy-run\n\n# for deepfloyd if\nsentencepiece\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "shencoder",
          "type": "tree",
          "content": null
        },
        {
          "name": "taichi_modules",
          "type": "tree",
          "content": null
        },
        {
          "name": "tets",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}