{
  "metadata": {
    "timestamp": 1736560457234,
    "page": 39,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "OpenMined/PySyft",
      "stars": 9578,
      "defaultBranch": "dev",
      "files": [
        {
          "name": ".bumpversion.cfg",
          "type": "blob",
          "size": 0.9716796875,
          "content": "[bumpversion]\ncurrent_version = 0.9.3-beta.2\ntag = False\ntag_name = {new_version}\ncommit = True\nparse = \n\t(?P<major>\\d+)\n\t\\.\n\t(?P<minor>\\d+)\n\t\\.\n\t(?P<patch>\\d+)\n\t(\\-(?P<pre>[a-z]+)\\.(?P<prenum>\\d+))?\nserialize = \n\t{major}.{minor}.{patch}-{pre}.{prenum}\n\t{major}.{minor}.{patch}\n\n[bumpversion:part:pre]\noptional_value = placeholder\nfirst_value = alpha\nvalues = \n\talpha\n\tbeta\n\trc\n\tplaceholder\n\n[bumpversion:part:prenum]\nfirst_value = 1\n\n[bumpversion:file:VERSION]\n\n[bumpversion:file:packages/syft/setup.cfg]\n\n[bumpversion:file:packages/syft/src/syft/__init__.py]\n\n[bumpversion:file:packages/syft/src/syft/VERSION]\n\n[bumpversion:file:packages/grid/devspace.yaml]\n\n[bumpversion:file:packages/grid/VERSION]\n\n[bumpversion:file:packages/grid/backend/grid/images/worker_cpu.dockerfile]\n\n[bumpversion:file:packages/grid/frontend/package.json]\n\n[bumpversion:file:packages/grid/helm/syft/Chart.yaml]\n\n[bumpversion:file:packages/grid/helm/syft/values.yaml]\n\n[bumpversion:file:packages/syftcli/manifest.yml]\n"
        },
        {
          "name": ".bumpversion_stable.cfg",
          "type": "blob",
          "size": 0.2451171875,
          "content": "[bumpversion]\ncurrent_version = 0.9.2\ntag = False\ntag_name = {new_version}\ncommit = True\nparse =\n\t(?P<major>\\d+)\n\t\\.\n\t(?P<minor>\\d+)\n\t\\.\n\t(?P<patch>\\d+)\nserialize =\n\t{major}.{minor}.{patch}\n\n[bumpversion:file:packages/syft/src/syft/stable_version.py]\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.2158203125,
          "content": "*.ipynb filter=nbstripout\n*.ipynb diff=ipynb\n*.py eol=lf\n*.md eol=lf\n*.js eol=lf\n*.json eol=lf\n*.ini eol=lf\n*.html eol=lf\n*.dockerfile eol=lf\n*.sh eol=lf\n\n*.jpg binary\n*.jpeg binary\n*.png binary\n*.pdf binary\n*.gif binary\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.3642578125,
          "content": "# dot files / folders\n.cache/\n.coverage\n.DS_Store\n.eggs/\n.idea/\n.mypy_cache\n.python-version\n.vscode/*\n!.vscode/launch.json\n.tox/*\n.creds\nbuild\n\n# python stuff\n*__pycache__*\n*.pyc\n*.swp\n*.swo\n*.ipynb_checkpoints*\n**.pytest_cache/\n**/pip-wheel-metadata\n\n# logs and database files\n**/*.log\n**/*.sqlite\n**/*.sqlite-journal\n\n# docker compose volumes\ndocker/data/*\n\n# env files\n.env\n\n# vagrant\n.vagrant\n\n# venv\nvenv\n.venv\n\n# docs\ndocs/build\ndocs/source/_build\n\n# helmcharts\nhelm-charts/**/*.tgz\nhelm-charts/**/*.lock\n.helm/**\n\n# docker cache\n.docker-cache/\n.docker\n\n# notebook data\nnotebooks/medical-federated-learning-program/data-owners/MedNIST.pkl\nnotebooks/**/*.pkl\n\n# k3d registry\nk3d-registry\n\n.envfile\n\n\n# rendered template dir\npackages/grid/rendered\n\njs/node_modules/*\n\n#nohup\nnohup.out\n\n# jupyter lsp\n.virtual_documents\n\n# notebook data\nnotebooks/helm/scenario_data.jsonl\n\n# tox syft.build.helm generated file\nout.*\n.git-blame-ignore-revs\n\n# migration data\npackages/grid/helm/examples/dev/migration.yaml\n\n# dynaconf settings file\n**/settings.yaml\n\n# Any temporary material created for scenarios\nnotebooks/scenarios/bigquery/*.json\nnotebooks/scenarios/bigquery/sync/*.json\nnotebooks/scenarios/bigquery/sync/*.json.lock\nnotebooks/tutorials/version-upgrades/*.yaml\nnotebooks/tutorials/version-upgrades/*.blob\nnotebooks/scenarios/bigquery/sync/emails.json\n\n# logs dir generated by sim tests\n.logs\n\n"
        },
        {
          "name": ".isort.cfg",
          "type": "blob",
          "size": 0.6767578125,
          "content": "[settings]\nprofile=black\nforce_single_line=True\nknown_syft=syft\nknown_server=grid\nknown_syftcli=syftcli\nknown_first_party=src\nremove_redundant_aliases=True\nsections=FUTURE,STDLIB,THIRDPARTY,SYFT,SERVER,SYFTCLI,FIRSTPARTY,LOCALFOLDER\nlines_between_types=0\nforce_sort_within_sections=True\nimport_heading_future=future\nimport_heading_stdlib=stdlib\nimport_heading_thirdparty=third party\nimport_heading_syft=syft absolute\nimport_heading_server=server absolute\nimport_heading_syftcli=syftcli absolute\nimport_heading_firstparty=first party\nimport_heading_localfolder=relative\nignore_comments=False\nforce_grid_wrap=True\nhonor_noqa=True\nskip_glob=packages/syft/src/syft/__init__.py,packages/grid/data/*"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 5.935546875,
          "content": "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.5.0\n    hooks:\n      - id: check-ast\n        always_run: true\n      - id: trailing-whitespace\n        always_run: true\n        exclude: ^(docs/|.+\\.md|.bumpversion.cfg)\n      - id: check-docstring-first\n        always_run: true\n      - id: check-json\n        always_run: true\n        exclude: ^(packages/grid/frontend/|.vscode)\n      - id: check-added-large-files\n        always_run: true\n        exclude: ^(packages/grid/backend/wheels/.*|docs/img/header.png|docs/img/terminalizer.gif|^notebooks/scenarios/bigquery/upgradability/sync/migration_.*\\.blob)\n      - id: check-yaml\n        always_run: true\n        exclude: ^(packages/grid/k8s/rendered/|packages/grid/helm/)\n      - id: check-merge-conflict\n        always_run: true\n        args: [\"--assume-in-merge\"]\n      - id: check-executables-have-shebangs\n        always_run: true\n      - id: debug-statements\n        always_run: true\n      - id: name-tests-test\n        always_run: true\n        exclude: ^(.*/tests/utils/)|^(.*fixtures.py)|^(tests/scenariosv2/(sim|flows))\n      - id: requirements-txt-fixer\n        always_run: true\n      - id: mixed-line-ending\n        args: [\"--fix=lf\"]\n        exclude: '\\.bat|\\.csv|\\.ps1$'\n\n  - repo: https://github.com/MarcoGorelli/absolufy-imports # This repository has been archived by the owner on Aug 15, 2023. It is now read-only.\n    rev: v0.3.1\n    hooks:\n      - id: absolufy-imports\n        args: [\"--never\", \"--application-directories=packages/syft/src\"]\n        always_run: true\n        files: ^packages/syft/src\n        exclude: |\n          (?x)^(\n              packages/syft/examples/.*|\n              packages/syft/src/syft/proto.*|\n              packages/syft/tests/syft/lib/python.*|\n              packages/grid.*|\n              packages/syft/src/syft/federated/model_serialization/protos.py|\n              packages/syft/src/syft/util/test_helpers/.*|\n          )$\n\n  - repo: https://github.com/MarcoGorelli/absolufy-imports\n    rev: v0.3.1\n    hooks:\n      - id: absolufy-imports\n        name: \"absolufy-imports: syft-cli\"\n        always_run: true\n        files: ^packages/syftcli\n        # This will ignore all syftcli/**/*.py where relative imports are okay\n        # but absolufy only top-level syftcli/<file>.py. where relative imports are not okay for pyinstaller\n        exclude: ^packages/syftcli/syftcli/(.*)/\n        args: [\"--application-directories=packages/syftcli/\"]\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.13.2\n    hooks:\n      - id: isort\n        name: isort\n        args: [\".\", \"--settings-path .isort.cfg\"]\n        always_run: true\n\n  - repo: https://github.com/nbQA-dev/nbQA\n    rev: 1.8.5\n    # nbQA has no files attribute\n    # files: \"^notebooks/(api|tutorials|admin)\"\n    hooks:\n      - id: nbqa-isort\n\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    # Ruff version.\n    rev: \"v0.4.7\"\n    hooks:\n      - id: ruff\n        args: [--fix, --exit-non-zero-on-fix, --show-fixes]\n        types_or: [python, pyi, jupyter]\n      - id: ruff-format\n        types_or: [python, pyi, jupyter]\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.10.0\n    hooks:\n      - id: mypy\n        name: \"mypy: syft-cli\"\n        always_run: true\n        files: ^packages/syftcli\n        args: [\n            \"--ignore-missing-imports\",\n            \"--scripts-are-modules\",\n            \"--disallow-incomplete-defs\",\n            \"--no-implicit-optional\",\n            \"--warn-unused-ignores\",\n            \"--warn-redundant-casts\",\n            \"--strict-equality\",\n            \"--warn-unreachable\",\n            # \"--disallow-untyped-decorators\",\n            \"--disallow-untyped-defs\",\n            \"--disallow-untyped-calls\",\n            \"--namespace-packages\",\n            \"--install-types\",\n            \"--non-interactive\",\n            \"--config-file=tox.ini\",\n          ]\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.10.0\n    hooks:\n      - id: mypy\n        name: \"mypy: grid\"\n        files: ^packages/grid\n        always_run: true\n        args: [\n            \"--ignore-missing-imports\",\n            \"--scripts-are-modules\",\n            \"--disallow-incomplete-defs\",\n            \"--no-implicit-optional\",\n            \"--warn-unused-ignores\",\n            \"--warn-redundant-casts\",\n            \"--strict-equality\",\n            \"--warn-unreachable\",\n            # \"--disallow-untyped-decorators\",\n            \"--disallow-untyped-defs\",\n            \"--disallow-untyped-calls\",\n            \"--namespace-packages\",\n            \"--install-types\",\n            \"--non-interactive\",\n            \"--config-file=tox.ini\",\n          ]\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.10.0\n    hooks:\n      - id: mypy\n        name: \"mypy: syft\"\n        always_run: true\n        files: \"^packages/syft/src/syft/\"\n        args: [\n            \"--follow-imports=skip\",\n            \"--ignore-missing-imports\",\n            \"--scripts-are-modules\",\n            \"--disallow-incomplete-defs\",\n            \"--no-implicit-optional\",\n            # \"--warn-unused-ignores\",\n            \"--warn-redundant-casts\",\n            \"--strict-equality\",\n            \"--warn-unreachable\",\n            # \"--disallow-untyped-decorators\",\n            \"--disallow-untyped-defs\",\n            \"--disallow-untyped-calls\",\n            \"--install-types\",\n            \"--non-interactive\",\n            \"--config-file=tox.ini\",\n          ]\n        exclude: ^(packages/syft/src/syft/util/test_helpers)\n\n  - repo: https://github.com/kynan/nbstripout\n    rev: 0.7.1\n    hooks:\n      - id: nbstripout\n        files: \"^notebooks/(api|tutorials|admin|scenarios)\"\n\n  - repo: https://github.com/pre-commit/mirrors-prettier # This repository has been archived by the owner on Apr 11, 2024. It is now read-only.\n    rev: \"v3.0.0-alpha.9-for-vscode\"\n    hooks:\n      - id: prettier\n        exclude: ^(packages/grid/helm|packages/grid/frontend/pnpm-lock.yaml|.vscode)\n\n  # - repo: meta\n  #   hooks:\n  #     - id: identity\n  #       always_run: true\n  #       files: \"notebooks/api/*\"\n"
        },
        {
          "name": ".prettierignore",
          "type": "blob",
          "size": 0.03515625,
          "content": "**/*.jinja2\n**/*.min.js\n**/*.min.css"
        },
        {
          "name": ".prettierrc",
          "type": "blob",
          "size": 0.0263671875,
          "content": "{\n  \"singleQuote\": false\n}\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "DEBUGGING.md",
          "type": "blob",
          "size": 2.00390625,
          "content": "# Debugging PySyft\n\nWe currently provide information on how to debug PySyft using Visual Studio Code and PyCharm. If you have any other IDE or debugger that you would like to add to this list, please feel free to contribute.\n\n## VSCode\n\nIf you're running Add the following in `.vscode/launch.json`\n\n```\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"name\": \"Python Debugger: Remote Attach\",\n            \"type\": \"debugpy\",\n            \"request\": \"attach\",\n            \"connect\": {\n                \"host\": \"localhost\",\n                \"port\": 5678\n            },\n            \"justMyCode\": false,\n            \"internalConsoleOptions\": \"openOnSessionStart\",\n            \"pathMappings\": [\n                {\n                    \"localRoot\": \"${workspaceFolder}/packages/syft/src\",\n                    \"remoteRoot\": \"/root/app/syft/src\"\n                },\n                {\n                    \"localRoot\": \"${workspaceFolder}/packages/grid/backend/grid\",\n                    \"remoteRoot\": \"/root/app/grid\"\n                }\n            ]\n        }\n    ]\n}\n```\n\nThen run\n\n```bash\ntox -e dev.k8s.hotreload\n```\n\nAnd you can attach the debugger running on port 5678.\n\n## PyCharm\n\nAdd the following to `packages/grid/backend/grid/__init__.py`\n\n```py\nimport pydevd_pycharm\npydevd_pycharm.settrace('your-local-addr', port=5678, suspend=False)\n```\n\nEnsure that `your-local-addr` is reachable from the containers.\n\nNext, replace the debugpy install and `DEBUG_CMD` in `packages/grid/backend/grid/start.sh`:\n\n```bash\n# only set by kubernetes to avoid conflict with docker tests\nif [[ ${DEBUGGER_ENABLED} == \"True\" ]];\nthen\n    pip install --user pydevd-pycharm==233.14475.56 # remove debugpy, add pydevd-pycharm\n    DEBUG_CMD=\"\" # empty the debug command\nfi\n```\n\nIf it fails to connect, check the backend logs. You might need to install a different pydevd-pycharm version. The version to be installed is shown in the log error message.\n\nWhenever you start a container, it attempts to connect to PyCharm.\n\n```bash\ntox -e dev.k8s.hotreload\n```\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.6611328125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n\nThe following files are copied from CPython and are under the PSF license.\nSee here for more: https://github.com/python/cpython/blob/master/LICENSE\n\npackages/syft/tests/syft/lib/python/collections/ordered_dict/ordered_dict_sanity_test.py\npackages/syft/tests/syft/lib/python/complex/complex_test.py\npackages/syft/tests/syft/lib/python/dict/dict_test.py\npackages/syft/tests/syft/lib/python/float/float_test.py\npackages/syft/tests/syft/lib/python/list/list_test.py\npackages/syft/tests/syft/lib/python/string/string_utils_test.py\npackages/syft/tests/syft/lib/python/range/range_test.py\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.5361328125,
          "content": "<div align=\"left\"> <a href=\"https://pypi.org/project/syft/\"><img src=\"https://static.pepy.tech/badge/pysyft\" /></a> <a href=\"https://pypi.org/project/syft/\"><img src=\"https://badge.fury.io/py/syft.svg\" /></a> <a href=\"https://hub.docker.com/u/openmined\"><img src=\"https://img.shields.io/badge/docker-images-blue?logo=docker\" /></a> <a href=\"https://github.com/OpenMined/PySyft/actions/workflows/nightlies.yml\"><img src=\"https://github.com/OpenMined/PySyft/actions/workflows/nightlies.yml/badge.svg?branch=dev\" /></a> <a href=\"https://join.slack.com/t/openmined/shared_invite/zt-2hxwk07i9-HO7u5C7XOgou4Z62VU78zA/\"><img src=\"https://img.shields.io/badge/chat-on%20slack-purple?logo=slack\" /></a> <a href=\"https://docs.openmined.org/en/latest/index.html\"><img src=\"https://img.shields.io/badge/read-docs-yellow?logo=mdbook\" /></a>\n<br /><br /></div>\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/img/Syft-Logo-Light.svg\">\n  <img alt=\"Syft Logo\" src=\"docs/img/Syft-Logo.svg\" width=\"200px\" />\n</picture>\n\n<h3> Data Science on data you are not allowed to see</h3>\n\nPySyft enables a new way to do data science, where you can use non-public information, without seeing nor obtaining a copy of the data itself. All you need is to connect to a <a href=\"https://docs.openmined.org/en/latest/components/datasite-server.html\">Datasite</a>!\n\nDatasites are like websites, but for data. Designed with the principles of <a href=\"https://arxiv.org/abs/2012.08347\">structured transparency</a>, they enable data owners to control how their data is protected and data scientists to use data without obtaining a copy.\n\nPySyft supports any statistical analysis or machine learning, offering support for directly running Python code - even using third-party Python libraries.\n\n<h4> Supported on:</h4>\n\n‚úÖ Linux\n‚úÖ macOS\n‚úÖ Windows\n‚úÖ Docker\n‚úÖ Kubernetes\n\n# Quickstart\n\nTry out your <a href=\"https://docs.openmined.org/en/latest/index.html\">first query against a live demo Datasite! </a>\n\n## Install Client\n\n```bash\npip install -U \"syft[data_science]\"\n```\n\nMore instructions are available <a href=\"https://docs.openmined.org/en/latest/quick-install.html\">here</a>.\n\n## Launch Server\n\nLaunch <a href=\"https://docs.openmined.org/en/latest/deployment/deployment-doc-1-2-intro-req.html\">a development server </a> directly in your Jupyter Notebook:\n\n```python\nimport syft as sy\n\nsy.requires(\">=0.9.2,<0.9.3\")\n\nserver = sy.orchestra.launch(\n    name=\"my-datasite\",\n    port=8080,\n    create_producer=True,\n    n_consumers=1,\n    dev_mode=False,\n    reset=True, # resets database\n)\n```\n\nor from the command line:\n\n```bash\n$ syft launch --name=my-datasite --port=8080 --reset=True\n\nStarting syft-datasite server on 0.0.0.0:8080\n```\n\nDatasite servers can be deployed as a single container using Docker or directly in Kubernetes. Check out our <a href=\"https://docs.openmined.org/en/latest/deployment/deployment-doc-1-2-intro-req.html\">deployment guide.</a>\n\n## Launch Client\n\nMain way to use a Datasite is via our Syft client, in a Jupyter Notebook. Check out our <a href=\"https://docs.openmined.org/en/latest/components/syft-client.html\"> PySyft client guide</a>:\n\n```python\nimport syft as sy\n\nsy.requires(\">=0.9.2,<0.9.3\")\n\ndatasite_client = sy.login(\n    port=8080,\n    email=\"info@openmined.org\",\n    password=\"changethis\"\n)\n```\n\n## PySyft - Getting started üìù\n\nLearn about PySyft via our getting started guide:\n\n- <a href=\"https://docs.openmined.org/en/latest/getting-started/introduction.html\">PySyft from the ground up</a>\n- <a href=\"https://docs.openmined.org/en/latest/getting-started/part1-dataset-and-assets.html\"> Part 1: Datasets & Assets</a>\n- <a href=\"https://docs.openmined.org/en/latest/getting-started/part2-datasite-access.html\"> Part 2: Client and Datasite Access</a>\n- <a href=\"https://docs.openmined.org/en/latest/getting-started/part3-research-study.html\"> Part 3: Propose the research study</a>\n- <a href=\"https://docs.openmined.org/en/latest/getting-started/part4-review-code-request.html\"> Part 4: Review Code Requests</a>\n- <a href=\"https://docs.openmined.org/en/latest/getting-started/part5-retrieving-results.html\"> Part 5: Retrieving Results</a>\n\n# PySyft In-depth\n\nüìö Check out <a href=\"https://docs.openmined.org/en/latest/index.html\">our docs website</a>.\n\nQuick PySyft components links:\n\n- <a href=\"https://docs.openmined.org/en/latest/components/datasite-server.html\">DataSite Server</a>\n\n- <a href=\"https://docs.openmined.org/en/latest//components/syft-client.html\">Syft Client</a>\n\n- <a href=\"https://docs.openmined.org/en/latest/components/datasets.html\">Datasets API (`.datasets`)</a>\n\n- <a href=\"https://docs.openmined.org/en/latest/components/users-api.html\">Users API (`.users`)</a>\n\n<!-- - <a href=\"https://docs.openmined.org/en/latest/components/projects-api.html\">Projects API (`.projects`)</a> -->\n\n- <a href=\"https://docs.openmined.org/en/latest/components/requests-api.html\">Request API (`.requests`)</a>\n\n- <a href=\"https://docs.openmined.org/en/latest/components/code-api.html\">Code API (`.code`)</a>\n\n- <a href=\"https://docs.openmined.org/en/latest/components/syft-policies.html\">Syft Policies API (`.policy`)</a>\n\n- <a href=\"https://docs.openmined.org/en/latest/components/settings-api.html\">Settings API (`.settings`)</a>\n\n- <a href=\"https://docs.openmined.org/en/latest/components/notifications.html\">Notifications API (`.notifications`)</a>\n\n- <a href=\"https://docs.openmined.org/en/latest/components/syncing-api.html\">Sync API (`.sync`)</a>\n\n## Why use PySyft?\n\nIn a variety of domains across society, data owners have **valid concerns about the risks associated with sharing their data**, such as legal risks, privacy invasion (_misuing the data_), or intellectual property (_copying and redistributing it_).\n\nDatasites enable data scientists to **answer questions** without even seeing or acquiring a copy of the data, **within the data owners's definition of acceptable use**. We call this process <b> Remote Data Science</b>.\n\nThis means that the **current risks** of sharing information with someone will **no longer prevent** the vast benefits such as innovation, insights and scientific discovery. With each Datasite, data owners are able to enable `1000x more accesible data` in each scientific field and lead, together with data scientists, breakthrough innovation.\n\nLearn more about our work on <a href=\"https://openmined.org/\">our website</a>.\n\n## Support\n\nFor questions about PySyft, reach out via `#support` on <a href=\"https://slack.openmined.org/\">Slack</a>.\n\n## Syft Versions\n\n:exclamation: PySyft and Syft Server must use the same `version`.\n\n**Latest Stable**\n\n- `0.9.2` (Stable) - <a href=\"https://docs.openmined.org/en/latest/index.html\">Docs</a>\n- Install PySyft (Stable): `pip install -U syft`\n\n**Latest Beta**\n\n- `0.9.3` (Beta) - `dev` branch üëàüèΩ\n- Install PySyft (Beta): `pip install -U syft --pre`\n\nFind more about previous <a href=\"./releases.md\">releases here</a>.\n\n# Community\n\nSupported by the OpenMined Foundation, the OpenMined Community is an online network of over 17,000 technologists, researchers, and industry professionals keen to _unlock 1000x more data in every scientific field and industry_.\n\n<a href=\"https://join.slack.com/t/openmined/shared_invite/zt-2hxwk07i9-HO7u5C7XOgou4Z62VU78zA\"><img width=150px src=\"https://img.shields.io/badge/Join_us-%20slack-purple?logo=slack\" /></a>\n\n# Courses\n\n<table border=\"5\" bordercolor=\"grey\">\n<tr>\n<th align=\"center\">\n<img width=\"200\" height=\"1\">\n<div align=\"center\">\n<a href=\"https://courses.openmined.org/courses/our-privacy-opportunity\"><img src=\"docs/img/course_privacy.png\" alt=\"\" width=\"100%\" align=\"center\" /></a>\n</th>\n<th align=\"center\">\n<img width=\"200\" height=\"1\">\n<div align=\"center\">\n<a href=\"https://courses.openmined.org/courses/foundations-of-private-computation\"><img src=\"docs/img/course_foundations.png\" alt=\"\" width=\"100%\" align=\"center\" /></a>\n</div>\n</th>\n<th align=\"center\">\n<img width=\"200\" height=\"1\">\n<div align=\"center\">\n<a href=\"https://courses.openmined.org/courses/introduction-to-remote-data-science\"><img src=\"docs/img/course_introduction.png\" alt=\"\" width=\"100%\" align=\"center\"></a>\n</div>\n</th>\n</tr>\n</table>\n\n# Contributors\n\nOpenMined and Syft appreciates all contributors, if you would like to fix a bug or suggest a new feature, please reach out via <a href=\"https://github.com/OpenMined/PySyft/issues\">Github</a> or <a href=\"https://join.slack.com/t/openmined/shared_invite/zt-2hxwk07i9-HO7u5C7XOgou4Z62VU78zA/\">Slack</a>!\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/img/contributors_dark.jpg\">\n  <img src=\"docs/img/contributors_light.jpg\" alt=\"Contributors\" width=\"100%\" />\n</picture>\n\n# About OpenMined\n\nOpenMined is a non-profit foundation creating technology infrastructure that helps researchers get answers from data without needing a copy or direct access. Our community of technologists is building Syft.\n\n<a href=\"https://donate.stripe.com/fZe03H0aLdAO59e9AA\n\"><img width=200px src=\"https://img.shields.io/badge/Donate_to-OpenMined-yellow?logo=stripe\" /></a>\n\n# Supporters\n\n<table border=\"0\">\n<tr>\n<th align=\"center\">\n<a href=\"https://sloan.org/\"><img src=\"docs/img/logo_sloan.png\" /></a>\n</th>\n<th align=\"center\">\n<a href=\"https://opensource.fb.com/\"><img src=\"docs/img/logo_meta.png\" /></a>\n</th>\n<th align=\"center\">\n<a href=\"https://pytorch.org/\"><img src=\"docs/img/logo_torch.png\" /></a>\n</th>\n<th align=\"center\">\n<a href=\"https://www.dpmc.govt.nz/\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/img/logo_nz_dark.png\">\n  <img src=\"docs/img/logo_nz_light.png\" />\n</picture>\n</a>\n</th>\n<th align=\"center\">\n<a href=\"https://twitter.com/\"><img src=\"docs/img/logo_twitter.png\" /></a>\n</th>\n<th align=\"center\">\n<a href=\"https://google.com/\"><img src=\"docs/img/logo_google.png\" /></a>\n</th>\n<th align=\"center\">\n<a href=\"https://microsoft.com/\"><img src=\"docs/img/logo_microsoft.png\" /></a>\n</th>\n<th align=\"center\">\n<a href=\"https://omidyar.com/\"><img src=\"docs/img/logo_on.png\" /></a>\n</th>\n<th align=\"center\">\n<a href=\"https://www.udacity.com/\"><img src=\"docs/img/logo_udacity.png\" /></a>\n</th>\n<th align=\"center\">\n<a href=\"https://www.centerfordigitalhealthinnovation.org/\">\n\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/img/logo_cdhi_dark.png\">\n  <img src=\"docs/img/logo_cdhi_light.png\" />\n</picture>\n\n</a>\n</th>\n<th align=\"center\">\n<a href=\"https://arkhn.org/\">\n<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/img/logo_arkhn.png\">\n  <img src=\"docs/img/logo_arkhn_light.png\" />\n</picture>\n</a>\n</th>\n</tr>\n</table>\n\n# License\n\n[Apache License 2.0](LICENSE)<br />\n<a href=\"https://www.flaticon.com/free-icons/person\" title=\"person icons\">Person icons created by Freepik - Flaticon</a>\n\n<!-- ü•á -->\n"
        },
        {
          "name": "VERSION",
          "type": "blob",
          "size": 0.56640625,
          "content": "# Mono Repo Global Version\n__version__ = \"0.9.3-beta.2\"\n# elsewhere we can call this file: `python VERSION` and simply take the stdout\n\n# stdlib\nimport os\nimport subprocess\nimport sys\n\n\ndef get_version() -> str:\n    return __version__\n\n\ndef get_hash() -> str:\n    cwd = os.path.dirname(os.path.abspath(__file__))\n    output = subprocess.check_output(\"git rev-parse HEAD\".split(\" \"), cwd=cwd)\n    return output.strip().decode(\"ascii\")\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1 and \"hash\" in sys.argv[1]:\n        print(get_hash())\n    else:\n        print(get_version())\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "justfile",
          "type": "blob",
          "size": 19.9541015625,
          "content": "# Rules for new commands\n# - Start with a verb\n# - Keep it short (max. 3 words)\n# - Group commands by context. Include group name in the command name.\n# - Mark things private that are util functions with [private] or _var\n# - Don't over-engineer, keep it simple.\n# - Don't break existing commands\n\nset dotenv-load\n\n# ---------------------------------------------------------------------------------------------------------------------\n# K3D cluster names\n# Note: These are private (_ prefix) because we don't want it to be editable from CLI.\n_name_default := \"syft-dev\"\n_name_high := \"syft-high\"\n_name_low := \"syft-low\"\n_name_gw := \"syft-gw\"\n_name_signoz := \"signoz\"\n\n# K3D Registry name is used only in k3d.\n_name_registry := \"registry.localhost\"\n\n# Kubernetes namespaces for the deployments\n# Note: These are private (_ prefix) because we don't want it to be editable from CLI.\n_ns_default := \"syft\"\n_ns_high := \"high\"\n_ns_low := \"low\"\n_ns_gw := \"gw\"\n\n# Kubernetes context names generated for the K3D clusters\n# Note: These are private (_ prefix) because we don't want it to be editable from CLI.\n_ctx_default := \"k3d-\" + _name_default\n_ctx_high := \"k3d-\" + _name_high\n_ctx_low := \"k3d-\" + _name_low\n_ctx_gw := \"k3d-\" + _name_gw\n_ctx_signoz := \"k3d-\" + _name_signoz\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n# Static Ports for the clusters\nport_default := \"8080\"\nport_high := port_default\nport_low := \"8081\"\nport_gw := \"8082\"\nport_signoz_ui := \"3301\"\nport_signoz_otel := \"4317\"\nport_registry := \"5800\"\n\n# Registry URL is used for\n#   - setting up the registry for k3d clusters\n#   - setting up the --var CONTAINER_REGISTRY for devspace deployments\n# Note: Do not add http:// or https:// prefix\nregistry_url := \"k3d-\" + _name_registry + \":\" + port_registry\n\n# Signoz OTel endpoint is used for setting up the Otel collector\nsignoz_otel_url := \"http://host.k3d.internal:\" + port_signoz_otel\n\n# ---------------------------------------------------------------------------------------------------------------------\n# devspace profiles (comma-separated)\nprofiles := \"\"\n\n# enable tracing by adding \"tracing\" profile in devspace\ntracing := \"true\"\n\n# add tracing profile if enabled\n# This is private ( _prefix) to have a simple `just tracing=true ...`\n_g_profiles := if tracing == \"true\" { profiles + \",tracing\" } else { profiles }\n\n# ---------------------------------------------------------------------------------------------------------------------\n# this might break if you have alias python = python3 or either of the executable not pointing to the correct one\n# just fix your system instead of making of fixing this\npython_path := `which python || which python3`\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n@default:\n    just --list\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n# Start a local registry on http://k3d-registry.local:5800 (port_registry=5800 or registry_url=\"gcr.io/path/to/registry\")\n[group('registry')]\nstart-registry:\n    k3d --version\n    @-docker volume create k3d-registry-vol\n    @-k3d registry create {{ _name_registry }} --port {{ port_registry }} -v k3d-registry-vol:/var/lib/registry --no-help\n\n    if ! grep -q {{ _name_registry }} /etc/hosts; then \\\n        sudo {{ python_path }} scripts/patch_hosts.py --add-k3d-registry --fix-docker-hosts; \\\n    fi\n\n    @curl --silent --retry 5 --retry-all-errors http://{{ registry_url }}/v2/_catalog | jq\n    @echo \"\\033[1;32mRegistring running at http://{{ registry_url }}\\033[0m\"\n\n[group('registry')]\ndelete-registry:\n    -k3d registry delete {{ _name_registry }}\n    -docker volume rm k3d-registry-vol\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n# Launch a Datasite high-side cluster on http://localhost:8080 (port_high=8080)\n[group('highside')]\nstart-high: (create-cluster _name_high port_high)\n\n# Stop the Datasite high-side cluster\n[group('highside')]\ndelete-high: (delete-cluster _name_high)\n\n# Deploy Syft to the high-side cluster\n[group('highside')]\ndeploy-high: (deploy-devspace _ctx_high _ns_default)\n\n# Reset Syft DB state in the high-side cluster\n[group('highside')]\nreset-high: (reset-syft _ctx_high _ns_default)\n\n# Remove namespace from the high-side cluster\n[group('highside')]\ncleanup-high: (yank-ns _ctx_high _ns_default)\n\n[group('highside')]\nwait-high: (wait-pods _ctx_high _ns_default)\n\n# K9s into the Datasite High cluster\n[group('highside')]\nk9s-high:\n    k9s --context {{ _ctx_high }}\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n# Launch a Datasite low-side cluster on http://localhost:8081 (port_low=8081)\n[group('lowside')]\nstart-low: (create-cluster _name_low port_low)\n\n# Stop the Datasite low-side cluster\n[group('lowside')]\ndelete-low: (delete-cluster _name_low)\n\n# Deploy Syft to the low-side cluster\n[group('lowside')]\ndeploy-low: (deploy-devspace _ctx_low _ns_default \"-p datasite-low\")\n\n# Reset Syft DB state in the low-side cluster\n[group('lowside')]\nreset-low: (reset-syft _ctx_low _ns_default)\n\n# Remove namespace from the low-side cluster\n[group('lowside')]\ncleanup-low: (yank-ns _ctx_low _ns_default)\n\n[group('lowside')]\nwait-low: (wait-pods _ctx_low _ns_default)\n\n# K9s into the Datesite Low cluster\n[group('lowside')]\nk9s-low:\n    k9s --context {{ _ctx_low }}\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n# Launch a Gateway cluster on http://localhost:8083 (port_gw=8083)\n[group('gateway')]\nstart-gw: (create-cluster _name_gw port_gw)\n\n# Delete the Gateway cluster\n[group('gateway')]\ndelete-gw: (delete-cluster _name_gw)\n\n# Deploy Syft to the gateway cluster\n[group('gateway')]\ndeploy-gw: (deploy-devspace _ctx_gw _ns_default \"-p gateway\")\n\n# Reset Syft DB state in the gateway cluster\n[group('gateway')]\nreset-gw: (reset-syft _ctx_gw _ns_default)\n\n# Remove namespace from the gateway cluster\n[group('gateway')]\ncleanup-gw: (yank-ns _ctx_gw _ns_default)\n\n[group('gateway')]\nwait-gw: (wait-pods _ctx_gw _ns_default)\n\n# K9s into the Gateway cluster\n[group('gateway')]\nk9s-gw:\n    k9s --context {{ _ctx_gw }}\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n# Launch SigNoz. UI=http://localhost:3301 OTEL=http://localhost:4317 (port_signoz_ui=3301 port_signoz_otel=4317)\n[group('signoz')]\nstart-signoz: && (apply-signoz _ctx_signoz) (setup-signoz _ctx_signoz)\n    k3d cluster create {{ _name_signoz }} \\\n        --port {{ port_signoz_ui }}:3301@loadbalancer \\\n        --port {{ port_signoz_otel }}:4317@loadbalancer \\\n        --k3s-arg \"--disable=metrics-server@server:*\"\n\n# Remove SigNoz from the cluster\n[group('signoz')]\ndelete-signoz: (delete-cluster _name_signoz)\n\n# Remove all SigNoz data without deleting\n[group('signoz')]\nreset-signoz:\n    @kubectl exec --context {{ _ctx_signoz }} -n platform chi-signoz-clickhouse-cluster-0-0-0 --container clickhouse -- \\\n        clickhouse-client --multiline --multiquery \"\\\n        TRUNCATE TABLE signoz_analytics.rule_state_history_v0; \\\n        TRUNCATE TABLE signoz_logs.logs_v2; \\\n        TRUNCATE TABLE signoz_logs.logs; \\\n        TRUNCATE TABLE signoz_logs.usage; \\\n        TRUNCATE TABLE signoz_metrics.usage; \\\n        TRUNCATE TABLE signoz_traces.durationSort; \\\n        TRUNCATE TABLE signoz_traces.signoz_error_index_v2; \\\n        TRUNCATE TABLE signoz_traces.signoz_index_v2; \\\n        TRUNCATE TABLE signoz_traces.signoz_spans; \\\n        TRUNCATE TABLE signoz_traces.top_level_operations; \\\n        TRUNCATE TABLE signoz_traces.usage_explorer; \\\n        TRUNCATE TABLE signoz_traces.usage;\"\n\n    @echo \"Done. Traces & logs are cleared, but graphs may still show old content.\"\n\n# K9s into the Signoz cluster\n[group('signoz')]\nk9s-signoz:\n    k9s --context {{ _ctx_signoz }}\n\n[group('signoz')]\n[private]\napply-collector kube_context:\n    @echo \"Installing SigNoz OTel Collector in kubernetes context {{ kube_context }}\"\n    helm install k8s-infra k8s-infra \\\n        --repo https://charts.signoz.io \\\n        --kube-context {{ kube_context }} \\\n        --set global.deploymentEnvironment=local \\\n        --set clusterName={{ kube_context }} \\\n        --set otelCollectorEndpoint={{ signoz_otel_url }} \\\n        --set otelInsecure=true \\\n        --set presets.otlpExporter.enabled=true \\\n        --set presets.loggingExporter.enabled=true\n\n# Remove SigNoz from the cluster\n[group('signoz')]\ndelete-collector:\n    helm uninstall k8s-infra\n\n[group('signoz')]\n[private]\napply-signoz kube_context:\n    @echo \"Installing SigNoz in kube context {{ kube_context }}\"\n    helm install signoz signoz \\\n        --repo https://charts.signoz.io \\\n        --kube-context {{ kube_context }} \\\n        --namespace platform \\\n        --create-namespace \\\n        --version 0.52.0 \\\n        --set frontend.service.type=LoadBalancer \\\n        --set otelCollector.service.type=LoadBalancer \\\n        --set otelCollectorMetrics.service.type=LoadBalancer\n\n[group('signoz')]\n[private]\nsetup-signoz kube_context:\n    #!/bin/bash\n    set -euo pipefail\n\n    SIGNOZ_URL=\"http://localhost:3301\"\n    USERNAME=\"admin@localhost\"\n    PASSWORD=\"password\"\n    DASHBOARDS=(\n        \"https://raw.githubusercontent.com/SigNoz/dashboards/refs/heads/main/k8s-infra-metrics/kubernetes-pod-metrics-detailed.json\"\n        \"https://raw.githubusercontent.com/SigNoz/dashboards/refs/heads/main/k8s-infra-metrics/kubernetes-node-metrics-detailed.json\"\n        \"https://raw.githubusercontent.com/SigNoz/dashboards/refs/heads/main/k8s-infra-metrics/kubernetes-cluster-metrics.json\"\n    )\n\n    echo \"Waiting for SigNoz frontend to be available...\"\n    bash ./packages/grid/scripts/wait_for.sh service signoz-frontend \\\n        --namespace platform --context {{ kube_context }} &> /dev/null\n\n    echo \"Setting up SigNoz account...\"\n    curl -s --retry 5 --retry-all-errors -X POST \\\n        -H \"Content-Type: application/json\" \\\n        --data \"{\\\"email\\\":\\\"$USERNAME\\\",\\\"name\\\":\\\"admin\\\",\\\"orgName\\\":\\\"openmined\\\",\\\"password\\\":\\\"$PASSWORD\\\"}\" \\\n        \"$SIGNOZ_URL/api/v1/register\"\n\n    echo \"Adding some dashboards...\"\n    AUTH_TOKEN=$(curl -s -X POST \\\n        -H \"Content-Type: application/json\" \\\n        -d \"{\\\"email\\\":\\\"$USERNAME\\\",\\\"password\\\":\\\"$PASSWORD\\\"}\" \\\n        \"$SIGNOZ_URL/api/v1/login\" | jq -r .accessJwt)\n\n    if [ -z \"$AUTH_TOKEN\" ] || [ \"$AUTH_TOKEN\" = \"null\" ]; then\n        echo \"Could not set up dashboards. But you can do it manually from the dashboard.\"\n        exit 0\n    fi\n\n    for URL in \"${DASHBOARDS[@]}\"; do\n        curl -s -X POST \\\n            -H \"Content-Type: application/json\" \\\n            -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n            -d \"$(curl -s --retry 3 --retry-all-errors \"$URL\")\" \\\n            \"$SIGNOZ_URL/api/v1/dashboards\" &> /dev/null\n    done\n\n    printf \"\\nSignoz is ready and running on %s\\n\" \"$SIGNOZ_URL\"\n    printf \"Email: \\033[1;36m%s\\033[0m\\n\" \"$USERNAME\"\n    printf \"Password: \\033[1;36m%s\\033[0m\\n\" \"$PASSWORD\"\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n# List all clusters\n[group('cluster')]\nlist-clusters:\n    k3d cluster list\n\n# Stop all clusters\n[group('cluster')]\ndelete-clusters:\n    k3d cluster delete --all\n\n[group('cluster')]\n[private]\ncreate-cluster cluster_name port *args='': start-registry && (apply-coredns \"k3d-\" + cluster_name) (apply-collector \"k3d-\" + cluster_name)\n    k3d cluster create {{ cluster_name }} \\\n        --port {{ port }}:80@loadbalancer \\\n        --k3s-arg \"--disable=metrics-server@server:*\" \\\n        --registry-use {{ registry_url }} {{ args }}\n\n[group('cluster')]\n[private]\ndelete-cluster *args='':\n    #!/bin/bash\n    set -euo pipefail\n\n    # remove the k3d- prefix\n    ARGS=$(echo \"{{ args }}\" | sed -e 's/k3d-//g')\n    k3d cluster delete $ARGS\n\n[group('cluster')]\n[private]\napply-coredns kube_context:\n    @echo \"Applying custom CoreDNS config\"\n\n    kubectl apply -f ./scripts/k8s-coredns-custom.yml --context {{ kube_context }}\n    kubectl delete pod -n kube-system -l k8s-app=kube-dns --context {{ kube_context }}\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n[group('devspace')]\n[private]\ndeploy-devspace kube_context namespace *args='':\n    #!/bin/bash\n    set -euo pipefail\n\n    cd packages/grid\n\n    PROFILE=\"{{ _g_profiles }}\"\n    PROFILE=$(echo \"$PROFILE\" | sed -E 's/^,*|,*$//g')\n    if [ -n \"$PROFILE\" ]; then\n        PROFILE=\"-p $PROFILE\"\n    fi\n\n    echo \"Deploying to kube context {{ kube_context }}\"\n\n    devspace deploy -b \\\n        --no-warn \\\n        --kube-context {{ kube_context }} \\\n        --namespace {{ namespace }} \\\n        $PROFILE \\\n        {{ args }} \\\n        --var CONTAINER_REGISTRY={{ registry_url }}\n\n[group('devspace')]\n[private]\npurge-devspace kube_context namespace:\n    #!/bin/bash\n    set -euo pipefail\n\n    cd packages/grid\n    devspace purge --force-purge --kube-context {{ kube_context }} --no-warn --namespace {{ namespace }}\n    sleep 3\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n[group('cloud')]\n[private]\ncheck-platform:\n    #!/bin/bash\n    set -euo pipefail\n\n    OSTYPE=$(uname -sm)\n    MSG=\"==================================================================================================\\n\\\n    Deploying dev->cloud k8s (x64 nodes) requires images to be built with --platform=linux/amd64\\n\\\n    On Apple Silicon, cross-platform image is unstable on different providers\\n\\n\\\n    Current status:\\n\\\n    ‚úÖ | Docker Desktop | 4.34.0+ | *Enable* containerd and *uncheck* 'Use Rosetta for x86_64/amd64...'\\n\\\n    ‚ùå | OrbStack       | 1.7.2   | Rosetta: gets stuck & qemu: errors with 'illegal instruction'\\n\\\n    ‚ùå | Lima VM/Colima | 0.23.2  | Rosetta: gets stuck & qemu: errors with 'illegal instruction'\\n\\\n    ==================================================================================================\"\n\n    if [[ \"$OSTYPE\" == \"Darwin arm64\" ]]; then\n        echo -e $MSG\n    fi\n\n[group('cloud')]\n[private]\ndeploy-cloud kube_context registry_url namespace profile: check-platform\n    #!/bin/bash\n\n    CONTEXT_NAME=$(kubectl config get-contexts -o=name | grep \"{{ kube_context }}\")\n\n    if [ -z \"$CONTEXT_NAME\" ]; then\n        echo \"Context not found: {{ kube_context }}. Authorized with cloud providers to get relevant K8s cluster contexts\"\n        exit 1\n    fi\n\n    set -euo pipefail\n\n    # cloud deployments always have tracing false + platform=amd64\n    just tracing=false registry_url={{ registry_url }} \\\n        deploy-devspace $CONTEXT_NAME {{ namespace }} \"-p {{ profile }} --var PLATFORM=amd64\"\n\n[group('cloud')]\n[private]\npurge-cloud kube_context namespace:\n    #!/bin/bash\n\n    CONTEXT_NAME=$(kubectl config get-contexts -o=name | grep \"{{ kube_context }}\")\n\n    if [ -z \"$CONTEXT_NAME\" ]; then\n        echo \"Context not found: {{ kube_context }}. Authorized with cloud providers to get relevant K8s cluster contexts\"\n        exit 1\n    fi\n\n    set -euo pipefail\n\n    just purge-devspace $CONTEXT_NAME {{ namespace }}\n    kubectl delete ns {{ namespace }} --force --grace-period=0 --context $CONTEXT_NAME\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n# Auth all components required for deploying Syft to Google Cloud\n[group('cloud-gcp')]\nauth-gcloud:\n    #!/bin/bash\n    set -euo pipefail\n\n    # login to gcloud\n    ACCOUNT=$(gcloud config get-value account)\n    if [ -z \"$ACCOUNT\" ]; then\n        gcloud auth login\n    fi\n\n    echo \"Logged in as \\\"$(gcloud config get-value account)\\\"\"\n\n    # install gke-gcloud-auth-plugin\n    gke_installed=$(gcloud components list --only-local-state --filter gke-gcloud-auth-plugin --format=list 2>/dev/null)\n    if [ -z \"$gke_installed\" ]; then\n        gcloud components install gke-gcloud-auth-plugin\n        echo \"Installed gke-gcloud-auth-plugin\"\n    fi\n\n# Deploy local code as datasite-high to Google Kubernetes Engine\n[group('cloud-gcp')]\ndeploy-gcp-high gcp_cluster gcp_registry_url namespace=\"syft\": (deploy-cloud gcp_cluster gcp_registry_url namespace \"gcp\")\n\n# Deploy local code as datasite-high to Google Kubernetes Engine\n[group('cloud-gcp')]\ndeploy-gcp-low gcp_cluster gcp_registry_url namespace=\"syft\": (deploy-cloud gcp_cluster gcp_registry_url namespace \"gcp-low\")\n\n# Purge deployment from a cluster\n[group('cloud-gcp')]\npurge-gcp gcp_cluster namespace=\"syft\": (purge-cloud gcp_cluster namespace)\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n[group('cloud-az')]\nauth-az tenant=\"creditsopenmined.onmicrosoft.com\":\n    #!/bin/bash\n\n    # login to azure\n    ACCOUNT=$(az account show --query user.name)\n    if [ -z \"$ACCOUNT\" ]; then\n        az login --tenant {{ tenant }}\n    fi\n\n    echo \"Logged in as $(az account show --query user.name)\"\n\n# Deploy local code as datasite-high to Azure Kubernetes Service\n[group('cloud-az')]\ndeploy-az-high aks_cluster az_registry namespace=\"syft\": (deploy-cloud aks_cluster az_registry namespace \"azure\")\n\n# ---------------------------------------------------------------------------------------------------------------------\n\n# Reset Syft state in a cluster\n[group('utils')]\nreset-syft kube_context namespace:\n    scripts/reset_k8s.sh --context {{ kube_context }} --namespace {{ namespace }}\n\n# Delete all local clusters and registry\n[group('utils')]\ndelete-all: delete-clusters delete-registry\n\n# Prune local docker cache. Run atleast once a month.\n[group('utils')]\nprune-docker:\n    -docker container prune -f\n    -docker volume prune -af\n    -docker image prune -af\n    -docker system prune -af --volumes\n\n# Delete all resources in a namespace\n[group('utils')]\nyank-ns kube_context namespace:\n    # delete pods ùôõ ùôñ ùô® ùô©\n    -kubectl delete statefulsets --all --context {{ kube_context }} --namespace {{ namespace }} --now\n    -kubectl delete deployments --all --context {{ kube_context }} --namespace {{ namespace }} --now\n    -kubectl delete pods --all --namespace {{ namespace }} --grace-period=0 --force\n\n    # delete resources ùôõ ùôñ ùô® ùô©\n    -kubectl delete configmap --all --context {{ kube_context }} --namespace {{ namespace }} --now\n    -kubectl delete secrets --all --context {{ kube_context }} --namespace {{ namespace }} --now\n    -kubectl delete ingress --all --context {{ kube_context }} --namespace {{ namespace }} --now\n\n    # delete namespace NOT ùôõ ùôñ ùô® ùô© :(\n    -kubectl delete ns {{ namespace }} --context {{ kube_context }} --grace-period=0 --force --timeout=5s\n\n    # Too slow... yanking it\n    -kubectl get ns {{ namespace }} --context {{ kube_context }} -o json | jq '.spec.finalizers = []' | \\\n        kubectl replace --context {{ kube_context }} --raw /api/v1/namespaces/{{ namespace }}/finalize -f -\n\n    @echo \"Done\"\n\n# Wait for all pods to be ready in a namespace\n[group('utils')]\n@wait-pods kube_context namespace:\n    echo \"Waiting for all pods to be ready in cluster={{ kube_context }} namespace={{ namespace }}\"\n    # Wait for at least one pod to appear (timeout after 5 minutes)\n    timeout 300 bash -c 'until kubectl get pods --context {{ kube_context }} --namespace {{ namespace }} 2>/dev/null | grep -q \"\"; do sleep 5; done'\n\n    kubectl wait --for=condition=ready pod --all --timeout=300s --context {{ kube_context }} --namespace {{ namespace }}\n\n    # if the above doesn't wait as we expect the drop the above and use the below\n    # @bash packages/grid/scripts/wait_for.sh service proxy --context {{ kube_context }} --namespace {{ namespace }}\n    # @bash packages/grid/scripts/wait_for.sh service frontend --context {{ kube_context }} --namespace {{ namespace }}\n    # @bash packages/grid/scripts/wait_for.sh service postgres --context {{ kube_context }} --namespace {{ namespace }}\n    # @bash packages/grid/scripts/wait_for.sh service seaweedfs --context {{ kube_context }} --namespace {{ namespace }}\n    # @bash packages/grid/scripts/wait_for.sh service backend --context {{ kube_context }} --namespace {{ namespace }}\n    echo \"All pods are ready\"\n"
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "packages",
          "type": "tree",
          "content": null
        },
        {
          "name": "releases.md",
          "type": "blob",
          "size": 1.7431640625,
          "content": "# Releases\n\n:exclamation: PySyft and Syft Server must use the same `version`.\n\n### Latest Stable\n\n- `0.9.1` (Stable) - <a href=\"https://docs.openmined.org/en/latest/index.html\">Docs</a>\n- Install PySyft (Stable): `pip install -U syft`\n\n### Latest Beta\n\n- `0.9.2` (Beta) - `dev` branch üëàüèΩ\n- Install PySyft (Beta): `pip install -U syft --pre`\n\n### Supported versions\n\n- `0.9.0` - <a href=\"https://github.com/OpenMined/PySyft/tree/0.9.0/notebooks/api/\">API</a>\n- Install PySyft (Beta): `pip install -U syft==0.9.0`\n\n**Deprecated**:\n\n- `0.8.8` - <a href=\"https://github.com/OpenMined/PySyft/tree/0.8.8/notebooks/api/0.8\">API</a>\n- `0.8.7` - <a href=\"https://github.com/OpenMined/PySyft/tree/0.8.7/notebooks/api/0.8\">API</a>\n- `0.8.6` - <a href=\"https://github.com/OpenMined/PySyft/tree/0.8.6/notebooks/api/0.8\">API</a>\n- `0.8.5-post.2` - <a href=\"https://github.com/OpenMined/PySyft/tree/0.8.5-post.2/notebooks/api/0.8\">API</a>\n- `0.8.4` - <a href=\"https://github.com/OpenMined/PySyft/tree/0.8.4/notebooks/api/0.8\">API</a>\n- `0.8.3` - <a href=\"https://github.com/OpenMined/PySyft/tree/0.8.3/notebooks/api/0.8\">API</a>\n- `0.8.2` - <a href=\"https://github.com/OpenMined/PySyft/tree/0.8.2/notebooks/api/0.8\">API</a>\n- `0.8.1` - <a href=\"https://github.com/OpenMined/PySyft/tree/0.8.1/notebooks/api/0.8\">API</a>\n- `0.8.0` - <a href=\"https://github.com/OpenMined/PySyft/tree/0.8/notebooks/api/0.8\">API</a>\n- `0.7.0` - <a href=\"https://github.com/OpenMined/courses/tree/introduction-to-remote-data-science-dev\">Course 3 Updated</a>\n- `0.6.0` - <a href=\"https://github.com/OpenMined/courses/tree/introduction-to-remote-data-science\">Course 3</a>\n- `0.5.1` - <a href=\"https://github.com/OpenMined/courses/tree/foundations-of-private-computation\">Course 2</a> + M1 Hotfix\n- `0.2.0` - `0.5.0`\n"
        },
        {
          "name": "ruff.toml",
          "type": "blob",
          "size": 0.6376953125,
          "content": "extend-include = [\"*.ipynb\"]\n\nline-length = 88\n\ntarget-version = \"py310\"\n\nextend-exclude = [\"*.gypi\"]\n\n# Enable flake8-bugbear (`B`) rules.\n# https://beta.ruff.rs/docs/configuration/#using-rufftoml\n[lint]\nselect = [\n    \"E\",    # pycodestyle\n    \"F\",    # pyflake\n    \"B\",    # flake8-bugbear\n    \"C4\",   # flake8-comprehensions\n    # \"PERF\", # perflint\n    \"UP\",   # pyupgrade\n]\nignore = [\n    \"B904\",  # check for raise statements in exception handlers that lack a from clause\n    \"B905\",  # zip() without an explicit strict= parameter\n]\n\n[lint.per-file-ignores]\n\"*.ipynb\" = [\"E402\"]\n\"__init__.py\" = [\"F401\"]\n\n[lint.pycodestyle]\nmax-line-length = 120\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "test_helpers",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tox.ini",
          "type": "blob",
          "size": 64.146484375,
          "content": "[tox]\nenvlist =\n    dev.k8s.launch.datasite\n    dev.k8s.launch.gateway\n    dev.k8s.launch.datasite.highlow\n    dev.k8s.destroy.datasite.highlow\n    dev.k8s.registry\n    dev.k8s.start\n    dev.k8s.deploy\n    dev.k8s.hotreload\n    dev.k8s.info\n    dev.k8s.cleanup\n    dev.k8s.destroy\n    dev.k8s.destroyall\n    dev.k8s.install.signoz\n    lint\n    stack.test.integration\n    syft.docs\n    syft.jupyter\n    syft.publish\n    syft.test.security\n    syft.test.unit\n    syft.test.scenario\n    syft.test.scenario.sync\n    syft.test.notebook\n    syft.test.notebook.scenario\n    syft.test.notebook.scenario.sync\n    single_container.launch\n    single_container.destroy\n    stack.test.notebook\n    stack.test.integration.k8s\n    stack.test.notebook.scenario.k8s\n    stack.test.notebook.scenario.k8s.sync\n    stack.test.scenario.k8s\n    stack.test.scenario.k8s.sync\n    frontend.test.unit\n    frontend.test.e2e\n    frontend.generate.types\n    syft.build.helm\n    syft.package.helm\n    syft.test.helm\n    syft.test.helm.upgrade\n    syft.protocol.check\n    syftcli.test.unit\n    syftcli.publish\n    syftcli.build\n    seaweedfs.test.unit\n    backend.test.basecpu\n    e2e.test.notebook\n    migration.prepare\n    migration.test\n    migration.k8s.prepare\n    migration.k8s.test\n    syft.api.snapshot\nskipsdist = True\n\n\n[testenv]\nbasepython = {env:TOX_PYTHON:python3}\ncommands =\n    python --version\nsetenv =\n    UV_HTTP_TIMEOUT = 600\n\n# Syft\n[testenv:syft]\ndeps =\n    -e{toxinidir}/packages/syft[dev,data_science]\nchangedir = {toxinidir}/packages/syft\ndescription = Syft\nallowlist_externals =\n    bash\ncommands =\n    bash -c 'uv pip list || pip list'\n\n# Syft Minimal - without dev+datascience packages\n[testenv:syft-minimal]\ndeps =\n    -e{toxinidir}/packages/syft\nchangedir = {toxinidir}/packages/syft\ndescription = Syft\nallowlist_externals =\n    bash\ncommands =\n    bash -c 'uv pip list || pip list'\n\n[testenv:syftcli]\ndeps =\n    -e{toxinidir}/packages/syftcli[dev]\nchangedir = {toxinidir}/packages/syftcli\ndescription = Syft CLI\nallowlist_externals =\n    bash\ncommands =\n    bash -c 'uv pip list || pip list'\n\n[testenv:syft.publish]\nchangedir = {toxinidir}/packages/syft\ndescription = Build and Publish Syft Wheel\ndeps =\n    build\ncommands =\n    python -c 'from shutil import rmtree; rmtree(\"build\", True); rmtree(\"dist\", True)'\n    python -m build .\n\n\n[testenv:syftcli.publish]\nchangedir = {toxinidir}/packages/syftcli\ndescription = Build and Publish Syft CLI Wheel\ndeps =\n    build\nallowlist_externals =\n    bash\ncommands =\n    bash -c 'rm -rf build/ dist/ syftcli.egg-info/'\n    python -m build .\n\n[testenv:syftcli.build]\nchangedir = {toxinidir}/packages/syftcli\ndescription = Build SyftCLI Binary for each platform\ndeps =\n    -e{toxinidir}/packages/syftcli[build]\nallowlist_externals =\n    bash\nsetenv =\n    SYFT_CLI_VERSION = {env:SYFT_CLI_VERSION}\ncommands =\n    python -c 'from shutil import rmtree; rmtree(\"build\", True); rmtree(\"dist\", True)'\n\n\n    ;Since we build universal binary for MacOS,we need to check the python is universal2 or not\n    bash -c 'if [[ \"$OSTYPE\" == \"darwin\"* ]]; then \\\n        arch_info=$(lipo -info \"$(which python3)\"); \\\n        echo \"Arch: $arch_info\"; \\\n        if [[ \"$arch_info\" == *\"Non-fat\"* ]]; then \\\n            echo \"Building on MacOS Requires Universal2 python\"; \\\n            echo \"Please install universal2 python from https://www.python.org/downloads/macos/\"; \\\n            exit 1; \\\n        fi; \\\n    fi'\n\n    ;check the platform and build accordingly by naming the binary as syftcli plus the extension\n    ; Check if SYFT_CLI_VERSION is set or choosing the current version available\n    bash -c 'if [ -z $SYFT_CLI_VERSION ]; then \\\n        echo \"SYFT_CLI_VERSION is not set\"; \\\n        SYFT_CLI_VERSION=$(python3 syftcli/version.py); \\\n        echo \"Setting SYFT_CLI_VERSION to $SYFT_CLI_VERSION\"; \\\n    else \\\n        echo \"SYFT_CLI_VERSION is already set to $SYFT_CLI_VERSION\"; \\\n    fi && \\\n\n    echo \"Building syftcli-$SYFT_CLI_VERSION for $OSTYPE\" && \\\n\n    if [[ \"$OSTYPE\" == \"darwin\"* ]]; then \\\n         pyinstaller --clean --onefile --name syftcli-v$SYFT_CLI_VERSION-macos-universal2 --distpath ./dist/cli syftcli/cli.py --target-arch universal2; \\\n    elif [[ \"$OSTYPE\" == \"linux-gnu\"* ]]; then \\\n        pyinstaller --clean --onefile --name syftcli-v$SYFT_CLI_VERSION-linux-x86_64 --distpath ./dist/cli syftcli/cli.py; \\\n    else \\\n        pyinstaller --clean --onefile --name syftcli-v$SYFT_CLI_VERSION-windows-x86_64  --distpath ./dist/cli syftcli/cli.py; \\\n    fi'\n\n\n[testenv:lint]\ndescription = Linting\nallowlist_externals =\n    bash\ndeps =\n    black[python2]\n    isort\n    pre-commit\ncommands =\n    black .\n    isort .\n    pre-commit run --all-files\n\n[testenv:frontend.test.unit]\ndescription = Frontend Unit Tests\nallowlist_externals =\n    docker\n    bash\n    pnpm\npassenv=HOME, USER\nchangedir = {toxinidir}/packages/grid/frontend\nsetenv =\n    DOCKER = {env:DOCKER:false}\ncommands =\n    bash -c \"echo Running with DOCKER=$DOCKER; date\"\n\n    bash -c 'if [[ \"$DOCKER\" == \"false\" ]]; then \\\n        bash ./scripts/check_pnpm.sh; \\\n        pnpm install; \\\n        pnpm run test:unit; \\\n    else \\\n        docker build --target syft-ui-tests -t ui-test -f frontend.dockerfile .; \\\n        docker run -t ui-test; \\\n    fi'\n\n\n\n[testenv:syft.docs]\ndescription = Build Docs for Syft\nchangedir = {toxinidir}/docs\ndeps =\n    {[testenv:syft]deps}\n    -r {toxinidir}/docs/requirements.txt\nallowlist_externals =\n    make\n    echo\n    cd\n    rm\n    ls\n    xargs\n    bash\ncommands =\n    python --version\n    bash -c \"cd source/api_reference && ls | grep -v index.rst | xargs rm\"\n    sphinx-apidoc -f -M -d 2 -o ./source/api_reference/ ../packages/syft/src/syft\n    make html\n    echo \"Open: {toxinidir}/docs/build/html/index.html\"\n\n[testenv:syft.jupyter]\ndescription = Jupyter Notebook with Editable Syft\ndeps =\n    {[testenv:syft]deps}\n    jupyter\n    jupyterlab\nallowlist_externals =\n    bash\ncommands =\n    bash -c 'if [ -z \"{posargs}\" ]; then \\\n        jupyter lab --ip 0.0.0.0; \\\n        else \\\n        jupyter lab --ip 0.0.0.0 --ServerApp.token={posargs}; \\\n        fi'\n\n[testenv:syft.protocol.check]\ndescription = Syft Protocol Check\ndeps =\n    {[testenv:syft-minimal]deps}\nchangedir = {toxinidir}/packages/syft\nallowlist_externals =\n    bash\nsetenv =\n    BUMP = {env:BUMP:False}\ncommands =\n    bash -c \"echo Using BUMP=${BUMP}\"\n    python -c 'import syft as sy; sy.check_or_stage_protocol()'\n    bash -c 'if [[ \"$BUMP\" != \"False\" ]]; then \\\n        python -c \"import syft as sy; sy.bump_protocol_version()\"; \\\n        fi'\n\n[testenv:syft.api.snapshot]\ndescription = Syft API Snapshot Check\ndeps =\n    {[testenv:syft-minimal]deps}\nchangedir = {toxinidir}/packages/syft\nallowlist_externals =\n    bash\nsetenv =\n    SAVE_SNAP = {env:SAVE_SNAP:False}\n    STABLE_RELEASE = {env:STABLE_RELEASE:False}\ncommands =\n    bash -c \"echo Using SAVE_SNAP=${SAVE_SNAP}, STABLE_RELEASE=${STABLE_RELEASE}\"\n    python -c 'import syft as sy; sy.show_api_diff()'\n    bash -c 'if [[ \"$SAVE_SNAP\" != \"False\" ]]; then \\\n        python -c \"import syft as sy; sy.take_api_snapshot()\"; \\\n        fi'\n\n\n[testenv:syft.test.security]\ndescription = Security Checks for Syft\nchangedir = {toxinidir}/packages/syft\ndeps =\n    {[testenv:syft]deps}\ncommands =\n    bandit -r src\n    # restrictedpython 6.2\n    # Temporarily ignore pytorch vulnerability warning here\n    # https://data.safetycli.com/v/71670/97c\n    # TODO: Remove `-i 71670` once torch is updated\n    safety check -i 70612 -i 71670\n\n[testenv:syft.test.unit]\ndescription = Syft Unit Tests\ndeps =\n    {[testenv:syft]deps}\nallowlist_externals =\n    bash\n    uv\nchangedir = {toxinidir}/packages/syft\nsetenv =\n    ENABLE_SIGNUP=False\ncommands =\n    bash -c 'ulimit -n 4096 || true'\n    pytest -n auto --dist loadgroup --durations=20 --disable-warnings\n\n[testenv:syft.test.scenario]\ndescription = BigQuery Scenario Tests on Python Servers (L2)\nchangedir = {toxinidir}\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:python}\ndeps =\n    {[testenv:syft]deps}\n    pytest-asyncio\n    pytest-timeout\n    db-dtypes\n    google-cloud-bigquery\nallowlist_externals =\n    bash\n    pytest\ncommands =\n    bash -c \"echo Running L2 BigQuery Scenario Tests with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE\"\n    bash -c \"pytest -s --disable-warnings tests/scenariosv2/l2_test.py\"\n\n[testenv:syft.test.scenario.sync]\ndescription = BigQuery Scenario Tests on Python Servers (L0)\nchangedir = {toxinidir}\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:python}\ndeps =\n    {[testenv:syft]deps}\n    pytest-asyncio\n    pytest-timeout\n    db-dtypes\n    google-cloud-bigquery\nallowlist_externals =\n    bash\n    pytest\ncommands =\n    bash -c \"echo Running L0 BigQuery Scenario Tests with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE\"\n    bash -c \"pytest -s --disable-warnings tests/scenariosv2/l0_test.py\"\n\n[testenv:stack.test.scenario.k8s]\ndescription = BigQuery Scenario Tests on K8s (L2)\nchangedir = {toxinidir}\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}\ndeps =\n    {[testenv:syft]deps}\n    pytest-asyncio\n    pytest-timeout\nallowlist_externals =\n    bash\n    just\n    pytest\ncommands_pre =\n    just delete-all start-high deploy-high wait-high\ncommands =\n    bash -c \"echo Running L2 BigQuery Scenario Tests on K8s with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE\"\n    bash -c \"pytest -s --disable-warnings tests/scenariosv2/l2_test.py\"\ncommands_post =\n    just delete-all\n\n[testenv:stack.test.scenario.k8s.sync]\ndescription = BigQuery Scenario Tests on K8s (L0)\nchangedir = {toxinidir}\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}\ndeps =\n    {[testenv:syft]deps}\n    pytest-asyncio\n    pytest-timeout\nallowlist_externals =\n    bash\n    just\n    pytest\ncommands_pre =\n    just delete-all start-high deploy-high wait-high\n    just start-low deploy-low wait-low\ncommands =\n    bash -c \"echo Running L0 BigQuery Scenario Tests on K8s with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE\"\n    bash -c \"pytest -s --disable-warnings tests/scenariosv2/l0_test.py\"\ncommands_post =\n    just delete-all\n\n[testenv:syft.test.notebook]\ndescription = Syft Notebook Tests\ndeps =\n    -e{toxinidir}/packages/syft[dev,data_science]\n    nbmake\nchangedir = {toxinidir}/notebooks\nallowlist_externals =\n    bash\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:python}\n    DEV_MODE = {env:DEV_MODE:True}\n    TEST_NOTEBOOK_PATHS = {env:TEST_NOTEBOOK_PATHS:api/0.8,tutorials}\n    ENABLE_SIGNUP={env:ENABLE_SIGNUP:False}\n    BUMP_PROTOCOL={env:BUMP_PROTOCOL:False}\ncommands =\n    bash -c 'if [[ $BUMP_PROTOCOL == \"True\" ]]; then \\\n                python -c \"import syft as sy; sy.bump_protocol_version()\"; \\\n            fi;'\n    bash -c \"echo Running with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE DEV_MODE=$DEV_MODE TEST_NOTEBOOK_PATHS=$TEST_NOTEBOOK_PATHS; ENABLE_SIGNUP=$ENABLE_SIGNUP; date\"\n    bash -c \"for subfolder in $(echo ${TEST_NOTEBOOK_PATHS} | tr ',' ' '); do \\\n    if [[ $subfolder == *tutorials* ]]; then \\\n        pytest -x --nbmake \"$subfolder\" \\\n            -p no:randomly \\\n            --ignore=tutorials/model-training \\\n            --ignore=tutorials/version-upgrades \\\n            -n $(python -c 'import multiprocessing; print(multiprocessing.cpu_count())') \\\n            -vvvv && \\\n        pytest -x --nbmake tutorials/model-training -p no:randomly -vvvv; \\\n    else \\\n        pytest -x --nbmake \"$subfolder\" -p no:randomly -k 'not 11-container-images-k8s.ipynb' -vvvv; \\\n    fi \\\n    done\"\n    ; pytest -x --nbmake api/0.8 -p no:randomly -vvvv\n    ; pytest -x --nbmake api/0.9 -p no:randomly -vvvv\n    ; pytest -x --nbmake tutorials -p no:randomly -vvvv\n    ; pytest -x --nbmake tutorials/pandas-cookbook -p no:randomly -vvvv\n\n\n# This is testing BQ without syncing and with in-memory python\n[testenv:syft.test.notebook.scenario]\ndescription = Syft Notebook Scenario Tests\ndeps =\n    {[testenv:syft]deps}\n    nbmake\n    db-dtypes\n    google-cloud-bigquery\n    aiosmtpd\nchangedir = {toxinidir}/notebooks\nallowlist_externals =\n    bash\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:python}\n    DEV_MODE = {env:DEV_MODE:True}\n    TEST_NOTEBOOK_PATHS = {env:TEST_NOTEBOOK_PATHS:scenarios/bigquery}\n    TEST_query_limit_size={env:test_query_limit_size:500000}\n    SERVER_URL = {env:SERVER_URL:http://localhost}\n    SERVER_PORT = {env:SERVER_PORT:8080}\n    NUM_TEST_USERS = {env:NUM_TEST_USERS:5}\n    NUM_TEST_JOBS = {env:NUM_TEST_JOBS:10}\ncommands =\n    python --version\n    bash -c \"echo Running with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE DEV_MODE=$DEV_MODE TEST_NOTEBOOK_PATHS=$TEST_NOTEBOOK_PATHS; date\"\n    bash -c \"for subfolder in $(echo ${TEST_NOTEBOOK_PATHS} | tr ',' ' ');\\\n    do \\\n        pytest -s -x --nbmake --nbmake-timeout=1000 \"$subfolder\" --ignore=scenarios/bigquery/sync --ignore=scenarios/bigquery/upgradability -p no:randomly -vvvv --log-cli-level=DEBUG --capture=no;\\\n    done\"\n\n# This is testing BQ with syncing and with in-memory python\n[testenv:syft.test.notebook.scenario.sync]\ndescription = Syft Notebook Scenario Tests\ndeps =\n    {[testenv:syft]deps}\n    nbmake\n    db-dtypes\n    google-cloud-bigquery\n    aiosmtpd\nchangedir = {toxinidir}/notebooks\nallowlist_externals =\n    bash\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:python}\n    DEV_MODE = {env:DEV_MODE:True}\n    TEST_NOTEBOOK_PATHS = {env:TEST_NOTEBOOK_PATHS:scenarios/bigquery/sync}\n    TEST_BIGQUERY_APIS_LIVE = {env:TEST_BIGQUERY_APIS_LIVE:false}\n    TEST_query_limit_size={env:test_query_limit_size:500000}\n    SERVER_URL = {env:SERVER_URL:http://localhost}\n    SERVER_PORT = {env:SERVER_PORT:8080}\ncommands =\n\n    bash -c \"echo Running with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE DEV_MODE=$DEV_MODE TEST_NOTEBOOK_PATHS=$TEST_NOTEBOOK_PATHS; date\"\n    bash -c \"for subfolder in $(echo ${TEST_NOTEBOOK_PATHS} | tr ',' ' ');\\\n    do \\\n        pytest -s -x --nbmake --nbmake-timeout=1000 \"$subfolder\" -p no:randomly -vvvv;\\\n    done\"\n\n\n# This is testing BQ without syncing over k8s\n[testenv:stack.test.notebook.scenario.k8s]\ndescription = Scenario Notebook Tests for Core Stack using K8s\ndeps =\n    {[testenv:syft]deps}\n    nbmake\n    db-dtypes\n    google-cloud-bigquery\n    aiosmtpd\nchangedir = {toxinidir}\npassenv=HOME, USER\nallowlist_externals =\n    devspace\n    kubectl\n    grep\n    sleep\n    bash\n    k3d\n    echo\n    tox\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}\n    DEVSPACE_PROFILE = bigquery-scenario-tests\n    GITHUB_CI = {env:GITHUB_CI:false}\n    SYFT_BASE_IMAGE_REGISTRY = {env:SYFT_BASE_IMAGE_REGISTRY:k3d-registry.localhost:5800}\n    DATASITE_CLUSTER_NAME = {env:DATASITE_CLUSTER_NAME:bigquery-high}\n    SERVER_URL = {env:SERVER_URL:http://localhost}\n    SERVER_PORT = {env:SERVER_PORT:8080}\n    TEST_EXTERNAL_REGISTRY = {env:TEST_EXTERNAL_REGISTRY:k3d-registry.localhost:5800}\n    TEST_QUERY_LIMIT_SIZE={env:TEST_QUERY_LIMIT_SIZE:500000}\n    TRACING={env:TRACING:False}\n    NUM_TEST_USERS = {env:NUM_TEST_USERS:5}\n    NUM_TEST_JOBS = {env:NUM_TEST_JOBS:10}\n    NUM_TEST_WORKERS = {env:NUM_TEST_WORKERS:2}\ncommands =\n    bash -c \"python --version || true\"\n    bash -c \"echo Running with GITHUB_CI=$GITHUB_CI; date\"\n    bash -c \"echo Running with TEST_EXTERNAL_REGISTRY=$TEST_EXTERNAL_REGISTRY; date\"\n    python -c 'import syft as sy; sy.stage_protocol_changes()'\n    k3d version\n\n    # Deleting Old Cluster\n    bash -c \"k3d cluster delete ${DATASITE_CLUSTER_NAME} || true\"\n\n    # Deleting registry & volumes\n    bash -c \"k3d registry delete k3d-registry.localhost || true\"\n    bash -c \"docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true\"\n\n    # Create registry\n    tox -e dev.k8s.registry\n\n\n    # Creating bigquery-high cluster on port SERVER_PORT\n    bash -c '\\\n        export CLUSTER_NAME=${DATASITE_CLUSTER_NAME} CLUSTER_HTTP_PORT=${SERVER_PORT} && \\\n        tox -e dev.k8s.start && \\\n        tox -e dev.k8s.deploy'\n\n    ; # free up build cache after build of images\n    ; bash -c 'if [[ \"$GITHUB_CI\" != \"false\" ]]; then \\\n    ;     docker image prune --all --force; \\\n    ;     docker builder prune --all --force; \\\n    ; fi'\n\n    ; sleep 30\n\n\n    # wait for bigquery-high\n    bash packages/grid/scripts/wait_for.sh service postgres --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service backend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service proxy --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service seaweedfs --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service frontend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash -c '(kubectl logs service/frontend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q -E \"Network:\\s+https?://[a-zA-Z0-9.-]+:[0-9]+/\" || true'\n\n    # Checking logs generated & startup of bigquery-high\n    bash -c '(kubectl logs service/backend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q \"Application startup complete\" || true'\n\n    bash -c \"pytest -s -x --nbmake notebooks/scenarios/bigquery -p no:randomly --ignore=notebooks/scenarios/bigquery/sync  --ignore=notebooks/scenarios/bigquery/upgradability -vvvv --nbmake-timeout=1000 --log-cli-level=DEBUG --capture=no;\"\n\n    # deleting clusters created\n    bash -c \"CLUSTER_NAME=${DATASITE_CLUSTER_NAME} tox -e dev.k8s.destroy || true\"\n    bash -c \"k3d registry delete k3d-registry.localhost || true\"\n    bash -c \"docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true\"\n\n\n# This is testing BQ with syncing over k8s\n[testenv:stack.test.notebook.scenario.k8s.sync]\ndescription = Syft Notebook Scenario Tests over k8s\ndeps =\n    {[testenv:syft]deps}\n    nbmake\n    db-dtypes\n    google-cloud-bigquery\n    aiosmtpd\nchangedir = {toxinidir}/notebooks\nallowlist_externals =\n    bash\n    devspace\nsetenv =\n    DEV_MODE = {env:DEV_MODE:True}\n    DEVSPACE_PROFILE = bigquery-scenario-tests\n    TEST_NOTEBOOK_PATHS = {env:TEST_NOTEBOOK_PATHS:scenarios/bigquery/sync}\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}\n    CLUSTER_NAME_HIGH = {env:CLUSTER_NAME_HIGH:bigquery-high}\n    CLUSTER_NAME_LOW = {env:CLUSTER_NAME_LOW:bigquery-low}\n    CLUSTER_HTTP_PORT_HIGH={env:CLUSTER_HTTP_PORT_HIGH:9081}\n    CLUSTER_HTTP_PORT_LOW={env:CLUSTER_HTTP_PORT_LOW:9083}\n    SYFT_BASE_IMAGE_REGISTRY = {env:SYFT_BASE_IMAGE_REGISTRY:k3d-registry.localhost:5800}\n    TEST_EXTERNAL_REGISTRY = {env:TEST_EXTERNAL_REGISTRY:k3d-registry.localhost:5800}\ncommands =\n    bash -c \"echo Running highlow with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE DEV_MODE=$DEV_MODE TEST_NOTEBOOK_PATHS=$TEST_NOTEBOOK_PATHS; date\"\n    bash -c 'tox -e dev.k8s.destroy.datasite.highlow'\n    bash -c \"k3d registry delete k3d-registry.localhost || true\"\n    bash -c \"docker volume rm k3d-${CLUSTER_NAME_HIGH}-images --force || true\"\n    bash -c \"docker volume rm k3d-${CLUSTER_NAME_LOW}-images --force || true\"\n\n    # Now create everything\n    bash -c 'tox -e dev.k8s.launch.datasite.highlow'\n\n    bash -c \"for subfolder in $(echo ${TEST_NOTEBOOK_PATHS} | tr ',' ' ');\\\n    do \\\n        pytest -x --nbmake --nbmake-timeout=1000 \"$subfolder\" -p no:randomly -vvvv;\\\n    done\"\n\n    # Clean up again\n    bash -c 'tox -e dev.k8s.destroy.datasite.highlow'\n    bash -c \"k3d registry delete k3d-registry.localhost || true\"\n    bash -c \"docker volume rm k3d-${CLUSTER_NAME_HIGH}-images --force || true\"\n    bash -c \"docker volume rm k3d-${CLUSTER_NAME_LOW}-images --force || true\"\n\n\n[testenv:single_container.launch]\ndescription = Launch a single backend container using the dockerfile\nchangedir = {toxinidir}/packages\nsetenv =\n    N_CONSUMERS = {env:N_CONSUMERS:1}\n    SERVER_NAME = {env:SERVER_NAME:test_datasite_sc}\n    SERVER_TYPE = {env:SERVER_TYPE:datasite}\n    SERVER_PORT = {env:SERVER_PORT:8080}\nallowlist_externals =\n    bash\ncommands =\n    bash -c 'tox -e single_container.destroy'\n    bash -c 'docker build -f grid/backend/backend.dockerfile . -t openmined/syft-backend:local-dev'\n    bash -c 'docker run -d \\\n    -e SERVER_NAME=${SERVER_NAME} \\\n    -e SERVER_TYPE=${SERVER_TYPE} \\\n    -e N_CONSUMERS=${N_CONSUMERS} \\\n    -e SINGLE_CONTAINER_MODE=true \\\n    -e CREATE_PRODUCER=true \\\n    -e INMEMORY_WORKERS=true \\\n    -p ${SERVER_PORT}:80 --add-host=host.docker.internal:host-gateway \\\n    --name ${SERVER_NAME} openmined/syft-backend:local-dev'\n\n[testenv:single_container.destroy]\ndescription = Destroy the single backend container run using single_container.launch\nchangedir = {toxinidir}/packages\nsetenv =\n    SERVER_NAME = {env:SERVER_NAME:test_datasite_sc}\nallowlist_externals =\n    bash\ncommands =\n    # Image is not cleaned up\n    bash -c 'docker stop ${SERVER_NAME} || true'\n    bash -c 'docker rm ${SERVER_NAME} || true'\n\n[testenv:stack.test.notebook]\ndescription = Stack Notebook Tests\ndeps =\n    {[testenv:syft]deps}\n    nbmake\nchangedir = {toxinidir}/notebooks\nallowlist_externals =\n    bash\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}\n    DEV_MODE = {env:DEV_MODE:True}\n    TEST_NOTEBOOK_PATHS = {env:TEST_NOTEBOOK_PATHS:api/0.8}\n    ENABLE_SIGNUP=True\n    SERVER_URL = {env:SERVER_URL:http://localhost}\n    SERVER_PORT = {env:SERVER_PORT:8080}\ncommands =\n\n    # Volume cleanup\n    bash -c 'docker volume rm -f $(docker volume ls -q --filter \"label=orgs.openmined.syft\") || true'\n    bash -c 'docker volume rm -f $(docker volume ls -q --filter \"label=com.docker.volume.anonymous\") || true'\n    bash -c 'docker network rm -f $(docker network ls -q --filter \"label=orgs.openmined.syft\") || true'\n\n    bash -c \"echo Running with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE DEV_MODE=$DEV_MODE TEST_NOTEBOOK_PATHS=$TEST_NOTEBOOK_PATHS; date\"\n    bash -c \"for subfolder in $(echo ${TEST_NOTEBOOK_PATHS} | tr ',' ' ');\\\n    do \\\n        pytest -x --nbmake --nbmake-timeout=1000 \"$subfolder\" -p no:randomly -vvvv -k 'not 11-container-images-k8s.ipynb';\\\n    done\"\n\n    ; pytest -x --nbmake --nbmake-timeout=1000 api/0.8 -p no:randomly -vvvv\n    ; pytest -x --nbmake --nbmake-timeout=1000 api/0.9 -p no:randomly -vvvv\n    ; pytest -x --nbmake --nbmake-timeout=1000 tutorials -p no:randomly -vvvv\n    ; pytest -x --nbmake --nbmake-timeout=1000 tutorials/pandas-cookbook -p no:randomly -vvvv\n\n    bash -c 'docker volume rm -f $(docker volume ls -q --filter \"label=orgs.openmined.syft\") || true'\n    bash -c 'docker volume rm -f $(docker volume ls -q --filter \"label=com.docker.volume.anonymous\") || true'\n    bash -c 'docker network rm -f $(docker network ls -q --filter \"label=orgs.openmined.syft\") || true'\n\n\n\n[testenv:frontend.generate.types]\ndescription = Generate Types for Frontend\ndeps =\n    {[testenv:syft]deps}\nallowlist_externals =\n    cd\n    bash\n    pnpm\nchangedir = {toxinidir}/packages/grid/frontend\npassenv =\n    PNPM_HOME\ncommands =\n    bash -c ./scripts/check_pnpm.sh\n    pnpm add -g json-schema-to-typescript\n\n    ; clear the old ones\n    bash -c 'rm -rf ./schema'\n    bash -c 'rm -rf ./src/types/generated'\n\n    ; generate new ones\n    bash -c 'python3 -c \"import syft as sy;sy.util.schema.generate_json_schemas()\"'\n    bash -c \"json2ts -i './schema/**/*.json' -o ./src/types/generated\"\n    bash -c \"python3 ./scripts/replace_imports.py ./src/types/generated\"\n\n[mypy]\npython_version = 3.12\ndisable_error_code = attr-defined, valid-type, no-untyped-call, arg-type\n\n[testenv:syft.test.integration]\ndescription = Integration Tests for Syft Stack\ndeps =\n    {[testenv:syft]deps}\n    pytest-asyncio\nchangedir = {toxinidir}\npassenv=HOME, USER\nallowlist_externals =\n    bash\nsetenv =\n    PYTEST_MODULES = {env:PYTEST_MODULES:asyncio local_server}\n    ASSOCIATION_REQUEST_AUTO_APPROVAL = {env:ASSOCIATION_REQUEST_AUTO_APPROVAL:true}\n    PYTEST_FLAGS = {env:PYTEST_FLAGS:--ignore=tests/integration/local/job_test.py}\ncommands =\n    python -c 'import syft as sy; sy.stage_protocol_changes()'\n\n    # Run Integration Tests\n    bash -c '\\\n        PYTEST_MODULES=($PYTEST_MODULES); \\\n        for i in \"${PYTEST_MODULES[@]}\"; do \\\n            echo \"Starting test for $i\"; date; \\\n            pytest tests/integration -m $i -vvvv -p no:randomly -p no:benchmark -o log_cli=True --capture=no $PYTEST_FLAGS; \\\n            return=$?; \\\n            echo \"Finished $i\"; \\\n            date; \\\n            if [[ $return -ne 0 ]]; then \\\n                exit $return; \\\n            fi; \\\n        done'\n\n[testenv:stack.test.integration.k8s]\ndescription = Integration Tests for Core Stack using K8s\ndeps =\n    {[testenv:syft]deps}\n    pytest-asyncio\nchangedir = {toxinidir}\npassenv=HOME, USER, AZURE_BLOB_STORAGE_KEY\nallowlist_externals =\n    devspace\n    kubectl\n    grep\n    sleep\n    bash\n    kubectx\n    k3d\n    echo\n    tox\nsetenv =\n    SERVER_PORT = {env:SERVER_PORT:9082}\n    GITHUB_CI = {env:GITHUB_CI:false}\n    PYTEST_MODULES = {env:PYTEST_MODULES:frontend network container_workload}\n    DATASITE_CLUSTER_NAME = {env:DATASITE_CLUSTER_NAME:test-datasite-1}\n    GATEWAY_CLUSTER_NAME = {env:GATEWAY_CLUSTER_NAME:test-gateway-1}\n    ASSOCIATION_REQUEST_AUTO_APPROVAL = {env:ASSOCIATION_REQUEST_AUTO_APPROVAL:true}\n    SYFT_BASE_IMAGE_REGISTRY = {env:SYFT_BASE_IMAGE_REGISTRY:k3d-registry.localhost:5800}\ncommands =\n    bash -c \"echo Running with GITHUB_CI=$GITHUB_CI; date\"\n    python -c 'import syft as sy; sy.stage_protocol_changes()'\n    k3d version\n\n    # Deleting Old Cluster\n    bash -c \"k3d cluster delete ${DATASITE_CLUSTER_NAME} || true\"\n    bash -c \"k3d cluster delete ${GATEWAY_CLUSTER_NAME} || true\"\n\n    # Deleting registry & volumes\n    bash -c \"k3d registry delete k3d-registry.localhost || true\"\n    bash -c \"docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true\"\n    bash -c \"docker volume rm k3d-${GATEWAY_CLUSTER_NAME}-images --force || true\"\n\n    # Create registry\n    tox -e dev.k8s.registry\n\n    # Creating test-gateway-1 cluster on port 9081\n    bash -c '\\\n        export CLUSTER_NAME=${GATEWAY_CLUSTER_NAME} CLUSTER_HTTP_PORT=9081 DEVSPACE_PROFILE=gateway && \\\n        tox -e dev.k8s.start && \\\n            tox -e dev.k8s.deploy'\n\n    # Creating test-datasite-1 cluster on port 9082\n    bash -c '\\\n        export CLUSTER_NAME=${DATASITE_CLUSTER_NAME} CLUSTER_HTTP_PORT=9082 DEVSPACE_PROFILE=datasite-tunnel && \\\n        tox -e dev.k8s.start && \\\n        tox -e dev.k8s.deploy'\n\n    # free up build cache after build of images\n    bash -c 'if [[ \"$GITHUB_CI\" != \"false\" ]]; then \\\n        docker image prune --all --force; \\\n        docker builder prune --all --force; \\\n    fi'\n\n    sleep 30\n\n    # wait for test gateway 1\n    bash packages/grid/scripts/wait_for.sh service postgres --context k3d-{env:GATEWAY_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service backend --context k3d-{env:GATEWAY_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service proxy --context k3d-{env:GATEWAY_CLUSTER_NAME} --namespace syft\n\n    # wait for test datasite 1\n    bash packages/grid/scripts/wait_for.sh service postgres --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service backend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service proxy --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service seaweedfs --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service frontend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash -c '(kubectl logs service/frontend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q -E \"Network:\\s+https?://[a-zA-Z0-9.-]+:[0-9]+/\" || true'\n\n    # Checking logs generated & startup of test-datasite 1\n    bash -c '(kubectl logs service/backend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q \"Application startup complete\" || true'\n    # Checking logs generated & startup of testgateway1\n    bash -c '(kubectl logs service/backend --context k3d-${GATEWAY_CLUSTER_NAME} --namespace syft -f &) | grep -q \"Application startup complete\" || true'\n\n    # Run Integration Tests\n    bash -c '\\\n        PYTEST_MODULES=($PYTEST_MODULES); \\\n        for i in \"${PYTEST_MODULES[@]}\"; do \\\n            echo \"Starting test for $i\"; date; \\\n            pytest tests/integration -m $i -vvvv -p no:randomly -p no:benchmark -o log_cli=True --capture=no; \\\n            return=$?; \\\n            echo \"Finished $i\"; \\\n            date; \\\n            if [[ $return -ne 0 ]]; then \\\n                exit $return; \\\n            fi; \\\n        done'\n\n    # deleting clusters created\n    bash -c \"CLUSTER_NAME=${DATASITE_CLUSTER_NAME} tox -e dev.k8s.destroy || true\"\n    bash -c \"CLUSTER_NAME=${GATEWAY_CLUSTER_NAME} tox -e dev.k8s.destroy || true\"\n    bash -c \"k3d registry delete k3d-registry.localhost || true\"\n    bash -c \"docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true\"\n    bash -c \"docker volume rm k3d-${GATEWAY_CLUSTER_NAME}-images --force || true\"\n\n[testenv:stack.test.notebook.k8s]\ndescription = Notebook Tests for Core Stack using K8s\ndeps =\n    {[testenv:syft]deps}\n    nbmake\nchangedir = {toxinidir}\npassenv=HOME, USER\nallowlist_externals =\n    devspace\n    kubectl\n    grep\n    sleep\n    bash\n    k3d\n    echo\n    tox\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}\n    GITHUB_CI = {env:GITHUB_CI:false}\n    SYFT_BASE_IMAGE_REGISTRY = {env:SYFT_BASE_IMAGE_REGISTRY:k3d-registry.localhost:5800}\n    DATASITE_CLUSTER_NAME = {env:DATASITE_CLUSTER_NAME:test-datasite-1}\n    SERVER_URL = {env:SERVER_URL:http://localhost}\n    SERVER_PORT = {env:SERVER_PORT:8080}\ncommands =\n    bash -c \"echo Running with GITHUB_CI=$GITHUB_CI; date\"\n    python -c 'import syft as sy; sy.stage_protocol_changes()'\n    k3d version\n\n    # Deleting Old Cluster\n    bash -c \"k3d cluster delete ${DATASITE_CLUSTER_NAME} || true\"\n\n    # Deleting registry & volumes\n    bash -c \"k3d registry delete k3d-registry.localhost || true\"\n    bash -c \"docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true\"\n\n    # Create registry\n    tox -e dev.k8s.registry\n\n\n    # Creating test-datasite-1 cluster on port SERVER_PORT\n    bash -c '\\\n        export CLUSTER_NAME=${DATASITE_CLUSTER_NAME} CLUSTER_HTTP_PORT=${SERVER_PORT} && \\\n        tox -e dev.k8s.start && \\\n        tox -e dev.k8s.deploy'\n\n    # free up build cache after build of images\n    bash -c 'if [[ \"$GITHUB_CI\" != \"false\" ]]; then \\\n        docker image prune --all --force; \\\n        docker builder prune --all --force; \\\n    fi'\n\n    sleep 30\n\n    # wait for test-datasite-1\n    bash packages/grid/scripts/wait_for.sh service postgres --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service backend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service proxy --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service seaweedfs --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service frontend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash -c '(kubectl logs service/frontend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q -E \"Network:\\s+https?://[a-zA-Z0-9.-]+:[0-9]+/\" || true'\n\n    # Checking logs generated & startup of test-datasite 1\n    bash -c '(kubectl logs service/backend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q \"Application startup complete\" || true'\n\n    bash -c \"pytest -x --nbmake notebooks/api/0.8 -p no:randomly -k 'not 14-container-images.ipynb' -vvvv --nbmake-timeout=1000\"\n\n    # deleting clusters created\n    bash -c \"CLUSTER_NAME=${DATASITE_CLUSTER_NAME} tox -e dev.k8s.destroy || true\"\n    bash -c \"k3d registry delete k3d-registry.localhost || true\"\n    bash -c \"docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true\"\n\n\n[testenv:syft.build.helm]\ndescription = Build Helm Chart for Kubernetes\nchangedir = {toxinidir}\npassenv=HOME, USER\nallowlist_externals =\n    bash\n    helm\ncommands =\n    bash -c 'cd packages/grid/helm && \\\n        python3 generate_helm_notes.py ./syft/templates'\n\n    bash -c 'cd packages/grid/helm && \\\n        helm lint syft'\n\n\n[testenv:syft.lint.helm]\ndescription = Lint helm chart\nchangedir = {toxinidir}/packages/grid/helm\npassenv=HOME, USER\nallowlist_externals =\n    bash\ncommands =\n    bash -c 'kube-linter lint ./syft --config ./kubelinter-config.yaml'\n\n[testenv:syft.package.helm]\ndescription = Package Helm Chart for Kubernetes\ndeps =\nchangedir = {toxinidir}\npassenv=HOME, USER\nallowlist_externals =\n    bash\n    helm\ncommands =\n    bash -c 'cd packages/grid/helm && \\\n        helm lint syft'\n\n    bash -c 'cd packages/grid/helm/syft && \\\n        helm dependency update'\n\n    bash -c 'cd packages/grid/helm && \\\n        helm package syft --destination repo'\n\n    bash -c 'cd packages/grid/helm/repo && \\\n        helm repo index . --url https://openmined.github.io/PySyft/helm'\n\n\n[testenv:dev.k8s.ready]\ndescription = Check readiness of k8s deployement\nchangedir = {toxinidir}/packages/grid\nallowlist_externals =\n    bash\n    tox\n    curl\nsetenv =\n    CLUSTER_NAME = {env:CLUSTER_NAME:syft}\n    CLUSTER_HTTP_PORT = {env:SERVER_PORT:8080}\n; Usage for posargs: names of the relevant services among {frontend backend proxy postgres seaweedfs registry}\ncommands =\n    bash -c \"env; date; k3d version\"\n\n    # Frontend\n    bash -c \"if echo '{posargs}' | grep -q 'frontend'; then \\\n            echo 'Checking readiness of frontend'; \\\n            ./scripts/wait_for.sh service frontend --context k3d-$CLUSTER_NAME --namespace syft && \\\n            (kubectl logs service/frontend --context k3d-$CLUSTER_NAME --namespace syft -f &) | grep -q -E 'Network:\\s+https?://[a-zA-Z0-9.-]+:[0-9]+/' || true; \\\n            fi\"\n\n    # Backend\n    bash -c \"if echo '{posargs}' | grep -q 'backend'; then \\\n            echo 'Checking readiness of backend'; \\\n            ./scripts/wait_for.sh service backend --context k3d-$CLUSTER_NAME --namespace syft && \\\n            (kubectl logs service/backend --context k3d-$CLUSTER_NAME --namespace syft -f &) | grep -q 'Application startup complete' || true; \\\n            fi\"\n\n    # Postgres\n    bash -c \"if echo '{posargs}' | grep -q 'postgres'; then echo 'Checking readiness of Postgres'; ./scripts/wait_for.sh service postgres --context k3d-$CLUSTER_NAME --namespace syft; fi\"\n\n    # Proxy\n    bash -c \"if echo '{posargs}' | grep -q 'proxy'; then echo 'Checking readiness of proxy'; ./scripts/wait_for.sh service proxy --context k3d-$CLUSTER_NAME --namespace syft; fi\"\n\n    # Seaweedfs\n    bash -c \"if echo '{posargs}' | grep -q 'seaweedfs'; then echo 'Checking readiness of SeaweedFS'; ./scripts/wait_for.sh service seaweedfs --context k3d-$CLUSTER_NAME --namespace syft; fi\"\n\n    # Registry\n    bash -c \"if echo '{posargs}' | grep -q 'registry'; then echo 'Checking readiness of Registry'; ./scripts/wait_for.sh service registry --context k3d-$CLUSTER_NAME --namespace syft; fi\"\n\n    # Extra\n    bash -c \"curl http://localhost:${CLUSTER_HTTP_PORT}/api/v2/metadata\"\n\n\n[testenv:syft.test.helm]\ndescription = Test Helm Chart for Kubernetes\nchangedir = {toxinidir}/packages/grid\npassenv=HOME, USER, EXTERNAL_REGISTRY_USERNAME, EXTERNAL_REGISTRY_PASSWORD\nallowlist_externals =\n    bash\n    tox\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}\n    SERVER_PORT = {env:SERVER_PORT:8080}\n    SERVER_URL = {env:SERVER_URL:http://localhost}\n    EXCLUDE_NOTEBOOKS = {env:EXCLUDE_NOTEBOOKS:not 14-container-images.ipynb}\n    SYFT_VERSION = {env:SYFT_VERSION:local}\n    EXTERNAL_REGISTRY = {env:EXTERNAL_REGISTRY:k3d-registry.localhost:5800}\n    ; env vars for dev.k8s.start\n    CLUSTER_NAME = testdatasite\n    CLUSTER_HTTP_PORT = {env:SERVER_PORT:8080}\n; Usage for posargs: if you pass override to this tox command, then resourcesPreset will be overridden\ncommands =\n    bash -c \"env; date; k3d version\"\n\n    bash -c \"k3d cluster delete ${CLUSTER_NAME} || true\"\n\n    tox -e dev.k8s.start\n\n    bash -c 'if [[ $SYFT_VERSION == \"local\" ]]; then \\\n        echo \"Installing local helm charts\"; \\\n        if [[ \"{posargs}\" == \"override\" ]]; then \\\n            echo \"Overriding resourcesPreset\"; \\\n            helm install ${CLUSTER_NAME} ./helm/syft -f ./helm/examples/dev/base.yaml --kube-context k3d-${CLUSTER_NAME} --namespace syft --create-namespace --set server.resourcesPreset=null --set seaweedfs.resourcesPreset=null --set postgres.resourcesPreset=null --set registry.resourcesPreset=null --set proxy.resourcesPreset=null --set frontend.resourcesPreset=null; \\\n        else \\\n            helm install ${CLUSTER_NAME} ./helm/syft -f ./helm/examples/dev/base.yaml --kube-context k3d-${CLUSTER_NAME} --namespace syft --create-namespace; \\\n        fi \\\n    else \\\n        echo \"Installing helm charts from repo for syft version: ${SYFT_VERSION}\"; \\\n        helm repo add openmined https://openmined.github.io/PySyft/helm; \\\n        helm repo update openmined; \\\n        if [[ \"{posargs}\" == \"override\" ]]; then \\\n            echo \"Overriding resourcesPreset\"; \\\n            helm install ${CLUSTER_NAME} openmined/syft --version=${SYFT_VERSION} -f ./helm/examples/dev/base.yaml --kube-context k3d-${CLUSTER_NAME} --namespace syft --create-namespace --set server.resourcesPreset=null --set seaweedfs.resourcesPreset=null --set postgres.resourcesPreset=null --set registry.resourcesPreset=null --set proxy.resourcesPreset=null --set frontend.resourcesPreset=null; \\\n        else \\\n            helm install ${CLUSTER_NAME} openmined/syft --version=${SYFT_VERSION} -f ./helm/examples/dev/base.yaml --kube-context k3d-${CLUSTER_NAME} --namespace syft --create-namespace; \\\n        fi \\\n    fi'\n\n    ; wait for everything else to be loaded\n    tox -e dev.k8s.ready -- frontend backend postgres proxy seaweedfs registry\n\n    # Run Notebook tests\n    tox -e e2e.test.notebook\n\n    bash -c \"k3d cluster delete ${CLUSTER_NAME} || true\"\n\n[testenv:syft.test.helm.upgrade]\ndescription = Test helm upgrade\nchangedir = {toxinidir}/packages/grid/\npassenv=HOME,USER,KUBE_CONTEXT\nsetenv =\n    UPGRADE_TYPE = {env:UPGRADE_TYPE:ProdToBeta}\nallowlist_externals =\n    bash\ncommands =\n    bash ./scripts/helm_upgrade.sh {env:UPGRADE_TYPE}\n\n[testenv:syftcli.test.unit]\ndescription = Syft CLI Unit Tests\ndeps =\n    {[testenv:syftcli]deps}\nchangedir = {toxinidir}/packages/syftcli\nallowlist_externals =\n    uv\n    pytest\ncommands =\n    pytest\n\n[testenv:dev.k8s.registry]\ndescription = Start local Kubernetes registry with k3d\nchangedir = {toxinidir}\npassenv=HOME,USER\nallowlist_externals =\n    bash\n    sudo\ncommands =\n    ; check k3d version\n    bash -c 'k3d --version'\n\n    ; create registry\n    bash -c 'docker volume create k3d-registry-vol || true'\n    bash -c 'k3d registry create registry.localhost --port 5800 -v k3d-registry-vol:/var/lib/registry --no-help || true'\n\n    ; add patches to host\n    bash -c 'if ! grep -q k3d-registry.localhost /etc/hosts; then sudo {envpython} scripts/patch_hosts.py --add-k3d-registry --fix-docker-hosts; fi'\n\n    ; Fail this command if registry is not working\n    bash -c 'curl --retry 5 --retry-all-errors http://k3d-registry.localhost:5800/v2/_catalog'\n\n[testenv:dev.k8s.patch.coredns]\ndescription = Patch CoreDNS to resolve k3d-registry.localhost\nchangedir = {toxinidir}\npassenv=HOME,USER,CLUSTER_NAME\nsetenv=\n    CLUSTER_NAME = {env:CLUSTER_NAME:syft-dev}\nallowlist_externals =\n    bash\ncommands =\n    ; patch coredns so k3d-registry.localhost works in k3d\n    bash -c 'kubectl apply -f ./scripts/k8s-coredns-custom.yml --context k3d-${CLUSTER_NAME}'\n\n    ; restarts coredns\n    bash -c 'kubectl delete pod -n kube-system -l k8s-app=kube-dns --context k3d-${CLUSTER_NAME}'\n\n\n[testenv:dev.k8s.add.collector]\ndescription = Install signoz/k8s-infra on Kubernetes cluster\nchangedir = {toxinidir}\npassenv=HOME,USER,CLUSTER_NAME\nsetenv =\n    SIGNOZ_HOST=host.k3d.internal\n    CLUSTER_NAME = {env:CLUSTER_NAME:syft-dev}\nallowlist_externals =\n    helm\ncommands =\n    helm install k8s-infra k8s-infra \\\n        --repo https://charts.signoz.io \\\n        --kube-context k3d-{env:CLUSTER_NAME} \\\n        --set global.deploymentEnvironment=local \\\n        --set clusterName={env:CLUSTER_NAME} \\\n        --set otelCollectorEndpoint=http://{env:SIGNOZ_HOST}:4317 \\\n        --set otelInsecure=true \\\n        --set presets.otlpExporter.enabled=true \\\n        --set presets.loggingExporter.enabled=true\n\n[testenv:dev.k8s.start]\ndescription = Start local Kubernetes registry & cluster with k3d\nchangedir = {toxinidir}\npassenv = HOME, USER\nsetenv =\n    CLUSTER_NAME = {env:CLUSTER_NAME:syft-dev}\n    CLUSTER_HTTP_PORT = {env:CLUSTER_HTTP_PORT:8080}\n    # SIGNOZ_PORT = {env:SIGNOZ_PORT:3301}\nallowlist_externals =\n    bash\n    sleep\n    tox\ncommands =\n    ; start registry\n    tox -e dev.k8s.registry\n\n    ; for NodePort to work add the following --> -p \"NodePort:NodePort@loadbalancer\"\n    bash -c 'k3d cluster create ${CLUSTER_NAME} \\\n            -p \"${CLUSTER_HTTP_PORT}:80@loadbalancer\" \\\n            --registry-use k3d-registry.localhost:5800 {posargs} && \\\n            kubectl --context k3d-${CLUSTER_NAME} create namespace syft || true'\n\n    ; patch coredns\n    tox -e dev.k8s.patch.coredns\n\n    ; add signoz/collector\n    tox -e dev.k8s.add.collector\n\n    ; dump cluster info\n    tox -e dev.k8s.info\n\n[testenv:dev.k8s.deploy]\ndescription = Deploy Syft to a local Kubernetes cluster with Devspace\nchangedir = {toxinidir}/packages/grid\npassenv = HOME, USER, DEVSPACE_PROFILE\nsetenv=\n    CLUSTER_NAME = {env:CLUSTER_NAME:syft-dev}\n    TRACING = {env:TRACING:False}\nallowlist_externals =\n    bash\ncommands =\n    ; deploy syft helm charts\n    bash -c 'echo \"profile=$DEVSPACE_PROFILE\"'\n    bash -c \"echo Running with TRACING=$TRACING; date\"\n    bash -c '\\\n        if [[ -n \"${DEVSPACE_PROFILE}\" ]]; then export DEVSPACE_PROFILE=\"-p ${DEVSPACE_PROFILE}\"; fi && \\\n        if [[ \"${TRACING}\" == \"True\" ]]; then DEVSPACE_PROFILE=\"${DEVSPACE_PROFILE} -p tracing\"; fi && \\\n        if [[ \"${TRACING}\" == \"True\" ]]; then echo \"TRACING PROFILE ENABLED\"; fi && \\\n        devspace deploy -b --kube-context k3d-${CLUSTER_NAME} --no-warn ${DEVSPACE_PROFILE} --namespace syft --var CONTAINER_REGISTRY=k3d-registry.localhost:5800'\n\n    # if TRACING is enabled start signoz\n    ; bash -c 'if [[ \"${TRACING}\" == \"True\" ]]; then tox -e dev.k8s.install.signoz; fi'\n\n[testenv:dev.k8s.hotreload]\ndescription = Start development with hot-reload in Kubernetes\nchangedir = {toxinidir}/packages/grid\npassenv = HOME, USER, DEVSPACE_PROFILE\nsetenv=\n    CLUSTER_NAME = {env:CLUSTER_NAME:syft-dev}\nallowlist_externals =\n    bash\ncommands =\n    ; deploy syft helm charts with hot-reload\n    bash -c '\\\n        if [[ -n \"${DEVSPACE_PROFILE}\" ]]; then export DEVSPACE_PROFILE=\"-p ${DEVSPACE_PROFILE}\"; fi && \\\n        devspace dev --kube-context k3d-${CLUSTER_NAME} --no-warn ${DEVSPACE_PROFILE} --namespace syft --var CONTAINER_REGISTRY=k3d-registry.localhost:5800'\n\n[testenv:dev.k8s.info]\ndescription = Gather info about the localKubernetes cluster\npassenv = HOME, USER\nignore_errors = True\nallowlist_externals =\n    k3d\n    kubectl\ncommands =\n    k3d cluster list\n    kubectl cluster-info\n    kubectl config current-context\n    kubectl get namespaces\n\n[testenv:dev.k8s.cleanup]\ndescription = Cleanup Syft deployment and associated resources, but keep the cluster running\nchangedir = {toxinidir}/packages/grid\npassenv=HOME, USER\nsetenv=\n    CLUSTER_NAME = {env:CLUSTER_NAME:syft-dev}\nallowlist_externals =\n    bash\ncommands =\n    bash -c 'devspace purge --force-purge --kube-context k3d-${CLUSTER_NAME} --no-warn --namespace syft; sleep 3'\n    bash -c 'devspace cleanup images --kube-context k3d-${CLUSTER_NAME} --no-warn --namespace syft --var CONTAINER_REGISTRY=k3d-registry.localhost:5800 || true'\n    bash -c 'kubectl --context k3d-${CLUSTER_NAME} delete namespace syft --now=true || true'\n\n[testenv:dev.k8s.render]\ndescription = Dump devspace rendered chargs for debugging. Save in `packages/grid/out.render`\nchangedir = {toxinidir}/packages/grid\npassenv = HOME, USER, DEVSPACE_PROFILE\nsetenv=\n    OUTPUT_DIR = {env:OUTPUT_DIR:./.devspace/rendered}\nallowlist_externals =\n    bash\ncommands =\n    bash -c '\\\n        if [[ -n \"${DEVSPACE_PROFILE}\" ]]; then export DEVSPACE_PROFILE=\"-p ${DEVSPACE_PROFILE}\"; fi && \\\n        rm -rf ${OUTPUT_DIR} && \\\n        mkdir -p ${OUTPUT_DIR} && \\\n        echo \"profile: $DEVSPACE_PROFILE\" && \\\n        devspace print ${DEVSPACE_PROFILE} > ${OUTPUT_DIR}/config.txt && \\\n        devspace deploy --render --skip-build --no-warn ${DEVSPACE_PROFILE} --namespace syft --var CONTAINER_REGISTRY=k3d-registry.localhost:5800 > ${OUTPUT_DIR}/chart.yaml'\n\n[testenv:dev.k8s.launch.gateway]\ndescription = Launch a single gateway on K8s\npassenv = HOME, USER\nsetenv=\n    CLUSTER_NAME = {env:CLUSTER_NAME:test-gateway-1}\n    CLUSTER_HTTP_PORT={env:CLUSTER_HTTP_PORT:9081}\n    DEVSPACE_PROFILE=gateway\nallowlist_externals =\n    tox\ncommands =\n    tox -e dev.k8s.start\n    tox -e dev.k8s.{posargs:deploy}\n\n\n[testenv:dev.k8s.launch.datasite.highlow]\ndescription = Launch a high and a low side datasite on K8s\npassenv = HOME, USER, DEVSPACE_PROFILE\nsetenv=\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}\n    CLUSTER_NAME_HIGH = {env:CLUSTER_NAME_HIGH:test-datasite-high-1}\n    CLUSTER_NAME_LOW = {env:CLUSTER_NAME_LOW:test-datasite-low-1}\n    CLUSTER_HTTP_PORT_HIGH={env:CLUSTER_HTTP_PORT_HIGH:9081}\n    CLUSTER_HTTP_PORT_LOW={env:CLUSTER_HTTP_PORT_LOW:9083}\n    SYFT_BASE_IMAGE_REGISTRY = {env:SYFT_BASE_IMAGE_REGISTRY:k3d-registry.localhost:5800}\n    TEST_EXTERNAL_REGISTRY = {env:TEST_EXTERNAL_REGISTRY:k3d-registry.localhost:5800}\n    DEVSPACE_PROFILE={env:DEVSPACE_PROFILE}\nallowlist_externals =\n    tox\n    echo\n    bash\ncommands =\n    bash -c 'echo USING DEVSPACE PROFILE: $DEVSPACE_PROFILE'\n    bash -c 'echo \"Launching high datasite: $CLUSTER_NAME_HIGH\" && \\\n    CLUSTER_NAME=$CLUSTER_NAME_HIGH CLUSTER_HTTP_PORT=$CLUSTER_HTTP_PORT_HIGH \\\n    tox -e dev.k8s.launch.datasite'\n\n    bash -c 'packages/grid/scripts/wait_for.sh service backend --context k3d-{env:CLUSTER_NAME_HIGH} --namespace syft'\n\n    bash -c 'echo \"Launching low datasite: $CLUSTER_NAME_LOW\" &&  \\\n    CLUSTER_NAME=$CLUSTER_NAME_LOW CLUSTER_HTTP_PORT=$CLUSTER_HTTP_PORT_LOW \\\n    DEVSPACE_PROFILE=\"$DEVSPACE_PROFILE -p datasite-low\" tox -e dev.k8s.launch.datasite'\n\n    bash -c 'echo \"Waiting for services to be ready\"'\n\n    bash -c 'CLUSTER_NAME=$CLUSTER_NAME_HIGH CLUSTER_HTTP_PORT=$CLUSTER_HTTP_PORT_HIGH source ./scripts/get_k8s_secret_ci.sh \\\n        && CLUSTER_NAME=$CLUSTER_NAME_HIGH CLUSTER_HTTP_PORT=$CLUSTER_HTTP_PORT_HIGH ./scripts/display_credentials.sh'\n\n    bash -c 'packages/grid/scripts/wait_for.sh service backend --context k3d-{env:CLUSTER_NAME_LOW} --namespace syft'\n\n    bash -c 'CLUSTER_NAME=$CLUSTER_NAME_LOW CLUSTER_HTTP_PORT=$CLUSTER_HTTP_PORT_LOW source ./scripts/get_k8s_secret_ci.sh \\\n        && CLUSTER_NAME=$CLUSTER_NAME_LOW CLUSTER_HTTP_PORT=$CLUSTER_HTTP_PORT_LOW ./scripts/display_credentials.sh'\n\n\n[testenv:dev.k8s.destroy.datasite.highlow]\ndescription = Destroy a high and a low side datasite on K8s\npassenv = HOME, USER\nsetenv=\n    CLUSTER_NAME_HIGH = {env:CLUSTER_NAME_HIGH:test-datasite-high-1}\n    CLUSTER_NAME_LOW = {env:CLUSTER_NAME_LOW:test-datasite-low-1}\nallowlist_externals =\n    tox\n    echo\n    bash\ncommands =\n    bash -c 'echo \"Destroying high datasite: $CLUSTER_NAME_HIGH\" && \\\n    CLUSTER_NAME=$CLUSTER_NAME_HIGH tox -e dev.k8s.destroy'\n\n    bash -c 'echo \"Destroying low datasite: $CLUSTER_NAME_LOW\" && \\\n    CLUSTER_NAME=$CLUSTER_NAME_LOW tox -e dev.k8s.destroy'\n\n\n[testenv:dev.k8s.launch.datasite]\ndescription = Launch a single datasite on K8s\npassenv = HOME, USER\nsetenv=\n    CLUSTER_NAME = {env:CLUSTER_NAME:test-datasite-1}\n    CLUSTER_HTTP_PORT={env:CLUSTER_HTTP_PORT:9082}\n    DEVSPACE_PROFILE={env:DEVSPACE_PROFILE}\nallowlist_externals =\n    tox\n    bash\ncommands =\n    bash -c \"CLUSTER_NAME=${CLUSTER_NAME} tox -e dev.k8s.destroy\"\n    tox -e dev.k8s.start\n    tox -e dev.k8s.{posargs:deploy}\n\n[testenv:dev.k8s.launch.enclave]\ndescription = Launch a single Enclave on K8s\npassenv = HOME, USER\nsetenv=\n    CLUSTER_NAME = {env:CLUSTER_NAME:test-enclave-1}\n    CLUSTER_HTTP_PORT={env:CLUSTER_HTTP_PORT:9083}\n    DEVSPACE_PROFILE=enclave\nallowlist_externals =\n    tox\ncommands =\n    tox -e dev.k8s.start -- --volume /sys/kernel/security:/sys/kernel/security --volume /dev/tpmrm0:/dev/tpmrm0\n    tox -e dev.k8s.{posargs:deploy}\n\n[testenv:dev.k8s.destroy]\ndescription = Destroy local Kubernetes cluster\nchangedir = {toxinidir}/packages/grid\npassenv = HOME, USER\nsetenv=\n    CLUSTER_NAME = {env:CLUSTER_NAME:syft-dev}\nallowlist_externals =\n    tox\n    bash\ncommands =\n    ; destroy cluster\n    bash -c '\\\n        rm -rf .devspace; echo \"\"; \\\n        k3d cluster delete ${CLUSTER_NAME};'\n\n[testenv:dev.k8s.destroyall]\ndescription = Destroy both local Kubernetes cluster and registry\nchangedir = {toxinidir}\npassenv = HOME, USER, CLUSTER_NAME\nignore_errors=True\nallowlist_externals =\n    bash\n    tox\ncommands =\n    ; destroy cluster\n    tox -e dev.k8s.destroy\n\n    ; destroy registry\n    bash -c 'k3d registry delete registry.localhost || true'\n    bash -c 'docker volume rm k3d-registry-vol --force || true'\n\n[testenv:backend.test.basecpu]\ndescription = Base CPU Docker Image Test\nchangedir = {toxinidir}/packages\nallowlist_externals =\n    docker\n    bash\n    env\nsetenv =\n    PIP_PACKAGES = {env:PIP_PACKAGES:llama-index opendp}\n    SYSTEM_PACKAGES = {env:SYSTEM_PACKAGES:curl wget}\n    BUILD_PLATFORM = {env:BUILD_PLATFORM:linux/amd64}\ncommands =\n    env\n\n    ; Build the base image\n    bash -c 'docker buildx build \\\n        --platform $BUILD_PLATFORM \\\n        -f ./grid/backend/grid/images/worker_cpu.dockerfile . \\\n        -t cpu-worker:latest'\n    bash -c 'docker rmi cpu-worker:latest'\n\n    bash -c '\\\n        docker buildx build \\\n            --platform $BUILD_PLATFORM \\\n            -f grid/backend/grid/images/worker_cpu.dockerfile . \\\n            -t cpu-worker:opendp \\\n            --build-arg PIP_PACKAGES=\"$PIP_PACKAGES\" \\\n            --build-arg SYSTEM_PACKAGES=\"$SYSTEM_PACKAGES\"'\n    ; bash -c 'for pkg in $PIP_PACKAGES; do docker run --rm cpu-worker:opendp pip list | grep $pkg || uv pip list | grep $pkg; done'\n    bash -c '\\\n        pip_pkgs=$(docker run --rm cpu-worker:opendp uv pip list || pip list); \\\n        for pkg in $PIP_PACKAGES; do echo $pip_pkgs | grep $pkg; done'\n    bash -c 'for pkg in $SYSTEM_PACKAGES; do docker run --rm cpu-worker:opendp apk -e info \"$pkg\"; done'\n    bash -c 'docker rmi cpu-worker:opendp'\n\n    bash -c '\\\n        docker buildx build \\\n            --platform $BUILD_PLATFORM \\\n            -f grid/backend/grid/images/worker_cpu.dockerfile . \\\n            -t cpu-worker:custom-cmd  \\\n            --build-arg SYSTEM_PACKAGES=\"perl wget curl make \" \\\n            --build-arg CUSTOM_CMD=\"\"\"wget -O - \"https://github.com/cowsay-org/cowsay/archive/refs/tags/v3.7.0.tar.gz\" | tar xvzf - && cd cowsay-3.7.0 && make\"\"\"'\n    bash -c 'for pkg in perl make curl wget; do docker run --rm cpu-worker:custom-cmd apk -e info \"$pkg\"; done'\n    bash -c 'docker run --rm cpu-worker:custom-cmd bash -c \"cd cowsay-3.7.0 && curl https://api.github.com/zen -s | ./cowsay\"'\n    bash -c 'docker rmi cpu-worker:custom-cmd'\n\n\n# There are other adjacent notebook tests like\n# stack.test.notebook, syft.test.notebook, etc.\n# They could be modularized and reused.\n# The below is the notebook test suite for point at external servers\n[testenv:e2e.test.notebook]\ndescription = E2E Notebook tests\nchangedir = {toxinidir}\ndeps =\n    {[testenv:syft]deps}\n    nbmake\nallowlist_externals =\n    bash\n    pytest\npassenv = EXTERNAL_REGISTRY,EXTERNAL_REGISTRY_USERNAME,EXTERNAL_REGISTRY_PASSWORD\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}\n    SERVER_PORT = {env:SERVER_PORT:8080}\n    SERVER_URL = {env:SERVER_URL:http://localhost}\n    EXCLUDE_NOTEBOOKS = {env:EXCLUDE_NOTEBOOKS:}\n    SYFT_VERSION = {env:SYFT_VERSION:local}\ncommands =\n    bash -c \"echo Running with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE SERVER_PORT=$SERVER_PORT SERVER_URL=$SERVER_URL \\\n    Excluding notebooks: $EXCLUDE_NOTEBOOKS SYFT_VERSION=$SYFT_VERSION \\\n    EXTERNAL_REGISTRY=$EXTERNAL_REGISTRY; date\"\n\n    # Schema for EXLUDE_NOTEBOOKS is\n    # for excluding\n    # notebook1.ipynb, notebook2.ipynb\n    # EXCLUDE_NOTEBOOKS=not notebook1.ipynb and not notebook2.ipynb\n\n    # If the syft version is local install the local version\n    # else install the version of syft specified\n    bash -c \"if [[ $SYFT_VERSION == 'local' ]]; then \\\n        echo 'Using local syft'; \\\n    else \\\n        echo 'Installing syft version: ${SYFT_VERSION}'; \\\n        uv pip install syft[data_science]==${SYFT_VERSION}; \\\n    fi\"\n\n    pytest -x --nbmake --nbmake-timeout=1000 notebooks/api/0.8 -p no:randomly -vvvv -k '{env:EXCLUDE_NOTEBOOKS:}'\n\n[testenv:seaweedfs.test.unit]\ndescription = Seaweedfs Unit Tests\ndeps =\n    -r{toxinidir}/packages/grid/seaweedfs/requirements.txt\n    -r{toxinidir}/packages/grid/seaweedfs/requirements.dev.txt\nchangedir = {toxinidir}/packages/grid/seaweedfs\nallowlist_externals =\n    bash\n    pytest\ncommands =\n    bash -c 'ulimit -n 4096 || true'\n    pytest --disable-warnings\n\n[testenv:migration.prepare]\ndescription = Prepare Migration Data\npip_pre = True\nsetenv =\n    MIGRATION_DATA_DIR = {env:MIGRATION_DATA_DIR:{temp_dir}/migration}\ndeps =\n    nbmake\n    requests\n    syft==0.9.2\nallowlist_externals =\n    bash\n    python\ncommands =\n    ; Run notebooks to prepare migration data\n    bash -c 'python -c \"import syft as sy; print(\\\"Migrating from syft version:\\\", sy.__version__)\"'\n    pytest -x --nbmake --nbmake-timeout=1000 notebooks/tutorials/version-upgrades/0-prepare-migration-data.ipynb -vvvv\n    pytest -x --nbmake --nbmake-timeout=1000 notebooks/tutorials/version-upgrades/1-dump-database-to-file.ipynb -vvvv\n    bash -c 'ls -l ${MIGRATION_DATA_DIR}/migration.blob'\n    bash -c 'ls -l ${MIGRATION_DATA_DIR}/migration.yaml'\n    bash -c \"echo 'Migration data prepared in ${MIGRATION_DATA_DIR}'\"\n\n[testenv:migration.test]\ndescription = Migration Test\nsetenv =\n    MIGRATION_DATA_DIR = {env:MIGRATION_DATA_DIR:{temp_dir}/migration}\ndeps =\n    -e{toxinidir}/packages/syft[dev]\n    nbmake\n; changedir = {toxinidir}/packages/syft\nallowlist_externals =\n    bash\n    tox\n    pytest\ncommands =\n    tox -e migration.prepare\n    bash -c 'python -c \"import syft as sy; print(\\\"Migrating to syft version:\\\", sy.__version__)\"'\n    pytest -x --nbmake --nbmake-timeout=1000 notebooks/tutorials/version-upgrades/2-migrate-from-file.ipynb -vvvv\ncommands_post =\n    bash -c 'rm -f ${MIGRATION_DATA_DIR}/migration.blob'\n    bash -c 'rm -f ${MIGRATION_DATA_DIR}/migration.yaml'\n\n[testenv:migration.k8s.prepare]\ndescription = Prepare migration data using a k8s cluster\nchangedir = {toxinidir}\npassenv=HOME, USER, EXTERNAL_REGISTRY_USERNAME, EXTERNAL_REGISTRY_PASSWORD\ndeps =\n    requests\n    nbmake\n    syft==0.9.1\nallowlist_externals =\n    bash\n    tox\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}\n    SERVER_PORT = {env:SERVER_PORT:8080}\n    SERVER_URL = {env:SERVER_URL:http://localhost}\n    EXTERNAL_REGISTRY = {env:EXTERNAL_REGISTRY:k3d-registry.localhost:5800}\n    ; env vars for dev.k8s.start\n    CLUSTER_NAME = syft-migration-source\n    CLUSTER_HTTP_PORT = {env:SERVER_PORT:8080}\n    MIGRATION_DATA_DIR = {env:MIGRATION_DATA_DIR:{temp_dir}/migration}\n    LATEST_SYFT_VERSION = 0.9.1\ncommands =\n    bash -c \"env; date; k3d version\"\n    bash -c \"k3d cluster delete ${CLUSTER_NAME} || true\"\n\n    tox -e dev.k8s.start\n\n    # Deploy cluster from latest stable syft version with Helm\n    bash -c '\\\n        echo \"Installing helm charts from repo for syft version: ${LATEST_SYFT_VERSION}\"; \\\n        helm repo add openmined https://openmined.github.io/PySyft/helm; \\\n        helm repo update openmined; \\\n        helm install ${CLUSTER_NAME} openmined/syft --version ${LATEST_SYFT_VERSION} -f ./packages/grid/helm/examples/dev/base.yaml --kube-context k3d-${CLUSTER_NAME} --namespace syft --create-namespace; \\\n    '\n\n    ; wait for everything else to be loaded\n    tox -e dev.k8s.ready -- frontend backend mongo proxy seaweedfs registry\n\n    bash -c 'python -c \"import syft as sy; print(\\\"Migrating from syft version:\\\", sy.__version__)\"'\n\n    ; Run notebooks to prepare migration data\n    pytest -x --nbmake --nbmake-timeout=1000 notebooks/tutorials/version-upgrades/0-prepare-migration-data.ipynb -vvvv\n    pytest -x --nbmake --nbmake-timeout=1000 notebooks/tutorials/version-upgrades/1-dump-database-to-file.ipynb -vvvv\n    bash -c 'ls -l ${MIGRATION_DATA_DIR}/migration.blob'\n    bash -c 'ls -l ${MIGRATION_DATA_DIR}/migration.yaml'\n    bash -c \"echo 'Migration data prepared in ${MIGRATION_DATA_DIR}'\"\n\ncommands_post =\n    bash -c \"k3d cluster delete ${CLUSTER_NAME} || true\"\n\n\n[testenv:migration.k8s.test]\ndescription = Migration Test on K8s\nsetenv =\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}\n    GITHUB_CI = {env:GITHUB_CI:false}\n    SYFT_BASE_IMAGE_REGISTRY = {env:SYFT_BASE_IMAGE_REGISTRY:k3d-registry.localhost:5800}\n    DATASITE_CLUSTER_NAME = {env:DATASITE_CLUSTER_NAME:test-datasite-1}\n    SERVER_PORT = {env:SERVER_PORT:8081}\n    MIGRATION_DATA_DIR = {env:MIGRATION_DATA_DIR:{temp_dir}/migration}\n    TEST_EXTERNAL_REGISTRY = {env:TEST_EXTERNAL_REGISTRY:k3d-registry.localhost:5800}\ndeps =\n    {[testenv:syft]deps}\n    nbmake\nchangedir = {toxinidir}\npassenv=HOME, USER\nallowlist_externals =\n    bash\n    tox\n    pytest\n    devspace\n    kubectl\n    grep\n    sleep\n    k3d\n    echo\ncommands =\n    # create migration data files on previous syft version\n    tox -e migration.k8s.prepare\n\n    # Make migration.yaml available for devspace migration\n    bash -c 'cp ${MIGRATION_DATA_DIR}/migration.yaml packages/grid/helm/examples/dev/migration.yaml'\n\n    # Start the new cluster on syft version we're migrating to\n    # set env variable for orchestra deployment type\n    bash -c \"echo Running with GITHUB_CI=$GITHUB_CI; date\"\n    python -c 'import syft as sy; sy.stage_protocol_changes()'\n    k3d version\n\n    # Deleting Old Cluster\n    bash -c \"k3d cluster delete ${DATASITE_CLUSTER_NAME} || true\"\n\n    # Deleting registry & volumes\n    bash -c \"k3d registry delete k3d-registry.localhost || true\"\n    bash -c \"docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true\"\n\n    # Create registry\n    tox -e dev.k8s.registry\n\n    # Creating test-datasite-1 cluster on port SERVER_PORT\n    # NOTE set DEVSPACE_PROFILE=migrated-datasite will start the cluster with variables from migration.yaml\n    bash -c '\\\n    export CLUSTER_NAME=${DATASITE_CLUSTER_NAME} \\\n    CLUSTER_HTTP_PORT=${SERVER_PORT} \\\n    DEVSPACE_PROFILE=migrated-datasite && \\\n    tox -e dev.k8s.start && \\\n    tox -e dev.k8s.deploy'\n\n    # free up build cache after build of images\n    bash -c 'if [[ \"$GITHUB_CI\" != \"false\" ]]; then \\\n        docker image prune --all --force; \\\n        docker builder prune --all --force; \\\n    fi'\n\n    sleep 30\n\n    ; # wait for test-datasite-1\n    bash packages/grid/scripts/wait_for.sh service postgres --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service backend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service proxy --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service seaweedfs --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash packages/grid/scripts/wait_for.sh service frontend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft\n    bash -c '(kubectl logs service/frontend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q -E \"Network:\\s+https?://[a-zA-Z0-9.-]+:[0-9]+/\" || true'\n\n    ; # Checking logs generated & startup of test-datasite 1\n    bash -c '(kubectl logs service/backend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q \"Application startup complete\" || true'\n\n    # Run migration tests\n    bash -c 'python -c \"import syft as sy; print(\\\"Migrating to syft version:\\\", sy.__version__)\"'\n    pytest -x --nbmake --nbmake-timeout=1000 notebooks/tutorials/version-upgrades/2-migrate-from-file.ipynb -vvvv\n\ncommands_post =\n    bash -c \"CLUSTER_NAME=${DATASITE_CLUSTER_NAME} tox -e dev.k8s.destroy || true\"\n    bash -c 'rm -f ${MIGRATION_DATA_DIR}/migration.blob'\n    bash -c 'rm -f ${MIGRATION_DATA_DIR}/migration.yaml'\n\n[testenv:migration.scenarios.prepare]\ndescription = Prepare Migration Data\npip_pre = True\nsetenv =\n    MIGRATION_DATA_DIR = {env:MIGRATION_DATA_DIR:notebooks/scenarios/bigquery/upgradability}\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:python}\n    DEV_MODE = {env:DEV_MODE:True}\n    TEST_NOTEBOOK_PATHS = {env:TEST_NOTEBOOK_PATHS:notebooks/scenarios/bigquery/upgradability/0.9.1_notebooks}\n    TEST_query_limit_size={env:test_query_limit_size:500000}\n    SERVER_URL = {env:SERVER_URL:http://localhost}\n    SERVER_PORT = {env:SERVER_PORT:8080}\n    NUM_TEST_USERS = {env:NUM_TEST_USERS:5}\n    NUM_TEST_JOBS = {env:NUM_TEST_JOBS:10}\ndeps =\n    nbmake\n    requests\n    syft[dev,datascience]==0.9.1\n    ; {[testenv:syft]deps}\n    db-dtypes\n    google-cloud-bigquery\n    aiosmtpd\nallowlist_externals =\n    bash\n    python\ncommands =\n    ; Run notebooks to prepare migration data\n    bash -c 'pwd'\n    bash -c 'python -c \"import syft as sy; print(\\\"Migrating from syft version:\\\", sy.__version__)\"'\n    bash -c \"echo Running with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE DEV_MODE=$DEV_MODE TEST_NOTEBOOK_PATHS=$TEST_NOTEBOOK_PATHS; date\"\n\n    bash -c \"for subfolder in $(echo ${TEST_NOTEBOOK_PATHS} | tr ',' ' ');\\\n    do \\\n        pytest -s -x --nbmake --nbmake-timeout=1000 \"$subfolder\" --ignore=notebooks/scenarios/bigquery/sync -p no:randomly -vvvv --log-cli-level=DEBUG --capture=no;\\\n    done\"\n\n    pytest -x --nbmake --nbmake-timeout=1000 notebooks/scenarios/bigquery/upgradability/1-dump-database-to-file.ipynb -vvvv\n    bash -c 'ls -l ${MIGRATION_DATA_DIR}/migration.blob'\n    bash -c 'ls -l ${MIGRATION_DATA_DIR}/migration.yaml'\n    bash -c \"echo 'Migration data prepared in ${MIGRATION_DATA_DIR}'\"\n\n[testenv:migration.scenarios.k8s.prepare]\ndescription = Prepare Migration Data\npip_pre = True\nsetenv =\n    MIGRATION_DATA_DIR = {env:MIGRATION_DATA_DIR:notebooks/scenarios/bigquery/upgradability/test}\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}\n    EXTERNAL_REGISTRY = {env:EXTERNAL_REGISTRY:k3d-registry.localhost:5800}\n    TEST_EXTERNAL_REGISTRY = {env:TEST_EXTERNAL_REGISTRY:k3d-registry.localhost:5800}\n    DEV_MODE = {env:DEV_MODE:True}\n    TEST_NOTEBOOK_PATHS = {env:TEST_NOTEBOOK_PATHS:notebooks/scenarios/bigquery/upgradability/0.9.1_notebooks}\n    TEST_query_limit_size={env:test_query_limit_size:500000}\n    SERVER_URL = {env:SERVER_URL:http://localhost}\n    SERVER_PORT = {env:SERVER_PORT:8080}\n    NUM_TEST_USERS = {env:NUM_TEST_USERS:5}\n    NUM_TEST_JOBS = {env:NUM_TEST_JOBS:10}\ndeps =\n    nbmake\n    requests\n    syft[dev,datascience]==0.9.1\n    ; {[testenv:syft]deps}\n    db-dtypes\n    google-cloud-bigquery\n    aiosmtpd\nallowlist_externals =\n    bash\n    python\ncommands =\n    ; Run notebooks to prepare migration data\n    bash -c 'pwd'\n    bash -c 'python -c \"import syft as sy; print(\\\"Migrating from syft version:\\\", sy.__version__)\"'\n    bash -c \"echo Running with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE DEV_MODE=$DEV_MODE TEST_NOTEBOOK_PATHS=$TEST_NOTEBOOK_PATHS; date\"\n\n    bash -c \"for subfolder in $(echo ${TEST_NOTEBOOK_PATHS} | tr ',' ' ');\\\n    do \\\n        pytest -s -x --nbmake --nbmake-timeout=1000 \"$subfolder\" --ignore=notebooks/scenarios/bigquery/sync -p no:randomly -vvvv --log-cli-level=DEBUG --capture=no;\\\n    done\"\n\n    pytest -x --nbmake --nbmake-timeout=1000 notebooks/scenarios/bigquery/upgradability/1-dump-database-to-file.ipynb -vvvv\n    bash -c 'ls -l ${MIGRATION_DATA_DIR}/migration.blob'\n    bash -c 'ls -l ${MIGRATION_DATA_DIR}/migration.yaml'\n    bash -c \"echo 'Migration data prepared in ${MIGRATION_DATA_DIR}'\"\n\n[testenv:migration.scenarios.test]\ndescription = Migration Test\nsetenv =\n    MIGRATION_DATA_DIR = {env:MIGRATION_DATA_DIR:.}\n    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:python}\n    DEV_MODE = {env:DEV_MODE:True}\n    TEST_NOTEBOOK_PATHS = {env:TEST_NOTEBOOK_PATHS:notebooks/scenarios/bigquery/upgradability/0.9.1_notebooks}\n    TEST_query_limit_size={env:test_query_limit_size:500000}\n    SERVER_URL = {env:SERVER_URL:http://localhost}\n    SERVER_PORT = {env:SERVER_PORT:8080}\n    NUM_TEST_USERS = {env:NUM_TEST_USERS:5}\n    NUM_TEST_JOBS = {env:NUM_TEST_JOBS:10}\ndeps =\n    -e{toxinidir}/packages/syft[dev]\n    nbmake\n    db-dtypes\n    google-cloud-bigquery\n    aiosmtpd\nchangedir = {toxinidir}/notebooks\nallowlist_externals =\n    bash\n    tox\n    pytest\ncommands =\n    ; tox -e migration.prepare\n    bash -c 'python -c \"import syft as sy; print(\\\"Migrating to syft version:\\\", sy.__version__)\"'\n    pytest -x --nbmake --nbmake-timeout=1000 scenarios/bigquery/upgradability/2-migrate-for-scenarios.ipynb -vvvv --log-cli-level=DEBUG\n"
        }
      ]
    }
  ]
}