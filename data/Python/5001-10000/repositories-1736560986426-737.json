{
  "metadata": {
    "timestamp": 1736560986426,
    "page": 737,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "karpathy/neuraltalk",
      "stars": 5413,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0126953125,
          "content": "*.pyc\ncv/*.p\n"
        },
        {
          "name": "Readme.md",
          "type": "blob",
          "size": 7.576171875,
          "content": "#NeuralTalk\n\n**Warning: Deprecated.**\nHi there, this code is now quite old and inefficient, and now deprecated. I am leaving it on Github for educational purposes, but if you would like to run or train image captioning I warmly recommend my new code release [NeuralTalk2](https://github.com/karpathy/neuraltalk2). NeuralTalk2 is written in [Torch](http://torch.ch/) and is SIGNIFICANTLY (I mean, ~100x+) faster because it is batched and runs on the GPU. It also supports CNN finetuning, which helps a lot with performance.\n\n\nThis project contains *Python+numpy* source code for learning **Multimodal Recurrent Neural Networks** that describe images with sentences.\n\nThis line of work was recently featured in a [New York Times article](http://www.nytimes.com/2014/11/18/science/researchers-announce-breakthrough-in-content-recognition-software.html) and has been the subject of multiple academic papers from the research community over the last few months. This code currently implements the models proposed by [Vinyals et al. from Google (CNN + LSTM)](http://arxiv.org/abs/1411.4555) and by [Karpathy and Fei-Fei from Stanford (CNN + RNN)](http://cs.stanford.edu/people/karpathy/deepimagesent/). Both models take an image and predict its sentence description with a Recurrent Neural Network (either an LSTM or an RNN).\n\n## Overview\nThe pipeline for the project looks as follows:\n\n- The **input** is a dataset of images and 5 sentence descriptions that were collected with Amazon Mechanical Turk. In particular, this code base is set up for [Flickr8K](http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.html), [Flickr30K](http://shannon.cs.illinois.edu/DenotationGraph/), and [MSCOCO](http://mscoco.org/) datasets. \n- In the **training stage**, the images are fed as input to RNN and the RNN is asked to predict the words of the sentence, conditioned on the current word and previous context as mediated by the hidden layers of the neural network. In this stage, the parameters of the networks are trained with backpropagation.\n- In the **prediction stage**, a witheld set of images is passed to RNN and the RNN generates the sentence one word at a time. The results are evaluated with **BLEU score**. The code also includes utilities for visualizing the results in HTML.\n\n## Dependencies\n**Python 2.7**, modern version of **numpy/scipy**, **perl** (if you want to do BLEU score evaluation), **argparse** module. Most of these are okay to install with **pip**. To install all dependencies at once, run the command `pip install -r requirements.txt`\n\nI only tested this code with Ubuntu 12.04, but I tried to make it as generic as possible (e.g. use of **os** module for file system interactions etc. So it might work on Windows and Mac relatively easily.)\n\n*Protip*: you really want to link your numpy to use a BLAS implementation for its matrix operations. I use **virtualenv** and link numpy against a system installation of **OpenBLAS**. Doing this will make this code almost an order of time faster because it relies very heavily on large matrix multiplies.\n\n## Getting started\n\n1. **Get the code.** `$ git clone` the repo and install the Python dependencies\n2. **Get the data.** I don't distribute the data in the Git repo, instead download the `data/` folder from [here](http://cs.stanford.edu/people/karpathy/deepimagesent/). Also, this download does not include the raw image files, so if you want to visualize the annotations on raw images, you have to obtain the images from Flickr8K / Flickr30K / COCO directly and dump them into the appropriate data folder.\n3. **Train the model.** Run the training `$ python driver.py` (see many additional argument settings inside the file) and wait. You'll see that the learning code writes checkpoints into `cv/` and periodically reports its status in `status/` folder. \n4. **Monitor the training.** The status can be inspected manually by reading the JSON and printing whatever you wish in a second process. In practice I run cross-validations on a cluster, so my `cv/` folder fills up with a lot of checkpoints that I further filter and inspect with other scripts. I am including my cluster training status visualization utility as well if you like. Run a local webserver (e.g. `$ python -m SimpleHTTPServer 8123`) and then open `monitorcv.html` in your browser on `http://localhost:8123/monitorcv.html`, or whatever the web server tells you the path is. You will have to edit the file to setup the paths properly and point it at the right json files.\n5. **Evaluate model checkpoints.** To evaluate a checkpoint from `cv/`, run the `evaluate_sentence_predctions.py` script and pass it the path to a checkpoint.\n6. **Visualize the predictions.** Use the included html file `visualize_result_struct.html` to visualize the JSON struct produced by the evaluation code. This will visualize the images and their predictions. Note that you'll have to download the raw images from the individual dataset pages and place them into the corresponding `data/` folder.\n\nLastly, note that this is currently research code, so a lot of the documentation is inside individual Python files. If you wish to work with this code, you'll have to get familiar with it and be comfortable reading Python code.\n\n## Pretrained model\n\nSome pretrained models can be found in the [NeuralTalk Model Zoo](http://cs.stanford.edu/people/karpathy/neuraltalk/). The slightly hairy part is that if you wish to apply these models to some arbitrary new image (one not from Flickr8k/30k/COCO) you have to first extract the CNN features. I use the 16-layer [VGG network](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) from Simonyan and Zisserman, because the model is beautiful, powerful and available with [Caffe](http://caffe.berkeleyvision.org/). There is opportunity for putting the preprocessing and inference into a single nice function that uses the Python wrapper to get the features and then runs the pretrained sentence model. I might add this in the future.\n\n## Using the model to predict on new images\n\nThe code allows you to easily predict and visualize results of running the model on COCO/Flickr8K/Flick30K images. If you want to run the code on arbitrary image (e.g. on your file system), things get a little more complicated because we need to first need to pipe your image through the VGG CNN to get the 4096-D activations on top. \n\nHave a look inside the folder `example_images` for instructions on how to do this. Currently, the code for extracting the raw features from each image is in Matlab, so you will need it installed on your system. Caffe also has a wrapper for Python, but I wasn't yet able to use the Python wrapper to exactly reproduce the features I get from Matlab. The `example_images` will walk you through the process, and you will eventually use `predict_on_images.py` to run the prediction.\n\n## Using your own data\n\nThe input to the system is the **data** folder, which contains the Flickr8K, Flickr30K and MSCOCO datasets. In particular, each folder (e.g. `data/flickr8k`) contains a `dataset.json` file that stores the image paths and sentences in the dataset (all images, sentences, raw preprocessed tokens, splits, and the mappings between images and sentences). Each folder additionally contains `vgg_feats.mat` , which is a `.mat` file that stores the CNN features from all images, one per row, using the VGG Net from ILSVRC 2014. Finally, there is the `imgs/` folder that holds the raw images. I also provide the Matlab script that I used to extract the features, which you may find helpful if you wish to use a different dataset. This is inside the `matlab_features_reference/` folder, and see the Readme file in that folder for more information.\n\n## License\nBSD license.\n"
        },
        {
          "name": "cv",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "driver.py",
          "type": "blob",
          "size": 16.2958984375,
          "content": "import argparse\nimport json\nimport time\nimport datetime\nimport numpy as np\nimport code\nimport socket\nimport os\nimport sys\nimport cPickle as pickle\n\nfrom imagernn.data_provider import getDataProvider\nfrom imagernn.solver import Solver\nfrom imagernn.imagernn_utils import decodeGenerator, eval_split\n\ndef preProBuildWordVocab(sentence_iterator, word_count_threshold):\n  # count up all word counts so that we can threshold\n  # this shouldnt be too expensive of an operation\n  print 'preprocessing word counts and creating vocab based on word count threshold %d' % (word_count_threshold, )\n  t0 = time.time()\n  word_counts = {}\n  nsents = 0\n  for sent in sentence_iterator:\n    nsents += 1\n    for w in sent['tokens']:\n      word_counts[w] = word_counts.get(w, 0) + 1\n  vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n  print 'filtered words from %d to %d in %.2fs' % (len(word_counts), len(vocab), time.time() - t0)\n\n  # with K distinct words:\n  # - there are K+1 possible inputs (START token and all the words)\n  # - there are K+1 possible outputs (END token and all the words)\n  # we use ixtoword to take predicted indeces and map them to words for output visualization\n  # we use wordtoix to take raw words and get their index in word vector matrix\n  ixtoword = {}\n  ixtoword[0] = '.'  # period at the end of the sentence. make first dimension be end token\n  wordtoix = {}\n  wordtoix['#START#'] = 0 # make first vector be the start token\n  ix = 1\n  for w in vocab:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1\n\n  # compute bias vector, which is related to the log probability of the distribution\n  # of the labels (words) and how often they occur. We will use this vector to initialize\n  # the decoder weights, so that the loss function doesnt show a huge increase in performance\n  # very quickly (which is just the network learning this anyway, for the most part). This makes\n  # the visualizations of the cost function nicer because it doesn't look like a hockey stick.\n  # for example on Flickr8K, doing this brings down initial perplexity from ~2500 to ~170.\n  word_counts['.'] = nsents\n  bias_init_vector = np.array([1.0*word_counts[ixtoword[i]] for i in ixtoword])\n  bias_init_vector /= np.sum(bias_init_vector) # normalize to frequencies\n  bias_init_vector = np.log(bias_init_vector)\n  bias_init_vector -= np.max(bias_init_vector) # shift to nice numeric range\n  return wordtoix, ixtoword, bias_init_vector\n\ndef RNNGenCost(batch, model, params, misc):\n  \"\"\" cost function, returns cost and gradients for model \"\"\"\n  regc = params['regc'] # regularization cost\n  BatchGenerator = decodeGenerator(params)\n  wordtoix = misc['wordtoix']\n\n  # forward the RNN on each image sentence pair\n  # the generator returns a list of matrices that have word probabilities\n  # and a list of cache objects that will be needed for backprop\n  Ys, gen_caches = BatchGenerator.forward(batch, model, params, misc, predict_mode = False)\n\n  # compute softmax costs for all generated sentences, and the gradients on top\n  loss_cost = 0.0\n  dYs = []\n  logppl = 0.0\n  logppln = 0\n  for i,pair in enumerate(batch):\n    img = pair['image']\n    # ground truth indeces for this sentence we expect to see\n    gtix = [ wordtoix[w] for w in pair['sentence']['tokens'] if w in wordtoix ]\n    gtix.append(0) # don't forget END token must be predicted in the end!\n    # fetch the predicted probabilities, as rows\n    Y = Ys[i]\n    maxes = np.amax(Y, axis=1, keepdims=True)\n    e = np.exp(Y - maxes) # for numerical stability shift into good numerical range\n    P = e / np.sum(e, axis=1, keepdims=True)\n    loss_cost += - np.sum(np.log(1e-20 + P[range(len(gtix)),gtix])) # note: add smoothing to not get infs\n    logppl += - np.sum(np.log2(1e-20 + P[range(len(gtix)),gtix])) # also accumulate log2 perplexities\n    logppln += len(gtix)\n\n    # lets be clever and optimize for speed here to derive the gradient in place quickly\n    for iy,y in enumerate(gtix):\n      P[iy,y] -= 1 # softmax derivatives are pretty simple\n    dYs.append(P)\n\n  # backprop the RNN\n  grads = BatchGenerator.backward(dYs, gen_caches)\n\n  # add L2 regularization cost and gradients\n  reg_cost = 0.0\n  if regc > 0:    \n    for p in misc['regularize']:\n      mat = model[p]\n      reg_cost += 0.5 * regc * np.sum(mat * mat)\n      grads[p] += regc * mat\n\n  # normalize the cost and gradient by the batch size\n  batch_size = len(batch)\n  reg_cost /= batch_size\n  loss_cost /= batch_size\n  for k in grads: grads[k] /= batch_size\n\n  # return output in json\n  out = {}\n  out['cost'] = {'reg_cost' : reg_cost, 'loss_cost' : loss_cost, 'total_cost' : loss_cost + reg_cost}\n  out['grad'] = grads\n  out['stats'] = { 'ppl2' : 2 ** (logppl / logppln)}\n  return out\n\ndef main(params):\n  batch_size = params['batch_size']\n  dataset = params['dataset']\n  word_count_threshold = params['word_count_threshold']\n  do_grad_check = params['do_grad_check']\n  max_epochs = params['max_epochs']\n  host = socket.gethostname() # get computer hostname\n\n  # fetch the data provider\n  dp = getDataProvider(dataset)\n\n  misc = {} # stores various misc items that need to be passed around the framework\n\n  # go over all training sentences and find the vocabulary we want to use, i.e. the words that occur\n  # at least word_count_threshold number of times\n  misc['wordtoix'], misc['ixtoword'], bias_init_vector = preProBuildWordVocab(dp.iterSentences('train'), word_count_threshold)\n\n  # delegate the initialization of the model to the Generator class\n  BatchGenerator = decodeGenerator(params)\n  init_struct = BatchGenerator.init(params, misc)\n  model, misc['update'], misc['regularize'] = (init_struct['model'], init_struct['update'], init_struct['regularize'])\n\n  # force overwrite here. This is a bit of a hack, not happy about it\n  model['bd'] = bias_init_vector.reshape(1, bias_init_vector.size)\n\n  print 'model init done.'\n  print 'model has keys: ' + ', '.join(model.keys())\n  print 'updating: ' + ', '.join( '%s [%dx%d]' % (k, model[k].shape[0], model[k].shape[1]) for k in misc['update'])\n  print 'updating: ' + ', '.join( '%s [%dx%d]' % (k, model[k].shape[0], model[k].shape[1]) for k in misc['regularize'])\n  print 'number of learnable parameters total: %d' % (sum(model[k].shape[0] * model[k].shape[1] for k in misc['update']), )\n\n  if params.get('init_model_from', ''):\n    # load checkpoint\n    checkpoint = pickle.load(open(params['init_model_from'], 'rb'))\n    model = checkpoint['model'] # overwrite the model\n\n  # initialize the Solver and the cost function\n  solver = Solver()\n  def costfun(batch, model):\n    # wrap the cost function to abstract some things away from the Solver\n    return RNNGenCost(batch, model, params, misc)\n\n  # calculate how many iterations we need\n  num_sentences_total = dp.getSplitSize('train', ofwhat = 'sentences')\n  num_iters_one_epoch = num_sentences_total / batch_size\n  max_iters = max_epochs * num_iters_one_epoch\n  eval_period_in_epochs = params['eval_period']\n  eval_period_in_iters = max(1, int(num_iters_one_epoch * eval_period_in_epochs))\n  abort = False\n  top_val_ppl2 = -1\n  smooth_train_ppl2 = len(misc['ixtoword']) # initially size of dictionary of confusion\n  val_ppl2 = len(misc['ixtoword'])\n  last_status_write_time = 0 # for writing worker job status reports\n  json_worker_status = {}\n  json_worker_status['params'] = params\n  json_worker_status['history'] = []\n  for it in xrange(max_iters):\n    if abort: break\n    t0 = time.time()\n    # fetch a batch of data\n    batch = [dp.sampleImageSentencePair() for i in xrange(batch_size)]\n    # evaluate cost, gradient and perform parameter update\n    step_struct = solver.step(batch, model, costfun, **params)\n    cost = step_struct['cost']\n    dt = time.time() - t0\n\n    # print training statistics\n    train_ppl2 = step_struct['stats']['ppl2']\n    smooth_train_ppl2 = 0.99 * smooth_train_ppl2 + 0.01 * train_ppl2 # smooth exponentially decaying moving average\n    if it == 0: smooth_train_ppl2 = train_ppl2 # start out where we start out\n    epoch = it * 1.0 / num_iters_one_epoch\n    print '%d/%d batch done in %.3fs. at epoch %.2f. loss cost = %f, reg cost = %f, ppl2 = %.2f (smooth %.2f)' \\\n          % (it, max_iters, dt, epoch, cost['loss_cost'], cost['reg_cost'], \\\n             train_ppl2, smooth_train_ppl2)\n\n    # perform gradient check if desired, with a bit of a burnin time (10 iterations)\n    if it == 10 and do_grad_check:\n      print 'disabling dropout for gradient check...'\n      params['drop_prob_encoder'] = 0\n      params['drop_prob_decoder'] = 0\n      solver.gradCheck(batch, model, costfun)\n      print 'done gradcheck, exitting.'\n      sys.exit() # hmmm. probably should exit here\n\n    # detect if loss is exploding and kill the job if so\n    total_cost = cost['total_cost']\n    if it == 0:\n      total_cost0 = total_cost # store this initial cost\n    if total_cost > total_cost0 * 2:\n      print 'Aboring, cost seems to be exploding. Run gradcheck? Lower the learning rate?'\n      abort = True # set the abort flag, we'll break out\n\n    # logging: write JSON files for visual inspection of the training\n    tnow = time.time()\n    if tnow > last_status_write_time + 60*1: # every now and then lets write a report\n      last_status_write_time = tnow\n      jstatus = {}\n      jstatus['time'] = datetime.datetime.now().isoformat()\n      jstatus['iter'] = (it, max_iters)\n      jstatus['epoch'] = (epoch, max_epochs)\n      jstatus['time_per_batch'] = dt\n      jstatus['smooth_train_ppl2'] = smooth_train_ppl2\n      jstatus['val_ppl2'] = val_ppl2 # just write the last available one\n      jstatus['train_ppl2'] = train_ppl2\n      json_worker_status['history'].append(jstatus)\n      status_file = os.path.join(params['worker_status_output_directory'], host + '_status.json')\n      try:\n        json.dump(json_worker_status, open(status_file, 'w'))\n      except Exception, e: # todo be more clever here\n        print 'tried to write worker status into %s but got error:' % (status_file, )\n        print e\n\n    # perform perplexity evaluation on the validation set and save a model checkpoint if it's good\n    is_last_iter = (it+1) == max_iters\n    if (((it+1) % eval_period_in_iters) == 0 and it < max_iters - 5) or is_last_iter:\n      val_ppl2 = eval_split('val', dp, model, params, misc) # perform the evaluation on VAL set\n      print 'validation perplexity = %f' % (val_ppl2, )\n      \n      # abort training if the perplexity is no good\n      min_ppl_or_abort = params['min_ppl_or_abort']\n      if val_ppl2 > min_ppl_or_abort and min_ppl_or_abort > 0:\n        print 'aborting job because validation perplexity %f < %f' % (val_ppl2, min_ppl_or_abort)\n        abort = True # abort the job\n\n      write_checkpoint_ppl_threshold = params['write_checkpoint_ppl_threshold']\n      if val_ppl2 < top_val_ppl2 or top_val_ppl2 < 0:\n        if val_ppl2 < write_checkpoint_ppl_threshold or write_checkpoint_ppl_threshold < 0:\n          # if we beat a previous record or if this is the first time\n          # AND we also beat the user-defined threshold or it doesnt exist\n          top_val_ppl2 = val_ppl2\n          filename = 'model_checkpoint_%s_%s_%s_%.2f.p' % (dataset, host, params['fappend'], val_ppl2)\n          filepath = os.path.join(params['checkpoint_output_directory'], filename)\n          checkpoint = {}\n          checkpoint['it'] = it\n          checkpoint['epoch'] = epoch\n          checkpoint['model'] = model\n          checkpoint['params'] = params\n          checkpoint['perplexity'] = val_ppl2\n          checkpoint['wordtoix'] = misc['wordtoix']\n          checkpoint['ixtoword'] = misc['ixtoword']\n          try:\n            pickle.dump(checkpoint, open(filepath, \"wb\"))\n            print 'saved checkpoint in %s' % (filepath, )\n          except Exception, e: # todo be more clever here\n            print 'tried to write checkpoint into %s but got error: ' % (filepat, )\n            print e\n\n\nif __name__ == \"__main__\":\n\n  parser = argparse.ArgumentParser()\n\n  # global setup settings, and checkpoints\n  parser.add_argument('-d', '--dataset', dest='dataset', default='flickr8k', help='dataset: flickr8k/flickr30k')\n  parser.add_argument('-a', '--do_grad_check', dest='do_grad_check', type=int, default=0, help='perform gradcheck? program will block for visual inspection and will need manual user input')\n  parser.add_argument('--fappend', dest='fappend', type=str, default='baseline', help='append this string to checkpoint filenames')\n  parser.add_argument('-o', '--checkpoint_output_directory', dest='checkpoint_output_directory', type=str, default='cv/', help='output directory to write checkpoints to')\n  parser.add_argument('--worker_status_output_directory', dest='worker_status_output_directory', type=str, default='status/', help='directory to write worker status JSON blobs to')\n  parser.add_argument('--write_checkpoint_ppl_threshold', dest='write_checkpoint_ppl_threshold', type=float, default=-1, help='ppl threshold above which we dont bother writing a checkpoint to save space')\n  parser.add_argument('--init_model_from', dest='init_model_from', type=str, default='', help='initialize the model parameters from some specific checkpoint?')\n  \n  # model parameters\n  parser.add_argument('--generator', dest='generator', type=str, default='lstm', help='generator to use')\n  parser.add_argument('--image_encoding_size', dest='image_encoding_size', type=int, default=256, help='size of the image encoding')\n  parser.add_argument('--word_encoding_size', dest='word_encoding_size', type=int, default=256, help='size of word encoding')\n  parser.add_argument('--hidden_size', dest='hidden_size', type=int, default=256, help='size of hidden layer in generator RNNs')\n  # lstm-specific params\n  parser.add_argument('--tanhC_version', dest='tanhC_version', type=int, default=0, help='use tanh version of LSTM?')\n  # rnn-specific params\n  parser.add_argument('--rnn_relu_encoders', dest='rnn_relu_encoders', type=int, default=0, help='relu encoders before going to RNN?')\n  parser.add_argument('--rnn_feed_once', dest='rnn_feed_once', type=int, default=0, help='feed image to the rnn only single time?')\n\n  # optimization parameters\n  parser.add_argument('-c', '--regc', dest='regc', type=float, default=1e-8, help='regularization strength')\n  parser.add_argument('-m', '--max_epochs', dest='max_epochs', type=int, default=50, help='number of epochs to train for')\n  parser.add_argument('--solver', dest='solver', type=str, default='rmsprop', help='solver type: vanilla/adagrad/adadelta/rmsprop')\n  parser.add_argument('--momentum', dest='momentum', type=float, default=0.0, help='momentum for vanilla sgd')\n  parser.add_argument('--decay_rate', dest='decay_rate', type=float, default=0.999, help='decay rate for adadelta/rmsprop')\n  parser.add_argument('--smooth_eps', dest='smooth_eps', type=float, default=1e-8, help='epsilon smoothing for rmsprop/adagrad/adadelta')\n  parser.add_argument('-l', '--learning_rate', dest='learning_rate', type=float, default=1e-3, help='solver learning rate')\n  parser.add_argument('-b', '--batch_size', dest='batch_size', type=int, default=100, help='batch size')\n  parser.add_argument('--grad_clip', dest='grad_clip', type=float, default=5, help='clip gradients (normalized by batch size)? elementwise. if positive, at what threshold?')\n  parser.add_argument('--drop_prob_encoder', dest='drop_prob_encoder', type=float, default=0.5, help='what dropout to apply right after the encoder to an RNN/LSTM')\n  parser.add_argument('--drop_prob_decoder', dest='drop_prob_decoder', type=float, default=0.5, help='what dropout to apply right before the decoder in an RNN/LSTM')\n\n  # data preprocessing parameters\n  parser.add_argument('--word_count_threshold', dest='word_count_threshold', type=int, default=5, help='if a word occurs less than this number of times in training data, it is discarded')\n\n  # evaluation parameters\n  parser.add_argument('-p', '--eval_period', dest='eval_period', type=float, default=1.0, help='in units of epochs, how often do we evaluate on val set?')\n  parser.add_argument('--eval_batch_size', dest='eval_batch_size', type=int, default=100, help='for faster validation performance evaluation, what batch size to use on val img/sentences?')\n  parser.add_argument('--eval_max_images', dest='eval_max_images', type=int, default=-1, help='for efficiency we can use a smaller number of images to get validation error')\n  parser.add_argument('--min_ppl_or_abort', dest='min_ppl_or_abort', type=float , default=-1, help='if validation perplexity is below this threshold the job will abort')\n\n  args = parser.parse_args()\n  params = vars(args) # convert to ordinary dict\n  print 'parsed parameters:'\n  print json.dumps(params, indent = 2)\n  main(params)"
        },
        {
          "name": "eval",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval_sentence_predictions.py",
          "type": "blob",
          "size": 4.7001953125,
          "content": "import argparse\nimport json\nimport time\nimport datetime\nimport numpy as np\nimport code\nimport socket\nimport os\nimport cPickle as pickle\nimport math\n\nfrom imagernn.data_provider import getDataProvider\nfrom imagernn.solver import Solver\nfrom imagernn.imagernn_utils import decodeGenerator, eval_split\n\ndef main(params):\n\n  # load the checkpoint\n  checkpoint_path = params['checkpoint_path']\n  max_images = params['max_images']\n\n  print 'loading checkpoint %s' % (checkpoint_path, )\n  checkpoint = pickle.load(open(checkpoint_path, 'rb'))\n  checkpoint_params = checkpoint['params']\n  dataset = checkpoint_params['dataset']\n  model = checkpoint['model']\n  dump_folder = params['dump_folder']\n\n  if dump_folder:\n    print 'creating dump folder ' + dump_folder\n    os.system('mkdir -p ' + dump_folder)\n    \n  # fetch the data provider\n  dp = getDataProvider(dataset)\n\n  misc = {}\n  misc['wordtoix'] = checkpoint['wordtoix']\n  ixtoword = checkpoint['ixtoword']\n\n  blob = {} # output blob which we will dump to JSON for visualizing the results\n  blob['params'] = params\n  blob['checkpoint_params'] = checkpoint_params\n  blob['imgblobs'] = []\n\n  # iterate over all images in test set and predict sentences\n  BatchGenerator = decodeGenerator(checkpoint_params)\n  n = 0\n  all_references = []\n  all_candidates = []\n  for img in dp.iterImages(split = 'test', max_images = max_images):\n    n+=1\n    print 'image %d/%d:' % (n, max_images)\n    references = [' '.join(x['tokens']) for x in img['sentences']] # as list of lists of tokens\n    kwparams = { 'beam_size' : params['beam_size'] }\n    Ys = BatchGenerator.predict([{'image':img}], model, checkpoint_params, **kwparams)\n\n    img_blob = {} # we will build this up\n    img_blob['img_path'] = img['local_file_path']\n    img_blob['imgid'] = img['imgid']\n\n    if dump_folder:\n      # copy source file to some folder. This makes it easier to distribute results\n      # into a webpage, because all images that were predicted on are in a single folder\n      source_file = img['local_file_path']\n      target_file = os.path.join(dump_folder, os.path.basename(img['local_file_path']))\n      os.system('cp %s %s' % (source_file, target_file))\n\n    # encode the human-provided references\n    img_blob['references'] = []\n    for gtsent in references:\n      print 'GT: ' + gtsent\n      img_blob['references'].append({'text': gtsent})\n\n    # now evaluate and encode the top prediction\n    top_predictions = Ys[0] # take predictions for the first (and only) image we passed in\n    top_prediction = top_predictions[0] # these are sorted with highest on top\n    candidate = ' '.join([ixtoword[ix] for ix in top_prediction[1] if ix > 0]) # ix 0 is the END token, skip that\n    print 'PRED: (%f) %s' % (top_prediction[0], candidate)\n\n    # save for later eval\n    all_references.append(references)\n    all_candidates.append(candidate)\n\n    img_blob['candidate'] = {'text': candidate, 'logprob': top_prediction[0]}    \n    blob['imgblobs'].append(img_blob)\n\n  # use perl script to eval BLEU score for fair comparison to other research work\n  # first write intermediate files\n  print 'writing intermediate files into eval/'\n  open('eval/output', 'w').write('\\n'.join(all_candidates))\n  for q in xrange(5):\n    open('eval/reference'+`q`, 'w').write('\\n'.join([x[q] for x in all_references]))\n  # invoke the perl script to get BLEU scores\n  print 'invoking eval/multi-bleu.perl script...'\n  owd = os.getcwd()\n  os.chdir('eval')\n  os.system('./multi-bleu.perl reference < output')\n  os.chdir(owd)\n\n  # now also evaluate test split perplexity\n  gtppl = eval_split('test', dp, model, checkpoint_params, misc, eval_max_images = max_images)\n  print 'perplexity of ground truth words based on dictionary of %d words: %f' % (len(ixtoword), gtppl)\n  blob['gtppl'] = gtppl\n\n  # dump result struct to file\n  print 'saving result struct to %s' % (params['result_struct_filename'], )\n  json.dump(blob, open(params['result_struct_filename'], 'w'))\n\nif __name__ == \"__main__\":\n\n  parser = argparse.ArgumentParser()\n  parser.add_argument('checkpoint_path', type=str, help='the input checkpoint')\n  parser.add_argument('-b', '--beam_size', type=int, default=1, help='beam size in inference. 1 indicates greedy per-word max procedure. Good value is approx 20 or so, and more = better.')\n  parser.add_argument('--result_struct_filename', type=str, default='result_struct.json', help='filename of the result struct to save')\n  parser.add_argument('-m', '--max_images', type=int, default=-1, help='max images to use')\n  parser.add_argument('-d', '--dump_folder', type=str, default=\"\", help='dump the relevant images to a separate folder with this name?')\n\n  args = parser.parse_args()\n  params = vars(args) # convert to ordinary dict\n  print 'parsed parameters:'\n  print json.dumps(params, indent = 2)\n  main(params)\n"
        },
        {
          "name": "example_images",
          "type": "tree",
          "content": null
        },
        {
          "name": "imagernn",
          "type": "tree",
          "content": null
        },
        {
          "name": "matlab_features_reference",
          "type": "tree",
          "content": null
        },
        {
          "name": "monitorcv.html",
          "type": "blob",
          "size": 4.1796875,
          "content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n<meta charset=\"utf-8\">\n<style>\nbody {\n  font-family: courier;\n  padding: 0;\n  margin: 0;\n}\n#wrap {\n  margin: 5px;\n}\n.wtitle {\n  font-size: 2em;\n}\n.wparams {\n  font-size: 0.8em;\n}\n.wstats {\n  color: #900;\n}\n.wtime {\n}\n.witer {\n  font-weight: bold;\n}\n.ww {\n  border: 1px solid #333;\n  margin: 5px;\n  padding: 5px;\n  width: 500px;\n  display: inline-block;\n  vertical-align: top;\n}\n.wscore {\n  font-size: 2em;\n  font-weight: bold;\n  color: #0A0;\n}\n.wdis {\n  color: #0A0;\n  font-size: 20px;\n}\n</style>\n<link rel=\"stylesheet\" type=\"text/css\" href=\"vis_resources/d3utils.css\">\n\n<script src=\"vis_resources/jquery-1.8.3.min.js\"></script>\n<script src=\"vis_resources/underscore-min.js\"></script>\n<script src=\"vis_resources/d3.min.js\" charset=\"utf-8\"></script>\n<script src=\"vis_resources/d3utils.js\" charset=\"utf-8\"></script>\n<script>\n\n// you'll have to change this code if you want to use it yourself to point to correct JSON files\n// refresh()\n\nNUM = 40;\n\nparams_seen = {};\n\nfilter_params = false;\ngraphtype = 'cost';\n\nfunction filterParams(params) {\n  // this is meant to take out params thata re shared across all runs\n  if(!filter_params) { return params; }\n  var pout = {}\n  for(k in params) {\n    var v = params[k];\n    if(! (k in params_seen)) { params_seen[k] = [v]; }\n    if(params_seen[k].indexOf(v) == -1) { params_seen[k].push(v); }\n    if(params_seen[k].length > 1) {\n      pout[k] = v;\n    }\n  }\n  return pout;\n}\n\ngdata = {}\nfunction updateWorker(i, status_struct) {\n  all_data = status_struct.history;\n\n  gdata[i] = all_data; // keep global pointer to data\n  var data = all_data[all_data.length - 1]; // get last report\n  var div = d3.select('#w'+i);\n  $(\"#w\"+i).empty();\n\n  var dupdated = new Date(data['time']); // not sure what's up\n  var dnow = new Date();\n  var ddiff = (dnow - dupdated)/1000/60 - 8*60; // in units of minutes\n  console.log(ddiff);\n  var dcol = Math.floor(Math.max(100, 255 - (ddiff/10.0)*100));\n  $('#w'+i).css('background-color', 'rgb(255,'+dcol + ',' + dcol + ')');\n\n  div.append('div').attr('class', 'wtitle').text('worker ' + i);\n  div.append('div').attr('class', 'wdis').text(status_struct['params']['dataset'] + ': ' + status_struct['params']['generator']);\n  div.append('div').attr('class', 'wtime').text('last updated: ' + data['time']);\n  div.append('div').attr('class', 'witer').text('epoch ' + data['epoch'][0].toFixed(2) + '/' + data['epoch'][1])\n  div.append('div').attr('class', 'witer').text('iter ' + data['iter'][0] + '/' + data['iter'][1])\n  div.append('div').attr('class', 'wparams').text(JSON.stringify(filterParams(status_struct['params']), null, ' '));\n  div.append('div').attr('class', 'wbatchtime').text('time per batch:' + data['time_per_batch'].toFixed(3) + 's');\n  div.append('div').attr('class', 'wstats').html('smooth train ppl: ' + data['smooth_train_ppl2'].toFixed(3));\n  div.append('div').attr('class', 'wscore').text(data['val_ppl2'].toFixed(3));\n  \n  var xdata = [];\n  var ydata = [];\n  for(var j=0;j<all_data.length;j++) {\n    data = all_data[j];\n    xdata.push(data['epoch'][0]);\n    ydata.push(data['train_ppl2']);\n  }\n  div.append('div').attr('id', 'wg'+i).attr('style', 'width:500px;height:300px;');\n  plotToDiv(document.getElementById('wg'+i), ydata, xdata);\n}\n\nfunction refresh() {\n  // load all jsons\n  for(var i=1;i<=NUM;i++) {\n    var json_path = 'status/visionlab'+i+'.stanford.edu_status.json';\n    $.getJSON(json_path, function(q){\n      return function(data) {\n        updateWorker(q, data);\n      };\n    }(i));\n  }\n}\n\niid = 0;\nfunction start() {\n  var wrap = d3.select('#wrap');\n  for(var i=1;i<=NUM;i++) { wrap.append('div').attr('class', 'ww').attr('id', 'w'+i); }\n  refresh();\n  iid = setInterval(refresh, 60000);\n}\n\nfunction updateall() {\n  for(var i=1;i<=NUM;i++) {\n    var data = gdata[i];\n    if(data) {\n      updateWorker(i, data);\n    }\n  }\n}\nfunction allparams() {\n  filter_params = false;\n  updateall();\n}\nfunction filterparams() {\n  filter_params = true;\n  updateall();\n}\n\n</script>\n</head>\n\n<body onload=\"start()\">\n  <div id=\"ui\" style=\"margin:20px;\">\n    <button type=\"button\" onclick=\"allparams()\">Show all params</button>\n    <button type=\"button\" onclick=\"filterparams()\">Filter params</button>\n  </div>\n  <div id=\"wrap\">\n  </div>\n</body>\n\n</html>"
        },
        {
          "name": "predict_on_images.py",
          "type": "blob",
          "size": 3.9345703125,
          "content": "import argparse\nimport json\nimport time\nimport datetime\nimport numpy as np\nimport code\nimport os\nimport cPickle as pickle\nimport math\nimport scipy.io\n\nfrom imagernn.solver import Solver\nfrom imagernn.imagernn_utils import decodeGenerator, eval_split\n\n\"\"\"\nThis script is used to predict sentences for arbitrary images\nthat are located in a folder we call root_folder. It is assumed that\nthe root_folder contains:\n- the raw images\n- a file tasks.txt that lists the images you'd like to use\n- a file vgg_feats.mat that contains the CNN features. \n  You'll need to use the Matlab script I provided and point it at the\n  root folder and its tasks.txt file to save the features.\n\nThen point this script at the folder and at a checkpoint model you'd\nlike to evaluate.\n\"\"\"\n\ndef main(params):\n\n  # load the checkpoint\n  checkpoint_path = params['checkpoint_path']\n  print 'loading checkpoint %s' % (checkpoint_path, )\n  checkpoint = pickle.load(open(checkpoint_path, 'rb'))\n  checkpoint_params = checkpoint['params']\n  dataset = checkpoint_params['dataset']\n  model = checkpoint['model']\n  misc = {}\n  misc['wordtoix'] = checkpoint['wordtoix']\n  ixtoword = checkpoint['ixtoword']\n\n  # output blob which we will dump to JSON for visualizing the results\n  blob = {} \n  blob['params'] = params\n  blob['checkpoint_params'] = checkpoint_params\n  blob['imgblobs'] = []\n\n  # load the tasks.txt file\n  root_path = params['root_path']\n  img_names = open(os.path.join(root_path, 'tasks.txt'), 'r').read().splitlines()\n\n  # load the features for all images\n  features_path = os.path.join(root_path, 'vgg_feats.mat')\n  features_struct = scipy.io.loadmat(features_path)\n  features = features_struct['feats'] # this is a 4096 x N numpy array of features\n  D,N = features.shape\n\n  # iterate over all images and predict sentences\n  BatchGenerator = decodeGenerator(checkpoint_params)\n  for n in xrange(N):\n    print 'image %d/%d:' % (n, N)\n\n    # encode the image\n    img = {}\n    img['feat'] = features[:, n]\n    img['local_file_path'] =img_names[n]\n\n    # perform the work. heavy lifting happens inside\n    kwparams = { 'beam_size' : params['beam_size'] }\n    Ys = BatchGenerator.predict([{'image':img}], model, checkpoint_params, **kwparams)\n\n    # build up the output\n    img_blob = {}\n    img_blob['img_path'] = img['local_file_path']\n\n    # encode the top prediction\n    top_predictions = Ys[0] # take predictions for the first (and only) image we passed in\n    top_prediction = top_predictions[0] # these are sorted with highest on top\n    candidate = ' '.join([ixtoword[ix] for ix in top_prediction[1] if ix > 0]) # ix 0 is the END token, skip that\n    print 'PRED: (%f) %s' % (top_prediction[0], candidate)\n    img_blob['candidate'] = {'text': candidate, 'logprob': top_prediction[0]}    \n    blob['imgblobs'].append(img_blob)\n\n  # dump result struct to file\n  save_file = os.path.join(root_path, 'result_struct.json')\n  print 'writing predictions to %s...' % (save_file, )\n  json.dump(blob, open(save_file, 'w'))\n\n  # dump output html\n  html = ''\n  for img in blob['imgblobs']:\n    html += '<img src=\"%s\" height=\"400\"><br>' % (img['img_path'], )\n    html += '(%f) %s <br><br>' % (img['candidate']['logprob'], img['candidate']['text'])\n  html_file = os.path.join(root_path, 'result.html')\n  print 'writing html result file to %s...' % (html_file, )\n  open(html_file, 'w').write(html)\n\nif __name__ == \"__main__\":\n\n  parser = argparse.ArgumentParser()\n  parser.add_argument('checkpoint_path', type=str, help='the input checkpoint')\n  parser.add_argument('-r', '--root_path', default='example_images', type=str, help='folder with the images, tasks.txt file, and corresponding vgg_feats.mat file')\n  parser.add_argument('-b', '--beam_size', type=int, default=1, help='beam size in inference. 1 indicates greedy per-word max procedure. Good value is approx 20 or so, and more = better.')\n\n  args = parser.parse_args()\n  params = vars(args) # convert to ordinary dict\n  print 'parsed parameters:'\n  print json.dumps(params, indent = 2)\n  main(params)\n"
        },
        {
          "name": "py_caffe_feat_extract.py",
          "type": "blob",
          "size": 9.595703125,
          "content": "'''\nauthor: ahmed osman\nemail : ahmed.osman99 AT GMAIL\n'''\n\nimport caffe\nimport numpy as np\nimport argparse\nimport os\nimport time\nimport scipy.io\n\n\ndef reduce_along_dim(img , dim , weights , indicies): \n    '''\n    Perform bilinear interpolation given along the image dimension dim\n    -weights are the kernel weights \n    -indicies are the crossponding indicies location\n    return img resize along dimension dim\n    '''\n    other_dim = abs(dim-1)       \n    if other_dim == 0:  #resizing image width\n        weights  = np.tile(weights[np.newaxis,:,:,np.newaxis],(img.shape[other_dim],1,1,3))\n        out_img = img[:,indicies,:]*weights\n        out_img = np.sum(out_img,axis=2)\n    else:   # resize image height     \n        weights  = np.tile(weights[:,:,np.newaxis,np.newaxis],(1,1,img.shape[other_dim],3))\n        out_img = img[indicies,:,:]*weights\n        out_img = np.sum(out_img,axis=1)\n        \n    return out_img\n\n            \ndef cubic_spline(x):\n    '''\n    Compute the kernel weights \n    See Keys, \"Cubic Convolution Interpolation for Digital Image\n    Processing,\" IEEE Transactions on Acoustics, Speech, and Signal\n    Processing, Vol. ASSP-29, No. 6, December 1981, p. 1155.\n    '''\n    absx   = np.abs(x)\n    absx2  = absx**2\n    absx3  = absx**3 \n    kernel_weight = (1.5*absx3 - 2.5*absx2 + 1) * (absx<=1) + (-0.5*absx3 + 2.5* absx2 - 4*absx + 2) * ((1<absx) & (absx<=2))\n    return kernel_weight\n    \ndef contribution(in_dim_len , out_dim_len , scale ):\n    '''\n    Compute the weights and indicies of the pixels involved in the cubic interpolation along each dimension.\n    \n    output:\n    weights a list of size 2 (one set of weights for each dimension). Each item is of size OUT_DIM_LEN*Kernel_Width\n    indicies a list of size 2(one set of pixel indicies for each dimension) Each item is of size OUT_DIM_LEN*kernel_width\n    \n    note that if the entire column weights is zero, it gets deleted since those pixels don't contribute to anything\n    '''\n    kernel_width = 4\n    if scale < 1:\n        kernel_width =  4 / scale\n        \n    x_out = np.array(range(1,out_dim_len+1))  \n    #project to the input space dimension\n    u = x_out/scale + 0.5*(1-1/scale)\n    \n    #position of the left most pixel in each calculation\n    l = np.floor( u - kernel_width/2)\n  \n    #maxium number of pixels in each computation\n    p = int(np.ceil(kernel_width) + 2)\n    \n    indicies = np.zeros((l.shape[0],p) , dtype = int)\n    indicies[:,0] = l\n      \n    for i in range(1,p):\n        indicies[:,i] = indicies[:,i-1]+1\n    \n    #compute the weights of the vectors\n    u = u.reshape((u.shape[0],1))\n    u = np.repeat(u,p,axis=1)\n    \n    if scale < 1:\n        weights = scale*cubic_spline(scale*(indicies-u ))\n    else:\n        weights = cubic_spline((indicies-u))\n         \n    weights_sums = np.sum(weights,1)\n    weights = weights/ weights_sums[:, np.newaxis] \n    \n    indicies = indicies - 1    \n    indicies[indicies<0] = 0                     \n    indicies[indicies>in_dim_len-1] = in_dim_len-1 #clamping the indicies at the ends\n    \n    valid_cols = np.all( weights==0 , axis = 0 ) == False #find columns that are not all zeros\n    \n    indicies  = indicies[:,valid_cols]           \n    weights    = weights[:,valid_cols]\n    \n    return weights , indicies\n     \ndef imresize(img , cropped_width , cropped_height):\n    '''\n    Function implementing matlab's imresize functionality default behaviour\n    Cubic spline interpolation with antialiasing correction when scaling down the image.\n    \n    '''\n    \n    \n    width_scale  = float(cropped_width)  / img.shape[1]\n    height_scale = float(cropped_height) / img.shape[0] \n    \n    if len(img.shape) == 2: #Gray Scale Case\n        img = np.tile(img[:,:,np.newaxis] , (1,1,3)) #Broadcast \n    \n    order   = np.argsort([height_scale , width_scale])\n    scale   = [height_scale , width_scale]\n    out_dim = [cropped_height , cropped_width] \n    \n    \n    weights  = [0,0]\n    indicies = [0,0]\n    \n    for i in range(0 , 2):\n        weights[i] , indicies[i] = contribution(img.shape[ i ],out_dim[i], scale[i])\n    \n    for i in range(0 , len(order)):\n        img = reduce_along_dim(img , order[i] , weights[order[i]] , indicies[order[i]])\n        \n    return img\n\n\ndef preprocess_image(img):\n    '''\n    Preprocess an input image before processing by the caffe module.\n    \n    \n    Preprocessing include:\n    -----------------------\n    1- Converting image to single precision data type\n    2- Resizing the input image to cropped_dimensions used in extract_features() matlab script\n    3- Reorder color Channel, RGB->BGR\n    4- Convert color scale from 0-1 to 0-255 range (actually because image type is a float the \n        actual range could be negative or >255 during the cubic spline interpolation for image resize.\n    5- Subtract the VGG dataset mean.\n    6- Reorder the image to standard caffe input dimension order ( 3xHxW) \n    '''\n    img      = img.astype(np.float32)\n    img      = imresize(img,224,224) #cropping the image\n    img      = img[:,:,[2,1,0]] #RGB-BGR\n    img      = img*255\n    \n    mean = np.array([103.939, 116.779, 123.68]) #mean of the vgg \n    \n    for i in range(0,3):\n        img[:,:,i] = img[:,:,i] - mean[i] #subtracting the mean\n    img = np.transpose(img, [2,0,1])\n    return img #HxWx3\n        \ndef caffe_extract_feats(path_imgs , path_model_def , path_model , WITH_GPU = True , batch_size = 10 ):\n    '''\n    Function using the caffe python wrapper to extract 4096 from VGG_ILSVRC_16_layers.caffemodel model\n    \n    Inputs:\n    ------\n    path_imgs      : list of the full path of images to be processed \n    path_model_def : path to the model definition file\n    path_model     : path to the pretrained model weight\n    WItH_GPU       : Use a GPU \n    \n    Output:\n    -------\n    features           : return the features extracted \n    '''\n    \n    if WITH_GPU:\n        caffe.set_mode_gpu()\n    else:\n        caffe.set_mode_cpu()\n    print \"loading model:\",path_model\n    caffe_net = caffe.Classifier(path_model_def , path_model , image_dims = (224,224) , raw_scale = 255, channel_swap=(2,1,0),\n                            mean = np.array([103.939, 116.779, 123.68]) )\n\n    feats = np.zeros((4096 , len(path_imgs)))\n    \n    for b in range(0 , len(path_imgs) , batch_size):\n        list_imgs = []\n        for i in range(b , b + batch_size ):\n            if i < len(path_imgs):\n                list_imgs.append( np.array( caffe.io.load_image(path_imgs[i]) ) ) #loading images HxWx3 (RGB)\n            else:\n                list_imgs.append(list_imgs[-1]) #Appending the last image in order to have a batch of size 10. The extra predictions are removed later..\n                \n        caffe_input = np.asarray([preprocess_image(in_) for in_ in list_imgs]) #preprocess the images\n\n        predictions =caffe_net.forward(data = caffe_input)\n        predictions = predictions[caffe_net.outputs[0]].transpose()\n        \n        if i < len(path_imgs):\n            feats[:,b:i+1] = predictions\n            n = i+1\n        else:\n            n = min(batch_size , len(path_imgs) - b) \n            feats[:,b:b+n] = predictions[:,0:n] #Removing extra predictions, due to the extra last image appending.\n            n += b \n        print \"%d out of %d done.....\"%(n ,len(path_imgs))\n\n    return feats      \n    \nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model_def_path',dest='model_def_path', type=str , help='Path to the VGG_ILSVRC_16_layers model definition file.')\n    parser.add_argument('--model_path', dest='model_path',type=str,  help='Path to VGG_ILSVRC_16_layers pretrained model weight file i.e VGG_ILSVRC_16_layers.caffemodel')\n    parser.add_argument('-i',dest='input_directory',help='Path to Directory containing images to be processed.')\n    parser.add_argument('--filter',default = None ,dest='filter', help='Text file containing images names in the input directory to be processed. If no argument provided all images are processed.')\n    parser.add_argument('--WITH_GPU', action='store_true', dest='WITH_GPU', help = 'Caffe uses GPU for feature extraction')\n    parser.add_argument('-o',dest='out_directory',help='Output directory to store the generated features')\n    \n    args = parser.parse_args()\n    \n    input_directory = args.input_directory\n    path_model_def_file = args.model_def_path\n    path_model  = args.model_path\n    filter_path = args.filter\n    WITH_GPU    = args.WITH_GPU\n    out_directory = args.out_directory\n    \n    if not os.path.exists(out_directory):\n        raise RuntimeError(\"Output directory does not exist %s\"%(out_directory))\n    \n    if not os.path.exists(input_directory):\n        raise RuntimeError(\"%s , Directory does not exist\"%(input_directory))\n    \n    if not os.path.exists(path_model_def_file):\n        raise RuntimeError(\"%s , Model definition file does not exist\"%(path_model_def_file))\n    \n    if not os.path.exists(path_model):\n        raise RuntimeError(\"%s , Path to pretrained model file does not exist\"%(path_model))\n    \n    if not filter_path == None:\n        imgs = open(filter_path,'r').read().splitlines()        \n    else:\n        imgs = os.listdir(input_directory)\n    \n    path_imgs = [ os.path.join(input_directory , file) for file in imgs ]\n    \n    start_time = time.time()\n    print \"Feature Extraction for %d images starting now\"%(len(path_imgs))\n    feats = caffe_extract_feats(path_imgs, path_model_def_file, path_model, WITH_GPU)\n    print \"Total Duration for generating predictions %.2f seconds\"%(time.time()-start_time)\n    \n    out_path = os.path.join(out_directory,'vgg_feats.mat')\n    print \"Saving prediction to disk %s\"%(out_path)\n    vgg_feats = {}\n    vgg_feats['feats'] = feats\n    \n    scipy.io.savemat(out_path , vgg_feats)\n    \n    print \"Have a Good day!\"\n    \n    \n    \n    \n   "
        },
        {
          "name": "python_features",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0205078125,
          "content": "numpy\nscipy\nargparse\n"
        },
        {
          "name": "status",
          "type": "tree",
          "content": null
        },
        {
          "name": "vis_resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "visualize_result_struct.html",
          "type": "blob",
          "size": 4.9072265625,
          "content": "<!doctype html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"utf-8\">\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\">\n    <title>Image Annotation Viewer</title>\n    <script src=\"vis_resources/jquery-1.8.3.min.js\"></script>\n    <script src=\"vis_resources/d3.min.js\" charset=\"utf-8\"></script>\n    <script src=\"vis_resources/jsutils.js\" charset=\"utf-8\"></script>\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"http://cs.stanford.edu/people/karpathy/cssutils.css\">\n\n    <!-- Google fonts -->\n    <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>\n\n    <style>\n    body {\n      color: #333;\n      font-family: 'Roboto', sans-serif;\n      font-weight: 300;\n      font-size: 18px;\n      margin: 0;\n      padding: 0;\n    }\n    #wrap {\n      padding-left: 10px;\n      padding-right: 10px;\n    }\n    .hannot {\n      background-color: #EFE;\n    }\n    .rnnannot {\n      background-color: #EEF; \n    }\n    .rannot {\n      background-color: #FEE;  \n    }\n    .annot {\n      padding: 3px;\n    }\n    .idiv {\n      display: inline-block;\n      vertical-align: top;\n      margin: 10px;\n    }\n    h1 {\n      font-weight: 300;\n      margin: 0;\n    }\n    h2 {\n      font-weight: 300;\n      font-size: 20px;\n    }\n    #wrap {\n      margin: 20px;\n    }\n    #header {\n      background-color: #f7f6f1;\n      padding: 20px;\n      border-bottom: 1px solid #555;\n      box-shadow: 0px 0px 4px 2px #555;\n    }\n    .logprob {\n      font-family: Courier, monospace;\n    }\n    </style>\n    \n    <script type=\"application/javascript\">\n\n    // globals\n    var db = [];\n    var imgperm = [];\n    var current_img_i = 0;\n\n    function start() {\n      loadDataset('result_struct.json');\n    }\n\n    function writeHeader() {\n      html = '<h2>Showing results for ' + db.checkpoint_params.dataset + ' on ' + db.imgblobs.length + ' images</h2>';\n      html += '<br>Eval params were: ' + JSON.stringify(db.params);\n      html += '<br>Final average perplexity of ground truth words: ' + db.gtppl.toFixed(2);\n      $(\"#blobsheader\").html(html);\n    }\n\n    function renderNextImage() {\n      var ib = db.imgblobs[imgperm[current_img_i]]; // next image blob\n      var newdiv = d3.select(\"#blobs\").append(\"div\");\n      newdiv.attr(\"id\", 'img'+current_img_i);\n      newdiv.attr(\"class\", \"idiv\")\n      current_img_i++;\n      visImg(ib, newdiv); // function fills the pdiv with content\n    }\n\n    function visSentencesInit() {\n      $(\"#blobs\").empty();\n      current_img_i = 0;\n      for(var i=0;i<20;i++) { renderNextImage(); }\n    }\n\n    // pix is index of the sentece\n    function visImg(ib, div) {\n      \n      // fetch the associated top image\n      var fname = ib.img_path;\n      var img = new Image();\n      img.src = fname;\n      img.onload = function() {\n\n        var width = this.width;\n        var height = this.height;\n        var desired_height = 300;\n        var hscale = desired_height / height;\n        var nwidth = width * hscale; \n\n        // okay now lets render the image\n        var sdiv = div.append(\"div\").attr('class', 'svgdiv');\n        sdiv.append('img').attr('src', fname).attr('height', desired_height).attr('width', nwidth);\n\n        // add human predictions\n        if('references' in ib && false) {\n          var dnew = div.append('div').attr('class','hannot annot').style('width', Math.floor(nwidth-6) + 'px');\n          insertAnnot(ib.references[0], dnew);\n        }\n        \n        // add predictions\n        if('candidate' in ib) {\n          var dnew = div.append('div').attr('class','rnnannot annot').style('width', Math.floor(nwidth-6) + 'px');\n          insertAnnot(ib.candidate, dnew);\n        }\n      }\n    }\n\n    function insertAnnot(annot, dnew) {\n      dnew.append('div').attr('class', 'atxt').text(annot.text);\n      dnew.append('div').attr('class', 'logprob').text('logprob: ' + annot.logprob.toFixed(2));\n    }\n\n    // Handle infinite Scroll\n    $(window).scroll(function() {\n      if($(window).scrollTop() > 0.9 * ($(document).height() - $(window).height())) {\n        if(current_img_i < imgperm.length) {\n          // try to stay 20 images ahead of the scroll\n          for(var q=0;q<20;q++) { renderNextImage(); }\n        }\n      }\n    });\n    \n    // Data Loading\n    function loadDataset(jsonpath) {\n      // ugly hack to prevent caching below ;(\n      var jsonmod = jsonpath + '?sigh=' + Math.floor(Math.random() * 100000);\n      $.getJSON(jsonpath, function(data) {\n        db = data; // assign to global\n        imgperm = randperm(db.imgblobs.length);\n        //imgperm = []; for(var i=0;i<db.imgblobs.length;i++) { imgperm.push(i); }\n        writeHeader();\n        visSentencesInit();\n      });\n    }\n    </script>\n  </head>\n  <body onload=\"start()\">\n    <div id=\"header\">\n        <h1><a href=\"https://github.com/karpathy/neuraltalk\">NeuralTalk</a> Sentence Generation Results</h1>\n        <div id=\"blobsheader\"></div>\n      </div>\n    <div id=\"wrap\">\n      <div id=\"wrap\">\n        <div id=\"blobs\"></div>  \n      </div>\n    </div>\n  </body>\n</html>\n"
        }
      ]
    }
  ]
}