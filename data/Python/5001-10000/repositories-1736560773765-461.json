{
  "metadata": {
    "timestamp": 1736560773765,
    "page": 461,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "alirezamika/autoscraper",
      "stars": 6574,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1455078125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n.idea/\n.vscode/\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0439453125,
          "content": "MIT License\n\nCopyright (c) 2020 Alireza Mika\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.580078125,
          "content": "# AutoScraper: A Smart, Automatic, Fast and Lightweight Web Scraper for Python\n\n![img](https://user-images.githubusercontent.com/17881612/91968083-5ee92080-ed29-11ea-82ec-d99ec85367a5.png)\n\nThis project is made for automatic web scraping to make scraping easy. \nIt gets a url or the html content of a web page and a list of sample data which we want to scrape from that page. **This data can be text, url or any html tag value of that page.** It learns the scraping rules and returns the similar elements. Then you can use this learned object with new urls to get similar content or the exact same element of those new pages.\n\n\n## Installation\n\nIt's compatible with python 3.\n\n- Install latest version from git repository using pip:\n```bash\n$ pip install git+https://github.com/alirezamika/autoscraper.git\n```\n\n- Install from PyPI:\n```bash\n$ pip install autoscraper\n```\n\n- Install from source:\n```bash\n$ python setup.py install\n```\n\n## How to use\n\n### Getting similar results\n\nSay we want to fetch all related post titles in a stackoverflow page:\n\n```python\nfrom autoscraper import AutoScraper\n\nurl = 'https://stackoverflow.com/questions/2081586/web-scraping-with-python'\n\n# We can add one or multiple candidates here.\n# You can also put urls here to retrieve urls.\nwanted_list = [\"What are metaclasses in Python?\"]\n\nscraper = AutoScraper()\nresult = scraper.build(url, wanted_list)\nprint(result)\n```\n\nHere's the output:\n```python\n[\n    'How do I merge two dictionaries in a single expression in Python (taking union of dictionaries)?', \n    'How to call an external command?', \n    'What are metaclasses in Python?', \n    'Does Python have a ternary conditional operator?', \n    'How do you remove duplicates from a list whilst preserving order?', \n    'Convert bytes to a string', \n    'How to get line count of a large file cheaply in Python?', \n    \"Does Python have a string 'contains' substring method?\", \n    'Why is “1000000000000000 in range(1000000000000001)” so fast in Python 3?'\n]\n```\nNow you can use the `scraper` object to get related topics of any stackoverflow page:\n```python\nscraper.get_result_similar('https://stackoverflow.com/questions/606191/convert-bytes-to-a-string')\n```\n\n### Getting exact result\n\nSay we want to scrape live stock prices from yahoo finance:\n\n```python\nfrom autoscraper import AutoScraper\n\nurl = 'https://finance.yahoo.com/quote/AAPL/'\n\nwanted_list = [\"124.81\"]\n\nscraper = AutoScraper()\n\n# Here we can also pass html content via the html parameter instead of the url (html=html_content)\nresult = scraper.build(url, wanted_list)\nprint(result)\n```\nNote that you should update the `wanted_list` if you want to copy this code, as the content of the page dynamically changes.\n\nYou can also pass any custom `requests` module parameter. for example you may want to use proxies or custom headers:\n\n```python\nproxies = {\n    \"http\": 'http://127.0.0.1:8001',\n    \"https\": 'https://127.0.0.1:8001',\n}\n\nresult = scraper.build(url, wanted_list, request_args=dict(proxies=proxies))\n```\n\nNow we can get the price of any symbol:\n\n```python\nscraper.get_result_exact('https://finance.yahoo.com/quote/MSFT/')\n```\n\n**You may want to get other info as well.** For example if you want to get market cap too, you can just append it to the wanted list. By using the `get_result_exact` method, it will retrieve the data as the same exact order in the wanted list.\n\n**Another example:** Say we want to scrape the about text, number of stars and the link to issues of Github repo pages:\n\n```python\nfrom autoscraper import AutoScraper\n\nurl = 'https://github.com/alirezamika/autoscraper'\n\nwanted_list = ['A Smart, Automatic, Fast and Lightweight Web Scraper for Python', '6.2k', 'https://github.com/alirezamika/autoscraper/issues']\n\nscraper = AutoScraper()\nscraper.build(url, wanted_list)\n```\n\nSimple, right?\n\n\n### Saving the model\n\nWe can now save the built model to use it later. To save:\n\n```python\n# Give it a file path\nscraper.save('yahoo-finance')\n```\n\nAnd to load:\n\n```python\nscraper.load('yahoo-finance')\n```\n\n## Tutorials\n\n- See [this gist](https://gist.github.com/alirezamika/72083221891eecd991bbc0a2a2467673) for more advanced usages.\n- [AutoScraper and Flask: Create an API From Any Website in Less Than 5 Minutes](https://medium.com/better-programming/autoscraper-and-flask-create-an-api-from-any-website-in-less-than-5-minutes-3f0f176fc4a3)\n\n## Issues\nFeel free to open an issue if you have any problem using the module.\n\n\n## Support the project\n\n<a href=\"https://www.buymeacoffee.com/alirezam\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/default-black.png\" alt=\"Buy Me A Coffee\" height=\"45\" width=\"163\" ></a>\n\n\n#### Happy Coding  ♥️\n"
        },
        {
          "name": "autoscraper",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.923828125,
          "content": "from codecs import open\nfrom os import path\n\nfrom setuptools import find_packages, setup\n\nhere = path.abspath(path.dirname(__file__))\n\nwith open(path.join(here, \"README.md\"), encoding=\"utf-8\") as f:\n    long_description = f.read()\n\nsetup(\n    name=\"autoscraper\",\n    version=\"1.1.14\",\n    description=\"A Smart, Automatic, Fast and Lightweight Web Scraper for Python\",\n    long_description_content_type=\"text/markdown\",\n    long_description=long_description,\n    url=\"https://github.com/alirezamika/autoscraper\",\n    author=\"Alireza Mika\",\n    author_email=\"alirezamika@gmail.com\",\n    license=\"MIT\",\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3\",\n    ],\n    keywords=\"scraping - scraper\",\n    packages=find_packages(exclude=[\"contrib\", \"docs\", \"tests\"]),\n    python_requires=\">=3.6\",\n    install_requires=[\"requests\", \"bs4\", \"lxml\"],\n)\n"
        }
      ]
    }
  ]
}