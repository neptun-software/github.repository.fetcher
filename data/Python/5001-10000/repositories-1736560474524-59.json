{
  "metadata": {
    "timestamp": 1736560474524,
    "page": 59,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/maskrcnn-benchmark",
      "stars": 9321,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.2412109375,
          "content": "# This is an example .flake8 config, used when developing *Black* itself.\n# Keep in sync with setup.cfg which is used for source packages.\n\n[flake8]\nignore = E203, E266, E501, W503\nmax-line-length = 80\nmax-complexity = 18\nselect = B,C,E,F,W,T4,B9\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.373046875,
          "content": "# compilation and distribution\n__pycache__\n_ext\n*.pyc\n*.so\nmaskrcnn_benchmark.egg-info/\nbuild/\ndist/\n\n# pytorch/python/numpy formats\n*.pth\n*.pkl\n*.npy\n\n# ipython/jupyter notebooks\n*.ipynb\n**/.ipynb_checkpoints/\n\n# Editor temporaries\n*.swn\n*.swo\n*.swp\n*~\n\n# Pycharm editor settings\n.idea\n\n# vscode editor settings\n.vscode\n\n# MacOS\n.DS_Store\n\n# project dirs\n/datasets\n/models\n/output\n"
        },
        {
          "name": "ABSTRACTIONS.md",
          "type": "blob",
          "size": 2.591796875,
          "content": "## Abstractions\nThe main abstractions introduced by `maskrcnn_benchmark` that are useful to\nhave in mind are the following:\n\n### ImageList\nIn PyTorch, the first dimension of the input to the network generally represents\nthe batch dimension, and thus all elements of the same batch have the same\nheight / width.\nIn order to support images with different sizes and aspect ratios in the same\nbatch, we created the `ImageList` class, which holds internally a batch of\nimages (os possibly different sizes). The images are padded with zeros such that\nthey have the same final size and batched over the first dimension. The original\nsizes of the images before padding are stored in the `image_sizes` attribute,\nand the batched tensor in `tensors`.\nWe provide a convenience function `to_image_list` that accepts a few different\ninput types, including a list of tensors, and returns an `ImageList` object.\n\n```python\nfrom maskrcnn_benchmark.structures.image_list import to_image_list\n\nimages = [torch.rand(3, 100, 200), torch.rand(3, 150, 170)]\nbatched_images = to_image_list(images)\n\n# it is also possible to make the final batched image be a multiple of a number\nbatched_images_32 = to_image_list(images, size_divisible=32)\n```\n\n### BoxList\nThe `BoxList` class holds a set of bounding boxes (represented as a `Nx4` tensor) for\na specific image, as well as the size of the image as a `(width, height)` tuple.\nIt also contains a set of methods that allow to perform geometric\ntransformations to the bounding boxes (such as cropping, scaling and flipping).\nThe class accepts bounding boxes from two different input formats:\n- `xyxy`, where each box is encoded as a `x1`, `y1`, `x2` and `y2` coordinates, and\n- `xywh`, where each box is encoded as `x1`, `y1`, `w` and `h`.\n\nAdditionally, each `BoxList` instance can also hold arbitrary additional information\nfor each bounding box, such as labels, visibility, probability scores etc.\n\nHere is an example on how to create a `BoxList` from a list of coordinates:\n```python\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList, FLIP_LEFT_RIGHT\n\nwidth = 100\nheight = 200\nboxes = [\n  [0, 10, 50, 50],\n  [50, 20, 90, 60],\n  [10, 10, 50, 50]\n]\n# create a BoxList with 3 boxes\nbbox = BoxList(boxes, image_size=(width, height), mode='xyxy')\n\n# perform some box transformations, has similar API as PIL.Image\nbbox_scaled = bbox.resize((width * 2, height * 3))\nbbox_flipped = bbox.transpose(FLIP_LEFT_RIGHT)\n\n# add labels for each bbox\nlabels = torch.tensor([0, 10, 1])\nbbox.add_field('labels', labels)\n\n# bbox also support a few operations, like indexing\n# here, selects boxes 0 and 2\nbbox_subset = bbox[[0, 2]]\n```\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.23828125,
          "content": "# Code of Conduct\n\nFacebook has adopted a Code of Conduct that we expect project participants to adhere to.\nPlease read the [full text](https://code.fb.com/codeofconduct/)\nso that you can understand what actions will and will not be tolerated.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.6025390625,
          "content": "# Contributing to Mask-RCNN Benchmark\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Our Development Process\nMinor changes and improvements will be released on an ongoing basis. Larger changes (e.g., changesets implementing a new paper) will be released on a more periodic basis.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `master`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nFacebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## Coding Style  \n* 4 spaces for indentation rather than tabs\n* 80 character line length\n* PEP8 formatting following [Black](https://black.readthedocs.io/en/stable/)\n\n## License\nBy contributing to Mask-RCNN Benchmark, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "INSTALL.md",
          "type": "blob",
          "size": 4.2373046875,
          "content": "## Installation\n\n### Requirements:\n- PyTorch 1.0 from a nightly release. It **will not** work with 1.0 nor 1.0.1. Installation instructions can be found in https://pytorch.org/get-started/locally/\n- torchvision from master\n- cocoapi\n- yacs\n- matplotlib\n- GCC >= 4.9\n- OpenCV\n- CUDA >= 9.0\n\n\n### Option 1: Step-by-step installation\n\n```bash\n# first, make sure that your conda is setup properly with the right environment\n# for that, check that `which conda`, `which pip` and `which python` points to the\n# right path. From a clean conda env, this is what you need to do\n\nconda create --name maskrcnn_benchmark -y\nconda activate maskrcnn_benchmark\n\n# this installs the right pip and dependencies for the fresh python\nconda install ipython pip\n\n# maskrcnn_benchmark and coco api dependencies\npip install ninja yacs cython matplotlib tqdm opencv-python\n\n# follow PyTorch installation in https://pytorch.org/get-started/locally/\n# we give the instructions for CUDA 9.0\nconda install -c pytorch pytorch-nightly torchvision cudatoolkit=9.0\n\nexport INSTALL_DIR=$PWD\n\n# install pycocotools\ncd $INSTALL_DIR\ngit clone https://github.com/cocodataset/cocoapi.git\ncd cocoapi/PythonAPI\npython setup.py build_ext install\n\n# install cityscapesScripts\ncd $INSTALL_DIR\ngit clone https://github.com/mcordts/cityscapesScripts.git\ncd cityscapesScripts/\npython setup.py build_ext install\n\n# install apex\ncd $INSTALL_DIR\ngit clone https://github.com/NVIDIA/apex.git\ncd apex\npython setup.py install --cuda_ext --cpp_ext\n\n# install PyTorch Detection\ncd $INSTALL_DIR\ngit clone https://github.com/facebookresearch/maskrcnn-benchmark.git\ncd maskrcnn-benchmark\n\n# the following will install the lib with\n# symbolic links, so that you can modify\n# the files if you want and won't need to\n# re-build it\npython setup.py build develop\n\n\nunset INSTALL_DIR\n\n# or if you are on macOS\n# MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build develop\n```\n#### Windows 10\n```bash\nopen a cmd and change to desired installation directory\nfrom now on will be refered as INSTALL_DIR\nconda create --name maskrcnn_benchmark\nconda activate maskrcnn_benchmark\n\n# this installs the right pip and dependencies for the fresh python\nconda install ipython\n\n# maskrcnn_benchmark and coco api dependencies\npip install ninja yacs cython matplotlib tqdm opencv-python\n\n# follow PyTorch installation in https://pytorch.org/get-started/locally/\n# we give the instructions for CUDA 9.0\n## Important : check the cuda version installed on your computer by running the command in the cmd :\nnvcc -- version\nconda install -c pytorch pytorch-nightly torchvision cudatoolkit=9.0\n\ngit clone https://github.com/cocodataset/cocoapi.git\n\n    #To prevent installation error do the following after commiting cocooapi :\n    #using file explorer  naviagate to cocoapi\\PythonAPI\\setup.py and change line 14 from:\n    #extra_compile_args=['-Wno-cpp', '-Wno-unused-function', '-std=c99'],\n    #to\n    #extra_compile_args={'gcc': ['/Qstd=c99']},\n    #Based on  https://github.com/cocodataset/cocoapi/issues/51\n\ncd cocoapi/PythonAPI\npython setup.py build_ext install\n\n# navigate back to INSTALL_DIR\ncd ..\ncd ..\n# install apex\n\ngit clone https://github.com/NVIDIA/apex.git\ncd apex\npython setup.py install --cuda_ext --cpp_ext\n# navigate back to INSTALL_DIR\ncd ..\n# install PyTorch Detection\n\ngit clone https://github.com/Idolized22/maskrcnn-benchmark.git\ncd maskrcnn-benchmark\n\n# the following will install the lib with\n# symbolic links, so that you can modify\n# the files if you want and won't need to\n# re-build it\npython setup.py build develop\n```\n### Option 2: Docker Image (Requires CUDA, Linux only)\n\nBuild image with defaults (`CUDA=9.0`, `CUDNN=7`, `FORCE_CUDA=1`):\n\n    nvidia-docker build -t maskrcnn-benchmark docker/\n\nBuild image with other CUDA and CUDNN versions:\n\n    nvidia-docker build -t maskrcnn-benchmark --build-arg CUDA=9.2 --build-arg CUDNN=7 docker/\n\nBuild image with FORCE_CUDA disabled:\n\n    nvidia-docker build -t maskrcnn-benchmark --build-arg FORCE_CUDA=0 docker/\n\nBuild and run image with built-in jupyter notebook(note that the password is used to log in jupyter notebook):\n\n    nvidia-docker build -t maskrcnn-benchmark-jupyter docker/docker-jupyter/\n    nvidia-docker run -td -p 8888:8888 -e PASSWORD=<password> -v <host-dir>:<container-dir> maskrcnn-benchmark-jupyter\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0400390625,
          "content": "MIT License\n\nCopyright (c) 2018 Facebook\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MODEL_ZOO.md",
          "type": "blob",
          "size": 6.7158203125,
          "content": "## Model Zoo and Baselines\n\n### Hardware\n- 8 NVIDIA V100 GPUs\n\n### Software\n- PyTorch version: 1.0.0a0+dd2c487\n- CUDA 9.2\n- CUDNN 7.1\n- NCCL 2.2.13-1\n\n### End-to-end Faster and Mask R-CNN baselines\n\nAll the baselines were trained using the exact same experimental setup as in Detectron.\nWe initialize the detection models with ImageNet weights from Caffe2, the same as used by Detectron.\n\nThe pre-trained models are available in the link in the model id.\n\nbackbone | type | lr sched | im / gpu | train mem(GB) | train time (s/iter) | total train time(hr) | inference time(s/im) | box AP | mask AP | model id\n-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --\nR-50-C4 | Fast | 1x | 1 | 5.8 | 0.4036 | 20.2 | 0.17130 | 34.8 | - | [6358800](https://download.pytorch.org/models/maskrcnn/e2e_faster_rcnn_R_50_C4_1x.pth)\nR-50-FPN | Fast | 1x | 2 | 4.4 | 0.3530 | 8.8 | 0.12580 | 36.8 | - | [6358793](https://download.pytorch.org/models/maskrcnn/e2e_faster_rcnn_R_50_FPN_1x.pth)\nR-101-FPN | Fast | 1x | 2 | 7.1 | 0.4591 | 11.5 | 0.143149 | 39.1 | - | [6358804](https://download.pytorch.org/models/maskrcnn/e2e_faster_rcnn_R_101_FPN_1x.pth)\nX-101-32x8d-FPN | Fast | 1x | 1 | 7.6 | 0.7007 | 35.0 | 0.209965 | 41.2 | - | [6358717](https://download.pytorch.org/models/maskrcnn/e2e_faster_rcnn_X_101_32x8d_FPN_1x.pth)\nR-50-C4 | Mask | 1x | 1 | 5.8 | 0.4520 | 22.6 | 0.17796 + 0.028 | 35.6 | 31.5 | [6358801](https://download.pytorch.org/models/maskrcnn/e2e_mask_rcnn_R_50_C4_1x.pth)\nR-50-FPN | Mask | 1x | 2 | 5.2 | 0.4536 | 11.3 | 0.12966 + 0.034 | 37.8 | 34.2 | [6358792](https://download.pytorch.org/models/maskrcnn/e2e_mask_rcnn_R_50_FPN_1x.pth)\nR-101-FPN | Mask | 1x | 2 | 7.9 | 0.5665 | 14.2 | 0.15384 + 0.034 | 40.1 | 36.1 | [6358805](https://download.pytorch.org/models/maskrcnn/e2e_mask_rcnn_R_101_FPN_1x.pth)\nX-101-32x8d-FPN | Mask | 1x | 1 | 7.8 | 0.7562 | 37.8 | 0.21739 + 0.034 | 42.2 | 37.8 | [6358718](https://download.pytorch.org/models/maskrcnn/e2e_mask_rcnn_X_101_32x8d_FPN_1x.pth)\n\nFor person keypoint detection:\n\nbackbone | type | lr sched | im / gpu | train mem(GB) | train time (s/iter) | total train time(hr) | inference time(s/im) | box AP | keypoint AP | model id\n-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --\nR-50-FPN | Keypoint | 1x | 2 | 5.7 | 0.3771 | 9.4 | 0.10941 | 53.7 | 64.3 | 9981060\n\n### Light-weight Model baselines\n\nWe provided pre-trained models for selected FBNet models. \n* All the models are trained from scratched with BN using the training schedule specified below. \n* Evaluation is performed on a single NVIDIA V100 GPU with `MODEL.RPN.POST_NMS_TOP_N_TEST` set to `200`. \n\nThe following inference time is reported:\n  * inference total batch=8: Total inference time including data loading, model inference and pre/post preprocessing using 8 images per batch.\n  * inference model batch=8: Model inference time only and using 8 images per batch.\n  * inference model batch=1: Model inference time only and using 1 image per batch.\n  * inferenee caffe2 batch=1: Model inference time for the model in Caffe2 format using 1 image per batch. The Caffe2 models fused the BN to Conv and purely run on C++/CUDA by using Caffe2 ops for rpn/detection post processing.\n\nThe pre-trained models are available in the link in the model id.\n\nbackbone | type | resolution | lr sched | im / gpu | train mem(GB) | train time (s/iter) | total train time (hr) | inference total batch=8 (s/im) | inference model batch=8 (s/im) | inference model batch=1 (s/im) | inference caffe2 batch=1 (s/im) | box AP | mask AP | model id\n-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --\n[R-50-C4](configs/e2e_faster_rcnn_R_50_C4_1x.yaml) (reference) | Fast | 800 | 1x | 1 | 5.8 | 0.4036 | 20.2 | 0.0875 | **0.0793** | 0.0831 | **0.0625** | 34.4 | - | f35857197\n[fbnet_chamv1a](configs/e2e_faster_rcnn_fbnet_chamv1a_600.yaml) | Fast | 600 | 0.75x | 12 | 13.6 | 0.5444 | 20.5 | 0.0315 | **0.0260** | 0.0376 | **0.0188** | 33.5 | - | [f100940543](https://download.pytorch.org/models/maskrcnn/e2e_faster_rcnn_fbnet_chamv1a_600.pth)\n[fbnet_default](configs/e2e_faster_rcnn_fbnet_600.yaml) | Fast | 600 | 0.5x | 16 | 11.1 | 0.4872 | 12.5 | 0.0316 | **0.0250** | 0.0297 | **0.0130** | 28.2 | - | [f101086388](https://download.pytorch.org/models/maskrcnn/e2e_faster_rcnn_fbnet_600.pth)\n[R-50-C4](configs/e2e_mask_rcnn_R_50_C4_1x.yaml) (reference) | Mask | 800 | 1x | 1 | 5.8 | 0.452 | 22.6 | 0.0918 | **0.0848** | 0.0844 | - | 35.2 | 31.0 | f35858791\n[fbnet_xirb16d](configs/e2e_mask_rcnn_fbnet_xirb16d_dsmask_600.yaml) | Mask | 600 | 0.5x | 16 | 13.4 | 1.1732 | 29 | 0.0386 | **0.0319** | 0.0356 | - | 30.7 | 26.9 | [f101086394](https://download.pytorch.org/models/maskrcnn/e2e_mask_rcnn_fbnet_xirb16d_dsmask.pth)\n[fbnet_default](configs/e2e_mask_rcnn_fbnet_600.yaml) | Mask | 600 | 0.5x | 16 | 13.0 | 0.9036 | 23.0 | 0.0327 | **0.0269** | 0.0385 | - | 29.0 | 26.1 | [f101086385](https://download.pytorch.org/models/maskrcnn/e2e_mask_rcnn_fbnet_600.pth)\n\n## Comparison with Detectron and mmdetection\n\nIn the following section, we compare our implementation with [Detectron](https://github.com/facebookresearch/Detectron)\nand [mmdetection](https://github.com/open-mmlab/mmdetection).\nThe same remarks from [mmdetection](https://github.com/open-mmlab/mmdetection/blob/master/MODEL_ZOO.md#training-speed)\nabout different hardware applies here.\n\n### Training speed\n\nThe numbers here are in seconds / iteration. The lower, the better.\n\ntype | Detectron (P100) | mmdetection (V100) | maskrcnn_benchmark (V100)\n-- | -- | -- | --\nFaster R-CNN R-50 C4 | 0.566 | - | 0.4036\nFaster R-CNN R-50 FPN | 0.544 | 0.554 | 0.3530\nFaster R-CNN R-101 FPN | 0.647 | - | 0.4591\nFaster R-CNN X-101-32x8d FPN | 0.799 | - | 0.7007\nMask R-CNN R-50 C4 | 0.620 | - | 0.4520\nMask R-CNN R-50 FPN | 0.889 | 0.690 | 0.4536\nMask R-CNN R-101 FPN | 1.008 | - | 0.5665\nMask R-CNN X-101-32x8d FPN | 0.961 | - | 0.7562\n\n### Training memory\n\nThe lower, the better\n\ntype | Detectron (P100) | mmdetection (V100) | maskrcnn_benchmark (V100)\n-- | -- | -- | --\nFaster R-CNN R-50 C4 | 6.3 | - | 5.8\nFaster R-CNN R-50 FPN | 7.2 | 4.9 | 4.4\nFaster R-CNN R-101 FPN | 8.9 | - | 7.1\nFaster R-CNN X-101-32x8d FPN | 7.0 | - | 7.6\nMask R-CNN R-50 C4 | 6.6 | - | 5.8\nMask R-CNN R-50 FPN | 8.6 | 5.9 | 5.2\nMask R-CNN R-101 FPN | 10.2 | - | 7.9\nMask R-CNN X-101-32x8d FPN | 7.7 | - | 7.8\n\n### Accuracy\n\nThe higher, the better\n\ntype | Detectron (P100) | mmdetection (V100) | maskrcnn_benchmark (V100)\n-- | -- | -- | --\nFaster R-CNN R-50 C4 | 34.8 | - | 34.8\nFaster R-CNN R-50 FPN | 36.7 | 36.7 | 36.8\nFaster R-CNN R-101 FPN | 39.4 | - | 39.1\nFaster R-CNN X-101-32x8d FPN | 41.3 | - | 41.2\nMask R-CNN R-50 C4 | 35.8 & 31.4 | - | 35.6 & 31.5\nMask R-CNN R-50 FPN | 37.7 & 33.9 | 37.5 & 34.4 | 37.8 & 34.2\nMask R-CNN R-101 FPN | 40.0 & 35.9 | - | 40.1 & 36.1\nMask R-CNN X-101-32x8d FPN | 42.1 & 37.3 | - | 42.2 & 37.8\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.1083984375,
          "content": "# Faster R-CNN and Mask R-CNN in PyTorch 1.0\n\n**maskrcnn-benchmark has been deprecated. Please see [detectron2](https://github.com/facebookresearch/detectron2), which includes implementations for all models in maskrcnn-benchmark**\n\nThis project aims at providing the necessary building blocks for easily\ncreating detection and segmentation models using PyTorch 1.0.\n\n![alt text](demo/demo_e2e_mask_rcnn_X_101_32x8d_FPN_1x.png \"from http://cocodataset.org/#explore?id=345434\")\n\n## Highlights\n- **PyTorch 1.0:** RPN, Faster R-CNN and Mask R-CNN implementations that matches or exceeds Detectron accuracies\n- **Very fast**: up to **2x** faster than [Detectron](https://github.com/facebookresearch/Detectron) and **30%** faster than [mmdetection](https://github.com/open-mmlab/mmdetection) during training. See [MODEL_ZOO.md](MODEL_ZOO.md) for more details.\n- **Memory efficient:** uses roughly 500MB less GPU memory than mmdetection during training\n- **Multi-GPU training and inference**\n- **Mixed precision training:** trains faster with less GPU memory on [NVIDIA tensor cores](https://developer.nvidia.com/tensor-cores).\n- **Batched inference:** can perform inference using multiple images per batch per GPU\n- **CPU support for inference:** runs on CPU in inference time. See our [webcam demo](demo) for an example\n- Provides pre-trained models for almost all reference Mask R-CNN and Faster R-CNN configurations with 1x schedule.\n\n## Webcam and Jupyter notebook demo\n\nWe provide a simple webcam demo that illustrates how you can use `maskrcnn_benchmark` for inference:\n```bash\ncd demo\n# by default, it runs on the GPU\n# for best results, use min-image-size 800\npython webcam.py --min-image-size 800\n# can also run it on the CPU\npython webcam.py --min-image-size 300 MODEL.DEVICE cpu\n# or change the model that you want to use\npython webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu\n# in order to see the probability heatmaps, pass --show-mask-heatmaps\npython webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu\n# for the keypoint demo\npython webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu\n```\n\nA notebook with the demo can be found in [demo/Mask_R-CNN_demo.ipynb](demo/Mask_R-CNN_demo.ipynb).\n\n## Installation\n\nCheck [INSTALL.md](INSTALL.md) for installation instructions.\n\n\n## Model Zoo and Baselines\n\nPre-trained models, baselines and comparison with Detectron and mmdetection\ncan be found in [MODEL_ZOO.md](MODEL_ZOO.md)\n\n## Inference in a few lines\nWe provide a helper class to simplify writing inference pipelines using pre-trained models.\nHere is how we would do it. Run this from the `demo` folder:\n```python\nfrom maskrcnn_benchmark.config import cfg\nfrom predictor import COCODemo\n\nconfig_file = \"../configs/caffe2/e2e_mask_rcnn_R_50_FPN_1x_caffe2.yaml\"\n\n# update the config options with the config file\ncfg.merge_from_file(config_file)\n# manual override some options\ncfg.merge_from_list([\"MODEL.DEVICE\", \"cpu\"])\n\ncoco_demo = COCODemo(\n    cfg,\n    min_image_size=800,\n    confidence_threshold=0.7,\n)\n# load image and then run prediction\nimage = ...\npredictions = coco_demo.run_on_opencv_image(image)\n```\n\n## Perform training on COCO dataset\n\nFor the following examples to work, you need to first install `maskrcnn_benchmark`.\n\nYou will also need to download the COCO dataset.\nWe recommend to symlink the path to the coco dataset to `datasets/` as follows\n\nWe use `minival` and `valminusminival` sets from [Detectron](https://github.com/facebookresearch/Detectron/blob/master/detectron/datasets/data/README.md#coco-minival-annotations)\n\n```bash\n# symlink the coco dataset\ncd ~/github/maskrcnn-benchmark\nmkdir -p datasets/coco\nln -s /path_to_coco_dataset/annotations datasets/coco/annotations\nln -s /path_to_coco_dataset/train2014 datasets/coco/train2014\nln -s /path_to_coco_dataset/test2014 datasets/coco/test2014\nln -s /path_to_coco_dataset/val2014 datasets/coco/val2014\n# or use COCO 2017 version\nln -s /path_to_coco_dataset/annotations datasets/coco/annotations\nln -s /path_to_coco_dataset/train2017 datasets/coco/train2017\nln -s /path_to_coco_dataset/test2017 datasets/coco/test2017\nln -s /path_to_coco_dataset/val2017 datasets/coco/val2017\n\n# for pascal voc dataset:\nln -s /path_to_VOCdevkit_dir datasets/voc\n```\n\nP.S. `COCO_2017_train` = `COCO_2014_train` + `valminusminival` , `COCO_2017_val` = `minival`\n      \n\nYou can also configure your own paths to the datasets.\nFor that, all you need to do is to modify `maskrcnn_benchmark/config/paths_catalog.py` to\npoint to the location where your dataset is stored.\nYou can also create a new `paths_catalog.py` file which implements the same two classes,\nand pass it as a config argument `PATHS_CATALOG` during training.\n\n### Single GPU training\n\nMost of the configuration files that we provide assume that we are running on 8 GPUs.\nIn order to be able to run it on fewer GPUs, there are a few possibilities:\n\n**1. Run the following without modifications**\n\n```bash\npython /path_to_maskrcnn_benchmark/tools/train_net.py --config-file \"/path/to/config/file.yaml\"\n```\nThis should work out of the box and is very similar to what we should do for multi-GPU training.\nBut the drawback is that it will use much more GPU memory. The reason is that we set in the\nconfiguration files a global batch size that is divided over the number of GPUs. So if we only\nhave a single GPU, this means that the batch size for that GPU will be 8x larger, which might lead\nto out-of-memory errors.\n\nIf you have a lot of memory available, this is the easiest solution.\n\n**2. Modify the cfg parameters**\n\nIf you experience out-of-memory errors, you can reduce the global batch size. But this means that\nyou'll also need to change the learning rate, the number of iterations and the learning rate schedule.\n\nHere is an example for Mask R-CNN R-50 FPN with the 1x schedule:\n```bash\npython tools/train_net.py --config-file \"configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\" SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0025 SOLVER.MAX_ITER 720000 SOLVER.STEPS \"(480000, 640000)\" TEST.IMS_PER_BATCH 1 MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN 2000\n```\nThis follows the [scheduling rules from Detectron.](https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14-L30)\nNote that we have multiplied the number of iterations by 8x (as well as the learning rate schedules),\nand we have divided the learning rate by 8x.\n\nWe also changed the batch size during testing, but that is generally not necessary because testing\nrequires much less memory than training.\n\nFurthermore, we set `MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN 2000` as the proposals are selected for per the batch rather than per image in the default training. The value is calculated by **1000 x images-per-gpu**. Here we have 2 images per GPU, therefore we set the number as 1000 x 2 = 2000. If we have 8 images per GPU, the value should be set as 8000. Note that this does not apply if `MODEL.RPN.FPN_POST_NMS_PER_BATCH` is set to `False` during training. See [#672](https://github.com/facebookresearch/maskrcnn-benchmark/issues/672) for more details.\n\n### Multi-GPU training\nWe use internally `torch.distributed.launch` in order to launch\nmulti-gpu training. This utility function from PyTorch spawns as many\nPython processes as the number of GPUs we want to use, and each Python\nprocess will only use a single GPU.\n\n```bash\nexport NGPUS=8\npython -m torch.distributed.launch --nproc_per_node=$NGPUS /path_to_maskrcnn_benchmark/tools/train_net.py --config-file \"path/to/config/file.yaml\" MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN images_per_gpu x 1000\n```\nNote we should set `MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN` follow the rule in Single-GPU training.\n\n### Mixed precision training\nWe currently use [APEX](https://github.com/NVIDIA/apex) to add [Automatic Mixed Precision](https://developer.nvidia.com/automatic-mixed-precision) support. To enable, just do Single-GPU or Multi-GPU training and set `DTYPE \"float16\"`.\n\n```bash\nexport NGPUS=8\npython -m torch.distributed.launch --nproc_per_node=$NGPUS /path_to_maskrcnn_benchmark/tools/train_net.py --config-file \"path/to/config/file.yaml\" MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN images_per_gpu x 1000 DTYPE \"float16\"\n```\nIf you want more verbose logging, set `AMP_VERBOSE True`. See [Mixed Precision Training guide](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html) for more details.\n\n## Evaluation\nYou can test your model directly on single or multiple gpus. Here is an example for Mask R-CNN R-50 FPN with the 1x schedule on 8 GPUS:\n```bash\nexport NGPUS=8\npython -m torch.distributed.launch --nproc_per_node=$NGPUS /path_to_maskrcnn_benchmark/tools/test_net.py --config-file \"configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\" TEST.IMS_PER_BATCH 16\n```\nTo calculate mAP for each class, you can simply modify a few lines in [coco_eval.py](https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/data/datasets/evaluation/coco/coco_eval.py). See [#524](https://github.com/facebookresearch/maskrcnn-benchmark/issues/524#issuecomment-475118810) for more details.\n\n## Abstractions\nFor more information on some of the main abstractions in our implementation, see [ABSTRACTIONS.md](ABSTRACTIONS.md).\n\n## Adding your own dataset\n\nThis implementation adds support for COCO-style datasets.\nBut adding support for training on a new dataset can be done as follows:\n```python\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\n\nclass MyDataset(object):\n    def __init__(self, ...):\n        # as you would do normally\n\n    def __getitem__(self, idx):\n        # load the image as a PIL Image\n        image = ...\n\n        # load the bounding boxes as a list of list of boxes\n        # in this case, for illustrative purposes, we use\n        # x1, y1, x2, y2 order.\n        boxes = [[0, 0, 10, 10], [10, 20, 50, 50]]\n        # and labels\n        labels = torch.tensor([10, 20])\n\n        # create a BoxList from the boxes\n        boxlist = BoxList(boxes, image.size, mode=\"xyxy\")\n        # add the labels to the boxlist\n        boxlist.add_field(\"labels\", labels)\n\n        if self.transforms:\n            image, boxlist = self.transforms(image, boxlist)\n\n        # return the image, the boxlist and the idx in your dataset\n        return image, boxlist, idx\n\n    def get_img_info(self, idx):\n        # get img_height and img_width. This is used if\n        # we want to split the batches according to the aspect ratio\n        # of the image, as it can be more efficient than loading the\n        # image from disk\n        return {\"height\": img_height, \"width\": img_width}\n```\nThat's it. You can also add extra fields to the boxlist, such as segmentation masks\n(using `structures.segmentation_mask.SegmentationMask`), or even your own instance type.\n\nFor a full example of how the `COCODataset` is implemented, check [`maskrcnn_benchmark/data/datasets/coco.py`](maskrcnn_benchmark/data/datasets/coco.py).\n\nOnce you have created your dataset, it needs to be added in a couple of places:\n- [`maskrcnn_benchmark/data/datasets/__init__.py`](maskrcnn_benchmark/data/datasets/__init__.py): add it to `__all__`\n- [`maskrcnn_benchmark/config/paths_catalog.py`](maskrcnn_benchmark/config/paths_catalog.py): `DatasetCatalog.DATASETS` and corresponding `if` clause in `DatasetCatalog.get()`\n\n### Testing\nWhile the aforementioned example should work for training, we leverage the\ncocoApi for computing the accuracies during testing. Thus, test datasets\nshould currently follow the cocoApi for now.\n\nTo enable your dataset for testing, add a corresponding if statement in [`maskrcnn_benchmark/data/datasets/evaluation/__init__.py`](maskrcnn_benchmark/data/datasets/evaluation/__init__.py):\n```python\nif isinstance(dataset, datasets.MyDataset):\n        return coco_evaluation(**args)\n```\n\n## Finetuning from Detectron weights on custom datasets\nCreate a script `tools/trim_detectron_model.py` like [here](https://gist.github.com/wangg12/aea194aa6ab6a4de088f14ee193fd968).\nYou can decide which keys to be removed and which keys to be kept by modifying the script.\n\nThen you can simply point the converted model path in the config file by changing `MODEL.WEIGHT`.\n\nFor further information, please refer to [#15](https://github.com/facebookresearch/maskrcnn-benchmark/issues/15).\n\n## Troubleshooting\nIf you have issues running or compiling this code, we have compiled a list of common issues in\n[TROUBLESHOOTING.md](TROUBLESHOOTING.md). If your issue is not present there, please feel\nfree to open a new issue.\n\n## Citations\nPlease consider citing this project in your publications if it helps your research. The following is a BibTeX reference. The BibTeX entry requires the `url` LaTeX package.\n```\n@misc{massa2018mrcnn,\nauthor = {Massa, Francisco and Girshick, Ross},\ntitle = {{maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch}},\nyear = {2018},\nhowpublished = {\\url{https://github.com/facebookresearch/maskrcnn-benchmark}},\nnote = {Accessed: [Insert date here]}\n}\n```\n\n## Projects using maskrcnn-benchmark\n\n- [RetinaMask: Learning to predict masks improves state-of-the-art single-shot detection for free](https://arxiv.org/abs/1901.03353). \n  Cheng-Yang Fu, Mykhailo Shvets, and Alexander C. Berg.\n  Tech report, arXiv,1901.03353.\n- [FCOS: Fully Convolutional One-Stage Object Detection](https://arxiv.org/abs/1904.01355).\n  Zhi Tian, Chunhua Shen, Hao Chen and Tong He.\n  Tech report, arXiv,1904.01355. [[code](https://github.com/tianzhi0549/FCOS)]\n- [MULAN: Multitask Universal Lesion Analysis Network for Joint Lesion Detection, Tagging, and Segmentation](https://arxiv.org/abs/1908.04373).\n  Ke Yan, Youbao Tang, Yifan Peng, Veit Sandfort, Mohammadhadi Bagheri, Zhiyong Lu, and Ronald M. Summers.\n  MICCAI 2019. [[code](https://github.com/rsummers11/CADLab/tree/master/MULAN_universal_lesion_analysis)]\n- [Is Sampling Heuristics Necessary in Training Deep Object Detectors?](https://arxiv.org/abs/1909.04868)\n  Joya Chen, Dong Liu, Tong Xu, Shilong Zhang, Shiwei Wu, Bin Luo, Xuezheng Peng, Enhong Chen.\n  Tech report, arXiv,1909.04868. [[code](https://github.com/ChenJoya/sampling-free)]\n  \n## License\n\nmaskrcnn-benchmark is released under the MIT license. See [LICENSE](LICENSE) for additional details.\n"
        },
        {
          "name": "TROUBLESHOOTING.md",
          "type": "blob",
          "size": 2.869140625,
          "content": "# Troubleshooting\n\nHere is a compilation if common issues that you might face\nwhile compiling / running this code:\n\n## Compilation errors when compiling the library\nIf you encounter build errors like the following:\n```\n/usr/include/c++/6/type_traits:1558:8: note: provided for ‘template<class _From, class _To> struct std::is_convertible’\n     struct is_convertible\n        ^~~~~~~~~~~~~~\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function ‘static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor>&&; bool <anonymous> = true; _Elements = {at::Tensor, at::Tensor, at::Tensor, at::Tensor}]’ not a return-statement\n     }\n ^\nerror: command '/usr/local/cuda/bin/nvcc' failed with exit status 1\n```\ncheck your CUDA version and your `gcc` version.\n```\nnvcc --version\ngcc --version\n```\nIf you are using CUDA 9.0 and gcc 6.4.0, then refer to https://github.com/facebookresearch/maskrcnn-benchmark/issues/25,\nwhich has a summary of the solution. Basically, CUDA 9.0 is not compatible with gcc 6.4.0.\n\n## ImportError: No module named maskrcnn_benchmark.config when running webcam.py\n\nThis means that `maskrcnn-benchmark` has not been properly installed.\nRefer to https://github.com/facebookresearch/maskrcnn-benchmark/issues/22 for a few possible issues.\nNote that we now support Python 2 as well.\n\n\n## ImportError: Undefined symbol: __cudaPopCallConfiguration error when import _C\n\nThis probably means that the inconsistent version of NVCC compile and your conda CUDAToolKit package. This is firstly mentioned in https://github.com/facebookresearch/maskrcnn-benchmark/issues/45 . All you need to do is:\n\n```\n# Check the NVCC compile version(e.g.)\n/usr/cuda-9.2/bin/nvcc --version\n# Check the CUDAToolKit version(e.g.)\n~/anaconda3/bin/conda list | grep cuda\n\n# If you need to update your CUDAToolKit\n~/anaconda3/bin/conda install -c anaconda cudatoolkit==9.2\n```\n\nBoth of them should have the **same** version. For example, if NVCC==9.2 and CUDAToolKit==9.2, this will be fine while when NVCC==9.2 but CUDAToolKit==9, it fails.\n\n\n## Segmentation fault (core dumped) when running the library\nThis probably means that you have compiled the library using GCC < 4.9, which is ABI incompatible with PyTorch.\nIndeed, during installation, you probably saw a message like\n```\nYour compiler (g++ 4.8) may be ABI-incompatible with PyTorch!\nPlease use a compiler that is ABI-compatible with GCC 4.9 and above.\nSee https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html.\n\nSee https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6\nfor instructions on how to install GCC 4.9 or higher.\n```\nFollow the instructions on https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6\nto install GCC 4.9 or higher, and try recompiling `maskrcnn-benchmark` again, after cleaning the\n`build` folder with\n```\nrm -rf build\n```\n\n\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "maskrcnn_benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.033203125,
          "content": "ninja\nyacs\ncython\nmatplotlib\ntqdm\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.03515625,
          "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n#!/usr/bin/env python\n\nimport glob\nimport os\n\nimport torch\nfrom setuptools import find_packages\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import CUDA_HOME\nfrom torch.utils.cpp_extension import CppExtension\nfrom torch.utils.cpp_extension import CUDAExtension\n\nrequirements = [\"torch\", \"torchvision\"]\n\n\ndef get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, \"maskrcnn_benchmark\", \"csrc\")\n\n    main_file = glob.glob(os.path.join(extensions_dir, \"*.cpp\"))\n    source_cpu = glob.glob(os.path.join(extensions_dir, \"cpu\", \"*.cpp\"))\n    source_cuda = glob.glob(os.path.join(extensions_dir, \"cuda\", \"*.cu\"))\n\n    sources = main_file + source_cpu\n    extension = CppExtension\n\n    extra_compile_args = {\"cxx\": []}\n    define_macros = []\n\n    if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv(\"FORCE_CUDA\", \"0\") == \"1\":\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [(\"WITH_CUDA\", None)]\n        extra_compile_args[\"nvcc\"] = [\n            \"-DCUDA_HAS_FP16=1\",\n            \"-D__CUDA_NO_HALF_OPERATORS__\",\n            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n        ]\n\n    sources = [os.path.join(extensions_dir, s) for s in sources]\n\n    include_dirs = [extensions_dir]\n\n    ext_modules = [\n        extension(\n            \"maskrcnn_benchmark._C\",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    ]\n\n    return ext_modules\n\n\nsetup(\n    name=\"maskrcnn_benchmark\",\n    version=\"0.1\",\n    author=\"fmassa\",\n    url=\"https://github.com/facebookresearch/maskrcnn-benchmark\",\n    description=\"object detection in pytorch\",\n    packages=find_packages(exclude=(\"configs\", \"tests\",)),\n    # install_requires=requirements,\n    ext_modules=get_extensions(),\n    cmdclass={\"build_ext\": torch.utils.cpp_extension.BuildExtension},\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}