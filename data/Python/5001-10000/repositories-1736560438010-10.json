{
  "metadata": {
    "timestamp": 1736560438010,
    "page": 10,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjEw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Theano/Theano",
      "stars": 9914,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".appveyor.yml",
          "type": "blob",
          "size": 1.1728515625,
          "content": "version: '1.0.{build}' # This number doesn't matter.\n\npull_requests:\n  do_not_increment_build_number: true\n\nplatform:\n  - x64\n\nclone_folder: C:\\projects\\theano\n\nenvironment:\n  BINSTAR_TOKEN:\n    secure: Z4ZN29hd1UKw4qUwSlpFk+58Ssa+DfIKSGhN3Wr5uOAsP3dCXrNDl5+ipVdzADFn\n  CONDA_LOC: \"C:\\\\Miniconda-x64\"\n  MKL_THREADING_LAYER: GNU\n\ninstall:\n  # This breaks conda-build because of git\n  - cmd: rmdir C:\\cygwin /s /q\n  - cmd: call %CONDA_LOC%\\Scripts\\activate.bat\n  - cmd: set PYTHONUNBUFFERED=1\n  - cmd: conda install -n root --yes conda conda-env conda-build anaconda-client\n  - cmd: conda config --append channels mila-udem\n\nbuild: off\n\ntest_script:\n  - cmd: for /f \"tokens=*\" %%i in ('python -c \"import versioneer; print(versioneer.get_version())\"') do set THEANO_VERSION=%%i\n  - cmd: echo %THEANO_VERSION%\n  - cmd: conda build --py 2.7 conda\n  - cmd: conda build --py 3.5 conda\n  - cmd: conda build --py 3.6 conda\n  - cmd: mkdir pkgs\n  - cmd: xcopy \"%CONDA_LOC%\"\\conda-bld\\win-64\\theano* pkgs\\ /Y\n  - ps: |\n      if($env:appveyor_repo_tag -eq 'True') {\n        cmd /c \"anaconda -t $env:BINSTAR_TOKEN upload --user=mila-udem pkgs/* 2>&1\"\n      }\n\nartifacts:\n  - path: pkgs/*\n    name: \"Conda Packages\"\n"
        },
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.03125,
          "content": "theano/_version.py export-subst\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4423828125,
          "content": "*.linkinfo\n*.o\n*.orig\n*.pyc\n*.pyo\n*.so\n*.sw?\n*~\n*.aux\n*.log\n*.nav\n*.out\n*.snm\n*.toc\n*.vrb\n.noseids\n*.DS_Store\n*.bak\nTheano.egg-info\n\\#*\\#\nbuild\ncompiled/*.cpp\ncore.*\ncutils_ext.cpp\ndist\ndoc/.build/\ndoc/indexes/oplist.txt\ndoc/indexes/typelist.txt\nhtml\npdf\nsetuptools-*.egg\ntheano/generated_version.py\ntheano/generated_version.py.out\ndistribute-*.egg\ndistribute-*.tar.gz\nTheano.suo\n.ipynb_checkpoints\n.pydevproject\n.ropeproject\ncore\n.idea\n.python-version\n"
        },
        {
          "name": ".jenkins",
          "type": "tree",
          "content": null
        },
        {
          "name": ".mailmap",
          "type": "blob",
          "size": 20.5400390625,
          "content": "# Prevent git from showing duplicate names with commands like \"git shortlog\"\n# # See the manpage of git-shortlog for details.\n# # The syntax is:\n# # Name that should be used <email that should be used> Bad name <bad email>\n# #\n# # You can skip Bad name if it is the same as the one that should be used, and is unique.\n# #\n# # This file is up-to-date if the command git log --format=\"%aN <%aE>\" | sort -u\n# # gives no duplicates.\n#     5\tFirstname Lastname <firstname.lastname@example.net>\n#     4\tLaboratoire d'Informatique des Systemes Adaptatifs <lisa@iro.umontreal.ca>\n#    25\tprojects@lgcm <projects@lgcm>\n\nabalkin <abalkin@enlnt.com> abalkin <abalkin>\nabalkin <abalkin@enlnt.com> abalkin <serpent.speak@gmail.com>\nabalkin <abalkin@enlnt.com> Alexander Belopolsky <abalkin@enlnt.com>\nabalkin <abalkin@enlnt.com> Alexander Belopolsky <a@enlnt.com>\nAdam Becker <junkkhaotik@gmail.com> khaotik <aruhanb@gmail.com>\nAdam Becker <junkkhaotik@gmail.com> khaotik <junkkhaotik@gmail.com>\nAdrian Seyboldt <aseyboldt@gmail.com> aseyboldt <aseyboldt@gmail.com>\nAleksandar Botev <botevmg@gmail.com> botev <botevmg@gmail.com>\nAlex Lamb <alex6200@gmail.com> AlexLamb <alex6200@gmail.com>\nAlex Lamb <alex6200@gmail.com> DeathMonster666 <alex6200@gmail.com>\nAlexandre de Brebisson <adbrebs@gmail.com> AdeB <adbrebs@gmail.com>\nAlexandre de Brebisson <adbrebs@gmail.com> Alexandre de Brébisson <adbrebs@users.noreply.github.com>\nAnatoly Belikov <awbelikov@gmail.com> Anatoly Belikov <wormblood@gmail.com>\nAndre Holzner <Andre.Georg.Holzner@cern.ch> Andre Holzner <holzner@pb-d-128-141-148-222.cern.ch>\nAndre Holzner <Andre.Georg.Holzner@cern.ch> andreh <andreh@localhost>\nAndre Holzner <Andre.Georg.Holzner@cern.ch> Andre Holzner <holzner@andres-mbp-2.fritz.box>\nAndrei Costinescu <andrei.costinescu@yahoo.com> Andrei Costinescu <AndreiCostinescu@users.noreply.github.com>\nAndrei Costinescu <andrei.costinescu@yahoo.com> AndreiCostinescu <andrei.costinescu@yahoo.com>\nAnirudh Goyal <anirudhgoyal9119@gmail.com> AndroidCloud <anirudhgoyal9119@gmail.com>\nArjun Jain <arjunjain@gmail.com> Arjun Jain <stencilman@users.noreply.github.com>\nArnaud Bergeron <abergeron@gmail.com> <abergeron@gmail.com>\nArnaud Bergeron <abergeron@gmail.com> <bergearn@iro.umontreal.ca>\n<abergeron@gmail.com> <anakha@kami.(none)>\nBalázs Hidasi <hidasi.balazs@gravityrd.com> Balázs <hidasib@gmail.com>\nBart van Merrienboer <bart.vanmerrienboer@gmail.com> Bart van Merriënboer <bart.vanmerrienboer@gmail.com>\nBart van Merrienboer <bart.vanmerrienboer@gmail.com> Bart <bart.vanmerrienboer@gmail.com>\nBenjamin Scellier <scellier@iro.umontreal.ca> Benjamin Scellier <scellier@bart4.iro.umontreal.ca>\nBenjamin Scellier <scellier@iro.umontreal.ca> Benjamin Scellier <scellier@bart5>\nBenjamin Scellier <scellier@iro.umontreal.ca> Benjamin Scellier <scellier@eos13.iro.umontreal.ca>\nBenjamin Scellier <scellier@iro.umontreal.ca> Benjamin Scellier <scellier@sencha.iro.umontreal.ca>\nBenjamin Scellier <scellier@iro.umontreal.ca> Benjamin Scellier <benjamin.scellier@gmail.com>\nBenjamin Scellier <scellier@iro.umontreal.ca> bscellier <bscellier@users.noreply.github.com>\nBogdan Budescu <bbudescu@gmail.com> bbudescu <bbudescu@gmail.com>\nBrian Cheung <briancheung@users.noreply.github.com> briancheung <bcheung5@gmail.com>\nCaglar <ca9lar@gmail.com> Caglar <caglar@users.noreply.github.com>\nCesar Laurent <cesarlaurent77@gmail.com> César Laurent <Thrandis@users.noreply.github.com>\nChienli Ma <maqianlie@gmail.com> Chienli Ma(马千里) <maqianlie@gmail.com>\nChienli Ma <maqianlie@gmail.com> ChienliMa <maqianlie@gmail.com>\nChiheb Trabelsi <chiheb.tr@gmail.com> Chiheb Trabelsi <trabelsc@bart5>\nChinnadhurai Sankar <chinnadhurai@gmail.com> Chinnadhurai Sankar <sankarch@leto04.iro.umontreal.ca>\nChinnadhurai Sankar <chinnadhurai@gmail.com> Chinnadhurai Sankar <sankarch@leto50.iro.umontreal.ca>\nChinnadhurai Sankar <chinnadhurai@gmail.com> Chinnadhurai Sankar <sankarch@kepler2.iro.umontreal.ca>\nChinnadhurai Sankar <chinnadhurai@gmail.com> Chinnadhurai Sankar <sankarch@kepler3.iro.umontreal.ca>\nChinnadhurai Sankar <chinnadhurai@gmail.com> chinnadhurai <chinnadhurai@gmail.com>\nClaude Coulombe <claude.coulombe@gmail.com> Claude Coulombe <claude.coulombe@gmail.comgit statuslscd /Users/claudecoulombe/gitlspwdsay Isabellegit config --global user.email claude.coulombe@gmail.com>\nDavid Warde-Farley <wardefar@iro.umontreal.ca> David Warde-Farley <dwf@cs.toronto.edu>\nDavid Warde-Farley <wardefar@iro.umontreal.ca> David Warde Farley <dwf@cs.toronto.edu>\nDavid Warde-Farley <wardefar@iro.umontreal.ca> David Warde-Farley <d.warde.farley@gmail.com>\nDouglas Eck <douglas.eck@gmail.com> eckdoug@localhost <eckdoug@localhost>\nDouglas Eck <douglas.eck@gmail.com> eckdoug@waits.local <eckdoug@waits.local>\nDmitrii Serdiuk <serdyuk.dmitriy@gmail.com> dima <serdyuk.dmitriy@gmail.com>\nDmitrii Serdiuk <serdyuk.dmitriy@gmail.com> dmitriy-serdyuk <serdyuk.dmitriy@gmail.com>\nDmitrii Serdiuk <serdyuk.dmitriy@gmail.com> serdyuk <serdyuk.dmitriy@gmail.com>\nDumitru Erhan <dumitru.erhan@gmail.com> dumitru@deepnets.mtv.corp.google.com <dumitru@deepnets.mtv.corp.google.com>\nDumitru Erhan <dumitru.erhan@gmail.com> erhandum@bikat.iro.umontreal.ca <erhandum@bikat.iro.umontreal.ca>\nDzmitry Bahdanau <dimabgv@gmail.com> rizar <dimabv@tut.by>\nEric Hunsberger <hunse@ctn> hunse <hunse@ctn>\nEthan Buchman <ebuchman@uoguelph.ca> ebuchman <ebuchman@uoguelph.ca>\nEvelyn Mitchell <efm-github@linsomniac.com> evelynmitchell <efm-github@linsomniac.com>\nFaruk Ahmed <faruk.ahmed.91@gmail.com> Faruk Ahmed <ahmedfar@bart7.iro.umontreal.ca>\nFaruk Ahmed <faruk.ahmed.91@gmail.com> Faruk Ahmed <ahmedfar@elisa1.iro.umontreal.ca>\nFaruk Ahmed <faruk.ahmed.91@gmail.com> Faruk Ahmed <ahmedfar@eos3.iro.umontreal.ca>\nFaruk Ahmed <faruk.ahmed.91@gmail.com> Faruk Ahmed <ahmedfar@kepler2.iro.umontreal.ca>\nFaruk Ahmed <faruk.ahmed.91@gmail.com> Faruk Ahmed <ahmedfar@kepler3.iro.umontreal.ca>\nFaruk Ahmed <faruk.ahmed.91@gmail.com> Faruk Ahmed <ahmedfar@ceylon.iro.umontreal.ca>\nFaruk Ahmed <faruk.ahmed.91@gmail.com> Faruk Ahmed <ahmedfar@leto21.iro.umontreal.ca>\nFaruk Ahmed <faruk.ahmed.91@gmail.com> Faruk Ahmed <ahmedfar@leto23.iro.umontreal.ca>\nFaruk Ahmed <faruk.ahmed.91@gmail.com> Faruk-Ahmed <faruk.ahmed.91@gmail.com>\nFei Wang <fay96816@gmail.com> fay <fay96816@gmail.com>\nFrancesco Visin <fvisin@gmail.com> Francesco <fvisin@users.noreply.github.com>\nFrancesco Visin <fvisin@gmail.com> fvisin <fvisin@gmail.com>\nFrancois Savard <devnull@localhost> fsavard <devnull@localhost>\nFrederic Bastien <nouiz@nouiz.org> Frederic Bastien <bastienf@briaree1.rqchp.qc.ca>\nFrederic Bastien <nouiz@nouiz.org> Frederic Bastien <Frederic Bastien>\nFrederic Bastien <nouiz@nouiz.org> Frederic Bastien <bastienf@iro.umontreal.ca>\nFrederic Bastien <nouiz@nouiz.org> Frédéric Bastien <nouiz@nouiz.org>\nFrederic Bastien <nouiz@nouiz.org> Nouiz <nouiz@Nouiz-lap-ub.(none)>\nFrederic Bastien <nouiz@nouiz.org> Nouiz <nouiz@nouiz.org>\nFrederic Bastien <nouiz@nouiz.org> bastienf@bikat.iro.umontreal.ca <bastienf@bikat.iro.umontreal.ca>\nFrederic Bastien <nouiz@nouiz.org> bastienf@ldapk3.scinet.utoronto.ca <bastienf@ldapk3.scinet.utoronto.ca>\nFrederic Bastien <nouiz@nouiz.org> nouiz <nouiz@nouiz.org>\nFrederic Bastien <nouiz@nouiz.org> Frederic <nouiz@nouiz.org>\nFrederic Bastien <nouiz@nouiz.org> Frédéric Bastien <frederic.bastien@gmail.com>\nFrederic Bastien <nouiz@nouiz.org> theano-bot <frederic.bastien.1@umontreal.ca>\nGennadiy Tupitsin <genichyar@genichyar.com> genichyar <genichyar@genichyar.com>\nGhislain Antony Vaillant <ghisvail@gmail.com> Ghislain Antony Vaillant <ghisvail@users.noreply.github.com>\nGokula Krishnan <gokul.uf@gmail.com> Gokul <gokul.uf@gmail.com>\nGrégoire Mesnil <gregoire.mesnil@gmail.com> Grégoire <gregoire.mesnil@laposte.net>\nGrégoire Mesnil <gregoire.mesnil@gmail.com> Grégoire <gregoire.mesnil@gmail.com>\nGuillaume Alain <guillaume.alain.umontreal@gmail.com> Guillaume Alain <gyomalin@gmail.com>\nGuillaume Desjardins <guillaume.desjardins@gmail.com> desjagui <devnull@localhost>\nGuillaume Desjardins <guillaume.desjardins@gmail.com> desjagui@atchoum.iro.umontreal.ca <desjagui@atchoum.iro.umontreal.ca>\nGuillaume Desjardins <guillaume.desjardins@gmail.com> desjagui@opale.iro.umontreal.ca <desjagui@opale.iro.umontreal.ca>\nGuillaume Desjardins <guillaume.desjardins@gmail.com> gdesjardins <devnull@localhost>\nGuillaume Desjardins <guillaume.desjardins@gmail.com> tutorial/debug_faq.txt <devnull@localhost>\ngw0 [http://gw.tnode.com/] <gw.2015@tnode.com> gw0 [http://gw.tnode.com/] <gw.2016@tnode.com>\nHani Almousli <hani.mousli@gmail.com> Hani <hani.mousli@gmail.com>\nHani Almousli <hani.mousli@gmail.com> HaniAlmousli <hani.mousli@gmail.com>\nHuy Nguyen <huy@huyng.com> huyng <huy@huyng.com>\nIan Goodfellow <goodfellow.ian@gmail.com> Ian Goodfellow <devnull@localhost>\nIan Goodfellow <goodfellow.ian@gmail.com> goodfeli <goodfellow.ian@gmail.com>\nIan Goodfellow <goodfellow.ian@gmail.com> Ian Goodfellow <goodfellow@google.com>\nIan Goodfellow <goodfellow.ian@gmail.com> Ian Goodfellow <ia3n@gryphon0.(none)>\nIban Harlouchet <iban.harlouchet@gmail.com> Iban Harlouchet <harlouci@eos3.iro.umontreal.ca>\nIulian Vlad Serban <julianserban@gmail.com> Iulian Vlad Serban <serbaniv@eos2.iro.umontreal.ca>\nIulian Vlad Serban <julianserban@gmail.com> Iulian Vlad Serban <serbaniv@bart7.iro.umontreal.ca>\nJakub Sygnowski <sygnowski@gmail.com> Jakub Sygnowski <sygi@google.com>\nJames Bergstra <james.bergstra@gmail.com> James Bergstra <bergstrj@iro.umontreal.ca>\nJames Bergstra <james.bergstra@gmail.com> bergstra@ip05.m <bergstra@ip05.m>\nJames Bergstra <james.bergstra@gmail.com> bergstra@mlp4.ais.sandbox <bergstra@mlp4.ais.sandbox>\nJames Bergstra <james.bergstra@gmail.com> bergstra@tikuanyin <bergstra@tikuanyin>\nJames Bergstra <james.bergstra@gmail.com> bergstrj@iro.umontreal.ca <bergstrj@iro.umontreal.ca>\nJames Bergstra <james.bergstra@gmail.com> bergstrj@lgcm <bergstrj@lgcm>\nJames Bergstra <james.bergstra@gmail.com> james@X40 <james@X40>\nJames Bergstra <james.bergstra@gmail.com> james@crane <james@crane>\nJames Bergstra <james.bergstra@gmail.com> james@mackie <james@mackie>\nJames Bergstra <james.bergstra@gmail.com> james@x40.unstable <james@x40.unstable>\nJames Bergstra <james.bergstra@gmail.com> test_rng_mrg.py <devnull@localhost>\nJan Schlüter <github@jan-schlueter.de> f0k <github@jan-schlueter.de>\nJeremiah Lowin <jlowin@lowindata.com> Jeremiah Lowin <jlowin@gmail.com>\nJeremiah Lowin <jlowin@lowindata.com> Jeremiah Lowin <jlowin@iHal.local>\nJeremie Tanguay <tanguaj@iro.umontreal.ca> Tanjay94 <you@yourdomain.example.com>\nJeremie Tanguay <tanguaj@iro.umontreal.ca> Jeremie Tanguay <tanguaj@bart4.iro.umontreal.ca>\nJesse Livezey <jesse.livezey@berkeley.edu> JesseLivezey <jesse.livezey@gmail.com>\nJesse Livezey <jesse.livezey@berkeley.edu> Jesse Livezey <jesse.livezey@gmail.com>\nJoão Victor Tozatti Risso <joaovictortr@gmail.com> João Victor Risso <joaovictor.risso@gmail.com>\nJoão Victor Tozatti Risso <joaovictortr@gmail.com> João Victor Tozatti Risso <joaovictor.risso@gmail.com>\nJohn Salvatier <jsalvatier@gmail.com> jsalvatier <jsalvatier@gmail.com>\nJohn Salvatier <jsalvatier@gmail.com> john salvatier <jsalvatier@gmail.com>\nJohn Schulman <john.d.schulman@gmail.com> joschu <john.d.schulman@gmail.com>\njojolalpin <jojolalpin@gmail.com> jojolalpin <jojolalpin gmail>\nJoseph Turian <turian@iro.umontreal.ca> Joseph Turian <turian@gmail.com>\nJoseph Turian <turian@iro.umontreal.ca> turian@grenat.iro.umontreal.ca <turian@grenat.iro.umontreal.ca>\nJoseph Turian <turian@iro.umontreal.ca> turian@lgcm <turian@lgcm>\nJoseph Turian <turian@iro.umontreal.ca> turian@lsvm.iro.umontreal.ca <turian@lsvm.iro.umontreal.ca>\nJoseph Turian <turian@iro.umontreal.ca> turian@rubis.iro.umontreal.ca <turian@rubis.iro.umontreal.ca>\nJoseph Turian <turian@iro.umontreal.ca> turianjo@is23.m <turianjo@is23.m>\nJosh Bleecher Snyder <josharian@gmail.com> Josh Bleecher Snyder <josharian+github@gmail.com>\nJörg Bornschein <jb@capsec.org> Joerg Bornschein <bornschein@fias.uni-frankfurt.de>\nKarthik Karanth <karanth.karthik@gmail.com> medakk <karanth.karthik@gmail.com>\nKelvin Xu <kelvin.xu@umontreal.ca> Kelvin Xu <iamkelvinxu@gmail.com>\nKelvin Xu <kelvin.xu@umontreal.ca> kelvinxu <iamkelvinxu@gmail.com>\nKelvin Xu <kelvin.xu@umontreal.ca> Kelvin Xu <xukelvin@leto01.iro.umontreal.ca>\nKelvin Xu <kelvin.xu@umontreal.ca> Kelvin Xu <xukelvin@eos1.iro.umontreal.ca>\nKelvin Xu <kelvin.xu@umontreal.ca> Kelvin Xu <xukelvin@eos16.iro.umontreal.ca>\nKelvin Xu <kelvin.xu@umontreal.ca> Kelvin Xu <xukelvin@bart5.iro.umontreal.ca>\nKelvin Xu <kelvin.xu@umontreal.ca> Kelvin Xu <xukelvin@eos13.iro.umontreal.ca>\nKelvin Xu <kelvin.xu@umontreal.ca> Kelvin Xu <xukelvin@eos7.iro.umontreal.ca>\nKelvin Xu <kelvin.xu@umontreal.ca> Kelvin Xu <xukelvin@eos18.iro.umontreal.ca>\nKelvin Xu <kelvin.xu@umontreal.ca> Kelvin Xu <xukelvin@eos20.iro.umontreal.ca>\nKv Manohar <kvmanohar22@gmail.com> kvmanohar22 <kvmanohar22@gmail.com>\nKyung Hyun Cho <cho.k.hyun@gmail.com> Kyunghyun Cho <kyunghyuncho@Kyunghyuns-MacBook-Pro.local>\nKyung Hyun Cho <cho.k.hyun@gmail.com> Kyunghyun Cho <kyunghyuncho@kyunghyuns-mbp.sf.umontreal.ca>\nLi Yao <yaoli.email@gmail.com> Li Yao <li.yao@umontreal.ca>\nLi Yao <yaoli.email@gmail.com> Li Yao <yaoli@iro>\nLi Yao <yaoli.email@gmail.com> yaoli <yaoli.email@gmail.com>\nLi Yao <yaoli.email@gmail.com> Li <you@yourdomain.example.com>\nLiwei Cai <cai_lw@126.com> cai-lw <cai_lw@126.com>\nLucas Beyer <lucasb.eyer.be@gmail.com> lucasb-eyer <lucasb.eyer.be@gmail.com>\nLudwig Schmidt-Hackenberg <ludwig@iupr.com> Ludwig Schmidt-Hackenberg <ludwig@schmidt-hackenberg.net>\nLuke Metz <luke.metz@students.olin.edu> = <luke.metz@students.olin.edu>\nMarkus Roth <markus.roth@herr-biber.de> Markus Roth <mail@rothmark.us>\nMathieu Germain <mathieu.germain@gmail.com> Mathieu Germain <mathieu.germain2@usherbrooke.ca>\nMehdi Mirza <memirzamo@gmail.com> Mehdi Mirza <memimo@users.noreply.github.com>\nMehdi Mirza <memirzamo@gmail.com> memimo <memirzamo@gmail.com>\nMohammed Affan <affanv14@gmail.com> affan <affanv14@gmail.com>\nMohammed Affan <affanv14@gmail.com> affanv14 <affanv14@gmail.com>\nMohammed Affan <affanv14@gmail.com> Ubuntu <ubuntu@ip-172-31-58-125.ec2.internal>\nMohammad Pezeshki <mohammadpz@gmail.com> Mohammad Pezeshki <mohammad@mohammads-mbp.sf.umontreal.ca>\nMoslem Kazemi <moslemk@gmail.com> Moslem Kazemi <moslemk@users.noreply.github.com>\nMoslem Kazemi <moslemk@gmail.com> Mo <moslemk@gmail.com>\nNan Rosemary Ke <rosemary.ke@west.cmu.edu> nke001 <rosemary.nan.ke@gmail.com>\nNicolas Ballas <ballas.n@gmail.com> Kcub <ballas@lrde.epita.fr>\nNicolas Ballas <ballas.n@gmail.com> ballasn <ballas.n@gmail.com>\nNicolas Boulanger-Lewandowski <nicolas_boulanger@hotmail.com> boulanni <nicolas_boulanger@hotmail.com>\nNicolas Pinto <pinto@alum.mit.edu> Nicolas Pinto <nicolas.pinto@gmail.com>\nOlivier Breuleux <breuleux@gmail.com> Olivier Breuleux <breuleuo@iro.umontreal.ca>\nOlivier Breuleux <breuleux@gmail.com> olivier@olivier-desktop <olivier@olivier-desktop>\nOlivier Breuleux <breuleux@gmail.com> olivier@ordinateur-de-olivier.local <olivier@ordinateur-de-olivier.local>\nOlivier Delalleau <delallea@iro> Olivier Delalleau <delallea@iro.umontreal.ca>\nOlivier Delalleau <delallea@iro> delallea <delallea@iro.umontreal.ca>\nOlivier Delalleau <delallea@iro> delallea@valhalla.apstat.com <delallea@valhalla.apstat.com>\nOrhan Firat <orhan.firat@ceng.metu.edu.tr> orhanf <orhan.firat@ceng.metu.edu.tr>\nPascal Lamblin <lamblinp@iro.umontreal.ca> lamblin <lamblinp@iro.umontreal.ca>\nPascal Lamblin <lamblinp@iro.umontreal.ca> lamblinp@lgcm <lamblinp@lgcm>\nPhilippe Hamel <hamel.phil@gmail.com> Philippe  Hamel <hamel.phil@gmail.com>\nPhilippe Hamel <hamel.phil@gmail.com> Philippe Hamel <higgsbosonh@hotmail.com>\nPierre-Antoine Manzagol <pierre.antoine.manzagol@gmail.com> Pierre-Antoine Manzagol <manzagop@iro>\nPierre Luc Carrier <carrier.pierreluc@gmail.com> --global <carrier.pierreluc@gmail.com>\nPierre Luc Carrier <carrier.pierreluc@gmail.com> Pierre Luc Carrier <carriepl@bart2.iro.umontreal.ca>\nPierre Luc Carrier <carrier.pierreluc@gmail.com> carriepl <carriepl@users.noreply.github.com>\nPierre Luc Carrier <carrier.pierreluc@gmail.com> Pierre Luc Carrier <carriepl@grincheux.iro.umontreal.ca>\nPierre Luc Carrier <carrier.pierreluc@gmail.com> Pierre Luc Carrier <carriepl@leprof.iro.umontreal.ca>\nPierre Luc Carrier <carrier.pierreluc@gmail.com> Pierre Luc Carrier <carriepl@chai.iro.umontreal.ca>\nPierre Luc Carrier <carrier.pierreluc@gmail.com> Pierre Luc Carrier <carriepl@eos3.iro.umontreal.ca>\nPierre Luc Carrier <carrier.pierreluc@gmail.com> Pierre Luc Carrier <carriepl@bart2.iro.umontreal.ca>\nPierre Luc Carrier <carrier.pierreluc@gmail.com> Pierre Luc Carrier <carriepl@bart3.iro.umontreal.ca>\nPierre Luc Carrier <carrier.pierreluc@gmail.com> pl <carrier.pierreluc@gmail.com>\nPierre Luc Carrier <carrier.pierreluc@gmail.com> carriepl <carrier.pierreluc@gmail.com>\nRamana Subramanyam <vxrram95@gmail.com> sentient07 <vxrram95@gmail.com>\nRamana Subramanyam <vxrram95@gmail.com> Ramana.S <vxrram95@gmail.com>\nRami Al-Rfou' <rmyeid@gmail.com> Rami Al-Rfou <rmyeid@gmail.com>\nRaul Chandias Ferrari <devnull@localhost> chandiar <devnull@localhost>\nRazvan Pascanu <r.pascanu@gmail.com> Razvan Pascanu <pascanur@iro>\nRazvan Pascanu <r.pascanu@gmail.com> Razvan Pascanu <rman@rman-Dell-System-XPS-L502X.(none)>\nRazvan Pascanu <r.pascanu@gmail.com> Razvan Pascanu <rman@rman-pad.(none)>\nRazvan Pascanu <r.pascanu@gmail.com> pascanur@simplet.iro.umontreal.ca <pascanur@simplet.iro.umontreal.ca>\nRazvan Pascanu <r.pascanu@gmail.com> rman@rpad <rman@rpad>\nReyhane Askari <r.askari.hemmat@gmail.com> Reyhane Askari <ReyhaneAskari@users.noreply.github.com>\nRoy Xue <xljroy@gmail.com> Lijun Xue <xljroy@gmail.com>\nRuslana Makovetsky <ruslana@cim.mcgill.ca> ruslanagit <ruslana@cim.mcgill.ca>\nSander Dieleman <sanderdieleman@gmail.com> benanne <sanderdieleman@gmail.com>\nSebastian Berg <sebastian@sipsolutions.net> seberg <sebastian@sipsolutions.net>\nSebastien Jean <jeasebas@iro.umontreal.ca> sebastien <jeasebas@iro.umontreal.ca>\nSebastien Jean <jeasebas@iro.umontreal.ca> sebastien-j <jeasebas@iro.umontreal.ca>\nSebastien Jean <jeasebas@iro.umontreal.ca> sebastien-j <sebastien.jean@mail.mcgill.ca>\nSimon Lefrancois <simon.lefrancois@umontreal.ca> slefrancois <simon.lefrancois@umontreal.ca>\nSimon Lefrancois <simon.lefrancois@umontreal.ca> Simon Lefrancois <lefransi@iro.umontreal.ca>\nSimon Lefrancois <simon.lefrancois@umontreal.ca> Jenkins <jenkins@milaburger.iro.umontreal.ca>\nSimon Lefrancois <simon.lefrancois@umontreal.ca> mila <mila@earlgrey.iro.umontreal.ca>\nSina Honari <honaris@iro.umontreal.ca> SinaHonari <sina2222@gmail.com>\nSina Honari <honaris@iro.umontreal.ca> Sina Honari <honaris@eos21.iro.umontreal.ca>\nSina Honari <honaris@iro.umontreal.ca> Sina Honari <sina.honari@gmail.com>\nSøren Kaae Sønderby <skaaesonderby@gmail.com> skaae <skaaesonderby@gmail.com>\nSteven Bocco <stevenbocco@gmail.com> notoraptor <stevenbocco@gmail.com>\nSteven Bocco <stevenbocco@gmail.com> notoraptor <notoraptor@users.noreply.github.com>\nSteven Bocco <stevenbocco@gmail.com> Seton Steven Bocco <boccoset@elisa1.iro.umontreal.ca>\nSteven Bocco <stevenbocco@gmail.com> Seton Steven Bocco <boccoset@leto01.iro.umontreal.ca>\nSteven Bocco <stevenbocco@gmail.com> Seton Steven Bocco <boccoset@leto15.iro.umontreal.ca>\nSteven Bocco <stevenbocco@gmail.com> Seton Steven Bocco <boccoset@leto51.iro.umontreal.ca>\nSteven Pigeon <pigeon@iro.umontreal.ca> steven-pigeon <pigeon@iro.umontreal.ca>\nThomas George <tfjgeorge@gmail.com> Thomas George <georgeth@helios1.helios>\nThomas Wiecki <thomas.wiecki@gmail.com> twiecki <thomas.wiecki@gmail.com>\nValentin Bisson <valentin.bisson@umontreal.ca> onze <onzeonline@gmail.com>\nXavier Bouthillier <xavier.bouthillier@gmail.com> Xavier Bouthillier <xavier.bouthillier@umontreal.ca>\nXavier Bouthillier <xavier.bouthillier@gmail.com> Xavier Bouthillier/ <xavier.bouthillier@gmail.com>\nXavier Glorot <glorotxa@iro.umontreal.ca> glorotxa <glorotxa@iro.umontreal.ca>\nXavier Glorot <glorotxa@iro.umontreal.ca> glorotxa@timide.iro.umontreal.ca <glorotxa@timide.iro.umontreal.ca>\nVincent Dumoulin <vi.dumoulin@gmail.com> vdumoulin <vi.dumoulin@gmail.com>\nVincent Michalski <vincent.michalski@umontreal.ca> Vincent Michalski <vmichals@rz.uni-frankfurt.de>\nVitaliy Kurlin <vitaliykurin@gmail.com> yobibyte <vitaliykurin@gmail.com>\nVivek Kulkarni <viveksck@gmail.com> Vivek Kulkarni <vvkulkarni@cs.stonybrook.edu>\nWei Li <kuantkid@gmail.com> kuantkid <kuantkid@gmail.com>\nYann N. Dauphin <yann@dauphin.io> Yann N. Dauphin <dhaemon@gmail.com>\nYaroslav Ganin <ganin@skoltech.ru> Yaroslav Ganin <ganin@skolkovotech.ru>\nYikang Shen <yikang.shn@gmail.com> yikang <yikang.shn@gmail.com>\nYoshua Bengio <bengioy@iro.umontreal.ca> bengioy@bengio-mac.local <bengioy@bengio-mac.local>\nZiye Fan <fanziye.cis@gmail.com> FanZiye(t13m) <fanziye.cis@gmail.com>\nZhouhan LIN <lin.zhouhan@gmail.com> hantek <lin.zhouhan@gmail.com>\nZhouhan LIN <lin.zhouhan@gmail.com> Zhouhan LIN <hantek@Zhouhans-MacBook-Pro.local>\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 6.3359375,
          "content": "# After changing this file, check it on:\n# http://lint.travis-ci.org/\nsudo: false\ncache:\n  directories:\n    - $HOME/.cache/pip\n    - $HOME/.theano\n    - $HOME/download # Sufficient to add miniconda.sh to TRAVIS cache.\n    - $HOME/miniconda2 # Add the installation to TRAVIS cache.\n\n\nlanguage: python\n# For now, Python versions have to be listed in the \"jobs\" matrix\n\n# NB:\n# In before_install and install sections below,\n# some codes have been moved to separate files\n# to better handle if-else shell syntax\n# for multiple lines. New files are in\n# new folder \".travis\".\n\n# command to install dependencies\nbefore_install:\n  - ./.travis/travis_before_install.sh\n  - export PATH=/home/travis/miniconda2/bin:$PATH\n\naddons:\n  apt_packages:\n   - texlive-latex-recommended\n   - texlive-latex-extra\n   - texlive-fonts-recommended\n   - dvipng\n\ninstall:\n  - ./.travis/travis_install.sh\n  - source activate pyenv\n  - if [[ $TRAVIS_PYTHON_VERSION == '2.7' ]]; then pip install pydot; else pip install pydot-ng; fi\n  - pip install . --no-deps --upgrade\n  - pip install flake8-future-import parameterized\n  - if [[ $TRAVIS_PYTHON_VERSION != '3.4' ]]; then pip install sphinx_rtd_theme; fi\n  # nose-exclude plugin allow us to tell nosetests to exclude folder with --exclude-dir=path/to/directory.\n  - pip install nose-exclude nose-timer\n  - if [[ $NUMPY_VERSION == '1.13.1' ]]; then conda install --yes -q scipy=0.19.1; else conda install --yes -q scipy=0.14; fi  # Try to reinstall it to fix the problem\n\njobs:\n  include:\n    # define prototype for doctest\n    - &doctest\n      stage: doc\n      python: \"2.7\"\n      env: NUMPY_VERSION=1.9.1 DOC=1 PART=\"theano/tests/test_flake8.py\"\n    # re-use prototype, changing the Python version\n    - <<: *doctest\n      python: \"2.7\"\n      env: NUMPY_VERSION=1.13.1 DOC=1 PART=\"theano/tests/test_flake8.py\"\n#    - <<: *doctest\n#      python: \"3.4\"\n#      env: NUMPY_VERSION=1.9.1 DOC=1 PART=\"theano/tests/test_flake8.py\"\n    - <<: *doctest\n      python: \"3.6\"\n      env: NUMPY_VERSION=1.13.1 DOC=1 PART=\"theano/tests/test_flake8.py\"\n    - &normaltest\n      stage: test\n      python: \"2.7\"\n      env: NUMPY_VERSION=1.9.1 PART=\"theano/compat theano/compile theano/d3viz theano/gof theano/misc theano/sandbox theano/scalar theano/scan_module theano/tests -e test_flake8.py theano/typed_list\"\n    - <<: *normaltest\n      python: \"3.4\"\n      env: NUMPY_VERSION=1.9.1 PART=\"theano/compat theano/compile theano/d3viz theano/gof theano/misc theano/sandbox theano/scalar theano/scan_module theano/tests -e test_flake8.py theano/typed_list\"\n    - <<: *normaltest\n      env: NUMPY_VERSION=1.9.1 PART=\"theano/sparse theano/tensor --exclude-test=theano.tensor.tests.test_basic --exclude-test=theano.tensor.tests.test_elemwise --exclude-test=theano.tensor.tests.test_opt --exclude-dir=theano/tensor/nnet\"\n    - <<: *normaltest\n      env: NUMPY_VERSION=1.13.1 PART=\"theano/sparse theano/tensor --exclude-test=theano.tensor.tests.test_basic --exclude-test=theano.tensor.tests.test_elemwise --exclude-test=theano.tensor.tests.test_opt --exclude-dir=theano/tensor/nnet\"\n    - <<: *normaltest\n      python: \"3.4\"\n      env: NUMPY_VERSION=1.9.1 PART=\"theano/sparse theano/tensor --exclude-test=theano.tensor.tests.test_basic --exclude-test=theano.tensor.tests.test_elemwise --exclude-test=theano.tensor.tests.test_opt --exclude-dir=theano/tensor/nnet\"\n    - <<: *normaltest\n      python: \"3.6\"\n      env: NUMPY_VERSION=1.13.1 PART=\"theano/sparse theano/tensor --exclude-test=theano.tensor.tests.test_basic --exclude-test=theano.tensor.tests.test_elemwise --exclude-test=theano.tensor.tests.test_opt --exclude-dir=theano/tensor/nnet\"\n    - <<: *normaltest\n      env: NUMPY_VERSION=1.9.1 PART=\"theano/tensor/tests/test_basic.py\"\n    - <<: *normaltest\n      python: \"3.4\"\n      env: NUMPY_VERSION=1.9.1 PART=\"theano/tensor/tests/test_basic.py\"\n    - <<: *normaltest\n      env: NUMPY_VERSION=1.9.1 PART=\"theano/tensor/tests/test_elemwise.py theano/tensor/tests/test_opt.py\"\n    - <<: *normaltest\n      python: \"3.4\"\n      env: NUMPY_VERSION=1.9.1 PART=\"theano/tensor/tests/test_elemwise.py theano/tensor/tests/test_opt.py\"\n    - <<: *normaltest\n      env: NUMPY_VERSION=1.9.1 PART=\"theano/tensor/nnet -e test_abstract_conv.py\"\n    - <<: *normaltest\n      python: \"3.4\"\n      env: NUMPY_VERSION=1.9.1 PART=\"theano/tensor/nnet -e test_abstract_conv.py\"\n    - <<: *normaltest\n      env: NUMPY_VERSION=1.9.1 PART=\"theano/tensor/nnet/tests/test_abstract_conv.py\"\n    - <<: *normaltest\n      python: \"3.4\"\n      env: NUMPY_VERSION=1.9.1 PART=\"theano/tensor/nnet/tests/test_abstract_conv.py\"\n    - <<: *normaltest\n      env: NUMPY_VERSION=1.9.1 FAST_COMPILE=1 FLOAT32=1 PART=\"theano -e test_flake8.py --exclude-dir=theano/tensor/nnet --exclude-dir=theano/tensor/signal\"\n    - <<: *normaltest\n      python: \"3.4\"\n      env: NUMPY_VERSION=1.9.1 FAST_COMPILE=1 PART=\"theano -e test_flake8.py --exclude-dir=theano/tensor/nnet --exclude-dir=theano/tensor/signal\"\n    - <<: *normaltest\n      env: NUMPY_VERSION=1.9.1 FAST_COMPILE=1 FLOAT32=1 PART=\"theano/tensor/nnet\"\n    - <<: *normaltest\n      python: \"3.4\"\n      env: NUMPY_VERSION=1.9.1 FAST_COMPILE=1 PART=\"theano/tensor/nnet\"\n    - <<: *normaltest\n      env: NUMPY_VERSION=1.9.1 FAST_COMPILE=1 FLOAT32=1 PART=\"theano/tensor/signal\"\n    - <<: *normaltest\n      python: \"3.4\"\n      env: NUMPY_VERSION=1.9.1 FAST_COMPILE=1 PART=\"theano/tensor/signal\"\n\nscript:\n  - if [[ $FAST_COMPILE == \"1\" ]]; then export THEANO_FLAGS=$THEANO_FLAGS,mode=FAST_COMPILE; fi\n  - if [[ $FLOAT32 == \"1\" ]]; then export THEANO_FLAGS=$THEANO_FLAGS,floatX=float32; fi\n  - export THEANO_FLAGS=$THEANO_FLAGS,warn.ignore_bug_before=all,on_opt_error=raise,on_shape_error=raise,gcc.cxxflags=-pipe\n  - export MKL_THREADING_LAYER=GNU\n  - export MKL_NUM_THREADS=1\n  - export OMP_NUM_THREADS=1\n  - python --version\n  - uname -a\n  - free -m\n  - df -h\n  - ulimit -a\n  - echo \"$PART\"\n# Print information to help debug problems\n  - python -c 'import numpy; print(numpy.__version__)'\n  - python -c 'import theano; print(theano.__version__)'\n  - python -c 'import theano; print(theano.config.__str__(print_doc=False))'\n  - python -c 'import theano; assert(theano.config.blas.ldflags != \"\")'\n# Run tests for the given part\n  - theano-nose -v --with-timer --timer-top-n 10 $PART\n  - if [[ $DOC == \"1\" ]]; then python doc/scripts/docgen.py --nopdf --check; fi\n  - if [[ $DOC == \"1\" ]]; then python doc/scripts/docgen.py --test --check; fi\n\nafter_failure:\n  - cat /home/travis/.pip/pip.log\n"
        },
        {
          "name": ".travis",
          "type": "tree",
          "content": null
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.1328125,
          "content": "If you want to contribute to Theano, have a look at the instructions here:\nhttp://deeplearning.net/software/theano/dev_start_guide.html\n"
        },
        {
          "name": "DESCRIPTION.txt",
          "type": "blob",
          "size": 1.111328125,
          "content": "Theano is a Python library that allows you to define, optimize, and efficiently evaluate mathematical expressions involving multi-dimensional arrays. It is built on top of NumPy_. Theano features:\n\n * **tight integration with NumPy:** a similar interface to NumPy's. numpy.ndarrays are also used internally in Theano-compiled functions.\n * **transparent use of a GPU:** perform data-intensive computations up to 140x faster than on a CPU (support for float32 only).\n * **efficient symbolic differentiation:** Theano can compute derivatives for functions of one or many inputs.\n * **speed and stability optimizations:** avoid nasty bugs when computing expressions such as log(1 + exp(x)) for large values of x.\n * **dynamic C code generation:** evaluate expressions faster.\n * **extensive unit-testing and self-verification:** includes tools for detecting and diagnosing bugs and/or potential problems.\n\nTheano has been powering large-scale computationally intensive scientific\nresearch since 2007, but it is also approachable enough to be used in the\nclassroom (IFT6266 at the University of Montreal).\n\n.. _NumPy: http://numpy.scipy.org/\n"
        },
        {
          "name": "EMAIL.txt",
          "type": "blob",
          "size": 3.5263671875,
          "content": "===========================\n Announcing Theano 0.5\n===========================\n\n## You can select and adapt one of the following templates.\n\n## Basic text for major version release:\n\nThis is a release for a major version, with lots of new\nfeatures, bug fixes, and some interface changes (deprecated or\npotentially misleading features were removed).\n\nUpgrading to Theano 0.5 is recommended for everyone, but you should first make\nsure that your code does not raise deprecation warnings with Theano 0.4.1.\nOtherwise, in one case the results can change. In other cases, the warnings are\nturned into errors (see below for details).\n\nFor those using the bleeding edge version in the\ngit repository, we encourage you to update to the `0.5` tag.\n\n\n## Basic text for major version release candidate:\n\nThis is a release candidate for a major version, with lots of new\nfeatures, bug fixes, and some interface changes (deprecated or\npotentially misleading features were removed).\n\nThe upgrade is recommended for developers who want to help test and\nreport bugs, or want to use new features now.  If you have updated\nto 0.5rc1, you are highly encouraged to update to 0.5rc2.\n\nFor those using the bleeding edge version in the\ngit repository, we encourage you to update to the `0.5rc2` tag.\n\n\n## Basic text for minor version release:\n\nTODO\n\n\n## Basic text for minor version release candidate:\n\nTODO\n\nWhat's New\n----------\n\n[Include the content of NEWS.txt here]\n\n\nDownload and Install\n--------------------\n\nYou can download Theano from http://pypi.python.org/pypi/Theano\n\nInstallation instructions are available at\nhttp://deeplearning.net/software/theano/install.html\n\nDescription\n-----------\n\nTheano is a Python library that allows you to define, optimize, and\nefficiently evaluate mathematical expressions involving\nmulti-dimensional arrays. It is built on top of NumPy. Theano\nfeatures:\n\n * tight integration with NumPy: a similar interface to NumPy's.\n   numpy.ndarrays are also used internally in Theano-compiled functions.\n * transparent use of a GPU: perform data-intensive computations much faster than on a CPU.\n * efficient symbolic differentiation: Theano can compute derivatives\n   for functions of one or many inputs.\n * speed and stability optimizations: avoid nasty bugs when computing\n   expressions such as log(1+ exp(x)) for large values of x.\n * dynamic C code generation: evaluate expressions faster.\n * extensive unit-testing and self-verification: includes tools for\n   detecting and diagnosing bugs and/or potential problems.\n\nTheano has been powering large-scale computationally intensive\nscientific research since 2007, but it is also approachable\nenough to be used in the classroom (IFT6266 at the University of Montreal).\n\nResources\n---------\n\nAbout Theano:\n\nhttp://deeplearning.net/software/theano/\n\nTheano-related projects:\n\nhttp://github.com/Theano/Theano/wiki/Related-projects\n\nAbout NumPy:\n\nhttp://numpy.scipy.org/\n\nAbout SciPy:\n\nhttp://www.scipy.org/\n\nMachine Learning Tutorial with Theano on Deep Architectures:\n\nhttp://deeplearning.net/tutorial/\n\nAcknowledgments\n---------------\n\n\n\nI would like to thank all contributors of Theano. For this particular\nrelease, many people have helped, notably (in alphabetical order):\n[Generate the list of commiters: git shortlog -s <previous_tag>...| cut -c8-]\n\nI would also like to thank users who submitted bug reports, notably:\n[TODO]\n\nAlso, thank you to all NumPy and Scipy developers as Theano builds on\ntheir strengths.\n\nAll questions/comments are always welcome on the Theano\nmailing-lists ( http://deeplearning.net/software/theano/#community )\n\n\n"
        },
        {
          "name": "HISTORY.txt",
          "type": "blob",
          "size": 146.384765625,
          "content": "\n.. _HISTORY:\n\n=================\nOld Release Notes\n=================\n\nTheano 1.0.4 (16th of January 2019)\n=====================================\n\nThis is a maintenance release of Theano, version ``1.0.4``, with no\nnew features, but some important bug fixes.\n\nWe recommend that everybody update to this version.\n\nHighlights (since 1.0.3):\n\n - Theano is now compatible with NumPy 1.16.\n\nA total of 10 people contributed to this release since ``1.0.3``:\n\n - wonghang\n - Frederic Bastien\n - Arnaud Bergeron\n - Duc Nguyen\n - Andrew Nelson\n - Björn Linse\n - Luis Mario Domenzain\n - Rebecca N. Palmer\n - Luciano Paz\n - Dan Foreman-Mackey\n\nTheano 1.0.3 (20th of September 2018)\n=====================================\n\nThis is a maintenance release of Theano, version ``1.0.3``, with no\nnew features, but some important bug fixes.\n\nWe recommend that everybody update to this version.\n\nHighlights (since 1.0.2):\n\n - Theano is now compatible with Python 3.7\n - Broadcasting for sparse dot products works correctly\n - Subtensor grads do not return int anymore\n\nA total of 5 people contributed to this release since ``1.0.2``:\n\n - Frederic Bastien\n - Arnaud Bergeron\n - Dmitry Mottl\n - Adrian Seyboldt\n - Thomas Wiecki\n\n\nTheano 1.0.2 (23rd of May, 2018)\n====================================\n\nThis is a maintenance release of Theano, version ``1.0.2``, with no\nnew features, but some important bug fixes.\n\nWe recommend that everybody update to this version.\n\nHighlights (since 1.0.1):\n\n - Theano should work under PyPy now (this is experimental).\n - Update for cuDNN 7.1 RNN API changes.\n - Fix for a crash related to mixed dtypes with cuDNN convolutions.\n - MAGMA should work in more cases without manual config.\n - Handle reductions with non-default accumulator dtype better on the GPU.\n - Improvements to the test suite so that it fails less often due to\n   random chance.\n\nA total of 6 people contributed to this release since ``1.0.1``:\n\n - Frederic Bastien\n - Steven Bocco\n - Jon Haygood\n - Arnaud Bergeron\n - Jordan Melendez\n - Desiree Vogt-Lee\n - Garming Sam\n - Pascal Lamblin\n - Vincent Dumoulin\n - Glexin\n - Simon Lefrancois\n\n\nTheano 1.0.1 (6th of December, 2017)\n====================================\n\nThis is a maintenance release of Theano, version ``1.0.1``, with no\nnew features, but some important bug fixes.\n\nWe recommend that everybody update to this version.\n\nHighlights (since 1.0.0):\n\n - Fixed compilation and improved float16 support for topK on GPU\n\n   - **NB**: topK support on GPU is experimental and may not work for\n             large input sizes on certain GPUs\n\n - Fixed cuDNN reductions when axes to reduce have size ``1``\n - Attempted to prevent re-initialization of the GPU in a child process\n - Fixed support for temporary paths with spaces in Theano initialization\n - Spell check pass on the documentation\n\nA total of 6 people contributed to this release since ``1.0.0``:\n\n - Frederic Bastien\n - Steven Bocco\n - Arnaud Bergeron\n - Sam Johnson\n - Edward Betts\n - Simon Lefrancois\n\n\nTheano 1.0.0 (15th of November, 2017)\n=====================================\n\nThis is a final release of Theano, version ``1.0.0``, with a lot of\nnew features, interface changes, improvements and bug fixes.\n\nWe recommend that everybody update to this version.\n\nHighlights (since 0.9.0):\n - Announcing that `MILA will stop developing Theano <https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ>`_\n - conda packages now available and updated in our own conda channel ``mila-udem``\n   To install it: ``conda install -c mila-udem theano pygpu``\n - Support NumPy ``1.13``\n - Support pygpu ``0.7``\n - Moved Python ``3.*`` minimum supported version from ``3.3`` to ``3.4``\n - Added conda recipe\n - Replaced deprecated package ``nose-parameterized`` with up-to-date package ``parameterized`` for Theano requirements\n - Theano now internally uses ``sha256`` instead of ``md5`` to work on systems that forbid ``md5`` for security reason\n - Removed old GPU backend ``theano.sandbox.cuda``. New backend ``theano.gpuarray`` is now the official GPU backend\n - Make sure MKL uses GNU OpenMP\n\n   - **NB**: Matrix dot product (``gemm``) with ``mkl`` from conda\n     could return wrong results in some cases. We have reported the problem upstream\n     and we have a work around that raises an error with information about how to fix it.\n\n - Improved elemwise operations\n\n   - Speed-up elemwise ops based on SciPy\n   - Fixed memory leaks related to elemwise ops on GPU\n\n - Scan improvements\n\n   - Speed up Theano scan compilation and gradient computation\n   - Added meaningful message when missing inputs to scan\n\n - Speed up graph toposort algorithm\n - Faster C compilation by massively using a new interface for op params\n - Faster optimization step, with new optional destroy handler\n - Documentation updated and more complete\n\n   - Added documentation for RNNBlock\n   - Updated ``conv`` documentation\n\n - Support more debuggers for ``PdbBreakpoint``\n - Many bug fixes, crash fixes and warning improvements\n\nA total of 71 people contributed to this release since 0.9.0, see list below.\n\nInterface changes:\n - Merged duplicated diagonal functions into two ops: ``ExtractDiag`` (extract a diagonal to a vector),\n   and ``AllocDiag`` (set a vector as a diagonal of an empty array)\n - Removed op ``ExtractDiag`` from ``theano.tensor.nlinalg``, now only in ``theano.tensor.basic``\n - Generalized ``AllocDiag`` for any non-scalar input\n - Added new parameter ``target`` for MRG functions\n - Renamed ``MultinomialWOReplacementFromUniform`` to ``ChoiceFromUniform``\n - Changed ``grad()`` method to ``L_op()`` in ops that need the outputs to compute gradient\n\n - Removed or deprecated Theano flags:\n\n   - ``cublas.lib``\n   - ``cuda.enabled``\n   - ``enable_initial_driver_test``\n   - ``gpuarray.sync``\n   - ``home``\n   - ``lib.cnmem``\n   - ``nvcc.*`` flags\n   - ``pycuda.init``\n\nConvolution updates:\n - Implemented separable convolutions for 2D and 3D\n - Implemented grouped convolutions for 2D and 3D\n - Added dilated causal convolutions for 2D\n - Added unshared convolutions\n - Implemented fractional bilinear upsampling\n - Removed old ``conv3d`` interface\n - Deprecated old ``conv2d`` interface\n\nGPU:\n - Added a meta-optimizer to select the fastest GPU implementations for convolutions\n - Prevent GPU initialization when not required\n - Added disk caching option for kernels\n - Added method ``my_theano_function.sync_shared()`` to help synchronize GPU Theano functions\n - Added useful stats for GPU in profile mode\n - Added Cholesky op based on ``cusolver`` backend\n - Added GPU ops based on `magma library <http://icl.cs.utk.edu/magma/software/>`_:\n   SVD, matrix inverse, QR, cholesky and eigh\n - Added ``GpuCublasTriangularSolve``\n - Added atomic addition and exchange for ``long long`` values in ``GpuAdvancedIncSubtensor1_dev20``\n - Support log gamma function for all non-complex types\n - Support GPU SoftMax in both OpenCL and CUDA\n - Support offset parameter ``k`` for ``GpuEye``\n - ``CrossentropyCategorical1Hot`` and its gradient are now lifted to GPU\n\n - cuDNN:\n\n   - Official support for ``v6.*`` and ``v7.*``\n   - Added spatial transformation operation based on cuDNN\n   - Updated and improved caching system for runtime-chosen cuDNN convolution algorithms\n   - Support cuDNN v7 tensor core operations for convolutions with runtime timed algorithms\n   - Better support and loading on Windows and Mac\n   - Support cuDNN v6 dilated convolutions\n   - Support cuDNN v6 reductions for contiguous inputs\n   - Optimized ``SUM(x^2)``, ``SUM(ABS(X))`` and ``MAX(ABS(X))`` operations with cuDNN reductions\n   - Added new Theano flags ``cuda.include_path``, ``dnn.base_path`` and ``dnn.bin_path``\n     to help configure Theano when CUDA and cuDNN can not be found automatically\n   - Extended Theano flag ``dnn.enabled`` with new option ``no_check`` to help speed up cuDNN importation\n   - Disallowed ``float16`` precision for convolution gradients\n   - Fixed memory alignment detection\n   - Added profiling in C debug mode (with theano flag ``cmodule.debug=True``)\n   - Added Python scripts to help test cuDNN convolutions\n   - Automatic addition of cuDNN DLL path to ``PATH`` environment variable on Windows\n\n - Updated ``float16`` support\n\n   - Added documentation for GPU float16 ops\n   - Support ``float16`` for ``GpuGemmBatch``\n   - Started to use ``float32`` precision for computations that don't support ``float16`` on GPU\n\nNew features:\n - Implemented truncated normal distribution with box-muller transform\n - Added ``L_op()`` overriding option for ``OpFromGraph``\n - Added NumPy C-API based fallback implementation for ``[sd]gemv_`` and ``[sd]dot_``\n - Implemented ``topk`` and ``argtopk`` on CPU and GPU\n - Implemented ``max()`` and ``min()`` functions for booleans and unsigned integers types\n - Added ``tensor6()`` and ``tensor7()`` in ``theano.tensor`` module\n - Added boolean indexing for sub-tensors\n - Added covariance matrix function ``theano.tensor.cov``\n - Added a wrapper for `Baidu's CTC <https://github.com/baidu-research/warp-ctc>`_ cost and gradient functions\n - Added scalar and elemwise CPU ops for modified Bessel function of order 0 and 1 from ``scipy.special``\n - Added Scaled Exponential Linear Unit (SELU) activation\n - Added sigmoid_binary_crossentropy function\n - Added tri-gamma function\n - Added ``unravel_index`` and ``ravel_multi_index`` functions on CPU\n - Added modes ``half`` and ``full`` for ``Images2Neibs`` ops\n - Implemented gradient for ``AbstractBatchNormTrainGrad``\n - Implemented gradient for matrix pseudoinverse op\n - Added new prop `replace` for ``ChoiceFromUniform`` op\n - Added new prop ``on_error`` for CPU ``Cholesky`` op\n - Added new Theano flag ``deterministic`` to help control how Theano optimize certain ops that have deterministic versions.\n   Currently used for subtensor Ops only.\n - Added new Theano flag ``cycle_detection`` to speed-up optimization step by reducing time spending in inplace optimizations\n - Added new Theano flag ``check_stack_trace`` to help check the stack trace during optimization process\n - Added new Theano flag ``cmodule.debug`` to allow a debug mode for Theano C code. Currently used for cuDNN convolutions only.\n - Added new Theano flag ``pickle_test_value`` to help disable pickling test values\n\nOthers:\n - Kept stack trace for optimizations in new GPU backend\n - Added deprecation warning for the softmax and logsoftmax vector case\n - Added a warning to announce that C++ compiler will become mandatory in next Theano release ``0.11``\n - Added ``R_op()`` for ``ZeroGrad``\n - Added description for rnnblock\n\nOther more detailed changes:\n - Fixed invalid casts and index overflows in ``theano.tensor.signal.pool``\n - Fixed gradient error for elemwise ``minimum`` and ``maximum`` when compared values are the same\n - Fixed gradient for ``ARange``\n - Removed ``ViewOp`` subclass during optimization\n - Removed useless warning when profile is manually disabled\n - Added tests for abstract conv\n - Added options for `disconnected_outputs` to Rop\n - Removed ``theano/compat/six.py``\n - Removed ``COp.get_op_params()``\n - Support of list of strings for ``Op.c_support_code()``, to help not duplicate support codes\n - Macro names provided for array properties are now standardized in both CPU and GPU C codes\n - Moved all C code files into separate folder ``c_code`` in every Theano module\n - Many improvements for Travis CI tests (with better splitting for faster testing)\n - Many improvements for Jenkins CI tests: daily testings on Mac and Windows in addition to Linux\n\nCommiters since 0.9.0:\n - Frederic Bastien\n - Steven Bocco\n - João Victor Tozatti Risso\n - Arnaud Bergeron\n - Mohammed Affan\n - amrithasuresh\n - Pascal Lamblin\n - Reyhane Askari\n - Alexander Matyasko\n - Shawn Tan\n - Simon Lefrancois\n - Adam Becker\n - Vikram\n - Gijs van Tulder\n - Faruk Ahmed\n - Thomas George\n - erakra\n - Andrei Costinescu\n - Boris Fomitchev\n - Zhouhan LIN\n - Aleksandar Botev\n - jhelie\n - xiaoqie\n - Tegan Maharaj\n - Matt Graham\n - Cesar Laurent\n - Gabe Schwartz\n - Juan Camilo Gamboa Higuera\n - Tim Cooijmans\n - Anirudh Goyal\n - Saizheng Zhang\n - Yikang Shen\n - vipulraheja\n - Florian Bordes\n - Sina Honari\n - Chiheb Trabelsi\n - Shubh Vachher\n - Daren Eiri\n - Joseph Paul Cohen\n - Laurent Dinh\n - Mohamed Ishmael Diwan Belghazi\n - Jeff Donahue\n - Ramana Subramanyam\n - Bogdan Budescu\n - Dzmitry Bahdanau\n - Ghislain Antony Vaillant\n - Jan Schlüter\n - Nan Jiang\n - Xavier Bouthillier\n - fo40225\n - mrTsjolder\n - wyjw\n - Aarni Koskela\n - Adam Geitgey\n - Adrian Keet\n - Adrian Seyboldt\n - Anmol Sahoo\n - Chong Wu\n - Holger Kohr\n - Jayanth Koushik\n - Lilian Besson\n - Lv Tao\n - Michael Manukyan\n - Murugesh Marvel\n - NALEPA\n - Rebecca N. Palmer\n - Zotov Yuriy\n - dareneiri\n - lrast\n - morrme\n - naitonium\n\n\nTheano 1.0.0rc1 (30th of October, 2017)\n=======================================\n\nThis release contains new features, improvements and bug fixes to prepare the upcoming release.\n\nWe recommend that every developer updates to this version.\n\nHighlights:\n - Make sure MKL uses GNU OpenMP\n\n   - **NB**: Matrix dot product (``gemm``) with ``mkl`` from conda\n     could return wrong results in some cases. We have reported the problem upstream\n     and we have a work around that raises an error with information about how to fix it.\n\n - Optimized ``SUM(x^2)``, ``SUM(ABS(X))`` and ``MAX(ABS(X))`` operations with cuDNN reductions\n - Added Python scripts to help test cuDNN convolutions\n - Fixed invalid casts and index overflows in ``theano.tensor.signal.pool``\n\nA total of 71 people contributed to this release since 0.9.0, see list below.\n\nCommiters since 0.9.0:\n - Frederic Bastien\n - Steven Bocco\n - João Victor Tozatti Risso\n - Arnaud Bergeron\n - Mohammed Affan\n - amrithasuresh\n - Pascal Lamblin\n - Reyhane Askari\n - Alexander Matyasko\n - Shawn Tan\n - Simon Lefrancois\n - Adam Becker\n - Vikram\n - Gijs van Tulder\n - Faruk Ahmed\n - Thomas George\n - erakra\n - Andrei Costinescu\n - Boris Fomitchev\n - Zhouhan LIN\n - Aleksandar Botev\n - jhelie\n - xiaoqie\n - Tegan Maharaj\n - Matt Graham\n - Cesar Laurent\n - Gabe Schwartz\n - Juan Camilo Gamboa Higuera\n - Tim Cooijmans\n - Anirudh Goyal\n - Saizheng Zhang\n - Yikang Shen\n - vipulraheja\n - Florian Bordes\n - Sina Honari\n - Chiheb Trabelsi\n - Shubh Vachher\n - Daren Eiri\n - Joseph Paul Cohen\n - Laurent Dinh\n - Mohamed Ishmael Diwan Belghazi\n - Jeff Donahue\n - Ramana Subramanyam\n - Bogdan Budescu\n - Dzmitry Bahdanau\n - Ghislain Antony Vaillant\n - Jan Schlüter\n - Nan Jiang\n - Xavier Bouthillier\n - fo40225\n - mrTsjolder\n - wyjw\n - Aarni Koskela\n - Adam Geitgey\n - Adrian Keet\n - Adrian Seyboldt\n - Anmol Sahoo\n - Chong Wu\n - Holger Kohr\n - Jayanth Koushik\n - Lilian Besson\n - Lv Tao\n - Michael Manukyan\n - Murugesh Marvel\n - NALEPA\n - Rebecca N. Palmer\n - Zotov Yuriy\n - dareneiri\n - lrast\n - morrme\n - naitonium\n\n\nTheano 0.10.0beta4 (16th of October, 2017)\n==========================================\n\nThis release contains new features, improvements and bug fixes to prepare the upcoming release candidate.\n\nWe recommend that every developer updates to this version.\n\nHighlights:\n - Announcing that `MILA will stop developing Theano <https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ>`_\n - Bug fixes, crash fixes, warning improvements and documentation updates\n\nA total of 70 people contributed to this release since 0.9.0, see list below.\n\nInterface changes:\n - Generalized ``AllocDiag`` for any non-scalar input\n\nConvolution updates:\n - Implemented fractional bilinear upsampling\n\ncuDNN (GPU):\n - Disallowed ``float16`` precision for convolution gradients\n - Fixed memory alignment detection\n - Added profiling in C debug mode (with theano flag ``cmodule.debug=True``)\n\nNew features:\n - Implemented truncated normal distribution with box-muller transform\n - Added ``L_op()`` overriding option for ``OpFromGraph``\n - Added NumPy C-API based fallback implementation for ``[sd]gemv_`` and ``[sd]dot_``\n\nOther more detailed changes:\n - Improved stack trace follow-up for GPU optimizations\n - Fixed gradient error for elemwise ``minimum`` and ``maximum`` when compared values are the same\n - Fixed gradient for ``ARange``\n - Removed ``ViewOp`` subclass during optimization\n\nCommiters since 0.9.0:\n - Frederic Bastien\n - João Victor Tozatti Risso\n - Arnaud Bergeron\n - Steven Bocco\n - Mohammed Affan\n - amrithasuresh\n - Pascal Lamblin\n - Reyhane Askari\n - Alexander Matyasko\n - Shawn Tan\n - Simon Lefrancois\n - Adam Becker\n - Vikram\n - Gijs van Tulder\n - Faruk Ahmed\n - Thomas George\n - erakra\n - Andrei Costinescu\n - Boris Fomitchev\n - Zhouhan LIN\n - Aleksandar Botev\n - jhelie\n - xiaoqie\n - Tegan Maharaj\n - Matt Graham\n - Cesar Laurent\n - Gabe Schwartz\n - Juan Camilo Gamboa Higuera\n - Tim Cooijmans\n - Anirudh Goyal\n - Saizheng Zhang\n - Yikang Shen\n - vipulraheja\n - Florian Bordes\n - Sina Honari\n - Chiheb Trabelsi\n - Shubh Vachher\n - Daren Eiri\n - Joseph Paul Cohen\n - Laurent Dinh\n - Mohamed Ishmael Diwan Belghazi\n - Jeff Donahue\n - Ramana Subramanyam\n - Bogdan Budescu\n - Dzmitry Bahdanau\n - Ghislain Antony Vaillant\n - Jan Schlüter\n - Nan Jiang\n - Xavier Bouthillier\n - fo40225\n - mrTsjolder\n - wyjw\n - Aarni Koskela\n - Adam Geitgey\n - Adrian Keet\n - Adrian Seyboldt\n - Anmol Sahoo\n - Chong Wu\n - Holger Kohr\n - Jayanth Koushik\n - Lilian Besson\n - Lv Tao\n - Michael Manukyan\n - Murugesh Marvel\n - NALEPA\n - Zotov Yuriy\n - dareneiri\n - lrast\n - morrme\n - naitonium\n\n\nTheano 0.10.0beta3 (20th of September, 2017)\n============================================\n\nThis release contains new features, improvements and bug fixes to prepare the upcoming release candidate.\n\nWe recommend that every developer updates to this version.\n\nHighlights:\n - conda packages now available and updated in our own conda channel ``mila-udem``.\n   To install it: ``conda install -c mila-udem -c mila-udem/label/pre theano pygpu``\n\n - Improved elemwise operations\n\n   - Speed-up elemwise ops based on SciPy\n   - Fixed memory leak related to elemwise ops on GPU\n\n - Fixed pygpu detection\n - Bug fixes, crash fixes, warning improvements and documentation updates\n\nA total of 69 people contributed to this release since 0.9.0, see list below.\n\nInterface changes:\n - Removed op ``ExtractDiag`` from ``theano.tensor.nlinalg``, now only in ``theano.tensor.basic``\n\nConvolution updates:\n - Added dilated causal convolutions for 2D\n\nNew features:\n - Implemented ``topk`` and ``argtopk`` on CPU and GPU\n - Added ``unravel_index`` and ``ravel_multi_index`` functions on CPU\n - Implemented ``max()`` and ``min()`` functions for booleans and unsigned integers types\n\nOthers:\n - Added ``R_op()`` for ``ZeroGrad``\n - Added description for rnnblock\n\nCommiters since 0.9.0:\n - Frederic Bastien\n - João Victor Tozatti Risso\n - Arnaud Bergeron\n - Steven Bocco\n - Mohammed Affan\n - amrithasuresh\n - Pascal Lamblin\n - Reyhane Askari\n - Alexander Matyasko\n - Simon Lefrancois\n - Adam Becker\n - Shawn Tan\n - Vikram\n - Gijs van Tulder\n - Thomas George\n - Andrei Costinescu\n - Faruk Ahmed\n - Boris Fomitchev\n - Zhouhan LIN\n - Aleksandar Botev\n - jhelie\n - xiaoqie\n - Tegan Maharaj\n - Matt Graham\n - Cesar Laurent\n - Gabe Schwartz\n - Juan Camilo Gamboa Higuera\n - Tim Cooijmans\n - Anirudh Goyal\n - Saizheng Zhang\n - Yikang Shen\n - vipulraheja\n - Florian Bordes\n - Sina Honari\n - erakra\n - Chiheb Trabelsi\n - Shubh Vachher\n - Daren Eiri\n - Joseph Paul Cohen\n - Laurent Dinh\n - Mohamed Ishmael Diwan Belghazi\n - Jeff Donahue\n - Ramana Subramanyam\n - Bogdan Budescu\n - Dzmitry Bahdanau\n - Ghislain Antony Vaillant\n - Jan Schlüter\n - Nan Jiang\n - Xavier Bouthillier\n - fo40225\n - wyjw\n - Aarni Koskela\n - Adam Geitgey\n - Adrian Keet\n - Adrian Seyboldt\n - Anmol Sahoo\n - Chong Wu\n - Holger Kohr\n - Jayanth Koushik\n - Lilian Besson\n - Lv Tao\n - Michael Manukyan\n - Murugesh Marvel\n - NALEPA\n - Zotov Yuriy\n - dareneiri\n - lrast\n - morrme\n - naitonium\n\n\nTheano 0.10.0beta2 (7th of September, 2017)\n===========================================\n\nThis release contains new features, improvements and bug fixes to prepare the upcoming release candidate.\n\nWe recommend that every developer updates to this version.\n\nHighlights:\n - Support NumPy ``1.13``\n - Support pygpu ``0.7``\n - Added conda recipe\n - Optional faster optimization step with new destroy handler\n - Added documentation for RNNBlock\n - Bug fixes, crash fixes, warning improvements and documentation updates\n\nA total of 67 people contributed to this release since 0.9.0, see list below.\n\nInterface changes:\n - Added new parameter ``target`` for MRG functions\n\nConvolution updates:\n - Added unshared convolutions\n - Added 3D separable convolutions\n - Added 3D grouped convolutions\n - Removed old ``conv3d`` interface\n - Deprecated old ``conv2d`` interface\n - Updated ``conv`` documentation\n\nGPU:\n - Added a meta-optimizer to select the fastest GPU implementations for convolutions\n\n - cuDNN:\n\n   - Official support for ``v6.*`` and ``v7.*``, support for ``v5.*`` will be removed in next release\n   - Added spatial transformation operation based on cuDNN\n   - Updated and improved caching system for runtime-chosen cuDNN convolution algorithms\n   - Support cuDNN v7 tensor core operations for convolutions with runtime timed algorithms\n   - Restricted cuDNN reductions to contiguous inputs\n   - Automatic addition of cuDNN DLL path to ``PATH`` environment variable on Windows\n\nNew features:\n - Added ``tensor6()`` and ``tensor7()`` in ``theano.tensor`` module\n - Added boolean indexing for sub-tensors\n - Added covariance matrix function ``theano.tensor.cov``\n - Added new Theano flag ``pickle_test_value`` to help disable pickling test values\n\nOthers:\n - Kept stack trace for optimizations in new GPU backend\n\nOther more detailed changes:\n - Moved all C code files into separate folder ``c_code`` in every Theano module\n - Improvements for Jenkins tests\n\nCommiters since 0.9.0:\n - Frederic Bastien\n - João Victor Tozatti Risso\n - Arnaud Bergeron\n - Steven Bocco\n - Mohammed Affan\n - amrithasuresh\n - Pascal Lamblin\n - Reyhane Askari\n - Alexander Matyasko\n - Simon Lefrancois\n - Shawn Tan\n - Gijs van Tulder\n - Thomas George\n - Vikram\n - Andrei Costinescu\n - Faruk Ahmed\n - Boris Fomitchev\n - Zhouhan LIN\n - Aleksandar Botev\n - jhelie\n - xiaoqie\n - Tegan Maharaj\n - Matt Graham\n - Cesar Laurent\n - Gabe Schwartz\n - Juan Camilo Gamboa Higuera\n - Tim Cooijmans\n - Anirudh Goyal\n - Saizheng Zhang\n - vipulraheja\n - Florian Bordes\n - Sina Honari\n - Yikang Shen\n - erakra\n - Chiheb Trabelsi\n - Shubh Vachher\n - Daren Eiri\n - Joseph Paul Cohen\n - Laurent Dinh\n - Mohamed Ishmael Diwan Belghazi\n - Jeff Donahue\n - Ramana Subramanyam\n - Bogdan Budescu\n - Dzmitry Bahdanau\n - Ghislain Antony Vaillant\n - Jan Schlüter\n - Xavier Bouthillier\n - fo40225\n - Aarni Koskela\n - Adam Becker\n - Adam Geitgey\n - Adrian Keet\n - Adrian Seyboldt\n - Anmol Sahoo\n - Chong Wu\n - Holger Kohr\n - Jayanth Koushik\n - Lilian Besson\n - Lv Tao\n - Michael Manukyan\n - Murugesh Marvel\n - NALEPA\n - Zotov Yuriy\n - dareneiri\n - lrast\n - morrme\n - wyjw\n\n\nTheano 0.10.0beta1 (9th of August, 2017)\n========================================\n\nThis release contains a lot of bug fixes, improvements and new features to prepare the upcoming release candidate.\n\nWe recommend that every developer updates to this version.\n\nHighlights:\n - Moved Python 3.* minimum supported version from 3.3 to 3.4\n - Replaced deprecated package ``nose-parameterized`` with up-to-date package ``parameterized`` for Theano requirements\n - Theano now internally uses ``sha256`` instead of ``md5`` to work on systems that forbide ``md5`` for security reason\n - Removed old GPU backend ``theano.sandbox.cuda``. New backend ``theano.gpuarray`` is now the official GPU backend\n - Support more debuggers for ``PdbBreakpoint``\n\n - Scan improvements\n\n   - Speed up Theano scan compilation and gradient computation\n   - Added meaningful message when missing inputs to scan\n\n - Speed up graph toposort algorithm\n - Faster C compilation by massively using a new interface for op params\n - Faster optimization step\n - Documentation updated and more complete\n - Many bug fixes, crash fixes and warning improvements\n\nA total of 65 people contributed to this release since 0.9.0, see list below.\n\nInterface changes:\n - Merged duplicated diagonal functions into two ops: ``ExtractDiag`` (extract a diagonal to a vector),\n   and ``AllocDiag`` (set a vector as a diagonal of an empty array)\n - Renamed ``MultinomialWOReplacementFromUniform`` to ``ChoiceFromUniform``\n\n - Removed or deprecated Theano flags:\n\n   - ``cublas.lib``\n   - ``cuda.enabled``\n   - ``enable_initial_driver_test``\n   - ``gpuarray.sync``\n   - ``home``\n   - ``lib.cnmem``\n   - ``nvcc.*`` flags\n   - ``pycuda.init``\n\n - Changed ``grad()`` method to ``L_op()`` in ops that need the outputs to compute gradient\n\nConvolution updates:\n - Extended Theano flag ``dnn.enabled`` with new option ``no_check`` to help speed up cuDNN importation\n - Implemented separable convolutions\n - Implemented grouped convolutions\n\nGPU:\n - Prevent GPU initialization when not required\n - Added disk caching option for kernels\n - Added method ``my_theano_function.sync_shared()`` to help synchronize GPU Theano functions\n - Added useful stats for GPU in profile mode\n - Added Cholesky op based on ``cusolver`` backend\n - Added GPU ops based on `magma library <http://icl.cs.utk.edu/magma/software/>`_:\n   SVD, matrix inverse, QR, cholesky and eigh\n - Added ``GpuCublasTriangularSolve``\n - Added atomic addition and exchange for ``long long`` values in ``GpuAdvancedIncSubtensor1_dev20``\n - Support log gamma function for all non-complex types\n - Support GPU SoftMax in both OpenCL and CUDA\n - Support offset parameter ``k`` for ``GpuEye``\n - ``CrossentropyCategorical1Hot`` and its gradient are now lifted to GPU\n\n - Better cuDNN support\n\n   - Official support for ``v5.*`` and ``v6.*``\n   - Better support and loading on Windows and Mac\n   - Support cuDNN v6 dilated convolutions\n   - Support cuDNN v6 reductions\n   - Added new Theano flags ``cuda.include_path``, ``dnn.base_path`` and ``dnn.bin_path``\n     to help configure Theano when CUDA and cuDNN can not be found automatically.\n\n - Updated ``float16`` support\n\n   - Added documentation for GPU float16 ops\n   - Support ``float16`` for ``GpuGemmBatch``\n   - Started to use ``float32`` precision for computations that don't support ``float16`` on GPU\n\nNew features:\n - Added a wrapper for `Baidu's CTC <https://github.com/baidu-research/warp-ctc>`_ cost and gradient functions\n - Added scalar and elemwise CPU ops for modified Bessel function of order 0 and 1 from ``scipy.special``.\n - Added Scaled Exponential Linear Unit (SELU) activation\n - Added sigmoid_binary_crossentropy function\n - Added tri-gamma function\n - Added modes ``half`` and ``full`` for ``Images2Neibs`` ops\n - Implemented gradient for ``AbstractBatchNormTrainGrad``\n - Implemented gradient for matrix pseudoinverse op\n - Added new prop `replace` for ``ChoiceFromUniform`` op\n - Added new prop ``on_error`` for CPU ``Cholesky`` op\n - Added new Theano flag ``deterministic`` to help control how Theano optimize certain ops that have deterministic versions.\n   Currently used for subtensor Ops only.\n - Added new Theano flag ``cycle_detection`` to speed-up optimization step by reducing time spending in inplace optimizations\n - Added new Theano flag ``check_stack_trace`` to help check the stack trace during optimization process\n - Added new Theano flag ``cmodule.debug`` to allow a debug mode for Theano C code. Currently used for cuDNN convolutions only.\n\nOthers:\n - Added deprecation warning for the softmax and logsoftmax vector case\n - Added a warning to announce that C++ compiler will become mandatory in next Theano release ``0.11``\n\nOther more detailed changes:\n - Removed useless warning when profile is manually disabled\n - Added tests for abstract conv\n - Added options for `disconnected_outputs` to Rop\n - Removed ``theano/compat/six.py``\n - Removed ``COp.get_op_params()``\n - Support of list of strings for ``Op.c_support_code()``, to help not duplicate support codes\n - Macro names provided for array properties are now standardized in both CPU and GPU C codes\n - Started to move C code files into separate folder ``c_code`` in every Theano module\n - Many improvements for Travis CI tests (with better splitting for faster testing)\n - Many improvements for Jenkins CI tests: daily testings on Mac and Windows in addition to Linux\n\nCommiters since 0.9.0:\n - Frederic Bastien\n - Arnaud Bergeron\n - amrithasuresh\n - João Victor Tozatti Risso\n - Steven Bocco\n - Pascal Lamblin\n - Mohammed Affan\n - Reyhane Askari\n - Alexander Matyasko\n - Simon Lefrancois\n - Shawn Tan\n - Thomas George\n - Faruk Ahmed\n - Zhouhan LIN\n - Aleksandar Botev\n - jhelie\n - xiaoqie\n - Tegan Maharaj\n - Matt Graham\n - Cesar Laurent\n - Gabe Schwartz\n - Juan Camilo Gamboa Higuera\n - AndroidCloud\n - Saizheng Zhang\n - vipulraheja\n - Florian Bordes\n - Sina Honari\n - Vikram\n - erakra\n - Chiheb Trabelsi\n - Shubh Vachher\n - Daren Eiri\n - Gijs van Tulder\n - Laurent Dinh\n - Mohamed Ishmael Diwan Belghazi\n - mila\n - Jeff Donahue\n - Ramana Subramanyam\n - Bogdan Budescu\n - Ghislain Antony Vaillant\n - Jan Schlüter\n - Xavier Bouthillier\n - fo40225\n - Aarni Koskela\n - Adam Becker\n - Adam Geitgey\n - Adrian Keet\n - Adrian Seyboldt\n - Andrei Costinescu\n - Anmol Sahoo\n - Chong Wu\n - Holger Kohr\n - Jayanth Koushik\n - Jenkins\n - Lilian Besson\n - Lv Tao\n - Michael Manukyan\n - Murugesh Marvel\n - NALEPA\n - Ubuntu\n - Zotov Yuriy\n - dareneiri\n - lrast\n - morrme\n - yikang\n\n\nTheano 0.9.0 (20th of March, 2017)\n==================================\n\nThis is a final release of Theano, version ``0.9.0``, with a lot of\nnew features, interface changes, improvements and bug fixes.\n\nWe recommend that everybody update to this version.\n\nHighlights (since 0.8.0):\n - Better Python 3.5 support\n - Better numpy 1.12 support\n - Conda packages for Mac, Linux and Windows\n - Support newer Mac and Windows versions\n - More Windows integration:\n\n   - Theano scripts (``theano-cache`` and ``theano-nose``) now works on Windows\n   - Better support for Windows end-lines into C codes\n   - Support for space in paths on Windows\n\n - Scan improvements:\n\n   - More scan optimizations, with faster compilation and gradient computation\n   - Support for checkpoint in scan (trade off between speed and memory usage, useful for long sequences)\n   - Fixed broadcast checking in scan\n\n - Graphs improvements:\n\n   - More numerical stability by default for some graphs\n   - Better handling of corner cases for theano functions and graph optimizations\n   - More graph optimizations with faster compilation and execution\n   - smaller and more readable graph\n\n - New GPU back-end:\n\n   - Removed warp-synchronous programming to get good results with newer CUDA drivers\n   - More pooling support on GPU when cuDNN isn't available\n   - Full support of ignore_border option for pooling\n   - Inplace storage for shared variables\n   - float16 storage\n   - Using PCI bus ID of graphic cards for a better mapping between theano device number and nvidia-smi number\n   - Fixed offset error in ``GpuIncSubtensor``\n\n - Less C code compilation\n - Added support for bool dtype\n - Updated and more complete documentation\n - Bug fixes related to merge optimizer and shape inference\n - Lot of other bug fixes, crashes fixes and warning improvements\n\nA total of 123 people contributed to this release since 0.8.0, see list below.\n\nInterface changes:\n - Merged ``CumsumOp/CumprodOp`` into ``CumOp``\n - In MRG module:\n\n   - Replaced method ``multinomial_wo_replacement()`` with new method ``choice()``\n   - Random generator now tries to infer the broadcast pattern of its output\n\n - New pooling interface\n - Pooling parameters can change at run time\n - Moved ``softsign`` out of sandbox to ``theano.tensor.nnet.softsign``\n - Using floatX dtype when converting empty list/tuple\n - ``Roll`` make the shift be modulo the size of the axis we roll on\n - ``round()`` default to the same as NumPy: half_to_even\n\nConvolution updates:\n - Support of full and half modes for 2D and 3D convolutions including in ``conv3d2d``\n - Allowed pooling of empty batch\n - Implement ``conv2d_transpose`` convenience function\n - Multi-cores convolution and pooling on CPU\n - New abstract 3d convolution interface similar to the 2d convolution interface\n - Dilated convolution\n\n\nGPU:\n - cuDNN: support versoin 5.1 and wrap batch normalization (2d and 3d) and RNN functions\n - Multiple-GPU, synchrone update (via platoon, use NCCL)\n - Gemv(matrix-vector product) speed up for special shape\n - cublas gemv workaround when we reduce on an axis with a dimensions size of 0\n - Warn user that some cuDNN algorithms may produce unexpected results in certain environments\n   for convolution backward filter operations\n - ``GPUMultinomialFromUniform`` op now supports multiple dtypes\n - Support for ``MaxAndArgMax`` for some axis combination\n - Support for solve (using cusolver), erfinv and erfcinv\n - Implemented ``GpuAdvancedSubtensor``\n\nNew features:\n - ``OpFromGraph`` now allows gradient overriding for every input\n - Added Abstract Ops for batch normalization that use cuDNN when available and pure Theano CPU/GPU alternatives otherwise\n - Added gradient of solve, tensorinv (CPU), tensorsolve (CPU), searchsorted (CPU), DownsampleFactorMaxGradGrad (CPU)\n - Added Multinomial Without Replacement\n - Allowed partial evaluation of compiled function\n - More Rop support\n - Indexing support ellipsis: ``a[..., 3]```, ``a[1,...,3]``\n - Added ``theano.tensor.{tensor5,dtensor5, ...}``\n - compiledir_format support device\n - Added New Theano flag ``conv.assert_shape`` to check user-provided shapes at runtime (for debugging)\n - Added new Theano flag ``cmodule.age_thresh_use``\n - Added new Theano flag ``cuda.enabled``\n - Added new Theano flag ``nvcc.cudafe`` to enable faster compilation and import with old CUDA back-end\n - Added new Theano flag ``print_global_stats`` to print some global statistics (time spent) at the end\n - Added new Theano flag ``profiling.ignore_first_call``, useful to profile the new gpu back-end\n - remove ProfileMode (use Theano flag ``profile=True`` instead)\n\n\nOthers:\n - Split op now has C code for CPU and GPU\n - ``theano-cache list`` now includes compilation times\n - Speed up argmax only on GPU (without also needing the max)\n - More stack trace in error messages\n - Speed up cholesky grad\n - ``log(sum(exp(...)))`` now get stability optimized\n\n\nOther more detailed changes:\n - Added Jenkins (gpu tests run on pull requests in addition to daily buildbot)\n - Removed old benchmark directory and other old files not used anymore\n - Use of 64-bit indexing in sparse ops to allow matrix with more then 2\\ :sup:`31`\\ -1 elements\n - Allowed more then one output to be an destructive inplace\n - More support of negative axis\n - Added the keepdims parameter to the norm function\n - Make scan gradient more deterministic\n\nCommiters since 0.8.0:\n - Frederic Bastien\n - Arnaud Bergeron\n - Pascal Lamblin\n - Steven Bocco\n - Ramana Subramanyam\n - Simon Lefrancois\n - Gijs van Tulder\n - Benjamin Scellier\n - khaotik\n - Chiheb Trabelsi\n - Chinnadhurai Sankar\n - Cesar Laurent\n - Reyhane Askari\n - Mohammad Pezeshki\n - Alexander Matyasko\n - Alexandre de Brebisson\n - Mathieu Germain\n - Nan Rosemary Ke\n - Pierre Luc Carrier\n - Olivier Mastropietro\n - Thomas George\n - Saizheng Zhang\n - Iulian Vlad Serban\n - Francesco Visin\n - Caglar\n - Faruk Ahmed\n - Harm de Vries\n - Samira Shabanian\n - Vincent Dumoulin\n - Nicolas Ballas\n - Jakub Sygnowski\n - Jan Schlüter\n - Samira Ebrahimi Kahou\n - Mikhail Korobov\n - Fei Wang\n - Kv Manohar\n - Jesse Livezey\n - Kelvin Xu\n - Matt Graham\n - Ruslana Makovetsky\n - Sina Honari\n - Bryn Keller\n - Ciyong Chen\n - Vitaliy Kurlin\n - Zhouhan LIN\n - Gokula Krishnan\n - Kumar Krishna Agrawal\n - Ozan Çağlayan\n - Vincent Michalski\n - affanv14\n - Amjad Almahairi\n - Ray Donnelly\n - Tim Cooijmans\n - happygds\n - mockingjamie\n - Christos Tsirigotis\n - Florian Bordes\n - Ilya Kulikov\n - RadhikaG\n - Taesup (TS) Kim\n - Ying Zhang\n - Anton Chechetka\n - Karthik Karanth\n - Kirill Bobyrev\n - Rebecca N. Palmer\n - Yang Zhang\n - Yaroslav Ganin\n - Jonas Degrave\n - Liwei Cai\n - Lucas Beyer\n - Michael Harradon\n - Morgan Stuart\n - Tim Gasper\n - Xavier Bouthillier\n - p\n - texot\n - Andrés Gottlieb\n - Ben Poole\n - Bhavishya Pohani\n - Carl Thomé\n - David Bau\n - Dimitar Dimitrov\n - Evelyn Mitchell\n - Fei Zhan\n - Fuchai\n - Fábio Perez\n - Gennadiy Tupitsin\n - Gilles Louppe\n - Greg Ciccarelli\n - He\n - Huan Zhang\n - Kaixhin\n - Kevin Keraudren\n - Maltimore\n - Marc-Alexandre Cote\n - Marco\n - Marius F. Killinger\n - Martin Drawitsch\n - Maxim Kochurov\n - Micah Bojrab\n - Neil\n - Nizar Assaf\n - Rithesh Kumar\n - Rizky Luthfianto\n - Robin Millette\n - Roman Ring\n - Sander Dieleman\n - Sebastin Santy\n - Shawn Tan\n - Wazeer Zulfikar\n - Wojciech Głogowski\n - Yann N. Dauphin\n - gw0 [http://gw.tnode.com/]\n - hexahedria\n - hsintone\n - jakirkham\n - joncrall\n - root\n - superantichrist\n - tillahoffmann\n - valtron\n - wazeerzulfikar\n - you-n-g\n\n\nTheano 0.9.0rc4 (13th of March, 2017)\n=====================================\n\nThis release extends the 0.9.0rc3 and announces the upcoming final release 0.9.\n\nHighlights (since 0.9.0rc3):\n - Documentation updates\n - DebugMode fixes, cache cleanup fixes and other small fixes\n\n - New GPU back-end:\n\n   - Fixed offset error in GpuIncSubtensor\n   - Fixed indexing error in GpuAdvancedSubtensor for more than 2 dimensions\n\nA total of 5 people contributed to this release since 0.9.0rc3 and 123 since 0.8.0, see the lists below.\n\n\nCommitters since 0.9.0rc3:\n - Frederic Bastien\n - Pascal Lamblin\n - Arnaud Bergeron\n - Cesar Laurent\n - Martin Drawitsch\n\n\nTheano 0.9.0rc3 (6th of March, 2017)\n====================================\n\nThis release extends the 0.9.0rc2 and announces the upcoming final release 0.9.\n\nHighlights (since 0.9.0rc2):\n - Graph clean up and faster compilation\n - New Theano flag conv.assert_shape to check user-provided shapes at runtime (for debugging)\n - Fix overflow in pooling\n - Warn if taking softmax over broadcastable dimension\n - Removed old files not used anymore\n - Test fixes and crash fixes\n\n - New GPU back-end:\n\n   - Removed warp-synchronous programming, to get good results with newer CUDA drivers\n\nA total of 5 people contributed to this release since 0.9.0rc2 and 122 since 0.8.0, see the lists below.\n\n\nCommitters since 0.9.0rc2:\n - Frederic Bastien\n - Arnaud Bergeron\n - Pascal Lamblin\n - Florian Bordes\n - Jan Schlüter\n\n\nTheano 0.9.0rc2 (27th of February, 2017)\n========================================\n\nThis release extends the 0.9.0rc1 and announces the upcoming final release 0.9.\n\nHighlights (since 0.9.0rc1):\n - Fixed dnn conv grad issues\n - Allowed pooling of empty batch\n - Use of 64-bit indexing in sparse ops to allow matrix with more then 2\\ :sup:`31`\\ -1 elements.\n - Removed old benchmark directory\n - Crash fixes, bug fixes, warnings improvements, and documentation update\n\nA total of 9 people contributed to this release since 0.9.0rc1 and 121 since 0.8.0, see the lists below.\n\n\nCommitters since 0.9.0rc1:\n - Frederic Bastien\n - Pascal Lamblin\n - Steven Bocco\n - Simon Lefrancois\n - Lucas Beyer\n - Michael Harradon\n - Rebecca N. Palmer\n - David Bau\n - Micah Bojrab\n\n\nTheano 0.9.0rc1 (20th of February, 2017)\n========================================\n\nThis release extends the 0.9.0beta1 and announces the upcoming final release 0.9.\n\nHighlights (since 0.9.0beta1):\n - Better integration of Theano+libgpuarray packages into conda distribution\n - Better handling of Windows end-lines into C codes\n - Better compatibility with NumPy 1.12\n - Faster scan optimizations\n - Fixed broadcast checking in scan\n - Bug fixes related to merge optimizer and shape inference\n - many other bug fixes and improvements\n - Updated documentation\n\n - New GPU back-end:\n\n   - Value of a shared variable is now set inplace\n\nA total of 26 people contributed to this release since 0.9.0beta1 and 117 since 0.8.0, see the list at the bottom.\n\nInterface changes:\n - In MRG, replaced method `multinomial_wo_replacement()` with new method `choice()`\n\nConvolution updates:\n - Implement conv2d_transpose convenience function\n\nGPU:\n - GPUMultinomialFromUniform op now supports multiple dtypes\n\nNew features:\n - OpFromGraph now allows gradient overriding for every input\n - Added Abstract Ops for batch normalization that use cuDNN when available and pure Theano CPU/GPU alternatives otherwise\n - Added new Theano flag cuda.enabled\n - Added new Theano flag print_global_stats to print some global statistics (time spent) at the end\n\nOthers:\n - Split op now has C code for CPU and GPU\n - \"theano-cache list\" now includes compilation times\n\n\nCommitters since 0.9.0beta1:\n - Frederic Bastien\n - Benjamin Scellier\n - khaotik\n - Steven Bocco\n - Arnaud Bergeron\n - Pascal Lamblin\n - Gijs van Tulder\n - Reyhane Askari\n - Chinnadhurai Sankar\n - Vincent Dumoulin\n - Alexander Matyasko\n - Cesar Laurent\n - Nicolas Ballas\n - affanv14\n - Faruk Ahmed\n - Anton Chechetka\n - Alexandre de Brebisson\n - Amjad Almahairi\n - Dimitar Dimitrov\n - Fuchai\n - Jan Schlüter\n - Jonas Degrave\n - Mathieu Germain\n - Rebecca N. Palmer\n - Simon Lefrancois\n - valtron\n\n\nTheano 0.9.0beta1 (24th of January, 2017)\n=========================================\n\nThis release contains a lot of bug fixes and improvements + new features, to prepare the upcoming release candidate.\n\nHighlights:\n - Many computation and compilation speed up\n - More numerical stability by default for some graph\n - Jenkins (gpu tests run on PR in addition to daily buildbot)\n - Better handling of corner cases for theano functions and graph optimizations\n - More graph optimization (faster execution and smaller graph, so more readable)\n - Less c code compilation\n - Better Python 3.5 support\n - Better numpy 1.12 support\n - Support newer Mac and Windows version\n - Conda packages for Mac, Linux and Windows\n - Theano scripts now works on Windows\n - scan with checkpoint (trade off between speed and memory usage, useful for long sequences)\n - Added a bool dtype\n\n - New GPU back-end:\n\n   - float16 storage\n   - better mapping between theano device number and nvidia-smi number, using the PCI bus ID of graphic cards\n   - More pooling support on GPU when cuDNN isn't there\n   - ignore_border=False is now implemented for pooling\n\n\nA total of 111 people contributed to this release since 0.8.0, see the list at the bottom.\n\n\nInterface changes:\n - New pooling interface\n - Pooling parameters can change at run time\n - When converting empty list/tuple, now we use floatX dtype\n - The MRG random generator now try to infer the broadcast pattern of its output\n - Move softsign out of sandbox to theano.tensor.nnet.softsign\n - Roll make the shift be modulo the size of the axis we roll on\n - Merge CumsumOp/CumprodOp into CumOp\n - round() default to the same as NumPy: half_to_even\n\nConvolution updates:\n - Multi-cores convolution and pooling on CPU\n - New abstract 3d convolution interface similar to the 2d convolution interface\n - Dilated convolution\n\nGPU:\n - cuDNN: support versoin 5.1 and wrap batch normalization (2d and 3d) and RNN functions\n - Multiple-GPU, synchrone update (via platoon, use NCCL)\n - GpuAdvancedSubtensor in new back-end\n - Gemv(matrix-vector product) speed up for special shape\n - Support for MaxAndArgMax for some axis combination\n - Support for solve (using cusolver), erfinv and erfcinv\n - cublas gemv workaround when we reduce on an axis with a dimensions size of 0\n - Warn user that some cuDNN algorithms may produce unexpected results in certain environments\n   for convolution backward filter operations\n\nNew features:\n - Add gradient of solve, tensorinv (CPU), tensorsolve (CPU) searchsorted (CPU)\n - Add Multinomial Without Replacement\n - conv3d2d support full and half mode (REMOVE?)\n - Add DownsampleFactorMaxGradGrad.grad\n - Allow partial evaluation of compiled function\n - More Rop support\n - Indexing support ellipsis: a[..., 3], a[1,...,3]\n - Added theano.tensor.{tensor5,dtensor5, ...}\n - compiledir_format support device\n - Added new Theano flag cmodule.age_thresh_use\n\nOthers:\n - Speed up argmax only on gpu (without also needing the max)\n - A few unfrequent bugfix\n - More stack trace in error message\n - Speed up cholesky grad\n - log(sum(exp(...))) now get stability optimized\n\nOther more detailed changes:\n - Allow more then one output to be an destructive inplace\n - Add flag profiling.ignore_first_call, useful to profile the new gpu back-end\n - Doc/error message fixes/updates\n - More support of negative axis\n - Added the keepdims parameter to the norm function\n - Crash fixes\n - Make scan gradient more deterministic\n - Add support for space in path on Windows\n - remove ProfileMode (use Theano flag profile=True instead)\n\n\nCommitters since 0.8.0:\n - Frederic Bastien\n - Arnaud Bergeron\n - Pascal Lamblin\n - Ramana Subramanyam\n - Simon Lefrancois\n - Steven Bocco\n - Gijs van Tulder\n - Cesar Laurent\n - Chiheb Trabelsi\n - Chinnadhurai Sankar\n - Mohammad Pezeshki\n - Reyhane Askari\n - Alexander Matyasko\n - Alexandre de Brebisson\n - Nan Rosemary Ke\n - Pierre Luc Carrier\n - Mathieu Germain\n - Olivier Mastropietro\n - khaotik\n - Saizheng Zhang\n - Thomas George\n - Iulian Vlad Serban\n - Benjamin Scellier\n - Francesco Visin\n - Caglar\n - Harm de Vries\n - Samira Shabanian\n - Jakub Sygnowski\n - Samira Ebrahimi Kahou\n - Mikhail Korobov\n - Faruk Ahmed\n - Fei Wang\n - Jan Schlüter\n - Kv Manohar\n - Jesse Livezey\n - Kelvin Xu\n - Matt Graham\n - Ruslana Makovetsky\n - Sina Honari\n - Bryn Keller\n - Ciyong Chen\n - Nicolas Ballas\n - Vitaliy Kurlin\n - Zhouhan LIN\n - Gokula Krishnan\n - Kumar Krishna Agrawal\n - Ozan Çağlayan\n - Vincent Michalski\n - Ray Donnelly\n - Tim Cooijmans\n - Vincent Dumoulin\n - happygds\n - mockingjamie\n - Amjad Almahairi\n - Christos Tsirigotis\n - Ilya Kulikov\n - RadhikaG\n - Taesup (TS) Kim\n - Ying Zhang\n - Karthik Karanth\n - Kirill Bobyrev\n - Yang Zhang\n - Yaroslav Ganin\n - Liwei Cai\n - Morgan Stuart\n - Tim Gasper\n - Xavier Bouthillier\n - p\n - texot\n - Andrés Gottlieb\n - Ben Poole\n - Bhavishya Pohani\n - Carl Thomé\n - Evelyn Mitchell\n - Fei Zhan\n - Fábio Perez\n - Gennadiy Tupitsin\n - Gilles Louppe\n - Greg Ciccarelli\n - He\n - Huan Zhang\n - Jonas Degrave\n - Kaixhin\n - Kevin Keraudren\n - Maltimore\n - Marc-Alexandre Cote\n - Marco\n - Marius F. Killinger\n - Maxim Kochurov\n - Neil\n - Nizar Assaf\n - Rithesh Kumar\n - Rizky Luthfianto\n - Robin Millette\n - Roman Ring\n - Sander Dieleman\n - Sebastin Santy\n - Shawn Tan\n - Wazeer Zulfikar\n - Wojciech Głogowski\n - Yann N. Dauphin\n - gw0 [http://gw.tnode.com/]\n - hexahedria\n - hsintone\n - jakirkham\n - joncrall\n - root\n - superantichrist\n - tillahoffmann\n - wazeerzulfikar\n - you-n-g\n\n\nTheano 0.8.2 (21th of April, 2016)\n==================================\n\nThis is a point release with only the support for cudnn v5 convolution\nand minor fixes.\n\nHighlights:\n- cuDNN v5 convolution support (cuDNN v3 isn't supported anymore)\n- A few crash fixes\n\n\nTheano 0.8.1 (29th of March, 2016)\n==================================\n\nThis is a point release without any new feature.\n\nIt fixes compilation issues on MacOS X with the command line tools for\nXCode 7.3, which was released shortly after Theano 0.8.0.\n\n\nTheano 0.8 (21th of March, 2016)\n================================\n\nWe recommend that everybody update to this version.\n\nHighlights:\n - Python 2 and 3 support with the same code base\n - Faster optimization\n - Integration of cuDNN for better GPU performance\n - Many Scan improvements (execution speed up, ...)\n - optimizer=fast_compile moves computation to the GPU.\n - Better convolution on CPU and GPU. (CorrMM, cudnn, 3d conv, more parameter)\n - Interactive visualization of graphs with d3viz\n - cnmem (better memory management on GPU)\n - BreakpointOp\n - Multi-GPU for data parallism via Platoon (https://github.com/mila-udem/platoon/)\n - More pooling parameter supported\n - Bilinear interpolation of images\n - New GPU back-end:\n\n   * Float16 new back-end (need cuda 7.5)\n   * Multi dtypes\n   * Multi-GPU support in the same process\n\n\nA total of 141 people contributed to this release, see the list at the bottom.\n\n\nInstallation:\n - Better BLAS detection\n - Fixes for more recent software and OS versions\n - Support Anaconda on Windows\n\nBug fixes:\n - GpuJoin now supports negative axis\n - Fix GpuCumsum for negative axis\n\nInterface Deprecation (a warning is printed):\n - Deprecate Param class, use In instead\n\nInterface Changes:\n - Rename DownsampleFactorMax to Pool.\n - tensor.stack now uses the same interface as numpy.stack\n - optimizer=fast_compile moves computation to the GPU\n - Raise the user stack trace more frequently.\n - Change dev version numbering to follow the PEP 440\n\n\nNew Interface (reuses existing functionality):\n - theano.tensor.nnet.relu\n - theano.tensor.nnet.elu\n - BatchNormalization.\n - MaxAndArgmax support axis=None\n - Add theano.tensor.compress (equivalent of numpy.compress)\n - theano.tensor.signal.downsamples.max_pool_2d_same_size\n - COp\n - __props__\n\nNew features\n - tensor.unique\n - map_variables\n - erfcx\n - mgrid, ogrid\n - allclose\n - BreakpointOp\n - Make bincount work on GPU\n - SolveOp on GPU\n - Optional optimization remove_all_assert\n - AllocEmpty\n - LogSoftmax, for stability optimization when the crossentropy optimization does not apply.\n - theano.tensor.repeat works on GPU\n - BatchedDot on the GPU and faster on the CPU.\n - Faster batched_tensordot and make it work on GPU.\n - SoftmaxGrad grad\n - 3d conv via CorrMM on the GPU\n - CPU Max Pool support of padding and strides!=windows size\n - theano.function() now accepts a dict for the outputs. When doing this, the function will return a dict. Helpful to keep track of which output is what.\n - Warn for unknown or misspelled theano config variables\n - theano.tensor.tile update (accept symbolic reps, work on GPU)\n - scan how have a strict flag. If set to True, this make scan building faster and could make execution faster.\n - theano.tensor.signal.conv2d(2d,2d) output 2d answer\n - More convolution parameter supported\n - Bilinear interpolation of images\n\n\nSpeed-ups:\n - Faster SetSubtensor on the GPU.\n - Support more reduction pattern on the GPU.\n - More graph optimization\n - Faster graph optimization\n - GpuCrossentropySoftmaxArgmax1HotWithBias\n\n\nCrash/no return fixes:\n - Fix crash in the assert op grad\n - Fix curand crash on Mac\n - Multiple Fix scan crashes\n - Finish to update all Op.grad() implementation to the new interface\n\nOthers:\n - Support ARM processor.\n - Better tests\n - Code clean up.\n - Doc updates\n - doctest and sphinx test in travis\n - More tests tagged as slow\n - Better same_shape implementation\n - More op with c code to lower overhead\n - Custom pickler for SharedVariable theano.misc.pkl_utils.{dump,load}\n - function_dump to help us reproduce user error during compilation\n - assert_no_cpu_op\n - pep8, flake8\n - Better error messages\n - On non-default modes, reduce the number of allocation when allow_gc=False\n - Better lock\n\n\nCommitters for this dev version only:\n - Frederic Bastien\n - Arnaud Bergeron\n - Pierre Luc Carrier\n - Iban Harlouchet\n - Pascal Lamblin\n - Chienli Ma\n - Tim Cooijmans\n - Nicolas Ballas\n - Amjad Almahairi\n - David Warde-Farley\n - Christof Angermueller\n - Ziye Fan\n - Caglar\n - Sina Honari\n - Roy Xue\n - hantek\n - Mohammad Pezeshki\n - Melanie Ducoffe\n - Alexandre de Brebisson\n - Harm de Vries\n - Samira Shabanian\n - Alex Lamb\n - Ramana.S\n - Francesco Visin\n - Saizheng Zhang\n - Ying Zhang\n - Jan Schlüter\n - Xavier Bouthillier\n - Bart van Merrienboer\n - Cesar Laurent\n - Iulian Vlad Serban\n - Li Yao\n - Sigurd Spieckermann\n - Dmitrii Serdiuk\n - Kelvin Xu\n - Sebastien Jean\n - Thomas Mesnard\n - Seon-Wook Park\n - Vincent Michalski\n - Dustin Webb\n - Mikhail Korobov\n - Orhan Firat\n - Olivier Mastropietro\n - Daniel Renshaw\n - Julien Rebetez\n - Peng Liu\n - Sean Lee\n - TimSalimans\n - Andre Holzner\n - Gijs van Tulder\n - Guillaume Alain\n - Julien Demouth\n - Markus Beissinger\n - Mehdi Mirza\n - Moslem Kazemi\n - Saxenauts\n - Søren Kaae Sønderby\n - sentient07\n - Anatoly Belikov\n - Diogo Moitinho de Almeida\n - Jakub Sygnowski\n - Kashif Rasul\n - Laurent Dinh\n - Rémy Léone\n - Taesup (TS) Kim\n - gw0 [http://gw.tnode.com/]\n - mronian\n - vesis84\n - Benni\n - Chiheb Trabelsi\n - JesseLivezey\n - Marius Killinger\n - Matt Graham\n - Matthew Willson\n - Piotr Frankowski\n - Stefan Krastanov\n - vdumoulin\n - Adithya Ganesh\n - Anish Shah\n - Balázs Hidasi\n - Colin Raffel\n - Cory Lorenz\n - Doug\n - Jesse Livezey\n - John Salvatier\n - John Zedlewski\n - Jonathan Ho\n - Kaixhin\n - Liang-Chi Hsieh\n - Lucas Beyer\n - Luke Metz\n - Marc-Alexandre Cote\n - Martin Arjovsky\n - Matthias Kümmerer\n - Sirisha Rambhatla\n - briancheung\n - cai-lw\n - ivdorelian\n - jan-matthis\n - jojolalpin\n - joncrall\n - peterjsadowski\n - scottsievert\n - Étienne Simon\n - A. Flaxman\n - AlOa\n - Albert Zeyer\n - Andrea\n - Andy Jiang\n - Balázs\n - Ben Poole\n - Brian Cheung\n - Christophe Van Gysel\n - Claude Coulombe\n - Clay McLeod\n - Dario Garcia\n - Jakob Lombacher\n - Joao Felipe Santos\n - John Arevalo\n - Jonas Degrave\n - Martin Thoma\n - Mathieu Germain\n - Matthew Koichi Grimes\n - Michael Eickenberg\n - Michael Opitz\n - Paul Hollensen\n - Prayag Verma\n - Saatvik Shah\n - Sergei Lebedev\n - Vik Kamath\n - Wei Ouyang\n - Wojciech Głogowski\n - Yi-Lin Juang\n - Yurii Shevchuk\n - Zach Dwiel\n - dan\n - eulerreich\n - jotterbach\n - rolf\n - theaverageguy\n - wuaalb\n\n\nTheano 0.7 (26th of March, 2015)\n================================\nWe recommand to everyone to upgrade to this version.\n\nHighlights:\n * Integration of cuDNN for 2D convolutions and pooling on supported GPUs\n * Too many optimizations and new features to count\n * Various fixes and improvements to scan\n * Better support for GPU on Windows\n * On Mac OS X, clang is used by default\n * Many crash fixes\n * Some bug fixes as well\n\n\nTheano 0.6 (December 3th, 2013)\n===================================\n\nWe recommend that everybody update to this version.\n\n\nHighlights (since 0.6rc5):\n * Last release with support for Python 2.4 and 2.5.\n * We will try to release more frequently.\n * Fix crash/installation problems.\n * Use less memory for conv3d2d.\n\n0.6rc4 skipped for a technical reason.\n\nHighlights (since 0.6rc3):\n * Python 3.3 compatibility with buildbot test for it.\n * Full advanced indexing support.\n * Better Windows 64 bit support.\n * New profiler.\n * Better error messages that help debugging.\n * Better support for newer NumPy versions (remove useless warning/crash).\n * Faster optimization/compilation for big graph.\n * Move in Theano the Conv3d2d implementation.\n * Better SymPy/Theano bridge: Make an Theano op from SymPy expression and use SymPy c code generator.\n * Bug fixes.\n\nChange from 0.6rc5:\n * Fix crash when specifing march in cxxflags Theano flag. (Frederic B., reported by FiReTiTi)\n * code cleanup (Jorg Bornschein)\n * Fix Canopy installation on windows when it was installed for all users: Raingo\n * Fix Theano tests due to a scipy change. (Frederic B.)\n * Work around bug introduced in scipy dev 0.14. (Frederic B.)\n * Fix Theano tests following bugfix in SciPy. (Frederic B., reported by Ziyuan Lin)\n * Add Theano flag cublas.lib (Misha Denil)\n * Make conv3d2d work more inplace (so less memory usage) (Frederic B., repoted by Jean-Philippe Ouellet)\n\n\nCommitters since 0.5:\n\nFrederic Bastien\nPascal Lamblin\nIan Goodfellow\nOlivier Delalleau\nRazvan Pascanu\nabalkin\nArnaud Bergeron\nNicolas Bouchard +\nJeremiah Lowin +\nMatthew Rocklin\nEric Larsen +\nJames Bergstra\nDavid Warde-Farley\nJohn Salvatier +\nVivek Kulkarni +\nYann N. Dauphin\nLudwig Schmidt-Hackenberg +\nGabe Schwartz +\nRami Al-Rfou' +\nGuillaume Desjardins\nCaglar +\nSigurd Spieckermann +\nSteven Pigeon +\nBogdan Budescu +\nJey Kottalam +\nMehdi Mirza +\nAlexander Belopolsky +\nEthan Buchman +\nJason Yosinski\nNicolas Pinto +\nSina Honari +\nBen McCann +\nGraham Taylor\nHani Almousli\nIlya Dyachenko +\nJan Schlüter +\nJorg Bornschein +\nMicky Latowicki +\nYaroslav Halchenko +\nEric Hunsberger +\nAmir Elaguizy +\nHannes Schulz +\nHuy Nguyen +\nIlan Schnell +\nLi Yao\nMisha Denil +\nRobert Kern +\nSebastian Berg +\nVincent Dumoulin +\nWei Li +\nXterNalz +\n\n\nA total of 51 people contributed to this release.\nPeople with a \"+\" by their names contributed a patch for the first time.\n\n\nTheano 0.6rc5 (November 25th, 2013)\n===================================\n\nWe recommend that everybody update to this version.\n\nWe plan to release 0.6 in one week if there is no problem introduced\nwith this release candidate.\n\nTheano 0.6rc4 was skipped due to a problem with pypi\n\nHighlights:\n * Python 3.3 compatibility with buildbot test for it.\n * Full advanced indexing support.\n * Better Windows 64 bit support.\n * New profiler.\n * Better error messages that help debugging.\n * Better support for newer NumPy versions (remove useless warning/crash).\n * Faster optimization/compilation for big graph.\n * Move in Theano the Conv3d2d implementation.\n * Better SymPy/Theano bridge: Make an Theano op from SymPy expression and use SymPy c code generator.\n * Bug fixes.\n\nCommitters for this rc5 only:\n\nFrederic Bastien\nPascal Lamblin\nArnaud Bergeron\nabalkin\nOlivier Delalleau\nJohn Salvatier\nRazvan Pascanu\nJeremiah Lowin\nLudwig Schmidt-Hackenberg +\nVivek Kulkarni\nMatthew Rocklin\nGabe Schwartz\nJames Bergstra\nSigurd Spieckermann +\nBogdan Budescu +\nMehdi Mirza +\nNicolas Bouchard\nEthan Buchman +\nGuillaume Desjardins\nIan Goodfellow\nJason Yosinski\nSina Honari +\nBen McCann +\nDavid Warde-Farley\nIlya Dyachenko +\nJan Schluter +\nMicky Latowicki +\nYaroslav Halchenko +\nAlexander Belopolsky\nHannes Schulz +\nHuy Nguyen +\nRobert Kern +\nSebastian Berg +\nVincent Dumoulin +\nWei Li +\nXterNalz +\n\n\nA total of 36 people contributed to this release.\nPeople with a \"+\" by their names contributed a patch for the first time.\n\nInstallation:\n * Canopy support (direct link to MKL):\n   * On Linux and Mac OSX (Frederic B., Robert Kern)\n   * On Windows (Edward Shi, Frederic B.)\n\n * Anaconda instructions (Pascal L., Frederic B.)\n * Doc Ubuntu 13.04 (Frederic B.)\n * Better support of newer NumPy version(remove useless warning/crash) (Frederic B., Huy Nguyen)\n\nBug fixes:\n * Scan: if a scan node was cloned (by theano.clone) with different inputs, and if both the initial and the cloned nodes are used in the function being compiled, the value of the outputs of one would be replaced with the outputs of the other one. (Pascal L.)\n * Sparse: Disable the optimization that introduce the CSMGradC op as it doesn't work correctly with unsorted indices. (Frederic B.)\n * Mac: Fix wrong result of GpuDownsampleFactorMaxGrad on Mac OSX. (Pascal L.)\n * Mac: Auto-Detect and work around a bug in BLAS on MacOS X (Pascal L.)\n * Mac: Work around bug in MacOS X. If 2 compiled modules had the same name, the OS or Python was not always the right one even when we used the right handle to it. (Pascal L.)\n   Use this hash in the Python module, and in %(nodename)s, so that different helper functions in the support code for different Ops will always have different names.\n * Sparse grad: Fix ConstructSparseFromList.infer_shape (Pascal L., reported by Rami Al-Rfou')\n * (introduced in the development version after 0.6rc3 release) (Frederic B.)\n   Reduction that upcasts the input on no axis (ex: call theano.sum() on a scalar when the original dtype isn't float64 or\n   [u]int64). It produced bad results as we did not upcasted the inputs in the code, we just copy them.\n * Fix some cases of theano.clone() when we get a replacement of x that is a function of x. (Razvan P., reported by Akio Takano)\n * Fix grad of Alloc when we unbroadcast the value and it isn't a scalar. (Frederic B., reported Ian G.)\n\n   * In some cases (I think most cases), there was an exception raised in the theano.tensor.grad() method.\n     But in theory, there could be bad shapes produced in the unbroadcasted dimensions.\n\nInterface Deprecation (a warning is printed):\n * The mode ProfileMode is now deprecated, use the Theano flag profile=True to replace it.\n * New theano.sparse_grad() interface to get the sparse grad of a_tensor[an_int_vector]. (Frederic B.)\n   This can speed up the sparse computations when a small fraction of a_tensor is taken.\n   Deprecate the old interface for this. (Frederic B.)\n\nInterface Changes:\n * Interface change subtensor and take are not in tensor.basic anymore. They were available from tensor.* and are still available from there. (Frederic B., Matthew Rocklin)\n   * This lowers the basic.py size to 191k, so under 200k for github search.\n * Add -m32 or -m64 in the module cache key and add the python bitwidth in the compiledir path. (Pascal L.)\n * mrg.normal now has the parameter size mandatory. It was crashing with the default value of None. (Olivier D.)\n * Remove the deprecated passing of multiple modes to theano function. (Frederic B.)\n * Change FunctionGraph Features interface of the {on_prune(),on_import()} call back to take a reason. (Frederic B.)\n * FunctionGraph now clone the input graph by default. (Frederic B.)\n   * Added a parameter to optionally not do this cloning.\n   * This was needed to speed up compilation\n\nNew Interface (reuses existing functionality):\n * Add hostname as a var in compiledir_format (Frederic B.)\n * Add a new Theano flag: compute_test_value_opt. It takes the same values as compute_test_value. It enables compute_test_value during Theano optimization. Only useful to debug Theano optimization. Also small changes to some optimization to work correctly in that setup. (Frederic B.)\n * Add the value pdb to the Theano flag: compute_test_value and compute_test_value_opt. (Frederic B.)\n * Add the Theano flag: optimizer_verbose. Default False. When True, we print all the optimization being applied.(Frederic B.)\n * Add Op.c_init_code() to allow running the code when the c cmodule is imported (Pascal L.)\n * Allow theano.tensor.ones(3) to support scalar and not just list of scalar as numpy.ones (Jeremiah Lowin)\n * Make the memory profiler print the FLOPS used for the ops that know how to compute it. (Frederic B.)\n\nNew Features:\n * Make tensor.{constant,as_tensor_variable} work with memmap. (Christian Hudon, Frederic Bastien)\n * compilation work on ARM processor (Raspberry Pi, Vincent Dumoulin)\n * Add numpy.random.choice wrapper to our random number generator (Sigurd Spieckermann)\n * Better SymPy/Theano bridge: Make an Theano op from SymPy expression and use SymPy c code generator (Matthew Rocklin)\n * Move in Theano the Conv3d2d implementation (James Bergstra, Frederic B., Pascal L.)\n * First version of the new GPU back-end available (Arnaud Bergeron, Frederic B.)\n\n   * Not all Ops have been converted to this new back-end.\n     To use, use Theano flag device=cudaN or device=openclN, where N is a integer.\n * Python 3.3 compatible (abalkin, Gabe Schwartz, Frederic B., Pascal L.)\n * A new profiler (Frederic B.)\n   The new profiler now can profile the memory with the Theano flag profile_memory=True.\n   The ProfileMode now can't profile memory anymore and prints a message about it.\n   Now we raise an error if we try to profile when the gpu is enabled if we didn't set\n   correctly the env variable to force the driver to sync the kernel launch.\n   Otherwise the profile information are useless.\n   The new profiler supports the enabling/disabling of the garbage collection.\n * Adds tensor.tri, tensor.triu, and tensor.tril functions that wrap Numpy equivalents (Jeremiah Lowin)\n * Adds tensor.nonzero, tensor.flatnonzero functions that wrap Numpy equivalents (Jeremiah Lowin)\n * Adds tensor.nonzero_values to get around lack of advanced indexing for nonzero elements (Jeremiah Lowin)\n * Make {inc,set}_subtensor work on output of take. (Pascal L.)\n * When device=cpu and force_device=True, force that we disable the gpu. (Frederic B.)\n * Better Windows 64 bit support for indexing/reshaping (Pascal L.)\n * Full advanced indexing support (John Salvatier, seberg)\n * Add theano.tensor.stacklist(). Recursivly stack lists of tensors to maintain similar structure (Matthew R.)\n * Add Theano flag value: on_opt_error=pdb (Olivier D.)\n * GpuSoftmax[WithBias] for bigger row. (Frederic B.)\n * Make Erfinv work on the GPU (Guillaume Desjardin, Pascal L.)\n * Add \"theano-cache basecompiledir purge\" (Pascal L.)\n   This purges all the compiledirs that are in the base compiledir.\n * A_tensor_variable.zeros_like() now supports the dtype parameter (Pascal L.)\n * More stable reduce operations by default (Pascal L.)\n   Add an accumulator dtype to CAReduceDtype (acc_dtype)\n   by default, acc_dtype is float64 for float32 inputs,\n   then cast to specified output dtype (float32 for float32 inputs)\n * Test default blas flag before using it (Pascal L.)\n   This makes it work correctly by default if no blas library is installed.\n * Add cuda.unuse() to help tests that need to enable/disable the GPU (Frederic B.)\n * Add theano.tensor.nnet.ultra_fast_sigmoid and the opt (disabled by default) local_ultra_fast_sigmoid. (Frederic B.)\n * Add theano.tensor.nnet.hard_sigmoid and the opt (disabled by default) local_hard_sigmoid. (Frederic B.)\n * Add class theano.compat.python2x.Counter() (Mehdi Mirza)\n * Allow a_cuda_ndarray += another_cuda_ndarray for 6d tensor. (Frederic B.)\n * Make the op ExtractDiag work on the GPU. (Frederic B.)\n * New op theano.tensor.chi2sf (Ethan Buchman)\n * Lift Flatten/Reshape toward input on unary elemwise. (Frederic B.)\n   This makes the \"log(1-sigmoid) -> softplus\" stability optimization being applied with a flatten/reshape in the middle.\n * Make MonitorMode use the default optimizers config and allow it to change used optimizers (Frederic B.)\n * Add support for ScalarOp.c_support_code in GpuElemwise. (Frederic B.)\n * Also make the Psi function run on GPU. (Frederic B.)\n * Make tensor.outer(x,y) work when ndim != 1 as numpy.outer.\n * Kron op: Speed up/generalize/GPU friendly. (Frederic B.)\n   (It is not an op anymore, but reuses current op)\n * Add gpu max for pattern (0, 1) and added all gpu max pattern for gpu min. (Frederic B.)\n * Add GpuEye (Frederic B.)\n * Make GpuCrossentropySoftmaxArgmax1HotWithBias and GpuCrossentropySoftmax1HotWithBiasDx work for bigger inputs (Frederic B., reported by Ryan Price)\n * Finish and move out of sandbox theano.sparse.basic.true_dot (Nicolas Bouchard, Frederic B.)\n   And document all sparse dot variants.\n * Implement the mode ignore_borders for GpuImages2Neibs (Frederic B.)\n * Make many reduction functions accept a numpy scalar as axis (Jeremiah Lowin)\n * Allow numpy.asarray(cuda_ndarray, dtype=...) (Frederic B.)\n * theano-cache cleanup now remove cached module old version of code. (Frederic B.)\n\n\nSpeed-ups:\n * Optimizer speed up. (Frederic B.)\n * Fix warning on newer llvm version on Mac. (Pascal L., reported by Jeremiah Lowin and Chris Fonnesbeck)\n * Allow pickling of more Ops to allow reusing the compiled code (Pascal L., Frederic B.)\n * Optimize more cases of dot22 and scalar when we can't make a gemm (Pascal L., Frederic B.)\n * Speed up GpuJoin with c code (Ludwig Schmidt-Hackenberg, Frederic B.)\n * Faster GpuAdvancedIncSubtensor1 on Fermi GPU (and up) on matrix. (Vivek Kulkarni)\n * Faster GPUAdvancedIncSubtensor1 in some cases on all GPU (Vivek Kulkarni)\n * Implemented c_code for AdvancedSubtensor1 (abalkin)\n * Add the equivalent of -march=native to g++ command line. (Frederic B., Pascal L.)\n * Speed up compilation with Scan (Jan Schluter)\n * Merge more Scan nodes together (Pascal L., Yao Li).\n * Add MakeVector.c_code (Frederic B.)\n * Add Shape.c_code (Frederic B.)\n * Optimize Elemwise when all the inputs are fortran (Frederic B.)\n   We now generate a fortran output and use vectorisable code.\n * Add ScalarOp.c_code_contiguous interface and do a default version. (Frederic B.)\n   This could optimize elemwise by helping the compiler generate SIMD instruction.\n * Use ScalarOp.c_code_contiguous with amdlibm. (Frederic B.)\n   This speeds up exp, pow, sin, cos, log, log2, log10 and sigmoid when the input is contiguous in memory.\n * A fix that removes a local_setsubtensor_of_allocs optimization warning and enables it in that case. (Frederic B., reported by John Salvatier)\n * Make inv_as_solve optimization work (Matthew Rocklin)\n\nCrash/no return fixes:\n * Fix scan crash in the grad of grad of a scan with special structure (including scan in a scan) (Razvan P., Bitton Tenessi)\n * Fix various crashes when calling scan() with inputs specified in unusual ways. (Pascal L.)\n * Fix shape crash inserted by Scan optimization. The gradient of some recursive scan was making the PushOutSeqScan optimization insert crash during the execution of a Theano function. (Frederic B., reported by Hugo Larochelle)\n * Fix command not returning with recent mingw64 on Windows (Pascal L., reported by many people)\n * Fix infinite loop related to Scan on the GPU. (Pascal L.)\n * Fix infinite loop when the compiledir is full. (Frederic B.)\n * Fix a shape cycle crash in the optimizer (Pascal L., Frederic B., reported by Cho KyungHyun)\n * Fix MRG normal() now allow it to generate scalars. (Pascal L.)\n * Fix some GPU compilation issue on Mac (John Yani, Frederic B.)\n * Fix crash when building symbolic random variables with a mix of symbolic and numeric scalar in the \"size\" parameter. (Pascal L., Reported by Wu Zhen Zhou)\n * Make some Op.grad() implementions not return None (Pascal L.)\n * Crash fix in the grad of elemwise about an DisconnectedType (Pascal L, reported by Thomas Wiecki)\n * Fix local_gpu_multinomial optimization handling of broadcast information. (Frederic B., reported by Caglar)\n * Fix crash with change introduced in NumPy 1.7.1 (Pascal L., reported by Thomas Wiecki)\n * Compilation failure with complex (Pascal L., reported by autumncat)\n * Gpu reduction on all dimensions of a 4d tensor. (Frederic B., reported by Arjun Jain)\n * Fix crash for a combination of grad of dot and dimshuffle when only one of the inputs for a corresponding dimensions was knowing that it was broadcastable. (Frederic B., reported by Micky Latowicki)\n * AdvancedSubtensor1: allow broadcasted index vector. (Frederic B., reported by Jeremiah Lowin)\n * Fix compute_test_value for ifelse (Olivier D., reported by Bitton Tenessi)\n * Fix import error with some versions of NumPy (Olivier D.)\n * Fix Scan grad exception (Razvan P., reported by Nicolas BL)\n * Fix compute_test_value for a non_sequence when calling the gradient of Scan (Pascal L., reported by Bitton Tenessi).\n * Crash fix in Scan following interface change in 0.6rc2 (Razvan P.)\n * Crash fix on Scan (Razvan P.)\n * Crash fix on Scan (Pascal L., reported by Sina Honari and Sigurd)\n * Fix crash in Scan gradient related to compute_test_value (Frederic B., reported by Bitton Tenessi)\n * Fix a scan optimization warning/error depending of Theano flags (Frederic B.)\n * Fixed crash for unimplemented elemwise gradient (Olivier D., reported by Michael McNeil Forbes)\n * Fix crash in the elemwise python code for some big shape with power of 2. (Sina Honari, Pascal L.)\n * Fix compile and import errors on Windows including for the GPU. (Bogdan Budescu)\n * Fix GPU compilation on Windows (XterNalz)\n * Fix local_abs_merge optimization crash (Pascal L., reported by Jeremiah Lowin)\n * Fix import theano crash when g++ isn't there (Olivier D.)\n * Fix crash related to rebuild of Theano graph (Pascal L., reported by Divine Eguzouwa)\n * Fix crash during compilation (David Ward-Farley)\n * Crash fix in the grad of GPU op in corner case (Pascal L.)\n * Crash fix on MacOS X (Robert Kern)\n * theano.misc.gnumpy_utils.garray_to_cudandarray() set strides correctly for dimensions of 1. (Frederic B., reported by Justin Bayer)\n * Fix crash during optimization with consecutive sums and some combination of axis (Frederic B., reported by Caglar Gulcehre)\n * Fix crash with keepdims and negative axis (Frederic B., reported by David W.-F.)\n * Fix crash of theano.[sparse.]dot(x,y) when x or y is a vector. (Frederic B., reported by Zsolt Bitvai)\n * Fix opt crash/disabled with ifelse on the gpu (Frederic B, reported by Ryan Price)\n * Fix crash in optimization involving dot22, (Pascal L., reported by @micklat)\n * Prevent shape optimizations from introducing cycles in the graph (Frederic Bastien, Pascal Lamblin, reported by Kyunghyun Cho)\n\nOthers:\n * Update/Fixes/Typo/pep8 documentation and/or tutorial (Olivier D., David W.-F., Frederic B., Yaroslav Halchenko, Micky Latowicki, Ben McCann, Jason Yosinski, reported by Arnaud Bergeron)\n * Doc how to make a sparse Op. (Frederic B.)\n * Doc compatibility guide (abalkin)\n * Fix problem in remove_constants_and_unused_inputs_scan. (useless warning and maybe slow down) (Pascal L.)\n * Fix rop dot.(Razvan P., reported by Jeremiah Lowin)\n * Raise better error related to pydot bug. (Frederic B., reported by Jason Yosinski and Ludwig Schmidt-Hackenberg)\n * Fix to Theano tutorial examples. (reported by Ilya Dyachenko)\n * Fix SharedVar.value property to make it raise an exception (Frederic B., reported by Drew Duncan)\n * Fix verification with compute_test_value in grad() (Frederic B.)\n * Theano flags are now evaluated lazily, only if requested (Frederic B.)\n * Fix test when g++ is not avail (Frederic B.)\n * Add manual instructions for OpenBLAS on Ubuntu by (Jianri Li )\n * Better/more error messages (Frederic B., Pascal L., Ian Goodfellow)\n * Fix Error reporting with GpuConv (Frederic B., reported by Heng Luo and Nicolas Pinto)\n * Now travis-ci tests with scipy the parts that need it (Frederic B.)\n * Export some functions that work on CudaNdarray for windows (Frederic B.)\n * If the user specifies a -arch=sm_* value in the Theano flags for the gpu, don't add one (Frederic B., Pascal L.)\n * If a C thunk returns an error, check if a python exception is set. Otherwise, set a default one (Pascal L.)\n * Crash fix introduced in the development version (Wei LI)\n * Added BLAS benchmark result (Frederic B., Ben McCann)\n * Fix code comment (Hannes Schulz)\n * More stable tests (Frederic B.)\n * Add utt.asset_allclose(a, b) to have better error message. (Frederic B.)\n * Better error message with compute_test_value (Frederic, reported by John Salvatier)\n * Stochastic order behavior fix (Frederic B.)\n\n * Simpler initial graph for subtensor infer shape (Olivier D.)\n   The optimization was doing the optimization, but this allows better reading of the graph before optimization.\n * Better detection of non-aligned ndarray (Frederic B.)\n * Update MRG multinomial gradient to the new interface (Mehdi Mirza)\n * Implement Image2Neibs.perform() to help debug (Frederic B.)\n * Remove some Theano flags from the compilation key (Frederic B.)\n * Make theano-nose work on executable '\\*.py' files. (Alistair Muldal)\n * Make theano-nose work with older nose version (Frederic B.)\n * Add extra debug info in verify_grad() (Frederic B.)\n\n\nTheano 0.6rc3 (February 14th, 2013)\n===================================\n\nHighlights:\n * Windows related fixes.\n * Speed-ups.\n * Crash fixes.\n * A few small interface changes.\n * GPU memory leak fix.\n * A few corner cases fixes without incidence.\n * More Theano determinism\n * tensor.{dot,tensordot} more complete/faster/GPU friendly.\n * tensor.tensordot now support Rop/Lop\n * tensor.dot support n-dimensional inputs as NumPy\n * To support more NumPy syntax:\n     * Add theano.tensor.take()\n     * Add a_tensor_variable.{sort,dot,std,argmin,argmax,argsort,clip,conj,conjugate,repeat,round,trace,real,imag,take}\n\nCommiters for this rc3 only:\nFrederic Bastien\nIan Goodfellow\nPascal Lamblin\nJeremiah Lowin\nabalkin\nOlivier Delalleau\nRazvan Pascanu\nRami Al-Rfou'\nVivek Kulkarni\nGuillaume Desjardins\nDavid Warde-Farley\nEric Hunsberger\nAmir Elaguizy\nJames Bergstra\n\nBug fix:\n * Fix memory leak on the GPU in some corner cases with the Theano flags `allow_gc=False`. (Frederic B., reported by Jonas Gehring)\n * Fix copy of random state between graph. (Guillaume D.)\n   http://deeplearning.net/software/theano/tutorial/examples.html#copying-random-state-between-theano-graphs\n * Fix wrong dtype in sandbox.linalg.ExtractDiag with shape of 0. (Frederic B., reported by abalkin)\n * Correctly support array with more then 2*10e32 element in AdvancedSubtensor1. (Abalkin)\n * Fix wrong broadcast dimensions of output of Repeat op. (Abalkin)\n   We where using the inputs broadcasting pattern in some cases when we shouldn't.\n * Fix theano.sandbox.linalg.eigh grad that didn't always returned the right dtype. (Frederic B., Olivier D.)\n\nNew Features:\n * More Theano determinism (Ian G., Olivier D., Pascal L.)\n     * Add and use a new class OrderedSet.\n     * theano.grad is now deterministic.\n     * Warn when the user uses a (non ordered) dictionary and this causes non-determinism in Theano.\n     * The Updates class was non-deterministic; replaced it with the OrderedUpdates class.\n * tensor.tensordot now support Rop/Lop (Jeremiah Lowin)\n   This remove the class TensorDot and TensorDotGrad. It is the Dot/Elemwise ops that are used.\n * tensor.dot support n-dimensional inputs as NumPy (Jeremiah Lowin)\n   Work on the GPU too.\n * The Theano flag `nvcc.flags` now accept `-ftz=true`, `--prec-div=false` and `--prec=sqrt=false` as value. (Frederic B.)\n   To enable all of them, use the Theano flag `nvcc.flags=--use_fast_math`.\n * New op theano.sparse.ConstructSparseFromList (Rami Al-Rfou'  Vivek Kulkarni)\n * Make Theano work with Anaconda on Windows. (Pascal L.)\n * Add tensor_var.diagonal and theano.tensor.{diag,diagonal}. (abalkin)\n * AdvencedSubtensor1 can now have a sparse gradient. (Rami Al-Rfou', Vivek Kulkarni)\n * Implemented GpuContiguous.grad. (Ian G.)\n\nInterface Deprecation (a warning is printed):\n * theano.misc.strutil.renderString -> render_string (Ian G.)\n * Print a warning when using dictionary and this makes Theano non-deterministic.\n\nInterface Change:\n * Raise an error when theano.shared called with a theano variable. (Frederic B.)\n * Don't print warning for bug before Theano 0.5 by default. (Frederic B.)\n * Theano functions now always have a field name, default to None. (Frederic B.)\n * Theano function fct.fgraph have a copy of the Theano function name field. (Ian G.)\n   This is needed to allow the fgraph to know it.\n * In the grad method, if it were asked to raise an error if there is no path between the variables, we didn't always returned an error. (Ian G.)\n   We returned the mathematical right answer 0 in those cases.\n * get_constant_value() renamed get_scalar_constant_value() and raise a new exception tensor.basic.NotScalarConstantError. (Ian G.)\n * theano.function raises an error when trying to replace inputs with the 'given' parameter. (Olivier D.)\n   This was doing nothing, the error message explains what the user probably wants to do.\n\nNew Interface (reuse existing functionality):\n * tensor_var.sort() as a shortcut for theano.tensor.sort. (Jeremiah Lowin)\n   We where already doing this for argsort.\n * Add theano.tensor.take() and a_tensor_var.take() to support NumPy syntax. (abalkin)\n * Add a_tensor_variable.{dot,std,argmin,argmax,argsort,clip,conj,conjugate,repeat,round,trace,real,imag}. (abalkin)\n\nNew debug feature:\n * DebugMode print more info when there is an error. (Frederic B.)\n * Better profiling of test time with `theano-nose --time-profile`. (Frederic B.)\n * Detection of infinite loop with global optimizer. (Pascal L.)\n * DebugMode.check_preallocated_output now also work on Theano function output. (Pascal L.)\n * DebugMode will now complain when the strides of CudaNdarray of dimensions of 1 are not 0. (Frederic B.)\n\nSpeed-ups:\n * c_code for SpecifyShape op. (Frederic B.)\n * cross-entropy optimization now work when specify_shape is used. (Pascal L.)\n * The Scan optimization ScanSaveMem and PushOutDot1 applied more frequently. (Razvan P, reported Abalkin)\n   A skipped optimization warning was printed.\n * dot(vector, vector) now faster with some BLAS implementation. (Eric Hunsberger)\n   OpenBLAS and possibly others didn't call {s,d}dot internally when we called {s,d}gemv.\n   MKL was doing this.\n * Compilation speed up: Take the compiledir lock only for op that generate c_code. (Frederic B)\n * More scan optimization (Razvan P.)\n     * Opt to make RNN fast in Theano.\n     * Optimize some case of dot, by moving them outside of Scan.\n     * Move some sequences outside of scan too.\n     * Merge more scan inputs, mostly byproduct of other Scan optimizations.\n * c_code for theano.sparse.AddSD. (Rami Al-Rfou',  Vivek Kulkarni)\n\nCrash Fixes:\n * Fix crash about dimshuffle. (abalkin)\n * Fix crash at compilation. (Olivier D.)\n * Fix openmp detection. (Pascal L.)\n   Resulted in a crash with EPD on Windows.\n * Fix for new BLAS interface in SciPy. (Olivier D.)\n   Fix crash with some development version of SciPy.\n * GpuSum work with bigger shape when summing on the first dim on 3d tensor. (Frederic B., reported Chris Currivan)\n * Windows compilation crash fix. (Frederic B.)\n * Make CrossentropySoftmax1HotWithBiasDx and CrossentropySoftmaxArgmax1HotWithBias support uint* dtype. (Frederic B., reported by Mark Fenner)\n * Fix GpuSoftmax and GpuSoftmaxWithBias crash on GTX285. (Frederic B.)\n * Fix crash due to a race condition when importing theano. (Ian G.)\n * Fix crash from path problem with `theano-nose --batch`. (Abalkin)\n * Fix crash with tensor.roll(Var, iscalar). (Frederic B., reported by Jeremiah Lowin)\n * Fix compilation crash with llvm on Mac. (Abalkin)\n * Fix the grad of Scan that told wrongly that there is no connection between cost and parameters. (Razvan P.)\n * The infer shape mechanism now force that broadcasted dimensions have a shape know to be equivalent to one during compilation.\n   Sometimes, we where not able knowing this before run time and resulted in crash. (Frederic B.)\n * Fix compilation problems on GPU on Windows. (Frederic B.)\n * Fix copy on the GPU with big shape for 4d tensor (Pascal L.)\n * GpuSubtensor didn't set the stride to 0 for dimensions of 1. This could lead to check failing later that caused a crash. (Frederic B., reported by vmichals)\n\nTheoretical bugfix (bug that won't happen with current Theano code, but if you messed with the internal, could have affected you):\n * GpuContiguous, GpuAlloc, GpuDownSampleGrad, Conv2d now check the preallocated outputs strides before using it. (Pascal L.)\n * GpuDownSample, GpuDownSampleGrad didn't work correctly with negative strides in their output due to problem with nvcc (Pascal L, reported by abalkin?)\n\nOthers:\n * Fix race condition when determining if g++ is available. (Abalkin)\n * Documentation improvements. (Many people including David W-F, abalkin, Amir Elaguizy, Olivier D., Frederic B.)\n * The current GPU back-end have a new function CudaNdarray_prep_output(CudaNdarray ** arr, int nd, const int * dims) (Ian G)\n\n\nTheano 0.6rc2 (November 21th, 2012)\n===================================\n\nHighlights:\n * Fix for a few regressions introduced in 0.6rc1.\n * A few new features.\n * Speed-ups.\n * Scan fixes.\n * Crash fixes.\n * A few small interface changes.\n\nCommiters for this rc2 only:\nRazvan Pascanu\nPascal Lamblin\nFrederic Bastien\nIan Goodfellow\nJeremiah Lowin\nCaglar Gulcehre\nJey Kottalam\nMatthew Rocklin\nabalkin\n\n\nRegressions in 0.6rc1 fixed:\n * Fixed the scan gradient dtype issue. In 0.6rc1, some upcast were inserted. (Razvan P.)\n * Now grad() will do as before 0.6rc1 for float, i.e. the grad dtype will be the same as the inputs inside the graph. If you ask for the direct grad, it will return the computed dtype. (Pascal L.)\n\nWrong results fixes:\n * Scan fix in some case didn't returned the good results. (Razvan P., reported by Jeremiah L.)\n   This happened if you had a state with only neg tap and the output of the state was a function of some sequence.\n   If you had multiple states, there was no problem.\n * Fixed bug in Scan with multiple outputs,\n   where one output would sometimes overwrite another one. (Razvan P.)\n * Clip.grad treated the gradient with respect to the clipping boundary as always 0. (Ian G.)\n\nInterface changes:\n * We do not support anymore unaligned ndarray in Python code. (Frederic B.)\n   We did not support it in C code and supporting it in Python code made\n   the detection harder.\n * Now we only officially support SciPy 0.7.2 and NumPy 1.5.0 (Frederic B.)\n   We weren't and aren't testing with older versions.\n * The theano.sparse.SparseType is available even when SciPy is not (Frederic B.)\n * Fixed issue where members of consider_constant grad parameter\n   were treated differently from Constant variables. (Ian G.)\n * Removed the parameter g_cost from theano.grad(). (Ian G.)\n   Use the new more powerful parameter known_grads instead.\n\nNumPy interface support:\n * theano.tensor.where is an alias for theano.tensor.switch to support NumPy semantic. (Ian G.)\n * TensorVariable objects now have dot, argmin, argmax, clip, conj, repeat, trace, std, round,\n   ravel and argsort functions and the real and imag properties as numpy.ndarray objects.\n   The functionality was already available in Theano. (abalkin)\n\nSpeed-ups:\n * A C version of the SoftMax op (Razvan P.)\n   There was C code for the softmax with bias code.\n * Faster GpuIncSubtensor (Ian G.)\n * Faster copy on the GPU for 4d tensor. (Ian G.)\n * The fix of flatten infer_shape re-enables an optimization (Pascal L.)\n   * The bug was introduced in 0.6rc1.\n * Enable inc_subtensor on the GPU when updating it with a float64 dtype. (Ian G.)\n   It was causing an optimization warning.\n * Make DeepCopy reuse preallocated memory. (Frederic B.)\n * Move the convolution to the GPU when the image shape and logical image shape differ. (Frederic Bastien)\n * C code for the View Op (Razvan P., Pascal L.)\n\nNew Features:\n * Added a monitoring mode \"MonitorMode\" as a debugging tool. (Olivier D.)\n * Allow integer axes when keepdims==True (Jeremiah Lowin)\n * Added erfinv and erfcinv op. (Jey Kottalam)\n * Added tensor.batched_dot(). (Caglar Gulcehre)\n   It uses scan behind the scenes, but makes doing this easier.\n * theano.get_constant_value(x) (Frederic B.)\n   This tries to have x as a constant int.\n   This does some constant folding to try to convert x into an int.\n   Used by some optimizations.\n * Add theano.tensor.io.{MPIRecv,MPIRecvWait,MPISend,MPISendWait} (Matthew Rocklin)\n   Theano does not automatically use them. It is up to you to use them and split your computations.\n * Added theano.sandbox.linalg.eig (abalkin)\n * Started some support for Python3 (abalkin)\n   setup.py supports python3 now.\n   It calls 2to3 during the setup.\n   Python3 is not fully supported as we didn't update the C code.\n\n\nCrash Fixes:\n * Fix a crash related to scan.grad due to the new mechanism. (Ian G.)\n * Fix an optimization warning. Now it gets optimized. (Frederic B.)\n * Fix crash introduced in 0.6rc1 in theano.grad (Ian G.)\n * Fix crash introduced in 0.6rc1 in the grad of scan (Razvan P.)\n * Fix crash introduced in 0.6rc1 in the grad of clip (Ian G.)\n   Also implement the gradient on the min/max bound.\n * Fix crash in the grad of tensor.switch for int (Ian G.)\n * Fix crash when mixing shared variable on the GPU and sparse dot. (Pascal L.)\n * Fix crash as sometimes sparse.dot would return a different dtype number\n   that is equivalent but not the one expected. (Pascal L., reported by Rami Al-Rfou)\n * Better error msg (Ian G.)\n * Move all sparse random functions back to sandbox as they don't have a state inside Theano. (Pascal L.)\n   They were moved outside the sandbox in 0.6rc1\n * LoadFromDisk now is allowed to only support some memmap mode. (Pascal L.)\n   Otherwise, this was causing errors, segmentation faults or wrong results.\n * Fix import problem on PiCloud (Jeremiah Lowin)\n    * You need to use the c|py linker with the default\n      environment. Otherwise, you need to create your own environment.\n * Fix a crash during optimization when we take a subtensor of a constant with a non constant index. (Ian G.)\n * Better handling and error message of gradients on integer. (Ian G.)\n * Fixed a crash where Scan assumed all TypeErrors raised by the grad function were due to undefined gradients (Ian G.)\n\nOther:\n * Doc typo fixes, Doc updates, Better error messages: Olivier D., David W.F., Frederic B., James B., Matthew Rocklin, Ian G., abalkin.\n\n\nTheano 0.6rc1 (October 1st, 2012)\n=================================\n\nHighlights:\n * Bug fixes, crash fixes, CPU and GPU speed up.\n * theano_var.eval({other_var: val[,...]} to simplify the usage of Theano (Ian G.)\n * New default linker `cvm`. This is the execution engine that tells ops to run in certain orders.\n   It is now implemented in C and enables lazy evaluation of ifelse op.\n * Faster theano.function compilation. (Pascal L., Ian G.)\n * Big sparse submodule update and documentation of it. (Nicolas Bouchard)\n * Use GPU asynchronous functionality (Frederic B.)\n * Better Windows support.\n\nKnown bugs:\n * A few crash cases that will be fixed by the final release.\n\nBug fixes:\n * Outputs of Scan nodes could contain corrupted values: some parts of the\n   output would be repeated a second time, instead of the correct values.\n   It happened randomly, and quite infrequently, but the bug has been present\n   (both in Python and Cython) since April 2011. (Pascal L.)\n * In Sparse sandbox, fix the grad of theano.sparse.sandbox.sp.row_scale.\n   It did not return the right number of elements. (Frederic B.)\n * set_subtensor(x[int vector], new_value) when moved to the GPU\n   was transformed into inc_subtensor on the GPU. Now we have a correct\n   (but slow) GPU implementation.\n   Note 1: set_subtensor(x[slice[,...]], new_value) was working correctly\n   in all cases as well as all inc_subtensor.\n   Note 2: If your code was affected by the incorrect behavior, we now print\n   a warning by default (Frederic B.)\n * Fixed an issue whereby config values were used as default arguments,\n   with those defaults then stuck at old values if the config variables were\n   changed during program execution. (David W-F)\n * Fixed many subtle bugs involving mutable default arguments which may have\n   led to unexpected behavior, such as objects sharing instance variables\n   they were not supposed to share. (David W-F)\n * Correctly record the GPU device number used when we let the driver select it.\n   (Frederic B.)\n * Min, max with NaN in inputs did not return the right output. (Pascal L.)\n * The grad of TensorDot, was returning the wrong shape for some combination of axes.\n   We now raise NotImplementedError in those cases. (Frederic B.)\n * conv2d with subsample >2 returned wrong values. (Pascal L.)\n     * Fixed when mode==valid, disabled when mode==full\n * theano.sparse.CSMGrad op (generated by the grad of CSM) didn't\n   handle unsorted input correctly and gradient that is sparser\n   than the input. In that case, a bad result was returned. But this could\n   happen only when a sparse input of a Theano function was not\n   sorted. This happens for example with sparse advanced indexing from\n   scipy. The conclusion is most of time Nan in the graph.\n   (Yann Dauphin)\n * theano.sparse._dot(CSC matrix, dense) optimized version UsmmCSCDense didn't handle\n   correctly not contiguous inputs/outputs. (Pascal L.)\n * Fix a corner case CVM updates case. (Pascal L.)\n   This happened if the update to a shared variable is itself after optimization.\n   The CVM was not used by default.\n * Fix the view_map of sparse.Transpose and sparse.sandbow.sp.RowScale. (Frederic B.)\n   This probably didn't cause problem as there is only the UsmmCscDense op\n   (used call to Usmm with CSC matrix) that could interfere with them.\n\nDeprecation:\n * Deprecated the Module class (Ian G.)\n   This was a predecessor of SharedVariable with a less pythonic philosophy.\n\nInterface changes:\n * Now the base version requirements are numpy >= 1.5.0 and the optional scipy >= 0.7.2.\n * In Theano 0.5, we removed the deprecated sharedvar.value property.\n   Now we raise an error if you access it. (Frederic B.)\n * theano.function does not accept duplicate inputs, so function([x, x], ...)\n   does not work anymore. (Pascal L.)\n * theano.function now raises an error if some of the provided inputs are\n   not part of the computational graph needed to compute the output, for\n   instance, function([x, y], [y]). You can use the kwarg\n   ``on_unused_input={'raise', 'warn', 'ignore'}`` to control this.\n   (Pascal L.)\n * New Theano flag \"on_unused_input\" that defines the default value of the\n   previous point. (Frederic B.)\n * tensor.alloc() now raises an error during graph build time\n   when we try to create less dimensions than the number of dimensions\n   the provided value have. In the past, the error was at run time.\n   (Frederic B.)\n * Remove theano.Value and related stuff (Ian G.)\n   This was a test of what ended up as SharedVariable.\n * Renamed Env to FunctionGraph, and object attribute \"env\" to \"fgraph\" (Ian G.)\n   Deprecation warning printed when you try to access the \"env\" attribute.\n * Renamed the FunctionGraph.nodes attribute to FunctionNodes.apply_nodes (Ian G.)\n * Warn when we don't handle correctly the parameter in Theano flags `nvcc.flags`\n   (Frederic B.)\n * Do not reorder the user flags passed to the compiler. They get set after other flags. (Frederic B.)\n * Make setuptools optional (Ilan Schnell)\n * We warn when a user tries to use an old GPU with which Theano is untested.\n   This could cause crash and will also be very slow. (Frederic B.)\n * Make theano.grad able to differentiate between not implemented, undefined and disconnected grad.\n   Op.grad function should return theano.gradient.{grad_not_implemented,grad_undefined} or\n   something of DisconectedType (Ian G.)\n * Make theano.grad expect to always receive a float or undefined\n   gradient and enforce that op with integer output values always\n   return 0. (Ian G.)\n\n\nNew memory output contract (was mentioned in the release notes of Theano 0.5):\n * Now the output memory received can be preallocated by other stuff.\n   In the past it was always the previous output an Apply node allocated.\n   So this means that the shape and strides can be different from previous calls\n   and there can be links to this memory at other places.\n   This means it could receive preallocated output that is not c_contiguous.\n   But we don't do that now. (Pascal L.)\n * New Theano flags to test this DebugMode.check_preallocated_output (Pascal L.)\n * Updated a few ops to respect this contract (Pascal L.)\n\n\nNew Features:\n * GPU scan now works (does not crash) when there is a mixture of float32 and other dtypes.\n * theano_var.eval({other_var:val[,...]} to simplify the usage of Theano (Ian G.)\n * debugprint new param ids=[\"CHAR\", \"id\", \"int\", \"\"]\n   This makes the identifier printed to be a unique char, the Python id, a\n   unique int, or not have it printed. We changed the default to be \"CHAR\"\n   as this is more readable. (Frederic B.)\n * debugprint new param stop_on_name=[False, True]. If True, we don't print\n   anything below an intermediate variable that has a name. Defaults to False.\n   (Frederic B.)\n * debugprint does not print anymore the \"|\" symbol in a column after the last input. (Frederic B.)\n * If you use Enthought Python Distribution (EPD) now we use its blas\n   implementation by default. (Frederic B., Graham Taylor, Simon McGregor)\n * MRG random now raises an error with a clear message when the passed shape\n   contains dimensions with bad value like 0. (Frederic B. reported by Ian G.)\n * \"CudaNdarray[*] = ndarray\" works in more cases (Frederic B.)\n * \"CudaNdarray[*] += ndarray\" works in more cases (Frederic B.)\n * We add dimensions to CudaNdarray to automatically broadcast more frequently.\n   (Frederic B.)\n * New theano flag cmodule.warn_no_version. Default False. If True,\n   will print a warning when compiling one or more Op with C code that\n   can't be cached because there is no c_code_cache_version() function\n   associated to at least one of those Ops.  (Frederic B.)\n * CPU alloc now always generate C code (Pascal L.)\n * New Theano flag cmodule.warn_no_version=False. When True, warn when an op\n   with C code is not versioned (which forces to recompile it everytimes).\n   (Frederic B.)\n * C code reuses preallocated outputs (only done by Scan) (Pascal L.)\n * Garbage collection of intermediate results during Theano function calls\n   for Ops with C code (Pascal L.)\n * Theano flag compiledir_format now supports the parameter \"numpy_version\" and \"g++\". (Frederic B.)\n * Theano GPU variables, shared variables and constants now support <, <=,\n   > and >= similar to those not on the GPU.\n * AdvancedIncSubtensor now supports the set_instead_of_inc parameter. (Eric L.)\n * Added Advanced Indexing support to inc_subtensor and set_subtensor. (Eric L.)\n * theano.tensor.{any,all,std,var,mean,prod,sum,argmin,argmax,min,max,max_and_argman}\n   have a new parameter keepdims (Eric L.)\n   This allows to broadcast it correctly against the input data to normalize it.\n * The Updates objects now check that the keys are SharedVariable when we pass them\n   in the __init__ function. (Pascal L.)\n * Set a Theano Variable name on transposed op when the input has one (Frederic B).\n * The cvm linker now supports garbage collection (enabled by default). (James B. Arnaud B., Pascal L.)\n * The cvm linker is now the default linker.\n   This makes the \"loop\" around the execution of apply node in C. So this lowers the overhead.\n * theano_variable[numpy.newaxis] is now supported (James B.)\n * Enable ifelse on the GPU. (Frederic B.)\n * Correctly support numpy.memmap everywhere (Pascal L.)\n   We add partial support for them before. Just use the normal tensor operation\n   on them and it should work.\n   But be careful not to exhaust your computer memory! (we always generate normal ndarray)\n * Add an optimization that stabilizes log(softmax(x)). (Ian G.)\n * Re-enable the Images2Neibs grad. It was not broken, the problem was how we tested it. (Frederic B.)\n * If `theano_fn.trust_input` is set to False, do not check if the inputs are good\n   when calling the theano function. (Frederic B.)\n * Add theano.tensor.blas,gem{m,v} as shortcut.\n * theano.grad(..., add_names=True). False for the old\n   behavior. Otherwise it tries to name the grad variables. (Ian G.)\n * theano-nose (Pascal L.)\n   A wrapper around nosetests that adds needed extensions.\n   * --profile-time option, to print time spent in each test (Eric L.)\n   * --batch option, to allow to run tests in batch to lower memory requirement.\n * m = mean(log(1 - sigm(x)))\n   x - scalar * theano.grad(m, x)\n   There is a stabilization optimization for this.\n   Now it is applied more frequently. (Pascal L.)\n\n\nNew Op/functions:\n * Added element-wise operation theano.tensor.{GammaLn,Psi} (John Salvatier, Nicolas Bouchard)\n * Added element-wise operation theano.tensor.{arcsin,arctan,arccosh,arcsinh,arctanh,exp2,arctan2} (Nicolas Bouchard)\n * Added element-wise operation theano.tensor.{gamma,conj,complex_from_polar,expm1,deg2rad,rad2deg,trunc,gamma} (Nicolas Bouchard)\n * Added theano.tensor.argsort that wraps numpy.argsort (Hani Almousli).\n * Added theano.tensor.diff that wraps numpy.diff (Nicolas B.)\n * Added theano.tensor.bincount that wraps numpy.bincount (Nicolas B., Pascal L, Frederic B.)\n * Added theano.tensor.squeeze (Nicolas B.)\n   This removes broadcasted dimensions from the variable.\n   Theano-esque version of numpy.squeeze.\n * Added theano.tensor.repeat that wraps numpy.repeat (Nicolas B. + PL)\n * Added theano.tensor.bartlett that wraps  numpy.bartlett (Eric L.)\n * Added theano.tensor.fill_diagonal that wraps numpy.fill_diagonal (Eric L., Frederic B.)\n * Added tensor.square that is an alias for tensor.sqr as NumPy (Ian G.)\n * Added theano.tensor.load(path, dtype, broadcastable, mmap_mode=None) op\n   that allows to load a .npy file in a theano graph (Matthew Rocklin)\n * theano.sandbox.linalg.kron.py:Kron op. (Eric L.)\n   Kronecker product\n\nSpeed up:\n * CPU convolutions are now parallelized (Frederic B.)\n   By default use all cores/hyper-threads.\n   To control it, use the `OMP_NUM_THREADS=N` environment variable where N is the number of\n   parallel threads to use. By default it is equal to the number of CPU cores/hyper\n   threads that you have.\n   There is a new Theano flag `openmp` to allow/disallow openmp op.\n   If your BLAS library is parallelized, this flag won't affect it, but the\n   env variable will.\n * Remove a corner case causing duplicated dot22/gemm in the graph. (Frederic B., Ian G.)\n * Enable fusion of elemwise that have the same clients multiple times. (Frederic B.)\n * New optimization: Remove reduction over broadcastable dimensions (James B., Frederic B.)\n * Faster theano.function compilation. (Pascal L., Ian G.)\n * Remove GPU transfer around specify_shape op. (Frederic B.)\n * Implemented/tested MANY op.infer_shape method (Eric Larsen)\n   This allows Theano to make better shape inferance.\n * Implement Solve.infer_shape (Matthew Rocklin)\n * Scan memory optimizations now work more frequently. (Razvan P.)\n   There was a warning printed by the subtensor optimization in those cases.\n * Faster rng_mrg Python code. (mostly used for tests) (Frederic B.)\n\nSpeed up GPU:\n * Convolution on the GPU now checks the generation of the card to make\n   it faster in some cases (especially medium/big ouput image) (Frederic B.)\n\n     * We had hardcoded 512 as the maximum number of threads per block. Newer cards\n       support up to 1024 threads per block.\n * Faster GpuAdvancedSubtensor1, GpuSubtensor, GpuAlloc (Frederic B.)\n * We now pass the GPU architecture to nvcc when compiling (Frederic B.)\n * Now we use the GPU function async feature by default. (Frederic B.)\n   Set the environment variable `CUDA_LAUNCH_BLOCKING` to `1` to disable this\n   for profiling or debugging.\n * Faster creation of CudaNdarray objects (Frederic B.)\n * Now some Max reductions are implemented on the GPU. (Ian G.)\n\nSparse Sandbox graduate (moved from theano.sparse.sandbox.sp):\n * sparse.remove0 (Frederic B., Nicolas B.)\n * sparse.sp_sum(a, axis=None) (Nicolas B.)\n     * bugfix: the not structured grad was returning a structured grad.\n * sparse.{col_scale,row_scale,ensure_sorted_indices,clean} (Nicolas B.)\n * sparse.{diag,square_diagonal} (Nicolas B.)\n\nSparse:\n * Support for uint* dtype.\n * Implement theano.sparse.mul(sparse1, sparse2) when both inputs don't\n   have the same sparsity pattern. (Frederic B.)\n * New Ops: sparse.{expm1,deg2rad,rad2deg,trunc} (Nicolas B.)\n * New Ops: sparse.{sqrt,sqr,log1p,floor,ceil,sgn,round_half_to_even} (Nicolas B.)\n * New Ops: sparse.{arctanh,tanh,arcsinh,sinh,arctan,arcsin,tan,sin} (Nicolas B.)\n * New functions: structured_{add,exp,log,pow,minimum,maximum,sigmoid} (Yann D., Nicolas B.)\n     * Optimized op: StructuredAddSV, StrucutedAddSVCSR (inserted automatically)\n * New Op: sparse.mul_s_v multiplication of sparse matrix by broadcasted vector (Yann D.)\n * New Op: sparse.Cast() (Yann D., Nicolas B.)\n     * Add sparse_variable.astype() and theano.sparse.cast() and\n       theano.sparse.{b,w,i,l,f,d,c,z}cast() as their tensor equivalent (Nicolas B.)\n * Op class: SamplingDot (Yann D., Nicolas B.)\n   * Optimized version: SamplingDotCsr, StructuredDotCSC\n   * Optimizations to insert the optimized version: local_sampling_dot_csr, local_structured_add_s_v\n * New Ops: sparse.{Multinomial,Poisson,Binomial} (Yann D., NB)\n * Implement the CSMProperties grad method (Yann Dauphin)\n * Move optimizations to theano/sparse/opt.py (Nicolas B.)\n\nNew flags:\n * `profile=True` flag now prints the sum of all printed profiles. (Frederic B.)\n     * It works with the linkers vm/cvm (default).\n     * Also print compile time, optimizer time and linker time.\n     * Also print a summary by op class.\n * new flag \"profile_optimizer\" (Frederic B.)\n   when profile=True, will also print the time spent in each optimizer.\n   Useful to find optimization bottleneck.\n * new flag \"cmodule.remove_gxx_opt\" (Frederic B.)\n   If True, will remove -O* parameter passed to g++.\n   This is useful to debug in gdb module compiled by Theano.\n   The parameter -g is passed by default to g++.\n * new flag cmodule.compilation_warning\n   if True, will print compilation warning.\n * new flag `allow_gc` (Frederic B.)\n   When False, do not garbage collect intermediate results when they are not needed.\n   This uses more memory, but allocates memory less frequently so faster.\n * new flag `vm.lazy` (Frederic B.)\n   Useful only for the vm linkers. When lazy is None,\n   auto detect if lazy evaluation is needed and use the apropriate\n   version. If lazy is True/False, force the version used between\n   Loop/LoopGC and Stack.\n * new flag `cxx`. This is the C++ compiler to use. If empty do not compile C code. (Frederic B.)\n * New flag `print_active_device` that defaults to True. (Matthew R.)\n\nDocumentation:\n * Added in the tutorial documentation on how to extend Theano.\n   This explains how to make a Theano Op from a Python function.\n   http://deeplearning.net/software/theano/tutorial/extending_theano.html\n   (Frederic B.)\n * New installation instructions for Windows using EPD (Pascal L.)\n * New installation on Windows by using a Linux VM from ContinuumIO (Frederic B.)\n * Revisions of Theano tutorial and addition of exercises to it. (Eric L.)\n * New tutorial on Sparse variable. (Nicolas B., Sebastien Lemieux, Frederic Bastien\n   http://www.deeplearning.net/software/theano/tutorial/sparse.html\n * Installation documentation for CentOS6 (Frederic B.)\n * Installation documentation for Ubuntu (with GPU) (Frederic B., Matthias Zoehrer)\n * Doc typo fixes, Doc updates, Better error messages: Olivier D., David W.F., Frederic B., James B., Matthew Rocklin, Ian G.\n * Python Memory Management tutorial (Steven Pigeon, Olivier D.)\n\nProposal:\n * Math framework for complex gradients (Pascal L.)\n\n\nInternal changes:\n * Define new exceptions MissingInputError and UnusedInputError, and use them\n   in theano.function, instead of TypeError and ValueError. (Pascal L.)\n * Better handling of bitwidth and max values of integers and pointers\n   across platforms (Pascal L.)\n * Made a few Ops with C code versioned to reduce compilation time.\n   (Frederic B, Pascal L.)\n * Better deletion of files in the compiledir (Frederic B.)\n * Safer import on sort op (Nicolas Pinto)\n * hash_from_dict for elemwise op (Fredric B.)\n * Renamed BadCLinkerOutput into BadThunkOutput. (PL)\n * tensor.utils.shape_of_variables (Matthew R.)\n * Add the numpy abi version and g++/nvcc version in the key of compiled code. (Frederic B.)\n * env.replace_all_validate_remove (Frederic B.)\n   This allows global optimizer to ensure it removed some nodes from the graph.\n   This is a generic way to catch errors that would otherwise duplicate\n   computation.\n   * It was used for GEMM and Scan optimization (Frederic B., Razvan P.)\n * Fix how exception are raised in GPU code (James B.)\n * Made code respect pep8: OD, Fred, Pascal L., Nicolas Bouchard, Eric Larsen and others.\n * TensorType and CudaNdarrayType now have a value_zeros method that call CudaNdarray.zeros or\n   numpy.zeros with the right dtype. (Pascal L., Olivier D.)\n   This allows to have the same code work with both types.\n * Renamed FunctionGraph.extend function to FunctionGraph.attach_feature. (Ian G.)\n * New exception MissingGXX when we try to compile but there is no cxx compiler. (Frederic B.)\n * New fct theano.gof.utils.give_variables_names(...) that gives unique names to variables. (Matthew R.)\n * Use most of the time the new NumPy C-API for later NumPy release. (Frederic B.)\n * New theano.gof.sched.sort_apply_nodes() that will allow other execution ordering. (Matthew R.)\n * New attribute sort_schedule_fn, a way to specify a scheduler to use. (Matthew R.)\n\nCrash Fix:\n * Fix import conflict name (usaar33, Frederic B.)\n    * This makes Theano work with PiCloud.\n * Do not try to use the BLAS library when blas.ldflags is manually set to an\n   empty string (Frederic B., Pascal L.)\n * When importing theano on a computer without GPU with the Theano\n   flags 'device' or 'init_gpu_device' set to gpu* (Frederic B., reported by  Luo Heng)\n * Optimization printed a useless error when scipy was not available. (Frederic B.)\n * GPU conv crash/slowdown on newer hardware (James B.)\n * Better error handling in GPU conv (Frederic B.)\n * GPU optimization that moves element-wise Ops to the GPU. Crash happened in\n   a particular execution order of this optimization and the\n   element-wise fusion optimization when upcasting some inputs to\n   float32 (to compute them on the GPU).\n   (Frederic B., reported by Sander Dieleman)\n * GpuReshape in some particular case when the input is not contiguous\n   (Frederic B., reported by Sander Dieleman)\n * GpuSoftmaxWithBias with shape (0, N) with N > 1.\n   (Frederic B., reported by Razvan P.)\n * Fix crash under 64-bit Windows, when taking subtensors of the form a[n:]\n   (Pascal L., reported by Simon McGregor)\n * Fixed issue with the MaxAndArgmax Op not properly preserving broadcastable\n   dimensions, which could typically result in optimization crashes (Olivier D.)\n * Fixed crash when concatenating some arrays with specific broadcasting\n   patterns (Olivier D.)\n * Work around a known issue with nvcc 4.1 on MacOS X. (Graham Taylor)\n * In advanced indexing, if some inputs are constant, no need to call constant(...)\n   on their value any more. (Pascal L., reported by John Salvatier)\n * Fix crash on GPU when the GpuSubtensor didn't put the right stride\n   when the result tensor had a dimension with size of 1. (Pascal L,\n   reported Graham T.)\n * Fix scan crash that made it not run on the GPU in one case. (Guillaume D.)\n * If you grad again a random state, don't crash (Razvan P.)\n * GpuDownsampleFactorMax and its grad with inputs dimensions 0 and 1 bigger then 65535.\n   (Frederic B. reported by Gabe Schwartz)\n * Potential crash due to parallel compilation when importing theano.sandbox.cuda\n   (Olivier D.)\n * Crash fix on python 2.4 with slicing. (Pascal L.)\n * grad of argmin and argmax (Razvan P.)\n * Don't compute the Rop for shared variables with updates (mostly random).\n   We don't use them and they caused crash. (Razvan P.)\n * MaxArgmax.grad() when one of the gradient it receives is None. (Razvan P, reported by Mark Fenner)\n * Fix crash of GpuSum when some dimensions shape was 0. (Frederic B.)\n\nTests:\n * Use less memory (Olivier D.) (fix crash on 32-bit computers)\n * Fix test with Theano flag \"blas.ldflags=\". (Frederic B., Pascal L.)\n * Fix crash with advanced subtensor and numpy constant.\n * Fix random tests crash due to random value. (Pascal L.)\n * Always introduce Alloc node when calling alloc and let the optimizer remove them if needed.\n   This allows DebugMode to catch some shape error. (Pascal L.)\n * DebugMode now checks the view_map for all types of Theano variables.\n   It was doing only variables of tensor type. (Frederic B.)\n\nOthers:\n * Remove python warning for some python version. (Gabe Schwartz)\n * Remove useless fill op in fast_compile mode to make the graph more readable. (Fredric B.)\n * Remove GpuOuter as it is a subset of the new GpuGer (Frederic B.)\n * Now we use http://travis-ci.org/ to run all CPU tests (without SciPy)\n   with the default mode on all Pull Requests.\n   This should make the trunk more stable. (Fredric B.)\n * Our nightly buildbot now checks on python 2.4 (Frederic B.)\n   This should make the trunk work on it more frequently.\n\nOther thanks:\n * blaxill reported an error introduced into the trunk.\n\nNew stuff that will probably be reworked/removed before the release:\n * Better PyCUDA sharing of the GPU context.(fix crash at exit) (Frederic B.)\n   TODO: there is still a crash at exit!\n\n\nTheano 0.5 (23 February 2012)\n=============================\n\nHighlights:\n * Moved to github: http://github.com/Theano/Theano/\n * Old trac tickets moved to assembla tickets: http://www.assembla.com/spaces/theano/tickets\n * Theano vision: http://deeplearning.net/software/theano/introduction.html#theano-vision (Many people)\n * Theano with GPU works in some cases on Windows now. Still experimental. (Sebastian Urban)\n * Faster dot() call: New/Better direct call to cpu and gpu ger, gemv, gemm\n   and dot(vector, vector). (James, Frédéric, Pascal)\n * C implementation of Alloc. (James, Pascal)\n * theano.grad() now also works with sparse variables. (Arnaud)\n * Macro to implement the Jacobian/Hessian with theano.tensor.{jacobian,hessian} (Razvan)\n * See the Interface changes.\n\n\nInterface Behavior Changes:\n * The current default value of the parameter axis of\n   theano.{max,min,argmax,argmin,max_and_argmax} is now the same as\n   numpy: None. i.e. operate on all dimensions of the tensor.\n   (Frédéric Bastien, Olivier Delalleau) (was deprecated and generated\n   a warning since Theano 0.3 released Nov. 23rd, 2010)\n * The current output dtype of sum with input dtype [u]int* is now always [u]int64.\n   You can specify the output dtype with a new dtype parameter to sum.\n   The output dtype is the one used for the summation.\n   There is no warning in previous Theano versions about this.\n   The consequence is that the sum is done in a dtype with more precision than before.\n   So the sum could be slower, but will be more resistant to overflow.\n   This new behavior is the same as numpy. (Olivier, Pascal)\n * When using a GPU, detect faulty nvidia drivers. This was detected\n   when running Theano tests. Now this is always tested. Faulty\n   drivers result in wrong results for reduce operations. (Frederic B.)\n\n\nInterface Features Removed (most were deprecated):\n * The string modes FAST_RUN_NOGC and STABILIZE are not accepted. They\n   were accepted only by theano.function().\n   Use Mode(linker='c|py_nogc') or Mode(optimizer='stabilize') instead.\n * tensor.grad(cost, wrt) now always returns an object of the \"same type\" as wrt\n   (list/tuple/TensorVariable). (Ian Goodfellow, Olivier)\n * A few tag.shape and Join.vec_length left have been removed. (Frederic)\n * The .value attribute of shared variables is removed, use shared.set_value()\n   or shared.get_value() instead. (Frederic)\n * Theano config option \"home\" is not used anymore as it was redundant with \"base_compiledir\".\n   If you use it, Theano will now raise an error. (Olivier D.)\n * scan interface changes: (Razvan Pascanu)\n    * The use of `return_steps` for specifying how many entries of the output\n      to return has been removed. Instead, apply a subtensor to the output\n      returned by scan to select a certain slice.\n    * The inner function (that scan receives) should return its outputs and\n      updates following this order:\n        [outputs], [updates], [condition].\n      One can skip any of the three if not used, but the order has to stay unchanged.\n\nInterface bug fix:\n * Rop in some case should have returned a list of one Theano variable,\n   but returned the variable itself. (Razvan)\n\nNew deprecation (will be removed in Theano 0.6, warning generated if you use them):\n * tensor.shared() renamed to tensor._shared(). You probably want to\n   call theano.shared() instead! (Olivier D.)\n\n\nBug fixes (incorrect results):\n * On CPU, if the convolution had received explicit shape information,\n   they were not checked at runtime.  This caused wrong result if the\n   input shape was not the one expected. (Frederic, reported by Sander\n   Dieleman)\n * Theoretical bug: in some case we could have GPUSum return bad value.\n   We were not able to reproduce this problem\n     * patterns affected ({0,1}*nb dim, 0 no reduction on this dim, 1 reduction on this dim):\n       01, 011, 0111, 010, 10, 001, 0011, 0101 (Frederic)\n * div by zero in verify_grad. This hid a bug in the grad of Images2Neibs. (James)\n * theano.sandbox.neighbors.Images2Neibs grad was returning a wrong value.\n   The grad is now disabled and returns an error. (Frederic)\n * An expression of the form \"1 / (exp(x) +- constant)\" was systematically matched to \"1 / (exp(x) + 1)\"\n   and turned into a sigmoid regardless of the value of the constant. A warning will be issued if your\n   code was affected by this bug. (Olivier, reported by Sander Dieleman)\n * When indexing into a subtensor of negative stride (for instance, x[a:b:-1][c]),\n   an optimization replacing it with a direct indexing (x[d]) used an incorrect formula,\n   leading to incorrect results. (Pascal, reported by Razvan)\n * The tile() function  is now stricter in what it accepts to allow for better\n   error-checking/avoiding nonsensical situations. The gradient has been\n   disabled for the time being as it only implemented (incorrectly) one special\n   case. The `reps` argument must be a constant (not a tensor variable), and\n   must have the same length as the number of dimensions in the `x` argument;\n   this is now checked. (David)\n\n\nScan fixes:\n * computing grad of a function of grad of scan (reported by Justin Bayer, fix by Razvan)\n   before: most of the time crash, but could be wrong value with bad number of dimensions (so a visible bug)\n   now: do the right thing.\n * gradient with respect to outputs using multiple taps (reported by Timothy, fix by Razvan)\n   before: it used to return wrong values\n   now: do the right thing.\n   Note: The reported case of this bug was happening in conjunction with the\n         save optimization of scan that give run time errors. So if you didn't\n         manually disable the same memory optimization (number in the list4),\n         you are fine if you didn't manually request multiple taps.\n * Rop of gradient of scan (reported by Timothy and Justin Bayer, fix by Razvan)\n   before: compilation error when computing R-op\n   now: do the right thing.\n * save memory optimization of scan (reported by Timothy and Nicolas BL, fix by Razvan)\n   before: for certain corner cases used to result in a runtime shape error\n   now: do the right thing.\n * Scan grad when the input of scan has sequences of different lengths. (Razvan, reported by Michael Forbes)\n * Scan.infer_shape now works correctly when working with a condition for the number of loops.\n   In the past, it returned n_steps as the length, which is not always true. (Razvan)\n * Scan.infer_shape crash fix. (Razvan)\n\nNew features:\n * AdvancedIncSubtensor grad defined and tested (Justin Bayer)\n * Adding 1D advanced indexing support to inc_subtensor and set_subtensor (James Bergstra)\n * tensor.{zeros,ones}_like now supports the dtype param as numpy (Frederic)\n * Added configuration flag \"exception_verbosity\" to control the verbosity of exceptions (Ian)\n * theano-cache list: list the content of the theano cache (Frederic)\n * theano-cache unlock: remove the Theano cache lock (Olivier)\n * tensor.ceil_int_div to compute ceil(a / float(b)) (Frederic)\n * MaxAndArgMax.grad now works with any axis (The op supports only 1 axis) (Frederic)\n     * used by tensor.{max,min,max_and_argmax}\n * tensor.{all,any} (Razvan)\n * tensor.roll as numpy: (Matthew Rocklin, David Warde-Farley)\n * Theano with GPU works in some cases on Windows now. Still experimental. (Sebastian Urban)\n * IfElse now allows to have a list/tuple as the result of the if/else branches.\n     * They must have the same length and corresponding type (Razvan)\n * Argmax output dtype is now int64 instead of int32. (Olivier)\n * Added the element-wise operation arccos. (Ian)\n * Added sparse dot with dense grad output. (Yann Dauphin)\n     * Optimized to Usmm and UsmmCscDense in some case (Yann)\n     * Note: theano.dot and theano.sparse.structured_dot() always had a gradient with the same sparsity pattern as the inputs.\n       The new theano.sparse.dot() has a dense gradient for all inputs.\n * GpuAdvancedSubtensor1 supports broadcasted dimensions. (Frederic)\n * TensorVariable.zeros_like() and SparseVariable.zeros_like()\n * theano.sandbox.cuda.cuda_ndarray.cuda_ndarray.device_properties() (Frederic)\n * theano.sandbox.cuda.cuda_ndarray.cuda_ndarray.mem_info() return free and total gpu memory (Frederic)\n * Theano flags compiledir_format. Keep the same default as before: compiledir_%(platform)s-%(processor)s-%(python_version)s. (Josh Bleecher Snyder)\n     * We also support the \"theano_version\" substitution.\n * IntDiv C code (faster and allows this elemwise to be fused with other elemwise) (Pascal)\n * Internal filter_variable mechanism in Type. (Pascal, Ian)\n    * Ifelse works on sparse.\n    * It makes use of gpu shared variable more transparent with theano.function updates and givens parameter.\n * Added a_tensor.transpose(axes) axes is optional (James)\n    * theano.tensor.transpose(a_tensor, kwargs) We were ignoring kwargs, now it is used as the axes.\n * a_CudaNdarray_object[*] = int, now works (Frederic)\n * tensor_variable.size (as numpy) computes the product of the shape elements. (Olivier)\n * sparse_variable.size (as scipy) computes the number of stored values. (Olivier)\n * sparse_variable[N, N] now works (Li Yao, Frederic)\n * sparse_variable[M:N, O:P] now works (Li Yao, Frederic, Pascal)\n   M, N, O, and P can be Python int or scalar tensor variables, None, or\n   omitted (sparse_variable[:, :M] or sparse_variable[:M, N:] work).\n * tensor.tensordot can now be moved to GPU (Sander Dieleman,\n   Pascal, based on code from Tijmen Tieleman's gnumpy,\n   http://www.cs.toronto.edu/~tijmen/gnumpy.html)\n * Many infer_shape implemented on sparse matrices op. (David W.F.)\n * Added theano.sparse.verify_grad_sparse to easily allow testing grad of\n   sparse op. It supports testing the full and structured gradients.\n * The keys in our cache now store the hash of constants and not the constant values\n   themselves. This is significantly more efficient for big constant arrays. (Frederic B.)\n * 'theano-cache list' lists key files bigger than 1M (Frederic B.)\n * 'theano-cache list' prints an histogram of the number of keys per compiled module (Frederic B.)\n * 'theano-cache list' prints the number of compiled modules per op class (Frederic B.)\n * The Theano flag \"nvcc.fastmath\" is now also used for the cuda_ndarray.cu file.\n * Add the header_dirs to the hard part of the compilation key. This is\n   currently used only by cuda, but if we use libraries that are only headers,\n   this can be useful. (Frederic B.)\n * The Theano flag \"nvcc.flags\" is now included in the hard part of the key.\n   This means that now we recompile all modules for each value of \"nvcc.flags\".\n   A change in \"nvcc.flags\" used to be ignored for modules that were already\n   compiled. (Frederic B.)\n * Alloc, GpuAlloc are not always pre-computed (constant_folding optimization)\n   at compile time if all their inputs are constant.\n   (Frederic B., Pascal L., reported by Sander Dieleman)\n * New Op tensor.sort(), wrapping numpy.sort (Hani Almousli)\n\n\nNew optimizations:\n * AdvancedSubtensor1 reuses preallocated memory if available (scan, c|py_nogc linker) (Frederic)\n * dot22, dot22scalar work with complex. (Frederic)\n * Generate Gemv/Gemm more often. (James)\n * Remove scan when all computations can be moved outside the loop. (Razvan)\n * scan optimization done earlier. This allows other optimizations to be applied. (Frederic, Guillaume, Razvan)\n * exp(x) * sigmoid(-x) is now correctly optimized to the more stable form sigmoid(x). (Olivier)\n * Added Subtensor(Rebroadcast(x)) => Rebroadcast(Subtensor(x)) optimization. (Guillaume)\n * Made the optimization process faster. (James)\n * Allow fusion of elemwise when the scalar op needs support code. (James)\n * Better opt that lifts transpose around dot. (James)\n\n\nCrashes fixed:\n * T.mean crash at graph building time. (Ian)\n * \"Interactive debugger\" crash fix. (Ian, Frederic)\n * Do not call gemm with strides 0, some blas refuse it. (Pascal Lamblin)\n * Optimization crash with gemm and complex. (Frederic)\n * GPU crash with elemwise. (Frederic, some reported by Chris Currivan)\n * Compilation crash with amdlibm and the GPU. (Frederic)\n * IfElse crash. (Frederic)\n * Execution crash fix in AdvancedSubtensor1 on 32 bit computers. (Pascal)\n * GPU compilation crash on MacOS X. (Olivier)\n * Support for OSX Enthought Python Distribution 7.x. (Graham Taylor, Olivier)\n * When the subtensor inputs had 0 dimensions and the outputs 0 dimensions. (Frederic)\n * Crash when the step to subtensor was not 1 in conjunction with some optimization. (Frederic, reported by Olivier Chapelle)\n * Runtime crash related to an optimization with subtensor of alloc (reported by Razvan, fixed by Frederic)\n * Fix dot22scalar cast of integer scalars (Justin Bayer, Frédéric, Olivier)\n * Fix runtime crash in gemm, dot22. FB\n * Fix on 32 bit computer: make sure all shapes are int64. (Olivier)\n * Fix to deque on python 2.4 (Olivier)\n * Fix crash when not using C code (or using DebugMode) (not used by\n   default) with numpy 1.6*. Numpy has a bug in the reduction code that\n   made it crash. (Pascal)\n * Crashes of blas functions (Gemv on CPU; Ger, Gemv and Gemm on GPU)\n   when matrices had non-unit stride in both dimensions (CPU and GPU),\n   or when matrices had negative strides (GPU only). In those cases,\n   we are now making copies. (Pascal)\n * More cases supported in AdvancedIncSubtensor1. (Olivier D.)\n * Fix crash when a broadcasted constant was used as input of an\n   elemwise Op and needed to be upcasted to match the op's output.\n   (Reported by John Salvatier, fixed by Pascal L.)\n * Fixed a memory leak with shared variable (we kept a pointer to the original value) (Ian G.)\n\n\nKnown bugs:\n * CAReduce with nan in inputs don't return the good output (`Ticket <https://www.assembla.com/spaces/theano/tickets/763>`_).\n     * This is used in tensor.{max,mean,prod,sum} and in the grad of PermuteRowElements.\n\n\nSandbox:\n * cvm interface more consistent with current linker. (James)\n   * Now all tests pass with the linker=cvm flags.\n * vm linker has a callback parameter. (James)\n * review/finish/doc: diag/extract_diag. (Arnaud Bergeron, Frederic, Olivier)\n * review/finish/doc: AllocDiag/diag. (Arnaud, Frederic, Guillaume)\n * review/finish/doc: MatrixInverse, matrix_inverse. (Razvan)\n * review/finish/doc: matrix_dot. (Razvan)\n * review/finish/doc: det (determinent) op. (Philippe Hamel)\n * review/finish/doc: Cholesky determinent op. (David)\n * review/finish/doc: ensure_sorted_indices. (Li Yao)\n * review/finish/doc: spectral_radius_boud. (Xavier Glorot)\n * review/finish/doc: sparse sum. (Valentin Bisson)\n * review/finish/doc: Remove0 (Valentin)\n * review/finish/doc: SquareDiagonal (Eric)\n\n\nSandbox New features (not enabled by default):\n * CURAND_RandomStreams for uniform and normal (not picklable, GPU only) (James)\n * New sandbox.linalg.ops.pinv(pseudo-inverse) op (Razvan)\n\n\nDocumentation:\n * Many updates. (Many people)\n * Updates to install doc on MacOS. (Olivier)\n * Updates to install doc on Windows. (David, Olivier)\n * Doc on the Rop function (Ian)\n * Added how to use scan to loop with a condition as the number of iteration. (Razvan)\n * Added how to wrap in Theano an existing python function (in numpy, scipy, ...). (Frederic)\n * Refactored GPU installation of Theano. (Olivier)\n\n\nOthers:\n * Better error messages in many places. (Many people)\n * PEP8 fixes. (Many people)\n * Add a warning about numpy bug when using advanced indexing on a\n   tensor with more than 2**32 elements (the resulting array is not\n   correctly filled and ends with zeros). (Pascal, reported by David WF)\n * Added Scalar.ndim=0 and ScalarSharedVariable.ndim=0 (simplify code) (Razvan)\n * New min_informative_str() function to print graph. (Ian)\n * Fix catching of exception. (Sometimes we used to catch interrupts) (Frederic, David, Ian, Olivier)\n * Better support for utf string. (David)\n * Fix pydotprint with a function compiled with a ProfileMode (Frederic)\n     * Was broken with change to the profiler.\n * Warning when people have old cache entries. (Olivier)\n * More tests for join on the GPU and CPU. (Frederic)\n * Do not request to load the GPU module by default in scan module. (Razvan)\n * Fixed some import problems. (Frederic and others)\n * Filtering update. (James)\n * On Windows, the default compiledir changed to be local to the\n   computer/user and not transferred with roaming profile. (Sebastian\n   Urban)\n * New theano flag \"on_shape_error\". Defaults to \"warn\" (same as previous behavior):\n   it prints a warning when an error occurs when inferring the shape of some apply node.\n   The other accepted value is \"raise\" to raise an error when this happens. (Frederic)\n * The buidbot now raises optimization/shape errors instead of just printing a warning. (Frederic)\n * better pycuda tests (Frederic)\n * check_blas.py now accepts the shape and the number of iterations as parameter (Frederic)\n * Fix opt warning when the opt ShapeOpt is disabled (enabled by default) (Frederic)\n * More internal verification on what each op.infer_shape return. (Frederic, James)\n * Improved docstring and basic tests for the Tile Op (David).\n\nReviewers (alphabetical order):\n * David, Frederic, Ian, James, Olivier, Razvan\n\n\nTheano 0.4.1 (12 August 2011)\n=============================\n\nNew features:\n\n * `R_op <http://deeplearning.net/software/theano/tutorial/gradients.html>`_ macro like theano.tensor.grad\n\n   * Not all tests are done yet (TODO)\n * Added alias theano.tensor.bitwise_{and,or,xor,not}. They are the numpy names.\n * Updates returned by Scan (you need to pass them to the theano.function) are now a new Updates class.\n   That allow more check and easier work with them. The Updates class is a subclass of dict\n * Scan can now work in a \"do while\" loop style.\n\n   * We scan until a condition is met.\n   * There is a minimum of 1 iteration(can't do \"while do\" style loop)\n * The \"Interactive Debugger\" (compute_test_value theano flags)\n\n   * Now should work with all ops (even the one with only C code)\n   * In the past some errors were caught and re-raised as unrelated errors (ShapeMismatch replaced with NotImplemented). We don't do that anymore.\n * The new Op.make_thunk function(introduced in 0.4.0) is now used by constant_folding and DebugMode\n * Added A_TENSOR_VARIABLE.astype() as a way to cast. NumPy allows this syntax.\n * New BLAS GER implementation.\n * Insert GEMV more frequently.\n * Added new ifelse(scalar condition, rval_if_true, rval_if_false) Op.\n\n   * This is a subset of the elemwise switch (tensor condition, rval_if_true, rval_if_false).\n   * With the new feature in the sandbox, only one of rval_if_true or rval_if_false will be evaluated.\n\nOptimizations:\n\n * Subtensor has C code\n * {Inc,Set}Subtensor has C code\n * ScalarFromTensor has C code\n * dot(zeros,x) and dot(x,zeros)\n * IncSubtensor(x, zeros, idx) -> x\n * SetSubtensor(x, x[idx], idx) -> x (when x is a constant)\n * subtensor(alloc,...) -> alloc\n * Many new scan optimization\n\n   * Lower scan execution overhead with a Cython implementation\n   * Removed scan double compilation (by using the new Op.make_thunk mechanism)\n   * Certain computations from the inner graph are now Pushed out into the outer\n     graph. This means they are not re-comptued at every step of scan.\n   * Different scan ops get merged now into a single op (if possible), reducing\n     the overhead and sharing computations between the two instances\n\nGPU:\n\n * PyCUDA/CUDAMat/Gnumpy/Theano bridge and `documentation <http://deeplearning.net/software/theano/tutorial/gpu_data_convert.html>`_.\n\n   * New function to easily convert pycuda GPUArray object to and from CudaNdarray object\n   * Fixed a bug if you crated a view of a manually created CudaNdarray that are view of GPUArray.\n * Removed a warning when nvcc is not available and the user did not requested it.\n * renamed config option cuda.nvccflags -> nvcc.flags\n * Allow GpuSoftmax and GpuSoftmaxWithBias to work with bigger input.\n\nBugs fixed:\n\n * In one case an AdvancedSubtensor1 could be converted to a GpuAdvancedIncSubtensor1 insted of GpuAdvancedSubtensor1.\n   It probably didn't happen due to the order of optimizations, but that order is not guaranteed to be the same on all computers.\n * Derivative of set_subtensor was wrong.\n * Derivative of Alloc was wrong.\n\nCrash fixed:\n\n * On an unusual Python 2.4.4 on Windows\n * When using a C cache copied from another location\n * On Windows 32 bits when setting a complex64 to 0.\n * Compilation crash with CUDA 4\n * When wanting to copy the compilation cache from a computer to another\n\n   * This can be useful for using Theano on a computer without a compiler.\n * GPU:\n\n   * Compilation crash fixed under Ubuntu 11.04\n   * Compilation crash fixed with CUDA 4.0\n\nKnow bug:\n\n * CAReduce with nan in inputs don't return the good output (`Ticket <http://trac-hg.assembla.com/theano/ticket/763>`_).\n\n   * This is used in tensor.{max,mean,prod,sum} and in the grad of PermuteRowElements.\n   * This is not a new bug, just a bug discovered since the last release that we didn't had time to fix.\n\nDeprecation (will be removed in Theano 0.5, warning generated if you use them):\n\n * The string mode (accepted only by theano.function()) FAST_RUN_NOGC. Use Mode(linker='c|py_nogc') instead.\n * The string mode (accepted only by theano.function()) STABILIZE. Use Mode(optimizer='stabilize') instead.\n * scan interface change:\n\n   * The use of `return_steps` for specifying how many entries of the output\n     scan has been deprecated\n\n     * The same thing can be done by applying a subtensor on the output\n       return by scan to select a certain slice\n   * The inner function (that scan receives) should return its outputs and\n     updates following this order:\n\n        [outputs], [updates], [condition]. One can skip any of the three if not\n        used, but the order has to stay unchanged.\n * tensor.grad(cost, wrt) will return an object of the \"same type\" as wrt\n   (list/tuple/TensorVariable).\n\n   * Currently tensor.grad return a type list when the wrt is a list/tuple of\n     more than 1 element.\n\nDecrecated in 0.4.0(Reminder, warning generated if you use them):\n\n * Dividing integers with / is deprecated: use // for integer division, or\n   cast one of the integers to a float type if you want a float result (you may\n   also change this behavior with config.int_division).\n * tag.shape attribute deprecated (#633)\n * CudaNdarray_new_null is deprecated in favour of CudaNdarray_New\n\nSandbox:\n\n * MRG random generator now implements the same casting behavior as the regular random generator.\n\nSandbox New features(not enabled by default):\n\n * New Linkers (theano flags linker={vm,cvm})\n\n   * The new linker allows lazy evaluation of the new ifelse op, meaning we compute only the true or false branch depending of the condition. This can speed up some types of computation.\n   * Uses a new profiling system (that currently tracks less stuff)\n   * The cvm is implemented in C, so it lowers Theano's overhead.\n   * The vm is implemented in python. So it can help debugging in some cases.\n   * In the future, the default will be the cvm.\n * Some new not yet well tested sparse ops: theano.sparse.sandbox.{SpSum, Diag, SquareDiagonal, ColScaleCSC, RowScaleCSC, Remove0, EnsureSortedIndices, ConvolutionIndices}\n\nDocumentation:\n\n * How to compute the `Jacobian, Hessian, Jacobian times a vector, Hessian times a vector <http://deeplearning.net/software/theano/tutorial/gradients.html>`_.\n * Slide for a 3 hours class with exercises that was done at the HPCS2011 Conference in Montreal.\n\nOthers:\n\n * Logger name renamed to be consistent.\n * Logger function simplified and made more consistent.\n * Fixed transformation of error by other not related error with the compute_test_value Theano flag.\n * Compilation cache enhancements.\n * Made compatible with NumPy 1.6 and SciPy 0.9\n * Fix tests when there was new dtype in NumPy that is not supported by Theano.\n * Fixed some tests when SciPy is not available.\n * Don't compile anything when Theano is imported. Compile support code when we compile the first C code.\n * Python 2.4 fix:\n\n   * Fix the file theano/misc/check_blas.py\n   * For python 2.4.4 on Windows, replaced float(\"inf\") with numpy.inf.\n * Removes useless inputs to a scan node\n\n   * Beautification mostly, making the graph more visible. Such inputs would appear as a consequence of other optimizations\n\nCore:\n\n * there is a new mechanism that lets an Op permit that one of its\n   inputs to be aliased to another destroyed input.  This will generally\n   result in incorrect calculation, so it should be used with care!  The\n   right way to use it is when the caller can guarantee that even if\n   these two inputs look aliased, they actually will never overlap. This\n   mechanism can be used, for example, by a new alternative approach to\n   implementing Scan.  If an op has an attribute called\n   \"destroyhandler_tolerate_aliased\" then this is what's going on.\n   IncSubtensor is thus far the only Op to use this mechanism.Mechanism\n\nTheano 0.4.0 (2011-06-13)\n=========================\n\nChange in output memory storage for Ops:\n If you implemented custom Ops, with either C or Python implementation,\n this will concern you.\n\n The contract for memory storage of Ops has been changed. In particular,\n it is no longer guaranteed that output memory buffers are either empty,\n or allocated by a previous execution of the same Op.\n\n Right now, here is the situation:\n  * For Python implementation (perform), what is inside output_storage\n    may have been allocated from outside the perform() function, for\n    instance by another node (e.g., Scan) or the Mode. If that was the\n    case, the memory can be assumed to be C-contiguous (for the moment).\n  * For C implementations (c_code), nothing has changed yet.\n\n In a future version, the content of the output storage, both for Python and C\n versions, will either be NULL, or have the following guarantees:\n  * It will be a Python object of the appropriate Type (for a Tensor variable,\n    a numpy.ndarray, for a GPU variable, a CudaNdarray, for instance)\n  * It will have the correct number of dimensions, and correct dtype\n However, its shape and memory layout (strides) will not be guaranteed.\n\n When that change is made, the config flag DebugMode.check_preallocated_output\n will help you find implementations that are not up-to-date.\n\nDeprecation:\n * tag.shape attribute deprecated (#633)\n * CudaNdarray_new_null is deprecated in favour of CudaNdarray_New\n * Dividing integers with / is deprecated: use // for integer division, or\n   cast one of the integers to a float type if you want a float result (you may\n   also change this behavior with config.int_division).\n * Removed (already deprecated) sandbox/compile module\n * Removed (already deprecated) incsubtensor and setsubtensor functions,\n   inc_subtensor and set_subtensor are to be used instead.\n\nBugs fixed:\n * In CudaNdarray.__{iadd,idiv}__, when it is not implemented, return the error.\n * THEANO_FLAGS='optimizer=None' now works as expected\n * Fixed memory leak in error handling on GPU-to-host copy\n * Fix relating specifically to Python 2.7 on Mac OS X\n * infer_shape can now handle Python longs\n * Trying to compute x % y with one or more arguments being complex now\n   raises an error.\n * The output of random samples computed with uniform(..., dtype=...) is\n   guaranteed to be of the specified dtype instead of potentially being of a\n   higher-precision dtype.\n * The perform() method of DownsampleFactorMax did not give the right result\n   when reusing output storage. This happen only if you use the Theano flags\n   'linker=c|py_nogc' or manually specify the mode to be 'c|py_nogc'.\n\nCrash fixed:\n * Work around a bug in gcc 4.3.0 that make the compilation of 2d convolution\n   crash.\n * Some optimizations crashed when the \"ShapeOpt\" optimization was disabled.\n\nOptimization:\n * Optimize all subtensor followed by subtensor.\n\nGPU:\n * Move to the gpu fused elemwise that have other dtype then float32 in them\n   (except float64) if the input and output are float32.\n   * This allow to move elemwise comparisons to the GPU if we cast it to\n     float32 after that.\n * Implemented CudaNdarray.ndim to have the same interface in ndarray.\n * Fixed slowdown caused by multiple chained views on CudaNdarray objects\n * CudaNdarray_alloc_contiguous changed so as to never try to free\n   memory on a view: new \"base\" property\n * Safer decref behaviour in CudaNdarray in case of failed allocations\n * New GPU implementation of tensor.basic.outer\n * Multinomial random variates now available on GPU\n\nNew features:\n * ProfileMode\n    * profile the scan overhead\n    * simple hook system to add profiler\n    * reordered the output to be in the order of more general to more specific\n * DebugMode now checks Ops with different patterns of preallocated memory,\n   configured by config.DebugMode.check_preallocated_output.\n * var[vector of index] now work, (grad work recursively, the direct grad\n   work inplace, gpu work)\n    * limitation: work only of the outer most dimensions.\n * New way to test the graph as we build it. Allow to easily find the source\n   of shape mismatch error:\n   `http://deeplearning.net/software/theano/tutorial/debug_faq.html#interactive-debugger`__\n * cuda.root inferred if nvcc is on the path, otherwise defaults to\n   /usr/local/cuda\n * Better graph printing for graphs involving a scan subgraph\n * Casting behavior can be controlled through config.cast_policy,\n   new (experimental) mode.\n * Smarter C module cache, avoiding erroneous usage of the wrong C\n   implementation when some options change, and avoiding recompiling the\n   same module multiple times in some situations.\n * The \"theano-cache clear\" command now clears the cache more thoroughly.\n * More extensive linear algebra ops (CPU only) that wrap scipy.linalg\n   now available in the sandbox.\n * CUDA devices 4 - 16 should now be available if present.\n * infer_shape support for the View op, better infer_shape support in Scan\n * infer_shape supported in all case of subtensor\n * tensor.grad now gives an error by default when computing the gradient\n   wrt a node that is disconnected from the cost (not in the graph, or\n   no continuous path from that op to the cost).\n * New tensor.isnan and isinf functions.\n\nDocumentation:\n * Better commenting of cuda_ndarray.cu\n * Fixes in the scan documentation: add missing declarations/print statements\n * Better error message on failed __getitem__\n * Updated documentation on profile mode\n * Better documentation of testing on Windows\n * Better documentation of the 'run_individual_tests' script\n\nUnit tests:\n * More strict float comparaison by default\n * Reuse test for subtensor of tensor for gpu tensor(more gpu test)\n * Tests that check for aliased function inputs and assure appropriate copying\n   (#374)\n * Better test of copies in CudaNdarray\n * New tests relating to the new base pointer requirements\n * Better scripts to run tests individually or in batches\n * Some tests are now run whenever cuda is available and not just when it has\n   been enabled before\n * Tests display less pointless warnings.\n\nOther:\n * Correctly put the broadcast flag to True in the output var of\n   a Reshape op when we receive an int 1 in the new shape.\n * pydotprint: high contrast mode is now the default, option to print\n   more compact node names.\n * pydotprint: How trunk label that are too long.\n * More compact printing (ignore leading \"Composite\" in op names)\n\n\nTheano 0.3.1 (2011-02-21)\n=========================\n\nDeprecation:\n * The theano shared variable attribute `value` is deprecated, use `get_value()` or `set_value()`!\n    See http://deeplearning.net/software/theano/tutorial/aliasing.html\n\nBugs fixed:\n * The random number generator in theano/sandbox/rng_mrg.py did not always return the same sequence of number on the CPU and GPU.\n    * In some cases, there was a (possibly large) fraction of non-random garbage in the returned sequence.\n\n * In python mode (not the default mode) when input of elemwise operation was an empty ndarray, we were not returning an empty ndarray.\n * Scan cached the number of steps. This caused no problem because each time you called scan the number of steps would got refreshed.\n   The problem was when you called ScanGrad which would use the cached number of steps without refreshing it.\n   To be affected by this bug, one would have to compile two graph, one that would contain a Scan and the other the corresponding GradScan, and\n   call the first function to cache the number of steps, and then call the second function with a different number of steps.\n * In GpuConv, errors in conv_patch_stack_reduce when the entire kernel doesn't fit into shared memory.\n   The error was not found before as the impact was less then the relative tolerance of 1e-3. Now the relative tolerance is 1e-5.\n\nCrash fixed:\n * Add a feature to not have an exception that makes Theano crash when taking the gradient on DimShuffle in some particular case.\n * Compilation crash for GpuElemwise with tensor with high number of dimensions (~6 or more).\n * Disabled C code generator that make gcc crash on complex type.\n * Crash in optimization when an Op has no input.\n * Output shape is now computed correctly for matrix-vector multiplication on GPU.\n * In Scan, when using numbers as inputs, not symbolic variables.\n * In GradScan, when there is only 1 inputs in the Scan.\n * In GpuSum, bug in calculation of n_blocks for the 10 pattern. (Sum on the row of a matrix)\n * Some segfault at exit with GPU code.\n\nOptimization:\n * New SpecifyShape op that allow to pass more shape info in the graph.\n * Speed up gemv by a work around scipy gemv slowness when the matrix is in C order (the default).\n * Remove join of only 1 element.\n * During optimization, consider one more case in get_constant_value.\n\nGPU:\n * cuda_shared.value = X now works inplace!\n     * cuda_shared_var.set_value(new_ndarray) will overwrite the old value inplace in the most common case.\n * Allow to create a CudaNdarraySharedVariable from a CudaNdarray.\n * New init_gpu_device theano flags.\n * Fuse GpuElemwise more often (in the case where there are so many inputs that fusing them all would bust the 256 bytes limit of parameter to gpu function).\n * CPU join of only 1 element that was not moved to the GPU.\n\nNew features:\n * tensor.reshape now makes dimensions of length 1 broadcastable.\n * tensor.prod now implements the gradient.\n * DebugMode now warns if an Op declared itself as returning a view of the input but did not do so.\n    * This behaviour is a problem, because it can block other Ops from being inplace on the same inputs. This could lower the reuse of memory.\n * Sparse.structured_dot now works when both matrices are sparse\n * Sparse type is now supported by the shape op, and the ShapeFeature optimizer works correctly with them.\n * New 3D convolution ops, with CPU and GPU implementations.\n * New colors in pydotprint.\n\nDocumentation:\n * Documented lib.amdlibm and (new) init_gpu_device config variables.\n * A new page (was done for 0.3 but an error was hiding it on the web page) on the memory aliasing contract of Theano.\n * Revision to the Windows installation instructions.\n * The cuda documentation is now generated on the web server.\n * Better documentation of .theanorc and its sections.\n\nUnit tests:\n * Stop usage of deprecated functions or syntax in the unit tests.\n * Better testing of GPU convolution nets.\n * Make more tests able to use different random seeds.\n * Tests of sparse now use default mode, not a hard-coded one.\n * Remove some tests of unimplemented features.\n\nOther:\n * The name of compiledir now includes the Python version to make it easier for people with many Python versions\n * Added theano.tensor.std as a shortcut to sqrt(var(input=input, axis=axis)).\n * Whitespace, tabulation and indentation clean-up in the code.\n * Better detection of memory sharing between variables.\n\n\nTheano 0.3 (2010-11-23)\n=======================\n\nThis is the first major release of Theano since 0.1. Version 0.2 development started internally but it was never advertised as a release.\n\nThere have been so many changes since 0.1 that we have lost track of many of them. Below is a *partial* list of changes since 0.1.\n\n * GPU code using NVIDIA's CUDA framework is now generated for many Ops.\n * Some interface changes since 0.1:\n     * A new \"shared variable\" system to allow reusing memory space between Theano functions.\n         * A new memory contract has been formally written for Theano, for people who want to minimize memory copies.\n     * The old module system has been deprecated.\n     * By default, inputs to a Theano function will not be silently downcasted (e.g. from float64 to float32).\n     * An error is now raised when using the result of logical operation on Theano variable in an 'if' (i.e. an implicit call to __nonzeros__).\n     * An error is now raised when we receive a non-aligned ndarray as input to a function (this is not supported).\n     * An error is raised when the list of dimensions passed to dimshuffle() contains duplicates or is otherwise not sensible.\n     * Call NumPy BLAS bindings for gemv operations in addition to the already supported gemm.\n     * If gcc is unavailable at import time, Theano now falls back to a Python-based emulation mode after raising a warning.\n     * An error is now raised when tensor.grad is called on a non-scalar Theano variable (in the past we would implicitly do a sum on the tensor to make it a scalar).\n     * Added support for \"erf\" and \"erfc\" functions.\n * The current default value of the parameter axis of theano.{max,min,argmax,argmin,max_and_argmax} is deprecated. We now use the default NumPy behavior of operating on the entire tensor.\n * Theano is now available from PyPI and installable through \"easy_install\" or \"pip\".\n\n\nTheano 0.1\n==========\n\n*Release date: 2009-04-02*\n\nWhat works\n----------\n\n- building symbolic expression.\n- arranging symbolic expressions into Modules so that multiple functions\n  can work on the same data.\n- symbolic gradient descent.\n- graph optimization.\n- compilation to C for many kinds of expression.\n- a debugging mode that checks that your expression results are correct,\n  using a variety of sanity checks.\n\nWhat's missing?\n---------------\n\n- An algorithm library. We're missing a library of examples and standard\n  component implementations.  Some examples will find their way into\n  the Theano repo, but standard algorithms will go into the 'pylearn'\n  project (toolbox style). Now that we have a stable foundation, we\n  can reach a consensus on style for algorithms.\n"
        },
        {
          "name": "ISSUE_TEMPLATE.md",
          "type": "blob",
          "size": 0.6318359375,
          "content": "If you have any questions on Theano, please ask your questions in either [theano-user mailing list](https://groups.google.com/forum/#!forum/theano-users) or [stackoverflow](http://stackoverflow.com/) with the \"theano\" tag.\n\nUse issue only to report bugs or to ask for new features in Theano.\n\nBefore reporting a bug, update to Theano development version. Maybe it\nis already fixed. If not, tell us the Theano flags and Theano version\nthat generate the problem. If it is a regression, can do a \"git\nbisect\" to idenfy when the problem appeard. This really help fix it\nrapidly.\n\nIf you add a feature request, describe in which case it will be useful."
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 0.0146484375,
          "content": "doc/LICENSE.txt"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.3037109375,
          "content": "global-include *.txt\nglobal-include *.c\nglobal-include *.cu\nglobal-include *.cuh\nglobal-include *.cpp\nglobal-include *.h\nglobal-include *.sh\nglobal-include *.pkl\nrecursive-include doc\ninclude bin/theano-cache\ninclude bin/theano-nose\nprune .jenkins\nprune .travis\ninclude versioneer.py\ninclude theano/_version.py\n"
        },
        {
          "name": "NEWS.txt",
          "type": "blob",
          "size": 0.65234375,
          "content": "=============\nRelease Notes\n=============\n\nTheano 1.0.5 (27th of July 2020)\n================================\n\nThis is a maintenance release of Theano, version ``1.0.5``, with no\nnew features, but some deprecation fixes.\n\nWe recommend that everybody update to this version.\n\nHighlights (since 1.0.4):\n\n - Theano is now compatible with Python 3.9\n - Fixed many deprecation warnings\n\nA total of 13 people contributed to this release since ``1.0.4``:\n\n - 1fish2\n - Frederic Bastien\n - Rebecca Palmer\n - Miro Hrončok\n - Dan Foreman-Mackey\n - Adrian Seyboldt\n - abergeron\n - Tim Gates\n - Tim Odonnell\n - Robert P. Goldman\n - Duc Nguyen\n - Igor Varfolomeev\n - Thomas Wiecki\n"
        },
        {
          "name": "NEWS_DEV.txt",
          "type": "blob",
          "size": 16.79296875,
          "content": ".. _NEWS_DEV:\n\n===================\nDRAFT Release Notes\n===================\n\ngit log -p rel-1.0.1... |grep Merge|grep '#[0123456789]' |cut -f 8 -d ' ' | sed 's\\#\\* https://github.com/Theano/Theano/pull/\\'\n\n# Commit count per user\ngit shortlog -sn rel-1.0.1..\n\n\nHighlights:\n - ...\n\nInterface changes:\n - ...\n\nConvolution updates:\n - ...\n\nGPU:\n - ...\n\n - cuDNN support\n   - ...\n\nNew features:\n - ...\n\nOthers:\n - ...\n\nOther more detailed changes:\n - ...\n\n\nPULL REQUESTS CHECKED FOR 1.0.1 SINCE 1.0.0\n* https://github.com/Theano/Theano/pull/6530\n* https://github.com/Theano/Theano/pull/6513\n* https://github.com/Theano/Theano/pull/6520\n* https://github.com/Theano/Theano/pull/6525\n* https://github.com/Theano/Theano/pull/6517\n* https://github.com/Theano/Theano/pull/6506\n* https://github.com/Theano/Theano/pull/6512\n\nALL THE PR BELLOW HAVE BEEN CHECKED FOR FINAL RELEASE 1.0.0 SINCE 0.9.0\n* https://github.com/Theano/Theano/pull/6509\n* https://github.com/Theano/Theano/pull/6508\n* https://github.com/Theano/Theano/pull/6505\n* https://github.com/Theano/Theano/pull/6496\n* https://github.com/Theano/Theano/pull/6495\n* https://github.com/Theano/Theano/pull/6492\n* https://github.com/Theano/Theano/pull/6489\n* https://github.com/Theano/Theano/pull/6488\n* https://github.com/Theano/Theano/pull/6490\n* https://github.com/Theano/Theano/pull/5932\n* https://github.com/Theano/Theano/pull/6479\n* https://github.com/Theano/Theano/pull/6401\n* https://github.com/Theano/Theano/pull/6472\n* https://github.com/Theano/Theano/pull/6477\n* https://github.com/Theano/Theano/pull/6475\n* https://github.com/Theano/Theano/pull/6468\n* https://github.com/Theano/Theano/pull/6467\n* https://github.com/Theano/Theano/pull/6469\n* https://github.com/Theano/Theano/pull/6466\n* https://github.com/Theano/Theano/pull/6460\n* https://github.com/Theano/Theano/pull/6459\n* https://github.com/Theano/Theano/pull/6457\n* https://github.com/Theano/Theano/pull/6456\n* https://github.com/Theano/Theano/pull/6453\n* https://github.com/Theano/Theano/pull/6452\n* https://github.com/Theano/Theano/pull/6430\n* https://github.com/Theano/Theano/pull/6447\n* https://github.com/Theano/Theano/pull/6446\n* https://github.com/Theano/Theano/pull/6431\n* https://github.com/Theano/Theano/pull/6445\n* https://github.com/Theano/Theano/pull/6348\n* https://github.com/Theano/Theano/pull/6416\n* https://github.com/Theano/Theano/pull/6443\n* https://github.com/Theano/Theano/pull/6440\n* https://github.com/Theano/Theano/pull/6388\n* https://github.com/Theano/Theano/pull/5641\n* https://github.com/Theano/Theano/pull/6367\n* https://github.com/Theano/Theano/pull/6437\n* https://github.com/Theano/Theano/pull/6439\n* https://github.com/Theano/Theano/pull/6425\n* https://github.com/Theano/Theano/pull/6434\n* https://github.com/Theano/Theano/pull/5959\n* https://github.com/Theano/Theano/pull/6005\n* https://github.com/Theano/Theano/pull/6427\n* https://github.com/Theano/Theano/pull/6424\n* https://github.com/Theano/Theano/pull/6419\n* https://github.com/Theano/Theano/pull/6415\n* https://github.com/Theano/Theano/pull/6418\n* https://github.com/Theano/Theano/pull/5891\n* https://github.com/Theano/Theano/pull/6316\n* https://github.com/Theano/Theano/pull/6331\n* https://github.com/Theano/Theano/pull/6100\n* https://github.com/Theano/Theano/pull/6412\n* https://github.com/Theano/Theano/pull/6221\n* https://github.com/Theano/Theano/pull/6386\n* https://github.com/Theano/Theano/pull/6411\n* https://github.com/Theano/Theano/pull/6405\n* https://github.com/Theano/Theano/pull/6410\n* https://github.com/Theano/Theano/pull/6413\n* https://github.com/Theano/Theano/pull/6389\n* https://github.com/Theano/Theano/pull/6409\n* https://github.com/Theano/Theano/pull/6406\n* https://github.com/Theano/Theano/pull/6396\n* https://github.com/Theano/Theano/pull/6392\n* https://github.com/Theano/Theano/pull/6393\n* https://github.com/Theano/Theano/pull/6384\n* https://github.com/Theano/Theano/pull/6326\n* https://github.com/Theano/Theano/pull/6317\n* https://github.com/Theano/Theano/pull/6269\n* https://github.com/Theano/Theano/pull/5688\n* https://github.com/Theano/Theano/pull/6376\n* https://github.com/Theano/Theano/pull/6377\n* https://github.com/Theano/Theano/pull/6355\n* https://github.com/Theano/Theano/pull/6373\n* https://github.com/Theano/Theano/pull/6374\n* https://github.com/Theano/Theano/pull/6371\n* https://github.com/Theano/Theano/pull/6362\n* https://github.com/Theano/Theano/pull/6368\n* https://github.com/Theano/Theano/pull/6339\n* https://github.com/Theano/Theano/pull/6366\n* https://github.com/Theano/Theano/pull/6364\n* https://github.com/Theano/Theano/pull/6349\n* https://github.com/Theano/Theano/pull/6361\n* https://github.com/Theano/Theano/pull/6356\n* https://github.com/Theano/Theano/pull/6359\n* https://github.com/Theano/Theano/pull/6286\n* https://github.com/Theano/Theano/pull/6357\n* https://github.com/Theano/Theano/pull/6354\n* https://github.com/Theano/Theano/pull/6336\n* https://github.com/Theano/Theano/pull/6351\n* https://github.com/Theano/Theano/pull/6301\n* https://github.com/Theano/Theano/pull/6333\n* https://github.com/Theano/Theano/pull/6341\n* https://github.com/Theano/Theano/pull/6332\n* https://github.com/Theano/Theano/pull/6319\n* https://github.com/Theano/Theano/pull/6302\n* https://github.com/Theano/Theano/pull/6300\n* https://github.com/Theano/Theano/pull/6323\n* https://github.com/Theano/Theano/pull/6324\n* https://github.com/Theano/Theano/pull/5817\n* https://github.com/Theano/Theano/pull/6312\n* https://github.com/Theano/Theano/pull/6061\n* https://github.com/Theano/Theano/pull/6305\n* https://github.com/Theano/Theano/pull/6059\n* https://github.com/Theano/Theano/pull/6315\n* https://github.com/Theano/Theano/pull/6295\n* https://github.com/Theano/Theano/pull/6252\n* https://github.com/Theano/Theano/pull/6267\n* https://github.com/Theano/Theano/pull/6207\n* https://github.com/Theano/Theano/pull/6309\n* https://github.com/Theano/Theano/pull/6307\n* https://github.com/Theano/Theano/pull/6000\n* https://github.com/Theano/Theano/pull/6293\n* https://github.com/Theano/Theano/pull/6292\n* https://github.com/Theano/Theano/pull/6299\n* https://github.com/Theano/Theano/pull/6143\n* https://github.com/Theano/Theano/pull/6296\n* https://github.com/Theano/Theano/pull/6280\n* https://github.com/Theano/Theano/pull/6289\n* https://github.com/Theano/Theano/pull/6285\n* https://github.com/Theano/Theano/pull/6275\n* https://github.com/Theano/Theano/pull/6218\n* https://github.com/Theano/Theano/pull/6271\n* https://github.com/Theano/Theano/pull/6253\n* https://github.com/Theano/Theano/pull/6273\n* https://github.com/Theano/Theano/pull/6262\n* https://github.com/Theano/Theano/pull/6214\n* https://github.com/Theano/Theano/pull/6264\n* https://github.com/Theano/Theano/pull/6256\n* https://github.com/Theano/Theano/pull/6254\n* https://github.com/Theano/Theano/pull/6220\n* https://github.com/Theano/Theano/pull/5949\n* https://github.com/Theano/Theano/pull/6243\n* https://github.com/Theano/Theano/pull/6250\n* https://github.com/Theano/Theano/pull/6225\n* https://github.com/Theano/Theano/pull/6242\n* https://github.com/Theano/Theano/pull/6213\n* https://github.com/Theano/Theano/pull/6199\n* https://github.com/Theano/Theano/pull/6209\n* https://github.com/Theano/Theano/pull/6216\n* https://github.com/Theano/Theano/pull/6215\n* https://github.com/Theano/Theano/pull/6182\n* https://github.com/Theano/Theano/pull/6194\n* https://github.com/Theano/Theano/pull/6190\n* https://github.com/Theano/Theano/pull/6146\n* https://github.com/Theano/Theano/pull/6201\n* https://github.com/Theano/Theano/pull/6150\n* https://github.com/Theano/Theano/pull/6204\n* https://github.com/Theano/Theano/pull/6166\n* https://github.com/Theano/Theano/pull/6174\n* https://github.com/Theano/Theano/pull/6205\n* https://github.com/Theano/Theano/pull/6183\n* https://github.com/Theano/Theano/pull/6186\n* https://github.com/Theano/Theano/pull/6203\n* https://github.com/Theano/Theano/pull/6161\n* https://github.com/Theano/Theano/pull/6164\n* https://github.com/Theano/Theano/pull/6050\n* https://github.com/Theano/Theano/pull/6178\n* https://github.com/Theano/Theano/pull/6180\n* https://github.com/Theano/Theano/pull/6173\n* https://github.com/Theano/Theano/pull/6170\n* https://github.com/Theano/Theano/pull/6092\n* https://github.com/Theano/Theano/pull/6163\n* https://github.com/Theano/Theano/pull/6171\n* https://github.com/Theano/Theano/pull/6169\n* https://github.com/Theano/Theano/pull/6165\n* https://github.com/Theano/Theano/pull/5914\n* https://github.com/Theano/Theano/pull/5775\n* https://github.com/Theano/Theano/pull/6147\n* https://github.com/Theano/Theano/pull/6159\n* https://github.com/Theano/Theano/pull/6156\n* https://github.com/Theano/Theano/pull/6154\n* https://github.com/Theano/Theano/pull/5991\n* https://github.com/Theano/Theano/pull/6149\n* https://github.com/Theano/Theano/pull/6151\n* https://github.com/Theano/Theano/pull/6116\n* https://github.com/Theano/Theano/pull/6111\n* https://github.com/Theano/Theano/pull/6139\n* https://github.com/Theano/Theano/pull/6097\n* https://github.com/Theano/Theano/pull/6070\n* https://github.com/Theano/Theano/pull/6148\n* https://github.com/Theano/Theano/pull/6140\n* https://github.com/Theano/Theano/pull/6138\n* https://github.com/Theano/Theano/pull/5881\n* https://github.com/Theano/Theano/pull/6130\n* https://github.com/Theano/Theano/pull/6044\n* https://github.com/Theano/Theano/pull/6060\n* https://github.com/Theano/Theano/pull/6109\n* https://github.com/Theano/Theano/pull/6119\n* https://github.com/Theano/Theano/pull/6123\n* https://github.com/Theano/Theano/pull/6117\n* https://github.com/Theano/Theano/pull/6120\n* https://github.com/Theano/Theano/pull/5747\n* https://github.com/Theano/Theano/pull/6087\n* https://github.com/Theano/Theano/pull/6108\n* https://github.com/Theano/Theano/pull/6112\n* https://github.com/Theano/Theano/pull/6106\n* https://github.com/Theano/Theano/pull/6107\n* https://github.com/Theano/Theano/pull/6105\n* https://github.com/Theano/Theano/pull/6102\n* https://github.com/Theano/Theano/pull/6101\n* https://github.com/Theano/Theano/pull/6077\n* https://github.com/Theano/Theano/pull/6085\n* https://github.com/Theano/Theano/pull/6091\n* https://github.com/Theano/Theano/pull/6013\n* https://github.com/Theano/Theano/pull/6088\n* https://github.com/Theano/Theano/pull/6069\n* https://github.com/Theano/Theano/pull/6084\n* https://github.com/Theano/Theano/pull/6083\n* https://github.com/Theano/Theano/pull/6081\n* https://github.com/Theano/Theano/pull/6072\n* https://github.com/Theano/Theano/pull/6045\n* https://github.com/Theano/Theano/pull/6082\n* https://github.com/Theano/Theano/pull/6049\n* https://github.com/Theano/Theano/pull/6076\n* https://github.com/Theano/Theano/pull/6062\n* https://github.com/Theano/Theano/pull/6041\n* https://github.com/Theano/Theano/pull/6057\n* https://github.com/Theano/Theano/pull/6055\n* https://github.com/Theano/Theano/pull/6056\n* https://github.com/Theano/Theano/pull/6043\n* https://github.com/Theano/Theano/pull/6032\n* https://github.com/Theano/Theano/pull/6030\n* https://github.com/Theano/Theano/pull/5942\n* https://github.com/Theano/Theano/pull/6025\n* https://github.com/Theano/Theano/pull/6038\n* https://github.com/Theano/Theano/pull/6034\n* https://github.com/Theano/Theano/pull/6012\n* https://github.com/Theano/Theano/pull/6029\n* https://github.com/Theano/Theano/pull/6015\n* https://github.com/Theano/Theano/pull/6027\n* https://github.com/Theano/Theano/pull/6026\n* https://github.com/Theano/Theano/pull/5980\n* https://github.com/Theano/Theano/pull/6021\n* https://github.com/Theano/Theano/pull/6022\n* https://github.com/Theano/Theano/pull/6011\n* https://github.com/Theano/Theano/pull/5935\n* https://github.com/Theano/Theano/pull/5955\n* https://github.com/Theano/Theano/pull/6009\n* https://github.com/Theano/Theano/pull/5016\n* https://github.com/Theano/Theano/pull/5794\n* https://github.com/Theano/Theano/pull/5996\n* https://github.com/Theano/Theano/pull/5923\n* https://github.com/Theano/Theano/pull/5993\n* https://github.com/Theano/Theano/pull/5983\n* https://github.com/Theano/Theano/pull/5964\n* https://github.com/Theano/Theano/pull/5940\n* https://github.com/Theano/Theano/pull/5915\n* https://github.com/Theano/Theano/pull/5989\n* https://github.com/Theano/Theano/pull/5988\n* https://github.com/Theano/Theano/pull/5987\n* https://github.com/Theano/Theano/pull/5908\n* https://github.com/Theano/Theano/pull/5974\n* https://github.com/Theano/Theano/pull/5965\n* https://github.com/Theano/Theano/pull/5960\n* https://github.com/Theano/Theano/pull/5957\n* https://github.com/Theano/Theano/pull/5936\n* https://github.com/Theano/Theano/pull/5950\n* https://github.com/Theano/Theano/pull/5948\n* https://github.com/Theano/Theano/pull/5946\n* https://github.com/Theano/Theano/pull/5947\n* https://github.com/Theano/Theano/pull/5927\n* https://github.com/Theano/Theano/pull/5944\n* https://github.com/Theano/Theano/pull/5918\n* https://github.com/Theano/Theano/pull/5941\n* https://github.com/Theano/Theano/pull/5931\n* https://github.com/Theano/Theano/pull/5937\n* https://github.com/Theano/Theano/pull/5852\n* https://github.com/Theano/Theano/pull/5922\n* https://github.com/Theano/Theano/pull/5921\n* https://github.com/Theano/Theano/pull/5902\n* https://github.com/Theano/Theano/pull/5903\n* https://github.com/Theano/Theano/pull/5909\n* https://github.com/Theano/Theano/pull/5758\n* https://github.com/Theano/Theano/pull/5778\n* https://github.com/Theano/Theano/pull/5900\n* https://github.com/Theano/Theano/pull/5895\n* https://github.com/Theano/Theano/pull/5883\n* https://github.com/Theano/Theano/pull/5896\n* https://github.com/Theano/Theano/pull/5888\n* https://github.com/Theano/Theano/pull/5886\n* https://github.com/Theano/Theano/pull/5885\n* https://github.com/Theano/Theano/pull/5873\n* https://github.com/Theano/Theano/pull/5877\n* https://github.com/Theano/Theano/pull/5878\n* https://github.com/Theano/Theano/pull/5872\n* https://github.com/Theano/Theano/pull/5870\n* https://github.com/Theano/Theano/pull/5854\n* https://github.com/Theano/Theano/pull/5865\n* https://github.com/Theano/Theano/pull/5853\n* https://github.com/Theano/Theano/pull/5850\n* https://github.com/Theano/Theano/pull/5538\n* https://github.com/Theano/Theano/pull/5863\n* https://github.com/Theano/Theano/pull/5799\n* https://github.com/Theano/Theano/pull/5859\n* https://github.com/Theano/Theano/pull/5755\n* https://github.com/Theano/Theano/pull/5860\n* https://github.com/Theano/Theano/pull/5716\n* https://github.com/Theano/Theano/pull/5842\n* https://github.com/Theano/Theano/pull/5821\n* https://github.com/Theano/Theano/pull/5789\n* https://github.com/Theano/Theano/pull/5847\n* https://github.com/Theano/Theano/pull/5735\n* https://github.com/Theano/Theano/pull/5710\n* https://github.com/Theano/Theano/pull/5843\n* https://github.com/Theano/Theano/pull/5832\n* https://github.com/Theano/Theano/pull/5814\n* https://github.com/Theano/Theano/pull/5835\n* https://github.com/Theano/Theano/pull/5834\n* https://github.com/Theano/Theano/pull/5829\n* https://github.com/Theano/Theano/pull/5785\n* https://github.com/Theano/Theano/pull/5824\n* https://github.com/Theano/Theano/pull/5820\n* https://github.com/Theano/Theano/pull/5808\n* https://github.com/Theano/Theano/pull/5815\n* https://github.com/Theano/Theano/pull/5819\n* https://github.com/Theano/Theano/pull/5612\n* https://github.com/Theano/Theano/pull/5802\n* https://github.com/Theano/Theano/pull/5796\n* https://github.com/Theano/Theano/pull/5806\n* https://github.com/Theano/Theano/pull/5782\n* https://github.com/Theano/Theano/pull/5787\n* https://github.com/Theano/Theano/pull/5774\n* https://github.com/Theano/Theano/pull/5751\n* https://github.com/Theano/Theano/pull/5779\n* https://github.com/Theano/Theano/pull/5763\n* https://github.com/Theano/Theano/pull/5746\n* https://github.com/Theano/Theano/pull/5579\n* https://github.com/Theano/Theano/pull/5772\n* https://github.com/Theano/Theano/pull/5756\n* https://github.com/Theano/Theano/pull/5769\n* https://github.com/Theano/Theano/pull/5433\n* https://github.com/Theano/Theano/pull/5760\n* https://github.com/Theano/Theano/pull/5470\n* https://github.com/Theano/Theano/pull/5759\n* https://github.com/Theano/Theano/pull/5739\n* https://github.com/Theano/Theano/pull/5752\n* https://github.com/Theano/Theano/pull/5548\n* https://github.com/Theano/Theano/pull/5749\n* https://github.com/Theano/Theano/pull/5665\n* https://github.com/Theano/Theano/pull/5562\n* https://github.com/Theano/Theano/pull/5686\n* https://github.com/Theano/Theano/pull/5718\n* https://github.com/Theano/Theano/pull/5698\n* https://github.com/Theano/Theano/pull/5720\n* https://github.com/Theano/Theano/pull/5717\n* https://github.com/Theano/Theano/pull/5715\n* https://github.com/Theano/Theano/pull/5502\n* https://github.com/Theano/Theano/pull/5533\n* https://github.com/Theano/Theano/pull/5660\n* https://github.com/Theano/Theano/pull/5682\n* https://github.com/Theano/Theano/pull/5704\n* https://github.com/Theano/Theano/pull/5687\n* https://github.com/Theano/Theano/pull/5455\n* https://github.com/Theano/Theano/pull/5667\n* https://github.com/Theano/Theano/pull/5554\n* https://github.com/Theano/Theano/pull/5486\n* https://github.com/Theano/Theano/pull/5567\n* https://github.com/Theano/Theano/pull/5615\n* https://github.com/Theano/Theano/pull/5672\n* https://github.com/Theano/Theano/pull/5524\n\nTheano Development version\n==========================\n\nNEWS.txt:\n\n"
        },
        {
          "name": "README.rst",
          "type": "blob",
          "size": 0.4580078125,
          "content": "============================================================================================================\nMILA has stopped developing Theano: https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ\n\nThe PyMC developers have forked Theano to a new project called PyTensor that is being actively developed: https://github.com/pymc-devs/pytensor\n============================================================================================================\n"
        },
        {
          "name": "Theano.pyproj",
          "type": "blob",
          "size": 11.484375,
          "content": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<Project DefaultTargets=\"Build\" xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\">\n  <PropertyGroup>\n    <Configuration Condition=\" '$(Configuration)' == '' \">Debug</Configuration>\n    <SchemaVersion>2.0</SchemaVersion>\n    <ProjectGuid>{b67d762d-0020-4e02-9ddf-7db4f89b1dd3}</ProjectGuid>\n    <ProjectHome>.</ProjectHome>\n    <StartupFile>\n    </StartupFile>\n    <SearchPath>\n    </SearchPath>\n    <WorkingDirectory>.</WorkingDirectory>\n    <OutputPath>.</OutputPath>\n    <Name>Theano</Name>\n    <RootNamespace>Theano</RootNamespace>\n    <IsWindowsApplication>False</IsWindowsApplication>\n    <InterpreterId>2af0f10d-7135-4994-9156-5d01c9c11b7e</InterpreterId>\n    <InterpreterVersion>2.7</InterpreterVersion>\n  </PropertyGroup>\n  <PropertyGroup Condition=\" '$(Configuration)' == 'Debug' \">\n    <DebugSymbols>true</DebugSymbols>\n    <EnableUnmanagedDebugging>false</EnableUnmanagedDebugging>\n  </PropertyGroup>\n  <PropertyGroup Condition=\" '$(Configuration)' == 'Release' \">\n    <DebugSymbols>true</DebugSymbols>\n    <EnableUnmanagedDebugging>false</EnableUnmanagedDebugging>\n  </PropertyGroup>\n  <ItemGroup>\n    <Compile Include=\"theano\\compile\\builders.py\" />\n    <Compile Include=\"theano\\compile\\debugmode.py\" />\n    <Compile Include=\"theano\\compile\\function.py\" />\n    <Compile Include=\"theano\\compile\\function_module.py\" />\n    <Compile Include=\"theano\\compile\\io.py\" />\n    <Compile Include=\"theano\\compile\\mode.py\" />\n    <Compile Include=\"theano\\compile\\module.py\" />\n    <Compile Include=\"theano\\compile\\pfunc.py\" />\n    <Compile Include=\"theano\\compile\\profiling.py\" />\n    <Compile Include=\"theano\\compile\\sandbox\\__init__.py\" />\n    <Compile Include=\"theano\\compile\\sharedvalue.py\" />\n    <Compile Include=\"theano\\compile\\tests\\test_builders.py\" />\n    <Compile Include=\"theano\\compile\\tests\\test_debugmode.py\" />\n    <Compile Include=\"theano\\compile\\tests\\test_function_module.py\" />\n    <Compile Include=\"theano\\compile\\tests\\test_inplace_opt_for_value.py\" />\n    <Compile Include=\"theano\\compile\\tests\\test_misc.py\" />\n    <Compile Include=\"theano\\compile\\tests\\test_modes.py\" />\n    <Compile Include=\"theano\\compile\\tests\\test_module.py\" />\n    <Compile Include=\"theano\\compile\\tests\\test_pfunc.py\" />\n    <Compile Include=\"theano\\compile\\tests\\test_shared.py\" />\n    <Compile Include=\"theano\\compile\\tests\\__init__.py\" />\n    <Compile Include=\"theano\\compile\\__init__.py\" />\n    <Compile Include=\"theano\\configdefaults.py\" />\n    <Compile Include=\"theano\\configparser.py\" />\n    <Compile Include=\"theano\\gof\\callcache.py\" />\n    <Compile Include=\"theano\\gof\\cc.py\" />\n    <Compile Include=\"theano\\gof\\cmodule.py\" />\n    <Compile Include=\"theano\\gof\\compiledir.py\" />\n    <Compile Include=\"theano\\gof\\compilelock.py\" />\n    <Compile Include=\"theano\\gof\\cutils.py\" />\n    <Compile Include=\"theano\\gof\\destroyhandler.py\" />\n    <Compile Include=\"theano\\gof\\env.py\" />\n    <Compile Include=\"theano\\gof\\graph.py\" />\n    <Compile Include=\"theano\\gof\\lazylinker_c.py\" />\n    <Compile Include=\"theano\\gof\\link.py\" />\n    <Compile Include=\"theano\\gof\\op.py\" />\n    <Compile Include=\"theano\\gof\\opt.py\" />\n    <Compile Include=\"theano\\gof\\optdb.py\" />\n    <Compile Include=\"theano\\gof\\python25.py\" />\n    <Compile Include=\"theano\\gof\\sandbox\\equilibrium.py\" />\n    <Compile Include=\"theano\\gof\\tests\\test_cc.py\" />\n    <Compile Include=\"theano\\gof\\tests\\test_compute_test_value.py\" />\n    <Compile Include=\"theano\\gof\\tests\\test_destroyhandler.py\" />\n    <Compile Include=\"theano\\gof\\tests\\test_graph.py\" />\n    <Compile Include=\"theano\\gof\\tests\\test_lazy.py\" />\n    <Compile Include=\"theano\\gof\\tests\\test_link.py\" />\n    <Compile Include=\"theano\\gof\\tests\\test_op.py\" />\n    <Compile Include=\"theano\\gof\\tests\\test_opt.py\" />\n    <Compile Include=\"theano\\gof\\tests\\test_optdb.py\" />\n    <Compile Include=\"theano\\gof\\tests\\test_toolbox.py\" />\n    <Compile Include=\"theano\\gof\\tests\\test_types.py\" />\n    <Compile Include=\"theano\\gof\\tests\\test_vm.py\" />\n    <Compile Include=\"theano\\gof\\tests\\__init__.py\" />\n    <Compile Include=\"theano\\gof\\toolbox.py\" />\n    <Compile Include=\"theano\\gof\\type.py\" />\n    <Compile Include=\"theano\\gof\\unify.py\" />\n    <Compile Include=\"theano\\gof\\utils.py\" />\n    <Compile Include=\"theano\\gof\\vm.py\" />\n    <Compile Include=\"theano\\gof\\__init__.py\" />\n    <Compile Include=\"theano\\gradient.py\" />\n    <Compile Include=\"theano\\ifelse.py\" />\n    <Compile Include=\"theano\\misc\\buildbot_filter.py\" />\n    <Compile Include=\"theano\\misc\\check_blas.py\" />\n    <Compile Include=\"theano\\misc\\check_duplicate_key.py\" />\n    <Compile Include=\"theano\\misc\\cudamat_utils.py\" />\n    <Compile Include=\"theano\\misc\\gnumpy_utils.py\" />\n    <Compile Include=\"theano\\misc\\hooks\\argparse.py\" />\n    <Compile Include=\"theano\\misc\\hooks\\check_whitespace.py\" />\n    <Compile Include=\"theano\\misc\\hooks\\reindent.py\" />\n    <Compile Include=\"theano\\misc\\latence_gpu_transfert.py\" />\n    <Compile Include=\"theano\\misc\\may_share_memory.py\" />\n    <Compile Include=\"theano\\misc\\pycuda_example.py\" />\n    <Compile Include=\"theano\\misc\\pycuda_init.py\" />\n    <Compile Include=\"theano\\misc\\pycuda_utils.py\" />\n    <Compile Include=\"theano\\misc\\safe_asarray.py\" />\n    <Compile Include=\"theano\\misc\\strutil.py\" />\n    <Compile Include=\"theano\\misc\\tests\\test_cudamat_utils.py\" />\n    <Compile Include=\"theano\\misc\\tests\\test_gnumpy_utils.py\" />\n    <Compile Include=\"theano\\misc\\tests\\test_may_share_memory.py\" />\n    <Compile Include=\"theano\\misc\\tests\\test_pycuda_example.py\" />\n    <Compile Include=\"theano\\misc\\tests\\test_pycuda_theano_simple.py\" />\n    <Compile Include=\"theano\\misc\\tests\\test_pycuda_utils.py\" />\n    <Compile Include=\"theano\\misc\\__init__.py\" />\n    <Compile Include=\"theano\\printing.py\" />\n    <Compile Include=\"theano\\raise_op.py\" />\n    <Compile Include=\"theano\\sandbox\\conv.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\basic_ops.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\blas.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\elemwise.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\GpuConv3D.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\GpuConvGrad3D.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\GpuConvTransp3D.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\kernel_codegen.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\nnet.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\nvcc_compiler.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\opt.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\rng_curand.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\type.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\var.py\" />\n    <Compile Include=\"theano\\sandbox\\cuda\\__init__.py\" />\n    <Compile Include=\"theano\\sandbox\\debug.py\" />\n    <Compile Include=\"theano\\sandbox\\downsample.py\" />\n    <Compile Include=\"theano\\sandbox\\fourier.py\" />\n    <Compile Include=\"theano\\sandbox\\linalg\\ops.py\" />\n    <Compile Include=\"theano\\sandbox\\linalg\\__init__.py\" />\n    <Compile Include=\"theano\\sandbox\\minimal.py\" />\n    <Compile Include=\"theano\\sandbox\\multinomial.py\" />\n    <Compile Include=\"theano\\sandbox\\neighbours.py\" />\n    <Compile Include=\"theano\\sandbox\\rng_mrg.py\" />\n    <Compile Include=\"theano\\sandbox\\softsign.py\" />\n    <Compile Include=\"theano\\sandbox\\solve.py\" />\n    <Compile Include=\"theano\\sandbox\\symbolic_module.py\" />\n    <Compile Include=\"theano\\sandbox\\test_multinomial.py\" />\n    <Compile Include=\"theano\\sandbox\\test_neighbours.py\" />\n    <Compile Include=\"theano\\sandbox\\test_rng_mrg.py\" />\n    <Compile Include=\"theano\\sandbox\\test_theano_object.py\" />\n    <Compile Include=\"theano\\sandbox\\theano_object.py\" />\n    <Compile Include=\"theano\\sandbox\\__init__.py\" />\n    <Compile Include=\"theano\\scalar\\basic.py\" />\n    <Compile Include=\"theano\\scalar\\basic_scipy.py\" />\n    <Compile Include=\"theano\\scalar\\sharedvar.py\" />\n    <Compile Include=\"theano\\scalar\\__init__.py\" />\n    <Compile Include=\"theano\\scan_module\\scan.py\" />\n    <Compile Include=\"theano\\scan_module\\scan_op.py\" />\n    <Compile Include=\"theano\\scan_module\\scan_opt.py\" />\n    <Compile Include=\"theano\\scan_module\\scan_perform_ext.py\" />\n    <Compile Include=\"theano\\scan_module\\scan_utils.py\" />\n    <Compile Include=\"theano\\scan_module\\scan_views.py\" />\n    <Compile Include=\"theano\\scan_module\\__init__.py\" />\n    <Compile Include=\"theano\\sparse\\basic.py\" />\n    <Compile Include=\"theano\\sparse\\sandbox\\sp.py\" />\n    <Compile Include=\"theano\\sparse\\sandbox\\test_sp.py\" />\n    <Compile Include=\"theano\\sparse\\sandbox\\truedot.py\" />\n    <Compile Include=\"theano\\sparse\\sandbox\\__init__.py\" />\n    <Compile Include=\"theano\\sparse\\sharedvar.py\" />\n    <Compile Include=\"theano\\sparse\\__init__.py\" />\n    <Compile Include=\"theano\\tensor\\basic.py\" />\n    <Compile Include=\"theano\\tensor\\blas.py\" />\n    <Compile Include=\"theano\\tensor\\blas_headers.py\" />\n    <Compile Include=\"theano\\tensor\\blas_scipy.py\" />\n    <Compile Include=\"theano\\tensor\\deprecated\\rmodule.py\" />\n    <Compile Include=\"theano\\tensor\\deprecated\\test_rmodule.py\" />\n    <Compile Include=\"theano\\tensor\\deprecated\\__init__.py\" />\n    <Compile Include=\"theano\\tensor\\elemwise.py\" />\n    <Compile Include=\"theano\\tensor\\elemwise_cgen.py\" />\n    <Compile Include=\"theano\\tensor\\inplace.py\" />\n    <Compile Include=\"theano\\tensor\\nnet\\conv.py\" />\n    <Compile Include=\"theano\\tensor\\nnet\\Conv3D.py\" />\n    <Compile Include=\"theano\\tensor\\nnet\\ConvGrad3D.py\" />\n    <Compile Include=\"theano\\tensor\\nnet\\ConvTransp3D.py\" />\n    <Compile Include=\"theano\\tensor\\nnet\\nnet.py\" />\n    <Compile Include=\"theano\\tensor\\nnet\\sigm.py\" />\n    <Compile Include=\"theano\\tensor\\nnet\\__init__.py\" />\n    <Compile Include=\"theano\\tensor\\opt.py\" />\n    <Compile Include=\"theano\\tensor\\opt_uncanonicalize.py\" />\n    <Compile Include=\"theano\\tensor\\randomstreams.py\" />\n    <Compile Include=\"theano\\tensor\\raw_random.py\" />\n    <Compile Include=\"theano\\tensor\\sharedvar.py\" />\n    <Compile Include=\"theano\\tensor\\shared_randomstreams.py\" />\n    <Compile Include=\"theano\\tensor\\signal\\conv.py\" />\n    <Compile Include=\"theano\\tensor\\signal\\pool.py\" />\n    <Compile Include=\"theano\\tensor\\signal\\__init__.py\" />\n    <Compile Include=\"theano\\tensor\\tensor_grad.py\" />\n    <Compile Include=\"theano\\tensor\\xlogx.py\" />\n    <Compile Include=\"theano\\tensor\\__init__.py\" />\n    <Compile Include=\"theano\\updates.py\" />\n    <Compile Include=\"theano\\__init__.py\" />\n  </ItemGroup>\n  <ItemGroup>\n    <Folder Include=\"theano\\\" />\n    <Folder Include=\"theano\\compile\\\" />\n    <Folder Include=\"theano\\compile\\sandbox\\\" />\n    <Folder Include=\"theano\\compile\\tests\\\" />\n    <Folder Include=\"theano\\gof\\\" />\n    <Folder Include=\"theano\\gof\\sandbox\\\" />\n    <Folder Include=\"theano\\gof\\tests\\\" />\n    <Folder Include=\"theano\\misc\\\" />\n    <Folder Include=\"theano\\misc\\hooks\\\" />\n    <Folder Include=\"theano\\misc\\tests\\\" />\n    <Folder Include=\"theano\\sandbox\\\" />\n    <Folder Include=\"theano\\sandbox\\cuda\\\" />\n    <Folder Include=\"theano\\sandbox\\linalg\\\" />\n    <Folder Include=\"theano\\scalar\\\" />\n    <Folder Include=\"theano\\scan_module\\\" />\n    <Folder Include=\"theano\\sparse\\\" />\n    <Folder Include=\"theano\\sparse\\sandbox\\\" />\n    <Folder Include=\"theano\\tensor\\\" />\n    <Folder Include=\"theano\\tensor\\deprecated\\\" />\n    <Folder Include=\"theano\\tensor\\nnet\\\" />\n    <Folder Include=\"theano\\tensor\\signal\\\" />\n  </ItemGroup>\n  <ItemGroup>\n    <Content Include=\"theano\\sandbox\\cuda\\conv.cu\" />\n    <Content Include=\"theano\\sandbox\\cuda\\conv_full_kernel.cu\" />\n    <Content Include=\"theano\\sandbox\\cuda\\conv_kernel.cu\" />\n    <Content Include=\"theano\\sandbox\\cuda\\cuda_ndarray.cu\" />\n    <Content Include=\"theano\\sandbox\\cuda\\cuda_ndarray.cuh\" />\n  </ItemGroup>\n  <Import Project=\"$(MSBuildToolsPath)\\Microsoft.Common.targets\" />\n</Project>\n"
        },
        {
          "name": "Theano.sln",
          "type": "blob",
          "size": 0.7021484375,
          "content": "﻿\nMicrosoft Visual Studio Solution File, Format Version 11.00\n# Visual Studio 2010\nProject(\"{888888A0-9F3D-457C-B088-3A5042F75D52}\") = \"Theano\", \"Theano.pyproj\", \"{B67D762D-0020-4E02-9DDF-7DB4F89B1DD3}\"\nEndProject\nGlobal\n\tGlobalSection(SolutionConfigurationPlatforms) = preSolution\n\t\tDebug|Any CPU = Debug|Any CPU\n\t\tRelease|Any CPU = Release|Any CPU\n\tEndGlobalSection\n\tGlobalSection(ProjectConfigurationPlatforms) = postSolution\n\t\t{B67D762D-0020-4E02-9DDF-7DB4F89B1DD3}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{B67D762D-0020-4E02-9DDF-7DB4F89B1DD3}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\tEndGlobalSection\n\tGlobalSection(SolutionProperties) = preSolution\n\t\tHideSolutionNode = FALSE\n\tEndGlobalSection\nEndGlobal\n"
        },
        {
          "name": "bin",
          "type": "tree",
          "content": null
        },
        {
          "name": "conda",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirement-rtd.txt",
          "type": "blob",
          "size": 0.09375,
          "content": "sphinx>=1.3.0\npygments\nnose>=1.3.0\nnumpy\ngnumpy\npydot\npydot2\nCython\nparameterized\nscipy==0.13\n\n\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2119140625,
          "content": "# pip install --upgrade pip setuptools virtualenv virtualenvwrapper virtualenv-clone wheel\n# pip install -r requirements.txt && pyenv rehash\n\ncython\n# mpi4py\nnose\nnumpy\nparameterized\nreindent\nrequests\nscipy\nsix\nsympy\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.2275390625,
          "content": "[nosetest]\nmatch=^test\nnocapture=1\n\n[flake8]\nignore=E501,E123,E133,FI12,FI14,FI15,FI50,FI51,FI53\n\n[versioneer]\nVCS = git\nstyle = pep440\nversionfile_source = theano/_version.py\nversionfile_build = theano/_version.py\ntag_prefix = rel-\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 4.3935546875,
          "content": "#!/usr/bin/env python\n#\n#  TODO:\n#   * Figure out how to compile and install documentation automatically\n#   * Add download_url\n\nfrom __future__ import absolute_import, print_function, division\nimport os\nimport codecs\nfrom fnmatch import fnmatchcase\nfrom distutils.util import convert_path\ntry:\n    from setuptools import setup\nexcept ImportError:\n    from distutils.core import setup\n\nimport versioneer\n\nCLASSIFIERS = \"\"\"\\\nDevelopment Status :: 4 - Beta\nIntended Audience :: Education\nIntended Audience :: Science/Research\nIntended Audience :: Developers\nLicense :: OSI Approved :: BSD License\nProgramming Language :: Python\nTopic :: Software Development :: Code Generators\nTopic :: Software Development :: Compilers\nTopic :: Scientific/Engineering :: Mathematics\nOperating System :: Microsoft :: Windows\nOperating System :: POSIX\nOperating System :: Unix\nOperating System :: MacOS\nProgramming Language :: Python :: 2\nProgramming Language :: Python :: 2.7\nProgramming Language :: Python :: 3\nProgramming Language :: Python :: 3.4\nProgramming Language :: Python :: 3.5\nProgramming Language :: Python :: 3.6\nProgramming Language :: Python :: 3.7\nProgramming Language :: Python :: 3.8\n\"\"\"\nNAME                = 'Theano'\nMAINTAINER          = \"PyMC devs\"\nMAINTAINER_EMAIL    = \"pymc-devs@gmail.com\"\nDESCRIPTION         = ('Optimizing compiler for evaluating mathematical ' +\n                       'expressions on CPUs and GPUs.')\nLONG_DESCRIPTION    = (codecs.open(\"DESCRIPTION.txt\", encoding='utf-8').read() +\n                       \"\\n\\n\" + codecs.open(\"NEWS.txt\", encoding='utf-8').read())\nURL                 = \"http://deeplearning.net/software/theano/\"\nDOWNLOAD_URL        = \"\"\nLICENSE             = 'BSD'\nCLASSIFIERS         = [_f for _f in CLASSIFIERS.split('\\n') if _f]\nAUTHOR              = \"LISA laboratory, University of Montreal\"\nAUTHOR_EMAIL        = \"theano-dev@googlegroups.com\"\nPLATFORMS           = [\"Windows\", \"Linux\", \"Solaris\", \"Mac OS-X\", \"Unix\"]\n\n\ndef find_packages(where='.', exclude=()):\n    out = []\n    stack = [(convert_path(where), '')]\n    while stack:\n        where, prefix = stack.pop(0)\n        for name in os.listdir(where):\n            fn = os.path.join(where, name)\n            if ('.' not in name and os.path.isdir(fn) and\n                    os.path.isfile(os.path.join(fn, '__init__.py'))):\n                out.append(prefix + name)\n                stack.append((fn, prefix + name + '.'))\n    for pat in list(exclude) + ['ez_setup', 'distribute_setup']:\n        out = [item for item in out if not fnmatchcase(item, pat)]\n    return out\n\n\nversion_data = versioneer.get_versions()\n\nif version_data['error'] is not None:\n    # Get the fallback version\n    # We can't import theano.version as it isn't yet installed, so parse it.\n    fname = os.path.join(os.path.split(__file__)[0], \"theano\", \"version.py\")\n    with open(fname, \"r\") as f:\n        lines = f.readlines()\n    lines = [l for l in lines if l.startswith(\"FALLBACK_VERSION\")]\n    assert len(lines) == 1\n\n    FALLBACK_VERSION = lines[0].split(\"=\")[1].strip().strip('\"\"')\n\n    version_data['version'] = FALLBACK_VERSION\n\n\ndef do_setup():\n    setup(name=NAME,\n          version=version_data['version'],\n          description=DESCRIPTION,\n          long_description=LONG_DESCRIPTION,\n          classifiers=CLASSIFIERS,\n          author=AUTHOR,\n          author_email=AUTHOR_EMAIL,\n          url=URL,\n          license=LICENSE,\n          platforms=PLATFORMS,\n          packages=find_packages(),\n          cmdclass=versioneer.get_cmdclass(),\n          install_requires=['numpy>=1.9.1', 'scipy>=0.14', 'six>=1.9.0'],\n          # pygments is a dependency for Sphinx code highlight\n          extras_require={\n              'test': ['nose>=1.3.0', 'parameterized', 'flake8'],\n              'doc': ['Sphinx>=0.5.1', 'pygments']\n          },\n          package_data={\n              '': ['*.txt', '*.rst', '*.cu', '*.cuh', '*.c', '*.sh', '*.pkl',\n                   '*.h', '*.cpp', 'ChangeLog', 'c_code/*'],\n              'theano.misc': ['*.sh'],\n              'theano.d3viz': ['html/*', 'css/*', 'js/*']\n          },\n          entry_points={\n              'console_scripts': ['theano-cache = bin.theano_cache:main',\n                                  'theano-nose = bin.theano_nose:main']\n          },\n          keywords=' '.join([\n              'theano', 'math', 'numerical', 'symbolic', 'blas',\n              'numpy', 'gpu', 'autodiff', 'differentiation'\n          ]),\n    )\n\n\nif __name__ == \"__main__\":\n    do_setup()\n"
        },
        {
          "name": "theano",
          "type": "tree",
          "content": null
        },
        {
          "name": "versioneer.py",
          "type": "blob",
          "size": 67.0029296875,
          "content": "\n# Version: 0.18\n\n\"\"\"The Versioneer - like a rocketeer, but for versions.\n\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/warner/python-versioneer\n* Brian Warner\n* License: Public Domain\n* Compatible With: python2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, and pypy\n* [![Latest Version]\n(https://pypip.in/version/versioneer/badge.svg?style=flat)\n](https://pypi.python.org/pypi/versioneer/)\n* [![Build Status]\n(https://travis-ci.org/warner/python-versioneer.png?branch=master)\n](https://travis-ci.org/warner/python-versioneer)\n\nThis is a tool for managing a recorded version number in distutils-based\npython projects. The goal is to remove the tedious and error-prone \"update\nthe embedded version string\" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\n* `pip install versioneer` to somewhere to your $PATH\n* add a `[versioneer]` section to your setup.cfg (see below)\n* run `versioneer install` in your source tree, commit the results\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github's\n  \"tarball from tag\" feature\n* a release tarball, produced by \"setup.py sdist\", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. \"git describe\" (for checkouts), which knows\n  about recent \"tags\" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. \"myproject-1.2\" instead of just \"1.2\"), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n\"0.7-1-g574ab98-dirty\" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of \"574ab98\", and is \"dirty\" (it has\nuncommitted changes.\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a 'setup.py sdist' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nSee [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the \"outside\" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `['version']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project's version\n  string. The default \"pep440\" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the \"Styles\" section\n  below for alternative styles.\n\n* `['full-revisionid']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. \"1076c978a8d3cfc70f408fe5974aa6c092c949ac\".\n\n* `['date']`: Date and time of the latest `HEAD` commit. For Git, it is the\n  commit date in ISO 8601 format. This will be None if the date is not\n  available.\n\n* `['dirty']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `['error']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of \"unknown\".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an \"about\" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()['version']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, \"pep440\", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional \"local\nversion\" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example \"0.11+2.g1076c97.dirty\" indicates that the\ntree is like the \"1076c97\" commit but has uncommitted changes (\".dirty\"), and\nthat this commit is two revisions (\"+2\") beyond the \"0.11\" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. \"0.11\".\n\nOther styles are available. See [details.md](details.md) in the Versioneer\nsource tree for descriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of \"0+unknown\". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Known Limitations\n\nSome situations are known to cause problems for Versioneer. This details the\nmost significant ones. More can be found on Github\n[issues page](https://github.com/warner/python-versioneer/issues).\n\n### Subprojects\n\nVersioneer has limited support for source trees in which `setup.py` is not in\nthe root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\ntwo common reasons why `setup.py` might not be in the root:\n\n* Source trees which contain multiple subprojects, such as\n  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n  \"master\" and \"slave\" subprojects, each with their own `setup.py`,\n  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n  distributions (and upload multiple independently-installable tarballs).\n* Source trees whose main purpose is to contain a C library, but which also\n  provide bindings to Python (and perhaps other langauges) in subdirectories.\n\nVersioneer will look for `.git` in parent directories, and most operations\nshould get the right version string. However `pip` and `setuptools` have bugs\nand implementation details which frequently cause `pip install .` from a\nsubproject directory to fail to find a correct version string (so it usually\ndefaults to `0+unknown`).\n\n`pip install --editable .` should work correctly. `setup.py install` might\nwork too.\n\nPip-8.1.1 is known to have this problem, but hopefully it will get fixed in\nsome later version.\n\n[Bug #38](https://github.com/warner/python-versioneer/issues/38) is tracking\nthis issue. The discussion in\n[PR #61](https://github.com/warner/python-versioneer/pull/61) describes the\nissue from the Versioneer side in more detail.\n[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\npip to let Versioneer work correctly.\n\nVersioneer-0.16 and earlier only looked for a `.git` directory next to the\n`setup.cfg`, so subprojects were completely unsupported with those releases.\n\n### Editable installs with setuptools <= 18.5\n\n`setup.py develop` and `pip install --editable .` allow you to install a\nproject into a virtualenv once, then continue editing the source code (and\ntest) without re-installing after every change.\n\n\"Entry-point scripts\" (`setup(entry_points={\"console_scripts\": ..})`) are a\nconvenient way to specify executable scripts that should be installed along\nwith the python package.\n\nThese both work as expected when using modern setuptools. When using\nsetuptools-18.5 or earlier, however, certain operations will cause\n`pkg_resources.DistributionNotFound` errors when running the entrypoint\nscript, which must be resolved by re-installing the package. This happens\nwhen the install happens with one version, then the egg_info data is\nregenerated while a different version is checked out. Many setup.py commands\ncause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\na different virtualenv), so this can be surprising.\n\n[Bug #83](https://github.com/warner/python-versioneer/issues/83) describes\nthis one, but upgrading to a newer version of setuptools should probably\nresolve it.\n\n### Unicode version strings\n\nWhile Versioneer works (and is continually tested) with both Python 2 and\nPython 3, it is not entirely consistent with bytes-vs-unicode distinctions.\nNewer releases probably generate unicode version strings on py2. It's not\nclear that this is wrong, but it may be surprising for applications when then\nwrite these strings to a network connection or include them in bytes-oriented\nAPIs like cryptographic checksums.\n\n[Bug #71](https://github.com/warner/python-versioneer/issues/71) investigates\nthis question.\n\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg`, if necessary, to include any new configuration settings\n  indicated by the release notes. See [UPGRADING](./UPGRADING.md) for details.\n* re-run `versioneer install` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n\n## License\n\nTo make Versioneer easier to embed, all its code is dedicated to the public\ndomain. The `_version.py` that it creates is also in the public domain.\nSpecifically, both are released under the Creative Commons \"Public Domain\nDedication\" license (CC0-1.0), as described in\nhttps://creativecommons.org/publicdomain/zero/1.0/ .\n\n\"\"\"\n\nfrom __future__ import print_function\ntry:\n    import configparser\nexcept ImportError:\n    import ConfigParser as configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n\ndef get_root():\n    \"\"\"Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    \"\"\"\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, \"setup.py\")\n    versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow 'python path/to/setup.py COMMAND'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, \"setup.py\")\n        versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (\"Versioneer was unable to run the project root directory. \"\n               \"Versioneer requires setup.py to be executed from \"\n               \"its immediate directory (like 'python setup.py COMMAND'), \"\n               \"or in a way that lets it use sys.argv[0] to find the root \"\n               \"(like 'python path/to/setup.py COMMAND').\")\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # \"versioneer\" may be imported multiple times, and python's shared\n        # module-import table will cache the first one. So we can't use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        me = os.path.realpath(os.path.abspath(__file__))\n        me_dir = os.path.normcase(os.path.splitext(me)[0])\n        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n        if me_dir != vsr_dir:\n            print(\"Warning: build in %s is using versioneer.py from %s\"\n                  % (os.path.dirname(me), versioneer_py))\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root):\n    \"\"\"Read the project setup.cfg file to determine Versioneer config.\"\"\"\n    # This might raise EnvironmentError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks \"VCS=\"). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    setup_cfg = os.path.join(root, \"setup.cfg\")\n    parser = configparser.SafeConfigParser()\n    with open(setup_cfg, \"r\") as f:\n        parser.readfp(f)\n    VCS = parser.get(\"versioneer\", \"VCS\")  # mandatory\n\n    def get(parser, name):\n        if parser.has_option(\"versioneer\", name):\n            return parser.get(\"versioneer\", name)\n        return None\n    cfg = VersioneerConfig()\n    cfg.VCS = VCS\n    cfg.style = get(parser, \"style\") or \"\"\n    cfg.versionfile_source = get(parser, \"versionfile_source\")\n    cfg.versionfile_build = get(parser, \"versionfile_build\")\n    cfg.tag_prefix = get(parser, \"tag_prefix\")\n    if cfg.tag_prefix in (\"''\", '\"\"'):\n        cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = get(parser, \"parentdir_prefix\")\n    cfg.verbose = get(parser, \"verbose\")\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Decorator to mark a method as the handler for a particular VCS.\"\"\"\n    def decorate(f):\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %s\" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % dispcmd)\n            print(\"stdout was %s\" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\nLONG_VERSION_PY['git'] = '''\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n\"\"\"Git implementation of _version.py.\"\"\"\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = \"%(DOLLAR)sFormat:%%d%(DOLLAR)s\"\n    git_full = \"%(DOLLAR)sFormat:%%H%(DOLLAR)s\"\n    git_date = \"%(DOLLAR)sFormat:%%ci%(DOLLAR)s\"\n    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n\ndef get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    # these strings are filled in when 'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"%(STYLE)s\"\n    cfg.tag_prefix = \"%(TAG_PREFIX)s\"\n    cfg.parentdir_prefix = \"%(PARENTDIR_PREFIX)s\"\n    cfg.versionfile_source = \"%(VERSIONFILE_SOURCE)s\"\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Decorator to mark a method as the handler for a particular VCS.\"\"\"\n    def decorate(f):\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %%s\" %% dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %%s\" %% (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(\"unable to run %%s (error)\" %% dispcmd)\n            print(\"stdout was %%s\" %% stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\"version\": dirname[len(parentdir_prefix):],\n                    \"full-revisionid\": None,\n                    \"dirty\": False, \"error\": None, \"date\": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\"Tried directories %%s but none started with prefix %%s\" %%\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs):\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, \"r\")\n        for line in f.readlines():\n            if line.strip().startswith(\"git_refnames =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"refnames\"] = mo.group(1)\n            if line.strip().startswith(\"git_full =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"full\"] = mo.group(1)\n            if line.strip().startswith(\"git_date =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"date\"] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    \"\"\"Get version information from git keywords.\"\"\"\n    if not keywords:\n        raise NotThisMethod(\"no keywords at all, weird\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # git-2.2.0 added \"%%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = set([r for r in refs if re.search(r'\\d', r)])\n        if verbose:\n            print(\"discarding '%%s', no digits\" %% \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %%s\" %% \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(\"picking %%s\" %% r)\n            return {\"version\": r,\n                    \"full-revisionid\": keywords[\"full\"].strip(),\n                    \"dirty\": False, \"error\": None,\n                    \"date\": date}\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\"version\": \"0+unknown\",\n            \"full-revisionid\": keywords[\"full\"].strip(),\n            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    out, rc = run_command(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %%s not under git control\" %% root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n                                          \"--always\", \"--long\",\n                                          \"--match\", \"%%s*\" %% tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = (\"unable to parse git-describe output: '%%s'\"\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%%s' doesn't start with prefix '%%s'\"\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[\"error\"] = (\"tag '%%s' doesn't start with prefix '%%s'\"\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n                                    cwd=root)\n        pieces[\"distance\"] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [\"show\", \"-s\", \"--format=%%ci\", \"HEAD\"],\n                       cwd=root)[0].strip()\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces):\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n                                          pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    \"\"\"TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \".post.dev%%d\" %% pieces[\"distance\"]\n    else:\n        # exception #1\n        rendered = \"0.post.dev%%d\" %% pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%%s\" %% pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%%s\" %% pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%%d\" %% pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%%d\" %% pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces):\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%%s'\" %% style)\n\n    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n            \"dirty\": pieces[\"dirty\"], \"error\": None,\n            \"date\": pieces.get(\"date\")}\n\n\ndef get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n                \"dirty\": None,\n                \"error\": \"unable to find root of source tree\",\n                \"date\": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to compute version\", \"date\": None}\n'''\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs):\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, \"r\")\n        for line in f.readlines():\n            if line.strip().startswith(\"git_refnames =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"refnames\"] = mo.group(1)\n            if line.strip().startswith(\"git_full =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"full\"] = mo.group(1)\n            if line.strip().startswith(\"git_date =\"):\n                mo = re.search(r'=\\s*\"(.*)\"', line)\n                if mo:\n                    keywords[\"date\"] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    \"\"\"Get version information from git keywords.\"\"\"\n    if not keywords:\n        raise NotThisMethod(\"no keywords at all, weird\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = set([r for r in refs if re.search(r'\\d', r)])\n        if verbose:\n            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(\"picking %s\" % r)\n            return {\"version\": r,\n                    \"full-revisionid\": keywords[\"full\"].strip(),\n                    \"dirty\": False, \"error\": None,\n                    \"date\": date}\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\"version\": \"0+unknown\",\n            \"full-revisionid\": keywords[\"full\"].strip(),\n            \"dirty\": False, \"error\": \"no suitable tags\", \"date\": None}\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    out, rc = run_command(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %s not under git control\" % root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n                                          \"--always\", \"--long\",\n                                          \"--match\", \"%s*\" % tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n                                    cwd=root)\n        pieces[\"distance\"] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"],\n                       cwd=root)[0].strip()\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef do_vcs_install(manifest_in, versionfile_source, ipy):\n    \"\"\"Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n    files = [manifest_in, versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        me = __file__\n        if me.endswith(\".pyc\") or me.endswith(\".pyo\"):\n            me = os.path.splitext(me)[0] + \".py\"\n        versioneer_file = os.path.relpath(me)\n    except NameError:\n        versioneer_file = \"versioneer.py\"\n    files.append(versioneer_file)\n    present = False\n    try:\n        f = open(\".gitattributes\", \"r\")\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if \"export-subst\" in line.strip().split()[1:]:\n                    present = True\n        f.close()\n    except EnvironmentError:\n        pass\n    if not present:\n        f = open(\".gitattributes\", \"a+\")\n        f.write(\"%s export-subst\\n\" % versionfile_source)\n        f.close()\n        files.append(\".gitattributes\")\n    run_command(GITS, [\"add\", \"--\"] + files)\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\"version\": dirname[len(parentdir_prefix):],\n                    \"full-revisionid\": None,\n                    \"dirty\": False, \"error\": None, \"date\": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\"Tried directories %s but none started with prefix %s\" %\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\nSHORT_VERSION_PY = \"\"\"\n# This file was generated by 'versioneer.py' (0.18) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\n\nversion_json = '''\n%s\n'''  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n\"\"\"\n\n\ndef versions_from_file(filename):\n    \"\"\"Try to determine the version from _version.py if present.\"\"\"\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except EnvironmentError:\n        raise NotThisMethod(\"unable to read _version.py\")\n    mo = re.search(r\"version_json = '''\\n(.*)'''  # END VERSION_JSON\",\n                   contents, re.M | re.S)\n    if not mo:\n        mo = re.search(r\"version_json = '''\\r\\n(.*)'''  # END VERSION_JSON\",\n                       contents, re.M | re.S)\n    if not mo:\n        raise NotThisMethod(\"no version_json in _version.py\")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename, versions):\n    \"\"\"Write the given version number to the given _version.py file.\"\"\"\n    os.unlink(filename)\n    contents = json.dumps(versions, sort_keys=True,\n                          indent=1, separators=(\",\", \": \"))\n    with open(filename, \"w\") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(\"set %s to '%s'\" % (filename, versions[\"version\"]))\n\n\ndef plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces):\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"],\n                                          pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    \"\"\"TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \".post.dev%d\" % pieces[\"distance\"]\n    else:\n        # exception #1\n        rendered = \"0.post.dev%d\" % pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces):\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\"version\": \"unknown\",\n                \"full-revisionid\": pieces.get(\"long\"),\n                \"dirty\": None,\n                \"error\": pieces[\"error\"],\n                \"date\": None}\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n            \"dirty\": pieces[\"dirty\"], \"error\": None,\n            \"date\": pieces.get(\"date\")}\n\n\nclass VersioneerBadRootError(Exception):\n    \"\"\"The project root directory is unknown or missing key files.\"\"\"\n\n\ndef get_versions(verbose=False):\n    \"\"\"Get the project version from whatever source is available.\n\n    Returns dict with two keys: 'version' and 'full'.\n    \"\"\"\n    if \"versioneer\" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[\"versioneer\"]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, \"please set [versioneer]VCS= in setup.cfg\"\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, \"unrecognized VCS '%s'\" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert cfg.versionfile_source is not None, \\\n        \"please set versioneer.versionfile_source\"\n    assert cfg.tag_prefix is not None, \"please set versioneer.tag_prefix\"\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. 'git\n    # describe'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by 'setup.py sdist',\n    # and for users of a tarball/zipball created by 'git archive' or github's\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(\"get_keywords\")\n    from_keywords_f = handlers.get(\"keywords\")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(\"got version from expanded keyword %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(\"got version from file %s %s\" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(\"pieces_from_vcs\")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(\"got version from VCS %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(\"got version from parentdir %s\" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(\"unable to compute version\")\n\n    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n            \"dirty\": None, \"error\": \"unable to compute version\",\n            \"date\": None}\n\n\ndef get_version():\n    \"\"\"Get the short version string for this project.\"\"\"\n    return get_versions()[\"version\"]\n\n\ndef get_cmdclass():\n    \"\"\"Get the custom setuptools/distutils subclasses used by Versioneer.\"\"\"\n    if \"versioneer\" in sys.modules:\n        del sys.modules[\"versioneer\"]\n        # this fixes the \"python setup.py develop\" case (also 'install' and\n        # 'easy_install .'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A's setup.py imports A's Versioneer, leaving it in\n        # sys.modules by the time B's setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it's pre-build state, so the\n        # parent is protected against the child's \"import versioneer\". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent's versioneer too.\n        # Also see https://github.com/warner/python-versioneer/issues/52\n\n    cmds = {}\n\n    # we add \"version\" to both distutils and setuptools\n    from distutils.core import Command\n\n    class cmd_version(Command):\n        description = \"report generated version string\"\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(\"Version: %s\" % vers[\"version\"])\n            print(\" full-revisionid: %s\" % vers.get(\"full-revisionid\"))\n            print(\" dirty: %s\" % vers.get(\"dirty\"))\n            print(\" date: %s\" % vers.get(\"date\"))\n            if vers[\"error\"]:\n                print(\" error: %s\" % vers[\"error\"])\n    cmds[\"version\"] = cmd_version\n\n    # we override \"build_py\" in both distutils and setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n    #  pip install:\n    #   copies source tree to a tempdir before running egg_info/etc\n    #   if .git isn't copied too, 'git describe' will fail\n    #   then does setup.py bdist_wheel, or sometimes setup.py install\n    #  setup.py egg_info -> ?\n\n    # we override different \"build_py\" commands for both environments\n    if \"setuptools\" in sys.modules:\n        from setuptools.command.build_py import build_py as _build_py\n    else:\n        from distutils.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib,\n                                                  cfg.versionfile_build)\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n    cmds[\"build_py\"] = cmd_build_py\n\n    if \"cx_Freeze\" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n        # nczeczulin reports that py2exe won't like the pep440-style string\n        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n        # setup(console=[{\n        #   \"version\": versioneer.get_version().split(\"+\", 1)[0], # FILEVERSION\n        #   \"product_version\": versioneer.get_version(),\n        #   ...\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, \"w\") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(LONG %\n                            {\"DOLLAR\": \"$\",\n                             \"STYLE\": cfg.style,\n                             \"TAG_PREFIX\": cfg.tag_prefix,\n                             \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                             \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                             })\n        cmds[\"build_exe\"] = cmd_build_exe\n        del cmds[\"build_py\"]\n\n    if 'py2exe' in sys.modules:  # py2exe enabled?\n        try:\n            from py2exe.distutils_buildexe import py2exe as _py2exe  # py3\n        except ImportError:\n            from py2exe.build_exe import py2exe as _py2exe  # py2\n\n        class cmd_py2exe(_py2exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _py2exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, \"w\") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(LONG %\n                            {\"DOLLAR\": \"$\",\n                             \"STYLE\": cfg.style,\n                             \"TAG_PREFIX\": cfg.tag_prefix,\n                             \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                             \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                             })\n        cmds[\"py2exe\"] = cmd_py2exe\n\n    # we override different \"sdist\" commands for both environments\n    if \"setuptools\" in sys.modules:\n        from setuptools.command.sdist import sdist as _sdist\n    else:\n        from distutils.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[\"version\"]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(\"UPDATING %s\" % target_versionfile)\n            write_to_version_file(target_versionfile,\n                                  self._versioneer_generated_versions)\n    cmds[\"sdist\"] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = \"\"\"\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix =\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or 'python versioneer.py setup'.\n\"\"\"\n\nSAMPLE_CONFIG = \"\"\"\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run 'versioneer.py setup' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n\"\"\"\n\nINIT_PY_SNIPPET = \"\"\"\nfrom ._version import get_versions\n__version__ = get_versions()['version']\ndel get_versions\n\"\"\"\n\n\ndef do_setup():\n    \"\"\"Main VCS-independent setup function for installing Versioneer.\"\"\"\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (EnvironmentError, configparser.NoSectionError,\n            configparser.NoOptionError) as e:\n        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n            print(\"Adding sample versioneer config to setup.cfg\",\n                  file=sys.stderr)\n            with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print(\" creating %s\" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, \"w\") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(LONG % {\"DOLLAR\": \"$\",\n                        \"STYLE\": cfg.style,\n                        \"TAG_PREFIX\": cfg.tag_prefix,\n                        \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                        \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                        })\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),\n                       \"__init__.py\")\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, \"r\") as f:\n                old = f.read()\n        except EnvironmentError:\n            old = \"\"\n        if INIT_PY_SNIPPET not in old:\n            print(\" appending to %s\" % ipy)\n            with open(ipy, \"a\") as f:\n                f.write(INIT_PY_SNIPPET)\n        else:\n            print(\" %s unmodified\" % ipy)\n    else:\n        print(\" %s doesn't exist, ok\" % ipy)\n        ipy = None\n\n    # Make sure both the top-level \"versioneer.py\" and versionfile_source\n    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n    # they'll be copied into source distributions. Pip won't be able to\n    # install the package without this.\n    manifest_in = os.path.join(root, \"MANIFEST.in\")\n    simple_includes = set()\n    try:\n        with open(manifest_in, \"r\") as f:\n            for line in f:\n                if line.startswith(\"include \"):\n                    for include in line.split()[1:]:\n                        simple_includes.add(include)\n    except EnvironmentError:\n        pass\n    # That doesn't cover everything MANIFEST.in can do\n    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n    # it might give some false negatives. Appending redundant 'include'\n    # lines is safe, though.\n    if \"versioneer.py\" not in simple_includes:\n        print(\" appending 'versioneer.py' to MANIFEST.in\")\n        with open(manifest_in, \"a\") as f:\n            f.write(\"include versioneer.py\\n\")\n    else:\n        print(\" 'versioneer.py' already in MANIFEST.in\")\n    if cfg.versionfile_source not in simple_includes:\n        print(\" appending versionfile_source ('%s') to MANIFEST.in\" %\n              cfg.versionfile_source)\n        with open(manifest_in, \"a\") as f:\n            f.write(\"include %s\\n\" % cfg.versionfile_source)\n    else:\n        print(\" versionfile_source already in MANIFEST.in\")\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-subst keyword\n    # substitution.\n    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n    return 0\n\n\ndef scan_setup_py():\n    \"\"\"Validate the contents of setup.py against Versioneer's expectations.\"\"\"\n    found = set()\n    setters = False\n    errors = 0\n    with open(\"setup.py\", \"r\") as f:\n        for line in f.readlines():\n            if \"import versioneer\" in line:\n                found.add(\"import\")\n            if \"versioneer.get_cmdclass()\" in line:\n                found.add(\"cmdclass\")\n            if \"versioneer.get_version()\" in line:\n                found.add(\"get_version\")\n            if \"versioneer.VCS\" in line:\n                setters = True\n            if \"versioneer.versionfile_source\" in line:\n                setters = True\n    if len(found) != 3:\n        print(\"\")\n        print(\"Your setup.py appears to be missing some important items\")\n        print(\"(but I might be wrong). Please make sure it has something\")\n        print(\"roughly like the following:\")\n        print(\"\")\n        print(\" import versioneer\")\n        print(\" setup( version=versioneer.get_version(),\")\n        print(\"        cmdclass=versioneer.get_cmdclass(),  ...)\")\n        print(\"\")\n        errors += 1\n    if setters:\n        print(\"You should remove lines like 'versioneer.VCS = ' and\")\n        print(\"'versioneer.versionfile_source = ' . This configuration\")\n        print(\"now lives in setup.cfg, and should be removed from setup.py\")\n        print(\"\")\n        errors += 1\n    return errors\n\n\nif __name__ == \"__main__\":\n    cmd = sys.argv[1]\n    if cmd == \"setup\":\n        errors = do_setup()\n        errors += scan_setup_py()\n        if errors:\n            sys.exit(1)\n"
        }
      ]
    }
  ]
}