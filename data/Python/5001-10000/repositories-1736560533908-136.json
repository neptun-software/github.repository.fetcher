{
  "metadata": {
    "timestamp": 1736560533908,
    "page": 136,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "triton-inference-server/server",
      "stars": 8568,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 0.7724609375,
          "content": "---\nBasedOnStyle: Google\n\nIndentWidth: 2\nColumnLimit: 80\nContinuationIndentWidth: 4\nUseTab: Never\nMaxEmptyLinesToKeep: 2\n\nSortIncludes: true\nCompactNamespaces: true\nReflowComments: true\n\nDerivePointerAlignment: false\nPointerAlignment: Left\n\nAllowShortIfStatementsOnASingleLine: false\nAllowShortBlocksOnASingleLine: false\nAllowShortFunctionsOnASingleLine: Inline\n\nAlwaysBreakAfterReturnType: TopLevelDefinitions\nAlignAfterOpenBracket: AlwaysBreak\nBreakBeforeBraces: Custom\nBraceWrapping:\n  AfterClass: false\n  AfterControlStatement: false\n  AfterEnum: false\n  AfterFunction: true\n  AfterNamespace: false\n  AfterStruct: false\n  AfterUnion: false\n  BeforeCatch: true\n\nBinPackArguments: true\nBinPackParameters: true\nConstructorInitializerAllOnOneLineOrOnePerLine: false\n\nIndentCaseLabels: true\n\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.005859375,
          "content": ".git*\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1728515625,
          "content": "/build\n/builddir\n/.vscode\n*.so\n__pycache__\ntmp\n*.log\n*.xml\ntest_results.txt\nartifacts\ncprofile\n*.prof\n\n# Test exclusions\nqa/L0_openai/openai\ntensorrtllm_models\ncustom_tokenizer\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 3.126953125,
          "content": "# Copyright 2023-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nrepos:\n- repo: https://github.com/timothycrosley/isort\n  rev: 5.12.0\n  hooks:\n  - id: isort\n    additional_dependencies: [toml]\n- repo: https://github.com/psf/black\n  rev: 23.1.0\n  hooks:\n  - id: black\n    types_or: [python, cython]\n- repo: https://github.com/PyCQA/flake8\n  rev: 5.0.4\n  hooks:\n  - id: flake8\n    args: [--max-line-length=88, --select=C,E,F,W,B,B950, --extend-ignore = E203,E501]\n    types_or: [python, cython]\n- repo: https://github.com/pre-commit/mirrors-clang-format\n  rev: v16.0.5\n  hooks:\n  - id: clang-format\n    types_or: [c, c++, cuda, proto, textproto, java]\n    args: [\"-fallback-style=none\", \"-style=file\", \"-i\"]\n- repo: https://github.com/codespell-project/codespell\n  rev: v2.2.4\n  hooks:\n  - id: codespell\n    additional_dependencies: [tomli]\n    args: [\"--toml\", \"pyproject.toml\"]\n    exclude: (?x)^(.*stemmer.*|.*stop_words.*|^CHANGELOG.md$)\n# More details about these pre-commit hooks here:\n# https://pre-commit.com/hooks.html\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v4.4.0\n  hooks:\n  - id: check-case-conflict\n  - id: check-executables-have-shebangs\n  - id: check-merge-conflict\n  - id: check-json\n  - id: check-toml\n  - id: check-yaml\n    exclude: ^deploy(\\/[^\\/]+)*\\/templates\\/.*$\n  - id: check-shebang-scripts-are-executable\n  - id: end-of-file-fixer\n    types_or: [c, c++, cuda, proto, textproto, java, python]\n  - id: mixed-line-ending\n  - id: requirements-txt-fixer\n  - id: trailing-whitespace\n\n- repo: local\n  hooks:\n  - id: add-license\n    name: Add License\n    entry: python tools/add_copyright.py\n    language: python\n    stages: [pre-commit]\n    verbose: true\n    require_serial: true\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.3125,
          "content": "cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\ntitle: \"Triton Inference Server: An Optimized Cloud and Edge Inferencing Solution.\"\nurl: https://github.com/triton-inference-server\nrepository-code: https://github.com/triton-inference-server/server\nauthors:\n  - name: \"NVIDIA Corporation\"\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 12.533203125,
          "content": "# Copyright 2020-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\ncmake_minimum_required(VERSION 3.18)\n\nproject(tritonserver LANGUAGES C CXX)\n\ninclude(CMakeDependentOption)\n\n# Use C++17 standard as Triton's minimum required.\nset(TRITON_MIN_CXX_STANDARD 17 CACHE STRING \"The minimum C++ standard which features are requested to build this target.\")\n\nset(TRITON_VERSION \"0.0.0\" CACHE STRING \"The version of the Triton shared library\" )\n\noption(TRITON_ENABLE_LOGGING \"Include logging support in server\" ON)\noption(TRITON_ENABLE_STATS \"Include statistics collections in server\" ON)\noption(TRITON_ENABLE_TRACING \"Include tracing support in server\" OFF)\noption(TRITON_ENABLE_NVTX \"Include NVTX support in server\" OFF)\noption(TRITON_ENABLE_GPU \"Enable GPU support in server\" ON)\noption(TRITON_ENABLE_MALI_GPU \"Enable Arm Mali GPU support in server\" OFF)\noption(TRITON_IGPU_BUILD \"Enable options for iGPU compilation in sever\" OFF)\nset(TRITON_MIN_COMPUTE_CAPABILITY \"7.5\" CACHE STRING\n    \"The minimum CUDA compute capability supported by Triton\" )\nset(TRITON_EXTRA_LIB_PATHS \"\" CACHE PATH \"Extra library paths for Triton Server build\")\n\n# Ensemble\noption(TRITON_ENABLE_ENSEMBLE \"Include ensemble support in server\" OFF)\n\n# Endpoints\noption(TRITON_ENABLE_HTTP \"Include HTTP API in server\" ON)\noption(TRITON_ENABLE_GRPC \"Include GRPC API in server\" ON)\noption(TRITON_ENABLE_SAGEMAKER \"Include AWS SageMaker API in server\" OFF)\noption(TRITON_ENABLE_VERTEX_AI \"Include Vertex AI API in server\" OFF)\n\n# Metrics\noption(TRITON_ENABLE_METRICS \"Include metrics support in server\" ON)\noption(TRITON_ENABLE_METRICS_GPU \"Include GPU metrics support in server\" ON)\noption(TRITON_ENABLE_METRICS_CPU \"Include CPU metrics support in server\" ON)\n\n# Cloud storage\noption(TRITON_ENABLE_GCS \"Include GCS Filesystem support in server\" OFF)\noption(TRITON_ENABLE_S3 \"Include S3 Filesystem support in server\" OFF)\noption(TRITON_ENABLE_AZURE_STORAGE \"Include Azure Storage Filesystem support in server\" OFF)\n\n# Need to know if TensorRT is available when building unit tests\noption(TRITON_ENABLE_TENSORRT \"Include TensorRT backend in server\" OFF)\n\n# ASAN\noption(TRITON_ENABLE_ASAN \"Build with address sanitizer\" OFF)\n\n# Repo tags\nset(TRITON_REPO_ORGANIZATION \"https://github.com/triton-inference-server\" CACHE STRING \"Git repository to pull from\")\nset(TRITON_THIRD_PARTY_REPO_TAG \"main\" CACHE STRING\n    \"Tag for triton-inference-server/third_party repo\")\nset(TRITON_COMMON_REPO_TAG \"main\" CACHE STRING \"Tag for triton-inference-server/common repo\")\nset(TRITON_CORE_REPO_TAG \"main\" CACHE STRING \"Tag for triton-inference-server/core repo\")\nset(TRITON_BACKEND_REPO_TAG \"main\" CACHE STRING \"Tag for triton-inference-server/backend repo\")\n\n# Third-party location\nset(TRITON_THIRD_PARTY_INSTALL_PREFIX \"${CMAKE_CURRENT_BINARY_DIR}/third-party\" CACHE STRING \"Location of third-party build\")\nset(TRITON_THIRD_PARTY_SRC_INSTALL_PREFIX \"${CMAKE_CURRENT_BINARY_DIR}/third-party-src\" CACHE STRING \"Location of third-party source\")\n\nif(TRITON_ENABLE_METRICS AND NOT TRITON_ENABLE_STATS)\n  message(FATAL_ERROR \"TRITON_ENABLE_METRICS=ON requires TRITON_ENABLE_STATS=ON\")\nendif()\n\nif(TRITON_ENABLE_TRACING AND NOT TRITON_ENABLE_STATS)\n  message(FATAL_ERROR \"TRITON_ENABLE_TRACING=ON requires TRITON_ENABLE_STATS=ON\")\nendif()\n\nif (TRITON_ENABLE_METRICS_CPU AND NOT TRITON_ENABLE_METRICS)\n  message(FATAL_ERROR \"TRITON_ENABLE_METRICS_CPU=ON requires TRITON_ENABLE_METRICS=ON\")\nendif()\n\nif (TRITON_ENABLE_METRICS_GPU AND NOT TRITON_ENABLE_METRICS)\n  message(FATAL_ERROR \"TRITON_ENABLE_METRICS_GPU=ON requires TRITON_ENABLE_METRICS=ON\")\nendif()\n\nif (TRITON_ENABLE_METRICS_GPU AND NOT TRITON_ENABLE_GPU)\n  message(FATAL_ERROR \"TRITON_ENABLE_METRICS_GPU=ON requires TRITON_ENABLE_GPU=ON\")\nendif()\n\nif(TRITON_ENABLE_ASAN AND TRITON_ENABLE_GPU)\n  message(FATAL_ERROR \"TRITON_ENABLE_ASAN=ON requires TRITON_ENABLE_GPU=OFF\")\nendif()\n\n#\n# Dependencies\n#\ninclude(FetchContent)\n\nFetchContent_Declare(\n  repo-core\n  GIT_REPOSITORY ${TRITON_REPO_ORGANIZATION}/core.git\n  GIT_TAG ${TRITON_CORE_REPO_TAG}\n)\nFetchContent_Declare(\n  repo-third-party\n  GIT_REPOSITORY ${TRITON_REPO_ORGANIZATION}/third_party.git\n  GIT_TAG ${TRITON_THIRD_PARTY_REPO_TAG}\n)\n\n# Some libs are installed to ${TRITON_THIRD_PARTY_INSTALL_PREFIX}/{LIB}/lib64 instead\n# of ${TRITON_THIRD_PARTY_INSTALL_PREFIX}/{LIB}/lib on Centos\nset(LIB_DIR \"lib\")\nif(LINUX)\n  file(STRINGS \"/etc/os-release\" DISTRO_ID_LIKE REGEX \"ID_LIKE\")\n  if(${DISTRO_ID_LIKE} MATCHES \"rhel|centos\")\n    set (LIB_DIR \"lib64\")\n  endif(${DISTRO_ID_LIKE} MATCHES \"rhel|centos\")\nendif(LINUX)\nset(TRITON_CORE_HEADERS_ONLY OFF)\n\nFetchContent_MakeAvailable(repo-third-party repo-core)\n\n#\n# Triton server executable and examples\n#\n\n# Need to use ExternalProject for our builds so that we can get the\n# correct dependencies between Triton executable and the\n# ExternalProject dependencies (found in the third_party repo)\ninclude(ExternalProject)\n\n# If CMAKE_TOOLCHAIN_FILE is set, propagate that hint path to the external\n# projects.\nset(_CMAKE_ARGS_CMAKE_TOOLCHAIN_FILE \"\")\nif (CMAKE_TOOLCHAIN_FILE)\n  set(_CMAKE_ARGS_CMAKE_TOOLCHAIN_FILE \"-DCMAKE_TOOLCHAIN_FILE:PATH=${CMAKE_TOOLCHAIN_FILE}\")\nendif()\n\n# If VCPKG_TARGET_TRIPLET is set, propagate that hint path to the external\n# projects.\nset(_CMAKE_ARGS_VCPKG_TARGET_TRIPLET \"\")\nif (VCPKG_TARGET_TRIPLET)\n  set(_CMAKE_ARGS_VCPKG_TARGET_TRIPLET \"-DVCPKG_TARGET_TRIPLET:STRING=${VCPKG_TARGET_TRIPLET}\")\nendif()\n\n# If OPENSSL_ROOT_DIR is set, propagate that hint path to the external\n# projects with OpenSSL dependency.\nset(_CMAKE_ARGS_OPENSSL_ROOT_DIR \"\")\nif (OPENSSL_ROOT_DIR)\n  set(_CMAKE_ARGS_OPENSSL_ROOT_DIR \"-DOPENSSL_ROOT_DIR:PATH=${OPENSSL_ROOT_DIR}\")\nendif()\n\n# Location where protobuf-config.cmake will be installed varies by\n# platform\nif (WIN32)\n  set(_FINDPACKAGE_PROTOBUF_CONFIG_DIR \"${TRITON_THIRD_PARTY_INSTALL_PREFIX}/protobuf/cmake\")\nelse()\n  set(_FINDPACKAGE_PROTOBUF_CONFIG_DIR \"${TRITON_THIRD_PARTY_INSTALL_PREFIX}/protobuf/${LIB_DIR}/cmake/protobuf\")\nendif()\n\n# Triton with Opentelemetry is not supported on Windows\n# FIXME: add location for Windows, when support is added\n# JIRA DLIS-4786\nif (WIN32)\n  set(_FINDPACKAGE_OPENTELEMETRY_CONFIG_DIR \"\")\nelse()\n  set(_FINDPACKAGE_OPENTELEMETRY_CONFIG_DIR \"${TRITON_THIRD_PARTY_INSTALL_PREFIX}/opentelemetry-cpp/${LIB_DIR}/cmake/opentelemetry-cpp\")\nendif()\n\nif (CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)\n  set(TRITON_INSTALL_PREFIX ${CMAKE_CURRENT_BINARY_DIR}/install)\nelse()\n  set(TRITON_INSTALL_PREFIX ${CMAKE_INSTALL_PREFIX})\nendif()\n\nset(TRITON_DEPENDS triton-core protobuf googletest re2)\nif(${TRITON_ENABLE_GCS})\n  set(TRITON_DEPENDS ${TRITON_DEPENDS} google-cloud-cpp)\nendif() # TRITON_ENABLE_GCS\nif(${TRITON_ENABLE_S3})\n  set(TRITON_DEPENDS ${TRITON_DEPENDS} aws-sdk-cpp)\nendif() # TRITON_ENABLE_S3\nif(${TRITON_ENABLE_HTTP} OR ${TRITON_ENABLE_METRICS} OR ${TRITON_ENABLE_SAGEMAKER} OR ${TRITON_ENABLE_VERTEX_AI})\n  set(TRITON_DEPENDS ${TRITON_DEPENDS} libevent libevhtp)\nendif() # TRITON_ENABLE_HTTP || TRITON_ENABLE_METRICS || TRITON_ENABLE_SAGEMAKER || TRITON_ENABLE_VERTEX_AI\nif(${TRITON_ENABLE_GRPC})\n  set(TRITON_DEPENDS ${TRITON_DEPENDS} grpc)\nendif() # TRITON_ENABLE_GRPC\nif(NOT WIN32 AND ${TRITON_ENABLE_TRACING})\n  set(TRITON_DEPENDS ${TRITON_DEPENDS} opentelemetry-cpp)\nendif() # TRITON_ENABLE_TRACING\n\nExternalProject_Add(triton-server\n  PREFIX triton-server\n  SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/src\"\n  BINARY_DIR \"${CMAKE_CURRENT_BINARY_DIR}/triton-server\"\n  CMAKE_CACHE_ARGS\n    -DProtobuf_DIR:PATH=${_FINDPACKAGE_PROTOBUF_CONFIG_DIR}\n    ${_CMAKE_ARGS_OPENSSL_ROOT_DIR}\n    ${_CMAKE_ARGS_CMAKE_TOOLCHAIN_FILE}\n    ${_CMAKE_ARGS_VCPKG_TARGET_TRIPLET}\n    -DGTEST_ROOT:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/googletest\n    -DgRPC_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/grpc/lib/cmake/grpc\n    -Dc-ares_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/c-ares/${LIB_DIR}/cmake/c-ares\n    -Dre2_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/re2/${LIB_DIR}/cmake/re2\n    -Dabsl_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/absl/${LIB_DIR}/cmake/absl\n    -DCURL_DIR:STRING=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/curl/${LIB_DIR}/cmake/CURL\n    -Dnlohmann_json_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/nlohmann_json/share/cmake/nlohmann_json\n    -DLibevent_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/libevent/lib/cmake/libevent\n    -Dlibevhtp_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/libevhtp/lib/cmake/libevhtp\n    -Dstorage_client_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/google-cloud-cpp/${LIB_DIR}/cmake/storage_client\n    -Dgoogle_cloud_cpp_common_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/google-cloud-cpp/${LIB_DIR}/cmake/google_cloud_cpp_common\n    -DCrc32c_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/crc32c/${LIB_DIR}/cmake/Crc32c\n    -DAWSSDK_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/aws-sdk-cpp/${LIB_DIR}/cmake/AWSSDK\n    -Daws-cpp-sdk-core_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/aws-sdk-cpp/${LIB_DIR}/cmake/aws-cpp-sdk-core\n    -Daws-cpp-sdk-s3_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/aws-sdk-cpp/${LIB_DIR}/cmake/aws-cpp-sdk-s3\n    -Daws-c-event-stream_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/aws-sdk-cpp/${LIB_DIR}/aws-c-event-stream/cmake\n    -Daws-c-common_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/aws-sdk-cpp/${LIB_DIR}/aws-c-common/cmake\n    -Daws-checksums_DIR:PATH=${TRITON_THIRD_PARTY_INSTALL_PREFIX}/aws-sdk-cpp/${LIB_DIR}/aws-checksums/cmake\n    -Dopentelemetry-cpp_DIR:PATH=${_FINDPACKAGE_OPENTELEMETRY_CONFIG_DIR}\n    -DTRITON_REPO_ORGANIZATION:STRING=${TRITON_REPO_ORGANIZATION}\n    -DTRITON_IGPU_BUILD:BOOL=${TRITON_IGPU_BUILD}\n    -DTRITON_THIRD_PARTY_REPO_TAG:STRING=${TRITON_THIRD_PARTY_REPO_TAG}\n    -DTRITON_COMMON_REPO_TAG:STRING=${TRITON_COMMON_REPO_TAG}\n    -DTRITON_CORE_REPO_TAG:STRING=${TRITON_CORE_REPO_TAG}\n    -DTRITON_BACKEND_REPO_TAG:STRING=${TRITON_BACKEND_REPO_TAG}\n    -DTRITON_EXTRA_LIB_PATHS:PATH=${TRITON_EXTRA_LIB_PATHS}\n    -DTRITON_ENABLE_ASAN:BOOL=${TRITON_ENABLE_ASAN}\n    -DTRITON_ENABLE_NVTX:BOOL=${TRITON_ENABLE_NVTX}\n    -DTRITON_ENABLE_TRACING:BOOL=${TRITON_ENABLE_TRACING}\n    -DTRITON_ENABLE_LOGGING:BOOL=${TRITON_ENABLE_LOGGING}\n    -DTRITON_ENABLE_STATS:BOOL=${TRITON_ENABLE_STATS}\n    -DTRITON_ENABLE_GPU:BOOL=${TRITON_ENABLE_GPU}\n    -DTRITON_ENABLE_MALI_GPU:BOOL=${TRITON_ENABLE_MALI_GPU}\n    -DTRITON_ENABLE_HTTP:BOOL=${TRITON_ENABLE_HTTP}\n    -DTRITON_ENABLE_SAGEMAKER:BOOL=${TRITON_ENABLE_SAGEMAKER}\n    -DTRITON_ENABLE_VERTEX_AI:BOOL=${TRITON_ENABLE_VERTEX_AI}\n    -DTRITON_ENABLE_GRPC:BOOL=${TRITON_ENABLE_GRPC}\n    -DTRITON_MIN_COMPUTE_CAPABILITY:STRING=${TRITON_MIN_COMPUTE_CAPABILITY}\n    -DTRITON_ENABLE_METRICS:BOOL=${TRITON_ENABLE_METRICS}\n    -DTRITON_ENABLE_METRICS_GPU:BOOL=${TRITON_ENABLE_METRICS_GPU}\n    -DTRITON_ENABLE_METRICS_CPU:BOOL=${TRITON_ENABLE_METRICS_CPU}\n    -DTRITON_ENABLE_GCS:BOOL=${TRITON_ENABLE_GCS}\n    -DTRITON_ENABLE_AZURE_STORAGE:BOOL=${TRITON_ENABLE_AZURE_STORAGE}\n    -DTRITON_ENABLE_S3:BOOL=${TRITON_ENABLE_S3}\n    -DTRITON_ENABLE_TENSORRT:BOOL=${TRITON_ENABLE_TENSORRT}\n    -DTRITON_ENABLE_ENSEMBLE:BOOL=${TRITON_ENABLE_ENSEMBLE}\n    -DTRITON_MIN_CXX_STANDARD:STRING=${TRITON_MIN_CXX_STANDARD}\n    -DCMAKE_BUILD_TYPE:STRING=${CMAKE_BUILD_TYPE}\n    -DCMAKE_INSTALL_PREFIX:PATH=${TRITON_INSTALL_PREFIX}\n    -DTRITON_VERSION:STRING=${TRITON_VERSION}\n  DEPENDS ${TRITON_DEPENDS}\n)\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 5.8251953125,
          "content": "<!--\n# Copyright 2018-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n-->\n\n# Contribution Guidelines\n\nContributions that fix documentation errors or that make small changes\nto existing code can be contributed directly by following the rules\nbelow and submitting an appropriate PR.\n\nContributions intended to add significant new functionality must\nfollow a more collaborative path described in the following\npoints. Before submitting a large PR that adds a major enhancement or\nextension, be sure to submit a GitHub issue that describes the\nproposed change so that the Triton team can provide feedback.\n\n- As part of the GitHub issue discussion, a design for your change\n  will be agreed upon. An up-front design discussion is required to\n  ensure that your enhancement is done in a manner that is consistent\n  with Triton's overall architecture.\n\n- The Triton project is spread across multiple repos. The Triton team\n  will provide guidance about how and where your enhancement should be\n  implemented.\n\n- [Testing](docs/customization_guide/test.md) is a critical part of any Triton\n  enhancement. You should plan on spending significant time on\n  creating tests for your change. The Triton team will help you to\n  design your testing so that it is compatible with existing testing\n  infrastructure.\n\n- If your enhancement provides a user visible feature then you need to\n  provide documentation.\n\n# Contribution Rules\n\n- The code style convention is enforced by clang-format. See below on\n  how to ensure your contributions conform. In general please follow\n  the existing conventions in the relevant file, submodule, module,\n  and project when you add new code or when you extend/fix existing\n  functionality.\n\n- Avoid introducing unnecessary complexity into existing code so that\n  maintainability and readability are preserved.\n\n- Try to keep pull requests (PRs) as concise as possible:\n\n  - Avoid committing commented-out code.\n\n  - Wherever possible, each PR should address a single concern. If\n    there are several otherwise-unrelated things that should be fixed\n    to reach a desired endpoint, it is perfectly fine to open several\n    PRs and state in the description which PR depends on another\n    PR. The more complex the changes are in a single PR, the more time\n    it will take to review those changes.\n\n  - Make sure that the build log is clean, meaning no warnings or\n    errors should be present.\n\n- Make sure all `L0_*` tests pass:\n\n  - In the `qa/` directory, there are basic sanity tests scripted in\n    directories named `L0_...`.  See the [Test](docs/customization_guide/test.md)\n    documentation for instructions on running these tests.\n\n- Triton Inference Server's default build assumes recent versions of\n  dependencies (CUDA, TensorFlow, PyTorch, TensorRT,\n  etc.). Contributions that add compatibility with older versions of\n  those dependencies will be considered, but NVIDIA cannot guarantee\n  that all possible build configurations work, are not broken by\n  future contributions, and retain highest performance.\n\n- Make sure that you can contribute your work to open source (no\n  license and/or patent conflict is introduced by your code). You need\n  to complete the CLA described below before your PR can be merged.\n\n- Thanks in advance for your patience as we review your contributions;\n  we do appreciate them!\n\n# Coding Convention\n\nAll pull requests are checked against the\n[pre-commit hooks](https://github.com/pre-commit/pre-commit-hooks)\nlocated [in the repository's top-level .pre-commit-config.yaml](https://github.com/NVIDIA/triton-inference-server/blob/master/pre-commit-config.yaml).\nThe hooks do some sanity checking like linting and formatting.\nThese checks must pass to merge a change.\n\nTo run these locally, you can\n[install pre-commit,](https://pre-commit.com/#install)\nthen run `pre-commit install` inside the cloned repo. When you\ncommit a change, the pre-commit hooks will run automatically.\nIf a fix is implemented by a pre-commit hook, adding the file again\nand running `git commit` a second time will pass and successfully\ncommit.\n\n# Contributor License Agreement (CLA)\n\nTriton requires that all contributors (or their corporate entity) send\na signed copy of the [Contributor License\nAgreement](https://github.com/NVIDIA/triton-inference-server/blob/master/Triton-CCLA-v1.pdf)\nto triton-cla@nvidia.com.\n*NOTE*: Contributors with no company affiliation can fill `N/A` in the\n`Corporation Name` and `Corporation Address` fields.\n"
        },
        {
          "name": "Dockerfile.QA",
          "type": "blob",
          "size": 19.08203125,
          "content": "# Copyright 2018-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nARG BASE_IMAGE=tritonserver\nARG CIBASE_IMAGE=tritonserver_cibase\nARG SDK_IMAGE=tritonserver_sdk\nARG TRITON_REPO_ORGANIZATION=http://github.com/triton-inference-server\nARG TRITON_COMMON_REPO_TAG=main\nARG TRITON_CORE_REPO_TAG=main\nARG TRITON_THIRD_PARTY_REPO_TAG=main\nARG TRITON_BACKEND_REPO_TAG=main\nARG TRITONTMP_DIR=/tmp\nARG IGPU_BUILD=0\n\n############################################################################\n## Test artifacts built as part of the tritonserver build are\n## available in CIBASE_IMAGE. Copy these artifacts into the QA area.\n############################################################################\nFROM ${CIBASE_IMAGE} AS cibase\n\nARG TRITONTMP_DIR\nARG TRITON_REPO_ORGANIZATION\nARG TRITON_COMMON_REPO_TAG\nARG TRITON_CORE_REPO_TAG\nARG TRITON_THIRD_PARTY_REPO_TAG\nARG TRITON_BACKEND_REPO_TAG\nARG IGPU_BUILD\n\n# Ensure apt-get won't prompt for selecting options\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n            build-essential \\\n            libarchive-dev \\\n            libboost-dev \\\n            python3-dev \\\n            python3-pip \\\n            python3-wheel \\\n            python3-setuptools \\\n            rapidjson-dev \\\n            software-properties-common && \\\n    rm -rf /var/lib/apt/lists/*\n\nRUN apt update -q=2 \\\n    && apt install -y gpg wget \\\n    && wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - |  tee /usr/share/keyrings/kitware-archive-keyring.gpg >/dev/null \\\n    && . /etc/os-release \\\n    && echo \"deb [signed-by=/usr/share/keyrings/kitware-archive-keyring.gpg] https://apt.kitware.com/ubuntu/ $UBUNTU_CODENAME main\" | tee /etc/apt/sources.list.d/kitware.list >/dev/null \\\n    && apt-get update -q=2 \\\n    && apt-get install -y --no-install-recommends cmake=3.28.3* cmake-data=3.28.3*\n\n# Add inception_graphdef model to example repo\nWORKDIR /workspace/docs/examples/model_repository\nRUN mkdir -p inception_graphdef/1 && \\\n    wget -O ${TRITONTMP_DIR}/inception_v3_2016_08_28_frozen.pb.tar.gz \\\n        https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz && \\\n    (cd ${TRITONTMP_DIR} && tar xzf inception_v3_2016_08_28_frozen.pb.tar.gz) && \\\n    mv ${TRITONTMP_DIR}/inception_v3_2016_08_28_frozen.pb inception_graphdef/1/model.graphdef\n\n# Update the qa/ directory with test executables, models, etc.\nWORKDIR /workspace\nRUN mkdir -p qa/common && \\\n    cp -r /workspace/src/test/models/repeat_int32 qa/L0_decoupled/models/ && \\\n    cp -r /workspace/src/test/models/square_int32 qa/L0_decoupled/models/ && \\\n    mkdir qa/L0_simple_example/models && \\\n    cp -r docs/examples/model_repository/simple qa/L0_simple_example/models/. && \\\n    mkdir qa/L0_simple_go_client/models && \\\n    cp -r docs/examples/model_repository/simple qa/L0_simple_go_client/models/. && \\\n    mkdir qa/L0_backend_release/simple_models && \\\n    cp -r docs/examples/model_repository/simple qa/L0_backend_release/simple_models/. && \\\n    mkdir qa/L0_simple_nodejs_client/models && \\\n    cp -r docs/examples/model_repository/simple qa/L0_simple_nodejs_client/models/. && \\\n    mkdir qa/L0_backend_release/simple_seq_models && \\\n    cp -r /workspace/docs/examples/model_repository/simple_sequence qa/L0_backend_release/simple_seq_models/. && \\\n    mkdir qa/L0_shared_memory/models && \\\n    cp -r docs/examples/model_repository/simple qa/L0_shared_memory/models/. && \\\n    mkdir qa/L0_cuda_shared_memory/models && \\\n    cp -r docs/examples/model_repository/simple qa/L0_cuda_shared_memory/models/. && \\\n    mkdir qa/L0_client_java/models && \\\n    cp -r docs/examples/model_repository/simple qa/L0_client_java/models && \\\n    mkdir qa/L0_grpc/models && \\\n    cp -r docs/examples/model_repository/simple qa/L0_grpc/models && \\\n    cp -r docs/examples/model_repository/simple_dyna_sequence qa/L0_grpc/models && \\\n    cp -r docs/examples/model_repository/simple_int8 qa/L0_grpc/models && \\\n    cp -r docs/examples/model_repository/simple_identity qa/L0_grpc/models && \\\n    cp -r docs/examples/model_repository/simple_sequence qa/L0_grpc/models && \\\n    cp -r docs/examples/model_repository/simple_string qa/L0_grpc/models && \\\n    cp -r docs/examples/model_repository/inception_graphdef qa/L0_grpc/models && \\\n    mkdir qa/L0_grpc_state_cleanup/models && \\\n    cp -r /workspace/src/test/models/repeat_int32 qa/L0_grpc_state_cleanup/models/ && \\\n    mkdir qa/L0_http/models && \\\n    cp -r docs/examples/model_repository/simple qa/L0_http/models && \\\n    cp -r docs/examples/model_repository/simple_dyna_sequence qa/L0_http/models && \\\n    cp -r docs/examples/model_repository/simple_identity qa/L0_http/models && \\\n    cp -r docs/examples/model_repository/simple_sequence qa/L0_http/models && \\\n    cp -r docs/examples/model_repository/simple_string qa/L0_http/models && \\\n    cp -r docs/examples/model_repository/inception_graphdef qa/L0_http/models && \\\n    mkdir qa/L0_https/models && \\\n    cp -r docs/examples/model_repository/simple qa/L0_https/models/. && \\\n    mkdir qa/L0_secure_grpc/models && \\\n    cp -r docs/examples/model_repository/simple qa/L0_secure_grpc/models/. && \\\n    cp bin/simple qa/L0_simple_lib/. && \\\n    cp bin/memory_alloc qa/L0_io/. && \\\n    cp bin/multi_server qa/L0_multi_server/. && \\\n    cp bin/memory_test qa/L0_memory/. && \\\n    cp bin/pinned_memory_manager_test qa/L0_memory/. && \\\n    cp bin/repo_agent_test qa/L0_triton_repo_agent/. && \\\n    cp lib/libtritonrepoagent_relocation.so qa/L0_triton_repo_agent/. && \\\n    mkdir qa/L0_query/models/query/1 && \\\n    cp tritonbuild/tritonserver/backends/query/libtriton_query.so qa/L0_query/models/query/1/. && \\\n    cp bin/query_test qa/L0_query/. && \\\n    mkdir qa/L0_iterative_sequence/models/iterative_sequence/1 && \\\n    cp tritonbuild/tritonserver/backends/iterative_sequence/libtriton_iterative_sequence.so qa/L0_iterative_sequence/models/iterative_sequence/1/. && \\\n    cp bin/register_api_test qa/L0_register/. && \\\n    cp bin/async_work_queue_test qa/L0_async_work_queue/. && \\\n    cp tritonbuild/tritonserver/backends/implicit_state/libtriton_implicit_state.so \\\n       qa/L0_implicit_state/. && \\\n    mkdir qa/L0_data_compression/models && \\\n    cp -r docs/examples/model_repository/simple qa/L0_data_compression/models && \\\n    cp bin/data_compressor_test qa/L0_data_compression/. && \\\n    cp bin/metrics_api_test qa/L0_metrics/. && \\\n    cp bin/response_cache_test qa/L0_response_cache/. && \\\n    cp bin/request_cancellation_test qa/L0_request_cancellation/. && \\\n    cp bin/triton_json_test qa/L0_json/. && \\\n    cp bin/backend_output_detail_test qa/L0_backend_output_detail/. && \\\n    cp -r deploy/mlflow-triton-plugin qa/L0_mlflow/. && \\\n    cp bin/input_byte_size_test qa/L0_input_validation/. && \\\n    cp -r docs/examples/model_repository/simple_identity qa/L0_input_validation/models\n\nRUN mkdir -p qa/pkgs && \\\n    cp python/triton*.whl qa/pkgs/. && \\\n    cp -rf python/test/. qa/L0_python_api/.\n\nRUN mkdir -p qa/L0_simple_ensemble/models/simple/1 && \\\n    cp docs/examples/model_repository/simple/1/model.graphdef \\\n        qa/L0_simple_ensemble/models/simple/1/. && \\\n    mkdir -p qa/L0_simple_ensemble/models/simple/2 && \\\n    cp docs/examples/model_repository/simple/1/model.graphdef \\\n        qa/L0_simple_ensemble/models/simple/2/. && \\\n    mkdir -p qa/L0_socket/models/simple/1 && \\\n    cp docs/examples/model_repository/simple/1/model.graphdef \\\n        qa/L0_socket/models/simple/1/.\n\nRUN mkdir -p qa/L0_backend_identity/models && \\\n    cp -r src/test/models/identity_fp32 qa/L0_backend_identity/models/. && \\\n    mkdir -p qa/L0_backend_identity/models/identity_fp32/1\n\nRUN mkdir -p qa/custom_models/custom_sequence_int32/1 && \\\n    cp tritonbuild/tritonserver/backends/sequence/libtriton_sequence.so \\\n        qa/custom_models/custom_sequence_int32/1/. && \\\n    mkdir -p qa/custom_models/custom_dyna_sequence_int32/1 && \\\n    cp tritonbuild/tritonserver/backends/dyna_sequence/libtriton_dyna_sequence.so \\\n        qa/custom_models/custom_dyna_sequence_int32/1/.\n\n# L0_lifecycle needs No-GPU build of identity backend.\nRUN cd tritonbuild/identity && \\\n    rm -rf install build && mkdir build && cd build && \\\n    cmake -DTRITON_ENABLE_GPU=OFF \\\n        -DCMAKE_INSTALL_PREFIX:PATH=/workspace/tritonbuild/identity/install \\\n        -DTRITON_REPO_ORGANIZATION:STRING=${TRITON_REPO_ORGANIZATION} \\\n        -DTRITON_COMMON_REPO_TAG:STRING=${TRITON_COMMON_REPO_TAG} \\\n        -DTRITON_CORE_REPO_TAG:STRING=${TRITON_CORE_REPO_TAG} \\\n        -DTRITON_THIRD_PARTY_REPO_TAG:STRING=${TRITON_THIRD_PARTY_REPO_TAG} \\\n        -DTRITON_BACKEND_REPO_TAG:STRING=${TRITON_BACKEND_REPO_TAG} .. && \\\n    make -j16 install\n\n# L0_backend_python test require triton_shm_monitor\nRUN cd tritonbuild/python && \\\n    rm -rf install build && mkdir build && cd build && \\\n    cmake -DCMAKE_INSTALL_PREFIX:PATH=/workspace/tritonbuild/python/install \\\n        -DTRITON_REPO_ORGANIZATION:STRING=${TRITON_REPO_ORGANIZATION} \\\n        -DTRITON_COMMON_REPO_TAG:STRING=${TRITON_COMMON_REPO_TAG} \\\n        -DTRITON_CORE_REPO_TAG:STRING=${TRITON_CORE_REPO_TAG} \\\n        -DTRITON_BACKEND_REPO_TAG:STRING=${TRITON_BACKEND_REPO_TAG} .. && \\\n    make -j16 triton-shm-monitor install\n\nRUN cp tritonbuild/identity/install/backends/identity/libtriton_identity.so \\\n        qa/L0_lifecycle/. && \\\n    cp tritonbuild/python/install/backends/python/triton_shm_monitor*.so \\\n        qa/common/. && \\\n    mkdir -p qa/L0_perf_nomodel/custom_models/custom_zero_1_float32/1 && \\\n    mkdir -p qa/L0_perf_pyclients/custom_models/custom_zero_1_int32/1 && \\\n    mkdir -p qa/L0_infer_shm && \\\n    cp -r qa/L0_infer/. qa/L0_infer_shm && \\\n    mkdir -p qa/L0_infer_cudashm && \\\n    cp -r qa/L0_infer/. qa/L0_infer_cudashm && \\\n    mkdir -p qa/L0_infer_valgrind && \\\n    cp -r qa/L0_infer/. qa/L0_infer_valgrind && \\\n    mkdir -p qa/L0_trt_shape_tensors_shm && \\\n    cp -r qa/L0_trt_shape_tensors/. qa/L0_trt_shape_tensors_shm && \\\n    mkdir -p qa/L0_trt_shape_tensors_cudashm && \\\n    cp -r qa/L0_trt_shape_tensors/. qa/L0_trt_shape_tensors_cudashm && \\\n    mkdir -p qa/L0_batcher_shm && \\\n    cp -r qa/L0_batcher/. qa/L0_batcher_shm && \\\n    mkdir -p qa/L0_batcher_cudashm && \\\n    cp -r qa/L0_batcher/. qa/L0_batcher_cudashm && \\\n    mkdir -p qa/L0_batcher_valgrind && \\\n    cp -r qa/L0_batcher/. qa/L0_batcher_valgrind && \\\n    mkdir -p qa/L0_sequence_batcher_shm && \\\n    cp -r qa/L0_sequence_batcher/. qa/L0_sequence_batcher_shm && \\\n    mkdir -p qa/L0_sequence_batcher_cudashm && \\\n    cp -r qa/L0_sequence_batcher/. qa/L0_sequence_batcher_cudashm && \\\n    mkdir -p qa/L0_sequence_batcher_valgrind && \\\n    cp -r qa/L0_sequence_batcher/. qa/L0_sequence_batcher_valgrind && \\\n    mkdir -p qa/L0_perf_nomodel_shm && \\\n    cp -r qa/L0_perf_nomodel/. qa/L0_perf_nomodel_shm && \\\n    mkdir -p qa/L0_perf_nomodel_cudashm && \\\n    cp -r qa/L0_perf_nomodel/. qa/L0_perf_nomodel_cudashm\n\n# L0_model_control_stress will not be present if gitlab tests are not available\nRUN if [ -d qa/L0_model_control_stress ]; then \\\n        mkdir -p qa/L0_model_control_stress_valgrind && \\\n            cp -r qa/L0_model_control_stress/. qa/L0_model_control_stress_valgrind && \\\n            mkdir -p qa/L0_model_control_stress_valgrind_massif && \\\n            cp -r qa/L0_model_control_stress/. qa/L0_model_control_stress_valgrind_massif; \\\n    fi\n\nRUN mkdir -p qa/L0_decoupled/models/repeat_int32/1 && \\\n    mkdir -p qa/L0_decoupled/models/square_int32/1 && \\\n    mkdir -p qa/L0_decoupled/models/identity_int32/1 && \\\n    mkdir -p qa/L0_decoupled/models/simple_repeat/1 && \\\n    mkdir -p qa/L0_decoupled/models/fan_repeat/1 && \\\n    mkdir -p qa/L0_decoupled/models/sequence_repeat/1 && \\\n    mkdir -p qa/L0_decoupled/models/repeat_square/1 && \\\n    mkdir -p qa/L0_decoupled/models/nested_square/1 && \\\n    mkdir -p qa/L0_grpc_state_cleanup/models/repeat_int32/1\n\nRUN if [ \"$IGPU_BUILD\" == \"0\" ]; then \\\n        cp backends/repeat/libtriton_repeat.so qa/L0_model_config && \\\n        cp backends/repeat/libtriton_repeat.so qa/L0_decoupled/models/repeat_int32/1 && \\\n        cp backends/repeat/libtriton_repeat.so qa/L0_grpc_state_cleanup/models/repeat_int32/1/. && \\\n        cp backends/square/libtriton_square.so qa/L0_decoupled/models/square_int32/1; \\\n    fi\n\nRUN cp -r qa/L0_decoupled/models qa/L0_decoupled/python_models/ && \\\n    cp /workspace/tritonbuild/python/examples/decoupled/repeat_model.py \\\n        qa/L0_decoupled/python_models/repeat_int32/1/. && \\\n    cp /workspace/tritonbuild/python/examples/decoupled/repeat_config.pbtxt \\\n        qa/L0_decoupled/python_models/repeat_int32/. && \\\n    cp /workspace/tritonbuild/python/examples/decoupled/square_model.py \\\n        qa/L0_decoupled/python_models/square_int32/1/. && \\\n    cp /workspace/tritonbuild/python/examples/decoupled/square_config.pbtxt \\\n        qa/L0_decoupled/python_models/square_int32/.\n\nRUN mkdir -p qa/L0_decoupled_grpc_error && \\\n    cp -r qa/L0_decoupled/. qa/L0_decoupled_grpc_error\n\nRUN mkdir -p qa/L0_grpc_error_state_cleanup && \\\n    cp -r qa/L0_grpc_state_cleanup/. qa/L0_grpc_error_state_cleanup\n\nRUN mkdir -p qa/L0_repoagent_checksum/models/identity_int32/1 && \\\n    cp tritonbuild/identity/install/backends/identity/libtriton_identity.so \\\n        qa/L0_repoagent_checksum/models/identity_int32/1/.\nRUN mkdir -p qa/L0_passive_instance/models/distributed_int32_int32_int32/1 && \\\n    cp tritonbuild/tritonserver/backends/distributed_addsub/libtriton_distributed_addsub.so \\\n        qa/L0_passive_instance/models/distributed_int32_int32_int32/1/.\n\n############################################################################\n## Copy artifacts from sdk container\n############################################################################\nFROM ${SDK_IMAGE} AS sdk\n\nARG TARGETPLATFORM\nWORKDIR /workspace\nCOPY --from=cibase /workspace/qa/ qa/\nRUN mkdir -p qa/clients && mkdir -p qa/pkgs && \\\n    cp -a install/bin/* qa/clients/. && \\\n    cp install/lib/libgrpcclient.so qa/clients/. && \\\n    cp install/lib/libhttpclient.so qa/clients/. && \\\n    cp install/python/*.py qa/clients/. && \\\n    cp install/python/triton*.whl qa/pkgs/. && \\\n    cp install/java/examples/*.jar qa/clients/.\nRUN cp client/src/grpc_generated/go/*.go qa/L0_simple_go_client/. && \\\n    cp client/src/grpc_generated/javascript/*.js qa/L0_simple_nodejs_client/. && \\\n    cp client/src/grpc_generated/javascript/*.json qa/L0_simple_nodejs_client/. && \\\n    cp -r client/src/grpc_generated/java qa/L0_client_java/.\n\n############################################################################\n## Create CI enabled image\n############################################################################\nFROM $BASE_IMAGE\n\nARG TARGETPLATFORM\n\n# Ensure apt-get won't prompt for selecting options\nENV DEBIAN_FRONTEND=noninteractive\n\n# install platform specific packages\nRUN if grep -qE '^VERSION_ID=\"(18\\.04|20\\.04|22\\.04|24\\.04)' /etc/os-release; then \\\n        apt-get update && \\\n        apt-get install -y --no-install-recommends \\\n                libpng-dev; \\\n    else \\\n        echo \"Ubuntu version must be either 18.04, 20.04, 22.04 or 24.04\" && \\\n        exit 1; \\\n    fi\n\n# CI/QA for memcheck requires valgrind\n# libarchive-dev is required by Python backend\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n                              curl \\\n                              gdb \\\n                              libopencv-dev \\\n                              libarchive-dev \\\n                              libopencv-core-dev \\\n                              libzmq3-dev \\\n                              openjdk-11-jdk \\\n                              nginx \\\n                              npm \\\n                              protobuf-compiler \\\n                              python3-dev \\\n                              python3-pip \\\n                              python3-protobuf \\\n                              python3-wheel \\\n                              python3-setuptools \\\n                              swig \\\n                              valgrind && \\\n    rm -rf /var/lib/apt/lists/*\n\n# CI/QA expects \"python\" executable (not python3).\nRUN rm -f /usr/bin/python && \\\n    ln -s /usr/bin/python3 /usr/bin/python\n\nRUN pip3 install --upgrade \"numpy<2\" pillow attrdict future \"grpcio<1.68\" requests gsutil \\\n                           awscli six \"grpcio-channelz<1.68\" prettytable virtualenv \\\n                           check-jsonschema\n\n# go needed for example go client test.\nRUN if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n      wget https://golang.org/dl/go1.22.3.linux-arm64.tar.gz && \\\n      rm -rf /usr/local/go && tar -C /usr/local -xzf go1.22.3.linux-arm64.tar.gz && \\\n      rm -f go1.22.3.linux-arm64.tar.gz; \\\n    else \\\n      wget https://golang.org/dl/go1.22.3.linux-amd64.tar.gz && \\\n      rm -rf /usr/local/go && tar -C /usr/local -xzf go1.22.3.linux-amd64.tar.gz && \\\n      rm -f go1.22.3.linux-amd64.tar.gz; \\\n    fi\nENV GOPATH /root/go\nENV PATH $PATH:/usr/local/go/bin:$GOPATH/bin\nRUN go install google.golang.org/protobuf/cmd/protoc-gen-go@latest && \\\n    go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest\n# CI expects tests in /opt/tritonserver/qa. The triton-server (1000)\n# user should own all artifacts in case CI is run using triton-server\n# user.\nWORKDIR /opt/tritonserver\nCOPY --chown=1000:1000 --from=sdk /workspace/qa/ qa/\n\n# Remove CI tests that are meant to run only on build image and\n# install the tritonserver/triton python client APIs.\nRUN rm -fr qa/L0_copyrights qa/L0_build_variants && \\\n    find qa/pkgs/ -maxdepth 1 -type f -name \\\n    \"tritonclient-*linux*.whl\" | xargs printf -- '%s[all]' | \\\n    xargs pip3 install --upgrade\n\nENV LD_LIBRARY_PATH /opt/tritonserver/qa/clients:${LD_LIBRARY_PATH}\n\n# DLIS-3631: Needed to run Perf Analyzer CI tests correctly\nENV LD_LIBRARY_PATH /opt/hpcx/ompi/lib:${LD_LIBRARY_PATH}\n\n# Required for PyTorch to pickup the correct HPCX libraries\nENV LD_LIBRARY_PATH /opt/hpcx/ucc/lib/:/opt/hpcx/ucx/lib/:${LD_LIBRARY_PATH}\n"
        },
        {
          "name": "Dockerfile.sdk",
          "type": "blob",
          "size": 11.173828125,
          "content": "# Copyright 2019-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#\n# Multistage build.\n#\n\n# Base image on the minimum Triton container\nARG BASE_IMAGE=nvcr.io/nvidia/tritonserver:24.12-py3-min\n\nARG TRITON_CLIENT_REPO_SUBDIR=clientrepo\nARG TRITON_PA_REPO_SUBDIR=perfanalyzerrepo\nARG TRITON_REPO_ORGANIZATION=http://github.com/triton-inference-server\nARG TRITON_COMMON_REPO_TAG=main\nARG TRITON_CORE_REPO_TAG=main\nARG TRITON_CLIENT_REPO_TAG=main\nARG TRITON_THIRD_PARTY_REPO_TAG=main\nARG TRITON_MODEL_ANALYZER_REPO_TAG=main\nARG TRITON_ENABLE_GPU=ON\nARG JAVA_BINDINGS_MAVEN_VERSION=3.8.4\nARG JAVA_BINDINGS_JAVACPP_PRESETS_TAG=1.5.8\n\n# DCGM version to install for Model Analyzer\nARG DCGM_VERSION=3.3.6\n\nARG NVIDIA_TRITON_SERVER_SDK_VERSION=unknown\nARG NVIDIA_BUILD_ID=unknown\n\n############################################################################\n##  Build image\n############################################################################\n\nFROM ${BASE_IMAGE} AS sdk_build\n\n# Ensure apt-get won't prompt for selecting options\nENV DEBIAN_FRONTEND=noninteractive\nENV PIP_BREAK_SYSTEM_PACKAGES=1\n\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n            ca-certificates \\\n            software-properties-common \\\n            autoconf \\\n            automake \\\n            build-essential \\\n            curl \\\n            git \\\n            gperf \\\n            libb64-dev \\\n            libgoogle-perftools-dev \\\n            libopencv-dev \\\n            libopencv-core-dev \\\n            libssl-dev \\\n            libtool \\\n            pkg-config \\\n            python3 \\\n            python3-pip \\\n            python3-dev \\\n            python3-wheel \\\n            python3-setuptools \\\n            rapidjson-dev \\\n            vim \\\n            wget \\\n            python3-pdfkit \\\n            openjdk-11-jdk \\\n            maven && \\\n    pip3 install --upgrade \"grpcio-tools<1.68\"\n\n# Client build requires recent version of CMake (FetchContent required)\n# Using CMAKE installation instruction from:: https://apt.kitware.com/\nRUN apt update -q=2 \\\n    && apt install -y gpg wget \\\n    && wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - |  tee /usr/share/keyrings/kitware-archive-keyring.gpg >/dev/null \\\n    && . /etc/os-release \\\n    && echo \"deb [signed-by=/usr/share/keyrings/kitware-archive-keyring.gpg] https://apt.kitware.com/ubuntu/ $UBUNTU_CODENAME main\" | tee /etc/apt/sources.list.d/kitware.list >/dev/null \\\n    && apt-get update -q=2 \\\n    && apt-get install -y --no-install-recommends cmake=3.28.3* cmake-data=3.28.3* \\\n    && cmake --version\n\n# Build expects \"python\" executable (not python3).\nRUN rm -f /usr/bin/python && \\\n    ln -s /usr/bin/python3 /usr/bin/python\n\n# Build the client library and examples\nARG TRITON_REPO_ORGANIZATION\nARG TRITON_CLIENT_REPO_SUBDIR\nARG TRITON_PA_REPO_SUBDIR\nARG TRITON_COMMON_REPO_TAG\nARG TRITON_CORE_REPO_TAG\nARG TRITON_CLIENT_REPO_TAG\nARG TRITON_THIRD_PARTY_REPO_TAG\nARG TRITON_ENABLE_GPU\nARG JAVA_BINDINGS_MAVEN_VERSION\nARG JAVA_BINDINGS_JAVACPP_PRESETS_TAG\nARG TARGETPLATFORM\n\nWORKDIR /workspace\nCOPY TRITON_VERSION .\nCOPY ${TRITON_CLIENT_REPO_SUBDIR} client\nCOPY ${TRITON_PA_REPO_SUBDIR} perf_analyzer\n\nWORKDIR /workspace/client_build\nRUN cmake -DCMAKE_INSTALL_PREFIX=/workspace/install \\\n          -DTRITON_VERSION=`cat /workspace/TRITON_VERSION` \\\n          -DTRITON_REPO_ORGANIZATION=${TRITON_REPO_ORGANIZATION} \\\n          -DTRITON_COMMON_REPO_TAG=${TRITON_COMMON_REPO_TAG} \\\n          -DTRITON_CORE_REPO_TAG=${TRITON_CORE_REPO_TAG} \\\n          -DTRITON_THIRD_PARTY_REPO_TAG=${TRITON_THIRD_PARTY_REPO_TAG} \\\n          -DTRITON_ENABLE_PERF_ANALYZER=OFF \\\n          -DTRITON_ENABLE_CC_HTTP=ON -DTRITON_ENABLE_CC_GRPC=ON \\\n          -DTRITON_ENABLE_PYTHON_HTTP=OFF -DTRITON_ENABLE_PYTHON_GRPC=OFF \\\n          -DTRITON_ENABLE_JAVA_HTTP=ON \\\n          -DTRITON_ENABLE_EXAMPLES=ON -DTRITON_ENABLE_TESTS=ON \\\n          -DTRITON_ENABLE_GPU=${TRITON_ENABLE_GPU} /workspace/client\nRUN make -j16 cc-clients java-clients && \\\n    rm -fr ~/.m2\n\n# TODO: PA will rebuild the CC clients since it depends on it.\n# This should be optimized so that we do not have to build\n# the CC clients twice. Similarly, because the SDK expectation is\n# that PA is packaged with the python client, we hold off on building\n# the python client until now. Post-migration we should focus\n# effort on de-tangling these flows.\nWORKDIR /workspace/pa_build\nRUN cmake -DCMAKE_INSTALL_PREFIX=/workspace/install \\\n          -DTRITON_VERSION=`cat /workspace/TRITON_VERSION` \\\n          -DTRITON_REPO_ORGANIZATION=${TRITON_REPO_ORGANIZATION} \\\n          -DTRITON_COMMON_REPO_TAG=${TRITON_COMMON_REPO_TAG} \\\n          -DTRITON_CORE_REPO_TAG=${TRITON_CORE_REPO_TAG} \\\n          -DTRITON_CLIENT_REPO_TAG=${TRITON_CLIENT_REPO_TAG} \\\n          -DTRITON_ENABLE_PERF_ANALYZER_C_API=ON \\\n          -DTRITON_ENABLE_PERF_ANALYZER_TFS=ON \\\n          -DTRITON_ENABLE_PERF_ANALYZER_TS=ON \\\n          -DTRITON_ENABLE_PERF_ANALYZER_OPENAI=ON \\\n          -DTRITON_ENABLE_CC_HTTP=ON \\\n          -DTRITON_ENABLE_CC_GRPC=ON \\\n          -DTRITON_ENABLE_PYTHON_HTTP=ON \\\n          -DTRITON_ENABLE_PYTHON_GRPC=ON \\\n          -DTRITON_PACKAGE_PERF_ANALYZER=ON \\\n          -DTRITON_ENABLE_GPU=${TRITON_ENABLE_GPU} \\\n          /workspace/perf_analyzer\nRUN make -j16 perf-analyzer python-clients\n\nRUN pip3 install build \\\n    && cd /workspace/perf_analyzer/genai-perf \\\n    && python3 -m build --wheel --outdir /workspace/install/python\n\n# Install Java API Bindings\nRUN if [ \"$TARGETPLATFORM\" = \"linux/amd64\" ]; then \\\n        source /workspace/client/src/java-api-bindings/scripts/install_dependencies_and_build.sh \\\n        --maven-version ${JAVA_BINDINGS_MAVEN_VERSION} \\\n        --core-tag ${TRITON_CORE_REPO_TAG} \\\n        --javacpp-tag ${JAVA_BINDINGS_JAVACPP_PRESETS_TAG} \\\n        --jar-install-path /workspace/install/java-api-bindings; \\\n    fi\n\n############################################################################\n## Create sdk container\n############################################################################\nFROM ${BASE_IMAGE}\n\n# Ensure apt-get won't prompt for selecting options\nENV DEBIAN_FRONTEND=noninteractive\nENV PIP_BREAK_SYSTEM_PACKAGES=1\n\nARG DCGM_VERSION\nARG TRITON_REPO_ORGANIZATION\nARG TRITON_CORE_REPO_TAG\nARG TARGETPLATFORM\nARG TRITON_ENABLE_GPU\n\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends \\\n            software-properties-common \\\n            curl \\\n            git \\\n            gperf \\\n            libb64-dev \\\n            libgoogle-perftools-dev \\\n            libopencv-dev \\\n            libopencv-core-dev \\\n            libssl-dev \\\n            libtool \\\n            python3 \\\n            python3-pip \\\n            python3-dev \\\n            python3-wheel \\\n            python3-setuptools \\\n            vim \\\n            wget \\\n            python3-pdfkit \\\n            maven \\\n            default-jdk && \\\n    pip3 install \"grpcio<1.68\" \"grpcio-tools<1.68\"\n\nWORKDIR /workspace\nCOPY TRITON_VERSION .\nCOPY NVIDIA_Deep_Learning_Container_License.pdf .\nCOPY --from=sdk_build /workspace/client/ client/\nCOPY --from=sdk_build /workspace/perf_analyzer/ perf_analyzer/\nCOPY --from=sdk_build /workspace/install/ install/\nRUN cd install && \\\n    export VERSION=`cat /workspace/TRITON_VERSION` && \\\n    tar zcf /workspace/v$VERSION.clients.tar.gz *\n\n# For CI testing need to copy over L0_sdk test and L0_client_build_variants test.\nRUN mkdir qa\nCOPY qa/L0_sdk qa/L0_sdk\nCOPY qa/L0_client_build_variants qa/L0_client_build_variants\n\n# Create a directory for all the python client tests to enable unit testing\nRUN mkdir -p qa/python_client_unit_tests/\nCOPY --from=sdk_build /workspace/client/src/python/library/tests/* qa/python_client_unit_tests/\n\n# Install an image needed by the quickstart and other documentation.\nCOPY qa/images/mug.jpg images/mug.jpg\n\n# Install the dependencies needed to run the client examples. These\n# are not needed for building but including them allows this image to\n# be used to run the client examples.\nRUN pip3 install --upgrade \"numpy<2\" pillow attrdict && \\\n    find install/python/ -maxdepth 1 -type f -name \\\n         \"tritonclient-*linux*.whl\" | xargs printf -- '%s[all]' | \\\n    xargs pip3 install --upgrade\n\nRUN pip3 install install/python/genai_perf-*.whl\n\n# Install DCGM\nRUN if [ \"$TRITON_ENABLE_GPU\" = \"ON\" ]; then \\\n        [ \"$(uname -m)\" != \"x86_64\" ] && arch=\"sbsa\" || arch=\"x86_64\" && \\\n        curl -o /tmp/cuda-keyring.deb \\\n        https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/$arch/cuda-keyring_1.1-1_all.deb \\\n        && apt install /tmp/cuda-keyring.deb && rm /tmp/cuda-keyring.deb && \\\n        apt-get update && apt-get install -y datacenter-gpu-manager=1:${DCGM_VERSION}; \\\n    fi\n\n# Build expects \"python\" executable (not python3).\nRUN rm -f /usr/bin/python && \\\n    ln -s /usr/bin/python3 /usr/bin/python\n\n# Install Model Analyzer\nARG TRITON_MODEL_ANALYZER_REPO_TAG\nARG TRITON_MODEL_ANALYZER_REPO=\"${TRITON_REPO_ORGANIZATION}/model_analyzer@${TRITON_MODEL_ANALYZER_REPO_TAG}\"\nRUN pip3 install \"git+${TRITON_MODEL_ANALYZER_REPO}\"\n\n# Entrypoint Banner\nENV NVIDIA_PRODUCT_NAME=\"Triton Server SDK\"\nCOPY docker/entrypoint.d/ /opt/nvidia/entrypoint.d/\nRUN sed 's/Server/Server SDK/' /opt/nvidia/entrypoint.d/10-banner.txt | \\\n    sed 's/^===/=======/' > /opt/nvidia/entrypoint.d/10-banner.new && \\\n    mv /opt/nvidia/entrypoint.d/10-banner.new /opt/nvidia/entrypoint.d/10-banner.txt\n\nARG NVIDIA_TRITON_SERVER_SDK_VERSION\nARG NVIDIA_BUILD_ID\nENV NVIDIA_TRITON_SERVER_SDK_VERSION=${NVIDIA_TRITON_SERVER_SDK_VERSION}\nENV NVIDIA_BUILD_ID=${NVIDIA_BUILD_ID}\n\nENV PATH=/workspace/install/bin:${PATH}\nENV LD_LIBRARY_PATH=/workspace/install/lib:${LD_LIBRARY_PATH}\n\n# DLIS-3631: Needed to run Perf Analyzer CI tests correctly\nENV LD_LIBRARY_PATH=/opt/hpcx/ompi/lib:${LD_LIBRARY_PATH}\n\n# Set TCMALLOC_RELEASE_RATE for users setting LD_PRELOAD with tcmalloc\nENV TCMALLOC_RELEASE_RATE=200\n"
        },
        {
          "name": "Dockerfile.win10.min",
          "type": "blob",
          "size": 8.5791015625,
          "content": "# Copyright 2021-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n# Windows min container for Triton build\n\nARG BASE_IMAGE=mcr.microsoft.com/windows:10.0.19042.1889\n\nFROM ${BASE_IMAGE} as dependency_base\n\nRUN powershell.exe Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope LocalMachine\nRUN powershell.exe [Net.ServicePointManager]::Expect100Continue=$true;[Net.ServicePointManager]::SecurityProtocol=[Net.SecurityProtocolType]::Tls,[Net.SecurityProtocolType]::Tls11,[Net.SecurityProtocolType]::Tls12,[Net.SecurityProtocolType]::Ssl3;Invoke-Expression( New-Object System.Net.WebClient ).DownloadString('https://chocolatey.org/install.ps1')\nRUN choco install unzip -y\n\n#\n# Installing TensorRT\n#\nARG TENSORRT_VERSION=10.7.0.23\nARG TENSORRT_ZIP=\"TensorRT-${TENSORRT_VERSION}.Windows.win10.cuda-12.6.zip\"\nARG TENSORRT_SOURCE=https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.7.0/zip/TensorRT-10.7.0.23.Windows.win10.cuda-12.6.zip\n# COPY ${TENSORRT_ZIP} /tmp/${TENSORRT_ZIP}\nADD ${TENSORRT_SOURCE} /tmp/${TENSORRT_ZIP}\nRUN unzip /tmp/%TENSORRT_ZIP%\nRUN move TensorRT-* TensorRT\n\nLABEL TENSORRT_VERSION=\"${TENSORRT_VERSION}\"\n\n\n#\n# Installing cuDNN\n#\nARG CUDNN_VERSION=9.6.0.74\nARG CUDNN_ZIP=cudnn-windows-x86_64-${CUDNN_VERSION}_cuda12-archive.zip\nARG CUDNN_SOURCE=https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/windows-x86_64/cudnn-windows-x86_64-9.6.0.74_cuda12-archive.zip\nADD ${CUDNN_SOURCE} /tmp/${CUDNN_ZIP}\nRUN unzip /tmp/%CUDNN_ZIP%\nRUN move cudnn-* cudnn\n\nLABEL CUDNN_VERSION=\"${CUDNN_VERSION}\"\n\n\nFROM ${BASE_IMAGE} as build_base\n\nSHELL [\"cmd\", \"/S\", \"/C\"]\n\nRUN mkdir c:\\tmp\nWORKDIR /tmp\n\nRUN powershell.exe Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope LocalMachine\nRUN powershell.exe [Net.ServicePointManager]::Expect100Continue=$true;[Net.ServicePointManager]::SecurityProtocol=[Net.SecurityProtocolType]::Tls,[Net.SecurityProtocolType]::Tls11,[Net.SecurityProtocolType]::Tls12,[Net.SecurityProtocolType]::Ssl3;Invoke-Expression( New-Object System.Net.WebClient ).DownloadString('https://chocolatey.org/install.ps1')\nRUN choco install git docker unzip -y\n\n#\n# Installing python\n#\nARG PYTHON_VERSION=3.12.3\nARG PYTHON_SOURCE=https://www.python.org/ftp/python/${PYTHON_VERSION}/python-${PYTHON_VERSION}-amd64.exe\nADD ${PYTHON_SOURCE} python-${PYTHON_VERSION}-amd64.exe\nRUN python-%PYTHON_VERSION%-amd64.exe /quiet InstallAllUsers=1 PrependPath=1 Include_doc=0 TargetDir=\"C:\\python%PYTHON_VERSION%\"\nRUN mklink \"C:\\python%PYTHON_VERSION%\\python3.exe\" \"C:\\python%PYTHON_VERSION%\\python.exe\"\nRUN pip install --upgrade wheel setuptools docker\n\nLABEL PYTHON_VERSION=${PYTHON_VERSION}\n\n#\n# Installing CMake\n#\nARG CMAKE_VERSION=3.30.5\nRUN pip install cmake==%CMAKE_VERSION%\n\nENV CMAKE_TOOLCHAIN_FILE /vcpkg/scripts/buildsystems/vcpkg.cmake\nENV VCPKG_TARGET_TRIPLET x64-windows\n\nLABEL CMAKE_VERSION=${CMAKE_VERSION}\n\n# Be aware that pip can interact badly with VS cmd shell so need to pip install before\n# vsdevcmd.bat (see https://bugs.python.org/issue38989)\n#\n# Installing Visual Studio BuildTools: VS17 2022\n#\n# Download collect.exe in case of an install failure.\nADD https://aka.ms/vscollect.exe \"C:\\tmp\\collect.exe\"\n\n# Use the latest release channel. For more control, specify the location of an internal layout.\n# Download the Build Tools bootstrapper.\n# ARG BUILD_TOOLS_SOURCE=https://aka.ms/vs/17/release/vs_buildtools.exe\n\nARG BUILDTOOLS_VERSION=17.12.35506.116\nARG BUILD_TOOLS_SOURCE=https://download.visualstudio.microsoft.com/download/pr/5536698c-711c-4834-876f-2817d31a2ef2/58894fc272e86d3c3a6d85bf3a1df1e5a0685be8b9ab65d9f3cc5c2a8c6921cc/vs_BuildTools.exe\n\nADD ${BUILD_TOOLS_SOURCE} vs_buildtools.exe\n# Install Build Tools with the Microsoft.VisualStudio.Workload.VCTools workload, including recommended.\nARG VS_INSTALL_PATH_WP=\"C:\\BuildTools\"\nRUN vs_buildtools.exe --quiet --wait --norestart --nocache install \\\n      --installPath %VS_INSTALL_PATH_WP% \\\n      --add Microsoft.VisualStudio.Workload.VCTools \\\n      --includeRecommended \\\n      --locale \"En-us\"\n\nLABEL BUILDTOOLS_VERSION=${BUILDTOOLS_VERSION}\n\nWORKDIR /\n\n#\n# Installing Vcpkg\n#\nARG VCPGK_VERSION=2024.03.19\nRUN git clone --single-branch --depth=1 -b %VCPGK_VERSION% https://github.com/microsoft/vcpkg.git\nWORKDIR /vcpkg\nRUN bootstrap-vcpkg.bat\nRUN vcpkg.exe update\nRUN vcpkg.exe install \\\n      boost-interprocess:x64-windows \\\n      boost-stacktrace:x64-windows \\\n      b64:x64-windows \\\n      openssl-windows:x64-windows \\\n      openssl:x64-windows \\\n      pthread:x64-windows \\\n      rapidjson:x64-windows \\\n      zlib:x64-windows\nRUN vcpkg.exe integrate install\n\nLABEL VCPGK_VERSION=${VCPGK_VERSION}\n\nWORKDIR /\n\n#\n# Installing CUDA\n#\nARG CUDA_MAJOR=12\nARG CUDA_MINOR=6\nARG CUDA_PATCH=3\nARG CUDA_VERSION=${CUDA_MAJOR}.${CUDA_MINOR}.${CUDA_PATCH}\nARG CUDA_PACKAGES=\"nvcc_${CUDA_MAJOR}.${CUDA_MINOR} \\\n                   cudart_${CUDA_MAJOR}.${CUDA_MINOR} \\\n                   nvml_dev_${CUDA_MAJOR}.${CUDA_MINOR} \\\n                   nvrtc_${CUDA_MAJOR}.${CUDA_MINOR} nvrtc_dev_${CUDA_MAJOR}.${CUDA_MINOR} \\\n                   cublas_${CUDA_MAJOR}.${CUDA_MINOR} cublas_dev_${CUDA_MAJOR}.${CUDA_MINOR} \\\n                   cufft_${CUDA_MAJOR}.${CUDA_MINOR} cufft_dev_${CUDA_MAJOR}.${CUDA_MINOR} \\\n                   curand_${CUDA_MAJOR}.${CUDA_MINOR} curand_dev_${CUDA_MAJOR}.${CUDA_MINOR} \\\n                   cusolver_${CUDA_MAJOR}.${CUDA_MINOR} cusolver_dev_${CUDA_MAJOR}.${CUDA_MINOR} \\\n                   cusparse_${CUDA_MAJOR}.${CUDA_MINOR} cusparse_dev_${CUDA_MAJOR}.${CUDA_MINOR} \\\n                   cupti_${CUDA_MAJOR}.${CUDA_MINOR} \\\n                   thrust_${CUDA_MAJOR}.${CUDA_MINOR} \\\n                   visual_studio_integration_${CUDA_MAJOR}.${CUDA_MINOR}\"\nARG CUDA_INSTALL_ROOT_WP=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v${CUDA_MAJOR}.${CUDA_MINOR}\"\n\nARG CUDA_SOURCE=https://developer.download.nvidia.com/compute/cuda/${CUDA_VERSION}/network_installers/cuda_${CUDA_VERSION}_windows_network.exe\nADD ${CUDA_SOURCE} cuda_${CUDA_VERSION}_windows_network.exe\n\nRUN cuda_%CUDA_VERSION%_windows_network.exe -s %CUDA_PACKAGES%\n# Copy the CUDA visualstudio integration from where it was installed\n# into the appropriate place in BuildTools\nRUN copy \"%CUDA_INSTALL_ROOT_WP%\\extras\\visual_studio_integration\\MSBuildExtensions\\*\" \"%VS_INSTALL_PATH_WP%\\MSBuild\\Microsoft\\VC\\v170\\BuildCustomizations\"\n\nRUN setx PATH \"%CUDA_INSTALL_ROOT_WP%\\bin;%PATH%\"\n\nENV CUDA_VERSION=${CUDA_VERSION}\nLABEL CUDA_VERSION=\"${CUDA_VERSION}\"\n\nARG CUDNN_VERSION=9.6.0.74\nENV CUDNN_VERSION ${CUDNN_VERSION}\nCOPY --from=dependency_base /cudnn /cudnn\nRUN copy cudnn\\bin\\cudnn*.dll \"%CUDA_INSTALL_ROOT_WP%\\bin\\.\"\nRUN copy cudnn\\lib\\x64\\cudnn*.lib \"%CUDA_INSTALL_ROOT_WP%\\lib\\x64\\.\"\nRUN copy cudnn\\include\\cudnn*.h \"%CUDA_INSTALL_ROOT_WP%\\include\\.\"\nLABEL CUDNN_VERSION=\"${CUDNN_VERSION}\"\n\nARG TENSORRT_VERSION=10.7.0.23\nENV TRT_VERSION ${TENSORRT_VERSION}\nCOPY --from=dependency_base /TensorRT /TensorRT\nRUN setx PATH \"c:\\TensorRT\\lib;%PATH%\"\nLABEL TENSORRT_VERSION=\"${TENSORRT_VERSION}\"\n\n# It is important that the entrypoint initialize VisualStudio\n# environment otherwise the build will fail. Also set\n# CMAKE_TOOLCHAIN_FILE and VCPKG_TARGET_TRIPLET so\n# that cmake can find the packages installed by vcpkg.\nENTRYPOINT C:\\BuildTools\\VC\\Auxiliary\\Build\\vcvars64.bat &&\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.4501953125,
          "content": "Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n * Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n * Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n * Neither the name of NVIDIA CORPORATION nor the names of its\n   contributors may be used to endorse or promote products derived\n   from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\nEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\nOF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "NVIDIA_Deep_Learning_Container_License.pdf",
          "type": "blob",
          "size": 2942.03125,
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.611328125,
          "content": "<!--\n# Copyright 2018-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n-->\n\n# Triton Inference Server\n\n[![License](https://img.shields.io/badge/License-BSD3-lightgrey.svg)](https://opensource.org/licenses/BSD-3-Clause)\n\n>[!WARNING]\n>You are currently on the `main` branch which tracks under-development progress\n>towards the next release. The current release is version [2.53.0](https://github.com/triton-inference-server/server/releases/latest)\n>and corresponds to the 24.12 container release on NVIDIA GPU Cloud (NGC).\n\nTriton Inference Server is an open source inference serving software that\nstreamlines AI inferencing. Triton enables teams to deploy any AI model from\nmultiple deep learning and machine learning frameworks, including TensorRT,\nTensorFlow, PyTorch, ONNX, OpenVINO, Python, RAPIDS FIL, and more. Triton\nInference Server supports inference across cloud, data center, edge and embedded\ndevices on NVIDIA GPUs, x86 and ARM CPU, or AWS Inferentia. Triton Inference\nServer delivers optimized performance for many query types, including real time,\nbatched, ensembles and audio/video streaming. Triton inference Server is part of\n[NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/),\na software platform that accelerates the data science pipeline and streamlines\nthe development and deployment of production AI.\n\nMajor features include:\n\n- [Supports multiple deep learning\n  frameworks](https://github.com/triton-inference-server/backend#where-can-i-find-all-the-backends-that-are-available-for-triton)\n- [Supports multiple machine learning\n  frameworks](https://github.com/triton-inference-server/fil_backend)\n- [Concurrent model\n  execution](docs/user_guide/architecture.md#concurrent-model-execution)\n- [Dynamic batching](docs/user_guide/model_configuration.md#dynamic-batcher)\n- [Sequence batching](docs/user_guide/model_configuration.md#sequence-batcher) and\n  [implicit state management](docs/user_guide/architecture.md#implicit-state-management)\n  for stateful models\n- Provides [Backend API](https://github.com/triton-inference-server/backend) that\n  allows adding custom backends and pre/post processing operations\n- Supports writing custom backends in python, a.k.a.\n  [Python-based backends.](https://github.com/triton-inference-server/backend/blob/main/docs/python_based_backends.md#python-based-backends)\n- Model pipelines using\n  [Ensembling](docs/user_guide/architecture.md#ensemble-models) or [Business\n  Logic Scripting\n  (BLS)](https://github.com/triton-inference-server/python_backend#business-logic-scripting)\n- [HTTP/REST and GRPC inference\n  protocols](docs/customization_guide/inference_protocols.md) based on the community\n  developed [KServe\n  protocol](https://github.com/kserve/kserve/tree/master/docs/predict-api/v2)\n- A [C API](docs/customization_guide/inference_protocols.md#in-process-triton-server-api) and\n  [Java API](docs/customization_guide/inference_protocols.md#java-bindings-for-in-process-triton-server-api)\n  allow Triton to link directly into your application for edge and other in-process use cases\n- [Metrics](docs/user_guide/metrics.md) indicating GPU utilization, server\n  throughput, server latency, and more\n\n**New to Triton Inference Server?** Make use of\n[these tutorials](https://github.com/triton-inference-server/tutorials)\nto begin your Triton journey!\n\nJoin the [Triton and TensorRT community](https://www.nvidia.com/en-us/deep-learning-ai/triton-tensorrt-newsletter/) and\nstay current on the latest product updates, bug fixes, content, best practices,\nand more.  Need enterprise support?  NVIDIA global support is available for Triton\nInference Server with the\n[NVIDIA AI Enterprise software suite](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/).\n\n## Serve a Model in 3 Easy Steps\n\n```bash\n# Step 1: Create the example model repository\ngit clone -b r24.12 https://github.com/triton-inference-server/server.git\ncd server/docs/examples\n./fetch_models.sh\n\n# Step 2: Launch triton from the NGC Triton container\ndocker run --gpus=1 --rm --net=host -v ${PWD}/model_repository:/models nvcr.io/nvidia/tritonserver:24.12-py3 tritonserver --model-repository=/models\n\n# Step 3: Sending an Inference Request\n# In a separate console, launch the image_client example from the NGC Triton SDK container\ndocker run -it --rm --net=host nvcr.io/nvidia/tritonserver:24.12-py3-sdk\n/workspace/install/bin/image_client -m densenet_onnx -c 3 -s INCEPTION /workspace/images/mug.jpg\n\n# Inference should return the following\nImage '/workspace/images/mug.jpg':\n    15.346230 (504) = COFFEE MUG\n    13.224326 (968) = CUP\n    10.422965 (505) = COFFEEPOT\n```\nPlease read the [QuickStart](docs/getting_started/quickstart.md) guide for additional information\nregarding this example. The quickstart guide also contains an example of how to launch Triton on [CPU-only systems](docs/getting_started/quickstart.md#run-on-cpu-only-system). New to Triton and wondering where to get started? Watch the [Getting Started video](https://youtu.be/NQDtfSi5QF4).\n\n## Examples and Tutorials\n\nCheck out [NVIDIA LaunchPad](https://www.nvidia.com/en-us/data-center/products/ai-enterprise-suite/trial/)\nfor free access to a set of hands-on labs with Triton Inference Server hosted on\nNVIDIA infrastructure.\n\nSpecific end-to-end examples for popular models, such as ResNet, BERT, and DLRM\nare located in the\n[NVIDIA Deep Learning Examples](https://github.com/NVIDIA/DeepLearningExamples)\npage on GitHub. The\n[NVIDIA Developer Zone](https://developer.nvidia.com/nvidia-triton-inference-server)\ncontains additional documentation, presentations, and examples.\n\n## Documentation\n\n### Build and Deploy\n\nThe recommended way to build and use Triton Inference Server is with Docker\nimages.\n\n- [Install Triton Inference Server with Docker containers](docs/customization_guide/build.md#building-with-docker) (*Recommended*)\n- [Install Triton Inference Server without Docker containers](docs/customization_guide/build.md#building-without-docker)\n- [Build a custom Triton Inference Server Docker container](docs/customization_guide/compose.md)\n- [Build Triton Inference Server from source](docs/customization_guide/build.md#building-on-unsupported-platforms)\n- [Build Triton Inference Server for Windows 10](docs/customization_guide/build.md#building-for-windows-10)\n- Examples for deploying Triton Inference Server with Kubernetes and Helm on [GCP](deploy/gcp/README.md),\n  [AWS](deploy/aws/README.md), and [NVIDIA FleetCommand](deploy/fleetcommand/README.md)\n- [Secure Deployment Considerations](docs/customization_guide/deploy.md)\n\n### Using Triton\n\n#### Preparing Models for Triton Inference Server\n\nThe first step in using Triton to serve your models is to place one or\nmore models into a [model repository](docs/user_guide/model_repository.md). Depending on\nthe type of the model and on what Triton capabilities you want to enable for\nthe model, you may need to create a [model\nconfiguration](docs/user_guide/model_configuration.md) for the model.\n\n- [Add custom operations to Triton if needed by your model](docs/user_guide/custom_operations.md)\n- Enable model pipelining with [Model Ensemble](docs/user_guide/architecture.md#ensemble-models)\n  and [Business Logic Scripting (BLS)](https://github.com/triton-inference-server/python_backend#business-logic-scripting)\n- Optimize your models setting [scheduling and batching](docs/user_guide/architecture.md#models-and-schedulers)\n  parameters and [model instances](docs/user_guide/model_configuration.md#instance-groups).\n- Use the [Model Analyzer tool](https://github.com/triton-inference-server/model_analyzer)\n  to help optimize your model configuration with profiling\n- Learn how to [explicitly manage what models are available by loading and\n  unloading models](docs/user_guide/model_management.md)\n\n#### Configure and Use Triton Inference Server\n\n- Read the [Quick Start Guide](docs/getting_started/quickstart.md) to run Triton Inference\n  Server on both GPU and CPU\n- Triton supports multiple execution engines, called\n  [backends](https://github.com/triton-inference-server/backend#where-can-i-find-all-the-backends-that-are-available-for-triton), including\n  [TensorRT](https://github.com/triton-inference-server/tensorrt_backend),\n  [TensorFlow](https://github.com/triton-inference-server/tensorflow_backend),\n  [PyTorch](https://github.com/triton-inference-server/pytorch_backend),\n  [ONNX](https://github.com/triton-inference-server/onnxruntime_backend),\n  [OpenVINO](https://github.com/triton-inference-server/openvino_backend),\n  [Python](https://github.com/triton-inference-server/python_backend), and more\n- Not all the above backends are supported on every platform supported by Triton.\n  Look at the\n  [Backend-Platform Support Matrix](https://github.com/triton-inference-server/backend/blob/main/docs/backend_platform_support_matrix.md)\n  to learn which backends are supported on your target platform.\n- Learn how to [optimize performance](docs/user_guide/optimization.md) using the\n  [Performance Analyzer](https://github.com/triton-inference-server/perf_analyzer/blob/main/README.md)\n  and\n  [Model Analyzer](https://github.com/triton-inference-server/model_analyzer)\n- Learn how to [manage loading and unloading models](docs/user_guide/model_management.md) in\n  Triton\n- Send requests directly to Triton with the [HTTP/REST JSON-based\n  or gRPC protocols](docs/customization_guide/inference_protocols.md#httprest-and-grpc-protocols)\n\n#### Client Support and Examples\n\nA Triton *client* application sends inference and other requests to Triton. The\n[Python and C++ client libraries](https://github.com/triton-inference-server/client)\nprovide APIs to simplify this communication.\n\n- Review client examples for [C++](https://github.com/triton-inference-server/client/blob/main/src/c%2B%2B/examples),\n  [Python](https://github.com/triton-inference-server/client/blob/main/src/python/examples),\n  and [Java](https://github.com/triton-inference-server/client/blob/main/src/java/src/main/java/triton/client/examples)\n- Configure [HTTP](https://github.com/triton-inference-server/client#http-options)\n  and [gRPC](https://github.com/triton-inference-server/client#grpc-options)\n  client options\n- Send input data (e.g. a jpeg image) directly to Triton in the [body of an HTTP\n  request without any additional metadata](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md#raw-binary-request)\n\n### Extend Triton\n\n[Triton Inference Server's architecture](docs/user_guide/architecture.md) is specifically\ndesigned for modularity and flexibility\n\n- [Customize Triton Inference Server container](docs/customization_guide/compose.md) for your use case\n- [Create custom backends](https://github.com/triton-inference-server/backend)\n  in either [C/C++](https://github.com/triton-inference-server/backend/blob/main/README.md#triton-backend-api)\n  or [Python](https://github.com/triton-inference-server/python_backend)\n- Create [decoupled backends and models](docs/user_guide/decoupled_models.md) that can send\n  multiple responses for a request or not send any responses for a request\n- Use a [Triton repository agent](docs/customization_guide/repository_agents.md) to add functionality\n  that operates when a model is loaded and unloaded, such as authentication,\n  decryption, or conversion\n- Deploy Triton on [Jetson and JetPack](docs/user_guide/jetson.md)\n- [Use Triton on AWS\n   Inferentia](https://github.com/triton-inference-server/python_backend/tree/main/inferentia)\n\n### Additional Documentation\n\n- [FAQ](docs/user_guide/faq.md)\n- [User Guide](docs/README.md#user-guide)\n- [Customization Guide](docs/README.md#customization-guide)\n- [Release Notes](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/index.html)\n- [GPU, Driver, and CUDA Support\nMatrix](https://docs.nvidia.com/deeplearning/dgx/support-matrix/index.html)\n\n## Contributing\n\nContributions to Triton Inference Server are more than welcome. To\ncontribute please review the [contribution\nguidelines](CONTRIBUTING.md). If you have a backend, client,\nexample or similar contribution that is not modifying the core of\nTriton, then you should file a PR in the [contrib\nrepo](https://github.com/triton-inference-server/contrib).\n\n## Reporting problems, asking questions\n\nWe appreciate any feedback, questions or bug reporting regarding this project.\nWhen posting [issues in GitHub](https://github.com/triton-inference-server/server/issues),\nfollow the process outlined in the [Stack Overflow document](https://stackoverflow.com/help/mcve).\nEnsure posted examples are:\n- minimal – use as little code as possible that still produces the\n  same problem\n- complete – provide all parts needed to reproduce the problem. Check\n  if you can strip external dependencies and still show the problem. The\n  less time we spend on reproducing problems the more time we have to\n  fix it\n- verifiable – test the code you're about to provide to make sure it\n  reproduces the problem. Remove all other problems that are not\n  related to your request/question.\n\nFor issues, please use the provided bug report and feature request templates.\n\nFor questions, we recommend posting in our community\n[GitHub Discussions.](https://github.com/triton-inference-server/server/discussions)\n\n## For more information\n\nPlease refer to the [NVIDIA Developer Triton page](https://developer.nvidia.com/nvidia-triton-inference-server)\nfor more information.\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 2.51171875,
          "content": "<!--\n# Copyright 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n-->\n\n# Report a Security Vulnerability\n\nTo report a potential security vulnerability in any NVIDIA product, please use either:\n* This web form: [Security Vulnerability Submission Form](https://www.nvidia.com/object/submit-security-vulnerability.html), or\n* Send email to: [NVIDIA PSIRT](mailto:psirt@nvidia.com)\n\n**OEM Partners should contact their NVIDIA Customer Program Manager**\n\nIf reporting a potential vulnerability via email, please encrypt it using NVIDIA’s public PGP key ([see PGP Key page](https://www.nvidia.com/en-us/security/pgp-key/)) and include the following information:\n1. Product/Driver name and version/branch that contains the vulnerability\n2. Type of vulnerability (code execution, denial of service, buffer overflow, etc.)\n3. Instructions to reproduce the vulnerability\n4. Proof-of-concept or exploit code\n5. Potential impact of the vulnerability, including how an attacker could exploit the vulnerability\n\nSee https://www.nvidia.com/en-us/security/ for past NVIDIA Security Bulletins and Notices.\n"
        },
        {
          "name": "TRITON_VERSION",
          "type": "blob",
          "size": 0.009765625,
          "content": "2.54.0dev\n"
        },
        {
          "name": "Triton-CCLA-v1.pdf",
          "type": "blob",
          "size": 52.4501953125,
          "content": null
        },
        {
          "name": "build.py",
          "type": "blob",
          "size": 108.2890625,
          "content": "#!/usr/bin/env python3\n# Copyright 2020-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport argparse\nimport importlib.util\nimport multiprocessing\nimport os\nimport os.path\nimport pathlib\nimport platform\nimport stat\nimport subprocess\nimport sys\nfrom inspect import getsourcefile\n\nimport distro\nimport requests\n\n#\n# Build Triton Inference Server.\n#\n\n# By default build.py builds the Triton Docker image, but can also be\n# used to build without Docker.  See docs/build.md and --help for more\n# information.\n#\n# The TRITON_VERSION file indicates the Triton version and\n# DEFAULT_TRITON_VERSION_MAP is used to determine the corresponding container\n# version and upstream container version (upstream containers are\n# dependencies required by Triton). These versions may be overridden.\n\n# Map from Triton version to corresponding container and component versions.\n#\n#   triton version ->\n#     (triton container version,\n#      upstream container version,\n#      ORT version,\n#      ORT OpenVINO version (use None to disable OpenVINO in ORT),\n#      Standalone OpenVINO version,\n#      DCGM version\n#     )\n#\n# Currently the OpenVINO versions used in ORT and standalone must\n# match because of the way dlopen works with loading the backends. If\n# different versions are used then one backend or the other will\n# incorrectly load the other version of the openvino libraries.\n#\n\nDEFAULT_TRITON_VERSION_MAP = {\n    \"release_version\": \"2.54.0dev\",\n    \"triton_container_version\": \"24.01dev\",\n    \"upstream_container_version\": \"24.12\",\n    \"ort_version\": \"1.20.1\",\n    \"ort_openvino_version\": \"2024.4.0\",\n    \"standalone_openvino_version\": \"2024.4.0\",\n    \"dcgm_version\": \"3.3.6\",\n    \"vllm_version\": \"0.6.3.post1\",\n    \"rhel_py_version\": \"3.12.3\",\n}\n\nCORE_BACKENDS = [\"ensemble\"]\n\nFLAGS = None\nEXTRA_CORE_CMAKE_FLAGS = {}\nOVERRIDE_CORE_CMAKE_FLAGS = {}\nEXTRA_BACKEND_CMAKE_FLAGS = {}\nOVERRIDE_BACKEND_CMAKE_FLAGS = {}\n\nTHIS_SCRIPT_DIR = os.path.dirname(os.path.abspath(getsourcefile(lambda: 0)))\n\n\ndef log(msg, force=False):\n    if force or not FLAGS.quiet:\n        try:\n            print(msg, file=sys.stderr)\n        except Exception:\n            print(\"<failed to log>\", file=sys.stderr)\n\n\ndef log_verbose(msg):\n    if FLAGS.verbose:\n        log(msg, force=True)\n\n\ndef fail(msg):\n    fail_if(True, msg)\n\n\ndef fail_if(p, msg):\n    if p:\n        print(\"error: {}\".format(msg), file=sys.stderr)\n        sys.exit(1)\n\n\ndef target_platform():\n    # When called by compose.py, FLAGS will be None\n    if FLAGS and FLAGS.target_platform is not None:\n        return FLAGS.target_platform\n    platform_string = platform.system().lower()\n    if platform_string == \"linux\":\n        # Need to inspect the /etc/os-release file to get\n        # the distribution of linux\n        id_like_list = distro.like().split()\n        if \"debian\" in id_like_list:\n            return \"linux\"\n        else:\n            return \"rhel\"\n    else:\n        return platform_string\n\n\ndef target_machine():\n    # When called by compose.py, FLAGS will be None\n    if FLAGS and FLAGS.target_machine is not None:\n        return FLAGS.target_machine\n    return platform.machine().lower()\n\n\ndef container_versions(version, container_version, upstream_container_version):\n    if container_version is None:\n        container_version = FLAGS.triton_container_version\n    if upstream_container_version is None:\n        upstream_container_version = FLAGS.upstream_container_version\n    return container_version, upstream_container_version\n\n\nclass BuildScript:\n    \"\"\"Utility class for writing build scripts\"\"\"\n\n    def __init__(self, filepath, desc=None, verbose=False):\n        self._filepath = filepath\n        self._file = open(self._filepath, \"w\")\n        self._verbose = verbose\n        self.header(desc)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.close()\n\n    def __del__(self):\n        self.close()\n\n    def close(self):\n        if self._file is not None:\n            if target_platform() == \"windows\":\n                self.blankln()\n                self._file.write(\"}\\n\")\n                self._file.write(\"catch {\\n\")\n                self._file.write(\"    $_;\\n\")\n                self._file.write(\"    ExitWithCode 1;\\n\")\n                self._file.write(\"}\\n\")\n            \"\"\"Close the file\"\"\"\n            self._file.close()\n            self._file = None\n            st = os.stat(self._filepath)\n            os.chmod(self._filepath, st.st_mode | stat.S_IEXEC)\n\n    def blankln(self):\n        self._file.write(\"\\n\")\n\n    def commentln(self, cnt):\n        self._file.write(\"#\" * cnt + \"\\n\")\n\n    def comment(self, msg=\"\"):\n        if not isinstance(msg, str):\n            try:\n                for m in msg:\n                    self._file.write(f\"# {msg}\\n\")\n                return\n            except TypeError:\n                pass\n        self._file.write(f\"# {msg}\\n\")\n\n    def comment_verbose(self, msg=\"\"):\n        if self._verbose:\n            self.comment(msg)\n\n    def header(self, desc=None):\n        if target_platform() != \"windows\":\n            self._file.write(\"#!/usr/bin/env bash\\n\\n\")\n\n        if desc is not None:\n            self.comment()\n            self.comment(desc)\n            self.comment()\n            self.blankln()\n\n        self.comment(\"Exit script immediately if any command fails\")\n        if target_platform() == \"windows\":\n            self._file.write(\"$UseStructuredOutput = $false\\n\")\n            self.blankln()\n            self._file.write(\"function ExitWithCode($exitcode) {\\n\")\n            self._file.write(\"    $host.SetShouldExit($exitcode)\\n\")\n            self._file.write(\"    exit $exitcode\\n\")\n            self._file.write(\"}\\n\")\n            self.blankln()\n            if self._verbose:\n                self._file.write(\"Set-PSDebug -Trace 1\\n\")\n            self.blankln()\n            self._file.write(\"try {\\n\")\n        else:\n            self._file.write(\"set -e\\n\")\n            if self._verbose:\n                self._file.write(\"set -x\\n\")\n        self.blankln()\n\n    def envvar_ref(self, v):\n        if target_platform() == \"windows\":\n            return f\"${{env:{v}}}\"\n        return f\"${{{v}}}\"\n\n    def cmd(self, clist, check_exitcode=False):\n        if isinstance(clist, str):\n            self._file.write(f\"{clist}\\n\")\n        else:\n            for c in clist:\n                self._file.write(f\"{c} \")\n            self.blankln()\n\n        if check_exitcode:\n            if target_platform() == \"windows\":\n                self._file.write(\"if ($LASTEXITCODE -ne 0) {\\n\")\n                self._file.write(\n                    '  Write-Output \"exited with status code $LASTEXITCODE\";\\n'\n                )\n                self._file.write(\"  ExitWithCode 1;\\n\")\n                self._file.write(\"}\\n\")\n\n    def cwd(self, path):\n        if target_platform() == \"windows\":\n            self.cmd(f\"Set-Location -EV Err -EA Stop {path}\")\n        else:\n            self.cmd(f\"cd {path}\")\n\n    def cp(self, src, dest):\n        if target_platform() == \"windows\":\n            self.cmd(f\"Copy-Item -EV Err -EA Stop {src} -Destination {dest}\")\n        else:\n            self.cmd(f\"cp {src} {dest}\")\n\n    def mkdir(self, path):\n        if target_platform() == \"windows\":\n            self.cmd(\n                f\"New-Item -EV Err -EA Stop -ItemType Directory -Force -Path {path}\"\n            )\n        else:\n            self.cmd(f\"mkdir -p {pathlib.Path(path)}\")\n\n    def rmdir(self, path):\n        if target_platform() == \"windows\":\n            self.cmd(f\"if (Test-Path -Path {path}) {{\")\n            self.cmd(f\"  Remove-Item -EV Err -EA Stop -Recurse -Force {path}\")\n            self.cmd(\"}\")\n        else:\n            self.cmd(f\"rm -fr {pathlib.Path(path)}\")\n\n    def cpdir(self, src, dest):\n        if target_platform() == \"windows\":\n            self.cmd(f\"Copy-Item -EV Err -EA Stop -Recurse {src} -Destination {dest}\")\n        else:\n            self.cmd(f\"cp -r {src} {dest}\")\n\n    def tar(self, subdir, tar_filename):\n        if target_platform() == \"windows\":\n            fail(\"unsupported operation: tar\")\n        else:\n            self.cmd(f\"tar zcf {tar_filename} {subdir}\")\n\n    def cmake(self, args):\n        # Pass some additional envvars into cmake...\n        env_args = []\n        for k in (\"TRT_VERSION\", \"CMAKE_TOOLCHAIN_FILE\", \"VCPKG_TARGET_TRIPLET\"):\n            env_args += [f'\"-D{k}={self.envvar_ref(k)}\"']\n        self.cmd(f'cmake {\" \".join(env_args)} {\" \".join(args)}', check_exitcode=True)\n\n    def makeinstall(self, target=\"install\"):\n        verbose_flag = \"-v\" if self._verbose else \"\"\n        self.cmd(\n            f\"cmake --build . --config {FLAGS.build_type} -j{FLAGS.build_parallel} {verbose_flag} -t {target}\"\n        )\n\n    def gitclone(self, repo, tag, subdir, org):\n        clone_dir = subdir\n        if not FLAGS.no_force_clone:\n            self.rmdir(clone_dir)\n\n        if target_platform() == \"windows\":\n            self.cmd(f\"if (-Not (Test-Path -Path {clone_dir})) {{\")\n        else:\n            self.cmd(f\"if [[ ! -e {clone_dir} ]]; then\")\n\n        # FIXME [DLIS-4045 - Currently the tag starting with \"pull/\" is not\n        # working with \"--repo-tag\" as the option is not forwarded to the\n        # individual repo build correctly.]\n        # If 'tag' starts with \"pull/\" then it must be of form\n        # \"pull/<pr>/head\". We just clone at \"main\" and then fetch the\n        # reference onto a new branch we name \"tritonbuildref\".\n        if tag.startswith(\"pull/\"):\n            self.cmd(\n                f\"  git clone --recursive --depth=1 {org}/{repo}.git {subdir};\",\n                check_exitcode=True,\n            )\n            self.cmd(\"}\" if target_platform() == \"windows\" else \"fi\")\n            self.cwd(subdir)\n            self.cmd(f\"git fetch origin {tag}:tritonbuildref\", check_exitcode=True)\n            self.cmd(f\"git checkout tritonbuildref\", check_exitcode=True)\n        else:\n            self.cmd(\n                f\"  git clone --recursive --single-branch --depth=1 -b {tag} {org}/{repo}.git {subdir};\",\n                check_exitcode=True,\n            )\n            self.cmd(\"}\" if target_platform() == \"windows\" else \"fi\")\n\n\ndef cmake_core_arg(name, type, value):\n    # Return cmake -D setting to set name=value for core build. Use\n    # command-line specified value if one is given.\n    if name in OVERRIDE_CORE_CMAKE_FLAGS:\n        value = OVERRIDE_CORE_CMAKE_FLAGS[name]\n    if type is None:\n        type = \"\"\n    else:\n        type = \":{}\".format(type)\n    return '\"-D{}{}={}\"'.format(name, type, value)\n\n\ndef cmake_core_enable(name, flag):\n    # Return cmake -D setting to set name=flag?ON:OFF for core\n    # build. Use command-line specified value for 'flag' if one is\n    # given.\n    if name in OVERRIDE_CORE_CMAKE_FLAGS:\n        value = OVERRIDE_CORE_CMAKE_FLAGS[name]\n    else:\n        value = \"ON\" if flag else \"OFF\"\n    return '\"-D{}:BOOL={}\"'.format(name, value)\n\n\ndef cmake_core_extra_args():\n    args = []\n    for k, v in EXTRA_CORE_CMAKE_FLAGS.items():\n        args.append('\"-D{}={}\"'.format(k, v))\n    return args\n\n\ndef cmake_backend_arg(backend, name, type, value):\n    # Return cmake -D setting to set name=value for backend build. Use\n    # command-line specified value if one is given.\n    if backend in OVERRIDE_BACKEND_CMAKE_FLAGS:\n        if name in OVERRIDE_BACKEND_CMAKE_FLAGS[backend]:\n            value = OVERRIDE_BACKEND_CMAKE_FLAGS[backend][name]\n    if type is None:\n        type = \"\"\n    else:\n        type = \":{}\".format(type)\n    return '\"-D{}{}={}\"'.format(name, type, value)\n\n\ndef cmake_backend_enable(backend, name, flag):\n    # Return cmake -D setting to set name=flag?ON:OFF for backend\n    # build. Use command-line specified value for 'flag' if one is\n    # given.\n    value = None\n    if backend in OVERRIDE_BACKEND_CMAKE_FLAGS:\n        if name in OVERRIDE_BACKEND_CMAKE_FLAGS[backend]:\n            value = OVERRIDE_BACKEND_CMAKE_FLAGS[backend][name]\n    if value is None:\n        value = \"ON\" if flag else \"OFF\"\n    return '\"-D{}:BOOL={}\"'.format(name, value)\n\n\ndef cmake_backend_extra_args(backend):\n    args = []\n    if backend in EXTRA_BACKEND_CMAKE_FLAGS:\n        for k, v in EXTRA_BACKEND_CMAKE_FLAGS[backend].items():\n            args.append('\"-D{}={}\"'.format(k, v))\n    return args\n\n\ndef cmake_repoagent_arg(name, type, value):\n    # For now there is no override for repo-agents\n    if type is None:\n        type = \"\"\n    else:\n        type = \":{}\".format(type)\n    return '\"-D{}{}={}\"'.format(name, type, value)\n\n\ndef cmake_repoagent_enable(name, flag):\n    # For now there is no override for repo-agents\n    value = \"ON\" if flag else \"OFF\"\n    return '\"-D{}:BOOL={}\"'.format(name, value)\n\n\ndef cmake_repoagent_extra_args():\n    # For now there is no extra args for repo-agents\n    args = []\n    return args\n\n\ndef cmake_cache_arg(name, type, value):\n    # For now there is no override for caches\n    if type is None:\n        type = \"\"\n    else:\n        type = \":{}\".format(type)\n    return '\"-D{}{}={}\"'.format(name, type, value)\n\n\ndef cmake_cache_enable(name, flag):\n    # For now there is no override for caches\n    value = \"ON\" if flag else \"OFF\"\n    return '\"-D{}:BOOL={}\"'.format(name, value)\n\n\ndef cmake_cache_extra_args():\n    # For now there is no extra args for caches\n    args = []\n    return args\n\n\ndef core_cmake_args(components, backends, cmake_dir, install_dir):\n    cargs = [\n        cmake_core_arg(\"CMAKE_BUILD_TYPE\", None, FLAGS.build_type),\n        cmake_core_arg(\"CMAKE_INSTALL_PREFIX\", \"PATH\", install_dir),\n        cmake_core_arg(\"TRITON_VERSION\", \"STRING\", FLAGS.version),\n        cmake_core_arg(\"TRITON_REPO_ORGANIZATION\", \"STRING\", FLAGS.github_organization),\n        cmake_core_arg(\"TRITON_COMMON_REPO_TAG\", \"STRING\", components[\"common\"]),\n        cmake_core_arg(\"TRITON_CORE_REPO_TAG\", \"STRING\", components[\"core\"]),\n        cmake_core_arg(\"TRITON_BACKEND_REPO_TAG\", \"STRING\", components[\"backend\"]),\n        cmake_core_arg(\n            \"TRITON_THIRD_PARTY_REPO_TAG\", \"STRING\", components[\"thirdparty\"]\n        ),\n    ]\n\n    cargs.append(cmake_core_enable(\"TRITON_ENABLE_LOGGING\", FLAGS.enable_logging))\n    cargs.append(cmake_core_enable(\"TRITON_ENABLE_STATS\", FLAGS.enable_stats))\n    cargs.append(cmake_core_enable(\"TRITON_ENABLE_METRICS\", FLAGS.enable_metrics))\n    cargs.append(\n        cmake_core_enable(\"TRITON_ENABLE_METRICS_GPU\", FLAGS.enable_gpu_metrics)\n    )\n    cargs.append(\n        cmake_core_enable(\"TRITON_ENABLE_METRICS_CPU\", FLAGS.enable_cpu_metrics)\n    )\n    cargs.append(cmake_core_enable(\"TRITON_ENABLE_TRACING\", FLAGS.enable_tracing))\n    cargs.append(cmake_core_enable(\"TRITON_ENABLE_NVTX\", FLAGS.enable_nvtx))\n\n    cargs.append(cmake_core_enable(\"TRITON_ENABLE_GPU\", FLAGS.enable_gpu))\n    cargs.append(\n        cmake_core_arg(\n            \"TRITON_MIN_COMPUTE_CAPABILITY\", None, FLAGS.min_compute_capability\n        )\n    )\n\n    cargs.append(cmake_core_enable(\"TRITON_ENABLE_MALI_GPU\", FLAGS.enable_mali_gpu))\n\n    cargs.append(cmake_core_enable(\"TRITON_ENABLE_GRPC\", \"grpc\" in FLAGS.endpoint))\n    cargs.append(cmake_core_enable(\"TRITON_ENABLE_HTTP\", \"http\" in FLAGS.endpoint))\n    cargs.append(\n        cmake_core_enable(\"TRITON_ENABLE_SAGEMAKER\", \"sagemaker\" in FLAGS.endpoint)\n    )\n    cargs.append(\n        cmake_core_enable(\"TRITON_ENABLE_VERTEX_AI\", \"vertex-ai\" in FLAGS.endpoint)\n    )\n\n    cargs.append(cmake_core_enable(\"TRITON_ENABLE_GCS\", \"gcs\" in FLAGS.filesystem))\n    cargs.append(cmake_core_enable(\"TRITON_ENABLE_S3\", \"s3\" in FLAGS.filesystem))\n    cargs.append(\n        cmake_core_enable(\n            \"TRITON_ENABLE_AZURE_STORAGE\", \"azure_storage\" in FLAGS.filesystem\n        )\n    )\n\n    cargs.append(cmake_core_enable(\"TRITON_ENABLE_ENSEMBLE\", \"ensemble\" in backends))\n    cargs.append(cmake_core_enable(\"TRITON_ENABLE_TENSORRT\", \"tensorrt\" in backends))\n\n    cargs += cmake_core_extra_args()\n    cargs.append(cmake_dir)\n    return cargs\n\n\ndef repoagent_repo(ra):\n    return \"{}_repository_agent\".format(ra)\n\n\ndef repoagent_cmake_args(images, components, ra, install_dir):\n    args = []\n\n    cargs = args + [\n        cmake_repoagent_arg(\"CMAKE_BUILD_TYPE\", None, FLAGS.build_type),\n        cmake_repoagent_arg(\"CMAKE_INSTALL_PREFIX\", \"PATH\", install_dir),\n        cmake_repoagent_arg(\n            \"TRITON_REPO_ORGANIZATION\", \"STRING\", FLAGS.github_organization\n        ),\n        cmake_repoagent_arg(\"TRITON_COMMON_REPO_TAG\", \"STRING\", components[\"common\"]),\n        cmake_repoagent_arg(\"TRITON_CORE_REPO_TAG\", \"STRING\", components[\"core\"]),\n    ]\n\n    cargs.append(cmake_repoagent_enable(\"TRITON_ENABLE_GPU\", FLAGS.enable_gpu))\n    cargs += cmake_repoagent_extra_args()\n    cargs.append(\"..\")\n    return cargs\n\n\ndef cache_repo(cache):\n    # example: \"local\", or \"redis\"\n    return \"{}_cache\".format(cache)\n\n\ndef cache_cmake_args(images, components, cache, install_dir):\n    args = []\n\n    cargs = args + [\n        cmake_cache_arg(\"CMAKE_BUILD_TYPE\", None, FLAGS.build_type),\n        cmake_cache_arg(\"CMAKE_INSTALL_PREFIX\", \"PATH\", install_dir),\n        cmake_cache_arg(\n            \"TRITON_REPO_ORGANIZATION\", \"STRING\", FLAGS.github_organization\n        ),\n        cmake_cache_arg(\"TRITON_COMMON_REPO_TAG\", \"STRING\", components[\"common\"]),\n        cmake_cache_arg(\"TRITON_CORE_REPO_TAG\", \"STRING\", components[\"core\"]),\n    ]\n\n    cargs.append(cmake_cache_enable(\"TRITON_ENABLE_GPU\", FLAGS.enable_gpu))\n    cargs += cmake_cache_extra_args()\n    cargs.append(\"..\")\n    return cargs\n\n\ndef backend_repo(be):\n    return \"{}_backend\".format(be)\n\n\ndef backend_cmake_args(images, components, be, install_dir, library_paths):\n    cmake_build_type = FLAGS.build_type\n\n    if be == \"onnxruntime\":\n        args = onnxruntime_cmake_args(images, library_paths)\n    elif be == \"openvino\":\n        args = openvino_cmake_args()\n    elif be == \"tensorflow\":\n        args = tensorflow_cmake_args(images, library_paths)\n    elif be == \"python\":\n        args = python_cmake_args()\n    elif be == \"dali\":\n        args = dali_cmake_args()\n    elif be == \"pytorch\":\n        args = pytorch_cmake_args(images)\n    elif be == \"armnn_tflite\":\n        args = armnn_tflite_cmake_args()\n    elif be == \"fil\":\n        args = fil_cmake_args(images)\n        # DLIS-4618: FIL backend fails debug build, so override it for now.\n        cmake_build_type = \"Release\"\n    elif be == \"fastertransformer\":\n        args = fastertransformer_cmake_args()\n    elif be == \"tensorrt\":\n        args = tensorrt_cmake_args()\n    elif be == \"tensorrtllm\":\n        args = tensorrtllm_cmake_args(images)\n    else:\n        args = []\n\n    cargs = args + [\n        cmake_backend_arg(be, \"CMAKE_BUILD_TYPE\", None, cmake_build_type),\n        cmake_backend_arg(be, \"CMAKE_INSTALL_PREFIX\", \"PATH\", install_dir),\n        cmake_backend_arg(\n            be, \"TRITON_REPO_ORGANIZATION\", \"STRING\", FLAGS.github_organization\n        ),\n        cmake_backend_arg(be, \"TRITON_COMMON_REPO_TAG\", \"STRING\", components[\"common\"]),\n        cmake_backend_arg(be, \"TRITON_CORE_REPO_TAG\", \"STRING\", components[\"core\"]),\n        cmake_backend_arg(\n            be, \"TRITON_BACKEND_REPO_TAG\", \"STRING\", components[\"backend\"]\n        ),\n    ]\n\n    cargs.append(cmake_backend_enable(be, \"TRITON_ENABLE_GPU\", FLAGS.enable_gpu))\n    cargs.append(\n        cmake_backend_enable(be, \"TRITON_ENABLE_MALI_GPU\", FLAGS.enable_mali_gpu)\n    )\n    cargs.append(cmake_backend_enable(be, \"TRITON_ENABLE_STATS\", FLAGS.enable_stats))\n    cargs.append(\n        cmake_backend_enable(be, \"TRITON_ENABLE_METRICS\", FLAGS.enable_metrics)\n    )\n\n    # [DLIS-4950] always enable below once Windows image is updated with CUPTI\n    # cargs.append(cmake_backend_enable(be, 'TRITON_ENABLE_MEMORY_TRACKER', True))\n    if (target_platform() == \"windows\") and (not FLAGS.no_container_build):\n        print(\n            \"Warning: Detected docker build is used for Windows, backend utility 'device memory tracker' will be disabled due to missing library in CUDA Windows docker image.\"\n        )\n        cargs.append(cmake_backend_enable(be, \"TRITON_ENABLE_MEMORY_TRACKER\", False))\n    elif target_platform() == \"igpu\":\n        print(\n            \"Warning: Detected iGPU build, backend utility 'device memory tracker' will be disabled as iGPU doesn't contain required version of the library.\"\n        )\n        cargs.append(cmake_backend_enable(be, \"TRITON_ENABLE_MEMORY_TRACKER\", False))\n    elif FLAGS.enable_gpu:\n        cargs.append(cmake_backend_enable(be, \"TRITON_ENABLE_MEMORY_TRACKER\", True))\n\n    cargs += cmake_backend_extra_args(be)\n    if be == \"tensorrtllm\":\n        cargs.append(\"-S ../inflight_batcher_llm -B .\")\n\n    else:\n        cargs.append(\"..\")\n    return cargs\n\n\ndef python_cmake_args():\n    cargs = []\n    if target_platform() == \"rhel\":\n        cargs.append(\n            cmake_backend_arg(\n                \"python\", \"PYBIND11_PYTHON_VERSION\", \"STRING\", FLAGS.rhel_py_version\n            )\n        )\n\n    return cargs\n\n\ndef pytorch_cmake_args(images):\n    if \"pytorch\" in images:\n        image = images[\"pytorch\"]\n    else:\n        image = \"nvcr.io/nvidia/pytorch:{}-py3\".format(FLAGS.upstream_container_version)\n    cargs = [\n        cmake_backend_arg(\"pytorch\", \"TRITON_PYTORCH_DOCKER_IMAGE\", None, image),\n    ]\n\n    # TODO: TPRD-372 TorchTRT extension is not currently supported by our manylinux build\n    # TODO: TPRD-373 NVTX extension is not currently supported by our manylinux build\n    if target_platform() != \"rhel\":\n        if FLAGS.enable_gpu:\n            cargs.append(\n                cmake_backend_enable(\"pytorch\", \"TRITON_PYTORCH_ENABLE_TORCHTRT\", True)\n            )\n        cargs.append(\n            cmake_backend_enable(\"pytorch\", \"TRITON_ENABLE_NVTX\", FLAGS.enable_nvtx)\n        )\n    return cargs\n\n\ndef onnxruntime_cmake_args(images, library_paths):\n    cargs = [\n        cmake_backend_arg(\n            \"onnxruntime\",\n            \"TRITON_BUILD_ONNXRUNTIME_VERSION\",\n            None,\n            os.getenv(\"TRITON_BUILD_ONNXRUNTIME_VERSION\")\n            if os.getenv(\"TRITON_BUILD_ONNXRUNTIME_VERSION\")\n            else FLAGS.ort_version,\n        )\n    ]\n\n    # TRITON_ENABLE_GPU is already set for all backends in backend_cmake_args()\n    if FLAGS.enable_gpu:\n        # TODO: TPRD-712 TensorRT is not currently supported by our RHEL build for SBSA.\n        if target_platform() != \"rhel\" or (\n            target_platform() == \"rhel\" and target_machine() == \"x86_64\"\n        ):\n            cargs.append(\n                cmake_backend_enable(\n                    \"onnxruntime\", \"TRITON_ENABLE_ONNXRUNTIME_TENSORRT\", True\n                )\n            )\n\n    if target_platform() == \"windows\":\n        if \"base\" in images:\n            cargs.append(\n                cmake_backend_arg(\n                    \"onnxruntime\", \"TRITON_BUILD_CONTAINER\", None, images[\"base\"]\n                )\n            )\n    else:\n        if \"base\" in images:\n            cargs.append(\n                cmake_backend_arg(\n                    \"onnxruntime\", \"TRITON_BUILD_CONTAINER\", None, images[\"base\"]\n                )\n            )\n        else:\n            cargs.append(\n                cmake_backend_arg(\n                    \"onnxruntime\",\n                    \"TRITON_BUILD_CONTAINER_VERSION\",\n                    None,\n                    FLAGS.triton_container_version,\n                )\n            )\n\n        # TODO: TPRD-333 OpenVino extension is not currently supported by our manylinux build\n        if (\n            (target_machine() != \"aarch64\")\n            and (target_platform() != \"rhel\")\n            and (FLAGS.ort_openvino_version is not None)\n        ):\n            cargs.append(\n                cmake_backend_enable(\n                    \"onnxruntime\", \"TRITON_ENABLE_ONNXRUNTIME_OPENVINO\", True\n                )\n            )\n            cargs.append(\n                cmake_backend_arg(\n                    \"onnxruntime\",\n                    \"TRITON_BUILD_ONNXRUNTIME_OPENVINO_VERSION\",\n                    None,\n                    FLAGS.ort_openvino_version,\n                )\n            )\n\n        if (target_platform() == \"igpu\") or (target_platform() == \"rhel\"):\n            cargs.append(\n                cmake_backend_arg(\n                    \"onnxruntime\",\n                    \"TRITON_BUILD_TARGET_PLATFORM\",\n                    None,\n                    target_platform(),\n                )\n            )\n\n    return cargs\n\n\ndef openvino_cmake_args():\n    cargs = [\n        cmake_backend_arg(\n            \"openvino\",\n            \"TRITON_BUILD_OPENVINO_VERSION\",\n            None,\n            FLAGS.standalone_openvino_version,\n        )\n    ]\n    if target_platform() == \"windows\":\n        if \"base\" in images:\n            cargs.append(\n                cmake_backend_arg(\n                    \"openvino\", \"TRITON_BUILD_CONTAINER\", None, images[\"base\"]\n                )\n            )\n    else:\n        if \"base\" in images:\n            cargs.append(\n                cmake_backend_arg(\n                    \"openvino\", \"TRITON_BUILD_CONTAINER\", None, images[\"base\"]\n                )\n            )\n        else:\n            cargs.append(\n                cmake_backend_arg(\n                    \"openvino\",\n                    \"TRITON_BUILD_CONTAINER_VERSION\",\n                    None,\n                    FLAGS.upstream_container_version,\n                )\n            )\n    return cargs\n\n\ndef tensorrt_cmake_args():\n    cargs = [\n        cmake_backend_enable(\"tensorrt\", \"TRITON_ENABLE_NVTX\", FLAGS.enable_nvtx),\n    ]\n    if target_platform() == \"windows\":\n        cargs.append(\n            cmake_backend_arg(\n                \"tensorrt\", \"TRITON_TENSORRT_INCLUDE_PATHS\", None, \"c:/TensorRT/include\"\n            )\n        )\n\n    return cargs\n\n\ndef tensorflow_cmake_args(images, library_paths):\n    backend_name = \"tensorflow\"\n    extra_args = []\n\n    # If a specific TF image is specified use it, otherwise pull from NGC.\n    if backend_name in images:\n        image = images[backend_name]\n    else:\n        image = \"nvcr.io/nvidia/tensorflow:{}-tf2-py3\".format(\n            FLAGS.upstream_container_version\n        )\n    extra_args = [\n        cmake_backend_arg(backend_name, \"TRITON_TENSORFLOW_DOCKER_IMAGE\", None, image)\n    ]\n    return extra_args\n\n\ndef dali_cmake_args():\n    return [\n        cmake_backend_enable(\"dali\", \"TRITON_DALI_SKIP_DOWNLOAD\", False),\n    ]\n\n\ndef fil_cmake_args(images):\n    cargs = [cmake_backend_enable(\"fil\", \"TRITON_FIL_DOCKER_BUILD\", True)]\n    if \"base\" in images:\n        cargs.append(\n            cmake_backend_arg(\"fil\", \"TRITON_BUILD_CONTAINER\", None, images[\"base\"])\n        )\n    else:\n        cargs.append(\n            cmake_backend_arg(\n                \"fil\",\n                \"TRITON_BUILD_CONTAINER_VERSION\",\n                None,\n                FLAGS.upstream_container_version,\n            )\n        )\n\n    return cargs\n\n\ndef armnn_tflite_cmake_args():\n    return [\n        cmake_backend_arg(\"armnn_tflite\", \"JOBS\", None, multiprocessing.cpu_count()),\n    ]\n\n\ndef fastertransformer_cmake_args():\n    print(\"Warning: FasterTransformer backend is not officially supported.\")\n    cargs = [\n        cmake_backend_arg(\n            \"fastertransformer\", \"CMAKE_EXPORT_COMPILE_COMMANDS\", None, 1\n        ),\n        cmake_backend_arg(\"fastertransformer\", \"ENABLE_FP8\", None, \"OFF\"),\n    ]\n    return cargs\n\n\ndef tensorrtllm_cmake_args(images):\n    cargs = []\n    cargs.append(cmake_backend_enable(\"tensorrtllm\", \"USE_CXX11_ABI\", True))\n    return cargs\n\n\ndef install_dcgm_libraries(dcgm_version, target_machine):\n    if dcgm_version == \"\":\n        fail(\n            \"unable to determine default repo-tag, DCGM version not known for {}\".format(\n                FLAGS.version\n            )\n        )\n        return \"\"\n    else:\n        # RHEL has the same install instructions for both aarch64 and x86\n        if target_platform() == \"rhel\":\n            if target_machine == \"aarch64\":\n                return \"\"\"\nENV DCGM_VERSION {}\n# Install DCGM. Steps from https://developer.nvidia.com/dcgm#Downloads\nRUN dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/sbsa/cuda-rhel8.repo \\\\\n    && dnf clean expire-cache \\\\\n    && dnf install -y datacenter-gpu-manager-{}\n\"\"\".format(\n                    dcgm_version, dcgm_version\n                )\n            else:\n                return \"\"\"\nENV DCGM_VERSION {}\n# Install DCGM. Steps from https://developer.nvidia.com/dcgm#Downloads\nRUN dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo \\\\\n    && dnf clean expire-cache \\\\\n    && dnf install -y datacenter-gpu-manager-{}\n\"\"\".format(\n                    dcgm_version, dcgm_version\n                )\n        else:\n            if target_machine == \"aarch64\":\n                return \"\"\"\nENV DCGM_VERSION {}\n# Install DCGM. Steps from https://developer.nvidia.com/dcgm#Downloads\nRUN curl -o /tmp/cuda-keyring.deb \\\\\n        https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/sbsa/cuda-keyring_1.1-1_all.deb \\\\\n      && apt install /tmp/cuda-keyring.deb \\\\\n      && rm /tmp/cuda-keyring.deb \\\\\n      && apt-get update \\\\\n      && apt-get install -y datacenter-gpu-manager=1:{}\n\"\"\".format(\n                    dcgm_version, dcgm_version\n                )\n            else:\n                return \"\"\"\nENV DCGM_VERSION {}\n# Install DCGM. Steps from https://developer.nvidia.com/dcgm#Downloads\nRUN curl -o /tmp/cuda-keyring.deb \\\\\n          https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-keyring_1.1-1_all.deb \\\\\n      && apt install /tmp/cuda-keyring.deb \\\\\n      && rm /tmp/cuda-keyring.deb \\\\\n      && apt-get update \\\\\n      && apt-get install -y datacenter-gpu-manager=1:{}\n\"\"\".format(\n                    dcgm_version, dcgm_version\n                )\n\n\ndef create_dockerfile_buildbase_rhel(ddir, dockerfile_name, argmap):\n    df = \"\"\"\nARG TRITON_VERSION={}\nARG TRITON_CONTAINER_VERSION={}\nARG BASE_IMAGE={}\n\"\"\".format(\n        argmap[\"TRITON_VERSION\"],\n        argmap[\"TRITON_CONTAINER_VERSION\"],\n        argmap[\"BASE_IMAGE\"],\n    )\n\n    df += \"\"\"\nFROM ${BASE_IMAGE}\n\nARG TRITON_VERSION\nARG TRITON_CONTAINER_VERSION\nENV PIP_BREAK_SYSTEM_PACKAGES=1\n\"\"\"\n    df += \"\"\"\n# Install docker docker buildx\nRUN yum install -y ca-certificates curl gnupg yum-utils \\\\\n      && yum-config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo \\\\\n      && yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n#   && yum install -y docker.io docker-buildx-plugin\n\n# libcurl4-openSSL-dev is needed for GCS\n# python3-dev is needed by Torchvision\n# python3-pip and libarchive-dev is needed by python backend\n# libxml2-dev is needed for Azure Storage\n# scons is needed for armnn_tflite backend build dep\nRUN yum install -y \\\\\n            ca-certificates \\\\\n            autoconf \\\\\n            automake \\\\\n            git \\\\\n            gperf \\\\\n            re2-devel \\\\\n            openssl-devel \\\\\n            libtool \\\\\n            libcurl-devel \\\\\n            libb64-devel \\\\\n            gperftools-devel \\\\\n            patchelf \\\\\n            python3-pip \\\\\n            python3-setuptools \\\\\n            rapidjson-devel \\\\\n            python3-scons \\\\\n            pkg-config \\\\\n            unzip \\\\\n            wget \\\\\n            ncurses-devel \\\\\n            readline-devel \\\\\n            xz-devel \\\\\n            bzip2-devel \\\\\n            zlib-devel \\\\\n            libarchive-devel \\\\\n            libxml2-devel \\\\\n            numactl-devel \\\\\n            wget\n\"\"\"\n    # Requires openssl-devel to be installed first for pyenv build to be successful\n    df += change_default_python_version_rhel(FLAGS.rhel_py_version)\n    df += \"\"\"\n\nRUN pip3 install --upgrade pip \\\\\n      && pip3 install --upgrade \\\\\n          build \\\\\n          wheel \\\\\n          setuptools \\\\\n          docker \\\\\n          virtualenv\n\n# Install boost version >= 1.78 for boost::span\n# Current libboost-dev apt packages are < 1.78, so install from tar.gz\nRUN wget -O /tmp/boost.tar.gz \\\\\n          https://archives.boost.io/release/1.80.0/source/boost_1_80_0.tar.gz \\\\\n      && (cd /tmp && tar xzf boost.tar.gz) \\\\\n      && mv /tmp/boost_1_80_0/boost /usr/include/boost\n\n# Server build requires recent version of CMake (FetchContent required)\n# Might not need this if the installed version of cmake is high enough for our build.\n# RUN apt update -q=2 \\\\\n#       && apt install -y gpg wget \\\\\n#       && wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - |  tee /usr/share/keyrings/kitware-archive-keyring.gpg >/dev/null \\\\\n#       && . /etc/os-release \\\\\n#       && echo \"deb [signed-by=/usr/share/keyrings/kitware-archive-keyring.gpg] https://apt.kitware.com/ubuntu/ $UBUNTU_CODENAME main\" | tee /etc/apt/sources.list.d/kitware.list >/dev/null \\\\\n#       && apt-get update -q=2 \\\\\n#       && apt-get install -y --no-install-recommends cmake=3.27.7* cmake-data=3.27.7*\n\"\"\"\n    if FLAGS.enable_gpu:\n        df += install_dcgm_libraries(argmap[\"DCGM_VERSION\"], target_machine())\n    df += \"\"\"\nENV TRITON_SERVER_VERSION ${TRITON_VERSION}\nENV NVIDIA_TRITON_SERVER_VERSION ${TRITON_CONTAINER_VERSION}\n\"\"\"\n\n    df += \"\"\"\nWORKDIR /workspace\nRUN rm -fr *\nCOPY . .\nENTRYPOINT []\n\"\"\"\n\n    with open(os.path.join(ddir, dockerfile_name), \"w\") as dfile:\n        dfile.write(df)\n\n\ndef create_dockerfile_buildbase(ddir, dockerfile_name, argmap):\n    df = \"\"\"\nARG TRITON_VERSION={}\nARG TRITON_CONTAINER_VERSION={}\nARG BASE_IMAGE={}\n\"\"\".format(\n        argmap[\"TRITON_VERSION\"],\n        argmap[\"TRITON_CONTAINER_VERSION\"],\n        argmap[\"BASE_IMAGE\"],\n    )\n\n    df += \"\"\"\nFROM ${BASE_IMAGE}\n\nARG TRITON_VERSION\nARG TRITON_CONTAINER_VERSION\nENV PIP_BREAK_SYSTEM_PACKAGES=1\n\"\"\"\n    # Install the windows- or linux-specific buildbase dependencies\n    if target_platform() == \"windows\":\n        df += \"\"\"\nSHELL [\"cmd\", \"/S\", \"/C\"]\n\"\"\"\n    else:\n        df += \"\"\"\n# Ensure apt-get won't prompt for selecting options\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install docker docker buildx\nRUN apt-get update \\\\\n      && apt-get install -y ca-certificates curl gnupg \\\\\n      && install -m 0755 -d /etc/apt/keyrings \\\\\n      && curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg \\\\\n      && chmod a+r /etc/apt/keyrings/docker.gpg \\\\\n      && echo \\\\\n          \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\\\n          \"$(. /etc/os-release && echo \"$VERSION_CODENAME\")\" stable\" | \\\\\n          tee /etc/apt/sources.list.d/docker.list > /dev/null \\\\\n      && apt-get update \\\\\n      && apt-get install -y docker.io docker-buildx-plugin\n\n# libcurl4-openSSL-dev is needed for GCS\n# python3-dev is needed by Torchvision\n# python3-pip and libarchive-dev is needed by python backend\n# libxml2-dev is needed for Azure Storage\n# scons is needed for armnn_tflite backend build dep\nRUN apt-get update \\\\\n      && apt-get install -y --no-install-recommends \\\\\n            ca-certificates \\\\\n            autoconf \\\\\n            automake \\\\\n            build-essential \\\\\n            git \\\\\n            gperf \\\\\n            libre2-dev \\\\\n            libssl-dev \\\\\n            libtool \\\\\n            libcurl4-openssl-dev \\\\\n            libb64-dev \\\\\n            libgoogle-perftools-dev \\\\\n            patchelf \\\\\n            python3-dev \\\\\n            python3-pip \\\\\n            python3-wheel \\\\\n            python3-setuptools \\\\\n            rapidjson-dev \\\\\n            scons \\\\\n            software-properties-common \\\\\n            pkg-config \\\\\n            unzip \\\\\n            wget \\\\\n            zlib1g-dev \\\\\n            libarchive-dev \\\\\n            libxml2-dev \\\\\n            libnuma-dev \\\\\n            wget \\\\\n      && rm -rf /var/lib/apt/lists/*\n\nRUN pip3 install --upgrade \\\\\n          build \\\\\n          docker \\\\\n          virtualenv\n\n# Install boost version >= 1.78 for boost::span\n# Current libboost-dev apt packages are < 1.78, so install from tar.gz\nRUN wget -O /tmp/boost.tar.gz \\\\\n          https://archives.boost.io/release/1.80.0/source/boost_1_80_0.tar.gz \\\\\n      && (cd /tmp && tar xzf boost.tar.gz) \\\\\n      && mv /tmp/boost_1_80_0/boost /usr/include/boost\n\n# Server build requires recent version of CMake (FetchContent required)\nRUN apt update -q=2 \\\\\n      && apt install -y gpg wget \\\\\n      && wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - |  tee /usr/share/keyrings/kitware-archive-keyring.gpg >/dev/null \\\\\n      && . /etc/os-release \\\\\n      && echo \"deb [signed-by=/usr/share/keyrings/kitware-archive-keyring.gpg] https://apt.kitware.com/ubuntu/ $UBUNTU_CODENAME main\" | tee /etc/apt/sources.list.d/kitware.list >/dev/null \\\\\n      && apt-get update -q=2 \\\\\n      && apt-get install -y --no-install-recommends cmake=3.28.3* cmake-data=3.28.3*\n\"\"\"\n\n        if FLAGS.enable_gpu:\n            df += install_dcgm_libraries(argmap[\"DCGM_VERSION\"], target_machine())\n\n    df += \"\"\"\nENV TRITON_SERVER_VERSION ${TRITON_VERSION}\nENV NVIDIA_TRITON_SERVER_VERSION ${TRITON_CONTAINER_VERSION}\n\"\"\"\n\n    # Copy in the triton source. We remove existing contents first in\n    # case the FROM container has something there already.\n    if target_platform() == \"windows\":\n        df += \"\"\"\nWORKDIR /workspace\nRUN rmdir /S/Q * || exit 0\nCOPY . .\n\"\"\"\n    else:\n        df += \"\"\"\nWORKDIR /workspace\nRUN rm -fr *\nCOPY . .\nENTRYPOINT []\n\"\"\"\n\n    with open(os.path.join(ddir, dockerfile_name), \"w\") as dfile:\n        dfile.write(df)\n\n\ndef create_dockerfile_cibase(ddir, dockerfile_name, argmap):\n    df = \"\"\"\nARG TRITON_VERSION={}\nARG TRITON_CONTAINER_VERSION={}\nARG BASE_IMAGE={}\n\"\"\".format(\n        argmap[\"TRITON_VERSION\"],\n        argmap[\"TRITON_CONTAINER_VERSION\"],\n        argmap[\"BASE_IMAGE\"],\n    )\n\n    df += \"\"\"\nFROM ${BASE_IMAGE}\n\nARG TRITON_VERSION\nARG TRITON_CONTAINER_VERSION\n\nCOPY build/ci /workspace\n\nWORKDIR /workspace\n\nENV TRITON_SERVER_VERSION ${TRITON_VERSION}\nENV NVIDIA_TRITON_SERVER_VERSION ${TRITON_CONTAINER_VERSION}\nENV PIP_BREAK_SYSTEM_PACKAGES=1\n\"\"\"\n\n    with open(os.path.join(ddir, dockerfile_name), \"w\") as dfile:\n        dfile.write(df)\n\n\ndef create_dockerfile_linux(\n    ddir, dockerfile_name, argmap, backends, repoagents, caches, endpoints\n):\n    df = \"\"\"\nARG TRITON_VERSION={}\nARG TRITON_CONTAINER_VERSION={}\nARG BASE_IMAGE={}\n\n\"\"\".format(\n        argmap[\"TRITON_VERSION\"],\n        argmap[\"TRITON_CONTAINER_VERSION\"],\n        argmap[\"BASE_IMAGE\"],\n    )\n\n    # PyTorch and TensorFlow backends need extra CUDA and other\n    # dependencies during runtime that are missing in the CPU-only base container.\n    # These dependencies must be copied from the Triton Min image.\n    if not FLAGS.enable_gpu and ((\"pytorch\" in backends) or (\"tensorflow\" in backends)):\n        df += \"\"\"\n############################################################################\n##  Triton Min image\n############################################################################\nFROM {} AS min_container\n\n\"\"\".format(\n            argmap[\"GPU_BASE_IMAGE\"]\n        )\n\n    df += \"\"\"\n############################################################################\n##  Production stage: Create container with just inference server executable\n############################################################################\nFROM ${BASE_IMAGE}\n\nENV PIP_BREAK_SYSTEM_PACKAGES=1\n\"\"\"\n\n    df += dockerfile_prepare_container_linux(\n        argmap, backends, FLAGS.enable_gpu, target_machine()\n    )\n\n    df += \"\"\"\nWORKDIR /opt\nCOPY --chown=1000:1000 build/install tritonserver\n\nWORKDIR /opt/tritonserver\nCOPY --chown=1000:1000 NVIDIA_Deep_Learning_Container_License.pdf .\n\nRUN find /opt/tritonserver/python -maxdepth 1 -type f -name \\\\\n    \"tritonserver-*.whl\" | xargs -I {} pip install --upgrade {}[all] && \\\\\n    find /opt/tritonserver/python -maxdepth 1 -type f -name \\\\\n    \"tritonfrontend-*.whl\" | xargs -I {} pip install --upgrade {}[all]\n\nRUN pip3 install -r python/openai/requirements.txt\n\n\"\"\"\n    if not FLAGS.no_core_build:\n        # Add feature labels for SageMaker endpoint\n        if \"sagemaker\" in endpoints:\n            df += \"\"\"\nLABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\nLABEL com.amazonaws.sagemaker.capabilities.multi-models=true\nCOPY --chown=1000:1000 docker/sagemaker/serve /usr/bin/.\n\"\"\"\n    # This is required since libcublasLt.so is not present during the build\n    # stage of the PyTorch backend\n    if not FLAGS.enable_gpu and (\"pytorch\" in backends):\n        df += \"\"\"\nRUN patchelf --add-needed /usr/local/cuda/lib64/stubs/libcublasLt.so.12 backends/pytorch/libtorch_cuda.so\n\"\"\"\n    if \"tensorrtllm\" in backends:\n        df += \"\"\"\n# Install required packages for TRT-LLM models\n# Remove contents that are not needed in runtime\n# Setuptools has breaking changes in version 70.0.0, so fix it to 69.5.1\n# The generated code in grpc_service_pb2_grpc.py depends on grpcio>=1.64.0, so fix it to 1.64.0\nRUN ldconfig && \\\\\n    ARCH=\"$(uname -i)\" && \\\\\n    rm -fr ${TRT_ROOT}/bin ${TRT_ROOT}/targets/${ARCH}-linux-gnu/bin ${TRT_ROOT}/data && \\\\\n    rm -fr ${TRT_ROOT}/doc ${TRT_ROOT}/onnx_graphsurgeon ${TRT_ROOT}/python && \\\\\n    rm -fr ${TRT_ROOT}/samples ${TRT_ROOT}/targets/${ARCH}-linux-gnu/samples && \\\\\n    pip3 install --no-cache-dir transformers && \\\\\n    find /usr -name libtensorrt_llm.so -exec dirname {} \\; > /etc/ld.so.conf.d/tensorrt-llm.conf && \\\\\n    find /opt/tritonserver -name libtritonserver.so -exec dirname {} \\; > /etc/ld.so.conf.d/triton-tensorrtllm-worker.conf && \\\\\n    pip3 install --no-cache-dir  grpcio-tools==1.64.0 && \\\\\n    pip3 uninstall -y setuptools\nENV LD_LIBRARY_PATH=/usr/local/tensorrt/lib/:/opt/tritonserver/backends/tensorrtllm:$LD_LIBRARY_PATH\n\n# There are some ucc issues when spawning mpi processes with ompi v4.1.7a1.\n# Downgrade to ompi v4.1.5rc2 to avoid the issue.\nRUN rm -fr /opt/hpcx/ompi\nCOPY --from=nvcr.io/nvidia/tritonserver:24.02-py3-min /opt/hpcx/ompi /opt/hpcx/ompi\n\"\"\"\n    with open(os.path.join(ddir, dockerfile_name), \"w\") as dfile:\n        dfile.write(df)\n\n\ndef dockerfile_prepare_container_linux(argmap, backends, enable_gpu, target_machine):\n    gpu_enabled = 1 if enable_gpu else 0\n    # Common steps to produce docker images shared by build.py and compose.py.\n    # Sets environment variables, installs dependencies and adds entrypoint\n    df = \"\"\"\nARG TRITON_VERSION\nARG TRITON_CONTAINER_VERSION\n\nENV TRITON_SERVER_VERSION ${TRITON_VERSION}\nENV NVIDIA_TRITON_SERVER_VERSION ${TRITON_CONTAINER_VERSION}\nLABEL com.nvidia.tritonserver.version=\"${TRITON_SERVER_VERSION}\"\n\nENV PATH /opt/tritonserver/bin:${PATH}\n# Remove once https://github.com/openucx/ucx/pull/9148 is available\n# in the min container.\nENV UCX_MEM_EVENTS no\n\"\"\"\n\n    # Necessary for libtorch.so to find correct HPCX libraries\n    if \"pytorch\" in backends:\n        df += \"\"\"\nENV LD_LIBRARY_PATH /opt/hpcx/ucc/lib/:/opt/hpcx/ucx/lib/:${LD_LIBRARY_PATH}\n\"\"\"\n\n    backend_dependencies = \"\"\n    # libgomp1 is needed by both onnxruntime and pytorch backends\n    if (\"onnxruntime\" in backends) or (\"pytorch\" in backends):\n        backend_dependencies = \"libgomp1\"\n\n    # libgfortran5 is needed by pytorch backend on ARM\n    if (\"pytorch\" in backends) and (target_machine == \"aarch64\"):\n        backend_dependencies += \" libgfortran5\"\n    # openssh-server is needed for fastertransformer\n    if \"fastertransformer\" in backends:\n        backend_dependencies += \" openssh-server\"\n\n    df += \"\"\"\nENV TF_ADJUST_HUE_FUSED         1\nENV TF_ADJUST_SATURATION_FUSED  1\nENV TF_ENABLE_WINOGRAD_NONFUSED 1\nENV TF_AUTOTUNE_THRESHOLD       2\nENV TRITON_SERVER_GPU_ENABLED    {gpu_enabled}\n\n# Create a user that can be used to run triton as\n# non-root. Make sure that this user to given ID 1000. All server\n# artifacts copied below are assign to this user.\nENV TRITON_SERVER_USER=triton-server\nRUN userdel tensorrt-server > /dev/null 2>&1 || true \\\\\n      && userdel ubuntu > /dev/null 2>&1 || true \\\\\n      && if ! id -u $TRITON_SERVER_USER > /dev/null 2>&1 ; then \\\\\n          useradd $TRITON_SERVER_USER; \\\\\n        fi \\\\\n      && [ `id -u $TRITON_SERVER_USER` -eq 1000 ] \\\\\n      && [ `id -g $TRITON_SERVER_USER` -eq 1000 ]\n\"\"\".format(\n        gpu_enabled=gpu_enabled\n    )\n\n    if target_platform() == \"rhel\":\n        df += \"\"\"\n# Common dependencies.\nRUN yum install -y \\\\\n        git \\\\\n        gperf \\\\\n        re2-devel \\\\\n        openssl-devel \\\\\n        libtool \\\\\n        libcurl-devel \\\\\n        libb64-devel \\\\\n        gperftools-devel \\\\\n        patchelf \\\\\n        wget \\\\\n        python3-pip \\\\\n        numactl-devel\n\n\"\"\"\n    else:\n        df += \"\"\"\n# Ensure apt-get won't prompt for selecting options\nENV DEBIAN_FRONTEND=noninteractive\n\n# Common dependencies. FIXME (can any of these be conditional? For\n# example libcurl only needed for GCS?)\nRUN apt-get update \\\\\n      && apt-get install -y --no-install-recommends \\\\\n              clang \\\\\n              curl \\\\\n              dirmngr \\\\\n              git \\\\\n              gperf \\\\\n              libb64-0d \\\\\n              libcurl4-openssl-dev \\\\\n              libgoogle-perftools-dev \\\\\n              libjemalloc-dev \\\\\n              libnuma-dev \\\\\n              software-properties-common \\\\\n              wget \\\\\n              {backend_dependencies} \\\\\n              python3-pip \\\\\n      && rm -rf /var/lib/apt/lists/*\n\"\"\".format(\n            backend_dependencies=backend_dependencies\n        )\n\n    df += \"\"\"\n# Set TCMALLOC_RELEASE_RATE for users setting LD_PRELOAD with tcmalloc\nENV TCMALLOC_RELEASE_RATE 200\n\"\"\"\n\n    if \"fastertransformer\" in backends:\n        be = \"fastertransformer\"\n        url = \"https://raw.githubusercontent.com/triton-inference-server/fastertransformer_backend/{}/docker/create_dockerfile_and_build.py\".format(\n            backends[be]\n        )\n        response = requests.get(url)\n        spec = importlib.util.spec_from_loader(\n            \"fastertransformer_buildscript\", loader=None, origin=url\n        )\n        fastertransformer_buildscript = importlib.util.module_from_spec(spec)\n        exec(response.content, fastertransformer_buildscript.__dict__)\n        df += fastertransformer_buildscript.create_postbuild(is_multistage_build=False)\n\n    if enable_gpu:\n        df += install_dcgm_libraries(argmap[\"DCGM_VERSION\"], target_machine)\n        # This segment will break the RHEL SBSA build. Need to determine whether\n        # this is necessary to incorporate.\n        if target_platform() != \"rhel\":\n            df += \"\"\"\n# Extra defensive wiring for CUDA Compat lib\nRUN ln -sf ${_CUDA_COMPAT_PATH}/lib.real ${_CUDA_COMPAT_PATH}/lib \\\\\n    && echo ${_CUDA_COMPAT_PATH}/lib > /etc/ld.so.conf.d/00-cuda-compat.conf \\\\\n    && ldconfig \\\\\n    && rm -f ${_CUDA_COMPAT_PATH}/lib\n\"\"\"\n    else:\n        df += add_cpu_libs_to_linux_dockerfile(backends, target_machine)\n\n    # Add dependencies needed for python backend\n    if \"python\" in backends:\n        if target_platform() == \"rhel\":\n            df += \"\"\"\n# python3, python3-pip and some pip installs required for the python backend\nRUN yum install -y \\\\\n        libarchive-devel \\\\\n        openssl-devel \\\\\n        readline-devel\n\"\"\"\n            # Requires openssl-devel to be installed first for pyenv build to be successful\n            df += change_default_python_version_rhel(FLAGS.rhel_py_version)\n            df += \"\"\"\nRUN pip3 install --upgrade pip \\\\\n    && pip3 install --upgrade \\\\\n        wheel \\\\\n        setuptools \\\\\n        \\\"numpy<2\\\" \\\\\n        virtualenv\n\"\"\"\n        else:\n            df += \"\"\"\n# python3, python3-pip and some pip installs required for the python backend\nRUN apt-get update \\\\\n      && apt-get install -y --no-install-recommends \\\\\n            python3 \\\\\n            libarchive-dev \\\\\n            python3-pip \\\\\n            python3-wheel \\\\\n            python3-setuptools \\\\\n            libpython3-dev \\\\\n      && pip3 install --upgrade \\\\\n            \\\"numpy<2\\\" \\\\\n            virtualenv \\\\\n      && rm -rf /var/lib/apt/lists/*\n\"\"\"\n    if \"tensorrtllm\" in backends:\n        df += \"\"\"\n# Updating the openssh-client to fix for the CVE-2024-6387. This can be removed when trtllm uses a later CUDA container(12.5 or later)\nRUN apt-get update \\\\\n    && apt-get install -y --no-install-recommends \\\\\n        openssh-client \\\\\n    && rm -rf /var/lib/apt/lists/*\n    \"\"\"\n\n    if \"vllm\" in backends:\n        df += \"\"\"\n# vLLM needed for vLLM backend\nRUN pip3 install vllm=={}\n\"\"\".format(\n            FLAGS.vllm_version\n        )\n\n    if \"dali\" in backends:\n        df += \"\"\"\n# Update Python path to include DALI\nENV PYTHONPATH=/opt/tritonserver/backends/dali/wheel/dali:$PYTHONPATH\n\"\"\"\n\n    df += \"\"\"\nWORKDIR /opt/tritonserver\nRUN rm -fr /opt/tritonserver/*\nENV NVIDIA_PRODUCT_NAME=\"Triton Server\"\nCOPY docker/entrypoint.d/ /opt/nvidia/entrypoint.d/\n\"\"\"\n\n    # The CPU-only build uses ubuntu as the base image, and so the\n    # entrypoint files are not available in /opt/nvidia in the base\n    # image, so we must provide them ourselves.\n    if not enable_gpu:\n        df += \"\"\"\nCOPY docker/cpu_only/ /opt/nvidia/\nENTRYPOINT [\"/opt/nvidia/nvidia_entrypoint.sh\"]\n\"\"\"\n\n    df += \"\"\"\nENV NVIDIA_BUILD_ID {}\nLABEL com.nvidia.build.id={}\nLABEL com.nvidia.build.ref={}\n\"\"\".format(\n        argmap[\"NVIDIA_BUILD_ID\"], argmap[\"NVIDIA_BUILD_ID\"], argmap[\"NVIDIA_BUILD_REF\"]\n    )\n    return df\n\n\ndef add_cpu_libs_to_linux_dockerfile(backends, target_machine):\n    df = \"\"\n    libs_arch = \"aarch64\" if target_machine == \"aarch64\" else \"x86_64\"\n    if \"pytorch\" in backends:\n        # Add extra dependencies for pytorch backend.\n        # Note: Even though the build is CPU-only, the version of pytorch\n        # we are using depend upon libraries like cuda and cudnn. Since\n        # these dependencies are not present in the ubuntu base image,\n        # we must copy these from the Triton min container ourselves.\n        cuda_arch = \"sbsa\" if target_machine == \"aarch64\" else \"x86_64\"\n        df += \"\"\"\nRUN mkdir -p /usr/local/cuda/lib64/stubs\nCOPY --from=min_container /usr/local/cuda/lib64/stubs/libcusparse.so /usr/local/cuda/lib64/stubs/libcusparse.so.12\nCOPY --from=min_container /usr/local/cuda/lib64/stubs/libcusolver.so /usr/local/cuda/lib64/stubs/libcusolver.so.11\nCOPY --from=min_container /usr/local/cuda/lib64/stubs/libcurand.so /usr/local/cuda/lib64/stubs/libcurand.so.10\nCOPY --from=min_container /usr/local/cuda/lib64/stubs/libcufft.so /usr/local/cuda/lib64/stubs/libcufft.so.11\nCOPY --from=min_container /usr/local/cuda/lib64/stubs/libcublas.so /usr/local/cuda/lib64/stubs/libcublas.so.12\nCOPY --from=min_container /usr/local/cuda/lib64/stubs/libcublasLt.so /usr/local/cuda/lib64/stubs/libcublasLt.so.12\nCOPY --from=min_container /usr/local/cuda/lib64/stubs/libcublasLt.so /usr/local/cuda/lib64/stubs/libcublasLt.so.11\n\nRUN mkdir -p /usr/local/cuda/targets/{cuda_arch}-linux/lib\nCOPY --from=min_container /usr/local/cuda/lib64/libcudart.so.12 /usr/local/cuda/targets/{cuda_arch}-linux/lib/.\nCOPY --from=min_container /usr/local/cuda/lib64/libcupti.so.12 /usr/local/cuda/targets/{cuda_arch}-linux/lib/.\nCOPY --from=min_container /usr/local/cuda/lib64/libnvToolsExt.so.1 /usr/local/cuda/targets/{cuda_arch}-linux/lib/.\nCOPY --from=min_container /usr/local/cuda/lib64/libnvJitLink.so.12 /usr/local/cuda/targets/{cuda_arch}-linux/lib/.\n\nRUN mkdir -p /opt/hpcx/ucc/lib/ /opt/hpcx/ucx/lib/\nCOPY --from=min_container /opt/hpcx/ucc/lib/libucc.so.1 /opt/hpcx/ucc/lib/libucc.so.1\nCOPY --from=min_container /opt/hpcx/ucx/lib/libucm.so.0 /opt/hpcx/ucx/lib/libucm.so.0\nCOPY --from=min_container /opt/hpcx/ucx/lib/libucp.so.0 /opt/hpcx/ucx/lib/libucp.so.0\nCOPY --from=min_container /opt/hpcx/ucx/lib/libucs.so.0 /opt/hpcx/ucx/lib/libucs.so.0\nCOPY --from=min_container /opt/hpcx/ucx/lib/libuct.so.0 /opt/hpcx/ucx/lib/libuct.so.0\n\nCOPY --from=min_container /usr/lib/{libs_arch}-linux-gnu/libcudnn.so.9 /usr/lib/{libs_arch}-linux-gnu/libcudnn.so.9\n\n# patchelf is needed to add deps of libcublasLt.so.12 to libtorch_cuda.so\nRUN apt-get update \\\\\n      && apt-get install -y --no-install-recommends openmpi-bin patchelf\n\nENV LD_LIBRARY_PATH /usr/local/cuda/targets/{cuda_arch}-linux/lib:/usr/local/cuda/lib64/stubs:${{LD_LIBRARY_PATH}}\n\"\"\".format(\n            cuda_arch=cuda_arch, libs_arch=libs_arch\n        )\n\n    if (\"pytorch\" in backends) or (\"tensorflow\" in backends):\n        # Add NCCL dependency for tensorflow/pytorch backend.\n        # Note: Even though the build is CPU-only, the version of\n        # tensorflow/pytorch we are using depends upon the NCCL library.\n        # Since this dependency is not present in the ubuntu base image,\n        # we must copy it from the Triton min container ourselves.\n        df += \"\"\"\nCOPY --from=min_container /usr/lib/{libs_arch}-linux-gnu/libnccl.so.2 /usr/lib/{libs_arch}-linux-gnu/libnccl.so.2\n\"\"\".format(\n            libs_arch=libs_arch\n        )\n\n    return df\n\n\ndef change_default_python_version_rhel(version):\n    df = f\"\"\"\n# The python library version available for install via 'yum install python3.X-devel' does not\n# match the version of python inside the RHEL base container. This means that python packages\n# installed within the container will not be picked up by the python backend stub process pybind\n# bindings. It must instead must be installed via pyenv.\nENV PYENV_ROOT=/opt/pyenv_build\nRUN curl https://pyenv.run | bash\nENV PATH=\"${{PYENV_ROOT}}/bin:$PATH\"\nRUN eval \"$(pyenv init -)\"\nRUN CONFIGURE_OPTS=\\\"--with-openssl=/usr/lib64\\\" && pyenv install {version} \\\\\n    && cp ${{PYENV_ROOT}}/versions/{version}/lib/libpython3* /usr/lib64/\n\n# RHEL image has several python versions. It's important\n# to set the correct version, otherwise, packages that are\n# pip installed will not be found during testing.\nENV PYVER={version} PYTHONPATH=/opt/python/v\nRUN ln -sf ${{PYENV_ROOT}}/versions/${{PYVER}}* ${{PYTHONPATH}}\nENV PYBIN=${{PYTHONPATH}}/bin\nENV PYTHON_BIN_PATH=${{PYBIN}}/python${{PYVER}} PATH=${{PYBIN}}:${{PATH}}\n\"\"\"\n    return df\n\n\ndef create_dockerfile_windows(\n    ddir, dockerfile_name, argmap, backends, repoagents, caches\n):\n    df = \"\"\"\nARG TRITON_VERSION={}\nARG TRITON_CONTAINER_VERSION={}\nARG BASE_IMAGE={}\n\n############################################################################\n##  Production stage: Create container with just inference server executable\n############################################################################\nFROM ${{BASE_IMAGE}}\n\nARG TRITON_VERSION\nARG TRITON_CONTAINER_VERSION\n\nENV TRITON_SERVER_VERSION ${{TRITON_VERSION}}\nENV NVIDIA_TRITON_SERVER_VERSION ${{TRITON_CONTAINER_VERSION}}\nLABEL com.nvidia.tritonserver.version=\"${{TRITON_SERVER_VERSION}}\"\n\nRUN setx path \"%path%;C:\\opt\\tritonserver\\bin\"\n\n\"\"\".format(\n        argmap[\"TRITON_VERSION\"],\n        argmap[\"TRITON_CONTAINER_VERSION\"],\n        argmap[\"BASE_IMAGE\"],\n    )\n    df += \"\"\"\nWORKDIR /opt\nRUN rmdir /S/Q tritonserver || exit 0\nCOPY --chown=1000:1000 build/install tritonserver\n\nWORKDIR /opt/tritonserver\nCOPY --chown=1000:1000 NVIDIA_Deep_Learning_Container_License.pdf .\n\n\"\"\"\n    df += \"\"\"\nENTRYPOINT []\nENV NVIDIA_BUILD_ID {}\nLABEL com.nvidia.build.id={}\nLABEL com.nvidia.build.ref={}\n\"\"\".format(\n        argmap[\"NVIDIA_BUILD_ID\"], argmap[\"NVIDIA_BUILD_ID\"], argmap[\"NVIDIA_BUILD_REF\"]\n    )\n\n    with open(os.path.join(ddir, dockerfile_name), \"w\") as dfile:\n        dfile.write(df)\n\n\ndef create_build_dockerfiles(\n    container_build_dir, images, backends, repoagents, caches, endpoints\n):\n    if \"base\" in images:\n        base_image = images[\"base\"]\n    elif target_platform() == \"windows\":\n        base_image = \"mcr.microsoft.com/dotnet/framework/sdk:4.8\"\n    elif FLAGS.enable_gpu:\n        base_image = \"nvcr.io/nvidia/tritonserver:{}-py3-min\".format(\n            FLAGS.upstream_container_version\n        )\n    else:\n        base_image = \"ubuntu:24.04\"\n\n    dockerfileargmap = {\n        \"NVIDIA_BUILD_REF\": \"\" if FLAGS.build_sha is None else FLAGS.build_sha,\n        \"NVIDIA_BUILD_ID\": \"<unknown>\" if FLAGS.build_id is None else FLAGS.build_id,\n        \"TRITON_VERSION\": FLAGS.version,\n        \"TRITON_CONTAINER_VERSION\": FLAGS.container_version,\n        \"BASE_IMAGE\": base_image,\n        \"DCGM_VERSION\": FLAGS.dcgm_version,\n    }\n\n    # For CPU-only image we need to copy some cuda libraries and dependencies\n    # since we are using PyTorch and TensorFlow containers that\n    # are not CPU-only.\n    if (\n        not FLAGS.enable_gpu\n        and ((\"pytorch\" in backends) or (\"tensorflow\" in backends))\n        and (target_platform() != \"windows\")\n    ):\n        if \"gpu-base\" in images:\n            gpu_base_image = images[\"gpu-base\"]\n        else:\n            gpu_base_image = \"nvcr.io/nvidia/tritonserver:{}-py3-min\".format(\n                FLAGS.upstream_container_version\n            )\n        dockerfileargmap[\"GPU_BASE_IMAGE\"] = gpu_base_image\n\n    if target_platform() == \"rhel\":\n        create_dockerfile_buildbase_rhel(\n            FLAGS.build_dir, \"Dockerfile.buildbase\", dockerfileargmap\n        )\n    else:\n        create_dockerfile_buildbase(\n            FLAGS.build_dir, \"Dockerfile.buildbase\", dockerfileargmap\n        )\n\n    if target_platform() == \"windows\":\n        create_dockerfile_windows(\n            FLAGS.build_dir,\n            \"Dockerfile\",\n            dockerfileargmap,\n            backends,\n            repoagents,\n            caches,\n        )\n    else:\n        create_dockerfile_linux(\n            FLAGS.build_dir,\n            \"Dockerfile\",\n            dockerfileargmap,\n            backends,\n            repoagents,\n            caches,\n            endpoints,\n        )\n\n    # Dockerfile used for the creating the CI base image.\n    create_dockerfile_cibase(FLAGS.build_dir, \"Dockerfile.cibase\", dockerfileargmap)\n\n\ndef create_docker_build_script(script_name, container_install_dir, container_ci_dir):\n    with BuildScript(\n        os.path.join(FLAGS.build_dir, script_name),\n        verbose=FLAGS.verbose,\n        desc=(\"Docker-based build script for Triton Inference Server\"),\n    ) as docker_script:\n        #\n        # Build base image... tritonserver_buildbase\n        #\n        docker_script.commentln(8)\n        docker_script.comment(\"Create Triton base build image\")\n        docker_script.comment(\n            \"This image contains all dependencies necessary to build Triton\"\n        )\n        docker_script.comment()\n\n        cachefrommap = [\n            \"tritonserver_buildbase\",\n            \"tritonserver_buildbase_cache0\",\n            \"tritonserver_buildbase_cache1\",\n        ]\n\n        baseargs = [\n            \"docker\",\n            \"build\",\n            \"-t\",\n            \"tritonserver_buildbase\",\n            \"-f\",\n            os.path.join(FLAGS.build_dir, \"Dockerfile.buildbase\"),\n        ]\n\n        if not FLAGS.no_container_pull:\n            baseargs += [\n                \"--pull\",\n            ]\n\n        # Windows docker runs in a VM and memory needs to be specified\n        # explicitly (at least for some configurations of docker).\n        if target_platform() == \"windows\":\n            if FLAGS.container_memory:\n                baseargs += [\"--memory\", FLAGS.container_memory]\n\n        if target_platform() != \"windows\":\n            baseargs += [\"--cache-from={}\".format(k) for k in cachefrommap]\n\n        baseargs += [\".\"]\n\n        docker_script.cwd(THIS_SCRIPT_DIR)\n        docker_script.cmd(baseargs, check_exitcode=True)\n\n        #\n        # Build...\n        #\n        docker_script.blankln()\n        docker_script.commentln(8)\n        docker_script.comment(\"Run build in tritonserver_buildbase container\")\n        docker_script.comment(\"Mount a directory into the container where the install\")\n        docker_script.comment(\"artifacts will be placed.\")\n        docker_script.comment()\n\n        # Don't use '-v' to communicate the built artifacts out of the\n        # build, because we want this code to work even if run within\n        # Docker (i.e. docker-in-docker) and not just if run directly\n        # from host.\n        runargs = [\n            \"docker\",\n            \"run\",\n            \"-w\",\n            \"/workspace/build\",\n            \"--name\",\n            \"tritonserver_builder\",\n        ]\n\n        if not FLAGS.no_container_interactive:\n            runargs += [\"-it\"]\n\n        if target_platform() == \"windows\":\n            if FLAGS.container_memory:\n                runargs += [\"--memory\", FLAGS.container_memory]\n            runargs += [\"-v\", \"\\\\\\\\.\\pipe\\docker_engine:\\\\\\\\.\\pipe\\docker_engine\"]\n        else:\n            runargs += [\"-v\", \"/var/run/docker.sock:/var/run/docker.sock\"]\n\n        runargs += [\"tritonserver_buildbase\"]\n\n        if target_platform() == \"windows\":\n            runargs += [\"powershell.exe\", \"-noexit\", \"-File\", \"./cmake_build.ps1\"]\n        else:\n            runargs += [\"./cmake_build\"]\n\n        # Remove existing tritonserver_builder container...\n        if target_platform() == \"windows\":\n            docker_script.cmd([\"docker\", \"rm\", \"tritonserver_builder\"])\n        else:\n            docker_script._file.write(\n                'if [ \"$(docker ps -a | grep tritonserver_builder)\" ]; then  docker rm -f tritonserver_builder; fi\\n'\n            )\n\n        docker_script.cmd(runargs, check_exitcode=True)\n\n        docker_script.cmd(\n            [\n                \"docker\",\n                \"cp\",\n                \"tritonserver_builder:/tmp/tritonbuild/install\",\n                FLAGS.build_dir,\n            ],\n            check_exitcode=True,\n        )\n        docker_script.cmd(\n            [\n                \"docker\",\n                \"cp\",\n                \"tritonserver_builder:/tmp/tritonbuild/ci\",\n                FLAGS.build_dir,\n            ],\n            check_exitcode=True,\n        )\n\n        #\n        # Final image... tritonserver\n        #\n        docker_script.blankln()\n        docker_script.commentln(8)\n        docker_script.comment(\"Create final tritonserver image\")\n        docker_script.comment()\n\n        finalargs = [\n            \"docker\",\n            \"build\",\n            \"-t\",\n            \"tritonserver\",\n            \"-f\",\n            os.path.join(FLAGS.build_dir, \"Dockerfile\"),\n            \".\",\n        ]\n\n        docker_script.cwd(THIS_SCRIPT_DIR)\n        docker_script.cmd(finalargs, check_exitcode=True)\n\n        #\n        # CI base image... tritonserver_cibase\n        #\n        docker_script.blankln()\n        docker_script.commentln(8)\n        docker_script.comment(\"Create CI base image\")\n        docker_script.comment()\n\n        cibaseargs = [\n            \"docker\",\n            \"build\",\n            \"-t\",\n            \"tritonserver_cibase\",\n            \"-f\",\n            os.path.join(FLAGS.build_dir, \"Dockerfile.cibase\"),\n            \".\",\n        ]\n\n        docker_script.cwd(THIS_SCRIPT_DIR)\n        docker_script.cmd(cibaseargs, check_exitcode=True)\n\n\ndef core_build(\n    cmake_script, repo_dir, cmake_dir, build_dir, install_dir, components, backends\n):\n    repo_build_dir = os.path.join(build_dir, \"tritonserver\", \"build\")\n    repo_install_dir = os.path.join(build_dir, \"tritonserver\", \"install\")\n\n    cmake_script.commentln(8)\n    cmake_script.comment(\"Triton core library and tritonserver executable\")\n    cmake_script.comment()\n    cmake_script.mkdir(repo_build_dir)\n    cmake_script.cwd(repo_build_dir)\n    cmake_script.cmake(\n        core_cmake_args(components, backends, cmake_dir, repo_install_dir)\n    )\n    cmake_script.makeinstall()\n\n    if target_platform() == \"windows\":\n        cmake_script.mkdir(os.path.join(install_dir, \"bin\"))\n        cmake_script.cp(\n            os.path.join(repo_install_dir, \"bin\", \"tritonserver.exe\"),\n            os.path.join(install_dir, \"bin\"),\n        )\n        cmake_script.cp(\n            os.path.join(repo_install_dir, \"bin\", \"tritonserver.dll\"),\n            os.path.join(install_dir, \"bin\"),\n        )\n        cmake_script.cp(\n            os.path.join(repo_install_dir, \"lib\", \"tritonserver.lib\"),\n            os.path.join(install_dir, \"bin\"),\n        )\n    elif target_platform() == \"rhel\":\n        cmake_script.mkdir(os.path.join(install_dir, \"bin\"))\n        cmake_script.cp(\n            os.path.join(repo_install_dir, \"bin\", \"tritonserver\"),\n            os.path.join(install_dir, \"bin\"),\n        )\n        cmake_script.mkdir(os.path.join(install_dir, \"lib64\"))\n        cmake_script.cp(\n            os.path.join(repo_install_dir, \"lib64\", \"libtritonserver.so\"),\n            os.path.join(install_dir, \"lib64\"),\n        )\n    else:\n        cmake_script.mkdir(os.path.join(install_dir, \"bin\"))\n        cmake_script.cp(\n            os.path.join(repo_install_dir, \"bin\", \"tritonserver\"),\n            os.path.join(install_dir, \"bin\"),\n        )\n        cmake_script.mkdir(os.path.join(install_dir, \"lib\"))\n        cmake_script.cp(\n            os.path.join(repo_install_dir, \"lib\", \"libtritonserver.so\"),\n            os.path.join(install_dir, \"lib\"),\n        )\n    # [FIXME] Placing the tritonserver and tritonfrontend wheel files in 'python' for now,\n    # should be uploaded to pip registry to be able to install directly\n    cmake_script.mkdir(os.path.join(install_dir, \"python\"))\n    cmake_script.cp(\n        os.path.join(repo_install_dir, \"python\", \"triton*.whl\"),\n        os.path.join(install_dir, \"python\"),\n    )\n\n    cmake_script.mkdir(os.path.join(install_dir, \"include\", \"triton\"))\n    cmake_script.cpdir(\n        os.path.join(repo_install_dir, \"include\", \"triton\", \"core\"),\n        os.path.join(install_dir, \"include\", \"triton\", \"core\"),\n    )\n\n    cmake_script.cpdir(\n        os.path.join(repo_dir, \"python\", \"openai\"), os.path.join(install_dir, \"python\")\n    )\n\n    cmake_script.cp(os.path.join(repo_dir, \"LICENSE\"), install_dir)\n    cmake_script.cp(os.path.join(repo_dir, \"TRITON_VERSION\"), install_dir)\n\n    # If requested, package the source code for all OSS used to build\n    # For windows, Triton is not delivered as a container so skip for\n    # windows platform.\n    if target_platform() != \"windows\":\n        if (\n            (not FLAGS.no_container_build)\n            and (not FLAGS.no_core_build)\n            and (not FLAGS.no_container_source)\n        ):\n            cmake_script.mkdir(os.path.join(install_dir, \"third-party-src\"))\n            cmake_script.cwd(repo_build_dir)\n            cmake_script.tar(\n                \"third-party-src\",\n                os.path.join(install_dir, \"third-party-src\", \"src.tar.gz\"),\n            )\n            cmake_script.cp(\n                os.path.join(repo_dir, \"docker\", \"README.third-party-src\"),\n                os.path.join(install_dir, \"third-party-src\", \"README\"),\n            )\n\n    cmake_script.comment()\n    cmake_script.comment(\"end Triton core library and tritonserver executable\")\n    cmake_script.commentln(8)\n    cmake_script.blankln()\n\n\ndef tensorrtllm_prebuild(cmake_script):\n    # Export the TRT_ROOT environment variable\n    cmake_script.cmd(\"export TRT_ROOT=/usr/local/tensorrt\")\n    cmake_script.cmd(\"export ARCH=$(uname -m)\")\n    cmake_script.cmd(\n        'export LD_LIBRARY_PATH=\"/usr/local/cuda/compat/lib.real:${LD_LIBRARY_PATH}\"'\n    )\n\n\ndef tensorrtllm_postbuild(cmake_script, repo_install_dir, tensorrtllm_be_dir):\n    # TODO: Update the CMakeLists.txt of TRT-LLM backend to install the artifacts to the correct location\n    cmake_destination_dir = os.path.join(repo_install_dir, \"backends/tensorrtllm\")\n    cmake_script.mkdir(cmake_destination_dir)\n\n    # Copy over the TRT-LLM backend libraries\n    cmake_script.cp(\n        os.path.join(tensorrtllm_be_dir, \"build\", \"libtriton_tensorrtllm*.so\"),\n        cmake_destination_dir,\n    )\n    cmake_script.cp(\n        os.path.join(tensorrtllm_be_dir, \"build\", \"trtllmExecutorWorker\"),\n        cmake_destination_dir,\n    )\n\n\ndef backend_build(\n    be,\n    cmake_script,\n    tag,\n    build_dir,\n    install_dir,\n    github_organization,\n    images,\n    components,\n    library_paths,\n):\n    repo_build_dir = os.path.join(build_dir, be, \"build\")\n    repo_install_dir = os.path.join(build_dir, be, \"install\")\n\n    cmake_script.commentln(8)\n    cmake_script.comment(f\"'{be}' backend\")\n    cmake_script.comment(\"Delete this section to remove backend from build\")\n    cmake_script.comment()\n    cmake_script.mkdir(build_dir)\n    cmake_script.cwd(build_dir)\n    cmake_script.gitclone(backend_repo(be), tag, be, github_organization)\n\n    if be == \"tensorrtllm\":\n        tensorrtllm_prebuild(cmake_script)\n\n    cmake_script.mkdir(repo_build_dir)\n    cmake_script.cwd(repo_build_dir)\n    cmake_script.cmake(\n        backend_cmake_args(images, components, be, repo_install_dir, library_paths)\n    )\n    cmake_script.makeinstall()\n\n    if be == \"tensorrtllm\":\n        tensorrtllm_be_dir = os.path.join(build_dir, be)\n        tensorrtllm_postbuild(cmake_script, repo_install_dir, tensorrtllm_be_dir)\n\n    cmake_script.mkdir(os.path.join(install_dir, \"backends\"))\n    cmake_script.rmdir(os.path.join(install_dir, \"backends\", be))\n\n    # The python library version available for install via 'yum install python3.X-devel' does not\n    # match the version of python inside the RHEL base container. This means that python packages\n    # installed within the container will not be picked up by the python backend stub process pybind\n    # bindings. It must instead must be installed via pyenv. We package it here for better usability.\n    if target_platform() == \"rhel\" and be == \"python\":\n        major_minor_version = \".\".join((FLAGS.rhel_py_version).split(\".\")[:2])\n        version_matched_files = \"/usr/lib64/libpython\" + major_minor_version + \"*\"\n        cmake_script.cp(\n            version_matched_files, os.path.join(repo_install_dir, \"backends\", be)\n        )\n\n    cmake_script.cpdir(\n        os.path.join(repo_install_dir, \"backends\", be),\n        os.path.join(install_dir, \"backends\"),\n    )\n\n    cmake_script.comment()\n    cmake_script.comment(f\"end '{be}' backend\")\n    cmake_script.commentln(8)\n    cmake_script.blankln()\n\n\ndef backend_clone(\n    be,\n    clone_script,\n    tag,\n    build_dir,\n    install_dir,\n    github_organization,\n):\n    clone_script.commentln(8)\n    clone_script.comment(f\"'{be}' backend\")\n    clone_script.comment(\"Delete this section to remove backend from build\")\n    clone_script.comment()\n    clone_script.mkdir(build_dir)\n    clone_script.cwd(build_dir)\n    clone_script.gitclone(backend_repo(be), tag, be, github_organization)\n\n    repo_target_dir = os.path.join(install_dir, \"backends\")\n    clone_script.mkdir(repo_target_dir)\n    backend_dir = os.path.join(repo_target_dir, be)\n    clone_script.rmdir(backend_dir)\n    clone_script.mkdir(backend_dir)\n\n    clone_script.cp(\n        os.path.join(build_dir, be, \"src\", \"model.py\"),\n        backend_dir,\n    )\n    clone_script.cpdir(\n        os.path.join(build_dir, be, \"src\", \"utils\"),\n        backend_dir,\n    )\n\n    clone_script.comment()\n    clone_script.comment(f\"end '{be}' backend\")\n    clone_script.commentln(8)\n    clone_script.blankln()\n\n\ndef repo_agent_build(\n    ra, cmake_script, build_dir, install_dir, repoagent_repo, repoagents\n):\n    repo_build_dir = os.path.join(build_dir, ra, \"build\")\n    repo_install_dir = os.path.join(build_dir, ra, \"install\")\n\n    cmake_script.commentln(8)\n    cmake_script.comment(f\"'{ra}' repository agent\")\n    cmake_script.comment(\"Delete this section to remove repository agent from build\")\n    cmake_script.comment()\n    cmake_script.mkdir(build_dir)\n    cmake_script.cwd(build_dir)\n    cmake_script.gitclone(\n        repoagent_repo(ra), repoagents[ra], ra, FLAGS.github_organization\n    )\n\n    cmake_script.mkdir(repo_build_dir)\n    cmake_script.cwd(repo_build_dir)\n    cmake_script.cmake(repoagent_cmake_args(images, components, ra, repo_install_dir))\n    cmake_script.makeinstall()\n\n    cmake_script.mkdir(os.path.join(install_dir, \"repoagents\"))\n    cmake_script.rmdir(os.path.join(install_dir, \"repoagents\", ra))\n    cmake_script.cpdir(\n        os.path.join(repo_install_dir, \"repoagents\", ra),\n        os.path.join(install_dir, \"repoagents\"),\n    )\n    cmake_script.comment()\n    cmake_script.comment(f\"end '{ra}' repository agent\")\n    cmake_script.commentln(8)\n    cmake_script.blankln()\n\n\ndef cache_build(cache, cmake_script, build_dir, install_dir, cache_repo, caches):\n    repo_build_dir = os.path.join(build_dir, cache, \"build\")\n    repo_install_dir = os.path.join(build_dir, cache, \"install\")\n\n    cmake_script.commentln(8)\n    cmake_script.comment(f\"'{cache}' cache\")\n    cmake_script.comment(\"Delete this section to remove cache from build\")\n    cmake_script.comment()\n    cmake_script.mkdir(build_dir)\n    cmake_script.cwd(build_dir)\n    cmake_script.gitclone(\n        cache_repo(cache), caches[cache], cache, FLAGS.github_organization\n    )\n\n    cmake_script.mkdir(repo_build_dir)\n    cmake_script.cwd(repo_build_dir)\n    cmake_script.cmake(cache_cmake_args(images, components, cache, repo_install_dir))\n    cmake_script.makeinstall()\n\n    cmake_script.mkdir(os.path.join(install_dir, \"caches\"))\n    cmake_script.rmdir(os.path.join(install_dir, \"caches\", cache))\n    cmake_script.cpdir(\n        os.path.join(repo_install_dir, \"caches\", cache),\n        os.path.join(install_dir, \"caches\"),\n    )\n    cmake_script.comment()\n    cmake_script.comment(f\"end '{cache}' cache\")\n    cmake_script.commentln(8)\n    cmake_script.blankln()\n\n\ndef cibase_build(\n    cmake_script, repo_dir, cmake_dir, build_dir, install_dir, ci_dir, backends\n):\n    repo_install_dir = os.path.join(build_dir, \"tritonserver\", \"install\")\n\n    cmake_script.commentln(8)\n    cmake_script.comment(\"Collect Triton CI artifacts\")\n    cmake_script.comment()\n\n    cmake_script.mkdir(ci_dir)\n\n    # On windows we are not yet using a CI/QA docker image for\n    # testing, so don't do anything...\n    if target_platform() == \"windows\":\n        return\n\n    # The core build produces some artifacts that are needed for CI\n    # testing, so include those in the install.\n    cmake_script.cpdir(os.path.join(repo_dir, \"qa\"), ci_dir)\n    cmake_script.cpdir(os.path.join(repo_dir, \"deploy\"), ci_dir)\n    cmake_script.mkdir(os.path.join(ci_dir, \"docs\"))\n    cmake_script.cpdir(\n        os.path.join(repo_dir, \"docs\", \"examples\"), os.path.join(ci_dir, \"docs\")\n    )\n    cmake_script.mkdir(os.path.join(ci_dir, \"src\", \"test\"))\n    cmake_script.cpdir(\n        os.path.join(repo_dir, \"src\", \"test\", \"models\"),\n        os.path.join(ci_dir, \"src\", \"test\"),\n    )\n    # Skip copying the artifacts in the bin, lib, and python as those directories will\n    # be missing when the core build is not enabled.\n    if not FLAGS.no_core_build:\n        cmake_script.cpdir(os.path.join(repo_install_dir, \"bin\"), ci_dir)\n        cmake_script.mkdir(os.path.join(ci_dir, \"lib\"))\n        cmake_script.cp(\n            os.path.join(repo_install_dir, \"lib\", \"libtritonrepoagent_relocation.so\"),\n            os.path.join(ci_dir, \"lib\"),\n        )\n        cmake_script.cpdir(os.path.join(repo_install_dir, \"python\"), ci_dir)\n\n    # Some of the backends are needed for CI testing\n    cmake_script.mkdir(os.path.join(ci_dir, \"backends\"))\n    for be in (\"identity\", \"repeat\", \"square\"):\n        be_install_dir = os.path.join(build_dir, be, \"install\", \"backends\", be)\n        if target_platform() == \"windows\":\n            cmake_script.cmd(f\"if (Test-Path -Path {be_install_dir}) {{\")\n        else:\n            cmake_script.cmd(f\"if [[ -e {be_install_dir} ]]; then\")\n        cmake_script.cpdir(be_install_dir, os.path.join(ci_dir, \"backends\"))\n        cmake_script.cmd(\"}\" if target_platform() == \"windows\" else \"fi\")\n\n    # Some of the unit-test built backends are needed for CI testing\n    cmake_script.mkdir(os.path.join(ci_dir, \"tritonbuild\", \"tritonserver\", \"backends\"))\n    for be in (\n        \"query\",\n        \"implicit_state\",\n        \"sequence\",\n        \"dyna_sequence\",\n        \"distributed_addsub\",\n        \"iterative_sequence\",\n    ):\n        be_install_dir = os.path.join(repo_install_dir, \"backends\", be)\n        if target_platform() == \"windows\":\n            cmake_script.cmd(f\"if (Test-Path -Path {be_install_dir}) {{\")\n        else:\n            cmake_script.cmd(f\"if [[ -e {be_install_dir} ]]; then\")\n        cmake_script.cpdir(\n            be_install_dir,\n            os.path.join(ci_dir, \"tritonbuild\", \"tritonserver\", \"backends\"),\n        )\n        cmake_script.cmd(\"}\" if target_platform() == \"windows\" else \"fi\")\n\n    # The onnxruntime_backend build produces some artifacts that\n    # are needed for CI testing.\n    if \"onnxruntime\" in backends:\n        ort_install_dir = os.path.join(build_dir, \"onnxruntime\", \"install\")\n        cmake_script.mkdir(os.path.join(ci_dir, \"qa\", \"L0_custom_ops\"))\n        if target_platform() != \"igpu\":\n            cmake_script.cp(\n                os.path.join(ort_install_dir, \"test\", \"libcustom_op_library.so\"),\n                os.path.join(ci_dir, \"qa\", \"L0_custom_ops\"),\n            )\n            cmake_script.cp(\n                os.path.join(ort_install_dir, \"test\", \"custom_op_test.onnx\"),\n                os.path.join(ci_dir, \"qa\", \"L0_custom_ops\"),\n            )\n        # [WIP] other way than wildcard?\n        backend_tests = os.path.join(build_dir, \"onnxruntime\", \"test\", \"*\")\n        cmake_script.cpdir(backend_tests, os.path.join(ci_dir, \"qa\"))\n\n    # Need the build area for some backends so that they can be\n    # rebuilt with specific options.\n    cmake_script.mkdir(os.path.join(ci_dir, \"tritonbuild\"))\n    for be in (\"identity\", \"python\"):\n        if be in backends:\n            cmake_script.rmdir(os.path.join(build_dir, be, \"build\"))\n            cmake_script.rmdir(os.path.join(build_dir, be, \"install\"))\n            cmake_script.cpdir(\n                os.path.join(build_dir, be), os.path.join(ci_dir, \"tritonbuild\")\n            )\n\n    cmake_script.comment()\n    cmake_script.comment(\"end Triton CI artifacts\")\n    cmake_script.commentln(8)\n    cmake_script.blankln()\n\n\ndef finalize_build(cmake_script, install_dir, ci_dir):\n    cmake_script.cmd(f\"chmod -R a+rw {install_dir}\")\n    cmake_script.cmd(f\"chmod -R a+rw {ci_dir}\")\n\n\ndef enable_all():\n    if target_platform() != \"windows\":\n        all_backends = [\n            \"ensemble\",\n            \"identity\",\n            \"square\",\n            \"repeat\",\n            \"tensorflow\",\n            \"onnxruntime\",\n            \"python\",\n            \"dali\",\n            \"pytorch\",\n            \"openvino\",\n            \"fil\",\n            \"tensorrt\",\n        ]\n        all_repoagents = [\"checksum\"]\n        all_caches = [\"local\", \"redis\"]\n        all_filesystems = [\"gcs\", \"s3\", \"azure_storage\"]\n        all_endpoints = [\"http\", \"grpc\", \"sagemaker\", \"vertex-ai\"]\n\n        FLAGS.enable_logging = True\n        FLAGS.enable_stats = True\n        FLAGS.enable_metrics = True\n        FLAGS.enable_gpu_metrics = True\n        FLAGS.enable_cpu_metrics = True\n        FLAGS.enable_tracing = True\n        FLAGS.enable_nvtx = True\n        FLAGS.enable_gpu = True\n    else:\n        all_backends = [\n            \"ensemble\",\n            \"identity\",\n            \"square\",\n            \"repeat\",\n            \"onnxruntime\",\n            \"openvino\",\n            \"tensorrt\",\n        ]\n        all_repoagents = [\"checksum\"]\n        all_caches = [\"local\", \"redis\"]\n        all_filesystems = []\n        all_endpoints = [\"http\", \"grpc\"]\n\n        FLAGS.enable_logging = True\n        FLAGS.enable_stats = True\n        FLAGS.enable_tracing = True\n        FLAGS.enable_gpu = True\n\n    requested_backends = []\n    for be in FLAGS.backend:\n        parts = be.split(\":\")\n        requested_backends += [parts[0]]\n    for be in all_backends:\n        if be not in requested_backends:\n            FLAGS.backend += [be]\n\n    requested_repoagents = []\n    for ra in FLAGS.repoagent:\n        parts = ra.split(\":\")\n        requested_repoagents += [parts[0]]\n    for ra in all_repoagents:\n        if ra not in requested_repoagents:\n            FLAGS.repoagent += [ra]\n\n    requested_caches = []\n    for cache in FLAGS.cache:\n        parts = cache.split(\":\")\n        requested_caches += [parts[0]]\n    for cache in all_caches:\n        if cache not in requested_caches:\n            FLAGS.cache += [cache]\n\n    for fs in all_filesystems:\n        if fs not in FLAGS.filesystem:\n            FLAGS.filesystem += [fs]\n\n    for ep in all_endpoints:\n        if ep not in FLAGS.endpoint:\n            FLAGS.endpoint += [ep]\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    group_qv = parser.add_mutually_exclusive_group()\n    group_qv.add_argument(\n        \"-q\",\n        \"--quiet\",\n        action=\"store_true\",\n        required=False,\n        help=\"Disable console output.\",\n    )\n    group_qv.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        required=False,\n        help=\"Enable verbose output.\",\n    )\n\n    parser.add_argument(\n        \"--dryrun\",\n        action=\"store_true\",\n        required=False,\n        help=\"Output the build scripts, but do not perform build.\",\n    )\n    parser.add_argument(\n        \"--no-container-build\",\n        action=\"store_true\",\n        required=False,\n        help=\"Do not use Docker container for build.\",\n    )\n    parser.add_argument(\n        \"--no-container-interactive\",\n        action=\"store_true\",\n        required=False,\n        help='Do not use -it argument to \"docker run\" when performing container build.',\n    )\n    parser.add_argument(\n        \"--no-container-pull\",\n        action=\"store_true\",\n        required=False,\n        help=\"Do not use Docker --pull argument when building container.\",\n    )\n    parser.add_argument(\n        \"--container-memory\",\n        default=None,\n        required=False,\n        help=\"Value for Docker --memory argument. Used only for windows builds.\",\n    )\n    parser.add_argument(\n        \"--target-platform\",\n        required=False,\n        default=None,\n        help='Target platform for build, can be \"linux\", \"rhel\", \"windows\" or \"igpu\". If not specified, build targets the current platform.',\n    )\n    parser.add_argument(\n        \"--target-machine\",\n        required=False,\n        default=None,\n        help=\"Target machine/architecture for build. If not specified, build targets the current machine/architecture.\",\n    )\n\n    parser.add_argument(\n        \"--build-id\",\n        type=str,\n        required=False,\n        help=\"Build ID associated with the build.\",\n    )\n    parser.add_argument(\n        \"--build-sha\", type=str, required=False, help=\"SHA associated with the build.\"\n    )\n    parser.add_argument(\n        \"--build-dir\",\n        type=str,\n        required=False,\n        help=\"Build directory. All repo clones and builds will be performed in this directory.\",\n    )\n    parser.add_argument(\n        \"--install-dir\",\n        type=str,\n        required=False,\n        default=None,\n        help=\"Install directory, default is <builddir>/opt/tritonserver.\",\n    )\n    parser.add_argument(\n        \"--cmake-dir\",\n        type=str,\n        required=False,\n        help=\"Directory containing the CMakeLists.txt file for Triton server.\",\n    )\n    parser.add_argument(\n        \"--tmp-dir\",\n        type=str,\n        required=False,\n        default=\"/tmp\",\n        help=\"Temporary directory used for building inside docker. Default is /tmp.\",\n    )\n    parser.add_argument(\n        \"--library-paths\",\n        action=\"append\",\n        required=False,\n        default=None,\n        help=\"Specify library paths for respective backends in build as <backend-name>[:<library_path>].\",\n    )\n    parser.add_argument(\n        \"--build-type\",\n        required=False,\n        default=\"Release\",\n        help='Build type, one of \"Release\", \"Debug\", \"RelWithDebInfo\" or \"MinSizeRel\". Default is \"Release\".',\n    )\n    parser.add_argument(\n        \"-j\",\n        \"--build-parallel\",\n        type=int,\n        required=False,\n        default=None,\n        help=\"Build parallelism. Defaults to 2 * number-of-cores.\",\n    )\n\n    parser.add_argument(\n        \"--github-organization\",\n        type=str,\n        required=False,\n        default=\"https://github.com/triton-inference-server\",\n        help='The GitHub organization containing the repos used for the build. Defaults to \"https://github.com/triton-inference-server\".',\n    )\n    parser.add_argument(\n        \"--version\",\n        type=str,\n        required=False,\n        help=\"The Triton version. If not specified defaults to the value in the TRITON_VERSION file.\",\n    )\n    parser.add_argument(\n        \"--container-version\",\n        type=str,\n        required=False,\n        help=\"The Triton container version to build. If not specified the container version will be chosen automatically based on --version value.\",\n    )\n    parser.add_argument(\n        \"--container-prebuild-command\",\n        type=str,\n        required=False,\n        help=\"When performing a container build, this command will be executed within the container just before the build it performed.\",\n    )\n    parser.add_argument(\n        \"--no-container-source\",\n        action=\"store_true\",\n        required=False,\n        help=\"Do not include OSS source code in Docker container.\",\n    )\n    parser.add_argument(\n        \"--image\",\n        action=\"append\",\n        required=False,\n        help='Use specified Docker image in build as <image-name>,<full-image-name>. <image-name> can be \"base\", \"gpu-base\", \"tensorflow\", or \"pytorch\".',\n    )\n\n    parser.add_argument(\n        \"--enable-all\",\n        action=\"store_true\",\n        required=False,\n        help=\"Enable all standard released Triton features, backends, repository agents, caches, endpoints and file systems.\",\n    )\n    parser.add_argument(\n        \"--enable-logging\", action=\"store_true\", required=False, help=\"Enable logging.\"\n    )\n    parser.add_argument(\n        \"--enable-stats\",\n        action=\"store_true\",\n        required=False,\n        help=\"Enable statistics collection.\",\n    )\n    parser.add_argument(\n        \"--enable-metrics\",\n        action=\"store_true\",\n        required=False,\n        help=\"Enable metrics reporting.\",\n    )\n    parser.add_argument(\n        \"--enable-gpu-metrics\",\n        action=\"store_true\",\n        required=False,\n        help=\"Include GPU metrics in reported metrics.\",\n    )\n    parser.add_argument(\n        \"--enable-cpu-metrics\",\n        action=\"store_true\",\n        required=False,\n        help=\"Include CPU metrics in reported metrics.\",\n    )\n    parser.add_argument(\n        \"--enable-tracing\", action=\"store_true\", required=False, help=\"Enable tracing.\"\n    )\n    parser.add_argument(\n        \"--enable-nvtx\", action=\"store_true\", required=False, help=\"Enable NVTX.\"\n    )\n    parser.add_argument(\n        \"--enable-gpu\", action=\"store_true\", required=False, help=\"Enable GPU support.\"\n    )\n    parser.add_argument(\n        \"--enable-mali-gpu\",\n        action=\"store_true\",\n        required=False,\n        help=\"Enable ARM MALI GPU support.\",\n    )\n    parser.add_argument(\n        \"--min-compute-capability\",\n        type=str,\n        required=False,\n        default=\"6.0\",\n        help=\"Minimum CUDA compute capability supported by server.\",\n    )\n\n    parser.add_argument(\n        \"--endpoint\",\n        action=\"append\",\n        required=False,\n        help='Include specified endpoint in build. Allowed values are \"grpc\", \"http\", \"vertex-ai\" and \"sagemaker\".',\n    )\n    parser.add_argument(\n        \"--filesystem\",\n        action=\"append\",\n        required=False,\n        help='Include specified filesystem in build. Allowed values are \"gcs\", \"azure_storage\" and \"s3\".',\n    )\n    parser.add_argument(\n        \"--no-core-build\",\n        action=\"store_true\",\n        required=False,\n        help=\"Do not build Triton core shared library or executable.\",\n    )\n    parser.add_argument(\n        \"--backend\",\n        action=\"append\",\n        required=False,\n        help='Include specified backend in build as <backend-name>[:<repo-tag>]. If <repo-tag> starts with \"pull/\" then it refers to a pull-request reference, otherwise <repo-tag> indicates the git tag/branch to use for the build. If the version is non-development then the default <repo-tag> is the release branch matching the container version (e.g. version YY.MM -> branch rYY.MM); otherwise the default <repo-tag> is \"main\" (e.g. version YY.MMdev -> branch main).',\n    )\n    parser.add_argument(\n        \"--repo-tag\",\n        action=\"append\",\n        required=False,\n        help='The version of a component to use in the build as <component-name>:<repo-tag>. <component-name> can be \"common\", \"core\", \"backend\" or \"thirdparty\". <repo-tag> indicates the git tag/branch to use for the build. Currently <repo-tag> does not support pull-request reference. If the version is non-development then the default <repo-tag> is the release branch matching the container version (e.g. version YY.MM -> branch rYY.MM); otherwise the default <repo-tag> is \"main\" (e.g. version YY.MMdev -> branch main).',\n    )\n    parser.add_argument(\n        \"--repoagent\",\n        action=\"append\",\n        required=False,\n        help='Include specified repo agent in build as <repoagent-name>[:<repo-tag>]. If <repo-tag> starts with \"pull/\" then it refers to a pull-request reference, otherwise <repo-tag> indicates the git tag/branch to use for the build. If the version is non-development then the default <repo-tag> is the release branch matching the container version (e.g. version YY.MM -> branch rYY.MM); otherwise the default <repo-tag> is \"main\" (e.g. version YY.MMdev -> branch main).',\n    )\n    parser.add_argument(\n        \"--cache\",\n        action=\"append\",\n        required=False,\n        help='Include specified cache in build as <cache-name>[:<repo-tag>]. If <repo-tag> starts with \"pull/\" then it refers to a pull-request reference, otherwise <repo-tag> indicates the git tag/branch to use for the build. If the version is non-development then the default <repo-tag> is the release branch matching the container version (e.g. version YY.MM -> branch rYY.MM); otherwise the default <repo-tag> is \"main\" (e.g. version YY.MMdev -> branch main).',\n    )\n    parser.add_argument(\n        \"--no-force-clone\",\n        action=\"store_true\",\n        default=False,\n        help=\"Do not create fresh clones of repos that have already been cloned.\",\n    )\n    parser.add_argument(\n        \"--extra-core-cmake-arg\",\n        action=\"append\",\n        required=False,\n        help=\"Extra CMake argument as <name>=<value>. The argument is passed to CMake as -D<name>=<value> and is included after all CMake arguments added by build.py for the core builds.\",\n    )\n    parser.add_argument(\n        \"--override-core-cmake-arg\",\n        action=\"append\",\n        required=False,\n        help=\"Override specified CMake argument in the build as <name>=<value>. The argument is passed to CMake as -D<name>=<value>. This flag only impacts CMake arguments that are used by build.py. To unconditionally add a CMake argument to the core build use --extra-core-cmake-arg.\",\n    )\n    parser.add_argument(\n        \"--extra-backend-cmake-arg\",\n        action=\"append\",\n        required=False,\n        help=\"Extra CMake argument for a backend build as <backend>:<name>=<value>. The argument is passed to CMake as -D<name>=<value> and is included after all CMake arguments added by build.py for the backend.\",\n    )\n    parser.add_argument(\n        \"--override-backend-cmake-arg\",\n        action=\"append\",\n        required=False,\n        help=\"Override specified backend CMake argument in the build as <backend>:<name>=<value>. The argument is passed to CMake as -D<name>=<value>. This flag only impacts CMake arguments that are used by build.py. To unconditionally add a CMake argument to the backend build use --extra-backend-cmake-arg.\",\n    )\n    parser.add_argument(\n        \"--release-version\",\n        required=False,\n        default=DEFAULT_TRITON_VERSION_MAP[\"release_version\"],\n        help=\"This flag sets the release version for Triton Inference Server to be built. Default: the latest released version.\",\n    )\n    parser.add_argument(\n        \"--triton-container-version\",\n        required=False,\n        default=DEFAULT_TRITON_VERSION_MAP[\"triton_container_version\"],\n        help=\"This flag sets the container version for Triton Inference Server to be built. Default: the latest released version.\",\n    )\n    parser.add_argument(\n        \"--upstream-container-version\",\n        required=False,\n        default=DEFAULT_TRITON_VERSION_MAP[\"upstream_container_version\"],\n        help=\"This flag sets the upstream container version for Triton Inference Server to be built. Default: the latest released version.\",\n    )\n    parser.add_argument(\n        \"--ort-version\",\n        required=False,\n        default=DEFAULT_TRITON_VERSION_MAP[\"ort_version\"],\n        help=\"This flag sets the ORT version for Triton Inference Server to be built. Default: the latest supported version.\",\n    )\n    parser.add_argument(\n        \"--ort-openvino-version\",\n        required=False,\n        default=DEFAULT_TRITON_VERSION_MAP[\"ort_openvino_version\"],\n        help=\"This flag sets the OpenVino version for Triton Inference Server to be built. Default: the latest supported version.\",\n    )\n    parser.add_argument(\n        \"--standalone-openvino-version\",\n        required=False,\n        default=DEFAULT_TRITON_VERSION_MAP[\"standalone_openvino_version\"],\n        help=\"This flag sets the standalon OpenVino version for Triton Inference Server to be built. Default: the latest supported version.\",\n    )\n    parser.add_argument(\n        \"--dcgm-version\",\n        required=False,\n        default=DEFAULT_TRITON_VERSION_MAP[\"dcgm_version\"],\n        help=\"This flag sets the DCGM version for Triton Inference Server to be built. Default: the latest supported version.\",\n    )\n    parser.add_argument(\n        \"--vllm-version\",\n        required=False,\n        default=DEFAULT_TRITON_VERSION_MAP[\"vllm_version\"],\n        help=\"This flag sets the vLLM version for Triton Inference Server to be built. Default: the latest supported version.\",\n    )\n    parser.add_argument(\n        \"--rhel-py-version\",\n        required=False,\n        default=DEFAULT_TRITON_VERSION_MAP[\"rhel_py_version\"],\n        help=\"This flag sets the Python version for RHEL platform of Triton Inference Server to be built. Default: the latest supported version.\",\n    )\n    FLAGS = parser.parse_args()\n\n    if FLAGS.image is None:\n        FLAGS.image = []\n    if FLAGS.repo_tag is None:\n        FLAGS.repo_tag = []\n    if FLAGS.backend is None:\n        FLAGS.backend = []\n    if FLAGS.endpoint is None:\n        FLAGS.endpoint = []\n    if FLAGS.filesystem is None:\n        FLAGS.filesystem = []\n    if FLAGS.repoagent is None:\n        FLAGS.repoagent = []\n    if FLAGS.cache is None:\n        FLAGS.cache = []\n    if FLAGS.library_paths is None:\n        FLAGS.library_paths = []\n    if FLAGS.extra_core_cmake_arg is None:\n        FLAGS.extra_core_cmake_arg = []\n    if FLAGS.override_core_cmake_arg is None:\n        FLAGS.override_core_cmake_arg = []\n    if FLAGS.override_backend_cmake_arg is None:\n        FLAGS.override_backend_cmake_arg = []\n    if FLAGS.extra_backend_cmake_arg is None:\n        FLAGS.extra_backend_cmake_arg = []\n\n    # if --enable-all is specified, then update FLAGS to enable all\n    # settings, backends, repo-agents, caches, file systems, endpoints, etc.\n    if FLAGS.enable_all:\n        enable_all()\n\n    # When doing a docker build, --build-dir, --install-dir and\n    # --cmake-dir must not be set. We will use the build/ subdir\n    # within the server/ repo that contains this build.py script for\n    # --build-dir. If not doing a docker build, --build-dir must be\n    # set.\n    if FLAGS.no_container_build:\n        if FLAGS.build_dir is None:\n            fail(\"--no-container-build requires --build-dir\")\n        if FLAGS.install_dir is None:\n            FLAGS.install_dir = os.path.join(FLAGS.build_dir, \"opt\", \"tritonserver\")\n        if FLAGS.cmake_dir is None:\n            FLAGS.cmake_dir = THIS_SCRIPT_DIR\n    else:\n        if FLAGS.build_dir is not None:\n            fail(\"--build-dir must not be set for container-based build\")\n        if FLAGS.install_dir is not None:\n            fail(\"--install-dir must not be set for container-based build\")\n        if FLAGS.cmake_dir is not None:\n            fail(\"--cmake-dir must not be set for container-based build\")\n        FLAGS.build_dir = os.path.join(THIS_SCRIPT_DIR, \"build\")\n\n    # Determine the versions. Start with Triton version, if --version\n    # is not explicitly specified read from TRITON_VERSION file.\n    if FLAGS.version is None:\n        FLAGS.version = DEFAULT_TRITON_VERSION_MAP[\"release_version\"]\n\n    if FLAGS.build_parallel is None:\n        FLAGS.build_parallel = multiprocessing.cpu_count() * 2\n\n    log(\"Building Triton Inference Server\")\n    log(\"platform {}\".format(target_platform()))\n    log(\"machine {}\".format(target_machine()))\n    log(\"version {}\".format(FLAGS.version))\n    log(\"build dir {}\".format(FLAGS.build_dir))\n    log(\"install dir {}\".format(FLAGS.install_dir))\n    log(\"cmake dir {}\".format(FLAGS.cmake_dir))\n\n    # Determine the default repo-tag that should be used for images,\n    # backends, repo-agents, and caches if a repo-tag is not given\n    # explicitly. For release branches we use the release branch as\n    # the default, otherwise we use 'main'.\n    default_repo_tag = \"main\"\n    cver = FLAGS.upstream_container_version\n    if cver is None:\n        cver = FLAGS.triton_container_version\n    if not cver.endswith(\"dev\"):\n        default_repo_tag = \"r\" + cver\n    log(\"default repo-tag: {}\".format(default_repo_tag))\n\n    # For other versions use the TRITON_VERSION_MAP unless explicitly\n    # given.\n    FLAGS.container_version, FLAGS.upstream_container_version = container_versions(\n        FLAGS.version, FLAGS.container_version, FLAGS.upstream_container_version\n    )\n\n    log(\"container version {}\".format(FLAGS.container_version))\n    log(\"upstream container version {}\".format(FLAGS.upstream_container_version))\n\n    for ep in FLAGS.endpoint:\n        log(f'endpoint \"{ep}\"')\n    for fs in FLAGS.filesystem:\n        log(f'filesystem \"{fs}\"')\n\n    # Initialize map of backends to build and repo-tag for each.\n    backends = {}\n    for be in FLAGS.backend:\n        parts = be.split(\":\")\n        if len(parts) == 1:\n            parts.append(default_repo_tag)\n        if parts[0] == \"tensorflow1\":\n            fail(\n                \"Starting from Triton version 23.04, support for TensorFlow 1 has been discontinued. Please switch to Tensorflow 2.\"\n            )\n        if parts[0] == \"tensorflow2\":\n            parts[0] = \"tensorflow\"\n        log('backend \"{}\" at tag/branch \"{}\"'.format(parts[0], parts[1]))\n        backends[parts[0]] = parts[1]\n\n    if \"vllm\" in backends:\n        if \"python\" not in backends:\n            log(\n                \"vLLM backend requires Python backend, adding Python backend with tag {}\".format(\n                    backends[\"vllm\"]\n                )\n            )\n            backends[\"python\"] = backends[\"vllm\"]\n\n    # Initialize map of repo agents to build and repo-tag for each.\n    repoagents = {}\n    for be in FLAGS.repoagent:\n        parts = be.split(\":\")\n        if len(parts) == 1:\n            parts.append(default_repo_tag)\n        log('repoagent \"{}\" at tag/branch \"{}\"'.format(parts[0], parts[1]))\n        repoagents[parts[0]] = parts[1]\n\n    # Initialize map of caches to build and repo-tag for each.\n    caches = {}\n    for be in FLAGS.cache:\n        parts = be.split(\":\")\n        if len(parts) == 1:\n            parts.append(default_repo_tag)\n        log('cache \"{}\" at tag/branch \"{}\"'.format(parts[0], parts[1]))\n        caches[parts[0]] = parts[1]\n\n    # Initialize map of docker images.\n    images = {}\n    for img in FLAGS.image:\n        parts = img.split(\",\")\n        fail_if(\n            len(parts) != 2, \"--image must specify <image-name>,<full-image-registry>\"\n        )\n        fail_if(\n            parts[0]\n            not in [\"base\", \"gpu-base\", \"pytorch\", \"tensorflow\", \"tensorflow2\"],\n            \"unsupported value for --image\",\n        )\n        log('image \"{}\": \"{}\"'.format(parts[0], parts[1]))\n        if parts[0] == \"tensorflow2\":\n            parts[0] = \"tensorflow\"\n        images[parts[0]] = parts[1]\n\n    # Initialize map of library paths for each backend.\n    library_paths = {}\n    for lpath in FLAGS.library_paths:\n        parts = lpath.split(\":\")\n        if len(parts) == 2:\n            log('backend \"{}\" library path \"{}\"'.format(parts[0], parts[1]))\n            if parts[0] == \"tensorflow2\":\n                parts[0] = \"tensorflow\"\n            library_paths[parts[0]] = parts[1]\n\n    # Parse any explicitly specified cmake arguments\n    for cf in FLAGS.extra_core_cmake_arg:\n        parts = cf.split(\"=\")\n        fail_if(len(parts) != 2, \"--extra-core-cmake-arg must specify <name>=<value>\")\n        log('CMake core extra \"-D{}={}\"'.format(parts[0], parts[1]))\n        EXTRA_CORE_CMAKE_FLAGS[parts[0]] = parts[1]\n\n    for cf in FLAGS.override_core_cmake_arg:\n        parts = cf.split(\"=\")\n        fail_if(\n            len(parts) != 2, \"--override-core-cmake-arg must specify <name>=<value>\"\n        )\n        log('CMake core override \"-D{}={}\"'.format(parts[0], parts[1]))\n        OVERRIDE_CORE_CMAKE_FLAGS[parts[0]] = parts[1]\n\n    for cf in FLAGS.extra_backend_cmake_arg:\n        parts = cf.split(\":\", 1)\n        fail_if(\n            len(parts) != 2,\n            \"--extra-backend-cmake-arg must specify <backend>:<name>=<value>\",\n        )\n        be = parts[0]\n        parts = parts[1].split(\"=\", 1)\n        fail_if(\n            len(parts) != 2,\n            \"--extra-backend-cmake-arg must specify <backend>:<name>=<value>\",\n        )\n        fail_if(\n            be not in backends,\n            '--extra-backend-cmake-arg specifies backend \"{}\" which is not included in build'.format(\n                be\n            ),\n        )\n        log('backend \"{}\" CMake extra \"-D{}={}\"'.format(be, parts[0], parts[1]))\n        if be not in EXTRA_BACKEND_CMAKE_FLAGS:\n            EXTRA_BACKEND_CMAKE_FLAGS[be] = {}\n        EXTRA_BACKEND_CMAKE_FLAGS[be][parts[0]] = parts[1]\n\n    for cf in FLAGS.override_backend_cmake_arg:\n        parts = cf.split(\":\", 1)\n        fail_if(\n            len(parts) != 2,\n            \"--override-backend-cmake-arg must specify <backend>:<name>=<value>\",\n        )\n        be = parts[0]\n        parts = parts[1].split(\"=\", 1)\n        fail_if(\n            len(parts) != 2,\n            \"--override-backend-cmake-arg must specify <backend>:<name>=<value>\",\n        )\n        fail_if(\n            be not in backends,\n            '--override-backend-cmake-arg specifies backend \"{}\" which is not included in build'.format(\n                be\n            ),\n        )\n        log('backend \"{}\" CMake override \"-D{}={}\"'.format(be, parts[0], parts[1]))\n        if be not in OVERRIDE_BACKEND_CMAKE_FLAGS:\n            OVERRIDE_BACKEND_CMAKE_FLAGS[be] = {}\n        OVERRIDE_BACKEND_CMAKE_FLAGS[be][parts[0]] = parts[1]\n\n    # Initialize map of common components and repo-tag for each.\n    components = {\n        \"common\": default_repo_tag,\n        \"core\": default_repo_tag,\n        \"backend\": default_repo_tag,\n        \"thirdparty\": default_repo_tag,\n    }\n    for be in FLAGS.repo_tag:\n        parts = be.split(\":\")\n        fail_if(len(parts) != 2, \"--repo-tag must specify <component-name>:<repo-tag>\")\n        fail_if(\n            parts[0] not in components,\n            '--repo-tag <component-name> must be \"common\", \"core\", \"backend\", or \"thirdparty\"',\n        )\n        components[parts[0]] = parts[1]\n    for c in components:\n        log('component \"{}\" at tag/branch \"{}\"'.format(c, components[c]))\n\n    # Set the build, install, and cmake directories to use for the\n    # generated build scripts and Dockerfiles. If building without\n    # Docker, these are the directories specified on the cmdline. If\n    # building with Docker, we change these to be directories within\n    # FLAGS.tmp_dir inside the Docker container.\n    script_repo_dir = THIS_SCRIPT_DIR\n    script_build_dir = FLAGS.build_dir\n    script_install_dir = script_ci_dir = FLAGS.install_dir\n    script_cmake_dir = FLAGS.cmake_dir\n    if not FLAGS.no_container_build:\n        # FLAGS.tmp_dir may be specified with \"\\\" on Windows, adjust\n        # to \"/\" for docker usage.\n        script_build_dir = os.path.normpath(\n            os.path.join(FLAGS.tmp_dir, \"tritonbuild\").replace(\"\\\\\", \"/\")\n        )\n        script_install_dir = os.path.normpath(os.path.join(script_build_dir, \"install\"))\n        script_ci_dir = os.path.normpath(os.path.join(script_build_dir, \"ci\"))\n        if target_platform() == \"windows\":\n            script_repo_dir = script_cmake_dir = os.path.normpath(\"c:/workspace\")\n        else:\n            script_repo_dir = script_cmake_dir = \"/workspace\"\n\n    script_name = \"cmake_build\"\n    if target_platform() == \"windows\":\n        script_name += \".ps1\"\n\n    # Write the build script that invokes cmake for the core, backends, repo-agents, and caches.\n    pathlib.Path(FLAGS.build_dir).mkdir(parents=True, exist_ok=True)\n    with BuildScript(\n        os.path.join(FLAGS.build_dir, script_name),\n        verbose=FLAGS.verbose,\n        desc=(\"Build script for Triton Inference Server\"),\n    ) as cmake_script:\n        # Run the container pre-build command if the cmake build is\n        # being done within the build container.\n        if not FLAGS.no_container_build and FLAGS.container_prebuild_command:\n            cmake_script.cmd(FLAGS.container_prebuild_command, check_exitcode=True)\n            cmake_script.blankln()\n\n        # Commands to build the core shared library and the server executable.\n        if not FLAGS.no_core_build:\n            core_build(\n                cmake_script,\n                script_repo_dir,\n                script_cmake_dir,\n                script_build_dir,\n                script_install_dir,\n                components,\n                backends,\n            )\n\n        # Commands to build each backend...\n        for be in backends:\n            # Core backends are not built separately from core so skip...\n            if be in CORE_BACKENDS:\n                continue\n\n            # If armnn_tflite backend, source from external repo for git clone\n            if be == \"armnn_tflite\":\n                github_organization = \"https://gitlab.com/arm-research/smarter/\"\n            else:\n                github_organization = FLAGS.github_organization\n\n            if be == \"vllm\":\n                backend_clone(\n                    be,\n                    cmake_script,\n                    backends[be],\n                    script_build_dir,\n                    script_install_dir,\n                    github_organization,\n                )\n            else:\n                backend_build(\n                    be,\n                    cmake_script,\n                    backends[be],\n                    script_build_dir,\n                    script_install_dir,\n                    github_organization,\n                    images,\n                    components,\n                    library_paths,\n                )\n\n        # Commands to build each repo agent...\n        for ra in repoagents:\n            repo_agent_build(\n                ra,\n                cmake_script,\n                script_build_dir,\n                script_install_dir,\n                repoagent_repo,\n                repoagents,\n            )\n\n        # Commands to build each cache...\n        for cache in caches:\n            cache_build(\n                cache,\n                cmake_script,\n                script_build_dir,\n                script_install_dir,\n                cache_repo,\n                caches,\n            )\n\n        # Commands needed only when building with Docker...\n        if not FLAGS.no_container_build:\n            # Commands to collect all the build artifacts needed for CI\n            # testing.\n            cibase_build(\n                cmake_script,\n                script_repo_dir,\n                script_cmake_dir,\n                script_build_dir,\n                script_install_dir,\n                script_ci_dir,\n                backends,\n            )\n\n            # When building with Docker the install and ci artifacts\n            # written to the build-dir while running the docker container\n            # may have root ownership, so give them permissions to be\n            # managed by all users on the host system.\n            if target_platform() != \"windows\":\n                finalize_build(cmake_script, script_install_dir, script_ci_dir)\n\n    # If --no-container-build is not specified then we perform the\n    # actual build within a docker container and from that create the\n    # final tritonserver docker image. For the build we need to\n    # generate a few Dockerfiles and a top-level script that drives\n    # the build process.\n    if not FLAGS.no_container_build:\n        script_name = \"docker_build\"\n        if target_platform() == \"windows\":\n            script_name += \".ps1\"\n\n        create_build_dockerfiles(\n            script_build_dir, images, backends, repoagents, caches, FLAGS.endpoint\n        )\n        create_docker_build_script(script_name, script_install_dir, script_ci_dir)\n\n    # In not dry-run, execute the script to perform the build...  If a\n    # container-based build is requested use 'docker_build' script,\n    # otherwise build directly on this system using cmake script.\n    if not FLAGS.dryrun:\n        if target_platform() == \"windows\":\n            p = subprocess.Popen(\n                [\"powershell.exe\", \"-noexit\", \"-File\", f\"./{script_name}\"],\n                cwd=FLAGS.build_dir,\n            )\n        else:\n            p = subprocess.Popen([f\"./{script_name}\"], cwd=FLAGS.build_dir)\n        p.wait()\n        fail_if(p.returncode != 0, \"build failed\")\n"
        },
        {
          "name": "compose.py",
          "type": "blob",
          "size": 17.251953125,
          "content": "#!/usr/bin/env python3\n# Copyright 2021-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\nimport argparse\nimport os\nimport platform\nimport subprocess\nimport sys\n\nFLAGS = None\n\n\n#### helper functions\ndef log(msg, force=False):\n    if force or not FLAGS.quiet:\n        try:\n            print(msg, file=sys.stderr)\n        except Exception:\n            print(\"<failed to log>\", file=sys.stderr)\n\n\ndef log_verbose(msg):\n    if FLAGS.verbose:\n        log(msg, force=True)\n\n\ndef fail(msg):\n    print(\"error: {}\".format(msg), file=sys.stderr)\n    sys.exit(1)\n\n\ndef fail_if(p, msg):\n    if p:\n        fail(msg)\n\n\ndef start_dockerfile(ddir, images, argmap, dockerfile_name, backends):\n    # Set environment variables, set default user and install dependencies\n    df = \"\"\"\n#\n# Multistage build.\n#\nARG TRITON_VERSION={}\nARG TRITON_CONTAINER_VERSION={}\n\nFROM {} AS full\n\"\"\".format(\n        argmap[\"TRITON_VERSION\"], argmap[\"TRITON_CONTAINER_VERSION\"], images[\"full\"]\n    )\n\n    # PyTorch, TensorFlow backends need extra CUDA and other\n    # dependencies during runtime that are missing in the CPU-only base container.\n    # These dependencies must be copied from the Triton Min image.\n    if not FLAGS.enable_gpu and (\n        (\"pytorch\" in backends)\n        or (\"tensorflow\" in backends)\n        or (\"tensorflow2\" in backends)\n    ):\n        df += \"\"\"\nFROM {} AS min_container\n\n\"\"\".format(\n            images[\"gpu-min\"]\n        )\n\n    df += \"\"\"\nFROM {}\n\nENV PIP_BREAK_SYSTEM_PACKAGES=1\n\"\"\".format(\n        images[\"min\"]\n    )\n\n    import build\n\n    df += build.dockerfile_prepare_container_linux(\n        argmap, backends, FLAGS.enable_gpu, platform.machine().lower()\n    )\n    # Copy over files\n    df += \"\"\"\nWORKDIR /opt/tritonserver\nCOPY --chown=1000:1000 --from=full /opt/tritonserver/LICENSE .\nCOPY --chown=1000:1000 --from=full /opt/tritonserver/TRITON_VERSION .\nCOPY --chown=1000:1000 --from=full /opt/tritonserver/NVIDIA_Deep_Learning_Container_License.pdf .\nCOPY --chown=1000:1000 --from=full /opt/tritonserver/bin bin/\nCOPY --chown=1000:1000 --from=full /opt/tritonserver/lib lib/\nCOPY --chown=1000:1000 --from=full /opt/tritonserver/include include/\n\"\"\"\n    with open(os.path.join(ddir, dockerfile_name), \"w\") as dfile:\n        dfile.write(df)\n\n\ndef add_requested_backends(ddir, dockerfile_name, backends):\n    df = \"# Copying over backends \\n\"\n    for backend in backends:\n        df += \"\"\"COPY --chown=1000:1000 --from=full /opt/tritonserver/backends/{} /opt/tritonserver/backends/{}\n\"\"\".format(\n            backend, backend\n        )\n    if len(backends) > 0:\n        df += \"\"\"\n# Top-level /opt/tritonserver/backends not copied so need to explicitly set permissions here\nRUN chown triton-server:triton-server /opt/tritonserver/backends\n\"\"\"\n    with open(os.path.join(ddir, dockerfile_name), \"a\") as dfile:\n        dfile.write(df)\n\n\ndef add_requested_repoagents(ddir, dockerfile_name, repoagents):\n    df = \"#  Copying over repoagents \\n\"\n    for ra in repoagents:\n        df += \"\"\"COPY --chown=1000:1000 --from=full /opt/tritonserver/repoagents/{} /opt/tritonserver/repoagents/{}\n\"\"\".format(\n            ra, ra\n        )\n    if len(repoagents) > 0:\n        df += \"\"\"\n# Top-level /opt/tritonserver/repoagents not copied so need to explicitly set permissions here\nRUN chown triton-server:triton-server /opt/tritonserver/repoagents\n\"\"\"\n    with open(os.path.join(ddir, dockerfile_name), \"a\") as dfile:\n        dfile.write(df)\n\n\ndef add_requested_caches(ddir, dockerfile_name, caches):\n    df = \"#  Copying over caches \\n\"\n    for cache in caches:\n        df += \"\"\"COPY --chown=1000:1000 --from=full /opt/tritonserver/caches/{} /opt/tritonserver/caches/{}\n\"\"\".format(\n            cache, cache\n        )\n    if len(caches) > 0:\n        df += \"\"\"\n# Top-level /opt/tritonserver/caches not copied so need to explicitly set permissions here\nRUN chown triton-server:triton-server /opt/tritonserver/caches\n\"\"\"\n    with open(os.path.join(ddir, dockerfile_name), \"a\") as dfile:\n        dfile.write(df)\n\n\ndef end_dockerfile(ddir, dockerfile_name, argmap):\n    # Install additional dependencies\n    df = \"\"\n    if argmap[\"SAGEMAKER_ENDPOINT\"]:\n        df += \"\"\"\nLABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\nCOPY --chown=1000:1000 --from=full /usr/bin/serve /usr/bin/.\n\"\"\"\n    with open(os.path.join(ddir, dockerfile_name), \"a\") as dfile:\n        dfile.write(df)\n\n\ndef build_docker_image(ddir, dockerfile_name, container_name):\n    # Create container with docker build\n    p = subprocess.Popen(\n        [\n            \"docker\",\n            \"build\",\n            \"-t\",\n            container_name,\n            \"-f\",\n            os.path.join(ddir, dockerfile_name),\n            \".\",\n        ]\n    )\n    p.wait()\n    fail_if(p.returncode != 0, \"docker build {} failed\".format(container_name))\n\n\ndef get_container_version_if_not_specified():\n    if FLAGS.container_version is None:\n        # Read from TRITON_VERSION file in server repo to determine version\n        with open(\"TRITON_VERSION\", \"r\") as vfile:\n            version = vfile.readline().strip()\n        import build\n\n        _, FLAGS.container_version = build.container_versions(\n            version, None, FLAGS.container_version\n        )\n        log(\"version {}\".format(version))\n    log(\"using container version {}\".format(FLAGS.container_version))\n\n\ndef create_argmap(images, skip_pull):\n    # Extract information from upstream build and create map other functions can\n    # use\n    full_docker_image = images[\"full\"]\n    min_docker_image = images[\"min\"]\n    enable_gpu = FLAGS.enable_gpu\n    # Docker inspect environment variables\n    base_run_args = [\"docker\", \"inspect\", \"-f\"]\n    import re  # parse all PATH environment variables\n\n    # first pull docker images\n    if not skip_pull:\n        log(\"pulling container:{}\".format(full_docker_image))\n        p = subprocess.run([\"docker\", \"pull\", full_docker_image])\n        fail_if(\n            p.returncode != 0,\n            \"docker pull container {} failed, {}\".format(full_docker_image, p.stderr),\n        )\n    if enable_gpu:\n        if not skip_pull:\n            pm = subprocess.run([\"docker\", \"pull\", min_docker_image])\n            fail_if(\n                pm.returncode != 0 and not skip_pull,\n                \"docker pull container {} failed, {}\".format(\n                    min_docker_image, pm.stderr\n                ),\n            )\n        pm_path = subprocess.run(\n            base_run_args\n            + [\n                \"{{range $index, $value := .Config.Env}}{{$value}} {{end}}\",\n                min_docker_image,\n            ],\n            capture_output=True,\n            text=True,\n        )\n        fail_if(\n            pm_path.returncode != 0,\n            \"docker inspect to find triton environment variables for min container failed, {}\".format(\n                pm_path.stderr\n            ),\n        )\n        # min container needs to be GPU-support-enabled if the build is GPU build\n        vars = pm_path.stdout\n        e = re.search(\"CUDA_VERSION\", vars)\n        gpu_enabled = False if e is None else True\n        fail_if(\n            not gpu_enabled,\n            \"Composing container with gpu support enabled but min container provided does not have CUDA installed\",\n        )\n\n    # Check full container environment variables\n    p_path = subprocess.run(\n        base_run_args\n        + [\n            \"{{range $index, $value := .Config.Env}}{{$value}} {{end}}\",\n            full_docker_image,\n        ],\n        capture_output=True,\n        text=True,\n    )\n    fail_if(\n        p_path.returncode != 0,\n        \"docker inspect to find environment variables for full container failed, {}\".format(\n            p_path.stderr\n        ),\n    )\n    vars = p_path.stdout\n    log_verbose(\"inspect args: {}\".format(vars))\n\n    e0 = re.search(\"TRITON_SERVER_GPU_ENABLED=([\\S]{1,}) \", vars)\n    e1 = re.search(\"CUDA_VERSION\", vars)\n    gpu_enabled = False\n    if e0 != None:\n        gpu_enabled = e0.group(1) == \"1\"\n    elif e1 != None:\n        gpu_enabled = True\n    fail_if(\n        gpu_enabled != enable_gpu,\n        \"Error: full container provided was build with \"\n        \"'TRITON_SERVER_GPU_ENABLED' as {} and you are composing container\"\n        \"with 'TRITON_SERVER_GPU_ENABLED' as {}\".format(gpu_enabled, enable_gpu),\n    )\n    e = re.search(\"TRITON_SERVER_VERSION=([\\S]{6,}) \", vars)\n    version = \"\" if e is None else e.group(1)\n    fail_if(\n        len(version) == 0,\n        \"docker inspect to find triton server version failed, {}\".format(p_path.stderr),\n    )\n    e = re.search(\"NVIDIA_TRITON_SERVER_VERSION=([\\S]{5,}) \", vars)\n    container_version = \"\" if e is None else e.group(1)\n    fail_if(\n        len(container_version) == 0,\n        \"docker inspect to find triton container version failed, {}\".format(vars),\n    )\n    dcgm_ver = re.search(\"DCGM_VERSION=([\\S]{4,}) \", vars)\n    dcgm_version = \"\"\n    if dcgm_ver is None:\n        dcgm_version = \"2.2.3\"\n        log(\n            \"WARNING: DCGM version not found from image, installing the earlierst version {}\".format(\n                dcgm_version\n            )\n        )\n    else:\n        dcgm_version = dcgm_ver.group(1)\n    fail_if(\n        len(dcgm_version) == 0,\n        \"docker inspect to find DCGM version failed, {}\".format(vars),\n    )\n\n    p_sha = subprocess.run(\n        base_run_args\n        + ['{{ index .Config.Labels \"com.nvidia.build.ref\"}}', full_docker_image],\n        capture_output=True,\n        text=True,\n    )\n    fail_if(\n        p_sha.returncode != 0,\n        \"docker inspect of upstream docker image build sha failed, {}\".format(\n            p_sha.stderr\n        ),\n    )\n    p_build = subprocess.run(\n        base_run_args\n        + ['{{ index .Config.Labels \"com.nvidia.build.id\"}}', full_docker_image],\n        capture_output=True,\n        text=True,\n    )\n    fail_if(\n        p_build.returncode != 0,\n        \"docker inspect of upstream docker image build sha failed, {}\".format(\n            p_build.stderr\n        ),\n    )\n\n    p_find = subprocess.run(\n        [\"docker\", \"run\", full_docker_image, \"bash\", \"-c\", \"ls /usr/bin/\"],\n        capture_output=True,\n        text=True,\n    )\n    f = re.search(\"serve\", p_find.stdout)\n    fail_if(\n        p_find.returncode != 0,\n        \"Cannot search for 'serve' in /usr/bin, {}\".format(p_find.stderr),\n    )\n    argmap = {\n        \"NVIDIA_BUILD_REF\": p_sha.stdout.rstrip(),\n        \"NVIDIA_BUILD_ID\": p_build.stdout.rstrip(),\n        \"TRITON_VERSION\": version,\n        \"TRITON_CONTAINER_VERSION\": container_version,\n        \"DCGM_VERSION\": dcgm_version,\n        \"SAGEMAKER_ENDPOINT\": f is not None,\n    }\n    return argmap\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    group_qv = parser.add_mutually_exclusive_group()\n    group_qv.add_argument(\n        \"-q\",\n        \"--quiet\",\n        action=\"store_true\",\n        required=False,\n        help=\"Disable console output.\",\n    )\n    group_qv.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        required=False,\n        help=\"Enable verbose output.\",\n    )\n    parser.add_argument(\n        \"--output-name\",\n        type=str,\n        required=False,\n        help='Name for the generated Docker image. Default is \"tritonserver\".',\n    )\n    parser.add_argument(\n        \"--work-dir\",\n        type=str,\n        required=False,\n        help=\"Generated dockerfiles are placed here. Default to current directory.\",\n    )\n    parser.add_argument(\n        \"--container-version\",\n        type=str,\n        required=False,\n        help=\"The version to use for the generated Docker image. If not specified \"\n        \"the container version will be chosen automatically based on the \"\n        \"repository branch.\",\n    )\n    parser.add_argument(\n        \"--image\",\n        action=\"append\",\n        required=False,\n        help=\"Use specified Docker image to generate Docker image. Specified as \"\n        '<image-name>,<full-image-name>. <image-name> can be \"min\", \"gpu-min\" '\n        'or \"full\". Both \"min\" and \"full\" need to be specified at the same time.'\n        'This will override \"--container-version\". \"gpu-min\" is needed for '\n        \"CPU-only container to copy TensorFlow and PyTorch deps.\",\n    )\n    parser.add_argument(\n        \"--enable-gpu\",\n        nargs=\"?\",\n        type=lambda x: (str(x).lower() == \"true\"),\n        const=True,\n        default=True,\n        required=False,\n        help=argparse.SUPPRESS,\n    )\n    parser.add_argument(\n        \"--backend\",\n        action=\"append\",\n        required=False,\n        help=\"Include <backend-name> in the generated Docker image. The flag may be \"\n        \"specified multiple times.\",\n    )\n    parser.add_argument(\n        \"--repoagent\",\n        action=\"append\",\n        required=False,\n        help=\"Include <repoagent-name> in the generated Docker image. The flag may \"\n        \"be specified multiple times.\",\n    )\n    parser.add_argument(\n        \"--cache\",\n        action=\"append\",\n        required=False,\n        help=\"Include <cache-name> in the generated Docker image. The flag may \"\n        \"be specified multiple times.\",\n    )\n    parser.add_argument(\n        \"--skip-pull\",\n        action=\"store_true\",\n        required=False,\n        help=\"Do not pull the required docker images. The user is responsible \"\n        \"for pulling the upstream images needed to compose the image.\",\n    )\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        required=False,\n        help=\"Only creates Dockerfile.compose, does not build the Docker image.\",\n    )\n\n    FLAGS = parser.parse_args()\n\n    if FLAGS.work_dir is None:\n        FLAGS.work_dir = \".\"\n    if FLAGS.output_name is None:\n        FLAGS.output_name = \"tritonserver\"\n\n    dockerfile_name = \"Dockerfile.compose\"\n\n    if FLAGS.backend is None:\n        FLAGS.backend = []\n    if FLAGS.repoagent is None:\n        FLAGS.repoagent = []\n    if FLAGS.cache is None:\n        FLAGS.cache = []\n\n    # Initialize map of docker images.\n    images = {}\n    if FLAGS.image:\n        for img in FLAGS.image:\n            parts = img.split(\",\")\n            fail_if(\n                len(parts) != 2,\n                \"--image must specific <image-name>,<full-image-registry>\",\n            )\n            fail_if(\n                parts[0] not in [\"min\", \"full\", \"gpu-min\"],\n                \"unsupported image-name '{}' for --image\".format(parts[0]),\n            )\n            log('image \"{}\": \"{}\"'.format(parts[0], parts[1]))\n            images[parts[0]] = parts[1]\n    else:\n        get_container_version_if_not_specified()\n        if FLAGS.enable_gpu:\n            images = {\n                \"full\": \"nvcr.io/nvidia/tritonserver:{}-py3\".format(\n                    FLAGS.container_version\n                ),\n                \"min\": \"nvcr.io/nvidia/tritonserver:{}-py3-min\".format(\n                    FLAGS.container_version\n                ),\n            }\n        else:\n            images = {\n                \"full\": \"nvcr.io/nvidia/tritonserver:{}-cpu-only-py3\".format(\n                    FLAGS.container_version\n                ),\n                \"min\": \"ubuntu:22.04\",\n            }\n    fail_if(len(images) < 2, \"Need to specify both 'full' and 'min' images if at all\")\n\n    # For CPU-only image we need to copy some cuda libraries and dependencies\n    # since we are using PyTorch, TensorFlow 1, TensorFlow 2 containers that\n    # are not CPU-only.\n    if (\n        (\"pytorch\" in FLAGS.backend)\n        or (\"tensorflow\" in FLAGS.backend)\n        or (\"tensorflow2\" in FLAGS.backend)\n    ) and (\"gpu-min\" not in images):\n        images[\"gpu-min\"] = \"nvcr.io/nvidia/tritonserver:{}-py3-min\".format(\n            FLAGS.container_version\n        )\n\n    argmap = create_argmap(images, FLAGS.skip_pull)\n\n    start_dockerfile(FLAGS.work_dir, images, argmap, dockerfile_name, FLAGS.backend)\n    add_requested_backends(FLAGS.work_dir, dockerfile_name, FLAGS.backend)\n    add_requested_repoagents(FLAGS.work_dir, dockerfile_name, FLAGS.repoagent)\n    add_requested_caches(FLAGS.work_dir, dockerfile_name, FLAGS.cache)\n    end_dockerfile(FLAGS.work_dir, dockerfile_name, argmap)\n\n    if not FLAGS.dry_run:\n        build_docker_image(FLAGS.work_dir, dockerfile_name, FLAGS.output_name)\n"
        },
        {
          "name": "deploy",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 2.248046875,
          "content": "# Copyright 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n[tool.codespell]\n# note: pre-commit passes explicit lists of files here, which this skip file list doesn't override -\n# this is only to allow you to run codespell interactively\nskip = \"./.git,./.github\"\n# ignore short words, and typename parameters like OffsetT\nignore-regex = \"\\\\b(.{1,4}|[A-Z]\\\\w*T)\\\\b\"\n# ignore allowed words\nignore-words-list = \"passin,couldn\"\n# use the 'clear' dictionary for unambiguous spelling mistakes\nbuiltin = \"clear\"\n# disable warnings about binary files and wrong encoding\nquiet-level = 3\n\n[tool.isort]\nprofile = \"black\"\nuse_parentheses = true\nmulti_line_output = 3\ninclude_trailing_comma = true\nforce_grid_wrap = 0\nensure_newline_before_comments = true\nline_length = 88\nbalanced_wrapping = true\nindent = \"    \"\nskip = [\"build\"]\n\n"
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "qa",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}