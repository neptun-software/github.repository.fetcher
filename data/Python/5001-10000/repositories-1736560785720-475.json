{
  "metadata": {
    "timestamp": 1736560785720,
    "page": 475,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVlabs/stylegan3",
      "stars": 6521,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.7001953125,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\nFROM nvcr.io/nvidia/pytorch:21.08-py3\n\nENV PYTHONDONTWRITEBYTECODE 1\nENV PYTHONUNBUFFERED 1\n\nRUN pip install imageio imageio-ffmpeg==0.4.4 pyspng==0.1.0\n\nWORKDIR /workspace\n\nRUN (printf '#!/bin/bash\\nexec \\\"$@\\\"\\n' >> /entry.sh) && chmod a+x /entry.sh\nENTRYPOINT [\"/entry.sh\"]\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 4.2841796875,
          "content": "Copyright (c) 2021, NVIDIA Corporation & affiliates. All rights reserved.\n\n\nNVIDIA Source Code License for StyleGAN3\n\n\n=======================================================================\n\n1. Definitions\n\n\"Licensor\" means any person or entity that distributes its Work.\n\n\"Software\" means the original work of authorship made available under\nthis License.\n\n\"Work\" means the Software and any additions to or derivative works of\nthe Software that are made available under this License.\n\nThe terms \"reproduce,\" \"reproduction,\" \"derivative works,\" and\n\"distribution\" have the meaning as provided under U.S. copyright law;\nprovided, however, that for the purposes of this License, derivative\nworks shall not include works that remain separable from, or merely\nlink (or bind by name) to the interfaces of, the Work.\n\nWorks, including the Software, are \"made available\" under this License\nby including in or with the Work either (a) a copyright notice\nreferencing the applicability of this License to the Work, or (b) a\ncopy of this License.\n\n2. License Grants\n\n    2.1 Copyright Grant. Subject to the terms and conditions of this\n    License, each Licensor grants to you a perpetual, worldwide,\n    non-exclusive, royalty-free, copyright license to reproduce,\n    prepare derivative works of, publicly display, publicly perform,\n    sublicense and distribute its Work and any resulting derivative\n    works in any form.\n\n3. Limitations\n\n    3.1 Redistribution. You may reproduce or distribute the Work only\n    if (a) you do so under this License, (b) you include a complete\n    copy of this License with your distribution, and (c) you retain\n    without modification any copyright, patent, trademark, or\n    attribution notices that are present in the Work.\n\n    3.2 Derivative Works. You may specify that additional or different\n    terms apply to the use, reproduction, and distribution of your\n    derivative works of the Work (\"Your Terms\") only if (a) Your Terms\n    provide that the use limitation in Section 3.3 applies to your\n    derivative works, and (b) you identify the specific derivative\n    works that are subject to Your Terms. Notwithstanding Your Terms,\n    this License (including the redistribution requirements in Section\n    3.1) will continue to apply to the Work itself.\n\n    3.3 Use Limitation. The Work and any derivative works thereof only\n    may be used or intended for use non-commercially. Notwithstanding\n    the foregoing, NVIDIA and its affiliates may use the Work and any\n    derivative works commercially. As used herein, \"non-commercially\"\n    means for research or evaluation purposes only.\n\n    3.4 Patent Claims. If you bring or threaten to bring a patent claim\n    against any Licensor (including any claim, cross-claim or\n    counterclaim in a lawsuit) to enforce any patents that you allege\n    are infringed by any Work, then your rights under this License from\n    such Licensor (including the grant in Section 2.1) will terminate\n    immediately.\n\n    3.5 Trademarks. This License does not grant any rights to use any\n    Licensor’s or its affiliates’ names, logos, or trademarks, except\n    as necessary to reproduce the notices described in this License.\n\n    3.6 Termination. If you violate any term of this License, then your\n    rights under this License (including the grant in Section 2.1) will\n    terminate immediately.\n\n4. Disclaimer of Warranty.\n\nTHE WORK IS PROVIDED \"AS IS\" WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, EITHER EXPRESS OR IMPLIED, INCLUDING WARRANTIES OR CONDITIONS OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR\nNON-INFRINGEMENT. YOU BEAR THE RISK OF UNDERTAKING ANY ACTIVITIES UNDER\nTHIS LICENSE.\n\n5. Limitation of Liability.\n\nEXCEPT AS PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL\nTHEORY, WHETHER IN TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE\nSHALL ANY LICENSOR BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY DIRECT,\nINDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF\nOR RELATED TO THIS LICENSE, THE USE OR INABILITY TO USE THE WORK\n(INCLUDING BUT NOT LIMITED TO LOSS OF GOODWILL, BUSINESS INTERRUPTION,\nLOST PROFITS OR DATA, COMPUTER FAILURE OR MALFUNCTION, OR ANY OTHER\nCOMMERCIAL DAMAGES OR LOSSES), EVEN IF THE LICENSOR HAS BEEN ADVISED OF\nTHE POSSIBILITY OF SUCH DAMAGES.\n\n=======================================================================\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 20.662109375,
          "content": "## Alias-Free Generative Adversarial Networks (StyleGAN3)<br><sub>Official PyTorch implementation of the NeurIPS 2021 paper</sub>\n\n![Teaser image](./docs/stylegan3-teaser-1920x1006.png)\n\n**Alias-Free Generative Adversarial Networks**<br>\nTero Karras, Miika Aittala, Samuli Laine, Erik H&auml;rk&ouml;nen, Janne Hellsten, Jaakko Lehtinen, Timo Aila<br>\nhttps://nvlabs.github.io/stylegan3<br>\n\nAbstract: *We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.*\n\nFor business inquiries, please visit our website and submit the form: [NVIDIA Research Licensing](https://www.nvidia.com/en-us/research/inquiries/)\n\n## Release notes\n\nThis repository is an updated version of [stylegan2-ada-pytorch](https://github.com/NVlabs/stylegan2-ada-pytorch), with several new features:\n- Alias-free generator architecture and training configurations (`stylegan3-t`, `stylegan3-r`).\n- Tools for interactive visualization (`visualizer.py`), spectral analysis (`avg_spectra.py`), and video generation (`gen_video.py`).\n- Equivariance metrics (`eqt50k_int`, `eqt50k_frac`, `eqr50k`).\n- General improvements: reduced memory usage, slightly faster training, bug fixes.\n\nCompatibility:\n- Compatible with old network pickles created using [stylegan2-ada](https://github.com/NVlabs/stylegan2-ada) and [stylegan2-ada-pytorch](https://github.com/NVlabs/stylegan2-ada-pytorch).  (Note: running old StyleGAN2 models on StyleGAN3 code will produce the same results as running them on stylegan2-ada/stylegan2-ada-pytorch.  To benefit from the StyleGAN3 architecture, you need to retrain.)\n- Supports old StyleGAN2 training configurations, including ADA and transfer learning. See [Training configurations](./docs/configs.md) for details.\n- Improved compatibility with Ampere GPUs and newer versions of PyTorch, CuDNN, etc.\n\n## Synthetic image detection\n\nWhile new generator approaches enable new media synthesis capabilities, they may also present a new challenge for AI forensics algorithms for detection and attribution of synthetic media. In collaboration with digital forensic researchers participating in DARPA's SemaFor program, we curated a synthetic image dataset that allowed the researchers to test and validate the performance of their image detectors in advance of the public release. Please see [here](https://github.com/NVlabs/stylegan3-detector) for more details.\n\n## Additional material\n\n- [Result videos](https://nvlabs-fi-cdn.nvidia.com/stylegan3/videos/)\n- [Curated example images](https://nvlabs-fi-cdn.nvidia.com/stylegan3/images/)\n- [StyleGAN3 pre-trained models](https://ngc.nvidia.com/catalog/models/nvidia:research:stylegan3) for config T (translation equiv.) and config R (translation and rotation equiv.)\n  > <sub>Access individual networks via `https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/<MODEL>`, where `<MODEL>` is one of:</sub><br>\n  > <sub>`stylegan3-t-ffhq-1024x1024.pkl`, `stylegan3-t-ffhqu-1024x1024.pkl`, `stylegan3-t-ffhqu-256x256.pkl`</sub><br>\n  > <sub>`stylegan3-r-ffhq-1024x1024.pkl`, `stylegan3-r-ffhqu-1024x1024.pkl`, `stylegan3-r-ffhqu-256x256.pkl`</sub><br>\n  > <sub>`stylegan3-t-metfaces-1024x1024.pkl`, `stylegan3-t-metfacesu-1024x1024.pkl`</sub><br>\n  > <sub>`stylegan3-r-metfaces-1024x1024.pkl`, `stylegan3-r-metfacesu-1024x1024.pkl`</sub><br>\n  > <sub>`stylegan3-t-afhqv2-512x512.pkl`</sub><br>\n  > <sub>`stylegan3-r-afhqv2-512x512.pkl`</sub><br>\n- [StyleGAN2 pre-trained models](https://ngc.nvidia.com/catalog/models/nvidia:research:stylegan2) compatible with this codebase\n  > <sub>Access individual networks via `https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/<MODEL>`, where `<MODEL>` is one of:</sub><br>\n  > <sub>`stylegan2-ffhq-1024x1024.pkl`, `stylegan2-ffhq-512x512.pkl`, `stylegan2-ffhq-256x256.pkl`</sub><br>\n  > <sub>`stylegan2-ffhqu-1024x1024.pkl`, `stylegan2-ffhqu-256x256.pkl`</sub><br>\n  > <sub>`stylegan2-metfaces-1024x1024.pkl`, `stylegan2-metfacesu-1024x1024.pkl`</sub><br>\n  > <sub>`stylegan2-afhqv2-512x512.pkl`</sub><br>\n  > <sub>`stylegan2-afhqcat-512x512.pkl`, `stylegan2-afhqdog-512x512.pkl`, `stylegan2-afhqwild-512x512.pkl`</sub><br>\n  > <sub>`stylegan2-brecahad-512x512.pkl`, `stylegan2-cifar10-32x32.pkl`</sub><br>\n  > <sub>`stylegan2-celebahq-256x256.pkl`, `stylegan2-lsundog-256x256.pkl`</sub><br>\n\n## Requirements\n\n* Linux and Windows are supported, but we recommend Linux for performance and compatibility reasons.\n* 1&ndash;8 high-end NVIDIA GPUs with at least 12 GB of memory. We have done all testing and development using Tesla V100 and A100 GPUs.\n* 64-bit Python 3.8 and PyTorch 1.9.0 (or later). See https://pytorch.org for PyTorch install instructions.\n* CUDA toolkit 11.1 or later.  (Why is a separate CUDA toolkit installation required?  See [Troubleshooting](./docs/troubleshooting.md#why-is-cuda-toolkit-installation-necessary)).\n* GCC 7 or later (Linux) or Visual Studio (Windows) compilers.  Recommended GCC version depends on CUDA version, see for example [CUDA 11.4 system requirements](https://docs.nvidia.com/cuda/archive/11.4.1/cuda-installation-guide-linux/index.html#system-requirements).\n* Python libraries: see [environment.yml](./environment.yml) for exact library dependencies.  You can use the following commands with Miniconda3 to create and activate your StyleGAN3 Python environment:\n  - `conda env create -f environment.yml`\n  - `conda activate stylegan3`\n* Docker users:\n  - Ensure you have correctly installed the [NVIDIA container runtime](https://docs.docker.com/config/containers/resource_constraints/#gpu).\n  - Use the [provided Dockerfile](./Dockerfile) to build an image with the required library dependencies.\n\nThe code relies heavily on custom PyTorch extensions that are compiled on the fly using NVCC. On Windows, the compilation requires Microsoft Visual Studio. We recommend installing [Visual Studio Community Edition](https://visualstudio.microsoft.com/vs/) and adding it into `PATH` using `\"C:\\Program Files (x86)\\Microsoft Visual Studio\\<VERSION>\\Community\\VC\\Auxiliary\\Build\\vcvars64.bat\"`.\n\nSee [Troubleshooting](./docs/troubleshooting.md) for help on common installation and run-time problems.\n\n## Getting started\n\nPre-trained networks are stored as `*.pkl` files that can be referenced using local filenames or URLs:\n\n```.bash\n# Generate an image using pre-trained AFHQv2 model (\"Ours\" in Figure 1, left).\npython gen_images.py --outdir=out --trunc=1 --seeds=2 \\\n    --network=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-afhqv2-512x512.pkl\n\n# Render a 4x2 grid of interpolations for seeds 0 through 31.\npython gen_video.py --output=lerp.mp4 --trunc=1 --seeds=0-31 --grid=4x2 \\\n    --network=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-afhqv2-512x512.pkl\n```\n\nOutputs from the above commands are placed under `out/*.png`, controlled by `--outdir`. Downloaded network pickles are cached under `$HOME/.cache/dnnlib`, which can be overridden by setting the `DNNLIB_CACHE_DIR` environment variable. The default PyTorch extension build directory is `$HOME/.cache/torch_extensions`, which can be overridden by setting `TORCH_EXTENSIONS_DIR`.\n\n**Docker**: You can run the above curated image example using Docker as follows:\n\n```.bash\n# Build the stylegan3:latest image\ndocker build --tag stylegan3 .\n\n# Run the gen_images.py script using Docker:\ndocker run --gpus all -it --rm --user $(id -u):$(id -g) \\\n    -v `pwd`:/scratch --workdir /scratch -e HOME=/scratch \\\n    stylegan3 \\\n    python gen_images.py --outdir=out --trunc=1 --seeds=2 \\\n         --network=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-afhqv2-512x512.pkl\n```\n\nNote: The Docker image requires NVIDIA driver release `r470` or later.\n\nThe `docker run` invocation may look daunting, so let's unpack its contents here:\n\n- `--gpus all -it --rm --user $(id -u):$(id -g)`: with all GPUs enabled, run an interactive session with current user's UID/GID to avoid Docker writing files as root.\n- ``-v `pwd`:/scratch --workdir /scratch``: mount current running dir (e.g., the top of this git repo on your host machine) to `/scratch` in the container and use that as the current working dir.\n- `-e HOME=/scratch`: let PyTorch and StyleGAN3 code know where to cache temporary files such as pre-trained models and custom PyTorch extension build results. Note: if you want more fine-grained control, you can instead set `TORCH_EXTENSIONS_DIR` (for custom extensions build dir) and `DNNLIB_CACHE_DIR` (for pre-trained model download cache). You want these cache dirs to reside on persistent volumes so that their contents are retained across multiple `docker run` invocations.\n\n## Interactive visualization\n\nThis release contains an interactive model visualization tool that can be used to explore various characteristics of a trained model.  To start it, run:\n\n```.bash\npython visualizer.py\n```\n\n<a href=\"./docs/visualizer_screen0.png\"><img alt=\"Visualizer screenshot\" src=\"./docs/visualizer_screen0_half.png\"></img></a>\n\n## Using networks from Python\n\nYou can use pre-trained networks in your own Python code as follows:\n\n```.python\nwith open('ffhq.pkl', 'rb') as f:\n    G = pickle.load(f)['G_ema'].cuda()  # torch.nn.Module\nz = torch.randn([1, G.z_dim]).cuda()    # latent codes\nc = None                                # class labels (not used in this example)\nimg = G(z, c)                           # NCHW, float32, dynamic range [-1, +1], no truncation\n```\n\nThe above code requires `torch_utils` and `dnnlib` to be accessible via `PYTHONPATH`. It does not need source code for the networks themselves &mdash; their class definitions are loaded from the pickle via `torch_utils.persistence`.\n\nThe pickle contains three networks. `'G'` and `'D'` are instantaneous snapshots taken during training, and `'G_ema'` represents a moving average of the generator weights over several training steps. The networks are regular instances of `torch.nn.Module`, with all of their parameters and buffers placed on the CPU at import and gradient computation disabled by default.\n\nThe generator consists of two submodules, `G.mapping` and `G.synthesis`, that can be executed separately. They also support various additional options:\n\n```.python\nw = G.mapping(z, c, truncation_psi=0.5, truncation_cutoff=8)\nimg = G.synthesis(w, noise_mode='const', force_fp32=True)\n```\n\nPlease refer to [`gen_images.py`](./gen_images.py) for complete code example.\n\n## Preparing datasets\n\nDatasets are stored as uncompressed ZIP archives containing uncompressed PNG files and a metadata file `dataset.json` for labels. Custom datasets can be created from a folder containing images; see [`python dataset_tool.py --help`](./docs/dataset-tool-help.txt) for more information. Alternatively, the folder can also be used directly as a dataset, without running it through `dataset_tool.py` first, but doing so may lead to suboptimal performance.\n\n**FFHQ**: Download the [Flickr-Faces-HQ dataset](https://github.com/NVlabs/ffhq-dataset) as 1024x1024 images and create a zip archive using `dataset_tool.py`:\n\n```.bash\n# Original 1024x1024 resolution.\npython dataset_tool.py --source=/tmp/images1024x1024 --dest=~/datasets/ffhq-1024x1024.zip\n\n# Scaled down 256x256 resolution.\npython dataset_tool.py --source=/tmp/images1024x1024 --dest=~/datasets/ffhq-256x256.zip \\\n    --resolution=256x256\n```\n\nSee the [FFHQ README](https://github.com/NVlabs/ffhq-dataset) for information on how to obtain the unaligned FFHQ dataset images. Use the same steps as above to create a ZIP archive for training and validation.\n\n**MetFaces**: Download the [MetFaces dataset](https://github.com/NVlabs/metfaces-dataset) and create a ZIP archive:\n\n```.bash\npython dataset_tool.py --source=~/downloads/metfaces/images --dest=~/datasets/metfaces-1024x1024.zip\n```\n\nSee the [MetFaces README](https://github.com/NVlabs/metfaces-dataset) for information on how to obtain the unaligned MetFaces dataset images. Use the same steps as above to create a ZIP archive for training and validation.\n\n**AFHQv2**: Download the [AFHQv2 dataset](https://github.com/clovaai/stargan-v2/blob/master/README.md#animal-faces-hq-dataset-afhq) and create a ZIP archive:\n\n```.bash\npython dataset_tool.py --source=~/downloads/afhqv2 --dest=~/datasets/afhqv2-512x512.zip\n```\n\nNote that the above command creates a single combined dataset using all images of all three classes (cats, dogs, and wild animals), matching the setup used in the StyleGAN3 paper. Alternatively, you can also create a separate dataset for each class:\n\n```.bash\npython dataset_tool.py --source=~/downloads/afhqv2/train/cat --dest=~/datasets/afhqv2cat-512x512.zip\npython dataset_tool.py --source=~/downloads/afhqv2/train/dog --dest=~/datasets/afhqv2dog-512x512.zip\npython dataset_tool.py --source=~/downloads/afhqv2/train/wild --dest=~/datasets/afhqv2wild-512x512.zip\n```\n\n## Training\n\nYou can train new networks using `train.py`. For example:\n\n```.bash\n# Train StyleGAN3-T for AFHQv2 using 8 GPUs.\npython train.py --outdir=~/training-runs --cfg=stylegan3-t --data=~/datasets/afhqv2-512x512.zip \\\n    --gpus=8 --batch=32 --gamma=8.2 --mirror=1\n\n# Fine-tune StyleGAN3-R for MetFaces-U using 1 GPU, starting from the pre-trained FFHQ-U pickle.\npython train.py --outdir=~/training-runs --cfg=stylegan3-r --data=~/datasets/metfacesu-1024x1024.zip \\\n    --gpus=8 --batch=32 --gamma=6.6 --mirror=1 --kimg=5000 --snap=5 \\\n    --resume=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-1024x1024.pkl\n\n# Train StyleGAN2 for FFHQ at 1024x1024 resolution using 8 GPUs.\npython train.py --outdir=~/training-runs --cfg=stylegan2 --data=~/datasets/ffhq-1024x1024.zip \\\n    --gpus=8 --batch=32 --gamma=10 --mirror=1 --aug=noaug\n```\n\nNote that the result quality and training time depend heavily on the exact set of options. The most important ones (`--gpus`, `--batch`, and `--gamma`) must be specified explicitly, and they should be selected with care. See [`python train.py --help`](./docs/train-help.txt) for the full list of options and [Training configurations](./docs/configs.md) for general guidelines &amp; recommendations, along with the expected training speed &amp; memory usage in different scenarios.\n\nThe results of each training run are saved to a newly created directory, for example `~/training-runs/00000-stylegan3-t-afhqv2-512x512-gpus8-batch32-gamma8.2`. The training loop exports network pickles (`network-snapshot-<KIMG>.pkl`) and random image grids (`fakes<KIMG>.png`) at regular intervals (controlled by `--snap`). For each exported pickle, it evaluates FID (controlled by `--metrics`) and logs the result in `metric-fid50k_full.jsonl`. It also records various statistics in `training_stats.jsonl`, as well as `*.tfevents` if TensorBoard is installed.\n\n## Quality metrics\n\nBy default, `train.py` automatically computes FID for each network pickle exported during training. We recommend inspecting `metric-fid50k_full.jsonl` (or TensorBoard) at regular intervals to monitor the training progress. When desired, the automatic computation can be disabled with `--metrics=none` to speed up the training slightly.\n\nAdditional quality metrics can also be computed after the training:\n\n```.bash\n# Previous training run: look up options automatically, save result to JSONL file.\npython calc_metrics.py --metrics=eqt50k_int,eqr50k \\\n    --network=~/training-runs/00000-stylegan3-r-mydataset/network-snapshot-000000.pkl\n\n# Pre-trained network pickle: specify dataset explicitly, print result to stdout.\npython calc_metrics.py --metrics=fid50k_full --data=~/datasets/ffhq-1024x1024.zip --mirror=1 \\\n    --network=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-ffhq-1024x1024.pkl\n```\n\nThe first example looks up the training configuration and performs the same operation as if `--metrics=eqt50k_int,eqr50k` had been specified during training. The second example downloads a pre-trained network pickle, in which case the values of `--data` and `--mirror` must be specified explicitly.\n\nNote that the metrics can be quite expensive to compute (up to 1h), and many of them have an additional one-off cost for each new dataset (up to 30min). Also note that the evaluation is done using a different random seed each time, so the results will vary if the same metric is computed multiple times.\n\nRecommended metrics:\n* `fid50k_full`: Fr&eacute;chet inception distance<sup>[1]</sup> against the full dataset.\n* `kid50k_full`: Kernel inception distance<sup>[2]</sup> against the full dataset.\n* `pr50k3_full`: Precision and recall<sup>[3]</sup> againt the full dataset.\n* `ppl2_wend`: Perceptual path length<sup>[4]</sup> in W, endpoints, full image.\n* `eqt50k_int`: Equivariance<sup>[5]</sup> w.r.t. integer translation (EQ-T).\n* `eqt50k_frac`: Equivariance w.r.t. fractional translation (EQ-T<sub>frac</sub>).\n* `eqr50k`: Equivariance w.r.t. rotation (EQ-R).\n\nLegacy metrics:\n* `fid50k`: Fr&eacute;chet inception distance against 50k real images.\n* `kid50k`: Kernel inception distance against 50k real images.\n* `pr50k3`: Precision and recall against 50k real images.\n* `is50k`: Inception score<sup>[6]</sup> for CIFAR-10.\n\nReferences:\n1. [GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium](https://arxiv.org/abs/1706.08500), Heusel et al. 2017\n2. [Demystifying MMD GANs](https://arxiv.org/abs/1801.01401), Bi&nacute;kowski et al. 2018\n3. [Improved Precision and Recall Metric for Assessing Generative Models](https://arxiv.org/abs/1904.06991), Kynk&auml;&auml;nniemi et al. 2019\n4. [A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1812.04948), Karras et al. 2018\n5. [Alias-Free Generative Adversarial Networks](https://nvlabs.github.io/stylegan3), Karras et al. 2021\n6. [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498), Salimans et al. 2016\n\n## Spectral analysis\n\nThe easiest way to inspect the spectral properties of a given generator is to use the built-in FFT mode in `visualizer.py`. In addition, you can visualize average 2D power spectra (Appendix A, Figure 15) as follows:\n\n```.bash\n# Calculate dataset mean and std, needed in subsequent steps.\npython avg_spectra.py stats --source=~/datasets/ffhq-1024x1024.zip\n\n# Calculate average spectrum for the training data.\npython avg_spectra.py calc --source=~/datasets/ffhq-1024x1024.zip \\\n    --dest=tmp/training-data.npz --mean=112.684 --std=69.509\n\n# Calculate average spectrum for a pre-trained generator.\npython avg_spectra.py calc \\\n    --source=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl \\\n    --dest=tmp/stylegan3-r.npz --mean=112.684 --std=69.509 --num=70000\n\n# Display results.\npython avg_spectra.py heatmap tmp/training-data.npz\npython avg_spectra.py heatmap tmp/stylegan3-r.npz\npython avg_spectra.py slices tmp/training-data.npz tmp/stylegan3-r.npz\n```\n\n<a href=\"./docs/avg_spectra_screen0.png\"><img alt=\"Average spectra screenshot\" src=\"./docs/avg_spectra_screen0_half.png\"></img></a>\n\n## License\n\nCopyright &copy; 2021, NVIDIA Corporation & affiliates. All rights reserved.\n\nThis work is made available under the [Nvidia Source Code License](https://github.com/NVlabs/stylegan3/blob/main/LICENSE.txt).\n\n## Citation\n\n```\n@inproceedings{Karras2021,\n  author = {Tero Karras and Miika Aittala and Samuli Laine and Erik H\\\"ark\\\"onen and Janne Hellsten and Jaakko Lehtinen and Timo Aila},\n  title = {Alias-Free Generative Adversarial Networks},\n  booktitle = {Proc. NeurIPS},\n  year = {2021}\n}\n```\n\n## Development\n\nThis is a research reference implementation and is treated as a one-time code drop. As such, we do not accept outside code contributions in the form of pull requests.\n\n## Acknowledgements\n\nWe thank David Luebke, Ming-Yu Liu, Koki Nagano, Tuomas Kynk&auml;&auml;nniemi, and Timo Viitanen for reviewing early drafts and helpful suggestions. Fr&eacute;do Durand for early discussions. Tero Kuosmanen for maintaining our compute infrastructure. AFHQ authors for an updated version of their dataset. Getty Images for the training images in the Beaches dataset. We did not receive external funding or additional revenues for this project.\n"
        },
        {
          "name": "avg_spectra.py",
          "type": "blob",
          "size": 12.3955078125,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Compare average power spectra between real and generated images,\nor between multiple generators.\"\"\"\n\nimport os\nimport numpy as np\nimport torch\nimport torch.fft\nimport scipy.ndimage\nimport matplotlib.pyplot as plt\nimport click\nimport tqdm\nimport dnnlib\n\nimport legacy\nfrom training import dataset\n\n#----------------------------------------------------------------------------\n# Setup an iterator for streaming images, in uint8 NCHW format, based on the\n# respective command line options.\n\ndef stream_source_images(source, num, seed, device, data_loader_kwargs=None): # => num_images, image_size, image_iter\n    ext = source.split('.')[-1].lower()\n    if data_loader_kwargs is None:\n        data_loader_kwargs = dict(pin_memory=True, num_workers=3, prefetch_factor=2)\n\n    if ext == 'pkl':\n        if num is None:\n            raise click.ClickException('--num is required when --source points to network pickle')\n        with dnnlib.util.open_url(source) as f:\n            G = legacy.load_network_pkl(f)['G_ema'].to(device)\n        def generate_image(seed):\n            rnd = np.random.RandomState(seed)\n            z = torch.from_numpy(rnd.randn(1, G.z_dim)).to(device)\n            c = torch.zeros([1, G.c_dim], device=device)\n            if G.c_dim > 0:\n                c[:, rnd.randint(G.c_dim)] = 1\n            return (G(z=z, c=c) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n        _ = generate_image(seed) # warm up\n        image_iter = (generate_image(seed + idx) for idx in range(num))\n        return num, G.img_resolution, image_iter\n\n    elif ext == 'zip' or os.path.isdir(source):\n        dataset_obj = dataset.ImageFolderDataset(path=source, max_size=num, random_seed=seed)\n        if num is not None and num != len(dataset_obj):\n            raise click.ClickException(f'--source contains fewer than {num} images')\n        data_loader = torch.utils.data.DataLoader(dataset_obj, batch_size=1, **data_loader_kwargs)\n        image_iter = (image.to(device) for image, _label in data_loader)\n        return len(dataset_obj), dataset_obj.resolution, image_iter\n\n    else:\n        raise click.ClickException('--source must point to network pickle, dataset zip, or directory')\n\n#----------------------------------------------------------------------------\n# Load average power spectrum from the specified .npz file and construct\n# the corresponding heatmap for visualization.\n\ndef construct_heatmap(npz_file, smooth):\n    npz_data = np.load(npz_file)\n    spectrum = npz_data['spectrum']\n    image_size = npz_data['image_size']\n    hmap = np.log10(spectrum) * 10 # dB\n    hmap = np.fft.fftshift(hmap)\n    hmap = np.concatenate([hmap, hmap[:1, :]], axis=0)\n    hmap = np.concatenate([hmap, hmap[:, :1]], axis=1)\n    if smooth > 0:\n        sigma = spectrum.shape[0] / image_size * smooth\n        hmap = scipy.ndimage.gaussian_filter(hmap, sigma=sigma, mode='nearest')\n    return hmap, image_size\n\n#----------------------------------------------------------------------------\n\n@click.group()\ndef main():\n    \"\"\"Compare average power spectra between real and generated images,\n    or between multiple generators.\n\n    Example:\n\n    \\b\n    # Calculate dataset mean and std, needed in subsequent steps.\n    python avg_spectra.py stats --source=~/datasets/ffhq-1024x1024.zip\n\n    \\b\n    # Calculate average spectrum for the training data.\n    python avg_spectra.py calc --source=~/datasets/ffhq-1024x1024.zip \\\\\n        --dest=tmp/training-data.npz --mean=112.684 --std=69.509\n\n    \\b\n    # Calculate average spectrum for a pre-trained generator.\n    python avg_spectra.py calc \\\\\n        --source=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl \\\\\n        --dest=tmp/stylegan3-r.npz --mean=112.684 --std=69.509 --num=70000\n\n    \\b\n    # Display results.\n    python avg_spectra.py heatmap tmp/training-data.npz\n    python avg_spectra.py heatmap tmp/stylegan3-r.npz\n    python avg_spectra.py slices tmp/training-data.npz tmp/stylegan3-r.npz\n\n    \\b\n    # Save as PNG.\n    python avg_spectra.py heatmap tmp/training-data.npz --save=tmp/training-data.png --dpi=300\n    python avg_spectra.py heatmap tmp/stylegan3-r.npz --save=tmp/stylegan3-r.png --dpi=300\n    python avg_spectra.py slices tmp/training-data.npz tmp/stylegan3-r.npz --save=tmp/slices.png --dpi=300\n    \"\"\"\n\n#----------------------------------------------------------------------------\n\n@main.command()\n@click.option('--source', help='Network pkl, dataset zip, or directory', metavar='[PKL|ZIP|DIR]', required=True)\n@click.option('--num', help='Number of images to process  [default: all]', metavar='INT', type=click.IntRange(min=1))\n@click.option('--seed', help='Random seed for selecting the images', metavar='INT', type=click.IntRange(min=0), default=0, show_default=True)\ndef stats(source, num, seed, device=torch.device('cuda')):\n    \"\"\"Calculate dataset mean and standard deviation needed by 'calc'.\"\"\"\n    torch.multiprocessing.set_start_method('spawn')\n    num_images, _image_size, image_iter = stream_source_images(source=source, num=num, seed=seed, device=device)\n\n    # Accumulate moments.\n    moments = torch.zeros([3], dtype=torch.float64, device=device)\n    for image in tqdm.tqdm(image_iter, total=num_images):\n        image = image.to(torch.float64)\n        moments += torch.stack([torch.ones_like(image).sum(), image.sum(), image.square().sum()])\n    moments = moments / moments[0]\n\n    # Compute mean and standard deviation.\n    mean = moments[1]\n    std = (moments[2] - moments[1].square()).sqrt()\n    print(f'--mean={mean:g} --std={std:g}')\n\n#----------------------------------------------------------------------------\n\n@main.command()\n@click.option('--source', help='Network pkl, dataset zip, or directory', metavar='[PKL|ZIP|DIR]', required=True)\n@click.option('--dest', help='Where to store the result', metavar='NPZ', required=True)\n@click.option('--mean', help='Dataset mean for whitening', metavar='FLOAT', type=float, required=True)\n@click.option('--std', help='Dataset standard deviation for whitening', metavar='FLOAT', type=click.FloatRange(min=0), required=True)\n@click.option('--num', help='Number of images to process  [default: all]', metavar='INT', type=click.IntRange(min=1))\n@click.option('--seed', help='Random seed for selecting the images', metavar='INT', type=click.IntRange(min=0), default=0, show_default=True)\n@click.option('--beta', help='Shape parameter for the Kaiser window', metavar='FLOAT', type=click.FloatRange(min=0), default=8, show_default=True)\n@click.option('--interp', help='Frequency-domain interpolation factor', metavar='INT', type=click.IntRange(min=1), default=4, show_default=True)\ndef calc(source, dest, mean, std, num, seed, beta, interp, device=torch.device('cuda')):\n    \"\"\"Calculate average power spectrum and store it in .npz file.\"\"\"\n    torch.multiprocessing.set_start_method('spawn')\n    num_images, image_size, image_iter = stream_source_images(source=source, num=num, seed=seed, device=device)\n    spectrum_size = image_size * interp\n    padding = spectrum_size - image_size\n\n    # Setup window function.\n    window = torch.kaiser_window(image_size, periodic=False, beta=beta, device=device)\n    window *= window.square().sum().rsqrt()\n    window = window.ger(window).unsqueeze(0).unsqueeze(1)\n\n    # Accumulate power spectrum.\n    spectrum = torch.zeros([spectrum_size, spectrum_size], dtype=torch.float64, device=device)\n    for image in tqdm.tqdm(image_iter, total=num_images):\n        image = (image.to(torch.float64) - mean) / std\n        image = torch.nn.functional.pad(image * window, [0, padding, 0, padding])\n        spectrum += torch.fft.fftn(image, dim=[2,3]).abs().square().mean(dim=[0,1])\n    spectrum /= num_images\n\n    # Save result.\n    if os.path.dirname(dest):\n        os.makedirs(os.path.dirname(dest), exist_ok=True)\n    np.savez(dest, spectrum=spectrum.cpu().numpy(), image_size=image_size)\n\n#----------------------------------------------------------------------------\n\n@main.command()\n@click.argument('npz-file', nargs=1)\n@click.option('--save', help='Save the plot and exit', metavar='[PNG|PDF|...]')\n@click.option('--dpi', help='Figure resolution', metavar='FLOAT', type=click.FloatRange(min=1), default=100, show_default=True)\n@click.option('--smooth', help='Amount of smoothing', metavar='FLOAT', type=click.FloatRange(min=0), default=1.25, show_default=True)\ndef heatmap(npz_file, save, smooth, dpi):\n    \"\"\"Visualize 2D heatmap based on the given .npz file.\"\"\"\n    hmap, image_size = construct_heatmap(npz_file=npz_file, smooth=smooth)\n\n    # Setup plot.\n    plt.figure(figsize=[6, 4.8], dpi=dpi, tight_layout=True)\n    freqs = np.linspace(-0.5, 0.5, num=hmap.shape[0], endpoint=True) * image_size\n    ticks = np.linspace(freqs[0], freqs[-1], num=5, endpoint=True)\n    levels = np.linspace(-40, 20, num=13, endpoint=True)\n\n    # Draw heatmap.\n    plt.xlim(ticks[0], ticks[-1])\n    plt.ylim(ticks[0], ticks[-1])\n    plt.xticks(ticks)\n    plt.yticks(ticks)\n    plt.contourf(freqs, freqs, hmap, levels=levels, extend='both', cmap='Blues')\n    plt.gca().set_aspect('equal')\n    plt.colorbar(ticks=levels)\n    plt.contour(freqs, freqs, hmap, levels=levels, extend='both', linestyles='solid', linewidths=1, colors='midnightblue', alpha=0.2)\n\n    # Display or save.\n    if save is None:\n        plt.show()\n    else:\n        if os.path.dirname(save):\n            os.makedirs(os.path.dirname(save), exist_ok=True)\n        plt.savefig(save)\n\n#----------------------------------------------------------------------------\n\n@main.command()\n@click.argument('npz-files', nargs=-1, required=True)\n@click.option('--save', help='Save the plot and exit', metavar='[PNG|PDF|...]')\n@click.option('--dpi', help='Figure resolution', metavar='FLOAT', type=click.FloatRange(min=1), default=100, show_default=True)\n@click.option('--smooth', help='Amount of smoothing', metavar='FLOAT', type=click.FloatRange(min=0), default=0, show_default=True)\ndef slices(npz_files, save, dpi, smooth):\n    \"\"\"Visualize 1D slices based on the given .npz files.\"\"\"\n    cases = [dnnlib.EasyDict(npz_file=npz_file) for npz_file in npz_files]\n    for c in cases:\n        c.hmap, c.image_size = construct_heatmap(npz_file=c.npz_file, smooth=smooth)\n        c.label = os.path.splitext(os.path.basename(c.npz_file))[0]\n\n    # Check consistency.\n    image_size = cases[0].image_size\n    hmap_size = cases[0].hmap.shape[0]\n    if any(c.image_size != image_size or c.hmap.shape[0] != hmap_size for c in cases):\n        raise click.ClickException('All .npz must have the same resolution')\n\n    # Setup plot.\n    plt.figure(figsize=[12, 4.6], dpi=dpi, tight_layout=True)\n    hmap_center = hmap_size // 2\n    hmap_range = np.arange(hmap_center, hmap_size)\n    freqs0 = np.linspace(0, image_size / 2, num=(hmap_size // 2 + 1), endpoint=True)\n    freqs45 = np.linspace(0, image_size / np.sqrt(2), num=(hmap_size // 2 + 1), endpoint=True)\n    xticks0 = np.linspace(freqs0[0], freqs0[-1], num=9, endpoint=True)\n    xticks45 = np.round(np.linspace(freqs45[0], freqs45[-1], num=9, endpoint=True))\n    yticks = np.linspace(-50, 30, num=9, endpoint=True)\n\n    # Draw 0 degree slice.\n    plt.subplot(1, 2, 1)\n    plt.title('0\\u00b0 slice')\n    plt.xlim(xticks0[0], xticks0[-1])\n    plt.ylim(yticks[0], yticks[-1])\n    plt.xticks(xticks0)\n    plt.yticks(yticks)\n    for c in cases:\n        plt.plot(freqs0, c.hmap[hmap_center, hmap_range], label=c.label)\n    plt.grid()\n    plt.legend(loc='upper right')\n\n    # Draw 45 degree slice.\n    plt.subplot(1, 2, 2)\n    plt.title('45\\u00b0 slice')\n    plt.xlim(xticks45[0], xticks45[-1])\n    plt.ylim(yticks[0], yticks[-1])\n    plt.xticks(xticks45)\n    plt.yticks(yticks)\n    for c in cases:\n        plt.plot(freqs45, c.hmap[hmap_range, hmap_range], label=c.label)\n    plt.grid()\n    plt.legend(loc='upper right')\n\n    # Display or save.\n    if save is None:\n        plt.show()\n    else:\n        if os.path.dirname(save):\n            os.makedirs(os.path.dirname(save), exist_ok=True)\n        plt.savefig(save)\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "calc_metrics.py",
          "type": "blob",
          "size": 7.923828125,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Calculate quality metrics for previous training run or pretrained network pickle.\"\"\"\n\nimport os\nimport click\nimport json\nimport tempfile\nimport copy\nimport torch\n\nimport dnnlib\nimport legacy\nfrom metrics import metric_main\nfrom metrics import metric_utils\nfrom torch_utils import training_stats\nfrom torch_utils import custom_ops\nfrom torch_utils import misc\nfrom torch_utils.ops import conv2d_gradfix\n\n#----------------------------------------------------------------------------\n\ndef subprocess_fn(rank, args, temp_dir):\n    dnnlib.util.Logger(should_flush=True)\n\n    # Init torch.distributed.\n    if args.num_gpus > 1:\n        init_file = os.path.abspath(os.path.join(temp_dir, '.torch_distributed_init'))\n        if os.name == 'nt':\n            init_method = 'file:///' + init_file.replace('\\\\', '/')\n            torch.distributed.init_process_group(backend='gloo', init_method=init_method, rank=rank, world_size=args.num_gpus)\n        else:\n            init_method = f'file://{init_file}'\n            torch.distributed.init_process_group(backend='nccl', init_method=init_method, rank=rank, world_size=args.num_gpus)\n\n    # Init torch_utils.\n    sync_device = torch.device('cuda', rank) if args.num_gpus > 1 else None\n    training_stats.init_multiprocessing(rank=rank, sync_device=sync_device)\n    if rank != 0 or not args.verbose:\n        custom_ops.verbosity = 'none'\n\n    # Configure torch.\n    device = torch.device('cuda', rank)\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = False\n    conv2d_gradfix.enabled = True\n\n    # Print network summary.\n    G = copy.deepcopy(args.G).eval().requires_grad_(False).to(device)\n    if rank == 0 and args.verbose:\n        z = torch.empty([1, G.z_dim], device=device)\n        c = torch.empty([1, G.c_dim], device=device)\n        misc.print_module_summary(G, [z, c])\n\n    # Calculate each metric.\n    for metric in args.metrics:\n        if rank == 0 and args.verbose:\n            print(f'Calculating {metric}...')\n        progress = metric_utils.ProgressMonitor(verbose=args.verbose)\n        result_dict = metric_main.calc_metric(metric=metric, G=G, dataset_kwargs=args.dataset_kwargs,\n            num_gpus=args.num_gpus, rank=rank, device=device, progress=progress)\n        if rank == 0:\n            metric_main.report_metric(result_dict, run_dir=args.run_dir, snapshot_pkl=args.network_pkl)\n        if rank == 0 and args.verbose:\n            print()\n\n    # Done.\n    if rank == 0 and args.verbose:\n        print('Exiting...')\n\n#----------------------------------------------------------------------------\n\ndef parse_comma_separated_list(s):\n    if isinstance(s, list):\n        return s\n    if s is None or s.lower() == 'none' or s == '':\n        return []\n    return s.split(',')\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.pass_context\n@click.option('network_pkl', '--network', help='Network pickle filename or URL', metavar='PATH', required=True)\n@click.option('--metrics', help='Quality metrics', metavar='[NAME|A,B,C|none]', type=parse_comma_separated_list, default='fid50k_full', show_default=True)\n@click.option('--data', help='Dataset to evaluate against  [default: look up]', metavar='[ZIP|DIR]')\n@click.option('--mirror', help='Enable dataset x-flips  [default: look up]', type=bool, metavar='BOOL')\n@click.option('--gpus', help='Number of GPUs to use', type=int, default=1, metavar='INT', show_default=True)\n@click.option('--verbose', help='Print optional information', type=bool, default=True, metavar='BOOL', show_default=True)\n\ndef calc_metrics(ctx, network_pkl, metrics, data, mirror, gpus, verbose):\n    \"\"\"Calculate quality metrics for previous training run or pretrained network pickle.\n\n    Examples:\n\n    \\b\n    # Previous training run: look up options automatically, save result to JSONL file.\n    python calc_metrics.py --metrics=eqt50k_int,eqr50k \\\\\n        --network=~/training-runs/00000-stylegan3-r-mydataset/network-snapshot-000000.pkl\n\n    \\b\n    # Pre-trained network pickle: specify dataset explicitly, print result to stdout.\n    python calc_metrics.py --metrics=fid50k_full --data=~/datasets/ffhq-1024x1024.zip --mirror=1 \\\\\n        --network=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-ffhq-1024x1024.pkl\n\n    \\b\n    Recommended metrics:\n      fid50k_full  Frechet inception distance against the full dataset.\n      kid50k_full  Kernel inception distance against the full dataset.\n      pr50k3_full  Precision and recall againt the full dataset.\n      ppl2_wend    Perceptual path length in W, endpoints, full image.\n      eqt50k_int   Equivariance w.r.t. integer translation (EQ-T).\n      eqt50k_frac  Equivariance w.r.t. fractional translation (EQ-T_frac).\n      eqr50k       Equivariance w.r.t. rotation (EQ-R).\n\n    \\b\n    Legacy metrics:\n      fid50k       Frechet inception distance against 50k real images.\n      kid50k       Kernel inception distance against 50k real images.\n      pr50k3       Precision and recall against 50k real images.\n      is50k        Inception score for CIFAR-10.\n    \"\"\"\n    dnnlib.util.Logger(should_flush=True)\n\n    # Validate arguments.\n    args = dnnlib.EasyDict(metrics=metrics, num_gpus=gpus, network_pkl=network_pkl, verbose=verbose)\n    if not all(metric_main.is_valid_metric(metric) for metric in args.metrics):\n        ctx.fail('\\n'.join(['--metrics can only contain the following values:'] + metric_main.list_valid_metrics()))\n    if not args.num_gpus >= 1:\n        ctx.fail('--gpus must be at least 1')\n\n    # Load network.\n    if not dnnlib.util.is_url(network_pkl, allow_file_urls=True) and not os.path.isfile(network_pkl):\n        ctx.fail('--network must point to a file or URL')\n    if args.verbose:\n        print(f'Loading network from \"{network_pkl}\"...')\n    with dnnlib.util.open_url(network_pkl, verbose=args.verbose) as f:\n        network_dict = legacy.load_network_pkl(f)\n        args.G = network_dict['G_ema'] # subclass of torch.nn.Module\n\n    # Initialize dataset options.\n    if data is not None:\n        args.dataset_kwargs = dnnlib.EasyDict(class_name='training.dataset.ImageFolderDataset', path=data)\n    elif network_dict['training_set_kwargs'] is not None:\n        args.dataset_kwargs = dnnlib.EasyDict(network_dict['training_set_kwargs'])\n    else:\n        ctx.fail('Could not look up dataset options; please specify --data')\n\n    # Finalize dataset options.\n    args.dataset_kwargs.resolution = args.G.img_resolution\n    args.dataset_kwargs.use_labels = (args.G.c_dim != 0)\n    if mirror is not None:\n        args.dataset_kwargs.xflip = mirror\n\n    # Print dataset options.\n    if args.verbose:\n        print('Dataset options:')\n        print(json.dumps(args.dataset_kwargs, indent=2))\n\n    # Locate run dir.\n    args.run_dir = None\n    if os.path.isfile(network_pkl):\n        pkl_dir = os.path.dirname(network_pkl)\n        if os.path.isfile(os.path.join(pkl_dir, 'training_options.json')):\n            args.run_dir = pkl_dir\n\n    # Launch processes.\n    if args.verbose:\n        print('Launching processes...')\n    torch.multiprocessing.set_start_method('spawn')\n    with tempfile.TemporaryDirectory() as temp_dir:\n        if args.num_gpus == 1:\n            subprocess_fn(rank=0, args=args, temp_dir=temp_dir)\n        else:\n            torch.multiprocessing.spawn(fn=subprocess_fn, args=(args, temp_dir), nprocs=args.num_gpus)\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    calc_metrics() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "dataset_tool.py",
          "type": "blob",
          "size": 17.7001953125,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Tool for creating ZIP/PNG based datasets.\"\"\"\n\nimport functools\nimport gzip\nimport io\nimport json\nimport os\nimport pickle\nimport re\nimport sys\nimport tarfile\nimport zipfile\nfrom pathlib import Path\nfrom typing import Callable, Optional, Tuple, Union\n\nimport click\nimport numpy as np\nimport PIL.Image\nfrom tqdm import tqdm\n\n#----------------------------------------------------------------------------\n\ndef error(msg):\n    print('Error: ' + msg)\n    sys.exit(1)\n\n#----------------------------------------------------------------------------\n\ndef parse_tuple(s: str) -> Tuple[int, int]:\n    '''Parse a 'M,N' or 'MxN' integer tuple.\n\n    Example:\n        '4x2' returns (4,2)\n        '0,1' returns (0,1)\n    '''\n    m = re.match(r'^(\\d+)[x,](\\d+)$', s)\n    if m:\n        return (int(m.group(1)), int(m.group(2)))\n    raise ValueError(f'cannot parse tuple {s}')\n\n#----------------------------------------------------------------------------\n\ndef maybe_min(a: int, b: Optional[int]) -> int:\n    if b is not None:\n        return min(a, b)\n    return a\n\n#----------------------------------------------------------------------------\n\ndef file_ext(name: Union[str, Path]) -> str:\n    return str(name).split('.')[-1]\n\n#----------------------------------------------------------------------------\n\ndef is_image_ext(fname: Union[str, Path]) -> bool:\n    ext = file_ext(fname).lower()\n    return f'.{ext}' in PIL.Image.EXTENSION # type: ignore\n\n#----------------------------------------------------------------------------\n\ndef open_image_folder(source_dir, *, max_images: Optional[int]):\n    input_images = [str(f) for f in sorted(Path(source_dir).rglob('*')) if is_image_ext(f) and os.path.isfile(f)]\n\n    # Load labels.\n    labels = {}\n    meta_fname = os.path.join(source_dir, 'dataset.json')\n    if os.path.isfile(meta_fname):\n        with open(meta_fname, 'r') as file:\n            labels = json.load(file)['labels']\n            if labels is not None:\n                labels = { x[0]: x[1] for x in labels }\n            else:\n                labels = {}\n\n    max_idx = maybe_min(len(input_images), max_images)\n\n    def iterate_images():\n        for idx, fname in enumerate(input_images):\n            arch_fname = os.path.relpath(fname, source_dir)\n            arch_fname = arch_fname.replace('\\\\', '/')\n            img = np.array(PIL.Image.open(fname))\n            yield dict(img=img, label=labels.get(arch_fname))\n            if idx >= max_idx-1:\n                break\n    return max_idx, iterate_images()\n\n#----------------------------------------------------------------------------\n\ndef open_image_zip(source, *, max_images: Optional[int]):\n    with zipfile.ZipFile(source, mode='r') as z:\n        input_images = [str(f) for f in sorted(z.namelist()) if is_image_ext(f)]\n\n        # Load labels.\n        labels = {}\n        if 'dataset.json' in z.namelist():\n            with z.open('dataset.json', 'r') as file:\n                labels = json.load(file)['labels']\n                if labels is not None:\n                    labels = { x[0]: x[1] for x in labels }\n                else:\n                    labels = {}\n\n    max_idx = maybe_min(len(input_images), max_images)\n\n    def iterate_images():\n        with zipfile.ZipFile(source, mode='r') as z:\n            for idx, fname in enumerate(input_images):\n                with z.open(fname, 'r') as file:\n                    img = PIL.Image.open(file) # type: ignore\n                    img = np.array(img)\n                yield dict(img=img, label=labels.get(fname))\n                if idx >= max_idx-1:\n                    break\n    return max_idx, iterate_images()\n\n#----------------------------------------------------------------------------\n\ndef open_lmdb(lmdb_dir: str, *, max_images: Optional[int]):\n    import cv2  # pip install opencv-python # pylint: disable=import-error\n    import lmdb  # pip install lmdb # pylint: disable=import-error\n\n    with lmdb.open(lmdb_dir, readonly=True, lock=False).begin(write=False) as txn:\n        max_idx = maybe_min(txn.stat()['entries'], max_images)\n\n    def iterate_images():\n        with lmdb.open(lmdb_dir, readonly=True, lock=False).begin(write=False) as txn:\n            for idx, (_key, value) in enumerate(txn.cursor()):\n                try:\n                    try:\n                        img = cv2.imdecode(np.frombuffer(value, dtype=np.uint8), 1)\n                        if img is None:\n                            raise IOError('cv2.imdecode failed')\n                        img = img[:, :, ::-1] # BGR => RGB\n                    except IOError:\n                        img = np.array(PIL.Image.open(io.BytesIO(value)))\n                    yield dict(img=img, label=None)\n                    if idx >= max_idx-1:\n                        break\n                except:\n                    print(sys.exc_info()[1])\n\n    return max_idx, iterate_images()\n\n#----------------------------------------------------------------------------\n\ndef open_cifar10(tarball: str, *, max_images: Optional[int]):\n    images = []\n    labels = []\n\n    with tarfile.open(tarball, 'r:gz') as tar:\n        for batch in range(1, 6):\n            member = tar.getmember(f'cifar-10-batches-py/data_batch_{batch}')\n            with tar.extractfile(member) as file:\n                data = pickle.load(file, encoding='latin1')\n            images.append(data['data'].reshape(-1, 3, 32, 32))\n            labels.append(data['labels'])\n\n    images = np.concatenate(images)\n    labels = np.concatenate(labels)\n    images = images.transpose([0, 2, 3, 1]) # NCHW -> NHWC\n    assert images.shape == (50000, 32, 32, 3) and images.dtype == np.uint8\n    assert labels.shape == (50000,) and labels.dtype in [np.int32, np.int64]\n    assert np.min(images) == 0 and np.max(images) == 255\n    assert np.min(labels) == 0 and np.max(labels) == 9\n\n    max_idx = maybe_min(len(images), max_images)\n\n    def iterate_images():\n        for idx, img in enumerate(images):\n            yield dict(img=img, label=int(labels[idx]))\n            if idx >= max_idx-1:\n                break\n\n    return max_idx, iterate_images()\n\n#----------------------------------------------------------------------------\n\ndef open_mnist(images_gz: str, *, max_images: Optional[int]):\n    labels_gz = images_gz.replace('-images-idx3-ubyte.gz', '-labels-idx1-ubyte.gz')\n    assert labels_gz != images_gz\n    images = []\n    labels = []\n\n    with gzip.open(images_gz, 'rb') as f:\n        images = np.frombuffer(f.read(), np.uint8, offset=16)\n    with gzip.open(labels_gz, 'rb') as f:\n        labels = np.frombuffer(f.read(), np.uint8, offset=8)\n\n    images = images.reshape(-1, 28, 28)\n    images = np.pad(images, [(0,0), (2,2), (2,2)], 'constant', constant_values=0)\n    assert images.shape == (60000, 32, 32) and images.dtype == np.uint8\n    assert labels.shape == (60000,) and labels.dtype == np.uint8\n    assert np.min(images) == 0 and np.max(images) == 255\n    assert np.min(labels) == 0 and np.max(labels) == 9\n\n    max_idx = maybe_min(len(images), max_images)\n\n    def iterate_images():\n        for idx, img in enumerate(images):\n            yield dict(img=img, label=int(labels[idx]))\n            if idx >= max_idx-1:\n                break\n\n    return max_idx, iterate_images()\n\n#----------------------------------------------------------------------------\n\ndef make_transform(\n    transform: Optional[str],\n    output_width: Optional[int],\n    output_height: Optional[int]\n) -> Callable[[np.ndarray], Optional[np.ndarray]]:\n    def scale(width, height, img):\n        w = img.shape[1]\n        h = img.shape[0]\n        if width == w and height == h:\n            return img\n        img = PIL.Image.fromarray(img)\n        ww = width if width is not None else w\n        hh = height if height is not None else h\n        img = img.resize((ww, hh), PIL.Image.LANCZOS)\n        return np.array(img)\n\n    def center_crop(width, height, img):\n        crop = np.min(img.shape[:2])\n        img = img[(img.shape[0] - crop) // 2 : (img.shape[0] + crop) // 2, (img.shape[1] - crop) // 2 : (img.shape[1] + crop) // 2]\n        img = PIL.Image.fromarray(img, 'RGB')\n        img = img.resize((width, height), PIL.Image.LANCZOS)\n        return np.array(img)\n\n    def center_crop_wide(width, height, img):\n        ch = int(np.round(width * img.shape[0] / img.shape[1]))\n        if img.shape[1] < width or ch < height:\n            return None\n\n        img = img[(img.shape[0] - ch) // 2 : (img.shape[0] + ch) // 2]\n        img = PIL.Image.fromarray(img, 'RGB')\n        img = img.resize((width, height), PIL.Image.LANCZOS)\n        img = np.array(img)\n\n        canvas = np.zeros([width, width, 3], dtype=np.uint8)\n        canvas[(width - height) // 2 : (width + height) // 2, :] = img\n        return canvas\n\n    if transform is None:\n        return functools.partial(scale, output_width, output_height)\n    if transform == 'center-crop':\n        if (output_width is None) or (output_height is None):\n            error ('must specify --resolution=WxH when using ' + transform + 'transform')\n        return functools.partial(center_crop, output_width, output_height)\n    if transform == 'center-crop-wide':\n        if (output_width is None) or (output_height is None):\n            error ('must specify --resolution=WxH when using ' + transform + ' transform')\n        return functools.partial(center_crop_wide, output_width, output_height)\n    assert False, 'unknown transform'\n\n#----------------------------------------------------------------------------\n\ndef open_dataset(source, *, max_images: Optional[int]):\n    if os.path.isdir(source):\n        if source.rstrip('/').endswith('_lmdb'):\n            return open_lmdb(source, max_images=max_images)\n        else:\n            return open_image_folder(source, max_images=max_images)\n    elif os.path.isfile(source):\n        if os.path.basename(source) == 'cifar-10-python.tar.gz':\n            return open_cifar10(source, max_images=max_images)\n        elif os.path.basename(source) == 'train-images-idx3-ubyte.gz':\n            return open_mnist(source, max_images=max_images)\n        elif file_ext(source) == 'zip':\n            return open_image_zip(source, max_images=max_images)\n        else:\n            assert False, 'unknown archive type'\n    else:\n        error(f'Missing input file or directory: {source}')\n\n#----------------------------------------------------------------------------\n\ndef open_dest(dest: str) -> Tuple[str, Callable[[str, Union[bytes, str]], None], Callable[[], None]]:\n    dest_ext = file_ext(dest)\n\n    if dest_ext == 'zip':\n        if os.path.dirname(dest) != '':\n            os.makedirs(os.path.dirname(dest), exist_ok=True)\n        zf = zipfile.ZipFile(file=dest, mode='w', compression=zipfile.ZIP_STORED)\n        def zip_write_bytes(fname: str, data: Union[bytes, str]):\n            zf.writestr(fname, data)\n        return '', zip_write_bytes, zf.close\n    else:\n        # If the output folder already exists, check that is is\n        # empty.\n        #\n        # Note: creating the output directory is not strictly\n        # necessary as folder_write_bytes() also mkdirs, but it's better\n        # to give an error message earlier in case the dest folder\n        # somehow cannot be created.\n        if os.path.isdir(dest) and len(os.listdir(dest)) != 0:\n            error('--dest folder must be empty')\n        os.makedirs(dest, exist_ok=True)\n\n        def folder_write_bytes(fname: str, data: Union[bytes, str]):\n            os.makedirs(os.path.dirname(fname), exist_ok=True)\n            with open(fname, 'wb') as fout:\n                if isinstance(data, str):\n                    data = data.encode('utf8')\n                fout.write(data)\n        return dest, folder_write_bytes, lambda: None\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.pass_context\n@click.option('--source', help='Directory or archive name for input dataset', required=True, metavar='PATH')\n@click.option('--dest', help='Output directory or archive name for output dataset', required=True, metavar='PATH')\n@click.option('--max-images', help='Output only up to `max-images` images', type=int, default=None)\n@click.option('--transform', help='Input crop/resize mode', type=click.Choice(['center-crop', 'center-crop-wide']))\n@click.option('--resolution', help='Output resolution (e.g., \\'512x512\\')', metavar='WxH', type=parse_tuple)\ndef convert_dataset(\n    ctx: click.Context,\n    source: str,\n    dest: str,\n    max_images: Optional[int],\n    transform: Optional[str],\n    resolution: Optional[Tuple[int, int]]\n):\n    \"\"\"Convert an image dataset into a dataset archive usable with StyleGAN2 ADA PyTorch.\n\n    The input dataset format is guessed from the --source argument:\n\n    \\b\n    --source *_lmdb/                    Load LSUN dataset\n    --source cifar-10-python.tar.gz     Load CIFAR-10 dataset\n    --source train-images-idx3-ubyte.gz Load MNIST dataset\n    --source path/                      Recursively load all images from path/\n    --source dataset.zip                Recursively load all images from dataset.zip\n\n    Specifying the output format and path:\n\n    \\b\n    --dest /path/to/dir                 Save output files under /path/to/dir\n    --dest /path/to/dataset.zip         Save output files into /path/to/dataset.zip\n\n    The output dataset format can be either an image folder or an uncompressed zip archive.\n    Zip archives makes it easier to move datasets around file servers and clusters, and may\n    offer better training performance on network file systems.\n\n    Images within the dataset archive will be stored as uncompressed PNG.\n    Uncompresed PNGs can be efficiently decoded in the training loop.\n\n    Class labels are stored in a file called 'dataset.json' that is stored at the\n    dataset root folder.  This file has the following structure:\n\n    \\b\n    {\n        \"labels\": [\n            [\"00000/img00000000.png\",6],\n            [\"00000/img00000001.png\",9],\n            ... repeated for every image in the datase\n            [\"00049/img00049999.png\",1]\n        ]\n    }\n\n    If the 'dataset.json' file cannot be found, the dataset is interpreted as\n    not containing class labels.\n\n    Image scale/crop and resolution requirements:\n\n    Output images must be square-shaped and they must all have the same power-of-two\n    dimensions.\n\n    To scale arbitrary input image size to a specific width and height, use the\n    --resolution option.  Output resolution will be either the original\n    input resolution (if resolution was not specified) or the one specified with\n    --resolution option.\n\n    Use the --transform=center-crop or --transform=center-crop-wide options to apply a\n    center crop transform on the input image.  These options should be used with the\n    --resolution option.  For example:\n\n    \\b\n    python dataset_tool.py --source LSUN/raw/cat_lmdb --dest /tmp/lsun_cat \\\\\n        --transform=center-crop-wide --resolution=512x384\n    \"\"\"\n\n    PIL.Image.init() # type: ignore\n\n    if dest == '':\n        ctx.fail('--dest output filename or directory must not be an empty string')\n\n    num_files, input_iter = open_dataset(source, max_images=max_images)\n    archive_root_dir, save_bytes, close_dest = open_dest(dest)\n\n    if resolution is None: resolution = (None, None)\n    transform_image = make_transform(transform, *resolution)\n\n    dataset_attrs = None\n\n    labels = []\n    for idx, image in tqdm(enumerate(input_iter), total=num_files):\n        idx_str = f'{idx:08d}'\n        archive_fname = f'{idx_str[:5]}/img{idx_str}.png'\n\n        # Apply crop and resize.\n        img = transform_image(image['img'])\n\n        # Transform may drop images.\n        if img is None:\n            continue\n\n        # Error check to require uniform image attributes across\n        # the whole dataset.\n        channels = img.shape[2] if img.ndim == 3 else 1\n        cur_image_attrs = {\n            'width': img.shape[1],\n            'height': img.shape[0],\n            'channels': channels\n        }\n        if dataset_attrs is None:\n            dataset_attrs = cur_image_attrs\n            width = dataset_attrs['width']\n            height = dataset_attrs['height']\n            if width != height:\n                error(f'Image dimensions after scale and crop are required to be square.  Got {width}x{height}')\n            if dataset_attrs['channels'] not in [1, 3]:\n                error('Input images must be stored as RGB or grayscale')\n            if width != 2 ** int(np.floor(np.log2(width))):\n                error('Image width/height after scale and crop are required to be power-of-two')\n        elif dataset_attrs != cur_image_attrs:\n            err = [f'  dataset {k}/cur image {k}: {dataset_attrs[k]}/{cur_image_attrs[k]}' for k in dataset_attrs.keys()] # pylint: disable=unsubscriptable-object\n            error(f'Image {archive_fname} attributes must be equal across all images of the dataset.  Got:\\n' + '\\n'.join(err))\n\n        # Save the image as an uncompressed PNG.\n        img = PIL.Image.fromarray(img, { 1: 'L', 3: 'RGB' }[channels])\n        image_bits = io.BytesIO()\n        img.save(image_bits, format='png', compress_level=0, optimize=False)\n        save_bytes(os.path.join(archive_root_dir, archive_fname), image_bits.getbuffer())\n        labels.append([archive_fname, image['label']] if image['label'] is not None else None)\n\n    metadata = {\n        'labels': labels if all(x is not None for x in labels) else None\n    }\n    save_bytes(os.path.join(archive_root_dir, 'dataset.json'), json.dumps(metadata))\n    close_dest()\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    convert_dataset() # pylint: disable=no-value-for-parameter\n"
        },
        {
          "name": "dnnlib",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.3837890625,
          "content": "name: stylegan3\nchannels:\n  - pytorch\n  - nvidia\ndependencies:\n  - python >= 3.8\n  - pip\n  - numpy>=1.20\n  - click>=8.0\n  - pillow=8.3.1\n  - scipy=1.7.1\n  - pytorch=1.9.1\n  - cudatoolkit=11.1\n  - requests=2.26.0\n  - tqdm=4.62.2\n  - ninja=1.10.2\n  - matplotlib=3.4.2\n  - imageio=2.9.0\n  - pip:\n    - imgui==1.3.0\n    - glfw==2.2.0\n    - pyopengl==3.1.5\n    - imageio-ffmpeg==0.4.3\n    - pyspng\n"
        },
        {
          "name": "gen_images.py",
          "type": "blob",
          "size": 5.6005859375,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Generate images using pretrained network pickle.\"\"\"\n\nimport os\nimport re\nfrom typing import List, Optional, Tuple, Union\n\nimport click\nimport dnnlib\nimport numpy as np\nimport PIL.Image\nimport torch\n\nimport legacy\n\n#----------------------------------------------------------------------------\n\ndef parse_range(s: Union[str, List]) -> List[int]:\n    '''Parse a comma separated list of numbers or ranges and return a list of ints.\n\n    Example: '1,2,5-10' returns [1, 2, 5, 6, 7]\n    '''\n    if isinstance(s, list): return s\n    ranges = []\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    for p in s.split(','):\n        m = range_re.match(p)\n        if m:\n            ranges.extend(range(int(m.group(1)), int(m.group(2))+1))\n        else:\n            ranges.append(int(p))\n    return ranges\n\n#----------------------------------------------------------------------------\n\ndef parse_vec2(s: Union[str, Tuple[float, float]]) -> Tuple[float, float]:\n    '''Parse a floating point 2-vector of syntax 'a,b'.\n\n    Example:\n        '0,1' returns (0,1)\n    '''\n    if isinstance(s, tuple): return s\n    parts = s.split(',')\n    if len(parts) == 2:\n        return (float(parts[0]), float(parts[1]))\n    raise ValueError(f'cannot parse 2-vector {s}')\n\n#----------------------------------------------------------------------------\n\ndef make_transform(translate: Tuple[float,float], angle: float):\n    m = np.eye(3)\n    s = np.sin(angle/360.0*np.pi*2)\n    c = np.cos(angle/360.0*np.pi*2)\n    m[0][0] = c\n    m[0][1] = s\n    m[0][2] = translate[0]\n    m[1][0] = -s\n    m[1][1] = c\n    m[1][2] = translate[1]\n    return m\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.option('--network', 'network_pkl', help='Network pickle filename', required=True)\n@click.option('--seeds', type=parse_range, help='List of random seeds (e.g., \\'0,1,4-6\\')', required=True)\n@click.option('--trunc', 'truncation_psi', type=float, help='Truncation psi', default=1, show_default=True)\n@click.option('--class', 'class_idx', type=int, help='Class label (unconditional if not specified)')\n@click.option('--noise-mode', help='Noise mode', type=click.Choice(['const', 'random', 'none']), default='const', show_default=True)\n@click.option('--translate', help='Translate XY-coordinate (e.g. \\'0.3,1\\')', type=parse_vec2, default='0,0', show_default=True, metavar='VEC2')\n@click.option('--rotate', help='Rotation angle in degrees', type=float, default=0, show_default=True, metavar='ANGLE')\n@click.option('--outdir', help='Where to save the output images', type=str, required=True, metavar='DIR')\ndef generate_images(\n    network_pkl: str,\n    seeds: List[int],\n    truncation_psi: float,\n    noise_mode: str,\n    outdir: str,\n    translate: Tuple[float,float],\n    rotate: float,\n    class_idx: Optional[int]\n):\n    \"\"\"Generate images using pretrained network pickle.\n\n    Examples:\n\n    \\b\n    # Generate an image using pre-trained AFHQv2 model (\"Ours\" in Figure 1, left).\n    python gen_images.py --outdir=out --trunc=1 --seeds=2 \\\\\n        --network=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-afhqv2-512x512.pkl\n\n    \\b\n    # Generate uncurated images with truncation using the MetFaces-U dataset\n    python gen_images.py --outdir=out --trunc=0.7 --seeds=600-605 \\\\\n        --network=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-metfacesu-1024x1024.pkl\n    \"\"\"\n\n    print('Loading networks from \"%s\"...' % network_pkl)\n    device = torch.device('cuda')\n    with dnnlib.util.open_url(network_pkl) as f:\n        G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n\n    os.makedirs(outdir, exist_ok=True)\n\n    # Labels.\n    label = torch.zeros([1, G.c_dim], device=device)\n    if G.c_dim != 0:\n        if class_idx is None:\n            raise click.ClickException('Must specify class label with --class when using a conditional network')\n        label[:, class_idx] = 1\n    else:\n        if class_idx is not None:\n            print ('warn: --class=lbl ignored when running on an unconditional network')\n\n    # Generate images.\n    for seed_idx, seed in enumerate(seeds):\n        print('Generating image for seed %d (%d/%d) ...' % (seed, seed_idx, len(seeds)))\n        z = torch.from_numpy(np.random.RandomState(seed).randn(1, G.z_dim)).to(device)\n\n        # Construct an inverse rotation/translation matrix and pass to the generator.  The\n        # generator expects this matrix as an inverse to avoid potentially failing numerical\n        # operations in the network.\n        if hasattr(G.synthesis, 'input'):\n            m = make_transform(translate, rotate)\n            m = np.linalg.inv(m)\n            G.synthesis.input.transform.copy_(torch.from_numpy(m))\n\n        img = G(z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n        img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n        PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB').save(f'{outdir}/seed{seed:04d}.png')\n\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    generate_images() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "gen_video.py",
          "type": "blob",
          "size": 7.1123046875,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Generate lerp videos using pretrained network pickle.\"\"\"\n\nimport copy\nimport os\nimport re\nfrom typing import List, Optional, Tuple, Union\n\nimport click\nimport dnnlib\nimport imageio\nimport numpy as np\nimport scipy.interpolate\nimport torch\nfrom tqdm import tqdm\n\nimport legacy\n\n#----------------------------------------------------------------------------\n\ndef layout_grid(img, grid_w=None, grid_h=1, float_to_uint8=True, chw_to_hwc=True, to_numpy=True):\n    batch_size, channels, img_h, img_w = img.shape\n    if grid_w is None:\n        grid_w = batch_size // grid_h\n    assert batch_size == grid_w * grid_h\n    if float_to_uint8:\n        img = (img * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n    img = img.reshape(grid_h, grid_w, channels, img_h, img_w)\n    img = img.permute(2, 0, 3, 1, 4)\n    img = img.reshape(channels, grid_h * img_h, grid_w * img_w)\n    if chw_to_hwc:\n        img = img.permute(1, 2, 0)\n    if to_numpy:\n        img = img.cpu().numpy()\n    return img\n\n#----------------------------------------------------------------------------\n\ndef gen_interp_video(G, mp4: str, seeds, shuffle_seed=None, w_frames=60*4, kind='cubic', grid_dims=(1,1), num_keyframes=None, wraps=2, psi=1, device=torch.device('cuda'), **video_kwargs):\n    grid_w = grid_dims[0]\n    grid_h = grid_dims[1]\n\n    if num_keyframes is None:\n        if len(seeds) % (grid_w*grid_h) != 0:\n            raise ValueError('Number of input seeds must be divisible by grid W*H')\n        num_keyframes = len(seeds) // (grid_w*grid_h)\n\n    all_seeds = np.zeros(num_keyframes*grid_h*grid_w, dtype=np.int64)\n    for idx in range(num_keyframes*grid_h*grid_w):\n        all_seeds[idx] = seeds[idx % len(seeds)]\n\n    if shuffle_seed is not None:\n        rng = np.random.RandomState(seed=shuffle_seed)\n        rng.shuffle(all_seeds)\n\n    zs = torch.from_numpy(np.stack([np.random.RandomState(seed).randn(G.z_dim) for seed in all_seeds])).to(device)\n    ws = G.mapping(z=zs, c=None, truncation_psi=psi)\n    _ = G.synthesis(ws[:1]) # warm up\n    ws = ws.reshape(grid_h, grid_w, num_keyframes, *ws.shape[1:])\n\n    # Interpolation.\n    grid = []\n    for yi in range(grid_h):\n        row = []\n        for xi in range(grid_w):\n            x = np.arange(-num_keyframes * wraps, num_keyframes * (wraps + 1))\n            y = np.tile(ws[yi][xi].cpu().numpy(), [wraps * 2 + 1, 1, 1])\n            interp = scipy.interpolate.interp1d(x, y, kind=kind, axis=0)\n            row.append(interp)\n        grid.append(row)\n\n    # Render video.\n    video_out = imageio.get_writer(mp4, mode='I', fps=60, codec='libx264', **video_kwargs)\n    for frame_idx in tqdm(range(num_keyframes * w_frames)):\n        imgs = []\n        for yi in range(grid_h):\n            for xi in range(grid_w):\n                interp = grid[yi][xi]\n                w = torch.from_numpy(interp(frame_idx / w_frames)).to(device)\n                img = G.synthesis(ws=w.unsqueeze(0), noise_mode='const')[0]\n                imgs.append(img)\n        video_out.append_data(layout_grid(torch.stack(imgs), grid_w=grid_w, grid_h=grid_h))\n    video_out.close()\n\n#----------------------------------------------------------------------------\n\ndef parse_range(s: Union[str, List[int]]) -> List[int]:\n    '''Parse a comma separated list of numbers or ranges and return a list of ints.\n\n    Example: '1,2,5-10' returns [1, 2, 5, 6, 7]\n    '''\n    if isinstance(s, list): return s\n    ranges = []\n    range_re = re.compile(r'^(\\d+)-(\\d+)$')\n    for p in s.split(','):\n        m = range_re.match(p)\n        if m:\n            ranges.extend(range(int(m.group(1)), int(m.group(2))+1))\n        else:\n            ranges.append(int(p))\n    return ranges\n\n#----------------------------------------------------------------------------\n\ndef parse_tuple(s: Union[str, Tuple[int,int]]) -> Tuple[int, int]:\n    '''Parse a 'M,N' or 'MxN' integer tuple.\n\n    Example:\n        '4x2' returns (4,2)\n        '0,1' returns (0,1)\n    '''\n    if isinstance(s, tuple): return s\n    m = re.match(r'^(\\d+)[x,](\\d+)$', s)\n    if m:\n        return (int(m.group(1)), int(m.group(2)))\n    raise ValueError(f'cannot parse tuple {s}')\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.option('--network', 'network_pkl', help='Network pickle filename', required=True)\n@click.option('--seeds', type=parse_range, help='List of random seeds', required=True)\n@click.option('--shuffle-seed', type=int, help='Random seed to use for shuffling seed order', default=None)\n@click.option('--grid', type=parse_tuple, help='Grid width/height, e.g. \\'4x3\\' (default: 1x1)', default=(1,1))\n@click.option('--num-keyframes', type=int, help='Number of seeds to interpolate through.  If not specified, determine based on the length of the seeds array given by --seeds.', default=None)\n@click.option('--w-frames', type=int, help='Number of frames to interpolate between latents', default=120)\n@click.option('--trunc', 'truncation_psi', type=float, help='Truncation psi', default=1, show_default=True)\n@click.option('--output', help='Output .mp4 filename', type=str, required=True, metavar='FILE')\ndef generate_images(\n    network_pkl: str,\n    seeds: List[int],\n    shuffle_seed: Optional[int],\n    truncation_psi: float,\n    grid: Tuple[int,int],\n    num_keyframes: Optional[int],\n    w_frames: int,\n    output: str\n):\n    \"\"\"Render a latent vector interpolation video.\n\n    Examples:\n\n    \\b\n    # Render a 4x2 grid of interpolations for seeds 0 through 31.\n    python gen_video.py --output=lerp.mp4 --trunc=1 --seeds=0-31 --grid=4x2 \\\\\n        --network=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-afhqv2-512x512.pkl\n\n    Animation length and seed keyframes:\n\n    The animation length is either determined based on the --seeds value or explicitly\n    specified using the --num-keyframes option.\n\n    When num keyframes is specified with --num-keyframes, the output video length\n    will be 'num_keyframes*w_frames' frames.\n\n    If --num-keyframes is not specified, the number of seeds given with\n    --seeds must be divisible by grid size W*H (--grid).  In this case the\n    output video length will be '# seeds/(w*h)*w_frames' frames.\n    \"\"\"\n\n    print('Loading networks from \"%s\"...' % network_pkl)\n    device = torch.device('cuda')\n    with dnnlib.util.open_url(network_pkl) as f:\n        G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n\n    gen_interp_video(G=G, mp4=output, bitrate='12M', grid_dims=grid, num_keyframes=num_keyframes, w_frames=w_frames, seeds=seeds, shuffle_seed=shuffle_seed, psi=truncation_psi)\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    generate_images() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "gui_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "legacy.py",
          "type": "blob",
          "size": 16.1728515625,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Converting legacy network pickle into the new format.\"\"\"\n\nimport click\nimport pickle\nimport re\nimport copy\nimport numpy as np\nimport torch\nimport dnnlib\nfrom torch_utils import misc\n\n#----------------------------------------------------------------------------\n\ndef load_network_pkl(f, force_fp16=False):\n    data = _LegacyUnpickler(f).load()\n\n    # Legacy TensorFlow pickle => convert.\n    if isinstance(data, tuple) and len(data) == 3 and all(isinstance(net, _TFNetworkStub) for net in data):\n        tf_G, tf_D, tf_Gs = data\n        G = convert_tf_generator(tf_G)\n        D = convert_tf_discriminator(tf_D)\n        G_ema = convert_tf_generator(tf_Gs)\n        data = dict(G=G, D=D, G_ema=G_ema)\n\n    # Add missing fields.\n    if 'training_set_kwargs' not in data:\n        data['training_set_kwargs'] = None\n    if 'augment_pipe' not in data:\n        data['augment_pipe'] = None\n\n    # Validate contents.\n    assert isinstance(data['G'], torch.nn.Module)\n    assert isinstance(data['D'], torch.nn.Module)\n    assert isinstance(data['G_ema'], torch.nn.Module)\n    assert isinstance(data['training_set_kwargs'], (dict, type(None)))\n    assert isinstance(data['augment_pipe'], (torch.nn.Module, type(None)))\n\n    # Force FP16.\n    if force_fp16:\n        for key in ['G', 'D', 'G_ema']:\n            old = data[key]\n            kwargs = copy.deepcopy(old.init_kwargs)\n            fp16_kwargs = kwargs.get('synthesis_kwargs', kwargs)\n            fp16_kwargs.num_fp16_res = 4\n            fp16_kwargs.conv_clamp = 256\n            if kwargs != old.init_kwargs:\n                new = type(old)(**kwargs).eval().requires_grad_(False)\n                misc.copy_params_and_buffers(old, new, require_all=True)\n                data[key] = new\n    return data\n\n#----------------------------------------------------------------------------\n\nclass _TFNetworkStub(dnnlib.EasyDict):\n    pass\n\nclass _LegacyUnpickler(pickle.Unpickler):\n    def find_class(self, module, name):\n        if module == 'dnnlib.tflib.network' and name == 'Network':\n            return _TFNetworkStub\n        return super().find_class(module, name)\n\n#----------------------------------------------------------------------------\n\ndef _collect_tf_params(tf_net):\n    # pylint: disable=protected-access\n    tf_params = dict()\n    def recurse(prefix, tf_net):\n        for name, value in tf_net.variables:\n            tf_params[prefix + name] = value\n        for name, comp in tf_net.components.items():\n            recurse(prefix + name + '/', comp)\n    recurse('', tf_net)\n    return tf_params\n\n#----------------------------------------------------------------------------\n\ndef _populate_module_params(module, *patterns):\n    for name, tensor in misc.named_params_and_buffers(module):\n        found = False\n        value = None\n        for pattern, value_fn in zip(patterns[0::2], patterns[1::2]):\n            match = re.fullmatch(pattern, name)\n            if match:\n                found = True\n                if value_fn is not None:\n                    value = value_fn(*match.groups())\n                break\n        try:\n            assert found\n            if value is not None:\n                tensor.copy_(torch.from_numpy(np.array(value)))\n        except:\n            print(name, list(tensor.shape))\n            raise\n\n#----------------------------------------------------------------------------\n\ndef convert_tf_generator(tf_G):\n    if tf_G.version < 4:\n        raise ValueError('TensorFlow pickle version too low')\n\n    # Collect kwargs.\n    tf_kwargs = tf_G.static_kwargs\n    known_kwargs = set()\n    def kwarg(tf_name, default=None, none=None):\n        known_kwargs.add(tf_name)\n        val = tf_kwargs.get(tf_name, default)\n        return val if val is not None else none\n\n    # Convert kwargs.\n    from training import networks_stylegan2\n    network_class = networks_stylegan2.Generator\n    kwargs = dnnlib.EasyDict(\n        z_dim               = kwarg('latent_size',          512),\n        c_dim               = kwarg('label_size',           0),\n        w_dim               = kwarg('dlatent_size',         512),\n        img_resolution      = kwarg('resolution',           1024),\n        img_channels        = kwarg('num_channels',         3),\n        channel_base        = kwarg('fmap_base',            16384) * 2,\n        channel_max         = kwarg('fmap_max',             512),\n        num_fp16_res        = kwarg('num_fp16_res',         0),\n        conv_clamp          = kwarg('conv_clamp',           None),\n        architecture        = kwarg('architecture',         'skip'),\n        resample_filter     = kwarg('resample_kernel',      [1,3,3,1]),\n        use_noise           = kwarg('use_noise',            True),\n        activation          = kwarg('nonlinearity',         'lrelu'),\n        mapping_kwargs      = dnnlib.EasyDict(\n            num_layers      = kwarg('mapping_layers',       8),\n            embed_features  = kwarg('label_fmaps',          None),\n            layer_features  = kwarg('mapping_fmaps',        None),\n            activation      = kwarg('mapping_nonlinearity', 'lrelu'),\n            lr_multiplier   = kwarg('mapping_lrmul',        0.01),\n            w_avg_beta      = kwarg('w_avg_beta',           0.995,  none=1),\n        ),\n    )\n\n    # Check for unknown kwargs.\n    kwarg('truncation_psi')\n    kwarg('truncation_cutoff')\n    kwarg('style_mixing_prob')\n    kwarg('structure')\n    kwarg('conditioning')\n    kwarg('fused_modconv')\n    unknown_kwargs = list(set(tf_kwargs.keys()) - known_kwargs)\n    if len(unknown_kwargs) > 0:\n        raise ValueError('Unknown TensorFlow kwarg', unknown_kwargs[0])\n\n    # Collect params.\n    tf_params = _collect_tf_params(tf_G)\n    for name, value in list(tf_params.items()):\n        match = re.fullmatch(r'ToRGB_lod(\\d+)/(.*)', name)\n        if match:\n            r = kwargs.img_resolution // (2 ** int(match.group(1)))\n            tf_params[f'{r}x{r}/ToRGB/{match.group(2)}'] = value\n            kwargs.synthesis.kwargs.architecture = 'orig'\n    #for name, value in tf_params.items(): print(f'{name:<50s}{list(value.shape)}')\n\n    # Convert params.\n    G = network_class(**kwargs).eval().requires_grad_(False)\n    # pylint: disable=unnecessary-lambda\n    # pylint: disable=f-string-without-interpolation\n    _populate_module_params(G,\n        r'mapping\\.w_avg',                                  lambda:     tf_params[f'dlatent_avg'],\n        r'mapping\\.embed\\.weight',                          lambda:     tf_params[f'mapping/LabelEmbed/weight'].transpose(),\n        r'mapping\\.embed\\.bias',                            lambda:     tf_params[f'mapping/LabelEmbed/bias'],\n        r'mapping\\.fc(\\d+)\\.weight',                        lambda i:   tf_params[f'mapping/Dense{i}/weight'].transpose(),\n        r'mapping\\.fc(\\d+)\\.bias',                          lambda i:   tf_params[f'mapping/Dense{i}/bias'],\n        r'synthesis\\.b4\\.const',                            lambda:     tf_params[f'synthesis/4x4/Const/const'][0],\n        r'synthesis\\.b4\\.conv1\\.weight',                    lambda:     tf_params[f'synthesis/4x4/Conv/weight'].transpose(3, 2, 0, 1),\n        r'synthesis\\.b4\\.conv1\\.bias',                      lambda:     tf_params[f'synthesis/4x4/Conv/bias'],\n        r'synthesis\\.b4\\.conv1\\.noise_const',               lambda:     tf_params[f'synthesis/noise0'][0, 0],\n        r'synthesis\\.b4\\.conv1\\.noise_strength',            lambda:     tf_params[f'synthesis/4x4/Conv/noise_strength'],\n        r'synthesis\\.b4\\.conv1\\.affine\\.weight',            lambda:     tf_params[f'synthesis/4x4/Conv/mod_weight'].transpose(),\n        r'synthesis\\.b4\\.conv1\\.affine\\.bias',              lambda:     tf_params[f'synthesis/4x4/Conv/mod_bias'] + 1,\n        r'synthesis\\.b(\\d+)\\.conv0\\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/weight'][::-1, ::-1].transpose(3, 2, 0, 1),\n        r'synthesis\\.b(\\d+)\\.conv0\\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/bias'],\n        r'synthesis\\.b(\\d+)\\.conv0\\.noise_const',           lambda r:   tf_params[f'synthesis/noise{int(np.log2(int(r)))*2-5}'][0, 0],\n        r'synthesis\\.b(\\d+)\\.conv0\\.noise_strength',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/noise_strength'],\n        r'synthesis\\.b(\\d+)\\.conv0\\.affine\\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/mod_weight'].transpose(),\n        r'synthesis\\.b(\\d+)\\.conv0\\.affine\\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/Conv0_up/mod_bias'] + 1,\n        r'synthesis\\.b(\\d+)\\.conv1\\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/weight'].transpose(3, 2, 0, 1),\n        r'synthesis\\.b(\\d+)\\.conv1\\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/bias'],\n        r'synthesis\\.b(\\d+)\\.conv1\\.noise_const',           lambda r:   tf_params[f'synthesis/noise{int(np.log2(int(r)))*2-4}'][0, 0],\n        r'synthesis\\.b(\\d+)\\.conv1\\.noise_strength',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/noise_strength'],\n        r'synthesis\\.b(\\d+)\\.conv1\\.affine\\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/mod_weight'].transpose(),\n        r'synthesis\\.b(\\d+)\\.conv1\\.affine\\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/Conv1/mod_bias'] + 1,\n        r'synthesis\\.b(\\d+)\\.torgb\\.weight',                lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/weight'].transpose(3, 2, 0, 1),\n        r'synthesis\\.b(\\d+)\\.torgb\\.bias',                  lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/bias'],\n        r'synthesis\\.b(\\d+)\\.torgb\\.affine\\.weight',        lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/mod_weight'].transpose(),\n        r'synthesis\\.b(\\d+)\\.torgb\\.affine\\.bias',          lambda r:   tf_params[f'synthesis/{r}x{r}/ToRGB/mod_bias'] + 1,\n        r'synthesis\\.b(\\d+)\\.skip\\.weight',                 lambda r:   tf_params[f'synthesis/{r}x{r}/Skip/weight'][::-1, ::-1].transpose(3, 2, 0, 1),\n        r'.*\\.resample_filter',                             None,\n        r'.*\\.act_filter',                                  None,\n    )\n    return G\n\n#----------------------------------------------------------------------------\n\ndef convert_tf_discriminator(tf_D):\n    if tf_D.version < 4:\n        raise ValueError('TensorFlow pickle version too low')\n\n    # Collect kwargs.\n    tf_kwargs = tf_D.static_kwargs\n    known_kwargs = set()\n    def kwarg(tf_name, default=None):\n        known_kwargs.add(tf_name)\n        return tf_kwargs.get(tf_name, default)\n\n    # Convert kwargs.\n    kwargs = dnnlib.EasyDict(\n        c_dim                   = kwarg('label_size',           0),\n        img_resolution          = kwarg('resolution',           1024),\n        img_channels            = kwarg('num_channels',         3),\n        architecture            = kwarg('architecture',         'resnet'),\n        channel_base            = kwarg('fmap_base',            16384) * 2,\n        channel_max             = kwarg('fmap_max',             512),\n        num_fp16_res            = kwarg('num_fp16_res',         0),\n        conv_clamp              = kwarg('conv_clamp',           None),\n        cmap_dim                = kwarg('mapping_fmaps',        None),\n        block_kwargs = dnnlib.EasyDict(\n            activation          = kwarg('nonlinearity',         'lrelu'),\n            resample_filter     = kwarg('resample_kernel',      [1,3,3,1]),\n            freeze_layers       = kwarg('freeze_layers',        0),\n        ),\n        mapping_kwargs = dnnlib.EasyDict(\n            num_layers          = kwarg('mapping_layers',       0),\n            embed_features      = kwarg('mapping_fmaps',        None),\n            layer_features      = kwarg('mapping_fmaps',        None),\n            activation          = kwarg('nonlinearity',         'lrelu'),\n            lr_multiplier       = kwarg('mapping_lrmul',        0.1),\n        ),\n        epilogue_kwargs = dnnlib.EasyDict(\n            mbstd_group_size    = kwarg('mbstd_group_size',     None),\n            mbstd_num_channels  = kwarg('mbstd_num_features',   1),\n            activation          = kwarg('nonlinearity',         'lrelu'),\n        ),\n    )\n\n    # Check for unknown kwargs.\n    kwarg('structure')\n    kwarg('conditioning')\n    unknown_kwargs = list(set(tf_kwargs.keys()) - known_kwargs)\n    if len(unknown_kwargs) > 0:\n        raise ValueError('Unknown TensorFlow kwarg', unknown_kwargs[0])\n\n    # Collect params.\n    tf_params = _collect_tf_params(tf_D)\n    for name, value in list(tf_params.items()):\n        match = re.fullmatch(r'FromRGB_lod(\\d+)/(.*)', name)\n        if match:\n            r = kwargs.img_resolution // (2 ** int(match.group(1)))\n            tf_params[f'{r}x{r}/FromRGB/{match.group(2)}'] = value\n            kwargs.architecture = 'orig'\n    #for name, value in tf_params.items(): print(f'{name:<50s}{list(value.shape)}')\n\n    # Convert params.\n    from training import networks_stylegan2\n    D = networks_stylegan2.Discriminator(**kwargs).eval().requires_grad_(False)\n    # pylint: disable=unnecessary-lambda\n    # pylint: disable=f-string-without-interpolation\n    _populate_module_params(D,\n        r'b(\\d+)\\.fromrgb\\.weight',     lambda r:       tf_params[f'{r}x{r}/FromRGB/weight'].transpose(3, 2, 0, 1),\n        r'b(\\d+)\\.fromrgb\\.bias',       lambda r:       tf_params[f'{r}x{r}/FromRGB/bias'],\n        r'b(\\d+)\\.conv(\\d+)\\.weight',   lambda r, i:    tf_params[f'{r}x{r}/Conv{i}{[\"\",\"_down\"][int(i)]}/weight'].transpose(3, 2, 0, 1),\n        r'b(\\d+)\\.conv(\\d+)\\.bias',     lambda r, i:    tf_params[f'{r}x{r}/Conv{i}{[\"\",\"_down\"][int(i)]}/bias'],\n        r'b(\\d+)\\.skip\\.weight',        lambda r:       tf_params[f'{r}x{r}/Skip/weight'].transpose(3, 2, 0, 1),\n        r'mapping\\.embed\\.weight',      lambda:         tf_params[f'LabelEmbed/weight'].transpose(),\n        r'mapping\\.embed\\.bias',        lambda:         tf_params[f'LabelEmbed/bias'],\n        r'mapping\\.fc(\\d+)\\.weight',    lambda i:       tf_params[f'Mapping{i}/weight'].transpose(),\n        r'mapping\\.fc(\\d+)\\.bias',      lambda i:       tf_params[f'Mapping{i}/bias'],\n        r'b4\\.conv\\.weight',            lambda:         tf_params[f'4x4/Conv/weight'].transpose(3, 2, 0, 1),\n        r'b4\\.conv\\.bias',              lambda:         tf_params[f'4x4/Conv/bias'],\n        r'b4\\.fc\\.weight',              lambda:         tf_params[f'4x4/Dense0/weight'].transpose(),\n        r'b4\\.fc\\.bias',                lambda:         tf_params[f'4x4/Dense0/bias'],\n        r'b4\\.out\\.weight',             lambda:         tf_params[f'Output/weight'].transpose(),\n        r'b4\\.out\\.bias',               lambda:         tf_params[f'Output/bias'],\n        r'.*\\.resample_filter',         None,\n    )\n    return D\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.option('--source', help='Input pickle', required=True, metavar='PATH')\n@click.option('--dest', help='Output pickle', required=True, metavar='PATH')\n@click.option('--force-fp16', help='Force the networks to use FP16', type=bool, default=False, metavar='BOOL', show_default=True)\ndef convert_network_pickle(source, dest, force_fp16):\n    \"\"\"Convert legacy network pickle into the native PyTorch format.\n\n    The tool is able to load the main network configurations exported using the TensorFlow version of StyleGAN2 or StyleGAN2-ADA.\n    It does not support e.g. StyleGAN2-ADA comparison methods, StyleGAN2 configs A-D, or StyleGAN1 networks.\n\n    Example:\n\n    \\b\n    python legacy.py \\\\\n        --source=https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/stylegan2-cat-config-f.pkl \\\\\n        --dest=stylegan2-cat-config-f.pkl\n    \"\"\"\n    print(f'Loading \"{source}\"...')\n    with dnnlib.util.open_url(source) as f:\n        data = load_network_pkl(f, force_fp16=force_fp16)\n    print(f'Saving \"{dest}\"...')\n    with open(dest, 'wb') as f:\n        pickle.dump(data, f)\n    print('Done.')\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    convert_network_pickle() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "metrics",
          "type": "tree",
          "content": null
        },
        {
          "name": "torch_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 15.537109375,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\n\"\"\"Train a GAN using the techniques described in the paper\n\"Alias-Free Generative Adversarial Networks\".\"\"\"\n\nimport os\nimport click\nimport re\nimport json\nimport tempfile\nimport torch\n\nimport dnnlib\nfrom training import training_loop\nfrom metrics import metric_main\nfrom torch_utils import training_stats\nfrom torch_utils import custom_ops\n\n#----------------------------------------------------------------------------\n\ndef subprocess_fn(rank, c, temp_dir):\n    dnnlib.util.Logger(file_name=os.path.join(c.run_dir, 'log.txt'), file_mode='a', should_flush=True)\n\n    # Init torch.distributed.\n    if c.num_gpus > 1:\n        init_file = os.path.abspath(os.path.join(temp_dir, '.torch_distributed_init'))\n        if os.name == 'nt':\n            init_method = 'file:///' + init_file.replace('\\\\', '/')\n            torch.distributed.init_process_group(backend='gloo', init_method=init_method, rank=rank, world_size=c.num_gpus)\n        else:\n            init_method = f'file://{init_file}'\n            torch.distributed.init_process_group(backend='nccl', init_method=init_method, rank=rank, world_size=c.num_gpus)\n\n    # Init torch_utils.\n    sync_device = torch.device('cuda', rank) if c.num_gpus > 1 else None\n    training_stats.init_multiprocessing(rank=rank, sync_device=sync_device)\n    if rank != 0:\n        custom_ops.verbosity = 'none'\n\n    # Execute training loop.\n    training_loop.training_loop(rank=rank, **c)\n\n#----------------------------------------------------------------------------\n\ndef launch_training(c, desc, outdir, dry_run):\n    dnnlib.util.Logger(should_flush=True)\n\n    # Pick output directory.\n    prev_run_dirs = []\n    if os.path.isdir(outdir):\n        prev_run_dirs = [x for x in os.listdir(outdir) if os.path.isdir(os.path.join(outdir, x))]\n    prev_run_ids = [re.match(r'^\\d+', x) for x in prev_run_dirs]\n    prev_run_ids = [int(x.group()) for x in prev_run_ids if x is not None]\n    cur_run_id = max(prev_run_ids, default=-1) + 1\n    c.run_dir = os.path.join(outdir, f'{cur_run_id:05d}-{desc}')\n    assert not os.path.exists(c.run_dir)\n\n    # Print options.\n    print()\n    print('Training options:')\n    print(json.dumps(c, indent=2))\n    print()\n    print(f'Output directory:    {c.run_dir}')\n    print(f'Number of GPUs:      {c.num_gpus}')\n    print(f'Batch size:          {c.batch_size} images')\n    print(f'Training duration:   {c.total_kimg} kimg')\n    print(f'Dataset path:        {c.training_set_kwargs.path}')\n    print(f'Dataset size:        {c.training_set_kwargs.max_size} images')\n    print(f'Dataset resolution:  {c.training_set_kwargs.resolution}')\n    print(f'Dataset labels:      {c.training_set_kwargs.use_labels}')\n    print(f'Dataset x-flips:     {c.training_set_kwargs.xflip}')\n    print()\n\n    # Dry run?\n    if dry_run:\n        print('Dry run; exiting.')\n        return\n\n    # Create output directory.\n    print('Creating output directory...')\n    os.makedirs(c.run_dir)\n    with open(os.path.join(c.run_dir, 'training_options.json'), 'wt') as f:\n        json.dump(c, f, indent=2)\n\n    # Launch processes.\n    print('Launching processes...')\n    torch.multiprocessing.set_start_method('spawn')\n    with tempfile.TemporaryDirectory() as temp_dir:\n        if c.num_gpus == 1:\n            subprocess_fn(rank=0, c=c, temp_dir=temp_dir)\n        else:\n            torch.multiprocessing.spawn(fn=subprocess_fn, args=(c, temp_dir), nprocs=c.num_gpus)\n\n#----------------------------------------------------------------------------\n\ndef init_dataset_kwargs(data):\n    try:\n        dataset_kwargs = dnnlib.EasyDict(class_name='training.dataset.ImageFolderDataset', path=data, use_labels=True, max_size=None, xflip=False)\n        dataset_obj = dnnlib.util.construct_class_by_name(**dataset_kwargs) # Subclass of training.dataset.Dataset.\n        dataset_kwargs.resolution = dataset_obj.resolution # Be explicit about resolution.\n        dataset_kwargs.use_labels = dataset_obj.has_labels # Be explicit about labels.\n        dataset_kwargs.max_size = len(dataset_obj) # Be explicit about dataset size.\n        return dataset_kwargs, dataset_obj.name\n    except IOError as err:\n        raise click.ClickException(f'--data: {err}')\n\n#----------------------------------------------------------------------------\n\ndef parse_comma_separated_list(s):\n    if isinstance(s, list):\n        return s\n    if s is None or s.lower() == 'none' or s == '':\n        return []\n    return s.split(',')\n\n#----------------------------------------------------------------------------\n\n@click.command()\n\n# Required.\n@click.option('--outdir',       help='Where to save the results', metavar='DIR',                required=True)\n@click.option('--cfg',          help='Base configuration',                                      type=click.Choice(['stylegan3-t', 'stylegan3-r', 'stylegan2']), required=True)\n@click.option('--data',         help='Training data', metavar='[ZIP|DIR]',                      type=str, required=True)\n@click.option('--gpus',         help='Number of GPUs to use', metavar='INT',                    type=click.IntRange(min=1), required=True)\n@click.option('--batch',        help='Total batch size', metavar='INT',                         type=click.IntRange(min=1), required=True)\n@click.option('--gamma',        help='R1 regularization weight', metavar='FLOAT',               type=click.FloatRange(min=0), required=True)\n\n# Optional features.\n@click.option('--cond',         help='Train conditional model', metavar='BOOL',                 type=bool, default=False, show_default=True)\n@click.option('--mirror',       help='Enable dataset x-flips', metavar='BOOL',                  type=bool, default=False, show_default=True)\n@click.option('--aug',          help='Augmentation mode',                                       type=click.Choice(['noaug', 'ada', 'fixed']), default='ada', show_default=True)\n@click.option('--resume',       help='Resume from given network pickle', metavar='[PATH|URL]',  type=str)\n@click.option('--freezed',      help='Freeze first layers of D', metavar='INT',                 type=click.IntRange(min=0), default=0, show_default=True)\n\n# Misc hyperparameters.\n@click.option('--p',            help='Probability for --aug=fixed', metavar='FLOAT',            type=click.FloatRange(min=0, max=1), default=0.2, show_default=True)\n@click.option('--target',       help='Target value for --aug=ada', metavar='FLOAT',             type=click.FloatRange(min=0, max=1), default=0.6, show_default=True)\n@click.option('--batch-gpu',    help='Limit batch size per GPU', metavar='INT',                 type=click.IntRange(min=1))\n@click.option('--cbase',        help='Capacity multiplier', metavar='INT',                      type=click.IntRange(min=1), default=32768, show_default=True)\n@click.option('--cmax',         help='Max. feature maps', metavar='INT',                        type=click.IntRange(min=1), default=512, show_default=True)\n@click.option('--glr',          help='G learning rate  [default: varies]', metavar='FLOAT',     type=click.FloatRange(min=0))\n@click.option('--dlr',          help='D learning rate', metavar='FLOAT',                        type=click.FloatRange(min=0), default=0.002, show_default=True)\n@click.option('--map-depth',    help='Mapping network depth  [default: varies]', metavar='INT', type=click.IntRange(min=1))\n@click.option('--mbstd-group',  help='Minibatch std group size', metavar='INT',                 type=click.IntRange(min=1), default=4, show_default=True)\n\n# Misc settings.\n@click.option('--desc',         help='String to include in result dir name', metavar='STR',     type=str)\n@click.option('--metrics',      help='Quality metrics', metavar='[NAME|A,B,C|none]',            type=parse_comma_separated_list, default='fid50k_full', show_default=True)\n@click.option('--kimg',         help='Total training duration', metavar='KIMG',                 type=click.IntRange(min=1), default=25000, show_default=True)\n@click.option('--tick',         help='How often to print progress', metavar='KIMG',             type=click.IntRange(min=1), default=4, show_default=True)\n@click.option('--snap',         help='How often to save snapshots', metavar='TICKS',            type=click.IntRange(min=1), default=50, show_default=True)\n@click.option('--seed',         help='Random seed', metavar='INT',                              type=click.IntRange(min=0), default=0, show_default=True)\n@click.option('--fp32',         help='Disable mixed-precision', metavar='BOOL',                 type=bool, default=False, show_default=True)\n@click.option('--nobench',      help='Disable cuDNN benchmarking', metavar='BOOL',              type=bool, default=False, show_default=True)\n@click.option('--workers',      help='DataLoader worker processes', metavar='INT',              type=click.IntRange(min=1), default=3, show_default=True)\n@click.option('-n','--dry-run', help='Print training options and exit',                         is_flag=True)\n\ndef main(**kwargs):\n    \"\"\"Train a GAN using the techniques described in the paper\n    \"Alias-Free Generative Adversarial Networks\".\n\n    Examples:\n\n    \\b\n    # Train StyleGAN3-T for AFHQv2 using 8 GPUs.\n    python train.py --outdir=~/training-runs --cfg=stylegan3-t --data=~/datasets/afhqv2-512x512.zip \\\\\n        --gpus=8 --batch=32 --gamma=8.2 --mirror=1\n\n    \\b\n    # Fine-tune StyleGAN3-R for MetFaces-U using 1 GPU, starting from the pre-trained FFHQ-U pickle.\n    python train.py --outdir=~/training-runs --cfg=stylegan3-r --data=~/datasets/metfacesu-1024x1024.zip \\\\\n        --gpus=8 --batch=32 --gamma=6.6 --mirror=1 --kimg=5000 --snap=5 \\\\\n        --resume=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-1024x1024.pkl\n\n    \\b\n    # Train StyleGAN2 for FFHQ at 1024x1024 resolution using 8 GPUs.\n    python train.py --outdir=~/training-runs --cfg=stylegan2 --data=~/datasets/ffhq-1024x1024.zip \\\\\n        --gpus=8 --batch=32 --gamma=10 --mirror=1 --aug=noaug\n    \"\"\"\n\n    # Initialize config.\n    opts = dnnlib.EasyDict(kwargs) # Command line arguments.\n    c = dnnlib.EasyDict() # Main config dict.\n    c.G_kwargs = dnnlib.EasyDict(class_name=None, z_dim=512, w_dim=512, mapping_kwargs=dnnlib.EasyDict())\n    c.D_kwargs = dnnlib.EasyDict(class_name='training.networks_stylegan2.Discriminator', block_kwargs=dnnlib.EasyDict(), mapping_kwargs=dnnlib.EasyDict(), epilogue_kwargs=dnnlib.EasyDict())\n    c.G_opt_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', betas=[0,0.99], eps=1e-8)\n    c.D_opt_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', betas=[0,0.99], eps=1e-8)\n    c.loss_kwargs = dnnlib.EasyDict(class_name='training.loss.StyleGAN2Loss')\n    c.data_loader_kwargs = dnnlib.EasyDict(pin_memory=True, prefetch_factor=2)\n\n    # Training set.\n    c.training_set_kwargs, dataset_name = init_dataset_kwargs(data=opts.data)\n    if opts.cond and not c.training_set_kwargs.use_labels:\n        raise click.ClickException('--cond=True requires labels specified in dataset.json')\n    c.training_set_kwargs.use_labels = opts.cond\n    c.training_set_kwargs.xflip = opts.mirror\n\n    # Hyperparameters & settings.\n    c.num_gpus = opts.gpus\n    c.batch_size = opts.batch\n    c.batch_gpu = opts.batch_gpu or opts.batch // opts.gpus\n    c.G_kwargs.channel_base = c.D_kwargs.channel_base = opts.cbase\n    c.G_kwargs.channel_max = c.D_kwargs.channel_max = opts.cmax\n    c.G_kwargs.mapping_kwargs.num_layers = (8 if opts.cfg == 'stylegan2' else 2) if opts.map_depth is None else opts.map_depth\n    c.D_kwargs.block_kwargs.freeze_layers = opts.freezed\n    c.D_kwargs.epilogue_kwargs.mbstd_group_size = opts.mbstd_group\n    c.loss_kwargs.r1_gamma = opts.gamma\n    c.G_opt_kwargs.lr = (0.002 if opts.cfg == 'stylegan2' else 0.0025) if opts.glr is None else opts.glr\n    c.D_opt_kwargs.lr = opts.dlr\n    c.metrics = opts.metrics\n    c.total_kimg = opts.kimg\n    c.kimg_per_tick = opts.tick\n    c.image_snapshot_ticks = c.network_snapshot_ticks = opts.snap\n    c.random_seed = c.training_set_kwargs.random_seed = opts.seed\n    c.data_loader_kwargs.num_workers = opts.workers\n\n    # Sanity checks.\n    if c.batch_size % c.num_gpus != 0:\n        raise click.ClickException('--batch must be a multiple of --gpus')\n    if c.batch_size % (c.num_gpus * c.batch_gpu) != 0:\n        raise click.ClickException('--batch must be a multiple of --gpus times --batch-gpu')\n    if c.batch_gpu < c.D_kwargs.epilogue_kwargs.mbstd_group_size:\n        raise click.ClickException('--batch-gpu cannot be smaller than --mbstd')\n    if any(not metric_main.is_valid_metric(metric) for metric in c.metrics):\n        raise click.ClickException('\\n'.join(['--metrics can only contain the following values:'] + metric_main.list_valid_metrics()))\n\n    # Base configuration.\n    c.ema_kimg = c.batch_size * 10 / 32\n    if opts.cfg == 'stylegan2':\n        c.G_kwargs.class_name = 'training.networks_stylegan2.Generator'\n        c.loss_kwargs.style_mixing_prob = 0.9 # Enable style mixing regularization.\n        c.loss_kwargs.pl_weight = 2 # Enable path length regularization.\n        c.G_reg_interval = 4 # Enable lazy regularization for G.\n        c.G_kwargs.fused_modconv_default = 'inference_only' # Speed up training by using regular convolutions instead of grouped convolutions.\n        c.loss_kwargs.pl_no_weight_grad = True # Speed up path length regularization by skipping gradient computation wrt. conv2d weights.\n    else:\n        c.G_kwargs.class_name = 'training.networks_stylegan3.Generator'\n        c.G_kwargs.magnitude_ema_beta = 0.5 ** (c.batch_size / (20 * 1e3))\n        if opts.cfg == 'stylegan3-r':\n            c.G_kwargs.conv_kernel = 1 # Use 1x1 convolutions.\n            c.G_kwargs.channel_base *= 2 # Double the number of feature maps.\n            c.G_kwargs.channel_max *= 2\n            c.G_kwargs.use_radial_filters = True # Use radially symmetric downsampling filters.\n            c.loss_kwargs.blur_init_sigma = 10 # Blur the images seen by the discriminator.\n            c.loss_kwargs.blur_fade_kimg = c.batch_size * 200 / 32 # Fade out the blur during the first N kimg.\n\n    # Augmentation.\n    if opts.aug != 'noaug':\n        c.augment_kwargs = dnnlib.EasyDict(class_name='training.augment.AugmentPipe', xflip=1, rotate90=1, xint=1, scale=1, rotate=1, aniso=1, xfrac=1, brightness=1, contrast=1, lumaflip=1, hue=1, saturation=1)\n        if opts.aug == 'ada':\n            c.ada_target = opts.target\n        if opts.aug == 'fixed':\n            c.augment_p = opts.p\n\n    # Resume.\n    if opts.resume is not None:\n        c.resume_pkl = opts.resume\n        c.ada_kimg = 100 # Make ADA react faster at the beginning.\n        c.ema_rampup = None # Disable EMA rampup.\n        c.loss_kwargs.blur_init_sigma = 0 # Disable blur rampup.\n\n    # Performance-related toggles.\n    if opts.fp32:\n        c.G_kwargs.num_fp16_res = c.D_kwargs.num_fp16_res = 0\n        c.G_kwargs.conv_clamp = c.D_kwargs.conv_clamp = None\n    if opts.nobench:\n        c.cudnn_benchmark = False\n\n    # Description string.\n    desc = f'{opts.cfg:s}-{dataset_name:s}-gpus{c.num_gpus:d}-batch{c.batch_size:d}-gamma{c.loss_kwargs.r1_gamma:g}'\n    if opts.desc is not None:\n        desc += f'-{opts.desc}'\n\n    # Launch.\n    launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main() # pylint: disable=no-value-for-parameter\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "training",
          "type": "tree",
          "content": null
        },
        {
          "name": "visualizer.py",
          "type": "blob",
          "size": 14.419921875,
          "content": "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n#\n# NVIDIA CORPORATION and its licensors retain all intellectual property\n# and proprietary rights in and to this software, related documentation\n# and any modifications thereto.  Any use, reproduction, disclosure or\n# distribution of this software and related documentation without an express\n# license agreement from NVIDIA CORPORATION is strictly prohibited.\n\nimport click\nimport os\n\nimport multiprocessing\nimport numpy as np\nimport imgui\nimport dnnlib\nfrom gui_utils import imgui_window\nfrom gui_utils import imgui_utils\nfrom gui_utils import gl_utils\nfrom gui_utils import text_utils\nfrom viz import renderer\nfrom viz import pickle_widget\nfrom viz import latent_widget\nfrom viz import stylemix_widget\nfrom viz import trunc_noise_widget\nfrom viz import performance_widget\nfrom viz import capture_widget\nfrom viz import layer_widget\nfrom viz import equivariance_widget\n\n#----------------------------------------------------------------------------\n\nclass Visualizer(imgui_window.ImguiWindow):\n    def __init__(self, capture_dir=None):\n        super().__init__(title='GAN Visualizer', window_width=3840, window_height=2160)\n\n        # Internals.\n        self._last_error_print  = None\n        self._async_renderer    = AsyncRenderer()\n        self._defer_rendering   = 0\n        self._tex_img           = None\n        self._tex_obj           = None\n\n        # Widget interface.\n        self.args               = dnnlib.EasyDict()\n        self.result             = dnnlib.EasyDict()\n        self.pane_w             = 0\n        self.label_w            = 0\n        self.button_w           = 0\n\n        # Widgets.\n        self.pickle_widget      = pickle_widget.PickleWidget(self)\n        self.latent_widget      = latent_widget.LatentWidget(self)\n        self.stylemix_widget    = stylemix_widget.StyleMixingWidget(self)\n        self.trunc_noise_widget = trunc_noise_widget.TruncationNoiseWidget(self)\n        self.perf_widget        = performance_widget.PerformanceWidget(self)\n        self.capture_widget     = capture_widget.CaptureWidget(self)\n        self.layer_widget       = layer_widget.LayerWidget(self)\n        self.eq_widget          = equivariance_widget.EquivarianceWidget(self)\n\n        if capture_dir is not None:\n            self.capture_widget.path = capture_dir\n\n        # Initialize window.\n        self.set_position(0, 0)\n        self._adjust_font_size()\n        self.skip_frame() # Layout may change after first frame.\n\n    def close(self):\n        super().close()\n        if self._async_renderer is not None:\n            self._async_renderer.close()\n            self._async_renderer = None\n\n    def add_recent_pickle(self, pkl, ignore_errors=False):\n        self.pickle_widget.add_recent(pkl, ignore_errors=ignore_errors)\n\n    def load_pickle(self, pkl, ignore_errors=False):\n        self.pickle_widget.load(pkl, ignore_errors=ignore_errors)\n\n    def print_error(self, error):\n        error = str(error)\n        if error != self._last_error_print:\n            print('\\n' + error + '\\n')\n            self._last_error_print = error\n\n    def defer_rendering(self, num_frames=1):\n        self._defer_rendering = max(self._defer_rendering, num_frames)\n\n    def clear_result(self):\n        self._async_renderer.clear_result()\n\n    def set_async(self, is_async):\n        if is_async != self._async_renderer.is_async:\n            self._async_renderer.set_async(is_async)\n            self.clear_result()\n            if 'image' in self.result:\n                self.result.message = 'Switching rendering process...'\n                self.defer_rendering()\n\n    def _adjust_font_size(self):\n        old = self.font_size\n        self.set_font_size(min(self.content_width / 120, self.content_height / 60))\n        if self.font_size != old:\n            self.skip_frame() # Layout changed.\n\n    def draw_frame(self):\n        self.begin_frame()\n        self.args = dnnlib.EasyDict()\n        self.pane_w = self.font_size * 45\n        self.button_w = self.font_size * 5\n        self.label_w = round(self.font_size * 4.5)\n\n        # Detect mouse dragging in the result area.\n        dragging, dx, dy = imgui_utils.drag_hidden_window('##result_area', x=self.pane_w, y=0, width=self.content_width-self.pane_w, height=self.content_height)\n        if dragging:\n            self.latent_widget.drag(dx, dy)\n\n        # Begin control pane.\n        imgui.set_next_window_position(0, 0)\n        imgui.set_next_window_size(self.pane_w, self.content_height)\n        imgui.begin('##control_pane', closable=False, flags=(imgui.WINDOW_NO_TITLE_BAR | imgui.WINDOW_NO_RESIZE | imgui.WINDOW_NO_MOVE))\n\n        # Widgets.\n        expanded, _visible = imgui_utils.collapsing_header('Network & latent', default=True)\n        self.pickle_widget(expanded)\n        self.latent_widget(expanded)\n        self.stylemix_widget(expanded)\n        self.trunc_noise_widget(expanded)\n        expanded, _visible = imgui_utils.collapsing_header('Performance & capture', default=True)\n        self.perf_widget(expanded)\n        self.capture_widget(expanded)\n        expanded, _visible = imgui_utils.collapsing_header('Layers & channels', default=True)\n        self.layer_widget(expanded)\n        with imgui_utils.grayed_out(not self.result.get('has_input_transform', False)):\n            expanded, _visible = imgui_utils.collapsing_header('Equivariance', default=True)\n            self.eq_widget(expanded)\n\n        # Render.\n        if self.is_skipping_frames():\n            pass\n        elif self._defer_rendering > 0:\n            self._defer_rendering -= 1\n        elif self.args.pkl is not None:\n            self._async_renderer.set_args(**self.args)\n            result = self._async_renderer.get_result()\n            if result is not None:\n                self.result = result\n\n        # Display.\n        max_w = self.content_width - self.pane_w\n        max_h = self.content_height\n        pos = np.array([self.pane_w + max_w / 2, max_h / 2])\n        if 'image' in self.result:\n            if self._tex_img is not self.result.image:\n                self._tex_img = self.result.image\n                if self._tex_obj is None or not self._tex_obj.is_compatible(image=self._tex_img):\n                    self._tex_obj = gl_utils.Texture(image=self._tex_img, bilinear=False, mipmap=False)\n                else:\n                    self._tex_obj.update(self._tex_img)\n            zoom = min(max_w / self._tex_obj.width, max_h / self._tex_obj.height)\n            zoom = np.floor(zoom) if zoom >= 1 else zoom\n            self._tex_obj.draw(pos=pos, zoom=zoom, align=0.5, rint=True)\n        if 'error' in self.result:\n            self.print_error(self.result.error)\n            if 'message' not in self.result:\n                self.result.message = str(self.result.error)\n        if 'message' in self.result:\n            tex = text_utils.get_texture(self.result.message, size=self.font_size, max_width=max_w, max_height=max_h, outline=2)\n            tex.draw(pos=pos, align=0.5, rint=True, color=1)\n\n        # End frame.\n        self._adjust_font_size()\n        imgui.end()\n        self.end_frame()\n\n#----------------------------------------------------------------------------\n\nclass AsyncRenderer:\n    def __init__(self):\n        self._closed        = False\n        self._is_async      = False\n        self._cur_args      = None\n        self._cur_result    = None\n        self._cur_stamp     = 0\n        self._renderer_obj  = None\n        self._args_queue    = None\n        self._result_queue  = None\n        self._process       = None\n\n    def close(self):\n        self._closed = True\n        self._renderer_obj = None\n        if self._process is not None:\n            self._process.terminate()\n        self._process = None\n        self._args_queue = None\n        self._result_queue = None\n\n    @property\n    def is_async(self):\n        return self._is_async\n\n    def set_async(self, is_async):\n        self._is_async = is_async\n\n    def set_args(self, **args):\n        assert not self._closed\n        if args != self._cur_args:\n            if self._is_async:\n                self._set_args_async(**args)\n            else:\n                self._set_args_sync(**args)\n            self._cur_args = args\n\n    def _set_args_async(self, **args):\n        if self._process is None:\n            self._args_queue = multiprocessing.Queue()\n            self._result_queue = multiprocessing.Queue()\n            try:\n                multiprocessing.set_start_method('spawn')\n            except RuntimeError:\n                pass\n            self._process = multiprocessing.Process(target=self._process_fn, args=(self._args_queue, self._result_queue), daemon=True)\n            self._process.start()\n        self._args_queue.put([args, self._cur_stamp])\n\n    def _set_args_sync(self, **args):\n        if self._renderer_obj is None:\n            self._renderer_obj = renderer.Renderer()\n        self._cur_result = self._renderer_obj.render(**args)\n\n    def get_result(self):\n        assert not self._closed\n        if self._result_queue is not None:\n            while self._result_queue.qsize() > 0:\n                result, stamp = self._result_queue.get()\n                if stamp == self._cur_stamp:\n                    self._cur_result = result\n        return self._cur_result\n\n    def clear_result(self):\n        assert not self._closed\n        self._cur_args = None\n        self._cur_result = None\n        self._cur_stamp += 1\n\n    @staticmethod\n    def _process_fn(args_queue, result_queue):\n        renderer_obj = renderer.Renderer()\n        cur_args = None\n        cur_stamp = None\n        while True:\n            args, stamp = args_queue.get()\n            while args_queue.qsize() > 0:\n                args, stamp = args_queue.get()\n            if args != cur_args or stamp != cur_stamp:\n                result = renderer_obj.render(**args)\n                if 'error' in result:\n                    result.error = renderer.CapturedException(result.error)\n                result_queue.put([result, stamp])\n                cur_args = args\n                cur_stamp = stamp\n\n#----------------------------------------------------------------------------\n\n@click.command()\n@click.argument('pkls', metavar='PATH', nargs=-1)\n@click.option('--capture-dir', help='Where to save screenshot captures', metavar='PATH', default=None)\n@click.option('--browse-dir', help='Specify model path for the \\'Browse...\\' button', metavar='PATH')\ndef main(\n    pkls,\n    capture_dir,\n    browse_dir\n):\n    \"\"\"Interactive model visualizer.\n\n    Optional PATH argument can be used specify which .pkl file to load.\n    \"\"\"\n    viz = Visualizer(capture_dir=capture_dir)\n\n    if browse_dir is not None:\n        viz.pickle_widget.search_dirs = [browse_dir]\n\n    # List pickles.\n    if len(pkls) > 0:\n        for pkl in pkls:\n            viz.add_recent_pickle(pkl)\n        viz.load_pickle(pkls[0])\n    else:\n        pretrained = [\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-afhqv2-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-256x256.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-metfaces-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-metfacesu-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-afhqv2-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-ffhq-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-ffhqu-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-ffhqu-256x256.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-metfaces-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-metfacesu-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqcat-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqdog-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqv2-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-afhqwild-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-brecahad-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-celebahq-256x256.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-cifar10-32x32.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-256x256.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-512x512.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhqu-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhqu-256x256.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-lsundog-256x256.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-metfaces-1024x1024.pkl',\n            'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-metfacesu-1024x1024.pkl'\n        ]\n\n        # Populate recent pickles list with pretrained model URLs.\n        for url in pretrained:\n            viz.add_recent_pickle(url)\n\n    # Run.\n    while not viz.should_close():\n        viz.draw_frame()\n    viz.close()\n\n#----------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    main()\n\n#----------------------------------------------------------------------------\n"
        },
        {
          "name": "viz",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}