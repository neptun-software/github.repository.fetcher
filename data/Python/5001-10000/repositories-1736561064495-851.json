{
  "metadata": {
    "timestamp": 1736561064495,
    "page": 851,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "chatopera/Synonyms",
      "stars": 5040,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1572265625,
          "content": "*.swp\n*.swo\n*.sublime-*\n*.pyc\n__pycache__\ntmp/\nnode_modules/\nsftp-config.json\n.DS_Store\ndist/\nsynonyms.egg-info\n.vscode/\nbuild/\n.env\nsynonyms/data/words.vector*\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.8359375,
          "content": "language: python\ncache: pip\npython:\n    #- 2.7\n    - 3.6\n    #- nightly\n    #- pypy\n    #- pypy3\nmatrix:\n    allow_failures:\n        - python: nightly\n        - python: pypy\n        - python: pypy3\ninstall:\n    #- pip install -r requirements.txt\n    - pip install flake8  # pytest  # add another testing frameworks later\nbefore_script:\n    # stop the build if there are Python syntax errors or undefined names\n    - flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics\n    # exit-zero treats all errors as warnings.  The GitHub editor is 127 chars wide\n    - flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\nscript:\n    - true  # pytest --capture=sys  # add other tests here\nnotifications:\n    on_success: change\n    on_failure: change  # `always` will be the setting once code changes slow down\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 1.7548828125,
          "content": "# 3.23\n\n- Use chatoperastore to download model file\n\n# 3.16\n\n- Use github vector pkg download link\n\n# 3.15\n\n- Fix jieba exports 冲突，改为只暴露 keywords, seg 接口\n- 修正 vocab.txt 里的错误\n\n# 3.13\n\n- 减少依赖\n- export jieba as synonyms.jieba\n\n# 3.12\n\n- 使用更大词向量，42W+ 词汇表\n- 优化下载速度\n\n# 3.11\n\n- 支持定义查询词汇数量，默认 10 个词\n\n# 3.10\n\n- 计算编辑距离时去停用词\n\n# 3.9\n\n- fix bug\n\n# 3.8\n\n- 获得一个分词后句子的向量，向量以 BoW 方式组成\n\n```\n    sentence: 句子是分词后通过空格联合起来\n    ignore: 是否忽略OOV，False时，随机生成一个向量\n```\n\n# 3.7\n\n- change import path of utils in word2vec.py to local path\n- expose vector fn\n\n# 3.6\n\n- Fix Bug: compare 保证交换两个句子后分数一致 [#60](https://github.com/huyingxi/Synonyms/issues/60)\n\n# 3.5\n\n- 根据实际情况，降低向量距离对近似度分数的影响\n\n# 3.3\n\n- 增加分词接口\n- 优化分词器初始化加载字典\n- 使用 jieba 分词源码\n- 使用 glog 作为日志输出模块\n\n# 3.2\n\n- 将发布证书改为 MIT\n\n# 3.1\n\n- 对空间临近词的邻居进行缓存，提高返回速度\n- nearby 中处理 OOV，返回 ([], [])\n\n# 3.0 - 更简单的定制和配置，增加了额外的开销\n\n- 去掉 nearby words, 使用 kdtree 检索空间词汇的最近临\n- 增加了对 sk-learn 的依赖，但是减少了对词向量的预处理\n- 优化了分词所使用的字典，也可以使用环境变量声明主字典\n- 支持自定义 word2vec 模型，使用环境变量声明\n\n# 2.5\n\n- 使用空间距离近的词汇优化编辑距离计算\n\n# 2.3\n\n- 计算相似度时增加平滑策略\n\n# v1.6\n\n- use `jieba` instead of `thulac` as tokeninzer.\n- refine console log for Jupyter notebook.\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.142578125,
          "content": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment include:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at hain_wang@foxmail.com. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at [http://contributor-covenant.org/version/1/4][version]\n\n[homepage]: http://contributor-covenant.org\n[version]: http://contributor-covenant.org/version/1/4/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 0.6025390625,
          "content": "Copyright 2023 Beijing Huaxia Chunsong Technology Co., Ltd. <https://www.chatopera.com> \n\nLicensed under the Chunsong Public License, Version 1.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n https://docs.cskefu.com/licenses/v1.html\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.775390625,
          "content": "[![PyPI](https://img.shields.io/pypi/v/synonyms.svg)](https://pypi.python.org/pypi/synonyms) [![PyPI download month](https://img.shields.io/pypi/dm/synonyms.svg)](https://pypi.python.org/pypi/synonyms/) [![](https://img.shields.io/pypi/pyversions/synonyms.svg)](https://pypi.org/pypi/synonyms/) [![License](https://cdndownload2.chatopera.com/cskefu/licenses/chunsong1.0.svg)](https://www.cskefu.com/licenses/v1.html \"开源许可协议\") [![](https://img.shields.io/pypi/format/synonyms.svg)](https://pypi.org/pypi/synonyms/)\n\n# Synonyms\n\nChinese Synonyms for Natural Language Processing and Understanding.\n\n更好的中文近义词：聊天机器人、智能问答工具包。\n\n`synonyms`可以用于自然语言理解的很多任务：文本对齐，推荐算法，相似度计算，语义偏移，关键字提取，概念提取，自动摘要，搜索引擎等。\n\n为提供稳定、可靠、长期优化的服务，Synonyms 改为使用 [春松许可证, v1.0](https://www.cskefu.com/licenses/v1.html) 并针对机器学习模型的下载进行收费，详见[证书商店](https://store.chatopera.com/product/syns001)。之前的贡献者（突出贡献的代码贡献者），可与我们联系，讨论收费问题。-- [Chatopera Inc.](https://www.chatopera.com) @ Oct. 2023\n\n# Table of Content:\n\n- [Install](https://github.com/chatopera/Synonyms#welcome)\n- [Usage](https://github.com/chatopera/Synonyms#usage)\n- [Quick Get Start](https://github.com/chatopera/Synonyms#quick-get-start)\n- [Valuation](https://github.com/chatopera/Synonyms#valuation)\n- [Benchmark](https://github.com/chatopera/Synonyms#benchmark)\n- [Statement](https://github.com/chatopera/Synonyms#statement)\n- [References](https://github.com/chatopera/Synonyms#references)\n- [Frequently Asked Questions](https://github.com/chatopera/Synonyms#frequently-asked-questions-faq)\n- [License](https://github.com/chatopera/Synonyms#license)\n\n# Welcome\n\nFollow steps below to install and activate packages.\n\n## 1/3 Install Sourcecodes Package\n\n```bash\npip install -U synonyms\n```\n\n当前稳定版本 v3.x。\n\n## 2/3 Config license id\n\nSynonyms's machine learning model package(s) requires a License from [Chatopera License Store](https://store.chatopera.com/product/syns001), first purchase a License and get the `license id` from **Licenses** page on Chatopera License Store(`license id`：在证书商店，证书详情页，点击【复制证书标识】).\n\n![image](https://cdndownload2.chatopera.com/store/imgs/syn_order_post.jpg)\n\nSecondly, set environment variable in your terminal or shell scripts as below.\n\n* For Shell Users\n\ne.g. Shell, CMD Scripts on Linux, Windows, macOS.\n\n```bash\n# Linux / macOS\nexport SYNONYMS_DL_LICENSE=YOUR_LICENSE\n## e.g. if your license id is `FOOBAR`, run `export SYNONYMS_DL_LICENSE=FOOBAR`\n\n# Windows\n## 1/2 Command Prompt\nset SYNONYMS_DL_LICENSE=YOUR_LICENSE\n## 2/2 PowerShell\n$env:SYNONYMS_DL_LICENSE='YOUR_LICENSE'\n```\n\n* For Python Code Users\n\nJupyter Notebook, etc.\n\n```python\nimport os\nos.environ[\"SYNONYMS_DL_LICENSE\"] = \"YOUR_LICENSE\"\n_licenseid = os.environ.get(\"SYNONYMS_DL_LICENSE\", None)\nprint(\"SYNONYMS_DL_LICENSE=\", _licenseid)\n```\n\n![](./assets/screenshot_20231124180125.png)\n\n**提示：安装后初次使用会下载词向量文件，下载速度取决于网络情况。**\n\n## 3/3 Download Model Package\n\nLast, download the model package by command or script -\n\n```bash\npython -c \"import synonyms; synonyms.display('能量')\" # download word vectors file\n```\n\n![](./assets/3.gif)\n\n## Usage\n\n支持使用环境变量配置分词词表和 word2vec 词向量文件。\n\n| 环境变量                            | 描述                                                                                                                                                                                               |\n| ----------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| _SYNONYMS_WORD2VEC_BIN_MODEL_ZH_CN_ | 使用 word2vec 训练的词向量文件，二进制格式。                                                                                                                                                       |\n| _SYNONYMS_WORDSEG_DICT_             | 中文分词[**主字典**](https://github.com/fxsjy/jieba#%E5%BB%B6%E8%BF%9F%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6)，格式和使用[参考](https://github.com/fxsjy/jieba#%E8%BD%BD%E5%85%A5%E8%AF%8D%E5%85%B8) |\n| _SYNONYMS_DEBUG_                    | [\"TRUE\"\\|\"FALSE\"], 是否输出调试日志，设置为 “TRUE” 输出，默认为 “FALSE”                                                                                                                            |\n\n### synonyms#nearby(word [, size = 10])\n\n```python\nimport synonyms\nprint(\"人脸: \", synonyms.nearby(\"人脸\"))\nprint(\"识别: \", synonyms.nearby(\"识别\"))\nprint(\"NOT_EXIST: \", synonyms.nearby(\"NOT_EXIST\"))\n```\n\n`synonyms.nearby(WORD [,SIZE])`返回一个元组，元组中包含两项：`([nearby_words], [nearby_words_score])`，`nearby_words`是 WORD 的近义词们，也以 list 的方式存储，并且按照距离的长度由近及远排列，`nearby_words_score`是`nearby_words`中**对应位置**的词的距离的分数，分数在(0-1)区间内，越接近于 1，代表越相近；`SIZE` 是返回词汇数量，默认 10。比如:\n\n```python\nsynonyms.nearby(人脸, 10) = (\n    [\"图片\", \"图像\", \"通过观察\", \"数字图像\", \"几何图形\", \"脸部\", \"图象\", \"放大镜\", \"面孔\", \"Mii\"],\n    [0.597284, 0.580373, 0.568486, 0.535674, 0.531835, 0.530\n095, 0.525344, 0.524009, 0.523101, 0.516046])\n```\n\n在 OOV 的情况下，返回 `([], [])`，目前的字典大小: 435,729。\n\n### synonyms#compare(sen1, sen2 [, seg=True])\n\n两个句子的相似度比较\n\n```python\n    sen1 = \"发生历史性变革\"\n    sen2 = \"发生历史性变革\"\n    r = synonyms.compare(sen1, sen2, seg=True)\n```\n\n其中，参数 seg 表示 synonyms.compare 是否对 sen1 和 sen2 进行分词，默认为 True。返回值：[0-1]，并且越接近于 1 代表两个句子越相似。\n\n```python\n旗帜引领方向 vs 道路决定命运: 0.429\n旗帜引领方向 vs 旗帜指引道路: 0.93\n发生历史性变革 vs 发生历史性变革: 1.0\n```\n\n### synonyms#display(word [, size = 10])\n\n以友好的方式打印近义词，方便调试，`display(WORD [, SIZE])`调用了 `synonyms#nearby` 方法。\n\n```python\n>>> synonyms.display(\"飞机\")\n'飞机'近义词：\n  1. 飞机:1.0\n  2. 直升机:0.8423391\n  3. 客机:0.8393003\n  4. 滑翔机:0.7872388\n  5. 军用飞机:0.7832081\n  6. 水上飞机:0.77857226\n  7. 运输机:0.7724742\n  8. 航机:0.7664748\n  9. 航空器:0.76592904\n  10. 民航机:0.74209654\n```\n\n`SIZE` 是打印词汇表的数量，默认 10。\n\n### synonyms#describe()\n\n打印当前包的描述信息：\n\n```\n>>> synonyms.describe()\nVocab size in vector model: 435729\nmodel_path: /Users/hain/chatopera/Synonyms/synonyms/data/words.vector.gz\nversion: 3.18.0\n{'vocab_size': 435729, 'version': '3.18.0', 'model_path': '/chatopera/Synonyms/synonyms/data/words.vector.gz'}\n```\n\n### synonyms#v(word)\n\n获得一个词语的向量，该向量为 numpy 的 array，当该词语是未登录词时，抛出 KeyError 异常。\n\n```python\n>>> synonyms.v(\"飞机\")\narray([-2.412167  ,  2.2628384 , -7.0214124 ,  3.9381874 ,  0.8219283 ,\n       -3.2809453 ,  3.8747153 , -5.217062  , -2.2786229 , -1.2572327 ],\n      dtype=float32)\n```\n\n### synonyms#sv(sentence, ignore=False)\n\n获得一个分词后句子的向量，向量以 BoW 方式组成\n\n```python\n    sentence: 句子是分词后通过空格联合起来\n    ignore: 是否忽略OOV，False时，随机生成一个向量\n```\n\n### synonyms#seg(sentence)\n\n中文分词\n\n```python\nsynonyms.seg(\"中文近义词工具包\")\n```\n\n分词结果，由两个 list 组成的元组，分别是单词和对应的词性。\n\n```python\n(['中文', '近义词', '工具包'], ['nz', 'n', 'n'])\n```\n\n**该分词不去停用词和标点。**\n\n### synonyms#keywords(sentence [, topK=5, withWeight=False])\n\n提取关键词，默认按照重要程度提取关键词。\n\n```\nkeywords = synonyms.keywords(\"9月15日以来，台积电、高通、三星等华为的重要合作伙伴，只要没有美国的相关许可证，都无法供应芯片给华为，而中芯国际等国产芯片企业，也因采用美国技术，而无法供货给华为。目前华为部分型号的手机产品出现货少的现象，若该形势持续下去，华为手机业务将遭受重创。\")\n```\n\n## Contribution\n\nGet more logs for debugging, set environment variable.\n\n```\nSYNONYMS_DEBUG=TRUE\n```\n\n## PCA\n\n以“人脸”为例主要成分分析：\n\n![](assets/1.png)\n\n## Quick Get Start\n\n```bash\n$ pip install -r Requirements.txt\n$ python demo.py\n```\n\n## Change logs\n\n更新情况[说明](./CHANGELOG.md)。\n\n## Voice of Users\n\n用户怎么说：\n\n<img src=\"https://github.com/chatopera/Synonyms/raw/master/assets/4.png\" width=\"600\">\n\n## Data\n\ndata is built based on [wikidata-corpus](https://github.com/Samurais/wikidata-corpus).\n\n## Valuation\n\n### 同义词词林\n\n《同义词词林》是梅家驹等人于 1983 年编纂而成，现在使用广泛的是哈工大社会计算与信息检索研究中心维护的《同义词词林扩展版》，它精细的将中文词汇划分成大类和小类，梳理了词汇间的关系，同义词词林扩展版包含词语 7 万余条，其中 3 万余条被以开放数据形式共享。\n\n### 知网, HowNet\n\nHowNet，也被称为知网，它并不只是一个语义字典，而是一个知识系统，词汇之间的关系是其一个基本使用场景。知网包含词语 8 余条。\n\n国际上对词语相似度算法的评价标准普遍采用 Miller&Charles 发布的英语词对集的人工判定值。该词对集由十对高度相关、十对中度相关、十对低度相关共 30 个英语词对组成,然后让 38 个受试者对这 30 对进行语义相关度判断，最后取他们的平均值作为人工判定标准。然后不同近义词工具也对这些词汇进行相似度评分，与人工判定标准做比较，比如使用皮尔森相关系数。在中文领域，使用这个词表的翻译版进行中文近义词比较也是常用的办法。\n\n### 对比\n\nSynonyms 的词表容量是 435,729，下面选择一些在同义词词林、知网和 Synonyms 都存在的几个词，给出其近似度的对比：\n\n![](./assets/5.png)\n\n注：同义词林及知网数据、分数[来源](https://github.com/yaleimeng/Final_word_Similarity)。Synonyms 也在不断优化中，新的分数可能和上图不一致。\n\n更多[比对结果](./VALUATION.md)。\n\n## Used by\n\n[Github 关联用户列表](https://github.com/chatopera/Synonyms/network/dependents?package_id=UGFja2FnZS01MjY2NDc1Nw%3D%3D)\n\n![](./assets/6.png)\n\n## Benchmark\n\nTest with py3, MacBook Pro.\n\n```\npython benchmark.py\n```\n\n++++++++++ OS Name and version ++++++++++\n\nPlatform: Darwin\n\nKernel: 16.7.0\n\nArchitecture: ('64bit', '')\n\n++++++++++ CPU Cores ++++++++++\n\nCores: 4\n\nCPU Load: 60\n\n++++++++++ System Memory ++++++++++\n\nmeminfo 8GB\n\n`synonyms#nearby: 100000 loops, best of 3 epochs: 0.209 usec per loop`\n\n## Live Sharing\n\n[52nlp.cn](http://www.52nlp.cn/synonyms-%E4%B8%AD%E6%96%87%E8%BF%91%E4%B9%89%E8%AF%8D%E5%B7%A5%E5%85%B7%E5%8C%85)\n\n[机器之心](https://www.jiqizhixin.com/articles/2018-01-14-3)\n\n[线上分享实录: Synonyms 中文近义词工具包 @ 2018-02-07](http://gitbook.cn/gitchat/activity/5a563545a8b23d387720ccd5)\n\n## Statement\n\n[Synonyms](https://github.com/chatopera/Synonyms)发布证书 MIT。数据和程序可用于研究和商业产品，必须注明引用和地址，比如发布的任何媒体、期刊、杂志或博客等内容。\n\n```\n@online{Synonyms:hain2017,\n  author = {Hai Liang Wang, Hu Ying Xi},\n  title = {中文近义词工具包Synonyms},\n  year = 2017,\n  url = {https://github.com/chatopera/Synonyms},\n  urldate = {2017-09-27}\n}\n```\n\n# References\n\n[wikidata-corpus](https://github.com/Samurais/wikidata-corpus)\n\n[word2vec 原理推导与代码分析](http://www.hankcs.com/nlp/word2vec.html)\n\n# Frequently Asked Questions (FAQ)\n\n1. 是否支持添加单词到词表中？\n\n不支持，欲了解更多请看 [#5](https://github.com/chatopera/Synonyms/issues/5)\n\n2. 词向量的训练是用哪个工具？\n\nGoogle 发布的[word2vec](https://code.google.com/archive/p/word2vec/)，该库由 C 语言编写，内存使用效率高，训练速度快。gensim 可以加载 word2vec 输出的模型文件。\n\n3. 相似度计算的方法是什么？\n\n[详见 #64](https://github.com/chatopera/Synonyms/issues/64)\n\n4. [#118 词向量文件一直下载不下来？](https://github.com/chatopera/Synonyms/issues/118)\n\n# Authors\n\n[Hai Liang Wang](https://pre-angel.com/peoples/hailiang-wang/)\n\n[Hu Ying Xi](https://github.com/huyingxi)\n\n# 自然语言处理推荐入门&工具书\n\n本书由 [Synonyms](https://github.com/chatopera/Synonyms) 作者参与著作。\n\n<p align=\"center\">\n  <b>快速购书<a href=\"https://item.jd.com/12479014.html\" target=\"_blank\">链接</a></b><br>\n  <a href=\"https://item.jd.com/12479014.html\" target=\"_blank\">\n  <img src=\"https://user-images.githubusercontent.com/3538629/48657619-bcd24880-ea6e-11e8-8c4e-8bcb00761942.png\" width=\"400\">      \n  </a>\n</p>\n\n[《智能问答与深度学习》](https://item.jd.com/12479014.html) 这本书是服务于准备入门机器学习和自然语言处理的学生和软件工程师的，在理论上介绍了很多原理、算法，同时也提供很多示例程序增加实践性，这些程序被汇总到示例程序代码库，这些程序主要是帮助大家理解原理和算法的，欢迎大家下载和执行。代码库的地址是：\n\n[https://github.com/l11x0m7/book-of-qna-code](https://github.com/l11x0m7/book-of-qna-code)\n\n# Give credits to\n\n[Word2vec by Google](https://code.google.com/archive/p/word2vec/)\n\n[Wikimedia: 训练语料来源](https://dumps.wikimedia.org/)\n\n[gensim: word2vec.py](https://github.com/RaRe-Technologies/gensim)\n\n[SentenceSim: 相似度评测语料](https://github.com/fssqawj/SentenceSim/)\n\n[jieba: 中文分词](https://github.com/fxsjy/jieba)\n\n# License\n\n[Chunsong Public License, version 1.0](./LICENSE)\n\n# Project Sponsor\n\n## Chatopera 云服务\n\n[https://bot.chatopera.com/](https://bot.chatopera.com/)\n\n[Chatopera 云服务](https://bot.chatopera.com)是一站式实现聊天机器人的云服务，按接口调用次数计费。Chatopera 云服务是 [Chatopera 机器人平台](https://docs.chatopera.com/products/chatbot-platform/index.html)的软件即服务实例。在云计算基础上，Chatopera 云服务属于**聊天机器人即服务**的云服务。\n\nChatopera 机器人平台包括知识库、多轮对话、意图识别和语音识别等组件，标准化聊天机器人开发，支持企业 OA 智能问答、HR 智能问答、智能客服和网络营销等场景。企业 IT 部门、业务部门借助 Chatopera 云服务快速让聊天机器人上线！"
        },
        {
          "name": "Requirements.txt",
          "type": "blob",
          "size": 0.013671875,
          "content": "synonyms>=3.23"
        },
        {
          "name": "VALUATION.md",
          "type": "blob",
          "size": 1.140625,
          "content": "# synonyms 分数评测 [(v3.23.0)](https://pypi.python.org/pypi/synonyms/3.23.0)\n| 词1 |  词2 |   synonyms  |  人工评定 |\n| --- | --- | --- | --- |\n| 轿车 | 汽车 | 0.892  |  0.98 |\n| 宝石 | 宝物 | 1.0  |  0.96 |\n| 旅游 | 游历 | 0.649  |  0.96 |\n| 男孩子 | 小伙子 | 0.77  |  0.94 |\n| 海岸 | 海滨 | 0.889  |  0.925 |\n| 庇护所 | 精神病院 | 0.211  |  0.9025 |\n| 魔术师 | 巫师 | 0.95  |  0.875 |\n| 中午 | 正午 | 0.9  |  0.855 |\n| 火炉 | 炉灶 | 0.889  |  0.7775 |\n| 食物 | 水果 | 0.363  |  0.77 |\n| 鸟 | 公鸡 | 0.895  |  0.7625 |\n| 鸟 | 鹤 | 1.0  |  0.7425 |\n| 工具 | 器械 | 0.881  |  0.7375 |\n| 兄弟 | 和尚 | 0.139  |  0.705 |\n| 起重机 | 器械 | 0.195  |  0.42 |\n| 小伙子 | 兄弟 | 0.703  |  0.415 |\n| 旅行 | 轿车 | 0.088  |  0.29 |\n| 和尚 | 圣贤 | 0.222  |  0.275 |\n| 墓地 | 林地 | 0.874  |  0.2375 |\n| 食物 | 公鸡 | 0.151  |  0.2225 |\n| 海岸 | 丘陵 | 0.248  |  0.2175 |\n| 森林 | 墓地 | 0.14  |  0.21 |\n| 岸边 | 林地 | 0.193  |  0.1575 |\n| 和尚 | 奴隶 | 0.059  |  0.1375 |\n| 海岸 | 森林 | 0.23  |  0.105 |\n| 小伙子 | 巫师 | 0.182  |  0.105 |\n| 琴弦 | 微笑 | 0.089  |  0.0325 |\n| 玻璃 | 魔术师 | 0.02  |  0.0275 |\n| 中午 | 绳子 | 0.049  |  0.02 |\n| 公鸡 | 航行 | 0.0  |  0.02 |\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "benchmark.py",
          "type": "blob",
          "size": 2.1611328125,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#=========================================================================\n#\n# Copyright (c) 2017 <> All Rights Reserved\n#\n#\n# File: /Users/hain/ai/Synonyms/benchmark.py\n# Author: Hai Liang Wang\n# Date: 2017-10-21:11:26:53\n#\n#=========================================================================\n\n\"\"\"\n\n\"\"\"\nfrom __future__ import print_function\nfrom __future__ import division\n\n__copyright__ = \"Copyright (c) 2017-2023 Chatopera Inc. All Rights Reserved\"\n__author__ = \"Hai Liang Wang\"\n__date__ = \"2017-10-21:11:26:53\"\n\n\nimport os\nimport sys\nimport platform\nimport multiprocessing\ncurdir = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(curdir)\n\nif sys.version_info[0] < 3:\n    reload(sys)\n    sys.setdefaultencoding(\"utf-8\")\n    # raise \"Must be using Python 3\"\n\nimport timeit\n\nprint(\"\\nEnumerating Available System Resources...\")\n\nprint(\"\\n++++++++++ OS Name and version ++++++++++\")\n\nprint(\"Platform:\", platform.system())\nprint(\"Kernel:\", platform.release())\nprint(\"Distro:\", platform.linux_distribution())\nprint(\"Architecture:\", platform.architecture())\n\nprint(\"\\n++++++++++ CPU Cores ++++++++++\")\np = os.popen(\"ps aux|awk 'NR > 0{s +=$3};END{print s}'\").read()\nprint(\"Cores:\", multiprocessing.cpu_count(), '\\nCPU Load:', p)\n\nprint(\"\\n++++++++++ System Memory ++++++++++\\n\")\n\n\ndef meminfo():\n    meminfo = dict()\n\n    with os.popen('cat /proc/meminfo') as f:\n        for line in f:\n            meminfo[line.split(':')[0]] = line.split(':')[1].strip()\n    return meminfo\n\n\ntry:\n    meminfo = meminfo()\n    print('Total Memory: {0}'.format(meminfo['MemTotal']))\n    print('Free Memory: {0}'.format(meminfo['MemFree']))\nexcept BaseException:\n    print(\"meminfo unavailable\")\n\n\ndef main():\n    repeat = 3\n    number = 100000\n    unit = \"usec\"  # 微秒\n    unittosec = {\"usec\": 1e6, \"msec\": 1000, \"sec\": 1}\n    result = timeit.repeat(\n        \"synonyms.nearby('人脸')\",\n        \"import synonyms\",\n        number=number,\n        repeat=repeat)\n    print(\"%s: %d loops, best of %d epochs: %.3g %s per loop\" %\n          (\"synonyms#nearby\", number, repeat,\n           min(result) / number * unittosec[unit], unit))\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "demo.py",
          "type": "blob",
          "size": 5.8564453125,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#=========================================================================\n#\n# Copyright (c) 2017 <> All Rights Reserved\n#\n#\n# File: /Users/hain/ai/Synonyms/demo.py\n# Author: Hai Liang Wang\n# Date: 2017-09-28:22:23:34\n#\n#=========================================================================\n\n\"\"\"\n\n\"\"\"\nfrom __future__ import print_function\nfrom __future__ import division\n\n__copyright__ = \"Copyright (c) (2017-2022) Chatopera Inc. All Rights Reserved\"\n__author__ = \"Hai Liang Wang\"\n__date__ = \"2017-09-28:22:23:34\"\n\n\nimport os\nimport sys\ncurdir = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, curdir)\n\nif sys.version_info[0] < 3:\n    reload(sys)\n    sys.setdefaultencoding(\"utf-8\")\n    # raise \"Must be using Python 3\"\n    # \n\nimport synonyms  # https://github.com/chatopera/Synonyms\nimport numpy\nimport unittest\n\ncompare_ = lambda x,y,z: \"%s vs %s: %f\" % (x, y, synonyms.compare(x, y, seg=z)) + \"\\n\" +\"*\"* 30 + \"\\n\"\n\n# run testcase: python /Users/hain/ai/Synonyms/demo.py Test.testExample\nclass Test(unittest.TestCase):\n    '''\n\n    '''\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_wordseg(self):\n        print(\"test_wordseg\")\n        print(synonyms.seg(\"中文近义词工具包\"))\n\n\n    def test_word_vector(self):\n        print(\"test_word_vector\")\n        word = \"三国\"\n        print(word, \"向量\", synonyms.v(word))\n\n    def test_diff(self):\n        print(\"test_diff\")\n        result = []\n        # 30个  评测词对中的左侧词\n        left = ['轿车', '宝石', '旅游', '男孩子', '海岸', '庇护所', '魔术师', '中午', '火炉', '食物', '鸟', '鸟', '工具', '兄弟', '起重机', '小伙子',\n                '旅行', '和尚', '墓地', '食物', '海岸', '森林', '岸边', '和尚', '海岸', '小伙子', '琴弦', '玻璃', '中午', '公鸡']\n        # 30个  评测词对中的右侧词\n        right = ['汽车', '宝物', '游历', '小伙子', '海滨', '精神病院', '巫师', '正午', '炉灶', '水果', '公鸡', '鹤', '器械', '和尚', '器械', '兄弟',\n                 '轿车', '圣贤', '林地', '公鸡', '丘陵', '墓地', '林地', '奴隶', '森林', '巫师', '微笑', '魔术师', '绳子', '航行']\n        # 人工评定的相似度列表。\n        human = [0.98, 0.96, 0.96, 0.94, 0.925, 0.9025, 0.875, 0.855, 0.7775, 0.77, 0.7625, 0.7425, 0.7375, 0.705, 0.42, 0.415,\n                 0.29, 0.275, 0.2375,\n                 0.2225, 0.2175, 0.21, 0.1575, 0.1375, 0.105, 0.105, 0.0325, 0.0275, 0.02, 0.02]\n        result.append(\"# synonyms 分数评测 [(v%s)](https://pypi.python.org/pypi/synonyms/%s)\" % (synonyms.__version__, synonyms.__version__))\n        result.append(\"| %s |  %s |   %s  |  %s |\" % (\"词1\", \"词2\", \"synonyms\", \"人工评定\"))\n        result.append(\"| --- | --- | --- | --- |\")\n        for x,y,z in zip(left, right, human):\n            result.append(\"| %s | %s | %s  |  %s |\" % (x, y, synonyms.compare(x, y), z))\n        for x in result: print(x)\n        with open(os.path.join(curdir, \"VALUATION.md\"), \"w\") as fout:\n            for x in result: fout.write(x + \"\\n\")\n\n    def test_similarity(self):\n        '''\n        Generate sentence similarity\n        '''\n        sen1 = \"旗帜引领方向\"\n        sen2 = \"道路决定命运\"\n        r = synonyms.compare(sen1, sen2, seg=True)\n        print(\"旗帜引领方向 vs 道路决定命运:\", r)\n        # assert r == 0.0, \"the similarity should be zero\"\n\n        sen1 = \"旗帜引领方向\"\n        sen2 = \"旗帜指引道路\"\n        r = synonyms.compare(sen1, sen2, seg=True)\n        print(\"旗帜引领方向 vs 旗帜指引道路:\", r)\n        # assert r > 0, \"the similarity should be bigger then zero\"\n\n        sen1 = \"发生历史性变革\"\n        sen2 = \"发生历史性变革\"\n        r = synonyms.compare(sen1, sen2, seg=True)\n        print(\"发生历史性变革 vs 发生历史性变革:\", r)\n        # assert r > 0, \"the similarity should be bigger then zero\"\n\n        sen1 = \"骨折\"\n        sen2 = \"巴赫\"\n        r = synonyms.compare(sen1, sen2, seg=True)\n        print(\"%s vs %s\" % (sen1, sen2), r)\n\n\n        sen1 = \"你们好呀\"\n        sen2 = \"大家好\"\n        r = synonyms.compare(sen1, sen2, seg=False)\n        print(\"%s vs %s\" % (sen1, sen2), r)\n\n\n    def test_swap_sent(self):\n        print(\"test_swap_sent\")        \n        s1 = synonyms.compare(\"教学\", \"老师\")\n        s2 = synonyms.compare(\"老师\", \"教学\")\n        print('\"教学\", \"老师\": %s ' % s1)\n        print('\"老师\", \"教学\": %s ' % s2)\n        assert s1 == s2, \"Scores should be the same after swap sents\"\n\n    def test_nearby(self):\n        synonyms.display(\"奥运\")  # synonyms.display calls synonyms.nearby\n        synonyms.display(\"北新桥\")  # synonyms.display calls synonyms.nearby\n\n\n    def test_badcase_1(self):\n        synonyms.display(\"人脸\")  # synonyms.display calls synonyms.nearby\n\n\n    def test_basecase_2(self):\n        print(\"test_basecase_2\")\n        sen1 = \"今天天气\"\n        sen2 = \"今天天气怎么样\"\n        r = synonyms.compare(sen1, sen2, seg=True)\n\n\n    def test_analyse_extract_tags(self):\n        '''\n        使用 Tag 方式获得关键词\n        https://github.com/fxsjy/jieba/tree/v0.39\n        '''\n        sentence = \"华为芯片被断供，源于美国关于华为的修订版禁令生效——9月15日以来，台积电、高通、三星等华为的重要合作伙伴，只要没有美国的相关许可证，都无法供应芯片给华为，而中芯国际等国产芯片企业，也因采用美国技术，而无法供货给华为。目前华为部分型号的手机产品出现货少的现象，若该形势持续下去，华为手机业务将遭受重创。\"\n        keywords = synonyms.keywords(sentence, topK=5, withWeight=False, allowPOS=())\n        print(\"[test_analyse_extract_tags] keywords %s\" % keywords)\n\ndef test():\n    unittest.main()\n\n\nif __name__ == '__main__':\n    test()\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.0380859375,
          "content": "[metadata]\ndescription-file = README.md"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.5888671875,
          "content": "# -*- coding: utf-8 -*-\nfrom setuptools import setup, find_packages\nLONGDOC = \"\"\"\nSynonyms\n=====================\n\n中文近义词\n\nhttps://github.com/chatopera/Synonyms\n\n\"\"\"\n\nsetup(\n    name='synonyms',\n    version='3.23.5',\n    description='中文近义词：聊天机器人，智能问答工具包；Chinese Synonyms for Natural Language Processing and Understanding',\n    long_description=LONGDOC,\n    author='Hai Liang Wang, Hu Ying Xi',\n    author_email='hain@chatopera.com',\n    url='https://github.com/chatopera/Synonyms',\n    license=\"Chunsong Public License, version 1.0\",\n    classifiers=[\n        'Intended Audience :: Developers',\n        'Operating System :: OS Independent',\n        'Natural Language :: Chinese (Simplified)',\n        'Natural Language :: Chinese (Traditional)',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Topic :: Text Processing',\n        'Topic :: Text Processing :: Indexing',\n        'Topic :: Text Processing :: Linguistic'],\n    keywords='corpus,machine-learning,NLU,NLP,Synonyms,Similarity,chatbot',\n    packages=find_packages(),\n    install_requires=[\n        'six>=1.11.0',\n        'numpy>=1.13.1',\n        'scipy>=1.0.0',\n        'scikit-learn>=0.19.1',\n        'jieba>=0.40',\n        'chatoperastore>=1.2.0'\n    ],\n    package_data={\n        'synonyms': [\n            '**/**/idf.txt',\n            '**/**/*.p',\n            '**/*.gz',\n            '**/*.txt',\n            'LICENSE']})\n"
        },
        {
          "name": "synonyms",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}