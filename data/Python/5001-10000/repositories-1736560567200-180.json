{
  "metadata": {
    "timestamp": 1736560567200,
    "page": 180,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "axolotl-ai-cloud/axolotl",
      "stars": 8262,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".bandit",
          "type": "blob",
          "size": 0.037109375,
          "content": "[bandit]\nexclude = tests\nskips = B101\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.181640625,
          "content": "root = true\n\n[*]\nend_of_line = lf\ninsert_final_newline = true\ntrim_trailing_whitespace = true\n\n[*.py]\nindent_style = space\nindent_size = 4\n\n[**.yml]\nindent_style = space\nindent_size = 2\n"
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.0859375,
          "content": "[flake8]\nmax-line-length = 88\n\nselect = C,E,F,W,B,B950\nextend-ignore = E203, E501, W503\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0478515625,
          "content": "data/*.jsonl filter=lfs diff=lfs merge=lfs -text\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.2666015625,
          "content": "**/axolotl.egg-info\nconfigs\nlast_run_prepared/\noutputs\n.vscode\n_site/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\nvenv3.10/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n.idea/\n\n# WandB\n# wandb creates a folder to store logs for training runs\nwandb\n\n# Runs\nlora-out/*\nqlora-out/*\nmlruns/*\n\n/.quarto/\nprepared-datasets/\nsubmit.sh\n*.out*\n\ntypings/\nout/\n\n# vim\n*.swp\n"
        },
        {
          "name": ".isort.cfg",
          "type": "blob",
          "size": 0.056640625,
          "content": "[settings]\nprofile=black\nknown_third_party=wandb,comet_ml\n"
        },
        {
          "name": ".mypy.ini",
          "type": "blob",
          "size": 0.8779296875,
          "content": "[mypy]\nplugins = pydantic.mypy\nexclude = venv\n\n[mypy-alpaca_lora_4bit.*]\nignore_missing_imports = True\n\n[mypy-axolotl.monkeypatch.*]\nignore_errors = True\n\n[mypy-axolotl.models.mixtral.*]\nignore_errors = True\n\n[mypy-axolotl.integrations.liger.models.*]\nignore_errors = True\n\n[mypy-axolotl.models.phi.*]\nignore_errors = True\n\n[mypy-flash_attn.*]\nignore_missing_imports = True\n\n[mypy-huggingface_hub]\nignore_missing_imports = True\n\n[mypy-transformers.*]\nignore_missing_imports = True\n\n[mypy-peft]\nignore_missing_imports = True\n\n[mypy-wandb]\nignore_missing_imports = True\n\n[mypy-bitsandbytes]\nignore_missing_imports = True\n\n[mypy-requests]\nignore_missing_imports = True\n\n[mypy-datasets]\nignore_missing_imports = True\n\n[mypy-fire]\nignore_missing_imports = True\n\n[mypy-setuptools]\nignore_missing_imports = True\n\n[mypy-addict]\nignore_missing_imports = True\n\n[mypy-xformers.*]\nignore_missing_imports = True\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.9697265625,
          "content": "default_language_version:\n    python: python3\n\nrepos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n    -   id: check-yaml\n    -   id: end-of-file-fixer\n    -   id: trailing-whitespace\n    -   id: no-commit-to-branch\n        args: ['--branch', 'main']\n-   repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n    -   id: black\n-   repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n-   repo: https://github.com/PyCQA/flake8\n    rev: 6.0.0\n    hooks:\n    - id: flake8\n-   repo: https://github.com/PyCQA/pylint\n    rev: v3.3.0\n    hooks:\n    - id: pylint\n-   repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.3.0\n    hooks:\n    - id: mypy\n      additional_dependencies:\n        [\n            'types-PyYAML',\n            'pydantic>=2.5.3',\n        ]\n-   repo: https://github.com/PyCQA/bandit\n    rev: 1.7.5\n    hooks:\n    -   id: bandit\n        args: [\n            '--ini',\n            '.bandit',\n        ]\n"
        },
        {
          "name": ".pylintrc",
          "type": "blob",
          "size": 0.6806640625,
          "content": "[MASTER]\ninit-hook=\"from pylint.config import find_default_config_files; import sys; sys.path.append(next(find_default_config_files()).parent.as_posix())\"\n\n[TYPECHECK]\n\n# List of members which are set dynamically and missed by Pylint inference\n# system, and so shouldn't trigger E1101 when accessed.\ngenerated-members=numpy.*, torch.*\n\n\n[pylint.messages_control]\ndisable=missing-function-docstring, line-too-long, import-error,\n    too-many-arguments, too-many-locals, too-many-statements, too-many-branches, too-few-public-methods,\n    too-many-instance-attributes, fixme, import-outside-toplevel, logging-fstring-interpolation,\n    too-many-positional-arguments, possibly-used-before-assignment\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "FAQS.md",
          "type": "blob",
          "size": 0.6328125,
          "content": "# FAQs\n\n- Can you train StableLM with this? Yes, but only with a single GPU atm. Multi GPU support is coming soon! Just waiting on this [PR](https://github.com/huggingface/transformers/pull/22874)\n- Will this work with Deepspeed? That's still a WIP, but setting `export ACCELERATE_USE_DEEPSPEED=true` should work in some cases\n- `Error invalid argument at line 359 in file /workspace/bitsandbytes/csrc/pythonInterface.c`\n`/arrow/cpp/src/arrow/filesystem/s3fs.cc:2598:  arrow::fs::FinalizeS3 was not called even though S3 was initialized.`\nThis could lead to a segmentation fault at exit. Try reinstalling bitsandbytes and transformers from source.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.1416015625,
          "content": "include requirements.txt\ninclude README.md\ninclude LICENSE\ninclude src/setuptools_axolotl_dynamic_dependencies.py\nrecursive-include axolotl *.py\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 31.802734375,
          "content": "<p align=\"center\">\n    <picture>\n        <source media=\"(prefers-color-scheme: dark)\" srcset=\"image/axolotl_logo_digital_white.svg\">\n        <source media=\"(prefers-color-scheme: light)\" srcset=\"image/axolotl_logo_digital_black.svg\">\n        <img alt=\"Axolotl\" src=\"image/axolotl_logo_digital_black.svg\" width=\"400\" height=\"104\" style=\"max-width: 100%;\">\n    </picture>\n</p>\n\n<p align=\"center\">\n    <img src=\"https://img.shields.io/github/license/axolotl-ai-cloud/axolotl.svg?color=blue\" alt=\"GitHub License\">\n    <img src=\"https://github.com/axolotl-ai-cloud/axolotl/actions/workflows/tests.yml/badge.svg\" alt=\"tests\">\n    <a href=\"https://github.com/axolotl-ai-cloud/axolotl/releases\"><img src=\"https://img.shields.io/github/release/axolotl-ai-cloud/axolotl.svg\" alt=\"Releases\"></a>\n    <br/>\n    <a href=\"https://github.com/axolotl-ai-cloud/axolotl/graphs/contributors\"><img src=\"https://img.shields.io/github/contributors-anon/axolotl-ai-cloud/axolotl?color=yellow&style=flat-square\" alt=\"contributors\" style=\"height: 20px;\"></a>\n    <img src=\"https://img.shields.io/github/stars/axolotl-ai-cloud/axolotl\" alt=\"GitHub Repo stars\">\n    <br/>\n    <a href=\"https://discord.com/invite/HhrNrHJPRb\"><img src=\"https://img.shields.io/badge/discord-7289da.svg?style=flat-square&logo=discord\" alt=\"discord\" style=\"height: 20px;\"></a>\n    <a href=\"https://twitter.com/axolotl_ai\"><img src=\"https://img.shields.io/twitter/follow/axolotl_ai?style=social\" alt=\"twitter\" style=\"height: 20px;\"></a>\n    <br/>\n    <img src=\"https://github.com/axolotl-ai-cloud/axolotl/actions/workflows/tests-nightly.yml/badge.svg\" alt=\"tests-nightly\">\n    <img src=\"https://github.com/axolotl-ai-cloud/axolotl/actions/workflows/multi-gpu-e2e.yml/badge.svg\" alt=\"multigpu-semi-weekly tests\">\n</p>\n\nAxolotl is a tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures.\n\nFeatures:\n- Train various Huggingface models such as llama, pythia, falcon, mpt\n- Supports fullfinetune, lora, qlora, relora, and gptq\n- Customize configurations using a simple yaml file or CLI overwrite\n- Load different dataset formats, use custom formats, or bring your own tokenized datasets\n- Integrated with xformer, flash attention, [liger kernel](https://github.com/linkedin/Liger-Kernel), rope scaling, and multipacking\n- Works with single GPU or multiple GPUs via FSDP or Deepspeed\n- Easily run with Docker locally or on the cloud\n- Log results and optionally checkpoints to wandb, mlflow or Comet\n- And more!\n\n<a href=\"https://www.phorm.ai/query?projectId=e315ba4a-4e14-421f-ab05-38a1f9076f25\">\n  <img alt=\"phorm.ai\" src=\"https://img.shields.io/badge/Phorm-Ask_AI-%23F2777A.svg?&logo=data:image/svg+xml;base64,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\">\n</a>\n\n<table>\n<tr>\n<td>\n\n## Table of Contents\n- [Axolotl](#axolotl)\n  - [Table of Contents](#table-of-contents)\n  - [Quickstart ⚡](#quickstart-)\n    - [Edge Builds](#edge-builds-)\n    - [Axolotl CLI Usage](#axolotl-cli-usage)\n  - [Badge ❤🏷️](#badge-️)\n  - [Contributing 🤝](#contributing-)\n  - [Sponsors 🤝❤](#sponsors-)\n  - [Axolotl supports](#axolotl-supports)\n  - [Advanced Setup](#advanced-setup)\n    - [Environment](#environment)\n      - [Docker](#docker)\n      - [Conda/Pip venv](#condapip-venv)\n      - [Cloud GPU](#cloud-gpu)\n      - [Bare Metal Cloud GPU](#bare-metal-cloud-gpu)\n        - [LambdaLabs](#lambdalabs)\n        - [GCP](#gcp)\n      - [Windows](#windows)\n      - [Mac](#mac)\n      - [Google Colab](#google-colab)\n      - [Launching on public clouds via SkyPilot](#launching-on-public-clouds-via-skypilot)\n      - [Launching on public clouds via dstack](#launching-on-public-clouds-via-dstack)\n    - [Dataset](#dataset)\n    - [Config](#config)\n      - [All Config Options](#all-config-options)\n    - [Train](#train)\n      - [Preprocess dataset](#preprocess-dataset)\n      - [Multi-GPU](#multi-gpu)\n        - [DeepSpeed](#deepspeed)\n        - [FSDP](#fsdp)\n        - [FSDP + QLoRA](#fsdp--qlora)\n        - [Weights \\& Biases Logging](#weights--biases-logging)\n        - [Special Tokens](#special-tokens)\n      - [Liger Kernel](#liger-kernel)\n    - [Inference Playground](#inference-playground)\n    - [Merge LORA to base](#merge-lora-to-base)\n  - [Common Errors 🧰](#common-errors-)\n    - [Tokenization Mismatch b/w Inference \\& Training](#tokenization-mismatch-bw-inference--training)\n  - [Debugging Axolotl](#debugging-axolotl)\n  - [Need help? 🙋](#need-help-)\n\n</td>\n<td>\n\n<div align=\"center\">\n  <img src=\"image/axolotl_symbol_digital_white.svg\" alt=\"axolotl\" width=\"160\">\n  <div>\n    <p>\n      <b>Axolotl provides a unified repository for fine-tuning <br />a variety of AI models with ease</b>\n    </p>\n    <p>\n      Go ahead and Axolotl questions!!\n    </p>\n    <img src=\"https://github.com/axolotl-ai-cloud/axolotl/actions/workflows/pre-commit.yml/badge.svg?branch=main\" alt=\"pre-commit\">\n    <img alt=\"PyTest Status\" src=\"https://github.com/axolotl-ai-cloud/axolotl/actions/workflows/tests.yml/badge.svg?branch=main\">\n  </div>\n</div>\n\n</td>\n</tr>\n</table>\n\n## Quickstart ⚡\n\nGet started with Axolotl in just a few steps! This quickstart guide will walk you through setting up and running a basic fine-tuning task.\n\n**Requirements**: *Nvidia* GPU (Ampere architecture or newer for `bf16` and Flash Attention) or *AMD* GPU, Python >=3.10 and PyTorch >=2.3.1.\n\n```bash\npip3 install --no-build-isolation axolotl[flash-attn,deepspeed]\n\n# download examples and optionally deepspeed configs to the local path\naxolotl fetch examples\naxolotl fetch deepspeed_configs  # OPTIONAL\n\n# finetune using lora\naxolotl train examples/llama-3/lora-1b.yml\n```\n\n### Edge Builds 🏎️\n\nIf you're looking for the latest features and updates between releases, you'll need to install\nfrom source.\n\n```bash\ngit clone https://github.com/axolotl-ai-cloud/axolotl.git\ncd axolotl\npip3 install packaging ninja\npip3 install --no-build-isolation -e '.[flash-attn,deepspeed]'\n```\n\n### Axolotl CLI Usage\nWe now support a new, more streamlined CLI using [click](https://click.palletsprojects.com/en/stable/).\n\n```bash\n# preprocess datasets - optional but recommended\nCUDA_VISIBLE_DEVICES=\"0\" axolotl preprocess examples/llama-3/lora-1b.yml\n\n# finetune lora\naxolotl train examples/llama-3/lora-1b.yml\n\n# inference\naxolotl inference examples/llama-3/lora-1b.yml \\\n    --lora-model-dir=\"./outputs/lora-out\"\n\n# gradio\naxolotl inference examples/llama-3/lora-1b.yml \\\n    --lora-model-dir=\"./outputs/lora-out\" --gradio\n\n# remote yaml files - the yaml config can be hosted on a public URL\n# Note: the yaml config must directly link to the **raw** yaml\naxolotl train https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/examples/llama-3/lora-1b.yml\n```\n\nWe've also added a new command for fetching `examples` and `deepspeed_configs` to your\nlocal machine. This will come in handy when installing `axolotl` from PyPI.\n\n```bash\n# Fetch example YAML files (stores in \"examples/\" folder)\naxolotl fetch examples\n\n# Fetch deepspeed config files (stores in \"deepspeed_configs/\" folder)\naxolotl fetch deepspeed_configs\n\n# Optionally, specify a destination folder\naxolotl fetch examples --dest path/to/folder\n```\n\n### Legacy Usage\n<details>\n\n<summary>Click to Expand</summary>\n\nWhile the Axolotl CLI is the preferred method for interacting with axolotl, we\nstill support the legacy `-m axolotl.cli.*` usage.\n\n```bash\n# preprocess datasets - optional but recommended\nCUDA_VISIBLE_DEVICES=\"0\" python -m axolotl.cli.preprocess examples/llama-3/lora-1b.yml\n\n# finetune lora\naccelerate launch -m axolotl.cli.train examples/llama-3/lora-1b.yml\n\n# inference\naccelerate launch -m axolotl.cli.inference examples/llama-3/lora-1b.yml \\\n    --lora_model_dir=\"./outputs/lora-out\"\n\n# gradio\naccelerate launch -m axolotl.cli.inference examples/llama-3/lora-1b.yml \\\n    --lora_model_dir=\"./outputs/lora-out\" --gradio\n\n# remote yaml files - the yaml config can be hosted on a public URL\n# Note: the yaml config must directly link to the **raw** yaml\naccelerate launch -m axolotl.cli.train https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/examples/llama-3/lora-1b.yml\n```\n\n</details>\n\n## Badge ❤🏷️\n\nBuilding something cool with Axolotl? Consider adding a badge to your model card.\n\n```markdown\n[<img src=\"https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\" width=\"200\" height=\"32\"/>](https://github.com/axolotl-ai-cloud/axolotl)\n```\n\n[<img src=\"https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\" width=\"200\" height=\"32\"/>](https://github.com/axolotl-ai-cloud/axolotl)\n\n## Sponsors 🤝❤\n\nIf you love axolotl, consider sponsoring the project by reaching out directly to [wing@axolotl.ai](mailto:wing@axolotl.ai).\n\n---\n\n- [Modal](https://modal.com/) Modal lets you run data/AI jobs in the cloud, by just writing a few lines of Python. Customers use Modal to deploy Gen AI models at large scale, fine-tune LLM models, run protein folding simulations, and much more.\n\n---\n\n## Contributing 🤝\n\nPlease read the [contributing guide](./.github/CONTRIBUTING.md)\n\nBugs? Please check the [open issues](https://github.com/axolotl-ai-cloud/axolotl/issues/bug) else create a new Issue.\n\nPRs are **greatly welcome**!\n\nPlease run the quickstart instructions followed by the below to setup env:\n```bash\npip3 install -r requirements-dev.txt -r requirements-tests.txt\npre-commit install\n\n# test\npytest tests/\n\n# optional: run against all files\npre-commit run --all-files\n```\n\nThanks to all of our contributors to date. Help drive open source AI progress forward by contributing to Axolotl.\n\n<a href=\"https://github.com/axolotl-ai-cloud/axolotl/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=openaccess-ai-collective/axolotl\" alt=\"contributor chart by https://contrib.rocks\"/>\n</a>\n\n## Axolotl supports\n\n|             | fp16/fp32 | lora | qlora | gptq | gptq w/flash attn | flash attn | xformers attn |\n|-------------|:----------|:-----|-------|------|-------------------|------------|--------------|\n| llama       | ✅         | ✅    | ✅     | ✅             | ✅                 | ✅          | ✅            |\n| Mistral     | ✅         | ✅    | ✅     | ✅             | ✅                 | ✅          | ✅            |\n| Mixtral-MoE | ✅         | ✅    | ✅     | ❓             | ❓                 | ❓          | ❓            |\n| Mixtral8X22 | ✅         | ✅    | ✅     | ❓             | ❓                 | ❓          | ❓            |\n| Pythia      | ✅         | ✅    | ✅     | ❌             | ❌                 | ❌          | ❓            |\n| cerebras    | ✅         | ✅    | ✅     | ❌             | ❌                 | ❌          | ❓            |\n| btlm        | ✅         | ✅    | ✅     | ❌             | ❌                 | ❌          | ❓            |\n| mpt         | ✅         | ❌    | ❓     | ❌             | ❌                 | ❌          | ❓            |\n| falcon      | ✅         | ✅    | ✅     | ❌             | ❌                 | ❌          | ❓            |\n| gpt-j       | ✅         | ✅    | ✅     | ❌             | ❌                 | ❓          | ❓            |\n| XGen        | ✅         | ❓    | ✅     | ❓             | ❓                 | ❓          | ✅            |\n| phi         | ✅         | ✅    | ✅     | ❓             | ❓                 | ❓          | ❓            |\n| RWKV        | ✅         | ❓    | ❓     | ❓             | ❓                 | ❓          | ❓            |\n| Qwen        | ✅         | ✅    | ✅     | ❓             | ❓                 | ❓          | ❓            |\n| Gemma       | ✅         | ✅    | ✅     | ❓             | ❓                 | ✅          | ❓            |\n| Jamba       | ✅         | ✅    | ✅     | ❓             | ❓                 | ✅          | ❓            |\n\n✅: supported\n❌: not supported\n❓: untested\n\n## Advanced Setup\n\n### Environment\n\n#### Docker\n\n  ```bash\n  docker run --gpus '\"all\"' --rm -it axolotlai/axolotl:main-latest\n  ```\n\n  Or run on the current files for development:\n\n  ```sh\n  docker compose up -d\n  ```\n\n>[!Tip]\n> If you want to debug axolotl or prefer to use Docker as your development environment, see the [debugging guide's section on Docker](docs/debugging.qmd#debugging-with-docker).\n\n  <details>\n\n  <summary>Docker advanced</summary>\n\n  A more powerful Docker command to run would be this:\n\n  ```bash\ndocker run --privileged --gpus '\"all\"' --shm-size 10g --rm -it --name axolotl --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --mount type=bind,src=\"${PWD}\",target=/workspace/axolotl -v ${HOME}/.cache/huggingface:/root/.cache/huggingface axolotlai/axolotl:main-latest\n  ```\n\n  It additionally:\n  * Prevents memory issues when running e.g. deepspeed (e.g. you could hit SIGBUS/signal 7 error) through `--ipc` and `--ulimit` args.\n  * Persists the downloaded HF data (models etc.) and your modifications to axolotl code through `--mount`/`-v` args.\n  * The `--name` argument simply makes it easier to refer to the container in vscode (`Dev Containers: Attach to Running Container...`) or in your terminal.\n  * The `--privileged` flag gives all capabilities to the container.\n  * The `--shm-size 10g` argument increases the shared memory size. Use this if you see `exitcode: -7` errors using deepspeed.\n\n  [More information on nvidia website](https://docs.nvidia.com/deeplearning/frameworks/user-guide/index.html#setincshmem)\n\n  </details>\n\n#### Conda/Pip venv\n  1. Install python >=**3.10**\n\n  2. Install pytorch stable https://pytorch.org/get-started/locally/\n\n  3. Install Axolotl along with python dependencies\n        ```bash\n        pip3 install packaging\n        pip3 install --no-build-isolation -e '.[flash-attn,deepspeed]'\n        ```\n  4. (Optional) Login to Huggingface to use gated models/datasets.\n        ```bash\n        huggingface-cli login\n        ```\n        Get the token at huggingface.co/settings/tokens\n\n#### Cloud GPU\n\nFor cloud GPU providers that support docker images, use [`axolotlai/axolotl-cloud:main-latest`](https://hub.docker.com/r/axolotlai/axolotl-cloud/tags)\n\n- on Latitude.sh use this [direct link](https://latitude.sh/blueprint/989e0e79-3bf6-41ea-a46b-1f246e309d5c)\n- on JarvisLabs.ai use this [direct link](https://jarvislabs.ai/templates/axolotl)\n- on RunPod use this [direct link](https://runpod.io/gsc?template=v2ickqhz9s&ref=6i7fkpdz)\n\n#### Bare Metal Cloud GPU\n\n##### LambdaLabs\n\n  <details>\n\n  <summary>Click to Expand</summary>\n\n  1. Install python\n  ```bash\n  sudo apt update\n  sudo apt install -y python3.10\n\n  sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1\n  sudo update-alternatives --config python # pick 3.10 if given option\n  python -V # should be 3.10\n\n  ```\n\n  2. Install pip\n  ```bash\n  wget https://bootstrap.pypa.io/get-pip.py\n  python get-pip.py\n  ```\n\n  3. Install Pytorch https://pytorch.org/get-started/locally/\n\n  4. Follow instructions on quickstart.\n\n  5. Run\n  ```bash\n  pip3 install protobuf==3.20.3\n  pip3 install -U --ignore-installed requests Pillow psutil scipy\n  ```\n\n  6. Set path\n  ```bash\n  export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH\n  ```\n  </details>\n\n##### GCP\n\n<details>\n\n<summary>Click to Expand</summary>\n\nUse a Deeplearning linux OS with cuda and pytorch installed. Then follow instructions on quickstart.\n\nMake sure to run the below to uninstall xla.\n```bash\npip uninstall -y torch_xla[tpu]\n```\n\n</details>\n\n#### Windows\nPlease use WSL or Docker!\n\n#### Mac\n\nUse the below instead of the install method in QuickStart.\n```\npip3 install --no-build-isolation -e '.'\n```\nMore info: [mac.md](/docs/mac.qmd)\n\n#### Google Colab\n\nPlease use this example [notebook](examples/colab-notebooks/colab-axolotl-example.ipynb).\n\n#### Launching on public clouds via SkyPilot\nTo launch on GPU instances (both on-demand and spot instances) on 7+ clouds (GCP, AWS, Azure, OCI, and more), you can use [SkyPilot](https://skypilot.readthedocs.io/en/latest/index.html):\n\n```bash\npip install \"skypilot-nightly[gcp,aws,azure,oci,lambda,kubernetes,ibm,scp]\"  # choose your clouds\nsky check\n```\n\nGet the [example YAMLs](https://github.com/skypilot-org/skypilot/tree/master/llm/axolotl) of using Axolotl to finetune `mistralai/Mistral-7B-v0.1`:\n```\ngit clone https://github.com/skypilot-org/skypilot.git\ncd skypilot/llm/axolotl\n```\n\nUse one command to launch:\n```bash\n# On-demand\nHF_TOKEN=xx sky launch axolotl.yaml --env HF_TOKEN\n\n# Managed spot (auto-recovery on preemption)\nHF_TOKEN=xx BUCKET=<unique-name> sky spot launch axolotl-spot.yaml --env HF_TOKEN --env BUCKET\n```\n\n#### Launching on public clouds via dstack\nTo launch on GPU instance (both on-demand and spot instances) on public clouds (GCP, AWS, Azure, Lambda Labs, TensorDock, Vast.ai, and CUDO), you can use [dstack](https://dstack.ai/).\n\nWrite a job description in YAML as below:\n\n```yaml\n# dstack.yaml\ntype: task\n\nimage: axolotlai/axolotl-cloud:main-latest\n\nenv:\n  - HUGGING_FACE_HUB_TOKEN\n  - WANDB_API_KEY\n\ncommands:\n  - accelerate launch -m axolotl.cli.train config.yaml\n\nports:\n  - 6006\n\nresources:\n  gpu:\n    memory: 24GB..\n    count: 2\n```\n\nthen, simply run the job with `dstack run` command. Append `--spot` option if you want spot instance. `dstack run` command will show you the instance with cheapest price across multi cloud services:\n\n```bash\npip install dstack\nHUGGING_FACE_HUB_TOKEN=xxx WANDB_API_KEY=xxx dstack run . -f dstack.yaml # --spot\n```\n\nFor further and fine-grained use cases, please refer to the official [dstack documents](https://dstack.ai/docs/) and the detailed description of [axolotl example](https://github.com/dstackai/dstack/tree/master/examples/fine-tuning/axolotl) on the official repository.\n\n### Dataset\n\nAxolotl supports a variety of dataset formats.  It is recommended to use a JSONL.  The schema of the JSONL depends upon the task and the prompt template you wish to use.  Instead of a JSONL, you can also use a HuggingFace dataset with columns for each JSONL field.\n\nSee [the documentation](https://axolotl-ai-cloud.github.io/axolotl/docs/dataset-formats/) for more information on how to use different dataset formats.\n\n### Config\n\nSee [examples](examples) for quick start. It is recommended to duplicate and modify to your needs. The most important options are:\n\n- model\n  ```yaml\n  base_model: ./llama-7b-hf # local or huggingface repo\n  ```\n  Note: The code will load the right architecture.\n\n- dataset\n  ```yaml\n  datasets:\n      # huggingface repo\n    - path: vicgalle/alpaca-gpt4\n      type: alpaca\n\n      # huggingface repo with specific configuration/subset\n    - path: EleutherAI/pile\n      name: enron_emails\n      type: completion # format from earlier\n      field: text # Optional[str] default: text, field to use for completion data\n\n      # huggingface repo with multiple named configurations/subsets\n    - path: bigcode/commitpackft\n      name:\n        - ruby\n        - python\n        - typescript\n      type: ... # unimplemented custom format\n\n      # chat_template https://axolotl-ai-cloud.github.io/axolotl/docs/dataset-formats/conversation.html#chat_template\n    - path: ...\n      type: chat_template\n      chat_template: chatml # defaults to tokenizer's chat_template\n\n      # local\n    - path: data.jsonl # or json\n      ds_type: json # see other options below\n      type: alpaca\n\n      # dataset with splits, but no train split\n    - path: knowrohit07/know_sql\n      type: context_qa.load_v2\n      train_on_split: validation\n\n      # loading from s3 or gcs\n      # s3 creds will be loaded from the system default and gcs only supports public access\n    - path: s3://path_to_ds # Accepts folder with arrow/parquet or file path like above. Supports s3, gcs.\n      ...\n\n      # Loading Data From a Public URL\n      # - The file format is `json` (which includes `jsonl`) by default. For different formats, adjust the `ds_type` option accordingly.\n    - path: https://some.url.com/yourdata.jsonl # The URL should be a direct link to the file you wish to load. URLs must use HTTPS protocol, not HTTP.\n      ds_type: json # this is the default, see other options below.\n  ```\n\n- loading\n  ```yaml\n  load_in_4bit: true\n  load_in_8bit: true\n\n  bf16: auto # require >=ampere, auto will detect if your GPU supports this and choose automatically.\n  fp16: # leave empty to use fp16 when bf16 is 'auto'. set to false if you want to fallback to fp32\n  tf32: true # require >=ampere\n\n  bfloat16: true # require >=ampere, use instead of bf16 when you don't want AMP (automatic mixed precision)\n  float16: true # use instead of fp16 when you don't want AMP\n  ```\n  Note: Repo does not do 4-bit quantization.\n\n- lora\n  ```yaml\n  adapter: lora # 'qlora' or leave blank for full finetune\n  lora_r: 8\n  lora_alpha: 16\n  lora_dropout: 0.05\n  lora_target_modules:\n    - q_proj\n    - v_proj\n  ```\n\n#### All Config Options\n\nSee [these docs](docs/config.qmd) for all config options.\n\n### Train\n\nRun\n```bash\naccelerate launch -m axolotl.cli.train your_config.yml\n```\n\n> [!TIP]\n> You can also reference a config file that is hosted on a public URL, for example `accelerate launch -m axolotl.cli.train https://yourdomain.com/your_config.yml`\n\n#### Preprocess dataset\n\nYou can optionally pre-tokenize dataset with the following before finetuning.\nThis is recommended for large datasets.\n\n- Set `dataset_prepared_path:` to a local folder for saving and loading pre-tokenized dataset.\n- (Optional): Set `push_dataset_to_hub: hf_user/repo` to push it to Huggingface.\n- (Optional): Use `--debug` to see preprocessed examples.\n\n```bash\npython -m axolotl.cli.preprocess your_config.yml\n```\n\n#### Multi-GPU\n\nBelow are the options available in axolotl for training with multiple GPUs. Note that DeepSpeed\nis the recommended multi-GPU option currently because FSDP may experience\n[loss instability](https://github.com/huggingface/transformers/issues/26498).\n\n##### DeepSpeed\n\nDeepspeed is an optimization suite for multi-gpu systems allowing you to train much larger models than you\nmight typically be able to fit into your GPU's VRAM. More information about the various optimization types\nfor deepspeed is available at https://huggingface.co/docs/accelerate/main/en/usage_guides/deepspeed#what-is-integrated\n\nWe provide several default deepspeed JSON configurations for ZeRO stage 1, 2, and 3.\n\n```yaml\ndeepspeed: deepspeed_configs/zero1.json\n```\n\n```shell\naccelerate launch -m axolotl.cli.train examples/llama-2/config.yml --deepspeed deepspeed_configs/zero1.json\n```\n\n##### FSDP\n\n- llama FSDP\n```yaml\nfsdp:\n  - full_shard\n  - auto_wrap\nfsdp_config:\n  fsdp_offload_params: true\n  fsdp_state_dict_type: FULL_STATE_DICT\n  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer\n```\n\n##### FSDP + QLoRA\n\nAxolotl supports training with FSDP and QLoRA, see [these docs](docs/fsdp_qlora.qmd) for more information.\n\n##### Weights & Biases Logging\n\nMake sure your `WANDB_API_KEY` environment variable is set (recommended) or you login to wandb with `wandb login`.\n\n- wandb options\n```yaml\nwandb_mode:\nwandb_project:\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n```\n\n##### Comet Logging\n\nMake sure your `COMET_API_KEY` environment variable is set (recommended) or you login to wandb with `comet login`.\n\n- wandb options\n```yaml\nuse_comet:\ncomet_api_key:\ncomet_workspace:\ncomet_project_name:\ncomet_experiment_key:\ncomet_mode:\ncomet_online:\ncomet_experiment_config:\n```\n\n##### Special Tokens\n\nIt is important to have special tokens like delimiters, end-of-sequence, beginning-of-sequence in your tokenizer's vocabulary.  This will help you avoid tokenization issues and help your model train better.  You can do this in axolotl like this:\n\n```yml\nspecial_tokens:\n  bos_token: \"<s>\"\n  eos_token: \"</s>\"\n  unk_token: \"<unk>\"\ntokens: # these are delimiters\n  - \"<|im_start|>\"\n  - \"<|im_end|>\"\n```\n\nWhen you include these tokens in your axolotl config, axolotl adds these tokens to the tokenizer's vocabulary.\n\n##### Liger Kernel\n\nLiger Kernel: Efficient Triton Kernels for LLM Training\n\nhttps://github.com/linkedin/Liger-Kernel\n\nLiger (LinkedIn GPU Efficient Runtime) Kernel is a collection of Triton kernels designed specifically for LLM training.\nIt can effectively increase multi-GPU training throughput by 20% and reduces memory usage by 60%. The Liger Kernel\ncomposes well and is compatible with both FSDP and Deepspeed.\n\n```yaml\nplugins:\n  - axolotl.integrations.liger.LigerPlugin\nliger_rope: true\nliger_rms_norm: true\nliger_glu_activation: true\nliger_layer_norm: true\nliger_fused_linear_cross_entropy: true\n```\n\n### Inference Playground\n\nAxolotl allows you to load your model in an interactive terminal playground for quick experimentation.\nThe config file is the same config file used for training.\n\nPass the appropriate flag to the inference command, depending upon what kind of model was trained:\n\n- Pretrained LORA:\n  ```bash\n  python -m axolotl.cli.inference examples/your_config.yml --lora_model_dir=\"./lora-output-dir\"\n  ```\n- Full weights finetune:\n  ```bash\n  python -m axolotl.cli.inference examples/your_config.yml --base_model=\"./completed-model\"\n  ```\n- Full weights finetune w/ a prompt from a text file:\n  ```bash\n  cat /tmp/prompt.txt | python -m axolotl.cli.inference examples/your_config.yml \\\n    --base_model=\"./completed-model\" --prompter=None --load_in_8bit=True\n  ```\n-- With gradio hosting\n  ```bash\n  python -m axolotl.cli.inference examples/your_config.yml --gradio\n  ```\n\nPlease use `--sample_packing False` if you have it on and receive the error similar to below:\n\n> RuntimeError: stack expects each tensor to be equal size, but got [1, 32, 1, 128] at entry 0 and [1, 32, 8, 128] at entry 1\n\n### Merge LORA to base\n\nThe following command will merge your LORA adapater with your base model. You can optionally pass the argument `--lora_model_dir` to specify the directory where your LORA adapter was saved, otherwhise, this will be inferred from `output_dir` in your axolotl config file.  The merged model is saved in the sub-directory `{lora_model_dir}/merged`.\n\n```bash\npython3 -m axolotl.cli.merge_lora your_config.yml --lora_model_dir=\"./completed-model\"\n```\n\nYou may need to use the `gpu_memory_limit` and/or `lora_on_cpu` config options to avoid running out of memory. If you still run out of CUDA memory, you can try to merge in system RAM with\n\n```bash\nCUDA_VISIBLE_DEVICES=\"\" python3 -m axolotl.cli.merge_lora ...\n```\n\nalthough this will be very slow, and using the config options above are recommended instead.\n\n## Common Errors 🧰\n\nSee also the [FAQ's](./docs/faq.qmd) and [debugging guide](docs/debugging.qmd).\n\n> If you encounter a 'Cuda out of memory' error, it means your GPU ran out of memory during the training process. Here's how to resolve it:\n\nPlease reduce any below\n  - `micro_batch_size`\n  - `eval_batch_size`\n  - `gradient_accumulation_steps`\n  - `sequence_len`\n\nIf it does not help, try running without deepspeed and without accelerate (replace \"accelerate launch\" with \"python\") in the command.\n\nUsing adamw_bnb_8bit might also save you some memory.\n\n> `failed (exitcode: -9)`\n\nUsually means your system has run out of system memory.\nSimilarly, you should consider reducing the same settings as when you run out of VRAM.\nAdditionally, look into upgrading your system RAM which should be simpler than GPU upgrades.\n\n> RuntimeError: expected scalar type Float but found Half\n\nTry set `fp16: true`\n\n> NotImplementedError: No operator found for `memory_efficient_attention_forward` ...\n\nTry to turn off xformers.\n\n> accelerate config missing\n\nIt's safe to ignore it.\n\n> NCCL Timeouts during training\n\nSee the [NCCL](docs/nccl.qmd) guide.\n\n\n### Tokenization Mismatch b/w Inference & Training\n\nFor many formats, Axolotl constructs prompts by concatenating token ids _after_ tokenizing strings.  The reason for concatenating token ids rather than operating on strings is to maintain precise accounting for attention masks.\n\nIf you decode a prompt constructed by axolotl, you might see spaces between tokens (or lack thereof) that you do not expect, especially around delimiters and special tokens.  When you are starting out with a new format, you should always do the following:\n\n1. Materialize some data using `python -m axolotl.cli.preprocess your_config.yml --debug`, and then decode the first few rows with your model's tokenizer.\n2. During inference, right before you pass a tensor of token ids to your model, decode these tokens back into a string.\n3. Make sure the inference string from #2 looks **exactly** like the data you fine tuned on from #1, including spaces and new lines.  If they aren't the same, adjust your inference server accordingly.\n4. As an additional troubleshooting step, you can look at the token ids between 1 and 2 to make sure they are identical.\n\nHaving misalignment between your prompts during training and inference can cause models to perform very poorly, so it is worth checking this.  See [this blog post](https://hamel.dev/notes/llm/finetuning/05_tokenizer_gotchas.html) for a concrete example.\n\n## Debugging Axolotl\n\nSee [this debugging guide](docs/debugging.qmd) for tips on debugging Axolotl, along with an example configuration for debugging with VSCode.\n\n## Need help? 🙋\n\nJoin our [Discord server](https://discord.gg/HhrNrHJPRb) where our community members can help you.\n\nNeed dedicated support? Please contact us at [✉️wing@axolotl.ai](ailto:wing@axolotl.ai) for dedicated support options.\n"
        },
        {
          "name": "TODO.md",
          "type": "blob",
          "size": 0.255859375,
          "content": "# todo list\n\n- [] Validation of parameters for combinations that won't work\n\n\n\n## things that are known not to work\n\n- FSDP offload and gradient_checkpointing - https://github.com/pytorch/pytorch/issues/82203\n- adamw_bnb_8bit doesn't play well with FSDP offload\n"
        },
        {
          "name": "_quarto.yml",
          "type": "blob",
          "size": 1.2041015625,
          "content": "project:\n  type: website\n\nwebsite:\n  title: \"Axolotl\"\n  description: \"Fine-tuning\"\n  favicon: favicon.jpg\n  navbar:\n    title: Axolotl\n    background: dark\n    pinned: false\n    collapse: false\n    tools:\n    - icon: twitter\n      href: https://twitter.com/axolotl_ai\n    - icon: github\n      href: https://github.com/axolotl-ai-cloud/axolotl/\n    - icon: discord\n      href: https://discord.gg/7m9sfhzaf3\n\n  sidebar:\n      pinned: true\n      collapse-level: 2\n      style: docked\n      contents:\n        - text: Home\n          href: index.qmd\n        - section: \"How-To Guides\"\n          contents:\n          # TODO Edit folder structure after we have more docs.\n            - docs/debugging.qmd\n            - docs/multipack.qmd\n            - docs/fsdp_qlora.qmd\n            - docs/input_output.qmd\n            - docs/rlhf.qmd\n            - docs/nccl.qmd\n            - docs/mac.qmd\n            - docs/multi-node.qmd\n            - docs/unsloth.qmd\n            - docs/amd_hpc.qmd\n        - section: \"Dataset Formats\"\n          contents: docs/dataset-formats/*\n        - section: \"Reference\"\n          contents:\n            - docs/config.qmd\n        - docs/faq.qmd\n\n\nformat:\n  html:\n    theme: materia\n    css: styles.css\n    toc: true\n"
        },
        {
          "name": "cicd",
          "type": "tree",
          "content": null
        },
        {
          "name": "deepspeed_configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "devtools",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose.yaml",
          "type": "blob",
          "size": 0.6845703125,
          "content": "# version: '3.8'\nservices:\n  axolotl:\n    build:\n      context: .\n      dockerfile: ./docker/Dockerfile\n    volumes:\n      - .:/workspace/axolotl\n      - ~/.cache/huggingface/:/root/.cache/huggingface/\n    # set environment variables\n    environment:\n      # Set environment variables\n      - GIT_AUTHOR_NAME=${GIT_AUTHOR_NAME}\n      - GIT_AUTHOR_EMAIL=${GIT_AUTHOR_EMAIL}\n      - GIT_COMMITTER_NAME=${GIT_COMMITTER_NAME}\n      - GIT_COMMITTER_EMAIL=${GIT_COMMITTER_EMAIL}\n      - WANDB_API_KEY=${WANDB_API_KEY}\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              # count: 1\n              capabilities: [gpu]\n    command: tail -f /dev/null\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "favicon.jpg",
          "type": "blob",
          "size": 4.529296875,
          "content": null
        },
        {
          "name": "image",
          "type": "tree",
          "content": null
        },
        {
          "name": "index.qmd",
          "type": "blob",
          "size": 0.4697265625,
          "content": "---\ntoc-location: right-body\ntoc-title: Table Of Contents\ntoc-expand: 2\n---\n\n```{python}\n#|output: asis\n#|echo: false\n\n# This cell steals the README as the home page for now, but excludes the table of contents (quarto adds its own)\nimport re\npattern = re.compile(\n    r\"<table>\\s*<tr>\\s*<td>\\s*## Table of Contents.*?</td>\\s*</tr>\\s*</table>\",\n    re.DOTALL | re.IGNORECASE\n)\n\nwith open('README.md', 'r') as f:\n    txt = f.read()\n\ncleaned = pattern.sub(\"\", txt)\nprint(cleaned)\n```\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.677734375,
          "content": "[build-system]\nrequires = [\"setuptools>=64\", \"wheel\", \"setuptools_scm>=8\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"axolotl\"\ndynamic = [\"version\", \"dependencies\", \"optional-dependencies\"]\ndescription = \"LLM Trainer\"\nreadme = \"README.md\"\nrequires-python = \">=3.10\"\n\n[project.scripts]\naxolotl = \"axolotl.cli.main:main\"\n\n[project.urls]\nHomepage = \"https://axolotl-ai-cloud.github.io/axolotl/\"\nRepository = \"https://github.com/axolotl-ai-cloud/axolotl.git\"\n\n[tool.setuptools_scm]\n\n[tool.setuptools]\npy-modules = [\"setuptools_axolotl_dynamic_dependencies\"]\ninclude-package-data = true\n\n[tool.setuptools.cmdclass]\nbuild_py = \"setuptools_axolotl_dynamic_dependencies.BuildPyCommand\"\n"
        },
        {
          "name": "requirements-dev.txt",
          "type": "blob",
          "size": 0.0361328125,
          "content": "pre-commit\nblack\nmypy\ntypes-requests\n"
        },
        {
          "name": "requirements-tests.txt",
          "type": "blob",
          "size": 0.052734375,
          "content": "pytest\npytest-xdist\npytest-retry\npytest-sugar\ntbparse\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.94921875,
          "content": "--extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n\n# START section of dependencies that don't install on Darwin/MacOS\nbitsandbytes==0.45.0\ntriton>=3.0.0\nmamba-ssm==1.2.0.post1\nflash-attn==2.7.0.post2\nxformers>=0.0.23.post1\nautoawq==0.2.7.post3\nliger-kernel==0.5.2\n# END section\n\npackaging==23.2\n\npeft==0.14.0\ntransformers==4.47.1\ntokenizers>=0.21.0\naccelerate==1.2.1\ndatasets==3.2.0\ndeepspeed==0.16.1\ntrl==0.13.0\n\noptimum==1.16.2\nhf_transfer\nsentencepiece\ngradio==3.50.2\n\npydantic==2.6.3\naddict\nfire\nPyYAML>=6.0\nrequests\nwandb\neinops\ncolorama\nnumba\nnumpy>=1.24.4,<=2.0.1\n# qlora things\nevaluate==0.4.1\nscipy\nscikit-learn==1.4.2\nnvidia-ml-py==12.560.30\nart\ntensorboard\npython-dotenv==1.0.1\n\n# remote filesystems\ns3fs>=2024.5.0\ngcsfs>=2024.5.0\n# adlfs\n\nzstandard==0.22.0\nfastcore\n\n# lm eval harness\nlm_eval==0.4.7\nlangdetect==1.0.9\nimmutabledict==4.2.0\nantlr4-python3-runtime==4.13.2\n\ntorchao==0.7.0\nschedulefree==1.3.0\n\naxolotl-contribs-lgpl==0.0.3\n"
        },
        {
          "name": "requirements_env.txt",
          "type": "blob",
          "size": 5.7880859375,
          "content": "accelerate==0.34.1\naddict==2.4.0\naiofiles==23.2.1\naiohttp==3.9.0\naiosignal==1.3.1\naiostream==0.5.2\nalembic==1.13.1\nannotated-types==0.6.0\nannoy==1.17.3\nansible==6.7.0\nansible-core==2.13.13\nansible-vault==2.1.0\nanyio==3.7.1\nappdirs==1.4.4\nart==6.0\nasgiref==3.7.2\nasync-timeout==4.0.2\nattrdict==2.0.1\nattrs==22.2.0\nawscli==1.32.75\n-e git+ssh://git@github.com/OpenAccess-AI-Collective/axolotl.git@6e354682e3c1735d3f7fb9e362280c38e922260f#egg=axolotl\nbackoff==2.2.1\nbase58==2.1.1\nbeartype==0.17.2\nbitnet==0.2.1\nbitsandbytes==0.42.0\nbittensor==6.7.0\nblack==23.7.0\nblinker==1.7.0\nboto3==1.34.75\nbotocore==1.34.75\ncachetools==5.3.3\ncachy==0.1.1\ncertifi==2023.7.22\ncffi==1.16.0\ncfgv==3.3.1\nchai-guanaco==1.2.4\ncharset-normalizer==3.2.0\ncleo==0.6.8\nclick==8.1.7\ncloudpickle==2.0.0\ncohere==4.11.2\ncolorama==0.4.4\ncoloredlogs==15.0.1\nCoLT5-attention==0.10.20\ncontextlib2==21.6.0\ncontourpy==1.2.0\ncryptography==41.0.3\ncycler==0.12.1\ncytoolz==0.12.3\ndatabricks-cli==0.18.0\ndataclasses-json==0.5.7\ndatasets==2.11.0\nddt==1.6.0\ndecorator==5.1.1\ndeepspeed==0.15.0\n# Editable Git install with no remote (dialogpt==0.1)\n-e /Users/wing/Projects/ml/dialogpt/src\ndill==0.3.6\ndistlib==0.3.6\ndocker==7.0.0\ndocker-pycreds==0.4.0\ndocstring-parser==0.15\ndocutils==0.16\necdsa==0.18.0\neinops==0.7.0\neinops-exts==0.0.4\neinx==0.1.3\nentrypoints==0.4\neth-hash==0.6.0\neth-keys==0.5.0\neth-typing==4.0.0\neth-utils==2.3.1\nevaluate==0.4.0\nexceptiongroup==1.1.1\nfastapi==0.109.2\nfastcore==1.5.29\nffmpy==0.4.0\nfilelock==3.12.2\n-e git+https://github.com/NousResearch/finetuning-subnet.git@24e9407d6b4430a7ca39d344692f89ce5a97d27e#egg=finetuning_subnet\nfire==0.5.0\nfirst==2.0.2\nflake8==7.0.0\nFlask==3.0.1\nfonttools==4.47.2\nfrozendict==2.4.1\nfrozenlist==1.3.3\nfschat @ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe\nfsspec==2023.6.0\nfuzzywuzzy==0.18.0\ngitdb==4.0.10\nGitPython==3.1.31\ngoogle-pasta==0.2.0\ngradio==4.42.0\ngradio_client==1.3.0\ngreenlet==2.0.2\ngrpclib==0.4.7\ngunicorn==21.2.0\nh11==0.14.0\nh2==4.1.0\nhpack==4.0.0\nhttpcore==0.17.3\nhttpx==0.24.1\nhuggingface-hub==0.23.4\nhumanfriendly==10.0\nhyperframe==6.0.1\nidentify==2.5.24\nidna==3.4\nimmutables==0.20\nimportlib-metadata==6.7.0\nimportlib-resources==6.1.1\ninflection==0.5.1\niniconfig==2.0.0\nitsdangerous==2.1.2\nJinja2==3.1.2\njmespath==1.0.1\njoblib==1.3.2\njsonlines==3.1.0\njsonschema==2.6.0\nkiwisolver==1.4.5\nlangchain==0.0.144\nLevenshtein==0.24.0\nlibcst==1.1.0\nliger-kernel==0.0.0\nlion-pytorch==0.1.2\nllama-cpp-python==0.1.36\nllvmlite==0.40.1\nlocal-attention==1.9.0\nloguru==0.7.0\nMako==1.3.2\nMarkdown==3.5.2\nmarkdown-it-py==3.0.0\nmarkdown2==2.4.10\nMarkupSafe==2.1.2\nmarshmallow==3.19.0\nmarshmallow-enum==1.5.1\nmatplotlib==3.8.2\nmccabe==0.7.0\nmdurl==0.1.2\nMEGABYTE-pytorch==0.0.7\n-e git+https://github.com/cg123/mergekit.git@53c5f414774a0558b8d84858fb6374bc93a8f1c1#egg=mergekit\nmlflow==2.10.0\nmodal==0.62.77\nmore-itertools==10.2.0\nmpmath==1.2.1\nmsgpack==1.0.7\nmsgpack-numpy-opentensor==0.5.0\nmultidict==6.0.4\nmultiprocess==0.70.14\nmunch==2.5.0\nmypy==1.3.0\nmypy-extensions==1.0.0\nnest-asyncio==1.6.0\nnetaddr==0.10.1\nnetworkx==3.0rc1\nnh3==0.2.14\nnodeenv==1.8.0\nnomic==2.0.2\nnumba==0.57.1\nnumexpr==2.8.4\nnumpy==1.24.4\noauthlib==3.2.2\nopenai==0.27.4\nopenapi==1.1.0\nopenapi-schema-pydantic==1.2.4\noptimum==1.8.6\norjson==3.10.7\npackaging==23.1\npandas==2.0.0\nparameterized==0.9.0\npassword-strength==0.0.3.post2\npastel==0.1.1\npathos==0.3.0\npathspec==0.11.1\npathtools==0.1.2\npeft==0.11.1\npendulum==3.0.0\nPillow==9.5.0\npip-tools==1.11.0\nplatformdirs==3.2.0\npluggy==1.4.0\npoetry==0.7.1\npox==0.3.2\nppft==1.7.6.6\npre-commit==3.3.2\nprettytable==3.10.0\nprompt-toolkit==3.0.39\nprotobuf==3.20.2\nprotobuf3-to-dict==0.1.5\npsutil==5.9.5\npsycopg==3.1.18\nPuLP==2.8.0\npy==1.11.0\npy-bip39-bindings==0.1.11\npy-cpuinfo==9.0.0\npy-ed25519-zebra-bindings==1.0.1\npy-sr25519-bindings==0.2.0\npyarrow==11.0.0\npyasn1==0.6.0\npycodestyle==2.11.1\npycparser==2.21\npycryptodome==3.20.0\npydantic==2.5.3\npydantic_core==2.14.6\npydub==0.25.1\npyfiglet==0.8.post1\npyflakes==3.2.0\nPygments==2.15.1\nPyJWT==2.8.0\npylev==1.4.0\nPyNaCl==1.5.0\npynvml==11.5.0\npyparsing==2.4.7\npyrsistent==0.14.11\npytest==8.0.2\npytest-asyncio==0.23.4\npython-dateutil==2.8.2\npython-dotenv==1.0.1\npython-Levenshtein==0.24.0\npython-multipart==0.0.9\npytz==2023.3\nPyYAML==6.0.1\nquerystring-parser==1.2.4\nrapidfuzz==3.6.1\nregex==2023.6.3\nrequests==2.31.0\nrequests-toolbelt==0.8.0\nresolvelib==0.8.1\nresponses==0.18.0\nretry==0.9.2\nrich==13.7.0\nrsa==4.7.2\nruff==0.6.3\ns3transfer==0.10.1\nsafetensors==0.4.5\nsagemaker==2.148.0\nscalecodec==1.2.7\nschedulefree==1.2.1\nschema==0.7.5\nscikit-learn==1.4.0\nscipy==1.9.3\nseaborn==0.13.2\nsemantic-version==2.10.0\nsentencepiece==0.2.0\nsentry-sdk==1.19.1\nsetproctitle==1.3.2\nshellingham==1.5.4\nshortuuid==1.0.11\nshtab==1.6.5\nsigtools==4.0.1\nsix==1.16.0\nskypilot==0.4.1\nsmdebug-rulesconfig==1.0.1\nsmmap==5.0.0\nsniffio==1.3.0\nSQLAlchemy==1.4.47\nsqlparse==0.4.4\nstarlette==0.36.3\nsubstrate-interface==1.5.2\nsvgwrite==1.4.3\nsympy==1.11.1\nsynchronicity==0.6.7\ntabulate==0.9.0\ntblib==1.7.0\ntenacity==8.2.2\ntensor-parallel==2.0.0\ntermcolor==2.2.0\ntext2art==0.2.0\nthreadpoolctl==3.2.0\ntiktoken==0.6.0\ntime-machine==2.14.1\ntimm==0.9.16\ntokenizers==0.19.1\ntokenmonster==1.1.12\ntoml==0.9.6\ntomli==2.0.1\ntomlkit==0.12.0\ntoolz==0.12.1\ntorch==2.2.0\ntorchdata==0.6.1\ntorchdiffeq==0.2.3\nTorchFix==0.4.0\ntorchtext==0.15.2\ntorchvision==0.17.0\ntqdm==4.66.2\ntransformers==4.44.2\ntrl==0.9.6\ntyper==0.12.5\ntypes-certifi==2021.10.8.3\ntypes-requests==2.31.0.20240125\ntypes-setuptools==69.0.0.20240125\ntypes-toml==0.10.8.7\ntyping==3.7.4.3\ntyping-inspect==0.8.0\ntyping_extensions==4.9.0\ntyro==0.5.18\ntzdata==2023.3\nunique-names-generator==1.0.2\nurllib3==2.2.2\nuvicorn==0.22.0\nvector_quantize_pytorch==1.14.1\nvirtualenv==20.23.0\nvoyager==2.0.2\nwandb==0.16.2\nwatchfiles==0.21.0\nwavedrom==2.0.3.post3\nwcwidth==0.2.6\nwebsocket-client==1.7.0\nwebsockets==12.0\nWerkzeug==3.0.1\nwonderwords==2.2.0\nxxhash==3.2.0\nyarl==1.8.2\nzetascale==2.2.7\nzipp==3.15.0\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 6.16796875,
          "content": "\"\"\"setup.py for axolotl\"\"\"\n\nimport ast\nimport os\nimport platform\nimport re\nfrom importlib.metadata import PackageNotFoundError, version\nfrom pathlib import Path\n\nfrom setuptools import find_packages, setup\n\n\ndef parse_requirements():\n    _install_requires = []\n    _dependency_links = []\n    with open(\"./requirements.txt\", encoding=\"utf-8\") as requirements_file:\n        lines = [r.strip() for r in requirements_file.readlines()]\n        for line in lines:\n            is_extras = (\n                \"flash-attn\" in line\n                or \"flash-attention\" in line\n                or \"deepspeed\" in line\n                or \"mamba-ssm\" in line\n                or \"lion-pytorch\" in line\n            )\n            if line.startswith(\"--extra-index-url\"):\n                # Handle custom index URLs\n                _, url = line.split()\n                _dependency_links.append(url)\n            elif not is_extras and line and line[0] != \"#\":\n                # Handle standard packages\n                _install_requires.append(line)\n    try:\n        xformers_version = [req for req in _install_requires if \"xformers\" in req][0]\n        triton_version = [req for req in _install_requires if \"triton\" in req][0]\n        torchao_version = [req for req in _install_requires if \"torchao\" in req][0]\n        autoawq_version = [req for req in _install_requires if \"autoawq\" in req][0]\n        if \"Darwin\" in platform.system():\n            # skip packages not compatible with OSX\n            skip_packages = [\n                \"bitsandbytes\",\n                \"triton\",\n                \"mamba-ssm\",\n                \"flash-attn\",\n                \"xformers\",\n                \"autoawq\",\n                \"liger-kernel\",\n            ]\n            _install_requires = [\n                req\n                for req in _install_requires\n                if re.split(r\"[>=<]\", req)[0].strip() not in skip_packages\n            ]\n            print(\n                _install_requires, [req in skip_packages for req in _install_requires]\n            )\n        else:\n            # detect the version of torch already installed\n            # and set it so dependencies don't clobber the torch version\n            try:\n                torch_version = version(\"torch\")\n            except PackageNotFoundError:\n                torch_version = \"2.5.1\"\n            _install_requires.append(f\"torch=={torch_version}\")\n\n            version_match = re.match(r\"^(\\d+)\\.(\\d+)(?:\\.(\\d+))?\", torch_version)\n            if version_match:\n                major, minor, patch = version_match.groups()\n                major, minor = int(major), int(minor)\n                patch = (\n                    int(patch) if patch is not None else 0\n                )  # Default patch to 0 if not present\n            else:\n                raise ValueError(\"Invalid version format\")\n\n            if (major, minor) >= (2, 5):\n                _install_requires.pop(_install_requires.index(xformers_version))\n                if patch == 0:\n                    _install_requires.append(\"xformers==0.0.28.post2\")\n                else:\n                    _install_requires.append(\"xformers==0.0.28.post3\")\n                _install_requires.pop(_install_requires.index(autoawq_version))\n            elif (major, minor) >= (2, 4):\n                if patch == 0:\n                    _install_requires.pop(_install_requires.index(xformers_version))\n                    _install_requires.append(\"xformers>=0.0.27\")\n                else:\n                    _install_requires.pop(_install_requires.index(xformers_version))\n                    _install_requires.append(\"xformers==0.0.28.post1\")\n            elif (major, minor) >= (2, 3):\n                _install_requires.pop(_install_requires.index(torchao_version))\n                _install_requires.pop(_install_requires.index(triton_version))\n                _install_requires.append(\"triton>=2.3.1\")\n                if patch == 0:\n                    _install_requires.pop(_install_requires.index(xformers_version))\n                    _install_requires.append(\"xformers>=0.0.26.post1\")\n                else:\n                    _install_requires.pop(_install_requires.index(xformers_version))\n                    _install_requires.append(\"xformers>=0.0.27\")\n            elif (major, minor) >= (2, 2):\n                _install_requires.pop(_install_requires.index(torchao_version))\n                _install_requires.pop(_install_requires.index(xformers_version))\n                _install_requires.append(\"xformers>=0.0.25.post1\")\n            else:\n                _install_requires.pop(_install_requires.index(torchao_version))\n                _install_requires.pop(_install_requires.index(xformers_version))\n                _install_requires.append(\"xformers>=0.0.23.post1\")\n\n    except PackageNotFoundError:\n        pass\n    return _install_requires, _dependency_links\n\n\ndef get_package_version():\n    with open(\n        Path(os.path.dirname(os.path.abspath(__file__)))\n        / \"src\"\n        / \"axolotl\"\n        / \"__init__.py\",\n        \"r\",\n        encoding=\"utf-8\",\n    ) as fin:\n        version_match = re.search(r\"^__version__\\s*=\\s*(.*)$\", fin.read(), re.MULTILINE)\n    version_ = ast.literal_eval(version_match.group(1))\n    return version_\n\n\ninstall_requires, dependency_links = parse_requirements()\n\nsetup(\n    version=get_package_version(),\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    install_requires=install_requires,\n    dependency_links=dependency_links,\n    entry_points={\n        \"console_scripts\": [\n            \"axolotl=axolotl.cli.main:main\",\n        ],\n    },\n    extras_require={\n        \"flash-attn\": [\n            \"flash-attn==2.7.0.post2\",\n        ],\n        \"deepspeed\": [\n            \"deepspeed==0.16.1\",\n            \"deepspeed-kernels\",\n        ],\n        \"mamba-ssm\": [\n            \"mamba-ssm==1.2.0.post1\",\n            \"causal_conv1d\",\n        ],\n        \"auto-gptq\": [\n            \"auto-gptq==0.5.1\",\n        ],\n        \"mlflow\": [\n            \"mlflow\",\n        ],\n        \"lion-pytorch\": [\n            \"lion-pytorch==0.1.2\",\n        ],\n        \"galore\": [\n            \"galore_torch\",\n        ],\n        \"optimizers\": [\n            \"galore_torch\",\n            \"lion-pytorch==0.1.2\",\n            \"lomo-optim==0.1.1\",\n            \"torch-optimi==0.2.1\",\n        ],\n    },\n)\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "styles.css",
          "type": "blob",
          "size": 0.0166015625,
          "content": "/* css styles */\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}