{
  "metadata": {
    "timestamp": 1736560934447,
    "page": 672,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "dennybritz/cnn-text-classification-tf",
      "stars": 5651,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.849609375,
          "content": "*.npy\nruns/\n\n# Created by https://www.gitignore.io/api/python,ipythonnotebook\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n\n### IPythonNotebook ###\n# Temporary data\n.ipynb_checkpoints/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright {yyyy} {name of copyright owner}\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.2265625,
          "content": "**[This code belongs to the \"Implementing a CNN for Text Classification in Tensorflow\" blog post.](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)**\n\nIt is slightly simplified implementation of Kim's [Convolutional Neural Networks for Sentence Classification](http://arxiv.org/abs/1408.5882) paper in Tensorflow.\n\n## Requirements\n\n- Python 3\n- Tensorflow > 0.12\n- Numpy\n\n## Training\n\nPrint parameters:\n\n```bash\n./train.py --help\n```\n\n```\noptional arguments:\n  -h, --help            show this help message and exit\n  --embedding_dim EMBEDDING_DIM\n                        Dimensionality of character embedding (default: 128)\n  --filter_sizes FILTER_SIZES\n                        Comma-separated filter sizes (default: '3,4,5')\n  --num_filters NUM_FILTERS\n                        Number of filters per filter size (default: 128)\n  --l2_reg_lambda L2_REG_LAMBDA\n                        L2 regularizaion lambda (default: 0.0)\n  --dropout_keep_prob DROPOUT_KEEP_PROB\n                        Dropout keep probability (default: 0.5)\n  --batch_size BATCH_SIZE\n                        Batch Size (default: 64)\n  --num_epochs NUM_EPOCHS\n                        Number of training epochs (default: 100)\n  --evaluate_every EVALUATE_EVERY\n                        Evaluate model on dev set after this many steps\n                        (default: 100)\n  --checkpoint_every CHECKPOINT_EVERY\n                        Save model after this many steps (default: 100)\n  --allow_soft_placement ALLOW_SOFT_PLACEMENT\n                        Allow device soft device placement\n  --noallow_soft_placement\n  --log_device_placement LOG_DEVICE_PLACEMENT\n                        Log placement of ops on devices\n  --nolog_device_placement\n\n```\n\nTrain:\n\n```bash\n./train.py\n```\n\n## Evaluating\n\n```bash\n./eval.py --eval_train --checkpoint_dir=\"./runs/1459637919/checkpoints/\"\n```\n\nReplace the checkpoint dir with the output from the training. To use your own data, change the `eval.py` script to load your data.\n\n\n## References\n\n- [Convolutional Neural Networks for Sentence Classification](http://arxiv.org/abs/1408.5882)\n- [A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification](http://arxiv.org/abs/1510.03820)\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "data_helpers.py",
          "type": "blob",
          "size": 2.4140625,
          "content": "import numpy as np\nimport re\n\n\ndef clean_str(string):\n    \"\"\"\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip().lower()\n\n\ndef load_data_and_labels(positive_data_file, negative_data_file):\n    \"\"\"\n    Loads MR polarity data from files, splits the data into words and generates labels.\n    Returns split sentences and labels.\n    \"\"\"\n    # Load data from files\n    positive_examples = list(open(positive_data_file, \"r\", encoding='utf-8').readlines())\n    positive_examples = [s.strip() for s in positive_examples]\n    negative_examples = list(open(negative_data_file, \"r\", encoding='utf-8').readlines())\n    negative_examples = [s.strip() for s in negative_examples]\n    # Split by words\n    x_text = positive_examples + negative_examples\n    x_text = [clean_str(sent) for sent in x_text]\n    # Generate labels\n    positive_labels = [[0, 1] for _ in positive_examples]\n    negative_labels = [[1, 0] for _ in negative_examples]\n    y = np.concatenate([positive_labels, negative_labels], 0)\n    return [x_text, y]\n\n\ndef batch_iter(data, batch_size, num_epochs, shuffle=True):\n    \"\"\"\n    Generates a batch iterator for a dataset.\n    \"\"\"\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        if shuffle:\n            shuffle_indices = np.random.permutation(np.arange(data_size))\n            shuffled_data = data[shuffle_indices]\n        else:\n            shuffled_data = data\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]\n"
        },
        {
          "name": "eval.py",
          "type": "blob",
          "size": 3.650390625,
          "content": "#! /usr/bin/env python\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport time\nimport datetime\nimport data_helpers\nfrom text_cnn import TextCNN\nfrom tensorflow.contrib import learn\nimport csv\n\n# Parameters\n# ==================================================\n\n# Data Parameters\ntf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\ntf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n\n# Eval Parameters\ntf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\ntf.flags.DEFINE_string(\"checkpoint_dir\", \"\", \"Checkpoint directory from training run\")\ntf.flags.DEFINE_boolean(\"eval_train\", False, \"Evaluate on all training data\")\n\n# Misc Parameters\ntf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\ntf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n\n\nFLAGS = tf.flags.FLAGS\nFLAGS._parse_flags()\nprint(\"\\nParameters:\")\nfor attr, value in sorted(FLAGS.__flags.items()):\n    print(\"{}={}\".format(attr.upper(), value))\nprint(\"\")\n\n# CHANGE THIS: Load data. Load your own data here\nif FLAGS.eval_train:\n    x_raw, y_test = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n    y_test = np.argmax(y_test, axis=1)\nelse:\n    x_raw = [\"a masterpiece four years in the making\", \"everything is off.\"]\n    y_test = [1, 0]\n\n# Map data into vocabulary\nvocab_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"vocab\")\nvocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\nx_test = np.array(list(vocab_processor.transform(x_raw)))\n\nprint(\"\\nEvaluating...\\n\")\n\n# Evaluation\n# ==================================================\ncheckpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\ngraph = tf.Graph()\nwith graph.as_default():\n    session_conf = tf.ConfigProto(\n      allow_soft_placement=FLAGS.allow_soft_placement,\n      log_device_placement=FLAGS.log_device_placement)\n    sess = tf.Session(config=session_conf)\n    with sess.as_default():\n        # Load the saved meta graph and restore variables\n        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n        saver.restore(sess, checkpoint_file)\n\n        # Get the placeholders from the graph by name\n        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n\n        # Tensors we want to evaluate\n        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n\n        # Generate batches for one epoch\n        batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n\n        # Collect the predictions here\n        all_predictions = []\n\n        for x_test_batch in batches:\n            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n            all_predictions = np.concatenate([all_predictions, batch_predictions])\n\n# Print accuracy if y_test is defined\nif y_test is not None:\n    correct_predictions = float(sum(all_predictions == y_test))\n    print(\"Total number of test examples: {}\".format(len(y_test)))\n    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))\n\n# Save the evaluation to a csv\npredictions_human_readable = np.column_stack((np.array(x_raw), all_predictions))\nout_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"prediction.csv\")\nprint(\"Saving evaluation to {0}\".format(out_path))\nwith open(out_path, 'w') as f:\n    csv.writer(f).writerows(predictions_human_readable)\n"
        },
        {
          "name": "text_cnn.py",
          "type": "blob",
          "size": 3.6875,
          "content": "import tensorflow as tf\nimport numpy as np\n\n\nclass TextCNN(object):\n    \"\"\"\n    A CNN for text classification.\n    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n    \"\"\"\n    def __init__(\n      self, sequence_length, num_classes, vocab_size,\n      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n\n        # Placeholders for input, output and dropout\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n\n        # Keeping track of l2 regularization loss (optional)\n        l2_loss = tf.constant(0.0)\n\n        # Embedding layer\n        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n            self.W = tf.Variable(\n                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n                name=\"W\")\n            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs = []\n        for i, filter_size in enumerate(filter_sizes):\n            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n                # Convolution Layer\n                filter_shape = [filter_size, embedding_size, 1, num_filters]\n                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n                conv = tf.nn.conv2d(\n                    self.embedded_chars_expanded,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=\"VALID\",\n                    name=\"conv\")\n                # Apply nonlinearity\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(\n                    h,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding='VALID',\n                    name=\"pool\")\n                pooled_outputs.append(pooled)\n\n        # Combine all the pooled features\n        num_filters_total = num_filters * len(filter_sizes)\n        self.h_pool = tf.concat(pooled_outputs, 3)\n        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n\n        # Add dropout\n        with tf.name_scope(\"dropout\"):\n            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n\n        # Final (unnormalized) scores and predictions\n        with tf.name_scope(\"output\"):\n            W = tf.get_variable(\n                \"W\",\n                shape=[num_filters_total, num_classes],\n                initializer=tf.contrib.layers.xavier_initializer())\n            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n            l2_loss += tf.nn.l2_loss(W)\n            l2_loss += tf.nn.l2_loss(b)\n            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n\n        # Calculate mean cross-entropy loss\n        with tf.name_scope(\"loss\"):\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\n        # Accuracy\n        with tf.name_scope(\"accuracy\"):\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 8.8603515625,
          "content": "#! /usr/bin/env python\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport time\nimport datetime\nimport data_helpers\nfrom text_cnn import TextCNN\nfrom tensorflow.contrib import learn\n\n# Parameters\n# ==================================================\n\n# Data loading params\ntf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\ntf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\ntf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n\n# Model Hyperparameters\ntf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\ntf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\ntf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\ntf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\ntf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n\n# Training parameters\ntf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\ntf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\ntf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\ntf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\ntf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n# Misc Parameters\ntf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\ntf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n\nFLAGS = tf.flags.FLAGS\n# FLAGS._parse_flags()\n# print(\"\\nParameters:\")\n# for attr, value in sorted(FLAGS.__flags.items()):\n#     print(\"{}={}\".format(attr.upper(), value))\n# print(\"\")\n\ndef preprocess():\n    # Data Preparation\n    # ==================================================\n\n    # Load data\n    print(\"Loading data...\")\n    x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n\n    # Build vocabulary\n    max_document_length = max([len(x.split(\" \")) for x in x_text])\n    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n    x = np.array(list(vocab_processor.fit_transform(x_text)))\n\n    # Randomly shuffle data\n    np.random.seed(10)\n    shuffle_indices = np.random.permutation(np.arange(len(y)))\n    x_shuffled = x[shuffle_indices]\n    y_shuffled = y[shuffle_indices]\n\n    # Split train/test set\n    # TODO: This is very crude, should use cross-validation\n    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n\n    del x, y, x_shuffled, y_shuffled\n\n    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n    return x_train, y_train, vocab_processor, x_dev, y_dev\n\ndef train(x_train, y_train, vocab_processor, x_dev, y_dev):\n    # Training\n    # ==================================================\n\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n          allow_soft_placement=FLAGS.allow_soft_placement,\n          log_device_placement=FLAGS.log_device_placement)\n        sess = tf.Session(config=session_conf)\n        with sess.as_default():\n            cnn = TextCNN(\n                sequence_length=x_train.shape[1],\n                num_classes=y_train.shape[1],\n                vocab_size=len(vocab_processor.vocabulary_),\n                embedding_size=FLAGS.embedding_dim,\n                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n                num_filters=FLAGS.num_filters,\n                l2_reg_lambda=FLAGS.l2_reg_lambda)\n\n            # Define Training procedure\n            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n            optimizer = tf.train.AdamOptimizer(1e-3)\n            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n\n            # Keep track of gradient values and sparsity (optional)\n            grad_summaries = []\n            for g, v in grads_and_vars:\n                if g is not None:\n                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n                    grad_summaries.append(grad_hist_summary)\n                    grad_summaries.append(sparsity_summary)\n            grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n            # Output directory for models and summaries\n            timestamp = str(int(time.time()))\n            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n            print(\"Writing to {}\\n\".format(out_dir))\n\n            # Summaries for loss and accuracy\n            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n\n            # Train Summaries\n            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n            # Dev summaries\n            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n\n            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n            if not os.path.exists(checkpoint_dir):\n                os.makedirs(checkpoint_dir)\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n\n            # Write vocabulary\n            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n\n            # Initialize all variables\n            sess.run(tf.global_variables_initializer())\n\n            def train_step(x_batch, y_batch):\n                \"\"\"\n                A single training step\n                \"\"\"\n                feed_dict = {\n                  cnn.input_x: x_batch,\n                  cnn.input_y: y_batch,\n                  cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n                }\n                _, step, summaries, loss, accuracy = sess.run(\n                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n                    feed_dict)\n                time_str = datetime.datetime.now().isoformat()\n                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n                train_summary_writer.add_summary(summaries, step)\n\n            def dev_step(x_batch, y_batch, writer=None):\n                \"\"\"\n                Evaluates model on a dev set\n                \"\"\"\n                feed_dict = {\n                  cnn.input_x: x_batch,\n                  cnn.input_y: y_batch,\n                  cnn.dropout_keep_prob: 1.0\n                }\n                step, summaries, loss, accuracy = sess.run(\n                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n                    feed_dict)\n                time_str = datetime.datetime.now().isoformat()\n                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n                if writer:\n                    writer.add_summary(summaries, step)\n\n            # Generate batches\n            batches = data_helpers.batch_iter(\n                list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n            # Training loop. For each batch...\n            for batch in batches:\n                x_batch, y_batch = zip(*batch)\n                train_step(x_batch, y_batch)\n                current_step = tf.train.global_step(sess, global_step)\n                if current_step % FLAGS.evaluate_every == 0:\n                    print(\"\\nEvaluation:\")\n                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n                    print(\"\")\n                if current_step % FLAGS.checkpoint_every == 0:\n                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                    print(\"Saved model checkpoint to {}\\n\".format(path))\n\ndef main(argv=None):\n    x_train, y_train, vocab_processor, x_dev, y_dev = preprocess()\n    train(x_train, y_train, vocab_processor, x_dev, y_dev)\n\nif __name__ == '__main__':\n    tf.app.run()"
        }
      ]
    }
  ]
}