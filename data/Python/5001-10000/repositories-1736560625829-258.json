{
  "metadata": {
    "timestamp": 1736560625829,
    "page": 258,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Plachtaa/VALL-E-X",
      "stars": 7755,
      "defaultBranch": "master",
      "files": [
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0400390625,
          "content": "MIT License\n\nCopyright (c) 2023 Songting\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README-ZH.md",
          "type": "blob",
          "size": 13.3076171875,
          "content": "# VALL-E X: 多语言文本到语音合成与语音克隆 🔊\n[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/qCBRmAnTxg)\n<br>\n[English](README.md) | 中文\n<br>\n微软[VALL-E X](https://arxiv.org/pdf/2303.03926) 零样本语音合成模型的开源实现.<br>\n**预训练模型现已向公众开放，供研究或应用使用。**\n![vallex-framework](/images/vallex_framework.jpg \"VALL-E X framework\")\n\nVALL-E X 是一个强大而创新的多语言文本转语音（TTS）模型，最初由微软发布。虽然微软最初在他们的研究论文中提出了该概念，但并未发布任何代码或预训练模型。我们认识到了这项技术的潜力和价值，复现并训练了一个开源可用的VALL-E X模型。我们很乐意与社区分享我们的预训练模型，让每个人都能体验到次世代TTS的威力。 🎧\n<br>\n更多细节请查看 [model card](./model-card.md).\n\n## 📖 目录\n* [🚀 更新日志](#-更新日志)\n* [📢 功能特点](#-功能特点)\n* [💻 本地安装](#-本地安装)\n* [🎧 在线Demo](#-在线Demo)\n* [🐍 使用方法](#-Python中的使用方法)\n* [❓ FAQ](#-faq)\n* [🧠 TODO](#-todo)\n\n## 🚀 Updates\n**2023.09.10**\n- 支持AR decoder的batch decoding以实现更稳定的生成结果\n\n**2023.08.30**\n- 将EnCodec解码器替换成了Vocos解码器，提升了音质。 (感谢[@v0xie](https://github.com/v0xie))\n\n**2023.08.23**\n- 加入了长文本生成功能\n\n**2023.08.20**\n- 加入了中文版README\n\n**2023.08.14**\n- 预训练模型权重已发布，从[这里](https://drive.google.com/file/d/10gdQWvP-K_e1undkvv0p2b7SU6I4Egyl/view?usp=sharing)下载。\n\n## 💻 本地安装\n### 使用pip安装，必须使用Python 3.10，CUDA 11.7 ~ 12.0，PyTorch 2.0+\n```commandline\ngit clone https://github.com/Plachtaa/VALL-E-X.git\ncd VALL-E-X\npip install -r requirements.txt\n```\n\n> 注意：如果需要制作prompt，需要安装 ffmpeg 并将其所在文件夹加入到环境变量PATH中\n\n第一次运行程序时，会自动下载相应的模型。如果下载失败并报错，请按照以下步骤手动下载模型。\n\n（请注意目录和文件夹的大小写）\n\n1.检查安装目录下是否存在`checkpoints`文件夹，如果没有，在安装目录下手动创建`checkpoints`文件夹（`./checkpoints/`）。\n\n2.检查`checkpoints`文件夹中是否有`vallex-checkpoint.pt`文件。如果没有，请从[这里](https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt)\n手动下载`vallex-checkpoint.pt`文件并放到`checkpoints`文件夹里。\n\n3.检查安装目录下是否存在`whisper`文件夹，如果没有，在安装目录下手动创建`whisper`文件夹（`./whisper/`）。\n\n4.检查`whisper`文件夹中是否有`medium.pt`文件。如果没有，请从[这里](https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt)\n手动下载`medium.pt`文件并放到`whisper`文件夹里。\n\n##  🎧 在线Demo\n如果你不想在本地安装，你可以在线体验VALL-E X的功能，点击下面的任意一个链接即可开始体验。\n<br>\n[![Open in Spaces](https://img.shields.io/badge/🤗-Open%20in%20Spaces-blue.svg)](https://huggingface.co/spaces/Plachta/VALL-E-X)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1yyD_sz531QntLKowMHo-XxorsFBCfKul?usp=sharing)\n\n\n## 📢 功能特点\n\nVALL-E X 配备有一系列尖端功能：\n\n1. **多语言 TTS**: 可使用三种语言 - 英语、中文和日语 - 进行自然、富有表现力的语音合成。\n\n2. **零样本语音克隆**: 仅需录制任意说话人的短短的 3~10 秒录音，VALL-E X 就能生成个性化、高质量的语音，完美还原他们的声音。\n\n<details>\n  <summary><h5>查看示例</h5></summary>\n\n[prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/a7baa51d-a53a-41cc-a03d-6970f25fcca7)\n\n\n[output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/b895601a-d126-4138-beff-061aabdc7985)\n\n</details>\n\n3. **语音情感控制**: VALL-E X 可以合成与给定说话人录音相同情感的语音，为音频增添更多表现力。\n\n<details>\n  <summary><h5>查看示例</h5></summary>\n\nhttps://github.com/Plachtaa/VALL-E-X/assets/112609742/56fa9988-925e-4757-82c5-83ecb0df6266\n\n\nhttps://github.com/Plachtaa/VALL-E-X/assets/112609742/699c47a3-d502-4801-8364-bd89bcc0b8f1\n\n</details>\n\n4. **零样本跨语言语音合成**: VALL-E X 可以合成与给定说话人母语不同的另一种语言，在不影响口音和流利度的同时，保留该说话人的音色与情感。以下是一个使用日语母语者进行英文与中文合成的样例： 🇯🇵 🗣\n\n<details>\n  <summary><h5>查看示例</h5></summary>\n\n[jp-prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/ea6e2ee4-139a-41b4-837e-0bd04dda6e19)\n\n\n[en-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/db8f9782-923f-425e-ba94-e8c1bd48f207)\n\n\n[zh-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/15829d79-e448-44d3-8965-fafa7a3f8c28)\n\n</details>\n\n5. **口音控制**: VALL-E X 允许您控制所合成音频的口音，比如说中文带英语口音或反之。 🇨🇳 💬\n\n<details>\n  <summary><h5>查看示例</h5></summary>\n\n[en-prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/f688d7f6-70ef-46ec-b1cc-355c31e78b3b)\n\n\n[zh-accent-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/be59c7ca-b45b-44ca-a30d-4d800c950ccc)\n\n\n[en-accent-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/8b4f4f9b-f299-4ea4-a548-137437b71738)\n\n</details>\n\n6. **声学环境保留**: 当给定说话人的录音在不同的声学环境下录制时，VALL-E X 可以保留该声学环境，使合成语音听起来更加自然。\n\n<details>\n  <summary><h5>查看示例</h5></summary>\n\n[noise-prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/68986d88-abd0-4d1d-96e4-4f893eb9259e)\n\n\n[noise-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/96c4c612-4516-4683-8804-501b70938608)\n\n</details>\n\n\n你可以访问我们的[demo页面](https://plachtaa.github.io/) 来浏览更多示例!\n\n## 💻 Python中的使用方法\n\n<details open>\n  <summary><h3>🪑 基本使用</h3></summary>\n\n```python\nfrom utils.generation import SAMPLE_RATE, generate_audio, preload_models\nfrom scipy.io.wavfile import write as write_wav\nfrom IPython.display import Audio\n\n# download and load all models\npreload_models()\n\n# generate audio from text\ntext_prompt = \"\"\"\nHello, my name is Nose. And uh, and I like hamburger. Hahaha... But I also have other interests such as playing tactic toast.\n\"\"\"\naudio_array = generate_audio(text_prompt)\n\n# save audio to disk\nwrite_wav(\"vallex_generation.wav\", SAMPLE_RATE, audio_array)\n\n# play text in notebook\nAudio(audio_array, rate=SAMPLE_RATE)\n```\n\n[hamburger.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/578d7bbe-cda9-483e-898c-29646edc8f2e)\n\n</details>\n\n<details open>\n  <summary><h3>🌎 多语言</h3></summary>\n<br>\n该VALL-E X实现支持三种语言：英语、中文和日语。您可以通过设置`language`参数来指定语言。默认情况下，该模型将自动检测语言。\n<br>\n\n```python\n\ntext_prompt = \"\"\"\n    チュソクは私のお気に入りの祭りです。 私は数日間休んで、友人や家族との時間を過ごすことができます。\n\"\"\"\naudio_array = generate_audio(text_prompt)\n```\n\n[vallex_japanese.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/ee57a688-3e83-4be5-b0fe-019d16eec51c)\n\n*注意：即使在一句话中混合多种语言的情况下，VALL-E X也能完美地控制口音，但是您需要手动标记各个句子对应的语言以便于我们的G2P工具识别它们。*\n```python\ntext_prompt = \"\"\"\n    [EN]The Thirty Years' War was a devastating conflict that had a profound impact on Europe.[EN]\n    [ZH]这是历史的开始。 如果您想听更多，请继续。[ZH]\n\"\"\"\naudio_array = generate_audio(text_prompt, language='mix')\n```\n\n[vallex_codeswitch.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/d8667abf-bd08-499f-a383-a861d852f98a)\n\n</details>\n\n<details open>\n<summary><h3>📼 预设音色</h3></summary>\n  \n我们提供十几种说话人音色可直接VALL-E X使用! 在[这里](/presets)浏览所有可用音色。\n\n> VALL-E X 尝试匹配给定预设音色的音调、音高、情感和韵律。该模型还尝试保留音乐、环境噪声等。\n```python\ntext_prompt = \"\"\"\nI am an innocent boy with a smoky voice. It is a great honor for me to speak at the United Nations today.\n\"\"\"\naudio_array = generate_audio(text_prompt, prompt=\"dingzhen\")\n```\n\n[smoky.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/d3f55732-b1cd-420f-87d6-eab60db14dc5)\n\n</details>\n\n<details open>\n<summary><h3>🎙声音克隆</h3></summary>\n  \nVALL-E X 支持声音克隆！你可以使用任何人，角色，甚至是你自己的声音，来制作一个音频提示。在你使用该音频提示时，VALL-E X 将会使用与其相似的声音来合成文本。\n<br>\n你需要提供一段3~10秒长的语音，以及该语音对应的文本，来制作音频提示。你也可以将文本留空，让[Whisper](https://github.com/openai/whisper)模型为你生成文本。\n> VALL-E X 尝试匹配给定音频提示的音调、音高、情感和韵律。该模型还尝试保留音乐、环境噪声等。\n\n```python\nfrom utils.prompt_making import make_prompt\n\n### Use given transcript\nmake_prompt(name=\"paimon\", audio_prompt_path=\"paimon_prompt.wav\",\n                transcript=\"Just, what was that? Paimon thought we were gonna get eaten.\")\n\n### Alternatively, use whisper\nmake_prompt(name=\"paimon\", audio_prompt_path=\"paimon_prompt.wav\")\n```\n来尝试一下刚刚做好的音频提示吧！\n```python\nfrom utils.generation import SAMPLE_RATE, generate_audio, preload_models\nfrom scipy.io.wavfile import write as write_wav\n\n# download and load all models\npreload_models()\n\ntext_prompt = \"\"\"\nHey, Traveler, Listen to this, This machine has taken my voice, and now it can talk just like me!\n\"\"\"\naudio_array = generate_audio(text_prompt, prompt=\"paimon\")\n\nwrite_wav(\"paimon_cloned.wav\", SAMPLE_RATE, audio_array)\n\n```\n\n[paimon_prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/e7922859-9d12-4e2a-8651-e156e4280311)\n\n\n[paimon_cloned.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/60d3b7e9-5ead-4024-b499-a897ce5f3d5e)\n\n\n</details>\n\n\n<details open>\n<summary><h3>🎢用户界面</h3></summary>\n\n如果你不擅长代码，我们还为VALL-E X创建了一个用户友好的图形界面。它可以让您轻松地与模型进行交互，使语音克隆和多语言语音合成变得轻而易举。\n<br>\n使用以下命令启动用户界面：\n```commandline\npython -X utf8 launch-ui.py\n```\n</details>\n\n## 🛠️ 硬件要求及推理速度\n\nVALL-E X 可以在CPU或GPU上运行 (`pytorch 2.0+`, CUDA 11.7 ~ CUDA 12.0).\n\n若使用GPU运行，你需要至少6GB的显存。\n\n## ⚙️ Details\n\nVALL-E X 与 [Bark](https://github.com/suno-ai/bark), [VALL-E](https://arxiv.org/abs/2301.02111) and [AudioLM](https://arxiv.org/abs/2209.03143)类似, 使用GPT风格的模型以自回归方式预测量化音频token，并由[EnCodec](https://github.com/facebookresearch/encodec)解码.\n<br>\n与 [Bark](https://github.com/suno-ai/bark) 相比:\n- ✔ **轻量**: 3️⃣ ✖ 更小,\n- ✔ **快速**: 4️⃣ ✖ 更快, \n- ✔ **中文&日文的更高质量**\n- ✔ **跨语言合成时没有外国口音**\n- ✔ **开放且易于操作的声音克隆**\n- ❌ **支持的语言较少**\n- ❌ **没有用于合成音乐及特殊音效的token**\n\n### 支持的语言\n\n| 语言      | 状态 |\n|---------| :---: |\n| 英语 (en) | ✅ |\n| 日语 (ja) | ✅ |\n| 中文 (zh) | ✅ |\n\n## ❓ FAQ\n\n#### 在哪里可以下载checkpoint?\n* 当您第一次运行程序时,我们使用`wget`将模型下载到`./checkpoints/`目录里。\n* 如果第一次运行时下载失败，请从[这里](https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt)手动下载模型，并将文件放在`./checkpoints/`里。\n\n#### 需要多少显存?\n* 6GB 显存(GPU VRAM) - 几乎所有NVIDIA GPU都满足要求.\n\n#### 为什么模型无法生成长文本?\n* 当序列长度增加时，Transformer的计算复杂度呈二次方增长。因此，所有训练音频都保持在22秒以下。请确保音频提示（audio prompt）和生成的音频的总长度小于22秒以确保可接受的性能。\n\n#### 更多...\n\n## 🧠 待办事项\n- [x] 添加中文 README\n- [x] 长文本生成\n- [x] 用Vocos解码器替换Encodec解码器\n- [ ] 微调以实现更好的语音自适应\n- [ ] 给非python用户的`.bat`脚本\n- [ ] 更多...\n\n## 🙏 感谢\n- [VALL-E X paper](https://arxiv.org/pdf/2303.03926) for the brilliant idea\n- [lifeiteng's vall-e](https://github.com/lifeiteng/vall-e) for related training code\n- [bark](https://github.com/suno-ai/bark) for the amazing pioneering work in neuro-codec TTS model\n\n## ⭐️ 表示出你的支持\n\n如果您觉得VALL-E X有趣且有用，请在GitHub上给我们一颗星！ ⭐️ 它鼓励我们不断改进模型并添加令人兴奋的功能。\n\n## 📜 License\n\nVALL-E X 使用 [MIT License](./LICENSE).\n\n---\n\n有问题或需要帮助？ 可以随便 [open an issue](https://github.com/Plachtaa/VALL-E-X/issues/new) 或加入我们的 [Discord](https://discord.gg/qCBRmAnTxg)\n\nHappy voice cloning! 🎤\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.1806640625,
          "content": "# VALL-E X: Multilingual Text-to-Speech Synthesis and Voice Cloning 🔊\n[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/qCBRmAnTxg)\n<br>\nEnglish | [中文](README-ZH.md)\n<br>\nAn open source implementation of Microsoft's [VALL-E X](https://arxiv.org/pdf/2303.03926) zero-shot TTS model.<br>\n**We release our trained model to the public for research or application usage.**\n\n![vallex-framework](/images/vallex_framework.jpg \"VALL-E X framework\")\n\nVALL-E X is an amazing multilingual text-to-speech (TTS) model proposed by Microsoft. While Microsoft initially publish in their research paper, they did not release any code or pretrained models. Recognizing the potential and value of this technology, our team took on the challenge to reproduce the results and train our own model. We are glad to share our trained VALL-E X model with the community, allowing everyone to experience the power next-generation TTS! 🎧\n<br>\n<br>\nMore details about the model are presented in [model card](./model-card.md).\n\n## 📖 Quick Index\n* [🚀 Updates](#-updates)\n* [📢 Features](#-features)\n* [💻 Installation](#-installation)\n* [🎧 Demos](#-demos)\n* [🐍 Usage](#-usage-in-python)\n* [❓ FAQ](#-faq)\n* [🧠 TODO](#-todo)\n\n## 🚀 Updates\n**2023.09.10**\n- Added AR decoder batch decoding for more stable generation result.\n\n**2023.08.30**\n- Replaced EnCodec decoder with Vocos decoder, improved audio quality. (Thanks to [@v0xie](https://github.com/v0xie))\n\n**2023.08.23**\n- Added long text generation.\n\n**2023.08.20**\n- Added [Chinese README](README-ZH.md).\n\n**2023.08.14**\n- Pretrained VALL-E X checkpoint is now released. Download it [here](https://drive.google.com/file/d/10gdQWvP-K_e1undkvv0p2b7SU6I4Egyl/view?usp=sharing)\n\n## 💻 Installation\n### Install with pip, Python 3.10, CUDA 11.7 ~ 12.0, PyTorch 2.0+\n```commandline\ngit clone https://github.com/Plachtaa/VALL-E-X.git\ncd VALL-E-X\npip install -r requirements.txt\n```\n\n> Note: If you want to make prompt, you need to install ffmpeg and add its folder to the environment variable PATH.\n\nWhen you run the program for the first time, it will automatically download the corresponding model. \n\nIf the download fails and reports an error, please follow the steps below to manually download the model.\n\n(Please pay attention to the capitalization of folders)\n\n1. Check whether there is a `checkpoints` folder in the installation directory. \nIf not, manually create a `checkpoints` folder (`./checkpoints/`) in the installation directory.\n\n2. Check whether there is a `vallex-checkpoint.pt` file in the `checkpoints` folder. \nIf not, please manually download the `vallex-checkpoint.pt` file from [here](https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt) and put it in the `checkpoints` folder.\n\n3. Check whether there is a `whisper` folder in the installation directory. \nIf not, manually create a `whisper` folder (`./whisper/`) in the installation directory.\n\n4. Check whether there is a `medium.pt` file in the `whisper` folder. \nIf not, please manually download the `medium.pt` file from [here](https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt) and put it in the `whisper` folder.\n\n##  🎧 Demos\nNot ready to set up the environment on your local machine just yet? No problem! We've got you covered with our online demos. You can try out VALL-E X directly on Hugging Face or Google Colab, experiencing the model's capabilities hassle-free!\n<br>\n[![Open in Spaces](https://img.shields.io/badge/🤗-Open%20in%20Spaces-blue.svg)](https://huggingface.co/spaces/Plachta/VALL-E-X)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1yyD_sz531QntLKowMHo-XxorsFBCfKul?usp=sharing)\n\n\n## 📢 Features\n\nVALL-E X comes packed with cutting-edge functionalities:\n\n1. **Multilingual TTS**: Speak in three languages - English, Chinese, and Japanese - with natural and expressive speech synthesis.\n\n2. **Zero-shot Voice Cloning**: Enroll a short 3~10 seconds recording of an unseen speaker, and watch VALL-E X create personalized, high-quality speech that sounds just like them!\n\n<details>\n  <summary><h5>see example</h5></summary>\n\n[prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/a7baa51d-a53a-41cc-a03d-6970f25fcca7)\n\n\n[output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/b895601a-d126-4138-beff-061aabdc7985)\n\n</details>\n\n3. **Speech Emotion Control**: Experience the power of emotions! VALL-E X can synthesize speech with the same emotion as the acoustic prompt provided, adding an extra layer of expressiveness to your audio.\n\n<details>\n  <summary><h5>see example</h5></summary>\n\nhttps://github.com/Plachtaa/VALL-E-X/assets/112609742/56fa9988-925e-4757-82c5-83ecb0df6266\n\n\nhttps://github.com/Plachtaa/VALL-E-X/assets/112609742/699c47a3-d502-4801-8364-bd89bcc0b8f1\n\n</details>\n\n4. **Zero-shot Cross-Lingual Speech Synthesis**: Take monolingual speakers on a linguistic journey! VALL-E X can produce personalized speech in another language without compromising on fluency or accent. Below is a Japanese speaker talk in Chinese & English. 🇯🇵 🗣\n\n<details>\n  <summary><h5>see example</h5></summary>\n\n[jp-prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/ea6e2ee4-139a-41b4-837e-0bd04dda6e19)\n\n\n[en-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/db8f9782-923f-425e-ba94-e8c1bd48f207)\n\n\n[zh-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/15829d79-e448-44d3-8965-fafa7a3f8c28)\n\n</details>\n\n5. **Accent Control**: Get creative with accents! VALL-E X allows you to experiment with different accents, like speaking Chinese with an English accent or vice versa. 🇨🇳 💬\n\n<details>\n  <summary><h5>see example</h5></summary>\n\n[en-prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/f688d7f6-70ef-46ec-b1cc-355c31e78b3b)\n\n\n[zh-accent-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/be59c7ca-b45b-44ca-a30d-4d800c950ccc)\n\n\n[en-accent-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/8b4f4f9b-f299-4ea4-a548-137437b71738)\n\n</details>\n\n6. **Acoustic Environment Maintenance**: No need for perfectly clean audio prompts! VALL-E X adapts to the acoustic environment of the input, making speech generation feel natural and immersive.\n\n<details>\n  <summary><h5>see example</h5></summary>\n\n[noise-prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/68986d88-abd0-4d1d-96e4-4f893eb9259e)\n\n\n[noise-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/96c4c612-4516-4683-8804-501b70938608)\n\n</details>\n\n\nExplore our [demo page](https://plachtaa.github.io/) for a lot more examples!\n\n## 🐍 Usage in Python\n\n<details open>\n  <summary><h3>🪑 Basics</h3></summary>\n\n```python\nfrom utils.generation import SAMPLE_RATE, generate_audio, preload_models\nfrom scipy.io.wavfile import write as write_wav\nfrom IPython.display import Audio\n\n# download and load all models\npreload_models()\n\n# generate audio from text\ntext_prompt = \"\"\"\nHello, my name is Nose. And uh, and I like hamburger. Hahaha... But I also have other interests such as playing tactic toast.\n\"\"\"\naudio_array = generate_audio(text_prompt)\n\n# save audio to disk\nwrite_wav(\"vallex_generation.wav\", SAMPLE_RATE, audio_array)\n\n# play text in notebook\nAudio(audio_array, rate=SAMPLE_RATE)\n```\n\n[hamburger.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/578d7bbe-cda9-483e-898c-29646edc8f2e)\n\n</details>\n\n<details open>\n  <summary><h3>🌎 Foreign Language</h3></summary>\n<br>\nThis VALL-E X implementation also supports Chinese and Japanese. All three languages have equally awesome performance!\n<br>\n\n```python\n\ntext_prompt = \"\"\"\n    チュソクは私のお気に入りの祭りです。 私は数日間休んで、友人や家族との時間を過ごすことができます。\n\"\"\"\naudio_array = generate_audio(text_prompt)\n```\n\n[vallex_japanese.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/ee57a688-3e83-4be5-b0fe-019d16eec51c)\n\n*Note: VALL-E X controls accent perfectly even when synthesizing code-switch text. However, you need to manually denote language of respective sentences (since our g2p tool is rule-base)*\n```python\ntext_prompt = \"\"\"\n    [EN]The Thirty Years' War was a devastating conflict that had a profound impact on Europe.[EN]\n    [ZH]这是历史的开始。 如果您想听更多，请继续。[ZH]\n\"\"\"\naudio_array = generate_audio(text_prompt, language='mix')\n```\n\n[vallex_codeswitch.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/d8667abf-bd08-499f-a383-a861d852f98a)\n\n</details>\n\n<details open>\n<summary><h3>📼 Voice Presets</h3></summary>\n  \nVALL-E X provides tens of speaker voices which you can directly used for inference! Browse all voices in the [code](/presets)\n\n> VALL-E X tries to match the tone, pitch, emotion and prosody of a given preset. The model also attempts to preserve music, ambient noise, etc.\n\n```python\ntext_prompt = \"\"\"\nI am an innocent boy with a smoky voice. It is a great honor for me to speak at the United Nations today.\n\"\"\"\naudio_array = generate_audio(text_prompt, prompt=\"dingzhen\")\n```\n\n[smoky.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/d3f55732-b1cd-420f-87d6-eab60db14dc5)\n\n</details>\n\n<details open>\n<summary><h3>🎙Voice Cloning</h3></summary>\n  \nVALL-E X supports voice cloning! You can make a voice prompt with any person, character or even your own voice, and use it like other voice presets.<br>\nTo make a voice prompt, you need to provide a speech of 3~10 seconds long, as well as the transcript of the speech. \nYou can also leave the transcript blank to let the [Whisper](https://github.com/openai/whisper) model to generate the transcript.\n> VALL-E X tries to match the tone, pitch, emotion and prosody of a given prompt. The model also attempts to preserve music, ambient noise, etc.\n\n```python\nfrom utils.prompt_making import make_prompt\n\n### Use given transcript\nmake_prompt(name=\"paimon\", audio_prompt_path=\"paimon_prompt.wav\",\n                transcript=\"Just, what was that? Paimon thought we were gonna get eaten.\")\n\n### Alternatively, use whisper\nmake_prompt(name=\"paimon\", audio_prompt_path=\"paimon_prompt.wav\")\n```\nNow let's try out the prompt we've just made!\n```python\nfrom utils.generation import SAMPLE_RATE, generate_audio, preload_models\nfrom scipy.io.wavfile import write as write_wav\n\n# download and load all models\npreload_models()\n\ntext_prompt = \"\"\"\nHey, Traveler, Listen to this, This machine has taken my voice, and now it can talk just like me!\n\"\"\"\naudio_array = generate_audio(text_prompt, prompt=\"paimon\")\n\nwrite_wav(\"paimon_cloned.wav\", SAMPLE_RATE, audio_array)\n\n```\n\n[paimon_prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/e7922859-9d12-4e2a-8651-e156e4280311)\n\n\n[paimon_cloned.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/60d3b7e9-5ead-4024-b499-a897ce5f3d5e)\n\n\n</details>\n\n\n<details open>\n<summary><h3>🎢User Interface</h3></summary>\n\nNot comfortable with codes? No problem! We've also created a user-friendly graphical interface for VALL-E X. It allows you to interact with the model effortlessly, making voice cloning and multilingual speech synthesis a breeze.\n<br>\nYou can launch the UI by the following command:\n```commandline\npython -X utf8 launch-ui.py\n```\n</details>\n\n## 🛠️ Hardware and Inference Speed\n\nVALL-E X works well on both CPU and GPU (`pytorch 2.0+`, CUDA 11.7 and CUDA 12.0).\n\nA GPU VRAM of 6GB is enough for running VALL-E X without offloading.\n\n## ⚙️ Details\n\nVALL-E X is similar to [Bark](https://github.com/suno-ai/bark), [VALL-E](https://arxiv.org/abs/2301.02111) and [AudioLM](https://arxiv.org/abs/2209.03143), which generates audio in GPT-style by predicting audio tokens quantized by [EnCodec](https://github.com/facebookresearch/encodec).\n<br>\nComparing to [Bark](https://github.com/suno-ai/bark):\n- ✔ **Light-weighted**: 3️⃣ ✖ smaller,\n- ✔ **Efficient**: 4️⃣ ✖ faster, \n- ✔ **Better quality on Chinese & Japanese**\n- ✔ **Cross-lingual speech without foreign accent**\n- ✔ **Easy voice-cloning**\n- ❌ **Less languages**\n- ❌ **No special tokens for music / sound effects**\n\n### Supported Languages\n\n| Language | Status |\n| --- | :---: |\n| English (en) | ✅ |\n| Japanese (ja) | ✅ |\n| Chinese, simplified (zh) | ✅ |\n\n## ❓ FAQ\n\n#### Where is code for training?\n* [lifeiteng's vall-e](https://github.com/lifeiteng/vall-e) has almost everything. There is no plan to release our training code because there is no difference between lifeiteng's implementation.\n\n#### Where can I download the model checkpoint?\n* We use `wget` to download the model to directory `./checkpoints/` when you run the program for the first time.\n* If the download fails on the first run, please manually download from [this link](https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt), and put the file under directory `./checkpoints/`.\n\n#### How much VRAM do I need?\n* 6GB GPU VRAM - Almost all NVIDIA GPUs satisfy the requirement.\n\n#### Why the model fails to generate long text?\n* Transformer's computation complexity increases quadratically while the sequence length increases. Hence, all training \nare kept under 22 seconds. Please make sure the total length of audio prompt and generated audio is less than 22 seconds \nto ensure acceptable performance. \n\n\n#### MORE TO BE ADDED...\n\n## 🧠 TODO\n- [x] Add Chinese README\n- [x] Long text generation\n- [x] Replace Encodec decoder with Vocos decoder\n- [ ] Fine-tuning for better voice adaptation\n- [ ] `.bat` scripts for non-python users\n- [ ] To be added...\n\n## 🙏 Appreciation\n- [VALL-E X paper](https://arxiv.org/pdf/2303.03926) for the brilliant idea\n- [lifeiteng's vall-e](https://github.com/lifeiteng/vall-e) for related training code\n- [bark](https://github.com/suno-ai/bark) for the amazing pioneering work in neuro-codec TTS model\n\n## ⭐️ Show Your Support\n\nIf you find VALL-E X interesting and useful, give us a star on GitHub! ⭐️ It encourages us to keep improving the model and adding exciting features.\n\n## 📜 License\n\nVALL-E X is licensed under the [MIT License](./LICENSE).\n\n---\n\nHave questions or need assistance? Feel free to [open an issue](https://github.com/Plachtaa/VALL-E-X/issues/new) or join our [Discord](https://discord.gg/qCBRmAnTxg)\n\nHappy voice cloning! 🎤\n"
        },
        {
          "name": "customs",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "descriptions.py",
          "type": "blob",
          "size": 2.25390625,
          "content": "top_md = \"\"\"\n# VALL-E X  \nVALL-E X can synthesize high-quality personalized speech with only a 3-second enrolled recording of \nan unseen speaker as an acoustic prompt, even in another language for a monolingual speaker.<br>\nThis implementation supports zero-shot, mono-lingual/cross-lingual text-to-speech functionality of three languages (English, Chinese, Japanese)<br>  \nSee this [demo](https://plachtaa.github.io/) page for more details.\n\"\"\"\n\ninfer_from_audio_md = \"\"\"\nUpload a speech of 3~10 seconds as the audio prompt and type in the text you'd like to synthesize.<br>\nThe model will synthesize speech of given text with the same voice of your audio prompt.<br>\nThe model also tends to preserve the emotion & acoustic environment of your given speech.<br>\nFor faster inference, please use **\"Make prompt\"** to get a `.npz` file as the encoded audio prompt, and use it by **\"Infer from prompt\"**\n\"\"\"\n\nmake_prompt_md = \"\"\"\nUpload a speech of 3~10 seconds as the audio prompt.<br>\nGet a `.npz` file as the encoded audio prompt. Use it by **\"Infer with prompt\"**\n\"\"\"\n\ninfer_from_prompt_md = \"\"\"\nFaster than **\"Infer from audio\"**.<br>\nYou need to **\"Make prompt\"** first, and upload the encoded prompt (a `.npz` file)\n\"\"\"\n\nlong_text_md = \"\"\"\nVery long text is chunked into several sentences, and each sentence is synthesized separately.<br>\nPlease make a prompt or use a preset prompt to infer long text.\n\"\"\"\n\nlong_text_example = \"Just a few years ago, there were no legions of deep learning scientists developing intelligent products and services at major companies and startups. When we entered the field, machine learning did not command headlines in daily newspapers. Our parents had no idea what machine learning was, let alone why we might prefer it to a career in medicine or law. Machine learning was a blue skies academic discipline whose industrial significance was limited to a narrow set of real-world applications, including speech recognition and computer vision. Moreover, many of these applications required so much domain knowledge that they were often regarded as entirely separate areas for which machine learning was one small component. At that time, neural networks—the predecessors of the deep learning methods that we focus on in this book—were generally regarded as outmoded.\""
        },
        {
          "name": "examples.py",
          "type": "blob",
          "size": 1.5927734375,
          "content": "infer_from_audio_examples = [\n    [\"This is how this machine has taken my voice.\", 'English', 'no-accent', \"prompts/en-2.wav\", None, \"Wow, look at that! That's no ordinary Teddy bear!\"],\n    [\"我喜欢抽电子烟，尤其是锐刻五代。\", '中文', 'no-accent', \"prompts/zh-1.wav\", None, \"今天我很荣幸，\"],\n    [\"私の声を真似するのはそんなに面白いですか？\", '日本語', 'no-accent', \"prompts/ja-2.ogg\", None, \"初めまして、朝武よしのです。\"],\n    [\"你可以听得出来我有多困。\", '中文', 'no-accent', \"prompts/en-1.wav\", None, \"\"],\n    [\"この文は、クロスリンガル合成の例です。\", '日本語', 'no-accent', \"prompts/zh-2.wav\", None, \"\"],\n    [\"Actually, I can't speak English, but this machine helped me do it.\", 'English', 'no-accent', \"prompts/ja-1.wav\", None, \"\"],\n]\n\nmake_npz_prompt_examples = [\n    [\"Gem-trader\", \"prompts/en-2.wav\", None, \"Wow, look at that! That's no ordinary Teddy bear!\"],\n    [\"Ding Zhen\", \"prompts/zh-1.wav\", None, \"今天我很荣幸，\"],\n    [\"Yoshino\", \"prompts/ja-2.ogg\", None, \"初めまして、朝武よしのです。\"],\n    [\"Sleepy-woman\", \"prompts/en-1.wav\", None, \"\"],\n    [\"Yae\", \"prompts/zh-2.wav\", None, \"\"],\n    [\"Cafe\", \"prompts/ja-1.wav\", None, \"\"],\n]\n\ninfer_from_prompt_examples = [\n    [\"A prompt contains voice, prosody and emotion information of a certain speaker.\", \"English\", \"no-accent\", \"vctk_1\", None],\n    [\"This prompt is made with an audio of three seconds.\", \"English\", \"no-accent\", \"librispeech_1\", None],\n    [\"This prompt is made with Chinese speech\", \"English\", \"no-accent\", \"seel\", None],\n]\n\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "launch-ui.py",
          "type": "blob",
          "size": 26.5439453125,
          "content": "# coding: utf-8\nimport argparse\nimport logging\nimport os\nimport pathlib\nimport time\nimport tempfile\nimport platform\nimport webbrowser\nimport sys\nprint(f\"default encoding is {sys.getdefaultencoding()},file system encoding is {sys.getfilesystemencoding()}\")\nprint(f\"You are using Python version {platform.python_version()}\")\nif(sys.version_info[0]<3 or sys.version_info[1]<7):\n    print(\"The Python version is too low and may cause problems\")\n\nif platform.system().lower() == 'windows':\n    temp = pathlib.PosixPath\n    pathlib.PosixPath = pathlib.WindowsPath\nelse:\n    temp = pathlib.WindowsPath\n    pathlib.WindowsPath = pathlib.PosixPath\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n\nimport langid\nlangid.set_languages(['en', 'zh', 'ja'])\n\nimport nltk\nnltk.data.path = nltk.data.path + [os.path.join(os.getcwd(), \"nltk_data\")]\n\nimport torch\nimport torchaudio\nimport random\n\nimport numpy as np\n\nfrom data.tokenizer import (\n    AudioTokenizer,\n    tokenize_audio,\n)\nfrom data.collation import get_text_token_collater\nfrom models.vallex import VALLE\nfrom utils.g2p import PhonemeBpeTokenizer\nfrom descriptions import *\nfrom macros import *\nfrom examples import *\n\nimport gradio as gr\nimport whisper\nfrom vocos import Vocos\nimport multiprocessing\n\nthread_count = multiprocessing.cpu_count()\n\nprint(\"Use\",thread_count,\"cpu cores for computing\")\n\ntorch.set_num_threads(thread_count)\ntorch.set_num_interop_threads(thread_count)\ntorch._C._jit_set_profiling_executor(False)\ntorch._C._jit_set_profiling_mode(False)\ntorch._C._set_graph_executor_optimize(False)\n\ntext_tokenizer = PhonemeBpeTokenizer(tokenizer_path=\"./utils/g2p/bpe_69.json\")\ntext_collater = get_text_token_collater()\n\ndevice = torch.device(\"cpu\")\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\", 0)\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n# VALL-E-X model\nif not os.path.exists(\"./checkpoints/\"): os.mkdir(\"./checkpoints/\")\nif not os.path.exists(os.path.join(\"./checkpoints/\", \"vallex-checkpoint.pt\")):\n    import wget\n    try:\n        logging.info(\"Downloading model from https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt ...\")\n        # download from https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt to ./checkpoints/vallex-checkpoint.pt\n        wget.download(\"https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt\",\n                      out=\"./checkpoints/vallex-checkpoint.pt\", bar=wget.bar_adaptive)\n    except Exception as e:\n        logging.info(e)\n        raise Exception(\n            \"\\n Model weights download failed, please go to 'https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt'\"\n            \"\\n manually download model weights and put it to {} .\".format(os.getcwd() + \"\\checkpoints\"))\n\nmodel = VALLE(\n        N_DIM,\n        NUM_HEAD,\n        NUM_LAYERS,\n        norm_first=True,\n        add_prenet=False,\n        prefix_mode=PREFIX_MODE,\n        share_embedding=True,\n        nar_scale_factor=1.0,\n        prepend_bos=True,\n        num_quantizers=NUM_QUANTIZERS,\n    )\ncheckpoint = torch.load(\"./checkpoints/vallex-checkpoint.pt\", map_location='cpu')\nmissing_keys, unexpected_keys = model.load_state_dict(\n    checkpoint[\"model\"], strict=True\n)\nassert not missing_keys\nmodel.eval()\n\n# Encodec model\naudio_tokenizer = AudioTokenizer(device)\n\n# Vocos decoder\nvocos = Vocos.from_pretrained('charactr/vocos-encodec-24khz').to(device)\n\n# ASR\nif not os.path.exists(\"./whisper/\"): os.mkdir(\"./whisper/\")\ntry:\n    whisper_model = whisper.load_model(\"medium\",download_root=os.path.join(os.getcwd(), \"whisper\")).cpu()\nexcept Exception as e:\n    logging.info(e)\n    raise Exception(\n        \"\\n Whisper download failed or damaged, please go to \"\n        \"'https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt'\"\n        \"\\n manually download model and put it to {} .\".format(os.getcwd() + \"\\whisper\"))\n\n# Voice Presets\npreset_list = os.walk(\"./presets/\").__next__()[2]\npreset_list = [preset[:-4] for preset in preset_list if preset.endswith(\".npz\")]\n\ndef clear_prompts():\n    try:\n        path = tempfile.gettempdir()\n        for eachfile in os.listdir(path):\n            filename = os.path.join(path, eachfile)\n            if os.path.isfile(filename) and filename.endswith(\".npz\"):\n                lastmodifytime = os.stat(filename).st_mtime\n                endfiletime = time.time() - 60\n                if endfiletime > lastmodifytime:\n                    os.remove(filename)\n    except:\n        return\n\ndef transcribe_one(model, audio_path):\n    # load audio and pad/trim it to fit 30 seconds\n    audio = whisper.load_audio(audio_path)\n    audio = whisper.pad_or_trim(audio)\n\n    # make log-Mel spectrogram and move to the same device as the model\n    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n\n    # detect the spoken language\n    _, probs = model.detect_language(mel)\n    print(f\"Detected language: {max(probs, key=probs.get)}\")\n    lang = max(probs, key=probs.get)\n    # decode the audio\n    options = whisper.DecodingOptions(temperature=1.0, best_of=5, fp16=False if device == torch.device(\"cpu\") else True, sample_len=150)\n    result = whisper.decode(model, mel, options)\n\n    # print the recognized text\n    print(result.text)\n\n    text_pr = result.text\n    if text_pr.strip(\" \")[-1] not in \"?!.,。，？！。、\":\n        text_pr += \".\"\n    return lang, text_pr\n\ndef make_npz_prompt(name, uploaded_audio, recorded_audio, transcript_content):\n    global model, text_collater, text_tokenizer, audio_tokenizer\n    clear_prompts()\n    audio_prompt = uploaded_audio if uploaded_audio is not None else recorded_audio\n    sr, wav_pr = audio_prompt\n    if not isinstance(wav_pr, torch.FloatTensor):\n        wav_pr = torch.FloatTensor(wav_pr)\n    if wav_pr.abs().max() > 1:\n        wav_pr /= wav_pr.abs().max()\n    if wav_pr.size(-1) == 2:\n        wav_pr = wav_pr[:, 0]\n    if wav_pr.ndim == 1:\n        wav_pr = wav_pr.unsqueeze(0)\n    assert wav_pr.ndim and wav_pr.size(0) == 1\n\n    if transcript_content == \"\":\n        text_pr, lang_pr = make_prompt(name, wav_pr, sr, save=False)\n    else:\n        lang_pr = langid.classify(str(transcript_content))[0]\n        lang_token = lang2token[lang_pr]\n        text_pr = f\"{lang_token}{str(transcript_content)}{lang_token}\"\n    # tokenize audio\n    encoded_frames = tokenize_audio(audio_tokenizer, (wav_pr, sr))\n    audio_tokens = encoded_frames[0][0].transpose(2, 1).cpu().numpy()\n\n    # tokenize text\n    phonemes, _ = text_tokenizer.tokenize(text=f\"{text_pr}\".strip())\n    text_tokens, enroll_x_lens = text_collater(\n        [\n            phonemes\n        ]\n    )\n\n    message = f\"Detected language: {lang_pr}\\n Detected text {text_pr}\\n\"\n\n    # save as npz file\n    np.savez(os.path.join(tempfile.gettempdir(), f\"{name}.npz\"),\n             audio_tokens=audio_tokens, text_tokens=text_tokens, lang_code=lang2code[lang_pr])\n    return message, os.path.join(tempfile.gettempdir(), f\"{name}.npz\")\n\n\ndef make_prompt(name, wav, sr, save=True):\n    global whisper_model\n    whisper_model.to(device)\n    if not isinstance(wav, torch.FloatTensor):\n        wav = torch.tensor(wav)\n    if wav.abs().max() > 1:\n        wav /= wav.abs().max()\n    if wav.size(-1) == 2:\n        wav = wav.mean(-1, keepdim=False)\n    if wav.ndim == 1:\n        wav = wav.unsqueeze(0)\n    assert wav.ndim and wav.size(0) == 1\n    torchaudio.save(f\"./prompts/{name}.wav\", wav, sr)\n    lang, text = transcribe_one(whisper_model, f\"./prompts/{name}.wav\")\n    lang_token = lang2token[lang]\n    text = lang_token + text + lang_token\n    with open(f\"./prompts/{name}.txt\", 'w', encoding='utf-8') as f:\n        f.write(text)\n    if not save:\n        os.remove(f\"./prompts/{name}.wav\")\n        os.remove(f\"./prompts/{name}.txt\")\n\n    whisper_model.cpu()\n    torch.cuda.empty_cache()\n    return text, lang\n\n@torch.no_grad()\ndef infer_from_audio(text, language, accent, audio_prompt, record_audio_prompt, transcript_content):\n    global model, text_collater, text_tokenizer, audio_tokenizer\n    audio_prompt = audio_prompt if audio_prompt is not None else record_audio_prompt\n    sr, wav_pr = audio_prompt\n    if not isinstance(wav_pr, torch.FloatTensor):\n        wav_pr = torch.FloatTensor(wav_pr)\n    if wav_pr.abs().max() > 1:\n        wav_pr /= wav_pr.abs().max()\n    if wav_pr.size(-1) == 2:\n        wav_pr = wav_pr[:, 0]\n    if wav_pr.ndim == 1:\n        wav_pr = wav_pr.unsqueeze(0)\n    assert wav_pr.ndim and wav_pr.size(0) == 1\n\n    if transcript_content == \"\":\n        text_pr, lang_pr = make_prompt('dummy', wav_pr, sr, save=False)\n    else:\n        lang_pr = langid.classify(str(transcript_content))[0]\n        lang_token = lang2token[lang_pr]\n        text_pr = f\"{lang_token}{str(transcript_content)}{lang_token}\"\n\n    if language == 'auto-detect':\n        lang_token = lang2token[langid.classify(text)[0]]\n    else:\n        lang_token = langdropdown2token[language]\n    lang = token2lang[lang_token]\n    text = lang_token + text + lang_token\n\n    # onload model\n    model.to(device)\n\n    # tokenize audio\n    encoded_frames = tokenize_audio(audio_tokenizer, (wav_pr, sr))\n    audio_prompts = encoded_frames[0][0].transpose(2, 1).to(device)\n\n    # tokenize text\n    logging.info(f\"synthesize text: {text}\")\n    phone_tokens, langs = text_tokenizer.tokenize(text=f\"_{text}\".strip())\n    text_tokens, text_tokens_lens = text_collater(\n        [\n            phone_tokens\n        ]\n    )\n\n    enroll_x_lens = None\n    if text_pr:\n        text_prompts, _ = text_tokenizer.tokenize(text=f\"{text_pr}\".strip())\n        text_prompts, enroll_x_lens = text_collater(\n            [\n                text_prompts\n            ]\n        )\n    text_tokens = torch.cat([text_prompts, text_tokens], dim=-1)\n    text_tokens_lens += enroll_x_lens\n    lang = lang if accent == \"no-accent\" else token2lang[langdropdown2token[accent]]\n    encoded_frames = model.inference(\n        text_tokens.to(device),\n        text_tokens_lens.to(device),\n        audio_prompts,\n        enroll_x_lens=enroll_x_lens,\n        top_k=-100,\n        temperature=1,\n        prompt_language=lang_pr,\n        text_language=langs if accent == \"no-accent\" else lang,\n        best_of=5,\n    )\n    # Decode with Vocos\n    frames = encoded_frames.permute(2,0,1)\n    features = vocos.codes_to_features(frames)\n    samples = vocos.decode(features, bandwidth_id=torch.tensor([2], device=device))\n\n    # offload model\n    model.to('cpu')\n    torch.cuda.empty_cache()\n\n    message = f\"text prompt: {text_pr}\\nsythesized text: {text}\"\n    return message, (24000, samples.squeeze(0).cpu().numpy())\n\n@torch.no_grad()\ndef infer_from_prompt(text, language, accent, preset_prompt, prompt_file):\n    clear_prompts()\n    model.to(device)\n    # text to synthesize\n    if language == 'auto-detect':\n        lang_token = lang2token[langid.classify(text)[0]]\n    else:\n        lang_token = langdropdown2token[language]\n    lang = token2lang[lang_token]\n    text = lang_token + text + lang_token\n\n    # load prompt\n    if prompt_file is not None:\n        prompt_data = np.load(prompt_file.name)\n    else:\n        prompt_data = np.load(os.path.join(\"./presets/\", f\"{preset_prompt}.npz\"))\n    audio_prompts = prompt_data['audio_tokens']\n    text_prompts = prompt_data['text_tokens']\n    lang_pr = prompt_data['lang_code']\n    lang_pr = code2lang[int(lang_pr)]\n\n    # numpy to tensor\n    audio_prompts = torch.tensor(audio_prompts).type(torch.int32).to(device)\n    text_prompts = torch.tensor(text_prompts).type(torch.int32)\n\n    enroll_x_lens = text_prompts.shape[-1]\n    logging.info(f\"synthesize text: {text}\")\n    phone_tokens, langs = text_tokenizer.tokenize(text=f\"_{text}\".strip())\n    text_tokens, text_tokens_lens = text_collater(\n        [\n            phone_tokens\n        ]\n    )\n    text_tokens = torch.cat([text_prompts, text_tokens], dim=-1)\n    text_tokens_lens += enroll_x_lens\n    # accent control\n    lang = lang if accent == \"no-accent\" else token2lang[langdropdown2token[accent]]\n    encoded_frames = model.inference(\n        text_tokens.to(device),\n        text_tokens_lens.to(device),\n        audio_prompts,\n        enroll_x_lens=enroll_x_lens,\n        top_k=-100,\n        temperature=1,\n        prompt_language=lang_pr,\n        text_language=langs if accent == \"no-accent\" else lang,\n        best_of=5,\n    )\n    # Decode with Vocos\n    frames = encoded_frames.permute(2,0,1)\n    features = vocos.codes_to_features(frames)\n    samples = vocos.decode(features, bandwidth_id=torch.tensor([2], device=device))\n\n    model.to('cpu')\n    torch.cuda.empty_cache()\n\n    message = f\"sythesized text: {text}\"\n    return message, (24000, samples.squeeze(0).cpu().numpy())\n\n\nfrom utils.sentence_cutter import split_text_into_sentences\n@torch.no_grad()\ndef infer_long_text(text, preset_prompt, prompt=None, language='auto', accent='no-accent'):\n    \"\"\"\n    For long audio generation, two modes are available.\n    fixed-prompt: This mode will keep using the same prompt the user has provided, and generate audio sentence by sentence.\n    sliding-window: This mode will use the last sentence as the prompt for the next sentence, but has some concern on speaker maintenance.\n    \"\"\"\n    mode = 'fixed-prompt'\n    global model, audio_tokenizer, text_tokenizer, text_collater\n    model.to(device)\n    if (prompt is None or prompt == \"\") and preset_prompt == \"\":\n        mode = 'sliding-window'  # If no prompt is given, use sliding-window mode\n    sentences = split_text_into_sentences(text)\n    # detect language\n    if language == \"auto-detect\":\n        language = langid.classify(text)[0]\n    else:\n        language = token2lang[langdropdown2token[language]]\n\n    # if initial prompt is given, encode it\n    if prompt is not None and prompt != \"\":\n        # load prompt\n        prompt_data = np.load(prompt.name)\n        audio_prompts = prompt_data['audio_tokens']\n        text_prompts = prompt_data['text_tokens']\n        lang_pr = prompt_data['lang_code']\n        lang_pr = code2lang[int(lang_pr)]\n\n        # numpy to tensor\n        audio_prompts = torch.tensor(audio_prompts).type(torch.int32).to(device)\n        text_prompts = torch.tensor(text_prompts).type(torch.int32)\n    elif preset_prompt is not None and preset_prompt != \"\":\n        prompt_data = np.load(os.path.join(\"./presets/\", f\"{preset_prompt}.npz\"))\n        audio_prompts = prompt_data['audio_tokens']\n        text_prompts = prompt_data['text_tokens']\n        lang_pr = prompt_data['lang_code']\n        lang_pr = code2lang[int(lang_pr)]\n\n        # numpy to tensor\n        audio_prompts = torch.tensor(audio_prompts).type(torch.int32).to(device)\n        text_prompts = torch.tensor(text_prompts).type(torch.int32)\n    else:\n        audio_prompts = torch.zeros([1, 0, NUM_QUANTIZERS]).type(torch.int32).to(device)\n        text_prompts = torch.zeros([1, 0]).type(torch.int32)\n        lang_pr = language if language != 'mix' else 'en'\n    if mode == 'fixed-prompt':\n        complete_tokens = torch.zeros([1, NUM_QUANTIZERS, 0]).type(torch.LongTensor).to(device)\n        for text in sentences:\n            text = text.replace(\"\\n\", \"\").strip(\" \")\n            if text == \"\":\n                continue\n            lang_token = lang2token[language]\n            lang = token2lang[lang_token]\n            text = lang_token + text + lang_token\n\n            enroll_x_lens = text_prompts.shape[-1]\n            logging.info(f\"synthesize text: {text}\")\n            phone_tokens, langs = text_tokenizer.tokenize(text=f\"_{text}\".strip())\n            text_tokens, text_tokens_lens = text_collater(\n                [\n                    phone_tokens\n                ]\n            )\n            text_tokens = torch.cat([text_prompts, text_tokens], dim=-1)\n            text_tokens_lens += enroll_x_lens\n            # accent control\n            lang = lang if accent == \"no-accent\" else token2lang[langdropdown2token[accent]]\n            encoded_frames = model.inference(\n                text_tokens.to(device),\n                text_tokens_lens.to(device),\n                audio_prompts,\n                enroll_x_lens=enroll_x_lens,\n                top_k=-100,\n                temperature=1,\n                prompt_language=lang_pr,\n                text_language=langs if accent == \"no-accent\" else lang,\n                best_of=5,\n            )\n            complete_tokens = torch.cat([complete_tokens, encoded_frames.transpose(2, 1)], dim=-1)\n        # Decode with Vocos\n        frames = complete_tokens.permute(1, 0, 2)\n        features = vocos.codes_to_features(frames)\n        samples = vocos.decode(features, bandwidth_id=torch.tensor([2], device=device))\n\n        model.to('cpu')\n        message = f\"Cut into {len(sentences)} sentences\"\n        return message, (24000, samples.squeeze(0).cpu().numpy())\n    elif mode == \"sliding-window\":\n        complete_tokens = torch.zeros([1, NUM_QUANTIZERS, 0]).type(torch.LongTensor).to(device)\n        original_audio_prompts = audio_prompts\n        original_text_prompts = text_prompts\n        for text in sentences:\n            text = text.replace(\"\\n\", \"\").strip(\" \")\n            if text == \"\":\n                continue\n            lang_token = lang2token[language]\n            lang = token2lang[lang_token]\n            text = lang_token + text + lang_token\n\n            enroll_x_lens = text_prompts.shape[-1]\n            logging.info(f\"synthesize text: {text}\")\n            phone_tokens, langs = text_tokenizer.tokenize(text=f\"_{text}\".strip())\n            text_tokens, text_tokens_lens = text_collater(\n                [\n                    phone_tokens\n                ]\n            )\n            text_tokens = torch.cat([text_prompts, text_tokens], dim=-1)\n            text_tokens_lens += enroll_x_lens\n            # accent control\n            lang = lang if accent == \"no-accent\" else token2lang[langdropdown2token[accent]]\n            encoded_frames = model.inference(\n                text_tokens.to(device),\n                text_tokens_lens.to(device),\n                audio_prompts,\n                enroll_x_lens=enroll_x_lens,\n                top_k=-100,\n                temperature=1,\n                prompt_language=lang_pr,\n                text_language=langs if accent == \"no-accent\" else lang,\n                best_of=5,\n            )\n            complete_tokens = torch.cat([complete_tokens, encoded_frames.transpose(2, 1)], dim=-1)\n            if torch.rand(1) < 1.0:\n                audio_prompts = encoded_frames[:, :, -NUM_QUANTIZERS:]\n                text_prompts = text_tokens[:, enroll_x_lens:]\n            else:\n                audio_prompts = original_audio_prompts\n                text_prompts = original_text_prompts\n        # Decode with Vocos\n        frames = complete_tokens.permute(1, 0, 2)\n        features = vocos.codes_to_features(frames)\n        samples = vocos.decode(features, bandwidth_id=torch.tensor([2], device=device))\n\n        model.to('cpu')\n        message = f\"Cut into {len(sentences)} sentences\"\n        return message, (24000, samples.squeeze(0).cpu().numpy())\n    else:\n        raise ValueError(f\"No such mode {mode}\")\n\n\ndef main():\n    app = gr.Blocks(title=\"VALL-E X\")\n    with app:\n        gr.Markdown(top_md)\n        with gr.Tab(\"Infer from audio\"):\n            gr.Markdown(infer_from_audio_md)\n            with gr.Row():\n                with gr.Column():\n\n                    textbox = gr.TextArea(label=\"Text\",\n                                          placeholder=\"Type your sentence here\",\n                                          value=\"Welcome back, Master. What can I do for you today?\", elem_id=f\"tts-input\")\n                    language_dropdown = gr.Dropdown(choices=['auto-detect', 'English', '中文', '日本語'], value='auto-detect', label='language')\n                    accent_dropdown = gr.Dropdown(choices=['no-accent', 'English', '中文', '日本語'], value='no-accent', label='accent')\n                    textbox_transcript = gr.TextArea(label=\"Transcript\",\n                                          placeholder=\"Write transcript here. (leave empty to use whisper)\",\n                                          value=\"\", elem_id=f\"prompt-name\")\n                    upload_audio_prompt = gr.Audio(label='uploaded audio prompt', source='upload', interactive=True)\n                    record_audio_prompt = gr.Audio(label='recorded audio prompt', source='microphone', interactive=True)\n                with gr.Column():\n                    text_output = gr.Textbox(label=\"Message\")\n                    audio_output = gr.Audio(label=\"Output Audio\", elem_id=\"tts-audio\")\n                    btn = gr.Button(\"Generate!\")\n                    btn.click(infer_from_audio,\n                              inputs=[textbox, language_dropdown, accent_dropdown, upload_audio_prompt, record_audio_prompt, textbox_transcript],\n                              outputs=[text_output, audio_output])\n                    textbox_mp = gr.TextArea(label=\"Prompt name\",\n                                          placeholder=\"Name your prompt here\",\n                                          value=\"prompt_1\", elem_id=f\"prompt-name\")\n                    btn_mp = gr.Button(\"Make prompt!\")\n                    prompt_output = gr.File(interactive=False)\n                    btn_mp.click(make_npz_prompt,\n                                inputs=[textbox_mp, upload_audio_prompt, record_audio_prompt, textbox_transcript],\n                                outputs=[text_output, prompt_output])\n            gr.Examples(examples=infer_from_audio_examples,\n                        inputs=[textbox, language_dropdown, accent_dropdown, upload_audio_prompt, record_audio_prompt, textbox_transcript],\n                        outputs=[text_output, audio_output],\n                        fn=infer_from_audio,\n                        cache_examples=False,)\n        with gr.Tab(\"Make prompt\"):\n            gr.Markdown(make_prompt_md)\n            with gr.Row():\n                with gr.Column():\n                    textbox2 = gr.TextArea(label=\"Prompt name\",\n                                          placeholder=\"Name your prompt here\",\n                                          value=\"prompt_1\", elem_id=f\"prompt-name\")\n                    # 添加选择语言和输入台本的地方\n                    textbox_transcript2 = gr.TextArea(label=\"Transcript\",\n                                          placeholder=\"Write transcript here. (leave empty to use whisper)\",\n                                          value=\"\", elem_id=f\"prompt-name\")\n                    upload_audio_prompt_2 = gr.Audio(label='uploaded audio prompt', source='upload', interactive=True)\n                    record_audio_prompt_2 = gr.Audio(label='recorded audio prompt', source='microphone', interactive=True)\n                with gr.Column():\n                    text_output_2 = gr.Textbox(label=\"Message\")\n                    prompt_output_2 = gr.File(interactive=False)\n                    btn_2 = gr.Button(\"Make!\")\n                    btn_2.click(make_npz_prompt,\n                              inputs=[textbox2, upload_audio_prompt_2, record_audio_prompt_2, textbox_transcript2],\n                              outputs=[text_output_2, prompt_output_2])\n            gr.Examples(examples=make_npz_prompt_examples,\n                        inputs=[textbox2, upload_audio_prompt_2, record_audio_prompt_2, textbox_transcript2],\n                        outputs=[text_output_2, prompt_output_2],\n                        fn=make_npz_prompt,\n                        cache_examples=False,)\n        with gr.Tab(\"Infer from prompt\"):\n            gr.Markdown(infer_from_prompt_md)\n            with gr.Row():\n                with gr.Column():\n                    textbox_3 = gr.TextArea(label=\"Text\",\n                                          placeholder=\"Type your sentence here\",\n                                          value=\"Welcome back, Master. What can I do for you today?\", elem_id=f\"tts-input\")\n                    language_dropdown_3 = gr.Dropdown(choices=['auto-detect', 'English', '中文', '日本語', 'Mix'], value='auto-detect',\n                                                    label='language')\n                    accent_dropdown_3 = gr.Dropdown(choices=['no-accent', 'English', '中文', '日本語'], value='no-accent',\n                                                  label='accent')\n                    preset_dropdown_3 = gr.Dropdown(choices=preset_list, value=None, label='Voice preset')\n                    prompt_file = gr.File(file_count='single', file_types=['.npz'], interactive=True)\n                with gr.Column():\n                    text_output_3 = gr.Textbox(label=\"Message\")\n                    audio_output_3 = gr.Audio(label=\"Output Audio\", elem_id=\"tts-audio\")\n                    btn_3 = gr.Button(\"Generate!\")\n                    btn_3.click(infer_from_prompt,\n                              inputs=[textbox_3, language_dropdown_3, accent_dropdown_3, preset_dropdown_3, prompt_file],\n                              outputs=[text_output_3, audio_output_3])\n            gr.Examples(examples=infer_from_prompt_examples,\n                        inputs=[textbox_3, language_dropdown_3, accent_dropdown_3, preset_dropdown_3, prompt_file],\n                        outputs=[text_output_3, audio_output_3],\n                        fn=infer_from_prompt,\n                        cache_examples=False,)\n        with gr.Tab(\"Infer long text\"):\n            gr.Markdown(\"This is a long text generation demo. You can use this to generate long audio. \")\n            with gr.Row():\n                with gr.Column():\n                    textbox_4 = gr.TextArea(label=\"Text\",\n                                          placeholder=\"Type your sentence here\",\n                                          value=long_text_example, elem_id=f\"tts-input\")\n                    language_dropdown_4 = gr.Dropdown(choices=['auto-detect', 'English', '中文', '日本語'], value='auto-detect',\n                                                    label='language')\n                    accent_dropdown_4 = gr.Dropdown(choices=['no-accent', 'English', '中文', '日本語'], value='no-accent',\n                                                    label='accent')\n                    preset_dropdown_4 = gr.Dropdown(choices=preset_list, value=None, label='Voice preset')\n                    prompt_file_4 = gr.File(file_count='single', file_types=['.npz'], interactive=True)\n                with gr.Column():\n                    text_output_4 = gr.TextArea(label=\"Message\")\n                    audio_output_4 = gr.Audio(label=\"Output Audio\", elem_id=\"tts-audio\")\n                    btn_4 = gr.Button(\"Generate!\")\n                    btn_4.click(infer_long_text,\n                              inputs=[textbox_4, preset_dropdown_4, prompt_file_4, language_dropdown_4, accent_dropdown_4],\n                              outputs=[text_output_4, audio_output_4])\n\n    webbrowser.open(\"http://127.0.0.1:7860\")\n    app.launch()\n\nif __name__ == \"__main__\":\n    formatter = (\n        \"%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(message)s\"\n    )\n    logging.basicConfig(format=formatter, level=logging.INFO)\n    main()\n"
        },
        {
          "name": "macros.py",
          "type": "blob",
          "size": 0.4814453125,
          "content": "NUM_LAYERS = 12\nNUM_HEAD = 16\nN_DIM = 1024\nPREFIX_MODE = 1\nNUM_QUANTIZERS = 8\nSAMPLE_RATE = 24000\n\nlang2token = {\n    'zh': \"[ZH]\",\n    'ja': \"[JA]\",\n    \"en\": \"[EN]\",\n    'mix': \"\",\n}\n\nlang2code = {\n    'zh': 0,\n    'ja': 1,\n    \"en\": 2,\n}\n\ntoken2lang = {\n    '[ZH]': \"zh\",\n    '[JA]': \"ja\",\n    \"[EN]\": \"en\",\n    \"\": \"mix\"\n}\n\ncode2lang = {\n    0: 'zh',\n    1: 'ja',\n    2: \"en\",\n}\n\nlangdropdown2token = {\n    'English': \"[EN]\",\n    '中文': \"[ZH]\",\n    '日本語': \"[JA]\",\n    'Mix': \"\",\n}"
        },
        {
          "name": "model-card.md",
          "type": "blob",
          "size": 1.3916015625,
          "content": "# Model Card: VALL-E X\n\n**Author**: [Songting](https://github.com/Plachtaa).<br>\n<br>\nThis is the official codebase for running open-sourced VALL-E X.\n\nThe following is additional information about the models released here.\n\n## Model Details\n\nVALL-E X is a series of two transformer models that turn text into audio.\n\n### Phoneme to acoustic tokens\n - Input: IPAs converted from input text by a rule-based G2P tool.\n - Output: tokens from the first codebook of the [EnCodec Codec](https://github.com/facebookresearch/encodec) from facebook\n\n### Coarse to fine tokens\n - Input: IPAs converted from input text by a rule-based G2P tool & the first codebook from EnCodec\n - Output: 8 codebooks from EnCodec\n\n### Architecture\n|          Model           | Parameters | Attention  | Output Vocab size |  \n|:------------------------:|:----------:|------------|:-----------------:|\n|         G2P tool         |     -      | -          |        69         |\n| Phoneme to coarse tokens |   150 M    | Causal     |     1x 1,024      |\n|  Coarse to fine tokens   |   150 M    | Non-causal |     7x 1,024      |\n\n### Release date\nAugust 2023\n\n## Broader Implications\nWe anticipate that this model's text to audio capabilities can be used to improve accessbility tools in a variety of languages. \nStraightforward improvements will allow models to run faster than realtime, rendering them useful for applications such as virtual assistants. "
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "modules",
          "type": "tree",
          "content": null
        },
        {
          "name": "nltk_data",
          "type": "tree",
          "content": null
        },
        {
          "name": "presets",
          "type": "tree",
          "content": null
        },
        {
          "name": "prompts",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.220703125,
          "content": "soundfile\nnumpy\ntorch\ntorchvision\ntorchaudio\ntokenizers\nencodec\nlangid\nwget\nunidecode\npyopenjtalk-prebuilt\npypinyin\ninflect\ncn2an\njieba\neng_to_ipa\nopenai-whisper\nmatplotlib\ngradio==3.41.2\nnltk\nsudachipy\nsudachidict_core\nvocos\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}