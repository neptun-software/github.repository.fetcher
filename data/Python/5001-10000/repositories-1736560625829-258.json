{
  "metadata": {
    "timestamp": 1736560625829,
    "page": 258,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Plachtaa/VALL-E-X",
      "stars": 7755,
      "defaultBranch": "master",
      "files": [
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0400390625,
          "content": "MIT License\n\nCopyright (c) 2023 Songting\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README-ZH.md",
          "type": "blob",
          "size": 13.3076171875,
          "content": "# VALL-E X: å¤šè¯­è¨€æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸è¯­éŸ³å…‹éš† ğŸ”Š\n[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/qCBRmAnTxg)\n<br>\n[English](README.md) | ä¸­æ–‡\n<br>\nå¾®è½¯[VALL-E X](https://arxiv.org/pdf/2303.03926) é›¶æ ·æœ¬è¯­éŸ³åˆæˆæ¨¡å‹çš„å¼€æºå®ç°.<br>\n**é¢„è®­ç»ƒæ¨¡å‹ç°å·²å‘å…¬ä¼—å¼€æ”¾ï¼Œä¾›ç ”ç©¶æˆ–åº”ç”¨ä½¿ç”¨ã€‚**\n![vallex-framework](/images/vallex_framework.jpg \"VALL-E X framework\")\n\nVALL-E X æ˜¯ä¸€ä¸ªå¼ºå¤§è€Œåˆ›æ–°çš„å¤šè¯­è¨€æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œæœ€åˆç”±å¾®è½¯å‘å¸ƒã€‚è™½ç„¶å¾®è½¯æœ€åˆåœ¨ä»–ä»¬çš„ç ”ç©¶è®ºæ–‡ä¸­æå‡ºäº†è¯¥æ¦‚å¿µï¼Œä½†å¹¶æœªå‘å¸ƒä»»ä½•ä»£ç æˆ–é¢„è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬è®¤è¯†åˆ°äº†è¿™é¡¹æŠ€æœ¯çš„æ½œåŠ›å’Œä»·å€¼ï¼Œå¤ç°å¹¶è®­ç»ƒäº†ä¸€ä¸ªå¼€æºå¯ç”¨çš„VALL-E Xæ¨¡å‹ã€‚æˆ‘ä»¬å¾ˆä¹æ„ä¸ç¤¾åŒºåˆ†äº«æˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè®©æ¯ä¸ªäººéƒ½èƒ½ä½“éªŒåˆ°æ¬¡ä¸–ä»£TTSçš„å¨åŠ›ã€‚ ğŸ§\n<br>\næ›´å¤šç»†èŠ‚è¯·æŸ¥çœ‹ [model card](./model-card.md).\n\n## ğŸ“– ç›®å½•\n* [ğŸš€ æ›´æ–°æ—¥å¿—](#-æ›´æ–°æ—¥å¿—)\n* [ğŸ“¢ åŠŸèƒ½ç‰¹ç‚¹](#-åŠŸèƒ½ç‰¹ç‚¹)\n* [ğŸ’» æœ¬åœ°å®‰è£…](#-æœ¬åœ°å®‰è£…)\n* [ğŸ§ åœ¨çº¿Demo](#-åœ¨çº¿Demo)\n* [ğŸ ä½¿ç”¨æ–¹æ³•](#-Pythonä¸­çš„ä½¿ç”¨æ–¹æ³•)\n* [â“ FAQ](#-faq)\n* [ğŸ§  TODO](#-todo)\n\n## ğŸš€ Updates\n**2023.09.10**\n- æ”¯æŒAR decoderçš„batch decodingä»¥å®ç°æ›´ç¨³å®šçš„ç”Ÿæˆç»“æœ\n\n**2023.08.30**\n- å°†EnCodecè§£ç å™¨æ›¿æ¢æˆäº†Vocosè§£ç å™¨ï¼Œæå‡äº†éŸ³è´¨ã€‚ (æ„Ÿè°¢[@v0xie](https://github.com/v0xie))\n\n**2023.08.23**\n- åŠ å…¥äº†é•¿æ–‡æœ¬ç”ŸæˆåŠŸèƒ½\n\n**2023.08.20**\n- åŠ å…¥äº†ä¸­æ–‡ç‰ˆREADME\n\n**2023.08.14**\n- é¢„è®­ç»ƒæ¨¡å‹æƒé‡å·²å‘å¸ƒï¼Œä»[è¿™é‡Œ](https://drive.google.com/file/d/10gdQWvP-K_e1undkvv0p2b7SU6I4Egyl/view?usp=sharing)ä¸‹è½½ã€‚\n\n## ğŸ’» æœ¬åœ°å®‰è£…\n### ä½¿ç”¨pipå®‰è£…ï¼Œå¿…é¡»ä½¿ç”¨Python 3.10ï¼ŒCUDA 11.7 ~ 12.0ï¼ŒPyTorch 2.0+\n```commandline\ngit clone https://github.com/Plachtaa/VALL-E-X.git\ncd VALL-E-X\npip install -r requirements.txt\n```\n\n> æ³¨æ„ï¼šå¦‚æœéœ€è¦åˆ¶ä½œpromptï¼Œéœ€è¦å®‰è£… ffmpeg å¹¶å°†å…¶æ‰€åœ¨æ–‡ä»¶å¤¹åŠ å…¥åˆ°ç¯å¢ƒå˜é‡PATHä¸­\n\nç¬¬ä¸€æ¬¡è¿è¡Œç¨‹åºæ—¶ï¼Œä¼šè‡ªåŠ¨ä¸‹è½½ç›¸åº”çš„æ¨¡å‹ã€‚å¦‚æœä¸‹è½½å¤±è´¥å¹¶æŠ¥é”™ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ã€‚\n\nï¼ˆè¯·æ³¨æ„ç›®å½•å’Œæ–‡ä»¶å¤¹çš„å¤§å°å†™ï¼‰\n\n1.æ£€æŸ¥å®‰è£…ç›®å½•ä¸‹æ˜¯å¦å­˜åœ¨`checkpoints`æ–‡ä»¶å¤¹ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåœ¨å®‰è£…ç›®å½•ä¸‹æ‰‹åŠ¨åˆ›å»º`checkpoints`æ–‡ä»¶å¤¹ï¼ˆ`./checkpoints/`ï¼‰ã€‚\n\n2.æ£€æŸ¥`checkpoints`æ–‡ä»¶å¤¹ä¸­æ˜¯å¦æœ‰`vallex-checkpoint.pt`æ–‡ä»¶ã€‚å¦‚æœæ²¡æœ‰ï¼Œè¯·ä»[è¿™é‡Œ](https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt)\næ‰‹åŠ¨ä¸‹è½½`vallex-checkpoint.pt`æ–‡ä»¶å¹¶æ”¾åˆ°`checkpoints`æ–‡ä»¶å¤¹é‡Œã€‚\n\n3.æ£€æŸ¥å®‰è£…ç›®å½•ä¸‹æ˜¯å¦å­˜åœ¨`whisper`æ–‡ä»¶å¤¹ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåœ¨å®‰è£…ç›®å½•ä¸‹æ‰‹åŠ¨åˆ›å»º`whisper`æ–‡ä»¶å¤¹ï¼ˆ`./whisper/`ï¼‰ã€‚\n\n4.æ£€æŸ¥`whisper`æ–‡ä»¶å¤¹ä¸­æ˜¯å¦æœ‰`medium.pt`æ–‡ä»¶ã€‚å¦‚æœæ²¡æœ‰ï¼Œè¯·ä»[è¿™é‡Œ](https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt)\næ‰‹åŠ¨ä¸‹è½½`medium.pt`æ–‡ä»¶å¹¶æ”¾åˆ°`whisper`æ–‡ä»¶å¤¹é‡Œã€‚\n\n##  ğŸ§ åœ¨çº¿Demo\nå¦‚æœä½ ä¸æƒ³åœ¨æœ¬åœ°å®‰è£…ï¼Œä½ å¯ä»¥åœ¨çº¿ä½“éªŒVALL-E Xçš„åŠŸèƒ½ï¼Œç‚¹å‡»ä¸‹é¢çš„ä»»æ„ä¸€ä¸ªé“¾æ¥å³å¯å¼€å§‹ä½“éªŒã€‚\n<br>\n[![Open in Spaces](https://img.shields.io/badge/ğŸ¤—-Open%20in%20Spaces-blue.svg)](https://huggingface.co/spaces/Plachta/VALL-E-X)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1yyD_sz531QntLKowMHo-XxorsFBCfKul?usp=sharing)\n\n\n## ğŸ“¢ åŠŸèƒ½ç‰¹ç‚¹\n\nVALL-E X é…å¤‡æœ‰ä¸€ç³»åˆ—å°–ç«¯åŠŸèƒ½ï¼š\n\n1. **å¤šè¯­è¨€ TTS**: å¯ä½¿ç”¨ä¸‰ç§è¯­è¨€ - è‹±è¯­ã€ä¸­æ–‡å’Œæ—¥è¯­ - è¿›è¡Œè‡ªç„¶ã€å¯Œæœ‰è¡¨ç°åŠ›çš„è¯­éŸ³åˆæˆã€‚\n\n2. **é›¶æ ·æœ¬è¯­éŸ³å…‹éš†**: ä»…éœ€å½•åˆ¶ä»»æ„è¯´è¯äººçš„çŸ­çŸ­çš„ 3~10 ç§’å½•éŸ³ï¼ŒVALL-E X å°±èƒ½ç”Ÿæˆä¸ªæ€§åŒ–ã€é«˜è´¨é‡çš„è¯­éŸ³ï¼Œå®Œç¾è¿˜åŸä»–ä»¬çš„å£°éŸ³ã€‚\n\n<details>\n  <summary><h5>æŸ¥çœ‹ç¤ºä¾‹</h5></summary>\n\n[prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/a7baa51d-a53a-41cc-a03d-6970f25fcca7)\n\n\n[output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/b895601a-d126-4138-beff-061aabdc7985)\n\n</details>\n\n3. **è¯­éŸ³æƒ…æ„Ÿæ§åˆ¶**: VALL-E X å¯ä»¥åˆæˆä¸ç»™å®šè¯´è¯äººå½•éŸ³ç›¸åŒæƒ…æ„Ÿçš„è¯­éŸ³ï¼Œä¸ºéŸ³é¢‘å¢æ·»æ›´å¤šè¡¨ç°åŠ›ã€‚\n\n<details>\n  <summary><h5>æŸ¥çœ‹ç¤ºä¾‹</h5></summary>\n\nhttps://github.com/Plachtaa/VALL-E-X/assets/112609742/56fa9988-925e-4757-82c5-83ecb0df6266\n\n\nhttps://github.com/Plachtaa/VALL-E-X/assets/112609742/699c47a3-d502-4801-8364-bd89bcc0b8f1\n\n</details>\n\n4. **é›¶æ ·æœ¬è·¨è¯­è¨€è¯­éŸ³åˆæˆ**: VALL-E X å¯ä»¥åˆæˆä¸ç»™å®šè¯´è¯äººæ¯è¯­ä¸åŒçš„å¦ä¸€ç§è¯­è¨€ï¼Œåœ¨ä¸å½±å“å£éŸ³å’Œæµåˆ©åº¦çš„åŒæ—¶ï¼Œä¿ç•™è¯¥è¯´è¯äººçš„éŸ³è‰²ä¸æƒ…æ„Ÿã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨æ—¥è¯­æ¯è¯­è€…è¿›è¡Œè‹±æ–‡ä¸ä¸­æ–‡åˆæˆçš„æ ·ä¾‹ï¼š ğŸ‡¯ğŸ‡µ ğŸ—£\n\n<details>\n  <summary><h5>æŸ¥çœ‹ç¤ºä¾‹</h5></summary>\n\n[jp-prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/ea6e2ee4-139a-41b4-837e-0bd04dda6e19)\n\n\n[en-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/db8f9782-923f-425e-ba94-e8c1bd48f207)\n\n\n[zh-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/15829d79-e448-44d3-8965-fafa7a3f8c28)\n\n</details>\n\n5. **å£éŸ³æ§åˆ¶**: VALL-E X å…è®¸æ‚¨æ§åˆ¶æ‰€åˆæˆéŸ³é¢‘çš„å£éŸ³ï¼Œæ¯”å¦‚è¯´ä¸­æ–‡å¸¦è‹±è¯­å£éŸ³æˆ–åä¹‹ã€‚ ğŸ‡¨ğŸ‡³ ğŸ’¬\n\n<details>\n  <summary><h5>æŸ¥çœ‹ç¤ºä¾‹</h5></summary>\n\n[en-prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/f688d7f6-70ef-46ec-b1cc-355c31e78b3b)\n\n\n[zh-accent-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/be59c7ca-b45b-44ca-a30d-4d800c950ccc)\n\n\n[en-accent-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/8b4f4f9b-f299-4ea4-a548-137437b71738)\n\n</details>\n\n6. **å£°å­¦ç¯å¢ƒä¿ç•™**: å½“ç»™å®šè¯´è¯äººçš„å½•éŸ³åœ¨ä¸åŒçš„å£°å­¦ç¯å¢ƒä¸‹å½•åˆ¶æ—¶ï¼ŒVALL-E X å¯ä»¥ä¿ç•™è¯¥å£°å­¦ç¯å¢ƒï¼Œä½¿åˆæˆè¯­éŸ³å¬èµ·æ¥æ›´åŠ è‡ªç„¶ã€‚\n\n<details>\n  <summary><h5>æŸ¥çœ‹ç¤ºä¾‹</h5></summary>\n\n[noise-prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/68986d88-abd0-4d1d-96e4-4f893eb9259e)\n\n\n[noise-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/96c4c612-4516-4683-8804-501b70938608)\n\n</details>\n\n\nä½ å¯ä»¥è®¿é—®æˆ‘ä»¬çš„[demoé¡µé¢](https://plachtaa.github.io/) æ¥æµè§ˆæ›´å¤šç¤ºä¾‹!\n\n## ğŸ’» Pythonä¸­çš„ä½¿ç”¨æ–¹æ³•\n\n<details open>\n  <summary><h3>ğŸª‘ åŸºæœ¬ä½¿ç”¨</h3></summary>\n\n```python\nfrom utils.generation import SAMPLE_RATE, generate_audio, preload_models\nfrom scipy.io.wavfile import write as write_wav\nfrom IPython.display import Audio\n\n# download and load all models\npreload_models()\n\n# generate audio from text\ntext_prompt = \"\"\"\nHello, my name is Nose. And uh, and I like hamburger. Hahaha... But I also have other interests such as playing tactic toast.\n\"\"\"\naudio_array = generate_audio(text_prompt)\n\n# save audio to disk\nwrite_wav(\"vallex_generation.wav\", SAMPLE_RATE, audio_array)\n\n# play text in notebook\nAudio(audio_array, rate=SAMPLE_RATE)\n```\n\n[hamburger.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/578d7bbe-cda9-483e-898c-29646edc8f2e)\n\n</details>\n\n<details open>\n  <summary><h3>ğŸŒ å¤šè¯­è¨€</h3></summary>\n<br>\nè¯¥VALL-E Xå®ç°æ”¯æŒä¸‰ç§è¯­è¨€ï¼šè‹±è¯­ã€ä¸­æ–‡å’Œæ—¥è¯­ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½®`language`å‚æ•°æ¥æŒ‡å®šè¯­è¨€ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹å°†è‡ªåŠ¨æ£€æµ‹è¯­è¨€ã€‚\n<br>\n\n```python\n\ntext_prompt = \"\"\"\n    ãƒãƒ¥ã‚½ã‚¯ã¯ç§ã®ãŠæ°—ã«å…¥ã‚Šã®ç¥­ã‚Šã§ã™ã€‚ ç§ã¯æ•°æ—¥é–“ä¼‘ã‚“ã§ã€å‹äººã‚„å®¶æ—ã¨ã®æ™‚é–“ã‚’éã”ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n\"\"\"\naudio_array = generate_audio(text_prompt)\n```\n\n[vallex_japanese.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/ee57a688-3e83-4be5-b0fe-019d16eec51c)\n\n*æ³¨æ„ï¼šå³ä½¿åœ¨ä¸€å¥è¯ä¸­æ··åˆå¤šç§è¯­è¨€çš„æƒ…å†µä¸‹ï¼ŒVALL-E Xä¹Ÿèƒ½å®Œç¾åœ°æ§åˆ¶å£éŸ³ï¼Œä½†æ˜¯æ‚¨éœ€è¦æ‰‹åŠ¨æ ‡è®°å„ä¸ªå¥å­å¯¹åº”çš„è¯­è¨€ä»¥ä¾¿äºæˆ‘ä»¬çš„G2På·¥å…·è¯†åˆ«å®ƒä»¬ã€‚*\n```python\ntext_prompt = \"\"\"\n    [EN]The Thirty Years' War was a devastating conflict that had a profound impact on Europe.[EN]\n    [ZH]è¿™æ˜¯å†å²çš„å¼€å§‹ã€‚ å¦‚æœæ‚¨æƒ³å¬æ›´å¤šï¼Œè¯·ç»§ç»­ã€‚[ZH]\n\"\"\"\naudio_array = generate_audio(text_prompt, language='mix')\n```\n\n[vallex_codeswitch.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/d8667abf-bd08-499f-a383-a861d852f98a)\n\n</details>\n\n<details open>\n<summary><h3>ğŸ“¼ é¢„è®¾éŸ³è‰²</h3></summary>\n  \næˆ‘ä»¬æä¾›åå‡ ç§è¯´è¯äººéŸ³è‰²å¯ç›´æ¥VALL-E Xä½¿ç”¨! åœ¨[è¿™é‡Œ](/presets)æµè§ˆæ‰€æœ‰å¯ç”¨éŸ³è‰²ã€‚\n\n> VALL-E X å°è¯•åŒ¹é…ç»™å®šé¢„è®¾éŸ³è‰²çš„éŸ³è°ƒã€éŸ³é«˜ã€æƒ…æ„Ÿå’ŒéŸµå¾‹ã€‚è¯¥æ¨¡å‹è¿˜å°è¯•ä¿ç•™éŸ³ä¹ã€ç¯å¢ƒå™ªå£°ç­‰ã€‚\n```python\ntext_prompt = \"\"\"\nI am an innocent boy with a smoky voice. It is a great honor for me to speak at the United Nations today.\n\"\"\"\naudio_array = generate_audio(text_prompt, prompt=\"dingzhen\")\n```\n\n[smoky.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/d3f55732-b1cd-420f-87d6-eab60db14dc5)\n\n</details>\n\n<details open>\n<summary><h3>ğŸ™å£°éŸ³å…‹éš†</h3></summary>\n  \nVALL-E X æ”¯æŒå£°éŸ³å…‹éš†ï¼ä½ å¯ä»¥ä½¿ç”¨ä»»ä½•äººï¼Œè§’è‰²ï¼Œç”šè‡³æ˜¯ä½ è‡ªå·±çš„å£°éŸ³ï¼Œæ¥åˆ¶ä½œä¸€ä¸ªéŸ³é¢‘æç¤ºã€‚åœ¨ä½ ä½¿ç”¨è¯¥éŸ³é¢‘æç¤ºæ—¶ï¼ŒVALL-E X å°†ä¼šä½¿ç”¨ä¸å…¶ç›¸ä¼¼çš„å£°éŸ³æ¥åˆæˆæ–‡æœ¬ã€‚\n<br>\nä½ éœ€è¦æä¾›ä¸€æ®µ3~10ç§’é•¿çš„è¯­éŸ³ï¼Œä»¥åŠè¯¥è¯­éŸ³å¯¹åº”çš„æ–‡æœ¬ï¼Œæ¥åˆ¶ä½œéŸ³é¢‘æç¤ºã€‚ä½ ä¹Ÿå¯ä»¥å°†æ–‡æœ¬ç•™ç©ºï¼Œè®©[Whisper](https://github.com/openai/whisper)æ¨¡å‹ä¸ºä½ ç”Ÿæˆæ–‡æœ¬ã€‚\n> VALL-E X å°è¯•åŒ¹é…ç»™å®šéŸ³é¢‘æç¤ºçš„éŸ³è°ƒã€éŸ³é«˜ã€æƒ…æ„Ÿå’ŒéŸµå¾‹ã€‚è¯¥æ¨¡å‹è¿˜å°è¯•ä¿ç•™éŸ³ä¹ã€ç¯å¢ƒå™ªå£°ç­‰ã€‚\n\n```python\nfrom utils.prompt_making import make_prompt\n\n### Use given transcript\nmake_prompt(name=\"paimon\", audio_prompt_path=\"paimon_prompt.wav\",\n                transcript=\"Just, what was that? Paimon thought we were gonna get eaten.\")\n\n### Alternatively, use whisper\nmake_prompt(name=\"paimon\", audio_prompt_path=\"paimon_prompt.wav\")\n```\næ¥å°è¯•ä¸€ä¸‹åˆšåˆšåšå¥½çš„éŸ³é¢‘æç¤ºå§ï¼\n```python\nfrom utils.generation import SAMPLE_RATE, generate_audio, preload_models\nfrom scipy.io.wavfile import write as write_wav\n\n# download and load all models\npreload_models()\n\ntext_prompt = \"\"\"\nHey, Traveler, Listen to this, This machine has taken my voice, and now it can talk just like me!\n\"\"\"\naudio_array = generate_audio(text_prompt, prompt=\"paimon\")\n\nwrite_wav(\"paimon_cloned.wav\", SAMPLE_RATE, audio_array)\n\n```\n\n[paimon_prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/e7922859-9d12-4e2a-8651-e156e4280311)\n\n\n[paimon_cloned.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/60d3b7e9-5ead-4024-b499-a897ce5f3d5e)\n\n\n</details>\n\n\n<details open>\n<summary><h3>ğŸ¢ç”¨æˆ·ç•Œé¢</h3></summary>\n\nå¦‚æœä½ ä¸æ“…é•¿ä»£ç ï¼Œæˆ‘ä»¬è¿˜ä¸ºVALL-E Xåˆ›å»ºäº†ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„å›¾å½¢ç•Œé¢ã€‚å®ƒå¯ä»¥è®©æ‚¨è½»æ¾åœ°ä¸æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œä½¿è¯­éŸ³å…‹éš†å’Œå¤šè¯­è¨€è¯­éŸ³åˆæˆå˜å¾—è½»è€Œæ˜“ä¸¾ã€‚\n<br>\nä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ç”¨æˆ·ç•Œé¢ï¼š\n```commandline\npython -X utf8 launch-ui.py\n```\n</details>\n\n## ğŸ› ï¸ ç¡¬ä»¶è¦æ±‚åŠæ¨ç†é€Ÿåº¦\n\nVALL-E X å¯ä»¥åœ¨CPUæˆ–GPUä¸Šè¿è¡Œ (`pytorch 2.0+`, CUDA 11.7 ~ CUDA 12.0).\n\nè‹¥ä½¿ç”¨GPUè¿è¡Œï¼Œä½ éœ€è¦è‡³å°‘6GBçš„æ˜¾å­˜ã€‚\n\n## âš™ï¸ Details\n\nVALL-E X ä¸ [Bark](https://github.com/suno-ai/bark), [VALL-E](https://arxiv.org/abs/2301.02111) and [AudioLM](https://arxiv.org/abs/2209.03143)ç±»ä¼¼, ä½¿ç”¨GPTé£æ ¼çš„æ¨¡å‹ä»¥è‡ªå›å½’æ–¹å¼é¢„æµ‹é‡åŒ–éŸ³é¢‘tokenï¼Œå¹¶ç”±[EnCodec](https://github.com/facebookresearch/encodec)è§£ç .\n<br>\nä¸ [Bark](https://github.com/suno-ai/bark) ç›¸æ¯”:\n- âœ” **è½»é‡**: 3ï¸âƒ£ âœ– æ›´å°,\n- âœ” **å¿«é€Ÿ**: 4ï¸âƒ£ âœ– æ›´å¿«, \n- âœ” **ä¸­æ–‡&æ—¥æ–‡çš„æ›´é«˜è´¨é‡**\n- âœ” **è·¨è¯­è¨€åˆæˆæ—¶æ²¡æœ‰å¤–å›½å£éŸ³**\n- âœ” **å¼€æ”¾ä¸”æ˜“äºæ“ä½œçš„å£°éŸ³å…‹éš†**\n- âŒ **æ”¯æŒçš„è¯­è¨€è¾ƒå°‘**\n- âŒ **æ²¡æœ‰ç”¨äºåˆæˆéŸ³ä¹åŠç‰¹æ®ŠéŸ³æ•ˆçš„token**\n\n### æ”¯æŒçš„è¯­è¨€\n\n| è¯­è¨€      | çŠ¶æ€ |\n|---------| :---: |\n| è‹±è¯­ (en) | âœ… |\n| æ—¥è¯­ (ja) | âœ… |\n| ä¸­æ–‡ (zh) | âœ… |\n\n## â“ FAQ\n\n#### åœ¨å“ªé‡Œå¯ä»¥ä¸‹è½½checkpoint?\n* å½“æ‚¨ç¬¬ä¸€æ¬¡è¿è¡Œç¨‹åºæ—¶,æˆ‘ä»¬ä½¿ç”¨`wget`å°†æ¨¡å‹ä¸‹è½½åˆ°`./checkpoints/`ç›®å½•é‡Œã€‚\n* å¦‚æœç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ä¸‹è½½å¤±è´¥ï¼Œè¯·ä»[è¿™é‡Œ](https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt)æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¹¶å°†æ–‡ä»¶æ”¾åœ¨`./checkpoints/`é‡Œã€‚\n\n#### éœ€è¦å¤šå°‘æ˜¾å­˜?\n* 6GB æ˜¾å­˜(GPU VRAM) - å‡ ä¹æ‰€æœ‰NVIDIA GPUéƒ½æ»¡è¶³è¦æ±‚.\n\n#### ä¸ºä»€ä¹ˆæ¨¡å‹æ— æ³•ç”Ÿæˆé•¿æ–‡æœ¬?\n* å½“åºåˆ—é•¿åº¦å¢åŠ æ—¶ï¼ŒTransformerçš„è®¡ç®—å¤æ‚åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚å› æ­¤ï¼Œæ‰€æœ‰è®­ç»ƒéŸ³é¢‘éƒ½ä¿æŒåœ¨22ç§’ä»¥ä¸‹ã€‚è¯·ç¡®ä¿éŸ³é¢‘æç¤ºï¼ˆaudio promptï¼‰å’Œç”Ÿæˆçš„éŸ³é¢‘çš„æ€»é•¿åº¦å°äº22ç§’ä»¥ç¡®ä¿å¯æ¥å—çš„æ€§èƒ½ã€‚\n\n#### æ›´å¤š...\n\n## ğŸ§  å¾…åŠäº‹é¡¹\n- [x] æ·»åŠ ä¸­æ–‡ README\n- [x] é•¿æ–‡æœ¬ç”Ÿæˆ\n- [x] ç”¨Vocosè§£ç å™¨æ›¿æ¢Encodecè§£ç å™¨\n- [ ] å¾®è°ƒä»¥å®ç°æ›´å¥½çš„è¯­éŸ³è‡ªé€‚åº”\n- [ ] ç»™épythonç”¨æˆ·çš„`.bat`è„šæœ¬\n- [ ] æ›´å¤š...\n\n## ğŸ™ æ„Ÿè°¢\n- [VALL-E X paper](https://arxiv.org/pdf/2303.03926) for the brilliant idea\n- [lifeiteng's vall-e](https://github.com/lifeiteng/vall-e) for related training code\n- [bark](https://github.com/suno-ai/bark) for the amazing pioneering work in neuro-codec TTS model\n\n## â­ï¸ è¡¨ç¤ºå‡ºä½ çš„æ”¯æŒ\n\nå¦‚æœæ‚¨è§‰å¾—VALL-E Xæœ‰è¶£ä¸”æœ‰ç”¨ï¼Œè¯·åœ¨GitHubä¸Šç»™æˆ‘ä»¬ä¸€é¢—æ˜Ÿï¼ â­ï¸ å®ƒé¼“åŠ±æˆ‘ä»¬ä¸æ–­æ”¹è¿›æ¨¡å‹å¹¶æ·»åŠ ä»¤äººå…´å¥‹çš„åŠŸèƒ½ã€‚\n\n## ğŸ“œ License\n\nVALL-E X ä½¿ç”¨ [MIT License](./LICENSE).\n\n---\n\næœ‰é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Ÿ å¯ä»¥éšä¾¿ [open an issue](https://github.com/Plachtaa/VALL-E-X/issues/new) æˆ–åŠ å…¥æˆ‘ä»¬çš„ [Discord](https://discord.gg/qCBRmAnTxg)\n\nHappy voice cloning! ğŸ¤\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.1806640625,
          "content": "# VALL-E X: Multilingual Text-to-Speech Synthesis and Voice Cloning ğŸ”Š\n[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/qCBRmAnTxg)\n<br>\nEnglish | [ä¸­æ–‡](README-ZH.md)\n<br>\nAn open source implementation of Microsoft's [VALL-E X](https://arxiv.org/pdf/2303.03926) zero-shot TTS model.<br>\n**We release our trained model to the public for research or application usage.**\n\n![vallex-framework](/images/vallex_framework.jpg \"VALL-E X framework\")\n\nVALL-E X is an amazing multilingual text-to-speech (TTS) model proposed by Microsoft. While Microsoft initially publish in their research paper, they did not release any code or pretrained models. Recognizing the potential and value of this technology, our team took on the challenge to reproduce the results and train our own model. We are glad to share our trained VALL-E X model with the community, allowing everyone to experience the power next-generation TTS! ğŸ§\n<br>\n<br>\nMore details about the model are presented in [model card](./model-card.md).\n\n## ğŸ“– Quick Index\n* [ğŸš€ Updates](#-updates)\n* [ğŸ“¢ Features](#-features)\n* [ğŸ’» Installation](#-installation)\n* [ğŸ§ Demos](#-demos)\n* [ğŸ Usage](#-usage-in-python)\n* [â“ FAQ](#-faq)\n* [ğŸ§  TODO](#-todo)\n\n## ğŸš€ Updates\n**2023.09.10**\n- Added AR decoder batch decoding for more stable generation result.\n\n**2023.08.30**\n- Replaced EnCodec decoder with Vocos decoder, improved audio quality. (Thanks to [@v0xie](https://github.com/v0xie))\n\n**2023.08.23**\n- Added long text generation.\n\n**2023.08.20**\n- Added [Chinese README](README-ZH.md).\n\n**2023.08.14**\n- Pretrained VALL-E X checkpoint is now released. Download it [here](https://drive.google.com/file/d/10gdQWvP-K_e1undkvv0p2b7SU6I4Egyl/view?usp=sharing)\n\n## ğŸ’» Installation\n### Install with pip, Python 3.10, CUDA 11.7 ~ 12.0, PyTorch 2.0+\n```commandline\ngit clone https://github.com/Plachtaa/VALL-E-X.git\ncd VALL-E-X\npip install -r requirements.txt\n```\n\n> Note: If you want to make prompt, you need to install ffmpeg and add its folder to the environment variable PATH.\n\nWhen you run the program for the first time, it will automatically download the corresponding model. \n\nIf the download fails and reports an error, please follow the steps below to manually download the model.\n\n(Please pay attention to the capitalization of folders)\n\n1. Check whether there is a `checkpoints` folder in the installation directory. \nIf not, manually create a `checkpoints` folder (`./checkpoints/`) in the installation directory.\n\n2. Check whether there is a `vallex-checkpoint.pt` file in the `checkpoints` folder. \nIf not, please manually download the `vallex-checkpoint.pt` file from [here](https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt) and put it in the `checkpoints` folder.\n\n3. Check whether there is a `whisper` folder in the installation directory. \nIf not, manually create a `whisper` folder (`./whisper/`) in the installation directory.\n\n4. Check whether there is a `medium.pt` file in the `whisper` folder. \nIf not, please manually download the `medium.pt` file from [here](https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt) and put it in the `whisper` folder.\n\n##  ğŸ§ Demos\nNot ready to set up the environment on your local machine just yet? No problem! We've got you covered with our online demos. You can try out VALL-E X directly on Hugging Face or Google Colab, experiencing the model's capabilities hassle-free!\n<br>\n[![Open in Spaces](https://img.shields.io/badge/ğŸ¤—-Open%20in%20Spaces-blue.svg)](https://huggingface.co/spaces/Plachta/VALL-E-X)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1yyD_sz531QntLKowMHo-XxorsFBCfKul?usp=sharing)\n\n\n## ğŸ“¢ Features\n\nVALL-E X comes packed with cutting-edge functionalities:\n\n1. **Multilingual TTS**: Speak in three languages - English, Chinese, and Japanese - with natural and expressive speech synthesis.\n\n2. **Zero-shot Voice Cloning**: Enroll a short 3~10 seconds recording of an unseen speaker, and watch VALL-E X create personalized, high-quality speech that sounds just like them!\n\n<details>\n  <summary><h5>see example</h5></summary>\n\n[prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/a7baa51d-a53a-41cc-a03d-6970f25fcca7)\n\n\n[output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/b895601a-d126-4138-beff-061aabdc7985)\n\n</details>\n\n3. **Speech Emotion Control**: Experience the power of emotions! VALL-E X can synthesize speech with the same emotion as the acoustic prompt provided, adding an extra layer of expressiveness to your audio.\n\n<details>\n  <summary><h5>see example</h5></summary>\n\nhttps://github.com/Plachtaa/VALL-E-X/assets/112609742/56fa9988-925e-4757-82c5-83ecb0df6266\n\n\nhttps://github.com/Plachtaa/VALL-E-X/assets/112609742/699c47a3-d502-4801-8364-bd89bcc0b8f1\n\n</details>\n\n4. **Zero-shot Cross-Lingual Speech Synthesis**: Take monolingual speakers on a linguistic journey! VALL-E X can produce personalized speech in another language without compromising on fluency or accent. Below is a Japanese speaker talk in Chinese & English. ğŸ‡¯ğŸ‡µ ğŸ—£\n\n<details>\n  <summary><h5>see example</h5></summary>\n\n[jp-prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/ea6e2ee4-139a-41b4-837e-0bd04dda6e19)\n\n\n[en-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/db8f9782-923f-425e-ba94-e8c1bd48f207)\n\n\n[zh-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/15829d79-e448-44d3-8965-fafa7a3f8c28)\n\n</details>\n\n5. **Accent Control**: Get creative with accents! VALL-E X allows you to experiment with different accents, like speaking Chinese with an English accent or vice versa. ğŸ‡¨ğŸ‡³ ğŸ’¬\n\n<details>\n  <summary><h5>see example</h5></summary>\n\n[en-prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/f688d7f6-70ef-46ec-b1cc-355c31e78b3b)\n\n\n[zh-accent-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/be59c7ca-b45b-44ca-a30d-4d800c950ccc)\n\n\n[en-accent-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/8b4f4f9b-f299-4ea4-a548-137437b71738)\n\n</details>\n\n6. **Acoustic Environment Maintenance**: No need for perfectly clean audio prompts! VALL-E X adapts to the acoustic environment of the input, making speech generation feel natural and immersive.\n\n<details>\n  <summary><h5>see example</h5></summary>\n\n[noise-prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/68986d88-abd0-4d1d-96e4-4f893eb9259e)\n\n\n[noise-output.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/96c4c612-4516-4683-8804-501b70938608)\n\n</details>\n\n\nExplore our [demo page](https://plachtaa.github.io/) for a lot more examples!\n\n## ğŸ Usage in Python\n\n<details open>\n  <summary><h3>ğŸª‘ Basics</h3></summary>\n\n```python\nfrom utils.generation import SAMPLE_RATE, generate_audio, preload_models\nfrom scipy.io.wavfile import write as write_wav\nfrom IPython.display import Audio\n\n# download and load all models\npreload_models()\n\n# generate audio from text\ntext_prompt = \"\"\"\nHello, my name is Nose. And uh, and I like hamburger. Hahaha... But I also have other interests such as playing tactic toast.\n\"\"\"\naudio_array = generate_audio(text_prompt)\n\n# save audio to disk\nwrite_wav(\"vallex_generation.wav\", SAMPLE_RATE, audio_array)\n\n# play text in notebook\nAudio(audio_array, rate=SAMPLE_RATE)\n```\n\n[hamburger.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/578d7bbe-cda9-483e-898c-29646edc8f2e)\n\n</details>\n\n<details open>\n  <summary><h3>ğŸŒ Foreign Language</h3></summary>\n<br>\nThis VALL-E X implementation also supports Chinese and Japanese. All three languages have equally awesome performance!\n<br>\n\n```python\n\ntext_prompt = \"\"\"\n    ãƒãƒ¥ã‚½ã‚¯ã¯ç§ã®ãŠæ°—ã«å…¥ã‚Šã®ç¥­ã‚Šã§ã™ã€‚ ç§ã¯æ•°æ—¥é–“ä¼‘ã‚“ã§ã€å‹äººã‚„å®¶æ—ã¨ã®æ™‚é–“ã‚’éã”ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n\"\"\"\naudio_array = generate_audio(text_prompt)\n```\n\n[vallex_japanese.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/ee57a688-3e83-4be5-b0fe-019d16eec51c)\n\n*Note: VALL-E X controls accent perfectly even when synthesizing code-switch text. However, you need to manually denote language of respective sentences (since our g2p tool is rule-base)*\n```python\ntext_prompt = \"\"\"\n    [EN]The Thirty Years' War was a devastating conflict that had a profound impact on Europe.[EN]\n    [ZH]è¿™æ˜¯å†å²çš„å¼€å§‹ã€‚ å¦‚æœæ‚¨æƒ³å¬æ›´å¤šï¼Œè¯·ç»§ç»­ã€‚[ZH]\n\"\"\"\naudio_array = generate_audio(text_prompt, language='mix')\n```\n\n[vallex_codeswitch.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/d8667abf-bd08-499f-a383-a861d852f98a)\n\n</details>\n\n<details open>\n<summary><h3>ğŸ“¼ Voice Presets</h3></summary>\n  \nVALL-E X provides tens of speaker voices which you can directly used for inference! Browse all voices in the [code](/presets)\n\n> VALL-E X tries to match the tone, pitch, emotion and prosody of a given preset. The model also attempts to preserve music, ambient noise, etc.\n\n```python\ntext_prompt = \"\"\"\nI am an innocent boy with a smoky voice. It is a great honor for me to speak at the United Nations today.\n\"\"\"\naudio_array = generate_audio(text_prompt, prompt=\"dingzhen\")\n```\n\n[smoky.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/d3f55732-b1cd-420f-87d6-eab60db14dc5)\n\n</details>\n\n<details open>\n<summary><h3>ğŸ™Voice Cloning</h3></summary>\n  \nVALL-E X supports voice cloning! You can make a voice prompt with any person, character or even your own voice, and use it like other voice presets.<br>\nTo make a voice prompt, you need to provide a speech of 3~10 seconds long, as well as the transcript of the speech. \nYou can also leave the transcript blank to let the [Whisper](https://github.com/openai/whisper) model to generate the transcript.\n> VALL-E X tries to match the tone, pitch, emotion and prosody of a given prompt. The model also attempts to preserve music, ambient noise, etc.\n\n```python\nfrom utils.prompt_making import make_prompt\n\n### Use given transcript\nmake_prompt(name=\"paimon\", audio_prompt_path=\"paimon_prompt.wav\",\n                transcript=\"Just, what was that? Paimon thought we were gonna get eaten.\")\n\n### Alternatively, use whisper\nmake_prompt(name=\"paimon\", audio_prompt_path=\"paimon_prompt.wav\")\n```\nNow let's try out the prompt we've just made!\n```python\nfrom utils.generation import SAMPLE_RATE, generate_audio, preload_models\nfrom scipy.io.wavfile import write as write_wav\n\n# download and load all models\npreload_models()\n\ntext_prompt = \"\"\"\nHey, Traveler, Listen to this, This machine has taken my voice, and now it can talk just like me!\n\"\"\"\naudio_array = generate_audio(text_prompt, prompt=\"paimon\")\n\nwrite_wav(\"paimon_cloned.wav\", SAMPLE_RATE, audio_array)\n\n```\n\n[paimon_prompt.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/e7922859-9d12-4e2a-8651-e156e4280311)\n\n\n[paimon_cloned.webm](https://github.com/Plachtaa/VALL-E-X/assets/112609742/60d3b7e9-5ead-4024-b499-a897ce5f3d5e)\n\n\n</details>\n\n\n<details open>\n<summary><h3>ğŸ¢User Interface</h3></summary>\n\nNot comfortable with codes? No problem! We've also created a user-friendly graphical interface for VALL-E X. It allows you to interact with the model effortlessly, making voice cloning and multilingual speech synthesis a breeze.\n<br>\nYou can launch the UI by the following command:\n```commandline\npython -X utf8 launch-ui.py\n```\n</details>\n\n## ğŸ› ï¸ Hardware and Inference Speed\n\nVALL-E X works well on both CPU and GPU (`pytorch 2.0+`, CUDA 11.7 and CUDA 12.0).\n\nA GPU VRAM of 6GB is enough for running VALL-E X without offloading.\n\n## âš™ï¸ Details\n\nVALL-E X is similar to [Bark](https://github.com/suno-ai/bark), [VALL-E](https://arxiv.org/abs/2301.02111) and [AudioLM](https://arxiv.org/abs/2209.03143), which generates audio in GPT-style by predicting audio tokens quantized by [EnCodec](https://github.com/facebookresearch/encodec).\n<br>\nComparing to [Bark](https://github.com/suno-ai/bark):\n- âœ” **Light-weighted**: 3ï¸âƒ£ âœ– smaller,\n- âœ” **Efficient**: 4ï¸âƒ£ âœ– faster, \n- âœ” **Better quality on Chinese & Japanese**\n- âœ” **Cross-lingual speech without foreign accent**\n- âœ” **Easy voice-cloning**\n- âŒ **Less languages**\n- âŒ **No special tokens for music / sound effects**\n\n### Supported Languages\n\n| Language | Status |\n| --- | :---: |\n| English (en) | âœ… |\n| Japanese (ja) | âœ… |\n| Chinese, simplified (zh) | âœ… |\n\n## â“ FAQ\n\n#### Where is code for training?\n* [lifeiteng's vall-e](https://github.com/lifeiteng/vall-e) has almost everything. There is no plan to release our training code because there is no difference between lifeiteng's implementation.\n\n#### Where can I download the model checkpoint?\n* We use `wget` to download the model to directory `./checkpoints/` when you run the program for the first time.\n* If the download fails on the first run, please manually download from [this link](https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt), and put the file under directory `./checkpoints/`.\n\n#### How much VRAM do I need?\n* 6GB GPU VRAM - Almost all NVIDIA GPUs satisfy the requirement.\n\n#### Why the model fails to generate long text?\n* Transformer's computation complexity increases quadratically while the sequence length increases. Hence, all training \nare kept under 22 seconds. Please make sure the total length of audio prompt and generated audio is less than 22 seconds \nto ensure acceptable performance. \n\n\n#### MORE TO BE ADDED...\n\n## ğŸ§  TODO\n- [x] Add Chinese README\n- [x] Long text generation\n- [x] Replace Encodec decoder with Vocos decoder\n- [ ] Fine-tuning for better voice adaptation\n- [ ] `.bat` scripts for non-python users\n- [ ] To be added...\n\n## ğŸ™ Appreciation\n- [VALL-E X paper](https://arxiv.org/pdf/2303.03926) for the brilliant idea\n- [lifeiteng's vall-e](https://github.com/lifeiteng/vall-e) for related training code\n- [bark](https://github.com/suno-ai/bark) for the amazing pioneering work in neuro-codec TTS model\n\n## â­ï¸ Show Your Support\n\nIf you find VALL-E X interesting and useful, give us a star on GitHub! â­ï¸ It encourages us to keep improving the model and adding exciting features.\n\n## ğŸ“œ License\n\nVALL-E X is licensed under the [MIT License](./LICENSE).\n\n---\n\nHave questions or need assistance? Feel free to [open an issue](https://github.com/Plachtaa/VALL-E-X/issues/new) or join our [Discord](https://discord.gg/qCBRmAnTxg)\n\nHappy voice cloning! ğŸ¤\n"
        },
        {
          "name": "customs",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "descriptions.py",
          "type": "blob",
          "size": 2.25390625,
          "content": "top_md = \"\"\"\n# VALL-E X  \nVALL-E X can synthesize high-quality personalized speech with only a 3-second enrolled recording of \nan unseen speaker as an acoustic prompt, even in another language for a monolingual speaker.<br>\nThis implementation supports zero-shot, mono-lingual/cross-lingual text-to-speech functionality of three languages (English, Chinese, Japanese)<br>  \nSee this [demo](https://plachtaa.github.io/) page for more details.\n\"\"\"\n\ninfer_from_audio_md = \"\"\"\nUpload a speech of 3~10 seconds as the audio prompt and type in the text you'd like to synthesize.<br>\nThe model will synthesize speech of given text with the same voice of your audio prompt.<br>\nThe model also tends to preserve the emotion & acoustic environment of your given speech.<br>\nFor faster inference, please use **\"Make prompt\"** to get a `.npz` file as the encoded audio prompt, and use it by **\"Infer from prompt\"**\n\"\"\"\n\nmake_prompt_md = \"\"\"\nUpload a speech of 3~10 seconds as the audio prompt.<br>\nGet a `.npz` file as the encoded audio prompt. Use it by **\"Infer with prompt\"**\n\"\"\"\n\ninfer_from_prompt_md = \"\"\"\nFaster than **\"Infer from audio\"**.<br>\nYou need to **\"Make prompt\"** first, and upload the encoded prompt (a `.npz` file)\n\"\"\"\n\nlong_text_md = \"\"\"\nVery long text is chunked into several sentences, and each sentence is synthesized separately.<br>\nPlease make a prompt or use a preset prompt to infer long text.\n\"\"\"\n\nlong_text_example = \"Just a few years ago, there were no legions of deep learning scientists developing intelligent products and services at major companies and startups. When we entered the field, machine learning did not command headlines in daily newspapers. Our parents had no idea what machine learning was, let alone why we might prefer it to a career in medicine or law. Machine learning was a blue skies academic discipline whose industrial significance was limited to a narrow set of real-world applications, including speech recognition and computer vision. Moreover, many of these applications required so much domain knowledge that they were often regarded as entirely separate areas for which machine learning was one small component. At that time, neural networksâ€”the predecessors of the deep learning methods that we focus on in this bookâ€”were generally regarded as outmoded.\""
        },
        {
          "name": "examples.py",
          "type": "blob",
          "size": 1.5927734375,
          "content": "infer_from_audio_examples = [\n    [\"This is how this machine has taken my voice.\", 'English', 'no-accent', \"prompts/en-2.wav\", None, \"Wow, look at that! That's no ordinary Teddy bear!\"],\n    [\"æˆ‘å–œæ¬¢æŠ½ç”µå­çƒŸï¼Œå°¤å…¶æ˜¯é”åˆ»äº”ä»£ã€‚\", 'ä¸­æ–‡', 'no-accent', \"prompts/zh-1.wav\", None, \"ä»Šå¤©æˆ‘å¾ˆè£å¹¸ï¼Œ\"],\n    [\"ç§ã®å£°ã‚’çœŸä¼¼ã™ã‚‹ã®ã¯ãã‚“ãªã«é¢ç™½ã„ã§ã™ã‹ï¼Ÿ\", 'æ—¥æœ¬èª', 'no-accent', \"prompts/ja-2.ogg\", None, \"åˆã‚ã¾ã—ã¦ã€æœæ­¦ã‚ˆã—ã®ã§ã™ã€‚\"],\n    [\"ä½ å¯ä»¥å¬å¾—å‡ºæ¥æˆ‘æœ‰å¤šå›°ã€‚\", 'ä¸­æ–‡', 'no-accent', \"prompts/en-1.wav\", None, \"\"],\n    [\"ã“ã®æ–‡ã¯ã€ã‚¯ãƒ­ã‚¹ãƒªãƒ³ã‚¬ãƒ«åˆæˆã®ä¾‹ã§ã™ã€‚\", 'æ—¥æœ¬èª', 'no-accent', \"prompts/zh-2.wav\", None, \"\"],\n    [\"Actually, I can't speak English, but this machine helped me do it.\", 'English', 'no-accent', \"prompts/ja-1.wav\", None, \"\"],\n]\n\nmake_npz_prompt_examples = [\n    [\"Gem-trader\", \"prompts/en-2.wav\", None, \"Wow, look at that! That's no ordinary Teddy bear!\"],\n    [\"Ding Zhen\", \"prompts/zh-1.wav\", None, \"ä»Šå¤©æˆ‘å¾ˆè£å¹¸ï¼Œ\"],\n    [\"Yoshino\", \"prompts/ja-2.ogg\", None, \"åˆã‚ã¾ã—ã¦ã€æœæ­¦ã‚ˆã—ã®ã§ã™ã€‚\"],\n    [\"Sleepy-woman\", \"prompts/en-1.wav\", None, \"\"],\n    [\"Yae\", \"prompts/zh-2.wav\", None, \"\"],\n    [\"Cafe\", \"prompts/ja-1.wav\", None, \"\"],\n]\n\ninfer_from_prompt_examples = [\n    [\"A prompt contains voice, prosody and emotion information of a certain speaker.\", \"English\", \"no-accent\", \"vctk_1\", None],\n    [\"This prompt is made with an audio of three seconds.\", \"English\", \"no-accent\", \"librispeech_1\", None],\n    [\"This prompt is made with Chinese speech\", \"English\", \"no-accent\", \"seel\", None],\n]\n\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "launch-ui.py",
          "type": "blob",
          "size": 26.5439453125,
          "content": "# coding: utf-8\nimport argparse\nimport logging\nimport os\nimport pathlib\nimport time\nimport tempfile\nimport platform\nimport webbrowser\nimport sys\nprint(f\"default encoding is {sys.getdefaultencoding()},file system encoding is {sys.getfilesystemencoding()}\")\nprint(f\"You are using Python version {platform.python_version()}\")\nif(sys.version_info[0]<3 or sys.version_info[1]<7):\n    print(\"The Python version is too low and may cause problems\")\n\nif platform.system().lower() == 'windows':\n    temp = pathlib.PosixPath\n    pathlib.PosixPath = pathlib.WindowsPath\nelse:\n    temp = pathlib.WindowsPath\n    pathlib.WindowsPath = pathlib.PosixPath\nos.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n\nimport langid\nlangid.set_languages(['en', 'zh', 'ja'])\n\nimport nltk\nnltk.data.path = nltk.data.path + [os.path.join(os.getcwd(), \"nltk_data\")]\n\nimport torch\nimport torchaudio\nimport random\n\nimport numpy as np\n\nfrom data.tokenizer import (\n    AudioTokenizer,\n    tokenize_audio,\n)\nfrom data.collation import get_text_token_collater\nfrom models.vallex import VALLE\nfrom utils.g2p import PhonemeBpeTokenizer\nfrom descriptions import *\nfrom macros import *\nfrom examples import *\n\nimport gradio as gr\nimport whisper\nfrom vocos import Vocos\nimport multiprocessing\n\nthread_count = multiprocessing.cpu_count()\n\nprint(\"Use\",thread_count,\"cpu cores for computing\")\n\ntorch.set_num_threads(thread_count)\ntorch.set_num_interop_threads(thread_count)\ntorch._C._jit_set_profiling_executor(False)\ntorch._C._jit_set_profiling_mode(False)\ntorch._C._set_graph_executor_optimize(False)\n\ntext_tokenizer = PhonemeBpeTokenizer(tokenizer_path=\"./utils/g2p/bpe_69.json\")\ntext_collater = get_text_token_collater()\n\ndevice = torch.device(\"cpu\")\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\", 0)\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n# VALL-E-X model\nif not os.path.exists(\"./checkpoints/\"): os.mkdir(\"./checkpoints/\")\nif not os.path.exists(os.path.join(\"./checkpoints/\", \"vallex-checkpoint.pt\")):\n    import wget\n    try:\n        logging.info(\"Downloading model from https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt ...\")\n        # download from https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt to ./checkpoints/vallex-checkpoint.pt\n        wget.download(\"https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt\",\n                      out=\"./checkpoints/vallex-checkpoint.pt\", bar=wget.bar_adaptive)\n    except Exception as e:\n        logging.info(e)\n        raise Exception(\n            \"\\n Model weights download failed, please go to 'https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt'\"\n            \"\\n manually download model weights and put it to {} .\".format(os.getcwd() + \"\\checkpoints\"))\n\nmodel = VALLE(\n        N_DIM,\n        NUM_HEAD,\n        NUM_LAYERS,\n        norm_first=True,\n        add_prenet=False,\n        prefix_mode=PREFIX_MODE,\n        share_embedding=True,\n        nar_scale_factor=1.0,\n        prepend_bos=True,\n        num_quantizers=NUM_QUANTIZERS,\n    )\ncheckpoint = torch.load(\"./checkpoints/vallex-checkpoint.pt\", map_location='cpu')\nmissing_keys, unexpected_keys = model.load_state_dict(\n    checkpoint[\"model\"], strict=True\n)\nassert not missing_keys\nmodel.eval()\n\n# Encodec model\naudio_tokenizer = AudioTokenizer(device)\n\n# Vocos decoder\nvocos = Vocos.from_pretrained('charactr/vocos-encodec-24khz').to(device)\n\n# ASR\nif not os.path.exists(\"./whisper/\"): os.mkdir(\"./whisper/\")\ntry:\n    whisper_model = whisper.load_model(\"medium\",download_root=os.path.join(os.getcwd(), \"whisper\")).cpu()\nexcept Exception as e:\n    logging.info(e)\n    raise Exception(\n        \"\\n Whisper download failed or damaged, please go to \"\n        \"'https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt'\"\n        \"\\n manually download model and put it to {} .\".format(os.getcwd() + \"\\whisper\"))\n\n# Voice Presets\npreset_list = os.walk(\"./presets/\").__next__()[2]\npreset_list = [preset[:-4] for preset in preset_list if preset.endswith(\".npz\")]\n\ndef clear_prompts():\n    try:\n        path = tempfile.gettempdir()\n        for eachfile in os.listdir(path):\n            filename = os.path.join(path, eachfile)\n            if os.path.isfile(filename) and filename.endswith(\".npz\"):\n                lastmodifytime = os.stat(filename).st_mtime\n                endfiletime = time.time() - 60\n                if endfiletime > lastmodifytime:\n                    os.remove(filename)\n    except:\n        return\n\ndef transcribe_one(model, audio_path):\n    # load audio and pad/trim it to fit 30 seconds\n    audio = whisper.load_audio(audio_path)\n    audio = whisper.pad_or_trim(audio)\n\n    # make log-Mel spectrogram and move to the same device as the model\n    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n\n    # detect the spoken language\n    _, probs = model.detect_language(mel)\n    print(f\"Detected language: {max(probs, key=probs.get)}\")\n    lang = max(probs, key=probs.get)\n    # decode the audio\n    options = whisper.DecodingOptions(temperature=1.0, best_of=5, fp16=False if device == torch.device(\"cpu\") else True, sample_len=150)\n    result = whisper.decode(model, mel, options)\n\n    # print the recognized text\n    print(result.text)\n\n    text_pr = result.text\n    if text_pr.strip(\" \")[-1] not in \"?!.,ã€‚ï¼Œï¼Ÿï¼ã€‚ã€\":\n        text_pr += \".\"\n    return lang, text_pr\n\ndef make_npz_prompt(name, uploaded_audio, recorded_audio, transcript_content):\n    global model, text_collater, text_tokenizer, audio_tokenizer\n    clear_prompts()\n    audio_prompt = uploaded_audio if uploaded_audio is not None else recorded_audio\n    sr, wav_pr = audio_prompt\n    if not isinstance(wav_pr, torch.FloatTensor):\n        wav_pr = torch.FloatTensor(wav_pr)\n    if wav_pr.abs().max() > 1:\n        wav_pr /= wav_pr.abs().max()\n    if wav_pr.size(-1) == 2:\n        wav_pr = wav_pr[:, 0]\n    if wav_pr.ndim == 1:\n        wav_pr = wav_pr.unsqueeze(0)\n    assert wav_pr.ndim and wav_pr.size(0) == 1\n\n    if transcript_content == \"\":\n        text_pr, lang_pr = make_prompt(name, wav_pr, sr, save=False)\n    else:\n        lang_pr = langid.classify(str(transcript_content))[0]\n        lang_token = lang2token[lang_pr]\n        text_pr = f\"{lang_token}{str(transcript_content)}{lang_token}\"\n    # tokenize audio\n    encoded_frames = tokenize_audio(audio_tokenizer, (wav_pr, sr))\n    audio_tokens = encoded_frames[0][0].transpose(2, 1).cpu().numpy()\n\n    # tokenize text\n    phonemes, _ = text_tokenizer.tokenize(text=f\"{text_pr}\".strip())\n    text_tokens, enroll_x_lens = text_collater(\n        [\n            phonemes\n        ]\n    )\n\n    message = f\"Detected language: {lang_pr}\\n Detected text {text_pr}\\n\"\n\n    # save as npz file\n    np.savez(os.path.join(tempfile.gettempdir(), f\"{name}.npz\"),\n             audio_tokens=audio_tokens, text_tokens=text_tokens, lang_code=lang2code[lang_pr])\n    return message, os.path.join(tempfile.gettempdir(), f\"{name}.npz\")\n\n\ndef make_prompt(name, wav, sr, save=True):\n    global whisper_model\n    whisper_model.to(device)\n    if not isinstance(wav, torch.FloatTensor):\n        wav = torch.tensor(wav)\n    if wav.abs().max() > 1:\n        wav /= wav.abs().max()\n    if wav.size(-1) == 2:\n        wav = wav.mean(-1, keepdim=False)\n    if wav.ndim == 1:\n        wav = wav.unsqueeze(0)\n    assert wav.ndim and wav.size(0) == 1\n    torchaudio.save(f\"./prompts/{name}.wav\", wav, sr)\n    lang, text = transcribe_one(whisper_model, f\"./prompts/{name}.wav\")\n    lang_token = lang2token[lang]\n    text = lang_token + text + lang_token\n    with open(f\"./prompts/{name}.txt\", 'w', encoding='utf-8') as f:\n        f.write(text)\n    if not save:\n        os.remove(f\"./prompts/{name}.wav\")\n        os.remove(f\"./prompts/{name}.txt\")\n\n    whisper_model.cpu()\n    torch.cuda.empty_cache()\n    return text, lang\n\n@torch.no_grad()\ndef infer_from_audio(text, language, accent, audio_prompt, record_audio_prompt, transcript_content):\n    global model, text_collater, text_tokenizer, audio_tokenizer\n    audio_prompt = audio_prompt if audio_prompt is not None else record_audio_prompt\n    sr, wav_pr = audio_prompt\n    if not isinstance(wav_pr, torch.FloatTensor):\n        wav_pr = torch.FloatTensor(wav_pr)\n    if wav_pr.abs().max() > 1:\n        wav_pr /= wav_pr.abs().max()\n    if wav_pr.size(-1) == 2:\n        wav_pr = wav_pr[:, 0]\n    if wav_pr.ndim == 1:\n        wav_pr = wav_pr.unsqueeze(0)\n    assert wav_pr.ndim and wav_pr.size(0) == 1\n\n    if transcript_content == \"\":\n        text_pr, lang_pr = make_prompt('dummy', wav_pr, sr, save=False)\n    else:\n        lang_pr = langid.classify(str(transcript_content))[0]\n        lang_token = lang2token[lang_pr]\n        text_pr = f\"{lang_token}{str(transcript_content)}{lang_token}\"\n\n    if language == 'auto-detect':\n        lang_token = lang2token[langid.classify(text)[0]]\n    else:\n        lang_token = langdropdown2token[language]\n    lang = token2lang[lang_token]\n    text = lang_token + text + lang_token\n\n    # onload model\n    model.to(device)\n\n    # tokenize audio\n    encoded_frames = tokenize_audio(audio_tokenizer, (wav_pr, sr))\n    audio_prompts = encoded_frames[0][0].transpose(2, 1).to(device)\n\n    # tokenize text\n    logging.info(f\"synthesize text: {text}\")\n    phone_tokens, langs = text_tokenizer.tokenize(text=f\"_{text}\".strip())\n    text_tokens, text_tokens_lens = text_collater(\n        [\n            phone_tokens\n        ]\n    )\n\n    enroll_x_lens = None\n    if text_pr:\n        text_prompts, _ = text_tokenizer.tokenize(text=f\"{text_pr}\".strip())\n        text_prompts, enroll_x_lens = text_collater(\n            [\n                text_prompts\n            ]\n        )\n    text_tokens = torch.cat([text_prompts, text_tokens], dim=-1)\n    text_tokens_lens += enroll_x_lens\n    lang = lang if accent == \"no-accent\" else token2lang[langdropdown2token[accent]]\n    encoded_frames = model.inference(\n        text_tokens.to(device),\n        text_tokens_lens.to(device),\n        audio_prompts,\n        enroll_x_lens=enroll_x_lens,\n        top_k=-100,\n        temperature=1,\n        prompt_language=lang_pr,\n        text_language=langs if accent == \"no-accent\" else lang,\n        best_of=5,\n    )\n    # Decode with Vocos\n    frames = encoded_frames.permute(2,0,1)\n    features = vocos.codes_to_features(frames)\n    samples = vocos.decode(features, bandwidth_id=torch.tensor([2], device=device))\n\n    # offload model\n    model.to('cpu')\n    torch.cuda.empty_cache()\n\n    message = f\"text prompt: {text_pr}\\nsythesized text: {text}\"\n    return message, (24000, samples.squeeze(0).cpu().numpy())\n\n@torch.no_grad()\ndef infer_from_prompt(text, language, accent, preset_prompt, prompt_file):\n    clear_prompts()\n    model.to(device)\n    # text to synthesize\n    if language == 'auto-detect':\n        lang_token = lang2token[langid.classify(text)[0]]\n    else:\n        lang_token = langdropdown2token[language]\n    lang = token2lang[lang_token]\n    text = lang_token + text + lang_token\n\n    # load prompt\n    if prompt_file is not None:\n        prompt_data = np.load(prompt_file.name)\n    else:\n        prompt_data = np.load(os.path.join(\"./presets/\", f\"{preset_prompt}.npz\"))\n    audio_prompts = prompt_data['audio_tokens']\n    text_prompts = prompt_data['text_tokens']\n    lang_pr = prompt_data['lang_code']\n    lang_pr = code2lang[int(lang_pr)]\n\n    # numpy to tensor\n    audio_prompts = torch.tensor(audio_prompts).type(torch.int32).to(device)\n    text_prompts = torch.tensor(text_prompts).type(torch.int32)\n\n    enroll_x_lens = text_prompts.shape[-1]\n    logging.info(f\"synthesize text: {text}\")\n    phone_tokens, langs = text_tokenizer.tokenize(text=f\"_{text}\".strip())\n    text_tokens, text_tokens_lens = text_collater(\n        [\n            phone_tokens\n        ]\n    )\n    text_tokens = torch.cat([text_prompts, text_tokens], dim=-1)\n    text_tokens_lens += enroll_x_lens\n    # accent control\n    lang = lang if accent == \"no-accent\" else token2lang[langdropdown2token[accent]]\n    encoded_frames = model.inference(\n        text_tokens.to(device),\n        text_tokens_lens.to(device),\n        audio_prompts,\n        enroll_x_lens=enroll_x_lens,\n        top_k=-100,\n        temperature=1,\n        prompt_language=lang_pr,\n        text_language=langs if accent == \"no-accent\" else lang,\n        best_of=5,\n    )\n    # Decode with Vocos\n    frames = encoded_frames.permute(2,0,1)\n    features = vocos.codes_to_features(frames)\n    samples = vocos.decode(features, bandwidth_id=torch.tensor([2], device=device))\n\n    model.to('cpu')\n    torch.cuda.empty_cache()\n\n    message = f\"sythesized text: {text}\"\n    return message, (24000, samples.squeeze(0).cpu().numpy())\n\n\nfrom utils.sentence_cutter import split_text_into_sentences\n@torch.no_grad()\ndef infer_long_text(text, preset_prompt, prompt=None, language='auto', accent='no-accent'):\n    \"\"\"\n    For long audio generation, two modes are available.\n    fixed-prompt: This mode will keep using the same prompt the user has provided, and generate audio sentence by sentence.\n    sliding-window: This mode will use the last sentence as the prompt for the next sentence, but has some concern on speaker maintenance.\n    \"\"\"\n    mode = 'fixed-prompt'\n    global model, audio_tokenizer, text_tokenizer, text_collater\n    model.to(device)\n    if (prompt is None or prompt == \"\") and preset_prompt == \"\":\n        mode = 'sliding-window'  # If no prompt is given, use sliding-window mode\n    sentences = split_text_into_sentences(text)\n    # detect language\n    if language == \"auto-detect\":\n        language = langid.classify(text)[0]\n    else:\n        language = token2lang[langdropdown2token[language]]\n\n    # if initial prompt is given, encode it\n    if prompt is not None and prompt != \"\":\n        # load prompt\n        prompt_data = np.load(prompt.name)\n        audio_prompts = prompt_data['audio_tokens']\n        text_prompts = prompt_data['text_tokens']\n        lang_pr = prompt_data['lang_code']\n        lang_pr = code2lang[int(lang_pr)]\n\n        # numpy to tensor\n        audio_prompts = torch.tensor(audio_prompts).type(torch.int32).to(device)\n        text_prompts = torch.tensor(text_prompts).type(torch.int32)\n    elif preset_prompt is not None and preset_prompt != \"\":\n        prompt_data = np.load(os.path.join(\"./presets/\", f\"{preset_prompt}.npz\"))\n        audio_prompts = prompt_data['audio_tokens']\n        text_prompts = prompt_data['text_tokens']\n        lang_pr = prompt_data['lang_code']\n        lang_pr = code2lang[int(lang_pr)]\n\n        # numpy to tensor\n        audio_prompts = torch.tensor(audio_prompts).type(torch.int32).to(device)\n        text_prompts = torch.tensor(text_prompts).type(torch.int32)\n    else:\n        audio_prompts = torch.zeros([1, 0, NUM_QUANTIZERS]).type(torch.int32).to(device)\n        text_prompts = torch.zeros([1, 0]).type(torch.int32)\n        lang_pr = language if language != 'mix' else 'en'\n    if mode == 'fixed-prompt':\n        complete_tokens = torch.zeros([1, NUM_QUANTIZERS, 0]).type(torch.LongTensor).to(device)\n        for text in sentences:\n            text = text.replace(\"\\n\", \"\").strip(\" \")\n            if text == \"\":\n                continue\n            lang_token = lang2token[language]\n            lang = token2lang[lang_token]\n            text = lang_token + text + lang_token\n\n            enroll_x_lens = text_prompts.shape[-1]\n            logging.info(f\"synthesize text: {text}\")\n            phone_tokens, langs = text_tokenizer.tokenize(text=f\"_{text}\".strip())\n            text_tokens, text_tokens_lens = text_collater(\n                [\n                    phone_tokens\n                ]\n            )\n            text_tokens = torch.cat([text_prompts, text_tokens], dim=-1)\n            text_tokens_lens += enroll_x_lens\n            # accent control\n            lang = lang if accent == \"no-accent\" else token2lang[langdropdown2token[accent]]\n            encoded_frames = model.inference(\n                text_tokens.to(device),\n                text_tokens_lens.to(device),\n                audio_prompts,\n                enroll_x_lens=enroll_x_lens,\n                top_k=-100,\n                temperature=1,\n                prompt_language=lang_pr,\n                text_language=langs if accent == \"no-accent\" else lang,\n                best_of=5,\n            )\n            complete_tokens = torch.cat([complete_tokens, encoded_frames.transpose(2, 1)], dim=-1)\n        # Decode with Vocos\n        frames = complete_tokens.permute(1, 0, 2)\n        features = vocos.codes_to_features(frames)\n        samples = vocos.decode(features, bandwidth_id=torch.tensor([2], device=device))\n\n        model.to('cpu')\n        message = f\"Cut into {len(sentences)} sentences\"\n        return message, (24000, samples.squeeze(0).cpu().numpy())\n    elif mode == \"sliding-window\":\n        complete_tokens = torch.zeros([1, NUM_QUANTIZERS, 0]).type(torch.LongTensor).to(device)\n        original_audio_prompts = audio_prompts\n        original_text_prompts = text_prompts\n        for text in sentences:\n            text = text.replace(\"\\n\", \"\").strip(\" \")\n            if text == \"\":\n                continue\n            lang_token = lang2token[language]\n            lang = token2lang[lang_token]\n            text = lang_token + text + lang_token\n\n            enroll_x_lens = text_prompts.shape[-1]\n            logging.info(f\"synthesize text: {text}\")\n            phone_tokens, langs = text_tokenizer.tokenize(text=f\"_{text}\".strip())\n            text_tokens, text_tokens_lens = text_collater(\n                [\n                    phone_tokens\n                ]\n            )\n            text_tokens = torch.cat([text_prompts, text_tokens], dim=-1)\n            text_tokens_lens += enroll_x_lens\n            # accent control\n            lang = lang if accent == \"no-accent\" else token2lang[langdropdown2token[accent]]\n            encoded_frames = model.inference(\n                text_tokens.to(device),\n                text_tokens_lens.to(device),\n                audio_prompts,\n                enroll_x_lens=enroll_x_lens,\n                top_k=-100,\n                temperature=1,\n                prompt_language=lang_pr,\n                text_language=langs if accent == \"no-accent\" else lang,\n                best_of=5,\n            )\n            complete_tokens = torch.cat([complete_tokens, encoded_frames.transpose(2, 1)], dim=-1)\n            if torch.rand(1) < 1.0:\n                audio_prompts = encoded_frames[:, :, -NUM_QUANTIZERS:]\n                text_prompts = text_tokens[:, enroll_x_lens:]\n            else:\n                audio_prompts = original_audio_prompts\n                text_prompts = original_text_prompts\n        # Decode with Vocos\n        frames = complete_tokens.permute(1, 0, 2)\n        features = vocos.codes_to_features(frames)\n        samples = vocos.decode(features, bandwidth_id=torch.tensor([2], device=device))\n\n        model.to('cpu')\n        message = f\"Cut into {len(sentences)} sentences\"\n        return message, (24000, samples.squeeze(0).cpu().numpy())\n    else:\n        raise ValueError(f\"No such mode {mode}\")\n\n\ndef main():\n    app = gr.Blocks(title=\"VALL-E X\")\n    with app:\n        gr.Markdown(top_md)\n        with gr.Tab(\"Infer from audio\"):\n            gr.Markdown(infer_from_audio_md)\n            with gr.Row():\n                with gr.Column():\n\n                    textbox = gr.TextArea(label=\"Text\",\n                                          placeholder=\"Type your sentence here\",\n                                          value=\"Welcome back, Master. What can I do for you today?\", elem_id=f\"tts-input\")\n                    language_dropdown = gr.Dropdown(choices=['auto-detect', 'English', 'ä¸­æ–‡', 'æ—¥æœ¬èª'], value='auto-detect', label='language')\n                    accent_dropdown = gr.Dropdown(choices=['no-accent', 'English', 'ä¸­æ–‡', 'æ—¥æœ¬èª'], value='no-accent', label='accent')\n                    textbox_transcript = gr.TextArea(label=\"Transcript\",\n                                          placeholder=\"Write transcript here. (leave empty to use whisper)\",\n                                          value=\"\", elem_id=f\"prompt-name\")\n                    upload_audio_prompt = gr.Audio(label='uploaded audio prompt', source='upload', interactive=True)\n                    record_audio_prompt = gr.Audio(label='recorded audio prompt', source='microphone', interactive=True)\n                with gr.Column():\n                    text_output = gr.Textbox(label=\"Message\")\n                    audio_output = gr.Audio(label=\"Output Audio\", elem_id=\"tts-audio\")\n                    btn = gr.Button(\"Generate!\")\n                    btn.click(infer_from_audio,\n                              inputs=[textbox, language_dropdown, accent_dropdown, upload_audio_prompt, record_audio_prompt, textbox_transcript],\n                              outputs=[text_output, audio_output])\n                    textbox_mp = gr.TextArea(label=\"Prompt name\",\n                                          placeholder=\"Name your prompt here\",\n                                          value=\"prompt_1\", elem_id=f\"prompt-name\")\n                    btn_mp = gr.Button(\"Make prompt!\")\n                    prompt_output = gr.File(interactive=False)\n                    btn_mp.click(make_npz_prompt,\n                                inputs=[textbox_mp, upload_audio_prompt, record_audio_prompt, textbox_transcript],\n                                outputs=[text_output, prompt_output])\n            gr.Examples(examples=infer_from_audio_examples,\n                        inputs=[textbox, language_dropdown, accent_dropdown, upload_audio_prompt, record_audio_prompt, textbox_transcript],\n                        outputs=[text_output, audio_output],\n                        fn=infer_from_audio,\n                        cache_examples=False,)\n        with gr.Tab(\"Make prompt\"):\n            gr.Markdown(make_prompt_md)\n            with gr.Row():\n                with gr.Column():\n                    textbox2 = gr.TextArea(label=\"Prompt name\",\n                                          placeholder=\"Name your prompt here\",\n                                          value=\"prompt_1\", elem_id=f\"prompt-name\")\n                    # æ·»åŠ é€‰æ‹©è¯­è¨€å’Œè¾“å…¥å°æœ¬çš„åœ°æ–¹\n                    textbox_transcript2 = gr.TextArea(label=\"Transcript\",\n                                          placeholder=\"Write transcript here. (leave empty to use whisper)\",\n                                          value=\"\", elem_id=f\"prompt-name\")\n                    upload_audio_prompt_2 = gr.Audio(label='uploaded audio prompt', source='upload', interactive=True)\n                    record_audio_prompt_2 = gr.Audio(label='recorded audio prompt', source='microphone', interactive=True)\n                with gr.Column():\n                    text_output_2 = gr.Textbox(label=\"Message\")\n                    prompt_output_2 = gr.File(interactive=False)\n                    btn_2 = gr.Button(\"Make!\")\n                    btn_2.click(make_npz_prompt,\n                              inputs=[textbox2, upload_audio_prompt_2, record_audio_prompt_2, textbox_transcript2],\n                              outputs=[text_output_2, prompt_output_2])\n            gr.Examples(examples=make_npz_prompt_examples,\n                        inputs=[textbox2, upload_audio_prompt_2, record_audio_prompt_2, textbox_transcript2],\n                        outputs=[text_output_2, prompt_output_2],\n                        fn=make_npz_prompt,\n                        cache_examples=False,)\n        with gr.Tab(\"Infer from prompt\"):\n            gr.Markdown(infer_from_prompt_md)\n            with gr.Row():\n                with gr.Column():\n                    textbox_3 = gr.TextArea(label=\"Text\",\n                                          placeholder=\"Type your sentence here\",\n                                          value=\"Welcome back, Master. What can I do for you today?\", elem_id=f\"tts-input\")\n                    language_dropdown_3 = gr.Dropdown(choices=['auto-detect', 'English', 'ä¸­æ–‡', 'æ—¥æœ¬èª', 'Mix'], value='auto-detect',\n                                                    label='language')\n                    accent_dropdown_3 = gr.Dropdown(choices=['no-accent', 'English', 'ä¸­æ–‡', 'æ—¥æœ¬èª'], value='no-accent',\n                                                  label='accent')\n                    preset_dropdown_3 = gr.Dropdown(choices=preset_list, value=None, label='Voice preset')\n                    prompt_file = gr.File(file_count='single', file_types=['.npz'], interactive=True)\n                with gr.Column():\n                    text_output_3 = gr.Textbox(label=\"Message\")\n                    audio_output_3 = gr.Audio(label=\"Output Audio\", elem_id=\"tts-audio\")\n                    btn_3 = gr.Button(\"Generate!\")\n                    btn_3.click(infer_from_prompt,\n                              inputs=[textbox_3, language_dropdown_3, accent_dropdown_3, preset_dropdown_3, prompt_file],\n                              outputs=[text_output_3, audio_output_3])\n            gr.Examples(examples=infer_from_prompt_examples,\n                        inputs=[textbox_3, language_dropdown_3, accent_dropdown_3, preset_dropdown_3, prompt_file],\n                        outputs=[text_output_3, audio_output_3],\n                        fn=infer_from_prompt,\n                        cache_examples=False,)\n        with gr.Tab(\"Infer long text\"):\n            gr.Markdown(\"This is a long text generation demo. You can use this to generate long audio. \")\n            with gr.Row():\n                with gr.Column():\n                    textbox_4 = gr.TextArea(label=\"Text\",\n                                          placeholder=\"Type your sentence here\",\n                                          value=long_text_example, elem_id=f\"tts-input\")\n                    language_dropdown_4 = gr.Dropdown(choices=['auto-detect', 'English', 'ä¸­æ–‡', 'æ—¥æœ¬èª'], value='auto-detect',\n                                                    label='language')\n                    accent_dropdown_4 = gr.Dropdown(choices=['no-accent', 'English', 'ä¸­æ–‡', 'æ—¥æœ¬èª'], value='no-accent',\n                                                    label='accent')\n                    preset_dropdown_4 = gr.Dropdown(choices=preset_list, value=None, label='Voice preset')\n                    prompt_file_4 = gr.File(file_count='single', file_types=['.npz'], interactive=True)\n                with gr.Column():\n                    text_output_4 = gr.TextArea(label=\"Message\")\n                    audio_output_4 = gr.Audio(label=\"Output Audio\", elem_id=\"tts-audio\")\n                    btn_4 = gr.Button(\"Generate!\")\n                    btn_4.click(infer_long_text,\n                              inputs=[textbox_4, preset_dropdown_4, prompt_file_4, language_dropdown_4, accent_dropdown_4],\n                              outputs=[text_output_4, audio_output_4])\n\n    webbrowser.open(\"http://127.0.0.1:7860\")\n    app.launch()\n\nif __name__ == \"__main__\":\n    formatter = (\n        \"%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(message)s\"\n    )\n    logging.basicConfig(format=formatter, level=logging.INFO)\n    main()\n"
        },
        {
          "name": "macros.py",
          "type": "blob",
          "size": 0.4814453125,
          "content": "NUM_LAYERS = 12\nNUM_HEAD = 16\nN_DIM = 1024\nPREFIX_MODE = 1\nNUM_QUANTIZERS = 8\nSAMPLE_RATE = 24000\n\nlang2token = {\n    'zh': \"[ZH]\",\n    'ja': \"[JA]\",\n    \"en\": \"[EN]\",\n    'mix': \"\",\n}\n\nlang2code = {\n    'zh': 0,\n    'ja': 1,\n    \"en\": 2,\n}\n\ntoken2lang = {\n    '[ZH]': \"zh\",\n    '[JA]': \"ja\",\n    \"[EN]\": \"en\",\n    \"\": \"mix\"\n}\n\ncode2lang = {\n    0: 'zh',\n    1: 'ja',\n    2: \"en\",\n}\n\nlangdropdown2token = {\n    'English': \"[EN]\",\n    'ä¸­æ–‡': \"[ZH]\",\n    'æ—¥æœ¬èª': \"[JA]\",\n    'Mix': \"\",\n}"
        },
        {
          "name": "model-card.md",
          "type": "blob",
          "size": 1.3916015625,
          "content": "# Model Card: VALL-E X\n\n**Author**: [Songting](https://github.com/Plachtaa).<br>\n<br>\nThis is the official codebase for running open-sourced VALL-E X.\n\nThe following is additional information about the models released here.\n\n## Model Details\n\nVALL-E X is a series of two transformer models that turn text into audio.\n\n### Phoneme to acoustic tokens\n - Input: IPAs converted from input text by a rule-based G2P tool.\n - Output: tokens from the first codebook of the [EnCodec Codec](https://github.com/facebookresearch/encodec) from facebook\n\n### Coarse to fine tokens\n - Input: IPAs converted from input text by a rule-based G2P tool & the first codebook from EnCodec\n - Output: 8 codebooks from EnCodec\n\n### Architecture\n|          Model           | Parameters | Attention  | Output Vocab size |  \n|:------------------------:|:----------:|------------|:-----------------:|\n|         G2P tool         |     -      | -          |        69         |\n| Phoneme to coarse tokens |   150 M    | Causal     |     1x 1,024      |\n|  Coarse to fine tokens   |   150 M    | Non-causal |     7x 1,024      |\n\n### Release date\nAugust 2023\n\n## Broader Implications\nWe anticipate that this model's text to audio capabilities can be used to improve accessbility tools in a variety of languages. \nStraightforward improvements will allow models to run faster than realtime, rendering them useful for applications such as virtual assistants. "
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "modules",
          "type": "tree",
          "content": null
        },
        {
          "name": "nltk_data",
          "type": "tree",
          "content": null
        },
        {
          "name": "presets",
          "type": "tree",
          "content": null
        },
        {
          "name": "prompts",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.220703125,
          "content": "soundfile\nnumpy\ntorch\ntorchvision\ntorchaudio\ntokenizers\nencodec\nlangid\nwget\nunidecode\npyopenjtalk-prebuilt\npypinyin\ninflect\ncn2an\njieba\neng_to_ipa\nopenai-whisper\nmatplotlib\ngradio==3.41.2\nnltk\nsudachipy\nsudachidict_core\nvocos\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}