{
  "metadata": {
    "timestamp": 1736560442252,
    "page": 16,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "cumulo-autumn/StreamDiffusion",
      "stars": 9852,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0439453125,
          "content": "*\n!README.md\n!src/\n!pyproject.toml\n!setup.py\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.611328125,
          "content": "# https://github.com/github/gitignore/blob/main/Python.gitignore\n\n.vscode/\nengines/\noutput/\n*.csv\n*.mp4\n*.safetensors\nresult_lcm.png\nmodel.ckpt\n*.png\n!assets/img2img_example.png\n!assets/cfg_conparision.png\n!images/inputs/input.png\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n\n# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.\n\n# dependencies\n*node_modules\n*/.pnp\n.pnp.js\n\n# testing\n*/coverage\n\n# production\n*/build\n\n# misc\n.DS_Store\n.env.local\n.env.development.local\n.env.test.local\n.env.production.local\n\nnpm-debug.log*\nyarn-debug.log*\nyarn-error.log*\n\n*.venv\n\n__pycache__/\n*.py[cod]\n*$py.class\n\nmodels/RealESR*\n*.safetensors\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.1513671875,
          "content": "FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\n\nENV DEBIAN_FRONTEND=noninteractive PIP_PREFER_BINARY=1 \\\n        CUDA_HOME=/usr/local/cuda-11.8 TORCH_CUDA_ARCH_LIST=\"8.6\"\nRUN rm /bin/sh && ln -s /bin/bash /bin/sh\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n        make \\\n        wget \\\n        tar \\\n        build-essential \\\n        libgl1-mesa-dev \\\n        curl \\\n        unzip \\\n        git \\\n        python3-dev \\\n        python3-pip \\\n        libglib2.0-0 \\\n    && apt clean && rm -rf /var/lib/apt/lists/* \\\n    && ln -s /usr/bin/python3 /usr/bin/python\n\nRUN echo \"export PATH=/usr/local/cuda/bin:$PATH\" >> /etc/bash.bashrc \\\n    && echo \"export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\" >> /etc/bash.bashrc \\\n    && echo \"export CUDA_HOME=/usr/local/cuda-11.8\" >> /etc/bash.bashrc\n\nRUN pip3 install \\\n    torch==2.1.0 \\\n    torchvision==0.16.0 \\\n    xformers \\\n    --index-url https://download.pytorch.org/whl/cu118\n\nCOPY . /streamdiffusion\nWORKDIR /streamdiffusion\n\nRUN python setup.py develop easy_install streamdiffusion[tensorrt] \\\n    && python -m streamdiffusion.tools.install-tensorrt\n\nWORKDIR /home/ubuntu/streamdiffusion\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README-ja.md",
          "type": "blob",
          "size": 13.01953125,
          "content": "# StreamDiffusion\n\n[English](./README.md) | [日本語](./README-ja.md) | [한국어](./README-ko.md)\n\n<p align=\"center\">\n  <img src=\"./assets/demo_07.gif\" width=90%>\n  <img src=\"./assets/demo_09.gif\" width=90%>\n</p>\n\n# StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation\n\n**Authors:** [Akio Kodaira](https://www.linkedin.com/in/akio-kodaira-1a7b98252/), [Chenfeng Xu](https://www.chenfengx.com/), Toshiki Hazama, [Takanori Yoshimoto](https://twitter.com/__ramu0e__), [Kohei Ohno](https://www.linkedin.com/in/kohei--ohno/), [Shogo Mitsuhori](https://me.ddpn.world/), [Soichi Sugano](https://twitter.com/toni_nimono), [Hanying Cho](https://twitter.com/hanyingcl), [Zhijian Liu](https://zhijianliu.com/), [Kurt Keutzer](https://scholar.google.com/citations?hl=en&user=ID9QePIAAAAJ)\n\nStreamDiffusion は、リアルタイム画像生成を実現するために最適化されたパイプラインです。従来の画像生成パイプラインと比べて飛躍的な速度向上を実現しました。\n\n[![arXiv](https://img.shields.io/badge/arXiv-2312.12491-b31b1b.svg)](https://arxiv.org/abs/2312.12491)\n[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-yellow)](https://huggingface.co/papers/2312.12491)\n\nStreamDiffusion の開発にあたり、丁寧なサポート、有意義なフィードバックと議論をしていただいた [Taku Fujimoto](https://twitter.com/AttaQjp) 様と [Radamés Ajna](https://twitter.com/radamar) 様、そして Hugging Face チームに心より感謝いたします。\n\n## 主な特徴\n\n1. **Stream Batch**\n\n   - デノイジングバッチ処理によるデータ処理の効率化\n\n2. **Residual Classifier-Free Guidance** - [詳細](#residual-cfg-rcfg)\n\n   - 計算の冗長性を最小限に抑える CFG\n\n3. **Stochastic Similarity Filter** - [詳細](#stochastic-similarity-filter)\n\n   - 類似度によるフィルタリングで GPU の使用効率を最大化\n\n4. **IO Queues**\n\n   - 入出力操作を効率的に管理し、よりスムーズな実行を実現\n\n5. **Pre-Computation for KV-Caches**\n\n   - 高速処理のためのキャッシュ戦略を最適化します。\n\n6. **Model Acceleration Tools**\n   - モデルの最適化とパフォーマンス向上のための様々なツールの利用\n\n**GPU: RTX 4090**, **CPU: Core i9-13900K**, **OS: Ubuntu 22.04.3 LTS**　環境で StreamDiffusion pipeline を用いて 画像を生成した場合、以下のような結果が得られました。\n\n|            model            | Denoising Step | fps on Txt2Img | fps on Img2Img |\n| :-------------------------: | :------------: | :------------: | :------------: |\n|          SD-turbo           |       1        |     106.16     |     93.897     |\n| LCM-LoRA <br>+<br> KohakuV2 |       4        |     38.023     |     37.133     |\n\n_Feel free to explore each feature by following the provided links to learn more about StreamDiffusion's capabilities. If you find it helpful, please consider citing our work:_\n\n```bash\n@article{kodaira2023streamdiffusion,\n      title={StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation},\n      author={Akio Kodaira and Chenfeng Xu and Toshiki Hazama and Takanori Yoshimoto and Kohei Ohno and Shogo Mitsuhori and Soichi Sugano and Hanying Cho and Zhijian Liu and Kurt Keutzer},\n      year={2023},\n      eprint={2312.12491},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n## インストール\n\n### 環境構築\n\n### Step0: リポジトリのクローン\n\n```bash\ngit clone https://github.com/cumulo-autumn/StreamDiffusion.git\n```\n\n### Step1: 仮想環境構築\n\nanaconda、pip、または後述する Docker で仮想環境を作成します。\n\nanaconda を用いる場合\n\n```bash\nconda create -n streamdiffusion python=3.10\nconda activate streamdiffusion\n```\n\npip を用いる場合\n\n```cmd\npython -m venv .venv\n# Windows\n.\\.venv\\Scripts\\activate\n# Linux\nsource .venv/bin/activate\n```\n\n### Step2: PyTorch のインストール\n\n使用する GPU の CUDA バージョンに合わせて PyTorch をインストールしてください。\n\nCUDA 11.8\n\n```bash\npip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu118\n```\n\nCUDA 12.1\n\n```bash\npip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu121\n```\n\n詳しくは[こちら](https://pytorch.org/)\n\n### Step3: StreamDiffusion のインストール\n\nStreamDiffusion をインストール\n\n#### ユーザー向け\n\n```bash\n#最新バージョン (推奨)\npip install git+https://github.com/cumulo-autumn/StreamDiffusion.git@main#egg=streamdiffusion[tensorrt]\n\n\n#もしくは\n\n\n#リリースバージョン\npip install streamdiffusion[tensorrt]\n```\n\nTensorRT 拡張をインストール\n\n```bash\npython -m streamdiffusion.tools.install-tensorrt\n```\n\n(Only for Windows)リリースバージョン(`pip install streamdiffusion[tensorrt]`)では pywin32 のインストールが別途必要です。\n\n```bash\npip install --force-reinstall pywin32\n```\n\n#### 開発者向け\n\n```bash\npython setup.py develop easy_install streamdiffusion[tensorrt]\npython -m streamdiffusion.tools.install-tensorrt\n```\n\n### Docker の場合(TensorRT 対応)\n\n```bash\ngit clone https://github.com/cumulo-autumn/StreamDiffusion.git\ncd StreamDiffusion\ndocker build -t stream-diffusion:latest -f Dockerfile .\ndocker run --gpus all -it -v $(pwd):/home/ubuntu/streamdiffusion stream-diffusion:latest\n```\n\n## 動作例\n\n[`examples`](./examples) からサンプルを実行できます。\n\n| ![画像3](./assets/demo_02.gif) | ![画像4](./assets/demo_03.gif) |\n| :----------------------------: | :----------------------------: |\n| ![画像5](./assets/demo_04.gif) | ![画像6](./assets/demo_05.gif) |\n\n具体的な詳細設定及びユーザカスタマイズは以下をお読みください。\n\n## Real-Time Txt2Img Demo\n\nリアルタイムの txt2img デモは [`demo/realtime-txt2img`](./demo/realtime-txt2img)にあります。\n\n<p align=\"center\">\n  <img src=\"./assets/demo_01.gif\" width=80%>\n</p>\n\n## Real-Time Img2Img Demo\n\nWeb カメラを使ったリアルタイムの img2img デモは [`demo/realtime-img2img`](./demo/realtime-img2img)にあります。\n\n<p align=\"center\">\n  <img src=\"./assets/img2img1.gif\" width=100%>\n</p>\n\n## 使用例\n\nシンプルな StreamDiffusion の使用例を取り上げる. より詳細かつ様々な使用例は[`examples`](./examples)を参照してください。\n\n### Image-to-Image\n\n```python\nimport torch\nfrom diffusers import AutoencoderTiny, StableDiffusionPipeline\nfrom diffusers.utils import load_image\n\nfrom streamdiffusion import StreamDiffusion\nfrom streamdiffusion.image_utils import postprocess_image\n\npipe = StableDiffusionPipeline.from_pretrained(\"KBlueLeaf/kohaku-v2.1\").to(\n    device=torch.device(\"cuda\"),\n    dtype=torch.float16,\n)\n\n# Diffusers pipelineをStreamDiffusionにラップ\nstream = StreamDiffusion(\n    pipe,\n    t_index_list=[32, 45],\n    torch_dtype=torch.float16,\n)\n\n# 読み込んだモデルがLCMでなければマージする\nstream.load_lcm_lora()\nstream.fuse_lora()\n# Tiny VAEで高速化\nstream.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\").to(device=pipe.device, dtype=pipe.dtype)\n# xformersで高速化\npipe.enable_xformers_memory_efficient_attention()\n\n\nprompt = \"1girl with dog hair, thick frame glasses\"\n# streamを準備する\nstream.prepare(prompt)\n\n# 画像を読み込む\ninit_image = load_image(\"assets/img2img_example.png\").resize((512, 512))\n\n# Warmup >= len(t_index_list) x frame_buffer_size\nfor _ in range(2):\n    stream(init_image)\n\n# 実行\nwhile True:\n    x_output = stream(init_image)\n    postprocess_image(x_output, output_type=\"pil\")[0].show()\n    input_response = input(\"Press Enter to continue or type 'stop' to exit: \")\n    if input_response == \"stop\":\n        break\n```\n\n### Text-to-Image\n\n```python\nimport torch\nfrom diffusers import AutoencoderTiny, StableDiffusionPipeline\n\nfrom streamdiffusion import StreamDiffusion\nfrom streamdiffusion.image_utils import postprocess_image\n\npipe = StableDiffusionPipeline.from_pretrained(\"KBlueLeaf/kohaku-v2.1\").to(\n    device=torch.device(\"cuda\"),\n    dtype=torch.float16,\n)\n\n# Diffusers pipelineをStreamDiffusionにラップ\n# text2imageにおいてはより長いステップ(len(t_index_list))を要求する\n# text2imageにおいてはcfg_type=\"none\"が推奨される\nstream = StreamDiffusion(\n    pipe,\n    t_index_list=[0, 16, 32, 45],\n    torch_dtype=torch.float16,\n    cfg_type=\"none\",\n)\n\n# 読み込んだモデルがLCMでなければマージする\nstream.load_lcm_lora()\nstream.fuse_lora()\n# Tiny VAEで高速化\nstream.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\").to(device=pipe.device, dtype=pipe.dtype)\n# xformersで高速化\npipe.enable_xformers_memory_efficient_attention()\n\n\nprompt = \"1girl with dog hair, thick frame glasses\"\n# streamを準備する\nstream.prepare(prompt)\n\n# Warmup >= len(t_index_list) x frame_buffer_size\nfor _ in range(4):\n    stream()\n\n# 実行\nwhile True:\n    x_output = stream.txt2img()\n    postprocess_image(x_output, output_type=\"pil\")[0].show()\n    input_response = input(\"Press Enter to continue or type 'stop' to exit: \")\n    if input_response == \"stop\":\n        break\n```\n\nSD-Turbo を使用するとさらに高速化も可能である\n\n### More fast generation\n\n上のコードの以下の部分を書き換えることで、より高速な生成が可能である。\n\n```python\npipe.enable_xformers_memory_efficient_attention()\n```\n\n以下に書き換える\n\n```python\nfrom streamdiffusion.acceleration.tensorrt import accelerate_with_tensorrt\n\nstream = accelerate_with_tensorrt(\n    stream, \"engines\", max_batch_size=2,\n)\n```\n\nただし、TensorRT のインストールとエンジンのビルドに時間を要する。\n\n## オプション\n\n### Stochastic Similarity Filter\n\n![demo](assets/demo_06.gif)\n\nStochastic Similarity Filter は動画入力時、前フレームからあまり変化しないときの変換処理を減らすことで、上の GIF の赤枠の様に GPU の負荷を軽減する。使用方法は以下のとおりである。\n\n```python\nstream = StreamDiffusion(\n    pipe,\n    [32, 45],\n    torch_dtype=torch.float16,\n)\nstream.enable_similar_image_filter(\n    similar_image_filter_threshold,\n    similar_image_filter_max_skip_frame,\n)\n```\n\n関数で設定できる引数として以下がある。\n\n#### `similar_image_filter_threshold`\n\n- 処理を休止する前フレームと現フレームの類似度の閾値\n\n#### `similar_image_filter_max_skip_frame`\n\n- 休止中に変換を再開する最大の間隔\n\n### Residual CFG (RCFG)\n\n![rcfg](assets/cfg_conparision.png)\n\nRCFG は CFG 使用しない場合と比較し、競争力のある計算量で近似的に CFG を実現させる方法である。StreamDiffusion の引数 cfg_type から指定可能である。\n\nRCFG は二種類あり、negative prompt の指定項目なしの RCFG Self-Negative と negative prompt が指定可能な Onetime-Negative が利用可能である。計算量は CFG なしの計算量を N、通常の CFG ありの計算量を２ N としたとき、RCFG Self-Negative は N 回で、Onetime-Negative は N+1 回で計算できる。\n\nThe usage is as follows:\n\n```python\n# CFG なし\ncfg_type = \"none\"\n\n# 通常のCFG\ncfg_type = \"full\"\n\n# RCFG Self-Negative\ncfg_type = \"self\"\n\n# RCFG Onetime-Negative\ncfg_type = \"initialize\"\n\nstream = StreamDiffusion(\n    pipe,\n    [32, 45],\n    torch_dtype=torch.float16,\n    cfg_type=cfg_type,\n)\n\nstream.prepare(\n    prompt=\"1girl, purple hair\",\n    guidance_scale=guidance_scale,\n    delta=delta,\n)\n```\n\ndelta は RCFG の効きをマイルドにする効果を持つ\n\n## 開発チーム\n\n[Aki](https://twitter.com/cumulo_autumn),\n[Ararat](https://twitter.com/AttaQjp),\n[Chenfeng Xu](https://twitter.com/Chenfeng_X),\n[ddPn08](https://twitter.com/ddPn08),\n[kizamimi](https://twitter.com/ArtengMimi),\n[ramune](https://twitter.com/__ramu0e__),\n[teftef](https://twitter.com/hanyingcl),\n[Tonimono](https://twitter.com/toni_nimono),\n[Verb](https://twitter.com/IMG_5955),\n\n(\\*alphabetical order)\n</br>\n\n## 謝辞\n\nこの GitHub リポジトリ にある動画と画像のデモは、[LCM-LoRA](https://huggingface.co/latent-consistency/lcm-lora-sdv1-5) + [KohakuV2](https://civitai.com/models/136268/kohaku-v2)と[SD-Turbo](https://arxiv.org/abs/2311.17042)を使用して生成されました。\n\nLCM-LoRA を提供していただいた[LCM-LoRA authors](https://latent-consistency-models.github.io/)、KohakuV2 モデルを提供していただいた Kohaku BlueLeaf 様 ([@KBlueleaf](https://twitter.com/KBlueleaf))、[SD-Turbo](https://arxiv.org/abs/2311.17042)を提供していただいた[Stability AI](https://ja.stability.ai/)様に心より感謝いたします。\n\nKohakuV2 モデルは [Civitai](https://civitai.com/models/136268/kohaku-v2) と [Hugging Face](https://huggingface.co/KBlueLeaf/kohaku-v2.1) からダウンロードでき、[SD-Turbo](https://huggingface.co/stabilityai/sd-turbo) は Hugging Face で使用可能です。\n\n## Contributors\n\n<a href=\"https://github.com/cumulo-autumn/StreamDiffusion/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=cumulo-autumn/StreamDiffusion\" />\n</a>\n"
        },
        {
          "name": "README-ko.md",
          "type": "blob",
          "size": 13.63671875,
          "content": "# StreamDiffusion\n\n[English](./README.md) | [日本語](./README-ja.md) | [한국어](./README-ko.md)\n\n<p align=\"center\">\n  <img src=\"./assets/demo_07.gif\" width=90%>\n  <img src=\"./assets/demo_09.gif\" width=90%>\n</p>\n\n# StreamDiffusion: 실시간 이미지 생성을 위한 파이프라인 수준의 솔루션\n\n**Authors:** [Akio Kodaira](https://www.linkedin.com/in/akio-kodaira-1a7b98252/), [Chenfeng Xu](https://www.chenfengx.com/), Toshiki Hazama, [Takanori Yoshimoto](https://twitter.com/__ramu0e__), [Kohei Ohno](https://www.linkedin.com/in/kohei--ohno/), [Shogo Mitsuhori](https://me.ddpn.world/), [Soichi Sugano](https://twitter.com/toni_nimono), [Hanying Cho](https://twitter.com/hanyingcl), [Zhijian Liu](https://zhijianliu.com/), [Kurt Keutzer](https://scholar.google.com/citations?hl=en&user=ID9QePIAAAAJ)\n\nStreamDiffusion은 실시간 이미지 생성을 위해 최적화 된 파이프라인입니다. diffusion을 기반으로 한 기존 이미지 생성 파이프라인과 비교하여, 압도적인 성능 개선을 이뤄냈습니다.\n\n[![arXiv](https://img.shields.io/badge/arXiv-2312.12491-b31b1b.svg)](https://arxiv.org/abs/2312.12491)\n[![Hugging Face Papers](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-papers-yellow)](https://huggingface.co/papers/2312.12491)\n\nStreamDiffusion을 개발하면서 세심한 지원과 의미 있는 피드백과 토론을 해주신 [Taku Fujimoto](https://twitter.com/AttaQjp), [Radamés Ajna](https://twitter.com/radamar), 그리고 Hugging Face 팀에게 진심으로 감사드립니다.\n\n## 주요 기능\n\n1. **Stream Batch**\n\n   - 디노이징 배치 작업을 통해, 데이터 처리를 효율적으로 진행합니다.\n\n2. **Residual Classifier-Free Guidance** - [자세히](#residual-cfg-rcfg)\n\n   - CFG 메커니즘을 이용하여 중복되는 계산을 최소화합니다.\n\n3. **Stochastic Similarity Filter** - [자세히](#stochastic-similarity-filter)\n\n   - 유사도에 따른 필터링을 진행해 GPU 사용 효율을 극대화합니다.\n\n4. **IO Queues** - [자세히](#IO Queues\\*\\*)\n\n   - 입출력 연산을 효율적으로 관리하여, 원활한 실행 환경을 제공합니다.\n\n5. **Pre-Computation for KV-Caches**\n\n   - 빠른 처리속도를 위해 캐시 전략을 최적화하였습니다.\n\n6. \\*\\*Model Acceleration Tools\n   - 모델 최적화 및 성능 향상을 위해 다양한 툴들을 활용합니다.\n\n**GPU: RTX 4090**, **CPU: Core i9-13900K**, **OS: Ubuntu 22.04.3 LTS** 환경에서 StreamDiffusion pipeline을 이용하여 이미지를 생성한 결과입니다.\n\n|            model            | Denoising Step | fps on Txt2Img | fps on Img2Img |\n| :-------------------------: | :------------: | :------------: | :------------: |\n|          SD-turbo           |       1        |     106.16     |     93.897     |\n| LCM-LoRA <br>+<br> KohakuV2 |       4        |     38.023     |     37.133     |\n\n제공된 링크를 따라 StreamDiffusion의 기능을 자유롭게 탐색해보시길 바랍니다. 이 프로젝트가 도움이 되셨다면, 저희 작업을 인용해 주시면 감사하겠습니다.\n\n```bash\n@article{kodaira2023streamdiffusion,\n      title={StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation},\n      author={Akio Kodaira and Chenfeng Xu and Toshiki Hazama and Takanori Yoshimoto and Kohei Ohno and Shogo Mitsuhori and Soichi Sugano and Hanying Cho and Zhijian Liu and Kurt Keutzer},\n      year={2023},\n      eprint={2312.12491},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n## 설치\n\n### Step0: 저장소 clone\n\n```bash\ngit clone https://github.com/cumulo-autumn/StreamDiffusion.git\n```\n\n### 1단계: 환경 설정\n\nStreamDiffusion은 pip, conda 또는 Docker(하단 설명 참조)를 통해 설치할 수 있습니다.\n\n```bash\nconda create -n streamdiffusion python=3.10\nconda activate streamdiffusion\n```\n\n또는\n\n```cmd\npython -m venv .venv\n# Windows\n.\\.venv\\Scripts\\activate\n# Linux\nsource .venv/bin/activate\n```\n\n### 2단계: PyTorch 설치\n\n시스템에 알맞는 버전을 설치하시면 됩니다.\n\nCUDA 11.8\n\n```bash\npip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu118\n```\n\nCUDA 12.1\n\n```bash\npip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu121\n```\n\ndetails: https://pytorch.org/\n\n### 3단계: StreamDiffusion 설치\n\n#### 일반 사용자용\n\nStreamDiffusion 설치\n\n```bash\n#최신 버전 설치 (추천)\npip install git+https://github.com/cumulo-autumn/StreamDiffusion.git@main#egg=streamdiffusion[tensorrt]\n\n\n#또는\n\n\n#안정 버전 설치\npip install streamdiffusion[tensorrt]\n```\n\nTensorRT 확장 프로그램 설치\n\n```bash\npython -m streamdiffusion.tools.install-tensorrt\n```\n\n(윈도우 사용자용) 안정 버젼(`pip install streamdiffusion[tensorrt]`)을 설치한 경우, pywin32를 별도로 설치해야 할 수 있습니다.\n\n```bash\npip install --force-reinstall pywin32\n```\n\n#### 개발자용\n\n```bash\npython setup.py develop easy_install streamdiffusion[tensorrt]\npython -m streamdiffusion.tools.install-tensorrt\n```\n\n### Docker 설치 (TensorRT 지원)\n\n```bash\ngit clone https://github.com/cumulo-autumn/StreamDiffusion.git\ncd StreamDiffusion\ndocker build -t stream-diffusion:latest -f Dockerfile .\ndocker run --gpus all -it -v $(pwd):/home/ubuntu/streamdiffusion stream-diffusion:latest\n```\n\n## Quick Start\n\n[`examples`](./examples) 디렉토리에서 StreamDiffusion을 체험해 보실 수 있습니다.\n\n| ![그림3](./assets/demo_02.gif) | ![그림4](./assets/demo_03.gif) |\n| :----------------------------: | :----------------------------: |\n| ![그림5](./assets/demo_04.gif) | ![그림6](./assets/demo_05.gif) |\n\n## 실시간 Txt2Img 데모\n\n[`demo/realtime-txt2img`](./demo/realtime-txt2img) 디렉토리에서 실시간 txt2img 데모를 사용해 보실 수 있습니다!\n\n<p align=\"center\">\n  <img src=\"./assets/demo_01.gif\" width=100%>\n</p>\n\n## 실시간 Img2Img 데모\n\n실시간 웹 카메라 및 스크린 캡쳐에 대한 Web img2img 데모는 [`demo/realtime-img2img`](./demo/realtime-img2img)에서 보실 수 있습니다.\n\n<p align=\"center\">\n  <img src=\"./assets/img2img1.gif\" width=100%>\n</p>\n\n## 사용 예제\n\nStreamDiffusion 사용법 간단한 예시들을 제공합니다. 더 자세한 예제를 확인하고자 한다면 [`examples`](./examples)를 참고해주시길 바랍니다.\n\n### Image-to-Image\n\n```python\nimport torch\nfrom diffusers import AutoencoderTiny, StableDiffusionPipeline\nfrom diffusers.utils import load_image\n\nfrom streamdiffusion import StreamDiffusion\nfrom streamdiffusion.image_utils import postprocess_image\n\n# diffusers의 StableDiffusionPipeline을 사용하여 모든 모델을 로드할 수 있습니다\npipe = StableDiffusionPipeline.from_pretrained(\"KBlueLeaf/kohaku-v2.1\").to(\n    device=torch.device(\"cuda\"),\n    dtype=torch.float16,\n)\n\n# StreamDiffusion에 파이프라인을 래핑합니다\nstream = StreamDiffusion(\n    pipe,\n    t_index_list=[32, 45],\n    torch_dtype=torch.float16,\n)\n\n# 로드된 모델이 LCM이 아닌 경우, LCM을 병합합니다\nstream.load_lcm_lora()\nstream.fuse_lora()\n# 더 빠른 가속을 위해 Tiny VAE 사용합니다\nstream.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\").to(device=pipe.device, dtype=pipe.dtype)\n# 가속화 활성화\npipe.enable_xformers_memory_efficient_attention()\n\n\nprompt = \"1girl with dog hair, thick frame glasses\"\n# 스트림을 준비합니다\nstream.prepare(prompt)\n\n# 이미지를 준비합니다\ninit_image = load_image(\"assets/img2img_example.png\").resize((512, 512))\n\n# 웜업 >= len(t_index_list) x frame_buffer_size\nfor _ in range(2):\n    stream(init_image)\n\n# 스트림 루프를 실행합니다\nwhile True:\n    x_output = stream(init_image)\n    postprocess_image(x_output, output_type=\"pil\")[0].show()\n    input_response = input(\"계속하려면 Enter를 누르거나, 종료하려면 'stop'을 입력하세요: \")\n    if input_response == \"stop\":\n        break\n```\n\n### Text-to-Image\n\n```python\nimport torch\nfrom diffusers import AutoencoderTiny, StableDiffusionPipeline\n\nfrom streamdiffusion import StreamDiffusion\nfrom streamdiffusion.image_utils import postprocess_image\n\n# diffusers의 StableDiffusionPipeline을 사용하여 모든 모델을 로드할 수 있습니다\npipe = StableDiffusionPipeline.from_pretrained(\"KBlueLeaf/kohaku-v2.1\").to(\n    device=torch.device(\"cuda\"),\n    dtype=torch.float16,\n)\n\n# StreamDiffusion에 파이프라인을 래핑합니다\n# 텍스트-이미지 변환에서는 더 많은 긴 단계(len(t_index_list))가 필요합니다\n# 텍스트-이미지 변환 시 cfg_type=\"none\"을 사용하는 것을 권장합니다\nstream = StreamDiffusion(\n    pipe,\n    t_index_list=[0, 16, 32, 45],\n    torch_dtype=torch.float16,\n    cfg_type=\"none\",\n)\n\n# 로드된 모델이 LCM이 아닌 경우, LCM을 병합합니다\nstream.load_lcm_lora()\nstream.fuse_lora()\n# 더 빠른 가속을 위해 Tiny VAE 사용합니다\nstream.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\").to(device=pipe.device, dtype=pipe.dtype)\n# 가속화 활성화\npipe.enable_xformers_memory_efficient_attention()\n\n\nprompt = \"1girl with dog hair, thick frame glasses\"\n# 스트림을 준비합니다\nstream.prepare(prompt)\n\n# 웜업 >= len(t_index_list) x frame_buffer_size\nfor _ in range(4):\n    stream()\n\n# 스트림 루프를 실행합니다\nwhile True:\n    x_output = stream.txt2img()\n    postprocess_image(x_output, output_type=\"pil\")[0].show()\n    input_response = input(\"계속하려면 Enter를 누르거나, 종료하려면 'stop'을 입력하세요: \")\n    if input_response == \"stop\":\n        break\n```\n\nSD-Turbo를 사용하면 속도를 더 개선할 수 있습니다.\n\n### 속도 개선하기\n\n위 예제에서 다음의 코드들을 교체하세요.\n\n```python\npipe.enable_xformers_memory_efficient_attention()\n```\n\n해당 부분을\n\n```python\nfrom streamdiffusion.acceleration.tensorrt import accelerate_with_tensorrt\n\nstream = accelerate_with_tensorrt(\n    stream, \"engines\", max_batch_size=2,\n)\n```\n\nTensorRT 확장과 엔진 빌드 시간이 필요하지만, 위 예제보다 속도를 개선할 수 있습니다.\n\n## 선택 사항\n\n### Stochastic Similarity Filter\n\n![Demo](assets/demo_06.gif)\n\nStochastic Similarity Filter 기법을 사용하여, 이전 프레임과의 변화가 적은 경우, 변환 작업을 통합하여 비디오 입력 처리 시간을 개선합니다. 이를 통해, GPU 처리 부하를 줄입니다. 위 GIF의 빨간 프레임에서 볼 수 있듯이 사용 방법은 다음과 같습니다:\n\n```python\nstream = StreamDiffusion(\n    pipe,\n    [32, 45],\n    torch_dtype=torch.float16,\n)\nstream.enable_similar_image_filter(\n    similar_image_filter_threshold,\n    similar_image_filter_max_skip_frame,\n)\n```\n\n다음과 같은 매개변수를 함수의 인수로 설정할 수 있습니다:\n\n#### `similar_image_filter_threshold`\n\n- 이전 프레임과 현재 프레임 사이의 유사성 임계값으로, 이 값 이하일 때 처리가 일시 중단됩니다.\n\n#### `similar_image_filter_max_skip_frame`\n\n- 일시 중단 기간 동안 최대 스킵할 수 있는 프레임 간격입니다.\n\n### Residual CFG (RCFG)\n\n![rcfg](assets/cfg_conparision.png)\n\nRCFG는 CFG를 사용하지 않는 경우와 비교하여 경쟁적인 계산 복잡성을 가진 CFG를 대략적으로 구현하는 방법입니다. StreamDiffusion의 cfg_type 인수를 통해 지정할 수 있습니다. RCFG에는 두 가지 유형이 있습니다: 부정적 프롬프트를 지정하지 않은 RCFG Self-Negative와 부정적 프롬프트를 지정할 수 있는 RCFG Onetime-Negative. 계산 복잡성 측면에서, CFG 없이 N으로 표시되는 복잡성과 일반적인 CFG로 2N으로 표시되는 복잡성을 비교할 때, RCFG Self-Negative는 N단계로 계산할 수 있으며, RCFG Onetime-Negative는 N+1단계로 계산할 수 있습니다.\n\n사용 방법은 다음과 같습니다:\n\n```python\n# CFG 없음\ncfg_type = \"none\"\n# CFG\ncfg_type = \"full\"\n# RCFG Self-Negative\ncfg_type = \"self\"\n# RCFG Onetime-Negative\ncfg_type = \"initialize\"\nstream = StreamDiffusion(\n    pipe,\n    [32, 45],\n    torch_dtype=torch.float16,\n    cfg_type=cfg_type,\n)\nstream.prepare(\n    prompt=\"1girl, purple hair\",\n    guidance_scale=guidance_scale,\n    delta=delta,\n)\n```\n\ndelta는 RCFG의 효과를 완화하는 역할을 합니다.\n\n## 개발팀\n\n[Aki](https://twitter.com/cumulo_autumn),\n[Ararat](https://twitter.com/AttaQjp),\n[Chenfeng Xu](https://twitter.com/Chenfeng_X),\n[ddPn08](https://twitter.com/ddPn08),\n[kizamimi](https://twitter.com/ArtengMimi),\n[ramune](https://twitter.com/__ramu0e__),\n[teftef](https://twitter.com/hanyingcl),\n[Tonimono](https://twitter.com/toni_nimono),\n[Verb](https://twitter.com/IMG_5955),\n\n(\\*alphabetical order)\n</br>\n\n## 감사의 말\n\n이 GitHub 저장소의 비디오 및 이미지 데모는 [LCM-LoRA](https://huggingface.co/latent-consistency/lcm-lora-sdv1-5) + [KohakuV2](https://civitai.com/models/136268/kohaku-v2) 및 [SD-Turbo](https://arxiv.org/abs/2311.17042)를 사용하여 생성되었습니다.\n\nLCM-LoRA를 제공해 주신 [LCM-LoRA authors](https://latent-consistency-models.github.io/)과 KohakuV2 모델을 제공해 주신 Kohaku BlueLeaf ([@KBlueleaf](https://twitter.com/KBlueleaf)), 그리고 [SD-Turbo](https://arxiv.org/abs/2311.17042)를 제공해 주신 [Stability AI](https://ja.stability.ai/)에 특별한 감사를 드립니다.\n\nKohakuV2 모델은 [Civitai](https://civitai.com/models/136268/kohaku-v2) 및 [Hugging Face](https://huggingface.co/KBlueLeaf/kohaku-v2.1)에서 다운로드할 수 있습니다.\n\nSD-Turbo는 [Hugging Face Space](https://huggingface.co/stabilityai/sd-turbo)에서도 이용할 수 있습니다.\n\n## Contributors\n\n<a href=\"https://github.com/cumulo-autumn/StreamDiffusion/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=cumulo-autumn/StreamDiffusion\" />\n</a>\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.6318359375,
          "content": "# StreamDiffusion\n\n[English](./README.md) | [日本語](./README-ja.md) | [한국어](./README-ko.md)\n\n<p align=\"center\">\n  <img src=\"./assets/demo_07.gif\" width=90%>\n  <img src=\"./assets/demo_09.gif\" width=90%>\n</p>\n\n# StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation\n\n**Authors:** [Akio Kodaira*](https://www.linkedin.com/in/akio-kodaira-1a7b98252/), [Chenfeng Xu*](https://www.chenfengx.com/), Toshiki Hazama*, [Takanori Yoshimoto](https://twitter.com/__ramu0e__), [Kohei Ohno](https://www.linkedin.com/in/kohei--ohno/), [Shogo Mitsuhori](https://me.ddpn.world/), [Soichi Sugano](https://twitter.com/toni_nimono), [Hanying Cho](https://twitter.com/hanyingcl), [Zhijian Liu](https://zhijianliu.com/), [Masayoshi Tomizuka](https://me.berkeley.edu/people/masayoshi-tomizuka/), [Kurt Keutzer](https://scholar.google.com/citations?hl=en&user=ID9QePIAAAAJ)\n\nStreamDiffusion is an innovative diffusion pipeline designed for real-time interactive generation. It introduces significant performance enhancements to current diffusion-based image generation techniques.\n\n[![arXiv](https://img.shields.io/badge/arXiv-2312.12491-b31b1b.svg)](https://arxiv.org/abs/2312.12491)\n[![Hugging Face Papers](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-papers-yellow)](https://huggingface.co/papers/2312.12491)\n\nWe sincerely thank [Taku Fujimoto](https://twitter.com/AttaQjp) and [Radamés Ajna](https://twitter.com/radamar) and Hugging Face team for their invaluable feedback, courteous support, and insightful discussions.\n\n## Key Features\n\n1. **Stream Batch**\n\n   - Streamlined data processing through efficient batch operations.\n\n2. **Residual Classifier-Free Guidance** - [Learn More](#residual-cfg-rcfg)\n\n   - Improved guidance mechanism that minimizes computational redundancy.\n\n3. **Stochastic Similarity Filter** - [Learn More](#stochastic-similarity-filter)\n\n   - Improves GPU utilization efficiency through advanced filtering techniques.\n\n4. **IO Queues**\n\n   - Efficiently manages input and output operations for smoother execution.\n\n5. **Pre-Computation for KV-Caches**\n\n   - Optimizes caching strategies for accelerated processing.\n\n6. **Model Acceleration Tools**\n   - Utilizes various tools for model optimization and performance boost.\n\nWhen images are produced using our proposed StreamDiffusion pipeline in an environment with **GPU: RTX 4090**, **CPU: Core i9-13900K**, and **OS: Ubuntu 22.04.3 LTS**.\n\n|            model            | Denoising Step | fps on Txt2Img | fps on Img2Img |\n| :-------------------------: | :------------: | :------------: | :------------: |\n|          SD-turbo           |       1        |     106.16     |     93.897     |\n| LCM-LoRA <br>+<br> KohakuV2 |       4        |     38.023     |     37.133     |\n\nFeel free to explore each feature by following the provided links to learn more about StreamDiffusion's capabilities. If you find it helpful, please consider citing our work:\n\n```bash\n@article{kodaira2023streamdiffusion,\n      title={StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation},\n      author={Akio Kodaira and Chenfeng Xu and Toshiki Hazama and Takanori Yoshimoto and Kohei Ohno and Shogo Mitsuhori and Soichi Sugano and Hanying Cho and Zhijian Liu and Kurt Keutzer},\n      year={2023},\n      eprint={2312.12491},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n## Installation\n\n### Step0: clone this repository\n\n```bash\ngit clone https://github.com/cumulo-autumn/StreamDiffusion.git\n```\n\n### Step1: Make Environment\n\nYou can install StreamDiffusion via pip, conda, or Docker(explanation below).\n\n```bash\nconda create -n streamdiffusion python=3.10\nconda activate streamdiffusion\n```\n\nOR\n\n```cmd\npython -m venv .venv\n# Windows\n.\\.venv\\Scripts\\activate\n# Linux\nsource .venv/bin/activate\n```\n\n### Step2: Install PyTorch\n\nSelect the appropriate version for your system.\n\nCUDA 11.8\n\n```bash\npip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu118\n```\n\nCUDA 12.1\n\n```bash\npip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu121\n```\n\ndetails: https://pytorch.org/\n\n### Step3: Install StreamDiffusion\n\n#### For User\n\nInstall StreamDiffusion\n\n```bash\n#for Latest Version (recommended)\npip install git+https://github.com/cumulo-autumn/StreamDiffusion.git@main#egg=streamdiffusion[tensorrt]\n\n\n#or\n\n\n#for Stable Version\npip install streamdiffusion[tensorrt]\n```\n\nInstall TensorRT extension\n\n```bash\npython -m streamdiffusion.tools.install-tensorrt\n```\n\n(Only for Windows) You may need to install pywin32 additionally, if you installed Stable Version(`pip install streamdiffusion[tensorrt]`).\n\n```bash\npip install --force-reinstall pywin32\n```\n\n#### For Developer\n\n```bash\npython setup.py develop easy_install streamdiffusion[tensorrt]\npython -m streamdiffusion.tools.install-tensorrt\n```\n\n### Docker Installation (TensorRT Ready)\n\n```bash\ngit clone https://github.com/cumulo-autumn/StreamDiffusion.git\ncd StreamDiffusion\ndocker build -t stream-diffusion:latest -f Dockerfile .\ndocker run --gpus all -it -v $(pwd):/home/ubuntu/streamdiffusion stream-diffusion:latest\n```\n\n## Quick Start\n\nYou can try StreamDiffusion in [`examples`](./examples) directory.\n\n| ![画像3](./assets/demo_02.gif) | ![画像4](./assets/demo_03.gif) |\n| :----------------------------: | :----------------------------: |\n| ![画像5](./assets/demo_04.gif) | ![画像6](./assets/demo_05.gif) |\n\n## Real-Time Txt2Img Demo\n\nThere is an interactive txt2img demo in [`demo/realtime-txt2img`](./demo/realtime-txt2img) directory!\n\n<p align=\"center\">\n  <img src=\"./assets/demo_01.gif\" width=100%>\n</p>\n\n## Real-Time Img2Img Demo\n\nThere is a real time img2img demo with a live webcam feed or screen capture on a web browser in [`demo/realtime-img2img`](./demo/realtime-img2img) directory!\n\n<p align=\"center\">\n  <img src=\"./assets/img2img1.gif\" width=100%>\n</p>\n\n## Usage Example\n\nWe provide a simple example of how to use StreamDiffusion. For more detailed examples, please refer to [`examples`](./examples) directory.\n\n### Image-to-Image\n\n```python\nimport torch\nfrom diffusers import AutoencoderTiny, StableDiffusionPipeline\nfrom diffusers.utils import load_image\n\nfrom streamdiffusion import StreamDiffusion\nfrom streamdiffusion.image_utils import postprocess_image\n\n# You can load any models using diffuser's StableDiffusionPipeline\npipe = StableDiffusionPipeline.from_pretrained(\"KBlueLeaf/kohaku-v2.1\").to(\n    device=torch.device(\"cuda\"),\n    dtype=torch.float16,\n)\n\n# Wrap the pipeline in StreamDiffusion\nstream = StreamDiffusion(\n    pipe,\n    t_index_list=[32, 45],\n    torch_dtype=torch.float16,\n)\n\n# If the loaded model is not LCM, merge LCM\nstream.load_lcm_lora()\nstream.fuse_lora()\n# Use Tiny VAE for further acceleration\nstream.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\").to(device=pipe.device, dtype=pipe.dtype)\n# Enable acceleration\npipe.enable_xformers_memory_efficient_attention()\n\n\nprompt = \"1girl with dog hair, thick frame glasses\"\n# Prepare the stream\nstream.prepare(prompt)\n\n# Prepare image\ninit_image = load_image(\"assets/img2img_example.png\").resize((512, 512))\n\n# Warmup >= len(t_index_list) x frame_buffer_size\nfor _ in range(2):\n    stream(init_image)\n\n# Run the stream infinitely\nwhile True:\n    x_output = stream(init_image)\n    postprocess_image(x_output, output_type=\"pil\")[0].show()\n    input_response = input(\"Press Enter to continue or type 'stop' to exit: \")\n    if input_response == \"stop\":\n        break\n```\n\n### Text-to-Image\n\n```python\nimport torch\nfrom diffusers import AutoencoderTiny, StableDiffusionPipeline\n\nfrom streamdiffusion import StreamDiffusion\nfrom streamdiffusion.image_utils import postprocess_image\n\n# You can load any models using diffuser's StableDiffusionPipeline\npipe = StableDiffusionPipeline.from_pretrained(\"KBlueLeaf/kohaku-v2.1\").to(\n    device=torch.device(\"cuda\"),\n    dtype=torch.float16,\n)\n\n# Wrap the pipeline in StreamDiffusion\n# Requires more long steps (len(t_index_list)) in text2image\n# You recommend to use cfg_type=\"none\" when text2image\nstream = StreamDiffusion(\n    pipe,\n    t_index_list=[0, 16, 32, 45],\n    torch_dtype=torch.float16,\n    cfg_type=\"none\",\n)\n\n# If the loaded model is not LCM, merge LCM\nstream.load_lcm_lora()\nstream.fuse_lora()\n# Use Tiny VAE for further acceleration\nstream.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taesd\").to(device=pipe.device, dtype=pipe.dtype)\n# Enable acceleration\npipe.enable_xformers_memory_efficient_attention()\n\n\nprompt = \"1girl with dog hair, thick frame glasses\"\n# Prepare the stream\nstream.prepare(prompt)\n\n# Warmup >= len(t_index_list) x frame_buffer_size\nfor _ in range(4):\n    stream()\n\n# Run the stream infinitely\nwhile True:\n    x_output = stream.txt2img()\n    postprocess_image(x_output, output_type=\"pil\")[0].show()\n    input_response = input(\"Press Enter to continue or type 'stop' to exit: \")\n    if input_response == \"stop\":\n        break\n```\n\nYou can make it faster by using SD-Turbo.\n\n### Faster generation\n\nReplace the following code in the above example.\n\n```python\npipe.enable_xformers_memory_efficient_attention()\n```\n\nTo\n\n```python\nfrom streamdiffusion.acceleration.tensorrt import accelerate_with_tensorrt\n\nstream = accelerate_with_tensorrt(\n    stream, \"engines\", max_batch_size=2,\n)\n```\n\nIt requires TensorRT extension and time to build the engine, but it will be faster than the above example.\n\n## Optionals\n\n### Stochastic Similarity Filter\n\n![demo](assets/demo_06.gif)\n\nStochastic Similarity Filter reduces processing during video input by minimizing conversion operations when there is little change from the previous frame, thereby alleviating GPU processing load, as shown by the red frame in the above GIF. The usage is as follows:\n\n```python\nstream = StreamDiffusion(\n    pipe,\n    [32, 45],\n    torch_dtype=torch.float16,\n)\nstream.enable_similar_image_filter(\n    similar_image_filter_threshold,\n    similar_image_filter_max_skip_frame,\n)\n```\n\nThere are the following parameters that can be set as arguments in the function:\n\n#### `similar_image_filter_threshold`\n\n- The threshold for similarity between the previous frame and the current frame before the processing is paused.\n\n#### `similar_image_filter_max_skip_frame`\n\n- The maximum interval during the pause before resuming the conversion.\n\n### Residual CFG (RCFG)\n\n![rcfg](assets/cfg_conparision.png)\n\nRCFG is a method for approximately realizing CFG with competitive computational complexity compared to cases where CFG is not used. It can be specified through the cfg_type argument in the StreamDiffusion. There are two types of RCFG: one with no specified items for negative prompts RCFG Self-Negative and one where negative prompts can be specified RCFG Onetime-Negative. In terms of computational complexity, denoting the complexity without CFG as N and the complexity with a regular CFG as 2N, RCFG Self-Negative can be computed in N steps, while RCFG Onetime-Negative can be computed in N+1 steps.\n\nThe usage is as follows:\n\n```python\n# w/0 CFG\ncfg_type = \"none\"\n# CFG\ncfg_type = \"full\"\n# RCFG Self-Negative\ncfg_type = \"self\"\n# RCFG Onetime-Negative\ncfg_type = \"initialize\"\nstream = StreamDiffusion(\n    pipe,\n    [32, 45],\n    torch_dtype=torch.float16,\n    cfg_type=cfg_type,\n)\nstream.prepare(\n    prompt=\"1girl, purple hair\",\n    guidance_scale=guidance_scale,\n    delta=delta,\n)\n```\n\nThe delta has a moderating effect on the effectiveness of RCFG.\n\n## Development Team\n\n[Aki](https://twitter.com/cumulo_autumn),\n[Ararat](https://twitter.com/AttaQjp),\n[Chenfeng Xu](https://twitter.com/Chenfeng_X),\n[ddPn08](https://twitter.com/ddPn08),\n[kizamimi](https://twitter.com/ArtengMimi),\n[ramune](https://twitter.com/__ramu0e__),\n[teftef](https://twitter.com/hanyingcl),\n[Tonimono](https://twitter.com/toni_nimono),\n[Verb](https://twitter.com/IMG_5955),\n\n(\\*alphabetical order)\n</br>\n\n## Acknowledgements\n\nThe video and image demos in this GitHub repository were generated using [LCM-LoRA](https://huggingface.co/latent-consistency/lcm-lora-sdv1-5) + [KohakuV2](https://civitai.com/models/136268/kohaku-v2) and [SD-Turbo](https://arxiv.org/abs/2311.17042).\n\nSpecial thanks to [LCM-LoRA authors](https://latent-consistency-models.github.io/) for providing the LCM-LoRA and Kohaku BlueLeaf ([@KBlueleaf](https://twitter.com/KBlueleaf)) for providing the KohakuV2 model and ,to [Stability AI](https://ja.stability.ai/) for [SD-Turbo](https://arxiv.org/abs/2311.17042).\n\nKohakuV2 Models can be downloaded from [Civitai](https://civitai.com/models/136268/kohaku-v2) and [Hugging Face](https://huggingface.co/KBlueLeaf/kohaku-v2.1).\n\nSD-Turbo is also available on [Hugging Face Space](https://huggingface.co/stabilityai/sd-turbo).\n\n## Contributors\n\n<a href=\"https://github.com/cumulo-autumn/StreamDiffusion/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=cumulo-autumn/StreamDiffusion\" />\n</a>\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.701171875,
          "content": "[tool.ruff]\n# Never enforce `E501` (line length violations).\nignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F403\", \"F405\", \"F823\"]\nselect = [\"C\", \"E\", \"F\", \"I\", \"W\"]\nline-length = 119\n\n# Ignore import violations in all `__init__.py` files.\n[tool.ruff.per-file-ignores]\n\"__init__.py\" = [\"E402\", \"F401\", \"F811\"]\n\n[tool.ruff.isort]\nlines-after-imports = 2\nknown-first-party = [\"streamdiffusion\"]\n\n[tool.ruff.format]\n# Like Black, use double quotes for strings.\nquote-style = \"double\"\n\n# Like Black, indent with spaces, rather than tabs.\nindent-style = \"space\"\n\n# Like Black, respect magic trailing commas.\nskip-magic-trailing-comma = false\n\n# Like Black, automatically detect the appropriate line ending.\nline-ending = \"auto\""
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.80859375,
          "content": "import os\nimport re\n\nfrom setuptools import find_packages, setup\n\n\n_deps = [\n    \"torch\",\n    \"xformers\",\n    \"diffusers==0.24.0\",\n    \"transformers\",\n    \"accelerate\",\n    \"fire\",\n    \"omegaconf\",\n    \"cuda-python\",\n    \"onnx==1.15.0\",\n    \"onnxruntime==1.16.3\",\n    \"protobuf==3.20.2\",\n    \"colored\",\n    \"pywin32;sys_platform == 'win32'\"\n]\n\ndeps = {b: a for a, b in (re.findall(r\"^(([^!=<>~]+)(?:[!=<>~].*)?$)\", x)[0] for x in _deps)}\n\n\ndef deps_list(*pkgs):\n    return [deps[pkg] for pkg in pkgs]\n\n\nextras = {}\nextras[\"xformers\"] = deps_list(\"xformers\")\nextras[\"torch\"] = deps_list(\"torch\", \"accelerate\")\nextras[\"tensorrt\"] = deps_list(\"protobuf\", \"cuda-python\", \"onnx\", \"onnxruntime\", \"colored\")\n\nextras[\"dev\"] = extras[\"xformers\"] + extras[\"torch\"] + extras[\"tensorrt\"]\n\ninstall_requires = [\n    deps[\"fire\"],\n    deps[\"omegaconf\"],\n    deps[\"diffusers\"],\n    deps[\"transformers\"],\n    deps[\"accelerate\"],\n]\n\nsetup(\n    name=\"streamdiffusion\",\n    version=\"0.1.1\",\n    description=\"real-time interactive image generation pipeline\",\n    long_description=open(\"README.md\", \"r\", encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    keywords=\"deep learning diffusion pytorch stable diffusion audioldm streamdiffusion real-time\",\n    license=\"Apache 2.0 License\",\n    author=\"Aki, kizamimi, ddPn08, Verb, ramune, teftef6220, Tonimono, Chenfeng Xu, Ararat with the help of all our contributors (https://github.com/cumulo-autumn/StreamDiffusion/graphs/contributors)\",\n    author_email=\"cumulokyoukai@gmail.com\",\n    url=\"https://github.com/cumulo-autumn/StreamDiffusion\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    package_data={\"streamdiffusion\": [\"py.typed\"]},\n    include_package_data=True,\n    python_requires=\">=3.10.0\",\n    install_requires=list(install_requires),\n    extras_require=extras,\n)\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}