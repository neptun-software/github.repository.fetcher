{
  "metadata": {
    "timestamp": 1736560586104,
    "page": 204,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "microsoft/UFO",
      "stars": 8127,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.611328125,
          "content": "# Ignore login file\n*.bin\n\n# Ignore Jupyter Notebook checkpoints\n.ipynb_checkpoints\n/test/*\n/deprecated/*\n/test/*.ipynb\n/logs/*\n/customization/*\n__pycache__/\n**/__pycache__/\n*.pyc\n*.ipynb\n/.VSCodeCounter\n/analysis/*\n\n# Ignore the config file\nufo/config/config.yaml\nufo/config/config_llm.yaml\n\n\n# Ignore the helper files\nufo/rag/app_docs/*\nlearner/records.json\nvectordb/docs/*\nvectordb/experience/*\nvectordb/demonstration/*\n\n# Ignore the data files and scripts\ntasks/*\nscripts/*\n\n# Don't ignore the example files\n!vectordb/docs/example/\n!vectordb/demonstration/example.yaml\n\n.vscode\n\n# Ignore the record files\ntasks_status.json"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.43359375,
          "content": "# Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.9931640625,
          "content": "# Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to,\nand actually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the\ninstructions provided by the bot. You will only need to do this once across all repositories using our CLA.\n\n## note\nYou should sunmit your pull request to the `pre-release` branch, not the `main` branch.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments."
        },
        {
          "name": "DISCLAIMER.md",
          "type": "blob",
          "size": 2.697265625,
          "content": "# Disclaimer: Code Execution and Data Handling Notice\n\nBy choosing to run the provided code, you acknowledge and agree to the following terms and conditions regarding the functionality and data handling practices:\n\n## 1. Code Functionality:\nThe code you are about to execute has the capability to capture screenshots of your working desktop environment and active applications. These screenshots will be processed and sent to the GPT model for inference.\n\n\n## 2. Data Privacy and Storage:\nIt is crucial to note that Microsoft, the provider of this code, explicitly states that it does not collect or save any of the transmitted data. The captured screenshots are processed in real-time for the purpose of inference, and no permanent storage or record of this data is retained by Microsoft.\n\n## 3. User Responsibility:\nBy running the code, you understand and accept the responsibility for the content and nature of the data present on your desktop during the execution period. It is your responsibility to ensure that no sensitive or confidential information is visible or captured during this process.\n\n## 4. Security Measures:\nMicrosoft has implemented security measures to safeguard the action execution. However, it is recommended that you run the code in a secure and controlled environment to minimize potential risks. Ensure that you are running the latest security updates on your system.\n\n## 5. Consent for Inference:\nYou explicitly provide consent for the GPT model to analyze the captured screenshots for the purpose of generating relevant outputs. This consent is inherent in the act of executing the code.\n\n## 6. No Guarantee of Accuracy:\nThe outputs generated by the GPT model are based on patterns learned during training and may not always be accurate or contextually relevant. Microsoft does not guarantee the accuracy or suitability of the inferences made by the model.\n\n## 7. Indemnification:\nUsers agree to defend, indemnify, and hold Microsoft harmless from and against all damages, costs, and attorneys' fees in connection with any claims arising from the use of this Repo.\n\n## 8. Reporting Infringements:\nIf anyone believes that this Repo infringes on their rights, please notify the project owner via the provided project owner email. Microsoft will investigate and take appropriate actions as necessary.\n\n## 9. Modifications to the Disclaimer:\nMicrosoft reserves the right to update or modify this disclaimer at any time without prior notice. It is your responsibility to review the disclaimer periodically for any changes.\n\nBy proceeding to execute the code, you acknowledge that you have read, understood, and agreed to the terms outlined in this disclaimer. If you do not agree with these terms, refrain from running the provided code."
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0478515625,
          "content": "Copyright (c) Microsoft Corporation.\n\nMIT License\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 20.7705078125,
          "content": "<h1 align=\"center\">\n    <b>UFO</b> <img src=\"./assets/ufo_blue.png\" alt=\"UFO Image\" width=\"40\">: A <b>U</b>I-<b>Fo</b>cused Agent for Windows OS Interaction\n</h1>\n\n\n<div align=\"center\">\n\n[![arxiv](https://img.shields.io/badge/Paper-arXiv:202402.07939-b31b1b.svg)](https://arxiv.org/abs/2402.07939)&ensp;\n![Python Version](https://img.shields.io/badge/Python-3776AB?&logo=python&logoColor=white-blue&label=3.10%20%7C%203.11)&ensp;\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)&ensp;\n[![Documentation](https://img.shields.io/badge/Documentation-%230ABAB5?style=flat&logo=readthedocs&logoColor=black)](https://microsoft.github.io/UFO/)&ensp;\n[![YouTube](https://img.shields.io/badge/YouTube-white?logo=youtube&logoColor=%23FF0000)](https://www.youtube.com/watch?v=QT_OhygMVXU)&ensp;\n<!-- [![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/UFO_Agent)](https://twitter.com/intent/follow?screen_name=UFO_Agent) -->\n<!-- ![Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)&ensp; -->\n\n</div>\n\n**UFO** is a **UI-Focused** multi-agent framework to fulfill user requests on **Windows OS** by seamlessly navigating and operating within individual or spanning multiple applications.\n\n<h1 align=\"center\">\n    <img src=\"./assets/overview_n.png\"/> \n</h1>\n\n\n## 🕌 Framework\n<b>UFO</b> <img src=\"./assets/ufo_blue.png\" alt=\"UFO Image\" width=\"24\"> operates as a multi-agent framework, encompassing:\n- <b>HostAgent 🤖</b>, tasked with choosing an application for fulfilling user requests. This agent may also switch to a different application when a request spans multiple applications, and the task is partially completed in the preceding application. \n- <b>AppAgent 👾</b>, responsible for iteratively executing actions on the selected applications until the task is successfully concluded within a specific application. \n- <b>Application Automator 🎮</b>, is tasked with translating actions from HostAgent and AppAgent into interactions with the application and through UI controls, native APIs or AI tools. Check out more details [here](https://microsoft.github.io/UFO/automator/overview/).\n\nBoth agents leverage the multi-modal capabilities of GPT-4V(o) to comprehend the application UI and fulfill the user's request. For more details, please consult our [technical report](https://arxiv.org/abs/2402.07939) and [documentation](https://microsoft.github.io/UFO/).\n<h1 align=\"center\">\n    <img src=\"./assets/framework_v2.png\"/> \n</h1>\n\n\n## 📢 News\n- 📅 2024-12-13: We have a **New Release for v1.2.0!**! Checkout our new features and improvements:\n    1. **Large Action Model (LAM) Data Collection:** We have released the code and sample data for Large Action Model (LAM) data collection with UFO! Please checkout our [new paper](https://arxiv.org/abs/2412.10047), [code](dataflow/README.md) and [documentation](https://microsoft.github.io/UFO/dataflow/overview/) for more details.    \n    2. **Bash Command Support:** HostAgent also support bash command now!\n    3. **Bug Fixes:** We have fixed some bugs, error handling, and improved the overall performance.\n- 📅 2024-09-08: We have a **New Release for v1.1.0!**, to allows UFO to click on any region of the application and reduces its latency by up tp 1/3!\n- 📅 2024-07-06: We have a **New Release for v1.0.0!**.  You can check out our [documentation](https://microsoft.github.io/UFO/). We welcome your contributions and feedback!\n- 📅 2024-06-28: We are thrilled to announce that our official introduction video is now available on [YouTube](https://www.youtube.com/watch?v=QT_OhygMVXU)!\n<!-- - 📅 2024-06-25: **New Release for v0.2.1!**  We are excited to announce the release of version 0.2.1! This update includes several new features and improvements:\n    1. **HostAgent Refactor:** We've refactored the HostAgent to enhance its efficiency in managing AppAgents within UFO.\n    2. **Evaluation Agent:** Introducing an evaluation agent that assesses task completion and provides real-time feedback.\n    3. **Google Gemini && Claude Support:** UFO now supports Google Gemini and Cluade as the inference engine. Refer to our detailed guide in [Gemini documentation](https://microsoft.github.io/UFO/supported_models/gemini/) or [Claude documentation](https://microsoft.github.io/UFO/supported_models/claude/).\n    4. **Customized User Agents:** Users can now create customized agents by simply answering a few questions.\n- 📅 2024-05-21: We have reached 5K stars!✨\n- 📅 2024-05-08: **New Release for v0.1.1!** We've made some significant updates! Previously known as AppAgent and ActAgent, we've rebranded them to HostAgent and AppAgent to better align with their functionalities. Explore the latest enhancements:\n    1. **Learning from Human Demonstration:** UFO now supports learning from human demonstration! Utilize the [Windows Step Recorder](https://support.microsoft.com/en-us/windows/record-steps-to-reproduce-a-problem-46582a9b-620f-2e36-00c9-04e25d784e47) to record your steps and demonstrate them for UFO. Refer to our detailed guide in [README.md](https://microsoft.github.io/UFO/creating_app_agent/demonstration_provision/) for more information.\n    2. **Win32 Support:** We've incorporated support for [Win32](https://learn.microsoft.com/en-us/windows/win32/controls/window-controls) as a control backend, enhancing our UI automation capabilities.\n    3. **Extended Application Interaction:** UFO now goes beyond UI controls, allowing interaction with your application through keyboard inputs and native APIs! Presently, we support Word ([examples](/ufo/prompts/apps/word/api.yaml)), with more to come soon. Customize and build your own interactions.\n    4. **Control Filtering:** Streamline LLM's action process by using control filters to remove irrelevant control items. Enable them in [config_dev.yaml](/ufo/config/config_dev.yaml) under the `control filtering` section at the bottom.\n- 📅 2024-03-25: **New Release for v0.0.1!** Check out our exciting new features.\n    1. We now support creating your help documents for each Windows application to become an app expert. Check the [documentation](https://microsoft.github.io/UFO/creating_app_agent/help_document_provision/) for more details!\n    2. UFO now supports RAG from offline documents and online Bing search.\n    3. You can save the task completion trajectory into its memory for UFO's reference, improving its future success rate!\n    4. You can customize different GPT models for HostAgent and AppAgent. Text-only models (e.g., GPT-4) are now supported! -->\n- 📅 ...\n- 📅 2024-02-14: Our [technical report](https://arxiv.org/abs/2402.07939) is online!\n- 📅 2024-02-10: UFO is released on GitHub🎈. Happy Chinese New year🐉!\n\n\n## 🌐 Media Coverage \n\nUFO sightings have garnered attention from various media outlets, including:\n- [Microsoft's UFO abducts traditional user interfaces for a smarter Windows experience](https://the-decoder.com/microsofts-ufo-abducts-traditional-user-interfaces-for-a-smarter-windows-experience/)\n- [🚀 UFO & GPT-4-V: Sit back and relax, mientras GPT lo hace todo🌌](https://www.linkedin.com/posts/gutierrezfrancois_ai-ufo-microsoft-activity-7176819900399652865-pLoo?utm_source=share&utm_medium=member_desktop)\n- [The AI PC - The Future of Computers? - Microsoft UFO](https://www.youtube.com/watch?v=1k4LcffCq3E)\n- [下一代Windows系统曝光：基于GPT-4V，Agent跨应用调度，代号UFO](https://baijiahao.baidu.com/s?id=1790938358152188625&wfr=spider&for=pc)\n- [下一代智能版 Windows 要来了？微软推出首个 Windows Agent，命名为 UFO！](https://blog.csdn.net/csdnnews/article/details/136161570)\n- [Microsoft発のオープンソース版「UFO」登場！　Windowsを自動操縦するAIエージェントを試す](https://internet.watch.impress.co.jp/docs/column/shimizu/1570581.html)\n- ...\n\nThese sources provide insights into the evolving landscape of technology and the implications of UFO phenomena on various platforms.\n\n\n## 💥 Highlights\n\n- [x] **First Windows Agent** - UFO is the pioneering agent framework capable of translating user requests in natural language into actionable operations on Windows OS.\n- [x] **Agent as an Expert** - UFO is enhanced by Retrieval Augmented Generation (RAG) from heterogeneous sources, including offline help documents, online search engines, and human demonstrations, making the agent an application \"expert\".\n- [x] **Rich Skill Set** - UFO is equipped with a diverse set of skills to support comprehensive automation, such as mouse, keyboard, native API, and \"Copilot\".\n- [x] **Interactive Mode** - UFO facilitates multiple sub-requests from users within the same session, enabling the seamless completion of complex tasks.\n- [x] **Agent Customization** - UFO allows users to customize their own agents by providing additional information. The agent will proactively query users for details when necessary to better tailor its behavior.\n- [x] **Scalable AppAgent Creation** - UFO offers extensibility, allowing users and app developers to create their own AppAgents in an easy and scalable way.\n\n\n## ✨ Getting Started\n\n\n### 🛠️ Step 1: Installation\nUFO requires **Python >= 3.10** running on **Windows OS >= 10**. It can be installed by running the following command:\n```bash\n# [optional to create conda environment]\n# conda create -n ufo python=3.10\n# conda activate ufo\n\n# clone the repository\ngit clone https://github.com/microsoft/UFO.git\ncd UFO\n# install the requirements\npip install -r requirements.txt\n# If you want to use the Qwen as your LLMs, uncomment the related libs.\n```\n\n### ⚙️ Step 2: Configure the LLMs\nBefore running UFO, you need to provide your LLM configurations **individually for HostAgent and AppAgent**. You can create your own config file `ufo/config/config.yaml`, by copying the `ufo/config/config.yaml.template` and editing config for **HOST_AGENT** and **APP_AGENT** as follows: \n\n\n#### OpenAI\n```bash\nVISUAL_MODE: True, # Whether to use the visual mode\nAPI_TYPE: \"openai\" , # The API type, \"openai\" for the OpenAI API.  \nAPI_BASE: \"https://api.openai.com/v1/chat/completions\", # The the OpenAI API endpoint.\nAPI_KEY: \"sk-\",  # The OpenAI API key, begin with sk-\nAPI_VERSION: \"2024-02-15-preview\", # \"2024-02-15-preview\" by default\nAPI_MODEL: \"gpt-4-vision-preview\",  # The only OpenAI model\n```\n\n#### Azure OpenAI (AOAI)\n```bash\nVISUAL_MODE: True, # Whether to use the visual mode\nAPI_TYPE: \"aoai\" , # The API type, \"aoai\" for the Azure OpenAI.  \nAPI_BASE: \"YOUR_ENDPOINT\", #  The AOAI API address. Format: https://{your-resource-name}.openai.azure.com\nAPI_KEY: \"YOUR_KEY\",  # The aoai API key\nAPI_VERSION: \"2024-02-15-preview\", # \"2024-02-15-preview\" by default\nAPI_MODEL: \"gpt-4-vision-preview\",  # The only OpenAI model\nAPI_DEPLOYMENT_ID: \"YOUR_AOAI_DEPLOYMENT\", # The deployment id for the AOAI API\n```\nYou can also non-visial model (e.g., GPT-4) for each agent, by setting `VISUAL_MODE: False` and proper `API_MODEL` (openai) and `API_DEPLOYMENT_ID` (aoai). You can also optionally set an backup LLM engine in the field of `BACKUP_AGENT` if the above engines failed during the inference.\n\n\n####  Non-Visual Model Configuration\nYou can utilize non-visual models (e.g., GPT-4) for each agent by configuring the following settings in the `config.yaml` file:\n\n- ```VISUAL_MODE: False # To enable non-visual mode.```\n- Specify the appropriate `API_MODEL` (OpenAI) and `API_DEPLOYMENT_ID` (AOAI) for each agent.\n\nOptionally, you can set a backup language model (LLM) engine in the `BACKUP_AGENT` field to handle cases where the primary engines fail during inference. Ensure you configure these settings accurately to leverage non-visual models effectively.\n\n#### NOTE 💡 \nUFO also supports other LLMs and advanced configurations, such as customize your own model, please check the [documents](https://microsoft.github.io/UFO/supported_models/overview/) for more details. Because of the limitations of model input, a lite version of the prompt is provided to allow users to experience it, which is configured in `config_dev.yaml`.\n\n### 📔 Step 3: Additional Setting for RAG (optional).\nIf you want to enhance UFO's ability with external knowledge, you can optionally configure it with an external database for retrieval augmented generation (RAG) in the `ufo/config/config.yaml` file. \n\nWe provide the following options for RAG to enhance UFO's capabilities:\n- [Offline Help Document](https://microsoft.github.io/UFO/advanced_usage/reinforce_appagent/learning_from_help_document/) Enable UFO to retrieve information from offline help documents.\n- [Online Bing Search Engine](https://microsoft.github.io/UFO/advanced_usage/reinforce_appagent/learning_from_bing_search/): Enhance UFO's capabilities by utilizing the most up-to-date online search results.\n- [Self-Experience](https://microsoft.github.io/UFO/advanced_usage/reinforce_appagent/experience_learning/): Save task completion trajectories into UFO's memory for future reference.\n- [User-Demonstration](https://microsoft.github.io/UFO/advanced_usage/reinforce_appagent/learning_from_demonstration/): Boost UFO's capabilities through user demonstration.\n\nConsult their respective documentation for more information on how to configure these settings.\n\n<!-- #### RAG from Offline Help Document\nBefore enabling this function, you need to create an offline indexer for your help document. Please refer to the [README](./learner/README.md) to learn how to create an offline vectored database for retrieval. You can enable this function by setting the following configuration:\n```bash\n## RAG Configuration for the offline docs\nRAG_OFFLINE_DOCS: True  # Whether to use the offline RAG.\nRAG_OFFLINE_DOCS_RETRIEVED_TOPK: 1  # The topk for the offline retrieved documents\n```\nAdjust `RAG_OFFLINE_DOCS_RETRIEVED_TOPK` to optimize performance.\n\n\n####  RAG from Online Bing Search Engine\nEnhance UFO's ability by utilizing the most up-to-date online search results! To use this function, you need to obtain a Bing search API key. Activate this feature by setting the following configuration:\n```bash\n## RAG Configuration for the Bing search\nBING_API_KEY: \"YOUR_BING_SEARCH_API_KEY\"  # The Bing search API key\nRAG_ONLINE_SEARCH: True  # Whether to use the online search for the RAG.\nRAG_ONLINE_SEARCH_TOPK: 5  # The topk for the online search\nRAG_ONLINE_RETRIEVED_TOPK: 1 # The topk for the online retrieved documents\n```\nAdjust `RAG_ONLINE_SEARCH_TOPK` and `RAG_ONLINE_RETRIEVED_TOPK` to get better performance.\n\n\n#### RAG from Self-Demonstration\nSave task completion trajectories into UFO's memory for future reference. This can improve its future success rates based on its previous experiences!\n\nAfter completing a task, you'll see the following message:\n```\nWould you like to save the current conversation flow for future reference by the agent?\n[Y] for yes, any other key for no.\n```\nPress `Y` to save it into its memory and enable memory retrieval via the following configuration:\n```bash\n## RAG Configuration for experience\nRAG_EXPERIENCE: True  # Whether to use the RAG from its self-experience.\nRAG_EXPERIENCE_RETRIEVED_TOPK: 5  # The topk for the offline retrieved documents\n```\n\n#### RAG from User-Demonstration\nBoost UFO's capabilities through user demonstration! Utilize Microsoft Steps Recorder to record step-by-step processes for achieving specific tasks. With a simple command processed by the record_processor (refer to the [README](./record_processor/README.md)), UFO can store these trajectories in its memory for future reference, enhancing its learning from user interactions.\n\nYou can enable this function by setting the following configuration:\n```bash\n## RAG Configuration for demonstration\nRAG_DEMONSTRATION: True  # Whether to use the RAG from its user demonstration.\nRAG_DEMONSTRATION_RETRIEVED_TOPK: 5  # The topk for the demonstration examples.\n``` -->\n\n\n### 🎉 Step 4: Start UFO\n\n#### ⌨️ You can execute the following on your Windows command Line (CLI):\n\n```bash\n# assume you are in the cloned UFO folder\npython -m ufo --task <your_task_name>\n```\n\nThis will start the UFO process and you can interact with it through the command line interface. \nIf everything goes well, you will see the following message:\n\n```bash\nWelcome to use UFO🛸, A UI-focused Agent for Windows OS Interaction. \n _   _  _____   ___\n| | | ||  ___| / _ \\\n| | | || |_   | | | |\n| |_| ||  _|  | |_| |\n \\___/ |_|     \\___/\nPlease enter your request to be completed🛸:\n```\n#### ⚠️Reminder:  ####\n- Before UFO executing your request, please make sure the targeted applications are active on the system.\n- The GPT-V accepts screenshots of your desktop and application GUI as input. Please ensure that no sensitive or confidential information is visible or captured during the execution process. For further information, refer to [DISCLAIMER.md](./DISCLAIMER.md).\n\n\n###  Step 5 🎥: Execution Logs \n\nYou can find the screenshots taken and request & response logs in the following folder:\n```\n./ufo/logs/<your_task_name>/\n```\nYou may use them to debug, replay, or analyze the agent output.\n\n\n## ❓Get help \n* Please first check our our documentation [here](https://microsoft.github.io/UFO/).\n* ❔GitHub Issues (prefered)\n* For other communications, please contact [ufo-agent@microsoft.com](mailto:ufo-agent@microsoft.com).\n---\n\n\n<!-- ## 🎬 Demo Examples\n\nWe present two demo videos that complete user request on Windows OS using UFO. For more case study, please consult our [technical report](https://arxiv.org/abs/2402.07939).\n\n#### 1️⃣🗑️ Example 1: Deleting all notes on a PowerPoint presentation.\nIn this example, we will demonstrate how to efficiently use UFO to delete all notes on a PowerPoint presentation with just a few simple steps. Explore this functionality to enhance your productivity and work smarter, not harder!\n\n\nhttps://github.com/microsoft/UFO/assets/11352048/cf60c643-04f7-4180-9a55-5fb240627834\n\n\n\n#### 2️⃣📧 Example 2: Composing an email using text from multiple sources.\nIn this example, we will demonstrate how to utilize UFO to extract text from Word documents, describe an image, compose an email, and send it seamlessly. Enjoy the versatility and efficiency of cross-application experiences with UFO!\n\n\nhttps://github.com/microsoft/UFO/assets/11352048/aa41ad47-fae7-4334-8e0b-ba71c4fc32e0 -->\n\n\n\n\n\n## 📊 Evaluation\n\nPlease consult the [WindowsBench](https://arxiv.org/pdf/2402.07939.pdf) provided in Section A of the Appendix within our technical report. Here are some tips (and requirements) to aid in completing your request:\n\n- Prior to UFO execution of your request, ensure that the targeted application is active (though it may be minimized).\n- Please note that the output of GPT-V may not consistently align with the same request. If unsuccessful with your initial attempt, consider trying again.\n\n\n\n## 📚 Citation\nOur technical report paper can be found [here](https://arxiv.org/abs/2402.07939). Note that previous AppAgent and ActAgent in the paper are renamed to HostAgent and AppAgent in the code base to better reflect their functions.\nIf you use UFO in your research, please cite our paper:\n```\n@article{ufo,\n  title={{UFO: A UI-Focused Agent for Windows OS Interaction}},\n  author={Zhang, Chaoyun and Li, Liqun and He, Shilin and Zhang, Xu and Qiao, Bo and  Qin, Si and Ma, Minghua and Kang, Yu and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei and  Zhang, Qi},\n  journal={arXiv preprint arXiv:2402.07939},\n  year={2024}\n}\n```\n\n## 📝 Todo List\n- [x] RAG enhanced UFO.\n- [x] Support more control using Win32 API.\n- [x] [Documentation](https://microsoft.github.io/UFO/).\n- [ ] Support local host GUI interaction model.\n- [ ] Chatbox GUI for UFO.\n\n\n\n## 🎨 Related Projects\n1. If you're interested in data analytics agent frameworks, check out [TaskWeaver](https://github.com/microsoft/TaskWeaver?tab=readme-ov-file), a code-first LLM agent framework designed for seamlessly planning and executing data analytics tasks.\n\n2. For more information on GUI agents, refer to our survey paper: [Large Language Model-Brained GUI Agents: A Survey](https://arxiv.org/abs/2411.18279). You can also explore the survey through:\n- [Paper](https://arxiv.org/abs/2411.18279)\n- [GitHub Repository](https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey)\n- [Searchable Website](https://vyokky.github.io/LLM-Brained-GUI-Agents-Survey/)\n\n## ⚠️ Disclaimer\nBy choosing to run the provided code, you acknowledge and agree to the following terms and conditions regarding the functionality and data handling practices in [DISCLAIMER.md](./DISCLAIMER.md)\n\n\n## <img src=\"./assets/ufo_blue.png\" alt=\"logo\" width=\"30\"> Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 2.59375,
          "content": "<!-- BEGIN MICROSOFT SECURITY.MD V0.0.9 BLOCK -->\n\n## Security\n\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet) and [Xamarin](https://github.com/xamarin).\n\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/security.md/definition), please report it to us as described below.\n\n## Reporting Security Issues\n\n**Please do not report security vulnerabilities through public GitHub issues.**\n\nInstead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/security.md/msrc/create-report).\n\nIf you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/security.md/msrc/pgp).\n\nYou should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc). \n\nPlease include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:\n\n  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)\n  * Full paths of source file(s) related to the manifestation of the issue\n  * The location of the affected source code (tag/branch/commit or direct URL)\n  * Any special configuration required to reproduce the issue\n  * Step-by-step instructions to reproduce the issue\n  * Proof-of-concept or exploit code (if possible)\n  * Impact of the issue, including how an attacker might exploit the issue\n\nThis information will help us triage your report more quickly.\n\nIf you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/security.md/msrc/bounty) page for more details about our active programs.\n\n## Preferred Languages\n\nWe prefer all communications to be in English.\n\n## Policy\n\nMicrosoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/security.md/cvd).\n\n<!-- END MICROSOFT SECURITY.MD BLOCK -->\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataflow",
          "type": "tree",
          "content": null
        },
        {
          "name": "documents",
          "type": "tree",
          "content": null
        },
        {
          "name": "learner",
          "type": "tree",
          "content": null
        },
        {
          "name": "model_worker",
          "type": "tree",
          "content": null
        },
        {
          "name": "record_processor",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.4052734375,
          "content": "art==6.1\ncolorama==0.4.6\nlangchain==0.1.11\nlangchain_community==0.0.27\nmsal==1.25.0\nopenai==1.13.3\nPillow==10.3.0\npywin32==306\npywinauto==0.6.8\nPyYAML==6.0.1\nRequests==2.32.0\nfaiss-cpu==1.8.0\nlxml==5.1.0\npsutil==5.9.8\nbeautifulsoup4==4.12.3\nsentence-transformers==2.5.1\npandas==1.4.3\nhtml2text==2024.2.26\n##For Qwen\n#dashscope==1.15.0\n##For removing stopwords\n#nltk==3.8.1\n##For Gemini\n#google-generativeai==0.7.0\n\n"
        },
        {
          "name": "ufo",
          "type": "tree",
          "content": null
        },
        {
          "name": "vectordb",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}