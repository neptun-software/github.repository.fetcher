{
  "metadata": {
    "timestamp": 1736561038194,
    "page": 811,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "TachibanaYoshino/AnimeGANv2",
      "stars": 5163,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": "AnimeGANv2.png",
          "type": "blob",
          "size": 8221.0859375,
          "content": ""
        },
        {
          "name": "AnimeGANv2.py",
          "type": "blob",
          "size": 14.5322265625,
          "content": "from tools.ops import *\nfrom tools.utils import *\nfrom glob import glob\nimport time\nimport numpy as np\nfrom net import generator\nfrom net.discriminator import D_net\nfrom tools.data_loader import ImageGenerator\nfrom tools.vgg19 import Vgg19\n\nclass AnimeGANv2(object) :\n    def __init__(self, sess, args):\n        self.model_name = 'AnimeGANv2'\n        self.sess = sess\n        self.checkpoint_dir = args.checkpoint_dir\n        self.log_dir = args.log_dir\n        self.dataset_name = args.dataset\n\n        self.epoch = args.epoch\n        self.init_epoch = args.init_epoch # args.epoch // 20\n\n        self.gan_type = args.gan_type\n        self.batch_size = args.batch_size\n        self.save_freq = args.save_freq\n\n        self.init_lr = args.init_lr\n        self.d_lr = args.d_lr\n        self.g_lr = args.g_lr\n\n        \"\"\" Weight \"\"\"\n        self.g_adv_weight = args.g_adv_weight\n        self.d_adv_weight = args.d_adv_weight\n        self.con_weight = args.con_weight\n        self.sty_weight = args.sty_weight\n        self.color_weight = args.color_weight\n        self.tv_weight = args.tv_weight\n\n        self.training_rate = args.training_rate\n        self.ld = args.ld\n\n        self.img_size = args.img_size\n        self.img_ch = args.img_ch\n\n        \"\"\" Discriminator \"\"\"\n        self.n_dis = args.n_dis\n        self.ch = args.ch\n        self.sn = args.sn\n\n        self.sample_dir = os.path.join(args.sample_dir, self.model_dir)\n        check_folder(self.sample_dir)\n\n        self.real = tf.placeholder(tf.float32, [self.batch_size, self.img_size[0], self.img_size[1], self.img_ch], name='real_A')\n        self.anime = tf.placeholder(tf.float32, [self.batch_size, self.img_size[0], self.img_size[1], self.img_ch], name='anime_A')\n        self.anime_smooth = tf.placeholder(tf.float32, [self.batch_size, self.img_size[0], self.img_size[1], self.img_ch], name='anime_smooth_A')\n        self.test_real = tf.placeholder(tf.float32, [1, None, None, self.img_ch], name='test_input')\n\n        self.anime_gray = tf.placeholder(tf.float32, [self.batch_size, self.img_size[0], self.img_size[1], self.img_ch],name='anime_B')\n\n\n        self.real_image_generator = ImageGenerator('./dataset/train_photo', self.img_size, self.batch_size)\n        self.anime_image_generator = ImageGenerator('./dataset/{}'.format(self.dataset_name + '/style'), self.img_size, self.batch_size)\n        self.anime_smooth_generator = ImageGenerator('./dataset/{}'.format(self.dataset_name + '/smooth'), self.img_size, self.batch_size)\n        self.dataset_num = max(self.real_image_generator.num_images, self.anime_image_generator.num_images)\n\n        self.vgg = Vgg19()\n\n        print()\n        print(\"##### Information #####\")\n        print(\"# gan type : \", self.gan_type)\n        print(\"# dataset : \", self.dataset_name)\n        print(\"# max dataset number : \", self.dataset_num)\n        print(\"# batch_size : \", self.batch_size)\n        print(\"# epoch : \", self.epoch)\n        print(\"# init_epoch : \", self.init_epoch)\n        print(\"# training image size [H, W] : \", self.img_size)\n        print(\"# g_adv_weight,d_adv_weight,con_weight,sty_weight,color_weight,tv_weight : \", self.g_adv_weight,self.d_adv_weight,self.con_weight,self.sty_weight,self.color_weight,self.tv_weight)\n        print(\"# init_lr,g_lr,d_lr : \", self.init_lr,self.g_lr,self.d_lr)\n        print(f\"# training_rate G -- D: {self.training_rate} : 1\" )\n        print()\n\n    ##################################################################################\n    # Generator\n    ##################################################################################\n\n    def generator(self, x_init, reuse=False, scope=\"generator\"):\n        with tf.variable_scope(scope, reuse=reuse):\n            G = generator.G_net(x_init)\n            return G.fake\n\n    ##################################################################################\n    # Discriminator\n    ##################################################################################\n\n    def discriminator(self, x_init, reuse=False, scope=\"discriminator\"):\n            D = D_net(x_init, self.ch, self.n_dis, self.sn, reuse=reuse, scope=scope)\n            return D\n\n    ##################################################################################\n    # Model\n    ##################################################################################\n    def gradient_panalty(self, real, fake, scope=\"discriminator\"):\n        if self.gan_type.__contains__('dragan') :\n            eps = tf.random_uniform(shape=tf.shape(real), minval=0., maxval=1.)\n            _, x_var = tf.nn.moments(real, axes=[0, 1, 2, 3])\n            x_std = tf.sqrt(x_var)  # magnitude of noise decides the size of local region\n\n            fake = real + 0.5 * x_std * eps\n\n        alpha = tf.random_uniform(shape=[self.batch_size, 1, 1, 1], minval=0., maxval=1.)\n        interpolated = real + alpha * (fake - real)\n\n        logit, _= self.discriminator(interpolated, reuse=True, scope=scope)\n\n        grad = tf.gradients(logit, interpolated)[0] # gradient of D(interpolated)\n        grad_norm = tf.norm(flatten(grad), axis=1) # l2 norm\n\n        GP = 0\n        # WGAN - LP\n        if self.gan_type.__contains__('lp'):\n            GP = self.ld * tf.reduce_mean(tf.square(tf.maximum(0.0, grad_norm - 1.)))\n\n        elif self.gan_type.__contains__('gp') or self.gan_type == 'dragan' :\n            GP = self.ld * tf.reduce_mean(tf.square(grad_norm - 1.))\n\n        return GP\n\n    def build_model(self):\n\n        \"\"\" Define Generator, Discriminator \"\"\"\n        self.generated = self.generator(self.real)\n        self.test_generated = self.generator(self.test_real, reuse=True)\n\n\n        anime_logit = self.discriminator(self.anime)\n        anime_gray_logit = self.discriminator(self.anime_gray, reuse=True)\n\n        generated_logit = self.discriminator(self.generated, reuse=True)\n        smooth_logit = self.discriminator(self.anime_smooth, reuse=True)\n\n        \"\"\" Define Loss \"\"\"\n        if self.gan_type.__contains__('gp') or self.gan_type.__contains__('lp') or self.gan_type.__contains__('dragan') :\n            GP = self.gradient_panalty(real=self.anime, fake=self.generated)\n        else :\n            GP = 0.0\n\n        # init pharse\n        init_c_loss = con_loss(self.vgg, self.real, self.generated)\n        init_loss = self.con_weight * init_c_loss\n        \n        self.init_loss = init_loss\n\n        # gan\n        c_loss, s_loss = con_sty_loss(self.vgg, self.real, self.anime_gray, self.generated)\n        tv_loss = self.tv_weight * total_variation_loss(self.generated)\n        t_loss = self.con_weight * c_loss + self.sty_weight * s_loss + color_loss(self.real,self.generated) * self.color_weight + tv_loss\n\n        g_loss = self.g_adv_weight * generator_loss(self.gan_type, generated_logit)\n        d_loss = self.d_adv_weight * discriminator_loss(self.gan_type, anime_logit, anime_gray_logit, generated_logit, smooth_logit) + GP\n\n        self.Generator_loss =  t_loss + g_loss\n        self.Discriminator_loss = d_loss\n\n        \"\"\" Training \"\"\"\n        t_vars = tf.trainable_variables()\n        G_vars = [var for var in t_vars if 'generator' in var.name]\n        D_vars = [var for var in t_vars if 'discriminator' in var.name]\n\n        self.init_optim = tf.train.AdamOptimizer(self.init_lr, beta1=0.5, beta2=0.999).minimize(self.init_loss, var_list=G_vars)\n        self.G_optim = tf.train.AdamOptimizer(self.g_lr , beta1=0.5, beta2=0.999).minimize(self.Generator_loss, var_list=G_vars)\n        self.D_optim = tf.train.AdamOptimizer(self.d_lr , beta1=0.5, beta2=0.999).minimize(self.Discriminator_loss, var_list=D_vars)\n\n        \"\"\"\" Summary \"\"\"\n        self.G_loss = tf.summary.scalar(\"Generator_loss\", self.Generator_loss)\n        self.D_loss = tf.summary.scalar(\"Discriminator_loss\", self.Discriminator_loss)\n\n        self.G_gan = tf.summary.scalar(\"G_gan\", g_loss)\n        self.G_vgg = tf.summary.scalar(\"G_vgg\", t_loss)\n        self.G_init_loss = tf.summary.scalar(\"G_init\", init_loss)\n\n        self.V_loss_merge = tf.summary.merge([self.G_init_loss])\n        self.G_loss_merge = tf.summary.merge([self.G_loss, self.G_gan, self.G_vgg, self.G_init_loss])\n        self.D_loss_merge = tf.summary.merge([self.D_loss])\n\n    def train(self):\n        # initialize all variables\n        self.sess.run(tf.global_variables_initializer())\n\n        # saver to save model\n        self.saver = tf.train.Saver(max_to_keep=self.epoch)\n\n        # summary writer\n        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_dir, self.sess.graph)\n\n        \"\"\" Input Image\"\"\"\n        real_img_op, anime_img_op, anime_smooth_op  = self.real_image_generator.load_images(), self.anime_image_generator.load_images(), self.anime_smooth_generator.load_images()\n\n\n        # restore check-point if it exits\n        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n        if could_load:\n            start_epoch = checkpoint_counter + 1\n\n            print(\" [*] Load SUCCESS\")\n        else:\n            start_epoch = 0\n\n            print(\" [!] Load failed...\")\n\n        # loop for epoch\n        init_mean_loss = []\n        mean_loss = []\n        # training times , G : D = self.training_rate : 1\n        j = self.training_rate\n        for epoch in range(start_epoch, self.epoch):\n            for idx in range(int(self.dataset_num / self.batch_size)):\n                anime, anime_smooth, real = self.sess.run([anime_img_op, anime_smooth_op, real_img_op])\n                train_feed_dict = {\n                    self.real:real[0],\n                    self.anime:anime[0],\n                    self.anime_gray:anime[1],\n                    self.anime_smooth:anime_smooth[1]\n                }\n\n                if epoch < self.init_epoch :\n                    # Init G\n                    start_time = time.time()\n\n                    real_images, generator_images, _, v_loss, summary_str = self.sess.run([self.real, self.generated,\n                                                                             self.init_optim,\n                                                                             self.init_loss, self.V_loss_merge], feed_dict = train_feed_dict)\n                    self.writer.add_summary(summary_str, epoch)\n                    init_mean_loss.append(v_loss)\n\n                    print(\"Epoch: %3d Step: %5d / %5d  time: %f s init_v_loss: %.8f  mean_v_loss: %.8f\" % (epoch, idx,int(self.dataset_num / self.batch_size), time.time() - start_time, v_loss, np.mean(init_mean_loss)))\n                    if (idx+1)%200 ==0:\n                        init_mean_loss.clear()\n                else :\n                    start_time = time.time()\n\n                    if j == self.training_rate:\n                        # Update D\n                        _, d_loss, summary_str = self.sess.run([self.D_optim, self.Discriminator_loss, self.D_loss_merge],\n                                                            feed_dict=train_feed_dict)\n                        self.writer.add_summary(summary_str, epoch)\n\n                    # Update G\n                    real_images, generator_images, _, g_loss, summary_str = self.sess.run([self.real, self.generated,self.G_optim,\n                                                                                              self.Generator_loss, self.G_loss_merge], feed_dict = train_feed_dict)\n                    self.writer.add_summary(summary_str, epoch)\n\n                    mean_loss.append([d_loss, g_loss])\n                    if j == self.training_rate:\n\n                        print(\n                            \"Epoch: %3d Step: %5d / %5d  time: %f s d_loss: %.8f, g_loss: %.8f -- mean_d_loss: %.8f, mean_g_loss: %.8f\" % (\n                                epoch, idx, int(self.dataset_num / self.batch_size), time.time() - start_time, d_loss, g_loss, np.mean(mean_loss, axis=0)[0],\n                                np.mean(mean_loss, axis=0)[1]))\n                    else:\n                        print(\n                            \"Epoch: %3d Step: %5d / %5d time: %f s , g_loss: %.8f --  mean_g_loss: %.8f\" % (\n                                epoch, idx, int(self.dataset_num / self.batch_size), time.time() - start_time, g_loss, np.mean(mean_loss, axis=0)[1]))\n\n                    if (idx + 1) % 200 == 0:\n                        mean_loss.clear()\n\n                    j = j - 1\n                    if j < 1:\n                        j = self.training_rate\n\n\n            if (epoch + 1) >= self.init_epoch and np.mod(epoch + 1, self.save_freq) == 0:\n                self.save(self.checkpoint_dir, epoch)\n\n            if epoch >= self.init_epoch -1:\n                \"\"\" Result Image \"\"\"\n                val_files = glob('./dataset/{}/*.*'.format('val'))\n                save_path = './{}/{:03d}/'.format(self.sample_dir, epoch)\n                check_folder(save_path)\n                for i, sample_file in enumerate(val_files):\n                    print('val: '+ str(i) + sample_file)\n                    sample_image = np.asarray(load_test_data(sample_file, self.img_size))\n                    test_real,test_generated = self.sess.run([self.test_real,self.test_generated],feed_dict = {self.test_real:sample_image} )\n                    save_images(test_real, save_path+'{:03d}_a.jpg'.format(i), None)\n                    save_images(test_generated, save_path+'{:03d}_b.jpg'.format(i), None)\n\n    @property\n    def model_dir(self):\n        return \"{}_{}_{}_{}_{}_{}_{}_{}_{}\".format(self.model_name, self.dataset_name,\n                                                          self.gan_type,\n                                                          int(self.g_adv_weight), int(self.d_adv_weight),\n                                                          int(self.con_weight), int(self.sty_weight),\n                                                          int(self.color_weight), int(self.tv_weight))\n\n\n    def save(self, checkpoint_dir, step):\n        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        self.saver.save(self.sess, os.path.join(checkpoint_dir, self.model_name + '.model'), global_step=step)\n\n    def load(self, checkpoint_dir):\n        print(\" [*] Reading checkpoints...\")\n        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)\n\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir) # checkpoint file information\n\n        if ckpt and ckpt.model_checkpoint_path:\n            ckpt_name = os.path.basename(ckpt.model_checkpoint_path) # first line\n            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n            counter = int(ckpt_name.split('-')[-1])\n            print(\" [*] Success to read {}\".format(os.path.join(checkpoint_dir, ckpt_name)))\n            return True, counter\n        else:\n            print(\" [*] Failed to find a checkpoint\")\n            return False, 0\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.5107421875,
          "content": "# AnimeGANv2    \n\n The improved version of [AnimeGAN](https://github.com/TachibanaYoshino/AnimeGAN).  \n \n**[Project Page](https://tachibanayoshino.github.io/AnimeGANv2/)** | Landscape photos / videos to anime   \n\n   \n    \n  \n\n-----  \n**News**  \n* (2022.08.03)  Added the AnimeGANv2 Colab:    üñºÔ∏è Photos [![Photos Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1PbBkmj1EhULvEE8AXr2z84pZ2DQJN4hc/view?usp=sharing)  |   üéûÔ∏è Videos [![Colab for videos](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1qhBxA72Wxbh6Eyhd-V0zY_jTIblP9rHz/view?usp=sharing)      \n* (2021.12.25)  [**AnimeGANv3**](https://github.com/TachibanaYoshino/AnimeGANv3) has been released. :christmas_tree:  \n* (2021.02.21)  [The pytorch version of AnimeGANv2 has been released](https://github.com/bryandlee/animegan2-pytorch), Be grateful to @bryandlee for his contribution. \n* (2020.12.25)  AnimeGANv3 will be released along with its paper in the spring of 2021.  \n  \n\n------\n\n**Focus:**  \n<table border=\"1px ridge\">\n\t<tr align=\"center\">\n\t    <th>Anime style</th>\n\t    <th>Film</th>  \n\t    <th>Picture Number</th>  \n      <th>Quality</th>\n      <th>Download Style Dataset</th>\n\t</tr >\n\t<tr align=\"center\">\n      <td>Miyazaki Hayao</td>\n      <td>The Wind Rises</td>\n      <td>1752</td>\n      <td>1080p</td>\n\t    <td rowspan=\"3\"><a href=\"https://github.com/TachibanaYoshino/AnimeGANv2/releases/tag/1.0\">Link</a></td>\n\t</tr>\n\t<tr align=\"center\">\n\t    <td>Makoto Shinkai</td>  \n\t    <td>Your Name & Weathering with you</td>\n      <td>1445</td>\n      <td>BD</td>\n\t</tr>\n\t<tr align=\"center\">\n\t    <td>Kon Satoshi</td>\n\t    <td>Paprika</td>\n      <td>1284</td>\n      <td>BDRip</td>\n\t</tr>\n</table>  \n   \n**News:**    \n```yaml\nThe improvement directions of AnimeGANv2 mainly include the following 4 points:  \n```  \n- [x] 1. Solve the problem of high-frequency artifacts in the generated image.  \n- [x] 2. It is easy to train and directly achieve the effects in the paper.  \n- [x] 3. Further reduce the number of parameters of the generator network. **(generator size: 8.17 Mb)**, The lite version has a smaller generator model.  \n- [x] 4. Use new high-quality style data, which come from BD movies as much as possible.  \n   \n   &ensp;&ensp;&ensp;&ensp;&ensp;  AnimeGAN can be accessed from [here](https://github.com/TachibanaYoshino/AnimeGAN).  \n___  \n\n## Requirements  \n- python 3.6  \n- tensorflow-gpu 1.15.0 (GPU 2080Ti, cuda 10.0.130, cudnn 7.6.0)  \n- opencv  \n- tqdm  \n- numpy  \n- glob  \n- argparse  \n- onnxruntime (If onnx file needs to be run.)  \n  \n## Usage  \n### 1. Inference  \n  > `python test.py  --checkpoint_dir  checkpoint/generator_Hayao_weight  --test_dir dataset/test/HR_photo --save_dir Hayao/HR_photo`  \n    \n### 2. Convert video to anime  \n  > `python video2anime.py  --video video/input/„ÅäËä±Ë¶ã.mp4  --checkpoint_dir  checkpoint/generator_Hayao_weight  --output video/output`  \n    \n### 3. Train \n#### 1. Download vgg19    \n  > [vgg19.npy](https://github.com/TachibanaYoshino/AnimeGAN/releases/tag/vgg16%2F19.npy)  \n\n#### 2. Download Train/Val Photo dataset  \n  > [Link](https://github.com/TachibanaYoshino/AnimeGAN/releases/tag/dataset-1)  \n\n#### 3. Do edge_smooth  \n  > `python edge_smooth.py --dataset Hayao --img_size 256`  \n\n#### 4. Train  \n  >  `python train.py --dataset Hayao --epoch 101 --init_epoch 10`  \n  \n#### 5. Extract the weights of the generator  \n  >  `python get_generator_ckpt.py --checkpoint_dir  ../checkpoint/AnimeGANv2_Shinkai_lsgan_300_300_1_2_10_1  --style_name Shinkai`  \n  \n____  \n## Results  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/AnimeGANv2.png)   \n     \n____ \n:heart_eyes:  Photo  to  Paprika  Style  \n  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Paprika/concat/37.jpg)   \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Paprika/concat/38.jpg)     \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Paprika/concat/6.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Paprika/concat/7.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Paprika/concat/9.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Paprika/concat/21.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Paprika/concat/44.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Paprika/concat/1.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Paprika/concat/8.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Paprika/concat/11.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Paprika/concat/5.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Paprika/concat/15.jpg)   \n____  \n:heart_eyes:  Photo  to  Hayao  Style   \n  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Hayao/concat/AE86.jpg)   \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Hayao/concat/10.jpg)     \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Hayao/concat/15.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Hayao/concat/35.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Hayao/concat/39.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Hayao/concat/42.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Hayao/concat/44.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Hayao/concat/41.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Hayao/concat/32.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Hayao/concat/11.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Hayao/concat/34.jpg)   \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Hayao/concat/18.jpg)    \n____  \n:heart_eyes:  Photo  to  Shinkai  Style   \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Shinkai/concat/7.jpg)   \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Shinkai/concat/9.jpg)     \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Shinkai/concat/11.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Shinkai/concat/15.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Shinkai/concat/17.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Shinkai/concat/22.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Shinkai/concat/27.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Shinkai/concat/33.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Shinkai/concat/32.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Shinkai/concat/21.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Shinkai/concat/3.jpg)  \n![](https://github.com/TachibanaYoshino/AnimeGANv2/blob/master/results/Shinkai/concat/26.jpg)  \n  \n## License  \nThis repo is made freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications. Permission is granted to use the AnimeGANv2 given that you agree to my license terms. Regarding the request for commercial use, please contact us via email to help you obtain the  authorization letter.  \n## Author  \nXin Chen\n"
        },
        {
          "name": "checkpoint",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "net",
          "type": "tree",
          "content": null
        },
        {
          "name": "pb_and_onnx_model",
          "type": "tree",
          "content": null
        },
        {
          "name": "results",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 3.30078125,
          "content": "import argparse\r\nfrom tools.utils import *\r\nimport os\r\nfrom tqdm import tqdm\r\nfrom glob import glob\r\nimport time\r\nimport numpy as np\r\nfrom net import generator\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\ndef parse_args():\r\n    desc = \"AnimeGANv2\"\r\n    parser = argparse.ArgumentParser(description=desc)\r\n\r\n    parser.add_argument('--checkpoint_dir', type=str, default='checkpoint/'+'generator_Shinkai_weight',\r\n                        help='Directory name to save the checkpoints')\r\n    parser.add_argument('--test_dir', type=str, default='dataset/test/t',\r\n                        help='Directory name of test photos')\r\n    parser.add_argument('--save_dir', type=str, default='Shinkai/t',\r\n                        help='what style you want to get')\r\n    parser.add_argument('--if_adjust_brightness', type=bool, default=True,\r\n                        help='adjust brightness by the real photo')\r\n\r\n    \"\"\"checking arguments\"\"\"\r\n\r\n    return parser.parse_args()\r\n\r\ndef stats_graph(graph):\r\n    flops = tf.profiler.profile(graph, options=tf.profiler.ProfileOptionBuilder.float_operation())\r\n    # params = tf.profiler.profile(graph, options=tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())\r\n    print('FLOPs: {}'.format(flops.total_float_ops))\r\n\r\ndef test(checkpoint_dir, style_name, test_dir, if_adjust_brightness, img_size=[256,256]):\r\n    # tf.reset_default_graph()\r\n    result_dir = 'results/'+style_name\r\n    check_folder(result_dir)\r\n    test_files = glob('{}/*.*'.format(test_dir))\r\n\r\n    test_real = tf.placeholder(tf.float32, [1, None, None, 3], name='test')\r\n\r\n    with tf.variable_scope(\"generator\", reuse=False):\r\n        test_generated = generator.G_net(test_real).fake\r\n    saver = tf.train.Saver()\r\n\r\n    gpu_options = tf.GPUOptions(allow_growth=True)\r\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)) as sess:\r\n        # tf.global_variables_initializer().run()\r\n        # load model\r\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)  # checkpoint file information\r\n        if ckpt and ckpt.model_checkpoint_path:\r\n            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)  # first line\r\n            saver.restore(sess, os.path.join(checkpoint_dir, ckpt_name))\r\n            print(\" [*] Success to read {}\".format(os.path.join(checkpoint_dir, ckpt_name)))\r\n        else:\r\n            print(\" [*] Failed to find a checkpoint\")\r\n            return\r\n        # stats_graph(tf.get_default_graph())\r\n\r\n        begin = time.time()\r\n        for sample_file  in tqdm(test_files) :\r\n            # print('Processing image: ' + sample_file)\r\n            sample_image = np.asarray(load_test_data(sample_file, img_size))\r\n            image_path = os.path.join(result_dir,'{0}'.format(os.path.basename(sample_file)))\r\n            fake_img = sess.run(test_generated, feed_dict = {test_real : sample_image})\r\n            if if_adjust_brightness:\r\n                save_images(fake_img, image_path, sample_file)\r\n            else:\r\n                save_images(fake_img, image_path, None)\r\n        end = time.time()\r\n        print(f'test-time: {end-begin} s')\r\n        print(f'one image test time : {(end-begin)/len(test_files)} s')\r\nif __name__ == '__main__':\r\n    arg = parse_args()\r\n    print(arg.checkpoint_dir)\r\n    test(arg.checkpoint_dir, arg.save_dir, arg.test_dir, arg.if_adjust_brightness)\r\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 4.1796875,
          "content": "from AnimeGANv2 import AnimeGANv2\nimport argparse\nfrom tools.utils import *\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n\"\"\"parsing and configuration\"\"\"\n\ndef parse_args():\n    desc = \"AnimeGANv2\"\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument('--dataset', type=str, default='Hayao', help='dataset_name')\n\n    parser.add_argument('--epoch', type=int, default=101, help='The number of epochs to run')\n    parser.add_argument('--init_epoch', type=int, default=10, help='The number of epochs for weight initialization')\n    parser.add_argument('--batch_size', type=int, default=12, help='The size of batch size') # if light : batch_size = 20\n    parser.add_argument('--save_freq', type=int, default=1, help='The number of ckpt_save_freq')\n\n    parser.add_argument('--init_lr', type=float, default=2e-4, help='The learning rate')\n    parser.add_argument('--g_lr', type=float, default=2e-5, help='The learning rate')\n    parser.add_argument('--d_lr', type=float, default=4e-5, help='The learning rate')\n    parser.add_argument('--ld', type=float, default=10.0, help='The gradient penalty lambda')\n\n    parser.add_argument('--g_adv_weight', type=float, default=300.0, help='Weight about GAN')\n    parser.add_argument('--d_adv_weight', type=float, default=300.0, help='Weight about GAN')\n    parser.add_argument('--con_weight', type=float, default=1.5, help='Weight about VGG19')# 1.5 for Hayao, 2.0 for Paprika, 1.2 for Shinkai\n    # ------ the follow weight used in AnimeGAN\n    parser.add_argument('--sty_weight', type=float, default=2.5, help='Weight about style')# 2.5 for Hayao, 0.6 for Paprika, 2.0 for Shinkai\n    parser.add_argument('--color_weight', type=float, default=10., help='Weight about color') # 15. for Hayao, 50. for Paprika, 10. for Shinkai\n    parser.add_argument('--tv_weight', type=float, default=1., help='Weight about tv')# 1. for Hayao, 0.1 for Paprika, 1. for Shinkai\n    # ---------------------------------------------\n    parser.add_argument('--training_rate', type=int, default=1, help='training rate about G & D')\n    parser.add_argument('--gan_type', type=str, default='lsgan', help='[gan / lsgan / wgan-gp / wgan-lp / dragan / hinge')\n\n    parser.add_argument('--img_size', type=list, default=[256,256], help='The size of image: H and W')\n    parser.add_argument('--img_ch', type=int, default=3, help='The size of image channel')\n\n    parser.add_argument('--ch', type=int, default=64, help='base channel number per layer')\n    parser.add_argument('--n_dis', type=int, default=3, help='The number of discriminator layer')\n    parser.add_argument('--sn', type=str2bool, default=True, help='using spectral norm')\n\n\n    parser.add_argument('--checkpoint_dir', type=str, default='checkpoint',\n                        help='Directory name to save the checkpoints')\n    parser.add_argument('--log_dir', type=str, default='logs',\n                        help='Directory name to save training logs')\n    parser.add_argument('--sample_dir', type=str, default='samples',\n                        help='Directory name to save the samples on training')\n\n    return check_args(parser.parse_args())\n\n\"\"\"checking arguments\"\"\"\ndef check_args(args):\n    # --checkpoint_dir\n    check_folder(args.checkpoint_dir)\n\n    # --log_dir\n    check_folder(args.log_dir)\n\n    # --sample_dir\n    check_folder(args.sample_dir)\n\n    # --epoch\n    try:\n        assert args.epoch >= 1\n    except:\n        print('number of epochs must be larger than or equal to one')\n\n    # --batch_size\n    try:\n        assert args.batch_size >= 1\n    except:\n        print('batch size must be larger than or equal to one')\n    return args\n\n\n\"\"\"main\"\"\"\ndef main():\n    # parse arguments\n    args = parse_args()\n    if args is None:\n      exit()\n\n    # open session\n    gpu_options = tf.GPUOptions(allow_growth=True)\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,inter_op_parallelism_threads=8,\n                               intra_op_parallelism_threads=8,gpu_options=gpu_options)) as sess:\n        gan = AnimeGANv2(sess, args)\n\n        # build graph\n        gan.build_model()\n\n        # show network architecture\n        show_all_variables()\n\n        gan.train()\n        print(\" [*] Training finished!\")\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "vgg19_weight",
          "type": "tree",
          "content": null
        },
        {
          "name": "video",
          "type": "tree",
          "content": null
        },
        {
          "name": "video2anime.py",
          "type": "blob",
          "size": 4.3310546875,
          "content": "import argparse\nimport os\nimport cv2\nfrom tqdm import tqdm\nimport numpy as np\nimport tensorflow as tf\nfrom net import generator\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\ndef parse_args():\n    desc = \"Tensorflow implementation of AnimeGANv2\"\n    parser = argparse.ArgumentParser(description=desc)\n    parser.add_argument('--video', type=str, default='video/input/'+ '2.mp4',\n                        help='video file or number for webcam')\n    parser.add_argument('--checkpoint_dir', type=str, default='../checkpoint/generator_Paprika_weight',\n                        help='Directory name to save the checkpoints')\n    parser.add_argument('--output', type=str, default='video/output/' + 'Paprika',\n                        help='output path')\n    parser.add_argument('--output_format', type=str, default='MP4V',\n                        help='codec used in VideoWriter when saving video to file')\n    \"\"\"\n    output_format: xxx.mp4('MP4V'), xxx.mkv('FMP4'), xxx.flv('FLV1'), xxx.avi('XIVD')\n    ps. ffmpeg -i xxx.mkv -c:v libx264 -strict -2 xxxx.mp4, this command can convert mkv to mp4, which has small size.\n    \"\"\"\n    return parser.parse_args()\n\n\ndef check_folder(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n    return path\n\ndef process_image(img, x32=True):\n    h, w = img.shape[:2]\n    if x32: # resize image to multiple of 32s\n        def to_32s(x):\n            return 256 if x < 256 else x - x%32\n        img = cv2.resize(img, (to_32s(w), to_32s(h)))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)/ 127.5 - 1.0\n    return img\n\ndef post_precess(img, wh):\n    img = (img.squeeze()+1.) / 2 * 255\n    img = img.astype(np.uint8)\n    img = cv2.resize(img, (wh[0], wh[1]))\n    return img\n\ndef cvt2anime_video(video, output, checkpoint_dir, output_format='MP4V'):\n    '''\n    output_format: 4-letter code that specify codec to use for specific video type. e.g. for mp4 support use \"H264\", \"MP4V\", or \"X264\"\n    '''\n    gpu_stat = bool(len(tf.config.experimental.list_physical_devices('GPU')))\n    if gpu_stat:\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n    gpu_options = tf.GPUOptions(allow_growth=gpu_stat)\n\n    test_real = tf.placeholder(tf.float32, [1, None, None, 3], name='test')\n    with tf.variable_scope(\"generator\", reuse=False):\n        test_generated = generator.G_net(test_real).fake\n         \n    saver = tf.train.Saver()\n\n    # load video\n    vid = cv2.VideoCapture(video)\n    vid_name = os.path.basename(video)\n    total = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n    fps = vid.get(cv2.CAP_PROP_FPS)\n    width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    codec = cv2.VideoWriter_fourcc(*output_format)\n\n    tfconfig = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n    with tf.Session(config=tfconfig) as sess:\n        # tf.global_variables_initializer().run()\n        # load model\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)  # checkpoint file information\n        if ckpt and ckpt.model_checkpoint_path:\n            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)  # first line\n            saver.restore(sess, os.path.join(checkpoint_dir, ckpt_name))\n            print(\" [*] Success to read {}\".format(os.path.join(checkpoint_dir, ckpt_name)))\n        else:\n            print(\" [*] Failed to find a checkpoint\")\n            return\n         \n        video_out = cv2.VideoWriter(os.path.join(output, vid_name.rsplit('.', 1)[0] + \"_AnimeGANv2.mp4\"), codec, fps, (width, height))\n\n        pbar = tqdm(total=total, ncols=80)\n        pbar.set_description(f\"Making: {os.path.basename(video).rsplit('.', 1)[0] + '_AnimeGANv2.mp4'}\")\n        while True:\n            ret, frame = vid.read()\n            if not ret:\n                break\n            frame = np.asarray(np.expand_dims(process_image(frame),0))\n            fake_img = sess.run(test_generated, feed_dict={test_real: frame})\n            fake_img = post_precess(fake_img, (width, height))\n            video_out.write(cv2.cvtColor(fake_img, cv2.COLOR_BGR2RGB))\n            pbar.update(1)\n\n        pbar.close()\n        vid.release()\n        video_out.release()\n        return os.path.join(output, vid_name.rsplit('.', 1)[0] + \"_AnimeGANv2.mp4\")\n\nif __name__ == '__main__':\n    arg = parse_args()\n    check_folder(arg.output)\n    info = cvt2anime_video(arg.video, arg.output, arg.checkpoint_dir)\n    print(f'output video: {info}')\n"
        }
      ]
    }
  ]
}