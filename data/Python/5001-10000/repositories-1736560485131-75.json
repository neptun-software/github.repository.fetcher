{
  "metadata": {
    "timestamp": 1736560485131,
    "page": 75,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/nougat",
      "stars": 9139,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.8955078125,
          "content": "core.*\n*.bin\n.nfs*\n.vscode/*\nresult/*\n!result/extract.py\nmisc/*\nwandb/\n!misc/*.png\n!dataset/gen_seek.py\n!result/.gitkeep\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\nckpt*/\n\n# Misc\npdfs\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.4541015625,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\nThis Code of Conduct also applies outside the project spaces when there is a\nreasonable belief that an individual's behavior may have a negative impact on\nthe project or its community.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <opensource-conduct@meta.com>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.5556640625,
          "content": "# Contributing to Nougat\n\n## Pull Requests\n\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\n## License\nBy contributing to this repo, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0625,
          "content": "MIT License\n\nCopyright (c) Meta Platforms, Inc. and affiliates.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "LICENSE-MODEL.md",
          "type": "blob",
          "size": 13.2666015625,
          "content": "# Creative Commons Attribution-NonCommercial 4.0 International Public License\n\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial 4.0 International Public License (\"Public License\"). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\n## Section 1 â€“ Definitions.\n\na. Adapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\n\nb. Adapter's License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\n\nc. Copyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not d. Copyright and Similar Rights.\n\nd. Effective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\n\ne. Exceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\n\nf. Licensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\n\ng. Licensor means the individual(s) or entity(ies) granting rights under this Public License.\n\ni. NonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\n\nj. Share means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\n\nk. Sui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\n\nl. You means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n## Section 2 â€“ Scope.\n\na. License grant.\n\t1. Subject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\t\tA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\n\t\tB. produce, reproduce, and Share Adapted Material for NonCommercial purposes only.\n\n\t2. Exceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\n\t3. Term. The term of this Public License is specified in Section 6(a).\n\t4. Media and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\n\t5. Downstream recipients.\n\t\ta. Offer from the Licensor â€“ Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\n\t\tb. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\t6. No endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nb. Other rights.\n\n1. Moral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\n\n2. Patent and trademark rights are not licensed under this Public License.\n\n3. To the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n## Section 3 â€“ License Conditions.\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\na. Attribution.\n\n1. If You Share the Licensed Material (including in modified form), You must:\n\n\tA. retain the following if it is supplied by the Licensor with the Licensed Material:\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\n\t\ti) a copyright notice;\n\t\tii) a notice that refers to this Public License;\n\t\tiii) a notice that refers to the disclaimer of warranties;\n\t\tiv) a URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\tB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\n\tC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n2. You may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\n3. If requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n4. If You Share Adapted Material You produce, the Adapter's License You apply must not prevent recipients of the Adapted Material from complying with this Public License.\n\n## Section 4 â€“ Sui Generis Database Rights.\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\n\ta. for the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;\n\tb. if You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\n\tc. You must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n## Section 5 â€“ Disclaimer of Warranties and Limitation of Liability.\n\n\ta. Unless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\n\n\tb. To the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\n\n\tc. The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n## Section 6 â€“ Term and Termination.\n\na. This Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\n\nb. Where Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\n\t1. automatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\n\t2. upon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\n\nc. For the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\n\nd. Sections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n## Section 7 â€“ Other Terms and Conditions.\n\na. The Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\n\nb. Any arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n## Section 8 â€“ Interpretation.\n\na. For the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\n\nb. To the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\n\nc. No term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\n\nd. Nothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority."
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.013671875,
          "content": "include ./*.*\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 8.72265625,
          "content": "Donut\nCopyright (c) 2022-present NAVER Corp.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\n--------------------------------------------------------------------------------------\n\nThis project contains subcomponents with separate copyright notices and license terms. \nYour use of the source code for these subcomponents is subject to the terms and conditions of the following licenses.\n\n=====\n\ngooglefonts/noto-fonts\nhttps://fonts.google.com/specimen/Noto+Sans\n\n\nCopyright 2018 The Noto Project Authors (github.com/googlei18n/noto-fonts)\n\nThis Font Software is licensed under the SIL Open Font License,\nVersion 1.1.\n\nThis license is copied below, and is also available with a FAQ at:\nhttp://scripts.sil.org/OFL\n\n-----------------------------------------------------------\nSIL OPEN FONT LICENSE Version 1.1 - 26 February 2007\n-----------------------------------------------------------\n\nPREAMBLE\nThe goals of the Open Font License (OFL) are to stimulate worldwide\ndevelopment of collaborative font projects, to support the font\ncreation efforts of academic and linguistic communities, and to\nprovide a free and open framework in which fonts may be shared and\nimproved in partnership with others.\n\nThe OFL allows the licensed fonts to be used, studied, modified and\nredistributed freely as long as they are not sold by themselves. The\nfonts, including any derivative works, can be bundled, embedded,\nredistributed and/or sold with any software provided that any reserved\nnames are not used by derivative works. The fonts and derivatives,\nhowever, cannot be released under any other type of license. The\nrequirement for fonts to remain under this license does not apply to\nany document created using the fonts or their derivatives.\n\nDEFINITIONS\n\"Font Software\" refers to the set of files released by the Copyright\nHolder(s) under this license and clearly marked as such. This may\ninclude source files, build scripts and documentation.\n\n\"Reserved Font Name\" refers to any names specified as such after the\ncopyright statement(s).\n\n\"Original Version\" refers to the collection of Font Software\ncomponents as distributed by the Copyright Holder(s).\n\n\"Modified Version\" refers to any derivative made by adding to,\ndeleting, or substituting -- in part or in whole -- any of the\ncomponents of the Original Version, by changing formats or by porting\nthe Font Software to a new environment.\n\n\"Author\" refers to any designer, engineer, programmer, technical\nwriter or other person who contributed to the Font Software.\n\nPERMISSION & CONDITIONS\nPermission is hereby granted, free of charge, to any person obtaining\na copy of the Font Software, to use, study, copy, merge, embed,\nmodify, redistribute, and sell modified and unmodified copies of the\nFont Software, subject to the following conditions:\n\n1) Neither the Font Software nor any of its individual components, in\nOriginal or Modified Versions, may be sold by itself.\n\n2) Original or Modified Versions of the Font Software may be bundled,\nredistributed and/or sold with any software, provided that each copy\ncontains the above copyright notice and this license. These can be\nincluded either as stand-alone text files, human-readable headers or\nin the appropriate machine-readable metadata fields within text or\nbinary files as long as those fields can be easily viewed by the user.\n\n3) No Modified Version of the Font Software may use the Reserved Font\nName(s) unless explicit written permission is granted by the\ncorresponding Copyright Holder. This restriction only applies to the\nprimary font name as presented to the users.\n\n4) The name(s) of the Copyright Holder(s) or the Author(s) of the Font\nSoftware shall not be used to promote, endorse or advertise any\nModified Version, except to acknowledge the contribution(s) of the\nCopyright Holder(s) and the Author(s) or with their explicit written\npermission.\n\n5) The Font Software, modified or unmodified, in part or in whole,\nmust be distributed entirely under this license, and must not be\ndistributed under any other license. The requirement for fonts to\nremain under this license does not apply to any document created using\nthe Font Software.\n\nTERMINATION\nThis license becomes null and void if any of the above conditions are\nnot met.\n\nDISCLAIMER\nTHE FONT SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT\nOF COPYRIGHT, PATENT, TRADEMARK, OR OTHER RIGHT. IN NO EVENT SHALL THE\nCOPYRIGHT HOLDER BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nINCLUDING ANY GENERAL, SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL\nDAMAGES, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF THE USE OR INABILITY TO USE THE FONT SOFTWARE OR FROM\nOTHER DEALINGS IN THE FONT SOFTWARE.\n\n=====\n\nhuggingface/transformers\nhttps://github.com/huggingface/transformers\n\n\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and limitations under the License.\n\n=====\n\nclovaai/synthtiger\nhttps://github.com/clovaai/synthtiger\n\n\nCopyright (c) 2021-present NAVER Corp.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\n=====\n\nrwightman/pytorch-image-models\nhttps://github.com/rwightman/pytorch-image-models\n\n\n   Copyright 2019 Ross Wightman\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n=====\n\nankush-me/SynthText\nhttps://github.com/ankush-me/SynthText\n\n\n   Copyright 2017, Ankush Gupta.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n=====\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.8515625,
          "content": "<div align=\"center\">\n<h1>Nougat: Neural Optical Understanding for Academic Documents</h1>\n\n[![Paper](https://img.shields.io/badge/Paper-arxiv.2308.13418-white)](https://arxiv.org/abs/2308.13418)\n[![GitHub](https://img.shields.io/github/license/facebookresearch/nougat)](https://github.com/facebookresearch/nougat)\n[![PyPI](https://img.shields.io/pypi/v/nougat-ocr?logo=pypi)](https://pypi.org/project/nougat-ocr)\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Hugging Face Spaces](https://img.shields.io/badge/ðŸ¤—%20Hugging%20Face-Community%20Space-blue)](https://huggingface.co/spaces/ysharma/nougat)\n\n</div>\n\nThis is the official repository for Nougat, the academic document PDF parser that understands LaTeX math and tables.\n\nProject page: https://facebookresearch.github.io/nougat/\n\n## Install\n\nFrom pip:\n```\npip install nougat-ocr\n```\n\nFrom repository:\n```\npip install git+https://github.com/facebookresearch/nougat\n```\n\n> Note, on Windows: If you want to utilize a GPU, make sure you first install the correct PyTorch version. Follow instructions [here](https://pytorch.org/get-started/locally/)\n\nThere are extra dependencies if you want to call the model from an API or generate a dataset.\nInstall via\n\n`pip install \"nougat-ocr[api]\"` or `pip install \"nougat-ocr[dataset]\"`\n\n### Get prediction for a PDF\n#### CLI\n\nTo get predictions for a PDF run\n\n```\n$ nougat path/to/file.pdf -o output_directory\n```\n\nA path to a directory or to a file where each line is a path to a PDF can also be passed as a positional argument\n\n```\n$ nougat path/to/directory -o output_directory\n```\n\n```\nusage: nougat [-h] [--batchsize BATCHSIZE] [--checkpoint CHECKPOINT] [--model MODEL] [--out OUT]\n              [--recompute] [--markdown] [--no-skipping] pdf [pdf ...]\n\npositional arguments:\n  pdf                   PDF(s) to process.\n\noptions:\n  -h, --help            show this help message and exit\n  --batchsize BATCHSIZE, -b BATCHSIZE\n                        Batch size to use.\n  --checkpoint CHECKPOINT, -c CHECKPOINT\n                        Path to checkpoint directory.\n  --model MODEL_TAG, -m MODEL_TAG\n                        Model tag to use.\n  --out OUT, -o OUT     Output directory.\n  --recompute           Recompute already computed PDF, discarding previous predictions.\n  --full-precision      Use float32 instead of bfloat16. Can speed up CPU conversion for some setups.\n  --no-markdown         Do not add postprocessing step for markdown compatibility.\n  --markdown            Add postprocessing step for markdown compatibility (default).\n  --no-skipping         Don't apply failure detection heuristic.\n  --pages PAGES, -p PAGES\n                        Provide page numbers like '1-4,7' for pages 1 through 4 and page 7. Only works for single PDFs.\n```\n\nThe default model tag is `0.1.0-small`. If you want to use the base model, use `0.1.0-base`.\n```\n$ nougat path/to/file.pdf -o output_directory -m 0.1.0-base\n```\n\nIn the output directory every PDF will be saved as a `.mmd` file, the lightweight markup language, mostly compatible with [Mathpix Markdown](https://github.com/Mathpix/mathpix-markdown-it) (we make use of the LaTeX tables).\n\n> Note: On some devices the failure detection heuristic is not working properly. If you experience a lot of `[MISSING_PAGE]` responses, try to run with the `--no-skipping` flag. Related: [#11](https://github.com/facebookresearch/nougat/issues/11), [#67](https://github.com/facebookresearch/nougat/issues/67)\n\n#### API\n\nWith the extra dependencies you use `app.py` to start an API. Call\n\n```sh\n$ nougat_api\n```\n\nTo get a prediction of a PDF file by making a POST request to http://127.0.0.1:8503/predict/. It also accepts parameters `start` and `stop` to limit the computation to select page numbers (boundaries are included).\n\nThe response is a string with the markdown text of the document.\n\n```sh\ncurl -X 'POST' \\\n  'http://127.0.0.1:8503/predict/' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@<PDFFILE.pdf>;type=application/pdf'\n```\nTo use the limit the conversion to pages 1 to 5, use the start/stop parameters in the request URL: http://127.0.0.1:8503/predict/?start=1&stop=5\n\n## Dataset\n### Generate dataset\n\nTo generate a dataset you need \n\n1. A directory containing the PDFs\n2. A directory containing the `.html` files (processed `.tex` files by [LaTeXML](https://math.nist.gov/~BMiller/LaTeXML/)) with the same folder structure\n3. A binary file of [pdffigures2](https://github.com/allenai/pdffigures2) and a corresponding environment variable `export PDFFIGURES_PATH=\"/path/to/binary.jar\"`\n\nNext run\n\n```\npython -m nougat.dataset.split_htmls_to_pages --html path/html/root --pdfs path/pdf/root --out path/paired/output --figure path/pdffigures/outputs\n```\n\nAdditional arguments include\n\n| Argument              | Description                                |\n| --------------------- | ------------------------------------------ |\n| `--recompute`         | recompute all splits                       |\n| `--markdown MARKDOWN` | Markdown output dir                        |\n| `--workers WORKERS`   | How many processes to use                  |\n| `--dpi DPI`           | What resolution the pages will be saved at |\n| `--timeout TIMEOUT`   | max time per paper in seconds              |\n| `--tesseract`         | Tesseract OCR prediction for each page     |\n\nFinally create a `jsonl` file that contains all the image paths, markdown text and meta information.\n\n```\npython -m nougat.dataset.create_index --dir path/paired/output --out index.jsonl\n```\n\nFor each `jsonl` file you also need to generate a seek map for faster data loading:\n\n```\npython -m nougat.dataset.gen_seek file.jsonl\n```\n\nThe resulting directory structure can look as follows:\n\n```\nroot/\nâ”œâ”€â”€ images\nâ”œâ”€â”€ train.jsonl\nâ”œâ”€â”€ train.seek.map\nâ”œâ”€â”€ test.jsonl\nâ”œâ”€â”€ test.seek.map\nâ”œâ”€â”€ validation.jsonl\nâ””â”€â”€ validation.seek.map\n```\n\nNote that the `.mmd` and `.json` files in the `path/paired/output` (here `images`) are no longer required.\nThis can be useful for pushing to a S3 bucket by halving the amount of files.\n\n## Training\n\nTo train or fine tune a Nougat model, run \n\n```\npython train.py --config config/train_nougat.yaml\n```\n\n## Evaluation\n\nRun \n\n```\npython test.py --checkpoint path/to/checkpoint --dataset path/to/test.jsonl --save_path path/to/results.json\n```\n\nTo get the results for the different text modalities, run\n\n```\npython -m nougat.metrics path/to/results.json\n```\n\n## FAQ\n\n- Why am I only getting `[MISSING_PAGE]`?\n\n  Nougat was trained on scientific papers found on arXiv and PMC. Is the document you're processing similar to that?\n  What language is the document in? Nougat works best with English papers, other Latin-based languages might work. **Chinese, Russian, Japanese etc. will not work**.\n  If these requirements are fulfilled it might be because of false positives in the failure detection, when computing on CPU or older GPUs ([#11](https://github.com/facebookresearch/nougat/issues/11)). Try passing the `--no-skipping` flag for now.\n\n- Where can I download the model checkpoint from.\n\n  They are uploaded here on GitHub in the release section. You can also download them during the first execution of the program. Choose the preferred preferred model by passing `--model 0.1.0-{base,small}`\n\n## Citation\n\n```\n@misc{blecher2023nougat,\n      title={Nougat: Neural Optical Understanding for Academic Documents}, \n      author={Lukas Blecher and Guillem Cucurull and Thomas Scialom and Robert Stojnic},\n      year={2023},\n      eprint={2308.13418},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n## Acknowledgments\n\nThis repository builds on top of the [Donut](https://github.com/clovaai/donut/) repository.\n\n## License\n\nNougat codebase is licensed under MIT.\n\nNougat model weights are licensed under CC-BY-NC.\n"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 5.091796875,
          "content": "\"\"\"\nCopyright (c) Meta Platforms, Inc. and affiliates.\n\nThis source code is licensed under the MIT license found in the\nLICENSE file in the root directory of this source tree.\n\"\"\"\nimport os\nimport sys\nfrom functools import partial\nfrom http import HTTPStatus\nfrom fastapi import FastAPI, File, UploadFile\nfrom PIL import Image\nfrom pathlib import Path\nimport hashlib\nfrom fastapi.middleware.cors import CORSMiddleware\nimport pypdfium2\nimport torch\nfrom nougat import NougatModel\nfrom nougat.postprocessing import markdown_compatible, close_envs\nfrom nougat.utils.dataset import ImageDataset\nfrom nougat.utils.checkpoint import get_checkpoint\nfrom nougat.dataset.rasterize import rasterize_paper\nfrom nougat.utils.device import move_to_device, default_batch_size\nfrom tqdm import tqdm\n\n\nSAVE_DIR = Path(\"./pdfs\")\nBATCHSIZE = int(os.environ.get(\"NOUGAT_BATCHSIZE\", default_batch_size()))\nNOUGAT_CHECKPOINT = get_checkpoint()\nif NOUGAT_CHECKPOINT is None:\n    print(\n        \"Set environment variable 'NOUGAT_CHECKPOINT' with a path to the model checkpoint!\"\n    )\n    sys.exit(1)\n\napp = FastAPI(title=\"Nougat API\")\norigins = [\"http://localhost\", \"http://127.0.0.1\"]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\nmodel = None\n\n\n@app.on_event(\"startup\")\nasync def load_model(\n    checkpoint: str = NOUGAT_CHECKPOINT,\n):\n    global model, BATCHSIZE\n    if model is None:\n        model = NougatModel.from_pretrained(checkpoint)\n        model = move_to_device(model, cuda=BATCHSIZE > 0)\n        if BATCHSIZE <= 0:\n            BATCHSIZE = 1\n        model.eval()\n\n\n@app.get(\"/\")\ndef root():\n    \"\"\"Health check.\"\"\"\n    response = {\n        \"status-code\": HTTPStatus.OK,\n        \"data\": {},\n    }\n    return response\n\n\n@app.post(\"/predict/\")\nasync def predict(\n    file: UploadFile = File(...), start: int = None, stop: int = None\n) -> str:\n    \"\"\"\n    Perform predictions on a PDF document and return the extracted text in Markdown format.\n\n    Args:\n        file (UploadFile): The uploaded PDF file to process.\n        start (int, optional): The starting page number for prediction.\n        stop (int, optional): The ending page number for prediction.\n\n    Returns:\n        str: The extracted text in Markdown format.\n    \"\"\"\n    pdfbin = file.file.read()\n    pdf = pypdfium2.PdfDocument(pdfbin)\n    md5 = hashlib.md5(pdfbin).hexdigest()\n    save_path = SAVE_DIR / md5\n\n    if start is not None and stop is not None:\n        pages = list(range(start - 1, stop))\n    else:\n        pages = list(range(len(pdf)))\n    predictions = [\"\"] * len(pages)\n    dellist = []\n    if save_path.exists():\n        for computed in (save_path / \"pages\").glob(\"*.mmd\"):\n            try:\n                idx = int(computed.stem) - 1\n                if idx in pages:\n                    i = pages.index(idx)\n                    print(\"skip page\", idx + 1)\n                    predictions[i] = computed.read_text(encoding=\"utf-8\")\n                    dellist.append(idx)\n            except Exception as e:\n                print(e)\n    compute_pages = pages.copy()\n    for el in dellist:\n        compute_pages.remove(el)\n    images = rasterize_paper(pdf, pages=compute_pages)\n    global model\n\n    dataset = ImageDataset(\n        images,\n        partial(model.encoder.prepare_input, random_padding=False),\n    )\n\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=BATCHSIZE,\n        pin_memory=True,\n        shuffle=False,\n    )\n\n    for idx, sample in tqdm(enumerate(dataloader), total=len(dataloader)):\n        if sample is None:\n            continue\n        model_output = model.inference(image_tensors=sample)\n        for j, output in enumerate(model_output[\"predictions\"]):\n            if model_output[\"repeats\"][j] is not None:\n                if model_output[\"repeats\"][j] > 0:\n                    disclaimer = \"\\n\\n+++ ==WARNING: Truncated because of repetitions==\\n%s\\n+++\\n\\n\"\n                else:\n                    disclaimer = (\n                        \"\\n\\n+++ ==ERROR: No output for this page==\\n%s\\n+++\\n\\n\"\n                    )\n                rest = close_envs(model_output[\"repetitions\"][j]).strip()\n                if len(rest) > 0:\n                    disclaimer = disclaimer % rest\n                else:\n                    disclaimer = \"\"\n            else:\n                disclaimer = \"\"\n\n            predictions[pages.index(compute_pages[idx * BATCHSIZE + j])] = (\n                markdown_compatible(output) + disclaimer\n            )\n\n    (save_path / \"pages\").mkdir(parents=True, exist_ok=True)\n    pdf.save(save_path / \"doc.pdf\")\n    if len(images) > 0:\n        thumb = Image.open(images[0])\n        thumb.thumbnail((400, 400))\n        thumb.save(save_path / \"thumb.jpg\")\n    for idx, page_num in enumerate(pages):\n        (save_path / \"pages\" / (\"%02d.mmd\" % (page_num + 1))).write_text(\n            predictions[idx], encoding=\"utf-8\"\n        )\n    final = \"\".join(predictions).strip()\n    (save_path / \"doc.mmd\").write_text(final, encoding=\"utf-8\")\n    return final\n\n\ndef main():\n    import uvicorn\n\n    uvicorn.run(\"app:app\", port=8503)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "lightning_module.py",
          "type": "blob",
          "size": 8.9072265625,
          "content": "\"\"\"\nDonut\nCopyright (c) 2022-present NAVER Corp.\nMIT License\nCopyright (c) Meta Platforms, Inc. and affiliates.\n\"\"\"\nimport math\nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport lightning.pytorch as pl\nimport torch\nfrom lightning.pytorch.utilities import rank_zero_only\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.utils.data import DataLoader\n\nfrom nougat import NougatConfig, NougatModel\nfrom nougat.metrics import get_metrics\n\n\nclass NougatModelPLModule(pl.LightningModule):\n    def __init__(self, config):\n        super().__init__()\n        self.validation_step_outputs = []\n        self.config = config\n        if self.config.get(\"model_path\", False):\n            self.model = NougatModel.from_pretrained(\n                self.config.model_path,\n                input_size=self.config.input_size,\n                max_length=self.config.max_length,\n                align_long_axis=self.config.align_long_axis,\n                window_size=self.config.window_size,\n                encoder_layer=self.config.encoder_layer,\n                decoder_layer=self.config.decoder_layer,\n                patch_size=self.config.patch_size,\n                embed_dim=self.config.embed_dim,\n                num_heads=self.config.num_heads,\n                hidden_dimension=self.config.hidden_dimension,\n                ignore_mismatched_sizes=True,\n            )\n        else:\n            self.model = NougatModel(\n                config=NougatConfig(\n                    input_size=self.config.input_size,\n                    max_length=self.config.max_length,\n                    align_long_axis=self.config.align_long_axis,\n                    window_size=self.config.window_size,\n                    encoder_layer=self.config.encoder_layer,\n                    decoder_layer=self.config.decoder_layer,\n                    tokenizer_file=self.config.tokenizer,\n                    patch_size=self.config.patch_size,\n                    embed_dim=self.config.embed_dim,\n                    num_heads=self.config.num_heads,\n                    hidden_dimension=self.config.hidden_dimension,\n                )\n            )\n\n    def training_step(self, batch, batch_idx):\n        image_tensors, decoder_input_ids, attention_masks = list(), list(), list()\n        if batch is None:\n            return\n        for batch_data in batch:\n            if batch_data is None or batch_data[0] is None:\n                continue\n            image_tensors.append(batch_data[0])\n            decoder_input_ids.append(batch_data[1])\n            attention_masks.append(batch_data[2])\n        image_tensors = torch.cat(image_tensors)\n        decoder_input_ids = torch.cat(decoder_input_ids)\n        attention_masks = torch.cat(attention_masks)\n        loss = self.model(image_tensors, decoder_input_ids, attention_masks)[0]\n        if loss is not None:\n            self.log_dict({\"train/loss\": loss}, sync_dist=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx, dataset_idx=0):\n        if batch is None:\n            return\n        image_tensors, decoder_input_ids, _ = batch\n        if image_tensors is None:\n            return\n        markdown = pad_sequence(\n            decoder_input_ids,\n            batch_first=True,\n        )\n        preds = self.model.inference(\n            image_tensors=image_tensors,\n            return_attentions=False,\n        )[\"predictions\"]\n        gts = self.model.decoder.tokenizer.batch_decode(\n            markdown, skip_special_tokens=True\n        )\n        metrics = get_metrics(gts, preds, pool=False)\n        scores = {\n            \"val/\" + key: sum(values) / len(values) for key, values in metrics.items()\n        }\n        self.validation_step_outputs.append(scores)\n        return scores\n\n    def on_validation_epoch_end(self):\n        if (\n            self.validation_step_outputs is not None\n            and len(self.validation_step_outputs) >= 1\n        ):\n            self.log_dict(self.validation_step_outputs[0], sync_dist=True)\n            self.validation_step_outputs.clear()\n\n    def configure_optimizers(self):\n        def _get_device_count():\n            if torch.cuda.is_available():\n                return torch.cuda.device_count()\n            elif torch.backends.mps.is_available():\n                # Can MPS have more than one device?\n                return 1\n            return 1\n\n        max_iter = None\n\n        if int(self.config.get(\"max_epochs\", -1)) > 0:\n            assert (\n                len(self.config.train_batch_sizes) == 1\n            ), \"Set max_epochs only if the number of datasets is 1\"\n            steps = self.config.num_training_samples_per_epoch\n            max_iter = (self.config.max_epochs * steps) / max(\n                1,\n                (\n                    self.config.train_batch_sizes[0]\n                    * _get_device_count()\n                    * self.config.get(\"num_nodes\", 1)\n                ),\n            )\n\n        if int(self.config.get(\"max_steps\", -1)) > 0:\n            max_iter = (\n                min(self.config.max_steps, max_iter)\n                if max_iter is not None\n                else self.config.max_steps\n            )\n\n        assert max_iter is not None\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.lr)\n        scheduler = {\n            \"scheduler\": self.exponential_scheduler(\n                optimizer,\n                self.config.warmup_steps,\n                self.config.lr,\n                self.config.get(\"min_lr\", 5e-5),\n                self.config.get(\"gamma\", 0.9996),\n            ),\n            \"name\": \"learning_rate\",\n            \"interval\": \"step\",\n            \"frequency\": self.config.get(\"lr_step\", 1),\n        }\n        return [optimizer], [scheduler]\n\n    @staticmethod\n    def cosine_scheduler(optimizer, training_steps, warmup_steps):\n        def lr_lambda(current_step):\n            if current_step < warmup_steps:\n                return current_step / max(1, warmup_steps)\n            progress = current_step - warmup_steps\n            progress /= max(1, training_steps - warmup_steps)\n            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n\n        return LambdaLR(optimizer, lr_lambda)\n\n    @staticmethod\n    def exponential_scheduler(optimizer, warmup_steps, lr, min_lr=5e-5, gamma=0.9999):\n        def lr_lambda(x):\n            if x > warmup_steps or warmup_steps <= 0:\n                if lr * gamma ** (x - warmup_steps) > min_lr:\n                    return gamma ** (x - warmup_steps)\n                else:\n                    return min_lr / lr\n            else:\n                return x / warmup_steps\n\n        return LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n    def get_progress_bar_dict(self):\n        items = super().get_progress_bar_dict()\n        items.pop(\"v_num\", None)\n        items[\"exp_name\"] = f\"{self.config.get('exp_name', '')}\"\n        items[\"exp_version\"] = f\"{self.config.get('exp_version', '')}\"\n        return items\n\n    @rank_zero_only\n    def on_save_checkpoint(self, checkpoint):\n        save_path = (\n            Path(self.config.result_path)\n            / self.config.exp_name\n            / self.config.exp_version\n        )\n        self.model.save_pretrained(save_path)\n        self.model.decoder.tokenizer.save_pretrained(save_path)\n\n\nclass NougatDataPLModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.train_batch_sizes = self.config.train_batch_sizes\n        self.val_batch_sizes = self.config.val_batch_sizes\n        self.train_datasets = []\n        self.val_datasets = []\n        self.g = torch.Generator()\n        self.g.manual_seed(self.config.seed)\n\n    def train_dataloader(self):\n        loaders = [\n            DataLoader(\n                torch.utils.data.ConcatDataset(self.train_datasets),\n                batch_size=self.train_batch_sizes[0],\n                num_workers=self.config.num_workers,\n                pin_memory=True,\n                worker_init_fn=self.seed_worker,\n                generator=self.g,\n                shuffle=True,\n                collate_fn=self.ignore_none_collate,\n            )\n        ]\n        return loaders\n\n    def val_dataloader(self):\n        loaders = [\n            DataLoader(\n                torch.utils.data.ConcatDataset(self.val_datasets),\n                batch_size=self.val_batch_sizes[0],\n                pin_memory=True,\n                shuffle=True,\n                collate_fn=self.ignore_none_collate,\n            )\n        ]\n        return loaders\n\n    @staticmethod\n    def seed_worker(wordker_id):\n        worker_seed = torch.initial_seed() % 2**32\n        np.random.seed(worker_seed)\n        random.seed(worker_seed)\n\n    @staticmethod\n    def ignore_none_collate(batch):\n        if batch is None:\n            return\n        try:\n            batch = [x for x in batch if x is not None and x[0] is not None]\n            if len(batch) == 0:\n                return\n            return torch.utils.data.dataloader.default_collate(batch)\n        except AttributeError:\n            pass\n"
        },
        {
          "name": "nougat",
          "type": "tree",
          "content": null
        },
        {
          "name": "predict.py",
          "type": "blob",
          "size": 7.2646484375,
          "content": "\"\"\"\nCopyright (c) Meta Platforms, Inc. and affiliates.\n\nThis source code is licensed under the MIT license found in the\nLICENSE file in the root directory of this source tree.\n\"\"\"\nimport sys\nfrom pathlib import Path\nimport logging\nimport re\nimport argparse\nimport re\nimport os\nfrom functools import partial\nimport torch\nfrom torch.utils.data import ConcatDataset\nfrom tqdm import tqdm\nfrom nougat import NougatModel\nfrom nougat.utils.dataset import LazyDataset\nfrom nougat.utils.device import move_to_device, default_batch_size\nfrom nougat.utils.checkpoint import get_checkpoint\nfrom nougat.postprocessing import markdown_compatible\nimport pypdf\n\nlogging.basicConfig(level=logging.INFO)\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--batchsize\",\n        \"-b\",\n        type=int,\n        default=default_batch_size(),\n        help=\"Batch size to use.\",\n    )\n    parser.add_argument(\n        \"--checkpoint\",\n        \"-c\",\n        type=Path,\n        default=None,\n        help=\"Path to checkpoint directory.\",\n    )\n    parser.add_argument(\n        \"--model\",\n        \"-m\",\n        type=str,\n        default=\"0.1.0-small\",\n        help=f\"Model tag to use.\",\n    )\n    parser.add_argument(\"--out\", \"-o\", type=Path, help=\"Output directory.\")\n    parser.add_argument(\n        \"--recompute\",\n        action=\"store_true\",\n        help=\"Recompute already computed PDF, discarding previous predictions.\",\n    )\n    parser.add_argument(\n        \"--full-precision\",\n        action=\"store_true\",\n        help=\"Use float32 instead of bfloat16. Can speed up CPU conversion for some setups.\",\n    )\n    parser.add_argument(\n        \"--no-markdown\",\n        dest=\"markdown\",\n        action=\"store_false\",\n        help=\"Do not add postprocessing step for markdown compatibility.\",\n    )\n    parser.add_argument(\n        \"--markdown\",\n        action=\"store_true\",\n        help=\"Add postprocessing step for markdown compatibility (default).\",\n    )\n    parser.add_argument(\n        \"--no-skipping\",\n        dest=\"skipping\",\n        action=\"store_false\",\n        help=\"Don't apply failure detection heuristic.\",\n    )\n    parser.add_argument(\n        \"--pages\",\n        \"-p\",\n        type=str,\n        help=\"Provide page numbers like '1-4,7' for pages 1 through 4 and page 7. Only works for single PDF input.\",\n    )\n    parser.add_argument(\"pdf\", nargs=\"+\", type=Path, help=\"PDF(s) to process.\")\n    args = parser.parse_args()\n    if args.checkpoint is None or not args.checkpoint.exists():\n        args.checkpoint = get_checkpoint(args.checkpoint, model_tag=args.model)\n    if args.out is None:\n        logging.warning(\"No output directory. Output will be printed to console.\")\n    else:\n        if not args.out.exists():\n            logging.info(\"Output directory does not exist. Creating output directory.\")\n            args.out.mkdir(parents=True)\n        if not args.out.is_dir():\n            logging.error(\"Output has to be directory.\")\n            sys.exit(1)\n    if len(args.pdf) == 1 and not args.pdf[0].suffix == \".pdf\":\n        # input is a list of pdfs\n        try:\n            pdfs_path = args.pdf[0]\n            if pdfs_path.is_dir():\n                args.pdf = list(pdfs_path.rglob(\"*.pdf\"))\n            else:\n                args.pdf = [\n                    Path(l) for l in open(pdfs_path).read().split(\"\\n\") if len(l) > 0\n                ]\n            logging.info(f\"Found {len(args.pdf)} files.\")\n        except:\n            pass\n    if args.pages and len(args.pdf) == 1:\n        pages = []\n        for p in args.pages.split(\",\"):\n            if \"-\" in p:\n                start, end = p.split(\"-\")\n                pages.extend(range(int(start) - 1, int(end)))\n            else:\n                pages.append(int(p) - 1)\n        args.pages = pages\n    else:\n        args.pages = None\n    return args\n\n\ndef main():\n    args = get_args()\n    model = NougatModel.from_pretrained(args.checkpoint)\n    model = move_to_device(model, bf16=not args.full_precision, cuda=args.batchsize > 0)\n    if args.batchsize <= 0:\n        # set batch size to 1. Need to check if there are benefits for CPU conversion for >1\n        args.batchsize = 1\n    model.eval()\n    datasets = []\n    for pdf in args.pdf:\n        if not pdf.exists():\n            continue\n        if args.out:\n            out_path = args.out / pdf.with_suffix(\".mmd\").name\n            if out_path.exists() and not args.recompute:\n                logging.info(\n                    f\"Skipping {pdf.name}, already computed. Run with --recompute to convert again.\"\n                )\n                continue\n        try:\n            dataset = LazyDataset(\n                pdf,\n                partial(model.encoder.prepare_input, random_padding=False),\n                args.pages,\n            )\n        except pypdf.errors.PdfStreamError:\n            logging.info(f\"Could not load file {str(pdf)}.\")\n            continue\n        datasets.append(dataset)\n    if len(datasets) == 0:\n        return\n    dataloader = torch.utils.data.DataLoader(\n        ConcatDataset(datasets),\n        batch_size=args.batchsize,\n        shuffle=False,\n        collate_fn=LazyDataset.ignore_none_collate,\n    )\n\n    predictions = []\n    file_index = 0\n    page_num = 0\n    for i, (sample, is_last_page) in enumerate(tqdm(dataloader)):\n        model_output = model.inference(\n            image_tensors=sample, early_stopping=args.skipping\n        )\n        # check if model output is faulty\n        for j, output in enumerate(model_output[\"predictions\"]):\n            if page_num == 0:\n                logging.info(\n                    \"Processing file %s with %i pages\"\n                    % (datasets[file_index].name, datasets[file_index].size)\n                )\n            page_num += 1\n            if output.strip() == \"[MISSING_PAGE_POST]\":\n                # uncaught repetitions -- most likely empty page\n                predictions.append(f\"\\n\\n[MISSING_PAGE_EMPTY:{page_num}]\\n\\n\")\n            elif args.skipping and model_output[\"repeats\"][j] is not None:\n                if model_output[\"repeats\"][j] > 0:\n                    # If we end up here, it means the output is most likely not complete and was truncated.\n                    logging.warning(f\"Skipping page {page_num} due to repetitions.\")\n                    predictions.append(f\"\\n\\n[MISSING_PAGE_FAIL:{page_num}]\\n\\n\")\n                else:\n                    # If we end up here, it means the document page is too different from the training domain.\n                    # This can happen e.g. for cover pages.\n                    predictions.append(\n                        f\"\\n\\n[MISSING_PAGE_EMPTY:{i*args.batchsize+j+1}]\\n\\n\"\n                    )\n            else:\n                if args.markdown:\n                    output = markdown_compatible(output)\n                predictions.append(output)\n            if is_last_page[j]:\n                out = \"\".join(predictions).strip()\n                out = re.sub(r\"\\n{3,}\", \"\\n\\n\", out).strip()\n                if args.out:\n                    out_path = args.out / Path(is_last_page[j]).with_suffix(\".mmd\").name\n                    out_path.parent.mkdir(parents=True, exist_ok=True)\n                    out_path.write_text(out, encoding=\"utf-8\")\n                else:\n                    print(out, \"\\n\\n\")\n                predictions = []\n                page_num = 0\n                file_index += 1\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.0380859375,
          "content": "[metadata]\ndescription_file = README.md"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.7001953125,
          "content": "\"\"\"\nDonut\nCopyright (c) 2022-present NAVER Corp.\nMIT License\nCopyright (c) Meta Platforms, Inc. and affiliates.\n\"\"\"\nimport os\nfrom setuptools import find_packages, setup\n\nROOT = os.path.abspath(os.path.dirname(__file__))\n\n\ndef read_version():\n    data = {}\n    path = os.path.join(ROOT, \"nougat\", \"_version.py\")\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        exec(f.read(), data)\n    return data[\"__version__\"]\n\n\ndef read_long_description():\n    path = os.path.join(ROOT, \"README.md\")\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        text = f.read()\n    return text\n\n\nsetup(\n    name=\"nougat-ocr\",\n    version=read_version(),\n    description=\"Nougat: Neural Optical Understanding for Academic Documents\",\n    long_description=read_long_description(),\n    long_description_content_type=\"text/markdown\",\n    author=\"Lukas Blecher\",\n    author_email=\"lblecher@meta.com\",\n    url=\"https://github.com/facebookresearch/nougat\",\n    license=\"MIT\",\n    packages=find_packages(\n        exclude=[\n            \"result\",\n        ]\n    ),\n    py_modules=[\"predict\", \"app\", \"train\", \"test\"],\n    python_requires=\">=3.7\",\n    install_requires=[\n        \"transformers>=4.25.1\",\n        \"timm==0.5.4\",\n        \"orjson\",\n        \"opencv-python-headless\",\n        \"datasets[vision]\",\n        \"lightning>=2.0.0,<2022\",\n        \"nltk\",\n        \"python-Levenshtein\",\n        \"sentencepiece\",\n        \"sconf>=0.2.3\",\n        \"albumentations>=1.0.0\",\n        \"pypdf>=3.1.0\",\n        \"pypdfium2\",\n    ],\n    extras_require={\n        \"api\": [\n            \"fastapi\",\n            \"uvicorn[standard]\",\n            \"python-multipart\",\n        ],\n        \"dataset\": [\n            \"pytesseract\",\n            \"beautifulsoup4\",\n            \"scikit-learn\",\n            \"Pebble\",\n            \"pylatexenc\",\n            \"fuzzysearch\",\n            \"unidecode\",\n            \"htmlmin\",\n            \"pdfminer.six>=20221105\",\n        ],\n    },\n    entry_points={\n        \"console_scripts\": [\n            \"nougat = predict:main\",\n            \"nougat_api = app:main\",\n        ],\n    },\n    classifiers=[\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Information Technology\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Software Development :: Libraries\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n    ],\n)\n"
        },
        {
          "name": "test.py",
          "type": "blob",
          "size": 3.720703125,
          "content": "\"\"\"\nDonut\nCopyright (c) 2022-present NAVER Corp.\nMIT License\nCopyright (c) Meta Platforms, Inc. and affiliates.\n\"\"\"\nimport argparse\nimport json\nimport os\nimport logging\nfrom multiprocessing import Pool\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nfrom nougat import NougatModel\nfrom nougat.metrics import compute_metrics\nfrom nougat.utils.checkpoint import get_checkpoint\nfrom nougat.utils.dataset import NougatDataset\nfrom nougat.utils.device import move_to_device\nfrom lightning_module import NougatDataPLModule\n\n\ndef test(args):\n    pretrained_model = NougatModel.from_pretrained(args.checkpoint)\n    pretrained_model = move_to_device(pretrained_model)\n\n    pretrained_model.eval()\n\n    if args.save_path:\n        os.makedirs(os.path.dirname(args.save_path), exist_ok=True)\n    else:\n        logging.warning(\"Results can not be saved. Please provide a -o/--save_path\")\n    predictions = []\n    ground_truths = []\n    metrics = defaultdict(list)\n    dataset = NougatDataset(\n        dataset_path=args.dataset,\n        nougat_model=pretrained_model,\n        max_length=pretrained_model.config.max_length,\n        split=args.split,\n    )\n\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        num_workers=6,\n        pin_memory=True,\n        shuffle=args.shuffle,\n        collate_fn=NougatDataPLModule.ignore_none_collate,\n    )\n\n    for idx, sample in tqdm(enumerate(dataloader), total=len(dataloader)):\n        if sample is None:\n            continue\n        image_tensors, decoder_input_ids, _ = sample\n        if image_tensors is None:\n            return\n        if len(predictions) >= args.num_samples:\n            break\n        ground_truth = pretrained_model.decoder.tokenizer.batch_decode(\n            decoder_input_ids, skip_special_tokens=True\n        )\n        outputs = pretrained_model.inference(\n            image_tensors=image_tensors,\n            return_attentions=False,\n        )[\"predictions\"]\n        predictions.extend(outputs)\n        ground_truths.extend(ground_truth)\n        with Pool(args.batch_size) as p:\n            _metrics = p.starmap(compute_metrics, iterable=zip(outputs, ground_truth))\n            for m in _metrics:\n                for key, value in m.items():\n                    metrics[key].append(value)\n\n            print({key: sum(values) / len(values) for key, values in metrics.items()})\n\n    scores = {}\n    for metric, vals in metrics.items():\n        scores[f\"{metric}_accuracies\"] = vals\n        scores[f\"{metric}_accuracy\"] = np.mean(vals)\n    try:\n        print(\n            f\"Total number of samples: {len(vals)}, Edit Distance (ED) based accuracy score: {scores['edit_dist_accuracy']}, BLEU score: {scores['bleu_accuracy']}, METEOR score: {scores['meteor_accuracy']}\"\n        )\n    except:\n        pass\n    if args.save_path:\n        scores[\"predictions\"] = predictions\n        scores[\"ground_truths\"] = ground_truths\n        with open(args.save_path, \"w\") as f:\n            json.dump(scores, f)\n\n    return predictions\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--checkpoint\", \"-c\", type=Path, default=None)\n    parser.add_argument(\"-d\", \"--dataset\", type=str, required=True)\n    parser.add_argument(\"--split\", type=str, default=\"test\")\n    parser.add_argument(\n        \"--save_path\", \"-o\", type=str, default=None, help=\"json file to save results to\"\n    )\n    parser.add_argument(\"--num_samples\", \"-N\", type=int, default=-1)\n    parser.add_argument(\"--shuffle\", action=\"store_true\")\n    parser.add_argument(\"--batch_size\", \"-b\", type=int, default=10)\n    args, left_argv = parser.parse_known_args()\n    args.checkpoint = get_checkpoint(args.checkpoint)\n\n    predictions = test(args)\n"
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 7.255859375,
          "content": "\"\"\"\nDonut\nCopyright (c) 2022-present NAVER Corp.\nMIT License\nCopyright (c) Meta Platforms, Inc. and affiliates.\n\"\"\"\nimport argparse\nimport datetime\nimport os\nfrom os.path import basename\nfrom pathlib import Path\n\nimport lightning.pytorch as pl\nimport torch\nfrom lightning.pytorch.callbacks import (\n    LearningRateMonitor,\n    ModelCheckpoint,\n    Callback,\n    GradientAccumulationScheduler,\n)\nfrom lightning.pytorch.loggers.tensorboard import TensorBoardLogger\nfrom lightning.pytorch.plugins import CheckpointIO\nfrom lightning.pytorch.plugins.environments import SLURMEnvironment\nfrom lightning.pytorch.utilities import rank_zero_only\nfrom sconf import Config\n\nfrom nougat import NougatDataset\nfrom lightning_module import NougatDataPLModule, NougatModelPLModule\n\ntry:\n    import wandb\n    from lightning.pytorch.loggers import WandbLogger as Logger\nexcept ModuleNotFoundError:\n    from lightning.pytorch.loggers.tensorboard import TensorBoardLogger as Logger\n\nimport logging\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n\nclass CustomCheckpointIO(CheckpointIO):\n    \"\"\"\n    A custom class for saving and loading checkpoints with additional functionality.\n\n    Args:\n        `CheckpointIO` (class): The base class for checkpoint I/O operations.\n\n    Methods:\n        `save_checkpoint(checkpoint, path, storage_options=None)`:\n            Save a checkpoint to the specified path.\n\n        `load_checkpoint(path, storage_options=None)`:\n            Load a checkpoint from the specified path.\n\n        `remove_checkpoint(path) -> None`:\n            Remove a checkpoint from the specified path.\n\n    \"\"\"\n\n    @rank_zero_only\n    def save_checkpoint(self, checkpoint, path, storage_options=None):\n        \"\"\"\n        Save a checkpoint to the specified path.\n\n        Args:\n            `checkpoint` (dict): The dictionary containing the checkpoint data.\n            `path` (str): The path where the checkpoint will be saved.\n            `storage_options` (dict, optional): Additional storage options.\n        \"\"\"\n        torch.save(checkpoint, path)\n\n    def load_checkpoint(self, path, storage_options=None):\n        \"\"\"\n        Load a checkpoint from the specified path.\n\n        Args:\n            `path` (str): The path from which the checkpoint will be loaded.\n            `storage_options` (dict, optional): Additional storage options.\n        \"\"\"\n        path = Path(path)\n        if path.is_file():\n            print(\"path:\", path, path.is_dir())\n            ckpt = torch.load(path)\n            if not \"state_dict\" in ckpt:\n                ckpt[\"state_dict\"] = {\n                    \"model.\" + key: value\n                    for key, value in torch.load(\n                        path.parent / \"pytorch_model.bin\"\n                    ).items()\n                }\n            return ckpt\n        else:\n            checkpoint = torch.load(path / \"artifacts.ckpt\")\n            state_dict = torch.load(path / \"pytorch_model.bin\")\n            checkpoint[\"state_dict\"] = {\n                \"model.\" + key: value for key, value in state_dict.items()\n            }\n            return checkpoint\n\n    def remove_checkpoint(self, path) -> None:\n        return super().remove_checkpoint(path)\n\n\nclass GradNormCallback(Callback):\n    \"\"\"\n    Logs the gradient norm.\n    \"\"\"\n\n    @staticmethod\n    def gradient_norm(model):\n        total_norm = 0.0\n        for p in model.parameters():\n            if p.grad is not None:\n                param_norm = p.grad.detach().data.norm(2)\n                total_norm += param_norm.item() ** 2\n        total_norm = total_norm**0.5\n        return total_norm\n\n    def on_after_backward(self, trainer, model):\n        model.log(\"train/grad_norm\", self.gradient_norm(model))\n\n\n@rank_zero_only\ndef save_config_file(config, path):\n    if not Path(path).exists():\n        os.makedirs(path)\n    save_path = Path(path) / \"config.yaml\"\n    print(config.dumps())\n    with open(save_path, \"w\") as f:\n        f.write(config.dumps(modified_color=None, quote_str=True))\n        print(f\"Config is saved at {save_path}\")\n\n\ndef train(config):\n    \"\"\"\n    Train a Nougat model using the provided configuration.\n\n    Args:\n        `config` (dict): A dictionary containing configuration settings for training.\n    \"\"\"\n    pl.seed_everything(config.get(\"seed\", 42), workers=True)\n\n    model_module = NougatModelPLModule(config)\n    data_module = NougatDataPLModule(config)\n\n    # add datasets to data_module\n    datasets = {\"train\": [], \"validation\": []}\n    for i, dataset_path in enumerate(config.dataset_paths):\n        for split in [\"train\", \"validation\"]:\n            datasets[split].append(\n                NougatDataset(\n                    dataset_path=dataset_path,\n                    nougat_model=model_module.model,\n                    max_length=config.max_length,\n                    split=split,\n                )\n            )\n    data_module.train_datasets = datasets[\"train\"]\n    data_module.val_datasets = datasets[\"validation\"]\n\n    lr_callback = LearningRateMonitor(logging_interval=\"step\")\n\n    checkpoint_callback = ModelCheckpoint(\n        save_last=True,\n        dirpath=Path(config.result_path) / config.exp_name / config.exp_version,\n    )\n    grad_norm_callback = GradNormCallback()\n    custom_ckpt = CustomCheckpointIO()\n\n    if not config.debug:\n        logger = Logger(config.exp_name, project=\"Nougat\", config=dict(config))\n    else:\n        logger = TensorBoardLogger(\n            save_dir=config.result_path,\n            name=config.exp_name,\n            version=config.exp_version,\n            default_hp_metric=False,\n        )\n    trainer = pl.Trainer(\n        num_nodes=config.get(\"num_nodes\", 1),\n        devices=\"auto\",\n        strategy=\"ddp_find_unused_parameters_true\",\n        accelerator=\"auto\",\n        # plugins=[SLURMEnvironment(auto_requeue=False)],\n        max_epochs=config.max_epochs,\n        max_steps=config.max_steps,\n        val_check_interval=config.val_check_interval,\n        check_val_every_n_epoch=config.check_val_every_n_epoch,\n        limit_val_batches=config.val_batches,\n        gradient_clip_val=config.gradient_clip_val,\n        log_every_n_steps=15,\n        precision=\"bf16-mixed\",\n        num_sanity_val_steps=0,\n        logger=logger,\n        callbacks=[\n            lr_callback,\n            grad_norm_callback,\n            checkpoint_callback,\n            GradientAccumulationScheduler({0: config.accumulate_grad_batches}),\n        ],\n    )\n\n    trainer.fit(\n        model_module,\n        data_module,\n        ckpt_path=config.get(\"resume_from_checkpoint_path\", None),\n    )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, required=True)\n    parser.add_argument(\"--exp_version\", type=str, required=False)\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    parser.add_argument(\"--job\", type=int, default=None)\n    args, left_argv = parser.parse_known_args()\n\n    config = Config(args.config)\n    config.argv_update(left_argv)\n    config.debug = args.debug\n    config.job = args.job\n    if not config.get(\"exp_name\", False):\n        config.exp_name = basename(args.config).split(\".\")[0]\n    config.exp_version = (\n        datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        if not args.exp_version\n        else args.exp_version\n    )\n\n    save_config_file(\n        config, Path(config.result_path) / config.exp_name / config.exp_version\n    )\n    train(config)\n"
        }
      ]
    }
  ]
}