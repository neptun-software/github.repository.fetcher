{
  "metadata": {
    "timestamp": 1736560712081,
    "page": 377,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "EleutherAI/gpt-neox",
      "stars": 7032,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 4.4033203125,
          "content": "---\n# Refer to the following link for the explanation of each params:\n#   http://releases.llvm.org/8.0.0/tools/clang/docs/ClangFormatStyleOptions.html\nLanguage: Cpp\n# BasedOnStyle: Google\nAccessModifierOffset: -4\nAlignAfterOpenBracket: Align\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nAlignEscapedNewlines: Left\nAlignOperands: true\nAlignTrailingComments: true\nAllowAllParametersOfDeclarationOnNextLine: false\nAllowShortBlocksOnASingleLine: true\nAllowShortCaseLabelsOnASingleLine: true\nAllowShortFunctionsOnASingleLine: All\nAllowShortIfStatementsOnASingleLine: true\nAllowShortLoopsOnASingleLine: true\n# This is deprecated\nAlwaysBreakAfterDefinitionReturnType: None\nAlwaysBreakAfterReturnType: None\nAlwaysBreakBeforeMultilineStrings: true\nAlwaysBreakTemplateDeclarations: true\nBinPackArguments:  false\nBinPackParameters: false\nBraceWrapping:\n  AfterClass:            false\n  AfterControlStatement: false\n  AfterEnum:             false\n  AfterFunction:         false\n  AfterNamespace:        false\n  AfterObjCDeclaration:  false\n  AfterStruct:           false\n  AfterUnion:            false\n  AfterExternBlock:      false\n  BeforeCatch:           false\n  BeforeElse:            false\n  IndentBraces:          false\n  # disabling the below splits, else, they'll just add to the vertical length of source files!\n  SplitEmptyFunction: false\n  SplitEmptyRecord: false\n  SplitEmptyNamespace: false\nBreakBeforeBinaryOperators: None\nBreakBeforeBraces: WebKit\nBreakBeforeInheritanceComma: false\nBreakInheritanceList: BeforeColon\nBreakBeforeTernaryOperators: true\nBreakConstructorInitializersBeforeComma: false\nBreakConstructorInitializers: BeforeColon\nBreakAfterJavaFieldAnnotations: false\nBreakStringLiterals: true\nColumnLimit: 100\nCommentPragmas: '^ IWYU pragma:'\nCompactNamespaces: false\nConstructorInitializerAllOnOneLineOrOnePerLine: true\n# Kept the below 2 to be the same as `IndentWidth` to keep everything uniform\nConstructorInitializerIndentWidth: 4\nContinuationIndentWidth: 4\nCpp11BracedListStyle: true\nDerivePointerAlignment: false\nDisableFormat: false\nExperimentalAutoDetectBinPacking: false\nFixNamespaceComments: true\nForEachMacros:\n  - foreach\n  - Q_FOREACH\n  - BOOST_FOREACH\nIncludeBlocks: Preserve\nIncludeCategories:\n  - Regex:           '^<ext/.*\\.h>'\n    Priority:        2\n  - Regex:           '^<.*\\.h>'\n    Priority:        1\n  - Regex:           '^<.*'\n    Priority:        2\n  - Regex:           '.*'\n    Priority:        3\nIncludeIsMainRegex: '([-_](test|unittest))?$'\nIndentCaseLabels: true\nIndentPPDirectives: None\nIndentWidth:     4\nIndentWrappedFunctionNames: false\nJavaScriptQuotes: Leave\nJavaScriptWrapImports: true\nKeepEmptyLinesAtTheStartOfBlocks: false\nMacroBlockBegin: ''\nMacroBlockEnd:   ''\nMaxEmptyLinesToKeep: 1\nNamespaceIndentation: None\nObjCBinPackProtocolList: Never\nObjCBlockIndentWidth: 4\nObjCSpaceAfterProperty: false\nObjCSpaceBeforeProtocolList: true\nPenaltyBreakAssignment: 4\nPenaltyBreakBeforeFirstCallParameter: 1\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakString: 1000\nPenaltyBreakTemplateDeclaration: 10\nPenaltyExcessCharacter: 1000000\nPenaltyReturnTypeOnItsOwnLine: 200\nPointerAlignment: Left\nRawStringFormats:\n  - Language: Cpp\n    Delimiters:\n      - cc\n      - CC\n      - cpp\n      - Cpp\n      - CPP\n      - 'c++'\n      - 'C++'\n    CanonicalDelimiter: ''\n  - Language: TextProto\n    Delimiters:\n      - pb\n      - PB\n      - proto\n      - PROTO\n    EnclosingFunctions:\n      - EqualsProto\n      - EquivToProto\n      - PARSE_PARTIAL_TEXT_PROTO\n      - PARSE_TEST_PROTO\n      - PARSE_TEXT_PROTO\n      - ParseTextOrDie\n      - ParseTextProtoOrDie\n    CanonicalDelimiter: ''\n    BasedOnStyle: google\n# Enabling comment reflow causes doxygen comments to be messed up in their formats!\nReflowComments: true\nSortIncludes: true\nSortUsingDeclarations: true\nSpaceAfterCStyleCast: false\nSpaceAfterTemplateKeyword: true\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeCpp11BracedList: false\nSpaceBeforeCtorInitializerColon: true\nSpaceBeforeInheritanceColon: true\nSpaceBeforeParens: ControlStatements\nSpaceBeforeRangeBasedForLoopColon: true\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 2\nSpacesInAngles: false\nSpacesInContainerLiterals: true\nSpacesInCStyleCastParentheses: false\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nStandard: Cpp11\nStatementMacros:\n  - Q_UNUSED\n  - QT_REQUIRE_VERSION\n# Be consistent with indent-width, even for people who use tab for indentation!\nTabWidth: 4\nUseTab: Never\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.0166015625,
          "content": "20B_checkpoints/\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.048828125,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# wandb logs\nwandb/\n\n# data files\ndata/**/*.idx\ndata/**/*.bin\ndata/**/*.json*\ndata/**/*.txt\ndata/**/*.gz\ndata/**/*.zip\ndata/**/*.np*\ndata/**/*.npy\ncheckpoints/\n.vscode/\n*.pt\n*.ckpt\n\n#test logs\ntest_checkpoint/\ntest_logs/\nlogs/\ntensorboard/\nsrc/\n\n# test data files\ntests/data/*.bin\ntests/data/*.idx\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.3798828125,
          "content": "repos:\n    - repo: https://github.com/pre-commit/pre-commit-hooks\n      rev: v4.1.0\n      hooks:\n          - id: check-case-conflict\n          - id: check-json\n          - id: check-symlinks\n          - id: check-yaml\n          - id: destroyed-symlinks\n          - id: end-of-file-fixer\n            exclude: ^(docs/CNAME/|configs/neox_arguments.md)\n          - id: fix-byte-order-marker\n          - id: fix-encoding-pragma\n            args: [--remove]\n          - id: mixed-line-ending\n            args: [--fix=lf]\n          - id: requirements-txt-fixer\n          - id: trailing-whitespace\n            exclude: ^(docs/CNAME/|configs/neox_arguments.md)\n    - repo: https://gitlab.com/daverona/pre-commit/cpp\n      rev: 0.8.0\n      hooks:\n          - id: clang-format  # formatter of C/C++ code based on a style guide: LLVM, Google, Chromium, Mozilla, and WebKit available\n            args: []\n\n    - repo: https://github.com/psf/black\n      rev: 22.3.0\n      hooks:\n          - id: black\n            language_version: python3\n    - repo: https://github.com/codespell-project/codespell\n      rev: v2.1.0\n      hooks:\n      - id: codespell\n        args: [\n              '--ignore-words-list=reord,dout,te',  # Word used in error messages that need rewording. te --> transformerengine\n              --check-filenames,\n              --check-hidden,\n          ]\n        exclude: tests/data/hf_cache/tokenizer/gpt2.json\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 2.0166015625,
          "content": "# YAML 1.2\n---\nauthors:\n  - affiliation: EleutherAI\n    family-names: Andonian\n    given-names: Alex\n  - affiliation: EleutherAI\n    family-names: Anthony\n    given-names: Quentin\n  - affiliation: EleutherAI\n    family-names: Biderman\n    given-names: Stella\n  - affiliation: EleutherAI\n    family-names: Black\n    given-names: Sid\n  - affiliation: EleutherAI\n    family-names: Gali\n    given-names: Preetham\n  - affiliation: EleutherAI\n    family-names: Gao\n    given-names: Leo\n  - affiliation: EleutherAI\n    family-names: Hallahan\n    given-names: Eric\n  - affiliation: EleutherAI\n    family-names: Levy-Kramer\n    given-names: Josh\n  - affiliation: EleutherAI\n    family-names: Leahy\n    given-names: Connor\n  - affiliation: EleutherAI\n    family-names: Nestler\n    given-names: Lucas\n  - affiliation: EleutherAI\n    family-names: Parker\n    given-names: Kip\n  - affiliation: EleutherAI\n    family-names: Pieler\n    given-names: Michael\n  - affiliation: EleutherAI\n    family-names: Phang\n    given-names: Jason\n  - affiliation: EleutherAI\n    family-names: Purohit\n    given-names: Shivanshu\n  - affiliation: EleutherAI\n    family-names: Schoelkopf\n    given-names: Hailey\n  - affiliation: EleutherAI\n    family-names: Stander\n    given-names: Dashiell\n  - affiliation: EleutherAI\n    family-names: Songz\n    given-names: Tri\n  - affiliation: EleutherAI\n    family-names: Tigges\n    given-names: Curt\n  - affiliation: EleutherAI\n    family-names: Thérien\n    given-names: Benjamin\n  - affiliation: EleutherAI\n    family-names: Wang\n    given-names: Phil\n  - affiliation: EleutherAI\n    family-names: Weinbach\n    given-names: Samuel\ncff-version: \"1.1.0\"\nkeywords:\n  - \"Transformers\"\n  - \"Massive language model\"\n  - \"Autoregressive language model\"\nlicense: \"Apache-2.0\"\nmessage: \"If you use this software, please cite it using these metadata.\"\nrepository-code: \"https://www.github.com/eleutherai/gpt-neox\"\ntitle: \"GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch\"\nversion: \"2.0.0\"\ndoi: \"10.5281/zenodo.5879544\"\ndate-released: 2021-08-23\n...\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 4.6201171875,
          "content": "# Contributing\nGPT-NeoX welcomes your contributions!\n\n## Prerequisites\nGPT-NeoX uses [pre-commit](https://pre-commit.com/) to ensure that formatting is\nconsistent across GPT-NeoX. First, ensure that `pre-commit` is installed with\n`pip install pre-commit`. Next, the pre-commit hooks must be installed once\nbefore commits can be made:\n```bash\npre-commit install\n```\nPlease install `clang-format` from Conda:\n```bash\nconda install clang-format\n```\n\nAfterwards, our suite of formatting tests run automatically before each `git commit`. You\ncan also run these manually:\n```bash\npre-commit run --all-files\n```\nIf a formatting test fails, it will fix the modified code in place and abort\nthe `git commit`. After looking over the changes, you can `git add <modified files>`\nand then repeat the previous `git commit` command.\n\n\n## Testing\nGPT-NeoX tracks two types of tests: unit tests and more costly model convergence tests.\nUnit tests are found in `tests/unit/` and the model convergence tests are found in\n`tests/model/`.\n\n### Unit Tests\n[PyTest](https://docs.pytest.org/en/latest/) is used to execute tests. PyTest can be\ninstalled from PyPI via `pip install pytest`. Simply invoke `pytest --forked` to run the\nunit tests:\n```bash\npytest --forked tests/unit/\n```\nYou can also provide the `-v` flag to `pytest` to see additional information about the\ntests. Note that [pytest-forked](https://github.com/pytest-dev/pytest-forked) and the\n`--forked` flag are required to test CUDA functionality in distributed tests.\n\n### Model Tests\nTo execute model tests, first install GPT-NeoX. Next, execute the model test driver:\n```bash\ncd tests/model/\npytest run_sanity_check.py\n```\nNote that the `--forked` flag is not necessary for the model tests.\n\n## Contributor License Agreement\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla-assistant.io/EleutherAI/gpt-neox.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\n\n## New Feature Contribution Guidelines\nUnlike bug fix or improving existing feature (where users usually directly submit a PR and we review it), adding a new feature to GPT-NeoX requires several steps: (1) proposal and discussion, (2) implementation and verification, (3) release and maintenance. This general guideline applies to all new feature contributions. Core GPT-NeoX team member contributions may complete step 1 internally.\n\n### Step 1: Proposal and Discussion\nWe ask users to first post your intended feature in an issue. This issue needs to include:\n\n* A description of the proposed feature.\n* A motivation of why it will be useful to GPT-NeoX users.\n* A rough design of how you implement the feature inside GPT-NeoX.\n* (Important) Results or planned experiments to demonstrate the effectiveness and correctness of the feature.\n  * If the feature only affects performance and does not affect training convergence, we require testing on a fraction of training to demonstrate that the training/validation loss are consistent with baseline, and that the performance is better than baseline.\n  * If the feature does affect training convergence, we require testing the whole training to demonstrate that the feature achieves better/on-par final model quality and training performance compared to baseline.\n\nBased on the issue we shall discuss the merit of the new feature and decide whether to accept or decline the proposal. Once accepted and after we confirm the design and implementation plan, we are ready for step 2.\n\n### Step 2: Implementation and Verification\nThe contributor will proceed and implement the feature, and the GPT-NeoX team will provide guidance/helps as needed. The required deliverables include:\n\n* A PR to [EleutherAI/GPT-NeoX](https://github.com/EleutherAI/gpt-neox) including (1) the feature implementation (2) unit tests (3) documentation (4) example usage.\n* In the implementation (code, documentation, tutorial), we require the feature author to record their GitHub username as a contact method for future questions/maintenance.\n\nAfter receiving the PRs, we will review them and merge them after necessary tests/fixes.\n\n### Step 3: Release and Maintenance\nAfter the PRs are merged, we will announce the feature on our website (with credit to the feature author). We ask the feature author to commit to the maintenance of the feature.\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 3.7626953125,
          "content": "# Copyright (c) 2024, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nFROM nvcr.io/nvidia/pytorch:24.02-py3\n\nENV DEBIAN_FRONTEND=noninteractive\n\n# metainformation\nLABEL org.opencontainers.image.version = \"2.0\"\nLABEL org.opencontainers.image.authors = \"contact@eleuther.ai\"\nLABEL org.opencontainers.image.source = \"https://www.github.com/eleutherai/gpt-neox\"\nLABEL org.opencontainers.image.licenses = \" Apache-2.0\"\nLABEL org.opencontainers.image.base.name=\"nvcr.io/nvidia/pytorch:24.02-py3\"\n\n#### System package (uses default Python 3 version in Ubuntu 20.04)\nRUN apt-get update -y && \\\n    apt-get install -y \\\n    python3-pip sudo pdsh \\\n    htop tmux zstd software-properties-common \\\n    nfs-common pdsh cmake htop iftop iotop ssh \\\n    iputils-ping net-tools libcupti-dev libmlx4-1 infiniband-diags ibutils \\\n    rdmacm-utils perftest rdma-core && \\\n    update-alternatives --install /usr/bin/python python /usr/bin/python3 1 && \\\n    update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1 && \\\n    python -m pip install --upgrade pip && \\\n    python -m pip install gpustat\n\n### SSH\nRUN mkdir /var/run/sshd && \\\n    # Prevent user being kicked off after login\n    sed -i 's@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g' /etc/pam.d/sshd && \\\n    echo 'AuthorizedKeysFile     .ssh/authorized_keys' >> /etc/ssh/sshd_config && \\\n    echo 'PasswordAuthentication yes' >> /etc/ssh/sshd_config && \\\n    # FIX SUDO BUG: https://github.com/sudo-project/sudo/issues/42\n    echo \"Set disable_coredump false\" >> /etc/sudo.conf\n\n# Expose SSH port\nEXPOSE 22\n\n# Needs to be in docker PATH if compiling other items & bashrc PATH (later)\nENV PATH=/usr/local/mpi/bin:${PATH} \\\n    LD_LIBRARY_PATH=/usr/local/lib:/usr/local/mpi/lib:/usr/local/mpi/lib64:${LD_LIBRARY_PATH}\n\n# Create a wrapper for OpenMPI to allow running as root by default\nRUN mv /usr/local/mpi/bin/mpirun /usr/local/mpi/bin/mpirun.real && \\\n    echo '#!/bin/bash' > /usr/local/mpi/bin/mpirun && \\\n    echo 'mpirun.real --allow-run-as-root --prefix /usr/local/mpi \"$@\"' >> /usr/local/mpi/bin/mpirun && \\\n    chmod a+x /usr/local/mpi/bin/mpirun\n\n#### User account\nRUN useradd --create-home --uid 1000 --shell /bin/bash mchorse && \\\n    usermod -aG sudo mchorse && \\\n    echo \"mchorse ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers\n\n## SSH config and bashrc\nRUN mkdir -p /home/mchorse/.ssh /job && \\\n    echo 'Host *' > /home/mchorse/.ssh/config && \\\n    echo '    StrictHostKeyChecking no' >> /home/mchorse/.ssh/config && \\\n    echo 'export PDSH_RCMD_TYPE=ssh' >> /home/mchorse/.bashrc && \\\n    echo 'export PATH=/home/mchorse/.local/bin:$PATH' >> /home/mchorse/.bashrc && \\\n    echo 'export PATH=/usr/local/mpi/bin:$PATH' >> /home/mchorse/.bashrc && \\\n    echo 'export LD_LIBRARY_PATH=/usr/local/lib:/usr/local/mpi/lib:/usr/local/mpi/lib64:$LD_LIBRARY_PATH' >> /home/mchorse/.bashrc\n\n#### Python packages\nCOPY requirements/* ./\nRUN python -m pip install --no-cache-dir -r requirements.txt && pip install -r requirements-onebitadam.txt\nRUN python -m pip install -r requirements-wandb.txt\nRUN python -m pip install protobuf==3.20.*\n\nCOPY megatron/fused_kernels/ /megatron/fused_kernels\nWORKDIR /megatron/fused_kernels\nRUN python setup.py install\n\n# Clear staging\nRUN mkdir -p /tmp && chmod 0777 /tmp\n\n#### SWITCH TO mchorse USER\nUSER mchorse\nWORKDIR /home/mchorse\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 25.1806640625,
          "content": "                                 Apache License\n                           Version 2.0, January 2024\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n--\n\nThis repository also contains code from Hugging Face Inc., Google Research,\nand Facebook (from their Fairseq project). Files from these\norganizations have notices at the top of each file. Below are licenses\nused in those files, as indicated.\n\n\n------------- LICENSE FOR NVIDIA code  --------------\n\n\n# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#  * Neither the name of NVIDIA CORPORATION nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n------------- LICENSE FOR huggingface and Google Research code  --------------\n\n\n                                 Apache License\n                           Version 2.0, January 2024\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n------------- LICENSE FOR Facebook Fairseq code --------------\n\nMIT License\n\nCopyright (c) Facebook, Inc. and its affiliates.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0634765625,
          "content": "include megatron/data/Makefile\ninclude megatron/data/helpers.cpp\n"
        },
        {
          "name": "README-MUP.md",
          "type": "blob",
          "size": 1.5341796875,
          "content": "# How to use Mup (https://github.com/microsoft/mup)\n\n## Add mup neox args to your config\n\n```\n# mup\n\n\"use-mup\": true,\n\n\"save-base-shapes\": false, # this only needs to be enabled once in order to generate the base-shapes-file on each rank\n\n\"base-shapes-file\": \"base-shapes\", # load base shapes from this file\n\n\"coord-check\": false, # generate coord check plots to verify mup's implementation in neox\n\n# mup hp search\n\n\"mup-init-scale\": 1.0,\n\n\"mup-attn-temp\": 1.0,\n\n\"mup-output-temp\": 1.0,\n\n\"mup-embedding-mult\": 1.0,\n\n\"mup-rp-embedding-mult\": 1.0,\n```\n\n## Generate base shapes\n\n1. Set use-mup to true\n2. Set save-base-shapes to true\n3. Run once. gpt-neox will instantiate a base model and a delta model, then save one file per rank named <base-shapes-file>.<rank>. gpt-neox will exit immediately.\n4. Set save-base-shapes to false\n\n## Generate coord check plots (optional)\n\n1. Keep use-mup true\n2. Set coord-check to true\n3. Run once. gpt-neox will output jpg images similar to https://github.com/microsoft/mutransformers/blob/main/README.md#coord-check. gpt-neox will exit immediately\n4. Set coord-check to false\n\n## Tune mup hyperparameters and LR\n\nThe values under `mup hp search` were added and correspond to appendix F.4 from https://arxiv.org/pdf/2203.03466.pdf. These and LR are tuned with a random search using the scaled-up config (tested with 6-7B.yml) but with hidden-size set to the value from the scaled-down config (125M.yml).\n\n## Transfer\n\nWith the best LR set and the best mup HPs set, revert the value of hidden-size in the scaled-up config and run again.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 57.423828125,
          "content": "[![GitHub issues](https://img.shields.io/github/issues/EleutherAI/gpt-neox)](https://github.com/EleutherAI/gpt-neox/issues)\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Weights & Biases monitoring\" height=20>](https://wandb.ai/eleutherai/neox)\n\n# GPT-NeoX\n\nThis repository records [EleutherAI](https://www.eleuther.ai)'s library for training large-scale language models on GPUs. Our current framework is based on NVIDIA's [Megatron Language Model](https://github.com/NVIDIA/Megatron-LM) and has been augmented with techniques from [DeepSpeed](https://www.deepspeed.ai) as well as some novel optimizations. We aim to make this repo a centralized and accessible place to gather techniques for training large-scale autoregressive language models, and accelerate research into large-scale training. This library is in widespread use in [academic, industry, and government labs](https://github.com/EleutherAI/gpt-neox#adoption-and-publications), including by researchers at Oak Ridge National Lab, CarperAI, Stability AI, Together.ai, Korea University, Carnegie Mellon University, and the University of Tokyo among others. Uniquely among similar libraries GPT-NeoX supports a wide variety of systems and hardwares, including launching via Slurm, MPI, and the IBM Job Step Manager, and has been run at scale on [AWS](https://aws.amazon.com/), [CoreWeave](https://www.coreweave.com/), [ORNL Summit](https://www.olcf.ornl.gov/summit/), [ORNL Frontier](https://www.olcf.ornl.gov/frontier/),  [LUMI](https://www.lumi-supercomputer.eu/), and others.\n\n**If you are not looking to train models with billions of parameters from scratch, this is likely the wrong library to use. For generic inference needs, we recommend you use the Hugging Face `transformers` library instead which supports GPT-NeoX models.**\n\n## Why GPT-NeoX?\n\nGPT-NeoX leverages many of the same features and technologies as the popular Megatron-DeepSpeed library but with substantially increased usability and novel optimizations. Major features include:\n* Distributed training with ZeRO and 3D parallelism\n* A wide variety of systems and hardwares, including launching via Slurm, MPI, and the IBM Job Step Manager, and has been run at scale on [AWS](https://aws.amazon.com/), [CoreWeave](https://www.coreweave.com/), Oak Ridge's [Summit](https://www.olcf.ornl.gov/summit/) and [Frontier](https://www.olcf.ornl.gov/frontier/),  [Pacific Northwest National Laboratory](https://hpc.pnl.gov/index.shtml), Argonne's [Polaris](https://docs.alcf.anl.gov/polaris/data-science-workflows/applications/gpt-neox/), [LUMI](https://www.lumi-supercomputer.eu/), and more.\n* Cutting edge architectural innovations including rotary and alibi positional embeddings, parallel feedforward attention layers, and flash attention.\n* Predefined configurations for popular architectures including Pythia, PaLM, Falcon, and LLaMA 1 \\& 2\n* Curriculum Learning\n* Easy connections with the open source ecosystem, including Hugging Face's [tokenizers](https://github.com/huggingface/tokenizers) and [transformers](https://github.com/huggingface/transformers/) libraries, monitor experiments via [WandB](https://wandb.ai/site)/[Comet](https://www.comet.com/site/)/TensorBoard, and evaluation via our [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness).\n\n## News\n**[10/9/2024]** We now support [Transformer Engine](https://github.com/NVIDIA/TransformerEngine) integration\n\n**[9/9/2024]** We now support preference learning via [DPO](https://arxiv.org/abs/2305.18290), [KTO](https://arxiv.org/abs/2402.01306), and reward modeling\n\n**[9/9/2024]** We now support integration with [Comet ML](https://www.comet.com/site/), a machine learning monitoring platform\n\n**[5/21/2024]** We now support [RWKV](https://www.rwkv.com/) with pipeline parallelism!. See the PRs for [RWKV](https://github.com/EleutherAI/gpt-neox/pull/1198) and [RWKV+pipeline](https://github.com/EleutherAI/gpt-neox/pull/1221)\n\n**[3/21/2024]** We now support Mixture-of-Experts (MoE)\n\n**[3/17/2024]** We now support AMD MI250X GPUs\n\n**[3/15/2024]** We now support [Mamba](https://github.com/state-spaces/mamba) with tensor parallelism! See [the PR](https://github.com/EleutherAI/gpt-neox/pull/1184)\n\n**[8/10/2023]** We now support checkpointing with AWS S3! Activate with the `s3_path` config option (for more detail, see [the PR](https://github.com/EleutherAI/gpt-neox/pull/1010))\n\n**[9/20/2023]** As of https://github.com/EleutherAI/gpt-neox/pull/1035, we have deprecated Flash Attention 0.x and 1.x, and migrated support to Flash Attention 2.x. We don't believe this will cause problems, but if you have a specific use-case that requires old flash support using the latest GPT-NeoX, please raise an issue.\n\n**[8/10/2023]** We have experimental support for LLaMA 2 and Flash Attention v2 supported in our [math-lm](https://github.com/EleutherAI/math-lm) project that will be upstreamed later this month.\n\n**[5/17/2023]** After fixing some miscellaneous bugs we now fully support bf16.\n\n**[4/11/2023]** We have upgraded our Flash Attention implementation to now support Alibi positional embeddings.\n\n**[3/9/2023]** We have released GPT-NeoX 2.0.0, an upgraded version built on the latest DeepSpeed which will be regularly synced with going forward.\n\n## Versions\n\nPrior to 3/9/2023, GPT-NeoX relied on [DeeperSpeed](https://github.com/EleutherAI/DeeperSpeed), which was based on an old version of DeepSpeed (0.3.15). In order to migrate to the latest upstream DeepSpeed version while allowing users to access the old versions of GPT-NeoX and DeeperSpeed, we have introduced two versioned releases for both libraries:\n\n- Version 2.0 of [GPT-NeoX](https://github.com/EleutherAI/gpt-neox/releases/tag/v2.0) and [DeeperSpeed](https://github.com/EleutherAI/DeeperSpeed/releases/tag/v2.0) are the latest versions built on the latest DeepSpeed, and will be maintained going forward.\n- Version 1.0 of [GPT-NeoX](https://github.com/EleutherAI/gpt-neox/releases/tag/v1.0) and [DeeperSpeed](https://github.com/EleutherAI/DeeperSpeed/releases/tag/v1.0) maintain snapshots of the old stable versions that [GPT-NeoX-20B](https://arxiv.org/abs/2204.06745) and the [Pythia Suite](https://github.com/EleutherAI/pythia) were trained on.\n\n# Contents\n\n- [GPT-NeoX](#gpt-neox)\n  * [Why GPT-NeoX?](#why-gpt-neox)\n  * [News](#news)\n  * [Versions](#versions)\n- [Contents](#contents)\n- [Quick Start](#quick-start)\n  * [Environment and Dependencies](#environment-and-dependencies)\n    + [Host Setup](#host-setup)\n    + [Flash Attention](#flash-attention)\n    + [Transformer Engine](#transformer-engine)\n    + [Multi-Node Launching](#multi-node-launching)\n    + [Containerized Setup](#containerized-setup)\n  * [Usage](#usage)\n- [Configuration](#configuration)\n    * [Mixture of Experts](#mixture-of-experts)\n- [Datasets](#datasets)\n  * [Preconfigured Datasets](#preconfigured-datasets)\n  * [Using Custom Data](#using-custom-data)\n- [Training and Finetuning](#training-and-finetuning)\n  * [Pretrained Models](#pretrained-models)\n    + [GPT-NeoX-20B](#gpt-neox-20b)\n    + [Pythia](#pythia)\n    + [Polyglot](#polyglot)\n- [Inference](#inference)\n- [Evaluation](#evaluation)\n- [Exporting to Hugging Face](#exporting-to-hugging-face)\n- [Monitoring](#monitoring)\n  * [Weights and Biases](#weights-and-biases)\n  * [TensorBoard](#tensorboard)\n- [Running on multi-node](#running-on-multi-node)\n- [Profiling](#profiling)\n- [Adoption and Publications](#adoption-and-publications)\n  * [Publications](#publications)\n  * [Models](#models)\n    + [English LLMs](#english-llms)\n    + [Non-English LLMs](#non-english-llms)\n    + [Code Models](#code-models)\n    + [Other Modalities](#other-modalities)\n- [Administrative Notes](#administrative-notes)\n  * [Citing GPT-NeoX](#citing-gpt-neox)\n  * [Contributing](#contributing)\n  * [Licensing](#licensing)\n  * [Acknowledgements](#acknowledgements)\n\n# Quick Start\n\n## Environment and Dependencies\n\n### Host Setup\n\nThis codebase has primarily developed and tested for Python 3.8-3.10, and PyTorch 1.8-2.0. This is not a strict requirement, and other versions and combinations of libraries may work.\n\nTo install the remaining basic dependencies, run:\n\n```bash\npip install -r requirements/requirements.txt\npip install -r requirements/requirements-wandb.txt # optional, if logging using WandB\npip install -r requirements/requirements-tensorboard.txt # optional, if logging via tensorboard\npip install -r requirements/requirements-comet.txt # optional, if logging via Comet\n```\n\nfrom the repository root.\n\n> [!Warning]\n> Our codebase relies on [DeeperSpeed](https://github.com/EleutherAI/DeeperSpeed), our fork of the [DeepSpeed](https://github.com/microsoft/DeepSpeed) library with some added changes. We strongly recommend using Anaconda, a virtual machine, or some other form of environment isolation before continuing. Failure to do so may cause other repositories that rely on DeepSpeed to break.\n\n</aside>\n\n### Fused Kernels\nWe now support AMD GPUs (MI100, MI250X) through JIT fused-kernel compilation. Fused kernels will be built and loaded as needed. To avoid waiting during job launching, you can also do the following for manual pre-build:\n\n```python\npython\nfrom megatron.fused_kernels import load\nload()\n```\nThis will automatically adapts building process over different GPU vendors (AMD, NVIDIA) without platform specific code changes. To further test fused kernels using `pytest`, use `pytest tests/model/test_fused_kernels.py`\n\n### Flash Attention\n\nTo use [Flash-Attention](https://github.com/HazyResearch/flash-attention), install the additional dependencies in  `./requirements/requirements-flashattention.txt` or use a PyTorch NGC container with it pre-installed (note that functionality is not guaranteed using versions different from our requirements file). Then set the attention type in your configuration accordingly (see [configs](./configs/)). This can provide significant speed-ups over regular attention on certain GPU architectures, including Ampere GPUs (such as A100s); see the repository for more details.\n\n### Transformer Engine\n\nTo use [Transformer Engine (TE)](https://github.com/NVIDIA/TransformerEngine), install the additional dependencies in  `./requirements/requirements-transformer-engine.txt` or use a PyTorch NGC container with it pre-installed (note that functionality is not guaranteed using versions different from our requirements file). See [this config](https://github.com/EleutherAI/gpt-neox/configs/1-3B-transformer-engine.yml) for an example of using TE on a 1.3B model. This can provide significant speed-ups over regular attention on certain GPU architectures, including Ampere and Hopper GPUs; see the repository for more details.\n\n\nTE provides very efficient kernels for both A100 and H100 GPUs. We've run some sample ablations on A100:\n\n\n\nand H100:\n\n\n\n\n### Multi-Node Launching\n\nNeoX and Deep(er)Speed support training on multiple different nodes and you have the option of using a variety of different launchers to orchestrate multi-node jobs.\n\nIn general there needs to be a \"hostfile\" somewhere accessible with the format:\n\n```bash\nnode1_ip slots=8\nnode2_ip slots=8\n```\n\nwhere the first column contains the IP address for each node in your setup and the number of slots is the number of GPUs that node has access to. In your config you must pass in the path to the hostfile with `\"hostfile\": \"/path/to/hostfile\"`. Alternatively the path to the hostfile can be in the environment variable `DLTS_HOSTFILE`.\n\n#### pdsh\n\n`pdsh` is the default launcher, and if you're using `pdsh` then all you must do (besides ensuring that pdsh is installed in your environment) is set `{\"launcher\": \"pdsh\"}` in your config files.\n\n#### MPI\n\nIf using MPI then you must specify the MPI library (DeepSpeed/GPT-NeoX currently supports `mvapich`, `openmpi`, `mpich`, and `impi`, though `openmpi` is the most commonly used and tested) as well as pass the `deepspeed_mpi` flag in your config file:\n\n```json\n{\n    \"launcher\": \"openmpi\",\n    \"deepspeed_mpi\": true\n}\n```\n\nWith your environment properly set up and the correct configuration files you can use `deepy.py` like a normal python script and start (for example) a training job with:\n\n`python3 deepy.py train.py /path/to/configs/my_model.yml`\n\n#### Slurm\n\nUsing Slurm can be slightly more involved. Like with MPI, you must add the following to your config:\n\n```json\n{\n    \"launcher\": \"slurm\",\n    \"deepspeed_slurm\": true\n}\n```\nIf you do not have ssh access to the compute nodes in your Slurm cluster you need to add `{\"no_ssh_check\": true}`\n\n#### (Advanced) Custom Launching\n\nThere are many cases where the above default launching options are not sufficient\n\n- Many clusters have their own unique job scheduler or specific MPI/Slurm arguments necessary for launching jobs such as [Summit JSRun](https://docs.olcf.ornl.gov/systems/summit_user_guide.html#job-launcher-jsrun) or [LLNL Flux](https://computing.llnl.gov/projects/flux-building-framework-resource-management)\n- While the above Slurm/MPI/pdsh default options are enough for most job runs, advanced users may want to add arguments for optimization or debugging purposes\n\nIn these cases, you will need to modify the DeepSpeed [multinode runner](https://github.com/microsoft/DeepSpeed/blob/17957728c0362bf8ae70feca308e491e55ef9feb/deepspeed/launcher/multinode_runner.py) utility to support your usecase. Broadly, these enhancements fall under two categories:\n\n##### 1. Adding a Launcher (e.g. [JSRun](https://docs.olcf.ornl.gov/systems/summit_user_guide.html#job-launcher-jsrun), [Flux](https://computing.llnl.gov/projects/flux-building-framework-resource-management), etc)\n\nIn this case, you must add a new multinode runner class to `deepspeed/launcher/multinode_runner.py` and expose it as a configuration option in GPT-NeoX. Examples on how we did this for [Summit JSRun](https://docs.olcf.ornl.gov/systems/summit_user_guide.html#job-launcher-jsrun) are in [this DeeperSpeed commit](https://github.com/EleutherAI/DeeperSpeed/commit/9aed6c8500d7c492d85c5c88687322dbda70e370) and [this GPT-NeoX commit](https://github.com/EleutherAI/gpt-neox/commit/3782c7ae60f8624e566e3879b89bb09e8b59b869), respectively.\n\n##### 2. Modifying Run Command or Environment Variables\n\nWe have encountered many cases where we wish to modify the MPI/Slurm run command for an optimization or to debug (e.g. to modify the [Slurm srun CPU binding](https://slurm.schedmd.com/srun.html#OPT_cpu-bind) or to tag MPI logs with the rank). In this case, you must modify the multinode runner class' run command under its `get_cmd` method (e.g. [mpirun_cmd](https://github.com/microsoft/DeepSpeed/blob/17957728c0362bf8ae70feca308e491e55ef9feb/deepspeed/launcher/multinode_runner.py#L135-L147) for OpenMPI). Examples on how we did this to provide optimized and rank-tagged run commands using Slurm and OpenMPI for the Stability cluster are in [this DeeperSpeed branch](https://github.com/microsoft/DeepSpeed/compare/master...EleutherAI:DeeperSpeed:v2.0-stability)\n\n\n#### Hostfile Generation\n\nIn general you will not be able to have a single fixed hostfile, so you need to have a script to generate one dynamically when your job starts. An example script to dynamically generate a hostfile using [Slurm](https://slurm.schedmd.com/documentation.html) and 8 GPUs per node is:\n\n```bash\n#!/bin/bash\nGPUS_PER_NODE=8\nmkdir -p /sample/path/to/hostfiles\n# need to add the current slurm jobid to hostfile name so that we don't add to previous hostfile\nhostfile=/sample/path/to/hostfiles/hosts_$SLURM_JOBID\n# be extra sure we aren't appending to a previous hostfile\nrm $hostfile &> /dev/null\n# loop over the node names\nfor i in `scontrol show hostnames $SLURM_NODELIST`\ndo\n    # add a line to the hostfile\n    echo $i slots=$GPUS_PER_NODE >>$hostfile\ndone\n```\n\n`$SLURM_JOBID` and `$SLURM_NODELIST` being environment variables Slurm will create for you. See the [sbatch documentation](https://slurm.schedmd.com/sbatch.html#SECTION_OUTPUT-ENVIRONMENT-VARIABLES) for a full list of available Slurm environment variables set at job creation time.\n\n#### Job Launching\n\nThen you can create an [sbatch](https://slurm.schedmd.com/sbatch.html) script from which to kick off your GPT-NeoX job. A bare-bones sbatch script on a Slurm-based cluster with 8 GPUs per node would look like this:\n\n```bash\n#!/bin/bash\n#SBATCH --job-name=\"neox\"\n#SBATCH --partition=your-partition\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=8\n#SBATCH --gres=gpu:8\n\n# Some potentially useful distributed environment variables\nexport HOSTNAMES=`scontrol show hostnames \"$SLURM_JOB_NODELIST\"`\nexport MASTER_ADDR=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\nexport MASTER_PORT=12802\nexport COUNT_NODE=`scontrol show hostnames \"$SLURM_JOB_NODELIST\" | wc -l`\n\n# Your hostfile creation script from above\n./write_hostfile.sh\n# Tell DeepSpeed where to find our generated hostfile via DLTS_HOSTFILE\nexport DLTS_HOSTFILE=/sample/path/to/hostfiles/hosts_$SLURM_JOBID\n\n# Launch training\npython3 deepy.py train.py /sample/path/to/your/configs/my_model.yml\n\n```\n\nYou can then kick off a training run with `sbatch my_sbatch_script.sh`\n\n\n### Containerized Setup\n\nWe also provide a Dockerfile and docker-compose configuration if you prefer to run NeoX in a container.\n\nRequirements to run the container are to have appropriate GPU drivers, an up-to-date installation of Docker, and [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) installed. To test if your installation is good you can use their \"sample workload\", which is:\n\n```\ndocker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi\n```\n\nProvided that will run, you need to export NEOX_DATA_PATH and NEOX_CHECKPOINT_PATH in your environment to specify your data directory and directory for storing and loading checkpoints:\n\n```\nexport NEOX_DATA_PATH=/mnt/sda/data/enwiki8 #or wherever your data is stored on your system\nexport NEOX_CHECKPOINT_PATH=/mnt/sda/checkpoints\n```\n\nAnd then, from the gpt-neox directory, you can build the image and run a shell in a container with\n\n```\ndocker compose run gpt-neox bash\n```\n\nAfter the build, you should be able to do this:\n```\nmchorse@537851ed67de:~$ echo $(pwd)\n/home/mchorse\nmchorse@537851ed67de:~$ ls -al\ntotal 48\ndrwxr-xr-x  1 mchorse mchorse 4096 Jan  8 05:33 .\ndrwxr-xr-x  1 root    root    4096 Jan  8 04:09 ..\n-rw-r--r--  1 mchorse mchorse  220 Feb 25  2020 .bash_logout\n-rw-r--r--  1 mchorse mchorse 3972 Jan  8 04:09 .bashrc\ndrwxr-xr-x  4 mchorse mchorse 4096 Jan  8 05:35 .cache\ndrwx------  3 mchorse mchorse 4096 Jan  8 05:33 .nv\n-rw-r--r--  1 mchorse mchorse  807 Feb 25  2020 .profile\ndrwxr-xr-x  2 root    root    4096 Jan  8 04:09 .ssh\ndrwxrwxr-x  8 mchorse mchorse 4096 Jan  8 05:35 chk\ndrwxrwxrwx  6 root    root    4096 Jan  7 17:02 data\ndrwxr-xr-x 11 mchorse mchorse 4096 Jan  8 03:52 gpt-neox\n```\n\nFor a long-running job, you should run\n\n```\ndocker compose up -d\n```\n\nto run the container in detached mode, and then, in a separate terminal session, run\n\n```\ndocker compose exec gpt-neox bash\n```\n\nYou can then run any job you want from inside the container.\n\nConcerns when running for a long time or in detached mode include\n - You will have to terminate the container manually when you are no longer using it\n - If you want processes to continue running when your shell session ends, you will need to background them.\n - If you then want logging, you will have to make sure to pipe logs to disk, and set up wandb and/or Comet logging.\n\nIf you prefer to run the prebuilt container image from dockerhub, you can run the docker compose commands with ```-f docker-compose-dockerhub.yml``` instead, e.g.,\n\n```\ndocker compose run -f docker-compose-dockerhub.yml gpt-neox bash\n```\n\n## Usage\n\nAll functionality should be launched using `deepy.py`, a wrapper around the `deepspeed` launcher.\n\nWe currently offer three main functions:\n1. `train.py` is used for training and finetuning models.\n2. `eval.py` is used to evaluate a trained model using the [language model evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness).\n3. `generate.py` is used to sample text from a trained model.\n\nwhich can be launched with:\n\n```bash\n./deepy.py [script.py] [./path/to/config_1.yml] [./path/to/config_2.yml] ... [./path/to/config_n.yml]\n```\n\nFor example, to launch training you can run\n```bash\n./deepy.py train.py ./configs/20B.yml ./configs/local_cluster.yml\n```\n\nFor more details on each entry point, see the [Training and Finetuning](#training-and-finetuning), [Inference](#inference) and [Evaluation](#evaluation) respectively.\n\n# Configuration\n\nGPT-NeoX parameters are defined in a YAML configuration file which is passed to the deepy.py launcher. We have provided some example .yml files in [configs](./configs/), showing a diverse array of features and model sizes.\n\nThese files are generally complete, but non-optimal. For example, depending on your specific GPU configuration, you may need to change some settings such as `pipe-parallel-size`, `model-parallel-size` to increase or decrease the degree of parallelisation, `train_micro_batch_size_per_gpu` or `gradient-accumulation-steps` to modify batch size related settings, or the `zero_optimization` dict to modify how optimizer states are parallelised across workers.\n\nFor a more detailed guide to the features available and how to configure them, see [the configuration README](configs/README.md), and for documentation of every possible argument, see [configs/neox_arguments.md](configs/neox_arguments.md).\n\n## Mixture of Experts\n\nGPT-NeoX includes multiple expert implementations for MoE. To select between them, specify `moe_type` of `megablocks` (default) or `deepspeed`.\n\nBoth are based on the DeepSpeed MoE parallelism framework, which supports tensor-expert-data parallelism.\nBoth allow you to toggle between token-dropping and dropless (default, and this is what Megablocks was designed for).\nSinkhorn routing to come soon!\n\nFor an example of a basic complete configuration, see configs/125M-dmoe.yml (for Megablocks dropless) or configs/125M-moe.yml.\n\nMost MoE related configuration arguments are prefixed with `moe`. Some common configuration parameters and their defaults are as follows:\n\n```\nmoe_type: megablocks\nmoe_num_experts: 1 # 1 disables MoE. 8 is a reasonable value.\nmoe_loss_coeff: 0.1\nexpert_interval: 2 # See details below\nenable_expert_tensor_parallelism: false # See details below\nmoe_expert_parallel_size: 1 # See details below\nmoe_token_dropping: false\n```\n\nDeepSpeed can be further configured with the following:\n\n```\nmoe_top_k: 1\nmoe_min_capacity: 4\nmoe_train_capacity_factor: 1.0 # Setting to 1.0\nmoe_eval_capacity_factor: 1.0 # Setting to 1.0\n```\n\nOne MoE layer is present every `expert_interval` transformer layers including the first, so with 12 layers total:\n\n```\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11\n```\n\nExperts would be in these layers:\n\n```\n0, 2, 4, 6, 8, 10\n```\n\nBy default, we use expert-data parallelism, so any available tensor parallelism (`model_parallel_size`) will be used for expert routing. For instance, given the following:\n\n```\nexpert_parallel_size: 4\nmodel_parallel_size: 2 # aka tensor parallelism\n```\n\nWith 32 GPUs, the behavior will be look like:\n\n- In non-expert layers:\n  - Tensor parallelism is 2. (There are 32 / 2 = 16 such tensor parallel groups, each of size 2.)\n  - Data parallelism implicitly becomes 32 / 2 = 16.\n- In expert layers:\n  - There is no tensor parallelism.\n  - Expert parallelism is 4. (There are 32 / 4 = 8 expert parallel groups, each of size 4.)\n  - Data parallelism implicitly becomes 32 / 4 = 8.  Some cross-node token routing happens as a result of this redivision of data parallelism between 16 and 8.  To avoid it, ensure that `expert_parallel_size == model_parallel_size`.\n\nSetting `enable_expert_tensor_parallelism` enables tensor-expert-data (TED) parallelism. The way to interpret the above would then be:\n\n- In non-expert layers: same as before.\n- In expert layers:\n  - Tensor parallelism is 2. (There are 32 / 2 = 16 tensor parallel groups, each of size 2.)\n  - Expert parallelism is 4. (There are 32 / 4 = 8 expert parallel groups, each of size 4.)\n  - Data parallelism implicitly becomes 32 / (2 * 4) = 4.  Again, cross-node token routing happens.  To avoid, ensure `expert_parallel_size == 1` or `model_parallel_size == 1`.\n\nSo note that DP must be divisible by (MP * EP).  For more details, see the [TED paper].\n\nPipeline parallelism is not yet supported - coming soon!\n\n[TED paper]: https://arxiv.org/abs/2303.06318\n\n# Datasets\n\n## Preconfigured Datasets\n\nSeveral preconfigured datasets are available, including most components from [the Pile](https://arxiv.org/abs/2101.00027), as well as the Pile train set itself, for straightforward tokenization using the `prepare_data.py` entry point.\n\nE.G, to download and tokenize the enwik8 dataset with the GPT2 Tokenizer, saving them to `./data` you can run:\n\n```\npython prepare_data.py -d ./data\n```\n\nor a single shard of the pile (`pile_subset`) with the GPT-NeoX-20B tokenizer (assuming you have it saved at `./20B_checkpoints/20B_tokenizer.json`):\n\n```\npython prepare_data.py -d ./data -t HFTokenizer --vocab-file ./20B_checkpoints/20B_tokenizer.json pile_subset\n```\n\nThe tokenized data will be saved out to two files: `[data-dir]/[dataset-name]/[dataset-name]_text_document.bin`and `[data-dir]/[dataset-name]/[dataset-name]_text_document.idx`. You will need to add the prefix that both these files share to your training configuration file under the `data-path` field. E.G:\n\n```yaml\n  \"data-path\": \"./data/enwik8/enwik8_text_document\",\n```\n\n## Using Custom Data\n\nTo prepare your own dataset for training with custom data, format it as one large [jsonl](https://jsonlines.org/)-formatted file with each item in the list of dictionaries being a separate document. The document text should be grouped under one JSON key, i.e `\"text\"`. Any auxiliary data stored in other fields will not be used.\n\nNext make sure to download the GPT2 tokenizer vocab, and merge files from the following links:\n\n- Vocab: https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n- Merge: https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n\nOr use the 20B tokenizer (for which only a single Vocab file is needed):\n\n- Vocab: https://the-eye.eu/public/AI/models/GPT-NeoX-20B/slim_weights/20B_tokenizer.json\n\n(alternatively, you can provide any tokenizer file that can be loaded by Hugging Face's tokenizers library with the `Tokenizer.from_pretrained()` command)\n\nYou can now pretokenize your data using `tools/datasets/preprocess_data.py`, the arguments for which are detailed below:\n\n```\nusage: preprocess_data.py [-h] --input INPUT [--jsonl-keys JSONL_KEYS [JSONL_KEYS ...]] [--num-docs NUM_DOCS] --tokenizer-type {HFGPT2Tokenizer,HFTokenizer,GPT2BPETokenizer,CharLevelTokenizer} [--vocab-file VOCAB_FILE] [--merge-file MERGE_FILE] [--append-eod] [--ftfy] --output-prefix OUTPUT_PREFIX\n                          [--dataset-impl {lazy,cached,mmap}] [--workers WORKERS] [--log-interval LOG_INTERVAL]\n\noptional arguments:\n  -h, --help            show this help message and exit\n\ninput data:\n  --input INPUT         Path to input jsonl files or lmd archive(s) - if using multiple archives, put them in a comma separated list\n  --jsonl-keys JSONL_KEYS [JSONL_KEYS ...]\n                        space separate listed of keys to extract from jsonl. Default: text\n  --num-docs NUM_DOCS   Optional: Number of documents in the input data (if known) for an accurate progress bar.\n\ntokenizer:\n  --tokenizer-type {HFGPT2Tokenizer,HFTokenizer,GPT2BPETokenizer,CharLevelTokenizer}\n                        What type of tokenizer to use.\n  --vocab-file VOCAB_FILE\n                        Path to the vocab file\n  --merge-file MERGE_FILE\n                        Path to the BPE merge file (if necessary).\n  --append-eod          Append an <eod> token to the end of a document.\n  --ftfy                Use ftfy to clean text\n\noutput data:\n  --output-prefix OUTPUT_PREFIX\n                        Path to binary output file without suffix\n  --dataset-impl {lazy,cached,mmap}\n                        Dataset implementation to use. Default: mmap\n\nruntime:\n  --workers WORKERS     Number of worker processes to launch\n  --log-interval LOG_INTERVAL\n                        Interval between progress updates\n\n```\n\nFor example:\n\n```bash\npython tools/datasets/preprocess_data.py \\\n            --input ./data/mydataset.jsonl.zst \\\n            --output-prefix ./data/mydataset \\\n            --vocab ./data/gpt2-vocab.json \\\n            --merge-file gpt2-merges.txt \\\n            --dataset-impl mmap \\\n            --tokenizer-type GPT2BPETokenizer \\\n            --append-eod\n```\n\nYou would then run training with the following settings added to your configuration file:\n\n```yaml\n  \"data-path\": \"data/mydataset_text_document\",\n```\n\n# Training and Finetuning\n\nTraining is launched using `deepy.py`, a wrapper around DeepSpeed's launcher, which launches the same script in parallel across many GPUs / nodes.\n\nThe general usage pattern is:\n\n```bash\npython ./deepy.py train.py [path/to/config1.yml] [path/to/config2.yml] ...\n```\n\nYou can pass in an arbitrary number of configs which will all be merged at runtime.\n\nYou can also optionally pass in a config prefix, which will assume all your configs are in the same folder and append that prefix to their path.\n\nFor example:\n\n```bash\npython ./deepy.py train.py -d configs 125M.yml local_setup.yml\n```\n\nThis will deploy the `train.py` script on all nodes with one process per GPU. The worker nodes and number of GPUs are specified in the `/job/hostfile` file (see [parameter documentation](configs/README.md)), or can simply be passed in as the `num_gpus` arg if running on a single node setup.\n\nAlthough this is not strictly necessary, we find it useful to define the model parameters in one config file (e.g `configs/125M.yml`) and the data path parameters in another (e.g `configs/local_setup.yml`).\n\n\n## Pretrained Models\n\n### GPT-NeoX-20B\n\nGPT-NeoX-20B is a 20 billion parameter autoregressive language model trained on [the Pile](https://arxiv.org/abs/2101.00027). Technical details about GPT-NeoX-20B can be found in [the associated paper](https://arxiv.org/abs/2204.06745). The configuration file for this model is both available at [`./configs/20B.yml`](./configs/20B.yml) and included in the download links below.\n\n[Slim weights](https://the-eye.eu/public/AI/models/GPT-NeoX-20B/slim_weights/) - (No optimizer states, for inference or finetuning, 39GB)\n\nTo download from the command line to a folder named `20B_checkpoints`, use the following command:\n\n```bash\nwget --cut-dirs=5 -nH -r --no-parent --reject \"index.html*\" https://the-eye.eu/public/AI/models/GPT-NeoX-20B/slim_weights/ -P 20B_checkpoints\n```\n\n[Full weights](https://the-eye.eu/public/AI/models/GPT-NeoX-20B/full_weights/) - (Including optimizer states, 268GB)\n\nTo download from the command line to a folder named `20B_checkpoints`, use the following command:\n\n```bash\nwget --cut-dirs=5 -nH -r --no-parent --reject \"index.html*\" https://the-eye.eu/public/AI/models/GPT-NeoX-20B/full_weights/ -P 20B_checkpoints\n```\n\nWeights can be alternatively be downloaded using a BitTorrent client. Torrent files can be downloaded here: [slim weights](https://the-eye.eu/public/AI/models/GPT-NeoX-20B/slim_weights.torrent), [full weights](https://the-eye.eu/public/AI/models/GPT-NeoX-20B/full_weights.torrent).\n\nWe additionally have 150 checkpoints saved throughout training, one every 1,000 steps. We are working on figuring out how to best serve these at scale, but in the meanwhile people interested in working with the partially trained checkpoints can email us at contact@eleuther.ai to arrange access.\n\n### Pythia\n\nThe Pythia Scaling Suite is a suite of models ranging from 70M parameters to 12B parameters trained on [the Pile](https://pile.eleuther.ai) intended to promote research on interpretability and training dynamics of large language models. Further details about the project and links to the models can be found in the [in the paper](https://arxiv.org/abs/2304.01373) and [on the project's GitHub](https://github.com/EleutherAI/pythia).\n\n### Polyglot\n\nThe Polyglot Project is an effort to train powerful non-English pretrained language models to promote the accessibility of this technology to researchers outside the dominant powerhouses of machine learning. EleutherAI has trained and released 1.3B, 3.8B, and 5.8B parameter Korean language models, the largest of which outpreforms all other publicly available language models on Korean language tasks. Further details about the project and links to the models can be found [here](https://github.com/EleutherAI/polyglot).\n\n# Inference\n\n**For most uses we recommend deploying models trained using the GPT-NeoX library via the Hugging Face Transformers library which is better optimized for inference.**\n\nWe support three types of generation from a pretrained model:\n1. Unconditional generation\n2. Conditional generation based on an input read from a file\n3. Interactive generation, which allows for multiple rounds of back-and-forth between a user and the language model via a command line interface\n\nAll three types of text generation can be launched via `python ./deepy.py generate.py -d configs 125M.yml local_setup.yml text_generation.yml` with the appropriate values set in `configs/text_generation.yml`.\n\n# Evaluation\n\nGPT-NeoX supports evaluation on downstream tasks through the [language model evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness).\n\nTo evaluate a trained model on the evaluation harness, simply run:\n\n```bash\npython ./deepy.py eval.py -d configs your_configs.yml --eval_tasks task1 task2 ... taskn\n```\n\nwhere `--eval_tasks` is a list of evaluation tasks followed by spaces, e.g `--eval_tasks lambada hellaswag piqa sciq`. For details of all tasks available, refer to the [lm-evaluation-harness repo](https://github.com/EleutherAI/lm-evaluation-harness).\n\n# Exporting to Hugging Face\n\nGPT-NeoX is optimized heavily for training only, and GPT-NeoX model checkpoints are not compatible out of the box with other deep learning libraries. To make models easily loadable and shareable with end users, and for further exporting to various other frameworks, GPT-NeoX supports checkpoint conversion to the [Hugging Face Transformers](https://arxiv.org/abs/1910.03771) format.\n\nThough NeoX supports a number of different architectural configurations, including AliBi positional embeddings, not all of these configurations map cleanly onto the supported configurations within Hugging Face Transformers.\n\nNeoX supports export of compatible models into the following architectures:\n- GPTNeoXForCausalLM\n- LlamaForCausalLM\n- MistralForCausalLM\n\nTraining a model which does not fit into one of these Hugging Face Transformers architectures cleanly will require writing custom modeling code for the exported model.\n\nTo convert a GPT-NeoX library checkpoint to Hugging Face-loadable format, run:\n```bash\npython ./tools/ckpts/convert_neox_to_hf.py --input_dir /path/to/model/global_stepXXX --config_file your_config.yml --output_dir hf_model/save/location --precision {auto,fp16,bf16,fp32} --architecture {neox,mistral,llama}\n```\n\nThen to upload a model to [the Hugging Face Hub](https://huggingface.co/), run:\n```bash\nhuggingface-cli login\npython ./tools/ckpts/upload.py\n```\nand input the requested information, including HF hub user token.\n\n### Importing Models Into GPT-NeoX\n\nNeoX supplies several utilities for converting a pretrained model checkpoint into a format that can be trained within the library.\n\nThe following models or model families can be loaded in GPT-NeoX:\n- Llama 1\n- Llama 2\n- CodeLlama\n- Mistral-7b-v0.1\n\nWe provide two utilities for converting from two different checkpoint formats into a format compatible with GPT-NeoX.\n\nTo convert a Llama 1 or Llama 2 checkpoint distributed by Meta AI from its original file format (downloadable [here](https://github.com/facebookresearch/llama) or [here](https://huggingface.co/meta-llama/Llama-2-7b)) into the GPT-NeoX library, run\n\n```\npython tools/ckpts/convert_raw_llama_weights_to_neox.py --input_dir /path/to/model/parent/dir/7B --model_size 7B --output_dir /path/to/save/ckpt --num_output_shards <TENSOR_PARALLEL_SIZE> (--pipeline_parallel if pipeline-parallel-size >= 1)\n```\n\n\nTo convert from a Hugging Face model into a NeoX-loadable, run `tools/ckpts/convert_hf_to_sequential.py`. See documentation within that file for further options.\n\n\n# Monitoring\n\nIn addition to storing logs locally, we provide built-in support for two popular experiment monitoring frameworks: [Weights & Biases](https://wandb.ai/site), [TensorBoard](https://www.tensorflow.org/tensorboard/), and [Comet](https://www.comet.com/site)\n\n## Weights and Biases\n\n[Weights & Biases to record our experiments](https://wandb.ai/eleutherai/neox) is a machine learning monitoring platform. To use wandb to monitor your gpt-neox experiments:\n1. Create an account at https://wandb.ai/site to generate your API key\n2. Log into Weights & Biases on your machine&mdash;you can do this by executing `wandb login`&mdash;your runs will automatically be recorded.\n3. Dependencies required for wandb monitoring can be found in and installed from `./requirements/requirements-wandb.txt`. An example config is provided in `./configs/local_setup_wandb.yml`.\n4. There are two optional fields associated with Weights & Biases: <code><var>wandb_group</var></code> allows you to name the run group and <code><var>wandb_team</var></code> allows you to assign your runs to an organization or team account. An example config is provided in `./configs/local_setup_wandb.yml`.\n\n## TensorBoard\n\nWe support using TensorBoard via the <code><var>tensorboard-dir</var></code> field. Dependencies required for TensorBoard monitoring can be found in and installed from  `./requirements/requirements-tensorboard.txt`.\n\n## Comet\n\n[Comet](https://www.comet.com/site) is a machine learning monitoring platform. To use comet to monitor your gpt-neox experiments:\n1. Create an account at https://www.comet.com/login to generate your API key.\n2. Once generated, link your API key at runtime by running `comet login` or passing `export COMET_API_KEY=<your-key-here>`\n3. Install `comet_ml` and any dependency libraries via `pip install -r requirements/requirements-comet.txt`\n4. Enable Comet with `use_comet: True`. You can also customize where data is being logged with `comet_workspace` and `comet_project`. A full example config with comet enabled is provided in `configs/local_setup_comet.yml`.\n5. Run your experiment, and monitor metrics in the Comet workspace that you passed!\n\n# Running on multi-node\n\nIf you need to supply a hostfile for use with the MPI-based DeepSpeed launcher, you can set the environment variable `DLTS_HOSTFILE` to point to the hostfile.\n\n# Profiling\n\nWe support profiling with Nsight Systems, the PyTorch Profiler, and PyTorch Memory Profiling.\n\n## Nsight Systems Profiling\n\nTo use the Nsight Systems profiling, set config options `profile`, `profile_step_start`, and `profile_step_stop` (see [here](https://github.com/EleutherAI/gpt-neox/blob/main/configs/neox_arguments.md) for argument usage, and [here](https://github.com/EleutherAI/gpt-neox/blob/main/configs/prof.yml) for a sample config).\n\nTo populate nsys metrics, launch training with:\n\n```\nnsys profile -s none -t nvtx,cuda -o <path/to/profiling/output> --force-overwrite true \\\n--capture-range=cudaProfilerApi --capture-range-end=stop python $TRAIN_PATH/deepy.py \\\n$TRAIN_PATH/train.py --conf_dir configs <config files>\n```\n\nThe generated output file can then by viewed with the Nsight Systems GUI:\n\n![nsight-prof](images/nsight_profiling.png)\n\n## PyTorch Profiling\n\nTo use the built-in PyTorch profiler, set config options `profile`, `profile_step_start`, and `profile_step_stop` (see [here](https://github.com/EleutherAI/gpt-neox/blob/main/configs/neox_arguments.md) for argument usage, and [here](https://github.com/EleutherAI/gpt-neox/blob/main/configs/prof.yml) for a sample config).\n\nThe PyTorch profiler will save traces to your `tensorboard` log directory.  You can view these traces within\nTensorBoard by following the steps [here](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html).\n\n![torch-prof](images/pytorch_profiling.png)\n\n## PyTorch Memory Profiling\n\nTo use PyTorch Memory Profiling, set config options `memory_profiling` and `memory_profiling_path` (see [here](https://github.com/EleutherAI/gpt-neox/blob/main/configs/neox_arguments.md) for argument usage, and [here](https://github.com/EleutherAI/gpt-neox/blob/main/configs/prof.yml) for a sample config).\n\n![mem-prof](images/memory_profiling.png)\n\nView the generated profile with the [memory_viz.py](https://github.com/pytorch/pytorch/blob/main/torch/cuda/_memory_viz.py) script. Run with:\n\n```\npython _memory_viz.py trace_plot <generated_profile> -o trace.html\n```\n\n# Adoption and Publications\n\nThe GPT-NeoX library was been widely adopted by academic and industry researchers and ported on to many HPC systems.\n\nIf you have found this library useful in your research, please reach out and let us know! We would love to add you to our lists.\n\n## Publications\n\nEleutherAI and our collaborators have used it in the following publications:\n - **Sid Black**, **Stella Biderman**, **Eric Hallahan**, **Quentin Anthony**, **Leo Gao**, **Laurence Golding**, **Horace He**, **Connor Leahy**, **Kyle McDonell**, **Jason Phang**, **Michael Pieler**, **Shivanshu Purohit**, **Laria Reynolds**, **Jon Tow**, **Ben Wang**, and **Samuel Weinbach**. \"[GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745).\" In *Proceedings of the ACL Workshop on Challenges \\& Perspectives in Creating Large Language Models*, 2022.\n - **Stella Biderman**, **Hailey Schoelkopf**, **Quentin Anthony**, **Herbie Bradley**, **Kyle O'Brien**, **Eric Hallahan**, **Mohammad Aflah Khan**, **Shivanshu Purohit**, **USVSN Sai Prashanth**, Edward Raff, **Aviya Skowron**, **Lintang Sutawika**, **Oskar van der Wal**. \"[Pythia: A suite for analyzing large language models across training and scaling](https://arxiv.org/abs/2304.01373).\" In _International Conference on Machine Learning_, pp. 2397-2430. _PMLR_, 2023.\n - Zhangir Azerbayev, Bartosz Piotrowski, **Hailey Schoelkopf**, Edward W. Ayers, Dragomir Radev, and Jeremy Avigad. \"[Proofnet: Autoformalizing and formally proving undergraduate-level mathematics](https://arxiv.org/abs/2302.12433). *arXiv preprint arXiv:2302.12433*, 2023.\n - **Stella Biderman**, **USVSN Sai Prashanth**, **Lintang Sutawika**, **Hailey Schoelkopf**, **Quentin Anthony**, **Shivanshu Purohit**, and Edward Raff. \"[Emergent and predictable memorization in large language models.](https://arxiv.org/abs/2304.11158)\" In _Neural Information Processing Systems_, 2023.\n - **Hyunwoong Ko**, **Kichang Yang**, **Minho Ryu**, **Taekyoon Choi**, **Seungmu Yang,** and Sungho Park. \"[A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models](https://arxiv.org/abs/2306.02254).\" *arXiv preprint arXiv:2306.02254*, 2023.\n - Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats Leon Richter, **Quentin Anthony**, Eugene Belilovsky, Irina Rish, and Timothée Lesort. \"[Continual Pre-Training of Large Language Models: How to re-warm your model?](https://arxiv.org/abs/2308.04014)\" In _Workshop on Efficient Systems for Foundation Models @ ICML_, 2023.\n - **Zhangir Azerbayev**, **Hailey Schoelkopf**, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, **Stella Biderman**, and Sean Welleck. \"[Llemma: An open language model for mathematics]([https://arxiv.org/abs/2308.04014](https://arxiv.org/abs/2310.10631))\" In _Math-AI Workshop @ NeurIPS_, 2023.\n - Alexander Havrilla, Maksym Zhuravinskyi, Duy Phung, Aman Tiwari, Jonathan Tow, **Stella Biderman**, **Quentin Anthony**, and **Louis Castricato**. \"[trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback](https://aclanthology.org/2023.emnlp-main.530/).\" In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, 2023.\n -  **Quentin Anthony**, **Jacob Hatef**, Deepak Narayanan, **Stella Biderman**, Stas Bekman, Junqi Yin, Aamir Shafi, Hari Subramoni, and Dhabaleswar Panda. \"[The Case for Co-Designing Model Architectures with Hardware](https://arxiv.org/abs/2401.14489).\" In _arXiv preprint_, 2024.\n - Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, **Quentin Anthony**, Timothée Lesort, Eugene Belilovsky, Irina Rish. \"[Simple and Scalable Strategies to Continually Pre-train Large Language Models](https://arxiv.org/abs/2403.08763).\" In _arXiv preprint_, 2024.\n - Junqi Yin, Avishek Bose, Guojing Cong, Isaac Lyngaas, **Quentin Anthony**. \"[Comparative Study of Large Language Model Architectures on Frontier](https://arxiv.org/abs/2402.00691).\" In _arXiv preprint_, 2024.\n\nThe following publications by other research groups use this library:\n- Ta-Chung Chi, Ting-Han Fan, Peter J. Ramadge, and Alexander Rudnicky. \"[KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation](https://arxiv.org/abs/2205.09921).\" In *Advances in Neural Information Processing Systems* 35, 2022.\n- Sameera Horawalavithana, Ellyn Ayton, Shivam Sharma, Scott Howland, Megha Subramanian, Scott Vasquez, Robin Cosbey, Maria Glenski, and Svitlana Volkova. \"[Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned](https://aclanthology.org/2022.bigscience-1.12/).\" In *Proceedings of the ACL Workshop on Challenges \\& Perspectives in Creating Large Language Models*, 2022.\n- Sophia Kolak, Ruben Martins, Claire Le Goues, and Vincent J. Hellendoorn. \"[Patch Generation with Language Models: Feasibility and Scaling Behavior](https://par.nsf.gov/biblio/10340618)\".\" In *Proceedings of the Deep Learning for Code Workshop at ICLR*, 2022.\n- Frank F. Xu, Uri Alon, Graham Neubig, and Vincent J. Hellendoorn. \"[A Systematic Evaluation of Large Language Models of Code](https://arxiv.org/abs/2202.13169).\" In *Proceedings of the ICLR Workshop on Deep Learning For Code*, 2022.\n- Byung-Doh Oh and William Schuler. \"[Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens](https://arxiv.org/abs/2304.11389).\" In *Findings of the Association for Computational Linguistics*, 2023.\n- Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. \"[Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis](https://aclanthology.org/2023.acl-long.756/).\" In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 13522-13537, 2023.\n- Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander Rudnicky, and Peter Ramadge. \"[Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings](https://aclanthology.org/2023.acl-short.102/).\" In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pp. 13522-13537, 2023.\n- Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, and Jun Wang. \"[ChessGPT: Bridging Policy Learning and Language Modeling.](https://arxiv.org/abs/2306.09200)\" _arXiv preprint arXiv:2306.09200_, 2023.\n- Orion Walker Dollar, Sameera Horawalavithana, Scott Vasquez, W. James Pfaendtner, and Svitlana Volkova. \"[MolJET: Multimodal Joint Embedding Transformer for Conditional de novo Molecular Design and Multi-Property Optimization.](https://openreview.net/pdf?id=7UudBVsIrr)\" _preprint under review_, 2023.\n- Jean Kaddour and Qi Liu. \"[Text Data Augmentation in Low-Resource Settings via Fine-Tuning of Large Language Models](https://arxiv.org/abs/2310.01119).\" _arXiv:2310.01119_, 2023.\n- Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. \"[Efficient Online Data Mixing For Language Model Pre-Training](https://arxiv.org/abs/2312.02406).\" In _NeurIPS Workshop on R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models_, 2023.\n- Eghbal A. Hosseini and Evelina Fedorenko. \"[Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language](https://www.biorxiv.org/content/10.1101/2023.11.05.564832v1).\" In _Neural Information Processing Systems_, 2023.\n- Junqi Yin, Sajal Dash, Feiyi Wang, and Mallikarjun Shankar. \"[FORGE: Pre-Training Open Foundation Models for Science](https://dl.acm.org/doi/abs/10.1145/3581784.3613215). In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, 1-13, 2023.\n- Jean Kaddour and Qi Liu. \"[Text Data Augmentation in Low-Resource Settings via Fine-Tuning of Large Language Models](https://arxiv.org/abs/2310.01119).\" In _arXiv preprint arXiv:2310.01119_, 2023.\n- Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang Fan, Jie Gong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li, Ming Liang, Cong Liao, Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Min Shen, Guangpei Wang, Huan Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu Zhang, Zelin Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, and Xianying Zhu. \"[CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model](https://arxiv.org/abs/2310.06266).\" In _arXiv preprint arXiv:2310.06266_, 2023.\n- Nikitha Rao, Kush Jain, Uri Alon, Claire Le Goues, and Vincent J Hellendoorn. \"[CAT-LM Training Language Models on Aligned Code And Tests](https://arxiv.org/abs/2310.01602).\" In _38th IEEE/ACM International Conference on Automated Software Engineering (ASE)_, pp. 409-420. IEEE, 2023.\n- Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier, Nithish Mahalingam, Ricardo Bianchini. \"[POLCA: Power Oversubscription in LLM Cloud Providers](https://arxiv.org/abs/2308.12908).\" In _arXiv preprint_, 2023.\n- Junqi Yin, Sajal Dash, John Gounley, Feiyi Wang, and Georgia Tourassi. \"[Evaluation of pre-training large language models on leadership-class supercomputers](https://link.springer.com/article/10.1007/s11227-023-05479-7).\" In _the Journal of Supercomputing_ 79, no. 18, 2023.\n- Tal Kadosh, Niranjan Hasabnis, Vy A. Vo, Nadav Schneider, Neva Krien, Mihai Capota, Abdul Wasay, Nesreen Ahmed, Ted Willke, Guy Tamir, Yuval Pinter, Timothy Mattson, and Gal Oren. \"[Domain-Specific Code Language Models: Unraveling the Potential for HPC Codes and Tasks](https://arxiv.org/abs/2312.13322).\" In _arXiv preprint_, 2023.\n- Guobin Shen, Dongcheng Zhao, Yiting Dong, Yang Li, Jindong Li, Kang Sun, and Yi Zeng. \"[Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling](https://arxiv.org/abs/2312.07625).\" In _arXiv preprint_, 2023.\n- Eghbal A. Hosseini, Martin A. Schrimpf, Yian Zhang, Samuel Bowman, Noga Zaslavsky, and Evelina Fedorenko. \"[Artificial neural network language models align neurally and behaviorally with humans even after a developmentally realistic amount of training.](https://www.biorxiv.org/content/10.1101/2022.10.04.510681)\" In _Neurobiology of Language_, 2024.\n- Xiongye Xiao, Chenyu Zhou, Heng Ping, Defu Cao, Yaxing Li, Yizhuo Zhou, Shixuan Li, and Paul Bogdan. \"[Exploring Neuron Interactions and Emergence in LLMs: From the Multifractal Analysis Perspective](https://arxiv.org/abs/2402.09099).\" In _arXiv preprint_, 2024.\n- Zhiyuan Zeng, Qipeng Guo, Zhaoye Fei, Zhangyue Yin, Yunhua Zhou, Linyang Li, Tianxiang Sun, Hang Yan, Dahua Lin, and Xipeng Qiu. \"[Turn Waste into Worth: Rectifying Top-k Router of MoE](https://arxiv.org/abs/2402.12399).\" In _arXiv preprint_, 2024.\n\n## Models\nThe following models were trained using this library:\n\n### English LLMs\n- EleutherAI's [GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b) and [Pythia (70M through 13B)](https://github.com/EleutherAI/pythia)\n- CarperAI's [FIM-NeoX-1.3B](https://huggingface.co/CarperAI/FIM-NeoX-1.3B)\n- StabilityAI's [StableLM (3B and 7B)](https://github.com/Stability-AI/StableLM)\n- Together.ai's [RedPajama-INCITE (3B and 7B)](https://together.ai/blog/redpajama-models-v1)\n- Carnegie Mellon University's [proofGPT (1.3B and 6.7B)](https://huggingface.co/hoskinson-center/proofGPT-v0.1-6.7B)\n- Dampish's [StellarX (2.8B and 4B)](https://huggingface.co/Dampish/StellarX-4B-V0.2)\n- Chinese Academy of Sciences's [AstroSNN (1.5B)](https://arxiv.org/abs/2312.07625)\n\n### Non-English LLMs\n- EleutherAI's [Polyglot-Ko (1.3B through 12.8B)](https://github.com/EleutherAI/polyglot) (Korean)\n- Korea University's [KULLM-Polyglot (5.8B and 12.8B)](https://github.com/nlpai-lab/KULLM) (Korean)\n- Stability AI's [Japanese Stable LM (7B)](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b) (Japanese)\n- LearnItAnyway's [LLaVA-Polyglot-Ko (1.3B)](https://huggingface.co/LearnItAnyway/llava-polyglot-ko-1.3b-hf) (Korean)\n- Rinna Co.'s [japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b) (Japanese) and [bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b) (English / Japanese)\n- CyberAgent's [Open-CLM (125M through 7B)](https://huggingface.co/cyberagent/open-calm-7b) (Japanese)\n- The Hungarian Research Centre for Linguistics's [PULI GPTrio (6.7B)](https://huggingface.co/NYTK/PULI-GPTrio) (Hungarian / English / Chinese)\n- The University of Tokyo's [weblab-10b](https://huggingface.co/Kojima777/weblab-10b) and [weblab-10b-instruct](https://huggingface.co/Kojima777/weblab-10b-instruction-sft) (Japanese)\n- nolando.ai's [Hi-NOLIN (9B)](https://blog.nolano.ai/Hi-NOLIN/) (English, Hindi)\n- Renmin University of China's [YuLan (12B)](https://huggingface.co/yulan-team/YuLan-Base-12b) (English, Chinese)\n- The Basque Center for Language Technology's [Latixna (70B)](https://huggingface.co/HiTZ/latxa-70b-v1.2) (Basque)\n\n### Code Models\n- Carnegie Mellon University's [PolyCoder (160M through 2.7B)](https://github.com/VHellendoorn/Code-LMs) and [CAT-LM (2.7B)](https://huggingface.co/nikitharao/catlm)\n- StabilityAI's [StableCode (1.3B)](https://stability.ai/blog/stablecode-llm-generative-ai-coding) and [StableCode-Completion-Alpha (3B)](https://stability.ai/blog/stablecode-llm-generative-ai-coding)\n- CodeFuse AI's [CodeFuse (13B)](https://huggingface.co/codefuse-ai/CodeFuse-13B)\n\n### AI for Science\n- EleutherAI's [LLeMMA (34B)](https://arxiv.org/abs/2310.10631)\n- Oak Ridge National Lab's [FORGE (26B)](https://github.com/at-aaims/forge)\n- Oak Ridge National Lab's [Unnamed Material Science Domain Models (7B)](https://arxiv.org/abs/2402.00691)\n- Pacific Northwest National Lab's [MolJet (undisclosed size)](https://openreview.net/pdf?id=7UudBVsIrr)\n\n### Other Modalities\n-  Rinna Co.'s [PSLM (7B)](https://arxiv.org/abs/2406.12428) (speech / text)\n-  University College London's [ChessGPT-3B](https://huggingface.co/Waterhorse/chessgpt-base-v1)\n-  Gretel's [Text-to-Table (3B)](https://huggingface.co/gretelai/text2table)\n\n# Administrative Notes\n\n## Citing GPT-NeoX\n\nIf you have found the GPT-NeoX library helpful in your work, you can cite this repository as\n\n```bibtex\n@software{gpt-neox-library,\n  title = {{GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch}},\n  author = {Andonian, Alex and Anthony, Quentin and Biderman, Stella and Black, Sid and Gali, Preetham and Gao, Leo and Hallahan, Eric and Levy-Kramer, Josh and Leahy, Connor and Nestler, Lucas and Parker, Kip and Pieler, Michael and Phang, Jason and Purohit, Shivanshu and Schoelkopf, Hailey and Stander, Dashiell and Songz, Tri and Tigges, Curt and Thérien, Benjamin and Wang, Phil and Weinbach, Samuel},\n  url = {https://www.github.com/eleutherai/gpt-neox},\n  doi = {10.5281/zenodo.5879544},\n  month = {9},\n  year = {2023},\n  version = {2.0.0},\n}\n```\n\nTo cite the 20 billion parameter model named `GPT-NeoX-20B`, please use\n\n```bibtex\n@inproceedings{gpt-neox-20b,\n  title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},\n  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},\n  booktitle={Proceedings of the ACL Workshop on Challenges \\& Perspectives in Creating Large Language Models},\n  url={https://arxiv.org/abs/2204.06745},\n  year={2022}\n}\n```\n\n## Contributing\nGPT-NeoX is built by the open-source AI community, and relies on our amazing contributors! Please see our\n[contributing](CONTRIBUTING.md) guide for more details on our CLA, code formatting, testing,\netc.\n\n## Licensing\n\nThis repository hosts code that is part of EleutherAI's GPT-NeoX project. Copyright (c) 2024, EleutherAI. Licensed under the Apache License:\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\nThis repository is based off code written by NVIDIA that is licensed under the Apache License, Version 2.0. In accordance with the Apache License, all files that are modifications of code originally written by NVIDIA maintain a NVIDIA copyright header. All files that do not contain such a header are the exclusive copyright of EleutherAI. When the NVIDIA code has been modified from its original version, that fact is noted in the copyright header. All derivative works of this repository must preserve these headers under the terms of the Apache License.\n\nThis repository also contains code written by a number of other authors. Such contributions are marked and the relevant licensing is included where appropriate.\n\nFor full terms, see the `LICENSE` file. If you have any questions, comments, or concerns about licensing please email us at contact@eleuther.ai.\n\n## Acknowledgements\n\nWe run our experiments on a Kubernetes cluster provided by [CoreWeave](https://coreweave.com/) and a Slurm cluster provided by [Stability AI](https://stability.ai). We are thankful to the DeepSpeed team for their advice and consultation.\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "deepy.py",
          "type": "blob",
          "size": 1.3134765625,
          "content": "#!/usr/bin/env python\n# Copyright (c) 2024, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport os\n\nimport deepspeed.launcher.runner\n\n\ndef main(input_args=None):\n    logging.basicConfig(level=os.environ.get(\"LOGLEVEL\", \"INFO\"))\n\n    from megatron.neox_arguments import NeoXArgs\n    from megatron.utils import get_wandb_api_key\n\n    neox_args = NeoXArgs.consume_deepy_args(input_args)\n    deepspeed_main_args = neox_args.get_deepspeed_main_args()\n\n    # Extract wandb API key and inject into worker environments\n    wandb_token = get_wandb_api_key(neox_args=neox_args)\n    if wandb_token is not None:\n        deepspeed.launcher.runner.EXPORT_ENVS.append(\"WANDB_API_KEY\")\n        os.environ[\"WANDB_API_KEY\"] = wandb_token\n\n    deepspeed.launcher.runner.main(deepspeed_main_args)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "docker-compose-dockerhub.yml",
          "type": "blob",
          "size": 0.5322265625,
          "content": "version: '3'\nservices:\n  gpt-neox:\n    command: nvidia-smi dmon\n    image: leogao2/gpt-neox:main\n    shm_size: 1g\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    runtime: nvidia\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              capabilities: [gpu]\n    logging:\n      options:\n        max-size: \"100m\"\n        max-file: \"3\"\n    volumes:\n      - ${NEOX_DATA_PATH}:/home/mchorse/data\n      - ${NEOX_CHECKPOINT_PATH}:/home/mchorse/chk\n      - .:/home/mchorse/gpt-neox\n"
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 0.5751953125,
          "content": "version: '3'\nservices:\n  gpt-neox:\n    command: nvidia-smi dmon\n    image: gpt-neox\n    build:\n      context: .\n      dockerfile: Dockerfile\n    shm_size: 1g\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    runtime: nvidia\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              capabilities: [gpu]\n    logging:\n      options:\n        max-size: \"100m\"\n        max-file: \"3\"\n    volumes:\n      - ${NEOX_DATA_PATH}:/home/mchorse/data\n      - ${NEOX_CHECKPOINT_PATH}:/home/mchorse/chk\n      - .:/home/mchorse/gpt-neox\n"
        },
        {
          "name": "eval.py",
          "type": "blob",
          "size": 2.7294921875,
          "content": "# Copyright (c) 2024, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Evaluation tasks - modified from https://github.com/EleutherAI/lm-evaluation-harness\"\"\"\nimport os\nimport sys\n\nsys.path.append(\n    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir))\n)\nfrom megatron.training import forward_step\nfrom megatron.utils import setup_for_inference_or_eval, init_wandb\nfrom megatron.logging import tb_wandb_log\nfrom eval_tasks import run_eval_harness\nfrom pprint import pprint\nfrom datetime import datetime\nimport json\n\n\ndef main(input_args=None, overwrite_values=None):\n    model, neox_args = setup_for_inference_or_eval(\n        use_cache=False, input_args=input_args, overwrite_values=overwrite_values\n    )\n    results = run_eval_harness(\n        model,\n        forward_step,\n        neox_args,\n        eval_tasks=neox_args.eval_tasks,\n        bootstrap_iters=10000,\n    )\n    if neox_args.rank == 0:\n        init_wandb(neox_args=neox_args)\n        # log to wandb\n        for k, v in results[\"results\"].items():\n            if isinstance(v, dict):\n                for k2, v2 in v.items():\n                    k3 = \"_\".join([k, k2])\n                    tb_wandb_log(\n                        f\"eval/{k3}\",\n                        v2,\n                        neox_args.iteration,\n                        use_wandb=neox_args.use_wandb,\n                        comet_experiment=neox_args.comet_experiment,\n                    )\n            else:\n                tb_wandb_log(\n                    f\"eval/{k}\",\n                    v,\n                    neox_args.iteration,\n                    use_wandb=neox_args.use_wandb,\n                    comet_experiment=neox_args.comet_experiment,\n                )\n\n        pprint(results)\n        results_path = (\n            f'eval_results_{datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")}.json'\n        )\n        if neox_args.eval_results_prefix:\n            results_path = f\"{neox_args.eval_results_prefix}_{results_path}\"\n        with open(results_path, \"w\") as f:\n            json.dump(results, f, indent=4)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "eval_tasks",
          "type": "tree",
          "content": null
        },
        {
          "name": "generate.py",
          "type": "blob",
          "size": 3.369140625,
          "content": "#!/usr/bin/env python\n# Copyright (c) 2024 EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom megatron.utils import print_rank_0, setup_for_inference_or_eval\n\nfrom megatron.text_generation_utils import (\n    generate_samples_input_from_file,\n    generate_samples_from_prompt,\n    generate_samples_unconditional,\n    generate_samples_interactive,\n    precompute_logits,\n)\n\n\ndef main(input_args=None, overwrite_values=None):\n    \"\"\"\n    Generate text/sample model\n    \"\"\"\n    model, neox_args = setup_for_inference_or_eval(\n        use_cache=True, input_args=input_args, overwrite_values=overwrite_values\n    )\n    if neox_args.recompute:\n        model.module.inference_mode(\n            use_cache=False\n        )  # don't use kv cache if recomputing\n    if neox_args.text_gen_type == \"unconditional\":\n        print_rank_0(\n            f\"Generating samples unconditionally and saving results to {neox_args.sample_output_file}\"\n        )\n        generate_samples_unconditional(\n            neox_args=neox_args,\n            model=model,\n            number_of_samples=neox_args.num_samples,\n            output_file=neox_args.sample_output_file,\n            maximum_tokens=neox_args.maximum_tokens,\n            recompute=neox_args.recompute,\n            temperature=neox_args.temperature,\n            top_k=neox_args.top_k,\n            top_p=neox_args.top_p,\n        )\n\n    elif neox_args.text_gen_type == \"input-file\":\n        print_rank_0(\n            f\"Generating samples from input file {neox_args.sample_input_file}\"\n        )\n        assert neox_args.sample_input_file is not None\n        generate_samples_input_from_file(\n            neox_args=neox_args,\n            model=model,\n            input_file=neox_args.sample_input_file,\n            output_file=neox_args.sample_output_file,\n            maximum_tokens=neox_args.maximum_tokens,\n            prompt_end=neox_args.prompt_end,\n            recompute=neox_args.recompute,\n            temperature=neox_args.temperature,\n            top_k=neox_args.top_k,\n            top_p=neox_args.top_p,\n        )\n\n    elif neox_args.text_gen_type == \"interactive\":\n        generate_samples_interactive(\n            neox_args=neox_args,\n            model=model,\n            recompute=neox_args.recompute,\n            temperature=neox_args.temperature,\n            maximum_tokens=neox_args.maximum_tokens,\n            prompt_end=neox_args.prompt_end,\n            top_k=neox_args.top_k,\n            top_p=neox_args.top_p,\n        )\n\n    elif neox_args.text_gen_type == \"precompute\":\n        precompute_logits(neox_args=neox_args, model=model)\n    else:\n        raise ValueError(\n            f\"`text_gen_type` either not specified or not recognised: {neox_args.text_gen_type}\"\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "megatron",
          "type": "tree",
          "content": null
        },
        {
          "name": "post-training",
          "type": "tree",
          "content": null
        },
        {
          "name": "prepare_data.py",
          "type": "blob",
          "size": 2.275390625,
          "content": "# Copyright (c) 2024, EleutherAI\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom tools.datasets.corpora import prepare_dataset, DATA_DOWNLOADERS\nimport argparse\n\nTOKENIZER_CHOICES = [\n    \"HFGPT2Tokenizer\",\n    \"HFTokenizer\",\n    \"GPT2BPETokenizer\",\n    \"CharLevelTokenizer\",\n    \"TiktokenTokenizer\",\n    \"SPMTokenizer\",\n]\nDATASET_CHOICES = [i for i in DATA_DOWNLOADERS.keys() if i != \"pass\"]\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description=\"Download & preprocess neox datasets\")\n    parser.add_argument(\n        \"dataset\",\n        nargs=\"?\",\n        default=\"enwik8\",\n        help=\"name of dataset to download.\",\n        choices=DATASET_CHOICES,\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--tokenizer\",\n        default=\"GPT2BPETokenizer\",\n        choices=TOKENIZER_CHOICES,\n        help=f'Type of tokenizer to use - choose from {\", \".join(TOKENIZER_CHOICES)}',\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--data-dir\",\n        default=None,\n        help=f\"Directory to which to download datasets / tokenizer \"\n        f\"files - defaults to ./data\",\n    )\n    parser.add_argument(\n        \"-v\", \"--vocab-file\", default=None, help=f\"Tokenizer vocab file (if required)\"\n    )\n    parser.add_argument(\n        \"-m\", \"--merge-file\", default=None, help=f\"Tokenizer merge file (if required)\"\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--force-redownload\",\n        dest=\"force_redownload\",\n        default=False,\n        action=\"store_true\",\n    )\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n    prepare_dataset(\n        dataset_name=args.dataset,\n        tokenizer_type=args.tokenizer,\n        data_dir=args.data_dir,\n        vocab_file=args.vocab_file,\n        merge_file=args.merge_file,\n        force_redownload=args.force_redownload,\n    )\n"
        },
        {
          "name": "requirements",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 1.3896484375,
          "content": "# Copyright (c) 2024, EleutherAI\n# This file is based on code by the authors denoted below and has been modified from its original version.\n#\n# Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Train\"\"\"\nfrom megatron.neox_arguments import NeoXArgs\nfrom megatron.training import pretrain\n\n\ndef main(input_args=None, overwrite_values=None):\n    neox_args = NeoXArgs.consume_neox_args(\n        input_args=input_args, overwrite_values=overwrite_values\n    )\n    neox_args.configure_distributed_args()\n    neox_args.build_tokenizer()  # tokenizer needs to be build in training in order to set the padding vocab\n    neox_args.initialize_tensorboard_writer()  # is initialized if tensorboard directory is defined\n    neox_args.initialize_comet()  # is initialized if comet directory is defined\n    pretrain(neox_args=neox_args)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
      ]
    }
  ]
}