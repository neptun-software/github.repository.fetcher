{
  "metadata": {
    "timestamp": 1736560614232,
    "page": 242,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "brightmart/text_classification",
      "stars": 7884,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.587890625,
          "content": "language: python\npython:\n    - 2.7.13\n    - 3.6.2\ninstall:\n    - pip install flake8==3.3.0  # pytest  # add another testing frameworks later\nbefore_script:\n    # stop the build if there are Python syntax errors or undefined names\n    - time flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics\n    # exit-zero treats all errors as warnings.  The GitHub editor is 127 chars wide\n    - time flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\nscript:\n    - true  # add other tests here\nnotifications:\n    on_success: change\n    on_failure: always\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.0439453125,
          "content": "MIT License\n\nCopyright (c) [year] [fullname]\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 37.482421875,
          "content": "Text Classification\n-------------------------------------------------------------------------\nThe purpose of this repository is to explore text classification methods in NLP with deep learning.\n\n#### Update: \n\nCustomize an NLP API in three minutes, for free: <a href='https://www.cluebenchmarks.com/clueai.html'>NLP API Demo</a>\n\nLanguage Understanding Evaluation benchmark for Chinese(<a href='https://www.CLUEbenchmarks.com'>CLUE benchmark<a/>): run 10 tasks & 9 baselines with one line of code, performance comparision with details.\n\nReleasing Pre-trained Model of <a href=\"https://github.com/brightmart/albert_zh\">ALBERT_Chinese</a> Training with 30G+ Raw Chinese Corpus, xxlarge, xlarge and more, Target to match State of the Art performance in Chinese, 2019-Oct-7, During the National Day of China!\n \n<a href='https://github.com/brightmart/nlp_chinese_corpus'>Large Amount of Chinese Corpus for NLP Available!</a>\n\nGoogle's BERT achieved new state of art result on more than 10 tasks in NLP using pre-train in language model then \n\nfine-tuning. <a href='https://github.com/brightmart/bert_language_understanding'>Pre-train TexCNN: idea from BERT for language understanding with running code and data set</a>\n\n\n#### Introduction\nit has all kinds of baseline models for text classification.\n\nit also support for multi-label classification where multi labels associate with an sentence or document.\n\nalthough many of these models are simple, and may not get you to top level of the task. but some of these models are very \n\nclassic, so they may be good to serve as baseline models. each model has a test function under model class. you can run \n\nit to performance toy task first. the model is independent from data set.\n\n<a href='https://github.com/brightmart/text_classification/blob/master/multi-label-classification.pdf'>check here for formal report of large scale multi-label text classification with deep learning</a>\n\nseveral models here can also be used for modelling question answering (with or without context), or to do sequences generating. \n\nwe explore two seq2seq model(seq2seq with attention,transformer-attention is all you need) to do text classification. \n\nand these two models can also be used for sequences generating and other tasks. if your task is a multi-label classification, \n\nyou can cast the problem to sequences generating.\n\nwe implement two memory network. one is dynamic memory network. previously it reached state of art in question \n\nanswering, sentiment analysis and sequence generating tasks. it is so called one model to do several different tasks, \n\nand reach high performance. it has four modules. the key component is episodic memory module. it use gate mechanism to \n\nperformance attention, and use gated-gru to update episode memory, then it has another gru( in a vertical direction) to \n\nperformance hidden state update. it has ability to do transitive inference.\n\nthe second memory network we implemented is recurrent entity network: tracking state of the world. it has blocks of \n\nkey-value pairs as memory, run in parallel, which achieve new state of art. it can be used for modelling question \n\nanswering with contexts(or history). for example, you can let the model to read some sentences(as context), and ask a \n\nquestion(as query), then ask the model to predict an answer; if you feed story same as query, then it can do \n\nclassification task. \n\nTo discuss ML/DL/NLP problems and get tech support from each other, you can join QQ group: 836811304\n\nModels:\n-------------------------------------------------------------------------\n\n1) fastText\n2) TextCNN \n3) Bert:Pre-training of Deep Bidirectional Transformers for Language Understanding  \n4) TextRNN    \n5) RCNN     \n6) Hierarchical Attention Network    \n7) seq2seq with attention   \n8) Transformer(\"Attend Is All You Need\")\n9) Dynamic Memory Network\n10) EntityNetwork:tracking state of the world\n11) Ensemble models\n12) Boosting: \n\n    for a single model, stack identical models together. each layer is a model. the result will be based on logits added together. the only connection between layers are label's weights. the front layer's prediction error rate of each label will become weight for the next layers. those labels with high error rate will have big weight. so later layer's will pay more attention to those mis-predicted labels, and try to fix previous mistake of former layer. as a result, we will get a much strong model.\n    check a00_boosting/boosting.py\n\nand other models:\n\n1) BiLstmTextRelation;\n\n2) twoCNNTextRelation;\n\n3) BiLstmTextRelationTwoRNN\n\nPerformance\n-------------------------------------------------------------------------\n\n(mulit-label label prediction task,ask to prediction top5, 3 million training data,full score:0.5)\n\nModel   | fastText|TextCNN|TextRNN| RCNN | HierAtteNet|Seq2seqAttn|EntityNet|DynamicMemory|Transformer\n---     | ---     | ---   | ---   |---   |---         |---        |---      |---          |----\nScore   | 0.362   |  0.405| 0.358 | 0.395| 0.398      |0.322      |0.400    |0.392        |0.322\nTraining| 10m     |  2h   |10h    | 2h   | 2h         |3h         |3h       |5h           |7h\n--------------------------------------------------------------------------------------------------\n \n Bert model achieves 0.368 after first 9 epoch from validation set.\n \n Ensemble of TextCNN,EntityNet,DynamicMemory: 0.411\n \n Ensemble EntityNet,DynamicMemory: 0.403\n \n\n \n --------------------------------------------------------------------------------------------------\n \n Notice: \n \n `m` stand for **minutes**; `h` stand for **hours**;\n \n`HierAtteNet` means Hierarchical Attention Networkk;\n\n`Seq2seqAttn` means Seq2seq with attention;\n\n`DynamicMemory` means DynamicMemoryNetwork;\n\n`Transformer` stand for model from 'Attention Is All You Need'.\n\nUsage:\n-------------------------------------------------------------------------------------------------------\n1) model is in `xxx_model.py`\n2) run python `xxx_train.py` to train the model\n3) run python `xxx_predict.py` to do inference(test).\n\nEach model has a test method under the model class. you can run the test method first to check whether the model can work properly.\n\n-------------------------------------------------------------------------\n\nEnvironment:\n-------------------------------------------------------------------------------------------------------\npython 2.7+ tensorflow 1.8 \n\n(tensorflow 1.1 to 1.13 should also works; most of models should also work fine in other tensorflow version, since we \n\nuse very few features bond to certain version.\n\nif you use python3, it will be fine as long as you change print/try catch function in case you meet any error.\n\nTextCNN model is already transfomed to python 3.6\n\n\nSample data: <a href='https://pan.baidu.com/s/1yWZf2eAPxq15-r2hHk2M-Q'>cached file of baidu</a> or <a href=\"https://drive.google.com/drive/folders/0AKEuT4gza2AlUk9PVA\">Google Drive:</a>send me an email\n-------------------------------------------------------------------------------------------------------\nto help you run this repository, currently we re-generate training/validation/test data and vocabulary/labels, and saved \n\nthem as cache file using h5py. we suggest you to download it from above link.\n\nit contain everything you need to run this repository: data is pre-processed, you can start to train the model in a minute.\n  \nit's a zip file about 1.8G, contains 3 million training data. although after unzip it's quite big, but with the help of \n\nhdf5, it only need a normal size of memory of computer(e.g.8 G or less) during training.\n\nwe use jupyter notebook: <a href='https://github.com/brightmart/text_classification/blob/master/pre-processing.ipynb'>pre-processing.ipynb</a> to pre-process data. you can have a better understanding of this task and \n\ndata by taking a look of it. you can also generate data by yourself in the way your want, just change few lines of code \n\nusing this jupyter notebook.\n\nIf you want to try a model now, you can dowload cached file from above, then go to folder 'a02_TextCNN', run \n        \n     python  p7_TextCNN_train.py \n   \nit will use data from cached files to train the model, and print loss and F1 score periodically.\n\nold sample data source:\nif you need some sample data and word embedding per-trained on word2vec, you can find it in closed issues, such as: <a href=\"https://github.com/brightmart/text_classification/issues/3\">issue 3</a>. \n\nyou can also find some sample data at folder \"data\". it contains two files:'sample_single_label.txt', contains 50k data \n\nwith single label; 'sample_multiple_label.txt', contains 20k data with multiple labels. input and label of is separate by \"   __label__\".\n\nif you want to know more detail about data set of text classification or task these models can be used, one of choose is below:\n\nhttps://biendata.com/competition/zhihu/\n\nRoad Map\n-------------------------------------------------------------------------------------------------------\nOne way you can use this repository:\n \nstep 1: you can read through this article. you will get a general idea of various classic models used to do text classification.\n\nstep 2: pre-process data and/or download cached file.\n\n      a. take a look a look of jupyter notebook('pre-processing.ipynb'), where you can familiar with this text \n\n           classification task and data set. you will also know how we pre-process data and generate training/validation/test \n           \n           set. there are a list of things you can try at the end of this jupyter.\n\n       b. download zip file that contains cached files, so you will have all necessary data, and can start to train models.\n\nstep 3: run some of models list here, and change some codes and configurations as you want, to get a good performance.\n\n      record performances, and things you done that works, and things that are not.\n\n      for example, you can take this sequence to explore: \n      \n      1) fasttext---> 2)TextCNN---> 3)Transformer---> 4)BERT\n\nadditionally, write your article about this topic, you can follow paper's style to write. you may need to read some papers\n       \n       on the way, many of these papers list in the # Reference at the end of this article; or join  a machine learning \n       \n       competition, and apply it with what you've learned. \n       \nUse Your Own Data:\n-------------------------------------------------------------------------------------------------------\nreplace data in 'data/sample_multiple_label.txt', and make sure format as below:\n\n'word1 word2 word3 __label__l1 __label__l2 __label__l3'\n \nwhere part1: 'word1 word2 word3' is input(X), part2: '__label__l1 __label__l2 __label__l3' \n\nrepresenting there are three labels: [l1,l2,l3]. between part1 and part2 there should be a empty string: ' '.\n\nfor example: each line (multiple labels) like: \n\n'w5466 w138990 w1638 w4301 w6 w470 w202 c1834 c1400 c134 c57 c73 c699 c317 c184 __label__5626661657638885119 __label__4921793805334628695 __label__8904735555009151318'\n\nwhere '5626661657638885119','4921793805334628695'，‘8904735555009151318’ are three labels associate with this input string 'w5466 w138990...c699 c317 c184'\n\nNotice:\n\n\nSome util function is in data_util.py;  check load_data_multilabel() of data_util for how process input and labels from raw data.\n\nthere is a function to load and assign pretrained word embedding to the model,where word embedding is pretrained in word2vec or fastText. \n\nPretrain Work Embedding:\n-------------------------------------------------------------------------------------------------------\nif word2vec.load not works, you may load pretrained word embedding, especially for chinese word embedding use following lines:\n\nimport gensim\n\nfrom gensim.models import KeyedVectors\n\nword2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True, unicode_errors='ignore')  #\n\nor you can turn off use pretrain word embedding flag to false to disable loading word embedding.\n\n\nModels Detail:\n-------------------------------------------------------------------------\n\n1.fastText:  \n-------------\nimplmentation of <a href=\"https://arxiv.org/abs/1607.01759\">Bag of Tricks for Efficient Text Classification</a>\n\nafter embed each word in the sentence, this word representations are then averaged into a text representation, which is in turn fed to a linear classifier.it use softmax function to compute the probability distribution over the predefined classes. then cross entropy is used to compute loss. bag of word representation does not consider word order. in order to take account of word order, n-gram features is used to capture some partial information about the local word order; when the number of classes is large, computing the linear classifier is computational expensive. so it usehierarchical softmax to speed training process.\n1) use bi-gram and/or tri-gram\n2) use NCE loss to speed us softmax computation(not use hierarchy softmax as original paper)\n\nresult: performance is as good as paper, speed also very fast.\n\ncheck: p5_fastTextB_model.py\n\n![alt text](https://github.com/brightmart/text_classification/blob/master/images/fastText.JPG)\n-------------------------------------------------------------------------\n\n2.TextCNN:\n-------------\nImplementation of <a href=\"http://www.aclweb.org/anthology/D14-1181\"> Convolutional Neural Networks for Sentence Classification </a>\n\nStructure:embedding--->conv--->max pooling--->fully connected layer-------->softmax\n\nCheck: p7_TextCNN_model.py\n\nIn order to get very good result with TextCNN, you also need to read carefully about this paper <a href=\"https://arxiv.org/abs/1510.03820\">A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification</a>: it give you some insights of things that can affect performance. although you need to  change some settings according to your specific task.\n\nConvolutional Neural Network is main building box for solve problems of computer vision. Now we will show how CNN can be used for NLP, in in particular, text classification. Sentence length will be different from one to another. So we will use pad to get fixed length, n. For each token in the sentence, we will use word embedding to get a fixed dimension vector, d. So our input is a 2-dimension matrix:(n,d). This is similar with image for CNN. \n\nFirstly, we will do convolutional operation to our input. It is a element-wise multiply between filter and part of input. We use k number of filters, each filter size is a 2-dimension matrix (f,d). Now the output will be k number of lists. Each list has a length of n-f+1. each element is a scalar. Notice that the second dimension will be always the dimension of word embedding. We are using different size of filters to get rich features from text inputs. And this is something similar with n-gram features. \n\nSecondly, we will do max pooling for the output of convolutional operation. For k number of lists, we will get k number of scalars. \n\nThirdly, we will concatenate scalars to form final features. It is a fixed-size vector. And it is independent from the size of filters we use.\n\nFinally, we will use linear layer to project these features to per-defined labels.\n\n![alt text](https://github.com/brightmart/text_classification/blob/master/images/TextCNN.JPG)\n\n-------------------------------------------------------------------------\n\n\n3.BERT: \n-------------------------------------------------------------------------\n#### Pre-training of Deep Bidirectional Transformers for Language Understanding \n\nBERT currently achieve state of art results on more than 10 NLP tasks. the key ideas behind this model is that we can \n\npre-train the model by using one kind of language model with huge amount of raw data, where you can find it easily.\n\nas most of parameters of the model is pre-trained, only last layer for classifier need to be need for different tasks.\n\nas a result, this model is generic and very powerful. you can just fine-tuning based on the pre-trained model within\n \na short period of time.\n \nhowever, this model is quite big. with sequence length 128, you may only able to train with a batch size of 32; for long\n\ndocument such as sequence length 512, it can only train a batch size 4 for a normal GPU(with 11G); and very few people\n\ncan pre-train this model from scratch, as it takes many days or weeks to train, and a normal GPU's memory is too small \n\nfor this model.\n\nSpecially, the backbone model is Transformer, where you can find it in Attention Is All You Need. it use two kind of \n\ntasks to pre-train the model.\n\n#### Masked Languge Model\ngenerally speaking, given a sentence, some percentage of words are masked, you will need to predict the masked words\n\nbased on this masked sentence. masked words are chosed randomly.\n\nwe feed the input through a deep Transformer encoder and then use the final hidden states corresponding to the masked \n\npositions to predict what word was masked, exactly like we would train a language model.\n\n    source_file each line is a sequence of token, can be a sentence.\n    \n    Input Sequence  : The man went to [MASK] store with [MASK] dog\n    Target Sequence :                  the                his\n         \n\n#### Next Sentence Prediction\nmany language understanding task, like question answering, inference, need understand relationship\n  \nbetween sentence. however, language model is only able to understand without a sentence. next sentence\n\nprediction is a sample task to help model understand better in these kinds of task.\n\n50% of chance the second sentence is tbe next sentence of the first one, 50% of not the next one.\n\ngiven two sentence, the model is asked to predict whether the second sentence is real next sentence of \n\nthe first one.\n  \n    Input : [CLS] the man went to the store [SEP] he bought a gallon of milk [SEP]\n    Label : IsNext\n\n    Input = [CLS] the man heading to the store [SEP] penguin [MASK] are flight ##less birds [SEP]\n    Label = NotNext\n    \n<img src=\"https://github.com/brightmart/text_classification/blob/master/images/bert_1.jpeg\"  width=\"65%\" height=\"65%\" />\n\n<img src=\"https://github.com/brightmart/text_classification/blob/master/images/bert_2.jpeg\"  width=\"65%\" height=\"65%\" />\n\n\n#### How to use BERT?\n\nbasically, you can download pre-trained model, can just fine-tuning on your task with your own data.\n\nfor classification task, you can add processor to define the format you want to let input and labels from source data.\n\n#### Use BERT for multi-label classification?\n\nrun the following command under folder a00_Bert:\n \n      python  train_bert_multi-label.py\n   \nIt achieve 0.368 after 9 epoch.\nor you can run multi-label classification with downloadable data using BERT from \n\n<a href='https://github.com/brightmart/sentiment_analysis_fine_grain'>sentiment_analysis_fine_grain with BERT</a>\n \n#### Use BERT for online prediction \n\nyou can use session and feed style to restore model and feed data, then get logits to make a online prediction.\n\n<a href='https://github.com/brightmart/sentiment_analysis_fine_grain'>online prediction with BERT</a>\n\noriginally, it train or evaluate model based on file, not for online.\n\n#### How to get better model for BERT?\n\nfirstly, you can use pre-trained model download from google. run a few epoch on you dataset, and find a suitable \n\nsequence length.\n\nsecondly, you can pre-train the base model in your own data as long as  you can find a dataset that is related to \n\nyour task, then fine-tuning on your specific task.\n\nthirdly, you can change loss function and last layer to better suit for your task.\n\nadditionally, you can add define some pre-trained tasks that will help the model understand your task much better.\n\nas experienced we got from experiments, pre-trained task is independent from model and pre-train is not limit to \n\nthe tasks above.\n\n-------------------------------------------------------------------------\n\n\n4.TextRNN\n-------------\nStructure v1:embedding--->bi-directional lstm--->concat output--->average----->softmax layer\n\ncheck: p8_TextRNN_model.py\n\n![alt text](https://github.com/brightmart/text_classification/blob/master/images/bi-directionalRNN.JPG)\n\nStructure v2:embedding-->bi-directional lstm---->dropout-->concat ouput--->lstm--->droput-->FC layer-->softmax layer\n\ncheck: p8_TextRNN_model_multilayer.py\n\n![alt text](https://github.com/brightmart/text_classification/blob/master/images/emojifier-v2.png)\n\n\n-------------------------------------------------------------------------\n\n\n5.BiLstmTextRelation\n-------------\nStructure same as TextRNN. but input is special designed. e.g.input:\"how much is the computer? EOS price of laptop\". where 'EOS' is a special\ntoken spilted question1 and question2.\n\ncheck:p9_BiLstmTextRelation_model.py\n\n\n-------------------------------------------------------------------------\n\n\n6.twoCNNTextRelation\n-------------\nStructure: first use two different convolutional to extract feature of two sentences. then concat two features. use linear\ntransform layer to out projection to target label, then softmax.\n\ncheck: p9_twoCNNTextRelation_model.py\n\n\n-------------------------------------------------------------------------\n\n\n7.BiLstmTextRelationTwoRNN\n-------------\nStructure: one bi-directional lstm for one sentence(get output1), another bi-directional lstm for another sentence(get output2). then:\nsoftmax(output1*M*output2)\n\ncheck:p9_BiLstmTextRelationTwoRNN_model.py\n\nfor more detail you can go to: <a herf=\"http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow\">Deep Learning for Chatbots, Part 2 – Implementing a Retrieval-Based Model in Tensorflow<a>\n\n\n-------------------------------------------------------------------------\n\n\n8.RCNN:\n-------------\nRecurrent convolutional neural network for text classification\n\nimplementation of <a href=\"https://scholar.google.com.hk/scholar?q=Recurrent+Convolutional+Neural+Networks+for+Text+Classification&hl=zh-CN&as_sdt=0&as_vis=1&oi=scholart&sa=X&ved=0ahUKEwjpx82cvqTUAhWHspQKHUbDBDYQgQMIITAA\"> Recurrent Convolutional Neural Network for Text Classification </a>\n \nstructure:1)recurrent structure (convolutional layer) 2)max pooling 3) fully connected layer+softmax\n\nit learn represenation of each word in the sentence or document with left side context and right side context:\n\nrepresentation current word=[left_side_context_vector,current_word_embedding,right_side_context_vecotor].\n\nfor left side context, it use a recurrent structure, a no-linearity transfrom of previous word and left side previous context; similarly to right side context.\n\ncheck: p71_TextRCNN_model.py\n\n![alt text](https://github.com/brightmart/text_classification/blob/master/images/RCNN.JPG)\n\n-------------------------------------------------------------------------\n\n9.Hierarchical Attention Network:\n-------------\nImplementation of <a href=\"https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf\">Hierarchical Attention Networks for Document Classification</a>\n\nStructure:\n\n1) embedding \n\n2) Word Encoder: word level bi-directional GRU to get rich representation of words\n\n3) Word Attention:word level attention to get important information in a sentence\n\n4) Sentence Encoder: sentence level bi-directional GRU to get rich representation of sentences\n\n5) Sentence Attetion: sentence level attention to get important sentence among sentences\n\n5) FC+Softmax\n\n![alt text](https://github.com/brightmart/text_classification/blob/master/images/HAN.JPG)\n\nIn NLP, text classification can be done for single sentence, but it can also be used for multiple sentences. we may call it document classification. Words are form to sentence. And sentence are form to document. In this circumstance, there may exists a intrinsic structure. So how can we model this kinds of task? Does all parts of document are equally relevant? And how we determine which part are more important than another?\n\nIt has two unique features: \n\n1)it has a hierarchical structure that reflect the hierarchical structure of documents; \n\n2)it has two levels of attention mechanisms used at the word and sentence-level. it enable the model to capture important information in different levels.\n\nWord Encoder:\nFor each words in a sentence, it is embedded into word vector in distribution vector space. It use a bidirectional GRU to encode the sentence. By concatenate vector from two direction, it now can form a representation of the sentence, which also capture contextual information.\n\nWord Attention:\nSame words are more important than another for the sentence. So attention mechanism is used. It first use one layer MLP to get uit hidden representation of the sentence, then measure the importance of the word as the similarity of uit with a word level context vector uw and get a normalized importance through a softmax function. \n\nSentence Encoder: \nfor sentence vectors, bidirectional GRU is used to encode it. Similarly to word encoder.\n\nSentence Attention: \nsentence level vector is used to measure importance among sentences. Similarly to word attention.\n\nInput of data: \n\nGenerally speaking, input of this model should have serveral sentences instead of sinle sentence. shape is:[None,sentence_lenght]. where None means the batch_size.\n\nIn my training data, for each example, i have four parts. each part has same length. i concat four parts to form one single sentence. the model will split the sentence into four parts, to form a tensor with shape:[None,num_sentence,sentence_length]. where num_sentence is number of sentences(equal to 4, in my setting).\n\ncheck:p1_HierarchicalAttention_model.py\n\nfor attentive attention you can check <a href='https://github.com/brightmart/text_classification/issues/55'>attentive attention</a>\n\n-------------------------------------------------------------------------\n\n10.Seq2seq with attention\n-------------\nImplementation seq2seq with attention derived from <a href=\"https://arxiv.org/pdf/1409.0473.pdf\">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a>\n\nI.Structure:\n\n1)embedding 2)bi-GRU too get rich representation from source sentences(forward & backward). 3)decoder with attention.\n\n![alt text](https://github.com/brightmart/text_classification/blob/master/images/seq2seqAttention.JPG)\n\nII.Input of data:\n\nthere are two kinds of three kinds of inputs:1)encoder inputs, which is a sentence; 2)decoder inputs, it is labels list with fixed length;3)target labels, it is also a list of labels.\n\nfor example, labels is:\"L1 L2 L3 L4\", then decoder inputs will be:[_GO,L1,L2,L2,L3,_PAD]; target label will be:[L1,L2,L3,L3,_END,_PAD]. length is fixed to 6, any exceed labels will be trancated, will pad if label is not enough to fill.\n\nIII.Attention Mechanism:\n\n1) transfer encoder input list and hidden state of decoder\n\n2) calculate similarity of hidden state with each encoder input, to get possibility distribution for each encoder input.\n\n3) weighted sum of encoder input based on possibility distribution.\n\n   go though RNN Cell using this weight sum together with decoder input to get new hidden state\n\nIV.How Vanilla Encoder Decoder Works:\n\nthe source sentence will be encoded using RNN as fixed size vector (\"thought vector\"). then during decoder:\n\n1) when it is training, another RNN will be used to try to get a word by using this \"thought vector\"  as init state, and take input from decoder input at each timestamp. decoder start from special token \"_GO\". \nafter one step is performanced, new hidden state will be get and together with new input, we can continue this process until we reach to a special token \"_END\". \nwe can calculate loss by compute cross entropy loss of logits and target label. logits is get through a projection layer for the hidden state(for output of decoder step(in GRU we can just use hidden states from decoder as output).\n\n2) when it is testing, there is no label. so we should feed the output we get from previous timestamp, and continue the process util we reached \"_END\" TOKEN.\n\nV.Notices:\n\n1) here i use two kinds of vocabularies. one is from words,used by encoder; another is for labels,used by decoder\n\n2) for vocabulary of lables, i insert three special token:\"_GO\",\"_END\",\"_PAD\"; \"_UNK\" is not used, since all labels is pre-defined.\n\n-------------------------------------------------------------------------\n\n11.Transformer(\"Attention Is All You Need\")\n-------------\nStatus: it was able to do task classification. and able to generate reverse order of its sequences in toy task. you can check it by running test function in the model. check: a2_train_classification.py(train) or a2_transformer_classification.py(model)\n\nwe do it in parallell style.layer normalization,residual connection, and mask are also used in the model. \n\nFor every building blocks, we include a test function in the each file below, and we've test each small piece successfully.\n\nSequence to sequence with attention is a typical model to solve sequence generation problem, such as translate, dialogue system. most of time, it use RNN as buidling block to do these tasks. util recently, people also apply convolutional Neural Network for sequence to sequence problem. Transformer, however, it perform these tasks solely on attention mechansim. it is fast and achieve new state-of-art result.\n\n![alt text](https://github.com/brightmart/text_classification/blob/master/images/attention_is_all_you_need.JPG)\n\nIt also has two main parts: encoder and decoder. below is desc from paper:\n\nEncoder:\n\n6 layers.each layers has two sub-layers.\nthe first is multi-head self-attention mechanism;\nthe second is position-wise fully connected feed-forward network.\nfor each sublayer. use LayerNorm(x+Sublayer(x)). all dimension=512.\n\nDecoder:\n\n1. The decoder is composed of a stack of N= 6 identical layers.\n2. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack.\n3. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n\nMain Take away from this model:\n\n1) multi-head self attention: use self attention, linear transform multi-times to get projection of key-values, then do ordinary attention; 2) some tricks to improve performance(residual connection,position encoding, poistion feed forward, label smooth, mask to ignore things we want to ignore).\n\nUse this model to do task classification:\n\nHere we only use encode part for task classification, removed resdiual connection, used only 1 layer.no need to use mask. we use multi-head attention and postionwise feed forward to extract features of input sentence, then use linear layer to project it to get logits.\n\nfor detail of the model, please check: a2_transformer_classification.py\n\n-------------------------------------------------------------------------\n\n12.Recurrent Entity Network\n-------------------------------------------------------------------------\nInput:1. story: it is multi-sentences, as context. 2.query: a sentence, which is a question, 3. ansewr: a single label.\n\nModel Structure:\n\n1) Input encoding: use bag of word to encode story(context) and query(question); take account of position by using position mask\n\n   by using bi-directional rnn to encode story and query, performance boost from 0.392 to 0.398, increase 1.5%.\n\n2) Dynamic memory: \n\na. compute gate by using 'similarity' of keys,values with input of story. \n\nb. get candidate hidden state by transform each key,value and input.\n\nc. combine gate and candidate hidden state to update current hidden state.\n\n3) Output moudle( use attention mechanism):\na. to get possibility distribution by computing 'similarity' of query and hidden state\n\nb. get weighted sum of hidden state using possibility distribution.\n\nc. non-linearity transform of query and hidden state to get predict label.\n\n![alt text](https://github.com/brightmart/text_classification/blob/master/images/EntityNet.JPG)\n\nMain take away from this model:\n\n1) use blocks of keys and values, which is independent from each other. so it can be run in parallel.\n\n2) modelling context and question together. use memory to track state of world; and use non-linearity transform of hidden state and question(query) to make a prediction.\n\n3) simple model can also achieve very good performance. simple encode as use bag of word.\n\nfor detail of the model, please check: a3_entity_network.py\n\nunder this model, it has a test function, which ask this model to count numbers both for story(context) and query(question). but weights of story is smaller than query.\n\n-------------------------------------------------------------------------\n\n13.Dynamic Memory Network\n-------------------------------------------------------------------------\nOutlook of Model:\n\n1.Input Module: encode raw texts into vector representation\n\n2.Question Module: encode question into vector representation\n\n3.Episodic Memory Module: with inputs,it chooses which parts of inputs to focus on through the attention mechanism, taking into account of question and previous memory====>it poduce a 'memory' vecotr.\n\n4.Answer Module:generate an answer from the final memory vector.\n\n![alt text](https://github.com/brightmart/text_classification/blob/master/images/DMN.JPG)\n\nDetail:\n\n1.Input Module:\n\n  a.single sentence: use gru to get hidden state\n  b.list of sentences: use gru to get the hidden states for each sentence. e.g. [hidden states 1,hidden states 2, hidden states...,hidden state n]\n  \n2.Question Module:\n  use gru to get hidden state\n  \n3.Episodic Memory Module:\n\n  use an attention mechanism and recurrent network to updates its memory. \n     \n  a. gate as attention mechanism:\n  \n     two-layer feed forward nueral network.input is candidate fact c,previous memory m and question q. features get by take: element-wise,matmul and absolute distance of q with c, and q with m.\n     \n  b.memory update mechanism: take candidate sentence, gate and previous hidden state, it use gated-gru to update hidden state. like: h=f(c,h_previous,g). the final hidden state is the input for answer module.\n  \n  c.need for multiple episodes===>transitive inference. \n  \n  e.g. ask where is the football? it will attend to sentence of \"john put down the football\"), then in second pass, it need to attend location of john.\n\n4.Answer Module:\ntake the final epsoidic memory, question, it update hidden state of answer module.\n\n\nTODO \n-------------------------------------------------------------------------------------------------------\n1.Character-level Convolutional Networks for Text Classification\n\n2.Convolutional Neural Networks for Text Categorization:Shallow Word-level vs. Deep Character-level\n\n3.Very Deep Convolutional Networks for Text Classification\n\n4.Adversarial Training Methods For Semi-supervised Text Classification\n\n5.Ensemble Models\n\n\nConclusion:\n-------------------------------------------------------------------------\nDuring the process of doing large scale of multi-label classification, serveral lessons has been learned, and some list as below:\n\n1) What is most important thing to reach a high accuracy? \nIt depend the task you are doing. From the task we conducted here, we believe that ensemble models based on models trained from multiple features including word, character for title and description can help to reach very high accuarcy; However, in some cases,as just alphaGo Zero demonstrated, algorithm is more important then data or computational power, in fact alphaGo Zero did not use any humam data. \n\n2) Is there a ceiling for any specific model or algorithm?\nThe answer is yes. lots of different models were used here, we found many models have similar performances, even though there are quite different in structure. In some extent, the difference of performance is not so big.\n\n3) Is case study of error useful?\nI think it is quite useful especially when you have done many different things, but reached a limit. For example, by doing case study, you can find labels that models can make correct prediction, and where they make mistakes. And to imporove performance by  increasing weights of these wrong predicted labels or finding potential errors from data.\n\n4) How can we become expert in a specific of Machine Learning?\nIn my opinion,join a machine learning competation or begin a task with lots of data, then read papers and implement some, is a good starting point. So we will have some really experience and ideas of handling specific task, and know the challenges of it.\nBut what's more important is that we should not only follow ideas from papers, but to explore some new ideas we think may help to slove the problem. For example, by changing structures of classic models or even invent some new structures, we may able to tackle the problem in a much better way as it may more suitable for task we are doing.\n\nReference:\n-------------------------------------------------------------------------------------------------------\n1.Bag of Tricks for Efficient Text Classification\n\n2.Convolutional Neural Networks for Sentence Classification\n\n3.A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification\n\n4.Deep Learning for Chatbots, Part 2 – Implementing a Retrieval-Based Model in Tensorflow, from www.wildml.com\n\n5.Recurrent Convolutional Neural Network for Text Classification\n\n6.Hierarchical Attention Networks for Document Classification\n\n7.Neural Machine Translation by Jointly Learning to Align and Translate\n\n8.Attention Is All You Need\n\n9.Ask Me Anything:Dynamic Memory Networks for Natural Language Processing\n\n10.Tracking the state of world with recurrent entity networks\n\n11.Ensemble Selection from Libraries of Models\n\n12.<a href='https://arxiv.org/abs/1810.04805'>BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding</a>\n\n13.<a href='https://github.com/google-research/bert'>google-research/bert</a>\n\n-------------------------------------------------------------------------\n\nto be continued. for any problem, concat brightmart@hotmail.com\n"
        },
        {
          "name": "a00_Bert",
          "type": "tree",
          "content": null
        },
        {
          "name": "a00_boosting",
          "type": "tree",
          "content": null
        },
        {
          "name": "a01_FastText",
          "type": "tree",
          "content": null
        },
        {
          "name": "a02_TextCNN",
          "type": "tree",
          "content": null
        },
        {
          "name": "a03_TextRNN",
          "type": "tree",
          "content": null
        },
        {
          "name": "a04_TextRCNN",
          "type": "tree",
          "content": null
        },
        {
          "name": "a05_HierarchicalAttentionNetwork",
          "type": "tree",
          "content": null
        },
        {
          "name": "a06_Seq2seqWithAttention",
          "type": "tree",
          "content": null
        },
        {
          "name": "a07_Transformer",
          "type": "tree",
          "content": null
        },
        {
          "name": "a08_EntityNetwork",
          "type": "tree",
          "content": null
        },
        {
          "name": "a08_predict_ensemble.py",
          "type": "blob",
          "size": 16.3974609375,
          "content": "# -*- coding: utf-8 -*-\r\n#prediction using multi-models. take out: create multiple graphs. each graph associate with a session. add logits of models.\r\n#process--->1.load data(X:list of lint,y:int). 2.create session. 3.feed data. 4.predict\r\nimport sys\r\nreload(sys)\r\nsys.setdefaultencoding('utf8')\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nfrom a3_entity_network import EntityNetwork\r\nsys.path.append(\"..\")\r\nfrom a08_DynamicMemoryNetwork.data_util_zhihu import load_data_predict,load_final_test_data,create_voabulary,create_voabulary_label\r\nfrom tflearn.data_utils import pad_sequences #to_categorical\r\nimport codecs\r\nfrom a08_DynamicMemoryNetwork.a8_dynamic_memory_network import DynamicMemoryNetwork\r\nfrom p7_TextCNN_model import TextCNN\r\nfrom p71_TextRCNN_mode2 import TextRCNN\r\n\r\n#configuration\r\nFLAGS=tf.app.flags.FLAGS\r\ntf.app.flags.DEFINE_integer(\"num_classes\",1999,\"number of label\")\r\ntf.app.flags.DEFINE_float(\"learning_rate\",0.01,\"learning rate\")\r\ntf.app.flags.DEFINE_integer(\"batch_size\", 80, \"Batch size for training/evaluating.\") #批处理的大小 32-->128\r\ntf.app.flags.DEFINE_integer(\"decay_steps\", 6000, \"how many steps before decay learning rate.\") #6000批处理的大小 32-->128\r\ntf.app.flags.DEFINE_float(\"decay_rate\", 1.0, \"Rate of decay for learning rate.\") #0.65一次衰减多少\r\ntf.app.flags.DEFINE_string(\"ckpt_dir_dmn\",\"../checkpoint_dynamic_memory_network/\",\"checkpoint location for the model\")\r\ntf.app.flags.DEFINE_integer(\"sequence_length\",60,\"max sentence length\")\r\ntf.app.flags.DEFINE_integer(\"embed_size\",100,\"embedding size\")\r\ntf.app.flags.DEFINE_boolean(\"is_training\",False,\"is traning.true:tranining,false:testing/inference\")\r\ntf.app.flags.DEFINE_integer(\"num_epochs\",1,\"number of epochs to run.\")\r\ntf.app.flags.DEFINE_integer(\"validate_every\", 1, \"Validate every validate_every epochs.\") #每10轮做一次验证\r\ntf.app.flags.DEFINE_boolean(\"use_embedding\",True,\"whether to use embedding or not.\")\r\n#tf.app.flags.DEFINE_string(\"cache_path\",\"text_cnn_checkpoint/data_cache.pik\",\"checkpoint location for the model\")\r\ntf.app.flags.DEFINE_string(\"traning_data_path\",\"../train-zhihu4-only-title-all.txt\",\"path of traning data.\") #O.K.train-zhihu4-only-title-all.txt-->training-data/test-zhihu4-only-title.txt--->'training-data/train-zhihu5-only-title-multilabel.txt'\r\ntf.app.flags.DEFINE_string(\"word2vec_model_path\",\"../zhihu-word2vec-title-desc.bin-100\",\"word2vec's vocabulary and vectors\") #zhihu-word2vec.bin-100-->zhihu-word2vec-multilabel-minicount15.bin-100\r\ntf.app.flags.DEFINE_boolean(\"multi_label_flag\",True,\"use multi label or single label.\")\r\ntf.app.flags.DEFINE_integer(\"hidden_size\",100,\"hidden size\")\r\ntf.app.flags.DEFINE_string(\"predict_target_file\",\"zhihu_result_ensemble_2_0814.csv\",\"target file path for final prediction\")\r\ntf.app.flags.DEFINE_string(\"predict_source_file\",'../test-zhihu-forpredict-title-desc-v6.txt',\"target file path for final prediction\") #test-zhihu-forpredict-v4only-title.txt\r\ntf.app.flags.DEFINE_integer(\"story_length\",1,\"story length\")\r\ntf.app.flags.DEFINE_boolean(\"use_gated_gru\",True,\"whether to use gated gru as  memory update mechanism. if false,use weighted sum of candidate sentences according to gate\")\r\ntf.app.flags.DEFINE_integer(\"num_pass\",3,\"number of pass to run\") #e.g. num_pass=1,2,3,4.\r\ntf.app.flags.DEFINE_float(\"l2_lambda\", 0.0001, \"l2 regularization\")\r\ntf.app.flags.DEFINE_boolean(\"decode_with_sequences\",False,\"if your task is sequence generating, you need to set this true.default is false, for predict a label\")\r\n###################################above from dynamic memory. below from entityNet#######################################################################################\r\ntf.app.flags.DEFINE_string(\"ckpt_dir_entity\",\"../checkpoint_entity_network5-b40-60-l2B/\",\"checkpoint location for the model\")\r\ntf.app.flags.DEFINE_integer(\"block_size\",40,\"block size\")\r\ntf.app.flags.DEFINE_boolean(\"use_bi_lstm\",True,\"whether to use bi-directional lstm for encode of story and query\")\r\n###################################above from dynamic memory. below from entityNet#######################################################################################\r\ntf.app.flags.DEFINE_string(\"ckpt_dir_cnn\",\"../checkpoint_text_cnn/text_cnn_title_desc_checkpoint_exp512/bak_important/\",\"checkpoint location for the model\")\r\ntf.app.flags.DEFINE_integer(\"sentence_len\",100,\"max sentence length\")\r\ntf.app.flags.DEFINE_integer(\"num_filters\", 512, \"number of filters\") #128\r\nfilter_sizes=[3,4,5,7,10,15,20,25]\r\n###################################above is TextRCNN######################################################################################################################\r\ntf.app.flags.DEFINE_string(\"ckpt_dir_rcnn\",\"../checkpoint_rcnn/text_rcnn_title_desc_checkpoint2/\",\"checkpoint location for the model\")\r\n#tf.app.flags.DEFINE_integer(\"sentence_len\",100,\"max sentence length\")\r\n###################################above is RCNN############################################################################################################################\r\n###################################above is TextCNN_256embedding############################################################################################################\r\ntf.app.flags.DEFINE_string(\"ckpt_dir_cnn_256_embedding\",\"../checkpoint_text_cnn/text_cnn_title_desc_checkpoint_exp512_0814/\",\"checkpoint location for the model\")\r\nfilter_sizes_256_embedding=[3,4,5,6,7,8,9,10,15,20,25] #[1,2,3,4,5,6,7,8,9,10]#[1,2,3,4,5,6,7,8,9]#[5,6,7,8,9] #[2,3,5,6,7,8]#[3,4,5,7,10,15,20,25] #[1,2,3,4,5,6,7][3,5,7]#[7,8,9,10,15,20,25] #[3,4,5,7,10,15,20,25]-->[6,7,8,10,15,20,25,30,35]BAD EPOCH2:13.2  #\r\ntf.app.flags.DEFINE_integer(\"num_filters_256_embedding\", 128, \"number of filters\") #256--->512--->600\r\ntf.app.flags.DEFINE_integer(\"embed_size_256_embedding\", 256, \"embedding and hidden size\") #256--->512--->600\r\n###################################above is TextCNN_256embedding############################################################################################################\r\n###################################above is HAN############################################################################################################\r\ntf.app.flags.DEFINE_string(\"ckpt_dir_cnn_256_embedding\",\"../checkpoint_text_cnn/text_cnn_title_desc_checkpoint_exp512_0814/\",\"checkpoint location for the model\")\r\n\r\n###################################above is THAN############################################################################################################\r\n\r\ndef main(_):\r\n    # 1.load data with vocabulary of words and labels\r\n    vocabulary_word2index, vocabulary_index2word = create_voabulary(word2vec_model_path=FLAGS.word2vec_model_path,name_scope=\"dynamic_memory_network\")\r\n    vocab_size = len(vocabulary_word2index)\r\n    vocabulary_word2index_label, vocabulary_index2word_label = create_voabulary_label(name_scope=\"dynamic_memory_network\")\r\n    questionid_question_lists=load_final_test_data(FLAGS.predict_source_file)\r\n    test= load_data_predict(vocabulary_word2index,vocabulary_word2index_label,questionid_question_lists)\r\n    testX=[]\r\n    question_id_list=[]\r\n    for tuple in test:\r\n        question_id,question_string_list=tuple\r\n        question_id_list.append(question_id)\r\n        testX.append(question_string_list)\r\n    # 2.Data preprocessing: Sequence padding\r\n    print(\"start padding....\")\r\n    testX2 = pad_sequences(testX, maxlen=FLAGS.sequence_length, value=0.)  # padding to max length\r\n    testX2_cnn = pad_sequences(testX, maxlen=FLAGS.sentence_len, value=0.)  # padding to max length, for CNN\r\n    print(\"end padding...\")\r\n   # 3.create session.\r\n    config=tf.ConfigProto()\r\n    config.gpu_options.allow_growth=True\r\n    graph1 = tf.Graph().as_default()\r\n    graph2 = tf.Graph().as_default()\r\n    graph3 = tf.Graph().as_default()\r\n    graph4 = tf.Graph().as_default()\r\n    graph5 = tf.Graph().as_default()\r\n    global sess_dmn\r\n    global sess_entity\r\n    global sess_cnn\r\n    global sess_rcnn\r\n    with graph1:#DynamicMemoryNetwork\r\n        sess_dmn = tf.Session(config=config)\r\n        model_dmn = DynamicMemoryNetwork(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\r\n                                     FLAGS.story_length,vocab_size, FLAGS.embed_size, FLAGS.hidden_size, FLAGS.is_training,num_pass=FLAGS.num_pass,\r\n                                     use_gated_gru=FLAGS.use_gated_gru,decode_with_sequences=FLAGS.decode_with_sequences,multi_label_flag=FLAGS.multi_label_flag,l2_lambda=FLAGS.l2_lambda)\r\n        saver_dmn = tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir_dmn + \"checkpoint\"):\r\n            print(\"Restoring Variables from Checkpoint of DMN.\")\r\n            saver_dmn.restore(sess_dmn, tf.train.latest_checkpoint(FLAGS.ckpt_dir_dmn))\r\n        else:\r\n            print(\"Can't find the checkpoint.going to stop.DMN\")\r\n            return\r\n    with graph2:#EntityNet\r\n        sess_entity = tf.Session(config=config)\r\n        model_entity = EntityNetwork(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sequence_length,\r\n                              FLAGS.story_length,vocab_size, FLAGS.embed_size, FLAGS.hidden_size, FLAGS.is_training,\r\n                              multi_label_flag=True, block_size=FLAGS.block_size,use_bi_lstm=FLAGS.use_bi_lstm)\r\n        saver_entity = tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir_entity + \"checkpoint\"):\r\n            print(\"Restoring Variables from Checkpoint of EntityNet.\")\r\n            saver_entity.restore(sess_entity, tf.train.latest_checkpoint(FLAGS.ckpt_dir_entity))\r\n        else:\r\n            print(\"Can't find the checkpoint.going to stop.EntityNet.\")\r\n            return\r\n    with graph3:#TextCNN\r\n        sess_cnn=tf.Session(config=config)\r\n        model_cnn = TextCNN(filter_sizes, FLAGS.num_filters, FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size,\r\n                          FLAGS.decay_steps, FLAGS.decay_rate,FLAGS.sentence_len, vocab_size, FLAGS.embed_size, FLAGS.is_training)\r\n        saver_cnn = tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir_cnn + \"checkpoint\"):\r\n            print(\"Restoring Variables from Checkpoint.TextCNN.\")\r\n            saver_cnn.restore(sess_cnn, tf.train.latest_checkpoint(FLAGS.ckpt_dir_cnn))\r\n        else:\r\n            print(\"Can't find the checkpoint.going to stop.TextCNN.\")\r\n            return\r\n    with graph5:  #TextCNN_256embedding\r\n        sess_cnn_256_embedding = tf.Session(config=config)\r\n        model_cnn_256_embedding = TextCNN(filter_sizes_256_embedding, FLAGS.num_filters_256_embedding, FLAGS.num_classes, FLAGS.learning_rate,\r\n                                FLAGS.batch_size,FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sentence_len, vocab_size,\r\n                                FLAGS.embed_size_256_embedding, FLAGS.is_training)\r\n        saver_cnn_256_embedding = tf.train.Saver()\r\n        if os.path.exists(FLAGS.ckpt_dir_cnn_256_embedding + \"checkpoint\"):\r\n            print(\"Restoring Variables from Checkpoint.TextCNN_256_embedding\")\r\n            saver_cnn_256_embedding.restore(sess_cnn_256_embedding, tf.train.latest_checkpoint(FLAGS.ckpt_dir_cnn_256_embedding))\r\n        else:\r\n            print(\"Can't find the checkpoint.going to stop.TextCNN_256_embedding.\")\r\n            return\r\n    #with graph4:#RCNN\r\n    #    sess_rcnn=tf.Session(config=config)\r\n    #    model_rcnn=TextRCNN(FLAGS.num_classes, FLAGS.learning_rate, FLAGS.decay_steps, FLAGS.decay_rate,FLAGS.sentence_len,\r\n    #            vocab_size,FLAGS.embed_size,FLAGS.is_training,FLAGS.batch_size,multi_label_flag=FLAGS.multi_label_flag)\r\n    #    saver_rcnn = tf.train.Saver()\r\n    #    if os.path.exists(FLAGS.ckpt_dir_rcnn + \"checkpoint\"):\r\n    #        print(\"Restoring Variables from Checkpoint.TextRCNN.\")\r\n    #        saver_rcnn.restore(sess_rcnn, tf.train.latest_checkpoint(FLAGS.ckpt_dir_rcnn))\r\n    #    else:\r\n    #        print(\"Can't find the checkpoint.going to stop.TextRCNN.\")\r\n    #        return\r\n\r\n        # 5.feed data, to get logits\r\n        number_of_training_data=len(testX2);print(\"number_of_training_data:\",number_of_training_data)\r\n        index=0\r\n        predict_target_file_f = codecs.open(FLAGS.predict_target_file, 'a', 'utf8')\r\n        global sess_dmn\r\n        global sess_entity\r\n        for start, end in zip(range(0, number_of_training_data, FLAGS.batch_size),range(FLAGS.batch_size, number_of_training_data+1, FLAGS.batch_size)):\r\n            #1.DMN\r\n            logits_dmn=sess_dmn.run(model_dmn.logits,feed_dict={model_dmn.query:testX2[start:end],model_dmn.story: np.expand_dims(testX2[start:end],axis=1),\r\n                                                        model_dmn.dropout_keep_prob:1.0})\r\n            #2.EntityNet\r\n            logits_entity=sess_entity.run(model_entity.logits,feed_dict={model_entity.query:testX2[start:end],model_entity.story: np.expand_dims(testX2[start:end],axis=1),\r\n                                                        model_entity.dropout_keep_prob:1.0})\r\n            #3.CNN\r\n            logits_cnn = sess_cnn.run(model_cnn.logits,feed_dict={model_cnn.input_x: testX2_cnn[start:end], model_cnn.dropout_keep_prob: 1})\r\n            #4.RCNN\r\n            #logits_rcnn = sess_rcnn.run(model_rcnn.logits, feed_dict={model_rcnn.input_x: testX2_cnn[start:end],model_rcnn.dropout_keep_prob: 1})  # 'shape of logits:', ( 1, 1999)\r\n            #5.CN_256_original_embeddding\r\n            logits_cnn_256_embedding =sess_cnn_256_embedding.run(model_cnn_256_embedding.logits,feed_dict={model_cnn_256_embedding.input_x: testX2_cnn[start:end],\r\n                                                                 model_cnn_256_embedding.dropout_keep_prob: 1})\r\n            #how to combine to logits: average\r\n            logits=logits_cnn*0.3+logits_cnn_256_embedding*0.3+logits_entity*0.2+logits_dmn*0.2#+logits_rcnn*0.15\r\n            question_id_sublist=question_id_list[start:end]\r\n            get_label_using_logits_batch(question_id_sublist, logits, vocabulary_index2word_label, predict_target_file_f)\r\n            index=index+1\r\n        predict_target_file_f.close()\r\n\r\n# get label using logits\r\ndef get_label_using_logits(logits,vocabulary_index2word_label,top_number=5):\r\n    index_list=np.argsort(logits)[-top_number:] #print(\"sum_p\", np.sum(1.0 / (1 + np.exp(-logits))))\r\n    index_list=index_list[::-1]\r\n    label_list=[]\r\n    for index in index_list:\r\n        label=vocabulary_index2word_label[index]\r\n        label_list.append(label) #('get_label_using_logits.label_list:', [u'-3423450385060590478', u'2838091149470021485', u'-3174907002942471215', u'-1812694399780494968', u'6815248286057533876'])\r\n    return label_list\r\n\r\n# get label using logits\r\ndef get_label_using_logits_with_value(logits,vocabulary_index2word_label,top_number=5):\r\n    index_list=np.argsort(logits)[-top_number:] #print(\"sum_p\", np.sum(1.0 / (1 + np.exp(-logits))))\r\n    index_list=index_list[::-1]\r\n    value_list=[]\r\n    label_list=[]\r\n    for index in index_list:\r\n        label=vocabulary_index2word_label[index]\r\n        label_list.append(label) #('get_label_using_logits.label_list:', [u'-3423450385060590478', u'2838091149470021485', u'-3174907002942471215', u'-1812694399780494968', u'6815248286057533876'])\r\n        value_list.append(logits[index])\r\n    return label_list,value_list\r\n\r\n# write question id and labels to file system.\r\ndef write_question_id_with_labels(question_id,labels_list,f):\r\n    labels_string=\",\".join(labels_list)\r\n    f.write(question_id+\",\"+labels_string+\"\\n\")\r\n\r\n# get label using logits\r\ndef get_label_using_logits_batch(question_id_sublist,logits_batch,vocabulary_index2word_label,f,top_number=5):\r\n    #print(\"get_label_using_logits.shape:\", logits_batch.shape) # (10, 1999))=[batch_size,num_labels]===>需要(10,5)\r\n    for i,logits in enumerate(logits_batch):\r\n        index_list=np.argsort(logits)[-top_number:] #print(\"sum_p\", np.sum(1.0 / (1 + np.exp(-logits))))\r\n        index_list=index_list[::-1]\r\n        label_list=[]\r\n        for index in index_list:\r\n            label=vocabulary_index2word_label[index]\r\n            label_list.append(label) #('get_label_using_logits.label_list:', [u'-3423450385060590478', u'2838091149470021485', u'-3174907002942471215', u'-1812694399780494968', u'6815248286057533876'])\r\n        #print(\"get_label_using_logits.label_list\",label_list)\r\n        write_question_id_with_labels(question_id_sublist[i], label_list, f)\r\n    f.flush()\r\n    #return label_list\r\n# write question id and labels to file system.\r\ndef write_question_id_with_labels(question_id,labels_list,f):\r\n    labels_string=\",\".join(labels_list)\r\n    f.write(question_id+\",\"+labels_string+\"\\n\")\r\n\r\nif __name__ == \"__main__\":\r\n    tf.app.run()\r\n"
        },
        {
          "name": "a09_DynamicMemoryNet",
          "type": "tree",
          "content": null
        },
        {
          "name": "aa1_data_util",
          "type": "tree",
          "content": null
        },
        {
          "name": "aa2_ClassificationTflearn",
          "type": "tree",
          "content": null
        },
        {
          "name": "aa3_CNNSentenceClassificationTflearn",
          "type": "tree",
          "content": null
        },
        {
          "name": "aa4_TextCNN_with_RCNN",
          "type": "tree",
          "content": null
        },
        {
          "name": "aa5_BiLstmTextRelation",
          "type": "tree",
          "content": null
        },
        {
          "name": "aa6_TwoCNNTextRelation",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "multi-label-classification.pdf",
          "type": "blob",
          "size": 805.5634765625,
          "content": null
        },
        {
          "name": "pre-processing.ipynb",
          "type": "blob",
          "size": 61.7568359375,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"###                                             Task Background and Data Pre-processing\\n\",\n    \"#### this jupyter notebook is used to pre-processing data. at the end you will have vocabulary, labels, training/validation/test set.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \" in this task, you will be asked to predict top 5 topics given a question and its description.  \\n\",\n    \" \\n\",\n    \"###### source files: \\n\",\n    \" \\n\",\n    \" 1. question_train_set.txt.  you can get question id and its string information, you can transform it to train_X.\\n\",\n    \" \\n\",\n    \"     it has 5 columns, each is split with '\\\\t'. format as below:\\n\",\n    \"     \\n\",\n    \"     question_id ct1,ct2,ct3,...,ctn wt1,wt2,wt3,...,wtn cd1,cd2,cd3,...cdn wd1,wd2,wd3,...,wdn\\n\",\n    \"     \\n\",\n    \"     second column is character token of title, third column is word token of title, forth column is character token of description, fifth column is \\n\",\n    \"     \\n\",\n    \"     word token of description.\\n\",\n    \"     \\n\",\n    \" 2. question_topic_train_set.txt.  you can get question id and its labels. you can transform it to  train_Y. \\n\",\n    \" \\n\",\n    \"     topics associated with a question. it contains with two columns, each column is splitted with '\\\\t'. \\n\",\n    \"     \\n\",\n    \" 3. question_eval_set.txt.  you can get question id and its string information, this will be valid_X. this is same format as question_train_set.txt\\n\",\n    \" \\n\",\n    \" \\n\",\n    \"###### additional stats information:\\n\",\n    \"\\n\",\n    \"1. averaged_length:\\n\",\n    \"\\n\",\n    \"   {'desc_char': 117.39879138670524, 'title_char': 22.207077611187056, 'desc_word': 58.272774333851004, 'title_word': 12.841507923253822}\\n\",\n    \"\\n\",\n    \"2. averaged length of a input. total length of all information(words,character of title+desc): 210.\\n\",\n    \"\\n\",\n    \"3. word of title+desc: 71\\n\",\n    \"\\n\",\n    \"4. character of title+desc: 139\\n\",\n    \"\\n\",\n    \"5. as can see from word embeding files, there are about 11k of charactor tokens, and 410k of word tokens that frequency is more than 5 times\\n\",\n    \"\\n\",\n    \"  in the data set.\\n\",\n    \"  \\n\",\n    \"6. total unique labels: 1999\\n\",\n    \"\\n\",\n    \"###### basic processes\\n\",\n    \"\\n\",\n    \"1. in this notebook, we will use character token of title and description. max sequence length will be set to 200. any sequence exceed of it, will be\\n\",\n    \"\\n\",\n    \"   truncated, any sequence short of it, will be padded. \\n\",\n    \"\\n\",\n    \"2. we will generate vocabulary/ labels dict, and training/validation/test data, then save to cache file(as a pickle file), so during training we can\\n\",\n    \"\\n\",\n    \"   load it quickly.\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 39,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"import package successful...\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# import some packages\\n\",\n    \"import pandas as pd\\n\",\n    \"from collections import Counter\\n\",\n    \"from tflearn.data_utils import pad_sequences\\n\",\n    \"import random\\n\",\n    \"import numpy as np\\n\",\n    \"import h5py\\n\",\n    \"import pickle\\n\",\n    \"print(\\\"import package successful...\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 40,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"('train_data_x:', (2999967, 5))\\n\",\n      \"('train_data_y:', (2999967, 2))\\n\",\n      \"('valid_data_x:', (217360, 5))\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# read source file as csv\\n\",\n    \"base_path='data/ieee_zhihu_cup/'\\n\",\n    \"train_data_x=pd.read_csv(base_path+'question_train_set3.txt',sep='\\\\t', encoding=\\\"utf-8\\\")\\n\",\n    \"train_data_y=pd.read_csv(base_path+'question_topic_train_set3.txt',sep='\\\\t', encoding=\\\"utf-8\\\")\\n\",\n    \"valid_data_x=pd.read_csv(base_path+'question_eval_set3.txt', sep='\\\\t',encoding=\\\"utf-8\\\")\\n\",\n    \"\\n\",\n    \"train_data_x=train_data_x.fillna('')\\n\",\n    \"train_data_y=train_data_y.fillna('')\\n\",\n    \"valid_data_x=valid_data_x.fillna('')\\n\",\n    \"print(\\\"train_data_x:\\\",train_data_x.shape)\\n\",\n    \"print(\\\"train_data_y:\\\",train_data_y.shape)\\n\",\n    \"print(\\\"valid_data_x:\\\",valid_data_x.shape)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 41,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>question_id</th>\\n\",\n       \"      <th>title_char</th>\\n\",\n       \"      <th>title_word</th>\\n\",\n       \"      <th>desc_char</th>\\n\",\n       \"      <th>desc_word</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>0</th>\\n\",\n       \"      <td>6555699376639805223</td>\\n\",\n       \"      <td>c324,c39,c40,c155,c180,c180,c181,c17,c4,c1153,...</td>\\n\",\n       \"      <td>w305,w13549,w22752,w11,w7225,w2565,w1106,w16,w...</td>\\n\",\n       \"      <td>c335,c101,c611,c189,c97,c144,c147,c101,c15,c76...</td>\\n\",\n       \"      <td>w231,w54,w1681,w54,w11506,w5714,w7,w54,w744,w1...</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>1</th>\\n\",\n       \"      <td>2887834264226772863</td>\\n\",\n       \"      <td>c44,c110,c101,c286,c106,c150,c101,c892,c632,c1...</td>\\n\",\n       \"      <td>w377,w54,w285,w57,w349,w54,w108215,w6,w47986,w...</td>\\n\",\n       \"      <td>c1265,c518,c74,c131,c274,c57,c768,c769,c368,c3...</td>\\n\",\n       \"      <td>w12508,w1380,w72,w27045,w276,w111</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>2</th>\\n\",\n       \"      <td>-2687466858632038806</td>\\n\",\n       \"      <td>c15,c768,c769,c1363,c650,c1218,c2361,c11,c90,c...</td>\\n\",\n       \"      <td>w875,w15450,w42394,w15863,w6,w95421,w25,w803,w...</td>\\n\",\n       \"      <td>c693,c100,c279,c99,c189,c532,c101,c189,c145,c1...</td>\\n\",\n       \"      <td>w140340,w54,w48398,w54,w140341,w54,w12856,w54,...</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>3</th>\\n\",\n       \"      <td>-5698296155734268</td>\\n\",\n       \"      <td>c473,c1528,c528,c428,c295,c15,c101,c188,c146,c...</td>\\n\",\n       \"      <td>w8646,w2744,w1462,w9,w54,w138,w54,w50,w110,w14...</td>\\n\",\n       \"      <td></td>\\n\",\n       \"      <td></td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>4</th>\\n\",\n       \"      <td>-6719100304248915192</td>\\n\",\n       \"      <td>c190,c147,c105,c219,c220,c101,c647,c219,c220,c...</td>\\n\",\n       \"      <td>w380,w54,w674,w133,w54,w134,w614,w54,w929,w307...</td>\\n\",\n       \"      <td>c644,c1212,c253,c199,c431,c452,c424,c207,c2,c1...</td>\\n\",\n       \"      <td>w4821,w1301,w16003,w928,w1961,w2565,w50803,w11...</td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"           question_id                                         title_char  \\\\\\n\",\n       \"0  6555699376639805223  c324,c39,c40,c155,c180,c180,c181,c17,c4,c1153,...   \\n\",\n       \"1  2887834264226772863  c44,c110,c101,c286,c106,c150,c101,c892,c632,c1...   \\n\",\n       \"2 -2687466858632038806  c15,c768,c769,c1363,c650,c1218,c2361,c11,c90,c...   \\n\",\n       \"3    -5698296155734268  c473,c1528,c528,c428,c295,c15,c101,c188,c146,c...   \\n\",\n       \"4 -6719100304248915192  c190,c147,c105,c219,c220,c101,c647,c219,c220,c...   \\n\",\n       \"\\n\",\n       \"                                          title_word  \\\\\\n\",\n       \"0  w305,w13549,w22752,w11,w7225,w2565,w1106,w16,w...   \\n\",\n       \"1  w377,w54,w285,w57,w349,w54,w108215,w6,w47986,w...   \\n\",\n       \"2  w875,w15450,w42394,w15863,w6,w95421,w25,w803,w...   \\n\",\n       \"3  w8646,w2744,w1462,w9,w54,w138,w54,w50,w110,w14...   \\n\",\n       \"4  w380,w54,w674,w133,w54,w134,w614,w54,w929,w307...   \\n\",\n       \"\\n\",\n       \"                                           desc_char  \\\\\\n\",\n       \"0  c335,c101,c611,c189,c97,c144,c147,c101,c15,c76...   \\n\",\n       \"1  c1265,c518,c74,c131,c274,c57,c768,c769,c368,c3...   \\n\",\n       \"2  c693,c100,c279,c99,c189,c532,c101,c189,c145,c1...   \\n\",\n       \"3                                                      \\n\",\n       \"4  c644,c1212,c253,c199,c431,c452,c424,c207,c2,c1...   \\n\",\n       \"\\n\",\n       \"                                           desc_word  \\n\",\n       \"0  w231,w54,w1681,w54,w11506,w5714,w7,w54,w744,w1...  \\n\",\n       \"1                  w12508,w1380,w72,w27045,w276,w111  \\n\",\n       \"2  w140340,w54,w48398,w54,w140341,w54,w12856,w54,...  \\n\",\n       \"3                                                     \\n\",\n       \"4  w4821,w1301,w16003,w928,w1961,w2565,w50803,w11...  \"\n      ]\n     },\n     \"execution_count\": 41,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"# understand your data: that's take a look of data\\n\",\n    \"train_data_x.head()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 44,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"('dict_length_columns:', {'desc_char': 117.39879138670524, 'title_char': 22.207077611187056, 'desc_word': 58.272774333851004, 'title_word': 12.841507923253822})\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# compute average length of title_char, title_word, desc_char, desc_word\\n\",\n    \"\\n\",\n    \"dict_length_columns={'title_char':0,'title_word':0,'desc_char':0,'desc_word':0}\\n\",\n    \"num_examples=len(train_data_x)\\n\",\n    \"train_data_x_small=train_data_x.sample(frac=0.01)\\n\",\n    \"for index, row in train_data_x_small.iterrows():\\n\",\n    \"    title_char_length=len(row['title_char'].split(\\\",\\\"))\\n\",\n    \"    title_word_length=len(row['title_word'].split(\\\",\\\"))\\n\",\n    \"    desc_char_length=len(row['desc_char'].split(\\\",\\\"))\\n\",\n    \"    desc_word_length=len(row['desc_word'].split(\\\",\\\"))\\n\",\n    \"    dict_length_columns['title_char']=dict_length_columns['title_char']+title_char_length\\n\",\n    \"    dict_length_columns['title_word']=dict_length_columns['title_word']+title_word_length\\n\",\n    \"    dict_length_columns['desc_char']=dict_length_columns['desc_char']+desc_char_length\\n\",\n    \"    dict_length_columns['desc_word']=dict_length_columns['desc_word']+desc_word_length\\n\",\n    \"dict_length_columns={k:float(v)/float(num_examples*0.01) for k,v in dict_length_columns.items()}\\n\",\n    \"print(\\\"dict_length_columns:\\\",dict_length_columns)\\n\",\n    \"\\n\",\n    \"# averaged length of a input. total length of all information(words,character of title+desc): 210.\\n\",\n    \"# word of title+desc: 71\\n\",\n    \"# character of title+desc: 139\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 42,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>question_id</th>\\n\",\n       \"      <th>topic_ids</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>0</th>\\n\",\n       \"      <td>6555699376639805223</td>\\n\",\n       \"      <td>7739004195693774975,3738968195649774859</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>1</th>\\n\",\n       \"      <td>2887834264226772863</td>\\n\",\n       \"      <td>-3149765934180654494</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>2</th>\\n\",\n       \"      <td>-2687466858632038806</td>\\n\",\n       \"      <td>-760432988437306018</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>3</th>\\n\",\n       \"      <td>-5698296155734268</td>\\n\",\n       \"      <td>-6758942141122113907,3195914392210930723</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>4</th>\\n\",\n       \"      <td>-6719100304248915192</td>\\n\",\n       \"      <td>3804601920633030746,4797226510592237555,435133...</td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"           question_id                                          topic_ids\\n\",\n       \"0  6555699376639805223            7739004195693774975,3738968195649774859\\n\",\n       \"1  2887834264226772863                               -3149765934180654494\\n\",\n       \"2 -2687466858632038806                                -760432988437306018\\n\",\n       \"3    -5698296155734268           -6758942141122113907,3195914392210930723\\n\",\n       \"4 -6719100304248915192  3804601920633030746,4797226510592237555,435133...\"\n      ]\n     },\n     \"execution_count\": 42,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"train_data_y.head()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 53,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"('average_num_labels:', 2.3440333333333334)\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# average labels for a input\\n\",\n    \"train_data_y_small=train_data_y.sample(frac=0.01)\\n\",\n    \"num_examples=len(train_data_y_small)\\n\",\n    \"num_labels=0\\n\",\n    \"for index, row in train_data_y_small.iterrows():\\n\",\n    \"    topic_ids=row['topic_ids']\\n\",\n    \"    topic_id_list=topic_ids.split(\\\",\\\")\\n\",\n    \"    num_labels+=len(topic_id_list)\\n\",\n    \"average_num_labels=float(num_labels)/float(num_examples)\\n\",\n    \"print(\\\"average_num_labels:\\\",average_num_labels)\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 43,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>question_id</th>\\n\",\n       \"      <th>title_char</th>\\n\",\n       \"      <th>title_word</th>\\n\",\n       \"      <th>desc_char</th>\\n\",\n       \"      <th>desc_word</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>0</th>\\n\",\n       \"      <td>6215603645409872328</td>\\n\",\n       \"      <td>c924,c531,c102,c284,c188,c104,c98,c107,c11,c11...</td>\\n\",\n       \"      <td>w1340,w1341,w55,w1344,w58,w6,w24178,w26959,w47...</td>\\n\",\n       \"      <td>c1128,c529,c636,c572,c1321,c139,c540,c223,c510...</td>\\n\",\n       \"      <td>w4094,w1618,w20104,w19234,w1097,w1005,w4228,w2...</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>1</th>\\n\",\n       \"      <td>6649324930261961840</td>\\n\",\n       \"      <td>c346,c1549,c413,c294,c675,c504,c183,c74,c541,c...</td>\\n\",\n       \"      <td>w40132,w1357,w1556,w1380,w2464,w33,w16791,w109...</td>\\n\",\n       \"      <td></td>\\n\",\n       \"      <td></td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>2</th>\\n\",\n       \"      <td>-4251899610700378615</td>\\n\",\n       \"      <td>c96,c97,c97,c98,c99,c100,c101,c141,c42,c42,c10...</td>\\n\",\n       \"      <td>w53,w54,w1779,w54,w1309,w54,w369,w949,w65587,w...</td>\\n\",\n       \"      <td>c149,c148,c148,c42,c185,c95,c95,c186,c186,c186...</td>\\n\",\n       \"      <td></td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>3</th>\\n\",\n       \"      <td>6213817087034420233</td>\\n\",\n       \"      <td>c504,c157,c221,c221,c633,c468,c469,c1637,c1072...</td>\\n\",\n       \"      <td>w5083,w12537,w10427,w29724,w6,w2566,w11,w18476...</td>\\n\",\n       \"      <td>c15,c131,c39,c40,c85,c166,c969,c2456,c17,c636,...</td>\\n\",\n       \"      <td>w2550,w24,w239,w98,w19456,w11,w108710,w3483,w2...</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>4</th>\\n\",\n       \"      <td>-8930652370334418373</td>\\n\",\n       \"      <td>c0,c310,c35,c122,c123,c11,c317,c91,c175,c476,c...</td>\\n\",\n       \"      <td>w33792,w21,w83,w6,w21542,w21,w140670,w25,w1110...</td>\\n\",\n       \"      <td></td>\\n\",\n       \"      <td></td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"           question_id                                         title_char  \\\\\\n\",\n       \"0  6215603645409872328  c924,c531,c102,c284,c188,c104,c98,c107,c11,c11...   \\n\",\n       \"1  6649324930261961840  c346,c1549,c413,c294,c675,c504,c183,c74,c541,c...   \\n\",\n       \"2 -4251899610700378615  c96,c97,c97,c98,c99,c100,c101,c141,c42,c42,c10...   \\n\",\n       \"3  6213817087034420233  c504,c157,c221,c221,c633,c468,c469,c1637,c1072...   \\n\",\n       \"4 -8930652370334418373  c0,c310,c35,c122,c123,c11,c317,c91,c175,c476,c...   \\n\",\n       \"\\n\",\n       \"                                          title_word  \\\\\\n\",\n       \"0  w1340,w1341,w55,w1344,w58,w6,w24178,w26959,w47...   \\n\",\n       \"1  w40132,w1357,w1556,w1380,w2464,w33,w16791,w109...   \\n\",\n       \"2  w53,w54,w1779,w54,w1309,w54,w369,w949,w65587,w...   \\n\",\n       \"3  w5083,w12537,w10427,w29724,w6,w2566,w11,w18476...   \\n\",\n       \"4  w33792,w21,w83,w6,w21542,w21,w140670,w25,w1110...   \\n\",\n       \"\\n\",\n       \"                                           desc_char  \\\\\\n\",\n       \"0  c1128,c529,c636,c572,c1321,c139,c540,c223,c510...   \\n\",\n       \"1                                                      \\n\",\n       \"2  c149,c148,c148,c42,c185,c95,c95,c186,c186,c186...   \\n\",\n       \"3  c15,c131,c39,c40,c85,c166,c969,c2456,c17,c636,...   \\n\",\n       \"4                                                      \\n\",\n       \"\\n\",\n       \"                                           desc_word  \\n\",\n       \"0  w4094,w1618,w20104,w19234,w1097,w1005,w4228,w2...  \\n\",\n       \"1                                                     \\n\",\n       \"2                                                     \\n\",\n       \"3  w2550,w24,w239,w98,w19456,w11,w108710,w3483,w2...  \\n\",\n       \"4                                                     \"\n      ]\n     },\n     \"execution_count\": 43,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"valid_data_x.head()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 44,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>question_id</th>\\n\",\n       \"      <th>topic_ids</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>0</th>\\n\",\n       \"      <td>6555699376639805223</td>\\n\",\n       \"      <td>7739004195693774975,3738968195649774859</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>1</th>\\n\",\n       \"      <td>2887834264226772863</td>\\n\",\n       \"      <td>-3149765934180654494</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>2</th>\\n\",\n       \"      <td>-2687466858632038806</td>\\n\",\n       \"      <td>-760432988437306018</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>3</th>\\n\",\n       \"      <td>-5698296155734268</td>\\n\",\n       \"      <td>-6758942141122113907,3195914392210930723</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>4</th>\\n\",\n       \"      <td>-6719100304248915192</td>\\n\",\n       \"      <td>3804601920633030746,4797226510592237555,435133...</td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"           question_id                                          topic_ids\\n\",\n       \"0  6555699376639805223            7739004195693774975,3738968195649774859\\n\",\n       \"1  2887834264226772863                               -3149765934180654494\\n\",\n       \"2 -2687466858632038806                                -760432988437306018\\n\",\n       \"3    -5698296155734268           -6758942141122113907,3195914392210930723\\n\",\n       \"4 -6719100304248915192  3804601920633030746,4797226510592237555,435133...\"\n      ]\n     },\n     \"execution_count\": 44,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"train_data_y.head()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 45,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>question_id</th>\\n\",\n       \"      <th>title_char</th>\\n\",\n       \"      <th>title_word</th>\\n\",\n       \"      <th>desc_char</th>\\n\",\n       \"      <th>desc_word</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>0</th>\\n\",\n       \"      <td>6215603645409872328</td>\\n\",\n       \"      <td>c924,c531,c102,c284,c188,c104,c98,c107,c11,c11...</td>\\n\",\n       \"      <td>w1340,w1341,w55,w1344,w58,w6,w24178,w26959,w47...</td>\\n\",\n       \"      <td>c1128,c529,c636,c572,c1321,c139,c540,c223,c510...</td>\\n\",\n       \"      <td>w4094,w1618,w20104,w19234,w1097,w1005,w4228,w2...</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>1</th>\\n\",\n       \"      <td>6649324930261961840</td>\\n\",\n       \"      <td>c346,c1549,c413,c294,c675,c504,c183,c74,c541,c...</td>\\n\",\n       \"      <td>w40132,w1357,w1556,w1380,w2464,w33,w16791,w109...</td>\\n\",\n       \"      <td></td>\\n\",\n       \"      <td></td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>2</th>\\n\",\n       \"      <td>-4251899610700378615</td>\\n\",\n       \"      <td>c96,c97,c97,c98,c99,c100,c101,c141,c42,c42,c10...</td>\\n\",\n       \"      <td>w53,w54,w1779,w54,w1309,w54,w369,w949,w65587,w...</td>\\n\",\n       \"      <td>c149,c148,c148,c42,c185,c95,c95,c186,c186,c186...</td>\\n\",\n       \"      <td></td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>3</th>\\n\",\n       \"      <td>6213817087034420233</td>\\n\",\n       \"      <td>c504,c157,c221,c221,c633,c468,c469,c1637,c1072...</td>\\n\",\n       \"      <td>w5083,w12537,w10427,w29724,w6,w2566,w11,w18476...</td>\\n\",\n       \"      <td>c15,c131,c39,c40,c85,c166,c969,c2456,c17,c636,...</td>\\n\",\n       \"      <td>w2550,w24,w239,w98,w19456,w11,w108710,w3483,w2...</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>4</th>\\n\",\n       \"      <td>-8930652370334418373</td>\\n\",\n       \"      <td>c0,c310,c35,c122,c123,c11,c317,c91,c175,c476,c...</td>\\n\",\n       \"      <td>w33792,w21,w83,w6,w21542,w21,w140670,w25,w1110...</td>\\n\",\n       \"      <td></td>\\n\",\n       \"      <td></td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"           question_id                                         title_char  \\\\\\n\",\n       \"0  6215603645409872328  c924,c531,c102,c284,c188,c104,c98,c107,c11,c11...   \\n\",\n       \"1  6649324930261961840  c346,c1549,c413,c294,c675,c504,c183,c74,c541,c...   \\n\",\n       \"2 -4251899610700378615  c96,c97,c97,c98,c99,c100,c101,c141,c42,c42,c10...   \\n\",\n       \"3  6213817087034420233  c504,c157,c221,c221,c633,c468,c469,c1637,c1072...   \\n\",\n       \"4 -8930652370334418373  c0,c310,c35,c122,c123,c11,c317,c91,c175,c476,c...   \\n\",\n       \"\\n\",\n       \"                                          title_word  \\\\\\n\",\n       \"0  w1340,w1341,w55,w1344,w58,w6,w24178,w26959,w47...   \\n\",\n       \"1  w40132,w1357,w1556,w1380,w2464,w33,w16791,w109...   \\n\",\n       \"2  w53,w54,w1779,w54,w1309,w54,w369,w949,w65587,w...   \\n\",\n       \"3  w5083,w12537,w10427,w29724,w6,w2566,w11,w18476...   \\n\",\n       \"4  w33792,w21,w83,w6,w21542,w21,w140670,w25,w1110...   \\n\",\n       \"\\n\",\n       \"                                           desc_char  \\\\\\n\",\n       \"0  c1128,c529,c636,c572,c1321,c139,c540,c223,c510...   \\n\",\n       \"1                                                      \\n\",\n       \"2  c149,c148,c148,c42,c185,c95,c95,c186,c186,c186...   \\n\",\n       \"3  c15,c131,c39,c40,c85,c166,c969,c2456,c17,c636,...   \\n\",\n       \"4                                                      \\n\",\n       \"\\n\",\n       \"                                           desc_word  \\n\",\n       \"0  w4094,w1618,w20104,w19234,w1097,w1005,w4228,w2...  \\n\",\n       \"1                                                     \\n\",\n       \"2                                                     \\n\",\n       \"3  w2550,w24,w239,w98,w19456,w11,w108710,w3483,w2...  \\n\",\n       \"4                                                     \"\n      ]\n     },\n     \"execution_count\": 45,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"valid_data_x.head()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 46,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"(0, 'PAD')\\n\",\n      \"(1, 'UNK')\\n\",\n      \"(2, 'CLS')\\n\",\n      \"(3, 'SEP')\\n\",\n      \"(4, 'unused1')\\n\",\n      \"(5, 'unused2')\\n\",\n      \"(6, 'unused3')\\n\",\n      \"(7, 'unused4')\\n\",\n      \"(8, 'unused5')\\n\",\n      \"(9, '</s>')\\n\",\n      \"vocabulary of char generated....\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \" # create vocabulary_dict, label_dict, generate training/validation data, and save to some place \\n\",\n    \"    \\n\",\n    \" # create vocabulary of charactor token by read word_embedding.txt \\n\",\n    \"word_embedding_object=open(base_path+'unused_current/char_embedding.txt')\\n\",\n    \"lines_wv=word_embedding_object.readlines()\\n\",\n    \"word_embedding_object.close()\\n\",\n    \"char_list=[]\\n\",\n    \"char_list.extend(['PAD','UNK','CLS','SEP','unused1','unused2','unused3','unused4','unused5'])\\n\",\n    \"PAD_ID=0\\n\",\n    \"UNK_ID=1\\n\",\n    \"for i, line in enumerate(lines_wv):\\n\",\n    \"    if i==0: continue\\n\",\n    \"    char_embedding_list=line.split(\\\" \\\")\\n\",\n    \"    char_token=char_embedding_list[0]\\n\",\n    \"    char_list.append(char_token)    \\n\",\n    \"    \\n\",\n    \"# write to vocab.txt under data/ieee_zhihu_cup\\n\",\n    \"vocab_path=base_path+'vocab.txt'\\n\",\n    \"vocab_char_object=open(vocab_path,'w')\\n\",\n    \"\\n\",\n    \"word2index={}\\n\",\n    \"for i, char in enumerate(char_list):\\n\",\n    \"    if i<10:print(i,char)\\n\",\n    \"    word2index[char]=i\\n\",\n    \"    vocab_char_object.write(char+\\\"\\\\n\\\")\\n\",\n    \"vocab_char_object.close()\\n\",\n    \"print(\\\"vocabulary of char generated....\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 47,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"(u'7476760589625268543', 2308)\\n\",\n      \"(u'4697014490911193675', 1746)\\n\",\n      \"(u'-4653836020042332281', 1579)\\n\",\n      \"(u'-8175048003539471998', 1475)\\n\",\n      \"(u'-8377411942628634656', 1382)\\n\",\n      \"(u'-7046289575185911002', 1338)\\n\",\n      \"(u'-5932391056759866388', 1283)\\n\",\n      \"(u'2787171473654490487', 1145)\\n\",\n      \"(u'-7129272008741138808', 1085)\\n\",\n      \"(u'2587540952280802350', 1079)\\n\",\n      \"(u'-4931965624608608932', 1079)\\n\",\n      \"(u'-6748914495015758455', 1049)\\n\",\n      \"(u'-5513826101327857645', 993)\\n\",\n      \"(u'2347973810368732059', 970)\\n\",\n      \"(u'9069451131871918127', 958)\\n\",\n      \"(u'-8132909213241034354', 904)\\n\",\n      \"(u'-3517637179126242000', 867)\\n\",\n      \"(u'-5872443091340192918', 834)\\n\",\n      \"(u'-3522198575349379632', 830)\\n\",\n      \"(u'1127459907694805235', 829)\\n\",\n      \"generate label dict successful...\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \" # generate labels list, and save to file system \\n\",\n    \"c_labels=Counter()\\n\",\n    \"train_data_y_small=train_data_y[0:100000]#.sample(frac=0.1)\\n\",\n    \"for index, row in train_data_y_small.iterrows():\\n\",\n    \"    topic_ids=row['topic_ids']\\n\",\n    \"    topic_list=topic_ids.split(',')\\n\",\n    \"    c_labels.update(topic_list)\\n\",\n    \"\\n\",\n    \"label_list=c_labels.most_common()\\n\",\n    \"label2index={}\\n\",\n    \"label_target_object=open(base_path+'label_set.txt','w')\\n\",\n    \"for i, label_freq in enumerate(label_list):\\n\",\n    \"    label,freq=label_freq\\n\",\n    \"    label2index[label]=i\\n\",\n    \"    label_target_object.write(label+\\\"\\\\n\\\")\\n\",\n    \"    if i<20: print(label,freq)\\n\",\n    \"label_target_object.close()\\n\",\n    \"print(\\\"generate label dict successful...\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 48,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"('label_list_sparse:', array([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\\n\",\n      \"       0., 0., 0.]))\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"def transform_multilabel_as_multihot(label_list,label_size):\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    convert to multi-hot style\\n\",\n    \"    :param label_list: e.g.[0,1,4], here 4 means in the 4th position it is true value(as indicate by'1')\\n\",\n    \"    :param label_size: e.g.199\\n\",\n    \"    :return:e.g.[1,1,0,1,0,0,........]\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    result=np.zeros(label_size)\\n\",\n    \"    #set those location as 1, all else place as 0.\\n\",\n    \"    result[label_list] = 1\\n\",\n    \"    return result\\n\",\n    \"\\n\",\n    \"label_list=[0,1,2,10]\\n\",\n    \"label_size=20\\n\",\n    \"label_list_sparse=transform_multilabel_as_multihot(label_list,label_size)\\n\",\n    \"print(\\\"label_list_sparse:\\\",label_list_sparse)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 49,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"\\n\",\n    \"def get_X_Y(train_data_x,train_data_y,label_size, test_mode=False):\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    get X and Y given input and labels\\n\",\n    \"    input:\\n\",\n    \"    train_data_x:\\n\",\n    \"    train_data_y:\\n\",\n    \"    label_size: number of total unique labels(e.g. 1999 in this task)\\n\",\n    \"    output:\\n\",\n    \"    X,Y\\n\",\n    \"    \\\"\\\"\\\"\\n\",\n    \"    X=[]\\n\",\n    \"    Y=[]\\n\",\n    \"    if test_mode:\\n\",\n    \"        train_data_x_tiny_test=train_data_x[0:1000] # todo todo todo todo todo todo todo todo todo todo todo todo \\n\",\n    \"        train_data_y_tiny_test=train_data_y[0:1000] # todo todo todo todo todo todo todo todo todo todo todo todo \\n\",\n    \"    else:\\n\",\n    \"        train_data_x_tiny_test=train_data_x\\n\",\n    \"        train_data_y_tiny_test=train_data_y\\n\",\n    \"\\n\",\n    \"    for index, row in train_data_x_tiny_test.iterrows():\\n\",\n    \"        if index==0: continue\\n\",\n    \"        # get character of title and dssc\\n\",\n    \"        title_char=row['title_char']\\n\",\n    \"        desc_char=row['desc_char']\\n\",\n    \"        # split into list\\n\",\n    \"        title_char_list=title_char.split(',')\\n\",\n    \"        desc_char_list=desc_char.split(\\\",\\\")\\n\",\n    \"        # transform to indices\\n\",\n    \"        title_char_id_list=[vocabulary_word2index.get(x,UNK_ID) for x in title_char_list if x.strip()]\\n\",\n    \"        desc_char_id_list=[vocabulary_word2index.get(x,UNK_ID) for x in desc_char_list if x.strip()]\\n\",\n    \"        # merge title and desc: in the middle is special token 'SEP'\\n\",\n    \"        title_char_id_list.append(vocabulary_word2index['SEP'])\\n\",\n    \"        title_char_id_list.extend(desc_char_id_list)\\n\",\n    \"        X.append(title_char_id_list)\\n\",\n    \"        if index<3: print(index,title_char_id_list)\\n\",\n    \"        if index%100000==0: print(index,title_char_id_list)\\n\",\n    \"\\n\",\n    \"    for index, row in train_data_y_tiny_test.iterrows():\\n\",\n    \"        if index==0: continue\\n\",\n    \"        topic_ids=row['topic_ids']\\n\",\n    \"        topic_id_list=topic_ids.split(\\\",\\\")\\n\",\n    \"        label_list_dense=[label2index[l] for l in topic_id_list if l.strip()]\\n\",\n    \"        label_list_sparse=transform_multilabel_as_multihot(label_list_dense,label_size)\\n\",\n    \"        Y.append(label_list_sparse)\\n\",\n    \"        if index%100000==0: print(index,\\\";label_list_dense:\\\",label_list_dense)\\n\",\n    \"\\n\",\n    \"    return X,Y\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 50,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/plain\": [\n       \"3\"\n      ]\n     },\n     \"execution_count\": 50,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"vocabulary_word2index['SEP']\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 51,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def save_data(cache_file_h5py,cache_file_pickle,word2index,label2index,train_X,train_Y,vaild_X,valid_Y,test_X,test_Y):\\n\",\n    \"    # train/valid/test data using h5py\\n\",\n    \"    f = h5py.File(cache_file_h5py, 'w')\\n\",\n    \"    f['train_X'] = train_X\\n\",\n    \"    f['train_Y'] = train_Y\\n\",\n    \"    f['vaild_X'] = vaild_X\\n\",\n    \"    f['valid_Y'] = valid_Y\\n\",\n    \"    f['test_X'] = test_X\\n\",\n    \"    f['test_Y'] = test_Y\\n\",\n    \"    f.close()\\n\",\n    \"    # save word2index, label2index\\n\",\n    \"    with open(cache_file_pickle, 'ab') as target_file:\\n\",\n    \"        pickle.dump((word2index,label2index), target_file)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 52,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"(1, [110, 143, 11, 31, 35, 28, 11, 522, 1392, 197, 667, 12, 194, 915, 1611, 509, 58, 67, 33, 15, 60, 64, 84, 1417, 648, 268, 66, 143, 109, 16, 3, 543, 96, 64, 26, 73, 19, 67, 33, 363, 601, 16])\\n\",\n      \"(2, [58, 67, 33, 2152, 562, 1354, 822, 12, 137, 1690, 165, 13, 134, 95, 93, 12, 356, 529, 43, 119, 16, 3, 624, 24, 91, 120, 106, 203, 11, 106, 52, 106, 14, 120, 120, 359, 11, 55, 24, 14, 401, 52, 11, 14, 21, 11, 37, 11, 90, 57, 83, 21, 36, 52, 11, 14, 83, 34, 11, 52, 21, 57, 55, 52, 11, 76, 359, 11, 20, 28, 11, 3662, 11, 20, 27, 11, 90, 57, 83, 21, 36, 52, 11, 345, 742, 84, 669, 239, 36, 21, 21, 55, 185, 38, 38, 39, 39, 39, 35, 79, 14, 52, 21, 46, 57, 401, 30, 34, 52, 35, 57, 46, 79, 38, 42, 57, 83, 21, 24, 83, 21, 38, 28, 28, 38, 24, 83, 38, 624, 24, 91, 120, 106, 203, 431, 28, 23, 906, 431, 28, 23, 310, 426, 624, 455, 38, 624, 24, 91, 120, 106, 203, 431, 28, 23, 906, 431, 28, 23, 310, 426, 624, 455, 431, 28, 23, 30, 83, 431, 28, 23, 318, 83, 91, 14, 83, 21, 52, 35, 36, 21, 90, 120])\\n\",\n      \"(3, [260, 715, 587, 235, 99, 58, 11, 30, 331, 36, 57, 83, 24, 11, 289, 647, 131, 145, 223, 1140, 342, 299, 16, 1218, 209, 1480, 326, 647, 241, 430, 12, 191, 155, 137, 136, 16, 3])\\n\",\n      \"(4, [90, 14, 42, 402, 357, 11, 438, 402, 357, 11, 440, 435, 11, 294, 460, 47, 19, 131, 145, 575, 124, 396, 564, 12, 265, 64, 651, 299, 16, 3, 495, 394, 351, 865, 93, 175, 60, 300, 18, 145, 468, 921, 10, 600, 67, 33, 64, 12, 11, 111, 216, 1628, 239, 86, 14, 11, 36, 46, 24, 91, 50, 25, 36, 21, 21, 55, 185, 38, 38, 39, 39, 39, 35, 251, 36, 30, 36, 106, 35, 42, 57, 90, 38, 244, 106, 24, 52, 21, 30, 57, 83, 38, 28, 28, 20, 23, 49, 37, 37, 37, 25, 11, 42, 120, 14, 52, 52, 50, 25, 30, 83, 21, 24, 46, 83, 14, 120, 25, 85, 411, 30, 83, 34, 57, 39, 11, 294, 460, 19, 131, 145, 575, 124, 396, 564, 12, 265, 64, 651, 299, 16, 86, 38, 14, 85])\\n\",\n      \"(100000, [131, 43, 715, 1137, 12, 227, 95, 147, 533, 1672, 16, 3, 155, 41, 273, 760, 1243, 1137, 126, 10, 349, 230, 160, 274, 267, 531, 121, 715, 1137, 10, 207, 74, 97, 26, 232, 18, 118, 131, 43, 715, 1137, 1585, 227, 147, 92, 16])\\n\",\n      \"(200000, [18, 109, 243, 199, 308, 96, 138, 263, 229, 351, 398, 134, 351, 16, 3, 18, 123, 243, 12, 199, 308, 11, 96, 138, 28, 62, 285, 263, 229, 351, 398, 191, 188, 309, 19, 351, 12, 199, 308, 16])\\n\",\n      \"(300000, [716, 565, 573, 607, 453, 375, 839, 19, 67, 33, 400, 1273, 136, 16, 3, 103, 141, 369, 504, 573, 607, 10, 1377, 95, 375, 839, 96, 209, 19, 573, 607, 71, 533, 15, 92, 99, 60, 453, 134, 84, 597, 11, 168, 13, 19, 145, 573, 607, 114, 11, 1172, 843, 573, 10, 19, 145, 11, 667, 573, 11, 573, 607, 792, 277, 11, 114, 573, 104, 268, 694, 375, 839, 659, 851, 15, 18, 102, 10, 11, 19, 67, 33, 400, 1273, 89, 609, 136, 16, 11, 67, 33, 80, 181, 114, 1172, 843, 573, 16, 573, 607, 94, 102, 792, 277, 11, 114, 268, 694, 12, 573, 16, 11, 56, 92, 336, 99, 60, 716, 565, 573, 607, 453, 134, 84, 11, 201, 336, 12, 11, 15, 659, 985, 12, 11, 192, 94, 33, 245, 16])\\n\",\n      \"(400000, [32, 390, 457, 252, 12, 289, 159, 175, 449, 115, 361, 110, 19, 131, 145, 265, 410, 12, 121, 177, 16, 3])\\n\",\n      \"(500000, [258, 121, 32, 135, 1095, 238, 118, 12, 41, 264, 343, 198, 376, 436, 89, 74, 505, 53, 194, 156, 517, 64, 136, 16, 3])\\n\",\n      \"(600000, [96, 138, 547, 372, 18, 43, 1294, 66, 142, 12, 65, 938, 16, 3])\\n\",\n      \"(700000, [58, 67, 33, 354, 1494, 158, 292, 113, 56, 342, 173, 593, 84, 41, 99, 13, 878, 212, 235, 1113, 301, 764, 16, 3, 15, 917, 13, 132, 363, 10, 252, 228, 310, 113, 175, 12, 71, 13, 10, 63, 337, 13, 315, 51, 61, 113, 99, 174, 265, 89, 74, 117, 53, 593, 84, 41, 99, 13, 878, 212, 235, 1113, 301, 764, 12, 623, 599, 22, 11, 268, 66, 262, 617, 170, 743, 1187, 743, 12, 479, 320, 92, 26, 10, 54, 113, 63, 327, 43, 92, 182, 252, 856, 356, 18, 800, 12, 479, 158, 775, 144, 11, 86, 30, 90, 79, 11, 52, 46, 42, 50, 25, 14, 20, 91, 23, 37, 40, 31, 44, 76, 20, 14, 40, 34, 37, 34, 40, 14, 34, 31, 24, 37, 91, 44, 76, 44, 20, 27, 14, 20, 20, 28, 23, 25, 11, 34, 14, 21, 14, 45, 46, 14, 39, 39, 30, 34, 21, 36, 50, 25, 40, 28, 29, 25, 11, 34, 14, 21, 14, 45, 46, 14, 39, 36, 24, 30, 79, 36, 21, 50, 25, 49, 29, 44, 25, 85])\\n\",\n      \"(800000, [40, 62, 12, 589, 298, 10, 192, 15, 192, 173, 142, 16, 3])\\n\",\n      \"(900000, [590, 605, 19, 92, 201, 278, 1101, 550, 1964, 142, 16, 110, 334, 1104, 314, 1211, 92, 201, 16, 467, 110, 334, 1104, 260, 143, 12, 1964, 142, 110, 19, 1101, 550, 1964, 142, 33, 16, 3])\\n\",\n      \"(1000000, [476, 1232, 423, 341, 159, 38, 317, 1131, 96, 138, 32, 635, 84, 339, 1012, 16, 3, 86, 36, 20, 85, 11, 17, 97, 453, 476, 1232, 423, 341, 125, 13, 137, 336, 403, 157, 87, 1569, 87, 1569, 187, 187, 187, 86, 38, 36, 20, 85, 11, 17, 109, 41, 99, 501, 116, 61, 54, 189, 129, 59, 159, 1131, 10, 63, 168, 13, 172, 221, 41, 743, 743, 1631, 144, 10, 71, 59, 41, 579, 56, 317, 258, 154, 423, 341, 1631, 144, 22, 22, 22, 22, 22, 11, 214, 395, 100, 32, 156, 151, 33, 201, 151, 33, 999, 10, 328, 15, 89, 60, 19, 41, 18, 112, 476, 1631, 144, 22, 22, 22, 22, 11, 105, 61, 54, 78, 453, 54, 629, 469, 64, 87, 56, 80, 123, 293, 71, 87, 731, 264, 413, 453, 10, 134, 84, 53, 282, 59, 159, 1131, 71, 59, 317, 1131, 22, 22, 22, 22, 11, 66, 108, 99, 476, 674, 17, 100, 149, 171, 453, 318, 215, 271, 73, 341, 333, 22, 22, 22, 22, 22, 89, 13, 54, 272, 102, 17, 249, 133, 59, 646, 813, 10, 73, 261, 17, 328, 56, 731, 264, 22, 22, 22, 22])\\n\",\n      \"(1100000, [56, 284, 427, 129, 11, 270, 1516, 11, 238, 962, 389, 16, 3])\\n\",\n      \"(1200000, [58, 138, 126, 658, 1665, 144, 56, 92, 15, 64, 1592, 507, 16, 3])\\n\",\n      \"(1300000, [47, 26, 56, 75, 10, 163, 140, 59, 19, 274, 267, 12, 41, 68, 3, 17, 348, 174, 464, 15, 557, 274, 267, 18, 43, 41, 13, 67, 33, 163, 140, 26, 10, 168, 464, 124, 621, 110, 12, 80, 181, 274, 267, 115, 17, 108, 535, 113, 18, 43, 126, 95, 10, 151, 71, 13, 263, 621, 343, 107, 248, 12, 10, 621, 18, 356, 18, 545, 410, 53, 146, 140, 124, 146, 377, 344, 10, 53, 71, 19, 171, 15, 274, 267, 146, 10, 140, 124, 19, 171, 137, 877, 10, 15, 127, 148, 263, 67, 33, 80, 181, 63, 107, 248, 274, 267, 146, 26, 10, 151, 80, 181, 1713, 15, 124, 80, 80, 948, 948, 104, 146, 32, 18, 220, 10, 270, 184, 54, 13, 18, 471, 1181, 589, 22, 11, 137, 110, 12, 80, 181, 19, 171, 103, 1539, 1301, 790, 10, 17, 71, 15, 127, 148, 17, 137, 110, 12, 80, 181, 53, 487, 19, 59, 19, 274, 267, 115, 616, 10, 19, 115, 151, 33, 286, 43, 126, 95, 104, 17, 223, 283, 65, 10, 17, 63, 116, 146, 108, 65, 10, 370, 95, 26, 65, 163, 10, 19, 80, 17, 140, 124, 17, 15, 13, 274, 267, 146, 108, 12, 10, 125, 13, 282, 97, 104, 146, 108, 32, 18, 220, 10, 100, 32, 56, 75, 26, 10, 17, 231, 97, 10, 56, 701, 270, 80, 17, 320, 1901, 51, 1301, 790, 10, 168, 97, 19, 43, 41, 1203, 1196, 606, 1576, 10, 56, 701, 15, 13, 164, 12, 274, 267, 425, 22, 11, 100, 32, 17, 32, 18, 207, 281, 354, 330, 56, 75, 227, 126, 188, 260, 188, 1066, 10, 17, 108, 535, 227, 126, 188, 18, 188, 529, 10, 19, 87, 92, 210, 75, 293, 124, 377, 344, 12, 10, 17, 71, 19, 115, 286, 246, 18, 410, 886, 186, 12, 151, 183, 150, 170, 10, 125, 13, 200, 93, 282, 59, 26, 54, 183, 163, 140, 10, 22, 17, 164, 12, 15, 127, 148, 10, 17, 97, 660, 18, 471, 589, 298, 10, 125, 13, 164, 12, 348, 174, 59, 19, 26, 621, 110, 151, 183, 150, 170, 12, 163, 140, 26, 22, 11, 17, 192, 94, 33, 245, 273])\\n\",\n      \"(1400000, [137, 260, 238, 95, 10, 116, 196, 110, 493, 56, 75, 130, 325, 428, 56, 75, 130, 929, 861, 56, 75, 130, 690, 110, 354, 400, 56, 75, 130, 339, 205, 349, 304, 56, 75, 130, 1467, 162, 56, 75, 192, 96, 138, 2869, 563, 16, 3, 155, 41, 58, 508, 313, 1241, 137, 238, 12, 511, 339, 157, 354, 126, 95, 10, 931, 702, 351, 49, 29, 23, 711, 717, 10, 89, 74, 161, 393, 12, 284, 129, 1836, 472, 56, 701, 19, 341, 337, 204, 330, 130, 382, 75, 330, 130, 421, 784, 330, 130, 761, 785, 330, 22, 241, 192, 73, 453, 450, 576, 707, 10, 495, 394, 147, 65, 60, 638, 134, 143, 22, 11, 635, 47, 78, 1598, 572, 998, 753, 10, 1374, 184, 188, 309, 825, 1124, 10, 15, 127, 511, 56, 127, 160, 13, 408, 60, 263, 156, 1078, 381, 204, 672, 130, 374, 640, 75, 640, 130, 75, 389, 636, 235, 1768, 884, 130, 75, 389, 1002, 299, 130, 284, 129, 149, 264, 296, 152, 175, 116, 74, 47, 286, 207, 56, 75, 110, 12, 18, 207, 271, 286, 207, 153, 134, 173, 802, 16, 15, 1365, 163, 901, 22, 11, 746, 10, 116, 196, 157, 354, 126, 95, 12, 284, 129, 327, 563, 10, 56, 109, 19, 67, 33, 476, 674, 136, 16])\\n\",\n      \"(1500000, [96, 138, 322, 60, 32, 417, 530, 88, 53, 126, 230, 160, 16, 3, 348, 174, 28, 44, 26, 10, 490, 129, 361, 927, 2619, 10, 881, 430, 105, 189, 10, 1061, 762, 186, 430, 645, 10, 371, 140, 124, 103, 141, 410, 115, 12, 126, 95, 201, 10, 277, 150, 166, 12, 328, 201, 10, 17, 71, 127, 148, 59, 19, 249, 197, 229, 3729, 10, 125, 13, 105, 13, 163, 140, 126, 95, 19, 271, 92, 271, 201, 987, 515, 103, 141, 487, 480, 12, 880, 171, 10, 17, 192, 94, 33, 245])\\n\"\n     ]\n    },\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"(1600000, [135, 1054, 1612, 53, 12, 325, 339, 10, 430, 109, 15, 114, 737, 10, 924, 527, 524, 81, 61, 168, 60, 892, 536, 10, 54, 337, 15, 337, 1703, 1383, 16, 3, 17, 32, 135, 1054, 47, 1612, 986, 53, 18, 43, 18, 667, 12, 2029, 144, 10, 270, 392, 109, 78, 236, 3782, 1897, 657, 854, 10, 63, 13, 78, 17, 406, 118, 375, 213, 746, 194, 18, 299, 430, 292, 10, 322, 104, 1612, 986, 53, 12, 430, 292, 18, 220, 172, 17, 1691, 115, 84, 22, 54, 13, 408, 277, 195, 10, 17, 192, 96, 138, 937, 698, 16, 868, 612, 61, 18, 118, 10, 924, 527, 12, 241, 116, 204, 421, 164, 12, 15, 189, 10, 1612, 986, 53, 12, 325, 339, 392, 109, 15, 114, 737, 66, 108, 71, 59, 245, 195, 22])\\n\",\n      \"(1700000, [532, 413, 1128, 566, 13, 18, 183, 94, 102, 12, 235, 433, 16, 3, 32, 653, 18, 471, 532, 413, 110, 1128, 566, 10, 32, 532, 413, 268, 123, 130, 268, 93, 74, 515, 231, 1395, 110, 99, 19, 131, 145, 235, 433, 16, 11, 86, 253, 30, 34, 24, 57, 11, 30, 34, 50, 25, 31, 29, 28, 20, 40, 25, 11, 34, 14, 21, 14, 45, 52, 39, 91, 106, 46, 120, 50, 25, 36, 21, 21, 55, 185, 38, 38, 55, 120, 14, 359, 24, 46, 35, 359, 57, 106, 401, 106, 35, 42, 57, 90, 38, 55, 120, 14, 359, 24, 46, 35, 55, 36, 55, 38, 52, 30, 34, 38, 440, 390, 251, 380, 23, 390, 251, 318, 37, 478, 251, 79, 39, 38, 253, 35, 52, 39, 91, 25, 11, 55, 57, 52, 21, 24, 46, 50, 25, 36, 21, 21, 55, 185, 38, 38, 79, 31, 35, 359, 401, 30, 90, 79, 35, 42, 57, 90, 38, 23, 20, 23, 23, 40, 37, 20, 598, 37, 40, 29, 20, 28, 23, 40, 23, 31, 252, 228, 27, 31, 23, 23, 29, 252, 426, 457, 252, 28, 20, 40, 228, 27, 426, 40, 27, 40, 37, 45, 31, 455, 598, 31, 45, 426, 29, 457, 252, 45, 37, 29, 23, 31, 45, 28, 29, 23, 598, 44, 31, 44, 44, 455, 455, 228, 455, 25, 11, 34, 14, 21, 14, 45, 52, 57, 106, 46, 42, 24, 106, 46, 120, 50, 25, 36, 21, 21, 55, 185, 38, 38, 253, 35, 359, 57, 106, 401, 106, 35, 42, 57, 90, 38, 253, 384, 52, 36, 57, 39, 38, 30, 34, 384, 440, 390, 251, 380, 23, 390, 251, 318, 37, 478, 251, 79, 39, 35, 36, 21, 90, 120, 25, 11, 34, 14, 21, 14, 45, 83, 14, 90, 24, 50, 25, 143, 109, 156, 157, 11, 532, 110, 1922, 2236, 11, 860, 2545, 18, 480, 25, 85, 86, 38, 253, 30, 34, 24, 57, 85])\\n\",\n      \"(1800000, [428, 530, 56, 75, 1981, 461, 1044, 2051, 333, 56, 136, 16, 3])\\n\",\n      \"(1900000, [243, 107, 12, 392, 1362, 104, 87, 641, 59, 560, 158, 12, 392, 1362, 291, 188, 131, 43, 328, 525, 580, 220, 644, 16, 3, 19, 18, 43, 260, 150, 51, 392, 10, 87, 641, 59, 1212, 157, 115, 12, 10, 19, 18, 43, 508, 468, 1127, 12, 10, 190, 337, 505, 268, 110, 18, 43, 153, 732, 129, 392, 1362, 10, 291, 188, 200, 118, 10, 131, 43, 328, 525, 580, 19, 220, 526, 16])\\n\",\n      \"(2000000, [31, 23, 546, 10, 227, 10, 18, 180, 229, 191, 10, 671, 129, 153, 171, 894, 51, 95, 166, 65, 16, 3, 227, 10, 56, 75, 427, 129, 668, 62, 10, 18, 180, 229, 191, 10, 153, 115, 657, 854, 10, 71, 59, 1644, 53, 317, 10, 149, 32, 15, 97, 190, 178, 47, 535, 26, 22, 97, 153, 171, 51, 95, 166, 89, 13, 10, 1449, 229, 405, 1263, 10, 19, 59, 19, 41, 494, 171, 18, 118, 22])\\n\",\n      \"(2100000, [96, 209, 17, 434, 62, 1097, 536, 213, 404, 10, 115, 272, 62, 406, 693, 134, 98, 10, 151, 33, 1463, 1058, 12, 744, 189, 1097, 536, 104, 477, 459, 94, 33, 105, 16, 3, 96, 209, 434, 62, 1097, 536, 213, 404, 10, 115, 272, 62, 93, 693, 134, 10, 258, 121, 1463, 1058, 12, 744, 189, 477, 459, 104, 1097, 536, 94, 33, 245, 16])\\n\",\n      \"(2200000, [223, 283, 87, 65, 12, 765, 259, 230, 160, 87, 413, 191, 58, 186, 1736, 136, 16, 3, 19, 18, 765, 259, 454, 160, 10, 89, 74, 725, 291, 1062, 1136, 10, 89, 74, 725, 291, 893, 116, 152, 12, 222, 10, 125, 146, 61, 153, 186, 1736, 163, 140, 533, 18, 171, 10, 58, 894])\\n\",\n      \"(2300000, [357, 359, 46, 30, 83, 203, 38, 495, 4254, 1994, 1599, 94, 33, 102, 16, 3])\\n\",\n      \"(2400000, [856, 410, 104, 930, 930, 153, 298, 26, 10, 54, 295, 265, 136, 16, 3, 32, 294, 80, 104, 930, 930, 12, 223, 283, 63, 87, 65, 10, 105, 266, 221, 41, 1050, 73, 13, 186, 1736, 10, 125, 13, 153, 856, 856, 53, 66, 10, 103, 141, 13, 15, 13, 15, 295, 265, 347, 438, 45, 35, 45, 435])\\n\",\n      \"(2500000, [1453, 758, 12, 614, 345, 75, 104, 174, 729, 75, 497, 12, 174, 729, 330, 284, 129, 63, 129, 186, 498, 96, 138, 16, 3, 155, 41, 155, 354, 453, 12, 13, 174, 729, 75, 284, 129, 10, 75, 124, 511, 282, 15, 583, 10, 450, 608, 95, 97, 572, 238, 1453, 758, 12, 614, 345, 75, 10, 125, 13, 614, 345, 75, 13, 32, 484, 157, 178, 361, 75, 497, 10, 15, 127, 148, 369, 89, 288, 94, 33, 102, 187, 219, 163, 140, 210, 75, 99, 15, 97, 406, 453, 320, 157, 255, 174, 729, 75, 12, 284, 129, 26, 217])\\n\",\n      \"(2600000, [147, 234, 383, 651, 56, 1117, 372, 10, 1071, 1630, 28, 575, 124, 134, 142, 33, 16, 3, 383, 651, 919, 372, 10, 1071, 1630, 28, 100, 32, 168, 78, 28, 23, 23, 23, 840, 10, 1413, 184, 19, 183, 78, 4097, 142, 12, 1100, 170, 11, 1071, 1630, 28, 100, 32, 575, 124, 352, 142, 33, 16])\\n\",\n      \"(2700000, [538, 109, 548, 365, 179, 68, 3, 69, 411, 411, 411, 22, 31, 20, 28, 27, 27, 27, 22, 228, 402, 390, 70])\\n\",\n      \"(2800000, [137, 238, 32, 798, 10, 241, 192, 94, 102, 615, 636, 150, 577, 84, 137, 584, 740, 12, 75, 340, 16, 3, 279, 279, 508, 322, 322, 429, 118, 542, 177, 12, 870, 22, 22, 22, 155, 41, 74, 159, 263, 15, 542, 54, 33, 569, 10, 303, 1270, 186, 498, 214, 348, 11, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 17, 140, 124, 32, 54, 43, 604, 1226, 545, 47, 10, 261, 616, 114, 1716, 933, 264, 99, 60, 153, 53, 10, 125, 13, 17, 140, 124, 100, 32, 12, 150, 577, 78, 615, 636, 65, 10, 15, 184, 73, 1033, 26, 604, 1846, 10, 323, 352, 1039, 177, 12, 1035, 259, 1905, 761, 22, 22, 22, 11, 105, 19, 17, 12, 338, 607, 13, 557, 690, 22, 22, 22, 708, 184, 338, 159, 163, 140, 286, 740, 15, 56, 125, 17, 631, 116, 15, 73, 429, 1007, 54, 43, 856, 97, 22, 11, 168, 13, 15, 97, 202, 58, 15, 65, 12, 150, 577, 261, 103, 141, 93, 1424])\\n\",\n      \"(2900000, [15, 13, 291, 223, 284, 129, 10, 97, 283, 571, 75, 340, 341, 337, 204, 152, 175, 12, 127, 504, 10, 19, 67, 33, 476, 674, 68, 3, 204, 1445, 284, 129, 10, 125, 116, 341, 337, 204, 87, 163, 646, 813, 10, 1002, 299, 776, 821, 792, 513, 15, 13, 121, 177, 10, 476, 115, 411, 57, 46, 34, 55, 46, 24, 52, 52, 790, 663, 677, 524, 10, 107, 242, 179, 392, 10, 153, 115, 51, 12, 42, 90, 55, 37, 200, 330, 12, 51, 651, 299, 10, 274, 267, 1441, 2810, 283, 571, 10, 132, 578, 15, 513, 986, 1142, 651, 299, 10, 1190, 242, 55, 21, 193, 73, 171, 585, 73, 178, 361, 75, 22, 163, 140, 99, 13, 810, 980, 12, 325, 339, 10, 94, 102, 261, 103, 141, 328, 234, 18, 644, 22, 22])\\n\",\n      \"(100000, ';label_list_dense:', [1603, 1237, 826])\\n\",\n      \"(200000, ';label_list_dense:', [946, 19, 180])\\n\",\n      \"(300000, ';label_list_dense:', [734, 217, 36])\\n\",\n      \"(400000, ';label_list_dense:', [1988, 242])\\n\",\n      \"(500000, ';label_list_dense:', [1067])\\n\",\n      \"(600000, ';label_list_dense:', [659, 1215, 491, 15])\\n\",\n      \"(700000, ';label_list_dense:', [756, 1422, 380, 196, 848])\\n\",\n      \"(800000, ';label_list_dense:', [414, 201, 30, 4, 163])\\n\",\n      \"(900000, ';label_list_dense:', [873])\\n\",\n      \"(1000000, ';label_list_dense:', [271, 448, 40, 557])\\n\",\n      \"(1100000, ';label_list_dense:', [1176])\\n\",\n      \"(1200000, ';label_list_dense:', [1241])\\n\",\n      \"(1300000, ';label_list_dense:', [567, 4, 12, 2])\\n\",\n      \"(1400000, ';label_list_dense:', [526, 934, 12])\\n\",\n      \"(1500000, ';label_list_dense:', [1163])\\n\",\n      \"(1600000, ';label_list_dense:', [320, 71, 0, 7, 70])\\n\",\n      \"(1700000, ';label_list_dense:', [503, 387, 269])\\n\",\n      \"(1800000, ';label_list_dense:', [1475])\\n\",\n      \"(1900000, ';label_list_dense:', [872, 317, 71, 241, 70])\\n\",\n      \"(2000000, ';label_list_dense:', [637, 1825])\\n\",\n      \"(2100000, ';label_list_dense:', [527])\\n\",\n      \"(2200000, ';label_list_dense:', [696, 30, 383])\\n\",\n      \"(2300000, ';label_list_dense:', [457, 1890])\\n\",\n      \"(2400000, ';label_list_dense:', [1385, 2, 3, 35])\\n\",\n      \"(2500000, ';label_list_dense:', [79, 54])\\n\",\n      \"(2600000, ';label_list_dense:', [794, 1560, 668])\\n\",\n      \"(2700000, ';label_list_dense:', [1716])\\n\",\n      \"(2800000, ';label_list_dense:', [1585, 50])\\n\",\n      \"(2900000, ';label_list_dense:', [504, 28, 249, 71, 128])\\n\",\n      \"('num_examples:', 2999966, ';X.shape:', (2999966, 200), ';Y.shape:', (2999966, 1999))\\n\",\n      \"('train_X:', (2959966, 200), ';train_Y:', (2959966, 1999), ';vaild_X.shape:', (20000, 200), ';valid_Y:', array([[0., 0., 0., ..., 0., 0., 0.],\\n\",\n      \"       [0., 0., 0., ..., 0., 0., 0.],\\n\",\n      \"       [0., 0., 0., ..., 0., 0., 0.],\\n\",\n      \"       ...,\\n\",\n      \"       [0., 0., 0., ..., 0., 0., 0.],\\n\",\n      \"       [0., 1., 0., ..., 0., 0., 0.],\\n\",\n      \"       [0., 0., 0., ..., 0., 0., 0.]]), ';test_X:', (20000, 200), ';test_Y:', (20000, 1999))\\n\"\n     ]\n    },\n    {\n     \"name\": \"stdout\",\n     \"output_type\": \"stream\",\n     \"text\": [\n      \"save cache files to file system successfully!\\n\"\n     ]\n    }\n   ],\n   \"source\": [\n    \"# generate training/validation/test data using source file and vocabulary/label set.\\n\",\n    \"#  get X,Y---> shuffle and split data----> save to file system.\\n\",\n    \"test_mode=False\\n\",\n    \"label_size=len(label2index)\\n\",\n    \"cache_path_h5py=base_path+'data.h5'\\n\",\n    \"cache_path_pickle=base_path+'vocab_label.pik'\\n\",\n    \"max_sentence_length=200\\n\",\n    \"\\n\",\n    \"# step 1: get (X,y) \\n\",\n    \"X,Y=get_X_Y(train_data_x,train_data_y,label_size,test_mode=test_mode)\\n\",\n    \"\\n\",\n    \"# pad and truncate to a max_sequence_length\\n\",\n    \"X = pad_sequences(X, maxlen=max_sentence_length, value=0.)  # padding to max length\\n\",\n    \"\\n\",\n    \"# step 2. shuffle, split,\\n\",\n    \"xy=list(zip(X,Y))\\n\",\n    \"random.Random(10000).shuffle(xy)\\n\",\n    \"X,Y=zip(*xy)\\n\",\n    \"X=np.array(X); Y=np.array(Y)\\n\",\n    \"num_examples=len(X)\\n\",\n    \"num_valid=20000\\n\",\n    \"num_valid=20000\\n\",\n    \"num_train=num_examples-(num_valid+num_valid)\\n\",\n    \"train_X, train_Y=X[0:num_train], Y[0:num_train]\\n\",\n    \"vaild_X, valid_Y=X[num_train:num_train+num_valid], Y[num_train:num_train+num_valid]\\n\",\n    \"test_X, test_Y=X[num_train+num_valid:], Y[num_train+num_valid:]\\n\",\n    \"print(\\\"num_examples:\\\",num_examples,\\\";X.shape:\\\",X.shape,\\\";Y.shape:\\\",Y.shape)\\n\",\n    \"print(\\\"train_X:\\\",train_X.shape,\\\";train_Y:\\\",train_Y.shape,\\\";vaild_X.shape:\\\",vaild_X.shape,\\\";valid_Y:\\\",valid_Y.shape,\\\";test_X:\\\",test_X.shape,\\\";test_Y:\\\",test_Y.shape)\\n\",\n    \"\\n\",\n    \"# step 3: save to file system\\n\",\n    \"save_data(cache_path_h5py,cache_path_pickle,word2index,label2index,train_X,train_Y,vaild_X,valid_Y,test_X,test_Y)\\n\",\n    \"print(\\\"save cache files to file system successfully!\\\")\\n\",\n    \"\\n\",\n    \"del X,Y,train_X, train_Y,vaild_X, valid_Y,test_X, test_Y\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### TODO 1: use topic information\\n\",\n    \"below are some of things you can do, to have a better model.\\n\",\n    \"\\n\",\n    \"if you want to get better performance, you can use pre-trained word embedding and char embedding. \\n\",\n    \"\\n\",\n    \"addtionally,  if you want to model this task in a better way, you can use topic information. you can find it in topic_info.txt,  \\n\",\n    \"\\n\",\n    \"where each topic is assocate:\\n\",\n    \"\\n\",\n    \"this its parent topics(zeor,one or more); \\n\",\n    \"\\n\",\n    \"charactor tokens of topic's name; \\n\",\n    \"\\n\",\n    \"word tokens of topic's name;\\n\",\n    \"\\n\",\n    \"charactor tokens of topic's description; \\n\",\n    \"\\n\",\n    \"word tokens of topic's description.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 13,\n   \"metadata\": {},\n   \"outputs\": [\n    {\n     \"data\": {\n      \"text/html\": [\n       \"<div>\\n\",\n       \"<style scoped>\\n\",\n       \"    .dataframe tbody tr th:only-of-type {\\n\",\n       \"        vertical-align: middle;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe tbody tr th {\\n\",\n       \"        vertical-align: top;\\n\",\n       \"    }\\n\",\n       \"\\n\",\n       \"    .dataframe thead th {\\n\",\n       \"        text-align: right;\\n\",\n       \"    }\\n\",\n       \"</style>\\n\",\n       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n       \"  <thead>\\n\",\n       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n       \"      <th></th>\\n\",\n       \"      <th>738845194850773558</th>\\n\",\n       \"      <th>-5833678375673307423</th>\\n\",\n       \"      <th>c0,c1</th>\\n\",\n       \"      <th>w0</th>\\n\",\n       \"      <th>c0,c1,c2,c3,c4,c5,c6,c7,c0,c1,c8,c9,c10,c11,c12,c13,c14,c15,c16,c11,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c20,c31,c24,c25,c26,c27,c11,c24,c32,c33,c34,c35,c36,c31,c8,c37,c38</th>\\n\",\n       \"      <th>w0,w1,w2,w3,w4,w5,w6,w7,w8,w9,w10,w11,w12,w13,w14,w15,w16,w17,w18,w15,w6,w19,w20,w21,w22,w23</th>\\n\",\n       \"    </tr>\\n\",\n       \"  </thead>\\n\",\n       \"  <tbody>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>0</th>\\n\",\n       \"      <td>3738968195649774859</td>\\n\",\n       \"      <td>2027693463582123305</td>\\n\",\n       \"      <td>c39,c40</td>\\n\",\n       \"      <td>w24</td>\\n\",\n       \"      <td>c41,c42,c43,c39,c40,c4,c44,c45,c46,c47,c48,c49...</td>\\n\",\n       \"      <td>w24,w25,w26,w27,w28,w6,w29,w30,w11,w31,w32,w33...</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>1</th>\\n\",\n       \"      <td>4738849194894773882</td>\\n\",\n       \"      <td>1127459907694805235</td>\\n\",\n       \"      <td>c172,c31,c0,c1</td>\\n\",\n       \"      <td>w102</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"      <td>NaN</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>2</th>\\n\",\n       \"      <td>7739004195693774975</td>\\n\",\n       \"      <td>2904932941037075699,1160326435131345730,725917...</td>\\n\",\n       \"      <td>c39,c40,c5,c173</td>\\n\",\n       \"      <td>w103</td>\\n\",\n       \"      <td>c39,c40,c23,c21,c174,c74,c5,c173,c17,c35,c39,c...</td>\\n\",\n       \"      <td>w104,w105,w11,w21,w24,w6,w106,w23,w54,w24,w107...</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>3</th>\\n\",\n       \"      <td>-7261194805221226386</td>\\n\",\n       \"      <td>-5833678375673307423</td>\\n\",\n       \"      <td>c36,c31,c45,c237</td>\\n\",\n       \"      <td>w148</td>\\n\",\n       \"      <td>c238,c239</td>\\n\",\n       \"      <td>w149,w150</td>\\n\",\n       \"    </tr>\\n\",\n       \"    <tr>\\n\",\n       \"      <th>4</th>\\n\",\n       \"      <td>-3689337711138901728</td>\\n\",\n       \"      <td>-2689200710357900655,-1689319711084901730</td>\\n\",\n       \"      <td>c215,c147,c105,c284,c97,c97,c168,c101,c146,c14...</td>\\n\",\n       \"      <td>w205,w54,w206</td>\\n\",\n       \"      <td>c196,c197,c0,c1,c313,c314,c315,c316,c317,c200,...</td>\\n\",\n       \"      <td>w125,w207,w208,w209,w166,w167,w23</td>\\n\",\n       \"    </tr>\\n\",\n       \"  </tbody>\\n\",\n       \"</table>\\n\",\n       \"</div>\"\n      ],\n      \"text/plain\": [\n       \"    738845194850773558                               -5833678375673307423  \\\\\\n\",\n       \"0  3738968195649774859                                2027693463582123305   \\n\",\n       \"1  4738849194894773882                                1127459907694805235   \\n\",\n       \"2  7739004195693774975  2904932941037075699,1160326435131345730,725917...   \\n\",\n       \"3 -7261194805221226386                               -5833678375673307423   \\n\",\n       \"4 -3689337711138901728          -2689200710357900655,-1689319711084901730   \\n\",\n       \"\\n\",\n       \"                                               c0,c1             w0  \\\\\\n\",\n       \"0                                            c39,c40            w24   \\n\",\n       \"1                                     c172,c31,c0,c1           w102   \\n\",\n       \"2                                    c39,c40,c5,c173           w103   \\n\",\n       \"3                                   c36,c31,c45,c237           w148   \\n\",\n       \"4  c215,c147,c105,c284,c97,c97,c168,c101,c146,c14...  w205,w54,w206   \\n\",\n       \"\\n\",\n       \"  c0,c1,c2,c3,c4,c5,c6,c7,c0,c1,c8,c9,c10,c11,c12,c13,c14,c15,c16,c11,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c20,c31,c24,c25,c26,c27,c11,c24,c32,c33,c34,c35,c36,c31,c8,c37,c38  \\\\\\n\",\n       \"0  c41,c42,c43,c39,c40,c4,c44,c45,c46,c47,c48,c49...                                                                                                                                               \\n\",\n       \"1                                                NaN                                                                                                                                               \\n\",\n       \"2  c39,c40,c23,c21,c174,c74,c5,c173,c17,c35,c39,c...                                                                                                                                               \\n\",\n       \"3                                          c238,c239                                                                                                                                               \\n\",\n       \"4  c196,c197,c0,c1,c313,c314,c315,c316,c317,c200,...                                                                                                                                               \\n\",\n       \"\\n\",\n       \"  w0,w1,w2,w3,w4,w5,w6,w7,w8,w9,w10,w11,w12,w13,w14,w15,w16,w17,w18,w15,w6,w19,w20,w21,w22,w23  \\n\",\n       \"0  w24,w25,w26,w27,w28,w6,w29,w30,w11,w31,w32,w33...                                            \\n\",\n       \"1                                                NaN                                            \\n\",\n       \"2  w104,w105,w11,w21,w24,w6,w106,w23,w54,w24,w107...                                            \\n\",\n       \"3                                          w149,w150                                            \\n\",\n       \"4                  w125,w207,w208,w209,w166,w167,w23                                            \"\n      ]\n     },\n     \"execution_count\": 13,\n     \"metadata\": {},\n     \"output_type\": \"execute_result\"\n    }\n   ],\n   \"source\": [\n    \"topic_info_data=pd.read_csv(base_path+'topic_info.txt', sep='\\\\t',encoding=\\\"utf-8\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### TODO 2: use both character and word tokens\\n\",\n    \"in this notebook we just use character tokens. it is fine. however, as many people observed, use word tokens to represent inputs, performance may be \\n\",\n    \"better. and if you can use both word and character tokens to represent inputs, performance can be much better. \\n\",\n    \"\\n\",\n    \"one of draw back to use word  is there are much more words then character. for example, in this task, total word token that frequency more than 5 is around 410k, while character token with frequency more than 5 is only around 11k. so much more memory is need. \\n\",\n    \"\\n\",\n    \"but you can still have a try if you want. with word token only, sequence length is shorter than character token, only about 50% length is needed.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### TODO 3: use pre-trained character and word embedding\\n\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 2\",\n   \"language\": \"python\",\n   \"name\": \"python2\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 2\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython2\",\n   \"version\": \"2.7.10\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n"
        },
        {
          "name": "sample_data.zip",
          "type": "blob",
          "size": 4677.677734375,
          "content": null
        }
      ]
    }
  ]
}