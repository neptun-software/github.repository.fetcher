{
  "metadata": {
    "timestamp": 1736560953123,
    "page": 700,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "PaddlePaddle/PaddleClas",
      "stars": 5534,
      "defaultBranch": "release/2.6",
      "files": [
        {
          "name": ".clang_format.hook",
          "type": "blob",
          "size": 0.3447265625,
          "content": "#!/bin/bash\nset -e\n\nreadonly VERSION=\"3.8\"\n\nversion=$(clang-format -version)\n\nif ! [[ $version == *\"$VERSION\"* ]]; then\n    echo \"clang-format version check failed.\"\n    echo \"a version contains '$VERSION' is needed, but get '$version'\"\n    echo \"you can install the right version, and make an soft-link to '\\$PATH' env\"\n    exit -1\nfi\n\nclang-format $@\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.162109375,
          "content": "__pycache__/\n*.pyc\n*.sw*\n*/workerlog*\ncheckpoints/\noutput*/\npretrained/\n.ipynb_checkpoints/\n*.ipynb*\n_build/\nbuild/\nlog/\nnohup.out\n.DS_Store\n.idea\ninference/\ntest.py\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1,
          "content": "repos:\n-   repo: https://github.com/PaddlePaddle/mirrors-yapf.git\n    rev: 0d79c0c469bab64f7229c9aca2b1186ef47f0e37\n    hooks:\n    -   id: yapf\n        files: \\.py$\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: a11d9314b22d8f8c7556443875b731ef05965464\n    hooks:\n    -   id: check-merge-conflict\n    -   id: check-symlinks\n    -   id: detect-private-key\n        files: (?!.*paddle)^.*$\n    -   id: end-of-file-fixer\n        files: \\.md$\n    -   id: trailing-whitespace\n        files: \\.md$\n-   repo: https://github.com/Lucas-C/pre-commit-hooks\n    rev: v1.0.1\n    hooks:\n    -   id: forbid-crlf\n        files: \\.md$\n    -   id: remove-crlf\n        files: \\.md$\n    -   id: forbid-tabs\n        files: \\.md$\n    -   id: remove-tabs\n        files: \\.md$\n-   repo: local\n    hooks:\n    -   id: clang-format\n        name: clang-format\n        description: Format files with ClangFormat\n        entry: bash .clang_format.hook -i\n        language: system\n        files: \\.(c|cc|cxx|cpp|cu|h|hpp|hxx|cuh|proto)$\n"
        },
        {
          "name": ".travis",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.26953125,
          "content": "include LICENSE\ninclude README.md\ninclude docs/en/inference_deployment/whl_deploy_en.md\nrecursive-include deploy/python *.py\nrecursive-include deploy/utils *.py\nrecursive-include ppcls/arch *.py\nrecursive-include ppcls/utils *.py *.txt\nrecursive-include deploy/configs *.yaml\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 0.01171875,
          "content": "README_ch.md"
        },
        {
          "name": "README_ch.md",
          "type": "blob",
          "size": 14.5009765625,
          "content": "ç®€ä½“ä¸­æ–‡ | [English](README_en.md)\n\n# PaddleClas\n\n## ç®€ä»‹\n\né£æ¡¨å›¾åƒè¯†åˆ«å¥—ä»¶PaddleClasæ˜¯é£æ¡¨ä¸ºå·¥ä¸šç•Œå’Œå­¦æœ¯ç•Œæ‰€å‡†å¤‡çš„ä¸€ä¸ªå›¾åƒè¯†åˆ«å’Œå›¾åƒåˆ†ç±»ä»»åŠ¡çš„å·¥å…·é›†ï¼ŒåŠ©åŠ›ä½¿ç”¨è€…è®­ç»ƒå‡ºæ›´å¥½çš„è§†è§‰æ¨¡å‹å’Œåº”ç”¨è½åœ°ã€‚\n\n|             PP-ShiTuV2å›¾åƒè¯†åˆ«ç³»ç»Ÿæ•ˆæœå±•ç¤º             |                PULCå®ç”¨å›¾åƒåˆ†ç±»æ¨¡å‹æ•ˆæœå±•ç¤º                 |\n| :----------------------------------------------------: | :---------------------------------------------------------: |\n| <img src=\"./docs/images/shituv2.gif\"  width = \"450\" /> | <img src=\"./docs/images/class_simple.gif\"  width = \"600\" /> |\n\n\n## ğŸ“£ è¿‘æœŸæ›´æ–°\n\n- **ğŸ”¥2024.11.5 æ·»åŠ å›¾åƒåˆ†ç±»å’Œå›¾åƒæ£€ç´¢é¢†åŸŸä½ä»£ç å…¨æµç¨‹å¼€å‘èƒ½åŠ›**:\n  *  é£æ¡¨ä½ä»£ç å¼€å‘å·¥å…·PaddleXï¼Œä¾æ‰˜äºPaddleClasçš„å…ˆè¿›æŠ€æœ¯ï¼Œæ”¯æŒäº†å›¾åƒåˆ†ç±»å’Œå›¾åƒæ£€ç´¢é¢†åŸŸçš„**ä½ä»£ç å…¨æµç¨‹**å¼€å‘èƒ½åŠ›ï¼š\n     * ğŸ¨ [**æ¨¡å‹ä¸°å¯Œä¸€é”®è°ƒç”¨**](docs/zh_CN/paddlex/quick_start.md)ï¼šå°†é€šç”¨å›¾åƒåˆ†ç±»ã€å›¾åƒå¤šæ ‡ç­¾åˆ†ç±»ã€é€šç”¨å›¾åƒè¯†åˆ«ã€äººè„¸è¯†åˆ«æ¶‰åŠçš„**98ä¸ªæ¨¡å‹**æ•´åˆä¸º6æ¡æ¨¡å‹äº§çº¿ï¼Œé€šè¿‡æç®€çš„**Python APIä¸€é”®è°ƒç”¨**ï¼Œå¿«é€Ÿä½“éªŒæ¨¡å‹æ•ˆæœã€‚æ­¤å¤–ï¼ŒåŒä¸€å¥—APIï¼Œä¹Ÿæ”¯æŒç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†å‰²ã€æ–‡æœ¬å›¾åƒæ™ºèƒ½åˆ†æã€é€šç”¨OCRã€æ—¶åºé¢„æµ‹ç­‰å…±è®¡**200+æ¨¡å‹**ï¼Œå½¢æˆ20+å•åŠŸèƒ½æ¨¡å—ï¼Œæ–¹ä¾¿å¼€å‘è€…è¿›è¡Œ**æ¨¡å‹ç»„åˆä½¿ç”¨**ã€‚\n     * ğŸš€ [**æé«˜æ•ˆç‡é™ä½é—¨æ§›**](docs/zh_CN/paddlex/overview.md)ï¼šæä¾›åŸºäº**ç»Ÿä¸€å‘½ä»¤**å’Œ**å›¾å½¢ç•Œé¢**ä¸¤ç§æ–¹å¼ï¼Œå®ç°æ¨¡å‹ç®€æ´é«˜æ•ˆçš„ä½¿ç”¨ã€ç»„åˆä¸å®šåˆ¶ã€‚æ”¯æŒ**é«˜æ€§èƒ½æ¨ç†ã€æœåŠ¡åŒ–éƒ¨ç½²å’Œç«¯ä¾§éƒ¨ç½²**ç­‰å¤šç§éƒ¨ç½²æ–¹å¼ã€‚æ­¤å¤–ï¼Œå¯¹äºå„ç§ä¸»æµç¡¬ä»¶å¦‚**è‹±ä¼Ÿè¾¾GPUã€æ˜†ä»‘èŠ¯ã€æ˜‡è…¾ã€å¯’æ­¦çºªå’Œæµ·å…‰**ç­‰ï¼Œè¿›è¡Œæ¨¡å‹å¼€å‘æ—¶ï¼Œéƒ½å¯ä»¥**æ— ç¼åˆ‡æ¢**ã€‚\n  * æ–°å¢å›¾åƒåˆ†ç±»ç®—æ³•[**MobileNetV4ã€StarNetã€FasterNet**](https://github.com/PaddlePaddle/PaddleX/blob/release/3.0-beta1/docs/module_usage/tutorials/cv_modules/image_classification.md)\n  * æ–°å¢æœåŠ¡ç«¯å›¾åƒè¯†åˆ«æ¨¡å‹ï¼ˆå›¾åƒç‰¹å¾ï¼‰[**PP-ShiTuV2_rec_CLIP_vit_baseã€PP-ShiTuV2_rec_CLIP_vit_large**](https://github.com/PaddlePaddle/PaddleX/blob/release/3.0-beta1/docs/module_usage/tutorials/cv_modules/image_feature.md)\n  * æ–°å¢å¤šæ ‡ç­¾å›¾åƒåˆ†ç±»æ¨¡å‹[**CLIP_vit_base_patch16_448_MLã€PP-HGNetV2-B0_MLã€PP-HGNetV2-B4_MLã€PP-HGNetV2-B6_MLã€PP-LCNet_x1_0_MLã€ResNet50_ML**](https://github.com/PaddlePaddle/PaddleX/blob/release/3.0-beta1/docs/module_usage/tutorials/cv_modules/ml_classification.md)\n  * æ–°å¢äººè„¸è¯†åˆ«æ¨¡å‹[**MobileFaceNetã€ResNet50_face**](https://github.com/PaddlePaddle/PaddleX/blob/develop/docs/module_usage/tutorials/cv_modules/face_recognition.md)ï¼Œæ–°å¢[äººè„¸è¯†åˆ«ç«¯åˆ°ç«¯ç³»ç»Ÿ](https://github.com/PaddlePaddle/PaddleX/blob/develop/docs/pipeline_usage/tutorials/cv_pipelines/face_recognition.md)ã€‚\n\n- 2022.9.13 å‘å¸ƒè¶…è½»é‡å›¾åƒè¯†åˆ«ç³»ç»Ÿ[PP-ShiTuV2](docs/zh_CN/models/PP-ShiTu/README.md)ï¼š\n  - recall1ç²¾åº¦æå‡8ä¸ªç‚¹ï¼Œè¦†ç›–å•†å“è¯†åˆ«ã€åƒåœ¾åˆ†ç±»ã€èˆªæ‹åœºæ™¯ç­‰[20+è¯†åˆ«åœºæ™¯](docs/zh_CN/deployment/PP-ShiTu/application_scenarios.md)ï¼Œ\n  - æ–°å¢[åº“ç®¡ç†å·¥å…·](./deploy/shitu_index_manager/)ï¼Œ[Android Demo](./docs/zh_CN/quick_start/quick_start_recognition.md)å…¨æ–°ä½“éªŒã€‚\n\n- [more](docs/zh_CN/version_history.md)\n\n\n## ğŸŒŸ ç‰¹æ€§\n\nPaddleClasæ”¯æŒå¤šç§å‰æ²¿å›¾åƒåˆ†ç±»ã€è¯†åˆ«ç›¸å…³ç®—æ³•ï¼Œå‘å¸ƒäº§ä¸šçº§ç‰¹è‰²éª¨å¹²ç½‘ç»œ[PP-HGNet](docs/zh_CN/models/ImageNet1k/PP-HGNet.md)ã€[PP-LCNetv2](docs/zh_CN/models/ImageNet1k/PP-LCNetV2.md)ã€ [PP-LCNet](docs/zh_CN/models/ImageNet1k/PP-LCNet.md)å’Œ[SSLDåŠç›‘ç£çŸ¥è¯†è’¸é¦æ–¹æ¡ˆ](docs/zh_CN/training/advanced/ssld.md)ç­‰æ¨¡å‹ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šæ‰“é€ [PULCè¶…è½»é‡å›¾åƒåˆ†ç±»æ–¹æ¡ˆ](docs/zh_CN/quick_start/PULC.md)å’Œ[PP-ShiTuå›¾åƒè¯†åˆ«ç³»ç»Ÿ](./docs/zh_CN/quick_start/quick_start_recognition.md)ã€‚\n\n<div align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/50011306/198961573-06a1a78d-7669-4061-aba5-79e9a2fc84dc.png\"/>\n</div>\n\n> ä¸Šè¿°å†…å®¹çš„ä½¿ç”¨æ–¹æ³•å»ºè®®ä»æ–‡æ¡£æ•™ç¨‹ä¸­çš„å¿«é€Ÿå¼€å§‹ä½“éªŒ\n\n\n## âš¡ [å¿«é€Ÿå¼€å§‹](docs/zh_CN/paddlex/quick_start.md)\n\n- [ğŸ”¥ ä¸€é”®è°ƒç”¨98ä¸ªPaddleClasæ ¸å¿ƒæ¨¡å‹](docs/zh_CN/paddlex/quick_start.md)\n- PULCè¶…è½»é‡å›¾åƒåˆ†ç±»æ–¹æ¡ˆå¿«é€Ÿä½“éªŒï¼š[ç‚¹å‡»è¿™é‡Œ](docs/zh_CN/quick_start/PULC.md)\n- PP-ShiTuå›¾åƒè¯†åˆ«å¿«é€Ÿä½“éªŒï¼š[ç‚¹å‡»è¿™é‡Œ](./docs/zh_CN/quick_start/quick_start_recognition.md)\n- PP-ShiTuV2 Android Demo APPï¼Œå¯æ‰«æå¦‚ä¸‹äºŒç»´ç ï¼Œä¸‹è½½ä½“éªŒ\n\n<div align=\"center\">\n<img src=\"./docs/images/quick_start/android_demo/PPShiTu_qrcode.png\"  width = \"170\" height = \"170\" />\n</div>\n\n## ğŸ”¥ [ä½ä»£ç å…¨æµç¨‹å¼€å‘](docs/zh_CN/paddlex/overview.md)\n\n\n## ğŸ› ï¸ PPç³»åˆ—æ¨¡å‹åˆ—è¡¨\n\n| æ¨¡å‹ç®€ä»‹                    | åº”ç”¨åœºæ™¯                             | æ¨¡å‹ä¸‹è½½é“¾æ¥                                                 |\n| --------------------------- | ------------------------------------ | ------------------------------------------------------------ |\n| PULC è¶…è½»é‡å›¾åƒåˆ†ç±»æ–¹æ¡ˆ     | å›ºå®šå›¾åƒç±»åˆ«åˆ†ç±»æ–¹æ¡ˆ                 | äººä½“ã€è½¦è¾†ã€æ–‡å­—ç›¸å…³9å¤§æ¨¡å‹ï¼š[æ¨¡å‹åº“è¿æ¥](./docs/zh_CN/models/PULC/model_list.md) |\n| PP-ShituV2 è½»é‡å›¾åƒè¯†åˆ«ç³»ç»Ÿ | é’ˆå¯¹åœºæ™¯æ•°æ®ç±»åˆ«é¢‘ç¹å˜åŠ¨ã€ç±»åˆ«æ•°æ®å¤š | ä¸»ä½“æ£€æµ‹æ¨¡å‹ï¼š[é¢„è®­ç»ƒæ¨¡å‹](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/rec/models/pretrain/picodet_PPLCNet_x2_5_mainbody_lite_v1.0_pretrained.pdparams)  / [æ¨ç†æ¨¡å‹](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/rec/models/inference/picodet_PPLCNet_x2_5_mainbody_lite_v1.0_infer.tar)<br />è¯†åˆ«æ¨¡å‹ï¼š[é¢„è®­ç»ƒæ¨¡å‹](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/rec/models/pretrain/PPShiTuV2/general_PPLCNetV2_base_pretrained_v1.0.pdparams)  / [æ¨ç†æ¨¡å‹](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/rec/models/inference/PP-ShiTuV2/general_PPLCNetV2_base_pretrained_v1.0_infer.tar) |\n| PP-LCNet è½»é‡éª¨å¹²ç½‘ç»œ       | é’ˆå¯¹Intel CPUè®¾å¤‡åŠMKLDNNåŠ é€Ÿåº“å®šåˆ¶  | PPLCNet_x1_0ï¼š[é¢„è®­ç»ƒæ¨¡å‹](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNet_x1_0_pretrained.pdparams)  / [æ¨ç†æ¨¡å‹](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/inference/PPLCNet_x1_0_infer.tar) |\n| PP-LCNetV2 è½»é‡éª¨å¹²ç½‘ç»œ     | é’ˆå¯¹Intel CPUè®¾å¤‡ï¼Œé€‚é…OpenVINO      | PPLCNetV2_baseï¼š[é¢„è®­ç»ƒæ¨¡å‹](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNetV2_base_pretrained.pdparams)  / [æ¨ç†æ¨¡å‹](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/inference/PPLCNetV2_base_infer.tar) |\n| PP-HGNet é«˜ç²¾åº¦éª¨å¹²ç½‘ç»œ     | GPUè®¾å¤‡ä¸Šç›¸åŒæ¨ç†æ—¶é—´ç²¾åº¦æ›´é«˜        | PPHGNet_smallï¼š[é¢„è®­ç»ƒæ¨¡å‹](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPHGNet_small_pretrained.pdparams)  / [æ¨ç†æ¨¡å‹](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/inference/PPHGNet_small_infer.tar) |\n\n> å…¨éƒ¨æ¨¡å‹ä¸‹è½½é“¾æ¥å¯æŸ¥çœ‹ æ–‡æ¡£æ•™ç¨‹ ä¸­çš„å„æ¨¡å‹ä»‹ç»\n\n### äº§ä¸šèŒƒä¾‹\n\n- åŸºäºPP-ShiTuV2çš„ç”Ÿé²œå“è‡ªåŠ©ç»“ç®—ï¼š [ç‚¹å‡»è¿™é‡Œ](./docs/zh_CN/samples/Fresh_Food_Recogniiton/README.md)\n- åŸºäºPULCäººå‘˜å‡ºå…¥è§†é¢‘ç®¡ç†ï¼š [ç‚¹å‡»è¿™é‡Œ](./docs/zh_CN/samples/Personnel_Access/README.md)\n- åŸºäºPP-ShiTu çš„æ™ºæ…§å•†è¶…å•†å“è¯†åˆ«ï¼š[ç‚¹å‡»è¿™é‡Œ](./docs/zh_CN/samples/Goods_Recognition/README.md)\n- åŸºäºPP-ShiTuç”µæ¢¯å†…ç”µç“¶è½¦å…¥å®¤è¯†åˆ«ï¼š[ç‚¹å‡»è¿™é‡Œ](./docs/zh_CN/samples//Electromobile_In_Elevator_Detection/README.md)\n\n## ğŸ“– æ–‡æ¡£æ•™ç¨‹\n- [ç¯å¢ƒå‡†å¤‡](docs/zh_CN/installation.md)\n- [PP-ShiTuV2å›¾åƒè¯†åˆ«ç³»ç»Ÿä»‹ç»](docs/zh_CN/models/PP-ShiTu/README.md)\n  - [å›¾åƒè¯†åˆ«å¿«é€Ÿä½“éªŒ](docs/zh_CN/quick_start/quick_start_recognition.md)\n  - [20+åº”ç”¨åœºæ™¯åº“](docs/zh_CN/deployment/PP-ShiTu/application_scenarios.md)\n  - å­æ¨¡å—ç®—æ³•ä»‹ç»åŠæ¨¡å‹è®­ç»ƒ\n    - [ä¸»ä½“æ£€æµ‹](docs/zh_CN/training/PP-ShiTu/mainbody_detection.md)\n    - [ç‰¹å¾æå–æ¨¡å‹](docs/zh_CN/training/PP-ShiTu/feature_extraction.md)\n    - [å‘é‡æ£€ç´¢](docs/zh_CN/deployment/PP-ShiTu/vector_search.md)\n    - [å“ˆå¸Œç¼–ç ](docs/zh_CN/training/PP-ShiTu/deep_hashing.md)\n  - PipeLine æ¨ç†éƒ¨ç½²\n    - [åŸºäºpythoné¢„æµ‹å¼•æ“æ¨ç†](docs/zh_CN/deployment/PP-ShiTu/python.md)\n    - [åŸºäºC++é¢„æµ‹å¼•æ“æ¨ç†](docs/zh_CN/deployment/PP-ShiTu/cpp.md)\n    - [æœåŠ¡åŒ–éƒ¨ç½²](docs/zh_CN/deployment/PP-ShiTu/paddle_serving.md)\n    - [ç«¯ä¾§éƒ¨ç½²](docs/zh_CN/deployment/PP-ShiTu/paddle_lite.md)\n    - [åº“ç®¡ç†å·¥å…·](docs/zh_CN/deployment/PP-ShiTu/gallery_manager.md)\n- [PULCè¶…è½»é‡å›¾åƒåˆ†ç±»å®ç”¨æ–¹æ¡ˆ](docs/zh_CN/training/PULC.md)\n  - [è¶…è½»é‡å›¾åƒåˆ†ç±»å¿«é€Ÿä½“éªŒ](docs/zh_CN/quick_start/PULC.md)\n  - [è¶…è½»é‡å›¾åƒåˆ†ç±»æ¨¡å‹åº“](docs/zh_CN/models/PULC/model_list.md)\n    - [PULCæœ‰äºº/æ— äººåˆ†ç±»æ¨¡å‹](docs/zh_CN/models/PULC/PULC_person_exists.md)\n    - [PULCäººä½“å±æ€§è¯†åˆ«æ¨¡å‹](docs/zh_CN/models/PULC/PULC_person_attribute.md)\n    - [PULCä½©æˆ´å®‰å…¨å¸½åˆ†ç±»æ¨¡å‹](docs/zh_CN/models/PULC/PULC_safety_helmet.md)\n    - [PULCäº¤é€šæ ‡å¿—åˆ†ç±»æ¨¡å‹](docs/zh_CN/models/PULC/PULC_traffic_sign.md)\n    - [PULCè½¦è¾†å±æ€§è¯†åˆ«æ¨¡å‹](docs/zh_CN/models/PULC/PULC_vehicle_attribute.md)\n    - [PULCæœ‰è½¦/æ— è½¦åˆ†ç±»æ¨¡å‹](docs/zh_CN/models/PULC/PULC_car_exists.md)\n    - [PULCå«æ–‡å­—å›¾åƒæ–¹å‘åˆ†ç±»æ¨¡å‹](docs/zh_CN/models/PULC/PULC_text_image_orientation.md)\n    - [PULCæ–‡æœ¬è¡Œæ–¹å‘åˆ†ç±»æ¨¡å‹](docs/zh_CN/models/PULC/PULC_textline_orientation.md)\n    - [PULCè¯­ç§åˆ†ç±»æ¨¡å‹](docs/zh_CN/models/PULC/PULC_language_classification.md)\n    - [PULCè¡¨æ ¼å±æ€§è¯†åˆ«æ¨¡å‹](docs/zh_CN/models/PULC/PULC_table_attribute.md)\n    - [PULCæœ‰æ— å¹¿å‘Šç åˆ†ç±»æ¨¡å‹](docs/zh_CN/models/PULC/PULC_code_exists.md)\n    - [PULCæ¸…æ™°åº¦è¯„ä¼°æ¨¡å‹](docs/zh_CN/models/PULC/PULC_clarity_assessment.md)\n    - [PULCå›¾åƒæ–¹å‘åˆ†ç±»æ¨¡å‹](docs/zh_CN/models/PULC/PULC_image_orientation.md)\n  - [æ¨¡å‹è®­ç»ƒ](docs/zh_CN/training/PULC.md)\n  - æ¨ç†éƒ¨ç½²\n    - [åŸºäºpythoné¢„æµ‹å¼•æ“æ¨ç†](docs/zh_CN/deployment/image_classification/python.md#1)\n    - [åŸºäºC++é¢„æµ‹å¼•æ“æ¨ç†](docs/zh_CN/deployment/image_classification/cpp/linux.md)\n    - [æœåŠ¡åŒ–éƒ¨ç½²](docs/zh_CN/deployment/image_classification/paddle_serving.md)\n    - [ç«¯ä¾§éƒ¨ç½²](docs/zh_CN/deployment/image_classification/paddle_lite.md)\n    - [Paddle2ONNXæ¨¡å‹è½¬åŒ–ä¸é¢„æµ‹](docs/zh_CN/deployment/image_classification/paddle2onnx.md)\n  - [æ¨¡å‹å‹ç¼©](deploy/slim/README.md)\n- PPç³»åˆ—éª¨å¹²ç½‘ç»œæ¨¡å‹\n  - [PP-HGNet](docs/zh_CN/models/ImageNet1k/PP-HGNet.md)\n  - [PP-LCNetv2](docs/zh_CN/models/ImageNet1k/PP-LCNetV2.md)\n  - [PP-LCNet](docs/zh_CN/models/ImageNet1k/PP-LCNet.md)\n- [SSLDåŠç›‘ç£çŸ¥è¯†è’¸é¦æ–¹æ¡ˆ](docs/zh_CN/training/advanced/ssld.md)\n- å‰æ²¿ç®—æ³•\n  - [éª¨å¹²ç½‘ç»œå’Œé¢„è®­ç»ƒæ¨¡å‹åº“](docs/zh_CN/models/ImageNet1k/model_list.md)\n  - [åº¦é‡å­¦ä¹ ](docs/zh_CN/algorithm_introduction/metric_learning.md)\n    - [ReID](./docs/zh_CN/algorithm_introduction/ReID.md)\n  - [æ¨¡å‹å‹ç¼©](docs/zh_CN/algorithm_introduction/prune_quantization.md)\n  - [æ¨¡å‹è’¸é¦](./docs/zh_CN/training/advanced/knowledge_distillation.md)\n  - [æ•°æ®å¢å¼º](docs/zh_CN/training/config_description/data_augmentation.md)\n- [äº§ä¸šå®ç”¨èŒƒä¾‹åº“](docs/zh_CN/samples)\n- [30åˆ†é’Ÿå¿«é€Ÿä½“éªŒå›¾åƒåˆ†ç±»](docs/zh_CN/quick_start/quick_start_classification_new_user.md)\n- FAQ\n  - [å›¾åƒè¯†åˆ«ç²¾é€‰é—®é¢˜](docs/zh_CN/FAQ/faq_2021_s2.md)\n  - [å›¾åƒåˆ†ç±»ç²¾é€‰é—®é¢˜](docs/zh_CN/FAQ/faq_selected_30.md)\n  - [å›¾åƒåˆ†ç±»FAQç¬¬ä¸€å­£](docs/zh_CN/FAQ/faq_2020_s1.md)\n  - [å›¾åƒåˆ†ç±»FAQç¬¬äºŒå­£](docs/zh_CN/FAQ/faq_2021_s1.md)\n  - [å›¾åƒåˆ†ç±»FAQç¬¬ä¸‰å­£](docs/zh_CN/FAQ/faq_2022_s1.md)\n- [ç¤¾åŒºè´¡çŒ®æŒ‡å—](docs/zh_CN/community/how_to_contribute.md)\n- [è®¸å¯è¯ä¹¦](#è®¸å¯è¯ä¹¦)\n- [è´¡çŒ®ä»£ç ](#è´¡çŒ®ä»£ç )\n\n<a name=\"å›¾åƒè¯†åˆ«ç³»ç»Ÿä»‹ç»\"></a>\n\n## PP-ShiTuV2å›¾åƒè¯†åˆ«ç³»ç»Ÿ\n\n<div align=\"center\">\n<img src=\"./docs/images/structure.jpg\"  width = \"800\" />\n</div>\n\n\nPP-ShiTuV2æ˜¯ä¸€ä¸ªå®ç”¨çš„è½»é‡çº§é€šç”¨å›¾åƒè¯†åˆ«ç³»ç»Ÿï¼Œä¸»è¦ç”±ä¸»ä½“æ£€æµ‹ã€ç‰¹å¾å­¦ä¹ å’Œå‘é‡æ£€ç´¢ä¸‰ä¸ªæ¨¡å—ç»„æˆã€‚è¯¥ç³»ç»Ÿä»éª¨å¹²ç½‘ç»œé€‰æ‹©å’Œè°ƒæ•´ã€æŸå¤±å‡½æ•°çš„é€‰æ‹©ã€æ•°æ®å¢å¼ºã€å­¦ä¹ ç‡å˜æ¢ç­–ç•¥ã€æ­£åˆ™åŒ–å‚æ•°é€‰æ‹©ã€é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨ä»¥åŠæ¨¡å‹è£å‰ªé‡åŒ–å¤šä¸ªæ–¹é¢ï¼Œé‡‡ç”¨å¤šç§ç­–ç•¥ï¼Œå¯¹å„ä¸ªæ¨¡å—çš„æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼ŒPP-ShiTuV2ç›¸æ¯”V1ï¼ŒRecall1æå‡è¿‘8ä¸ªç‚¹ã€‚æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ[PP-ShiTuV2è¯¦ç»†ä»‹ç»](docs/zh_CN/models/PP-ShiTu/README.md)ã€‚\n\n<a name=\"è¯†åˆ«æ•ˆæœå±•ç¤º\"></a>\n\n## PP-ShiTuV2å›¾åƒè¯†åˆ«ç³»ç»Ÿæ•ˆæœå±•ç¤º\n\n- ç“¶è£…é¥®æ–™è¯†åˆ«\n\n<div align=\"center\">\n<img src=\"docs/images/drink_demo.gif\">\n</div>\n\n\n- å•†å“è¯†åˆ«\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769644-51604f80-d2d7-11eb-8290-c53b12a5c1f6.gif\"  width = \"400\" />\n</div>\n\n\n- åŠ¨æ¼«äººç‰©è¯†åˆ«\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769746-6b019700-d2d7-11eb-86df-f1d710999ba6.gif\"  width = \"400\" />\n</div>\n\n\n- logoè¯†åˆ«\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769837-7fde2a80-d2d7-11eb-9b69-04140e9d785f.gif\"  width = \"400\" />\n</div>\n\n\n\n- è½¦è¾†è¯†åˆ«\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769916-8ec4dd00-d2d7-11eb-8c60-42d89e25030c.gif\"  width = \"400\" />\n</div>\n\n\n\n<a name=\"PULCè¶…è½»é‡å›¾åƒåˆ†ç±»æ–¹æ¡ˆ\"></a>\n\n## PULCè¶…è½»é‡å›¾åƒåˆ†ç±»æ–¹æ¡ˆ\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/19523330/173011854-b10fcd7a-b799-4dfd-a1cf-9504952a3c44.png\"  width = \"800\" />\n</div>\nPULCèåˆäº†éª¨å¹²ç½‘ç»œã€æ•°æ®å¢å¹¿ã€è’¸é¦ç­‰å¤šç§å‰æ²¿ç®—æ³•ï¼Œå¯ä»¥è‡ªåŠ¨è®­ç»ƒå¾—åˆ°è½»é‡ä¸”é«˜ç²¾åº¦çš„å›¾åƒåˆ†ç±»æ¨¡å‹ã€‚\nPaddleClasæä¾›äº†è¦†ç›–äººã€è½¦ã€OCRåœºæ™¯ä¹å¤§å¸¸è§ä»»åŠ¡çš„åˆ†ç±»æ¨¡å‹ï¼ŒCPUæ¨ç†3msï¼Œç²¾åº¦æ¯”è‚©SwinTransformerã€‚\n\n<a name=\"åˆ†ç±»æ•ˆæœå±•ç¤º\"></a>\n\n## PULCå®ç”¨å›¾åƒåˆ†ç±»æ¨¡å‹æ•ˆæœå±•ç¤º\n<div align=\"center\">\n<img src=\"docs/images/classification.gif\">\n</div>\n\n\n<a name=\"è®¸å¯è¯ä¹¦\"></a>\n\n## è®¸å¯è¯ä¹¦\næœ¬é¡¹ç›®çš„å‘å¸ƒå—<a href=\"https://github.com/PaddlePaddle/PaddleCLS/blob/master/LICENSE\">Apache 2.0 license</a>è®¸å¯è®¤è¯ã€‚\n\n\n<a name=\"è´¡çŒ®ä»£ç \"></a>\n## è´¡çŒ®ä»£ç \næˆ‘ä»¬éå¸¸æ¬¢è¿ä½ ä¸ºPaddleClasè´¡çŒ®ä»£ç ï¼Œä¹Ÿååˆ†æ„Ÿè°¢ä½ çš„åé¦ˆã€‚\nå¦‚æœæƒ³ä¸ºPaddleCLasè´¡çŒ®ä»£ç ï¼Œå¯ä»¥å‚è€ƒ[è´¡çŒ®æŒ‡å—](docs/zh_CN/community/how_to_contribute.md)ã€‚\n\n- éå¸¸æ„Ÿè°¢[nblib](https://github.com/nblib)ä¿®æ­£äº†PaddleClasä¸­RandErasingçš„æ•°æ®å¢å¹¿é…ç½®æ–‡ä»¶ã€‚\n- éå¸¸æ„Ÿè°¢[chenpy228](https://github.com/chenpy228)ä¿®æ­£äº†PaddleClasæ–‡æ¡£ä¸­çš„éƒ¨åˆ†é”™åˆ«å­—ã€‚\n- éå¸¸æ„Ÿè°¢[jm12138](https://github.com/jm12138)ä¸ºPaddleClasæ·»åŠ ViTï¼ŒDeiTç³»åˆ—æ¨¡å‹å’ŒRepVGGç³»åˆ—æ¨¡å‹ã€‚\n"
        },
        {
          "name": "README_en.md",
          "type": "blob",
          "size": 10.873046875,
          "content": "[ç®€ä½“ä¸­æ–‡](README_ch.md) | English\n\n# PaddleClas\n\n## Introduction\n\nPaddleClas is an image classification and image recognition toolset for industry and academia, helping users train better computer vision models and apply them in real scenarios.\n\n|                       PP-ShiTuV2                       | PULC: **P**ractical **U**ltra **L**ight-weight image **C**lassification solutions |\n| :----------------------------------------------------: | :----------------------------------------------------------: |\n| <img src=\"./docs/images/shituv2.gif\"  width = \"450\" /> | <img src=\"./docs/images/class_simple_en.gif\"  width = \"600\" /> |\n\n## ğŸ“£ Recent updates\n\n- ğŸ”¥ï¸ Release [PP-ShiTuV2](./docs/en/PPShiTu/PPShiTuV2_introduction.md), recall1 is improved by nearly 8 points, covering 20+ recognition scenarios, with [index management tool](./deploy/shitu_index_manager) and [Android Demo](./docs/en/quick_start/quick_start_recognition_en.md) for better experience.\n- 2022.6.15 Release [**P**ractical **U**ltra **L**ight-weight image **C**lassification solutions](./docs/en/PULC/PULC_quickstart_en.md). PULC models inference within 3ms on CPU devices, with accuracy on par with SwinTransformer. We also release 9 practical classification models covering pedestrian, vehicle and OCR scenario.\n- 2022.4.21 Added the related [code](https://github.com/PaddlePaddle/PaddleClas/pull/1820/files) of the CVPR2022 oral paper [MixFormer](https://arxiv.org/pdf/2204.02557.pdf).\n\n- 2021.09.17 Add PP-LCNet series model developed by PaddleClas, these models show strong competitiveness on Intel CPUs.\nFor the introduction of PP-LCNet, please refer to [paper](https://arxiv.org/pdf/2109.15099.pdf) or [PP-LCNet model introduction](docs/en/models/PP-LCNet_en.md). The metrics and pretrained model are available [here](docs/en/algorithm_introduction/ImageNet_models_en.md).\n\n- 2021.06.29 Add [Swin-transformer](docs/en/models/SwinTransformer_en.md)) series modelï¼ŒHighest top1 acc on ImageNet1k dataset reaches 87.2%, training, evaluation and inference are all supported. Pretrained models can be downloaded [here](docs/en/algorithm_introduction/ImageNet_models_en.md#16).\n- 2021.06.16 PaddleClas release/2.2. Add metric learning and vector search modules. Add product recognition, animation character recognition, vehicle recognition and logo recognition. Added 30 pretrained models of LeViT, Twins, TNT, DLA, HarDNet, and RedNet, and the accuracy is roughly the same as that of the paper.\n- [more](./docs/en/others/update_history_en.md)\n\n## ğŸŒŸ Features\n\nPaddleClas release PP-HGNetã€PP-LCNetv2ã€ PP-LCNet and **S**imple **S**emi-supervised **L**abel **D**istillation algorithms, and support plenty of image classification and image recognition algorithms.Based on th algorithms above, PaddleClas release PP-ShiTu image recognition system and [**P**ractical **U**ltra **L**ight-weight image **C**lassification solutions](docs/en/PULC/PULC_quickstart_en.md).\n\n\n![](https://user-images.githubusercontent.com/11568925/189268878-43d9d35b-90cf-425a-859e-767f8d94c5f7.png)\n\n## Welcome to Join the Technical Exchange Group\n\n* You can also scan the QR code below to join the PaddleClas QQ group and WeChat group (add and replay \"C\") to get more efficient answers to your questions and to communicate with developers from all walks of life. We look forward to hearing from you.\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/80816848/164383225-e375eb86-716e-41b4-a9e0-4b8a3976c1aa.jpg\" width=\"200\"/>\n<img src=\"https://user-images.githubusercontent.com/48054808/160531099-9811bbe6-cfbb-47d5-8bdb-c2b40684d7dd.png\" width=\"200\"/>\n</div>\n\n## Quick Start\nQuick experience of PP-ShiTu image recognition systemï¼š[Link](./docs/en/quick_start/quick_start_recognition_en.md)\n\n<div align=\"center\">\n<img src=\"./docs/images/quick_start/android_demo/PPShiTu_qrcode.png\"  width = \"40%\" />\n<p>PP-ShiTuV2 Android Demo</p>\n</div>\n\nQuick experience of **P**ractical **U**ltra **L**ight-weight image **C**lassification modelsï¼š[Link](docs/en/PULC/PULC_quickstart_en.md)\n\n## Tutorials\n\n- [Install Paddle](./docs/en/installation/install_paddle_en.md)\n- [Install PaddleClas Environment](./docs/en/installation/install_paddleclas_en.md)\n- [PP-ShiTuV2 Image Recognition Systems Introduction](./docs/en/PPShiTu/PPShiTuV2_introduction.md)\n  - [Image Recognition Quick Start](docs/en/quick_start/quick_start_recognition_en.md)\n  - [20+ application scenarios](docs/zh_CN/deployment/PP-ShiTu/application_scenarios.md)\n  - Submodule Introduction and Model Training\n    - [Mainbody Detection](docs/zh_CN/training/PP-ShiTu/mainbody_detection.md)\n    - [Feature Extraction](./docs/en/image_recognition_pipeline/feature_extraction_en.md)\n    - [Vector Search](./docs/en/image_recognition_pipeline/vector_search_en.md)\n    - [Hash Encoding](docs/zh_CN/training/PP-ShiTu/deep_hashing.md)\n  - PipeLine Inference and Deployment\n    - [Python Inference](docs/en/inference_deployment/python_deploy_en.md)\n    - [C++ Inference](deploy/cpp_shitu/readme_en.md)\n    - [Serving Deployment](docs/en/inference_deployment/recognition_serving_deploy_en.md)\n    - [Lite Deployment](docs/en/inference_deployment/paddle_lite_deploy_en.md)\n    - [Shitu Gallery Manager Tool](docs/zh_CN/deployment/PP-ShiTu/gallery_manager.md)\n- [Practical Ultra Light-weight image Classification solutions](./docs/en/PULC/PULC_train_en.md)\n  - [PULC Quick Start](docs/en/PULC/PULC_quickstart_en.md)\n  - [PULC Model Zoo](docs/en/PULC/PULC_model_list_en.md)\n    - [PULC Classification Model of Someone or Nobody](docs/en/PULC/PULC_person_exists_en.md)\n    - [PULC Recognition Model of Person Attribute](docs/en/PULC/PULC_person_attribute_en.md)\n    - [PULC Classification Model of Wearing or Unwearing Safety Helmet](docs/en/PULC/PULC_safety_helmet_en.md)\n    - [PULC Classification Model of Traffic Sign](docs/en/PULC/PULC_traffic_sign_en.md)\n    - [PULC Recognition Model of Vehicle Attribute](docs/en/PULC/PULC_vehicle_attribute_en.md)\n    - [PULC Classification Model of Containing or Uncontaining Car](docs/en/PULC/PULC_car_exists_en.md)\n    - [PULC Classification Model of Text Image Orientation](docs/en/PULC/PULC_text_image_orientation_en.md)\n    - [PULC Classification Model of Textline Orientation](docs/en/PULC/PULC_textline_orientation_en.md)\n    - [PULC Classification Model of Language](docs/en/PULC/PULC_language_classification_en.md)\n- PP Series Backbone\n    - [PP-HGNet](docs/en/models/PP-HGNet_en.md)\n    - [PP-LCNet](docs/en/models/PP-LCNet_en.md)\n    - [PP-LCNetv2](docs/en/models/PP-LCNetv2_en.md)\n- [Introduction to Image Recognition Systems](#Introduction_to_Image_Recognition_Systems)\n- [Image Recognition Demo images](#Rec_Demo_images)\n- [PULC demo images](#Clas_Demo_images)\n- Algorithms Introduction\n    - [Backbone Network and Pre-trained Model Library](./docs/en/algorithm_introduction/ImageNet_models_en.md)\n    - [Mainbody Detection](./docs/en/image_recognition_pipeline/mainbody_detection_en.md)\n    - [Feature Learning](./docs/en/image_recognition_pipeline/feature_extraction_en.md)\n    - [Vector Search](./deploy/vector_search/README.md)\n- Inference Model Prediction\n    - [Python Inference](./docs/en/inference_deployment/python_deploy_en.md)\n    - [C++ Classfication Inference](./deploy/cpp/readme_en.md)\n- Model Deploy (only support classification for now, recognition coming soon)\n    - [Hub Serving Deployment](./deploy/hubserving/readme_en.md)\n    - [Mobile Deployment](./deploy/lite/readme_en.md)\n    - [Inference Using whl](./docs/en/inference_deployment/whl_deploy_en.md)\n- Advanced Tutorial\n    - [Knowledge Distillation](./docs/en/advanced_tutorials/distillation/distillation_en.md)\n    - [Model Quantization](./docs/en/algorithm_introduction/model_prune_quantization_en.md)\n    - [Data Augmentation](./docs/en/advanced_tutorials/DataAugmentation_en.md)\n- [License](#License)\n- [Contribution](#Contribution)\n\n<a name=\"Introduction_to_PULC\"></a>\n## Introduction to Practical Ultra Light-weight image Classification solutions\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/19523330/173011854-b10fcd7a-b799-4dfd-a1cf-9504952a3c44.png\"  width = \"800\" />\n</div>\nPULC solutions consists of PP-LCNet light-weight backbone, SSLD pretrained models, Ensemble of Data Augmentation strategy and SKL-UGI knowledge distillation.\nPULC models inference within 3ms on CPU devices, with accuracy comparable with SwinTransformer. We also release 9 practical models covering pedestrian, vehicle and OCR.\n\n<a name=\"Introduction_to_Image_Recognition_Systems\"></a>\n## Introduction to Image Recognition Systems\n\n<div align=\"center\">\n<img src=\"./docs/images/structure.jpg\"  width = \"800\" />\n</div>\n\nPP-ShiTuV2 is a practical lightweight general image recognition system, which is mainly composed of three modules: mainbody detection model, feature extraction model and vector search tool. The system adopts a variety of strategies including backbone network, loss function, data augmentations, optimal hyperparameters, pre-training model, model pruning and quantization. Compared to V1, PP-ShiTuV2, Recall1 is improved by nearly 8 points. For more details, please refer to [PP-ShiTuV2 introduction](./docs/en/PPShiTu/PPShiTuV2_introduction.md).\nFor a new unknown category, there is no need to retrain the model, just prepare images of new category, extract features and update retrieval database and the category can be recognised.\n\n<a name=\"Rec_Demo_images\"></a>\n## PP-ShiTuV2 Demo images\n\n- Drinks recognition\n\n<div align=\"center\">\n<img src=\"docs/images/drink_demo.gif\">\n</div>\n\n\n- Product recognition\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769644-51604f80-d2d7-11eb-8290-c53b12a5c1f6.gif\"  width = \"400\" />\n</div>\n\n\n- Cartoon character recognition\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769746-6b019700-d2d7-11eb-86df-f1d710999ba6.gif\"  width = \"400\" />\n</div>\n\n\n- Logo recognition\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769837-7fde2a80-d2d7-11eb-9b69-04140e9d785f.gif\"  width = \"400\" />\n</div>\n\n\n\n- Car recognition\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769916-8ec4dd00-d2d7-11eb-8c60-42d89e25030c.gif\"  width = \"400\" />\n</div>\n\n\n<a name=\"Clas_Demo_images\"></a>\n## PULC demo images\n<div align=\"center\">\n<img src=\"docs/images/classification_en.gif\">\n</div>\n\n\n<a name=\"License\"></a>\n## License\nPaddleClas is released under the Apache 2.0 license <a href=\"https://github.com/PaddlePaddle/PaddleCLS/blob/master/LICENSE\">Apache 2.0 license</a>\n\n\n<a name=\"Contribution\"></a>\n## Contribution\nContributions are highly welcomed and we would really appreciate your feedback!!\n\n\n- Thank [nblib](https://github.com/nblib) to fix bug of RandErasing.\n- Thank [chenpy228](https://github.com/chenpy228) to fix some typos PaddleClas.\n- Thank [jm12138](https://github.com/jm12138) to add ViT, DeiT models and RepVGG models into PaddleClas.\n- Thank [FutureSI](https://aistudio.baidu.com/aistudio/personalcenter/thirdview/76563) to parse and summarize the PaddleClas code.\n"
        },
        {
          "name": "__init__.py",
          "type": "blob",
          "size": 0.689453125,
          "content": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__all__ = ['PaddleClas']\nfrom .paddleclas import PaddleClas\nfrom .ppcls.arch.backbone import *\n"
        },
        {
          "name": "benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "deploy",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "hubconf.py",
          "type": "blob",
          "size": 27.4365234375,
          "content": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ndependencies = ['paddle']\n\nimport paddle\nimport os\nimport sys\n\n\nclass _SysPathG(object):\n    \"\"\"\n    _SysPathG used to add/clean path for sys.path. Making sure minimal pkgs dependents by skiping parent dirs.\n\n    __enter__\n        add path into sys.path\n    __exit__\n        clean user's sys.path to avoid unexpect behaviors\n    \"\"\"\n\n    def __init__(self, path):\n        self.path = path\n\n    def __enter__(self, ):\n        sys.path.insert(0, self.path)\n\n    def __exit__(self, type, value, traceback):\n        _p = sys.path.pop(0)\n        assert _p == self.path, 'Make sure sys.path cleaning {} correctly.'.format(\n            self.path)\n\n\nwith _SysPathG(os.path.dirname(os.path.abspath(__file__)), ):\n    import ppcls\n    import ppcls.arch.backbone as backbone\n\n    def ppclas_init():\n        if ppcls.utils.logger._logger is None:\n            ppcls.utils.logger.init_logger()\n\n    ppclas_init()\n\n    def _load_pretrained_parameters(model, name):\n        url = 'https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/{}_pretrained.pdparams'.format(\n            name)\n        path = paddle.utils.download.get_weights_path_from_url(url)\n        model.set_state_dict(paddle.load(path))\n        return model\n\n    def alexnet(pretrained=False, **kwargs):\n        \"\"\"\n        AlexNet\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `AlexNet` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.AlexNet(**kwargs)\n\n        return model\n\n    def vgg11(pretrained=False, **kwargs):\n        \"\"\"\n        VGG11\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                stop_grad_layers: int=0. The parameters in blocks which index larger than `stop_grad_layers`, will be set `param.trainable=False`\n        Returns:\n            model: nn.Layer. Specific `VGG11` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.VGG11(**kwargs)\n\n        return model\n\n    def vgg13(pretrained=False, **kwargs):\n        \"\"\"\n        VGG13\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                stop_grad_layers: int=0. The parameters in blocks which index larger than `stop_grad_layers`, will be set `param.trainable=False`\n        Returns:\n            model: nn.Layer. Specific `VGG13` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.VGG13(**kwargs)\n\n        return model\n\n    def vgg16(pretrained=False, **kwargs):\n        \"\"\"\n        VGG16\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                stop_grad_layers: int=0. The parameters in blocks which index larger than `stop_grad_layers`, will be set `param.trainable=False`\n        Returns:\n            model: nn.Layer. Specific `VGG16` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.VGG16(**kwargs)\n\n        return model\n\n    def vgg19(pretrained=False, **kwargs):\n        \"\"\"\n        VGG19\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                stop_grad_layers: int=0. The parameters in blocks which index larger than `stop_grad_layers`, will be set `param.trainable=False`\n        Returns:\n            model: nn.Layer. Specific `VGG19` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.VGG19(**kwargs)\n\n        return model\n\n    def resnet18(pretrained=False, **kwargs):\n        \"\"\"\n        ResNet18\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                input_image_channel: int=3. The number of input image channels\n                data_format: str='NCHW'. The data format of batch input images, should in ('NCHW', 'NHWC')\n        Returns:\n            model: nn.Layer. Specific `ResNet18` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNet18(**kwargs)\n\n        return model\n\n    def resnet34(pretrained=False, **kwargs):\n        \"\"\"\n        ResNet34\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                input_image_channel: int=3. The number of input image channels\n                data_format: str='NCHW'. The data format of batch input images, should in ('NCHW', 'NHWC')\n        Returns:\n            model: nn.Layer. Specific `ResNet34` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNet34(**kwargs)\n\n        return model\n\n    def resnet50(pretrained=False, **kwargs):\n        \"\"\"\n        ResNet50\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                input_image_channel: int=3. The number of input image channels\n                data_format: str='NCHW'. The data format of batch input images, should in ('NCHW', 'NHWC')\n        Returns:\n            model: nn.Layer. Specific `ResNet50` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNet50(**kwargs)\n\n        return model\n\n    def resnet101(pretrained=False, **kwargs):\n        \"\"\"\n        ResNet101\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                input_image_channel: int=3. The number of input image channels\n                data_format: str='NCHW'. The data format of batch input images, should in ('NCHW', 'NHWC')\n        Returns:\n            model: nn.Layer. Specific `ResNet101` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNet101(**kwargs)\n\n        return model\n\n    def resnet152(pretrained=False, **kwargs):\n        \"\"\"\n        ResNet152\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                input_image_channel: int=3. The number of input image channels\n                data_format: str='NCHW'. The data format of batch input images, should in ('NCHW', 'NHWC')\n        Returns:\n            model: nn.Layer. Specific `ResNet152` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNet152(**kwargs)\n\n        return model\n\n    def squeezenet1_0(pretrained=False, **kwargs):\n        \"\"\"\n        SqueezeNet1_0\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `SqueezeNet1_0` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.SqueezeNet1_0(**kwargs)\n\n        return model\n\n    def squeezenet1_1(pretrained=False, **kwargs):\n        \"\"\"\n        SqueezeNet1_1\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `SqueezeNet1_1` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.SqueezeNet1_1(**kwargs)\n\n        return model\n\n    def densenet121(pretrained=False, **kwargs):\n        \"\"\"\n        DenseNet121\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                dropout: float=0. Probability of setting units to zero.\n                bn_size: int=4. The number of channals per group\n        Returns:\n            model: nn.Layer. Specific `DenseNet121` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.DenseNet121(**kwargs)\n\n        return model\n\n    def densenet161(pretrained=False, **kwargs):\n        \"\"\"\n        DenseNet161\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                dropout: float=0. Probability of setting units to zero.\n                bn_size: int=4. The number of channals per group\n        Returns:\n            model: nn.Layer. Specific `DenseNet161` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.DenseNet161(**kwargs)\n\n        return model\n\n    def densenet169(pretrained=False, **kwargs):\n        \"\"\"\n        DenseNet169\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                dropout: float=0. Probability of setting units to zero.\n                bn_size: int=4. The number of channals per group\n        Returns:\n            model: nn.Layer. Specific `DenseNet169` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.DenseNet169(**kwargs)\n\n        return model\n\n    def densenet201(pretrained=False, **kwargs):\n        \"\"\"\n        DenseNet201\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                dropout: float=0. Probability of setting units to zero.\n                bn_size: int=4. The number of channals per group\n        Returns:\n            model: nn.Layer. Specific `DenseNet201` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.DenseNet201(**kwargs)\n\n        return model\n\n    def densenet264(pretrained=False, **kwargs):\n        \"\"\"\n        DenseNet264\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                dropout: float=0. Probability of setting units to zero.\n                bn_size: int=4. The number of channals per group\n        Returns:\n            model: nn.Layer. Specific `DenseNet264` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.DenseNet264(**kwargs)\n\n        return model\n\n    def inceptionv3(pretrained=False, **kwargs):\n        \"\"\"\n        InceptionV3\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `InceptionV3` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.InceptionV3(**kwargs)\n\n        return model\n\n    def inceptionv4(pretrained=False, **kwargs):\n        \"\"\"\n        InceptionV4\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `InceptionV4` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.InceptionV4(**kwargs)\n\n        return model\n\n    def googlenet(pretrained=False, **kwargs):\n        \"\"\"\n        GoogLeNet\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `GoogLeNet` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.GoogLeNet(**kwargs)\n\n        return model\n\n    def shufflenetv2_x0_25(pretrained=False, **kwargs):\n        \"\"\"\n        ShuffleNetV2_x0_25\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ShuffleNetV2_x0_25` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ShuffleNetV2_x0_25(**kwargs)\n\n        return model\n\n    def mobilenetv1(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV1\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV1` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV1(**kwargs)\n\n        return model\n\n    def mobilenetv1_x0_25(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV1_x0_25\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV1_x0_25` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV1_x0_25(**kwargs)\n\n        return model\n\n    def mobilenetv1_x0_5(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV1_x0_5\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV1_x0_5` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV1_x0_5(**kwargs)\n\n        return model\n\n    def mobilenetv1_x0_75(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV1_x0_75\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV1_x0_75` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV1_x0_75(**kwargs)\n\n        return model\n\n    def mobilenetv2_x0_25(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV2_x0_25\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV2_x0_25` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV2_x0_25(**kwargs)\n\n        return model\n\n    def mobilenetv2_x0_5(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV2_x0_5\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV2_x0_5` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV2_x0_5(**kwargs)\n\n        return model\n\n    def mobilenetv2_x0_75(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV2_x0_75\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV2_x0_75` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV2_x0_75(**kwargs)\n\n        return model\n\n    def mobilenetv2_x1_5(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV2_x1_5\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV2_x1_5` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV2_x1_5(**kwargs)\n\n        return model\n\n    def mobilenetv2_x2_0(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV2_x2_0\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV2_x2_0` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV2_x2_0(**kwargs)\n\n        return model\n\n    def mobilenetv3_large_x0_35(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_large_x0_35\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_large_x0_35` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_large_x0_35(**kwargs)\n\n        return model\n\n    def mobilenetv3_large_x0_5(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_large_x0_5\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_large_x0_5` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_large_x0_5(**kwargs)\n\n        return model\n\n    def mobilenetv3_large_x0_75(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_large_x0_75\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_large_x0_75` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_large_x0_75(**kwargs)\n\n        return model\n\n    def mobilenetv3_large_x1_0(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_large_x1_0\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_large_x1_0` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_large_x1_0(**kwargs)\n\n        return model\n\n    def mobilenetv3_large_x1_25(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_large_x1_25\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_large_x1_25` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_large_x1_25(**kwargs)\n\n        return model\n\n    def mobilenetv3_small_x0_35(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_small_x0_35\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_small_x0_35` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_small_x0_35(**kwargs)\n\n        return model\n\n    def mobilenetv3_small_x0_5(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_small_x0_5\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_small_x0_5` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_small_x0_5(**kwargs)\n\n        return model\n\n    def mobilenetv3_small_x0_75(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_small_x0_75\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_small_x0_75` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_small_x0_75(**kwargs)\n\n        return model\n\n    def mobilenetv3_small_x1_0(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_small_x1_0\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_small_x1_0` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_small_x1_0(**kwargs)\n\n        return model\n\n    def mobilenetv3_small_x1_25(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_small_x1_25\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_small_x1_25` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_small_x1_25(**kwargs)\n\n        return model\n\n    def resnext101_32x4d(pretrained=False, **kwargs):\n        \"\"\"\n        ResNeXt101_32x4d\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt101_32x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNeXt101_32x4d(**kwargs)\n\n        return model\n\n    def resnext101_64x4d(pretrained=False, **kwargs):\n        \"\"\"\n        ResNeXt101_64x4d\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt101_64x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNeXt101_64x4d(**kwargs)\n\n        return model\n\n    def resnext152_32x4d(pretrained=False, **kwargs):\n        \"\"\"\n        ResNeXt152_32x4d\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt152_32x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNeXt152_32x4d(**kwargs)\n\n        return model\n\n    def resnext152_64x4d(pretrained=False, **kwargs):\n        \"\"\"\n        ResNeXt152_64x4d\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt152_64x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNeXt152_64x4d(**kwargs)\n\n        return model\n\n    def resnext50_32x4d(pretrained=False, **kwargs):\n        \"\"\"\n        ResNeXt50_32x4d\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt50_32x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNeXt50_32x4d(**kwargs)\n\n        return model\n\n    def resnext50_64x4d(pretrained=False, **kwargs):\n        \"\"\"\n        ResNeXt50_64x4d\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt50_64x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNeXt50_64x4d(**kwargs)\n\n        return model\n\n    def darknet53(pretrained=False, **kwargs):\n        \"\"\"\n        DarkNet53\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt50_64x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.DarkNet53(**kwargs)\n\n        return model\n"
        },
        {
          "name": "paddleclas.py",
          "type": "blob",
          "size": 34.1220703125,
          "content": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom typing import Union, Generator\nimport argparse\nimport shutil\nimport textwrap\nimport tarfile\nimport requests\nfrom functools import partial\nfrom difflib import SequenceMatcher\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom prettytable import PrettyTable\nimport paddle\n\nfrom .ppcls.arch import backbone\nfrom .ppcls.utils import logger\n\nfrom .deploy.python.predict_cls import ClsPredictor\nfrom .deploy.python.predict_system import SystemPredictor\nfrom .deploy.python.build_gallery import GalleryBuilder\nfrom .deploy.utils.get_image_list import get_image_list\nfrom .deploy.utils import config\n\n# for the PaddleClas Project\nfrom . import deploy\nfrom . import ppcls\n\n# for building model with loading pretrained weights from backbone\nlogger.init_logger()\n\n__all__ = [\"PaddleClas\"]\n\nBASE_DIR = os.path.expanduser(\"~/.paddleclas/\")\nBASE_INFERENCE_MODEL_DIR = os.path.join(BASE_DIR, \"inference_model\")\nBASE_IMAGES_DIR = os.path.join(BASE_DIR, \"images\")\nIMN_MODEL_BASE_DOWNLOAD_URL = \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/inference/{}_infer.tar\"\nIMN_MODEL_SERIES = {\n    \"AlexNet\": [\"AlexNet\"],\n    \"ConvNeXt\": [\"ConvNeXt_tiny\"],\n    \"CSPNet\": [\"CSPDarkNet53\"],\n    \"CSWinTransformer\": [\n        \"CSWinTransformer_tiny_224\", \"CSWinTransformer_small_224\",\n        \"CSWinTransformer_base_224\", \"CSWinTransformer_base_384\",\n        \"CSWinTransformer_large_224\", \"CSWinTransformer_large_384\"\n    ],\n    \"DarkNet\": [\"DarkNet53\"],\n    \"DeiT\": [\n        \"DeiT_base_distilled_patch16_224\", \"DeiT_base_distilled_patch16_384\",\n        \"DeiT_base_patch16_224\", \"DeiT_base_patch16_384\",\n        \"DeiT_small_distilled_patch16_224\", \"DeiT_small_patch16_224\",\n        \"DeiT_tiny_distilled_patch16_224\", \"DeiT_tiny_patch16_224\"\n    ],\n    \"DenseNet\": [\n        \"DenseNet121\", \"DenseNet161\", \"DenseNet169\", \"DenseNet201\",\n        \"DenseNet264\"\n    ],\n    \"DLA\": [\n        \"DLA46_c\", \"DLA60x_c\", \"DLA34\", \"DLA60\", \"DLA60x\", \"DLA102\", \"DLA102x\",\n        \"DLA102x2\", \"DLA169\"\n    ],\n    \"DPN\": [\"DPN68\", \"DPN92\", \"DPN98\", \"DPN107\", \"DPN131\"],\n    \"EfficientNet\": [\n        \"EfficientNetB0\", \"EfficientNetB0_small\", \"EfficientNetB1\",\n        \"EfficientNetB2\", \"EfficientNetB3\", \"EfficientNetB4\", \"EfficientNetB5\",\n        \"EfficientNetB6\", \"EfficientNetB7\"\n    ],\n    \"ESNet\": [\"ESNet_x0_25\", \"ESNet_x0_5\", \"ESNet_x0_75\", \"ESNet_x1_0\"],\n    \"GhostNet\":\n    [\"GhostNet_x0_5\", \"GhostNet_x1_0\", \"GhostNet_x1_3\", \"GhostNet_x1_3_ssld\"],\n    \"HarDNet\": [\"HarDNet39_ds\", \"HarDNet68_ds\", \"HarDNet68\", \"HarDNet85\"],\n    \"HRNet\": [\n        \"HRNet_W18_C\", \"HRNet_W30_C\", \"HRNet_W32_C\", \"HRNet_W40_C\",\n        \"HRNet_W44_C\", \"HRNet_W48_C\", \"HRNet_W64_C\", \"HRNet_W18_C_ssld\",\n        \"HRNet_W48_C_ssld\"\n    ],\n    \"Inception\": [\"GoogLeNet\", \"InceptionV3\", \"InceptionV4\"],\n    \"LeViT\":\n    [\"LeViT_128S\", \"LeViT_128\", \"LeViT_192\", \"LeViT_256\", \"LeViT_384\"],\n    \"MixNet\": [\"MixNet_S\", \"MixNet_M\", \"MixNet_L\"],\n    \"MobileNetV1\": [\n        \"MobileNetV1_x0_25\", \"MobileNetV1_x0_5\", \"MobileNetV1_x0_75\",\n        \"MobileNetV1\", \"MobileNetV1_ssld\"\n    ],\n    \"MobileNetV2\": [\n        \"MobileNetV2_x0_25\", \"MobileNetV2_x0_5\", \"MobileNetV2_x0_75\",\n        \"MobileNetV2\", \"MobileNetV2_x1_5\", \"MobileNetV2_x2_0\",\n        \"MobileNetV2_ssld\"\n    ],\n    \"MobileNetV3\": [\n        \"MobileNetV3_small_x0_35\", \"MobileNetV3_small_x0_5\",\n        \"MobileNetV3_small_x0_75\", \"MobileNetV3_small_x1_0\",\n        \"MobileNetV3_small_x1_25\", \"MobileNetV3_large_x0_35\",\n        \"MobileNetV3_large_x0_5\", \"MobileNetV3_large_x0_75\",\n        \"MobileNetV3_large_x1_0\", \"MobileNetV3_large_x1_25\",\n        \"MobileNetV3_small_x1_0_ssld\", \"MobileNetV3_large_x1_0_ssld\"\n    ],\n    \"MobileViT\": [\"MobileViT_XXS\", \"MobileViT_XS\", \"MobileViT_S\"],\n    \"PeleeNet\": [\"PeleeNet\"],\n    \"PPHGNet\": [\n        \"PPHGNet_tiny\",\n        \"PPHGNet_small\",\n        \"PPHGNet_tiny_ssld\",\n        \"PPHGNet_small_ssld\",\n    ],\n    \"PPLCNet\": [\n        \"PPLCNet_x0_25\", \"PPLCNet_x0_35\", \"PPLCNet_x0_5\", \"PPLCNet_x0_75\",\n        \"PPLCNet_x1_0\", \"PPLCNet_x1_5\", \"PPLCNet_x2_0\", \"PPLCNet_x2_5\"\n    ],\n    \"PPLCNetV2\": [\"PPLCNetV2_base\"],\n    \"PVTV2\": [\n        \"PVT_V2_B0\", \"PVT_V2_B1\", \"PVT_V2_B2\", \"PVT_V2_B2_Linear\", \"PVT_V2_B3\",\n        \"PVT_V2_B4\", \"PVT_V2_B5\"\n    ],\n    \"RedNet\": [\"RedNet26\", \"RedNet38\", \"RedNet50\", \"RedNet101\", \"RedNet152\"],\n    \"RegNet\": [\"RegNetX_4GF\"],\n    \"Res2Net\": [\n        \"Res2Net50_14w_8s\", \"Res2Net50_26w_4s\", \"Res2Net50_vd_26w_4s\",\n        \"Res2Net200_vd_26w_4s\", \"Res2Net101_vd_26w_4s\",\n        \"Res2Net50_vd_26w_4s_ssld\", \"Res2Net101_vd_26w_4s_ssld\",\n        \"Res2Net200_vd_26w_4s_ssld\"\n    ],\n    \"ResNeSt\": [\"ResNeSt50\", \"ResNeSt50_fast_1s1x64d\"],\n    \"ResNet\": [\n        \"ResNet18\", \"ResNet18_vd\", \"ResNet34\", \"ResNet34_vd\", \"ResNet50\",\n        \"ResNet50_vc\", \"ResNet50_vd\", \"ResNet50_vd_v2\", \"ResNet101\",\n        \"ResNet101_vd\", \"ResNet152\", \"ResNet152_vd\", \"ResNet200_vd\",\n        \"ResNet34_vd_ssld\", \"ResNet50_vd_ssld\", \"ResNet50_vd_ssld_v2\",\n        \"ResNet101_vd_ssld\", \"Fix_ResNet50_vd_ssld_v2\", \"ResNet50_ACNet_deploy\"\n    ],\n    \"ResNeXt\": [\n        \"ResNeXt50_32x4d\", \"ResNeXt50_vd_32x4d\", \"ResNeXt50_64x4d\",\n        \"ResNeXt50_vd_64x4d\", \"ResNeXt101_32x4d\", \"ResNeXt101_vd_32x4d\",\n        \"ResNeXt101_32x8d_wsl\", \"ResNeXt101_32x16d_wsl\",\n        \"ResNeXt101_32x32d_wsl\", \"ResNeXt101_32x48d_wsl\",\n        \"Fix_ResNeXt101_32x48d_wsl\", \"ResNeXt101_64x4d\", \"ResNeXt101_vd_64x4d\",\n        \"ResNeXt152_32x4d\", \"ResNeXt152_vd_32x4d\", \"ResNeXt152_64x4d\",\n        \"ResNeXt152_vd_64x4d\"\n    ],\n    \"ReXNet\":\n    [\"ReXNet_1_0\", \"ReXNet_1_3\", \"ReXNet_1_5\", \"ReXNet_2_0\", \"ReXNet_3_0\"],\n    \"SENet\": [\n        \"SENet154_vd\", \"SE_HRNet_W64_C_ssld\", \"SE_ResNet18_vd\",\n        \"SE_ResNet34_vd\", \"SE_ResNet50_vd\", \"SE_ResNeXt50_32x4d\",\n        \"SE_ResNeXt50_vd_32x4d\", \"SE_ResNeXt101_32x4d\"\n    ],\n    \"ShuffleNetV2\": [\n        \"ShuffleNetV2_swish\", \"ShuffleNetV2_x0_25\", \"ShuffleNetV2_x0_33\",\n        \"ShuffleNetV2_x0_5\", \"ShuffleNetV2_x1_0\", \"ShuffleNetV2_x1_5\",\n        \"ShuffleNetV2_x2_0\"\n    ],\n    \"SqueezeNet\": [\"SqueezeNet1_0\", \"SqueezeNet1_1\"],\n    \"SwinTransformer\": [\n        \"SwinTransformer_large_patch4_window7_224_22kto1k\",\n        \"SwinTransformer_large_patch4_window12_384_22kto1k\",\n        \"SwinTransformer_base_patch4_window7_224_22kto1k\",\n        \"SwinTransformer_base_patch4_window12_384_22kto1k\",\n        \"SwinTransformer_base_patch4_window12_384\",\n        \"SwinTransformer_base_patch4_window7_224\",\n        \"SwinTransformer_small_patch4_window7_224\",\n        \"SwinTransformer_tiny_patch4_window7_224\"\n    ],\n    \"Twins\": [\n        \"pcpvt_small\", \"pcpvt_base\", \"pcpvt_large\", \"alt_gvt_small\",\n        \"alt_gvt_base\", \"alt_gvt_large\"\n    ],\n    \"TNT\": [\"TNT_small\"],\n    \"VAN\": [\"VAN_B0\"],\n    \"VGG\": [\"VGG11\", \"VGG13\", \"VGG16\", \"VGG19\"],\n    \"VisionTransformer\": [\n        \"ViT_base_patch16_224\", \"ViT_base_patch16_384\", \"ViT_base_patch32_384\",\n        \"ViT_large_patch16_224\", \"ViT_large_patch16_384\",\n        \"ViT_large_patch32_384\", \"ViT_small_patch16_224\"\n    ],\n    \"Xception\": [\n        \"Xception41\", \"Xception41_deeplab\", \"Xception65\", \"Xception65_deeplab\",\n        \"Xception71\"\n    ]\n}\n\nPULC_MODEL_BASE_DOWNLOAD_URL = \"https://paddleclas.bj.bcebos.com/models/PULC/inference/{}_infer.tar\"\nPULC_MODELS = [\n    \"car_exists\", \"language_classification\", \"person_attribute\",\n    \"person_exists\", \"safety_helmet\", \"text_image_orientation\",\n    \"image_orientation\", \"textline_orientation\", \"traffic_sign\",\n    \"vehicle_attribute\", \"table_attribute\", \"clarity_assessment\"\n]\n\nSHITU_MODEL_BASE_DOWNLOAD_URL = \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/rec/models/inference/{}_infer.tar\"\nSHITU_MODELS = [\n    # \"picodet_PPLCNet_x2_5_mainbody_lite_v1.0\",  # ShiTuV1(V2)_mainbody_det\n    # \"general_PPLCNet_x2_5_lite_v1.0\"  # ShiTuV1_general_rec\n    # \"PP-ShiTuV2/general_PPLCNetV2_base_pretrained_v1.0\",  # ShiTuV2_general_rec TODO(hesensen): add lite model\n    \"PP-ShiTuV2\"\n]\n\n\nclass ImageTypeError(Exception):\n    \"\"\"ImageTypeError.\n    \"\"\"\n\n    def __init__(self, message=\"\"):\n        super().__init__(message)\n\n\nclass InputModelError(Exception):\n    \"\"\"InputModelError.\n    \"\"\"\n\n    def __init__(self, message=\"\"):\n        super().__init__(message)\n\n\ndef init_config(model_type, model_name, inference_model_dir, **kwargs):\n\n    if kwargs.get(\"build_gallery\", False):\n        cfg_path = \"deploy/configs/inference_general.yaml\"\n    elif model_type == \"pulc\":\n        cfg_path = f\"deploy/configs/PULC/{model_name}/inference_{model_name}.yaml\"\n    elif model_type == \"shitu\":\n        cfg_path = \"deploy/configs/inference_general.yaml\"\n    else:\n        cfg_path = \"deploy/configs/inference_cls.yaml\"\n\n    __dir__ = os.path.dirname(__file__)\n    cfg_path = os.path.join(__dir__, cfg_path)\n    cfg = config.get_config(\n        cfg_path, overrides=kwargs.get(\"override\", None), show=False)\n    if cfg.Global.get(\"inference_model_dir\"):\n        cfg.Global.inference_model_dir = inference_model_dir\n    else:\n        cfg.Global.rec_inference_model_dir = os.path.join(\n            inference_model_dir,\n            \"PP-ShiTuV2/general_PPLCNetV2_base_pretrained_v1.0\")\n        cfg.Global.det_inference_model_dir = os.path.join(\n            inference_model_dir, \"picodet_PPLCNet_x2_5_mainbody_lite_v1.0\")\n\n    if \"batch_size\" in kwargs and kwargs[\"batch_size\"]:\n        cfg.Global.batch_size = kwargs[\"batch_size\"]\n\n    if \"use_gpu\" in kwargs and kwargs[\"use_gpu\"] is not None:\n        cfg.Global.use_gpu = kwargs[\"use_gpu\"]\n    if cfg.Global.use_gpu and not paddle.device.is_compiled_with_cuda():\n        msg = \"The current running environment does not support the use of GPU. CPU has been used instead.\"\n        logger.warning(msg)\n        cfg.Global.use_gpu = False\n\n    if \"infer_imgs\" in kwargs and kwargs[\"infer_imgs\"]:\n        cfg.Global.infer_imgs = kwargs[\"infer_imgs\"]\n    if \"index_dir\" in kwargs and kwargs[\"index_dir\"]:\n        cfg.IndexProcess.index_dir = kwargs[\"index_dir\"]\n    if \"data_file\" in kwargs and kwargs[\"data_file\"]:\n        cfg.IndexProcess.data_file = kwargs[\"data_file\"]\n    if \"enable_mkldnn\" in kwargs and kwargs[\"enable_mkldnn\"] is not None:\n        cfg.Global.enable_mkldnn = kwargs[\"enable_mkldnn\"]\n    if \"cpu_num_threads\" in kwargs and kwargs[\"cpu_num_threads\"]:\n        cfg.Global.cpu_num_threads = kwargs[\"cpu_num_threads\"]\n    if \"use_fp16\" in kwargs and kwargs[\"use_fp16\"] is not None:\n        cfg.Global.use_fp16 = kwargs[\"use_fp16\"]\n    if \"use_tensorrt\" in kwargs and kwargs[\"use_tensorrt\"] is not None:\n        cfg.Global.use_tensorrt = kwargs[\"use_tensorrt\"]\n    if \"gpu_mem\" in kwargs and kwargs[\"gpu_mem\"]:\n        cfg.Global.gpu_mem = kwargs[\"gpu_mem\"]\n    if \"resize_short\" in kwargs and kwargs[\"resize_short\"]:\n        cfg.PreProcess.transform_ops[0][\"ResizeImage\"][\n            \"resize_short\"] = kwargs[\"resize_short\"]\n    if \"crop_size\" in kwargs and kwargs[\"crop_size\"]:\n        cfg.PreProcess.transform_ops[1][\"CropImage\"][\"size\"] = kwargs[\n            \"crop_size\"]\n\n    # TODO(gaotingquan): not robust\n    if cfg.get(\"PostProcess\"):\n        if \"Topk\" in cfg.PostProcess:\n            if \"topk\" in kwargs and kwargs[\"topk\"]:\n                cfg.PostProcess.Topk.topk = kwargs[\"topk\"]\n            if \"class_id_map_file\" in kwargs and kwargs[\"class_id_map_file\"]:\n                cfg.PostProcess.Topk.class_id_map_file = kwargs[\n                    \"class_id_map_file\"]\n            else:\n                class_id_map_file_path = os.path.relpath(\n                    cfg.PostProcess.Topk.class_id_map_file, \"../\")\n                cfg.PostProcess.Topk.class_id_map_file = os.path.join(\n                    __dir__, class_id_map_file_path)\n        if \"ThreshOutput\" in cfg.PostProcess:\n            if \"thresh\" in kwargs and kwargs[\"thresh\"]:\n                cfg.PostProcess.ThreshOutput.thresh = kwargs[\"thresh\"]\n            if \"class_id_map_file\" in kwargs and kwargs[\"class_id_map_file\"]:\n                cfg.PostProcess.ThreshOutput[\"class_id_map_file\"] = kwargs[\n                    \"class_id_map_file\"]\n            elif \"class_id_map_file\" in cfg.PostProcess.ThreshOutput:\n                class_id_map_file_path = os.path.relpath(\n                    cfg.PostProcess.ThreshOutput.class_id_map_file, \"../\")\n                cfg.PostProcess.ThreshOutput.class_id_map_file = os.path.join(\n                    __dir__, class_id_map_file_path)\n        if \"VehicleAttribute\" in cfg.PostProcess:\n            if \"color_threshold\" in kwargs and kwargs[\"color_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"color_threshold\"]\n            if \"type_threshold\" in kwargs and kwargs[\"type_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.type_threshold = kwargs[\n                    \"type_threshold\"]\n        if \"TableAttribute\" in cfg.PostProcess:\n            if \"source_threshold\" in kwargs and kwargs[\"source_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"source_threshold\"]\n            if \"number_threshold\" in kwargs and kwargs[\"number_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"number_threshold\"]\n            if \"color_threshold\" in kwargs and kwargs[\"color_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"color_threshold\"]\n            if \"clarity_threshold\" in kwargs and kwargs[\"clarity_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"clarity_threshold\"]\n            if \"obstruction_threshold\" in kwargs and kwargs[\n                    \"obstruction_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"obstruction_threshold\"]\n            if \"angle_threshold\" in kwargs and kwargs[\"angle_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"angle_threshold\"]\n    if \"save_dir\" in kwargs and kwargs[\"save_dir\"]:\n        cfg.PostProcess.SavePreLabel.save_dir = kwargs[\"save_dir\"]\n\n    return cfg\n\n\ndef args_cfg():\n    def str2bool(v):\n        return v.lower() in (\"true\", \"t\", \"1\")\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--infer_imgs\",\n        type=str,\n        required=False,\n        help=\"The image(s) to be predicted.\")\n    parser.add_argument(\n        \"--model_name\", type=str, help=\"The model name to be used.\")\n    parser.add_argument(\n        \"--predict_type\",\n        type=str,\n        default=\"cls\",\n        help=\"The predict type to be selected.\")\n    parser.add_argument(\n        \"--inference_model_dir\",\n        type=str,\n        help=\"The directory of model files. Valid when model_name not specifed.\"\n    )\n    parser.add_argument(\n        \"--index_dir\",\n        type=str,\n        required=False,\n        help=\"The index directory path.\")\n    parser.add_argument(\n        \"--data_file\", type=str, required=False, help=\"The label file path.\")\n    parser.add_argument(\"--use_gpu\", type=str2bool, help=\"Whether use GPU.\")\n    parser.add_argument(\n        \"--gpu_mem\",\n        type=int,\n        help=\"The memory size of GPU allocated to predict.\")\n    parser.add_argument(\n        \"--enable_mkldnn\",\n        type=str2bool,\n        help=\"Whether use MKLDNN. Valid when use_gpu is False\")\n    parser.add_argument(\n        \"--cpu_num_threads\",\n        type=int,\n        help=\"The threads number when predicting on CPU.\")\n    parser.add_argument(\n        \"--use_tensorrt\",\n        type=str2bool,\n        help=\"Whether use TensorRT to accelerate.\")\n    parser.add_argument(\n        \"--use_fp16\", type=str2bool, help=\"Whether use FP16 to predict.\")\n    parser.add_argument(\"--batch_size\", type=int, help=\"Batch size.\")\n    parser.add_argument(\n        \"--topk\",\n        type=int,\n        help=\"Return topk score(s) and corresponding results when Topk postprocess is used.\"\n    )\n    parser.add_argument(\n        \"--class_id_map_file\",\n        type=str,\n        help=\"The path of file that map class_id and label.\")\n    parser.add_argument(\n        \"--threshold\",\n        type=float,\n        help=\"The threshold of ThreshOutput when postprocess is used.\")\n    parser.add_argument(\"--color_threshold\", type=float, help=\"\")\n    parser.add_argument(\"--type_threshold\", type=float, help=\"\")\n    parser.add_argument(\n        \"--save_dir\",\n        type=str,\n        help=\"The directory to save prediction results as pre-label.\")\n    parser.add_argument(\n        \"--resize_short\", type=int, help=\"Resize according to short size.\")\n    parser.add_argument(\"--crop_size\", type=int, help=\"Centor crop size.\")\n    parser.add_argument(\n        \"--build_gallery\",\n        type=str2bool,\n        default=False,\n        help=\"Whether build gallery.\")\n    parser.add_argument(\n        '-o',\n        '--override',\n        action='append',\n        default=[],\n        help='config options to be overridden')\n    args = parser.parse_args()\n    return vars(args)\n\n\ndef print_info():\n    \"\"\"Print list of supported models in formatted.\n    \"\"\"\n    imn_table = PrettyTable([\"IMN Model Series\", \"Model Name\"])\n    pulc_table = PrettyTable([\"PULC Models\"])\n    shitu_table = PrettyTable([\"PP-ShiTu Models\"])\n    try:\n        sz = os.get_terminal_size()\n        total_width = sz.columns\n        first_width = 30\n        second_width = total_width - first_width if total_width > 50 else 10\n    except OSError:\n        total_width = 100\n        second_width = 100\n    for series in IMN_MODEL_SERIES:\n        names = textwrap.fill(\n            \"  \".join(IMN_MODEL_SERIES[series]), width=second_width)\n        imn_table.add_row([series, names])\n\n    table_width = len(str(imn_table).split(\"\\n\")[0])\n    pulc_table.add_row([\n        textwrap.fill(\n            \"  \".join(PULC_MODELS), width=total_width).center(table_width - 4)\n    ])\n    shitu_table.add_row([\n        textwrap.fill(\n            \"  \".join(SHITU_MODELS), width=total_width).center(table_width - 4)\n    ])\n\n    print(\"{}\".format(\"-\" * table_width))\n    print(\"Models supported by PaddleClas\".center(table_width))\n    print(imn_table)\n    print(pulc_table)\n    print(shitu_table)\n    print(\"Powered by PaddlePaddle!\".rjust(table_width))\n    print(\"{}\".format(\"-\" * table_width))\n\n\ndef get_imn_model_names():\n    \"\"\"Get the model names list.\n    \"\"\"\n    model_names = []\n    for series in IMN_MODEL_SERIES:\n        model_names += (IMN_MODEL_SERIES[series])\n    return model_names\n\n\ndef similar_model_names(name=\"\", names=[], thresh=0.1, topk=5):\n    \"\"\"Find the most similar topk model names.\n    \"\"\"\n    scores = []\n    for idx, n in enumerate(names):\n        if n.startswith(\"__\"):\n            continue\n        score = SequenceMatcher(None, n.lower(), name.lower()).quick_ratio()\n        if score > thresh:\n            scores.append((idx, score))\n    scores.sort(key=lambda x: x[1], reverse=True)\n    similar_names = [names[s[0]] for s in scores[:min(topk, len(scores))]]\n    return similar_names\n\n\ndef download_with_progressbar(url, save_path):\n    \"\"\"Download from url with progressbar.\n    \"\"\"\n    if os.path.isfile(save_path):\n        os.remove(save_path)\n    response = requests.get(url, stream=True)\n    total_size_in_bytes = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024  # 1 Kibibyte\n    progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n    with open(save_path, \"wb\") as file:\n        for data in response.iter_content(block_size):\n            progress_bar.update(len(data))\n            file.write(data)\n    progress_bar.close()\n    if total_size_in_bytes == 0 or progress_bar.n != total_size_in_bytes or not os.path.isfile(\n            save_path):\n        raise Exception(\n            f\"Something went wrong while downloading file from {url}\")\n\n\ndef check_model_file(model_type, model_name):\n    \"\"\"Check the model files exist and download and untar when no exist.\n    \"\"\"\n    if model_type == \"pulc\":\n        storage_directory = partial(os.path.join, BASE_INFERENCE_MODEL_DIR,\n                                    \"PULC\", model_name)\n        url = PULC_MODEL_BASE_DOWNLOAD_URL.format(model_name)\n    elif model_type == \"shitu\":\n        storage_directory = partial(os.path.join, BASE_INFERENCE_MODEL_DIR,\n                                    \"PP-ShiTu\", model_name)\n        url = SHITU_MODEL_BASE_DOWNLOAD_URL.format(model_name)\n    else:\n        storage_directory = partial(os.path.join, BASE_INFERENCE_MODEL_DIR,\n                                    \"IMN\", model_name)\n        url = IMN_MODEL_BASE_DOWNLOAD_URL.format(model_name)\n\n    tar_file_name_list = [\n        \"inference.pdiparams\", \"inference.pdiparams.info\", \"inference.pdmodel\"\n    ]\n    model_file_path = storage_directory(\"inference.pdmodel\")\n    params_file_path = storage_directory(\"inference.pdiparams\")\n    if not os.path.exists(model_file_path) or not os.path.exists(\n            params_file_path):\n        tmp_path = storage_directory(url.split(\"/\")[-1])\n        logger.info(f\"download {url} to {tmp_path}\")\n        os.makedirs(storage_directory(), exist_ok=True)\n        download_with_progressbar(url, tmp_path)\n        with tarfile.open(tmp_path, \"r\") as tarObj:\n            for member in tarObj.getmembers():\n                filename = None\n                for tar_file_name in tar_file_name_list:\n                    if tar_file_name in member.name:\n                        filename = tar_file_name\n                if filename is None:\n                    continue\n                file = tarObj.extractfile(member)\n                with open(storage_directory(filename), \"wb\") as f:\n                    f.write(file.read())\n        os.remove(tmp_path)\n    if not os.path.exists(model_file_path) or not os.path.exists(\n            params_file_path):\n        raise Exception(\n            f\"Something went wrong while praparing the model[{model_name}] files!\"\n        )\n\n    return storage_directory()\n\n\nclass PaddleClas(object):\n    \"\"\"PaddleClas.\n    \"\"\"\n\n    def __init__(self,\n                 build_gallery: bool=False,\n                 gallery_image_root: str=None,\n                 gallery_data_file: str=None,\n                 index_dir: str=None,\n                 model_name: str=None,\n                 inference_model_dir: str=None,\n                 **kwargs):\n        \"\"\"Init PaddleClas with config.\n\n        Args:\n            model_name (str, optional): The model name supported by PaddleClas. If specified, override config. Defaults to None.\n            inference_model_dir (str, optional): The directory that contained model file and params file to be used. If specified, override config. Defaults to None.\n            use_gpu (bool, optional): Whether use GPU. If specified, override config. Defaults to True.\n            batch_size (int, optional): The batch size to pridict. If specified, override config. Defaults to 1.\n            topk (int, optional): Return the top k prediction results with the highest score. Defaults to 5.\n        \"\"\"\n        super().__init__()\n\n        if build_gallery:\n            self.model_type, inference_model_dir = self._check_input_model(\n                model_name\n                if model_name else \"PP-ShiTuV2\", inference_model_dir)\n            self._config = init_config(self.model_type, model_name\n                                       if model_name else \"PP-ShiTuV2\",\n                                       inference_model_dir, **kwargs)\n            if gallery_image_root:\n                self._config.IndexProcess.image_root = gallery_image_root\n            if gallery_data_file:\n                self._config.IndexProcess.data_file = gallery_data_file\n            if index_dir:\n                self._config.IndexProcess.index_dir = index_dir\n\n            logger.info(\"Building Gallery...\")\n            GalleryBuilder(self._config)\n\n        else:\n            self.model_type, inference_model_dir = self._check_input_model(\n                model_name, inference_model_dir)\n            self._config = init_config(self.model_type, model_name,\n                                       inference_model_dir, **kwargs)\n\n            if self.model_type == \"shitu\":\n                if index_dir:\n                    self._config.IndexProcess.index_dir = index_dir\n                self.predictor = SystemPredictor(self._config)\n            else:\n                self.predictor = ClsPredictor(self._config)\n\n    def get_config(self):\n        \"\"\"Get the config.\n        \"\"\"\n        return self._config\n\n    def _check_input_model(self, model_name, inference_model_dir):\n        \"\"\"Check input model name or model files.\n        \"\"\"\n        all_imn_model_names = get_imn_model_names()\n        all_pulc_model_names = PULC_MODELS\n        all_shitu_model_names = SHITU_MODELS\n\n        if model_name:\n            if model_name in all_imn_model_names:\n                inference_model_dir = check_model_file(\"imn\", model_name)\n                return \"imn\", inference_model_dir\n            elif model_name in all_pulc_model_names:\n                inference_model_dir = check_model_file(\"pulc\", model_name)\n                return \"pulc\", inference_model_dir\n            elif model_name in all_shitu_model_names:\n                inference_model_dir = check_model_file(\n                    \"shitu\",\n                    \"PP-ShiTuV2/general_PPLCNetV2_base_pretrained_v1.0\")\n                inference_model_dir = check_model_file(\n                    \"shitu\", \"picodet_PPLCNet_x2_5_mainbody_lite_v1.0\")\n                inference_model_dir = os.path.abspath(\n                    os.path.dirname(inference_model_dir))\n                return \"shitu\", inference_model_dir\n            else:\n                similar_imn_names = similar_model_names(model_name,\n                                                        all_imn_model_names)\n                similar_pulc_names = similar_model_names(model_name,\n                                                         all_pulc_model_names)\n                similar_names_str = \", \".join(similar_imn_names +\n                                              similar_pulc_names)\n                err = f\"{model_name} is not provided by PaddleClas. \\nMaybe you want the : [{similar_names_str}]. \\nIf you want to use your own model, please specify inference_model_dir!\"\n                raise InputModelError(err)\n        elif inference_model_dir:\n            model_file_path = os.path.join(inference_model_dir,\n                                           \"inference.pdmodel\")\n            params_file_path = os.path.join(inference_model_dir,\n                                            \"inference.pdiparams\")\n            if not os.path.isfile(model_file_path) or not os.path.isfile(\n                    params_file_path):\n                err = f\"There is no model file or params file in this directory: {inference_model_dir}\"\n                raise InputModelError(err)\n            return \"custom\", inference_model_dir\n        else:\n            err = \"Please specify the model name supported by PaddleClas or directory contained model files(inference.pdmodel, inference.pdiparams).\"\n            raise InputModelError(err)\n        return None\n\n    def predict_cls(self,\n                    input_data: Union[str, np.array],\n                    print_pred: bool=False) -> Generator[list, None, None]:\n        \"\"\"Predict input_data.\n\n        Args:\n            input_data (Union[str, np.array]):\n                When the type is str, it is the path of image, or the directory containing images, or the URL of image from Internet.\n                When the type is np.array, it is the image data whose channel order is RGB.\n            print_pred (bool, optional): Whether print the prediction result. Defaults to False.\n\n        Raises:\n            ImageTypeError: Illegal input_data.\n\n        Yields:\n            Generator[list, None, None]:\n                The prediction result(s) of input_data by batch_size. For every one image,\n                prediction result(s) is zipped as a dict, that includs topk \"class_ids\", \"scores\" and \"label_names\".\n                The format of batch prediction result(s) is as follow: [{\"class_ids\": [...], \"scores\": [...], \"label_names\": [...]}, ...]\n        \"\"\"\n\n        if isinstance(input_data, np.ndarray):\n            yield self.predictor.predict(input_data)\n        elif isinstance(input_data, str):\n            if input_data.startswith(\"http\") or input_data.startswith(\"https\"):\n                image_storage_dir = partial(os.path.join, BASE_IMAGES_DIR)\n                if not os.path.exists(image_storage_dir()):\n                    os.makedirs(image_storage_dir())\n                image_save_path = image_storage_dir(\"tmp.jpg\")\n                download_with_progressbar(input_data, image_save_path)\n                logger.info(\n                    f\"Image to be predicted from Internet: {input_data}, has been saved to: {image_save_path}\"\n                )\n                input_data = image_save_path\n            image_list = get_image_list(input_data)\n\n            batch_size = self._config.Global.get(\"batch_size\", 1)\n\n            img_list = []\n            img_path_list = []\n            cnt = 0\n            for idx_img, img_path in enumerate(image_list):\n                img = cv2.imread(img_path)\n                if img is None:\n                    logger.warning(\n                        f\"Image file failed to read and has been skipped. The path: {img_path}\"\n                    )\n                    continue\n                img = img[:, :, ::-1]\n                img_list.append(img)\n                img_path_list.append(img_path)\n                cnt += 1\n\n                if cnt % batch_size == 0 or (idx_img + 1) == len(image_list):\n                    preds = self.predictor.predict(img_list)\n\n                    if preds:\n                        for idx_pred, pred in enumerate(preds):\n                            pred[\"filename\"] = img_path_list[idx_pred]\n                            if print_pred:\n                                logger.info(\", \".join(\n                                    [f\"{k}: {pred[k]}\" for k in pred]))\n\n                    img_list = []\n                    img_path_list = []\n                    yield preds\n        else:\n            err = \"Please input legal image! The type of image supported by PaddleClas are: NumPy.ndarray and string of local path or Ineternet URL\"\n            raise ImageTypeError(err)\n        return\n\n    def predict_shitu(self,\n                      input_data: Union[str, np.array],\n                      print_pred: bool=False) -> Generator[list, None, None]:\n        \"\"\"Predict input_data.\n        Args:\n            input_data (Union[str, np.array]):\n                When the type is str, it is the path of image, or the directory containing images, or the URL of image from Internet.\n                When the type is np.array, it is the image data whose channel order is RGB.\n            print_pred (bool, optional): Whether print the prediction result. Defaults to False.\n\n        Raises:\n            ImageTypeError: Illegal input_data.\n\n        Yields:\n            Generator[list, None, None]:\n                The prediction result(s) of input_data by batch_size. For every one image,\n                prediction result(s) is zipped as a dict, that includs topk \"class_ids\", \"scores\" and \"label_names\".\n                The format of batch prediction result(s) is as follow: [{\"class_ids\": [...], \"scores\": [...], \"label_names\": [...]}, ...]\n        \"\"\"\n        if input_data is None and self._config.Global.infer_imgs:\n            input_data = self._config.Global.infer_imgs\n\n        if isinstance(input_data, np.ndarray):\n            yield self.predictor.predict(input_data)\n        elif isinstance(input_data, str):\n            if input_data.startswith(\"http\") or input_data.startswith(\"https\"):\n                image_storage_dir = partial(os.path.join, BASE_IMAGES_DIR)\n                if not os.path.exists(image_storage_dir()):\n                    os.makedirs(image_storage_dir())\n                image_save_path = image_storage_dir(\"tmp.jpg\")\n                download_with_progressbar(input_data, image_save_path)\n                logger.info(\n                    f\"Image to be predicted from Internet: {input_data}, has been saved to: {image_save_path}\"\n                )\n                input_data = image_save_path\n            image_list = get_image_list(input_data)\n\n            cnt = 0\n            for idx_img, img_path in enumerate(image_list):\n                img = cv2.imread(img_path)\n                if img is None:\n                    logger.warning(\n                        f\"Image file failed to read and has been skipped. The path: {img_path}\"\n                    )\n                    continue\n                img = img[:, :, ::-1]\n                cnt += 1\n\n                preds = self.predictor.predict(\n                    img)  # [dict1, dict2, ..., dictn]\n                if preds:\n                    if print_pred:\n                        logger.info(f\"{preds}, filename: {img_path}\")\n\n                yield preds\n        else:\n            err = \"Please input legal image! The type of image supported by PaddleClas are: NumPy.ndarray and string of local path or Ineternet URL\"\n            raise ImageTypeError(err)\n        return\n\n    def predict(self,\n                input_data: Union[str, np.array],\n                print_pred: bool=False,\n                predict_type=\"cls\"):\n        assert predict_type in [\"cls\", \"shitu\"\n                                ], \"Predict type should be 'cls' or 'shitu'.\"\n        if predict_type == \"cls\":\n            return self.predict_cls(input_data, print_pred)\n        elif predict_type == \"shitu\":\n            assert not isinstance(input_data, (\n                list, tuple\n            )), \"PP-ShiTu predictor only support single image as input now.\"\n            return self.predict_shitu(input_data, print_pred)\n        else:\n            raise ModuleNotFoundError\n\n\n# for CLI\ndef main():\n    \"\"\"Function API used for commad line.\n    \"\"\"\n    print_info()\n    cfg = args_cfg()\n    clas_engine = PaddleClas(**cfg)\n    if cfg[\"build_gallery\"] == False:\n        res = clas_engine.predict(\n            cfg[\"infer_imgs\"],\n            print_pred=True,\n            predict_type=cfg[\"predict_type\"])\n        for _ in res:\n            pass\n        logger.info(\"Predict complete!\")\n    return\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "ppcls",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.0087890625,
          "content": "[build-system]\nrequires = [\"setuptools==72.1.0\"]\nbuild-backend = \"setuptools.build_meta\" \n\n[project]\nname = \"paddleclas\"\ndescription = \"A treasure chest for visual recognition powered by PaddlePaddle.\"\n\nkeywords=[\n    'image-classification', 'image-recognition', 'pretrained-models',\n    'knowledge-distillation', 'product-recognition', 'autoaugment',\n    'cutmix', 'randaugment', 'gridmask', 'deit', 'repvgg',\n    'swin-transformer', 'image-retrieval-system'\n]\nclassifiers=[\n    'Development Status :: 5 - Production/Stable',\n    'Operating System :: OS Independent',\n    'Intended Audience :: Developers',\n    'Intended Audience :: Education',\n    'Intended Audience :: Science/Research',\n    'License :: OSI Approved :: Apache Software License',\n]\n\nreadme = \"README.md\"\nlicense = {file = \"LICENSE\"}\nrequires-python = \">=3.8\"\n\ndynamic = [\"version\", \"dependencies\"]\n\n[project.scripts]\npaddleclas = \"paddleclas.paddleclas:main\"\n\n[tool.setuptools.dynamic]\nversion = {file = \"version.txt\"}\ndependencies = {file = \"requirements.txt\"}\n\n\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2236328125,
          "content": "prettytable\nujson\nopencv-python<=4.6.0.66\npillow>=9.0.0\ntqdm\nPyYAML>=5.1\nvisualdl>=2.2.0\nscipy>=1.0.0\nscikit-learn>=0.21.0\ngast==0.3.3\nfaiss-cpu\neasydict\nnumpy==1.24.4; python_version<\"3.13\"\nnumpy==1.26.4; python_version>=\"3.13\"\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.84765625,
          "content": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom setuptools import setup\n\n\nsetup(\n    packages=['paddleclas'],\n    package_dir={'paddleclas': ''},\n    include_package_data=True,\n    url='https://github.com/PaddlePaddle/PaddleClas',\n    download_url='https://github.com/PaddlePaddle/PaddleClas.git',\n)\n"
        },
        {
          "name": "test_tipc",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "version.txt",
          "type": "blob",
          "size": 0.005859375,
          "content": "2.6.0\n"
        }
      ]
    }
  ]
}