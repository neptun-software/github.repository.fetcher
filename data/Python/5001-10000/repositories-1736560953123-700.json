{
  "metadata": {
    "timestamp": 1736560953123,
    "page": 700,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "PaddlePaddle/PaddleClas",
      "stars": 5534,
      "defaultBranch": "release/2.6",
      "files": [
        {
          "name": ".clang_format.hook",
          "type": "blob",
          "size": 0.3447265625,
          "content": "#!/bin/bash\nset -e\n\nreadonly VERSION=\"3.8\"\n\nversion=$(clang-format -version)\n\nif ! [[ $version == *\"$VERSION\"* ]]; then\n    echo \"clang-format version check failed.\"\n    echo \"a version contains '$VERSION' is needed, but get '$version'\"\n    echo \"you can install the right version, and make an soft-link to '\\$PATH' env\"\n    exit -1\nfi\n\nclang-format $@\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.162109375,
          "content": "__pycache__/\n*.pyc\n*.sw*\n*/workerlog*\ncheckpoints/\noutput*/\npretrained/\n.ipynb_checkpoints/\n*.ipynb*\n_build/\nbuild/\nlog/\nnohup.out\n.DS_Store\n.idea\ninference/\ntest.py\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1,
          "content": "repos:\n-   repo: https://github.com/PaddlePaddle/mirrors-yapf.git\n    rev: 0d79c0c469bab64f7229c9aca2b1186ef47f0e37\n    hooks:\n    -   id: yapf\n        files: \\.py$\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: a11d9314b22d8f8c7556443875b731ef05965464\n    hooks:\n    -   id: check-merge-conflict\n    -   id: check-symlinks\n    -   id: detect-private-key\n        files: (?!.*paddle)^.*$\n    -   id: end-of-file-fixer\n        files: \\.md$\n    -   id: trailing-whitespace\n        files: \\.md$\n-   repo: https://github.com/Lucas-C/pre-commit-hooks\n    rev: v1.0.1\n    hooks:\n    -   id: forbid-crlf\n        files: \\.md$\n    -   id: remove-crlf\n        files: \\.md$\n    -   id: forbid-tabs\n        files: \\.md$\n    -   id: remove-tabs\n        files: \\.md$\n-   repo: local\n    hooks:\n    -   id: clang-format\n        name: clang-format\n        description: Format files with ClangFormat\n        entry: bash .clang_format.hook -i\n        language: system\n        files: \\.(c|cc|cxx|cpp|cu|h|hpp|hxx|cuh|proto)$\n"
        },
        {
          "name": ".travis",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0908203125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.26953125,
          "content": "include LICENSE\ninclude README.md\ninclude docs/en/inference_deployment/whl_deploy_en.md\nrecursive-include deploy/python *.py\nrecursive-include deploy/utils *.py\nrecursive-include ppcls/arch *.py\nrecursive-include ppcls/utils *.py *.txt\nrecursive-include deploy/configs *.yaml\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 0.01171875,
          "content": "README_ch.md"
        },
        {
          "name": "README_ch.md",
          "type": "blob",
          "size": 14.5009765625,
          "content": "简体中文 | [English](README_en.md)\n\n# PaddleClas\n\n## 简介\n\n飞桨图像识别套件PaddleClas是飞桨为工业界和学术界所准备的一个图像识别和图像分类任务的工具集，助力使用者训练出更好的视觉模型和应用落地。\n\n|             PP-ShiTuV2图像识别系统效果展示             |                PULC实用图像分类模型效果展示                 |\n| :----------------------------------------------------: | :---------------------------------------------------------: |\n| <img src=\"./docs/images/shituv2.gif\"  width = \"450\" /> | <img src=\"./docs/images/class_simple.gif\"  width = \"600\" /> |\n\n\n## 📣 近期更新\n\n- **🔥2024.11.5 添加图像分类和图像检索领域低代码全流程开发能力**:\n  *  飞桨低代码开发工具PaddleX，依托于PaddleClas的先进技术，支持了图像分类和图像检索领域的**低代码全流程**开发能力：\n     * 🎨 [**模型丰富一键调用**](docs/zh_CN/paddlex/quick_start.md)：将通用图像分类、图像多标签分类、通用图像识别、人脸识别涉及的**98个模型**整合为6条模型产线，通过极简的**Python API一键调用**，快速体验模型效果。此外，同一套API，也支持目标检测、图像分割、文本图像智能分析、通用OCR、时序预测等共计**200+模型**，形成20+单功能模块，方便开发者进行**模型组合使用**。\n     * 🚀 [**提高效率降低门槛**](docs/zh_CN/paddlex/overview.md)：提供基于**统一命令**和**图形界面**两种方式，实现模型简洁高效的使用、组合与定制。支持**高性能推理、服务化部署和端侧部署**等多种部署方式。此外，对于各种主流硬件如**英伟达GPU、昆仑芯、昇腾、寒武纪和海光**等，进行模型开发时，都可以**无缝切换**。\n  * 新增图像分类算法[**MobileNetV4、StarNet、FasterNet**](https://github.com/PaddlePaddle/PaddleX/blob/release/3.0-beta1/docs/module_usage/tutorials/cv_modules/image_classification.md)\n  * 新增服务端图像识别模型（图像特征）[**PP-ShiTuV2_rec_CLIP_vit_base、PP-ShiTuV2_rec_CLIP_vit_large**](https://github.com/PaddlePaddle/PaddleX/blob/release/3.0-beta1/docs/module_usage/tutorials/cv_modules/image_feature.md)\n  * 新增多标签图像分类模型[**CLIP_vit_base_patch16_448_ML、PP-HGNetV2-B0_ML、PP-HGNetV2-B4_ML、PP-HGNetV2-B6_ML、PP-LCNet_x1_0_ML、ResNet50_ML**](https://github.com/PaddlePaddle/PaddleX/blob/release/3.0-beta1/docs/module_usage/tutorials/cv_modules/ml_classification.md)\n  * 新增人脸识别模型[**MobileFaceNet、ResNet50_face**](https://github.com/PaddlePaddle/PaddleX/blob/develop/docs/module_usage/tutorials/cv_modules/face_recognition.md)，新增[人脸识别端到端系统](https://github.com/PaddlePaddle/PaddleX/blob/develop/docs/pipeline_usage/tutorials/cv_pipelines/face_recognition.md)。\n\n- 2022.9.13 发布超轻量图像识别系统[PP-ShiTuV2](docs/zh_CN/models/PP-ShiTu/README.md)：\n  - recall1精度提升8个点，覆盖商品识别、垃圾分类、航拍场景等[20+识别场景](docs/zh_CN/deployment/PP-ShiTu/application_scenarios.md)，\n  - 新增[库管理工具](./deploy/shitu_index_manager/)，[Android Demo](./docs/zh_CN/quick_start/quick_start_recognition.md)全新体验。\n\n- [more](docs/zh_CN/version_history.md)\n\n\n## 🌟 特性\n\nPaddleClas支持多种前沿图像分类、识别相关算法，发布产业级特色骨干网络[PP-HGNet](docs/zh_CN/models/ImageNet1k/PP-HGNet.md)、[PP-LCNetv2](docs/zh_CN/models/ImageNet1k/PP-LCNetV2.md)、 [PP-LCNet](docs/zh_CN/models/ImageNet1k/PP-LCNet.md)和[SSLD半监督知识蒸馏方案](docs/zh_CN/training/advanced/ssld.md)等模型，在此基础上打造[PULC超轻量图像分类方案](docs/zh_CN/quick_start/PULC.md)和[PP-ShiTu图像识别系统](./docs/zh_CN/quick_start/quick_start_recognition.md)。\n\n<div align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/50011306/198961573-06a1a78d-7669-4061-aba5-79e9a2fc84dc.png\"/>\n</div>\n\n> 上述内容的使用方法建议从文档教程中的快速开始体验\n\n\n## ⚡ [快速开始](docs/zh_CN/paddlex/quick_start.md)\n\n- [🔥 一键调用98个PaddleClas核心模型](docs/zh_CN/paddlex/quick_start.md)\n- PULC超轻量图像分类方案快速体验：[点击这里](docs/zh_CN/quick_start/PULC.md)\n- PP-ShiTu图像识别快速体验：[点击这里](./docs/zh_CN/quick_start/quick_start_recognition.md)\n- PP-ShiTuV2 Android Demo APP，可扫描如下二维码，下载体验\n\n<div align=\"center\">\n<img src=\"./docs/images/quick_start/android_demo/PPShiTu_qrcode.png\"  width = \"170\" height = \"170\" />\n</div>\n\n## 🔥 [低代码全流程开发](docs/zh_CN/paddlex/overview.md)\n\n\n## 🛠️ PP系列模型列表\n\n| 模型简介                    | 应用场景                             | 模型下载链接                                                 |\n| --------------------------- | ------------------------------------ | ------------------------------------------------------------ |\n| PULC 超轻量图像分类方案     | 固定图像类别分类方案                 | 人体、车辆、文字相关9大模型：[模型库连接](./docs/zh_CN/models/PULC/model_list.md) |\n| PP-ShituV2 轻量图像识别系统 | 针对场景数据类别频繁变动、类别数据多 | 主体检测模型：[预训练模型](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/rec/models/pretrain/picodet_PPLCNet_x2_5_mainbody_lite_v1.0_pretrained.pdparams)  / [推理模型](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/rec/models/inference/picodet_PPLCNet_x2_5_mainbody_lite_v1.0_infer.tar)<br />识别模型：[预训练模型](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/rec/models/pretrain/PPShiTuV2/general_PPLCNetV2_base_pretrained_v1.0.pdparams)  / [推理模型](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/rec/models/inference/PP-ShiTuV2/general_PPLCNetV2_base_pretrained_v1.0_infer.tar) |\n| PP-LCNet 轻量骨干网络       | 针对Intel CPU设备及MKLDNN加速库定制  | PPLCNet_x1_0：[预训练模型](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNet_x1_0_pretrained.pdparams)  / [推理模型](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/inference/PPLCNet_x1_0_infer.tar) |\n| PP-LCNetV2 轻量骨干网络     | 针对Intel CPU设备，适配OpenVINO      | PPLCNetV2_base：[预训练模型](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNetV2_base_pretrained.pdparams)  / [推理模型](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/inference/PPLCNetV2_base_infer.tar) |\n| PP-HGNet 高精度骨干网络     | GPU设备上相同推理时间精度更高        | PPHGNet_small：[预训练模型](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPHGNet_small_pretrained.pdparams)  / [推理模型](https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/inference/PPHGNet_small_infer.tar) |\n\n> 全部模型下载链接可查看 文档教程 中的各模型介绍\n\n### 产业范例\n\n- 基于PP-ShiTuV2的生鲜品自助结算： [点击这里](./docs/zh_CN/samples/Fresh_Food_Recogniiton/README.md)\n- 基于PULC人员出入视频管理： [点击这里](./docs/zh_CN/samples/Personnel_Access/README.md)\n- 基于PP-ShiTu 的智慧商超商品识别：[点击这里](./docs/zh_CN/samples/Goods_Recognition/README.md)\n- 基于PP-ShiTu电梯内电瓶车入室识别：[点击这里](./docs/zh_CN/samples//Electromobile_In_Elevator_Detection/README.md)\n\n## 📖 文档教程\n- [环境准备](docs/zh_CN/installation.md)\n- [PP-ShiTuV2图像识别系统介绍](docs/zh_CN/models/PP-ShiTu/README.md)\n  - [图像识别快速体验](docs/zh_CN/quick_start/quick_start_recognition.md)\n  - [20+应用场景库](docs/zh_CN/deployment/PP-ShiTu/application_scenarios.md)\n  - 子模块算法介绍及模型训练\n    - [主体检测](docs/zh_CN/training/PP-ShiTu/mainbody_detection.md)\n    - [特征提取模型](docs/zh_CN/training/PP-ShiTu/feature_extraction.md)\n    - [向量检索](docs/zh_CN/deployment/PP-ShiTu/vector_search.md)\n    - [哈希编码](docs/zh_CN/training/PP-ShiTu/deep_hashing.md)\n  - PipeLine 推理部署\n    - [基于python预测引擎推理](docs/zh_CN/deployment/PP-ShiTu/python.md)\n    - [基于C++预测引擎推理](docs/zh_CN/deployment/PP-ShiTu/cpp.md)\n    - [服务化部署](docs/zh_CN/deployment/PP-ShiTu/paddle_serving.md)\n    - [端侧部署](docs/zh_CN/deployment/PP-ShiTu/paddle_lite.md)\n    - [库管理工具](docs/zh_CN/deployment/PP-ShiTu/gallery_manager.md)\n- [PULC超轻量图像分类实用方案](docs/zh_CN/training/PULC.md)\n  - [超轻量图像分类快速体验](docs/zh_CN/quick_start/PULC.md)\n  - [超轻量图像分类模型库](docs/zh_CN/models/PULC/model_list.md)\n    - [PULC有人/无人分类模型](docs/zh_CN/models/PULC/PULC_person_exists.md)\n    - [PULC人体属性识别模型](docs/zh_CN/models/PULC/PULC_person_attribute.md)\n    - [PULC佩戴安全帽分类模型](docs/zh_CN/models/PULC/PULC_safety_helmet.md)\n    - [PULC交通标志分类模型](docs/zh_CN/models/PULC/PULC_traffic_sign.md)\n    - [PULC车辆属性识别模型](docs/zh_CN/models/PULC/PULC_vehicle_attribute.md)\n    - [PULC有车/无车分类模型](docs/zh_CN/models/PULC/PULC_car_exists.md)\n    - [PULC含文字图像方向分类模型](docs/zh_CN/models/PULC/PULC_text_image_orientation.md)\n    - [PULC文本行方向分类模型](docs/zh_CN/models/PULC/PULC_textline_orientation.md)\n    - [PULC语种分类模型](docs/zh_CN/models/PULC/PULC_language_classification.md)\n    - [PULC表格属性识别模型](docs/zh_CN/models/PULC/PULC_table_attribute.md)\n    - [PULC有无广告码分类模型](docs/zh_CN/models/PULC/PULC_code_exists.md)\n    - [PULC清晰度评估模型](docs/zh_CN/models/PULC/PULC_clarity_assessment.md)\n    - [PULC图像方向分类模型](docs/zh_CN/models/PULC/PULC_image_orientation.md)\n  - [模型训练](docs/zh_CN/training/PULC.md)\n  - 推理部署\n    - [基于python预测引擎推理](docs/zh_CN/deployment/image_classification/python.md#1)\n    - [基于C++预测引擎推理](docs/zh_CN/deployment/image_classification/cpp/linux.md)\n    - [服务化部署](docs/zh_CN/deployment/image_classification/paddle_serving.md)\n    - [端侧部署](docs/zh_CN/deployment/image_classification/paddle_lite.md)\n    - [Paddle2ONNX模型转化与预测](docs/zh_CN/deployment/image_classification/paddle2onnx.md)\n  - [模型压缩](deploy/slim/README.md)\n- PP系列骨干网络模型\n  - [PP-HGNet](docs/zh_CN/models/ImageNet1k/PP-HGNet.md)\n  - [PP-LCNetv2](docs/zh_CN/models/ImageNet1k/PP-LCNetV2.md)\n  - [PP-LCNet](docs/zh_CN/models/ImageNet1k/PP-LCNet.md)\n- [SSLD半监督知识蒸馏方案](docs/zh_CN/training/advanced/ssld.md)\n- 前沿算法\n  - [骨干网络和预训练模型库](docs/zh_CN/models/ImageNet1k/model_list.md)\n  - [度量学习](docs/zh_CN/algorithm_introduction/metric_learning.md)\n    - [ReID](./docs/zh_CN/algorithm_introduction/ReID.md)\n  - [模型压缩](docs/zh_CN/algorithm_introduction/prune_quantization.md)\n  - [模型蒸馏](./docs/zh_CN/training/advanced/knowledge_distillation.md)\n  - [数据增强](docs/zh_CN/training/config_description/data_augmentation.md)\n- [产业实用范例库](docs/zh_CN/samples)\n- [30分钟快速体验图像分类](docs/zh_CN/quick_start/quick_start_classification_new_user.md)\n- FAQ\n  - [图像识别精选问题](docs/zh_CN/FAQ/faq_2021_s2.md)\n  - [图像分类精选问题](docs/zh_CN/FAQ/faq_selected_30.md)\n  - [图像分类FAQ第一季](docs/zh_CN/FAQ/faq_2020_s1.md)\n  - [图像分类FAQ第二季](docs/zh_CN/FAQ/faq_2021_s1.md)\n  - [图像分类FAQ第三季](docs/zh_CN/FAQ/faq_2022_s1.md)\n- [社区贡献指南](docs/zh_CN/community/how_to_contribute.md)\n- [许可证书](#许可证书)\n- [贡献代码](#贡献代码)\n\n<a name=\"图像识别系统介绍\"></a>\n\n## PP-ShiTuV2图像识别系统\n\n<div align=\"center\">\n<img src=\"./docs/images/structure.jpg\"  width = \"800\" />\n</div>\n\n\nPP-ShiTuV2是一个实用的轻量级通用图像识别系统，主要由主体检测、特征学习和向量检索三个模块组成。该系统从骨干网络选择和调整、损失函数的选择、数据增强、学习率变换策略、正则化参数选择、预训练模型使用以及模型裁剪量化多个方面，采用多种策略，对各个模块的模型进行优化，PP-ShiTuV2相比V1，Recall1提升近8个点。更多细节请参考[PP-ShiTuV2详细介绍](docs/zh_CN/models/PP-ShiTu/README.md)。\n\n<a name=\"识别效果展示\"></a>\n\n## PP-ShiTuV2图像识别系统效果展示\n\n- 瓶装饮料识别\n\n<div align=\"center\">\n<img src=\"docs/images/drink_demo.gif\">\n</div>\n\n\n- 商品识别\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769644-51604f80-d2d7-11eb-8290-c53b12a5c1f6.gif\"  width = \"400\" />\n</div>\n\n\n- 动漫人物识别\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769746-6b019700-d2d7-11eb-86df-f1d710999ba6.gif\"  width = \"400\" />\n</div>\n\n\n- logo识别\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769837-7fde2a80-d2d7-11eb-9b69-04140e9d785f.gif\"  width = \"400\" />\n</div>\n\n\n\n- 车辆识别\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769916-8ec4dd00-d2d7-11eb-8c60-42d89e25030c.gif\"  width = \"400\" />\n</div>\n\n\n\n<a name=\"PULC超轻量图像分类方案\"></a>\n\n## PULC超轻量图像分类方案\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/19523330/173011854-b10fcd7a-b799-4dfd-a1cf-9504952a3c44.png\"  width = \"800\" />\n</div>\nPULC融合了骨干网络、数据增广、蒸馏等多种前沿算法，可以自动训练得到轻量且高精度的图像分类模型。\nPaddleClas提供了覆盖人、车、OCR场景九大常见任务的分类模型，CPU推理3ms，精度比肩SwinTransformer。\n\n<a name=\"分类效果展示\"></a>\n\n## PULC实用图像分类模型效果展示\n<div align=\"center\">\n<img src=\"docs/images/classification.gif\">\n</div>\n\n\n<a name=\"许可证书\"></a>\n\n## 许可证书\n本项目的发布受<a href=\"https://github.com/PaddlePaddle/PaddleCLS/blob/master/LICENSE\">Apache 2.0 license</a>许可认证。\n\n\n<a name=\"贡献代码\"></a>\n## 贡献代码\n我们非常欢迎你为PaddleClas贡献代码，也十分感谢你的反馈。\n如果想为PaddleCLas贡献代码，可以参考[贡献指南](docs/zh_CN/community/how_to_contribute.md)。\n\n- 非常感谢[nblib](https://github.com/nblib)修正了PaddleClas中RandErasing的数据增广配置文件。\n- 非常感谢[chenpy228](https://github.com/chenpy228)修正了PaddleClas文档中的部分错别字。\n- 非常感谢[jm12138](https://github.com/jm12138)为PaddleClas添加ViT，DeiT系列模型和RepVGG系列模型。\n"
        },
        {
          "name": "README_en.md",
          "type": "blob",
          "size": 10.873046875,
          "content": "[简体中文](README_ch.md) | English\n\n# PaddleClas\n\n## Introduction\n\nPaddleClas is an image classification and image recognition toolset for industry and academia, helping users train better computer vision models and apply them in real scenarios.\n\n|                       PP-ShiTuV2                       | PULC: **P**ractical **U**ltra **L**ight-weight image **C**lassification solutions |\n| :----------------------------------------------------: | :----------------------------------------------------------: |\n| <img src=\"./docs/images/shituv2.gif\"  width = \"450\" /> | <img src=\"./docs/images/class_simple_en.gif\"  width = \"600\" /> |\n\n## 📣 Recent updates\n\n- 🔥️ Release [PP-ShiTuV2](./docs/en/PPShiTu/PPShiTuV2_introduction.md), recall1 is improved by nearly 8 points, covering 20+ recognition scenarios, with [index management tool](./deploy/shitu_index_manager) and [Android Demo](./docs/en/quick_start/quick_start_recognition_en.md) for better experience.\n- 2022.6.15 Release [**P**ractical **U**ltra **L**ight-weight image **C**lassification solutions](./docs/en/PULC/PULC_quickstart_en.md). PULC models inference within 3ms on CPU devices, with accuracy on par with SwinTransformer. We also release 9 practical classification models covering pedestrian, vehicle and OCR scenario.\n- 2022.4.21 Added the related [code](https://github.com/PaddlePaddle/PaddleClas/pull/1820/files) of the CVPR2022 oral paper [MixFormer](https://arxiv.org/pdf/2204.02557.pdf).\n\n- 2021.09.17 Add PP-LCNet series model developed by PaddleClas, these models show strong competitiveness on Intel CPUs.\nFor the introduction of PP-LCNet, please refer to [paper](https://arxiv.org/pdf/2109.15099.pdf) or [PP-LCNet model introduction](docs/en/models/PP-LCNet_en.md). The metrics and pretrained model are available [here](docs/en/algorithm_introduction/ImageNet_models_en.md).\n\n- 2021.06.29 Add [Swin-transformer](docs/en/models/SwinTransformer_en.md)) series model，Highest top1 acc on ImageNet1k dataset reaches 87.2%, training, evaluation and inference are all supported. Pretrained models can be downloaded [here](docs/en/algorithm_introduction/ImageNet_models_en.md#16).\n- 2021.06.16 PaddleClas release/2.2. Add metric learning and vector search modules. Add product recognition, animation character recognition, vehicle recognition and logo recognition. Added 30 pretrained models of LeViT, Twins, TNT, DLA, HarDNet, and RedNet, and the accuracy is roughly the same as that of the paper.\n- [more](./docs/en/others/update_history_en.md)\n\n## 🌟 Features\n\nPaddleClas release PP-HGNet、PP-LCNetv2、 PP-LCNet and **S**imple **S**emi-supervised **L**abel **D**istillation algorithms, and support plenty of image classification and image recognition algorithms.Based on th algorithms above, PaddleClas release PP-ShiTu image recognition system and [**P**ractical **U**ltra **L**ight-weight image **C**lassification solutions](docs/en/PULC/PULC_quickstart_en.md).\n\n\n![](https://user-images.githubusercontent.com/11568925/189268878-43d9d35b-90cf-425a-859e-767f8d94c5f7.png)\n\n## Welcome to Join the Technical Exchange Group\n\n* You can also scan the QR code below to join the PaddleClas QQ group and WeChat group (add and replay \"C\") to get more efficient answers to your questions and to communicate with developers from all walks of life. We look forward to hearing from you.\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/80816848/164383225-e375eb86-716e-41b4-a9e0-4b8a3976c1aa.jpg\" width=\"200\"/>\n<img src=\"https://user-images.githubusercontent.com/48054808/160531099-9811bbe6-cfbb-47d5-8bdb-c2b40684d7dd.png\" width=\"200\"/>\n</div>\n\n## Quick Start\nQuick experience of PP-ShiTu image recognition system：[Link](./docs/en/quick_start/quick_start_recognition_en.md)\n\n<div align=\"center\">\n<img src=\"./docs/images/quick_start/android_demo/PPShiTu_qrcode.png\"  width = \"40%\" />\n<p>PP-ShiTuV2 Android Demo</p>\n</div>\n\nQuick experience of **P**ractical **U**ltra **L**ight-weight image **C**lassification models：[Link](docs/en/PULC/PULC_quickstart_en.md)\n\n## Tutorials\n\n- [Install Paddle](./docs/en/installation/install_paddle_en.md)\n- [Install PaddleClas Environment](./docs/en/installation/install_paddleclas_en.md)\n- [PP-ShiTuV2 Image Recognition Systems Introduction](./docs/en/PPShiTu/PPShiTuV2_introduction.md)\n  - [Image Recognition Quick Start](docs/en/quick_start/quick_start_recognition_en.md)\n  - [20+ application scenarios](docs/zh_CN/deployment/PP-ShiTu/application_scenarios.md)\n  - Submodule Introduction and Model Training\n    - [Mainbody Detection](docs/zh_CN/training/PP-ShiTu/mainbody_detection.md)\n    - [Feature Extraction](./docs/en/image_recognition_pipeline/feature_extraction_en.md)\n    - [Vector Search](./docs/en/image_recognition_pipeline/vector_search_en.md)\n    - [Hash Encoding](docs/zh_CN/training/PP-ShiTu/deep_hashing.md)\n  - PipeLine Inference and Deployment\n    - [Python Inference](docs/en/inference_deployment/python_deploy_en.md)\n    - [C++ Inference](deploy/cpp_shitu/readme_en.md)\n    - [Serving Deployment](docs/en/inference_deployment/recognition_serving_deploy_en.md)\n    - [Lite Deployment](docs/en/inference_deployment/paddle_lite_deploy_en.md)\n    - [Shitu Gallery Manager Tool](docs/zh_CN/deployment/PP-ShiTu/gallery_manager.md)\n- [Practical Ultra Light-weight image Classification solutions](./docs/en/PULC/PULC_train_en.md)\n  - [PULC Quick Start](docs/en/PULC/PULC_quickstart_en.md)\n  - [PULC Model Zoo](docs/en/PULC/PULC_model_list_en.md)\n    - [PULC Classification Model of Someone or Nobody](docs/en/PULC/PULC_person_exists_en.md)\n    - [PULC Recognition Model of Person Attribute](docs/en/PULC/PULC_person_attribute_en.md)\n    - [PULC Classification Model of Wearing or Unwearing Safety Helmet](docs/en/PULC/PULC_safety_helmet_en.md)\n    - [PULC Classification Model of Traffic Sign](docs/en/PULC/PULC_traffic_sign_en.md)\n    - [PULC Recognition Model of Vehicle Attribute](docs/en/PULC/PULC_vehicle_attribute_en.md)\n    - [PULC Classification Model of Containing or Uncontaining Car](docs/en/PULC/PULC_car_exists_en.md)\n    - [PULC Classification Model of Text Image Orientation](docs/en/PULC/PULC_text_image_orientation_en.md)\n    - [PULC Classification Model of Textline Orientation](docs/en/PULC/PULC_textline_orientation_en.md)\n    - [PULC Classification Model of Language](docs/en/PULC/PULC_language_classification_en.md)\n- PP Series Backbone\n    - [PP-HGNet](docs/en/models/PP-HGNet_en.md)\n    - [PP-LCNet](docs/en/models/PP-LCNet_en.md)\n    - [PP-LCNetv2](docs/en/models/PP-LCNetv2_en.md)\n- [Introduction to Image Recognition Systems](#Introduction_to_Image_Recognition_Systems)\n- [Image Recognition Demo images](#Rec_Demo_images)\n- [PULC demo images](#Clas_Demo_images)\n- Algorithms Introduction\n    - [Backbone Network and Pre-trained Model Library](./docs/en/algorithm_introduction/ImageNet_models_en.md)\n    - [Mainbody Detection](./docs/en/image_recognition_pipeline/mainbody_detection_en.md)\n    - [Feature Learning](./docs/en/image_recognition_pipeline/feature_extraction_en.md)\n    - [Vector Search](./deploy/vector_search/README.md)\n- Inference Model Prediction\n    - [Python Inference](./docs/en/inference_deployment/python_deploy_en.md)\n    - [C++ Classfication Inference](./deploy/cpp/readme_en.md)\n- Model Deploy (only support classification for now, recognition coming soon)\n    - [Hub Serving Deployment](./deploy/hubserving/readme_en.md)\n    - [Mobile Deployment](./deploy/lite/readme_en.md)\n    - [Inference Using whl](./docs/en/inference_deployment/whl_deploy_en.md)\n- Advanced Tutorial\n    - [Knowledge Distillation](./docs/en/advanced_tutorials/distillation/distillation_en.md)\n    - [Model Quantization](./docs/en/algorithm_introduction/model_prune_quantization_en.md)\n    - [Data Augmentation](./docs/en/advanced_tutorials/DataAugmentation_en.md)\n- [License](#License)\n- [Contribution](#Contribution)\n\n<a name=\"Introduction_to_PULC\"></a>\n## Introduction to Practical Ultra Light-weight image Classification solutions\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/19523330/173011854-b10fcd7a-b799-4dfd-a1cf-9504952a3c44.png\"  width = \"800\" />\n</div>\nPULC solutions consists of PP-LCNet light-weight backbone, SSLD pretrained models, Ensemble of Data Augmentation strategy and SKL-UGI knowledge distillation.\nPULC models inference within 3ms on CPU devices, with accuracy comparable with SwinTransformer. We also release 9 practical models covering pedestrian, vehicle and OCR.\n\n<a name=\"Introduction_to_Image_Recognition_Systems\"></a>\n## Introduction to Image Recognition Systems\n\n<div align=\"center\">\n<img src=\"./docs/images/structure.jpg\"  width = \"800\" />\n</div>\n\nPP-ShiTuV2 is a practical lightweight general image recognition system, which is mainly composed of three modules: mainbody detection model, feature extraction model and vector search tool. The system adopts a variety of strategies including backbone network, loss function, data augmentations, optimal hyperparameters, pre-training model, model pruning and quantization. Compared to V1, PP-ShiTuV2, Recall1 is improved by nearly 8 points. For more details, please refer to [PP-ShiTuV2 introduction](./docs/en/PPShiTu/PPShiTuV2_introduction.md).\nFor a new unknown category, there is no need to retrain the model, just prepare images of new category, extract features and update retrieval database and the category can be recognised.\n\n<a name=\"Rec_Demo_images\"></a>\n## PP-ShiTuV2 Demo images\n\n- Drinks recognition\n\n<div align=\"center\">\n<img src=\"docs/images/drink_demo.gif\">\n</div>\n\n\n- Product recognition\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769644-51604f80-d2d7-11eb-8290-c53b12a5c1f6.gif\"  width = \"400\" />\n</div>\n\n\n- Cartoon character recognition\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769746-6b019700-d2d7-11eb-86df-f1d710999ba6.gif\"  width = \"400\" />\n</div>\n\n\n- Logo recognition\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769837-7fde2a80-d2d7-11eb-9b69-04140e9d785f.gif\"  width = \"400\" />\n</div>\n\n\n\n- Car recognition\n\n<div align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/18028216/122769916-8ec4dd00-d2d7-11eb-8c60-42d89e25030c.gif\"  width = \"400\" />\n</div>\n\n\n<a name=\"Clas_Demo_images\"></a>\n## PULC demo images\n<div align=\"center\">\n<img src=\"docs/images/classification_en.gif\">\n</div>\n\n\n<a name=\"License\"></a>\n## License\nPaddleClas is released under the Apache 2.0 license <a href=\"https://github.com/PaddlePaddle/PaddleCLS/blob/master/LICENSE\">Apache 2.0 license</a>\n\n\n<a name=\"Contribution\"></a>\n## Contribution\nContributions are highly welcomed and we would really appreciate your feedback!!\n\n\n- Thank [nblib](https://github.com/nblib) to fix bug of RandErasing.\n- Thank [chenpy228](https://github.com/chenpy228) to fix some typos PaddleClas.\n- Thank [jm12138](https://github.com/jm12138) to add ViT, DeiT models and RepVGG models into PaddleClas.\n- Thank [FutureSI](https://aistudio.baidu.com/aistudio/personalcenter/thirdview/76563) to parse and summarize the PaddleClas code.\n"
        },
        {
          "name": "__init__.py",
          "type": "blob",
          "size": 0.689453125,
          "content": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__all__ = ['PaddleClas']\nfrom .paddleclas import PaddleClas\nfrom .ppcls.arch.backbone import *\n"
        },
        {
          "name": "benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "deploy",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "hubconf.py",
          "type": "blob",
          "size": 27.4365234375,
          "content": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ndependencies = ['paddle']\n\nimport paddle\nimport os\nimport sys\n\n\nclass _SysPathG(object):\n    \"\"\"\n    _SysPathG used to add/clean path for sys.path. Making sure minimal pkgs dependents by skiping parent dirs.\n\n    __enter__\n        add path into sys.path\n    __exit__\n        clean user's sys.path to avoid unexpect behaviors\n    \"\"\"\n\n    def __init__(self, path):\n        self.path = path\n\n    def __enter__(self, ):\n        sys.path.insert(0, self.path)\n\n    def __exit__(self, type, value, traceback):\n        _p = sys.path.pop(0)\n        assert _p == self.path, 'Make sure sys.path cleaning {} correctly.'.format(\n            self.path)\n\n\nwith _SysPathG(os.path.dirname(os.path.abspath(__file__)), ):\n    import ppcls\n    import ppcls.arch.backbone as backbone\n\n    def ppclas_init():\n        if ppcls.utils.logger._logger is None:\n            ppcls.utils.logger.init_logger()\n\n    ppclas_init()\n\n    def _load_pretrained_parameters(model, name):\n        url = 'https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/{}_pretrained.pdparams'.format(\n            name)\n        path = paddle.utils.download.get_weights_path_from_url(url)\n        model.set_state_dict(paddle.load(path))\n        return model\n\n    def alexnet(pretrained=False, **kwargs):\n        \"\"\"\n        AlexNet\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `AlexNet` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.AlexNet(**kwargs)\n\n        return model\n\n    def vgg11(pretrained=False, **kwargs):\n        \"\"\"\n        VGG11\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                stop_grad_layers: int=0. The parameters in blocks which index larger than `stop_grad_layers`, will be set `param.trainable=False`\n        Returns:\n            model: nn.Layer. Specific `VGG11` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.VGG11(**kwargs)\n\n        return model\n\n    def vgg13(pretrained=False, **kwargs):\n        \"\"\"\n        VGG13\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                stop_grad_layers: int=0. The parameters in blocks which index larger than `stop_grad_layers`, will be set `param.trainable=False`\n        Returns:\n            model: nn.Layer. Specific `VGG13` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.VGG13(**kwargs)\n\n        return model\n\n    def vgg16(pretrained=False, **kwargs):\n        \"\"\"\n        VGG16\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                stop_grad_layers: int=0. The parameters in blocks which index larger than `stop_grad_layers`, will be set `param.trainable=False`\n        Returns:\n            model: nn.Layer. Specific `VGG16` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.VGG16(**kwargs)\n\n        return model\n\n    def vgg19(pretrained=False, **kwargs):\n        \"\"\"\n        VGG19\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                stop_grad_layers: int=0. The parameters in blocks which index larger than `stop_grad_layers`, will be set `param.trainable=False`\n        Returns:\n            model: nn.Layer. Specific `VGG19` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.VGG19(**kwargs)\n\n        return model\n\n    def resnet18(pretrained=False, **kwargs):\n        \"\"\"\n        ResNet18\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                input_image_channel: int=3. The number of input image channels\n                data_format: str='NCHW'. The data format of batch input images, should in ('NCHW', 'NHWC')\n        Returns:\n            model: nn.Layer. Specific `ResNet18` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNet18(**kwargs)\n\n        return model\n\n    def resnet34(pretrained=False, **kwargs):\n        \"\"\"\n        ResNet34\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                input_image_channel: int=3. The number of input image channels\n                data_format: str='NCHW'. The data format of batch input images, should in ('NCHW', 'NHWC')\n        Returns:\n            model: nn.Layer. Specific `ResNet34` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNet34(**kwargs)\n\n        return model\n\n    def resnet50(pretrained=False, **kwargs):\n        \"\"\"\n        ResNet50\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                input_image_channel: int=3. The number of input image channels\n                data_format: str='NCHW'. The data format of batch input images, should in ('NCHW', 'NHWC')\n        Returns:\n            model: nn.Layer. Specific `ResNet50` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNet50(**kwargs)\n\n        return model\n\n    def resnet101(pretrained=False, **kwargs):\n        \"\"\"\n        ResNet101\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                input_image_channel: int=3. The number of input image channels\n                data_format: str='NCHW'. The data format of batch input images, should in ('NCHW', 'NHWC')\n        Returns:\n            model: nn.Layer. Specific `ResNet101` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNet101(**kwargs)\n\n        return model\n\n    def resnet152(pretrained=False, **kwargs):\n        \"\"\"\n        ResNet152\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                input_image_channel: int=3. The number of input image channels\n                data_format: str='NCHW'. The data format of batch input images, should in ('NCHW', 'NHWC')\n        Returns:\n            model: nn.Layer. Specific `ResNet152` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNet152(**kwargs)\n\n        return model\n\n    def squeezenet1_0(pretrained=False, **kwargs):\n        \"\"\"\n        SqueezeNet1_0\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `SqueezeNet1_0` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.SqueezeNet1_0(**kwargs)\n\n        return model\n\n    def squeezenet1_1(pretrained=False, **kwargs):\n        \"\"\"\n        SqueezeNet1_1\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `SqueezeNet1_1` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.SqueezeNet1_1(**kwargs)\n\n        return model\n\n    def densenet121(pretrained=False, **kwargs):\n        \"\"\"\n        DenseNet121\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                dropout: float=0. Probability of setting units to zero.\n                bn_size: int=4. The number of channals per group\n        Returns:\n            model: nn.Layer. Specific `DenseNet121` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.DenseNet121(**kwargs)\n\n        return model\n\n    def densenet161(pretrained=False, **kwargs):\n        \"\"\"\n        DenseNet161\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                dropout: float=0. Probability of setting units to zero.\n                bn_size: int=4. The number of channals per group\n        Returns:\n            model: nn.Layer. Specific `DenseNet161` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.DenseNet161(**kwargs)\n\n        return model\n\n    def densenet169(pretrained=False, **kwargs):\n        \"\"\"\n        DenseNet169\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                dropout: float=0. Probability of setting units to zero.\n                bn_size: int=4. The number of channals per group\n        Returns:\n            model: nn.Layer. Specific `DenseNet169` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.DenseNet169(**kwargs)\n\n        return model\n\n    def densenet201(pretrained=False, **kwargs):\n        \"\"\"\n        DenseNet201\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                dropout: float=0. Probability of setting units to zero.\n                bn_size: int=4. The number of channals per group\n        Returns:\n            model: nn.Layer. Specific `DenseNet201` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.DenseNet201(**kwargs)\n\n        return model\n\n    def densenet264(pretrained=False, **kwargs):\n        \"\"\"\n        DenseNet264\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n                dropout: float=0. Probability of setting units to zero.\n                bn_size: int=4. The number of channals per group\n        Returns:\n            model: nn.Layer. Specific `DenseNet264` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.DenseNet264(**kwargs)\n\n        return model\n\n    def inceptionv3(pretrained=False, **kwargs):\n        \"\"\"\n        InceptionV3\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `InceptionV3` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.InceptionV3(**kwargs)\n\n        return model\n\n    def inceptionv4(pretrained=False, **kwargs):\n        \"\"\"\n        InceptionV4\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `InceptionV4` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.InceptionV4(**kwargs)\n\n        return model\n\n    def googlenet(pretrained=False, **kwargs):\n        \"\"\"\n        GoogLeNet\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `GoogLeNet` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.GoogLeNet(**kwargs)\n\n        return model\n\n    def shufflenetv2_x0_25(pretrained=False, **kwargs):\n        \"\"\"\n        ShuffleNetV2_x0_25\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ShuffleNetV2_x0_25` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ShuffleNetV2_x0_25(**kwargs)\n\n        return model\n\n    def mobilenetv1(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV1\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV1` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV1(**kwargs)\n\n        return model\n\n    def mobilenetv1_x0_25(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV1_x0_25\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV1_x0_25` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV1_x0_25(**kwargs)\n\n        return model\n\n    def mobilenetv1_x0_5(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV1_x0_5\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV1_x0_5` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV1_x0_5(**kwargs)\n\n        return model\n\n    def mobilenetv1_x0_75(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV1_x0_75\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV1_x0_75` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV1_x0_75(**kwargs)\n\n        return model\n\n    def mobilenetv2_x0_25(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV2_x0_25\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV2_x0_25` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV2_x0_25(**kwargs)\n\n        return model\n\n    def mobilenetv2_x0_5(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV2_x0_5\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV2_x0_5` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV2_x0_5(**kwargs)\n\n        return model\n\n    def mobilenetv2_x0_75(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV2_x0_75\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV2_x0_75` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV2_x0_75(**kwargs)\n\n        return model\n\n    def mobilenetv2_x1_5(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV2_x1_5\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV2_x1_5` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV2_x1_5(**kwargs)\n\n        return model\n\n    def mobilenetv2_x2_0(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV2_x2_0\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV2_x2_0` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV2_x2_0(**kwargs)\n\n        return model\n\n    def mobilenetv3_large_x0_35(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_large_x0_35\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_large_x0_35` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_large_x0_35(**kwargs)\n\n        return model\n\n    def mobilenetv3_large_x0_5(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_large_x0_5\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_large_x0_5` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_large_x0_5(**kwargs)\n\n        return model\n\n    def mobilenetv3_large_x0_75(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_large_x0_75\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_large_x0_75` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_large_x0_75(**kwargs)\n\n        return model\n\n    def mobilenetv3_large_x1_0(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_large_x1_0\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_large_x1_0` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_large_x1_0(**kwargs)\n\n        return model\n\n    def mobilenetv3_large_x1_25(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_large_x1_25\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_large_x1_25` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_large_x1_25(**kwargs)\n\n        return model\n\n    def mobilenetv3_small_x0_35(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_small_x0_35\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_small_x0_35` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_small_x0_35(**kwargs)\n\n        return model\n\n    def mobilenetv3_small_x0_5(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_small_x0_5\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_small_x0_5` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_small_x0_5(**kwargs)\n\n        return model\n\n    def mobilenetv3_small_x0_75(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_small_x0_75\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_small_x0_75` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_small_x0_75(**kwargs)\n\n        return model\n\n    def mobilenetv3_small_x1_0(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_small_x1_0\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_small_x1_0` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_small_x1_0(**kwargs)\n\n        return model\n\n    def mobilenetv3_small_x1_25(pretrained=False, **kwargs):\n        \"\"\"\n        MobileNetV3_small_x1_25\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `MobileNetV3_small_x1_25` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.MobileNetV3_small_x1_25(**kwargs)\n\n        return model\n\n    def resnext101_32x4d(pretrained=False, **kwargs):\n        \"\"\"\n        ResNeXt101_32x4d\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt101_32x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNeXt101_32x4d(**kwargs)\n\n        return model\n\n    def resnext101_64x4d(pretrained=False, **kwargs):\n        \"\"\"\n        ResNeXt101_64x4d\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt101_64x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNeXt101_64x4d(**kwargs)\n\n        return model\n\n    def resnext152_32x4d(pretrained=False, **kwargs):\n        \"\"\"\n        ResNeXt152_32x4d\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt152_32x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNeXt152_32x4d(**kwargs)\n\n        return model\n\n    def resnext152_64x4d(pretrained=False, **kwargs):\n        \"\"\"\n        ResNeXt152_64x4d\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt152_64x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNeXt152_64x4d(**kwargs)\n\n        return model\n\n    def resnext50_32x4d(pretrained=False, **kwargs):\n        \"\"\"\n        ResNeXt50_32x4d\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt50_32x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNeXt50_32x4d(**kwargs)\n\n        return model\n\n    def resnext50_64x4d(pretrained=False, **kwargs):\n        \"\"\"\n        ResNeXt50_64x4d\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt50_64x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.ResNeXt50_64x4d(**kwargs)\n\n        return model\n\n    def darknet53(pretrained=False, **kwargs):\n        \"\"\"\n        DarkNet53\n        Args:\n            pretrained: bool=False. If `True` load pretrained parameters, `False` otherwise.\n            kwargs: \n                class_dim: int=1000. Output dim of last fc layer.\n        Returns:\n            model: nn.Layer. Specific `ResNeXt50_64x4d` model depends on args.\n        \"\"\"\n        kwargs.update({'pretrained': pretrained})\n        model = backbone.DarkNet53(**kwargs)\n\n        return model\n"
        },
        {
          "name": "paddleclas.py",
          "type": "blob",
          "size": 34.1220703125,
          "content": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom typing import Union, Generator\nimport argparse\nimport shutil\nimport textwrap\nimport tarfile\nimport requests\nfrom functools import partial\nfrom difflib import SequenceMatcher\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom prettytable import PrettyTable\nimport paddle\n\nfrom .ppcls.arch import backbone\nfrom .ppcls.utils import logger\n\nfrom .deploy.python.predict_cls import ClsPredictor\nfrom .deploy.python.predict_system import SystemPredictor\nfrom .deploy.python.build_gallery import GalleryBuilder\nfrom .deploy.utils.get_image_list import get_image_list\nfrom .deploy.utils import config\n\n# for the PaddleClas Project\nfrom . import deploy\nfrom . import ppcls\n\n# for building model with loading pretrained weights from backbone\nlogger.init_logger()\n\n__all__ = [\"PaddleClas\"]\n\nBASE_DIR = os.path.expanduser(\"~/.paddleclas/\")\nBASE_INFERENCE_MODEL_DIR = os.path.join(BASE_DIR, \"inference_model\")\nBASE_IMAGES_DIR = os.path.join(BASE_DIR, \"images\")\nIMN_MODEL_BASE_DOWNLOAD_URL = \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/inference/{}_infer.tar\"\nIMN_MODEL_SERIES = {\n    \"AlexNet\": [\"AlexNet\"],\n    \"ConvNeXt\": [\"ConvNeXt_tiny\"],\n    \"CSPNet\": [\"CSPDarkNet53\"],\n    \"CSWinTransformer\": [\n        \"CSWinTransformer_tiny_224\", \"CSWinTransformer_small_224\",\n        \"CSWinTransformer_base_224\", \"CSWinTransformer_base_384\",\n        \"CSWinTransformer_large_224\", \"CSWinTransformer_large_384\"\n    ],\n    \"DarkNet\": [\"DarkNet53\"],\n    \"DeiT\": [\n        \"DeiT_base_distilled_patch16_224\", \"DeiT_base_distilled_patch16_384\",\n        \"DeiT_base_patch16_224\", \"DeiT_base_patch16_384\",\n        \"DeiT_small_distilled_patch16_224\", \"DeiT_small_patch16_224\",\n        \"DeiT_tiny_distilled_patch16_224\", \"DeiT_tiny_patch16_224\"\n    ],\n    \"DenseNet\": [\n        \"DenseNet121\", \"DenseNet161\", \"DenseNet169\", \"DenseNet201\",\n        \"DenseNet264\"\n    ],\n    \"DLA\": [\n        \"DLA46_c\", \"DLA60x_c\", \"DLA34\", \"DLA60\", \"DLA60x\", \"DLA102\", \"DLA102x\",\n        \"DLA102x2\", \"DLA169\"\n    ],\n    \"DPN\": [\"DPN68\", \"DPN92\", \"DPN98\", \"DPN107\", \"DPN131\"],\n    \"EfficientNet\": [\n        \"EfficientNetB0\", \"EfficientNetB0_small\", \"EfficientNetB1\",\n        \"EfficientNetB2\", \"EfficientNetB3\", \"EfficientNetB4\", \"EfficientNetB5\",\n        \"EfficientNetB6\", \"EfficientNetB7\"\n    ],\n    \"ESNet\": [\"ESNet_x0_25\", \"ESNet_x0_5\", \"ESNet_x0_75\", \"ESNet_x1_0\"],\n    \"GhostNet\":\n    [\"GhostNet_x0_5\", \"GhostNet_x1_0\", \"GhostNet_x1_3\", \"GhostNet_x1_3_ssld\"],\n    \"HarDNet\": [\"HarDNet39_ds\", \"HarDNet68_ds\", \"HarDNet68\", \"HarDNet85\"],\n    \"HRNet\": [\n        \"HRNet_W18_C\", \"HRNet_W30_C\", \"HRNet_W32_C\", \"HRNet_W40_C\",\n        \"HRNet_W44_C\", \"HRNet_W48_C\", \"HRNet_W64_C\", \"HRNet_W18_C_ssld\",\n        \"HRNet_W48_C_ssld\"\n    ],\n    \"Inception\": [\"GoogLeNet\", \"InceptionV3\", \"InceptionV4\"],\n    \"LeViT\":\n    [\"LeViT_128S\", \"LeViT_128\", \"LeViT_192\", \"LeViT_256\", \"LeViT_384\"],\n    \"MixNet\": [\"MixNet_S\", \"MixNet_M\", \"MixNet_L\"],\n    \"MobileNetV1\": [\n        \"MobileNetV1_x0_25\", \"MobileNetV1_x0_5\", \"MobileNetV1_x0_75\",\n        \"MobileNetV1\", \"MobileNetV1_ssld\"\n    ],\n    \"MobileNetV2\": [\n        \"MobileNetV2_x0_25\", \"MobileNetV2_x0_5\", \"MobileNetV2_x0_75\",\n        \"MobileNetV2\", \"MobileNetV2_x1_5\", \"MobileNetV2_x2_0\",\n        \"MobileNetV2_ssld\"\n    ],\n    \"MobileNetV3\": [\n        \"MobileNetV3_small_x0_35\", \"MobileNetV3_small_x0_5\",\n        \"MobileNetV3_small_x0_75\", \"MobileNetV3_small_x1_0\",\n        \"MobileNetV3_small_x1_25\", \"MobileNetV3_large_x0_35\",\n        \"MobileNetV3_large_x0_5\", \"MobileNetV3_large_x0_75\",\n        \"MobileNetV3_large_x1_0\", \"MobileNetV3_large_x1_25\",\n        \"MobileNetV3_small_x1_0_ssld\", \"MobileNetV3_large_x1_0_ssld\"\n    ],\n    \"MobileViT\": [\"MobileViT_XXS\", \"MobileViT_XS\", \"MobileViT_S\"],\n    \"PeleeNet\": [\"PeleeNet\"],\n    \"PPHGNet\": [\n        \"PPHGNet_tiny\",\n        \"PPHGNet_small\",\n        \"PPHGNet_tiny_ssld\",\n        \"PPHGNet_small_ssld\",\n    ],\n    \"PPLCNet\": [\n        \"PPLCNet_x0_25\", \"PPLCNet_x0_35\", \"PPLCNet_x0_5\", \"PPLCNet_x0_75\",\n        \"PPLCNet_x1_0\", \"PPLCNet_x1_5\", \"PPLCNet_x2_0\", \"PPLCNet_x2_5\"\n    ],\n    \"PPLCNetV2\": [\"PPLCNetV2_base\"],\n    \"PVTV2\": [\n        \"PVT_V2_B0\", \"PVT_V2_B1\", \"PVT_V2_B2\", \"PVT_V2_B2_Linear\", \"PVT_V2_B3\",\n        \"PVT_V2_B4\", \"PVT_V2_B5\"\n    ],\n    \"RedNet\": [\"RedNet26\", \"RedNet38\", \"RedNet50\", \"RedNet101\", \"RedNet152\"],\n    \"RegNet\": [\"RegNetX_4GF\"],\n    \"Res2Net\": [\n        \"Res2Net50_14w_8s\", \"Res2Net50_26w_4s\", \"Res2Net50_vd_26w_4s\",\n        \"Res2Net200_vd_26w_4s\", \"Res2Net101_vd_26w_4s\",\n        \"Res2Net50_vd_26w_4s_ssld\", \"Res2Net101_vd_26w_4s_ssld\",\n        \"Res2Net200_vd_26w_4s_ssld\"\n    ],\n    \"ResNeSt\": [\"ResNeSt50\", \"ResNeSt50_fast_1s1x64d\"],\n    \"ResNet\": [\n        \"ResNet18\", \"ResNet18_vd\", \"ResNet34\", \"ResNet34_vd\", \"ResNet50\",\n        \"ResNet50_vc\", \"ResNet50_vd\", \"ResNet50_vd_v2\", \"ResNet101\",\n        \"ResNet101_vd\", \"ResNet152\", \"ResNet152_vd\", \"ResNet200_vd\",\n        \"ResNet34_vd_ssld\", \"ResNet50_vd_ssld\", \"ResNet50_vd_ssld_v2\",\n        \"ResNet101_vd_ssld\", \"Fix_ResNet50_vd_ssld_v2\", \"ResNet50_ACNet_deploy\"\n    ],\n    \"ResNeXt\": [\n        \"ResNeXt50_32x4d\", \"ResNeXt50_vd_32x4d\", \"ResNeXt50_64x4d\",\n        \"ResNeXt50_vd_64x4d\", \"ResNeXt101_32x4d\", \"ResNeXt101_vd_32x4d\",\n        \"ResNeXt101_32x8d_wsl\", \"ResNeXt101_32x16d_wsl\",\n        \"ResNeXt101_32x32d_wsl\", \"ResNeXt101_32x48d_wsl\",\n        \"Fix_ResNeXt101_32x48d_wsl\", \"ResNeXt101_64x4d\", \"ResNeXt101_vd_64x4d\",\n        \"ResNeXt152_32x4d\", \"ResNeXt152_vd_32x4d\", \"ResNeXt152_64x4d\",\n        \"ResNeXt152_vd_64x4d\"\n    ],\n    \"ReXNet\":\n    [\"ReXNet_1_0\", \"ReXNet_1_3\", \"ReXNet_1_5\", \"ReXNet_2_0\", \"ReXNet_3_0\"],\n    \"SENet\": [\n        \"SENet154_vd\", \"SE_HRNet_W64_C_ssld\", \"SE_ResNet18_vd\",\n        \"SE_ResNet34_vd\", \"SE_ResNet50_vd\", \"SE_ResNeXt50_32x4d\",\n        \"SE_ResNeXt50_vd_32x4d\", \"SE_ResNeXt101_32x4d\"\n    ],\n    \"ShuffleNetV2\": [\n        \"ShuffleNetV2_swish\", \"ShuffleNetV2_x0_25\", \"ShuffleNetV2_x0_33\",\n        \"ShuffleNetV2_x0_5\", \"ShuffleNetV2_x1_0\", \"ShuffleNetV2_x1_5\",\n        \"ShuffleNetV2_x2_0\"\n    ],\n    \"SqueezeNet\": [\"SqueezeNet1_0\", \"SqueezeNet1_1\"],\n    \"SwinTransformer\": [\n        \"SwinTransformer_large_patch4_window7_224_22kto1k\",\n        \"SwinTransformer_large_patch4_window12_384_22kto1k\",\n        \"SwinTransformer_base_patch4_window7_224_22kto1k\",\n        \"SwinTransformer_base_patch4_window12_384_22kto1k\",\n        \"SwinTransformer_base_patch4_window12_384\",\n        \"SwinTransformer_base_patch4_window7_224\",\n        \"SwinTransformer_small_patch4_window7_224\",\n        \"SwinTransformer_tiny_patch4_window7_224\"\n    ],\n    \"Twins\": [\n        \"pcpvt_small\", \"pcpvt_base\", \"pcpvt_large\", \"alt_gvt_small\",\n        \"alt_gvt_base\", \"alt_gvt_large\"\n    ],\n    \"TNT\": [\"TNT_small\"],\n    \"VAN\": [\"VAN_B0\"],\n    \"VGG\": [\"VGG11\", \"VGG13\", \"VGG16\", \"VGG19\"],\n    \"VisionTransformer\": [\n        \"ViT_base_patch16_224\", \"ViT_base_patch16_384\", \"ViT_base_patch32_384\",\n        \"ViT_large_patch16_224\", \"ViT_large_patch16_384\",\n        \"ViT_large_patch32_384\", \"ViT_small_patch16_224\"\n    ],\n    \"Xception\": [\n        \"Xception41\", \"Xception41_deeplab\", \"Xception65\", \"Xception65_deeplab\",\n        \"Xception71\"\n    ]\n}\n\nPULC_MODEL_BASE_DOWNLOAD_URL = \"https://paddleclas.bj.bcebos.com/models/PULC/inference/{}_infer.tar\"\nPULC_MODELS = [\n    \"car_exists\", \"language_classification\", \"person_attribute\",\n    \"person_exists\", \"safety_helmet\", \"text_image_orientation\",\n    \"image_orientation\", \"textline_orientation\", \"traffic_sign\",\n    \"vehicle_attribute\", \"table_attribute\", \"clarity_assessment\"\n]\n\nSHITU_MODEL_BASE_DOWNLOAD_URL = \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/rec/models/inference/{}_infer.tar\"\nSHITU_MODELS = [\n    # \"picodet_PPLCNet_x2_5_mainbody_lite_v1.0\",  # ShiTuV1(V2)_mainbody_det\n    # \"general_PPLCNet_x2_5_lite_v1.0\"  # ShiTuV1_general_rec\n    # \"PP-ShiTuV2/general_PPLCNetV2_base_pretrained_v1.0\",  # ShiTuV2_general_rec TODO(hesensen): add lite model\n    \"PP-ShiTuV2\"\n]\n\n\nclass ImageTypeError(Exception):\n    \"\"\"ImageTypeError.\n    \"\"\"\n\n    def __init__(self, message=\"\"):\n        super().__init__(message)\n\n\nclass InputModelError(Exception):\n    \"\"\"InputModelError.\n    \"\"\"\n\n    def __init__(self, message=\"\"):\n        super().__init__(message)\n\n\ndef init_config(model_type, model_name, inference_model_dir, **kwargs):\n\n    if kwargs.get(\"build_gallery\", False):\n        cfg_path = \"deploy/configs/inference_general.yaml\"\n    elif model_type == \"pulc\":\n        cfg_path = f\"deploy/configs/PULC/{model_name}/inference_{model_name}.yaml\"\n    elif model_type == \"shitu\":\n        cfg_path = \"deploy/configs/inference_general.yaml\"\n    else:\n        cfg_path = \"deploy/configs/inference_cls.yaml\"\n\n    __dir__ = os.path.dirname(__file__)\n    cfg_path = os.path.join(__dir__, cfg_path)\n    cfg = config.get_config(\n        cfg_path, overrides=kwargs.get(\"override\", None), show=False)\n    if cfg.Global.get(\"inference_model_dir\"):\n        cfg.Global.inference_model_dir = inference_model_dir\n    else:\n        cfg.Global.rec_inference_model_dir = os.path.join(\n            inference_model_dir,\n            \"PP-ShiTuV2/general_PPLCNetV2_base_pretrained_v1.0\")\n        cfg.Global.det_inference_model_dir = os.path.join(\n            inference_model_dir, \"picodet_PPLCNet_x2_5_mainbody_lite_v1.0\")\n\n    if \"batch_size\" in kwargs and kwargs[\"batch_size\"]:\n        cfg.Global.batch_size = kwargs[\"batch_size\"]\n\n    if \"use_gpu\" in kwargs and kwargs[\"use_gpu\"] is not None:\n        cfg.Global.use_gpu = kwargs[\"use_gpu\"]\n    if cfg.Global.use_gpu and not paddle.device.is_compiled_with_cuda():\n        msg = \"The current running environment does not support the use of GPU. CPU has been used instead.\"\n        logger.warning(msg)\n        cfg.Global.use_gpu = False\n\n    if \"infer_imgs\" in kwargs and kwargs[\"infer_imgs\"]:\n        cfg.Global.infer_imgs = kwargs[\"infer_imgs\"]\n    if \"index_dir\" in kwargs and kwargs[\"index_dir\"]:\n        cfg.IndexProcess.index_dir = kwargs[\"index_dir\"]\n    if \"data_file\" in kwargs and kwargs[\"data_file\"]:\n        cfg.IndexProcess.data_file = kwargs[\"data_file\"]\n    if \"enable_mkldnn\" in kwargs and kwargs[\"enable_mkldnn\"] is not None:\n        cfg.Global.enable_mkldnn = kwargs[\"enable_mkldnn\"]\n    if \"cpu_num_threads\" in kwargs and kwargs[\"cpu_num_threads\"]:\n        cfg.Global.cpu_num_threads = kwargs[\"cpu_num_threads\"]\n    if \"use_fp16\" in kwargs and kwargs[\"use_fp16\"] is not None:\n        cfg.Global.use_fp16 = kwargs[\"use_fp16\"]\n    if \"use_tensorrt\" in kwargs and kwargs[\"use_tensorrt\"] is not None:\n        cfg.Global.use_tensorrt = kwargs[\"use_tensorrt\"]\n    if \"gpu_mem\" in kwargs and kwargs[\"gpu_mem\"]:\n        cfg.Global.gpu_mem = kwargs[\"gpu_mem\"]\n    if \"resize_short\" in kwargs and kwargs[\"resize_short\"]:\n        cfg.PreProcess.transform_ops[0][\"ResizeImage\"][\n            \"resize_short\"] = kwargs[\"resize_short\"]\n    if \"crop_size\" in kwargs and kwargs[\"crop_size\"]:\n        cfg.PreProcess.transform_ops[1][\"CropImage\"][\"size\"] = kwargs[\n            \"crop_size\"]\n\n    # TODO(gaotingquan): not robust\n    if cfg.get(\"PostProcess\"):\n        if \"Topk\" in cfg.PostProcess:\n            if \"topk\" in kwargs and kwargs[\"topk\"]:\n                cfg.PostProcess.Topk.topk = kwargs[\"topk\"]\n            if \"class_id_map_file\" in kwargs and kwargs[\"class_id_map_file\"]:\n                cfg.PostProcess.Topk.class_id_map_file = kwargs[\n                    \"class_id_map_file\"]\n            else:\n                class_id_map_file_path = os.path.relpath(\n                    cfg.PostProcess.Topk.class_id_map_file, \"../\")\n                cfg.PostProcess.Topk.class_id_map_file = os.path.join(\n                    __dir__, class_id_map_file_path)\n        if \"ThreshOutput\" in cfg.PostProcess:\n            if \"thresh\" in kwargs and kwargs[\"thresh\"]:\n                cfg.PostProcess.ThreshOutput.thresh = kwargs[\"thresh\"]\n            if \"class_id_map_file\" in kwargs and kwargs[\"class_id_map_file\"]:\n                cfg.PostProcess.ThreshOutput[\"class_id_map_file\"] = kwargs[\n                    \"class_id_map_file\"]\n            elif \"class_id_map_file\" in cfg.PostProcess.ThreshOutput:\n                class_id_map_file_path = os.path.relpath(\n                    cfg.PostProcess.ThreshOutput.class_id_map_file, \"../\")\n                cfg.PostProcess.ThreshOutput.class_id_map_file = os.path.join(\n                    __dir__, class_id_map_file_path)\n        if \"VehicleAttribute\" in cfg.PostProcess:\n            if \"color_threshold\" in kwargs and kwargs[\"color_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"color_threshold\"]\n            if \"type_threshold\" in kwargs and kwargs[\"type_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.type_threshold = kwargs[\n                    \"type_threshold\"]\n        if \"TableAttribute\" in cfg.PostProcess:\n            if \"source_threshold\" in kwargs and kwargs[\"source_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"source_threshold\"]\n            if \"number_threshold\" in kwargs and kwargs[\"number_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"number_threshold\"]\n            if \"color_threshold\" in kwargs and kwargs[\"color_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"color_threshold\"]\n            if \"clarity_threshold\" in kwargs and kwargs[\"clarity_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"clarity_threshold\"]\n            if \"obstruction_threshold\" in kwargs and kwargs[\n                    \"obstruction_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"obstruction_threshold\"]\n            if \"angle_threshold\" in kwargs and kwargs[\"angle_threshold\"]:\n                cfg.PostProcess.VehicleAttribute.color_threshold = kwargs[\n                    \"angle_threshold\"]\n    if \"save_dir\" in kwargs and kwargs[\"save_dir\"]:\n        cfg.PostProcess.SavePreLabel.save_dir = kwargs[\"save_dir\"]\n\n    return cfg\n\n\ndef args_cfg():\n    def str2bool(v):\n        return v.lower() in (\"true\", \"t\", \"1\")\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--infer_imgs\",\n        type=str,\n        required=False,\n        help=\"The image(s) to be predicted.\")\n    parser.add_argument(\n        \"--model_name\", type=str, help=\"The model name to be used.\")\n    parser.add_argument(\n        \"--predict_type\",\n        type=str,\n        default=\"cls\",\n        help=\"The predict type to be selected.\")\n    parser.add_argument(\n        \"--inference_model_dir\",\n        type=str,\n        help=\"The directory of model files. Valid when model_name not specifed.\"\n    )\n    parser.add_argument(\n        \"--index_dir\",\n        type=str,\n        required=False,\n        help=\"The index directory path.\")\n    parser.add_argument(\n        \"--data_file\", type=str, required=False, help=\"The label file path.\")\n    parser.add_argument(\"--use_gpu\", type=str2bool, help=\"Whether use GPU.\")\n    parser.add_argument(\n        \"--gpu_mem\",\n        type=int,\n        help=\"The memory size of GPU allocated to predict.\")\n    parser.add_argument(\n        \"--enable_mkldnn\",\n        type=str2bool,\n        help=\"Whether use MKLDNN. Valid when use_gpu is False\")\n    parser.add_argument(\n        \"--cpu_num_threads\",\n        type=int,\n        help=\"The threads number when predicting on CPU.\")\n    parser.add_argument(\n        \"--use_tensorrt\",\n        type=str2bool,\n        help=\"Whether use TensorRT to accelerate.\")\n    parser.add_argument(\n        \"--use_fp16\", type=str2bool, help=\"Whether use FP16 to predict.\")\n    parser.add_argument(\"--batch_size\", type=int, help=\"Batch size.\")\n    parser.add_argument(\n        \"--topk\",\n        type=int,\n        help=\"Return topk score(s) and corresponding results when Topk postprocess is used.\"\n    )\n    parser.add_argument(\n        \"--class_id_map_file\",\n        type=str,\n        help=\"The path of file that map class_id and label.\")\n    parser.add_argument(\n        \"--threshold\",\n        type=float,\n        help=\"The threshold of ThreshOutput when postprocess is used.\")\n    parser.add_argument(\"--color_threshold\", type=float, help=\"\")\n    parser.add_argument(\"--type_threshold\", type=float, help=\"\")\n    parser.add_argument(\n        \"--save_dir\",\n        type=str,\n        help=\"The directory to save prediction results as pre-label.\")\n    parser.add_argument(\n        \"--resize_short\", type=int, help=\"Resize according to short size.\")\n    parser.add_argument(\"--crop_size\", type=int, help=\"Centor crop size.\")\n    parser.add_argument(\n        \"--build_gallery\",\n        type=str2bool,\n        default=False,\n        help=\"Whether build gallery.\")\n    parser.add_argument(\n        '-o',\n        '--override',\n        action='append',\n        default=[],\n        help='config options to be overridden')\n    args = parser.parse_args()\n    return vars(args)\n\n\ndef print_info():\n    \"\"\"Print list of supported models in formatted.\n    \"\"\"\n    imn_table = PrettyTable([\"IMN Model Series\", \"Model Name\"])\n    pulc_table = PrettyTable([\"PULC Models\"])\n    shitu_table = PrettyTable([\"PP-ShiTu Models\"])\n    try:\n        sz = os.get_terminal_size()\n        total_width = sz.columns\n        first_width = 30\n        second_width = total_width - first_width if total_width > 50 else 10\n    except OSError:\n        total_width = 100\n        second_width = 100\n    for series in IMN_MODEL_SERIES:\n        names = textwrap.fill(\n            \"  \".join(IMN_MODEL_SERIES[series]), width=second_width)\n        imn_table.add_row([series, names])\n\n    table_width = len(str(imn_table).split(\"\\n\")[0])\n    pulc_table.add_row([\n        textwrap.fill(\n            \"  \".join(PULC_MODELS), width=total_width).center(table_width - 4)\n    ])\n    shitu_table.add_row([\n        textwrap.fill(\n            \"  \".join(SHITU_MODELS), width=total_width).center(table_width - 4)\n    ])\n\n    print(\"{}\".format(\"-\" * table_width))\n    print(\"Models supported by PaddleClas\".center(table_width))\n    print(imn_table)\n    print(pulc_table)\n    print(shitu_table)\n    print(\"Powered by PaddlePaddle!\".rjust(table_width))\n    print(\"{}\".format(\"-\" * table_width))\n\n\ndef get_imn_model_names():\n    \"\"\"Get the model names list.\n    \"\"\"\n    model_names = []\n    for series in IMN_MODEL_SERIES:\n        model_names += (IMN_MODEL_SERIES[series])\n    return model_names\n\n\ndef similar_model_names(name=\"\", names=[], thresh=0.1, topk=5):\n    \"\"\"Find the most similar topk model names.\n    \"\"\"\n    scores = []\n    for idx, n in enumerate(names):\n        if n.startswith(\"__\"):\n            continue\n        score = SequenceMatcher(None, n.lower(), name.lower()).quick_ratio()\n        if score > thresh:\n            scores.append((idx, score))\n    scores.sort(key=lambda x: x[1], reverse=True)\n    similar_names = [names[s[0]] for s in scores[:min(topk, len(scores))]]\n    return similar_names\n\n\ndef download_with_progressbar(url, save_path):\n    \"\"\"Download from url with progressbar.\n    \"\"\"\n    if os.path.isfile(save_path):\n        os.remove(save_path)\n    response = requests.get(url, stream=True)\n    total_size_in_bytes = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024  # 1 Kibibyte\n    progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n    with open(save_path, \"wb\") as file:\n        for data in response.iter_content(block_size):\n            progress_bar.update(len(data))\n            file.write(data)\n    progress_bar.close()\n    if total_size_in_bytes == 0 or progress_bar.n != total_size_in_bytes or not os.path.isfile(\n            save_path):\n        raise Exception(\n            f\"Something went wrong while downloading file from {url}\")\n\n\ndef check_model_file(model_type, model_name):\n    \"\"\"Check the model files exist and download and untar when no exist.\n    \"\"\"\n    if model_type == \"pulc\":\n        storage_directory = partial(os.path.join, BASE_INFERENCE_MODEL_DIR,\n                                    \"PULC\", model_name)\n        url = PULC_MODEL_BASE_DOWNLOAD_URL.format(model_name)\n    elif model_type == \"shitu\":\n        storage_directory = partial(os.path.join, BASE_INFERENCE_MODEL_DIR,\n                                    \"PP-ShiTu\", model_name)\n        url = SHITU_MODEL_BASE_DOWNLOAD_URL.format(model_name)\n    else:\n        storage_directory = partial(os.path.join, BASE_INFERENCE_MODEL_DIR,\n                                    \"IMN\", model_name)\n        url = IMN_MODEL_BASE_DOWNLOAD_URL.format(model_name)\n\n    tar_file_name_list = [\n        \"inference.pdiparams\", \"inference.pdiparams.info\", \"inference.pdmodel\"\n    ]\n    model_file_path = storage_directory(\"inference.pdmodel\")\n    params_file_path = storage_directory(\"inference.pdiparams\")\n    if not os.path.exists(model_file_path) or not os.path.exists(\n            params_file_path):\n        tmp_path = storage_directory(url.split(\"/\")[-1])\n        logger.info(f\"download {url} to {tmp_path}\")\n        os.makedirs(storage_directory(), exist_ok=True)\n        download_with_progressbar(url, tmp_path)\n        with tarfile.open(tmp_path, \"r\") as tarObj:\n            for member in tarObj.getmembers():\n                filename = None\n                for tar_file_name in tar_file_name_list:\n                    if tar_file_name in member.name:\n                        filename = tar_file_name\n                if filename is None:\n                    continue\n                file = tarObj.extractfile(member)\n                with open(storage_directory(filename), \"wb\") as f:\n                    f.write(file.read())\n        os.remove(tmp_path)\n    if not os.path.exists(model_file_path) or not os.path.exists(\n            params_file_path):\n        raise Exception(\n            f\"Something went wrong while praparing the model[{model_name}] files!\"\n        )\n\n    return storage_directory()\n\n\nclass PaddleClas(object):\n    \"\"\"PaddleClas.\n    \"\"\"\n\n    def __init__(self,\n                 build_gallery: bool=False,\n                 gallery_image_root: str=None,\n                 gallery_data_file: str=None,\n                 index_dir: str=None,\n                 model_name: str=None,\n                 inference_model_dir: str=None,\n                 **kwargs):\n        \"\"\"Init PaddleClas with config.\n\n        Args:\n            model_name (str, optional): The model name supported by PaddleClas. If specified, override config. Defaults to None.\n            inference_model_dir (str, optional): The directory that contained model file and params file to be used. If specified, override config. Defaults to None.\n            use_gpu (bool, optional): Whether use GPU. If specified, override config. Defaults to True.\n            batch_size (int, optional): The batch size to pridict. If specified, override config. Defaults to 1.\n            topk (int, optional): Return the top k prediction results with the highest score. Defaults to 5.\n        \"\"\"\n        super().__init__()\n\n        if build_gallery:\n            self.model_type, inference_model_dir = self._check_input_model(\n                model_name\n                if model_name else \"PP-ShiTuV2\", inference_model_dir)\n            self._config = init_config(self.model_type, model_name\n                                       if model_name else \"PP-ShiTuV2\",\n                                       inference_model_dir, **kwargs)\n            if gallery_image_root:\n                self._config.IndexProcess.image_root = gallery_image_root\n            if gallery_data_file:\n                self._config.IndexProcess.data_file = gallery_data_file\n            if index_dir:\n                self._config.IndexProcess.index_dir = index_dir\n\n            logger.info(\"Building Gallery...\")\n            GalleryBuilder(self._config)\n\n        else:\n            self.model_type, inference_model_dir = self._check_input_model(\n                model_name, inference_model_dir)\n            self._config = init_config(self.model_type, model_name,\n                                       inference_model_dir, **kwargs)\n\n            if self.model_type == \"shitu\":\n                if index_dir:\n                    self._config.IndexProcess.index_dir = index_dir\n                self.predictor = SystemPredictor(self._config)\n            else:\n                self.predictor = ClsPredictor(self._config)\n\n    def get_config(self):\n        \"\"\"Get the config.\n        \"\"\"\n        return self._config\n\n    def _check_input_model(self, model_name, inference_model_dir):\n        \"\"\"Check input model name or model files.\n        \"\"\"\n        all_imn_model_names = get_imn_model_names()\n        all_pulc_model_names = PULC_MODELS\n        all_shitu_model_names = SHITU_MODELS\n\n        if model_name:\n            if model_name in all_imn_model_names:\n                inference_model_dir = check_model_file(\"imn\", model_name)\n                return \"imn\", inference_model_dir\n            elif model_name in all_pulc_model_names:\n                inference_model_dir = check_model_file(\"pulc\", model_name)\n                return \"pulc\", inference_model_dir\n            elif model_name in all_shitu_model_names:\n                inference_model_dir = check_model_file(\n                    \"shitu\",\n                    \"PP-ShiTuV2/general_PPLCNetV2_base_pretrained_v1.0\")\n                inference_model_dir = check_model_file(\n                    \"shitu\", \"picodet_PPLCNet_x2_5_mainbody_lite_v1.0\")\n                inference_model_dir = os.path.abspath(\n                    os.path.dirname(inference_model_dir))\n                return \"shitu\", inference_model_dir\n            else:\n                similar_imn_names = similar_model_names(model_name,\n                                                        all_imn_model_names)\n                similar_pulc_names = similar_model_names(model_name,\n                                                         all_pulc_model_names)\n                similar_names_str = \", \".join(similar_imn_names +\n                                              similar_pulc_names)\n                err = f\"{model_name} is not provided by PaddleClas. \\nMaybe you want the : [{similar_names_str}]. \\nIf you want to use your own model, please specify inference_model_dir!\"\n                raise InputModelError(err)\n        elif inference_model_dir:\n            model_file_path = os.path.join(inference_model_dir,\n                                           \"inference.pdmodel\")\n            params_file_path = os.path.join(inference_model_dir,\n                                            \"inference.pdiparams\")\n            if not os.path.isfile(model_file_path) or not os.path.isfile(\n                    params_file_path):\n                err = f\"There is no model file or params file in this directory: {inference_model_dir}\"\n                raise InputModelError(err)\n            return \"custom\", inference_model_dir\n        else:\n            err = \"Please specify the model name supported by PaddleClas or directory contained model files(inference.pdmodel, inference.pdiparams).\"\n            raise InputModelError(err)\n        return None\n\n    def predict_cls(self,\n                    input_data: Union[str, np.array],\n                    print_pred: bool=False) -> Generator[list, None, None]:\n        \"\"\"Predict input_data.\n\n        Args:\n            input_data (Union[str, np.array]):\n                When the type is str, it is the path of image, or the directory containing images, or the URL of image from Internet.\n                When the type is np.array, it is the image data whose channel order is RGB.\n            print_pred (bool, optional): Whether print the prediction result. Defaults to False.\n\n        Raises:\n            ImageTypeError: Illegal input_data.\n\n        Yields:\n            Generator[list, None, None]:\n                The prediction result(s) of input_data by batch_size. For every one image,\n                prediction result(s) is zipped as a dict, that includs topk \"class_ids\", \"scores\" and \"label_names\".\n                The format of batch prediction result(s) is as follow: [{\"class_ids\": [...], \"scores\": [...], \"label_names\": [...]}, ...]\n        \"\"\"\n\n        if isinstance(input_data, np.ndarray):\n            yield self.predictor.predict(input_data)\n        elif isinstance(input_data, str):\n            if input_data.startswith(\"http\") or input_data.startswith(\"https\"):\n                image_storage_dir = partial(os.path.join, BASE_IMAGES_DIR)\n                if not os.path.exists(image_storage_dir()):\n                    os.makedirs(image_storage_dir())\n                image_save_path = image_storage_dir(\"tmp.jpg\")\n                download_with_progressbar(input_data, image_save_path)\n                logger.info(\n                    f\"Image to be predicted from Internet: {input_data}, has been saved to: {image_save_path}\"\n                )\n                input_data = image_save_path\n            image_list = get_image_list(input_data)\n\n            batch_size = self._config.Global.get(\"batch_size\", 1)\n\n            img_list = []\n            img_path_list = []\n            cnt = 0\n            for idx_img, img_path in enumerate(image_list):\n                img = cv2.imread(img_path)\n                if img is None:\n                    logger.warning(\n                        f\"Image file failed to read and has been skipped. The path: {img_path}\"\n                    )\n                    continue\n                img = img[:, :, ::-1]\n                img_list.append(img)\n                img_path_list.append(img_path)\n                cnt += 1\n\n                if cnt % batch_size == 0 or (idx_img + 1) == len(image_list):\n                    preds = self.predictor.predict(img_list)\n\n                    if preds:\n                        for idx_pred, pred in enumerate(preds):\n                            pred[\"filename\"] = img_path_list[idx_pred]\n                            if print_pred:\n                                logger.info(\", \".join(\n                                    [f\"{k}: {pred[k]}\" for k in pred]))\n\n                    img_list = []\n                    img_path_list = []\n                    yield preds\n        else:\n            err = \"Please input legal image! The type of image supported by PaddleClas are: NumPy.ndarray and string of local path or Ineternet URL\"\n            raise ImageTypeError(err)\n        return\n\n    def predict_shitu(self,\n                      input_data: Union[str, np.array],\n                      print_pred: bool=False) -> Generator[list, None, None]:\n        \"\"\"Predict input_data.\n        Args:\n            input_data (Union[str, np.array]):\n                When the type is str, it is the path of image, or the directory containing images, or the URL of image from Internet.\n                When the type is np.array, it is the image data whose channel order is RGB.\n            print_pred (bool, optional): Whether print the prediction result. Defaults to False.\n\n        Raises:\n            ImageTypeError: Illegal input_data.\n\n        Yields:\n            Generator[list, None, None]:\n                The prediction result(s) of input_data by batch_size. For every one image,\n                prediction result(s) is zipped as a dict, that includs topk \"class_ids\", \"scores\" and \"label_names\".\n                The format of batch prediction result(s) is as follow: [{\"class_ids\": [...], \"scores\": [...], \"label_names\": [...]}, ...]\n        \"\"\"\n        if input_data is None and self._config.Global.infer_imgs:\n            input_data = self._config.Global.infer_imgs\n\n        if isinstance(input_data, np.ndarray):\n            yield self.predictor.predict(input_data)\n        elif isinstance(input_data, str):\n            if input_data.startswith(\"http\") or input_data.startswith(\"https\"):\n                image_storage_dir = partial(os.path.join, BASE_IMAGES_DIR)\n                if not os.path.exists(image_storage_dir()):\n                    os.makedirs(image_storage_dir())\n                image_save_path = image_storage_dir(\"tmp.jpg\")\n                download_with_progressbar(input_data, image_save_path)\n                logger.info(\n                    f\"Image to be predicted from Internet: {input_data}, has been saved to: {image_save_path}\"\n                )\n                input_data = image_save_path\n            image_list = get_image_list(input_data)\n\n            cnt = 0\n            for idx_img, img_path in enumerate(image_list):\n                img = cv2.imread(img_path)\n                if img is None:\n                    logger.warning(\n                        f\"Image file failed to read and has been skipped. The path: {img_path}\"\n                    )\n                    continue\n                img = img[:, :, ::-1]\n                cnt += 1\n\n                preds = self.predictor.predict(\n                    img)  # [dict1, dict2, ..., dictn]\n                if preds:\n                    if print_pred:\n                        logger.info(f\"{preds}, filename: {img_path}\")\n\n                yield preds\n        else:\n            err = \"Please input legal image! The type of image supported by PaddleClas are: NumPy.ndarray and string of local path or Ineternet URL\"\n            raise ImageTypeError(err)\n        return\n\n    def predict(self,\n                input_data: Union[str, np.array],\n                print_pred: bool=False,\n                predict_type=\"cls\"):\n        assert predict_type in [\"cls\", \"shitu\"\n                                ], \"Predict type should be 'cls' or 'shitu'.\"\n        if predict_type == \"cls\":\n            return self.predict_cls(input_data, print_pred)\n        elif predict_type == \"shitu\":\n            assert not isinstance(input_data, (\n                list, tuple\n            )), \"PP-ShiTu predictor only support single image as input now.\"\n            return self.predict_shitu(input_data, print_pred)\n        else:\n            raise ModuleNotFoundError\n\n\n# for CLI\ndef main():\n    \"\"\"Function API used for commad line.\n    \"\"\"\n    print_info()\n    cfg = args_cfg()\n    clas_engine = PaddleClas(**cfg)\n    if cfg[\"build_gallery\"] == False:\n        res = clas_engine.predict(\n            cfg[\"infer_imgs\"],\n            print_pred=True,\n            predict_type=cfg[\"predict_type\"])\n        for _ in res:\n            pass\n        logger.info(\"Predict complete!\")\n    return\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "ppcls",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 1.0087890625,
          "content": "[build-system]\nrequires = [\"setuptools==72.1.0\"]\nbuild-backend = \"setuptools.build_meta\" \n\n[project]\nname = \"paddleclas\"\ndescription = \"A treasure chest for visual recognition powered by PaddlePaddle.\"\n\nkeywords=[\n    'image-classification', 'image-recognition', 'pretrained-models',\n    'knowledge-distillation', 'product-recognition', 'autoaugment',\n    'cutmix', 'randaugment', 'gridmask', 'deit', 'repvgg',\n    'swin-transformer', 'image-retrieval-system'\n]\nclassifiers=[\n    'Development Status :: 5 - Production/Stable',\n    'Operating System :: OS Independent',\n    'Intended Audience :: Developers',\n    'Intended Audience :: Education',\n    'Intended Audience :: Science/Research',\n    'License :: OSI Approved :: Apache Software License',\n]\n\nreadme = \"README.md\"\nlicense = {file = \"LICENSE\"}\nrequires-python = \">=3.8\"\n\ndynamic = [\"version\", \"dependencies\"]\n\n[project.scripts]\npaddleclas = \"paddleclas.paddleclas:main\"\n\n[tool.setuptools.dynamic]\nversion = {file = \"version.txt\"}\ndependencies = {file = \"requirements.txt\"}\n\n\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2236328125,
          "content": "prettytable\nujson\nopencv-python<=4.6.0.66\npillow>=9.0.0\ntqdm\nPyYAML>=5.1\nvisualdl>=2.2.0\nscipy>=1.0.0\nscikit-learn>=0.21.0\ngast==0.3.3\nfaiss-cpu\neasydict\nnumpy==1.24.4; python_version<\"3.13\"\nnumpy==1.26.4; python_version>=\"3.13\"\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.84765625,
          "content": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom setuptools import setup\n\n\nsetup(\n    packages=['paddleclas'],\n    package_dir={'paddleclas': ''},\n    include_package_data=True,\n    url='https://github.com/PaddlePaddle/PaddleClas',\n    download_url='https://github.com/PaddlePaddle/PaddleClas.git',\n)\n"
        },
        {
          "name": "test_tipc",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "version.txt",
          "type": "blob",
          "size": 0.005859375,
          "content": "2.6.0\n"
        }
      ]
    }
  ]
}