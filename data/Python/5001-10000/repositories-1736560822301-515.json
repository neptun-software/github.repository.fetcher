{
  "metadata": {
    "timestamp": 1736560822301,
    "page": 515,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/pytext",
      "stars": 6334,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.0673828125,
          "content": "[run]\nbranch = True\nconcurrency = multiprocessing\ninclude = pytext/*\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.400390625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[codi]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\neggs/\n.eggs/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# Environments\n.env\n.venv\npytext_venv/\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Backups\n*~\n\n# Coverage Files\n.coverage\n\n# Generated Document Sources\npytext/docs/source/configs/*\npytext/docs/source/modules/*\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 21.8955078125,
          "content": "# Changelog\n\n## v0.3.3\n\n### New features\n\n- Add XLM-R document classification server + console (#1358)\n- MLP layer embed for float tensors and `FloatListSeqTensorizer` for `List[List[[float]]` features. (#1374)\n- Add `class_accuracy` in `MultiLabelSoftClassificationMetrics` (#1371)\n- Add an option to skip test run after models have been trained (#1372)\n- Support DP in PyText (#1366)\n- Support torchscriptify in multi_label_classification_layer (#1350)\n- Add custom metric class for reporting Joint model metrics (#1339)\n- MultiLabel-MultiClass Model for Joint Sequence Tagging (#1335)\n- Scripted tokenizer support for DocModel (#1314)\n\n### Bugfixes\n- Fixed metric reporter aggregation and output layer for the multi-label classification\n- Remove `move_state_dict_to_gpu`, which is causing CUDA OOM (#1367)\n- Fix Flow's default conversion of dict to AttrDict\n- Fix bug in `ClassificationOutputLayer` that `pad_idx` is never respected (#1347)\n- Serializing/Deserializing type Any: bugfix and simplification (#1344)\n- Fix RoBERTa Q&A Training Bug with multiple BoS tokens. (#1343)\n\n### Other\n- Better error message for misconfigured data fields\n- Replace deprecated integer division with floor division operator\n- Add informative prints to assert statements (#1360)\n- TorchScript: Put dense tensor on the same device with other input tensors (#1361)\n- Update PyTorch + ONNX (#1340)\n- Update PyTorch + ONNX (#1340)- binary ONNX\n- Update PR Template (#1349)\n- Reduce memory request for pytext train operator\n- Add 'contrib' directory for experimental code (#1333)\n\n## v0.3.2\n\n### New features\n\n- Support read file from http URL (#1317)\n- add a new PyText get_num_examples_from_batch function in model (#1319)\n- Add support for length label smoothing (#1308)\n- Add new metrics type for Masked Seq2Seq Joint Model (#1304)\n- Add mask generator and strategy (#1302)\n- Add separate logging for label loss and length loss (#1294)\n- Add tensorizer support for masking of target tokens (#1297)\n- Add length prediction and basic masked generator (#1290)\n- Add self attention option to conv_encoder and conv_decoder (#1291)\n- Entity Saliency modeling on PyText: EntitySalienceMetricReporter/EntitySalienceTask\n- In-batch negative training for BertPairwiseModel\n- Support embedding from decoder (#1284)\n- Add dense features to Roberta\n- Add projection layer to HuggingFace encoder (#1273)\n- add PyText Embedding TorchScript Wrapper\n- Add option to pad missing label in LabelListTensorizer (#1269)\n- Integrate PET and Introduce ElasticTrainer (#1266)\n- support PoolingType in DocNN. (#1259)\n- Added WordSeqEmbedding (#1255)\n- Open source Assistant NLU seq2seq model (#1236)\n- Support multi label classification\n- BART in decoupled model\n\n### Bug fixes\n\n- Cast model output to cpu (#1329)\n- Fix OSS predict-py API (#1320)\n- Fix \"calling median on empty tensor\" issue in MR (#1322)\n- Add ScriptDoNothingTokenizer so that torchscriptification of SPM does not fail (#1316)\n- Fix creating generator everytime (#1301)\n- fix dense feature for fp16\n- Avoid edge cases with quantization by setting a known seed (#1295)\n- Make torchscript predictions even on empty text / token inputs\n- fix dense feature TorchScript typing (#1281)\n- avoid zero division error in metrics reporter (#1271)\n- Fix contiguous issue in bilstm export (#1270)\n- fix debug file generation for multilabel classification (#1247)\n- Fix fp16 optimizer attribute name\n\n### Other\n\n- New Debug File for masked seq2seq\n- Move MockConfigLoader to OSS (#1324)\n- Pass in optimizer config instead of create_optimizer to trainer\n- Remove unnecessary torch.no_grad() block (#1323)\n- Fix Memory Issues in Metric Reporter for Classification Tasks over large Label Spaces\n- Add contextual embedding support to OS seq2seq model (#1299)\n- recover xlm_r tutorial notebook (#1305)\n- Enable controlling bias in MLP decoder\n- Migrate serving tutorial to TorchScript (#1310)\n- delete caffe2 export (#1307)\n- add whitelist for ONNX export\n- Use dynamic quantization api for BeamSearch (#1303)\n- Remove requirement that eos/bos be supplied for sequence export. (#1300)\n- Multicolumn support\n- Multicolumn support in torchscriptify\n- Add caching support to RawExample and batch predict API (#1298)\n- Add save-pytext-snapshot command to PyText cmdline (#1285)\n- Update with Whatsapp calling data + support dictionary features (#1293)\n- add arrange_caffe2_model_inputs in BaseModel (#1292)\n- Replace unit-tests on LMModel and FLLanguageModelingTask by LiteLMModel and FLLiteLMTask (#1296)\n- changes to make mbart work (#1911)\n- handle encoder and decoder embedding\n- Add tutorial for semantic parsing. (#1288)\n- Add new fb beam search with fused operator (#1287)\n- Move generator builder to constructor so that it can easily overridden. (#1286)\n- Torchscriptify ELTensorizer (#1282)\n- Torchscript export for Seq2Seq model (#1265)\n- Change Seq2Seq model from_config() to a more general api (#1280)\n- add max_seq_len to DocNN TorchScript model (#1279)\n- support XLM-R model Embedding in TorchScript (#1278)\n- Generic PyText Checkpoint Manager Interface (#1267)\n- Fix backward compatibility issue of pad_missing in LabelListTensorizer (#1277)\n- Update mean reduction in NLLLoss (#1272)\n- migrate pages.integrity.scam.docnn_models.xxx (#1275)\n- Unify model input for ByteTokensDocumentModel (#1274)\n- Torchscriptify TokenTensorizer\n- Allow dictionaries to overwrite entries with #fairseq:overwrite comment (#1073)\n- Make WordSeqEmbedding ONNX compatible\n- If the snapshot path provided is not valid, throw error (#1268)\n- support vocab filter by min count\n- Unify input for TorchScript Tensorizers and Models (#1256)\n- Torchscriptify XLM-R\n- Add class logging to task (#1264)\n- Add usage logging to exporter (#1262)\n- Add usage logging across models (#1263)\n- Usage logging on data classes (#1261)\n- GPT2 BPE add lower casing support (#1260)\n- FAISS Embedding Search Space [3/5]\n- Return len of tokens of each sequence in SeqTokenTensorizer (#1254)\n- Vocab Limited Pretrained Embedding [2/5] (#1248)\n- add Stage.OTHERS and allow TB to print to a seperate prefix not in (TRAIN, TEST, EVAL) (#1258)\n- Add option to skip 2 stage tokenizer and bpe decode sequences in the debug file (#1257)\n- Add Testcase for Wordpiece Tokenizer (#1249)\n- modify accuracy calculation for multi-label classification (#1244)\n- Enable tests in pytext/config:pytext_all_config_test\n- Introduce Class Usage Logging (#1243)\n- Make PyText compatible with Any type (#1242)\n- Make dict_embedding Torchscript friendly (#1240)\n- Support MultipleData for export and kd generation\n- delete flaky/broken tests (#1238)\n- Add support for returning start & end indices.\n\n\n## v0.3.1\n\n### New features\n- Implement SquadQA tensorizer in TorchScript (#1211)\n- Add session data source for df (#1202)\n- Dynamic Batch Scheduler Implementation (#1200)\n- Implement loss aware sparsifier (#1204)\n- Ability to Fine-tune XLM-R for NER on CoNLL Datasets (#1201)\n- TorchScriptify Tokenizer after training (#1191)\n- Linear Layer only blockwise sparsifier (#478)\n- Adding performance graph to pytext models (#1192)\n- Enable inference on GPUs by moving tensors to specified device (#472)\n- Add support for learning from soft labels for Squad (MRC) models (#1188)\n- Create byte-aware model that can make byte predictions (#468)\n- Minimum Trust Lamb (#1186)\n- Allow model to take byte-level input and make byte-level prediction (#1187)\n- Scheduler with Warmup (#1184)\n- Implement LAMB optimizer (#1183)\n- CyclicLRScheduler (#1157)\n- PyText Entity Linking: ELTask and ELMetricReporter (#1165)\n\n### Bug fixes\n- Don't upgrade if Tensorizer already given (#504)\n- avoid torchscriptify on a ScriptModule (#1214)\n- Make tensorboard robust to NaN and Inf in model params (#1206)\n- Fix circleCLI Test broken in D19027834 (#1205)\n- Fix small bug in pytext vocabulary (#401)\n- Fix CircleCI failure caused by black and regex (#1199)\n- Fix CircleCI (#1194)\n- Fix Circle CI Test broken by D18880705 (#1190)\n- fix weight load for new fairseq checkpoints (#1189)\n- Fix Heirarchical intent and slot filling demo is broken (#1012) (#1151)\n- Fix index error in dict embedding when exported to Caffe2 (#1182)\n- Fix zero loss tensor in SquadOutputLayer (#1181)\n- qa fix for ignore_impossible=False\n\n### Other\n\n- Printing out error's underlying reason (#1227)\n- tidy file path in help text for invocation of docnn.json example (#1221)\n- PyText option to disable CUDA when testing. (#1223)\n- make augmented lstm compatible w other lstms (#1224)\n- Vocab recursive lookup (#1222)\n- Fix simple typo: valus -> value (#1219)\n- support using RoundRobin ProcessGroup in Distributed training (#1213)\n- Use PathManager for all I/O (#1198)\n- Make PathManager robust to API changes in fvcore (#1196)\n- Support for TVM training (BERT) (#1210)\n- Exit LM task if special token exists in text for ByteTensorizer (#1207)\n- Config adapter for pytext XLM (#1172)\n- Use TensorizerImpl for both training and inference for BERT, RoBERTa and XLM tensorizer (#1195)\n- Replace gluster paths with local file paths for NLG configs (#1197)\n- Make BERT Classification compatible with TSEs that return Encoded Layers.\n- implement BertTensorizerImpl and XLMTensorizerImpl (#1193)\n- Make is_input field of tensorizer configurable (#474)\n- BERTTensorizerBaseImpl to reimplement BERTTensorizerBase to be TorchScriptable (#1163)\n- Improve LogitsWorkflow to handle dumping of raw inputs and multiple output tensors (#683)\n- Accumulative blockwise pruning (#1170)\n- Patch for UnicodeDecodeError due to BPE. (#1179)\n- Add pre-loaded task as parameter to caffe2 batch prediction API\n- Specify CFLAGS to install fairseq in MacOS (#1175)\n- Resolve dependency conflict by specifying python-dateutil==2.8.0 (#1176)\n- Proper training behavior if setting do_eval=False (#1155)\n- Make DeepCNNRepresentation torchscriptable (#453)\n\n\n## v0.3.0\n\n### New Features\n**RoBERTa and XLM-R**\n- Integrate XLM-R into PyText (#1120)\n- Consolidate BERT, XLM and RobERTa Tensorizers (#1119)\n- Add XLM-R for joint model (#1135)\n- Open source Roberta (#1032)\n- Simple Transformer module components for RoBERTa (#1043)\n- RoBERTa models for document classification (#933)\n- Enable MLM training for RobertaEncoder (#1126)\n- Standardize RoBERTa Tensorizer Vocab Creation (#1113)\n- Make RoBERTa usable in more tasks including QA (#1017)\n- RoBERTa-QA JIT (#1088)\n- Unify GPT2BPE Tokenizer (#1110)\n- Adding Google SentencePiece as a Tokenizer (#1106)\n\n**TorchScript support**\n- General torchscript module (#1134)\n- Support torchscriptify XLM-R (#1138)\n- Add support for torchscriptification of XLM intent slot models (#1167)\n- Script xlm tensorizer (#1118)\n- Refactor ScriptTensorizer with general tensorize API (#1117)\n- ScriptXLMTensorizer (#1123)\n- Add support for Torchscript export of IntentSlotOutputLayer and CRF (#1146)\n- Refactor ScriptTensorizor to support both text and tokens input (#1096)\n- Add torchscriptify API in tokenizer and tensorizer (#1055)\n- Add more stats in torchscript latency script (#1044)\n- Exported Roberta torchscript model include both traced_model and pre-processing logic (#1013)\n- Native Torchscript Wordpiece Tokenizer Op for BERTSquadQA, Torchscriptify BertSQUADQAModel (#879)\n- TorchScript-ify BERT training (#887)\n- Modify Return Signature of TorchScript BERT (#1058)\n- Implement BertTensorizer and RoBERTaTensorizer in TorchScript (#1053)\n\n**Others**\n- FairseqModelEnsemble class (#1116)\n- Inverse Sqrt Scheduler (#1150)\n- Lazy modules (#1039)\n- Adopt Fairseq MemoryEfficientFP16Optimizer in PyText (#910)\n- Add RAdam (#952)\n- Add AdamW (#945)\n- Unify FP16&FP32 API (#1006)\n- Add precision at recall metric (#1079)\n- Added PandasDataSource (#1098)\n- Support testing Caffe2 model (#1097)\n- Add contextual feature support to export for Seq2Seq models\n- Convert matmuls to quantizable nn.Linear modules (#1304)\n- PyTorch eager mode implementation (#1072)\n- Implement Blockwise Sparsification (#1050)\n- Support Fairseq FP16Optimizer (#1008)\n- Make FP16OptimizerApex wrapper on Apex/amp (#1007)\n- Remove vocab from cuda (#955)\n- Add dense input to XLMModel (#997)\n- Replace tensorboardX with torch.utils.tensorboard (#1003)\n- Add mentioning of mixed precision training support (#643)\n- Sparsification for CRF transition matrix (#982)\n- Add dense feature normalization to Char-LSTM TorchScript model. (#986)\n- Cosine similarity support for BERT pairwise model training (#967)\n- Combine training data from multiple sources (#953)\n- Support visualization of word embeddings in Tensorboard (#969)\n- Decouple decoder and output layer creation in BasePairwiseModel (#973)\n- Drop rows with insufficient columns in TSV data source (#954)\n- Add use_config_from_snapshot option(load config from snapshot or current task) (#970)\n- Add predict function for NewTask (#936)\n- Use `create_module` to create CharacterEmbedding (#920)\n- Add XLM based joint model\n- Add `ConsistentXLMModel` (#913)\n- Optimize Gelu module for caffe2 export (#918)\n- Save best model's sub-modules when enabled (#912)\n\n### Documentation / Usability\n- XLM-R tutorial in notebook (#1159)\n- Update XLM-R OSS tutorial and add Google Colab link (#1168)\n- Update \"raw_text\" to \"text\" in tutorial (#1010)\n- Make tutorial more trivial (add git clone) (#1037)\n- Changes to make tutorial code simpler (#1002)\n- Fix datasource tutorial example (#998)\n- Handle long documents in squad qa datasource and models (#975)\n- Fix pytext tutorial syntax (#971)\n- Use torch.equal() instead of \"==\" in Custom Tensorizer tutorial (#939)\n- Remove and mock doc dependencies because readthedocs is OOM (#983)\n- Fix Circle CI build_docs error (#959)\n- Add OSS integration tests: DocNN (#1021)\n- Print model into the output log (#1127)\n- Migrate pytext/utils/torch.py logic into pytext/torchscript/ for long term maintainability (#1082)\n- Demo datasource fix + cleanup (#994)\n- Documentation on the config files and config-related commands (#984)\n- Config adapter old data handler helper (#943)\n- Nicer gen_config_impl (#944)\n\n### Deprecated Features\n- Remove DocModel_Deprecated (#916)\n- Remove RNNGParser_Deprecated, SemanticParsingTask_Deprecated, SemanticParsingCppTask_Deprecate, RnngJitTask,\n- Remove QueryDocumentTask_Deprecated(#926)\n- Remove LMTask_Deprecated and LMLSTM_Deprecated (#882)\n- CompositionDataHandler to fb/deprecated (#963)\n- Delete deprecated Word Tagging tasks, models and data handlers (#910)\n\n### Bug Fixes\n- Fix caffe2 predict (#1103)\n- Fix bug when tensorizer is not defined (#1169)\n- Fix multitask metric reporter for lr logging (#1164)\n- Fix broken gradients logging and add lr logging to tensorboard (#1158)\n- Minor fix in blockwise sparsifier (#1130)\n- Fix clip_grad_norm API (#1143)\n- Fix for roberta squad tensorizer (#1137)\n- Fix multilabel metric reporter (#1115)\n- Fixed prepare_input in tensorizer (#1102)\n- Fix unk bug in exported model (#1076)\n- Fp16 fixes for byte-lstm and distillation (#1059)\n- Fix clip_grad_norm_ if grad_norm > max_norm > 0: TypeError: '>' not supported between instances of 'float' and 'NoneType' (#1054)\n- Fix context in multitask (#1040)\n- Fix regression in ensemble trainer caused by recent fp16 change (#1033)\n- ReadTheDocs OOM fix with CPU Torch (#1027)\n- Dimension mismatch after setting max sequence length (#1154)\n- Allow null learning rate (#1156)\n- Don't fail on 0 input (#1104)\n- Remove side effect during pickling PickleableGPT2BPEEncoder\n- Set onnx==1.5.0 to fix CircleCI build temporarily (#1014)\n- Complete training loop gracefully even if no timing is reported (#1128)\n- Propagate min_freq for vocab correctly (#907)\n- Fix gen-default-config with Model param (#917)\n- Fix torchscript export for PyText modules (#1125)\n- Fix label_weights in DocModel (#1081)\n- Fix label_weights in bert models (#1100)\n- Fix config issues with Python 3.7 (#1066)\n- Temporary fix for Fairseq dependency (#1026)\n- Fix MultipleData by making tensorizers able to initialize from multiple data sources (#972)\n- Fix bug in copy_unk (#964)\n- Division by Zero bug in MLM Metric Reporter (#968)\n\n\n## v0.2.2\n\n*Note:* this is the last release with _Deprecated classes. Those classes will be removed in the next release.\n\n**New Features:**\n- DeepCNN Representation for word tagging\n- Combine KLDivergenceBCELoss with SoftHardBCELoss and F.cross_entropy() in CrossEntropyLoss (#689)\n- add dense feature support for doc model (#710)\n- add torchscript quantizaiton support in pytext\n- pytext multi-label support (#731)\n- open source transformer representations (#736)\n- open source transformer based models - data, tensorizers and tokenizer (#708)\n- Create AlternatingRandomizedBatchSampler (#737)\n- open source MaskedLM and BERT models (#734)\n- Support bytes input in word tagging model OSS (#745)\n- open source extractive question answering models (#742)\n- torchscriptify for ensemle task\n- enabled lmlstm labels exporting (#767)\n- Enable dense features in ByteTokensDocumentModel (#763)\n- created bilstm dropout condition (#769)\n- enabled lmlstm caffe2 exporting (#766)\n- PolynomialDecayScheduler (#791)\n- removed bilstm dependence on seq_lengths (#776)\n- fp16 optimizer (#782)\n- Add Dense Feature Normalization to FloatListTensorizer and DocModel (#859)\n- Add Sparsifier component to PyText and L0-projection based sparsifier (#860)\n- implemented cnn pooling for doc classification (#872)\n- implemented bottleneck separable convolutions (#855)\n- Add eps to Adam (#858)\n- implemented mobile exporter (#785)\n- support starting training from saved checkpoint (#824)\n- implemented separable convolutions (#830)\n- implemented gelu activations (#829)\n- implemented causal convolutions (#811)\n- implemented dilation for convolutions (#810)\n- created weight norm option (#809)\n- Ordered Neuron LSTM (#854)\n- Add PersonalizedByteDocModel (#816)\n- CNN based language models (#827)\n- improve csv support in TSVDataSource (#777)\n- Change default batch sampler DisjointMultitaskData to RoundRobinBatchSampler (#802)\n- Support using serialized pretrained embedding file (#797)\n\n**Documentation / Usability / Logging:**\n- Fewer out-of-vocab print messages, with some stats (#697)\n- Echo epoch number to console while training (#712)\n- Separate timing for prediction and metric calculation. (#738)\n- multi-label soft metrics (#754)\n- changed lm metric reporting (#765)\n- fix data source tutorial (#762)\n- fix doc sphinx deprecation warning (#775)\n- Add the ability to pass parameter values to gen-default-config (#856)\n- Remove \"pytext/\" from paths in demo json config (#878)\n- New documentation about hacking pytext and dealing with github. (#862)\n- install_deps supports updates (#863)\n- Reduce number of PEP print (#861)\n- better error message for config with unknown component (#801)\n- Add Recall at Precision Thresholds to Config (#792)\n- implemented perplexity reductions for lm score reporting (#799)\n- adapt prediction workflow to new design (#746)\n\n**Bug fixes:**\n- block sharded tsv eval/test fix (#698)\n- Fix BoundaryPooling tracing (#713)\n- fixes LMLSTM weight tying bug (#704)\n- Fix duplicate entries in vocab (#721)\n- Bugfix for trainer not reporting eval results (#740)\n- Reintroduce metrics export in new task (#748)\n- fix open source tests (#750)\n- Fix missing init_tensorizers arg (#893)\n- Fix intent slot metric reporter not working with byte offset (#883)\n- Fix issue with some tensorizers still re-initializing vocab when loaded from saved state (#848)\n- fixed overflow error in lm reporting (#831)\n- fix BlockShardedTSVDataSource (#832)\n\n\n## v0.2.1\n\n(skipped because of packaging issues)\n\n\n## v0.2.0\n\n*Note:* This release makes the new data handler API the default and deprecates Task and Model classes using the old data handler API. We recommend that you migrate your models to the new API as soon as possible. More details here: http://...\n\n**New Stuff**\n- most tasks and models deprecated, replaced with better versions using the new data handler API\n- performance improvements in metric reporter\n- Add Multilingual TSV Data Source\n- LabelSmoothedCrossEntropyLoss\n- Support for pretrained word embedding in TokenTensorizer\n- option to use pretrained embedding\n- TorchScript export for document classification\n- Improve log in trainer\n- performance measurement: reporting tokens_per_second and updates_per_second\n- Implement DocumentReader from DrQA in PyText (StackedBidirectionalRNN)\n- improved and updated documentation\n- Implement SWA(SGD|ADAM) and Adagrad Optimizers\n- cache numerized data in memory\n- TorchScript BPE tokenization\n- CLI command to update configs\n- Visualize gradients with tensorboard\n\n*Many bug fixes and code clean-ups*\n\n\n## v0.1.5\n\n*Note:* this is a last release in 0.1.x. The next release will deprecate Task and Model base classes and make the improved API of the new data handler the default. You can start using it already by inheriting from NewTask. NewDocumentClassification and NewWordTaggingTask use this new API, and you can get the first example in the tutorial \"Custom Data Format\".\n\n**New Stuff**\n- add config adapter\n  - PyText is very young and its API is still in flux, making the config files brittle\n  - config files now have a version number reflecting the API at the time it was created\n  - older versions can be loaded and internally transformed into newer versions\n- better metrics and reporting\n  - better training time tracking\n  - cool new visualization of model state in TensorBoard\n  - pretty results in the terminal\n- improved distributed training\n- torchscript export\n- support for SQuAD dataset\n- add AugmentedLSTM\n- add dense features support\n- new plugin system: command line option --include to import custom user classes (see tutorial \"Custom Data Format\" for example)\n\n*Many bug fixes and code clean-ups*\n\n\n## v0.1.4\n**New Stuff**\n- Refactor Metric Reporters to reduce coupling\n- RNNG Improvements:\n  - Support Pretrained embeddings in RNNG\n  - Support GPU Training\n  - More Test Coverage\n  - Tensorboard Support\n- Added `QueryDocumentPairwiseRankingModel`\n- Distributed Training Improvments:\n  - Sharded Data Loading to reduce memory consumption\n  - Fix Several issues with race conditions and unserializable state\n- Reduced GPU memory Consumption by skipping gradient computation on evaluation\n\n*And lots of bug fixes*\n\n**Known Issues**\nPyText doesn't work with the new ONNX v1.4.0, so we have pinned it to 1.3.0 for now\n\n\n## v0.1.3\n - Remove epoch_size param from DisjointMultitask, use target_task (or shortest) to set epoch_size\n\n## v0.1.0\n\nInitial version\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.27734375,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <opensource-conduct@fb.com>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.6865234375,
          "content": "# Contributing to PyText\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `master`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nFacebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## Coding Style  \nWe use isort and black to format our code, you can use the following commands to format your code prior to submission:\n\n```\n(pytext_venv) $ pip install isort black\n(pytext_venv) $ black pytext \n(pytext_venv) $ isort pytext --recursive --multi-line 3 --trailing-comma --force-grid-wrap 0 --line-width 88 --lines-after-imports 2 --combine-as --section-default THIRDPARTY\n```\n\n## Updates to Docs\nThe documentation build process work with Python 3.7 and above. \n \n## License\nBy contributing to PyText, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.4970703125,
          "content": "BSD License\n\nFor PyText software\n\nCopyright (c) Facebook, Inc. and its affiliates. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n * Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n * Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\n * Neither the name Facebook nor the names of its contributors may be used to\n   endorse or promote products derived from this software without specific\n   prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.4296875,
          "content": "<table align=\"center\">\n<tr>\n<td>\n<h1 align=\"center\">\n⚠️ Please migrate to <a href=\"https://github.com/pytorch/text\"><b>torchtext</b></a> ⚠️\n</h1>\n<p align=\"center\">\nPyText is deprecated and will no longer be actively maintained. Please check out <a href=\"https://github.com/pytorch/text\">torchtext</a> and contribute there!\n</p>\n</td>\n</tr>\n</table>\n\n\n# Overview\n\n[![Support Ukraine](https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB)](https://opensource.fb.com/support-ukraine)\n[![CircleCI](https://circleci.com/gh/facebookresearch/pytext.svg?style=svg&circle-token=2e0e0cb6dc686b646df887c2e0f07a8429712243)](https://circleci.com/gh/facebookresearch/pytext)\n\nPyText is a deep-learning based NLP modeling framework built on PyTorch. PyText addresses the often-conflicting requirements of enabling rapid experimentation and of serving models at scale. It achieves this by providing simple and extensible interfaces and abstractions for model components, and by using PyTorch’s capabilities of exporting models for inference via the optimized Caffe2 execution engine. We are using PyText in Facebook to iterate quickly on new modeling ideas and then seamlessly ship them at scale.\n\n**Core PyText features:**\n- Production ready models for various NLP/NLU tasks:\n  - Text classifiers\n    - [Yoon Kim (2014): Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)\n    - [Lin et al. (2017): A Structured Self-attentive Sentence Embedding](https://arxiv.org/abs/1703.03130)\n  - Sequence taggers\n    - [Lample et al. (2016): Neural Architectures for Named Entity Recognition](https://www.aclweb.org/anthology/N16-1030)\n  - Joint intent-slot model\n    - [Zhang et al. (2016): A Joint Model of Intent Determination and Slot Filling for Spoken Language Understanding](https://www.ijcai.org/Proceedings/16/Papers/425.pdf)\n  - Contextual intent-slot models\n- Distributed-training support built on the new C10d backend in PyTorch 1.0\n- Mixed precision training support through [APEX](https://github.com/NVIDIA/apex) (trains faster with less GPU memory on [NVIDIA Tensor Cores](https://developer.nvidia.com/tensor-cores))\n- Extensible components that allows easy creation of new models and tasks\n- Reference implementation and a pretrained model for the paper: [Gupta et al. (2018): Semantic Parsing for Task Oriented Dialog using Hierarchical Representations](http://aclweb.org/anthology/D18-1300)\n- Ensemble training support\n- Reference implementation for the paper: [Babu et al. 2021: Non-autoregressive Semantic Parsing for Compositional Task Oriented Dialog](https://github.com/facebookresearch/pytext/tree/master/pytext/models/seq_models/benchmarks/nar_top.json)\n\n# Installing PyText\n\n### PyText requires Python 3.6.1 or above.\n\n*To get started on a Cloud VM, check out [our guide](https://pytext.readthedocs.io/en/master/installation.html#cloud-vm-setup)*.\n\nGet the source code:\n```\n  $ git clone https://github.com/facebookresearch/pytext\n  $ cd pytext\n```\nCreate a virtualenv and install PyText:\n\n```\n  $ python3 -m venv pytext_venv\n  $ source pytext_venv/bin/activate\n  (pytext_venv) $ pip install pytext-nlp\n```\n\nDetailed instructions and more installation options can be found in our [Documentation](https://pytext.readthedocs.io/en/master/installation.html). If you encounter issues with missing dependencies during installation, please refer to [OS Dependencies](https://pytext.readthedocs.io/en/master/installation.html#os-dependencies).\n\n# Train your first text classifier\n\nFor this first example, we'll train a CNN-based text-classifier that classifies text utterances, using the examples in `tests/data/train_data_tiny.tsv`. The data and configs files can be obtained either by cloning the repository or by downloading the files manually from GitHub.\n\n```\n  (pytext_venv) $ pytext train < demo/configs/docnn.json\n```\n\nBy default, the model is created in `/tmp/model.pt`\n\nNow you can export your model as a caffe2 net:\n\n```\n  (pytext_venv) $ pytext export < demo/configs/docnn.json\n```\n\nYou can use the exported caffe2 model to predict the class of raw utterances like this:\n\n```\n  (pytext_venv) $ pytext --config-file demo/configs/docnn.json predict <<< '{\"text\": \"create an alarm for 1:30 pm\"}'\n```\n\nMore examples and tutorials can be found in [Full Documentation](https://pytext.readthedocs.io/en/master/).\n\n# Join the community\n\n* Facebook group: https://www.facebook.com/groups/pytext/\n\n# License\nPyText is BSD-licensed, as found in the LICENSE file.\n"
        },
        {
          "name": "activation_venv",
          "type": "blob",
          "size": 0.173828125,
          "content": "#!/usr/bin/env bash\nVENV_NAME=${1:-pytext_venv}\n\nif [ ! -d \"$VENV_NAME\" ]\nthen\n    python3 -m venv \"$VENV_NAME\"\nfi\n# shellcheck source=/dev/null\nsource \"$VENV_NAME/bin/activate\"\n"
        },
        {
          "name": "activation_venv.bat",
          "type": "blob",
          "size": 0.33203125,
          "content": "@ECHO OFF\n::Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n::Use venv name if passed, otherwise default\nIF \"%1\"==\"\" (\n  SET \"_PYTEXT_ENV_NAME_=pytext_venv\"\n) ELSE (\n  SET \"_PYTEXT_ENV_NAME_=%1\"\n)\n\nIF NOT EXIST %_PYTEXT_ENV_NAME_% (\n  python -m venv %_PYTEXT_ENV_NAME_%\n)\n\ncall %_PYTEXT_ENV_NAME_%\\Scripts\\activate.bat\n"
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs_requirements.txt",
          "type": "blob",
          "size": 0.1572265625,
          "content": "click\nfairseq\nfuture\nhypothesis<4.0\niopath\nmock\nnumpy\nonnx\npytorch-pretrained-bert\nrequests\nsentencepiece\ntorchtext\ntensorboard\ntorch\ntransformers==3.4.0\npandas\n"
        },
        {
          "name": "install_deps",
          "type": "blob",
          "size": 0.412109375,
          "content": "#!/usr/bin/env bash\npip install --upgrade pip\nif [[ \"$OSTYPE\" == \"darwin\"* ]]; then\n    # Mac OSX, need to specify the CFLAGS for fairseq https://github.com/pytorch/fairseq\n    echo \"Mac OS\"\n    CFLAGS=\"-stdlib=libc++\" pip install -e . --upgrade --no-cache-dir --progress-bar off --upgrade-strategy eager\nelse\n    # Any other OS\n    pip install -e . --upgrade --no-cache-dir --progress-bar off --upgrade-strategy eager\nfi\n"
        },
        {
          "name": "install_deps.bat",
          "type": "blob",
          "size": 0.1904296875,
          "content": "@ECHO OFF\n::Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\npython -m pip install --upgrade pip\npip install -e . --process-dependency-links --no-cache-dir --progress-bar off\n"
        },
        {
          "name": "pytest.ini",
          "type": "blob",
          "size": 0.0654296875,
          "content": "[pytest]\nfilterwarnings =\n    ignore::DeprecationWarning:.*caffe2.*"
        },
        {
          "name": "pytext",
          "type": "tree",
          "content": null
        },
        {
          "name": "readthedocs.yml",
          "type": "blob",
          "size": 0.142578125,
          "content": "build:\n  image: latest\n\npython:\n  version: 3.7\n  setup_py_install: true\n  use_system_site_packages: true\nrequirements_file: docs_requirements.txt\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1650390625,
          "content": "click\nfairseq\nfuture\nhypothesis<4.0\niopath\njoblib\nnumpy\nonnx>=1.6.0\npandas\ntransformers==3.4.0\nregex==2019.11.1\nrequests\nscipy\nsentencepiece\ntensorboard\ntorch\ntorchtext\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.6416015625,
          "content": "#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\nimport os\n\nimport setuptools\n\n\nDIR = os.path.dirname(__file__)\nREQUIREMENTS = os.path.join(DIR, \"requirements.txt\")\n\n\nwith open(REQUIREMENTS) as f:\n    reqs = f.read()\n\nsetuptools.setup(\n    name=\"pytext-nlp\",\n    version=\"0.3.3\",\n    description=\"pytorch modeling framework and model zoo for text models\",\n    url=\"https://github.com/facebookresearch/PyText\",\n    author=\"Facebook\",\n    license=\"BSD\",\n    packages=setuptools.find_packages(),\n    install_requires=reqs.strip().split(\"\\n\"),\n    entry_points={\"console_scripts\": [\"pytext = pytext.main:main\"]},\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}