{
  "metadata": {
    "timestamp": 1736560800009,
    "page": 493,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "openai/guided-diffusion",
      "stars": 6449,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0498046875,
          "content": ".DS_Store\n__pycache__/\nclassify_image_graph_def.pb\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.037109375,
          "content": "MIT License\n\nCopyright (c) 2021 OpenAI\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.71484375,
          "content": "# guided-diffusion\n\nThis is the codebase for [Diffusion Models Beat GANS on Image Synthesis](http://arxiv.org/abs/2105.05233).\n\nThis repository is based on [openai/improved-diffusion](https://github.com/openai/improved-diffusion), with modifications for classifier conditioning and architecture improvements.\n\n# Download pre-trained models\n\nWe have released checkpoints for the main models in the paper. Before using these models, please review the corresponding [model card](model-card.md) to understand the intended use and limitations of these models.\n\nHere are the download links for each model checkpoint:\n\n * 64x64 classifier: [64x64_classifier.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/64x64_classifier.pt)\n * 64x64 diffusion: [64x64_diffusion.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/64x64_diffusion.pt)\n * 128x128 classifier: [128x128_classifier.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/128x128_classifier.pt)\n * 128x128 diffusion: [128x128_diffusion.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/128x128_diffusion.pt)\n * 256x256 classifier: [256x256_classifier.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_classifier.pt)\n * 256x256 diffusion: [256x256_diffusion.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion.pt)\n * 256x256 diffusion (not class conditional): [256x256_diffusion_uncond.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt)\n * 512x512 classifier: [512x512_classifier.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/512x512_classifier.pt)\n * 512x512 diffusion: [512x512_diffusion.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/512x512_diffusion.pt)\n * 64x64 -&gt; 256x256 upsampler: [64_256_upsampler.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/64_256_upsampler.pt)\n * 128x128 -&gt; 512x512 upsampler: [128_512_upsampler.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/128_512_upsampler.pt)\n * LSUN bedroom: [lsun_bedroom.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/lsun_bedroom.pt)\n * LSUN cat: [lsun_cat.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/lsun_cat.pt)\n * LSUN horse: [lsun_horse.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/lsun_horse.pt)\n * LSUN horse (no dropout): [lsun_horse_nodropout.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/lsun_horse_nodropout.pt)\n\n# Sampling from pre-trained models\n\nTo sample from these models, you can use the `classifier_sample.py`, `image_sample.py`, and `super_res_sample.py` scripts.\nHere, we provide flags for sampling from all of these models.\nWe assume that you have downloaded the relevant model checkpoints into a folder called `models/`.\n\nFor these examples, we will generate 100 samples with batch size 4. Feel free to change these values.\n\n```\nSAMPLE_FLAGS=\"--batch_size 4 --num_samples 100 --timestep_respacing 250\"\n```\n\n## Classifier guidance\n\nNote for these sampling runs that you can set `--classifier_scale 0` to sample from the base diffusion model.\nYou may also use the `image_sample.py` script instead of `classifier_sample.py` in that case.\n\n * 64x64 model:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --dropout 0.1 --image_size 64 --learn_sigma True --noise_schedule cosine --num_channels 192 --num_head_channels 64 --num_res_blocks 3 --resblock_updown True --use_new_attention_order True --use_fp16 True --use_scale_shift_norm True\"\npython classifier_sample.py $MODEL_FLAGS --classifier_scale 1.0 --classifier_path models/64x64_classifier.pt --classifier_depth 4 --model_path models/64x64_diffusion.pt $SAMPLE_FLAGS\n```\n\n * 128x128 model:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --image_size 128 --learn_sigma True --noise_schedule linear --num_channels 256 --num_heads 4 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython classifier_sample.py $MODEL_FLAGS --classifier_scale 0.5 --classifier_path models/128x128_classifier.pt --model_path models/128x128_diffusion.pt $SAMPLE_FLAGS\n```\n\n * 256x256 model:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython classifier_sample.py $MODEL_FLAGS --classifier_scale 1.0 --classifier_path models/256x256_classifier.pt --model_path models/256x256_diffusion.pt $SAMPLE_FLAGS\n```\n\n * 256x256 model (unconditional):\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython classifier_sample.py $MODEL_FLAGS --classifier_scale 10.0 --classifier_path models/256x256_classifier.pt --model_path models/256x256_diffusion_uncond.pt $SAMPLE_FLAGS\n```\n\n * 512x512 model:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --image_size 512 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 False --use_scale_shift_norm True\"\npython classifier_sample.py $MODEL_FLAGS --classifier_scale 4.0 --classifier_path models/512x512_classifier.pt --model_path models/512x512_diffusion.pt $SAMPLE_FLAGS\n```\n\n## Upsampling\n\nFor these runs, we assume you have some base samples in a file `64_samples.npz` or `128_samples.npz` for the two respective models.\n\n * 64 -&gt; 256:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --diffusion_steps 1000 --large_size 256  --small_size 64 --learn_sigma True --noise_schedule linear --num_channels 192 --num_heads 4 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython super_res_sample.py $MODEL_FLAGS --model_path models/64_256_upsampler.pt --base_samples 64_samples.npz $SAMPLE_FLAGS\n```\n\n * 128 -&gt; 512:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16 --class_cond True --diffusion_steps 1000 --large_size 512 --small_size 128 --learn_sigma True --noise_schedule linear --num_channels 192 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython super_res_sample.py $MODEL_FLAGS --model_path models/128_512_upsampler.pt $SAMPLE_FLAGS --base_samples 128_samples.npz\n```\n\n## LSUN models\n\nThese models are class-unconditional and correspond to a single LSUN class. Here, we show how to sample from `lsun_bedroom.pt`, but the other two LSUN checkpoints should work as well:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --dropout 0.1 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython image_sample.py $MODEL_FLAGS --model_path models/lsun_bedroom.pt $SAMPLE_FLAGS\n```\n\nYou can sample from `lsun_horse_nodropout.pt` by changing the dropout flag:\n\n```\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond False --diffusion_steps 1000 --dropout 0.0 --image_size 256 --learn_sigma True --noise_schedule linear --num_channels 256 --num_head_channels 64 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\npython image_sample.py $MODEL_FLAGS --model_path models/lsun_horse_nodropout.pt $SAMPLE_FLAGS\n```\n\nNote that for these models, the best samples result from using 1000 timesteps:\n\n```\nSAMPLE_FLAGS=\"--batch_size 4 --num_samples 100 --timestep_respacing 1000\"\n```\n\n# Results\n\nThis table summarizes our ImageNet results for pure guided diffusion models:\n\n| Dataset          | FID  | Precision | Recall |\n|------------------|------|-----------|--------|\n| ImageNet 64x64   | 2.07 | 0.74      | 0.63   |\n| ImageNet 128x128 | 2.97 | 0.78      | 0.59   |\n| ImageNet 256x256 | 4.59 | 0.82      | 0.52   |\n| ImageNet 512x512 | 7.72 | 0.87      | 0.42   |\n\nThis table shows the best results for high resolutions when using upsampling and guidance together:\n\n| Dataset          | FID  | Precision | Recall |\n|------------------|------|-----------|--------|\n| ImageNet 256x256 | 3.94 | 0.83      | 0.53   |\n| ImageNet 512x512 | 3.85 | 0.84      | 0.53   |\n\nFinally, here are the unguided results on individual LSUN classes:\n\n| Dataset      | FID  | Precision | Recall |\n|--------------|------|-----------|--------|\n| LSUN Bedroom | 1.90 | 0.66      | 0.51   |\n| LSUN Cat     | 5.57 | 0.63      | 0.52   |\n| LSUN Horse   | 2.57 | 0.71      | 0.55   |\n\n# Training models\n\nTraining diffusion models is described in the [parent repository](https://github.com/openai/improved-diffusion). Training a classifier is similar. We assume you have put training hyperparameters into a `TRAIN_FLAGS` variable, and classifier hyperparameters into a `CLASSIFIER_FLAGS` variable. Then you can run:\n\n```\nmpiexec -n N python scripts/classifier_train.py --data_dir path/to/imagenet $TRAIN_FLAGS $CLASSIFIER_FLAGS\n```\n\nMake sure to divide the batch size in `TRAIN_FLAGS` by the number of MPI processes you are using.\n\nHere are flags for training the 128x128 classifier. You can modify these for training classifiers at other resolutions:\n\n```sh\nTRAIN_FLAGS=\"--iterations 300000 --anneal_lr True --batch_size 256 --lr 3e-4 --save_interval 10000 --weight_decay 0.05\"\nCLASSIFIER_FLAGS=\"--image_size 128 --classifier_attention_resolutions 32,16,8 --classifier_depth 2 --classifier_width 128 --classifier_pool attention --classifier_resblock_updown True --classifier_use_scale_shift_norm True\"\n```\n\nFor sampling from a 128x128 classifier-guided model, 25 step DDIM:\n\n```sh\nMODEL_FLAGS=\"--attention_resolutions 32,16,8 --class_cond True --image_size 128 --learn_sigma True --num_channels 256 --num_heads 4 --num_res_blocks 2 --resblock_updown True --use_fp16 True --use_scale_shift_norm True\"\nCLASSIFIER_FLAGS=\"--image_size 128 --classifier_attention_resolutions 32,16,8 --classifier_depth 2 --classifier_width 128 --classifier_pool attention --classifier_resblock_updown True --classifier_use_scale_shift_norm True --classifier_scale 1.0 --classifier_use_fp16 True\"\nSAMPLE_FLAGS=\"--batch_size 4 --num_samples 50000 --timestep_respacing ddim25 --use_ddim True\"\nmpiexec -n N python scripts/classifier_sample.py \\\n    --model_path /path/to/model.pt \\\n    --classifier_path path/to/classifier.pt \\\n    $MODEL_FLAGS $CLASSIFIER_FLAGS $SAMPLE_FLAGS\n```\n\nTo sample for 250 timesteps without DDIM, replace `--timestep_respacing ddim25` to `--timestep_respacing 250`, and replace `--use_ddim True` with `--use_ddim False`.\n"
        },
        {
          "name": "datasets",
          "type": "tree",
          "content": null
        },
        {
          "name": "evaluations",
          "type": "tree",
          "content": null
        },
        {
          "name": "guided_diffusion",
          "type": "tree",
          "content": null
        },
        {
          "name": "model-card.md",
          "type": "blob",
          "size": 4.91796875,
          "content": "# Overview\n\nThese are diffusion models and noised image classifiers described in the paper [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233).\nIncluded in this release are the following models:\n\n * Noisy ImageNet classifiers at resolutions 64x64, 128x128, 256x256, 512x512\n * A class-unconditional ImageNet diffusion model at resolution 256x256\n * Class conditional ImageNet diffusion models at 64x64, 128x128, 256x256, 512x512 resolutions\n * Class-conditional ImageNet upsampling diffusion models: 64x64->256x256, 128x128->512x512\n * Diffusion models trained on three LSUN classes at 256x256 resolution: cat, horse, bedroom\n\n# Datasets\n\nAll of the models we are releasing were either trained on the [ILSVRC 2012 subset of ImageNet](http://www.image-net.org/challenges/LSVRC/2012/) or on single classes of [LSUN](https://arxiv.org/abs/1506.03365).\nHere, we describe characteristics of these datasets which impact model behavior:\n\n**LSUN**: This dataset was collected in 2015 using a combination of human labeling (from Amazon Mechanical Turk) and automated data labeling.\n * Each of the three classes we consider contain over a million images.\n * The dataset creators found that the label accuracy was roughly 90% across the entire LSUN dataset when measured by trained experts.\n * Images are scraped from the internet, and LSUN cat images in particular tend to often follow a “meme” format.\n * We found that there are occasionally humans in these photos, including faces, especially within the cat class.  \n\n**ILSVRC 2012 subset of ImageNet**: This dataset was curated in 2012 and consists of roughly one million images, each belonging to one of 1000 classes.\n * A large portion of the classes in this dataset are animals, plants, and other naturally-occurring objects.\n * Many images contain humans, although usually these humans aren’t reflected by the class label (e.g. the class “Tench, tinca tinca” contains many photos of people holding fish).\n\n# Performance\n\nThese models are intended to generate samples consistent with their training distributions.\nThis has been measured in terms of FID, Precision, and Recall.\nThese metrics all rely on the representations of a [pre-trained Inception-V3 model](https://arxiv.org/abs/1512.00567),\nwhich was trained on ImageNet, and so is likely to focus more on the ImageNet classes (such as animals) than on other visual features (such as human faces).\n\nQualitatively, the samples produced by these models often look highly realistic, especially when a diffusion model is combined with a noisy classifier.\n\n# Intended Use\n\nThese models are intended to be used for research purposes only.\nIn particular, they can be used as a baseline for generative modeling research, or as a starting point to build off of for such research.\n\nThese models are not intended to be commercially deployed.\nAdditionally, they are not intended to be used to create propaganda or offensive imagery.\n\nBefore releasing these models, we probed their ability to ease the creation of targeted imagery, since doing so could be potentially harmful.\nWe did this either by fine-tuning our ImageNet models on a target LSUN class, or through classifier guidance with publicly available [CLIP models](https://github.com/openai/CLIP).\n * To probe fine-tuning capabilities, we restricted our compute budget to roughly $100 and tried both standard fine-tuning,\nand a diffusion-specific approach where we train a specialized classifier for the LSUN class. The resulting FIDs were significantly worse than publicly available GAN models, indicating that fine-tuning an ImageNet diffusion model does not significantly lower the cost of image generation.\n * To probe guidance with CLIP, we tried two approaches for using pre-trained CLIP models for classifier guidance. Either we fed the noised image to CLIP directly and used its gradients, or we fed the diffusion model's denoised prediction to the CLIP model and differentiated through the whole process. In both cases, we found that it was difficult to recover information from the CLIP model, indicating that these diffusion models are unlikely to make it significantly easier to extract knowledge from CLIP compared to existing GAN models.\n\n# Limitations\n\nThese models sometimes produce highly unrealistic outputs, particularly when generating images containing human faces.\nThis may stem from ImageNet's emphasis on non-human objects.\n\nWhile classifier guidance can improve sample quality, it reduces diversity, resulting in some modes of the data distribution being underrepresented.\nThis can potentially amplify existing biases in the training dataset such as gender and racial biases.\n\nBecause ImageNet and LSUN contain images from the internet, they include photos of real people, and the model may have memorized some of the information contained in these photos.\nHowever, these images are already publicly available, and existing generative models trained on ImageNet have not demonstrated significant leakage of this information."
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.16015625,
          "content": "from setuptools import setup\n\nsetup(\n    name=\"guided-diffusion\",\n    py_modules=[\"guided_diffusion\"],\n    install_requires=[\"blobfile>=1.0.5\", \"torch\", \"tqdm\"],\n)\n"
        }
      ]
    }
  ]
}