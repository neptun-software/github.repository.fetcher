{
  "metadata": {
    "timestamp": 1736560635747,
    "page": 270,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "thuml/Time-Series-Library",
      "stars": 7673,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.515625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n/scripts/long_term_forecast/Traffic_script/PatchTST1.sh\n/backups/\n/result.xlsx\n/~$result.xlsx\n/Time-Series-Library.zip\n/temp.sh\n\n.idea\n/tv_result.xlsx\n/test.py\n/m4_results/\n/test_results/\n/PatchTST_results.xlsx\n/seq_len_long_term_forecast/\n/progress.xlsx\n/scripts/short_term_forecast/PatchTST_M4.sh\n/run_tv.py\n\n/scripts/long_term_forecast/ETT_tv_script/\n/dataset/\n/data/\ndata_factory_all.py\ndata_loader_all.py\n/scripts/short_term_forecast/tv_script/\n/exp/exp_short_term_forecasting_tv.py\n/exp/exp_long_term_forecasting_tv.py\n/timesnetv2.xlsx\n/scripts/anomaly_detection/tmp/\n/scripts/imputation/tmp/\n/utils/self_tools.py\n/scripts/exp_scripts/\n\ncheckpoints/\nresults/\nresult_long_term_forecast.txt\nresult_anomaly_detection.txt\nscripts/augmentation/\nrun_anylearn.py\nenvironment.txt"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.4609375,
          "content": "## Instructions for Contributing to TSlib\n\nSincerely thanks to all the researchers who want to use or contribute to TSlib.\n\nSince our team may not have enough time to fix all the bugs and catch up with the latest model, your contribution is essential to this project.\n\n### (1) Fix Bug\n\nYou can directly propose a pull request and add detailed descriptions to the comment, such as [this pull request](https://github.com/thuml/Time-Series-Library/pull/498).\n\n### (2) Add a new time series model\n\nThanks to creative researchers, extensive great TS models are presented, which advance this community significantly. If you want to add your model to TSlib, here are some instructions:\n\n- Propose an issue to describe your model and give a link to your paper and official code. We will discuss whether your model is suitable for this library, such as [this issue](https://github.com/thuml/Time-Series-Library/issues/346).\n- Propose a pull request in a similar style as TSlib, which means adding an additional file to ./models and providing corresponding scripts for reproduction, such as [this pull request](https://github.com/thuml/Time-Series-Library/pull/446).\n\nNote: Given that there are a lot of TS models that have been proposed, we may not have enough time to judge which model can be a remarkable supplement to the current library. Thus, we decide ONLY to add the officially published paper to our library. Peer review can be a reliable criterion.\n\nThanks again for your valuable contributions.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.05859375,
          "content": "MIT License\n\nCopyright (c) 2021 THUML @ Tsinghua University\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.076171875,
          "content": "# Time Series Library (TSLib)\nTSLib is an open-source library for deep learning researchers, especially for deep time series analysis.\n\nWe provide a neat code base to evaluate advanced deep time series models or develop your model, which covers five mainstream tasks: **long- and short-term forecasting, imputation, anomaly detection, and classification.**\n\n:triangular_flag_on_post:**News** (2024.10) We have included [[TimeXer]](https://arxiv.org/abs/2402.19072), which defined a practical forecasting paradigm: Forecasting with Exogenous Variables. Considering both practicability and computation efficiency, we believe the new forecasting paradigm defined in TimeXer can be the \"right\" task for future research.\n\n:triangular_flag_on_post:**News** (2024.10) Our lab has open-sourced [[OpenLTM]](https://github.com/thuml/OpenLTM), which provides a distinct pretrain-finetuning paradigm compared to TSLib. If you are interested in Large Time Series Models, you may find this repository helpful.\n\n:triangular_flag_on_post:**News** (2024.07) We wrote a comprehensive survey of [[Deep Time Series Models]](https://arxiv.org/abs/2407.13278) with a rigorous benchmark based on TSLib. In this paper, we summarized the design principles of current time series models supported by insightful experiments, hoping to be helpful to future research.\n\n:triangular_flag_on_post:**News** (2024.04) Many thanks for the great work from [frecklebars](https://github.com/thuml/Time-Series-Library/pull/378). The famous sequential model [Mamba](https://arxiv.org/abs/2312.00752) has been included in our library. See [this file](https://github.com/thuml/Time-Series-Library/blob/main/models/Mamba.py), where you need to install `mamba_ssm` with pip at first.\n\n:triangular_flag_on_post:**News** (2024.03) Given the inconsistent look-back length of various papers, we split the long-term forecasting in the leaderboard into two categories: Look-Back-96 and Look-Back-Searching. We recommend researchers read [TimeMixer](https://openreview.net/pdf?id=7oLshfEIC2), which includes both look-back length settings in experiments for scientific rigor.\n\n:triangular_flag_on_post:**News** (2023.10) We add an implementation to [iTransformer](https://arxiv.org/abs/2310.06625), which is the state-of-the-art model for long-term forecasting. The official code and complete scripts of iTransformer can be found [here](https://github.com/thuml/iTransformer).\n\n:triangular_flag_on_post:**News** (2023.09) We added a detailed [tutorial](https://github.com/thuml/Time-Series-Library/blob/main/tutorial/TimesNet_tutorial.ipynb) for [TimesNet](https://openreview.net/pdf?id=ju_Uqw384Oq) and this library, which is quite friendly to beginners of deep time series analysis.\n\n:triangular_flag_on_post:**News** (2023.02) We release the TSlib as a comprehensive benchmark and code base for time series models, which is extended from our previous GitHub repository [Autoformer](https://github.com/thuml/Autoformer).\n\n## Leaderboard for Time Series Analysis\n\nTill March 2024, the top three models for five different tasks are:\n\n| Model<br>Ranking | Long-term<br>Forecasting<br>Look-Back-96              | Long-term<br/>Forecasting<br/>Look-Back-Searching     | Short-term<br>Forecasting                                    | Imputation                                                   | Classification                                               | Anomaly<br>Detection                               |\n| ---------------- | ----------------------------------------------------- | ----------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------------------------------------- |\n| ðŸ¥‡ 1st            | [TimeXer](https://arxiv.org/abs/2402.19072)      | [TimeMixer](https://openreview.net/pdf?id=7oLshfEIC2) | [TimesNet](https://arxiv.org/abs/2210.02186)                 | [TimesNet](https://arxiv.org/abs/2210.02186)                 | [TimesNet](https://arxiv.org/abs/2210.02186)                 | [TimesNet](https://arxiv.org/abs/2210.02186)       |\n| ðŸ¥ˆ 2nd            | [iTransformer](https://arxiv.org/abs/2310.06625) | [PatchTST](https://github.com/yuqinie98/PatchTST)     | [Non-stationary<br/>Transformer](https://github.com/thuml/Nonstationary_Transformers) | [Non-stationary<br/>Transformer](https://github.com/thuml/Nonstationary_Transformers) | [Non-stationary<br/>Transformer](https://github.com/thuml/Nonstationary_Transformers) | [FEDformer](https://github.com/MAZiqing/FEDformer) |\n| ðŸ¥‰ 3rd            | [TimeMixer](https://openreview.net/pdf?id=7oLshfEIC2)          | [DLinear](https://arxiv.org/pdf/2205.13504.pdf)       | [FEDformer](https://github.com/MAZiqing/FEDformer)           | [Autoformer](https://github.com/thuml/Autoformer)            | [Informer](https://github.com/zhouhaoyi/Informer2020)        | [Autoformer](https://github.com/thuml/Autoformer)  |\n\n\n**Note: We will keep updating this leaderboard.** If you have proposed advanced and awesome models, you can send us your paper/code link or raise a pull request. We will add them to this repo and update the leaderboard as soon as possible.\n\n**Compared models of this leaderboard.** â˜‘ means that their codes have already been included in this repo.\n  - [x] **TimeXer** - TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables [[NeurIPS 2024]](https://arxiv.org/abs/2402.19072) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/TimeXer.py)\n  - [x] **TimeMixer** - TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting [[ICLR 2024]](https://openreview.net/pdf?id=7oLshfEIC2) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/TimeMixer.py).\n  - [x] **TSMixer** - TSMixer: An All-MLP Architecture for Time Series Forecasting [[arXiv 2023]](https://arxiv.org/pdf/2303.06053.pdf) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/TSMixer.py)\n  - [x] **iTransformer** - iTransformer: Inverted Transformers Are Effective for Time Series Forecasting [[ICLR 2024]](https://arxiv.org/abs/2310.06625) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/iTransformer.py).\n  - [x] **PatchTST** - A Time Series is Worth 64 Words: Long-term Forecasting with Transformers [[ICLR 2023]](https://openreview.net/pdf?id=Jbdc0vTOcol) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/PatchTST.py).\n  - [x] **TimesNet** - TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis [[ICLR 2023]](https://openreview.net/pdf?id=ju_Uqw384Oq) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/TimesNet.py).\n  - [x] **DLinear** - Are Transformers Effective for Time Series Forecasting? [[AAAI 2023]](https://arxiv.org/pdf/2205.13504.pdf) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/DLinear.py).\n  - [x] **LightTS** - Less Is More: Fast Multivariate Time Series Forecasting with Light Sampling-oriented MLP Structures [[arXiv 2022]](https://arxiv.org/abs/2207.01186) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/LightTS.py).\n  - [x] **ETSformer** - ETSformer: Exponential Smoothing Transformers for Time-series Forecasting [[arXiv 2022]](https://arxiv.org/abs/2202.01381) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/ETSformer.py).\n  - [x] **Non-stationary Transformer** - Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting [[NeurIPS 2022]](https://openreview.net/pdf?id=ucNDIDRNjjv) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Nonstationary_Transformer.py).\n  - [x] **FEDformer** - FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting [[ICML 2022]](https://proceedings.mlr.press/v162/zhou22g.html) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/FEDformer.py).\n  - [x] **Pyraformer** - Pyraformer: Low-complexity Pyramidal Attention for Long-range Time Series Modeling and Forecasting [[ICLR 2022]](https://openreview.net/pdf?id=0EXmFzUn5I) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Pyraformer.py).\n  - [x] **Autoformer** - Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting [[NeurIPS 2021]](https://openreview.net/pdf?id=I55UqU-M11y) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Autoformer.py).\n  - [x] **Informer** - Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting [[AAAI 2021]](https://ojs.aaai.org/index.php/AAAI/article/view/17325/17132) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Informer.py).\n  - [x] **Reformer** - Reformer: The Efficient Transformer [[ICLR 2020]](https://openreview.net/forum?id=rkgNKkHtvB) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Reformer.py).\n  - [x] **Transformer** - Attention is All You Need [[NeurIPS 2017]](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Transformer.py).\n\nSee our latest paper [[TimesNet]](https://arxiv.org/abs/2210.02186) for the comprehensive benchmark. We will release a real-time updated online version soon.\n\n**Newly added baselines.** We will add them to the leaderboard after a comprehensive evaluation.\n  - [x] **WPMixer** - WPMixer: Efficient Multi-Resolution Mixing for Long-Term Time Series Forecasting [[AAAI 2025]](https://arxiv.org/abs/2412.17176) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/WPMixer.py)\n  - [x] **PAttn** - Are Language Models Actually Useful for Time Series Forecasting? [[NeurIPS 2024]](https://arxiv.org/pdf/2406.16964) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/PAttn.py)\n  - [x] **Mamba** - Mamba: Linear-Time Sequence Modeling with Selective State Spaces [[arXiv 2023]](https://arxiv.org/abs/2312.00752) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Mamba.py)\n  - [x] **SegRNN** - SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting [[arXiv 2023]](https://arxiv.org/abs/2308.11200.pdf) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/SegRNN.py).\n  - [x] **Koopa** - Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors [[NeurIPS 2023]](https://arxiv.org/pdf/2305.18803.pdf) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Koopa.py).\n  - [x] **FreTS** - Frequency-domain MLPs are More Effective Learners in Time Series Forecasting [[NeurIPS 2023]](https://arxiv.org/pdf/2311.06184.pdf) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/FreTS.py).\n  - [x] **MICN** - MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting [[ICLR 2023]](https://openreview.net/pdf?id=zt53IDUR1U)[[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/MICN.py).\n  - [x] **Crossformer** - Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting [[ICLR 2023]](https://openreview.net/pdf?id=vSVLM2j9eie)[[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Crossformer.py).\n  - [x] **TiDE** - Long-term Forecasting with TiDE: Time-series Dense Encoder [[arXiv 2023]](https://arxiv.org/pdf/2304.08424.pdf) [[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/TiDE.py).\n  - [x] **SCINet** - SCINet: Time Series Modeling and Forecasting with Sample Convolution and Interaction [[NeurIPS 2022]](https://openreview.net/pdf?id=AyajSjTAzmg)[[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/SCINet.py).\n  - [x] **FiLM** - FiLM: Frequency improved Legendre Memory Model for Long-term Time Series Forecasting [[NeurIPS 2022]](https://openreview.net/forum?id=zTQdHSQUQWc)[[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/FiLM.py).\n  - [x] **TFT** - Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting [[arXiv 2019]](https://arxiv.org/abs/1912.09363)[[Code]](https://github.com/thuml/Time-Series-Library/blob/main/models/TemporalFusionTransformer.py). \n \n## Usage\n\n1. Install Python 3.8. For convenience, execute the following command.\n\n```\npip install -r requirements.txt\n```\n\n2. Prepare Data. You can obtain the well pre-processed datasets from [[Google Drive]](https://drive.google.com/drive/folders/13Cg1KYOlzM5C7K8gK8NfC-F3EYxkM3D2?usp=sharing) orÂ [[Baidu Drive]](https://pan.baidu.com/s/1r3KhGd0Q9PJIUZdfEYoymg?pwd=i9iy), Then place the downloaded data in the folder`./dataset`. Here is a summary of supported datasets.\n\n<p align=\"center\">\n<img src=\".\\pic\\dataset.png\" height = \"200\" alt=\"\" align=center />\n</p>\n\n3. Train and evaluate model. We provide the experiment scripts for all benchmarks under the folder `./scripts/`. You can reproduce the experiment results as the following examples:\n\n```\n# long-term forecast\nbash ./scripts/long_term_forecast/ETT_script/TimesNet_ETTh1.sh\n# short-term forecast\nbash ./scripts/short_term_forecast/TimesNet_M4.sh\n# imputation\nbash ./scripts/imputation/ETT_script/TimesNet_ETTh1.sh\n# anomaly detection\nbash ./scripts/anomaly_detection/PSM/TimesNet.sh\n# classification\nbash ./scripts/classification/TimesNet.sh\n```\n\n4. Develop your own model.\n\n- Add the model file to the folder `./models`. You can follow the `./models/Transformer.py`.\n- Include the newly added model in the `Exp_Basic.model_dict` of  `./exp/exp_basic.py`.\n- Create the corresponding scripts under the folder `./scripts`.\n\nNote: \n\n(1) About classification: Since we include all five tasks in a unified code base, the accuracy of each subtask may fluctuate but the average performance can be reproduced (even a bit better). We have provided the reproduced checkpoints [here](https://github.com/thuml/Time-Series-Library/issues/494).\n\n(2) About anomaly detection: Some discussion about the adjustment strategy in anomaly detection can be found [here](https://github.com/thuml/Anomaly-Transformer/issues/14). The key point is that the adjustment strategy corresponds to an event-level metric.\n\n## Citation\n\nIf you find this repo useful, please cite our paper.\n\n```\n@inproceedings{wu2023timesnet,\n  title={TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},\n  author={Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long},\n  booktitle={International Conference on Learning Representations},\n  year={2023},\n}\n\n@article{wang2024tssurvey,\n  title={Deep Time Series Models: A Comprehensive Survey and Benchmark},\n  author={Yuxuan Wang and Haixu Wu and Jiaxiang Dong and Yong Liu and Mingsheng Long and Jianmin Wang},\n  booktitle={arXiv preprint arXiv:2407.13278},\n  year={2024},\n}\n```\n\n## Contact\nIf you have any questions or suggestions, feel free to contact our maintenance team:\n\nCurrent:\n- Haixu Wu (Ph.D. student, wuhx23@mails.tsinghua.edu.cn)\n- Yong Liu (Ph.D. student, liuyong21@mails.tsinghua.edu.cn)\n- Yuxuan Wang (Ph.D. student, wangyuxu22@mails.tsinghua.edu.cn)\n- Huikun Weng (Undergraduate, wenghk22@mails.tsinghua.edu.cn)\n\nPrevious:\n- Tengge Hu (Master student, htg21@mails.tsinghua.edu.cn)\n- Haoran Zhang (Master student, z-hr20@mails.tsinghua.edu.cn)\n- Jiawei Guo (Undergraduate, guo-jw21@mails.tsinghua.edu.cn)\n\nOr describe it in Issues.\n\n## Acknowledgement\n\nThis project is supported by the National Key R&D Program of China (2021YFB1715200).\n\nThis library is constructed based on the following repos:\n\n- Forecasting: https://github.com/thuml/Autoformer.\n\n- Anomaly Detection: https://github.com/thuml/Anomaly-Transformer.\n\n- Classification: https://github.com/thuml/Flowformer.\n\nAll the experiment datasets are public, and we obtain them from the following links:\n\n- Long-term Forecasting and Imputation: https://github.com/thuml/Autoformer.\n\n- Short-term Forecasting: https://github.com/ServiceNow/N-BEATS.\n\n- Anomaly Detection: https://github.com/thuml/Anomaly-Transformer.\n\n- Classification: https://www.timeseriesclassification.com/.\n\n## All Thanks To Our Contributors\n\n<a href=\"https://github.com/thuml/Time-Series-Library/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=thuml/Time-Series-Library\" />\n</a>\n"
        },
        {
          "name": "data_provider",
          "type": "tree",
          "content": null
        },
        {
          "name": "exp",
          "type": "tree",
          "content": null
        },
        {
          "name": "layers",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "pic",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.21484375,
          "content": "einops==0.8.0\nlocal-attention==1.9.14\nmatplotlib==3.7.0\nnumpy==1.23.5\npandas==1.5.3\npatool==1.12\nreformer-pytorch==1.4.4\nscikit-learn==1.2.2\nscipy==1.10.1\nsktime==0.16.1\nsympy==1.11.1\ntorch==1.7.1\ntqdm==4.64.1\nPyWavelets"
        },
        {
          "name": "run.py",
          "type": "blob",
          "size": 12.9287109375,
          "content": "import argparse\nimport os\nimport torch\nimport torch.backends\nfrom exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\nfrom exp.exp_imputation import Exp_Imputation\nfrom exp.exp_short_term_forecasting import Exp_Short_Term_Forecast\nfrom exp.exp_anomaly_detection import Exp_Anomaly_Detection\nfrom exp.exp_classification import Exp_Classification\nfrom utils.print_args import print_args\nimport random\nimport numpy as np\n\nif __name__ == '__main__':\n    fix_seed = 2021\n    random.seed(fix_seed)\n    torch.manual_seed(fix_seed)\n    np.random.seed(fix_seed)\n\n    parser = argparse.ArgumentParser(description='TimesNet')\n\n    # basic config\n    parser.add_argument('--task_name', type=str, required=True, default='long_term_forecast',\n                        help='task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]')\n    parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n    parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n    parser.add_argument('--model', type=str, required=True, default='Autoformer',\n                        help='model name, options: [Autoformer, Transformer, TimesNet]')\n\n    # data loader\n    parser.add_argument('--data', type=str, required=True, default='ETTh1', help='dataset type')\n    parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n    parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n    parser.add_argument('--features', type=str, default='M',\n                        help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n    parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n    parser.add_argument('--freq', type=str, default='h',\n                        help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n    parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n\n    # forecasting task\n    parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n    parser.add_argument('--label_len', type=int, default=48, help='start token length')\n    parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n    parser.add_argument('--seasonal_patterns', type=str, default='Monthly', help='subset for M4')\n    parser.add_argument('--inverse', action='store_true', help='inverse output data', default=False)\n\n    # inputation task\n    parser.add_argument('--mask_rate', type=float, default=0.25, help='mask ratio')\n\n    # anomaly detection task\n    parser.add_argument('--anomaly_ratio', type=float, default=0.25, help='prior anomaly ratio (%%)')\n\n    # model define\n    parser.add_argument('--expand', type=int, default=2, help='expansion factor for Mamba')\n    parser.add_argument('--d_conv', type=int, default=4, help='conv kernel size for Mamba')\n    parser.add_argument('--top_k', type=int, default=5, help='for TimesBlock')\n    parser.add_argument('--num_kernels', type=int, default=6, help='for Inception')\n    parser.add_argument('--enc_in', type=int, default=7, help='encoder input size')\n    parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n    parser.add_argument('--c_out', type=int, default=7, help='output size')\n    parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n    parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n    parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n    parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n    parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n    parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n    parser.add_argument('--factor', type=int, default=1, help='attn factor')\n    parser.add_argument('--distil', action='store_false',\n                        help='whether to use distilling in encoder, using this argument means not using distilling',\n                        default=True)\n    parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n    parser.add_argument('--embed', type=str, default='timeF',\n                        help='time features encoding, options:[timeF, fixed, learned]')\n    parser.add_argument('--activation', type=str, default='gelu', help='activation')\n    parser.add_argument('--channel_independence', type=int, default=1,\n                        help='0: channel dependence 1: channel independence for FreTS model')\n    parser.add_argument('--decomp_method', type=str, default='moving_avg',\n                        help='method of series decompsition, only support moving_avg or dft_decomp')\n    parser.add_argument('--use_norm', type=int, default=1, help='whether to use normalize; True 1 False 0')\n    parser.add_argument('--down_sampling_layers', type=int, default=0, help='num of down sampling layers')\n    parser.add_argument('--down_sampling_window', type=int, default=1, help='down sampling window size')\n    parser.add_argument('--down_sampling_method', type=str, default=None,\n                        help='down sampling method, only support avg, max, conv')\n    parser.add_argument('--seg_len', type=int, default=96,\n                        help='the length of segmen-wise iteration of SegRNN')\n\n    # optimization\n    parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n    parser.add_argument('--itr', type=int, default=1, help='experiments times')\n    parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n    parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n    parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n    parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n    parser.add_argument('--des', type=str, default='test', help='exp description')\n    parser.add_argument('--loss', type=str, default='MSE', help='loss function')\n    parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n    parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n\n    # GPU\n    parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n    parser.add_argument('--gpu', type=int, default=0, help='gpu')\n    parser.add_argument('--gpu_type', type=str, default='cuda', help='gpu type')  # cuda or mps\n    parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n    parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n\n    # de-stationary projector params\n    parser.add_argument('--p_hidden_dims', type=int, nargs='+', default=[128, 128],\n                        help='hidden layer dimensions of projector (List)')\n    parser.add_argument('--p_hidden_layers', type=int, default=2, help='number of hidden layers in projector')\n\n    # metrics (dtw)\n    parser.add_argument('--use_dtw', type=bool, default=False,\n                        help='the controller of using dtw metric (dtw is time consuming, not suggested unless necessary)')\n\n    # Augmentation\n    parser.add_argument('--augmentation_ratio', type=int, default=0, help=\"How many times to augment\")\n    parser.add_argument('--seed', type=int, default=2, help=\"Randomization seed\")\n    parser.add_argument('--jitter', default=False, action=\"store_true\", help=\"Jitter preset augmentation\")\n    parser.add_argument('--scaling', default=False, action=\"store_true\", help=\"Scaling preset augmentation\")\n    parser.add_argument('--permutation', default=False, action=\"store_true\",\n                        help=\"Equal Length Permutation preset augmentation\")\n    parser.add_argument('--randompermutation', default=False, action=\"store_true\",\n                        help=\"Random Length Permutation preset augmentation\")\n    parser.add_argument('--magwarp', default=False, action=\"store_true\", help=\"Magnitude warp preset augmentation\")\n    parser.add_argument('--timewarp', default=False, action=\"store_true\", help=\"Time warp preset augmentation\")\n    parser.add_argument('--windowslice', default=False, action=\"store_true\", help=\"Window slice preset augmentation\")\n    parser.add_argument('--windowwarp', default=False, action=\"store_true\", help=\"Window warp preset augmentation\")\n    parser.add_argument('--rotation', default=False, action=\"store_true\", help=\"Rotation preset augmentation\")\n    parser.add_argument('--spawner', default=False, action=\"store_true\", help=\"SPAWNER preset augmentation\")\n    parser.add_argument('--dtwwarp', default=False, action=\"store_true\", help=\"DTW warp preset augmentation\")\n    parser.add_argument('--shapedtwwarp', default=False, action=\"store_true\", help=\"Shape DTW warp preset augmentation\")\n    parser.add_argument('--wdba', default=False, action=\"store_true\", help=\"Weighted DBA preset augmentation\")\n    parser.add_argument('--discdtw', default=False, action=\"store_true\",\n                        help=\"Discrimitive DTW warp preset augmentation\")\n    parser.add_argument('--discsdtw', default=False, action=\"store_true\",\n                        help=\"Discrimitive shapeDTW warp preset augmentation\")\n    parser.add_argument('--extra_tag', type=str, default=\"\", help=\"Anything extra\")\n\n    # TimeXer\n    parser.add_argument('--patch_len', type=int, default=16, help='patch length')\n\n    args = parser.parse_args()\n    if torch.cuda.is_available() and args.use_gpu:\n        args.device = torch.device('cuda:{}'.format(args.gpu))\n        print('Using GPU')\n    else:\n        if hasattr(torch.backends, \"mps\"):\n            args.device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n        else:\n            args.device = torch.device(\"cpu\")\n        print('Using cpu or mps')\n\n    if args.use_gpu and args.use_multi_gpu:\n        args.devices = args.devices.replace(' ', '')\n        device_ids = args.devices.split(',')\n        args.device_ids = [int(id_) for id_ in device_ids]\n        args.gpu = args.device_ids[0]\n\n    print('Args in experiment:')\n    print_args(args)\n\n    if args.task_name == 'long_term_forecast':\n        Exp = Exp_Long_Term_Forecast\n    elif args.task_name == 'short_term_forecast':\n        Exp = Exp_Short_Term_Forecast\n    elif args.task_name == 'imputation':\n        Exp = Exp_Imputation\n    elif args.task_name == 'anomaly_detection':\n        Exp = Exp_Anomaly_Detection\n    elif args.task_name == 'classification':\n        Exp = Exp_Classification\n    else:\n        Exp = Exp_Long_Term_Forecast\n\n    if args.is_training:\n        for ii in range(args.itr):\n            # setting record of experiments\n            exp = Exp(args)  # set experiments\n            setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_expand{}_dc{}_fc{}_eb{}_dt{}_{}_{}'.format(\n                args.task_name,\n                args.model_id,\n                args.model,\n                args.data,\n                args.features,\n                args.seq_len,\n                args.label_len,\n                args.pred_len,\n                args.d_model,\n                args.n_heads,\n                args.e_layers,\n                args.d_layers,\n                args.d_ff,\n                args.expand,\n                args.d_conv,\n                args.factor,\n                args.embed,\n                args.distil,\n                args.des, ii)\n\n            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n            exp.train(setting)\n\n            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n            exp.test(setting)\n            if args.gpu_type == 'mps':\n                torch.backends.mps.empty_cache()\n            elif args.gpu_type == 'cuda':\n                torch.cuda.empty_cache()\n    else:\n        exp = Exp(args)  # set experiments\n        ii = 0\n        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_expand{}_dc{}_fc{}_eb{}_dt{}_{}_{}'.format(\n            args.task_name,\n            args.model_id,\n            args.model,\n            args.data,\n            args.features,\n            args.seq_len,\n            args.label_len,\n            args.pred_len,\n            args.d_model,\n            args.n_heads,\n            args.e_layers,\n            args.d_layers,\n            args.d_ff,\n            args.expand,\n            args.d_conv,\n            args.factor,\n            args.embed,\n            args.distil,\n            args.des, ii)\n\n        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n        exp.test(setting, test=1)\n        if args.gpu_type == 'mps':\n            torch.backends.mps.empty_cache()\n        elif args.gpu_type == 'cuda':\n            torch.cuda.empty_cache()\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "tutorial",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}