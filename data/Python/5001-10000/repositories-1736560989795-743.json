{
  "metadata": {
    "timestamp": 1736560989795,
    "page": 743,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "bojone/bert4keras",
      "stars": 5383,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.58203125,
          "content": "# bert4keras\n- Our light reimplement of bert for keras\n- 更清晰、更轻量级的keras版bert\n- 个人博客：https://kexue.fm/\n- 在线文档：http://bert4keras.spaces.ac.cn/ （还在构建中）\n\n## 说明\n这是笔者重新实现的keras版的transformer模型库，致力于用尽可能清爽的代码来实现结合transformer和keras。\n\n本项目的初衷是为了修改、定制上的方便，所以可能会频繁更新。\n\n因此欢迎star，但不建议fork，因为你fork下来的版本可能很快就过期了。\n\n## 功能\n目前已经实现：\n- 加载bert/roberta/albert的预训练权重进行finetune；\n- 实现语言模型、seq2seq所需要的attention mask；\n- 丰富的<a href=\"https://github.com/bojone/bert4keras/tree/master/examples\">examples</a>；\n- 从零预训练代码（支持TPU、多GPU，请看<a href=\"https://github.com/bojone/bert4keras/tree/master/pretraining\">pretraining</a>）；\n- 兼容keras、tf.keras\n\n## 使用\n安装稳定版：\n```shell\npip install bert4keras\n```\n安装最新版：\n```shell\npip install git+https://www.github.com/bojone/bert4keras.git\n```\n\n使用例子请参考<a href=\"https://github.com/bojone/bert4keras/blob/master/examples\">examples</a>目录。\n\n之前基于keras-bert给出的<a href=\"https://github.com/bojone/bert_in_keras\">例子</a>，仍适用于本项目，只需要将`bert_model`的加载方式换成本项目的。\n\n理论上兼容Python2和Python3，兼容tensorflow 1.14+和tensorflow 2.x，实验环境是Python 2.7、Tesorflow 1.14+以及Keras 2.3.1（已经在2.2.4、2.3.0、2.3.1、tf.keras下测试通过）。\n\n**为了获得最好的体验，建议你使用Tensorflow 1.14 + Keras 2.3.1组合。**\n\n<blockquote><strong>关于环境组合</strong>\n  \n- 支持tf+keras和tf+tf.keras，后者需要提前传入环境变量TF_KERAS=1。\n\n- 当使用tf+keras时，建议2.2.4 <= keras <= 2.3.1，以及 1.14 <= tf <= 2.2，不能使用tf 2.3+。\n\n- keras 2.4+可以用，但事实上keras 2.4.x基本上已经完全等价于tf.keras了，因此如果你要用keras 2.4+，倒不如直接用tf.keras。\n</blockquote>\n\n当然，乐于贡献的朋友如果发现了某些bug的话，也欢迎指出修正甚至Pull Requests～\n\n## 权重\n\n目前支持加载的权重：\n- <strong>Google原版bert</strong>: https://github.com/google-research/bert\n- <strong>brightmart版roberta</strong>: https://github.com/brightmart/roberta_zh\n- <strong>哈工大版roberta</strong>: https://github.com/ymcui/Chinese-BERT-wwm\n- <strong>Google原版albert</strong><sup><a href=\"https://github.com/bojone/bert4keras/issues/29#issuecomment-552188981\">[例子]</a></sup>: https://github.com/google-research/ALBERT\n- <strong>brightmart版albert</strong>: https://github.com/brightmart/albert_zh\n- <strong>转换后的albert</strong>: https://github.com/bojone/albert_zh\n- <strong>华为的NEZHA</strong>: https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow\n- <strong>华为的NEZHA-GEN</strong>: https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-Gen-TensorFlow\n- <strong>自研语言模型</strong>: https://github.com/ZhuiyiTechnology/pretrained-models\n- <strong>T5模型</strong>: https://github.com/google-research/text-to-text-transfer-transformer\n- <strong>GPT_OpenAI</strong>: https://github.com/bojone/CDial-GPT-tf\n- <strong>GPT2_ML</strong>: https://github.com/imcaspar/gpt2-ml\n- <strong>Google原版ELECTRA</strong>: https://github.com/google-research/electra\n- <strong>哈工大版ELECTRA</strong>: https://github.com/ymcui/Chinese-ELECTRA\n- <strong>CLUE版ELECTRA</strong>: https://github.com/CLUEbenchmark/ELECTRA\n- <strong>LaBSE（多国语言BERT）</strong>: https://github.com/bojone/labse\n- <strong>Chinese-GEN项目下的模型</strong>: https://github.com/bojone/chinese-gen\n- <strong>T5.1.1</strong>: https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md#t511\n- <strong>Multilingual T5</strong>: https://github.com/google-research/multilingual-t5/\n\n<strong>注意事项</strong>\n- 注1：brightmart版albert的开源时间早于Google版albert，这导致早期brightmart版albert的权重与Google版的不完全一致，换言之两者不能直接相互替换。为了减少代码冗余，bert4keras的0.2.4及后续版本均只支持加载<u>Google版</u>以brightmart版中<u>带Google字眼</u>的权重。如果要加载早期版本的权重，请用<a href=\"https://github.com/bojone/bert4keras/releases/tag/v0.2.3\">0.2.3版本</a>，或者考虑作者转换过的<a href=\"https://github.com/bojone/albert_zh\">albert_zh</a>。\n- 注2：下载下来的ELECTRA权重，如果没有json配置文件的话，参考<a href=\"https://github.com/ymcui/Chinese-ELECTRA/issues/3\">这里</a>自己改一个（需要加上`type_vocab_size`字段）。\n\n## 更新\n- <strong>2023.03.06</strong>: [无穷大改np.inf；优化显存占用](https://github.com/bojone/bert4keras/commit/20a46946156b4bc15ceaa00671fcd00c8b702640)。将无穷大改为np.inf，运算更加准确，而且在低精度运算时不容易出错；同时合并了若干mask算子，减少了显存占用。实测在A100上训练base和large级别模型时，速度有明显加快，显存占用也有降低。\n- <strong>2022.03.20</strong>: 增加[RoFormerV2](https://kexue.fm/archives/8998)。\n- <strong>2022.02.28</strong>: 增加[GatedAttentionUnit](https://kexue.fm/archives/8934)。\n- <strong>2021.04.23</strong>: 增加[GlobalPointer](https://kexue.fm/archives/8373)。\n- <strong>2021.03.23</strong>: 增加[RoFormer](https://kexue.fm/archives/8265)。\n- <strong>2021.01.30</strong>: 发布0.9.9版，完善多GPU支持，增加多GPU例子：[task_seq2seq_autotitle_multigpu.py](https://github.com/bojone/bert4keras/blob/master/examples/task_seq2seq_autotitle_multigpu.py)。\n- <strong>2020.12.29</strong>: 增加`residual_attention_scores`参数来实现RealFormer，只需要在`build_transformer_model`中传入参数`residual_attention_scores=True`启用。\n- <strong>2020.12.04</strong>: `PositionEmbedding`引入层次分解，可以让BERT直接处理超长文本，在`build_transformer_model`中传入参数`hierarchical_position=True`启用。\n- <strong>2020.11.19</strong>: 支持GPT2模型，参考[CPM_LM_bert4keras](https://github.com/bojone/CPM_LM_bert4keras)项目。\n- <strong>2020.11.14</strong>: 新增分参数学习率`extend_with_parameter_wise_lr`，可用于给每层设置不同的学习率。\n- <strong>2020.10.27</strong>: 支持<a href=\"https://github.com/google-research/text-to-text-transfer-transformer/blob/master/released_checkpoints.md#t511\">T5.1.1</a>和<a href=\"https://github.com/google-research/multilingual-t5/\">Multilingual T5</a>。\n- <strong>2020.08.28</strong>: 支持<a href=\"https://github.com/bojone/CDial-GPT-tf\">GPT_OpenAI</a>。\n- <strong>2020.08.22</strong>: 新增`WebServing`类，允许简单地将模型转换为Web接口，详情请参考该类的<a href=\"https://github.com/bojone/bert4keras/blob/8d55512a12e4677262363ac189ebf504fc451716/bert4keras/snippets.py#L580\">说明</a>。\n- <strong>2020.07.14</strong>: `Transformer`类加入`prefix`参数；`snippets.py`引入`to_array`函数；`AutoRegressiveDecoder`修改`rtype='logits'`时的一个隐藏bug。\n- <strong>2020.06.06</strong>: 强迫症作祟：将`Tokenizer`原来的`max_length`参数重命名为`maxlen`，同时保留向后兼容性，建议大家用新参数名。\n- <strong>2020.04.29</strong>: 增加重计算（参考<a href=\"https://github.com/bojone/keras_recompute\">keras_recompute</a>），可以通过时间换空间，通过设置环境变量`RECOMPUTE=1`启用。\n- <strong>2020.04.25</strong>: 优化tf2下的表现。\n- <strong>2020.04.16</strong>: 所有example均适配tensorflow 2.0。\n- <strong>2020.04.06</strong>: 增加UniLM预训练模式（测试中）。\n- <strong>2020.04.06</strong>: 完善`rematch`方法。\n- <strong>2020.04.01</strong>: `Tokenizer`增加`rematch`方法，给出分词结果与原序列的映射关系。\n- <strong>2020.03.30</strong>: 尽量统一py文件的写法。\n- <strong>2020.03.25</strong>: 支持ELECTRA。\n- <strong>2020.03.24</strong>: 继续加强`DataGenerator`，允许传入迭代器时进行局部shuffle。\n- <strong>2020.03.23</strong>: 增加调整Attention的`key_size`的选项。\n- <strong>2020.03.17</strong>: 增强`DataGenerator`；优化模型写法。\n- <strong>2020.03.15</strong>: 支持<a href=\"https://github.com/imcaspar/gpt2-ml\">GPT2_ML</a>。\n- <strong>2020.03.10</strong>: 支持Google的<a href=\"https://github.com/google-research/text-to-text-transfer-transformer\">T5</a>模型。\n- <strong>2020.03.05</strong>: 将`tokenizer.py`更名为`tokenizers.py`。\n- <strong>2020.03.05</strong>: `application='seq2seq'`改名为`application='unilm'`。\n- <strong>2020.03.05</strong>: `build_bert_model`更名为`build_transformer_model`。\n- <strong>2020.03.05</strong>: 重写`models.py`结构。\n- <strong>2020.03.04</strong>: 将`bert.py`更名为`models.py`。\n- <strong>2020.03.02</strong>: 重构mask机制（用回Keras自带的mask机制），以便更好地编写更复杂的应用。\n- <strong>2020.02.22</strong>: 新增`AutoRegressiveDecoder`类，统一处理Seq2Seq的解码问题。\n- <strong>2020.02.19</strong>: transformer block的前缀改为Transformer（本来是Encoder），使得其含义局限性更少。\n- <strong>2020.02.13</strong>: 优化`load_vocab`函数；将`build_bert_model`中的`keep_words`参数更名为`keep_tokens`，此处改动可能会对部分脚本产生影响。\n- <strong>2020.01.18</strong>: 调整文本处理方式，去掉codecs的使用。\n- <strong>2020.01.17</strong>: 各api日趋稳定，为了方便大家使用，打包到<a href=\"https://pypi.org/project/bert4keras/\">pypi</a>，首个打包版本号为0.4.6。\n- <strong>2020.01.10</strong>: 重写模型mask方案，某种程度上让代码更为简练清晰；后端优化。\n- <strong>2019.12.27</strong>: 重构预训练代码，减少冗余；目前支持RoBERTa和GPT两种预训练方式，详见<a href=\"https://github.com/bojone/bert4keras/tree/master/pretraining/\">pretraining</a>。\n- <strong>2019.12.17</strong>: 适配华为的<a href=\"https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA\">nezha</a>权重，只需要在`build_bert_model`函数里加上`model='nezha'`；此外原来albert的加载方式`albert=True`改为`model='albert'`。\n- <strong>2019.12.16</strong>: 通过跟keras 2.3+版本类似的思路给低版本引入层中层功能，从而恢复对低于2.3.0版本的keras的支持。\n- <strong>2019.12.14</strong>: 新增Conditional Layer Normalization及相关demo。\n- <strong>2019.12.09</strong>: 各example的data_generator规范化；修复application='lm'时的一个错误。\n- <strong>2019.12.05</strong>: 优化tokenizer的do_lower_case，同时微调各个example。\n- <strong>2019.11.23</strong>: 将train.py重命名为optimizers.py，更新大量优化器实现，全面兼容keras和tf.keras。\n- <strong>2019.11.19</strong>: 将utils.py重命名为tokenizer.py。\n- <strong>2019.11.19</strong>: 想来想去，最后还是决定把snippets放到<a href=\"https://github.com/bojone/bert4keras/blob/master/bert4keras/snippets.py\">bert4keras.snippets</a>下面去好了。\n- <strong>2019.11.18</strong>: 优化预训练权重加载逻辑，增加保存模型权重至Bert的checkpoint格式方法。\n- <strong>2019.11.17</strong>: <del>分离一些与Bert本身不直接相关的常用代码片段到<a href=\"https://github.com/bojone/python-snippets\">python_snippets</a>，供其它项目共用。</del>\n- <strong>2019.11.11</strong>: 添加NSP部分。\n- <strong>2019.11.05</strong>: 适配<a href=\"https://github.com/google-research/google-research/tree/master/albert\">google版albert</a>，不再支持<a href=\"https://github.com/brightmart/albert_zh\">非Google版albert_zh</a>。\n- <strong>2019.11.05</strong>: 以RoBERTa为例子的预训练代码开发完毕，同时支持TPU/多GPU训练，详见<a href=\"https://github.com/bojone/bert4keras/tree/master/pretraining/roberta/\">roberta</a>。欢迎在此基础上构建更多的预训练代码。\n- <strong>2019.11.01</strong>: 逐步增加预训练相关代码，详见<a href=\"https://github.com/bojone/bert4keras/tree/master/pretraining\">pretraining</a>。\n- <strong>2019.10.28</strong>: 支持使用基于<a href=\"https://github.com/google/sentencepiece\">sentencepiece</a>的tokenizer。\n- <strong>2019.10.25</strong>: 引入原生tokenizer。\n- <strong>2019.10.22</strong>: 引入梯度累积优化器。\n- <strong>2019.10.21</strong>: 为了简化代码结构，决定放弃keras 2.3.0之前的版本的支持，目前只支持keras 2.3.0+以及tf.keras。\n- <strong>2019.10.20</strong>: 应网友要求，现支持直接用`model.save`保存模型结构，用`load_model`加载整个模型（只需要在`load_model`之前执行`from bert4keras.layers import *`，不需要额外写`custom_objects`）。\n- <strong>2019.10.09</strong>: 已兼容tf.keras，同时在tf 1.13和tf 2.0下的tf.keras测试通过，通过设置环境变量`TF_KERAS=1`来切换tf.keras。\n- <strong>2019.10.09</strong>: 已兼容Keras 2.3.x，但只是临时方案，后续可能直接移除掉2.3之前版本的支持。\n- <strong>2019.10.02</strong>: 适配albert，能成功加载<a href=\"https://github.com/brightmart/albert_zh\">albert_zh</a>的权重，只需要在`load_pretrained_model`函数里加上`albert=True`。\n\n## 背景\n之前一直用CyberZHG大佬的<a href=\"https://github.com/CyberZHG/keras-bert\">keras-bert</a>，如果纯粹只是为了在keras下对bert进行调用和fine tune来说，keras-bert已经足够能让人满意了。\n\n然而，如果想要在加载官方预训练权重的基础上，对bert的内部结构进行修改，那么keras-bert就比较难满足我们的需求了，因为keras-bert为了代码的复用性，几乎将每个小模块都封装为了一个单独的库，比如keras-bert依赖于keras-transformer，而keras-transformer依赖于keras-multi-head，keras-multi-head依赖于keras-self-attention，这样一重重依赖下去，改起来就相当头疼了。\n\n所以，我决定重新写一个keras版的bert，争取在几个文件内把它完整地实现出来，减少这些依赖性，并且保留可以加载官方预训练权重的特性。\n\n## 鸣谢\n感谢CyberZHG大佬实现的<a href=\"https://github.com/CyberZHG/keras-bert\">keras-bert</a>，本实现有不少地方参考了keras-bert的源码，在此衷心感谢大佬的无私奉献。\n\n## 相关\n\n[bert4torch](https://github.com/Tongjilibo/bert4torch)：一个跟bert4keras风格很相似的pytorch-based的transofrmer库，使用pytorch的读者可以尝试。\n\n## 引用\n\n```\n@misc{bert4keras,\n  title={bert4keras},\n  author={Jianlin Su},\n  year={2020},\n  howpublished={\\url{https://bert4keras.spaces.ac.cn}},\n}\n```\n"
        },
        {
          "name": "bert4keras",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "pretraining",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.439453125,
          "content": "#! -*- coding: utf-8 -*-\n\nfrom setuptools import setup, find_packages\n\nsetup(\n    name='bert4keras',\n    version='0.11.5',\n    description='an elegant bert4keras',\n    long_description='bert4keras: https://github.com/bojone/bert4keras',\n    license='Apache License 2.0',\n    url='https://github.com/bojone/bert4keras',\n    author='bojone',\n    author_email='bojone@spaces.ac.cn',\n    install_requires=['keras<=2.3.1'],\n    packages=find_packages()\n)\n"
        }
      ]
    }
  ]
}