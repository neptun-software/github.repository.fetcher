{
  "metadata": {
    "timestamp": 1736560784883,
    "page": 474,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Ucas-HaoranWei/GOT-OCR2.0",
      "stars": 6532,
      "defaultBranch": "main",
      "files": [
        {
          "name": "GOT-OCR-2.0-master",
          "type": "tree",
          "content": null
        },
        {
          "name": "GOT-OCR-2.0-paper.pdf",
          "type": "blob",
          "size": 6991.4638671875,
          "content": ""
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.8935546875,
          "content": "<h3><a href=\"\">General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model</a></h3>\n\n<a href=\"https://huggingface.co/ucaslcl/GOT-OCR2_0\"><img src=\"https://img.shields.io/badge/Huggingface-yellow\"></a>\n<a href=\"https://modelscope.cn/models/stepfun-ai/GOT-OCR2_0\"><img src=\"https://img.shields.io/badge/Modelscope-red\"></a>\n<a href=\"https://arxiv.org/abs/2409.01704\"><img src=\"https://img.shields.io/badge/Paper-PDF-orange\"></a> \n<a href=\"https://zhuanlan.zhihu.com/p/718163422\"><img src=\"https://img.shields.io/badge/zhihu-red\"></a> \n<a href=\"https://huggingface.co/spaces/ucaslcl/GOT_online\"><img src=\"https://img.shields.io/badge/demo-green\"></a> \n\n[Haoran Wei*](https://scholar.google.com/citations?user=J4naK0MAAAAJ&hl=en), Chenglong Liu*, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu,  [Zheng Ge](https://joker316701882.github.io/), Liang Zhao, [Jianjian Sun](https://scholar.google.com/citations?user=MVZrGkYAAAAJ&hl=en), [Yuang Peng](https://yuangpeng.com), Chunrui Han, [Xiangyu Zhang](https://scholar.google.com/citations?user=yuB-cfoAAAAJ&hl=en)\n\n<p align=\"center\">\n<img src=\"assets/got_logo.png\" style=\"width: 200px\" align=center>\n</p>\n\n\n## Release\n- [2024/12/24] ðŸ”¥ðŸ”¥ðŸ”¥ My new work on system-2 perception is released [slow-perception](https://github.com/Ucas-HaoranWei/Slow-Perception).\n- [2024/12/18] ðŸš€ðŸš€ðŸš€ GOT-OCR2.0 is supported in [PaddleMIX](https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/GOT_OCR_2_0) by Paddle Team. Thanks for the Paddle team!\n- [2024/12/8] ðŸ”¥ðŸ”¥ðŸ”¥ The model download has exceeded 1M on [Huggingface](https://huggingface.co/stepfun-ai/GOT-OCR2_0).\n- [2024/12/5] The seven wechat [group](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/blob/main/assets/Wechat7.jpg).\n- [2024/11/4] The six wechat [group](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/blob/main/assets/wechat6-2.jpg).\n- [2024/10/24] The previous four wechat groups are full, so we created a fifth [group](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/blob/main/assets/wechat5.png).\n- [2024/10/11] Too many friends want to join the wechat group, so we created a fourth [group](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/blob/main/assets/wechat4.jpg).\n- [2024/10/2] [onnx](https://github.com/BaofengZan/GOT-OCRv2-onnx) and [mnn](https://github.com/BaofengZan/mnn-llm-GOT-OCR2.0) versions of GOT-OCR2.0.\n- [2024/9/29]ðŸ”¥ðŸ”¥ðŸ”¥ The community has implemented the first version of [llama_cpp_inference](https://github.com/1694439208/GOT-OCR-Inference).\n- [2024/9/24]ðŸ”¥ðŸ”¥ðŸ”¥ Support [ms-swift](https://github.com/modelscope/ms-swift/issues/2122) quick [Fine-tune](#fine-tune) for your own data. \n- [2024/9/23]ðŸ”¥ðŸ”¥ðŸ”¥ We release the official [Modelscope demo](https://modelscope.cn/studios/stepfun-ai/GOT_official_online_demo). Thanks very much for Modelscope providing the GPU resource.\n- [2024/9/19]ðŸ”¥ðŸ”¥ðŸ”¥ GOT-OCR2.0 achieves Huggingface trending #1.\n- [2024/9/14]ðŸ”¥ðŸ”¥ðŸ”¥ We release the official [demo](https://huggingface.co/spaces/ucaslcl/GOT_online). Thanks very much for Huggingface providing the GPU resource. \n- [2024/9/13]ðŸ”¥ðŸ”¥ðŸ”¥ We release the [Huggingface](https://huggingface.co/ucaslcl/GOT-OCR2_0) deployment. \n- [2024/9/03]ðŸ”¥ðŸ”¥ðŸ”¥ We open-source the codes, weights, and benchmarks. The paper can be found in this [repo](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/blob/main/GOT-OCR-2.0-paper.pdf). We also have submitted it to Arxiv. \n- [2024/9/03]ðŸ”¥ðŸ”¥ðŸ”¥ We release the OCR-2.0 model GOT! \n\n\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE)\n[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/DATA_LICENSE)\n\n\n\n\n## Community contributions\nWe encourage everyone to develop GOT applications based on this repo. Thanks for the following contributions :\n\n[GGUF and Llama.cpp inference](https://github.com/MosRat/got.cpp)~ contributor: [@MosRat](https://github.com/MosRat)\n\n[vllm reference](https://github.com/liunian-Jay/MU-GOT/blob/master/PDF_parsing/GOT/GOT/model/modeling_GOT_vllm.py) ~ contributor: [@Jay](https://github.com/liunian-Jay)\n\n[onnx and mnn supports](https://github.com/BaofengZan/GOT-OCRv2-onnx) ~ contributor: [@BaofengZan](https://github.com/BaofengZan)\n\n[llama_cpp inference](https://github.com/1694439208/GOT-OCR-Inference) ~ contributor: [@1694439208](https://github.com/1694439208)\n\n[Colab of GOT](https://colab.research.google.com/drive/1nmiNciZ5ugQVp4rFbL9ZWpEPd92Y9o7p?usp=sharing)   ~      contributor: [@Zizhe Wang](https://github.com/PaperPlaneDeemo)\n\n[CPU version of GOT](https://github.com/ElvisClaros/GOT-OCR2.0) ~ contributor: [@ElvisClaros](https://github.com/ElvisClaros)\n\n[Online demo](https://huggingface.co/spaces/Tonic/GOT-OCR) ~ contributor: [@Joseph Pollack](https://huggingface.co/Tonic)\n\n[Dokcer & client demo](https://github.com/QIN2DIM/GOT-OCR2.0) ~ contributor: [@QIN2DIM](https://github.com/QIN2DIM) \n\n[GUI of GOT](https://github.com/XJF2332/GOT-OCR-2-GUI) ~ contributor: [@XJF2332](https://github.com/XJF2332) \n\n## Contents\n- [Install](#install)\n- [GOT Weights](#got-weights)\n- [Benchmarks](#benchmarks)\n- [Demo](#demo)\n- [Train](#train)\n- [Fine-tune](#fine-tune)\n- [Eval](#eval)\n\n***\n<p align=\"center\">\n<img src=\"assets/got_support.jpg\" style=\"width: 800px\" align=center>\n</p>\n<p align=\"center\">\n<a href=\"\">Towards OCR-2.0 via a Unified End-to-end Model</a>       \n</p>\n\n***\n\n\n## Install\n0. Our environment is cuda11.8+torch2.0.1\n1. Clone this repository and navigate to the GOT folder\n```bash\ngit clone https://github.com/Ucas-HaoranWei/GOT-OCR2.0.git\ncd 'the GOT folder'\n```\n2. Install Package\n```Shell\nconda create -n got python=3.10 -y\nconda activate got\npip install -e .\n```\n\n3. Install Flash-Attention\n```\npip install ninja\npip install flash-attn --no-build-isolation\n```\n## GOT Weights\n- [Huggingface](https://huggingface.co/ucaslcl/GOT-OCR2_0)\n- [Google Drive](https://drive.google.com/drive/folders/1OdDtsJ8bFJYlNUzCQG4hRkUL6V-qBQaN?usp=sharing)\n- [BaiduYun](https://pan.baidu.com/s/1G4aArpCOt6I_trHv_1SE2g) code: OCR2\n\n## Benchmarks\n- [Google Drive](https://drive.google.com/drive/folders/1OdDtsJ8bFJYlNUzCQG4hRkUL6V-qBQaN?usp=sharing)\n- [BaiduYun](https://pan.baidu.com/s/1G4aArpCOt6I_trHv_1SE2g) code: OCR2\n\n## Demo\n1. plain texts OCR:\n```Shell\npython3 GOT/demo/run_ocr_2.0.py  --model-name  /GOT_weights/  --image-file  /an/image/file.png  --type ocr\n```\n2. format texts OCR:\n```Shell\npython3 GOT/demo/run_ocr_2.0.py  --model-name  /GOT_weights/  --image-file  /an/image/file.png  --type format\n```\n3. fine-grained OCR:\n```Shell\npython3 GOT/demo/run_ocr_2.0.py  --model-name  /GOT_weights/  --image-file  /an/image/file.png  --type format/ocr --box [x1,y1,x2,y2]\n```\n```Shell\npython3 GOT/demo/run_ocr_2.0.py  --model-name  /GOT_weights/  --image-file  /an/image/file.png  --type format/ocr --color red/green/blue\n```\n4. multi-crop OCR:\n```Shell\npython3 GOT/demo/run_ocr_2.0_crop.py  --model-name  /GOT_weights/ --image-file  /an/image/file.png \n```\n5. **Note**: This feature is not batch inference!! It works on the token level.  Please read the paper and then correct use multi-page OCR (the image path contains multiple .png files):\n```Shell\npython3 GOT/demo/run_ocr_2.0_crop.py  --model-name  /GOT_weights/ --image-file  /images/path/  --multi-page\n```\n6. render the formatted OCR results:\n```Shell\npython3 GOT/demo/run_ocr_2.0.py  --model-name  /GOT_weights/  --image-file  /an/image/file.png  --type format --render\n ```\n**Note**:\nThe rendering results can be found in /results/demo.html. Please open the demo.html to see the results.\n\n\n## Train\n0. Train sample can be found [here](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/blob/main/assets/train_sample.jpg). Note that the '\\<image>' in the 'conversations'-'human'-'value' is necessary!\n1. This codebase only supports post-training (stage-2/stage-3) upon our GOT weights.\n2. If you want to train from stage-1 described in our paper, you need this [repo](https://github.com/Ucas-HaoranWei/Vary-tiny-600k).\n\n```Shell\ndeepspeed   /GOT-OCR-2.0-master/GOT/train/train_GOT.py \\\n --deepspeed /GOT-OCR-2.0-master/zero_config/zero2.json    --model_name_or_path /GOT_weights/ \\\n --use_im_start_end True   \\\n --bf16 True   \\\n --gradient_accumulation_steps 2    \\\n --evaluation_strategy \"no\"   \\\n --save_strategy \"steps\"  \\\n --save_steps 200   \\\n --save_total_limit 1   \\\n --weight_decay 0.    \\\n --warmup_ratio 0.001     \\\n --lr_scheduler_type \"cosine\"    \\\n --logging_steps 1    \\\n --tf32 True     \\\n --model_max_length 8192    \\\n --gradient_checkpointing True   \\\n --dataloader_num_workers 8    \\\n --report_to none  \\\n --per_device_train_batch_size 2    \\\n --num_train_epochs 1  \\\n --learning_rate 2e-5   \\\n --datasets pdf-ocr+scence \\\n --output_dir /your/output/path\n```\n\n\n**Note**:\n1. Change the corresponding data information in [constant.py](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/tree/main/GOT-OCR-2.0-master/GOT/utils).\n2. Change line 37 in [conversation_dataset_qwen.py](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/tree/main/GOT-OCR-2.0-master/GOT/data) to your data_name.\n\n## Fine-tune\nQuick Fine-tune with ms-swift:\n\n```Shell\ngit clone https://github.com/modelscope/ms-swift.git\ncd ms-swift\npip install -e .[llm]\n```\n```Shell\n# defaultï¼šsft LLM & projector, freeze vision encoder\nCUDA_VISIBLE_DEVICES=0 swift sft\\\n--model_type got-ocr2 \\\n--model_id_or_path stepfun-ai/GOT-OCR2_0 \\\n--sft_type lora \\\n--dataset latex-ocr-print#5000\n\n# Deepspeed ZeRO2\nNPROC_PER_NODE=4 \\\nCUDA_VISIBLE_DEVICES=0,1,2,3 swift sft \\\n--model_type got-ocr2 \\\n--model_id_or_path stepfun-ai/GOT-OCR2_0 \\\n--sft_type lora \\\n--dataset latex-ocr-print#5000 \\\n--deepspeed default-zero2\n```\n\n**With your data**:\n```Shell\n--dataset train.jsonl\n--val_dataset val.jsonl (optional)\n```\n**Data format**:\n```Shell\n{\"query\": \"<image>55555\", \"response\": \"66666\", \"images\": [\"image_path\"]}\n{\"query\": \"<image><image>eeeee\", \"response\": \"fffff\", \"history\": [], \"images\": [\"image_path1\", \"image_path2\"]}\n{\"query\": \"EEEEE\", \"response\": \"FFFFF\", \"history\": [[\"query1\", \"response1\"], [\"query2\", \"response2\"]]}\n```\nMore details can be seen in [ms-swift](https://github.com/modelscope/ms-swift/issues/2122).\n\n## Eval\n1. We use the [Fox](https://github.com/ucaslcl/Fox) and [OneChart](https://github.com/LingyvKong/OneChart) benchmarks, and other benchmarks can be found in the weights download link.\n2. The eval codes can be found in GOT/eval.\n3. You can use the evaluate_GOT.py to run the eval. If you have 8 GPUsï¼Œ the --num-chunks can be set to 8.\n ```Shell\npython3 GOT/eval/evaluate_GOT.py --model-name /GOT_weights/ --gtfile_path xxxx.json --image_path  /image/path/ --out_path /data/eval_results/GOT_mathpix_test/ --num-chunks 8 --datatype OCR\n```\n\n## Contact\nIf you are interested in this work or have questions about the code or the paper, please join our communication [Wechat](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/blob/main/assets/wechat.jpg) group.\n\n**Note**:\nAll six wechat groups are full, please join [group 7](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/blob/main/assets/Wechat7.jpg).\n\nDon't hesitate to contact me by email, weihaoran18@mails.ucas.ac.cn, if you have any questions.\n\n## Acknowledgement\n- [Vary](https://github.com/Ucas-HaoranWei/Vary/): the codebase we built upon!\n- [Qwen](https://github.com/QwenLM/Qwen): the LLM base model of Vary, which is good at both English and Chinese!\n\n\n## Citation\n```bibtex\n@article{wei2024general,\n  title={General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model},\n  author={Wei, Haoran and Liu, Chenglong and Chen, Jinyue and Wang, Jia and Kong, Lingyu and Xu, Yanming and Ge, Zheng and Zhao, Liang and Sun, Jianjian and Peng, Yuang and others},\n  journal={arXiv preprint arXiv:2409.01704},\n  year={2024}\n}\n@article{wei2023vary,\n  title={Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models},\n  author={Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yang, Jinrong and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},\n  journal={arXiv preprint arXiv:2312.06109},\n  year={2023}\n}\n\n\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}