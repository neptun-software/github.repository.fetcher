{
  "metadata": {
    "timestamp": 1736560923918,
    "page": 654,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "rtqichen/torchdiffeq",
      "stars": 5708,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0703125,
          "content": "*.egg-info/\n.installed.cfg\n*.egg\n*__pycache__*\n*.pyc\n.vscode\nbuild\ndist\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.720703125,
          "content": "# YAML 1.2\n---\nabstract: |\n    \"This library provides ordinary differential equation (ODE) solvers implemented in PyTorch. Backpropagation through ODE solutions is supported using the adjoint method for constant memory cost. We also allow terminating an ODE solution based on an event function, with exact gradient computed.\n    \n    As the solvers are implemented in PyTorch, algorithms in this repository are fully supported to run on the GPU.\"\nauthors: \n  -\n    family-names: Chen\n    given-names: \"Ricky T. Q.\"\ncff-version: \"1.1.0\"\ndate-released: 2021-06-02\nlicense: MIT\nmessage: \"PyTorch Implementation of Differentiable ODE Solvers\"\nrepository-code: \"https://github.com/rtqichen/torchdiffeq\"\ntitle: torchdiffeq\nversion: \"0.2.2\"\n...\n"
        },
        {
          "name": "FAQ.md",
          "type": "blob",
          "size": 5.2216796875,
          "content": "# Frequently Asked Questions (FAQ)\n\n**What are good resources to understand how ODEs can be solved?**<br>\n*Solving Ordinary Differential Equations I Nonstiff Problems* by Hairer et al.<br>\n[ODE solver selection in MatLab](https://blogs.mathworks.com/loren/2015/09/23/ode-solver-selection-in-matlab/)<br>\n\n**What are the ODE solvers available in this repo?**<br>\n\n- Adaptive-step:\n  - `dopri8` Runge-Kutta 7(8) of Dormand-Prince-Shampine\n  - `dopri5` Runge-Kutta 4(5) of Dormand-Prince **[default]**.\n  - `bosh3` Runge-Kutta 2(3) of Bogacki-Shampine\n  - `adaptive_heun` Runge-Kutta 1(2)\n\n- Fixed-step:\n  - `euler` Euler method.\n  - `midpoint` Midpoint method.\n  - `rk4` Fourth-order Runge-Kutta with 3/8 rule.\n  - `explicit_adams` Explicit Adams.\n  - `implicit_adams` Implicit Adams.\n\n- `scipy_solver`: Wraps a SciPy solver.\n\n\n**What are `NFE-F` and `NFE-B`?**<br>\nNumber of function evaluations for forward and backward pass.\n\n**What are `rtol` and `atol`?**<br>\nThey refer to relative `rtol` and absolute `atol` error tolerance.\n\n**What is the role of error tolerance in adaptive solvers?**<br>\nThe basic idea is each adaptive solver can produce an error estimate of the current step, and if the error is greater than some tolerance, then the step is redone with a smaller step size, and this repeats until the error is smaller than the provided tolerance.<br>\n[Error Tolerances for Variable-Step Solvers](https://www.mathworks.com/help/simulink/ug/types-of-solvers.html#f11-44943)\n\n**How is the error tolerance calculated?**<br>\nThe error tolerance is [calculated]((https://github.com/rtqichen/torchdiffeq/blob/master/torchdiffeq/_impl/misc.py#L74)) as `atol + rtol * norm of current state`, where the norm being used is a mixed L-infinity/RMS norm.\n\n**Where is the code that computes the error tolerance?**<br>\nIt is computed [here.](https://github.com/rtqichen/torchdiffeq/blob/c4c9c61c939c630b9b88267aa56ddaaec319cb16/torchdiffeq/_impl/misc.py#L94)\n\n**How many states must a Neural ODE solver store during a forward pass with the adjoint method?**<br>\nThe number of states required to be stored in memory during a forward pass is solver dependent. For example, `dopri5` requires 6 intermediate states to be stored.\n\n**How many function evaluations are there per ODE step on adaptive solvers?**<br>\n\n- `dopri5`<br>\n\tThe `dopri5` ODE solver stores at least 6 evaluations of the ODE, then takes a step using a linear combination of them. The diagram below illustrates it: the evaluations marked with `o` are on the estimated path, the others with `x` are not. The first two are for selecting the initial step size.\n\n    ```\n\t0  1 |  2  3  4  5  6  7 |  8  9  10 12 13 14\n\to  x |  x  x  x  x  x  o |  x  x  x  x  x  o\n    ```\n\n\n**How do I obtain evaluations on the estimated path when using an adaptive solver?**<br>\nThe argument `t` of `odeint` specifies what times should the ODE solver output.<br>\n```odeint(func, x0, t=torch.linspace(0, 1, 50))```\n\nNote that the ODE solver will always integrate from `min t(0)` to `max t(1)`, and the intermediate values of `t` have no effect on how the ODE the solved. Intermediate values are computed using polynomial interpolation and have very small cost.\n\n**What non-linearities should I use in my Neural ODE?**<br>\nAvoid non-smooth non-linearities such as ReLU and LeakyReLU.<br>\nPrefer non-linearities with a theoretically unique adjoint/gradient such as Softplus.\n\n**Where is backpropagation for the Neural ODE defined?**<br>\nIt's defined [here](https://github.com/rtqichen/torchdiffeq/blob/master/torchdiffeq/_impl/adjoint.py) if you use the adjoint method `odeint_adjoint`.\n\n**What are Tableaus?**<br>\nTableaus are ways to describe coefficients for [RK methods](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods). The particular set of coefficients used on this repo was taken from [here](https://www.ams.org/journals/mcom/1986-46-173/S0025-5718-1986-0815836-3/).\n\n**How do I install the repo on Windows?**<br>\nTry downloading the code directly and just running python setup.py install.\nhttps://stackoverflow.com/questions/52528955/installing-a-python-module-from-github-in-windows-10\n\n**What is the most memory-expensive operation during training?**<br>\nThe most memory-expensive operation is the single [backward call](https://github.com/rtqichen/torchdiffeq/blob/master/torchdiffeq/_impl/adjoint.py#L75) made to the network.\n\n**My Neural ODE's numerical solution is farther away from the target than the initial value**<br>\nMost tricks for initializing residual nets (like zeroing the weights of the last layer) should help for ODEs as well. This will initialize the ODE as an identity.\n\n\n**My Neural ODE takes too long to train**<br>\nThis might be because you're running on CPU. Being extremely slow on CPU is expected, as training requires evaluating a neural net multiple times.\n\n\n**My Neural ODE produces underflow in dt when using adaptive solvers like `dopri5`**<br>\nThis is a problem of the ODE becoming stiff, essentially acting too erratic in a region and the step size becomes so close to zero that no progress can be made in the solver. We were able to avoid this with regularization such as weight decay and using \"nice\" activation functions, but YMMV. Other potential options are just to accept a larger error by increasing `atol`, `rtol`, or by switching to a fixed solver."
        },
        {
          "name": "FURTHER_DOCUMENTATION.md",
          "type": "blob",
          "size": 5.7958984375,
          "content": "# Further documentation\n\n## Solver options\n\nAdaptive and fixed solvers all support several options. Also shown are their default values.\n\n**Adaptive solvers (dopri8, dopri5, bosh3, adaptive_heun):**<br>\nFor these solvers, `rtol` and `atol` correspond to the tolerances for accepting/rejecting an adaptive step.\n\n- `first_step=None`: What size the first step of the solver should be; by default this is selected empirically.\n\n- `safety=0.9, ifactor=10.0, dfactor=0.2`: How the next optimal step size is calculated, see E. Hairer, S. P. Norsett G. Wanner, *Solving Ordinary Differential Equations I: Nonstiff Problems*, Sec. II.4. Roughly speaking, `safety` will try to shrink the step size slightly by this amount, `ifactor` is the most that the step size can grow by, and `dfactor` is the most that it can shrink by.\n\n- `max_num_steps=2 ** 31 - 1`: The maximum number of steps the solver is allowed to take.\n\n- `dtype=torch.float64`: what dtype to use for timelike quantities. Setting this to `torch.float32` will improve speed but may produce underflow errors more easily.\n\n- `step_t=None`: Times that a step must me made to. In particular this is useful when `func` has kinks (derivative discontinuities) at these times, as the solver then does not need to (slowly) discover these for itself. If passed this should be a `torch.Tensor`.\n\n- `jump_t=None`: Times that a step must be made to, and `func` re-evaluated at. In particular this is useful when `func` has discontinuites at these times, as then the solver knows that the final function evaluation of the previous step is not equal to the first function evaluation of this step. (i.e. the FSAL property does not hold at this point.) If passed this should be a `torch.Tensor`. Note that this may not be efficient when using PyTorch 1.6.0 or earlier.\n\n- `norm`: What norm to compute the accept/reject criterion with respect to. Given tensor input, this defaults to an RMS norm. Given tupled input, this defaults to computing an RMS norm over each tensor, and then taking a max over the tuple, producing a mixed L-infinity/RMS norm. If passed this should be a function consuming a tensor/tuple with the same shape as `y0`, and return a scalar corresponding to its norm. When passed as part of `adjoint_options`, then the special value `\"seminorm\"` may be used to zero out the contribution from the parameters, as per the [\"Hey, that's not an ODE\"](https://arxiv.org/abs/2009.09457) paper.\n\n**Fixed solvers (euler, midpoint, rk4, explicit_adams, implicit_adams):**<br>\n\n- `step_size=None`: How large each discrete step should be. If not passed then this defaults to stepping between the values of `t`. Note that if using `t` just to specify the start and end of the regions of integration, then it is very important to specify this argument! It is mutually exclusive with the `grid_constructor` argument, below.\n\n- `grid_constructor=None`: A more fine-grained way of setting the steps, by setting these particular locations as the locations of the steps. Should be a callable `func, y0, t -> grid`, transforming the arguments `func, y0, t` of `odeint` into the desired grid (which should be a one dimensional tensor).\n\n- `perturb`: Defaults to False. If True, then automatically add small perturbations to the start and end of each step, so that stepping to discontinuities works. Note that this this may not be efficient when using PyTorch 1.6.0 or earlier.\n\nIndividual solvers also offer certain options.\n\n**explicit_adams:**<br>\nFor this solver, `rtol` and `atol` are ignored. This solver also supports:\n\n- `max_order`: The maximum order of the Adams-Bashforth predictor.\n\n**implicit_adams:**<br>\nFor this solver, `rtol` and `atol` correspond to the tolerance for convergence of the Adams-Moulton corrector. This solver also supports:\n\n- `max_order`: The maximum order of the Adams-Bashforth-Moulton predictor-corrector.\n\n- `max_iters`: The maximum number of iterations to run the Adams-Moulton corrector for.\n\n**scipy_solver:**<br>\n- `solver`: which SciPy solver to use; corresponds to the `'method'` argument of `scipy.integrate.solve_ivp`.\n\n ## Adjoint options\n\n The function `odeint_adjoint` offers some adjoint-specific options.\n\n - `adjoint_rtol`,<br>`adjoint_atol`,<br>`adjoint_method`,<br>`adjoint_options`:<br>The `rtol, atol, method, options` to use for the backward pass. Defaults to the values used for the forward pass.\n\n - `adjoint_options` has the special key-value pair `{\"norm\": \"seminorm\"}` that provides a potentially more efficient adjoint solve when using adaptive step solvers, as described in the [\"Hey, that's not an ODE\"](https://arxiv.org/abs/2009.09457) paper.\n\n - `adjoint_params`: The parameters to compute gradients with respect to in the backward pass. Should be a tuple of tensors. Defaults to `tuple(func.parameters())`.\n   - If passed then `func` does not have to be a `torch.nn.Module`.\n   - If `func` has no parameters, `adjoint_params=()` must be specified.\n\n\n ## Callbacks\n\n Callbacks can be triggered during the solve. Callbacks should be specified as methods of the `func` argument to `odeint` and `odeint_adjoint`.\n\n At the moment support for this is minimal: let us know if you'd find additional callbacks useful.\n\n **callback_step(self, t0, y0, dt):**<br>\n This is called immediately before taking a step of size `dt`, at time `t0`, with current solution value `y0`. This is supported by every solver except `scipy_solver`.\n\n **callback_accept_step(self, t0, y0, dt):**<br>\n This is called when accepting a step of size `dt` at time `t0`, with current solution value `y0`. This is supported by the adaptive solvers (dopri8, dopri5, bosh3, adaptive_heun).\n\n **callback_reject_step(self, t0, y0, dt):**<br>\n As `callback_accept_step`, except called when rejecting steps.\n\n In addition, callbacks can be triggered during the adjoint pass by adding `_adjoint` to the name of any one of the supported callbacks, e.g. `callback_step_adjoint`."
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0498046875,
          "content": "MIT License\n\nCopyright (c) 2018 Ricky Tian Qi Chen\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.53125,
          "content": "# PyTorch Implementation of Differentiable ODE Solvers\n\nThis library provides ordinary differential equation (ODE) solvers implemented in PyTorch. Backpropagation through ODE solutions is supported using the adjoint method for constant memory cost. For usage of ODE solvers in deep learning applications, see reference [1].\n\nAs the solvers are implemented in PyTorch, algorithms in this repository are fully supported to run on the GPU.\n\n## Installation\n\nTo install latest stable version:\n```\npip install torchdiffeq\n```\n\nTo install latest on GitHub:\n```\npip install git+https://github.com/rtqichen/torchdiffeq\n```\n\n## Examples\nExamples are placed in the [`examples`](./examples) directory.\n\nWe encourage those who are interested in using this library to take a look at [`examples/ode_demo.py`](./examples/ode_demo.py) for understanding how to use `torchdiffeq` to fit a simple spiral ODE.\n\n<p align=\"center\">\n<img align=\"middle\" src=\"./assets/ode_demo.gif\" alt=\"ODE Demo\" width=\"500\" height=\"250\" />\n</p>\n\n## Basic usage\nThis library provides one main interface `odeint` which contains general-purpose algorithms for solving initial value problems (IVP), with gradients implemented for all main arguments. An initial value problem consists of an ODE and an initial value,\n```\ndy/dt = f(t, y)    y(t_0) = y_0.\n```\nThe goal of an ODE solver is to find a continuous trajectory satisfying the ODE that passes through the initial condition.\n\nTo solve an IVP using the default solver:\n```\nfrom torchdiffeq import odeint\n\nodeint(func, y0, t)\n```\nwhere `func` is any callable implementing the ordinary differential equation `f(t, x)`, `y0` is an _any_-D Tensor representing the initial values, and `t` is a 1-D Tensor containing the evaluation points. The initial time is taken to be `t[0]`.\n\nBackpropagation through `odeint` goes through the internals of the solver. Note that this is not numerically stable for all solvers (but should probably be fine with the default `dopri5` method). Instead, we encourage the use of the adjoint method explained in [1], which will allow solving with as many steps as necessary due to O(1) memory usage.\n\nTo use the adjoint method:\n```\nfrom torchdiffeq import odeint_adjoint as odeint\n\nodeint(func, y0, t)\n```\n`odeint_adjoint` simply wraps around `odeint`, but will use only O(1) memory in exchange for solving an adjoint ODE in the backward call.\n\nThe biggest **gotcha** is that `func` must be a `nn.Module` when using the adjoint method. This is used to collect parameters of the differential equation.\n\n## Differentiable event handling\n\nWe allow terminating an ODE solution based on an event function. Backpropagation through most solvers is supported. For usage of event handling in deep learning applications, see reference [2].\n\nThis can be invoked with `odeint_event`:\n```\nfrom torchdiffeq import odeint_event\nodeint_event(func, y0, t0, *, event_fn, reverse_time=False, odeint_interface=odeint, **kwargs)\n```\n - `func` and `y0` are the same as `odeint`.\n - `t0` is a scalar representing the initial time value.\n - `event_fn(t, y)` returns a tensor, and is a required keyword argument.\n - `reverse_time` is a boolean specifying whether we should solve in reverse time. Default is `False`.\n - `odeint_interface` is one of `odeint` or `odeint_adjoint`, specifying whether adjoint mode should be used for differentiating through the ODE solution. Default is `odeint`.\n - `**kwargs`: any remaining keyword arguments are passed to `odeint_interface`.\n\nThe solve is terminated at an event time `t` and state `y` when an element of `event_fn(t, y)` is equal to zero. Multiple outputs from `event_fn` can be used to specify multiple event functions, of which the first to trigger will terminate the solve.\n\nBoth the event time and final state are returned from `odeint_event`, and can be differentiated. Gradients will be backpropagated through the event function. **NOTE**: parameters for the event function must be in the state itself to obtain gradients. \n\nThe numerical precision for the event time is determined by the `atol` argument.\n\nSee example of simulating and differentiating through a bouncing ball in [`examples/bouncing_ball.py`](./examples/bouncing_ball.py). See example code for learning a simple event function in [`examples/learn_physics.py`](./examples/learn_physics.py).\n\n<p align=\"center\">\n<img align=\"middle\" src=\"./assets/bouncing_ball.png\" alt=\"Bouncing Ball\" width=\"500\" height=\"250\" />\n</p>\n\n## Keyword arguments for odeint(_adjoint)\n\n#### Keyword arguments:\n - `rtol` Relative tolerance.\n - `atol` Absolute tolerance.\n - `method` One of the solvers listed below.\n - `options` A dictionary of solver-specific options, see the [further documentation](FURTHER_DOCUMENTATION.md).\n\n#### List of ODE Solvers:\n\nAdaptive-step:\n - `dopri8` Runge-Kutta of order 8 of Dormand-Prince-Shampine.\n - `dopri5` Runge-Kutta of order 5 of Dormand-Prince-Shampine **[default]**.\n - `bosh3` Runge-Kutta of order 3 of Bogacki-Shampine.\n - `fehlberg2` Runge-Kutta-Fehlberg of order 2.\n - `adaptive_heun` Runge-Kutta of order 2.\n\nFixed-step:\n - `euler` Euler method.\n - `midpoint` Midpoint method.\n - `rk4` Fourth-order Runge-Kutta with 3/8 rule.\n - `explicit_adams` Explicit Adams-Bashforth.\n - `implicit_adams` Implicit Adams-Bashforth-Moulton.\n\nAdditionally, all solvers available through SciPy are wrapped for use with `scipy_solver`.\n\nFor most problems, good choices are the default `dopri5`, or to use `rk4` with `options=dict(step_size=...)` set appropriately small. Adjusting the tolerances (adaptive solvers) or step size (fixed solvers), will allow for trade-offs between speed and accuracy.\n\n## Frequently Asked Questions\nTake a look at our [FAQ](FAQ.md) for frequently asked questions.\n\n## Further documentation\nFor details of the adjoint-specific and solver-specific options, check out the [further documentation](FURTHER_DOCUMENTATION.md).\n\n## References\n\nApplications of differentiable ODE solvers and event handling are discussed in these two papers:\n\nRicky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud. \"Neural Ordinary Differential Equations.\" *Advances in Neural Information Processing Systems.* 2018. [[arxiv]](https://arxiv.org/abs/1806.07366)\n\n```\n@article{chen2018neuralode,\n  title={Neural Ordinary Differential Equations},\n  author={Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},\n  journal={Advances in Neural Information Processing Systems},\n  year={2018}\n}\n```\n\nRicky T. Q. Chen, Brandon Amos, Maximilian Nickel. \"Learning Neural Event Functions for Ordinary Differential Equations.\" *International Conference on Learning Representations.* 2021. [[arxiv]](https://arxiv.org/abs/2011.03902)\n\n```\n@article{chen2021eventfn,\n  title={Learning Neural Event Functions for Ordinary Differential Equations},\n  author={Chen, Ricky T. Q. and Amos, Brandon and Nickel, Maximilian},\n  journal={International Conference on Learning Representations},\n  year={2021}\n}\n```\n\nThe seminorm option for computing adjoints is discussed in\n\nPatrick Kidger, Ricky T. Q. Chen, Terry Lyons. \"'Hey, that’s not an ODE': Faster ODE Adjoints via Seminorms.\" *International Conference on Machine\nLearning.* 2021. [[arxiv]](https://arxiv.org/abs/2009.09457)\n```\n@article{kidger2021hey,\n  title={\"Hey, that's not an ODE\": Faster ODE Adjoints via Seminorms.},\n  author={Kidger, Patrick and Chen, Ricky T. Q. and Lyons, Terry J.},\n  journal={International Conference on Machine Learning},\n  year={2021}\n}\n```\n\n---\n\nIf you found this library useful in your research, please consider citing.\n```\n@misc{torchdiffeq,\n\tauthor={Chen, Ricky T. Q.},\n\ttitle={torchdiffeq},\n\tyear={2018},\n\turl={https://github.com/rtqichen/torchdiffeq},\n}\n```\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.9560546875,
          "content": "import os\nimport re\nimport setuptools\n\n\n# for simplicity we actually store the version in the __version__ attribute in the source\nhere = os.path.realpath(os.path.dirname(__file__))\nwith open(os.path.join(here, 'torchdiffeq', '__init__.py')) as f:\n    meta_match = re.search(r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\", f.read(), re.M)\n    if meta_match:\n        version = meta_match.group(1)\n    else:\n        raise RuntimeError(\"Unable to find __version__ string.\")\n\n\nsetuptools.setup(\n    name=\"torchdiffeq\",\n    version=version,\n    author=\"Ricky Tian Qi Chen\",\n    author_email=\"rtqichen@cs.toronto.edu\",\n    description=\"ODE solvers and adjoint sensitivity analysis in PyTorch.\",\n    url=\"https://github.com/rtqichen/torchdiffeq\",\n    packages=setuptools.find_packages(),\n    install_requires=['torch>=1.5.0', 'scipy>=1.4.0'],\n    python_requires='~=3.6',\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n    ],\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "torchdiffeq",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}