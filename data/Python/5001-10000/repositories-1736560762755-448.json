{
  "metadata": {
    "timestamp": 1736560762755,
    "page": 448,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "yenchenlin/DeepLearningFlappyBird",
      "stars": 6685,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0302734375,
          "content": "# ignore all pyc files.\n*.pyc\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0693359375,
          "content": "The MIT License\n\nCopyright (c) 2016-2018 Yen-Chen Lin http://yclin.me/\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.0947265625,
          "content": "# Using Deep Q-Network to Learn How To Play Flappy Bird\n\n<img src=\"./images/flappy_bird_demp.gif\" width=\"250\">\n\n7 mins version: [DQN for flappy bird](https://www.youtube.com/watch?v=THhUXIhjkCM)\n\n## Overview\nThis project follows the description of the Deep Q Learning algorithm described in Playing Atari with Deep Reinforcement Learning [2] and shows that this learning algorithm can be further generalized to the notorious Flappy Bird.\n\n## Installation Dependencies:\n* Python 2.7 or 3\n* TensorFlow 0.7\n* pygame\n* OpenCV-Python\n\n## How to Run?\n```\ngit clone https://github.com/yenchenlin1994/DeepLearningFlappyBird.git\ncd DeepLearningFlappyBird\npython deep_q_network.py\n```\n\n## What is Deep Q-Network?\nIt is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards.\n\nFor those who are interested in deep reinforcement learning, I highly recommend to read the following post:\n\n[Demystifying Deep Reinforcement Learning](http://www.nervanasys.com/demystifying-deep-reinforcement-learning/)\n\n## Deep Q-Network Algorithm\n\nThe pseudo-code for the Deep Q Learning algorithm, as given in [1], can be found below:\n\n```\nInitialize replay memory D to size N\nInitialize action-value function Q with random weights\nfor episode = 1, M do\n    Initialize state s_1\n    for t = 1, T do\n        With probability ϵ select random action a_t\n        otherwise select a_t=max_a  Q(s_t,a; θ_i)\n        Execute action a_t in emulator and observe r_t and s_(t+1)\n        Store transition (s_t,a_t,r_t,s_(t+1)) in D\n        Sample a minibatch of transitions (s_j,a_j,r_j,s_(j+1)) from D\n        Set y_j:=\n            r_j for terminal s_(j+1)\n            r_j+γ*max_(a^' )  Q(s_(j+1),a'; θ_i) for non-terminal s_(j+1)\n        Perform a gradient step on (y_j-Q(s_j,a_j; θ_i))^2 with respect to θ\n    end for\nend for\n```\n\n## Experiments\n\n#### Environment\nSince deep Q-network is trained on the raw pixel values observed from the game screen at each time step, [3] finds that remove the background appeared in the original game can make it converge faster. This process can be visualized as the following figure:\n\n<img src=\"./images/preprocess.png\" width=\"450\">\n\n#### Network Architecture\nAccording to [1], I first preprocessed the game screens with following steps:\n\n1. Convert image to grayscale\n2. Resize image to 80x80\n3. Stack last 4 frames to produce an 80x80x4 input array for network\n\nThe architecture of the network is shown in the figure below. The first layer convolves the input image with an 8x8x4x32 kernel at a stride size of 4. The output is then put through a 2x2 max pooling layer. The second layer convolves with a 4x4x32x64 kernel at a stride of 2. We then max pool again. The third layer convolves with a 3x3x64x64 kernel at a stride of 1. We then max pool one more time. The last hidden layer consists of 256 fully connected ReLU nodes.\n\n<img src=\"./images/network.png\">\n\nThe final output layer has the same dimensionality as the number of valid actions which can be performed in the game, where the 0th index always corresponds to doing nothing. The values at this output layer represent the Q function given the input state for each valid action. At each time step, the network performs whichever action corresponds to the highest Q value using a ϵ greedy policy.\n\n\n#### Training\nAt first, I initialize all weight matrices randomly using a normal distribution with a standard deviation of 0.01, then set the replay memory with a max size of 500,00 experiences.\n\nI start training by choosing actions uniformly at random for the first 10,000 time steps, without updating the network weights. This allows the system to populate the replay memory before training begins.\n\nNote that unlike [1], which initialize ϵ = 1, I linearly anneal ϵ from 0.1 to 0.0001 over the course of the next 3000,000 frames. The reason why I set it this way is that agent can choose an action every 0.03s (FPS=30) in our game, high ϵ will make it **flap** too much and thus keeps itself at the top of the game screen and finally bump the pipe in a clumsy way. This condition will make Q function converge relatively slow since it only start to look other conditions when ϵ is low.\nHowever, in other games, initialize ϵ to 1 is more reasonable.\n\nDuring training time, at each time step, the network samples minibatches of size 32 from the replay memory to train on, and performs a gradient step on the loss function described above using the Adam optimization algorithm with a learning rate of 0.000001. After annealing finishes, the network continues to train indefinitely, with ϵ fixed at 0.001.\n\n## FAQ\n\n#### Checkpoint not found\nChange [first line of `saved_networks/checkpoint`](https://github.com/yenchenlin1994/DeepLearningFlappyBird/blob/master/saved_networks/checkpoint#L1) to \n\n`model_checkpoint_path: \"saved_networks/bird-dqn-2920000\"`\n\n#### How to reproduce?\n1. Comment out [these lines](https://github.com/yenchenlin1994/DeepLearningFlappyBird/blob/master/deep_q_network.py#L108-L112)\n\n2. Modify `deep_q_network.py`'s parameter as follow:\n```python\nOBSERVE = 10000\nEXPLORE = 3000000\nFINAL_EPSILON = 0.0001\nINITIAL_EPSILON = 0.1\n```\n\n## References\n\n[1] Mnih Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. **Human-level Control through Deep Reinforcement Learning**. Nature, 529-33, 2015.\n\n[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. **Playing Atari with Deep Reinforcement Learning**. NIPS, Deep Learning workshop\n\n[3] Kevin Chen. **Deep Reinforcement Learning for Flappy Bird** [Report](http://cs229.stanford.edu/proj2015/362_report.pdf) | [Youtube result](https://youtu.be/9WKBzTUsPKc)\n\n## Disclaimer\nThis work is highly based on the following repos:\n\n1. [sourabhv/FlapPyBird] (https://github.com/sourabhv/FlapPyBird)\n2. [asrivat1/DeepLearningVideoGames](https://github.com/asrivat1/DeepLearningVideoGames)\n\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "deep_q_network.py",
          "type": "blob",
          "size": 7.078125,
          "content": "#!/usr/bin/env python\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport cv2\nimport sys\nsys.path.append(\"game/\")\nimport wrapped_flappy_bird as game\nimport random\nimport numpy as np\nfrom collections import deque\n\nGAME = 'bird' # the name of the game being played for log files\nACTIONS = 2 # number of valid actions\nGAMMA = 0.99 # decay rate of past observations\nOBSERVE = 100000. # timesteps to observe before training\nEXPLORE = 2000000. # frames over which to anneal epsilon\nFINAL_EPSILON = 0.0001 # final value of epsilon\nINITIAL_EPSILON = 0.0001 # starting value of epsilon\nREPLAY_MEMORY = 50000 # number of previous transitions to remember\nBATCH = 32 # size of minibatch\nFRAME_PER_ACTION = 1\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev = 0.01)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    initial = tf.constant(0.01, shape = shape)\n    return tf.Variable(initial)\n\ndef conv2d(x, W, stride):\n    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n\ndef createNetwork():\n    # network weights\n    W_conv1 = weight_variable([8, 8, 4, 32])\n    b_conv1 = bias_variable([32])\n\n    W_conv2 = weight_variable([4, 4, 32, 64])\n    b_conv2 = bias_variable([64])\n\n    W_conv3 = weight_variable([3, 3, 64, 64])\n    b_conv3 = bias_variable([64])\n\n    W_fc1 = weight_variable([1600, 512])\n    b_fc1 = bias_variable([512])\n\n    W_fc2 = weight_variable([512, ACTIONS])\n    b_fc2 = bias_variable([ACTIONS])\n\n    # input layer\n    s = tf.placeholder(\"float\", [None, 80, 80, 4])\n\n    # hidden layers\n    h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1)\n    h_pool1 = max_pool_2x2(h_conv1)\n\n    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2)\n    #h_pool2 = max_pool_2x2(h_conv2)\n\n    h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)\n    #h_pool3 = max_pool_2x2(h_conv3)\n\n    #h_pool3_flat = tf.reshape(h_pool3, [-1, 256])\n    h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\n\n    h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n\n    # readout layer\n    readout = tf.matmul(h_fc1, W_fc2) + b_fc2\n\n    return s, readout, h_fc1\n\ndef trainNetwork(s, readout, h_fc1, sess):\n    # define the cost function\n    a = tf.placeholder(\"float\", [None, ACTIONS])\n    y = tf.placeholder(\"float\", [None])\n    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\n    cost = tf.reduce_mean(tf.square(y - readout_action))\n    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\n\n    # open up a game state to communicate with emulator\n    game_state = game.GameState()\n\n    # store the previous observations in replay memory\n    D = deque()\n\n    # printing\n    a_file = open(\"logs_\" + GAME + \"/readout.txt\", 'w')\n    h_file = open(\"logs_\" + GAME + \"/hidden.txt\", 'w')\n\n    # get the first state by doing nothing and preprocess the image to 80x80x4\n    do_nothing = np.zeros(ACTIONS)\n    do_nothing[0] = 1\n    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n\n    # saving and loading networks\n    saver = tf.train.Saver()\n    sess.run(tf.initialize_all_variables())\n    checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n    if checkpoint and checkpoint.model_checkpoint_path:\n        saver.restore(sess, checkpoint.model_checkpoint_path)\n        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n    else:\n        print(\"Could not find old network weights\")\n\n    # start training\n    epsilon = INITIAL_EPSILON\n    t = 0\n    while \"flappy bird\" != \"angry bird\":\n        # choose an action epsilon greedily\n        readout_t = readout.eval(feed_dict={s : [s_t]})[0]\n        a_t = np.zeros([ACTIONS])\n        action_index = 0\n        if t % FRAME_PER_ACTION == 0:\n            if random.random() <= epsilon:\n                print(\"----------Random Action----------\")\n                action_index = random.randrange(ACTIONS)\n                a_t[random.randrange(ACTIONS)] = 1\n            else:\n                action_index = np.argmax(readout_t)\n                a_t[action_index] = 1\n        else:\n            a_t[0] = 1 # do nothing\n\n        # scale down epsilon\n        if epsilon > FINAL_EPSILON and t > OBSERVE:\n            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n\n        # run the selected action and observe next state and reward\n        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)\n        ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\n        x_t1 = np.reshape(x_t1, (80, 80, 1))\n        #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)\n        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n\n        # store the transition in D\n        D.append((s_t, a_t, r_t, s_t1, terminal))\n        if len(D) > REPLAY_MEMORY:\n            D.popleft()\n\n        # only train if done observing\n        if t > OBSERVE:\n            # sample a minibatch to train on\n            minibatch = random.sample(D, BATCH)\n\n            # get the batch variables\n            s_j_batch = [d[0] for d in minibatch]\n            a_batch = [d[1] for d in minibatch]\n            r_batch = [d[2] for d in minibatch]\n            s_j1_batch = [d[3] for d in minibatch]\n\n            y_batch = []\n            readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\n            for i in range(0, len(minibatch)):\n                terminal = minibatch[i][4]\n                # if terminal, only equals reward\n                if terminal:\n                    y_batch.append(r_batch[i])\n                else:\n                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n\n            # perform gradient step\n            train_step.run(feed_dict = {\n                y : y_batch,\n                a : a_batch,\n                s : s_j_batch}\n            )\n\n        # update the old values\n        s_t = s_t1\n        t += 1\n\n        # save progress every 10000 iterations\n        if t % 10000 == 0:\n            saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)\n\n        # print info\n        state = \"\"\n        if t <= OBSERVE:\n            state = \"observe\"\n        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n            state = \"explore\"\n        else:\n            state = \"train\"\n\n        print(\"TIMESTEP\", t, \"/ STATE\", state, \\\n            \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \\\n            \"/ Q_MAX %e\" % np.max(readout_t))\n        # write info to files\n        '''\n        if t % 10000 <= 100:\n            a_file.write(\",\".join([str(x) for x in readout_t]) + '\\n')\n            h_file.write(\",\".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\\n')\n            cv2.imwrite(\"logs_tetris/frame\" + str(t) + \".png\", x_t1)\n        '''\n\ndef playGame():\n    sess = tf.InteractiveSession()\n    s, readout, h_fc1 = createNetwork()\n    trainNetwork(s, readout, h_fc1, sess)\n\ndef main():\n    playGame()\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "game",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "logs_bird",
          "type": "tree",
          "content": null
        },
        {
          "name": "saved_networks",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}