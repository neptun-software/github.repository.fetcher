{
  "metadata": {
    "timestamp": 1736560809168,
    "page": 505,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "microsoft/TRELLIS",
      "stars": 6378,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 6.69921875,
          "content": "## Ignore Visual Studio temporary files, build results, and\n## files generated by popular Visual Studio add-ons.\n##\n## Get latest from https://github.com/github/gitignore/blob/main/VisualStudio.gitignore\n\n# User-specific files\n*.rsuser\n*.suo\n*.user\n*.userosscache\n*.sln.docstates\n\n# User-specific files (MonoDevelop/Xamarin Studio)\n*.userprefs\n\n# Mono auto generated files\nmono_crash.*\n\n# Build results\n[Dd]ebug/\n[Dd]ebugPublic/\n[Rr]elease/\n[Rr]eleases/\nx64/\nx86/\n[Ww][Ii][Nn]32/\n[Aa][Rr][Mm]/\n[Aa][Rr][Mm]64/\nbld/\n[Bb]in/\n[Oo]bj/\n[Ll]og/\n[Ll]ogs/\n\n# Visual Studio 2015/2017 cache/options directory\n.vs/\n# Uncomment if you have tasks that create the project's static files in wwwroot\n#wwwroot/\n\n# Visual Studio 2017 auto generated files\nGenerated\\ Files/\n\n# MSTest test Results\n[Tt]est[Rr]esult*/\n[Bb]uild[Ll]og.*\n\n# NUnit\n*.VisualState.xml\nTestResult.xml\nnunit-*.xml\n\n# Build Results of an ATL Project\n[Dd]ebugPS/\n[Rr]eleasePS/\ndlldata.c\n\n# Benchmark Results\nBenchmarkDotNet.Artifacts/\n\n# .NET Core\nproject.lock.json\nproject.fragment.lock.json\nartifacts/\n\n# ASP.NET Scaffolding\nScaffoldingReadMe.txt\n\n# StyleCop\nStyleCopReport.xml\n\n# Files built by Visual Studio\n*_i.c\n*_p.c\n*_h.h\n*.ilk\n*.meta\n*.obj\n*.iobj\n*.pch\n*.pdb\n*.ipdb\n*.pgc\n*.pgd\n*.rsp\n*.sbr\n*.tlb\n*.tli\n*.tlh\n*.tmp\n*.tmp_proj\n*_wpftmp.csproj\n*.log\n*.tlog\n*.vspscc\n*.vssscc\n.builds\n*.pidb\n*.svclog\n*.scc\n\n# Chutzpah Test files\n_Chutzpah*\n\n# Visual C++ cache files\nipch/\n*.aps\n*.ncb\n*.opendb\n*.opensdf\n*.sdf\n*.cachefile\n*.VC.db\n*.VC.VC.opendb\n\n# Visual Studio profiler\n*.psess\n*.vsp\n*.vspx\n*.sap\n\n# Visual Studio Trace Files\n*.e2e\n\n# TFS 2012 Local Workspace\n$tf/\n\n# Guidance Automation Toolkit\n*.gpState\n\n# ReSharper is a .NET coding add-in\n_ReSharper*/\n*.[Rr]e[Ss]harper\n*.DotSettings.user\n\n# TeamCity is a build add-in\n_TeamCity*\n\n# DotCover is a Code Coverage Tool\n*.dotCover\n\n# AxoCover is a Code Coverage Tool\n.axoCover/*\n!.axoCover/settings.json\n\n# Coverlet is a free, cross platform Code Coverage Tool\ncoverage*.json\ncoverage*.xml\ncoverage*.info\n\n# Visual Studio code coverage results\n*.coverage\n*.coveragexml\n\n# NCrunch\n_NCrunch_*\n.*crunch*.local.xml\nnCrunchTemp_*\n\n# MightyMoose\n*.mm.*\nAutoTest.Net/\n\n# Web workbench (sass)\n.sass-cache/\n\n# Installshield output folder\n[Ee]xpress/\n\n# DocProject is a documentation generator add-in\nDocProject/buildhelp/\nDocProject/Help/*.HxT\nDocProject/Help/*.HxC\nDocProject/Help/*.hhc\nDocProject/Help/*.hhk\nDocProject/Help/*.hhp\nDocProject/Help/Html2\nDocProject/Help/html\n\n# Click-Once directory\npublish/\n\n# Publish Web Output\n*.[Pp]ublish.xml\n*.azurePubxml\n# Note: Comment the next line if you want to checkin your web deploy settings,\n# but database connection strings (with potential passwords) will be unencrypted\n*.pubxml\n*.publishproj\n\n# Microsoft Azure Web App publish settings. Comment the next line if you want to\n# checkin your Azure Web App publish settings, but sensitive information contained\n# in these scripts will be unencrypted\nPublishScripts/\n\n# NuGet Packages\n*.nupkg\n# NuGet Symbol Packages\n*.snupkg\n# The packages folder can be ignored because of Package Restore\n**/[Pp]ackages/*\n# except build/, which is used as an MSBuild target.\n!**/[Pp]ackages/build/\n# Uncomment if necessary however generally it will be regenerated when needed\n#!**/[Pp]ackages/repositories.config\n# NuGet v3's project.json files produces more ignorable files\n*.nuget.props\n*.nuget.targets\n\n# Microsoft Azure Build Output\ncsx/\n*.build.csdef\n\n# Microsoft Azure Emulator\necf/\nrcf/\n\n# Windows Store app package directories and files\nAppPackages/\nBundleArtifacts/\nPackage.StoreAssociation.xml\n_pkginfo.txt\n*.appx\n*.appxbundle\n*.appxupload\n\n# Visual Studio cache files\n# files ending in .cache can be ignored\n*.[Cc]ache\n# but keep track of directories ending in .cache\n!?*.[Cc]ache/\n\n# Others\nClientBin/\n~$*\n*~\n*.dbmdl\n*.dbproj.schemaview\n*.jfm\n*.pfx\n*.publishsettings\norleans.codegen.cs\n\n# Including strong name files can present a security risk\n# (https://github.com/github/gitignore/pull/2483#issue-259490424)\n#*.snk\n\n# Since there are multiple workflows, uncomment next line to ignore bower_components\n# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)\n#bower_components/\n\n# RIA/Silverlight projects\nGenerated_Code/\n\n# Backup & report files from converting an old project file\n# to a newer Visual Studio version. Backup files are not needed,\n# because we have git ;-)\n_UpgradeReport_Files/\nBackup*/\nUpgradeLog*.XML\nUpgradeLog*.htm\nServiceFabricBackup/\n*.rptproj.bak\n\n# SQL Server files\n*.mdf\n*.ldf\n*.ndf\n\n# Business Intelligence projects\n*.rdl.data\n*.bim.layout\n*.bim_*.settings\n*.rptproj.rsuser\n*- [Bb]ackup.rdl\n*- [Bb]ackup ([0-9]).rdl\n*- [Bb]ackup ([0-9][0-9]).rdl\n\n# Microsoft Fakes\nFakesAssemblies/\n\n# GhostDoc plugin setting file\n*.GhostDoc.xml\n\n# Node.js Tools for Visual Studio\n.ntvs_analysis.dat\nnode_modules/\n\n# Visual Studio 6 build log\n*.plg\n\n# Visual Studio 6 workspace options file\n*.opt\n\n# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)\n*.vbw\n\n# Visual Studio 6 auto-generated project file (contains which files were open etc.)\n*.vbp\n\n# Visual Studio 6 workspace and project file (working project files containing files to include in project)\n*.dsw\n*.dsp\n\n# Visual Studio 6 technical files\n*.ncb\n*.aps\n\n# Visual Studio LightSwitch build output\n**/*.HTMLClient/GeneratedArtifacts\n**/*.DesktopClient/GeneratedArtifacts\n**/*.DesktopClient/ModelManifest.xml\n**/*.Server/GeneratedArtifacts\n**/*.Server/ModelManifest.xml\n_Pvt_Extensions\n\n# Paket dependency manager\n.paket/paket.exe\npaket-files/\n\n# FAKE - F# Make\n.fake/\n\n# CodeRush personal settings\n.cr/personal\n\n# Python Tools for Visual Studio (PTVS)\n__pycache__/\n*.pyc\n\n# Cake - Uncomment if you are using it\n# tools/**\n# !tools/packages.config\n\n# Tabs Studio\n*.tss\n\n# Telerik's JustMock configuration file\n*.jmconfig\n\n# BizTalk build output\n*.btp.cs\n*.btm.cs\n*.odx.cs\n*.xsd.cs\n\n# OpenCover UI analysis results\nOpenCover/\n\n# Azure Stream Analytics local run output\nASALocalRun/\n\n# MSBuild Binary and Structured Log\n*.binlog\n\n# NVidia Nsight GPU debugger configuration file\n*.nvuser\n\n# MFractors (Xamarin productivity tool) working folder\n.mfractor/\n\n# Local History for Visual Studio\n.localhistory/\n\n# Visual Studio History (VSHistory) files\n.vshistory/\n\n# BeatPulse healthcheck temp database\nhealthchecksdb\n\n# Backup folder for Package Reference Convert tool in Visual Studio 2017\nMigrationBackup/\n\n# Ionide (cross platform F# VS Code tools) working folder\n.ionide/\n\n# Fody - auto-generated XML schema\nFodyWeavers.xsd\n\n# VS Code files for those working on multiple tools\n.vscode/*\n!.vscode/settings.json\n!.vscode/tasks.json\n!.vscode/launch.json\n!.vscode/extensions.json\n*.code-workspace\n\n# Local History for Visual Studio Code\n.history/\n\n# Windows Installer files from build outputs\n*.cab\n*.msi\n*.msix\n*.msm\n*.msp\n\n# JetBrains Rider\n*.sln.iml\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.1513671875,
          "content": "[submodule \"trellis/representations/mesh/flexicubes\"]\n\tpath = trellis/representations/mesh/flexicubes\n\turl = https://github.com/MaxtirError/FlexiCubes.git\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.43359375,
          "content": "# Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns\n"
        },
        {
          "name": "DATASET.md",
          "type": "blob",
          "size": 8.1123046875,
          "content": "# TRELLIS-500K\n\nTRELLIS-500K is a dataset of 500K 3D assets curated from [Objaverse(XL)](https://objaverse.allenai.org/), [ABO](https://amazon-berkeley-objects.s3.amazonaws.com/index.html), [3D-FUTURE](https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future), [HSSD](https://huggingface.co/datasets/hssd/hssd-models), and [Toys4k](https://github.com/rehg-lab/lowshot-shapebias/tree/main/toys4k), filtered based on aesthetic scores.\nThis dataset serves for 3D generation tasks.\n\nThe dataset is provided as csv files containing the 3D assets' metadata.\n\n## Dataset Statistics\n\nThe following table summarizes the dataset's filtering and composition:\n\n***NOTE: Some of the 3D assets lack text captions. Please filter out such assets if captions are required.***\n| Source | Aesthetic Score Threshold | Filtered Size | With Captions |\n|:-:|:-:|:-:|:-:|\n| ObjaverseXL (sketchfab) | 5.5 | 168307 | 167638 |\n| ObjaverseXL (github) | 5.5 | 311843 | 306790 |\n| ABO | 4.5 | 4485 | 4390 |\n| 3D-FUTURE | 4.5 | 9472 | 9291 |\n| HSSD | 4.5 | 6670 | 6661 |\n| All (training set) | - | 500777 | 494770 |\n| Toys4k (evaluation set) | 4.5 | 3229 | 3180 |\n\n## Dataset Location\n\nThe dataset is hosted on Hugging Face Datasets. You can preview the dataset at\n\n[https://huggingface.co/datasets/JeffreyXiang/TRELLIS-500K](https://huggingface.co/datasets/JeffreyXiang/TRELLIS-500K)\n\nThere is no need to download the csv files manually. We provide toolkits to load and prepare the dataset.\n\n## Dataset Toolkits\n\nWe provide [toolkits](dataset_toolkits) for data preparation.\n\n### Step 1: Install Dependencies\n\n```\n. ./dataset_toolkits/setup.sh\n```\n\n### Step 2: Load Metadata\n\nFirst, we need to load the metadata of the dataset.\n\n```\npython dataset_toolkits/build_metadata.py <SUBSET> --output_dir <OUTPUT_DIR> [--source <SOURCE>]\n```\n\n- `SUBSET`: The subset of the dataset to load. Options are `ObjaverseXL`, `ABO`, `3D-FUTURE`, `HSSD`, and `Toys4k`.\n- `OUTPUT_DIR`: The directory to save the data.\n- `SOURCE`: Required if `SUBSET` is `ObjaverseXL`. Options are `sketchfab` and `github`.\n\nFor example, to load the metadata of the ObjaverseXL (sketchfab) subset and save it to `datasets/ObjaverseXL_sketchfab`, we can run:\n\n```\npython dataset_toolkits/build_metadata.py ObjaverseXL --source sketchfab --output_dir datasets/ObjaverseXL_sketchfab\n```\n\n### Step 3: Download Data\n\nNext, we need to download the 3D assets.\n\n```\npython dataset_toolkits/download.py <SUBSET> --output_dir <OUTPUT_DIR> [--rank <RANK> --world_size <WORLD_SIZE>]\n```\n\n- `SUBSET`: The subset of the dataset to download. Options are `ObjaverseXL`, `ABO`, `3D-FUTURE`, `HSSD`, and `Toys4k`.\n- `OUTPUT_DIR`: The directory to save the data.\n\nYou can also specify the `RANK` and `WORLD_SIZE` of the current process if you are using multiple nodes for data preparation.\n\nFor example, to download the ObjaverseXL (sketchfab) subset and save it to `datasets/ObjaverseXL_sketchfab`, we can run: \n\n***NOTE: The example command below sets a large `WORLD_SIZE` for demonstration purposes. Only a small portion of the dataset will be downloaded.***\n\n```\npython dataset_toolkits/download.py ObjaverseXL --output_dir datasets/ObjaverseXL_sketchfab --world_size 160000\n```\n\nSome datasets may require interactive login to Hugging Face or manual downloading. Please follow the instructions given by the toolkits.\n\nAfter downloading, update the metadata file with:\n\n```\npython dataset_toolkits/build_metadata.py ObjaverseXL --output_dir datasets/ObjaverseXL_sketchfab\n```\n\n### Step 4: Render Multiview Images\n\nMultiview images can be rendered with:\n\n```\npython dataset_toolkits/render.py <SUBSET> --output_dir <OUTPUT_DIR> [--num_views <NUM_VIEWS>] [--rank <RANK> --world_size <WORLD_SIZE>]\n```\n\n- `SUBSET`: The subset of the dataset to render. Options are `ObjaverseXL`, `ABO`, `3D-FUTURE`, `HSSD`, and `Toys4k`.\n- `OUTPUT_DIR`: The directory to save the data.\n- `NUM_VIEWS`: The number of views to render. Default is 150.\n- `RANK` and `WORLD_SIZE`: Multi-node configuration.\n\nFor example, to render the ObjaverseXL (sketchfab) subset and save it to `datasets/ObjaverseXL_sketchfab`, we can run:\n\n```\npython dataset_toolkits/render.py ObjaverseXL --output_dir datasets/ObjaverseXL_sketchfab\n```\n\nDon't forget to update the metadata file with:\n\n```\npython dataset_toolkits/build_metadata.py ObjaverseXL --output_dir datasets/ObjaverseXL_sketchfab\n```\n\n### Step 5: Voxelize 3D Models\n\nWe can voxelize the 3D models with:\n\n```\npython dataset_toolkits/voxelize.py <SUBSET> --output_dir <OUTPUT_DIR> [--rank <RANK> --world_size <WORLD_SIZE>]\n```\n\n- `SUBSET`: The subset of the dataset to voxelize. Options are `ObjaverseXL`, `ABO`, `3D-FUTURE`, `HSSD`, and `Toys4k`.\n- `OUTPUT_DIR`: The directory to save the data.\n- `RANK` and `WORLD_SIZE`: Multi-node configuration.\n\nFor example, to voxelize the ObjaverseXL (sketchfab) subset and save it to `datasets/ObjaverseXL_sketchfab`, we can run:\n```\npython dataset_toolkits/voxelize.py ObjaverseXL --output_dir datasets/ObjaverseXL_sketchfab\n```\n\nThen update the metadata file with:\n\n```\npython dataset_toolkits/build_metadata.py ObjaverseXL --output_dir datasets/ObjaverseXL_sketchfab\n```\n\n### Step 6: Extract DINO Features\n\nTo prepare the training data for SLat VAE, we need to extract DINO features from multiview images and aggregate them into sparse voxel grids.\n\n```\npython dataset_toolkits/extract_features.py --output_dir <OUTPUT_DIR> [--rank <RANK> --world_size <WORLD_SIZE>]\n```\n\n- `OUTPUT_DIR`: The directory to save the data.\n- `RANK` and `WORLD_SIZE`: Multi-node configuration.\n\n\nFor example, to extract DINO features from the ObjaverseXL (sketchfab) subset and save it to `datasets/ObjaverseXL_sketchfab`, we can run:\n\n```\npython dataset_toolkits/extract_feature.py --output_dir datasets/ObjaverseXL_sketchfab\n```\n\nThen update the metadata file with:\n\n```\npython dataset_toolkits/build_metadata.py ObjaverseXL --output_dir datasets/ObjaverseXL_sketchfab\n```\n\n### Step 7: Encode Sparse Structures\n\nEncoding the sparse structures into latents to train the first stage generator:\n\n```\npython dataset_toolkits/encode_ss_latent.py --output_dir <OUTPUT_DIR> [--rank <RANK> --world_size <WORLD_SIZE>]\n```\n\n- `OUTPUT_DIR`: The directory to save the data.\n- `RANK` and `WORLD_SIZE`: Multi-node configuration.\n\nFor example, to encode the sparse structures into latents for the ObjaverseXL (sketchfab) subset and save it to `datasets/ObjaverseXL_sketchfab`, we can run:\n\n```\npython dataset_toolkits/encode_ss_latent.py --output_dir datasets/ObjaverseXL_sketchfab\n```\n\nThen update the metadata file with:\n\n```\npython dataset_toolkits/build_metadata.py ObjaverseXL --output_dir datasets/ObjaverseXL_sketchfab\n```\n\n### Step 8: Encode SLat\n\nEncoding SLat for second stage generator training:\n\n```\npython dataset_toolkits/encode_latent.py --output_dir <OUTPUT_DIR> [--rank <RANK> --world_size <WORLD_SIZE>]\n```\n\n- `OUTPUT_DIR`: The directory to save the data.\n- `RANK` and `WORLD_SIZE`: Multi-node configuration.\n\nFor example, to encode SLat for the ObjaverseXL (sketchfab) subset and save it to `datasets/ObjaverseXL_sketchfab`, we can run:\n\n```\npython dataset_toolkits/encode_latent.py --output_dir datasets/ObjaverseXL_sketchfab\n```\n\nThen update the metadata file with:\n\n```\npython dataset_toolkits/build_metadata.py ObjaverseXL --output_dir datasets/ObjaverseXL_sketchfab\n```\n\n### Step 9: Render Image Conditions\n\nTo train the image conditioned generator, we need to render image conditions with augmented views.\n\n```\npython dataset_toolkits/render_cond.py <SUBSET> --output_dir <OUTPUT_DIR> [--num_views <NUM_VIEWS>] [--rank <RANK> --world_size <WORLD_SIZE>]\n```\n\n- `SUBSET`: The subset of the dataset to render. Options are `ObjaverseXL`, `ABO`, `3D-FUTURE`, `HSSD`, and `Toys4k`.\n- `OUTPUT_DIR`: The directory to save the data.\n- `NUM_VIEWS`: The number of views to render. Default is 24.\n- `RANK` and `WORLD_SIZE`: Multi-node configuration.\n\nFor example, to render image conditions for the ObjaverseXL (sketchfab) subset and save it to `datasets/ObjaverseXL_sketchfab`, we can run:\n\n```\npython dataset_toolkits/render_cond.py ObjaverseXL --output_dir datasets/ObjaverseXL_sketchfab\n```\n\nThen update the metadata file with:\n\n```\npython dataset_toolkits/build_metadata.py ObjaverseXL --output_dir datasets/ObjaverseXL_sketchfab\n```\n\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.1142578125,
          "content": "    MIT License\n\n    Copyright (c) Microsoft Corporation.\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all\n    copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n    SOFTWARE\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.7333984375,
          "content": "<img src=\"assets/logo.webp\" width=\"100%\" align=\"center\">\n<h1 align=\"center\">Structured 3D Latents<br>for Scalable and Versatile 3D Generation</h1>\n<p align=\"center\"><a href=\"https://arxiv.org/abs/2412.01506\"><img src='https://img.shields.io/badge/arXiv-Paper-red?logo=arxiv&logoColor=white' alt='arXiv'></a>\n<a href='https://trellis3d.github.io'><img src='https://img.shields.io/badge/Project_Page-Website-green?logo=googlechrome&logoColor=white' alt='Project Page'></a>\n<a href='https://huggingface.co/spaces/JeffreyXiang/TRELLIS'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Live_Demo-blue'></a>\n</p>\n<p align=\"center\"><img src=\"assets/teaser.png\" width=\"100%\"></p>\n\n<span style=\"font-size: 16px; font-weight: 600;\">T</span><span style=\"font-size: 12px; font-weight: 700;\">RELLIS</span> is a large 3D asset generation model. It takes in text or image prompts and generates high-quality 3D assets in various formats, such as Radiance Fields, 3D Gaussians, and meshes. The cornerstone of <span style=\"font-size: 16px; font-weight: 600;\">T</span><span style=\"font-size: 12px; font-weight: 700;\">RELLIS</span> is a unified Structured LATent (<span style=\"font-size: 16px; font-weight: 600;\">SL</span><span style=\"font-size: 12px; font-weight: 700;\">AT</span>) representation that allows decoding to different output formats and Rectified Flow Transformers tailored for <span style=\"font-size: 16px; font-weight: 600;\">SL</span><span style=\"font-size: 12px; font-weight: 700;\">AT</span> as the powerful backbones. We provide large-scale pre-trained models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. <span style=\"font-size: 16px; font-weight: 600;\">T</span><span style=\"font-size: 12px; font-weight: 700;\">RELLIS</span> significantly surpasses existing methods, including recent ones at similar scales, and showcases flexible output format selection and local 3D editing capabilities which were not offered by previous models.\n\n***Check out our [Project Page](https://trellis3d.github.io) for more videos and interactive demos!***\n\n<!-- Features -->\n## üåü Features\n- **High Quality**: It produces diverse 3D assets at high quality with intricate shape and texture details.\n- **Versatility**: It takes text or image prompts and can generate various final 3D representations including but not limited to *Radiance Fields*, *3D Gaussians*, and *meshes*, accommodating diverse downstream requirements.\n- **Flexible Editing**: It allows for easy editings of generated 3D assets, such as generating variants of the same object or local editing of the 3D asset.\n\n<!-- Updates -->\n## ‚è© Updates\n\n**12/26/2024**\n- Release [**TRELLIS-500K**](https://github.com/microsoft/TRELLIS#-dataset) dataset and toolkits for data preparation.\n\n**12/18/2024**\n- Implementation of multi-image conditioning for TRELLIS-image model. ([#7](https://github.com/microsoft/TRELLIS/issues/7)). This is based on tuning-free algorithm without training a specialized model, so it may not give the best results for all input images.\n- Add Gaussian export in `app.py` and `example.py`. ([#40](https://github.com/microsoft/TRELLIS/issues/40))\n\n<!-- TODO List -->\n## üöß TODO List\n- [x] Release inference code and TRELLIS-image-large model\n- [x] Release dataset and dataset toolkits\n- [ ] Release TRELLIS-text model series\n- [ ] Release training code\n\n<!-- Installation -->\n## üì¶ Installation\n\n### Prerequisites\n- **System**: The code is currently tested only on **Linux**.  For windows setup, you may refer to [#3](https://github.com/microsoft/TRELLIS/issues/3) (not fully tested).\n- **Hardware**: An NVIDIA GPU with at least 16GB of memory is necessary. The code has been verified on NVIDIA A100 and A6000 GPUs.  \n- **Software**:   \n  - The [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive) is needed to compile certain submodules. The code has been tested with CUDA versions 11.8 and 12.2.  \n  - [Conda](https://docs.anaconda.com/miniconda/install/#quick-command-line-install) is recommended for managing dependencies.  \n  - Python version 3.8 or higher is required. \n\n### Installation Steps\n1. Clone the repo:\n    ```sh\n    git clone --recurse-submodules https://github.com/microsoft/TRELLIS.git\n    cd TRELLIS\n    ```\n\n2. Install the dependencies:\n    \n    **Before running the following command there are somethings to note:**\n    - By adding `--new-env`, a new conda environment named `trellis` will be created. If you want to use an existing conda environment, please remove this flag.\n    - By default the `trellis` environment will use pytorch 2.4.0 with CUDA 11.8. If you want to use a different version of CUDA (e.g., if you have CUDA Toolkit 12.2 installed and do not want to install another 11.8 version for submodule compilation), you can remove the `--new-env` flag and manually install the required dependencies. Refer to [PyTorch](https://pytorch.org/get-started/previous-versions/) for the installation command.\n    - If you have multiple CUDA Toolkit versions installed, `PATH` should be set to the correct version before running the command. For example, if you have CUDA Toolkit 11.8 and 12.2 installed, you should run `export PATH=/usr/local/cuda-11.8/bin:$PATH` before running the command.\n    - By default, the code uses the `flash-attn` backend for attention. For GPUs do not support `flash-attn` (e.g., NVIDIA V100), you can remove the `--flash-attn` flag to install `xformers` only and set the `ATTN_BACKEND` environment variable to `xformers` before running the code. See the [Minimal Example](#minimal-example) for more details.\n    - The installation may take a while due to the large number of dependencies. Please be patient. If you encounter any issues, you can try to install the dependencies one by one, specifying one flag at a time.\n    - If you encounter any issues during the installation, feel free to open an issue or contact us.\n    \n    Create a new conda environment named `trellis` and install the dependencies:\n    ```sh\n    . ./setup.sh --new-env --basic --xformers --flash-attn --diffoctreerast --spconv --mipgaussian --kaolin --nvdiffrast\n    ```\n    The detailed usage of `setup.sh` can be found by running `. ./setup.sh --help`.\n    ```sh\n    Usage: setup.sh [OPTIONS]\n    Options:\n        -h, --help              Display this help message\n        --new-env               Create a new conda environment\n        --basic                 Install basic dependencies\n        --xformers              Install xformers\n        --flash-attn            Install flash-attn\n        --diffoctreerast        Install diffoctreerast\n        --vox2seq               Install vox2seq\n        --spconv                Install spconv\n        --mipgaussian           Install mip-splatting\n        --kaolin                Install kaolin\n        --nvdiffrast            Install nvdiffrast\n        --demo                  Install all dependencies for demo\n    ```\n\n<!-- Pretrained Models -->\n## ü§ñ Pretrained Models\n\nWe provide the following pretrained models:\n\n| Model | Description | #Params | Download |\n| --- | --- | --- | --- |\n| TRELLIS-image-large | Large image-to-3D model | 1.2B | [Download](https://huggingface.co/JeffreyXiang/TRELLIS-image-large) |\n| TRELLIS-text-base | Base text-to-3D model | 342M | Coming Soon |\n| TRELLIS-text-large | Large text-to-3D model | 1.1B | Coming Soon |\n| TRELLIS-text-xlarge | Extra-large text-to-3D model | 2.0B | Coming Soon |\n\nThe models are hosted on Hugging Face. You can directly load the models with their repository names in the code:\n```python\nTrellisImageTo3DPipeline.from_pretrained(\"JeffreyXiang/TRELLIS-image-large\")\n```\n\nIf you prefer loading the model from local, you can download the model files from the links above and load the model with the folder path (folder structure should be maintained):\n```python\nTrellisImageTo3DPipeline.from_pretrained(\"/path/to/TRELLIS-image-large\")\n```\n\n<!-- Usage -->\n## üí° Usage\n\n### Minimal Example\n\nHere is an [example](example.py) of how to use the pretrained models for 3D asset generation.\n\n```python\nimport os\n# os.environ['ATTN_BACKEND'] = 'xformers'   # Can be 'flash-attn' or 'xformers', default is 'flash-attn'\nos.environ['SPCONV_ALGO'] = 'native'        # Can be 'native' or 'auto', default is 'auto'.\n                                            # 'auto' is faster but will do benchmarking at the beginning.\n                                            # Recommended to set to 'native' if run only once.\n\nimport imageio\nfrom PIL import Image\nfrom trellis.pipelines import TrellisImageTo3DPipeline\nfrom trellis.utils import render_utils, postprocessing_utils\n\n# Load a pipeline from a model folder or a Hugging Face model hub.\npipeline = TrellisImageTo3DPipeline.from_pretrained(\"JeffreyXiang/TRELLIS-image-large\")\npipeline.cuda()\n\n# Load an image\nimage = Image.open(\"assets/example_image/T.png\")\n\n# Run the pipeline\noutputs = pipeline.run(\n    image,\n    seed=1,\n    # Optional parameters\n    # sparse_structure_sampler_params={\n    #     \"steps\": 12,\n    #     \"cfg_strength\": 7.5,\n    # },\n    # slat_sampler_params={\n    #     \"steps\": 12,\n    #     \"cfg_strength\": 3,\n    # },\n)\n# outputs is a dictionary containing generated 3D assets in different formats:\n# - outputs['gaussian']: a list of 3D Gaussians\n# - outputs['radiance_field']: a list of radiance fields\n# - outputs['mesh']: a list of meshes\n\n# Render the outputs\nvideo = render_utils.render_video(outputs['gaussian'][0])['color']\nimageio.mimsave(\"sample_gs.mp4\", video, fps=30)\nvideo = render_utils.render_video(outputs['radiance_field'][0])['color']\nimageio.mimsave(\"sample_rf.mp4\", video, fps=30)\nvideo = render_utils.render_video(outputs['mesh'][0])['normal']\nimageio.mimsave(\"sample_mesh.mp4\", video, fps=30)\n\n# GLB files can be extracted from the outputs\nglb = postprocessing_utils.to_glb(\n    outputs['gaussian'][0],\n    outputs['mesh'][0],\n    # Optional parameters\n    simplify=0.95,          # Ratio of triangles to remove in the simplification process\n    texture_size=1024,      # Size of the texture used for the GLB\n)\nglb.export(\"sample.glb\")\n\n# Save Gaussians as PLY files\noutputs['gaussian'][0].save_ply(\"sample.ply\")\n```\n\nAfter running the code, you will get the following files:\n- `sample_gs.mp4`: a video showing the 3D Gaussian representation\n- `sample_rf.mp4`: a video showing the Radiance Field representation\n- `sample_mesh.mp4`: a video showing the mesh representation\n- `sample.glb`: a GLB file containing the extracted textured mesh\n- `sample.ply`: a PLY file containing the 3D Gaussian representation\n\n\n### Web Demo\n\n[app.py](app.py) provides a simple web demo for 3D asset generation. Since this demo is based on [Gradio](https://gradio.app/), additional dependencies are required:\n```sh\n. ./setup.sh --demo\n```\n\nAfter installing the dependencies, you can run the demo with the following command:\n```sh\npython app.py\n```\n\nThen, you can access the demo at the address shown in the terminal.\n\n***The web demo is also available on [Hugging Face Spaces](https://huggingface.co/spaces/JeffreyXiang/TRELLIS)!***\n\n\n<!-- Dataset -->\n## üìö Dataset\n\nWe provide **TRELLIS-500K**, a large-scale dataset containing 500K 3D assets curated from [Objaverse(XL)](https://objaverse.allenai.org/), [ABO](https://amazon-berkeley-objects.s3.amazonaws.com/index.html), [3D-FUTURE](https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future), [HSSD](https://huggingface.co/datasets/hssd/hssd-models), and [Toys4k](https://github.com/rehg-lab/lowshot-shapebias/tree/main/toys4k), filtered based on aesthetic scores. Please refer to the [dataset README](DATASET.md) for more details.\n\n<!-- License -->\n## ‚öñÔ∏è License\n\nTRELLIS models and the majority of the code are licensed under the [MIT License](LICENSE). The following submodules may have different licenses:\n- [**diffoctreerast**](https://github.com/JeffreyXiang/diffoctreerast): We developed a CUDA-based real-time differentiable octree renderer for rendering radiance fields as part of this project. This renderer is derived from the [diff-gaussian-rasterization](https://github.com/graphdeco-inria/diff-gaussian-rasterization) project and is available under the [LICENSE](https://github.com/JeffreyXiang/diffoctreerast/blob/master/LICENSE).\n\n\n- [**Modified Flexicubes**](https://github.com/MaxtirError/FlexiCubes): In this project, we used a modified version of [Flexicubes](https://github.com/nv-tlabs/FlexiCubes) to support vertex attributes. This modified version is licensed under the [LICENSE](https://github.com/nv-tlabs/FlexiCubes/blob/main/LICENSE.txt).\n\n\n\n\n<!-- Citation -->\n## üìú Citation\n\nIf you find this work helpful, please consider citing our paper:\n\n```bibtex\n@article{xiang2024structured,\n    title   = {Structured 3D Latents for Scalable and Versatile 3D Generation},\n    author  = {Xiang, Jianfeng and Lv, Zelong and Xu, Sicheng and Deng, Yu and Wang, Ruicheng and Zhang, Bowen and Chen, Dong and Tong, Xin and Yang, Jiaolong},\n    journal = {arXiv preprint arXiv:2412.01506},\n    year    = {2024}\n}\n```\n\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 2.59375,
          "content": "<!-- BEGIN MICROSOFT SECURITY.MD V0.0.9 BLOCK -->\n\n## Security\n\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet) and [Xamarin](https://github.com/xamarin).\n\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/security.md/definition), please report it to us as described below.\n\n## Reporting Security Issues\n\n**Please do not report security vulnerabilities through public GitHub issues.**\n\nInstead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/security.md/msrc/create-report).\n\nIf you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/security.md/msrc/pgp).\n\nYou should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc). \n\nPlease include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:\n\n  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)\n  * Full paths of source file(s) related to the manifestation of the issue\n  * The location of the affected source code (tag/branch/commit or direct URL)\n  * Any special configuration required to reproduce the issue\n  * Step-by-step instructions to reproduce the issue\n  * Proof-of-concept or exploit code (if possible)\n  * Impact of the issue, including how an attacker might exploit the issue\n\nThis information will help us triage your report more quickly.\n\nIf you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/security.md/msrc/bounty) page for more details about our active programs.\n\n## Preferred Languages\n\nWe prefer all communications to be in English.\n\n## Policy\n\nMicrosoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/security.md/cvd).\n\n<!-- END MICROSOFT SECURITY.MD BLOCK -->\n"
        },
        {
          "name": "SUPPORT.md",
          "type": "blob",
          "size": 1.21484375,
          "content": "# TODO: The maintainer of this repo has not yet edited this file\r\n\r\n**REPO OWNER**: Do you want Customer Service & Support (CSS) support for this product/project?\r\n\r\n- **No CSS support:** Fill out this template with information about how to file issues and get help.\r\n- **Yes CSS support:** Fill out an intake form at [aka.ms/onboardsupport](https://aka.ms/onboardsupport). CSS will work with/help you to determine next steps.\r\n- **Not sure?** Fill out an intake as though the answer were \"Yes\". CSS will help you decide.\r\n\r\n*Then remove this first heading from this SUPPORT.MD file before publishing your repo.*\r\n\r\n# Support\r\n\r\n## How to file issues and get help  \r\n\r\nThis project uses GitHub Issues to track bugs and feature requests. Please search the existing \r\nissues before filing new issues to avoid duplicates.  For new issues, file your bug or \r\nfeature request as a new Issue.\r\n\r\nFor help and questions about using this project, please **REPO MAINTAINER: INSERT INSTRUCTIONS HERE \r\nFOR HOW TO ENGAGE REPO OWNERS OR COMMUNITY FOR HELP. COULD BE A STACK OVERFLOW TAG OR OTHER\r\nCHANNEL. WHERE WILL YOU HELP PEOPLE?**.\r\n\r\n## Microsoft Support Policy  \r\n\r\nSupport for this **PROJECT or PRODUCT** is limited to the resources listed above.\r\n"
        },
        {
          "name": "app.py",
          "type": "blob",
          "size": 14.658203125,
          "content": "import gradio as gr\nfrom gradio_litmodel3d import LitModel3D\n\nimport os\nimport shutil\nfrom typing import *\nimport torch\nimport numpy as np\nimport imageio\nfrom easydict import EasyDict as edict\nfrom PIL import Image\nfrom trellis.pipelines import TrellisImageTo3DPipeline\nfrom trellis.representations import Gaussian, MeshExtractResult\nfrom trellis.utils import render_utils, postprocessing_utils\n\n\nMAX_SEED = np.iinfo(np.int32).max\nTMP_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'tmp')\nos.makedirs(TMP_DIR, exist_ok=True)\n\n\ndef start_session(req: gr.Request):\n    user_dir = os.path.join(TMP_DIR, str(req.session_hash))\n    os.makedirs(user_dir, exist_ok=True)\n    \n    \ndef end_session(req: gr.Request):\n    user_dir = os.path.join(TMP_DIR, str(req.session_hash))\n    shutil.rmtree(user_dir)\n\n\ndef preprocess_image(image: Image.Image) -> Image.Image:\n    \"\"\"\n    Preprocess the input image.\n\n    Args:\n        image (Image.Image): The input image.\n\n    Returns:\n        Image.Image: The preprocessed image.\n    \"\"\"\n    processed_image = pipeline.preprocess_image(image)\n    return processed_image\n\n\ndef preprocess_images(images: List[Tuple[Image.Image, str]]) -> List[Image.Image]:\n    \"\"\"\n    Preprocess a list of input images.\n    \n    Args:\n        images (List[Tuple[Image.Image, str]]): The input images.\n        \n    Returns:\n        List[Image.Image]: The preprocessed images.\n    \"\"\"\n    images = [image[0] for image in images]\n    processed_images = [pipeline.preprocess_image(image) for image in images]\n    return processed_images\n\n\ndef pack_state(gs: Gaussian, mesh: MeshExtractResult) -> dict:\n    return {\n        'gaussian': {\n            **gs.init_params,\n            '_xyz': gs._xyz.cpu().numpy(),\n            '_features_dc': gs._features_dc.cpu().numpy(),\n            '_scaling': gs._scaling.cpu().numpy(),\n            '_rotation': gs._rotation.cpu().numpy(),\n            '_opacity': gs._opacity.cpu().numpy(),\n        },\n        'mesh': {\n            'vertices': mesh.vertices.cpu().numpy(),\n            'faces': mesh.faces.cpu().numpy(),\n        },\n    }\n    \n    \ndef unpack_state(state: dict) -> Tuple[Gaussian, edict, str]:\n    gs = Gaussian(\n        aabb=state['gaussian']['aabb'],\n        sh_degree=state['gaussian']['sh_degree'],\n        mininum_kernel_size=state['gaussian']['mininum_kernel_size'],\n        scaling_bias=state['gaussian']['scaling_bias'],\n        opacity_bias=state['gaussian']['opacity_bias'],\n        scaling_activation=state['gaussian']['scaling_activation'],\n    )\n    gs._xyz = torch.tensor(state['gaussian']['_xyz'], device='cuda')\n    gs._features_dc = torch.tensor(state['gaussian']['_features_dc'], device='cuda')\n    gs._scaling = torch.tensor(state['gaussian']['_scaling'], device='cuda')\n    gs._rotation = torch.tensor(state['gaussian']['_rotation'], device='cuda')\n    gs._opacity = torch.tensor(state['gaussian']['_opacity'], device='cuda')\n    \n    mesh = edict(\n        vertices=torch.tensor(state['mesh']['vertices'], device='cuda'),\n        faces=torch.tensor(state['mesh']['faces'], device='cuda'),\n    )\n    \n    return gs, mesh\n\n\ndef get_seed(randomize_seed: bool, seed: int) -> int:\n    \"\"\"\n    Get the random seed.\n    \"\"\"\n    return np.random.randint(0, MAX_SEED) if randomize_seed else seed\n\n\ndef image_to_3d(\n    image: Image.Image,\n    multiimages: List[Tuple[Image.Image, str]],\n    is_multiimage: bool,\n    seed: int,\n    ss_guidance_strength: float,\n    ss_sampling_steps: int,\n    slat_guidance_strength: float,\n    slat_sampling_steps: int,\n    multiimage_algo: Literal[\"multidiffusion\", \"stochastic\"],\n    req: gr.Request,\n) -> Tuple[dict, str]:\n    \"\"\"\n    Convert an image to a 3D model.\n\n    Args:\n        image (Image.Image): The input image.\n        multiimages (List[Tuple[Image.Image, str]]): The input images in multi-image mode.\n        is_multiimage (bool): Whether is in multi-image mode.\n        seed (int): The random seed.\n        ss_guidance_strength (float): The guidance strength for sparse structure generation.\n        ss_sampling_steps (int): The number of sampling steps for sparse structure generation.\n        slat_guidance_strength (float): The guidance strength for structured latent generation.\n        slat_sampling_steps (int): The number of sampling steps for structured latent generation.\n        multiimage_algo (Literal[\"multidiffusion\", \"stochastic\"]): The algorithm for multi-image generation.\n\n    Returns:\n        dict: The information of the generated 3D model.\n        str: The path to the video of the 3D model.\n    \"\"\"\n    user_dir = os.path.join(TMP_DIR, str(req.session_hash))\n    if not is_multiimage:\n        outputs = pipeline.run(\n            image,\n            seed=seed,\n            formats=[\"gaussian\", \"mesh\"],\n            preprocess_image=False,\n            sparse_structure_sampler_params={\n                \"steps\": ss_sampling_steps,\n                \"cfg_strength\": ss_guidance_strength,\n            },\n            slat_sampler_params={\n                \"steps\": slat_sampling_steps,\n                \"cfg_strength\": slat_guidance_strength,\n            },\n        )\n    else:\n        outputs = pipeline.run_multi_image(\n            [image[0] for image in multiimages],\n            seed=seed,\n            formats=[\"gaussian\", \"mesh\"],\n            preprocess_image=False,\n            sparse_structure_sampler_params={\n                \"steps\": ss_sampling_steps,\n                \"cfg_strength\": ss_guidance_strength,\n            },\n            slat_sampler_params={\n                \"steps\": slat_sampling_steps,\n                \"cfg_strength\": slat_guidance_strength,\n            },\n            mode=multiimage_algo,\n        )\n    video = render_utils.render_video(outputs['gaussian'][0], num_frames=120)['color']\n    video_geo = render_utils.render_video(outputs['mesh'][0], num_frames=120)['normal']\n    video = [np.concatenate([video[i], video_geo[i]], axis=1) for i in range(len(video))]\n    video_path = os.path.join(user_dir, 'sample.mp4')\n    imageio.mimsave(video_path, video, fps=15)\n    state = pack_state(outputs['gaussian'][0], outputs['mesh'][0])\n    torch.cuda.empty_cache()\n    return state, video_path\n\n\ndef extract_glb(\n    state: dict,\n    mesh_simplify: float,\n    texture_size: int,\n    req: gr.Request,\n) -> Tuple[str, str]:\n    \"\"\"\n    Extract a GLB file from the 3D model.\n\n    Args:\n        state (dict): The state of the generated 3D model.\n        mesh_simplify (float): The mesh simplification factor.\n        texture_size (int): The texture resolution.\n\n    Returns:\n        str: The path to the extracted GLB file.\n    \"\"\"\n    user_dir = os.path.join(TMP_DIR, str(req.session_hash))\n    gs, mesh = unpack_state(state)\n    glb = postprocessing_utils.to_glb(gs, mesh, simplify=mesh_simplify, texture_size=texture_size, verbose=False)\n    glb_path = os.path.join(user_dir, 'sample.glb')\n    glb.export(glb_path)\n    torch.cuda.empty_cache()\n    return glb_path, glb_path\n\n\ndef extract_gaussian(state: dict, req: gr.Request) -> Tuple[str, str]:\n    \"\"\"\n    Extract a Gaussian file from the 3D model.\n\n    Args:\n        state (dict): The state of the generated 3D model.\n\n    Returns:\n        str: The path to the extracted Gaussian file.\n    \"\"\"\n    user_dir = os.path.join(TMP_DIR, str(req.session_hash))\n    gs, _ = unpack_state(state)\n    gaussian_path = os.path.join(user_dir, 'sample.ply')\n    gs.save_ply(gaussian_path)\n    torch.cuda.empty_cache()\n    return gaussian_path, gaussian_path\n\n\ndef prepare_multi_example() -> List[Image.Image]:\n    multi_case = list(set([i.split('_')[0] for i in os.listdir(\"assets/example_multi_image\")]))\n    images = []\n    for case in multi_case:\n        _images = []\n        for i in range(1, 4):\n            img = Image.open(f'assets/example_multi_image/{case}_{i}.png')\n            W, H = img.size\n            img = img.resize((int(W / H * 512), 512))\n            _images.append(np.array(img))\n        images.append(Image.fromarray(np.concatenate(_images, axis=1)))\n    return images\n\n\ndef split_image(image: Image.Image) -> List[Image.Image]:\n    \"\"\"\n    Split an image into multiple views.\n    \"\"\"\n    image = np.array(image)\n    alpha = image[..., 3]\n    alpha = np.any(alpha>0, axis=0)\n    start_pos = np.where(~alpha[:-1] & alpha[1:])[0].tolist()\n    end_pos = np.where(alpha[:-1] & ~alpha[1:])[0].tolist()\n    images = []\n    for s, e in zip(start_pos, end_pos):\n        images.append(Image.fromarray(image[:, s:e+1]))\n    return [preprocess_image(image) for image in images]\n\n\nwith gr.Blocks(delete_cache=(600, 600)) as demo:\n    gr.Markdown(\"\"\"\n    ## Image to 3D Asset with [TRELLIS](https://trellis3d.github.io/)\n    * Upload an image and click \"Generate\" to create a 3D asset. If the image has alpha channel, it be used as the mask. Otherwise, we use `rembg` to remove the background.\n    * If you find the generated 3D asset satisfactory, click \"Extract GLB\" to extract the GLB file and download it.\n    \"\"\")\n    \n    with gr.Row():\n        with gr.Column():\n            with gr.Tabs() as input_tabs:\n                with gr.Tab(label=\"Single Image\", id=0) as single_image_input_tab:\n                    image_prompt = gr.Image(label=\"Image Prompt\", format=\"png\", image_mode=\"RGBA\", type=\"pil\", height=300)\n                with gr.Tab(label=\"Multiple Images\", id=1) as multiimage_input_tab:\n                    multiimage_prompt = gr.Gallery(label=\"Image Prompt\", format=\"png\", type=\"pil\", height=300, columns=3)\n                    gr.Markdown(\"\"\"\n                        Input different views of the object in separate images. \n                        \n                        *NOTE: this is an experimental algorithm without training a specialized model. It may not produce the best results for all images, especially those having different poses or inconsistent details.*\n                    \"\"\")\n            \n            with gr.Accordion(label=\"Generation Settings\", open=False):\n                seed = gr.Slider(0, MAX_SEED, label=\"Seed\", value=0, step=1)\n                randomize_seed = gr.Checkbox(label=\"Randomize Seed\", value=True)\n                gr.Markdown(\"Stage 1: Sparse Structure Generation\")\n                with gr.Row():\n                    ss_guidance_strength = gr.Slider(0.0, 10.0, label=\"Guidance Strength\", value=7.5, step=0.1)\n                    ss_sampling_steps = gr.Slider(1, 50, label=\"Sampling Steps\", value=12, step=1)\n                gr.Markdown(\"Stage 2: Structured Latent Generation\")\n                with gr.Row():\n                    slat_guidance_strength = gr.Slider(0.0, 10.0, label=\"Guidance Strength\", value=3.0, step=0.1)\n                    slat_sampling_steps = gr.Slider(1, 50, label=\"Sampling Steps\", value=12, step=1)\n                multiimage_algo = gr.Radio([\"stochastic\", \"multidiffusion\"], label=\"Multi-image Algorithm\", value=\"stochastic\")\n\n            generate_btn = gr.Button(\"Generate\")\n            \n            with gr.Accordion(label=\"GLB Extraction Settings\", open=False):\n                mesh_simplify = gr.Slider(0.9, 0.98, label=\"Simplify\", value=0.95, step=0.01)\n                texture_size = gr.Slider(512, 2048, label=\"Texture Size\", value=1024, step=512)\n            \n            with gr.Row():\n                extract_glb_btn = gr.Button(\"Extract GLB\", interactive=False)\n                extract_gs_btn = gr.Button(\"Extract Gaussian\", interactive=False)\n            gr.Markdown(\"\"\"\n                        *NOTE: Gaussian file can be very large (~50MB), it will take a while to display and download.*\n                        \"\"\")\n\n        with gr.Column():\n            video_output = gr.Video(label=\"Generated 3D Asset\", autoplay=True, loop=True, height=300)\n            model_output = LitModel3D(label=\"Extracted GLB/Gaussian\", exposure=10.0, height=300)\n            \n            with gr.Row():\n                download_glb = gr.DownloadButton(label=\"Download GLB\", interactive=False)\n                download_gs = gr.DownloadButton(label=\"Download Gaussian\", interactive=False)  \n    \n    is_multiimage = gr.State(False)\n    output_buf = gr.State()\n\n    # Example images at the bottom of the page\n    with gr.Row() as single_image_example:\n        examples = gr.Examples(\n            examples=[\n                f'assets/example_image/{image}'\n                for image in os.listdir(\"assets/example_image\")\n            ],\n            inputs=[image_prompt],\n            fn=preprocess_image,\n            outputs=[image_prompt],\n            run_on_click=True,\n            examples_per_page=64,\n        )\n    with gr.Row(visible=False) as multiimage_example:\n        examples_multi = gr.Examples(\n            examples=prepare_multi_example(),\n            inputs=[image_prompt],\n            fn=split_image,\n            outputs=[multiimage_prompt],\n            run_on_click=True,\n            examples_per_page=8,\n        )\n\n    # Handlers\n    demo.load(start_session)\n    demo.unload(end_session)\n    \n    single_image_input_tab.select(\n        lambda: tuple([False, gr.Row.update(visible=True), gr.Row.update(visible=False)]),\n        outputs=[is_multiimage, single_image_example, multiimage_example]\n    )\n    multiimage_input_tab.select(\n        lambda: tuple([True, gr.Row.update(visible=False), gr.Row.update(visible=True)]),\n        outputs=[is_multiimage, single_image_example, multiimage_example]\n    )\n    \n    image_prompt.upload(\n        preprocess_image,\n        inputs=[image_prompt],\n        outputs=[image_prompt],\n    )\n    multiimage_prompt.upload(\n        preprocess_images,\n        inputs=[multiimage_prompt],\n        outputs=[multiimage_prompt],\n    )\n\n    generate_btn.click(\n        get_seed,\n        inputs=[randomize_seed, seed],\n        outputs=[seed],\n    ).then(\n        image_to_3d,\n        inputs=[image_prompt, multiimage_prompt, is_multiimage, seed, ss_guidance_strength, ss_sampling_steps, slat_guidance_strength, slat_sampling_steps, multiimage_algo],\n        outputs=[output_buf, video_output],\n    ).then(\n        lambda: tuple([gr.Button(interactive=True), gr.Button(interactive=True)]),\n        outputs=[extract_glb_btn, extract_gs_btn],\n    )\n\n    video_output.clear(\n        lambda: tuple([gr.Button(interactive=False), gr.Button(interactive=False)]),\n        outputs=[extract_glb_btn, extract_gs_btn],\n    )\n\n    extract_glb_btn.click(\n        extract_glb,\n        inputs=[output_buf, mesh_simplify, texture_size],\n        outputs=[model_output, download_glb],\n    ).then(\n        lambda: gr.Button(interactive=True),\n        outputs=[download_glb],\n    )\n    \n    extract_gs_btn.click(\n        extract_gaussian,\n        inputs=[output_buf],\n        outputs=[model_output, download_gs],\n    ).then(\n        lambda: gr.Button(interactive=True),\n        outputs=[download_gs],\n    )\n\n    model_output.clear(\n        lambda: gr.Button(interactive=False),\n        outputs=[download_glb],\n    )\n    \n\n# Launch the Gradio app\nif __name__ == \"__main__\":\n    pipeline = TrellisImageTo3DPipeline.from_pretrained(\"JeffreyXiang/TRELLIS-image-large\")\n    pipeline.cuda()\n    demo.launch()\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset_toolkits",
          "type": "tree",
          "content": null
        },
        {
          "name": "example.py",
          "type": "blob",
          "size": 2.0517578125,
          "content": "import os\n# os.environ['ATTN_BACKEND'] = 'xformers'   # Can be 'flash-attn' or 'xformers', default is 'flash-attn'\nos.environ['SPCONV_ALGO'] = 'native'        # Can be 'native' or 'auto', default is 'auto'.\n                                            # 'auto' is faster but will do benchmarking at the beginning.\n                                            # Recommended to set to 'native' if run only once.\n\nimport imageio\nfrom PIL import Image\nfrom trellis.pipelines import TrellisImageTo3DPipeline\nfrom trellis.utils import render_utils, postprocessing_utils\n\n# Load a pipeline from a model folder or a Hugging Face model hub.\npipeline = TrellisImageTo3DPipeline.from_pretrained(\"JeffreyXiang/TRELLIS-image-large\")\npipeline.cuda()\n\n# Load an image\nimage = Image.open(\"assets/example_image/T.png\")\n\n# Run the pipeline\noutputs = pipeline.run(\n    image,\n    seed=1,\n    # Optional parameters\n    # sparse_structure_sampler_params={\n    #     \"steps\": 12,\n    #     \"cfg_strength\": 7.5,\n    # },\n    # slat_sampler_params={\n    #     \"steps\": 12,\n    #     \"cfg_strength\": 3,\n    # },\n)\n# outputs is a dictionary containing generated 3D assets in different formats:\n# - outputs['gaussian']: a list of 3D Gaussians\n# - outputs['radiance_field']: a list of radiance fields\n# - outputs['mesh']: a list of meshes\n\n# Render the outputs\nvideo = render_utils.render_video(outputs['gaussian'][0])['color']\nimageio.mimsave(\"sample_gs.mp4\", video, fps=30)\nvideo = render_utils.render_video(outputs['radiance_field'][0])['color']\nimageio.mimsave(\"sample_rf.mp4\", video, fps=30)\nvideo = render_utils.render_video(outputs['mesh'][0])['normal']\nimageio.mimsave(\"sample_mesh.mp4\", video, fps=30)\n\n# GLB files can be extracted from the outputs\nglb = postprocessing_utils.to_glb(\n    outputs['gaussian'][0],\n    outputs['mesh'][0],\n    # Optional parameters\n    simplify=0.95,          # Ratio of triangles to remove in the simplification process\n    texture_size=1024,      # Size of the texture used for the GLB\n)\nglb.export(\"sample.glb\")\n\n# Save Gaussians as PLY files\noutputs['gaussian'][0].save_ply(\"sample.ply\")\n"
        },
        {
          "name": "example_multi_image.py",
          "type": "blob",
          "size": 1.7138671875,
          "content": "import os\n# os.environ['ATTN_BACKEND'] = 'xformers'   # Can be 'flash-attn' or 'xformers', default is 'flash-attn'\nos.environ['SPCONV_ALGO'] = 'native'        # Can be 'native' or 'auto', default is 'auto'.\n                                            # 'auto' is faster but will do benchmarking at the beginning.\n                                            # Recommended to set to 'native' if run only once.\n\nimport numpy as np\nimport imageio\nfrom PIL import Image\nfrom trellis.pipelines import TrellisImageTo3DPipeline\nfrom trellis.utils import render_utils\n\n# Load a pipeline from a model folder or a Hugging Face model hub.\npipeline = TrellisImageTo3DPipeline.from_pretrained(\"JeffreyXiang/TRELLIS-image-large\")\npipeline.cuda()\n\n# Load an image\nimages = [\n    Image.open(\"assets/example_multi_image/character_1.png\"),\n    Image.open(\"assets/example_multi_image/character_2.png\"),\n    Image.open(\"assets/example_multi_image/character_3.png\"),\n]\n\n# Run the pipeline\noutputs = pipeline.run_multi_image(\n    images,\n    seed=1,\n    # Optional parameters\n    sparse_structure_sampler_params={\n        \"steps\": 12,\n        \"cfg_strength\": 7.5,\n    },\n    slat_sampler_params={\n        \"steps\": 12,\n        \"cfg_strength\": 3,\n    },\n)\n# outputs is a dictionary containing generated 3D assets in different formats:\n# - outputs['gaussian']: a list of 3D Gaussians\n# - outputs['radiance_field']: a list of radiance fields\n# - outputs['mesh']: a list of meshes\n\nvideo_gs = render_utils.render_video(outputs['gaussian'][0])['color']\nvideo_mesh = render_utils.render_video(outputs['mesh'][0])['normal']\nvideo = [np.concatenate([frame_gs, frame_mesh], axis=1) for frame_gs, frame_mesh in zip(video_gs, video_mesh)]\nimageio.mimsave(\"sample_multi.mp4\", video, fps=30)\n"
        },
        {
          "name": "extensions",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.sh",
          "type": "blob",
          "size": 11.0166015625,
          "content": "# Read Arguments\nTEMP=`getopt -o h --long help,new-env,basic,xformers,flash-attn,diffoctreerast,vox2seq,spconv,mipgaussian,kaolin,nvdiffrast,demo -n 'setup.sh' -- \"$@\"`\n\neval set -- \"$TEMP\"\n\nHELP=false\nNEW_ENV=false\nBASIC=false\nXFORMERS=false\nFLASHATTN=false\nDIFFOCTREERAST=false\nVOX2SEQ=false\nLINEAR_ASSIGNMENT=false\nSPCONV=false\nERROR=false\nMIPGAUSSIAN=false\nKAOLIN=false\nNVDIFFRAST=false\nDEMO=false\n\nif [ \"$#\" -eq 1 ] ; then\n    HELP=true\nfi\n\nwhile true ; do\n    case \"$1\" in\n        -h|--help) HELP=true ; shift ;;\n        --new-env) NEW_ENV=true ; shift ;;\n        --basic) BASIC=true ; shift ;;\n        --xformers) XFORMERS=true ; shift ;;\n        --flash-attn) FLASHATTN=true ; shift ;;\n        --diffoctreerast) DIFFOCTREERAST=true ; shift ;;\n        --vox2seq) VOX2SEQ=true ; shift ;;\n        --spconv) SPCONV=true ; shift ;;\n        --mipgaussian) MIPGAUSSIAN=true ; shift ;;\n        --kaolin) KAOLIN=true ; shift ;;\n        --nvdiffrast) NVDIFFRAST=true ; shift ;;\n        --demo) DEMO=true ; shift ;;\n        --) shift ; break ;;\n        *) ERROR=true ; break ;;\n    esac\ndone\n\nif [ \"$ERROR\" = true ] ; then\n    echo \"Error: Invalid argument\"\n    HELP=true\nfi\n\nif [ \"$HELP\" = true ] ; then\n    echo \"Usage: setup.sh [OPTIONS]\"\n    echo \"Options:\"\n    echo \"  -h, --help              Display this help message\"\n    echo \"  --new-env               Create a new conda environment\"\n    echo \"  --basic                 Install basic dependencies\"\n    echo \"  --xformers              Install xformers\"\n    echo \"  --flash-attn            Install flash-attn\"\n    echo \"  --diffoctreerast        Install diffoctreerast\"\n    echo \"  --vox2seq               Install vox2seq\"\n    echo \"  --spconv                Install spconv\"\n    echo \"  --mipgaussian           Install mip-splatting\"\n    echo \"  --kaolin                Install kaolin\"\n    echo \"  --nvdiffrast            Install nvdiffrast\"\n    echo \"  --demo                  Install all dependencies for demo\"\n    return\nfi\n\nif [ \"$NEW_ENV\" = true ] ; then\n    conda create -n trellis python=3.10\n    conda activate trellis\n    conda install pytorch==2.4.0 torchvision==0.19.0 pytorch-cuda=11.8 -c pytorch -c nvidia\nfi\n\n# Get system information\nWORKDIR=$(pwd)\nPYTORCH_VERSION=$(python -c \"import torch; print(torch.__version__)\")\nPLATFORM=$(python -c \"import torch; print(('cuda' if torch.version.cuda else ('hip' if torch.version.hip else 'unknown')) if torch.cuda.is_available() else 'cpu')\")\ncase $PLATFORM in\n    cuda)\n        CUDA_VERSION=$(python -c \"import torch; print(torch.version.cuda)\")\n        CUDA_MAJOR_VERSION=$(echo $CUDA_VERSION | cut -d'.' -f1)\n        CUDA_MINOR_VERSION=$(echo $CUDA_VERSION | cut -d'.' -f2)\n        echo \"[SYSTEM] PyTorch Version: $PYTORCH_VERSION, CUDA Version: $CUDA_VERSION\"\n        ;;\n    hip)\n        HIP_VERSION=$(python -c \"import torch; print(torch.version.hip)\")\n        HIP_MAJOR_VERSION=$(echo $HIP_VERSION | cut -d'.' -f1)\n        HIP_MINOR_VERSION=$(echo $HIP_VERSION | cut -d'.' -f2)\n        # Install pytorch 2.4.1 for hip\n        if [ \"$PYTORCH_VERSION\" != \"2.4.1+rocm6.1\" ] ; then\n        echo \"[SYSTEM] Installing PyTorch 2.4.1 for HIP ($PYTORCH_VERSION -> 2.4.1+rocm6.1)\"\n            pip install torch==2.4.1 torchvision==0.19.1 --index-url https://download.pytorch.org/whl/rocm6.1 --user\n            mkdir -p /tmp/extensions\n            sudo cp /opt/rocm/share/amd_smi /tmp/extensions/amd_smi -r\n            cd /tmp/extensions/amd_smi\n            sudo chmod -R 777 .\n            pip install .\n            cd $WORKDIR\n            PYTORCH_VERSION=$(python -c \"import torch; print(torch.__version__)\")\n        fi\n        echo \"[SYSTEM] PyTorch Version: $PYTORCH_VERSION, HIP Version: $HIP_VERSION\"\n        ;;\n    *)\n        ;;\nesac\n\nif [ \"$BASIC\" = true ] ; then\n    pip install pillow imageio imageio-ffmpeg tqdm easydict opencv-python-headless scipy ninja rembg onnxruntime trimesh xatlas pyvista pymeshfix igraph transformers\n    pip install git+https://github.com/EasternJournalist/utils3d.git@9a4eb15e4021b67b12c460c7057d642626897ec8\nfi\n\nif [ \"$XFORMERS\" = true ] ; then\n    # install xformers\n    if [ \"$PLATFORM\" = \"cuda\" ] ; then\n        if [ \"$CUDA_VERSION\" = \"11.8\" ] ; then\n            case $PYTORCH_VERSION in\n                2.0.1) pip install https://files.pythonhosted.org/packages/52/ca/82aeee5dcc24a3429ff5de65cc58ae9695f90f49fbba71755e7fab69a706/xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl ;;\n                2.1.0) pip install xformers==0.0.22.post7 --index-url https://download.pytorch.org/whl/cu118 ;;\n                2.1.1) pip install xformers==0.0.23 --index-url https://download.pytorch.org/whl/cu118 ;;\n                2.1.2) pip install xformers==0.0.23.post1 --index-url https://download.pytorch.org/whl/cu118 ;;\n                2.2.0) pip install xformers==0.0.24 --index-url https://download.pytorch.org/whl/cu118 ;;\n                2.2.1) pip install xformers==0.0.25 --index-url https://download.pytorch.org/whl/cu118 ;;\n                2.2.2) pip install xformers==0.0.25.post1 --index-url https://download.pytorch.org/whl/cu118 ;;\n                2.3.0) pip install xformers==0.0.26.post1 --index-url https://download.pytorch.org/whl/cu118 ;;\n                2.4.0) pip install xformers==0.0.27.post2 --index-url https://download.pytorch.org/whl/cu118 ;;\n                2.4.1) pip install xformers==0.0.28 --index-url https://download.pytorch.org/whl/cu118 ;;\n                2.5.0) pip install xformers==0.0.28.post2 --index-url https://download.pytorch.org/whl/cu118 ;;\n                *) echo \"[XFORMERS] Unsupported PyTorch & CUDA version: $PYTORCH_VERSION & $CUDA_VERSION\" ;;\n            esac\n        elif [ \"$CUDA_VERSION\" = \"12.1\" ] ; then\n            case $PYTORCH_VERSION in\n                2.1.0) pip install xformers==0.0.22.post7 --index-url https://download.pytorch.org/whl/cu121 ;;\n                2.1.1) pip install xformers==0.0.23 --index-url https://download.pytorch.org/whl/cu121 ;;\n                2.1.2) pip install xformers==0.0.23.post1 --index-url https://download.pytorch.org/whl/cu121 ;;\n                2.2.0) pip install xformers==0.0.24 --index-url https://download.pytorch.org/whl/cu121 ;;\n                2.2.1) pip install xformers==0.0.25 --index-url https://download.pytorch.org/whl/cu121 ;;\n                2.2.2) pip install xformers==0.0.25.post1 --index-url https://download.pytorch.org/whl/cu121 ;;\n                2.3.0) pip install xformers==0.0.26.post1 --index-url https://download.pytorch.org/whl/cu121 ;;\n                2.4.0) pip install xformers==0.0.27.post2 --index-url https://download.pytorch.org/whl/cu121 ;;\n                2.4.1) pip install xformers==0.0.28 --index-url https://download.pytorch.org/whl/cu121 ;;\n                2.5.0) pip install xformers==0.0.28.post2 --index-url https://download.pytorch.org/whl/cu121 ;;\n                *) echo \"[XFORMERS] Unsupported PyTorch & CUDA version: $PYTORCH_VERSION & $CUDA_VERSION\" ;;\n            esac\n        elif [ \"$CUDA_VERSION\" = \"12.4\" ] ; then\n            case $PYTORCH_VERSION in\n                2.5.0) pip install xformers==0.0.28.post2 --index-url https://download.pytorch.org/whl/cu124 ;;\n                *) echo \"[XFORMERS] Unsupported PyTorch & CUDA version: $PYTORCH_VERSION & $CUDA_VERSION\" ;;\n            esac\n        else\n            echo \"[XFORMERS] Unsupported CUDA version: $CUDA_MAJOR_VERSION\"\n        fi\n    elif [ \"$PLATFORM\" = \"hip\" ] ; then\n        case $PYTORCH_VERSION in\n            2.4.1\\+rocm6.1) pip install xformers==0.0.28 --index-url https://download.pytorch.org/whl/rocm6.1 ;;\n            *) echo \"[XFORMERS] Unsupported PyTorch version: $PYTORCH_VERSION\" ;;\n        esac\n    else\n        echo \"[XFORMERS] Unsupported platform: $PLATFORM\"\n    fi\nfi\n\nif [ \"$FLASHATTN\" = true ] ; then\n    if [ \"$PLATFORM\" = \"cuda\" ] ; then\n        pip install flash-attn\n    elif [ \"$PLATFORM\" = \"hip\" ] ; then\n        echo \"[FLASHATTN] Prebuilt binaries not found. Building from source...\"\n        mkdir -p /tmp/extensions\n        git clone --recursive https://github.com/ROCm/flash-attention.git /tmp/extensions/flash-attention\n        cd /tmp/extensions/flash-attention\n        git checkout tags/v2.6.3-cktile\n        GPU_ARCHS=gfx942 python setup.py install #MI300 series\n        cd $WORKDIR\n    else\n        echo \"[FLASHATTN] Unsupported platform: $PLATFORM\"\n    fi\nfi\n\nif [ \"$KAOLIN\" = true ] ; then\n    # install kaolin\n    if [ \"$PLATFORM\" = \"cuda\" ] ; then\n        case $PYTORCH_VERSION in\n            2.0.1) pip install kaolin -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.0.1_cu118.html;;\n            2.1.0) pip install kaolin -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.1.0_cu118.html;;\n            2.1.1) pip install kaolin -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.1.1_cu118.html;;\n            2.2.0) pip install kaolin -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.2.0_cu118.html;;\n            2.2.1) pip install kaolin -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.2.1_cu118.html;;\n            2.2.2) pip install kaolin -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.2.2_cu118.html;;\n            2.4.0) pip install kaolin -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.4.0_cu121.html;;\n            *) echo \"[KAOLIN] Unsupported PyTorch version: $PYTORCH_VERSION\" ;;\n        esac\n    else\n        echo \"[KAOLIN] Unsupported platform: $PLATFORM\"\n    fi\nfi\n\nif [ \"$NVDIFFRAST\" = true ] ; then\n    if [ \"$PLATFORM\" = \"cuda\" ] ; then\n        mkdir -p /tmp/extensions\n        git clone https://github.com/NVlabs/nvdiffrast.git /tmp/extensions/nvdiffrast\n        pip install /tmp/extensions/nvdiffrast\n    else\n        echo \"[NVDIFFRAST] Unsupported platform: $PLATFORM\"\n    fi\nfi\n\nif [ \"$DIFFOCTREERAST\" = true ] ; then\n    if [ \"$PLATFORM\" = \"cuda\" ] ; then\n        mkdir -p /tmp/extensions\n        git clone --recurse-submodules https://github.com/JeffreyXiang/diffoctreerast.git /tmp/extensions/diffoctreerast\n        pip install /tmp/extensions/diffoctreerast\n    else\n        echo \"[DIFFOCTREERAST] Unsupported platform: $PLATFORM\"\n    fi\nfi\n\nif [ \"$MIPGAUSSIAN\" = true ] ; then\n    if [ \"$PLATFORM\" = \"cuda\" ] ; then\n        mkdir -p /tmp/extensions\n        git clone https://github.com/autonomousvision/mip-splatting.git /tmp/extensions/mip-splatting\n        pip install /tmp/extensions/mip-splatting/submodules/diff-gaussian-rasterization/\n    else\n        echo \"[MIPGAUSSIAN] Unsupported platform: $PLATFORM\"\n    fi\nfi\n\nif [ \"$VOX2SEQ\" = true ] ; then\n    if [ \"$PLATFORM\" = \"cuda\" ] ; then\n        mkdir -p /tmp/extensions\n        cp -r extensions/vox2seq /tmp/extensions/vox2seq\n        pip install /tmp/extensions/vox2seq\n    else\n        echo \"[VOX2SEQ] Unsupported platform: $PLATFORM\"\n    fi\nfi\n\nif [ \"$SPCONV\" = true ] ; then\n    # install spconv\n    if [ \"$PLATFORM\" = \"cuda\" ] ; then\n        case $CUDA_MAJOR_VERSION in\n            11) pip install spconv-cu118 ;;\n            12) pip install spconv-cu120 ;;\n            *) echo \"[SPCONV] Unsupported PyTorch CUDA version: $CUDA_MAJOR_VERSION\" ;;\n        esac\n    else\n        echo \"[SPCONV] Unsupported platform: $PLATFORM\"\n    fi\nfi\n\nif [ \"$DEMO\" = true ] ; then\n    pip install gradio==4.44.1 gradio_litmodel3d==0.0.1\nfi\n"
        },
        {
          "name": "trellis",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}