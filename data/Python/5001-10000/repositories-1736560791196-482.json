{
  "metadata": {
    "timestamp": 1736560791196,
    "page": 482,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "bitsandbytes-foundation/bitsandbytes",
      "stars": 6495,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.0625,
          "content": "[*]\ntrim_trailing_whitespace = true\ninsert_final_newline = true\n"
        },
        {
          "name": ".git-blame-ignore-revs",
          "type": "blob",
          "size": 0.5087890625,
          "content": "# ran black and isort for coherent code formatting\nbfa0e33294f2b1dc25e65a33be2397f989824298\n\n# reran black with linelength 80 for greater readability\nea7c14f8ef64924f2d0ff80df3cdabf2c7299848\n\n# Remove f-prefix from strings that don't use formatting\n7727fa4c8c6c1ef2b109120aff4196a0a6bf3ed6\n\n# format tests/linear_4bit.py\n34735ba89de8235ea9da6ef409f814dcea9e2038\n\n# Reformat with ruff-format\n5a4263f4dc05fe8f78f4111beab9f68a81deeab1\n\n# CHANGELOG: to reverse chron order + mdformat\n4743ff0d43e04e4cc3e5d8b9e7cd016c0defa36d\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.0263671875,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n*.dll\n*.dylib\n*.o\n*.obj\n*.air\n*.metallib\n\n# CMake generated files\nCMakeCache.txt\nCMakeScripts/\ncmake_install.cmake\nMakefile\nCMakeFiles/\n*.sln\n*.vcxproj*\n*.xcodeproj/\nbitsandbytes.dir/\nDebug/\nRelease/\ncmake-build-*/\n\n# IDE local files\n.vs/\n.idea/\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# vim\n*.swp\n\ndependencies\ncuda_build\noutput/\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.5400390625,
          "content": "repos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.6.9\n    hooks:\n      - id: ruff\n        args:\n          - --fix\n      - id: ruff-format\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v5.0.0\n    hooks:\n      - id: check-merge-conflict\n      - id: check-yaml\n      - id: end-of-file-fixer\n      - id: fix-byte-order-marker\n      - id: trailing-whitespace\n      - id: mixed-line-ending\n        args:\n          - --fix=lf\n  - repo: https://github.com/crate-ci/typos\n    rev: v1.26.0\n    hooks:\n      - id: typos\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 21.1904296875,
          "content": "### 0.43.3\n\n#### Improvements:\n\n- FSDP: Enable loading prequantized weights with bf16/fp16/fp32 quant_storage\n    - Background: This update, linked to [Transformer PR #32276](https://github.com/huggingface/transformers/pull/32276), allows loading prequantized weights with alternative storage formats. Metadata is tracked similarly to `Params4bit.__new__` post PR #970. It supports models exported with non-default `quant_storage`, such as [this NF4 model with BF16 storage](https://huggingface.co/hugging-quants/Meta-Llama-3.1-405B-BNB-NF4-BF16).\n    - Special thanks to @winglian and @matthewdouglas for enabling FSDP+QLoRA finetuning of Llama 3.1 405B on a single 8xH100 or 8xA100 node with as little as 256GB system RAM.\n\n\n### 0.43.2\n\nThis release is quite significant as the QLoRA bug fix big implications for higher `seqlen` and batch sizes.\n\nFor each sequence (i.e. batch size increase of one) we expect memory savings of:\n- 405B: 39GB for `seqlen=1024`, and 4888GB for `seqlen=128,00`\n- 70B: 10.1GB for `seqlen=1024` and  1258GB for `seqlen=128,00`\n\nThis was due to activations being unnecessary for frozen parameters, yet the memory for them was still erroneously allocated due to the now fixed bug.\n\n#### Improvements:\n\n- docs: FSDP+QLoRA and CPU install guide (#1211 #1227, thanks @stevhliu)\n- Add CUDA 12.5 and update 12.4 builds (#1284)\n\n#### Bug Fixes\n\n- 4bit getstate and 8bit deepcopy (#1230 #1231, thanks @BenjaminBossan)\n- missing optimizers in `str2optimizer32bit` (#1222, thanks @EtienneDosSantos)\n- CUDA 12.5 build issue (#1273, thanks @HennerM)\n- fix for min_8bit_size functionality in Optimizer base classes (#1286, thanks @Edenzzzz)\n- QLoRA mem bug (#1270, thanks @Ther-nullptr)\n- tests for cpu only platforms (#1259, thanks @galqiwi)\n- restoration of quant_storage for CPU offloading (#1279)\n- optim update error with non-contiguous grads/params (deepspeed) (#1187)\n\n### 0.43.1\n\n#### Improvements:\n\n- Improved the serialization format for 8-bit weights; this change is fully backwards compatible. (#1164, thanks to @younesbelkada for the contributions and @akx for the review).\n- Added CUDA 12.4 support to the Linux x86-64 build workflow, expanding the library's compatibility with the latest CUDA versions. (#1171, kudos to @matthewdouglas for this addition).\n- Docs enhancement: Improved the instructions for installing the library from source. (#1149, special thanks to @stevhliu for the enhancements).\n\n#### Bug Fixes\n\n- Fix 4bit quantization with blocksize = 4096, where an illegal memory access was encountered. (#1160, thanks @matthewdouglas for fixing and @YLGH for reporting)\n\n#### Internal Improvements:\n\n- Tests: improve memory usage (#1147, thanks @matthewdouglas)\n- Add CUDA 12.4 to docs/install helper (#1136, thanks @matthewdouglas)\n- Minor type/doc fixes (#1128, thanks @akx)\n- Reformat Python code with Ruff (#1081, thanks @akx)\n- Rework of CUDA/native-library setup and diagnostics (#1041, thanks @akx)\n\n### 0.43.0\n\n#### Improvements and New Features:\n\n- QLoRA + FSDP official support is now live! https://github.com/TimDettmers/bitsandbytes/pull/970 by @warner-benjamin and team - with FSDP you can train very large models (70b scale) on multiple 24GB consumer-type GPUs. See https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html for more details.\n- Introduced improvements to the CI process for enhanced performance and efficiency during builds, specifically enabling more effective cross-compilation on Linux platforms. This was accomplished by deprecating Make and migrating to Cmake, as well as implementing new corresponding workflows. Huge thanks go to @wkpark, @rickardp, @matthewdouglas and @younesbelkada; #1055, #1050, #1111.\n- Windows should be officially supported in bitsandbytes if you install the library from source. See: https://huggingface.co/docs/bitsandbytes/main/en/index for more details\n- Updated installation instructions to provide more comprehensive guidance for users. This includes clearer explanations and additional tips for various setup scenarios, making the library more accessible to a broader audience (@rickardp, #1047).\n- Enhanced the library's compatibility and setup process, including fixes for CPU-only installations and improvements in CUDA setup error messaging. This effort aims to streamline the installation process and improve user experience across different platforms and setups (@wkpark, @akx, #1038, #996, #1012).\n- Setup a new documentation at https://huggingface.co/docs/bitsandbytes/main with extensive new sections and content to help users better understand and utilize the library. Especially notable are the new API docs. (big thanks to @stevhliu and @mishig25 from HuggingFace #1012). The API docs have been also addressed in #1075.\n\n#### Bug Fixes:\n\n- Addressed a race condition in kEstimateQuantiles, enhancing the reliability of quantile estimation in concurrent environments (@pnunna93, #1061).\n- Fixed various minor issues, including typos in code comments and documentation, to improve code clarity and prevent potential confusion (@Brian Vaughan, #1063).\n\n#### Backwards Compatibility\n\n- After upgrading from `v0.42` to `v0.43`, when using 4bit quantization, models may generate slightly different outputs (approximately up to the 2nd decimal place) due to a fix in the code. For anyone interested in the details, [see this comment](https://github.com/TimDettmers/bitsandbytes/discussions/1094#discussioncomment-8984069).\n\n#### Internal and Build System Enhancements:\n\n- Implemented several enhancements to the internal and build systems, including adjustments to the CI workflows, portability improvements, and build artifact management. These changes contribute to a more robust and flexible development process, ensuring the library's ongoing quality and maintainability (@rickardp, @akx, @wkpark, @matthewdouglas; #949, #1053, #1045, #1037).\n\n#### Contributors:\n\nThis release is made possible thanks to the many active contributors that submitted PRs and many others who contributed to discussions, reviews, and testing. Your efforts greatly enhance the library's quality and user experience. It's truly inspiring to work with such a dedicated and competent group of volunteers and professionals!\n\nWe give a special thanks to @TimDettmers for managing to find a little bit of time for valuable consultations on critical topics, despite preparing for and touring the states applying for professor positions. We wish him the utmost success!\n\nWe also extend our gratitude to the broader community for your continued support, feedback, and engagement, which play a crucial role in driving the library's development forward.\n\n### 0.42.0\n\nFeatures:\n\n- 4-bit serialization now supported. This enables 4-bit load/store. Thank you @poedator #753\n- the bitsandbytes library now has a version attribute: `bitsandbytes.__version__` @rasbt #710\n\nBug fixes:\n\n- Fixed bugs in dynamic exponent data type creation. Thank you @RossM, @KohakuBlueleaf, @ArrowM #659 #227 #262 #152\n- Fixed an issue where 4-bit serialization would fail for layers without double quantization #868. Thank you, @poedator\n- Fixed an issue where calling .to() or .cuda() on a 4-bit layer twice would result in an error #867. Thank you, @jph00\n- Fixed a bug where a missing access permission in a path searched for CUDA would lead to an error @osma #677\n- Fixed a bug where the GOOGLE_VM_CONFIG_LOCK_FILE variable could cause errors in colab environments @akrentsel @xaptronic #715 #883 #622\n- Fixed a bug where kgetColRowStats (LLM.int8()) would fail for certain dimensions @LucQueen @905\n- Fixed a bug where the adjusted regular Embedding layer was not available via bnb.nn.Embedding @neel04 #563\n- Fixed added missing scipy requirement @dulalbert #525\n\n### 0.41.3\n\nBug fixes:\n\n- Fixed an issue where 4-bit serialization would fail for layers without double quantization #868. Thank you, @poedator\n- Fixed an issue where calling .to() or .cuda() on a 4-bit layer twice would result in an error #867. Thank you, @jph00\n\n### 0.41.2\n\nFeature:\n\n- 4-bit serialization now supported. This enables 4-bit load/store. Thank you @poedator #753\n\n### 0.41.1\n\nBug fixes:\n\n- Fixed bugs in dynamic exponent data type creation. Thank you @RossM, @KohakuBlueleaf, @ArrowM #659 #227 #262 #152\n\n### 0.41.0\n\nFeatures:\n\n- Added precompiled CUDA 11.8 binaries to support H100 GPUs without compilation #571\n- CUDA SETUP now no longer looks for libcuda and libcudart and relies PyTorch CUDA libraries. To manually override this behavior see: how_to_use_nonpytorch_cuda.md. Thank you @rapsealk\n\nBug fixes:\n\n- Fixed a bug where the default type of absmax was undefined which leads to errors if the default type is different than torch.float32. # 553\n- Fixed a missing scipy dependency in requirements.txt. #544\n- Fixed a bug, where a view operation could cause an error in 8-bit layers.\n- Fixed a bug where CPU bitsandbytes would during the import. #593 Thank you @bilelomrani\n- Fixed a but where a non-existent LD_LIBRARY_PATH variable led to a failure in python -m bitsandbytes #588\n- Removed outdated get_cuda_lib_handle calls that lead to errors. #595 Thank you @ihsanturk\n- Fixed bug where read-permission was assumed for a file. #497\n- Fixed a bug where prefetchAsync lead to errors on GPUs that do not support unified memory but not prefetching (Maxwell, SM52). #470 #451 #453 #477 Thank you @jllllll and @stoperro\n\nDocumentation:\n\n- Improved documentation for GPUs that do not support 8-bit matmul. #529\n- Added description and pointers for the NF4 data type. #543\n\nUser experience:\n\n- Improved handling of default compute_dtype for Linear4bit Layers, so that compute_dtype = input_dtype if the input data type is stable enough (float32, bfloat16, but not float16).\n\nPerformance:\n\n- improved 4-bit inference performance for A100 GPUs. This degraded performance for A40/RTX3090 and RTX 4090 GPUs slightly.\n\n### 0.40.2\n\nBug fixes:\n\n- Fixed a but where a non-existent LD_LIBRARY_PATH variable led to a failure in python -m bitsandbytes #588\n- Removed outdated get_cuda_lib_handle calls that lead to errors. #595 Thank you @ihsanturk\n- Fixed bug where read-permission was assumed for a file. #497\n- Fixed a bug where prefetchAsync lead to errors on GPUs that do not support unified memory but not prefetching (Maxwell, SM52). #470 #451 #453 #477 Thank you @jllllll and @stoperro\n\n### 0.40.1\n\nFeatures:\n\n- Added precompiled CUDA 11.8 binaries to support H100 GPUs without compilation #571\n- CUDA SETUP now no longer looks for libcuda and libcudart and relies PyTorch CUDA libraries. To manually override this behavior see: how_to_use_nonpytorch_cuda.md. Thank you @rapsealk\n\nBug fixes:\n\n- Fixed a bug where the default type of absmax was undefined which leads to errors if the default type is different than torch.float32. # 553\n- Fixed a missing scipy dependency in requirements.txt. #544\n- Fixed a bug, where a view operation could cause an error in 8-bit layers.\n- Fixed a bug where CPU bitsandbytes would during the import. #593 Thank you @bilelomrani\n\nDocumentation:\n\n- Improved documentation for GPUs that do not support 8-bit matmul. #529\n- Added description and pointers for the NF4 data type. #543\n\n### 0.40.0\n\nFeatures:\n\n- Added 4-bit inference kernels for batch size=1. Currently support are the NF4, FP4 data types.\n- Added support for quantizations of bfloat16 input data.\n\nBug fixes:\n\n- Added `device` variable for bitsandbytes layers to be compatible with PyTorch layers.\n\nDeprecated:\n\n- Binaries for CUDA 11.2, 11.6 no longer ship with `pip install bitsandbytes` and need to be compiled from source.\n\n### 0.39.0\n\nFeatures:\n\n- 4-bit matrix multiplication for Float4 and NormalFloat4 data types.\n- Added 4-bit quantization routines\n- Doubled quantization routines for 4-bit quantization\n- Paged optimizers for Adam and Lion.\n- bfloat16 gradient / weight support for Adam and Lion with 8 or 32-bit states.\n\nBug fixes:\n\n- Fixed a bug where 8-bit models consumed twice the memory as expected after serialization\n\nDeprecated:\n\n- Kepler binaries (GTX 700s and Tesla K40/K80) are not longer provided via pip and need to be compiled from source. Kepler support might be fully removed in the future.\n\n### 0.38.1\n\nFeatures:\n\n- Added Int8 SwitchBack layers\n- Added Fake FP8 layers for research purposes (available under `bnb.research.nn. ...`)\n\n### 0.38.0\n\n#### 8-bit Lion, Load/Store 8-bit Models directly from/to HF Hub\n\nFeatures:\n\n- Support for 32 and 8-bit Lion has been added. Thank you @lucidrains\n- Support for serialization of Linear8bitLt layers (LLM.int8()). This allows to store and load 8-bit weights directly from the HuggingFace Hub. Thank you @myrab\n- New bug report features `python -m bitsandbytes` now gives extensive debugging details to debug CUDA setup failures.\n\nBug fixes:\n\n- Fixed a bug where some bitsandbytes methods failed in a model-parallel setup on multiple GPUs. Thank you @tonylins\n- Fixed a bug where cudart.so libraries could not be found in newer PyTorch releases.\n\nImprovements:\n\n- Improved the CUDA Setup procedure by doing a more extensive search for CUDA libraries\n\nDeprecated:\n\n- Devices with compute capability 3.0 (GTX 700s, K10) and 3.2 (Tegra K1, Jetson TK1) are now deprecated and support will be removed in 0.39.0.\n- Support for CUDA 10.0 and 10.2 will be removed in bitsandbytes 0.39.0\n\n### 0.37.0\n\n#### Int8 Matmul + backward support for all GPUs\n\nFeatures:\n\n- Int8 MatmulLt now supports backward through inversion of the ColTuring/ColAmpere format. Slow, but memory efficient. Big thanks to @borzunov\n- Int8 now supported on all GPUs. On devices with compute capability \\< 7.5, the Int weights are cast to 16/32-bit for the matrix multiplication. Contributed by @borzunov\n\nImprovements:\n\n- Improved logging for the CUDA detection mechanism.\n\n### 0.36.0\n\n#### Improvements, Ada/Hopper support, fake k-bit quantization.\n\nFeatures:\n\n- CUDA 11.8 and 12.0 support added\n- support for Ada and Hopper GPUs added (compute capability 8.9 and 9.0)\n- support for fake k-bit block-wise quantization for Int, Float, quantile quantization, and dynamic exponent data types added\n- Added CUDA instruction generator to fix some installations.\n- Added additional block sizes for quantization {64, 128, 256, 512, 1024}\n- Added SRAM Quantile algorithm to quickly estimate less than 256 quantiles\n- Added option to suppress the bitsandbytes welcome message (@Cyberes)\n\nRegression:\n\n- Compute capability 3.0 removed: GTX 600s and 700s series is no longer supported (except GTX 780 and GTX 780 Ti)\n\nBug fixes:\n\n- fixed a bug where too long directory names would crash the CUDA SETUP #35 (@tomaarsen)\n- fixed a bug where CPU installations on Colab would run into an error  #34 (@tomaarsen)\n- fixed an issue where the default CUDA version with fast-DreamBooth was not supported #52\n- fixed a bug where the CUDA setup failed due to a wrong function call.\n- fixed a bug in the CUDA Setup which led to an incomprehensible error if no GPU was detected.\n- fixed a bug in the CUDA Setup failed with the cuda runtime was found, but not the cuda library.\n- fixed a bug where not finding the cuda runtime led to an incomprehensible error.\n- fixed a bug where with missing CUDA the default was an error instead of the loading the CPU library\n- fixed a bug where the CC version of the GPU was not detected appropriately (@BlackHC)\n- fixed a bug in CPU quantization which lead to errors when the input buffer exceeded 2^31 elements\n\nImprovements:\n\n- multiple improvements in formatting, removal of unused imports, and slight performance improvements (@tomaarsen)\n- StableEmbedding layer now has device and dtype parameters to make it 1:1 replaceable with regular Embedding layers (@lostmsu)\n- runtime performance of block-wise quantization slightly improved\n- added error message for the case multiple libcudart.so are installed and bitsandbytes picks the wrong one\n\n### 0.35.4\n\nBug fixes:\n\n- Fixed a bug in the CUDA Setup failed with the cuda runtime was found, but not the cuda library.\n- Fixed a bug where not finding the cuda runtime led to an incomprehensible error.\n\n### 0.35.3\n\nBug fixes:\n\n- Fixed a bug in the CUDA Setup which led to an incomprehensible error if no GPU was detected.\n\n### 0.35.2\n\nBug fixes:\n\n- Fixed a bug where the CUDA setup failed due to a wrong function call.\n\n### 0.35.1\n\nFeatures:\n\n- Added CUDA instruction generator to fix some installations.\n\nBug fixes:\n\n- Fixed a problem where warning messages would be displayed even though everything worked correctly.\n\n### 0.35.0\n\n#### CUDA 11.8 support and bug fixes\n\nFeatures:\n\n- CUDA 11.8 support added and binaries added to the PyPI release.\n\nBug fixes:\n\n- fixed a bug where too long directory names would crash the CUDA SETUP #35 (thank you @tomaarsen)\n- fixed a bug where CPU installations on Colab would run into an error  #34 (thank you @tomaarsen)\n- fixed an issue where the default CUDA version with fast-DreamBooth was not supported #52\n\n### 0.34.0\n\n#### Bug fixes and memory efficient backprop\n\nFeatures:\n\n- Linear8bitLt layer now supports `memory_efficient_backward=True` which enables backprop of gradients through frozen weights.\n\nBug fixes:\n\n- fixed an issue where too many threads were created in blockwise quantization on the CPU for large tensors\n\n### 0.33.0\n\n#### Various bug fixes\n\nFeatures:\n\n- CPU quantization now supports a variable `blocksize` variable to enhance quantization speed or precision.\n\nBug fixes:\n\n- fixed an issue in CPU quantization where tensors with more than 2^31 elements would fail 19a7adca7a6c9bf7061a384d7e9d9b13676a1a88\n- fixed a bug where cpu binaries would fail if no GPU would be detected eab4d8232d558f2e6bd7f7cc3d00e2e6e94f4e80\n- fixed an issue where cpu binaries cause additional stdout messages 92a3363096e10ad6a5c4e944af898bd1186d806a\n- fixed an import of bnb.utils 2e630b55f51d454f3bd723dffda68a07ef93190c\n\nWe thank @mryab, @mbrukman, @chessgecko, @dbaranchuk for pull request with bug fixes and new features.\n\n### 0.32.0\n\n#### 8-bit Inference Performance Enhancements\n\nWe added performance enhancements for small models. This makes small models about 2x faster for LLM.int8() inference.\n\nFeatures:\n\n- Int32 dequantization now supports fused biases.\n- Linear8bitLt now uses a fused bias implementation.\n- Change `.data.storage().data_ptr()` to `.data.data_ptr()` to enhance inference performance.\n\nBug fixes:\n\n- Now throws and error if LLM.int8() is used on a GPU that is not supported.\n- Enhances error messaging if CUDA SETUP fails.\n\n### 0.31.0\n\n#### 8-bit Inference and Packaging Update\n\nFeatures:\n\n- added direct outlier extraction. This enables outlier extraction without fp16 weights without performance degradation.\n- Added automatic CUDA SETUP procedure and packaging all binaries into a single bitsandbytes package.\n\n### 0.30.0\n\n#### 8-bit Inference Update\n\nFeatures:\n\n- Added 8-bit matrix multiplication form cuBLAS,  and cuBLASLt as well as multiple GEMM kernels (GEMM, GEMMEx, GEMMLt)\n- Added 8-bit Linear layers with 8-bit Params that perform memory efficient inference with an option for 8-bit mixed precision matrix decomposition for inference without performance degradation\n- Added quantization methods for \"fake\" quantization as well as optimized kernels vector-wise quantization and equalization as well as optimized cuBLASLt transformations\n- CPU only build now available (Thank you, @mryab)\n\nDeprecated:\n\n- Pre-compiled release for CUDA 9.2, 10.0, 10.2 no longer available\n\n### 0.26.0:\n\nFeatures:\n\n- Added Adagrad (without grad clipping) as 32-bit and 8-bit block-wise optimizer.\n- Added AdamW (copy of Adam with weight decay init 1e-2). #10\n- Introduced ModuleConfig overrides which can be seamlessly be used at initialization time of a module.\n- Added `bnb.nn.Embedding` layer which runs at 32-bit but without the layernorm. This works well if you need to fine-tune pretrained models that do not have a embedding layer norm. #19\n\nBug fixes:\n\n- Fixed a bug where weight decay was incorrectly applied to 32-bit Adam. #13\n- Fixed an unsafe use of eval. #8\n- Fixed a bug where the StableEmbedding layer 32-bit optimizer override would not work without registering the whole model first (`bnb.optim.GlobalOptimManager.get_instance().register_parameters(model.parameters())`).  #13 #15\n\nDocs:\n\n- Added instructions how to solve \"\\_\\_fatbinwrap\\_\" errors.\n\n### 0.0.25:\n\nFeatures:\n\n- Added `skip_zeros` for block-wise and 32-bit optimizers. This ensures correct updates for sparse gradients and sparse models.\n- Added support for Kepler GPUs. (#4)\n- Added Analysis Adam to track 8-bit vs 32-bit quantization errors over time.\n- Make compilation more user friendly.\n\nBug fixes:\n\n- fixed \"undefined symbol: \\_\\_fatbinwrap_38\" error for P100 GPUs on CUDA 10.1 (#5)\n\nDocs:\n\n- Added docs with instructions to compile from source.\n\n### 0.0.24:\n\n- Fixed a bug where a float/half conversion led to a compilation error for CUDA 11.1 on Turning GPUs.\n- removed Apex dependency for bnb LAMB\n\n### 0.0.23:\n\nBugs:\n\n- Unified quantization API: each quantization function now returns `Q, S` where `Q` is the quantized tensor and `S` the quantization state which may hold absolute max values, a quantization map or more. For dequantization all functions now accept the inputs `Q, S` so that `Q` is dequantized with the quantization state `S`.\n- Fixed an issue where the CUDA 11.1 binary was not compiled with the right headers\n\nAPI changes:\n\n- Block-wise quantization for optimizers now enabled by default\n\nFeatures:\n\n- Block-wise quantization routines now support CPU Tensors.\n\n### 0.0.22:\n\n- Fixed an error where a `reset_parameters()` call on the `StableEmbedding` would lead to an error in older PyTorch versions (from 1.7.0).\n\n### 0.0.21\n\n- Ampere, RTX 30 series GPUs now compatible with the library.\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 9.6689453125,
          "content": "# This CMake config hopefully makes it easier to compile.\n# Ensure the CUDA Toolkit is available on your path. Then run:\n#   For  GCC: `cmake -B build . && cmake --build build`\n#   For MSVC: `cmake -B build . && cmake --build build --config Release`\n# You can also use the following options and variables\n#  - COMPUTE_BACKEND: Set to `cpu`, `cuda`, or `mps` to select the backend\n#  - CUDA_VERSION: The expected CUDA version, for sanity checking. The actual version\n#                  is whatever CMake finds on your path.\n#  - COMPUTE_CAPABILITY: Which GPU Arch/Compute codes to provide to NVCC.\n#                        Separate by semicolons, i.e. `-DCOMPUTE_CAPABILITY=89;90`\n#                        Check your compute capability here: https://developer.nvidia.com/cuda-gpus\n#  - PTXAS_VERBOSE: Pass the `-v` option to the PTX Assembler\ncmake_minimum_required(VERSION 3.22.1)\n\nproject(bitsandbytes LANGUAGES CXX)\n\n# If run without specifying a build type, default to using the Release configuration:\n#    optimizing the generated binaries for performance and also adds the `-DNDEBUG` flag,\n#    which turns off a bunch of asserts which seem to link to new symbols in libstdc++,\n#    worsening our many_linux compliance..\nif(NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE Release)\nendif()\n\n# Define included source files\nset(CPP_FILES csrc/common.cpp csrc/cpu_ops.cpp csrc/pythonInterface.cpp)\nset(CUDA_FILES csrc/ops.cu csrc/kernels.cu)\nset(MPS_FILES csrc/mps_ops.mm)\nset(METAL_FILES csrc/mps_kernels.metal)\n# C++ sources are always included\nlist(APPEND SRC_FILES ${CPP_FILES})\n\nset(COMPUTE_BACKEND \"cpu\" CACHE STRING \"The compute backend to use (cpu, cuda, mps)\")\nset_property(CACHE COMPUTE_BACKEND PROPERTY STRINGS cpu cuda mps)\noption(PTXAS_VERBOSE \"Pass through -v flag to PTX Assembler\" OFF)\n\nif(APPLE)\n  set(CMAKE_OSX_DEPLOYMENT_TARGET 13.1)\nendif()\n\nset(BNB_OUTPUT_NAME \"bitsandbytes\")\n\nmessage(STATUS \"Configuring ${PROJECT_NAME} (Backend: ${COMPUTE_BACKEND})\")\n\nif(${COMPUTE_BACKEND} STREQUAL \"cuda\")\n    if(APPLE)\n        message(FATAL_ERROR \"CUDA is not supported on macOS\" )\n    endif()\n    set(BUILD_CUDA ON)\n    set(BUILD_MPS OFF)\nelseif(${COMPUTE_BACKEND} STREQUAL \"mps\")\n    if(NOT APPLE)\n        message(FATAL_ERROR \"MPS is only supported on macOS\" )\n    endif()\n    set(BUILD_CUDA OFF)\n    set(BUILD_MPS ON)\nelse()\n    set(BUILD_CUDA OFF)\n    set(BUILD_MPS OFF)\nendif()\n\n\nif(BUILD_CUDA)\n    # NVCC normally will only work with MSVC up to 1939. VS2022 17.10+ starts using versions 1940+.\n    # Workaround: use --allow-unsupported-compiler\n    # This needs to be added *before* we try to enable the CUDA language so CMake's compiler check passes.\n    if(MSVC AND MSVC_VERSION VERSION_GREATER_EQUAL 1940)\n        string(APPEND CMAKE_CUDA_FLAGS \" --allow-unsupported-compiler\")\n\n        # This is needed to build with VS2022 17.11+ and CUDA < 12.4.\n        if (MSVC_VERSION VERSION_GREATER_EQUAL 1941)\n            string(APPEND CMAKE_CUDA_FLAGS \" -D_ALLOW_COMPILER_AND_STL_VERSION_MISMATCH\")\n        endif()\n    endif()\n\n    enable_language(CUDA) # This will fail if CUDA is not found\n    find_package(CUDAToolkit REQUIRED)\n\n    # Convert the CUDA version from X.Y.z to XY. There's probably a shorter way of doing this\n    string(REGEX MATCH \"^[0-9]+.[0-9]+\" _CUDA_VERSION_FIRST_TWO \"${CMAKE_CUDA_COMPILER_VERSION}\")\n    string(REPLACE \".\" \"\" CUDA_VERSION_SHORT \"${_CUDA_VERSION_FIRST_TWO}\")\n\n    # Expose a cache variable that the user can set to ensure the correct version of CUDA is found\n    set(CUDA_VERSION \"${CUDA_VERSION_SHORT}\" CACHE STRING \"Expected CUDA Version Shortcode\")\n\n    message(STATUS \"CUDA Version: ${CUDA_VERSION_SHORT} (${CMAKE_CUDA_COMPILER_VERSION})\")\n    message(STATUS \"CUDA Compiler: ${CMAKE_CUDA_COMPILER}\")\n\n    # It should match the discovered version\n    if(NOT CUDA_VERSION STREQUAL \"${CUDA_VERSION_SHORT}\")\n        message(FATAL_ERROR \"You've specified CUDA version ${CUDA_VERSION} however the CUDA compiler found is ${CUDA_VERSION_SHORT}.\"\n            \" Ensure the desired CUDA compiler is the first one available on your PATH.\"\n        )\n    endif()\n\n    if(CMAKE_CUDA_COMPILER_VERSION VERSION_LESS \"11.0\")\n        message(FATAL_ERROR \"CUDA Version < 11 is not supported\")\n    elseif(CMAKE_CUDA_COMPILER_VERSION VERSION_GREATER_EQUAL \"13.0\")\n        message(FATAL_ERROR \"CUDA Version > 12 is not supported\")\n    endif()\n\n    # CMake < 3.23.0 does not define CMAKE_CUDA_ARCHITECTURES_ALL.\n    if(CMAKE_VERSION VERSION_LESS \"3.23.0\")\n        message(STATUS \"CMake < 3.23.0; determining CUDA architectures supported...\")\n\n        # 11.x and 12.x both support these at a minimum.\n        set(CMAKE_CUDA_ARCHITECTURES_ALL 50 52 53 60 61 62 70 72 75 80)\n        set(CMAKE_CUDA_ARCHITECTURES_ALL_MAJOR 50 60 70 80)\n\n        # CUDA 11.1 adds Ampere support for GA102-GA107.\n        if (CMAKE_CUDA_COMPILER_VERSION VERSION_GREATER_EQUAL \"11.1\")\n            list(APPEND CMAKE_CUDA_ARCHITECTURES_ALL 86)\n        endif()\n\n        # CUDA 11.4 adds Ampere support for GA10B.\n        if (CMAKE_CUDA_COMPILER_VERSION VERSION_GREATER_EQUAL \"11.4\")\n            list(APPEND CMAKE_CUDA_ARCHITECTURES_ALL 87)\n        endif()\n\n        # CUDA 11.8 adds support for Ada and Hopper.\n        if (CMAKE_CUDA_COMPILER_VERSION VERSION_GREATER_EQUAL \"11.8\")\n            list(APPEND CMAKE_CUDA_ARCHITECTURES_ALL 89 90)\n            list(APPEND CMAKE_CUDA_ARCHITECTURES_ALL_MAJOR 90)\n        endif()\n    endif()\n\n    string(APPEND CMAKE_CUDA_FLAGS \" --use_fast_math\")\n\n    if(PTXAS_VERBOSE)\n        # Verbose? Outputs register usage information, and other things...\n        string(APPEND CMAKE_CUDA_FLAGS \" -Xptxas=-v\")\n    endif()\n\n    foreach(capability ${CMAKE_CUDA_ARCHITECTURES_ALL})\n        # Most of the items here are like: `xx-real`, so we just extract the `xx` portion\n        string(REGEX MATCH \"[0-9]+\" capability_id \"${capability}\")\n        if(capability_id GREATER 0)\n            list(APPEND POSSIBLE_CAPABILITIES ${capability_id})\n        endif()\n    endforeach()\n\n    # This can be changed via -D argument to CMake\n    # By default all possible capabilities are compiled\n    set(COMPUTE_CAPABILITY \"${POSSIBLE_CAPABILITIES}\" CACHE STRING \"Compute Capabilities Targeted\")\n\n    message(STATUS \"CUDA Capabilities Available: ${POSSIBLE_CAPABILITIES}\")\n    message(STATUS \"CUDA Capabilities  Selected: ${COMPUTE_CAPABILITY}\")\n\n    # Use the \"real\" option to build native cubin for all selections.\n    # Ensure we build the PTX for the latest version.\n    # This behavior of adding a PTX (virtual) target for the highest architecture\n    # is similar to how the \"all\" and \"all-major\" options would behave in CMake >= 3.23.\n    # TODO: Consider bumping CMake requirement and using CMAKE_CUDA_ARCHITECTURES=[all | native] by default\n    list(REMOVE_DUPLICATES COMPUTE_CAPABILITY)\n    list(SORT COMPUTE_CAPABILITY COMPARE NATURAL)\n    list(POP_BACK COMPUTE_CAPABILITY _LATEST_CAPABILITY)\n    list(TRANSFORM COMPUTE_CAPABILITY APPEND \"-real\" OUTPUT_VARIABLE CMAKE_CUDA_ARCHITECTURES)\n    list(APPEND CMAKE_CUDA_ARCHITECTURES ${_LATEST_CAPABILITY})\n\n    message(STATUS \"CUDA Targets: ${CMAKE_CUDA_ARCHITECTURES}\")\n    message(STATUS \"CUDA NVCC Flags: ${CMAKE_CUDA_FLAGS}\")\n\n    list(APPEND SRC_FILES ${CUDA_FILES})\n\n    string(APPEND BNB_OUTPUT_NAME \"_cuda${CUDA_VERSION_SHORT}\")\n    add_compile_definitions(BUILD_CUDA)\nelseif(BUILD_MPS)\n    if(NOT APPLE)\n        message(FATAL_ERROR \"MPS is only supported on macOS\" )\n    endif()\n\n    enable_language(OBJCXX)\n\n    list(APPEND SRC_FILES ${MPS_FILES})\n\n    string(APPEND BNB_OUTPUT_NAME \"_mps\")\n    add_compile_definitions(BUILD_MPS)\n    file(MAKE_DIRECTORY \"build\")\n    add_custom_command(OUTPUT \"bitsandbytes/bitsandbytes.metallib\"\n                COMMAND xcrun metal -c -o \"build/bitsandbytes.air\" ${METAL_FILES}\n                COMMAND xcrun metallib \"build/bitsandbytes.air\" -o \"bitsandbytes/bitsandbytes.metallib\"\n                DEPENDS \"${METAL_FILES}\"\n                COMMENT \"Compiling Metal kernels\"\n                VERBATIM)\n    add_custom_target(metallib DEPENDS \"bitsandbytes/bitsandbytes.metallib\")\nelse()\n    string(APPEND BNB_OUTPUT_NAME \"_cpu\")\n    set(GPU_SOURCES)\nendif()\n\n\nif(WIN32)\n    # Export all symbols\n    set(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS ON)\nendif()\n\nif(MSVC)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /arch:AVX2 /fp:fast\")\nendif()\n\nset_source_files_properties(${CPP_FILES} PROPERTIES LANGUAGE CXX)\nadd_library(bitsandbytes SHARED ${SRC_FILES})\ntarget_compile_features(bitsandbytes PUBLIC cxx_std_14)\ntarget_include_directories(bitsandbytes PUBLIC csrc include)\n\n\nif(BUILD_CUDA)\n    target_include_directories(bitsandbytes PUBLIC ${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES})\n    target_link_libraries(bitsandbytes PUBLIC CUDA::cudart CUDA::cublas CUDA::cublasLt CUDA::cusparse)\n    set_target_properties(bitsandbytes\n        PROPERTIES\n            CUDA_SEPARABLE_COMPILATION ON\n    )\nendif()\nif(BUILD_MPS)\n    add_dependencies(bitsandbytes metallib)\n    target_link_libraries(bitsandbytes objc \"-framework Foundation\" \"-framework Metal\" \"-framework MetalPerformanceShaders\" \"-framework MetalPerformanceShadersGraph\")\nendif()\n\nif(WIN32)\n    set_target_properties(bitsandbytes PROPERTIES PREFIX \"lib\")\nendif()\nset_target_properties(bitsandbytes PROPERTIES OUTPUT_NAME ${BNB_OUTPUT_NAME})\nif(MSVC)\n    set_target_properties(bitsandbytes PROPERTIES LIBRARY_OUTPUT_DIRECTORY_RELEASE \"${PROJECT_SOURCE_DIR}/bitsandbytes\")\n    set_target_properties(bitsandbytes PROPERTIES LIBRARY_OUTPUT_DIRECTORY_DEBUG \"${PROJECT_SOURCE_DIR}/bitsandbytes\")\n    set_target_properties(bitsandbytes PROPERTIES RUNTIME_OUTPUT_DIRECTORY_RELEASE \"${PROJECT_SOURCE_DIR}/bitsandbytes\")\n    set_target_properties(bitsandbytes PROPERTIES RUNTIME_OUTPUT_DIRECTORY_DEBUG \"${PROJECT_SOURCE_DIR}/bitsandbytes\")\nendif()\n\nset_target_properties(bitsandbytes PROPERTIES LIBRARY_OUTPUT_DIRECTORY \"${PROJECT_SOURCE_DIR}/bitsandbytes\")\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.4580078125,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\nThis Code of Conduct also applies outside the project spaces when there is a\nreasonable belief that an individual's behavior may have a negative impact on\nthe project or its community.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <opensource-conduct@fb.com>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.8388671875,
          "content": "# Contributing to bitsandbytes\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `main`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints, install the [pre-commit hooks as documented here](https://huggingface.co/docs/bitsandbytes/main/en/contributing#setup-pre-commit-hooks).\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\n## License\nBy contributing to bitsandbytes, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.060546875,
          "content": "MIT License\n\nCopyright (c) Facebook, Inc. and its affiliates.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "NOTICE.md",
          "type": "blob",
          "size": 0.2587890625,
          "content": "The majority of bitsandbytes is licensed under MIT, however portions of the project are available under separate license terms: Pytorch is licensed under the BSD license.\n\nWe thank Fabio Cannizzo for this work on FastBinarySearch which is included in this project.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 1.9677734375,
          "content": "# `bitsandbytes`\n\n[![Downloads](https://static.pepy.tech/badge/bitsandbytes)](https://pepy.tech/project/bitsandbytes) [![Downloads](https://static.pepy.tech/badge/bitsandbytes/month)](https://pepy.tech/project/bitsandbytes) [![Downloads](https://static.pepy.tech/badge/bitsandbytes/week)](https://pepy.tech/project/bitsandbytes)\n\nThe `bitsandbytes` library is a lightweight Python wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and 8 & 4-bit quantization functions.\n\nThe library includes quantization primitives for 8-bit & 4-bit operations, through `bitsandbytes.nn.Linear8bitLt` and `bitsandbytes.nn.Linear4bit` and 8-bit optimizers through `bitsandbytes.optim` module.\n\nThere are ongoing efforts to support further hardware backends, i.e. Intel CPU + GPU, AMD GPU, Apple Silicon. Windows support is quite far along and is on its way as well.\n\n**Please head to the official documentation page:**\n\n**[https://huggingface.co/docs/bitsandbytes/main](https://huggingface.co/docs/bitsandbytes/main)**\n\n## `bitsandbytes` multi-backend _alpha_ release is out!\n\nðŸš€ Big news! After months of hard work and incredible community contributions, we're thrilled to announce the **bitsandbytes multi-backend _alpha_ release**! ðŸ’¥\n\nNow supporting:\n- ðŸ”¥ **AMD GPUs** (ROCm)\n- âš¡ **Intel CPUs** & **GPUs**\n\nWeâ€™d love your early feedback! ðŸ™\n\nðŸ‘‰ [Instructions for your `pip install` here](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend)\n\nWe're super excited about these recent developments and grateful for any constructive input or support that you can give to help us make this a reality (e.g. helping us with the upcoming Apple Silicon backend or reporting bugs). BNB is a community project and we're excited for your collaboration ðŸ¤—\n\n## License\n\n`bitsandbytes` is MIT licensed.\n\nWe thank Fabio Cannizzo for his work on [FastBinarySearch](https://github.com/fabiocannizzo/FastBinarySearch) which we use for CPU quantization.\n"
        },
        {
          "name": "_typos.toml",
          "type": "blob",
          "size": 0.3798828125,
          "content": "[files]\n\n[default]\nextend-ignore-re = [\n    \"@Ther-nul\",  # valid Github user\n]\nextend-ignore-identifiers-re = [\n    \".*arange.*\",\n    \".*ARANGE.*\",\n]\n\n[type.py.extend-words]\n\"BA\" = \"BA\"  # used as a commented-out variable in tests\n\n[type.cuda.extend-words]\n\"subtile\" = \"subtile\"\n\"subtiles\" = \"subtiles\"\n\"transation\" = \"transation\"  # TODO: is this transition, transaction, translation..?\n"
        },
        {
          "name": "benchmarking",
          "type": "tree",
          "content": null
        },
        {
          "name": "bitsandbytes",
          "type": "tree",
          "content": null
        },
        {
          "name": "check_bnb_install.py",
          "type": "blob",
          "size": 0.3251953125,
          "content": "import torch\n\nimport bitsandbytes as bnb\n\np = torch.nn.Parameter(torch.rand(10, 10).cuda())\na = torch.rand(10, 10).cuda()\n\np1 = p.data.sum().item()\n\nadam = bnb.optim.Adam([p])\n\nout = a * p\nloss = out.sum()\nloss.backward()\nadam.step()\n\np2 = p.data.sum().item()\n\nassert p1 != p2\nprint(\"SUCCESS!\")\nprint(\"Installation was successful!\")\n"
        },
        {
          "name": "csrc",
          "type": "tree",
          "content": null
        },
        {
          "name": "deploy.sh",
          "type": "blob",
          "size": 6.166015625,
          "content": "#!/bin/bash\nBASE_PATH=$1\n\necho \"MAKE SURE LD_LIBRARY_PATH IS EMPTY!\"\necho $LD_LIBRARY_PATH\n\nif [[ ! -z \"${LD_LIBRARY_PATH}\" ]]; then\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\n\nmodule unload cuda && echo \"no module function available. Probably not on a slurm cluster.\"\nmodule unload gcc && echo \"no module function available. Probably not on a slurm cluster.\"\n\nrm -rf dist build\nmake cleaneggs\nmake cleanlibs\n\nrm -rf build/*\nexport CUDA_HOME=\nexport CUDA_VERSION=\nmake cpuonly CUDA_VERSION=\"CPU\"\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cpu.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-11.0\nmake cuda110 CUDA_VERSION=110\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda110.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-11.1\nmake cuda11x CUDA_VERSION=111\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda111.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-11.4\nmake cuda11x CUDA_VERSION=114\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda114.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-11.5\nmake cuda11x CUDA_VERSION=115\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda115.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-11.7\nmake cuda11x CUDA_VERSION=117\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda117.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-11.8\nmake cuda118 CUDA_VERSION=118\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda118.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-12.0\nmake cuda12x CUDA_VERSION=120\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda120.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-12.1\nmake cuda12x CUDA_VERSION=121\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda121.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-12.2\nmake cuda12x CUDA_VERSION=122\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda122.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-12.3\nmake cuda12x CUDA_VERSION=123\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda123.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\n############################# START NO CUBLASLT #############################################\n# binaries without 8-bit matmul support START HERE\n# ###########################################################################################\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-11.0\nmake cuda110_nomatmul CUDA_VERSION=110\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda110_nocublaslt.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-11.1\nmake cuda11x_nomatmul CUDA_VERSION=111\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda111_nocublaslt.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-11.4\nmake cuda11x_nomatmul CUDA_VERSION=114\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda114_nocublaslt.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-11.5\nmake cuda11x_nomatmul CUDA_VERSION=115\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda115_nocublaslt.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-11.7\nmake cuda11x_nomatmul CUDA_VERSION=117\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-11.8\nmake cuda118_nomatmul CUDA_VERSION=118\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-12.0\nmake cuda12x_nomatmul CUDA_VERSION=120\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda120_nocublaslt.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-12.1\nmake cuda12x_nomatmul CUDA_VERSION=121\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda121_nocublaslt.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-12.2\nmake cuda12x_nomatmul CUDA_VERSION=122\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda122_nocublaslt.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\nrm -rf build/*\nexport CUDA_HOME=$BASE_PATH/cuda-12.3\nmake cuda12x_nomatmul CUDA_VERSION=123\n\nif [ ! -f \"./bitsandbytes/libbitsandbytes_cuda123_nocublaslt.so\" ]; then\n  # Control will enter here if $DIRECTORY doesn't exist.\n  echo \"Compilation unsuccessful!\" 1>&2\n  exit 64\nfi\n\npython -m build\npython -m twine upload dist/* --verbose\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment-bnb.yml",
          "type": "blob",
          "size": 0.2548828125,
          "content": "# for cmake build\nname: bnb\nchannels:\n  - pytorch\n  - nvidia\n  - conda-forge\n\ndependencies:\n  - python\n  #- accelerate\n  #- einops\n  - scipy\n  #- transformers\n  - pytest\n  - pytest-cases\n  - ipython\n  - debugpy\n  - yapf\n  - monkeytype\n  - rich\n  - pytest-sugar\n"
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 1.693359375,
          "content": "name: bnb\nchannels:\n  - pytorch\n  - nvidia\n  - conda-forge\n\ndependencies:\n  # Base\n  - conda-forge::python=3.8\n  - pytorch::pytorch=>2.1\n  - pytorch::pytorch-cuda=11.8\n  - nvidia::cuda=11.8\n  # Libraries\n  - conda-forge::accelerate\n  - conda-forge::einops\n  - conda-forge::scipy\n  - conda-forge::transformers\n  # Development\n  - conda-forge::pytest\n  - conda-forge::build        # build Python packages\n  - conda-forge::twine        # upload Python packages\n  - conda-forge::pytest-cases # more readable and composable parametrized tests\n  - conda-forge::ipython      # better interactive shell\n  - conda-forge::debugpy      # debugger-support for VSCode\n  - conda-forge::ruff         # linting\n  - conda-forge::yapf         # code formatting\n  - conda-forge::monkeytype   # infer type annotations\n  - conda-forge::rich         # better, colored tracebacks, etc\n  - conda-forge::pytest-sugar # better pytest output\n  # - conda-forge::nodejs       # for `doc-builder preview` (optional)\n\n## ENV CREATION - steps to reproduce:\n# mamba env remove -n bnb\n# mamba create -y -n bnb python=3.8 # creating an empty env bypasses conda\n# # and leads to much faster env resolution in the next step https://github.com/mamba-org/mamba/issues/633#issuecomment-812272143\n# mamba env update -n bnb -f environment.yml\n# mamba activate bnb\n\n## PIP dependencies (install *after* ENV CREATION):\n# pip install --no-cache-dir --no-deps lion_pytorch triton hf-doc-builder watchdog\n## NOTE: conda peft is not up to date, so we install from pip\n# cd pip install -e .  ## installs bitsandbytes as editable development install from within repo root dir\n\n## ENV UPDATE:\n# # add new packages to environment.yml, then:\n# mamba env update -n bnb -f environment.yml\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "install_cuda.py",
          "type": "blob",
          "size": 4.5693359375,
          "content": "import os\nimport subprocess\nimport sys\nfrom urllib.request import urlretrieve\n\ncuda_versions = {\n    \"110\": \"https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda_11.0.3_450.51.06_linux.run\",\n    \"111\": \"https://developer.download.nvidia.com/compute/cuda/11.1.1/local_installers/cuda_11.1.1_455.32.00_linux.run\",\n    \"112\": \"https://developer.download.nvidia.com/compute/cuda/11.2.2/local_installers/cuda_11.2.2_460.32.03_linux.run\",\n    \"113\": \"https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda_11.3.1_465.19.01_linux.run\",\n    \"114\": \"https://developer.download.nvidia.com/compute/cuda/11.4.4/local_installers/cuda_11.4.4_470.82.01_linux.run\",\n    \"115\": \"https://developer.download.nvidia.com/compute/cuda/11.5.2/local_installers/cuda_11.5.2_495.29.05_linux.run\",\n    \"116\": \"https://developer.download.nvidia.com/compute/cuda/11.6.2/local_installers/cuda_11.6.2_510.47.03_linux.run\",\n    \"117\": \"https://developer.download.nvidia.com/compute/cuda/11.7.1/local_installers/cuda_11.7.1_515.65.01_linux.run\",\n    \"118\": \"https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run\",\n    \"120\": \"https://developer.download.nvidia.com/compute/cuda/12.0.1/local_installers/cuda_12.0.1_525.85.12_linux.run\",\n    \"121\": \"https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda_12.1.1_530.30.02_linux.run\",\n    \"122\": \"https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda_12.2.2_535.104.05_linux.run\",\n    \"123\": \"https://developer.download.nvidia.com/compute/cuda/12.3.2/local_installers/cuda_12.3.2_545.23.08_linux.run\",\n    \"124\": \"https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda_12.4.1_550.54.15_linux.run\",\n    \"125\": \"https://developer.download.nvidia.com/compute/cuda/12.5.1/local_installers/cuda_12.5.1_555.42.06_linux.run\",\n    \"126\": \"https://developer.download.nvidia.com/compute/cuda/12.6.2/local_installers/cuda_12.6.2_560.35.03_linux.run\",\n}\n\n\ndef install_cuda(version, base_path, download_path):\n    formatted_version = f\"{version[:-1]}.{version[-1]}\"\n    folder = f\"cuda-{formatted_version}\"\n    install_path = os.path.join(base_path, folder)\n\n    if os.path.exists(install_path):\n        print(f\"Removing existing CUDA version {version} at {install_path}...\")\n        subprocess.run([\"rm\", \"-rf\", install_path], check=True)\n\n    url = cuda_versions[version]\n    filename = url.split(\"/\")[-1]\n    filepath = os.path.join(download_path, filename)\n\n    if not os.path.exists(filepath):\n        print(f\"Downloading CUDA version {version} from {url}...\")\n        urlretrieve(url, filepath)\n    else:\n        print(f\"Installer for CUDA version {version} already downloaded.\")\n\n    # Make the installer executable\n    subprocess.run([\"chmod\", \"+x\", filepath], check=True)\n\n    # Install CUDA\n    print(f\"Installing CUDA version {version}...\")\n    install_command = [\n        \"bash\",\n        filepath,\n        \"--no-drm\",\n        \"--no-man-page\",\n        \"--override\",\n        \"--toolkitpath=\" + install_path,\n        \"--toolkit\",\n        \"--silent\",\n    ]\n\n    print(f\"Running command: {' '.join(install_command)}\")\n\n    try:\n        subprocess.run(install_command, check=True)\n    except subprocess.CalledProcessError as e:\n        print(f\"Installation failed for CUDA version {version}: {e}\")\n        return\n    finally:\n        # Delete the installer file\n        os.remove(filepath)\n\n    print(f\"CUDA version {version} installed at {install_path}\")\n\n\ndef main():\n    user_base_path = os.path.expanduser(\"~/cuda\")\n    system_base_path = \"/usr/local/cuda\"\n    base_path = user_base_path  # default to user-specific installation\n    download_path = \"/tmp\"  # default download path\n\n    if len(sys.argv) < 2:\n        print(\"Usage: python install_cuda.py <version/all> [user/system] [download_path]\")\n        sys.exit(1)\n\n    version = sys.argv[1]\n    if len(sys.argv) > 2:\n        base_path = system_base_path if sys.argv[2] == \"system\" else user_base_path\n    if len(sys.argv) > 3:\n        download_path = sys.argv[3]\n\n    if not os.path.exists(base_path):\n        os.makedirs(base_path)\n    if not os.path.exists(download_path):\n        os.makedirs(download_path)\n\n    # Install CUDA version(s)\n    if version == \"all\":\n        for ver in cuda_versions.keys():\n            install_cuda(ver, base_path, download_path)\n    elif version in cuda_versions:\n        install_cuda(version, base_path, download_path)\n    else:\n        print(f\"Invalid CUDA version: {version}. Available versions are: {', '.join(cuda_versions.keys())}\")\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "install_cuda.sh",
          "type": "blob",
          "size": 3.76953125,
          "content": "URL110=https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda_11.0.3_450.51.06_linux.run\nURL111=https://developer.download.nvidia.com/compute/cuda/11.1.1/local_installers/cuda_11.1.1_455.32.00_linux.run\nURL112=https://developer.download.nvidia.com/compute/cuda/11.2.2/local_installers/cuda_11.2.2_460.32.03_linux.run\nURL113=https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda_11.3.1_465.19.01_linux.run\nURL114=https://developer.download.nvidia.com/compute/cuda/11.4.4/local_installers/cuda_11.4.4_470.82.01_linux.run\nURL115=https://developer.download.nvidia.com/compute/cuda/11.5.2/local_installers/cuda_11.5.2_495.29.05_linux.run\nURL116=https://developer.download.nvidia.com/compute/cuda/11.6.2/local_installers/cuda_11.6.2_510.47.03_linux.run\nURL117=https://developer.download.nvidia.com/compute/cuda/11.7.1/local_installers/cuda_11.7.1_515.65.01_linux.run\nURL118=https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run\nURL120=https://developer.download.nvidia.com/compute/cuda/12.0.1/local_installers/cuda_12.0.1_525.85.12_linux.run\nURL121=https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda_12.1.1_530.30.02_linux.run\nURL122=https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda_12.2.2_535.104.05_linux.run\nURL123=https://developer.download.nvidia.com/compute/cuda/12.3.2/local_installers/cuda_12.3.2_545.23.08_linux.run\nURL124=https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda_12.4.1_550.54.15_linux.run\nURL125=https://developer.download.nvidia.com/compute/cuda/12.5.1/local_installers/cuda_12.5.1_555.42.06_linux.run\nURL126=https://developer.download.nvidia.com/compute/cuda/12.6.2/local_installers/cuda_12.6.2_560.35.03_linux.run\n\nCUDA_VERSION=$1\nBASE_PATH=$2\nEXPORT_BASHRC=$3\n\nif [[ -n \"$CUDA_VERSION\" ]]; then\n  if   [[ \"$CUDA_VERSION\" -eq \"110\" ]]; then\n    URL=$URL110\n    FOLDER=cuda-11.0\n  elif [[ \"$CUDA_VERSION\" -eq \"111\" ]]; then\n    URL=$URL111\n    FOLDER=cuda-11.1\n  elif [[ \"$CUDA_VERSION\" -eq \"112\" ]]; then\n    URL=$URL112\n    FOLDER=cuda-11.2\n  elif [[ \"$CUDA_VERSION\" -eq \"113\" ]]; then\n    URL=$URL113\n    FOLDER=cuda-11.3\n  elif [[ \"$CUDA_VERSION\" -eq \"114\" ]]; then\n    URL=$URL114\n    FOLDER=cuda-11.4\n  elif [[ \"$CUDA_VERSION\" -eq \"115\" ]]; then\n    URL=$URL115\n    FOLDER=cuda-11.5\n  elif [[ \"$CUDA_VERSION\" -eq \"116\" ]]; then\n    URL=$URL116\n    FOLDER=cuda-11.6\n  elif [[ \"$CUDA_VERSION\" -eq \"117\" ]]; then\n    URL=$URL117\n    FOLDER=cuda-11.7\n  elif [[ \"$CUDA_VERSION\" -eq \"118\" ]]; then\n    URL=$URL118\n    FOLDER=cuda-11.8\n  elif [[ \"$CUDA_VERSION\" -eq \"120\" ]]; then\n    URL=$URL120\n    FOLDER=cuda-12.0\n  elif [[ \"$CUDA_VERSION\" -eq \"121\" ]]; then\n    URL=$URL121\n    FOLDER=cuda-12.1\n  elif [[ \"$CUDA_VERSION\" -eq \"122\" ]]; then\n    URL=$URL122\n    FOLDER=cuda-12.2\n  elif [[ \"$CUDA_VERSION\" -eq \"123\" ]]; then\n    URL=$URL123\n    FOLDER=cuda-12.3\n  elif [[ \"$CUDA_VERSION\" -eq \"124\" ]]; then\n    URL=$URL124\n    FOLDER=cuda-12.4\n  elif [[ \"$CUDA_VERSION\" -eq \"125\" ]]; then\n    URL=$URL125\n    FOLDER=cuda-12.5\n  elif [[ \"$CUDA_VERSION\" -eq \"126\" ]]; then\n    URL=$URL126\n    FOLDER=cuda-12.6\n  else\n    echo \"argument error: No cuda version passed as input. Choose among versions 110 to 125\"\n  fi\nelse\n    echo \"argument error: No cuda version passed as input. Choose among versions 110 to 125\"\nfi\n\nFILE=$(basename $URL)\n\nif [[ -n \"$CUDA_VERSION\" ]]; then\n  echo $URL\n  echo $FILE\n  wget $URL\n  bash $FILE --no-drm --no-man-page --override --toolkitpath=$BASE_PATH/$FOLDER/ --toolkit --silent\n  if [ \"$EXPORT_BASHRC\" -eq \"1\" ]; then\n    echo \"export LD_LIBRARY_PATH=\\$LD_LIBRARY_PATH:$BASE_PATH/$FOLDER/lib64\" >> ~/.bashrc\n    echo \"export PATH=\\$PATH:$BASE_PATH/$FOLDER/bin\" >> ~/.bashrc\n    source ~/.bashrc\n  fi\nelse\n  echo \"\"\nfi\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 4.2236328125,
          "content": "[build-system]\nrequires = [\"setuptools >= 63.0.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"bitsandbytes\"\ndynamic = [\"version\"]\ndescription = \"k-bit optimizers and matrix multiplication routines.\"\nauthors = [{name=\"Tim Dettmers\", email=\"dettmers@cs.washington.edu\"}]\nrequires-python = \">=3.8\"\nreadme = \"README.md\"\nlicense = {file=\"LICENSE\"}\nkeywords = [\n    \"gpu\",\n    \"optimizers\",\n    \"optimization\",\n    \"8-bit\",\n    \"quantization\",\n    \"compression\"\n]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Environment :: GPU :: NVIDIA CUDA :: 11\",\n    \"Environment :: GPU :: NVIDIA CUDA :: 12\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Science/Research\",\n    \"Operating System :: POSIX :: Linux\",\n    \"Operating System :: MacOS\",\n    \"Operating System :: Microsoft :: Windows\",\n    \"Programming Language :: C++\",\n    \"Programming Language :: Python :: Implementation :: CPython\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Scientific/Engineering :: Artificial Intelligence\"\n]\ndependencies = [\n    \"torch>=1.11,!=1.12.0\",\n    \"numpy>=1.17\"\n]\n\n[project.optional-dependencies]\nbenchmark = [\"pandas\", \"matplotlib\"]\ndocs = [\"hf-doc-builder==0.5.0\"]\ndev = [\n    \"bitsandbytes[test]\",\n    \"build>=1.0.0,<2\",\n    \"ruff==0.6.9\",\n    \"pre-commit>=3.5.0,<4\",\n    \"wheel>=0.42,<1\"\n]\ntest = [\n    \"einops~=0.6.0\",\n    \"lion-pytorch==0.0.6\",\n    \"pytest~=7.4\",\n    \"scipy>=1.10.1,<2; python_version < '3.9'\",\n    \"scipy>=1.11.4,<2; python_version >= '3.9'\",\n    \"transformers>=4.30.1,<5\"\n]\ntriton = [\"triton~=2.0.0; sys_platform=='linux' and platform_machine=='x86_64'\"]\n\n[project.urls]\nhomepage = \"https://github.com/TimDettmers/bitsandbytes\"\nchangelog = \"https://github.com/TimDettmers/bitsandbytes/blob/main/CHANGELOG.md\"\ndocs = \"https://huggingface.co/docs/bitsandbytes/main\"\nissues = \"https://github.com/TimDettmers/bitsandbytes/issues\"\n\n[tool.setuptools]\npackage-data = { \"*\" = [\"libbitsandbytes*.*\"] }\n\n[tool.setuptools.dynamic]\nversion = {attr = \"bitsandbytes.__version__\"}\n\n[tool.pytest.ini_options]\naddopts = \"-rP\"\n#    ; --cov=bitsandbytes\n#    ; # contexts: record which test ran which line; can be seen in html coverage report\n#    ; --cov-context=test\n#    ; --cov-report html\nlog_cli = true\nlog_cli_level = \"INFO\"\nlog_file = \"logs/pytest.log\"\nmarkers = [\n    \"benchmark: mark test as a benchmark\",\n    \"deprecated: mark test as covering a deprecated feature\",\n    \"slow: mark test as slow\",\n]\n\n[tool.ruff]\nsrc = [\n    \"bitsandbytes\",\n    \"tests\",\n    \"benchmarking\"\n]\ntarget-version = \"py38\"\nline-length = 119\n\n[tool.ruff.lint]\nselect = [\n    \"B\",    # bugbear: security warnings\n    \"E\",    # pycodestyle (error)\n    \"W\",    # pycodestyle (warning)\n    \"F\",    # pyflakes\n    \"I\",    # isort\n    \"ISC\",  # implicit string concatenation\n    \"UP\",   # alert you when better syntax is available in your python version\n    \"RUF\",  # the ruff developer's own rules\n]\nignore = [\n    \"B007\",  # Loop control variable not used within the loop body (TODO: enable)\n    \"B028\",  # Warning without stacklevel (TODO: enable)\n    \"E501\",  # Supress line-too-long warnings: trust yapf's judgement on this one.\n    \"E701\",  # Multiple statements on one line (TODO: enable)\n    \"E712\",  # Allow using if x == False, as it's not always equivalent to if x.\n    \"E731\",  # Do not use lambda\n    \"F841\",  # Local assigned but not used (TODO: enable, these are likely bugs)\n    \"RUF012\",  # Mutable class attribute annotations\n    \"ISC001\",   # single-line-implicit-string-concatenation incompatible with formatter\n]\n\n[tool.ruff.lint.extend-per-file-ignores]\n\"**/__init__.py\" = [\"F401\"]  # allow unused imports in __init__.py\n\"{benchmarking,tests}/**/*.py\" = [\n    \"B007\",\n    \"B011\",\n    \"B023\",\n    \"E701\",\n    \"E731\",\n    \"F841\",\n    \"UP030\",\n]\n\n[tool.ruff.lint.isort]\ncombine-as-imports = true\ndetect-same-package = true\nforce-sort-within-sections = true\nknown-first-party = [\"bitsandbytes\"]\n\n[[tool.mypy.overrides]]\nmodule = \"triton.*\"\nignore_missing_imports = true\n\n[[tool.mypy.overrides]]\nmodule = \"scipy.stats\"\nignore_missing_imports = true\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 0.458984375,
          "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nfrom setuptools import find_packages, setup\nfrom setuptools.dist import Distribution\n\n\n# Tested with wheel v0.29.0\nclass BinaryDistribution(Distribution):\n    def has_ext_modules(self):\n        return True\n\n\nsetup(version=\"0.45.1.dev0\", packages=find_packages(), distclass=BinaryDistribution)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}