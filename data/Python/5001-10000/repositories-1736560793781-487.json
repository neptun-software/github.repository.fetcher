{
  "metadata": {
    "timestamp": 1736560793781,
    "page": 487,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "timothybrooks/instruct-pix2pix",
      "stars": 6471,
      "defaultBranch": "main",
      "files": [
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.4638671875,
          "content": "Copyright 2023 Timothy Brooks, Aleksander Holynski, Alexei A. Efros\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nPortions of code and models (such as pretrained checkpoints, which are fine-tuned starting from released Stable Diffusion checkpoints) are derived from the Stable Diffusion codebase (https://github.com/CompVis/stable-diffusion). Further restrictions may apply. Please consult the Stable Diffusion license `stable_diffusion/LICENSE`. Modified code is denoted as such in comments at the start of each file. \n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.3701171875,
          "content": "# InstructPix2Pix: Learning to Follow Image Editing Instructions\n### [Project Page](https://www.timothybrooks.com/instruct-pix2pix/) | [Paper](https://arxiv.org/abs/2211.09800) | [Data](http://instruct-pix2pix.eecs.berkeley.edu/)\nPyTorch implementation of InstructPix2Pix, an instruction-based image editing model, based on the original [CompVis/stable_diffusion](https://github.com/CompVis/stable-diffusion) repo. <br>\n\n[InstructPix2Pix: Learning to Follow Image Editing Instructions](https://www.timothybrooks.com/instruct-pix2pix/)  \n [Tim Brooks](https://www.timothybrooks.com/)\\*,\n [Aleksander Holynski](https://holynski.org/)\\*,\n [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros/) <br>\n UC Berkeley <br>\n  \\*denotes equal contribution  \n  \n  <img src='https://instruct-pix2pix.timothybrooks.com/teaser.jpg'/>\n\n## TL;DR: quickstart \n\nFollow the instructions below to download and run InstructPix2Pix on your own images. These instructions have been tested on a GPU with >18GB VRAM. If you don't have a GPU, you may need to change the default configuration, or check out [other ways of using the model](https://github.com/timothybrooks/instruct-pix2pix#other-ways-of-using-instructpix2pix). \n\n### Set up a conda environment, and download a pretrained model:\n```\nconda env create -f environment.yaml\nconda activate ip2p\nbash scripts/download_checkpoints.sh\n```\n\n### Edit a single image:\n```\npython edit_cli.py --input imgs/example.jpg --output imgs/output.jpg --edit \"turn him into a cyborg\"\n\n# Optionally, you can specify parameters to tune your result:\n# python edit_cli.py --steps 100 --resolution 512 --seed 1371 --cfg-text 7.5 --cfg-image 1.2 --input imgs/example.jpg --output imgs/output.jpg --edit \"turn him into a cyborg\"\n```\n\n### Or launch your own interactive editing Gradio app:\n```\npython edit_app.py \n```\n![Edit app](https://github.com/timothybrooks/instruct-pix2pix/blob/main/imgs/edit_app.jpg?raw=true)\n\n_(For advice on how to get the best results by tuning parameters, see the [Tips](https://github.com/timothybrooks/instruct-pix2pix#tips) section)._\n\n## Setup\n\nInstall all dependencies with:\n```\nconda env create -f environment.yaml\n```\n\nDownload the pretrained models by running:\n```\nbash scripts/download_checkpoints.sh\n```\n\n## Generated Dataset\n\nOur image editing model is trained on a generated dataset consisting of 454,445 examples. Each example contains (1) an input image, (2) an editing instruction, and (3) an output edited image. We provide two versions of the dataset, one in which each pair of edited images is generated 100 times, and the best examples are chosen based on CLIP metrics (Section 3.1.2 in the paper) (`clip-filtered-dataset`), and one in which examples are randomly chosen (`random-sample-dataset`).\n\nFor the released version of this dataset, we've additionally filtered prompts and images for NSFW content. After NSFW filtering, the GPT-3 generated dataset contains 451,990 examples. The final image-pair datasets contain:\n\n|  | # of image editing examples | Dataset size |\n|--|-----------------------|----------------------- |\n| `random-sample-dataset` |451990|727GB|\n|  `clip-filtered-dataset` |313010|436GB|\n\nTo download one of these datasets, along with the entire NSFW-filtered text data, run the following command with the appropriate dataset name:\n\n```\nbash scripts/download_data.sh clip-filtered-dataset\n```\n\n\n## Training InstructPix2Pix\n\nInstructPix2Pix is trained by fine-tuning from an initial StableDiffusion checkpoint. The first step is to download a Stable Diffusion checkpoint. For our trained models, we used the v1.5 checkpoint as the starting point. To download the same ones we used, you can run the following script:\n```\nbash scripts/download_pretrained_sd.sh\n```\nIf you'd like to use a different checkpoint, point to it in the config file `configs/train.yaml`, on line 8, after `ckpt_path:`. \n\nNext, we need to change the config to point to our downloaded (or generated) dataset. If you're using the `clip-filtered-dataset` from above, you can skip this. Otherwise, you may need to edit lines 85 and 94 of the config (`data.params.train.params.path`, `data.params.validation.params.path`). \n\nFinally, start a training job with the following command:\n\n```\npython main.py --name default --base configs/train.yaml --train --gpus 0,1,2,3,4,5,6,7\n```\n\n\n## Creating your own dataset\n\nOur generated dataset of paired images and editing instructions is made in two phases: First, we use GPT-3 to generate text triplets: (a) a caption describing an image, (b) an edit instruction, (c) a caption describing the image after the edit. Then, we turn pairs of captions (before/after the edit) into pairs of images using Stable Diffusion and Prompt-to-Prompt.\n\n### (1) Generate a dataset of captions and instructions\n\nWe provide our generated dataset of captions and edit instructions [here](https://instruct-pix2pix.eecs.berkeley.edu/gpt-generated-prompts.jsonl). If you plan to use our captions+instructions, skip to step (2). Otherwise, if you would like to create your own text dataset, please follow steps (1.1-1.3) below. Note that generating very large datasets using GPT-3 can be expensive.\n\n#### (1.1) Manually write a dataset of instructions and captions\n\nThe first step of the process is fine-tuning GPT-3. To do this, we made a dataset of 700 examples broadly covering of edits that we might want our model to be able to perform. Our examples are available [here](https://instruct-pix2pix.eecs.berkeley.edu/human-written-prompts.jsonl). These should be diverse and cover a wide range of possible captions and types of edits. Ideally, they should avoid duplication or significant overlap of captions and instructions. It is also important to be mindful of limitations of Stable Diffusion and Prompt-to-Prompt in writing these examples, such as inability to perform large spatial transformations (e.g., moving the camera, zooming in, swapping object locations). \n\nInput prompts should closely match the distribution of input prompts used to generate the larger dataset. We sampled the 700 input prompts from the _LAION Improved Aesthetics 6.5+_ dataset and also use this dataset for generating examples. We found this dataset is quite noisy (many of the captions are overly long and contain irrelevant text). For this reason, we also considered MSCOCO and LAION-COCO datasets, but ultimately chose _LAION Improved Aesthetics 6.5+_ due to its diversity of content, proper nouns, and artistic mediums. If you choose to use another dataset or combination of datasets as input to GPT-3 when generating examples, we recommend you sample the input prompts from the same distribution when manually writing training examples.\n\n#### (1.2) Finetune GPT-3\n\nThe next step is to finetune a large language model on the manually written instructions/outputs to generate edit instructions and edited caption from a new input caption. For this, we finetune GPT-3's Davinci model via the OpenAI API, although other language models could be used.\n\nTo prepare training data for GPT-3, one must first create an OpenAI developer account to access the needed APIs, and [set up the API keys on your local device](https://beta.openai.com/docs/api-reference/introduction). Also, run the `prompts/prepare_for_gpt.py` script, which forms the prompts into the correct format by concatenating instructions and captions and adding delimiters and stop sequences.\n\n```bash\npython dataset_creation/prepare_for_gpt.py --input-path data/human-written-prompts.jsonl --output-path data/human-written-prompts-for-gpt.jsonl\n```\n\nNext, finetune GPT-3 via the OpenAI CLI. We provide an example below, although please refer to OpenAI's official documentation for this, as best practices may change. We trained the Davinci model for a single epoch. You can experiment with smaller less expensive GPT-3 variants or with open source language models, although this may negatively affect performance.\n\n```bash\nopenai api fine_tunes.create -t data/human-written-prompts-for-gpt.jsonl -m davinci --n_epochs 1 --suffix \"instruct-pix2pix\"\n```\n\nYou can test out the finetuned GPT-3 model by launching the provided Gradio app:\n\n```bash\npython prompt_app.py --openai-api-key OPENAI_KEY --openai-model OPENAI_MODEL_NAME\n```\n\n![Prompt app](https://github.com/timothybrooks/instruct-pix2pix/blob/main/imgs/prompt_app.jpg?raw=true)\n\n#### (1.3) Generate a large dataset of captions and instructions\n\nWe now use the finetuned GPT-3 model to generate a large dataset. Our dataset cost thousands of dollars to create. See `prompts/gen_instructions_and_captions.py` for the script which generates these examples. We recommend first generating a small number of examples (by setting a low value of `--num-samples`) and gradually increasing the scale to ensure the results are working as desired before increasing scale.\n\n```bash\npython dataset_creation/generate_txt_dataset.py --openai-api-key OPENAI_KEY --openai-model OPENAI_MODEL_NAME\n```\n\nIf you are generating at a very large scale (e.g., 100K+), it will be noteably faster to generate the dataset with multiple processes running in parallel. This can be accomplished by setting `--partitions=N` to a higher number and running multiple processes, setting each `--partition` to the corresponding value.\n\n```bash\npython dataset_creation/generate_txt_dataset.py --openai-api-key OPENAI_KEY --openai-model OPENAI_MODEL_NAME --partitions=10 --partition=0\n```\n\n### (2) Turn paired captions into paired images\n\nThe next step is to turn pairs of text captions into pairs of images. For this, we need to copy some pre-trained Stable Diffusion checkpoints to `stable_diffusion/models/ldm/stable-diffusion-v1/`. You may have already done this if you followed the instructions above for training with our provided data, but if not, you can do this by running:\n\n```bash\nbash scripts/download_pretrained_sd.sh\n```\n\nFor our model, we used [checkpoint v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned.ckpt), and the [new autoencoder](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt), but other models may work as well. If you choose to use other models, make sure to change point to the corresponding checkpoints by passing in the `--ckpt` and `--vae-ckpt` arguments. Once all checkpoints have been downloaded, we can generate the dataset with the following command:\n\n```\npython dataset_creation/generate_img_dataset.py --out_dir data/instruct-pix2pix-dataset-000 --prompts_file path/to/generated_prompts.jsonl\n```\n\nThis command operates on a single GPU (typically a V100 or A100). To parallelize over many GPUs/machines, set `--n-partitions` to the total number of parallel jobs and `--partition` to the index of each job.\n\n```\npython dataset_creation/generate_img_dataset.py --out_dir data/instruct-pix2pix-dataset-000 --prompts_file path/to/generated_prompts.jsonl --n-partitions 100 --partition 0\n```\n\nThe default parameters match that of our dataset, although in practice you can use a smaller number of steps (e.g., `--steps=25`) to generate high quality data faster. By default, we generate 100 samples per prompt and use CLIP filtering to keep a max of 4 per prompt. You can experiment with fewer samples by setting `--n-samples`. The command below turns off CLIP filtering entirely and is therefore faster:\n\n```\npython dataset_creation/generate_img_dataset.py --out_dir data/instruct-pix2pix-dataset-000 --prompts_file path/to/generated_prompts.jsonl --n-samples 4 --clip-threshold 0 --clip-dir-threshold 0 --clip-img-threshold 0 --n-partitions 100 --partition 0\n```\n\nAfter generating all of the dataset examples, run the following command below to create a list of the examples. This is needed for the dataset onject to efficiently be able to sample examples without needing to iterate over the entire dataset directory at the start of each training run.\n\n```\npython dataset_creation/prepare_dataset.py data/instruct-pix2pix-dataset-000\n```\n\n## Evaluation\n\nTo generate plots like the ones in Figures 8 and 10 in the paper, run the following command:\n\n```\npython metrics/compute_metrics.py --ckpt /path/to/your/model.ckpt\n```\n\n## Tips\n\nIf you're not getting the quality result you want, there may be a few reasons:\n1. **Is the image not changing enough?** Your Image CFG weight may be too high. This value dictates how similar the output should be to the input. It's possible your edit requires larger changes from the original image, and your Image CFG weight isn't allowing that. Alternatively, your Text CFG weight may be too low. This value dictates how much to listen to the text instruction. The default Image CFG of 1.5 and Text CFG of 7.5 are a good starting point, but aren't necessarily optimal for each edit. Try:\n    * Decreasing the Image CFG weight, or\n    * Increasing the Text CFG weight, or\n2. Conversely, **is the image changing too much**, such that the details in the original image aren't preserved? Try:\n    * Increasing the Image CFG weight, or\n    * Decreasing the Text CFG weight\n3. Try generating results with different random seeds by setting \"Randomize Seed\" and running generation multiple times. You can also try setting \"Randomize CFG\" to sample new Text CFG and Image CFG values each time.\n4. Rephrasing the instruction sometimes improves results (e.g., \"turn him into a dog\" vs. \"make him a dog\" vs. \"as a dog\").\n5. Increasing the number of steps sometimes improves results.\n6. Do faces look weird? The Stable Diffusion autoencoder has a hard time with faces that are small in the image. Try cropping the image so the face takes up a larger portion of the frame.\n\n## Comments\n\n- Our codebase is based on the [Stable Diffusion codebase](https://github.com/CompVis/stable-diffusion).\n\n## BibTeX\n\n```\n@article{brooks2022instructpix2pix,\n  title={InstructPix2Pix: Learning to Follow Image Editing Instructions},\n  author={Brooks, Tim and Holynski, Aleksander and Efros, Alexei A},\n  journal={arXiv preprint arXiv:2211.09800},\n  year={2022}\n}\n```\n## Other ways of using InstructPix2Pix\n\n### InstructPix2Pix on [HuggingFace](https://huggingface.co/spaces/timbrooks/instruct-pix2pix):\n> A browser-based version of the demo is available as a [HuggingFace space](https://huggingface.co/spaces/timbrooks/instruct-pix2pix). For this version, you only need a browser, a picture you want to edit, and an instruction! Note that this is a shared online demo, and processing time may be slower during peak utilization. \n\n### InstructPix2Pix on [Replicate](https://replicate.com/timothybrooks/instruct-pix2pix):\n> Replicate provides a production-ready cloud API for running the InstructPix2Pix model. You can run the model from any environment using a simple API call with cURL, Python, JavaScript, or your language of choice. Replicate also provides a web interface for running the model and sharing predictions.\n\n### InstructPix2Pix in [Imaginairy](https://github.com/brycedrennan/imaginAIry#-edit-images-with-instructions-alone-by-instructpix2pix):\n> Imaginairy offers another way of easily installing InstructPix2Pix with a single command. It can run on devices without GPUs (like a Macbook!). \n> ```bash\n> pip install imaginairy --upgrade\n> aimg edit any-image.jpg --gif \"turn him into a cyborg\" \n> ```\n> It also offers an easy way to perform a bunch of edits on an image, and can save edits out to an animated GIF:\n> ```\n> aimg edit --gif --surprise-me pearl-earring.jpg \n> ```\n> <img src=\"https://raw.githubusercontent.com/brycedrennan/imaginAIry/7c05c3aae2740278978c5e84962b826e58201bac/assets/girl_with_a_pearl_earring_suprise.gif\" width=\"512\">\n\n### InstructPix2Pix in [🧨 Diffusers](https://github.com/huggingface/diffusers):\n\n> InstructPix2Pix in Diffusers is a bit more optimized, so it may be faster and more suitable for GPUs with less memory. Below are instructions for installing the library and editing an image: \n> 1. Install diffusers and relevant dependencies:\n>\n> ```bash\n> pip install transformers accelerate torch\n>\n> pip install git+https://github.com/huggingface/diffusers.git\n> ```\n> \n> 2. Load the model and edit the image:\n>\n> ```python\n> \n> import torch\n> from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n> \n> model_id = \"timbrooks/instruct-pix2pix\"\n> pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n> pipe.to(\"cuda\")\n> pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n> # `image` is an RGB PIL.Image\n> images = pipe(\"turn him into cyborg\", image=image).images\n> images[0]\n> ```\n> \n> For more information, check the docs [here](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/pix2pix).\n"
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset_creation",
          "type": "tree",
          "content": null
        },
        {
          "name": "edit_app.py",
          "type": "blob",
          "size": 10.3505859375,
          "content": "from __future__ import annotations\n\nimport math\nimport random\nimport sys\nfrom argparse import ArgumentParser\n\nimport einops\nimport gradio as gr\nimport k_diffusion as K\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\nfrom omegaconf import OmegaConf\nfrom PIL import Image, ImageOps\nfrom torch import autocast\n\nsys.path.append(\"./stable_diffusion\")\n\nfrom stable_diffusion.ldm.util import instantiate_from_config\n\n\nhelp_text = \"\"\"\nIf you're not getting what you want, there may be a few reasons:\n1. Is the image not changing enough? Your Image CFG weight may be too high. This value dictates how similar the output should be to the input. It's possible your edit requires larger changes from the original image, and your Image CFG weight isn't allowing that. Alternatively, your Text CFG weight may be too low. This value dictates how much to listen to the text instruction. The default Image CFG of 1.5 and Text CFG of 7.5 are a good starting point, but aren't necessarily optimal for each edit. Try:\n    * Decreasing the Image CFG weight, or\n    * Incerasing the Text CFG weight, or\n2. Conversely, is the image changing too much, such that the details in the original image aren't preserved? Try:\n    * Increasing the Image CFG weight, or\n    * Decreasing the Text CFG weight\n3. Try generating results with different random seeds by setting \"Randomize Seed\" and running generation multiple times. You can also try setting \"Randomize CFG\" to sample new Text CFG and Image CFG values each time.\n4. Rephrasing the instruction sometimes improves results (e.g., \"turn him into a dog\" vs. \"make him a dog\" vs. \"as a dog\").\n5. Increasing the number of steps sometimes improves results.\n6. Do faces look weird? The Stable Diffusion autoencoder has a hard time with faces that are small in the image. Try:\n    * Cropping the image so the face takes up a larger portion of the frame.\n\"\"\"\n\n\nexample_instructions = [\n    \"Make it a picasso painting\",\n    \"as if it were by modigliani\",\n    \"convert to a bronze statue\",\n    \"Turn it into an anime.\",\n    \"have it look like a graphic novel\",\n    \"make him gain weight\",\n    \"what would he look like bald?\",\n    \"Have him smile\",\n    \"Put him in a cocktail party.\",\n    \"move him at the beach.\",\n    \"add dramatic lighting\",\n    \"Convert to black and white\",\n    \"What if it were snowing?\",\n    \"Give him a leather jacket\",\n    \"Turn him into a cyborg!\",\n    \"make him wear a beanie\",\n]\n\n\nclass CFGDenoiser(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.inner_model = model\n\n    def forward(self, z, sigma, cond, uncond, text_cfg_scale, image_cfg_scale):\n        cfg_z = einops.repeat(z, \"1 ... -> n ...\", n=3)\n        cfg_sigma = einops.repeat(sigma, \"1 ... -> n ...\", n=3)\n        cfg_cond = {\n            \"c_crossattn\": [torch.cat([cond[\"c_crossattn\"][0], uncond[\"c_crossattn\"][0], uncond[\"c_crossattn\"][0]])],\n            \"c_concat\": [torch.cat([cond[\"c_concat\"][0], cond[\"c_concat\"][0], uncond[\"c_concat\"][0]])],\n        }\n        out_cond, out_img_cond, out_uncond = self.inner_model(cfg_z, cfg_sigma, cond=cfg_cond).chunk(3)\n        return out_uncond + text_cfg_scale * (out_cond - out_img_cond) + image_cfg_scale * (out_img_cond - out_uncond)\n\n\ndef load_model_from_config(config, ckpt, vae_ckpt=None, verbose=False):\n    print(f\"Loading model from {ckpt}\")\n    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n    if \"global_step\" in pl_sd:\n        print(f\"Global Step: {pl_sd['global_step']}\")\n    sd = pl_sd[\"state_dict\"]\n    if vae_ckpt is not None:\n        print(f\"Loading VAE from {vae_ckpt}\")\n        vae_sd = torch.load(vae_ckpt, map_location=\"cpu\")[\"state_dict\"]\n        sd = {\n            k: vae_sd[k[len(\"first_stage_model.\") :]] if k.startswith(\"first_stage_model.\") else v\n            for k, v in sd.items()\n        }\n    model = instantiate_from_config(config.model)\n    m, u = model.load_state_dict(sd, strict=False)\n    if len(m) > 0 and verbose:\n        print(\"missing keys:\")\n        print(m)\n    if len(u) > 0 and verbose:\n        print(\"unexpected keys:\")\n        print(u)\n    return model\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument(\"--resolution\", default=512, type=int)\n    parser.add_argument(\"--config\", default=\"configs/generate.yaml\", type=str)\n    parser.add_argument(\"--ckpt\", default=\"checkpoints/instruct-pix2pix-00-22000.ckpt\", type=str)\n    parser.add_argument(\"--vae-ckpt\", default=None, type=str)\n    args = parser.parse_args()\n\n    config = OmegaConf.load(args.config)\n    model = load_model_from_config(config, args.ckpt, args.vae_ckpt)\n    model.eval().cuda()\n    model_wrap = K.external.CompVisDenoiser(model)\n    model_wrap_cfg = CFGDenoiser(model_wrap)\n    null_token = model.get_learned_conditioning([\"\"])\n    example_image = Image.open(\"imgs/example.jpg\").convert(\"RGB\")\n\n    def load_example(\n        steps: int,\n        randomize_seed: bool,\n        seed: int,\n        randomize_cfg: bool,\n        text_cfg_scale: float,\n        image_cfg_scale: float,\n    ):\n        example_instruction = random.choice(example_instructions)\n        return [example_image, example_instruction] + generate(\n            example_image,\n            example_instruction,\n            steps,\n            randomize_seed,\n            seed,\n            randomize_cfg,\n            text_cfg_scale,\n            image_cfg_scale,\n        )\n\n    def generate(\n        input_image: Image.Image,\n        instruction: str,\n        steps: int,\n        randomize_seed: bool,\n        seed: int,\n        randomize_cfg: bool,\n        text_cfg_scale: float,\n        image_cfg_scale: float,\n    ):\n        seed = random.randint(0, 100000) if randomize_seed else seed\n        text_cfg_scale = round(random.uniform(6.0, 9.0), ndigits=2) if randomize_cfg else text_cfg_scale\n        image_cfg_scale = round(random.uniform(1.2, 1.8), ndigits=2) if randomize_cfg else image_cfg_scale\n\n        width, height = input_image.size\n        factor = args.resolution / max(width, height)\n        factor = math.ceil(min(width, height) * factor / 64) * 64 / min(width, height)\n        width = int((width * factor) // 64) * 64\n        height = int((height * factor) // 64) * 64\n        input_image = ImageOps.fit(input_image, (width, height), method=Image.Resampling.LANCZOS)\n\n        if instruction == \"\":\n            return [input_image, seed]\n\n        with torch.no_grad(), autocast(\"cuda\"), model.ema_scope():\n            cond = {}\n            cond[\"c_crossattn\"] = [model.get_learned_conditioning([instruction])]\n            input_image = 2 * torch.tensor(np.array(input_image)).float() / 255 - 1\n            input_image = rearrange(input_image, \"h w c -> 1 c h w\").to(model.device)\n            cond[\"c_concat\"] = [model.encode_first_stage(input_image).mode()]\n\n            uncond = {}\n            uncond[\"c_crossattn\"] = [null_token]\n            uncond[\"c_concat\"] = [torch.zeros_like(cond[\"c_concat\"][0])]\n\n            sigmas = model_wrap.get_sigmas(steps)\n\n            extra_args = {\n                \"cond\": cond,\n                \"uncond\": uncond,\n                \"text_cfg_scale\": text_cfg_scale,\n                \"image_cfg_scale\": image_cfg_scale,\n            }\n            torch.manual_seed(seed)\n            z = torch.randn_like(cond[\"c_concat\"][0]) * sigmas[0]\n            z = K.sampling.sample_euler_ancestral(model_wrap_cfg, z, sigmas, extra_args=extra_args)\n            x = model.decode_first_stage(z)\n            x = torch.clamp((x + 1.0) / 2.0, min=0.0, max=1.0)\n            x = 255.0 * rearrange(x, \"1 c h w -> h w c\")\n            edited_image = Image.fromarray(x.type(torch.uint8).cpu().numpy())\n\n            return [seed, text_cfg_scale, image_cfg_scale, edited_image]\n\n    def reset():\n        return [0, \"Randomize Seed\", 1371, \"Fix CFG\", 7.5, 1.5, None]\n\n    with gr.Blocks(css=\"footer {visibility: hidden}\") as demo:\n        with gr.Row():\n            with gr.Column(scale=1, min_width=100):\n                generate_button = gr.Button(\"Generate\")\n            with gr.Column(scale=1, min_width=100):\n                load_button = gr.Button(\"Load Example\")\n            with gr.Column(scale=1, min_width=100):\n                reset_button = gr.Button(\"Reset\")\n            with gr.Column(scale=3):\n                instruction = gr.Textbox(lines=1, label=\"Edit Instruction\", interactive=True)\n\n        with gr.Row():\n            input_image = gr.Image(label=\"Input Image\", type=\"pil\", interactive=True)\n            edited_image = gr.Image(label=f\"Edited Image\", type=\"pil\", interactive=False)\n            input_image.style(height=512, width=512)\n            edited_image.style(height=512, width=512)\n\n        with gr.Row():\n            steps = gr.Number(value=100, precision=0, label=\"Steps\", interactive=True)\n            randomize_seed = gr.Radio(\n                [\"Fix Seed\", \"Randomize Seed\"],\n                value=\"Randomize Seed\",\n                type=\"index\",\n                show_label=False,\n                interactive=True,\n            )\n            seed = gr.Number(value=1371, precision=0, label=\"Seed\", interactive=True)\n            randomize_cfg = gr.Radio(\n                [\"Fix CFG\", \"Randomize CFG\"],\n                value=\"Fix CFG\",\n                type=\"index\",\n                show_label=False,\n                interactive=True,\n            )\n            text_cfg_scale = gr.Number(value=7.5, label=f\"Text CFG\", interactive=True)\n            image_cfg_scale = gr.Number(value=1.5, label=f\"Image CFG\", interactive=True)\n\n        gr.Markdown(help_text)\n\n        load_button.click(\n            fn=load_example,\n            inputs=[\n                steps,\n                randomize_seed,\n                seed,\n                randomize_cfg,\n                text_cfg_scale,\n                image_cfg_scale,\n            ],\n            outputs=[input_image, instruction, seed, text_cfg_scale, image_cfg_scale, edited_image],\n        )\n        generate_button.click(\n            fn=generate,\n            inputs=[\n                input_image,\n                instruction,\n                steps,\n                randomize_seed,\n                seed,\n                randomize_cfg,\n                text_cfg_scale,\n                image_cfg_scale,\n            ],\n            outputs=[seed, text_cfg_scale, image_cfg_scale, edited_image],\n        )\n        reset_button.click(\n            fn=reset,\n            inputs=[],\n            outputs=[steps, randomize_seed, seed, randomize_cfg, text_cfg_scale, image_cfg_scale, edited_image],\n        )\n\n    demo.queue(concurrency_count=1)\n    demo.launch(share=True)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "edit_cli.py",
          "type": "blob",
          "size": 4.8583984375,
          "content": "from __future__ import annotations\n\nimport math\nimport random\nimport sys\nfrom argparse import ArgumentParser\n\nimport einops\nimport k_diffusion as K\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom einops import rearrange\nfrom omegaconf import OmegaConf\nfrom PIL import Image, ImageOps\nfrom torch import autocast\n\nsys.path.append(\"./stable_diffusion\")\n\nfrom stable_diffusion.ldm.util import instantiate_from_config\n\n\nclass CFGDenoiser(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.inner_model = model\n\n    def forward(self, z, sigma, cond, uncond, text_cfg_scale, image_cfg_scale):\n        cfg_z = einops.repeat(z, \"1 ... -> n ...\", n=3)\n        cfg_sigma = einops.repeat(sigma, \"1 ... -> n ...\", n=3)\n        cfg_cond = {\n            \"c_crossattn\": [torch.cat([cond[\"c_crossattn\"][0], uncond[\"c_crossattn\"][0], uncond[\"c_crossattn\"][0]])],\n            \"c_concat\": [torch.cat([cond[\"c_concat\"][0], cond[\"c_concat\"][0], uncond[\"c_concat\"][0]])],\n        }\n        out_cond, out_img_cond, out_uncond = self.inner_model(cfg_z, cfg_sigma, cond=cfg_cond).chunk(3)\n        return out_uncond + text_cfg_scale * (out_cond - out_img_cond) + image_cfg_scale * (out_img_cond - out_uncond)\n\n\ndef load_model_from_config(config, ckpt, vae_ckpt=None, verbose=False):\n    print(f\"Loading model from {ckpt}\")\n    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n    if \"global_step\" in pl_sd:\n        print(f\"Global Step: {pl_sd['global_step']}\")\n    sd = pl_sd[\"state_dict\"]\n    if vae_ckpt is not None:\n        print(f\"Loading VAE from {vae_ckpt}\")\n        vae_sd = torch.load(vae_ckpt, map_location=\"cpu\")[\"state_dict\"]\n        sd = {\n            k: vae_sd[k[len(\"first_stage_model.\") :]] if k.startswith(\"first_stage_model.\") else v\n            for k, v in sd.items()\n        }\n    model = instantiate_from_config(config.model)\n    m, u = model.load_state_dict(sd, strict=False)\n    if len(m) > 0 and verbose:\n        print(\"missing keys:\")\n        print(m)\n    if len(u) > 0 and verbose:\n        print(\"unexpected keys:\")\n        print(u)\n    return model\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument(\"--resolution\", default=512, type=int)\n    parser.add_argument(\"--steps\", default=100, type=int)\n    parser.add_argument(\"--config\", default=\"configs/generate.yaml\", type=str)\n    parser.add_argument(\"--ckpt\", default=\"checkpoints/instruct-pix2pix-00-22000.ckpt\", type=str)\n    parser.add_argument(\"--vae-ckpt\", default=None, type=str)\n    parser.add_argument(\"--input\", required=True, type=str)\n    parser.add_argument(\"--output\", required=True, type=str)\n    parser.add_argument(\"--edit\", required=True, type=str)\n    parser.add_argument(\"--cfg-text\", default=7.5, type=float)\n    parser.add_argument(\"--cfg-image\", default=1.5, type=float)\n    parser.add_argument(\"--seed\", type=int)\n    args = parser.parse_args()\n\n    config = OmegaConf.load(args.config)\n    model = load_model_from_config(config, args.ckpt, args.vae_ckpt)\n    model.eval().cuda()\n    model_wrap = K.external.CompVisDenoiser(model)\n    model_wrap_cfg = CFGDenoiser(model_wrap)\n    null_token = model.get_learned_conditioning([\"\"])\n\n    seed = random.randint(0, 100000) if args.seed is None else args.seed\n    input_image = Image.open(args.input).convert(\"RGB\")\n    width, height = input_image.size\n    factor = args.resolution / max(width, height)\n    factor = math.ceil(min(width, height) * factor / 64) * 64 / min(width, height)\n    width = int((width * factor) // 64) * 64\n    height = int((height * factor) // 64) * 64\n    input_image = ImageOps.fit(input_image, (width, height), method=Image.Resampling.LANCZOS)\n\n    if args.edit == \"\":\n        input_image.save(args.output)\n        return\n\n    with torch.no_grad(), autocast(\"cuda\"), model.ema_scope():\n        cond = {}\n        cond[\"c_crossattn\"] = [model.get_learned_conditioning([args.edit])]\n        input_image = 2 * torch.tensor(np.array(input_image)).float() / 255 - 1\n        input_image = rearrange(input_image, \"h w c -> 1 c h w\").to(model.device)\n        cond[\"c_concat\"] = [model.encode_first_stage(input_image).mode()]\n\n        uncond = {}\n        uncond[\"c_crossattn\"] = [null_token]\n        uncond[\"c_concat\"] = [torch.zeros_like(cond[\"c_concat\"][0])]\n\n        sigmas = model_wrap.get_sigmas(args.steps)\n\n        extra_args = {\n            \"cond\": cond,\n            \"uncond\": uncond,\n            \"text_cfg_scale\": args.cfg_text,\n            \"image_cfg_scale\": args.cfg_image,\n        }\n        torch.manual_seed(seed)\n        z = torch.randn_like(cond[\"c_concat\"][0]) * sigmas[0]\n        z = K.sampling.sample_euler_ancestral(model_wrap_cfg, z, sigmas, extra_args=extra_args)\n        x = model.decode_first_stage(z)\n        x = torch.clamp((x + 1.0) / 2.0, min=0.0, max=1.0)\n        x = 255.0 * rearrange(x, \"1 c h w -> h w c\")\n        edited_image = Image.fromarray(x.type(torch.uint8).cpu().numpy())\n    edited_image.save(args.output)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "edit_dataset.py",
          "type": "blob",
          "size": 4.107421875,
          "content": "from __future__ import annotations\n\nimport json\nimport math\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nimport torch\nimport torchvision\nfrom einops import rearrange\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\n\nclass EditDataset(Dataset):\n    def __init__(\n        self,\n        path: str,\n        split: str = \"train\",\n        splits: tuple[float, float, float] = (0.9, 0.05, 0.05),\n        min_resize_res: int = 256,\n        max_resize_res: int = 256,\n        crop_res: int = 256,\n        flip_prob: float = 0.0,\n    ):\n        assert split in (\"train\", \"val\", \"test\")\n        assert sum(splits) == 1\n        self.path = path\n        self.min_resize_res = min_resize_res\n        self.max_resize_res = max_resize_res\n        self.crop_res = crop_res\n        self.flip_prob = flip_prob\n\n        with open(Path(self.path, \"seeds.json\")) as f:\n            self.seeds = json.load(f)\n\n        split_0, split_1 = {\n            \"train\": (0.0, splits[0]),\n            \"val\": (splits[0], splits[0] + splits[1]),\n            \"test\": (splits[0] + splits[1], 1.0),\n        }[split]\n\n        idx_0 = math.floor(split_0 * len(self.seeds))\n        idx_1 = math.floor(split_1 * len(self.seeds))\n        self.seeds = self.seeds[idx_0:idx_1]\n\n    def __len__(self) -> int:\n        return len(self.seeds)\n\n    def __getitem__(self, i: int) -> dict[str, Any]:\n        name, seeds = self.seeds[i]\n        propt_dir = Path(self.path, name)\n        seed = seeds[torch.randint(0, len(seeds), ()).item()]\n        with open(propt_dir.joinpath(\"prompt.json\")) as fp:\n            prompt = json.load(fp)[\"edit\"]\n\n        image_0 = Image.open(propt_dir.joinpath(f\"{seed}_0.jpg\"))\n        image_1 = Image.open(propt_dir.joinpath(f\"{seed}_1.jpg\"))\n\n        reize_res = torch.randint(self.min_resize_res, self.max_resize_res + 1, ()).item()\n        image_0 = image_0.resize((reize_res, reize_res), Image.Resampling.LANCZOS)\n        image_1 = image_1.resize((reize_res, reize_res), Image.Resampling.LANCZOS)\n\n        image_0 = rearrange(2 * torch.tensor(np.array(image_0)).float() / 255 - 1, \"h w c -> c h w\")\n        image_1 = rearrange(2 * torch.tensor(np.array(image_1)).float() / 255 - 1, \"h w c -> c h w\")\n\n        crop = torchvision.transforms.RandomCrop(self.crop_res)\n        flip = torchvision.transforms.RandomHorizontalFlip(float(self.flip_prob))\n        image_0, image_1 = flip(crop(torch.cat((image_0, image_1)))).chunk(2)\n\n        return dict(edited=image_1, edit=dict(c_concat=image_0, c_crossattn=prompt))\n\n\nclass EditDatasetEval(Dataset):\n    def __init__(\n        self,\n        path: str,\n        split: str = \"train\",\n        splits: tuple[float, float, float] = (0.9, 0.05, 0.05),\n        res: int = 256,\n    ):\n        assert split in (\"train\", \"val\", \"test\")\n        assert sum(splits) == 1\n        self.path = path\n        self.res = res\n\n        with open(Path(self.path, \"seeds.json\")) as f:\n            self.seeds = json.load(f)\n\n        split_0, split_1 = {\n            \"train\": (0.0, splits[0]),\n            \"val\": (splits[0], splits[0] + splits[1]),\n            \"test\": (splits[0] + splits[1], 1.0),\n        }[split]\n\n        idx_0 = math.floor(split_0 * len(self.seeds))\n        idx_1 = math.floor(split_1 * len(self.seeds))\n        self.seeds = self.seeds[idx_0:idx_1]\n\n    def __len__(self) -> int:\n        return len(self.seeds)\n\n    def __getitem__(self, i: int) -> dict[str, Any]:\n        name, seeds = self.seeds[i]\n        propt_dir = Path(self.path, name)\n        seed = seeds[torch.randint(0, len(seeds), ()).item()]\n        with open(propt_dir.joinpath(\"prompt.json\")) as fp:\n            prompt = json.load(fp)\n            edit = prompt[\"edit\"]\n            input_prompt = prompt[\"input\"]\n            output_prompt = prompt[\"output\"]\n\n        image_0 = Image.open(propt_dir.joinpath(f\"{seed}_0.jpg\"))\n\n        reize_res = torch.randint(self.res, self.res + 1, ()).item()\n        image_0 = image_0.resize((reize_res, reize_res), Image.Resampling.LANCZOS)\n\n        image_0 = rearrange(2 * torch.tensor(np.array(image_0)).float() / 255 - 1, \"h w c -> c h w\")\n\n        return dict(image_0=image_0, input_prompt=input_prompt, edit=edit, output_prompt=output_prompt)\n"
        },
        {
          "name": "environment.yaml",
          "type": "blob",
          "size": 0.95703125,
          "content": "# File modified by authors of InstructPix2Pix from original (https://github.com/CompVis/stable-diffusion).\n# See more details in LICENSE.\n\nname: ip2p\nchannels:\n  - pytorch\n  - defaults\ndependencies:\n  - python=3.8.5\n  - pip=20.3\n  - cudatoolkit=11.3\n  - pytorch=1.11.0\n  - torchvision=0.12.0\n  - numpy=1.19.2\n  - pip:\n    - albumentations==0.4.3\n    - datasets==2.8.0\n    - diffusers\n    - opencv-python==4.1.2.30\n    - pudb==2019.2\n    - invisible-watermark\n    - imageio==2.9.0\n    - imageio-ffmpeg==0.4.2\n    - pytorch-lightning==1.4.2\n    - omegaconf==2.1.1\n    - test-tube>=0.7.5\n    - streamlit>=0.73.1\n    - einops==0.3.0\n    - torch-fidelity==0.3.0\n    - transformers==4.19.2\n    - torchmetrics==0.6.0\n    - kornia==0.6\n    - -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n    - -e git+https://github.com/openai/CLIP.git@main#egg=clip\n    - openai\n    - gradio\n    - seaborn\n    - git+https://github.com/crowsonkb/k-diffusion.git\n"
        },
        {
          "name": "imgs",
          "type": "tree",
          "content": null
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 29.2744140625,
          "content": "import argparse, os, sys, datetime, glob\nimport numpy as np\nimport time\nimport torch\nimport torchvision\nimport pytorch_lightning as pl\nimport json\nimport pickle\n\nfrom packaging import version\nfrom omegaconf import OmegaConf\nfrom torch.utils.data import DataLoader, Dataset\nfrom functools import partial\nfrom PIL import Image\n\nimport torch.distributed as dist\nfrom pytorch_lightning import seed_everything\nfrom pytorch_lightning.trainer import Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint, Callback, LearningRateMonitor\nfrom pytorch_lightning.utilities.distributed import rank_zero_only\nfrom pytorch_lightning.utilities import rank_zero_info\nfrom pytorch_lightning.plugins import DDPPlugin\n\nsys.path.append(\"./stable_diffusion\")\n\nfrom ldm.data.base import Txt2ImgIterableBaseDataset\nfrom ldm.util import instantiate_from_config\n\n\ndef get_parser(**parser_kwargs):\n    def str2bool(v):\n        if isinstance(v, bool):\n            return v\n        if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n            return True\n        elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n            return False\n        else:\n            raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\n    parser = argparse.ArgumentParser(**parser_kwargs)\n    parser.add_argument(\n        \"-n\",\n        \"--name\",\n        type=str,\n        const=True,\n        default=\"\",\n        nargs=\"?\",\n        help=\"postfix for logdir\",\n    )\n    parser.add_argument(\n        \"-r\",\n        \"--resume\",\n        type=str,\n        const=True,\n        default=\"\",\n        nargs=\"?\",\n        help=\"resume from logdir or checkpoint in logdir\",\n    )\n    parser.add_argument(\n        \"-b\",\n        \"--base\",\n        nargs=\"*\",\n        metavar=\"base_config.yaml\",\n        help=\"paths to base configs. Loaded from left-to-right. \"\n             \"Parameters can be overwritten or added with command-line options of the form `--key value`.\",\n        default=list(),\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--train\",\n        type=str2bool,\n        const=True,\n        default=False,\n        nargs=\"?\",\n        help=\"train\",\n    )\n    parser.add_argument(\n        \"--no-test\",\n        type=str2bool,\n        const=True,\n        default=False,\n        nargs=\"?\",\n        help=\"disable test\",\n    )\n    parser.add_argument(\n        \"-p\",\n        \"--project\",\n        help=\"name of new or path to existing project\"\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--debug\",\n        type=str2bool,\n        nargs=\"?\",\n        const=True,\n        default=False,\n        help=\"enable post-mortem debugging\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--seed\",\n        type=int,\n        default=23,\n        help=\"seed for seed_everything\",\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--postfix\",\n        type=str,\n        default=\"\",\n        help=\"post-postfix for default name\",\n    )\n    parser.add_argument(\n        \"-l\",\n        \"--logdir\",\n        type=str,\n        default=\"logs\",\n        help=\"directory for logging dat shit\",\n    )\n    parser.add_argument(\n        \"--scale_lr\",\n        action=\"store_true\",\n        default=False,\n        help=\"scale base-lr by ngpu * batch_size * n_accumulate\",\n    )\n    return parser\n\n\ndef nondefault_trainer_args(opt):\n    parser = argparse.ArgumentParser()\n    parser = Trainer.add_argparse_args(parser)\n    args = parser.parse_args([])\n    return sorted(k for k in vars(args) if getattr(opt, k) != getattr(args, k))\n\n\nclass WrappedDataset(Dataset):\n    \"\"\"Wraps an arbitrary object with __len__ and __getitem__ into a pytorch dataset\"\"\"\n\n    def __init__(self, dataset):\n        self.data = dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n\ndef worker_init_fn(_):\n    worker_info = torch.utils.data.get_worker_info()\n\n    dataset = worker_info.dataset\n    worker_id = worker_info.id\n\n    if isinstance(dataset, Txt2ImgIterableBaseDataset):\n        split_size = dataset.num_records // worker_info.num_workers\n        # reset num_records to the true number to retain reliable length information\n        dataset.sample_ids = dataset.valid_ids[worker_id * split_size:(worker_id + 1) * split_size]\n        current_id = np.random.choice(len(np.random.get_state()[1]), 1)\n        return np.random.seed(np.random.get_state()[1][current_id] + worker_id)\n    else:\n        return np.random.seed(np.random.get_state()[1][0] + worker_id)\n\n\nclass DataModuleFromConfig(pl.LightningDataModule):\n    def __init__(self, batch_size, train=None, validation=None, test=None, predict=None,\n                 wrap=False, num_workers=None, shuffle_test_loader=False, use_worker_init_fn=False,\n                 shuffle_val_dataloader=False):\n        super().__init__()\n        self.batch_size = batch_size\n        self.dataset_configs = dict()\n        self.num_workers = num_workers if num_workers is not None else batch_size * 2\n        self.use_worker_init_fn = use_worker_init_fn\n        if train is not None:\n            self.dataset_configs[\"train\"] = train\n            self.train_dataloader = self._train_dataloader\n        if validation is not None:\n            self.dataset_configs[\"validation\"] = validation\n            self.val_dataloader = partial(self._val_dataloader, shuffle=shuffle_val_dataloader)\n        if test is not None:\n            self.dataset_configs[\"test\"] = test\n            self.test_dataloader = partial(self._test_dataloader, shuffle=shuffle_test_loader)\n        if predict is not None:\n            self.dataset_configs[\"predict\"] = predict\n            self.predict_dataloader = self._predict_dataloader\n        self.wrap = wrap\n\n    def prepare_data(self):\n        for data_cfg in self.dataset_configs.values():\n            instantiate_from_config(data_cfg)\n\n    def setup(self, stage=None):\n        self.datasets = dict(\n            (k, instantiate_from_config(self.dataset_configs[k]))\n            for k in self.dataset_configs)\n        if self.wrap:\n            for k in self.datasets:\n                self.datasets[k] = WrappedDataset(self.datasets[k])\n\n    def _train_dataloader(self):\n        is_iterable_dataset = isinstance(self.datasets['train'], Txt2ImgIterableBaseDataset)\n        if is_iterable_dataset or self.use_worker_init_fn:\n            init_fn = worker_init_fn\n        else:\n            init_fn = None\n        return DataLoader(self.datasets[\"train\"], batch_size=self.batch_size,\n                          num_workers=self.num_workers, shuffle=False if is_iterable_dataset else True,\n                          worker_init_fn=init_fn, persistent_workers=True)\n\n    def _val_dataloader(self, shuffle=False):\n        if isinstance(self.datasets['validation'], Txt2ImgIterableBaseDataset) or self.use_worker_init_fn:\n            init_fn = worker_init_fn\n        else:\n            init_fn = None\n        return DataLoader(self.datasets[\"validation\"],\n                          batch_size=self.batch_size,\n                          num_workers=self.num_workers,\n                          worker_init_fn=init_fn,\n                          shuffle=shuffle, persistent_workers=True)\n\n    def _test_dataloader(self, shuffle=False):\n        is_iterable_dataset = isinstance(self.datasets['train'], Txt2ImgIterableBaseDataset)\n        if is_iterable_dataset or self.use_worker_init_fn:\n            init_fn = worker_init_fn\n        else:\n            init_fn = None\n\n        # do not shuffle dataloader for iterable dataset\n        shuffle = shuffle and (not is_iterable_dataset)\n\n        return DataLoader(self.datasets[\"test\"], batch_size=self.batch_size,\n                          num_workers=self.num_workers, worker_init_fn=init_fn, shuffle=shuffle, persistent_workers=True)\n\n    def _predict_dataloader(self, shuffle=False):\n        if isinstance(self.datasets['predict'], Txt2ImgIterableBaseDataset) or self.use_worker_init_fn:\n            init_fn = worker_init_fn\n        else:\n            init_fn = None\n        return DataLoader(self.datasets[\"predict\"], batch_size=self.batch_size,\n                          num_workers=self.num_workers, worker_init_fn=init_fn, persistent_workers=True)\n\n\nclass SetupCallback(Callback):\n    def __init__(self, resume, now, logdir, ckptdir, cfgdir, config, lightning_config):\n        super().__init__()\n        self.resume = resume\n        self.now = now\n        self.logdir = logdir\n        self.ckptdir = ckptdir\n        self.cfgdir = cfgdir\n        self.config = config\n        self.lightning_config = lightning_config\n\n    def on_keyboard_interrupt(self, trainer, pl_module):\n        if trainer.global_rank == 0:\n            print(\"Summoning checkpoint.\")\n            ckpt_path = os.path.join(self.ckptdir, \"last.ckpt\")\n            trainer.save_checkpoint(ckpt_path)\n\n    def on_pretrain_routine_start(self, trainer, pl_module):\n        if trainer.global_rank == 0:\n            # Create logdirs and save configs\n            # os.makedirs(self.logdir, exist_ok=True)\n            # os.makedirs(self.ckptdir, exist_ok=True)\n            # os.makedirs(self.cfgdir, exist_ok=True)\n\n            if \"callbacks\" in self.lightning_config:\n                if 'metrics_over_trainsteps_checkpoint' in self.lightning_config['callbacks']:\n                    os.makedirs(os.path.join(self.ckptdir, 'trainstep_checkpoints'), exist_ok=True)\n            print(\"Project config\")\n            print(OmegaConf.to_yaml(self.config))\n            OmegaConf.save(self.config,\n                           os.path.join(self.cfgdir, \"{}-project.yaml\".format(self.now)))\n\n            print(\"Lightning config\")\n            print(OmegaConf.to_yaml(self.lightning_config))\n            OmegaConf.save(OmegaConf.create({\"lightning\": self.lightning_config}),\n                           os.path.join(self.cfgdir, \"{}-lightning.yaml\".format(self.now)))\n\ndef get_world_size():\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\n\ndef all_gather(data):\n    \"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"\n    world_size = get_world_size()\n    if world_size == 1:\n        return [data]\n\n    # serialized to a Tensor\n    origin_size = None\n    if not isinstance(data, torch.Tensor):\n        buffer = pickle.dumps(data)\n        storage = torch.ByteStorage.from_buffer(buffer)\n        tensor = torch.ByteTensor(storage).to(\"cuda\")\n    else:\n        origin_size = data.size()\n        tensor = data.reshape(-1)\n\n    tensor_type = tensor.dtype\n\n    # obtain Tensor size of each rank\n    local_size = torch.LongTensor([tensor.numel()]).to(\"cuda\")\n    size_list = [torch.LongTensor([0]).to(\"cuda\") for _ in range(world_size)]\n    dist.all_gather(size_list, local_size)\n    size_list = [int(size.item()) for size in size_list]\n    max_size = max(size_list)\n\n    # receiving Tensor from all ranks\n    # we pad the tensor because torch all_gather does not support\n    # gathering tensors of different shapes\n    tensor_list = []\n    for _ in size_list:\n        tensor_list.append(torch.FloatTensor(size=(max_size,)).cuda().to(tensor_type))\n    if local_size != max_size:\n        padding = torch.FloatTensor(size=(max_size - local_size,)).cuda().to(tensor_type)\n        tensor = torch.cat((tensor, padding), dim=0)\n    dist.all_gather(tensor_list, tensor)\n\n    data_list = []\n    for size, tensor in zip(size_list, tensor_list):\n        if origin_size is None:\n            buffer = tensor.cpu().numpy().tobytes()[:size]\n            data_list.append(pickle.loads(buffer))\n        else:\n            buffer = tensor[:size]\n            data_list.append(buffer)\n\n    if origin_size is not None:\n        new_shape = [-1] + list(origin_size[1:])\n        resized_list = []\n        for data in data_list:\n            # suppose the difference of tensor size exist in first dimension\n            data = data.reshape(new_shape)\n            resized_list.append(data)\n\n        return resized_list\n    else:\n        return data_list\n\nclass ImageLogger(Callback):\n    def __init__(self, batch_frequency, max_images, clamp=True, increase_log_steps=True,\n                 rescale=True, disabled=False, log_on_batch_idx=False, log_first_step=False,\n                 log_images_kwargs=None):\n        super().__init__()\n        self.rescale = rescale\n        self.batch_freq = batch_frequency\n        self.max_images = max_images\n        self.logger_log_images = {\n            pl.loggers.TestTubeLogger: self._testtube,\n        }\n        self.log_steps = [2 ** n for n in range(6, int(np.log2(self.batch_freq)) + 1)]\n        if not increase_log_steps:\n            self.log_steps = [self.batch_freq]\n        self.clamp = clamp\n        self.disabled = disabled\n        self.log_on_batch_idx = log_on_batch_idx\n        self.log_images_kwargs = log_images_kwargs if log_images_kwargs else {}\n        self.log_first_step = log_first_step\n\n    @rank_zero_only\n    def _testtube(self, pl_module, images, batch_idx, split):\n        for k in images:\n            grid = torchvision.utils.make_grid(images[k])\n            grid = (grid + 1.0) / 2.0  # -1,1 -> 0,1; c,h,w\n\n            tag = f\"{split}/{k}\"\n            pl_module.logger.experiment.add_image(\n                tag, grid,\n                global_step=pl_module.global_step)\n\n    @rank_zero_only\n    def log_local(self, save_dir, split, images, prompts,\n                  global_step, current_epoch, batch_idx):\n        root = os.path.join(save_dir, \"images\", split)\n        names = {\"reals\": \"before\", \"inputs\": \"after\", \"reconstruction\": \"before-vq\", \"samples\": \"after-gen\"}\n        # print(root)\n        for k in images:\n            grid = torchvision.utils.make_grid(images[k], nrow=8)\n            if self.rescale:\n                grid = (grid + 1.0) / 2.0  # -1,1 -> 0,1; c,h,w\n            grid = grid.transpose(0, 1).transpose(1, 2).squeeze(-1)\n            grid = grid.numpy()\n            grid = (grid * 255).astype(np.uint8)\n            filename = \"gs-{:06}_e-{:06}_b-{:06}_{}.png\".format(\n                global_step,\n                current_epoch,\n                batch_idx,\n                names[k])\n            path = os.path.join(root, filename)\n            os.makedirs(os.path.split(path)[0], exist_ok=True)\n            # print(path)\n            Image.fromarray(grid).save(path)\n\n        filename = \"gs-{:06}_e-{:06}_b-{:06}_prompt.json\".format(\n            global_step,\n            current_epoch,\n            batch_idx)\n        path = os.path.join(root, filename)\n        with open(path, \"w\") as f:\n            for p in prompts:\n                f.write(f\"{json.dumps(p)}\\n\")\n\n    def log_img(self, pl_module, batch, batch_idx, split=\"train\"):\n        check_idx = batch_idx if self.log_on_batch_idx else pl_module.global_step\n        if (self.check_frequency(check_idx) and  # batch_idx % self.batch_freq == 0\n                hasattr(pl_module, \"log_images\") and\n                callable(pl_module.log_images) and\n                self.max_images > 0) or (split == \"val\" and batch_idx == 0):\n            logger = type(pl_module.logger)\n\n            is_train = pl_module.training\n            if is_train:\n                pl_module.eval()\n\n            with torch.no_grad():\n                images = pl_module.log_images(batch, split=split, **self.log_images_kwargs)\n\n            prompts = batch[\"edit\"][\"c_crossattn\"][:self.max_images]\n            prompts = [p for ps in all_gather(prompts) for p in ps]\n\n            for k in images:\n                N = min(images[k].shape[0], self.max_images)\n                images[k] = images[k][:N]\n                images[k] = torch.cat(all_gather(images[k][:N]))\n                if isinstance(images[k], torch.Tensor):\n                    images[k] = images[k].detach().cpu()\n                    if self.clamp:\n                        images[k] = torch.clamp(images[k], -1., 1.)\n\n            self.log_local(pl_module.logger.save_dir, split, images, prompts,\n                           pl_module.global_step, pl_module.current_epoch, batch_idx)\n\n            logger_log_images = self.logger_log_images.get(logger, lambda *args, **kwargs: None)\n            logger_log_images(pl_module, images, pl_module.global_step, split)\n\n            if is_train:\n                pl_module.train()\n\n    def check_frequency(self, check_idx):\n        if ((check_idx % self.batch_freq) == 0 or (check_idx in self.log_steps)) and (\n                check_idx > 0 or self.log_first_step):\n            if len(self.log_steps) > 0:\n                self.log_steps.pop(0)\n            return True\n        return False\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n        if not self.disabled and (pl_module.global_step > 0 or self.log_first_step):\n            self.log_img(pl_module, batch, batch_idx, split=\"train\")\n\n    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n        if not self.disabled and pl_module.global_step > 0:\n            self.log_img(pl_module, batch, batch_idx, split=\"val\")\n        if hasattr(pl_module, 'calibrate_grad_norm'):\n            if (pl_module.calibrate_grad_norm and batch_idx % 25 == 0) and batch_idx > 0:\n                self.log_gradients(trainer, pl_module, batch_idx=batch_idx)\n\n\nclass CUDACallback(Callback):\n    # see https://github.com/SeanNaren/minGPT/blob/master/mingpt/callback.py\n    def on_train_epoch_start(self, trainer, pl_module):\n        # Reset the memory use counter\n        torch.cuda.reset_peak_memory_stats(trainer.root_gpu)\n        torch.cuda.synchronize(trainer.root_gpu)\n        self.start_time = time.time()\n\n    def on_train_epoch_end(self, trainer, pl_module, outputs):\n        torch.cuda.synchronize(trainer.root_gpu)\n        max_memory = torch.cuda.max_memory_allocated(trainer.root_gpu) / 2 ** 20\n        epoch_time = time.time() - self.start_time\n\n        try:\n            max_memory = trainer.training_type_plugin.reduce(max_memory)\n            epoch_time = trainer.training_type_plugin.reduce(epoch_time)\n\n            rank_zero_info(f\"Average Epoch time: {epoch_time:.2f} seconds\")\n            rank_zero_info(f\"Average Peak memory {max_memory:.2f}MiB\")\n        except AttributeError:\n            pass\n\n\nif __name__ == \"__main__\":\n    # custom parser to specify config files, train, test and debug mode,\n    # postfix, resume.\n    # `--key value` arguments are interpreted as arguments to the trainer.\n    # `nested.key=value` arguments are interpreted as config parameters.\n    # configs are merged from left-to-right followed by command line parameters.\n\n    # model:\n    #   base_learning_rate: float\n    #   target: path to lightning module\n    #   params:\n    #       key: value\n    # data:\n    #   target: main.DataModuleFromConfig\n    #   params:\n    #      batch_size: int\n    #      wrap: bool\n    #      train:\n    #          target: path to train dataset\n    #          params:\n    #              key: value\n    #      validation:\n    #          target: path to validation dataset\n    #          params:\n    #              key: value\n    #      test:\n    #          target: path to test dataset\n    #          params:\n    #              key: value\n    # lightning: (optional, has sane defaults and can be specified on cmdline)\n    #   trainer:\n    #       additional arguments to trainer\n    #   logger:\n    #       logger to instantiate\n    #   modelcheckpoint:\n    #       modelcheckpoint to instantiate\n    #   callbacks:\n    #       callback1:\n    #           target: importpath\n    #           params:\n    #               key: value\n\n    now = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n\n    # add cwd for convenience and to make classes in this file available when\n    # running as `python main.py`\n    # (in particular `main.DataModuleFromConfig`)\n    sys.path.append(os.getcwd())\n\n    parser = get_parser()\n    parser = Trainer.add_argparse_args(parser)\n\n    opt, unknown = parser.parse_known_args()\n\n    assert opt.name\n    cfg_fname = os.path.split(opt.base[0])[-1]\n    cfg_name = os.path.splitext(cfg_fname)[0]\n    nowname = f\"{cfg_name}_{opt.name}\"\n    logdir = os.path.join(opt.logdir, nowname)\n    ckpt = os.path.join(logdir, \"checkpoints\", \"last.ckpt\")\n    resume = False\n\n    if os.path.isfile(ckpt):\n        opt.resume_from_checkpoint = ckpt\n        base_configs = sorted(glob.glob(os.path.join(logdir, \"configs/*.yaml\")))\n        opt.base = base_configs + opt.base\n        _tmp = logdir.split(\"/\")\n        nowname = _tmp[-1]\n        resume = True\n\n    ckptdir = os.path.join(logdir, \"checkpoints\")\n    cfgdir = os.path.join(logdir, \"configs\")\n\n    os.makedirs(logdir, exist_ok=True)\n    os.makedirs(ckptdir, exist_ok=True)\n    os.makedirs(cfgdir, exist_ok=True)\n\n    try:\n        # init and save configs\n        configs = [OmegaConf.load(cfg) for cfg in opt.base]\n        cli = OmegaConf.from_dotlist(unknown)\n        config = OmegaConf.merge(*configs, cli)\n\n        if resume:\n            # By default, when finetuning from Stable Diffusion, we load the EMA-only checkpoint to initialize all weights.\n            # If resuming InstructPix2Pix from a finetuning checkpoint, instead load both EMA and non-EMA weights.\n            config.model.params.load_ema = True\n\n        lightning_config = config.pop(\"lightning\", OmegaConf.create())\n        # merge trainer cli with config\n        trainer_config = lightning_config.get(\"trainer\", OmegaConf.create())\n        # default to ddp\n        trainer_config[\"accelerator\"] = \"ddp\"\n        for k in nondefault_trainer_args(opt):\n            trainer_config[k] = getattr(opt, k)\n        if not \"gpus\" in trainer_config:\n            del trainer_config[\"accelerator\"]\n            cpu = True\n        else:\n            gpuinfo = trainer_config[\"gpus\"]\n            print(f\"Running on GPUs {gpuinfo}\")\n            cpu = False\n        trainer_opt = argparse.Namespace(**trainer_config)\n        lightning_config.trainer = trainer_config\n\n        # model\n        model = instantiate_from_config(config.model)\n\n        # trainer and callbacks\n        trainer_kwargs = dict()\n\n        # default logger configs\n        default_logger_cfgs = {\n            \"wandb\": {\n                \"target\": \"pytorch_lightning.loggers.WandbLogger\",\n                \"params\": {\n                    \"name\": nowname,\n                    \"save_dir\": logdir,\n                    \"id\": nowname,\n                }\n            },\n            \"testtube\": {\n                \"target\": \"pytorch_lightning.loggers.TestTubeLogger\",\n                \"params\": {\n                    \"name\": \"testtube\",\n                    \"save_dir\": logdir,\n                }\n            },\n        }\n        default_logger_cfg = default_logger_cfgs[\"wandb\"]\n        if \"logger\" in lightning_config:\n            logger_cfg = lightning_config.logger\n        else:\n            logger_cfg = OmegaConf.create()\n        logger_cfg = OmegaConf.merge(default_logger_cfg, logger_cfg)\n        trainer_kwargs[\"logger\"] = instantiate_from_config(logger_cfg)\n\n        # modelcheckpoint - use TrainResult/EvalResult(checkpoint_on=metric) to\n        # specify which metric is used to determine best models\n        default_modelckpt_cfg = {\n            \"target\": \"pytorch_lightning.callbacks.ModelCheckpoint\",\n            \"params\": {\n                \"dirpath\": ckptdir,\n                \"filename\": \"{epoch:06}\",\n                \"verbose\": True,\n                \"save_last\": True,\n            }\n        }\n\n        if \"modelcheckpoint\" in lightning_config:\n            modelckpt_cfg = lightning_config.modelcheckpoint\n        else:\n            modelckpt_cfg =  OmegaConf.create()\n        modelckpt_cfg = OmegaConf.merge(default_modelckpt_cfg, modelckpt_cfg)\n        print(f\"Merged modelckpt-cfg: \\n{modelckpt_cfg}\")\n        if version.parse(pl.__version__) < version.parse('1.4.0'):\n            trainer_kwargs[\"checkpoint_callback\"] = instantiate_from_config(modelckpt_cfg)\n\n        # add callback which sets up log directory\n        default_callbacks_cfg = {\n            \"setup_callback\": {\n                \"target\": \"main.SetupCallback\",\n                \"params\": {\n                    \"resume\": opt.resume,\n                    \"now\": now,\n                    \"logdir\": logdir,\n                    \"ckptdir\": ckptdir,\n                    \"cfgdir\": cfgdir,\n                    \"config\": config,\n                    \"lightning_config\": lightning_config,\n                }\n            },\n            \"image_logger\": {\n                \"target\": \"main.ImageLogger\",\n                \"params\": {\n                    \"batch_frequency\": 750,\n                    \"max_images\": 4,\n                    \"clamp\": True\n                }\n            },\n            \"learning_rate_logger\": {\n                \"target\": \"main.LearningRateMonitor\",\n                \"params\": {\n                    \"logging_interval\": \"step\",\n                    # \"log_momentum\": True\n                }\n            },\n            \"cuda_callback\": {\n                \"target\": \"main.CUDACallback\"\n            },\n        }\n        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n            default_callbacks_cfg.update({'checkpoint_callback': modelckpt_cfg})\n\n        if \"callbacks\" in lightning_config:\n            callbacks_cfg = lightning_config.callbacks\n        else:\n            callbacks_cfg = OmegaConf.create()\n\n        print(\n            'Caution: Saving checkpoints every n train steps without deleting. This might require some free space.')\n        default_metrics_over_trainsteps_ckpt_dict = {\n            'metrics_over_trainsteps_checkpoint': {\n                \"target\": 'pytorch_lightning.callbacks.ModelCheckpoint',\n                'params': {\n                    \"dirpath\": os.path.join(ckptdir, 'trainstep_checkpoints'),\n                    \"filename\": \"{epoch:06}-{step:09}\",\n                    \"verbose\": True,\n                    'save_top_k': -1,\n                    'every_n_train_steps': 1000,\n                    'save_weights_only': True\n                }\n            }\n        }\n        default_callbacks_cfg.update(default_metrics_over_trainsteps_ckpt_dict)\n\n        callbacks_cfg = OmegaConf.merge(default_callbacks_cfg, callbacks_cfg)\n        if 'ignore_keys_callback' in callbacks_cfg and hasattr(trainer_opt, 'resume_from_checkpoint'):\n            callbacks_cfg.ignore_keys_callback.params['ckpt_path'] = trainer_opt.resume_from_checkpoint\n        elif 'ignore_keys_callback' in callbacks_cfg:\n            del callbacks_cfg['ignore_keys_callback']\n\n        trainer_kwargs[\"callbacks\"] = [instantiate_from_config(callbacks_cfg[k]) for k in callbacks_cfg]\n\n        trainer = Trainer.from_argparse_args(trainer_opt, plugins=DDPPlugin(find_unused_parameters=False), **trainer_kwargs)\n        trainer.logdir = logdir  ###\n\n        # data\n        data = instantiate_from_config(config.data)\n        # NOTE according to https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html\n        # calling these ourselves should not be necessary but it is.\n        # lightning still takes care of proper multiprocessing though\n        data.prepare_data()\n        data.setup()\n        print(\"#### Data #####\")\n        for k in data.datasets:\n            print(f\"{k}, {data.datasets[k].__class__.__name__}, {len(data.datasets[k])}\")\n\n        # configure learning rate\n        bs, base_lr = config.data.params.batch_size, config.model.base_learning_rate\n        if not cpu:\n            ngpu = len(lightning_config.trainer.gpus.strip(\",\").split(','))\n        else:\n            ngpu = 1\n        if 'accumulate_grad_batches' in lightning_config.trainer:\n            accumulate_grad_batches = lightning_config.trainer.accumulate_grad_batches\n        else:\n            accumulate_grad_batches = 1\n        print(f\"accumulate_grad_batches = {accumulate_grad_batches}\")\n        lightning_config.trainer.accumulate_grad_batches = accumulate_grad_batches\n        if opt.scale_lr:\n            model.learning_rate = accumulate_grad_batches * ngpu * bs * base_lr\n            print(\n                \"Setting learning rate to {:.2e} = {} (accumulate_grad_batches) * {} (num_gpus) * {} (batchsize) * {:.2e} (base_lr)\".format(\n                    model.learning_rate, accumulate_grad_batches, ngpu, bs, base_lr))\n        else:\n            model.learning_rate = base_lr\n            print(\"++++ NOT USING LR SCALING ++++\")\n            print(f\"Setting learning rate to {model.learning_rate:.2e}\")\n\n\n        # allow checkpointing via USR1\n        def melk(*args, **kwargs):\n            # run all checkpoint hooks\n            if trainer.global_rank == 0:\n                print(\"Summoning checkpoint.\")\n                ckpt_path = os.path.join(ckptdir, \"last.ckpt\")\n                trainer.save_checkpoint(ckpt_path)\n\n\n        def divein(*args, **kwargs):\n            if trainer.global_rank == 0:\n                import pudb;\n                pudb.set_trace()\n\n\n        import signal\n\n        signal.signal(signal.SIGUSR1, melk)\n        signal.signal(signal.SIGUSR2, divein)\n\n        # run\n        if opt.train:\n            try:\n                trainer.fit(model, data)\n            except Exception:\n                melk()\n                raise\n        if not opt.no_test and not trainer.interrupted:\n            trainer.test(model, data)\n    except Exception:\n        if opt.debug and trainer.global_rank == 0:\n            try:\n                import pudb as debugger\n            except ImportError:\n                import pdb as debugger\n            debugger.post_mortem()\n        raise\n    finally:\n        # move newly created debug project to debug_runs\n        if opt.debug and not opt.resume and trainer.global_rank == 0:\n            dst, name = os.path.split(logdir)\n            dst = os.path.join(dst, \"debug_runs\", name)\n            os.makedirs(os.path.split(dst)[0], exist_ok=True)\n            os.rename(logdir, dst)\n        if trainer.global_rank == 0:\n            print(trainer.profiler.summary())\n"
        },
        {
          "name": "metrics",
          "type": "tree",
          "content": null
        },
        {
          "name": "prompt_app.py",
          "type": "blob",
          "size": 1.9697265625,
          "content": "from __future__ import annotations\n\nfrom argparse import ArgumentParser\n\nimport datasets\nimport gradio as gr\nimport numpy as np\nimport openai\n\nfrom dataset_creation.generate_txt_dataset import generate\n\n\ndef main(openai_model: str):\n    dataset = datasets.load_dataset(\"ChristophSchuhmann/improved_aesthetics_6.5plus\", split=\"train\")\n    captions = dataset[np.random.permutation(len(dataset))][\"TEXT\"]\n    index = 0\n\n    def click_random():\n        nonlocal index\n        output = captions[index]\n        index = (index + 1) % len(captions)\n        return output\n\n    def click_generate(input: str):\n        if input == \"\":\n            raise gr.Error(\"Input caption is missing!\")\n        edit_output = generate(openai_model, input)\n        if edit_output is None:\n            return \"Failed :(\", \"Failed :(\"\n        return edit_output\n\n    with gr.Blocks(css=\"footer {visibility: hidden}\") as demo:\n        txt_input = gr.Textbox(lines=3, label=\"Input Caption\", interactive=True, placeholder=\"Type image caption here...\")  # fmt: skip\n        txt_edit = gr.Textbox(lines=1, label=\"GPT-3 Instruction\", interactive=False)\n        txt_output = gr.Textbox(lines=3, label=\"GPT3 Edited Caption\", interactive=False)\n\n        with gr.Row():\n            clear_btn = gr.Button(\"Clear\")\n            random_btn = gr.Button(\"Random Input\")\n            generate_btn = gr.Button(\"Generate Instruction + Edited Caption\")\n\n            clear_btn.click(fn=lambda: (\"\", \"\", \"\"), inputs=[], outputs=[txt_input, txt_edit, txt_output])\n            random_btn.click(fn=click_random, inputs=[], outputs=[txt_input])\n            generate_btn.click(fn=click_generate, inputs=[txt_input], outputs=[txt_edit, txt_output])\n\n    demo.launch(share=True)\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--openai-api-key\", required=True, type=str)\n    parser.add_argument(\"--openai-model\", required=True, type=str)\n    args = parser.parse_args()\n    openai.api_key = args.openai_api_key\n    main(args.openai_model)\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "stable_diffusion",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}