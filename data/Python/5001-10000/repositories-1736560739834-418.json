{
  "metadata": {
    "timestamp": 1736560739834,
    "page": 418,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "OpenNMT/OpenNMT-py",
      "stars": 6802,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.2353515625,
          "content": "# repo-specific stuff\npred.txt\nmulti-bleu.perl\n*.pt\n\\#*#\n.idea\n*.sublime-*\n.DS_Store\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# Tensorboard\nruns/\n\n*.kdev4\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 17.8095703125,
          "content": "\n**Notes on versioning**\n\n## [Unreleased]\n\n## [3.5.1](https://github.com/OpenNMT/OpenNMT-py/tree/3.5.1) (2024-03-13)\n\n* Further fixes\n* added wikitext runs\n\n## [3.5.0](https://github.com/OpenNMT/OpenNMT-py/tree/3.5.0) (2024-02-22)\n\n* Further improvements and fixes\n* Suport for AWQ models\n* Add n_best for topp/topk generation\n* Support MoE (MIxtral) inference\n* Extend HF models converter\n* use flash_attn_with_kvcache for faster inference\n* Add wikitext2 PPL computation\n* Support for Phi-2 models\n\n## [3.4.3](https://github.com/OpenNMT/OpenNMT-py/tree/3.4.3) (2023-11-2)\n\n* Further improvements to beam search and decoding\n* New indexing \"in bucket\" for faster inference cf #2496\n* Code cleanup\n* Fix int8 for CPU dynamic quantization (still slow...)\n\n## [3.4.2](https://github.com/OpenNMT/OpenNMT-py/tree/3.4.2) (2023-10-20)\n\n* torch 2.1 (scaled_dot_product improvements)\n* Mistral 7B sliding window\n* Speed-up inference\n* flash attention 2 (with sliding window) >= v2.3.1\n* use FusedRMSNorm from apex if available\n* fixed attn_debug\n\n## [3.4.1](https://github.com/OpenNMT/OpenNMT-py/tree/3.4.1) (2023-09-26)\n\n* bug fixes\n* torch 2.x requirement (flash attention requires it)\n* zero-out the prompt loss in LM finetuning\n* batching sorted on src then tgt instead of max len\n* six dependancy\n\n## [3.4.0](https://github.com/OpenNMT/OpenNMT-py/tree/3.4.0) (2023-09-06)\n\n* bitsandbytes 4/8 bit quantization at inference\n* MMLU-FR results and scoring\n* flan-T5 support\n* flash attention\n* terminology transform\n* tensor parallelism (inference, training)\n\n## [3.3.0](https://github.com/OpenNMT/OpenNMT-py/tree/3.3.0) (2023-06-22)\n\n* Switch to pytorch 2.0.1\n* Eval LLM with MMLU benchmark\n* Fix Falcon 40B conversion / finetuning / inference\n* Plugin encoder/decoder thanks @kleag / @n2oblife\n* Safetensors for model storage (beta)\n* finetuning config templates for supported LLMs\n\n\n## [3.2.0](https://github.com/OpenNMT/OpenNMT-py/tree/3.2.0) (2023-06-07)\n* Skip init during model build (way faster building)\n* Enable quantization of LoRA layers\n* Enable 4bit quantization from bitsandbytes (NF4 / FP4)\n* Enable \"some\" bnb.optim Optimizers for benchmarking purpose\n* Refactor model state_dict loading to enable pseudo lazy loading with move on GPU as it loads\n* Enable Gradient checkpointing for FFN, MHA, LoRA modules\n* Make FFN bias optional (same as QKV): llama, mpt, redpajama, openllama converters changed accordingly.\n  Convertv2_v3 set add_qkvbias=True, add_ffnbias=True.\n  load_checkpoint: if w1_bias detected in checkpoint then add_ffnbias=True\n* Add Multi Query attention\n* Add Parallel Residual attention\n* Add Falcon 7B converter\n\n## [3.1.3](https://github.com/OpenNMT/OpenNMT-py/tree/3.1.3) (2023-05-24)\n* Step-by-step Tuto for Vicuna replication thanks Lina\n* MosaicML MPT7B converter and support (Alibi embeddings)\n* Open Llama converter\n* Switch GCLD3 to Fasttext thanks ArtanieTheOne\n* fix coverage attention in beam decoding\n* fix ct2 keys for \"Llama / MPT7B based\" OpenNMT-y models\n\n## [3.1.2](https://github.com/OpenNMT/OpenNMT-py/tree/3.1.2) (2023-05-10)\n* fixes: transforms (normalize, clean, inlinetags)\n* Llama support (rotary embeddings, RMSNorm, Silu activation)\n* 8bit loading for specific layers (along with LoRa for other layers)\n* subword learner added to build_vocab\n\n## [3.1.1](https://github.com/OpenNMT/OpenNMT-py/tree/3.1.1) (2023-03-30)\n* fix major bug in 3.1.0 introduced with LoRa (3.1.0 not available)\n\n## [3.1.0](https://github.com/OpenNMT/OpenNMT-py/tree/3.1.0) (2023-03-27)\n* updated docs with Sphinx 6.4\n* Restore source features to v3 (thanks @anderleich)\n* add inline tags transform (thanks @panosk)\n* add docify transform to allow doc-level training / inference\n* fix NLLB training (decoder_start_token)\n* New! LoRa adapters to finetune big models (egs: NLLB 3.3B)\n* various bug fixes\n\n## [3.0.4](https://github.com/OpenNMT/OpenNMT-py/tree/3.0.4) (2023-02-06)\n* override_opts to override checkpoints opt when training from\n* normalize transform based on (Sacre)Moses scripts\n* uppercase transform for adhoc data augmentation\n* suffix transform\n* Fuzzy match transform\n* WMT17 detailed example\n* NLLB-200 (from Meta/FB) models support (after conversion)\n* various bug fixes\n\n## [3.0.3](https://github.com/OpenNMT/OpenNMT-py/tree/3.0.3) (2022-12-16)\n* fix loss normalization when using accum or nb GPU > 1\n* use native CrossEntropyLoss with Label Smoothing. reported loss/ppl impacted by LS\n* fix long-time coverage loss bug thanks Sanghyuk-Choi\n* fix detok at scoring / fix tokenization Subword_nmt + Sentencepiece\n* various small bugs fixed\n\n## [3.0.2](https://github.com/OpenNMT/OpenNMT-py/tree/3.0.2) (2022-12-07)\n* pyonmttok.Vocab is now pickable. dataloader switched to spawn. (MacOS/Windows compatible)\n* fix scoring with specific metrics (BLEU, TER)\n* fix tensorboard logging\n* fix dedup in batch iterator (only for TRAIN, was happening at inference also)\n* New: Change: tgt_prefix renamed to tgt_file_prefix\n* New: tgt_prefix / src_prefix used for \"prefix\" Transform (onmt/transforms/misc.py)\n* New: process transforms of buckets in batches (vs per example) / faster\n\n## [3.0.1](https://github.com/OpenNMT/OpenNMT-py/tree/3.0.1) (2022-11-23)\n\n* fix dynamic scoring\n* reinstate apex.amp level O1/O2 for benchmarking\n* New: LM distillation for NMT training\n* New: bucket_size ramp-up to avoid slow start\n* fix special tokens order\n* remove Library and add link to Yasmin's Tuto\n\n## [3.0.0](https://github.com/OpenNMT/OpenNMT-py/tree/3.0.0) (2022-11-3)\n\n* Removed completely torchtext. Use [Vocab object of pyonmttok](https://github.com/OpenNMT/Tokenizer/tree/master/bindings/python#vocabulary) instead\n* Dataloading changed accordingly with the use of pytorch Dataloader (num_workers)\n* queue_size / pool_factor no longer needed. bucket_size optimal value > 64K\n* options renamed: rnn_size => hidden_size (enc/dec_rnn_size => enc/dec_hid_size)\n* new tools/convertv2_v3.py to upgrade v2 models.pt\n* inference with length_penalty=avg is now the default\n* add_qkvbias (default false, but true for old model)\n\n## [2.3.0](https://github.com/OpenNMT/OpenNMT-py/tree/2.3.0) (2022-09-14)\n\n### New features\n* BLEU/TER (& custom) scoring during training and validation (#2198)\n* LM related tools (#2197)\n* Allow encoder/decoder freezing (#2176)\n* Dynamic data loading for inference (#2145)\n* Sentence-level scores at inference (#2196)\n* MBR and oracle reranking scoring tools (#2196)\n\n### Fixes and improvements\n* Updated beam exit condition (#2190)\n* Improve scores reporting (#2191)\n* Fix dropout scheduling (#2194)\n* Better catch CUDA ooms when training (#2195)\n* Fix source features support in inference and REST server (#2109)\n* Make REST server more flexible with dictionaries (#2104)\n* Fix target prefixing in LM decoding (#2099)\n\n## [2.2.0](https://github.com/OpenNMT/OpenNMT-py/tree/2.2.0) (2021-09-14)\n\n### New features\n* Support source features (thanks @anderleich !)\n\n### Fixes and improvements\n* Adaptations to relax torch version\n* Customizable transform statistics (#2059)\n* Adapt release code for ctranslate2 2.0\n\n## [2.1.2](https://github.com/OpenNMT/OpenNMT-py/tree/2.1.2) (2021-04-30)\n\n### Fixes and improvements\n*  Fix update_vocab for LM (#2056)\n\n## [2.1.1](https://github.com/OpenNMT/OpenNMT-py/tree/2.1.1) (2021-04-30)\n\n### Fixes and improvements\n* Fix potential deadlock (b1a4615)\n* Add more CT2 conversion checks (e4ab06c)\n\n## [2.1.0](https://github.com/OpenNMT/OpenNMT-py/tree/2.1.0) (2021-04-16)\n\n### New features\n* Allow vocab update when training from a checkpoint (cec3cc8, 2f70dfc)\n\n### Fixes and improvements\n* Various transforms related bug fixes\n* Fix beam warning and buffers reuse\n* Handle invalid lines in vocab file gracefully\n\n## [2.0.1](https://github.com/OpenNMT/OpenNMT-py/tree/2.0.1) (2021-01-27)\n\n### Fixes and improvements\n* Support embedding layer for larger vocabularies with GGNN (e8065b7)\n* Reorganize some inference options (9fb5f30)\n\n## [2.0.0](https://github.com/OpenNMT/OpenNMT-py/tree/2.0.0) (2021-01-20)\n\nFirst official release for OpenNMT-py major upgdate to 2.0!\n\n### New features\n* Language Model (GPT-2 style) training and inference\n* Nucleus (top-p) sampling decoding\n\n### Fixes and improvements\n* Fix some BART default values\n\n## [2.0.0rc2](https://github.com/OpenNMT/OpenNMT-py/tree/2.0.0rc2) (2020-11-10)\n\n### Fixes and improvements\n* Parallelize onmt_build_vocab (422d824)\n* Some fixes to the on-the-fly transforms\n* Some CTranslate2 related updates\n* Some fixes to the docs\n\n## [2.0.0rc1](https://github.com/OpenNMT/OpenNMT-py/tree/2.0.0rc1) (2020-09-25)\n\nThis is the first release candidate for OpenNMT-py major upgdate to 2.0.0!\n\nThe major idea behind this release is the -- almost -- complete **makeover of the data loading pipeline** . A new 'dynamic' paradigm is introduced, allowing to apply on the fly transforms to the data.\n\nThis has a few advantages, amongst which:\n\n* remove or drastically reduce the preprocessing required to train a model;\n* increase and simplify the possibilities of data augmentation and manipulation through on-the fly transforms.\n\nThese transforms can be specific **tokenization** methods, **filters**, **noising**, or **any custom transform** users may want to implement. Custom transform implementation is quite straightforward thanks to the existing base class and example implementations.\n\nYou can check out how to use this new data loading pipeline in the updated [docs and examples](https://opennmt.net/OpenNMT-py).\n\nAll the **readily available transforms** are described [here](https://opennmt.net/OpenNMT-py/FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms).\n\n### Performance\n\nGiven sufficient CPU resources according to GPU computing power, most of the transforms should not slow the training down. (Note: for now, one producer process per GPU is spawned -- meaning you would ideally need 2N CPU threads for N GPUs).\n\n### Breaking changes\n\nA few features are dropped, at least for now:\n\n* audio, image and video inputs;\n* source word features.\n\nSome very old checkpoints with previous fields and vocab structure are also incompatible with this new version.\n\nFor any user that still need some of these features, the previous codebase will be retained as [`legacy` in a separate branch](https://github.com/OpenNMT/OpenNMT-py/tree/legacy). It will no longer receive extensive development from the core team but PRs may still be accepted.\n\n\n-----\n\n## [1.2.0](https://github.com/OpenNMT/OpenNMT-py/tree/1.2.0) (2020-08-17)\n### Fixes and improvements\n* Support pytorch 1.6 (e813f4d, eaaae6a)\n* Support official torch 1.6 AMP for mixed precision training (2ac1ed0)\n* Flag to override batch_size_multiple in FP16 mode, useful in some memory constrained setups (23e5018)\n* Pass a dict and allow custom options in preprocess/postprocess functions of REST server (41f0c02, 8ec54d2)\n* Allow different tokenization for source and target in REST server (bb2d045, 4659170)\n* Various bug fixes\n\n### New features\n* Gated Graph Sequence Neural Networks encoder (11e8d0), thanks @SteveKommrusch\n* Decoding with a target prefix (95aeefb, 0e143ff, 91ab592), thanks @Zenglinxiao\n\n## [1.1.1](https://github.com/OpenNMT/OpenNMT-py/tree/1.1.1) (2020-03-20)\n### Fixes and improvements\n* Fix backcompatibility when no 'corpus_id' field (c313c28)\n\n## [1.1.0](https://github.com/OpenNMT/OpenNMT-py/tree/1.1.0) (2020-03-19)\n### New features\n* Support CTranslate2 models in REST server (91d5d57)\n* Extend support for custom preprocessing/postprocessing function in REST server by using return dictionaries (d14613d, 9619ac3, 92a7ba5)\n* Experimental: BART-like source noising (5940dcf)\n\n### Fixes and improvements\n* Add options to CTranslate2 release (e442f3f)\n* Fix dataset shard order (458fc48)\n* Rotate only the server logs, not training (189583a)\n* Fix alignment error with empty prediction (91287eb)\n\n## [1.0.2](https://github.com/OpenNMT/OpenNMT-py/tree/1.0.2) (2020-03-05)\n### Fixes and improvements\n* Enable CTranslate2 conversion of Transformers with relative position (db11135)\n* Adapt `-replace_unk` to use with learned alignments if they exist (7625b53)\n\n## [1.0.1](https://github.com/OpenNMT/OpenNMT-py/tree/1.0.1) (2020-02-17)\n### Fixes and improvements\n* Ctranslate2 conversion handled in release script (1b50e0c)\n* Use `attention_dropout` properly in MHA (f5c9cd4)\n* Update apex FP16_Optimizer path (d3e2268)\n* Some REST server optimizations\n* Fix and add some docs\n\n## [1.0.0](https://github.com/OpenNMT/OpenNMT-py/tree/1.0.0) (2019-10-01)\n### New features\n* Implementation of \"Jointly Learning to Align & Translate with Transformer\" (@Zenglinxiao)\n\n### Fixes and improvements\n* Add nbest support to REST server (@Zenglinxiao)\n* Merge greedy and beam search codepaths (@Zenglinxiao)\n* Fix \"block ngram repeats\" (@KaijuML, @pltrdy)\n* Small fixes, some more docs\n\n## [1.0.0.rc2](https://github.com/OpenNMT/OpenNMT-py/tree/1.0.0.rc1) (2019-10-01)\n* Fix Apex / FP16 training (Apex new API is buggy)\n* Multithread preprocessing way faster (Thanks @francoishernandez)\n* Pip Installation v1.0.0.rc1 (thanks @pltrdy)\n\n## [0.9.2](https://github.com/OpenNMT/OpenNMT-py/tree/0.9.2) (2019-09-04)\n* Switch to Pytorch 1.2\n* Pre/post processing on the translation server\n* option to remove the FFN layer in AAN + AAN optimization (faster)\n* Coverage loss (per Abisee paper 2017) implementation\n* Video Captioning task: Thanks Dylan Flaute!\n* Token batch at inference\n* Small fixes and add-ons\n\n\n## [0.9.1](https://github.com/OpenNMT/OpenNMT-py/tree/0.9.1) (2019-06-13)\n* New mechanism for MultiGPU training \"1 batch producer / multi batch consumers\"\n  resulting in big memory saving when handling huge datasets\n* New APEX AMP (mixed precision) API\n* Option to overwrite shards when preprocessing\n* Small fixes and add-ons\n\n## [0.9.0](https://github.com/OpenNMT/OpenNMT-py/tree/0.9.0) (2019-05-16)\n* Faster vocab building when processing shards (no reloading)\n* New dataweighting feature\n* New dropout scheduler.\n* Small fixes and add-ons\n\n## [0.8.2](https://github.com/OpenNMT/OpenNMT-py/tree/0.8.2) (2019-02-16)\n* Update documentation and Library example\n* Revamp args\n* Bug fixes, save moving average in FP32\n* Allow FP32 inference for FP16 models\n\n## [0.8.1](https://github.com/OpenNMT/OpenNMT-py/tree/0.8.1) (2019-02-12)\n* Update documentation\n* Random sampling scores fixes\n* Bug fixes\n\n## [0.8.0](https://github.com/OpenNMT/OpenNMT-py/tree/0.8.0) (2019-02-09)\n* Many fixes and code cleaning thanks @flauted, @guillaumekln\n* Datasets code refactor (thanks @flauted) you need to r-preeprocess datasets\n\n### New features\n* FP16 Support: Experimental, using Apex, Checkpoints may break in future version.\n* Continuous exponential moving average (thanks @francoishernandez, and Marian)\n* Relative positions encoding (thanks @francoishernanndez, and Google T2T)\n* Deprecate the old beam search, fast batched beam search supports all options\n\n\n## [0.7.2](https://github.com/OpenNMT/OpenNMT-py/tree/0.7.2) (2019-01-31)\n* Many fixes and code cleaning thanks @bpopeters, @flauted, @guillaumekln\n\n### New features\n* Multilevel fields for better handling of text featuer embeddinggs. \n\n\n## [0.7.1](https://github.com/OpenNMT/OpenNMT-py/tree/0.7.1) (2019-01-24)\n* Many fixes and code refactoring thanks @bpopeters, @flauted, @guillaumekln\n\n### New features\n* Random sampling thanks @daphnei\n* Enable sharding for huge files at translation\n\n## [0.7.0](https://github.com/OpenNMT/OpenNMT-py/tree/0.7.0) (2019-01-02)\n* Many fixes and code refactoring thanks @benopeters\n* Migrated to Pytorch 1.0\n\n## [0.6.0](https://github.com/OpenNMT/OpenNMT-py/tree/0.6.0) (2018-11-28)\n* Many fixes and code improvements\n* New: Ability to load a yml config file. See examples in config folder.\n\n## [0.5.0](https://github.com/OpenNMT/OpenNMT-py/tree/0.5.0) (2018-10-24)\n* Fixed advance n_best beam in translate_batch_fast\n* Fixed remove valid set vocab from total vocab\n* New: Ability to reset optimizer when using train_from\n* New: create_vocabulary tool + fix when loading existing vocab.\n\n## [0.4.1](https://github.com/OpenNMT/OpenNMT-py/tree/0.4.1) (2018-10-11)\n* Fixed preprocessing files names, cleaning intermediary files.\n\n## [0.4.0](https://github.com/OpenNMT/OpenNMT-py/tree/0.4.0) (2018-10-08)\n* Fixed Speech2Text training (thanks Yuntian)\n\n* Removed -max_shard_size, replaced by -shard_size = number of examples in a shard.\n  Default value = 1M which works fine in most Text dataset cases. (will avoid Ram OOM in most cases)\n\n\n## [0.3.0](https://github.com/OpenNMT/OpenNMT-py/tree/0.3.0) (2018-09-27)\n* Now requires Pytorch 0.4.1\n\n* Multi-node Multi-GPU with Torch Distributed\n\n  New options are:\n  -master_ip: ip address of the master node\n  -master_port: port number of th emaster node\n  -world_size = total number of processes to be run (total GPUs accross all nodes)\n  -gpu_ranks = list of indices of processes accross all nodes\n\n* gpuid is deprecated\nSee examples in https://github.com/OpenNMT/OpenNMT-py/blob/master/docs/source/FAQ.md\n\n* Fixes to img2text now working\n\n* New sharding based on number of examples\n\n* Fixes to avoid 0.4.1 deprecated functions.\n\n\n## [0.2.1](https://github.com/OpenNMT/OpenNMT-py/tree/0.2.1) (2018-08-31)\n\n### Fixes and improvements\n\n* First compatibility steps with Pytorch 0.4.1 (non breaking)\n* Fix TranslationServer (when various request try to load the same model at the same time)\n* Fix StopIteration error (python 3.7)\n\n### New features\n* Ensemble at inference (thanks @Waino)\n\n## [0.2](https://github.com/OpenNMT/OpenNMT-py/tree/v0.2) (2018-08-28)\n\n### improvements\n\n* Compatibility fixes with Pytorch 0.4 / Torchtext 0.3\n* Multi-GPU based on Torch Distributed\n* Average Attention Network (AAN) for the Transformer (thanks @francoishernandez )\n* New fast beam search (see -fast in translate.py) (thanks @guillaumekln)\n* Sparse attention / sparsemax (thanks to @bpopeters)\n* Refactoring of many parts of the code base:\n - change from -epoch to -train_steps -valid_steps (see opts.py)\n - reorg of the logic train => train_multi / train_single => trainer\n* Many fixes / improvements in the translationserver (thanks @pltrdy @francoishernandez)\n* fix BPTT\n\n## [0.1](https://github.com/OpenNMT/OpenNMT-py/tree/v0.1) (2018-06-08)\n\n### First and Last Release using Pytorch 0.3.x\n\n\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 4.029296875,
          "content": "# Contributors\n\nOpenNMT-py is a community developed project and we love developer contributions.\n\n## Guidelines\nBefore sending a PR, please do this checklist first:\n\n- Please run `onmt/tests/pull_request_chk.sh` and fix any errors. When adding new functionality, also add tests to this script. Included checks:\n    1. flake8 check for coding style;\n    2. unittest;\n    3. continuous integration tests listed in `.travis.yml`.\n- When adding/modifying class constructor, please make the arguments as same naming style as its superclass in PyTorch.\n- If your change is based on a paper, please include a clear comment and reference in the code (more on that below).\n\n### Docstrings\nAbove all, try to follow the Google docstring format\n([Napoleon example](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html),\n[Google styleguide](http://google.github.io/styleguide/pyguide.html)).\nThis makes it easy to include your contributions in the Sphinx documentation. And, do feel free\nto autodoc your contributions in the API ``.rst`` files in the `docs/source` folder! If you do, check that\nyour additions look right.\n\n```bash\ncd docs\n# install some dependencies if necessary:\n# recommonmark, sphinx_rtd_theme, sphinxcontrib-bibtex\nmake html\nfirefox build/html/main.html  # or your browser of choice\n```\n\nSome particular advice:\n- Try to follow Python 3 [``typing`` module](https://docs.python.org/3/library/typing.html) conventions when documenting types.\n    - Exception: use \"or\" instead of unions for more readability\n    - For external types, use the full \"import name\". Common abbreviations (e.g. ``np``) are acceptable.\n      For ``torch.Tensor`` types, the ``torch.`` is optional.\n    - Please don't use tics like `` (`str`) `` or rst directives like `` (:obj:`str`) ``. Napoleon handles types\n      very well without additional help, so avoid the clutter.\n- [Google docstrings don't support multiple returns](https://stackoverflow.com/questions/29221551/can-sphinx-napoleon-document-function-returning-multiple-arguments).\nFor multiple returns, the following works well with Sphinx and is still very readable.\n  ```python\n  def foo(a, b):\n      \"\"\"This is my docstring.\n\n      Args:\n          a (object): Something.\n          b (class): Another thing.\n\n      Returns:\n          (object, class):\n\n          * a: Something or rather with a long\n            description that spills over.\n          * b: And another thing.\n      \"\"\"\n\n      return a, b\n  ```\n- When citing a paper, avoid directly linking in the docstring! Add a Bibtex entry to `docs/source/refs.bib`.\nE.g., to cite \"Attention Is All You Need\", visit [arXiv](https://arxiv.org/abs/1706.03762), choose the\n[bibtext](https://dblp.uni-trier.de/rec/bibtex/journals/corr/VaswaniSPUJGKP17) link, search `docs/source/refs.bib`\nusing `CTRL-F` for `DBLP:journals/corr/VaswaniSPUJGKP17`, and if you do not find it then copy-paste the\ncitation into `refs.bib`. Then, in your docstring, use ``:cite:`DBLP:journals/corr/VaswaniSPUJGKP17` ``.\n    - However, a link is better than nothing.\n- Please document tensor shapes. Prefer the format\n  ``` ``(a, b, c)`` ```. This style is easy to read, allows using ``x`` for multplication, and is common\n  (PyTorch uses a few variations on the parentheses format, AllenNLP uses exactly this format, Fairseq uses\n  the parentheses format with single ticks).\n    - Again, a different style is better than no shape documentation.\n- Please avoid unnecessary space characters, try to capitalize, and try to punctuate.\n\n  For multi-line docstrings, add a blank line after the closing ``\"\"\"``.\n  Don't use a blank line before the closing quotes.\n\n  ``\"\"\" not this \"\"\"`` ``\"\"\"This.\"\"\"``\n\n  ```python\n  \"\"\"\n      Not this.\n  \"\"\"\n  ```\n  ```python\n  \"\"\"This.\"\"\"\n  ```\n\n  This note is the least important. Focus on content first, but remember that consistent docs look good.\n- Be sensible about the first line. Generally, one stand-alone summary line (per the Google guidelines) is good.\n  Sometimes, it's better to cut directly to the args or an extended description. It's always acceptable to have a\n  \"trailing\" citation.\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.046875,
          "content": "MIT License\n\nCopyright (c) 2017-Present OpenNMT\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.5556640625,
          "content": "# Announcement: OpenNMT-py is no longer actively supported.\n\nWe started a new project [Eole](https://eole-nlp.github.io/eole/) available on [Github](https://github.com/eole-nlp/eole)\n\nIt is a spin-off of OpenNMT-py in terms of features but we revamped a lot of stuff.\n\nEole handles NMT, LLM, Encoders as well as a new concept of Estimator within a NMT Model See this [post](https://medium.com/p/05b00b271a47) and this [news](https://www.linkedin.com/posts/vincentnguyenngoc_embarrassingly-small-english-to-german-model-activity-7203400634727841792-FCre?utm_source=share&utm_medium=member_desktop)\n\nIf you are a developer, switch now. If you are a user only, then we will publish the first py-pi versions shortly.\n\n\n# OpenNMT-py: Open-Source Neural Machine Translation and (Large) Language Models\n\n[![Build Status](https://github.com/OpenNMT/OpenNMT-py/workflows/Lint%20&%20Tests/badge.svg)](https://github.com/OpenNMT/OpenNMT-py/actions)\n[![Documentation](https://img.shields.io/badge/docs-latest-blue.svg)](https://opennmt.net/OpenNMT-py/)\n[![Gitter](https://badges.gitter.im/OpenNMT/OpenNMT-py.svg)](https://gitter.im/OpenNMT/OpenNMT-py?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n[![Forum](https://img.shields.io/discourse/status?server=https%3A%2F%2Fforum.opennmt.net%2F)](https://forum.opennmt.net/)\n\nOpenNMT-py is the [PyTorch](https://github.com/pytorch/pytorch) version of the [OpenNMT](https://opennmt.net) project, an open-source (MIT) neural machine translation (and beyond!) framework. It is designed to be research friendly to try out new ideas in translation, language modeling, summarization, and many other NLP tasks. Some companies have proven the code to be production ready.\n\nWe love contributions! Please look at issues marked with the [contributions welcome](https://github.com/OpenNMT/OpenNMT-py/issues?q=is%3Aissue+is%3Aopen+label%3A%22contributions+welcome%22) tag.\n\nBefore raising an issue, make sure you read the requirements and the [Full Documentation](https://opennmt.net/OpenNMT-py/) examples.\n\nUnless there is a bug, please use the [Forum](https://forum.opennmt.net) or [Gitter](https://gitter.im/OpenNMT/OpenNMT-py) to ask questions.\n\n----\n## For beginners:\n\nThere is a step-by-step and explained tuto (Thanks to Yasmin Moslem): [Tutorial](https://github.com/ymoslem/OpenNMT-Tutorial)\n\nPlease try to read and/or follow before raising newbies issues.\n\nOtherwise you can just have a look at the [Quickstart](https://opennmt.net/OpenNMT-py/quickstart.html) steps\n\n----\n## New:\n\n* You will need Pytorch v2 preferably v2.2 which fixes some `scaled_dot_product_attention` issues\n* LLM support with converters for: Llama (+ Mistral), OpenLlama, Redpajama, MPT-7B, Falcon.\n* Support for 8bit and 4bit quantization along with LoRA adapters, with or without checkpointing.\n* You can finetune 7B and 13B models on a single RTX 24GB with 4-bit quantization.\n* Inference can be forced in 4/8bit using the same layer quantization as in finetuning.\n* Tensor parallelism when the model does not fit on one GPU's memory (both training and inference)\n* Once your model is finetuned you can run inference either with OpenNMT-py or faster with CTranslate2.\n* MMLU evaluation script, see results [here](https://github.com/OpenNMT/OpenNMT-py/blob/master/eval_llm/MMLU/readme.md)\n\nFor all usecases including NMT, you can now use Multiquery instead of Multihead attention (faster at training and inference) and remove biases from all Linear (QKV as well as FeedForward modules).\n\n\nIf you used previous versions of OpenNMT-py, you can check the [Changelog](https://github.com/OpenNMT/OpenNMT-py/blob/master/CHANGELOG.md) or the [Breaking Changes](https://github.com/OpenNMT/OpenNMT-py/blob/master/docs/source/changes.md)\n\n----\n\n## Tutorials:\n\n* How to replicate Vicuna with a 7B or 13B llama (or Open llama, MPT-7B, Redpajama)  Language Model: [Tuto Vicuna](https://github.com/OpenNMT/OpenNMT-py/blob/master/docs/source/examples/replicate_vicuna/ReplicateVicuna.md)\n* How to finetune NLLB-200 with your dataset: [Tuto Finetune NLLB-200](https://forum.opennmt.net/t/finetuning-and-curating-nllb-200-with-opennmt-py/5238)\n* How to create a simple OpenNMT-py REST Server: [Tuto REST](https://forum.opennmt.net/t/simple-opennmt-py-rest-server/1392)\n* How to create a simple Web Interface: [Tuto Streamlit](https://forum.opennmt.net/t/simple-web-interface/4527)\n* Replicate the WMT17 en-de experiment: [WMT17 ENDE](https://github.com/OpenNMT/OpenNMT-py/blob/master/docs/source/examples/wmt17/Translation.md)\n\n----\n\n## Setup\n\n### Using docker\n\nTo facilitate setup and reproducibility, some docker images are made available via the Github Container Registry:\nhttps://github.com/OpenNMT/OpenNMT-py/pkgs/container/opennmt-py\n\nYou can adapt the workflow and build your own image(s) depending on specific needs by using `build.sh` and `Dockerfile` in the `docker` directory of the repo.\n\n```\ndocker pull ghcr.io/opennmt/opennmt-py:3.4.3-ubuntu22.04-cuda12.1\n```\n\nExample oneliner to run a container and open a bash shell within it\n```\ndocker run --rm -it --runtime=nvidia ghcr.io/opennmt/opennmt-py:test-ubuntu22.04-cuda12.1\n```\nNote: you need to have the [Nvidia Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) (formerly nvidia-docker) installed to properly take advantage of the CUDA/GPU features.\n\nDepending on your needs you can add various flags:\n- `-p 5000:5000` to forward some exposed port from your container to your host;\n- `-v /some/local/directory:/some/container/directory` to mount some local directory to some container directory;\n- `--entrypoint some_command` to directly run some specific command as the container entry point (instead of the default bash shell);\n\n### Installing locally\n\nOpenNMT-py requires:\n\n- Python >= 3.8\n- PyTorch >= 2.0 <2.2\n\nInstall `OpenNMT-py` from `pip`:\n```bash\npip install OpenNMT-py\n```\n\nor from the source:\n```bash\ngit clone https://github.com/OpenNMT/OpenNMT-py.git\ncd OpenNMT-py\npip install -e .\n```\n\nNote: if you encounter a `MemoryError` during installation, try to use `pip` with `--no-cache-dir`.\n\n*(Optional)* Some advanced features (e.g. working pretrained models or specific transforms) require extra packages, you can install them with:\n\n```bash\npip install -r requirements.opt.txt\n```\n\n### Manual installation of some dependencies\n\nApex is highly recommended to have fast performance (especially the legacy fusedadam optimizer and FusedRMSNorm)\n\n```shell\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip3 install -v --no-build-isolation --config-settings --build-option=\"--cpp_ext --cuda_ext --deprecated_fused_adam --xentropy --fast_multihead_attn\" ./\ncd ..\n```\n\nFlash attention:\n\nAs of Oct. 2023 flash attention 1 has been upstreamed to pytorch v2 but it is recommended to use flash attention 2 with v2.3.1 for sliding window attention support.\n\nWhen using regular `position_encoding=True` or Rotary with `max_relative_positions=-1` OpenNMT-py will try to use an optimized dot-product path.\n\nif you want to use [flash attention](https://github.com/Dao-AILab/flash-attention#installation-and-features) then you need to manually install it first:\n\n```bash\npip install flash-attn --no-build-isolation\n```\n\nif flash attention 2 is not installed, then we will use `F.scaled_dot_product_attention` from pytorch 2.x\n\nWhen using `max_relative_positions > 0` or Alibi `max_relative_positions=-2` OpenNMT-py will use its legacy code for matrix multiplications.\n\nflash attention and `F.scaled_dot_product_attention` are a bit faster and saves some GPU memory.\n\n\nAWQ:\n\nIf you want to run inference or quantize an AWQ model you will need AutoAWQ.\n\nFor [AutoAWQ](https://github.com/casper-hansen/AutoAWQ):\n    pip install autoawq\n\n\n## Documentation & FAQs\n\n[Full HTML Documentation](https://opennmt.net/OpenNMT-py/quickstart.html)\n\n[FAQs](https://github.com/OpenNMT/OpenNMT-py/blob/master/docs/source/FAQ.md)\n\n## Acknowledgements\n\nOpenNMT-py is run as a collaborative open-source project.\nProject was incubated by Systran and Harvard NLP in 2016 in Lua and ported to Pytorch in 2017.\n\nCurrent maintainers (since 2018):\n\n[François Hernandez](https://github.com/francoishernandez)\n[Vincent Nguyen](https://github.com/vince62s) (Seedfall)\n\n## Citation\n\nIf you are using OpenNMT-py for academic work, please cite the initial [system demonstration paper](https://www.aclweb.org/anthology/P17-4012) published in ACL 2017:\n\n```\n@misc{klein2018opennmt,\n      title={OpenNMT: Neural Machine Translation Toolkit}, \n      author={Guillaume Klein and Yoon Kim and Yuntian Deng and Vincent Nguyen and Jean Senellart and Alexander M. Rush},\n      year={2018},\n      eprint={1805.11462},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.3056640625,
          "content": "# Security Policy\n\n## Supported Versions\n\n| Version | Supported          |\n| ------- | ------------------ |\n| 2.2.x   | :white_check_mark: |\n| 1.2.x   | :x:                |\n\n## Reporting a Vulnerability\n\nPlease open an issue, or contact one of the maintainers via [gitter](https://gitter.im/OpenNMT/OpenNMT-py).\n"
        },
        {
          "name": "available_models",
          "type": "tree",
          "content": null
        },
        {
          "name": "build_vocab.py",
          "type": "blob",
          "size": 0.09765625,
          "content": "#!/usr/bin/env python\nfrom onmt.bin.build_vocab import main\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval_llm",
          "type": "tree",
          "content": null
        },
        {
          "name": "onmt",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.opt.txt",
          "type": "blob",
          "size": 0.1162109375,
          "content": "pyrouge\nsentencepiece>=0.1.94,<0.1.98\nsubword-nmt>=0.3.7\nrapidfuzz\nscipy\nbitsandbytes>=0.41.2\nsafetensors\nspacy\ngradio\n"
        },
        {
          "name": "server.py",
          "type": "blob",
          "size": 0.0927734375,
          "content": "#!/usr/bin/env python\nfrom onmt.bin.server import main\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.0625,
          "content": "[flake8]\nmax-line-length = 100\nignore =\n  E203,\n  E731,\n  W503,\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.4658203125,
          "content": "#!/usr/bin/env python\nfrom setuptools import setup, find_packages\nfrom os import path\n\nthis_directory = path.abspath(path.dirname(__file__))\nwith open(path.join(this_directory, \"README.md\"), encoding=\"utf-8\") as f:\n    long_description = f.read()\n\nsetup(\n    name=\"OpenNMT-py\",\n    description=\"A python implementation of OpenNMT\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    version=\"3.5.1\",\n    packages=find_packages(),\n    project_urls={\n        \"Documentation\": \"http://opennmt.net/OpenNMT-py/\",\n        \"Forum\": \"http://forum.opennmt.net/\",\n        \"Gitter\": \"https://gitter.im/OpenNMT/OpenNMT-py\",\n        \"Source\": \"https://github.com/OpenNMT/OpenNMT-py/\",\n    },\n    python_requires=\">=3.9\",\n    install_requires=[\n        \"torch>=2.1,<2.3\",\n        \"configargparse\",\n        \"ctranslate2>=4,<5\",\n        \"tensorboard>=2.3\",\n        \"flask\",\n        \"waitress\",\n        \"pyonmttok>=1.37,<2\",\n        \"pyyaml\",\n        \"sacrebleu\",\n        \"rapidfuzz\",\n        \"pyahocorasick\",\n        \"fasttext-wheel\",\n        \"spacy\",\n        \"six\",\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"onmt_server=onmt.bin.server:main\",\n            \"onmt_train=onmt.bin.train:main\",\n            \"onmt_translate=onmt.bin.translate:main\",\n            \"onmt_release_model=onmt.bin.release_model:main\",\n            \"onmt_average_models=onmt.bin.average_models:main\",\n            \"onmt_build_vocab=onmt.bin.build_vocab:main\",\n        ],\n    },\n)\n"
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 0.091796875,
          "content": "#!/usr/bin/env python\nfrom onmt.bin.train import main\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "translate.py",
          "type": "blob",
          "size": 0.095703125,
          "content": "#!/usr/bin/env python\nfrom onmt.bin.translate import main\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
      ]
    }
  ]
}