{
  "metadata": {
    "timestamp": 1736560444005,
    "page": 19,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "EpistasisLab/tpot",
      "stars": 9799,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".appveyor.yml",
          "type": "blob",
          "size": 0.8076171875,
          "content": "build: false\n\nenvironment:\n  matrix:\n    - PYTHON_VERSION: 3.7\n      MINICONDA: C:/Miniconda36-x64\n      DASK_ML_VERSION: 1.0.0\n\nplatform:\n  - x64\n\ninit:\n  - ECHO %PYTHON_VERSION% %MINICONDA%\n  - ECHO conda --version\n\ninstall:\n  - set PATH=%MINICONDA%;%MINICONDA%\\\\Scripts;%PATH%\n  - conda config --set always_yes yes --set changeps1 no\n  - conda update -q conda\n  - conda info -a\n  - conda create -q -n test-environment python=%PYTHON_VERSION% numpy scipy scikit-learn nose cython pandas joblib\n  - activate test-environment\n  - pip install deap tqdm update_checker stopit xgboost dask[delayed] dask[dataframe] cloudpickle>=1.5.0 fsspec>=0.3.3 dask_ml==%DASK_ML_VERSION%\n  - pip install torch==1.3.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n  - pip install imbalanced-learn\n\n\ntest_script:\n  - nosetests -s -v\n"
        },
        {
          "name": ".coveragerc",
          "type": "blob",
          "size": 0.052734375,
          "content": "[run]\nbranch = True\nsource = tpot\ninclude = tpot/*.py\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.9736328125,
          "content": "# Static Documentation\ndocs/site/\n\n# System-specific temporary and index files\n.DS_Store\n.swp\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nvenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# IPython Notebooks\n.ipynb_checkpoints\n.ipynb_checkpoints/*\n\ndocs/sources/examples/.Rhistory\n\ntesting*/\n\n# PyCharm\n.idea\n\nanalyze-oj2-tpot-mdr.ipynb\n\ntpot-mdr-demo.ipynb\n\ngithub.com/\n\n.vscode/\n"
        },
        {
          "name": ".landscape.yaml",
          "type": "blob",
          "size": 0.06640625,
          "content": "doc-warnings: yes\n\nignore-patterns:\n  - __init__.py\n  - tutorials/*\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 0.9931640625,
          "content": "language: python\nmatrix:\n  # let's start simple:\n  include:\n  - name: \"Python 3.7 on Xenial Linux\"\n    dist: xenial        # required for Python >= 3.7\n    env: PYTHON_VERSION=\"3.7\"  DASK_ML_VERSION=\"1.0.0\"\n    before_install:\n      - wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\n  - name: \"Python 3.7 on Xenial Linux with coverage\"\n    dist: xenial        # required for Python >= 3.7\n    env: PYTHON_VERSION=\"3.7\"  COVERAGE=\"true\"  DASK_ML_VERSION=\"1.0.0\"\n    before_install:\n      - wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\ninstall: source ./ci/.travis_install.sh\nscript: bash ./ci/.travis_test.sh\nafter_success:\n    # Ignore coveralls failures as the coveralls server is not very reliable\n    # but we don't want travis to report a failure in the github UI just\n    # because the coverage report failed to be published.\n    - if [[ \"$COVERAGE\" == \"true\" ]]; then coveralls || echo \"failed\"; fi\ncache: apt\nsudo: false\n"
        },
        {
          "name": "ISSUE_TEMPLATE.md",
          "type": "blob",
          "size": 0.837890625,
          "content": "[provide general introduction to the issue and why it is relevant to this repository]\n\n## Context of the issue\n\n[provide more detailed introduction to the issue itself and why it is relevant]\n\n[the remaining entries are only necessary if you are reporting a bug]\n\n## Process to reproduce the issue\n\n[ordered list the process to finding and recreating the issue, example below]\n\n1. User creates TPOT instance\n2. User calls TPOT `fit()` function with training data\n3. TPOT crashes with a `KeyError` after 5 generations\n\n## Expected result\n\n[describe what you would expect to have resulted from this process]\n\n## Current result\n\n[describe what you currently experience from this process, and thereby explain the bug]\n\n## Possible fix\n\n[not necessary, but suggest fixes or reasons for the bug]\n\n## `name of issue` screenshot\n\n[if relevant, include a screenshot]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 7.4716796875,
          "content": "                   GNU LESSER GENERAL PUBLIC LICENSE\n                       Version 3, 29 June 2007\n\n Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n\n  This version of the GNU Lesser General Public License incorporates\nthe terms and conditions of version 3 of the GNU General Public\nLicense, supplemented by the additional permissions listed below.\n\n  0. Additional Definitions.\n\n  As used herein, \"this License\" refers to version 3 of the GNU Lesser\nGeneral Public License, and the \"GNU GPL\" refers to version 3 of the GNU\nGeneral Public License.\n\n  \"The Library\" refers to a covered work governed by this License,\nother than an Application or a Combined Work as defined below.\n\n  An \"Application\" is any work that makes use of an interface provided\nby the Library, but which is not otherwise based on the Library.\nDefining a subclass of a class defined by the Library is deemed a mode\nof using an interface provided by the Library.\n\n  A \"Combined Work\" is a work produced by combining or linking an\nApplication with the Library.  The particular version of the Library\nwith which the Combined Work was made is also called the \"Linked\nVersion\".\n\n  The \"Minimal Corresponding Source\" for a Combined Work means the\nCorresponding Source for the Combined Work, excluding any source code\nfor portions of the Combined Work that, considered in isolation, are\nbased on the Application, and not on the Linked Version.\n\n  The \"Corresponding Application Code\" for a Combined Work means the\nobject code and/or source code for the Application, including any data\nand utility programs needed for reproducing the Combined Work from the\nApplication, but excluding the System Libraries of the Combined Work.\n\n  1. Exception to Section 3 of the GNU GPL.\n\n  You may convey a covered work under sections 3 and 4 of this License\nwithout being bound by section 3 of the GNU GPL.\n\n  2. Conveying Modified Versions.\n\n  If you modify a copy of the Library, and, in your modifications, a\nfacility refers to a function or data to be supplied by an Application\nthat uses the facility (other than as an argument passed when the\nfacility is invoked), then you may convey a copy of the modified\nversion:\n\n   a) under this License, provided that you make a good faith effort to\n   ensure that, in the event an Application does not supply the\n   function or data, the facility still operates, and performs\n   whatever part of its purpose remains meaningful, or\n\n   b) under the GNU GPL, with none of the additional permissions of\n   this License applicable to that copy.\n\n  3. Object Code Incorporating Material from Library Header Files.\n\n  The object code form of an Application may incorporate material from\na header file that is part of the Library.  You may convey such object\ncode under terms of your choice, provided that, if the incorporated\nmaterial is not limited to numerical parameters, data structure\nlayouts and accessors, or small macros, inline functions and templates\n(ten or fewer lines in length), you do both of the following:\n\n   a) Give prominent notice with each copy of the object code that the\n   Library is used in it and that the Library and its use are\n   covered by this License.\n\n   b) Accompany the object code with a copy of the GNU GPL and this license\n   document.\n\n  4. Combined Works.\n\n  You may convey a Combined Work under terms of your choice that,\ntaken together, effectively do not restrict modification of the\nportions of the Library contained in the Combined Work and reverse\nengineering for debugging such modifications, if you also do each of\nthe following:\n\n   a) Give prominent notice with each copy of the Combined Work that\n   the Library is used in it and that the Library and its use are\n   covered by this License.\n\n   b) Accompany the Combined Work with a copy of the GNU GPL and this license\n   document.\n\n   c) For a Combined Work that displays copyright notices during\n   execution, include the copyright notice for the Library among\n   these notices, as well as a reference directing the user to the\n   copies of the GNU GPL and this license document.\n\n   d) Do one of the following:\n\n       0) Convey the Minimal Corresponding Source under the terms of this\n       License, and the Corresponding Application Code in a form\n       suitable for, and under terms that permit, the user to\n       recombine or relink the Application with a modified version of\n       the Linked Version to produce a modified Combined Work, in the\n       manner specified by section 6 of the GNU GPL for conveying\n       Corresponding Source.\n\n       1) Use a suitable shared library mechanism for linking with the\n       Library.  A suitable mechanism is one that (a) uses at run time\n       a copy of the Library already present on the user's computer\n       system, and (b) will operate properly with a modified version\n       of the Library that is interface-compatible with the Linked\n       Version.\n\n   e) Provide Installation Information, but only if you would otherwise\n   be required to provide such information under section 6 of the\n   GNU GPL, and only to the extent that such information is\n   necessary to install and execute a modified version of the\n   Combined Work produced by recombining or relinking the\n   Application with a modified version of the Linked Version. (If\n   you use option 4d0, the Installation Information must accompany\n   the Minimal Corresponding Source and Corresponding Application\n   Code. If you use option 4d1, you must provide the Installation\n   Information in the manner specified by section 6 of the GNU GPL\n   for conveying Corresponding Source.)\n\n  5. Combined Libraries.\n\n  You may place library facilities that are a work based on the\nLibrary side by side in a single library together with other library\nfacilities that are not Applications and are not covered by this\nLicense, and convey such a combined library under terms of your\nchoice, if you do both of the following:\n\n   a) Accompany the combined library with a copy of the same work based\n   on the Library, uncombined with any other library facilities,\n   conveyed under the terms of this License.\n\n   b) Give prominent notice with the combined library that part of it\n   is a work based on the Library, and explaining where to find the\n   accompanying uncombined form of the same work.\n\n  6. Revised Versions of the GNU Lesser General Public License.\n\n  The Free Software Foundation may publish revised and/or new versions\nof the GNU Lesser General Public License from time to time. Such new\nversions will be similar in spirit to the present version, but may\ndiffer in detail to address new problems or concerns.\n\n  Each version is given a distinguishing version number. If the\nLibrary as you received it specifies that a certain numbered version\nof the GNU Lesser General Public License \"or any later version\"\napplies to it, you have the option of following the terms and\nconditions either of that published version or of any later version\npublished by the Free Software Foundation. If the Library as you\nreceived it does not specify a version number of the GNU Lesser\nGeneral Public License, you may choose any version of the GNU Lesser\nGeneral Public License ever published by the Free Software Foundation.\n\n  If the Library as you received it specifies that a proxy can decide\nwhether future versions of the GNU Lesser General Public License shall\napply, that proxy's public statement of acceptance of any version is\npermanent authorization for you to choose that version for the\nLibrary.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.11328125,
          "content": "include README.md LICENSE\nrecursive-include images *\nrecursive-include tpot *.py\nrecursive-include tests *.py *.csv\n"
        },
        {
          "name": "PULL_REQUEST_TEMPLATE.md",
          "type": "blob",
          "size": 0.6240234375,
          "content": "[please review the [Contribution Guidelines](http://epistasislab.github.io/tpot/contributing/) prior to submitting your pull request. go ahead and delete this line if you've already reviewed said guidelines.]\n\n## What does this PR do?\n\n\n\n## Where should the reviewer start?\n\n\n\n## How should this PR be tested?\n\n\n\n## Any background context you want to provide?\n\n\n\n## What are the relevant issues?\n\n[you can link directly to issues by entering # then the number of the issue, for example, #3 links to issue 3]\n\n## Screenshots (if appropriate)\n\n\n\n## Questions:\n\n- Do the docs need to be updated?\n- Does this PR add new (Python) dependencies?\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 12.181640625,
          "content": "Master status: [![Master Build Status - Mac/Linux](https://travis-ci.com/EpistasisLab/tpot.svg?branch=master)](https://travis-ci.com/EpistasisLab/tpot)\n[![Master Build Status - Windows](https://ci.appveyor.com/api/projects/status/b7bmpwpkjhifrm7v/branch/master?svg=true)](https://ci.appveyor.com/project/weixuanfu/tpot?branch=master)\n[![Master Coverage Status](https://coveralls.io/repos/github/EpistasisLab/tpot/badge.svg?branch=master)](https://coveralls.io/github/EpistasisLab/tpot?branch=master)\n\nDevelopment status: [![Development Build Status - Mac/Linux](https://travis-ci.com/EpistasisLab/tpot.svg?branch=development)](https://travis-ci.com/EpistasisLab/tpot/branches)\n[![Development Build Status - Windows](https://ci.appveyor.com/api/projects/status/b7bmpwpkjhifrm7v/branch/development?svg=true)](https://ci.appveyor.com/project/weixuanfu/tpot?branch=development)\n[![Development Coverage Status](https://coveralls.io/repos/github/EpistasisLab/tpot/badge.svg?branch=development)](https://coveralls.io/github/EpistasisLab/tpot?branch=development)\n\nPackage information: [![Python 3.7](https://img.shields.io/badge/python-3.7-blue.svg)](https://www.python.org/downloads/release/python-370/)\n[![License: LGPL v3](https://img.shields.io/badge/license-LGPL%20v3-blue.svg)](http://www.gnu.org/licenses/lgpl-3.0)\n[![PyPI version](https://badge.fury.io/py/TPOT.svg)](https://badge.fury.io/py/TPOT)\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/EpistasisLab/tpot/master/images/tpot-logo.jpg\" width=300 />\n</p>\n\n---\nTo try the ![NEW!](https://raw.githubusercontent.com/EpistasisLab/tpot/master/images/NEW-small.gif \"NEW!\") TPOT2 (*alpha*) please go [here](https://github.com/EpistasisLab/tpot2)!\n\n- - - -\n\n**TPOT** stands for **T**ree-based **P**ipeline **O**ptimization **T**ool. Consider TPOT your **Data Science Assistant**. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.\n\n![TPOT Demo](https://github.com/EpistasisLab/tpot/blob/master/images/tpot-demo.gif \"TPOT Demo\")\n\nTPOT will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.\n\n![An example Machine Learning pipeline](https://github.com/EpistasisLab/tpot/blob/master/images/tpot-ml-pipeline.png \"An example Machine Learning pipeline\")\n\n<p align=\"center\"><strong>An example Machine Learning pipeline</strong></p>\n\nOnce TPOT is finished searching (or you get tired of waiting), it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there.\n\n![An example TPOT pipeline](https://github.com/EpistasisLab/tpot/blob/master/images/tpot-pipeline-example.png \"An example TPOT pipeline\")\n\nTPOT is built on top of scikit-learn, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.\n\n**TPOT is still under active development** and we encourage you to check back on this repository regularly for updates.\n\nFor further information about TPOT, please see the [project documentation](http://epistasislab.github.io/tpot/).\n\n## License\n\nPlease see the [repository license](https://github.com/EpistasisLab/tpot/blob/master/LICENSE) for the licensing and usage information for TPOT.\n\nGenerally, we have licensed TPOT to make it as widely usable as possible.\n\n## Installation\n\nWe maintain the [TPOT installation instructions](http://epistasislab.github.io/tpot/installing/) in the documentation. TPOT requires a working installation of Python.\n\n## Usage\n\nTPOT can be used [on the command line](http://epistasislab.github.io/tpot/using/#tpot-on-the-command-line) or [with Python code](http://epistasislab.github.io/tpot/using/#tpot-with-code).\n\nClick on the corresponding links to find more information on TPOT usage in the documentation.\n\n## Examples\n\n### Classification\n\nBelow is a minimal working example with the optical recognition of handwritten digits dataset.\n\n```python\nfrom tpot import TPOTClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\ndigits = load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n                                                    train_size=0.75, test_size=0.25, random_state=42)\n\ntpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_digits_pipeline.py')\n```\n\nRunning this code should discover a pipeline that achieves about 98% testing accuracy, and the corresponding Python code should be exported to the `tpot_digits_pipeline.py` file and look similar to the following:\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tpot.builtins import StackingEstimator\nfrom tpot.export_utils import set_param_recursive\n\n# NOTE: Make sure that the outcome column is labeled 'target' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1)\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'], random_state=42)\n\n# Average CV score on the training set was: 0.9799428471757372\nexported_pipeline = make_pipeline(\n    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n    StackingEstimator(estimator=LogisticRegression(C=0.1, dual=False, penalty=\"l1\")),\n    RandomForestClassifier(bootstrap=True, criterion=\"entropy\", max_features=0.35000000000000003, min_samples_leaf=20, min_samples_split=19, n_estimators=100)\n)\n# Fix random state for all the steps in exported pipeline\nset_param_recursive(exported_pipeline.steps, 'random_state', 42)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n```\n\n### Regression\n\nSimilarly, TPOT can optimize pipelines for regression problems. Below is a minimal working example with the practice Boston housing prices data set.\n\n```python\nfrom tpot import TPOTRegressor\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\n\nhousing = load_boston()\nX_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,\n                                                    train_size=0.75, test_size=0.25, random_state=42)\n\ntpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42)\ntpot.fit(X_train, y_train)\nprint(tpot.score(X_test, y_test))\ntpot.export('tpot_boston_pipeline.py')\n```\n\nwhich should result in a pipeline that achieves about 12.77 mean squared error (MSE), and the Python code in `tpot_boston_pipeline.py` should look similar to:\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tpot.export_utils import set_param_recursive\n\n# NOTE: Make sure that the outcome column is labeled 'target' in the data file\ntpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\nfeatures = tpot_data.drop('target', axis=1)\ntraining_features, testing_features, training_target, testing_target = \\\n            train_test_split(features, tpot_data['target'], random_state=42)\n\n# Average CV score on the training set was: -10.812040755234403\nexported_pipeline = make_pipeline(\n    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),\n    ExtraTreesRegressor(bootstrap=False, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=100)\n)\n# Fix random state for all the steps in exported pipeline\nset_param_recursive(exported_pipeline.steps, 'random_state', 42)\n\nexported_pipeline.fit(training_features, training_target)\nresults = exported_pipeline.predict(testing_features)\n```\n\nCheck the documentation for [more examples and tutorials](http://epistasislab.github.io/tpot/examples/).\n\n## Contributing to TPOT\n\nWe welcome you to [check the existing issues](https://github.com/EpistasisLab/tpot/issues/) for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please [file a new issue](https://github.com/EpistasisLab/tpot/issues/new) so we can discuss it.\n\nBefore submitting any contributions, please review our [contribution guidelines](http://epistasislab.github.io/tpot/contributing/).\n\n## Having problems or have questions about TPOT?\n\nPlease [check the existing open and closed issues](https://github.com/EpistasisLab/tpot/issues?utf8=%E2%9C%93&q=is%3Aissue) to see if your issue has already been attended to. If it hasn't, [file a new issue](https://github.com/EpistasisLab/tpot/issues/new) on this repository so we can review your issue.\n\n## Citing TPOT\n\nIf you use TPOT in a scientific publication, please consider citing at least one of the following papers:\n\nTrang T. Le, Weixuan Fu and Jason H. Moore (2020). [Scaling tree-based automated machine learning to biomedical big data with a feature set selector](https://academic.oup.com/bioinformatics/article/36/1/250/5511404). *Bioinformatics*.36(1): 250-256.\n\nBibTeX entry:\n\n```bibtex\n@article{le2020scaling,\n  title={Scaling tree-based automated machine learning to biomedical big data with a feature set selector},\n  author={Le, Trang T and Fu, Weixuan and Moore, Jason H},\n  journal={Bioinformatics},\n  volume={36},\n  number={1},\n  pages={250--256},\n  year={2020},\n  publisher={Oxford University Press}\n}\n```\n\n\nRandal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, and Jason H. Moore (2016). [Automating biomedical data science through tree-based pipeline optimization](http://link.springer.com/chapter/10.1007/978-3-319-31204-0_9). *Applications of Evolutionary Computation*, pages 123-137.\n\nBibTeX entry:\n\n```bibtex\n@inbook{Olson2016EvoBio,\n    author={Olson, Randal S. and Urbanowicz, Ryan J. and Andrews, Peter C. and Lavender, Nicole A. and Kidd, La Creis and Moore, Jason H.},\n    editor={Squillero, Giovanni and Burelli, Paolo},\n    chapter={Automating Biomedical Data Science Through Tree-Based Pipeline Optimization},\n    title={Applications of Evolutionary Computation: 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 -- April 1, 2016, Proceedings, Part I},\n    year={2016},\n    publisher={Springer International Publishing},\n    pages={123--137},\n    isbn={978-3-319-31204-0},\n    doi={10.1007/978-3-319-31204-0_9},\n    url={http://dx.doi.org/10.1007/978-3-319-31204-0_9}\n}\n```\n\nRandal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore (2016). [Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science](http://dl.acm.org/citation.cfm?id=2908918). *Proceedings of GECCO 2016*, pages 485-492.\n\nBibTeX entry:\n\n```bibtex\n@inproceedings{OlsonGECCO2016,\n    author = {Olson, Randal S. and Bartley, Nathan and Urbanowicz, Ryan J. and Moore, Jason H.},\n    title = {Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science},\n    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},\n    series = {GECCO '16},\n    year = {2016},\n    isbn = {978-1-4503-4206-3},\n    location = {Denver, Colorado, USA},\n    pages = {485--492},\n    numpages = {8},\n    url = {http://doi.acm.org/10.1145/2908812.2908918},\n    doi = {10.1145/2908812.2908918},\n    acmid = {2908918},\n    publisher = {ACM},\n    address = {New York, NY, USA},\n}\n```\n\nAlternatively, you can cite the repository directly with the following DOI:\n\n[![DOI](https://zenodo.org/badge/20747/rhiever/tpot.svg)](https://zenodo.org/badge/latestdoi/20747/rhiever/tpot)\n\n## Support for TPOT\n\nTPOT was developed in the [Computational Genetics Lab](http://epistasis.org/) at the [University of Pennsylvania](https://www.upenn.edu/) with funding from the [NIH](http://www.nih.gov/) under grant R01 AI117694. We are incredibly grateful for the support of the NIH and the University of Pennsylvania during the development of this project.\n\nThe TPOT logo was designed by Todd Newmuis, who generously donated his time to the project.\n"
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs_sources",
          "type": "tree",
          "content": null
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "mkdocs.yml",
          "type": "blob",
          "size": 1.220703125,
          "content": "site_name: TPOT\nsite_url: http://epistasislab.github.io/tpot\nsite_author: Randal S. Olson\nsite_description: Documentation for TPOT, a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.\n\nrepo_url: https://github.com/epistasislab/tpot\nedit_uri: edit/master/docs_sources/\ndocs_dir: docs_sources/\nsite_dir: docs/\n  #theme: readthedocs\ntheme:\n  name: material\n  features:\n    - toc.integrate\n  palette:\n    # light mode\n    - scheme: default\n      toggle:\n        icon: material/brightness-7\n        name: Switch to dark mode\n\n    # dark mode\n    - scheme: slate\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n\nmarkdown_extensions:\n  - tables\n  - fenced_code\n  - pymdownx.highlight:\n      anchor_linenums: true\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.superfences\n\ncopyright: Developed by <a href=\"http://www.randalolson.com\">Randal S. Olson</a> and others at the University of Pennsylvania\n\nnav:\n- Home: index.md\n- Installation: installing.md\n- Using TPOT: using.md\n- TPOT API: api.md\n- Examples: examples.md\n- Contributing: contributing.md\n- Release Notes: releases.md\n- Citing TPOT: citing.md\n- Support: support.md\n- Related: related.md\n"
        },
        {
          "name": "optional-requirements.txt",
          "type": "blob",
          "size": 0.046875,
          "content": "scikit-mdr==0.4.4\nskrebate==0.3.4\ntorch==1.13.1\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.181640625,
          "content": "deap>=1.2\nnose==1.3.7\nnumpy>=1.16.3\nscikit-learn>=1.4.1\nimbalanced-learn>=0.7.0\nscipy>=1.3.1\ntqdm>=4.36.1\nupdate-checker>=0.16\nstopit>=1.1.2\npandas>=0.24.2\njoblib>=0.13.2\nxgboost>=1.1.0\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.296875,
          "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom setuptools import setup, find_packages\n\n\ndef calculate_version():\n    initpy = open('tpot/_version.py').read().split('\\n')\n    version = list(filter(lambda x: '__version__' in x, initpy))[0].split('\\'')[1]\n    return version\n\n\npackage_version = calculate_version()\n\nsetup(\n    name='TPOT',\n    version=package_version,\n    author='Randal S. Olson',\n    author_email='rso@randalolson.com',\n    packages=find_packages(),\n    url='https://github.com/EpistasisLab/tpot',\n    license='GNU/LGPLv3',\n    entry_points={'console_scripts': ['tpot=tpot:main', ]},\n    description=('Tree-based Pipeline Optimization Tool'),\n    long_description='''\nA Python tool that automatically creates and optimizes machine learning pipelines using genetic programming.\n\nContact\n=============\nIf you have any questions or comments about TPOT, please feel free to contact us via:\n\nE-mail: ttle@pennmedicine.upenn.edu or weixuanf@pennmedicine.upenn.edu\n\nor Twitter: https://twitter.com/trang1618 or https://twitter.com/WeixuanFu\n\nThis project is hosted at https://github.com/EpistasisLab/tpot\n''',\n    zip_safe=True,\n    install_requires=['numpy>=1.16.3',\n                    'scipy>=1.3.1',\n                    'scikit-learn>=1.4.1',\n                    'deap>=1.2',\n                    'update_checker>=0.16',\n                    'tqdm>=4.36.1',\n                    'stopit>=1.1.1',\n                    'pandas>=0.24.2',\n                    'joblib>=0.13.2',\n                    'xgboost>=1.1.0'],\n    extras_require={\n        'skrebate': ['skrebate>=0.3.4'],\n        'mdr': ['scikit-mdr>=0.4.4'],\n        'dask': ['dask>=0.18.2',\n                 'distributed>=1.22.1',\n                 'dask-ml>=1.0.0'],\n        'torch': ['torch==1.13.1'],\n\t'imblearn': ['imbalanced-learn>=0.7.0']\n    },\n    classifiers=[\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Topic :: Scientific/Engineering :: Artificial Intelligence'\n    ],\n    keywords=['pipeline optimization', 'hyperparameter optimization', 'data science', 'machine learning', 'genetic programming', 'evolutionary computation'],\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tpot-cuml.yml",
          "type": "blob",
          "size": 0.22265625,
          "content": "channels:\n  - rapidsai\n  - nvidia\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.7\n  - cudatoolkit=10.2\n  - cuml=0.16\n  - scikit-learn\n  - ipython\n  - ipywidgets\n  - jupyterlab\n  - pip\n  - pip:\n    - xgboost\n    - tpot\n"
        },
        {
          "name": "tpot",
          "type": "tree",
          "content": null
        },
        {
          "name": "tutorials",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}