{
  "metadata": {
    "timestamp": 1736560899050,
    "page": 622,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "facebookresearch/ConvNeXt",
      "stars": 5836,
      "defaultBranch": "main",
      "files": [
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 3.4521484375,
          "content": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\nThis Code of Conduct also applies outside the project spaces when there is a\nreasonable belief that an individual's behavior may have a negative impact on\nthe project or its community.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <opensource-conduct@fb.com>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.2138671875,
          "content": "# Contributing to ConvNeXt\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `main`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Meta's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nMeta has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## License\nBy contributing to ConvNeXt, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.\n"
        },
        {
          "name": "INSTALL.md",
          "type": "blob",
          "size": 1.2685546875,
          "content": "# Installation\n\nWe provide installation instructions for ImageNet classification experiments here.\n\n## Dependency Setup\nCreate an new conda virtual environment\n```\nconda create -n convnext python=3.8 -y\nconda activate convnext\n```\n\nInstall [Pytorch](https://pytorch.org/)>=1.8.0, [torchvision](https://pytorch.org/vision/stable/index.html)>=0.9.0 following official instructions. For example:\n```\npip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n```\n\nClone this repo and install required packages:\n```\ngit clone https://github.com/facebookresearch/ConvNeXt\npip install timm==0.3.2 tensorboardX six\n```\n\nThe results in the paper are produced with `torch==1.8.0+cu111 torchvision==0.9.0+cu111 timm==0.3.2`.\n\n## Dataset Preparation\n\nDownload the [ImageNet-1K](http://image-net.org/) classification dataset and structure the data as follows:\n```\n/path/to/imagenet-1k/\n  train/\n    class1/\n      img1.jpeg\n    class2/\n      img2.jpeg\n  val/\n    class1/\n      img3.jpeg\n    class2/\n      img4.jpeg\n```\n\nFor pre-training on [ImageNet-22K](http://image-net.org/), download the dataset and structure the data as follows:\n```\n/path/to/imagenet-22k/\n  class1/\n    img1.jpeg\n  class2/\n    img2.jpeg\n  class3/\n    img3.jpeg\n  class4/\n    img4.jpeg\n```"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0625,
          "content": "MIT License\n\nCopyright (c) Meta Platforms, Inc. and affiliates.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.8349609375,
          "content": "# [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)\n\nOfficial PyTorch implementation of **ConvNeXt**, from the following paper:\n\n[A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545). CVPR 2022.\\\n[Zhuang Liu](https://liuzhuang13.github.io), [Hanzi Mao](https://hanzimao.me/), [Chao-Yuan Wu](https://chaoyuan.org/), [Christoph Feichtenhofer](https://feichtenhofer.github.io/), [Trevor Darrell](https://people.eecs.berkeley.edu/~trevor/) and [Saining Xie](https://sainingxie.com)\\\nFacebook AI Research, UC Berkeley\\\n[[`arXiv`](https://arxiv.org/abs/2201.03545)][[`video`](https://www.youtube.com/watch?v=QzCjXqFnWPE)]\n\n--- \n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/8370623/180626875-fe958128-6102-4f01-9ca4-e3a30c3148f9.png\" width=100% height=100% \nclass=\"center\">\n</p>\n\nWe propose **ConvNeXt**, a pure ConvNet model constructed entirely from standard ConvNet modules. ConvNeXt is accurate, efficient, scalable and very simple in design.\n\n## Catalog\n- [x] ImageNet-1K Training Code  \n- [x] ImageNet-22K Pre-training Code  \n- [x] ImageNet-1K Fine-tuning Code  \n- [x] Downstream Transfer (Detection, Segmentation) Code\n- [x] Image Classification [\\[Colab\\]](https://colab.research.google.com/drive/1CBYTIZ4tBMsVL5cqu9N_-Q3TBprqsfEO?usp=sharing) and Web Demo [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/akhaliq/convnext)\n- [x] Fine-tune on CIFAR with Weights & Biases logging [\\[Colab\\]](https://colab.research.google.com/drive/1ijAxGthE9RENJJQRO17v9A7PTd1Tei9F?usp=sharing)\n\n\n\n<!-- ✅ ⬜️  -->\n\n## Results and Pre-trained Models\n### ImageNet-1K trained models\n\n| name | resolution |acc@1 | #params | FLOPs | model |\n|:---:|:---:|:---:|:---:| :---:|:---:|\n| ConvNeXt-T | 224x224 | 82.1 | 28M | 4.5G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth) |\n| ConvNeXt-S | 224x224 | 83.1 | 50M | 8.7G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth) |\n| ConvNeXt-B | 224x224 | 83.8 | 89M | 15.4G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth) |\n| ConvNeXt-B | 384x384 | 85.1 | 89M | 45.0G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_384.pth) |\n| ConvNeXt-L | 224x224 | 84.3 | 198M | 34.4G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth) |\n| ConvNeXt-L | 384x384 | 85.5 | 198M | 101.0G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_384.pth) |\n\n### ImageNet-22K trained models\n\n| name | resolution |acc@1 | #params | FLOPs | 22k model | 1k model |\n|:---:|:---:|:---:|:---:| :---:| :---:|:---:|\n| ConvNeXt-T | 224x224 | 82.9 | 29M | 4.5G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth)   | [model](https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_1k_224.pth)\n| ConvNeXt-T | 384x384 | 84.1 | 29M | 13.1G |     -          | [model](https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_1k_384.pth)\n| ConvNeXt-S | 224x224 | 84.6 | 50M | 8.7G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth)   | [model](https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_1k_224.pth)\n| ConvNeXt-S | 384x384 | 85.8 | 50M | 25.5G |     -          | [model](https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_1k_384.pth)\n| ConvNeXt-B | 224x224 | 85.8 | 89M | 15.4G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth)   | [model](https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_224.pth)\n| ConvNeXt-B | 384x384 | 86.8 | 89M | 47.0G |     -          | [model](https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_384.pth)\n| ConvNeXt-L | 224x224 | 86.6 | 198M | 34.4G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth)  | [model](https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_1k_224.pth)\n| ConvNeXt-L | 384x384 | 87.5 | 198M | 101.0G |    -         | [model](https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_1k_384.pth)\n| ConvNeXt-XL | 224x224 | 87.0 | 350M | 60.9G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth) | [model](https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_1k_224_ema.pth)\n| ConvNeXt-XL | 384x384 | 87.8 | 350M | 179.0G |  -          | [model](https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_1k_384_ema.pth)\n\n\n### ImageNet-1K trained models (isotropic)\n| name | resolution |acc@1 | #params | FLOPs | model |\n|:---:|:---:|:---:|:---:| :---:|:---:|\n| ConvNeXt-S | 224x224 | 78.7 | 22M | 4.3G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_iso_small_1k_224_ema.pth) |\n| ConvNeXt-B | 224x224 | 82.0 | 87M | 16.9G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_iso_base_1k_224_ema.pth) |\n| ConvNeXt-L | 224x224 | 82.6 | 306M | 59.7G | [model](https://dl.fbaipublicfiles.com/convnext/convnext_iso_large_1k_224_ema.pth) |\n\n\n## Installation\nPlease check [INSTALL.md](INSTALL.md) for installation instructions. \n\n## Evaluation\nWe give an example evaluation command for a ImageNet-22K pre-trained, then ImageNet-1K fine-tuned ConvNeXt-B:\n\nSingle-GPU\n```\npython main.py --model convnext_base --eval true \\\n--resume https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_224.pth \\\n--input_size 224 --drop_path 0.2 \\\n--data_path /path/to/imagenet-1k\n```\nMulti-GPU\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_base --eval true \\\n--resume https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_224.pth \\\n--input_size 224 --drop_path 0.2 \\\n--data_path /path/to/imagenet-1k\n```\n\nThis should give \n```\n* Acc@1 85.820 Acc@5 97.868 loss 0.563\n```\n\n- For evaluating other model variants, change `--model`, `--resume`, `--input_size` accordingly. You can get the url to pre-trained models from the tables above. \n- Setting model-specific `--drop_path` is not strictly required in evaluation, as the `DropPath` module in timm behaves the same during evaluation; but it is required in training. See [TRAINING.md](TRAINING.md) or our paper for the values used for different models.\n\n## Training\nSee [TRAINING.md](TRAINING.md) for training and fine-tuning instructions.\n\n## Acknowledgement\nThis repository is built using the [timm](https://github.com/rwightman/pytorch-image-models) library, [DeiT](https://github.com/facebookresearch/deit) and [BEiT](https://github.com/microsoft/unilm/tree/master/beit) repositories.\n\n## License\nThis project is released under the MIT license. Please see the [LICENSE](LICENSE) file for more information.\n\n## Citation\nIf you find this repository helpful, please consider citing:\n```\n@Article{liu2022convnet,\n  author  = {Zhuang Liu and Hanzi Mao and Chao-Yuan Wu and Christoph Feichtenhofer and Trevor Darrell and Saining Xie},\n  title   = {A ConvNet for the 2020s},\n  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  year    = {2022},\n}\n```\n"
        },
        {
          "name": "TRAINING.md",
          "type": "blob",
          "size": 16.1796875,
          "content": "# Training\n\nWe provide ImageNet-1K training, ImageNet-22K pre-training, and ImageNet-1K fine-tuning commands here.\nPlease check [INSTALL.md](INSTALL.md) for installation instructions first.\n\n## Multi-node Training\nWe use multi-node training on a SLURM cluster with [submitit](https://github.com/facebookincubator/submitit) for producing the results and models in the paper. Please install:\n```\npip install submitit\n```\nWe will give example commands for both multi-node and single-machine training below.\n\n## ImageNet-1K Training \nConvNeXt-T training on ImageNet-1K with 4 8-GPU nodes:\n```\npython run_with_submitit.py --nodes 4 --ngpus 8 \\\n--model convnext_tiny --drop_path 0.1 \\\n--batch_size 128 --lr 4e-3 --update_freq 1 \\\n--model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results\n```\n\n- You may need to change cluster-specific arguments in `run_with_submitit.py`.\n- You can add `--use_amp true` to train in PyTorch's Automatic Mixed Precision (AMP).\n- Use `--resume /path_or_url/to/checkpoint.pth` to resume training from a previous checkpoint; use `--auto_resume true` to auto-resume from latest checkpoint in the specified output folder.\n- `--batch_size`: batch size per GPU; `--update_freq`: gradient accumulation steps.\n- The effective batch size = `--nodes` * `--ngpus` * `--batch_size` * `--update_freq`. In the example above, the effective batch size is `4*8*128*1 = 4096`. You can adjust these four arguments together to keep the effective batch size at 4096 and avoid OOM issues, based on the model size, number of nodes and GPU memory.\n\nYou can use the following command to run this experiment on a single machine: \n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_tiny --drop_path 0.1 \\\n--batch_size 128 --lr 4e-3 --update_freq 4 \\\n--model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \n--output_dir /path/to/save_results\n```\n\n- Here, the effective batch size = `--nproc_per_node` * `--batch_size` * `--update_freq`. In the example above, the effective batch size is `8*128*4 = 4096`. Running on one machine, we increased `update_freq` so that the total batch size is unchanged.\n\nTo train other ConvNeXt variants, `--model` and `--drop_path` need to be changed. Examples are given below, each with both multi-node and single-machine commands:\n\n<details>\n<summary>\nConvNeXt-S\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 4 --ngpus 8 \\\n--model convnext_small --drop_path 0.4 \\\n--batch_size 128 --lr 4e-3 --update_freq 1 \\\n--model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results\n```\n\nSingle-machine\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_small --drop_path 0.4 \\\n--batch_size 128 --lr 4e-3 --update_freq 4 \\\n--model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results\n```\n</details>\n<details>\n<summary>\nConvNeXt-B\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 4 --ngpus 8 \\\n--model convnext_base --drop_path 0.5 \\\n--batch_size 128 --lr 4e-3 --update_freq 1 \\\n--model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results\n``` \n\nSingle-machine\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_base --drop_path 0.5 \\\n--batch_size 128 --lr 4e-3 --update_freq 4 \\\n--model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results\n``` \n\n</details>\n<details>\n<summary>\nConvNeXt-L\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 8 --ngpus 8 \\\n--model convnext_large --drop_path 0.5 \\\n--batch_size 64 --lr 4e-3 --update_freq 1 \\\n--model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results\n``` \n\nSingle-machine\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_large --drop_path 0.5 \\\n--batch_size 64 --lr 4e-3 --update_freq 8 \\\n--model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results\n``` \n\n</details>\n\n<details>\n<summary>\nConvNeXt-S (isotropic)\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 4 --ngpus 8 \\\n--model convnext_isotropic_small --drop_path 0.1 \\\n--batch_size 128 --lr 4e-3 --update_freq 1 \\\n--layer_scale_init_value 0 \\\n--warmup_epochs 50 --model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results\n``` \n\nSingle-machine \n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_isotropic_small --drop_path 0.1 \\\n--batch_size 128 --lr 4e-3 --update_freq 4 \\\n--layer_scale_init_value 0 \\\n--warmup_epochs 50 --model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results\n``` \n\n</details>\n\n<details>\n<summary>\nConvNeXt-B (isotropic)\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 4 --ngpus 8 \\\n--model convnext_isotropic_base --drop_path 0.2 \\\n--batch_size 128 --lr 4e-3 --update_freq 1 \\\n--layer_scale_init_value 0 \\\n--warmup_epochs 50 --model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results\n``` \n\nSingle-machine \n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_isotropic_base --drop_path 0.2 \\\n--batch_size 128 --lr 4e-3 --update_freq 4 \\\n--layer_scale_init_value 0 \\\n--warmup_epochs 50 --model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results\n``` \n\n</details>\n\n<details>\n<summary>\nConvNeXt-L (isotropic)\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 8 --ngpus 8 \\\n--model convnext_isotropic_large --drop_path 0.5 \\\n--batch_size 64 --lr 4e-3 --update_freq 1 \\\n--warmup_epochs 50 --model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results\n``` \n\nSingle-machine\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_isotropic_large --drop_path 0.5 \\\n--batch_size 64 --lr 4e-3 --update_freq 8 \\\n--warmup_epochs 50 --model_ema true --model_ema_eval true \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results\n``` \n\n</details>\n\n## ImageNet-22K Pre-training\nImageNet-22K is significantly larger than ImageNet-1K in terms of data size, so we use 16 8-GPU nodes for pre-training on ImageNet-22K.\n\nConvNeXt-B pre-training on ImageNet-22K:\n\nMulti-node\n```\npython run_with_submitit.py --nodes 16 --ngpus 8 \\\n--model convnext_base --drop_path 0.1 \\\n--batch_size 32 --lr 4e-3 --update_freq 1 \\\n--warmup_epochs 5 --epochs 90 \\\n--data_set image_folder --nb_classes 21841 --disable_eval true \\\n--data_path /path/to/imagenet-22k \\\n--job_dir /path/to/save_results\n```\n\nSingle-machine\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_base --drop_path 0.1 \\\n--batch_size 32 --lr 4e-3 --update_freq 16 \\\n--warmup_epochs 5 --epochs 90 \\\n--data_set image_folder --nb_classes 21841 --disable_eval true \\\n--data_path /path/to/imagenet-22k \\\n--output_dir /path/to/save_results\n```\n\n<details>\n<summary>\nConvNeXt-L\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 16 --ngpus 8 \\\n--model convnext_large --drop_path 0.1 \\\n--batch_size 32 --lr 4e-3 --update_freq 1 \\\n--warmup_epochs 5 --epochs 90 \\\n--data_set image_folder --nb_classes 21841 --disable_eval true \\\n--data_path /path/to/imagenet-22k \\\n--job_dir /path/to/save_results\n``` \n    \nSingle-machine\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_large --drop_path 0.1 \\\n--batch_size 32 --lr 4e-3 --update_freq 16 \\\n--warmup_epochs 5 --epochs 90 \\\n--data_set image_folder --nb_classes 21841 --disable_eval true \\\n--data_path /path/to/imagenet-22k \\\n--output_dir /path/to/save_results\n``` \n\n</details>\n\n<details>\n<summary>\nConvNeXt-XL\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 16 --ngpus 8 \\\n--model convnext_xlarge --drop_path 0.2 \\\n--batch_size 32 --lr 4e-3 --update_freq 1 \\\n--warmup_epochs 5 --epochs 90 \\\n--data_set image_folder --nb_classes 21841 --disable_eval true \\\n--data_path /path/to/imagenet-22k \\\n--job_dir /path/to/save_results\n``` \n    \nSingle-machine\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_xlarge --drop_path 0.2 \\\n--batch_size 32 --lr 4e-3 --update_freq 16 \\\n--warmup_epochs 5 --epochs 90 \\\n--data_set image_folder --nb_classes 21841 --disable_eval true \\\n--data_path /path/to/imagenet-22k \\\n--output_dir /path/to/save_results\n``` \n\n</details>\n\n\n## ImageNet-1K Fine-tuning\n### Finetune from ImageNet-1K pre-training \nThe training commands given above for ImageNet-1K use the default resolution (224). We also fine-tune these trained models with a larger resolution (384). Please specify the path or url to the checkpoint in `--finetune`.\n\nConvNeXt-B fine-tuning on ImageNet-1K (384x384):\n\nMulti-node\n```\npython run_with_submitit.py --nodes 2 --ngpus 8 \\\n--model convnext_base --drop_path 0.8 --input_size 384 \\\n--batch_size 32 --lr 5e-5 --update_freq 1 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.7 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results\n```\n\nSingle-machine\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_base --drop_path 0.8 --input_size 384 \\\n--batch_size 32 --lr 5e-5 --update_freq 2 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.7 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results\n```\n\n<details>\n<summary>\nConvNeXt-L (384x384)\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 2 --ngpus 8 \\\n--model convnext_large --drop_path 0.95 --input_size 384 \\\n--batch_size 32 --lr 5e-5 --update_freq 1 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.7 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results\n``` \n\nSingle-machine\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_large --drop_path 0.95 --input_size 384 \\\n--batch_size 32 --lr 5e-5 --update_freq 2 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.7 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results\n``` \n\n- The fine-tuning for ImageNet-1K pre-trained ConvNeXt-L starts from the best ema weights during pre-training. You can add `--model_key model_ema` to load from a saved checkpoint that has `model_ema` as a key (e.g., obtained by training with `--model_ema true`), to load ema weights. Note that our provided pre-trained checkpoints only have `model` as the only key.\n\n</details>\n\n### Fine-tune from ImageNet-22K pre-training\nWe finetune from ImageNet-22K pre-trained models, in both 224 and 384 resolutions.\n\nConvNeXt-B fine-tuning on ImageNet-1K (224x224)\n\nMulti-node\n```\npython run_with_submitit.py --nodes 2 --ngpus 8 \\\n--model convnext_base --drop_path 0.2 --input_size 224 \\\n--batch_size 32 --lr 5e-5 --update_freq 1 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.8 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results\n```\n\nSingle-machine\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_base --drop_path 0.2 --input_size 224 \\\n--batch_size 32 --lr 5e-5 --update_freq 2 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.8 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results\n```\n\n<details>\n<summary>\nConvNeXt-L (224x224)\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 2 --ngpus 8 \\\n--model convnext_large --drop_path 0.3 --input_size 224 \\\n--batch_size 32 --lr 5e-5 --update_freq 1 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.8 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results\n``` \n\nSingle-machine\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_large --drop_path 0.3 --input_size 224 \\\n--batch_size 32 --lr 5e-5 --update_freq 2 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.8 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results\n``` \n\n</details>\n\n<details>\n<summary>\nConvNeXt-XL (224x224)\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 4 --ngpus 8 \\\n--model convnext_xlarge --drop_path 0.4 --input_size 224 \\\n--batch_size 16 --lr 5e-5 --update_freq 1 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.8 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results \\\n--model_ema true --model_ema_eval true\n``` \n\nSingle-machine\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_xlarge --drop_path 0.4 --input_size 224 \\\n--batch_size 16 --lr 5e-5 --update_freq 4 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.8 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results \\\n--model_ema true --model_ema_eval true\n``` \n\n</details>\n\n<details>\n<summary>\nConvNeXt-B (384x384)\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 4 --ngpus 8 \\\n--model convnext_base --drop_path 0.2 --input_size 384 \\\n--batch_size 16 --lr 5e-5 --update_freq 1 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.8 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results\n``` \n\nSingle-machine   \n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_base --drop_path 0.2 --input_size 384 \\\n--batch_size 16 --lr 5e-5 --update_freq 4 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.8 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results\n``` \n\n</details>\n\n<details>\n<summary>\nConvNeXt-L (384x384)\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 4 --ngpus 8 \\\n--model convnext_large --drop_path 0.3 --input_size 384 \\\n--batch_size 16 --lr 5e-5 --update_freq 1 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.8 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results\n``` \n\nSingle-machine    \n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_large --drop_path 0.3 --input_size 384 \\\n--batch_size 16 --lr 5e-5 --update_freq 4 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.8 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results\n``` \n\n</details>\n\n<details>\n<summary>\nConvNeXt-XL (384x384)\n</summary>\n\nMulti-node\n```\npython run_with_submitit.py --nodes 8 --ngpus 8 \\\n--model convnext_xlarge --drop_path 0.4 --input_size 384 \\\n--batch_size 8 --lr 5e-5 --update_freq 1 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.8 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--job_dir /path/to/save_results \\\n--model_ema true --model_ema_eval true\n``` \n\nSingle-machine\n```\npython -m torch.distributed.launch --nproc_per_node=8 main.py \\\n--model convnext_xlarge --drop_path 0.4 --input_size 384 \\\n--batch_size 8 --lr 5e-5 --update_freq 8 \\\n--warmup_epochs 0 --epochs 30 --weight_decay 1e-8  \\\n--layer_decay 0.8 --head_init_scale 0.001 --cutmix 0 --mixup 0 \\\n--finetune /path/to/checkpoint.pth \\\n--data_path /path/to/imagenet-1k \\\n--output_dir /path/to/save_results \\\n--model_ema true --model_ema_eval true\n``` \n\n</details>\n\n"
        },
        {
          "name": "datasets.py",
          "type": "blob",
          "size": 3.4326171875,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport os\nfrom torchvision import datasets, transforms\n\nfrom timm.data.constants import \\\n    IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\nfrom timm.data import create_transform\n\ndef build_dataset(is_train, args):\n    transform = build_transform(is_train, args)\n\n    print(\"Transform = \")\n    if isinstance(transform, tuple):\n        for trans in transform:\n            print(\" - - - - - - - - - - \")\n            for t in trans.transforms:\n                print(t)\n    else:\n        for t in transform.transforms:\n            print(t)\n    print(\"---------------------------\")\n\n    if args.data_set == 'CIFAR':\n        dataset = datasets.CIFAR100(args.data_path, train=is_train, transform=transform, download=True)\n        nb_classes = 100\n    elif args.data_set == 'IMNET':\n        print(\"reading from datapath\", args.data_path)\n        root = os.path.join(args.data_path, 'train' if is_train else 'val')\n        dataset = datasets.ImageFolder(root, transform=transform)\n        nb_classes = 1000\n    elif args.data_set == \"image_folder\":\n        root = args.data_path if is_train else args.eval_data_path\n        dataset = datasets.ImageFolder(root, transform=transform)\n        nb_classes = args.nb_classes\n        assert len(dataset.class_to_idx) == nb_classes\n    else:\n        raise NotImplementedError()\n    print(\"Number of the class = %d\" % nb_classes)\n\n    return dataset, nb_classes\n\n\ndef build_transform(is_train, args):\n    resize_im = args.input_size > 32\n    imagenet_default_mean_and_std = args.imagenet_default_mean_and_std\n    mean = IMAGENET_INCEPTION_MEAN if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_MEAN\n    std = IMAGENET_INCEPTION_STD if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_STD\n\n    if is_train:\n        # this should always dispatch to transforms_imagenet_train\n        transform = create_transform(\n            input_size=args.input_size,\n            is_training=True,\n            color_jitter=args.color_jitter,\n            auto_augment=args.aa,\n            interpolation=args.train_interpolation,\n            re_prob=args.reprob,\n            re_mode=args.remode,\n            re_count=args.recount,\n            mean=mean,\n            std=std,\n        )\n        if not resize_im:\n            transform.transforms[0] = transforms.RandomCrop(\n                args.input_size, padding=4)\n        return transform\n\n    t = []\n    if resize_im:\n        # warping (no cropping) when evaluated at 384 or larger\n        if args.input_size >= 384:  \n            t.append(\n            transforms.Resize((args.input_size, args.input_size), \n                            interpolation=transforms.InterpolationMode.BICUBIC), \n        )\n            print(f\"Warping {args.input_size} size input images...\")\n        else:\n            if args.crop_pct is None:\n                args.crop_pct = 224 / 256\n            size = int(args.input_size / args.crop_pct)\n            t.append(\n                # to maintain same ratio w.r.t. 224 images\n                transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC),  \n            )\n            t.append(transforms.CenterCrop(args.input_size))\n\n    t.append(transforms.ToTensor())\n    t.append(transforms.Normalize(mean, std))\n    return transforms.Compose(t)\n"
        },
        {
          "name": "engine.py",
          "type": "blob",
          "size": 7.1123046875,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport math\nfrom typing import Iterable, Optional\nimport torch\nfrom timm.data import Mixup\nfrom timm.utils import accuracy, ModelEma\n\nimport utils\n\ndef train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n                    device: torch.device, epoch: int, loss_scaler, max_norm: float = 0,\n                    model_ema: Optional[ModelEma] = None, mixup_fn: Optional[Mixup] = None, log_writer=None,\n                    wandb_logger=None, start_steps=None, lr_schedule_values=None, wd_schedule_values=None,\n                    num_training_steps_per_epoch=None, update_freq=None, use_amp=False):\n    model.train(True)\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    metric_logger.add_meter('min_lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n    print_freq = 10\n\n    optimizer.zero_grad()\n\n    for data_iter_step, (samples, targets) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n        step = data_iter_step // update_freq\n        if step >= num_training_steps_per_epoch:\n            continue\n        it = start_steps + step  # global training iteration\n        # Update LR & WD for the first acc\n        if lr_schedule_values is not None or wd_schedule_values is not None and data_iter_step % update_freq == 0:\n            for i, param_group in enumerate(optimizer.param_groups):\n                if lr_schedule_values is not None:\n                    param_group[\"lr\"] = lr_schedule_values[it] * param_group[\"lr_scale\"]\n                if wd_schedule_values is not None and param_group[\"weight_decay\"] > 0:\n                    param_group[\"weight_decay\"] = wd_schedule_values[it]\n\n        samples = samples.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n\n        if mixup_fn is not None:\n            samples, targets = mixup_fn(samples, targets)\n\n        if use_amp:\n            with torch.cuda.amp.autocast():\n                output = model(samples)\n                loss = criterion(output, targets)\n        else: # full precision\n            output = model(samples)\n            loss = criterion(output, targets)\n\n        loss_value = loss.item()\n\n        if not math.isfinite(loss_value): # this could trigger if using AMP\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            assert math.isfinite(loss_value)\n\n        if use_amp:\n            # this attribute is added by timm on one optimizer (adahessian)\n            is_second_order = hasattr(optimizer, 'is_second_order') and optimizer.is_second_order\n            loss /= update_freq\n            grad_norm = loss_scaler(loss, optimizer, clip_grad=max_norm,\n                                    parameters=model.parameters(), create_graph=is_second_order,\n                                    update_grad=(data_iter_step + 1) % update_freq == 0)\n            if (data_iter_step + 1) % update_freq == 0:\n                optimizer.zero_grad()\n                if model_ema is not None:\n                    model_ema.update(model)\n        else: # full precision\n            loss /= update_freq\n            loss.backward()\n            if (data_iter_step + 1) % update_freq == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n                if model_ema is not None:\n                    model_ema.update(model)\n\n        torch.cuda.synchronize()\n\n        if mixup_fn is None:\n            class_acc = (output.max(-1)[-1] == targets).float().mean()\n        else:\n            class_acc = None\n        metric_logger.update(loss=loss_value)\n        metric_logger.update(class_acc=class_acc)\n        min_lr = 10.\n        max_lr = 0.\n        for group in optimizer.param_groups:\n            min_lr = min(min_lr, group[\"lr\"])\n            max_lr = max(max_lr, group[\"lr\"])\n\n        metric_logger.update(lr=max_lr)\n        metric_logger.update(min_lr=min_lr)\n        weight_decay_value = None\n        for group in optimizer.param_groups:\n            if group[\"weight_decay\"] > 0:\n                weight_decay_value = group[\"weight_decay\"]\n        metric_logger.update(weight_decay=weight_decay_value)\n        if use_amp:\n            metric_logger.update(grad_norm=grad_norm)\n\n        if log_writer is not None:\n            log_writer.update(loss=loss_value, head=\"loss\")\n            log_writer.update(class_acc=class_acc, head=\"loss\")\n            log_writer.update(lr=max_lr, head=\"opt\")\n            log_writer.update(min_lr=min_lr, head=\"opt\")\n            log_writer.update(weight_decay=weight_decay_value, head=\"opt\")\n            if use_amp:\n                log_writer.update(grad_norm=grad_norm, head=\"opt\")\n            log_writer.set_step()\n\n        if wandb_logger:\n            wandb_logger._wandb.log({\n                'Rank-0 Batch Wise/train_loss': loss_value,\n                'Rank-0 Batch Wise/train_max_lr': max_lr,\n                'Rank-0 Batch Wise/train_min_lr': min_lr\n            }, commit=False)\n            if class_acc:\n                wandb_logger._wandb.log({'Rank-0 Batch Wise/train_class_acc': class_acc}, commit=False)\n            if use_amp:\n                wandb_logger._wandb.log({'Rank-0 Batch Wise/train_grad_norm': grad_norm}, commit=False)\n            wandb_logger._wandb.log({'Rank-0 Batch Wise/global_train_step': it})\n            \n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n\n@torch.no_grad()\ndef evaluate(data_loader, model, device, use_amp=False):\n    criterion = torch.nn.CrossEntropyLoss()\n\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = 'Test:'\n\n    # switch to evaluation mode\n    model.eval()\n    for batch in metric_logger.log_every(data_loader, 10, header):\n        images = batch[0]\n        target = batch[-1]\n\n        images = images.to(device, non_blocking=True)\n        target = target.to(device, non_blocking=True)\n\n        # compute output\n        if use_amp:\n            with torch.cuda.amp.autocast():\n                output = model(images)\n                loss = criterion(output, target)\n        else:\n            output = model(images)\n            loss = criterion(output, target)\n\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n        batch_size = images.shape[0]\n        metric_logger.update(loss=loss.item())\n        metric_logger.meters['acc1'].update(acc1.item(), n=batch_size)\n        metric_logger.meters['acc5'].update(acc5.item(), n=batch_size)\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print('* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss {losses.global_avg:.3f}'\n          .format(top1=metric_logger.acc1, top5=metric_logger.acc5, losses=metric_logger.loss))\n\n    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n"
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 22.537109375,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport argparse\nimport datetime\nimport numpy as np\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport json\nimport os\n\nfrom pathlib import Path\n\nfrom timm.data.mixup import Mixup\nfrom timm.models import create_model\nfrom timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\nfrom timm.utils import ModelEma\nfrom optim_factory import create_optimizer, LayerDecayValueAssigner\n\nfrom datasets import build_dataset\nfrom engine import train_one_epoch, evaluate\n\nfrom utils import NativeScalerWithGradNormCount as NativeScaler\nimport utils\nimport models.convnext\nimport models.convnext_isotropic\n\ndef str2bool(v):\n    \"\"\"\n    Converts string to bool type; enables command line \n    arguments in the format of '--arg1 true --arg2 false'\n    \"\"\"\n    if isinstance(v, bool):\n        return v\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\ndef get_args_parser():\n    parser = argparse.ArgumentParser('ConvNeXt training and evaluation script for image classification', add_help=False)\n    parser.add_argument('--batch_size', default=64, type=int,\n                        help='Per GPU batch size')\n    parser.add_argument('--epochs', default=300, type=int)\n    parser.add_argument('--update_freq', default=1, type=int,\n                        help='gradient accumulation steps')\n\n    # Model parameters\n    parser.add_argument('--model', default='convnext_tiny', type=str, metavar='MODEL',\n                        help='Name of model to train')\n    parser.add_argument('--drop_path', type=float, default=0, metavar='PCT',\n                        help='Drop path rate (default: 0.0)')\n    parser.add_argument('--input_size', default=224, type=int,\n                        help='image input size')\n    parser.add_argument('--layer_scale_init_value', default=1e-6, type=float,\n                        help=\"Layer scale initial values\")\n\n    # EMA related parameters\n    parser.add_argument('--model_ema', type=str2bool, default=False)\n    parser.add_argument('--model_ema_decay', type=float, default=0.9999, help='')\n    parser.add_argument('--model_ema_force_cpu', type=str2bool, default=False, help='')\n    parser.add_argument('--model_ema_eval', type=str2bool, default=False, help='Using ema to eval during training.')\n\n    # Optimization parameters\n    parser.add_argument('--opt', default='adamw', type=str, metavar='OPTIMIZER',\n                        help='Optimizer (default: \"adamw\"')\n    parser.add_argument('--opt_eps', default=1e-8, type=float, metavar='EPSILON',\n                        help='Optimizer Epsilon (default: 1e-8)')\n    parser.add_argument('--opt_betas', default=None, type=float, nargs='+', metavar='BETA',\n                        help='Optimizer Betas (default: None, use opt default)')\n    parser.add_argument('--clip_grad', type=float, default=None, metavar='NORM',\n                        help='Clip gradient norm (default: None, no clipping)')\n    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n                        help='SGD momentum (default: 0.9)')\n    parser.add_argument('--weight_decay', type=float, default=0.05,\n                        help='weight decay (default: 0.05)')\n    parser.add_argument('--weight_decay_end', type=float, default=None, help=\"\"\"Final value of the\n        weight decay. We use a cosine schedule for WD and using a larger decay by\n        the end of training improves performance for ViTs.\"\"\")\n\n    parser.add_argument('--lr', type=float, default=4e-3, metavar='LR',\n                        help='learning rate (default: 4e-3), with total batch size 4096')\n    parser.add_argument('--layer_decay', type=float, default=1.0)\n    parser.add_argument('--min_lr', type=float, default=1e-6, metavar='LR',\n                        help='lower lr bound for cyclic schedulers that hit 0 (1e-6)')\n    parser.add_argument('--warmup_epochs', type=int, default=20, metavar='N',\n                        help='epochs to warmup LR, if scheduler supports')\n    parser.add_argument('--warmup_steps', type=int, default=-1, metavar='N',\n                        help='num of steps to warmup LR, will overload warmup_epochs if set > 0')\n\n    # Augmentation parameters\n    parser.add_argument('--color_jitter', type=float, default=0.4, metavar='PCT',\n                        help='Color jitter factor (default: 0.4)')\n    parser.add_argument('--aa', type=str, default='rand-m9-mstd0.5-inc1', metavar='NAME',\n                        help='Use AutoAugment policy. \"v0\" or \"original\". \" + \"(default: rand-m9-mstd0.5-inc1)'),\n    parser.add_argument('--smoothing', type=float, default=0.1,\n                        help='Label smoothing (default: 0.1)')\n    parser.add_argument('--train_interpolation', type=str, default='bicubic',\n                        help='Training interpolation (random, bilinear, bicubic default: \"bicubic\")')\n\n    # Evaluation parameters\n    parser.add_argument('--crop_pct', type=float, default=None)\n\n    # * Random Erase params\n    parser.add_argument('--reprob', type=float, default=0.25, metavar='PCT',\n                        help='Random erase prob (default: 0.25)')\n    parser.add_argument('--remode', type=str, default='pixel',\n                        help='Random erase mode (default: \"pixel\")')\n    parser.add_argument('--recount', type=int, default=1,\n                        help='Random erase count (default: 1)')\n    parser.add_argument('--resplit', type=str2bool, default=False,\n                        help='Do not random erase first (clean) augmentation split')\n\n    # * Mixup params\n    parser.add_argument('--mixup', type=float, default=0.8,\n                        help='mixup alpha, mixup enabled if > 0.')\n    parser.add_argument('--cutmix', type=float, default=1.0,\n                        help='cutmix alpha, cutmix enabled if > 0.')\n    parser.add_argument('--cutmix_minmax', type=float, nargs='+', default=None,\n                        help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n    parser.add_argument('--mixup_prob', type=float, default=1.0,\n                        help='Probability of performing mixup or cutmix when either/both is enabled')\n    parser.add_argument('--mixup_switch_prob', type=float, default=0.5,\n                        help='Probability of switching to cutmix when both mixup and cutmix enabled')\n    parser.add_argument('--mixup_mode', type=str, default='batch',\n                        help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n\n    # * Finetuning params\n    parser.add_argument('--finetune', default='',\n                        help='finetune from checkpoint')\n    parser.add_argument('--head_init_scale', default=1.0, type=float,\n                        help='classifier head initial scale, typically adjusted in fine-tuning')\n    parser.add_argument('--model_key', default='model|module', type=str,\n                        help='which key to load from saved state dict, usually model or model_ema')\n    parser.add_argument('--model_prefix', default='', type=str)\n\n    # Dataset parameters\n    parser.add_argument('--data_path', default='/datasets01/imagenet_full_size/061417/', type=str,\n                        help='dataset path')\n    parser.add_argument('--eval_data_path', default=None, type=str,\n                        help='dataset path for evaluation')\n    parser.add_argument('--nb_classes', default=1000, type=int,\n                        help='number of the classification types')\n    parser.add_argument('--imagenet_default_mean_and_std', type=str2bool, default=True)\n    parser.add_argument('--data_set', default='IMNET', choices=['CIFAR', 'IMNET', 'image_folder'],\n                        type=str, help='ImageNet dataset path')\n    parser.add_argument('--output_dir', default='',\n                        help='path where to save, empty for no saving')\n    parser.add_argument('--log_dir', default=None,\n                        help='path where to tensorboard log')\n    parser.add_argument('--device', default='cuda',\n                        help='device to use for training / testing')\n    parser.add_argument('--seed', default=0, type=int)\n\n    parser.add_argument('--resume', default='',\n                        help='resume from checkpoint')\n    parser.add_argument('--auto_resume', type=str2bool, default=True)\n    parser.add_argument('--save_ckpt', type=str2bool, default=True)\n    parser.add_argument('--save_ckpt_freq', default=1, type=int)\n    parser.add_argument('--save_ckpt_num', default=3, type=int)\n\n    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n                        help='start epoch')\n    parser.add_argument('--eval', type=str2bool, default=False,\n                        help='Perform evaluation only')\n    parser.add_argument('--dist_eval', type=str2bool, default=True,\n                        help='Enabling distributed evaluation')\n    parser.add_argument('--disable_eval', type=str2bool, default=False,\n                        help='Disabling evaluation during training')\n    parser.add_argument('--num_workers', default=10, type=int)\n    parser.add_argument('--pin_mem', type=str2bool, default=True,\n                        help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n\n    # distributed training parameters\n    parser.add_argument('--world_size', default=1, type=int,\n                        help='number of distributed processes')\n    parser.add_argument('--local_rank', default=-1, type=int)\n    parser.add_argument('--dist_on_itp', type=str2bool, default=False)\n    parser.add_argument('--dist_url', default='env://',\n                        help='url used to set up distributed training')\n\n    parser.add_argument('--use_amp', type=str2bool, default=False, \n                        help=\"Use PyTorch's AMP (Automatic Mixed Precision) or not\")\n\n    # Weights and Biases arguments\n    parser.add_argument('--enable_wandb', type=str2bool, default=False,\n                        help=\"enable logging to Weights and Biases\")\n    parser.add_argument('--project', default='convnext', type=str,\n                        help=\"The name of the W&B project where you're sending the new run.\")\n    parser.add_argument('--wandb_ckpt', type=str2bool, default=False,\n                        help=\"Save model checkpoints as W&B Artifacts.\")\n\n    return parser\n\ndef main(args):\n    utils.init_distributed_mode(args)\n    print(args)\n    device = torch.device(args.device)\n\n    # fix the seed for reproducibility\n    seed = args.seed + utils.get_rank()\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    cudnn.benchmark = True\n\n    dataset_train, args.nb_classes = build_dataset(is_train=True, args=args)\n    if args.disable_eval:\n        args.dist_eval = False\n        dataset_val = None\n    else:\n        dataset_val, _ = build_dataset(is_train=False, args=args)\n\n    num_tasks = utils.get_world_size()\n    global_rank = utils.get_rank()\n\n    sampler_train = torch.utils.data.DistributedSampler(\n        dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True, seed=args.seed,\n    )\n    print(\"Sampler_train = %s\" % str(sampler_train))\n    if args.dist_eval:\n        if len(dataset_val) % num_tasks != 0:\n            print('Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. '\n                    'This will slightly alter validation results as extra duplicate entries are added to achieve '\n                    'equal num of samples per-process.')\n        sampler_val = torch.utils.data.DistributedSampler(\n            dataset_val, num_replicas=num_tasks, rank=global_rank, shuffle=False)\n    else:\n        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n\n    if global_rank == 0 and args.log_dir is not None:\n        os.makedirs(args.log_dir, exist_ok=True)\n        log_writer = utils.TensorboardLogger(log_dir=args.log_dir)\n    else:\n        log_writer = None\n\n    if global_rank == 0 and args.enable_wandb:\n        wandb_logger = utils.WandbLogger(args)\n    else:\n        wandb_logger = None\n\n    data_loader_train = torch.utils.data.DataLoader(\n        dataset_train, sampler=sampler_train,\n        batch_size=args.batch_size,\n        num_workers=args.num_workers,\n        pin_memory=args.pin_mem,\n        drop_last=True,\n    )\n\n    if dataset_val is not None:\n        data_loader_val = torch.utils.data.DataLoader(\n            dataset_val, sampler=sampler_val,\n            batch_size=int(1.5 * args.batch_size),\n            num_workers=args.num_workers,\n            pin_memory=args.pin_mem,\n            drop_last=False\n        )\n    else:\n        data_loader_val = None\n\n    mixup_fn = None\n    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n    if mixup_active:\n        print(\"Mixup is activated!\")\n        mixup_fn = Mixup(\n            mixup_alpha=args.mixup, cutmix_alpha=args.cutmix, cutmix_minmax=args.cutmix_minmax,\n            prob=args.mixup_prob, switch_prob=args.mixup_switch_prob, mode=args.mixup_mode,\n            label_smoothing=args.smoothing, num_classes=args.nb_classes)\n\n    model = create_model(\n        args.model, \n        pretrained=False, \n        num_classes=args.nb_classes, \n        drop_path_rate=args.drop_path,\n        layer_scale_init_value=args.layer_scale_init_value,\n        head_init_scale=args.head_init_scale,\n        )\n\n    if args.finetune:\n        if args.finetune.startswith('https'):\n            checkpoint = torch.hub.load_state_dict_from_url(\n                args.finetune, map_location='cpu', check_hash=True)\n        else:\n            checkpoint = torch.load(args.finetune, map_location='cpu')\n\n        print(\"Load ckpt from %s\" % args.finetune)\n        checkpoint_model = None\n        for model_key in args.model_key.split('|'):\n            if model_key in checkpoint:\n                checkpoint_model = checkpoint[model_key]\n                print(\"Load state_dict by model_key = %s\" % model_key)\n                break\n        if checkpoint_model is None:\n            checkpoint_model = checkpoint\n        state_dict = model.state_dict()\n        for k in ['head.weight', 'head.bias']:\n            if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n                print(f\"Removing key {k} from pretrained checkpoint\")\n                del checkpoint_model[k]\n        utils.load_state_dict(model, checkpoint_model, prefix=args.model_prefix)\n    model.to(device)\n\n    model_ema = None\n    if args.model_ema:\n        # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n        model_ema = ModelEma(\n            model,\n            decay=args.model_ema_decay,\n            device='cpu' if args.model_ema_force_cpu else '',\n            resume='')\n        print(\"Using EMA with decay = %.8f\" % args.model_ema_decay)\n\n    model_without_ddp = model\n    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    print(\"Model = %s\" % str(model_without_ddp))\n    print('number of params:', n_parameters)\n\n    total_batch_size = args.batch_size * args.update_freq * utils.get_world_size()\n    num_training_steps_per_epoch = len(dataset_train) // total_batch_size\n    print(\"LR = %.8f\" % args.lr)\n    print(\"Batch size = %d\" % total_batch_size)\n    print(\"Update frequent = %d\" % args.update_freq)\n    print(\"Number of training examples = %d\" % len(dataset_train))\n    print(\"Number of training training per epoch = %d\" % num_training_steps_per_epoch)\n\n    if args.layer_decay < 1.0 or args.layer_decay > 1.0:\n        num_layers = 12 # convnext layers divided into 12 parts, each with a different decayed lr value.\n        assert args.model in ['convnext_small', 'convnext_base', 'convnext_large', 'convnext_xlarge'], \\\n             \"Layer Decay impl only supports convnext_small/base/large/xlarge\"\n        assigner = LayerDecayValueAssigner(list(args.layer_decay ** (num_layers + 1 - i) for i in range(num_layers + 2)))\n    else:\n        assigner = None\n\n    if assigner is not None:\n        print(\"Assigned values = %s\" % str(assigner.values))\n\n    if args.distributed:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=False)\n        model_without_ddp = model.module\n\n    optimizer = create_optimizer(\n        args, model_without_ddp, skip_list=None,\n        get_num_layer=assigner.get_layer_id if assigner is not None else None, \n        get_layer_scale=assigner.get_scale if assigner is not None else None)\n\n    loss_scaler = NativeScaler() # if args.use_amp is False, this won't be used\n\n    print(\"Use Cosine LR scheduler\")\n    lr_schedule_values = utils.cosine_scheduler(\n        args.lr, args.min_lr, args.epochs, num_training_steps_per_epoch,\n        warmup_epochs=args.warmup_epochs, warmup_steps=args.warmup_steps,\n    )\n\n    if args.weight_decay_end is None:\n        args.weight_decay_end = args.weight_decay\n    wd_schedule_values = utils.cosine_scheduler(\n        args.weight_decay, args.weight_decay_end, args.epochs, num_training_steps_per_epoch)\n    print(\"Max WD = %.7f, Min WD = %.7f\" % (max(wd_schedule_values), min(wd_schedule_values)))\n\n    if mixup_fn is not None:\n        # smoothing is handled with mixup label transform\n        criterion = SoftTargetCrossEntropy()\n    elif args.smoothing > 0.:\n        criterion = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n    else:\n        criterion = torch.nn.CrossEntropyLoss()\n\n    print(\"criterion = %s\" % str(criterion))\n\n    utils.auto_load_model(\n        args=args, model=model, model_without_ddp=model_without_ddp,\n        optimizer=optimizer, loss_scaler=loss_scaler, model_ema=model_ema)\n\n    if args.eval:\n        print(f\"Eval only mode\")\n        test_stats = evaluate(data_loader_val, model, device, use_amp=args.use_amp)\n        print(f\"Accuracy of the network on {len(dataset_val)} test images: {test_stats['acc1']:.5f}%\")\n        return\n\n    max_accuracy = 0.0\n    if args.model_ema and args.model_ema_eval:\n        max_accuracy_ema = 0.0\n\n    print(\"Start training for %d epochs\" % args.epochs)\n    start_time = time.time()\n    for epoch in range(args.start_epoch, args.epochs):\n        if args.distributed:\n            data_loader_train.sampler.set_epoch(epoch)\n        if log_writer is not None:\n            log_writer.set_step(epoch * num_training_steps_per_epoch * args.update_freq)\n        if wandb_logger:\n            wandb_logger.set_steps()\n        train_stats = train_one_epoch(\n            model, criterion, data_loader_train, optimizer,\n            device, epoch, loss_scaler, args.clip_grad, model_ema, mixup_fn,\n            log_writer=log_writer, wandb_logger=wandb_logger, start_steps=epoch * num_training_steps_per_epoch,\n            lr_schedule_values=lr_schedule_values, wd_schedule_values=wd_schedule_values,\n            num_training_steps_per_epoch=num_training_steps_per_epoch, update_freq=args.update_freq,\n            use_amp=args.use_amp\n        )\n        if args.output_dir and args.save_ckpt:\n            if (epoch + 1) % args.save_ckpt_freq == 0 or epoch + 1 == args.epochs:\n                utils.save_model(\n                    args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,\n                    loss_scaler=loss_scaler, epoch=epoch, model_ema=model_ema)\n        if data_loader_val is not None:\n            test_stats = evaluate(data_loader_val, model, device, use_amp=args.use_amp)\n            print(f\"Accuracy of the model on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\")\n            if max_accuracy < test_stats[\"acc1\"]:\n                max_accuracy = test_stats[\"acc1\"]\n                if args.output_dir and args.save_ckpt:\n                    utils.save_model(\n                        args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,\n                        loss_scaler=loss_scaler, epoch=\"best\", model_ema=model_ema)\n            print(f'Max accuracy: {max_accuracy:.2f}%')\n\n            if log_writer is not None:\n                log_writer.update(test_acc1=test_stats['acc1'], head=\"perf\", step=epoch)\n                log_writer.update(test_acc5=test_stats['acc5'], head=\"perf\", step=epoch)\n                log_writer.update(test_loss=test_stats['loss'], head=\"perf\", step=epoch)\n\n            log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n                         **{f'test_{k}': v for k, v in test_stats.items()},\n                         'epoch': epoch,\n                         'n_parameters': n_parameters}\n\n            # repeat testing routines for EMA, if ema eval is turned on\n            if args.model_ema and args.model_ema_eval:\n                test_stats_ema = evaluate(data_loader_val, model_ema.ema, device, use_amp=args.use_amp)\n                print(f\"Accuracy of the model EMA on {len(dataset_val)} test images: {test_stats_ema['acc1']:.1f}%\")\n                if max_accuracy_ema < test_stats_ema[\"acc1\"]:\n                    max_accuracy_ema = test_stats_ema[\"acc1\"]\n                    if args.output_dir and args.save_ckpt:\n                        utils.save_model(\n                            args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,\n                            loss_scaler=loss_scaler, epoch=\"best-ema\", model_ema=model_ema)\n                    print(f'Max EMA accuracy: {max_accuracy_ema:.2f}%')\n                if log_writer is not None:\n                    log_writer.update(test_acc1_ema=test_stats_ema['acc1'], head=\"perf\", step=epoch)\n                log_stats.update({**{f'test_{k}_ema': v for k, v in test_stats_ema.items()}})\n        else:\n            log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n                         'epoch': epoch,\n                         'n_parameters': n_parameters}\n\n        if args.output_dir and utils.is_main_process():\n            if log_writer is not None:\n                log_writer.flush()\n            with open(os.path.join(args.output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n                f.write(json.dumps(log_stats) + \"\\n\")\n\n        if wandb_logger:\n            wandb_logger.log_epoch_metrics(log_stats)\n\n    if wandb_logger and args.wandb_ckpt and args.save_ckpt and args.output_dir:\n        wandb_logger.log_checkpoints()\n\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print('Training time {}'.format(total_time_str))\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser('ConvNeXt training and evaluation script', parents=[get_args_parser()])\n    args = parser.parse_args()\n    if args.output_dir:\n        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n    main(args)\n"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "object_detection",
          "type": "tree",
          "content": null
        },
        {
          "name": "optim_factory.py",
          "type": "blob",
          "size": 7.1669921875,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport torch\nfrom torch import optim as optim\n\nfrom timm.optim.adafactor import Adafactor\nfrom timm.optim.adahessian import Adahessian\nfrom timm.optim.adamp import AdamP\nfrom timm.optim.lookahead import Lookahead\nfrom timm.optim.nadam import Nadam\nfrom timm.optim.novograd import NovoGrad\nfrom timm.optim.nvnovograd import NvNovoGrad\nfrom timm.optim.radam import RAdam\nfrom timm.optim.rmsprop_tf import RMSpropTF\nfrom timm.optim.sgdp import SGDP\n\nimport json\n\ntry:\n    from apex.optimizers import FusedNovoGrad, FusedAdam, FusedLAMB, FusedSGD\n    has_apex = True\nexcept ImportError:\n    has_apex = False\n\n\ndef get_num_layer_for_convnext(var_name):\n    \"\"\"\n    Divide [3, 3, 27, 3] layers into 12 groups; each group is three \n    consecutive blocks, including possible neighboring downsample layers;\n    adapted from https://github.com/microsoft/unilm/blob/master/beit/optim_factory.py\n    \"\"\"\n    num_max_layer = 12\n    if var_name.startswith(\"downsample_layers\"):\n        stage_id = int(var_name.split('.')[1])\n        if stage_id == 0:\n            layer_id = 0\n        elif stage_id == 1 or stage_id == 2:\n            layer_id = stage_id + 1\n        elif stage_id == 3:\n            layer_id = 12\n        return layer_id\n\n    elif var_name.startswith(\"stages\"):\n        stage_id = int(var_name.split('.')[1])\n        block_id = int(var_name.split('.')[2])\n        if stage_id == 0 or stage_id == 1:\n            layer_id = stage_id + 1\n        elif stage_id == 2:\n            layer_id = 3 + block_id // 3 \n        elif stage_id == 3:\n            layer_id = 12\n        return layer_id\n    else:\n        return num_max_layer + 1\n\nclass LayerDecayValueAssigner(object):\n    def __init__(self, values):\n        self.values = values\n\n    def get_scale(self, layer_id):\n        return self.values[layer_id]\n\n    def get_layer_id(self, var_name):\n        return get_num_layer_for_convnext(var_name)\n\n\ndef get_parameter_groups(model, weight_decay=1e-5, skip_list=(), get_num_layer=None, get_layer_scale=None):\n    parameter_group_names = {}\n    parameter_group_vars = {}\n\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue  # frozen weights\n        if len(param.shape) == 1 or name.endswith(\".bias\") or name in skip_list:\n            group_name = \"no_decay\"\n            this_weight_decay = 0.\n        else:\n            group_name = \"decay\"\n            this_weight_decay = weight_decay\n        if get_num_layer is not None:\n            layer_id = get_num_layer(name)\n            group_name = \"layer_%d_%s\" % (layer_id, group_name)\n        else:\n            layer_id = None\n\n        if group_name not in parameter_group_names:\n            if get_layer_scale is not None:\n                scale = get_layer_scale(layer_id)\n            else:\n                scale = 1.\n\n            parameter_group_names[group_name] = {\n                \"weight_decay\": this_weight_decay,\n                \"params\": [],\n                \"lr_scale\": scale\n            }\n            parameter_group_vars[group_name] = {\n                \"weight_decay\": this_weight_decay,\n                \"params\": [],\n                \"lr_scale\": scale\n            }\n\n        parameter_group_vars[group_name][\"params\"].append(param)\n        parameter_group_names[group_name][\"params\"].append(name)\n    print(\"Param groups = %s\" % json.dumps(parameter_group_names, indent=2))\n    return list(parameter_group_vars.values())\n\n\ndef create_optimizer(args, model, get_num_layer=None, get_layer_scale=None, filter_bias_and_bn=True, skip_list=None):\n    opt_lower = args.opt.lower()\n    weight_decay = args.weight_decay\n    # if weight_decay and filter_bias_and_bn:\n    if filter_bias_and_bn:\n        skip = {}\n        if skip_list is not None:\n            skip = skip_list\n        elif hasattr(model, 'no_weight_decay'):\n            skip = model.no_weight_decay()\n        parameters = get_parameter_groups(model, weight_decay, skip, get_num_layer, get_layer_scale)\n        weight_decay = 0.\n    else:\n        parameters = model.parameters()\n\n    if 'fused' in opt_lower:\n        assert has_apex and torch.cuda.is_available(), 'APEX and CUDA required for fused optimizers'\n\n    opt_args = dict(lr=args.lr, weight_decay=weight_decay)\n    if hasattr(args, 'opt_eps') and args.opt_eps is not None:\n        opt_args['eps'] = args.opt_eps\n    if hasattr(args, 'opt_betas') and args.opt_betas is not None:\n        opt_args['betas'] = args.opt_betas\n\n    opt_split = opt_lower.split('_')\n    opt_lower = opt_split[-1]\n    if opt_lower == 'sgd' or opt_lower == 'nesterov':\n        opt_args.pop('eps', None)\n        optimizer = optim.SGD(parameters, momentum=args.momentum, nesterov=True, **opt_args)\n    elif opt_lower == 'momentum':\n        opt_args.pop('eps', None)\n        optimizer = optim.SGD(parameters, momentum=args.momentum, nesterov=False, **opt_args)\n    elif opt_lower == 'adam':\n        optimizer = optim.Adam(parameters, **opt_args)\n    elif opt_lower == 'adamw':\n        optimizer = optim.AdamW(parameters, **opt_args)\n    elif opt_lower == 'nadam':\n        optimizer = Nadam(parameters, **opt_args)\n    elif opt_lower == 'radam':\n        optimizer = RAdam(parameters, **opt_args)\n    elif opt_lower == 'adamp':\n        optimizer = AdamP(parameters, wd_ratio=0.01, nesterov=True, **opt_args)\n    elif opt_lower == 'sgdp':\n        optimizer = SGDP(parameters, momentum=args.momentum, nesterov=True, **opt_args)\n    elif opt_lower == 'adadelta':\n        optimizer = optim.Adadelta(parameters, **opt_args)\n    elif opt_lower == 'adafactor':\n        if not args.lr:\n            opt_args['lr'] = None\n        optimizer = Adafactor(parameters, **opt_args)\n    elif opt_lower == 'adahessian':\n        optimizer = Adahessian(parameters, **opt_args)\n    elif opt_lower == 'rmsprop':\n        optimizer = optim.RMSprop(parameters, alpha=0.9, momentum=args.momentum, **opt_args)\n    elif opt_lower == 'rmsproptf':\n        optimizer = RMSpropTF(parameters, alpha=0.9, momentum=args.momentum, **opt_args)\n    elif opt_lower == 'novograd':\n        optimizer = NovoGrad(parameters, **opt_args)\n    elif opt_lower == 'nvnovograd':\n        optimizer = NvNovoGrad(parameters, **opt_args)\n    elif opt_lower == 'fusedsgd':\n        opt_args.pop('eps', None)\n        optimizer = FusedSGD(parameters, momentum=args.momentum, nesterov=True, **opt_args)\n    elif opt_lower == 'fusedmomentum':\n        opt_args.pop('eps', None)\n        optimizer = FusedSGD(parameters, momentum=args.momentum, nesterov=False, **opt_args)\n    elif opt_lower == 'fusedadam':\n        optimizer = FusedAdam(parameters, adam_w_mode=False, **opt_args)\n    elif opt_lower == 'fusedadamw':\n        optimizer = FusedAdam(parameters, adam_w_mode=True, **opt_args)\n    elif opt_lower == 'fusedlamb':\n        optimizer = FusedLAMB(parameters, **opt_args)\n    elif opt_lower == 'fusednovograd':\n        opt_args.setdefault('betas', (0.95, 0.98))\n        optimizer = FusedNovoGrad(parameters, **opt_args)\n    else:\n        assert False and \"Invalid optimizer\"\n\n    if len(opt_split) > 1:\n        if opt_split[0] == 'lookahead':\n            optimizer = Lookahead(optimizer)\n\n    return optimizer\n"
        },
        {
          "name": "run_with_submitit.py",
          "type": "blob",
          "size": 3.91796875,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport argparse\nimport os\nimport uuid\nfrom pathlib import Path\n\nimport main as classification\nimport submitit\n\ndef parse_args():\n    classification_parser = classification.get_args_parser()\n    parser = argparse.ArgumentParser(\"Submitit for ConvNeXt\", parents=[classification_parser])\n    parser.add_argument(\"--ngpus\", default=8, type=int, help=\"Number of gpus to request on each node\")\n    parser.add_argument(\"--nodes\", default=2, type=int, help=\"Number of nodes to request\")\n    parser.add_argument(\"--timeout\", default=72, type=int, help=\"Duration of the job, in hours\")\n    parser.add_argument(\"--job_name\", default=\"convnext\", type=str, help=\"Job name\")\n    parser.add_argument(\"--job_dir\", default=\"\", type=str, help=\"Job directory; leave empty for default\")\n    parser.add_argument(\"--partition\", default=\"learnlab\", type=str, help=\"Partition where to submit\")\n    parser.add_argument(\"--use_volta32\", action='store_true', default=True, help=\"Big models? Use this\")\n    parser.add_argument('--comment', default=\"\", type=str,\n                        help='Comment to pass to scheduler, e.g. priority message')\n    return parser.parse_args()\n\ndef get_shared_folder() -> Path:\n    user = os.getenv(\"USER\")\n    if Path(\"/checkpoint/\").is_dir():\n        p = Path(f\"/checkpoint/{user}/convnext\")\n        p.mkdir(exist_ok=True)\n        return p\n    raise RuntimeError(\"No shared folder available\")\n\ndef get_init_file():\n    # Init file must not exist, but it's parent dir must exist.\n    os.makedirs(str(get_shared_folder()), exist_ok=True)\n    init_file = get_shared_folder() / f\"{uuid.uuid4().hex}_init\"\n    if init_file.exists():\n        os.remove(str(init_file))\n    return init_file\n\nclass Trainer(object):\n    def __init__(self, args):\n        self.args = args\n\n    def __call__(self):\n        import main as classification\n\n        self._setup_gpu_args()\n        classification.main(self.args)\n\n    def checkpoint(self):\n        import os\n        import submitit\n\n        self.args.dist_url = get_init_file().as_uri()\n        self.args.auto_resume = True\n        print(\"Requeuing \", self.args)\n        empty_trainer = type(self)(self.args)\n        return submitit.helpers.DelayedSubmission(empty_trainer)\n\n    def _setup_gpu_args(self):\n        import submitit\n        from pathlib import Path\n\n        job_env = submitit.JobEnvironment()\n        self.args.output_dir = Path(self.args.job_dir)\n        self.args.gpu = job_env.local_rank\n        self.args.rank = job_env.global_rank\n        self.args.world_size = job_env.num_tasks\n        print(f\"Process group: {job_env.num_tasks} tasks, rank: {job_env.global_rank}\")\n\n\ndef main():\n    args = parse_args()\n    \n    if args.job_dir == \"\":\n        args.job_dir = get_shared_folder() / \"%j\"\n\n    executor = submitit.AutoExecutor(folder=args.job_dir, slurm_max_num_timeout=30)\n\n    num_gpus_per_node = args.ngpus\n    nodes = args.nodes\n    timeout_min = args.timeout * 60\n\n    partition = args.partition\n    kwargs = {}\n    if args.use_volta32:\n        kwargs['slurm_constraint'] = 'volta32gb'\n    if args.comment:\n        kwargs['slurm_comment'] = args.comment\n\n    executor.update_parameters(\n        mem_gb=40 * num_gpus_per_node,\n        gpus_per_node=num_gpus_per_node,\n        tasks_per_node=num_gpus_per_node,  # one task per GPU\n        cpus_per_task=10,\n        nodes=nodes,\n        timeout_min=timeout_min,  # max is 60 * 72\n        # Below are cluster dependent parameters\n        slurm_partition=partition,\n        slurm_signal_delay_s=120,\n        **kwargs\n    )\n\n    executor.update_parameters(name=args.job_name)\n\n    args.dist_url = get_init_file().as_uri()\n    args.output_dir = args.job_dir\n\n    trainer = Trainer(args)\n    job = executor.submit(trainer)\n\n    print(\"Submitted job_id:\", job.job_id)\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "semantic_segmentation",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 16.93359375,
          "content": "# Copyright (c) Meta Platforms, Inc. and affiliates.\n\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport os\nimport math\nimport time\nfrom collections import defaultdict, deque\nimport datetime\nimport numpy as np\nfrom timm.utils import get_state_dict\n\nfrom pathlib import Path\n\nimport torch\nimport torch.distributed as dist\nfrom torch._six import inf\n\nfrom tensorboardX import SummaryWriter\n\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)\n\n\nclass MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if v is None:\n                continue\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n            type(self).__name__, attr))\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\n                \"{}: {}\".format(name, str(meter))\n            )\n        return self.delimiter.join(loss_str)\n\n    def synchronize_between_processes(self):\n        for meter in self.meters.values():\n            meter.synchronize_between_processes()\n\n    def add_meter(self, name, meter):\n        self.meters[name] = meter\n\n    def log_every(self, iterable, print_freq, header=None):\n        i = 0\n        if not header:\n            header = ''\n        start_time = time.time()\n        end = time.time()\n        iter_time = SmoothedValue(fmt='{avg:.4f}')\n        data_time = SmoothedValue(fmt='{avg:.4f}')\n        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n        log_msg = [\n            header,\n            '[{0' + space_fmt + '}/{1}]',\n            'eta: {eta}',\n            '{meters}',\n            'time: {time}',\n            'data: {data}'\n        ]\n        if torch.cuda.is_available():\n            log_msg.append('max mem: {memory:.0f}')\n        log_msg = self.delimiter.join(log_msg)\n        MB = 1024.0 * 1024.0\n        for obj in iterable:\n            data_time.update(time.time() - end)\n            yield obj\n            iter_time.update(time.time() - end)\n            if i % print_freq == 0 or i == len(iterable) - 1:\n                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n                if torch.cuda.is_available():\n                    print(log_msg.format(\n                        i, len(iterable), eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time),\n                        memory=torch.cuda.max_memory_allocated() / MB))\n                else:\n                    print(log_msg.format(\n                        i, len(iterable), eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time)))\n            i += 1\n            end = time.time()\n        total_time = time.time() - start_time\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        print('{} Total time: {} ({:.4f} s / it)'.format(\n            header, total_time_str, total_time / len(iterable)))\n\n\nclass TensorboardLogger(object):\n    def __init__(self, log_dir):\n        self.writer = SummaryWriter(logdir=log_dir)\n        self.step = 0\n\n    def set_step(self, step=None):\n        if step is not None:\n            self.step = step\n        else:\n            self.step += 1\n\n    def update(self, head='scalar', step=None, **kwargs):\n        for k, v in kwargs.items():\n            if v is None:\n                continue\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.writer.add_scalar(head + \"/\" + k, v, self.step if step is None else step)\n\n    def flush(self):\n        self.writer.flush()\n\n\nclass WandbLogger(object):\n    def __init__(self, args):\n        self.args = args\n\n        try:\n            import wandb\n            self._wandb = wandb\n        except ImportError:\n            raise ImportError(\n                \"To use the Weights and Biases Logger please install wandb.\"\n                \"Run `pip install wandb` to install it.\"\n            )\n\n        # Initialize a W&B run \n        if self._wandb.run is None:\n            self._wandb.init(\n                project=args.project,\n                config=args\n            )\n\n    def log_epoch_metrics(self, metrics, commit=True):\n        \"\"\"\n        Log train/test metrics onto W&B.\n        \"\"\"\n        # Log number of model parameters as W&B summary\n        self._wandb.summary['n_parameters'] = metrics.get('n_parameters', None)\n        metrics.pop('n_parameters', None)\n\n        # Log current epoch\n        self._wandb.log({'epoch': metrics.get('epoch')}, commit=False)\n        metrics.pop('epoch')\n\n        for k, v in metrics.items():\n            if 'train' in k:\n                self._wandb.log({f'Global Train/{k}': v}, commit=False)\n            elif 'test' in k:\n                self._wandb.log({f'Global Test/{k}': v}, commit=False)\n\n        self._wandb.log({})\n\n    def log_checkpoints(self):\n        output_dir = self.args.output_dir\n        model_artifact = self._wandb.Artifact(\n            self._wandb.run.id + \"_model\", type=\"model\"\n        )\n\n        model_artifact.add_dir(output_dir)\n        self._wandb.log_artifact(model_artifact, aliases=[\"latest\", \"best\"])\n\n    def set_steps(self):\n        # Set global training step\n        self._wandb.define_metric('Rank-0 Batch Wise/*', step_metric='Rank-0 Batch Wise/global_train_step')\n        # Set epoch-wise step\n        self._wandb.define_metric('Global Train/*', step_metric='epoch')\n        self._wandb.define_metric('Global Test/*', step_metric='epoch')\n\n\ndef setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print\n\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\n\n\ndef init_distributed_mode(args):\n\n    if args.dist_on_itp:\n        args.rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n        args.world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n        args.gpu = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n        args.dist_url = \"tcp://%s:%s\" % (os.environ['MASTER_ADDR'], os.environ['MASTER_PORT'])\n        os.environ['LOCAL_RANK'] = str(args.gpu)\n        os.environ['RANK'] = str(args.rank)\n        os.environ['WORLD_SIZE'] = str(args.world_size)\n        # [\"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\"]\n    elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n    elif 'SLURM_PROCID' in os.environ:\n        args.rank = int(os.environ['SLURM_PROCID'])\n        args.gpu = args.rank % torch.cuda.device_count()\n\n        os.environ['RANK'] = str(args.rank)\n        os.environ['LOCAL_RANK'] = str(args.gpu)\n        os.environ['WORLD_SIZE'] = str(args.world_size)\n    else:\n        print('Not using distributed mode')\n        args.distributed = False\n        return\n\n    args.distributed = True\n\n    torch.cuda.set_device(args.gpu)\n    args.dist_backend = 'nccl'\n    print('| distributed init (rank {}): {}, gpu {}'.format(\n        args.rank, args.dist_url, args.gpu), flush=True)\n    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n                                         world_size=args.world_size, rank=args.rank)\n    torch.distributed.barrier()\n    setup_for_distributed(args.rank == 0)\n\n\ndef load_state_dict(model, state_dict, prefix='', ignore_missing=\"relative_position_index\"):\n    missing_keys = []\n    unexpected_keys = []\n    error_msgs = []\n    # copy state_dict so _load_from_state_dict can modify it\n    metadata = getattr(state_dict, '_metadata', None)\n    state_dict = state_dict.copy()\n    if metadata is not None:\n        state_dict._metadata = metadata\n\n    def load(module, prefix=''):\n        local_metadata = {} if metadata is None else metadata.get(\n            prefix[:-1], {})\n        module._load_from_state_dict(\n            state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + '.')\n\n    load(model, prefix=prefix)\n\n    warn_missing_keys = []\n    ignore_missing_keys = []\n    for key in missing_keys:\n        keep_flag = True\n        for ignore_key in ignore_missing.split('|'):\n            if ignore_key in key:\n                keep_flag = False\n                break\n        if keep_flag:\n            warn_missing_keys.append(key)\n        else:\n            ignore_missing_keys.append(key)\n\n    missing_keys = warn_missing_keys\n\n    if len(missing_keys) > 0:\n        print(\"Weights of {} not initialized from pretrained model: {}\".format(\n            model.__class__.__name__, missing_keys))\n    if len(unexpected_keys) > 0:\n        print(\"Weights from pretrained model not used in {}: {}\".format(\n            model.__class__.__name__, unexpected_keys))\n    if len(ignore_missing_keys) > 0:\n        print(\"Ignored weights of {} not initialized from pretrained model: {}\".format(\n            model.__class__.__name__, ignore_missing_keys))\n    if len(error_msgs) > 0:\n        print('\\n'.join(error_msgs))\n\n\nclass NativeScalerWithGradNormCount:\n    state_dict_key = \"amp_scaler\"\n\n    def __init__(self):\n        self._scaler = torch.cuda.amp.GradScaler()\n\n    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n        self._scaler.scale(loss).backward(create_graph=create_graph)\n        if update_grad:\n            if clip_grad is not None:\n                assert parameters is not None\n                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n            else:\n                self._scaler.unscale_(optimizer)\n                norm = get_grad_norm_(parameters)\n            self._scaler.step(optimizer)\n            self._scaler.update()\n        else:\n            norm = None\n        return norm\n\n    def state_dict(self):\n        return self._scaler.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self._scaler.load_state_dict(state_dict)\n\n\ndef get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = [p for p in parameters if p.grad is not None]\n    norm_type = float(norm_type)\n    if len(parameters) == 0:\n        return torch.tensor(0.)\n    device = parameters[0].grad.device\n    if norm_type == inf:\n        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n    else:\n        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n    return total_norm\n\n\ndef cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0,\n                     start_warmup_value=0, warmup_steps=-1):\n    warmup_schedule = np.array([])\n    warmup_iters = warmup_epochs * niter_per_ep\n    if warmup_steps > 0:\n        warmup_iters = warmup_steps\n    print(\"Set warmup steps = %d\" % warmup_iters)\n    if warmup_epochs > 0:\n        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n\n    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n    schedule = np.array(\n        [final_value + 0.5 * (base_value - final_value) * (1 + math.cos(math.pi * i / (len(iters)))) for i in iters])\n\n    schedule = np.concatenate((warmup_schedule, schedule))\n\n    assert len(schedule) == epochs * niter_per_ep\n    return schedule\n\ndef save_model(args, epoch, model, model_without_ddp, optimizer, loss_scaler, model_ema=None):\n    output_dir = Path(args.output_dir)\n    epoch_name = str(epoch)\n    checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]\n    for checkpoint_path in checkpoint_paths:\n        to_save = {\n            'model': model_without_ddp.state_dict(),\n            'optimizer': optimizer.state_dict(),\n            'epoch': epoch,\n            'scaler': loss_scaler.state_dict(),\n            'args': args,\n        }\n\n        if model_ema is not None:\n            to_save['model_ema'] = get_state_dict(model_ema)\n\n        save_on_master(to_save, checkpoint_path)\n    \n    if is_main_process() and isinstance(epoch, int):\n        to_del = epoch - args.save_ckpt_num * args.save_ckpt_freq\n        old_ckpt = output_dir / ('checkpoint-%s.pth' % to_del)\n        if os.path.exists(old_ckpt):\n            os.remove(old_ckpt)\n\n\ndef auto_load_model(args, model, model_without_ddp, optimizer, loss_scaler, model_ema=None):\n    output_dir = Path(args.output_dir)\n    if args.auto_resume and len(args.resume) == 0:\n        import glob\n        all_checkpoints = glob.glob(os.path.join(output_dir, 'checkpoint-*.pth'))\n        latest_ckpt = -1\n        for ckpt in all_checkpoints:\n            t = ckpt.split('-')[-1].split('.')[0]\n            if t.isdigit():\n                latest_ckpt = max(int(t), latest_ckpt)\n        if latest_ckpt >= 0:\n            args.resume = os.path.join(output_dir, 'checkpoint-%d.pth' % latest_ckpt)\n        print(\"Auto resume checkpoint: %s\" % args.resume)\n\n    if args.resume:\n        if args.resume.startswith('https'):\n            checkpoint = torch.hub.load_state_dict_from_url(\n                args.resume, map_location='cpu', check_hash=True)\n        else:\n            checkpoint = torch.load(args.resume, map_location='cpu')\n        model_without_ddp.load_state_dict(checkpoint['model'])\n        print(\"Resume checkpoint %s\" % args.resume)\n        if 'optimizer' in checkpoint and 'epoch' in checkpoint:\n            optimizer.load_state_dict(checkpoint['optimizer'])\n            if not isinstance(checkpoint['epoch'], str): # does not support resuming with 'best', 'best-ema'\n                args.start_epoch = checkpoint['epoch'] + 1\n            else:\n                assert args.eval, 'Does not support resuming with checkpoint-best'\n            if hasattr(args, 'model_ema') and args.model_ema:\n                if 'model_ema' in checkpoint.keys():\n                    model_ema.ema.load_state_dict(checkpoint['model_ema'])\n                else:\n                    model_ema.ema.load_state_dict(checkpoint['model'])\n            if 'scaler' in checkpoint:\n                loss_scaler.load_state_dict(checkpoint['scaler'])\n            print(\"With optim & sched!\")\n"
        }
      ]
    }
  ]
}