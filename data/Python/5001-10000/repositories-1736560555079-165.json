{
  "metadata": {
    "timestamp": 1736560555079,
    "page": 165,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "abetlen/llama-cpp-python",
      "stars": 8397,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 3.0322265625,
          "content": "_skbuild/\n\n.envrc\n\nmodels/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n.idea/\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.205078125,
          "content": "*.local\n\n.python-version\n\n.vscode/\n\n_skbuild/\n\n.envrc\n.direnv\n\nmodels/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\nllama_cpp/*.so\nllama_cpp/*.dylib\nllama_cpp/*.metal\nllama_cpp/*.dll\nllama_cpp/*.lib\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n.idea/\n\n# downloaded model .bin files\ndocker/open_llama/*.bin\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.103515625,
          "content": "[submodule \"vendor/llama.cpp\"]\n\tpath = vendor/llama.cpp\n\turl = https://github.com/ggerganov/llama.cpp.git\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.43359375,
          "content": "# Read the Docs configuration file for MkDocs projects\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Set the version of Python and other tools you might need\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.11\"\n\nmkdocs:\n  configuration: mkdocs.yml\n\npython:\n  install:\n    - method: pip\n      path: .\n    - requirements: docs/requirements.txt\n\nsubmodules:\n  include: all\n  recursive: true"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 39.064453125,
          "content": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n## [0.3.6]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@f7cd13301c2a88f97073fd119072b4cc92c08df1\n- fix(server): streaming resource lock by @gjpower in #1879\n\n## [0.3.5]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@26a8406ba9198eb6fdd8329fa717555b4f77f05f\n- fix(ci): Fix release by updating macos runner image to non-deprecated version by @abetlen in afedfc888462f9a6e809dc9455eb3b663764cc3f\n- fix(server): add missing await statements for async exit_stack handling by @gjpower in #1858\n\n## [0.3.4]\n\n- fix(ci): Build wheels for macos 13-15, cuda 12.1-12.4 by @abetlen in ca808028bd16b8327bd84128d48015a4b1304690\n\n## [0.3.3]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@ce8784bdb153ff7794dde5a50b0ebfa51baa6171\n- fix: chat API logprobs format by @domdomegg in #1788\n- feat: Add support for CUDA 12.6, fix CUDA 12.5 by @Smartappli in #1775\n- fix: Make content not required in ChatCompletionRequestAssistantMessage by @feloy in #1807\n- fix: Fix pickling of Llama class by setting seed from _seed member by @abetlen in 2523472c3eccb9ab9277117cc4ff705212b6888a\n- fix: Fix logit-bias type hint by @ddh0 in #1802\n- fix(server): Avoid thread starvation on many concurrent requests by making use of asyncio to lock llama_proxy context by @gjpower in #1798\n- fix(server): Added missing exit_stack.close() to /v1/chat/completions by @Ian321 in #1796\n- fix(examples): Refactor Batching notebook to use new sampler chain API by @lukestanley in #1793\n- fix(docs): Update development instructions by @Florents-Tselai in #1833\n- fix(docs): Remove ref to llama_eval in llama_cpp.py docs by @richdougherty in #1819\n\n## [0.3.2]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@74d73dc85cc2057446bf63cc37ff649ae7cebd80\n\n## [0.3.1]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@c919d5db39c8a7fcb64737f008e4b105ee0acd20\n- feat: Expose libggml in internal APIs by @abetlen in #1761\n- fix: Fix speculative decoding by @abetlen in 9992c5084a3df2f533e265d10f81d4269b97a1e6 and e975dabf74b3ad85689c9a07719cbb181313139b\n- misc: Rename all_text to remaining_text by @xu-song in #1658\n\n## [0.3.0]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@ea9c32be71b91b42ecc538bd902e93cbb5fb36cb\n- feat: Enable detokenizing special tokens with special=True by @benniekiss in #1596\n- feat(ci): Speed up CI workflows using uv, add support for CUDA 12.5 wheels by @Smartappli in e529940f45d42ed8aa31334123b8d66bc67b0e78\n- feat: Add loading sharded GGUF files from HuggingFace with Llama.from_pretrained(additional_files=[...]) by @Gnurro in 84c092063e8f222758dd3d60bdb2d1d342ac292e\n- feat: Add option to configure n_ubatch by @abetlen in 6c44a3f36b089239cb6396bb408116aad262c702\n- feat: Update sampling API for llama.cpp. Sampling now uses sampler chain by @abetlen in f8fcb3ea3424bcfba3a5437626a994771a02324b\n- fix: Don't store scores internally unless logits_all=True. Reduces memory requirements for large context by @abetlen in 29afcfdff5e75d7df4c13bad0122c98661d251ab\n- fix: Fix memory allocation of ndarray in by @xu-song in #1704\n- fix: Use system message in og qwen format by @abetlen in 98eb092d3c6e7c142c4ba2faaca6c091718abbb3\n\n\n## [0.2.90]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@1d1ccce67613674c75c9c7e3fa4c1e24e428ba48\n- feat: Add support for `MiniCPMv26ChatHandler` and `minicpm-v-26` in server by @abetlen in f70df824985d875226793b94dacc0c302a4256b2\n\n## [0.2.89]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@cfac111e2b3953cdb6b0126e67a2487687646971\n- fix: Llama.close didn't free lora adapter by @jkawamoto in #1679\n- fix: missing dependencies for test by @jkawamoto in #1680\n\n## [0.2.88]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@fc4ca27b25464a11b3b86c9dbb5b6ed6065965c2\n- fix: only print 'cache saved' in verbose mode by @lsorber in #1668 \n- fix: Added back from_file method to LlamaGrammar by @ExtReMLapin in #1673\n- fix: grammar prints on each call by @abetlen in 0998ea0deea076a547d54bd598d6b413b588ee2b\n- feat: Enable recursive search of HFFS.ls when using from_pretrained by @benHeidabetlen in #1656\n- feat: Add more detailed log for prefix-match by @xu-song in #1659\n\n## [0.2.87]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@be55695eff44784a141a863f273661a6bce63dfc\n- fix: Include all llama.cpp source files and subdirectories by @abetlen in 9cad5714ae6e7c250af8d0bbb179f631368c928b\n- feat(ci): Re-build wheel index automatically when releases are created by @abetlen in 198f47dc1bd202fd2b71b29e041a9f33fe40bfad\n\n## [0.2.86]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@398ede5efeb07b9adf9fbda7ea63f630d476a792\n- feat: Ported back new grammar changes from C++ to Python implementation by @ExtReMLapin in (#1637)\n- fix: llama_grammar_accept_token arg order by @tc-wolf in (#1649)\n\n## [0.2.85]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@398ede5efeb07b9adf9fbda7ea63f630d476a792\n- fix: Missing LoRA adapter after API change by @shamitv in #1630\n- fix(docker): Update Dockerfile BLAS options by @olivierdebauche in #1632\n- fix(docker): Fix GGML_CUDA param by @olivierdebauche in #1633\n- fix(docker): Update Dockerfile build options from `LLAMA_` to `GGML_` by @olivierdebauche in #1634\n- feat: FreeBSD compatibility by @yurivict in #1635\n\n## [0.2.84]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@4730faca618ff9cee0780580145e3cbe86f24876\n- fix: fix: Correcting run.sh filepath in Simple Docker implementation by @mashuk999 in #1626\n\n## [0.2.83]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@081fe431aa8fb6307145c4feb3eed4f48cab19f8\n- feat: Add 'required' literal to ChatCompletionToolChoiceOption by @mjschock in #1597\n- fix: Change repeat_penalty to 1.0 to match llama.cpp defaults by @ddh0 in #1590\n- fix(docs): Update README.md typo by @ericcurtin in #1589\n- fix(server): Use split_mode from model settings by @grider-withourai in #1594\n- feat(ci): Dockerfile update base images and post-install cleanup by @Smartappli in #1530\n\n## [0.2.82]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@7fdb6f73e35605c8dbc39e9f19cd9ed84dbc87f2\n\n## [0.2.81]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@968967376dc2c018d29f897c4883d335bbf384fb\n- fix(ci): Fix CUDA wheels, use LLAMA_CUDA instead of removed LLAMA_CUBLAS by @abetlen in 4fb6fc12a02a68884c25dd9f6a421cacec7604c6\n- fix(ci): Fix MacOS release, use macos-12 image instead of removed macos-11 by @abetlen in 3a551eb5263fdbd24b36d7770856374c04e92788\n\n## [0.2.80]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@023b8807e10bc3ade24a255f01c1ad2a01bb4228\n- fix(server): Fix bug in FastAPI streaming response where dependency was released before request completes causing SEGFAULT by @abetlen in 296304b60bb83689659883c9cc24f4c074dd88ff\n- fix(server): Update default config value for embeddings to False to fix error in text generation where logits were not allocated by llama.cpp by @abetlen in bf5e0bb4b151f4ca2f5a21af68eb832a96a79d75\n- fix(ci): Fix the CUDA workflow by @oobabooga in #1551\n- docs: Update readme examples to use newer Qwen2 model by @jncraton in #1544\n\n## [0.2.79]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@9c77ec1d74874ee22bdef8f110e8e8d41389abf2\n- feat(ci): Update workflows and pre-built wheels by @Smartappli in #1416\n- feat: Add .close() method to Llama class to explicitly free model from memory by @jkawamoto in #1513\n- feat: Support SPM infill by @CISC in #1492\n\n## [0.2.78]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@fd5ea0f897ecb3659d6c269ef6f3d833e865ead7\n- fix: Avoid duplicate special tokens in chat formats by @CISC in #1439\n- fix: fix logprobs when BOS is not present by @ghorbani in #1471\n- feat: adding rpc_servers parameter to Llama class by @chraac in #1477\n\n## [0.2.77]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@bde7cd3cd949c1a85d3a199498ac98e78039d46f\n- fix: string value kv_overrides by @abetlen in df45a4b3fe46e72664bda87301b318210c6d4782\n- fix: Fix typo in Llama3VisionAlphaChatHandler by @abetlen in 165b4dc6c188f8fda2fc616154e111f710484eba\n- fix: Use numpy recarray for candidates data, fixes bug with temp < 0 by @abetlen in af3ed503e9ce60fe6b5365031abad4176a3536b3\nfix: Disable Windows+CUDA workaround when compiling for HIPBLAS by Engininja2 in #1493\n\n## [0.2.76]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@0df0aa8e43c3378975269a51f9b876c8692e70da\n- feat: Improve Llama.eval performance by avoiding list conversion by @thoughtp0lice in #1476\n- example: LLM inference with Ray Serve by @rgerganov in #1465\n\n## [0.2.75]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@13ad16af1231ab2d245d35df3295bcfa23de1305\n- fix: segfault for models without eos / bos tokens by @abetlen in d99a6ba607a4885fb00e63e967964aa41bdbbbcb\n- feat: add MinTokensLogitProcessor and min_tokens argument to server by @twaka in #1333\n- misc: Remove unnecessary metadata lookups by @CISC in #1448\n\n## [0.2.74]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@b228aba91ac2cd9eb90e9d423ba1d0d20e0117e2\n- fix: Enable CUDA backend for llava by @abetlen in 7f59856fa6f3e23f07e12fc15aeb9359dc6c3bb4\n- docs: Fix typo in README.md by @yupbank in #1444\n\n## [0.2.73]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@25c6e82e7a1ad25a42b0894e87d9b5c557409516\n- fix: Clear kv cache at beginning of image chat formats to avoid bug when image is evaluated first by @abetlen in ac55d0a175115d1e719672ce1cb1bec776c738b1\n\n## [0.2.72]\n\n- fix(security): Remote Code Execution by Server-Side Template Injection in Model Metadata by @retr0reg in b454f40a9a1787b2b5659cd2cb00819d983185df\n- fix(security): Update remaining jinja chat templates to use immutable sandbox by @CISC in #1441\n\n## [0.2.71]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@911b3900dded9a1cfe0f0e41b82c7a29baf3a217\n- fix: Make leading bos_token optional for image chat formats, fix nanollava system message by @abetlen in 77122638b4153e31d9f277b3d905c2900b536632\n- fix: free last image embed in llava chat handler by @abetlen in 3757328b703b2cd32dcbd5853271e3a8c8599fe7\n\n## [0.2.70]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@c0e6fbf8c380718102bd25fcb8d2e55f8f9480d1\n- feat: fill-in-middle support by @CISC in #1386\n- fix: adding missing args in create_completion for functionary chat handler by @skalade in #1430\n- docs: update README.md @eltociear in #1432\n- fix: chat_format log where auto-detected format prints None by @balvisio in #1434\n- feat(server): Add support for setting root_path by @abetlen in 0318702cdc860999ee70f277425edbbfe0e60419\n- feat(ci): Add docker checks and check deps more frequently by @Smartappli in #1426\n- fix: detokenization case where first token does not start with a leading space by @noamgat in #1375\n- feat: Implement streaming for Functionary v2 + Bug fixes by @jeffrey-fong in #1419\n- fix: Use memmove to copy str_value kv_override by @abetlen in 9f7a85571ae80d3b6ddbd3e1bae407b9f1e3448a\n- feat(server): Remove temperature bounds checks for server by @abetlen in 0a454bebe67d12a446981eb16028c168ca5faa81\n- fix(server): Propagate flash_attn to model load by @dthuerck in #1424\n\n## [0.2.69]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@6ecf3189e00a1e8e737a78b6d10e1d7006e050a2\n- feat: Add llama-3-vision-alpha chat format by @abetlen in 31b1d95a6c19f5b615a3286069f181a415f872e8\n- fix: Change default verbose value of verbose in image chat format handlers to True to match Llama by @abetlen in 4f01c452b6c738dc56eacac3758119b12c57ea94\n- fix: Suppress all logs when verbose=False, use hardcoded fileno's to work in colab notebooks by @abetlen in f116175a5a7c84569c88cad231855c1e6e59ff6e\n- fix: UTF-8 handling with grammars by @jsoma in #1415\n\n## [0.2.68]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@77e15bec6217a39be59b9cc83d6b9afb6b0d8167\n- feat: Add option to enable flash_attn to Lllama params and ModelSettings by @abetlen in 22d77eefd2edaf0148f53374d0cac74d0e25d06e\n- fix(ci): Fix build-and-release.yaml by @Smartappli in #1413\n\n## [0.2.67]\n\n- fix: Ensure image renders before text in chat formats regardless of message content order by @abetlen in 3489ef09d3775f4a87fb7114f619e8ba9cb6b656\n- fix(ci): Fix bug in use of upload-artifact failing to merge multiple artifacts into a single release by @abetlen in d03f15bb73a1d520970357b702a9e7d4cc2a7a62\n\n## [0.2.66]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@8843a98c2ba97a25e93319a104f9ddfaf83ce4c4\n- feat: Generic Chat Formats, Tool Calling, and Huggingface Pull Support for Multimodal Models (Obsidian, LLaVA1.6, Moondream) by @abetlen in #1147\n- ci(fix): Workflow actions updates and fix arm64 wheels not included in release by @Smartappli in #1392\n- ci: Add support for pre-built cuda 12.4.1 wheels by @Smartappli in #1388\n- feat: Add support for str type kv_overrides by @abetlen in a411612b385cef100d76145da1fbd02a7b7cc894\n- fix: Functionary bug fixes by @jeffrey-fong in #1385\n- examples: fix quantize example by @iyubondyrev in #1387\n- ci: Update dependabot.yml by @Smartappli in #1391\n\n## [0.2.65]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@46e12c4692a37bdd31a0432fc5153d7d22bc7f72\n- feat: Allow for possibly non-pooled embeddings by @iamlemec in #1380\n\n## [0.2.64]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@4e96a812b3ce7322a29a3008db2ed73d9087b176\n- feat: Add `llama-3` chat format by @andreabak in #1371\n- feat: Use new llama_token_is_eog in create_completions by @abetlen in d40a250ef3cfaa8224d12c83776a2f1de96ae3d1\n- feat(server): Provide ability to dynamically allocate all threads if desired using -1 by @sean-bailey in #1364\n- ci: Build arm64 wheels by @gaby in 611781f5319719a3d05fefccbbf0cc321742a026\n- fix: Update scikit-build-core build dependency avoid bug in 0.9.1 by @evelkey in #1370\n\n## [0.2.63]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@0e4802b2ecbaab04b4f829fde4a3096ca19c84b5\n- feat: Add stopping_criteria to ChatFormatter, allow stopping on arbitrary token ids, fixes llama3 instruct by @abetlen in cc81afebf04d26ca1ac3cf72f23f18da6ab58588\n\n## [0.2.62]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@3b8f1ec4b18770531d0b1d792f3edf08254e4f0c\n- feat: update grammar schema converter to match llama.cpp by @themrzmaster in #1353\n- feat: add disable_ping_events flag by @khimaros in #1257\n- feat: Make saved state more compact on-disk by @tc-wolf in #1296\n- feat: Use all available CPUs for batch processing by @ddh0 in #1345\n\n## [0.2.61]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@ba5e134e073ec6837078c874aba44a702944a676\n- fix: pass correct type to chat handlers for chat completion logprobs by @abetlen in bb65b4d76411112c6fb0bf759efd746f99ef3c6b\n- feat: Add support for yaml based server configs by @abetlen in 060bfa64d529ade2af9b1f4e207a3937bbc4138f\n- feat: Add typechecking for ctypes structure attributes by @abetlen in 1347e1d050fc5a9a32ffe0bb3e22858da28003bd\n\n## [0.2.60]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@75cd4c77292034ecec587ecb401366f57338f7c0\n- fix: Always embed metal library by @abetlen in b3bfea6dbfb6ed9ce18f9a2723e0a9e4bd1da7ad\n- fix: missing logprobs in response, incorrect response type for functionary by @abetlen in 1ae3abbcc3af7f4a25a3ffc40b246f18039565e8\n- fix(docs): incorrect tool_choice example by @CISC in #1330\n\n## [0.2.59]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@ba0c7c70ab5b15f1f2be7fb0dfbe0366dda30d6c\n- feat: Binary wheels for CPU, CUDA (12.1 - 12.3), Metal by @abetlen, @jllllll, and @oobabooga in #1247\n- fix: segfault when logits_all=False by @abetlen in 8649d7671bd1a7c0d9cc6a5ad91c6ca286512ab3\n- fix: last tokens passing to sample_repetition_penalties function by @ymikhailov in #1295\n\n## [0.2.58]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@ba0c7c70ab5b15f1f2be7fb0dfbe0366dda30d6c\n- feat: add support for KV cache quantization options by @Limour-dev in #1307\n- feat: Add logprobs support to chat completions by @windspirit95 in #1311\n- fix: set LLAMA_METAL_EMBED_LIBRARY=on on MacOS arm64 by @bretello in #1289\n- feat: Add tools/functions variables to Jinja2ChatFormatter, add function response formatting for all simple chat formats by @CISC in #1273\n- fix: Changed local API doc references to hosted by by @lawfordp2017 in #1317\n\n## [0.2.57]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@ac9ee6a4ad740bc1ee484ede43e9f92b5af244c1\n- fix: set default embedding pooling type to unspecified by @abetlen in 4084aabe867b8ec2aba1b22659e59c9318b0d1f3\n- fix: Fix and optimize functionary chat handler by @jeffrey-fong in #1282\n- fix: json mode for basic chat formats by @abetlen in 20e6815252d0efd9f015f7adbf108faaf36e3f3c\n\n## [0.2.56]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@c2101a2e909ac7c08976d414e64e96c90ee5fa9e\n- feat(server): Add endpoints for tokenize, detokenize and count tokens by @felipelo in #1136\n- feat: Switch embed to llama_get_embeddings_seq by @iamlemec in #1263\n- fix: Fixed json strings grammar by blacklisting character control set by @ExtReMLapin in d02a9cf16ff88ad011e2eb1ce29f4d9400f13cd1\n- fix: Check for existence of clip model path by @kejcao in #1264\n\n## [0.2.55]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@9731134296af3a6839cd682e51d9c2109a871de5\n- docs: fix small typo in README: 'model know how' -> 'model knows how' by @boegel in #1244\n\n## [0.2.54]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@cb49e0f8c906e5da49e9f6d64a57742a9a241c6a\n- docs: fix typo in README.md embeddings example by @iamlemec in #1232\n\n## [0.2.53]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@cb49e0f8c906e5da49e9f6d64a57742a9a241c6a\n- fix: eos/bos_token set correctly for Jinja2ChatFormatter and automatic chat formatter by @CISC in #1230\n\n## [0.2.52]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@a33e6a0d2a66104ea9a906bdbf8a94d050189d91\n- fix: Llava15ChatHandler (this function takes at least 4 arguments) by @abetlen in 8383a9e5620f5df5a88f62da16813eac200dd706\n\n## [0.2.51]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@c39373398803c669056304090050fe3f44b41bf9\n- fix: Restore type hints for low-level api by @abetlen in 19234aa0dbd0c3c87656e65dd2b064665371925b\n\n## [0.2.50]\n\n- docs: Update Functionary OpenAI Server Readme by @jeffrey-fong in #1193\n- fix: LlamaHFTokenizer now receives pre_tokens by @abetlen in 47bad30dd716443652275099fa3851811168ff4a\n\n## [0.2.49]\n\n- fix: module 'llama_cpp.llama_cpp' has no attribute 'c_uint8' in Llama.save_state by @abetlen in db776a885cd4c20811f22f8bd1a27ecc71dba927\n- feat: Auto detect Mixtral's slightly different format by @lukestanley in #1214\n\n## [0.2.48]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@15499eb94227401bdc8875da6eb85c15d37068f7\n- feat: Add Google's Gemma formatting via chat_format=\"gemma\" by @alvarobartt in #1210\n- feat: support minItems/maxItems in JSON grammar converter by @nopperl in 3921e10770996d95a9eb22c8248bacef39f69365\n- fix: Update from_pretrained defaults to match hf_hub_download and pull to local cache folder by @abetlen in e6d6260a91b7831733f7d1f73c7af46a3e8185ed\n- fix: Raise exceptions when llama model or context fails to load by @abetlen in dd22010e85265ae840c76ec835d67a29ed852722\n- docs: Update README.md to fix pip install llama cpp server by @audip in #1187\n\n## [0.2.47]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@973053d8b0d04809836b3339a50f68d9c842de90\n\n## [0.2.46]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@ba2135ccae7462470b3865c6e41d2e1d734eac05\n- feat: Pull models directly from huggingface by @abetlen in #1206\n- feat(low-level-api): Improve API static type-safety and performance. Low level api functions are positional args only now. by @abetlen in #1205\n\n## [0.2.45]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@89febfed9322c8849520dc63c93ee4f5fd72556e\n\n## [0.2.44]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@4524290e87b8e107cc2b56e1251751546f4b9051\n- fix: create_embedding broken response for input type str by @abetlen in 0ce66bc080fe537590b05b24bf442480bf2dd045\n- fix: Use '\\n' seperator for EventSourceResponse by @khimaros in #1188\n- fix: Incorporate embedding pooling layer fixes by @iamlemec in #1194\n\n## [0.2.43]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@8084d554406b767d36b3250b3b787462d5dd626f\n- feat: Support batch embeddings by @iamlemec in #1186\n- fix: submodule kompute is not included in sdist by @abetlen in 7dbbfdecadebe7750be650d9409959640ff9a460\n- fix: fix: Update openbuddy prompt format by @abetlen in 07a783779a62a4aac0b11161c7e0eb983ff215f8\n\n## [0.2.42]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@ea9c8e11436ad50719987fa23a289c74b7b40d40\n- fix: sample idx off-by-one error for logit_processors by @lapp0 in #1179\n- fix: chat formatting bugs in `chatml-function-calling` by @abetlen in 4b0e3320bd8c2c209e29978d0b21e2e471cc9ee3 and 68fb71b6a26a1e57331868f959b47ab4b87851e1\n\n## [0.2.41]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@895407f31b358e3d9335e847d13f033491ec8a5b\n- fix: Don't change order of json schema object properties in generated grammar unless prop_order is passed by @abetlen in d1822fed6b706f38bd1ff0de4dec5baaa3cf84fa\n\n## [0.2.40]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@3bdc4cd0f595a6096cca4a64aa75ffa8a3503465\n- feat: Generic chatml Function Calling using chat_format=\"chatml-function-calling\"` by @abetlen in #957\n- fix: Circular dependancy preventing early Llama object free by @notwa in #1176\n- docs: Set the correct command for compiling with syscl support by @akarshanbiswas in #1172\n- feat: use gpu backend for clip if available by @iamlemec in #1175\n\n## [0.2.39]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@b08f22c882a1443e6b97081f3ce718a4d1a741f8\n- fix: Fix destructor logging bugs by using llama_log_callback to avoid suppress_stdout_stderr by @abetlen in 59760c85eddc72dfcc1839f43760ef72c23d6874\n\n## [0.2.38]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@1cfb5372cf5707c8ec6dde7c874f4a44a6c4c915\n- feat: Add speculative decoding by @abetlen in #1120\n- fix: Pass raise_exception and add_generation_prompt to jinja2 chat template by @abetlen in 078cca0361bf5a94d2cf52ed04980d20e32d6f95\n\n## [0.2.37]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@fea4fd4ba7f6b754ac795387b275e1a014a77bde\n- feat: Automatically set chat format from gguf by @abetlen in #1110\n\n## [0.2.36]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@2aed77eb06a329f0d82bb1c467f4244904d4073f\n- feat: Add mistral instruct chat format as \"mistral-instruct\" by @Rafaelblsilva in #799\n\n## [0.2.35]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@d2f650cb5b04ee2726663e79b47da5efe196ce00\n\n## [0.2.34]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@6db2b41a76ee78d5efdd5c3cddd5d7ad3f646855\n- feat: Add json schema mode by @abetlen in #1122\n\n## [0.2.33]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@faa3526a1eba458120987ed8269e5616385a76f4\n- feat(server): include llama-cpp-python version in openapi spec by @abetlen in cde7514c3d28e6d52f272614e9957208c344dde5\n- fix: use both eos and bos tokens as stop sequences for hf-tokenizer-config chat format. by @abetlen in 5b982d0f8c6f35242c8862ffdce00e17cea0b44f\n- fix: GGUF metadata KV overrides, re #1011 by @phiharri in #1116\n- fix: llama_log_set should be able to accept null pointer by @abetlen in c970d41a85381fd55235136f123422df0bf0c7e7\n\n## [0.2.32]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@504dc37be8446fb09b1ede70300250ad41be32a2\n- fix: from_json_schema oneof/anyof bug by @jndiogo in d3f5528ca8bcb9d69d4f27e21631e911f1fb9bfe\n- fix: pass chat handler not chat formatter for huggingface autotokenizer and tokenizer_config formats by @abetlen in 24f39454e91cf5dddbc4b6041aead4accc7c7a2d\n- feat: Add add_generation_prompt option for jinja2chatformatter by @abetlen in 7f3209b1eb4ad3260ba063801fab80a8c25a2f4c\n- feat: Add Jinja2ChatFormatter by @abetlen in be09318c26add8674ce494ae7cc480cce72a4146\n- feat: Expose gguf model metadata in metadata property by @abetlen in 5a34c57e5479e50c99aba9b38218cc48e6560b81\n\n## [0.2.31]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@a5cacb22b2114fd9adf61c00cbb237384d86bced\n- fix: Mirostat sampling now passes correct type to ctypes and tracks state during generation by @abetlen in 3babe3512cb95743108f2b595210c38ed6f1b904\n- fix: Python3.8 support in server by @abetlen in 141293a75b564a8699e0acba1da24d9aa1cf0ab1\n\n## [0.2.30]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@57e2a7a52a819883f40dada8a2edc24ecf48186b\n- feat(server): Add ability to load chat format from huggingface autotokenizer or tokenizer_config.json files by @abetlen in b8fc1c7d83ad4a9207c707ba1d954fe580286a01\n- feat: Integration of Jinja2 Templating for chat formats by @teleprint-me in #875\n- fix: Offload KQV by default by @abetlen in 48c3b77e6f558a9899de0e1155c7dc0c7958d8e8\n- fix: Support Accept text/event-stream in chat and completion endpoints, resolves #1083 by @aniljava in #1088\n- fix(cli): allow passing n_ctx=0 to openAI API server args to use model n_ctx_train field per #1015 by @K-Mistele in #1093\n\n## [0.2.29]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@4483396751c79dea540808b9cb9238245d06da2b\n- feat: Add split_mode option by @abetlen in 84615adbc6855c8384807c42f0130f9a1763f99d\n- feat: Implement GGUF metadata KV overrides by @phiharri in #1011\n- fix: Avoid \"LookupError: unknown encoding: ascii\" when open() called in a destructor by @yieldthought in #1012\n- fix: Fix low_level_api_chat_cpp example to match current API by @aniljava in #1086\n- fix: Fix Pydantic model parsing by @DeNeutoy in #1087\n\n## [0.2.28]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@6efb8eb30e7025b168f3fda3ff83b9b386428ad6\n- feat: Add ability to pass in penalize_nl param by @shankinson in #1068\n- fix: print_grammar to stderr by @turian in #1052\n\n## [0.2.27]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@b3a7c20b5c035250257d2b62851c379b159c899a\n- feat: Add `saiga` chat format by @femoiseev in #1050\n- feat: Added `chatglm3` chat format by @xaviviro in #1059\n- fix: Correct typo in README.md by @qeleb in (#1058)\n\n## [0.2.26]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@f6793491b5af6da75edad34d6f503ef86d31b09f\n\n## [0.2.25]\n\n- feat(server): Multi model support by @D4ve-R in #931\n- feat(server): Support none defaulting to infinity for completions by @swg in #111\n- feat(server): Implement openai api compatible authentication by @docmeth2 in #1010\n- fix: text_offset of multi-token characters by @twaka in #1037\n- fix: ctypes bindings for kv override by @phiharri in #1011\n- fix: ctypes definitions of llama_kv_cache_view_update and llama_kv_cache_view_free. by @e-c-d in #1028\n\n## [0.2.24]\n\n- feat: Update llama.cpp to ggerganov/llama.cpp@0e18b2e7d0b5c0a509ea40098def234b8d4a938a\n- feat: Add offload_kqv option to llama and server by @abetlen in 095c65000642a3cf73055d7428232fb18b73c6f3\n- feat: n_ctx=0 now uses the n_ctx_train of the model by @DanieleMorotti in #1015\n- feat: logits_to_logprobs supports both 2-D and 3-D logits arrays by @kddubey in #1002\n- fix: Remove f16_kv, add offload_kqv fields in low level and llama apis by @brandonrobertz in #1019\n- perf: Don't convert logprobs arrays to lists by @kddubey in #1021\n- docs: Fix README.md functionary demo typo by @evelynmitchell in #996\n- examples: Update low_level_api_llama_cpp.py to match current API by @jsoma in #1023\n\n## [0.2.23]\n\n- Update llama.cpp to ggerganov/llama.cpp@948ff137ec37f1ec74c02905917fa0afc9b97514\n- Add qwen chat format by @yhfgyyf in #1005\n- Add support for running the server with SSL by @rgerganov in #994\n- Replace logits_to_logprobs implementation with numpy equivalent to llama.cpp by @player1537 in #991\n- Fix UnsupportedOperation: fileno in suppress_stdout_stderr by @zocainViken in #961\n- Add Pygmalion chat format by @chiensen in #986\n- README.md multimodal params fix by @zocainViken in #967\n- Fix minor typo in README by @aniketmaurya in #958\n\n## [0.2.22]\n\n- Update llama.cpp to ggerganov/llama.cpp@8a7b2fa528f130631a5f43648481596ab320ed5a\n- Fix conflict with transformers library by kddubey in #952\n\n## [0.2.21]\n\n- Update llama.cpp to ggerganov/llama.cpp@64e64aa2557d97490b2fe1262b313e2f4a1607e3\n- Make building llava optional by setting `CMAKE_ARGS=\"-DLLAVA_BUILD=OFF\"` and using `LLAVA_CPP_LIB` to specify alternative path to shared library by @abetlen in e3941d9c674dbd9891dc3ceda390daeb21f05fd1\n\n## [0.2.20]\n\n- Update llama.cpp to ggerganov/llama.cpp@b38a16dfcff88d547f78f52d1bea31b84a05aff7\n- Add `zephyr` chat format by @fakerybakery in #938\n- Add `baichuan` chat format by @caiyesd in #938\n- Add `baichuan-2` chat format by @caiyesd in #936\n- Improve documentation for server chat formats by @jooray in #934\n- Fix typo in README by @antonvice in 940\n- Fix typo in the Open Orca chat format by @gardner in #947\n\n## [0.2.19]\n\n- Update llama.cpp to ggerganov/llama.cpp@0b871f1a04ef60e114bbe43004fd9c21114e802d\n- Fix #569: stop parameter in chat completion api should accept str by @abetlen in 128dc4731fa846ead7e684a137ca57d8931b8899\n- Document server host and port parameters by @jamesbraza in #768\n- Do not set grammar to None when initializing LlamaGrammar by @mthuurne in #834\n- Add mistrallite, intel, and openchat formats by @fakerybakery in #927\n- Add support for min_p parameter by @tk-master in #921\n- Fix #929: tokenizer adding leading space when generating from empty prompt by @abetlen in a34d48014192771d2e308a76c22f33bc0318d983\n- Fix low level api example by @zocainViken in #925\n- Fix missing package in openblas docker image by @ZisisTsatsas in #920\n\n## [0.2.18]\n\n- Update llama.cpp to ggerganov/llama.cpp@6bb4908a17150b49373b5f977685b2e180a04f6f\n\n## [0.2.17]\n\n- Update llama.cpp to ggerganov/llama.cpp@df9d1293defe783f42bc83af732d3c670552c541\n- Hotfix: Set `CUDA_ARCHITECTURES=OFF` for `llava_shared` target on Windows by @abetlen in 4388f3341413110217b98c4f097ac5c590bdf40b\n\n## [0.2.16]\n\n- Update llama.cpp to ggerganov/llama.cp@a75fa576abba9d37f463580c379e4bbf1e1ad03c\n- Add `set_seed` to `Llama` class by @abetlen in fd41ed3a908761d286102a019a34c2938a15118d\n- Fix server doc arguments by @kjunggithub in #892\n- Fix response_format handler in llava chat handler by @abetlen in b62c44983921197ed10a7d29dc4ba920e9979380\n- Fix default max_tokens, chat completion is now unlimited (to context length) and completion is 16 tokens to match OpenAI defaults by @abetlen in e7962d2c733cbbeec5a37392c81f64185a9a39e8\n- Fix json_schema_to_gbnf helper so that it takes a json schema string as input instead by @abetlen in faeae181b1e868643c0dc28fcf039f077baf0829\n- Add support for $ref and $def in json_schema_to_gbnf to handle more complex function schemas by @abetlen in 770df344369c0630df1be14be9f9e301e7c56d24\n- Update functionary chat handler for new OpenAI api by abetlen in 1b376c62b775b401653facf25a519d116aafe99a\n- Fix add default stop sequence to chatml chat format by @abetlen in b84d76a844149216d511cfd8cdb9827148a1853c\n- Fix sampling bug when logits_all=False by @abetlen in 6f0b0b1b840af846938ed74d0e8170a91c40e617\n\n## [0.2.15]\n\n- Update llama.cpp to ggerganov/llama.cpp@0a7c980b6f94a049cb804573df2d8092a34df8e4\n- Add support for Llava1.5 multimodal models by @damian0815 and @abetlen in #821\n- Update OpenAI API compatibility to match dev day update by @abetlen in #821\n- Add seed parameter to completion and chat_completion functions of Llama class by @abetlen in 86aeb9f3a14808575d2bb0076e6acb4a30907e6a\n- Add JSON mode support to constrain chat completion to JSON objects by @abetlen in b30b9c338bf9af316d497ea501d39f5c246900db\n\n## [0.2.14]\n\n- Update llama.cpp to ggerganov/llama.cpp@f0b30ef7dc1360922ccbea0a8cd3918ecf15eaa7\n- Add support for Huggingface Autotokenizer Chat Formats by @bioshazard and @abetlen in #790 and bbffdaebaa7bb04b543dbf683a07276087251f86\n- Fix llama-2 chat format by @earonesty in #869\n- Add support for functionary chat format by @abetlen in #784\n- Migrate inference from deprecated `llama_eval`API to `llama_batch` and `llama_decode` by @abetlen in #795\n\n## [0.2.13]\n\n- Update llama.cpp to ggerganov/llama.cpp@51b2fc11f7f605fff49725a4540e9a6ef7b51b70\n- Fix name 'open' is not defined exception when deleting model by @abetlen in 011b95d7f34cbfc528af75a892757bd9a20838ab\n- Fix tokenization of special characters by @antoine-lizee in #850\n\n## [0.2.12]\n\n- Update llama.cpp to ggerganov/llama.cpp@50337961a678fce4081554b24e56e86b67660163\n- Fix missing `n_seq_id` in `llama_batch` by @NickAlgra in #842\n- Fix for shared libraries on Windows that start with `lib` prefix by @sujeendran in #848\n- Fix exception raised in `__del__` when freeing models by @cebtenzzre in #846\n- Performance improvement for logit bias by @zolastro in #851\n- Fix suffix check arbitrary code execution bug by @mtasic85 in #854\n- Fix typo in `function_call` parameter in `llama_types.py` by @akatora28 in #849\n- Fix streaming not returning `finish_reason` by @gmcgoldr in #798\n- Fix `n_gpu_layers` check to allow values less than 1 for server by @hxy9243 in #826\n- Supppress stdout and stderr when freeing model by @paschembri in #803\n- Fix `llama2` chat format by @delock in #808\n- Add validation for tensor_split size by @eric1932 #820\n- Print stack trace on server error by @abetlen in d6a130a052db3a50975a719088a9226abfebb266\n- Update docs for gguf by @johnccshen in #783\n- Add `chatml` chat format by @abetlen in 305482bd4156c70802fc054044119054806f4126\n\n## [0.2.11]\n\n- Fix bug in `llama_model_params` object has no attribute `logits_all` by @abetlen in d696251fbe40015e8616ea7a7d7ad5257fd1b896\n\n## [0.2.10]\n\n- Fix bug 'llama_model_params' object has no attribute 'embedding' by @abetlen in 42bb721d64d744242f9f980f2b89d5a6e335b5e4\n\n## [0.2.9]\n\n- Fix critical bug in pip installation of v0.2.8 due to `.git` directory in ac853e01e1a217a578080a4e1b851d2d08450adf\n\n## [0.2.8]\n\n- Update llama.cpp to ggerganov/llama.cpp@40e07a60f9ce06e79f3ccd4c903eba300fb31b5e\n- Add configurable chat formats by @abetlen in #711\n- Fix rope scaling bug by @Josh-XT in #767\n- Fix missing numa parameter in server by @abetlen in d9bce17794d0dd6f7962d10aad768fedecf3ab89\n\n## [0.2.7]\n\n- Update llama.cpp to ggerganov/llama.cpp@a98b1633d5a94d0aa84c7c16e1f8df5ac21fc850\n- Install required runtime dlls to package directory on windows by @abetlen in 8d75016549e2ff62a511b1119d966ffc0df5c77b\n- Add openai-processing-ms to server response header by @Tradunsky in #748\n- Bump minimum version of scikit-build-core to 0.5.1 to fix msvc cmake issue by @abetlen in 1ed0f3ebe16993a0f961155aa4b2c85f1c68f668\n- Update `llama_types.py` to better match the openai api, old names are aliased to new ones by @abetlen in dbca136feaaf7f8b1182c4c3c90c32918b1d0bb3\n\n## [0.2.6]\n\n- Update llama.cpp to 80291a1d02a07f7f66666fb576c5b1e75aa48b46\n\n## [0.2.5]\n\n- Fix docker images missing starlette-context dependency by @abetlen in 22917989003c5e67623d54ab45affa1e0e475410\n- Fix loading dll in Windows Isolation Containers by @abetlen in 847466562573191efa655753d9252f308c4fbdb0\n- Fix build issue on m1 macs by @abetlen in dbd3a6d1ed8416a8fd800127251e730153afa305\n- Update docs to gguf and add hw acceleration docs for server by @jasonacox in #688\n\n## [0.2.4]\n\n- Add NUMA support. **NOTE** low level api users must call llama_backend_init at the start of their programs by abetlen in f4090a0bb2a2a25acfe28d31c82cc1aa273bedee\n- Fix tensor_split server cli argument by @abetlen in c4c440ba2dc86d9de728a751311fdd1c8e3756fa\n- Made all `Llama` init parameters into keyword-only parameters by @abetlen in c8f9b8a734b5b040379bbd93995ba177affab1fe\n- Added server params for `low_vram`, `main_gpu`, `lora_base`, and `lora_path` by @abetlen in 2920c4bf7ee1412d6bba7846e0e1b7ef6d34043b\n- Removed server params for `rms_norm_eps` and `n_gqa` by @abetlen in 2920c4bf7ee1412d6bba7846e0e1b7ef6d34043b\n- Fix boolean cli options by @abetlen in c999325e8e4507f6c6249dd2fb8de7f8bf57f71e and 0449d29b9f940e437231a07b9d56550226558bac\n- Silence Pydantic Settings warnings about `model_alias` setting by @earonesty in #705\n\n## [0.2.3]\n\n- Update llama.cpp to ggerganov/llama.cpp@71ca2fad7d6c0ef95ef9944fb3a1a843e481f314\n- Add X-Request-ID request header for mirroring custom IDs by @devrimcavusoglu in #703\n- Add pyproject extra for scikit-build-core to ensure compatible pathspec version by @abetlen in 6cfc54284b99ef1bff8193e2d5e483dbd89ada02\n- Fix issue with Literal and Optional cli arguments not working by @abetlen in #702\n\n## [0.2.2]\n\n- Fix bug in pip install of v0.2.1 due to scikit-build-core removing all `.metal` files in the source distribution (see #701)\n\n## [0.2.1]\n\n- Fix bug in pip install of v0.2.0 due to .git folder being included in the source distribution (see #701)\n\n## [0.2.0]\n\n- Migrated to scikit-build-core build system by @abetlen in #499\n- Use `numpy` views for `LogitsProcessor` and `StoppingCriteria` instead of python lists by @abetlen in #499\n- Drop support for end-of-life Python3.7 by @abetlen in #499\n- Convert low level `llama.cpp` constants to use basic python types instead of `ctypes` types by @abetlen in #499\n\n## [0.1.85]\n\n- Add `llama_cpp.__version__` attribute by @janvdp in #684\n- Fix low level api examples by @jbochi in #680\n\n## [0.1.84]\n\n- Update llama.cpp\n\n## [0.1.83]\n\n- Update llama.cpp\n\n## [0.1.82]\n\n- Update llama.cpp\n\n## [0.1.81]\n\n- Update llama.cpp\n\n## [0.1.80]\n\n- Update llama.cpp\n\n## [0.1.79]\n\n- GGUF Support (breaking change requiring new model format)\n\n## [0.1.78]\n\n- Grammar based sampling via LlamaGrammar which can be passed to completions\n- Make n_gpu_layers == -1 offload all layers\n\n## [0.1.77]\n\n- (llama.cpp) Update llama.cpp add support for LLaMa 2 70B\n- (server) Add temporary n_gqa and rms_norm_eps parameters required for LLaMa 2 70B\n\n## [0.1.76]\n\n- (llama.cpp) Update llama.cpp add support for LLaMa 2 70B\n\n## [0.1.75]\n\n- Update llama.cpp\n\n## [0.1.74]\n\n- (server) OpenAI style error responses\n\n## [0.1.73]\n\n- (server) Add rope parameters to server settings\n\n## [0.1.72]\n\n- (llama.cpp) Update llama.cpp added custom_rope for extended context lengths\n\n## [0.1.71]\n\n- (llama.cpp) Update llama.cpp\n\n- (server) Fix several pydantic v2 migration bugs\n\n## [0.1.70]\n\n- (Llama.create_completion) Revert change so that `max_tokens` is not truncated to `context_size` in `create_completion`\n- (server) Fixed changed settings field names from pydantic v2 migration\n\n## [0.1.69]\n\n- (server) Streaming requests can are now interrupted pre-maturely when a concurrent request is made. Can be controlled with the `interrupt_requests` setting.\n- (server) Moved to fastapi v0.100.0 and pydantic v2\n- (docker) Added a new \"simple\" image that builds llama.cpp from source when started.\n- (server) performance improvements by avoiding unnecessary memory allocations during sampling\n\n## [0.1.68]\n\n- (llama.cpp) Update llama.cpp\n\n## [0.1.67]\n\n- Fix performance bug in Llama model by pre-allocating memory tokens and logits.\n- Fix bug in Llama model where the model was not free'd after use.\n\n## [0.1.66]\n\n- (llama.cpp) New model API\n\n- Performance issue during eval caused by looped np.concatenate call\n- State pickling issue when saving cache to disk\n\n## [0.1.65]\n\n- (llama.cpp) Fix struct misalignment bug\n\n## [0.1.64]\n\n- (llama.cpp) Update llama.cpp\n- Fix docs for seed. Set -1 for random.\n\n## [0.1.63]\n\n- (llama.cpp) Add full gpu utilisation in CUDA\n- (llama.cpp) Add get_vocab\n- (llama.cpp) Add low_vram parameter\n- (server) Add logit_bias parameter\n\n## [0.1.62]\n\n- Metal support working\n- Cache re-enabled\n\n## [0.1.61]\n\n- Fix broken pip installation\n\n## [0.1.60]\n\nNOTE: This release was deleted due to a bug with the packaging system that caused pip installations to fail.\n\n- Truncate max_tokens in create_completion so requested tokens doesn't exceed context size.\n- Temporarily disable cache for completion requests\n\n## [v0.1.59]\n\n- (llama.cpp) k-quants support\n- (server) mirostat sampling parameters to server\n- Support both `.so` and `.dylib` for `libllama` on MacOS\n\n## [v0.1.58]\n\n- (llama.cpp) Metal Silicon support\n\n## [v0.1.57]\n\n- (llama.cpp) OpenLlama 3B support\n\n## [v0.1.56]\n\n- (misc) Added first version of the changelog\n- (server) Use async routes\n- (python-api) Use numpy for internal buffers to reduce memory usage and improve performance.\n- (python-api) Performance bug in stop sequence check slowing down streaming.\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 6.580078125,
          "content": "cmake_minimum_required(VERSION 3.21)\n\nproject(llama_cpp)\n\noption(LLAMA_BUILD \"Build llama.cpp shared library and install alongside python package\" ON)\noption(LLAVA_BUILD \"Build llava shared library and install alongside python package\" ON)\n\nfunction(llama_cpp_python_install_target target)\n    if(NOT TARGET ${target})\n        return()\n    endif()\n\n    install(\n        TARGETS ${target}\n        LIBRARY DESTINATION ${CMAKE_CURRENT_SOURCE_DIR}/llama_cpp/lib\n        RUNTIME DESTINATION ${CMAKE_CURRENT_SOURCE_DIR}/llama_cpp/lib\n        ARCHIVE DESTINATION ${CMAKE_CURRENT_SOURCE_DIR}/llama_cpp/lib\n        FRAMEWORK DESTINATION ${CMAKE_CURRENT_SOURCE_DIR}/llama_cpp/lib\n        RESOURCE DESTINATION ${CMAKE_CURRENT_SOURCE_DIR}/llama_cpp/lib\n    )\n    install(\n        TARGETS ${target}\n        LIBRARY DESTINATION ${SKBUILD_PLATLIB_DIR}/llama_cpp/lib\n        RUNTIME DESTINATION ${SKBUILD_PLATLIB_DIR}/llama_cpp/lib\n        ARCHIVE DESTINATION ${SKBUILD_PLATLIB_DIR}/llama_cpp/lib\n        FRAMEWORK DESTINATION ${SKBUILD_PLATLIB_DIR}/llama_cpp/lib\n        RESOURCE DESTINATION ${SKBUILD_PLATLIB_DIR}/llama_cpp/lib\n    )\n    set_target_properties(${target} PROPERTIES\n        INSTALL_RPATH \"$ORIGIN\"\n        BUILD_WITH_INSTALL_RPATH TRUE\n    )\n    if(UNIX)\n        if(APPLE)\n            set_target_properties(${target} PROPERTIES\n                INSTALL_RPATH \"@loader_path\"\n                BUILD_WITH_INSTALL_RPATH TRUE\n            )\n        else()\n            set_target_properties(${target} PROPERTIES\n                INSTALL_RPATH \"$ORIGIN\"\n                BUILD_WITH_INSTALL_RPATH TRUE\n            )\n        endif()\n    endif()\nendfunction()\n\nif (LLAMA_BUILD)\n    set(BUILD_SHARED_LIBS \"On\")\n\n    set(CMAKE_SKIP_BUILD_RPATH FALSE)\n\n    # When building, don't use the install RPATH already\n    # (but later on when installing)\n    set(CMAKE_BUILD_WITH_INSTALL_RPATH FALSE)\n \n    # Add the automatically determined parts of the RPATH\n    # which point to directories outside the build tree to the install RPATH\n    set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)\n    set(CMAKE_SKIP_RPATH FALSE)\n\n    # Enable building of the common library\n    set(LLAMA_BUILD_COMMON ON CACHE BOOL \"Build llama.cpp common library\" FORCE)\n\n    # Architecture detection and settings for Apple platforms\n    if (APPLE)\n        # Get the target architecture\n        execute_process(\n            COMMAND uname -m\n            OUTPUT_VARIABLE HOST_ARCH\n            OUTPUT_STRIP_TRAILING_WHITESPACE\n        )\n\n        # If CMAKE_OSX_ARCHITECTURES is not set, use the host architecture\n        if(NOT CMAKE_OSX_ARCHITECTURES)\n            set(CMAKE_OSX_ARCHITECTURES ${HOST_ARCH} CACHE STRING \"Build architecture for macOS\" FORCE)\n        endif()\n\n        message(STATUS \"Host architecture: ${HOST_ARCH}\")\n        message(STATUS \"Target architecture: ${CMAKE_OSX_ARCHITECTURES}\")\n\n        # Configure based on target architecture\n        if(CMAKE_OSX_ARCHITECTURES STREQUAL \"x86_64\")\n            # Intel Mac settings\n            set(GGML_AVX \"OFF\" CACHE BOOL \"ggml: enable AVX\" FORCE)\n            set(GGML_AVX2 \"OFF\" CACHE BOOL \"ggml: enable AVX2\" FORCE)\n            set(GGML_FMA \"OFF\" CACHE BOOL \"ggml: enable FMA\" FORCE)\n            set(GGML_F16C \"OFF\" CACHE BOOL \"ggml: enable F16C\" FORCE)\n        endif()\n\n        # Metal settings (enable for both architectures)\n        set(GGML_METAL \"ON\" CACHE BOOL \"ggml: enable Metal\" FORCE)\n        set(GGML_METAL_EMBED_LIBRARY \"ON\" CACHE BOOL \"ggml: embed metal library\" FORCE)\n    endif()\n\n    add_subdirectory(vendor/llama.cpp)\n    llama_cpp_python_install_target(llama)\n    llama_cpp_python_install_target(ggml)\n\n    llama_cpp_python_install_target(ggml-base)\n\n    llama_cpp_python_install_target(ggml-amx)\n    llama_cpp_python_install_target(ggml-blas)\n    llama_cpp_python_install_target(ggml-can)\n    llama_cpp_python_install_target(ggml-cpu)\n    llama_cpp_python_install_target(ggml-cuda)\n    llama_cpp_python_install_target(ggml-hip)\n    llama_cpp_python_install_target(ggml-kompute)\n    llama_cpp_python_install_target(ggml-metal)\n    llama_cpp_python_install_target(ggml-musa)\n    llama_cpp_python_install_target(ggml-rpc)\n    llama_cpp_python_install_target(ggml-sycl)\n    llama_cpp_python_install_target(ggml-vulkan)\n\n    # Workaround for Windows + CUDA https://github.com/abetlen/llama-cpp-python/issues/563\n    if (WIN32)\n        install(\n            FILES $<TARGET_RUNTIME_DLLS:llama>\n            DESTINATION ${CMAKE_CURRENT_SOURCE_DIR}/llama_cpp/lib\n        )\n        install(\n            FILES $<TARGET_RUNTIME_DLLS:llama>\n            DESTINATION ${SKBUILD_PLATLIB_DIR}/llama_cpp/lib\n        )\n        install(\n            FILES $<TARGET_RUNTIME_DLLS:ggml>\n            DESTINATION ${CMAKE_CURRENT_SOURCE_DIR}/llama_cpp/lib\n        )\n        install(\n            FILES $<TARGET_RUNTIME_DLLS:ggml>\n            DESTINATION ${SKBUILD_PLATLIB_DIR}/llama_cpp/lib\n        )\n    endif()\n\n    if (LLAVA_BUILD)\n        if (LLAMA_CUBLAS OR LLAMA_CUDA)\n            add_compile_definitions(GGML_USE_CUBLAS)\n            add_compile_definitions(GGML_USE_CUDA)\n        endif()\n\n        if (LLAMA_METAL)\n            add_compile_definitions(GGML_USE_METAL)\n        endif()\n\n        # Building llava\n        add_subdirectory(vendor/llama.cpp/examples/llava)\n        set_target_properties(llava_shared PROPERTIES OUTPUT_NAME \"llava\")\n\n        if (WIN32)\n            set_target_properties(llava_shared PROPERTIES CUDA_ARCHITECTURES OFF)\n        endif()\n        llama_cpp_python_install_target(llava_shared)\n        if (WIN32)\n            install(\n                FILES $<TARGET_RUNTIME_DLLS:llava_shared>\n                DESTINATION ${CMAKE_CURRENT_SOURCE_DIR}/llama_cpp/lib\n            )\n            install(\n                FILES $<TARGET_RUNTIME_DLLS:llava_shared>\n                DESTINATION ${SKBUILD_PLATLIB_DIR}/llama_cpp/lib\n            )\n        endif()\n\n        # Fix for llava build: Add include directory for llama.h\n        # Move these commands after the add_subdirectory call\n        target_include_directories(llava PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/vendor/llama.cpp/include)\n        target_include_directories(llava PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/vendor/llama.cpp/ggml/include)\n\n        if (BUILD_SHARED_LIBS)\n            target_include_directories(llava_shared PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/vendor/llama.cpp/include)\n            target_include_directories(llava_shared PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/vendor/llama.cpp/ggml/include)\n        endif()\n\n        target_include_directories(llama-llava-cli PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/vendor/llama.cpp/include)\n        target_include_directories(llama-minicpmv-cli PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/vendor/llama.cpp/include)\n    endif()\nendif()\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.0439453125,
          "content": "MIT License\n\nCopyright (c) 2023 Andrei Betlen\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 2.2998046875,
          "content": "update:\n\tpoetry install\n\tgit submodule update --init --recursive\n\nupdate.vendor:\n\tcd vendor/llama.cpp && git pull origin master\n\ndeps:\n\tpython3 -m pip install --upgrade pip\n\tpython3 -m pip install -e \".[all]\"\n\nbuild:\n\tpython3 -m pip install --verbose -e .\n\nbuild.debug:\n\tpython3 -m pip install \\\n\t\t--verbose \\\n\t\t--config-settings=cmake.verbose=true \\\n\t\t--config-settings=logging.level=INFO \\\n\t\t--config-settings=install.strip=false  \\\n\t\t--config-settings=cmake.args=\"-DCMAKE_BUILD_TYPE=Debug;-DCMAKE_C_FLAGS='-ggdb -O0';-DCMAKE_CXX_FLAGS='-ggdb -O0'\" \\\n\t\t--editable .\n\nbuild.debug.extra:\n\tpython3 -m pip install \\\n\t\t--verbose \\\n\t\t--config-settings=cmake.verbose=true \\\n\t\t--config-settings=logging.level=INFO \\\n\t\t--config-settings=install.strip=false  \\\n\t\t--config-settings=cmake.args=\"-DCMAKE_BUILD_TYPE=Debug;-DCMAKE_C_FLAGS='-fsanitize=address -ggdb -O0';-DCMAKE_CXX_FLAGS='-fsanitize=address -ggdb -O0'\" \\\n\t\t--editable .\n\nbuild.cuda:\n\tCMAKE_ARGS=\"-DGGML_CUDA=on\" python3 -m pip install --verbose -e .\n\nbuild.openblas:\n\tCMAKE_ARGS=\"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS\" python3 -m pip install --verbose -e .\n\nbuild.blis:\n\tCMAKE_ARGS=\"-DGGML_BLAS=on -DGGML_BLAS_VENDOR=FLAME\" python3 -m pip install --verbose -e .\n\nbuild.metal:\n\tCMAKE_ARGS=\"-DGGML_METAL=on\" python3 -m pip install --verbose -e .\n\nbuild.vulkan:\n\tCMAKE_ARGS=\"-DGGML_VULKAN=on\" python3 -m pip install --verbose -e .\n\nbuild.kompute:\n\tCMAKE_ARGS=\"-DGGML_KOMPUTE=on\" python3 -m pip install --verbose -e .\n\nbuild.sycl:\n\tCMAKE_ARGS=\"-DGGML_SYCL=on\" python3 -m pip install --verbose -e .\n\nbuild.rpc:\n\tCMAKE_ARGS=\"-DGGML_RPC=on\" python3 -m pip install --verbose -e .\n\nbuild.sdist:\n\tpython3 -m build --sdist --verbose\n\ndeploy.pypi:\n\tpython3 -m twine upload dist/*\n\ndeploy.gh-docs:\n\tmkdocs build\n\tmkdocs gh-deploy\n\ntest:\n\tpython3 -m pytest --full-trace -v\n\ndocker:\n\tdocker build -t llama-cpp-python:latest -f docker/simple/Dockerfile .\n\nrun-server:\n\tpython3 -m llama_cpp.server --model ${MODEL}\n\nclean:\n\t- cd vendor/llama.cpp && make clean\n\t- cd vendor/llama.cpp && rm libllama.so\n\t- rm -rf _skbuild\n\t- rm llama_cpp/lib/*.so\n\t- rm llama_cpp/lib/*.dylib\n\t- rm llama_cpp/lib/*.metal\n\t- rm llama_cpp/lib/*.dll\n\t- rm llama_cpp/lib/*.lib\n\n.PHONY: \\\n\tupdate \\\n\tupdate.vendor \\\n\tbuild \\\n\tbuild.cuda \\\n\tbuild.opencl \\\n\tbuild.openblas \\\n\tbuild.sdist \\\n\tdeploy.pypi \\\n\tdeploy.gh-docs \\\n\tdocker \\\n\tclean\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 30.50390625,
          "content": "<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/abetlen/llama-cpp-python/main/docs/icon.svg\" style=\"height: 5rem; width: 5rem\">\n</p>\n\n#  Python Bindings for [`llama.cpp`](https://github.com/ggerganov/llama.cpp)\n\n[![Documentation Status](https://readthedocs.org/projects/llama-cpp-python/badge/?version=latest)](https://llama-cpp-python.readthedocs.io/en/latest/?badge=latest)\n[![Tests](https://github.com/abetlen/llama-cpp-python/actions/workflows/test.yaml/badge.svg?branch=main)](https://github.com/abetlen/llama-cpp-python/actions/workflows/test.yaml)\n[![PyPI](https://img.shields.io/pypi/v/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)\n[![PyPI - License](https://img.shields.io/pypi/l/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-cpp-python)](https://pypi.org/project/llama-cpp-python/)\n[![Github All Releases](https://img.shields.io/github/downloads/abetlen/llama-cpp-python/total.svg?label=Github%20Downloads)]()\n\nSimple Python bindings for **@ggerganov's** [`llama.cpp`](https://github.com/ggerganov/llama.cpp) library.\nThis package provides:\n\n- Low-level access to C API via `ctypes` interface.\n- High-level Python API for text completion\n    - OpenAI-like API\n    - [LangChain compatibility](https://python.langchain.com/docs/integrations/llms/llamacpp)\n    - [LlamaIndex compatibility](https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp.html)\n- OpenAI compatible web server\n    - [Local Copilot replacement](https://llama-cpp-python.readthedocs.io/en/latest/server/#code-completion)\n    - [Function Calling support](https://llama-cpp-python.readthedocs.io/en/latest/server/#function-calling)\n    - [Vision API support](https://llama-cpp-python.readthedocs.io/en/latest/server/#multimodal-models)\n    - [Multiple Models](https://llama-cpp-python.readthedocs.io/en/latest/server/#configuration-and-multi-model-support)\n\nDocumentation is available at [https://llama-cpp-python.readthedocs.io/en/latest](https://llama-cpp-python.readthedocs.io/en/latest).\n\n## Installation\n\nRequirements:\n\n  - Python 3.8+\n  - C compiler\n      - Linux: gcc or clang\n      - Windows: Visual Studio or MinGW\n      - MacOS: Xcode\n\nTo install the package, run:\n\n```bash\npip install llama-cpp-python\n```\n\nThis will also build `llama.cpp` from source and install it alongside this python package.\n\nIf this fails, add `--verbose` to the `pip install` see the full cmake build log.\n\n**Pre-built Wheel (New)**\n\nIt is also possible to install a pre-built wheel with basic CPU support.\n\n```bash\npip install llama-cpp-python \\\n  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\n```\n\n### Installation Configuration\n\n`llama.cpp` supports a number of hardware acceleration backends to speed up inference as well as backend specific options. See the [llama.cpp README](https://github.com/ggerganov/llama.cpp#build) for a full list.\n\nAll `llama.cpp` cmake build options can be set via the `CMAKE_ARGS` environment variable or via the `--config-settings / -C` cli flag during installation.\n\n<details open>\n<summary>Environment Variables</summary>\n\n```bash\n# Linux and Mac\nCMAKE_ARGS=\"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS\" \\\n  pip install llama-cpp-python\n```\n\n```powershell\n# Windows\n$env:CMAKE_ARGS = \"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS\"\npip install llama-cpp-python\n```\n</details>\n\n<details>\n<summary>CLI / requirements.txt</summary>\n\nThey can also be set via `pip install -C / --config-settings` command and saved to a `requirements.txt` file:\n\n```bash\npip install --upgrade pip # ensure pip is up to date\npip install llama-cpp-python \\\n  -C cmake.args=\"-DGGML_BLAS=ON;-DGGML_BLAS_VENDOR=OpenBLAS\"\n```\n\n```txt\n# requirements.txt\n\nllama-cpp-python -C cmake.args=\"-DGGML_BLAS=ON;-DGGML_BLAS_VENDOR=OpenBLAS\"\n```\n\n</details>\n\n### Supported Backends\n\nBelow are some common backends, their build commands and any additional environment variables required.\n\n<details open>\n<summary>OpenBLAS (CPU)</summary>\n\nTo install with OpenBLAS, set the `GGML_BLAS` and `GGML_BLAS_VENDOR` environment variables before installing:\n\n```bash\nCMAKE_ARGS=\"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n```\n</details>\n\n<details>\n<summary>CUDA</summary>\n\nTo install with CUDA support, set the `GGML_CUDA=on` environment variable before installing:\n\n```bash\nCMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n```\n\n**Pre-built Wheel (New)**\n\nIt is also possible to install a pre-built wheel with CUDA support. As long as your system meets some requirements:\n\n- CUDA Version is 12.1, 12.2, 12.3, 12.4 or 12.5\n- Python Version is 3.10, 3.11 or 3.12\n\n```bash\npip install llama-cpp-python \\\n  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/<cuda-version>\n```\n\nWhere `<cuda-version>` is one of the following:\n- `cu121`: CUDA 12.1\n- `cu122`: CUDA 12.2\n- `cu123`: CUDA 12.3\n- `cu124`: CUDA 12.4\n- `cu125`: CUDA 12.5\n\nFor example, to install the CUDA 12.1 wheel:\n\n```bash\npip install llama-cpp-python \\\n  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n```\n\n</details>\n\n<details>\n<summary>Metal</summary>\n\nTo install with Metal (MPS), set the `GGML_METAL=on` environment variable before installing:\n\n```bash\nCMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\n```\n\n**Pre-built Wheel (New)**\n\nIt is also possible to install a pre-built wheel with Metal support. As long as your system meets some requirements:\n\n- MacOS Version is 11.0 or later\n- Python Version is 3.10, 3.11 or 3.12\n\n```bash\npip install llama-cpp-python \\\n  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/metal\n```\n\n</details>\n\n<details>\n<summary>hipBLAS (ROCm)</summary>\n\nTo install with hipBLAS / ROCm support for AMD cards, set the `GGML_HIPBLAS=on` environment variable before installing:\n\n```bash\nCMAKE_ARGS=\"-DGGML_HIPBLAS=on\" pip install llama-cpp-python\n```\n\n</details>\n\n<details>\n<summary>Vulkan</summary>\n\nTo install with Vulkan support, set the `GGML_VULKAN=on` environment variable before installing:\n\n```bash\nCMAKE_ARGS=\"-DGGML_VULKAN=on\" pip install llama-cpp-python\n```\n\n</details>\n\n<details>\n<summary>SYCL</summary>\n\nTo install with SYCL support, set the `GGML_SYCL=on` environment variable before installing:\n\n```bash\nsource /opt/intel/oneapi/setvars.sh   \nCMAKE_ARGS=\"-DGGML_SYCL=on -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx\" pip install llama-cpp-python\n```\n</details>\n\n<details>\n<summary>RPC</summary>\n\nTo install with RPC support, set the `GGML_RPC=on` environment variable before installing:\n\n```bash\nsource /opt/intel/oneapi/setvars.sh   \nCMAKE_ARGS=\"-DGGML_RPC=on\" pip install llama-cpp-python\n```\n</details>\n\n\n### Windows Notes\n\n<details>\n<summary>Error: Can't find 'nmake' or 'CMAKE_C_COMPILER'</summary>\n\nIf you run into issues where it complains it can't find `'nmake'` `'?'` or CMAKE_C_COMPILER, you can extract w64devkit as [mentioned in llama.cpp repo](https://github.com/ggerganov/llama.cpp#openblas) and add those manually to CMAKE_ARGS before running `pip` install:\n\n```ps\n$env:CMAKE_GENERATOR = \"MinGW Makefiles\"\n$env:CMAKE_ARGS = \"-DGGML_OPENBLAS=on -DCMAKE_C_COMPILER=C:/w64devkit/bin/gcc.exe -DCMAKE_CXX_COMPILER=C:/w64devkit/bin/g++.exe\"\n```\n\nSee the above instructions and set `CMAKE_ARGS` to the BLAS backend you want to use.\n</details>\n\n### MacOS Notes\n\nDetailed MacOS Metal GPU install documentation is available at [docs/install/macos.md](https://llama-cpp-python.readthedocs.io/en/latest/install/macos/)\n\n<details>\n<summary>M1 Mac Performance Issue</summary>\n\nNote: If you are using Apple Silicon (M1) Mac, make sure you have installed a version of Python that supports arm64 architecture. For example:\n\n```bash\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\nbash Miniforge3-MacOSX-arm64.sh\n```\n\nOtherwise, while installing it will build the llama.cpp x86 version which will be 10x slower on Apple Silicon (M1) Mac.\n</details>\n\n<details>\n<summary>M Series Mac Error: `(mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64'))`</summary>\n\nTry installing with\n\n```bash\nCMAKE_ARGS=\"-DCMAKE_OSX_ARCHITECTURES=arm64 -DCMAKE_APPLE_SILICON_PROCESSOR=arm64 -DGGML_METAL=on\" pip install --upgrade --verbose --force-reinstall --no-cache-dir llama-cpp-python\n```\n</details>\n\n### Upgrading and Reinstalling\n\nTo upgrade and rebuild `llama-cpp-python` add `--upgrade --force-reinstall --no-cache-dir` flags to the `pip install` command to ensure the package is rebuilt from source.\n\n## High-level API\n\n[API Reference](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#high-level-api)\n\nThe high-level API provides a simple managed interface through the [`Llama`](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama) class.\n\nBelow is a short example demonstrating how to use the high-level API to for basic text completion:\n\n```python\nfrom llama_cpp import Llama\n\nllm = Llama(\n      model_path=\"./models/7B/llama-model.gguf\",\n      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n      # seed=1337, # Uncomment to set a specific seed\n      # n_ctx=2048, # Uncomment to increase the context window\n)\noutput = llm(\n      \"Q: Name the planets in the solar system? A: \", # Prompt\n      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n      echo=True # Echo the prompt back in the output\n) # Generate a completion, can also call create_completion\nprint(output)\n```\n\nBy default `llama-cpp-python` generates completions in an OpenAI compatible format:\n\n```python\n{\n  \"id\": \"cmpl-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n  \"object\": \"text_completion\",\n  \"created\": 1679561337,\n  \"model\": \"./models/7B/llama-model.gguf\",\n  \"choices\": [\n    {\n      \"text\": \"Q: Name the planets in the solar system? A: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.\",\n      \"index\": 0,\n      \"logprobs\": None,\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 14,\n    \"completion_tokens\": 28,\n    \"total_tokens\": 42\n  }\n}\n```\n\nText completion is available through the [`__call__`](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.__call__) and [`create_completion`](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_completion) methods of the [`Llama`](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama) class.\n\n### Pulling models from Hugging Face Hub\n\nYou can download `Llama` models in `gguf` format directly from Hugging Face using the [`from_pretrained`](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.from_pretrained) method.\nYou'll need to install the `huggingface-hub` package to use this feature (`pip install huggingface-hub`).\n\n```python\nllm = Llama.from_pretrained(\n    repo_id=\"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n    filename=\"*q8_0.gguf\",\n    verbose=False\n)\n```\n\nBy default [`from_pretrained`](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.from_pretrained) will download the model to the huggingface cache directory, you can then manage installed model files with the [`huggingface-cli`](https://huggingface.co/docs/huggingface_hub/en/guides/cli) tool.\n\n### Chat Completion\n\nThe high-level API also provides a simple interface for chat completion.\n\nChat completion requires that the model knows how to format the messages into a single prompt.\nThe `Llama` class does this using pre-registered chat formats (ie. `chatml`, `llama-2`, `gemma`, etc) or by providing a custom chat handler object.\n\nThe model will will format the messages into a single prompt using the following order of precedence:\n  - Use the `chat_handler` if provided\n  - Use the `chat_format` if provided\n  - Use the `tokenizer.chat_template` from the `gguf` model's metadata (should work for most new models, older models may not have this)\n  - else, fallback to the `llama-2` chat format\n\nSet `verbose=True` to see the selected chat format.\n\n```python\nfrom llama_cpp import Llama\nllm = Llama(\n      model_path=\"path/to/llama-2/llama-model.gguf\",\n      chat_format=\"llama-2\"\n)\nllm.create_chat_completion(\n      messages = [\n          {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n          {\n              \"role\": \"user\",\n              \"content\": \"Describe this image in detail please.\"\n          }\n      ]\n)\n```\n\nChat completion is available through the [`create_chat_completion`](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion) method of the [`Llama`](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama) class.\n\nFor OpenAI API v1 compatibility, you use the [`create_chat_completion_openai_v1`](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion_openai_v1) method which will return pydantic models instead of dicts.\n\n\n### JSON and JSON Schema Mode\n\nTo constrain chat responses to only valid JSON or a specific JSON Schema use the `response_format` argument in [`create_chat_completion`](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_chat_completion).\n\n#### JSON Mode\n\nThe following example will constrain the response to valid JSON strings only.\n\n```python\nfrom llama_cpp import Llama\nllm = Llama(model_path=\"path/to/model.gguf\", chat_format=\"chatml\")\nllm.create_chat_completion(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n        },\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"},\n    ],\n    response_format={\n        \"type\": \"json_object\",\n    },\n    temperature=0.7,\n)\n```\n\n#### JSON Schema Mode\n\nTo constrain the response further to a specific JSON Schema add the schema to the `schema` property of the `response_format` argument.\n\n```python\nfrom llama_cpp import Llama\nllm = Llama(model_path=\"path/to/model.gguf\", chat_format=\"chatml\")\nllm.create_chat_completion(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that outputs in JSON.\",\n        },\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020\"},\n    ],\n    response_format={\n        \"type\": \"json_object\",\n        \"schema\": {\n            \"type\": \"object\",\n            \"properties\": {\"team_name\": {\"type\": \"string\"}},\n            \"required\": [\"team_name\"],\n        },\n    },\n    temperature=0.7,\n)\n```\n\n### Function Calling\n\nThe high-level API supports OpenAI compatible function and tool calling. This is possible through the `functionary` pre-trained models chat format or through the generic `chatml-function-calling` chat format.\n\n```python\nfrom llama_cpp import Llama\nllm = Llama(model_path=\"path/to/chatml/llama-model.gguf\", chat_format=\"chatml-function-calling\")\nllm.create_chat_completion(\n      messages = [\n        {\n          \"role\": \"system\",\n          \"content\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. The assistant calls functions with appropriate input when necessary\"\n\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"Extract Jason is 25 years old\"\n        }\n      ],\n      tools=[{\n        \"type\": \"function\",\n        \"function\": {\n          \"name\": \"UserDetail\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"title\": \"UserDetail\",\n            \"properties\": {\n              \"name\": {\n                \"title\": \"Name\",\n                \"type\": \"string\"\n              },\n              \"age\": {\n                \"title\": \"Age\",\n                \"type\": \"integer\"\n              }\n            },\n            \"required\": [ \"name\", \"age\" ]\n          }\n        }\n      }],\n      tool_choice={\n        \"type\": \"function\",\n        \"function\": {\n          \"name\": \"UserDetail\"\n        }\n      }\n)\n```\n\n<details>\n<summary>Functionary v2</summary>\n\nThe various gguf-converted files for this set of models can be found [here](https://huggingface.co/meetkai). Functionary is able to intelligently call functions and also analyze any provided function outputs to generate coherent responses. All v2 models of functionary supports **parallel function calling**. You can provide either `functionary-v1` or `functionary-v2` for the `chat_format` when initializing the Llama class.\n\nDue to discrepancies between llama.cpp and HuggingFace's tokenizers, it is required to provide HF Tokenizer for functionary. The `LlamaHFTokenizer` class can be initialized and passed into the Llama class. This will override the default llama.cpp tokenizer used in Llama class. The tokenizer files are already included in the respective HF repositories hosting the gguf files.\n\n```python\nfrom llama_cpp import Llama\nfrom llama_cpp.llama_tokenizer import LlamaHFTokenizer\nllm = Llama.from_pretrained(\n  repo_id=\"meetkai/functionary-small-v2.2-GGUF\",\n  filename=\"functionary-small-v2.2.q4_0.gguf\",\n  chat_format=\"functionary-v2\",\n  tokenizer=LlamaHFTokenizer.from_pretrained(\"meetkai/functionary-small-v2.2-GGUF\")\n)\n```\n\n**NOTE**: There is no need to provide the default system messages used in Functionary as they are added automatically in the Functionary chat handler. Thus, the messages should contain just the chat messages and/or system messages that provide additional context for the model (e.g.: datetime, etc.).\n</details>\n\n### Multi-modal Models\n\n`llama-cpp-python` supports such as llava1.5 which allow the language model to read information from both text and images.\n\nBelow are the supported multi-modal models and their respective chat handlers (Python API) and chat formats (Server API).\n\n| Model | `LlamaChatHandler` | `chat_format` |\n|:--- |:--- |:--- |\n| [llava-v1.5-7b](https://huggingface.co/mys/ggml_llava-v1.5-7b) | `Llava15ChatHandler` | `llava-1-5` |\n| [llava-v1.5-13b](https://huggingface.co/mys/ggml_llava-v1.5-13b) | `Llava15ChatHandler` | `llava-1-5` |\n| [llava-v1.6-34b](https://huggingface.co/cjpais/llava-v1.6-34B-gguf) | `Llava16ChatHandler` | `llava-1-6` |\n| [moondream2](https://huggingface.co/vikhyatk/moondream2) | `MoondreamChatHandler` | `moondream2` |\n| [nanollava](https://huggingface.co/abetlen/nanollava-gguf) | `NanollavaChatHandler` | `nanollava` |\n| [llama-3-vision-alpha](https://huggingface.co/abetlen/llama-3-vision-alpha-gguf) | `Llama3VisionAlphaChatHandler` | `llama-3-vision-alpha` |\n| [minicpm-v-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf) | `MiniCPMv26ChatHandler` | `minicpm-v-2.6` |\n\nThen you'll need to use a custom chat handler to load the clip model and process the chat messages and images.\n\n```python\nfrom llama_cpp import Llama\nfrom llama_cpp.llama_chat_format import Llava15ChatHandler\nchat_handler = Llava15ChatHandler(clip_model_path=\"path/to/llava/mmproj.bin\")\nllm = Llama(\n  model_path=\"./path/to/llava/llama-model.gguf\",\n  chat_handler=chat_handler,\n  n_ctx=2048, # n_ctx should be increased to accommodate the image embedding\n)\nllm.create_chat_completion(\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\" : \"text\", \"text\": \"What's in this image?\"},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" } }\n            ]\n        }\n    ]\n)\n```\n\nYou can also pull the model from the Hugging Face Hub using the `from_pretrained` method.\n\n```python\nfrom llama_cpp import Llama\nfrom llama_cpp.llama_chat_format import MoondreamChatHandler\n\nchat_handler = MoondreamChatHandler.from_pretrained(\n  repo_id=\"vikhyatk/moondream2\",\n  filename=\"*mmproj*\",\n)\n\nllm = Llama.from_pretrained(\n  repo_id=\"vikhyatk/moondream2\",\n  filename=\"*text-model*\",\n  chat_handler=chat_handler,\n  n_ctx=2048, # n_ctx should be increased to accommodate the image embedding\n)\n\nresponse = llm.create_chat_completion(\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\" : \"text\", \"text\": \"What's in this image?\"},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\" } }\n\n            ]\n        }\n    ]\n)\nprint(response[\"choices\"][0][\"text\"])\n```\n\n**Note**: Multi-modal models also support tool calling and JSON mode.\n\n<details>\n<summary>Loading a Local Image</summary>\n\nImages can be passed as base64 encoded data URIs. The following example demonstrates how to do this.\n\n```python\nimport base64\n\ndef image_to_base64_data_uri(file_path):\n    with open(file_path, \"rb\") as img_file:\n        base64_data = base64.b64encode(img_file.read()).decode('utf-8')\n        return f\"data:image/png;base64,{base64_data}\"\n\n# Replace 'file_path.png' with the actual path to your PNG file\nfile_path = 'file_path.png'\ndata_uri = image_to_base64_data_uri(file_path)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri }},\n            {\"type\" : \"text\", \"text\": \"Describe this image in detail please.\"}\n        ]\n    }\n]\n\n```\n\n</details>\n\n### Speculative Decoding\n\n`llama-cpp-python` supports speculative decoding which allows the model to generate completions based on a draft model.\n\nThe fastest way to use speculative decoding is through the `LlamaPromptLookupDecoding` class.\n\nJust pass this as a draft model to the `Llama` class during initialization.\n\n```python\nfrom llama_cpp import Llama\nfrom llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n\nllama = Llama(\n    model_path=\"path/to/model.gguf\",\n    draft_model=LlamaPromptLookupDecoding(num_pred_tokens=10) # num_pred_tokens is the number of tokens to predict 10 is the default and generally good for gpu, 2 performs better for cpu-only machines.\n)\n```\n\n### Embeddings\n\nTo generate text embeddings use [`create_embedding`](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.create_embedding) or [`embed`](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama.embed). Note that you must pass `embedding=True` to the constructor upon model creation for these to work properly.\n\n```python\nimport llama_cpp\n\nllm = llama_cpp.Llama(model_path=\"path/to/model.gguf\", embedding=True)\n\nembeddings = llm.create_embedding(\"Hello, world!\")\n\n# or create multiple embeddings at once\n\nembeddings = llm.create_embedding([\"Hello, world!\", \"Goodbye, world!\"])\n```\n\nThere are two primary notions of embeddings in a Transformer-style model: *token level* and *sequence level*. Sequence level embeddings are produced by \"pooling\" token level embeddings together, usually by averaging them or using the first token.\n\nModels that are explicitly geared towards embeddings will usually return sequence level embeddings by default, one for each input string. Non-embedding models such as those designed for text generation will typically return only token level embeddings, one for each token in each sequence. Thus the dimensionality of the return type will be one higher for token level embeddings.\n\nIt is possible to control pooling behavior in some cases using the `pooling_type` flag on model creation. You can ensure token level embeddings from any model using `LLAMA_POOLING_TYPE_NONE`. The reverse, getting a generation oriented model to yield sequence level embeddings is currently not possible, but you can always do the pooling manually.\n\n### Adjusting the Context Window\n\nThe context window of the Llama models determines the maximum number of tokens that can be processed at once. By default, this is set to 512 tokens, but can be adjusted based on your requirements.\n\nFor instance, if you want to work with larger contexts, you can expand the context window by setting the n_ctx parameter when initializing the Llama object:\n\n```python\nllm = Llama(model_path=\"./models/7B/llama-model.gguf\", n_ctx=2048)\n```\n\n## OpenAI Compatible Web Server\n\n`llama-cpp-python` offers a web server which aims to act as a drop-in replacement for the OpenAI API.\nThis allows you to use llama.cpp compatible models with any OpenAI compatible client (language libraries, services, etc).\n\nTo install the server package and get started:\n\n```bash\npip install 'llama-cpp-python[server]'\npython3 -m llama_cpp.server --model models/7B/llama-model.gguf\n```\n\nSimilar to Hardware Acceleration section above, you can also install with GPU (cuBLAS) support like this:\n\n```bash\nCMAKE_ARGS=\"-DGGML_CUDA=on\" FORCE_CMAKE=1 pip install 'llama-cpp-python[server]'\npython3 -m llama_cpp.server --model models/7B/llama-model.gguf --n_gpu_layers 35\n```\n\nNavigate to [http://localhost:8000/docs](http://localhost:8000/docs) to see the OpenAPI documentation.\n\nTo bind to `0.0.0.0` to enable remote connections, use `python3 -m llama_cpp.server --host 0.0.0.0`.\nSimilarly, to change the port (default is 8000), use `--port`.\n\nYou probably also want to set the prompt format. For chatml, use\n\n```bash\npython3 -m llama_cpp.server --model models/7B/llama-model.gguf --chat_format chatml\n```\n\nThat will format the prompt according to how model expects it. You can find the prompt format in the model card.\nFor possible options, see [llama_cpp/llama_chat_format.py](llama_cpp/llama_chat_format.py) and look for lines starting with \"@register_chat_format\".\n\nIf you have `huggingface-hub` installed, you can also use the `--hf_model_repo_id` flag to load a model from the Hugging Face Hub.\n\n```bash\npython3 -m llama_cpp.server --hf_model_repo_id Qwen/Qwen2-0.5B-Instruct-GGUF --model '*q8_0.gguf'\n```\n\n### Web Server Features\n\n- [Local Copilot replacement](https://llama-cpp-python.readthedocs.io/en/latest/server/#code-completion)\n- [Function Calling support](https://llama-cpp-python.readthedocs.io/en/latest/server/#function-calling)\n- [Vision API support](https://llama-cpp-python.readthedocs.io/en/latest/server/#multimodal-models)\n- [Multiple Models](https://llama-cpp-python.readthedocs.io/en/latest/server/#configuration-and-multi-model-support)\n\n## Docker image\n\nA Docker image is available on [GHCR](https://ghcr.io/abetlen/llama-cpp-python). To run the server:\n\n```bash\ndocker run --rm -it -p 8000:8000 -v /path/to/models:/models -e MODEL=/models/llama-model.gguf ghcr.io/abetlen/llama-cpp-python:latest\n```\n\n[Docker on termux (requires root)](https://gist.github.com/FreddieOliveira/efe850df7ff3951cb62d74bd770dce27) is currently the only known way to run this on phones, see [termux support issue](https://github.com/abetlen/llama-cpp-python/issues/389)\n\n## Low-level API\n\n[API Reference](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#low-level-api)\n\nThe low-level API is a direct [`ctypes`](https://docs.python.org/3/library/ctypes.html) binding to the C API provided by `llama.cpp`.\nThe entire low-level API can be found in [llama_cpp/llama_cpp.py](https://github.com/abetlen/llama-cpp-python/blob/master/llama_cpp/llama_cpp.py) and directly mirrors the C API in [llama.h](https://github.com/ggerganov/llama.cpp/blob/master/llama.h).\n\nBelow is a short example demonstrating how to use the low-level API to tokenize a prompt:\n\n```python\nimport llama_cpp\nimport ctypes\nllama_cpp.llama_backend_init(False) # Must be called once at the start of each program\nparams = llama_cpp.llama_context_default_params()\n# use bytes for char * params\nmodel = llama_cpp.llama_load_model_from_file(b\"./models/7b/llama-model.gguf\", params)\nctx = llama_cpp.llama_new_context_with_model(model, params)\nmax_tokens = params.n_ctx\n# use ctypes arrays for array params\ntokens = (llama_cpp.llama_token * int(max_tokens))()\nn_tokens = llama_cpp.llama_tokenize(ctx, b\"Q: Name the planets in the solar system? A: \", tokens, max_tokens, llama_cpp.c_bool(True))\nllama_cpp.llama_free(ctx)\n```\n\nCheck out the [examples folder](examples/low_level_api) for more examples of using the low-level API.\n\n## Documentation\n\nDocumentation is available via [https://llama-cpp-python.readthedocs.io/](https://llama-cpp-python.readthedocs.io/).\nIf you find any issues with the documentation, please open an issue or submit a PR.\n\n## Development\n\nThis package is under active development and I welcome any contributions.\n\nTo get started, clone the repository and install the package in editable / development mode:\n\n```bash\ngit clone --recurse-submodules https://github.com/abetlen/llama-cpp-python.git\ncd llama-cpp-python\n\n# Upgrade pip (required for editable mode)\npip install --upgrade pip\n\n# Install with pip\npip install -e .\n\n# if you want to use the fastapi / openapi server\npip install -e '.[server]'\n\n# to install all optional dependencies\npip install -e '.[all]'\n\n# to clear the local build cache\nmake clean\n```\n\nNow try running the tests\n\n```bash\npytest\n```\n\nThere's a `Makefile` available with useful targets.\nA typical workflow would look like this:\n\n```bash\nmake build\nmake test\n```\n\nYou can also test out specific commits of `llama.cpp` by checking out the desired commit in the `vendor/llama.cpp` submodule and then running `make clean` and `pip install -e .` again. Any changes in the `llama.h` API will require\nchanges to the `llama_cpp/llama_cpp.py` file to match the new API (additional changes may be required elsewhere).\n\n## FAQ\n\n### Are there pre-built binaries / binary wheels available?\n\nThe recommended installation method is to install from source as described above.\nThe reason for this is that `llama.cpp` is built with compiler optimizations that are specific to your system.\nUsing pre-built binaries would require disabling these optimizations or supporting a large number of pre-built binaries for each platform.\n\nThat being said there are some pre-built binaries available through the Releases as well as some community provided wheels.\n\nIn the future, I would like to provide pre-built binaries and wheels for common platforms and I'm happy to accept any useful contributions in this area.\nThis is currently being tracked in [#741](https://github.com/abetlen/llama-cpp-python/issues/741)\n\n### How does this compare to other Python bindings of `llama.cpp`?\n\nI originally wrote this package for my own use with two goals in mind:\n\n- Provide a simple process to install `llama.cpp` and access the full C API in `llama.h` from Python\n- Provide a high-level Python API that can be used as a drop-in replacement for the OpenAI API so existing apps can be easily ported to use `llama.cpp`\n\nAny contributions and changes to this package will be made with these goals in mind.\n\n## License\n\nThis project is licensed under the terms of the MIT license.\n"
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "llama_cpp",
          "type": "tree",
          "content": null
        },
        {
          "name": "mkdocs.yml",
          "type": "blob",
          "size": 1.7822265625,
          "content": "site_name: llama-cpp-python\nrepo_url: https://github.com/abetlen/llama-cpp-python\n\ntheme:\n  name: material\n  palette: \n\n    # Palette toggle for light mode\n    - scheme: default\n      primary: indigo\n      toggle:\n        icon: material/brightness-7 \n        name: Switch to dark mode\n\n    # Palette toggle for dark mode\n    - scheme: slate\n      primary: indigo\n      toggle:\n        icon: material/brightness-4\n        name: Switch to light mode\n\nplugins:\n  - search\n  - mkdocstrings:\n      handlers:\n        python:\n          options:\n            members_order: source\n            group_by_category: false\n            signature_crossrefs: true\n            show_signature: true\n            docstring_section_style: list\n            show_root_heading: true\n            heading_level: 3\n            preload_modules:\n              - typing\n              - typing_extensions\n              - ctypes\n          import:\n            - https://docs.python.org/3/objects.inv\n            - https://numpy.org/doc/stable/objects.inv\n\nwatch:\n  - llama_cpp\n  - README.md\n\nnav:\n  - \"Getting Started\": \"index.md\"\n  - \"Installation Guides\":\n    - \"macOS (Metal)\": \"install/macos.md\"\n  - \"API Reference\": \"api-reference.md\"\n  - \"OpenAI Compatible Web Server\": \"server.md\"\n  - \"Changelog\": \"changelog.md\"\n\nmarkdown_extensions:\n  - attr_list\n  - pymdownx.emoji:\n      emoji_index: !!python/name:materialx.emoji.twemoji\n      emoji_generator: !!python/name:materialx.emoji.to_svg\n  - pymdownx.highlight:\n      anchor_linenums: true\n      line_spans: __span\n      pygments_lang_class: true\n  - pymdownx.inlinehilite\n  - pymdownx.magiclink:\n      repo_url_shorthand: true\n      user: abetlen\n      repo: llama-cpp-python\n  - pymdownx.snippets\n  - pymdownx.superfences\n  - pymdownx.tabbed:\n      alternate_style: true \n  - pymdownx.tilde\n  - tables\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 2.0166015625,
          "content": "[build-system]\nrequires = [\"scikit-build-core[pyproject]>=0.9.2\"]\nbuild-backend = \"scikit_build_core.build\"\n\n[project]\nname = \"llama_cpp_python\"\ndynamic = [\"version\"]\ndescription = \"Python bindings for the llama.cpp library\"\nreadme = \"README.md\"\nlicense = { text = \"MIT\" }\nauthors = [\n    { name = \"Andrei Betlen\", email = \"abetlen@gmail.com\" },\n]\ndependencies = [\n    \"typing-extensions>=4.5.0\",\n    \"numpy>=1.20.0\",\n    \"diskcache>=5.6.1\",\n    \"jinja2>=2.11.3\",\n]\nrequires-python = \">=3.8\"\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n]\n\n\n[project.optional-dependencies]\nserver = [\n    \"uvicorn>=0.22.0\",\n    \"fastapi>=0.100.0\",\n    \"pydantic-settings>=2.0.1\",\n    \"sse-starlette>=1.6.1\",\n    \"starlette-context>=0.3.6,<0.4\",\n    \"PyYAML>=5.1\",\n]\ntest = [\n    \"pytest>=7.4.0\",\n    \"httpx>=0.24.1\",\n    \"scipy>=1.10\",\n    \"fastapi>=0.100.0\",\n    \"sse-starlette>=1.6.1\",\n    \"starlette-context>=0.3.6,<0.4\",\n    \"pydantic-settings>=2.0.1\",\n    \"huggingface-hub>=0.23.0\"\n]\ndev = [\n    \"black>=23.3.0\",\n    \"twine>=4.0.2\",\n    \"mkdocs>=1.4.3\",\n    \"mkdocstrings[python]>=0.22.0\",\n    \"mkdocs-material>=9.1.18\",\n    \"pytest>=7.4.0\",\n    \"httpx>=0.24.1\",\n]\nall = [\n    \"llama_cpp_python[server,test,dev]\",\n]\n\n[tool.scikit-build]\nwheel.packages = [\"llama_cpp\"]\ncmake.verbose = true\ncmake.minimum-version = \"3.21\"\nminimum-version = \"0.5.1\"\nsdist.include = [\".git\", \"vendor/llama.cpp/*\"]\n\n[tool.scikit-build.metadata.version]\nprovider = \"scikit_build_core.metadata.regex\"\ninput = \"llama_cpp/__init__.py\"\n\n[project.urls]\nHomepage = \"https://github.com/abetlen/llama-cpp-python\"\nIssues = \"https://github.com/abetlen/llama-cpp-python/issues\"\nDocumentation = \"https://llama-cpp-python.readthedocs.io/en/latest/\"\nChangelog = \"https://llama-cpp-python.readthedocs.io/en/latest/changelog/\"\n\n[tool.pytest.ini_options]\ntestpaths = \"tests\"\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "vendor",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}