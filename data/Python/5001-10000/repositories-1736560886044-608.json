{
  "metadata": {
    "timestamp": 1736560886044,
    "page": 608,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "vladmandic/automatic",
      "stars": 5904,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.31640625,
          "content": "# defaults\n.history\n.vscode/\n/__pycache__\n/.ruff_cache\n/cache\n/cache.json\n/config.json\n/extensions/*\n/html/extensions.json\n/html/themes.json\n/metadata.json\n/node_modules\n/outputs/*\n/package-lock.json\n/params.txt\n/pnpm-lock.yaml\n/styles.csv\n/tmp\n/ui-config.json\n/user.css\n/venv\n/webui-user.bat\n/webui-user.sh\n/*.log.*\n/*.log\n"
        },
        {
          "name": ".eslintrc.json",
          "type": "blob",
          "size": 3.5771484375,
          "content": "{\n  \"parserOptions\": {\n    \"ecmaVersion\": 2020\n  },\n  \"plugins\": [\"html\", \"json\"],\n  \"extends\": [\n    \"plugin:json/recommended\",\n    \"plugin:node/recommended\",\n    \"eslint:recommended\",\n    \"airbnb-base\"\n  ],\n  \"env\": {\n    \"browser\": true,\n    \"commonjs\": false,\n    \"node\": false,\n    \"jquery\": false,\n    \"es2020\": true\n  },\n  \"rules\": {\n    \"max-len\": [1, 275, 3],\n    \"camelcase\":\"off\",\n    \"default-case\":\"off\",\n    \"no-await-in-loop\":\"off\",\n    \"no-bitwise\":\"off\",\n    \"no-confusing-arrow\":\"off\",\n    \"no-console\":\"off\",\n    \"no-empty\":\"off\",\n    \"no-loop-func\":\"off\",\n    \"no-mixed-operators\":\"off\",\n    \"no-param-reassign\":\"off\",\n    \"no-plusplus\":\"off\",\n    \"no-restricted-globals\":\"off\",\n    \"no-restricted-syntax\":\"off\",\n    \"no-return-assign\":\"off\",\n    \"no-unused-vars\":\"off\",\n    \"no-useless-escape\":\"off\",\n    \"object-curly-newline\":\"off\",\n    \"prefer-rest-params\":\"off\",\n    \"prefer-destructuring\":\"off\",\n    \"radix\":\"off\",\n    \"node/shebang\": \"off\"\n  },\n  \"globals\": {\n    // asssets\n    \"panzoom\": \"readonly\",\n    // logger.js\n    \"log\": \"readonly\",\n    \"debug\": \"readonly\",\n    \"error\": \"readonly\",\n    \"xhrGet\": \"readonly\",\n    \"xhrPost\": \"readonly\",\n    // script.js\n    \"gradioApp\": \"readonly\",\n    \"executeCallbacks\": \"readonly\",\n    \"onAfterUiUpdate\": \"readonly\",\n    \"onOptionsChanged\": \"readonly\",\n    \"optionsChangedCallbacks\": \"readonly\",\n    \"onUiLoaded\": \"readonly\",\n    \"onUiUpdate\": \"readonly\",\n    \"onUiTabChange\": \"readonly\",\n    \"onUiReady\": \"readonly\",\n    \"uiCurrentTab\": \"writable\",\n    \"uiElementIsVisible\": \"readonly\",\n    \"uiElementInSight\": \"readonly\",\n    \"getUICurrentTabContent\": \"readonly\",\n    \"waitForFlag\": \"readonly\",\n    \"logFn\": \"readonly\",\n    // contextmenus.js\n    \"generateForever\": \"readonly\",\n    // contributors.js\n    \"showContributors\": \"readonly\",\n    // ui.js\n    \"opts\": \"writable\",\n    \"sortUIElements\": \"readonly\",\n    \"all_gallery_buttons\": \"readonly\",\n    \"selected_gallery_button\": \"readonly\",\n    \"selected_gallery_index\": \"readonly\",\n    \"switch_to_txt2img\": \"readonly\",\n    \"switch_to_img2img_tab\": \"readonly\",\n    \"switch_to_img2img\": \"readonly\",\n    \"switch_to_sketch\": \"readonly\",\n    \"switch_to_inpaint\": \"readonly\",\n    \"witch_to_inpaint_sketch\": \"readonly\",\n    \"switch_to_extras\": \"readonly\",\n    \"get_tab_index\": \"readonly\",\n    \"create_submit_args\": \"readonly\",\n    \"restartReload\": \"readonly\",\n    \"updateInput\": \"readonly\",\n    \"toggleCompact\": \"readonly\",\n    \"setFontSize\": \"readonly\",\n    \"setTheme\": \"readonly\",\n    // settings.js\n    \"registerDragDrop\": \"readonly\",\n    // extraNetworks.js\n    \"getENActiveTab\": \"readonly\",\n    \"quickApplyStyle\": \"readonly\",\n    \"quickSaveStyle\": \"readonly\",\n    \"setupExtraNetworks\": \"readonly\",\n    \"showNetworks\": \"readonly\",\n    // from python\n    \"localization\": \"readonly\",\n    // progressbar.js\n    \"randomId\": \"readonly\",\n    \"requestProgress\": \"readonly\",\n    // imageviewer.js\n    \"modalPrevImage\": \"readonly\",\n    \"modalNextImage\": \"readonly\",\n    \"galleryClickEventHandler\": \"readonly\",\n    \"getExif\": \"readonly\",\n    // logMonitor.js\n    \"jobStatusEl\": \"readonly\",\n    // loader.js\n    \"removeSplash\": \"readonly\",\n    // nvml.js\n    \"initNVML\": \"readonly\",\n    \"disableNVML\": \"readonly\",\n    // indexdb.js\n    \"idbGet\": \"readonly\",\n    \"idbPut\": \"readonly\",\n    \"idbDel\": \"readonly\",\n    \"idbAdd\": \"readonly\",\n    // changelog.js\n    \"initChangelog\": \"readonly\",\n    // notification.js\n    \"sendNotification\": \"readonly\"\n  },\n  \"ignorePatterns\": [\n    \"node_modules\",\n    \"extensions\",\n    \"extensions-builtin\",\n    \"repositories\",\n    \"venv\",\n    \"panzoom.js\",\n    \"split.js\",\n    \"exifr.js\",\n    \"iframeResizer.min.js\"\n  ]\n}\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.869140625,
          "content": "# defaults\n__pycache__\n.ruff_cache\n/cache.json\n/*.json\n/*.yaml\n/params.txt\n/styles.csv\n/user.css\n/webui-user.bat\n/webui-user.sh\n/html/extensions.json\n/html/themes.json\nnode_modules\npnpm-lock.yaml\npackage-lock.json\nvenv\n.history\ncache\n**/.DS_Store\ntunableop_results*.csv\n\n# all models and temp files\n*.log\n*.log.*\n*.bak\n*.ckpt\n*.safetensors\n*.pth\n*.pt\n*.bin\n*.optim\n*.lock\n*.zip\n*.rar\n*.7z\n*.pyc\n/*.bat\n/*.sh\n/*.txt\n/*.mp3\n/*.lnk\n!webui.bat\n!webui.sh\n!package.json\n!requirements.txt\n\n# pyinstaller\n*.spec\nbuild/\ndist/\n\n# dynamically generated\n/repositories/ip-instruct/\n\n# all dynamic stuff\n/extensions/**/*\n/outputs/**/*\n/embeddings/**/*\n/models/**/*\n/interrogate/**/*\n/train/log/**/*\n/textual_inversion/**/*\n/detected_maps/**/*\n/tmp\n/log\n/cert\n.vscode/\n.idea/\n/localizations\n.*/\n\n# force included\n!/models/VAE-approx\n!/models/VAE-approx/model.pt\n!/models/Reference\n!/models/Reference/**/*\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 1.115234375,
          "content": "[submodule \"wiki\"]\n  path = wiki\n  url = https://github.com/vladmandic/automatic.wiki\n  ignore = dirty\n[submodule \"modules/k-diffusion\"]\n  path = modules/k-diffusion\n  url = https://github.com/crowsonkb/k-diffusion\n  ignore = dirty\n[submodule \"extensions-builtin/sd-extension-system-info\"]\n  path = extensions-builtin/sd-extension-system-info\n  url = https://github.com/vladmandic/sd-extension-system-info\n  ignore = dirty\n[submodule \"extensions-builtin/sd-extension-chainner\"]\n  path = extensions-builtin/sd-extension-chainner\n  url = https://github.com/vladmandic/sd-extension-chainner\n  ignore = dirty\n[submodule \"extensions-builtin/stable-diffusion-webui-rembg\"]\n  path = extensions-builtin/stable-diffusion-webui-rembg\n  url = https://github.com/vladmandic/sd-extension-rembg\n  ignore = dirty\n[submodule \"extensions-builtin/sd-webui-agent-scheduler\"]\n  path = extensions-builtin/sd-webui-agent-scheduler\n  url = https://github.com/ArtVentureX/sd-webui-agent-scheduler\n  ignore = dirty\n[submodule \"extensions-builtin/sdnext-modernui\"]\n\tpath = extensions-builtin/sdnext-modernui\n\turl = https://github.com/BinaryQuantumSoul/sdnext-modernui\n"
        },
        {
          "name": ".markdownlint.json",
          "type": "blob",
          "size": 0.1259765625,
          "content": "{\n  \"MD004\": false,\n  \"MD012\": false,\n  \"MD013\": false,\n  \"MD032\": false,\n  \"MD033\": false,\n  \"MD036\": false,\n  \"MD041\": false\n}\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 1.390625,
          "content": "# To use:\n#\n#     pre-commit run -a\n#\n# Or:\n#\n#     pre-commit install  # (runs every time you commit in git)\n#\n# To update this file:\n#\n#     pre-commit autoupdate\n#\n# See https://github.com/pre-commit/pre-commit\n\nci:\n  autoupdate_commit_msg: \"chore: update pre-commit hooks\"\n  autofix_commit_msg: \"style: pre-commit fixes\"\n\nrepos:\n# Standard hooks\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v4.4.0\n  hooks:\n  - id: check-added-large-files\n  - id: check-case-conflict\n  - id: check-merge-conflict\n  - id: check-symlinks\n  - id: check-yaml\n    args: [\"--allow-multiple-documents\"]\n  - id: debug-statements\n  - id: end-of-file-fixer\n  - id: mixed-line-ending\n  - id: trailing-whitespace\n    exclude: |\n            (?x)^(\n                .*\\.md|\n                .github/ISSUE_TEMPLATE/.*\\.yml\n            )$\n\n- repo: https://github.com/charliermarsh/ruff-pre-commit\n  rev: 'v0.0.285'\n  hooks:\n    - id: ruff\n      args: [--fix, --exit-non-zero-on-fix]\n- repo: local\n  hooks:\n    - id: pylint\n      name: pylint\n      entry: pylint\n      language: system\n      types: [python]\n      args: []\n\n# Black, the code formatter, natively supports pre-commit\n# - repo: https://github.com/psf/black\n#   rev: 23.7.0\n#   hooks:\n#   - id: black\n#     exclude: ^(docs)\n\n# Changes tabs to spaces\n# - repo: https://github.com/Lucas-C/pre-commit-hooks\n#   rev: v1.5.3\n#   hooks:\n#   - id: remove-tabs\n#     exclude: ^(docs)\n"
        },
        {
          "name": ".pylintrc",
          "type": "blob",
          "size": 6.6318359375,
          "content": "[MAIN]\nanalyse-fallback-blocks=no\nclear-cache-post-run=no\nextension-pkg-allow-list=\nextension-pkg-whitelist=\nfail-on=\nfail-under=10\nignore=CVS\nignore-paths=/usr/lib/.*$,\n             modules/apg,\n             modules/consistory,\n             modules/control/proc,\n             modules/control/units,\n             modules/ctrlx,\n             modules/dml,\n             modules/freescale,\n             modules/ggml,\n             modules/hidiffusion,\n             modules/hijack,\n             modules/instantir,\n             modules/intel/ipex,\n             modules/intel/openvino,\n             modules/k-diffusion,\n             modules/ldsr,\n             modules/meissonic,\n             modules/omnigen,\n             modules/onnx_impl,\n             modules/pag,\n             modules/prompt_parser_xhinker.py,\n             modules/pulid/eva_clip,\n             modules/rife,\n             modules/schedulers,\n             modules/taesd,\n             modules/teacache,\n             modules/todo,\n             modules/unipc,\n             modules/xadapter,\n             repositories,\n             extensions-builtin/sd-webui-agent-scheduler,\n             extensions-builtin/sd-extension-chainner/nodes,\n             extensions-builtin/sdnext-modernui/node_modules,\nignore-patterns=.*test*.py$,\n                .*_model.py$,\n                .*_arch.py$,\n                .*_model_arch.py*,\n                .*_model_arch_v2.py$, \nignored-modules=\njobs=0\nlimit-inference-results=100\nload-plugins=\npersistent=yes\npy-version=3.9\nrecursive=no\nsource-roots=\nsuggestion-mode=yes\nunsafe-load-any-extension=no\n\n[BASIC]\nargument-naming-style=snake_case\nattr-naming-style=snake_case\nbad-names=foo, bar, baz, toto, tutu, tata\nbad-names-rgxs=\nclass-attribute-naming-style=any\nclass-const-naming-style=UPPER_CASE\nclass-naming-style=PascalCase\nconst-naming-style=snake_case\ndocstring-min-length=-1\nfunction-naming-style=snake_case\ngood-names=i,j,k,e,ex,ok,p,x,y,id\ngood-names-rgxs=\ninclude-naming-hint=no\ninlinevar-naming-style=any\nmethod-naming-style=snake_case\nmodule-naming-style=snake_case\nname-group=\nno-docstring-rgx=^_\nproperty-classes=abc.abstractproperty\nvariable-naming-style=snake_case\n\n[CLASSES]\ncheck-protected-access-in-special-methods=no\ndefining-attr-methods=__init__, __new__,\nexclude-protected=_asdict,_fields,_replace,_source,_make,os._exit\nvalid-classmethod-first-arg=cls\nvalid-metaclass-classmethod-first-arg=mcs\n\n[DESIGN]\nexclude-too-few-public-methods=\nignored-parents=\nmax-args=99\nmax-attributes=99\nmax-bool-expr=99\nmax-branches=199\nmax-locals=99\nmax-parents=99\nmax-public-methods=99\nmax-returns=99\nmax-statements=199\nmin-public-methods=1\n\n[EXCEPTIONS]\novergeneral-exceptions=builtins.BaseException,builtins.Exception\n\n[FORMAT]\nexpected-line-ending-format=\nignore-long-lines=^\\s*(# )?<?https?://\\S+>?$\nindent-after-paren=4\nindent-string='    '\nmax-line-length=200\nmax-module-lines=9999\nsingle-line-class-stmt=no\nsingle-line-if-stmt=no\n\n[IMPORTS]\nallow-any-import-level=\nallow-reexport-from-package=no\nallow-wildcard-with-all=no\ndeprecated-modules=\next-import-graph=\nimport-graph=\nint-import-graph=\nknown-standard-library=\nknown-third-party=enchant\npreferred-modules=\n\n[LOGGING]\nlogging-format-style=new\nlogging-modules=logging\n\n[MESSAGES CONTROL]\nconfidence=HIGH,\n           CONTROL_FLOW,\n           INFERENCE,\n           INFERENCE_FAILURE,\n           UNDEFINED\n# disable=C,R,W\ndisable=abstract-method,\n        bad-inline-option,\n        bare-except,\n        broad-exception-caught,\n        chained-comparison,\n        consider-iterating-dictionary,\n        consider-merging-isinstance,\n        consider-using-dict-items,\n        consider-using-enumerate,\n        consider-using-from-import,\n        consider-using-generator,\n        consider-using-get,\n        consider-using-in,\n        consider-using-min-builtin,\n        consider-using-max-builtin,\n        consider-using-sys-exit,\n        dangerous-default-value,\n        deprecated-pragma,\n        duplicate-code,\n        file-ignored,\n        import-error,\n        import-outside-toplevel,\n        invalid-name,\n        line-too-long,\n        locally-disabled,\n        logging-fstring-interpolation,\n        missing-class-docstring,\n        missing-function-docstring,\n        missing-module-docstring,\n        no-else-return,\n        not-callable,\n        pointless-string-statement,\n        raw-checker-failed,\n        simplifiable-if-expression,\n        suppressed-message,\n        too-few-public-methods,\n        too-many-instance-attributes,\n        too-many-locals,\n        too-many-nested-blocks,\n        too-many-statements,\n        too-many-positional-arguments,\n        unidiomatic-typecheck,\n        unnecessary-dict-index-lookup,\n        unnecessary-dunder-call,\n        unnecessary-lambda,\n        unnecessary-lambda-assigment,\n        use-dict-literal,\n        use-symbolic-message-instead,\n        unknown-option-value,\n        useless-suppression,\n        wrong-import-position,\nenable=c-extension-no-member\n\n[METHOD_ARGS]\ntimeout-methods=requests.api.delete,requests.api.get,requests.api.head,requests.api.options,requests.api.patch,requests.api.post,requests.api.put,requests.api.request\n\n[MISCELLANEOUS]\nnotes=FIXME,\n      XXX,\n      TODO\nnotes-rgx=\n\n[REFACTORING]\nmax-nested-blocks=5\nnever-returning-functions=sys.exit,argparse.parse_error\n\n[REPORTS]\nevaluation=max(0, 0 if fatal else 10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10))\nmsg-template=\nreports=no\nscore=no\n\n[SIMILARITIES]\nignore-comments=yes\nignore-docstrings=yes\nignore-imports=yes\nignore-signatures=yes\nmin-similarity-lines=4\n\n[SPELLING]\nmax-spelling-suggestions=4\nspelling-dict=\nspelling-ignore-comment-directives=fmt: on,fmt: off,noqa:,noqa,nosec,isort:skip,mypy:\nspelling-ignore-words=\nspelling-private-dict-file=\nspelling-store-unknown-words=no\n\n[STRING]\ncheck-quote-consistency=no\ncheck-str-concat-over-line-jumps=no\n\n[TYPECHECK]\ncontextmanager-decorators=contextlib.contextmanager\ngenerated-members=numpy.*,logging.*,torch.*,cv2.*\nignore-none=yes\nignore-on-opaque-inference=yes\nignored-checks-for-mixins=no-member,\n                          not-async-context-manager,\n                          not-context-manager,\n                          attribute-defined-outside-init\nignored-classes=optparse.Values,thread._local,_thread._local,argparse.Namespace\nmissing-member-hint=yes\nmissing-member-hint-distance=1\nmissing-member-max-choices=1\nmixin-class-rgx=.*[Mm]ixin\nsignature-mutators=\n\n[VARIABLES]\nadditional-builtins=\nallow-global-unused-variables=yes\nallowed-redefined-builtins=\ncallbacks=cb_,\ndummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_\nignored-argument-names=_.*|^ignored_|^unused_\ninit-import=no\nredefining-builtins-modules=six.moves,past.builtins,future.builtins,builtins,io\n"
        },
        {
          "name": ".ruff.toml",
          "type": "blob",
          "size": 2.1923828125,
          "content": "exclude = [\n    \"venv\",\n    \".git\",\n    \".ruff_cache\",\n    \".vscode\",\n    \"modules/apg\",\n    \"modules/consistory\",\n    \"modules/control/proc\",\n    \"modules/control/units\",\n    \"modules/freescale\",\n    \"modules/ggml\",\n    \"modules/hidiffusion\",\n    \"modules/hijack\",\n    \"modules/instantir\",\n    \"modules/intel/ipex\",\n    \"modules/intel/openvino\",\n    \"modules/k-diffusion\",\n    \"modules/ldsr\",\n    \"modules/meissonic\",\n    \"modules/omnigen\",\n    \"modules/pag\",\n    \"modules/postprocess/aurasr_arch.py\",\n    \"modules/prompt_parser_xhinker.py\",\n    \"modules/pulid/eva_clip\",\n    \"modules/rife\",\n    \"modules/schedulers\",\n    \"modules/segmoe\",\n    \"modules/taesd\",\n    \"modules/teacache\",\n    \"modules/todo\",\n    \"modules/unipc\",\n    \"modules/xadapter\",\n    \"repositories\",\n    \"extensions-builtin/sd-extension-chainner/nodes\",\n    \"extensions-builtin/sd-webui-agent-scheduler\",\n    \"extensions-builtin/sdnext-modernui/node_modules\",\n]\nline-length = 250\nindent-width = 4\ntarget-version = \"py39\"\n\n[lint]\nselect = [\n  \"F\",\n  \"E\",\n  \"W\",\n  \"C\",\n  \"B\",\n  \"I\",\n  \"YTT\",\n  \"ASYNC\",\n  \"RUF\",\n  \"AIR\",\n  \"NPY\",\n  \"C4\",\n  \"T10\",\n  \"EXE\",\n  \"ISC\",\n  \"ICN\",\n  \"RSE\",\n  \"TCH\",\n  \"TID\",\n  \"INT\",\n  \"PLE\",\n]\nignore = [\n  \"B006\",   # Do not use mutable data structures for argument defaults\n  \"B008\",   # Do not perform function call in argument defaults\n  \"C408\",   # Unnecessary `dict` call\n  \"I001\",   # Import block is un-sorted or un-formatted\n  \"E402\",   # Module level import not at top of file\n  \"E501\",   # Line too long\n  \"E721\",   # Do not compare types, use `isinstance()`\n  \"E731\",   # Do not assign a `lambda` expression, use a `def`\n  \"E741\",   # Ambiguous variable name\n  \"F401\",   # Imported by unused\n  \"NPY002\", # replace legacy random\n  \"RUF005\", # Consider iterable unpacking\n  \"RUF010\", # Use explicit conversion flag\n  \"RUF012\", # Mutable class attributes\n  \"RUF013\", # PEP 484 prohibits implicit `Optional`\n  \"RUF015\", # Prefer `next(...)` over single element slice\n]\nfixable = [\"ALL\"]\nunfixable = []\ndummy-variable-rgx = \"^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$\"\n\n[format]\nquote-style = \"double\"\nindent-style = \"space\"\nskip-magic-trailing-comma = false\nline-ending = \"auto\"\ndocstring-code-format = false\n\n[lint.mccabe]\nmax-complexity = 150\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 212.6533203125,
          "content": "# Change Log for SD.Next\n\n## Update for 2025-01-02\n\n- [Allegro Video](https://huggingface.co/rhymes-ai/Allegro)  \n  - optimizations: full offload and quantization support  \n  - *reference values*: width 1280 height 720 frames 88 steps 100 guidance 7.5  \n  - *note*: allegro model is really sensitive to input width/height/frames/steps  \n    and may result in completely corrupt output if those are not within expected range  \n- **Logging**:\n  - reverted enable debug by default  \n  - updated [debug wiki](https://github.com/vladmandic/automatic/wiki/debug)  \n  - sort logged timers by duration  \n  - allow min duration env variable for timers: `SD_MIN_TIMER=0.1` (default)  \n  - update installer messages  \n- **Detailer**:\n  - add explicit detailer steps setting  \n- **SysInfo**:\n  - update to collected data and benchmarks  \n- **Fixes**:\n  - explict clear caches on model load  \n  - lock adetailer commit: `#a89c01d`  \n  - xyzgrid progress calculation  \n  - xyzgrid detailer\n  - vae tiling use default value if not set  \n  - sd35 img2img\n  - samplers test for scale noise before using  \n  - scheduler api  \n\n## Update for 2024-12-31\n\nNYE refresh release with quite a few optimizatios and bug fixes...  \nCommit hash: `master: #dcfc9f3` `dev: #935cac6`  \n\n- **LoRA**:  \n  - LoRA load/apply/unapply methods have been changed in 12/2024 Xmass release and further tuned in this release\n  - for details on available methods, see <https://github.com/vladmandic/automatic/wiki/Lora#lora-loader>  \n  - **Sana** support  \n  - quantized models support  \n  - add fuse support with on-demand apply/unapply (new default)  \n  - add legacy option in *settings -> networks*  \n- **HunyuanVideo**:  \n  - optimizations: full offload, quantization and tiling support  \n- **LTXVideo**:  \n  - optimizations: full offload, quantization and tiling support  \n  - [TeaCache](https://github.com/ali-vilab/TeaCache/blob/main/TeaCache4LTX-Video/README.md) integration  \n- **VAE**:  \n  - tiling granular options in *settings -> variable auto encoder*  \n- **UI**:  \n  - live preview optimizations and error handling  \n  - live preview high quality output, thanks @Disty0  \n  - CSS optimizations when log view is disabled  \n- **Samplers**:  \n  - add flow shift options and separate dynamic thresholding from dynamic shifting  \n  - autodetect matching sigma capabilities  \n- **API**:  \n  - better default values for generate  \n- **Refactor**:  \n  - remove all LDM imports if running in native mode  \n  - startup optimizatios  \n- **Torch**:  \n  - support for `torch==2.6.0`  \n- **OpenVINO**:  \n  - disable re-compile on resolution change  \n  - fix shape mismatch on resolution change  \n- **Fixes**:  \n  - flux pipeline switches: txt/img/inpaint  \n  - flux custom unet loader for bnb  \n  - flux do not requantize already quantized model\n  - interrogate caption with T5  \n  - on-the-fly quantization using TorchAO  \n  - remove concurrent preview requests  \n  - xyz grid recover on error  \n  - hires batch  \n  - sdxl refiner  \n  - increase progress timeout\n  - kandinsky matmul  \n  - do not show disabled networks  \n  - enable debug logging by default\n  - image width/height calculation when doing img2img  \n  - corrections with batch processing  \n  - hires with refiner prompt and batch processing  \n  - processing with nested calls  \n  - ui networks initial sort  \n  - esrgan on cpu devices  \n\n## Update for 2024-12-24\n\n### Highlights for 2024-12-24\n\n### SD.Next Xmass edition: *What's new?*\n\nWhile we have several new supported models, workflows and tools, this release is primarily about *quality-of-life improvements*:  \n- New memory management engine  \n  list of changes that went into this one is long: changes to GPU offloading, brand new LoRA loader, system memory management, on-the-fly quantization, improved gguf loader, etc.  \n  but main goal is enabling modern large models to run on standard consumer GPUs  \n  without performance hits typically associated with aggressive memory swapping and needs for constant manual tweaks  \n- New [documentation website](https://vladmandic.github.io/sdnext-docs/)  \n  with full search and tons of new documentation  \n- New settings panel with simplified and streamlined configuration  \n\nWe've also added support for several new models such as highly anticipated [NVLabs Sana](https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px) (see [supported models](https://vladmandic.github.io/sdnext-docs/Model-Support/) for full list)  \nAnd several new SOTA video models: [Lightricks LTX-Video](https://huggingface.co/Lightricks/LTX-Video), [Hunyuan Video](https://huggingface.co/tencent/HunyuanVideo) and [Genmo Mochi.1 Preview](https://huggingface.co/genmo/mochi-1-preview)  \n\nAnd a lot of **Control** and **IPAdapter** goodies  \n- for **SDXL** there is new [ProMax](https://huggingface.co/xinsir/controlnet-union-sdxl-1.0), improved *Union* and *Tiling* models \n- for **FLUX.1** there are [Flux Tools](https://blackforestlabs.ai/flux-1-tools/) as well as official *Canny* and *Depth* models,  \n  a cool [Redux](https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev) model as well as [XLabs](https://huggingface.co/XLabs-AI/flux-ip-adapter-v2) IP-adapter\n- for **SD3.5** there are official *Canny*, *Blur* and *Depth* models in addition to existing 3rd party models  \n  as well as [InstantX](https://huggingface.co/InstantX/SD3.5-Large-IP-Adapter) IP-adapter  \n\nPlus couple of new integrated workflows such as [FreeScale](https://github.com/ali-vilab/FreeScale) and [Style Aligned Image Generation](https://style-aligned-gen.github.io/)  \n\nAnd it wouldn't be a *Xmass edition* without couple of custom themes: *Snowflake* and *Elf-Green*!  \nAll-in-all, we're around ~180 commits worth of updates, check the changelog for full list  \n\n[ReadMe](https://github.com/vladmandic/automatic/blob/master/README.md) | [ChangeLog](https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md) | [Docs](https://vladmandic.github.io/sdnext-docs/) | [WiKi](https://github.com/vladmandic/automatic/wiki) | [Discord](https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867)\n\n## Details for 2024-12-24\n\n### New models and integrations\n\n- [NVLabs Sana](https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px)\n  support for 1.6B 2048px, 1.6B 1024px and 0.6B 512px models  \n  **Sana** can synthesize high-resolution images with strong text-image alignment by using **Gemma2** as text-encoder  \n  and its *fast* - typically at least **2x** faster than sd-xl even for 1.6B variant and maintains performance regardless of resolution  \n  e.g., rendering at 4k is possible in less than 8GB vram  \n  to use, select from *networks -> models -> reference* and models will be auto-downloaded on first use  \n  *reference values*: sampler: default (or any flow-match variant), steps: 20, width/height: 1024, guidance scale: 4.5  \n  *note* like other LLM-based text-encoders, sana prefers long and descriptive prompts  \n  any short prompt below 300 characters will be auto-expanded using built in Gemma LLM before encoding while long prompts will be passed as-is  \n- **ControlNet**\n  - improved support for **Union** controlnets with granular control mode type\n  - added support for latest [Xinsir ProMax](https://huggingface.co/xinsir/controlnet-union-sdxl-1.0) all-in-one controlnet  \n  - added support for multiple **Tiling** controlnets, for example [Xinsir Tile](https://huggingface.co/xinsir/controlnet-tile-sdxl-1.0)  \n    *note*: when selecting tiles in control settings, you can also specify non-square ratios  \n    in which case it will use context-aware image resize to maintain overall composition  \n    *note*: available tiling options can be set in settings -> control  \n- **IP-Adapter**  \n  - FLUX.1 [XLabs](https://huggingface.co/XLabs-AI/flux-ip-adapter-v2) v1 and v2 IP-adapter  \n  - FLUX.1 secondary guidance, enabled using *Attention guidance* in advanced menu  \n  - SD 3.5 [InstantX](https://huggingface.co/InstantX/SD3.5-Large-IP-Adapter) IP-adapter  \n- [Flux Tools](https://blackforestlabs.ai/flux-1-tools/)  \n  **Redux** is actually a tool, **Fill** is inpaint/outpaint optimized version of *Flux-dev*  \n  **Canny** & **Depth** are optimized versions of *Flux-dev* for their respective tasks: they are *not* ControlNets that work on top of a model  \n  to use, go to image or control interface and select *Flux Tools* in scripts  \n  all models are auto-downloaded on first use  \n  *note*: All models are [gated](https://github.com/vladmandic/automatic/wiki/Gated) and require acceptance of terms and conditions via web page  \n  *recommended*: Enable on-the-fly [quantization](https://github.com/vladmandic/automatic/wiki/Quantization) or [compression](https://github.com/vladmandic/automatic/wiki/NNCF-Compression) to reduce resource usage  \n  *todo*: support for Canny/Depth LoRAs  \n  - [Redux](https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev): ~0.1GB  \n    works together with existing model and basically uses input image to analyze it and use that instead of prompt  \n    *optional* can use prompt to combine guidance with input image  \n    *recommended*: low denoise strength levels result in more variety  \n  - [Fill](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev): ~23.8GB, replaces currently loaded model  \n    *note*: can be used in inpaint/outpaint mode only  \n  - [Canny](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev): ~23.8GB, replaces currently loaded model  \n    *recommended*: guidance scale 30  \n  - [Depth](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev): ~23.8GB, replaces currently loaded model  \n    *recommended*: guidance scale 10  \n- [Flux ControlNet LoRA](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora)  \n  alternative to standard ControlNets, FLUX.1 also allows LoRA to help guide the generation process  \n  both **Depth** and **Canny** LoRAs are available in standard control menus  \n- [StabilityAI SD35 ControlNets](https://huggingface.co/stabilityai/stable-diffusion-3.5-controlnets)\n  - In addition to previously released `InstantX` and `Alimama`, we now have *official* ones from StabilityAI  \n- [Style Aligned Image Generation](https://style-aligned-gen.github.io/)  \n  enable in scripts, compatible with sd-xl  \n  enter multiple prompts in prompt field separated by new line  \n  style-aligned applies selected attention layers uniformly to all images to achive consistency  \n  can be used with or without input image in which case first prompt is used to establish baseline  \n  *note:* all prompts are processes as a single batch, so vram is limiting factor  \n- [FreeScale](https://github.com/ali-vilab/FreeScale)  \n  enable in scripts, compatible with sd-xl for text and img2img  \n  run iterative generation of images at different scales to achieve better results  \n  can render 4k sdxl images  \n  *note*: disable live preview to avoid memory issues when generating large images  \n\n### Video models\n\n- [Lightricks LTX-Video](https://huggingface.co/Lightricks/LTX-Video)  \n  model size: 27.75gb  \n  support for 0.9.0, 0.9.1 and custom safetensor-based models with full quantization and offloading support  \n  support for text-to-video and image-to-video, to use, select in *scripts -> ltx-video*  \n  *refrence values*: steps 50, width 704, height 512, frames 161, guidance scale 3.0  \n- [Hunyuan Video](https://huggingface.co/tencent/HunyuanVideo)  \n  model size: 40.92gb  \n  support for text-to-video, to use, select in *scripts -> hunyuan video*  \n  basic support only  \n  *refrence values*: steps 50, width 1280, height 720, frames 129, guidance scale 6.0  \n- [Genmo Mochi.1 Preview](https://huggingface.co/genmo/mochi-1-preview)  \n  support for text-to-video, to use, select in *scripts -> mochi.1 video*  \n  basic support only  \n  *refrence values*: steps 64, width 848, height 480, frames 19, guidance scale 4.5  \n\n*Notes*:\n- all video models are very large and resource intensive!  \n  any use on gpus below 16gb and systems below 48gb ram is experimental at best  \n- sdnext support for video models is relatively basic with further optimizations pending community interest  \n  any future optimizations would likely have to go into partial loading and excecution instead of offloading inactive parts of the model  \n- new video models use generic llms for prompting and due to that requires very long and descriptive prompt  \n- you may need to enable sequential offload for maximum gpu memory savings  \n- optionally enable pre-quantization using bnb for additional memory savings  \n- reduce number of frames and/or resolution to reduce memory usage  \n\n### UI and workflow improvements\n\n- **Docs**:\n  - New documentation site! <https://vladmandic.github.io/sdnext-docs/>\n  - Additional Wiki content: Styles, Wildcards, etc.\n- **LoRA** handler rewrite:  \n  - LoRA weights are no longer calculated on-the-fly during model execution, but are pre-calculated at the start  \n    this results in perceived overhead on generate startup, but results in overall faster execution as LoRA does not need to be processed on each step  \n    thanks @AI-Casanova  \n  - LoRA weights can be applied/unapplied as on each generate or they can store weights backups for later use  \n    this setting has large performance and resource implications, see [Offload](https://github.com/vladmandic/automatic/wiki/Offload) wiki for details  \n  - LoRA name in prompt can now also be an absolute path to a LoRA file, even if LoRA is not indexed  \n    example: `<lora:/test/folder/my-lora.safetensors:1.0>`\n  - LoRA name in prompt can now also be path to a LoRA file op `huggingface`  \n    example: `<lora:/huggingface.co/vendor/repo/my-lora.safetensors:1.0>`\n- **Model loader** improvements:  \n  - detect model components on model load fail  \n  - allow passing absolute path to model loader  \n  - Flux, SD35: force unload model  \n  - Flux: apply `bnb` quant when loading *unet/transformer*  \n  - Flux: all-in-one safetensors  \n    example: <https://civitai.com/models/646328?modelVersionId=1040235>  \n  - Flux: do not recast quants  \n- **Memory** improvements:  \n  - faster and more compatible *balanced offload* mode  \n  - balanced offload: units are now in percentage instead of bytes  \n  - balanced offload: add both high and low watermark, defaults as below  \n    `0.25` for low-watermark: skip offload if memory usage is below 25%  \n    `0.70` high-watermark: must offload if memory usage is above 70%  \n  - balanced offload will attempt to run offload as non-blocking and force gc at the end  \n  - change-in-behavior:  \n    low-end systems, triggered by either `lowvrwam` or by detection of <=4GB will use *sequential offload*  \n    all other systems use *balanced offload* by default (can be changed in settings)  \n    previous behavior was to use *model offload* on systems with <=8GB and `medvram` and no offload by default  \n  - VAE upcase is now disabled by default on all systems  \n    if you have issues with image decode, you'll need to enable it manually  \n- **UI**:  \n  - improved stats on generate completion  \n  - improved live preview display and performance  \n  - improved accordion behavior  \n  - auto-size networks height for sidebar  \n  - control: hide preview column by default\n  - control: optionn to hide input column\n  - control: add stats\n  - settings: reorganized and simplified  \n  - browser -> server logging framework  \n  - add addtional themes: `black-reimagined`, thanks @Artheriax  \n- **Batch**\n  - image batch processing will use caption files if they exist instead of default prompt  \n\n### Updates\n\n- **Quantization**\n  - Add `TorchAO` *pre* (during load) and *post* (during execution) quantization  \n    **torchao** supports 4 different int-based and 3 float-based quantization schemes  \n  This is in addition to existing support for:  \n  - `BitsAndBytes` with 3 float-based quantization schemes  \n  - `Optimium.Quanto` with 3 int-based and 2 float-based quantizations schemes  \n  - `GGUF` with pre-quantized weights  \n  - Switch `GGUF` loader from custom to diffuser native\n- **IPEX**: update to IPEX 2.5.10+xpu  \n- **OpenVINO**:  \n  - update to 2024.6.0  \n  - disable model caching by default  \n- **Sampler** improvements  \n  - UniPC, DEIS, SA, DPM-Multistep: allow FlowMatch sigma method and prediction type  \n  - Euler FlowMatch: add sigma methods (*karras/exponential/betas*)  \n  - Euler FlowMatch: allow using timestep presets to set sigmas  \n  - DPM FlowMatch: update all and add sigma methods  \n  - BDIA-DDIM: *experimental* new scheduler  \n  - UFOGen: *experimental* new scheduler  \n\n### Fixes  \n\n- add `SD_NO_CACHE=true` env variable to disable file/folder caching  \n- add settings -> networks -> embeddings -> enable/disable\n- update `diffusers`  \n- fix README links  \n- fix sdxl controlnet single-file loader  \n- relax settings validator  \n- improve js progress calls resiliency  \n- fix text-to-video pipeline  \n- avoid live-preview if vae-decode is running  \n- allow xyz-grid with multi-axis s&r  \n- fix xyz-grid with lora  \n- fix api script callbacks  \n- fix gpu memory monitoring  \n- simplify img2img/inpaint/sketch canvas handling  \n- fix prompt caching  \n- fix xyz grid skip final pass  \n- fix sd upscale script  \n- fix cogvideox-i2v  \n- lora auto-apply tags remove duplicates  \n- control load model on-demand if not already loaded  \n- taesd limit render to 2024px  \n- taesd downscale preview to 1024px max: configurable in settings -> live preview  \n- uninstall conflicting `wandb` package  \n- dont skip diffusers version check if quick is specified  \n- notify on torch install  \n- detect pipeline fro diffusers folder-style model  \n- do not recast flux quants  \n- fix xyz-grid with lora none  \n- fix svd image2video  \n- fix gallery display during generate  \n- fix wildcards replacement to be unique  \n- fix animatediff-xl  \n- fix pag with batch count  \n\n## Update for 2024-11-21\n\n### Highlights for 2024-11-21\n\nThree weeks is a long time in Generative AI world - and we're back with ~140 commits worth of updates!\n\n*What's New?*\n\nFirst, a massive update to docs including new UI top-level **info** tab with access to [changelog](https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md) and [wiki](https://github.com/vladmandic/automatic/wiki), many updates and new articles AND full **built-in documentation search** capabilities\n\n#### New integrations\n\n- [PuLID](https://github.com/ToTheBeginning/PuLID): Pure and Lightning ID Customization via Contrastive Alignment\n- [InstantX InstantIR](https://github.com/instantX-research/InstantIR): Blind Image Restoration with Instant Generative Reference\n- [nVidia Labs ConsiStory](https://github.com/NVlabs/consistory): Consistent Image Generation\n- [MiaoshouAI PromptGen v2.0](https://huggingface.co/MiaoshouAI/Florence-2-base-PromptGen-v2.0) VQA captioning\n\n#### Workflow Improvements\n\n- Native **Docker** support\n- **SD3x & Flux.1**: more ControlNets, all-in-one-safetensors, DPM samplers, skip-layer-guidance, etc.\n- **XYZ grid**: benchmarking, video creation, etc.\n- Enhanced **prompt** parsing\n- **UI** improvements\n- **Installer** self-healing `venv`\n\nAnd quite a few more improvements and fixes since the last update!\nFor full list and details see changelog...\n\n[README](https://github.com/vladmandic/automatic/blob/master/README.md) | [CHANGELOG](https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md) | [WiKi](https://github.com/vladmandic/automatic/wiki) | [Discord](https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867)\n\n### Details for 2024-11-21\n\n- Docs:  \n  - new top-level **info** tab with access to [changelog](https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md) and [wiki](https://github.com/vladmandic/automatic/wiki)  \n  - UI built-in [changelog](https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md) search  \n    since changelog is the best up-to-date source of info  \n    go to info -> changelog and search/highligh/navigate directly in UI!  \n  - UI built-in [wiki](https://github.com/vladmandic/automatic/wiki)  \n    go to info -> wiki and search wiki pages directly in UI!  \n  - major [Wiki](https://github.com/vladmandic/automatic/wiki) and [Home](https://github.com/vladmandic/automatic) updates  \n  - updated API swagger docs for at `/docs`  \n- Integrations:  \n  - [PuLID](https://github.com/ToTheBeginning/PuLID): Pure and Lightning ID Customization via Contrastive Alignment  \n    - advanced method of face id transfer with better quality as well as control over identity and appearance  \n      try it out, likely the best quality available for sdxl models  \n    - select in *scripts -> pulid*  \n    - compatible with *sdxl* for text-to-image, image-to-image, inpaint, refine, detailer workflows  \n    - can be used in xyz grid  \n    - *note*: this module contains several advanced features on top of original implementation  \n  - [InstantIR](https://github.com/instantX-research/InstantIR): Blind Image Restoration with Instant Generative Reference  \n    - alternative to traditional `img2img` with more control over restoration process  \n    - select in *image -> scripts -> instantir*  \n    - compatible with *sdxl*  \n    - *note*: after used once it cannot be unloaded without reloading base model  \n  - [ConsiStory](https://github.com/NVlabs/consistory): Consistent Image Generation  \n    - create consistent anchor image and then generate images that are consistent with anchor  \n    - select in *scripts -> consistory*  \n    - compatible with *sdxl*  \n    - *note*: very resource intensive and not compatible with model offloading  \n    - *note*: changing default parameters can lead to unexpected results and/or failures  \n    - *note*: after used once it cannot be unloaded without reloading base model  \n  - [MiaoshouAI PromptGen v2.0](https://huggingface.co/MiaoshouAI/Florence-2-base-PromptGen-v2.0) base and large:  \n    - *in process -> visual query*  \n    - caption modes:  \n      `<GENERATE_TAGS>` generate tags  \n      `<CAPTION>`, `<DETAILED_CAPTION>`, `<MORE_DETAILED_CAPTION>` caption image  \n      `<ANALYZE>` image composition  \n      `<MIXED_CAPTION>`, `<MIXED_CAPTION_PLUS>` detailed caption and tags with optional analyze  \n\n- Model improvements:  \n  - SD35: **ControlNets**:  \n    - *InstantX Canny, Pose, Depth, Tile*  \n    - *Alimama Inpainting, SoftEdge*  \n    - *note*: that just like with FLUX.1 or any large model, ControlNet are also large and can push your system over the limit  \n      e.g. SD3 controlnets vary from 1GB to over 4GB in size  \n  - SD35: **All-in-one** safetensors  \n    - *examples*: [large](https://civitai.com/models/882666/sd35-large-google-flan?modelVersionId=1003031), [medium](https://civitai.com/models/900327)  \n    - *note*: enable *bnb* on-the-fly quantization for even bigger gains  \n  - SD35: **skip-layer-guidance**  \n    - enable in *scripts -> slg*\n    - allows for granular strength/start/stop control of guidance for each layer of the model  \n  - [NoobAI XL ControlNets](https://huggingface.co/collections/Eugeoter/controlnext-673161eae023f413e0432799), thanks @lbeltrame\n\n- Workflow improvements:  \n  - Native Docker support with pre-defined [Dockerfile](https://github.com/vladmandic/automatic/blob/dev/Dockerfile)\n  - Samplers:\n    - **FlowMatch samplers**:\n      - Applicable to SD 3.x and Flux.1 models\n      - Complete family: *DPM2, DPM2a, DPM2++, DPM2++ 2M, DPM2++ 2S, DPM2++ SDE, DPM2++ 2M SDE, DPM2++ 3M SDE*\n    - **Beta and Exponential** sigma method enabled for all samplers\n  - **XYZ grid**:  \n    - optional time benchmark info to individual images  \n    - optional add params to individual images  \n    - create video from generated grid images  \n      supports all standard video types and interpolation  \n  - **Prompt parser**:  \n    - support for prompt scheduling  \n    - renamed parser options: `native`, `xhinker`, `compel`, `a1111`, `fixed`  \n    - parser options are available in xyz grid  \n    - improved caching  \n  - **UI**:  \n    - better gallery and networks sidebar sizing  \n    - add additional [hotkeys](https://github.com/vladmandic/automatic/wiki/Hotkeys)  \n    - add show networks on startup setting  \n    - better mapping of networks previews  \n    - optimize networks display load  \n  - Image2image:  \n    - integrated refine/upscale/hires workflow  \n- Other:  \n  - **Installer**:  \n    - Log `venv` and package search paths  \n    - Auto-remove invalid packages from `venv/site-packages`  \n      e.g. packages starting with `~` which are left-over due to windows access violation  \n    - Requirements: update  \n  - Scripts:  \n    - More verbose descriptions for all scripts  \n  - Model loader:  \n    - Report modules included in safetensors when attempting to load a model  \n  - CLI:  \n    - refactor command line params  \n      run `webui.sh`/`webui.bat` with `--help` to see all options  \n    - added `cli/model-metadata.py` to display metadata in any safetensors file  \n    - added `cli/model-keys.py` to quicky display content of any safetensors file  \n  - Internal:  \n    - Auto pipeline switching coveres wrapper classes and nested pipelines  \n    - Full settings validation on load of `config.json`  \n    - Refactor of all params in main processing classes  \n    - Improve API scripts usage resiliency  \n\n- Fixes:  \n  - custom watermark add alphablending  \n  - fix xyz grid include images  \n  - fix xyz skip on interrupted  \n  - fix vqa models ignoring hfcache folder setting  \n  - fix network height in standard vs modern ui  \n  - fix k-diff enum on startup  \n  - fix text2video scripts  \n  - multiple xyz-grid fixes  \n  - dont uninstall flash-attn  \n  - ui css fixes  \n\n## Update for 2024-11-01\n\nSmaller release just 3 days after the last one, but with some important fixes and improvements.  \nThis release can be considered an LTS release before we kick off the next round of major updates.  \n\n- Other:\n  - Repo: move screenshots to GH pages\n  - Update requirements\n- Fixes:\n  - detailer min/max size as fractions of image size  \n  - ipadapter load on-demand  \n  - ipadapter face use correct yolo model  \n  - list diffusers remove duplicates  \n  - fix legacy extensions access to shared objects  \n  - fix diffusers load from folder  \n  - fix lora enum logging on windows  \n  - fix xyz grid with batch count  \n  - move dowwloads of some auxillary models to hfcache instead of models folder  \n\n## Update for 2024-10-29\n\n### Highlights for 2024-10-29\n\n- Support for **all SD3.x variants**  \n  *SD3.0-Medium, SD3.5-Medium, SD3.5-Large, SD3.0-Large-Turbo*\n- Allow quantization using `bitsandbytes` on-the-fly during models load\n  Load any variant of SD3.x or FLUX.1 and apply quantization during load without the need for pre-quantized models  \n- Allow for custom model URL in standard model selector  \n  Can be used to specify any model from *HuggingFace* or *CivitAI*  \n- Full support for `torch==2.5.1`\n- New wiki articles: [Gated Access](https://github.com/vladmandic/automatic/wiki/Gated), [Quantization](https://github.com/vladmandic/automatic/wiki/Quantization), [Offloading](https://github.com/vladmandic/automatic/wiki/Offload)  \n\nPlus tons of smaller improvements and cumulative fixes reported since last release  \n\n[README](https://github.com/vladmandic/automatic/blob/master/README.md) | [CHANGELOG](https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md) | [WiKi](https://github.com/vladmandic/automatic/wiki) | [Discord](https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867)\n\n### Details for 2024-10-29\n\n- model selector:\n  - change-in-behavior\n  - when typing, it will auto-load model as soon as exactly one match is found\n  - allows entering model that are not on the list which triggers huggingface search  \n    e.g. `stabilityai/stable-diffusion-xl-base-1.0`  \n    partial search hits are displayed in the log  \n    if exact model is found, it will be auto-downloaded and loaded  \n  - allows entering civitai direct download link which triggers model download  \n    e.g. `https://civitai.com/api/download/models/72396?type=Model&format=SafeTensor&size=full&fp=fp16`  \n  - auto-search-and-download can be disabled in settings -> models -> auto-download  \n    this also disables reference models as they are auto-downloaded on first use as well  \n- sd3 enhancements:  \n  - allow on-the-fly bnb quantization during load\n  - report when loading incomplete model  \n  - handle missing model components during load  \n  - handle component preloading  \n  - native lora handler  \n  - support for all sd35 variants: *medium/large/large-turbo*\n  - gguf transformer loader (prototype)  \n- flux.1 enhancements:  \n  - allow on-the-fly bnb quantization during load\n- samplers:\n  - support for original k-diffusion samplers  \n    select via *scripts -> k-diffusion -> sampler*  \n- ipadapter:\n  - list available adapters based on loaded model type\n  - add adapter `ostris consistency` for sd15/sdxl\n- detailer:\n  - add `[prompt]` to refine/defailer prompts as placeholder referencing original prompt  \n- torch\n  - use `torch==2.5.1` by default on supported platforms\n  - CUDA set device memory limit\n    in *settings -> compute settings -> torch memory limit*  \n    default=0 meaning no limit, if set torch will limit memory usage to specified fraction  \n    *note*: this is not a hard limit, torch will try to stay under this value  \n- compute backends:\n  - OpenVINO: add accuracy option  \n  - ZLUDA: guess GPU arch  \n- major model load refactor\n- wiki: new articles\n  - [Gated Access Wiki](https://github.com/vladmandic/automatic/wiki/Gated)  \n  - [Quantization Wiki](https://github.com/vladmandic/automatic/wiki/Quantization)  \n  - [Offloading Wiki](https://github.com/vladmandic/automatic/wiki/Offload)  \n\nfixes:  \n- fix send-to-control  \n- fix k-diffusion  \n- fix sd3 img2img and hires  \n- fix ipadapter supported model detection  \n- fix t2iadapter auto-download\n- fix omnigen dynamic attention  \n- handle a1111 prompt scheduling  \n- handle omnigen image placeholder in prompt  \n\n## Update for 2024-10-23\n\n### Highlights for 2024-10-23\n\nA month later and with nearly 300 commits, here is the latest [SD.Next](https://github.com/vladmandic/automatic) update!  \n\n#### Workflow highlights for 2024-10-23\n\n- **Reprocess**: New workflow options that allow you to generate at lower quality and then  \n  reprocess at higher quality for select images only or generate without hires/refine and then reprocess with hires/refine  \n  and you can pick any previous latent from auto-captured history!  \n- **Detailer** Fully built-in detailer workflow with support for all standard models  \n- Built-in **model analyzer**  \n  See all details of your currently loaded model, including components, parameter count, layer count, etc.  \n- **Extract LoRA**: load any LoRA(s) and play with generate as usual  \n  and once you like the results simply extract combined LoRA for future use!  \n\n#### New models for 2024-10-23\n\n- New fine-tuned [CLiP-ViT-L](https://huggingface.co/zer0int/CLIP-GmP-ViT-L-14) 1st stage **text-encoders** used by most models (SD15/SDXL/SD3/Flux/etc.) brings additional details to your images  \n- New models:  \n  [Stable Diffusion 3.5 Large](https://huggingface.co/stabilityai/stable-diffusion-3.5-large)  \n  [OmniGen](https://arxiv.org/pdf/2409.11340)  \n  [CogView 3 Plus](https://huggingface.co/THUDM/CogView3-Plus-3B)  \n  [Meissonic](https://github.com/viiika/Meissonic)  \n- Additional integration:  \n  [Ctrl+X](https://github.com/genforce/ctrl-x) which allows for control of **structure and appearance** without the need for extra models,  \n  [APG: Adaptive Projected Guidance](https://arxiv.org/pdf/2410.02416) for optimal **guidance** control,  \n  [LinFusion](https://github.com/Huage001/LinFusion) for on-the-fly **distillation** of any sd15/sdxl model  \n\n#### What else for 2024-10-23\n\n- Tons of work on **dynamic quantization** that can be applied *on-the-fly* during model load to any model type (*you do not need to use pre-quantized models*)  \n  Supported quantization engines include `BitsAndBytes`, `TorchAO`, `Optimum.quanto`, `NNCF` compression, and more...  \n- Auto-detection of best available **device/dtype** settings for your platform and GPU reduces neeed for manual configuration  \n  *Note*: This is a breaking change to default settings and its recommended to check your preferred settings after upgrade  \n- Full rewrite of **sampler options**, not far more streamlined with tons of new options to tweak scheduler behavior  \n- Improved **LoRA** detection and handling for all supported models  \n- Several of [Flux.1](https://huggingface.co/black-forest-labs/FLUX.1-dev) optimizations and new quantization types  \n\nOh, and we've compiled a full table with list of top-30 (*how many have you tried?*) popular text-to-image generative models,  \ntheir respective parameters and architecture overview: [Models Overview](https://github.com/vladmandic/automatic/wiki/Models)  \n\nAnd there are also other goodies like multiple *XYZ grid* improvements, additional *Flux ControlNets*, additional *Interrogate models*, better *LoRA tags* support, and more...  \n[README](https://github.com/vladmandic/automatic/blob/master/README.md) | [CHANGELOG](https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md) | [WiKi](https://github.com/vladmandic/automatic/wiki) | [Discord](https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867)\n\n### Details for 2024-10-23\n\n- **reprocess**\n  - new top-level button: reprocess latent from your history of generated image(s)  \n  - generate using full-quality:off and then reprocess using *full quality decode*  \n  - generate without hires/refine and then *reprocess with hires/refine*  \n    *note*: you can change hires/refine settings and run-reprocess again!  \n  - reprocess using *detailer*  \n\n- **history**\n  - by default, **reprocess** will pick last latent, but you can select any latent from history!  \n  - history is under *networks -> history*  \n    each history item includes info on operations that were used, timestamp and metadata  \n  - any latent operation during workflow automatically adds one or more items to history  \n    e.g. generate base + upscale + hires + detailer  \n  - history size: *settings -> execution -> latent history size*  \n    memory usage is ~130kb of ram for 1mp image  \n  - *note* list of latents in history is not auto-refreshed, use refresh button  \n\n- **model analyzer**  \n  - see all details of your currently loaded model, including components, parameter count, layer count, etc.  \n  - in models -> current -> analyze  \n\n- **text encoder**:  \n  - allow loading different custom text encoders: *clip-vit-l, clip-vit-g, t5*  \n    will automatically find appropriate encoder in the loaded model and replace it with loaded text encoder  \n    download text encoders into folder set in settings -> system paths -> text encoders  \n    default `models/Text-encoder` folder is used if no custom path is set  \n    finetuned *clip-vit-l* models: [Detailed, Smooth](https://huggingface.co/zer0int/CLIP-GmP-ViT-L-14), [LongCLIP](https://huggingface.co/zer0int/LongCLIP-GmP-ViT-L-14)  \n    reference *clip-vit-l* and *clip-vit-g* models: [OpenCLIP-Laion2b](https://huggingface.co/collections/laion/openclip-laion-2b-64fcade42d20ced4e9389b30)  \n    *note* sd/sdxl contain heavily distilled versions of reference models, so switching to reference model produces vastly different results  \n  - xyz grid support for text encoder  \n  - full prompt parser now correctly works with different prompts in batch  \n\n- **detailer**:  \n  - replaced *face-hires* with *detailer* which can run any number of standard detailing models  \n  - includes *face/hand/person/eyes* predefined detailer models plus support for manually downloaded models  \n    set path in *settings -> system paths -> yolo*  \n  - select one or more models in detailer menu and thats it!  \n  - to avoid duplication of ui elements, detailer will use following values from **refiner**:  \n    *sampler, steps, prompts*  \n  - when using multiple detailers and prompt is *multi-line*, each line is applied to corresponding detailer  \n  - adjustable settings:  \n    *strength, max detected objects, edge padding, edge blur, min detection confidence, max detection overlap, min and max size of detected object*  \n  - image metadata includes info on used detailer models  \n  - *note* detailer defaults are not save in ui settings, they are saved in server settings  \n    to apply your defaults, set ui values and apply via *system -> settings -> apply settings*  \n  - if using models trained on multiple classes, you can specify which classes you want to detail  \n    e.g. original yolo detection model is trained on coco dataset with 80 predefined classes  \n    if you leave field blank, it will use any class found in the model  \n    you can see classes defined in the model while model itself is loaded for the first time  \n\n- **extract lora**: extract combined lora from current memory state, thanks @AI-Casanova  \n  load any LoRA(s) and play with generate as usual and once you like the results simply extract combined LoRA for future use!  \n  in *models -> extract lora*  \n\n- **sampler options**: full rewrite  \n\n  *sampler notes*:  \n  - pick a sampler and then pick values, all values have \"default\" as a choice to make it simpler  \n  - a lot of options are new, some are old but moved around  \n    e.g. karras checkbox is replaced with a choice of different sigma methods  \n  - not every combination of settings is valid  \n  - some settings are specific to model types  \n    e.g. sd15/sdxl typically use epsilon prediction  \n  - quite a few well-known schedulers are just variations of settings, for example:  \n    - *sampler sgm* is sampler with trailing spacing and sample prediction type  \n    - *dpm 2m* or *3m* are *dpm 1s* with orders of 2 or 3  \n    - *dpm 2m sde* is *dpm 2m* with *sde* as solver  \n    - *sampler simple* is sampler with trailing spacing and linear beta schedule\n  - xyz grid support for sampler options  \n  - metadata updates for sampler options  \n  - modernui updates for sampler options  \n  - *note* sampler options defaults are not saved in ui settings, they are saved in server settings  \n    to apply your defaults, set ui values and apply via *system -> settings -> apply settings*  \n\n  *sampler options*:  \n  - sigma method: *karas, beta, exponential*  \n  - timesteps spacing: *linspace, leading, trailing*  \n  - beta schedule: *linear, scaled, cosine*  \n  - prediction type: *epsilon, sample, v-prediction*  \n  - timesteps presents: *none, ays-sd15, ays-sdxl*  \n  - timesteps override: <custom>  \n  - sampler order: *0=default, 1-5*  \n  - options: *dynamic, low order, rescale*  \n\n- [Ctrl+X](https://github.com/genforce/ctrl-x):\n  - control **structure** (*similar to controlnet*) and **appearance** (*similar to ipadapter*)  \n    without the need for extra models, all via code feed-forwards!\n  - can run in structure-only or appearance-only or both modes\n  - when providing structure and appearance input images, its best to provide a short prompts describing them  \n  - structure image can be *almost anything*: *actual photo, openpose-style stick man, 3d render, sketch, depth-map, etc.*  \n    just describe what it is in a structure prompt so it can be de-structured and correctly applied  \n  - supports sdxl in both txt2img and img2img, simply select from scripts\n\n- [APG: Adaptive Projected Guidance](https://arxiv.org/pdf/2410.02416)\n  - latest algo to provide better guidance for image generation, can be used instead of existing guidance rescale and/or PAG  \n  - in addtion to stronger guidance and reduction of burn at high guidance values, it can also increase image details  \n  - compatible with *sd15/sdxl/sc*  \n  - select in scripts -> apg  \n  - for low    cfg scale, use positive momentum: e.g. cfg=2 => momentum=0.6\n  - for normal cfg scale, use negative momentum: e.g. cfg=6 => momentum=-0.3\n  - for high   cfg scale, use neutral  momentum: e.g. cfg=10 => momentum=0.0\n\n- [LinFusion](https://github.com/Huage001/LinFusion)  \n  - apply liner distillation to during load to any sd15/sdxl model  \n  - can reduce vram use for high resolutions and increase performance\n  - *note*: use lower cfg scales as typical for distilled models  \n\n- [Flux](https://huggingface.co/black-forest-labs/FLUX.1-dev)  \n  - see [wiki](https://github.com/vladmandic/automatic/wiki/FLUX#quantization) for details on `gguf`  \n  - support for `gguf` binary format for loading unet/transformer component  \n  - support for `gguf` binary format for loading t5/text-encoder component: requires transformers pr  \n  - additional controlnets: [JasperAI](https://huggingface.co/collections/jasperai/flux1-dev-controlnets-66f27f9459d760dcafa32e08) **Depth**, **Upscaler**, **Surface**, thanks @EnragedAntelope  \n  - additional controlnets: [XLabs-AI](https://huggingface.co/XLabs-AI/flux-controlnet-hed-diffusers) **Canny**, **Depth**, **HED**  \n  - mark specific unet as unavailable if load failed  \n  - fix diffusers local model name parsing  \n  - full prompt parser will auto-select `xhinker` for flux models  \n  - controlnet support for img2img and inpaint (in addition to previous txt2img controlnet)  \n  - allow separate vae load  \n  - support for both kohya and onetrainer loras in native load mode for fp16/nf4/fp4, thanks @AI-Casanova  \n  - support for differential diffusion  \n  - added native load mode for qint8/qint4 models\n  - avoid unet load if unchanged  \n\n- [OmniGen](https://arxiv.org/pdf/2409.11340)  \n  - Radical new model with pure LLM architecture based on Phi-3  \n  - Select from *networks -> models -> reference*  \n  - Can be used for text-to-image and image-to-image  \n  - Image-to-image is *very* different, you need to specify in prompt what do you want to do  \n    and add `|image|` placeholder where input image is used!  \n    examples: `in |image| remove glasses from face`, `using depth map from |image|, create new image of a cute robot`  \n  - Params used: prompt, steps, guidance scale for prompt guidance, refine guidance scale for image guidance  \n    Recommended: guidance=3.0, refine-guidance=1.6  \n\n- [Stable Diffusion 3.5 Large](https://huggingface.co/stabilityai/stable-diffusion-3.5-large)  \n  - New/improved variant of Stable Diffusion 3  \n  - Select from *networks -> models -> reference*  \n  - Available in standard and turbo variations  \n  - *Note*: Access to to both variations of SD3.5 model is gated, you must accept the conditions and use HF login  \n\n- [CogView 3 Plus](https://huggingface.co/THUDM/CogView3-Plus-3B)\n  - Select from *networks -> models -> reference*  \n  - resolution width and height can be from 512px to 2048px and must be divisible by 32  \n  - precision: bf16 or fp32  \n    fp16 is not supported due to internal model overflows  \n\n- [Meissonic](https://github.com/viiika/Meissonic)  \n  - Select from *networks -> models -> reference*  \n  - Experimental as upstream implemenation code is unstable\n  - Must set scheduler:default, generator:unset\n\n- [SageAttention](https://github.com/thu-ml/SageAttention)  \n  - new 8-bit attention implementation on top of SDP that can provide acceleration for some models, thanks @Disty0  \n  - enable in *settings -> compute settings -> sdp options -> sage attention*\n  - compatible with DiT-based models: e.g. *Flux.1, AuraFlow, CogVideoX*  \n  - not compatible with UNet-based models, e.g. *SD15, SDXL*  \n\n- **gpu**\n  - previously `cuda_dtype` in settings defaulted to `fp16` if available  \n  - now `cuda_type` defaults to **Auto** which executes `bf16` and `fp16` tests on startup and selects best available dtype  \n    if you have specific requirements, you can still set to fp32/fp16/bf16 as desired  \n    if you have gpu that incorrectly identifies bf16 or fp16 availablity, let us know so we can improve the auto-detection  \n  - support for torch **expandable segments**  \n    enable in *settings -> compute -> torch expandable segments*  \n    can provide significant memory savings for some models  \n    not enabled by default as its only supported on latest versions of torch and some gpus  \n\n- **xyz grid** full refactor  \n  - multi-mode: *selectable-script* and *alwayson-script*  \n  - allow usage combined with other scripts  \n  - allow **unet** selection  \n  - allow passing **model args** directly:  \n    allowed params will be checked against models call signature  \n    example: `width=768; height=512, width=512; height=768`  \n  - allow passing **processing args** directly:  \n    params are set directly on main processing object and can be known or new params  \n    example: `steps=10, steps=20; test=unknown`  \n  - enable working with different resolutions  \n    now you can adjust width/height in the grid just as any other param  \n  - renamed options to include section name and adjusted cost of each option  \n  - added additional metadata  \n\n- **interrogate**  \n  - add additional blip models: *blip-base, blip-large, blip-t5-xl, blip-t5-xxl, opt-2.7b, opt-6.7b*  \n  - change default params for better memory utilization  \n  - lock commits for miaoshouAI-promptgen  \n  - add optional advanced params  \n  - update logging  \n\n- **lora** auto-apply tags to prompt  \n  - controlled via *settings -> networks -> lora_apply_tags*  \n    *0:disable, -1:all-tags, n:top-n-tags*  \n  - uses tags from both model embedded data and civitai downloaded data  \n  - if lora contains no tags, lora name itself will be used as a tag  \n  - if prompt contains `_tags_` it will be used as placeholder for replacement, otherwise tags will be appended  \n  - used tags are also logged and registered in image metadata  \n  - loras are no longer filtered per detected type vs loaded model type as its unreliable  \n  - loras display in networks now shows possible version in top-left corner  \n  - correct using of `extra_networks_default_multiplier` if not scale is specified  \n  - improve lora base model detection  \n  - improve lora error handling and logging  \n  - setting `lora_load_gpu` to load LoRA directly to GPU  \n    *default*: true unless lovwram  \n\n- **quantization**  \n  - new top level settings group as we have quite a few quantization options now!  \n    configure in *settings -> quantization*  \n  - in addition to existing `optimum.quanto` and `nncf`, we now have `bitsandbytes` and `torchao`  \n  - **bitsandbytes**: fp8, fp4, nf4  \n    - quantization can be applied on-the-fly during model load  \n    - currently supports `transformers` and `t5` in **sd3** and **flux**  \n  - **torchao**: int8, int4, fp8, fp4, fpx  \n    - configure in settings -> quantization  \n    - can be applied to any model on-the-fly during load  \n\n- **huggingface**:  \n  - force logout/login on token change  \n  - unified handling of cache folder: set via `HF_HUB` or `HF_HUB_CACHE` or via settings -> system paths  \n\n- **cogvideox**:  \n  - add support for *image2video* (in addition to previous *text2video* and *video2video*)  \n  - *note*: *image2video* requires separate 5b model variant  \n\n- **torch**  \n  - due to numerous issues with torch 2.5.0 which was just released as stable, we are sticking with 2.4.1 for now  \n\n- **backend=original** is now marked as in maintenance-only mode  \n- **python 3.12** improved compatibility, automatically handle `setuptools`  \n- **control**\n  - persist/reapply units current state on server restart  \n  - better handle size before/after metadata  \n- **video** add option `gradio_skip_video` to avoid gradio issues with displaying generated videos  \n- add support for manually downloaded diffusers models from huggingface  \n- **ui**  \n  - move checkboxes `full quality, tiling, hidiffusion` to advanced section  \n  - hide token counter until tokens are known  \n  - minor ui optimizations  \n  - fix update infotext on image select  \n  - fix imageviewer exif parser  \n  - selectable info view in image viewer, thanks @ZeldaMaster501  \n  - setting to enable browser autolaunch, thanks @brknsoul  \n- **free-u** check if device/dtype are fft compatible and cast as necessary  \n- **rocm**\n  - additional gpu detection and auto-config code, thanks @lshqqytiger  \n  - experimental triton backend for flash attention, thanks @lshqqytiger  \n  - update to rocm 6.2, thanks @Disty0\n- **directml**  \n  - update `torch` to 2.4.1, thanks @lshqqytiger  \n- **extensions**  \n  - add mechanism to lock-down extension to specific working commit  \n  - added `sd-webui-controlnet` and `adetailer` last-known working commits  \n- **upscaling**  \n  - interruptible operations\n- **refactor**  \n  - general lora apply/unapply process  \n  - modularize main process loop  \n  - massive log cleanup  \n  - full lint pass  \n  - improve inference mode handling  \n  - unify quant lib loading  \n\n\n## Update for 2024-09-13\n\n### Highlights for 2024-09-13\n\nMajor refactor of [FLUX.1](https://blackforestlabs.ai/announcing-black-forest-labs/) support:  \n- Full **ControlNet** support, better **LoRA** support, full **prompt attention** implementation  \n- Faster execution, more flexible loading, additional quantization options, and more...  \n- Added **image-to-image**, **inpaint**, **outpaint**, **hires** modes  \n- Added workflow where FLUX can be used as **refiner** for other models  \n- Since both *Optimum-Quanto* and *BitsAndBytes* libraries are limited in their platform support matrix,  \n  try enabling **NNCF** for quantization/compression on-the-fly!  \n\nFew image related goodies...  \n- **Context-aware** resize that allows for *img2img/inpaint* even at massively different aspect ratios without distortions!\n- **LUT Color grading** apply professional color grading to your images using industry-standard *.cube* LUTs!\n- Auto **HDR** image create for SD and SDXL with both 16ch true-HDR and 8-ch HDR-effect images ;)  \n\nAnd few video related goodies...  \n- [CogVideoX](https://huggingface.co/THUDM/CogVideoX-5b) **2b** and **5b** variants  \n  with support for *text-to-video* and *video-to-video*!  \n- [AnimateDiff](https://github.com/guoyww/animatediff/) **prompt travel** and **long context windows**!  \n  create video which travels between different prompts and at long video lengths!  \n\nPlus tons of other items and fixes - see [changelog](https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md) for details!  \nExamples:\n- Built-in prompt-enhancer, TAESD optimizations, new DC-Solver scheduler, global XYZ grid management, etc.  \n- Updates to ZLUDA, IPEX, OpenVINO...\n\n### Details for 2024-09-13\n\n**Major refactor of FLUX.1 support:**\n- allow configuration of individual FLUX.1 model components: *transformer, text-encoder, vae*  \n  model load will load selected components first and then initialize model using pre-loaded components  \n  components that were not pre-loaded will be downloaded and initialized as needed  \n  as usual, components can also be loaded after initial model load  \n  *note*: use of transformer/unet is recommended as those are flux.1 finetunes  \n  *note*: manually selecting vae and text-encoder is not recommended  \n  *note*: mix-and-match of different quantizations for different components can lead to unexpected errors  \n  - transformer/unet is list of manually downloaded safetensors  \n  - vae is list of manually downloaded safetensors  \n  - text-encoder is list of predefined and manually downloaded text-encoders  \n- **controlnet** support:\n  support for **InstantX/Shakker-Labs** models including [Union-Pro](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Union)\n  note that flux controlnet models are large, up to 6.6GB on top of already large base model!  \n  as such, you may need to use offloading:sequential which is not as fast, but uses far less memory  \n  when using union model, you must also select control mode in the control unit  \n  flux does not yet support *img2img* so to use controlnet, you need to set contronet input via control unit override  \n- model support loading **all-in-one** safetensors  \n  not recommended due to massive duplication of components, but added due to popular demand  \n  each such model is 20-32GB in size vs ~11GB for typical unet fine-tune  \n- improve logging, warn when attempting to load unet as base model  \n- **refiner** support  \n  FLUX.1 can be used as refiner for other models such as sd/sdxl  \n  simply load sd/sdxl model as base and flux model as refiner and use as usual refiner workflow  \n- **img2img**, **inpaint** and **outpaint** support  \n  *note* flux may require higher denoising strength than typical sd/sdxl models  \n  *note*: img2img is not yet supported with controlnet  \n- transformer/unet support *fp8/fp4* quantization  \n  this brings supported quants to: *nf4/fp8/fp4/qint8/qint4*\n- vae support *fp16*  \n- **lora** support additional training tools  \n- **face-hires** support  \n- support **fuse-qkv** projections  \n  can speed up generate  \n  enable via *settings -> compute -> fused projections*  \n\n**Other improvements & Fixes:**\n- [CogVideoX](https://huggingface.co/THUDM/CogVideoX-5b)  \n  - support for both **2B** and **5B** variations  \n  - support for both **text2video** and **video2video** modes\n  - simply select in *scripts -> cogvideox*  \n  - as with any video modules, includes additional frame interpolation using RIFE  \n  - if init video is used, it will be automatically resized and interpolated to desired number of frames  \n- **AnimateDiff**:  \n  - **prompt travel**  \n     create video which travels between different prompts at different steps!  \n     example prompt:\n      > 0: dog  \n      > 5: cat  \n      > 10: bird  \n  - support for **v3** model (finally)  \n  - support for **LCM** model  \n  - support for **free-noise** rolling context window  \n    allow for creation of much longer videos, automatically enabled if frames > 16  \n- **Context-aware** image resize, thanks @AI-Casanova!  \n  based on [seam-carving](https://github.com/li-plus/seam-carving)  \n  allows for *img2img/inpaint* even at massively different aspect ratios without distortions!  \n  simply select as resize method when using *img2img* or *control* tabs  \n- **HDR** high-dynamic-range image create for SD and SDXL  \n  create hdr images from in multiple exposures by latent-space modifications during generation  \n  use via *scripts -> hdr*  \n  option *save hdr images* creates images in standard 8bit/channel (hdr-effect) *and* 16bit/channel (full-hdr) PNG format  \n  ui result is always 8bit/channel hdr-effect image plus grid of original images used to create hdr  \n  grid image can be disabled via settings -> user interface -> show grid  \n  actual full-hdr image is not displayed in ui, only optionally saved to disk  \n- new scheduler: [DC Solver](https://github.com/wl-zhao/DC-Solver)  \n- **color grading** apply professional color grading to your images  \n  using industry-standard *.cube* LUTs!\n  enable via *scripts -> color-grading*  \n- **hires** workflow now allows for full resize options  \n  not just limited width/height/scale  \n- **xyz grid** is now availabe as both local and global script!\n- **prompt enhance**: improve quality and/or verbosity of your prompts  \n  simply select in *scripts -> prompt enhance*\n  uses [gokaygokay/Flux-Prompt-Enhance](https://huggingface.co/gokaygokay/Flux-Prompt-Enhance) model  \n- **decode**\n  - auto-set upcast if first decode fails  \n  - restore dtype on upcast  \n- **taesd** configurable number of layers  \n  can be used to speed-up taesd decoding by reducing number of ops  \n  e.g. if generating 1024px image, reducing layers by 1 will result in preview being 512px  \n  set via *settings -> live preview -> taesd decode layers*  \n- **xhinker** prompt parser handle offloaded models  \n- **control** better handle offloading  \n- **upscale** will use resize-to if set to non-zero values over resize-by  \n  applies to any upscale options, including refine workflow  \n- **networks** add option to choose if mouse-over on network should attempt to fetch additional info  \n  option:`extra_networks_fetch` enable/disable in *settings -> networks*  \n- speed up some garbage collection ops  \n- sampler settings add **dynamic shift**  \n  used by flow-matching samplers to adjust between structure and details  \n- sampler settings force base shift  \n  improves quality of the flow-matching samplers  \n- **t5** support manually downloaded models  \n  applies to all models that use t5 transformer  \n- **modern-ui** add override field  \n- full **lint** updates  \n- use `diffusers` from main branch, no longer tied to release  \n- improve diffusers/transformers/huggingface_hub progress reporting  \n- use unique identifiers for all ui components  \n- **visual query** (a.ka vqa or vlm) added support for several models\n  - [MiaoshouAI PromptGen 1.5 Base](https://huggingface.co/MiaoshouAI/Florence-2-base-PromptGen-v1.5)\n  - [MiaoshouAI PromptGen 1.5 Large](https://huggingface.co/MiaoshouAI/Florence-2-large-PromptGen-v1.5)\n  - [CogFlorence 2.2 Large](https://huggingface.co/thwri/CogFlorence-2.2-Large)\n- **modernui** update  \n- **zluda** update to 3.8.4, thanks @lshqqytiger!\n- **ipex** update to 2.3.110+xpu on linux, thanks @Disty0!\n- **openvino** update to 2024.3.0, thanks @Disty0!\n- update `requirements`\n- fix **AuraFlow**  \n- fix handling of model configs if offline config is not available  \n- fix vae decode in backend original  \n- fix model path typos  \n- fix guidance end handler  \n- fix script sorting  \n- fix vae dtype during load  \n- fix all ui labels are unique\n\n## Update for 2024-08-31\n\n### Highlights for 2024-08-31\n\nSummer break is over and we are back with a massive update!  \n\nSupport for all of the new models:  \n- [Black Forest Labs FLUX.1](https://blackforestlabs.ai/announcing-black-forest-labs/)  \n- [AuraFlow 0.3](https://huggingface.co/fal/AuraFlow)  \n- [AlphaVLLM Lumina-Next-SFT](https://huggingface.co/Alpha-VLLM/Lumina-Next-SFT-diffusers)  \n- [Kwai Kolors](https://huggingface.co/Kwai-Kolors/Kolors)  \n- [HunyuanDiT 1.2](https://huggingface.co/Tencent-Hunyuan/HunyuanDiT-v1.2-Diffusers)  \n\nWhat else? Just a bit... ;)  \n\nNew **fast-install** mode, new **Optimum Quanto** and **BitsAndBytes** based quantization modes, new **balanced offload** mode that dynamically offloads GPU<->CPU as needed, and more...  \nAnd from previous service-pack: new **ControlNet-Union** *all-in-one* model, support for **DoRA** networks, additional **VLM** models, new **AuraSR** upscaler  \n\n**Breaking Changes...**\n\nDue to internal changes, you'll need to reset your **attention** and **offload** settings!  \nBut...For a good reason, new *balanced offload* is magic when it comes to memory utilization while sacrificing minimal performance!\n\n### Details for 2024-08-31\n\n**New Models...**\n\nTo use and of the new models, simply select model from *Networks -> Reference* and it will be auto-downloaded on first use  \n\n- [Black Forest Labs FLUX.1](https://blackforestlabs.ai/announcing-black-forest-labs/)  \n  FLUX.1 models are based on a hybrid architecture of multimodal and parallel diffusion transformer blocks, scaled to 12B parameters and builing on flow matching  \n  This is a very large model at ~32GB in size, its recommended to use a) offloading, b) quantization  \n  For more information on variations, requirements, options, and how to donwload and use FLUX.1, see [Wiki](https://github.com/vladmandic/automatic/wiki/FLUX)  \n  SD.Next supports:  \n  - [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) and [FLUX.1 Schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell) original variations  \n  - additional [qint8](https://huggingface.co/Disty0/FLUX.1-dev-qint8) and [qint4](https://huggingface.co/Disty0/FLUX.1-dev-qint4) quantized variations  \n  - additional [nf4](https://huggingface.co/sayakpaul/flux.1-dev-nf4) quantized variation  \n- [AuraFlow](https://huggingface.co/fal/AuraFlow)  \n  AuraFlow v0.3 is the fully open-sourced largest flow-based text-to-image generation model  \n  This is a very large model at 6.8B params and nearly 31GB in size, smaller variants are expected in the future  \n  Use scheduler: Default or Euler FlowMatch or Heun FlowMatch  \n- [AlphaVLLM Lumina-Next-SFT](https://huggingface.co/Alpha-VLLM/Lumina-Next-SFT-diffusers)  \n  Lumina-Next-SFT is a Next-DiT model containing 2B parameters, enhanced through high-quality supervised fine-tuning (SFT)  \n  This model uses T5 XXL variation of text encoder (previous version of Lumina used Gemma 2B as text encoder)  \n  Use scheduler: Default or Euler FlowMatch or Heun FlowMatch  \n- [Kwai Kolors](https://huggingface.co/Kwai-Kolors/Kolors)  \n  Kolors is a large-scale text-to-image generation model based on latent diffusion  \n  This is an SDXL style model that replaces standard CLiP-L and CLiP-G text encoders with a massive `chatglm3-6b` encoder supporting both English and Chinese prompting  \n- [HunyuanDiT 1.2](https://huggingface.co/Tencent-Hunyuan/HunyuanDiT-v1.2-Diffusers)  \n  Hunyuan-DiT is a powerful multi-resolution diffusion transformer (DiT) with fine-grained Chinese understanding  \n- [AnimateDiff](https://github.com/guoyww/animatediff/)  \n  support for additional models: **SD 1.5 v3** (Sparse), **SD Lightning** (4-step), **SDXL Beta**  \n\n**New Features...**\n\n- support for **Balanced Offload**, thanks @Disty0!  \n  balanced offload will dynamically split and offload models from the GPU based on the max configured GPU and CPU memory size  \n  model parts that dont fit in the GPU will be dynamically sliced and offloaded to the CPU  \n  see *Settings -> Diffusers Settings -> Max GPU memory and Max CPU memory*  \n  *note*: recommended value for max GPU memory is ~80% of your total GPU memory  \n  *note*: balanced offload will force loading LoRA with Diffusers method  \n  *note*: balanced offload is not compatible with Optimum Quanto  \n- support for **Optimum Quanto** with 8 bit and 4 bit quantization options, thanks @Disty0 and @Trojaner!  \n  to use, go to Settings -> Compute Settings and enable \"Quantize Model weights with Optimum Quanto\" option  \n  *note*: Optimum Quanto requires PyTorch 2.4  \n- new prompt attention mode: **xhinker** which brings support for prompt attention to new models such as FLUX.1 and SD3  \n  to use, enable in *Settings -> Execution -> Prompt attention*\n- use [PEFT](https://huggingface.co/docs/peft/main/en/index) for **LoRA** handling on all models other than SD15/SD21/SDXL  \n  this improves LoRA compatibility for SC, SD3, AuraFlow, Flux, etc.  \n\n**Changes & Fixes...**\n\n- default resolution bumped from 512x512 to 1024x1024, time to move on ;)\n- convert **Dynamic Attention SDP** into a global SDP option, thanks @Disty0!  \n  *note*: requires reset of selected attention option\n- update default **CUDA** version from 12.1 to 12.4\n- update `requirements`\n- samplers now prefers the model defaults over the diffusers defaults, thanks @Disty0!  \n- improve xyz grid for lora handling and add lora strength option  \n- don't enable Dynamic Attention by default on platforms that support Flash Attention, thanks @Disty0!  \n- convert offload options into a single choice list, thanks @Disty0!  \n  *note*: requires reset of selected offload option  \n- control module allows reszing of indivudual process override images to match input image  \n  for example: set size->before->method:nearest, mode:fixed or mode:fill  \n- control tab includes superset of txt and img scripts\n- automatically offload disabled controlnet units  \n- prioritize specified backend if `--use-*` option is used, thanks @lshqqytiger\n- ipadapter option to auto-crop input images to faces to improve efficiency of face-transfter ipadapters  \n- update **IPEX** to 2.1.40+xpu on Linux, thanks @Disty0!  \n- general **ROCm** fixes, thanks @lshqqytiger!  \n- support for HIP SDK 6.1 on ZLUDA backend, thanks @lshqqytiger!\n- fix full vae previews, thanks @Disty0!  \n- fix default scheduler not being applied, thanks @Disty0!  \n- fix Stable Cascade with custom schedulers, thanks @Disty0!  \n- fix LoRA apply with force-diffusers\n- fix LoRA scales with force-diffusers\n- fix control API\n- fix VAE load refrerencing incorrect configuration\n- fix NVML gpu monitoring\n\n## Update for 2024-07-08\n\nThis release is primary service release with cumulative fixes and several improvements, but no breaking changes.\n\n**New features...**\n- massive updates to [Wiki](https://github.com/vladmandic/automatic/wiki)  \n  with over 20 new pages and articles, now includes guides for nearly all major features  \n  *note*: this is work-in-progress, if you have any feedback or suggestions, please let us know!\n  thanks @GenesisArtemis!  \n- support for **DoRA** networks, thanks @AI-Casanova!\n- support for [uv](https://pypi.org/project/uv/), extremely fast installer, thanks @Yoinky3000!  \n  to use, simply add `--uv` to your command line params  \n- [Xinsir ControlNet++ Union](https://huggingface.co/xinsir/controlnet-union-sdxl-1.0)  \n  new SDXL *all-in-one* controlnet that can process any kind of preprocessors!\n- [CogFlorence 2 Large](https://huggingface.co/thwri/CogFlorence-2-Large-Freeze) VLM model  \n  to use, simply select in process -> visual query  \n- [AuraSR](https://huggingface.co/fal/AuraSR) high-quality 4x GAN-style upscaling model  \n  note: this is a large upscaler at 2.5GB  \n\n**And fixes...**\n- enable **Florence VLM**  for all platforms, thanks @lshqqytiger!  \n- improve ROCm detection under WSL2, thanks @lshqqytiger!  \n- add SD3 with FP16 T5 to list of detected models  \n- fix executing extensions with zero params  \n- add support for embeddings bundled in LoRA, thanks @AI-Casanova!  \n- fix executing extensions with zero params  \n- fix nncf for lora, thanks @Disty0!  \n- fix diffusers version detection for SD3  \n- fix current step for higher order samplers  \n- fix control input type video  \n- fix reset pipeline at the end of each iteration  \n- fix faceswap when no faces detected  \n- fix civitai search\n- multiple ModernUI fixes\n\n## Update for 2024-06-23\n\n### Highlights for 2024-06-23\n\nFollowing zero-day **SD3** release, a 10 days later heres a refresh with 10+ improvements  \nincluding full prompt attention, support for compressed weights, additional text-encoder quantization modes.  \n\nBut theres more than SD3:  \n- support for quantized **T5** text encoder *FP16/FP8/FP4/INT8* in all models that use T5: SD3, PixArt-, etc.  \n- support for **PixArt-Sigma** in small/medium/large variants  \n- support for **HunyuanDiT 1.1**  \n- additional **NNCF weights compression** support: SD3, PixArt, ControlNet, Lora  \n- integration of **MS Florence** VLM/VQA *Base* and *Large* models  \n- (finally) new release of **Torch-DirectML**  \n- additional efficiencies for users with low VRAM GPUs  \n- over 20 overall fixes  \n\n### Model Improvements for 2024-06-23\n\n- **SD3**: enable tiny-VAE (TAESD) preview and non-full quality mode  \n- SD3: enable base LoRA support  \n- SD3: add support for FP4 quantized T5 text encoder  \n  simply select in *settings -> model -> text encoder*  \n  *note* for SD3 with T5, set SD.Next to use FP16 precision, not BF16 precision  \n- SD3: add support for INT8 quantized T5 text encoder, thanks @Disty0!  \n- SD3: enable cpu-offloading for T5 text encoder, thanks @Disty0!  \n- SD3: simplified loading of model in single-file safetensors format  \n  model load can now be performed fully offline  \n- SD3: full support for prompt parsing and attention, thanks @AI-Casanova!\n- SD3: ability to target different prompts to each of text-encoders, thanks @AI-Casanova!  \n  example: `dog TE2: cat TE3: bird`\n- SD3: add support for sampler shift for Euler FlowMatch  \n  see *settings -> samplers*, also available as param in xyz grid  \n  higher shift means model will spend more time on structure and less on details  \n- SD3: add support for selecting T5 text encoder variant in XYZ grid\n- **Pixart-**: Add *small* (512px) and *large* (2k) variations, in addition to existing *medium* (1k)  \n- Pixart-: Add support for 4/8bit quantized t5 text encoder  \n  *note* by default pixart- uses full fp16 t5 encoder with large memory footprint  \n  simply select in *settings -> model -> text encoder* before or after model load  \n- **HunyuanDiT**: support for model version 1.1  \n- **MS Florence**: integration of Microsoft Florence VLM/VQA Base and Large models  \n  simply select in *process -> visual query*!\n\n### General Improvements for 2024-06-23\n\n- support FP4 quantized T5 text encoder, in addition to existing FP8 and FP16\n- support for T5 text-encoder loader in **all** models that use T5  \n  *example*: load FP4 or FP8 quantized T5 text-encoder into PixArt Sigma!\n- support for `torch-directml` **0.2.2**, thanks @lshqqytiger!  \n  *note*: new directml is finally based on modern `torch` 2.3.1!  \n- xyz grid: add support for LoRA selector\n- vae load: store original vae so it can be restored when set to none\n- extra networks: info display now contains link to source url if model if its known  \n  works for civitai and huggingface models  \n- force gc for lowvram users and improve gc logging\n- improved google.colab support\n- css tweaks for standardui\n- css tweaks for modernui\n- additional torch gc checks, thanks @Disty0!\n\n**Improvements: NNCF**, thanks @Disty0!  \n- SD3 and PixArt support  \n- moved the first compression step to CPU  \n- sequential cpu offload (lowvram) support  \n- Lora support without reloading the model  \n- ControlNet compression support  \n\n### Fixes for 2024-06-23\n\n- fix unsaturated outputs, force apply vae config on model load  \n- fix hidiffusion handling of non-square aspect ratios, thanks @ShenZhang-Shin!\n- fix control second pass resize  \n- fix hunyuandit set attention processor\n- fix civitai download without name\n- fix compatibility with latest adetailer\n- fix invalid sampler warning\n- fix starting from non git repo\n- fix control api negative prompt handling\n- fix saving style without name provided\n- fix t2i-color adapter\n- fix sdxl \"has been incorrectly initialized\"\n- fix api face-hires\n- fix api ip-adapter\n- fix memory exceptions with ROCm, thanks @Disty0!\n- fix face-hires with lowvram, thanks @Disty0!\n- fix pag incorrectly resetting pipeline\n- cleanup image metadata\n- restructure api examples: `cli/api-*`\n- handle theme fallback when invalid theme is specified\n- remove obsolete training code leftovers\n\n## Update for 2024-06-13\n\n### Highlights for 2024-06-13\n\nFirst, yes, it is here and supported: [**StabilityAI Stable Diffusion 3 Medium**](https://stability.ai/news/stable-diffusion-3-medium)  \nfor details on how to download and use, see [Wiki](https://github.com/vladmandic/automatic/wiki/SD3)\n\n#### What else 2024-06-13?\n\nA lot of work on state-of-the-art multi-lingual models with both [Tenecent HunyuanDiT](https://github.com/Tencent/HunyuanDiT) and [MuLan](https://github.com/mulanai/MuLan)  \nPlus tons of minor features such as optimized initial install experience, **T-Gate** and **ResAdapter**, additional ModernUI themes (both light and dark) and fixes since the last release which was only 2 weeks ago!\n\n### Full Changelog for 2024-06-13\n\n#### New Models for 2024-06-23\n\n- [StabilityAI Stable Diffusion 3 Medium](https://stability.ai/news/stable-diffusion-3-medium)  \n  yup, supported!  \n  quote: *\"Stable Diffusion 3 Medium is a multimodal diffusion transformer (MMDiT) model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency\"*  \n  sdnext also supports switching optional T5 text encoder on-the-fly as well as loading model from either diffusers repo or safetensors single-file  \n  for details, see [Wiki](https://github.com/vladmandic/automatic/wiki/SD3)\n- [Tenecent HunyuanDiT](https://github.com/Tencent/HunyuanDiT) bilingual english/chinese diffusion transformer model  \n  note: this is a very large model at ~17GB, but can be used with less VRAM using model offloading  \n  simply select from networks -> models -> reference, model will be auto-downloaded on first use  \n\n#### New Functionality for 2024-06-23\n\n- [MuLan](https://github.com/mulanai/MuLan) Multi-language prompts\n  write your prompts in ~110 auto-detected languages!  \n  compatible with *SD15* and *SDXL*  \n  enable in scripts -> MuLan and set encoder to `InternVL-14B-224px` encoder  \n  *note*: right now this is more of a proof-of-concept before smaller and/or quantized models are released  \n  model will be auto-downloaded on first use: note its huge size of 27GB  \n  even executing it in FP16 will require ~16GB of VRAM for text encoder alone  \n  examples:  \n  - English: photo of a beautiful woman wearing a white bikini on a beach with a city skyline in the background\n  - Croatian: fotografija lijepe ene u bijelom bikiniju na plai s gradskim obzorom u pozadini\n  - Italian: Foto di una bella donna che indossa un bikini bianco su una spiaggia con lo skyline di una citt sullo sfondo\n  - Spanish: Foto de una hermosa mujer con un bikini blanco en una playa con un horizonte de la ciudad en el fondo\n  - German: Foto einer schnen Frau in einem weien Bikini an einem Strand mit einer Skyline der Stadt im Hintergrund\n  - Arabic:             \n  - Japanese: \n  - Chinese: , \n  - Korean:           \n- [T-Gate](https://github.com/HaozheLiu-ST/T-GATE) Speed up generations by gating at which step cross-attention is no longer needed  \n  enable via scripts -> t-gate  \n  compatible with *SD15*  \n- **PCM LoRAs** allow for fast denoising using less steps with standard *SD15* and *SDXL* models  \n  download from <https://huggingface.co/Kijai/converted_pcm_loras_fp16/tree/main>\n- [ByteDance ResAdapter](https://github.com/bytedance/res-adapter) resolution-free model adapter  \n  allows to use resolutions from 0.5 to 2.0 of original model resolution, compatible with *SD15* and *SDXL*\n  enable via scripts -> resadapter and select desired model\n- **Kohya HiRes Fix** allows for higher resolution generation using standard *SD15* models  \n  enable via scripts -> kohya-hires-fix  \n  *note*: alternative to regular hidiffusion method, but with different approach to scaling  \n- additional built-in 4 great custom trained **ControlNet SDXL** models from Xinsir: OpenPose, Canny, Scribble, AnimePainter  \n  thanks @lbeltrame\n- add torch **full deterministic mode**\n  enable in settings -> compute -> use deterministic mode  \n  typical differences are not large and its disabled by default as it does have some performance impact  \n- new sampler: **Euler FlowMatch**  \n\n#### Improvements Fixes 2024-06-13\n\n- additional modernui themes\n- reintroduce prompt attention normalization, disabled by default, enable in settings -> execution  \n  this can drastically help with unbalanced prompts  \n- further work on improving python 3.12 functionality and remove experimental flag  \n  note: recommended version remains python 3.11 for all users, except if you are using directml in which case its python 3.10  \n- improved **installer** for initial installs  \n  initial install will do single-pass install of all required packages with correct versions  \n  subsequent runs will check package versions as necessary  \n- add env variable `SD_PIP_DEBUG` to write `pip.log` for all pip operations  \n  also improved installer logging  \n- add python version check for `torch-directml`  \n- do not install `tensorflow` by default  \n- improve metadata/infotext parser  \n  add `cli/image-exif.py` that can be used to view/extract metadata from images  \n- lower overhead on generate calls  \n- auto-synchronize modernui and core branches  \n- add option to pad prompt with zeros, thanks @Disty\n\n#### Fixes 2024-06-13\n\n- cumulative fixes since the last release  \n- fix apply/unapply hidiffusion for sd15  \n- fix controlnet reference enabled check  \n- fix face-hires with control batch count  \n- install pynvml on-demand  \n- apply rollback-vae option to latest torch versions, thanks @Iaotle  \n- face hires skip if strength is 0  \n- restore all sampler configuration on sampler change  \n\n## Update for 2024-06-02\n\n- fix textual inversion loading\n- fix gallery mtime display\n- fix extra network scrollable area when using modernui\n- fix control prompts list handling\n- fix restore variation seed and strength\n- fix negative prompt parsing from metadata\n- fix stable cascade progress monitoring\n- fix variation seed with hires pass\n- fix loading models trained with onetrainer\n- add variation seed info to metadata\n- workaround for scale-by when using modernui\n- lock torch-directml version\n- improve xformers installer\n- improve ultralytics installer (face-hires)\n- improve triton installer (compile)\n- improve insightface installer (faceip)\n- improve mim installer (dwpose)\n- add dpm++ 1s and dpm++ 3m aliases for dpm++ 2m scheduler with different orders\n\n## Update for 2024-05-28\n\n### Highlights for 2024-05-28\n\nNew [SD.Next](https://github.com/vladmandic/automatic) release has been baking in `dev` for a longer than usual, but changes are massive - about 350 commits for core and 300 for UI...\n\nStarting with the new UI - yup, this version ships with a *preview* of the new [ModernUI](https://github.com/BinaryQuantumSoul/sdnext-modernui)  \nFor details on how to enable and use it, see [Home](https://github.com/BinaryQuantumSoul/sdnext-modernui) and [WiKi](https://github.com/vladmandic/automatic/wiki/Themes)  \n**ModernUI** is still in early development and not all features are available yet, please report [issues and feedback](https://github.com/BinaryQuantumSoul/sdnext-modernui/issues)  \nThanks to @BinaryQuantumSoul for his hard work on this project!  \n\n*What else?*\n\n#### New built-in features\n\n- [PWA](https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps) SD.Next is now installable as a web-app\n- **Gallery**: extremely fast built-in gallery viewer  \n  List, preview, search through all your images and videos!  \n- **HiDiffusion** allows generating very-high resolution images out-of-the-box using standard models  \n- **Perturbed-Attention Guidance** (PAG) enhances sample quality in addition to standard CFG scale  \n- **LayerDiffuse** simply create transparent (foreground-only) images  \n- **IP adapter masking** allows to use multiple input images for each segment of the input image  \n- IP adapter **InstantStyle** implementation  \n- **Token Downsampling** (ToDo) provides significant speedups with minimal-to-none quality loss  \n- **Samplers optimizations** that allow normal samplers to complete work in 1/3 of the steps!  \n  Yup, even popular DPM++2M can now run in 10 steps with quality equaling 30 steps using **AYS** presets  \n- Native **wildcards** support  \n- Improved built-in **Face HiRes**  \n- Better **outpainting**  \n- And much more...  \n  For details of above features and full list, see [Changelog](https://github.com/vladmandic/automatic/blob/dev/CHANGELOG.md)\n\n#### New models\n\nWhile still waiting for *Stable Diffusion 3.0*, there have been some significant models released in the meantime:\n\n- [PixArt-](https://pixart-alpha.github.io/PixArt-sigma-project/), high end diffusion transformer model (*DiT*) capable of directly generating images at 4K resolution  \n- [SDXS](https://github.com/IDKiro/sdxs), extremely fast 1-step generation consistency model  \n- [Hyper-SD](https://huggingface.co/ByteDance/Hyper-SD), 1-step, 2-step, 4-step and 8-step optimized models  \n\n*Note*  \n[SD.Next](https://github.com/vladmandic/automatic) is no longer marked as a fork of [A1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui/) and github project has been fully detached  \nGiven huge number of changes with *+3443/-3342* commits diff (at the time of fork detach) over the past year,  \na completely different backend/engine and a change of focus, it is time to give credit to original [author](https://github.com/auTOMATIC1111),  and move on!  \n\n### Full ChangeLog for 2024-05-28\n\n- **Features**:\n  - **ModernUI** preview of the new [ModernUI](https://github.com/BinaryQuantumSoul/sdnext-modernui)  \n  - [PWA](https://developer.mozilla.org/en-US/docs/Web/Progressive_web_apps) SD.Next is now installable as a web-app and includes verified manifest  \n  - **Gallery\n  - **Gallery**: list, preview, search through all your images and videos!  \n    Implemented as infinite-scroll with client-side-caching and lazy-loading while being fully async and non-blocking  \n    Search or sort by path, name, size, width, height, mtime or any image metadata item, also with extended syntax like *width > 1000*  \n    *Settings*: optional additional user-defined folders, thumbnails in fixed or variable aspect-ratio  \n  - [HiDiffusion](https://github.com/megvii-research/HiDiffusion):  \n    Generate high-resolution images using your standard models without duplicates/distorsions AND improved performance  \n    For example, *SD15* can now go up to *2024x2048* and *SDXL* up to *4k* natively\n    Simply enable checkbox in advanced menu and set desired resolution  \n    Additional settings are available in *settings -> inference settings -> hidiffusion*  \n    And can also be set and used via *xyz grid*  \n    *Note*: HiDiffusion resolution sensitive, so if you get error, set resolution to be multiples of 128  \n  - [Perturbed-Attention Guidance](https://github.com/KU-CVLAB/Perturbed-Attention-Guidance)  \n    PAG enhances sample quality by utilizing self-attention in formation of latent in addition to standard CFG scale  \n    Simply set *advanced -> attention guidance* and *advanced -> adaptive scaling*  \n    Additional options are available in *settings -> inference settings -> pag*  \n    *Note*: PAG has replaced SAG as attention guidance method in SD.Next  \n  - [LayerDiffuse](https://github.com/rootonchair/diffuser_layerdiffuse)\n    Create transparent images with foreground-only being generated  \n    Simply select from scripts -> apply to current model  \n    All necessary files will be auto-downloaded on first use  \n  - **IP Adapter Masking**:  \n    Powerful method of using masking with ip-adapters  \n    When combined with multiple ip-adapters, it allows for different inputs guidance for each segment of the input image  \n    *Hint*: to create masks, you can use manually created masks or control->mask module with auto-segment to create masks and later upload them  \n  - **IP Adapter advanced layer configuration**:  \n    Allows for more control over how each layer of ip-adapter is applied, requires a valid dict to be passed as input  \n    See [InstantStyle](https://github.com/InstantStyle/InstantStyle) for details  \n  - **OneDiff**: new optimization/compile engine, thanks @aifartist  \n    As with all other compile engines, enable via *settings -> compute settings -> compile*  \n  - [ToDo](https://arxiv.org/html/2402.13573v2) Token Downsampling for Efficient Generation of High-Resolution Images  \n    Newer alternative method to [ToMe](https://github.com/dbolya/tomesd) that can provide speed-up with minimal quality loss  \n    Enable in *settings -> inference settings -> token merging*  \n    Also available in XYZ grid  \n  - **Outpaint**:  \n    New method of outpainting that uses a combination of auto-masking and edge generation to create seamless transitions between original and generated image  \n    Use on control tab:\n    - *input -> denoising strength: 0.5 or higher*\n    - *select image -> outpaint -> expand edges or zoom out to desired size*\n    - *size -> mode: outpaint, method: nearest*\n    - *mask -> inpaint masked only (if you want to keep original image)*\n  - **Wildcards**:\n    - native support of standard file-based wildcards in prompt  \n    - enabled by default, can be disabled in *settings -> extra networks* if you want to use 3rd party extension  \n    - wildcards folder is set in *settings -> system paths* and can be flat-file list or complex folder structure  \n    - matches strings `\"__*__\"` in positive and negative prompts  \n    - supports filename and path-based wildcards  \n    - supports nested wildcards (wildcard can refer to another wildcard, etc.)  \n    - supports wildcards files in one-choice per line or multiple choices per line separated by `|` format  \n    - *note*: this is in addition to previously released style-based wildcards  \n- **Models**:\n  - **Load UNET**: ability to override/load external UNET to a selected model  \n    Works similar to how VAE is selected and loaded: Set UNet folder and UNet model in settings  \n    Can be replaced on-the-fly, not just during initial model load  \n    Enables usage of fine-tunes such as [DPO-SD15](https://huggingface.co/mhdang/dpo-sd1.5-text2image-v1) or [DPO-SDXL](https://huggingface.co/mhdang/dpo-sdxl-text2image-v1)  \n    *Note*: if there is a `JSON` file with the same name as the model it will be used as Unet config, otherwise Unet config from currently loaded model will be used  \n  - [PixArt-](https://pixart-alpha.github.io/PixArt-sigma-project/)\n    pixart- is a high end diffusion Transformer model (DiT) with a T5 encoder/decoder capable of directly generating images at 4K resolution  \n    to use, simply select from *networks -> models -> reference -> PixArt-*  \n    *note*: this is a very large model at ~22GB  \n    set parameters: *sampler: Default*  \n  - [SDXS](https://github.com/IDKiro/sdxs)\n    sdxs is an extremely fast 1-step generation consistency model that also uses TAESD as quick VAE out-of-the-box  \n    to use, simply select from *networks -> models -> reference -> SDXS*  \n    set parameters: *sampler: CMSI, steps: 1, cfg_scale: 0.0*\n  - [Hyper-SD](https://huggingface.co/ByteDance/Hyper-SD)  \n    sd15 and sdxl 1-step, 2-step, 4-step and 8-step optimized models using lora  \n    set parameters: *sampler: TCD or LCM, steps: 1/2/4/8, cfg_scale: 0.0*  \n- **UI**:\n  - Faster **UI** load times\n  - Theme types:  \n    **Standard** (built-in themes), **Modern** (experimental nextgen ui), **None** (used for Gradio and Huggingface 3rd party themes)  \n    Specifying a theme type updates list of available themes  \n    For example, *Gradio* themes will not appear as available if theme type is set to *Standard*  \n  - Redesign of base txt2img interface  \n  - Minor tweaks to styles: refresh/apply/save\n  - See details in [WiKi](https://github.com/vladmandic/automatic/wiki/Themes)\n- **API**:\n  - Add API endpoint `/sdapi/v1/control` and CLI util `cli/simple-control.py`  \n    (in addition to previously added `/sdapi/v1/preprocessors` and `/sdapi/v1/masking`)  \n    example:\n    > simple-control.py --prompt 'woman in the city' --sampler UniPC --steps 20  \n    > --input \\~/generative/Samples/cutie-512.png --output /tmp/test.png --processed /tmp/proc.png  \n    > --control 'Canny:Canny FP16:0.7, OpenPose:OpenPose FP16:0.8' --type controlnet  \n    > --ipadapter 'Plus:~/generative/Samples/cutie-512.png:0.5'  \n  - Add API endpoint `/sdapi/v1/vqa` and CLI util `cli/simple-vqa.py`\n- **Changes**:\n  - Due to change in Diffusers model loading  \n    initial model load will now fetch config files required for the model  \n    from the Huggingface site instead of using predefined YAML files\n  - Removed built-in extensions: *ControlNet* and *Image-Browser*  \n    as both *image-browser* and *controlnet* have native built-in equivalents  \n    both can still be installed by user if desired  \n  - Different defaults depending on available GPU, thanks @Disty0\n    - 4GB and below: *lowvram*\n    - 8GB and below: *medvram*\n    - Cross-attention: Dynamic Attention SDP with *medvram* or *lowvram*, otherwise SDP  \n    - VAE Tiling enabled with *medvram* and *lowvram*\n    - Disable Extract EMA by default\n    - Disable forced VAE Slicing for *lowvram*\n  - Upscaler compile disabled by default with OpenVINO backend  \n  - Hypernetwork support disabled by default, can be enabled in settings  \n- **Improvements**:\n  - Faster server startup  \n  - Styles apply wildcards to params\n  - Face HiRes fully configurable and higher quality when using high-resolution models  \n  - Extra networks persistent sort order in settings  \n  - Add option to make batch generations use fully random seed vs sequential  \n  - Make metadata in full screen viewer optional\n  - Add VAE civitai scan metadata/preview\n  - More efficient in-browser callbacks\n  - Updated all system requirements  \n  - UI log monitor will auto-reconnect to server on server restart  \n  - UI styles includes indicator for active styles  \n  - UI reduce load on browser  \n  - Secondary sampler add option \"same as primary\"  \n  - Change attention mechanism on-the-fly without model reload, thanks @Disty0  \n  - Update stable-fast with support for torch 2.2.2 and 2.3.0, thanks @Aptronymist\n  - Add torch *cudaMallocAsync* in compute options  \n    Can improve memory utilization on compatible GPUs (RTX and newer)  \n  - Torch dynamic profiling  \n    You can enable/disable full torch profiling in settings top menu on-the-fly  \n  - Prompt caching - if you use the same prompt multiple times, no need to re-parse and encode it  \n    Useful for batches as prompt processing is ~0.1sec on each pass  \n  - Enhance `SD_PROMPT_DEBUG` to show actual tokens used\n  - Support controlnet manually downloads models in both standalone and diffusers format  \n    For standalone, simply copy safetensors file to `models/control/controlnet` folder  \n    For diffusers format, create folder with model name in `models/control/controlnet/`  \n    and copy `model.json` and `diffusion_pytorch_model.safetensors` to that folder  \n- **Samplers**\n  - Add *Euler SGM* variation (e.g. SGM Uniform), optimized for SDXL-Lightning models  \n    *note*: you can use other samplers as well with SDXL-Lightning models  \n  - Add *CMSI* sampler, optimized for consistency models  \n  - Add option *timestep spacing* to sampler settings and sampler section in main ui\n    Note: changing timestep spacing changes behavior of sampler and can help to make any sampler turbo/lightning compatibile\n  - Add option *timesteps* to manually set timesteps instead of relying on steps+spacing  \n    Additionally, presets from nVidias align-you-steps reasearch are provided  \n    Result is that perfectly aligned steps can drastically reduce number of steps needed!  \n    For example, **AYS** preset alows DPM++2M to run in ~10 steps with quality equallying ~30 steps!  \n- **IPEX**, thanks @Disty0\n  - Update to *IPEX 2.1.20* on Linux  \n    requires removing the venv folder to update properly  \n  - Removed 1024x1024 workaround  \n  - Disable ipexrun by default, set `IPEXRUN=True` if you want to use `ipexrun`  \n- **ROCm**, thanks @Disty0  \n  - Add support for ROCm 6.1 nighthly builds  \n  - Switch to stable branch of PyTorch  \n  - Compatibility improvenments  \n  - Add **MIGraphX** torch compile engine  \n- **ZLUDA**, thanks @lshqqytiger\n  - Rewrite ZLUDA installer\n  - ZLUDA **v3.8** updates: Runtime API support\n  - Add `--reinstall-zluda` (to download the latest ZLUDA)\n- **Fixes**:\n  - Update requirements\n  - Installer automatically handle detached git states  \n  - Prompt params parser\n  - Allowing forcing LoRA loading method for some or all models\n  - Image save without metadata\n  - API generate save metadata\n  - Face/InstantID faults\n  - CivitAI update model info for all models\n  - FP16/BF16 test on model load\n  - Variation seed possible NaNs\n  - Enumerate diffusers model with multiple variants\n  - Diffusers skip non-models on enum\n  - Face-HiRes compatibility with control modules\n  - Face-HiRes avoid doule save in some scenarios\n  - Loading safetensors embeddings\n  - CSS fixes\n  - Check if attention processor is compatible with model\n  - SD Upscale when used with control module\n  - Noise sampler seed, thanks @leppie\n  - Control module with ADetailer and active ControlNet\n  - Control module restore button full functionality\n  - Control improved handling with multiple control units and different init images\n  - Control add correct metadata to image\n  - Time embeddings load part of model load\n  - A1111 update OptionInfo properties\n  - MOTD exception handling\n  - Notifications not triggering\n  - Prompt cropping on copy\n\n## Update for 2024-03-19\n\n### Highlights 2024-03-19\n\nNew models:\n- [Stable Cascade](https://github.com/Stability-AI/StableCascade) *Full* and *Lite*\n- [Playground v2.5](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic)\n- [KOALA 700M](https://github.com/youngwanLEE/sdxl-koala)\n- [Stable Video Diffusion XT 1.1](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1)\n- [VGen](https://huggingface.co/ali-vilab/i2vgen-xl)  \n\nNew pipelines and features:\n- Img2img using [LEdit++](https://leditsplusplus-project.static.hf.space/index.html), context aware method with image analysis and positive/negative prompt handling\n- Trajectory Consistency Distillation [TCD](https://mhh0318.github.io/tcd) for processing in even less steps\n- Visual Query & Answer using [moondream2](https://github.com/vikhyat/moondream) as an addition to standard interrogate methods\n- **Face-HiRes**: simple built-in detailer for face refinements\n- Even simpler outpaint: when resizing image, simply pick outpaint method and if image has different aspect ratio, blank areas will be outpainted!\n- UI aspect-ratio controls and other UI improvements\n- User controllable invisibile and visible watermarking\n- Native composable LoRA\n\nWhat else?\n\n- **Reference models**: *Networks -> Models -> Reference*: All reference models now come with recommended settings that can be auto-applied if desired  \n- **Styles**: Not just for prompts! Styles can apply *generate parameters* as templates and can be used to *apply wildcards* to prompts  \nimprovements, Additional API endpoints  \n- Given the high interest in [ZLUDA](https://github.com/vosen/ZLUDA) engine introduced in last release weve updated much more flexible/automatic install procedure (see [wiki](https://github.com/vladmandic/automatic/wiki/ZLUDA) for details)  \n- Plus Additional Improvements such as: Smooth tiling, Refine/HiRes workflow improvements, Control workflow  \n\nFurther details:  \n- For basic instructions, see [README](https://github.com/vladmandic/automatic/blob/master/README.md)  \n- For more details on all new features see full [CHANGELOG](https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md)  \n- For documentation, see [WiKi](https://github.com/vladmandic/automatic/wiki)\n- [Discord](https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867) server  \n\n### Full Changelog 2024-03-19\n\n- [Stable Cascade](https://github.com/Stability-AI/StableCascade) *Full* and *Lite*\n  - large multi-stage high-quality model from warp-ai/wuerstchen team and released by stabilityai  \n  - download using networks -> reference\n  - see [wiki](https://github.com/vladmandic/automatic/wiki/Stable-Cascade) for details\n- [Playground v2.5](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic)\n  - new model version from Playground: based on SDXL, but with some cool new concepts\n  - download using networks -> reference\n  - set sampler to *DPM++ 2M EDM* or *Euler EDM*\n- [KOALA 700M](https://github.com/youngwanLEE/sdxl-koala)\n  - another very fast & light sdxl model where original unet was compressed and distilled to 54% of original size  \n  - download using networks -> reference\n  - *note* to download fp16 variant (recommended), set settings -> diffusers -> preferred model variant  \n- [LEdit++](https://leditsplusplus-project.static.hf.space/index.html)\n  - context aware img2img method with image analysis and positive/negative prompt handling  \n  - enable via img2img -> scripts -> ledit\n  - uses following params from standard img2img: cfg scale (recommended ~3), steps (recommended ~50), denoise strength (recommended ~0.7)\n  - can use postive and/or negative prompt to guide editing process\n    - positive prompt: what to enhance, strength and threshold for auto-masking\n    - negative prompt: what to remove, strength and threshold for auto-masking  \n  - *note*: not compatible with model offloading\n- **Second Pass / Refine**\n  - independent upscale and hires options: run hires without upscale or upscale without hires or both\n  - upscale can now run 0.1-8.0 scale and will also run if enabled at 1.0 to allow for upscalers that simply improve image quality\n  - update ui section to reflect changes\n  - *note*: behavior using backend:original is unchanged for backwards compatibilty\n- **Visual Query** visual query & answer in process tab  \n  - go to process -> visual query  \n  - ask your questions, e.g. \"describe the image\", \"what is behind the subject\", \"what are predominant colors of the image?\"\n  - primary model is [moondream2](https://github.com/vikhyat/moondream), a *tiny* 1.86B vision language model  \n    *note*: its still 3.7GB in size, so not really tiny  \n  - additional support for multiple variations of several base models: *GIT, BLIP, ViLT, PIX*, sizes range from 0.3 to 1.7GB  \n- **Video**\n  - **Image2Video**\n    - new module for creating videos from images  \n    - simply enable from *img2img -> scripts -> image2video*  \n    - model is auto-downloaded on first use\n    - based on [VGen](https://huggingface.co/ali-vilab/i2vgen-xl)  \n  - **Stable Video Diffusion**\n    - updated with *SVD 1.0, SVD XT 1.0 and SVD XT 1.1*\n    - models are auto-downloaded on first use\n    - simply enable from *img2img -> scripts -> stable video diffusion*  \n    - for svd 1.0, use frames=~14, for xt models use frames=~25\n- **Composable LoRA**, thanks @AI-Casanova\n  - control lora strength for each step\n    for example: `<xxx:0.1@0,0.9@1>` means strength=0.1 for step at 0% and intepolate towards strength=0.9 for step at 100%\n  - *note*: this is a very experimental feature and may not work as expected\n- **Control**\n  - added *refiner/hires* workflows\n  - added resize methods to before/after/mask: fixed, crop, fill\n- **Styles**: styles are not just for prompts!\n  - new styles editor: *networks -> styles -> edit*\n  - styles can apply generate parameters, for example to have a style that enables and configures hires:  \n    parameters=`enable_hr: True, hr_scale: 2, hr_upscaler: Latent Bilinear antialias, hr_sampler_name: DEIS, hr_second_pass_steps: 20, denoising_strength: 0.5`\n  - styles can apply wildcards to prompts, for example:  \n    wildcards=`movie=mad max, dune, star wars, star trek; intricate=realistic, color sketch, pencil sketch, intricate`\n  - as usual, you can apply any number of styles so you can choose which settings are applied and in which order and which wildcards are used\n- **UI**\n  - *aspect-ratio** add selector and lock to width/height control  \n    allowed aspect ration can be configured via *settings -> user interface*  \n  - *interrogate* tab is now merged into *process* tab  \n  - *image viewer* now displays image metadata\n  - *themes* improve on-the-fly switching\n  - *log monitor* flag server warnings/errors and overall improve display\n  - *control* separate processor settings from unit settings\n- **Face HiRes**\n  - new *face restore* option, works similar to well-known *adetailer* by running an inpaint on detected faces but with just a checkbox to enable/disable  \n  - set as default face restorer in settings -> postprocessing  \n  - disabled by default, to enable simply check *face restore* in your generate advanced settings  \n  - strength, steps and sampler are set using by hires section in refine menu  \n  - strength can be overriden in settings -> postprocessing  \n  - will use secondary prompt and secondary negative prompt if present in refine  \n- **Watermarking**\n  - SD.Next disables all known watermarks in models, but does allow user to set custom watermark  \n  - see *settings -> image options -> watermarking*  \n  - invisible watermark: using steganogephy  \n  - image watermark: overlaid on top of image  \n- **Reference models**\n  - additional reference models available for single-click download & run:\n    *Stable Cascade, Stable Cascade lite, Stable Video Diffusion XT 1.1*  \n  - reference models will now download *fp16* variation by default  \n  - reference models will print recommended settings to log if present\n  - new setting in extra network: *use reference values when available*  \n    disabled by default, if enabled will force use of reference settings for models that have them\n- **Samplers**\n  - [TCD](https://mhh0318.github.io/tcd/): Trajectory Consistency Distillation  \n    new sampler that produces consistent results in a very low number of steps (comparable to LCM but without reliance on LoRA)  \n    for best results, use with TCD LoRA: <https://huggingface.co/h1t/TCD-SDXL-LoRA>\n  - *DPM++ 2M EDM* and *Euler EDM*  \n    EDM is a new solver algorithm currently available for DPM++2M and Euler samplers  \n    Note that using EDM samplers with non-EDM optimized models will provide just noise and vice-versa  \n- **Improvements**\n  - **FaceID** extend support for LoRA, HyperTile and FreeU, thanks @Trojaner\n  - **Tiling** now extends to both Unet and VAE producing smoother outputs, thanks @AI-Casanova\n  - new setting in image options: *include mask in output*\n  - improved params parsing from from prompt string and styles\n  - default theme updates and additional built-in theme *black-gray*\n  - support models with their own YAML model config files\n  - support models with their own JSON per-component config files, for example: `playground-v2.5_vae.config`\n  - prompt can have comments enclosed with `/*` and `*/`  \n    comments are extracted from prompt and added to image metadata  \n- **ROCm**  \n  - add **ROCm** 6.0 nightly option to installer, thanks @jicka\n  - add *flash attention* support for rdna3, thanks @Disty0  \n    install flash_attn package for rdna3 manually and enable *flash attention* from *compute settings*  \n    to install flash_attn, activate the venv and run `pip install -U git+https://github.com/ROCm/flash-attention@howiejay/navi_support`  \n- **IPEX**\n  - disabled IPEX Optimize by default  \n- **API**\n  - add preprocessor api endpoints  \n    GET:`/sdapi/v1/preprocessors`, POST:`/sdapi/v1/preprocess`, sample script:`cli/simple-preprocess.py`\n  - add masking api endpoints  \n    GET:`/sdapi/v1/masking`, POST:`/sdapi/v1/mask`, sample script:`cli/simple-mask.py`\n- **Internal**\n  - improved vram efficiency for model compile, thanks @Disty0\n  - **stable-fast** compatibility with torch 2.2.1  \n  - remove obsolete textual inversion training code\n  - remove obsolete hypernetworks training code\n- **Refiner** validated workflows:\n  - Fully functional: SD15 + SD15, SDXL + SDXL, SDXL + SDXL-R\n  - Functional, but result is not as good: SD15 + SDXL, SDXL + SD15, SD15 + SDXL-R\n- **SDXL Lightning** models just-work, just makes sure to set CFG Scale to 0  \n    and choose a best-suited sampler, it may not be the one youre used to (e.g. maybe even basic Euler)  \n- **Fixes**\n  - improve *model cpu offload* compatibility\n  - improve *model sequential offload* compatibility\n  - improve *bfloat16* compatibility\n  - improve *xformers* installer to match cuda version and install triton\n  - fix extra networks refresh\n  - fix *sdp memory attention* in backend original\n  - fix autodetect sd21 models\n  - fix api info endpoint\n  - fix *sampler eta* in xyz grid, thanks @AI-Casanova\n  - fix *requires_aesthetics_score* errors\n  - fix t2i-canny\n  - fix *differenital diffusion* for manual mask, thanks @23pennies\n  - fix ipadapter apply/unapply on batch runs\n  - fix control with multiple units and override images\n  - fix control with hires\n  - fix control-lllite\n  - fix font fallback, thanks @NetroScript\n  - update civitai downloader to handler new metadata\n  - improve control error handling\n  - use default model variant if specified variant doesnt exist\n  - use diffusers lora load override for *lcm/tcd/turbo loras*\n  - exception handler around vram memory stats gather\n  - improve ZLUDA installer with `--use-zluda` cli param, thanks @lshqqytiger\n\n## Update for 2024-02-22\n\nOnly 3 weeks since last release, but heres another feature-packed one!\nThis time release schedule was shorter as we wanted to get some of the fixes out faster.\n\n### Highlights 2024-02-22\n\n- **IP-Adapters** & **FaceID**: multi-adapter and multi-image suport  \n- New optimization engines: [DeepCache](https://github.com/horseee/DeepCache), [ZLUDA](https://github.com/vosen/ZLUDA) and **Dynamic Attention Slicing**  \n- New built-in pipelines: [Differential diffusion](https://github.com/exx8/differential-diffusion) and [Regional prompting](https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#regional-prompting-pipeline)  \n- Big updates to: **Outpainting** (noised-edge-extend), **Clip-skip** (interpolate with non-integrer values!), **CFG end** (prevent overburn on high CFG scales), **Control** module masking functionality  \n- All reported issues since the last release are addressed and included in this release  \n\nFurther details:  \n- For basic instructions, see [README](https://github.com/vladmandic/automatic/blob/master/README.md)  \n- For more details on all new features see full [CHANGELOG](https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md)  \n- For documentation, see [WiKi](https://github.com/vladmandic/automatic/wiki)\n- [Discord](https://discord.com/invite/sd-next-federal-batch-inspectors-1101998836328697867) server  \n\n### Full ChangeLog for 2024-02-22\n\n- **Improvements**:\n  - **IP Adapter** major refactor  \n    - support for **multiple input images** per each ip adapter  \n    - support for **multiple concurrent ip adapters**  \n      *note*: you cannot mix & match ip adapters that use different *CLiP* models, for example `Base` and `Base ViT-G`  \n    - add **adapter start/end** to settings, thanks @AI-Casanova  \n      having adapter start late can help with better control over composition and prompt adherence  \n      having adapter end early can help with overal quality and performance  \n    - unified interface in txt2img, img2img and control  \n    - enhanced xyz grid support  \n  - **FaceID** now also works with multiple input images!  \n  - [Differential diffusion](https://github.com/exx8/differential-diffusion)  \n    img2img generation where you control strength of each pixel or image area  \n    can be used with manually created masks or with auto-generated depth-maps\n    uses general denoising strength value  \n    simply enable from *img2img -> scripts -> differential diffusion*  \n    *note*: supports sd15 and sdxl models  \n  - [Regional prompting](https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#regional-prompting-pipeline) as a built-in solution  \n    usage is same as original implementation from @hako-mikan  \n    click on title to open docs and see examples of full syntax on how to use it  \n    simply enable from *scripts -> regional prompting*  \n    *note*: supports sd15 models only  \n  - [DeepCache](https://github.com/horseee/DeepCache) model acceleration  \n    it can produce massive speedups (2x-5x) with no overhead, but with some loss of quality  \n    *settings -> compute -> model compile -> deep-cache* and *settings -> compute -> model compile -> cache interval*  \n  - [ZLUDA](https://github.com/vosen/ZLUDA) experimental support, thanks @lshqqytiger  \n    - ZLUDA is CUDA wrapper that can be used for GPUs without native support\n    - best use case is *AMD GPUs on Windows*, see [wiki](https://github.com/vladmandic/automatic/wiki/ZLUDA) for details  \n  - **Outpaint** control outpaint now uses new alghorithm: noised-edge-extend  \n    new method allows for much larger outpaint areas in a single pass, even outpaint 512->1024 works well  \n    note that denoise strength should be increased for larger the outpaint areas, for example outpainting 512->1024 works well with denoise 0.75  \n    outpaint can run in *img2img* mode (default) and *inpaint* mode where original image is masked (if inpaint masked only is selected)  \n  - **Clip-skip** reworked completely, thanks @AI-Casanova & @Disty0  \n    now clip-skip range is 0-12 where previously lowest value was 1 (default is still 1)  \n    values can also be decimal to interpolate between different layers, for example `clip-skip: 1.5`, thanks @AI-Casanova  \n  - **CFG End** new param to control image generation guidance, thanks @AI-Casanova  \n    sometimes you want strong control over composition, but you want it to stop at some point  \n    for example, when used with ip-adapters or controlnet, high cfg scale can overpower the guided image  \n  - **Control**\n    - when performing inpainting, you can specify processing resolution using **size->mask**  \n    - units now have extra option to re-use current preview image as processor input  \n  - **Cross-attention** refactored cross-attention methods, thanks @Disty0  \n    - for backend:original, its unchanged: SDP, xFormers, Doggettxs, InvokeAI, Sub-quadratic, Split attention  \n    - for backend:diffuers, list is now: SDP, xFormers, Batch matrix-matrix, Split attention, Dynamic Attention BMM, Dynamic Attention SDP  \n      note: you may need to update your settings! Attention Slicing is renamed to Split attention  \n    - for ROCm, updated default cross-attention to Scaled Dot Product  \n  - **Dynamic Attention Slicing**, thanks @Disty0  \n    - dynamically slices attention queries in order to keep them under the slice rate  \n      slicing gets only triggered if the query size is larger than the slice rate to gain performance  \n      *Dynamic Attention Slicing BMM* uses *Batch matrix-matrix*  \n      *Dynamic Attention Slicing SDP* uses *Scaled Dot Product*  \n    - *settings -> compute settings -> attention -> dynamic attention slicing*  \n  - **ONNX**:  \n    - allow specify onnx default provider and cpu fallback  \n      *settings -> diffusers*  \n    - allow manual install of specific onnx flavor  \n      *settings -> onnx*  \n    - better handling of `fp16` models/vae, thanks @lshqqytiger  \n  - **OpenVINO** update to `torch 2.2.0`, thanks @Disty0  \n  - **HyperTile** additional options thanks @Disty0  \n    - add swap size option  \n    - add use only for hires pass option  \n  - add `--theme` cli param to force theme on startup  \n  - add `--allow-paths` cli param to add additional paths that are allowed to be accessed via web, thanks @OuticNZ  \n- **Wiki**:\n  - added benchmark notes for IPEX, OpenVINO and Olive  \n  - added ZLUDA wiki page  \n- **Internal**\n  - update dependencies  \n  - refactor txt2img/img2img api  \n  - enhanced theme loader  \n  - add additional debug env variables  \n  - enhanced sdp cross-optimization control  \n    see *settings -> compute settings*  \n  - experimental support for *python 3.12*  \n- **Fixes**:  \n  - add variation seed to diffusers txt2img, thanks @AI-Casanova  \n  - add cmd param `--skip-env` to skip setting of environment parameters during sdnext load  \n  - handle extensions that install conflicting versions of packages  \n    `onnxruntime`, `opencv2-python`  \n  - installer refresh package cache on any install  \n  - fix embeddings registration on server startup, thanks @AI-Casanova  \n  - ipex handle dependencies, thanks @Disty0  \n  - insightface handle dependencies  \n  - img2img mask blur and padding  \n  - xyz grid handle ip adapter name and scale  \n  - lazy loading of image may prevent metadata from being loaded on time  \n  - allow startup without valid models folder  \n  - fix interrogate api endpoint  \n  - control fix resize causing runtime errors  \n  - control fix processor override image after processor change  \n  - control fix display grid with batch  \n  - control restore pipeline before running scripts/extensions  \n  - handle pipelines that return dict instead of object  \n  - lora use strict name matching if preferred option is by-filename  \n  - fix inpaint mask only for diffusers  \n  - fix vae dtype mismatch, thanks @Disty0  \n  - fix controlnet inpaint mask  \n  - fix theme list refresh  \n  - fix extensions update information in ui  \n  - fix taesd with bfloat16\n  - fix model merge manual merge settings, thanks @AI-Casanova  \n  - fix gradio instant update issues for textboxes in quicksettings  \n  - fix rembg missing dependency  \n  - bind controlnet extension to last known working commit, thanks @Aptronymist  \n  - prompts-from-file fix resizable prompt area  \n\n## Update for 2024-02-07\n\nAnother big release just hit the shelves!\n\n### Highlights 2024-02-07  \n\n- A lot more functionality in the **Control** module:\n  - Inpaint and outpaint support, flexible resizing options, optional hires  \n  - Built-in support for many new processors and models, all auto-downloaded on first use  \n  - Full support for scripts and extensions  \n- Complete **Face** module  \n  implements all variations of **FaceID**, **FaceSwap** and latest **PhotoMaker** and **InstantID**  \n- Much enhanced **IPAdapter** modules  \n- Brand new **Intelligent masking**, manual or automatic  \n  Using ML models (*LAMA* object removal, *REMBG* background removal, *SAM* segmentation, etc.) and with live previews  \n  With granular blur, erode and dilate controls  \n- New models and pipelines:  \n  **Segmind SegMoE**, **Mixture Tiling**, **InstaFlow**, **SAG**, **BlipDiffusion**  \n- Massive work integrating latest advances with [OpenVINO](https://github.com/vladmandic/automatic/wiki/OpenVINO), [IPEX](https://github.com/vladmandic/automatic/wiki/Intel-ARC) and [ONNX Olive](https://github.com/vladmandic/automatic/wiki/ONNX-Runtime-&-Olive)\n- Full control over brightness, sharpness and color shifts and color grading during generate process directly in latent space  \n- **Documentation**! This was a big one, with a lot of new content and updates in the [WiKi](https://github.com/vladmandic/automatic/wiki)  \n\nPlus welcome additions to **UI performance, usability and accessibility** and flexibility of deployment as well as **API** improvements  \nAnd it also includes fixes for all reported issues so far  \n\nAs of this release, default backend is set to **diffusers** as its more feature rich than **original** and supports many additional models (original backend does remain as fully supported)  \n\nAlso, previous versions of **SD.Next** were tuned for balance between performance and resource usage.  \nWith this release, focus is more on performance.  \nSee [Benchmark](https://github.com/vladmandic/automatic/wiki/Benchmark) notes for details, but as a highlight, we are now hitting **~110-150 it/s** on a standard nVidia RTX4090 in optimal scenarios!  \n\nFurther details:  \n- For basic instructions, see [README](https://github.com/vladmandic/automatic/blob/master/README.md)  \n- For more details on all new features see full [CHANGELOG](https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md)  \n- For documentation, see [WiKi](https://github.com/vladmandic/automatic/wiki)\n\n### Full ChangeLog 2024-02-07  \n\n- Heavily updated [Wiki](https://github.com/vladmandic/automatic/wiki)  \n- **Control**:  \n  - new docs:\n    - [Control overview](https://github.com/vladmandic/automatic/wiki/Control)  \n    - [Control guide](https://github.com/vladmandic/automatic/wiki/Control-Guide), thanks @Aptronymist  \n  - add **inpaint** support  \n    applies to both *img2img* and *controlnet* workflows  \n  - add **outpaint** support  \n    applies to both *img2img* and *controlnet* workflows  \n    *note*: increase denoising strength since outpainted area is blank by default  \n  - new **mask** module  \n    - granular blur (gaussian), erode (reduce or remove noise) and dilate (pad or expand)  \n    - optional **live preview**  \n    - optional **auto-segmentation** using ml models  \n      auto-segmentation can be done using **segment-anything** models or **rembg** models  \n      *note*: auto segmentation will automatically expand user-masked area to segments that include current user mask  \n    - optional **auto-mask**  \n      if you dont provide mask or mask is empty, you can instead use auto-mask to automatically generate mask  \n      this is especially useful if you want to use advanced masking on batch or video inputs and dont want to manually mask each image  \n      *note*: such auto-created mask is also subject to all other selected settings such as auto-segmentation, blur, erode and dilate  \n    - optional **object removal** using LaMA model  \n      remove selected objects from images with a single click  \n      works best when combined with auto-segmentation to remove smaller objects  \n    - masking can be combined with control processors in which case mask is applied before processor  \n    - unmasked part of can is optionally applied to final image as overlay, see settings `mask_apply_overlay`  \n  - support for many additional controlnet models  \n    now built-in models include 30+ SD15 models and 15+ SDXL models  \n  - allow **resize** both *before* and *after* generate operation  \n    this allows for workflows such as: *image -> upscale or downscale -> generate -> upscale or downscale -> output*  \n    providing more flexibility and than standard hires workflow  \n    *note*: resizing before generate can be done using standard upscalers or latent\n  - implicit **hires**  \n    since hires is only used for txt2img, control reuses existing resize functionality\n    any image size is used as txt2img target size  \n    but if resize scale is also set its used to additionally upscale image after initial txt2img and for hires pass  \n  - add support for **scripts** and **extensions**  \n    you can now combine control workflow with your favorite script or extension  \n    *note* extensions that are hard-coded for txt2img or img2img tabs may not work until they are updated  \n  - add **depth-anything** depth map processor and trained controlnet  \n  - add **marigold** depth map processor  \n    this is state-of-the-art depth estimation model, but its quite heavy on resources  \n  - add **openpose xl** controlnet  \n  - add blip/booru **interrogate** functionality to both input and output images  \n  - configurable output folder in settings  \n  - auto-refresh available models on tab activate  \n  - add image preview for override images set per-unit  \n  - more compact unit layout  \n  - reduce usage of temp files  \n  - add context menu to action buttons  \n  - move ip-adapter implementation to control tabs  \n  - resize by now applies to input image or frame individually  \n    allows for processing where input images are of different sizes  \n  - support controlnets with non-default yaml config files  \n  - implement resize modes for override images  \n  - allow any selection of units  \n  - dynamically install depenencies required by specific processors  \n  - fix input image size  \n  - fix video color mode  \n  - fix correct image mode  \n  - fix batch/folder/video modes  \n  - fix processor switching within same unit  \n  - fix pipeline switching between different modes  \n- **Face** module  \n  implements all variations of **FaceID**, **FaceSwap** and latest **PhotoMaker** and **InstantID**  \n  simply select from scripts and choose your favorite method and model  \n  *note*: all models are auto-downloaded on first use  \n  - [FaceID](https://huggingface.co/h94/IP-Adapter-FaceID)  \n    - faceid guides image generation given the input image  \n    - full implementation for *SD15* and *SD-XL*, to use simply select from *Scripts*  \n      **Base** (93MB) uses *InsightFace* to generate face embeds and *OpenCLIP-ViT-H-14* (2.5GB) as image encoder  \n      **Plus** (150MB) uses *InsightFace* to generate face embeds and *CLIP-ViT-H-14-laion2B* (3.8GB) as image encoder  \n      **SDXL** (1022MB) uses *InsightFace* to generate face embeds and *OpenCLIP-ViT-bigG-14* (3.7GB) as image encoder  \n  - [FaceSwap](https://github.com/deepinsight/insightface/blob/master/examples/in_swapper/README.md)  \n    - face swap performs face swapping at the end of generation  \n    - based on InsightFace in-swapper  \n  - [PhotoMaker](https://github.com/TencentARC/PhotoMaker)  \n    - for *SD-XL* only  \n    - new model from TenencentARC using similar concept as IPAdapter, but with different implementation and  \n      allowing full concept swaps between input images and generated images using trigger words  \n    - note: trigger word must match exactly one term in prompt for model to work  \n  - [InstantID](https://github.com/InstantID/InstantID)  \n    - for *SD-XL* only  \n    - based on custom trained ip-adapter and controlnet combined concepts  \n    - note: controlnet appears to be heavily watermarked  \n  - enable use via api, thanks @trojaner  \n- [IPAdapter](https://huggingface.co/h94/IP-Adapter)  \n  - additional models for *SD15* and *SD-XL*, to use simply select from *Scripts*:  \n    **SD15**: Base, Base ViT-G, Light, Plus, Plus Face, Full Face  \n    **SDXL**: Base SDXL, Base ViT-H SDXL, Plus ViT-H SDXL, Plus Face ViT-H SDXL  \n  - enable use via api, thanks @trojaner  \n- [Segmind SegMoE](https://github.com/segmind/segmoe)  \n  - initial support for reference models  \n    download&load via network -> models -> reference -> **SegMoE SD 4x2** (3.7GB), **SegMoE XL 2x1** (10GB), **SegMoE XL 4x2**  \n  - note: since segmoe is basically sequential mix of unets from multiple models, it can get large  \n    SD 4x2 is ~4GB, XL 2x1 is ~10GB and XL 4x2 is 18GB  \n  - supports lora, thanks @AI-Casanova\n  - support for create and load custom mixes will be added in the future  \n- [Mixture Tiling](https://arxiv.org/abs/2302.02412)  \n  - uses multiple prompts to guide different parts of the grid during diffusion process  \n  - can be used ot create complex scenes with multiple subjects  \n  - simply select from scripts  \n- [Self-attention guidance](https://github.com/SusungHong/Self-Attention-Guidance)  \n  - simply select scale in advanced menu  \n  - can drastically improve image coherence as well as reduce artifacts  \n  - note: only compatible with some schedulers  \n- [FreeInit](https://tianxingwu.github.io/pages/FreeInit/) for **AnimateDiff**\n  - greatly improves temporal consistency of generated outputs  \n  - all options are available in animateddiff script  \n- [SalesForce BlipDiffusion](https://huggingface.co/docs/diffusers/api/pipelines/blip_diffusion)  \n  - model can be used to place subject in a different context  \n  - requires input image  \n  - last word in prompt and negative prompt will be used as source and target subjects  \n  - sampler must be set to default before loading the model  \n- [InstaFlow](https://github.com/gnobitab/InstaFlow)  \n  - another take on super-fast image generation in a single step  \n  - set *sampler:default, steps:1, cfg-scale:0*  \n  - load from networks -> models -> reference  \n- **Improvements**  \n  - **ui**  \n    - check version and **update** SD.Next via UI  \n      simply go to: settings -> update\n    - globally configurable **font size**  \n      will dynamically rescale ui depending on settings -> user interface  \n    - built-in **themes** can be changed on-the-fly  \n      this does not work with gradio-default themes as css is created by gradio itself  \n    - two new **themes**: *simple-dark* and *simple-light*  \n    - modularized blip/booru interrogate  \n      now appears as toolbuttons on image/gallery output  \n    - faster browser page load  \n    - update hints, thanks @brknsoul  \n    - cleanup settings  \n  - **server**\n    - all move/offload options are disable by default for optimal performance  \n      enable manually if low on vram  \n  - **server startup**: performance  \n    - reduced module imports  \n      ldm support is now only loaded when running in backend=original  \n    - faster extension load  \n    - faster json parsing  \n    - faster lora indexing  \n    - lazy load optional imports  \n    - batch embedding load, thanks @midcoastal and @AI-Casanova  \n      10x+ faster embeddings load for large number of embeddings, now works for 1000+ embeddings  \n    - file and folder list caching, thanks @midcoastal\n      if you have a lot of files and and/or are using slower or non-local storage, this speeds up file access a lot  \n    - add `SD_INSTALL_DEBUG` env variable to trace all `git` and `pip` operations\n  - **extra networks**  \n    - 4x faster civitai metadata and previews lookup  \n    - better display and selection of tags & trigger words  \n      if hashes are calculated, trigger words will only be displayed for actual model version  \n    - better matching of previews  \n    - better search, including searching for multiple keywords or using full regex  \n      see wiki page for more details on syntax  \n      thanks @NetroScript  \n    - reduce html overhead  \n  - **model compression**, thanks @Disty0  \n    - using built-in NNCF model compression, you can reduce the size of your models significantly  \n      example: up to 3.4GB of VRAM saved for SD-XL model!  \n    - see [wiki](https://github.com/vladmandic/automatic/wiki/Model-Compression-with-NNCF) for details  \n  - **embeddings**  \n    you can now use sd 1.5 embeddings with your sd-xl models!, thanks @AI-Casanova  \n    conversion is done on-the-fly, is completely transparent and result is an approximation of embedding  \n    to enable: settings->extra networks->auto-convert embeddings  \n  - **offline deployment**: allow deployment without git clone  \n    for example, you can now deploy a zip of the sdnext folder  \n  - **latent upscale**: updated latent upscalers (some are new)  \n    *nearest, nearest-exact, area, bilinear, bicubic, bilinear-antialias, bicubic-antialias*\n  - **scheduler**: added `SA Solver`  \n  - **model load to gpu**  \n    new option in settings->diffusers allowing models to be loaded directly to GPU while keeping RAM free  \n    this option is not compatible with any kind of model offloading as model is expected to stay in GPU  \n    additionally, all model-moves can now be traced with env variable `SD_MOVE_DEBUG`  \n  - **xyz grid**\n    - range control  \n      example: `5.0-6.0:3` will generate 3 images with values `5.0,5.5,6.0`  \n      example: `10-20:4` will generate 4 images with values `10,13,16,20`  \n    - continue on error  \n      now you can use xyz grid with different params and test which ones work and which dont  \n    - correct font scaling, thanks @nCoderGit  \n  - **hypertile**  \n    - enable vae tiling  \n    - add autodetect optimial value  \n      set tile size to 0 to use autodetected value  \n  - **cli**  \n    - `sdapi.py` allow manual api invoke  \n      example: `python cli/sdapi.py /sdapi/v1/sd-models`  \n    - `image-exif.py` improve metadata parsing  \n    - `install-sf` helper script to automatically find best available stable-fast package for the platform  \n  - **memory**: add ram usage monitoring in addition to gpu memory usage monitoring  \n  - **vae**: enable taesd batch decode  \n    enable/disable with settings -> diffusers > vae slicing  \n- **compile**\n  - new option: **fused projections**  \n    pretty much free 5% performance boost for compatible models  \n    enable in settings -> compute settings  \n  - new option: **dynamic quantization** (experimental)  \n    reduces memory usage and increases performance  \n    enable in settings -> compute settings  \n    best used together with torch compile: *inductor*  \n    this feature is highly experimental and will evolve over time  \n    requires nightly versions of `torch` and `torchao`  \n    > `pip install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121`  \n    > `pip install -U git+https://github.com/pytorch-labs/ao`  \n  - new option: **compile text encoder** (experimental)  \n- **correction**  \n  - new section in generate, allows for image corrections during generataion directly in latent space  \n  - adds *brightness*, *sharpness* and *color* controls, thanks @AI-Casanova\n  - adds *color grading* controls, thanks @AI-Casanova\n  - replaces old **hdr** section\n- **IPEX**, thanks @disty0  \n  - see [wiki](https://github.com/vladmandic/automatic/wiki/Intel-ARC) for details  \n  - rewrite ipex hijacks without CondFunc  \n    improves compatibilty and performance  \n    fixes random memory leaks  \n  - out of the box support for Intel Data Center GPU Max Series  \n  - remove IPEX / Torch 2.0 specific hijacks  \n  - add `IPEX_SDPA_SLICE_TRIGGER_RATE`, `IPEX_ATTENTION_SLICE_RATE` and `IPEX_FORCE_ATTENTION_SLICE` env variables  \n  - disable 1024x1024 workaround if the GPU supports 64 bit  \n  - fix lock-ups at very high resolutions  \n- **OpenVINO**, thanks @disty0  \n  - see [wiki](https://github.com/vladmandic/automatic/wiki/OpenVINO) for details  \n  - **quantization support with NNCF**  \n    run 8 bit directly without autocast  \n    enable *OpenVINO Quantize Models with NNCF* from *Compute Settings*  \n  - **4-bit support with NNCF**  \n    enable *Compress Model weights with NNCF* from *Compute Settings* and set a 4-bit NNCF mode  \n    select both CPU and GPU from the device selection if you want to use 4-bit or 8-bit modes on GPU  \n  - experimental support for *Text Encoder* compiling  \n    OpenVINO is faster than IPEX now  \n  - update to OpenVINO 2023.3.0  \n  - add device selection to `Compute Settings`  \n    selecting multiple devices will use `HETERO` device  \n  - remove `OPENVINO_TORCH_BACKEND_DEVICE` env variable  \n  - reduce system memory usage after compile  \n  - fix cache loading with multiple models  \n- **Olive** support, thanks @lshqqytiger\n  - fully merged in in [wiki](https://github.com/vladmandic/automatic/wiki/ONNX-Runtime-&-Olive), see wiki for details  \n  - as a highlight, 4-5 it/s using DirectML on AMD GPU translates to 23-25 it/s using ONNX/Olive!  \n- **fixes**  \n  - civitai model download: enable downloads of embeddings\n  - ipadapter: allow changing of model/image on-the-fly  \n  - ipadapter: fix fallback of cross-attention on unload  \n  - rebasin iterations, thanks @AI-Casanova\n  - prompt scheduler, thanks @AI-Casanova\n  - python: fix python 3.9 compatibility  \n  - sdxl: fix positive prompt embeds\n  - img2img: clip and blip interrogate  \n  - img2img: sampler selection offset  \n  - img2img: support variable aspect ratio without explicit resize  \n  - cli: add `simple-upscale.py` script  \n  - cli: fix cmd args parsing  \n  - cli: add `run-benchmark.py` script  \n  - api: add `/sdapi/v1/version` endpoint\n  - api: add `/sdapi/v1/platform` endpoint\n  - api: return current image in progress api if requested  \n  - api: sanitize response object  \n  - api: cleanup error logging  \n  - api: fix api-only errors  \n  - api: fix image to base64\n  - api: fix upscale  \n  - refiner: fix use of sd15 model as refiners in second pass  \n  - refiner: enable none as option in xyz grid  \n  - sampler: add sampler options info to metadata\n  - sampler: guard against invalid sampler index  \n  - sampler: add img2img_extra_noise option\n  - config: reset default cfg scale to 6.0  \n  - hdr: fix math, thanks @AI-Casanova\n  - processing: correct display metadata  \n  - processing: fix batch file names  \n  - live preview: fix when using `bfloat16`  \n  - live preview: add thread locking  \n  - upscale: fix ldsr\n  - huggingface: handle fallback model variant on load  \n  - reference: fix links to models and use safetensors where possible  \n  - model merge: unbalanced models where not all keys are present, thanks @AI-Casanova\n  - better sdxl model detection\n  - global crlf->lf switch  \n  - model type switch if there is loaded submodels  \n  - cleanup samplers use of compute devices, thanks @Disty0  \n- **other**  \n  - extensions `sd-webui-controlnet` is locked to commit `ecd33eb` due to breaking changes  \n  - extension `stable-diffusion-webui-images-browser` is locked to commit `27fe4a7` due to breaking changes  \n  - updated core requirements  \n  - fully dynamic pipelines  \n    pipeline switch is now done on-the-fly and does not require manual initialization of individual components  \n    this allows for quick implementation of new pipelines  \n    see `modules/sd_models.py:switch_pipe` for details  \n  - major internal ui module refactoring  \n    this may cause compatibility issues if an extension is doing a direct import from `ui.py`  \n    in which case, report it so we can add a compatibility layer  \n  - major public api refactoring  \n    this may cause compatibility issues if an extension is doing a direct import from `api.py` or `models.py`  \n    in which case, report it so we can add a compatibility layer  \n\n## Update for 2023-12-29\n\nTo wrap up this amazing year, were releasing a new version of [SD.Next](https://github.com/vladmandic/automatic), this one is absolutely massive!  \n\n### Highlights 2023-12-29\n\n- Brand new Control module for *text, image, batch and video* processing  \n  Native implementation of all control methods for both *SD15* and *SD-XL*  \n   **ControlNet | ControlNet XS | Control LLLite | T2I Adapters | IP Adapters**  \n  For details, see [Wiki](https://github.com/vladmandic/automatic/wiki/Control) documentation:  \n- Support for new models types out-of-the-box  \n  This brings number of supported t2i/i2i model families to 13!  \n   **Stable Diffusion 1.5/2.1 | SD-XL | LCM | Segmind | Kandinsky | Pixart- | Wrstchen | aMUSEd | DeepFloyd IF | UniDiffusion | SD-Distilled | BLiP Diffusion | etc.**  \n- New video capabilities:  \n   **AnimateDiff | SVD | ModelScope | ZeroScope**  \n- Enhanced platform support  \n   **Windows | Linux | MacOS** with **nVidia | AMD | IntelArc | DirectML | OpenVINO | ONNX+Olive** backends  \n- Better onboarding experience (first install)  \n  with all model types available for single click download & load (networks -> reference)  \n- Performance optimizations!\n  For comparisment of different processing options and compile backends, see [Wiki](https://github.com/vladmandic/automatic/wiki/Benchmark)  \n  As a highlight, were reaching **~100 it/s** (no tricks, this is with full features enabled and end-to-end on a standard nVidia RTX4090)  \n- New [custom pipelines](https://github.com/vladmandic/automatic/blob/dev/scripts/example.py) framework for quickly porting any new pipeline  \n\nAnd others improvements in areas such as: Upscaling (up to 8x now with 40+ available upscalers), Inpainting (better quality), Prompt scheduling, new Sampler options, new LoRA types, additional UI themes, better HDR processing, built-in Video interpolation, parallel Batch processing, etc.  \n\nPlus some nifty new modules such as **FaceID** automatic face guidance using embeds during generation and **Depth 3D** image to 3D scene\n\n### Full ChangeLog 2023-12-29\n\n- **Control**  \n  - native implementation of all image control methods:  \n    **ControlNet**, **ControlNet XS**, **Control LLLite**, **T2I Adapters** and **IP Adapters**  \n  - top-level **Control** next to **Text** and **Image** generate  \n  - supports all variations of **SD15** and **SD-XL** models  \n  - supports *Text*, *Image*, *Batch* and *Video* processing  \n  - for details and list of supported models and workflows, see Wiki documentation:  \n    <https://github.com/vladmandic/automatic/wiki/Control>  \n- **Diffusers**  \n  - [Segmind Vega](https://huggingface.co/segmind/Segmind-Vega) model support  \n    - small and fast version of **SDXL**, only 3.1GB in size!  \n    - select from *networks -> reference*  \n  - [aMUSEd 256](https://huggingface.co/amused/amused-256) and [aMUSEd 512](https://huggingface.co/amused/amused-512) model support  \n    - lightweigt models that excel at fast image generation  \n    - *note*: must select: settings -> diffusers -> generator device: unset\n    - select from *networks -> reference*\n  - [Playground v1](https://huggingface.co/playgroundai/playground-v1), [Playground v2 256](https://huggingface.co/playgroundai/playground-v2-256px-base), [Playground v2 512](https://huggingface.co/playgroundai/playground-v2-512px-base), [Playground v2 1024](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic) model support  \n    - comparable to SD15 and SD-XL, trained from scratch for highly aesthetic images  \n    - simply select from *networks -> reference* and use as usual  \n  - [BLIP-Diffusion](https://dxli94.github.io/BLIP-Diffusion-website/)  \n    - img2img model that can replace subjects in images using prompt keywords  \n    - download and load by selecting from *networks -> reference -> blip diffusion*\n    - in image tab, select `blip diffusion` script\n  - [DemoFusion](https://github.com/PRIS-CV/DemoFusion) run your SDXL generations at any resolution!  \n    - in **Text** tab select *script* -> *demofusion*  \n    - *note*: GPU VRAM limits do not automatically go away so be careful when using it with large resolutions  \n      in the future, expect more optimizations, especially related to offloading/slicing/tiling,  \n      but at the moment this is pretty much experimental-only  \n  - [AnimateDiff](https://github.com/guoyww/animatediff/)  \n    - overall improved quality  \n    - can now be used with *second pass* - enhance, upscale and hires your videos!  \n  - [IP Adapter](https://github.com/tencent-ailab/IP-Adapter)  \n    - add support for **ip-adapter-plus_sd15, ip-adapter-plus-face_sd15 and ip-adapter-full-face_sd15**  \n    - can now be used in *xyz-grid*  \n  - **Text-to-Video**  \n    - in text tab, select `text-to-video` script  \n    - supported models: **ModelScope v1.7b, ZeroScope v1, ZeroScope v1.1, ZeroScope v2, ZeroScope v2 Dark, Potat v1**  \n      *if you know of any other t2v models youd like to see supported, let me know!*  \n    - models are auto-downloaded on first use  \n    - *note*: current base model will be unloaded to free up resources  \n  - **Prompt scheduling** now implemented for Diffusers backend, thanks @AI-Casanova\n  - **Custom pipelines** contribute by adding your own custom pipelines!  \n    - for details, see fully documented example:  \n      <https://github.com/vladmandic/automatic/blob/dev/scripts/example.py>  \n  - **Schedulers**  \n    - add timesteps range, changing it will make scheduler to be over-complete or under-complete  \n    - add rescale betas with zero SNR option (applicable to Euler, Euler a and DDIM, allows for higher dynamic range)  \n  - **Inpaint**  \n    - improved quality when using mask blur and padding  \n  - **UI**  \n    - 3 new native UI themes: **orchid-dreams**, **emerald-paradise** and **timeless-beige**, thanks @illu_Zn\n    - more dynamic controls depending on the backend (original or diffusers)  \n      controls that are not applicable in current mode are now hidden  \n    - allow setting of resize method directly in image tab  \n      (previously via settings -> upscaler_for_img2img)  \n- **Optional**\n  - **FaceID** face guidance during generation  \n    - also based on IP adapters, but with additional face detection and external embeddings calculation  \n    - calculates face embeds based on input image and uses it to guide generation  \n    - simply select from *scripts -> faceid*  \n    - *experimental module*: requirements must be installed manually:  \n        > pip install insightface ip_adapter  \n  - **Depth 3D** image to 3D scene\n    - delivered as an extension, install from extensions tab  \n      <https://github.com/vladmandic/sd-extension-depth3d>  \n    - creates fully compatible 3D scene from any image by using depth estimation  \n      and creating a fully populated mesh  \n    - scene can be freely viewed in 3D in the UI itself or downloaded for use in other applications  \n  - [ONNX/Olive](https://github.com/vladmandic/automatic/wiki/ONNX-Olive)  \n    - major work continues in olive branch, see wiki for details, thanks @lshqqytiger  \n      as a highlight, 4-5 it/s using DirectML on AMD GPU translates to 23-25 it/s using ONNX/Olive!  \n- **General**  \n  - new **onboarding**  \n    - if no models are found during startup, app will no longer ask to download default checkpoint  \n      instead, it will show message in UI with options to change model path or download any of the reference checkpoints  \n    - *extra networks -> models -> reference* section is now enabled for both original and diffusers backend  \n  - support for **Torch 2.1.2** (release) and **Torch 2.3** (dev)  \n  - **Process** create videos from batch or folder processing  \n      supports *GIF*, *PNG* and *MP4* with full interpolation, scene change detection, etc.  \n  - **LoRA**  \n    - add support for block weights, thanks @AI-Casanova  \n      example `<lora:SDXL_LCM_LoRA:1.0:in=0:mid=1:out=0>`  \n    - add support for LyCORIS GLora networks  \n    - add support for LoRA PEFT (*Diffusers*) networks  \n    - add support for Lora-OFT (*Kohya*) and Lyco-OFT (*Kohaku*) networks  \n    - reintroduce alternative loading method in settings: `lora_force_diffusers`  \n    - add support for `lora_fuse_diffusers` if using alternative method  \n      use if you have multiple complex loras that may be causing performance degradation  \n      as it fuses lora with model during load instead of interpreting lora on-the-fly  \n  - **CivitAI downloader** allow usage of access tokens for download of gated or private models  \n  - **Extra networks** new *settting -> extra networks -> build info on first access*  \n    indexes all networks on first access instead of server startup  \n  - **IPEX**, thanks @disty0  \n    - update to **Torch 2.1**  \n      if you get file not found errors, set `DISABLE_IPEXRUN=1` and run the webui with `--reinstall`  \n    - built-in *MKL* and *DPCPP* for IPEX, no need to install OneAPI anymore  \n    - **StableVideoDiffusion** is now supported with IPEX  \n    - **8 bit support with NNCF** on Diffusers backend  \n    - fix IPEX Optimize not applying with Diffusers backend  \n    - disable 32bit workarounds if the GPU supports 64bit  \n    - add `DISABLE_IPEXRUN` and `DISABLE_IPEX_1024_WA` environment variables  \n    - performance and compatibility improvements  \n  - **OpenVINO**, thanks @disty0  \n    - **8 bit support for CPUs**  \n    - reduce System RAM usage  \n    - update to Torch 2.1.2  \n    - add *Directory for OpenVINO cache* option to *System Paths*  \n    - remove Intel ARC specific 1024x1024 workaround  \n  - **HDR controls**  \n    - batch-aware for enhancement of multiple images or video frames  \n    - available in image tab  \n  - **Logging**\n    - additional *TRACE* logging enabled via specific env variables  \n      see <https://github.com/vladmandic/automatic/wiki/Debug> for details  \n    - improved profiling  \n      use with `--debug --profile`  \n    - log output file sizes  \n  - **Other**  \n    - **API** several minor but breaking changes to API behavior to better align response fields, thanks @Trojaner\n    - **Inpaint** add option `apply_overlay` to control if inpaint result should be applied as overlay or as-is  \n      can remove artifacts and hard edges of inpaint area but also remove some details from original  \n    - **chaiNNer** fix `NaN` issues due to autocast  \n    - **Upscale** increase limit from 4x to 8x given the quality of some upscalers  \n    - **Networks** fix sort  \n    - reduced default **CFG scale** from 6 to 4 to be more out-of-the-box compatibile with LCM/Turbo models\n    - disable google fonts check on server startup  \n    - fix torchvision/basicsr compatibility  \n    - fix styles quick save  \n    - add hdr settings to metadata  \n    - improve handling of long filenames and filenames during batch processing  \n    - do not set preview samples when using via api  \n    - avoid unnecessary resizes in img2img and inpaint  \n    - safe handling of config updates avoid file corruption on I/O errors  \n    - updated `cli/simple-txt2img.py` and `cli/simple-img2img.py` scripts  \n    - save `params.txt` regardless of image save status  \n    - update built-in log monitor in ui, thanks @midcoastal  \n    - major CHANGELOG doc cleanup, thanks @JetVarimax  \n    - major INSTALL doc cleanup, thanks JetVarimax  \n\n## Update for 2023-12-04\n\nWhats new? Native video in SD.Next via both **AnimateDiff** and **Stable-Video-Diffusion** - and including native MP4 encoding and smooth video outputs out-of-the-box, not just animated-GIFs.  \nAlso new is support for **SDXL-Turbo** as well as new **Kandinsky 3** models and cool latent correction via **HDR controls** for any *txt2img* workflows, best-of-class **SDXL model merge** using full ReBasin methods and further mobile UI optimizations.  \n\n- **Diffusers**\n  - **IP adapter**\n    - lightweight native implementation of T2I adapters which can guide generation towards specific image style  \n    - supports most T2I models, not limited to SD 1.5  \n    - models are auto-downloaded on first use\n    - for IP adapter support in *Original* backend, use standard *ControlNet* extension  \n  - **AnimateDiff**\n    - lightweight native implementation of AnimateDiff models:  \n      *AnimateDiff 1.4, 1.5 v1, 1.5 v2, AnimateFace*\n    - supports SD 1.5 only  \n    - models are auto-downloaded on first use  \n    - for video saving support, see video support section\n    - can be combined with IP-Adapter for even better results!  \n    - for AnimateDiff support in *Original* backend, use standard *AnimateDiff* extension  \n  - **HDR latent control**, based on [article](https://huggingface.co/blog/TimothyAlexisVass/explaining-the-sdxl-latent-space#long-prompts-at-high-guidance-scales-becoming-possible)  \n    - in *Advanced* params\n    - allows control of *latent clamping*, *color centering* and *range maximization*  \n    - supported by *XYZ grid*  \n  - [SD21 Turbo](https://huggingface.co/stabilityai/sd-turbo) and [SDXL Turbo](https://huggingface.co/stabilityai/sdxl-turbo) support  \n    - just set CFG scale (0.0-1.0) and steps (1-3) to a very low value  \n    - compatible with original StabilityAI SDXL-Turbo or any of the newer merges\n    - download safetensors or select from networks -> reference\n  - [Stable Video Diffusion](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid) and [Stable Video Diffusion XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt) support  \n    - download using built-in model downloader or simply select from *networks -> reference*  \n      support for manually downloaded safetensors models will be added later  \n    - for video saving support, see video support section\n    - go to *image* tab, enter input image and select *script* -> *stable video diffusion*\n  - [Kandinsky 3](https://huggingface.co/kandinsky-community/kandinsky-3) support  \n    - download using built-in model downloader or simply select from *networks -> reference*  \n    - this model is absolutely massive at 27.5GB at fp16, so be patient  \n    - model params count is at 11.9B (compared to SD-XL at 3.3B) and its trained on mixed resolutions from 256px to 1024px  \n    - use either model offload or sequential cpu offload to be able to use it  \n  - better autodetection of *inpaint* and *instruct* pipelines  \n  - support long seconary prompt for refiner  \n- **Video support**\n  - applies to any model that supports video generation, e.g. AnimateDiff and StableVideoDiffusion  \n  - support for **animated-GIF**, **animated-PNG** and **MP4**  \n  - GIF and PNG can be looped  \n  - MP4 can have additional padding at the start/end as well as motion-aware interpolated frames for smooth playback  \n    interpolation is done using [RIFE](https://arxiv.org/abs/2011.06294) with native implementation in SD.Next  \n    And its fast - interpolation from 16 frames with 10x frames to target 160 frames results takes 2-3sec\n  - output folder for videos is in *settings -> image paths -> video*  \n- **General**  \n  - redesigned built-in profiler  \n    - now includes both `python` and `torch` and traces individual functions  \n    - use with `--debug --profile`  \n  - **model merge** add **SD-XL ReBasin** support, thanks @AI-Casanova  \n  - further UI optimizations for **mobile devices**, thanks @iDeNoh  \n  - log level defaults to info for console and debug for log file  \n  - better prompt display in process tab  \n  - increase maximum lora cache values  \n  - fix extra networks sorting\n  - fix controlnet compatibility issues in original backend  \n  - fix img2img/inpaint paste params  \n  - fix save text file for manually saved images  \n  - fix python 3.9 compatibility issues  \n\n## Update for 2023-11-23\n\nNew release, primarily focused around three major new features: full **LCM** support, completely new **Model Merge** functionality and **Stable-fast** compile support  \nAlso included are several other improvements and large number of hotfixes - see full changelog for details  \n\n- **Diffusers**  \n  - **LCM** support for any *SD 1.5* or *SD-XL* model!  \n    - download [lcm-lora-sd15](https://huggingface.co/latent-consistency/lcm-lora-sdv1-5/tree/main) and/or [lcm-lora-sdxl](https://huggingface.co/latent-consistency/lcm-lora-sdxl/tree/main)  \n    - load for favorite *SD 1.5* or *SD-XL* model *(original LCM was SD 1.5 only, this is both)*  \n    - load **lcm lora** *(note: lcm lora is processed differently than any other lora)*  \n    - set **sampler** to **LCM**  \n    - set number of steps to some low number, for SD-XL 6-7 steps is normally sufficient  \n      note: LCM scheduler does not support steps higher than 50  \n    - set CFG to between 1 and 2  \n  - Add `cli/lcm-convert.py` script to convert any SD 1.5 or SD-XL model to LCM model  \n    by baking in LORA and uploading to Huggingface, thanks @Disty0  \n  - Support for [Stable Fast](https://github.com/chengzeyi/stable-fast) model compile on *Windows/Linux/WSL2* with *CUDA*  \n    See [Wiki:Benchmark](https://github.com/vladmandic/automatic/wiki/Benchmark) for details and comparison  \n    of different backends, precision modes, advanced settings and compile modes  \n    *Hint*: **70+ it/s** is possible on *RTX4090* with no special tweaks  \n  - Add additional pipeline types for manual model loads when loading from `safetensors`  \n  - Updated logic for calculating **steps** when using base/hires/refiner workflows  \n  - Improve **model offloading** for both model and sequential cpu offload when dealing with meta tensors\n  - Safe model offloading for non-standard models  \n  - Fix **DPM SDE** scheduler  \n  - Better support for SD 1.5 **inpainting** models  \n  - Add support for **OpenAI Consistency decoder VAE**\n  - Enhance prompt parsing with long prompts and support for *BREAK* keyword  \n    Change-in-behavior: new line in prompt now means *BREAK*  \n  - Add alternative Lora loading algorithm, triggered if `SD_LORA_DIFFUSERS` is set  \n- **Models**\n  - **Model merge**\n    - completely redesigned, now based on best-of-class `meh` by @s1dlx  \n      and heavily modified for additional functionality and fully integrated by @AI-Casanova (thanks!)  \n    - merge SD or SD-XL models using *simple merge* (12 methods),  \n      using one of *presets* (20 built-in presets) or custom block merge values  \n    - merge with ReBasin permutations and/or clipping protection  \n    - fully multithreaded for fastest merge possible  \n  - **Model update**  \n    - under UI -> Models - Update  \n    - scan existing models for updated metadata on CivitAI and  \n      provide download functionality for models with available  \n- **Extra networks**  \n  - Use multi-threading for 5x load speedup  \n  - Better Lora trigger words support  \n  - Auto refresh styles on change  \n- **General**  \n  - Many **mobile UI** optimizations, thanks @iDeNoh\n  - Support for **Torch 2.1.1** with CUDA 12.1 or CUDA 11.8  \n  - Configurable location for HF cache folder  \n    Default is standard `~/.cache/huggingface/hub`  \n  - Reworked parser when pasting previously generated images/prompts  \n    includes all `txt2img`, `img2img` and `override` params  \n  - Reworked **model compile**\n  - Support custom upscalers in subfolders  \n  - Add additional image info when loading image in process tab  \n  - Better file locking when sharing config and/or models between multiple instances  \n  - Handle custom API endpoints when using auth  \n  - Show logged in user in log when accessing via UI and/or API  \n  - Support `--ckpt none` to skip loading a model  \n- **XYZ grid**\n  - Add refiner options to XYZ Grid  \n  - Add option to create only subgrids in XYZ grid, thanks @midcoastal\n  - Allow custom font, background and text color in settings\n- **Fixes**  \n  - Fix `params.txt` saved before actual image\n  - Fix inpaint  \n  - Fix manual grid image save  \n  - Fix img2img init image save  \n  - Fix upscale in txt2img for batch counts when no hires is used  \n  - More uniform models paths  \n  - Safe scripts callback execution  \n  - Improved extension compatibility  \n  - Improved BF16 support  \n  - Match previews for reference models with downloaded models\n\n## Update for 2023-11-06\n\nAnother pretty big release, this time with focus on new models (3 new model types), new backends and optimizations\nPlus quite a few fixes  \n\nAlso, [Wiki](https://github.com/vladmandic/automatic/wiki) has been updated with new content, so check it out!  \nSome highlights: [OpenVINO](https://github.com/vladmandic/automatic/wiki/OpenVINO), [IntelArc](https://github.com/vladmandic/automatic/wiki/Intel-ARC), [DirectML](https://github.com/vladmandic/automatic/wiki/DirectML), [ONNX/Olive](https://github.com/vladmandic/automatic/wiki/ONNX-Olive)\n\n- **Diffusers**\n  - since now **SD.Next** supports **12** different model types, weve added reference model for each type in  \n    *Extra networks -> Reference* for easier select & auto-download  \n    Models can still be downloaded manually, this is just a convenience feature & a showcase for supported models  \n  - new model type: [Segmind SSD-1B](https://huggingface.co/segmind/SSD-1B)  \n    its a *distilled* model trained at 1024px, this time 50% smaller and faster version of SD-XL!  \n    (and quality does not suffer, its just more optimized)  \n    test shows batch-size:4 with 1k images at full quality used less than 6.5GB of VRAM  \n    and for further optimization, you can use built-in **TAESD** decoder,  \n    which results in batch-size:16 with 1k images using 7.9GB of VRAM\n    select from extra networks -> reference or download using built-in **Huggingface** downloader: `segmind/SSD-1B`  \n  - new model type: [Pixart- XL 2](https://github.com/PixArt-alpha/PixArt-alpha)  \n    in medium/512px and large/1024px variations  \n    comparable in quality to SD 1.5 and SD-XL, but with better text encoder and highly optimized training pipeline  \n    so finetunes can be done in as little as 10% compared to SD/SD-XL (note that due to much larger text encoder, it is a large model)  \n    select from extra networks -> reference or download using built-in **Huggingface** downloader: `PixArt-alpha/PixArt-XL-2-1024-MS`  \n  - new model type: [LCM: Latent Consistency Models](https://github.com/openai/consistency_models)  \n    trained at 512px, but with near-instant generate in a as little as 3 steps!  \n    combined with OpenVINO, generate on CPU takes less than 5-10 seconds: <https://www.youtube.com/watch?v=b90ESUTLsRo>  \n    and absolute beast when combined with **HyperTile** and **TAESD** decoder resulting in **28 FPS**  \n    (on RTX4090 for batch 16x16 at 512px)  \n    note: set sampler to **Default** before loading model as LCM comes with its own *LCMScheduler* sampler  \n    select from extra networks -> reference or download using built-in **Huggingface** downloader: `SimianLuo/LCM_Dreamshaper_v7`  \n  - support for **Custom pipelines**, thanks @disty0  \n    download using built-in **Huggingface** downloader  \n    think of them as plugins for diffusers not unlike original extensions that modify behavior of `ldm` backend  \n    list of community pipelines: <https://github.com/huggingface/diffusers/blob/main/examples/community/README.md>  \n  - new custom pipeline: `Disty0/zero123plus-pipeline`, thanks @disty0  \n    generate 4 output images with different camera positions: front, side, top, back!  \n    for more details, see <https://github.com/vladmandic/automatic/discussions/2421>  \n  - new backend: **ONNX/Olive** *(experimental)*, thanks @lshqqytiger  \n    for details, see [WiKi](https://github.com/vladmandic/automatic/wiki/ONNX-Runtime)\n  - extend support for [Free-U](https://github.com/ChenyangSi/FreeU)  \n    improve generations quality at no cost (other than finding params that work for you)  \n- **General**  \n  - attempt to auto-fix invalid samples which occur due to math errors in lower precision  \n    example: `RuntimeWarning: invalid value encountered in cast: sample = sample.astype(np.uint8)`  \n    begone **black images** *(note: if it proves as working, this solution will need to be expanded to cover all scenarios)*  \n  - add **Lora OFT** support, thanks @antis0007 and @ai-casanova  \n  - **Upscalers**  \n    - **compile** option, thanks @disty0  \n    - **chaiNNer** add high quality models from [Helaman](https://openmodeldb.info/users/helaman)  \n  - redesigned **Progress bar** with full details on current operation  \n  - new option: *settings -> images -> keep incomplete*  \n    can be used to skip vae decode on aborted/skipped/interrupted image generations  \n  - new option: *settings -> system paths -> models*  \n    can be used to set custom base path for *all* models (previously only as cli option)  \n  - remove external clone of items in `/repositories`  \n  - **Interrogator** module has been removed from `extensions-builtin`  \n    and fully implemented (and improved) natively  \n- **UI**  \n  - UI tweaks for default themes  \n  - UI switch core font in default theme to **noto-sans**  \n    previously default font was simply *system-ui*, but it lead to too much variations between browsers and platforms  \n  - UI tweaks for mobile devices, thanks @iDeNoh  \n  - updated **Context menu**  \n    right-click on any button in action menu (e.g. generate button)  \n- **Extra networks**  \n  - sort by name, size, date, etc.  \n  - switch between *gallery* and *list* views  \n  - add tags from user metadata (in addition to tags in model metadata) for **lora**  \n  - added **Reference** models for diffusers backend  \n  - faster enumeration of all networks on server startup  \n- **Packages**\n  - updated `diffusers` to 0.22.0, `transformers` to 4.34.1  \n  - update **openvino**, thanks @disty0  \n  - update **directml**, @lshqqytiger  \n- **Compute**  \n  - **OpenVINO**:  \n    - updated to mainstream `torch` *2.1.0*  \n    - support for **ESRGAN** upscalers  \n- **Fixes**  \n  - fix **freeu** for backend original and add it to xyz grid  \n  - fix loading diffuser models in huggingface format from non-standard location  \n  - fix default styles looking in wrong location  \n  - fix missing upscaler folder on initial startup  \n  - fix handling of relative path for models  \n  - fix simple live preview device mismatch  \n  - fix batch img2img  \n  - fix diffusers samplers: dpm++ 2m, dpm++ 1s, deis  \n  - fix new style filename template  \n  - fix image name template using model name  \n  - fix image name sequence  \n  - fix model path using relative path  \n  - fix safari/webkit layour, thanks @eadnams22\n  - fix `torch-rocm` and `tensorflow-rocm` version detection, thanks @xangelix  \n  - fix **chainner** upscalers color clipping  \n  - fix for base+refiner workflow in diffusers mode: number of steps, diffuser pipe mode  \n  - fix for prompt encoder with refiner in diffusers mode  \n  - fix prompts-from-file saving incorrect metadata  \n  - fix add/remove extra networks to prompt\n  - fix before-hires step  \n  - fix diffusers switch from invalid model  \n  - force second requirements check on startup  \n  - remove **lyco**, multiple_tqdm  \n  - enhance extension compatibility for extensions directly importing codeformers  \n  - enhance extension compatibility for extensions directly accessing processing params  \n  - **css** fixes  \n  - clearly mark external themes in ui  \n  - update `typing-extensions`  \n\n## Update for 2023-10-17\n\nThis is a major release, with many changes and new functionality...  \n\nChangelog is massive, but do read through or youll be missing on some very cool new functionality  \nor even free speedups and quality improvements (regardless of which workflows youre using)!  \n\nNote that for this release its recommended to perform a clean install (e.g. fresh `git clone`)  \nUpgrades are still possible and supported, but clean install is recommended for best experience  \n\n- **UI**  \n  - added **change log** to UI  \n    see *System -> Changelog*  \n  - converted submenus from checkboxes to accordion elements  \n    any ui state including state of open/closed menus can be saved as default!  \n    see *System -> User interface -> Set menu states*  \n  - new built-in theme **invoked**  \n    thanks @BinaryQuantumSoul  \n  - add **compact view** option in settings -> user interface  \n  - small visual indicator bottom right of page showing internal server job state  \n- **Extra networks**:  \n  - **Details**  \n    - new details interface to view and save data about extra networks  \n      main ui now has a single button on each en to trigger details view  \n    - details view includes model/lora metadata parser!  \n    - details view includes civitai model metadata!  \n  - **Metadata**:  \n    - you can scan [civitai](https://civitai.com/)  \n      for missing metadata and previews directly from extra networks  \n      simply click on button in top-right corner of extra networks page  \n  - **Styles**  \n    - save/apply icons moved to extra networks  \n    - can be edited in details view  \n    - support for single or multiple styles per json  \n    - support for embedded previews  \n    - large database of art styles included by default  \n      can be disabled in *settings -> extra networks -> show built-in*  \n    - styles can also be used in a prompt directly: `<style:style_name>`  \n      if style if an exact match, it will be used  \n      otherwise it will rotate between styles that match the start of the name  \n      that way you can use different styles as wildcards when processing batches  \n    - styles can have **extra** fields, not just prompt and negative prompt  \n      for example: *\"Extra: sampler: Euler a, width: 480, height: 640, steps: 30, cfg scale: 10, clip skip: 2\"*\n  - **VAE**  \n    - VAEs are now also listed as part of extra networks  \n    - Image preview methods have been redesigned: simple, approximate, taesd, full  \n      please set desired preview method in settings  \n    - both original and diffusers backend now support \"full quality\" setting  \n      if you desired model or platform does not support FP16 and/or you have a low-end hardware and cannot use FP32  \n      you can disable \"full quality\" in advanced params and it will likely reduce decode errors (infamous black images)  \n  - **LoRA**  \n    - LoRAs are now automatically filtered based on compatibility with currently loaded model  \n      note that if lora type cannot be auto-determined, it will be left in the list  \n  - **Refiner**  \n    - you can load model from extra networks as base model or as refiner  \n      simply select button in top-right of models page  \n  - **General**  \n    - faster search, ability to show/hide/sort networks  \n    - refactored subfolder handling  \n      *note*: this will trigger model hash recalculation on first model use  \n- **Diffusers**:  \n  - better pipeline **auto-detect** when loading from safetensors  \n  - **SDXL Inpaint**  \n    - although any model can be used for inpainiting, there is a case to be made for  \n      dedicated inpainting models as they are tuned to inpaint and not generate  \n    - model can be used as base model for **img2img** or refiner model for **txt2img**  \n      To download go to *Models -> Huggingface*:  \n      - `diffusers/stable-diffusion-xl-1.0-inpainting-0.1` *(6.7GB)*  \n  - **SDXL Instruct-Pix2Pix**  \n    - model can be used as base model for **img2img** or refiner model for **txt2img**  \n      this model is massive and requires a lot of resources!  \n      to download go to *Models -> Huggingface*:  \n      - `diffusers/sdxl-instructpix2pix-768` *(11.9GB)*  \n  - **SD Latent Upscale**  \n    - you can use *SD Latent Upscale* models as **refiner models**  \n      this is a bit experimental, but it works quite well!  \n      to download go to *Models -> Huggingface*:  \n      - `stabilityai/sd-x2-latent-upscaler` *(2.2GB)*  \n      - `stabilityai/stable-diffusion-x4-upscaler` *(1.7GB)*  \n  - better **Prompt attention**  \n    should better handle more complex prompts  \n    for sdxl, choose which part of prompt goes to second text encoder - just add `TE2:` separator in the prompt  \n    for hires and refiner, second pass prompt is used if present, otherwise primary prompt is used  \n    new option in *settings -> diffusers -> sdxl pooled embeds*  \n    thanks @AI-Casanova  \n  - better **Hires** support for SD and SDXL  \n  - better **TI embeddings** support for SD and SDXL  \n    faster loading, wider compatibility and support for embeddings with multiple vectors  \n    information about used embedding is now also added to image metadata  \n    thanks @AI-Casanova  \n  - better **Lora** handling  \n    thanks @AI-Casanova  \n  - better **SDXL preview** quality (approx method)  \n    thanks @BlueAmulet\n  - new setting: *settings -> diffusers -> force inpaint*  \n    as some models behave better when in *inpaint* mode even for normal *img2img* tasks  \n- **Upscalers**:\n  - pretty much a rewrite and tons of new upscalers - built-in list is now at **42**  \n  - fix long outstanding memory leak in legacy code, amazing this went undetected for so long  \n  - more high quality upscalers available by default  \n    **SwinIR** (2), **ESRGAN** (12), **RealESRGAN** (6), **SCUNet** (2)  \n  - if that is not enough, there is new **chaiNNer** integration:  \n    adds 15 more upscalers from different families out-of-the-box:  \n    **HAT** (6), **RealHAT** (2), **DAT** (1), **RRDBNet** (1), **SPSRNet** (1), **SRFormer** (2), **SwiftSR** (2)  \n    and yes, you can download and add your own, just place them in `models/chaiNNer`  \n  - two additional latent upscalers based on SD upscale models when using Diffusers backend  \n    **SD Upscale 2x**, **SD Upscale 4x***  \n    note: Recommended usage for *SD Upscale* is by using second pass instead of upscaler  \n    as it allows for tuning of prompt, seed, sampler settings which are used to guide upscaler  \n  - upscalers are available in **xyz grid**  \n  - simplified *settings->postprocessing->upscalers*  \n    e.g. all upsamplers share same settings for tiling  \n  - allow upscale-only as part of **txt2img** and **img2img** workflows  \n    simply set *denoising strength* to 0 so hires does not get triggered  \n  - unified init/download/execute/progress code  \n  - easier installation  \n- **Samplers**:  \n  - moved ui options to submenu  \n  - default list for new installs is now all samplers, list can be modified in settings  \n  - simplified samplers configuration in settings  \n    plus added few new ones like sigma min/max which can highly impact sampler behavior  \n  - note that list of samplers is now *different* since keeping a flat-list of all possible  \n    combinations results in 50+ samplers which is not practical  \n    items such as algorithm (e.g. karras) is actually a sampler option, not a sampler itself  \n- **CivitAI**:\n  - civitai model download is now multithreaded and resumable  \n    meaning that you can download multiple models in parallel  \n    as well as resume aborted/incomplete downloads  \n  - civitai integration in *models -> civitai* can now find most  \n    previews AND metadata for most models (checkpoints, loras, embeddings)  \n    metadata is now parsed and saved in *[model].json*  \n    typical hit rate is >95% for models, loras and embeddings  \n  - description from parsed model metadata is used as model description if there is no manual  \n    description file present in format of *[model].txt*  \n  - to enable search for models, make sure all models have set hash values  \n    *Models -> Valida -> Calculate hashes*  \n- **LoRA**\n  - new unified LoRA handler for all LoRA types (lora, lyco, loha, lokr, locon, ia3, etc.)  \n    applies to both original and diffusers backend  \n    thanks @AI-Casanova for diffusers port  \n  - for *backend:original*, separate lyco handler has been removed  \n- **Compute**  \n  - **CUDA**:  \n    - default updated to `torch` *2.1.0* with cuda *12.1*  \n    - testing moved to `torch` *2.2.0-dev/cu122*  \n    - check out *generate context menu -> show nvml* for live gpu stats (memory, power, temp, clock, etc.)\n  - **Intel Arc/IPEX**:  \n    - tons of optimizations, built-in binary wheels for Windows  \n      i have to say, intel arc/ipex is getting to be quite a player, especially with openvino  \n      thanks @Disty0 @Nuullll  \n  - **AMD ROCm**:  \n    - updated installer to support detect `ROCm` *5.4/5.5/5.6/5.7*  \n    - support for `torch-rocm-5.7`\n  - **xFormers**:\n    - default updated to *0.0.23*  \n    - note that latest xformers are still not compatible with cuda 12.1  \n      recommended to use torch 2.1.0 with cuda 11.8  \n      if you attempt to use xformers with cuda 12.1, it will force a full xformers rebuild on install  \n      which can take a very long time and may/may-not work  \n    - added cmd param `--use-xformers` to force usage of exformers  \n  - **GC**:  \n    - custom garbage collect threshold to reduce vram memory usage, thanks @Disty0  \n      see *settings -> compute -> gc*  \n- **Inference**  \n  - new section in **settings**  \n    - [HyperTile](https://github.com/tfernd/HyperTile): new!  \n      available for *diffusers* and *original* backends  \n      massive (up to 2x) speed-up your generations for free :)  \n      *note: hypertile is not compatible with any extension that modifies processing parameters such as resolution*  \n      thanks @tfernd\n    - [Free-U](https://github.com/ChenyangSi/FreeU): new!  \n      available for *diffusers* and *original* backends  \n      improve generations quality at no cost (other than finding params that work for you)  \n      *note: temporarily disabled for diffusers pending release of diffusers==0.22*  \n      thanks @ljleb  \n    - [Token Merging](https://github.com/dbolya/tomesd): not new, but updated  \n      available for *diffusers* and *original* backends  \n      speed-up your generations by merging redundant tokens  \n      speed up will depend on how aggressive you want to be with token merging  \n    - **Batch mode**  \n      new option *settings -> inference -> batch mode*  \n      when using img2img process batch, optionally process multiple images in batch in parallel  \n      thanks @Symbiomatrix\n- **NSFW Detection/Censor**  \n  - install extension: [NudeNet](https://github.com/vladmandic/sd-extension-nudenet)  \n    body part detection, image metadata, advanced censoring, etc...  \n    works for *text*, *image* and *process* workflows  \n    more in the extension notes  \n- **Extensions**\n  - automatic discovery of new extensions on github  \n    no more waiting for them to appear in index!\n  - new framework for extension validation  \n    extensions ui now shows actual status of extensions for reviewed extensions  \n    if you want to contribute/flag/update extension status, reach out on github or discord  \n  - better overall compatibility with A1111 extensions (up to a point)  \n  - [MultiDiffusion](https://github.com/pkuliyi2015/multidiffusion-upscaler-for-automatic1111)  \n    has been removed from list of built-in extensions  \n    you can still install it manually if desired  \n  - [LyCORIS]<https://github.com/KohakuBlueleaf/a1111-sd-webui-lycoris>  \n    has been removed from list of built-in extensions  \n    it is considered obsolete given that all functionality is now built-in  \n- **General**  \n  - **Startup**  \n    - all main CLI parameters can now be set as environment variable as well  \n      for example `--data-dir <path>` can be specified as `SD_DATADIR=<path>` before starting SD.Next  \n  - **XYZ Grid**\n    - more flexibility to use selection or strings  \n  - **Logging**  \n    - get browser session info in server log  \n    - allow custom log file destination  \n      see `webui --log`  \n    - when running with `--debug` flag, log is force-rotated  \n      so each `sdnext.log.*` represents exactly one server run  \n    - internal server job state tracking  \n  - **Launcher**  \n    - new `webui.ps1` powershell launcher for windows (old `webui.bat` is still valid)  \n      thanks @em411  \n  - **API**\n    - add end-to-end example how to use API: `cli/simple-txt2img.js`  \n      covers txt2img, upscale, hires, refiner  \n  - **train.py**\n    - wrapper script around built-in **kohyas lora** training script  \n      see `cli/train.py --help`  \n      new support for sd and sdxl, thanks @evshiron  \n      new support for full offline mode (without sdnext server running)  \n- **Themes**\n  - all built-in themes are fully supported:  \n    - *black-teal (default), light-teal, black-orange, invoked, amethyst-nightfall, midnight-barbie*  \n  - if youre using any **gradio default** themes or a **3rd party** theme or  that are not optimized for SD.Next, you may experience issues  \n    default minimal style has been updated for compatibility, but actual styling is completely outside of SD.Next control  \n\n## Update for 2023-09-13\n\nStarted as a mostly a service release with quite a few fixes, but then...  \nMajor changes how **hires** works as well as support for a very interesting new model [Wuerstchen](https://huggingface.co/blog/wuertschen)  \n\n- tons of fixes  \n- changes to **hires**  \n  - enable non-latent upscale modes (standard upscalers)  \n  - when using latent upscale, hires pass is run automatically  \n  - when using non-latent upscalers, hires pass is skipped by default  \n    enabled using **force hires** option in ui  \n    hires was not designed to work with standard upscalers, but i understand this is a common workflow  \n  - when using refiner, upscale/hires runs before refiner pass  \n  - second pass can now also utilize full/quick vae quality  \n  - note that when combining non-latent upscale, hires and refiner output quality is maximum,  \n    but operations are really resource intensive as it includes: *base->decode->upscale->encode->hires->refine*\n  - all combinations of: decode full/quick + upscale none/latent/non-latent + hires on/off + refiner on/off  \n    should be supported, but given the number of combinations, issues are possible  \n  - all operations are captured in image metadata\n- diffusers:\n  - allow loading of sd/sdxl models from safetensors without online connectivity\n  - support for new model: [wuerstchen](https://huggingface.co/warp-ai/wuerstchen)  \n    its a high-resolution model (1024px+) thats ~40% faster than sd-xl with a bit lower resource requirements  \n    go to *models -> huggingface -> search \"warp-ai/wuerstchen\" -> download*  \n    its nearly 12gb in size, so be patient :)\n- minor re-layout of the main ui  \n- updated **ui hints**  \n- updated **models -> civitai**  \n  - search and download loras  \n  - find previews for already downloaded models or loras  \n- new option **inference mode**  \n  - default is standard `torch.no_grad`  \n    new option is `torch.inference_only` which is slightly faster and uses less vram, but only works on some gpus  \n- new cmdline param `--no-metadata`  \n  skips reading metadata from models that are not already cached  \n- updated **gradio**  \n- **styles** support for subfolders  \n- **css** optimizations\n- clean-up **logging**  \n  - capture system info in startup log  \n  - better diagnostic output  \n  - capture extension output  \n  - capture ldm output  \n  - cleaner server restart  \n  - custom exception handling\n\n## Update for 2023-09-06\n\nOne week later, another large update!\n\n- system:  \n  - full **python 3.11** support  \n    note that changing python version does require reinstall  \n    and if youre already on python 3.10, really no need to upgrade  \n- themes:  \n  - new default theme: **black-teal**  \n  - new light theme: **light-teal**  \n  - new additional theme: **midnight-barbie**, thanks @nyxia  \n- extra networks:  \n  - support for **tags**  \n    show tags on hover, search by tag, list tags, add to prompt, etc.  \n  - **styles** are now also listed as part of extra networks  \n    existing `styles.csv` is converted upon startup to individual styles inside `models/style`  \n    this is stage one of new styles functionality  \n    old styles interface is still available, but will be removed in future  \n  - cache file lists for much faster startup  \n    speedups are 50+% for large number of extra networks  \n  - ui refresh button now refreshes selected page, not all pages  \n  - simplified handling of **descriptions**  \n    now shows on-mouse-over without the need for user interaction  \n  - **metadata** and **info** buttons only show if there is actual content  \n- diffusers:  \n  - add full support for **textual inversions** (embeddings)  \n    this applies to both sd15 and sdxl  \n    thanks @ai-casanova for porting compel/sdxl code  \n  - mix&match **base** and **refiner** models (*experimental*):  \n    most of those are \"because why not\" and can result in corrupt images, but some are actually useful  \n    also note that if youre not using actual refiner model, you need to bump refiner steps  \n    as normal models are not designed to work with low step count  \n    and if youre having issues, try setting prompt parser to \"fixed attention\" as majority of problems  \n    are due to token mismatches when using prompt attention  \n    - any sd15 + any sd15  \n    - any sd15 + sdxl-refiner  \n    - any sdxl-base + sdxl-refiner  \n    - any sdxl-base + any sd15  \n    - any sdxl-base + any sdxl-base  \n  - ability to **interrupt** (stop/skip) model generate  \n  - added **aesthetics score** setting (for sdxl)  \n    used to automatically guide unet towards higher pleasing images  \n    highly recommended for simple prompts  \n  - added **force zeros** setting  \n    create zero-tensor for prompt if prompt is empty (positive or negative)  \n- general:  \n  - `rembg` remove backgrounds support for **is-net** model  \n  - **settings** now show markers for all items set to non-default values  \n  - **metadata** refactored how/what/when metadata is added to images  \n    should result in much cleaner and more complete metadata  \n  - pre-create all system folders on startup  \n  - handle model load errors gracefully  \n  - improved vram reporting in ui  \n  - improved script profiling (when running in debug mode)  \n\n## Update for 2023-08-30\n\nTime for a quite a large update that has been leaking bit-by-bit over the past week or so...  \n*Note*: due to large changes, it is recommended to reset (delete) your `ui-config.json`  \n\n- diffusers:  \n  - support for **distilled** sd models  \n    just go to models/huggingface and download a model, for example:  \n    `segmind/tiny-sd`, `segmind/small-sd`, `segmind/portrait-finetuned`  \n    those are lower quality, but extremely small and fast  \n    up to 50% faster than sd 1.5 and execute in as little as 2.1gb of vram  \n- general:  \n  - redesigned **settings**  \n    - new layout with separated sections:  \n      *settings, ui config, licenses, system info, benchmark, models*  \n    - **system info** tab is now part of settings  \n      when running outside of sdnext, system info is shown in main ui  \n    - all system and image paths are now relative by default  \n    - add settings validation when performing load/save  \n    - settings tab in ui now shows settings that are changed from default values  \n    - settings tab switch to compact view  \n  - update **gradio** major version  \n    this may result in some smaller layout changes since its a major version change  \n    however, browser page load is now much faster  \n  - optimizations:\n    - optimize model hashing  \n    - add cli param `--skip-all` that skips all installer checks  \n      use at personal discretion, but it can be useful for bulk deployments  \n    - add model **precompile** option (when model compile is enabled)  \n    - **extra network** folder info caching  \n      results in much faster startup when you have large number of extra networks  \n    - faster **xyz grid** switching  \n      especially when using different checkpoints  \n  - update **second pass** options for clarity\n  - models:\n    - civitai download missing model previews\n  - add **openvino** (experimental) cpu optimized model compile and inference  \n    enable with `--use-openvino`  \n    thanks @disty0  \n  - enable batch **img2img** scale-by workflows  \n    now you can batch process with rescaling based on each individual original image size  \n  - fixes:\n    - fix extra networks previews  \n    - css fixes  \n    - improved extensions compatibility (e.g. *sd-cn-animation*)  \n    - allow changing **vae** on-the-fly for both original and diffusers backend\n\n## Update for 2023-08-20\n\nAnother release thats been baking in dev branch for a while...\n\n- general:\n  - caching of extra network information to enable much faster create/refresh operations  \n    thanks @midcoastal\n- diffusers:\n  - add **hires** support (*experimental*)  \n    applies to all model types that support img2img, including **sd** and **sd-xl**  \n    also supports all hires upscaler types as well as standard params like steps and denoising strength  \n    when used with **sd-xl**, it can be used with or without refiner loaded  \n    how to enable - there are no explicit checkboxes other than second pass itself:\n    - hires: upscaler is set and target resolution is not at default  \n    - refiner: if refiner model is loaded  \n  - images save options: *before hires*, *before refiner*\n  - redo `move model to cpu` logic in settings -> diffusers to be more reliable  \n    note that system defaults have also changed, so you may need to tweak to your liking  \n  - update dependencies\n\n## Update for 2023-08-17\n\nSmaller update, but with some breaking changes (to prepare for future larger functionality)...\n\n- general:\n  - update all metadata saved with images  \n    see <https://github.com/vladmandic/automatic/wiki/Metadata> for details  \n  - improved **amd** installer with support for **navi 2x & 3x** and **rocm 5.4/5.5/5.6**  \n    thanks @evshiron  \n  - fix **img2img** resizing (applies to *original, diffusers, hires*)  \n  - config change: main `config.json` no longer contains entire configuration  \n    but only differences from defaults (similar to recent change performed to `ui-config.json`)  \n- diffusers:\n  - enable **batch img2img** workflows  \n- original:  \n  - new samplers: **dpm++ 3M sde** (standard and karras variations)  \n    enable in *settings -> samplers -> show samplers*\n  - expose always/never discard penultimate sigma  \n    enable in *settings -> samplers*  \n\n## Update for 2023-08-11\n\nThis is a big one thats been cooking in `dev` for a while now, but finally ready for release...\n\n- diffusers:\n  - **pipeline autodetect**\n    if pipeline is set to autodetect (default for new installs), app will try to autodetect pipeline based on selected model  \n    this should reduce user errors such as loading **sd-xl** model when **sd** pipeline is selected  \n  - **quick vae decode** as alternative to full vae decode which is very resource intensive  \n    quick decode is based on `taesd` and produces lower quality, but its great for tests or grids as it runs much faster and uses far less vram  \n    disabled by default, selectable in *txt2img/img2img -> advanced -> full quality*  \n  - **prompt attention** for sd and sd-xl  \n    supports both `full parser` and native `compel`  \n    thanks @ai-casanova  \n  - advanced **lora load/apply** methods  \n    in addition to standard lora loading that was recently added to sd-xl using diffusers, now we have  \n    - **sequential apply** (load & apply multiple loras in sequential manner) and  \n    - **merge and apply** (load multiple loras and merge before applying to model)  \n    see *settings -> diffusers -> lora methods*  \n    thanks @hameerabbasi and @ai-casanova  \n  - **sd-xl vae** from safetensors now applies correct config  \n    result is that 3rd party vaes can be used without washed out colors  \n  - options for optimized memory handling for lower memory usage  \n    see *settings -> diffusers*\n- general:\n  - new **civitai model search and download**  \n    native support for civitai, integrated into ui as *models -> civitai*  \n  - updated requirements  \n    this time its a bigger change so upgrade may take longer to install new requirements\n  - improved **extra networks** performance with large number of networks\n\n## Update for 2023-08-05\n\nAnother minor update, but it unlocks some cool new items...\n\n- diffusers:\n  - vaesd live preview (sd and sd-xl)  \n  - fix inpainting (sd and sd-xl)  \n- general:\n  - new torch 2.0 with ipex (intel arc)  \n  - additional callbacks for extensions  \n    enables latest comfyui extension  \n\n## Update for 2023-07-30\n\nSmaller release, but IMO worth a post...\n\n- diffusers:\n  - sd-xl loras are now supported!\n  - memory optimizations: Enhanced sequential CPU offloading, model CPU offload, FP16 VAE\n    - significant impact if running SD-XL (for example, but applies to any model) with only 8GB VRAM\n  - update packages\n- minor bugfixes\n\n## Update for 2023-07-26\n\nThis is a big one, new models, new diffusers, new features and updated UI...\n\nFirst, **SD-XL 1.0** is released and yes, SD.Next supports it out of the box!\n\n- [SD-XL Base](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0.safetensors)\n- [SD-XL Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/blob/main/sd_xl_refiner_1.0.safetensors)\n\nAlso fresh is new **Kandinsky 2.2** model that does look quite nice:\n\n- [Kandinsky Decoder](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder)\n- [Kandinsky Prior](https://huggingface.co/kandinsky-community/kandinsky-2-2-prior)\n\nActual changelog is:\n\n- general:\n  - new loading screens and artwork\n  - major ui simplification for both txt2img and img2img  \n    nothing is removed, but you can show/hide individual sections  \n    default is very simple interface, but you can enable any sections and save it as default in settings  \n  - themes: add additional built-in theme, `amethyst-nightfall`\n  - extra networks: add add/remove tags to prompt (e.g. lora activation keywords)\n  - extensions: fix couple of compatibility items\n  - firefox compatibility improvements\n  - minor image viewer improvements\n  - add backend and operation info to metadata\n\n- diffusers:\n  - were out of experimental phase and diffusers backend is considered stable  \n  - sd-xl: support for **sd-xl 1.0** official model\n  - sd-xl: loading vae now applies to both base and refiner and saves a bit of vram  \n  - sd-xl: denoising_start/denoising_end\n  - sd-xl: enable dual prompts  \n    dual prompt is used if set regardless if refiner is enabled/loaded  \n    if refiner is loaded & enabled, refiner prompt will also be used for refiner pass  \n    - primary prompt goes to [OpenAI CLIP-ViT/L-14](https://huggingface.co/openai/clip-vit-large-patch14)\n    - refiner prompt goes to [OpenCLIP-ViT/bigG-14](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)\n  - **kandinsky 2.2** support  \n    note: kandinsky model must be downloaded using model downloader, not as safetensors due to specific model format  \n  - refiner: fix batch processing\n  - vae: enable loading of pure-safetensors vae files without config  \n    also enable *automatic* selection to work with diffusers  \n  - sd-xl: initial lora support  \n    right now this applies to official lora released by **stability-ai**, support for **kohyas** lora is expected soon  \n  - implement img2img and inpainting (experimental)  \n    actual support and quality depends on model  \n    it works as expected for sd 1.5, but not so much for sd-xl for now  \n  - implement limited stop/interrupt for diffusers\n    works between stages, not within steps  \n  - add option to save image before refiner pass  \n  - option to set vae upcast in settings  \n  - enable fp16 vae decode when using optimized vae  \n    this pretty much doubles performance of decode step (delay after generate is done)  \n\n- original\n  - fix hires secondary sampler  \n    this now fully obsoletes `fallback_sampler` and `force_hr_sampler_name`  \n\n\n## Update for 2023-07-18\n\nWhile were waiting for official SD-XL release, heres another update with some fixes and enhancements...\n\n- **global**\n  - image save: option to add invisible image watermark to all your generated images  \n    disabled by default, can be enabled in settings -> image options  \n    watermark information will be shown when loading image such as in process image tab  \n    also additional cli utility `/cli/image-watermark.py` to read/write/strip watermarks from images  \n  - batch processing: fix metadata saving, also allow to drag&drop images for batch processing  \n  - ui configuration: you can modify all ui default values from settings as usual,  \n    but only values that are non-default will be written to `ui-config.json`  \n  - startup: add cmd flag to skip all `torch` checks  \n  - startup: force requirements check on each server start  \n    there are too many misbehaving extensions that change system requirements  \n  - internal: safe handling of all config file read/write operations  \n    this allows sdnext to run in fully shared environments and prevents any possible configuration corruptions  \n- **diffusers**:\n  - sd-xl: remove image watermarks autocreated by 0.9 model  \n  - vae: enable loading of external vae, documented in diffusers wiki  \n    and mix&match continues, you can even use sd-xl vae with sd 1.5 models!  \n  - samplers: add concept of *default* sampler to avoid needing to tweak settings for primary or second pass  \n    note that sampler details will be printed in log when running in debug level  \n  - samplers: allow overriding of sampler beta values in settings  \n  - refiner: fix refiner applying only to first image in batch  \n  - refiner: allow using direct latents or processed output in refiner  \n  - model: basic support for one more model: [UniDiffuser](https://github.com/thu-ml/unidiffuser)  \n    download using model downloader: `thu-ml/unidiffuser-v1`  \n    and set resolution to 512x512  \n\n## Update for 2023-07-14\n\nTrying to unify settings for both original and diffusers backend without introducing duplicates...\n\n- renamed **hires fix** to **second pass**  \n  as that is what it actually is, name hires fix is misleading to start with  \n- actual **hires fix** and **refiner** are now options inside **second pass** section  \n- obsoleted settings -> sampler -> **force_hr_sampler_name**  \n  it is now part of **second pass** options and it works the same for both original and diffusers backend  \n  which means you can use different scheduler settings for txt2img and hires if you want  \n- sd-xl refiner will run if its loaded and if second pass is enabled  \n  so you can quickly enable/disable refiner by simply enabling/disabling second pass  \n- you can mix&match **model** and **refiner**  \n  for example, you can generate image using sd 1.5 and still use sd-xl refiner as second pass  \n- reorganized settings -> samplers to show which section refers to which backend  \n- added diffusers **lmsd** sampler  \n\n## Update for 2023-07-13\n\nAnother big one, but now improvements to both **diffusers** and **original** backends as well plus ability to dynamically switch between them!\n\n- swich backend between diffusers and original on-the-fly\n  - you can still use `--backend <backend>` and now that only means in which mode app will start,\n    but you can change it anytime in ui settings\n  - for example, you can even do things like generate image using sd-xl,  \n    then switch to original backend and perform inpaint using a different model  \n- diffusers backend:\n  - separate ui settings for refiner pass with sd-xl  \n    you can specify: prompt, negative prompt, steps, denoise start  \n  - fix loading from pure safetensors files  \n    now you can load sd-xl from safetensors file or from huggingface folder format  \n  - fix kandinsky model (2.1 working, 2.2 was just released and will be soon)  \n- original backend:\n  - improvements to vae/unet handling as well as cross-optimization heads  \n    in non-technical terms, this means lower memory usage and higher performance  \n    and you should be able to generate higher resolution images without any other changes\n- other:\n  - major refactoring of the javascript code  \n    includes fixes for text selections and navigation  \n  - system info tab now reports on nvidia driver version as well  \n  - minor fixes in extra-networks  \n  - installer handles origin changes for submodules  \n\nbig thanks to @huggingface team for great communication, support and fixing all the reported issues asap!\n\n\n## Update for 2023-07-10\n\nService release with some fixes and enhancements:\n\n- diffusers:\n  - option to move base and/or refiner model to cpu to free up vram  \n  - model downloader options to specify model variant / revision / mirror  \n  - now you can download `fp16` variant directly for reduced memory footprint  \n  - basic **img2img** workflow (*sketch* and *inpaint* are not supported yet)  \n    note that **sd-xl** img2img workflows are architecturaly different so it will take longer to implement  \n  - updated hints for settings  \n- extra networks:\n  - fix corrupt display on refesh when new extra network type found  \n  - additional ui tweaks  \n  - generate thumbnails from previews only if preview resolution is above 1k\n- image viewer:\n  - fixes for non-chromium browsers and mobile users and add option to download image  \n  - option to download image directly from image viewer\n- general\n  - fix startup issue with incorrect config  \n  - installer should always check requirements on upgrades\n\n## Update for 2023-07-08\n\nThis is a massive update which has been baking in a `dev` branch for a while now\n\n- merge experimental diffusers support  \n\n*TL;DR*: Yes, you can run **SD-XL** model in **SD.Next** now  \nFor details, see Wiki page: [Diffusers](https://github.com/vladmandic/automatic/wiki/Diffusers)  \nNote this is still experimental, so please follow Wiki  \nAdditional enhancements and fixes will be provided over the next few days  \n*Thanks to @huggingface team for making this possible and our internal @team for all the early testing*\n\nRelease also contains number of smaller updates:\n\n- add pan & zoom controls (touch and mouse) to image viewer (lightbox)  \n- cache extra networks between tabs  \n  this should result in neat 2x speedup on building extra networks  \n- add settings -> extra networks -> do not automatically build extra network pages  \n  speeds up app start if you have a lot of extra networks and you want to build them manually when needed  \n- extra network ui tweaks  \n\n## Update for 2023-07-01\n\nSmall quality-of-life updates and bugfixes:\n\n- add option to disallow usage of ckpt checkpoints\n- change lora and lyco dir without server restart\n- additional filename template fields: `uuid`, `seq`, `image_hash`  \n- image toolbar is now shown only when image is present\n- image `Zip` button gone and its not optional setting that applies to standard `Save` button\n- folder `Show` button is present only when working on localhost,  \n  otherwise its replaced with `Copy` that places image URLs on clipboard so they can be used in other apps\n\n## Update for 2023-06-30\n\nA bit bigger update this time, but contained to specific areas...\n\n- change in behavior  \n  extensions no longer auto-update on startup  \n  using `--upgrade` flag upgrades core app as well as all submodules and extensions  \n- **live server log monitoring** in ui  \n  configurable via settings -> live preview  \n- new **extra networks interface**  \n  *note: if youre using a 3rd party ui extension for extra networks, it will likely need to be updated to work with new interface*\n  - display in front of main ui, inline with main ui or as a sidebar  \n  - lazy load thumbnails  \n    drastically reduces load times for large number of extra networks  \n  - auto-create thumbnails from preview images in extra networks in a background thread  \n    significant load time saving on subsequent restarts  \n  - support for info files in addition to description files  \n  - support for variable aspect-ratio thumbnails  \n  - new folder view  \n- **extensions sort** by trending  \n- add requirements check for training  \n\n## Update for 2023-06-26\n\n- new training tab interface  \n  - redesigned preprocess, train embedding, train hypernetwork  \n- new models tab interface  \n  - new model convert functionality, thanks @akegarasu  \n  - new model verify functionality  \n- lot of ipex specific fixes/optimizations, thanks @disty0  \n\n## Update for 2023-06-20\n\nThis one is less relevant for standard users, but pretty major if youre running an actual server  \nBut even if not, it still includes bunch of cumulative fixes since last release - and going by number of new issues, this is probably the most stable release so far...\n(next one is not going to be as stable, but it will be fun :) )\n\n- minor improvements to extra networks ui  \n- more hints/tooltips integrated into ui  \n- new dedicated api server  \n  - but highly promising for high throughput server  \n- improve server logging and monitoring with  \n  - server log file rotation  \n  - ring buffer with api endpoint `/sdapi/v1/log`  \n  - real-time status and load endpoint `/sdapi/v1/system-info/status`\n\n## Update for 2023-06-14\n\nSecond stage of a jumbo merge from upstream plus few minor changes...\n\n- simplify token merging  \n- reorganize some settings  \n- all updates from upstream: **A1111** v1.3.2 [df004be] *(latest release)*  \n  pretty much nothing major that i havent released in previous versions, but its still a long list of tiny changes  \n  - skipped/did-not-port:  \n    add separate hires prompt: unnecessarily complicated and spread over large number of commits due to many regressions  \n    allow external scripts to add cross-optimization methods: dangerous and i dont see a use case for it so far  \n    load extension info in threads: unnecessary as other optimizations ive already put place perform equally good  \n  - broken/reverted:  \n    sub-quadratic optimization changes  \n\n## Update for 2023-06-13\n\nJust a day later and one *bigger update*...\nBoth some **new functionality** as well as **massive merges** from upstream  \n\n- new cache for models/lora/lyco metadata: `metadata.json`  \n  drastically reduces disk access on app startup  \n- allow saving/resetting of **ui default values**  \n  settings -> ui defaults\n- ability to run server without loaded model  \n  default is to auto-load model on startup, can be changed in settings -> stable diffusion  \n  if disabled, model will be loaded on first request, e.g. when you click generate  \n  useful when you want to start server to perform other tasks like upscaling which do not rely on model  \n- updated `accelerate` and `xformers`\n- huge nubmer of changes ported from **A1111** upstream  \n  this was a massive merge, hopefully this does not cause any regressions  \n  and still a bit more pending...\n\n## Update for 2023-06-12\n\n- updated ui labels and hints to improve clarity and provide some extra info  \n  this is 1st stage of the process, more to come...  \n  if you want to join the effort, see <https://github.com/vladmandic/automatic/discussions/1246>\n- new localization and hints engine  \n  how hints are displayed can be selected in settings -> ui  \n- reworked **installer** sequence  \n  as some extensions are loading packages directly from their preload sequence  \n  which was preventing some optimizations to take effect  \n- updated **settings** tab functionality, thanks @gegell  \n  with real-time monitor for all new and/or updated settings  \n- **launcher** will now warn if application owned files are modified  \n  you are free to add any user files, but do not modify app files unless youre sure in what youre doing  \n- add more profiling for scripts/extensions so you can see what takes time  \n  this applies both to initial load as well as execution  \n- experimental `sd_model_dict` setting which allows you to load model dictionary  \n  from one model and apply weights from another model specified in `sd_model_checkpoint`  \n  results? who am i to judge :)\n\n\n## Update for 2023-06-05\n\nFew new features and extra handling for broken extensions  \nthat caused my phone to go crazy with notifications over the weekend...\n\n- added extra networks to **xyz grid** options  \n  now you can have more fun with all your embeddings and loras :)  \n- new **vae decode** method to help with larger batch sizes, thanks @bigdog  \n- new setting -> lora -> **use lycoris to handle all lora types**  \n  this is still experimental, but the goal is to obsolete old built-in lora module  \n  as it doesnt understand many new loras and built-in lyco module can handle it all  \n- somewhat optimize browser page loading  \n  still slower than id want, but gradio is pretty bad at this  \n- profiling of scripts/extensions callbacks  \n  you can now see how much or pre/post processing is done, not just how long generate takes  \n- additional exception handling so bad exception does not crash main app  \n- additional background removal models  \n- some work on bfloat16 which nobody really should be using, but why not \n\n\n## Update for 2023-06-02\n\nSome quality-of-life improvements while working on larger stuff in the background...\n\n- redesign action box to be uniform across all themes  \n- add **pause** option next to stop/skip  \n- redesigned progress bar  \n- add new built-in extension: **agent-scheduler**  \n  very elegant way to getting full queueing capabilities, thank @artventurdev  \n- enable more image formats  \n  note: not all are understood by browser so previews and images may appear as blank  \n  unless you have some browser extensions that can handle them  \n  but they are saved correctly. and cant beat raw quality of 32-bit `tiff` or `psd` :)  \n- change in behavior: `xformers` will be uninstalled on startup if they are not active  \n  if you do have `xformers` selected as your desired cross-optimization method, then they will be used  \n  reason is that a lot of libaries try to blindly import xformers even if they are not selected or not functional  \n\n## Update for 2023-05-30\n\nAnother bigger one...And more to come in the next few days...\n\n- new live preview mode: taesd  \n  i really like this one, so its enabled as default for new installs  \n- settings search feature  \n- new sampler: dpm++ 2m sde  \n- fully common save/zip/delete (new) options in all tabs  \n  which (again) meant rework of process image tab  \n- system info tab: live gpu utilization/memory graphs for nvidia gpus  \n- updated controlnet interface  \n- minor style changes  \n- updated lora, swinir, scunet and ldsr code from upstream  \n- start of merge from a1111 v1.3  \n\n## Update for 2023-05-26\n\nSome quality-of-life improvements...\n\n- updated [README](https://github.com/vladmandic/automatic/blob/master/README.md)\n- created [CHANGELOG](https://github.com/vladmandic/automatic/blob/master/CHANGELOG.md)  \n  this will be the source for all info about new things moving forward  \n  and cross-posted to [Discussions#99](https://github.com/vladmandic/automatic/discussions/99) as well as discord [announcements](https://discord.com/channels/1101998836328697867/1109953953396957286)\n- optimize model loading on startup  \n  this should reduce startup time significantly  \n- set default cross-optimization method for each platform backend  \n  applicable for new installs only  \n  - `cuda` => Scaled-Dot-Product\n  - `rocm` => Sub-quadratic\n  - `directml` => Sub-quadratic\n  - `ipex` => invokeais\n  - `mps` => Doggettxs\n  - `cpu` => Doggettxs\n- optimize logging  \n- optimize profiling  \n  now includes startup profiling as well as `cuda` profiling during generate  \n- minor lightbox improvements  \n- bugfixes...i dont recall when was a release with at least several of those  \n\nother than that - first stage of [Diffusers](https://github.com/huggingface/diffusers) integration is now in master branch  \ni dont recommend anyone to try it (and dont even think reporting issues for it)  \nbut if anyone wants to contribute, take a look at [project page](https://github.com/users/vladmandic/projects/1/views/1)\n\n## Update for 2023-05-23\n\nMajor internal work with perhaps not that much user-facing to show for it ;)\n\n- update core repos: **stability-ai**, **taming-transformers**, **k-diffusion, blip**, **codeformer**  \n  note: to avoid disruptions, this is applicable for new installs only\n- tested with **torch 2.1**, **cuda 12.1**, **cudnn 8.9**  \n  (production remains on torch2.0.1+cuda11.8+cudnn8.8)  \n- fully extend support of `--data-dir`  \n  allows multiple installations to share pretty much everything, not just models  \n  especially useful if you want to run in a stateless container or cloud instance  \n- redo api authentication  \n  now api authentication will use same user/pwd (if specified) for ui and strictly enforce it using httpbasicauth  \n  new authentication is also fully supported in combination with ssl for both sync and async calls  \n  if you want to use api programatically, see examples in `cli/sdapi.py`  \n- add dark/light theme mode toggle  \n- redo some `clip-skip` functionality  \n- better matching for vae vs model  \n- update to `xyz grid` to allow creation of large number of images without creating grid itself  \n- update `gradio` (again)  \n- more prompt parser optimizations  \n- better error handling when importing image settings which are not compatible with current install  \n  for example, when upscaler or sampler originally used is not available  \n- fixes...amazing how many issues were introduced by porting a1111 v1.20 code without adding almost no new functionality  \n  next one is v1.30 (still in dev) which does bring a lot of new features  \n\n## Update for 2023-05-17\n\nThis is a massive one due to huge number of changes,  \nbut hopefully it will go ok...\n\n- new **prompt parsers**  \n  select in UI -> Settings -> Stable Diffusion  \n  - **Full**: my new implementation  \n  - **A1111**: for backward compatibility  \n  - **Compel**: as used in ComfyUI and InvokeAI (a.k.a *Temporal Weighting*)  \n  - **Fixed**: for really old backward compatibility  \n- monitor **extensions** install/startup and  \n  log if they modify any packages/requirements  \n  this is a *deep-experimental* python hack, but i think its worth it as extensions modifying requirements  \n  is one of most common causes of issues\n- added `--safe` command line flag mode which skips loading user extensions  \n  please try to use it before opening new issue  \n- reintroduce `--api-only` mode to start server without ui  \n- port *all* upstream changes from [A1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui)  \n  up to today - commit hash `89f9faa`  \n\n## Update for 2023-05-15\n\n- major work on **prompt parsing**\n  this can cause some differences in results compared to what youre used to, but its all about fixes & improvements\n  - prompt parser was adding commas and spaces as separate words and tokens and/or prefixes\n  - negative prompt weight using `[word:weight]` was ignored, it was always `0.909`\n  - bracket matching was anything but correct. complex nested attention brackets are now working.\n  - btw, if you run with `--debug` flag, youll now actually see parsed prompt & schedule\n- updated all scripts in `/cli`  \n- add option in settings to force different **latent sampler** instead of using primary only\n- add **interrupt/skip** capabilities to process images\n\n## Update for 2023-05-13\n\nThis is mostly about optimizations...\n\n- improved `torch-directml` support  \n  especially interesting for **amd** users on **windows**  where **torch+rocm** is not yet available  \n  dont forget to run using `--use-directml` or default is **cpu**  \n- improved compatibility with **nvidia** rtx 1xxx/2xxx series gpus  \n- fully working `torch.compile` with **torch 2.0.1**  \n  using `inductor` compile takes a while on first run, but does result in 5-10% performance increase  \n- improved memory handling  \n  for highest performance, you can also disable aggressive **gc** in settings  \n- improved performance  \n  especially *after* generate as image handling has been moved to separate thread  \n- allow per-extension updates in extension manager  \n- option to reset configuration in settings  \n\n## Update for 2023-05-11\n\n- brand new **extension manager**  \n  this is pretty much a complete rewrite, so new issues are possible\n- support for `torch` 2.0.1  \n  note that if you are experiencing frequent hangs, this may be a worth a try  \n- updated `gradio` to 3.29.0\n- added `--reinstall` flag to force reinstall of all packages  \n- auto-recover & re-attempt when `--upgrade` is requested but fails\n- check for duplicate extensions  \n\n## Update for 2023-05-08\n\nBack online with few updates:\n\n- bugfixes. yup, quite a lot of those  \n- auto-detect some cpu/gpu capabilities on startup  \n  this should reduce need to tweak and tune settings like no-half, no-half-vae, fp16 vs fp32, etc  \n- configurable order of top level tabs  \n- configurable order of scripts in txt2img and img2img  \n  for both, see sections in ui-> settings -> user interface\n\n## Update for 2023-05-04\n\nAgain, few days later...\n\n- reviewed/ported **all** commits from **A1111** upstream  \n  some a few are not applicable as i already have alternative implementations  \n  and very few i choose not to implement (save/restore last-known-good-config is a bad hack)  \n  otherwise, were fully up to date (it doesnt show on fork status as code merges were mostly manual due to conflicts)  \n  but...due to sheer size of the updates, this may introduce some temporary issues  \n- redesigned server restart function  \n  now available and working in ui  \n  actually, since server restart is now a true restart and not ui restart, it can be used much more flexibly  \n- faster model load  \n  plus support for slower devices via stream-load function (in ui settings)  \n- better logging  \n  this includes new `--debug` flag for more verbose logging when troubleshooting  \n\n## Update for 2023-05-01\n\nBeen a bit quieter for last few days as changes were quite significant, but finally here we are...\n\n- Updated core libraries: Gradio, Diffusers, Transformers\n- Added support for **Intel ARC** GPUs via Intel OneAPI IPEX (auto-detected)\n- Added support for **TorchML** (set by default when running on non-compatible GPU or on CPU)\n- Enhanced support for AMD GPUs with **ROCm**\n- Enhanced support for Apple **M1/M2**\n- Redesigned command params: run `webui --help` for details\n- Redesigned API and script processing\n- Experimental support for multiple **Torch compile** options\n- Improved sampler support\n- Google Colab: <https://colab.research.google.com/drive/126cDNwHfifCyUpCCQF9IHpEdiXRfHrLN>  \n  Maintained by <https://github.com/Linaqruf/sd-notebook-collection>\n- Fixes, fixes, fixes...\n\nTo take advantage of new out-of-the-box tunings, its recommended to delete your `config.json` so new defaults are applied. its not necessary, but otherwise you may need to play with UI Settings to get the best of Intel ARC, TorchML, ROCm or Apple M1/M2.\n\n## Update for 2023-04-27\n\na bit shorter list as:\n\n- ive been busy with bugfixing  \n  there are a lot of them, not going to list each here.  \n  but seems like critical issues backlog is quieting down and soon i can focus on new features development.  \n- ive started collaboration with couple of major projects,\n  hopefully this will accelerate future development.\n\nwhats new:\n\n- ability to view/add/edit model description shown in extra networks cards  \n- add option to specify fallback sampler if primary sampler is not compatible with desired operation  \n- make clip skip a local parameter  \n- remove obsolete items from UI settings  \n- set defaults for AMD ROCm  \n  if you have issues, you may want to start with a fresh install so configuration can be created from scratch\n- set defaults for Apple M1/M2  \n  if you have issues, you may want to start with a fresh install so configuration can be created from scratch\n\n## Update for 2023-04-25\n\n- update process image -> info\n- add VAE info to metadata\n- update GPU utility search paths for better GPU type detection\n- update git flags for wider compatibility\n- update environment tuning\n- update ti training defaults\n- update VAE search paths\n- add compatibility opts for some old extensions\n- validate script args for always-on scripts  \n  fixes: deforum with controlnet  \n\n## Update for 2023-04-24\n\n- identify race condition where generate locks up while fetching preview\n- add pulldowns to x/y/z script\n- add VAE rollback feature in case of NaNs\n- use samples format for live preview\n- add token merging\n- use **Approx NN** for live preview\n- create default `styles.csv`\n- fix setup not installing `tensorflow` dependencies\n- update default git flags to reduce number of warnings\n\n## Update for 2023-04-23\n\n- fix VAE dtype  \n  should fix most issues with NaN or black images  \n- add built-in Gradio themes  \n- reduce requirements  \n- more AMD specific work\n- initial work on Apple platform support\n- additional PR merges\n- handle torch cuda crashing in setup\n- fix setup race conditions\n- fix ui lightbox\n- mark tensorflow as optional\n- add additional image name templates\n\n## Update for 2023-04-22\n\n- autodetect which system libs should be installed  \n  this is a first pass of autoconfig for **nVidia** vs **AMD** environments  \n- fix parse cmd line args from extensions  \n- only install `xformers` if actually selected as desired cross-attention method\n- do not attempt to use `xformers` or `sdp` if running on cpu\n- merge tomesd token merging  \n- merge 23 PRs pending from a1111 backlog (!!)\n\n*expect shorter updates for the next few days as ill be partially ooo*\n\n## Update for 2023-04-20\n\n- full CUDA tuning section in UI Settings\n- improve exif/pnginfo metadata parsing  \n  it can now handle 3rd party images or images edited in external software\n- optimized setup performance and logging\n- improve compatibility with some 3rd party extensions\n  for example handle extensions that install packages directly from github urls\n- fix initial model download if no models found\n- fix vae not found issues\n- fix multiple git issues\n\nnote: if you previously had command line optimizations such as --no-half, those are now ignored and moved to ui settings\n\n## Update for 2023-04-19\n\n- fix live preview\n- fix model merge\n- fix handling of user-defined temp folders\n- fix submit benchmark\n- option to override `torch` and `xformers` installer\n- separate benchmark data for system-info extension\n- minor css fixes\n- created initial merge backlog from pending prs on a1111 repo  \n  see #258 for details\n\n## Update for 2023-04-18\n\n- reconnect ui to active session on browser restart  \n  this is one of most frequently asked for items, finally figured it out  \n  works for text and image generation, but not for process as there is no progress bar reported there to start with  \n- force unload `xformers` when not used  \n  improves compatibility with AMD/M1 platforms  \n- add `styles.csv` to UI settings to allow customizing path  \n- add `--skip-git` to cmd flags for power users that want  \n  to skip all git checks and operations and perform manual updates\n- add `--disable-queue` to cmd flags that disables Gradio queues (experimental)\n  this forces it to use HTTP instead of WebSockets and can help on unreliable network connections  \n- set scripts & extensions loading priority and allow custom priorities  \n  fixes random extension issues:  \n  `ScuNet` upscaler disappearing, `Additional Networks` not showing up on XYZ axis, etc.\n- improve html loading order\n- remove some `asserts` causing runtime errors and replace with user-friendly messages\n- update README.md\n\n## Update for 2023-04-17\n\n- **themes** are now dynamic and discovered from list of available gradio themes on huggingface  \n  its quite a list of 30+ supported themes so far  \n- added option to see **theme preview** without the need to apply it or restart server\n- integrated **image info** functionality into **process image** tab and removed separate **image info** tab\n- more installer improvements\n- fix urls\n- updated github integration\n- make model download as optional if no models found\n\n## Update for 2023-04-16\n\n- support for ui themes! to to *settings* -> *user interface* -> \"ui theme*\n  includes 12 predefined themes\n- ability to restart server from ui\n- updated requirements\n- removed `styles.csv` from repo, its now fully under user control\n- removed model-keyword extension as overly aggressive\n- rewrite of the fastapi middleware handlers\n- install bugfixes, hopefully new installer is now ok  \\\n  i really want to focus on features and not troubleshooting installer\n\n## Update for 2023-04-15\n\n- update default values\n- remove `ui-config.json` from repo, its now fully under user control\n- updated extensions manager\n- updated locon/lycoris plugin\n- enable quick launch by default\n- add multidiffusion upscaler extensions\n- add model keyword extension\n- enable strong linting\n- fix circular imports\n- fix extensions updated\n- fix git update issues\n- update github templates\n\n## Update for 2023-04-14\n\n- handle duplicate extensions\n- redo exception handler\n- fix generate forever\n- enable cmdflags compatibility\n- change default css font\n- fix ti previews on initial start\n- enhance tracebacks\n- pin transformers version to last known good version\n- fix extension loader\n\n## Update for 2023-04-12\n\nThis has been pending for a while, but finally uploaded some massive changes\n\n- New launcher\n  - `webui.bat` and `webui.sh`:  \n    Platform specific wrapper scripts that starts `launch.py` in Python virtual environment  \n    *Note*: Server can run without virtual environment, but it is recommended to use it  \n    This is carry-over from original repo  \n    **If youre unsure which launcher to use, this is the one you want**  \n  - `launch.py`:  \n    Main startup script  \n    Can be used directly to start server in manually activated `venv` or to run it without `venv`  \n  - `installer.py`:  \n    Main installer, used by `launch.py`  \n  - `webui.py`:  \n    Main server script  \n- New logger\n- New exception handler\n- Built-in performance profiler\n- New requirements handling\n- Move of most of command line flags into UI Settings\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.7763671875,
          "content": "cff-version: 1.2.0\ntitle: SD.Next\nurl: 'https://github.com/vladmandic/automatic'\nmessage: >-\n  If you use this software, please cite it using the\n  metadata from this file\ntype: software\nauthors:\n  - given-names: Vladimir\n    name-particle: Vlado\n    family-names: Mandic\n    orcid: 'https://orcid.org/0009-0003-4592-5074'\nidentifiers:\n  - type: url\n    value: 'https://github.com/vladmandic'\n    description: GitHub\n  - type: url\n    value: 'https://www.linkedin.com/in/cyan051/'\n    description: LinkedIn\nrepository-code: 'https://github.com/vladmandic/automatic'\nabstract: >-\n  SD.Next: Advanced Implementation of Stable Diffusion and\n  other diffusion models for text, image and video\n  generation\nkeywords:\n  - stablediffusion diffusers sdnext\nlicense: Apache-2.0\ndate-released: 2022-12-24\n"
        },
        {
          "name": "CODE_OF_CONDUCT",
          "type": "blob",
          "size": 0.921875,
          "content": "# Code of Conduct\n\nUse your best judgement  \nIf it will possibly make others uncomfortable, do not post it\n\n- Be respectful  \n  Disagreement is not an opportunity to attack someone else's thoughts or opinions  \n  Although views may differ, remember to approach every situation with patience and care  \n- Be considerate  \n  Think about how your contribution will affect others in the community  \n- Be open minded  \n  Embrace new people and new ideas. Our community is continually evolving and we welcome positive change  \n\nBe mindful of your language  \nAny of the following behavior is unacceptable:\n\n- Offensive comments of any kind\n- Threats or intimidation\n- Or any other kinds of harassment\n\nIf you believe someone is violating the code of conduct, we ask that you report it\n\nParticipants asked to stop any harassing behavior are expected to comply immediately  \n\n<br>\n\n## Usage Restrictions\n\nSee [LICENSE](LICENSE.txt) for more information\n"
        },
        {
          "name": "CONTRIBUTING",
          "type": "blob",
          "size": 0.6845703125,
          "content": "# Contributing Guidelines\n\nPull requests from everyone are welcome\n\nProcedure for contributing:\n\n- Create a fork of the repository on github  \n  In a top right corner of a GitHub, select \"Fork\"\n  Its recommended to fork latest version from main branch to avoid any possible conflicting code updates\n- Clone your forked repository to your local system  \n  `git clone https://github.com/<your-username>/<your-fork>\n- Make your changes  \n- Test your changes\n- Test your changes against code guidelines  \n  - `ruff check`\n  - `pylint <folder>/<filename>.py`\n- Push changes to your fork  \n- Submit a PR (pull request)\n\nYour pull request will be reviewed and pending review results, merged into main branch\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 2.16015625,
          "content": "# SD.Next Dockerfile\n# docs: <https://github.com/vladmandic/automatic/wiki/Docker>\n\n# base image\nFROM pytorch/pytorch:2.5.1-cuda12.4-cudnn9-runtime\n\n# metadata\nLABEL org.opencontainers.image.vendor=\"SD.Next\"\nLABEL org.opencontainers.image.authors=\"vladmandic\"\nLABEL org.opencontainers.image.url=\"https://github.com/vladmandic/automatic/\"\nLABEL org.opencontainers.image.documentation=\"https://github.com/vladmandic/automatic/wiki/Docker\"\nLABEL org.opencontainers.image.source=\"https://github.com/vladmandic/automatic/\"\nLABEL org.opencontainers.image.licenses=\"AGPL-3.0\"\nLABEL org.opencontainers.image.title=\"SD.Next\"\nLABEL org.opencontainers.image.description=\"SD.Next: Advanced Implementation of Stable Diffusion and other Diffusion-based generative image models\"\nLABEL org.opencontainers.image.base.name=\"https://hub.docker.com/pytorch/pytorch:2.5.1-cuda12.4-cudnn9-runtime\"\nLABEL org.opencontainers.image.version=\"latest\"\n\n# minimum install\nRUN [\"apt-get\", \"-y\", \"update\"]\nRUN [\"apt-get\", \"-y\", \"install\", \"git\", \"build-essential\", \"google-perftools\", \"curl\"]\n# optional if full cuda-dev is required by some downstream library\n# RUN [\"apt-get\", \"-y\", \"nvidia-cuda-toolkit\"]\nRUN [\"/usr/sbin/ldconfig\"]\n\n# copy sdnext\nCOPY . /app\nWORKDIR /app\n\n# stop pip and uv from caching\nENV PIP_NO_CACHE_DIR=true\nENV PIP_ROOT_USER_ACTION=ignore\nENV UV_NO_CACHE=true\n# disable model hashing for faster startup\nENV SD_NOHASHING=true\n# set data directories\nENV SD_DATADIR=\"/mnt/data\"\nENV SD_MODELSDIR=\"/mnt/models\"\nENV SD_DOCKER=true\n\n# tcmalloc is not required but it is highly recommended\nENV LD_PRELOAD=libtcmalloc.so.4  \n# sdnext will run all necessary pip install ops and then exit\nRUN [\"python\", \"/app/launch.py\", \"--debug\", \"--uv\", \"--use-cuda\", \"--log\", \"sdnext.log\", \"--test\", \"--optional\"]\n# preinstall additional packages to avoid installation during runtime\n\n# actually run sdnext\nCMD [\"python\", \"launch.py\", \"--debug\", \"--skip-all\", \"--listen\", \"--quick\", \"--api-log\", \"--log\", \"sdnext.log\"]\n\n# expose port\nEXPOSE 7860\n\n# healthcheck function\n# HEALTHCHECK --interval=60s --timeout=10s --start-period=60s --retries=3 CMD curl --fail http://localhost:7860/sdapi/v1/status || exit 1\n\n# stop signal\nSTOPSIGNAL SIGINT\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 33.7138671875,
          "content": "                    GNU AFFERO GENERAL PUBLIC LICENSE\n                       Version 3, 19 November 2007\n\n Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The GNU Affero General Public License is a free, copyleft license for\nsoftware and other kinds of works, specifically designed to ensure\ncooperation with the community in the case of network server software.\n\n  The licenses for most software and other practical works are designed\nto take away your freedom to share and change the works.  By contrast,\nour General Public Licenses are intended to guarantee your freedom to\nshare and change all versions of a program--to make sure it remains free\nsoftware for all its users.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthem if you wish), that you receive source code or can get it if you\nwant it, that you can change the software or use pieces of it in new\nfree programs, and that you know you can do these things.\n\n  Developers that use our General Public Licenses protect your rights\nwith two steps: (1) assert copyright on the software, and (2) offer\nyou this License which gives you legal permission to copy, distribute\nand/or modify the software.\n\n  A secondary benefit of defending all users' freedom is that\nimprovements made in alternate versions of the program, if they\nreceive widespread use, become available for other developers to\nincorporate.  Many developers of free software are heartened and\nencouraged by the resulting cooperation.  However, in the case of\nsoftware used on network servers, this result may fail to come about.\nThe GNU General Public License permits making a modified version and\nletting the public access it on a server without ever releasing its\nsource code to the public.\n\n  The GNU Affero General Public License is designed specifically to\nensure that, in such cases, the modified source code becomes available\nto the community.  It requires the operator of a network server to\nprovide the source code of the modified version running there to the\nusers of that server.  Therefore, public use of a modified version, on\na publicly accessible server, gives the public access to the source\ncode of the modified version.\n\n  An older license, called the Affero General Public License and\npublished by Affero, was designed to accomplish similar goals.  This is\na different license, not a version of the Affero GPL, but Affero has\nreleased a new version of the Affero GPL which permits relicensing under\nthis license.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                       TERMS AND CONDITIONS\n\n  0. Definitions.\n\n  \"This License\" refers to version 3 of the GNU Affero General Public License.\n\n  \"Copyright\" also means copyright-like laws that apply to other kinds of\nworks, such as semiconductor masks.\n\n  \"The Program\" refers to any copyrightable work licensed under this\nLicense.  Each licensee is addressed as \"you\".  \"Licensees\" and\n\"recipients\" may be individuals or organizations.\n\n  To \"modify\" a work means to copy from or adapt all or part of the work\nin a fashion requiring copyright permission, other than the making of an\nexact copy.  The resulting work is called a \"modified version\" of the\nearlier work or a work \"based on\" the earlier work.\n\n  A \"covered work\" means either the unmodified Program or a work based\non the Program.\n\n  To \"propagate\" a work means to do anything with it that, without\npermission, would make you directly or secondarily liable for\ninfringement under applicable copyright law, except executing it on a\ncomputer or modifying a private copy.  Propagation includes copying,\ndistribution (with or without modification), making available to the\npublic, and in some countries other activities as well.\n\n  To \"convey\" a work means any kind of propagation that enables other\nparties to make or receive copies.  Mere interaction with a user through\na computer network, with no transfer of a copy, is not conveying.\n\n  An interactive user interface displays \"Appropriate Legal Notices\"\nto the extent that it includes a convenient and prominently visible\nfeature that (1) displays an appropriate copyright notice, and (2)\ntells the user that there is no warranty for the work (except to the\nextent that warranties are provided), that licensees may convey the\nwork under this License, and how to view a copy of this License.  If\nthe interface presents a list of user commands or options, such as a\nmenu, a prominent item in the list meets this criterion.\n\n  1. Source Code.\n\n  The \"source code\" for a work means the preferred form of the work\nfor making modifications to it.  \"Object code\" means any non-source\nform of a work.\n\n  A \"Standard Interface\" means an interface that either is an official\nstandard defined by a recognized standards body, or, in the case of\ninterfaces specified for a particular programming language, one that\nis widely used among developers working in that language.\n\n  The \"System Libraries\" of an executable work include anything, other\nthan the work as a whole, that (a) is included in the normal form of\npackaging a Major Component, but which is not part of that Major\nComponent, and (b) serves only to enable use of the work with that\nMajor Component, or to implement a Standard Interface for which an\nimplementation is available to the public in source code form.  A\n\"Major Component\", in this context, means a major essential component\n(kernel, window system, and so on) of the specific operating system\n(if any) on which the executable work runs, or a compiler used to\nproduce the work, or an object code interpreter used to run it.\n\n  The \"Corresponding Source\" for a work in object code form means all\nthe source code needed to generate, install, and (for an executable\nwork) run the object code and to modify the work, including scripts to\ncontrol those activities.  However, it does not include the work's\nSystem Libraries, or general-purpose tools or generally available free\nprograms which are used unmodified in performing those activities but\nwhich are not part of the work.  For example, Corresponding Source\nincludes interface definition files associated with source files for\nthe work, and the source code for shared libraries and dynamically\nlinked subprograms that the work is specifically designed to require,\nsuch as by intimate data communication or control flow between those\nsubprograms and other parts of the work.\n\n  The Corresponding Source need not include anything that users\ncan regenerate automatically from other parts of the Corresponding\nSource.\n\n  The Corresponding Source for a work in source code form is that\nsame work.\n\n  2. Basic Permissions.\n\n  All rights granted under this License are granted for the term of\ncopyright on the Program, and are irrevocable provided the stated\nconditions are met.  This License explicitly affirms your unlimited\npermission to run the unmodified Program.  The output from running a\ncovered work is covered by this License only if the output, given its\ncontent, constitutes a covered work.  This License acknowledges your\nrights of fair use or other equivalent, as provided by copyright law.\n\n  You may make, run and propagate covered works that you do not\nconvey, without conditions so long as your license otherwise remains\nin force.  You may convey covered works to others for the sole purpose\nof having them make modifications exclusively for you, or provide you\nwith facilities for running those works, provided that you comply with\nthe terms of this License in conveying all material for which you do\nnot control copyright.  Those thus making or running the covered works\nfor you must do so exclusively on your behalf, under your direction\nand control, on terms that prohibit them from making any copies of\nyour copyrighted material outside their relationship with you.\n\n  Conveying under any other circumstances is permitted solely under\nthe conditions stated below.  Sublicensing is not allowed; section 10\nmakes it unnecessary.\n\n  3. Protecting Users' Legal Rights From Anti-Circumvention Law.\n\n  No covered work shall be deemed part of an effective technological\nmeasure under any applicable law fulfilling obligations under article\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\nsimilar laws prohibiting or restricting circumvention of such\nmeasures.\n\n  When you convey a covered work, you waive any legal power to forbid\ncircumvention of technological measures to the extent such circumvention\nis effected by exercising rights under this License with respect to\nthe covered work, and you disclaim any intention to limit operation or\nmodification of the work as a means of enforcing, against the work's\nusers, your or third parties' legal rights to forbid circumvention of\ntechnological measures.\n\n  4. Conveying Verbatim Copies.\n\n  You may convey verbatim copies of the Program's source code as you\nreceive it, in any medium, provided that you conspicuously and\nappropriately publish on each copy an appropriate copyright notice;\nkeep intact all notices stating that this License and any\nnon-permissive terms added in accord with section 7 apply to the code;\nkeep intact all notices of the absence of any warranty; and give all\nrecipients a copy of this License along with the Program.\n\n  You may charge any price or no price for each copy that you convey,\nand you may offer support or warranty protection for a fee.\n\n  5. Conveying Modified Source Versions.\n\n  You may convey a work based on the Program, or the modifications to\nproduce it from the Program, in the form of source code under the\nterms of section 4, provided that you also meet all of these conditions:\n\n    a) The work must carry prominent notices stating that you modified\n    it, and giving a relevant date.\n\n    b) The work must carry prominent notices stating that it is\n    released under this License and any conditions added under section\n    7.  This requirement modifies the requirement in section 4 to\n    \"keep intact all notices\".\n\n    c) You must license the entire work, as a whole, under this\n    License to anyone who comes into possession of a copy.  This\n    License will therefore apply, along with any applicable section 7\n    additional terms, to the whole of the work, and all its parts,\n    regardless of how they are packaged.  This License gives no\n    permission to license the work in any other way, but it does not\n    invalidate such permission if you have separately received it.\n\n    d) If the work has interactive user interfaces, each must display\n    Appropriate Legal Notices; however, if the Program has interactive\n    interfaces that do not display Appropriate Legal Notices, your\n    work need not make them do so.\n\n  A compilation of a covered work with other separate and independent\nworks, which are not by their nature extensions of the covered work,\nand which are not combined with it such as to form a larger program,\nin or on a volume of a storage or distribution medium, is called an\n\"aggregate\" if the compilation and its resulting copyright are not\nused to limit the access or legal rights of the compilation's users\nbeyond what the individual works permit.  Inclusion of a covered work\nin an aggregate does not cause this License to apply to the other\nparts of the aggregate.\n\n  6. Conveying Non-Source Forms.\n\n  You may convey a covered work in object code form under the terms\nof sections 4 and 5, provided that you also convey the\nmachine-readable Corresponding Source under the terms of this License,\nin one of these ways:\n\n    a) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by the\n    Corresponding Source fixed on a durable physical medium\n    customarily used for software interchange.\n\n    b) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by a\n    written offer, valid for at least three years and valid for as\n    long as you offer spare parts or customer support for that product\n    model, to give anyone who possesses the object code either (1) a\n    copy of the Corresponding Source for all the software in the\n    product that is covered by this License, on a durable physical\n    medium customarily used for software interchange, for a price no\n    more than your reasonable cost of physically performing this\n    conveying of source, or (2) access to copy the\n    Corresponding Source from a network server at no charge.\n\n    c) Convey individual copies of the object code with a copy of the\n    written offer to provide the Corresponding Source.  This\n    alternative is allowed only occasionally and noncommercially, and\n    only if you received the object code with such an offer, in accord\n    with subsection 6b.\n\n    d) Convey the object code by offering access from a designated\n    place (gratis or for a charge), and offer equivalent access to the\n    Corresponding Source in the same way through the same place at no\n    further charge.  You need not require recipients to copy the\n    Corresponding Source along with the object code.  If the place to\n    copy the object code is a network server, the Corresponding Source\n    may be on a different server (operated by you or a third party)\n    that supports equivalent copying facilities, provided you maintain\n    clear directions next to the object code saying where to find the\n    Corresponding Source.  Regardless of what server hosts the\n    Corresponding Source, you remain obligated to ensure that it is\n    available for as long as needed to satisfy these requirements.\n\n    e) Convey the object code using peer-to-peer transmission, provided\n    you inform other peers where the object code and Corresponding\n    Source of the work are being offered to the general public at no\n    charge under subsection 6d.\n\n  A separable portion of the object code, whose source code is excluded\nfrom the Corresponding Source as a System Library, need not be\nincluded in conveying the object code work.\n\n  A \"User Product\" is either (1) a \"consumer product\", which means any\ntangible personal property which is normally used for personal, family,\nor household purposes, or (2) anything designed or sold for incorporation\ninto a dwelling.  In determining whether a product is a consumer product,\ndoubtful cases shall be resolved in favor of coverage.  For a particular\nproduct received by a particular user, \"normally used\" refers to a\ntypical or common use of that class of product, regardless of the status\nof the particular user or of the way in which the particular user\nactually uses, or expects or is expected to use, the product.  A product\nis a consumer product regardless of whether the product has substantial\ncommercial, industrial or non-consumer uses, unless such uses represent\nthe only significant mode of use of the product.\n\n  \"Installation Information\" for a User Product means any methods,\nprocedures, authorization keys, or other information required to install\nand execute modified versions of a covered work in that User Product from\na modified version of its Corresponding Source.  The information must\nsuffice to ensure that the continued functioning of the modified object\ncode is in no case prevented or interfered with solely because\nmodification has been made.\n\n  If you convey an object code work under this section in, or with, or\nspecifically for use in, a User Product, and the conveying occurs as\npart of a transaction in which the right of possession and use of the\nUser Product is transferred to the recipient in perpetuity or for a\nfixed term (regardless of how the transaction is characterized), the\nCorresponding Source conveyed under this section must be accompanied\nby the Installation Information.  But this requirement does not apply\nif neither you nor any third party retains the ability to install\nmodified object code on the User Product (for example, the work has\nbeen installed in ROM).\n\n  The requirement to provide Installation Information does not include a\nrequirement to continue to provide support service, warranty, or updates\nfor a work that has been modified or installed by the recipient, or for\nthe User Product in which it has been modified or installed.  Access to a\nnetwork may be denied when the modification itself materially and\nadversely affects the operation of the network or violates the rules and\nprotocols for communication across the network.\n\n  Corresponding Source conveyed, and Installation Information provided,\nin accord with this section must be in a format that is publicly\ndocumented (and with an implementation available to the public in\nsource code form), and must require no special password or key for\nunpacking, reading or copying.\n\n  7. Additional Terms.\n\n  \"Additional permissions\" are terms that supplement the terms of this\nLicense by making exceptions from one or more of its conditions.\nAdditional permissions that are applicable to the entire Program shall\nbe treated as though they were included in this License, to the extent\nthat they are valid under applicable law.  If additional permissions\napply only to part of the Program, that part may be used separately\nunder those permissions, but the entire Program remains governed by\nthis License without regard to the additional permissions.\n\n  When you convey a copy of a covered work, you may at your option\nremove any additional permissions from that copy, or from any part of\nit.  (Additional permissions may be written to require their own\nremoval in certain cases when you modify the work.)  You may place\nadditional permissions on material, added by you to a covered work,\nfor which you have or can give appropriate copyright permission.\n\n  Notwithstanding any other provision of this License, for material you\nadd to a covered work, you may (if authorized by the copyright holders of\nthat material) supplement the terms of this License with terms:\n\n    a) Disclaiming warranty or limiting liability differently from the\n    terms of sections 15 and 16 of this License; or\n\n    b) Requiring preservation of specified reasonable legal notices or\n    author attributions in that material or in the Appropriate Legal\n    Notices displayed by works containing it; or\n\n    c) Prohibiting misrepresentation of the origin of that material, or\n    requiring that modified versions of such material be marked in\n    reasonable ways as different from the original version; or\n\n    d) Limiting the use for publicity purposes of names of licensors or\n    authors of the material; or\n\n    e) Declining to grant rights under trademark law for use of some\n    trade names, trademarks, or service marks; or\n\n    f) Requiring indemnification of licensors and authors of that\n    material by anyone who conveys the material (or modified versions of\n    it) with contractual assumptions of liability to the recipient, for\n    any liability that these contractual assumptions directly impose on\n    those licensors and authors.\n\n  All other non-permissive additional terms are considered \"further\nrestrictions\" within the meaning of section 10.  If the Program as you\nreceived it, or any part of it, contains a notice stating that it is\ngoverned by this License along with a term that is a further\nrestriction, you may remove that term.  If a license document contains\na further restriction but permits relicensing or conveying under this\nLicense, you may add to a covered work material governed by the terms\nof that license document, provided that the further restriction does\nnot survive such relicensing or conveying.\n\n  If you add terms to a covered work in accord with this section, you\nmust place, in the relevant source files, a statement of the\nadditional terms that apply to those files, or a notice indicating\nwhere to find the applicable terms.\n\n  Additional terms, permissive or non-permissive, may be stated in the\nform of a separately written license, or stated as exceptions;\nthe above requirements apply either way.\n\n  8. Termination.\n\n  You may not propagate or modify a covered work except as expressly\nprovided under this License.  Any attempt otherwise to propagate or\nmodify it is void, and will automatically terminate your rights under\nthis License (including any patent licenses granted under the third\nparagraph of section 11).\n\n  However, if you cease all violation of this License, then your\nlicense from a particular copyright holder is reinstated (a)\nprovisionally, unless and until the copyright holder explicitly and\nfinally terminates your license, and (b) permanently, if the copyright\nholder fails to notify you of the violation by some reasonable means\nprior to 60 days after the cessation.\n\n  Moreover, your license from a particular copyright holder is\nreinstated permanently if the copyright holder notifies you of the\nviolation by some reasonable means, this is the first time you have\nreceived notice of violation of this License (for any work) from that\ncopyright holder, and you cure the violation prior to 30 days after\nyour receipt of the notice.\n\n  Termination of your rights under this section does not terminate the\nlicenses of parties who have received copies or rights from you under\nthis License.  If your rights have been terminated and not permanently\nreinstated, you do not qualify to receive new licenses for the same\nmaterial under section 10.\n\n  9. Acceptance Not Required for Having Copies.\n\n  You are not required to accept this License in order to receive or\nrun a copy of the Program.  Ancillary propagation of a covered work\noccurring solely as a consequence of using peer-to-peer transmission\nto receive a copy likewise does not require acceptance.  However,\nnothing other than this License grants you permission to propagate or\nmodify any covered work.  These actions infringe copyright if you do\nnot accept this License.  Therefore, by modifying or propagating a\ncovered work, you indicate your acceptance of this License to do so.\n\n  10. Automatic Licensing of Downstream Recipients.\n\n  Each time you convey a covered work, the recipient automatically\nreceives a license from the original licensors, to run, modify and\npropagate that work, subject to this License.  You are not responsible\nfor enforcing compliance by third parties with this License.\n\n  An \"entity transaction\" is a transaction transferring control of an\norganization, or substantially all assets of one, or subdividing an\norganization, or merging organizations.  If propagation of a covered\nwork results from an entity transaction, each party to that\ntransaction who receives a copy of the work also receives whatever\nlicenses to the work the party's predecessor in interest had or could\ngive under the previous paragraph, plus a right to possession of the\nCorresponding Source of the work from the predecessor in interest, if\nthe predecessor has it or can get it with reasonable efforts.\n\n  You may not impose any further restrictions on the exercise of the\nrights granted or affirmed under this License.  For example, you may\nnot impose a license fee, royalty, or other charge for exercise of\nrights granted under this License, and you may not initiate litigation\n(including a cross-claim or counterclaim in a lawsuit) alleging that\nany patent claim is infringed by making, using, selling, offering for\nsale, or importing the Program or any portion of it.\n\n  11. Patents.\n\n  A \"contributor\" is a copyright holder who authorizes use under this\nLicense of the Program or a work on which the Program is based.  The\nwork thus licensed is called the contributor's \"contributor version\".\n\n  A contributor's \"essential patent claims\" are all patent claims\nowned or controlled by the contributor, whether already acquired or\nhereafter acquired, that would be infringed by some manner, permitted\nby this License, of making, using, or selling its contributor version,\nbut do not include claims that would be infringed only as a\nconsequence of further modification of the contributor version.  For\npurposes of this definition, \"control\" includes the right to grant\npatent sublicenses in a manner consistent with the requirements of\nthis License.\n\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\npatent license under the contributor's essential patent claims, to\nmake, use, sell, offer for sale, import and otherwise run, modify and\npropagate the contents of its contributor version.\n\n  In the following three paragraphs, a \"patent license\" is any express\nagreement or commitment, however denominated, not to enforce a patent\n(such as an express permission to practice a patent or covenant not to\nsue for patent infringement).  To \"grant\" such a patent license to a\nparty means to make such an agreement or commitment not to enforce a\npatent against the party.\n\n  If you convey a covered work, knowingly relying on a patent license,\nand the Corresponding Source of the work is not available for anyone\nto copy, free of charge and under the terms of this License, through a\npublicly available network server or other readily accessible means,\nthen you must either (1) cause the Corresponding Source to be so\navailable, or (2) arrange to deprive yourself of the benefit of the\npatent license for this particular work, or (3) arrange, in a manner\nconsistent with the requirements of this License, to extend the patent\nlicense to downstream recipients.  \"Knowingly relying\" means you have\nactual knowledge that, but for the patent license, your conveying the\ncovered work in a country, or your recipient's use of the covered work\nin a country, would infringe one or more identifiable patents in that\ncountry that you have reason to believe are valid.\n\n  If, pursuant to or in connection with a single transaction or\narrangement, you convey, or propagate by procuring conveyance of, a\ncovered work, and grant a patent license to some of the parties\nreceiving the covered work authorizing them to use, propagate, modify\nor convey a specific copy of the covered work, then the patent license\nyou grant is automatically extended to all recipients of the covered\nwork and works based on it.\n\n  A patent license is \"discriminatory\" if it does not include within\nthe scope of its coverage, prohibits the exercise of, or is\nconditioned on the non-exercise of one or more of the rights that are\nspecifically granted under this License.  You may not convey a covered\nwork if you are a party to an arrangement with a third party that is\nin the business of distributing software, under which you make payment\nto the third party based on the extent of your activity of conveying\nthe work, and under which the third party grants, to any of the\nparties who would receive the covered work from you, a discriminatory\npatent license (a) in connection with copies of the covered work\nconveyed by you (or copies made from those copies), or (b) primarily\nfor and in connection with specific products or compilations that\ncontain the covered work, unless you entered into that arrangement,\nor that patent license was granted, prior to 28 March 2007.\n\n  Nothing in this License shall be construed as excluding or limiting\nany implied license or other defenses to infringement that may\notherwise be available to you under applicable patent law.\n\n  12. No Surrender of Others' Freedom.\n\n  If conditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot convey a\ncovered work so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you may\nnot convey it at all.  For example, if you agree to terms that obligate you\nto collect a royalty for further conveying from those to whom you convey\nthe Program, the only way you could satisfy both those terms and this\nLicense would be to refrain entirely from conveying the Program.\n\n  13. Remote Network Interaction; Use with the GNU General Public License.\n\n  Notwithstanding any other provision of this License, if you modify the\nProgram, your modified version must prominently offer all users\ninteracting with it remotely through a computer network (if your version\nsupports such interaction) an opportunity to receive the Corresponding\nSource of your version by providing access to the Corresponding Source\nfrom a network server at no charge, through some standard or customary\nmeans of facilitating copying of software.  This Corresponding Source\nshall include the Corresponding Source for any work covered by version 3\nof the GNU General Public License that is incorporated pursuant to the\nfollowing paragraph.\n\n  Notwithstanding any other provision of this License, you have\npermission to link or combine any covered work with a work licensed\nunder version 3 of the GNU General Public License into a single\ncombined work, and to convey the resulting work.  The terms of this\nLicense will continue to apply to the part which is the covered work,\nbut the work with which it is combined will remain governed by version\n3 of the GNU General Public License.\n\n  14. Revised Versions of this License.\n\n  The Free Software Foundation may publish revised and/or new versions of\nthe GNU Affero General Public License from time to time.  Such new versions\nwill be similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\n  Each version is given a distinguishing version number.  If the\nProgram specifies that a certain numbered version of the GNU Affero General\nPublic License \"or any later version\" applies to it, you have the\noption of following the terms and conditions either of that numbered\nversion or of any later version published by the Free Software\nFoundation.  If the Program does not specify a version number of the\nGNU Affero General Public License, you may choose any version ever published\nby the Free Software Foundation.\n\n  If the Program specifies that a proxy can decide which future\nversions of the GNU Affero General Public License can be used, that proxy's\npublic statement of acceptance of a version permanently authorizes you\nto choose that version for the Program.\n\n  Later license versions may give you additional or different\npermissions.  However, no additional obligations are imposed on any\nauthor or copyright holder as a result of your choosing to follow a\nlater version.\n\n  15. Disclaimer of Warranty.\n\n  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\nAPPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\nOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\nIS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\nALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n  16. Limitation of Liability.\n\n  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGES.\n\n  17. Interpretation of Sections 15 and 16.\n\n  If the disclaimer of warranty and limitation of liability provided\nabove cannot be given local legal effect according to their terms,\nreviewing courts shall apply local law that most closely approximates\nan absolute waiver of all civil liability in connection with the\nProgram, unless a warranty or assumption of liability accompanies a\ncopy of the Program in return for a fee.\n\n                     END OF TERMS AND CONDITIONS\n\n            How to Apply These Terms to Your New Programs\n\n  If you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these terms.\n\n  To do so, attach the following notices to the program.  It is safest\nto attach them to the start of each source file to most effectively\nstate the exclusion of warranty; and each file should have at least\nthe \"copyright\" line and a pointer to where the full notice is found.\n\n    <one line to give the program's name and a brief idea of what it does.>\n    Copyright (C) <year>  <name of author>\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as published\n    by the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\nAlso add information on how to contact you by electronic and paper mail.\n\n  If your software can interact with users remotely through a computer\nnetwork, you should also make sure that it provides a way for users to\nget its source.  For example, if your program is a web application, its\ninterface could display a \"Source\" link that leads users to an archive\nof the code.  There are many ways you could offer source, and different\nsolutions will be better for different programs; see section 13 for the\nspecific requirements.\n\n  You should also get your employer (if you work as a programmer) or school,\nif any, to sign a \"copyright disclaimer\" for the program, if necessary.\nFor more information on this, and how to apply and follow the GNU AGPL, see\n<https://www.gnu.org/licenses/>.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.34375,
          "content": "<div align=\"center\">\n<img src=\"https://github.com/vladmandic/automatic/raw/master/html/logo-transparent.png\" width=200 alt=\"SD.Next\">\n\n**Image Diffusion implementation with advanced features**\n\n![Last update](https://img.shields.io/github/last-commit/vladmandic/automatic?svg=true)\n![License](https://img.shields.io/github/license/vladmandic/automatic?svg=true)\n[![Discord](https://img.shields.io/discord/1101998836328697867?logo=Discord&svg=true)](https://discord.gg/VjvR2tabEX)\n[![Sponsors](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/vladmandic)\n\n[Docs](https://vladmandic.github.io/sdnext-docs/) | [Wiki](https://github.com/vladmandic/automatic/wiki) | [Discord](https://discord.gg/VjvR2tabEX) | [Changelog](CHANGELOG.md)\n\n</div>\n</br>\n\n## Table of contents\n\n- [Documentation](https://vladmandic.github.io/sdnext-docs/)\n- [SD.Next Features](#sdnext-features)\n- [Model support](#model-support) and [Specifications]()\n- [Platform support](#platform-support)\n- [Getting started](#getting-started)\n\n## SD.Next Features\n\nAll individual features are not listed here, instead check [ChangeLog](CHANGELOG.md) for full list of changes\n- Multiple UIs!  \n   **Standard | Modern**  \n- Multiple [diffusion models](https://vladmandic.github.io/sdnext-docs/Model-Support/)!  \n- Built-in Control for Text, Image, Batch and video processing!  \n- Multiplatform!  \n  **Windows | Linux | MacOS | nVidia | AMD | IntelArc/IPEX | DirectML | OpenVINO | ONNX+Olive | ZLUDA**\n- Platform specific autodetection and tuning performed on install  \n- Optimized processing with latest `torch` developments with built-in support for model compile, quantize and compress  \n  Compile backends: *Triton | StableFast | DeepCache | OneDiff*  \n  Quantization and compression methods: *BitsAndBytes | TorchAO | Optimum-Quanto | NNCF*  \n- Built-in queue management  \n- Built in installer with automatic updates and dependency management  \n- Mobile compatible  \n\n<br>\n\n*Main interface using **StandardUI***:  \n![screenshot-standardui](https://github.com/user-attachments/assets/cab47fe3-9adb-4d67-aea9-9ee738df5dcc)\n\n*Main interface using **ModernUI***:  \n\n![screenshot-modernui](https://github.com/user-attachments/assets/39e3bc9a-a9f7-4cda-ba33-7da8def08032)\n\nFor screenshots and informations on other available themes, see [Themes](https://vladmandic.github.io/sdnext-docs/Themes/)\n\n<br>\n\n## Model support\n\nSD.Next supports broad range of models: [supported models](https://vladmandic.github.io/sdnext-docs/Model-Support/) and [model specs](https://vladmandic.github.io/sdnext-docs/Models/)  \n\n## Platform support\n\n- *nVidia* GPUs using **CUDA** libraries on both *Windows and Linux*  \n- *AMD* GPUs using **ROCm** libraries on *Linux*  \n  Support will be extended to *Windows* once AMD releases ROCm for Windows  \n- *Intel Arc* GPUs using **OneAPI** with *IPEX XPU* libraries on both *Windows and Linux*  \n- Any GPU compatible with *DirectX* on *Windows* using **DirectML** libraries  \n  This includes support for AMD GPUs that are not supported by native ROCm libraries  \n- Any GPU or device compatible with **OpenVINO** libraries on both *Windows and Linux*  \n- *Apple M1/M2* on *OSX* using built-in support in Torch with **MPS** optimizations  \n- *ONNX/Olive*  \n- *AMD* GPUs on Windows using **ZLUDA** libraries  \n\n## Getting started\n\n- Get started with **SD.Next** by following the [installation instructions](https://vladmandic.github.io/sdnext-docs/Installation/)  \n- For more details, check out [advanced installation](https://vladmandic.github.io/sdnext-docs/Advanced-Install/) guide  \n- List and explanation of [command line arguments](https://vladmandic.github.io/sdnext-docs/CLI-Arguments/)\n- Install walkthrough [video](https://www.youtube.com/watch?v=nWTnTyFTuAs)\n\n> [!TIP]\n> And for platform specific information, check out  \n> [WSL](https://vladmandic.github.io/sdnext-docs/WSL/) | [Intel Arc](https://vladmandic.github.io/sdnext-docs/Intel-ARC/) | [DirectML](https://vladmandic.github.io/sdnext-docs/DirectML/) | [OpenVINO](https://vladmandic.github.io/sdnext-docs/OpenVINO/) | [ONNX & Olive](https://vladmandic.github.io/sdnext-docs/ONNX-Runtime/) | [ZLUDA](https://vladmandic.github.io/sdnext-docs/ZLUDA/) | [AMD ROCm](https://vladmandic.github.io/sdnext-docs/AMD-ROCm/) | [MacOS](https://vladmandic.github.io/sdnext-docs/MacOS-Python/) | [nVidia](https://vladmandic.github.io/sdnext-docs/nVidia/) | [Docker](https://vladmandic.github.io/sdnext-docs/Docker/)\n\n> [!WARNING]\n> If you run into issues, check out [troubleshooting](https://vladmandic.github.io/sdnext-docs/Troubleshooting/) and [debugging](https://vladmandic.github.io/sdnext-docs/Debug/) guides  \n\n### Credits\n\n- Main credit goes to [Automatic1111 WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) for the original codebase  \n- Additional credits are listed in [Credits](https://github.com/AUTOMATIC1111/stable-diffusion-webui/#credits)  \n- Licenses for modules are listed in [Licenses](html/licenses.html)  \n\n### Evolution\n\n<a href=\"https://star-history.com/#vladmandic/automatic&Date\">\n  <picture width=640>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=vladmandic/automatic&type=Date&theme=dark\" />\n    <img src=\"https://api.star-history.com/svg?repos=vladmandic/automatic&type=Date\" alt=\"starts\" width=\"320\">\n  </picture>\n</a>\n\n- [OSS Stats](https://ossinsight.io/analyze/vladmandic/automatic#overview)\n\n### Docs\n\nIf you're unsure how to use a feature, best place to start is [Docs](https://vladmandic.github.io/sdnext-docs/) and if its not there,  \ncheck [ChangeLog](https://vladmandic.github.io/sdnext-docs/CHANGELOG/) for when feature was first introduced as it will always have a short note on how to use it  \n\n### Sponsors\n\n<div align=\"center\">\n<!-- sponsors --><a href=\"https://github.com/allangrant\"><img src=\"https://github.com/allangrant.png\" width=\"60px\" alt=\"Allan Grant\" /></a><a href=\"https://github.com/BrentOzar\"><img src=\"https://github.com/BrentOzar.png\" width=\"60px\" alt=\"Brent Ozar\" /></a><a href=\"https://github.com/mantzaris\"><img src=\"https://github.com/mantzaris.png\" width=\"60px\" alt=\"a.v.mantzaris\" /></a><a href=\"https://github.com/CurseWave\"><img src=\"https://github.com/CurseWave.png\" width=\"60px\" alt=\"\" /></a><a href=\"https://github.com/smlbiobot\"><img src=\"https://github.com/smlbiobot.png\" width=\"60px\" alt=\"SML (See-ming Lee)\" /></a><!-- sponsors -->\n</div>\n\n<br>\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 1.21484375,
          "content": "# Security & Privacy Policy\n\n<br>\n\n## Issues\n\nAll issues are tracked publicly on GitHub: <https://github.com/vladmandic/automatic/issues>\n\n<br>\n\n## Vulnerabilities\n\n`SD.Next` code base and included dependencies are automatically scanned against known security vulnerabilities\n\nAny code commit is validated before merge\n\n- [Dependencies](https://github.com/vladmandic/automatic/security/dependabot)\n- [Scanning Alerts](https://github.com/vladmandic/automatic/security/code-scanning)\n\n<br>\n\n## Privacy\n\n`SD.Next` app:\n\n- Is fully self-contained and does not send or share data of any kind with external targets\n- Does not store any user or system data tracking, user provided inputs (images, video) or detection results\n- Does not utilize any analytic services (such as Google Analytics)\n\n`SD.Next` library can establish external connections *only* for following purposes and *only* when explicitly configured by user:\n\n- Download extensions and themes indexes from automatically updated indexes  \n- Download required packages and repositories from GitHub during installation/upgrade\n- Download installed/enabled extensions\n- Download models from CivitAI and/or Huggingface when instructed by user\n- Submit benchmark info upon user interaction  \n"
        },
        {
          "name": "TODO.md",
          "type": "blob",
          "size": 1.1279296875,
          "content": "# TODO\n\nMain ToDo list can be found at [GitHub projects](https://github.com/users/vladmandic/projects)\n\n## Pending\n\n- LoRA direct with caching\n- Previewer issues\n- Redesign postprocessing\n\n## Future Candidates\n\n- Flux NF4 loader: <https://github.com/huggingface/diffusers/issues/9996>\n- IPAdapter negative: <https://github.com/huggingface/diffusers/discussions/7167>\n- Control API enhance scripts compatibility\n- PixelSmith: <https://github.com/Thanos-DB/Pixelsmith>\n\n## Code TODO\n\n- TODO install: python 3.12.4 or higher cause a mess with pydantic\n- TODO install: enable ROCm for windows when available\n- TODO resize image: enable full VAE mode for resize-latent\n- TODO processing: remove duplicate mask params\n- TODO flux: fix loader for civitai nf4 models\n- TODO model loader: implement model in-memory caching\n- TODO hypertile: vae breaks when using non-standard sizes\n- TODO model load: force-reloading entire model as loading transformers only leads to massive memory usage\n- TODO lora load: direct with bnb\n- TODO lora make: support quantized flux\n- TODO control: support scripts via api\n- TODO modernui: monkey-patch for missing tabs.select event\n"
        },
        {
          "name": "cli",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "extensions-builtin",
          "type": "tree",
          "content": null
        },
        {
          "name": "extensions",
          "type": "tree",
          "content": null
        },
        {
          "name": "html",
          "type": "tree",
          "content": null
        },
        {
          "name": "installer.py",
          "type": "blob",
          "size": 64.9755859375,
          "content": "from functools import lru_cache\nimport os\nimport sys\nimport json\nimport time\nimport shutil\nimport logging\nimport platform\nimport subprocess\nimport cProfile\n\n\nclass Dot(dict): # dot notation access to dictionary attributes\n    __getattr__ = dict.get\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n\npkg_resources, setuptools, distutils = None, None, None # defined via ensure_base_requirements\nversion = None\ncurrent_branch = None\nlog = logging.getLogger(\"sd\")\nconsole = None\ndebug = log.debug if os.environ.get('SD_INSTALL_DEBUG', None) is not None else lambda *args, **kwargs: None\npip_log = '--log pip.log ' if os.environ.get('SD_PIP_DEBUG', None) is not None else ''\nlog_file = os.path.join(os.path.dirname(__file__), 'sdnext.log')\nlog_rolled = False\nfirst_call = True\nquick_allowed = True\nerrors = []\nopts = {}\nargs = Dot({\n    'debug': False,\n    'reset': False,\n    'profile': False,\n    'upgrade': False,\n    'skip_extensions': False,\n    'skip_requirements': False,\n    'skip_git': False,\n    'skip_torch': False,\n    'use_directml': False,\n    'use_ipex': False,\n    'use_cuda': False,\n    'use_rocm': False,\n    'experimental': False,\n    'test': False,\n    'tls_selfsign': False,\n    'reinstall': False,\n    'version': False,\n    'ignore': False,\n    'uv': False,\n})\ngit_commit = \"unknown\"\ndiffusers_commit = \"unknown\"\nextensions_commit = {\n    'sd-webui-controlnet': 'ecd33eb',\n    'adetailer': 'a89c01d'\n    # 'stable-diffusion-webui-images-browser': '27fe4a7',\n}\n\n# setup console and file logging\ndef setup_logging():\n\n    class RingBuffer(logging.StreamHandler):\n        def __init__(self, capacity):\n            super().__init__()\n            self.capacity = capacity\n            self.buffer = []\n            self.formatter = logging.Formatter('{ \"asctime\":\"%(asctime)s\", \"created\":%(created)f, \"facility\":\"%(name)s\", \"pid\":%(process)d, \"tid\":%(thread)d, \"level\":\"%(levelname)s\", \"module\":\"%(module)s\", \"func\":\"%(funcName)s\", \"msg\":\"%(message)s\" }')\n\n        def emit(self, record):\n            if record.msg is not None and not isinstance(record.msg, str):\n                record.msg = str(record.msg)\n            try:\n                record.msg = record.msg.replace('\"', \"'\")\n            except Exception:\n                pass\n            msg = self.format(record)\n            # self.buffer.append(json.loads(msg))\n            self.buffer.append(msg)\n            if len(self.buffer) > self.capacity:\n                self.buffer.pop(0)\n\n        def get(self):\n            return self.buffer\n\n    from functools import partial, partialmethod\n    from logging.handlers import RotatingFileHandler\n    from rich.theme import Theme\n    from rich.logging import RichHandler\n    from rich.console import Console\n    from rich import print as rprint\n    from rich.pretty import install as pretty_install\n    from rich.traceback import install as traceback_install\n\n    if args.log:\n        global log_file # pylint: disable=global-statement\n        log_file = args.log\n\n    logging.TRACE = 25\n    logging.addLevelName(logging.TRACE, 'TRACE')\n    logging.Logger.trace = partialmethod(logging.Logger.log, logging.TRACE)\n    logging.trace = partial(logging.log, logging.TRACE)\n\n    level = logging.DEBUG if args.debug else logging.INFO\n    log.setLevel(logging.DEBUG) # log to file is always at level debug for facility `sd`\n    log.print = rprint\n    global console # pylint: disable=global-statement\n    console = Console(log_time=True, log_time_format='%H:%M:%S-%f', theme=Theme({\n        \"traceback.border\": \"black\",\n        \"traceback.border.syntax_error\": \"black\",\n        \"inspect.value.border\": \"black\",\n        \"logging.level.info\": \"blue_violet\",\n        \"logging.level.debug\": \"purple4\",\n        \"logging.level.trace\": \"dark_blue\",\n    }))\n    logging.basicConfig(level=logging.ERROR, format='%(asctime)s | %(name)s | %(levelname)s | %(module)s | %(message)s', handlers=[logging.NullHandler()]) # redirect default logger to null\n    pretty_install(console=console)\n    traceback_install(console=console, extra_lines=1, max_frames=16, width=console.width, word_wrap=False, indent_guides=False, suppress=[])\n    while log.hasHandlers() and len(log.handlers) > 0:\n        log.removeHandler(log.handlers[0])\n\n    # handlers\n    rh = RichHandler(show_time=True, omit_repeated_times=False, show_level=True, show_path=False, markup=False, rich_tracebacks=True, log_time_format='%H:%M:%S-%f', level=level, console=console)\n    rh.setLevel(level)\n    log.addHandler(rh)\n\n    fh = RotatingFileHandler(log_file, maxBytes=32*1024*1024, backupCount=9, encoding='utf-8', delay=True) # 10MB default for log rotation\n    global log_rolled # pylint: disable=global-statement\n    if not log_rolled and args.debug and not args.log:\n        fh.doRollover()\n        log_rolled = True\n\n    fh.formatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(module)s | %(message)s')\n    fh.setLevel(logging.DEBUG)\n    log.addHandler(fh)\n\n    rb = RingBuffer(100) # 100 entries default in log ring buffer\n    rb.setLevel(level)\n    log.addHandler(rb)\n    log.buffer = rb.buffer\n\n    # overrides\n    logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n    logging.getLogger(\"httpx\").setLevel(logging.ERROR)\n    logging.getLogger(\"diffusers\").setLevel(logging.ERROR)\n    logging.getLogger(\"torch\").setLevel(logging.ERROR)\n    logging.getLogger(\"ControlNet\").handlers = log.handlers\n    logging.getLogger(\"lycoris\").handlers = log.handlers\n    # logging.getLogger(\"DeepSpeed\").handlers = log.handlers\n\n\ndef get_logfile():\n    log_size = os.path.getsize(log_file) if os.path.exists(log_file) else 0\n    log.info(f'Logger: file=\"{log_file}\" level={logging.getLevelName(logging.DEBUG if args.debug else logging.INFO)} size={log_size} mode={\"append\" if not log_rolled else \"create\"}')\n    return log_file\n\n\ndef custom_excepthook(exc_type, exc_value, exc_traceback):\n    import traceback\n    if issubclass(exc_type, KeyboardInterrupt):\n        sys.__excepthook__(exc_type, exc_value, exc_traceback)\n        return\n    log.error(f\"Uncaught exception occurred: type={exc_type} value={exc_value}\")\n    if exc_traceback:\n        format_exception = traceback.format_tb(exc_traceback)\n        for line in format_exception:\n            log.error(repr(line))\n\n\ndef print_dict(d):\n    if d is None:\n        return ''\n    return ' '.join([f'{k}={v}' for k, v in d.items()])\n\n\ndef print_profile(profiler: cProfile.Profile, msg: str):\n    profiler.disable()\n    from modules.errors import profile\n    profile(profiler, msg)\n\n\n@lru_cache()\ndef package_version(package):\n    try:\n        return pkg_resources.get_distribution(package).version\n    except Exception:\n        return None\n\n\n@lru_cache()\ndef package_spec(package):\n    spec = pkg_resources.working_set.by_key.get(package, None) # more reliable than importlib\n    if spec is None:\n        spec = pkg_resources.working_set.by_key.get(package.lower(), None) # check name variations\n    if spec is None:\n        spec = pkg_resources.working_set.by_key.get(package.replace('_', '-'), None) # check name variations\n    return spec\n\n\n# check if package is installed\n@lru_cache()\ndef installed(package, friendly: str = None, reload = False, quiet = False):\n    ok = True\n    try:\n        if reload:\n            try:\n                import importlib # pylint: disable=deprecated-module\n                importlib.reload(pkg_resources)\n            except Exception:\n                pass\n        if friendly:\n            pkgs = friendly.split()\n        else:\n            pkgs = [p for p in package.split() if not p.startswith('-') and not p.startswith('=') and not p.startswith('git+')]\n            pkgs = [p.split('/')[-1] for p in pkgs] # get only package name if installing from url\n        for pkg in pkgs:\n            if '!=' in pkg:\n                p = pkg.split('!=')\n                return True # check for not equal always return true\n            elif '>=' in pkg:\n                p = pkg.split('>=')\n            else:\n                p = pkg.split('==')\n            spec = package_spec(p[0])\n            ok = ok and spec is not None\n            if ok:\n                pkg_version = package_version(p[0])\n                if len(p) > 1:\n                    exact = pkg_version == p[1]\n                    if not exact and not quiet:\n                        if args.experimental:\n                            log.warning(f'Install: package=\"{p[0]}\" installed={pkg_version} required={p[1]} allowing experimental')\n                        else:\n                            log.warning(f'Install: package=\"{p[0]}\" installed={pkg_version} required={p[1]} version mismatch')\n                    ok = ok and (exact or args.experimental)\n            else:\n                if not quiet:\n                    log.debug(f'Install: package=\"{p[0]}\" install required')\n        return ok\n    except Exception as e:\n        log.error(f'Install: package=\"{pkgs}\" {e}')\n        return False\n\n\ndef uninstall(package, quiet = False):\n    packages = package if isinstance(package, list) else [package]\n    res = ''\n    for p in packages:\n        if installed(p, p, quiet=True):\n            if not quiet:\n                log.warning(f'Package: {p} uninstall')\n            res += pip(f\"uninstall {p} --yes --quiet\", ignore=True, quiet=True, uv=False)\n    return res\n\n\n@lru_cache()\ndef pip(arg: str, ignore: bool = False, quiet: bool = True, uv = True):\n    originalArg = arg\n    arg = arg.replace('>=', '==')\n    package = arg.replace(\"install\", \"\").replace(\"--upgrade\", \"\").replace(\"--no-deps\", \"\").replace(\"--force\", \"\").replace(\" \", \" \").strip()\n    uv = uv and args.uv and not package.startswith('git+')\n    pipCmd = \"uv pip\" if uv else \"pip\"\n    if not quiet and '-r ' not in arg:\n        log.info(f'Install: package=\"{package}\" mode={\"uv\" if uv else \"pip\"}')\n    env_args = os.environ.get(\"PIP_EXTRA_ARGS\", \"\")\n    all_args = f'{pip_log}{arg} {env_args}'.strip()\n    if not quiet:\n        log.debug(f'Running: {pipCmd}=\"{all_args}\"')\n    result = subprocess.run(f'\"{sys.executable}\" -m {pipCmd} {all_args}', shell=True, check=False, env=os.environ, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    txt = result.stdout.decode(encoding=\"utf8\", errors=\"ignore\")\n    if len(result.stderr) > 0:\n        if uv and result.returncode != 0:\n            err = result.stderr.decode(encoding=\"utf8\", errors=\"ignore\")\n            log.warning('Install: cannot use uv, fallback to pip')\n            debug(f'Install: uv pip error: {err}')\n            return pip(originalArg, ignore, quiet, uv=False)\n        else:\n            txt += ('\\n' if len(txt) > 0 else '') + result.stderr.decode(encoding=\"utf8\", errors=\"ignore\")\n    txt = txt.strip()\n    debug(f'Install {pipCmd}: {txt}')\n    if result.returncode != 0 and not ignore:\n        errors.append(f'pip: {package}')\n        log.error(f'Install: {pipCmd}: {arg}')\n        log.debug(f'Install: pip output {txt}')\n    return txt\n\n\n# install package using pip if not already installed\n@lru_cache()\ndef install(package, friendly: str = None, ignore: bool = False, reinstall: bool = False, no_deps: bool = False, quiet: bool = False):\n    res = ''\n    if args.reinstall or args.upgrade:\n        global quick_allowed # pylint: disable=global-statement\n        quick_allowed = False\n    if args.reinstall or reinstall or not installed(package, friendly, quiet=quiet):\n        deps = '' if not no_deps else '--no-deps '\n        res = pip(f\"install{' --upgrade' if not args.uv else ''} {deps}{package}\", ignore=ignore, uv=package != \"uv\" and not package.startswith('git+'))\n        try:\n            import importlib # pylint: disable=deprecated-module\n            importlib.reload(pkg_resources)\n        except Exception:\n            pass\n    return res\n\n\n# execute git command\n@lru_cache()\ndef git(arg: str, folder: str = None, ignore: bool = False, optional: bool = False):\n    if args.skip_git:\n        return ''\n    if optional:\n        if 'google.colab' in sys.modules:\n            return ''\n    git_cmd = os.environ.get('GIT', \"git\")\n    if git_cmd != \"git\":\n        git_cmd = os.path.abspath(git_cmd)\n    result = subprocess.run(f'\"{git_cmd}\" {arg}', check=False, shell=True, env=os.environ, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=folder or '.')\n    txt = result.stdout.decode(encoding=\"utf8\", errors=\"ignore\")\n    if len(result.stderr) > 0:\n        txt += ('\\n' if len(txt) > 0 else '') + result.stderr.decode(encoding=\"utf8\", errors=\"ignore\")\n    txt = txt.strip()\n    if result.returncode != 0 and not ignore:\n        if \"couldn't find remote ref\" in txt: # not a git repo\n            return txt\n        errors.append(f'git: {folder}')\n        log.error(f'Git: {folder} / {arg}')\n        if 'or stash them' in txt:\n            log.error(f'Git local changes detected: check details log=\"{log_file}\"')\n        log.debug(f'Git output: {txt}')\n    return txt\n\n\n# reattach as needed as head can get detached\ndef branch(folder=None):\n    # if args.experimental:\n    #    return None\n    if not os.path.exists(os.path.join(folder or os.curdir, '.git')):\n        return None\n    branches = []\n    try:\n        b = git('branch --show-current', folder, optional=True)\n        if b == '':\n            branches = git('branch', folder).split('\\n')\n        if len(branches) > 0:\n            b = [x for x in branches if x.startswith('*')][0]\n            if 'detached' in b and len(branches) > 1:\n                b = branches[1].strip()\n                log.debug(f'Git detached head detected: folder=\"{folder}\" reattach={b}')\n    except Exception:\n        b = git('git rev-parse --abbrev-ref HEAD', folder, optional=True)\n    if 'main' in b:\n        b = 'main'\n    elif 'master' in b:\n        b = 'master'\n    else:\n        b = b.split('\\n')[0].replace('*', '').strip()\n    log.debug(f'Git submodule: {folder} / {b}')\n    git(f'checkout {b}', folder, ignore=True, optional=True)\n    return b\n\n\n# update git repository\ndef update(folder, keep_branch = False, rebase = True):\n    try:\n        git('config rebase.Autostash true')\n    except Exception:\n        pass\n    arg = '--rebase --force' if rebase else ''\n    if keep_branch:\n        res = git(f'pull {arg}', folder)\n        debug(f'Install update: folder={folder} args={arg} {res}')\n        return res\n    b = branch(folder)\n    if branch is None:\n        res = git(f'pull {arg}', folder)\n        debug(f'Install update: folder={folder} branch={b} args={arg} {res}')\n    else:\n        res = git(f'pull origin {b} {arg}', folder)\n        debug(f'Install update: folder={folder} branch={b} args={arg} {res}')\n    if not args.experimental:\n        commit = extensions_commit.get(os.path.basename(folder), None)\n        if commit is not None:\n            res = git(f'checkout {commit}', folder)\n            debug(f'Install update: folder={folder} branch={b} args={arg} commit={commit} {res}')\n    return res\n\n\n# clone git repository\ndef clone(url, folder, commithash=None):\n    if os.path.exists(folder):\n        if commithash is None:\n            update(folder)\n        else:\n            current_hash = git('rev-parse HEAD', folder).strip()\n            if current_hash != commithash:\n                res = git('fetch', folder)\n                debug(f'Install clone: {res}')\n                git(f'checkout {commithash}', folder)\n                return\n    else:\n        log.info(f'Cloning repository: {url}')\n        git(f'clone \"{url}\" \"{folder}\"')\n        if commithash is not None:\n            git(f'-C \"{folder}\" checkout {commithash}')\n\n\ndef get_platform():\n    try:\n        if platform.system() == 'Windows':\n            release = platform.platform(aliased = True, terse = False)\n        else:\n            release = platform.release()\n        return {\n            'arch': platform.machine(),\n            'cpu': platform.processor(),\n            'system': platform.system(),\n            'release': release,\n            'python': platform.python_version(),\n            'docker': os.environ.get('SD_INSTALL_DEBUG', None) is not None,\n            # 'host': platform.node(),\n            # 'version': platform.version(),\n        }\n    except Exception as e:\n        return { 'error': e }\n\n\n# check python version\ndef check_python(supported_minors=[9, 10, 11, 12], reason=None):\n    if args.quick:\n        return\n    log.info(f'Python: version={platform.python_version()} platform={platform.system()} bin=\"{sys.executable}\" venv=\"{sys.prefix}\"')\n    if int(sys.version_info.major) == 3 and int(sys.version_info.minor) == 12 and int(sys.version_info.micro) > 3: # TODO install: python 3.12.4 or higher cause a mess with pydantic\n        log.error(f\"Python version incompatible: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro} required 3.12.3 or lower\")\n        if reason is not None:\n            log.error(reason)\n        if not args.ignore:\n            sys.exit(1)\n    if not (int(sys.version_info.major) == 3 and int(sys.version_info.minor) in supported_minors):\n        log.error(f\"Python version incompatible: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro} required 3.{supported_minors}\")\n        if reason is not None:\n            log.error(reason)\n        if not args.ignore:\n            sys.exit(1)\n    if int(sys.version_info.minor) == 12:\n        os.environ.setdefault('SETUPTOOLS_USE_DISTUTILS', 'local') # hack for python 3.11 setuptools\n    if not args.skip_git:\n        git_cmd = os.environ.get('GIT', \"git\")\n        if shutil.which(git_cmd) is None:\n            log.error('Git not found')\n            if not args.ignore:\n                sys.exit(1)\n    else:\n        git_version = git('--version', folder=None, ignore=False)\n        log.debug(f'Git: version={git_version.replace(\"git version\", \"\").strip()}')\n\n\n# check diffusers version\ndef check_diffusers():\n    if args.skip_all or args.skip_git:\n        return\n    sha = '6dfaec348780c6153a4cfd03a01972a291d67f82' # diffusers commit hash\n    pkg = pkg_resources.working_set.by_key.get('diffusers', None)\n    minor = int(pkg.version.split('.')[1] if pkg is not None else 0)\n    cur = opts.get('diffusers_version', '') if minor > 0 else ''\n    if (minor == 0) or (cur != sha):\n        log.info(f'Diffusers {\"install\" if minor == 0 else \"upgrade\"}: package={pkg} current={cur} target={sha}')\n        if minor > 0:\n            pip('uninstall --yes diffusers', ignore=True, quiet=True, uv=False)\n        pip(f'install --upgrade git+https://github.com/huggingface/diffusers@{sha}', ignore=False, quiet=True, uv=False)\n        global diffusers_commit # pylint: disable=global-statement\n        diffusers_commit = sha\n\n\n# check onnx version\ndef check_onnx():\n    if args.skip_all or args.skip_requirements:\n        return\n    if not installed('onnx', quiet=True):\n        install('onnx', 'onnx', ignore=True)\n    if not installed('onnxruntime', quiet=True) and not (installed('onnxruntime-gpu', quiet=True) or installed('onnxruntime-openvino', quiet=True) or installed('onnxruntime-training', quiet=True)): # allow either\n        install('onnxruntime', 'onnxruntime', ignore=True)\n\n\ndef check_torchao():\n    \"\"\"\n    if args.skip_all or args.skip_requirements:\n        return\n    if installed('torchao', quiet=True):\n        ver = package_version('torchao')\n        if ver != '0.5.0':\n            log.debug(f'Uninstall: torchao=={ver}')\n            pip('uninstall --yes torchao', ignore=True, quiet=True, uv=False)\n            for m in [m for m in sys.modules if m.startswith('torchao')]:\n                del sys.modules[m]\n    \"\"\"\n    return\n\n\ndef install_cuda():\n    log.info('CUDA: nVidia toolkit detected')\n    if not (args.skip_all or args.skip_requirements):\n        install('onnxruntime-gpu', 'onnxruntime-gpu', ignore=True, quiet=True)\n    # return os.environ.get('TORCH_COMMAND', 'torch torchvision --index-url https://download.pytorch.org/whl/cu124')\n    return os.environ.get('TORCH_COMMAND', 'torch==2.5.1+cu124 torchvision==0.20.1+cu124 --index-url https://download.pytorch.org/whl/cu124')\n\n\ndef install_rocm_zluda():\n    if args.skip_all or args.skip_requirements:\n        return None\n    from modules import rocm\n    if not rocm.is_installed:\n        log.warning('ROCm: could not find ROCm toolkit installed')\n        log.info('Using CPU-only torch')\n        return os.environ.get('TORCH_COMMAND', 'torch torchvision')\n\n    check_python(supported_minors=[10, 11], reason='ROCm or ZLUDA backends require Python 3.10 or 3.11')\n    log.info('ROCm: AMD toolkit detected')\n    os.environ.setdefault('PYTORCH_HIP_ALLOC_CONF', 'garbage_collection_threshold:0.8,max_split_size_mb:512')\n    # if not is_windows:\n    #    os.environ.setdefault('TENSORFLOW_PACKAGE', 'tensorflow-rocm')\n\n    device = None\n    try:\n        amd_gpus = rocm.get_agents()\n        if len(amd_gpus) == 0:\n            log.warning('ROCm: no agent was found')\n        else:\n            log.info(f'ROCm: agents={[gpu.name for gpu in amd_gpus]}')\n            if args.device_id is None:\n                index = 0\n                for idx, gpu in enumerate(amd_gpus):\n                    index = idx\n                    # if gpu.name.startswith('gfx11') and os.environ.get('TENSORFLOW_PACKAGE') == 'tensorflow-rocm': # do not use tensorflow-rocm for navi 3x\n                    #    os.environ['TENSORFLOW_PACKAGE'] = 'tensorflow==2.13.0'\n                    if not gpu.is_apu:\n                        # although apu was found, there can be a dedicated card. do not break loop.\n                        # if no dedicated card was found, apu will be used.\n                        break\n                os.environ.setdefault('HIP_VISIBLE_DEVICES', str(index))\n                device = amd_gpus[index]\n            else:\n                device_id = int(args.device_id)\n                if device_id < len(amd_gpus):\n                    device = amd_gpus[device_id]\n    except Exception as e:\n        log.warning(f'ROCm agent enumerator failed: {e}')\n\n    msg = f'ROCm: version={rocm.version}'\n    if device is not None:\n        msg += f', using agent {device.name}'\n    log.info(msg)\n    torch_command = ''\n    if sys.platform == \"win32\":\n        # TODO install: enable ROCm for windows when available\n\n        if args.device_id is not None:\n            if os.environ.get('HIP_VISIBLE_DEVICES', None) is not None:\n                log.warning('Setting HIP_VISIBLE_DEVICES and --device-id at the same time may be mistake.')\n            os.environ['HIP_VISIBLE_DEVICES'] = args.device_id\n            del args.device_id\n\n        error = None\n        from modules import zluda_installer\n        zluda_installer.set_default_agent(device)\n        try:\n            if args.reinstall:\n                zluda_installer.uninstall()\n            zluda_path = zluda_installer.get_path()\n            zluda_installer.install(zluda_path)\n            zluda_installer.make_copy(zluda_path)\n        except Exception as e:\n            error = e\n            log.warning(f'Failed to install ZLUDA: {e}')\n        if error is None:\n            try:\n                zluda_installer.load(zluda_path)\n                torch_command = os.environ.get('TORCH_COMMAND', f'torch=={zluda_installer.get_default_torch_version(device)} torchvision --index-url https://download.pytorch.org/whl/cu118')\n                log.info(f'Using ZLUDA in {zluda_path}')\n            except Exception as e:\n                error = e\n                log.warning(f'Failed to load ZLUDA: {e}')\n        if error is not None:\n            log.info('Using CPU-only torch')\n            torch_command = os.environ.get('TORCH_COMMAND', 'torch torchvision')\n    else:\n        if rocm.version is None or float(rocm.version) > 6.1: # assume the latest if version check fails\n            # torch_command = os.environ.get('TORCH_COMMAND', 'torch==2.5.1+rocm6.2 torchvision==0.20.1+rocm6.2 --index-url https://download.pytorch.org/whl/rocm6.2')\n            torch_command = os.environ.get('TORCH_COMMAND', 'torch==2.4.1+rocm6.1 torchvision==0.19.1+rocm6.1 --index-url https://download.pytorch.org/whl/rocm6.1')\n        elif rocm.version == \"6.1\": # lock to 2.4.1, older rocm (5.7) uses torch 2.3\n            torch_command = os.environ.get('TORCH_COMMAND', 'torch==2.4.1+rocm6.1 torchvision==0.19.1+rocm6.1 --index-url https://download.pytorch.org/whl/rocm6.1')\n        elif rocm.version == \"6.0\": # lock to 2.4.1, older rocm (5.7) uses torch 2.3\n            torch_command = os.environ.get('TORCH_COMMAND', 'torch==2.4.1+rocm6.0 torchvision==0.19.1+rocm6.0 --index-url https://download.pytorch.org/whl/rocm6.0')\n        elif float(rocm.version) < 5.5: # oldest supported version is 5.5\n            log.warning(f\"ROCm: unsupported version={rocm.version}\")\n            log.warning(\"ROCm: minimum supported version=5.5\")\n            torch_command = os.environ.get('TORCH_COMMAND', 'torch torchvision --index-url https://download.pytorch.org/whl/rocm5.5')\n        else:\n            torch_command = os.environ.get('TORCH_COMMAND', f'torch torchvision --index-url https://download.pytorch.org/whl/rocm{rocm.version}')\n\n        if sys.version_info < (3, 11):\n            ort_version = os.environ.get('ONNXRUNTIME_VERSION', None)\n            if rocm.version is None or float(rocm.version) > 6.0:\n                ort_package = os.environ.get('ONNXRUNTIME_PACKAGE', f\"--pre onnxruntime-training{'' if ort_version is None else ('==' + ort_version)} --index-url https://pypi.lsh.sh/60 --extra-index-url https://pypi.org/simple\")\n            else:\n                ort_package = os.environ.get('ONNXRUNTIME_PACKAGE', f\"--pre onnxruntime-training{'' if ort_version is None else ('==' + ort_version)} --index-url https://pypi.lsh.sh/{rocm.version[0]}{rocm.version[2]} --extra-index-url https://pypi.org/simple\")\n            install(ort_package, 'onnxruntime-training')\n\n        if installed(\"torch\") and device is not None:\n            if 'Flash attention' in opts.get('sdp_options', ''):\n                if not installed('flash-attn'):\n                    install(rocm.get_flash_attention_command(device), reinstall=True)\n            #elif not args.experimental:\n            #    uninstall('flash-attn')\n\n        if device is not None and rocm.version != \"6.2\" and rocm.version == rocm.version_torch and rocm.get_blaslt_enabled():\n            log.debug(f'ROCm hipBLASLt: arch={device.name} available={device.blaslt_supported}')\n            rocm.set_blaslt_enabled(device.blaslt_supported)\n\n    if device is None:\n        log.debug('ROCm: HSA_OVERRIDE_GFX_VERSION auto config skipped')\n    else:\n        gfx_ver = device.get_gfx_version()\n        if gfx_ver is not None:\n            os.environ.setdefault('HSA_OVERRIDE_GFX_VERSION', gfx_ver)\n        else:\n            log.warning(f'ROCm: device={device.name} could not auto-detect HSA version')\n\n    return torch_command\n\n\ndef install_ipex(torch_command):\n    check_python(supported_minors=[10,11], reason='IPEX backend requires Python 3.10 or 3.11')\n    args.use_ipex = True # pylint: disable=attribute-defined-outside-init\n    log.info('IPEX: Intel OneAPI toolkit detected')\n    if os.environ.get(\"NEOReadDebugKeys\", None) is None:\n        os.environ.setdefault('NEOReadDebugKeys', '1')\n    if os.environ.get(\"ClDeviceGlobalMemSizeAvailablePercent\", None) is None:\n        os.environ.setdefault('ClDeviceGlobalMemSizeAvailablePercent', '100')\n    if os.environ.get(\"PYTORCH_ENABLE_XPU_FALLBACK\", None) is None:\n        os.environ.setdefault('PYTORCH_ENABLE_XPU_FALLBACK', '1')\n    if \"linux\" in sys.platform:\n        torch_command = os.environ.get('TORCH_COMMAND', 'torch==2.5.1+cxx11.abi torchvision==0.20.1+cxx11.abi intel-extension-for-pytorch==2.5.10+xpu oneccl_bind_pt==2.5.0+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/')\n        # torch_command = os.environ.get('TORCH_COMMAND', 'torch torchvision --index-url https://download.pytorch.org/whl/test/xpu') # test wheels are stable previews, significantly slower than IPEX\n        # os.environ.setdefault('TENSORFLOW_PACKAGE', 'tensorflow==2.15.1 intel-extension-for-tensorflow[xpu]==2.15.0.1')\n    else:\n        torch_command = os.environ.get('TORCH_COMMAND', '--pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/xpu') # torchvision doesn't exist on test/stable branch for windows\n    install(os.environ.get('OPENVINO_PACKAGE', 'openvino==2024.5.0'), 'openvino', ignore=True)\n    install('nncf==2.7.0', ignore=True, no_deps=True) # requires older pandas\n    install(os.environ.get('ONNXRUNTIME_PACKAGE', 'onnxruntime-openvino'), 'onnxruntime-openvino', ignore=True)\n    return torch_command\n\n\ndef install_openvino(torch_command):\n    check_python(supported_minors=[9, 10, 11, 12], reason='OpenVINO backend requires Python 3.9, 3.10 or 3.11')\n    log.info('OpenVINO: selected')\n    if sys.platform == 'darwin':\n        torch_command = os.environ.get('TORCH_COMMAND', 'torch==2.3.1 torchvision==0.18.1')\n    else:\n        torch_command = os.environ.get('TORCH_COMMAND', 'torch==2.3.1+cpu torchvision==0.18.1+cpu --index-url https://download.pytorch.org/whl/cpu')\n    install(os.environ.get('OPENVINO_PACKAGE', 'openvino==2024.6.0'), 'openvino')\n    install(os.environ.get('ONNXRUNTIME_PACKAGE', 'onnxruntime-openvino'), 'onnxruntime-openvino', ignore=True)\n    install('nncf==2.14.1', 'nncf')\n    os.environ.setdefault('PYTORCH_TRACING_MODE', 'TORCHFX')\n    if os.environ.get(\"NEOReadDebugKeys\", None) is None:\n        os.environ.setdefault('NEOReadDebugKeys', '1')\n    if os.environ.get(\"ClDeviceGlobalMemSizeAvailablePercent\", None) is None:\n        os.environ.setdefault('ClDeviceGlobalMemSizeAvailablePercent', '100')\n    return torch_command\n\n\ndef install_torch_addons():\n    xformers_package = os.environ.get('XFORMERS_PACKAGE', '--pre xformers') if opts.get('cross_attention_optimization', '') == 'xFormers' or args.use_xformers else 'none'\n    triton_command = os.environ.get('TRITON_COMMAND', 'triton') if sys.platform == 'linux' else None\n    if 'xformers' in xformers_package:\n        try:\n            install(xformers_package, ignore=True, no_deps=True)\n            import torch # pylint: disable=unused-import\n            import xformers # pylint: disable=unused-import\n        except Exception as e:\n            log.debug(f'xFormers cannot install: {e}')\n    elif not args.experimental and not args.use_xformers and opts.get('cross_attention_optimization', '') != 'xFormers':\n        uninstall('xformers')\n    if opts.get('cuda_compile_backend', '') == 'hidet':\n        install('hidet', 'hidet')\n    if opts.get('cuda_compile_backend', '') == 'deep-cache':\n        install('DeepCache')\n    if opts.get('cuda_compile_backend', '') == 'olive-ai':\n        install('olive-ai')\n    if opts.get('nncf_compress_weights', False) and not args.use_openvino:\n        install('nncf==2.7.0', 'nncf')\n    if opts.get('optimum_quanto_weights', False):\n        install('optimum-quanto==0.2.6', 'optimum-quanto')\n    if not args.experimental:\n        uninstall('wandb', quiet=True)\n    if triton_command is not None:\n        install(triton_command, 'triton', quiet=True)\n\n\n# check torch version\ndef check_torch():\n    if args.skip_torch:\n        log.info('Torch: skip tests')\n        return\n    if args.profile:\n        pr = cProfile.Profile()\n        pr.enable()\n    from modules import rocm\n    allow_cuda = not (args.use_rocm or args.use_directml or args.use_ipex or args.use_openvino)\n    allow_rocm = not (args.use_cuda or args.use_directml or args.use_ipex or args.use_openvino)\n    allow_ipex = not (args.use_cuda or args.use_rocm or args.use_directml or args.use_openvino)\n    allow_directml = not (args.use_cuda or args.use_rocm or args.use_ipex or args.use_openvino)\n    allow_openvino = not (args.use_cuda or args.use_rocm or args.use_ipex or args.use_directml)\n    log.debug(f'Torch overrides: cuda={args.use_cuda} rocm={args.use_rocm} ipex={args.use_ipex} directml={args.use_directml} openvino={args.use_openvino} zluda={args.use_zluda}')\n    # log.debug(f'Torch allowed: cuda={allow_cuda} rocm={allow_rocm} ipex={allow_ipex} diml={allow_directml} openvino={allow_openvino}')\n    torch_command = os.environ.get('TORCH_COMMAND', '')\n\n    if torch_command != '':\n        pass\n    else:\n        is_cuda_available = allow_cuda and (shutil.which('nvidia-smi') is not None or args.use_xformers or os.path.exists(os.path.join(os.environ.get('SystemRoot') or r'C:\\Windows', 'System32', 'nvidia-smi.exe')))\n        is_rocm_available = allow_rocm and rocm.is_installed\n        is_ipex_available = allow_ipex and (args.use_ipex or shutil.which('sycl-ls') is not None or shutil.which('sycl-ls.exe') is not None or os.environ.get('ONEAPI_ROOT') is not None or os.path.exists('/opt/intel/oneapi') or os.path.exists(\"C:/Program Files (x86)/Intel/oneAPI\") or os.path.exists(\"C:/oneAPI\"))\n\n        if is_cuda_available and args.use_cuda: # prioritize cuda\n            torch_command = install_cuda()\n        elif is_rocm_available and (args.use_rocm or args.use_zluda): # prioritize rocm\n            torch_command = install_rocm_zluda()\n        elif allow_ipex and args.use_ipex: # prioritize ipex\n            torch_command = install_ipex(torch_command)\n        elif allow_openvino and args.use_openvino: # prioritize openvino\n            torch_command = install_openvino(torch_command)\n\n        elif is_cuda_available:\n            torch_command = install_cuda()\n        elif is_rocm_available:\n            torch_command = install_rocm_zluda()\n        elif is_ipex_available:\n            torch_command = install_ipex(torch_command)\n\n        else:\n            machine = platform.machine()\n            if sys.platform == 'darwin':\n                torch_command = os.environ.get('TORCH_COMMAND', 'torch torchvision')\n            elif allow_directml and args.use_directml and ('arm' not in machine and 'aarch' not in machine):\n                log.info('DirectML: selected')\n                torch_command = os.environ.get('TORCH_COMMAND', 'torch==2.4.1 torchvision torch-directml')\n                if 'torch' in torch_command and not args.version:\n                    install(torch_command, 'torch torchvision')\n                install('onnxruntime-directml', 'onnxruntime-directml', ignore=True)\n            else:\n                if args.use_zluda:\n                    log.warning(\"ZLUDA failed to initialize: no HIP SDK found\")\n                log.warning('Torch: CPU-only version installed')\n                torch_command = os.environ.get('TORCH_COMMAND', 'torch torchvision')\n    if 'torch' in torch_command and not args.version:\n        if not installed('torch'):\n            log.info(f'Torch: download and install in progress... cmd=\"{torch_command}\"')\n        install(torch_command, 'torch torchvision', quiet=True)\n    else:\n        try:\n            import torch\n            log.info(f'Torch {torch.__version__}')\n            if args.use_ipex and allow_ipex:\n                try:\n                    import intel_extension_for_pytorch as ipex # pylint: disable=import-error, unused-import\n                    log.info(f'Torch backend: Intel IPEX {ipex.__version__}')\n                except Exception:\n                    log.warning('IPEX: not found')\n                if shutil.which('icpx') is not None:\n                    log.info(f'{os.popen(\"icpx --version\").read().rstrip()}')\n                for device in range(torch.xpu.device_count()):\n                    log.info(f'Torch detected GPU: {torch.xpu.get_device_name(device)} VRAM {round(torch.xpu.get_device_properties(device).total_memory / 1024 / 1024)} Compute Units {torch.xpu.get_device_properties(device).max_compute_units}')\n            elif torch.cuda.is_available() and (allow_cuda or allow_rocm):\n                # log.debug(f'Torch allocator: {torch.cuda.get_allocator_backend()}')\n                if torch.version.cuda and allow_cuda:\n                    log.info(f'Torch backend: nVidia CUDA {torch.version.cuda} cuDNN {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else \"N/A\"}')\n                elif torch.version.hip and allow_rocm:\n                    log.info(f'Torch backend: AMD ROCm HIP {torch.version.hip}')\n                else:\n                    log.warning('Unknown Torch backend')\n                for device in [torch.cuda.device(i) for i in range(torch.cuda.device_count())]:\n                    log.info(f'Torch detected GPU: {torch.cuda.get_device_name(device)} VRAM {round(torch.cuda.get_device_properties(device).total_memory / 1024 / 1024)} Arch {torch.cuda.get_device_capability(device)} Cores {torch.cuda.get_device_properties(device).multi_processor_count}')\n            else:\n                try:\n                    if args.use_directml and allow_directml:\n                        import torch_directml # pylint: disable=import-error\n                        dml_ver = pkg_resources.get_distribution(\"torch-directml\")\n                        log.info(f'Torch backend: DirectML ({dml_ver})')\n                        for i in range(0, torch_directml.device_count()):\n                            log.info(f'Torch detected GPU: {torch_directml.device_name(i)}')\n                except Exception:\n                    log.warning(\"Torch reports CUDA not available\")\n        except Exception as e:\n            log.error(f'Torch cannot load: {e}')\n            if not args.ignore:\n                sys.exit(1)\n    if rocm.is_installed:\n        if sys.platform == \"win32\": # CPU, DirectML, ZLUDA\n            rocm.conceal()\n        elif rocm.is_wsl: # WSL ROCm\n            try:\n                rocm.load_hsa_runtime()\n            except OSError:\n                log.error(\"ROCm: failed to preload HSA runtime\")\n    if args.version:\n        return\n    if not args.skip_all:\n        install_torch_addons()\n    if args.profile:\n        pr.disable()\n        print_profile(pr, 'Torch')\n\n\n# check modified files\ndef check_modified_files():\n    if args.quick:\n        return\n    if args.skip_git:\n        return\n    try:\n        res = git('status --porcelain')\n        files = [x[2:].strip() for x in res.split('\\n')]\n        files = [x for x in files if len(x) > 0 and (not x.startswith('extensions')) and (not x.startswith('wiki')) and (not x.endswith('.json')) and ('.log' not in x)]\n        deleted = [x for x in files if not os.path.exists(x)]\n        if len(deleted) > 0:\n            log.warning(f'Deleted files: {files}')\n        files = [x for x in files if os.path.exists(x) and not os.path.isdir(x)]\n        if len(files) > 0:\n            log.warning(f'Modified files: {files}')\n    except Exception:\n        pass\n\n\n# install required packages\ndef install_packages():\n    if args.profile:\n        pr = cProfile.Profile()\n        pr.enable()\n    log.info('Verifying packages')\n    clip_package = os.environ.get('CLIP_PACKAGE', \"git+https://github.com/openai/CLIP.git\")\n    install(clip_package, 'clip', quiet=True)\n    install('open-clip-torch', no_deps=True, quiet=True)\n    # tensorflow_package = os.environ.get('TENSORFLOW_PACKAGE', 'tensorflow==2.13.0')\n    # tensorflow_package = os.environ.get('TENSORFLOW_PACKAGE', None)\n    # if tensorflow_package is not None:\n    #    install(tensorflow_package, 'tensorflow-rocm' if 'rocm' in tensorflow_package else 'tensorflow', ignore=True, quiet=True)\n    if args.profile:\n        pr.disable( )\n        print_profile(pr, 'Packages')\n\n\n# run extension installer\ndef run_extension_installer(folder):\n    path_installer = os.path.realpath(os.path.join(folder, \"install.py\"))\n    if not os.path.isfile(path_installer):\n        return\n    try:\n        log.debug(f\"Extension installer: {path_installer}\")\n        env = os.environ.copy()\n        env['PYTHONPATH'] = os.path.abspath(\".\")\n        result = subprocess.run(f'\"{sys.executable}\" \"{path_installer}\"', shell=True, env=env, check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=folder)\n        txt = result.stdout.decode(encoding=\"utf8\", errors=\"ignore\")\n        debug(f'Extension installer: file=\"{path_installer}\" {txt}')\n        if result.returncode != 0:\n            errors.append(f'ext: {os.path.basename(folder)}')\n            if len(result.stderr) > 0:\n                txt = txt + '\\n' + result.stderr.decode(encoding=\"utf8\", errors=\"ignore\")\n            log.error(f'Extension installer error: {path_installer}')\n            log.debug(txt)\n    except Exception as e:\n        log.error(f'Extension installer exception: {e}')\n\n# get list of all enabled extensions\ndef list_extensions_folder(folder, quiet=False):\n    name = os.path.basename(folder)\n    disabled_extensions_all = opts.get('disable_all_extensions', 'none')\n    if disabled_extensions_all != 'none':\n        return []\n    disabled_extensions = opts.get('disabled_extensions', [])\n    enabled_extensions = [x for x in os.listdir(folder) if os.path.isdir(os.path.join(folder, x)) and x not in disabled_extensions and not x.startswith('.')]\n    if not quiet:\n        log.info(f'Extensions: enabled={enabled_extensions} {name}')\n    return enabled_extensions\n\n\n# run installer for each installed and enabled extension and optionally update them\ndef install_extensions(force=False):\n    if args.profile:\n        pr = cProfile.Profile()\n        pr.enable()\n    pkg_resources._initialize_master_working_set() # pylint: disable=protected-access\n    pkgs = [f'{p.project_name}=={p._version}' for p in pkg_resources.working_set] # pylint: disable=protected-access,not-an-iterable\n    log.debug(f'Installed packages: {len(pkgs)}')\n    from modules.paths import extensions_builtin_dir, extensions_dir\n    extensions_duplicates = []\n    extensions_enabled = []\n    extension_folders = [extensions_builtin_dir] if args.safe else [extensions_builtin_dir, extensions_dir]\n    res = []\n    for folder in extension_folders:\n        if not os.path.isdir(folder):\n            continue\n        extensions = list_extensions_folder(folder, quiet=True)\n        log.debug(f'Extensions all: {extensions}')\n        for ext in extensions:\n            if ext in extensions_enabled:\n                extensions_duplicates.append(ext)\n                continue\n            extensions_enabled.append(ext)\n            if args.upgrade or force:\n                try:\n                    res.append(update(os.path.join(folder, ext)))\n                except Exception:\n                    res.append(f'Extension update error: {os.path.join(folder, ext)}')\n                    log.error(f'Extension update error: {os.path.join(folder, ext)}')\n            if not args.skip_extensions:\n                commit = extensions_commit.get(os.path.basename(ext), None)\n                if commit is not None:\n                    log.debug(f'Extension force: name=\"{ext}\" commit={commit}')\n                    res.append(git(f'checkout {commit}', os.path.join(folder, ext)))\n                run_extension_installer(os.path.join(folder, ext))\n            pkg_resources._initialize_master_working_set() # pylint: disable=protected-access\n            try:\n                updated = [f'{p.project_name}=={p._version}' for p in pkg_resources.working_set] # pylint: disable=protected-access,not-an-iterable\n                diff = [x for x in updated if x not in pkgs]\n                pkgs = updated\n                if len(diff) > 0:\n                    log.info(f'Extension installed packages: {ext} {diff}')\n            except Exception as e:\n                log.error(f'Extension installed unknown package: {e}')\n    log.info(f'Extensions enabled: {extensions_enabled}')\n    if len(extensions_duplicates) > 0:\n        log.warning(f'Extensions duplicates: {extensions_duplicates}')\n    if args.profile:\n        pr.disable()\n        print_profile(pr, 'Extensions')\n    return '\\n'.join(res)\n\n\n# initialize and optionally update submodules\ndef install_submodules(force=True):\n    if args.profile:\n        pr = cProfile.Profile()\n        pr.enable()\n    log.info('Verifying submodules')\n    txt = git('submodule')\n    # log.debug(f'Submodules list: {txt}')\n    if force and 'no submodule mapping found' in txt and 'extension-builtin' not in txt:\n        txt = git('submodule')\n        git_reset()\n        log.info('Continuing setup')\n    git('submodule --quiet update --init --recursive')\n    git('submodule --quiet sync --recursive')\n    submodules = txt.splitlines()\n    res = []\n    for submodule in submodules:\n        try:\n            name = submodule.split()[1].strip()\n            if args.upgrade:\n                res.append(update(name))\n            else:\n                branch(name)\n        except Exception:\n            log.error(f'Submodule update error: {submodule}')\n    setup_logging()\n    if args.profile:\n        pr.disable()\n        print_profile(pr, 'Submodule')\n    return '\\n'.join(res)\n\n\ndef ensure_base_requirements():\n    setuptools_version = '69.5.1'\n\n    def update_setuptools():\n        # print('Install base requirements')\n        global pkg_resources, setuptools, distutils # pylint: disable=global-statement\n        # python may ship with incompatible setuptools\n        subprocess.run(f'\"{sys.executable}\" -m pip install setuptools=={setuptools_version}', shell=True, check=False, env=os.environ, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        import importlib\n        # need to delete all references to modules to be able to reload them otherwise python will use cached version\n        modules = [m for m in sys.modules if m.startswith('setuptools') or m.startswith('pkg_resources') or m.startswith('distutils')]\n        for m in modules:\n            del sys.modules[m]\n        setuptools = importlib.import_module('setuptools')\n        sys.modules['setuptools'] = setuptools\n        distutils = importlib.import_module('distutils')\n        sys.modules['distutils'] = distutils\n        pkg_resources = importlib.import_module('pkg_resources')\n        sys.modules['pkg_resources'] = pkg_resources\n\n    try:\n        global pkg_resources, setuptools # pylint: disable=global-statement\n        import pkg_resources # pylint: disable=redefined-outer-name\n        import setuptools # pylint: disable=redefined-outer-name\n        if setuptools.__version__ != setuptools_version:\n            update_setuptools()\n    except ImportError:\n        update_setuptools()\n\n    # used by installler itself so must be installed before requirements\n    install('rich', 'rich', quiet=True)\n    install('psutil', 'psutil', quiet=True)\n    install('requests', 'requests', quiet=True)\n\n\ndef install_optional():\n    log.info('Installing optional requirements...')\n    install('basicsr')\n    install('gfpgan')\n    install('clean-fid')\n    install('optimum-quanto=0.2.6', ignore=True)\n    install('bitsandbytes==0.45.0', ignore=True)\n    install('pynvml', ignore=True)\n    install('ultralytics==8.3.40', ignore=True)\n    install('Cython', ignore=True)\n    install('insightface', ignore=True) # problematic build\n    install('nncf==2.7.0', ignore=True, no_deps=True) # requires older pandas\n    # install('flash-attn', ignore=True) # requires cuda and nvcc to be installed\n    install('gguf', ignore=True)\n    try:\n        import gguf\n        scripts_dir = os.path.join(os.path.dirname(gguf.__file__), '..', 'scripts')\n        if os.path.exists(scripts_dir):\n            os.rename(scripts_dir, scripts_dir + '_gguf')\n    except Exception:\n        pass\n\n\ndef install_requirements():\n    if args.profile:\n        pr = cProfile.Profile()\n        pr.enable()\n    if args.skip_requirements and not args.requirements:\n        return\n    if not installed('diffusers', quiet=True): # diffusers are not installed, so run initial installation\n        global quick_allowed # pylint: disable=global-statement\n        quick_allowed = False\n        log.info('Install requirements: this may take a while...')\n        pip('install -r requirements.txt')\n    if args.optional:\n        quick_allowed = False\n        install_optional()\n    installed('torch', reload=True) # reload packages cache\n    log.info('Install: verifying requirements')\n    with open('requirements.txt', 'r', encoding='utf8') as f:\n        lines = [line.strip() for line in f.readlines() if line.strip() != '' and not line.startswith('#') and line is not None]\n        for line in lines:\n            if not installed(line, quiet=True):\n                _res = install(line)\n    if args.profile:\n        pr.disable()\n        print_profile(pr, 'Requirements')\n\n\n# set environment variables controling the behavior of various libraries\ndef set_environment():\n    log.debug('Setting environment tuning')\n    os.environ.setdefault('ACCELERATE', 'True')\n    os.environ.setdefault('ATTN_PRECISION', 'fp16')\n    os.environ.setdefault('CUDA_AUTO_BOOST', '1')\n    os.environ.setdefault('CUDA_CACHE_DISABLE', '0')\n    os.environ.setdefault('CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT', '0')\n    os.environ.setdefault('CUDA_LAUNCH_BLOCKING', '0')\n    os.environ.setdefault('CUDA_MODULE_LOADING', 'LAZY')\n    os.environ.setdefault('TORCH_CUDNN_V8_API_ENABLED', '1')\n    os.environ.setdefault('FORCE_CUDA', '1')\n    os.environ.setdefault('GRADIO_ANALYTICS_ENABLED', 'False')\n    os.environ.setdefault('HF_HUB_DISABLE_EXPERIMENTAL_WARNING', '1')\n    os.environ.setdefault('HF_HUB_DISABLE_TELEMETRY', '1')\n    os.environ.setdefault('K_DIFFUSION_USE_COMPILE', '0')\n    os.environ.setdefault('NUMEXPR_MAX_THREADS', '16')\n    os.environ.setdefault('PYTHONHTTPSVERIFY', '0')\n    os.environ.setdefault('SAFETENSORS_FAST_GPU', '1')\n    os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '2')\n    os.environ.setdefault('TF_ENABLE_ONEDNN_OPTS', '0')\n    os.environ.setdefault('USE_TORCH', '1')\n    os.environ.setdefault('TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD', '1')\n    os.environ.setdefault('UVICORN_TIMEOUT_KEEP_ALIVE', '60')\n    os.environ.setdefault('KINETO_LOG_LEVEL', '3')\n    os.environ.setdefault('DO_NOT_TRACK', '1')\n    os.environ.setdefault('UV_INDEX_STRATEGY', 'unsafe-any-match')\n    os.environ.setdefault('UV_NO_BUILD_ISOLATION', '1')\n    os.environ.setdefault('HF_HUB_CACHE', opts.get('hfcache_dir', os.path.join(os.path.expanduser('~'), '.cache', 'huggingface', 'hub')))\n    allocator = f'garbage_collection_threshold:{opts.get(\"torch_gc_threshold\", 80)/100:0.2f},max_split_size_mb:512'\n    if opts.get(\"torch_malloc\", \"native\") == 'cudaMallocAsync':\n        allocator += ',backend:cudaMallocAsync'\n    if opts.get(\"torch_expandable_segments\", False):\n        allocator += ',expandable_segments:True'\n    os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', allocator)\n    log.debug(f'Torch allocator: \"{allocator}\"')\n    if sys.platform == 'darwin':\n        os.environ.setdefault('PYTORCH_ENABLE_MPS_FALLBACK', '1')\n\n\ndef check_extensions():\n    newest_all = os.path.getmtime('requirements.txt')\n    from modules.paths import extensions_builtin_dir, extensions_dir\n    extension_folders = [extensions_builtin_dir] if args.safe else [extensions_builtin_dir, extensions_dir]\n    disabled_extensions_all = opts.get('disable_all_extensions', 'none')\n    if disabled_extensions_all != 'none':\n        log.info(f'Extensions: disabled={disabled_extensions_all}')\n    else:\n        log.info(f'Extensions: disabled={opts.get(\"disabled_extensions\", [])}')\n    for folder in extension_folders:\n        if not os.path.isdir(folder):\n            continue\n        extensions = list_extensions_folder(folder)\n        for ext in extensions:\n            newest = 0\n            extension_dir = os.path.join(folder, ext)\n            if not os.path.isdir(extension_dir):\n                log.debug(f'Extension listed as installed but folder missing: {extension_dir}')\n                continue\n            for f in os.listdir(extension_dir):\n                if '.json' in f or '.csv' in f or '__pycache__' in f:\n                    continue\n                ts = os.path.getmtime(os.path.join(extension_dir, f))\n                newest = max(newest, ts)\n            newest_all = max(newest_all, newest)\n            # log.debug(f'Extension version: {time.ctime(newest)} {folder}{os.pathsep}{ext}')\n    return round(newest_all)\n\n\ndef get_version(force=False):\n    global version # pylint: disable=global-statement\n    if version is None or force:\n        try:\n            subprocess.run('git config log.showsignature false', stdout = subprocess.PIPE, stderr = subprocess.PIPE, shell=True, check=True)\n        except Exception:\n            pass\n        try:\n            res = subprocess.run('git log --pretty=format:\"%h %ad\" -1 --date=short', stdout = subprocess.PIPE, stderr = subprocess.PIPE, shell=True, check=True)\n            ver = res.stdout.decode(encoding = 'utf8', errors='ignore') if len(res.stdout) > 0 else '  '\n            githash, updated = ver.split(' ')\n            res = subprocess.run('git remote get-url origin', stdout = subprocess.PIPE, stderr = subprocess.PIPE, shell=True, check=True)\n            origin = res.stdout.decode(encoding = 'utf8', errors='ignore') if len(res.stdout) > 0 else ''\n            res = subprocess.run('git rev-parse --abbrev-ref HEAD', stdout = subprocess.PIPE, stderr = subprocess.PIPE, shell=True, check=True)\n            branch_name = res.stdout.decode(encoding = 'utf8', errors='ignore') if len(res.stdout) > 0 else ''\n            version = {\n                'app': 'sd.next',\n                'updated': updated,\n                'hash': githash,\n                'branch': branch_name.replace('\\n', ''),\n                'url': origin.replace('\\n', '') + '/tree/' + branch_name.replace('\\n', '')\n            }\n        except Exception:\n            version = { 'app': 'sd.next', 'version': 'unknown', 'branch': 'unknown' }\n        try:\n            cwd = os.getcwd()\n            os.chdir('extensions-builtin/sdnext-modernui')\n            res = subprocess.run('git rev-parse --abbrev-ref HEAD', stdout = subprocess.PIPE, stderr = subprocess.PIPE, shell=True, check=True)\n            os.chdir(cwd)\n            branch_ui = res.stdout.decode(encoding = 'utf8', errors='ignore') if len(res.stdout) > 0 else ''\n            branch_ui = 'dev' if 'dev' in branch_ui else 'main'\n            version['ui'] = branch_ui\n        except Exception:\n            os.chdir(cwd)\n            version['ui'] = 'unknown'\n    return version\n\n\ndef check_ui(ver):\n    def same(ver):\n        core = ver['branch'] if ver is not None and 'branch' in ver else 'unknown'\n        ui = ver['ui'] if ver is not None and 'ui' in ver else 'unknown'\n        return core == ui or (core == 'master' and ui == 'main')\n\n    if not same(ver):\n        log.debug(f'Branch mismatch: sdnext={ver[\"branch\"]} ui={ver[\"ui\"]}')\n        cwd = os.getcwd()\n        try:\n            os.chdir('extensions-builtin/sdnext-modernui')\n            target = 'dev' if 'dev' in ver['branch'] else 'main'\n            git('checkout ' + target, ignore=True, optional=True)\n            os.chdir(cwd)\n            ver = get_version(force=True)\n            if not same(ver):\n                log.debug(f'Branch synchronized: {ver[\"branch\"]}')\n            else:\n                log.debug(f'Branch sync failed: sdnext={ver[\"branch\"]} ui={ver[\"ui\"]}')\n        except Exception as e:\n            log.debug(f'Branch switch: {e}')\n        os.chdir(cwd)\n\n\ndef check_venv():\n    def try_relpath(p):\n        try:\n            return os.path.relpath(p)\n        except ValueError:\n            return p\n\n    import site\n    pkg_path = [try_relpath(p) for p in site.getsitepackages() if os.path.exists(p)]\n    log.debug(f'Packages: venv={try_relpath(sys.prefix)} site={pkg_path}')\n    for p in pkg_path:\n        invalid = []\n        for f in os.listdir(p):\n            if f.startswith('~'):\n                invalid.append(f)\n        if len(invalid) > 0:\n            log.warning(f'Packages: site=\"{p}\" invalid={invalid} removing')\n        for f in invalid:\n            fn = os.path.join(p, f)\n            try:\n                if os.path.isdir(fn):\n                    shutil.rmtree(fn)\n                elif os.path.isfile(fn):\n                    os.unlink(fn)\n            except Exception as e:\n                log.error(f'Packages: site={p} invalid={f} error={e}')\n\n\n# check version of the main repo and optionally upgrade it\ndef check_version(offline=False, reset=True): # pylint: disable=unused-argument\n    if args.skip_all:\n        return\n    if not os.path.exists('.git'):\n        log.warning('Not a git repository, all git operations are disabled')\n        args.skip_git = True # pylint: disable=attribute-defined-outside-init\n    ver = get_version()\n    log.info(f'Version: {print_dict(ver)}')\n    if args.version or args.skip_git:\n        return\n    check_ui(ver)\n    commit = git('rev-parse HEAD')\n    global git_commit # pylint: disable=global-statement\n    git_commit = commit[:7]\n    if args.quick:\n        return\n    try:\n        import requests\n    except ImportError:\n        return\n    commits = None\n    try:\n        commits = requests.get('https://api.github.com/repos/vladmandic/automatic/branches/master', timeout=10).json()\n        if commits['commit']['sha'] != commit:\n            if args.upgrade:\n                global quick_allowed # pylint: disable=global-statement\n                quick_allowed = False\n                log.info('Updating main repository')\n                try:\n                    git('add .')\n                    git('stash')\n                    update('.', keep_branch=True)\n                    # git('git stash pop')\n                    ver = git('log -1 --pretty=format:\"%h %ad\"')\n                    log.info(f'Repository upgraded: {ver}')\n                except Exception:\n                    if not reset:\n                        log.error('Repository error upgrading')\n                    else:\n                        log.warning('Repository: retrying upgrade...')\n                        git_reset()\n                        check_version(offline=offline, reset=False)\n            else:\n                log.info(f'Repository latest available {commits[\"commit\"][\"sha\"]} {commits[\"commit\"][\"commit\"][\"author\"][\"date\"]}')\n    except Exception as e:\n        log.error(f'Repository failed to check version: {e} {commits}')\n\n\ndef update_wiki():\n    if args.upgrade:\n        log.info('Updating Wiki')\n        try:\n            update(os.path.join(os.path.dirname(__file__), \"wiki\"))\n        except Exception:\n            log.error('Wiki update error')\n\n\n# check if we can run setup in quick mode\ndef check_timestamp():\n    if not quick_allowed or not os.path.isfile(log_file):\n        return False\n    if args.quick:\n        return True\n    if args.skip_git:\n        return True\n    ok = True\n    setup_time = -1\n    version_time = -1\n    with open(log_file, 'r', encoding='utf8') as f:\n        lines = f.readlines()\n        for line in lines:\n            if 'Setup complete without errors' in line:\n                setup_time = int(line.split(' ')[-1])\n    try:\n        version_time = int(git('log -1 --pretty=format:\"%at\"'))\n    except Exception as e:\n        log.error(f'Timestamp local repository version: {e}')\n    log.debug(f'Timestamp repository update time: {time.ctime(int(version_time))}')\n    if setup_time == -1:\n        return False\n    log.debug(f'Timestamp previous setup time: {time.ctime(setup_time)}')\n    if setup_time < version_time:\n        ok = False\n    extension_time = check_extensions()\n    log.debug(f'Timestamp latest extensions time: {time.ctime(extension_time)}')\n    if setup_time < extension_time:\n        ok = False\n    log.debug(f'Timestamp: version:{version_time} setup:{setup_time} extension:{extension_time}')\n    if args.reinstall:\n        ok = False\n    return ok\n\n\ndef add_args(parser):\n    group_setup = parser.add_argument_group('Setup')\n    group_setup.add_argument('--reset', default=os.environ.get(\"SD_RESET\",False), action='store_true', help=\"Reset main repository to latest version, default: %(default)s\")\n    group_setup.add_argument('--upgrade', '--update', default=os.environ.get(\"SD_UPGRADE\",False), action='store_true', help=\"Upgrade main repository to latest version, default: %(default)s\")\n    group_setup.add_argument('--requirements', default=os.environ.get(\"SD_REQUIREMENTS\",False), action='store_true', help=\"Force re-check of requirements, default: %(default)s\")\n    group_setup.add_argument('--reinstall', default=os.environ.get(\"SD_REINSTALL\",False), action='store_true', help=\"Force reinstallation of all requirements, default: %(default)s\")\n    group_setup.add_argument('--optional', default=os.environ.get(\"SD_OPTIONAL\",False), action='store_true', help=\"Force installation of optional requirements, default: %(default)s\")\n    group_setup.add_argument('--uv', default=os.environ.get(\"SD_UV\",False), action='store_true', help=\"Use uv instead of pip to install the packages\")\n\n    group_startup = parser.add_argument_group('Startup')\n    group_startup.add_argument('--quick', default=os.environ.get(\"SD_QUICK\",False), action='store_true', help=\"Bypass version checks, default: %(default)s\")\n    group_startup.add_argument('--skip-requirements', default=os.environ.get(\"SD_SKIPREQUIREMENTS\",False), action='store_true', help=\"Skips checking and installing requirements, default: %(default)s\")\n    group_startup.add_argument('--skip-extensions', default=os.environ.get(\"SD_SKIPEXTENSION\",False), action='store_true', help=\"Skips running individual extension installers, default: %(default)s\")\n    group_startup.add_argument('--skip-git', default=os.environ.get(\"SD_SKIPGIT\",False), action='store_true', help=\"Skips running all GIT operations, default: %(default)s\")\n    group_startup.add_argument('--skip-torch', default=os.environ.get(\"SD_SKIPTORCH\",False), action='store_true', help=\"Skips running Torch checks, default: %(default)s\")\n    group_startup.add_argument('--skip-all', default=os.environ.get(\"SD_SKIPALL\",False), action='store_true', help=\"Skips running all checks, default: %(default)s\")\n    group_startup.add_argument('--skip-env', default=os.environ.get(\"SD_SKIPENV\",False), action='store_true', help=\"Skips setting of env variables during startup, default: %(default)s\")\n\n    group_compute = parser.add_argument_group('Compute Engine')\n    group_compute.add_argument('--use-directml', default=os.environ.get(\"SD_USEDIRECTML\",False), action='store_true', help=\"Use DirectML if no compatible GPU is detected, default: %(default)s\")\n    group_compute.add_argument(\"--use-openvino\", default=os.environ.get(\"SD_USEOPENVINO\",False), action='store_true', help=\"Use Intel OpenVINO backend, default: %(default)s\")\n    group_compute.add_argument(\"--use-ipex\", default=os.environ.get(\"SD_USEIPEX\",False), action='store_true', help=\"Force use Intel OneAPI XPU backend, default: %(default)s\")\n    group_compute.add_argument(\"--use-cuda\", default=os.environ.get(\"SD_USECUDA\",False), action='store_true', help=\"Force use nVidia CUDA backend, default: %(default)s\")\n    group_compute.add_argument(\"--use-rocm\", default=os.environ.get(\"SD_USEROCM\",False), action='store_true', help=\"Force use AMD ROCm backend, default: %(default)s\")\n    group_compute.add_argument('--use-zluda', default=os.environ.get(\"SD_USEZLUDA\", False), action='store_true', help=\"Force use ZLUDA, AMD GPUs only, default: %(default)s\")\n    group_compute.add_argument(\"--use-xformers\", default=os.environ.get(\"SD_USEXFORMERS\",False), action='store_true', help=\"Force use xFormers cross-optimization, default: %(default)s\")\n\n    group_diag = parser.add_argument_group('Diagnostics')\n    group_diag.add_argument('--safe', default=os.environ.get(\"SD_SAFE\",False), action='store_true', help=\"Run in safe mode with no user extensions\")\n    group_diag.add_argument('--experimental', default=os.environ.get(\"SD_EXPERIMENTAL\",False), action='store_true', help=\"Allow unsupported versions of libraries, default: %(default)s\")\n    group_diag.add_argument('--test', default=os.environ.get(\"SD_TEST\",False), action='store_true', help=\"Run test only and exit\")\n    group_diag.add_argument('--version', default=False, action='store_true', help=\"Print version information\")\n    group_diag.add_argument('--ignore', default=os.environ.get(\"SD_IGNORE\",False), action='store_true', help=\"Ignore any errors and attempt to continue\")\n\n    group_log = parser.add_argument_group('Logging')\n    group_log.add_argument(\"--log\", type=str, default=os.environ.get(\"SD_LOG\", None), help=\"Set log file, default: %(default)s\")\n    group_log.add_argument('--debug', default=os.environ.get(\"SD_DEBUG\",False), action='store_true', help=\"Run installer with debug logging, default: %(default)s\")\n    group_log.add_argument(\"--profile\", default=os.environ.get(\"SD_PROFILE\", False), action='store_true', help=\"Run profiler, default: %(default)s\")\n    group_log.add_argument('--docs', default=os.environ.get(\"SD_DOCS\", False), action='store_true', help=\"Mount API docs, default: %(default)s\")\n    group_log.add_argument(\"--api-log\", default=os.environ.get(\"SD_APILOG\", True), action='store_true', help=\"Log all API requests\")\n\n\ndef parse_args(parser):\n    # command line args\n    global args # pylint: disable=global-statement\n    args = parser.parse_args()\n    return args\n\n\ndef extensions_preload(parser):\n    if args.profile:\n        pr = cProfile.Profile()\n        pr.enable()\n    if args.safe:\n        log.info('Running in safe mode without user extensions')\n    try:\n        from modules.script_loading import preload_extensions\n        from modules.paths import extensions_builtin_dir, extensions_dir\n        extension_folders = [extensions_builtin_dir] if args.safe else [extensions_builtin_dir, extensions_dir]\n        preload_time = {}\n        for ext_dir in extension_folders:\n            t0 = time.time()\n            preload_extensions(ext_dir, parser)\n            t1 = time.time()\n            preload_time[ext_dir] = round(t1 - t0, 2)\n        log.debug(f'Extension preload: {preload_time}')\n    except Exception:\n        log.error('Error running extension preloading')\n    if args.profile:\n        pr.disable()\n        print_profile(pr, 'Preload')\n\n\ndef git_reset(folder='.'):\n    log.warning('Running GIT reset')\n    global quick_allowed # pylint: disable=global-statement\n    quick_allowed = False\n    b = branch(folder)\n    if b is None or b == '':\n        b = 'master'\n    git('add .')\n    git('stash')\n    git('merge --abort', folder=None, ignore=True)\n    git('fetch --all')\n    git(f'reset --hard origin/{b}')\n    git(f'checkout {b}')\n    git('submodule update --init --recursive')\n    git('submodule sync --recursive')\n    log.info('GIT reset complete')\n\n\ndef read_options():\n    global opts # pylint: disable=global-statement\n    if os.path.isfile(args.config):\n        with open(args.config, \"r\", encoding=\"utf8\") as file:\n            try:\n                opts = json.load(file)\n                if type(opts) is str:\n                    opts = json.loads(opts)\n            except Exception as e:\n                log.error(f'Error reading options file: {file} {e}')\n"
        },
        {
          "name": "javascript",
          "type": "tree",
          "content": null
        },
        {
          "name": "launch.py",
          "type": "blob",
          "size": 9.46484375,
          "content": "#!/usr/bin/env python\n\nimport os\nimport sys\nimport time\nimport shlex\nimport subprocess\nfrom functools import lru_cache\nimport installer\n\n\ndebug_install = installer.log.debug if os.environ.get('SD_INSTALL_DEBUG', None) is not None else lambda *args, **kwargs: None\ncommandline_args = os.environ.get('COMMANDLINE_ARGS', \"\")\nsys.argv += shlex.split(commandline_args)\nargs = None\nparser = None\nscript_path = None\nextensions_dir = None\ngit = os.environ.get('GIT', \"git\")\nindex_url = os.environ.get('INDEX_URL', \"\")\nstored_commit_hash = None\ndir_repos = \"repositories\"\npython = sys.executable # used by some extensions to run python\nskip_install = False # parsed by some extensions\n\n\ndef init_args():\n    global parser, args # pylint: disable=global-statement\n    import modules.cmd_args\n    parser = modules.cmd_args.parser\n    installer.add_args(parser)\n    args, _ = parser.parse_known_args()\n\n\ndef init_paths():\n    global script_path, extensions_dir # pylint: disable=global-statement\n    import modules.paths\n    modules.paths.register_paths()\n    script_path = modules.paths.script_path\n    extensions_dir = modules.paths.extensions_dir\n\n\ndef get_custom_args():\n    custom = {}\n    for arg in vars(args):\n        default = parser.get_default(arg)\n        current = getattr(args, arg)\n        if current != default:\n            custom[arg] = getattr(args, arg)\n    installer.log.info(f'Command line args: {sys.argv[1:]} {installer.print_dict(custom)}')\n    if os.environ.get('SD_ENV_DEBUG', None) is not None:\n        env = os.environ.copy()\n        if 'PATH' in env:\n            del env['PATH']\n        if 'PS1' in env:\n            del env['PS1']\n        installer.log.trace(f'Environment: {installer.print_dict(env)}')\n    env = [f'{k}={v}' for k, v in os.environ.items() if k.startswith('SD_')]\n    installer.log.debug(f'Env flags: {env}')\n    ldd = os.environ.get('LD_PRELOAD', None)\n    if ldd is not None:\n        installer.log.debug(f'Linker flags: \"{ldd}\"')\n\n\n@lru_cache()\ndef commit_hash(): # compatbility function\n    global stored_commit_hash # pylint: disable=global-statement\n    if stored_commit_hash is not None:\n        return stored_commit_hash\n    try:\n        stored_commit_hash = run(f\"{git} rev-parse HEAD\").strip()\n    except Exception:\n        stored_commit_hash = \"<none>\"\n    return stored_commit_hash\n\n\n@lru_cache()\ndef run(command, desc=None, errdesc=None, custom_env=None, live=False): # compatbility function\n    if desc is not None:\n        installer.log.info(desc)\n    if live:\n        result = subprocess.run(command, check=False, shell=True, env=os.environ if custom_env is None else custom_env)\n        if result.returncode != 0:\n            raise RuntimeError(f\"\"\"{errdesc or 'Error running command'} Command: {command} Error code: {result.returncode}\"\"\")\n        return ''\n    result = subprocess.run(command, stdout=subprocess.PIPE, check=False, stderr=subprocess.PIPE, shell=True, env=os.environ if custom_env is None else custom_env)\n    if result.returncode != 0:\n        raise RuntimeError(f\"\"\"{errdesc or 'Error running command'}: {command} code: {result.returncode}\n{result.stdout.decode(encoding=\"utf8\", errors=\"ignore\") if len(result.stdout)>0 else ''}\n{result.stderr.decode(encoding=\"utf8\", errors=\"ignore\") if len(result.stderr)>0 else ''}\n\"\"\")\n    return result.stdout.decode(encoding=\"utf8\", errors=\"ignore\")\n\n\ndef check_run(command): # compatbility function\n    result = subprocess.run(command, check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    return result.returncode == 0\n\n\n@lru_cache()\ndef is_installed(package): # compatbility function\n    return installer.installed(package)\n\n\n@lru_cache()\ndef repo_dir(name): # compatbility function\n    return os.path.join(script_path, dir_repos, name)\n\n\n@lru_cache()\ndef run_python(code, desc=None, errdesc=None): # compatbility function\n    return run(f'\"{sys.executable}\" -c \"{code}\"', desc, errdesc)\n\n\n@lru_cache()\ndef run_pip(pkg, desc=None): # compatbility function\n    forbidden = ['onnxruntime', 'opencv-python']\n    if desc is None:\n        desc = pkg\n    for f in forbidden:\n        if f in pkg:\n            debug_install('Blocked package installation: package={f}')\n            return True\n    index_url_line = f' --index-url {index_url}' if index_url != '' else ''\n    return run(f'\"{sys.executable}\" -m pip {pkg} --prefer-binary{index_url_line}', desc=f\"Installing {desc}\", errdesc=f\"Couldn't install {desc}\")\n\n\n@lru_cache()\ndef check_run_python(code): # compatbility function\n    return check_run(f'\"{sys.executable}\" -c \"{code}\"')\n\n\ndef git_clone(url, tgt, _name, commithash=None): # compatbility function\n    installer.clone(url, tgt, commithash)\n\n\ndef run_extension_installer(ext_dir): # compatbility function\n    installer.run_extension_installer(ext_dir)\n\n\ndef get_memory_stats():\n    import psutil\n    def gb(val: float):\n        return round(val / 1024 / 1024 / 1024, 2)\n    process = psutil.Process(os.getpid())\n    res = process.memory_info()\n    ram_total = 100 * res.rss / process.memory_percent()\n    return f'{gb(res.rss)}/{gb(ram_total)}'\n\n\ndef start_server(immediate=True, server=None):\n    if args.profile:\n        import cProfile\n        pr = cProfile.Profile()\n        pr.enable()\n    import gc\n    import importlib.util\n    collected = 0\n    if server is not None:\n        server = None\n        collected = gc.collect()\n    if not immediate:\n        time.sleep(3)\n    if collected > 0:\n        installer.log.debug(f'Memory: {get_memory_stats()} collected={collected}')\n    module_spec = importlib.util.spec_from_file_location('webui', 'webui.py')\n    server = importlib.util.module_from_spec(module_spec)\n    installer.log.debug(f'Starting module: {server}')\n    module_spec.loader.exec_module(server)\n    uvicorn = None\n    if args.test:\n        installer.log.info(\"Test only\")\n        installer.log.critical('Logging: level=critical')\n        installer.log.error('Logging: level=error')\n        installer.log.warning('Logging: level=warning')\n        installer.log.info('Logging: level=info')\n        installer.log.debug('Logging: level=debug')\n        installer.log.trace('Logging: level=trace')\n        server.wants_restart = False\n    else:\n        if args.api_only:\n            uvicorn = server.api_only()\n        else:\n            uvicorn = server.webui(restart=not immediate)\n    if args.profile:\n        pr.disable()\n        installer.print_profile(pr, 'WebUI')\n    return uvicorn, server\n\n\ndef main():\n    global args # pylint: disable=global-statement\n    installer.ensure_base_requirements()\n    init_args() # setup argparser and default folders\n    installer.args = args\n    installer.setup_logging()\n    installer.log.info('Starting SD.Next')\n    installer.get_logfile()\n    try:\n        sys.excepthook = installer.custom_excepthook\n    except Exception:\n        pass\n    installer.read_options()\n    if args.skip_all:\n        args.quick = True\n    installer.check_python()\n    if args.reset:\n        installer.git_reset()\n    if args.skip_git or args.skip_all:\n        installer.log.info('Skipping GIT operations')\n    installer.check_version()\n    installer.log.info(f'Platform: {installer.print_dict(installer.get_platform())}')\n    installer.check_venv()\n    installer.log.info(f'Args: {sys.argv[1:]}')\n    if not args.skip_env or args.skip_all:\n        installer.set_environment()\n    if args.uv:\n        installer.install(\"uv\", \"uv\")\n    installer.check_torch()\n    installer.check_onnx()\n    installer.check_torchao()\n    installer.check_diffusers()\n    installer.check_modified_files()\n    if args.reinstall:\n        installer.log.info('Forcing reinstall of all packages')\n        installer.quick_allowed = False\n    if args.skip_all:\n        installer.log.info('Startup: skip all')\n        installer.quick_allowed = True\n        init_paths()\n    else:\n        installer.install_requirements()\n        installer.install_packages()\n        if installer.check_timestamp():\n            installer.log.info('Startup: quick launch')\n            init_paths()\n            installer.check_extensions()\n        else:\n            installer.log.info('Startup: standard')\n            installer.install_submodules()\n            init_paths()\n            installer.install_extensions()\n            installer.install_requirements() # redo requirements since extensions may change them\n            installer.update_wiki()\n            if len(installer.errors) == 0:\n                installer.log.debug(f'Setup complete without errors: {round(time.time())}')\n            else:\n                installer.log.warning(f'Setup complete with errors: {installer.errors}')\n                installer.log.warning(f'See log file for more details: {installer.log_file}')\n    installer.extensions_preload(parser) # adds additional args from extensions\n    args = installer.parse_args(parser)\n    get_custom_args()\n\n    uv, instance = start_server(immediate=True, server=None)\n    while True:\n        try:\n            alive = uv.thread.is_alive()\n            requests = uv.server_state.total_requests if hasattr(uv, 'server_state') else 0\n        except Exception:\n            alive = False\n            requests = 0\n        if round(time.time()) % 120 == 0:\n            installer.log.debug(f'Server: alive={alive} requests={requests} memory={get_memory_stats()} {instance.state.status()}')\n        if not alive:\n            if uv is not None and uv.wants_restart:\n                installer.log.info('Server restarting...')\n                uv, instance = start_server(immediate=False, server=instance)\n            else:\n                installer.log.info('Exiting...')\n                break\n        time.sleep(1)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "modules",
          "type": "tree",
          "content": null
        },
        {
          "name": "motd",
          "type": "blob",
          "size": 0.0009765625,
          "content": " "
        },
        {
          "name": "package.json",
          "type": "blob",
          "size": 1.189453125,
          "content": "{\n  \"name\": \"@vladmandic/automatic\",\n  \"version\": \"dev\",\n  \"description\": \"SD.Next: Opinionated implementation of Stable Diffusion\",\n  \"author\": \"Vladimir Mandic <mandic00@live.com>\",\n  \"bugs\": {\n    \"url\": \"https://github.com/vladmandic/automatic/issues\"\n  },\n  \"homepage\": \"https://github.com/vladmandic/automatic\",\n  \"license\": \"Apache-2.0\",\n  \"engines\": {\n    \"node\": \">=14.0.0\"\n  },\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"git+https://github.com/vladmandic/automatic.git\"\n  },\n  \"scripts\": {\n    \"venv\": \"source venv/bin/activate\",\n    \"start\": \"python launch.py --debug --experimental\",\n    \"ruff\": \"ruff check\",\n    \"eslint\": \"eslint javascript/ extensions-builtin/sdnext-modernui/javascript/\",\n    \"pylint\": \"pylint *.py modules/ extensions-builtin/\",\n    \"lint\": \"npm run eslint; npm run ruff; npm run pylint\",\n    \"test\": \"cli/test.sh\"\n  },\n  \"devDependencies\": {\n    \"esbuild\": \"^0.18.15\"\n  },\n  \"dependencies\": {\n    \"argparse\": \"^2.0.1\",\n    \"eslint\": \"^8.57.0\",\n    \"eslint-config-airbnb-base\": \"^15.0.0\",\n    \"eslint-plugin-css\": \"^0.9.2\",\n    \"eslint-plugin-html\": \"^8.1.1\",\n    \"eslint-plugin-json\": \"^3.1.0\",\n    \"eslint-plugin-markdown\": \"^4.0.1\",\n    \"eslint-plugin-node\": \"^11.1.0\"\n  }\n}\n"
        },
        {
          "name": "repositories",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.921875,
          "content": "# required for python 3.12\nsetuptools==69.5.1\n\n# standard\npatch-ng\nanyio\naddict\nastunparse\nfiletype\nfuture\nGitPython\nhttpcore\ninflection\njsonmerge\nkornia\nlark\nomegaconf\noptimum\npiexif\npsutil\npyyaml\nresize-right\nrich\ntoml\nvoluptuous\nyapf\nfasteners\norjson\nruff\npylint\ninvisible-watermark\npi-heif\n\n# versioned\nsafetensors==0.4.5\ntensordict==0.1.2\npeft==0.14.0\nhttpx==0.24.1\ncompel==2.0.3\ntorchsde==0.2.6\nantlr4-python3-runtime==4.9.3\nrequests==2.32.3\ntqdm==4.66.5\naccelerate==1.2.1\nopencv-contrib-python-headless==4.9.0.80\neinops==0.4.1\ngradio==3.43.2\nhuggingface_hub==0.27.0\nnumexpr==2.8.8\nnumpy==1.26.4\nnumba==0.59.1\nprotobuf==4.25.3\npytorch_lightning==1.9.4\ntokenizers==0.21.0\ntransformers==4.47.1\nurllib3==1.26.19\nPillow==10.4.0\ntimm==0.9.16\npydantic==1.10.15\npyparsing==3.1.4\ntyping-extensions==4.12.2\n\n# additional\nblendmodes\nscipy\npandas\ntorchdiffeq\ndctorch\nscikit-image\nseam-carving\nsentencepiece\n\n# block\ntorch!=2.5.0\ntorchvision!=0.20.0\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "webui.bat",
          "type": "blob",
          "size": 2.2626953125,
          "content": ":: --------------------------------------------------------------------------------------------------------------\n:: Do not make any changes to this file. Instead, create a shortcut to this file and add needed arguments there.\n:: --------------------------------------------------------------------------------------------------------------\n\n@echo off\n\nif not defined PYTHON (set PYTHON=python)\nif not defined VENV_DIR (set \"VENV_DIR=%~dp0%venv\")\nset ERROR_REPORTING=FALSE\nmkdir tmp 2>NUL\n\n%PYTHON% -c \"\" >tmp/stdout.txt 2>tmp/stderr.txt\nif %ERRORLEVEL% == 0 goto :check_pip\necho Cannot launch python\ngoto :show_stdout_stderr\n\n:check_pip\n%PYTHON% -mpip --help >tmp/stdout.txt 2>tmp/stderr.txt\nif %ERRORLEVEL% == 0 goto :start_venv\nif \"%PIP_INSTALLER_LOCATION%\" == \"\" goto :show_stdout_stderr\n%PYTHON% \"%PIP_INSTALLER_LOCATION%\" >tmp/stdout.txt 2>tmp/stderr.txt\nif %ERRORLEVEL% == 0 goto :start_venv\necho Cannot install pip\ngoto :show_stdout_stderr\n\n:start_venv\nif [\"%VENV_DIR%\"] == [\"-\"] goto :skip_venv\nif [\"%SKIP_VENV%\"] == [\"1\"] goto :skip_venv\n\ndir \"%VENV_DIR%\\Scripts\\Python.exe\" >tmp/stdout.txt 2>tmp/stderr.txt\nif %ERRORLEVEL% == 0 goto :activate_venv\n\nfor /f \"delims=\" %%i in ('CALL %PYTHON% -c \"import sys; print(sys.executable)\"') do set PYTHON_FULLNAME=\"%%i\"\necho Using python: %PYTHON_FULLNAME%\necho Creating VENV: %VENV_DIR%\n%PYTHON_FULLNAME% -m venv \"%VENV_DIR%\" >tmp/stdout.txt 2>tmp/stderr.txt\nif %ERRORLEVEL% == 0 goto :activate_venv\necho Failed creating VENV: \"%VENV_DIR%\"\ngoto :show_stdout_stderr\n\n:activate_venv\nset PYTHON=\"%VENV_DIR%\\Scripts\\Python.exe\"\necho Using VENV: %VENV_DIR%\n\n:skip_venv\nif [%ACCELERATE%] == [\"True\"] goto :accelerate\ngoto :launch\n\n:accelerate\nset ACCELERATE=\"%VENV_DIR%\\Scripts\\accelerate.exe\"\nif EXIST %ACCELERATE% goto :accelerate_launch\n\n:launch\n%PYTHON% launch.py %*\npause\nexit /b\n\n:accelerate_launch\necho Using accelerate\n%ACCELERATE% launch --num_cpu_threads_per_process=6 launch.py %*\npause\nexit /b\n\n:show_stdout_stderr\n\necho.\necho exit code: %errorlevel%\n\nfor /f %%i in (\"tmp\\stdout.txt\") do set size=%%~zi\nif %size% equ 0 goto :show_stderr\necho.\necho stdout:\ntype tmp\\stdout.txt\n\n:show_stderr\nfor /f %%i in (\"tmp\\stderr.txt\") do set size=%%~zi\nif %size% equ 0 goto :show_stderr\necho.\necho stderr:\ntype tmp\\stderr.txt\n\n:endofscript\n\necho.\necho Launch Failed\npause\n"
        },
        {
          "name": "webui.ps1",
          "type": "blob",
          "size": 2.451171875,
          "content": "# --------------------------------------------------------------------------------------------------------------\n# Do not make any changes to this file, change the variables in webui-user.ps1 instead and call this file\n# --------------------------------------------------------------------------------------------------------------\n\nfunction ShowStdOutStdErr {\n    Write-Output \"exit code: $LASTEXITCODE\"\n\n    if ((Get-Item tmp\\stdout.txt).Length -ne 0) {\n        Write-Output \"`nstdout:\"\n        Get-Content tmp\\stdout.txt\n    }\n\n    if ((Get-Item tmp\\stderr.txt).Length -ne 0) {\n        Write-Error \"`nstderr:\"\n        Get-Content tmp\\stderr.txt\n    }\n\n    Write-Output \"`nLaunch Failed\"\n    Pause\n}\n\n$PYTHON = if ($env:PYTHON) { $env:PYTHON } else { 'python' }\n$VENV_DIR = if ($env:VENV_DIR) { $env:VENV_DIR } else { Join-Path $PSScriptRoot 'venv' }\n\nNew-Item -Path 'tmp' -ItemType Directory -ErrorAction SilentlyContinue\n\ntry {\n    & $PYTHON -c '' 2>tmp\\stderr.txt | Out-File tmp\\stdout.txt\n} catch {\n    Write-Output 'Cannot launch python'\n    . ShowStdOutStdErr\n\n    exit\n}\n\ntry {\n    & $PYTHON -m pip --help 2>tmp\\stderr.txt | Out-File tmp\\stdout.txt\n} catch {\n    if (!$env:PIP_INSTALLER_LOCATION) {\n        . ShowStdOutStdErr\n\n        exit\n    }\n\n    & $PYTHON $env:PIP_INSTALLER_LOCATION 2>tmp\\stderr.txt | Out-File tmp\\stdout.txt\n\n    if ($LASTEXITCODE -ne 0) {\n        Write-Output 'Cannot install pip'\n        . ShowStdOutStdErr\n        exit\n    }\n}\n\nif ($VENV_DIR -ne '-' -and $env:SKIP_VENV -ne '1') {\n    if (Test-Path (Join-Path $VENV_DIR 'Scripts\\Python.exe')) {\n        $PYTHON = Join-Path $VENV_DIR 'Scripts\\Python.exe'\n    } else {\n        $PYTHON_FULLNAME = & $PYTHON -c 'import sys; print(sys.executable)'\n\n        Write-Output \"Using python: $PYTHON_FULLNAME\"\n        Write-Output \"Creating VENV: $VENV_DIR\"\n\n        & $PYTHON_FULLNAME -m venv $VENV_DIR 2>tmp\\stderr.txt | Out-File tmp\\stdout.txt\n\n        if ($LASTEXITCODE -eq 0) {\n            $PYTHON = Join-Path $VENV_DIR 'Scripts\\Python.exe'\n        } else {\n            Write-Output \"Failed creating VENV: '$VENV_DIR'\"\n            . ShowStdOutStdErr\n\n            exit\n        }\n    }\n\n    Write-Output \"Using VENV: $VENV_DIR\"\n}\n\nif ($env:ACCELERATE -eq 'True') {\n    $ACCELERATE = Join-Path $VENV_DIR 'Scripts\\accelerate.exe'\n\n    if (Test-Path $ACCELERATE) {\n        Write-Output 'Using accelerate'\n        & $ACCELERATE launch --num_cpu_threads_per_process=6 launch.py $args\n    }\n} else {\n    & $PYTHON launch.py $args\n}\n\nPause\n"
        },
        {
          "name": "webui.py",
          "type": "blob",
          "size": 15.078125,
          "content": "import io\nimport os\nimport sys\nimport glob\nimport signal\nimport asyncio\nimport logging\nimport importlib\nimport contextlib\nfrom threading import Thread\nimport modules.hashes\nimport modules.loader\nimport torch # pylint: disable=wrong-import-order\nfrom modules import timer, errors, paths # pylint: disable=unused-import\nfrom installer import log, git_commit, custom_excepthook\n# import ldm.modules.encoders.modules # pylint: disable=unused-import, wrong-import-order\nfrom modules import shared, extensions, gr_tempdir, modelloader # pylint: disable=ungrouped-imports\nfrom modules import extra_networks, ui_extra_networks # pylint: disable=ungrouped-imports\nfrom modules.paths import create_paths\nfrom modules.call_queue import queue_lock, wrap_queued_call, wrap_gradio_gpu_call # pylint: disable=unused-import\nimport modules.devices\nimport modules.sd_checkpoint\nimport modules.sd_samplers\nimport modules.lowvram\nimport modules.scripts\nimport modules.sd_models\nimport modules.sd_vae\nimport modules.sd_unet\nimport modules.model_te\nimport modules.progress\nimport modules.ui\nimport modules.txt2img\nimport modules.img2img\nimport modules.upscaler\nimport modules.textual_inversion.textual_inversion\nimport modules.hypernetworks.hypernetwork\nimport modules.script_callbacks\nfrom modules.api.middleware import setup_middleware\nfrom modules.shared import cmd_opts, opts # pylint: disable=unused-import\n\n\nsys.excepthook = custom_excepthook\nlocal_url = None\nstate = shared.state\nbackend = shared.backend\nif not modules.loader.initialized:\n    timer.startup.record(\"libraries\")\nif cmd_opts.server_name:\n    server_name = cmd_opts.server_name\nelse:\n    server_name = \"0.0.0.0\" if cmd_opts.listen else None\nfastapi_args = {\n    \"version\": f'0.0.{git_commit}',\n    \"title\": \"SD.Next\",\n    \"description\": \"SD.Next\",\n    \"docs_url\": None,\n    \"redoc_url\": None,\n    # \"docs_url\": \"/docs\" if cmd_opts.docs else None, # custom handler in api.py\n    # \"redoc_url\": \"/redocs\" if cmd_opts.docs else None,\n}\n\nimport modules.sd_hijack\ntimer.startup.record(\"ldm\")\nmodules.loader.initialized = True\n\n\ndef check_rollback_vae():\n    if shared.cmd_opts.rollback_vae:\n        if not torch.cuda.is_available():\n            log.error(\"Rollback VAE functionality requires compatible GPU\")\n            shared.cmd_opts.rollback_vae = False\n        elif torch.__version__.startswith('1.') or torch.__version__.startswith('2.0'):\n            log.error(\"Rollback VAE functionality requires Torch 2.1 or higher\")\n            shared.cmd_opts.rollback_vae = False\n        elif 0 < torch.cuda.get_device_capability()[0] < 8:\n            log.error('Rollback VAE functionality device capabilities not met')\n            shared.cmd_opts.rollback_vae = False\n\n\ndef initialize():\n    log.debug('Initializing')\n\n    modules.sd_checkpoint.init_metadata()\n    modules.hashes.init_cache()\n    check_rollback_vae()\n\n    log.debug(f'Huggingface cache: path=\"{shared.opts.hfcache_dir}\"')\n\n    modules.sd_samplers.list_samplers()\n    timer.startup.record(\"samplers\")\n\n    modules.sd_vae.refresh_vae_list()\n    timer.startup.record(\"vae\")\n\n    modules.sd_unet.refresh_unet_list()\n    timer.startup.record(\"unet\")\n\n    modules.model_te.refresh_te_list()\n    timer.startup.record(\"te\")\n\n    modelloader.cleanup_models()\n    modules.sd_models.setup_model()\n    timer.startup.record(\"models\")\n\n    if not shared.opts.lora_legacy:\n        import modules.lora.networks as lora_networks\n        lora_networks.list_available_networks()\n        timer.startup.record(\"lora\")\n\n    shared.prompt_styles.reload()\n    timer.startup.record(\"styles\")\n\n    import modules.postprocess.codeformer_model as codeformer\n    codeformer.setup_model(shared.opts.codeformer_models_path)\n    sys.modules[\"modules.codeformer_model\"] = codeformer\n    import modules.postprocess.gfpgan_model as gfpgan\n    gfpgan.setup_model(shared.opts.gfpgan_models_path)\n    import modules.postprocess.yolo as yolo\n    yolo.initialize()\n    timer.startup.record(\"detailer\")\n\n    extensions.list_extensions()\n    timer.startup.record(\"extensions\")\n\n    log.info('Load extensions')\n    t_timer, t_total = modules.scripts.load_scripts()\n    timer.startup.record(\"extensions\")\n    timer.startup.records[\"extensions\"] = t_total # scripts can reset the time\n    log.debug(f'Extensions init time: {t_timer.summary()}')\n\n    modelloader.load_upscalers()\n    timer.startup.record(\"upscalers\")\n\n    if shared.opts.hypernetwork_enabled:\n        shared.reload_hypernetworks()\n        timer.startup.record(\"hypernetworks\")\n\n    ui_extra_networks.initialize()\n    ui_extra_networks.register_pages()\n    extra_networks.initialize()\n    extra_networks.register_default_extra_networks()\n    timer.startup.record(\"networks\")\n\n    if cmd_opts.tls_keyfile is not None and cmd_opts.tls_certfile is not None:\n        try:\n            if not os.path.exists(cmd_opts.tls_keyfile):\n                log.error(\"Invalid path to TLS keyfile given\")\n            if not os.path.exists(cmd_opts.tls_certfile):\n                log.error(f\"Invalid path to TLS certfile: '{cmd_opts.tls_certfile}'\")\n        except TypeError:\n            cmd_opts.tls_keyfile = cmd_opts.tls_certfile = None\n            log.error(\"TLS setup invalid, running webui without TLS\")\n        else:\n            log.info(\"Running with TLS\")\n        timer.startup.record(\"tls\")\n\n    # make the program just exit at ctrl+c without waiting for anything\n    def sigint_handler(_sig, _frame):\n        log.info('Exiting')\n        try:\n            for f in glob.glob(\"*.lock\"):\n                os.remove(f)\n        except Exception:\n            pass\n        sys.exit(0)\n\n    signal.signal(signal.SIGINT, sigint_handler)\n\n\ndef load_model():\n    if not shared.opts.sd_checkpoint_autoload and shared.cmd_opts.ckpt is None:\n        log.debug('Model auto load disabled')\n    else:\n        shared.state.begin('Load')\n        thread_model = Thread(target=lambda: shared.sd_model)\n        thread_model.start()\n        thread_refiner = Thread(target=lambda: shared.sd_refiner)\n        thread_refiner.start()\n        shared.state.end()\n        thread_model.join()\n        thread_refiner.join()\n    timer.startup.record(\"checkpoint\")\n    shared.opts.onchange(\"sd_model_checkpoint\", wrap_queued_call(lambda: modules.sd_models.reload_model_weights(op='model')), call=False)\n    shared.opts.onchange(\"sd_model_refiner\", wrap_queued_call(lambda: modules.sd_models.reload_model_weights(op='refiner')), call=False)\n    shared.opts.onchange(\"sd_model_dict\", wrap_queued_call(lambda: modules.sd_models.reload_model_weights(op='dict')), call=False)\n    shared.opts.onchange(\"sd_vae\", wrap_queued_call(lambda: modules.sd_vae.reload_vae_weights()), call=False)\n    shared.opts.onchange(\"sd_unet\", wrap_queued_call(lambda: modules.sd_unet.load_unet(shared.sd_model)), call=False)\n    shared.opts.onchange(\"sd_text_encoder\", wrap_queued_call(lambda: modules.sd_models.reload_text_encoder()), call=False)\n    shared.opts.onchange(\"sd_backend\", wrap_queued_call(lambda: modules.sd_models.change_backend()), call=False)\n    shared.opts.onchange(\"temp_dir\", gr_tempdir.on_tmpdir_changed)\n    timer.startup.record(\"onchange\")\n\n\ndef create_api(app):\n    log.debug('API initialize')\n    from modules.api.api import Api\n    api = Api(app, queue_lock)\n    return api\n\n\ndef async_policy():\n    _BasePolicy = asyncio.WindowsSelectorEventLoopPolicy if sys.platform == \"win32\" and hasattr(asyncio, \"WindowsSelectorEventLoopPolicy\") else asyncio.DefaultEventLoopPolicy\n\n    class AnyThreadEventLoopPolicy(_BasePolicy):\n        def handle_exception(self, context):\n            msg = context.get(\"exception\", context[\"message\"])\n            log.error(f\"AsyncIO loop: {msg}\")\n\n        def get_event_loop(self) -> asyncio.AbstractEventLoop:\n            try:\n                self.loop = super().get_event_loop()\n            except (RuntimeError, AssertionError):\n                self.loop = self.new_event_loop()\n                self.set_event_loop(self.loop)\n            return self.loop\n\n        def __init__(self):\n            super().__init__()\n            self.loop = self.get_event_loop()\n            self.loop.set_exception_handler(self.handle_exception)\n            # log.debug(f\"Event loop: {self.loop}\")\n\n    asyncio.set_event_loop_policy(AnyThreadEventLoopPolicy())\n\n\ndef start_common():\n    log.debug('Entering start sequence')\n    if shared.cmd_opts.data_dir is not None and len(shared.cmd_opts.data_dir) > 0:\n        log.info(f'Using data path: {shared.cmd_opts.data_dir}')\n    if shared.cmd_opts.models_dir is not None and len(shared.cmd_opts.models_dir) > 0 and shared.cmd_opts.models_dir != 'models':\n        log.info(f'Models path: {shared.cmd_opts.models_dir}')\n    create_paths(shared.opts)\n    async_policy()\n    initialize()\n    try:\n        from installer import diffusers_commit\n        if diffusers_commit != 'unknown':\n            shared.opts.diffusers_version = diffusers_commit # update installed diffusers version\n    except Exception:\n        pass\n    if shared.opts.clean_temp_dir_at_start:\n        gr_tempdir.cleanup_tmpdr()\n        timer.startup.record(\"cleanup\")\n\n\ndef start_ui():\n    log.debug('UI start sequence')\n    modules.script_callbacks.before_ui_callback()\n    timer.startup.record(\"before-ui\")\n    shared.demo = modules.ui.create_ui(timer.startup)\n    timer.startup.record(\"ui\")\n    if cmd_opts.disable_queue:\n        log.info('Server queues disabled')\n        shared.demo.progress_tracking = False\n    else:\n        shared.demo.queue(concurrency_count=64)\n\n    gradio_auth_creds = []\n    if cmd_opts.auth:\n        gradio_auth_creds += [x.strip() for x in cmd_opts.auth.strip('\"').replace('\\n', '').split(',') if x.strip()]\n    if cmd_opts.auth_file:\n        if not os.path.exists(cmd_opts.auth_file):\n            log.error(f\"Invalid path to auth file: '{cmd_opts.auth_file}'\")\n        else:\n            with open(cmd_opts.auth_file, 'r', encoding=\"utf8\") as file:\n                for line in file.readlines():\n                    gradio_auth_creds += [x.strip() for x in line.split(',') if x.strip()]\n    if len(gradio_auth_creds) > 0:\n        log.info(f'Authentication enabled: users={len(list(gradio_auth_creds))}')\n\n    global local_url # pylint: disable=global-statement\n    stdout = io.StringIO()\n    allowed_paths = [os.path.dirname(__file__)]\n    if cmd_opts.data_dir is not None and os.path.isdir(cmd_opts.data_dir):\n        allowed_paths.append(cmd_opts.data_dir)\n    if cmd_opts.allowed_paths is not None:\n        allowed_paths += [p for p in cmd_opts.allowed_paths if os.path.isdir(p)]\n    shared.log.debug(f'Root paths: {allowed_paths}')\n    with contextlib.redirect_stdout(stdout):\n        app, local_url, share_url = shared.demo.launch( # app is FastAPI(Starlette) instance\n            share=cmd_opts.share,\n            server_name=server_name,\n            server_port=cmd_opts.port if cmd_opts.port != 7860 else None,\n            ssl_keyfile=cmd_opts.tls_keyfile,\n            ssl_certfile=cmd_opts.tls_certfile,\n            ssl_verify=not cmd_opts.tls_selfsign,\n            debug=False,\n            auth=[tuple(cred.split(':')) for cred in gradio_auth_creds] if gradio_auth_creds else None,\n            prevent_thread_lock=True,\n            max_threads=64,\n            show_api=False,\n            quiet=True,\n            favicon_path='html/favicon.svg',\n            allowed_paths=allowed_paths,\n            app_kwargs=fastapi_args,\n            _frontend=True and cmd_opts.share,\n        )\n    if cmd_opts.data_dir is not None:\n        gr_tempdir.register_tmp_file(shared.demo, os.path.join(cmd_opts.data_dir, 'x'))\n    shared.log.info(f'Local URL: {local_url}')\n    if cmd_opts.docs:\n        shared.log.info(f'API Docs: {local_url[:-1]}/docs') # pylint: disable=unsubscriptable-object\n        shared.log.info(f'API ReDocs: {local_url[:-1]}/redocs') # pylint: disable=unsubscriptable-object\n    if share_url is not None:\n        shared.log.info(f'Share URL: {share_url}')\n    # shared.log.debug(f'Gradio functions: registered={len(shared.demo.fns)}')\n    shared.demo.server.wants_restart = False\n    setup_middleware(app, cmd_opts)\n\n    if cmd_opts.subpath:\n        import gradio\n        gradio.mount_gradio_app(app, shared.demo, path=f\"/{cmd_opts.subpath}\")\n        shared.log.info(f'Redirector mounted: /{cmd_opts.subpath}')\n\n    timer.startup.record(\"launch\")\n\n    modules.progress.setup_progress_api(app)\n    shared.api = create_api(app)\n    timer.startup.record(\"api\")\n\n    ui_extra_networks.init_api(app)\n\n    modules.script_callbacks.app_started_callback(shared.demo, app)\n    timer.startup.record(\"app-started\")\n\n    time_sorted = sorted(modules.scripts.time_setup.items(), key=lambda x: x[1], reverse=True)\n    time_script = [f'{k}:{round(v,3)}' for (k,v) in time_sorted if v > 0.01]\n    time_total = sum(modules.scripts.time_setup.values())\n    shared.log.debug(f'Scripts setup: time={time_total:.3f} {time_script}')\n    time_component = [f'{k}:{round(v,3)}' for (k,v) in modules.scripts.time_component.items() if v > 0.005]\n    if len(time_component) > 0:\n        shared.log.debug(f'Scripts components: {time_component}')\n\n\ndef webui(restart=False):\n    if restart:\n        modules.script_callbacks.app_reload_callback()\n        modules.script_callbacks.script_unloaded_callback()\n\n    start_common()\n    start_ui()\n    modules.script_callbacks.after_ui_callback()\n    modules.sd_models.write_metadata()\n    load_model()\n    shared.opts.save(shared.config_filename)\n    if cmd_opts.profile:\n        for k, v in modules.script_callbacks.callback_map.items():\n            shared.log.debug(f'Registered callbacks: {k}={len(v)} {[c.script for c in v]}')\n    debug = log.trace if os.environ.get('SD_SCRIPT_DEBUG', None) is not None else lambda *args, **kwargs: None\n    debug('Trace: SCRIPTS')\n    for m in modules.scripts.scripts_data:\n        debug(f'  {m}')\n    debug('Loaded postprocessing scripts:')\n    for m in modules.scripts.postprocessing_scripts_data:\n        debug(f'  {m}')\n    modules.script_callbacks.print_timers()\n    log.info(f\"Startup time: {timer.startup.summary()}\")\n    timer.startup.reset()\n\n    if not restart:\n        # override all loggers to use the same handlers as the main logger\n        for logger in [logging.getLogger(name) for name in logging.root.manager.loggerDict]: # pylint: disable=no-member\n            if logger.name.startswith('uvicorn') or logger.name.startswith('sd'):\n                continue\n            logger.handlers = log.handlers\n        # autolaunch only on initial start\n        if (shared.opts.autolaunch or cmd_opts.autolaunch) and local_url is not None:\n            cmd_opts.autolaunch = False\n            shared.log.info('Launching browser')\n            import webbrowser\n            webbrowser.open(local_url, new=2, autoraise=True)\n    else:\n        for module in [module for name, module in sys.modules.items() if name.startswith(\"modules.ui\")]:\n            importlib.reload(module)\n\n    return shared.demo.server\n\n\ndef api_only():\n    start_common()\n    from fastapi import FastAPI\n    app = FastAPI(**fastapi_args)\n    setup_middleware(app, cmd_opts)\n    shared.api = create_api(app)\n    shared.api.wants_restart = False\n    modules.script_callbacks.app_started_callback(None, app)\n    modules.sd_models.write_metadata()\n    log.info(f\"Startup time: {timer.startup.summary()}\")\n    server = shared.api.launch()\n    return server\n\n\nif __name__ == \"__main__\":\n    if cmd_opts.api_only:\n        api_only()\n    else:\n        webui()\n"
        },
        {
          "name": "webui.sh",
          "type": "blob",
          "size": 2.7412109375,
          "content": "#!/usr/bin/env bash\n# -------------------------------------------------------------------------------------------------------------\n# Do not make any changes to this file, change the variables in webui-user.sh instead and call this file\n# -------------------------------------------------------------------------------------------------------------\n\n# change to local directory\ncd -- \"$(dirname -- \"$0\")\"\n\ncan_run_as_root=0\nexport ERROR_REPORTING=FALSE\nexport PIP_IGNORE_INSTALLED=0\n\n# Read variables from webui-user.sh\nif [[ -f webui-user.sh ]]\nthen\n    source ./webui-user.sh\nfi\n\n# python3 executable\nPYTHON_ENV=\"${PYTHON}\"\nif [[ -z \"${PYTHON}\" ]]\nthen\n    PYTHON=\"python3\"\nfi\n\n# git executable\nif [[ -z \"${GIT}\" ]]\nthen\n    export GIT=\"git\"\nfi\n\nif [[ -z \"${venv_dir}\" ]]\nthen\n    venv_dir=\"venv\"\nfi\n\n# read any command line flags to the webui.sh script\nwhile getopts \"f\" flag > /dev/null 2>&1\ndo\n    case ${flag} in\n        f) can_run_as_root=1;;\n        *) break;;\n    esac\ndone\n\n# Do not run as root unless inside a Docker container\nif [[ $(id -u) -eq 0 && can_run_as_root -eq 0 && ! -f /.dockerenv ]]\nthen\n    echo \"Cannot run as root\"\n    exit 1\nfi\n\nfor preq in \"${GIT}\" \"${PYTHON}\"\ndo\n    if ! hash \"${preq}\" &>/dev/null\n    then\n        printf \"Error: %s is not installed, aborting...\\n\" \"${preq}\"\n        exit 1\n    fi\ndone\n\nif ! \"${PYTHON}\" -c \"import venv\" &>/dev/null\nthen\n    echo \"Error: python3-venv is not installed\"\n    exit 1\nfi\n\nif [[ ! -d \"${venv_dir}\" ]]\nthen\n    echo \"Create python venv\"\n    \"${PYTHON}\" -m venv \"${venv_dir}\"\n    first_launch=1\nfi\n\nif [[ -f \"${venv_dir}\"/bin/activate ]]\nthen\n    source \"${venv_dir}\"/bin/activate\n    echo \"Activate python venv: $VIRTUAL_ENV\"\nelse\n    echo \"Error: Cannot activate python venv\"\n    exit 1\nfi\n\n# Add venv lib folder to PATH\nif [ -d \"$(realpath \"$venv_dir\")/lib/\" ] && [[ -z \"${DISABLE_VENV_LIBS}\" ]]\nthen\n    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(realpath \"$venv_dir\")/lib/\nfi\n\n# Add ROCm to PATH if it's not already\nif  [ ! -x \"$(command -v rocminfo)\" ] && [ -f '/opt/rocm/bin/rocminfo' ]\nthen\n    export PATH=$PATH:/opt/rocm/bin\n    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/rocm/lib\nfi\n\nif [[ ! -z \"${ACCELERATE}\" ]] && [ ${ACCELERATE}=\"True\" ] && [ -x \"$(command -v accelerate)\" ]\nthen\n    echo \"Launch: accelerate\"\n    exec accelerate launch --num_cpu_threads_per_process=6 launch.py \"$@\"\nelif [[ ! -z \"${IPEXRUN}\" ]] && [ ${IPEXRUN}=\"True\" ] && [ -x \"$(command -v ipexrun)\" ]\nthen\n    echo \"Launch: ipexrun\"\n    exec ipexrun --multi-task-manager 'taskset' --memory-allocator 'jemalloc' launch.py \"$@\"\nelif [[ -f \"${venv_dir}/bin/python3\" ]]\nthen\n    PYTHON=\"${venv_dir}/bin/python3\"\n    echo \"Launch: ${PYTHON}\"\n    exec \"${PYTHON}\" launch.py \"$@\"\nelse\n    echo \"Launch: ${PYTHON}\"\n    exec \"${PYTHON}\" launch.py \"$@\"\nfi\n"
        },
        {
          "name": "wiki",
          "type": "commit",
          "content": null
        }
      ]
    }
  ]
}