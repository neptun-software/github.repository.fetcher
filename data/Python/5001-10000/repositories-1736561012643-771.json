{
  "metadata": {
    "timestamp": 1736561012643,
    "page": 771,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "QwenLM/Qwen-VL",
      "stars": 5292,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.126953125,
          "content": "__pycache__\n*.so\nbuild\n.coverage_*\n*.egg-info\n*~\n.vscode/\n.idea/\n.DS_Store\n\n/private/\nQwen-VL-Chat/\nQwen-VL-Chat-Int4/\nSimSun.ttf\n"
        },
        {
          "name": "BUILD.md",
          "type": "blob",
          "size": 1.0048828125,
          "content": "## qwen web demo\n\n### build\n\n```\ndocker build -t qwen-vl-chat:webdemo --platform linux/amd64 -f Dockerfile.qwendemo . \n```\n\n### run\n\n```\ndocker run -it --gpus device=0 -d --restart always -v /var/run/docker.sock:/var/run/docker.sock --name qwen-vl-chat -p 8000:8000 --user=20001:20001 --platform linux/amd64 qwen-vl-chat:webdemo\n```\n\n## qwen openai api\n\n### build\n\n```\ndocker build -t qwen-vl-chat:openai --platform linux/amd64 -f Dockerfile.qwenopenai . \n```\n\n### run\n\n```\ndocker run -it --gpus device=0 -d --restart always -v /var/run/docker.sock:/var/run/docker.sock --name qwen-vl-chat -p 8080:8080 --user=20001:20001 --platform linux/amd64 qwen-vl-chat:openai\n```\n\n## qwen-int4 openai api\n\n### build\n\n```\ndocker build -t qwen-vl-chat:int4-openai --platform linux/amd64 -f Dockerfile.qwenint4openai . \n```\n\n### run\n\n```\ndocker run -it --gpus device=0 -d --restart always -v /var/run/docker.sock:/var/run/docker.sock --name qwen-vl-chat-int4 -p 8080:8080 --user=20001:20001 --platform linux/amd64 qwen-vl-chat:int4-openai\n```\n"
        },
        {
          "name": "Dockerfile.qwendemo",
          "type": "blob",
          "size": 1.646484375,
          "content": "# python 3.8 and above\n# pytorch 1.12 and above, 2.0 and above are recommended\n# CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\n\n# based on modelscope docker image\n# registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\n# registry.cn-beijing.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\nFROM registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\n\nARG workdir=/var/app\nRUN mkdir -p ${workdir}\n\nRUN git lfs install\n\nWORKDIR ${workdir}\nCOPY requirements.txt requirements_web_demo.txt ./\n\n# Install Qwen dependencies\nRUN pip install -r requirements.txt\n\n# Install webUI dependencies\nWORKDIR ${workdir}\nRUN pip install -r requirements_web_demo.txt\n\n# Offline mode, check https://huggingface.co/docs/transformers/v4.15.0/installation#offline-mode\nENV HF_DATASETS_OFFLINE=1\nENV TRANSFORMERS_OFFLINE=1\n\n# set TZ, make logs dir, and expose port 8080\nENV TZ=Asia/Shanghai\nRUN mkdir -p ${workdir}/logs && chmod 777 ${workdir}/logs\nVOLUME /var/app/logs\n\n# create user 20001\nRUN useradd -r -m appuser -u 20001 -g 0\n\nWORKDIR ${workdir}\n# copy model\nRUN git clone https://huggingface.co/Qwen/Qwen-VL-Chat\n# COPY --chown=20001:20001 Qwen-VL-Chat ./Qwen-VL-Chat\n# copy fonts\nADD --chown=20001:20001 https://github.com/StellarCN/scp_zh/raw/master/fonts/SimSun.ttf ./\n# COPY --chown=20001:20001 SimSun.ttf ./\n# copy main app\nCOPY --chown=20001:20001 web_demo_mm.py ./\n\nEXPOSE 8000\nCMD [\"python3\", \"web_demo_mm.py\", \"-c\", \"./Qwen-VL-Chat\", \"--server-name\", \"0.0.0.0\", \"--server-port\", \"8000\"]\n"
        },
        {
          "name": "Dockerfile.qwenint4openai",
          "type": "blob",
          "size": 2.1572265625,
          "content": "# python 3.8 and above\n# pytorch 1.12 and above, 2.0 and above are recommended\n# CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\n\n# based on modelscope docker image\n# registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\n# registry.cn-beijing.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\nFROM registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\n\nARG workdir=/var/app\nRUN mkdir -p ${workdir}\n\nRUN git lfs install\n\nWORKDIR ${workdir}\nCOPY requirements.txt requirements_web_demo.txt ./\n\n# Install Qwen dependencies\nRUN pip install -r requirements.txt\n\n# Install webUI dependencies\nWORKDIR ${workdir}\nRUN pip install -r requirements_web_demo.txt\n\n# Offline mode, check https://huggingface.co/docs/transformers/v4.15.0/installation#offline-mode\nENV HF_DATASETS_OFFLINE=1\nENV TRANSFORMERS_OFFLINE=1\n\n# set TZ, make logs dir, and expose port 8080\nENV TZ=Asia/Shanghai\nRUN mkdir -p ${workdir}/logs && chmod 777 ${workdir}/logs\nVOLUME /var/app/logs\n\n# create user 20001\nRUN useradd -r -m appuser -u 20001 -g 0\n\nWORKDIR ${workdir}\n# copy model\nRUN git clone https://huggingface.co/Qwen/Qwen-VL-Chat-Int4\n# COPY --chown=20001:20001 Qwen-VL-Chat-Int4 ./Qwen-VL-Chat-Int4\n\n# Install AutoGPTQ\nRUN pip install optimum\n# RUN git clone https://github.com/JustinLin610/AutoGPTQ.git && \\\n#     cd AutoGPTQ && \\\n#     pip install -v .\nRUN pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/\n\n# Install OpenAI API dependencies\nWORKDIR ${workdir}\nCOPY requirements_openai_api.txt ./\nRUN pip install -r requirements_openai_api.txt\n# copy fonts\nADD --chown=20001:20001 https://github.com/StellarCN/scp_zh/raw/master/fonts/SimSun.ttf ./\n# COPY --chown=20001:20001 SimSun.ttf ./\n# copy main app\nCOPY --chown=20001:20001 openai_api.py ./\n\nEXPOSE 8080\n# CMD [\"python3\", \"openai_api.py\", \"-c\", \"./Qwen-VL-Chat\", \"--server-name\", \"0.0.0.0\", \"--server-port\", \"8080\"]\nCMD [\"python3\", \"openai_api.py\", \"-c\", \"./Qwen-VL-Chat-Int4\", \"--server-name\", \"0.0.0.0\", \"--server-port\", \"8080\"]\n"
        },
        {
          "name": "Dockerfile.qwenopenai",
          "type": "blob",
          "size": 1.7783203125,
          "content": "# python 3.8 and above\n# pytorch 1.12 and above, 2.0 and above are recommended\n# CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\n\n# based on modelscope docker image\n# registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\n# registry.cn-beijing.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\nFROM registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\n\nARG workdir=/var/app\nRUN mkdir -p ${workdir}\n\nRUN git lfs install\n\nWORKDIR ${workdir}\nCOPY requirements.txt requirements_web_demo.txt ./\n\n# Install Qwen dependencies\nRUN pip install -r requirements.txt\n\n# Install webUI dependencies\nWORKDIR ${workdir}\nRUN pip install -r requirements_web_demo.txt\n\n# Offline mode, check https://huggingface.co/docs/transformers/v4.15.0/installation#offline-mode\nENV HF_DATASETS_OFFLINE=1\nENV TRANSFORMERS_OFFLINE=1\n\n# set TZ, make logs dir, and expose port 8080\nENV TZ=Asia/Shanghai\nRUN mkdir -p ${workdir}/logs && chmod 777 ${workdir}/logs\nVOLUME /var/app/logs\n\n# create user 20001\nRUN useradd -r -m appuser -u 20001 -g 0\n\nWORKDIR ${workdir}\n# copy model\nRUN git clone https://huggingface.co/Qwen/Qwen-VL-Chat\n# COPY --chown=20001:20001 Qwen-VL-Chat ./Qwen-VL-Chat\n\n# Install OpenAI API dependencies\nWORKDIR ${workdir}\nCOPY requirements_openai_api.txt ./\nRUN pip install -r requirements_openai_api.txt\n# copy fonts\nADD --chown=20001:20001 https://github.com/StellarCN/scp_zh/raw/master/fonts/SimSun.ttf ./\n# COPY --chown=20001:20001 SimSun.ttf ./\n# copy main app\nCOPY --chown=20001:20001 openai_api.py ./\n\nEXPOSE 8080\nCMD [\"python3\", \"openai_api.py\", \"-c\", \"./Qwen-VL-Chat\", \"--server-name\", \"0.0.0.0\", \"--server-port\", \"8080\"]\n"
        },
        {
          "name": "FAQ.md",
          "type": "blob",
          "size": 1.7666015625,
          "content": "# FAQ\n\n## Installation & Environment\n\n#### Which version of transformers should I use?\n\n4.31.0 is preferred.\n\n#### I downloaded the codes and checkpoints but I can't load the model locally. What should I do?\n\nPlease check if you have updated the code to the latest, and correctly downloaded all the sharded checkpoint files.\n\n#### `qwen.tiktoken` is not found. What is it?\n\nThis is the merge file of the tokenizer. You have to download it. Note that if you just git clone the repo without [git-lfs](https://git-lfs.com), you cannot download this file.\n\n#### transformers_stream_generator/tiktoken/accelerate not found\n\nRun the command `pip install -r requirements.txt`. You can find the file at [https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt](https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt).\n<br><br>\n\n\n\n## Demo & Inference\n\n#### Is there any demo?\n\nYes, see `web_demo_mm.py` for web demo. See README for more information.\n\n\n\n#### Can Qwen-VL support streaming?\n\nNo. We do not support streaming yet.\n\n#### It seems that the generation is not related to the instruction...\n\nPlease check if you are loading Qwen-VL-Chat instead of Qwen-VL. Qwen-VL is the base model without alignment, which behaves differently from the SFT/Chat model.\n\n#### Is quantization supported?\n\nNo. We would support quantization asap.\n\n#### Unsatisfactory performance in processing long sequences\n\nPlease ensure that NTK is applied. `use_dynamc_ntk` and `use_logn_attn` in `config.json` should be set to `true` (`true` by default).\n<br><br>\n\n\n## Tokenizer\n\n#### bos_id/eos_id/pad_id not found\n\nIn our training, we only use `<|endoftext|>` as the separator and padding token. You can set bos_id, eos_id, and pad_id to tokenizer.eod_id. Learn more about our tokenizer from our documents about the tokenizer.\n\n"
        },
        {
          "name": "FAQ_ja.md",
          "type": "blob",
          "size": 2.48046875,
          "content": "# FAQ\n\n## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ç’°å¢ƒ\n\n#### transformers ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ï¼Ÿ\n\n4.31.0 ãŒæœ›ã¾ã—ã„ã§ã™ã€‚\n\n#### ã‚³ãƒ¼ãƒ‰ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸãŒã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã€‚ã©ã†ã™ã‚Œã°ã‚ˆã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n\nã‚³ãƒ¼ãƒ‰ã‚’æœ€æ–°ã®ã‚‚ã®ã«æ›´æ–°ã—ã€ã™ã¹ã¦ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£ã—ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‹ã©ã†ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n\n#### `qwen.tiktoken` ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã“ã‚Œã¯ä½•ã§ã™ã‹ï¼Ÿ\n\nã“ã‚Œã¯ tokenizer ã®ãƒãƒ¼ã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚[git-lfs](https://git-lfs.com) ã‚’ä½¿ã‚ãšã«ãƒªãƒã‚¸ãƒˆãƒªã‚’ git clone ã—ãŸã ã‘ã§ã¯ã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ããªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n\n#### transformers_stream_generator/tiktoken/accelerate ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\nã‚³ãƒãƒ³ãƒ‰ `pip install -r requirements.txt` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ [https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt](https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt) ã«ã‚ã‚Šã¾ã™ã€‚\n<br><br>\n\n\n\n## ãƒ‡ãƒ¢ã¨æ¨è«–\n\n#### ãƒ‡ãƒ¢ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\n\nã‚¦ã‚§ãƒ–ãƒ‡ãƒ¢ã¯ `web_demo_mm.py` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚è©³ç´°ã¯ README ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n\n\n#### Qwen-VLã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã‹ï¼Ÿ\n\nã„ã„ãˆã€ã¾ã ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚\n\n#### ä¸–ä»£ã¨å‘½ä»¤ã¯é–¢ä¿‚ãªã„ã‚ˆã†ã§ã™ãŒ...\n\nQwen-VL ã§ã¯ãªã Qwen-VL-Chat ã‚’èª­ã¿è¾¼ã‚“ã§ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚Qwen-VL ã¯ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãªã—ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã€SFT/Chat ãƒ¢ãƒ‡ãƒ«ã¨ã¯å‹•ä½œãŒç•°ãªã‚Šã¾ã™ã€‚\n\n#### é‡å­åŒ–ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\n\nã„ã„ãˆã€‚æ—©æ€¥ã«é‡å­åŒ–ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã¤ã‚‚ã‚Šã§ã™ã€‚\n\n#### é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å‡¦ç†ã§ä¸æº€è¶³ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n\nNTK ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚`config.json` ã® `use_dynamc_ntk` ã¨ `use_logn_attn` ã‚’ `true` ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ `true`ï¼‰ã€‚\n<br><br>\n\n\n## Tokenizer\n\n#### bos_id/eos_id/pad_id ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\nç§ãŸã¡ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ã€ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ã¨ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ `<|endoftext|>` ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚bos_idã€eos_idã€pad_id ã¯ tokenizer.eod_id ã«è¨­å®šã§ãã¾ã™ã€‚ç§ãŸã¡ã® tokenizer ã«ã¤ã„ã¦è©³ã—ãã¯ã€tokenizer ã«ã¤ã„ã¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã”è¦§ãã ã•ã„ã€‚\n\n"
        },
        {
          "name": "FAQ_ko.md",
          "type": "blob",
          "size": 2.1826171875,
          "content": "# FAQ\n\n## ì„¤ì¹˜ ë° í™˜ê²½\n\n#### ì–´ë–¤ ë²„ì „ì˜ transformersë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?\n\n4.31.0 ë²„ì „ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì„ í˜¸í•©ë‹ˆë‹¤.\n\n#### ì½”ë“œì™€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí–ˆëŠ”ë° ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì–´ìš”. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?\n\nì½”ë“œë¥¼ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸í–ˆëŠ”ì§€, ê·¸ë¦¬ê³  ëª¨ë“  ìƒ¤ë“œ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì˜¬ë°”ë¥´ê²Œ ë‹¤ìš´ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.\n\n#### `qwen.tiktoken`ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”. ì´ê²Œ ë¬´ì—‡ì¸ê°€ìš”?\n\nì´ê²ƒì€ í† í¬ë‚˜ì´ì €ì˜ ë³‘í•© íŒŒì¼ì…ë‹ˆë‹¤. ì´ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤. [git-lfs](https://git-lfs.com) ì—†ì´ ë‹¨ìˆœíˆ ê¹ƒ ì €ì¥ì†Œë¥¼ ë³µì œí–ˆë‹¤ë©´ ì´ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n#### transformers_stream_generator/tiktoken/accelerate not found ì˜¤ë¥˜\n\n`pip install -r requirements.txt` ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”. ì´ íŒŒì¼ì€ [https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt](https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n<br><br>\n\n\n## Demo & Inference\n\n#### ë°ëª¨ê°€ ìˆë‚˜ìš”?\n\në„¤, ì›¹ ë°ëª¨ëŠ” `web_demo_mm.py`ë¥¼ ì°¸ê³ í•˜ì„¸ìš”. ë” ë§ì€ ì •ë³´ëŠ” README íŒŒì¼ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\n#### Qwen-VLì€ ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ë‚˜ìš”?\n\nì•„ë‹ˆìš”. ì•„ì§ ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n#### ìƒì„±ëœ ë‚´ìš©ì´ ì§€ì‹œì‚¬í•­ê³¼ ê´€ë ¨ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n\nQwen-VL ëŒ€ì‹  Qwen-VL-Chatì„ ë¡œë“œí•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”. Qwen-VLì€ SFT/Chat ëª¨ë¸ê³¼ ë‹¬ë¦¬ ì •ë ¬ì´ ì—†ëŠ” ê¸°ë³¸ ëª¨ë¸ì´ë¯€ë¡œ ë‹¤ë¥´ê²Œ ì‘ë™í•©ë‹ˆë‹¤.\n\n#### ì–‘ìí™”ë¥¼ ì§€ì›í•˜ë‚˜ìš”?\n\nì•„ë‹ˆìš”. ê°€ëŠ¥í•œ ë¹¨ë¦¬ ì–‘ìí™”ë¥¼ ì§€ì›í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n\n#### ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ì—ì„œ ë§Œì¡±ìŠ¤ëŸ½ì§€ ëª»í•œ ì„±ëŠ¥\n\nNTKê°€ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”. `config.json`ì˜ `use_dynamc_ntk`ê³¼ `use_logn_attn`ì€ `true`ë¡œ ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤(`true`ê°€ ê¸°ë³¸ê°’).\n<br><br>\n\n\n## Tokenizer\n\n#### bos_id/eos_id/pad_id not found ì˜¤ë¥˜\n\nì €í¬ í›ˆë ¨ì—ì„œëŠ” ``ì„ êµ¬ë¶„ì ë° íŒ¨ë”© í† í°ìœ¼ë¡œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤. bos_id, eos_id, pad_idë¥¼ tokenizer.eod_idë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ì— ëŒ€í•œ ë¬¸ì„œì—ì„œ í† í¬ë‚˜ì´ì €ì— ëŒ€í•´ ë” ì•Œì•„ë³´ì„¸ìš”."
        },
        {
          "name": "FAQ_zh.md",
          "type": "blob",
          "size": 2.15625,
          "content": "# FAQ\n\n## å®‰è£…&ç¯å¢ƒ\n\n#### æˆ‘åº”è¯¥ç”¨å“ªä¸ªtransformersç‰ˆæœ¬ï¼Ÿ\n\nå»ºè®®ä½¿ç”¨4.31.0ã€‚\n\n#### æˆ‘æŠŠæ¨¡å‹å’Œä»£ç ä¸‹åˆ°æœ¬åœ°ï¼ŒæŒ‰ç…§æ•™ç¨‹æ— æ³•ä½¿ç”¨ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ\n\nç­”ï¼šåˆ«ç€æ€¥ï¼Œå…ˆæ£€æŸ¥ä½ çš„ä»£ç æ˜¯ä¸æ˜¯æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬ï¼Œç„¶åç¡®è®¤ä½ æ˜¯å¦å®Œæ•´åœ°å°†æ¨¡å‹checkpointä¸‹åˆ°æœ¬åœ°ã€‚\n\n#### `qwen.tiktoken`è¿™ä¸ªæ–‡ä»¶æ‰¾ä¸åˆ°ï¼Œæ€ä¹ˆåŠï¼Ÿ\n\nè¿™ä¸ªæ˜¯æˆ‘ä»¬çš„tokenizerçš„mergeæ–‡ä»¶ï¼Œä½ å¿…é¡»ä¸‹è½½å®ƒæ‰èƒ½ä½¿ç”¨æˆ‘ä»¬çš„tokenizerã€‚æ³¨æ„ï¼Œå¦‚æœä½ ä½¿ç”¨git cloneå´æ²¡æœ‰ä½¿ç”¨git-lfsï¼Œè¿™ä¸ªæ–‡ä»¶ä¸ä¼šè¢«ä¸‹è½½ã€‚å¦‚æœä½ ä¸äº†è§£git-lfsï¼Œå¯ç‚¹å‡»[å®˜ç½‘](https://git-lfs.com/)äº†è§£ã€‚\n\n#### transformers_stream_generator/tiktoken/accelerateï¼Œè¿™å‡ ä¸ªåº“æç¤ºæ‰¾ä¸åˆ°ï¼Œæ€ä¹ˆåŠï¼Ÿ\n\nè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š`pip install -r requirements.txt`ã€‚ç›¸å…³ä¾èµ–åº“åœ¨[https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt](https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt) å¯ä»¥æ‰¾åˆ°ã€‚\n<br><br>\n\n\n## Demo & æ¨ç†\n\n#### æ˜¯å¦æä¾›Demoï¼Ÿ\n\n`web_demo_mm.py`æä¾›äº†Web UIã€‚è¯·æŸ¥çœ‹READMEç›¸å…³å†…å®¹äº†è§£æ›´å¤šã€‚\n\n#### Qwen-VLæ”¯æŒæµå¼æ¨ç†å—ï¼Ÿ\n\nQwen-VLå½“å‰ä¸æ”¯æŒæµå¼æ¨ç†ã€‚\n\n#### æ¨¡å‹çš„è¾“å‡ºçœ‹èµ·æ¥ä¸è¾“å…¥æ— å…³/æ²¡æœ‰éµå¾ªæŒ‡ä»¤/çœ‹èµ·æ¥å‘†å‘†çš„\n\nè¯·æ£€æŸ¥æ˜¯å¦åŠ è½½çš„æ˜¯Qwen-VL-Chatæ¨¡å‹è¿›è¡Œæ¨ç†ï¼ŒQwen-VLæ¨¡å‹æ˜¯æœªç»alignçš„é¢„è®­ç»ƒåŸºæ¨¡å‹ï¼Œä¸æœŸæœ›å…·å¤‡å“åº”ç”¨æˆ·æŒ‡ä»¤çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨æ¨¡å‹æœ€æ–°ç‰ˆæœ¬å·²ç»å¯¹`chat`æ¥å£å†…è¿›è¡Œäº†æ£€æŸ¥ï¼Œé¿å…æ‚¨è¯¯å°†é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºSFT/Chatæ¨¡å‹ä½¿ç”¨ã€‚\n\n#### æ˜¯å¦æœ‰é‡åŒ–ç‰ˆæœ¬æ¨¡å‹\n\nç›®å‰Qwen-VLä¸æ”¯æŒé‡åŒ–ï¼Œåç»­æˆ‘ä»¬å°†æ”¯æŒé«˜æ•ˆçš„é‡åŒ–æ¨ç†å®ç°ã€‚\n\n#### å¤„ç†é•¿åºåˆ—æ—¶æ•ˆæœæœ‰é—®é¢˜\n\nè¯·ç¡®è®¤æ˜¯å¦å¼€å¯ntkã€‚è‹¥è¦å¯ç”¨è¿™äº›æŠ€å·§ï¼Œè¯·å°†`config.json`é‡Œçš„`use_dynamc_ntk`å’Œ`use_logn_attn`è®¾ç½®ä¸º`true`ã€‚æœ€æ–°ä»£ç é»˜è®¤ä¸º`true`ã€‚\n<br><br>\n\n\n## Tokenizer\n\n#### bos_id/eos_id/pad_idï¼Œè¿™äº›token idä¸å­˜åœ¨ï¼Œä¸ºä»€ä¹ˆï¼Ÿ\n\nåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨<|endoftext|>è¿™ä¸€tokenä½œä¸ºsample/documentä¹‹é—´çš„åˆ†éš”ç¬¦åŠpaddingä½ç½®å ä½ç¬¦ï¼Œä½ å¯ä»¥å°†bos_id, eos_id, pad_idå‡æŒ‡å‘tokenizer.eod_idã€‚è¯·é˜…è¯»æˆ‘ä»¬å…³äºtokenizerçš„æ–‡æ¡£ï¼Œäº†è§£å¦‚ä½•è®¾ç½®è¿™äº›idã€‚\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 6.7412109375,
          "content": "Tongyi Qianwen LICENSE AGREEMENT\n\nTongyi Qianwen Release Date: August 23, 2023\n\nBy clicking to agree or by using or distributing any portion or element of the Tongyi Qianwen Materials, you will be deemed to have recognized and accepted the content of this Agreement, which is effective immediately.\n\n1. Definitions\n    a. This Tongyi Qianwen LICENSE AGREEMENT (this \"Agreement\") shall mean the terms and conditions for use, reproduction, distribution and modification of the Materials as defined by this Agreement.\n    b. \"We\"(or \"Us\") shall mean Alibaba Cloud.\n    c. \"You\" (or \"Your\") shall mean a natural person or legal entity exercising the rights granted by this Agreement and/or using the Materials for any purpose and in any field of use.\n    d. \"Third Parties\" shall mean individuals or legal entities that are not under common control with Us or You.\n    e. \"Tongyi Qianwen\" shall mean the large language models (including Qwen-VL model and Qwen-VL-Chat model), and software and algorithms, consisting of trained model weights, parameters (including optimizer states), machine-learning model code, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Us.\n    f. \"Materials\" shall mean, collectively, Alibaba Cloud's proprietary Tongyi Qianwen and Documentation (and any portion thereof) made available under this Agreement.\n    g. \"Source\" form shall mean the preferred form for making modifications, including but not limited to model source code, documentation source, and configuration files.\n    h. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation,\n and conversions to other media types.\n\n2. Grant of Rights\nYou are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Alibaba Cloud's intellectual property or other rights owned by Us embodied in the Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Materials.\n\n3. Redistribution\nYou may reproduce and distribute copies of the Materials or derivative works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n    a. You shall give any other recipients of the Materials or derivative works a copy of this Agreement;\n    b. You shall cause any modified files to carry prominent notices stating that You changed the files;\n    c. You shall retain in all copies of the Materials that You distribute the following attribution notices within a \"Notice\" text file distributed as a part of such copies: \"Tongyi Qianwen is licensed under the Tongyi Qianwen LICENSE AGREEMENT, Copyright (c) Alibaba Cloud. All Rights Reserved.\"; and\n    d. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such derivative works as a whole, provided Your use, reproduction, and distribution of the work otherwise complies with the terms and conditions of this Agreement.\n\n4. Restrictions\nIf you are commercially using the Materials, and your product or service has more than 100 million monthly active users, You shall request a license from Us. You cannot exercise your rights under this Agreement without our express authorization.\n\n5. Rules of use\n    a. The Materials may be subject to export controls or restrictions in China, the United States or other countries or regions. You shall comply with applicable laws and regulations in your use of the Materials.\n    b. You can not use the Materials or any output therefrom to improve any other large language model (excluding Tongyi Qianwen or derivative works thereof).\n\n6. Intellectual Property\n    a. We retain ownership of all intellectual property rights in and to the Materials and derivatives made by or for Us. Conditioned upon compliance with the terms and conditions of this Agreement, with respect to any derivative works and modifications of the Materials that are made by you, you are and will be the owner of such derivative works and modifications.\n    b. No trademark license is granted to use the trade names, trademarks, service marks, or product names of Us, except as required to fulfill notice requirements under this Agreement or as required for reasonable and customary use in describing and redistributing the Materials.\n    c. If you commence a lawsuit or other proceedings (including a cross-claim or counterclaim in a lawsuit) against Us or any entity alleging that the Materials or any output therefrom, or any part of the foregoing, infringe any intellectual property or other right owned or licensable by you, then all licences granted to you under this Agreement shall terminate as of the date such lawsuit or other proceeding is commenced or brought.\n\n7. Disclaimer of Warranty and Limitation of Liability\n\n    a. We are not obligated to support, update, provide training for, or develop any further version of the Tongyi Qianwen Materials or to grant any license thereto.\n    b. THE MATERIALS ARE PROVIDED \"AS IS\" WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND INCLUDING WARRANTIES OF MERCHANTABILITY, NONINFRINGEMENT, OR FITNESS FOR A PARTICULAR PURPOSE. WE MAKE NO WARRANTY AND ASSUME NO RESPONSIBILITY FOR THE SAFETY OR STABILITY OF THE MATERIALS AND ANY OUTPUT THEREFROM.\n    c. IN NO EVENT SHALL WE BE LIABLE TO YOU FOR ANY DAMAGES, INCLUDING, BUT NOT LIMITED TO ANY DIRECT, OR INDIRECT, SPECIAL OR CONSEQUENTIAL DAMAGES ARISING FROM YOUR USE OR INABILITY TO USE THE MATERIALS OR ANY OUTPUT OF IT, NO MATTER HOW ITâ€™S CAUSED.\n    d. You will defend, indemnify and hold harmless Us from and against any claim by any third party arising out of or related to your use or distribution of the Materials.\n\n8. Survival and Termination.\n    a. The term of this Agreement shall commence upon your acceptance of this Agreement or access to the Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein.\n    b. We may terminate this Agreement if you breach any of the terms or conditions of this Agreement. Upon termination of this Agreement, you must delete and cease use of the Materials. Sections 7 and 9 shall survive the termination of this Agreement.\n\n9. Governing Law and Jurisdiction.\n    a. This Agreement and any dispute arising out of or relating to it will be governed by the laws of China, without regard to conflict of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement.\n    b. The People's Courts in Hangzhou City shall have exclusive jurisdiction over any dispute arising out of this Agreement."
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 2.6396484375,
          "content": "------------- LICENSE FOR NVIDIA Megatron-LM code  --------------\n\nCopyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n  * Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n  * Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in the\n    documentation and/or other materials provided with the distribution.\n  * Neither the name of NVIDIA CORPORATION nor the names of its\n    contributors may be used to endorse or promote products derived\n    from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\nEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\nOF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n------------- LICENSE FOR OpenAI tiktoken code  --------------\n\nMIT License\n\nCopyright (c) 2022 OpenAI, Shantanu Jain\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 44.0341796875,
          "content": "<p align=\"left\">\n        <a href=\"README_CN.md\">ä¸­æ–‡</a>&nbsp ï½œ &nbspEnglish&nbsp&nbsp ï½œ &nbsp<a href=\"README_JA.md\">æ—¥æœ¬èª</a>&nbspï½œ &nbsp<a href=\"README_KO.md\">í•œêµ­ì–´</a>&nbsp\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"assets/logo.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n  Qwen-VL \n  <a href=\"https://huggingface.co/Qwen/Qwen-VL\">ğŸ¤—</a>\n  <a href=\"https://modelscope.cn/models/qwen/Qwen-VL/summary\">ğŸ¤–</a>&nbsp ï½œ \n  Qwen-VL-Chat \n  <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat\">ğŸ¤—</a>\n  <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary\">ğŸ¤–</a>&nbsp \n  (Int4: \n  <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat-Int4\">ğŸ¤—</a> \n  <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat-Int4/summary\">ğŸ¤–</a>&nbsp) ï½œ\n  Qwen-VL-Plus \n  <a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Plus\">ğŸ¤—</a> \n  <a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary\">ğŸ¤–</a>&nbsp ï½œ \n  Qwen-VL-Max \n  <a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Max\">ğŸ¤—</a>\n  <a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Max/summary\">ğŸ¤–</a>&nbsp\n<br>\n  <a href=\"https://tongyi.aliyun.com/qianwen\">Web</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"http://ofasys-wlcb.oss-accelerate-overseas.aliyuncs.com/QwenVL/blog/app_qrcode.jpg\">APP</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start\">API</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://arxiv.org/abs/2308.12966\">Paper</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"TUTORIAL.md\">Tutorial</a>\n</p>\n<br><br>\n\n---\n## Qwen-VL-Plus & Qwen-VL-Max\n\nQwen-Vl-Plus and Qwen-VL-Max are the upgraded and latest versions of the Qwen-VL model family, currently supporting access for free through <a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Max\">ğŸ¤—</a>, <a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Max/summary\">ğŸ¤–</a>, [Web pages](https://qianwen.aliyun.com), [APP](http://ofasys-wlcb.oss-accelerate-overseas.aliyuncs.com/QwenVL/blog/app_qrcode.jpg) and [APIs](https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start/).\n\n| Model name | Model description |\n| --- | --- |\n| Qwen-VL-Plus | Qwen's **Enhanced Large Visual Language Model**. Significantly upgraded for detailed recognition capabilities and text recognition abilities, supporting ultra-high pixel resolutions up to millions of pixels and extreme aspect ratios for image input. It delivers **significant** performance across a broad range of visual tasks. |\n| Qwen-VL-Max | Qwen's **Most Capable Large Visual Language Model**. Compared to the enhanced version, further improvements have been made to visual reasoning and instruction-following capabilities, offering a higher level of visual perception and cognitive understanding. It delivers **optimal** performance on an even broader range of complex tasks. |\n\nThe key technical advancements in these versions include:\n- Substantially boost in image-related **reasoning capabilities**;\n- Considerable enhancement in recognizing, extracting, and analyzing **details of images**, especially for text-oriented tasks;\n- Support for **high-definition images** with resolutions above one million pixels and extreme aspect ratios;\n\nThese two models not only significantly surpass all previous best results from open-source LVLM models, but also perform on par with Gemini Ultra and GPT-4V in multiple text-image multimodal tasks.\n\nNotably, Qwen-VL-Max outperforms both GPT-4V from OpenAI and Gemini from Google in tasks on Chinese question answering and Chinese text comprehension. This breakthrough underscores the modelâ€™s advanced capabilities and its potential to set new standards in the field of multimodal AI research and application.\n\n<table>\n<thead>\n  <tr>\n    <th>Model</th>\n    <th>DocVQA</th>\n    <th>ChartQA</th>\n    <th>AI2D</th>\n    <th>TextVQA</th>\n    <th>MMMU</th>\n    <th>MathVista</th>\n    <th>MM-Bench-CN</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td>Other Best<br>Open-source LVLM</td>\n    <td>81.6%<br><sup><sup>(CogAgent)</sup></sup></td>\n    <td>68.4%<br><sup><sup>(CogAgent)</sup></sup></td>\n    <td>73.7%<br><sup><sup>(Fuyu-Medium)</sup></sup></td>\n    <td>76.1%<br><sup><sup>(CogAgent)</sup></sup></td>\n    <td>45.9%<br><sup><sup>(Yi-VL-34B)</sup></sup></td>\n    <td>36.7%<br><sup><sup>(SPHINX-V2)</sup></sup></td>\n    <td>72.4%<br><sup><sup>(InternLM-XComposer-VL)</sup></sup></td>\n  </tr>\n  <tr>\n    <td>Gemini Pro</td>\n    <td>88.1%</td>\n    <td>74.1%</td>\n    <td>73.9%</td>\n    <td>74.6%</td>\n    <td>47.9%</td>\n    <td>45.2%</td>\n    <td>74.3%</td>\n  </tr>\n  <tr>\n    <td>Gemini Ultra</td>\n    <td>90.9%</td>\n    <td>80.8% <sup>1</sup></td>\n    <td>79.5% <sup>1</sup></td>\n    <td>82.3% <sup>1</sup></td>\n    <td>59.4% <sup>1</sup></td>\n    <td>53.0% <sup>1</sup></td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>GPT-4V</td>\n    <td>88.4%</td>\n    <td>78.5%</td>\n    <td>78.2%</td>\n    <td>78.0%</td>\n    <td>56.8%</td>\n    <td>49.9%</td>\n    <td>73.9%</td>\n  </tr>\n  <tr>\n    <td><b>Qwen-VL-Plus</b></td>\n    <td>91.4%</td>\n    <td>78.1%</td>\n    <td>75.9%</td>\n    <td>78.9%</td>\n    <td>45.2%</td>\n    <td>43.3%</td>\n    <td>68.0%</td>\n  </tr>\n  <tr>\n    <td><b>Qwen-VL-Max</b></td>\n    <td>93.1% <sup>1</sup></td>\n    <td>79.8% <sup>2</sup></td>\n    <td>79.3% <sup>2</sup></td>\n    <td>79.5% <sup>2</sup></td>\n    <td>51.4% <sup>3</sup></td>\n    <td>51.0% <sup>2</sup></td>\n    <td>75.1% <sup>1</sup></td>\n  </tr>\n</tbody>\n</table>\n\nAll numbers are obtained without any use of external OCR tools ('pixel only').\n\n---\n\n## News and Updates\n* ```2024.01.18``` ğŸ’¥ğŸ’¥ğŸ’¥ We introduce Qwen-VL-Max, our most capable model that significantly surpasses all previous open-source LVLM models, and it performs on par with Gemini Ultra and GPT-4V in multiple text-image multimodal tasks. You can enjoy the new model by directly visiting our [web pages](https://qianwen.aliyun.com), <a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Max\">ğŸ¤—</a> and <a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Max/summary\">ğŸ¤–</a>.\n* ```2023.11.28``` ğŸ†ğŸ†ğŸ† Qwen-VL-Plus achieved the best performance in [DOCVQA](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1) by using a single model, surpassing GPT4V and PALI-X, without using model ensemble or OCR-pipeline. Meanwhile, it is also a general model that can help you analyze and understand various tasks by directly inputting images.\n* ```2023.9.25``` ğŸš€ğŸš€ğŸš€ We update Qwen-VL-Chat with more robust Chinese instruction-following ability, improved understanding of web pages and table images, and better dialogue performance (Touchstone: CN: 401.2->481.7, EN: 645.2->711.6).\n* ```2023.9.12``` ğŸ˜ƒğŸ˜ƒğŸ˜ƒ We now support finetuning on the Qwen-VL models, including full-parameter finetuning, LoRA and Q-LoRA.\n* ```2023.9.8``` ğŸ‘ğŸ‘ğŸ‘ Thanks to [camenduru](https://github.com/camenduru) for contributing the wonderful [Colab](https://github.com/camenduru/Qwen-VL-Chat-colab). Everyone can use it as a local or online Qwen-VL-Chat-Int4 Demo tutorial on one 12G GPU.\n* ```2023.9.5``` ğŸ‘ğŸ‘ğŸ‘ Qwen-VL-Chat achieves SOTAs on [MME Benchmark](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation), a comprehensive evaluation benchmark for multimodal large language models. It measures both perception and cognition abilities on a total of 14 subtasks.\n* ```2023.9.4``` â­â­â­ Qwen-VL series achieve SOTAs on [Seed-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard), a multimodal benchmark of 19K multiple-choice questions with accurate human annotations for evaluating Multimodal LLMs including both image and video understanding.\n* ```2023.9.1``` ğŸ”¥ğŸ”¥ğŸ”¥ We release the [TouchStone](https://github.com/OFA-Sys/TouchStone) Evaluation, which is a comprehensive assessment of multimodal language models, encompassing not only basic recognition and comprehension but also extending to literary creation. By using strong LLMs as judges and converting multimodal information into text.\n* ```2023.8.31``` ğŸŒŸğŸŒŸğŸŒŸ We release the Int4 quantized model for Qwen-VL-Chat, **Qwen-VL-Chat-Int4**, which requires low memory costs but achieves improved inference speed. Besides, there is no significant performance degradation on the benchmark evaluation.\n* ```2023.8.22``` ğŸ‰ğŸ‰ğŸ‰ We release both **Qwen-VL** and **Qwen-VL-Chat** on ModelScope and Hugging Face. We also provide a [paper](https://arxiv.org/abs/2308.12966) for more details about the model, including training details and model performance.\n\n---\n## Qwen-VL\n\n**Qwen-VL** (Qwen Large Vision Language Model) is the multimodal version of the large model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-VL accepts image, text, and bounding box as inputs, outputs text, and bounding box. The features of Qwen-VL include:\n\n- **Strong performance**: It significantly surpasses existing open-sourced Large Vision Language Models (LVLM) under a similar model scale on multiple English evaluation benchmarks (including Zero-shot Captioning, VQA, DocVQA, and Grounding).\n- **Multi-lingual LVLM supporting text recognition**: Qwen-VL naturally supports English, Chinese, and multi-lingual conversation, and it promotes end-to-end recognition of Chinese and English bi-lingual text in images.\n- **Multi-image interleaved conversations**: This feature allows for the input and comparison of multiple images, as well as the ability to specify questions related to the images and engage in multi-image storytelling.\n- **First generalist model supporting grounding in Chinese**: Detecting bounding boxes through open-domain language expression in both Chinese and English.\n- **Fine-grained recognition and understanding**: Compared to the 224\\*224 resolution currently used by other open-sourced LVLM, the 448\\*448 resolution promotes fine-grained text recognition, document QA, and bounding box annotation.\n\n<br>\n<p align=\"center\">\n    <img src=\"assets/demo_vl.gif\" width=\"400\"/>\n<p>\n<br>\n\nWe release two models of the Qwen-VL series:\n\n- Qwen-VL: The pre-trained LVLM model uses Qwen-7B as the initialization of the LLM, and [Openclip ViT-bigG](https://github.com/mlfoundations/open_clip) as the initialization of the visual encoder. And connects them with a randomly initialized cross-attention layer.\n- Qwen-VL-Chat: A multimodal LLM-based AI assistant, which is trained with alignment techniques. Qwen-VL-Chat supports more flexible interaction, such as multiple image inputs, multi-round question answering, and creative capabilities.\n  <br>\n\n## Evaluation\n\nWe evaluated the model's abilities from three perspectives:\n\n1. **Standard Benchmarks**: We evaluate the model's basic task capabilities on four major categories of multimodal tasks:\n   \n   - Zero-shot Captioning: Evaluate model's zero-shot image captioning ability on unseen datasets;\n   - General VQA: Evaluate the general question-answering ability of pictures, such as the judgment, color, number, category, etc;\n   - Text-based VQA: Evaluate the model's ability to recognize text in pictures, such as document QA, chart QA, etc;\n   - Referring Expression Comprehension: Evaluate the ability to localize a target object in an image described by a referring expression.\n2. **TouchStone**: To evaluate the overall text-image dialogue capability and alignment level with humans, we have constructed a benchmark called [TouchStone](https://github.com/OFA-Sys/TouchStone), which is based on scoring with GPT4 to evaluate the LVLM model.\n   \n   - The TouchStone benchmark covers a total of 300+ images, 800+ questions, and 27 categories. Such as attribute-based Q&A, celebrity recognition, writing poetry, summarizing multiple images, product comparison, math problem solving, etc;\n   - In order to break the current limitation of GPT4 in terms of direct image input, TouchStone provides fine-grained image annotations by human labeling. These detailed annotations, along with the questions and the model's output, are then presented to GPT4 for scoring.\n   - The benchmark includes both English and Chinese versions.\n  \n3. **Other Multimodal Benchmarks**: We also evaluated our model's capabilities in other multimodal benchmarks:\n\n   - [MME Benchmark](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation), a comprehensive evaluation benchmark for multimodal large language models. Qwen-VL-Chat achieves SOTAs on both perception and cognition tracks.\n   - [Seed-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard), a multimodal benchmark of 19K multiple-choice questions with accurate human annotations for evaluating Multimodal LLMs. Qwen series achieves SOTAs on this benchmark.\n\nThe results of the evaluation are as follows:\n\nQwen-VL outperforms current SOTA generalist models on multiple VL tasks and has a more comprehensive coverage in terms of capability range.\n\n<p align=\"center\">\n    <img src=\"assets/radar.png\" width=\"600\"/>\n<p>\n\n### Zero-shot Captioning & General VQA\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"2\">Zero-shot Captioning</th>\n    <th colspan=\"5\">General VQA</th>\n  </tr>\n  <tr>\n    <th>NoCaps</th>\n    <th>Flickr30K</th>\n    <th>VQAv2<sup>dev</sup></th>\n    <th>OK-VQA</th>\n    <th>GQA</th>\n    <th>SciQA-Img<br>(0-shot)</th>\n    <th>VizWiz<br>(0-shot)</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"10\">Generalist<br>Models</td>\n    <td>Flamingo-9B</td>\n    <td>-</td>\n    <td>61.5</td>\n    <td>51.8</td>\n    <td>44.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>28.8</td>\n  </tr>\n  <tr>\n    <td>Flamingo-80B</td>\n    <td>-</td>\n    <td>67.2</td>\n    <td>56.3</td>\n    <td>50.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>31.6</td>\n  </tr>\n  <tr>\n    <td>Unified-IO-XL</td>\n    <td>100.0</td>\n    <td>-</td>\n    <td>77.9</td>\n    <td>54.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Kosmos-1</td>\n    <td>-</td>\n    <td>67.1</td>\n    <td>51.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>29.2</td>\n  </tr>\n  <tr>\n    <td>Kosmos-2</td>\n    <td>-</td>\n    <td>80.5</td>\n    <td>51.1</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>103.9</td>\n    <td>71.6</td>\n    <td>65.0</td>\n    <td>45.9</td>\n    <td>32.3</td>\n    <td>61.0</td>\n    <td>19.6</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td><strong>121.9</strong></td>\n    <td>82.8</td>\n    <td>-</td>\n    <td>-</td>\n    <td>49.5</td>\n    <td>63.1</td>\n    <td>33.4</td>\n  </tr>\n  <tr>\n    <td>Shikra (Vicuna-13B)</td>\n    <td>-</td>\n    <td>73.9</td>\n    <td>77.36</td>\n    <td>47.16</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td><strong>Qwen-VL (Qwen-7B)</strong></td>\n    <td>121.4</td>\n    <td><b>85.8</b></td>\n    <td><b>78.8</b></td>\n    <td><b>58.6</b></td>\n    <td><b>59.3</b></td>\n    <td>67.1</td>\n    <td>35.2</td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>63.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>39.1</td>\n  </tr> -->\n  <tr>\n    <td>Qwen-VL-Chat</td>\n    <td>120.2</td>\n    <td>81.0</td>\n    <td>78.2</td>\n    <td>56.6</td>\n    <td>57.5</td>\n    <td><b>68.2</b></td>\n    <td><b>38.9</b></td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL-Chat (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>60.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>44.45</td>\n  </tr> -->\n  <tr>\n    <td>Previous SOTA<br>(Per Task Fine-tuning)</td>\n    <td>-</td>\n    <td>127.0<br>(PALI-17B)</td>\n    <td>84.5<br>(InstructBLIP<br>-FlanT5-XL)</td>\n    <td>86.1<br>(PALI-X<br>-55B)</td>\n    <td>66.1<br>(PALI-X<br>-55B)</td>\n    <td>72.1<br>(CFR)</td>\n    <td>92.53<br>(LLaVa+<br>GPT-4)</td>\n    <td>70.9<br>(PALI-X<br>-55B)</td>\n  </tr>\n</tbody>\n</table>\n\n- For zero-shot image captioning, Qwen-VL achieves the **SOTA** on Flickr30K and competitive results on Nocaps with InstructBlip.\n- For general VQA, Qwen-VL achieves the **SOTA** under the same generalist LVLM scale settings.\n\n### Text-oriented VQA (Focused on text understanding capabilities in images)\n\n<table>\n<thead>\n  <tr>\n    <th>Model type</th>\n    <th>Model</th>\n    <th>TextVQA</th>\n    <th>DocVQA</th>\n    <th>ChartQA</th>\n    <th>AI2D</th>\n    <th>OCR-VQA</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"5\">Generalist Models</td>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>42.4</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td>50.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>mPLUG-DocOwl (LLaMA-7B)</td>\n    <td>52.6</td>\n    <td>62.2</td>\n    <td>57.4</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Pix2Struct-Large (1.3B)</td>\n    <td>-</td>\n    <td><b>76.6</b></td>\n    <td>58.6</td>\n    <td>42.1</td>\n    <td>71.3</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL (Qwen-7B)</td>\n    <td><b>63.8</b></td>\n    <td>65.1</td>\n    <td><b>65.7</b></td>\n    <td><b>62.3</b></td>\n    <td><b>75.7</b></td>\n  </tr>\n  <tr>\n    <td>Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>PALI-X-55B (Single-task FT)<br>(Without OCR Pipeline)</td>\n    <td>71.44</td>\n    <td>80.0</td>\n    <td>70.0</td>\n    <td>81.2</td>\n    <td>75.0</td>\n  </tr>\n</tbody>\n</table>\n\n- In text-related recognition/QA evaluation, Qwen-VL achieves the SOTA under the generalist LVLM scale settings.\n- Resolution is important for several above evaluations. While most open-sourced LVLM models with 224 resolution are incapable of these evaluations or can only solve these by cutting images, Qwen-VL scales the resolution to 448 so that it can be evaluated end-to-end. Qwen-VL even outperforms Pix2Struct-Large models of 1024 resolution on some tasks.\n\n### Referring Expression Comprehension\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"3\">RefCOCO</th>\n    <th colspan=\"3\">RefCOCO+</th>\n    <th colspan=\"2\">RefCOCOg</th>\n    <th>GRIT</th>\n  </tr>\n  <tr>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val-u</th>\n    <th>test-u</th>\n    <th>refexp</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"8\">Generalist Models</td>\n    <td>GPV-2</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>51.50</td>\n  </tr>\n  <tr>\n    <td>OFA-L*</td>\n    <td>79.96</td>\n    <td>83.67</td>\n    <td>76.39</td>\n    <td>68.29</td>\n    <td>76.00</td>\n    <td>61.75</td>\n    <td>67.57</td>\n    <td>67.58</td>\n    <td>61.70</td>\n  </tr>\n  <tr>\n    <td>Unified-IO</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td><b>78.61</b></td>\n  </tr>\n  <tr>\n    <td>VisionLLM-H</td>\n    <td></td>\n    <td>86.70</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Shikra-7B</td>\n    <td>87.01</td>\n    <td>90.61</td>\n    <td>80.24 </td>\n    <td>81.60</td>\n    <td>87.36</td>\n    <td>72.12</td>\n    <td>82.27</td>\n    <td>82.19</td>\n    <td>69.34</td>\n  </tr>\n  <tr>\n    <td>Shikra-13B</td>\n    <td>87.83 </td>\n    <td>91.11</td>\n    <td>81.81</td>\n    <td>82.89</td>\n    <td>87.79</td>\n    <td>74.41</td>\n    <td>82.64</td>\n    <td>83.16</td>\n    <td>69.03</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B</td>\n    <td><b>89.36</b></td>\n    <td>92.26</td>\n    <td><b>85.34</b></td>\n    <td><b>83.12</b></td>\n    <td>88.25</td>\n    <td><b>77.21</b></td>\n    <td>85.58</td>\n    <td>85.48</td>\n    <td>78.22</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B-Chat</td>\n    <td>88.55</td>\n    <td><b>92.27</b></td>\n    <td>84.51</td>\n    <td>82.82</td>\n    <td><b>88.59</b></td>\n    <td>76.79</td>\n    <td><b>85.96</b></td>\n    <td><b>86.32</b></td>\n    <td>-</td>\n  <tr>\n    <td rowspan=\"3\">Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>G-DINO-L</td>\n    <td>90.56</td>\n    <td>93.19</td>\n    <td>88.24</td>\n    <td>82.75</td>\n    <td>88.95</td>\n    <td>75.92</td>\n    <td>86.13</td>\n    <td>87.02</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>UNINEXT-H</td>\n    <td>92.64 </td>\n    <td>94.33</td>\n    <td>91.46</td>\n    <td>85.24</td>\n    <td>89.63</td>\n    <td>79.79</td>\n    <td>88.73</td>\n    <td>89.37</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>ONE-PEACE</td>\n    <td>92.58 </td>\n    <td>94.18</td>\n    <td>89.26</td>\n    <td>88.77</td>\n    <td>92.21</td>\n    <td>83.23</td>\n    <td>89.22</td>\n    <td>89.27</td>\n    <td>-</td>\n  </tr>\n</tbody>\n</table>\n\n- Qwen-VL achieves the **SOTA** in all above referring expression comprehension benchmarks.\n- Qwen-VL has not been trained on any Chinese grounding data, but it can still generalize to the Chinese Grounding tasks in a zero-shot way by training Chinese Caption data and English Grounding data.\n\nWe provide all of the above evaluation scripts for reproducing our experimental results. Please read [eval_mm/EVALUATION.md](eval_mm/EVALUATION.md) for more information.\n\n### Chat evaluation\n\nTouchStone is a benchmark based on scoring with GPT4 to evaluate the abilities of the LVLM model on text-image dialogue and alignment levels with humans. It covers a total of 300+ images, 800+ questions, and 27 categories, such as attribute-based Q&A, celebrity recognition, writing poetry, summarizing multiple images, product comparison, math problem solving, etc. Please read [touchstone/README.md](touchstone/README.md) for more information.\n\n#### English evaluation\n\n| Model            | Score |\n| ---------------- | ----- |\n| PandaGPT         | 488.5 |\n| MiniGPT4         | 531.7 |\n| InstructBLIP     | 552.4 |\n| LLaMA-AdapterV2  | 590.1 |\n| LLaVA            | 602.7 |\n| mPLUG-Owl        | 605.4 |\n| Qwen-VL-Chat     | 645.2 |\n| Qwen-VL-Chat-1.1 | 711.6 |\n\n#### Chinese evaluation\n\n| Model            | Score |\n| ---------------- | ----- |\n| VisualGLM        | 247.1 |\n| Qwen-VL-Chat     | 401.2 |\n| Qwen-VL-Chat-1.1 | 481.7 |\n\nQwen-VL-Chat has achieved the best results in both Chinese and English alignment evaluation.\n\n### Other Benchmarks\n\n#### MME Benchmark\n\n[MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) is a comprehensive evaluation benchmark for multimodal large language models. It measures both perception and cognition abilities on a total of 14 subtasks, including existence, count, position, color, poster, celebrity, scene, landmark, artwork, OCR, commonsense reasoning, numerical calculation, text translation, and code reasoning.\n\nQwen-VL-Chat achieves SOTAs on both perception and cognition evaluation. See more details on [HERE](eval_mm/mme/EVAL_MME.md).\n\n<p align=\"center\">\n    <img src=\"eval_mm/mme/perception.jpg\" width=\"600\"/>\n<p>\n<p align=\"center\">\n    <img src=\"eval_mm/mme/cognition.jpg\" width=\"600\"/>\n<p>\n\n#### SEED-Bench\n\n[SEED-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard) is a multimodal benchmark of 19K multiple-choice questions with accurate human annotations for evaluating Multimodal LLMs, covering 12 evaluation dimensions including both **image** and **video** understanding. See more details on [HERE](eval_mm/seed_bench/EVAL_SEED.md).\n\nQwen-VL and Qwen-VL-Chat achieve SOTAs on this benchmark.\n\n<p align=\"center\">\n    <img src=\"eval_mm/seed_bench/leaderboard.jpg\"/>\n<p>\n\n## Requirements\n\n* python 3.8 and above\n* pytorch 1.12 and above, 2.0 and above are recommended\n* CUDA 11.4 and above are recommended (this is for GPU users)\n  <br>\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen-VL and Qwen-VL-Chat with ğŸ¤– ModelScope and ğŸ¤— Transformers.\n\nBefore running the code, make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries.\n\n```bash\npip install -r requirements.txt\n```\n\nNow you can start with ModelScope or Transformers. More usage aboue vision encoder, please refer to the [tutorial](TUTORIAL.md).\n\n#### ğŸ¤— Transformers\n\nTo use Qwen-VL-Chat for the inference, all you need to do is to input a few lines of codes as demonstrated below. However, **please make sure that you are using the latest code.**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use cuda device\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# 1st dialogue turn\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url\n    {'text': 'è¿™æ˜¯ä»€ä¹ˆ?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n# å›¾ä¸­æ˜¯ä¸€åå¥³å­åœ¨æ²™æ»©ä¸Šå’Œç‹—ç©è€ï¼Œæ—è¾¹æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œå®ƒä»¬å¤„äºæ²™æ»©ä¸Šã€‚\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, 'æ¡†å‡ºå›¾ä¸­å‡»æŒçš„ä½ç½®', history=history)\nprint(response)\n# <ref>å‡»æŒ</ref><box>(536,509),(588,602)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('1.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n\n<details>\n  <summary>Running Qwen-VL</summary>\n\nRunning Qwen-VL pretrained base model is also simple.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use cuda device\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation (No need to do this if you are using transformers>4.32.0)\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url\n    {'text': 'Generate the caption in English with grounding:'},\n])\ninputs = tokenizer(query, return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nresponse = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)\nprint(response)\n# <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>Generate the caption in English with grounding:<ref> Woman</ref><box>(451,379),(731,806)</box> and<ref> her dog</ref><box>(219,424),(576,896)</box> playing on the beach<|endoftext|>\nimage = tokenizer.draw_bbox_on_latest_picture(response)\nif image:\n  image.save('2.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_spotting_caption.jpg\" width=\"500\"/>\n<p>\n\n</details>\n\n\nIn the event of a network issue while attempting to download model checkpoints and codes from HuggingFace, an alternative approach is to initially fetch the checkpoint from ModelScope and then load it from the local directory as outlined below:\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-VL')\nmodel_dir = snapshot_download('qwen/Qwen-VL-Chat')\n\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"cuda\",\n    trust_remote_code=True\n).eval()\n```\n\n#### ğŸ¤– ModelScope\n\nModelScope is an opensource platform for Model-as-a-Service (MaaS), which provides flexible and cost-effective model service to AI developers. Similarly, you can run the models with ModelScope as shown below:\n\n```python\nfrom modelscope import (\n    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n)\nimport torch\nmodel_id = 'qwen/Qwen-VL-Chat'\nrevision = 'v1.0.0'\n\nmodel_dir = snapshot_download(model_id, revision=revision)\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nif not hasattr(tokenizer, 'model_dir'):\n    tokenizer.model_dir = model_dir\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation (No need to do this if you are using transformers>=4.32.0)\n# model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)\n\n# 1st dialogue turn\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)\nprint(response)\n# å›¾ä¸­æ˜¯ä¸€åå¹´è½»å¥³å­åœ¨æ²™æ»©ä¸Šå’Œå¥¹çš„ç‹—ç©è€ï¼Œç‹—çš„å“ç§æ˜¯æ‹‰å¸ƒæ‹‰å¤šã€‚å¥¹ä»¬ååœ¨æ²™æ»©ä¸Šï¼Œç‹—çš„å‰è…¿æŠ¬èµ·æ¥ï¼Œä¸äººäº’åŠ¨ã€‚\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, 'è¾“å‡ºå‡»æŒçš„æ£€æµ‹æ¡†', history=history)\nprint(response)\n# <ref>\"å‡»æŒ\"</ref><box>(211,412),(577,891)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('output_chat.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n<br>\n\n## Quantization\n\n### Usage\n\nWe provide a new solution based on [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), and release an Int4 quantized model for Qwen-VL-Chat, Qwen-VL-Chat-Int4 [Click here](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4), which achieves nearly lossless model effects but improved performance on both memory costs and inference speed.\n\nHere we demonstrate how to use our provided quantized models for inference. Before you start, make sure you meet the requirements (e.g., torch 2.0 and above, transformers 4.32.0 and above, etc.) and install the required packages:\n\n```bash\npip install optimum\ngit clone https://github.com/JustinLin610/AutoGPTQ.git & cd AutoGPTQ\npip install -v .\n```\n\nIf you meet problems installing `auto-gptq`, we advise you to check out the official [repo](https://github.com/PanQiWei/AutoGPTQ) to find a wheel.\n\nThen you can load the quantized model easily and run inference as same as usual:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-VL-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)\nprint(response)\n```\n\n### Performance\n\nWe illustrate the model performance of both BF16 and Int4 models on the benchmark **[TouchStone](https://github.com/OFA-Sys/TouchStone)**, and we find that the quantized model does not suffer from significant performance degradation. Results are shown below:\n\n| Quantization | ZH         | EN            |\n| ------------ | :--------: | :-----------: | \n| BF16         | 401.2      |    645.2      |\n| Int4         | 386.6      |    651.4      |\n\n### Inference Speed\n\nWe measured the average inference speed (tokens/s) of generating 1792 (2048-258) and 7934 (8192-258) tokens with the context of an image (which takes 258 tokens) under BF16 precision and Int4 quantization, respectively.\n\n| Quantization | Speed (2048 tokens) | Speed (8192 tokens) |\n| ------------ | :-----------------: | :-----------------: |\n| BF16         |        28.87        |        24.32        |\n| Int4         |        37.79        |        34.34        |\n\nThe profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.4.\n\n### GPU Memory Usage\n\nWe also profile the peak GPU memory usage for encoding 1792 (2048-258) tokens (including an image) as context (and generating single token) and generating 7934 (8192-258) tokens (with an image as context) under BF16 or Int4 quantization level, respectively. The results are shown below.\n\n| Quantization | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |\n| ------------ | :---------------------------------: | :-----------------------------------: |\n| BF16         |               22.60GB               |                28.01GB                |\n| Int4         |               11.82GB               |                17.23GB                |\n\nThe above speed and memory profiling are conducted using [this script](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile_mm.py).\n<br>\n\n## Finetuning\n\nNow we provide the official training script, `finetune.py`, for users to finetune the pretrained model for downstream applications in a simple fashion. Additionally, we provide shell scripts to launch finetuning with no worries. This script supports the training with DeepSpeed and FSDP. The shell scripts that we provide use DeepSpeed, and thus we advise you to install DeepSpeed before you start:\n\n```bash\npip install deepspeed\n```\n\n### Data preparation\nTo prepare your training data, you need to put all the samples into a list and save it to a json file. Each sample is a dictionary consisting of an id and a list for conversation. Below is a simple example list with 1 sample:\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"ä½ å¥½\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"æˆ‘æ˜¯Qwen-VL,ä¸€ä¸ªæ”¯æŒè§†è§‰è¾“å…¥çš„å¤§æ¨¡å‹ã€‚\"\n      }\n    ]\n  },\n  {\n    \"id\": \"identity_1\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>\\nå›¾ä¸­çš„ç‹—æ˜¯ä»€ä¹ˆå“ç§ï¼Ÿ\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"å›¾ä¸­æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚\"\n      },\n      {\n        \"from\": \"user\",\n        \"value\": \"æ¡†å‡ºå›¾ä¸­çš„æ ¼å­è¡¬è¡«\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"<ref>æ ¼å­è¡¬è¡«</ref><box>(588,499),(725,789)</box>\"\n      }\n    ]\n  },\n  { \n    \"id\": \"identity_2\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>assets/mm_tutorial/Chongqing.jpeg</img>\\nPicture 2: <img>assets/mm_tutorial/Beijing.jpeg</img>\\nå›¾ä¸­éƒ½æ˜¯å“ª\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"ç¬¬ä¸€å¼ å›¾ç‰‡æ˜¯é‡åº†çš„åŸå¸‚å¤©é™…çº¿ï¼Œç¬¬äºŒå¼ å›¾ç‰‡æ˜¯åŒ—äº¬çš„å¤©é™…çº¿ã€‚\"\n      }\n    ]\n  }\n]\n```\nFor the VL tasks, there are special tokens that are used, including `<img> </img> <ref> </ref> <box> </box>`.\n\nThe picture is represented as `Picture id: <img>img_path</img>\\n{your prompt}`, where `id` indicates the position of the image in the conversation, starting from 1. The \"img_path\" can be a local file path or a web link. \n\nThe coordinate box is expressed as `<box>(x1,y1),(x2,y2)</box>`Â·, where `(x1, y1)` and `(x2, y2)` are normalized values in the range `[0, 1000)`. Its corresponding text description can be identified by `<ref>text_caption</ref>`. \n\n\nAfter data preparation, you can use the provided shell scripts to run finetuning. Remember to specify the path to the data file, `$DATA`.\n\nThe finetuning scripts allow you to perform:\n- Full-parameter finetuning\n- LoRA\n- Q-LoRA\n\n### Full-parameter finetuning\nFull-parameter parameter finetuning requires updating all parameters of LLM in the whole training process. In our experiments, frozening the parameters of ViT during the fine-tuning phase achieves better performance. To launch your training, run the following script:\n\n```bash\nsh finetune/finetune_ds.sh\n```\n\nRemember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. If you want to make changes, just remove the argument `--deepspeed` or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use `--bf16 True` or `--fp16 True`. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.\n\n### LoRA\nSimilarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed `peft`. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load.\n\n```bash\n# Single GPU training\nsh finetune/finetune_lora_single_gpu.sh\n# Distributed training\nsh finetune/finetune_lora_ds.sh\n```\n\nIn comparison with full-parameter finetuning, LoRA ([paper](https://arxiv.org/abs/2106.09685)) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs. \n\nNote that if you use LoRA to finetune the base language model, e.g., Qwen-VL, instead of chat models, e.g., Qwen-VL-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting `modules_to_save` inside the code. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information.\n\n### Q-LoRA\nHowever, if you still suffer from insufficient memory, you can consider Q-LoRA ([paper](https://arxiv.org/abs/2305.14314)), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs. To run Q-LoRA, directly run the following script:\n\n```bash\n# Single GPU training\nsh finetune/finetune_qlora_single_gpu.sh\n# Distributed training\nsh finetune/finetune_qlora_ds.sh\n```\n\nFor Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-VL-Chat-Int4. \nYou **SHOULD NOT** use the bf16 models. Different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA. Besides, for Q-LoRA, the troubles with the special tokens in LoRA still exist. However, as we only provide the Int4 models for chat models, which means the language model has learned the special tokens of ChatML format, you have no worry about the layers. Note that the layers of the Int4 model should not be trainable, and thus if you introduce special tokens in your training, Q-LoRA might not work.\n\n\n\nDifferent from full-parameter finetuning, the training of both LoRA and Q-LoRA only saves the adapter parameters. You can load the finetuned model for inference as shown below:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\nIf you want to merge the adapters and save the finetuned model as a standalone model (you can only do this with LoRA, and you CANNOT merge the parameters from Q-LoRA), you can run the following codes:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\nNote: For multi-GPU training, you need to specify the proper hyperparameters for distributed training based on your machine. Besides, we advise you to specify your maximum sequence length with the argument --model_max_length, based on your consideration of data, memory footprint, and training speed.\n\n\n### Profiling of Memory and Speed\nWe profile the GPU memory and training speed of both LoRA (Base) refers to training the embedding and output layer, while LoRA (Chat) has no trainable embedding and output layer) and Q-LoRA in the setup of single-GPU training. In this test, we experiment on a single A100-SXM4-80G GPU, and we use CUDA 11.8 and Pytorch 2.0. We uniformly use a batch size of 1 and gradient accumulation of 8. Each sample contains an image. We profile the memory (GB) and speed (s/iter) of inputs of different lengths, namely 384, 512, 1024, and 2048. The statistics are listed below:\n\n\n<table>\n    <tr>\n <th rowspan=\"2\">Method</th><th colspan=\"4\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">384</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th>\n    </tr>\n    <tr>\n      <td>LoRA (Base)</td><td align=\"center\">37.1G / 2.3s/it</td><td align=\"center\">37.3G / 2.4s/it</td><td align=\"center\">38.7G / 3.6s/it</td><td align=\"center\">38.7G / 6.1s/it</td>\n    </tr>\n    <tr>\n      <td>LoRA (Chat)</td><td align=\"center\">23.3G / 2.2s/it</td><td align=\"center\">23.6G / 2.3s/it</td><td align=\"center\">25.1G / 3.5s/it</td><td align=\"center\">27.3G / 5.9s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">17.0G / 4.2s/it</td><td align=\"center\">17.2G / 4.5s/it</td><td align=\"center\">18.2G / 5.5s/it</td><td align=\"center\">19.3G / 7.9s/it</td>\n    </tr>\n\n</table>\n\n<br>\n\n## Demo\n\n### Web UI\n\nWe provide code for users to build a web UI demo. Before you start, make sure you install the following packages:\n\n```\npip install -r requirements_web_demo.txt\n```\n\nThen run the command below and click on the generated link:\n\n```\npython web_demo_mm.py\n```\n\n<br>\n\n## FAQ\n\nIf you meet problems, please refer to [FAQ](FAQ.md) and the issues first to search a solution before you launch a new issue.\n<br>\n\n## License Agreement\n\nResearchers and developers are free to use the codes and model weights of both Qwen-VL and Qwen-VL-Chat. We also allow their commercial use. Check our license at [LICENSE](LICENSE) for more details.\n<br>\n\n## Citation\n\nIf you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)\n\n```BibTeX\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n\n<br>\n\n## Contact Us\n\nIf you are interested to leave a message to either our research team or product team, feel free to send an email to qianwen_opensource@alibabacloud.com.\n\n"
        },
        {
          "name": "README_CN.md",
          "type": "blob",
          "size": 41.7705078125,
          "content": "<p align=\"left\">\n        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href=\"README.md\">English</a>&nbsp&nbsp ï½œ &nbsp<a href=\"README_JA.md\">æ—¥æœ¬èª</a>&nbspï½œ &nbsp<a href=\"README_KO.md\">í•œêµ­ì–´</a>&nbsp\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"assets/logo.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n  Qwen-VL \n  <a href=\"https://huggingface.co/Qwen/Qwen-VL\">ğŸ¤—</a>\n  <a href=\"https://modelscope.cn/models/qwen/Qwen-VL/summary\">ğŸ¤–</a>&nbsp ï½œ \n  Qwen-VL-Chat \n  <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat\">ğŸ¤—</a>\n  <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary\">ğŸ¤–</a>&nbsp \n  (Int4: \n  <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat-Int4\">ğŸ¤—</a> \n  <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat-Int4/summary\">ğŸ¤–</a>&nbsp) ï½œ\n  Qwen-VL-Plus \n  <a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Plus\">ğŸ¤—</a> \n  <a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary\">ğŸ¤–</a>&nbsp ï½œ \n  Qwen-VL-Max \n  <a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Max\">ğŸ¤—</a>\n  <a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Max/summary\">ğŸ¤–</a>&nbsp\n<br>\n  <a href=\"https://tongyi.aliyun.com/qianwen\">Web</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"http://ofasys-wlcb.oss-accelerate-overseas.aliyuncs.com/QwenVL/blog/app_qrcode.jpg\">APP</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start\">API</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://arxiv.org/abs/2308.12966\">Paper</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"TUTORIAL.md\">Tutorial</a>\n</p>\n<br><br>\n\n---\n## Qwen-VL-Plus & Qwen-VL-Max\n\nQwen-VL ç³»åˆ—å†æ¬¡è¿æ¥é‡ç£…å‡çº§ï¼Œæˆ‘ä»¬æ¨å‡º Qwen-VL-Plus å’Œ Qwen-VL-Max ä¸¤ä¸ªå‡çº§ç‰ˆçš„æ¨¡å‹ã€‚ç›®å‰æ”¯æŒé€šè¿‡<a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Max\">ğŸ¤—</a>ã€<a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Max/summary\">ğŸ¤–</a>ã€[ç½‘é¡µç«¯](https://qianwen.aliyun.com)ã€[APP](http://ofasys-wlcb.oss-accelerate-overseas.aliyuncs.com/QwenVL/blog/app_qrcode.jpg) å’Œ [API](https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start)å…è´¹è®¿é—®ã€‚\n\n| æ¨¡å‹å | æ¨¡å‹ç®€ä»‹ |\n| --- | --- |\n| Qwen-VL-Plus | é€šä¹‰åƒé—®å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹å¢å¼ºç‰ˆã€‚å¤§å¹…æå‡ç»†èŠ‚è¯†åˆ«èƒ½åŠ›å’Œæ–‡å­—è¯†åˆ«èƒ½åŠ›ï¼Œæ”¯æŒè¶…ç™¾ä¸‡åƒç´ åˆ†è¾¨ç‡å’Œä»»æ„é•¿å®½æ¯”è§„æ ¼çš„å›¾åƒã€‚åœ¨å¹¿æ³›çš„è§†è§‰ä»»åŠ¡ä¸Šæä¾›**å“è¶Š**çš„æ€§èƒ½ã€‚ |\n| Qwen-VL-Max | é€šä¹‰åƒé—®è¶…å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ã€‚ç›¸æ¯”å¢å¼ºç‰ˆï¼Œå†æ¬¡æå‡è§†è§‰æ¨ç†èƒ½åŠ›å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œæä¾›æ›´é«˜çš„è§†è§‰æ„ŸçŸ¥å’Œè®¤çŸ¥æ°´å¹³ã€‚åœ¨æ›´å¤šå¤æ‚ä»»åŠ¡ä¸Šæä¾›**æœ€ä½³**çš„æ€§èƒ½ã€‚ |\n\nè¿™ä¸¤ä¸ªç‰ˆæœ¬çš„ä¸»è¦æŠ€æœ¯å‡çº§åœ¨äºï¼š\n- å¤§å¹…æå‡å›¾åƒç›¸å…³çš„æ¨ç†èƒ½åŠ›ï¼›\n- å¤§å¹…æå‡å¯¹å›¾ä¸­ç»†èŠ‚å’Œæ–‡å­—çš„è¯†åˆ«ã€æå–å’Œåˆ†æèƒ½åŠ›ï¼›\n- æ”¯æŒç™¾ä¸‡åƒç´ ä»¥ä¸Šçš„é«˜æ¸…åˆ†è¾¨ç‡å›¾ï¼Œæ”¯æŒå„ç§é•¿å®½æ¯”çš„å›¾åƒï¼›\n\nè¿™ä¸¤ä¸ªæ¨¡å‹ä¸ä»…å¤§å¹…è¶…è¶Šæ­¤å‰æ‰€æœ‰å¼€æº LVLM æ¨¡å‹çš„æœ€ä½³æ°´å¹³ï¼Œå¹¶ä¸”åœ¨å¤šé¡¹å›¾æ–‡å¤šæ¨¡æ€æ ‡å‡†æµ‹è¯•ä¸­è·å¾—äº†å ªæ¯” Gemini Ultra å’Œ GPT4-v çš„æ°´å‡†ã€‚\nç”šè‡³ï¼ŒQwen-VL-Max åœ¨ä¸­æ–‡é—®ç­”ã€ä¸­æ–‡æ–‡å­—ç†è§£ç›¸å…³çš„ä»»åŠ¡ä¸Šè¶…è¶Šäº† OpenAIçš„ GPT4-v å’Œ Google çš„ Gemini-Proã€‚\n\n<table>\n<thead>\n  <tr>\n    <th>Model</th>\n    <th>DocVQA<br><sup>(æ–‡æ¡£ç†è§£)</sup></th>\n    <th>ChartQA<br><sup>(å›¾è¡¨ç†è§£)</sup></th>\n    <th>AI2D<br><sup>(ç§‘å­¦å›¾ä¾‹)</sup></th>\n    <th>TextVQA<br><sup>(æ–‡å­—é˜…è¯»)</sup></th>\n    <th>MMMU<br><sup>(å¤šå­¦ç§‘é—®é¢˜)</sup></th>\n    <th>MathVista<br><sup>(æ•°å­¦æ¨ç†)</sup></th>\n    <th>MM-Bench-CN<br><sup>(ä¸­æ–‡é—®ç­”)</sup></th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td>Other Best<br>Open-source LVLM</td>\n    <td>81.6%<br><sup>(CogAgent)</sup></td>\n    <td>68.4%<br><sup>(CogAgent)</sup></td>\n    <td>73.7%<br><sup>(Fuyu-Medium)</sup></td>\n    <td>76.1%<br><sup>(CogAgent)</sup></td>\n    <td>45.9%<br><sup>(Yi-VL-34B)</sup></td>\n    <td>36.7%<br><sup>(SPHINX-V2)</sup></td>\n    <td>72.4%<br><sup>(InternLM-XComposer-VL)</sup></td>\n  </tr>\n  <tr>\n    <td>Gemini Pro</td>\n    <td>88.1%</td>\n    <td>74.1%</td>\n    <td>73.9%</td>\n    <td>74.6%</td>\n    <td>47.9%</td>\n    <td>45.2%</td>\n    <td>74.3%</td>\n  </tr>\n  <tr>\n    <td>Gemini Ultra</td>\n    <td>90.9%</td>\n    <td>80.8% <sup>1</sup></td>\n    <td>79.5% <sup>1</sup></td>\n    <td>82.3% <sup>1</sup></td>\n    <td>59.4% <sup>1</sup></td>\n    <td>53.0% <sup>1</sup></td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>GPT-4V</td>\n    <td>88.4%</td>\n    <td>78.5%</td>\n    <td>78.2%</td>\n    <td>78.0%</td>\n    <td>56.8%</td>\n    <td>49.9%</td>\n    <td>73.9%</td>\n  </tr>\n  <tr>\n    <td><b>Qwen-VL-Plus</b></td>\n    <td>91.4%</td>\n    <td>78.1%</td>\n    <td>75.9%</td>\n    <td>78.9%</td>\n    <td>44.0%</td>\n    <td>43.3%</td>\n    <td>68.0%</td>\n  </tr>\n  <tr>\n    <td><b>Qwen-VL-Max</b></td>\n    <td>92.5% <sup>1</sup></td>\n    <td>79.8% <sup>2</sup></td>\n    <td>79.3% <sup>2</sup></td>\n    <td>79.5% <sup>2</sup></td>\n    <td>51.4% <sup>3</sup></td>\n    <td>51.0% <sup>2</sup></td>\n    <td>75.1% <sup>1</sup></td>\n  </tr>\n</tbody>\n</table>\n\næ‰€æœ‰è¯„æµ‹éƒ½æ˜¯åœ¨ä¸ä½¿ç”¨ä»»ä½•å¤–éƒ¨OCRå·¥å…·(â€œonly pixelâ€)çš„æƒ…å†µä¸‹è·å¾—çš„ã€‚\n\n---\n\n## æ–°é—»\n* 2024å¹´01æœˆ18æ—¥ æˆ‘ä»¬æ¨å‡º Qwen-Vl-Maxï¼Œå¤§å¹…è¶…è¶Šæ­¤å‰æ‰€æœ‰å¼€æº LVLM æ¨¡å‹çš„æœ€ä½³æ°´å¹³ï¼Œå¹¶ä¸”åœ¨å¤šé¡¹å›¾æ–‡å¤šæ¨¡æ€æ ‡å‡†æµ‹è¯•ä¸­è·å¾—äº†å ªæ¯” Gemini Ultra å’Œ GPT4-v çš„æ°´å‡†ã€‚ç›´æ¥è®¿é—®[é€šä¹‰åƒé—®ç½‘é¡µç«¯æˆ–APP](https://qianwen.aliyun.com)å°±èƒ½ä½“éªŒæ–°æ¨¡å‹ã€‚\n* 2023å¹´11æœˆ28æ—¥ Qwen-VLå•æ¨¡å‹åœ¨[DOCVQA](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1)è¾¾åˆ°äº†æœ€å¼ºæ°´å¹³ï¼Œè¶…è¶Šäº†GPT4V,PALI-Xï¼Œä¸æ­¤åŒæ—¶å®ƒè¿˜æ˜¯ä¸€ä¸ªé€šç”¨æ¨¡å‹ï¼Œç›´æ¥è¾“å…¥å›¾ç‰‡å°±èƒ½å¸®ä½ åˆ†æç†è§£å„ç§ä»»åŠ¡ã€‚\n* 2023å¹´9æœˆ12æ—¥ æ›´æ–°Qwen-VL-Chatæ¨¡å‹ï¼Œè¯¥æ¨¡å‹æœ‰æ›´é²æ£’çš„ä¸­æ–‡æŒ‡ä»¤è·Ÿéšï¼Œæ›´å¥½çš„ç½‘é¡µå’Œè¡¨æ ¼å›¾ç‰‡ç†è§£å’Œé—®ç­”èƒ½åŠ›ä»¥åŠæ›´å¥½çš„å¯¹è¯è¡¨ç°(Touchstone: ä¸­æ–‡: 401.2->481.7, è‹±æ–‡: 645.2->711.6)ã€‚\n* 2023å¹´9æœˆ12æ—¥ æ”¯æŒQwen-VLå’ŒQwen-VL-Chatçš„å¾®è°ƒï¼Œå…¶ä¸­åŒ…æ‹¬å…¨å‚æ•°å¾®è°ƒã€LoRAä»¥åŠQ-LoRA\n* 2023å¹´9æœˆ8æ—¥ æ„Ÿè°¢[camenduru](https://github.com/camenduru)è´¡çŒ®äº†[Colab](https://github.com/camenduru/Qwen-VL-Chat-colab)ç¤ºä¾‹ï¼Œæ¯ä¸ªäººéƒ½å¯ä»¥ä»¥æ­¤ä¸ºæ•™ç¨‹ï¼Œåœ¨12Gçš„GPUä¸Šåšæœ¬åœ°æˆ–åœ¨çº¿çš„Demoã€‚\n* 2023å¹´9æœˆ5æ—¥ åœ¨ç¤¾åŒºå¤šæ¨¡æ€é€šç”¨æ¨¡å‹æ¦œå• [MME Benchmark](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) ä¸Šå–å¾—äº†æ„ŸçŸ¥å’Œè®¤çŸ¥åŒèµ›é“çš„å½“å‰æœ€å¥½ç»“æœã€‚\n* 2023å¹´9æœˆ4æ—¥ åœ¨ç¤¾åŒºå¤šæ¨¡æ€é€šç”¨æ¨¡å‹æ¦œå• [SEED-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard) ä¸Šå–å¾—äº†å›¾åƒç†è§£å’Œè§†é¢‘ç†è§£çš„å½“å‰æœ€å¥½ç»“æœã€‚\n* 2023å¹´9æœˆ1æ—¥ å‘å¸ƒ[TouchStone](https://github.com/OFA-Sys/TouchStone) æµ‹è¯„, è¿™æ˜¯ä¸€ä¸ªç»¼åˆè¯„ä¼°LVLMèƒ½åŠ›çš„æµ‹è¯„,å®ƒä¸ä»…è€ƒå¯Ÿæ¨¡å‹çš„è§†è§‰æè¿°å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿˜åŒ…æ‹¬æ ¹æ®è§†è§‰å†…å®¹çš„æ–‡å­¦åˆ›ä½œèƒ½åŠ›ã€‚åŒæ—¶å®ƒæ˜¯å°†å¤šæ¨¡æ€ä¿¡æ¯ç”¨æ–‡æœ¬è¡¨è¿°å¹¶ç”¨LLMsè¿›è¡Œè¯„ä¼°çš„æ–¹æ³•ã€‚\n* 2023å¹´8æœˆ31æ—¥ å‘å¸ƒQwen-VL-Chaté‡åŒ–æ¨¡å‹ï¼Œ**Qwen-VL-Chat-Int4**,è¯¥æ¨¡å‹æ˜¾å­˜å ç”¨ä½ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åŠç²¾åº¦æ¨¡å‹æ˜¾è‘—æå‡ï¼Œåœ¨åŸºå‡†è¯„æµ‹ä¸Šæ•ˆæœæŸå¤±è¾ƒå°ã€‚\n* 2023å¹´8æœˆ22æ—¥ åœ¨é­”æ­ç¤¾åŒºï¼ˆModelScopeï¼‰å’ŒHugging FaceåŒæ­¥æ¨å‡ºQwen-VLå’ŒQwen-VL-Chatæ¨¡å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æä¾›ä¸€ä¸ª[è®ºæ–‡](https://arxiv.org/abs/2308.12966)ä»‹ç»äº†ç›¸å…³çš„æ¨¡å‹ç»“æ„ã€è®­ç»ƒç»†èŠ‚å’Œæ¨¡å‹è¡¨ç°ã€‚\n\n---\n\n## Qwen-VL\n\n**Qwen-VL** æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLarge Vision Language Model, LVLMï¼‰ã€‚Qwen-VL å¯ä»¥ä»¥å›¾åƒã€æ–‡æœ¬ã€æ£€æµ‹æ¡†ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä»¥æ–‡æœ¬å’Œæ£€æµ‹æ¡†ä½œä¸ºè¾“å‡ºã€‚Qwen-VL ç³»åˆ—æ¨¡å‹çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼š\n\n- **å¼ºå¤§çš„æ€§èƒ½**ï¼šåœ¨å››å¤§ç±»å¤šæ¨¡æ€ä»»åŠ¡çš„æ ‡å‡†è‹±æ–‡æµ‹è¯„ä¸­ï¼ˆZero-shot Captioning/VQA/DocVQA/Groundingï¼‰ä¸Šï¼Œå‡å–å¾—åŒç­‰é€šç”¨æ¨¡å‹å¤§å°ä¸‹æœ€å¥½æ•ˆæœï¼›\n- **å¤šè¯­è¨€å¯¹è¯æ¨¡å‹**ï¼šå¤©ç„¶æ”¯æŒè‹±æ–‡ã€ä¸­æ–‡ç­‰å¤šè¯­è¨€å¯¹è¯ï¼Œç«¯åˆ°ç«¯æ”¯æŒå›¾ç‰‡é‡Œä¸­è‹±åŒè¯­çš„é•¿æ–‡æœ¬è¯†åˆ«ï¼›\n- **å¤šå›¾äº¤é”™å¯¹è¯**ï¼šæ”¯æŒå¤šå›¾è¾“å…¥å’Œæ¯”è¾ƒï¼ŒæŒ‡å®šå›¾ç‰‡é—®ç­”ï¼Œå¤šå›¾æ–‡å­¦åˆ›ä½œç­‰ï¼›\n- **é¦–ä¸ªæ”¯æŒä¸­æ–‡å¼€æ”¾åŸŸå®šä½çš„é€šç”¨æ¨¡å‹**ï¼šé€šè¿‡ä¸­æ–‡å¼€æ”¾åŸŸè¯­è¨€è¡¨è¾¾è¿›è¡Œæ£€æµ‹æ¡†æ ‡æ³¨ï¼›\n- **ç»†ç²’åº¦è¯†åˆ«å’Œç†è§£**ï¼šç›¸æ¯”äºç›®å‰å…¶å®ƒå¼€æºLVLMä½¿ç”¨çš„224åˆ†è¾¨ç‡ï¼ŒQwen-VLæ˜¯é¦–ä¸ªå¼€æºçš„448åˆ†è¾¨ç‡çš„LVLMæ¨¡å‹ã€‚æ›´é«˜åˆ†è¾¨ç‡å¯ä»¥æå‡ç»†ç²’åº¦çš„æ–‡å­—è¯†åˆ«ã€æ–‡æ¡£é—®ç­”å’Œæ£€æµ‹æ¡†æ ‡æ³¨ã€‚\n\n<br>\n<p align=\"center\">\n    <img src=\"assets/demo_vl.gif\" width=\"400\"/>\n<p>\n<br>\n\nç›®å‰ï¼Œæˆ‘ä»¬æä¾›äº† Qwen-VL ç³»åˆ—çš„ä¸¤ä¸ªæ¨¡å‹ï¼š\n\n- Qwen-VL: Qwen-VL ä»¥ Qwen-7B çš„é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºè¯­è¨€æ¨¡å‹çš„åˆå§‹åŒ–ï¼Œå¹¶ä»¥ [Openclip ViT-bigG](https://github.com/mlfoundations/open_clip) ä½œä¸ºè§†è§‰ç¼–ç å™¨çš„åˆå§‹åŒ–ï¼Œä¸­é—´åŠ å…¥å•å±‚éšæœºåˆå§‹åŒ–çš„ cross-attentionï¼Œç»è¿‡çº¦1.5Bçš„å›¾æ–‡æ•°æ®è®­ç»ƒå¾—åˆ°ã€‚æœ€ç»ˆå›¾åƒè¾“å…¥åˆ†è¾¨ç‡ä¸º448ã€‚\n- Qwen-VL-Chat: åœ¨ Qwen-VL çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹é½æœºåˆ¶æ‰“é€ äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰AIåŠ©æ‰‹Qwen-VL-Chatï¼Œå®ƒæ”¯æŒæ›´çµæ´»çš„äº¤äº’æ–¹å¼ï¼ŒåŒ…æ‹¬å¤šå›¾ã€å¤šè½®é—®ç­”ã€åˆ›ä½œç­‰èƒ½åŠ›ã€‚\n  <br>\n\n## è¯„æµ‹\n\næˆ‘ä»¬ä»ä¸‰ä¸ªè§’åº¦è¯„æµ‹äº†æ¨¡å‹çš„èƒ½åŠ›ï¼š\n\n1. åœ¨**è‹±æ–‡æ ‡å‡† Benchmark** ä¸Šè¯„æµ‹æ¨¡å‹çš„åŸºç¡€ä»»åŠ¡èƒ½åŠ›ã€‚ç›®å‰è¯„æµ‹äº†å››å¤§ç±»å¤šæ¨¡æ€ä»»åŠ¡ï¼š\n   \n   - Zero-shot Captioning: è¯„æµ‹æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å›¾ç‰‡æè¿°èƒ½åŠ›ï¼›\n   - General VQA: è¯„æµ‹æ¨¡å‹çš„é€šç”¨é—®ç­”èƒ½åŠ›ï¼Œä¾‹å¦‚åˆ¤æ–­é¢˜ã€é¢œè‰²ã€ä¸ªæ•°ã€ç±»ç›®ç­‰é—®ç­”èƒ½åŠ›ï¼›\n   - Text-based VQAï¼šè¯„æµ‹æ¨¡å‹å¯¹äºå›¾ç‰‡ä¸­æ–‡å­—ç›¸å…³çš„è¯†åˆ«/é—®ç­”èƒ½åŠ›ï¼Œä¾‹å¦‚æ–‡æ¡£é—®ç­”ã€å›¾è¡¨é—®ç­”ã€æ–‡å­—é—®ç­”ç­‰ï¼›\n   - Referring Expression Compressionï¼šè¯„æµ‹æ¨¡å‹ç»™å®šç‰©ä½“æè¿°ç”»æ£€æµ‹æ¡†çš„èƒ½åŠ›ï¼›\n2. **è¯•é‡‘çŸ³ (TouchStone)**ï¼šä¸ºäº†è¯„æµ‹æ¨¡å‹æ•´ä½“çš„å›¾æ–‡å¯¹è¯èƒ½åŠ›å’Œäººç±»å¯¹é½æ°´å¹³ã€‚æˆ‘ä»¬ä¸ºæ­¤æ„å»ºäº†ä¸€ä¸ªåŸºäº GPT4 æ‰“åˆ†æ¥è¯„æµ‹ LVLM æ¨¡å‹çš„ Benchmarkï¼šTouchStoneã€‚åœ¨ TouchStone-v0.1 ä¸­ï¼š\n   \n   - è¯„æµ‹åŸºå‡†æ€»è®¡æ¶µç›– 300+å¼ å›¾ç‰‡ã€800+é“é¢˜ç›®ã€27ä¸ªç±»åˆ«ã€‚åŒ…æ‹¬åŸºç¡€å±æ€§é—®ç­”ã€äººç‰©åœ°æ ‡é—®ç­”ã€å½±è§†ä½œå“é—®ç­”ã€è§†è§‰æ¨ç†ã€åäº‹å®æ¨ç†ã€è¯—æ­Œåˆ›ä½œã€æ•…äº‹å†™ä½œï¼Œå•†å“æ¯”è¾ƒã€å›¾ç‰‡è§£é¢˜ç­‰**å°½å¯èƒ½å¹¿æ³›çš„ç±»åˆ«**ã€‚\n   - ä¸ºäº†å¼¥è¡¥ç›®å‰ GPT4 æ— æ³•ç›´æ¥è¯»å–å›¾ç‰‡çš„ç¼ºé™·ï¼Œæˆ‘ä»¬ç»™æ‰€æœ‰çš„å¸¦è¯„æµ‹å›¾ç‰‡æä¾›äº†**äººå·¥æ ‡æ³¨çš„å……åˆ†è¯¦ç»†æè¿°**ï¼Œå¹¶ä¸”å°†å›¾ç‰‡çš„è¯¦ç»†æè¿°ã€é—®é¢˜å’Œæ¨¡å‹çš„è¾“å‡ºç»“æœä¸€èµ·äº¤ç»™ GPT4 æ‰“åˆ†ã€‚\n   - è¯„æµ‹åŒæ—¶åŒ…å«è‹±æ–‡ç‰ˆæœ¬å’Œä¸­æ–‡ç‰ˆæœ¬ã€‚\n\n3. **å…¶å®ƒå¤šæ¨¡æ€é€šç”¨æ¨¡å‹æ¦œå•**ï¼šæˆ‘ä»¬ä¹Ÿåœ¨å…¶å®ƒå¤šæ¨¡æ€é€šç”¨æ¨¡å‹æ¦œå•ä¸­è¯„æµ‹äº†æ¨¡å‹çš„èƒ½åŠ›ï¼š\n   \n   - MME Benchmark: æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»¼åˆè¯„ä»·åŸºå‡†ã€‚å®ƒåœ¨æ€»å…±14ä¸ªå­ä»»åŠ¡ä¸Šè¯„æµ‹**æ„ŸçŸ¥å’Œè®¤çŸ¥**èƒ½åŠ›ï¼ŒQwen-VL-Chatåœ¨è¿™ä¸¤ä¸ªæ€»ç»´åº¦ä¸Šéƒ½å®ç°äº†å½“å‰æœ€å¥½ç»“æœã€‚\n   - SEED-Bench: æ˜¯ä¸€ä¸ªåŒ…å«1.9ä¸‡é€‰æ‹©é¢˜çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯„ï¼Œé€šè¿‡äººå·¥æ³¨é‡Šçš„ç»“æœè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œæ¶µç›–12ä¸ªè¯„ä¼°ç»´åº¦ï¼ŒåŒ…æ‹¬**å›¾åƒå’Œè§†é¢‘ç†è§£**ï¼ŒQwen-VLå’ŒQwen-VL-chatåœ¨è¿™ä¸ªåŸºå‡†ä¸Šå®ç°äº†å½“å‰æœ€å¥½ç»“æœã€‚\n\nè¯„æµ‹ç»“æœå¦‚ä¸‹ï¼š\n\nQwen-VLåœ¨å¤šä¸ªVLä»»åŠ¡ä¸Šç›¸æ¯”ç›®å‰SOTAçš„Generalist Modelséƒ½æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼Œå¹¶ä¸”åœ¨èƒ½åŠ›èŒƒå›´ä¹Ÿè¦†ç›–æ›´åŠ å…¨é¢ã€‚\n\n<p align=\"center\">\n    <img src=\"assets/radar.png\" width=\"600\"/>\n<p>\n\n### é›¶æ ·æœ¬å›¾åƒæè¿°ç”Ÿæˆï¼ˆZero-shot Image Captionï¼‰ åŠ é€šç”¨è§†è§‰é—®ç­”ï¼ˆGeneral VQAï¼‰\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"2\">Zero-shot Captioning</th>\n    <th colspan=\"5\">General VQA</th>\n  </tr>\n  <tr>\n    <th>NoCaps</th>\n    <th>Flickr30K</th>\n    <th>VQAv2<sup>dev</sup></th>\n    <th>OK-VQA</th>\n    <th>GQA</th>\n    <th>SciQA-Img<br>(0-shot)</th>\n    <th>VizWiz<br>(0-shot)</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"10\">Generalist<br>Models</td>\n    <td>Flamingo-9B</td>\n    <td>-</td>\n    <td>61.5</td>\n    <td>51.8</td>\n    <td>44.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>28.8</td>\n  </tr>\n  <tr>\n    <td>Flamingo-80B</td>\n    <td>-</td>\n    <td>67.2</td>\n    <td>56.3</td>\n    <td>50.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>31.6</td>\n  </tr>\n  <tr>\n    <td>Unified-IO-XL</td>\n    <td>100.0</td>\n    <td>-</td>\n    <td>77.9</td>\n    <td>54.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Kosmos-1</td>\n    <td>-</td>\n    <td>67.1</td>\n    <td>51.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>29.2</td>\n  </tr>\n  <tr>\n    <td>Kosmos-2</td>\n    <td>-</td>\n    <td>66.7</td>\n    <td>45.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>103.9</td>\n    <td>71.6</td>\n    <td>65.0</td>\n    <td>45.9</td>\n    <td>32.3</td>\n    <td>61.0</td>\n    <td>19.6</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td><strong>121.9</strong></td>\n    <td>82.8</td>\n    <td>-</td>\n    <td>-</td>\n    <td>49.5</td>\n    <td>63.1</td>\n    <td>33.4</td>\n  </tr>\n  <tr>\n    <td>Shikra (Vicuna-13B)</td>\n    <td>-</td>\n    <td>73.9</td>\n    <td>77.36</td>\n    <td>47.16</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td><strong>Qwen-VL (Qwen-7B)</strong></td>\n    <td>121.4</td>\n    <td><b>85.8</b></td>\n    <td><b>78.8</b></td>\n    <td><b>58.6</b></td>\n    <td><b>59.3</b></td>\n    <td>67.1</td>\n    <td>35.2</td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>63.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>39.1</td>\n  </tr> -->\n  <tr>\n    <td>Qwen-VL-Chat</td>\n    <td>120.2</td>\n    <td>81.0</td>\n    <td>78.2</td>\n    <td>56.6</td>\n    <td>57.5</td>\n    <td><b>68.2</b></td>\n    <td><b>38.9</b></td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL-Chat (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>60.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>44.45</td>\n  </tr> -->\n  <tr>\n    <td>Previous SOTA<br>(Per Task Fine-tuning)</td>\n    <td>-</td>\n    <td>127.0<br>(PALI-17B)</td>\n    <td>84.5<br>(InstructBLIP<br>-FlanT5-XL)</td>\n    <td>86.1<br>(PALI-X<br>-55B)</td>\n    <td>66.1<br>(PALI-X<br>-55B)</td>\n    <td>72.1<br>(CFR)</td>\n    <td>92.53<br>(LLaVa+<br>GPT-4)</td>\n    <td>70.9<br>(PALI-X<br>-55B)</td>\n  </tr>\n</tbody>\n</table>\n\n- åœ¨ Zero-shot Captioning ä¸­ï¼ŒQwen-VL åœ¨ Flickr30K æ•°æ®é›†ä¸Šå–å¾—äº† **SOTA** çš„ç»“æœï¼Œå¹¶åœ¨ Nocaps æ•°æ®é›†ä¸Šå–å¾—äº†å’Œ InstructBlip å¯ç«äº‰çš„ç»“æœã€‚\n- åœ¨ General VQA ä¸­ï¼ŒQwen-VL å–å¾—äº† LVLM æ¨¡å‹åŒç­‰é‡çº§å’Œè®¾å®šä¸‹ **SOTA** çš„ç»“æœã€‚\n\n### æ–‡æœ¬å¯¼å‘çš„è§†è§‰é—®ç­”ï¼ˆText-oriented VQAï¼‰\n\n<table>\n<thead>\n  <tr>\n    <th>Model type</th>\n    <th>Model</th>\n    <th>TextVQA</th>\n    <th>DocVQA</th>\n    <th>ChartQA</th>\n    <th>AI2D</th>\n    <th>OCR-VQA</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"5\">Generalist Models</td>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>42.4</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td>50.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>mPLUG-DocOwl (LLaMA-7B)</td>\n    <td>52.6</td>\n    <td>62.2</td>\n    <td>57.4</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Pix2Struct-Large (1.3B)</td>\n    <td>-</td>\n    <td><b>76.6</b></td>\n    <td>58.6</td>\n    <td>42.1</td>\n    <td>71.3</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL (Qwen-7B)</td>\n    <td><b>63.8</b></td>\n    <td>65.1</td>\n    <td><b>65.7</b></td>\n    <td><b>62.3</b></td>\n    <td><b>75.7</b></td>\n  </tr>\n  <tr>\n    <td>Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>PALI-X-55B (Single-task FT)<br>(Without OCR Pipeline)</td>\n    <td>71.44</td>\n    <td>80.0</td>\n    <td>70.0</td>\n    <td>81.2</td>\n    <td>75.0</td>\n  </tr>\n</tbody>\n</table>\n\n- åœ¨æ–‡å­—ç›¸å…³çš„è¯†åˆ«/é—®ç­”è¯„æµ‹ä¸Šï¼Œå–å¾—äº†å½“å‰è§„æ¨¡ä¸‹é€šç”¨ LVLM è¾¾åˆ°çš„æœ€å¥½ç»“æœã€‚\n- åˆ†è¾¨ç‡å¯¹ä¸Šè¿°æŸå‡ ä¸ªè¯„æµ‹éå¸¸é‡è¦ï¼Œå¤§éƒ¨åˆ† 224 åˆ†è¾¨ç‡çš„å¼€æº LVLM æ¨¡å‹æ— æ³•å®Œæˆä»¥ä¸Šè¯„æµ‹ï¼Œæˆ–åªèƒ½é€šè¿‡åˆ‡å›¾çš„æ–¹å¼è§£å†³ã€‚Qwen-VL å°†åˆ†è¾¨ç‡æå‡åˆ° 448ï¼Œå¯ä»¥ç›´æ¥ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œä»¥ä¸Šè¯„æµ‹ã€‚Qwen-VL åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šç”šè‡³è¶…è¿‡äº† 1024 åˆ†è¾¨ç‡çš„ Pix2Struct-Large æ¨¡å‹ã€‚\n\n### ç»†ç²’åº¦è§†è§‰å®šä½ï¼ˆReferring Expression Comprehensionï¼‰\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"3\">RefCOCO</th>\n    <th colspan=\"3\">RefCOCO+</th>\n    <th colspan=\"2\">RefCOCOg</th>\n    <th>GRIT</th>\n  </tr>\n  <tr>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val-u</th>\n    <th>test-u</th>\n    <th>refexp</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"8\">Generalist Models</td>\n    <td>GPV-2</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>51.50</td>\n  </tr>\n  <tr>\n    <td>OFA-L*</td>\n    <td>79.96</td>\n    <td>83.67</td>\n    <td>76.39</td>\n    <td>68.29</td>\n    <td>76.00</td>\n    <td>61.75</td>\n    <td>67.57</td>\n    <td>67.58</td>\n    <td>61.70</td>\n  </tr>\n  <tr>\n    <td>Unified-IO</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td><b>78.61</b></td>\n  </tr>\n  <tr>\n    <td>VisionLLM-H</td>\n    <td></td>\n    <td>86.70</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Shikra-7B</td>\n    <td>87.01</td>\n    <td>90.61</td>\n    <td>80.24 </td>\n    <td>81.60</td>\n    <td>87.36</td>\n    <td>72.12</td>\n    <td>82.27</td>\n    <td>82.19</td>\n    <td>69.34</td>\n  </tr>\n  <tr>\n    <td>Shikra-13B</td>\n    <td>87.83 </td>\n    <td>91.11</td>\n    <td>81.81</td>\n    <td>82.89</td>\n    <td>87.79</td>\n    <td>74.41</td>\n    <td>82.64</td>\n    <td>83.16</td>\n    <td>69.03</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B</td>\n    <td><b>89.36</b></td>\n    <td>92.26</td>\n    <td><b>85.34</b></td>\n    <td><b>83.12</b></td>\n    <td>88.25</td>\n    <td><b>77.21</b></td>\n    <td>85.58</td>\n    <td>85.48</td>\n    <td>78.22</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B-Chat</td>\n    <td>88.55</td>\n    <td><b>92.27</b></td>\n    <td>84.51</td>\n    <td>82.82</td>\n    <td><b>88.59</b></td>\n    <td>76.79</td>\n    <td><b>85.96</b></td>\n    <td><b>86.32</b></td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td rowspan=\"3\">Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>G-DINO-L</td>\n    <td>90.56&nbsp;&nbsp;</td>\n    <td>93.19</td>\n    <td>88.24</td>\n    <td>82.75</td>\n    <td>88.95</td>\n    <td>75.92</td>\n    <td>86.13</td>\n    <td>87.02</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>UNINEXT-H</td>\n    <td>92.64 </td>\n    <td>94.33</td>\n    <td>91.46</td>\n    <td>85.24</td>\n    <td>89.63</td>\n    <td>79.79</td>\n    <td>88.73</td>\n    <td>89.37</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>ONE-PEACE</td>\n    <td>92.58 </td>\n    <td>94.18</td>\n    <td>89.26</td>\n    <td>88.77</td>\n    <td>92.21</td>\n    <td>83.23</td>\n    <td>89.22</td>\n    <td>89.27</td>\n    <td>-</td>\n  </tr>\n</tbody>\n</table>\n\n- åœ¨å®šä½ä»»åŠ¡ä¸Šï¼ŒQwen-VL å…¨é¢è¶…è¿‡ Shikra-13Bï¼Œå–å¾—äº†ç›®å‰ Generalist LVLM æ¨¡å‹ä¸Šåœ¨ Refcoco ä¸Šçš„ **SOTA**ã€‚\n- Qwen-VL å¹¶æ²¡æœ‰åœ¨ä»»ä½•ä¸­æ–‡å®šä½æ•°æ®ä¸Šè®­ç»ƒè¿‡ï¼Œä½†é€šè¿‡ä¸­æ–‡ Caption æ•°æ®å’Œ è‹±æ–‡ Grounding æ•°æ®çš„è®­ç»ƒï¼Œå¯ä»¥ Zero-shot æ³›åŒ–å‡ºä¸­æ–‡ Grounding èƒ½åŠ›ã€‚\n\næˆ‘ä»¬æä¾›äº†ä»¥ä¸Š**æ‰€æœ‰**è¯„æµ‹è„šæœ¬ä»¥ä¾›å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœã€‚è¯·é˜…è¯» [eval_mm/EVALUATION.md](eval_mm/EVALUATION.md) äº†è§£æ›´å¤šä¿¡æ¯ã€‚\n\n### å¯¹è¯èƒ½åŠ›æµ‹è¯„\n\nTouchStone æ˜¯ä¸€ä¸ªåŸºäº GPT4 æ‰“åˆ†æ¥è¯„æµ‹ LVLM æ¨¡å‹çš„å›¾æ–‡å¯¹è¯èƒ½åŠ›å’Œäººç±»å¯¹é½æ°´å¹³çš„åŸºå‡†ã€‚å®ƒæ¶µç›–äº† 300+å¼ å›¾ç‰‡ã€800+é“é¢˜ç›®ã€27ä¸ªç±»åˆ«ï¼ŒåŒ…æ‹¬åŸºç¡€å±æ€§ã€äººç‰©åœ°æ ‡ã€è§†è§‰æ¨ç†ã€è¯—æ­Œåˆ›ä½œã€æ•…äº‹å†™ä½œã€å•†å“æ¯”è¾ƒã€å›¾ç‰‡è§£é¢˜ç­‰**å°½å¯èƒ½å¹¿æ³›çš„ç±»åˆ«**ã€‚å…³äº TouchStone çš„è¯¦ç»†ä»‹ç»ï¼Œè¯·å‚è€ƒ[touchstone/README_CN.md](touchstone/README_CN.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚\n\n#### è‹±è¯­\n\n| Model            | Score |\n| ---------------- | ----- |\n| PandaGPT         | 488.5 |\n| MiniGPT4         | 531.7 |\n| InstructBLIP     | 552.4 |\n| LLaMA-AdapterV2  | 590.1 |\n| LLaVA            | 602.7 |\n| mPLUG-Owl        | 605.4 |\n| Qwen-VL-Chat     | 645.2 |\n| Qwen-VL-Chat-1.1 | 711.6 |\n\n#### ä¸­æ–‡\n\n| Model            | Score |\n| ---------------- | ----- |\n| VisualGLM        | 247.1 |\n| Qwen-VL-Chat     | 401.2 |\n| Qwen-VL-Chat-1.1 | 481.7 |\n\nQwen-VL-Chat æ¨¡å‹åœ¨ä¸­è‹±æ–‡çš„å¯¹é½è¯„æµ‹ä¸­å‡å–å¾—å½“å‰ LVLM æ¨¡å‹ä¸‹çš„æœ€å¥½ç»“æœã€‚\n<br>\n\n### å…¶å®ƒæ¦œå•æµ‹è¯„\n\n#### MME Benchmark\n\nMMEæ˜¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»¼åˆè¯„ä»·åŸºå‡†ã€‚å®ƒåœ¨æ€»å…±14ä¸ªå­ä»»åŠ¡ä¸Šè¯„æµ‹**æ„ŸçŸ¥å’Œè®¤çŸ¥**èƒ½åŠ›ã€‚Qwen-VL-Chatåœ¨è¿™ä¸ªåŸºå‡†ä¸Šå®ç°äº†SOTAsã€‚å®Œæ•´å¤ç°[è§æ­¤](eval_mm/mme/EVAL_MME.md).\n\n<p align=\"center\">\n    <img src=\"eval_mm/mme/perception.jpg\" width=\"600\"/>\n<p>\n<p align=\"center\">\n    <img src=\"eval_mm/mme/cognition.jpg\" width=\"600\"/>\n<p>\n\n#### SEED-Bench\n\nSEED-Benchæ˜¯ä¸€ä¸ªåŒ…å«1.9ä¸‡é€‰æ‹©é¢˜çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯„ï¼Œé€šè¿‡äººå·¥æ³¨é‡Šçš„ç»“æœè¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œæ¶µç›–12ä¸ªè¯„ä¼°ç»´åº¦ï¼ŒåŒ…æ‹¬**å›¾åƒå’Œè§†é¢‘ç†è§£**ã€‚Qwen-VLå’ŒQwen-VL-chatåœ¨è¿™ä¸ªåŸºå‡†ä¸Šå®ç°äº†SOTAsã€‚å®Œæ•´å¤ç°[è§æ­¤](eval_mm/seed_bench/EVAL_SEED.md)ã€‚\n\n<p align=\"center\">\n    <img src=\"eval_mm/seed_bench/leaderboard.jpg\"/>\n<p>\n\n## éƒ¨ç½²è¦æ±‚\n\n* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬\n* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬\n* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰\n<br>\n\n## å¿«é€Ÿä½¿ç”¨\n\næˆ‘ä»¬æä¾›ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ ğŸ¤– ModelScope å’Œ ğŸ¤— Transformers å¿«é€Ÿä½¿ç”¨ Qwen-VL å’Œ Qwen-VL-Chatã€‚\n\nåœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»é…ç½®å¥½ç¯å¢ƒå¹¶å®‰è£…å¥½ç›¸å…³çš„ä»£ç åŒ…ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç¡®ä¿ä½ æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œç„¶åå®‰è£…ç›¸å…³çš„ä¾èµ–åº“ã€‚\n\n```bash\npip install -r requirements.txt\n```\n\næ¥ä¸‹æ¥ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨Transformersæˆ–è€…ModelScopeæ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚å…³äºè§†è§‰æ¨¡å—çš„æ›´å¤šç”¨æ³•ï¼Œè¯·å‚è€ƒ[æ•™ç¨‹](TUTORIAL_zh.md)ã€‚\n\n#### ğŸ¤— Transformers\n\nå¦‚å¸Œæœ›ä½¿ç”¨ Qwen-VL-chat è¿›è¡Œæ¨ç†ï¼Œæ‰€éœ€è¦å†™çš„åªæ˜¯å¦‚ä¸‹æ‰€ç¤ºçš„æ•°è¡Œä»£ç ã€‚**è¯·ç¡®ä¿ä½ ä½¿ç”¨çš„æ˜¯æœ€æ–°ä»£ç ã€‚**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\n# è¯·æ³¨æ„ï¼šåˆ†è¯å™¨é»˜è®¤è¡Œä¸ºå·²æ›´æ”¹ä¸ºé»˜è®¤å…³é—­ç‰¹æ®Štokenæ”»å‡»é˜²æŠ¤ã€‚\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# é»˜è®¤gpuè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦24GBæ˜¾å­˜\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚ï¼ˆtransformers 4.32.0åŠä»¥ä¸Šæ— éœ€æ‰§è¡Œæ­¤æ“ä½œï¼‰\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# ç¬¬ä¸€è½®å¯¹è¯\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url\n    {'text': 'è¿™æ˜¯ä»€ä¹ˆ?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n# å›¾ä¸­æ˜¯ä¸€åå¥³å­åœ¨æ²™æ»©ä¸Šå’Œç‹—ç©è€ï¼Œæ—è¾¹æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œå®ƒä»¬å¤„äºæ²™æ»©ä¸Šã€‚\n\n# ç¬¬äºŒè½®å¯¹è¯\nresponse, history = model.chat(tokenizer, 'æ¡†å‡ºå›¾ä¸­å‡»æŒçš„ä½ç½®', history=history)\nprint(response)\n# <ref>å‡»æŒ</ref><box>(536,509),(588,602)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('1.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n\nè¿è¡ŒQwen-VLåŒæ ·éå¸¸ç®€å•ã€‚\n\n<summary>è¿è¡ŒQwen-VL</summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\n# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cpu\", trust_remote_code=True).eval()\n# é»˜è®¤gpuè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦24GBæ˜¾å­˜\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚ï¼ˆtransformers 4.32.0åŠä»¥ä¸Šæ— éœ€æ‰§è¡Œæ­¤æ“ä½œï¼‰\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url\n    {'text': 'Generate the caption in English with grounding:'},\n])\ninputs = tokenizer(query, return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nresponse = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)\nprint(response)\n# <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>Generate the caption in English with grounding:<ref> Woman</ref><box>(451,379),(731,806)</box> and<ref> her dog</ref><box>(219,424),(576,896)</box> playing on the beach<|endoftext|>\nimage = tokenizer.draw_bbox_on_latest_picture(response)\nif image:\n  image.save('2.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_spotting_caption.jpg\" width=\"500\"/>\n<p>\n\nè‹¥åœ¨ä½¿ç”¨ä¸Šè¿°ä»£ç æ—¶ç”±äºå„ç§åŸå› æ— æ³•ä» HuggingFace æ‹‰å–æ¨¡å‹å’Œä»£ç ï¼Œå¯ä»¥å…ˆä» ModelScope ä¸‹è½½æ¨¡å‹åŠä»£ç è‡³æœ¬åœ°ï¼Œå†ä»æœ¬åœ°åŠ è½½æ¨¡å‹ï¼š\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-VL')\nmodel_dir = snapshot_download('qwen/Qwen-VL-Chat')\n\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"cuda\",\n    trust_remote_code=True\n).eval()\n```\n\n#### ğŸ¤– ModelScope\n\né­”æ­ï¼ˆModelScopeï¼‰æ˜¯å¼€æºçš„æ¨¡å‹å³æœåŠ¡å…±äº«å¹³å°ï¼Œä¸ºæ³›AIå¼€å‘è€…æä¾›çµæ´»ã€æ˜“ç”¨ã€ä½æˆæœ¬çš„ä¸€ç«™å¼æ¨¡å‹æœåŠ¡äº§å“ã€‚ä½¿ç”¨ModelScopeåŒæ ·éå¸¸ç®€å•ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š\n\n```python\nfrom modelscope import (\n    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n)\nimport torch\nmodel_id = 'qwen/Qwen-VL-Chat'\nrevision = 'v1.0.0'\n\nmodel_dir = snapshot_download(model_id, revision=revision)\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nif not hasattr(tokenizer, 'model_dir'):\n    tokenizer.model_dir = model_dir\n# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cpu\", trust_remote_code=True).eval()\n# é»˜è®¤gpuè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦24GBæ˜¾å­˜\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True).eval()\n\n# æŒ‡å®šç”Ÿæˆè¶…å‚æ•°ï¼ˆtransformers 4.32.0åŠä»¥ä¸Šæ— éœ€æ‰§è¡Œæ­¤æ“ä½œï¼‰\n# model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)\n\n# ç¬¬ä¸€è½®å¯¹è¯\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)\nprint(response)\n# å›¾ä¸­æ˜¯ä¸€åå¹´è½»å¥³å­åœ¨æ²™æ»©ä¸Šå’Œå¥¹çš„ç‹—ç©è€ï¼Œç‹—çš„å“ç§æ˜¯æ‹‰å¸ƒæ‹‰å¤šã€‚å¥¹ä»¬ååœ¨æ²™æ»©ä¸Šï¼Œç‹—çš„å‰è…¿æŠ¬èµ·æ¥ï¼Œä¸äººäº’åŠ¨ã€‚\n\n# ç¬¬äºŒè½®å¯¹è¯\nresponse, history = model.chat(tokenizer, 'è¾“å‡ºå‡»æŒçš„æ£€æµ‹æ¡†', history=history)\nprint(response)\n# <ref>\"å‡»æŒ\"</ref><box>(211,412),(577,891)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('output_chat.jpg')\nelse:\n  print(\"no box\")\n```\n\n<br>\n\n## é‡åŒ–\n\n### ç”¨æ³•\n\nå½“å‰æˆ‘ä»¬æä¾›äº†åŸºäº[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)çš„é‡åŒ–æ–¹æ¡ˆï¼Œå¹¶æä¾›äº†Qwen-VL-Chatçš„Int4é‡åŒ–ç‰ˆæœ¬Qwen-VL-Chat-Int4 [ç‚¹å‡»æ­¤å¤„](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4)ã€‚è¯¥æ¨¡å‹åœ¨æ•ˆæœè¯„æµ‹ä¸Šå‡ ä¹æ— æŸï¼Œå¹¶åœ¨æ˜¾å­˜å ç”¨å’Œæ¨ç†é€Ÿåº¦ä¸Šå…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚\n\nä¸‹æ–‡è¯´æ˜å¦‚ä½•ä½¿ç”¨è¯¥é‡åŒ–æ¨¡å‹ã€‚å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ æ»¡è¶³è¦æ±‚ï¼ˆå¦‚torch2.0åŠä»¥ä¸Šã€transformers 4.32.0åŠä»¥ä¸Šï¼Œç­‰ï¼‰å¹¶å®‰è£…æ‰€éœ€çš„ä»£ç åº“ï¼š\n\n```bash\npip install optimum\ngit clone https://github.com/JustinLin610/AutoGPTQ.git & cd AutoGPTQ\npip install -v .\n```\n\nå¦‚é‡åˆ°å®‰è£… `auto-gptq` çš„é—®é¢˜ï¼Œå»ºè®®æ‚¨å‰å¾€å®˜æ–¹[repo](https://github.com/PanQiWei/AutoGPTQ) å¯»æ‰¾åˆé€‚çš„wheelã€‚\n\néšåä½ ä¾¿å¯ä»¥æŒ‰ç…§ä¸Šè¿°ç”¨æ³•****ï¼Œè½»æ¾è°ƒç”¨é‡åŒ–æ¨¡å‹ï¼š\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-VL-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)\nprint(response)\n```\n\n### æ•ˆæœè¯„æµ‹\n\næˆ‘ä»¬åˆ—å‡ºä¸åŒç²¾åº¦ä¸‹æ¨¡å‹åœ¨è¯„æµ‹åŸºå‡† **[TouchStone](https://github.com/OFA-Sys/TouchStone)** ä¸Šçš„è¡¨ç°ï¼Œå¹¶å‘ç°é‡åŒ–æ¨¡å‹å¹¶æ²¡æœ‰æ˜¾è‘—æ€§èƒ½æŸå¤±ã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š\n\n| Quantization | ZH         | EN            |\n| ------------ | :--------: | :-----------: | \n| BF16         | 401.2      |    645.2      |\n| Int4         | 386.6      |    651.4      |\n\n### æ¨ç†é€Ÿåº¦\n\næˆ‘ä»¬æµ‹ç®—äº†åœ¨è¾“å…¥ä¸€å¼ å›¾ç‰‡ï¼ˆå³258ä¸ªtokenï¼‰çš„æ¡ä»¶ä¸‹BF16å’ŒInt4çš„æ¨¡å‹ç”Ÿæˆ1792 (2048-258) å’Œ 7934 (8192-258) ä¸ªtokençš„å¹³å‡é€Ÿåº¦ã€‚\n\n| Quantization | Speed (2048 tokens) | Speed (8192 tokens) |\n| ------------ | :-----------------: | :-----------------: |\n| BF16         |        28.87        |        24.32        |\n| Int4         |        37.79        |        34.34        |\n\næ¨ç†é€Ÿåº¦æµ‹ç®—æ˜¯åœ¨å•å¡ A100-SXM4-80G GPUä¸Šè¿è¡Œï¼Œä½¿ç”¨PyTorch 2.0.1åŠCUDA 11.4ã€‚\n\n### GPUæ˜¾å­˜å ç”¨\n\næˆ‘ä»¬è¿˜æµ‹ç®—äº†åœ¨ä¸€å¼ å›¾ç‰‡è¾“å…¥çš„æ¡ä»¶ä¸‹BF16å’ŒInt4æ¨¡å‹ç”Ÿæˆ1792 (2048-258) å’Œ 7934 (8192-258) ä¸ªtokenæ‰€éœ€æ˜¾å­˜ã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š\n\n| Quantization | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |\n| ------------ | :---------------------------------: | :-----------------------------------: |\n| BF16         |               22.60GB               |                28.01GB                |\n| Int4         |               11.82GB               |                17.23GB                |\n\nä¸Šè¿°é€Ÿåº¦å’Œæ˜¾å­˜æµ‹ç®—ä½¿ç”¨[æ­¤è„šæœ¬](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile_mm.py)å®Œæˆã€‚\n<br>\n\n## å¾®è°ƒ\n\næˆ‘ä»¬æä¾›äº†`finetune.py`è¿™ä¸ªè„šæœ¬ä¾›ç”¨æˆ·å®ç°åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„åŠŸèƒ½ï¼Œä»¥æ¥å…¥ä¸‹æ¸¸ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†shellè„šæœ¬å‡å°‘ç”¨æˆ·çš„å·¥ä½œé‡ã€‚è¿™ä¸ªè„šæœ¬æ”¯æŒ [DeepSpeed](https://github.com/microsoft/DeepSpeed) å’Œ [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) ã€‚æˆ‘ä»¬æä¾›çš„shellè„šæœ¬ä½¿ç”¨äº†DeepSpeedï¼Œå› æ­¤å»ºè®®æ‚¨ç¡®ä¿å·²ç»å®‰è£…DeepSpeedã€‚\n\né¦–å…ˆï¼Œä½ éœ€è¦å‡†å¤‡ä½ çš„è®­ç»ƒæ•°æ®ã€‚ä½ éœ€è¦å°†æ‰€æœ‰æ ·æœ¬æ”¾åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­å¹¶å­˜å…¥jsonæ–‡ä»¶ä¸­ã€‚æ¯ä¸ªæ ·æœ¬å¯¹åº”ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«idå’Œconversationï¼Œå…¶ä¸­åè€…ä¸ºä¸€ä¸ªåˆ—è¡¨ã€‚ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼š\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"ä½ å¥½\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"æˆ‘æ˜¯Qwen-VL,ä¸€ä¸ªæ”¯æŒè§†è§‰è¾“å…¥çš„å¤§æ¨¡å‹ã€‚\"\n      }\n    ]\n  },\n  {\n    \"id\": \"identity_1\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>\\nå›¾ä¸­çš„ç‹—æ˜¯ä»€ä¹ˆå“ç§ï¼Ÿ\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"å›¾ä¸­æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚\"\n      },\n      {\n        \"from\": \"user\",\n        \"value\": \"æ¡†å‡ºå›¾ä¸­çš„æ ¼å­è¡¬è¡«\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"<ref>æ ¼å­è¡¬è¡«</ref><box>(588,499),(725,789)</box>\"\n      }\n    ]\n  },\n  { \n    \"id\": \"identity_2\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>assets/mm_tutorial/Chongqing.jpeg</img>\\nPicture 2: <img>assets/mm_tutorial/Beijing.jpeg</img>\\nå›¾ä¸­éƒ½æ˜¯å“ª\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"ç¬¬ä¸€å¼ å›¾ç‰‡æ˜¯é‡åº†çš„åŸå¸‚å¤©é™…çº¿ï¼Œç¬¬äºŒå¼ å›¾ç‰‡æ˜¯åŒ—äº¬çš„å¤©é™…çº¿ã€‚\"\n      }\n    ]\n  }\n]\n```\nä¸ºé’ˆå¯¹å¤šæ ·çš„VLä»»åŠ¡ï¼Œæˆ‘ä»¬å¢åŠ äº†ä¸€ä¸‹çš„ç‰¹æ®Štokensï¼š `<img> </img> <ref> </ref> <box> </box>`.\n\nå¯¹äºå¸¦å›¾åƒè¾“å…¥çš„å†…å®¹å¯è¡¨ç¤ºä¸º `Picture id: <img>img_path</img>\\n{your prompt}`ï¼Œå…¶ä¸­`id`è¡¨ç¤ºå¯¹è¯ä¸­çš„ç¬¬å‡ å¼ å›¾ç‰‡ã€‚\"img_path\"å¯ä»¥æ˜¯æœ¬åœ°çš„å›¾ç‰‡æˆ–ç½‘ç»œåœ°å€ã€‚ \n\nå¯¹è¯ä¸­çš„æ£€æµ‹æ¡†å¯ä»¥è¡¨ç¤ºä¸º`<box>(x1,y1),(x2,y2)</box>`ï¼Œå…¶ä¸­ `(x1, y1)` å’Œ`(x2, y2)`åˆ†åˆ«å¯¹åº”å·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„åæ ‡ï¼Œå¹¶ä¸”è¢«å½’ä¸€åŒ–åˆ°`[0, 1000)`çš„èŒƒå›´å†…. æ£€æµ‹æ¡†å¯¹åº”çš„æ–‡æœ¬æè¿°ä¹Ÿå¯ä»¥é€šè¿‡`<ref>text_caption</ref>`è¡¨ç¤ºã€‚\n\n\nå‡†å¤‡å¥½æ•°æ®åï¼Œä½ å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›çš„shellè„šæœ¬å®ç°å¾®è°ƒã€‚æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šä½ çš„æ•°æ®çš„è·¯å¾„ã€‚\n\nå¾®è°ƒè„šæœ¬èƒ½å¤Ÿå¸®ä½ å®ç°ï¼š\n- å…¨å‚æ•°å¾®è°ƒ\n- LoRA\n- Q-LoRA\n\n### å…¨å‚æ•°å¾®è°ƒ\né»˜è®¤ä¸‹å…¨å‚æ•°å¾®è°ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°LLMæ‰€æœ‰å‚æ•°ã€‚æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œåœ¨å¾®è°ƒé˜¶æ®µä¸æ›´æ–°ViTçš„å‚æ•°ä¼šå–å¾—æ›´å¥½çš„è¡¨ç°ã€‚ä½ å¯ä»¥è¿è¡Œè¿™ä¸ªè„šæœ¬å¼€å§‹è®­ç»ƒï¼š\n\n```bash\n# åˆ†å¸ƒå¼è®­ç»ƒã€‚ç”±äºæ˜¾å­˜é™åˆ¶å°†å¯¼è‡´å•å¡è®­ç»ƒå¤±è´¥ï¼Œæˆ‘ä»¬ä¸æä¾›å•å¡è®­ç»ƒè„šæœ¬ã€‚\nsh finetune/finetune_ds.sh\n```\n\nå°¤å…¶æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹åç§°æˆ–è·¯å¾„ã€æ•°æ®è·¯å¾„ã€ä»¥åŠæ¨¡å‹è¾“å‡ºçš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚å¦‚æœä½ æƒ³ä¿®æ”¹deepspeedé…ç½®ï¼Œå¯ä»¥åˆ é™¤æ‰`--deepspeed`è¿™ä¸ªè¾“å…¥æˆ–è€…è‡ªè¡Œæ ¹æ®éœ€æ±‚ä¿®æ”¹DeepSpeedé…ç½®jsonæ–‡ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼Œå› æ­¤ä½ å¯ä»¥è®¾ç½®`--bf16 True`æˆ–è€…`--fp16 True`ã€‚ç»éªŒä¸Šï¼Œå¦‚æœä½ çš„æœºå™¨æ”¯æŒbf16ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨bf16ï¼Œè¿™æ ·å¯ä»¥å’Œæˆ‘ä»¬çš„é¢„è®­ç»ƒå’Œå¯¹é½è®­ç»ƒä¿æŒä¸€è‡´ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æŠŠé»˜è®¤é…ç½®è®¾ä¸ºå®ƒçš„åŸå› ã€‚\n\n### LoRA\nè¿è¡ŒLoRAçš„æ–¹æ³•ç±»ä¼¼å…¨å‚æ•°å¾®è°ƒã€‚ä½†åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…`peft`ä»£ç åº“ã€‚å¦å¤–ï¼Œè®°ä½è¦è®¾ç½®æ­£ç¡®çš„æ¨¡å‹ã€æ•°æ®å’Œè¾“å‡ºè·¯å¾„ã€‚æˆ‘ä»¬å»ºè®®ä½ ä¸ºæ¨¡å‹è·¯å¾„ä½¿ç”¨ç»å¯¹è·¯å¾„ã€‚è¿™æ˜¯å› ä¸ºLoRAä»…å­˜å‚¨adapteréƒ¨åˆ†å‚æ•°ï¼Œè€Œadapteré…ç½®jsonæ–‡ä»¶è®°å½•äº†é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ï¼Œç”¨äºè¯»å–é¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚åŒæ ·ï¼Œä½ å¯ä»¥è®¾ç½®bf16æˆ–è€…fp16ã€‚\n\n```bash\n# å•å¡è®­ç»ƒ\nsh finetune/finetune_lora_single_gpu.sh\n# åˆ†å¸ƒå¼è®­ç»ƒ\nsh finetune/finetune_lora_ds.sh\n```\n\nä¸å…¨å‚æ•°å¾®è°ƒä¸åŒï¼ŒLoRA ([è®ºæ–‡](https://arxiv.org/abs/2106.09685)) åªæ›´æ–°adapterå±‚çš„å‚æ•°è€Œæ— éœ€æ›´æ–°åŸæœ‰è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚è¿™ç§æ–¹æ³•å…è®¸ç”¨æˆ·ç”¨æ›´ä½çš„æ˜¾å­˜å¼€é”€æ¥è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿæ„å‘³ç€æ›´å°çš„è®¡ç®—å¼€é”€ã€‚\n\næ³¨æ„ï¼Œå¦‚æœä½ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒï¼Œè€Œéchatæ¨¡å‹ï¼Œæ¨¡å‹çš„embeddingå’Œè¾“å‡ºå±‚çš„å‚æ•°å°†è¢«è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ²¡æœ‰å­¦ä¹ è¿‡ChatMLæ ¼å¼ä¸­çš„ç‰¹æ®Štokenï¼Œå› æ­¤éœ€è¦å°†è¿™éƒ¨åˆ†å‚æ•°è®¾ä¸ºå¯è®­ç»ƒæ‰èƒ½è®©æ¨¡å‹å­¦ä¼šç†è§£å’Œé¢„æµ‹è¿™äº›tokenã€‚è¿™ä¹Ÿæ„å‘³ç€ï¼Œå‡å¦‚ä½ çš„è®­ç»ƒå¼•å…¥æ–°çš„ç‰¹æ®Štokenï¼Œä½ éœ€è¦é€šè¿‡ä»£ç ä¸­çš„`modules_to_save`å°†è¿™äº›å‚æ•°è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚å¦‚æœä½ æƒ³èŠ‚çœæ˜¾å­˜å ç”¨ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨chatæ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒï¼Œæ˜¾å­˜å ç”¨å°†å¤§å¹…åº¦é™ä½ã€‚ä¸‹æ–‡çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„è®°å½•å°†è¯¦ç»†ä»‹ç»è¿™éƒ¨åˆ†ç»†èŠ‚ã€‚\n\n### Q-LoRA\nå¦‚æœä½ ä¾ç„¶é‡åˆ°æ˜¾å­˜ä¸è¶³çš„é—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨Q-LoRA ([è®ºæ–‡](https://arxiv.org/abs/2305.14314))ã€‚è¯¥æ–¹æ³•ä½¿ç”¨4æ¯”ç‰¹é‡åŒ–æ¨¡å‹ä»¥åŠpaged attentionç­‰æŠ€æœ¯å®ç°æ›´å°çš„æ˜¾å­˜å¼€é”€ã€‚è¿è¡ŒQ-LoRAä½ åªéœ€è¿è¡Œå¦‚ä¸‹è„šæœ¬ï¼š\n\n```bash\n# å•å¡è®­ç»ƒ\nsh finetune/finetune_qlora_single_gpu.sh\n# åˆ†å¸ƒå¼è®­ç»ƒ\nsh finetune/finetune_qlora_ds.sh\n```\n\næˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨æˆ‘ä»¬æä¾›çš„Int4é‡åŒ–æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå³Qwen-VL-Chat-Int4ã€‚è¯·**ä¸è¦ä½¿ç”¨**éé‡åŒ–æ¨¡å‹ï¼ä¸å…¨å‚æ•°å¾®è°ƒä»¥åŠLoRAä¸åŒï¼ŒQ-LoRAä»…æ”¯æŒfp16ã€‚æ­¤å¤–ï¼Œä¸Šè¿°LoRAå…³äºç‰¹æ®Štokençš„é—®é¢˜åœ¨Q-LoRAä¾ç„¶å­˜åœ¨ã€‚å¹¶ä¸”ï¼ŒInt4æ¨¡å‹çš„å‚æ•°æ— æ³•è¢«è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚æ‰€å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬åªæä¾›äº†Chatæ¨¡å‹çš„Int4æ¨¡å‹ï¼Œå› æ­¤ä½ ä¸ç”¨æ‹…å¿ƒè¿™ä¸ªé—®é¢˜ã€‚ä½†æ˜¯ï¼Œå¦‚æœä½ æ‰§æ„è¦åœ¨Q-LoRAä¸­å¼•å…¥æ–°çš„ç‰¹æ®Štokenï¼Œå¾ˆæŠ±æ­‰ï¼Œæˆ‘ä»¬æ— æ³•ä¿è¯ä½ èƒ½æˆåŠŸè®­ç»ƒã€‚\n\nä¸å…¨å‚æ•°å¾®è°ƒä¸åŒï¼ŒLoRAå’ŒQ-LoRAçš„è®­ç»ƒåªéœ€å­˜å‚¨adapteréƒ¨åˆ†çš„å‚æ•°ã€‚å‡å¦‚ä½ éœ€è¦ä½¿ç”¨LoRAè®­ç»ƒåçš„æ¨¡å‹ï¼Œä½ éœ€è¦ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ã€‚ä½ å¯ä»¥ç”¨å¦‚ä¸‹ä»£ç è¯»å–æ¨¡å‹ï¼š\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\nå¦‚æœä½ è§‰å¾—è¿™æ ·ä¸€æ­¥åˆ°ä½çš„æ–¹å¼è®©ä½ å¾ˆä¸å®‰å¿ƒæˆ–è€…å½±å“ä½ æ¥å…¥ä¸‹æ¸¸åº”ç”¨ï¼Œä½ å¯ä»¥é€‰æ‹©å…ˆåˆå¹¶å¹¶å­˜å‚¨æ¨¡å‹ï¼ˆLoRAæ”¯æŒåˆå¹¶ï¼ŒQ-LoRAä¸æ”¯æŒï¼‰ï¼Œå†ç”¨å¸¸è§„æ–¹å¼è¯»å–ä½ çš„æ–°æ¨¡å‹ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\næ³¨æ„ï¼šåˆ†å¸ƒå¼è®­ç»ƒéœ€è¦æ ¹æ®ä½ çš„éœ€æ±‚å’Œæœºå™¨æŒ‡å®šæ­£ç¡®çš„åˆ†å¸ƒå¼è®­ç»ƒè¶…å‚æ•°ã€‚æ­¤å¤–ï¼Œä½ éœ€è¦æ ¹æ®ä½ çš„æ•°æ®ã€æ˜¾å­˜æƒ…å†µå’Œè®­ç»ƒé€Ÿåº¦é¢„æœŸï¼Œä½¿ç”¨`--model_max_length`è®¾å®šä½ çš„æ•°æ®é•¿åº¦ã€‚\n\n### æ˜¾å­˜å ç”¨åŠè®­ç»ƒé€Ÿåº¦\nä¸‹é¢è®°å½•Qwen_VLæ¨¡å‹åœ¨å•GPUä½¿ç”¨LoRAï¼ˆLoRA (Base)æŒ‡çš„æ˜¯embeddingå’Œè¾“å‡ºå±‚å‚ä¸è®­ç»ƒï¼Œè€ŒLoRA (Chat)åˆ™ä¸ä¼˜åŒ–è¿™éƒ¨åˆ†å‚æ•°ï¼‰å’ŒQLoRAæ—¶å¤„ç†ä¸åŒé•¿åº¦è¾“å…¥çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„æƒ…å†µã€‚æœ¬æ¬¡è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œä½¿ç”¨CUDA 11.8å’ŒPytorch 2.0ã€‚æˆ‘ä»¬ç»Ÿä¸€ä½¿ç”¨batch sizeä¸º1ï¼Œgradient accumulationä¸º8çš„è®­ç»ƒé…ç½®ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ä¸€å¼ å›¾ï¼Œåˆ†åˆ«è®°å½•è¾“å…¥é•¿åº¦åˆ†åˆ«ä¸º384ã€512ã€1024å’Œ2048çš„æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰å’Œè®­ç»ƒé€Ÿåº¦ï¼ˆs/iterï¼‰ã€‚å…·ä½“æ•°å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š\n\n<table>\n    <tr>\n <th rowspan=\"2\">Method</th><th colspan=\"4\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">384</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th>\n    </tr>\n    <tr>\n      <td>LoRA (Base)</td><td align=\"center\">37.1G / 2.3s/it</td><td align=\"center\">37.3G / 2.4s/it</td><td align=\"center\">38.7G / 3.6s/it</td><td align=\"center\">38.7G / 6.1s/it</td>\n    </tr>\n    <tr>\n      <td>LoRA (Chat)</td><td align=\"center\">23.3G / 2.2s/it</td><td align=\"center\">23.6G / 2.3s/it</td><td align=\"center\">25.1G / 3.5s/it</td><td align=\"center\">27.3G / 5.9s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">17.0G / 4.2s/it</td><td align=\"center\">17.2G / 4.5s/it</td><td align=\"center\">18.2G / 5.5s/it</td><td align=\"center\">19.3G / 7.9s/it</td>\n    </tr>\n\n</table>\n\n<br><br>\n## Demo\n\n### Web UI\n\næˆ‘ä»¬æä¾›äº†Web UIçš„demoä¾›ç”¨æˆ·ä½¿ç”¨ã€‚åœ¨å¼€å§‹å‰ï¼Œç¡®ä¿å·²ç»å®‰è£…å¦‚ä¸‹ä»£ç åº“ï¼š\n\n```\npip install -r requirements_web_demo.txt\n```\n\néšåè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¹¶ç‚¹å‡»ç”Ÿæˆé“¾æ¥ï¼š\n\n```\npython web_demo_mm.py\n```\n\n<br>\n\n## FAQ\n\nå¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜… [FAQ](FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚\n<br>\n\n## ä½¿ç”¨åè®®\n\nç ”ç©¶äººå‘˜ä¸å¼€å‘è€…å¯ä½¿ç”¨Qwen-VLå’ŒQwen-VL-Chatæˆ–è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚æˆ‘ä»¬åŒæ ·å…è®¸å•†ä¸šä½¿ç”¨ï¼Œå…·ä½“ç»†èŠ‚è¯·æŸ¥çœ‹[LICENSE](LICENSE)ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·å¡«å†™[é—®å·](https://dashscope.console.aliyun.com/openModelApply/qianwen)ç”³è¯·ã€‚\n<br>\n\n## å¼•ç”¨\n\nå¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„è®ºæ–‡å’Œä»£ç å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘:star: å’Œå¼•ç”¨ :pencil: :)\n\n```BibTeX\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n<br>\n\n## è”ç³»æˆ‘ä»¬\n\nå¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œè¯·é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚\n\n"
        },
        {
          "name": "README_JA.md",
          "type": "blob",
          "size": 42.5849609375,
          "content": "<p align=\"left\">\n        <a href=\"README_CN.md\">ä¸­æ–‡</a>&nbsp ï½œ &nbsp <a href=\"README.md\">English</a>&nbsp ï½œ &nbspæ—¥æœ¬èª&nbsp\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"assets/logo.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        Qwen-VL <a href=\"https://modelscope.cn/models/qwen/Qwen-VL/summary\">ğŸ¤– <a> | <a href=\"https://huggingface.co/Qwen/Qwen-VL\">ğŸ¤—</a>&nbsp ï½œ Qwen-VL-Chat <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary\">ğŸ¤– <a>| <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat\">ğŸ¤—</a>&nbsp ï½œ Qwen-VL-Chat-Int4 <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat-Int4\">ğŸ¤—</a>\n<br>\n<a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary\">Demo</a>&nbsp ï½œ &nbsp<a href=\"https://arxiv.org/abs/2308.12966\">Paper</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://github.com/camenduru/Qwen-VL-Chat-colab\">Colab</a>&nbsp&nbsp | &nbsp <a href=\"TUTORIAL_ja.md\">Tutorial</a>\n</p>\n<br><br>\n<p align=\"left\">\n        æ—¥æœ¬èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ãƒ³ãƒ†ãƒŠãƒ¼: <a href=\"https://github.com/eltociear\">Ikko Eltociear Ashimine</a>\n</p>\n<br>\n\n**Qwen-VL** ï¼ˆQwen Large Vision Language Modelï¼‰ã¯ã€ã‚¢ãƒªãƒãƒã‚¯ãƒ©ã‚¦ãƒ‰ãŒæå”±ã™ã‚‹ãƒ©ãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚º Qwenï¼ˆç•¥ç§°: Tongyi Qianwenï¼‰ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç‰ˆã§ã™ã€‚Qwen-VL ã¯ã€ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆã€ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘ä»˜ã‘ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚Qwen-VL ã®ç‰¹å¾´ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:\n\n- **å¥½èª¿ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: è¤‡æ•°ã®è‹±èªè©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆZero-shot Captioningã€VQAã€DocVQAã€Grounding ã‚’å«ã‚€ï¼‰ã«ãŠã„ã¦ã€åŒæ§˜ã®ãƒ¢ãƒ‡ãƒ«è¦æ¨¡ã§ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚ŒãŸæ—¢å­˜ã®å¤§è¦æ¨¡ãƒ“ã‚¸ãƒ§ãƒ³è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLVLMï¼‰ã‚’å¤§å¹…ã«ä¸Šå›ã‚Šã¾ã™ã€‚\n- **ãƒ†ã‚­ã‚¹ãƒˆèªè­˜ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å¤šè¨€èª LVLM**: Qwen-VL ã¯ã€è‹±èªã€ä¸­å›½èªã€å¤šè¨€èªã®ä¼šè©±ã‚’è‡ªç„¶ã«ã‚µãƒãƒ¼ãƒˆã—ã€ç”»åƒå†…ã®ä¸­å›½èªã¨è‹±èªã®äºŒè¨€èªãƒ†ã‚­ã‚¹ãƒˆã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®èªè­˜ã‚’ä¿ƒé€²ã—ã¾ã™ã€‚\n- **è¤‡æ•°ç”»åƒã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–ä¼šè©±**: ã“ã®æ©Ÿèƒ½ã«ã‚ˆã‚Šã€è¤‡æ•°ã®ç”»åƒã‚’å…¥åŠ›ã—ã€æ¯”è¼ƒã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ã¾ãŸã€ç”»åƒã«é–¢é€£ã™ã‚‹è³ªå•ã‚’æŒ‡å®šã—ã€è¤‡æ•°ã®ç”»åƒã«ã‚ˆã‚‹ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ãƒªãƒ³ã‚°ã‚’è¡Œã†ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n- **ä¸­å›½èªã®ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ”¯ãˆã‚‹åˆã®ã‚¸ã‚§ãƒãƒ©ãƒªã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«**: ä¸­å›½èªã¨è‹±èªã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‰ãƒ¡ã‚¤ãƒ³è¨€èªè¡¨ç¾ã«ã‚ˆã‚‹ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®æ¤œå‡ºã€‚\n- **ãã‚ç´°ã‚„ã‹ãªèªè­˜ã¨ç†è§£**: ç¾åœ¨ä»–ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ LVLM ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ 224\\*224 ã®è§£åƒåº¦ã¨æ¯”è¼ƒã—ã¦ã€448\\*448 ã®è§£åƒåº¦ã¯ã€ãã‚ç´°ã‹ã„ãƒ†ã‚­ã‚¹ãƒˆèªè­˜ã€æ–‡æ›¸ QAã€ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹æ³¨é‡ˆã‚’ä¿ƒé€²ã™ã‚‹ã€‚\n\n<br>\n<p align=\"center\">\n    <img src=\"assets/demo_vl.gif\" width=\"400\"/>\n<p>\n<br>\n\nQwen-VL ã‚·ãƒªãƒ¼ã‚ºã® 2 ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã¾ã™:\n\n- Qwen-VL: LLM ã®åˆæœŸåŒ–ã« Qwen-7B ã‚’ã€è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®åˆæœŸåŒ–ã« [Openclip ViT-bigG](https://github.com/mlfoundations/open_clip) ã‚’ç”¨ã„ãŸå­¦ç¿’æ¸ˆã¿ LVLM ãƒ¢ãƒ‡ãƒ«ã€‚ãã—ã¦ã€ãã‚Œã‚‰ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚ŒãŸã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§æ¥ç¶šã™ã‚‹ã€‚\n- Qwen-VL-Chat: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãª LLM ãƒ™ãƒ¼ã‚¹ã® AI ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€‚Qwen-VL-Chat ã¯ã€è¤‡æ•°ã®ç”»åƒå…¥åŠ›ã€è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®è³ªå•å¿œç­”ã€ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªæ©Ÿèƒ½ãªã©ã€ã‚ˆã‚ŠæŸ”è»Ÿãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\n  <br>\n\n## ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ\n* 2023.11.28 Qwen-VL ã¯ã€GPT4Vã€PALI-X ã‚’å‡Œé§•ã™ã‚‹æœ€é«˜ãƒ¬ãƒ™ãƒ«ã® [DOCVQA](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1) ã‚’ã‚·ãƒ³ã‚°ãƒ«ãƒ¢ãƒ‡ãƒ«ã§é”æˆã—ã€ç›´æ¥ç”»åƒã‚’å…¥åŠ›ã™ã‚‹ã ã‘ã§æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã‚’åˆ†æç†è§£ã§ãã‚‹æ±ç”¨ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€‚ https://qianwen.aliyun.com ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¿ãƒ–ã§ç›´æ¥æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½“é¨“ã§ãã¾ã™ã€‚\n* 2023.9.25 Qwen-VL-Chat ãƒ¢ãƒ‡ãƒ«ãŒæ›´æ–°ã•ã‚Œã€ä¸­å›½èªã‚³ãƒãƒ³ãƒ‰ã®ãƒ•ã‚©ãƒ­ãƒ¼ãŒã‚ˆã‚Šå …ç‰¢ã«ãªã‚Šã€Web ãƒšãƒ¼ã‚¸ã¨è¡¨ã®ç”»åƒã®ç†è§£ã¨è³ªå•ã¨å›ç­”ã®æ©Ÿèƒ½ãŒå‘ä¸Šã—ã€å¯¾è©±ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã—ã¾ã—ãŸ (ã‚¿ãƒƒãƒã‚¹ãƒˆãƒ¼ãƒ³: ä¸­å›½èª: 401.2->481.7ã€è‹±èª: 645.2->711.6)ã€‚\n* 2023.9.12 ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¾®èª¿æ•´ã€LoRAã€Q-LoRA ã‚’å«ã‚€ã€Qwen-VL ãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n* 2023.9.8 [Colab](https://github.com/camenduru/Qwen-VL-Chat-câ€‹â€‹olab) ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’æä¾›ã—ã¦ãã‚ŒãŸ [camenduru](https://github.com/camenduru) ã«æ„Ÿè¬ã—ã¾ã™ã€‚ã“ã‚Œã‚’ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¨ã—ã¦ä½¿ç”¨ã—ã¦ã€12G GPU ã§ãƒ­ãƒ¼ã‚«ãƒ«ã¾ãŸã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã®ãƒ‡ãƒ¢ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚\n* 2023.9.4 Qwen-VL ã‚·ãƒªãƒ¼ã‚ºã¯ã€ç”»åƒã¨ãƒ“ãƒ‡ã‚ªã®ä¸¡æ–¹ã®ç†è§£ã‚’å«ã‚€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« LLM ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®ã€æ­£ç¢ºãªäººã«ã‚ˆã‚‹æ³¨é‡ˆã‚’å‚™ãˆãŸ 19,000 å€‹ã®å¤šè‚¢é¸æŠè³ªå•ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã‚ã‚‹ [Seed-Bench](eval_mm/seed_bench/EVAL_SEED.md) ã§ SOTA ã‚’é”æˆã—ã¾ã™ã€‚\n* 2023.9.1 åŸºæœ¬çš„ãªèªè­˜ã¨ç†è§£ã ã‘ã§ãªãã€æ–‡å­¦å‰µä½œã¾ã§ã‚’å«ã‚€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ã®åŒ…æ‹¬çš„ãªè©•ä¾¡ã§ã‚ã‚‹ [TouchStone](https://github.com/OFA-Sys/TouchStone) è©•ä¾¡ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã™ ã€‚ å¼·åŠ›ãª LLM ã‚’åˆ¤å®šè€…ã¨ã—ã¦ä½¿ç”¨ã—ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªæƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚\n* 2023.8.31 ä½ãƒ¡ãƒ¢ãƒªã‚³ã‚¹ãƒˆã§ã‚ã‚ŠãªãŒã‚‰æ¨è«–é€Ÿåº¦ã®å‘ä¸Šã‚’å®Ÿç¾ã™ã‚‹ Qwen-VL-Chat ç”¨ã® Int4 é‡å­åŒ–ãƒ¢ãƒ‡ãƒ« **Qwen-VL-Chat-Int4** ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚ ã¾ãŸã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡ã«ãŠã„ã¦ã‚‚å¤§ããªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ä½ä¸‹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n* 2023.8.22 ModelScope ã¨ Hugging Face ã§ **Qwen-VL** ã¨ **Qwen-VL-Chat** ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚ ã¾ãŸã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®è©³ç´°ã‚„ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãªã©ã€ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã«ã¤ã„ã¦ã¯ [è«–æ–‡](https://arxiv.org/abs/2308.12966) ã‚‚æä¾›ã—ã¦ã„ã¾ã™ã€‚\n\n## è©•ä¾¡\n\nãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’2ã¤ã®è¦³ç‚¹ã‹ã‚‰è©•ä¾¡ã—ã¾ã—ãŸ:\n\n1. **æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚¿ã‚¹ã‚¯ã® 4 ã¤ã®ä¸»è¦ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«ã¤ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬çš„ãªã‚¿ã‚¹ã‚¯èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹:\n   \n   - ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³: æœªè¦‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹;\n   - ä¸€èˆ¬çš„ãª VQA: åˆ¤å®šã€è‰²ã€æ•°ã€ã‚«ãƒ†ã‚´ãƒªãªã©ã€ç”»åƒã®ä¸€èˆ¬çš„ãªè³ªå•å¿œç­”èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹;\n   - ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ VQA: æ–‡æ›¸ QAã€å›³è¡¨ QAãªã©ã€å†™çœŸå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’èªè­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹;\n   - å‚ç…§è¡¨ç¾ç†è§£: å‚ç…§è¡¨ç¾ç†è§£: å‚ç…§è¡¨ç¾ã§è¨˜è¿°ã•ã‚ŒãŸç”»åƒå†…ã®å¯¾è±¡ç‰©ã‚’ç‰¹å®šã™ã‚‹èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ã€‚\n2. **TouchStone**: ç·åˆçš„ãªãƒ†ã‚­ã‚¹ãƒˆç”»åƒå¯¾è©±èƒ½åŠ›ã¨äººé–“ã¨ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€GPT4 ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã«åŸºã¥ã TouchStone ã¨å‘¼ã°ã‚Œã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã€LVLM ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚\n   \n   - TouchStone ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã€åˆè¨ˆ 300 ä»¥ä¸Šã®ç”»åƒã€800 ä»¥ä¸Šã®è³ªå•ã€27 ã®ã‚«ãƒ†ã‚´ãƒªã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚ä¾‹ãˆã°ã€å±æ€§ãƒ™ãƒ¼ã‚¹ã® Q&Aã€æœ‰åäººã®èªè­˜ã€è©©ã®ä½œæ–‡ã€è¤‡æ•°ã®ç”»åƒã®è¦ç´„ã€å•†å“æ¯”è¼ƒã€æ•°å­¦ã®å•é¡Œè§£æ±ºãªã©ã§ã™;\n   - ç”»åƒã®ç›´æ¥å…¥åŠ›ã¨ã„ã† GPT4 ã®ç¾åœ¨ã®åˆ¶é™ã‚’æ‰“ã¡ç ´ã‚‹ãŸã‚ã€TouchStone ã¯äººé–“ã®ãƒ©ãƒ™ãƒ«ä»˜ã‘ã«ã‚ˆã‚‹ãã‚ç´°ã‹ã„ç”»åƒæ³¨é‡ˆã‚’æä¾›ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®è©³ç´°ãªæ³¨é‡ˆã¯ã€è³ªå•ã¨ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã¨å…±ã«ã€æ¡ç‚¹ã®ãŸã‚ã« GPT4 ã«æç¤ºã•ã‚Œã¾ã™ã€‚\n   - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ã¯è‹±èªç‰ˆã¨ä¸­å›½èªç‰ˆãŒã‚ã‚Šã¾ã™ã€‚\n\nè©•ä¾¡çµæœã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™:\n\nQwen-VL ã¯ã€è¤‡æ•°ã® VL ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€ç¾è¡Œã® SOTA ã‚¸ã‚§ãƒãƒ©ãƒªã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚Šã€ã¾ãŸã€èƒ½åŠ›ç¯„å›²ã®ç‚¹ã§ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’æŒã¡ã¾ã™ã€‚\n\n<p align=\"center\">\n    <img src=\"assets/radar.png\" width=\"600\"/>\n<p>\n\n### ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã¨ä¸€èˆ¬çš„ãª VQA\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"2\">Zero-shot Captioning</th>\n    <th colspan=\"5\">General VQA</th>\n  </tr>\n  <tr>\n    <th>NoCaps</th>\n    <th>Flickr30K</th>\n    <th>VQAv2<sup>dev</sup></th>\n    <th>OK-VQA</th>\n    <th>GQA</th>\n    <th>SciQA-Img<br>(0-shot)</th>\n    <th>VizWiz<br>(0-shot)</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"10\">Generalist<br>Models</td>\n    <td>Flamingo-9B</td>\n    <td>-</td>\n    <td>61.5</td>\n    <td>51.8</td>\n    <td>44.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>28.8</td>\n  </tr>\n  <tr>\n    <td>Flamingo-80B</td>\n    <td>-</td>\n    <td>67.2</td>\n    <td>56.3</td>\n    <td>50.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>31.6</td>\n  </tr>\n  <tr>\n    <td>Unified-IO-XL</td>\n    <td>100.0</td>\n    <td>-</td>\n    <td>77.9</td>\n    <td>54.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Kosmos-1</td>\n    <td>-</td>\n    <td>67.1</td>\n    <td>51.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>29.2</td>\n  </tr>\n  <tr>\n    <td>Kosmos-2</td>\n    <td>-</td>\n    <td>80.5</td>\n    <td>51.1</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>103.9</td>\n    <td>71.6</td>\n    <td>65.0</td>\n    <td>45.9</td>\n    <td>32.3</td>\n    <td>61.0</td>\n    <td>19.6</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td><strong>121.9</strong></td>\n    <td>82.8</td>\n    <td>-</td>\n    <td>-</td>\n    <td>49.5</td>\n    <td>63.1</td>\n    <td>33.4</td>\n  </tr>\n  <tr>\n    <td>Shikra (Vicuna-13B)</td>\n    <td>-</td>\n    <td>73.9</td>\n    <td>77.36</td>\n    <td>47.16</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td><strong>Qwen-VL (Qwen-7B)</strong></td>\n    <td>121.4</td>\n    <td><b>85.8</b></td>\n    <td><b>78.8</b></td>\n    <td><b>58.6</b></td>\n    <td><b>59.3</b></td>\n    <td>67.1</td>\n    <td>35.2</td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>63.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>39.1</td>\n  </tr> -->\n  <tr>\n    <td>Qwen-VL-Chat</td>\n    <td>120.2</td>\n    <td>81.0</td>\n    <td>78.2</td>\n    <td>56.6</td>\n    <td>57.5</td>\n    <td><b>68.2</b></td>\n    <td><b>38.9</b></td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL-Chat (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>60.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>44.45</td>\n  </tr> -->\n  <tr>\n    <td>Previous SOTA<br>(Per Task Fine-tuning)</td>\n    <td>-</td>\n    <td>127.0<br>(PALI-17B)</td>\n    <td>84.5<br>(InstructBLIP<br>-FlanT5-XL)</td>\n    <td>86.1<br>(PALI-X<br>-55B)</td>\n    <td>66.1<br>(PALI-X<br>-55B)</td>\n    <td>72.1<br>(CFR)</td>\n    <td>92.53<br>(LLaVa+<br>GPT-4)</td>\n    <td>70.9<br>(PALI-X<br>-55B)</td>\n  </tr>\n</tbody>\n</table>\n\n- ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ä»˜ã‘ã§ã¯ã€Qwen-VL ã¯ Flickr30K ã§ **SOTA** ã‚’é”æˆã—ã€InstructBlip ã‚’ä½¿ç”¨ã—ãŸ Nocaps ã§ã‚‚ç«¶äº‰åŠ›ã®ã‚ã‚‹çµæœã‚’å¾—ã¦ã„ã¾ã™ã€‚\n- ä¸€èˆ¬çš„ãª VQA ã§ã¯ã€Qwen-VL ã¯åŒã˜ä¸€èˆ¬çš„ãª LVLM ã‚¹ã‚±ãƒ¼ãƒ«è¨­å®šã§ **SOTA** ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚\n\n### ãƒ†ã‚­ã‚¹ãƒˆæŒ‡å‘VQAï¼ˆç”»åƒä¸­ã®ãƒ†ã‚­ã‚¹ãƒˆç†è§£èƒ½åŠ›ã«é‡ç‚¹ã‚’ç½®ãï¼‰\n\n<table>\n<thead>\n  <tr>\n    <th>Model type</th>\n    <th>Model</th>\n    <th>TextVQA</th>\n    <th>DocVQA</th>\n    <th>ChartQA</th>\n    <th>AI2D</th>\n    <th>OCR-VQA</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"5\">Generalist Models</td>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>42.4</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td>50.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>mPLUG-DocOwl (LLaMA-7B)</td>\n    <td>52.6</td>\n    <td>62.2</td>\n    <td>57.4</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Pix2Struct-Large (1.3B)</td>\n    <td>-</td>\n    <td><b>76.6</b></td>\n    <td>58.6</td>\n    <td>42.1</td>\n    <td>71.3</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL (Qwen-7B)</td>\n    <td><b>63.8</b></td>\n    <td>65.1</td>\n    <td><b>65.7</b></td>\n    <td><b>62.3</b></td>\n    <td><b>75.7</b></td>\n  </tr>\n  <tr>\n    <td>Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>PALI-X-55B (Single-task FT)<br>(Without OCR Pipeline)</td>\n    <td>71.44</td>\n    <td>80.0</td>\n    <td>70.0</td>\n    <td>81.2</td>\n    <td>75.0</td>\n  </tr>\n</tbody>\n</table>\n\n- ãƒ†ã‚­ã‚¹ãƒˆé–¢é€£ã®èªè­˜/QA è©•ä¾¡ã«ãŠã„ã¦ã€Qwen-VL ã¯æ±ç”¨ã® LVLM ã‚¹ã‚±ãƒ¼ãƒ«è¨­å®šã§ SOTA ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚\n- è§£åƒåº¦ã¯ä¸Šè¨˜ã®ã„ãã¤ã‹ã®è©•ä¾¡ã«ãŠã„ã¦é‡è¦ã§ã‚ã‚‹ã€‚è§£åƒåº¦ãŒ 224 ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã® LVLM ãƒ¢ãƒ‡ãƒ«ã®å¤šãã¯ã€ã“ã‚Œã‚‰ã®è©•ä¾¡ãŒã§ããªã„ã‹ã€ç”»åƒã‚’ã‚«ãƒƒãƒˆã™ã‚‹ã“ã¨ã§ã—ã‹è§£æ±ºã§ããªã„ãŒã€Qwen-VL ã¯è§£åƒåº¦ã‚’ 448 ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§è©•ä¾¡ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚Qwen-VL ã¯ã€ä¸€éƒ¨ã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€è§£åƒåº¦ 1024 ã® Pix2Struct-Large ãƒ¢ãƒ‡ãƒ«ã‚’ã‚‚å‡Œé§•ã—ã¦ã„ã¾ã™ã€‚\n\n### è¡¨ç¾ç†è§£ã®å‚ç…§\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"3\">RefCOCO</th>\n    <th colspan=\"3\">RefCOCO+</th>\n    <th colspan=\"2\">RefCOCOg</th>\n    <th>GRIT</th>\n  </tr>\n  <tr>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val-u</th>\n    <th>test-u</th>\n    <th>refexp</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"8\">Generalist Models</td>\n    <td>GPV-2</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>51.50</td>\n  </tr>\n  <tr>\n    <td>OFA-L*</td>\n    <td>79.96</td>\n    <td>83.67</td>\n    <td>76.39</td>\n    <td>68.29</td>\n    <td>76.00</td>\n    <td>61.75</td>\n    <td>67.57</td>\n    <td>67.58</td>\n    <td>61.70</td>\n  </tr>\n  <tr>\n    <td>Unified-IO</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td><b>78.61</b></td>\n  </tr>\n  <tr>\n    <td>VisionLLM-H</td>\n    <td></td>\n    <td>86.70</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Shikra-7B</td>\n    <td>87.01</td>\n    <td>90.61</td>\n    <td>80.24 </td>\n    <td>81.60</td>\n    <td>87.36</td>\n    <td>72.12</td>\n    <td>82.27</td>\n    <td>82.19</td>\n    <td>69.34</td>\n  </tr>\n  <tr>\n    <td>Shikra-13B</td>\n    <td>87.83 </td>\n    <td>91.11</td>\n    <td>81.81</td>\n    <td>82.89</td>\n    <td>87.79</td>\n    <td>74.41</td>\n    <td>82.64</td>\n    <td>83.16</td>\n    <td>69.03</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B</td>\n    <td><b>89.36</b></td>\n    <td>92.26</td>\n    <td><b>85.34</b></td>\n    <td><b>83.12</b></td>\n    <td>88.25</td>\n    <td><b>77.21</b></td>\n    <td>85.58</td>\n    <td>85.48</td>\n    <td>78.22</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B-Chat</td>\n    <td>88.55</td>\n    <td><b>92.27</b></td>\n    <td>84.51</td>\n    <td>82.82</td>\n    <td><b>88.59</b></td>\n    <td>76.79</td>\n    <td><b>85.96</b></td>\n    <td><b>86.32</b></td>\n    <td>-</td>\n  <tr>\n    <td rowspan=\"3\">Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>G-DINO-L</td>\n    <td>90.56&nbsp;&nbsp;</td>\n    <td>93.19</td>\n    <td>88.24</td>\n    <td>82.75</td>\n    <td>88.95</td>\n    <td>75.92</td>\n    <td>86.13</td>\n    <td>87.02</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>UNINEXT-H</td>\n    <td>92.64 </td>\n    <td>94.33</td>\n    <td>91.46</td>\n    <td>85.24</td>\n    <td>89.63</td>\n    <td>79.79</td>\n    <td>88.73</td>\n    <td>89.37</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>ONE-PEACE</td>\n    <td>92.58 </td>\n    <td>94.18</td>\n    <td>89.26</td>\n    <td>88.77</td>\n    <td>92.21</td>\n    <td>83.23</td>\n    <td>89.22</td>\n    <td>89.27</td>\n    <td>-</td>\n  </tr>\n</tbody>\n</table>\n\n- Qwen-VL ã¯ã€ä¸Šè¨˜ã®ã™ã¹ã¦ã®å‚ç…§è¡¨ç¾ç†è§£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ **SOTA** ã‚’é”æˆã—ãŸã€‚\n- Qwen-VL ã¯ä¸­å›½èªã®ä¸‹åœ°ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã—ã¦ã„ãªã„ãŒã€ä¸­å›½èªã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã¨è‹±èªã®ä¸‹åœ°ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§ä¸­å›½èªã®ä¸‹åœ°ã‚¿ã‚¹ã‚¯ã«æ±åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n\nç§ãŸã¡ã®å®Ÿé¨“çµæœã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€ä¸Šè¨˜ã®è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã™ã¹ã¦æä¾›ã—ã¦ã„ã¾ã™ã€‚è©³ã—ãã¯ [eval_mm/EVALUATION.md](eval_mm/EVALUATION.md) ã‚’ãŠèª­ã¿ãã ã•ã„ã€‚\n\n### ãƒãƒ£ãƒƒãƒˆè©•ä¾¡\n\nTouchStone ã¯ GPT4 ã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã«åŸºã¥ããƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®å¯¾è©±ãŠã‚ˆã³äººé–“ã¨ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã«ãŠã‘ã‚‹ LVLM ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ã€‚åˆè¨ˆ 300 ä»¥ä¸Šã®ç”»åƒã€800 ä»¥ä¸Šã®è³ªå•ã€å±æ€§ãƒ™ãƒ¼ã‚¹ã® Q&Aã€æœ‰åäººã®èªè­˜ã€è©©ã®ä½œæˆã€è¤‡æ•°ã®ç”»åƒã®è¦ç´„ã€å•†å“æ¯”è¼ƒã€æ•°å­¦ã®å•é¡Œè§£æ±ºãªã©27ã®ã‚«ãƒ†ã‚´ãƒªã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚è©³ã—ãã¯ [touchstone/README_JA.md](touchstone/README_JA.md) ã‚’ãŠèª­ã¿ãã ã•ã„ã€‚\n\n#### è‹±èª\n\n| Model            | Score |\n| ---------------- | ----- |\n| PandaGPT         | 488.5 |\n| MiniGPT4         | 531.7 |\n| InstructBLIP     | 552.4 |\n| LLaMA-AdapterV2  | 590.1 |\n| LLaVA            | 602.7 |\n| mPLUG-Owl        | 605.4 |\n| Qwen-VL-Chat     | 645.2 |\n| Qwen-VL-Chat-1.1 | 711.6 |\n\n#### ä¸­å›½èª\n\n| Model            | Score |\n| ---------------- | ----- |\n| VisualGLM        | 247.1 |\n| Qwen-VL-Chat     | 401.2 |\n| Qwen-VL-Chat-1.1 | 481.7 |\n\nQwen-VL-Chat ã¯ä¸­å›½èªã¨è‹±èªã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆè©•ä¾¡ã§æœ€é«˜ã®çµæœã‚’å¾—ã¾ã—ãŸã€‚\n<br>\n\n### ãã®ä»–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯\n\n#### SEED-Bench\n\nSEED-Bench ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« LLM ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®æ­£ç¢ºãªäººã«ã‚ˆã‚‹æ³¨é‡ˆã‚’å‚™ãˆãŸ 19,000 å€‹ã®å¤šè‚¢é¸æŠå¼è³ªå•ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã€**ç”»åƒ** ã¨ **ãƒ“ãƒ‡ã‚ª** ã®ä¸¡æ–¹ã®ç†è§£ã‚’å«ã‚€ 12 ã®è©•ä¾¡æ¬¡å…ƒã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚ è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ã“ã¡ã‚‰](eval_mm/seed_bench/EVAL_SEED.md) ã‚’ã”è¦§ãã ã•ã„ã€‚\n\nQwen-VL ã¨ Qwen-VL-Chat ã¯ã€ã“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ SOTA ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚\n\n<p align=\"center\">\n    <img src=\"eval_mm/seed_bench/leaderboard.jpg\"/>\n<p>\n\n## å¿…è¦æ¡ä»¶\n\n* python 3.8 ä»¥ä¸Š\n* pytorch 1.12 ä»¥ä¸Šã€2.0 ä»¥ä¸Šã‚’æ¨å¥¨\n* CUDA 11.4 ä»¥ä¸Šã‚’æ¨å¥¨ï¼ˆGPU ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã§ã™ï¼‰\n  <br>\n\n## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ\n\nä»¥ä¸‹ã§ã¯ã€Qwen-VL ã¨ Qwen-VL-Chat ã‚’ ğŸ¤– ModelScope ã¨ ğŸ¤— Transformers ã¨ã¨ã‚‚ã«ä½¿ã†æ–¹æ³•ã‚’ã€ç°¡å˜ãªä¾‹ã§ç¤ºã—ã¾ã™ã€‚\n\nã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ã€ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒæ¸ˆã‚“ã§ã„ã‚‹ã“ã¨ã‚’ ç¢ºèªã—ã¦ãã ã•ã„ã€‚ä¸Šè¨˜ã®è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ã€ä¾å­˜ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚\n\n```bash\npip install -r requirements.txt\n```\n\nã“ã‚Œã§ ModelScope ã‚„ Transformers ã‚’ä½¿ã„å§‹ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãƒ“ã‚¸ãƒ§ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ã¤ã„ã¦ã®è©³ã—ã„ä½¿ã„æ–¹ã¯ã€[ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](TUTORIAL_ja.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n#### ğŸ¤— Transformers\n\nQwen-VL-Chat ã‚’æ¨è«–ã«ä½¿ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ãªã®ã¯ã€ä»¥ä¸‹ã«ç¤ºã™æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã™ã‚‹ã“ã¨ã ã‘ã§ã™ã€‚ãŸã ã—ã€**æœ€æ–°ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\n# Note: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‹•ä½œã§ã¯ã€ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³æ”»æ’ƒé˜²æ­¢æ©Ÿèƒ½ãŒã‚ªãƒ•ã«ãªã‚Šã¾ã—ãŸã€‚\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# bf16 ã®ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# fp16 ã®ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# cpu ã®ã¿ã®ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# cuda ãƒ‡ãƒã‚¤ã‚¹ã®ä½¿ç”¨\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# ç”Ÿæˆã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŒ‡å®š\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# ç¬¬ 1 å› å¯¾è©±ã‚¿ãƒ¼ãƒ³\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯ url\n    {'text': 'è¿™æ˜¯ä»€ä¹ˆ?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n# å†™çœŸã¯ãƒ“ãƒ¼ãƒã§ãƒ©ãƒ–ãƒ©ãƒ‰ãƒ¼ãƒ«ã®éš£ã§æ„›çŠ¬ã¨æˆ¯ã‚Œã‚‹å¥³æ€§ãŒå†™ã£ã¦ãŠã‚Šã€å½¼ã‚‰ã¯ç ‚ã®ä¸­ã«ã„ã‚‹ã€‚\n\n# ç¬¬ 2 å› å¯¾è©±ã‚¿ãƒ¼ãƒ³\nresponse, history = model.chat(tokenizer, 'æ¡†å‡ºå›¾ä¸­å‡»æŒçš„ä½ç½®', history=history)\nprint(response)\n# <ref>å‡»æŒ</ref><box>(536,509),(588,602)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('1.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n\n<details>\n  <summary>Running Qwen-VL</summary>\n\nRunning Qwen-VL pretrained base model is also simple.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\n# bf16 ã®ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# fp16 ã®ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# cpu ã®ã¿ã®ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cpu\", trust_remote_code=True).eval()\n# cuda ãƒ‡ãƒã‚¤ã‚¹ã®ä½¿ç”¨\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# ç”Ÿæˆã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŒ‡å®š\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯ url\n    {'text': 'Generate the caption in English with grounding:'},\n])\ninputs = tokenizer(query, return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nresponse = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)\nprint(response)\n# <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>Generate the caption in English with grounding:<ref> Woman</ref><box>(451,379),(731,806)</box> and<ref> her dog</ref><box>(219,424),(576,896)</box> playing on the beach<|endoftext|>\nimage = tokenizer.draw_bbox_on_latest_picture(response)\nif image:\n  image.save('2.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_spotting_caption.jpg\" width=\"500\"/>\n<p>\n\n</details>\n\nHuggingFaceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ã‚³ãƒ¼ãƒ‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã€ModelScopeã‹ã‚‰ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã¯ã“ã¡ã‚‰ã§ã”ã–ã„ã¾ã™ã€‚\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-VL')\nmodel_dir = snapshot_download('qwen/Qwen-VL-Chat')\n\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"cuda\",\n    trust_remote_code=True\n).eval()\n```\n\n#### ğŸ¤– ModelScope\n\nModelScope ã¯ã€MaaSï¼ˆModel-as-a-Serviceï¼‰ã®ãŸã‚ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã‚ã‚Šã€AI é–‹ç™ºè€…ã«æŸ”è»Ÿã§è²»ç”¨å¯¾åŠ¹æœã®é«˜ã„ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚åŒæ§˜ã«ã€ä»¥ä¸‹ã®ã‚ˆã†ã« ModelScope ã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™:\n\n```python\nfrom modelscope import (\n    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n)\nimport torch\nmodel_id = 'qwen/Qwen-VL-Chat'\nrevision = 'v1.0.0'\n\nmodel_dir = snapshot_download(model_id, revision=revision)\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nif not hasattr(tokenizer, 'model_dir'):\n    tokenizer.model_dir = model_dir\n# bf16 ã®ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# fp16 ã®ä½¿ç”¨\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# cpu ã®ä½¿ç”¨\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cpu\", trust_remote_code=True).eval()\n# auto ã®ä½¿ç”¨\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True).eval()\n\n# ç”Ÿæˆã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŒ‡å®š\nmodel.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)\n\n# ç¬¬ 1 å› å¯¾è©±ã‚¿ãƒ¼ãƒ³\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)\nprint(response)\n# å†™çœŸã¯ã€è‹¥ã„å¥³æ€§ãŒãƒ“ãƒ¼ãƒã§æ„›çŠ¬ã®ãƒ©ãƒ–ãƒ©ãƒ‰ãƒ¼ãƒ«ç¨®ã¨æˆ¯ã‚Œã¦ã„ã‚‹ã¨ã“ã‚ã€‚ äºŒäººã¯æµœè¾ºã«åº§ã‚Šã€çŠ¬ã®å‰è„šã‚’ä¸Šã’ã¦è§¦ã‚Œåˆã£ã¦ã„ã‚‹ã€‚\n\n# ç¬¬ 2 å› å¯¾è©±ã‚¿ãƒ¼ãƒ³\nresponse, history = model.chat(tokenizer, 'è¾“å‡ºå‡»æŒçš„æ£€æµ‹æ¡†', history=history)\nprint(response)\n# <ref>\"å‡»æŒ\"</ref><box>(211,412),(577,891)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('output_chat.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n<br>\n\n## é‡å­åŒ–\n\n### ä½¿ç”¨æ–¹æ³•\n\nç§ãŸã¡ã¯ã€[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)ã«åŸºã¥ã„ãŸæ–°ã—ã„ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æä¾›ã—ã€Qwen-VL-Chatã®ãŸã‚ã®Int4é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã€Qwen-VL-Chat-Int4[Click here](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4)ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ã»ã¼ç„¡æå¤±ãªãƒ¢ãƒ‡ãƒ«åŠ¹æœã‚’é”æˆã—ãªãŒã‚‰ã€ãƒ¡ãƒ¢ãƒªã‚³ã‚¹ãƒˆã¨æ¨è«–é€Ÿåº¦ã®ä¸¡æ–¹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚\n\nã“ã“ã§ã¯ã€é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªè¦ä»¶ï¼ˆtorch 2.0ä»¥ä¸Šã€transformers 4.32.0ä»¥ä¸Šãªã©ï¼‰ã‚’æº€ãŸã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š\n\n```bash\npip install optimum\ngit clone https://github.com/JustinLin610/AutoGPTQ.git & cd AutoGPTQ\npip install -v .\n```\n\n`auto-gptq`ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã€å…¬å¼ã®[repo](https://github.com/PanQiWei/AutoGPTQ)ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€ãƒ›ã‚¤ãƒ¼ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã™ã‚‹ã€‚\n\nãã†ã™ã‚Œã°ã€é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ãŒã§ãã€ã„ã¤ã‚‚ã¨åŒã˜ã‚ˆã†ã«æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼š\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-VL-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)\nprint(response)\n```\n\n### æ€§èƒ½\n\nãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ **[TouchStone](https://github.com/OFA-Sys/TouchStone)** ã«ãŠã„ã¦ã€BF16 ãƒ¢ãƒ‡ãƒ«ã¨ Int4 ãƒ¢ãƒ‡ãƒ«ã®ä¸¡æ–¹ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’ä¾‹ç¤ºã—ã€é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ãŒå¤§ããªæ€§èƒ½åŠ£åŒ–ã«æ‚©ã¾ã•ã‚Œãªã„ã“ã¨ã‚’è¦‹å‡ºã—ã¾ã—ãŸã€‚çµæœã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ï¼š\n\n| Quantization | ZH         | EN            |\n| ------------ | :--------: | :-----------: | \n| BF16         | 401.2      |    645.2      |\n| Int4         | 386.6      |    651.4      |\n\n### æ¨è«–ã‚¹ãƒ”ãƒ¼ãƒ‰\n\nBF16 ç²¾åº¦ã¨ Int4 é‡å­åŒ–ã®ä¸‹ã§ã€ç”»åƒï¼ˆ258 ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦ã™ã‚‹ï¼‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ 1792ï¼ˆ2048-258ï¼‰ãƒˆãƒ¼ã‚¯ãƒ³ã¨ 7934ï¼ˆ8192-258ï¼‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹å¹³å‡æ¨è«–é€Ÿåº¦ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³/ç§’ï¼‰ã‚’ãã‚Œãã‚Œæ¸¬å®šã—ãŸã€‚\n\n| Quantization | Speed (2048 tokens) | Speed (8192 tokens) |\n| ------------ | :-----------------: | :-----------------: |\n| BF16         |        28.87        |        24.32        |\n| Int4         |        37.79        |        34.34        |\n\nãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¯ã€PyTorch 2.0.1 ã¨ CUDA 11.4 ã‚’æ­è¼‰ã—ãŸã‚·ãƒ³ã‚°ãƒ« A100-SXM4-80G GPU ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚\n\n### GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡\n\nã¾ãŸã€1792 (2048-258) å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ (ç”»åƒã‚’å«ã‚€) ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹å ´åˆ (ãŠã‚ˆã³å˜ä¸€ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹å ´åˆ) ã¨ã€7934 (8192-258) å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ (ç”»åƒã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ç”Ÿæˆã™ã‚‹å ´åˆ) ã‚’ãã‚Œãã‚Œ BF16 ã¾ãŸã¯ Int4 é‡å­åŒ–ãƒ¬ãƒ™ãƒ«ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹å ´åˆã® GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒ”ãƒ¼ã‚¯å€¤ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚çµæœã‚’ä»¥ä¸‹ã«ç¤ºã—ã¾ã™ã€‚\n\n| Quantization | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |\n| ------------ | :---------------------------------: | :-----------------------------------: |\n| BF16         |               22.60GB               |                28.01GB                |\n| Int4         |               11.82GB               |                17.23GB                |\n\nä¸Šè¨˜ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã¨ãƒ¡ãƒ¢ãƒªãƒ¼ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¯ã€[ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile_mm.py)ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\n<br>\n\n## ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n\nç¾åœ¨ã€å…¬å¼ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ `finetune.py` ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€finetune.py ã®ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æä¾›ã—ã€finetune.py ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€finetune.py ã‚’èµ·å‹•ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ã•ã‚‰ã«ã€å®‰å¿ƒã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹ãŸã‚ã®ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚‚æä¾›ã—ã¦ã„ã¾ã™ã€‚ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€[DeepSpeed](https://github.com/microsoft/DeepSpeed) ãŠã‚ˆã³ [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) ã‚’ä½¿ç”¨ã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚å¼Šç¤¾ãŒæä¾›ã™ã‚‹ã‚·ã‚§ãƒ«ãƒ»ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ DeepSpeed ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€äº‹å‰ã« DeepSpeed ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™:\n\nå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹ã«ã¯ã€ã™ã¹ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒªã‚¹ãƒˆã«ã¾ã¨ã‚ã€json ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å„ã‚µãƒ³ãƒ—ãƒ«ã¯ id ã¨ä¼šè©±ãƒªã‚¹ãƒˆã§æ§‹æˆã•ã‚Œã‚‹è¾æ›¸ã§ã™ã€‚ä»¥ä¸‹ã¯ 1 ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å«ã‚€å˜ç´”ãªãƒªã‚¹ãƒˆã®ä¾‹ã§ã™:\n\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"ä½ å¥½\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"æˆ‘æ˜¯Qwen-VL,ä¸€ä¸ªæ”¯æŒè§†è§‰è¾“å…¥çš„å¤§æ¨¡å‹ã€‚\"\n      }\n    ]\n  },\n  {\n    \"id\": \"identity_1\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>\\nå›¾ä¸­çš„ç‹—æ˜¯ä»€ä¹ˆå“ç§ï¼Ÿ\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"å›¾ä¸­æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚\"\n      },\n      {\n        \"from\": \"user\",\n        \"value\": \"æ¡†å‡ºå›¾ä¸­çš„æ ¼å­è¡¬è¡«\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"<ref>æ ¼å­è¡¬è¡«</ref><box>(588,499),(725,789)</box>\"\n      }\n    ]\n  },\n  { \n    \"id\": \"identity_2\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>assets/mm_tutorial/Chongqing.jpeg</img>\\nPicture 2: <img>assets/mm_tutorial/Beijing.jpeg</img>\\nå›¾ä¸­éƒ½æ˜¯å“ª\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"ç¬¬ä¸€å¼ å›¾ç‰‡æ˜¯é‡åº†çš„åŸå¸‚å¤©é™…çº¿ï¼Œç¬¬äºŒå¼ å›¾ç‰‡æ˜¯åŒ—äº¬çš„å¤©é™…çº¿ã€‚\"\n      }\n    ]\n  }\n]\n```\n\nVL ã‚¿ã‚¹ã‚¯ã®å ´åˆã€`<img> </img> <ref> </ref> <box> </box>` ãªã©ã®ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\n\nç”»åƒã¯ã€Œç”»åƒ ID: `<img>img_path</img>\\n{your prompt}`ã€ã¨ã—ã¦è¡¨ã•ã‚Œã¾ã™ã€‚ã“ã“ã§ã€ã€Œidã€ã¯ä¼šè©±å†…ã®ç”»åƒã®ä½ç½®ã‚’ 1 ã‹ã‚‰ç¤ºã—ã¾ã™ã€‚ã€Œimg_pathã€ã¯ ãƒ­ãƒ¼ã‚«ãƒ« ãƒ•ã‚¡ã‚¤ãƒ« ãƒ‘ã‚¹ã¾ãŸã¯ Web ãƒªãƒ³ã‚¯ã€‚\n\nåº§æ¨™ãƒœãƒƒã‚¯ã‚¹ã¯ `<box>(x1,y1),(x2,y2)</box>`ãƒ»ã¨ã—ã¦è¡¨ã•ã‚Œã¾ã™ã€‚ã“ã“ã§ã€`(x1, y1)` ã¨ `(x2, y2)` ã¯ç¯„å›²å†…ã®æ­£è¦åŒ–ã•ã‚ŒãŸå€¤ã§ã™ã€‚ `[0, 1000)`ã€‚ å¯¾å¿œã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜ã¯ `<ref>text_caption</ref>` ã«ã‚ˆã£ã¦è­˜åˆ¥ã§ãã¾ã™ã€‚\n\n\nãƒ‡ãƒ¼ã‚¿æº–å‚™ã®å¾Œã€æä¾›ã•ã‚Œã¦ã„ã‚‹ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ã£ã¦å¾®èª¿æ•´ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ `$DATA` ã‚’å¿˜ã‚Œãšã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\n\nãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®ã“ã¨ãŒå¯èƒ½ã«ãªã‚‹ï¼š\n- ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n- LoRA\n- Q-LoRA\n\n### ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\nãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ã«ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã§ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ï¼š\n\n```bash\n# åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚GPU ãƒ¡ãƒ¢ãƒªãŒä¸è¶³ã™ã‚‹ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒç ´ç¶»ã™ã‚‹ãŸã‚ã€ã‚·ãƒ³ã‚°ãƒ« GPU ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯æä¾›ã—ã¦ã„ã¾ã›ã‚“ã€‚\nsh finetune/finetune_ds.sh\n```\n\nã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ã€æ­£ã—ã„ãƒ¢ãƒ‡ãƒ«åã¾ãŸã¯ãƒ‘ã‚¹ã€ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã€å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚’å¿˜ã‚Œãªã„ã§ãã ã•ã„ã€‚å¤‰æ›´ã—ãŸã„å ´åˆã¯ã€å¼•æ•° `--deepspeed` ã‚’å‰Šé™¤ã™ã‚‹ã‹ã€è¦ä»¶ã«åŸºã¥ã„ã¦ DeepSpeed è¨­å®š json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚ã•ã‚‰ã«ã€ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯æ··åˆç²¾åº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«å¯¾å¿œã—ã¦ãŠã‚Šã€`--bf16 True` ã¾ãŸã¯ `--fp16 True` ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚çµŒé¨“çš„ã«ã€ã‚ãªãŸã®ãƒã‚·ãƒ³ãŒbf16ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹å ´åˆã€ç§ãŸã¡ã®ãƒ—ãƒªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’æ•´åˆã•ã›ã‚‹ãŸã‚ã«bf16ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n\n### LoRA\nåŒæ§˜ã«ã€LoRA ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«åˆ¥ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ã£ã¦å®Ÿè¡Œã™ã‚‹ã€‚å§‹ã‚ã‚‹å‰ã«ã€`peft` ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã€å‡ºåŠ›ã¸ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«ã¯çµ¶å¯¾ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãªãœãªã‚‰ã€LoRA ã¯ã‚¢ãƒ€ãƒ—ã‚¿ã®ã¿ã‚’ä¿å­˜ã—ã€ã‚¢ãƒ€ãƒ—ã‚¿è¨­å®š json ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹ã¯ã€ãƒ­ãƒ¼ãƒ‰ã™ã‚‹äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‹ã‚‰ã§ã™ã€‚ã¾ãŸã€ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ bf16 ã¨ fp16 ã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã€‚\n\n```bash\n# ã‚·ãƒ³ã‚°ãƒ« GPU ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\nsh finetune/finetune_lora_single_gpu.sh\n# åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\nsh finetune/finetune_lora_ds.sh\n```\n\nLoRA ([è«–æ–‡](https://arxiv.org/abs/2106.09685)) ã¯ã€ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦ã€adapter ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’æ›´æ–°ã™ã‚‹ã ã‘ã§ã€å…ƒã®å¤§ããªè¨€èªãƒ¢ãƒ‡ãƒ«å±¤ã¯å‡çµã•ã‚ŒãŸã¾ã¾ã§ã‚ã‚‹ã€‚ãã®ãŸã‚ã€ãƒ¡ãƒ¢ãƒªã‚³ã‚¹ãƒˆãŒå¤§å¹…ã«å‰Šæ¸›ã§ãã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚‚å‰Šæ¸›ã§ãã‚‹ã€‚\n\nãªãŠã€ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆQwen-VL-Chatãªã©ï¼‰ã§ã¯ãªãã€ãƒ™ãƒ¼ã‚¹è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆQwen-VLãªã©ï¼‰ã®å¾®èª¿æ•´ã«LoRAã‚’ä½¿ç”¨ã—ãŸå ´åˆã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯è‡ªå‹•çš„ã«å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦åŸ‹ã‚è¾¼ã¿å±¤ã¨å‡ºåŠ›å±¤ã‚’åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã¯ã€ChatMLãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã‚ˆã£ã¦ã‚‚ãŸã‚‰ã•ã‚Œã‚‹ç‰¹æ®Šãªãƒˆãƒ¼ã‚¯ãƒ³ã«é–¢ã™ã‚‹çŸ¥è­˜ãŒãªã„ãŸã‚ã§ã™ã€‚ã—ãŸãŒã£ã¦ã€ã“ã‚Œã‚‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç†è§£ã—äºˆæ¸¬ã™ã‚‹ãŸã‚ã«æ›´æ–°ã•ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚åˆ¥ã®è¨€ã„æ–¹ã‚’ã™ã‚Œã°ã€ã‚‚ã—LoRAã§ç‰¹æ®Šãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’å­¦ç¿’ã™ã‚‹ã®ã§ã‚ã‚Œã°ã€ã‚³ãƒ¼ãƒ‰å†…ã§ `modules_to_save` ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã•ã‚‰ã«ã€LoRAã®ãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆã¯ã€ã“ã®ã‚ˆã†ãªå­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¨ãªã„å ´åˆã§ã€å¤§ããªé–‹ããŒã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ãã®ãŸã‚ã€ãƒ¡ãƒ¢ãƒªã«å•é¡ŒãŒã‚ã‚‹å ´åˆã¯ã€LoRAã®Chatãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚è©³ç´°ã¯ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n### Q-LoRA\nã—ã‹ã—ã€ãã‚Œã§ã‚‚ãƒ¡ãƒ¢ãƒªä¸è¶³ã«æ‚©ã‚€å ´åˆã¯ã€Q-LoRAï¼ˆ[è«–æ–‡](https://arxiv.org/abs/2305.14314)ï¼‰ã‚’æ¤œè¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€é‡å­åŒ–ã•ã‚ŒãŸãƒ©ãƒ¼ã‚¸è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã€ãƒšãƒ¼ã‚¸ãƒ‰ãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãªã©ã®ä»–ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã€ã•ã‚‰ã«å°‘ãªã„ãƒ¡ãƒ¢ãƒªã‚³ã‚¹ãƒˆã§å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚Q-LoRA ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç›´æ¥å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š\n\n\n```bash\n# ã‚·ãƒ³ã‚°ãƒ«GPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\nsh finetune/finetune_qlora_single_gpu.sh\n# åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\nsh finetune/finetune_qlora_ds.sh\n```\n\nQ-LoRA ã«ã¤ã„ã¦ã¯ã€å¼Šç¤¾ãŒæä¾›ã™ã‚‹é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã€ä¾‹ãˆã° Qwen-7B-Chat-Int4 ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãŸã ã—ã€ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„ LoRA ã¨ã¯ç•°ãªã‚Šã€Q-LoRA ã§ã¯ fp16 ã®ã¿ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹ã€‚\n\nLoRA ã¨ Q-LoRA ã®å­¦ç¿’ã¯ã€ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯ç•°ãªã‚Šã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’ä¿å­˜ã™ã‚‹ã€‚ä»®ã« Qwen-7B ã‹ã‚‰å­¦ç¿’ã‚’é–‹å§‹ã—ãŸã¨ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§æ¨è«–ã‚’è¡Œã†ã“ã¨ãŒã§ãã‚‹ï¼š\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\nã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒãƒ¼ã‚¸ã—ã€å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä¿å­˜ã—ãŸã„å ´åˆã¯ï¼ˆã“ã‚Œã¯ LoRA ã§ã®ã¿å¯èƒ½ã§ã€Q-LoRA ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ï¼‰ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\næ³¨æ„ï¼šãƒãƒ«ãƒGPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å ´åˆã€åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®é©åˆ‡ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒã‚·ãƒ³ã«å¿œã˜ã¦æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€ãƒ‡ãƒ¼ã‚¿ã€ãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’è€ƒæ…®ã—ã¦ã€å¼•æ•° `--model_max_length` ã§æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n\n\n### ãƒ¡ãƒ¢ãƒªã¨é€Ÿåº¦ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°\nã‚·ãƒ³ã‚°ãƒ«GPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«ãŠã„ã¦ã€LoRA (LoRA (Base)ã¯embeddingã¨å‡ºåŠ›å±¤ã‚’å­¦ç¿’ã•ã›ã‚‹ãŒã€LoRA (Chat)ã¯embeddingã¨å‡ºåŠ›å±¤ã‚’å­¦ç¿’ã•ã›ãªã„) ã¨Q-LoRAã®GPUãƒ¡ãƒ¢ãƒªã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã™ã‚‹ã€‚ã“ã®ãƒ†ã‚¹ãƒˆã§ã¯ã€ã‚·ãƒ³ã‚°ãƒ«A100-SXM4-80G GPUã§å®Ÿé¨“ã—ã€CUDA 11.8ã¨Pytorch 2.0ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚å„ã‚µãƒ³ãƒ—ãƒ«ã«ã¯å†™çœŸãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚384ã€512ã€1024ã€2048ã¨ã„ã†ç•°ãªã‚‹é•·ã•ã®å…¥åŠ›ã®ãƒ¡ãƒ¢ãƒªï¼ˆGBï¼‰ã¨é€Ÿåº¦ï¼ˆs/iterï¼‰ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚çµ±è¨ˆé‡ã‚’ä»¥ä¸‹ã«ç¤ºã™ï¼š\n<table>\n    <tr>\n <th rowspan=\"2\">Method</th><th colspan=\"4\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">384</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th>\n    </tr>\n    <tr>\n      <td>LoRA (Base)</td><td align=\"center\">37.1G / 2.3s/it</td><td align=\"center\">37.3G / 2.4s/it</td><td align=\"center\">38.7G / 3.6s/it</td><td align=\"center\">38.7G / 6.1s/it</td>\n    </tr>\n    <tr>\n      <td>LoRA (Chat)</td><td align=\"center\">23.3G / 2.2s/it</td><td align=\"center\">23.6G / 2.3s/it</td><td align=\"center\">25.1G / 3.5s/it</td><td align=\"center\">27.3G / 5.9s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">17.0G / 4.2s/it</td><td align=\"center\">17.2G / 4.5s/it</td><td align=\"center\">18.2G / 5.5s/it</td><td align=\"center\">19.3G / 7.9s/it</td>\n    </tr>\n\n</table>\n\nã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ `torchrun` ã‚’ä½¿ç”¨ã—ã¦ã‚·ãƒ³ã‚°ãƒ« GPU ã¾ãŸã¯ãƒãƒ«ãƒGPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ãã®ãŸã‚ã€åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã®é©åˆ‡ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒã‚·ãƒ³ã«å¿œã˜ã¦æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n<br><br>\n\n## ãƒ‡ãƒ¢\n\n### Web UI\n\nWeb UI ãƒ‡ãƒ¢ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰ã‚’æä¾›ã—ã¾ã™ã€‚å§‹ã‚ã‚‹å‰ã«ã€ä»¥ä¸‹ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„:\n\n```bash\npip install -r requirements_web_demo.txt\n```\n\næ¬¡ã«ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã€ç”Ÿæˆã•ã‚ŒãŸãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™:\n\n```bash\npython web_demo_mm.py\n```\n\n<br>\n\n## FAQ\n\nå•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€[FAQ](FAQ_ja.md) ã‚„ issue ã‚’å‚ç…§ã—ã€æ–°ã—ã„ issue ã‚’ç«‹ã¡ä¸Šã’ã‚‹å‰ã«è§£æ±ºç­–ã‚’æ¢ã—ã¦ãã ã•ã„ã€‚\n<br>\n\n## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹å¥‘ç´„\n\nç ”ç©¶è€…ã‚„é–‹ç™ºè€…ã¯ã€Qwen-VL ã¨ Qwen-VL-Chat ã®ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã‚’è‡ªç”±ã«ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€å•†ç”¨åˆ©ç”¨ã‚‚å¯èƒ½ã§ã™ã€‚è©³ã—ãã¯ [LICENSE](LICENSE) ã‚’ã”è¦§ãã ã•ã„ã€‚\n<br>\n\n## å¼•ç”¨\n\nç§ãŸã¡ã®è«–æ–‡ã‚„ã‚³ãƒ¼ãƒ‰ãŒã‚ãªãŸã®ç ”ç©¶ã«å½¹ç«‹ã¤ã¨ãŠæ„Ÿã˜ã«ãªã‚Šã¾ã—ãŸã‚‰ã€ã‚¹ã‚¿ãƒ¼ :star: ã¨ å¼•ç”¨ :pencil: ã‚’ãŠä»˜ã‘ãã ã•ã„ :)\n\n```BibTeX\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n<br>\n\n## ãŠå•ã„åˆã‚ã›\n\nç ”ç©¶ãƒãƒ¼ãƒ ã¾ãŸã¯è£½å“ãƒãƒ¼ãƒ ã¸ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã€qianwen_opensource@alibabacloud.com ã¾ã§ãŠæ°—è»½ã«ãŠé€ã‚Šãã ã•ã„ã€‚\n\n"
        },
        {
          "name": "README_KO.md",
          "type": "blob",
          "size": 41.1201171875,
          "content": "<p align=\"left\">\n        <a href=\"README_CN.md\">ä¸­æ–‡</a>&nbsp ï½œ &nbspEnglish&nbsp&nbsp ï½œ &nbsp<a href=\"README_JA.md\">æ—¥æœ¬èª</a>&nbsp ï½œ &nbsp<a href=\"README_KO.md\">í•œêµ­ì–´</a>&nbsp\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"assets/logo.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        Qwen-VL <a href=\"https://modelscope.cn/models/qwen/Qwen-VL/summary\">ğŸ¤– <a> | <a href=\"https://huggingface.co/Qwen/Qwen-VL\">ğŸ¤—</a>&nbsp ï½œ Qwen-VL-Chat <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary\">ğŸ¤– <a>| <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat\">ğŸ¤—</a>&nbsp ï½œ Qwen-VL-Chat-Int4 <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat-Int4\">ğŸ¤—</a>\n<br>\n<a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary\">Demo</a>&nbsp ï½œ &nbsp<a href=\"https://arxiv.org/abs/2308.12966\">Paper</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://github.com/camenduru/Qwen-VL-Chat-colab\">Colab</a>&nbsp&nbsp | &nbsp <a href=\"TUTORIAL.md\">Tutorial</a>\n</p>\n<br><br>\n\n---\n\n\n**Qwen-VL**(Qwen Large Vision Language Model)ì€ ì•Œë¦¬ë°”ë°” í´ë¼ìš°ë“œê°€ ì œì•ˆí•œ í° ëª¨ë¸ ì‹œë¦¬ì¦ˆì¸ Qwen(ì•½ì¹­, Tongyi Qianwen)ì˜ ë©€í‹°ëª¨ë‹¬ ë²„ì „ì…ë‹ˆë‹¤. Qwen-VLì€ ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ê·¸ë¦¬ê³  ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ í…ìŠ¤íŠ¸ì™€ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. Qwen-VLì˜ íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n- **ê°•ë ¥í•œ ì„±ëŠ¥**: ë™ì¼í•œ ëª¨ë¸ ê·œëª¨ì˜ ê¸°ì¡´ ê³µê°œëœ ëŒ€ê·œëª¨ ì‹œê° ì–¸ì–´ ëª¨ë¸(Large Vision Language Models, ã„´LVLM)ë³´ë‹¤ ì˜ì–´ í‰ê°€ ë²¤ì¹˜ë§ˆí¬(Zero-shot Captioning, VQA, DocVQA, Grounding í¬í•¨)ì—ì„œ í˜„ì €íˆ ìš°ìˆ˜í•©ë‹ˆë‹¤.\n- **í…ìŠ¤íŠ¸ ì¸ì‹ì„ ì§€ì›í•˜ëŠ” ë‹¤êµ­ì–´ LVLM**: Qwen-VLì€ ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´, ì¤‘êµ­ì–´ ë° ë‹¤êµ­ì–´ ëŒ€í™”ë¥¼ ì§€ì›í•˜ë©°, ì´ë¯¸ì§€ ë‚´ ì¤‘êµ­ì–´-ì˜ì–´ ê°„ ì´ì¤‘ ì–¸ì–´ í…ìŠ¤íŠ¸ì˜ ì¢…ë‹¨ ê°„ ì¸ì‹ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.\n- **ë‹¤ì¤‘ ì´ë¯¸ì§€ êµì°¨ ëŒ€í™”**: ì´ ê¸°ëŠ¥ì€ ì—¬ëŸ¬ ì´ë¯¸ì§€ì˜ ì…ë ¥ê³¼ ë¹„êµë¿ë§Œ ì•„ë‹ˆë¼ ì´ë¯¸ì§€ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì„ ì§€ì •í•˜ê³  ë‹¤ì¤‘ ì´ë¯¸ì§€ ìŠ¤í† ë¦¬í…”ë§ì— ì°¸ì—¬í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n- **ì¤‘êµ­ì–´ì—ì„œ ì§€ìƒí™”ë¥¼ ì§€ì›í•˜ëŠ” ì²« ë²ˆì§¸ ì¼ë°˜ ëª¨ë¸**: ì¤‘êµ­ì–´ì™€ ì˜ì–´ì˜ ê°œë°©í˜• ì–¸ì–´ í‘œí˜„ì„ í†µí•´ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì¸ì‹í•©ë‹ˆë‹¤.\n- **ì„¸ë°€í•œ ì¸ì‹ ë° ì´í•´**: ë‹¤ë¥¸ ê³µê°œëœ LVLMì´ í˜„ì¬ ì‚¬ìš©í•˜ëŠ” 224\\*224 í•´ìƒë„ì™€ ë¹„êµí•˜ì—¬ 448\\*448 í•´ìƒë„ëŠ” ì„¸ë°€í•œ í…ìŠ¤íŠ¸ ì¸ì‹, ë¬¸ì„œ QA ë° ë°”ìš´ë”© ì–´ë…¸í…Œì´ì…˜ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤.\n\n<br>\n<p align=\"center\">\n    <img src=\"assets/demo_vl.gif\" width=\"400\"/>\n<p>\n<br>\n\nQwen-VL ì‹œë¦¬ì¦ˆì˜ ë‘ ëª¨ë¸ì„ ì¶œì‹œí•©ë‹ˆë‹¤.\n\n- Qwen-VL: ì‚¬ì „ í›ˆë ¨ëœ LVLM ëª¨ë¸ë¡œ, Qwen-7Bë¥¼ LLMì˜ ì´ˆê¸°í™”ì— ì‚¬ìš©í•˜ë©°, ì‹œê° ì¸ì½”ë”ì˜ ì´ˆê¸°í™”ë¡œëŠ” [Openclip ViT-bigG](https://github.com/mlfoundations/open_clip)ë¥¼ ì‚¬ìš©í•˜ì—¬, ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ êµì°¨ ì–´í…ì…˜ ë ˆì´ì–´(randomly initialized cross-attention layer)ì— ì—°ê²°í•©ë‹ˆë‹¤.\n- Qwen-VL-Chat: ì •ë ¬ ê¸°ìˆ ë¡œ í›ˆë ¨ëœ ë©€í‹°ëª¨ë‹¬ LLM ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. Qwen-VL-Chatì€ ì—¬ëŸ¬ ì´ë¯¸ì§€ ì…ë ¥, ë‹¤ì¤‘ ë¼ìš´ë“œ ì§ˆë¬¸ ì‘ë‹µ, ì°½ì˜ì  ëŠ¥ë ¥ê³¼ ê°™ì€ ë” ìœ ì—°í•œ ìƒí˜¸ì‘ìš©ì„ ì§€ì›í•©ë‹ˆë‹¤.\n\n<br>\n\n## ë‰´ìŠ¤ ë° ì—…ë°ì´íŠ¸\n* ```2023.9.25``` ğŸš€ğŸš€ğŸš€ Qwen-VL-Chatì„ ë”ìš± ê°•ë ¥í•œ ì¤‘êµ­ì–´ ì§€ì‹œ ìˆ˜í–‰ ëŠ¥ë ¥, ì›¹í˜ì´ì§€ ë° í‘œ ì´ë¯¸ì§€ì— ëŒ€í•œ ê°œì„ ëœ ì´í•´ë ¥, ë” ë‚˜ì€ ëŒ€í™” ì„±ëŠ¥(TouchStone: CN: 401.2->481.7, EN: 645.2->711.6)ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë˜ì—ˆìŠµë‹ˆë‹¤.\n* ```2023.9.12``` ğŸ˜ƒğŸ˜ƒğŸ˜ƒ ì´ì œ Qwen-VL ëª¨ë¸ì— ëŒ€í•œ íŒŒì¸íŠœë‹ì„ ì§€ì›í•©ë‹ˆë‹¤. ì´ì—ëŠ” ì „ì²´ íŒŒë¼ë¯¸í„° íŒŒì¸íŠœë‹, LoRA ë° Q-LoRAê°€ í¬í•¨ë©ë‹ˆë‹¤.\n* ```2023.9.8``` ğŸ‘ğŸ‘ğŸ‘ camenduruê°€ ë©‹ì§„ Colabì„ ê¸°ì—¬í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ëª¨ë‘ê°€ 12G GPUì—ì„œ ë¡œì»¬ ë˜ëŠ” ì˜¨ë¼ì¸ Qwen-VL-Chat-Int4 ë°ëª¨ íŠœí† ë¦¬ì–¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n* ```2023.9.5``` ğŸ‘ğŸ‘ğŸ‘ Qwen-VL-Chatì€ MME Benchmark, ë©€í‹°ëª¨ë‹¬ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ìœ„í•œ ì¢…í•©ì ì¸ í‰ê°€ ë²¤ì¹˜ë§ˆí¬ì—ì„œ SOTAsë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì´ 14ê°œì˜ í•˜ìœ„ ê³¼ì œì—ì„œ ì¸ì‹ê³¼ ì¸ì§€ ëŠ¥ë ¥ì„ ëª¨ë‘ ì¸¡ì •í•©ë‹ˆë‹¤.\n* ```2023.9.4``` â­â­â­ Qwen-VL ì‹œë¦¬ì¦ˆëŠ” Seed-Bench, ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì´í•´ë¥¼ í‰ê°€í•˜ëŠ” 19K ë‹¤ì¤‘ ì„ íƒ ì§ˆë¬¸ì˜ ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ SOTAsë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì •í™•í•œ ì¸ê°„ ì£¼ì„ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤.\n* ```2023.9.1``` ğŸ”¥ğŸ”¥ğŸ”¥ ê¸°ë³¸ì ì¸ ì¸ì‹ê³¼ ì´í•´ë ¥ë¿ë§Œ ì•„ë‹ˆë¼ ë¬¸í•™ ì°½ì‘ê¹Œì§€ ì•„ìš°ë¥´ëŠ” ë³µí•© ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ í‰ê°€ì¸ [TouchStone](https://github.com/OFA-Sys/TouchStone) í‰ê°€ë¥¼ ì¶œì‹œí•©ë‹ˆë‹¤. ê°•ë ¥í•œ LLMì„ ì‹¬ì‚¬ìœ„ì›ìœ¼ë¡œ í™œìš©í•˜ê³ , ë©€í‹°ëª¨ë‹¬ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.\n* ```2023.8.31``` ğŸŒŸğŸŒŸğŸŒŸ Qwen-VL-Chatìš© Int4 ì–‘ìí™” ëª¨ë¸ì¸ **Qwen-VL-Chat-Int4**ë¥¼ ì¶œì‹œí•˜ì—¬ ë©”ëª¨ë¦¬ ë¹„ìš©ì€ ë‚®ì¶”ê³  ì¶”ë¡  ì†ë„ëŠ” í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ë˜í•œ ë²¤ì¹˜ë§ˆí¬ í‰ê°€ì—ì„œë„ ì„±ëŠ¥ ì €í•˜ê°€ í¬ì§€ ì•ŠìŠµë‹ˆë‹¤.\n* ```2023.8.22``` ğŸ‰ğŸ‰ğŸ‰ ëª¨ë¸ìŠ¤ì½”í”„ì™€ í—ˆê¹…í˜ì´ìŠ¤ì— **Qwen-VL**ê³¼ **Qwen-VL-Chat**ì„ ëª¨ë‘ ì¶œì‹œí•©ë‹ˆë‹¤. í•™ìŠµ ë‚´ìš© ë° ëª¨ë¸ ì„±ëŠ¥ ë“± ëª¨ë¸ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ë…¼ë¬¸](https://arxiv.org/abs/2308.12966)ì„ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n## Evaluation\n\nì„¸ ê°€ì§€ ê´€ì ì—ì„œ ëª¨ë¸ì˜ ê¸°ëŠ¥ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤:\n\n1. **í‘œì¤€ ë²¤ì¹˜ë§ˆí¬**: ë©€í‹°ëª¨ë‹¬ ì‘ì—…ì˜ ë„¤ ê°€ì§€ ì£¼ìš” ë²”ì£¼ì— ëŒ€í•œ ëª¨ë¸ì˜ ê¸°ë³¸ ì‘ì—… ê¸°ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤:\n   \n   - ì œë¡œ ìƒ· ìº¡ì…˜: ë³´ì´ì§€ ì•ŠëŠ” ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ëª¨ë¸ì˜ ì œë¡œìƒ· ì´ë¯¸ì§€ ìº¡ì…˜ ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n   - ì¼ë°˜ VQA: íŒë‹¨, ìƒ‰ìƒ, ìˆ«ì, ì¹´í…Œê³ ë¦¬ ë“±ê³¼ ê°™ì€ ì‚¬ì§„ì˜ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n   - í…ìŠ¤íŠ¸ ê¸°ë°˜ VQA: ë¬¸ì„œ QA, ì°¨íŠ¸ QA ë“±ê³¼ ê°™ì´ ì‚¬ì§„ ì† í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•˜ëŠ” ëª¨ë¸ì˜ ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n   - ì°¸ì¡° í‘œí˜„ ì´í•´: ì°¸ì¡° í‘œí˜„ì‹ìœ¼ë¡œ ì„¤ëª…ëœ ì´ë¯¸ì§€ì—ì„œ ëŒ€ìƒ ê°ì²´ë¥¼ ì°¾ì•„ë‚´ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n  \n2. **í„°ì¹˜ìŠ¤í†¤**: ì „ë°˜ì ì¸ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ëŒ€í™” ëŠ¥ë ¥ê³¼ ì‚¬ëŒê³¼ì˜ ì¼ì¹˜ë„ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ [TouchStone](https://github.com/OFA-Sys/TouchStone)ì´ë¼ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ êµ¬ì¶•í–ˆìœ¼ë©°, ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” GPT4ë¡œ ì±„ì í•˜ì—¬ LVLM ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.\n   \n   - í„°ì¹˜ìŠ¤í†¤ ë²¤ì¹˜ë§ˆí¬ëŠ” ì´ 300ê°œ ì´ìƒì˜ ì´ë¯¸ì§€, 800ê°œ ì´ìƒì˜ ì§ˆë¬¸, 27ê°œ ì¹´í…Œê³ ë¦¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì†ì„± ê¸°ë°˜ Q&A, ìœ ëª…ì¸ ì¸ì‹, ì‹œ ì“°ê¸°, ì—¬ëŸ¬ ì´ë¯¸ì§€ ìš”ì•½, ì œí’ˆ ë¹„êµ, ìˆ˜í•™ ë¬¸ì œ í’€ì´ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\n   - ì§ì ‘ ì´ë¯¸ì§€ ì…ë ¥ì´ë¼ëŠ” í˜„ì¬ GPT4ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ TouchStoneì€ ì‚¬ëŒì´ ì§ì ‘ ë¼ë²¨ì„ ì§€ì •í•˜ì—¬ ì„¸ë¶„í™”ëœ ì´ë¯¸ì§€ ì£¼ì„ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì„¸ë¶€ ì£¼ì„ì€ ë¬¸ì œ ë° ëª¨ë¸ì˜ ì¶œë ¥ê³¼ í•¨ê»˜ ì±„ì ì„ ìœ„í•´ GPT4ì— ì œê³µë©ë‹ˆë‹¤.\n   - ë²¤ì¹˜ë§ˆí¬ì—ëŠ” ì˜ì–´ì™€ ì¤‘êµ­ì–´ ë²„ì „ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n  \n3. **ê¸°íƒ€ ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬**: ë‹¤ë¥¸ ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œë„ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤:\n\n   - ë©€í‹°ëª¨ë‹¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ í‰ê°€ ë²¤ì¹˜ë§ˆí¬ì¸ [MME ë²¤ì¹˜ë§ˆí¬](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation). Qwen-VL-Chatì€ ì§€ê°ê³¼ ì¸ì§€ íŠ¸ë™ ëª¨ë‘ì—ì„œ SOTAë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.\n   - [Seed-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard)ëŠ” ë©€í‹°ëª¨ë‹¬ LLMì„ í‰ê°€í•˜ê¸° ìœ„í•œ ì •í™•í•œ ì¸ê°„ ì£¼ì„ì´ í¬í•¨ëœ 19K ê°ê´€ì‹ ì§ˆë¬¸ìœ¼ë¡œ êµ¬ì„±ëœ ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤. íì› ì‹œë¦¬ì¦ˆëŠ” ì´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ SOTAë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.\n\ní‰ê°€ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\nQwen-VLì€ ì—¬ëŸ¬ VL ì‘ì—…ì—ì„œ í˜„ì¬ SOTA ì œë„ˆëŸ´ë¦¬ìŠ¤íŠ¸ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ë©°, ê¸°ëŠ¥ ë²”ìœ„ ì¸¡ë©´ì—ì„œ ë” í¬ê´„ì ì¸ ê¸°ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤.\n\n<p align=\"center\">\n    <img src=\"assets/radar.png\" width=\"600\"/>\n<p>\n\n### Zero-shot Captioning & General VQA\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"2\">Zero-shot Captioning</th>\n    <th colspan=\"5\">General VQA</th>\n  </tr>\n  <tr>\n    <th>NoCaps</th>\n    <th>Flickr30K</th>\n    <th>VQAv2<sup>dev</sup></th>\n    <th>OK-VQA</th>\n    <th>GQA</th>\n    <th>SciQA-Img<br>(0-shot)</th>\n    <th>VizWiz<br>(0-shot)</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"10\">Generalist<br>Models</td>\n    <td>Flamingo-9B</td>\n    <td>-</td>\n    <td>61.5</td>\n    <td>51.8</td>\n    <td>44.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>28.8</td>\n  </tr>\n  <tr>\n    <td>Flamingo-80B</td>\n    <td>-</td>\n    <td>67.2</td>\n    <td>56.3</td>\n    <td>50.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>31.6</td>\n  </tr>\n  <tr>\n    <td>Unified-IO-XL</td>\n    <td>100.0</td>\n    <td>-</td>\n    <td>77.9</td>\n    <td>54.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Kosmos-1</td>\n    <td>-</td>\n    <td>67.1</td>\n    <td>51.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>29.2</td>\n  </tr>\n  <tr>\n    <td>Kosmos-2</td>\n    <td>-</td>\n    <td>80.5</td>\n    <td>51.1</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>103.9</td>\n    <td>71.6</td>\n    <td>65.0</td>\n    <td>45.9</td>\n    <td>32.3</td>\n    <td>61.0</td>\n    <td>19.6</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td><strong>121.9</strong></td>\n    <td>82.8</td>\n    <td>-</td>\n    <td>-</td>\n    <td>49.5</td>\n    <td>63.1</td>\n    <td>33.4</td>\n  </tr>\n  <tr>\n    <td>Shikra (Vicuna-13B)</td>\n    <td>-</td>\n    <td>73.9</td>\n    <td>77.36</td>\n    <td>47.16</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td><strong>Qwen-VL (Qwen-7B)</strong></td>\n    <td>121.4</td>\n    <td><b>85.8</b></td>\n    <td><b>78.8</b></td>\n    <td><b>58.6</b></td>\n    <td><b>59.3</b></td>\n    <td>67.1</td>\n    <td>35.2</td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>63.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>39.1</td>\n  </tr> -->\n  <tr>\n    <td>Qwen-VL-Chat</td>\n    <td>120.2</td>\n    <td>81.0</td>\n    <td>78.2</td>\n    <td>56.6</td>\n    <td>57.5</td>\n    <td><b>68.2</b></td>\n    <td><b>38.9</b></td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL-Chat (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>60.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>44.45</td>\n  </tr> -->\n  <tr>\n    <td>Previous SOTA<br>(Per Task Fine-tuning)</td>\n    <td>-</td>\n    <td>127.0<br>(PALI-17B)</td>\n    <td>84.5<br>(InstructBLIP<br>-FlanT5-XL)</td>\n    <td>86.1<br>(PALI-X<br>-55B)</td>\n    <td>66.1<br>(PALI-X<br>-55B)</td>\n    <td>72.1<br>(CFR)</td>\n    <td>92.53<br>(LLaVa+<br>GPT-4)</td>\n    <td>70.9<br>(PALI-X<br>-55B)</td>\n  </tr>\n</tbody>\n</table>\n\n- ì œë¡œ ìƒ· ì´ë¯¸ì§€ ìº¡ì…˜ì˜ ê²½ìš°, Qwen-VLì€ Flickr30Kì—ì„œ **SOTA**ë¥¼ ë‹¬ì„±í–ˆê³  InstructBlipì„ ì‚¬ìš©í•˜ì—¬ ë…¸ìº¡ìŠ¤ì—ì„œ ê²½ìŸë ¥ ìˆëŠ” ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.\n- ì¼ë°˜ VQAì˜ ê²½ìš°, Qwen-VLì€ ë™ì¼í•œ ì¼ë°˜ LVLM ìŠ¤ì¼€ì¼ ì„¤ì •ì—ì„œ **SOTA**ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.\n\n\n### Text-oriented VQA (Focused on text understanding capabilities in images)\n\n<table>\n<thead>\n  <tr>\n    <th>Model type</th>\n    <th>Model</th>\n    <th>TextVQA</th>\n    <th>DocVQA</th>\n    <th>ChartQA</th>\n    <th>AI2D</th>\n    <th>OCR-VQA</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"5\">Generalist Models</td>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>42.4</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td>50.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>mPLUG-DocOwl (LLaMA-7B)</td>\n    <td>52.6</td>\n    <td>62.2</td>\n    <td>57.4</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Pix2Struct-Large (1.3B)</td>\n    <td>-</td>\n    <td><b>76.6</b></td>\n    <td>58.6</td>\n    <td>42.1</td>\n    <td>71.3</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL (Qwen-7B)</td>\n    <td><b>63.8</b></td>\n    <td>65.1</td>\n    <td><b>65.7</b></td>\n    <td><b>62.3</b></td>\n    <td><b>75.7</b></td>\n  </tr>\n  <tr>\n    <td>Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>PALI-X-55B (Single-task FT)<br>(Without OCR Pipeline)</td>\n    <td>71.44</td>\n    <td>80.0</td>\n    <td>70.0</td>\n    <td>81.2</td>\n    <td>75.0</td>\n  </tr>\n</tbody>\n</table>\n\n- í…ìŠ¤íŠ¸ ê´€ë ¨ ì¸ì‹/QA í‰ê°€ì—ì„œ Qwen-VLì€ ì¼ë°˜ì ì¸ LVLM ìŠ¤ì¼€ì¼ ì„¤ì •ì—ì„œ SOTAë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.\n- í•´ìƒë„ëŠ” ìœ„ì˜ ì—¬ëŸ¬ í‰ê°€ì—ì„œ ì¤‘ìš”í•©ë‹ˆë‹¤. 224 í•´ìƒë„ì˜ ëŒ€ë¶€ë¶„ì˜ ì˜¤í”ˆ ì†ŒìŠ¤ LVLM ëª¨ë¸ì€ ì´ëŸ¬í•œ í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ê±°ë‚˜ ì´ë¯¸ì§€ë¥¼ ì˜ë¼ë‚´ì•¼ë§Œ í•´ê²°í•  ìˆ˜ ìˆì§€ë§Œ, Qwen-VLì€ í•´ìƒë„ë¥¼ 448ë¡œ í™•ì¥í•˜ì—¬ ì—”ë“œíˆ¬ì—”ë“œ í‰ê°€ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. Qwen-VLì€ ì¼ë¶€ ì‘ì—…ì—ì„œ 1024 í•´ìƒë„ì˜ Pix2Struct-Large ëª¨ë¸ë³´ë‹¤ ë” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.\n\n### Referring Expression Comprehension\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"3\">RefCOCO</th>\n    <th colspan=\"3\">RefCOCO+</th>\n    <th colspan=\"2\">RefCOCOg</th>\n    <th>GRIT</th>\n  </tr>\n  <tr>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val-u</th>\n    <th>test-u</th>\n    <th>refexp</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"8\">Generalist Models</td>\n    <td>GPV-2</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>51.50</td>\n  </tr>\n  <tr>\n    <td>OFA-L*</td>\n    <td>79.96</td>\n    <td>83.67</td>\n    <td>76.39</td>\n    <td>68.29</td>\n    <td>76.00</td>\n    <td>61.75</td>\n    <td>67.57</td>\n    <td>67.58</td>\n    <td>61.70</td>\n  </tr>\n  <tr>\n    <td>Unified-IO</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td><b>78.61</b></td>\n  </tr>\n  <tr>\n    <td>VisionLLM-H</td>\n    <td></td>\n    <td>86.70</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Shikra-7B</td>\n    <td>87.01</td>\n    <td>90.61</td>\n    <td>80.24 </td>\n    <td>81.60</td>\n    <td>87.36</td>\n    <td>72.12</td>\n    <td>82.27</td>\n    <td>82.19</td>\n    <td>69.34</td>\n  </tr>\n  <tr>\n    <td>Shikra-13B</td>\n    <td>87.83 </td>\n    <td>91.11</td>\n    <td>81.81</td>\n    <td>82.89</td>\n    <td>87.79</td>\n    <td>74.41</td>\n    <td>82.64</td>\n    <td>83.16</td>\n    <td>69.03</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B</td>\n    <td><b>89.36</b></td>\n    <td>92.26</td>\n    <td><b>85.34</b></td>\n    <td><b>83.12</b></td>\n    <td>88.25</td>\n    <td><b>77.21</b></td>\n    <td>85.58</td>\n    <td>85.48</td>\n    <td>78.22</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B-Chat</td>\n    <td>88.55</td>\n    <td><b>92.27</b></td>\n    <td>84.51</td>\n    <td>82.82</td>\n    <td><b>88.59</b></td>\n    <td>76.79</td>\n    <td><b>85.96</b></td>\n    <td><b>86.32</b></td>\n    <td>-</td>\n  <tr>\n    <td rowspan=\"3\">Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>G-DINO-L</td>\n    <td>90.56</td>\n    <td>93.19</td>\n    <td>88.24</td>\n    <td>82.75</td>\n    <td>88.95</td>\n    <td>75.92</td>\n    <td>86.13</td>\n    <td>87.02</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>UNINEXT-H</td>\n    <td>92.64 </td>\n    <td>94.33</td>\n    <td>91.46</td>\n    <td>85.24</td>\n    <td>89.63</td>\n    <td>79.79</td>\n    <td>88.73</td>\n    <td>89.37</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>ONE-PEACE</td>\n    <td>92.58 </td>\n    <td>94.18</td>\n    <td>89.26</td>\n    <td>88.77</td>\n    <td>92.21</td>\n    <td>83.23</td>\n    <td>89.22</td>\n    <td>89.27</td>\n    <td>-</td>\n  </tr>\n</tbody>\n</table>\n\n- Qwen-VLì€ ìœ„ì˜ ëª¨ë“  ì°¸ì¡° í‘œí˜„ ì´í•´ë„ ë²¤ì¹˜ë§ˆí¬ì—ì„œ **SOTA**ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.\n- Qwen-VLì€ ì¤‘êµ­ì–´ ìë§‰ ë°ì´í„°ì— ëŒ€í•´ í•™ìŠµë˜ì§€ ì•Šì•˜ì§€ë§Œ, ì¤‘êµ­ì–´ ìë§‰ ë°ì´í„°ì™€ ì˜ì–´ ìë§‰ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ì œë¡œ ìƒ· ë°©ì‹ìœ¼ë¡œ ì¤‘êµ­ì–´ ìë§‰ ì‘ì—…ì— ì¼ë°˜í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nì‹¤í—˜ ê²°ê³¼ë¥¼ ì¬í˜„í•˜ê¸° ìœ„í•´ ìœ„ì˜ ëª¨ë“  í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [eval_mm/EVALUATION.md](eval_mm/EVALUATION.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n\n### Chat evaluation\n\nTouchStoneì€ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ëŒ€í™” ë° ì‚¬ëŒê³¼ì˜ ì¼ì¹˜ ìˆ˜ì¤€ì— ëŒ€í•œ LVLM ëª¨ë¸ì˜ ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ GPT4ë¡œ ì ìˆ˜ë¥¼ ë§¤ê¸°ëŠ” ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤. ì´ 300ê°œ ì´ìƒì˜ ì´ë¯¸ì§€, 800ê°œ ì´ìƒì˜ ì§ˆë¬¸, ì†ì„± ê¸°ë°˜ Q&A, ìœ ëª…ì¸ ì¸ì‹, ì‹œ ì“°ê¸°, ì—¬ëŸ¬ ì´ë¯¸ì§€ ìš”ì•½, ì œí’ˆ ë¹„êµ, ìˆ˜í•™ ë¬¸ì œ í’€ì´ ë“± 27ê°œ ì¹´í…Œê³ ë¦¬ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [í„°ì¹˜ìŠ¤í†¤/README.md](í„°ì¹˜ìŠ¤í†¤/README.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n\n#### English evaluation\n\n| Model            | Score |\n| ---------------- | ----- |\n| PandaGPT         | 488.5 |\n| MiniGPT4         | 531.7 |\n| InstructBLIP     | 552.4 |\n| LLaMA-AdapterV2  | 590.1 |\n| LLaVA            | 602.7 |\n| mPLUG-Owl        | 605.4 |\n| Qwen-VL-Chat     | 645.2 |\n| Qwen-VL-Chat-1.1 | 711.6 |\n\n#### Chinese evaluation\n\n| Model            | Score |\n| ---------------- | ----- |\n| VisualGLM        | 247.1 |\n| Qwen-VL-Chat     | 401.2 |\n| Qwen-VL-Chat-1.1 | 481.7 |\n\nQwen-VL-Chatì€ ì¤‘êµ­ì–´ì™€ ì˜ì–´ ì •ë ¬ í‰ê°€ì—ì„œ ëª¨ë‘ ìµœê³ ì˜ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.\n\n### Other Benchmarks\n\n#### MME Benchmark\n\n[MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)ëŠ” ë©€í‹°ëª¨ë‹¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ í‰ê°€ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤. ì¡´ì¬, ìˆ˜, ìœ„ì¹˜, ìƒ‰ìƒ, í¬ìŠ¤í„°, ìœ ëª…ì¸, ì¥ë©´, ëœë“œë§ˆí¬, ì˜ˆìˆ í’ˆ, OCR, ìƒì‹ ì¶”ë¡ , ìˆ«ì ê³„ì‚°, í…ìŠ¤íŠ¸ ë²ˆì—­, ì½”ë“œ ì¶”ë¡  ë“± ì´ 14ê°œì˜ í•˜ìœ„ ê³¼ì œì— ëŒ€í•œ ì§€ê°ê³¼ ì¸ì§€ ëŠ¥ë ¥ì„ ëª¨ë‘ ì¸¡ì •í•©ë‹ˆë‹¤.\n\nQwen-VL-Chatì€ ì§€ê°ê³¼ ì¸ì§€ í‰ê°€ ëª¨ë‘ì—ì„œ SOTAë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](eval_mm/mme/EVAL_MME.md)ì—ì„œ í™•ì¸í•˜ì„¸ìš”.\n\n<p align=\"center\">\n    <img src=\"eval_mm/mme/perception.jpg\" width=\"600\"/>\n<p>\n<p align=\"center\">\n    <img src=\"eval_mm/mme/cognition.jpg\" width=\"600\"/>\n<p>\n\n#### SEED-Bench\n\n[SEED-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard)ëŠ” **ì´ë¯¸ì§€** ë° **ë™ì˜ìƒ** ì´í•´ë„ë¥¼ í¬í•¨í•œ 12ê°€ì§€ í‰ê°€ ì°¨ì›ì„ í¬ê´„í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ LLMì„ í‰ê°€í•˜ê¸° ìœ„í•œ ì •í™•í•œ ì‚¬ëŒì˜ ì£¼ì„ì´ í¬í•¨ëœ 19K ê°œì˜ ê°ê´€ì‹ ë¬¸í•­ìœ¼ë¡œ êµ¬ì„±ëœ ë©€í‹°ëª¨ë‹¬ ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ì—¬ê¸°](eval_mm/seed_bench/EVAL_SEED.md)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ Qwen-VLê³¼ Qwen-VL-Chatì€ SOTAë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.\n\n<p align=\"center\">\n    <img src=\"eval_mm/seed_bench/leaderboard.jpg\"/>\n<p>\n\n## Requirements\n\n* python 3.8 and above\n* pytorch 1.12 and above, 2.0 and above are recommended\n* CUDA 11.4 and above are recommended (this is for GPU users)\n  <br>\n\n## Quickstart\n\nì•„ë˜ì—ì„œëŠ” ğŸ¤– ëª¨ë¸ìŠ¤ì½”í”„ ë° ğŸ¤— íŠ¸ëœìŠ¤í¬ë¨¸ì™€ í•¨ê»˜ Qwen-VL ë° Qwen-VL-Chatì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ê°„ë‹¨í•œ ì˜ˆì œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n\nì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— í™˜ê²½ì„ ì„¤ì •í•˜ê³  í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ìœ„ì˜ ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•˜ëŠ”ì§€ í™•ì¸í•œ ë‹¤ìŒ ì¢…ì† ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.\n```bash\npip install -r requirements.txt\n```\n\nì´ì œ ëª¨ë¸ìŠ¤ì½”í”„ ë˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¹„ì „ ì¸ì½”ë”ì— ëŒ€í•œ ìì„¸í•œ ì‚¬ìš©ë²•ì€ [íŠœí† ë¦¬ì–¼](TUTORIAL.md)ì„ ì°¸ì¡°í•˜ì„¸ìš”.\n\n#### ğŸ¤— Transformers\n\nì¶”ë¡ ì— Qwen-VL-Chatì„ ì‚¬ìš©í•˜ë ¤ë©´ ì•„ë˜ì— ì„¤ëª…ëœ ëŒ€ë¡œ ëª‡ ì¤„ì˜ ì½”ë“œë¥¼ ì…ë ¥í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤. ë‹¨, **ìµœì‹  ì½”ë“œë¥¼ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”**.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use cuda device\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# 1st dialogue turn\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url\n    {'text': 'è¿™æ˜¯ä»€ä¹ˆ?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n# å›¾ä¸­æ˜¯ä¸€åå¥³å­åœ¨æ²™æ»©ä¸Šå’Œç‹—ç©è€ï¼Œæ—è¾¹æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ï¼Œå®ƒä»¬å¤„äºæ²™æ»©ä¸Šã€‚\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, 'æ¡†å‡ºå›¾ä¸­å‡»æŒçš„ä½ç½®', history=history)\nprint(response)\n# <ref>å‡»æŒ</ref><box>(536,509),(588,602)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('1.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n\n<details>\n  <summary>Running Qwen-VL</summary>\n\nQwen-VL pretrained base modelì„ ì‹¤í–‰í•˜ëŠ” ê²ƒë„ ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use cuda device\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation (No need to do this if you are using transformers>4.32.0)\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url\n    {'text': 'Generate the caption in English with grounding:'},\n])\ninputs = tokenizer(query, return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nresponse = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)\nprint(response)\n# <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>Generate the caption in English with grounding:<ref> Woman</ref><box>(451,379),(731,806)</box> and<ref> her dog</ref><box>(219,424),(576,896)</box> playing on the beach<|endoftext|>\nimage = tokenizer.draw_bbox_on_latest_picture(response)\nif image:\n  image.save('2.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_spotting_caption.jpg\" width=\"500\"/>\n<p>\n\n</details>\n\n\nHuggingFaceì—ì„œ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì™€ ì½”ë“œë¥¼ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ë™ì•ˆ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œê°€ ë°œìƒí•˜ëŠ” ê²½ìš°, ì•„ë˜ì— ì„¤ëª…ëœ ëŒ€ë¡œ ëª¨ë¸ìŠ¤ì½”í”„ì—ì„œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¨¼ì € ê°€ì ¸ì˜¨ ë‹¤ìŒ ë¡œì»¬ ë””ë ‰í„°ë¦¬ì—ì„œ ë¡œë“œí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-VL')\nmodel_dir = snapshot_download('qwen/Qwen-VL-Chat')\n\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"cuda\",\n    trust_remote_code=True\n).eval()\n```\n\n#### ğŸ¤– ModelScope\n\nModelScopeëŠ” ì„œë¹„ìŠ¤í˜• ëª¨ë¸(MaaS)ì„ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”Œë«í¼ìœ¼ë¡œ, AI ê°œë°œìì—ê²Œ ìœ ì—°í•˜ê³  ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ ì•„ë˜ì™€ ê°™ì´ ModelScopeë¡œ ëª¨ë¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```python\nfrom modelscope import (\n    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n)\nimport torch\nmodel_id = 'qwen/Qwen-VL-Chat'\nrevision = 'v1.0.0'\n\nmodel_dir = snapshot_download(model_id, revision=revision)\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nif not hasattr(tokenizer, 'model_dir'):\n    tokenizer.model_dir = model_dir\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation (No need to do this if you are using transformers>=4.32.0)\n# model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)\n\n# 1st dialogue turn\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)\nprint(response)\n# å›¾ä¸­æ˜¯ä¸€åå¹´è½»å¥³å­åœ¨æ²™æ»©ä¸Šå’Œå¥¹çš„ç‹—ç©è€ï¼Œç‹—çš„å“ç§æ˜¯æ‹‰å¸ƒæ‹‰å¤šã€‚å¥¹ä»¬ååœ¨æ²™æ»©ä¸Šï¼Œç‹—çš„å‰è…¿æŠ¬èµ·æ¥ï¼Œä¸äººäº’åŠ¨ã€‚\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, 'è¾“å‡ºå‡»æŒçš„æ£€æµ‹æ¡†', history=history)\nprint(response)\n# <ref>\"å‡»æŒ\"</ref><box>(211,412),(577,891)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('output_chat.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n<br>\n\n## Quantization\n\n### Usage\n[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ìƒˆë¡œìš´ ì†”ë£¨ì…˜ì„ ì œê³µí•˜ê³ , ê±°ì˜ ë¬´ì†ì‹¤ ëª¨ë¸ íš¨ê³¼ë¥¼ ë‹¬ì„±í•˜ë©´ì„œë„ ë©”ëª¨ë¦¬ ë¹„ìš©ê³¼ ì¶”ë¡  ì†ë„ ëª¨ë‘ì—ì„œ ì„±ëŠ¥ì´ í–¥ìƒëœ Qwen-VL-Chatìš© Int4 ì–‘ìí™” ëª¨ë¸ì¸ [Qwen-VL-Chat-Int4](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4)ë¥¼ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤.\n\nì—¬ê¸°ì—ì„œëŠ” ì œê³µëœ ì–‘ìí™”ëœ ëª¨ë¸ì„ ì¶”ë¡ ì— ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‹œì‘í•˜ê¸° ì „ì— ìš”êµ¬ ì‚¬í•­(ì˜ˆ: torch 2.0 ì´ìƒ, transformers 4.32.0 ì´ìƒ ë“±) ë° í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì œëŒ€ë¡œ ì„¤ì¹˜í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n\n```bash\npip install optimum\ngit clone https://github.com/JustinLin610/AutoGPTQ.git & cd AutoGPTQ\npip install -v .\n```\n\në§Œì•½ 'auto-gptq' ì„¤ì¹˜ì— ë¬¸ì œê°€ ìˆë‹¤ë©´, ê³µì‹ [repo](https://github.com/PanQiWei/AutoGPTQ)ì—ì„œ íœ ì„ ì°¾ì•„ë³´ì‹œê¸¸ ê¶Œì¥í•©ë‹ˆë‹¤.\n\nê·¸ëŸ¬ë©´ ì •ëŸ‰í™”ëœ ëª¨ë¸ì„ ì‰½ê²Œ ë¡œë“œí•˜ê³  í‰ì†Œì™€ ë™ì¼í•˜ê²Œ ì¶”ë¡ ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-VL-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>è¿™æ˜¯ä»€ä¹ˆ', history=None)\nprint(response)\n```\n\n### Performance\n\n [TouchStone](https://github.com/OFA-Sys/TouchStone)ë²¤ì¹˜ë§ˆí¬ì—ì„œ BF16 ë° Int4 ëª¨ë¸ì˜ ëª¨ë¸ ì„±ëŠ¥ì„ ì‚´í´ë³¸ ê²°ê³¼, ì–‘ìí™”ëœ ëª¨ë¸ì—ì„œ ì„±ëŠ¥ ì €í•˜ê°€ í¬ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\n| Quantization | ZH         | EN            |\n| ------------ | :--------: | :-----------: | \n| BF16         | 401.2      |    645.2      |\n| Int4         | 386.6      |    651.4      |\n\n### Inference Speed\n\nì´ë¯¸ì§€ì˜ ì»¨í…ìŠ¤íŠ¸(258ê°œì˜ í† í°ì´ í•„ìš”í•œ)ë¥¼ ê°€ì§€ê³  ê°ê° 1792ê°œ(2048-258ê°œ), 7934ê°œ(8192-258ê°œ)ì˜ í† í°ì„ ìƒì„±í•˜ëŠ” í‰ê·  ì¶”ë¡  ì†ë„(í† í°/ì´ˆ)ë¥¼ BF16 ì •ë°€ë„ì™€ Int4 ì–‘ìí™” í•˜ì—ì„œ ì¸¡ì •í–ˆìŠµë‹ˆë‹¤.\n\n| Quantization | Speed (2048 tokens) | Speed (8192 tokens) |\n| ------------ | :-----------------: | :-----------------: |\n| BF16         |        28.87        |        24.32        |\n| Int4         |        37.79        |        34.34        |\n\ní”„ë¡œíŒŒì¼ë§ì€ PyTorch 2.0.1 ë° CUDA 11.4ê°€ íƒ‘ì¬ëœ ë‹¨ì¼ A100-SXM4-80G GPUì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n\n### GPU Memory Usage\n\në˜í•œ 1792ê°œ(2048-258ê°œ)ì˜ í† í°(ì´ë¯¸ì§€ í¬í•¨)ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¸ì½”ë”©í•˜ê³  ë‹¨ì¼ í† í°ì„ ìƒì„±í•  ë•Œì™€ 7934ê°œ(8192-258ê°œ)ì˜ í† í°(ì´ë¯¸ì§€ê°€ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬í•¨)ì„ ìƒì„±í•  ë•Œ ê°ê° BF16 ë˜ëŠ” Int4 ì–‘ìí™” ìˆ˜ì¤€ì—ì„œ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í”„ë¡œíŒŒì¼ë§í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\n| Quantization | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |\n| ------------ | :---------------------------------: | :-----------------------------------: |\n| BF16         |               22.60GB               |                28.01GB                |\n| Int4         |               11.82GB               |                17.23GB                |\n\nìœ„ì˜ ì†ë„ ë° ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ì€ [ì´ ìŠ¤í¬ë¦½íŠ¸](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile_mm.py)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.\n<br>\n\n## Finetuning\n\nì´ì œ ì‚¬ìš©ìê°€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•´ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆë„ë¡ ê³µì‹ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì¸ `finetune.py`ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë˜í•œ, ê±±ì • ì—†ì´ ë¯¸ì„¸ ì¡°ì •ì„ ì‹œì‘í•  ìˆ˜ ìˆëŠ” ì…¸ ìŠ¤í¬ë¦½íŠ¸ë„ ì œê³µí•©ë‹ˆë‹¤. ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë”¥ìŠ¤í”¼ë“œì™€ FSDPë¥¼ í†µí•œ í•™ìŠµì„ ì§€ì›í•©ë‹ˆë‹¤. ì œê³µë˜ëŠ” ì…¸ ìŠ¤í¬ë¦½íŠ¸ëŠ” DeepSpeedë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì‹œì‘í•˜ê¸° ì „ì— DeepSpeedë¥¼ ì„¤ì¹˜í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n\n```bash\npip install deepspeed\n```\n\n### Data preparation\ní•™ìŠµ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ë ¤ë©´ ëª¨ë“  ìƒ˜í”Œì„ ëª©ë¡ì— ë„£ê³  json íŒŒì¼ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤. ê° ìƒ˜í”Œì€ IDì™€ ëŒ€í™” ëª©ë¡ìœ¼ë¡œ êµ¬ì„±ëœ ì‚¬ì „ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ìƒ˜í”Œ 1ê°œê°€ í¬í•¨ëœ ê°„ë‹¨í•œ ì˜ˆì œ ëª©ë¡ì…ë‹ˆë‹¤.\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"ä½ å¥½\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"æˆ‘æ˜¯Qwen-VL,ä¸€ä¸ªæ”¯æŒè§†è§‰è¾“å…¥çš„å¤§æ¨¡å‹ã€‚\"\n      }\n    ]\n  },\n  {\n    \"id\": \"identity_1\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>\\nå›¾ä¸­çš„ç‹—æ˜¯ä»€ä¹ˆå“ç§ï¼Ÿ\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"å›¾ä¸­æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚\"\n      },\n      {\n        \"from\": \"user\",\n        \"value\": \"æ¡†å‡ºå›¾ä¸­çš„æ ¼å­è¡¬è¡«\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"<ref>æ ¼å­è¡¬è¡«</ref><box>(588,499),(725,789)</box>\"\n      }\n    ]\n  },\n  { \n    \"id\": \"identity_2\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>assets/mm_tutorial/Chongqing.jpeg</img>\\nPicture 2: <img>assets/mm_tutorial/Beijing.jpeg</img>\\nå›¾ä¸­éƒ½æ˜¯å“ª\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"ç¬¬ä¸€å¼ å›¾ç‰‡æ˜¯é‡åº†çš„åŸå¸‚å¤©é™…çº¿ï¼Œç¬¬äºŒå¼ å›¾ç‰‡æ˜¯åŒ—äº¬çš„å¤©é™…çº¿ã€‚\"\n      }\n    ]\n  }\n]\n```\nVL ì‘ì—…ì—ì„œëŠ” `<img> </img> <ref> </ref> <box> </box>`ë“±ê³¼ ê°™ì€ íŠ¹ìˆ˜ í† í°ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. \n\nì´ë¯¸ì§€ëŠ” `Picture id: <img>img_path</img>\\n{your prompt}`ë¡œ í‘œì‹œë˜ë©°, ì—¬ê¸°ì„œ `id`ëŠ” ëŒ€í™”ì—ì„œ ì´ë¯¸ì§€ì˜ ìœ„ì¹˜(1ë¶€í„° ì‹œì‘)ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. `img_path`ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ì›¹ ë§í¬ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\në°•ìŠ¤ì˜ ì¢Œí‘œëŠ” `<box>(x1,y1),(x2,y2)</box>`ë¡œ í‘œì‹œë˜ëŠ”ë°, ì—¬ê¸°ì—ì„œ `(x1, y1)`ê³¼ `(x2, y2)`ì˜ ì¢Œí‘œëŠ” `[0, 1000)`ìœ¼ë¡œ ì •ê·œí™”ë˜ê²Œ ë©ë‹ˆë‹¤. í•´ë‹¹ í…ìŠ¤íŠ¸ ì„¤ëª…ì€ `<ref>text_caption</ref>`ê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\në°ì´í„° ì¤€ë¹„ í›„ ì œê³µëœ ì…¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„° íŒŒì¼ ê²½ë¡œì¸ `$DATA`ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”.\n\në¯¸ì„¸ ì¡°ì • ìŠ¤í¬ë¦½íŠ¸ë¥¼ í†µí•´ ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n- Full-parameter finetuning\n- LoRA\n- Q-LoRA\n\n### Full-parameter finetuning\nì „ì²´ íŒŒë¼ë¯¸í„°ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ì „ì²´ í›ˆë ¨ ê³¼ì •ì—ì„œ LLMì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤. ì‹¤í—˜ ê²°ê³¼, ë¯¸ì„¸ ì¡°ì • ë‹¨ê³„ì—ì„œ **ViTì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë™ê²°(frozening)í•˜ë©´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.** í›ˆë ¨ì„ ì‹œì‘í•˜ë ¤ë©´ ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\n\n```bash\nsh finetune/finetune_ds.sh\n```\n\nì…¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì˜¬ë°”ë¥¸ ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ, ë°ì´í„° ê²½ë¡œ, ì¶œë ¥ ë””ë ‰í„°ë¦¬ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”. ë³€ê²½í•˜ë ¤ë©´ `--deepspeed` ì¸ìˆ˜ë¥¼ ì œê±°í•˜ê±°ë‚˜ ìš”êµ¬ ì‚¬í•­ì— ë”°ë¼ DeepSpeed êµ¬ì„± json íŒŒì¼ì„ ë³€ê²½í•˜ë©´ ë©ë‹ˆë‹¤. ë˜í•œ, ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í˜¼í•© ì •ë°€ë„ í›ˆë ¨ì„ ì§€ì›í•˜ë¯€ë¡œ `--bf16 True` ë˜ëŠ” `--fp16 True`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê²½í—˜ì ìœ¼ë¡œ ë¨¸ì‹ ì´ bf16ì„ ì§€ì›í•˜ëŠ” ê²½ìš° ì‚¬ì „ í›ˆë ¨ ë° ì •ë ¬ê³¼ ì¼ê´€ëœ í›ˆë ¨ì„ ìœ„í•´ bf16ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìœ¼ë©°, ë”°ë¼ì„œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n\n### LoRA\në§ˆì°¬ê°€ì§€ë¡œ LoRAë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ì™€ ê°™ì´ ë‹¤ë¥¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤. ì‹œì‘í•˜ê¸° ì „ì— `peft`ë¥¼ ì„¤ì¹˜í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ë˜í•œ ëª¨ë¸, ë°ì´í„°, ì¶œë ¥ì— ëŒ€í•œ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì—ëŠ” ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. LoRAëŠ” ì–´ëŒ‘í„°ë§Œ ì €ì¥í•˜ê³  ì–´ëŒ‘í„° êµ¬ì„± json íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œëŠ” ë¡œë“œí•  ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ëŠ” ë° ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\n```bash\n# Single GPU training\nsh finetune/finetune_lora_single_gpu.sh\n# Distributed training\nsh finetune/finetune_lora_ds.sh\n```\n\nì „ì²´ ë§¤ê°œë³€ìˆ˜ ë¯¸ì„¸ ì¡°ì •ê³¼ ë¹„êµí•  ë•Œ LoRA([paper](https://arxiv.org/abs/2106.09685))ëŠ” ì–´ëŒ‘í„° ë ˆì´ì–´ì˜ ë§¤ê°œë³€ìˆ˜ë§Œ ì—…ë°ì´íŠ¸í•˜ê³  ì›ë˜ì˜ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ë ˆì´ì–´ëŠ” ê³ ì •ëœ ìƒíƒœë¡œ ìœ ì§€í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë©”ëª¨ë¦¬ ë¹„ìš©ì´ í›¨ì”¬ ì ê²Œ ë“¤ê³  ê³„ì‚° ë¹„ìš©ë„ ì ê²Œ ë“­ë‹ˆë‹¤. \n\nLoRAë¥¼ ì‚¬ìš©í•˜ì—¬ ì±„íŒ… ëª¨ë¸ ëŒ€ì‹  ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸(ì˜ˆ: Qwen-VL)ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²½ìš°, ìŠ¤í¬ë¦½íŠ¸ëŠ” ì„ë² ë”© ë° ì¶œë ¥ ë ˆì´ì–´ë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤. ì´ëŠ” ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸ì— ChatML í˜•ì‹ì—ì„œ ê°€ì ¸ì˜¨ íŠ¹ìˆ˜ í† í°ì— ëŒ€í•œ ì§€ì‹ì´ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì´ í† í°ì„ ì´í•´í•˜ê³  ì˜ˆì¸¡í•˜ë ¤ë©´ ì´ëŸ¬í•œ ë ˆì´ì–´ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë§í•´, í•™ìŠµì´ LoRAì—ì„œ íŠ¹ìˆ˜ í† í°ì„ ê°€ì ¸ì˜¤ëŠ” ê²½ìš° ì½”ë“œ ë‚´ì—ì„œ `modules_to_save`ë¥¼ ì„¤ì •í•˜ì—¬ ë ˆì´ì–´ë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ ì´ëŸ¬í•œ íŠ¸ë ˆì´ë‹ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš° LoRAì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì—ëŠ” ìƒë‹¹í•œ ì°¨ì´ê°€ ìˆìŒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë©”ëª¨ë¦¬ì— ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° LoRAì—ì„œ ì±„íŒ… ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ í”„ë¡œí•„ì„ í™•ì¸í•˜ì„¸ìš”.\n\n### Q-LoRA\nê·¸ëŸ¬ë‚˜ ì—¬ì „íˆ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë‹¤ë©´ ì–‘ìí™”ëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ê³¼ í˜ì´ì§• ì£¼ì˜ì™€ ê°™ì€ ê¸°íƒ€ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ë¹„ìš©ì„ í›¨ì”¬ ë” ì ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Q-LoRA([paper](https://arxiv.org/abs/2305.14314))ë¥¼ ê³ ë ¤í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Q-LoRAë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•˜ì„¸ìš”.\n\n```bash\n# Single GPU training\nsh finetune/finetune_qlora_single_gpu.sh\n# Distributed training\nsh finetune/finetune_qlora_ds.sh\n```\n\nQ-LoRAì˜ ê²½ìš°, ë‹¹ì‚¬ì—ì„œ ì œê³µí•˜ëŠ” ì •ëŸ‰í™”ëœ ëª¨ë¸(ì˜ˆ: Qwen-VL-Chat-Int4)ì„ ë¡œë“œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. \nbf16 ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. ì „ì²´ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì • ë° LoRAì™€ ë‹¬ë¦¬ Q-LoRAì—ëŠ” fp16ë§Œ ì§€ì›ë©ë‹ˆë‹¤. ë˜í•œ Q-LoRAì˜ ê²½ìš° LoRAì˜ íŠ¹ìˆ˜ í† í°ì— ëŒ€í•œ ë¬¸ì œê°€ ì—¬ì „íˆ ì¡´ì¬í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì €í¬ëŠ” ì±„íŒ… ëª¨ë¸ì— Int4 ëª¨ë¸ë§Œ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ì–¸ì–´ ëª¨ë¸ì´ ChatML í˜•ì‹ì˜ íŠ¹ìˆ˜ í† í°ì„ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì— ë ˆì´ì–´ì— ëŒ€í•œ ê±±ì •ì€ í•˜ì§€ ì•Šìœ¼ì…”ë„ ë©ë‹ˆë‹¤. ë‹¨, Int4 ëª¨ë¸ì˜ ë ˆì´ì–´ëŠ” í•™ìŠµí•  ìˆ˜ ì—†ì–´ì•¼ í•˜ë¯€ë¡œ í•™ìŠµì— íŠ¹ìˆ˜ í† í°ì„ ë„ì…í•˜ë©´ Q-LoRAê°€ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì „ì²´ ë§¤ê°œë³€ìˆ˜ ë¯¸ì„¸ ì¡°ì •ê³¼ ë‹¬ë¦¬ LoRA ë° Q-LoRAì˜ í›ˆë ¨ì€ ì–´ëŒ‘í„° ë§¤ê°œë³€ìˆ˜ë§Œ ì €ì¥í•©ë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ ì¶”ë¡ ì„ ìœ„í•´ ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\nì–´ëŒ‘í„°ë¥¼ ë³‘í•©í•˜ê³  ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸ì„ ë…ë¦½í˜• ëª¨ë¸ë¡œ ì €ì¥í•˜ë ¤ë©´(ì´ ì‘ì—…ì€ LoRAì—ì„œë§Œ ê°€ëŠ¥í•˜ë©° Q-LoRAì—ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ë³‘í•©í•  ìˆ˜ ì—†ìŒ) ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_sizeì™€ ì•ˆì „í•œ ì§ë ¬í™”ëŠ” í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \n# ì´ë“¤ì€ ê°ê° ìƒ¤ë”© ì²´í¬í¬ì¸íŠ¸ì— ëŒ€í•´ ì‘ë™í•˜ê³  ëª¨ë¸ì„ ì„¸ì´í”„í…ì„œì— ì €ì¥í•©ë‹ˆë‹¤.\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n\n\n```\n\nì°¸ê³ : ë©€í‹° GPU íŠ¸ë ˆì´ë‹ì˜ ê²½ìš°, ë¨¸ì‹ ì— ë”°ë¼ ë¶„ì‚° íŠ¸ë ˆì´ë‹ì— ì í•©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ ë°ì´í„°, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, í›ˆë ¨ ì†ë„ ë“±ì„ ê³ ë ¤í•˜ì—¬ --model_max_length ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n\n\n### Profiling of Memory and Speed\në‹¨ì¼ GPU íŠ¸ë ˆì´ë‹ ì„¤ì •ì—ì„œ ì„ë² ë”© ë° ì¶œë ¥ ë ˆì´ì–´ë¥¼ íŠ¸ë ˆì´ë‹í•˜ëŠ” LoRA(Base)ì™€ ì„ë² ë”© ë° ì¶œë ¥ ë ˆì´ì–´ë¥¼ íŠ¸ë ˆì´ë‹í•  ìˆ˜ ì—†ëŠ” LoRA(Chat)ì˜ GPU ë©”ëª¨ë¦¬ ë° íŠ¸ë ˆì´ë‹ ì†ë„ë¥¼ í”„ë¡œíŒŒì¼ë§í•©ë‹ˆë‹¤. ì´ í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ë‹¨ì¼ A100-SXM4-80G GPUì—ì„œ ì‹¤í—˜í–ˆìœ¼ë©°, CUDA 11.8ê³¼ Python 2.0ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ëŠ” 1, ê·¸ë¼ë°ì´ì…˜ ëˆ„ì ì€ 8ì„ ê· ì¼í•˜ê²Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ê° ìƒ˜í”Œì—ëŠ” ì´ë¯¸ì§€ê°€ í¬í•¨ë©ë‹ˆë‹¤. 384, 512, 1024, 2048 ë“± ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ì…ë ¥ì— ëŒ€í•œ ë©”ëª¨ë¦¬(GB)ì™€ ì†ë„(s/iter)ë¥¼ í”„ë¡œíŒŒì¼ë§í•©ë‹ˆë‹¤. í†µê³„ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n\n\n<table>\n    <tr>\n <th rowspan=\"2\">Method</th><th colspan=\"4\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">384</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th>\n    </tr>\n    <tr>\n      <td>LoRA (Base)</td><td align=\"center\">37.1G / 2.3s/it</td><td align=\"center\">37.3G / 2.4s/it</td><td align=\"center\">38.7G / 3.6s/it</td><td align=\"center\">38.7G / 6.1s/it</td>\n    </tr>\n    <tr>\n      <td>LoRA (Chat)</td><td align=\"center\">23.3G / 2.2s/it</td><td align=\"center\">23.6G / 2.3s/it</td><td align=\"center\">25.1G / 3.5s/it</td><td align=\"center\">27.3G / 5.9s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">17.0G / 4.2s/it</td><td align=\"center\">17.2G / 4.5s/it</td><td align=\"center\">18.2G / 5.5s/it</td><td align=\"center\">19.3G / 7.9s/it</td>\n    </tr>\n\n</table>\n\n<br>\n\n## Demo\n\n### Web UI\n\nì‚¬ìš©ìê°€ ì›¹ UI ë°ëª¨ë¥¼ ë¹Œë“œí•  ìˆ˜ ìˆëŠ” ì½”ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n\n```\npip install -r requirements_web_demo.txt\n```\n\nThen run the command below and click on the generated link:\n\n```\npython web_demo_mm.py\n```\n\n<br>\n\n## FAQ\n\në¬¸ì œê°€ ë°œìƒí•˜ë©´ ìƒˆ ì´ìŠˆë¥¼ ì‹œì‘í•˜ê¸° ì „ì— ë¨¼ì € [ìì£¼ ë¬»ëŠ” ì§ˆë¬¸](FAQ.md)ê³¼ ì´ìŠˆë¥¼ ì°¸ì¡°í•˜ì—¬ í•´ê²° ë°©ë²•ì„ ì°¾ì•„ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.\n<br>\n\n## License Agreement\n\nì—°êµ¬ìì™€ ê°œë°œìëŠ” Qwen-VLê³¼ Qwen-VL-Chatì˜ ì½”ë“œì™€ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ììœ ë¡­ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ìƒì—…ì  ì‚¬ìš©ë„ í—ˆìš©ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](ë¼ì´ì„¼ìŠ¤)ì—ì„œ ë¼ì´ì„¼ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n<br>\n\n## Citation\n\nì €í¬ ë…¼ë¬¸ê³¼ ì½”ë“œê°€ ì—¬ëŸ¬ë¶„ì˜ ì—°êµ¬ì— ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ star:star: ì™€ ì¸ìš©:pencil: í•´ì£¼ì‹œë©´ ê°ì‚¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤. :)\n\n```BibTeX\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n\n<br>\n\n## Contact Us\n\nì—°êµ¬íŒ€ì´ë‚˜ ì œí’ˆíŒ€ì— ë©”ì‹œì§€ë¥¼ ë‚¨ê¸°ê³  ì‹¶ìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì´ë©”ì¼(qianwen_opensource@alibabacloud.com)ì„ ë³´ë‚´ì£¼ì„¸ìš”."
        },
        {
          "name": "TUTORIAL.md",
          "type": "blob",
          "size": 13.0771484375,
          "content": "# Qwen-VL-Chat Tutorial\nQwen-VL-Chat is a generalist multimodal large-scale language model, and it can perform a wide range of vision-language tasks. In this tutorial, we will give some concise examples to demonstrate the capabilities of Qwen-VL-Chat in **Visual Question Answering, Text Understanding, Mathematical Reasoning with Diagrams, Multi-Figure Reasoning, and Grounding**. Please note that the examples shown are far from the limit of Qwen-VL-Chat's capabilities, **you can further explore Qwen-VL-Chat's capabilities by changing the input images and prompts!**\n\n## Initializing the Qwen-VL-Chat model\nBefore you can use Qwen-VL-Chat, you first need to initialize Qwen-VL-Chat's tokenizer and Qwen-VL-Chat's model:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# If you expect the results to be reproducible, set a random seed.\n# torch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n```\nAfter executing the above code, ```tokenizer``` will correspond to the classifier used by Qwen-VL-Chat, while ```model``` will correspond to the model of Qwen-VL-Chat. The ```tokenizer``` is used for preprocessing the interleaved multimodal inputs, while the ```model``` is the Qwen-VL-Chat model itself.\n\n## Using Qwen-VL-Chat\n### **Multi-round visual question answering**\n#### **The first question**\nLet's get started with a simple example. As shown below, the file ```assets/mm_tutorial/Rebecca_(1939_poster).jpeg``` is a poster for the 1940 film Rebecca.\n\n![](assets/mm_tutorial/Rebecca_(1939_poster)_Small.jpeg)\n\nLet's ask what is the name of the film on the Qwen-VL-Chat poster. First of all, we use ```tokenizer.from_list_format``` which can preprocess and tokenize the input:\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Rebecca_(1939_poster).jpeg'},\n    {'text': 'What is the name of the movie in the poster?'},\n])\n```\nNext, we can use ```model.chat``` to ask questions to the Qwen-VL-Chat model and get its response. Note that for the first question, the dialogue history is empty, so we use ```history=None```.\n```python\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\nYou are expected to get an output similar to the following:\n\n> The name of the movie in the poster is \"Rebecca.\"\n\nThis shows that the model correctly answered the given question! According to the poster, the title of the film is \n indeed **Rebecca**.\n\n#### **Multi-round question answering**\nWe can also continue to ask the model other questions, such as who is the director of the film. The dialogue history is not empty for subsequent questions, therefore we use ```history=history``` to pass the history of previous conversations to ``model.chat``:\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Who directed this movie?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\nYou are expected to get an output similar to the following:\n\n> The movie \"Rebecca\" was directed by Alfred Hitchcock.\n\nAgain, the model answered the given question correctly! According to the poster, the director of the film is Alfred Hitchcockã€‚\n\n### **Text Understanding**\nQwen-VL-Chat also has the ability to understand images containing dense text. As shown below, the file ```assets/mm_tutorial/Hospital.jpeg``` is a hospital signage containing dense text.\n\n![](assets/mm_tutorial/Hospital_Small.jpg)\n\nWe can ask questions about the location of different departments in the Hospital. Since the dialogue history is empty, so we use ```history=None```.\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Hospital.jpg'},\n    {'text': 'Based on the photo, which floor is the Department of Otorhinolaryngology on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\nYou are expected to get an output similar to the following:\n\n> The Department of Otorhinolaryngology is located on the 4th floor.\n\nYou can also ask further questions. In this case you need to use ```history=history``` to pass a history of previous conversations to ```model.chat```. \n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Based on the photo, which floor is the Department of Surgery on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\nYou are expected to get an output similar to the following:\n\n> The Department of Surgery is located on the 3rd floor.\n\n### **Mathematical Reasoning with Diagram**\nUsing the model's diagram comprehension and mathematical reasoning capabilities, Qwen-VL-Chat can also perform some more complex tasks! As shown below, the file ```assets/mm_tutorial/Menu.jpeg``` is the menu of a restaurant. Now we want to know how much it would cost to purchase two Salmon Burgers and three Meat Lover's Pizzas.\n\n![](assets/mm_tutorial/Menu.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Menu.jpeg'},\n    {'text': 'How much would I pay if I want to order two Salmon Burger and three Meat Lover\\'s Pizza? Think carefully step by step.'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n```Think carefully step by step.``` is a common prompt that guides the model through complex tasks step by step. So if you have a complex task to complete, try using it to improve the accuracy of the model. You are expected to get an output similar to the following:\n\n> To order two Salmon Burgers and three Meat Lover's Pizzas, you would need to pay the following:\n> \n> 1. For two Salmon Burgers: x2 Salmon Burgers at $10 each = $20\n> 2. For three Meat Lover's Pizzas: x3 Meat Lover's Pizzas at $12 each = $36\n> \n> Therefore, the total cost would be $56.\n\n### **Multi-Figure Reasoning and Chinese Input**\nIn the previous examples, we have demonstrated Qwen-VL-Chat's question-answering capability for a single image and English questions. However, Qwen-VL-Chat is actually a multilingual model that supports Chinese input and multiple images! In the following example, we let Qwen-VL-Chat compare the photos of two cities (Chongqing and Beijing) for us (```assets/mm_tutorial/Chongqing.jpeg``` and ```assets/mm_tutorial/Beijing.jpeg```) in Chinese:\n\n![](assets/mm_tutorial/Chongqing_Small.jpeg)\n\n![](assets/mm_tutorial/Beijing_Small.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Chongqing.jpeg'},\n    {'image': 'assets/mm_tutorial/Beijing.jpeg'},\n    {'text': 'ä¸Šé¢ä¸¤å¼ å›¾ç‰‡åˆ†åˆ«æ˜¯å“ªä¸¤ä¸ªåŸå¸‚ï¼Ÿè¯·å¯¹å®ƒä»¬è¿›è¡Œå¯¹æ¯”ã€‚'},\n])\ntorch.manual_seed(5678)\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\nYou are expected to get an output similar to the following:\n\n> ç¬¬ä¸€å¼ å›¾ç‰‡æ˜¯é‡åº†çš„åŸå¸‚å¤©é™…çº¿ï¼Œå®ƒåæ˜ äº†ç°ä»£éƒ½å¸‚çš„ç¹åä¸å–§åš£ã€‚ç¬¬äºŒå¼ å›¾ç‰‡æ˜¯åŒ—äº¬çš„å¤©é™…çº¿ï¼Œå®ƒè±¡å¾ç€ä¸­å›½é¦–éƒ½çš„ç°ä»£åŒ–å’Œå›½é™…åŒ–ã€‚ä¸¤åº§åŸå¸‚éƒ½æ˜¯ä¸­å›½çš„é‡è¦åŸå¸‚ï¼Œæ‹¥æœ‰ç‹¬ç‰¹çš„æ–‡åŒ–å’Œå‘å±•å†å²ã€‚\n\n**Please note that comparing cities is a fairly subjective question, so the responses generated by the model may be subject to a high degree of randomness. If you do not set the random seed using ```torch.manual_seed(5678)```, the output will be different each time. Even if you set the random seed, the results obtained may still differ from this tutorial due to differences in hardware and software environments.**\n\n### **Grounding Capability**\nIn the last section of the tutorial, we demonstrate the ability of the Qwen-VL-Chat model to produce a bounding box. Qwen-VL-Chat can frame a specified area of an image with a rectangular box according to your language description. This may be a bit abstract, so let's look at the following example. As shown below, the file ```assets/mm_tutorial/Shanghai.jpg``` is a photo of Shanghai, and we'll start by asking the model to describe the image with a regular prompt.\n\n![](assets/mm_tutorial/Shanghai_Small.jpeg)\n\n```python\ntorch.manual_seed(1234)\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Shanghai.jpg'},\n    {'text': 'å›¾é‡Œæœ‰å•¥'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\nYou are expected to get an output similar to the following:\n\n> å›¾ä¸­æ˜¯ä¸­å›½ä¸Šæµ·çš„å¤©é™…çº¿ï¼ŒåŒ…æ‹¬äº†ä¸Šæµ·å¡”ã€é‡‘èŒ‚å¤§å¦ã€ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒã€æµ·æ´‹å¤§å¦ç­‰è‘—åå»ºç­‘ã€‚\n\nNext, let's talk to the model by using the prompt ```è¯·ç»™æˆ‘æ¡†å‡ºå›¾ä¸­ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒå’Œä¸œæ–¹æ˜ç ``` and see what happens. Note that at this point you need to pass the history of previous conversations to ```model.chat``` using ```history=history```.\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'è¯·ç»™æˆ‘æ¡†å‡ºå›¾ä¸­ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒå’Œä¸œæ–¹æ˜ç '},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\nYou are expected to get an output similar to the following:\n```xml\n<ref>ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒ</ref><box>(667,437),(760,874)</box>å’Œ<ref>ä¸œæ–¹æ˜ç </ref><box>(506,75),(582,946)</box>\n```\nThe Qwen-VL-Chat model doesn't have a hand, but it doesn't reject your request either. Instead, it outputs something \"strange\" - In fact, the output of the model gives the location of the ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒï¼ˆShanghai World Financial Centreï¼‰ and the ä¸œæ–¹æ˜ç ï¼ˆOriental Pearl Towerï¼‰ in markup language. You can visualise it using the following code:\n```python\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nimage.save('Shanghai_Output.jpg')\n```\nThe saved ```Shanghai_Output.jpg``` will look similar to the screenshot below: \n\n![](assets/mm_tutorial/Shanghai_Output_Small.jpeg)\n\nAfter that, you can still chat with Qwen-VL-Chat as before:\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'å¸®æˆ‘å†™ä¸ªè¿™åº§åŸå¸‚çš„æ—…æ¸¸è®¡åˆ’'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\nYou are expected to get an output similar to the following:\n\n> å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ä¸Šæµ·æ—…æ¸¸è®¡åˆ’ï¼š\n>\n> ç¬¬ä¸€å¤©ï¼š\n> ä¸Šåˆï¼šæŠµè¾¾ä¸Šæµ·ï¼Œå‰å¾€é…’åº—åŠç†å…¥ä½æ‰‹ç»­ã€‚\n> ä¸­åˆï¼šäº«ç”¨åˆé¤åï¼Œå‰å¾€å¤–æ»©ï¼Œæ¬£èµé»„æµ¦æ±Ÿç•”çš„ç¾æ™¯ï¼Œæ¸¸è§ˆä¸Šæµ·åœ°æ ‡æ€§å»ºç­‘å¦‚æµ¦å‘é“¶è¡Œå¤§æ¥¼ã€æ±‡ä¸°é“¶è¡Œå¤§æ¥¼ç­‰ã€‚\n> ä¸‹åˆï¼šæ¸¸è§ˆå—äº¬è·¯æ­¥è¡Œè¡—ï¼Œè´­ä¹°ç‰¹è‰²ç¤¼å“æˆ–å“å°å½“åœ°ç¾é£Ÿã€‚\n> æ™šä¸Šï¼šåœ¨å—äº¬è·¯é™„è¿‘çš„é¤å…äº«ç”¨æ™šé¤ï¼Œç„¶åå»çœ‹ä¸Šæµ·çš„å¤œæ™¯ã€‚\n>\n> ç¬¬äºŒå¤©ï¼š\n> ä¸Šåˆï¼šå‰å¾€ä¸Šæµ·ç§‘æŠ€é¦†ï¼Œäº†è§£ç§‘æŠ€å‘å±•å†å²ï¼Œè§‚çœ‹å„ç§ç§‘æŠ€å±•è§ˆã€‚\n> ä¸­åˆï¼šåœ¨ç§‘æŠ€é¦†é™„è¿‘çš„é¤å…äº«ç”¨åˆé¤ã€‚\n> ä¸‹åˆï¼šæ¸¸è§ˆä¸–çºªå…¬å›­ï¼Œæ¬£èµç¾æ™¯å¹¶æ”¾æ¾èº«å¿ƒã€‚\n> æ™šä¸Šï¼šåœ¨å—äº¬è·¯æˆ–é™„è¿‘çš„é™†å®¶å˜´åœ°åŒºäº«ç”¨æ™šé¤ï¼Œç„¶åå»çœ‹ä¸Šæµ·çš„å¤œæ™¯ã€‚\n>\n> ç¬¬ä¸‰å¤©ï¼š\n> ä¸Šåˆï¼šæ¸¸è§ˆä¸Šæµ·è¿ªå£«å°¼ä¹å›­æˆ–ä¸Šæµ·æµ·æ˜Œæµ·æ´‹å…¬å›­ï¼Œä¸å„ç§è¿ªå£«å°¼è§’è‰²äº’åŠ¨ï¼Œæˆ–è€…åœ¨æµ·æ´‹å…¬å›­è§‚çœ‹æµ·æ´‹ç”Ÿç‰©è¡¨æ¼”ã€‚\n> ä¸­åˆï¼šåœ¨è¿ªå£«å°¼ä¹å›­æˆ–æµ·æ´‹å…¬å›­é™„è¿‘çš„é¤å…äº«ç”¨åˆé¤ã€‚\n> ä¸‹åˆï¼šè‡ªç”±æ´»åŠ¨ï¼Œå¯ä»¥å»è´­ç‰©ã€å“å°å½“åœ°ç¾é£Ÿæˆ–è€…å»åšç‰©é¦†ç­‰ã€‚\n> æ™šä¸Šï¼šåœ¨é…’åº—é™„è¿‘äº«ç”¨æ™šé¤ï¼Œç„¶åç¦»å¼€ä¸Šæµ·ã€‚\n>\n> å½“ç„¶ï¼Œä»¥ä¸Šåªæ˜¯ä¸€ä¸ªç®€å•çš„è®¡åˆ’ï¼Œä¸Šæµ·æœ‰è®¸å¤šå…¶ä»–æ™¯ç‚¹å’Œæ´»åŠ¨ï¼Œä¾‹å¦‚å‚è§‚ä¸Šæµ·åšç‰©é¦†ã€æ¸¸è§ˆç”°å­åŠã€è§‚çœ‹ä¸Šæµ·è¯å‰§ç­‰ã€‚å…·ä½“è®¡åˆ’å¯ä»¥æ ¹æ®ä¸ªäººå…´è¶£å’Œæ—¶é—´è¿›è¡Œè°ƒæ•´ã€‚\n\n\n**Please note that travel planning is a fairly subjective question, so the responses generated by the model may be subject to a high degree of randomness. If you do not set the random seed using ```torch.manual_seed(1234)```, the output will be different each time. Even if you set the random seed, the results obtained may still differ from this tutorial due to differences in hardware and software environments.**\n\n### Grounded Captioning\nQwen-VL can output the bounding box information of the subject while captioning the image. For example:\n\n```\nimg_url = 'assets/apple.jpeg'\nquery = tokenizer.from_list_format([\n    {'image': img_url},\n    {'text': 'Generate the caption in English with grounding:'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image is not None:\n    image.save('apple.jpg')\n```\n\nThe saved ```apple.jpg``` will look similar to the screenshot below: \n<p align=\"left\">\n    <img src=\"assets/apple_r.jpeg\" width=\"600\"/>\n<p>\n\n#### How to get the caption without any box-like annotations\nSometimes you may expect no box-like annotations in the response. In the case, you can stably get the cleaned text by the following post-processing.\n\n```\n# response = '<ref> Two apples</ref><box>(302,257),(582,671)</box><box>(603,252),(878,642)</box> and<ref> a bowl</ref><box>(2,269),(304,674)</box>'\nimport re\nclean_response = re.sub(r'<ref>(.*?)</ref>(?:<box>.*?</box>)*(?:<quad>.*?</quad>)*', r'\\1', response).strip()\nprint(clean_response)\n# clean_response = 'Two apples and a bowl'\n```\n"
        },
        {
          "name": "TUTORIAL_ja.md",
          "type": "blob",
          "size": 12.8935546875,
          "content": "# Qwen-VL-Chat ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«\nQwen-VL-Chat ã¯æ±ç”¨ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€å¹…åºƒã„è¦–è¦šè¨€èªã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€Qwen-VL-Chat ã®**è¦–è¦šçš„è³ªå•å¿œç­”ã€ãƒ†ã‚­ã‚¹ãƒˆç†è§£ã€å›³ã‚’ç”¨ã„ãŸæ•°å­¦çš„æ¨è«–ã€å¤šè¦–ç‚¹æ¨è«–ã€ãŠã‚ˆã³ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°**ã®æ©Ÿèƒ½ã«ã¤ã„ã¦ã€ã„ãã¤ã‹ã®ç°¡æ½”ãªä¾‹ã‚’æŒ™ã’ã¦èª¬æ˜ã—ã¾ã™ã€‚Qwen-VL-Chat ã¯ã€å…¥åŠ›ç”»åƒã‚„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§ã€Qwen-VL-Chat ã®èƒ½åŠ›ã‚’ã•ã‚‰ã«å¼•ãå‡ºã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n\n## Qwen-VL-Chat ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–\nQwen-VL-Chat ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ã¾ãš Qwen-VL-Chat ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨ Qwen-VL-Chat ã®ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# çµæœã®å†ç¾æ€§ã‚’æœŸå¾…ã™ã‚‹å ´åˆã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®šã™ã‚‹ã€‚\n# torch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n```\nä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€```tokenizer``` ã¯ Qwen-VL-Chat ã§ä½¿ç”¨ã•ã‚Œã‚‹åˆ†é¡å™¨ã«å¯¾å¿œã—ã€```model``` ã¯ Qwen-VL-Chat ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã—ã¾ã™ã€‚```tokenizer``` ã¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–ã•ã‚ŒãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã®å‰å‡¦ç†ã«ä½¿ç”¨ã•ã‚Œã€```model``` ã¯ Qwen-VL-Chat ã®ãƒ¢ãƒ‡ãƒ«ãã®ã‚‚ã®ã§ã™ã€‚\n\n## Qwen-VL-Chat ã‚’ä½¿ã†\n### **è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«è³ªå•å›ç­”**\n#### **æœ€åˆã®è³ªå•**\nç°¡å˜ãªä¾‹ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚ä»¥ä¸‹ã«ç¤ºã™ã‚ˆã†ã«ã€```assets/mm_tutorial/Rebecca_(1939_poster).jpeg``` ã¯ 1940 å¹´ã®æ˜ ç”» ãƒ¬ãƒ™ãƒƒã‚«ã®ãƒã‚¹ã‚¿ãƒ¼ã§ã™ã€‚\n\n![](assets/mm_tutorial/Rebecca_(1939_poster)_Small.jpeg)\n\nQwen-VL-Chat ã®ãƒã‚¹ã‚¿ãƒ¼ã«æã‹ã‚Œã¦ã„ã‚‹æ˜ ç”»ã®åå‰ã‚’èã„ã¦ã¿ã‚ˆã†ã€‚ã¾ãšåˆã‚ã«ã€å…¥åŠ›ã‚’å‰å‡¦ç†ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ ```tokenizer.from_list_format``` ã‚’ä½¿ç”¨ã—ã¾ã™:\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Rebecca_(1939_poster).jpeg'},\n    {'text': 'What is the name of the movie in the poster?'},\n])\n```\næ¬¡ã«ã€```model.chat``` ã‚’ä½¿ã£ã¦ Qwen-VL-Chat ãƒ¢ãƒ‡ãƒ«ã«è³ªå•ã‚’ã—ã€ãã®å›ç­”ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚æœ€åˆã®è³ªå•ã§ã¯ã€ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã®å±¥æ­´ã¯ç©ºãªã®ã§ã€```history=None``` ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n```python\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\nä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒæœŸå¾…ã•ã‚Œã¾ã™:\n\n> The name of the movie in the poster is \"Rebecca.\"\n\nã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒä¸ãˆã‚‰ã‚ŒãŸå•é¡Œã«æ­£ã—ãç­”ãˆãŸã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼ãƒã‚¹ã‚¿ãƒ¼ã‚’ã¿ã‚‹ã¨ã€æ˜ ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯ç¢ºã‹ã«**ãƒ¬ãƒ™ãƒƒã‚«**ã§ã™ã€‚\n\n#### **è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰ã®è³ªå•å›ç­”**\nã¾ãŸã€æ˜ ç”»ã®ç›£ç£ã¯èª°ã‹ãªã©ã€ä»–ã®è³ªå•ã‚’ãƒ¢ãƒ‡ãƒ«ã«ç¶šã‘ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ãã®ãŸã‚ã€```history=history``` ã‚’ä½¿ã£ã¦ã€ä»¥å‰ã®ä¼šè©±ã®å±¥æ­´ã‚’ ``model.chat`` ã«æ¸¡ã—ã¾ã™:\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Who directed this movie?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\nä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒæœŸå¾…ã•ã‚Œã¾ã™:\n\n> The movie \"Rebecca\" was directed by Alfred Hitchcock.\n\nå†ã³ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ä¸ãˆã‚‰ã‚ŒãŸå•é¡Œã«æ­£è§£ã—ã¾ã—ãŸï¼ãƒã‚¹ã‚¿ãƒ¼ã«ã‚ˆã‚‹ã¨ã€ã“ã®æ˜ ç”»ã®ç›£ç£ã¯ã‚¢ãƒ«ãƒ•ãƒ¬ãƒƒãƒ‰ãƒ»ãƒ’ãƒƒãƒã‚³ãƒƒã‚¯ã§ã™ã€‚\n\n### **ãƒ†ã‚­ã‚¹ãƒˆç†è§£**\nQwen-VL-Chat ã«ã¯ã€é«˜å¯†åº¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€ç”»åƒã‚’ç†è§£ã™ã‚‹æ©Ÿèƒ½ã‚‚ã‚ã‚Šã¾ã™ã€‚ä¸‹å›³ã«ç¤ºã™ã‚ˆã†ã«ã€```assets/mm_tutorial/Hospital.jpeg``` ã¨ã„ã†ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€æ¿ƒã„ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€ç—…é™¢ã®çœ‹æ¿ã§ã™ã€‚\n\n![](assets/mm_tutorial/Hospital_Small.jpg)\n\nç—…é™¢å†…ã®ã•ã¾ã–ã¾ãªè¨ºç™‚ç§‘ã®å ´æ‰€ã«ã¤ã„ã¦è³ªå•ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚å¯¾è©±ã®å±¥æ­´ã¯ç©ºãªã®ã§ã€```history=None``` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Hospital.jpg'},\n    {'text': 'Based on the photo, which floor is the Department of Otorhinolaryngology on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\nä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒæœŸå¾…ã•ã‚Œã¾ã™:\n\n> The Department of Otorhinolaryngology is located on the 4th floor.\n\nã•ã‚‰ã«è³ªå•ã‚’ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã“ã®å ´åˆã€```history=history``` ã‚’ä½¿ç”¨ã—ã¦ã€ä»¥å‰ã®ä¼šè©±ã®å±¥æ­´ã‚’ ```model.chat``` ã«æ¸¡ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Based on the photo, which floor is the Department of Surgery on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\nä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒæœŸå¾…ã•ã‚Œã¾ã™:\n\n> The Department of Surgery is located on the 3rd floor.\n\n### **ãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ ã«ã‚ˆã‚‹æ•°å­¦çš„æ¨è«–**\nQwen-VL-Chat ã¯ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ ç†è§£èƒ½åŠ›ã¨æ•°å­¦çš„æ¨è«–èƒ½åŠ›ã‚’ä½¿ã£ã¦ã€ã‚ˆã‚Šè¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼ä¸‹ã«ç¤ºã™ã‚ˆã†ã«ã€```assets/mm_tutorial/Menu.jpeg``` ã¨ã„ã†ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã§ã™ã€‚ã§ã¯ã€Salmon Burger 2 å€‹ã¨ Meat Lover's Pizza 3 æšã‚’è³¼å…¥ã—ãŸå ´åˆã®å€¤æ®µã‚’çŸ¥ã‚ŠãŸã„ã€‚\n\n![](assets/mm_tutorial/Menu.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Menu.jpeg'},\n    {'text': 'How much would I pay if I want to order two Salmon Burger and three Meat Lover\\'s Pizza? Think carefully step by step.'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\nã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§æ³¨æ„æ·±ãè€ƒãˆã¦ãã ã•ã„(```Think carefully step by step.```)ã€ã¯ã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’ä¸€æ­©ãšã¤ã§ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¬ã‚¤ãƒ‰ã™ã‚‹ä¸€èˆ¬çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã™ã€‚è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’ã“ãªã•ãªã‘ã‚Œã°ãªã‚‰ãªã„å ´åˆã€ã“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’ä¸Šã’ã¦ã¿ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒæœŸå¾…ã•ã‚Œã¾ã™:\n\n> To order two Salmon Burgers and three Meat Lover's Pizzas, you would need to pay the following:\n>\n> 1. For two Salmon Burgers: x2 Salmon Burgers at $10 each = $20\n> 2. For three Meat Lover's Pizzas: x3 Meat Lover's Pizzas at $12 each = $36\n>\n> Therefore, the total cost would be $56.\n\n### **å¤šè¦–ç‚¹æ¨è«–ã¨ä¸­å›½èªå…¥åŠ›**\nã“ã‚Œã¾ã§ã®ä¾‹ã§ã¯ã€Qwen-VL-Chat ãŒ 1 ã¤ã®ç”»åƒã¨è‹±èªã®è³ªå•ã«å¯¾ã—ã¦è³ªå•å¿œç­”ãŒã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚ã—ã‹ã—ã€å®Ÿéš›ã«ã¯ Qwen-VL-Chat ã¯ä¸­å›½èªå…¥åŠ›ã¨è¤‡æ•°ã®ç”»åƒã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å¤šè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ï¼ä»¥ä¸‹ã®ä¾‹ã§ã¯ã€Qwen-VL-Chat ã« 2 ã¤ã®éƒ½å¸‚ï¼ˆé‡æ…¶ã¨åŒ—äº¬ï¼‰ã®å†™çœŸï¼ˆ```assets/mm_tutorial/Chongqing.jpeg``` ã¨ ```assets/mm_tutorial/Beijing.jpeg```ï¼‰ã‚’ä¸­å›½èªã§æ¯”è¼ƒã•ã›ã¦ã„ã¾ã™:\n\n![](assets/mm_tutorial/Chongqing_Small.jpeg)\n\n![](assets/mm_tutorial/Beijing_Small.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Chongqing.jpeg'},\n    {'image': 'assets/mm_tutorial/Beijing.jpeg'},\n    {'text': 'ä¸Šé¢ä¸¤å¼ å›¾ç‰‡åˆ†åˆ«æ˜¯å“ªä¸¤ä¸ªåŸå¸‚ï¼Ÿè¯·å¯¹å®ƒä»¬è¿›è¡Œå¯¹æ¯”ã€‚'},\n])\ntorch.manual_seed(5678)\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\nä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒæœŸå¾…ã•ã‚Œã¾ã™:\n\n> ç¬¬ä¸€å¼ å›¾ç‰‡æ˜¯é‡åº†çš„åŸå¸‚å¤©é™…çº¿ï¼Œå®ƒåæ˜ äº†ç°ä»£éƒ½å¸‚çš„ç¹åä¸å–§åš£ã€‚ç¬¬äºŒå¼ å›¾ç‰‡æ˜¯åŒ—äº¬çš„å¤©é™…çº¿ï¼Œå®ƒè±¡å¾ç€ä¸­å›½é¦–éƒ½çš„ç°ä»£åŒ–å’Œå›½é™…åŒ–ã€‚ä¸¤åº§åŸå¸‚éƒ½æ˜¯ä¸­å›½çš„é‡è¦åŸå¸‚ï¼Œæ‹¥æœ‰ç‹¬ç‰¹çš„æ–‡åŒ–å’Œå‘å±•å†å²ã€‚\n\n**éƒ½å¸‚ã®æ¯”è¼ƒã¯ã‹ãªã‚Šä¸»è¦³çš„ãªè³ªå•ã§ã‚ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚Œã‚‹å›ç­”ã¯é«˜åº¦ãªãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’æŒã¤å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚```torch.manual_seed(5678)``` ã‚’ä½¿ç”¨ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®šã—ãªã„å ´åˆã€å‡ºåŠ›ã¯æ¯å›ç•°ãªã‚Šã¾ã™ã€‚ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®šã—ãŸå ´åˆã§ã‚‚ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚„ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®ç’°å¢ƒã®é•ã„ã«ã‚ˆã‚Šã€å¾—ã‚‰ã‚Œã‚‹çµæœãŒã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¨ç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚**\n\n### **ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°èƒ½åŠ›**\nãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã®æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€Qwen-VL-Chat ãƒ¢ãƒ‡ãƒ«ãŒãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆã™ã‚‹æ©Ÿèƒ½ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚Qwen-VL-Chat ã¯ã€è¨€èªè¨˜è¿°ã«å¾“ã£ã¦ã€ç”»åƒã®æŒ‡å®šã•ã‚ŒãŸé ˜åŸŸã‚’çŸ©å½¢ã®æ ã§å›²ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚å°‘ã—æŠ½è±¡çš„ãªã®ã§ã€æ¬¡ã®ä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ä¸‹å›³ã®ã‚ˆã†ã«ã€ãƒ•ã‚¡ã‚¤ãƒ« ```assets/mm_tutorial/Shanghai.jpg``` ã¯ä¸Šæµ·ã®å†™çœŸã§ã™ã€‚ã¾ãšã€é€šå¸¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ãƒ¢ãƒ‡ãƒ«ã«ç”»åƒã‚’è¨˜è¿°ã—ã¦ã‚‚ã‚‰ã„ã¾ã™ã€‚\n\n![](assets/mm_tutorial/Shanghai_Small.jpeg)\n\n```python\ntorch.manual_seed(1234)\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Shanghai.jpg'},\n    {'text': 'å›¾é‡Œæœ‰å•¥'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\nä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒæœŸå¾…ã•ã‚Œã¾ã™:\n\n> å›¾ä¸­æ˜¯ä¸­å›½ä¸Šæµ·çš„å¤©é™…çº¿ï¼ŒåŒ…æ‹¬äº†ä¸Šæµ·å¡”ã€é‡‘èŒ‚å¤§å¦ã€ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒã€æµ·æ´‹å¤§å¦ç­‰è‘—åå»ºç­‘ã€‚\n\næ¬¡ã«ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ```è¯·ç»™æˆ‘æ¡†å‡ºå›¾ä¸­ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒå’Œä¸œæ–¹æ˜ç ``` ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã¨ä¼šè©±ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã“ã®ã¨ãã€```history=history``` ã‚’ä½¿ã£ã¦ã€ä»¥å‰ã®ä¼šè©±ã®å±¥æ­´ã‚’ ```model.chat``` ã«æ¸¡ã™å¿…è¦ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'è¯·ç»™æˆ‘æ¡†å‡ºå›¾ä¸­ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒå’Œä¸œæ–¹æ˜ç '},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\nä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒæœŸå¾…ã•ã‚Œã¾ã™:\n```xml\n<ref>ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒ</ref><box>(667,437),(760,874)</box>å’Œ<ref>ä¸œæ–¹æ˜ç </ref><box>(506,75),(582,946)</box>\n```\nQwen-VL-Chat ãƒ¢ãƒ‡ãƒ«ã«ã¯æ‰‹ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ã ã‹ã‚‰ã¨ã„ã£ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ‹’å¦ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚ãã®ä»£ã‚ã‚Šã«ã€\"å¥‡å¦™ãª\"ã‚‚ã®ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ - å®Ÿéš›ã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã¯ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒï¼ˆä¸Šæµ·ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ»ãƒ•ã‚£ãƒŠãƒ³ã‚·ãƒ£ãƒ«ãƒ»ã‚»ãƒ³ã‚¿ãƒ¼ï¼‰ã¨ä¸œæ–¹æ˜ç ï¼ˆæ±æ–¹ãƒ†ãƒ¬ãƒ“ã‚¿ãƒ¯ãƒ¼ï¼‰ã®ä½ç½®ã‚’ãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—è¨€èªã§ç¤ºã—ã¦ã„ã¾ã™ã€‚æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã§è¦–è¦šåŒ–ã§ãã¾ã™:\n```python\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nimage.save('Shanghai_Output.jpg')\n```\nä¿å­˜ã•ã‚ŒãŸ ```Shanghai_Output.jpg``` ã¯ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®ã‚ˆã†ã«ãªã‚Šã¾ã™:\n\n![](assets/mm_tutorial/Shanghai_Output_Small.jpeg)\n\nãã®å¾Œã€Qwen-VL-Chat ã§ä»¥å‰ã¨åŒã˜ã‚ˆã†ã«ãƒãƒ£ãƒƒãƒˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™:\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'å¸®æˆ‘å†™ä¸ªè¿™åº§åŸå¸‚çš„æ—…æ¸¸è®¡åˆ’'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\nä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒæœŸå¾…ã•ã‚Œã¾ã™:\n\n> å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ä¸Šæµ·æ—…æ¸¸è®¡åˆ’ï¼š\n>\n> ç¬¬ä¸€å¤©ï¼š\n> ä¸Šåˆï¼šæŠµè¾¾ä¸Šæµ·ï¼Œå‰å¾€é…’åº—åŠç†å…¥ä½æ‰‹ç»­ã€‚\n> ä¸­åˆï¼šäº«ç”¨åˆé¤åï¼Œå‰å¾€å¤–æ»©ï¼Œæ¬£èµé»„æµ¦æ±Ÿç•”çš„ç¾æ™¯ï¼Œæ¸¸è§ˆä¸Šæµ·åœ°æ ‡æ€§å»ºç­‘å¦‚æµ¦å‘é“¶è¡Œå¤§æ¥¼ã€æ±‡ä¸°é“¶è¡Œå¤§æ¥¼ç­‰ã€‚\n> ä¸‹åˆï¼šæ¸¸è§ˆå—äº¬è·¯æ­¥è¡Œè¡—ï¼Œè´­ä¹°ç‰¹è‰²ç¤¼å“æˆ–å“å°å½“åœ°ç¾é£Ÿã€‚\n> æ™šä¸Šï¼šåœ¨å—äº¬è·¯é™„è¿‘çš„é¤å…äº«ç”¨æ™šé¤ï¼Œç„¶åå»çœ‹ä¸Šæµ·çš„å¤œæ™¯ã€‚\n>\n> ç¬¬äºŒå¤©ï¼š\n> ä¸Šåˆï¼šå‰å¾€ä¸Šæµ·ç§‘æŠ€é¦†ï¼Œäº†è§£ç§‘æŠ€å‘å±•å†å²ï¼Œè§‚çœ‹å„ç§ç§‘æŠ€å±•è§ˆã€‚\n> ä¸­åˆï¼šåœ¨ç§‘æŠ€é¦†é™„è¿‘çš„é¤å…äº«ç”¨åˆé¤ã€‚\n> ä¸‹åˆï¼šæ¸¸è§ˆä¸–çºªå…¬å›­ï¼Œæ¬£èµç¾æ™¯å¹¶æ”¾æ¾èº«å¿ƒã€‚\n> æ™šä¸Šï¼šåœ¨å—äº¬è·¯æˆ–é™„è¿‘çš„é™†å®¶å˜´åœ°åŒºäº«ç”¨æ™šé¤ï¼Œç„¶åå»çœ‹ä¸Šæµ·çš„å¤œæ™¯ã€‚\n>\n> ç¬¬ä¸‰å¤©ï¼š\n> ä¸Šåˆï¼šæ¸¸è§ˆä¸Šæµ·è¿ªå£«å°¼ä¹å›­æˆ–ä¸Šæµ·æµ·æ˜Œæµ·æ´‹å…¬å›­ï¼Œä¸å„ç§è¿ªå£«å°¼è§’è‰²äº’åŠ¨ï¼Œæˆ–è€…åœ¨æµ·æ´‹å…¬å›­è§‚çœ‹æµ·æ´‹ç”Ÿç‰©è¡¨æ¼”ã€‚\n> ä¸­åˆï¼šåœ¨è¿ªå£«å°¼ä¹å›­æˆ–æµ·æ´‹å…¬å›­é™„è¿‘çš„é¤å…äº«ç”¨åˆé¤ã€‚\n> ä¸‹åˆï¼šè‡ªç”±æ´»åŠ¨ï¼Œå¯ä»¥å»è´­ç‰©ã€å“å°å½“åœ°ç¾é£Ÿæˆ–è€…å»åšç‰©é¦†ç­‰ã€‚\n> æ™šä¸Šï¼šåœ¨é…’åº—é™„è¿‘äº«ç”¨æ™šé¤ï¼Œç„¶åç¦»å¼€ä¸Šæµ·ã€‚\n>\n> å½“ç„¶ï¼Œä»¥ä¸Šåªæ˜¯ä¸€ä¸ªç®€å•çš„è®¡åˆ’ï¼Œä¸Šæµ·æœ‰è®¸å¤šå…¶ä»–æ™¯ç‚¹å’Œæ´»åŠ¨ï¼Œä¾‹å¦‚å‚è§‚ä¸Šæµ·åšç‰©é¦†ã€æ¸¸è§ˆç”°å­åŠã€è§‚çœ‹ä¸Šæµ·è¯å‰§ç­‰ã€‚å…·ä½“è®¡åˆ’å¯ä»¥æ ¹æ®ä¸ªäººå…´è¶£å’Œæ—¶é—´è¿›è¡Œè°ƒæ•´ã€‚\n\n\n**æ—…è¡Œè¨ˆç”»ã¯ã‹ãªã‚Šä¸»è¦³çš„ãªè³ªå•ã§ã‚ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚Œã‚‹å›ç­”ã¯é«˜ã„ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’æŒã¤å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚```torch.manual_seed(1234)``` ã‚’ä½¿ç”¨ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®šã—ãªã„å ´åˆã€å‡ºåŠ›ã¯æ¯å›ç•°ãªã‚Šã¾ã™ã€‚ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®šã—ãŸå ´åˆã§ã‚‚ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚„ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®ç’°å¢ƒã®é•ã„ã«ã‚ˆã‚Šã€å¾—ã‚‰ã‚Œã‚‹çµæœãŒã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¨ç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚**\n"
        },
        {
          "name": "TUTORIAL_ko.md",
          "type": "blob",
          "size": 13.80078125,
          "content": "# Qwen-VL-Chat Tutorial\n\nQwen-VL-Chatì€ ë²”ìš© ë©€í‹°ëª¨ë‹¬ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ë©° ê´‘ë²”ìœ„í•œ ì‹œê° ì–¸ì–´ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” **ì‹œê°ì  ì§ˆë¬¸ ë‹µë³€, í…ìŠ¤íŠ¸ ì´í•´, ë‹¤ì´ì–´ê·¸ë¨ì„ ì‚¬ìš©í•œ ìˆ˜í•™ì  ì¶”ë¡ , ë‹¤ì¤‘ ê·¸ë¦¼ ì¶”ë¡  ë° ê·¸ë¼ìš´ë”©(Grounding) ì‘ì—…**ì—ì„œ Qwen-VL-Chatì˜ ê¸°ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ëª‡ ê°€ì§€ ê°„ê²°í•œ ì˜ˆì œë¥¼ ì œì‹œí•©ë‹ˆë‹¤. Qwen-VL-Chatì˜ ê¸°ëŠ¥ì˜ í•œê³„ê°€ ì•„ë‹ˆë©°, **ì…ë ¥ ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ë³€ê²½í•˜ì—¬ Qwen-VL-Chatì˜ ê¸°ëŠ¥**ì„ ë” ìì„¸íˆ ì‚´í´ë³´ì‹¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\n## Initializing the Qwen-VL-Chat model\nQwen-VL-Chatì„ ì‚¬ìš©í•˜ê¸° ì „ì— ë¨¼ì € Qwen-VL-Chatì˜ Tokenizerì™€ Qwen-VL-Chatì˜ ëª¨ë¸ì„ ì´ˆê¸°í™”í•´ì•¼ í•©ë‹ˆë‹¤.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# If you expect the results to be reproducible, set a random seed.\n# torch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n```\n\nìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì‹œë©´ ```tokenizer```ë³€ìˆ˜ì— Qwen-VL-Chatì—ì„œ ì‚¬ìš©í•˜ëŠ” ë¶„ë¥˜ê¸°(classifier)ê°€ í• ë‹¹ë˜ê³ , ```model```ë³€ìˆ˜ì—ëŠ” Qwen-VL-Chatì˜ ëª¨ë¸ì„ í• ë‹¹í•˜ê²Œ ë©ë‹ˆë‹¤. ```tokenizer```ëŠ” ì¸í„°ë¦¬ë¸Œëœ ë©€í‹°ëª¨ë‹¬ ì…ë ¥(interleaved multimodal inputs)ì„ ì „ì²˜ë¦¬í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, ``model``ì€ Qwen-VL-Chat ëª¨ë¸ì…ë‹ˆë‹¤.\n\n## Using Qwen-VL-Chat\n### **Multi-round visual question answering**\n#### **ì²« ì§ˆë¬¸í•˜ê¸°**\n\nê°„ë‹¨í•œ ì˜ˆì œë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ```assets/mm_tutorial/Rebecca_(1939_poster).jpeg``` íŒŒì¼ì€ 1940ë…„ ì˜í™” <ë ˆë² ì¹´>ì˜ í¬ìŠ¤í„°ì…ë‹ˆë‹¤.\n\n![](assets/mm_tutorial/Rebecca_(1939_poster)_Small.jpeg)\n\nQwen-VL-Chat í¬ìŠ¤í„°ì— ìˆëŠ” ì˜í™” ì œëª©ì´ ë¬´ì—‡ì¸ì§€ ë¬¼ì–´ë´…ì‹œë‹¤. ìš°ì„ , ì…ë ¥ì„ ì „ì²˜ë¦¬í•˜ê³  í† í°í™”í•  ìˆ˜ ìˆëŠ” ```tokenizer.from_list_format```ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Rebecca_(1939_poster).jpeg'},\n    {'text': 'What is the name of the movie in the poster?'},\n])\n```\në‹¤ìŒìœ¼ë¡œ, ```model.chat```ì„ ì‚¬ìš©í•˜ì—¬ Qwen-VL-Chat ëª¨ë¸ì— ì§ˆë¬¸í•˜ê³  ì‘ë‹µì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ ì§ˆë¬¸ì˜ ê²½ìš° ëŒ€í™” ê¸°ë¡ì´ ë¹„ì–´ ìˆìœ¼ë¯€ë¡œ ``history=None``ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n```python\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\në‹¤ìŒê³¼ ë¹„ìŠ·í•œ ì¶œë ¥ì´ ë‚˜ì˜¬ ê²ƒì…ë‹ˆë‹¤.\n\n> The name of the movie in the poster is \"Rebecca.\"\n\nëª¨ë¸ì´ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ì •ë‹µì„ ë§í˜”ìŠµë‹ˆë‹¤. í¬ìŠ¤í„°ì— ë”°ë¥´ë©´, ì˜í™”ì˜ ì œëª©ì€ ì‹¤ì œë¡œ **ë ˆë² ì¹´**ì…ë‹ˆë‹¤.\n\n#### **Multi-round question answering**\në˜í•œ ëª¨ë¸ì—ê²Œ ì˜í™” ê°ë…ì´ ëˆ„êµ¬ì¸ì§€ì™€ ê°™ì€ ë‹¤ë¥¸ ì§ˆë¬¸ì„ ê³„ì†í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ëŒ€í™” ê¸°ë¡ì€ í›„ì† ì§ˆë¬¸ì„ ìœ„í•´ ë¹„ì–´ ìˆì§€ ì•Šìœ¼ë¯€ë¡œ ``history=history``ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ ëŒ€í™”ì˜ ê¸°ë¡ì„ ``model.chat``ì— ì „ë‹¬í•©ë‹ˆë‹¤:\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Who directed this movie?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\në‹¤ìŒê³¼ ë¹„ìŠ·í•œ ì¶œë ¥ì´ ë‚˜ì˜¬ ê²ƒì…ë‹ˆë‹¤.\n\n> The movie \"Rebecca\" was directed by Alfred Hitchcock.\n\në‹¤ì‹œ í•œ ë²ˆ, ëª¨ë¸ì´ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µì„ ë§í˜”ìŠµë‹ˆë‹¤. í¬ìŠ¤í„°ì— ë”°ë¥´ë©´ ì´ ì˜í™”ì˜ ê°ë…ì€ <ì•Œí”„ë ˆë“œ íˆì¹˜ì½•>ì…ë‹ˆë‹¤.\n\n### **Text Understanding**\nQwen-VL-Chatì€ ì´˜ì´˜í•œ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì´ë¯¸ì§€ë„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ``assets/mm_tutorial/Hospital.jpeg`` íŒŒì¼ì€ ì´˜ì´˜í•œ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ë³‘ì› ê°„íŒì…ë‹ˆë‹¤.\n\n![](assets/mm_tutorial/Hospital_Small.jpg)\n\në³‘ì› ë‚´ ì—¬ëŸ¬ ë¶€ì„œì˜ ìœ„ì¹˜ì— ëŒ€í•´ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì²« ì§ˆë¬¸ìœ¼ë¡œ ëŒ€í™”ì— ëŒ€í•œ ì´ì „ ê¸°ë¡ì´ ì—†ìœ¼ë¯€ë¡œ ```history=None```ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Hospital.jpg'},\n    {'text': 'Based on the photo, which floor is the Department of Otorhinolaryngology on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\në‹¤ìŒê³¼ ë¹„ìŠ·í•œ ì¶œë ¥ì´ ë‚˜ì˜¬ ê²ƒì…ë‹ˆë‹¤.\n\n> The Department of Otorhinolaryngology is located on the 4th floor.\n\nì¶”ê°€ ì§ˆë¬¸ì„ í•˜ì‹¤ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° ```history=history```ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ ëŒ€í™”ì˜ ê¸°ë¡ì„ ```model.chat```ì— ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Based on the photo, which floor is the Department of Surgery on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\në‹¤ìŒê³¼ ë¹„ìŠ·í•œ ì¶œë ¥ì´ ë‚˜ì˜¬ ê²ƒì…ë‹ˆë‹¤.\n\n> The Department of Surgery is located on the 3rd floor.\n\n### **Mathematical Reasoning with Diagram**\nëª¨ë¸ì˜ ë‹¤ì´ì–´ê·¸ë¨ ì´í•´ì™€ ìˆ˜í•™ì  ì¶”ë¡  ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ Qwen-VL-Chatì€ ì¢€ ë” ë³µì¡í•œ ì‘ì—…ë„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ``assets/mm_tutorial/Menu.jpeg`` íŒŒì¼ì€ ë ˆìŠ¤í† ë‘ì˜ ë©”ë‰´ ì´ë¯¸ì§€ ì…ë‹ˆë‹¤. ì´ì œ ì—°ì–´ ë²„ê±° ë‘ ê°œì™€ ë¯¸íŠ¸ ëŸ¬ë²„ìŠ¤ í”¼ì ì„¸ ê°œë¥¼ êµ¬ë§¤í•˜ëŠ” ë° ë“œëŠ” ë¹„ìš©ì„ ì•Œì•„ë´…ì‹œë‹¤.\n![](assets/mm_tutorial/Menu.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Menu.jpeg'},\n    {'text': 'How much would I pay if I want to order two Salmon Burger and three Meat Lover\\'s Pizza? Think carefully step by step.'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n``ë‹¨ê³„ë³„ë¡œ ì‹ ì¤‘í•˜ê²Œ ìƒê°í•˜ì„¸ìš”``ëŠ” ë³µì¡í•œ ì‘ì—…ì„ ë‹¨ê³„ë³„ë¡œ ëª¨ë¸ì— ì•ˆë‚´í•˜ëŠ” ì¼ë°˜ì ì¸ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì™„ë£Œí•´ì•¼ í•  ë³µì¡í•œ ì‘ì—…ì´ ìˆëŠ” ê²½ìš°ì—ëŠ” ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œì¼œ ë³´ì„¸ìš”. ë‹¤ìŒê³¼ ìœ ì‚¬í•œ ì¶œë ¥ì´ ë‚˜ì˜¬ ê²ƒì…ë‹ˆë‹¤.\n\n> To order two Salmon Burgers and three Meat Lover's Pizzas, you would need to pay the following:\n> \n> 1. For two Salmon Burgers: x2 Salmon Burgers at $10 each = $20\n> 2. For three Meat Lover's Pizzas: x3 Meat Lover's Pizzas at $12 each = $36\n> \n> Therefore, the total cost would be $56.\n\n### **Multi-Figure Reasoning and Chinese Input**\nì´ì „ ì˜ˆì œì—ì„œëŠ” ë‹¨ì¼ ì´ë¯¸ì§€ì™€ ì˜ì–´ ì§ˆë¬¸ì— ëŒ€í•œ Qwen-VL-Chatì˜ ì§ˆë¬¸ ë‹µë³€ ê¸°ëŠ¥ì„ ì‹œì—°í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì¤‘êµ­ì–´ ì…ë ¥ê³¼ ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ì§€ì›í•˜ëŠ” ë‹¤êµ­ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì˜ˆì œì—ì„œëŠ” ë‘ ë„ì‹œ(ì¶©ì¹­ê³¼ ë² ì´ì§•)ì˜ ì‚¬ì§„(`assets/mm_tutorial/Chongqing.jpeg` ë° `assets/mm_tutorial/Beijing.jpeg`)ì„ ì¤‘êµ­ì–´ë¡œ ë¹„êµí•˜ë„ë¡ Qwen-VL-Chatì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\n\n![](assets/mm_tutorial/Chongqing_Small.jpeg)\n\n![](assets/mm_tutorial/Beijing_Small.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Chongqing.jpeg'},\n    {'image': 'assets/mm_tutorial/Beijing.jpeg'},\n    {'text': 'ä¸Šé¢ä¸¤å¼ å›¾ç‰‡åˆ†åˆ«æ˜¯å“ªä¸¤ä¸ªåŸå¸‚ï¼Ÿè¯·å¯¹å®ƒä»¬è¿›è¡Œå¯¹æ¯”ã€‚'},\n])\ntorch.manual_seed(5678)\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\në‹¤ìŒê³¼ ìœ ì‚¬í•œ ì¶œë ¥ì´ ë‚˜ì˜¬ ê²ƒì…ë‹ˆë‹¤.\n\n> ç¬¬ä¸€å¼ å›¾ç‰‡æ˜¯é‡åº†çš„åŸå¸‚å¤©é™…çº¿ï¼Œå®ƒåæ˜ äº†ç°ä»£éƒ½å¸‚çš„ç¹åä¸å–§åš£ã€‚ç¬¬äºŒå¼ å›¾ç‰‡æ˜¯åŒ—äº¬çš„å¤©é™…çº¿ï¼Œå®ƒè±¡å¾ç€ä¸­å›½é¦–éƒ½çš„ç°ä»£åŒ–å’Œå›½é™…åŒ–ã€‚ä¸¤åº§åŸå¸‚éƒ½æ˜¯ä¸­å›½çš„é‡è¦åŸå¸‚ï¼Œæ‹¥æœ‰ç‹¬ç‰¹çš„æ–‡åŒ–å’Œå‘å±•å†å²ã€‚\n\n**ë„ì‹œ ë¹„êµëŠ” ìƒë‹¹íˆ ì£¼ê´€ì ì¸ ì§ˆë¬¸ì´ë¯€ë¡œ ëª¨ë¸ì— ì˜í•´ ìƒì„±ëœ ì‘ë‹µì—ëŠ” ë§¤ìš° ë‹¤ì–‘í•˜ê²Œ ë¬´ì‘ìœ„ì˜ ì‹œë“œê°€ ì ìš©ë  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ìœ ì˜í•˜ì„¸ìš”. ``torch.manual_seed(5678)```ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬´ì‘ìœ„ ì‹œë“œë¥¼ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´ ë§¤ë²ˆ ì¶œë ¥ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ëœë¤ ì‹œë“œë¥¼ ì„¤ì •í•˜ë”ë¼ë„ í•˜ë“œì›¨ì–´ ë° ì†Œí”„íŠ¸ì›¨ì–´ í™˜ê²½ì˜ ì°¨ì´ë¡œ ì¸í•´ ì–»ì€ ê²°ê³¼ê°€ ì´ íŠœí† ë¦¬ì–¼ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤**.\n\n\n### **Grounding Capability**\níŠœí† ë¦¬ì–¼ì˜ ë§ˆì§€ë§‰ ì„¹ì…˜ì—ì„œëŠ” Qwen-VL-Chat ëª¨ë¸ì´ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤. Qwen-VL-Chatì€ ì–¸ì–´ ì„¤ëª…ì— ë”°ë¼ ì§ì‚¬ê°í˜• ìƒìë¡œ ì´ë¯¸ì§€ì˜ ì§€ì •ëœ ì˜ì—­ì— í”„ë ˆì„ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì†Œ ì¶”ìƒì ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ìŒ ì˜ˆì œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ```assets/mm_tutorial/Shanghai.jpg`` íŒŒì¼ì€ ìƒí•˜ì´ì˜ ì‚¬ì§„ì´ë©°, ëª¨ë¸ì—ê²Œ ì¼ë°˜ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ë„ë¡ ìš”ì²­í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.\n\n![](assets/mm_tutorial/Shanghai_Small.jpeg)\n\n```python\ntorch.manual_seed(1234)\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Shanghai.jpg'},\n    {'text': 'å›¾é‡Œæœ‰å•¥'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\në‹¤ìŒê³¼ ìœ ì‚¬í•œ ì¶œë ¥ì„ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n> å›¾ä¸­æ˜¯ä¸­å›½ä¸Šæµ·çš„å¤©é™…çº¿ï¼ŒåŒ…æ‹¬äº†ä¸Šæµ·å¡”ã€é‡‘èŒ‚å¤§å¦ã€ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒã€æµ·æ´‹å¤§å¦ç­‰è‘—åå»ºç­‘ã€‚\n\në‹¤ìŒìœ¼ë¡œ '``è¯·ç»™æˆ‘æ¡†å‡ºå›¾ä¸­ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒå’Œä¸œæ–¹æ˜ç ``ë¼ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ê³¼ ëŒ€í™”í•˜ê³  ì–´ë–¤ ì¼ì´ ë°œìƒí•˜ëŠ”ì§€ ì‚´í´ë´…ì‹œë‹¤. ì´ ì‹œì ì—ì„œ ``history=history``ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ ëŒ€í™”ì˜ ê¸°ë¡ì„ ``model.chat``ì— ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'è¯·ç»™æˆ‘æ¡†å‡ºå›¾ä¸­ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒå’Œä¸œæ–¹æ˜ç '},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\në‹¤ìŒê³¼ ìœ ì‚¬í•œ ì¶œë ¥ì„ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```xml\n<ref>ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒ</ref><box>(667,437),(760,874)</box>å’Œ<ref>ä¸œæ–¹æ˜ç </ref><box>(506,75),(582,946)</box>\n```\n\nQwen-VL-Chat ëª¨ë¸ì—ëŠ” ì†ì´ ì—†ì§€ë§Œ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ê±°ë¶€í•˜ì§€ë„ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹  \"ì´ìƒí•œ\" ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ”ë°, ì‹¤ì œë¡œ ì´ ëª¨ë¸ì˜ ì¶œë ¥ì€ ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒ(ìƒí•˜ì´ ì›”ë“œ íŒŒì´ë‚¸ì…œ ì„¼í„°) ì™€ ä¸œæ–¹æ˜ç (ë™ë°©ëª…ì£¼) ì˜ ìœ„ì¹˜ë¥¼ ë§ˆí¬ì—… ì–¸ì–´ë¡œ ì œê³µí•©ë‹ˆë‹¤. ë‹¤ìŒ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```python\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nimage.save('Shanghai_Output.jpg')\n```\nThe saved ```Shanghai_Output.jpg``` will look similar to the screenshot below: \n\n![](assets/mm_tutorial/Shanghai_Output_Small.jpeg)\n\nê·¸ í›„ì—ë„ ì´ì „ì²˜ëŸ¼ Qwen-VL-Chatìœ¼ë¡œ ê³„ì† ì±„íŒ…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'å¸®æˆ‘å†™ä¸ªè¿™åº§åŸå¸‚çš„æ—…æ¸¸è®¡åˆ’'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\në‹¤ìŒê³¼ ìœ ì‚¬í•œ ì¶œë ¥ì„ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n> å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ä¸Šæµ·æ—…æ¸¸è®¡åˆ’ï¼š\n>\n> ç¬¬ä¸€å¤©ï¼š\n> ä¸Šåˆï¼šæŠµè¾¾ä¸Šæµ·ï¼Œå‰å¾€é…’åº—åŠç†å…¥ä½æ‰‹ç»­ã€‚\n> ä¸­åˆï¼šäº«ç”¨åˆé¤åï¼Œå‰å¾€å¤–æ»©ï¼Œæ¬£èµé»„æµ¦æ±Ÿç•”çš„ç¾æ™¯ï¼Œæ¸¸è§ˆä¸Šæµ·åœ°æ ‡æ€§å»ºç­‘å¦‚æµ¦å‘é“¶è¡Œå¤§æ¥¼ã€æ±‡ä¸°é“¶è¡Œå¤§æ¥¼ç­‰ã€‚\n> ä¸‹åˆï¼šæ¸¸è§ˆå—äº¬è·¯æ­¥è¡Œè¡—ï¼Œè´­ä¹°ç‰¹è‰²ç¤¼å“æˆ–å“å°å½“åœ°ç¾é£Ÿã€‚\n> æ™šä¸Šï¼šåœ¨å—äº¬è·¯é™„è¿‘çš„é¤å…äº«ç”¨æ™šé¤ï¼Œç„¶åå»çœ‹ä¸Šæµ·çš„å¤œæ™¯ã€‚\n>\n> ç¬¬äºŒå¤©ï¼š\n> ä¸Šåˆï¼šå‰å¾€ä¸Šæµ·ç§‘æŠ€é¦†ï¼Œäº†è§£ç§‘æŠ€å‘å±•å†å²ï¼Œè§‚çœ‹å„ç§ç§‘æŠ€å±•è§ˆã€‚\n> ä¸­åˆï¼šåœ¨ç§‘æŠ€é¦†é™„è¿‘çš„é¤å…äº«ç”¨åˆé¤ã€‚\n> ä¸‹åˆï¼šæ¸¸è§ˆä¸–çºªå…¬å›­ï¼Œæ¬£èµç¾æ™¯å¹¶æ”¾æ¾èº«å¿ƒã€‚\n> æ™šä¸Šï¼šåœ¨å—äº¬è·¯æˆ–é™„è¿‘çš„é™†å®¶å˜´åœ°åŒºäº«ç”¨æ™šé¤ï¼Œç„¶åå»çœ‹ä¸Šæµ·çš„å¤œæ™¯ã€‚\n>\n> ç¬¬ä¸‰å¤©ï¼š\n> ä¸Šåˆï¼šæ¸¸è§ˆä¸Šæµ·è¿ªå£«å°¼ä¹å›­æˆ–ä¸Šæµ·æµ·æ˜Œæµ·æ´‹å…¬å›­ï¼Œä¸å„ç§è¿ªå£«å°¼è§’è‰²äº’åŠ¨ï¼Œæˆ–è€…åœ¨æµ·æ´‹å…¬å›­è§‚çœ‹æµ·æ´‹ç”Ÿç‰©è¡¨æ¼”ã€‚\n> ä¸­åˆï¼šåœ¨è¿ªå£«å°¼ä¹å›­æˆ–æµ·æ´‹å…¬å›­é™„è¿‘çš„é¤å…äº«ç”¨åˆé¤ã€‚\n> ä¸‹åˆï¼šè‡ªç”±æ´»åŠ¨ï¼Œå¯ä»¥å»è´­ç‰©ã€å“å°å½“åœ°ç¾é£Ÿæˆ–è€…å»åšç‰©é¦†ç­‰ã€‚\n> æ™šä¸Šï¼šåœ¨é…’åº—é™„è¿‘äº«ç”¨æ™šé¤ï¼Œç„¶åç¦»å¼€ä¸Šæµ·ã€‚\n>\n> å½“ç„¶ï¼Œä»¥ä¸Šåªæ˜¯ä¸€ä¸ªç®€å•çš„è®¡åˆ’ï¼Œä¸Šæµ·æœ‰è®¸å¤šå…¶ä»–æ™¯ç‚¹å’Œæ´»åŠ¨ï¼Œä¾‹å¦‚å‚è§‚ä¸Šæµ·åšç‰©é¦†ã€æ¸¸è§ˆç”°å­åŠã€è§‚çœ‹ä¸Šæµ·è¯å‰§ç­‰ã€‚å…·ä½“è®¡åˆ’å¯ä»¥æ ¹æ®ä¸ªäººå…´è¶£å’Œæ—¶é—´è¿›è¡Œè°ƒæ•´ã€‚\n\n**ì—¬í–‰ ê³„íšì€ ìƒë‹¹íˆ ì£¼ê´€ì ì¸ ì§ˆë¬¸ì´ë¯€ë¡œ ëª¨ë¸ì— ì˜í•´ ìƒì„±ëœ ì‘ë‹µì—ëŠ” ë†’ì€ ìˆ˜ì¤€ì˜ ëœë¤ ì‹œë“œê°€ ì ìš©ë  ìˆ˜ ìˆë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì„¸ìš”. ``torch.manual_seed(1234)``ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬´ì‘ìœ„ ì‹œë“œë¥¼ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´ ë§¤ë²ˆ ë‹¤ë¥¸ ì¶œë ¥ì´ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤. ëœë¤ ì‹œë“œë¥¼ ì¼ì •í•˜ê²Œ ì„¤ì •í•˜ë”ë¼ë„ í•˜ë“œì›¨ì–´ ë° ì†Œí”„íŠ¸ì›¨ì–´ í™˜ê²½ì˜ ì°¨ì´ë¡œ ì¸í•´ ì–»ì€ ê²°ê³¼ê°€ ì´ íŠœí† ë¦¬ì–¼ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤**.\n\n### Grounded Captioning\nQwen-VLì€ ë‹¤ìŒê³¼ ê°™ì´ ì´ë¯¸ì§€ë¥¼ ìº¡ì³í•˜ëŠ” ë™ì•ˆ í”¼ì‚¬ì²´ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n\n```\nimg_url = 'assets/apple.jpeg'\nquery = tokenizer.from_list_format([\n    {'image': img_url},\n    {'text': 'Generate the caption in English with grounding:'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image is not None:\n    image.save('apple.jpg')\n```\n\nì €ì¥ëœ ``ì‚¬ê³¼.jpg``ëŠ” ì´ë¯¸ì§€ëŠ” ì•„ë˜ ìŠ¤í¬ë¦°ìƒ·ê³¼ ë¹„ìŠ·í•˜ê²Œ ë³´ì´ê²Œ ë  ê²ƒì…ë‹ˆë‹¤.\n<p align=\"left\">\n    <img src=\"assets/apple_r.jpeg\" width=\"600\"/>\n<p>\n\n#### How to get the caption without any box-like annotations\në•Œë¡œëŠ” ì‘ë‹µì— ë°•ìŠ¤í˜• ì£¼ì„ì´ ì—†ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì€ í›„ì²˜ë¦¬ë¥¼ í†µí•´ ì•ˆì •ì ìœ¼ë¡œ ì •ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n```\n# response = '<ref> Two apples</ref><box>(302,257),(582,671)</box><box>(603,252),(878,642)</box> and<ref> a bowl</ref><box>(2,269),(304,674)</box>'\nimport re\nclean_response = re.sub(r'<ref>(.*?)</ref>(?:<box>.*?</box>)*(?:<quad>.*?</quad>)*', r'\\1', response).strip()\nprint(clean_response)\n# clean_response = 'Two apples and a bowl'\n```\n"
        },
        {
          "name": "TUTORIAL_zh.md",
          "type": "blob",
          "size": 11.1552734375,
          "content": "# Qwen-VL-Chatä½¿ç”¨æ•™ç¨‹\nQwen-VL-Chatæ˜¯é€šç”¨å¤šæ¨¡æ€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œå› æ­¤å®ƒå¯ä»¥å®Œæˆå¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ã€‚åœ¨æœ¬æ•™ç¨‹ä¹‹ä¸­ï¼Œæˆ‘ä»¬ä¼šç»™å‡ºä¸€äº›ç®€æ˜çš„ä¾‹å­ï¼Œç”¨ä»¥å±•ç¤ºQwen-VL-Chatåœ¨**è§†è§‰é—®ç­”ï¼Œæ–‡å­—ç†è§£ï¼Œå›¾è¡¨æ•°å­¦æ¨ç†ï¼Œå¤šå›¾ç†è§£å’ŒGrounding**(æ ¹æ®æŒ‡ä»¤æ ‡æ³¨å›¾ç‰‡ä¸­æŒ‡å®šåŒºåŸŸçš„åŒ…å›´æ¡†)ç­‰å¤šæ–¹é¢çš„èƒ½åŠ›ã€‚è¯·æ³¨æ„ï¼Œå±•ç¤ºçš„ä¾‹å­è¿œéQwen-VL-Chatèƒ½åŠ›çš„æé™ï¼Œ**æ‚¨å¯ä»¥é€šè¿‡æ›´æ¢ä¸åŒçš„è¾“å…¥å›¾åƒå’Œæç¤ºè¯ï¼ˆPromptï¼‰ï¼Œæ¥è¿›ä¸€æ­¥æŒ–æ˜Qwen-VL-Chatçš„èƒ½åŠ›ï¼**\n\n## åˆå§‹åŒ–Qwen-VL-Chatæ¨¡å‹\nåœ¨ä½¿ç”¨Qwen-VL-Chatä¹‹å‰ï¼Œæ‚¨é¦–å…ˆéœ€è¦åˆå§‹åŒ–Qwen-VL-Chatçš„åˆ†è¯å™¨ï¼ˆTokenizerï¼‰å’ŒQwen-VL-Chatçš„æ¨¡å‹ï¼š\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# å¦‚æœæ‚¨å¸Œæœ›ç»“æœå¯å¤ç°ï¼Œå¯ä»¥è®¾ç½®éšæœºæ•°ç§å­ã€‚\n# torch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n```\nåœ¨æ‰§è¡Œå®Œä¸Šè¿°ä»£ç åï¼Œ```tokenizer```å°†å¯¹åº”Qwen-VL-Chatä½¿ç”¨çš„åˆ†è¯å™¨ï¼Œè€Œ```model```å°†å¯¹åº”Qwen-VL-Chatçš„æ¨¡å‹ã€‚```tokenizer```ç”¨äºå¯¹å›¾æ–‡æ··æ’è¾“å…¥è¿›è¡Œåˆ†è¯å’Œé¢„å¤„ç†ï¼Œè€Œ```model```åˆ™æ˜¯Qwen-VL-Chatæ¨¡å‹æœ¬èº«ã€‚\n\n## ä½¿ç”¨Qwen-VL-Chat\n### **å¤šè½®è§†è§‰é—®ç­”**\n#### **ç¬¬ä¸€ä¸ªé—®é¢˜**\né¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªæœ€ç®€å•çš„ä¾‹å­ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ–‡ä»¶```assets/mm_tutorial/Rebecca_(1939_poster).jpeg```æ˜¯1940å¹´ç”µå½±Rebeccaçš„äº1939å‘å¸ƒçš„æµ·æŠ¥ã€‚\n\n![](assets/mm_tutorial/Rebecca_(1939_poster)_Small.jpeg)\n\næˆ‘ä»¬æ¥é—®ä¸€é—®Qwen-VL-Chatæµ·æŠ¥ä¸Šç”µå½±çš„åç§°æ˜¯ä»€ä¹ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨tokenizer.from_list_formatå¯ä»¥å¯¹å›¾æ–‡æ··æ’è¾“å…¥è¿›è¡Œåˆ†è¯ä¸å¤„ç†ï¼š\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Rebecca_(1939_poster).jpeg'},\n    {'text': 'What is the name of the movie in the poster?'},\n])\n```\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨```model.chat```å‘Qwen-VL-Chatæ¨¡å‹æé—®å¹¶è·å¾—å›å¤ã€‚æ³¨æ„åœ¨ç¬¬ä¸€æ¬¡æé—®æ—¶ï¼Œå¯¹è¯å†å²ä¸ºç©ºï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨```history=None```ã€‚\n```python\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\næ‚¨åº”è¯¥ä¼šå¾—åˆ°ç±»ä¼¼ä¸‹åˆ—çš„è¾“å‡ºç»“æœï¼š\n\n> The name of the movie in the poster is \"Rebecca.\"\n\nè¿™è¯´æ˜æ¨¡å‹æ­£ç¡®çš„å›ç­”äº†é—®é¢˜ï¼æ ¹æ®æµ·æŠ¥ï¼Œè¯¥ç”µå½±çš„åç§°çš„ç¡®æ˜¯**Rebecca**ã€‚\n\n#### **å¤šè½®é—®ç­”**\næˆ‘ä»¬è¿˜å¯ä»¥ç»§ç»­å‘æ¨¡å‹å‘é—®ï¼Œä¾‹å¦‚è¯¢é—®ç”µå½±çš„å¯¼æ¼”æ˜¯è°ã€‚åœ¨åç»­æé—®æ—¶ï¼Œå¯¹è¯å†å²å¹¶ä¸ä¸ºç©ºï¼Œæˆ‘ä»¬ä½¿ç”¨```history=history```å‘```model.chat```ä¼ é€’ä¹‹å‰çš„å¯¹è¯å†å²ï¼š\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Who directed this movie?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\næ‚¨åº”è¯¥ä¼šå¾—åˆ°ç±»ä¼¼ä¸‹åˆ—çš„è¾“å‡ºç»“æœï¼š\n\n> The movie \"Rebecca\" was directed by Alfred Hitchcock.\n\næ¨¡å‹å†æ¬¡æ­£ç¡®å›ç­”äº†é—®é¢˜ï¼æ ¹æ®æµ·æŠ¥ï¼Œè¯¥ç”µå½±çš„å¯¼æ¼”æ˜¯Alfred Hitchcockã€‚\n\n### **æ–‡å­—ç†è§£**\nQwen-VL-Chatå…·æœ‰ä¸€å®šçš„é’ˆå¯¹åŒ…å«å¯†é›†æ–‡å­—å›¾ç‰‡çš„ç†è§£èƒ½åŠ›ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ–‡ä»¶```assets/mm_tutorial/Hospital.jpeg```æ˜¯ä¸€ä¸ªåŒ…å«å¯†é›†æ–‡å­—çš„åŒ»é™¢æŒ‡ç¤ºç‰Œã€‚\n\n![](assets/mm_tutorial/Hospital_Small.jpg)\n\næˆ‘ä»¬å¯ä»¥åƒä¹‹å‰ä¸€æ ·å‘æ¨¡å‹è¯¢é—®åŒ»é™¢ä¸­å„ä¸ªç§‘å®¤çš„ä½ç½®ï¼Œå¯¹è¯å†å²ä¸ºç©ºï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨```history=None```ã€‚\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Hospital.jpg'},\n    {'text': 'Based on the photo, which floor is the Department of Otorhinolaryngology on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\næ‚¨åº”è¯¥ä¼šå¾—åˆ°ç±»ä¼¼ä¸‹åˆ—çš„è¾“å‡ºç»“æœï¼š\n\n> The Department of Otorhinolaryngology is located on the 4th floor.\n\næ‚¨åŒæ ·å¯ä»¥è¿›ä¸€æ­¥æå‡ºåç»­é—®é¢˜ï¼Œæ­¤æ—¶éœ€è¦ä½¿ç”¨```history=history```å‘```model.chat```ä¼ é€’ä¹‹å‰çš„å¯¹è¯å†å²ã€‚\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Based on the photo, which floor is the Department of Surgery on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\næ‚¨åº”è¯¥ä¼šå¾—åˆ°ç±»ä¼¼ä¸‹åˆ—çš„è¾“å‡ºç»“æœï¼š\n\n> The Department of Surgery is located on the 3rd floor.\n\n### **å›¾è¡¨æ•°å­¦æ¨ç†**\nåˆ©ç”¨æ¨¡å‹çš„å›¾è¡¨ç†è§£å’Œæ•°å­¦æ¨ç†èƒ½åŠ›ï¼ŒQwen-VL-Chatè¿˜å¯ä»¥å®Œæˆæ›´å¤æ‚çš„ä¸€äº›ä»»åŠ¡ï¼å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ–‡ä»¶```assets/mm_tutorial/Menu.jpeg```å±•ç¤ºäº†ä¸€å®¶é¤å…çš„èœå•ã€‚ç°åœ¨æˆ‘ä»¬æƒ³çŸ¥é“ï¼Œå¦‚æœè´­ä¹°ä¸¤ä¸ªSalmon Burgerå’Œä¸‰ä¸ªMeat Lover's Pizzaéœ€è¦èŠ±å¤šå°‘é’±å‘¢ï¼Ÿ\n\n![](assets/mm_tutorial/Menu.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Menu.jpeg'},\n    {'text': 'How much would I pay if I want to order two Salmon Burger and three Meat Lover\\'s Pizza? Think carefully step by step.'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n```Think carefully step by step.```æ˜¯ä¸€ä¸ªå¼•å¯¼æ¨¡å‹åˆ†æ­¥å¤„ç†å¤æ‚ä»»åŠ¡çš„å¸¸è§æç¤ºè¯ï¼Œå¦‚æœæ‚¨éœ€è¦å®Œæˆçš„ä»»åŠ¡è¾ƒä¸ºå¤æ‚ï¼Œå¯ä»¥è¯•ç€ä½¿ç”¨å®ƒæ¥æé«˜å‡†ç¡®ç‡ã€‚æ‚¨åº”è¯¥ä¼šå¾—åˆ°ç±»ä¼¼ä¸‹åˆ—çš„è¾“å‡ºç»“æœï¼š\n\n> To order two Salmon Burgers and three Meat Lover's Pizzas, you would need to pay the following:\n> \n> 1. For two Salmon Burgers: x2 Salmon Burgers at $10 each = $20\n> 2. For three Meat Lover's Pizzas: x3 Meat Lover's Pizzas at $12 each = $36\n> \n> Therefore, the total cost would be $56.\n\n### **å¤šå›¾ç†è§£ä¸ä¸­æ–‡è¾“å…¥**\nåœ¨ä¹‹å‰çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦å±•ç¤ºäº†Qwen-VL-Chaté’ˆå¯¹å•å¼ å›¾åƒå’Œè‹±æ–‡é—®é¢˜çš„é—®ç­”èƒ½åŠ›ã€‚ä½†å®é™…ä¸Šï¼ŒQwen-VL-Chatæ˜¯æ”¯æŒä¸­æ–‡è¾“å…¥çš„å¤šè¯­è¨€æ¨¡å‹ï¼Œè€Œä¸”ä¹Ÿæ”¯æŒå¤šå¼ å›¾ç‰‡çš„è¾“å…¥ï¼ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸­æ–‡è®©Qwen-VL-Chatæ¥ä¸ºæˆ‘ä»¬æ¯”è¾ƒé‡åº†å’ŒåŒ—äº¬è¿™ä¸¤ä¸ªåŸå¸‚çš„ç…§ç‰‡ï¼ˆ```assets/mm_tutorial/Chongqing.jpeg```å’Œ```assets/mm_tutorial/Beijing.jpeg```ï¼‰ï¼š\n\n![](assets/mm_tutorial/Chongqing_Small.jpeg)\n\n![](assets/mm_tutorial/Beijing_Small.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Chongqing.jpeg'},\n    {'image': 'assets/mm_tutorial/Beijing.jpeg'},\n    {'text': 'ä¸Šé¢ä¸¤å¼ å›¾ç‰‡åˆ†åˆ«æ˜¯å“ªä¸¤ä¸ªåŸå¸‚ï¼Ÿè¯·å¯¹å®ƒä»¬è¿›è¡Œå¯¹æ¯”ã€‚'},\n])\ntorch.manual_seed(5678)\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\næ‚¨åº”è¯¥ä¼šå¾—åˆ°ç±»ä¼¼ä¸‹åˆ—çš„è¾“å‡ºç»“æœï¼š\n\n> ç¬¬ä¸€å¼ å›¾ç‰‡æ˜¯é‡åº†çš„åŸå¸‚å¤©é™…çº¿ï¼Œå®ƒåæ˜ äº†ç°ä»£éƒ½å¸‚çš„ç¹åä¸å–§åš£ã€‚ç¬¬äºŒå¼ å›¾ç‰‡æ˜¯åŒ—äº¬çš„å¤©é™…çº¿ï¼Œå®ƒè±¡å¾ç€ä¸­å›½é¦–éƒ½çš„ç°ä»£åŒ–å’Œå›½é™…åŒ–ã€‚ä¸¤åº§åŸå¸‚éƒ½æ˜¯ä¸­å›½çš„é‡è¦åŸå¸‚ï¼Œæ‹¥æœ‰ç‹¬ç‰¹çš„æ–‡åŒ–å’Œå‘å±•å†å²ã€‚\n\n**è¯·æ³¨æ„ï¼ŒåŸå¸‚é—´çš„æ¯”è¾ƒæ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å½“ä¸»è§‚æ€§çš„é—®é¢˜ï¼Œå› æ­¤æ¨¡å‹äº§ç”Ÿçš„å›å¤å¯èƒ½å…·æœ‰ç›¸å½“é«˜çš„éšæœºæ€§ã€‚è‹¥ä¸ä½¿ç”¨```torch.manual_seed(5678)```è®¾ç½®éšæœºæ•°ç§å­ï¼Œæ¯æ¬¡çš„è¾“å‡ºç»“æœä¼šä¸ä¸€æ ·ã€‚å³ä½¿æ‚¨è®¾ç½®äº†éšæœºæ•°ç§å­ï¼Œç”±äºè½¯ç¡¬ä»¶ç¯å¢ƒçš„å·®å¼‚ï¼Œå¾—åˆ°çš„ç»“æœä¹Ÿå¯èƒ½ä¸æœ¬æ–‡æ¡£ä¸­çš„æœ‰æ‰€ä¸åŒã€‚**\n\n### **Groundingèƒ½åŠ›**\nåœ¨æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºQwen-VL-Chatæ¨¡å‹äº§ç”ŸåŒ…å›´æ¡†çš„èƒ½åŠ›ã€‚Qwen-VL-Chatå¯ä»¥æ ¹æ®æ‚¨çš„è¯­è¨€æè¿°ï¼Œåœ¨å›¾åƒä¸­ç”¨çŸ©å½¢æ¡†æ¡†å‡ºæŒ‡å®šåŒºåŸŸã€‚è¿™æ ·è¯´å¯èƒ½æœ‰äº›æŠ½è±¡ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸‹é¢çš„ä¾‹å­ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ–‡ä»¶```assets/mm_tutorial/Shanghai.jpg```æ˜¯ä¸Šæµ·çš„ä¸€å¼ ç…§ç‰‡ï¼Œæˆ‘ä»¬å…ˆç”¨å¸¸è§„çš„æç¤ºè¯ï¼Œé—®ä¸€ä¸‹æ¨¡å‹å›¾é‡Œæœ‰ä»€ä¹ˆã€‚\n\n![](assets/mm_tutorial/Shanghai_Small.jpeg)\n\n```python\ntorch.manual_seed(1234)\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Shanghai.jpg'},\n    {'text': 'å›¾é‡Œæœ‰å•¥'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\næ‚¨åº”è¯¥ä¼šå¾—åˆ°ç±»ä¼¼ä¸‹åˆ—çš„è¾“å‡ºç»“æœï¼š\n\n> å›¾ä¸­æ˜¯ä¸­å›½ä¸Šæµ·çš„å¤©é™…çº¿ï¼ŒåŒ…æ‹¬äº†ä¸Šæµ·å¡”ã€é‡‘èŒ‚å¤§å¦ã€ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒã€æµ·æ´‹å¤§å¦ç­‰è‘—åå»ºç­‘ã€‚\n\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨```è¯·ç»™æˆ‘æ¡†å‡ºå›¾ä¸­ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒå’Œä¸œæ–¹æ˜ç ```è¿™ä¸ªæç¤ºè¯æ¥å’Œæ¨¡å‹å¯¹è¯ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚æ³¨æ„æ­¤æ—¶éœ€è¦ä½¿ç”¨```history=history```å‘```model.chat```ä¼ é€’ä¹‹å‰çš„å¯¹è¯å†å²ã€‚\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'è¯·ç»™æˆ‘æ¡†å‡ºå›¾ä¸­ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒå’Œä¸œæ–¹æ˜ç '},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\næ‚¨åº”è¯¥ä¼šå¾—åˆ°ç±»ä¼¼ä¸‹åˆ—çš„è¾“å‡ºç»“æœï¼š\n```xml\n<ref>ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒ</ref><box>(667,437),(760,874)</box>å’Œ<ref>ä¸œæ–¹æ˜ç </ref><box>(506,75),(582,946)</box>\n```\nQwen-VL-Chatæ¨¡å‹æ²¡æœ‰æ‰‹ï¼Œä½†ä¹Ÿæ²¡æœ‰æ‹’ç»æ‚¨çš„è¯·æ±‚ï¼Œè€Œæ˜¯è¾“å‡ºäº†ä¸€äº›â€œå¥‡æ€ªâ€çš„ä¸œè¥¿â€”â€”å¹¶ä¸æ˜¯ï¼Œå®é™…ä¸Šï¼Œæ¨¡å‹çš„è¾“å‡ºä»¥æ ‡è®°è¯­è¨€çš„å½¢å¼ç»™å‡ºäº†ä¸Šæµ·ç¯çƒé‡‘èä¸­å¿ƒå’Œä¸œæ–¹æ˜ç åœ¨å›¾ä¸­çš„å…·ä½“ä½ç½®ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä¸‹åˆ—ä»£ç å°†å…¶å¯è§†åŒ–ï¼š\n```python\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nimage.save('Shanghai_Output.jpg')\n```\nä¿å­˜ä¸‹æ¥çš„```Shanghai_Output.jpg```ç»“æœå°†ç±»ä¼¼äºä¸‹é¢çš„æˆªå›¾ï¼š\n\n![](assets/mm_tutorial/Shanghai_Output_Small.jpeg)\n\nåœ¨æ­¤ä¹‹åï¼Œæ‚¨è¿˜å¯ä»¥ç»§ç»­ç…§å¸¸å’ŒQwen-VL-Chatå¯¹è¯ï¼š\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'å¸®æˆ‘å†™ä¸ªè¿™åº§åŸå¸‚çš„æ—…æ¸¸è®¡åˆ’'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\næ‚¨åº”è¯¥ä¼šå¾—åˆ°ç±»ä¼¼ä¸‹åˆ—çš„è¾“å‡ºç»“æœï¼š\n\n> å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ä¸Šæµ·æ—…æ¸¸è®¡åˆ’ï¼š\n>\n> ç¬¬ä¸€å¤©ï¼š\n> ä¸Šåˆï¼šæŠµè¾¾ä¸Šæµ·ï¼Œå‰å¾€é…’åº—åŠç†å…¥ä½æ‰‹ç»­ã€‚\n> ä¸­åˆï¼šäº«ç”¨åˆé¤åï¼Œå‰å¾€å¤–æ»©ï¼Œæ¬£èµé»„æµ¦æ±Ÿç•”çš„ç¾æ™¯ï¼Œæ¸¸è§ˆä¸Šæµ·åœ°æ ‡æ€§å»ºç­‘å¦‚æµ¦å‘é“¶è¡Œå¤§æ¥¼ã€æ±‡ä¸°é“¶è¡Œå¤§æ¥¼ç­‰ã€‚\n> ä¸‹åˆï¼šæ¸¸è§ˆå—äº¬è·¯æ­¥è¡Œè¡—ï¼Œè´­ä¹°ç‰¹è‰²ç¤¼å“æˆ–å“å°å½“åœ°ç¾é£Ÿã€‚\n> æ™šä¸Šï¼šåœ¨å—äº¬è·¯é™„è¿‘çš„é¤å…äº«ç”¨æ™šé¤ï¼Œç„¶åå»çœ‹ä¸Šæµ·çš„å¤œæ™¯ã€‚\n>\n> ç¬¬äºŒå¤©ï¼š\n> ä¸Šåˆï¼šå‰å¾€ä¸Šæµ·ç§‘æŠ€é¦†ï¼Œäº†è§£ç§‘æŠ€å‘å±•å†å²ï¼Œè§‚çœ‹å„ç§ç§‘æŠ€å±•è§ˆã€‚\n> ä¸­åˆï¼šåœ¨ç§‘æŠ€é¦†é™„è¿‘çš„é¤å…äº«ç”¨åˆé¤ã€‚\n> ä¸‹åˆï¼šæ¸¸è§ˆä¸–çºªå…¬å›­ï¼Œæ¬£èµç¾æ™¯å¹¶æ”¾æ¾èº«å¿ƒã€‚\n> æ™šä¸Šï¼šåœ¨å—äº¬è·¯æˆ–é™„è¿‘çš„é™†å®¶å˜´åœ°åŒºäº«ç”¨æ™šé¤ï¼Œç„¶åå»çœ‹ä¸Šæµ·çš„å¤œæ™¯ã€‚\n>\n> ç¬¬ä¸‰å¤©ï¼š\n> ä¸Šåˆï¼šæ¸¸è§ˆä¸Šæµ·è¿ªå£«å°¼ä¹å›­æˆ–ä¸Šæµ·æµ·æ˜Œæµ·æ´‹å…¬å›­ï¼Œä¸å„ç§è¿ªå£«å°¼è§’è‰²äº’åŠ¨ï¼Œæˆ–è€…åœ¨æµ·æ´‹å…¬å›­è§‚çœ‹æµ·æ´‹ç”Ÿç‰©è¡¨æ¼”ã€‚\n> ä¸­åˆï¼šåœ¨è¿ªå£«å°¼ä¹å›­æˆ–æµ·æ´‹å…¬å›­é™„è¿‘çš„é¤å…äº«ç”¨åˆé¤ã€‚\n> ä¸‹åˆï¼šè‡ªç”±æ´»åŠ¨ï¼Œå¯ä»¥å»è´­ç‰©ã€å“å°å½“åœ°ç¾é£Ÿæˆ–è€…å»åšç‰©é¦†ç­‰ã€‚\n> æ™šä¸Šï¼šåœ¨é…’åº—é™„è¿‘äº«ç”¨æ™šé¤ï¼Œç„¶åç¦»å¼€ä¸Šæµ·ã€‚\n>\n> å½“ç„¶ï¼Œä»¥ä¸Šåªæ˜¯ä¸€ä¸ªç®€å•çš„è®¡åˆ’ï¼Œä¸Šæµ·æœ‰è®¸å¤šå…¶ä»–æ™¯ç‚¹å’Œæ´»åŠ¨ï¼Œä¾‹å¦‚å‚è§‚ä¸Šæµ·åšç‰©é¦†ã€æ¸¸è§ˆç”°å­åŠã€è§‚çœ‹ä¸Šæµ·è¯å‰§ç­‰ã€‚å…·ä½“è®¡åˆ’å¯ä»¥æ ¹æ®ä¸ªäººå…´è¶£å’Œæ—¶é—´è¿›è¡Œè°ƒæ•´ã€‚\n\n**è¯·æ³¨æ„ï¼Œæ—…æ¸¸è®¡åˆ’æ˜¯ä¸€ä¸ªå…·æœ‰ç›¸å½“ä¸»è§‚æ€§çš„é—®é¢˜ï¼Œå› æ­¤æ¨¡å‹äº§ç”Ÿçš„å›å¤å¯èƒ½å…·æœ‰ç›¸å½“é«˜çš„éšæœºæ€§ã€‚è‹¥ä¸ä½¿ç”¨```torch.manual_seed(1234)```è®¾ç½®éšæœºæ•°ç§å­ï¼Œæ¯æ¬¡çš„è¾“å‡ºç»“æœä¼šä¸ä¸€æ ·ã€‚å³ä½¿æ‚¨è®¾ç½®äº†éšæœºæ•°ç§å­ï¼Œç”±äºè½¯ç¡¬ä»¶ç¯å¢ƒçš„å·®å¼‚ï¼Œå¾—åˆ°çš„ç»“æœä¹Ÿå¯èƒ½ä¸æœ¬æ–‡æ¡£ä¸­çš„æœ‰æ‰€ä¸åŒã€‚**\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval_mm",
          "type": "tree",
          "content": null
        },
        {
          "name": "finetune.py",
          "type": "blob",
          "size": 12.1865234375,
          "content": "# This code is based on the revised code from fastchat based on tatsu-lab/stanford_alpaca.\n\n\nfrom dataclasses import dataclass, field\nimport json\nimport math\nimport logging\nimport os\nfrom typing import Dict, Optional, List\nimport torch\nfrom torch.utils.data import Dataset\nfrom deepspeed import zero\nfrom deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\nimport transformers\nfrom transformers import Trainer, GPTQConfig, deepspeed\nfrom transformers.trainer_pt_utils import LabelSmoother\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom accelerate.utils import DistributedType\n\nIGNORE_TOKEN_ID = LabelSmoother.ignore_index\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"Qwen/Qwen-7B\")\n\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the training data.\"}\n    )\n    eval_data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the evaluation data.\"}\n    )\n    lazy_preprocess: bool = False\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=8192,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n    use_lora: bool = False\n    fix_vit: bool = True\n\n\n@dataclass\nclass LoraArguments:\n    lora_r: int = 64\n    lora_alpha: int = 16\n    lora_dropout: float = 0.05\n    lora_target_modules: List[str] = field(\n        default_factory=lambda: [\"c_attn\", \"attn.c_proj\", \"w1\", \"w2\"] ##[\"in_proj\",\"out_proj\",\"c_fc\"]\n    )\n    lora_weight_path: str = \"\"\n    lora_bias: str = \"none\"\n    q_lora: bool = False\n\n\ndef maybe_zero_3(param):\n    if hasattr(param, \"ds_id\"):\n        assert param.ds_status == ZeroParamStatus.NOT_AVAILABLE\n        with zero.GatheredParameters([param]):\n            param = param.data.detach().cpu().clone()\n    else:\n        param = param.detach().cpu().clone()\n    return param\n\n\n# Borrowed from peft.utils.get_peft_model_state_dict\ndef get_peft_state_maybe_zero_3(named_params, bias):\n    if bias == \"none\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k}\n    elif bias == \"all\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k or \"bias\" in k}\n    elif bias == \"lora_only\":\n        to_return = {}\n        maybe_lora_bias = {}\n        lora_bias_names = set()\n        for k, t in named_params:\n            if \"lora_\" in k:\n                to_return[k] = t\n                bias_name = k.split(\"lora_\")[0] + \"bias\"\n                lora_bias_names.add(bias_name)\n            elif \"bias\" in k:\n                maybe_lora_bias[k] = t\n        for k, t in maybe_lora_bias:\n            if bias_name in lora_bias_names:\n                to_return[bias_name] = t\n    else:\n        raise NotImplementedError\n    to_return = {k: maybe_zero_3(v) for k, v in to_return.items()}\n    return to_return\n\nlocal_rank = None\n\ndef rank0_print(*args):\n    if local_rank == 0:\n        print(*args)\n\n\ndef safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str, bias=\"none\"):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n    # check if zero3 mode enabled\n    if deepspeed.is_deepspeed_zero3_enabled():\n        state_dict = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()\n    else:\n        if trainer.args.use_lora:\n            state_dict = get_peft_state_maybe_zero_3(\n                trainer.model.named_parameters(), bias\n            )\n        else:\n            state_dict = trainer.model.state_dict()\n    if trainer.args.should_save and trainer.args.local_rank == 0:\n        trainer._save(output_dir, state_dict=state_dict)\n\n\ndef preprocess(\n    sources,\n    tokenizer: transformers.PreTrainedTokenizer,\n    max_len: int,\n    system_message: str = \"You are a helpful assistant.\"\n) -> Dict:\n    roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\n\n    im_start = tokenizer.im_start_id\n    im_end = tokenizer.im_end_id\n    nl_tokens = tokenizer('\\n').input_ids\n    _system = tokenizer('system').input_ids + nl_tokens\n    _user = tokenizer('user').input_ids + nl_tokens\n    _assistant = tokenizer('assistant').input_ids + nl_tokens\n\n    # Apply prompt templates\n    input_ids, targets = [], []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != roles[\"user\"]:\n            source = source[1:]\n\n        input_id, target = [], []\n        system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n        input_id += system\n        target += [im_start] + [IGNORE_TOKEN_ID] * (len(system)-3) + [im_end] + nl_tokens\n        assert len(input_id) == len(target)\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            _input_id = tokenizer(role).input_ids + nl_tokens + \\\n                tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\n            input_id += _input_id\n            if role == '<|im_start|>user':\n                _target = [im_start] + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + [im_end] + nl_tokens\n            elif role == '<|im_start|>assistant':\n                _target = [im_start] + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \\\n                    _input_id[len(tokenizer(role).input_ids)+1:-2] + [im_end] + nl_tokens\n            else:\n                raise NotImplementedError\n            target += _target\n        assert len(input_id) == len(target)\n        input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n        target += [IGNORE_TOKEN_ID] * (max_len - len(target))\n        input_ids.append(input_id[:max_len])\n        targets.append(target[:max_len])\n    input_ids = torch.tensor(input_ids, dtype=torch.int)\n    targets = torch.tensor(targets, dtype=torch.int)\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n    )\n\n\nclass SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, max_len: int):\n        super(SupervisedDataset, self).__init__()\n\n        rank0_print(\"Formatting inputs...\")\n        sources = [example[\"conversations\"] for example in raw_data]\n        data_dict = preprocess(sources, tokenizer, max_len)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n        self.attention_mask = data_dict[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(\n            input_ids=self.input_ids[i],\n            labels=self.labels[i],\n            attention_mask=self.attention_mask[i],\n        )\n\n\nclass LazySupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, max_len: int):\n        super(LazySupervisedDataset, self).__init__()\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n        self.tokenizer = tokenizer\n        self.raw_data = raw_data\n        self.cached_data_dict = {}\n\n    def __len__(self):\n        return len(self.raw_data)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        if i in self.cached_data_dict:\n            return self.cached_data_dict[i]\n\n        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer, self.max_len)\n        ret = dict(\n            input_ids=ret[\"input_ids\"][0],\n            labels=ret[\"labels\"][0],\n            attention_mask=ret[\"attention_mask\"][0],\n        )\n        self.cached_data_dict[i] = ret\n\n        return ret\n\n\ndef make_supervised_data_module(\n    tokenizer: transformers.PreTrainedTokenizer, data_args, max_len,\n) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    dataset_cls = (\n        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n    )\n    rank0_print(\"Loading data...\")\n\n    train_json = json.load(open(data_args.data_path, \"r\"))\n    train_dataset = dataset_cls(train_json, tokenizer=tokenizer, max_len=max_len)\n\n    if data_args.eval_data_path:\n        eval_json = json.load(open(data_args.eval_data_path, \"r\"))\n        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer, max_len=max_len)\n    else:\n        eval_dataset = None\n\n    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n\n\ndef train():\n    global local_rank\n    \n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments, LoraArguments)\n    )\n    (\n        model_args,\n        data_args,\n        training_args,\n        lora_args,\n    ) = parser.parse_args_into_dataclasses()\n\n    if getattr(training_args, 'deepspeed', None) and getattr(lora_args, 'q_lora', False):\n        training_args.distributed_state.distributed_type = DistributedType.DEEPSPEED\n\n    compute_dtype = (\n        torch.float16\n        if training_args.fp16\n        else (torch.bfloat16 if training_args.bf16 else torch.float32)\n    )\n\n    local_rank = training_args.local_rank\n\n    device_map = None\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    if lora_args.q_lora:\n        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)} if ddp else None\n        if len(training_args.fsdp) > 0 or deepspeed.is_deepspeed_zero3_enabled():\n            logging.warning(\n                \"FSDP or ZeRO3 are not incompatible with QLoRA.\"\n            )\n\n    # Set RoPE scaling factor\n    config = transformers.AutoConfig.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        trust_remote_code=True,\n    )\n    config.use_cache = False\n\n    # Load model and tokenizer\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        config=config,\n        cache_dir=training_args.cache_dir,\n        device_map=device_map,\n        trust_remote_code=True,\n        quantization_config=GPTQConfig(\n            bits=4, disable_exllama=True\n        )\n        if training_args.use_lora and lora_args.q_lora\n        else None,\n    )\n\n    if not training_args.use_lora:\n        if training_args.fix_vit and hasattr(model,'transformer') and hasattr(model.transformer,'visual'):\n            model.transformer.visual.requires_grad_(False)\n            if hasattr(model.transformer.visual,'attn_pool'):\n                model.transformer.visual.attn_pool.requires_grad_(True)\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n        trust_remote_code=True,\n    )\n    tokenizer.pad_token_id = tokenizer.eod_id\n\n    if training_args.use_lora:\n        if lora_args.q_lora or \"chat\" in model_args.model_name_or_path.lower():\n            modules_to_save = None\n        else:\n            modules_to_save = [\"wte\", \"lm_head\"]\n        lora_config = LoraConfig(\n            r=lora_args.lora_r,\n            lora_alpha=lora_args.lora_alpha,\n            target_modules=lora_args.lora_target_modules,\n            lora_dropout=lora_args.lora_dropout,\n            bias=lora_args.lora_bias,\n            task_type=\"CAUSAL_LM\",\n            modules_to_save=modules_to_save  # This argument serves for adding new tokens.\n        )\n        if lora_args.q_lora:\n            model = prepare_model_for_kbit_training(\n                model, use_gradient_checkpointing=training_args.gradient_checkpointing\n            )\n\n        model = get_peft_model(model, lora_config)\n\n        if training_args.gradient_checkpointing:\n            model.enable_input_require_grads()\n\n    # Load data\n    data_module = make_supervised_data_module(\n        tokenizer=tokenizer, data_args=data_args, max_len=training_args.model_max_length\n    )\n\n    # Start trainner\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n\n    trainer.train()\n    trainer.save_state()\n\n    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir, bias=lora_args.lora_bias)\n\n\nif __name__ == \"__main__\":\n    train()\n"
        },
        {
          "name": "finetune",
          "type": "tree",
          "content": null
        },
        {
          "name": "openai_api.py",
          "type": "blob",
          "size": 17.2734375,
          "content": "# coding=utf-8\n# Implements API for Qwen-7B in OpenAI's format. (https://platform.openai.com/docs/api-reference/chat)\n# Usage: python openai_api.py\n# Visit http://localhost:8000/docs for documents.\n\nimport re\nimport copy\nimport json\nimport time\nfrom argparse import ArgumentParser\nfrom contextlib import asynccontextmanager\nfrom typing import Dict, List, Literal, Optional, Union\n\nimport torch\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field\nfrom sse_starlette.sse import EventSourceResponse\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers.generation import GenerationConfig\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):  # collects GPU memory\n    yield\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n\n\napp = FastAPI(lifespan=lifespan)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\nclass ModelCard(BaseModel):\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"owner\"\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: Optional[list] = None\n\n\nclass ModelList(BaseModel):\n    object: str = \"list\"\n    data: List[ModelCard] = []\n\n\nclass ChatMessage(BaseModel):\n    role: Literal[\"user\", \"assistant\", \"system\", \"function\"]\n    content: Optional[str]\n    function_call: Optional[Dict] = None\n\n\nclass DeltaMessage(BaseModel):\n    role: Optional[Literal[\"user\", \"assistant\", \"system\"]] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: List[ChatMessage]\n    functions: Optional[List[Dict]] = None\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    max_length: Optional[int] = None\n    stream: Optional[bool] = False\n    stop: Optional[List[str]] = None\n\n\nclass ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: ChatMessage\n    finish_reason: Literal[\"stop\", \"length\", \"function_call\"]\n\n\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]]\n\n\nclass ChatCompletionResponse(BaseModel):\n    model: str\n    object: Literal[\"chat.completion\", \"chat.completion.chunk\"]\n    choices: List[\n        Union[ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice]\n    ]\n    created: Optional[int] = Field(default_factory=lambda: int(time.time()))\n\n\n@app.get(\"/v1/models\", response_model=ModelList)\nasync def list_models():\n    global model_args\n    model_card = ModelCard(id=\"gpt-3.5-turbo\")\n    return ModelList(data=[model_card])\n\n\n# To work around that unpleasant leading-\\n tokenization issue!\ndef add_extra_stop_words(stop_words):\n    if stop_words:\n        _stop_words = []\n        _stop_words.extend(stop_words)\n        for x in stop_words:\n            s = x.lstrip(\"\\n\")\n            if s and (s not in _stop_words):\n                _stop_words.append(s)\n        return _stop_words\n    return stop_words\n\n\ndef trim_stop_words(response, stop_words):\n    if stop_words:\n        for stop in stop_words:\n            idx = response.find(stop)\n            if idx != -1:\n                response = response[:idx]\n    return response\n\n\nTOOL_DESC = \"\"\"{name_for_model}: Call this tool to interact with the {name_for_human} API. What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters}\"\"\"\n\nREACT_INSTRUCTION = \"\"\"Answer the following questions as best you can. You have access to the following APIs:\n\n{tools_text}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tools_name_text}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\"\"\"\n\n_TEXT_COMPLETION_CMD = object()\n\n\n#\n# Temporarily, the system role does not work as expected.\n# We advise that you write the setups for role-play in your query,\n# i.e., use the user role instead of the system role.\n#\n# TODO: Use real system role when the model is ready.\n#\ndef parse_messages(messages, functions):\n    if all(m.role != \"user\" for m in messages):\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Invalid request: Expecting at least one user message.\",\n        )\n\n    messages = copy.deepcopy(messages)\n    default_system = \"You are a helpful assistant.\"\n    system = \"\"\n    if messages[0].role == \"system\":\n        system = messages.pop(0).content.lstrip(\"\\n\").rstrip()\n        if system == default_system:\n            system = \"\"\n\n    if functions:\n        tools_text = []\n        tools_name_text = []\n        for func_info in functions:\n            name = func_info.get(\"name\", \"\")\n            name_m = func_info.get(\"name_for_model\", name)\n            name_h = func_info.get(\"name_for_human\", name)\n            desc = func_info.get(\"description\", \"\")\n            desc_m = func_info.get(\"description_for_model\", desc)\n            tool = TOOL_DESC.format(\n                name_for_model=name_m,\n                name_for_human=name_h,\n                # Hint: You can add the following format requirements in description:\n                #   \"Format the arguments as a JSON object.\"\n                #   \"Enclose the code within triple backticks (`) at the beginning and end of the code.\"\n                description_for_model=desc_m,\n                parameters=json.dumps(func_info[\"parameters\"], ensure_ascii=False),\n            )\n            tools_text.append(tool)\n            tools_name_text.append(name_m)\n        tools_text = \"\\n\\n\".join(tools_text)\n        tools_name_text = \", \".join(tools_name_text)\n        system += \"\\n\\n\" + REACT_INSTRUCTION.format(\n            tools_text=tools_text,\n            tools_name_text=tools_name_text,\n        )\n        system = system.lstrip(\"\\n\").rstrip()\n\n    dummy_thought = {\n        \"en\": \"\\nThought: I now know the final answer.\\nFinal answer: \",\n        \"zh\": \"\\nThought: æˆ‘ä¼šä½œç­”äº†ã€‚\\nFinal answer: \",\n    }\n\n    _messages = messages\n    messages = []\n    for m_idx, m in enumerate(_messages):\n        role, content, func_call = m.role, m.content, m.function_call\n        if content:\n            content = content.lstrip(\"\\n\").rstrip()\n        if role == \"function\":\n            if (len(messages) == 0) or (messages[-1].role != \"assistant\"):\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Invalid request: Expecting role assistant before role function.\",\n                )\n            messages[-1].content += f\"\\nObservation: {content}\"\n            if m_idx == len(_messages) - 1:\n                messages[-1].content += \"\\nThought:\"\n        elif role == \"assistant\":\n            if len(messages) == 0:\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Invalid request: Expecting role user before role assistant.\",\n                )\n            last_msg = messages[-1].content\n            last_msg_has_zh = len(re.findall(r\"[\\u4e00-\\u9fff]+\", last_msg)) > 0\n            if func_call is None:\n                if functions:\n                    content = dummy_thought[\"zh\" if last_msg_has_zh else \"en\"] + content\n            else:\n                f_name, f_args = func_call[\"name\"], func_call[\"arguments\"]\n                if not content:\n                    if last_msg_has_zh:\n                        content = f\"Thought: æˆ‘å¯ä»¥ä½¿ç”¨ {f_name} APIã€‚\"\n                    else:\n                        content = f\"Thought: I can use {f_name}.\"\n                content = f\"\\n{content}\\nAction: {f_name}\\nAction Input: {f_args}\"\n            if messages[-1].role == \"user\":\n                messages.append(\n                    ChatMessage(role=\"assistant\", content=content.lstrip(\"\\n\").rstrip())\n                )\n            else:\n                messages[-1].content += content\n        elif role == \"user\":\n            messages.append(\n                ChatMessage(role=\"user\", content=content.lstrip(\"\\n\").rstrip())\n            )\n        else:\n            raise HTTPException(\n                status_code=400, detail=f\"Invalid request: Incorrect role {role}.\"\n            )\n\n    query = _TEXT_COMPLETION_CMD\n    if messages[-1].role == \"user\":\n        query = messages[-1].content\n        messages = messages[:-1]\n\n    if len(messages) % 2 != 0:\n        raise HTTPException(status_code=400, detail=\"Invalid request\")\n\n    history = []  # [(Q1, A1), (Q2, A2), ..., (Q_last_turn, A_last_turn)]\n    for i in range(0, len(messages), 2):\n        if messages[i].role == \"user\" and messages[i + 1].role == \"assistant\":\n            usr_msg = messages[i].content.lstrip(\"\\n\").rstrip()\n            bot_msg = messages[i + 1].content.lstrip(\"\\n\").rstrip()\n            if system and (i == len(messages) - 2):\n                usr_msg = f\"{system}\\n\\nQuestion: {usr_msg}\"\n                system = \"\"\n            for t in dummy_thought.values():\n                t = t.lstrip(\"\\n\")\n                if bot_msg.startswith(t) and (\"\\nAction: \" in bot_msg):\n                    bot_msg = bot_msg[len(t) :]\n            history.append([usr_msg, bot_msg])\n        else:\n            raise HTTPException(\n                status_code=400,\n                detail=\"Invalid request: Expecting exactly one user (or function) role before every assistant role.\",\n            )\n    if system:\n        assert query is not _TEXT_COMPLETION_CMD\n        query = f\"{system}\\n\\nQuestion: {query}\"\n    return query, history\n\n\ndef parse_response(response):\n    func_name, func_args = \"\", \"\"\n    i = response.rfind(\"\\nAction:\")\n    j = response.rfind(\"\\nAction Input:\")\n    k = response.rfind(\"\\nObservation:\")\n    if 0 <= i < j:  # If the text has `Action` and `Action input`,\n        if k < j:  # but does not contain `Observation`,\n            # then it is likely that `Observation` is omitted by the LLM,\n            # because the output text may have discarded the stop word.\n            response = response.rstrip() + \"\\nObservation:\"  # Add it back.\n        k = response.rfind(\"\\nObservation:\")\n        func_name = response[i + len(\"\\nAction:\") : j].strip()\n        func_args = response[j + len(\"\\nAction Input:\") : k].strip()\n    if func_name:\n        choice_data = ChatCompletionResponseChoice(\n            index=0,\n            message=ChatMessage(\n                role=\"assistant\",\n                content=response[:i],\n                function_call={\"name\": func_name, \"arguments\": func_args},\n            ),\n            finish_reason=\"function_call\",\n        )\n        return choice_data\n    z = response.rfind(\"\\nFinal Answer: \")\n    if z >= 0:\n        response = response[z + len(\"\\nFinal Answer: \") :]\n    choice_data = ChatCompletionResponseChoice(\n        index=0,\n        message=ChatMessage(role=\"assistant\", content=response),\n        finish_reason=\"stop\",\n    )\n    return choice_data\n\n\n# completion mode, not chat mode\ndef text_complete_last_message(history, stop_words_ids):\n    im_start = \"<|im_start|>\"\n    im_end = \"<|im_end|>\"\n    prompt = f\"{im_start}system\\nYou are a helpful assistant.{im_end}\"\n    for i, (query, response) in enumerate(history):\n        query = query.lstrip(\"\\n\").rstrip()\n        response = response.lstrip(\"\\n\").rstrip()\n        prompt += f\"\\n{im_start}user\\n{query}{im_end}\"\n        prompt += f\"\\n{im_start}assistant\\n{response}{im_end}\"\n    prompt = prompt[: -len(im_end)]\n\n    _stop_words_ids = [tokenizer.encode(im_end)]\n    if stop_words_ids:\n        for s in stop_words_ids:\n            _stop_words_ids.append(s)\n    stop_words_ids = _stop_words_ids\n\n    input_ids = torch.tensor([tokenizer.encode(prompt)]).to(model.device)\n    output = model.generate(input_ids, stop_words_ids=stop_words_ids).tolist()[0]\n    output = tokenizer.decode(output, errors=\"ignore\")\n    assert output.startswith(prompt)\n    output = output[len(prompt) :]\n    output = trim_stop_words(output, [\"<|endoftext|>\", im_end])\n    print(f\"<completion>\\n{prompt}\\n<!-- *** -->\\n{output}\\n</completion>\")\n    return output\n\n\n@app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\nasync def create_chat_completion(request: ChatCompletionRequest):\n    global model, tokenizer\n\n    stop_words = add_extra_stop_words(request.stop)\n    if request.functions:\n        stop_words = stop_words or []\n        if \"Observation:\" not in stop_words:\n            stop_words.append(\"Observation:\")\n\n    query, history = parse_messages(request.messages, request.functions)\n\n    if request.stream:\n        if request.functions:\n            raise HTTPException(\n                status_code=400,\n                detail=\"Invalid request: Function calling is not yet implemented for stream mode.\",\n            )\n        # generate = predict(query, history, request.model, stop_words)\n        # return EventSourceResponse(generate, media_type=\"text/event-stream\")\n        raise HTTPException(status_code=400, detail=\"Stream request is not supported currently.\")\n\n    stop_words_ids = [tokenizer.encode(s) for s in stop_words] if stop_words else None\n    if query is _TEXT_COMPLETION_CMD:\n        response = text_complete_last_message(history, stop_words_ids=stop_words_ids)\n    else:\n        response, _ = model.chat(\n            tokenizer,\n            query,\n            history=history,\n            stop_words_ids=stop_words_ids,\n            append_history=False,\n            top_p=request.top_p,\n            temperature=request.temperature,\n        )\n        print(f\"<chat>\\n{history}\\n{query}\\n<!-- *** -->\\n{response}\\n</chat>\")\n    response = trim_stop_words(response, stop_words)\n    if request.functions:\n        choice_data = parse_response(response)\n    else:\n        choice_data = ChatCompletionResponseChoice(\n            index=0,\n            message=ChatMessage(role=\"assistant\", content=response),\n            finish_reason=\"stop\",\n        )\n    return ChatCompletionResponse(\n        model=request.model, choices=[choice_data], object=\"chat.completion\"\n    )\n\n\nasync def predict(\n    query: str, history: List[List[str]], model_id: str, stop_words: List[str]\n):\n    global model, tokenizer\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0, delta=DeltaMessage(role=\"assistant\"), finish_reason=None\n    )\n    chunk = ChatCompletionResponse(\n        model=model_id, choices=[choice_data], object=\"chat.completion.chunk\"\n    )\n    yield \"{}\".format(chunk.model_dump_json(exclude_unset=True))\n\n    current_length = 0\n    stop_words_ids = [tokenizer.encode(s) for s in stop_words] if stop_words else None\n    if stop_words:\n        # TODO: It's a little bit tricky to trim stop words in the stream mode.\n        raise HTTPException(\n            status_code=400,\n            detail=\"Invalid request: custom stop words are not yet supported for stream mode.\",\n        )\n    response_generator = model.chat_stream(\n        tokenizer, query, history=history, stop_words_ids=stop_words_ids\n    )\n    for new_response in response_generator:\n        if len(new_response) == current_length:\n            continue\n\n        new_text = new_response[current_length:]\n        current_length = len(new_response)\n\n        choice_data = ChatCompletionResponseStreamChoice(\n            index=0, delta=DeltaMessage(content=new_text), finish_reason=None\n        )\n        chunk = ChatCompletionResponse(\n            model=model_id, choices=[choice_data], object=\"chat.completion.chunk\"\n        )\n        yield \"{}\".format(chunk.model_dump_json(exclude_unset=True))\n\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0, delta=DeltaMessage(), finish_reason=\"stop\"\n    )\n    chunk = ChatCompletionResponse(\n        model=model_id, choices=[choice_data], object=\"chat.completion.chunk\"\n    )\n    yield \"{}\".format(chunk.model_dump_json(exclude_unset=True))\n    yield \"[DONE]\"\n\n\ndef _get_args():\n    parser = ArgumentParser()\n    parser.add_argument(\n        \"-c\",\n        \"--checkpoint-path\",\n        type=str,\n        default=\"QWen/QWen-7B-Chat\",\n        help=\"Checkpoint name or path, default to %(default)r\",\n    )\n    parser.add_argument(\n        \"--cpu-only\", action=\"store_true\", help=\"Run demo with CPU only\"\n    )\n    parser.add_argument(\n        \"--server-port\", type=int, default=8000, help=\"Demo server port.\"\n    )\n    parser.add_argument(\n        \"--server-name\",\n        type=str,\n        default=\"127.0.0.1\",\n        help=\"Demo server name. Default: 127.0.0.1, which is only visible from the local computer.\"\n        \" If you want other computers to access your server, use 0.0.0.0 instead.\",\n    )\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    args = _get_args()\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.checkpoint_path,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n\n    if args.cpu_only:\n        device_map = \"cpu\"\n    else:\n        device_map = \"auto\"\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.checkpoint_path,\n        device_map=device_map,\n        trust_remote_code=True,\n        resume_download=True,\n    ).eval()\n\n    model.generation_config = GenerationConfig.from_pretrained(\n        args.checkpoint_path,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n\n    uvicorn.run(app, host=args.server_name, port=args.server_port, workers=1)"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1298828125,
          "content": "transformers==4.32.0\naccelerate\ntiktoken\neinops\ntransformers_stream_generator==0.0.4\nscipy\ntorchvision\npillow\ntensorboard\nmatplotlib\n"
        },
        {
          "name": "requirements_openai_api.txt",
          "type": "blob",
          "size": 0.044921875,
          "content": "fastapi\nuvicorn\nopenai\npydantic\nsse_starlette\n"
        },
        {
          "name": "requirements_web_demo.txt",
          "type": "blob",
          "size": 0.017578125,
          "content": "gradio\nmodelscope\n"
        },
        {
          "name": "touchstone",
          "type": "tree",
          "content": null
        },
        {
          "name": "web_demo_mm.py",
          "type": "blob",
          "size": 9.4677734375,
          "content": "# Copyright (c) Alibaba Cloud.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"A simple web interactive chat demo based on gradio.\"\"\"\n\nfrom argparse import ArgumentParser\nfrom pathlib import Path\n\nimport copy\nimport gradio as gr\nimport os\nimport re\nimport secrets\nimport tempfile\nfrom modelscope import (\n    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n)\n\nDEFAULT_CKPT_PATH = 'qwen/Qwen-VL-Chat'\nBOX_TAG_PATTERN = r\"<box>([\\s\\S]*?)</box>\"\nPUNCTUATION = \"ï¼ï¼Ÿã€‚ï¼‚ï¼ƒï¼„ï¼…ï¼†ï¼‡ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼šï¼›ï¼œï¼ï¼ï¼ ï¼»ï¼¼ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ï½ï½Ÿï½ ï½¢ï½£ï½¤ã€ã€ƒã€‹ã€Œã€ã€ã€ã€ã€‘ã€”ã€•ã€–ã€—ã€˜ã€™ã€šã€›ã€œã€ã€ã€Ÿã€°ã€¾ã€¿â€“â€”â€˜â€™â€›â€œâ€â€â€Ÿâ€¦â€§ï¹.\"\n\n\ndef _get_args():\n    parser = ArgumentParser()\n    parser.add_argument(\"-c\", \"--checkpoint-path\", type=str, default=DEFAULT_CKPT_PATH,\n                        help=\"Checkpoint name or path, default to %(default)r\")\n    parser.add_argument(\"--cpu-only\", action=\"store_true\", help=\"Run demo with CPU only\")\n\n    parser.add_argument(\"--share\", action=\"store_true\", default=False,\n                        help=\"Create a publicly shareable link for the interface.\")\n    parser.add_argument(\"--inbrowser\", action=\"store_true\", default=False,\n                        help=\"Automatically launch the interface in a new tab on the default browser.\")\n    parser.add_argument(\"--server-port\", type=int, default=8000,\n                        help=\"Demo server port.\")\n    parser.add_argument(\"--server-name\", type=str, default=\"127.0.0.1\",\n                        help=\"Demo server name.\")\n\n    args = parser.parse_args()\n    return args\n\n\ndef _load_model_tokenizer(args):\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.checkpoint_path, trust_remote_code=True, resume_download=True, revision='master',\n    )\n\n    if args.cpu_only:\n        device_map = \"cpu\"\n    else:\n        device_map = \"cuda\"\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.checkpoint_path,\n        device_map=device_map,\n        trust_remote_code=True,\n        resume_download=True,\n        revision='master',\n    ).eval()\n    model.generation_config = GenerationConfig.from_pretrained(\n        args.checkpoint_path, trust_remote_code=True, resume_download=True, revision='master',\n    )\n\n    return model, tokenizer\n\n\ndef _parse_text(text):\n    lines = text.split(\"\\n\")\n    lines = [line for line in lines if line != \"\"]\n    count = 0\n    for i, line in enumerate(lines):\n        if \"```\" in line:\n            count += 1\n            items = line.split(\"`\")\n            if count % 2 == 1:\n                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n            else:\n                lines[i] = f\"<br></code></pre>\"\n        else:\n            if i > 0:\n                if count % 2 == 1:\n                    line = line.replace(\"`\", r\"\\`\")\n                    line = line.replace(\"<\", \"&lt;\")\n                    line = line.replace(\">\", \"&gt;\")\n                    line = line.replace(\" \", \"&nbsp;\")\n                    line = line.replace(\"*\", \"&ast;\")\n                    line = line.replace(\"_\", \"&lowbar;\")\n                    line = line.replace(\"-\", \"&#45;\")\n                    line = line.replace(\".\", \"&#46;\")\n                    line = line.replace(\"!\", \"&#33;\")\n                    line = line.replace(\"(\", \"&#40;\")\n                    line = line.replace(\")\", \"&#41;\")\n                    line = line.replace(\"$\", \"&#36;\")\n                lines[i] = \"<br>\" + line\n    text = \"\".join(lines)\n    return text\n\ndef _remove_image_special(text):\n    text = text.replace('<ref>', '').replace('</ref>', '')\n    return re.sub(r'<box>.*?(</box>|$)', '', text)\n\ndef _launch_demo(args, model, tokenizer):\n    uploaded_file_dir = os.environ.get(\"GRADIO_TEMP_DIR\") or str(\n        Path(tempfile.gettempdir()) / \"gradio\"\n    )\n\n    def predict(_chatbot, task_history):\n        chat_query = _chatbot[-1][0]\n        query = task_history[-1][0]\n        print(\"User: \" + _parse_text(query))\n        history_cp = copy.deepcopy(task_history)\n        full_response = \"\"\n\n        history_filter = []\n        pic_idx = 1\n        pre = \"\"\n        for i, (q, a) in enumerate(history_cp):\n            if isinstance(q, (tuple, list)):\n                q = f'Picture {pic_idx}: <img>{q[0]}</img>'\n                pre += q + '\\n'\n                pic_idx += 1\n            else:\n                pre += q\n                history_filter.append((pre, a))\n                pre = \"\"\n        history, message = history_filter[:-1], history_filter[-1][0]\n        # response, history = model.chat(tokenizer, message, history=history)\n        for response in model.chat_stream(tokenizer, message, history=history):\n            _chatbot[-1] = (_parse_text(chat_query), _remove_image_special(_parse_text(response)))\n\n            yield _chatbot\n            full_response = _parse_text(response)\n\n        response = full_response\n        history.append((message, response))\n        image = tokenizer.draw_bbox_on_latest_picture(response, history)\n        if image is not None:\n            temp_dir = secrets.token_hex(20)\n            temp_dir = Path(uploaded_file_dir) / temp_dir\n            temp_dir.mkdir(exist_ok=True, parents=True)\n            name = f\"tmp{secrets.token_hex(5)}.jpg\"\n            filename = temp_dir / name\n            image.save(str(filename))\n            _chatbot.append((None, (str(filename),)))\n        else:\n            _chatbot[-1] = (_parse_text(chat_query), response)\n        # full_response = _parse_text(response)\n\n        task_history[-1] = (query, full_response)\n        print(\"Qwen-VL-Chat: \" + _parse_text(full_response))\n        yield _chatbot\n\n    def regenerate(_chatbot, task_history):\n        if not task_history:\n            return _chatbot\n        item = task_history[-1]\n        if item[1] is None:\n            return _chatbot\n        task_history[-1] = (item[0], None)\n        chatbot_item = _chatbot.pop(-1)\n        if chatbot_item[0] is None:\n            _chatbot[-1] = (_chatbot[-1][0], None)\n        else:\n            _chatbot.append((chatbot_item[0], None))\n        return predict(_chatbot, task_history)\n\n    def add_text(history, task_history, text):\n        task_text = text\n        if len(text) >= 2 and text[-1] in PUNCTUATION and text[-2] not in PUNCTUATION:\n            task_text = text[:-1]\n        history = history + [(_parse_text(text), None)]\n        task_history = task_history + [(task_text, None)]\n        return history, task_history, \"\"\n\n    def add_file(history, task_history, file):\n        history = history + [((file.name,), None)]\n        task_history = task_history + [((file.name,), None)]\n        return history, task_history\n\n    def reset_user_input():\n        return gr.update(value=\"\")\n\n    def reset_state(task_history):\n        task_history.clear()\n        return []\n\n    with gr.Blocks() as demo:\n        gr.Markdown(\"\"\"\\\n<p align=\"center\"><img src=\"https://modelscope.cn/api/v1/models/qwen/Qwen-7B-Chat/repo?\nRevision=master&FilePath=assets/logo.jpeg&View=true\" style=\"height: 80px\"/><p>\"\"\")\n        gr.Markdown(\"\"\"<center><font size=8>Qwen-VL-Chat Bot</center>\"\"\")\n        gr.Markdown(\n            \"\"\"\\\n<center><font size=3>This WebUI is based on Qwen-VL-Chat, developed by Alibaba Cloud. \\\n(æœ¬WebUIåŸºäºQwen-VL-Chatæ‰“é€ ï¼Œå®ç°èŠå¤©æœºå™¨äººåŠŸèƒ½ã€‚)</center>\"\"\")\n        gr.Markdown(\"\"\"\\\n<center><font size=4>Qwen-VL <a href=\"https://modelscope.cn/models/qwen/Qwen-VL/summary\">ğŸ¤– </a> \n| <a href=\"https://huggingface.co/Qwen/Qwen-VL\">ğŸ¤—</a>&nbsp ï½œ \nQwen-VL-Chat <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary\">ğŸ¤– </a> | \n<a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat\">ğŸ¤—</a>&nbsp ï½œ \n&nbsp<a href=\"https://github.com/QwenLM/Qwen-VL\">Github</a></center>\"\"\")\n\n        chatbot = gr.Chatbot(label='Qwen-VL-Chat', elem_classes=\"control-height\", height=750)\n        query = gr.Textbox(lines=2, label='Input')\n        task_history = gr.State([])\n\n        with gr.Row():\n            empty_bin = gr.Button(\"ğŸ§¹ Clear History (æ¸…é™¤å†å²)\")\n            submit_btn = gr.Button(\"ğŸš€ Submit (å‘é€)\")\n            regen_btn = gr.Button(\"ğŸ¤”ï¸ Regenerate (é‡è¯•)\")\n            addfile_btn = gr.UploadButton(\"ğŸ“ Upload (ä¸Šä¼ æ–‡ä»¶)\", file_types=[\"image\"])\n\n        submit_btn.click(add_text, [chatbot, task_history, query], [chatbot, task_history]).then(\n            predict, [chatbot, task_history], [chatbot], show_progress=True\n        )\n        submit_btn.click(reset_user_input, [], [query])\n        empty_bin.click(reset_state, [task_history], [chatbot], show_progress=True)\n        regen_btn.click(regenerate, [chatbot, task_history], [chatbot], show_progress=True)\n        addfile_btn.upload(add_file, [chatbot, task_history, addfile_btn], [chatbot, task_history], show_progress=True)\n\n        gr.Markdown(\"\"\"\\\n<font size=2>Note: This demo is governed by the original license of Qwen-VL. \\\nWe strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, \\\nincluding hate speech, violence, pornography, deception, etc. \\\n(æ³¨ï¼šæœ¬æ¼”ç¤ºå—Qwen-VLçš„è®¸å¯åè®®é™åˆ¶ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®ï¼Œç”¨æˆ·ä¸åº”ä¼ æ’­åŠä¸åº”å…è®¸ä»–äººä¼ æ’­ä»¥ä¸‹å†…å®¹ï¼Œ\\\nåŒ…æ‹¬ä½†ä¸é™äºä»‡æ¨è¨€è®ºã€æš´åŠ›ã€è‰²æƒ…ã€æ¬ºè¯ˆç›¸å…³çš„æœ‰å®³ä¿¡æ¯ã€‚)\"\"\")\n\n    demo.queue().launch(\n        share=args.share,\n        inbrowser=args.inbrowser,\n        server_port=args.server_port,\n        server_name=args.server_name,\n    )\n\n\ndef main():\n    args = _get_args()\n\n    model, tokenizer = _load_model_tokenizer(args)\n\n    _launch_demo(args, model, tokenizer)\n\n\nif __name__ == '__main__':\n    main()\n"
        }
      ]
    }
  ]
}