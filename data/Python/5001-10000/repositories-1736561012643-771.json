{
  "metadata": {
    "timestamp": 1736561012643,
    "page": 771,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "QwenLM/Qwen-VL",
      "stars": 5292,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.126953125,
          "content": "__pycache__\n*.so\nbuild\n.coverage_*\n*.egg-info\n*~\n.vscode/\n.idea/\n.DS_Store\n\n/private/\nQwen-VL-Chat/\nQwen-VL-Chat-Int4/\nSimSun.ttf\n"
        },
        {
          "name": "BUILD.md",
          "type": "blob",
          "size": 1.0048828125,
          "content": "## qwen web demo\n\n### build\n\n```\ndocker build -t qwen-vl-chat:webdemo --platform linux/amd64 -f Dockerfile.qwendemo . \n```\n\n### run\n\n```\ndocker run -it --gpus device=0 -d --restart always -v /var/run/docker.sock:/var/run/docker.sock --name qwen-vl-chat -p 8000:8000 --user=20001:20001 --platform linux/amd64 qwen-vl-chat:webdemo\n```\n\n## qwen openai api\n\n### build\n\n```\ndocker build -t qwen-vl-chat:openai --platform linux/amd64 -f Dockerfile.qwenopenai . \n```\n\n### run\n\n```\ndocker run -it --gpus device=0 -d --restart always -v /var/run/docker.sock:/var/run/docker.sock --name qwen-vl-chat -p 8080:8080 --user=20001:20001 --platform linux/amd64 qwen-vl-chat:openai\n```\n\n## qwen-int4 openai api\n\n### build\n\n```\ndocker build -t qwen-vl-chat:int4-openai --platform linux/amd64 -f Dockerfile.qwenint4openai . \n```\n\n### run\n\n```\ndocker run -it --gpus device=0 -d --restart always -v /var/run/docker.sock:/var/run/docker.sock --name qwen-vl-chat-int4 -p 8080:8080 --user=20001:20001 --platform linux/amd64 qwen-vl-chat:int4-openai\n```\n"
        },
        {
          "name": "Dockerfile.qwendemo",
          "type": "blob",
          "size": 1.646484375,
          "content": "# python 3.8 and above\n# pytorch 1.12 and above, 2.0 and above are recommended\n# CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\n\n# based on modelscope docker image\n# registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\n# registry.cn-beijing.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\nFROM registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\n\nARG workdir=/var/app\nRUN mkdir -p ${workdir}\n\nRUN git lfs install\n\nWORKDIR ${workdir}\nCOPY requirements.txt requirements_web_demo.txt ./\n\n# Install Qwen dependencies\nRUN pip install -r requirements.txt\n\n# Install webUI dependencies\nWORKDIR ${workdir}\nRUN pip install -r requirements_web_demo.txt\n\n# Offline mode, check https://huggingface.co/docs/transformers/v4.15.0/installation#offline-mode\nENV HF_DATASETS_OFFLINE=1\nENV TRANSFORMERS_OFFLINE=1\n\n# set TZ, make logs dir, and expose port 8080\nENV TZ=Asia/Shanghai\nRUN mkdir -p ${workdir}/logs && chmod 777 ${workdir}/logs\nVOLUME /var/app/logs\n\n# create user 20001\nRUN useradd -r -m appuser -u 20001 -g 0\n\nWORKDIR ${workdir}\n# copy model\nRUN git clone https://huggingface.co/Qwen/Qwen-VL-Chat\n# COPY --chown=20001:20001 Qwen-VL-Chat ./Qwen-VL-Chat\n# copy fonts\nADD --chown=20001:20001 https://github.com/StellarCN/scp_zh/raw/master/fonts/SimSun.ttf ./\n# COPY --chown=20001:20001 SimSun.ttf ./\n# copy main app\nCOPY --chown=20001:20001 web_demo_mm.py ./\n\nEXPOSE 8000\nCMD [\"python3\", \"web_demo_mm.py\", \"-c\", \"./Qwen-VL-Chat\", \"--server-name\", \"0.0.0.0\", \"--server-port\", \"8000\"]\n"
        },
        {
          "name": "Dockerfile.qwenint4openai",
          "type": "blob",
          "size": 2.1572265625,
          "content": "# python 3.8 and above\n# pytorch 1.12 and above, 2.0 and above are recommended\n# CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\n\n# based on modelscope docker image\n# registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\n# registry.cn-beijing.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\nFROM registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\n\nARG workdir=/var/app\nRUN mkdir -p ${workdir}\n\nRUN git lfs install\n\nWORKDIR ${workdir}\nCOPY requirements.txt requirements_web_demo.txt ./\n\n# Install Qwen dependencies\nRUN pip install -r requirements.txt\n\n# Install webUI dependencies\nWORKDIR ${workdir}\nRUN pip install -r requirements_web_demo.txt\n\n# Offline mode, check https://huggingface.co/docs/transformers/v4.15.0/installation#offline-mode\nENV HF_DATASETS_OFFLINE=1\nENV TRANSFORMERS_OFFLINE=1\n\n# set TZ, make logs dir, and expose port 8080\nENV TZ=Asia/Shanghai\nRUN mkdir -p ${workdir}/logs && chmod 777 ${workdir}/logs\nVOLUME /var/app/logs\n\n# create user 20001\nRUN useradd -r -m appuser -u 20001 -g 0\n\nWORKDIR ${workdir}\n# copy model\nRUN git clone https://huggingface.co/Qwen/Qwen-VL-Chat-Int4\n# COPY --chown=20001:20001 Qwen-VL-Chat-Int4 ./Qwen-VL-Chat-Int4\n\n# Install AutoGPTQ\nRUN pip install optimum\n# RUN git clone https://github.com/JustinLin610/AutoGPTQ.git && \\\n#     cd AutoGPTQ && \\\n#     pip install -v .\nRUN pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/\n\n# Install OpenAI API dependencies\nWORKDIR ${workdir}\nCOPY requirements_openai_api.txt ./\nRUN pip install -r requirements_openai_api.txt\n# copy fonts\nADD --chown=20001:20001 https://github.com/StellarCN/scp_zh/raw/master/fonts/SimSun.ttf ./\n# COPY --chown=20001:20001 SimSun.ttf ./\n# copy main app\nCOPY --chown=20001:20001 openai_api.py ./\n\nEXPOSE 8080\n# CMD [\"python3\", \"openai_api.py\", \"-c\", \"./Qwen-VL-Chat\", \"--server-name\", \"0.0.0.0\", \"--server-port\", \"8080\"]\nCMD [\"python3\", \"openai_api.py\", \"-c\", \"./Qwen-VL-Chat-Int4\", \"--server-name\", \"0.0.0.0\", \"--server-port\", \"8080\"]\n"
        },
        {
          "name": "Dockerfile.qwenopenai",
          "type": "blob",
          "size": 1.7783203125,
          "content": "# python 3.8 and above\n# pytorch 1.12 and above, 2.0 and above are recommended\n# CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\n\n# based on modelscope docker image\n# registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\n# registry.cn-beijing.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\nFROM registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0\n\nARG workdir=/var/app\nRUN mkdir -p ${workdir}\n\nRUN git lfs install\n\nWORKDIR ${workdir}\nCOPY requirements.txt requirements_web_demo.txt ./\n\n# Install Qwen dependencies\nRUN pip install -r requirements.txt\n\n# Install webUI dependencies\nWORKDIR ${workdir}\nRUN pip install -r requirements_web_demo.txt\n\n# Offline mode, check https://huggingface.co/docs/transformers/v4.15.0/installation#offline-mode\nENV HF_DATASETS_OFFLINE=1\nENV TRANSFORMERS_OFFLINE=1\n\n# set TZ, make logs dir, and expose port 8080\nENV TZ=Asia/Shanghai\nRUN mkdir -p ${workdir}/logs && chmod 777 ${workdir}/logs\nVOLUME /var/app/logs\n\n# create user 20001\nRUN useradd -r -m appuser -u 20001 -g 0\n\nWORKDIR ${workdir}\n# copy model\nRUN git clone https://huggingface.co/Qwen/Qwen-VL-Chat\n# COPY --chown=20001:20001 Qwen-VL-Chat ./Qwen-VL-Chat\n\n# Install OpenAI API dependencies\nWORKDIR ${workdir}\nCOPY requirements_openai_api.txt ./\nRUN pip install -r requirements_openai_api.txt\n# copy fonts\nADD --chown=20001:20001 https://github.com/StellarCN/scp_zh/raw/master/fonts/SimSun.ttf ./\n# COPY --chown=20001:20001 SimSun.ttf ./\n# copy main app\nCOPY --chown=20001:20001 openai_api.py ./\n\nEXPOSE 8080\nCMD [\"python3\", \"openai_api.py\", \"-c\", \"./Qwen-VL-Chat\", \"--server-name\", \"0.0.0.0\", \"--server-port\", \"8080\"]\n"
        },
        {
          "name": "FAQ.md",
          "type": "blob",
          "size": 1.7666015625,
          "content": "# FAQ\n\n## Installation & Environment\n\n#### Which version of transformers should I use?\n\n4.31.0 is preferred.\n\n#### I downloaded the codes and checkpoints but I can't load the model locally. What should I do?\n\nPlease check if you have updated the code to the latest, and correctly downloaded all the sharded checkpoint files.\n\n#### `qwen.tiktoken` is not found. What is it?\n\nThis is the merge file of the tokenizer. You have to download it. Note that if you just git clone the repo without [git-lfs](https://git-lfs.com), you cannot download this file.\n\n#### transformers_stream_generator/tiktoken/accelerate not found\n\nRun the command `pip install -r requirements.txt`. You can find the file at [https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt](https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt).\n<br><br>\n\n\n\n## Demo & Inference\n\n#### Is there any demo?\n\nYes, see `web_demo_mm.py` for web demo. See README for more information.\n\n\n\n#### Can Qwen-VL support streaming?\n\nNo. We do not support streaming yet.\n\n#### It seems that the generation is not related to the instruction...\n\nPlease check if you are loading Qwen-VL-Chat instead of Qwen-VL. Qwen-VL is the base model without alignment, which behaves differently from the SFT/Chat model.\n\n#### Is quantization supported?\n\nNo. We would support quantization asap.\n\n#### Unsatisfactory performance in processing long sequences\n\nPlease ensure that NTK is applied. `use_dynamc_ntk` and `use_logn_attn` in `config.json` should be set to `true` (`true` by default).\n<br><br>\n\n\n## Tokenizer\n\n#### bos_id/eos_id/pad_id not found\n\nIn our training, we only use `<|endoftext|>` as the separator and padding token. You can set bos_id, eos_id, and pad_id to tokenizer.eod_id. Learn more about our tokenizer from our documents about the tokenizer.\n\n"
        },
        {
          "name": "FAQ_ja.md",
          "type": "blob",
          "size": 2.48046875,
          "content": "# FAQ\n\n## インストールと環境\n\n#### transformers のバージョンは？\n\n4.31.0 が望ましいです。\n\n#### コードとチェックポイントをダウンロードしましたが、モデルをローカルにロードできません。どうすればよいでしょうか？\n\nコードを最新のものに更新し、すべてのシャードされたチェックポイントファイルを正しくダウンロードしたかどうか確認してください。\n\n#### `qwen.tiktoken` が見つかりません。これは何ですか？\n\nこれは tokenizer のマージファイルです。ダウンロードする必要があります。[git-lfs](https://git-lfs.com) を使わずにリポジトリを git clone しただけでは、このファイルをダウンロードできないことに注意してください。\n\n#### transformers_stream_generator/tiktoken/accelerate が見つかりません。\n\nコマンド `pip install -r requirements.txt` を実行してください。このファイルは [https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt](https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt) にあります。\n<br><br>\n\n\n\n## デモと推論\n\n#### デモはありますか？\n\nウェブデモは `web_demo_mm.py` を参照してください。詳細は README を参照してください。\n\n\n\n#### Qwen-VLはストリーミングに対応していますか？\n\nいいえ、まだサポートしていません。\n\n#### 世代と命令は関係ないようですが...\n\nQwen-VL ではなく Qwen-VL-Chat を読み込んでいないか確認してください。Qwen-VL はアライメントなしのベースモデルで、SFT/Chat モデルとは動作が異なります。\n\n#### 量子化はサポートされていますか？\n\nいいえ。早急に量子化をサポートするつもりです。\n\n#### 長いシーケンスの処理で不満足なパフォーマンス\n\nNTK が適用されていることを確認してください。`config.json` の `use_dynamc_ntk` と `use_logn_attn` を `true` に設定する必要がある（デフォルトでは `true`）。\n<br><br>\n\n\n## Tokenizer\n\n#### bos_id/eos_id/pad_id が見つかりません。\n\n私たちのトレーニングでは、セパレータとパディングトークンとして `<|endoftext|>` のみを使用しています。bos_id、eos_id、pad_id は tokenizer.eod_id に設定できます。私たちの tokenizer について詳しくは、tokenizer についてのドキュメントをご覧ください。\n\n"
        },
        {
          "name": "FAQ_ko.md",
          "type": "blob",
          "size": 2.1826171875,
          "content": "# FAQ\n\n## 설치 및 환경\n\n#### 어떤 버전의 transformers를 사용해야 하나요?\n\n4.31.0 버전을 사용하는 것을 선호합니다.\n\n#### 코드와 체크포인트를 다운로드했는데 모델을 로컬에서 불러올 수 없어요. 어떻게 해야 하나요?\n\n코드를 최신 버전으로 업데이트했는지, 그리고 모든 샤드 체크포인트 파일을 올바르게 다운로드했는지 확인해 주세요.\n\n#### `qwen.tiktoken`을 찾을 수 없어요. 이게 무엇인가요?\n\n이것은 토크나이저의 병합 파일입니다. 이 파일을 다운로드해야 합니다. [git-lfs](https://git-lfs.com) 없이 단순히 깃 저장소를 복제했다면 이 파일을 다운로드할 수 없습니다.\n\n#### transformers_stream_generator/tiktoken/accelerate not found 오류\n\n`pip install -r requirements.txt` 명령을 실행하세요. 이 파일은 [https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt](https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt)에서 찾을 수 있습니다.\n<br><br>\n\n\n## Demo & Inference\n\n#### 데모가 있나요?\n\n네, 웹 데모는 `web_demo_mm.py`를 참고하세요. 더 많은 정보는 README 파일에서 확인할 수 있습니다.\n\n\n\n#### Qwen-VL은 스트리밍을 지원하나요?\n\n아니요. 아직 스트리밍을 지원하지 않습니다.\n\n#### 생성된 내용이 지시사항과 관련 없는 것 같습니다.\n\nQwen-VL 대신 Qwen-VL-Chat을 로드하고 있는지 확인해 주세요. Qwen-VL은 SFT/Chat 모델과 달리 정렬이 없는 기본 모델이므로 다르게 작동합니다.\n\n#### 양자화를 지원하나요?\n\n아니요. 가능한 빨리 양자화를 지원할 예정입니다.\n\n#### 긴 시퀀스 처리에서 만족스럽지 못한 성능\n\nNTK가 적용되었는지 확인해 주세요. `config.json`의 `use_dynamc_ntk`과 `use_logn_attn`은 `true`로 설정되어야 합니다(`true`가 기본값).\n<br><br>\n\n\n## Tokenizer\n\n#### bos_id/eos_id/pad_id not found 오류\n\n저희 훈련에서는 ``을 구분자 및 패딩 토큰으로만 사용합니다. bos_id, eos_id, pad_id를 tokenizer.eod_id로 설정할 수 있습니다. 토크나이저에 대한 문서에서 토크나이저에 대해 더 알아보세요."
        },
        {
          "name": "FAQ_zh.md",
          "type": "blob",
          "size": 2.15625,
          "content": "# FAQ\n\n## 安装&环境\n\n#### 我应该用哪个transformers版本？\n\n建议使用4.31.0。\n\n#### 我把模型和代码下到本地，按照教程无法使用，该怎么办？\n\n答：别着急，先检查你的代码是不是更新到最新版本，然后确认你是否完整地将模型checkpoint下到本地。\n\n#### `qwen.tiktoken`这个文件找不到，怎么办？\n\n这个是我们的tokenizer的merge文件，你必须下载它才能使用我们的tokenizer。注意，如果你使用git clone却没有使用git-lfs，这个文件不会被下载。如果你不了解git-lfs，可点击[官网](https://git-lfs.com/)了解。\n\n#### transformers_stream_generator/tiktoken/accelerate，这几个库提示找不到，怎么办？\n\n运行如下命令：`pip install -r requirements.txt`。相关依赖库在[https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt](https://github.com/QwenLM/Qwen-VL/blob/main/requirements.txt) 可以找到。\n<br><br>\n\n\n## Demo & 推理\n\n#### 是否提供Demo？\n\n`web_demo_mm.py`提供了Web UI。请查看README相关内容了解更多。\n\n#### Qwen-VL支持流式推理吗？\n\nQwen-VL当前不支持流式推理。\n\n#### 模型的输出看起来与输入无关/没有遵循指令/看起来呆呆的\n\n请检查是否加载的是Qwen-VL-Chat模型进行推理，Qwen-VL模型是未经align的预训练基模型，不期望具备响应用户指令的能力。我们在模型最新版本已经对`chat`接口内进行了检查，避免您误将预训练模型作为SFT/Chat模型使用。\n\n#### 是否有量化版本模型\n\n目前Qwen-VL不支持量化，后续我们将支持高效的量化推理实现。\n\n#### 处理长序列时效果有问题\n\n请确认是否开启ntk。若要启用这些技巧，请将`config.json`里的`use_dynamc_ntk`和`use_logn_attn`设置为`true`。最新代码默认为`true`。\n<br><br>\n\n\n## Tokenizer\n\n#### bos_id/eos_id/pad_id，这些token id不存在，为什么？\n\n在训练过程中，我们仅使用<|endoftext|>这一token作为sample/document之间的分隔符及padding位置占位符，你可以将bos_id, eos_id, pad_id均指向tokenizer.eod_id。请阅读我们关于tokenizer的文档，了解如何设置这些id。\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 6.7412109375,
          "content": "Tongyi Qianwen LICENSE AGREEMENT\n\nTongyi Qianwen Release Date: August 23, 2023\n\nBy clicking to agree or by using or distributing any portion or element of the Tongyi Qianwen Materials, you will be deemed to have recognized and accepted the content of this Agreement, which is effective immediately.\n\n1. Definitions\n    a. This Tongyi Qianwen LICENSE AGREEMENT (this \"Agreement\") shall mean the terms and conditions for use, reproduction, distribution and modification of the Materials as defined by this Agreement.\n    b. \"We\"(or \"Us\") shall mean Alibaba Cloud.\n    c. \"You\" (or \"Your\") shall mean a natural person or legal entity exercising the rights granted by this Agreement and/or using the Materials for any purpose and in any field of use.\n    d. \"Third Parties\" shall mean individuals or legal entities that are not under common control with Us or You.\n    e. \"Tongyi Qianwen\" shall mean the large language models (including Qwen-VL model and Qwen-VL-Chat model), and software and algorithms, consisting of trained model weights, parameters (including optimizer states), machine-learning model code, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Us.\n    f. \"Materials\" shall mean, collectively, Alibaba Cloud's proprietary Tongyi Qianwen and Documentation (and any portion thereof) made available under this Agreement.\n    g. \"Source\" form shall mean the preferred form for making modifications, including but not limited to model source code, documentation source, and configuration files.\n    h. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation,\n and conversions to other media types.\n\n2. Grant of Rights\nYou are granted a non-exclusive, worldwide, non-transferable and royalty-free limited license under Alibaba Cloud's intellectual property or other rights owned by Us embodied in the Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Materials.\n\n3. Redistribution\nYou may reproduce and distribute copies of the Materials or derivative works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n    a. You shall give any other recipients of the Materials or derivative works a copy of this Agreement;\n    b. You shall cause any modified files to carry prominent notices stating that You changed the files;\n    c. You shall retain in all copies of the Materials that You distribute the following attribution notices within a \"Notice\" text file distributed as a part of such copies: \"Tongyi Qianwen is licensed under the Tongyi Qianwen LICENSE AGREEMENT, Copyright (c) Alibaba Cloud. All Rights Reserved.\"; and\n    d. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such derivative works as a whole, provided Your use, reproduction, and distribution of the work otherwise complies with the terms and conditions of this Agreement.\n\n4. Restrictions\nIf you are commercially using the Materials, and your product or service has more than 100 million monthly active users, You shall request a license from Us. You cannot exercise your rights under this Agreement without our express authorization.\n\n5. Rules of use\n    a. The Materials may be subject to export controls or restrictions in China, the United States or other countries or regions. You shall comply with applicable laws and regulations in your use of the Materials.\n    b. You can not use the Materials or any output therefrom to improve any other large language model (excluding Tongyi Qianwen or derivative works thereof).\n\n6. Intellectual Property\n    a. We retain ownership of all intellectual property rights in and to the Materials and derivatives made by or for Us. Conditioned upon compliance with the terms and conditions of this Agreement, with respect to any derivative works and modifications of the Materials that are made by you, you are and will be the owner of such derivative works and modifications.\n    b. No trademark license is granted to use the trade names, trademarks, service marks, or product names of Us, except as required to fulfill notice requirements under this Agreement or as required for reasonable and customary use in describing and redistributing the Materials.\n    c. If you commence a lawsuit or other proceedings (including a cross-claim or counterclaim in a lawsuit) against Us or any entity alleging that the Materials or any output therefrom, or any part of the foregoing, infringe any intellectual property or other right owned or licensable by you, then all licences granted to you under this Agreement shall terminate as of the date such lawsuit or other proceeding is commenced or brought.\n\n7. Disclaimer of Warranty and Limitation of Liability\n\n    a. We are not obligated to support, update, provide training for, or develop any further version of the Tongyi Qianwen Materials or to grant any license thereto.\n    b. THE MATERIALS ARE PROVIDED \"AS IS\" WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND INCLUDING WARRANTIES OF MERCHANTABILITY, NONINFRINGEMENT, OR FITNESS FOR A PARTICULAR PURPOSE. WE MAKE NO WARRANTY AND ASSUME NO RESPONSIBILITY FOR THE SAFETY OR STABILITY OF THE MATERIALS AND ANY OUTPUT THEREFROM.\n    c. IN NO EVENT SHALL WE BE LIABLE TO YOU FOR ANY DAMAGES, INCLUDING, BUT NOT LIMITED TO ANY DIRECT, OR INDIRECT, SPECIAL OR CONSEQUENTIAL DAMAGES ARISING FROM YOUR USE OR INABILITY TO USE THE MATERIALS OR ANY OUTPUT OF IT, NO MATTER HOW IT’S CAUSED.\n    d. You will defend, indemnify and hold harmless Us from and against any claim by any third party arising out of or related to your use or distribution of the Materials.\n\n8. Survival and Termination.\n    a. The term of this Agreement shall commence upon your acceptance of this Agreement or access to the Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein.\n    b. We may terminate this Agreement if you breach any of the terms or conditions of this Agreement. Upon termination of this Agreement, you must delete and cease use of the Materials. Sections 7 and 9 shall survive the termination of this Agreement.\n\n9. Governing Law and Jurisdiction.\n    a. This Agreement and any dispute arising out of or relating to it will be governed by the laws of China, without regard to conflict of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement.\n    b. The People's Courts in Hangzhou City shall have exclusive jurisdiction over any dispute arising out of this Agreement."
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 2.6396484375,
          "content": "------------- LICENSE FOR NVIDIA Megatron-LM code  --------------\n\nCopyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n  * Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n  * Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in the\n    documentation and/or other materials provided with the distribution.\n  * Neither the name of NVIDIA CORPORATION nor the names of its\n    contributors may be used to endorse or promote products derived\n    from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\nEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\nOF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n------------- LICENSE FOR OpenAI tiktoken code  --------------\n\nMIT License\n\nCopyright (c) 2022 OpenAI, Shantanu Jain\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 44.0341796875,
          "content": "<p align=\"left\">\n        <a href=\"README_CN.md\">中文</a>&nbsp ｜ &nbspEnglish&nbsp&nbsp ｜ &nbsp<a href=\"README_JA.md\">日本語</a>&nbsp｜ &nbsp<a href=\"README_KO.md\">한국어</a>&nbsp\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"assets/logo.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n  Qwen-VL \n  <a href=\"https://huggingface.co/Qwen/Qwen-VL\">🤗</a>\n  <a href=\"https://modelscope.cn/models/qwen/Qwen-VL/summary\">🤖</a>&nbsp ｜ \n  Qwen-VL-Chat \n  <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat\">🤗</a>\n  <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary\">🤖</a>&nbsp \n  (Int4: \n  <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat-Int4\">🤗</a> \n  <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat-Int4/summary\">🤖</a>&nbsp) ｜\n  Qwen-VL-Plus \n  <a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Plus\">🤗</a> \n  <a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary\">🤖</a>&nbsp ｜ \n  Qwen-VL-Max \n  <a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Max\">🤗</a>\n  <a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Max/summary\">🤖</a>&nbsp\n<br>\n  <a href=\"https://tongyi.aliyun.com/qianwen\">Web</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"http://ofasys-wlcb.oss-accelerate-overseas.aliyuncs.com/QwenVL/blog/app_qrcode.jpg\">APP</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start\">API</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://arxiv.org/abs/2308.12966\">Paper</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"TUTORIAL.md\">Tutorial</a>\n</p>\n<br><br>\n\n---\n## Qwen-VL-Plus & Qwen-VL-Max\n\nQwen-Vl-Plus and Qwen-VL-Max are the upgraded and latest versions of the Qwen-VL model family, currently supporting access for free through <a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Max\">🤗</a>, <a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Max/summary\">🤖</a>, [Web pages](https://qianwen.aliyun.com), [APP](http://ofasys-wlcb.oss-accelerate-overseas.aliyuncs.com/QwenVL/blog/app_qrcode.jpg) and [APIs](https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start/).\n\n| Model name | Model description |\n| --- | --- |\n| Qwen-VL-Plus | Qwen's **Enhanced Large Visual Language Model**. Significantly upgraded for detailed recognition capabilities and text recognition abilities, supporting ultra-high pixel resolutions up to millions of pixels and extreme aspect ratios for image input. It delivers **significant** performance across a broad range of visual tasks. |\n| Qwen-VL-Max | Qwen's **Most Capable Large Visual Language Model**. Compared to the enhanced version, further improvements have been made to visual reasoning and instruction-following capabilities, offering a higher level of visual perception and cognitive understanding. It delivers **optimal** performance on an even broader range of complex tasks. |\n\nThe key technical advancements in these versions include:\n- Substantially boost in image-related **reasoning capabilities**;\n- Considerable enhancement in recognizing, extracting, and analyzing **details of images**, especially for text-oriented tasks;\n- Support for **high-definition images** with resolutions above one million pixels and extreme aspect ratios;\n\nThese two models not only significantly surpass all previous best results from open-source LVLM models, but also perform on par with Gemini Ultra and GPT-4V in multiple text-image multimodal tasks.\n\nNotably, Qwen-VL-Max outperforms both GPT-4V from OpenAI and Gemini from Google in tasks on Chinese question answering and Chinese text comprehension. This breakthrough underscores the model’s advanced capabilities and its potential to set new standards in the field of multimodal AI research and application.\n\n<table>\n<thead>\n  <tr>\n    <th>Model</th>\n    <th>DocVQA</th>\n    <th>ChartQA</th>\n    <th>AI2D</th>\n    <th>TextVQA</th>\n    <th>MMMU</th>\n    <th>MathVista</th>\n    <th>MM-Bench-CN</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td>Other Best<br>Open-source LVLM</td>\n    <td>81.6%<br><sup><sup>(CogAgent)</sup></sup></td>\n    <td>68.4%<br><sup><sup>(CogAgent)</sup></sup></td>\n    <td>73.7%<br><sup><sup>(Fuyu-Medium)</sup></sup></td>\n    <td>76.1%<br><sup><sup>(CogAgent)</sup></sup></td>\n    <td>45.9%<br><sup><sup>(Yi-VL-34B)</sup></sup></td>\n    <td>36.7%<br><sup><sup>(SPHINX-V2)</sup></sup></td>\n    <td>72.4%<br><sup><sup>(InternLM-XComposer-VL)</sup></sup></td>\n  </tr>\n  <tr>\n    <td>Gemini Pro</td>\n    <td>88.1%</td>\n    <td>74.1%</td>\n    <td>73.9%</td>\n    <td>74.6%</td>\n    <td>47.9%</td>\n    <td>45.2%</td>\n    <td>74.3%</td>\n  </tr>\n  <tr>\n    <td>Gemini Ultra</td>\n    <td>90.9%</td>\n    <td>80.8% <sup>1</sup></td>\n    <td>79.5% <sup>1</sup></td>\n    <td>82.3% <sup>1</sup></td>\n    <td>59.4% <sup>1</sup></td>\n    <td>53.0% <sup>1</sup></td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>GPT-4V</td>\n    <td>88.4%</td>\n    <td>78.5%</td>\n    <td>78.2%</td>\n    <td>78.0%</td>\n    <td>56.8%</td>\n    <td>49.9%</td>\n    <td>73.9%</td>\n  </tr>\n  <tr>\n    <td><b>Qwen-VL-Plus</b></td>\n    <td>91.4%</td>\n    <td>78.1%</td>\n    <td>75.9%</td>\n    <td>78.9%</td>\n    <td>45.2%</td>\n    <td>43.3%</td>\n    <td>68.0%</td>\n  </tr>\n  <tr>\n    <td><b>Qwen-VL-Max</b></td>\n    <td>93.1% <sup>1</sup></td>\n    <td>79.8% <sup>2</sup></td>\n    <td>79.3% <sup>2</sup></td>\n    <td>79.5% <sup>2</sup></td>\n    <td>51.4% <sup>3</sup></td>\n    <td>51.0% <sup>2</sup></td>\n    <td>75.1% <sup>1</sup></td>\n  </tr>\n</tbody>\n</table>\n\nAll numbers are obtained without any use of external OCR tools ('pixel only').\n\n---\n\n## News and Updates\n* ```2024.01.18``` 💥💥💥 We introduce Qwen-VL-Max, our most capable model that significantly surpasses all previous open-source LVLM models, and it performs on par with Gemini Ultra and GPT-4V in multiple text-image multimodal tasks. You can enjoy the new model by directly visiting our [web pages](https://qianwen.aliyun.com), <a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Max\">🤗</a> and <a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Max/summary\">🤖</a>.\n* ```2023.11.28``` 🏆🏆🏆 Qwen-VL-Plus achieved the best performance in [DOCVQA](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1) by using a single model, surpassing GPT4V and PALI-X, without using model ensemble or OCR-pipeline. Meanwhile, it is also a general model that can help you analyze and understand various tasks by directly inputting images.\n* ```2023.9.25``` 🚀🚀🚀 We update Qwen-VL-Chat with more robust Chinese instruction-following ability, improved understanding of web pages and table images, and better dialogue performance (Touchstone: CN: 401.2->481.7, EN: 645.2->711.6).\n* ```2023.9.12``` 😃😃😃 We now support finetuning on the Qwen-VL models, including full-parameter finetuning, LoRA and Q-LoRA.\n* ```2023.9.8``` 👍👍👍 Thanks to [camenduru](https://github.com/camenduru) for contributing the wonderful [Colab](https://github.com/camenduru/Qwen-VL-Chat-colab). Everyone can use it as a local or online Qwen-VL-Chat-Int4 Demo tutorial on one 12G GPU.\n* ```2023.9.5``` 👏👏👏 Qwen-VL-Chat achieves SOTAs on [MME Benchmark](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation), a comprehensive evaluation benchmark for multimodal large language models. It measures both perception and cognition abilities on a total of 14 subtasks.\n* ```2023.9.4``` ⭐⭐⭐ Qwen-VL series achieve SOTAs on [Seed-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard), a multimodal benchmark of 19K multiple-choice questions with accurate human annotations for evaluating Multimodal LLMs including both image and video understanding.\n* ```2023.9.1``` 🔥🔥🔥 We release the [TouchStone](https://github.com/OFA-Sys/TouchStone) Evaluation, which is a comprehensive assessment of multimodal language models, encompassing not only basic recognition and comprehension but also extending to literary creation. By using strong LLMs as judges and converting multimodal information into text.\n* ```2023.8.31``` 🌟🌟🌟 We release the Int4 quantized model for Qwen-VL-Chat, **Qwen-VL-Chat-Int4**, which requires low memory costs but achieves improved inference speed. Besides, there is no significant performance degradation on the benchmark evaluation.\n* ```2023.8.22``` 🎉🎉🎉 We release both **Qwen-VL** and **Qwen-VL-Chat** on ModelScope and Hugging Face. We also provide a [paper](https://arxiv.org/abs/2308.12966) for more details about the model, including training details and model performance.\n\n---\n## Qwen-VL\n\n**Qwen-VL** (Qwen Large Vision Language Model) is the multimodal version of the large model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-VL accepts image, text, and bounding box as inputs, outputs text, and bounding box. The features of Qwen-VL include:\n\n- **Strong performance**: It significantly surpasses existing open-sourced Large Vision Language Models (LVLM) under a similar model scale on multiple English evaluation benchmarks (including Zero-shot Captioning, VQA, DocVQA, and Grounding).\n- **Multi-lingual LVLM supporting text recognition**: Qwen-VL naturally supports English, Chinese, and multi-lingual conversation, and it promotes end-to-end recognition of Chinese and English bi-lingual text in images.\n- **Multi-image interleaved conversations**: This feature allows for the input and comparison of multiple images, as well as the ability to specify questions related to the images and engage in multi-image storytelling.\n- **First generalist model supporting grounding in Chinese**: Detecting bounding boxes through open-domain language expression in both Chinese and English.\n- **Fine-grained recognition and understanding**: Compared to the 224\\*224 resolution currently used by other open-sourced LVLM, the 448\\*448 resolution promotes fine-grained text recognition, document QA, and bounding box annotation.\n\n<br>\n<p align=\"center\">\n    <img src=\"assets/demo_vl.gif\" width=\"400\"/>\n<p>\n<br>\n\nWe release two models of the Qwen-VL series:\n\n- Qwen-VL: The pre-trained LVLM model uses Qwen-7B as the initialization of the LLM, and [Openclip ViT-bigG](https://github.com/mlfoundations/open_clip) as the initialization of the visual encoder. And connects them with a randomly initialized cross-attention layer.\n- Qwen-VL-Chat: A multimodal LLM-based AI assistant, which is trained with alignment techniques. Qwen-VL-Chat supports more flexible interaction, such as multiple image inputs, multi-round question answering, and creative capabilities.\n  <br>\n\n## Evaluation\n\nWe evaluated the model's abilities from three perspectives:\n\n1. **Standard Benchmarks**: We evaluate the model's basic task capabilities on four major categories of multimodal tasks:\n   \n   - Zero-shot Captioning: Evaluate model's zero-shot image captioning ability on unseen datasets;\n   - General VQA: Evaluate the general question-answering ability of pictures, such as the judgment, color, number, category, etc;\n   - Text-based VQA: Evaluate the model's ability to recognize text in pictures, such as document QA, chart QA, etc;\n   - Referring Expression Comprehension: Evaluate the ability to localize a target object in an image described by a referring expression.\n2. **TouchStone**: To evaluate the overall text-image dialogue capability and alignment level with humans, we have constructed a benchmark called [TouchStone](https://github.com/OFA-Sys/TouchStone), which is based on scoring with GPT4 to evaluate the LVLM model.\n   \n   - The TouchStone benchmark covers a total of 300+ images, 800+ questions, and 27 categories. Such as attribute-based Q&A, celebrity recognition, writing poetry, summarizing multiple images, product comparison, math problem solving, etc;\n   - In order to break the current limitation of GPT4 in terms of direct image input, TouchStone provides fine-grained image annotations by human labeling. These detailed annotations, along with the questions and the model's output, are then presented to GPT4 for scoring.\n   - The benchmark includes both English and Chinese versions.\n  \n3. **Other Multimodal Benchmarks**: We also evaluated our model's capabilities in other multimodal benchmarks:\n\n   - [MME Benchmark](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation), a comprehensive evaluation benchmark for multimodal large language models. Qwen-VL-Chat achieves SOTAs on both perception and cognition tracks.\n   - [Seed-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard), a multimodal benchmark of 19K multiple-choice questions with accurate human annotations for evaluating Multimodal LLMs. Qwen series achieves SOTAs on this benchmark.\n\nThe results of the evaluation are as follows:\n\nQwen-VL outperforms current SOTA generalist models on multiple VL tasks and has a more comprehensive coverage in terms of capability range.\n\n<p align=\"center\">\n    <img src=\"assets/radar.png\" width=\"600\"/>\n<p>\n\n### Zero-shot Captioning & General VQA\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"2\">Zero-shot Captioning</th>\n    <th colspan=\"5\">General VQA</th>\n  </tr>\n  <tr>\n    <th>NoCaps</th>\n    <th>Flickr30K</th>\n    <th>VQAv2<sup>dev</sup></th>\n    <th>OK-VQA</th>\n    <th>GQA</th>\n    <th>SciQA-Img<br>(0-shot)</th>\n    <th>VizWiz<br>(0-shot)</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"10\">Generalist<br>Models</td>\n    <td>Flamingo-9B</td>\n    <td>-</td>\n    <td>61.5</td>\n    <td>51.8</td>\n    <td>44.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>28.8</td>\n  </tr>\n  <tr>\n    <td>Flamingo-80B</td>\n    <td>-</td>\n    <td>67.2</td>\n    <td>56.3</td>\n    <td>50.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>31.6</td>\n  </tr>\n  <tr>\n    <td>Unified-IO-XL</td>\n    <td>100.0</td>\n    <td>-</td>\n    <td>77.9</td>\n    <td>54.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Kosmos-1</td>\n    <td>-</td>\n    <td>67.1</td>\n    <td>51.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>29.2</td>\n  </tr>\n  <tr>\n    <td>Kosmos-2</td>\n    <td>-</td>\n    <td>80.5</td>\n    <td>51.1</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>103.9</td>\n    <td>71.6</td>\n    <td>65.0</td>\n    <td>45.9</td>\n    <td>32.3</td>\n    <td>61.0</td>\n    <td>19.6</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td><strong>121.9</strong></td>\n    <td>82.8</td>\n    <td>-</td>\n    <td>-</td>\n    <td>49.5</td>\n    <td>63.1</td>\n    <td>33.4</td>\n  </tr>\n  <tr>\n    <td>Shikra (Vicuna-13B)</td>\n    <td>-</td>\n    <td>73.9</td>\n    <td>77.36</td>\n    <td>47.16</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td><strong>Qwen-VL (Qwen-7B)</strong></td>\n    <td>121.4</td>\n    <td><b>85.8</b></td>\n    <td><b>78.8</b></td>\n    <td><b>58.6</b></td>\n    <td><b>59.3</b></td>\n    <td>67.1</td>\n    <td>35.2</td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>63.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>39.1</td>\n  </tr> -->\n  <tr>\n    <td>Qwen-VL-Chat</td>\n    <td>120.2</td>\n    <td>81.0</td>\n    <td>78.2</td>\n    <td>56.6</td>\n    <td>57.5</td>\n    <td><b>68.2</b></td>\n    <td><b>38.9</b></td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL-Chat (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>60.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>44.45</td>\n  </tr> -->\n  <tr>\n    <td>Previous SOTA<br>(Per Task Fine-tuning)</td>\n    <td>-</td>\n    <td>127.0<br>(PALI-17B)</td>\n    <td>84.5<br>(InstructBLIP<br>-FlanT5-XL)</td>\n    <td>86.1<br>(PALI-X<br>-55B)</td>\n    <td>66.1<br>(PALI-X<br>-55B)</td>\n    <td>72.1<br>(CFR)</td>\n    <td>92.53<br>(LLaVa+<br>GPT-4)</td>\n    <td>70.9<br>(PALI-X<br>-55B)</td>\n  </tr>\n</tbody>\n</table>\n\n- For zero-shot image captioning, Qwen-VL achieves the **SOTA** on Flickr30K and competitive results on Nocaps with InstructBlip.\n- For general VQA, Qwen-VL achieves the **SOTA** under the same generalist LVLM scale settings.\n\n### Text-oriented VQA (Focused on text understanding capabilities in images)\n\n<table>\n<thead>\n  <tr>\n    <th>Model type</th>\n    <th>Model</th>\n    <th>TextVQA</th>\n    <th>DocVQA</th>\n    <th>ChartQA</th>\n    <th>AI2D</th>\n    <th>OCR-VQA</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"5\">Generalist Models</td>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>42.4</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td>50.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>mPLUG-DocOwl (LLaMA-7B)</td>\n    <td>52.6</td>\n    <td>62.2</td>\n    <td>57.4</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Pix2Struct-Large (1.3B)</td>\n    <td>-</td>\n    <td><b>76.6</b></td>\n    <td>58.6</td>\n    <td>42.1</td>\n    <td>71.3</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL (Qwen-7B)</td>\n    <td><b>63.8</b></td>\n    <td>65.1</td>\n    <td><b>65.7</b></td>\n    <td><b>62.3</b></td>\n    <td><b>75.7</b></td>\n  </tr>\n  <tr>\n    <td>Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>PALI-X-55B (Single-task FT)<br>(Without OCR Pipeline)</td>\n    <td>71.44</td>\n    <td>80.0</td>\n    <td>70.0</td>\n    <td>81.2</td>\n    <td>75.0</td>\n  </tr>\n</tbody>\n</table>\n\n- In text-related recognition/QA evaluation, Qwen-VL achieves the SOTA under the generalist LVLM scale settings.\n- Resolution is important for several above evaluations. While most open-sourced LVLM models with 224 resolution are incapable of these evaluations or can only solve these by cutting images, Qwen-VL scales the resolution to 448 so that it can be evaluated end-to-end. Qwen-VL even outperforms Pix2Struct-Large models of 1024 resolution on some tasks.\n\n### Referring Expression Comprehension\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"3\">RefCOCO</th>\n    <th colspan=\"3\">RefCOCO+</th>\n    <th colspan=\"2\">RefCOCOg</th>\n    <th>GRIT</th>\n  </tr>\n  <tr>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val-u</th>\n    <th>test-u</th>\n    <th>refexp</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"8\">Generalist Models</td>\n    <td>GPV-2</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>51.50</td>\n  </tr>\n  <tr>\n    <td>OFA-L*</td>\n    <td>79.96</td>\n    <td>83.67</td>\n    <td>76.39</td>\n    <td>68.29</td>\n    <td>76.00</td>\n    <td>61.75</td>\n    <td>67.57</td>\n    <td>67.58</td>\n    <td>61.70</td>\n  </tr>\n  <tr>\n    <td>Unified-IO</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td><b>78.61</b></td>\n  </tr>\n  <tr>\n    <td>VisionLLM-H</td>\n    <td></td>\n    <td>86.70</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Shikra-7B</td>\n    <td>87.01</td>\n    <td>90.61</td>\n    <td>80.24 </td>\n    <td>81.60</td>\n    <td>87.36</td>\n    <td>72.12</td>\n    <td>82.27</td>\n    <td>82.19</td>\n    <td>69.34</td>\n  </tr>\n  <tr>\n    <td>Shikra-13B</td>\n    <td>87.83 </td>\n    <td>91.11</td>\n    <td>81.81</td>\n    <td>82.89</td>\n    <td>87.79</td>\n    <td>74.41</td>\n    <td>82.64</td>\n    <td>83.16</td>\n    <td>69.03</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B</td>\n    <td><b>89.36</b></td>\n    <td>92.26</td>\n    <td><b>85.34</b></td>\n    <td><b>83.12</b></td>\n    <td>88.25</td>\n    <td><b>77.21</b></td>\n    <td>85.58</td>\n    <td>85.48</td>\n    <td>78.22</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B-Chat</td>\n    <td>88.55</td>\n    <td><b>92.27</b></td>\n    <td>84.51</td>\n    <td>82.82</td>\n    <td><b>88.59</b></td>\n    <td>76.79</td>\n    <td><b>85.96</b></td>\n    <td><b>86.32</b></td>\n    <td>-</td>\n  <tr>\n    <td rowspan=\"3\">Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>G-DINO-L</td>\n    <td>90.56</td>\n    <td>93.19</td>\n    <td>88.24</td>\n    <td>82.75</td>\n    <td>88.95</td>\n    <td>75.92</td>\n    <td>86.13</td>\n    <td>87.02</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>UNINEXT-H</td>\n    <td>92.64 </td>\n    <td>94.33</td>\n    <td>91.46</td>\n    <td>85.24</td>\n    <td>89.63</td>\n    <td>79.79</td>\n    <td>88.73</td>\n    <td>89.37</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>ONE-PEACE</td>\n    <td>92.58 </td>\n    <td>94.18</td>\n    <td>89.26</td>\n    <td>88.77</td>\n    <td>92.21</td>\n    <td>83.23</td>\n    <td>89.22</td>\n    <td>89.27</td>\n    <td>-</td>\n  </tr>\n</tbody>\n</table>\n\n- Qwen-VL achieves the **SOTA** in all above referring expression comprehension benchmarks.\n- Qwen-VL has not been trained on any Chinese grounding data, but it can still generalize to the Chinese Grounding tasks in a zero-shot way by training Chinese Caption data and English Grounding data.\n\nWe provide all of the above evaluation scripts for reproducing our experimental results. Please read [eval_mm/EVALUATION.md](eval_mm/EVALUATION.md) for more information.\n\n### Chat evaluation\n\nTouchStone is a benchmark based on scoring with GPT4 to evaluate the abilities of the LVLM model on text-image dialogue and alignment levels with humans. It covers a total of 300+ images, 800+ questions, and 27 categories, such as attribute-based Q&A, celebrity recognition, writing poetry, summarizing multiple images, product comparison, math problem solving, etc. Please read [touchstone/README.md](touchstone/README.md) for more information.\n\n#### English evaluation\n\n| Model            | Score |\n| ---------------- | ----- |\n| PandaGPT         | 488.5 |\n| MiniGPT4         | 531.7 |\n| InstructBLIP     | 552.4 |\n| LLaMA-AdapterV2  | 590.1 |\n| LLaVA            | 602.7 |\n| mPLUG-Owl        | 605.4 |\n| Qwen-VL-Chat     | 645.2 |\n| Qwen-VL-Chat-1.1 | 711.6 |\n\n#### Chinese evaluation\n\n| Model            | Score |\n| ---------------- | ----- |\n| VisualGLM        | 247.1 |\n| Qwen-VL-Chat     | 401.2 |\n| Qwen-VL-Chat-1.1 | 481.7 |\n\nQwen-VL-Chat has achieved the best results in both Chinese and English alignment evaluation.\n\n### Other Benchmarks\n\n#### MME Benchmark\n\n[MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) is a comprehensive evaluation benchmark for multimodal large language models. It measures both perception and cognition abilities on a total of 14 subtasks, including existence, count, position, color, poster, celebrity, scene, landmark, artwork, OCR, commonsense reasoning, numerical calculation, text translation, and code reasoning.\n\nQwen-VL-Chat achieves SOTAs on both perception and cognition evaluation. See more details on [HERE](eval_mm/mme/EVAL_MME.md).\n\n<p align=\"center\">\n    <img src=\"eval_mm/mme/perception.jpg\" width=\"600\"/>\n<p>\n<p align=\"center\">\n    <img src=\"eval_mm/mme/cognition.jpg\" width=\"600\"/>\n<p>\n\n#### SEED-Bench\n\n[SEED-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard) is a multimodal benchmark of 19K multiple-choice questions with accurate human annotations for evaluating Multimodal LLMs, covering 12 evaluation dimensions including both **image** and **video** understanding. See more details on [HERE](eval_mm/seed_bench/EVAL_SEED.md).\n\nQwen-VL and Qwen-VL-Chat achieve SOTAs on this benchmark.\n\n<p align=\"center\">\n    <img src=\"eval_mm/seed_bench/leaderboard.jpg\"/>\n<p>\n\n## Requirements\n\n* python 3.8 and above\n* pytorch 1.12 and above, 2.0 and above are recommended\n* CUDA 11.4 and above are recommended (this is for GPU users)\n  <br>\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen-VL and Qwen-VL-Chat with 🤖 ModelScope and 🤗 Transformers.\n\nBefore running the code, make sure you have setup the environment and installed the required packages. Make sure you meet the above requirements, and then install the dependent libraries.\n\n```bash\npip install -r requirements.txt\n```\n\nNow you can start with ModelScope or Transformers. More usage aboue vision encoder, please refer to the [tutorial](TUTORIAL.md).\n\n#### 🤗 Transformers\n\nTo use Qwen-VL-Chat for the inference, all you need to do is to input a few lines of codes as demonstrated below. However, **please make sure that you are using the latest code.**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use cuda device\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# 1st dialogue turn\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url\n    {'text': '这是什么?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n# 图中是一名女子在沙滩上和狗玩耍，旁边是一只拉布拉多犬，它们处于沙滩上。\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, '框出图中击掌的位置', history=history)\nprint(response)\n# <ref>击掌</ref><box>(536,509),(588,602)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('1.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n\n<details>\n  <summary>Running Qwen-VL</summary>\n\nRunning Qwen-VL pretrained base model is also simple.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use cuda device\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation (No need to do this if you are using transformers>4.32.0)\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url\n    {'text': 'Generate the caption in English with grounding:'},\n])\ninputs = tokenizer(query, return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nresponse = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)\nprint(response)\n# <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>Generate the caption in English with grounding:<ref> Woman</ref><box>(451,379),(731,806)</box> and<ref> her dog</ref><box>(219,424),(576,896)</box> playing on the beach<|endoftext|>\nimage = tokenizer.draw_bbox_on_latest_picture(response)\nif image:\n  image.save('2.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_spotting_caption.jpg\" width=\"500\"/>\n<p>\n\n</details>\n\n\nIn the event of a network issue while attempting to download model checkpoints and codes from HuggingFace, an alternative approach is to initially fetch the checkpoint from ModelScope and then load it from the local directory as outlined below:\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-VL')\nmodel_dir = snapshot_download('qwen/Qwen-VL-Chat')\n\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"cuda\",\n    trust_remote_code=True\n).eval()\n```\n\n#### 🤖 ModelScope\n\nModelScope is an opensource platform for Model-as-a-Service (MaaS), which provides flexible and cost-effective model service to AI developers. Similarly, you can run the models with ModelScope as shown below:\n\n```python\nfrom modelscope import (\n    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n)\nimport torch\nmodel_id = 'qwen/Qwen-VL-Chat'\nrevision = 'v1.0.0'\n\nmodel_dir = snapshot_download(model_id, revision=revision)\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nif not hasattr(tokenizer, 'model_dir'):\n    tokenizer.model_dir = model_dir\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation (No need to do this if you are using transformers>=4.32.0)\n# model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)\n\n# 1st dialogue turn\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>这是什么', history=None)\nprint(response)\n# 图中是一名年轻女子在沙滩上和她的狗玩耍，狗的品种是拉布拉多。她们坐在沙滩上，狗的前腿抬起来，与人互动。\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, '输出击掌的检测框', history=history)\nprint(response)\n# <ref>\"击掌\"</ref><box>(211,412),(577,891)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('output_chat.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n<br>\n\n## Quantization\n\n### Usage\n\nWe provide a new solution based on [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), and release an Int4 quantized model for Qwen-VL-Chat, Qwen-VL-Chat-Int4 [Click here](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4), which achieves nearly lossless model effects but improved performance on both memory costs and inference speed.\n\nHere we demonstrate how to use our provided quantized models for inference. Before you start, make sure you meet the requirements (e.g., torch 2.0 and above, transformers 4.32.0 and above, etc.) and install the required packages:\n\n```bash\npip install optimum\ngit clone https://github.com/JustinLin610/AutoGPTQ.git & cd AutoGPTQ\npip install -v .\n```\n\nIf you meet problems installing `auto-gptq`, we advise you to check out the official [repo](https://github.com/PanQiWei/AutoGPTQ) to find a wheel.\n\nThen you can load the quantized model easily and run inference as same as usual:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-VL-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>这是什么', history=None)\nprint(response)\n```\n\n### Performance\n\nWe illustrate the model performance of both BF16 and Int4 models on the benchmark **[TouchStone](https://github.com/OFA-Sys/TouchStone)**, and we find that the quantized model does not suffer from significant performance degradation. Results are shown below:\n\n| Quantization | ZH         | EN            |\n| ------------ | :--------: | :-----------: | \n| BF16         | 401.2      |    645.2      |\n| Int4         | 386.6      |    651.4      |\n\n### Inference Speed\n\nWe measured the average inference speed (tokens/s) of generating 1792 (2048-258) and 7934 (8192-258) tokens with the context of an image (which takes 258 tokens) under BF16 precision and Int4 quantization, respectively.\n\n| Quantization | Speed (2048 tokens) | Speed (8192 tokens) |\n| ------------ | :-----------------: | :-----------------: |\n| BF16         |        28.87        |        24.32        |\n| Int4         |        37.79        |        34.34        |\n\nThe profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.4.\n\n### GPU Memory Usage\n\nWe also profile the peak GPU memory usage for encoding 1792 (2048-258) tokens (including an image) as context (and generating single token) and generating 7934 (8192-258) tokens (with an image as context) under BF16 or Int4 quantization level, respectively. The results are shown below.\n\n| Quantization | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |\n| ------------ | :---------------------------------: | :-----------------------------------: |\n| BF16         |               22.60GB               |                28.01GB                |\n| Int4         |               11.82GB               |                17.23GB                |\n\nThe above speed and memory profiling are conducted using [this script](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile_mm.py).\n<br>\n\n## Finetuning\n\nNow we provide the official training script, `finetune.py`, for users to finetune the pretrained model for downstream applications in a simple fashion. Additionally, we provide shell scripts to launch finetuning with no worries. This script supports the training with DeepSpeed and FSDP. The shell scripts that we provide use DeepSpeed, and thus we advise you to install DeepSpeed before you start:\n\n```bash\npip install deepspeed\n```\n\n### Data preparation\nTo prepare your training data, you need to put all the samples into a list and save it to a json file. Each sample is a dictionary consisting of an id and a list for conversation. Below is a simple example list with 1 sample:\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"你好\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"我是Qwen-VL,一个支持视觉输入的大模型。\"\n      }\n    ]\n  },\n  {\n    \"id\": \"identity_1\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>\\n图中的狗是什么品种？\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"图中是一只拉布拉多犬。\"\n      },\n      {\n        \"from\": \"user\",\n        \"value\": \"框出图中的格子衬衫\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"<ref>格子衬衫</ref><box>(588,499),(725,789)</box>\"\n      }\n    ]\n  },\n  { \n    \"id\": \"identity_2\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>assets/mm_tutorial/Chongqing.jpeg</img>\\nPicture 2: <img>assets/mm_tutorial/Beijing.jpeg</img>\\n图中都是哪\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"第一张图片是重庆的城市天际线，第二张图片是北京的天际线。\"\n      }\n    ]\n  }\n]\n```\nFor the VL tasks, there are special tokens that are used, including `<img> </img> <ref> </ref> <box> </box>`.\n\nThe picture is represented as `Picture id: <img>img_path</img>\\n{your prompt}`, where `id` indicates the position of the image in the conversation, starting from 1. The \"img_path\" can be a local file path or a web link. \n\nThe coordinate box is expressed as `<box>(x1,y1),(x2,y2)</box>`·, where `(x1, y1)` and `(x2, y2)` are normalized values in the range `[0, 1000)`. Its corresponding text description can be identified by `<ref>text_caption</ref>`. \n\n\nAfter data preparation, you can use the provided shell scripts to run finetuning. Remember to specify the path to the data file, `$DATA`.\n\nThe finetuning scripts allow you to perform:\n- Full-parameter finetuning\n- LoRA\n- Q-LoRA\n\n### Full-parameter finetuning\nFull-parameter parameter finetuning requires updating all parameters of LLM in the whole training process. In our experiments, frozening the parameters of ViT during the fine-tuning phase achieves better performance. To launch your training, run the following script:\n\n```bash\nsh finetune/finetune_ds.sh\n```\n\nRemember to specify the correct model name or path, the data path, as well as the output directory in the shell scripts. If you want to make changes, just remove the argument `--deepspeed` or make changes in the DeepSpeed configuration json file based on your requirements. Additionally, this script supports mixed-precision training, and thus you can use `--bf16 True` or `--fp16 True`. Empirically we advise you to use bf16 to make your training consistent with our pretraining and alignment if your machine supports bf16, and thus we use it by default.\n\n### LoRA\nSimilarly, to run LoRA, use another script to run as shown below. Before you start, make sure that you have installed `peft`. Also, you need to specify your paths to your model, data, and output. We advise you to use absolute path for your pretrained model. This is because LoRA only saves the adapter and the absolute path in the adapter configuration json file is used for finding out the pretrained model to load.\n\n```bash\n# Single GPU training\nsh finetune/finetune_lora_single_gpu.sh\n# Distributed training\nsh finetune/finetune_lora_ds.sh\n```\n\nIn comparison with full-parameter finetuning, LoRA ([paper](https://arxiv.org/abs/2106.09685)) only updates the parameters of adapter layers but keeps the original large language model layers frozen. This allows much fewer memory costs and thus fewer computation costs. \n\nNote that if you use LoRA to finetune the base language model, e.g., Qwen-VL, instead of chat models, e.g., Qwen-VL-Chat, the script automatically switches the embedding and output layer as trainable parameters. This is because the base language model has no knowledge of special tokens brought by ChatML format. Thus these layers should be updated for the model to understand and predict the tokens. Or in another word, if your training brings in special tokens in LoRA, you should set the layers to trainable parameters by setting `modules_to_save` inside the code. Additionally, we find that there is a significant gap between the memory footprint of LoRA with and without these trainable parameters. Therefore, if you have trouble with memory, we advise you to LoRA finetune the chat models. Check the profile below for more information.\n\n### Q-LoRA\nHowever, if you still suffer from insufficient memory, you can consider Q-LoRA ([paper](https://arxiv.org/abs/2305.14314)), which uses the quantized large language model and other techniques such as paged attention to allow even fewer memory costs. To run Q-LoRA, directly run the following script:\n\n```bash\n# Single GPU training\nsh finetune/finetune_qlora_single_gpu.sh\n# Distributed training\nsh finetune/finetune_qlora_ds.sh\n```\n\nFor Q-LoRA, we advise you to load our provided quantized model, e.g., Qwen-VL-Chat-Int4. \nYou **SHOULD NOT** use the bf16 models. Different from full-parameter finetuning and LoRA, only fp16 is supported for Q-LoRA. Besides, for Q-LoRA, the troubles with the special tokens in LoRA still exist. However, as we only provide the Int4 models for chat models, which means the language model has learned the special tokens of ChatML format, you have no worry about the layers. Note that the layers of the Int4 model should not be trainable, and thus if you introduce special tokens in your training, Q-LoRA might not work.\n\n\n\nDifferent from full-parameter finetuning, the training of both LoRA and Q-LoRA only saves the adapter parameters. You can load the finetuned model for inference as shown below:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\nIf you want to merge the adapters and save the finetuned model as a standalone model (you can only do this with LoRA, and you CANNOT merge the parameters from Q-LoRA), you can run the following codes:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\nNote: For multi-GPU training, you need to specify the proper hyperparameters for distributed training based on your machine. Besides, we advise you to specify your maximum sequence length with the argument --model_max_length, based on your consideration of data, memory footprint, and training speed.\n\n\n### Profiling of Memory and Speed\nWe profile the GPU memory and training speed of both LoRA (Base) refers to training the embedding and output layer, while LoRA (Chat) has no trainable embedding and output layer) and Q-LoRA in the setup of single-GPU training. In this test, we experiment on a single A100-SXM4-80G GPU, and we use CUDA 11.8 and Pytorch 2.0. We uniformly use a batch size of 1 and gradient accumulation of 8. Each sample contains an image. We profile the memory (GB) and speed (s/iter) of inputs of different lengths, namely 384, 512, 1024, and 2048. The statistics are listed below:\n\n\n<table>\n    <tr>\n <th rowspan=\"2\">Method</th><th colspan=\"4\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">384</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th>\n    </tr>\n    <tr>\n      <td>LoRA (Base)</td><td align=\"center\">37.1G / 2.3s/it</td><td align=\"center\">37.3G / 2.4s/it</td><td align=\"center\">38.7G / 3.6s/it</td><td align=\"center\">38.7G / 6.1s/it</td>\n    </tr>\n    <tr>\n      <td>LoRA (Chat)</td><td align=\"center\">23.3G / 2.2s/it</td><td align=\"center\">23.6G / 2.3s/it</td><td align=\"center\">25.1G / 3.5s/it</td><td align=\"center\">27.3G / 5.9s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">17.0G / 4.2s/it</td><td align=\"center\">17.2G / 4.5s/it</td><td align=\"center\">18.2G / 5.5s/it</td><td align=\"center\">19.3G / 7.9s/it</td>\n    </tr>\n\n</table>\n\n<br>\n\n## Demo\n\n### Web UI\n\nWe provide code for users to build a web UI demo. Before you start, make sure you install the following packages:\n\n```\npip install -r requirements_web_demo.txt\n```\n\nThen run the command below and click on the generated link:\n\n```\npython web_demo_mm.py\n```\n\n<br>\n\n## FAQ\n\nIf you meet problems, please refer to [FAQ](FAQ.md) and the issues first to search a solution before you launch a new issue.\n<br>\n\n## License Agreement\n\nResearchers and developers are free to use the codes and model weights of both Qwen-VL and Qwen-VL-Chat. We also allow their commercial use. Check our license at [LICENSE](LICENSE) for more details.\n<br>\n\n## Citation\n\nIf you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)\n\n```BibTeX\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n\n<br>\n\n## Contact Us\n\nIf you are interested to leave a message to either our research team or product team, feel free to send an email to qianwen_opensource@alibabacloud.com.\n\n"
        },
        {
          "name": "README_CN.md",
          "type": "blob",
          "size": 41.7705078125,
          "content": "<p align=\"left\">\n        中文</a>&nbsp ｜ &nbsp<a href=\"README.md\">English</a>&nbsp&nbsp ｜ &nbsp<a href=\"README_JA.md\">日本語</a>&nbsp｜ &nbsp<a href=\"README_KO.md\">한국어</a>&nbsp\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"assets/logo.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n  Qwen-VL \n  <a href=\"https://huggingface.co/Qwen/Qwen-VL\">🤗</a>\n  <a href=\"https://modelscope.cn/models/qwen/Qwen-VL/summary\">🤖</a>&nbsp ｜ \n  Qwen-VL-Chat \n  <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat\">🤗</a>\n  <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary\">🤖</a>&nbsp \n  (Int4: \n  <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat-Int4\">🤗</a> \n  <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat-Int4/summary\">🤖</a>&nbsp) ｜\n  Qwen-VL-Plus \n  <a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Plus\">🤗</a> \n  <a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary\">🤖</a>&nbsp ｜ \n  Qwen-VL-Max \n  <a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Max\">🤗</a>\n  <a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Max/summary\">🤖</a>&nbsp\n<br>\n  <a href=\"https://tongyi.aliyun.com/qianwen\">Web</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"http://ofasys-wlcb.oss-accelerate-overseas.aliyuncs.com/QwenVL/blog/app_qrcode.jpg\">APP</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start\">API</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"https://arxiv.org/abs/2308.12966\">Paper</a>&nbsp&nbsp | &nbsp&nbsp\n  <a href=\"TUTORIAL.md\">Tutorial</a>\n</p>\n<br><br>\n\n---\n## Qwen-VL-Plus & Qwen-VL-Max\n\nQwen-VL 系列再次迎来重磅升级，我们推出 Qwen-VL-Plus 和 Qwen-VL-Max 两个升级版的模型。目前支持通过<a href=\"https://huggingface.co/spaces/Qwen/Qwen-VL-Max\">🤗</a>、<a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Max/summary\">🤖</a>、[网页端](https://qianwen.aliyun.com)、[APP](http://ofasys-wlcb.oss-accelerate-overseas.aliyuncs.com/QwenVL/blog/app_qrcode.jpg) 和 [API](https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start)免费访问。\n\n| 模型名 | 模型简介 |\n| --- | --- |\n| Qwen-VL-Plus | 通义千问大规模视觉语言模型增强版。大幅提升细节识别能力和文字识别能力，支持超百万像素分辨率和任意长宽比规格的图像。在广泛的视觉任务上提供**卓越**的性能。 |\n| Qwen-VL-Max | 通义千问超大规模视觉语言模型。相比增强版，再次提升视觉推理能力和指令遵循能力，提供更高的视觉感知和认知水平。在更多复杂任务上提供**最佳**的性能。 |\n\n这两个版本的主要技术升级在于：\n- 大幅提升图像相关的推理能力；\n- 大幅提升对图中细节和文字的识别、提取和分析能力；\n- 支持百万像素以上的高清分辨率图，支持各种长宽比的图像；\n\n这两个模型不仅大幅超越此前所有开源 LVLM 模型的最佳水平，并且在多项图文多模态标准测试中获得了堪比 Gemini Ultra 和 GPT4-v 的水准。\n甚至，Qwen-VL-Max 在中文问答、中文文字理解相关的任务上超越了 OpenAI的 GPT4-v 和 Google 的 Gemini-Pro。\n\n<table>\n<thead>\n  <tr>\n    <th>Model</th>\n    <th>DocVQA<br><sup>(文档理解)</sup></th>\n    <th>ChartQA<br><sup>(图表理解)</sup></th>\n    <th>AI2D<br><sup>(科学图例)</sup></th>\n    <th>TextVQA<br><sup>(文字阅读)</sup></th>\n    <th>MMMU<br><sup>(多学科问题)</sup></th>\n    <th>MathVista<br><sup>(数学推理)</sup></th>\n    <th>MM-Bench-CN<br><sup>(中文问答)</sup></th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td>Other Best<br>Open-source LVLM</td>\n    <td>81.6%<br><sup>(CogAgent)</sup></td>\n    <td>68.4%<br><sup>(CogAgent)</sup></td>\n    <td>73.7%<br><sup>(Fuyu-Medium)</sup></td>\n    <td>76.1%<br><sup>(CogAgent)</sup></td>\n    <td>45.9%<br><sup>(Yi-VL-34B)</sup></td>\n    <td>36.7%<br><sup>(SPHINX-V2)</sup></td>\n    <td>72.4%<br><sup>(InternLM-XComposer-VL)</sup></td>\n  </tr>\n  <tr>\n    <td>Gemini Pro</td>\n    <td>88.1%</td>\n    <td>74.1%</td>\n    <td>73.9%</td>\n    <td>74.6%</td>\n    <td>47.9%</td>\n    <td>45.2%</td>\n    <td>74.3%</td>\n  </tr>\n  <tr>\n    <td>Gemini Ultra</td>\n    <td>90.9%</td>\n    <td>80.8% <sup>1</sup></td>\n    <td>79.5% <sup>1</sup></td>\n    <td>82.3% <sup>1</sup></td>\n    <td>59.4% <sup>1</sup></td>\n    <td>53.0% <sup>1</sup></td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>GPT-4V</td>\n    <td>88.4%</td>\n    <td>78.5%</td>\n    <td>78.2%</td>\n    <td>78.0%</td>\n    <td>56.8%</td>\n    <td>49.9%</td>\n    <td>73.9%</td>\n  </tr>\n  <tr>\n    <td><b>Qwen-VL-Plus</b></td>\n    <td>91.4%</td>\n    <td>78.1%</td>\n    <td>75.9%</td>\n    <td>78.9%</td>\n    <td>44.0%</td>\n    <td>43.3%</td>\n    <td>68.0%</td>\n  </tr>\n  <tr>\n    <td><b>Qwen-VL-Max</b></td>\n    <td>92.5% <sup>1</sup></td>\n    <td>79.8% <sup>2</sup></td>\n    <td>79.3% <sup>2</sup></td>\n    <td>79.5% <sup>2</sup></td>\n    <td>51.4% <sup>3</sup></td>\n    <td>51.0% <sup>2</sup></td>\n    <td>75.1% <sup>1</sup></td>\n  </tr>\n</tbody>\n</table>\n\n所有评测都是在不使用任何外部OCR工具(“only pixel”)的情况下获得的。\n\n---\n\n## 新闻\n* 2024年01月18日 我们推出 Qwen-Vl-Max，大幅超越此前所有开源 LVLM 模型的最佳水平，并且在多项图文多模态标准测试中获得了堪比 Gemini Ultra 和 GPT4-v 的水准。直接访问[通义千问网页端或APP](https://qianwen.aliyun.com)就能体验新模型。\n* 2023年11月28日 Qwen-VL单模型在[DOCVQA](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1)达到了最强水平，超越了GPT4V,PALI-X，与此同时它还是一个通用模型，直接输入图片就能帮你分析理解各种任务。\n* 2023年9月12日 更新Qwen-VL-Chat模型，该模型有更鲁棒的中文指令跟随，更好的网页和表格图片理解和问答能力以及更好的对话表现(Touchstone: 中文: 401.2->481.7, 英文: 645.2->711.6)。\n* 2023年9月12日 支持Qwen-VL和Qwen-VL-Chat的微调，其中包括全参数微调、LoRA以及Q-LoRA\n* 2023年9月8日 感谢[camenduru](https://github.com/camenduru)贡献了[Colab](https://github.com/camenduru/Qwen-VL-Chat-colab)示例，每个人都可以以此为教程，在12G的GPU上做本地或在线的Demo。\n* 2023年9月5日 在社区多模态通用模型榜单 [MME Benchmark](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation) 上取得了感知和认知双赛道的当前最好结果。\n* 2023年9月4日 在社区多模态通用模型榜单 [SEED-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard) 上取得了图像理解和视频理解的当前最好结果。\n* 2023年9月1日 发布[TouchStone](https://github.com/OFA-Sys/TouchStone) 测评, 这是一个综合评估LVLM能力的测评,它不仅考察模型的视觉描述和推理能力，还包括根据视觉内容的文学创作能力。同时它是将多模态信息用文本表述并用LLMs进行评估的方法。\n* 2023年8月31日 发布Qwen-VL-Chat量化模型，**Qwen-VL-Chat-Int4**,该模型显存占用低，推理速度相比半精度模型显著提升，在基准评测上效果损失较小。\n* 2023年8月22日 在魔搭社区（ModelScope）和Hugging Face同步推出Qwen-VL和Qwen-VL-Chat模型。同时，我们提供一个[论文](https://arxiv.org/abs/2308.12966)介绍了相关的模型结构、训练细节和模型表现。\n\n---\n\n## Qwen-VL\n\n**Qwen-VL** 是阿里云研发的大规模视觉语言模型（Large Vision Language Model, LVLM）。Qwen-VL 可以以图像、文本、检测框作为输入，并以文本和检测框作为输出。Qwen-VL 系列模型的特点包括：\n\n- **强大的性能**：在四大类多模态任务的标准英文测评中（Zero-shot Captioning/VQA/DocVQA/Grounding）上，均取得同等通用模型大小下最好效果；\n- **多语言对话模型**：天然支持英文、中文等多语言对话，端到端支持图片里中英双语的长文本识别；\n- **多图交错对话**：支持多图输入和比较，指定图片问答，多图文学创作等；\n- **首个支持中文开放域定位的通用模型**：通过中文开放域语言表达进行检测框标注；\n- **细粒度识别和理解**：相比于目前其它开源LVLM使用的224分辨率，Qwen-VL是首个开源的448分辨率的LVLM模型。更高分辨率可以提升细粒度的文字识别、文档问答和检测框标注。\n\n<br>\n<p align=\"center\">\n    <img src=\"assets/demo_vl.gif\" width=\"400\"/>\n<p>\n<br>\n\n目前，我们提供了 Qwen-VL 系列的两个模型：\n\n- Qwen-VL: Qwen-VL 以 Qwen-7B 的预训练模型作为语言模型的初始化，并以 [Openclip ViT-bigG](https://github.com/mlfoundations/open_clip) 作为视觉编码器的初始化，中间加入单层随机初始化的 cross-attention，经过约1.5B的图文数据训练得到。最终图像输入分辨率为448。\n- Qwen-VL-Chat: 在 Qwen-VL 的基础上，我们使用对齐机制打造了基于大语言模型的视觉AI助手Qwen-VL-Chat，它支持更灵活的交互方式，包括多图、多轮问答、创作等能力。\n  <br>\n\n## 评测\n\n我们从三个角度评测了模型的能力：\n\n1. 在**英文标准 Benchmark** 上评测模型的基础任务能力。目前评测了四大类多模态任务：\n   \n   - Zero-shot Captioning: 评测模型在未见过数据集上的零样本图片描述能力；\n   - General VQA: 评测模型的通用问答能力，例如判断题、颜色、个数、类目等问答能力；\n   - Text-based VQA：评测模型对于图片中文字相关的识别/问答能力，例如文档问答、图表问答、文字问答等；\n   - Referring Expression Compression：评测模型给定物体描述画检测框的能力；\n2. **试金石 (TouchStone)**：为了评测模型整体的图文对话能力和人类对齐水平。我们为此构建了一个基于 GPT4 打分来评测 LVLM 模型的 Benchmark：TouchStone。在 TouchStone-v0.1 中：\n   \n   - 评测基准总计涵盖 300+张图片、800+道题目、27个类别。包括基础属性问答、人物地标问答、影视作品问答、视觉推理、反事实推理、诗歌创作、故事写作，商品比较、图片解题等**尽可能广泛的类别**。\n   - 为了弥补目前 GPT4 无法直接读取图片的缺陷，我们给所有的带评测图片提供了**人工标注的充分详细描述**，并且将图片的详细描述、问题和模型的输出结果一起交给 GPT4 打分。\n   - 评测同时包含英文版本和中文版本。\n\n3. **其它多模态通用模型榜单**：我们也在其它多模态通用模型榜单中评测了模型的能力：\n   \n   - MME Benchmark: 是一个多模态大型语言模型的综合评价基准。它在总共14个子任务上评测**感知和认知**能力，Qwen-VL-Chat在这两个总维度上都实现了当前最好结果。\n   - SEED-Bench: 是一个包含1.9万选择题的多模态基准测评，通过人工注释的结果评估多模态大模型，涵盖12个评估维度，包括**图像和视频理解**，Qwen-VL和Qwen-VL-chat在这个基准上实现了当前最好结果。\n\n评测结果如下：\n\nQwen-VL在多个VL任务上相比目前SOTA的Generalist Models都有明显优势，并且在能力范围也覆盖更加全面。\n\n<p align=\"center\">\n    <img src=\"assets/radar.png\" width=\"600\"/>\n<p>\n\n### 零样本图像描述生成（Zero-shot Image Caption） 及 通用视觉问答（General VQA）\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"2\">Zero-shot Captioning</th>\n    <th colspan=\"5\">General VQA</th>\n  </tr>\n  <tr>\n    <th>NoCaps</th>\n    <th>Flickr30K</th>\n    <th>VQAv2<sup>dev</sup></th>\n    <th>OK-VQA</th>\n    <th>GQA</th>\n    <th>SciQA-Img<br>(0-shot)</th>\n    <th>VizWiz<br>(0-shot)</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"10\">Generalist<br>Models</td>\n    <td>Flamingo-9B</td>\n    <td>-</td>\n    <td>61.5</td>\n    <td>51.8</td>\n    <td>44.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>28.8</td>\n  </tr>\n  <tr>\n    <td>Flamingo-80B</td>\n    <td>-</td>\n    <td>67.2</td>\n    <td>56.3</td>\n    <td>50.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>31.6</td>\n  </tr>\n  <tr>\n    <td>Unified-IO-XL</td>\n    <td>100.0</td>\n    <td>-</td>\n    <td>77.9</td>\n    <td>54.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Kosmos-1</td>\n    <td>-</td>\n    <td>67.1</td>\n    <td>51.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>29.2</td>\n  </tr>\n  <tr>\n    <td>Kosmos-2</td>\n    <td>-</td>\n    <td>66.7</td>\n    <td>45.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>103.9</td>\n    <td>71.6</td>\n    <td>65.0</td>\n    <td>45.9</td>\n    <td>32.3</td>\n    <td>61.0</td>\n    <td>19.6</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td><strong>121.9</strong></td>\n    <td>82.8</td>\n    <td>-</td>\n    <td>-</td>\n    <td>49.5</td>\n    <td>63.1</td>\n    <td>33.4</td>\n  </tr>\n  <tr>\n    <td>Shikra (Vicuna-13B)</td>\n    <td>-</td>\n    <td>73.9</td>\n    <td>77.36</td>\n    <td>47.16</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td><strong>Qwen-VL (Qwen-7B)</strong></td>\n    <td>121.4</td>\n    <td><b>85.8</b></td>\n    <td><b>78.8</b></td>\n    <td><b>58.6</b></td>\n    <td><b>59.3</b></td>\n    <td>67.1</td>\n    <td>35.2</td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>63.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>39.1</td>\n  </tr> -->\n  <tr>\n    <td>Qwen-VL-Chat</td>\n    <td>120.2</td>\n    <td>81.0</td>\n    <td>78.2</td>\n    <td>56.6</td>\n    <td>57.5</td>\n    <td><b>68.2</b></td>\n    <td><b>38.9</b></td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL-Chat (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>60.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>44.45</td>\n  </tr> -->\n  <tr>\n    <td>Previous SOTA<br>(Per Task Fine-tuning)</td>\n    <td>-</td>\n    <td>127.0<br>(PALI-17B)</td>\n    <td>84.5<br>(InstructBLIP<br>-FlanT5-XL)</td>\n    <td>86.1<br>(PALI-X<br>-55B)</td>\n    <td>66.1<br>(PALI-X<br>-55B)</td>\n    <td>72.1<br>(CFR)</td>\n    <td>92.53<br>(LLaVa+<br>GPT-4)</td>\n    <td>70.9<br>(PALI-X<br>-55B)</td>\n  </tr>\n</tbody>\n</table>\n\n- 在 Zero-shot Captioning 中，Qwen-VL 在 Flickr30K 数据集上取得了 **SOTA** 的结果，并在 Nocaps 数据集上取得了和 InstructBlip 可竞争的结果。\n- 在 General VQA 中，Qwen-VL 取得了 LVLM 模型同等量级和设定下 **SOTA** 的结果。\n\n### 文本导向的视觉问答（Text-oriented VQA）\n\n<table>\n<thead>\n  <tr>\n    <th>Model type</th>\n    <th>Model</th>\n    <th>TextVQA</th>\n    <th>DocVQA</th>\n    <th>ChartQA</th>\n    <th>AI2D</th>\n    <th>OCR-VQA</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"5\">Generalist Models</td>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>42.4</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td>50.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>mPLUG-DocOwl (LLaMA-7B)</td>\n    <td>52.6</td>\n    <td>62.2</td>\n    <td>57.4</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Pix2Struct-Large (1.3B)</td>\n    <td>-</td>\n    <td><b>76.6</b></td>\n    <td>58.6</td>\n    <td>42.1</td>\n    <td>71.3</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL (Qwen-7B)</td>\n    <td><b>63.8</b></td>\n    <td>65.1</td>\n    <td><b>65.7</b></td>\n    <td><b>62.3</b></td>\n    <td><b>75.7</b></td>\n  </tr>\n  <tr>\n    <td>Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>PALI-X-55B (Single-task FT)<br>(Without OCR Pipeline)</td>\n    <td>71.44</td>\n    <td>80.0</td>\n    <td>70.0</td>\n    <td>81.2</td>\n    <td>75.0</td>\n  </tr>\n</tbody>\n</table>\n\n- 在文字相关的识别/问答评测上，取得了当前规模下通用 LVLM 达到的最好结果。\n- 分辨率对上述某几个评测非常重要，大部分 224 分辨率的开源 LVLM 模型无法完成以上评测，或只能通过切图的方式解决。Qwen-VL 将分辨率提升到 448，可以直接以端到端的方式进行以上评测。Qwen-VL 在很多任务上甚至超过了 1024 分辨率的 Pix2Struct-Large 模型。\n\n### 细粒度视觉定位（Referring Expression Comprehension）\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"3\">RefCOCO</th>\n    <th colspan=\"3\">RefCOCO+</th>\n    <th colspan=\"2\">RefCOCOg</th>\n    <th>GRIT</th>\n  </tr>\n  <tr>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val-u</th>\n    <th>test-u</th>\n    <th>refexp</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"8\">Generalist Models</td>\n    <td>GPV-2</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>51.50</td>\n  </tr>\n  <tr>\n    <td>OFA-L*</td>\n    <td>79.96</td>\n    <td>83.67</td>\n    <td>76.39</td>\n    <td>68.29</td>\n    <td>76.00</td>\n    <td>61.75</td>\n    <td>67.57</td>\n    <td>67.58</td>\n    <td>61.70</td>\n  </tr>\n  <tr>\n    <td>Unified-IO</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td><b>78.61</b></td>\n  </tr>\n  <tr>\n    <td>VisionLLM-H</td>\n    <td></td>\n    <td>86.70</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Shikra-7B</td>\n    <td>87.01</td>\n    <td>90.61</td>\n    <td>80.24 </td>\n    <td>81.60</td>\n    <td>87.36</td>\n    <td>72.12</td>\n    <td>82.27</td>\n    <td>82.19</td>\n    <td>69.34</td>\n  </tr>\n  <tr>\n    <td>Shikra-13B</td>\n    <td>87.83 </td>\n    <td>91.11</td>\n    <td>81.81</td>\n    <td>82.89</td>\n    <td>87.79</td>\n    <td>74.41</td>\n    <td>82.64</td>\n    <td>83.16</td>\n    <td>69.03</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B</td>\n    <td><b>89.36</b></td>\n    <td>92.26</td>\n    <td><b>85.34</b></td>\n    <td><b>83.12</b></td>\n    <td>88.25</td>\n    <td><b>77.21</b></td>\n    <td>85.58</td>\n    <td>85.48</td>\n    <td>78.22</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B-Chat</td>\n    <td>88.55</td>\n    <td><b>92.27</b></td>\n    <td>84.51</td>\n    <td>82.82</td>\n    <td><b>88.59</b></td>\n    <td>76.79</td>\n    <td><b>85.96</b></td>\n    <td><b>86.32</b></td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td rowspan=\"3\">Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>G-DINO-L</td>\n    <td>90.56&nbsp;&nbsp;</td>\n    <td>93.19</td>\n    <td>88.24</td>\n    <td>82.75</td>\n    <td>88.95</td>\n    <td>75.92</td>\n    <td>86.13</td>\n    <td>87.02</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>UNINEXT-H</td>\n    <td>92.64 </td>\n    <td>94.33</td>\n    <td>91.46</td>\n    <td>85.24</td>\n    <td>89.63</td>\n    <td>79.79</td>\n    <td>88.73</td>\n    <td>89.37</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>ONE-PEACE</td>\n    <td>92.58 </td>\n    <td>94.18</td>\n    <td>89.26</td>\n    <td>88.77</td>\n    <td>92.21</td>\n    <td>83.23</td>\n    <td>89.22</td>\n    <td>89.27</td>\n    <td>-</td>\n  </tr>\n</tbody>\n</table>\n\n- 在定位任务上，Qwen-VL 全面超过 Shikra-13B，取得了目前 Generalist LVLM 模型上在 Refcoco 上的 **SOTA**。\n- Qwen-VL 并没有在任何中文定位数据上训练过，但通过中文 Caption 数据和 英文 Grounding 数据的训练，可以 Zero-shot 泛化出中文 Grounding 能力。\n\n我们提供了以上**所有**评测脚本以供复现我们的实验结果。请阅读 [eval_mm/EVALUATION.md](eval_mm/EVALUATION.md) 了解更多信息。\n\n### 对话能力测评\n\nTouchStone 是一个基于 GPT4 打分来评测 LVLM 模型的图文对话能力和人类对齐水平的基准。它涵盖了 300+张图片、800+道题目、27个类别，包括基础属性、人物地标、视觉推理、诗歌创作、故事写作、商品比较、图片解题等**尽可能广泛的类别**。关于 TouchStone 的详细介绍，请参考[touchstone/README_CN.md](touchstone/README_CN.md)了解更多信息。\n\n#### 英语\n\n| Model            | Score |\n| ---------------- | ----- |\n| PandaGPT         | 488.5 |\n| MiniGPT4         | 531.7 |\n| InstructBLIP     | 552.4 |\n| LLaMA-AdapterV2  | 590.1 |\n| LLaVA            | 602.7 |\n| mPLUG-Owl        | 605.4 |\n| Qwen-VL-Chat     | 645.2 |\n| Qwen-VL-Chat-1.1 | 711.6 |\n\n#### 中文\n\n| Model            | Score |\n| ---------------- | ----- |\n| VisualGLM        | 247.1 |\n| Qwen-VL-Chat     | 401.2 |\n| Qwen-VL-Chat-1.1 | 481.7 |\n\nQwen-VL-Chat 模型在中英文的对齐评测中均取得当前 LVLM 模型下的最好结果。\n<br>\n\n### 其它榜单测评\n\n#### MME Benchmark\n\nMME是多模态大型语言模型的综合评价基准。它在总共14个子任务上评测**感知和认知**能力。Qwen-VL-Chat在这个基准上实现了SOTAs。完整复现[见此](eval_mm/mme/EVAL_MME.md).\n\n<p align=\"center\">\n    <img src=\"eval_mm/mme/perception.jpg\" width=\"600\"/>\n<p>\n<p align=\"center\">\n    <img src=\"eval_mm/mme/cognition.jpg\" width=\"600\"/>\n<p>\n\n#### SEED-Bench\n\nSEED-Bench是一个包含1.9万选择题的多模态基准测评，通过人工注释的结果评估多模态大模型，涵盖12个评估维度，包括**图像和视频理解**。Qwen-VL和Qwen-VL-chat在这个基准上实现了SOTAs。完整复现[见此](eval_mm/seed_bench/EVAL_SEED.md)。\n\n<p align=\"center\">\n    <img src=\"eval_mm/seed_bench/leaderboard.jpg\"/>\n<p>\n\n## 部署要求\n\n* python 3.8及以上版本\n* pytorch 1.12及以上版本，推荐2.0及以上版本\n* 建议使用CUDA 11.4及以上（GPU用户需考虑此选项）\n<br>\n\n## 快速使用\n\n我们提供简单的示例来说明如何利用 🤖 ModelScope 和 🤗 Transformers 快速使用 Qwen-VL 和 Qwen-VL-Chat。\n\n在开始前，请确保你已经配置好环境并安装好相关的代码包。最重要的是，确保你满足上述要求，然后安装相关的依赖库。\n\n```bash\npip install -r requirements.txt\n```\n\n接下来你可以开始使用Transformers或者ModelScope来使用我们的模型。关于视觉模块的更多用法，请参考[教程](TUTORIAL_zh.md)。\n\n#### 🤗 Transformers\n\n如希望使用 Qwen-VL-chat 进行推理，所需要写的只是如下所示的数行代码。**请确保你使用的是最新代码。**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\n# 请注意：分词器默认行为已更改为默认关闭特殊token攻击防护。\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# 打开bf16精度，A100、H100、RTX3060、RTX3070等显卡建议启用以节省显存\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# 打开fp16精度，V100、P100、T4等显卡建议启用以节省显存\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# 使用CPU进行推理，需要约32GB内存\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# 默认gpu进行推理，需要约24GB显存\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# 可指定不同的生成长度、top_p等相关超参（transformers 4.32.0及以上无需执行此操作）\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# 第一轮对话\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url\n    {'text': '这是什么?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n# 图中是一名女子在沙滩上和狗玩耍，旁边是一只拉布拉多犬，它们处于沙滩上。\n\n# 第二轮对话\nresponse, history = model.chat(tokenizer, '框出图中击掌的位置', history=history)\nprint(response)\n# <ref>击掌</ref><box>(536,509),(588,602)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('1.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n\n运行Qwen-VL同样非常简单。\n\n<summary>运行Qwen-VL</summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\n# 打开bf16精度，A100、H100、RTX3060、RTX3070等显卡建议启用以节省显存\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# 打开fp16精度，V100、P100、T4等显卡建议启用以节省显存\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# 使用CPU进行推理，需要约32GB内存\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cpu\", trust_remote_code=True).eval()\n# 默认gpu进行推理，需要约24GB显存\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# 可指定不同的生成长度、top_p等相关超参（transformers 4.32.0及以上无需执行此操作）\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url\n    {'text': 'Generate the caption in English with grounding:'},\n])\ninputs = tokenizer(query, return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nresponse = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)\nprint(response)\n# <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>Generate the caption in English with grounding:<ref> Woman</ref><box>(451,379),(731,806)</box> and<ref> her dog</ref><box>(219,424),(576,896)</box> playing on the beach<|endoftext|>\nimage = tokenizer.draw_bbox_on_latest_picture(response)\nif image:\n  image.save('2.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_spotting_caption.jpg\" width=\"500\"/>\n<p>\n\n若在使用上述代码时由于各种原因无法从 HuggingFace 拉取模型和代码，可以先从 ModelScope 下载模型及代码至本地，再从本地加载模型：\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-VL')\nmodel_dir = snapshot_download('qwen/Qwen-VL-Chat')\n\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"cuda\",\n    trust_remote_code=True\n).eval()\n```\n\n#### 🤖 ModelScope\n\n魔搭（ModelScope）是开源的模型即服务共享平台，为泛AI开发者提供灵活、易用、低成本的一站式模型服务产品。使用ModelScope同样非常简单，代码如下所示：\n\n```python\nfrom modelscope import (\n    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n)\nimport torch\nmodel_id = 'qwen/Qwen-VL-Chat'\nrevision = 'v1.0.0'\n\nmodel_dir = snapshot_download(model_id, revision=revision)\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nif not hasattr(tokenizer, 'model_dir'):\n    tokenizer.model_dir = model_dir\n# 打开bf16精度，A100、H100、RTX3060、RTX3070等显卡建议启用以节省显存\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# 打开fp16精度，V100、P100、T4等显卡建议启用以节省显存\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# 使用CPU进行推理，需要约32GB内存\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cpu\", trust_remote_code=True).eval()\n# 默认gpu进行推理，需要约24GB显存\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True).eval()\n\n# 指定生成超参数（transformers 4.32.0及以上无需执行此操作）\n# model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)\n\n# 第一轮对话\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>这是什么', history=None)\nprint(response)\n# 图中是一名年轻女子在沙滩上和她的狗玩耍，狗的品种是拉布拉多。她们坐在沙滩上，狗的前腿抬起来，与人互动。\n\n# 第二轮对话\nresponse, history = model.chat(tokenizer, '输出击掌的检测框', history=history)\nprint(response)\n# <ref>\"击掌\"</ref><box>(211,412),(577,891)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('output_chat.jpg')\nelse:\n  print(\"no box\")\n```\n\n<br>\n\n## 量化\n\n### 用法\n\n当前我们提供了基于[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)的量化方案，并提供了Qwen-VL-Chat的Int4量化版本Qwen-VL-Chat-Int4 [点击此处](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4)。该模型在效果评测上几乎无损，并在显存占用和推理速度上具有明显优势。\n\n下文说明如何使用该量化模型。开始之前，请确保你满足要求（如torch2.0及以上、transformers 4.32.0及以上，等）并安装所需的代码库：\n\n```bash\npip install optimum\ngit clone https://github.com/JustinLin610/AutoGPTQ.git & cd AutoGPTQ\npip install -v .\n```\n\n如遇到安装 `auto-gptq` 的问题，建议您前往官方[repo](https://github.com/PanQiWei/AutoGPTQ) 寻找合适的wheel。\n\n随后你便可以按照上述用法****，轻松调用量化模型：\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-VL-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>这是什么', history=None)\nprint(response)\n```\n\n### 效果评测\n\n我们列出不同精度下模型在评测基准 **[TouchStone](https://github.com/OFA-Sys/TouchStone)** 上的表现，并发现量化模型并没有显著性能损失。结果如下所示：\n\n| Quantization | ZH         | EN            |\n| ------------ | :--------: | :-----------: | \n| BF16         | 401.2      |    645.2      |\n| Int4         | 386.6      |    651.4      |\n\n### 推理速度\n\n我们测算了在输入一张图片（即258个token）的条件下BF16和Int4的模型生成1792 (2048-258) 和 7934 (8192-258) 个token的平均速度。\n\n| Quantization | Speed (2048 tokens) | Speed (8192 tokens) |\n| ------------ | :-----------------: | :-----------------: |\n| BF16         |        28.87        |        24.32        |\n| Int4         |        37.79        |        34.34        |\n\n推理速度测算是在单卡 A100-SXM4-80G GPU上运行，使用PyTorch 2.0.1及CUDA 11.4。\n\n### GPU显存占用\n\n我们还测算了在一张图片输入的条件下BF16和Int4模型生成1792 (2048-258) 和 7934 (8192-258) 个token所需显存。结果如下所示：\n\n| Quantization | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |\n| ------------ | :---------------------------------: | :-----------------------------------: |\n| BF16         |               22.60GB               |                28.01GB                |\n| Int4         |               11.82GB               |                17.23GB                |\n\n上述速度和显存测算使用[此脚本](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile_mm.py)完成。\n<br>\n\n## 微调\n\n我们提供了`finetune.py`这个脚本供用户实现在自己的数据上进行微调的功能，以接入下游任务。此外，我们还提供了shell脚本减少用户的工作量。这个脚本支持 [DeepSpeed](https://github.com/microsoft/DeepSpeed) 和 [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) 。我们提供的shell脚本使用了DeepSpeed，因此建议您确保已经安装DeepSpeed。\n\n首先，你需要准备你的训练数据。你需要将所有样本放到一个列表中并存入json文件中。每个样本对应一个字典，包含id和conversation，其中后者为一个列表。示例如下所示：\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"你好\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"我是Qwen-VL,一个支持视觉输入的大模型。\"\n      }\n    ]\n  },\n  {\n    \"id\": \"identity_1\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>\\n图中的狗是什么品种？\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"图中是一只拉布拉多犬。\"\n      },\n      {\n        \"from\": \"user\",\n        \"value\": \"框出图中的格子衬衫\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"<ref>格子衬衫</ref><box>(588,499),(725,789)</box>\"\n      }\n    ]\n  },\n  { \n    \"id\": \"identity_2\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>assets/mm_tutorial/Chongqing.jpeg</img>\\nPicture 2: <img>assets/mm_tutorial/Beijing.jpeg</img>\\n图中都是哪\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"第一张图片是重庆的城市天际线，第二张图片是北京的天际线。\"\n      }\n    ]\n  }\n]\n```\n为针对多样的VL任务，我们增加了一下的特殊tokens： `<img> </img> <ref> </ref> <box> </box>`.\n\n对于带图像输入的内容可表示为 `Picture id: <img>img_path</img>\\n{your prompt}`，其中`id`表示对话中的第几张图片。\"img_path\"可以是本地的图片或网络地址。 \n\n对话中的检测框可以表示为`<box>(x1,y1),(x2,y2)</box>`，其中 `(x1, y1)` 和`(x2, y2)`分别对应左上角和右下角的坐标，并且被归一化到`[0, 1000)`的范围内. 检测框对应的文本描述也可以通过`<ref>text_caption</ref>`表示。\n\n\n准备好数据后，你可以使用我们提供的shell脚本实现微调。注意，你需要在脚本中指定你的数据的路径。\n\n微调脚本能够帮你实现：\n- 全参数微调\n- LoRA\n- Q-LoRA\n\n### 全参数微调\n默认下全参数微调在训练过程中更新LLM所有参数。我们的实验中，在微调阶段不更新ViT的参数会取得更好的表现。你可以运行这个脚本开始训练：\n\n```bash\n# 分布式训练。由于显存限制将导致单卡训练失败，我们不提供单卡训练脚本。\nsh finetune/finetune_ds.sh\n```\n\n尤其注意，你需要在脚本中指定正确的模型名称或路径、数据路径、以及模型输出的文件夹路径。如果你想修改deepspeed配置，可以删除掉`--deepspeed`这个输入或者自行根据需求修改DeepSpeed配置json文件。此外，我们支持混合精度训练，因此你可以设置`--bf16 True`或者`--fp16 True`。经验上，如果你的机器支持bf16，我们建议使用bf16，这样可以和我们的预训练和对齐训练保持一致，这也是为什么我们把默认配置设为它的原因。\n\n### LoRA\n运行LoRA的方法类似全参数微调。但在开始前，请确保已经安装`peft`代码库。另外，记住要设置正确的模型、数据和输出路径。我们建议你为模型路径使用绝对路径。这是因为LoRA仅存储adapter部分参数，而adapter配置json文件记录了预训练模型的路径，用于读取预训练模型权重。同样，你可以设置bf16或者fp16。\n\n```bash\n# 单卡训练\nsh finetune/finetune_lora_single_gpu.sh\n# 分布式训练\nsh finetune/finetune_lora_ds.sh\n```\n\n与全参数微调不同，LoRA ([论文](https://arxiv.org/abs/2106.09685)) 只更新adapter层的参数而无需更新原有语言模型的参数。这种方法允许用户用更低的显存开销来训练模型，也意味着更小的计算开销。\n\n注意，如果你使用预训练模型进行LoRA微调，而非chat模型，模型的embedding和输出层的参数将被设为可训练的参数。这是因为预训练模型没有学习过ChatML格式中的特殊token，因此需要将这部分参数设为可训练才能让模型学会理解和预测这些token。这也意味着，假如你的训练引入新的特殊token，你需要通过代码中的`modules_to_save`将这些参数设为可训练的参数。如果你想节省显存占用，可以考虑使用chat模型进行LoRA微调，显存占用将大幅度降低。下文的显存占用和训练速度的记录将详细介绍这部分细节。\n\n### Q-LoRA\n如果你依然遇到显存不足的问题，可以考虑使用Q-LoRA ([论文](https://arxiv.org/abs/2305.14314))。该方法使用4比特量化模型以及paged attention等技术实现更小的显存开销。运行Q-LoRA你只需运行如下脚本：\n\n```bash\n# 单卡训练\nsh finetune/finetune_qlora_single_gpu.sh\n# 分布式训练\nsh finetune/finetune_qlora_ds.sh\n```\n\n我们建议你使用我们提供的Int4量化模型进行训练，即Qwen-VL-Chat-Int4。请**不要使用**非量化模型！与全参数微调以及LoRA不同，Q-LoRA仅支持fp16。此外，上述LoRA关于特殊token的问题在Q-LoRA依然存在。并且，Int4模型的参数无法被设为可训练的参数。所幸的是，我们只提供了Chat模型的Int4模型，因此你不用担心这个问题。但是，如果你执意要在Q-LoRA中引入新的特殊token，很抱歉，我们无法保证你能成功训练。\n\n与全参数微调不同，LoRA和Q-LoRA的训练只需存储adapter部分的参数。假如你需要使用LoRA训练后的模型，你需要使用如下方法。你可以用如下代码读取模型：\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n如果你觉得这样一步到位的方式让你很不安心或者影响你接入下游应用，你可以选择先合并并存储模型（LoRA支持合并，Q-LoRA不支持），再用常规方式读取你的新模型，示例如下：\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\n注意：分布式训练需要根据你的需求和机器指定正确的分布式训练超参数。此外，你需要根据你的数据、显存情况和训练速度预期，使用`--model_max_length`设定你的数据长度。\n\n### 显存占用及训练速度\n下面记录Qwen_VL模型在单GPU使用LoRA（LoRA (Base)指的是embedding和输出层参与训练，而LoRA (Chat)则不优化这部分参数）和QLoRA时处理不同长度输入的显存占用和训练速度的情况。本次评测运行于单张A100-SXM4-80G GPU，使用CUDA 11.8和Pytorch 2.0。我们统一使用batch size为1，gradient accumulation为8的训练配置，每个样本包含一张图，分别记录输入长度分别为384、512、1024和2048的显存占用（GB）和训练速度（s/iter）。具体数值如下所示：\n\n<table>\n    <tr>\n <th rowspan=\"2\">Method</th><th colspan=\"4\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">384</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th>\n    </tr>\n    <tr>\n      <td>LoRA (Base)</td><td align=\"center\">37.1G / 2.3s/it</td><td align=\"center\">37.3G / 2.4s/it</td><td align=\"center\">38.7G / 3.6s/it</td><td align=\"center\">38.7G / 6.1s/it</td>\n    </tr>\n    <tr>\n      <td>LoRA (Chat)</td><td align=\"center\">23.3G / 2.2s/it</td><td align=\"center\">23.6G / 2.3s/it</td><td align=\"center\">25.1G / 3.5s/it</td><td align=\"center\">27.3G / 5.9s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">17.0G / 4.2s/it</td><td align=\"center\">17.2G / 4.5s/it</td><td align=\"center\">18.2G / 5.5s/it</td><td align=\"center\">19.3G / 7.9s/it</td>\n    </tr>\n\n</table>\n\n<br><br>\n## Demo\n\n### Web UI\n\n我们提供了Web UI的demo供用户使用。在开始前，确保已经安装如下代码库：\n\n```\npip install -r requirements_web_demo.txt\n```\n\n随后运行如下命令，并点击生成链接：\n\n```\npython web_demo_mm.py\n```\n\n<br>\n\n## FAQ\n\n如遇到问题，敬请查阅 [FAQ](FAQ_zh.md)以及issue区，如仍无法解决再提交issue。\n<br>\n\n## 使用协议\n\n研究人员与开发者可使用Qwen-VL和Qwen-VL-Chat或进行二次开发。我们同样允许商业使用，具体细节请查看[LICENSE](LICENSE)。如需商用，请填写[问卷](https://dashscope.console.aliyun.com/openModelApply/qianwen)申请。\n<br>\n\n## 引用\n\n如果你觉得我们的论文和代码对你的研究有帮助，请考虑:star: 和引用 :pencil: :)\n\n```BibTeX\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n<br>\n\n## 联系我们\n\n如果你想给我们的研发团队和产品团队留言，请通过邮件（qianwen_opensource@alibabacloud.com）联系我们。\n\n"
        },
        {
          "name": "README_JA.md",
          "type": "blob",
          "size": 42.5849609375,
          "content": "<p align=\"left\">\n        <a href=\"README_CN.md\">中文</a>&nbsp ｜ &nbsp <a href=\"README.md\">English</a>&nbsp ｜ &nbsp日本語&nbsp\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"assets/logo.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        Qwen-VL <a href=\"https://modelscope.cn/models/qwen/Qwen-VL/summary\">🤖 <a> | <a href=\"https://huggingface.co/Qwen/Qwen-VL\">🤗</a>&nbsp ｜ Qwen-VL-Chat <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary\">🤖 <a>| <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat\">🤗</a>&nbsp ｜ Qwen-VL-Chat-Int4 <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat-Int4\">🤗</a>\n<br>\n<a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary\">Demo</a>&nbsp ｜ &nbsp<a href=\"https://arxiv.org/abs/2308.12966\">Paper</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://github.com/camenduru/Qwen-VL-Chat-colab\">Colab</a>&nbsp&nbsp | &nbsp <a href=\"TUTORIAL_ja.md\">Tutorial</a>\n</p>\n<br><br>\n<p align=\"left\">\n        日本語ドキュメントメンテナー: <a href=\"https://github.com/eltociear\">Ikko Eltociear Ashimine</a>\n</p>\n<br>\n\n**Qwen-VL** （Qwen Large Vision Language Model）は、アリババクラウドが提唱するラージモデルシリーズ Qwen（略称: Tongyi Qianwen）のマルチモーダル版です。Qwen-VL は、画像、テキスト、バウンディングボックスを入力として受け付け、テキストとバウンディングボックスを出力します。Qwen-VL の特徴は以下の通りです:\n\n- **好調なパフォーマンス**: 複数の英語評価ベンチマーク（Zero-shot Captioning、VQA、DocVQA、Grounding を含む）において、同様のモデル規模でオープンソース化された既存の大規模ビジョン言語モデル（LVLM）を大幅に上回ります。\n- **テキスト認識をサポートする多言語 LVLM**: Qwen-VL は、英語、中国語、多言語の会話を自然にサポートし、画像内の中国語と英語の二言語テキストのエンドツーエンドの認識を促進します。\n- **複数画像のインターリーブ会話**: この機能により、複数の画像を入力し、比較することができる。また、画像に関連する質問を指定し、複数の画像によるストーリーテリングを行うこともできます。\n- **中国語のグラウンディングを支える初のジェネラリストモデル**: 中国語と英語のオープンドメイン言語表現によるバウンディングボックスの検出。\n- **きめ細やかな認識と理解**: 現在他のオープンソース LVLM で使用されている 224\\*224 の解像度と比較して、448\\*448 の解像度は、きめ細かいテキスト認識、文書 QA、バウンディングボックス注釈を促進する。\n\n<br>\n<p align=\"center\">\n    <img src=\"assets/demo_vl.gif\" width=\"400\"/>\n<p>\n<br>\n\nQwen-VL シリーズの 2 つのモデルを公開します:\n\n- Qwen-VL: LLM の初期化に Qwen-7B を、視覚エンコーダの初期化に [Openclip ViT-bigG](https://github.com/mlfoundations/open_clip) を用いた学習済み LVLM モデル。そして、それらをランダムに初期化されたクロスアテンションレイヤーで接続する。\n- Qwen-VL-Chat: マルチモーダルな LLM ベースの AI アシスタント。Qwen-VL-Chat は、複数の画像入力、複数ラウンドの質問応答、クリエイティブな機能など、より柔軟なインタラクションをサポートします。\n  <br>\n\n## ニュースとアップデート\n* 2023.11.28 Qwen-VL は、GPT4V、PALI-X を凌駕する最高レベルの [DOCVQA](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1) をシングルモデルで達成し、直接画像を入力するだけで様々なタスクを分析理解できる汎用モデルであり。 https://qianwen.aliyun.com のマルチモーダルタブで直接新しいモデルを体験できます。\n* 2023.9.25 Qwen-VL-Chat モデルが更新され、中国語コマンドのフォローがより堅牢になり、Web ページと表の画像の理解と質問と回答の機能が向上し、対話のパフォーマンスが向上しました (タッチストーン: 中国語: 401.2->481.7、英語: 645.2->711.6)。\n* 2023.9.12 フルパラメータ微調整、LoRA、Q-LoRA を含む、Qwen-VL モデルの微調整をサポートするようになりました。\n* 2023.9.8 [Colab](https://github.com/camenduru/Qwen-VL-Chat-c​​olab) のサンプルを提供してくれた [camenduru](https://github.com/camenduru) に感謝します。これをチュートリアルとして使用して、12G GPU でローカルまたはオンラインのデモを行うことができます。\n* 2023.9.4 Qwen-VL シリーズは、画像とビデオの両方の理解を含むマルチモーダル LLM を評価するための、正確な人による注釈を備えた 19,000 個の多肢選択質問のマルチモーダル ベンチマークである [Seed-Bench](eval_mm/seed_bench/EVAL_SEED.md) で SOTA を達成します。\n* 2023.9.1 基本的な認識と理解だけでなく、文学創作までを含むマルチモーダル言語モデルの包括的な評価である [TouchStone](https://github.com/OFA-Sys/TouchStone) 評価をリリースします 。 強力な LLM を判定者として使用し、マルチモーダルな情報をテキストに変換します。\n* 2023.8.31 低メモリコストでありながら推論速度の向上を実現する Qwen-VL-Chat 用の Int4 量子化モデル **Qwen-VL-Chat-Int4** をリリースしました。 また、ベンチマーク評価においても大きなパフォーマンスの低下はありません。\n* 2023.8.22 ModelScope と Hugging Face で **Qwen-VL** と **Qwen-VL-Chat** をリリースしました。 また、トレーニングの詳細やモデルのパフォーマンスなど、モデルの詳細については [論文](https://arxiv.org/abs/2308.12966) も提供しています。\n\n## 評価\n\nモデルの能力を2つの観点から評価しました:\n\n1. **標準ベンチマーク**: マルチモーダルなタスクの 4 つの主要カテゴリーについて、モデルの基本的なタスク能力を評価する:\n   \n   - ゼロショットキャプション: 未見のデータセットに対して、モデルのゼロショット画像キャプション能力を評価する;\n   - 一般的な VQA: 判定、色、数、カテゴリなど、画像の一般的な質問応答能力を評価する;\n   - テキストベース VQA: 文書 QA、図表 QAなど、写真内のテキストを認識するモデルの能力を評価する;\n   - 参照表現理解: 参照表現理解: 参照表現で記述された画像内の対象物を特定する能力を評価する。\n2. **TouchStone**: 総合的なテキスト画像対話能力と人間とのアライメントレベルを評価するために、GPT4 によるスコアリングに基づく TouchStone と呼ばれるベンチマークを構築し、LVLM モデルを評価しました。\n   \n   - TouchStone ベンチマークは、合計 300 以上の画像、800 以上の質問、27 のカテゴリをカバーしています。例えば、属性ベースの Q&A、有名人の認識、詩の作文、複数の画像の要約、商品比較、数学の問題解決などです;\n   - 画像の直接入力という GPT4 の現在の制限を打ち破るため、TouchStone は人間のラベル付けによるきめ細かい画像注釈を提供します。これらの詳細な注釈は、質問とモデルの出力と共に、採点のために GPT4 に提示されます。\n   - ベンチマークには英語版と中国語版があります。\n\n評価結果は以下の通りです:\n\nQwen-VL は、複数の VL タスクにおいて、現行の SOTA ジェネラリストモデルを上回り、また、能力範囲の点でより包括的なカバレッジを持ちます。\n\n<p align=\"center\">\n    <img src=\"assets/radar.png\" width=\"600\"/>\n<p>\n\n### ゼロショットキャプションと一般的な VQA\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"2\">Zero-shot Captioning</th>\n    <th colspan=\"5\">General VQA</th>\n  </tr>\n  <tr>\n    <th>NoCaps</th>\n    <th>Flickr30K</th>\n    <th>VQAv2<sup>dev</sup></th>\n    <th>OK-VQA</th>\n    <th>GQA</th>\n    <th>SciQA-Img<br>(0-shot)</th>\n    <th>VizWiz<br>(0-shot)</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"10\">Generalist<br>Models</td>\n    <td>Flamingo-9B</td>\n    <td>-</td>\n    <td>61.5</td>\n    <td>51.8</td>\n    <td>44.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>28.8</td>\n  </tr>\n  <tr>\n    <td>Flamingo-80B</td>\n    <td>-</td>\n    <td>67.2</td>\n    <td>56.3</td>\n    <td>50.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>31.6</td>\n  </tr>\n  <tr>\n    <td>Unified-IO-XL</td>\n    <td>100.0</td>\n    <td>-</td>\n    <td>77.9</td>\n    <td>54.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Kosmos-1</td>\n    <td>-</td>\n    <td>67.1</td>\n    <td>51.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>29.2</td>\n  </tr>\n  <tr>\n    <td>Kosmos-2</td>\n    <td>-</td>\n    <td>80.5</td>\n    <td>51.1</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>103.9</td>\n    <td>71.6</td>\n    <td>65.0</td>\n    <td>45.9</td>\n    <td>32.3</td>\n    <td>61.0</td>\n    <td>19.6</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td><strong>121.9</strong></td>\n    <td>82.8</td>\n    <td>-</td>\n    <td>-</td>\n    <td>49.5</td>\n    <td>63.1</td>\n    <td>33.4</td>\n  </tr>\n  <tr>\n    <td>Shikra (Vicuna-13B)</td>\n    <td>-</td>\n    <td>73.9</td>\n    <td>77.36</td>\n    <td>47.16</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td><strong>Qwen-VL (Qwen-7B)</strong></td>\n    <td>121.4</td>\n    <td><b>85.8</b></td>\n    <td><b>78.8</b></td>\n    <td><b>58.6</b></td>\n    <td><b>59.3</b></td>\n    <td>67.1</td>\n    <td>35.2</td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>63.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>39.1</td>\n  </tr> -->\n  <tr>\n    <td>Qwen-VL-Chat</td>\n    <td>120.2</td>\n    <td>81.0</td>\n    <td>78.2</td>\n    <td>56.6</td>\n    <td>57.5</td>\n    <td><b>68.2</b></td>\n    <td><b>38.9</b></td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL-Chat (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>60.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>44.45</td>\n  </tr> -->\n  <tr>\n    <td>Previous SOTA<br>(Per Task Fine-tuning)</td>\n    <td>-</td>\n    <td>127.0<br>(PALI-17B)</td>\n    <td>84.5<br>(InstructBLIP<br>-FlanT5-XL)</td>\n    <td>86.1<br>(PALI-X<br>-55B)</td>\n    <td>66.1<br>(PALI-X<br>-55B)</td>\n    <td>72.1<br>(CFR)</td>\n    <td>92.53<br>(LLaVa+<br>GPT-4)</td>\n    <td>70.9<br>(PALI-X<br>-55B)</td>\n  </tr>\n</tbody>\n</table>\n\n- ゼロショット画像のキャプション付けでは、Qwen-VL は Flickr30K で **SOTA** を達成し、InstructBlip を使用した Nocaps でも競争力のある結果を得ています。\n- 一般的な VQA では、Qwen-VL は同じ一般的な LVLM スケール設定で **SOTA** を達成しています。\n\n### テキスト指向VQA（画像中のテキスト理解能力に重点を置く）\n\n<table>\n<thead>\n  <tr>\n    <th>Model type</th>\n    <th>Model</th>\n    <th>TextVQA</th>\n    <th>DocVQA</th>\n    <th>ChartQA</th>\n    <th>AI2D</th>\n    <th>OCR-VQA</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"5\">Generalist Models</td>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>42.4</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td>50.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>mPLUG-DocOwl (LLaMA-7B)</td>\n    <td>52.6</td>\n    <td>62.2</td>\n    <td>57.4</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Pix2Struct-Large (1.3B)</td>\n    <td>-</td>\n    <td><b>76.6</b></td>\n    <td>58.6</td>\n    <td>42.1</td>\n    <td>71.3</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL (Qwen-7B)</td>\n    <td><b>63.8</b></td>\n    <td>65.1</td>\n    <td><b>65.7</b></td>\n    <td><b>62.3</b></td>\n    <td><b>75.7</b></td>\n  </tr>\n  <tr>\n    <td>Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>PALI-X-55B (Single-task FT)<br>(Without OCR Pipeline)</td>\n    <td>71.44</td>\n    <td>80.0</td>\n    <td>70.0</td>\n    <td>81.2</td>\n    <td>75.0</td>\n  </tr>\n</tbody>\n</table>\n\n- テキスト関連の認識/QA 評価において、Qwen-VL は汎用の LVLM スケール設定で SOTA を達成しています。\n- 解像度は上記のいくつかの評価において重要である。解像度が 224 のオープンソースの LVLM モデルの多くは、これらの評価ができないか、画像をカットすることでしか解決できないが、Qwen-VL は解像度を 448 にスケーリングし、エンドツーエンドで評価できるようにしました。Qwen-VL は、一部のタスクにおいて、解像度 1024 の Pix2Struct-Large モデルをも凌駕しています。\n\n### 表現理解の参照\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"3\">RefCOCO</th>\n    <th colspan=\"3\">RefCOCO+</th>\n    <th colspan=\"2\">RefCOCOg</th>\n    <th>GRIT</th>\n  </tr>\n  <tr>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val-u</th>\n    <th>test-u</th>\n    <th>refexp</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"8\">Generalist Models</td>\n    <td>GPV-2</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>51.50</td>\n  </tr>\n  <tr>\n    <td>OFA-L*</td>\n    <td>79.96</td>\n    <td>83.67</td>\n    <td>76.39</td>\n    <td>68.29</td>\n    <td>76.00</td>\n    <td>61.75</td>\n    <td>67.57</td>\n    <td>67.58</td>\n    <td>61.70</td>\n  </tr>\n  <tr>\n    <td>Unified-IO</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td><b>78.61</b></td>\n  </tr>\n  <tr>\n    <td>VisionLLM-H</td>\n    <td></td>\n    <td>86.70</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Shikra-7B</td>\n    <td>87.01</td>\n    <td>90.61</td>\n    <td>80.24 </td>\n    <td>81.60</td>\n    <td>87.36</td>\n    <td>72.12</td>\n    <td>82.27</td>\n    <td>82.19</td>\n    <td>69.34</td>\n  </tr>\n  <tr>\n    <td>Shikra-13B</td>\n    <td>87.83 </td>\n    <td>91.11</td>\n    <td>81.81</td>\n    <td>82.89</td>\n    <td>87.79</td>\n    <td>74.41</td>\n    <td>82.64</td>\n    <td>83.16</td>\n    <td>69.03</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B</td>\n    <td><b>89.36</b></td>\n    <td>92.26</td>\n    <td><b>85.34</b></td>\n    <td><b>83.12</b></td>\n    <td>88.25</td>\n    <td><b>77.21</b></td>\n    <td>85.58</td>\n    <td>85.48</td>\n    <td>78.22</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B-Chat</td>\n    <td>88.55</td>\n    <td><b>92.27</b></td>\n    <td>84.51</td>\n    <td>82.82</td>\n    <td><b>88.59</b></td>\n    <td>76.79</td>\n    <td><b>85.96</b></td>\n    <td><b>86.32</b></td>\n    <td>-</td>\n  <tr>\n    <td rowspan=\"3\">Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>G-DINO-L</td>\n    <td>90.56&nbsp;&nbsp;</td>\n    <td>93.19</td>\n    <td>88.24</td>\n    <td>82.75</td>\n    <td>88.95</td>\n    <td>75.92</td>\n    <td>86.13</td>\n    <td>87.02</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>UNINEXT-H</td>\n    <td>92.64 </td>\n    <td>94.33</td>\n    <td>91.46</td>\n    <td>85.24</td>\n    <td>89.63</td>\n    <td>79.79</td>\n    <td>88.73</td>\n    <td>89.37</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>ONE-PEACE</td>\n    <td>92.58 </td>\n    <td>94.18</td>\n    <td>89.26</td>\n    <td>88.77</td>\n    <td>92.21</td>\n    <td>83.23</td>\n    <td>89.22</td>\n    <td>89.27</td>\n    <td>-</td>\n  </tr>\n</tbody>\n</table>\n\n- Qwen-VL は、上記のすべての参照表現理解ベンチマークで **SOTA** を達成した。\n- Qwen-VL は中国語の下地データを学習していないが、中国語のキャプションデータと英語の下地データを学習することで、ゼロショットで中国語の下地タスクに汎化することができます。\n\n私たちの実験結果を再現するために、上記の評価スクリプトをすべて提供しています。詳しくは [eval_mm/EVALUATION.md](eval_mm/EVALUATION.md) をお読みください。\n\n### チャット評価\n\nTouchStone は GPT4 によるスコアリングに基づくベンチマークで、テキストと画像の対話および人間とのアライメントレベルにおける LVLM モデルの能力を評価する。合計 300 以上の画像、800 以上の質問、属性ベースの Q&A、有名人の認識、詩の作成、複数の画像の要約、商品比較、数学の問題解決など27のカテゴリをカバーしています。詳しくは [touchstone/README_JA.md](touchstone/README_JA.md) をお読みください。\n\n#### 英語\n\n| Model            | Score |\n| ---------------- | ----- |\n| PandaGPT         | 488.5 |\n| MiniGPT4         | 531.7 |\n| InstructBLIP     | 552.4 |\n| LLaMA-AdapterV2  | 590.1 |\n| LLaVA            | 602.7 |\n| mPLUG-Owl        | 605.4 |\n| Qwen-VL-Chat     | 645.2 |\n| Qwen-VL-Chat-1.1 | 711.6 |\n\n#### 中国語\n\n| Model            | Score |\n| ---------------- | ----- |\n| VisualGLM        | 247.1 |\n| Qwen-VL-Chat     | 401.2 |\n| Qwen-VL-Chat-1.1 | 481.7 |\n\nQwen-VL-Chat は中国語と英語のアライメント評価で最高の結果を得ました。\n<br>\n\n### その他のベンチマーク\n\n#### SEED-Bench\n\nSEED-Bench は、マルチモーダル LLM を評価するための正確な人による注釈を備えた 19,000 個の多肢選択式質問のマルチモーダル ベンチマークで、**画像** と **ビデオ** の両方の理解を含む 12 の評価次元をカバーしています。 詳細については、[こちら](eval_mm/seed_bench/EVAL_SEED.md) をご覧ください。\n\nQwen-VL と Qwen-VL-Chat は、このベンチマークで SOTA を達成しています。\n\n<p align=\"center\">\n    <img src=\"eval_mm/seed_bench/leaderboard.jpg\"/>\n<p>\n\n## 必要条件\n\n* python 3.8 以上\n* pytorch 1.12 以上、2.0 以上を推奨\n* CUDA 11.4 以上を推奨（GPU ユーザー向けです）\n  <br>\n\n## クイックスタート\n\n以下では、Qwen-VL と Qwen-VL-Chat を 🤖 ModelScope と 🤗 Transformers とともに使う方法を、簡単な例で示します。\n\nコードを実行する前に、環境のセットアップと必要なパッケージのインストールが済んでいることを 確認してください。上記の要件を満たしていることを確認してから、依存するライブラリをインストールしてください。\n\n```bash\npip install -r requirements.txt\n```\n\nこれで ModelScope や Transformers を使い始めることができます。ビジョンエンコーダについての詳しい使い方は、[チュートリアル](TUTORIAL_ja.md)を参照してください。\n\n#### 🤗 Transformers\n\nQwen-VL-Chat を推論に使用するために必要なのは、以下に示す数行のコードを入力することだけです。ただし、**最新のコードを使用していることを確認してください。**\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\n# Note: デフォルトの動作では、インジェクション攻撃防止機能がオフになりました。\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# bf16 の使用\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# fp16 の使用\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# cpu のみの使用\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# cuda デバイスの使用\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# 生成のためのハイパーパラメータの指定\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# 第 1 回 対話ターン\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # ローカルパスまたは url\n    {'text': '这是什么?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n# 写真はビーチでラブラドールの隣で愛犬と戯れる女性が写っており、彼らは砂の中にいる。\n\n# 第 2 回 対話ターン\nresponse, history = model.chat(tokenizer, '框出图中击掌的位置', history=history)\nprint(response)\n# <ref>击掌</ref><box>(536,509),(588,602)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('1.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n\n<details>\n  <summary>Running Qwen-VL</summary>\n\nRunning Qwen-VL pretrained base model is also simple.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\n# bf16 の使用\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# fp16 の使用\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# cpu のみの使用\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cpu\", trust_remote_code=True).eval()\n# cuda デバイスの使用\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# 生成のためのハイパーパラメータの指定\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # ローカルパスまたは url\n    {'text': 'Generate the caption in English with grounding:'},\n])\ninputs = tokenizer(query, return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nresponse = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)\nprint(response)\n# <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>Generate the caption in English with grounding:<ref> Woman</ref><box>(451,379),(731,806)</box> and<ref> her dog</ref><box>(219,424),(576,896)</box> playing on the beach<|endoftext|>\nimage = tokenizer.draw_bbox_on_latest_picture(response)\nif image:\n  image.save('2.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_spotting_caption.jpg\" width=\"500\"/>\n<p>\n\n</details>\n\nHuggingFaceからモデルのチェックポイントとコードをダウンロードする際にネットワークの問題が発生した場合、ModelScopeからチェックポイントをダウンロードする方法はこちらでございます。\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-VL')\nmodel_dir = snapshot_download('qwen/Qwen-VL-Chat')\n\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"cuda\",\n    trust_remote_code=True\n).eval()\n```\n\n#### 🤖 ModelScope\n\nModelScope は、MaaS（Model-as-a-Service）のためのオープンソースプラットフォームであり、AI 開発者に柔軟で費用対効果の高いモデルサービスを提供します。同様に、以下のように ModelScope でモデルを実行することができます:\n\n```python\nfrom modelscope import (\n    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n)\nimport torch\nmodel_id = 'qwen/Qwen-VL-Chat'\nrevision = 'v1.0.0'\n\nmodel_dir = snapshot_download(model_id, revision=revision)\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nif not hasattr(tokenizer, 'model_dir'):\n    tokenizer.model_dir = model_dir\n# bf16 の使用\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# fp16 の使用\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# cpu の使用\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cpu\", trust_remote_code=True).eval()\n# auto の使用\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True).eval()\n\n# 生成のためのハイパーパラメータの指定\nmodel.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)\n\n# 第 1 回 対話ターン\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>这是什么', history=None)\nprint(response)\n# 写真は、若い女性がビーチで愛犬のラブラドール種と戯れているところ。 二人は浜辺に座り、犬の前脚を上げて触れ合っている。\n\n# 第 2 回 対話ターン\nresponse, history = model.chat(tokenizer, '输出击掌的检测框', history=history)\nprint(response)\n# <ref>\"击掌\"</ref><box>(211,412),(577,891)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('output_chat.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n<br>\n\n## 量子化\n\n### 使用方法\n\n私たちは、[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)に基づいた新しいソリューションを提供し、Qwen-VL-ChatのためのInt4量子化モデル、Qwen-VL-Chat-Int4[Click here](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4)をリリースします。このモデルは、ほぼ無損失なモデル効果を達成しながら、メモリコストと推論速度の両方のパフォーマンスを向上させます。\n\nここでは、量子化されたモデルを推論に使用する方法を説明します。始める前に、必要な要件（torch 2.0以上、transformers 4.32.0以上など）を満たしていることを確認し、必要なパッケージをインストールしてください：\n\n```bash\npip install optimum\ngit clone https://github.com/JustinLin610/AutoGPTQ.git & cd AutoGPTQ\npip install -v .\n```\n\n`auto-gptq`のインストールに問題がある場合は、公式の[repo](https://github.com/PanQiWei/AutoGPTQ)をチェックして、ホイールを見つけることをお勧めする。\n\nそうすれば、量子化されたモデルを簡単にロードすることができ、いつもと同じように推論を実行することができる：\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-VL-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>这是什么', history=None)\nprint(response)\n```\n\n### 性能\n\nベンチマーク **[TouchStone](https://github.com/OFA-Sys/TouchStone)** において、BF16 モデルと Int4 モデルの両方のモデル性能を例示し、量子化モデルが大きな性能劣化に悩まされないことを見出しました。結果を以下に示します：\n\n| Quantization | ZH         | EN            |\n| ------------ | :--------: | :-----------: | \n| BF16         | 401.2      |    645.2      |\n| Int4         | 386.6      |    651.4      |\n\n### 推論スピード\n\nBF16 精度と Int4 量子化の下で、画像（258 トークンを要する）のコンテキストで 1792（2048-258）トークンと 7934（8192-258）トークンを生成する平均推論速度（トークン/秒）をそれぞれ測定した。\n\n| Quantization | Speed (2048 tokens) | Speed (8192 tokens) |\n| ------------ | :-----------------: | :-----------------: |\n| BF16         |        28.87        |        24.32        |\n| Int4         |        37.79        |        34.34        |\n\nプロファイリングは、PyTorch 2.0.1 と CUDA 11.4 を搭載したシングル A100-SXM4-80G GPU で実行されます。\n\n### GPU メモリ使用量\n\nまた、1792 (2048-258) 個のトークン (画像を含む) をコンテキストとしてエンコードする場合 (および単一のトークンを生成する場合) と、7934 (8192-258) 個のトークン (画像をコンテキストとして生成する場合) をそれぞれ BF16 または Int4 量子化レベルでエンコードする場合の GPU メモリ使用量のピーク値をプロファイリングしました。結果を以下に示します。\n\n| Quantization | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |\n| ------------ | :---------------------------------: | :-----------------------------------: |\n| BF16         |               22.60GB               |                28.01GB                |\n| Int4         |               11.82GB               |                17.23GB                |\n\n上記のスピードとメモリーのプロファイリングは、[このスクリプト](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile_mm.py)を使用しています。\n<br>\n\n## ファインチューニング\n\n現在、公式のトレーニングスクリプト `finetune.py` を提供しています。さらに、finetune.py のシェルスクリプトを提供し、finetune.py を実行することで、finetune.py を起動することができる。さらに、安心してファインチューニングを開始するためのシェルスクリプトも提供しています。このスクリプトは、[DeepSpeed](https://github.com/microsoft/DeepSpeed) および [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) を使用したトレーニングをサポートします。弊社が提供するシェル・スクリプトは DeepSpeed を使用するため、事前に DeepSpeed をインストールすることをお勧めします:\n\n学習データを準備するには、すべてのサンプルをリストにまとめ、json ファイルに保存する必要があります。各サンプルは id と会話リストで構成される辞書です。以下は 1 つのサンプルを含む単純なリストの例です:\n\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"你好\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"我是Qwen-VL,一个支持视觉输入的大模型。\"\n      }\n    ]\n  },\n  {\n    \"id\": \"identity_1\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>\\n图中的狗是什么品种？\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"图中是一只拉布拉多犬。\"\n      },\n      {\n        \"from\": \"user\",\n        \"value\": \"框出图中的格子衬衫\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"<ref>格子衬衫</ref><box>(588,499),(725,789)</box>\"\n      }\n    ]\n  },\n  { \n    \"id\": \"identity_2\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>assets/mm_tutorial/Chongqing.jpeg</img>\\nPicture 2: <img>assets/mm_tutorial/Beijing.jpeg</img>\\n图中都是哪\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"第一张图片是重庆的城市天际线，第二张图片是北京的天际线。\"\n      }\n    ]\n  }\n]\n```\n\nVL タスクの場合、`<img> </img> <ref> </ref> <box> </box>` などの特別なトークンが使用されます。\n\n画像は「画像 ID: `<img>img_path</img>\\n{your prompt}`」として表されます。ここで、「id」は会話内の画像の位置を 1 から示します。「img_path」は ローカル ファイル パスまたは Web リンク。\n\n座標ボックスは `<box>(x1,y1),(x2,y2)</box>`・として表されます。ここで、`(x1, y1)` と `(x2, y2)` は範囲内の正規化された値です。 `[0, 1000)`。 対応するテキスト説明は `<ref>text_caption</ref>` によって識別できます。\n\n\nデータ準備の後、提供されているシェルスクリプトを使って微調整を実行することができる。データファイルのパス `$DATA` を忘れずに指定してください。\n\nファインチューニングのスクリプトを使用することで、以下のことが可能になる：\n- フルパラメーター・ファインチューニング\n- LoRA\n- Q-LoRA\n\n### フルパラメーターファインチューニング\nフルパラメータパラメータのファインチューニングを行うには、トレーニングプロセス全体ですべてのパラメータを更新する必要があります。トレーニングを開始するには、以下のスクリプトを実行します：\n\n```bash\n# 分散トレーニング。GPU メモリが不足するとトレーニングが破綻するため、シングル GPU のトレーニングスクリプトは提供していません。\nsh finetune/finetune_ds.sh\n```\n\nシェルスクリプトでは、正しいモデル名またはパス、データパス、出力ディレクトリを指定することを忘れないでください。変更したい場合は、引数 `--deepspeed` を削除するか、要件に基づいて DeepSpeed 設定 json ファイルを変更してください。さらに、このスクリプトは混合精度のトレーニングに対応しており、`--bf16 True` または `--fp16 True` を使用することができます。経験的に、あなたのマシンがbf16をサポートしている場合、私たちのプリトレーニングとアライメントを整合させるためにbf16を使用することをお勧めします。\n\n### LoRA\n同様に、LoRA を実行するには、以下のように別のスクリプトを使って実行する。始める前に、`peft` がインストールされていることを確認してください。また、モデル、データ、出力へのパスを指定する必要があります。学習済みモデルには絶対パスを使用することをお勧めします。なぜなら、LoRA はアダプタのみを保存し、アダプタ設定 json ファイルの絶対パスは、ロードする事前学習済みモデルを見つけるために使用されるからです。また、このスクリプトは bf16 と fp16 の両方をサポートしている。\n\n```bash\n# シングル GPU トレーニング\nsh finetune/finetune_lora_single_gpu.sh\n# 分散トレーニング\nsh finetune/finetune_lora_ds.sh\n```\n\nLoRA ([論文](https://arxiv.org/abs/2106.09685)) は、フルパラメーターによるファインチューニングと比較して、adapter のパラメーターを更新するだけで、元の大きな言語モデル層は凍結されたままである。そのため、メモリコストが大幅に削減でき、計算コストも削減できる。\n\nなお、チャットモデル（Qwen-VL-Chatなど）ではなく、ベース言語モデル（Qwen-VLなど）の微調整にLoRAを使用した場合、スクリプトは自動的に学習可能なパラメータとして埋め込み層と出力層を切り替えます。これは、ベースとなる言語モデルには、ChatMLフォーマットによってもたらされる特殊なトークンに関する知識がないためです。したがって、これらのレイヤーは、モデルがトークンを理解し予測するために更新される必要があります。別の言い方をすれば、もしLoRAで特殊なトークンを学習するのであれば、コード内で `modules_to_save` を設定することで、レイヤーを学習可能なパラメータに設定する必要があります。さらに、LoRAのメモリフットプリントは、このような学習可能なパラメータがある場合とない場合で、大きな開きがあることがわかります。そのため、メモリに問題がある場合は、LoRAのChatモデルを微調整することをお勧めします。詳細は以下のプロファイルを参照してください。\n\n### Q-LoRA\nしかし、それでもメモリ不足に悩む場合は、Q-LoRA（[論文](https://arxiv.org/abs/2305.14314)）を検討することができます。これは、量子化されたラージ言語モデルと、ページド・アテンションなどの他のテクニックを使用し、さらに少ないメモリコストで実行することができます。Q-LoRA を実行するには、以下のスクリプトを直接実行してください：\n\n\n```bash\n# シングルGPUトレーニング\nsh finetune/finetune_qlora_single_gpu.sh\n# 分散トレーニング\nsh finetune/finetune_qlora_ds.sh\n```\n\nQ-LoRA については、弊社が提供する量子化モデル、例えば Qwen-7B-Chat-Int4 をロードすることをお勧めします。ただし、フルパラメータ・ファインチューニングや LoRA とは異なり、Q-LoRA では fp16 のみがサポートされる。\n\nLoRA と Q-LoRA の学習は、フルパラメータによるファインチューニングとは異なり、アダプターパラメータのみを保存する。仮に Qwen-7B から学習を開始したとすると、以下のようにファインチューニングされたモデルを読み込んで推論を行うことができる：\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\nアダプターをマージし、微調整したモデルをスタンドアロンモデルとして保存したい場合は（これは LoRA でのみ可能で、Q-LoRA からパラメータをマージすることはできません）、以下のコードを実行します：\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size and safe serialization are not necessary. \n# They respectively work for sharding checkpoint and save the model to safetensors\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n```\n\n注意：マルチGPUトレーニングの場合、分散トレーニング用の適切なハイパーパラメータをマシンに応じて指定する必要があります。また、データ、メモリフットプリント、トレーニング速度を考慮して、引数 `--model_max_length` で最大シーケンス長を指定することをお勧めします。\n\n\n### メモリと速度のプロファイリング\nシングルGPUトレーニングのセットアップにおいて、LoRA (LoRA (Base)はembeddingと出力層を学習させるが、LoRA (Chat)はembeddingと出力層を学習させない) とQ-LoRAのGPUメモリとトレーニング速度をプロファイリングする。このテストでは、シングルA100-SXM4-80G GPUで実験し、CUDA 11.8とPytorch 2.0を使用します。各サンプルには写真が含まれています。384、512、1024、2048という異なる長さの入力のメモリ（GB）と速度（s/iter）をプロファイリングします。統計量を以下に示す：\n<table>\n    <tr>\n <th rowspan=\"2\">Method</th><th colspan=\"4\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">384</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th>\n    </tr>\n    <tr>\n      <td>LoRA (Base)</td><td align=\"center\">37.1G / 2.3s/it</td><td align=\"center\">37.3G / 2.4s/it</td><td align=\"center\">38.7G / 3.6s/it</td><td align=\"center\">38.7G / 6.1s/it</td>\n    </tr>\n    <tr>\n      <td>LoRA (Chat)</td><td align=\"center\">23.3G / 2.2s/it</td><td align=\"center\">23.6G / 2.3s/it</td><td align=\"center\">25.1G / 3.5s/it</td><td align=\"center\">27.3G / 5.9s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">17.0G / 4.2s/it</td><td align=\"center\">17.2G / 4.5s/it</td><td align=\"center\">18.2G / 5.5s/it</td><td align=\"center\">19.3G / 7.9s/it</td>\n    </tr>\n\n</table>\n\nシェルスクリプトは `torchrun` を使用してシングル GPU またはマルチGPUトレーニングを実行します。そのため、分散トレーニングのための適切なハイパーパラメータをマシンに応じて指定する必要があります。\n<br><br>\n\n## デモ\n\n### Web UI\n\nWeb UI デモを構築するためのコードを提供します。始める前に、以下のパッケージがインストールされていることを確認してください:\n\n```bash\npip install -r requirements_web_demo.txt\n```\n\n次に以下のコマンドを実行し、生成されたリンクをクリックします:\n\n```bash\npython web_demo_mm.py\n```\n\n<br>\n\n## FAQ\n\n問題が発生した場合は、[FAQ](FAQ_ja.md) や issue を参照し、新しい issue を立ち上げる前に解決策を探してください。\n<br>\n\n## ライセンス契約\n\n研究者や開発者は、Qwen-VL と Qwen-VL-Chat のコードとモデルウェイトを自由に使用することができます。また、商用利用も可能です。詳しくは [LICENSE](LICENSE) をご覧ください。\n<br>\n\n## 引用\n\n私たちの論文やコードがあなたの研究に役立つとお感じになりましたら、スター :star: と 引用 :pencil: をお付けください :)\n\n```BibTeX\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n<br>\n\n## お問い合わせ\n\n研究チームまたは製品チームへのメッセージは、qianwen_opensource@alibabacloud.com までお気軽にお送りください。\n\n"
        },
        {
          "name": "README_KO.md",
          "type": "blob",
          "size": 41.1201171875,
          "content": "<p align=\"left\">\n        <a href=\"README_CN.md\">中文</a>&nbsp ｜ &nbspEnglish&nbsp&nbsp ｜ &nbsp<a href=\"README_JA.md\">日本語</a>&nbsp ｜ &nbsp<a href=\"README_KO.md\">한국어</a>&nbsp\n</p>\n<br><br>\n\n<p align=\"center\">\n    <img src=\"assets/logo.jpg\" width=\"400\"/>\n<p>\n<br>\n\n<p align=\"center\">\n        Qwen-VL <a href=\"https://modelscope.cn/models/qwen/Qwen-VL/summary\">🤖 <a> | <a href=\"https://huggingface.co/Qwen/Qwen-VL\">🤗</a>&nbsp ｜ Qwen-VL-Chat <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary\">🤖 <a>| <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat\">🤗</a>&nbsp ｜ Qwen-VL-Chat-Int4 <a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat-Int4\">🤗</a>\n<br>\n<a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://discord.gg/z3GAxXZ9Ce\">Discord</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary\">Demo</a>&nbsp ｜ &nbsp<a href=\"https://arxiv.org/abs/2308.12966\">Paper</a>&nbsp&nbsp | &nbsp&nbsp<a href=\"https://github.com/camenduru/Qwen-VL-Chat-colab\">Colab</a>&nbsp&nbsp | &nbsp <a href=\"TUTORIAL.md\">Tutorial</a>\n</p>\n<br><br>\n\n---\n\n\n**Qwen-VL**(Qwen Large Vision Language Model)은 알리바바 클라우드가 제안한 큰 모델 시리즈인 Qwen(약칭, Tongyi Qianwen)의 멀티모달 버전입니다. Qwen-VL은 이미지, 텍스트, 그리고 바운딩 박스를 입력으로 받아 텍스트와 바운딩 박스를 출력합니다. Qwen-VL의 특징은 다음과 같습니다.\n\n- **강력한 성능**: 동일한 모델 규모의 기존 공개된 대규모 시각 언어 모델(Large Vision Language Models, ㄴLVLM)보다 영어 평가 벤치마크(Zero-shot Captioning, VQA, DocVQA, Grounding 포함)에서 현저히 우수합니다.\n- **텍스트 인식을 지원하는 다국어 LVLM**: Qwen-VL은 자연스러운 영어, 중국어 및 다국어 대화를 지원하며, 이미지 내 중국어-영어 간 이중 언어 텍스트의 종단 간 인식을 개선했습니다.\n- **다중 이미지 교차 대화**: 이 기능은 여러 이미지의 입력과 비교뿐만 아니라 이미지와 관련된 질문을 지정하고 다중 이미지 스토리텔링에 참여할 수 있는 기능을 제공합니다.\n- **중국어에서 지상화를 지원하는 첫 번째 일반 모델**: 중국어와 영어의 개방형 언어 표현을 통해 바운딩 박스를 인식합니다.\n- **세밀한 인식 및 이해**: 다른 공개된 LVLM이 현재 사용하는 224\\*224 해상도와 비교하여 448\\*448 해상도는 세밀한 텍스트 인식, 문서 QA 및 바운딩 어노테이션을 개선했습니다.\n\n<br>\n<p align=\"center\">\n    <img src=\"assets/demo_vl.gif\" width=\"400\"/>\n<p>\n<br>\n\nQwen-VL 시리즈의 두 모델을 출시합니다.\n\n- Qwen-VL: 사전 훈련된 LVLM 모델로, Qwen-7B를 LLM의 초기화에 사용하며, 시각 인코더의 초기화로는 [Openclip ViT-bigG](https://github.com/mlfoundations/open_clip)를 사용하여, 무작위로 초기화된 교차 어텐션 레이어(randomly initialized cross-attention layer)에 연결합니다.\n- Qwen-VL-Chat: 정렬 기술로 훈련된 멀티모달 LLM 기반 AI 어시스턴트입니다. Qwen-VL-Chat은 여러 이미지 입력, 다중 라운드 질문 응답, 창의적 능력과 같은 더 유연한 상호작용을 지원합니다.\n\n<br>\n\n## 뉴스 및 업데이트\n* ```2023.9.25``` 🚀🚀🚀 Qwen-VL-Chat을 더욱 강력한 중국어 지시 수행 능력, 웹페이지 및 표 이미지에 대한 개선된 이해력, 더 나은 대화 성능(TouchStone: CN: 401.2->481.7, EN: 645.2->711.6)으로 업데이트 되었습니다.\n* ```2023.9.12``` 😃😃😃 이제 Qwen-VL 모델에 대한 파인튜닝을 지원합니다. 이에는 전체 파라미터 파인튜닝, LoRA 및 Q-LoRA가 포함됩니다.\n* ```2023.9.8``` 👍👍👍 camenduru가 멋진 Colab을 기여해 주셔서 감사합니다. 모두가 12G GPU에서 로컬 또는 온라인 Qwen-VL-Chat-Int4 데모 튜토리얼로 사용할 수 있습니다.\n* ```2023.9.5``` 👏👏👏 Qwen-VL-Chat은 MME Benchmark, 멀티모달 대형 언어 모델을 위한 종합적인 평가 벤치마크에서 SOTAs를 달성했습니다. 이는 총 14개의 하위 과제에서 인식과 인지 능력을 모두 측정합니다.\n* ```2023.9.4``` ⭐⭐⭐ Qwen-VL 시리즈는 Seed-Bench, 이미지 및 비디오 이해를 평가하는 19K 다중 선택 질문의 멀티모달 벤치마크에서 SOTAs를 달성했습니다. 이는 정확한 인간 주석을 갖추고 있습니다.\n* ```2023.9.1``` 🔥🔥🔥 기본적인 인식과 이해력뿐만 아니라 문학 창작까지 아우르는 복합 언어 모델에 대한 종합적인 평가인 [TouchStone](https://github.com/OFA-Sys/TouchStone) 평가를 출시합니다. 강력한 LLM을 심사위원으로 활용하고, 멀티모달 정보를 텍스트로 변환하여 평가합니다.\n* ```2023.8.31``` 🌟🌟🌟 Qwen-VL-Chat용 Int4 양자화 모델인 **Qwen-VL-Chat-Int4**를 출시하여 메모리 비용은 낮추고 추론 속도는 향상시켰습니다. 또한 벤치마크 평가에서도 성능 저하가 크지 않습니다.\n* ```2023.8.22``` 🎉🎉🎉 모델스코프와 허깅페이스에 **Qwen-VL**과 **Qwen-VL-Chat**을 모두 출시합니다. 학습 내용 및 모델 성능 등 모델에 대한 자세한 내용은 [논문](https://arxiv.org/abs/2308.12966)을 통해 확인할 수 있습니다.\n\n\n## Evaluation\n\n세 가지 관점에서 모델의 기능을 평가했습니다:\n\n1. **표준 벤치마크**: 멀티모달 작업의 네 가지 주요 범주에 대한 모델의 기본 작업 기능을 평가합니다:\n   \n   - 제로 샷 캡션: 보이지 않는 데이터 세트에 대한 모델의 제로샷 이미지 캡션 능력을 평가합니다.\n   - 일반 VQA: 판단, 색상, 숫자, 카테고리 등과 같은 사진의 일반적인 질문에 대한 답변 능력을 평가합니다.\n   - 텍스트 기반 VQA: 문서 QA, 차트 QA 등과 같이 사진 속 텍스트를 인식하는 모델의 능력을 평가합니다.\n   - 참조 표현 이해: 참조 표현식으로 설명된 이미지에서 대상 객체를 찾아내는 능력을 평가합니다.\n  \n2. **터치스톤**: 전반적인 텍스트-이미지 대화 능력과 사람과의 일치도를 평가하기 위해 [TouchStone](https://github.com/OFA-Sys/TouchStone)이라는 벤치마크를 구축했으며, 이 벤치마크는 GPT4로 채점하여 LVLM 모델을 평가합니다.\n   \n   - 터치스톤 벤치마크는 총 300개 이상의 이미지, 800개 이상의 질문, 27개 카테고리를 다룹니다. 속성 기반 Q&A, 유명인 인식, 시 쓰기, 여러 이미지 요약, 제품 비교, 수학 문제 풀이 등이 포함됩니다.\n   - 직접 이미지 입력이라는 현재 GPT4의 한계를 극복하기 위해 TouchStone은 사람이 직접 라벨을 지정하여 세분화된 이미지 주석을 제공합니다. 이러한 세부 주석은 문제 및 모델의 출력과 함께 채점을 위해 GPT4에 제공됩니다.\n   - 벤치마크에는 영어와 중국어 버전이 모두 포함되어 있습니다.\n  \n3. **기타 멀티모달 벤치마크**: 다른 멀티모달 벤치마크에서도 모델의 성능을 평가했습니다:\n\n   - 멀티모달 대규모 언어 모델에 대한 종합적인 평가 벤치마크인 [MME 벤치마크](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation). Qwen-VL-Chat은 지각과 인지 트랙 모두에서 SOTA를 달성했습니다.\n   - [Seed-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard)는 멀티모달 LLM을 평가하기 위한 정확한 인간 주석이 포함된 19K 객관식 질문으로 구성된 멀티모달 벤치마크입니다. 큐원 시리즈는 이 벤치마크에서 SOTA를 달성했습니다.\n\n평가 결과는 다음과 같습니다.\n\nQwen-VL은 여러 VL 작업에서 현재 SOTA 제너럴리스트 모델보다 성능이 뛰어나며, 기능 범위 측면에서 더 포괄적인 기능을 지원합니다.\n\n<p align=\"center\">\n    <img src=\"assets/radar.png\" width=\"600\"/>\n<p>\n\n### Zero-shot Captioning & General VQA\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"2\">Zero-shot Captioning</th>\n    <th colspan=\"5\">General VQA</th>\n  </tr>\n  <tr>\n    <th>NoCaps</th>\n    <th>Flickr30K</th>\n    <th>VQAv2<sup>dev</sup></th>\n    <th>OK-VQA</th>\n    <th>GQA</th>\n    <th>SciQA-Img<br>(0-shot)</th>\n    <th>VizWiz<br>(0-shot)</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"10\">Generalist<br>Models</td>\n    <td>Flamingo-9B</td>\n    <td>-</td>\n    <td>61.5</td>\n    <td>51.8</td>\n    <td>44.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>28.8</td>\n  </tr>\n  <tr>\n    <td>Flamingo-80B</td>\n    <td>-</td>\n    <td>67.2</td>\n    <td>56.3</td>\n    <td>50.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>31.6</td>\n  </tr>\n  <tr>\n    <td>Unified-IO-XL</td>\n    <td>100.0</td>\n    <td>-</td>\n    <td>77.9</td>\n    <td>54.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Kosmos-1</td>\n    <td>-</td>\n    <td>67.1</td>\n    <td>51.0</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>29.2</td>\n  </tr>\n  <tr>\n    <td>Kosmos-2</td>\n    <td>-</td>\n    <td>80.5</td>\n    <td>51.1</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>103.9</td>\n    <td>71.6</td>\n    <td>65.0</td>\n    <td>45.9</td>\n    <td>32.3</td>\n    <td>61.0</td>\n    <td>19.6</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td><strong>121.9</strong></td>\n    <td>82.8</td>\n    <td>-</td>\n    <td>-</td>\n    <td>49.5</td>\n    <td>63.1</td>\n    <td>33.4</td>\n  </tr>\n  <tr>\n    <td>Shikra (Vicuna-13B)</td>\n    <td>-</td>\n    <td>73.9</td>\n    <td>77.36</td>\n    <td>47.16</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td><strong>Qwen-VL (Qwen-7B)</strong></td>\n    <td>121.4</td>\n    <td><b>85.8</b></td>\n    <td><b>78.8</b></td>\n    <td><b>58.6</b></td>\n    <td><b>59.3</b></td>\n    <td>67.1</td>\n    <td>35.2</td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>63.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>39.1</td>\n  </tr> -->\n  <tr>\n    <td>Qwen-VL-Chat</td>\n    <td>120.2</td>\n    <td>81.0</td>\n    <td>78.2</td>\n    <td>56.6</td>\n    <td>57.5</td>\n    <td><b>68.2</b></td>\n    <td><b>38.9</b></td>\n  </tr>\n  <!-- <tr>\n    <td>Qwen-VL-Chat (4-shot)</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>60.6</td>\n    <td>-</td>\n    <td>-</td>\n    <td>44.45</td>\n  </tr> -->\n  <tr>\n    <td>Previous SOTA<br>(Per Task Fine-tuning)</td>\n    <td>-</td>\n    <td>127.0<br>(PALI-17B)</td>\n    <td>84.5<br>(InstructBLIP<br>-FlanT5-XL)</td>\n    <td>86.1<br>(PALI-X<br>-55B)</td>\n    <td>66.1<br>(PALI-X<br>-55B)</td>\n    <td>72.1<br>(CFR)</td>\n    <td>92.53<br>(LLaVa+<br>GPT-4)</td>\n    <td>70.9<br>(PALI-X<br>-55B)</td>\n  </tr>\n</tbody>\n</table>\n\n- 제로 샷 이미지 캡션의 경우, Qwen-VL은 Flickr30K에서 **SOTA**를 달성했고 InstructBlip을 사용하여 노캡스에서 경쟁력 있는 결과를 얻었습니다.\n- 일반 VQA의 경우, Qwen-VL은 동일한 일반 LVLM 스케일 설정에서 **SOTA**를 달성했습니다.\n\n\n### Text-oriented VQA (Focused on text understanding capabilities in images)\n\n<table>\n<thead>\n  <tr>\n    <th>Model type</th>\n    <th>Model</th>\n    <th>TextVQA</th>\n    <th>DocVQA</th>\n    <th>ChartQA</th>\n    <th>AI2D</th>\n    <th>OCR-VQA</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"5\">Generalist Models</td>\n    <td>BLIP-2 (Vicuna-13B)</td>\n    <td>42.4</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>InstructBLIP (Vicuna-13B)</td>\n    <td>50.7</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>mPLUG-DocOwl (LLaMA-7B)</td>\n    <td>52.6</td>\n    <td>62.2</td>\n    <td>57.4</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Pix2Struct-Large (1.3B)</td>\n    <td>-</td>\n    <td><b>76.6</b></td>\n    <td>58.6</td>\n    <td>42.1</td>\n    <td>71.3</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL (Qwen-7B)</td>\n    <td><b>63.8</b></td>\n    <td>65.1</td>\n    <td><b>65.7</b></td>\n    <td><b>62.3</b></td>\n    <td><b>75.7</b></td>\n  </tr>\n  <tr>\n    <td>Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>PALI-X-55B (Single-task FT)<br>(Without OCR Pipeline)</td>\n    <td>71.44</td>\n    <td>80.0</td>\n    <td>70.0</td>\n    <td>81.2</td>\n    <td>75.0</td>\n  </tr>\n</tbody>\n</table>\n\n- 텍스트 관련 인식/QA 평가에서 Qwen-VL은 일반적인 LVLM 스케일 설정에서 SOTA를 달성합니다.\n- 해상도는 위의 여러 평가에서 중요합니다. 224 해상도의 대부분의 오픈 소스 LVLM 모델은 이러한 평가를 수행할 수 없거나 이미지를 잘라내야만 해결할 수 있지만, Qwen-VL은 해상도를 448로 확장하여 엔드투엔드 평가가 가능합니다. Qwen-VL은 일부 작업에서 1024 해상도의 Pix2Struct-Large 모델보다 더 뛰어난 성능을 발휘합니다.\n\n### Referring Expression Comprehension\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Model type</th>\n    <th rowspan=\"2\">Model</th>\n    <th colspan=\"3\">RefCOCO</th>\n    <th colspan=\"3\">RefCOCO+</th>\n    <th colspan=\"2\">RefCOCOg</th>\n    <th>GRIT</th>\n  </tr>\n  <tr>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val</th>\n    <th>test-A</th>\n    <th>test-B</th>\n    <th>val-u</th>\n    <th>test-u</th>\n    <th>refexp</th>\n  </tr>\n</thead>\n<tbody align=\"center\">\n  <tr>\n    <td rowspan=\"8\">Generalist Models</td>\n    <td>GPV-2</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>51.50</td>\n  </tr>\n  <tr>\n    <td>OFA-L*</td>\n    <td>79.96</td>\n    <td>83.67</td>\n    <td>76.39</td>\n    <td>68.29</td>\n    <td>76.00</td>\n    <td>61.75</td>\n    <td>67.57</td>\n    <td>67.58</td>\n    <td>61.70</td>\n  </tr>\n  <tr>\n    <td>Unified-IO</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td><b>78.61</b></td>\n  </tr>\n  <tr>\n    <td>VisionLLM-H</td>\n    <td></td>\n    <td>86.70</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Shikra-7B</td>\n    <td>87.01</td>\n    <td>90.61</td>\n    <td>80.24 </td>\n    <td>81.60</td>\n    <td>87.36</td>\n    <td>72.12</td>\n    <td>82.27</td>\n    <td>82.19</td>\n    <td>69.34</td>\n  </tr>\n  <tr>\n    <td>Shikra-13B</td>\n    <td>87.83 </td>\n    <td>91.11</td>\n    <td>81.81</td>\n    <td>82.89</td>\n    <td>87.79</td>\n    <td>74.41</td>\n    <td>82.64</td>\n    <td>83.16</td>\n    <td>69.03</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B</td>\n    <td><b>89.36</b></td>\n    <td>92.26</td>\n    <td><b>85.34</b></td>\n    <td><b>83.12</b></td>\n    <td>88.25</td>\n    <td><b>77.21</b></td>\n    <td>85.58</td>\n    <td>85.48</td>\n    <td>78.22</td>\n  </tr>\n  <tr>\n    <td>Qwen-VL-7B-Chat</td>\n    <td>88.55</td>\n    <td><b>92.27</b></td>\n    <td>84.51</td>\n    <td>82.82</td>\n    <td><b>88.59</b></td>\n    <td>76.79</td>\n    <td><b>85.96</b></td>\n    <td><b>86.32</b></td>\n    <td>-</td>\n  <tr>\n    <td rowspan=\"3\">Specialist SOTAs<br>(Specialist/Finetuned)</td>\n    <td>G-DINO-L</td>\n    <td>90.56</td>\n    <td>93.19</td>\n    <td>88.24</td>\n    <td>82.75</td>\n    <td>88.95</td>\n    <td>75.92</td>\n    <td>86.13</td>\n    <td>87.02</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>UNINEXT-H</td>\n    <td>92.64 </td>\n    <td>94.33</td>\n    <td>91.46</td>\n    <td>85.24</td>\n    <td>89.63</td>\n    <td>79.79</td>\n    <td>88.73</td>\n    <td>89.37</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>ONE-PEACE</td>\n    <td>92.58 </td>\n    <td>94.18</td>\n    <td>89.26</td>\n    <td>88.77</td>\n    <td>92.21</td>\n    <td>83.23</td>\n    <td>89.22</td>\n    <td>89.27</td>\n    <td>-</td>\n  </tr>\n</tbody>\n</table>\n\n- Qwen-VL은 위의 모든 참조 표현 이해도 벤치마크에서 **SOTA**를 달성했습니다.\n- Qwen-VL은 중국어 자막 데이터에 대해 학습되지 않았지만, 중국어 자막 데이터와 영어 자막 데이터를 학습하여 제로 샷 방식으로 중국어 자막 작업에 일반화할 수 있습니다.\n\n\n실험 결과를 재현하기 위해 위의 모든 평가 스크립트를 제공합니다. 자세한 내용은 [eval_mm/EVALUATION.md](eval_mm/EVALUATION.md)를 참조하세요.\n\n### Chat evaluation\n\nTouchStone은 텍스트-이미지 대화 및 사람과의 일치 수준에 대한 LVLM 모델의 능력을 평가하기 위해 GPT4로 점수를 매기는 벤치마크입니다. 총 300개 이상의 이미지, 800개 이상의 질문, 속성 기반 Q&A, 유명인 인식, 시 쓰기, 여러 이미지 요약, 제품 비교, 수학 문제 풀이 등 27개 카테고리로 구성되어 있습니다. 자세한 내용은 [터치스톤/README.md](터치스톤/README.md)를 참조하세요.\n\n#### English evaluation\n\n| Model            | Score |\n| ---------------- | ----- |\n| PandaGPT         | 488.5 |\n| MiniGPT4         | 531.7 |\n| InstructBLIP     | 552.4 |\n| LLaMA-AdapterV2  | 590.1 |\n| LLaVA            | 602.7 |\n| mPLUG-Owl        | 605.4 |\n| Qwen-VL-Chat     | 645.2 |\n| Qwen-VL-Chat-1.1 | 711.6 |\n\n#### Chinese evaluation\n\n| Model            | Score |\n| ---------------- | ----- |\n| VisualGLM        | 247.1 |\n| Qwen-VL-Chat     | 401.2 |\n| Qwen-VL-Chat-1.1 | 481.7 |\n\nQwen-VL-Chat은 중국어와 영어 정렬 평가에서 모두 최고의 결과를 얻었습니다.\n\n### Other Benchmarks\n\n#### MME Benchmark\n\n[MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)는 멀티모달 대규모 언어 모델에 대한 종합적인 평가 벤치마크입니다. 존재, 수, 위치, 색상, 포스터, 유명인, 장면, 랜드마크, 예술품, OCR, 상식 추론, 숫자 계산, 텍스트 번역, 코드 추론 등 총 14개의 하위 과제에 대한 지각과 인지 능력을 모두 측정합니다.\n\nQwen-VL-Chat은 지각과 인지 평가 모두에서 SOTA를 달성했습니다. 자세한 내용은 [여기](eval_mm/mme/EVAL_MME.md)에서 확인하세요.\n\n<p align=\"center\">\n    <img src=\"eval_mm/mme/perception.jpg\" width=\"600\"/>\n<p>\n<p align=\"center\">\n    <img src=\"eval_mm/mme/cognition.jpg\" width=\"600\"/>\n<p>\n\n#### SEED-Bench\n\n[SEED-Bench](https://huggingface.co/spaces/AILab-CVC/SEED-Bench_Leaderboard)는 **이미지** 및 **동영상** 이해도를 포함한 12가지 평가 차원을 포괄하는 멀티모달 LLM을 평가하기 위한 정확한 사람의 주석이 포함된 19K 개의 객관식 문항으로 구성된 멀티모달 벤치마크입니다. 자세한 내용은 [여기](eval_mm/seed_bench/EVAL_SEED.md)에서 확인할 수 있습니다.\n\n이 벤치마크에서 Qwen-VL과 Qwen-VL-Chat은 SOTA를 달성했습니다.\n\n<p align=\"center\">\n    <img src=\"eval_mm/seed_bench/leaderboard.jpg\"/>\n<p>\n\n## Requirements\n\n* python 3.8 and above\n* pytorch 1.12 and above, 2.0 and above are recommended\n* CUDA 11.4 and above are recommended (this is for GPU users)\n  <br>\n\n## Quickstart\n\n아래에서는 🤖 모델스코프 및 🤗 트랜스포머와 함께 Qwen-VL 및 Qwen-VL-Chat을 사용하는 방법을 보여주는 간단한 예제를 제공합니다.\n\n코드를 실행하기 전에 환경을 설정하고 필요한 패키지를 설치했는지 확인하세요. 위의 요구 사항을 충족하는지 확인한 다음 종속 라이브러리를 설치하세요.\n```bash\npip install -r requirements.txt\n```\n\n이제 모델스코프 또는 트랜스포머로 시작할 수 있습니다. 비전 인코더에 대한 자세한 사용법은 [튜토리얼](TUTORIAL.md)을 참조하세요.\n\n#### 🤗 Transformers\n\n추론에 Qwen-VL-Chat을 사용하려면 아래에 설명된 대로 몇 줄의 코드를 입력하기만 하면 됩니다. 단, **최신 코드를 사용하고 있는지 확인하세요**.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use cuda device\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\n# 1st dialogue turn\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url\n    {'text': '这是什么?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n# 图中是一名女子在沙滩上和狗玩耍，旁边是一只拉布拉多犬，它们处于沙滩上。\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, '框出图中击掌的位置', history=history)\nprint(response)\n# <ref>击掌</ref><box>(536,509),(588,602)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('1.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n\n<details>\n  <summary>Running Qwen-VL</summary>\n\nQwen-VL pretrained base model을 실행하는 것도 매우 간단합니다.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\nimport torch\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cpu\", trust_remote_code=True).eval()\n# use cuda device\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cuda\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation (No need to do this if you are using transformers>4.32.0)\n# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n\nquery = tokenizer.from_list_format([\n    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}, # Either a local path or an url\n    {'text': 'Generate the caption in English with grounding:'},\n])\ninputs = tokenizer(query, return_tensors='pt')\ninputs = inputs.to(model.device)\npred = model.generate(**inputs)\nresponse = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)\nprint(response)\n# <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>Generate the caption in English with grounding:<ref> Woman</ref><box>(451,379),(731,806)</box> and<ref> her dog</ref><box>(219,424),(576,896)</box> playing on the beach<|endoftext|>\nimage = tokenizer.draw_bbox_on_latest_picture(response)\nif image:\n  image.save('2.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_spotting_caption.jpg\" width=\"500\"/>\n<p>\n\n</details>\n\n\nHuggingFace에서 모델 체크포인트와 코드를 다운로드하는 동안 네트워크 문제가 발생하는 경우, 아래에 설명된 대로 모델스코프에서 체크포인트를 먼저 가져온 다음 로컬 디렉터리에서 로드하는 방법을 사용할 수 있습니다.\n\n```python\nfrom modelscope import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Downloading model checkpoint to a local dir model_dir\n# model_dir = snapshot_download('qwen/Qwen-VL')\nmodel_dir = snapshot_download('qwen/Qwen-VL-Chat')\n\n\n# Loading local checkpoints\n# trust_remote_code is still set as True since we still load codes from local dir instead of transformers\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_dir,\n    device_map=\"cuda\",\n    trust_remote_code=True\n).eval()\n```\n\n#### 🤖 ModelScope\n\nModelScope는 서비스형 모델(MaaS)을 위한 오픈소스 플랫폼으로, AI 개발자에게 유연하고 비용 효율적인 모델 서비스를 제공합니다. 마찬가지로 아래와 같이 ModelScope로 모델을 실행할 수 있습니다.\n\n```python\nfrom modelscope import (\n    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n)\nimport torch\nmodel_id = 'qwen/Qwen-VL-Chat'\nrevision = 'v1.0.0'\n\nmodel_dir = snapshot_download(model_id, revision=revision)\ntorch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\nif not hasattr(tokenizer, 'model_dir'):\n    tokenizer.model_dir = model_dir\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n# use fp16\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n# use cpu\n# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cpu\", trust_remote_code=True).eval()\n# use auto\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation (No need to do this if you are using transformers>=4.32.0)\n# model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)\n\n# 1st dialogue turn\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>这是什么', history=None)\nprint(response)\n# 图中是一名年轻女子在沙滩上和她的狗玩耍，狗的品种是拉布拉多。她们坐在沙滩上，狗的前腿抬起来，与人互动。\n\n# 2nd dialogue turn\nresponse, history = model.chat(tokenizer, '输出击掌的检测框', history=history)\nprint(response)\n# <ref>\"击掌\"</ref><box>(211,412),(577,891)</box>\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image:\n  image.save('output_chat.jpg')\nelse:\n  print(\"no box\")\n```\n\n<p align=\"center\">\n    <img src=\"assets/demo_highfive.jpg\" width=\"500\"/>\n<p>\n<br>\n\n## Quantization\n\n### Usage\n[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)를 기반으로 하는 새로운 솔루션을 제공하고, 거의 무손실 모델 효과를 달성하면서도 메모리 비용과 추론 속도 모두에서 성능이 향상된 Qwen-VL-Chat용 Int4 양자화 모델인 [Qwen-VL-Chat-Int4](https://huggingface.co/Qwen/Qwen-VL-Chat-Int4)를 출시했습니다.\n\n여기에서는 제공된 양자화된 모델을 추론에 사용하는 방법을 보여줍니다. 시작하기 전에 요구 사항(예: torch 2.0 이상, transformers 4.32.0 이상 등) 및 필요한 패키지를 제대로 설치했는지 확인하세요.\n\n```bash\npip install optimum\ngit clone https://github.com/JustinLin610/AutoGPTQ.git & cd AutoGPTQ\npip install -v .\n```\n\n만약 'auto-gptq' 설치에 문제가 있다면, 공식 [repo](https://github.com/PanQiWei/AutoGPTQ)에서 휠을 찾아보시길 권장합니다.\n\n그러면 정량화된 모델을 쉽게 로드하고 평소와 동일하게 추론을 실행할 수 있습니다.\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen-VL-Chat-Int4\",\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n# Either a local path or an url between <img></img> tags.\nimage_path = 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'\nresponse, history = model.chat(tokenizer, query=f'<img>{image_path}</img>这是什么', history=None)\nprint(response)\n```\n\n### Performance\n\n [TouchStone](https://github.com/OFA-Sys/TouchStone)벤치마크에서 BF16 및 Int4 모델의 모델 성능을 살펴본 결과, 양자화된 모델에서 성능 저하가 크지 않은 것으로 나타났습니다. 결과는 아래와 같습니다.\n\n| Quantization | ZH         | EN            |\n| ------------ | :--------: | :-----------: | \n| BF16         | 401.2      |    645.2      |\n| Int4         | 386.6      |    651.4      |\n\n### Inference Speed\n\n이미지의 컨텍스트(258개의 토큰이 필요한)를 가지고 각각 1792개(2048-258개), 7934개(8192-258개)의 토큰을 생성하는 평균 추론 속도(토큰/초)를 BF16 정밀도와 Int4 양자화 하에서 측정했습니다.\n\n| Quantization | Speed (2048 tokens) | Speed (8192 tokens) |\n| ------------ | :-----------------: | :-----------------: |\n| BF16         |        28.87        |        24.32        |\n| Int4         |        37.79        |        34.34        |\n\n프로파일링은 PyTorch 2.0.1 및 CUDA 11.4가 탑재된 단일 A100-SXM4-80G GPU에서 실행됩니다.\n\n### GPU Memory Usage\n\n또한 1792개(2048-258개)의 토큰(이미지 포함)을 컨텍스트로 인코딩하고 단일 토큰을 생성할 때와 7934개(8192-258개)의 토큰(이미지가 컨텍스트로 포함)을 생성할 때 각각 BF16 또는 Int4 양자화 수준에서 최대 GPU 메모리 사용량을 프로파일링했습니다. 결과는 아래와 같습니다.\n\n| Quantization | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |\n| ------------ | :---------------------------------: | :-----------------------------------: |\n| BF16         |               22.60GB               |                28.01GB                |\n| Int4         |               11.82GB               |                17.23GB                |\n\n위의 속도 및 메모리 프로파일링은 [이 스크립트](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile_mm.py)를 사용하여 수행되었습니다.\n<br>\n\n## Finetuning\n\n이제 사용자가 다운스트림 애플리케이션을 위해 사전 학습된 모델을 간단한 방식으로 미세 조정할 수 있도록 공식 학습 스크립트인 `finetune.py`를 제공합니다. 또한, 걱정 없이 미세 조정을 시작할 수 있는 셸 스크립트도 제공합니다. 이 스크립트는 딥스피드와 FSDP를 통한 학습을 지원합니다. 제공되는 셸 스크립트는 DeepSpeed를 사용하므로 시작하기 전에 DeepSpeed를 설치하는 것이 좋습니다.\n\n```bash\npip install deepspeed\n```\n\n### Data preparation\n학습 데이터를 준비하려면 모든 샘플을 목록에 넣고 json 파일에 저장해야 합니다. 각 샘플은 ID와 대화 목록으로 구성된 사전입니다. 아래는 샘플 1개가 포함된 간단한 예제 목록입니다.\n```json\n[\n  {\n    \"id\": \"identity_0\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"你好\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"我是Qwen-VL,一个支持视觉输入的大模型。\"\n      }\n    ]\n  },\n  {\n    \"id\": \"identity_1\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>\\n图中的狗是什么品种？\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"图中是一只拉布拉多犬。\"\n      },\n      {\n        \"from\": \"user\",\n        \"value\": \"框出图中的格子衬衫\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"<ref>格子衬衫</ref><box>(588,499),(725,789)</box>\"\n      }\n    ]\n  },\n  { \n    \"id\": \"identity_2\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"Picture 1: <img>assets/mm_tutorial/Chongqing.jpeg</img>\\nPicture 2: <img>assets/mm_tutorial/Beijing.jpeg</img>\\n图中都是哪\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"第一张图片是重庆的城市天际线，第二张图片是北京的天际线。\"\n      }\n    ]\n  }\n]\n```\nVL 작업에서는 `<img> </img> <ref> </ref> <box> </box>`등과 같은 특수 토큰이 사용됩니다. \n\n이미지는 `Picture id: <img>img_path</img>\\n{your prompt}`로 표시되며, 여기서 `id`는 대화에서 이미지의 위치(1부터 시작)를 나타냅니다. `img_path`는 로컬 파일 경로 또는 웹 링크일 수 있습니다.\n\n박스의 좌표는 `<box>(x1,y1),(x2,y2)</box>`로 표시되는데, 여기에서 `(x1, y1)`과 `(x2, y2)`의 좌표는 `[0, 1000)`으로 정규화되게 됩니다. 해당 텍스트 설명은 `<ref>text_caption</ref>`과 같은 방법으로 식별할 수 있습니다.\n\n데이터 준비 후 제공된 셸 스크립트를 사용하여 미세 조정을 실행할 수 있습니다. 데이터 파일 경로인 `$DATA`를 지정하는 것을 잊지 마세요.\n\n미세 조정 스크립트를 통해 다음을 수행할 수 있습니다.\n- Full-parameter finetuning\n- LoRA\n- Q-LoRA\n\n### Full-parameter finetuning\n전체 파라미터를 미세 조정하려면 전체 훈련 과정에서 LLM의 모든 파라미터를 업데이트해야 합니다. 실험 결과, 미세 조정 단계에서 **ViT의 파라미터를 동결(frozening)하면 더 나은 성능을 얻을 수 있었습니다.** 훈련을 시작하려면 다음 스크립트를 실행합니다.\n\n```bash\nsh finetune/finetune_ds.sh\n```\n\n셸 스크립트에서 올바른 모델 이름 또는 경로, 데이터 경로, 출력 디렉터리를 지정하는 것을 잊지 마세요. 변경하려면 `--deepspeed` 인수를 제거하거나 요구 사항에 따라 DeepSpeed 구성 json 파일을 변경하면 됩니다. 또한, 이 스크립트는 혼합 정밀도 훈련을 지원하므로 `--bf16 True` 또는 `--fp16 True`를 사용할 수 있습니다. 경험적으로 머신이 bf16을 지원하는 경우 사전 훈련 및 정렬과 일관된 훈련을 위해 bf16을 사용하는 것이 좋으며, 따라서 기본값으로 사용됩니다.\n\n### LoRA\n마찬가지로 LoRA를 실행하려면 아래와 같이 다른 스크립트를 사용하여 실행합니다. 시작하기 전에 `peft`를 설치했는지 확인하세요. 또한 모델, 데이터, 출력에 대한 경로를 지정해야 합니다. 사전 학습된 모델에는 절대 경로를 사용하는 것이 좋습니다. LoRA는 어댑터만 저장하고 어댑터 구성 json 파일의 절대 경로는 로드할 사전 학습된 모델을 찾는 데 사용되기 때문입니다.\n\n```bash\n# Single GPU training\nsh finetune/finetune_lora_single_gpu.sh\n# Distributed training\nsh finetune/finetune_lora_ds.sh\n```\n\n전체 매개변수 미세 조정과 비교할 때 LoRA([paper](https://arxiv.org/abs/2106.09685))는 어댑터 레이어의 매개변수만 업데이트하고 원래의 대규모 언어 모델 레이어는 고정된 상태로 유지합니다. 따라서 메모리 비용이 훨씬 적게 들고 계산 비용도 적게 듭니다. \n\nLoRA를 사용하여 채팅 모델 대신 기본 언어 모델(예: Qwen-VL)을 미세 조정하는 경우, 스크립트는 임베딩 및 출력 레이어를 학습 가능한 파라미터로 자동 전환합니다. 이는 기본 언어 모델에 ChatML 형식에서 가져온 특수 토큰에 대한 지식이 없기 때문입니다. 따라서 모델이 토큰을 이해하고 예측하려면 이러한 레이어를 업데이트해야 합니다. 다시 말해, 학습이 LoRA에서 특수 토큰을 가져오는 경우 코드 내에서 `modules_to_save`를 설정하여 레이어를 학습 가능한 파라미터로 설정해야 합니다. 또한 이러한 트레이닝 가능한 파라미터가 있는 경우와 없는 경우 LoRA의 메모리 사용량에는 상당한 차이가 있음을 발견했습니다. 따라서 메모리에 문제가 있는 경우 LoRA에서 채팅 모델을 미세 조정하는 것이 좋습니다. 자세한 내용은 아래 프로필을 확인하세요.\n\n### Q-LoRA\n그러나 여전히 메모리가 부족하다면 양자화된 대규모 언어 모델과 페이징 주의와 같은 기타 기술을 사용하여 메모리 비용을 훨씬 더 적게 사용할 수 있는 Q-LoRA([paper](https://arxiv.org/abs/2305.14314))를 고려해 볼 수 있습니다. Q-LoRA를 실행하려면 다음 스크립트를 직접 실행하세요.\n\n```bash\n# Single GPU training\nsh finetune/finetune_qlora_single_gpu.sh\n# Distributed training\nsh finetune/finetune_qlora_ds.sh\n```\n\nQ-LoRA의 경우, 당사에서 제공하는 정량화된 모델(예: Qwen-VL-Chat-Int4)을 로드하는 것이 좋습니다. \nbf16 모델을 사용해서는 안 됩니다. 전체 파라미터 미세 조정 및 LoRA와 달리 Q-LoRA에는 fp16만 지원됩니다. 또한 Q-LoRA의 경우 LoRA의 특수 토큰에 대한 문제가 여전히 존재합니다. 하지만 저희는 채팅 모델에 Int4 모델만 제공하기 때문에 언어 모델이 ChatML 형식의 특수 토큰을 학습했기 때문에 레이어에 대한 걱정은 하지 않으셔도 됩니다. 단, Int4 모델의 레이어는 학습할 수 없어야 하므로 학습에 특수 토큰을 도입하면 Q-LoRA가 작동하지 않을 수 있습니다.\n\n전체 매개변수 미세 조정과 달리 LoRA 및 Q-LoRA의 훈련은 어댑터 매개변수만 저장합니다. 아래와 같이 추론을 위해 미세 조정된 모델을 로드할 수 있습니다:\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n```\n\n어댑터를 병합하고 미세 조정된 모델을 독립형 모델로 저장하려면(이 작업은 LoRA에서만 가능하며 Q-LoRA에서 파라미터를 병합할 수 없음) 다음 코드를 실행하면 됩니다.\n\n```python\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    path_to_adapter, # path to the output directory\n    device_map=\"auto\",\n    trust_remote_code=True\n).eval()\n\nmerged_model = model.merge_and_unload()\n# max_shard_size와 안전한 직렬화는 필요하지 않습니다. \n# 이들은 각각 샤딩 체크포인트에 대해 작동하고 모델을 세이프텐서에 저장합니다.\nmerged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n\n\n```\n\n참고: 멀티 GPU 트레이닝의 경우, 머신에 따라 분산 트레이닝에 적합한 하이퍼파라미터를 지정해야 합니다. 또한 데이터, 메모리 사용량, 훈련 속도 등을 고려하여 --model_max_length 인수를 사용하여 최대 시퀀스 길이를 지정하는 것이 좋습니다.\n\n\n### Profiling of Memory and Speed\n단일 GPU 트레이닝 설정에서 임베딩 및 출력 레이어를 트레이닝하는 LoRA(Base)와 임베딩 및 출력 레이어를 트레이닝할 수 없는 LoRA(Chat)의 GPU 메모리 및 트레이닝 속도를 프로파일링합니다. 이 테스트에서는 단일 A100-SXM4-80G GPU에서 실험했으며, CUDA 11.8과 Python 2.0을 사용했습니다. 배치 크기는 1, 그라데이션 누적은 8을 균일하게 사용합니다. 각 샘플에는 이미지가 포함됩니다. 384, 512, 1024, 2048 등 다양한 길이의 입력에 대한 메모리(GB)와 속도(s/iter)를 프로파일링합니다. 통계는 아래와 같습니다.\n\n\n<table>\n    <tr>\n <th rowspan=\"2\">Method</th><th colspan=\"4\" align=\"center\">Sequence Length</th>\n    </tr>\n    <tr>\n        <th align=\"center\">384</th><th align=\"center\">512</th><th align=\"center\">1024</th><th align=\"center\">2048</th>\n    </tr>\n    <tr>\n      <td>LoRA (Base)</td><td align=\"center\">37.1G / 2.3s/it</td><td align=\"center\">37.3G / 2.4s/it</td><td align=\"center\">38.7G / 3.6s/it</td><td align=\"center\">38.7G / 6.1s/it</td>\n    </tr>\n    <tr>\n      <td>LoRA (Chat)</td><td align=\"center\">23.3G / 2.2s/it</td><td align=\"center\">23.6G / 2.3s/it</td><td align=\"center\">25.1G / 3.5s/it</td><td align=\"center\">27.3G / 5.9s/it</td>\n    </tr>\n    <tr>\n        <td>Q-LoRA</td><td align=\"center\">17.0G / 4.2s/it</td><td align=\"center\">17.2G / 4.5s/it</td><td align=\"center\">18.2G / 5.5s/it</td><td align=\"center\">19.3G / 7.9s/it</td>\n    </tr>\n\n</table>\n\n<br>\n\n## Demo\n\n### Web UI\n\n사용자가 웹 UI 데모를 빌드할 수 있는 코드를 제공합니다. 시작하기 전에 다음 패키지를 설치해야 합니다.\n\n```\npip install -r requirements_web_demo.txt\n```\n\nThen run the command below and click on the generated link:\n\n```\npython web_demo_mm.py\n```\n\n<br>\n\n## FAQ\n\n문제가 발생하면 새 이슈를 시작하기 전에 먼저 [자주 묻는 질문](FAQ.md)과 이슈를 참조하여 해결 방법을 찾아보시기 바랍니다.\n<br>\n\n## License Agreement\n\n연구자와 개발자는 Qwen-VL과 Qwen-VL-Chat의 코드와 모델 가중치를 자유롭게 사용할 수 있습니다. 또한 상업적 사용도 허용됩니다. 자세한 내용은 [LICENSE](라이센스)에서 라이센스를 확인하세요.\n<br>\n\n## Citation\n\n저희 논문과 코드가 여러분의 연구에 도움이 되었다면 star:star: 와 인용:pencil: 해주시면 감사드리겠습니다. :)\n\n```BibTeX\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n\n<br>\n\n## Contact Us\n\n연구팀이나 제품팀에 메시지를 남기고 싶으시면 언제든지 이메일(qianwen_opensource@alibabacloud.com)을 보내주세요."
        },
        {
          "name": "TUTORIAL.md",
          "type": "blob",
          "size": 13.0771484375,
          "content": "# Qwen-VL-Chat Tutorial\nQwen-VL-Chat is a generalist multimodal large-scale language model, and it can perform a wide range of vision-language tasks. In this tutorial, we will give some concise examples to demonstrate the capabilities of Qwen-VL-Chat in **Visual Question Answering, Text Understanding, Mathematical Reasoning with Diagrams, Multi-Figure Reasoning, and Grounding**. Please note that the examples shown are far from the limit of Qwen-VL-Chat's capabilities, **you can further explore Qwen-VL-Chat's capabilities by changing the input images and prompts!**\n\n## Initializing the Qwen-VL-Chat model\nBefore you can use Qwen-VL-Chat, you first need to initialize Qwen-VL-Chat's tokenizer and Qwen-VL-Chat's model:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# If you expect the results to be reproducible, set a random seed.\n# torch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n```\nAfter executing the above code, ```tokenizer``` will correspond to the classifier used by Qwen-VL-Chat, while ```model``` will correspond to the model of Qwen-VL-Chat. The ```tokenizer``` is used for preprocessing the interleaved multimodal inputs, while the ```model``` is the Qwen-VL-Chat model itself.\n\n## Using Qwen-VL-Chat\n### **Multi-round visual question answering**\n#### **The first question**\nLet's get started with a simple example. As shown below, the file ```assets/mm_tutorial/Rebecca_(1939_poster).jpeg``` is a poster for the 1940 film Rebecca.\n\n![](assets/mm_tutorial/Rebecca_(1939_poster)_Small.jpeg)\n\nLet's ask what is the name of the film on the Qwen-VL-Chat poster. First of all, we use ```tokenizer.from_list_format``` which can preprocess and tokenize the input:\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Rebecca_(1939_poster).jpeg'},\n    {'text': 'What is the name of the movie in the poster?'},\n])\n```\nNext, we can use ```model.chat``` to ask questions to the Qwen-VL-Chat model and get its response. Note that for the first question, the dialogue history is empty, so we use ```history=None```.\n```python\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\nYou are expected to get an output similar to the following:\n\n> The name of the movie in the poster is \"Rebecca.\"\n\nThis shows that the model correctly answered the given question! According to the poster, the title of the film is \n indeed **Rebecca**.\n\n#### **Multi-round question answering**\nWe can also continue to ask the model other questions, such as who is the director of the film. The dialogue history is not empty for subsequent questions, therefore we use ```history=history``` to pass the history of previous conversations to ``model.chat``:\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Who directed this movie?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\nYou are expected to get an output similar to the following:\n\n> The movie \"Rebecca\" was directed by Alfred Hitchcock.\n\nAgain, the model answered the given question correctly! According to the poster, the director of the film is Alfred Hitchcock。\n\n### **Text Understanding**\nQwen-VL-Chat also has the ability to understand images containing dense text. As shown below, the file ```assets/mm_tutorial/Hospital.jpeg``` is a hospital signage containing dense text.\n\n![](assets/mm_tutorial/Hospital_Small.jpg)\n\nWe can ask questions about the location of different departments in the Hospital. Since the dialogue history is empty, so we use ```history=None```.\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Hospital.jpg'},\n    {'text': 'Based on the photo, which floor is the Department of Otorhinolaryngology on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\nYou are expected to get an output similar to the following:\n\n> The Department of Otorhinolaryngology is located on the 4th floor.\n\nYou can also ask further questions. In this case you need to use ```history=history``` to pass a history of previous conversations to ```model.chat```. \n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Based on the photo, which floor is the Department of Surgery on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\nYou are expected to get an output similar to the following:\n\n> The Department of Surgery is located on the 3rd floor.\n\n### **Mathematical Reasoning with Diagram**\nUsing the model's diagram comprehension and mathematical reasoning capabilities, Qwen-VL-Chat can also perform some more complex tasks! As shown below, the file ```assets/mm_tutorial/Menu.jpeg``` is the menu of a restaurant. Now we want to know how much it would cost to purchase two Salmon Burgers and three Meat Lover's Pizzas.\n\n![](assets/mm_tutorial/Menu.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Menu.jpeg'},\n    {'text': 'How much would I pay if I want to order two Salmon Burger and three Meat Lover\\'s Pizza? Think carefully step by step.'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n```Think carefully step by step.``` is a common prompt that guides the model through complex tasks step by step. So if you have a complex task to complete, try using it to improve the accuracy of the model. You are expected to get an output similar to the following:\n\n> To order two Salmon Burgers and three Meat Lover's Pizzas, you would need to pay the following:\n> \n> 1. For two Salmon Burgers: x2 Salmon Burgers at $10 each = $20\n> 2. For three Meat Lover's Pizzas: x3 Meat Lover's Pizzas at $12 each = $36\n> \n> Therefore, the total cost would be $56.\n\n### **Multi-Figure Reasoning and Chinese Input**\nIn the previous examples, we have demonstrated Qwen-VL-Chat's question-answering capability for a single image and English questions. However, Qwen-VL-Chat is actually a multilingual model that supports Chinese input and multiple images! In the following example, we let Qwen-VL-Chat compare the photos of two cities (Chongqing and Beijing) for us (```assets/mm_tutorial/Chongqing.jpeg``` and ```assets/mm_tutorial/Beijing.jpeg```) in Chinese:\n\n![](assets/mm_tutorial/Chongqing_Small.jpeg)\n\n![](assets/mm_tutorial/Beijing_Small.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Chongqing.jpeg'},\n    {'image': 'assets/mm_tutorial/Beijing.jpeg'},\n    {'text': '上面两张图片分别是哪两个城市？请对它们进行对比。'},\n])\ntorch.manual_seed(5678)\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\nYou are expected to get an output similar to the following:\n\n> 第一张图片是重庆的城市天际线，它反映了现代都市的繁华与喧嚣。第二张图片是北京的天际线，它象征着中国首都的现代化和国际化。两座城市都是中国的重要城市，拥有独特的文化和发展历史。\n\n**Please note that comparing cities is a fairly subjective question, so the responses generated by the model may be subject to a high degree of randomness. If you do not set the random seed using ```torch.manual_seed(5678)```, the output will be different each time. Even if you set the random seed, the results obtained may still differ from this tutorial due to differences in hardware and software environments.**\n\n### **Grounding Capability**\nIn the last section of the tutorial, we demonstrate the ability of the Qwen-VL-Chat model to produce a bounding box. Qwen-VL-Chat can frame a specified area of an image with a rectangular box according to your language description. This may be a bit abstract, so let's look at the following example. As shown below, the file ```assets/mm_tutorial/Shanghai.jpg``` is a photo of Shanghai, and we'll start by asking the model to describe the image with a regular prompt.\n\n![](assets/mm_tutorial/Shanghai_Small.jpeg)\n\n```python\ntorch.manual_seed(1234)\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Shanghai.jpg'},\n    {'text': '图里有啥'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\nYou are expected to get an output similar to the following:\n\n> 图中是中国上海的天际线，包括了上海塔、金茂大厦、上海环球金融中心、海洋大厦等著名建筑。\n\nNext, let's talk to the model by using the prompt ```请给我框出图中上海环球金融中心和东方明珠``` and see what happens. Note that at this point you need to pass the history of previous conversations to ```model.chat``` using ```history=history```.\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': '请给我框出图中上海环球金融中心和东方明珠'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\nYou are expected to get an output similar to the following:\n```xml\n<ref>上海环球金融中心</ref><box>(667,437),(760,874)</box>和<ref>东方明珠</ref><box>(506,75),(582,946)</box>\n```\nThe Qwen-VL-Chat model doesn't have a hand, but it doesn't reject your request either. Instead, it outputs something \"strange\" - In fact, the output of the model gives the location of the 上海环球金融中心（Shanghai World Financial Centre） and the 东方明珠（Oriental Pearl Tower） in markup language. You can visualise it using the following code:\n```python\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nimage.save('Shanghai_Output.jpg')\n```\nThe saved ```Shanghai_Output.jpg``` will look similar to the screenshot below: \n\n![](assets/mm_tutorial/Shanghai_Output_Small.jpeg)\n\nAfter that, you can still chat with Qwen-VL-Chat as before:\n```python\nquery = tokenizer.from_list_format([\n    {'text': '帮我写个这座城市的旅游计划'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\nYou are expected to get an output similar to the following:\n\n> 好的，以下是一个简单的上海旅游计划：\n>\n> 第一天：\n> 上午：抵达上海，前往酒店办理入住手续。\n> 中午：享用午餐后，前往外滩，欣赏黄浦江畔的美景，游览上海地标性建筑如浦发银行大楼、汇丰银行大楼等。\n> 下午：游览南京路步行街，购买特色礼品或品尝当地美食。\n> 晚上：在南京路附近的餐厅享用晚餐，然后去看上海的夜景。\n>\n> 第二天：\n> 上午：前往上海科技馆，了解科技发展历史，观看各种科技展览。\n> 中午：在科技馆附近的餐厅享用午餐。\n> 下午：游览世纪公园，欣赏美景并放松身心。\n> 晚上：在南京路或附近的陆家嘴地区享用晚餐，然后去看上海的夜景。\n>\n> 第三天：\n> 上午：游览上海迪士尼乐园或上海海昌海洋公园，与各种迪士尼角色互动，或者在海洋公园观看海洋生物表演。\n> 中午：在迪士尼乐园或海洋公园附近的餐厅享用午餐。\n> 下午：自由活动，可以去购物、品尝当地美食或者去博物馆等。\n> 晚上：在酒店附近享用晚餐，然后离开上海。\n>\n> 当然，以上只是一个简单的计划，上海有许多其他景点和活动，例如参观上海博物馆、游览田子坊、观看上海话剧等。具体计划可以根据个人兴趣和时间进行调整。\n\n\n**Please note that travel planning is a fairly subjective question, so the responses generated by the model may be subject to a high degree of randomness. If you do not set the random seed using ```torch.manual_seed(1234)```, the output will be different each time. Even if you set the random seed, the results obtained may still differ from this tutorial due to differences in hardware and software environments.**\n\n### Grounded Captioning\nQwen-VL can output the bounding box information of the subject while captioning the image. For example:\n\n```\nimg_url = 'assets/apple.jpeg'\nquery = tokenizer.from_list_format([\n    {'image': img_url},\n    {'text': 'Generate the caption in English with grounding:'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image is not None:\n    image.save('apple.jpg')\n```\n\nThe saved ```apple.jpg``` will look similar to the screenshot below: \n<p align=\"left\">\n    <img src=\"assets/apple_r.jpeg\" width=\"600\"/>\n<p>\n\n#### How to get the caption without any box-like annotations\nSometimes you may expect no box-like annotations in the response. In the case, you can stably get the cleaned text by the following post-processing.\n\n```\n# response = '<ref> Two apples</ref><box>(302,257),(582,671)</box><box>(603,252),(878,642)</box> and<ref> a bowl</ref><box>(2,269),(304,674)</box>'\nimport re\nclean_response = re.sub(r'<ref>(.*?)</ref>(?:<box>.*?</box>)*(?:<quad>.*?</quad>)*', r'\\1', response).strip()\nprint(clean_response)\n# clean_response = 'Two apples and a bowl'\n```\n"
        },
        {
          "name": "TUTORIAL_ja.md",
          "type": "blob",
          "size": 12.8935546875,
          "content": "# Qwen-VL-Chat チュートリアル\nQwen-VL-Chat は汎用のマルチモーダル大規模言語モデルであり、幅広い視覚言語タスクを実行できます。このチュートリアルでは、Qwen-VL-Chat の**視覚的質問応答、テキスト理解、図を用いた数学的推論、多視点推論、およびグラウンディング**の機能について、いくつかの簡潔な例を挙げて説明します。Qwen-VL-Chat は、入力画像やプロンプトを変更することで、Qwen-VL-Chat の能力をさらに引き出すことができます。\n\n## Qwen-VL-Chat モデルの初期化\nQwen-VL-Chat を使用する前に、まず Qwen-VL-Chat のトークナイザと Qwen-VL-Chat のモデルを初期化する必要があります:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# 結果の再現性を期待する場合は、ランダムシードを設定する。\n# torch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n```\n上記のコードを実行すると、```tokenizer``` は Qwen-VL-Chat で使用される分類器に対応し、```model``` は Qwen-VL-Chat のモデルに対応します。```tokenizer``` はインターリーブされたマルチモーダル入力の前処理に使用され、```model``` は Qwen-VL-Chat のモデルそのものです。\n\n## Qwen-VL-Chat を使う\n### **複数ラウンドのビジュアル質問回答**\n#### **最初の質問**\n簡単な例から始めましょう。以下に示すように、```assets/mm_tutorial/Rebecca_(1939_poster).jpeg``` は 1940 年の映画 レベッカのポスターです。\n\n![](assets/mm_tutorial/Rebecca_(1939_poster)_Small.jpeg)\n\nQwen-VL-Chat のポスターに描かれている映画の名前を聞いてみよう。まず初めに、入力を前処理してトークン化する ```tokenizer.from_list_format``` を使用します:\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Rebecca_(1939_poster).jpeg'},\n    {'text': 'What is the name of the movie in the poster?'},\n])\n```\n次に、```model.chat``` を使って Qwen-VL-Chat モデルに質問をし、その回答を得ることができます。最初の質問では、ダイアログの履歴は空なので、```history=None``` を使用することに注意してください。\n```python\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n以下のような出力が期待されます:\n\n> The name of the movie in the poster is \"Rebecca.\"\n\nこれは、モデルが与えられた問題に正しく答えたことを示しています！ポスターをみると、映画のタイトルは確かに**レベッカ**です。\n\n#### **複数ラウンドの質問回答**\nまた、映画の監督は誰かなど、他の質問をモデルに続けることもできます。そのため、```history=history``` を使って、以前の会話の履歴を ``model.chat`` に渡します:\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Who directed this movie?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\n以下のような出力が期待されます:\n\n> The movie \"Rebecca\" was directed by Alfred Hitchcock.\n\n再びこのモデルは与えられた問題に正解しました！ポスターによると、この映画の監督はアルフレッド・ヒッチコックです。\n\n### **テキスト理解**\nQwen-VL-Chat には、高密度なテキストを含む画像を理解する機能もあります。下図に示すように、```assets/mm_tutorial/Hospital.jpeg``` というファイルは、濃いテキストを含む病院の看板です。\n\n![](assets/mm_tutorial/Hospital_Small.jpg)\n\n病院内のさまざまな診療科の場所について質問することができます。対話の履歴は空なので、```history=None``` を使用します。\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Hospital.jpg'},\n    {'text': 'Based on the photo, which floor is the Department of Otorhinolaryngology on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n以下のような出力が期待されます:\n\n> The Department of Otorhinolaryngology is located on the 4th floor.\n\nさらに質問をすることもできます。この場合、```history=history``` を使用して、以前の会話の履歴を ```model.chat``` に渡す必要があります。\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Based on the photo, which floor is the Department of Surgery on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\n以下のような出力が期待されます:\n\n> The Department of Surgery is located on the 3rd floor.\n\n### **ダイアグラムによる数学的推論**\nQwen-VL-Chat は、このモデルのダイアグラム理解能力と数学的推論能力を使って、より複雑なタスクを実行することもできます！下に示すように、```assets/mm_tutorial/Menu.jpeg``` というファイルはレストランのメニューです。では、Salmon Burger 2 個と Meat Lover's Pizza 3 枚を購入した場合の値段を知りたい。\n\n![](assets/mm_tutorial/Menu.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Menu.jpeg'},\n    {'text': 'How much would I pay if I want to order two Salmon Burger and three Meat Lover\\'s Pizza? Think carefully step by step.'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\nステップバイステップで注意深く考えてください(```Think carefully step by step.```)」は、複雑なタスクを一歩ずつでモデルをガイドする一般的なプロンプトです。複雑なタスクをこなさなければならない場合、このプロンプトを使ってモデルの精度を上げてみてください。以下のような出力が期待されます:\n\n> To order two Salmon Burgers and three Meat Lover's Pizzas, you would need to pay the following:\n>\n> 1. For two Salmon Burgers: x2 Salmon Burgers at $10 each = $20\n> 2. For three Meat Lover's Pizzas: x3 Meat Lover's Pizzas at $12 each = $36\n>\n> Therefore, the total cost would be $56.\n\n### **多視点推論と中国語入力**\nこれまでの例では、Qwen-VL-Chat が 1 つの画像と英語の質問に対して質問応答ができることを示しました。しかし、実際には Qwen-VL-Chat は中国語入力と複数の画像をサポートする多言語モデルです！以下の例では、Qwen-VL-Chat に 2 つの都市（重慶と北京）の写真（```assets/mm_tutorial/Chongqing.jpeg``` と ```assets/mm_tutorial/Beijing.jpeg```）を中国語で比較させています:\n\n![](assets/mm_tutorial/Chongqing_Small.jpeg)\n\n![](assets/mm_tutorial/Beijing_Small.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Chongqing.jpeg'},\n    {'image': 'assets/mm_tutorial/Beijing.jpeg'},\n    {'text': '上面两张图片分别是哪两个城市？请对它们进行对比。'},\n])\ntorch.manual_seed(5678)\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n以下のような出力が期待されます:\n\n> 第一张图片是重庆的城市天际线，它反映了现代都市的繁华与喧嚣。第二张图片是北京的天际线，它象征着中国首都的现代化和国际化。两座城市都是中国的重要城市，拥有独特的文化和发展历史。\n\n**都市の比較はかなり主観的な質問であるため、モデルによって生成される回答は高度なランダム性を持つ可能性があることに注意してください。```torch.manual_seed(5678)``` を使用してランダムシードを設定しない場合、出力は毎回異なります。ランダムシードを設定した場合でも、ハードウェアやソフトウェアの環境の違いにより、得られる結果がこのチュートリアルと異なる場合があります。**\n\n### **グラウンディング能力**\nチュートリアルの最後のセクションでは、Qwen-VL-Chat モデルがバウンディングボックスを生成する機能を紹介します。Qwen-VL-Chat は、言語記述に従って、画像の指定された領域を矩形の枠で囲むことができます。少し抽象的なので、次の例を見てみましょう。下図のように、ファイル ```assets/mm_tutorial/Shanghai.jpg``` は上海の写真です。まず、通常のプロンプトでモデルに画像を記述してもらいます。\n\n![](assets/mm_tutorial/Shanghai_Small.jpeg)\n\n```python\ntorch.manual_seed(1234)\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Shanghai.jpg'},\n    {'text': '图里有啥'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n以下のような出力が期待されます:\n\n> 图中是中国上海的天际线，包括了上海塔、金茂大厦、上海环球金融中心、海洋大厦等著名建筑。\n\n次に、プロンプト ```请给我框出图中上海环球金融中心和东方明珠``` を使ってモデルと会話してみましょう。このとき、```history=history``` を使って、以前の会話の履歴を ```model.chat``` に渡す必要があることに注意してください。\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': '请给我框出图中上海环球金融中心和东方明珠'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n以下のような出力が期待されます:\n```xml\n<ref>上海环球金融中心</ref><box>(667,437),(760,874)</box>和<ref>东方明珠</ref><box>(506,75),(582,946)</box>\n```\nQwen-VL-Chat モデルには手はありませんが、だからといってリクエストを拒否することもありません。その代わりに、\"奇妙な\"ものが出力されます - 実際、モデルの出力は上海环球金融中心（上海ワールド・フィナンシャル・センター）と东方明珠（東方テレビタワー）の位置をマークアップ言語で示しています。次のコードで視覚化できます:\n```python\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nimage.save('Shanghai_Output.jpg')\n```\n保存された ```Shanghai_Output.jpg``` は以下のスクリーンショットのようになります:\n\n![](assets/mm_tutorial/Shanghai_Output_Small.jpeg)\n\nその後、Qwen-VL-Chat で以前と同じようにチャットすることができます:\n```python\nquery = tokenizer.from_list_format([\n    {'text': '帮我写个这座城市的旅游计划'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\n以下のような出力が期待されます:\n\n> 好的，以下是一个简单的上海旅游计划：\n>\n> 第一天：\n> 上午：抵达上海，前往酒店办理入住手续。\n> 中午：享用午餐后，前往外滩，欣赏黄浦江畔的美景，游览上海地标性建筑如浦发银行大楼、汇丰银行大楼等。\n> 下午：游览南京路步行街，购买特色礼品或品尝当地美食。\n> 晚上：在南京路附近的餐厅享用晚餐，然后去看上海的夜景。\n>\n> 第二天：\n> 上午：前往上海科技馆，了解科技发展历史，观看各种科技展览。\n> 中午：在科技馆附近的餐厅享用午餐。\n> 下午：游览世纪公园，欣赏美景并放松身心。\n> 晚上：在南京路或附近的陆家嘴地区享用晚餐，然后去看上海的夜景。\n>\n> 第三天：\n> 上午：游览上海迪士尼乐园或上海海昌海洋公园，与各种迪士尼角色互动，或者在海洋公园观看海洋生物表演。\n> 中午：在迪士尼乐园或海洋公园附近的餐厅享用午餐。\n> 下午：自由活动，可以去购物、品尝当地美食或者去博物馆等。\n> 晚上：在酒店附近享用晚餐，然后离开上海。\n>\n> 当然，以上只是一个简单的计划，上海有许多其他景点和活动，例如参观上海博物馆、游览田子坊、观看上海话剧等。具体计划可以根据个人兴趣和时间进行调整。\n\n\n**旅行計画はかなり主観的な質問であるため、モデルによって生成される回答は高いランダム性を持つ可能性があることに注意してください。```torch.manual_seed(1234)``` を使用してランダムシードを設定しない場合、出力は毎回異なります。ランダムシードを設定した場合でも、ハードウェアやソフトウェアの環境の違いにより、得られる結果がこのチュートリアルと異なる場合があります。**\n"
        },
        {
          "name": "TUTORIAL_ko.md",
          "type": "blob",
          "size": 13.80078125,
          "content": "# Qwen-VL-Chat Tutorial\n\nQwen-VL-Chat은 범용 멀티모달 대규모 언어 모델이며 광범위한 시각 언어 작업을 수행할 수 있습니다. 이 튜토리얼에서는 **시각적 질문 답변, 텍스트 이해, 다이어그램을 사용한 수학적 추론, 다중 그림 추론 및 그라운딩(Grounding) 작업**에서 Qwen-VL-Chat의 기능을 보여주는 몇 가지 간결한 예제를 제시합니다. Qwen-VL-Chat의 기능의 한계가 아니며, **입력 이미지와 프롬프트를 변경하여 Qwen-VL-Chat의 기능**을 더 자세히 살펴보실 수도 있습니다.\n\n## Initializing the Qwen-VL-Chat model\nQwen-VL-Chat을 사용하기 전에 먼저 Qwen-VL-Chat의 Tokenizer와 Qwen-VL-Chat의 모델을 초기화해야 합니다.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# If you expect the results to be reproducible, set a random seed.\n# torch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n```\n\n위 코드를 실행하시면 ```tokenizer```변수에 Qwen-VL-Chat에서 사용하는 분류기(classifier)가 할당되고, ```model```변수에는 Qwen-VL-Chat의 모델을 할당하게 됩니다. ```tokenizer```는 인터리브된 멀티모달 입력(interleaved multimodal inputs)을 전처리하는 데 사용되며, ``model``은 Qwen-VL-Chat 모델입니다.\n\n## Using Qwen-VL-Chat\n### **Multi-round visual question answering**\n#### **첫 질문하기**\n\n간단한 예제를 확인해보겠습니다. 아래에서 볼 수 있듯이, ```assets/mm_tutorial/Rebecca_(1939_poster).jpeg``` 파일은 1940년 영화 <레베카>의 포스터입니다.\n\n![](assets/mm_tutorial/Rebecca_(1939_poster)_Small.jpeg)\n\nQwen-VL-Chat 포스터에 있는 영화 제목이 무엇인지 물어봅시다. 우선, 입력을 전처리하고 토큰화할 수 있는 ```tokenizer.from_list_format```을 사용합니다.\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Rebecca_(1939_poster).jpeg'},\n    {'text': 'What is the name of the movie in the poster?'},\n])\n```\n다음으로, ```model.chat```을 사용하여 Qwen-VL-Chat 모델에 질문하고 응답을 얻을 수 있습니다. 첫 번째 질문의 경우 대화 기록이 비어 있으므로 ``history=None``을 사용합니다.\n```python\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n다음과 비슷한 출력이 나올 것입니다.\n\n> The name of the movie in the poster is \"Rebecca.\"\n\n모델이 주어진 질문에 정답을 맞혔습니다. 포스터에 따르면, 영화의 제목은 실제로 **레베카**입니다.\n\n#### **Multi-round question answering**\n또한 모델에게 영화 감독이 누구인지와 같은 다른 질문을 계속할 수도 있습니다. 대화 기록은 후속 질문을 위해 비어 있지 않으므로 ``history=history``를 사용하여 이전 대화의 기록을 ``model.chat``에 전달합니다:\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Who directed this movie?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\n다음과 비슷한 출력이 나올 것입니다.\n\n> The movie \"Rebecca\" was directed by Alfred Hitchcock.\n\n다시 한 번, 모델이 주어진 질문에 대한 정답을 맞혔습니다. 포스터에 따르면 이 영화의 감독은 <알프레드 히치콕>입니다.\n\n### **Text Understanding**\nQwen-VL-Chat은 촘촘한 텍스트가 포함된 이미지도 이해할 수 있습니다. 아래 그림과 같이 ``assets/mm_tutorial/Hospital.jpeg`` 파일은 촘촘한 텍스트가 포함된 병원 간판입니다.\n\n![](assets/mm_tutorial/Hospital_Small.jpg)\n\n병원 내 여러 부서의 위치에 대해 질문할 수 있습니다. 첫 질문으로 대화에 대한 이전 기록이 없으므로 ```history=None```을 사용합니다.\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Hospital.jpg'},\n    {'text': 'Based on the photo, which floor is the Department of Otorhinolaryngology on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n다음과 비슷한 출력이 나올 것입니다.\n\n> The Department of Otorhinolaryngology is located on the 4th floor.\n\n추가 질문을 하실 수도 있습니다. 이 경우 ```history=history```를 사용하여 이전 대화의 기록을 ```model.chat```에 전달해야 합니다.\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Based on the photo, which floor is the Department of Surgery on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\n다음과 비슷한 출력이 나올 것입니다.\n\n> The Department of Surgery is located on the 3rd floor.\n\n### **Mathematical Reasoning with Diagram**\n모델의 다이어그램 이해와 수학적 추론 기능을 사용하여 Qwen-VL-Chat은 좀 더 복잡한 작업도 수행할 수 있습니다. 아래에서 볼 수 있듯이 ``assets/mm_tutorial/Menu.jpeg`` 파일은 레스토랑의 메뉴 이미지 입니다. 이제 연어 버거 두 개와 미트 러버스 피자 세 개를 구매하는 데 드는 비용을 알아봅시다.\n![](assets/mm_tutorial/Menu.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Menu.jpeg'},\n    {'text': 'How much would I pay if I want to order two Salmon Burger and three Meat Lover\\'s Pizza? Think carefully step by step.'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n``단계별로 신중하게 생각하세요``는 복잡한 작업을 단계별로 모델에 안내하는 일반적인 프롬프트입니다. 따라서 완료해야 할 복잡한 작업이 있는 경우에는 이 프롬프트를 사용하여 모델의 정확도를 향상시켜 보세요. 다음과 유사한 출력이 나올 것입니다.\n\n> To order two Salmon Burgers and three Meat Lover's Pizzas, you would need to pay the following:\n> \n> 1. For two Salmon Burgers: x2 Salmon Burgers at $10 each = $20\n> 2. For three Meat Lover's Pizzas: x3 Meat Lover's Pizzas at $12 each = $36\n> \n> Therefore, the total cost would be $56.\n\n### **Multi-Figure Reasoning and Chinese Input**\n이전 예제에서는 단일 이미지와 영어 질문에 대한 Qwen-VL-Chat의 질문 답변 기능을 시연했습니다. 하지만 실제로는 중국어 입력과 여러 이미지를 지원하는 다국어 모델입니다. 다음 예제에서는 두 도시(충칭과 베이징)의 사진(`assets/mm_tutorial/Chongqing.jpeg` 및 `assets/mm_tutorial/Beijing.jpeg`)을 중국어로 비교하도록 Qwen-VL-Chat을 설정했습니다.\n\n![](assets/mm_tutorial/Chongqing_Small.jpeg)\n\n![](assets/mm_tutorial/Beijing_Small.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Chongqing.jpeg'},\n    {'image': 'assets/mm_tutorial/Beijing.jpeg'},\n    {'text': '上面两张图片分别是哪两个城市？请对它们进行对比。'},\n])\ntorch.manual_seed(5678)\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n다음과 유사한 출력이 나올 것입니다.\n\n> 第一张图片是重庆的城市天际线，它反映了现代都市的繁华与喧嚣。第二张图片是北京的天际线，它象征着中国首都的现代化和国际化。两座城市都是中国的重要城市，拥有独特的文化和发展历史。\n\n**도시 비교는 상당히 주관적인 질문이므로 모델에 의해 생성된 응답에는 매우 다양하게 무작위의 시드가 적용될 수 있다는 점을 유의하세요. ``torch.manual_seed(5678)```를 사용하여 무작위 시드를 설정하지 않으면 매번 출력이 달라집니다. 랜덤 시드를 설정하더라도 하드웨어 및 소프트웨어 환경의 차이로 인해 얻은 결과가 이 튜토리얼과 다를 수 있습니다**.\n\n\n### **Grounding Capability**\n튜토리얼의 마지막 섹션에서는 Qwen-VL-Chat 모델이 바운딩 박스를 생성하는 기능을 보여드립니다. Qwen-VL-Chat은 언어 설명에 따라 직사각형 상자로 이미지의 지정된 영역에 프레임을 지정할 수 있습니다. 다소 추상적일 수 있으므로 다음 예제를 살펴보겠습니다. 아래 그림과 같이 ```assets/mm_tutorial/Shanghai.jpg`` 파일은 상하이의 사진이며, 모델에게 일반 프롬프트로 이미지를 설명하도록 요청하는 것으로 시작하겠습니다.\n\n![](assets/mm_tutorial/Shanghai_Small.jpeg)\n\n```python\ntorch.manual_seed(1234)\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Shanghai.jpg'},\n    {'text': '图里有啥'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n다음과 유사한 출력을 보실 수 있습니다.\n\n> 图中是中国上海的天际线，包括了上海塔、金茂大厦、上海环球金融中心、海洋大厦等著名建筑。\n\n다음으로 '``请给我框出图中上海环球金融中心和东方明珠``라는 프롬프트를 사용하여 모델과 대화하고 어떤 일이 발생하는지 살펴봅시다. 이 시점에서 ``history=history``를 사용하여 이전 대화의 기록을 ``model.chat``에 전달해야 합니다.\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': '请给我框出图中上海环球金融中心和东方明珠'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\n다음과 유사한 출력을 보실 수 있습니다.\n\n```xml\n<ref>上海环球金融中心</ref><box>(667,437),(760,874)</box>和<ref>东方明珠</ref><box>(506,75),(582,946)</box>\n```\n\nQwen-VL-Chat 모델에는 손이 없지만 사용자의 요청을 거부하지도 않습니다. 대신 \"이상한\" 결과를 출력하는데, 실제로 이 모델의 출력은 上海环球金融中心(상하이 월드 파이낸셜 센터) 와 东方明珠(동방명주) 의 위치를 마크업 언어로 제공합니다. 다음 코드를 사용하여 시각화할 수 있습니다.\n\n```python\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nimage.save('Shanghai_Output.jpg')\n```\nThe saved ```Shanghai_Output.jpg``` will look similar to the screenshot below: \n\n![](assets/mm_tutorial/Shanghai_Output_Small.jpeg)\n\n그 후에도 이전처럼 Qwen-VL-Chat으로 계속 채팅할 수 있습니다.\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': '帮我写个这座城市的旅游计划'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\n다음과 유사한 출력을 보실 수 있습니다.\n\n> 好的，以下是一个简单的上海旅游计划：\n>\n> 第一天：\n> 上午：抵达上海，前往酒店办理入住手续。\n> 中午：享用午餐后，前往外滩，欣赏黄浦江畔的美景，游览上海地标性建筑如浦发银行大楼、汇丰银行大楼等。\n> 下午：游览南京路步行街，购买特色礼品或品尝当地美食。\n> 晚上：在南京路附近的餐厅享用晚餐，然后去看上海的夜景。\n>\n> 第二天：\n> 上午：前往上海科技馆，了解科技发展历史，观看各种科技展览。\n> 中午：在科技馆附近的餐厅享用午餐。\n> 下午：游览世纪公园，欣赏美景并放松身心。\n> 晚上：在南京路或附近的陆家嘴地区享用晚餐，然后去看上海的夜景。\n>\n> 第三天：\n> 上午：游览上海迪士尼乐园或上海海昌海洋公园，与各种迪士尼角色互动，或者在海洋公园观看海洋生物表演。\n> 中午：在迪士尼乐园或海洋公园附近的餐厅享用午餐。\n> 下午：自由活动，可以去购物、品尝当地美食或者去博物馆等。\n> 晚上：在酒店附近享用晚餐，然后离开上海。\n>\n> 当然，以上只是一个简单的计划，上海有许多其他景点和活动，例如参观上海博物馆、游览田子坊、观看上海话剧等。具体计划可以根据个人兴趣和时间进行调整。\n\n**여행 계획은 상당히 주관적인 질문이므로 모델에 의해 생성된 응답에는 높은 수준의 랜덤 시드가 적용될 수 있다는 점에 유의하세요. ``torch.manual_seed(1234)``를 사용하여 무작위 시드를 설정하지 않으면 매번 다른 출력이 나오게 됩니다. 랜덤 시드를 일정하게 설정하더라도 하드웨어 및 소프트웨어 환경의 차이로 인해 얻은 결과가 이 튜토리얼과 다를 수 있습니다**.\n\n### Grounded Captioning\nQwen-VL은 다음과 같이 이미지를 캡쳐하는 동안 피사체의 바운딩 박스 정보를 출력할 수 있습니다. \n\n```\nimg_url = 'assets/apple.jpeg'\nquery = tokenizer.from_list_format([\n    {'image': img_url},\n    {'text': 'Generate the caption in English with grounding:'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nif image is not None:\n    image.save('apple.jpg')\n```\n\n저장된 ``사과.jpg``는 이미지는 아래 스크린샷과 비슷하게 보이게 될 것입니다.\n<p align=\"left\">\n    <img src=\"assets/apple_r.jpeg\" width=\"600\"/>\n<p>\n\n#### How to get the caption without any box-like annotations\n때로는 응답에 박스형 주석이 없을 수도 있습니다. 이 경우 다음과 같은 후처리를 통해 안정적으로 정리된 텍스트를 얻을 수 있습니다.\n\n```\n# response = '<ref> Two apples</ref><box>(302,257),(582,671)</box><box>(603,252),(878,642)</box> and<ref> a bowl</ref><box>(2,269),(304,674)</box>'\nimport re\nclean_response = re.sub(r'<ref>(.*?)</ref>(?:<box>.*?</box>)*(?:<quad>.*?</quad>)*', r'\\1', response).strip()\nprint(clean_response)\n# clean_response = 'Two apples and a bowl'\n```\n"
        },
        {
          "name": "TUTORIAL_zh.md",
          "type": "blob",
          "size": 11.1552734375,
          "content": "# Qwen-VL-Chat使用教程\nQwen-VL-Chat是通用多模态大规模语言模型，因此它可以完成多种视觉语言任务。在本教程之中，我们会给出一些简明的例子，用以展示Qwen-VL-Chat在**视觉问答，文字理解，图表数学推理，多图理解和Grounding**(根据指令标注图片中指定区域的包围框)等多方面的能力。请注意，展示的例子远非Qwen-VL-Chat能力的极限，**您可以通过更换不同的输入图像和提示词（Prompt），来进一步挖掘Qwen-VL-Chat的能力！**\n\n## 初始化Qwen-VL-Chat模型\n在使用Qwen-VL-Chat之前，您首先需要初始化Qwen-VL-Chat的分词器（Tokenizer）和Qwen-VL-Chat的模型：\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# 如果您希望结果可复现，可以设置随机数种子。\n# torch.manual_seed(1234)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL-Chat\", device_map=\"cuda\", trust_remote_code=True).eval()\nmodel.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL-Chat\", trust_remote_code=True)\n```\n在执行完上述代码后，```tokenizer```将对应Qwen-VL-Chat使用的分词器，而```model```将对应Qwen-VL-Chat的模型。```tokenizer```用于对图文混排输入进行分词和预处理，而```model```则是Qwen-VL-Chat模型本身。\n\n## 使用Qwen-VL-Chat\n### **多轮视觉问答**\n#### **第一个问题**\n首先我们来看一个最简单的例子，如下图所示，文件```assets/mm_tutorial/Rebecca_(1939_poster).jpeg```是1940年电影Rebecca的于1939发布的海报。\n\n![](assets/mm_tutorial/Rebecca_(1939_poster)_Small.jpeg)\n\n我们来问一问Qwen-VL-Chat海报上电影的名称是什么。首先，我们使用tokenizer.from_list_format可以对图文混排输入进行分词与处理：\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Rebecca_(1939_poster).jpeg'},\n    {'text': 'What is the name of the movie in the poster?'},\n])\n```\n接下来，我们可以使用```model.chat```向Qwen-VL-Chat模型提问并获得回复。注意在第一次提问时，对话历史为空，因此我们使用```history=None```。\n```python\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n您应该会得到类似下列的输出结果：\n\n> The name of the movie in the poster is \"Rebecca.\"\n\n这说明模型正确的回答了问题！根据海报，该电影的名称的确是**Rebecca**。\n\n#### **多轮问答**\n我们还可以继续向模型发问，例如询问电影的导演是谁。在后续提问时，对话历史并不为空，我们使用```history=history```向```model.chat```传递之前的对话历史：\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Who directed this movie?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\n您应该会得到类似下列的输出结果：\n\n> The movie \"Rebecca\" was directed by Alfred Hitchcock.\n\n模型再次正确回答了问题！根据海报，该电影的导演是Alfred Hitchcock。\n\n### **文字理解**\nQwen-VL-Chat具有一定的针对包含密集文字图片的理解能力。如下图所示，文件```assets/mm_tutorial/Hospital.jpeg```是一个包含密集文字的医院指示牌。\n\n![](assets/mm_tutorial/Hospital_Small.jpg)\n\n我们可以像之前一样向模型询问医院中各个科室的位置，对话历史为空，因此我们使用```history=None```。\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Hospital.jpg'},\n    {'text': 'Based on the photo, which floor is the Department of Otorhinolaryngology on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n您应该会得到类似下列的输出结果：\n\n> The Department of Otorhinolaryngology is located on the 4th floor.\n\n您同样可以进一步提出后续问题，此时需要使用```history=history```向```model.chat```传递之前的对话历史。\n\n```python\nquery = tokenizer.from_list_format([\n    {'text': 'Based on the photo, which floor is the Department of Surgery on?'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\n您应该会得到类似下列的输出结果：\n\n> The Department of Surgery is located on the 3rd floor.\n\n### **图表数学推理**\n利用模型的图表理解和数学推理能力，Qwen-VL-Chat还可以完成更复杂的一些任务！如下图所示，文件```assets/mm_tutorial/Menu.jpeg```展示了一家餐厅的菜单。现在我们想知道，如果购买两个Salmon Burger和三个Meat Lover's Pizza需要花多少钱呢？\n\n![](assets/mm_tutorial/Menu.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Menu.jpeg'},\n    {'text': 'How much would I pay if I want to order two Salmon Burger and three Meat Lover\\'s Pizza? Think carefully step by step.'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n```Think carefully step by step.```是一个引导模型分步处理复杂任务的常见提示词，如果您需要完成的任务较为复杂，可以试着使用它来提高准确率。您应该会得到类似下列的输出结果：\n\n> To order two Salmon Burgers and three Meat Lover's Pizzas, you would need to pay the following:\n> \n> 1. For two Salmon Burgers: x2 Salmon Burgers at $10 each = $20\n> 2. For three Meat Lover's Pizzas: x3 Meat Lover's Pizzas at $12 each = $36\n> \n> Therefore, the total cost would be $56.\n\n### **多图理解与中文输入**\n在之前的例子中，我们主要展示了Qwen-VL-Chat针对单张图像和英文问题的问答能力。但实际上，Qwen-VL-Chat是支持中文输入的多语言模型，而且也支持多张图片的输入！下面的例子中，我们用中文让Qwen-VL-Chat来为我们比较重庆和北京这两个城市的照片（```assets/mm_tutorial/Chongqing.jpeg```和```assets/mm_tutorial/Beijing.jpeg```）：\n\n![](assets/mm_tutorial/Chongqing_Small.jpeg)\n\n![](assets/mm_tutorial/Beijing_Small.jpeg)\n\n```python\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Chongqing.jpeg'},\n    {'image': 'assets/mm_tutorial/Beijing.jpeg'},\n    {'text': '上面两张图片分别是哪两个城市？请对它们进行对比。'},\n])\ntorch.manual_seed(5678)\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n您应该会得到类似下列的输出结果：\n\n> 第一张图片是重庆的城市天际线，它反映了现代都市的繁华与喧嚣。第二张图片是北京的天际线，它象征着中国首都的现代化和国际化。两座城市都是中国的重要城市，拥有独特的文化和发展历史。\n\n**请注意，城市间的比较是一个具有相当主观性的问题，因此模型产生的回复可能具有相当高的随机性。若不使用```torch.manual_seed(5678)```设置随机数种子，每次的输出结果会不一样。即使您设置了随机数种子，由于软硬件环境的差异，得到的结果也可能与本文档中的有所不同。**\n\n### **Grounding能力**\n在最后，我们展示Qwen-VL-Chat模型产生包围框的能力。Qwen-VL-Chat可以根据您的语言描述，在图像中用矩形框框出指定区域。这样说可能有些抽象，让我们来看下面的例子。如下图所示，文件```assets/mm_tutorial/Shanghai.jpg```是上海的一张照片，我们先用常规的提示词，问一下模型图里有什么。\n\n![](assets/mm_tutorial/Shanghai_Small.jpeg)\n\n```python\ntorch.manual_seed(1234)\nquery = tokenizer.from_list_format([\n    {'image': 'assets/mm_tutorial/Shanghai.jpg'},\n    {'text': '图里有啥'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=None)\nprint(response)\n```\n\n您应该会得到类似下列的输出结果：\n\n> 图中是中国上海的天际线，包括了上海塔、金茂大厦、上海环球金融中心、海洋大厦等著名建筑。\n\n接下来，我们通过使用```请给我框出图中上海环球金融中心和东方明珠```这个提示词来和模型对话，看看会发生什么。注意此时需要使用```history=history```向```model.chat```传递之前的对话历史。\n```python\nquery = tokenizer.from_list_format([\n    {'text': '请给我框出图中上海环球金融中心和东方明珠'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n您应该会得到类似下列的输出结果：\n```xml\n<ref>上海环球金融中心</ref><box>(667,437),(760,874)</box>和<ref>东方明珠</ref><box>(506,75),(582,946)</box>\n```\nQwen-VL-Chat模型没有手，但也没有拒绝您的请求，而是输出了一些“奇怪”的东西——并不是，实际上，模型的输出以标记语言的形式给出了上海环球金融中心和东方明珠在图中的具体位置。您可以使用下列代码将其可视化：\n```python\nimage = tokenizer.draw_bbox_on_latest_picture(response, history)\nimage.save('Shanghai_Output.jpg')\n```\n保存下来的```Shanghai_Output.jpg```结果将类似于下面的截图：\n\n![](assets/mm_tutorial/Shanghai_Output_Small.jpeg)\n\n在此之后，您还可以继续照常和Qwen-VL-Chat对话：\n```python\nquery = tokenizer.from_list_format([\n    {'text': '帮我写个这座城市的旅游计划'},\n])\nresponse, history = model.chat(tokenizer, query=query, history=history)\nprint(response)\n```\n\n您应该会得到类似下列的输出结果：\n\n> 好的，以下是一个简单的上海旅游计划：\n>\n> 第一天：\n> 上午：抵达上海，前往酒店办理入住手续。\n> 中午：享用午餐后，前往外滩，欣赏黄浦江畔的美景，游览上海地标性建筑如浦发银行大楼、汇丰银行大楼等。\n> 下午：游览南京路步行街，购买特色礼品或品尝当地美食。\n> 晚上：在南京路附近的餐厅享用晚餐，然后去看上海的夜景。\n>\n> 第二天：\n> 上午：前往上海科技馆，了解科技发展历史，观看各种科技展览。\n> 中午：在科技馆附近的餐厅享用午餐。\n> 下午：游览世纪公园，欣赏美景并放松身心。\n> 晚上：在南京路或附近的陆家嘴地区享用晚餐，然后去看上海的夜景。\n>\n> 第三天：\n> 上午：游览上海迪士尼乐园或上海海昌海洋公园，与各种迪士尼角色互动，或者在海洋公园观看海洋生物表演。\n> 中午：在迪士尼乐园或海洋公园附近的餐厅享用午餐。\n> 下午：自由活动，可以去购物、品尝当地美食或者去博物馆等。\n> 晚上：在酒店附近享用晚餐，然后离开上海。\n>\n> 当然，以上只是一个简单的计划，上海有许多其他景点和活动，例如参观上海博物馆、游览田子坊、观看上海话剧等。具体计划可以根据个人兴趣和时间进行调整。\n\n**请注意，旅游计划是一个具有相当主观性的问题，因此模型产生的回复可能具有相当高的随机性。若不使用```torch.manual_seed(1234)```设置随机数种子，每次的输出结果会不一样。即使您设置了随机数种子，由于软硬件环境的差异，得到的结果也可能与本文档中的有所不同。**\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "eval_mm",
          "type": "tree",
          "content": null
        },
        {
          "name": "finetune.py",
          "type": "blob",
          "size": 12.1865234375,
          "content": "# This code is based on the revised code from fastchat based on tatsu-lab/stanford_alpaca.\n\n\nfrom dataclasses import dataclass, field\nimport json\nimport math\nimport logging\nimport os\nfrom typing import Dict, Optional, List\nimport torch\nfrom torch.utils.data import Dataset\nfrom deepspeed import zero\nfrom deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\nimport transformers\nfrom transformers import Trainer, GPTQConfig, deepspeed\nfrom transformers.trainer_pt_utils import LabelSmoother\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom accelerate.utils import DistributedType\n\nIGNORE_TOKEN_ID = LabelSmoother.ignore_index\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"Qwen/Qwen-7B\")\n\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the training data.\"}\n    )\n    eval_data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the evaluation data.\"}\n    )\n    lazy_preprocess: bool = False\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=8192,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n    use_lora: bool = False\n    fix_vit: bool = True\n\n\n@dataclass\nclass LoraArguments:\n    lora_r: int = 64\n    lora_alpha: int = 16\n    lora_dropout: float = 0.05\n    lora_target_modules: List[str] = field(\n        default_factory=lambda: [\"c_attn\", \"attn.c_proj\", \"w1\", \"w2\"] ##[\"in_proj\",\"out_proj\",\"c_fc\"]\n    )\n    lora_weight_path: str = \"\"\n    lora_bias: str = \"none\"\n    q_lora: bool = False\n\n\ndef maybe_zero_3(param):\n    if hasattr(param, \"ds_id\"):\n        assert param.ds_status == ZeroParamStatus.NOT_AVAILABLE\n        with zero.GatheredParameters([param]):\n            param = param.data.detach().cpu().clone()\n    else:\n        param = param.detach().cpu().clone()\n    return param\n\n\n# Borrowed from peft.utils.get_peft_model_state_dict\ndef get_peft_state_maybe_zero_3(named_params, bias):\n    if bias == \"none\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k}\n    elif bias == \"all\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k or \"bias\" in k}\n    elif bias == \"lora_only\":\n        to_return = {}\n        maybe_lora_bias = {}\n        lora_bias_names = set()\n        for k, t in named_params:\n            if \"lora_\" in k:\n                to_return[k] = t\n                bias_name = k.split(\"lora_\")[0] + \"bias\"\n                lora_bias_names.add(bias_name)\n            elif \"bias\" in k:\n                maybe_lora_bias[k] = t\n        for k, t in maybe_lora_bias:\n            if bias_name in lora_bias_names:\n                to_return[bias_name] = t\n    else:\n        raise NotImplementedError\n    to_return = {k: maybe_zero_3(v) for k, v in to_return.items()}\n    return to_return\n\nlocal_rank = None\n\ndef rank0_print(*args):\n    if local_rank == 0:\n        print(*args)\n\n\ndef safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str, bias=\"none\"):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n    # check if zero3 mode enabled\n    if deepspeed.is_deepspeed_zero3_enabled():\n        state_dict = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()\n    else:\n        if trainer.args.use_lora:\n            state_dict = get_peft_state_maybe_zero_3(\n                trainer.model.named_parameters(), bias\n            )\n        else:\n            state_dict = trainer.model.state_dict()\n    if trainer.args.should_save and trainer.args.local_rank == 0:\n        trainer._save(output_dir, state_dict=state_dict)\n\n\ndef preprocess(\n    sources,\n    tokenizer: transformers.PreTrainedTokenizer,\n    max_len: int,\n    system_message: str = \"You are a helpful assistant.\"\n) -> Dict:\n    roles = {\"user\": \"<|im_start|>user\", \"assistant\": \"<|im_start|>assistant\"}\n\n    im_start = tokenizer.im_start_id\n    im_end = tokenizer.im_end_id\n    nl_tokens = tokenizer('\\n').input_ids\n    _system = tokenizer('system').input_ids + nl_tokens\n    _user = tokenizer('user').input_ids + nl_tokens\n    _assistant = tokenizer('assistant').input_ids + nl_tokens\n\n    # Apply prompt templates\n    input_ids, targets = [], []\n    for i, source in enumerate(sources):\n        if roles[source[0][\"from\"]] != roles[\"user\"]:\n            source = source[1:]\n\n        input_id, target = [], []\n        system = [im_start] + _system + tokenizer(system_message).input_ids + [im_end] + nl_tokens\n        input_id += system\n        target += [im_start] + [IGNORE_TOKEN_ID] * (len(system)-3) + [im_end] + nl_tokens\n        assert len(input_id) == len(target)\n        for j, sentence in enumerate(source):\n            role = roles[sentence[\"from\"]]\n            _input_id = tokenizer(role).input_ids + nl_tokens + \\\n                tokenizer(sentence[\"value\"]).input_ids + [im_end] + nl_tokens\n            input_id += _input_id\n            if role == '<|im_start|>user':\n                _target = [im_start] + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + [im_end] + nl_tokens\n            elif role == '<|im_start|>assistant':\n                _target = [im_start] + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \\\n                    _input_id[len(tokenizer(role).input_ids)+1:-2] + [im_end] + nl_tokens\n            else:\n                raise NotImplementedError\n            target += _target\n        assert len(input_id) == len(target)\n        input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n        target += [IGNORE_TOKEN_ID] * (max_len - len(target))\n        input_ids.append(input_id[:max_len])\n        targets.append(target[:max_len])\n    input_ids = torch.tensor(input_ids, dtype=torch.int)\n    targets = torch.tensor(targets, dtype=torch.int)\n\n    return dict(\n        input_ids=input_ids,\n        labels=targets,\n        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n    )\n\n\nclass SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, max_len: int):\n        super(SupervisedDataset, self).__init__()\n\n        rank0_print(\"Formatting inputs...\")\n        sources = [example[\"conversations\"] for example in raw_data]\n        data_dict = preprocess(sources, tokenizer, max_len)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n        self.attention_mask = data_dict[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(\n            input_ids=self.input_ids[i],\n            labels=self.labels[i],\n            attention_mask=self.attention_mask[i],\n        )\n\n\nclass LazySupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, max_len: int):\n        super(LazySupervisedDataset, self).__init__()\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n        self.tokenizer = tokenizer\n        self.raw_data = raw_data\n        self.cached_data_dict = {}\n\n    def __len__(self):\n        return len(self.raw_data)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        if i in self.cached_data_dict:\n            return self.cached_data_dict[i]\n\n        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer, self.max_len)\n        ret = dict(\n            input_ids=ret[\"input_ids\"][0],\n            labels=ret[\"labels\"][0],\n            attention_mask=ret[\"attention_mask\"][0],\n        )\n        self.cached_data_dict[i] = ret\n\n        return ret\n\n\ndef make_supervised_data_module(\n    tokenizer: transformers.PreTrainedTokenizer, data_args, max_len,\n) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    dataset_cls = (\n        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n    )\n    rank0_print(\"Loading data...\")\n\n    train_json = json.load(open(data_args.data_path, \"r\"))\n    train_dataset = dataset_cls(train_json, tokenizer=tokenizer, max_len=max_len)\n\n    if data_args.eval_data_path:\n        eval_json = json.load(open(data_args.eval_data_path, \"r\"))\n        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer, max_len=max_len)\n    else:\n        eval_dataset = None\n\n    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n\n\ndef train():\n    global local_rank\n    \n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments, LoraArguments)\n    )\n    (\n        model_args,\n        data_args,\n        training_args,\n        lora_args,\n    ) = parser.parse_args_into_dataclasses()\n\n    if getattr(training_args, 'deepspeed', None) and getattr(lora_args, 'q_lora', False):\n        training_args.distributed_state.distributed_type = DistributedType.DEEPSPEED\n\n    compute_dtype = (\n        torch.float16\n        if training_args.fp16\n        else (torch.bfloat16 if training_args.bf16 else torch.float32)\n    )\n\n    local_rank = training_args.local_rank\n\n    device_map = None\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    if lora_args.q_lora:\n        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)} if ddp else None\n        if len(training_args.fsdp) > 0 or deepspeed.is_deepspeed_zero3_enabled():\n            logging.warning(\n                \"FSDP or ZeRO3 are not incompatible with QLoRA.\"\n            )\n\n    # Set RoPE scaling factor\n    config = transformers.AutoConfig.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        trust_remote_code=True,\n    )\n    config.use_cache = False\n\n    # Load model and tokenizer\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        config=config,\n        cache_dir=training_args.cache_dir,\n        device_map=device_map,\n        trust_remote_code=True,\n        quantization_config=GPTQConfig(\n            bits=4, disable_exllama=True\n        )\n        if training_args.use_lora and lora_args.q_lora\n        else None,\n    )\n\n    if not training_args.use_lora:\n        if training_args.fix_vit and hasattr(model,'transformer') and hasattr(model.transformer,'visual'):\n            model.transformer.visual.requires_grad_(False)\n            if hasattr(model.transformer.visual,'attn_pool'):\n                model.transformer.visual.attn_pool.requires_grad_(True)\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n        trust_remote_code=True,\n    )\n    tokenizer.pad_token_id = tokenizer.eod_id\n\n    if training_args.use_lora:\n        if lora_args.q_lora or \"chat\" in model_args.model_name_or_path.lower():\n            modules_to_save = None\n        else:\n            modules_to_save = [\"wte\", \"lm_head\"]\n        lora_config = LoraConfig(\n            r=lora_args.lora_r,\n            lora_alpha=lora_args.lora_alpha,\n            target_modules=lora_args.lora_target_modules,\n            lora_dropout=lora_args.lora_dropout,\n            bias=lora_args.lora_bias,\n            task_type=\"CAUSAL_LM\",\n            modules_to_save=modules_to_save  # This argument serves for adding new tokens.\n        )\n        if lora_args.q_lora:\n            model = prepare_model_for_kbit_training(\n                model, use_gradient_checkpointing=training_args.gradient_checkpointing\n            )\n\n        model = get_peft_model(model, lora_config)\n\n        if training_args.gradient_checkpointing:\n            model.enable_input_require_grads()\n\n    # Load data\n    data_module = make_supervised_data_module(\n        tokenizer=tokenizer, data_args=data_args, max_len=training_args.model_max_length\n    )\n\n    # Start trainner\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n\n    trainer.train()\n    trainer.save_state()\n\n    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir, bias=lora_args.lora_bias)\n\n\nif __name__ == \"__main__\":\n    train()\n"
        },
        {
          "name": "finetune",
          "type": "tree",
          "content": null
        },
        {
          "name": "openai_api.py",
          "type": "blob",
          "size": 17.2734375,
          "content": "# coding=utf-8\n# Implements API for Qwen-7B in OpenAI's format. (https://platform.openai.com/docs/api-reference/chat)\n# Usage: python openai_api.py\n# Visit http://localhost:8000/docs for documents.\n\nimport re\nimport copy\nimport json\nimport time\nfrom argparse import ArgumentParser\nfrom contextlib import asynccontextmanager\nfrom typing import Dict, List, Literal, Optional, Union\n\nimport torch\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field\nfrom sse_starlette.sse import EventSourceResponse\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers.generation import GenerationConfig\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):  # collects GPU memory\n    yield\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n\n\napp = FastAPI(lifespan=lifespan)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\nclass ModelCard(BaseModel):\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"owner\"\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    permission: Optional[list] = None\n\n\nclass ModelList(BaseModel):\n    object: str = \"list\"\n    data: List[ModelCard] = []\n\n\nclass ChatMessage(BaseModel):\n    role: Literal[\"user\", \"assistant\", \"system\", \"function\"]\n    content: Optional[str]\n    function_call: Optional[Dict] = None\n\n\nclass DeltaMessage(BaseModel):\n    role: Optional[Literal[\"user\", \"assistant\", \"system\"]] = None\n    content: Optional[str] = None\n\n\nclass ChatCompletionRequest(BaseModel):\n    model: str\n    messages: List[ChatMessage]\n    functions: Optional[List[Dict]] = None\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    max_length: Optional[int] = None\n    stream: Optional[bool] = False\n    stop: Optional[List[str]] = None\n\n\nclass ChatCompletionResponseChoice(BaseModel):\n    index: int\n    message: ChatMessage\n    finish_reason: Literal[\"stop\", \"length\", \"function_call\"]\n\n\nclass ChatCompletionResponseStreamChoice(BaseModel):\n    index: int\n    delta: DeltaMessage\n    finish_reason: Optional[Literal[\"stop\", \"length\"]]\n\n\nclass ChatCompletionResponse(BaseModel):\n    model: str\n    object: Literal[\"chat.completion\", \"chat.completion.chunk\"]\n    choices: List[\n        Union[ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice]\n    ]\n    created: Optional[int] = Field(default_factory=lambda: int(time.time()))\n\n\n@app.get(\"/v1/models\", response_model=ModelList)\nasync def list_models():\n    global model_args\n    model_card = ModelCard(id=\"gpt-3.5-turbo\")\n    return ModelList(data=[model_card])\n\n\n# To work around that unpleasant leading-\\n tokenization issue!\ndef add_extra_stop_words(stop_words):\n    if stop_words:\n        _stop_words = []\n        _stop_words.extend(stop_words)\n        for x in stop_words:\n            s = x.lstrip(\"\\n\")\n            if s and (s not in _stop_words):\n                _stop_words.append(s)\n        return _stop_words\n    return stop_words\n\n\ndef trim_stop_words(response, stop_words):\n    if stop_words:\n        for stop in stop_words:\n            idx = response.find(stop)\n            if idx != -1:\n                response = response[:idx]\n    return response\n\n\nTOOL_DESC = \"\"\"{name_for_model}: Call this tool to interact with the {name_for_human} API. What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters}\"\"\"\n\nREACT_INSTRUCTION = \"\"\"Answer the following questions as best you can. You have access to the following APIs:\n\n{tools_text}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tools_name_text}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\"\"\"\n\n_TEXT_COMPLETION_CMD = object()\n\n\n#\n# Temporarily, the system role does not work as expected.\n# We advise that you write the setups for role-play in your query,\n# i.e., use the user role instead of the system role.\n#\n# TODO: Use real system role when the model is ready.\n#\ndef parse_messages(messages, functions):\n    if all(m.role != \"user\" for m in messages):\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Invalid request: Expecting at least one user message.\",\n        )\n\n    messages = copy.deepcopy(messages)\n    default_system = \"You are a helpful assistant.\"\n    system = \"\"\n    if messages[0].role == \"system\":\n        system = messages.pop(0).content.lstrip(\"\\n\").rstrip()\n        if system == default_system:\n            system = \"\"\n\n    if functions:\n        tools_text = []\n        tools_name_text = []\n        for func_info in functions:\n            name = func_info.get(\"name\", \"\")\n            name_m = func_info.get(\"name_for_model\", name)\n            name_h = func_info.get(\"name_for_human\", name)\n            desc = func_info.get(\"description\", \"\")\n            desc_m = func_info.get(\"description_for_model\", desc)\n            tool = TOOL_DESC.format(\n                name_for_model=name_m,\n                name_for_human=name_h,\n                # Hint: You can add the following format requirements in description:\n                #   \"Format the arguments as a JSON object.\"\n                #   \"Enclose the code within triple backticks (`) at the beginning and end of the code.\"\n                description_for_model=desc_m,\n                parameters=json.dumps(func_info[\"parameters\"], ensure_ascii=False),\n            )\n            tools_text.append(tool)\n            tools_name_text.append(name_m)\n        tools_text = \"\\n\\n\".join(tools_text)\n        tools_name_text = \", \".join(tools_name_text)\n        system += \"\\n\\n\" + REACT_INSTRUCTION.format(\n            tools_text=tools_text,\n            tools_name_text=tools_name_text,\n        )\n        system = system.lstrip(\"\\n\").rstrip()\n\n    dummy_thought = {\n        \"en\": \"\\nThought: I now know the final answer.\\nFinal answer: \",\n        \"zh\": \"\\nThought: 我会作答了。\\nFinal answer: \",\n    }\n\n    _messages = messages\n    messages = []\n    for m_idx, m in enumerate(_messages):\n        role, content, func_call = m.role, m.content, m.function_call\n        if content:\n            content = content.lstrip(\"\\n\").rstrip()\n        if role == \"function\":\n            if (len(messages) == 0) or (messages[-1].role != \"assistant\"):\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Invalid request: Expecting role assistant before role function.\",\n                )\n            messages[-1].content += f\"\\nObservation: {content}\"\n            if m_idx == len(_messages) - 1:\n                messages[-1].content += \"\\nThought:\"\n        elif role == \"assistant\":\n            if len(messages) == 0:\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"Invalid request: Expecting role user before role assistant.\",\n                )\n            last_msg = messages[-1].content\n            last_msg_has_zh = len(re.findall(r\"[\\u4e00-\\u9fff]+\", last_msg)) > 0\n            if func_call is None:\n                if functions:\n                    content = dummy_thought[\"zh\" if last_msg_has_zh else \"en\"] + content\n            else:\n                f_name, f_args = func_call[\"name\"], func_call[\"arguments\"]\n                if not content:\n                    if last_msg_has_zh:\n                        content = f\"Thought: 我可以使用 {f_name} API。\"\n                    else:\n                        content = f\"Thought: I can use {f_name}.\"\n                content = f\"\\n{content}\\nAction: {f_name}\\nAction Input: {f_args}\"\n            if messages[-1].role == \"user\":\n                messages.append(\n                    ChatMessage(role=\"assistant\", content=content.lstrip(\"\\n\").rstrip())\n                )\n            else:\n                messages[-1].content += content\n        elif role == \"user\":\n            messages.append(\n                ChatMessage(role=\"user\", content=content.lstrip(\"\\n\").rstrip())\n            )\n        else:\n            raise HTTPException(\n                status_code=400, detail=f\"Invalid request: Incorrect role {role}.\"\n            )\n\n    query = _TEXT_COMPLETION_CMD\n    if messages[-1].role == \"user\":\n        query = messages[-1].content\n        messages = messages[:-1]\n\n    if len(messages) % 2 != 0:\n        raise HTTPException(status_code=400, detail=\"Invalid request\")\n\n    history = []  # [(Q1, A1), (Q2, A2), ..., (Q_last_turn, A_last_turn)]\n    for i in range(0, len(messages), 2):\n        if messages[i].role == \"user\" and messages[i + 1].role == \"assistant\":\n            usr_msg = messages[i].content.lstrip(\"\\n\").rstrip()\n            bot_msg = messages[i + 1].content.lstrip(\"\\n\").rstrip()\n            if system and (i == len(messages) - 2):\n                usr_msg = f\"{system}\\n\\nQuestion: {usr_msg}\"\n                system = \"\"\n            for t in dummy_thought.values():\n                t = t.lstrip(\"\\n\")\n                if bot_msg.startswith(t) and (\"\\nAction: \" in bot_msg):\n                    bot_msg = bot_msg[len(t) :]\n            history.append([usr_msg, bot_msg])\n        else:\n            raise HTTPException(\n                status_code=400,\n                detail=\"Invalid request: Expecting exactly one user (or function) role before every assistant role.\",\n            )\n    if system:\n        assert query is not _TEXT_COMPLETION_CMD\n        query = f\"{system}\\n\\nQuestion: {query}\"\n    return query, history\n\n\ndef parse_response(response):\n    func_name, func_args = \"\", \"\"\n    i = response.rfind(\"\\nAction:\")\n    j = response.rfind(\"\\nAction Input:\")\n    k = response.rfind(\"\\nObservation:\")\n    if 0 <= i < j:  # If the text has `Action` and `Action input`,\n        if k < j:  # but does not contain `Observation`,\n            # then it is likely that `Observation` is omitted by the LLM,\n            # because the output text may have discarded the stop word.\n            response = response.rstrip() + \"\\nObservation:\"  # Add it back.\n        k = response.rfind(\"\\nObservation:\")\n        func_name = response[i + len(\"\\nAction:\") : j].strip()\n        func_args = response[j + len(\"\\nAction Input:\") : k].strip()\n    if func_name:\n        choice_data = ChatCompletionResponseChoice(\n            index=0,\n            message=ChatMessage(\n                role=\"assistant\",\n                content=response[:i],\n                function_call={\"name\": func_name, \"arguments\": func_args},\n            ),\n            finish_reason=\"function_call\",\n        )\n        return choice_data\n    z = response.rfind(\"\\nFinal Answer: \")\n    if z >= 0:\n        response = response[z + len(\"\\nFinal Answer: \") :]\n    choice_data = ChatCompletionResponseChoice(\n        index=0,\n        message=ChatMessage(role=\"assistant\", content=response),\n        finish_reason=\"stop\",\n    )\n    return choice_data\n\n\n# completion mode, not chat mode\ndef text_complete_last_message(history, stop_words_ids):\n    im_start = \"<|im_start|>\"\n    im_end = \"<|im_end|>\"\n    prompt = f\"{im_start}system\\nYou are a helpful assistant.{im_end}\"\n    for i, (query, response) in enumerate(history):\n        query = query.lstrip(\"\\n\").rstrip()\n        response = response.lstrip(\"\\n\").rstrip()\n        prompt += f\"\\n{im_start}user\\n{query}{im_end}\"\n        prompt += f\"\\n{im_start}assistant\\n{response}{im_end}\"\n    prompt = prompt[: -len(im_end)]\n\n    _stop_words_ids = [tokenizer.encode(im_end)]\n    if stop_words_ids:\n        for s in stop_words_ids:\n            _stop_words_ids.append(s)\n    stop_words_ids = _stop_words_ids\n\n    input_ids = torch.tensor([tokenizer.encode(prompt)]).to(model.device)\n    output = model.generate(input_ids, stop_words_ids=stop_words_ids).tolist()[0]\n    output = tokenizer.decode(output, errors=\"ignore\")\n    assert output.startswith(prompt)\n    output = output[len(prompt) :]\n    output = trim_stop_words(output, [\"<|endoftext|>\", im_end])\n    print(f\"<completion>\\n{prompt}\\n<!-- *** -->\\n{output}\\n</completion>\")\n    return output\n\n\n@app.post(\"/v1/chat/completions\", response_model=ChatCompletionResponse)\nasync def create_chat_completion(request: ChatCompletionRequest):\n    global model, tokenizer\n\n    stop_words = add_extra_stop_words(request.stop)\n    if request.functions:\n        stop_words = stop_words or []\n        if \"Observation:\" not in stop_words:\n            stop_words.append(\"Observation:\")\n\n    query, history = parse_messages(request.messages, request.functions)\n\n    if request.stream:\n        if request.functions:\n            raise HTTPException(\n                status_code=400,\n                detail=\"Invalid request: Function calling is not yet implemented for stream mode.\",\n            )\n        # generate = predict(query, history, request.model, stop_words)\n        # return EventSourceResponse(generate, media_type=\"text/event-stream\")\n        raise HTTPException(status_code=400, detail=\"Stream request is not supported currently.\")\n\n    stop_words_ids = [tokenizer.encode(s) for s in stop_words] if stop_words else None\n    if query is _TEXT_COMPLETION_CMD:\n        response = text_complete_last_message(history, stop_words_ids=stop_words_ids)\n    else:\n        response, _ = model.chat(\n            tokenizer,\n            query,\n            history=history,\n            stop_words_ids=stop_words_ids,\n            append_history=False,\n            top_p=request.top_p,\n            temperature=request.temperature,\n        )\n        print(f\"<chat>\\n{history}\\n{query}\\n<!-- *** -->\\n{response}\\n</chat>\")\n    response = trim_stop_words(response, stop_words)\n    if request.functions:\n        choice_data = parse_response(response)\n    else:\n        choice_data = ChatCompletionResponseChoice(\n            index=0,\n            message=ChatMessage(role=\"assistant\", content=response),\n            finish_reason=\"stop\",\n        )\n    return ChatCompletionResponse(\n        model=request.model, choices=[choice_data], object=\"chat.completion\"\n    )\n\n\nasync def predict(\n    query: str, history: List[List[str]], model_id: str, stop_words: List[str]\n):\n    global model, tokenizer\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0, delta=DeltaMessage(role=\"assistant\"), finish_reason=None\n    )\n    chunk = ChatCompletionResponse(\n        model=model_id, choices=[choice_data], object=\"chat.completion.chunk\"\n    )\n    yield \"{}\".format(chunk.model_dump_json(exclude_unset=True))\n\n    current_length = 0\n    stop_words_ids = [tokenizer.encode(s) for s in stop_words] if stop_words else None\n    if stop_words:\n        # TODO: It's a little bit tricky to trim stop words in the stream mode.\n        raise HTTPException(\n            status_code=400,\n            detail=\"Invalid request: custom stop words are not yet supported for stream mode.\",\n        )\n    response_generator = model.chat_stream(\n        tokenizer, query, history=history, stop_words_ids=stop_words_ids\n    )\n    for new_response in response_generator:\n        if len(new_response) == current_length:\n            continue\n\n        new_text = new_response[current_length:]\n        current_length = len(new_response)\n\n        choice_data = ChatCompletionResponseStreamChoice(\n            index=0, delta=DeltaMessage(content=new_text), finish_reason=None\n        )\n        chunk = ChatCompletionResponse(\n            model=model_id, choices=[choice_data], object=\"chat.completion.chunk\"\n        )\n        yield \"{}\".format(chunk.model_dump_json(exclude_unset=True))\n\n    choice_data = ChatCompletionResponseStreamChoice(\n        index=0, delta=DeltaMessage(), finish_reason=\"stop\"\n    )\n    chunk = ChatCompletionResponse(\n        model=model_id, choices=[choice_data], object=\"chat.completion.chunk\"\n    )\n    yield \"{}\".format(chunk.model_dump_json(exclude_unset=True))\n    yield \"[DONE]\"\n\n\ndef _get_args():\n    parser = ArgumentParser()\n    parser.add_argument(\n        \"-c\",\n        \"--checkpoint-path\",\n        type=str,\n        default=\"QWen/QWen-7B-Chat\",\n        help=\"Checkpoint name or path, default to %(default)r\",\n    )\n    parser.add_argument(\n        \"--cpu-only\", action=\"store_true\", help=\"Run demo with CPU only\"\n    )\n    parser.add_argument(\n        \"--server-port\", type=int, default=8000, help=\"Demo server port.\"\n    )\n    parser.add_argument(\n        \"--server-name\",\n        type=str,\n        default=\"127.0.0.1\",\n        help=\"Demo server name. Default: 127.0.0.1, which is only visible from the local computer.\"\n        \" If you want other computers to access your server, use 0.0.0.0 instead.\",\n    )\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    args = _get_args()\n\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.checkpoint_path,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n\n    if args.cpu_only:\n        device_map = \"cpu\"\n    else:\n        device_map = \"auto\"\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.checkpoint_path,\n        device_map=device_map,\n        trust_remote_code=True,\n        resume_download=True,\n    ).eval()\n\n    model.generation_config = GenerationConfig.from_pretrained(\n        args.checkpoint_path,\n        trust_remote_code=True,\n        resume_download=True,\n    )\n\n    uvicorn.run(app, host=args.server_name, port=args.server_port, workers=1)"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1298828125,
          "content": "transformers==4.32.0\naccelerate\ntiktoken\neinops\ntransformers_stream_generator==0.0.4\nscipy\ntorchvision\npillow\ntensorboard\nmatplotlib\n"
        },
        {
          "name": "requirements_openai_api.txt",
          "type": "blob",
          "size": 0.044921875,
          "content": "fastapi\nuvicorn\nopenai\npydantic\nsse_starlette\n"
        },
        {
          "name": "requirements_web_demo.txt",
          "type": "blob",
          "size": 0.017578125,
          "content": "gradio\nmodelscope\n"
        },
        {
          "name": "touchstone",
          "type": "tree",
          "content": null
        },
        {
          "name": "web_demo_mm.py",
          "type": "blob",
          "size": 9.4677734375,
          "content": "# Copyright (c) Alibaba Cloud.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"A simple web interactive chat demo based on gradio.\"\"\"\n\nfrom argparse import ArgumentParser\nfrom pathlib import Path\n\nimport copy\nimport gradio as gr\nimport os\nimport re\nimport secrets\nimport tempfile\nfrom modelscope import (\n    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n)\n\nDEFAULT_CKPT_PATH = 'qwen/Qwen-VL-Chat'\nBOX_TAG_PATTERN = r\"<box>([\\s\\S]*?)</box>\"\nPUNCTUATION = \"！？。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n\n\ndef _get_args():\n    parser = ArgumentParser()\n    parser.add_argument(\"-c\", \"--checkpoint-path\", type=str, default=DEFAULT_CKPT_PATH,\n                        help=\"Checkpoint name or path, default to %(default)r\")\n    parser.add_argument(\"--cpu-only\", action=\"store_true\", help=\"Run demo with CPU only\")\n\n    parser.add_argument(\"--share\", action=\"store_true\", default=False,\n                        help=\"Create a publicly shareable link for the interface.\")\n    parser.add_argument(\"--inbrowser\", action=\"store_true\", default=False,\n                        help=\"Automatically launch the interface in a new tab on the default browser.\")\n    parser.add_argument(\"--server-port\", type=int, default=8000,\n                        help=\"Demo server port.\")\n    parser.add_argument(\"--server-name\", type=str, default=\"127.0.0.1\",\n                        help=\"Demo server name.\")\n\n    args = parser.parse_args()\n    return args\n\n\ndef _load_model_tokenizer(args):\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.checkpoint_path, trust_remote_code=True, resume_download=True, revision='master',\n    )\n\n    if args.cpu_only:\n        device_map = \"cpu\"\n    else:\n        device_map = \"cuda\"\n\n    model = AutoModelForCausalLM.from_pretrained(\n        args.checkpoint_path,\n        device_map=device_map,\n        trust_remote_code=True,\n        resume_download=True,\n        revision='master',\n    ).eval()\n    model.generation_config = GenerationConfig.from_pretrained(\n        args.checkpoint_path, trust_remote_code=True, resume_download=True, revision='master',\n    )\n\n    return model, tokenizer\n\n\ndef _parse_text(text):\n    lines = text.split(\"\\n\")\n    lines = [line for line in lines if line != \"\"]\n    count = 0\n    for i, line in enumerate(lines):\n        if \"```\" in line:\n            count += 1\n            items = line.split(\"`\")\n            if count % 2 == 1:\n                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n            else:\n                lines[i] = f\"<br></code></pre>\"\n        else:\n            if i > 0:\n                if count % 2 == 1:\n                    line = line.replace(\"`\", r\"\\`\")\n                    line = line.replace(\"<\", \"&lt;\")\n                    line = line.replace(\">\", \"&gt;\")\n                    line = line.replace(\" \", \"&nbsp;\")\n                    line = line.replace(\"*\", \"&ast;\")\n                    line = line.replace(\"_\", \"&lowbar;\")\n                    line = line.replace(\"-\", \"&#45;\")\n                    line = line.replace(\".\", \"&#46;\")\n                    line = line.replace(\"!\", \"&#33;\")\n                    line = line.replace(\"(\", \"&#40;\")\n                    line = line.replace(\")\", \"&#41;\")\n                    line = line.replace(\"$\", \"&#36;\")\n                lines[i] = \"<br>\" + line\n    text = \"\".join(lines)\n    return text\n\ndef _remove_image_special(text):\n    text = text.replace('<ref>', '').replace('</ref>', '')\n    return re.sub(r'<box>.*?(</box>|$)', '', text)\n\ndef _launch_demo(args, model, tokenizer):\n    uploaded_file_dir = os.environ.get(\"GRADIO_TEMP_DIR\") or str(\n        Path(tempfile.gettempdir()) / \"gradio\"\n    )\n\n    def predict(_chatbot, task_history):\n        chat_query = _chatbot[-1][0]\n        query = task_history[-1][0]\n        print(\"User: \" + _parse_text(query))\n        history_cp = copy.deepcopy(task_history)\n        full_response = \"\"\n\n        history_filter = []\n        pic_idx = 1\n        pre = \"\"\n        for i, (q, a) in enumerate(history_cp):\n            if isinstance(q, (tuple, list)):\n                q = f'Picture {pic_idx}: <img>{q[0]}</img>'\n                pre += q + '\\n'\n                pic_idx += 1\n            else:\n                pre += q\n                history_filter.append((pre, a))\n                pre = \"\"\n        history, message = history_filter[:-1], history_filter[-1][0]\n        # response, history = model.chat(tokenizer, message, history=history)\n        for response in model.chat_stream(tokenizer, message, history=history):\n            _chatbot[-1] = (_parse_text(chat_query), _remove_image_special(_parse_text(response)))\n\n            yield _chatbot\n            full_response = _parse_text(response)\n\n        response = full_response\n        history.append((message, response))\n        image = tokenizer.draw_bbox_on_latest_picture(response, history)\n        if image is not None:\n            temp_dir = secrets.token_hex(20)\n            temp_dir = Path(uploaded_file_dir) / temp_dir\n            temp_dir.mkdir(exist_ok=True, parents=True)\n            name = f\"tmp{secrets.token_hex(5)}.jpg\"\n            filename = temp_dir / name\n            image.save(str(filename))\n            _chatbot.append((None, (str(filename),)))\n        else:\n            _chatbot[-1] = (_parse_text(chat_query), response)\n        # full_response = _parse_text(response)\n\n        task_history[-1] = (query, full_response)\n        print(\"Qwen-VL-Chat: \" + _parse_text(full_response))\n        yield _chatbot\n\n    def regenerate(_chatbot, task_history):\n        if not task_history:\n            return _chatbot\n        item = task_history[-1]\n        if item[1] is None:\n            return _chatbot\n        task_history[-1] = (item[0], None)\n        chatbot_item = _chatbot.pop(-1)\n        if chatbot_item[0] is None:\n            _chatbot[-1] = (_chatbot[-1][0], None)\n        else:\n            _chatbot.append((chatbot_item[0], None))\n        return predict(_chatbot, task_history)\n\n    def add_text(history, task_history, text):\n        task_text = text\n        if len(text) >= 2 and text[-1] in PUNCTUATION and text[-2] not in PUNCTUATION:\n            task_text = text[:-1]\n        history = history + [(_parse_text(text), None)]\n        task_history = task_history + [(task_text, None)]\n        return history, task_history, \"\"\n\n    def add_file(history, task_history, file):\n        history = history + [((file.name,), None)]\n        task_history = task_history + [((file.name,), None)]\n        return history, task_history\n\n    def reset_user_input():\n        return gr.update(value=\"\")\n\n    def reset_state(task_history):\n        task_history.clear()\n        return []\n\n    with gr.Blocks() as demo:\n        gr.Markdown(\"\"\"\\\n<p align=\"center\"><img src=\"https://modelscope.cn/api/v1/models/qwen/Qwen-7B-Chat/repo?\nRevision=master&FilePath=assets/logo.jpeg&View=true\" style=\"height: 80px\"/><p>\"\"\")\n        gr.Markdown(\"\"\"<center><font size=8>Qwen-VL-Chat Bot</center>\"\"\")\n        gr.Markdown(\n            \"\"\"\\\n<center><font size=3>This WebUI is based on Qwen-VL-Chat, developed by Alibaba Cloud. \\\n(本WebUI基于Qwen-VL-Chat打造，实现聊天机器人功能。)</center>\"\"\")\n        gr.Markdown(\"\"\"\\\n<center><font size=4>Qwen-VL <a href=\"https://modelscope.cn/models/qwen/Qwen-VL/summary\">🤖 </a> \n| <a href=\"https://huggingface.co/Qwen/Qwen-VL\">🤗</a>&nbsp ｜ \nQwen-VL-Chat <a href=\"https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary\">🤖 </a> | \n<a href=\"https://huggingface.co/Qwen/Qwen-VL-Chat\">🤗</a>&nbsp ｜ \n&nbsp<a href=\"https://github.com/QwenLM/Qwen-VL\">Github</a></center>\"\"\")\n\n        chatbot = gr.Chatbot(label='Qwen-VL-Chat', elem_classes=\"control-height\", height=750)\n        query = gr.Textbox(lines=2, label='Input')\n        task_history = gr.State([])\n\n        with gr.Row():\n            empty_bin = gr.Button(\"🧹 Clear History (清除历史)\")\n            submit_btn = gr.Button(\"🚀 Submit (发送)\")\n            regen_btn = gr.Button(\"🤔️ Regenerate (重试)\")\n            addfile_btn = gr.UploadButton(\"📁 Upload (上传文件)\", file_types=[\"image\"])\n\n        submit_btn.click(add_text, [chatbot, task_history, query], [chatbot, task_history]).then(\n            predict, [chatbot, task_history], [chatbot], show_progress=True\n        )\n        submit_btn.click(reset_user_input, [], [query])\n        empty_bin.click(reset_state, [task_history], [chatbot], show_progress=True)\n        regen_btn.click(regenerate, [chatbot, task_history], [chatbot], show_progress=True)\n        addfile_btn.upload(add_file, [chatbot, task_history, addfile_btn], [chatbot, task_history], show_progress=True)\n\n        gr.Markdown(\"\"\"\\\n<font size=2>Note: This demo is governed by the original license of Qwen-VL. \\\nWe strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, \\\nincluding hate speech, violence, pornography, deception, etc. \\\n(注：本演示受Qwen-VL的许可协议限制。我们强烈建议，用户不应传播及不应允许他人传播以下内容，\\\n包括但不限于仇恨言论、暴力、色情、欺诈相关的有害信息。)\"\"\")\n\n    demo.queue().launch(\n        share=args.share,\n        inbrowser=args.inbrowser,\n        server_port=args.server_port,\n        server_name=args.server_name,\n    )\n\n\ndef main():\n    args = _get_args()\n\n    model, tokenizer = _load_model_tokenizer(args)\n\n    _launch_demo(args, model, tokenizer)\n\n\nif __name__ == '__main__':\n    main()\n"
        }
      ]
    }
  ]
}