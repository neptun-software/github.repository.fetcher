{
  "metadata": {
    "timestamp": 1736561058747,
    "page": 842,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjg1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "dbolya/yolact",
      "stars": 5061,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.775390625,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*,cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# IPython Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# dotenv\n.env\n\n# virtualenv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n\n# Rope project settings\n.ropeproject\n\n# atom remote-sync package\n.remote-sync.json\n\n# weights\nweights/\n\n#DS_Store\n.DS_Store\n\n# dev stuff\neval/\neval.ipynb\ndev.ipynb\n.vscode/\n\n# not ready\nvideos/\ntemplates/\ndata/ssd_dataloader.py\ndata/datasets/\ndoc/visualize.py\nread_results.py\nssd300_120000/\ndemos/live\nwebdemo.py\ntest_data_aug.py\n\n# attributes\n\n# pycharm\n.idea/\n\n# temp checkout soln\ndata/coco\ndata/sbd\ndata/cityscapes\n\n# pylint\n.pylintrc\n\n# ssd.pytorch master branch (for merging)\nssd.pytorch/\n\n# some datasets\ndata/VOCdevkit/\ndata/coco/images/\ndata/coco/annotations/\nap_data.pkl\nresults/\nlogs/\nscripts/aws/\nscripts/gt.npy\nscripts/proto.npy\nscripts/info.txt\ntest.pkl\ntesteval.py\nscripts/aws2/\nstatus.sh\ntrain.sh\nimg/\nscripts/aws-ohio/\nscripts/aws3/\ndata/config_dev.py\ndata/coco/\ndata/sbd/\n\nvid/\nvidres/\n\n_.py\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 2.8642578125,
          "content": "# YOLACT Change Log\n\nThis document will detail all changes I make.\nI don't know how I'm going to be versioning things yet, so you get dates for now.\n\n```\n2020.01.25:\n  - Fixed the mask IoU branch crashing when all masks in a batch are discarded (fixes #302, #259).\n2020.01.24:\n  - Fixed the conv layer detection during initialization to work with pytorch 1.4 (fixes #292). \n2020.01.23:\n  - Fixed the video playback crashing if there's nothing in the scene (fixes #266).\n  - Fixed the logger logging the last loss as total loss instead of the actual total (fixes #254).\n\n2019.12.16 (v1.2):\n  - Added YOLACT++ implementation, paper, and code.\n    - Added DCN support (need to compile CUDA kernels if you want to use them, see README).\n    - Added a mask rescoring network trained with mask iou.\n    - Added configs with more anchors.\n\n2019.12.06:\n  - Made training much more stable (no more infs and hopefully fewer loss explosions) by ignoring\n    augmented boxes with < 4px of height and width (this includes 0 area boxes which caused the inf).\n    See #222 for details.\n\n2019.11.20:\n  - Fixed bug where saving videos wouldn't work when using cv2 not compiled with display support (#197).\n\n2019.11.06:\n  - Changed Cython import to only active when using traditional nms.\n  - Added cross-class fast NMS.\n\n2019.11.04:\n  - Fixed a bug where the learning rate auto-scaling wasn't being applied properly.\n  - Fixed a logging bug were lr was sometimes not properly logged after a resume.\n\n2019.10.25 (v1.1):\n  - Added proper Multi-GPU support. Simply increase your batch size to 8*num_gpus and everything will scale.\n    - I get an ~1.8x training speed increase when using 2 gpus and an ~3x increase when using 4.\n  - Added a logger that logs everything about your training.\n    - Check the Logging section of the README to see how to visualize your logs. (Not written yet)\n  - Savevideo now uses the evalvideo framework and suports --video_multiframe. It's much faster now!\n  - Added the ability to display fps right on the videos themselves by using --display_fps\n  - Evalvideo now doesn't crash when it runs out of frames.\n  - Pascal SBD is now officially supported! Check the training section for more details.\n  - Preserve_aspect_ratio kinda sorta works now, but it's iffy and the way I have it set up doesn't perform better.\n  - Added a ton of new config settings, most of which don't improve performance :/\n\n2019.09.20\n  - Fixed a bug where custom label maps weren't being applied properly because of global default argument initialization.\n2019.08.29\n  - Fixed a bug where the fpn conv layers weren't getting initialized with xavier since they were being overwritten by jit modules (see #127).\n2019.08.04\n  - Improved the matching algorithm used to match anchors to gt by making it less greedy (see #104).\n2019.06.27\n  - Sped up save video by ~8 ms per frame because I forgot to apply a speed fix I applied to the other modes.\n```\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0439453125,
          "content": "MIT License\n\nCopyright (c) 2019 Daniel Bolya\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 15.8203125,
          "content": "# **Y**ou **O**nly **L**ook **A**t **C**oefficien**T**s\n```\n    ██╗   ██╗ ██████╗ ██╗      █████╗  ██████╗████████╗\n    ╚██╗ ██╔╝██╔═══██╗██║     ██╔══██╗██╔════╝╚══██╔══╝\n     ╚████╔╝ ██║   ██║██║     ███████║██║        ██║   \n      ╚██╔╝  ██║   ██║██║     ██╔══██║██║        ██║   \n       ██║   ╚██████╔╝███████╗██║  ██║╚██████╗   ██║   \n       ╚═╝    ╚═════╝ ╚══════╝╚═╝  ╚═╝ ╚═════╝   ╚═╝ \n```\n\nA simple, fully convolutional model for real-time instance segmentation. This is the code for our papers:\n - [YOLACT: Real-time Instance Segmentation](https://arxiv.org/abs/1904.02689)\n - [YOLACT++: Better Real-time Instance Segmentation](https://arxiv.org/abs/1912.06218)\n\n#### YOLACT++ (v1.2) released! ([Changelog](CHANGELOG.md))\nYOLACT++'s resnet50 model runs at 33.5 fps on a Titan Xp and achieves 34.1 mAP on COCO's `test-dev` (check out our journal paper [here](https://arxiv.org/abs/1912.06218)).\n\nIn order to use YOLACT++, make sure you compile the DCNv2 code. (See [Installation](https://github.com/dbolya/yolact#installation))\n\n#### For a real-time demo, check out our ICCV video:\n[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/0pMfmo8qfpQ/0.jpg)](https://www.youtube.com/watch?v=0pMfmo8qfpQ)\n\nSome examples from our YOLACT base model (33.5 fps on a Titan Xp and 29.8 mAP on COCO's `test-dev`):\n\n![Example 0](data/yolact_example_0.png)\n\n![Example 1](data/yolact_example_1.png)\n\n![Example 2](data/yolact_example_2.png)\n\n# Installation\n - Clone this repository and enter it:\n   ```Shell\n   git clone https://github.com/dbolya/yolact.git\n   cd yolact\n   ```\n - Set up the environment using one of the following methods:\n   - Using [Anaconda](https://www.anaconda.com/distribution/)\n     - Run `conda env create -f environment.yml`\n   - Manually with pip\n     - Set up a Python3 environment (e.g., using virtenv).\n     - Install [Pytorch](http://pytorch.org/) 1.0.1 (or higher) and TorchVision.\n     - Install some other packages:\n       ```Shell\n       # Cython needs to be installed before pycocotools\n       pip install cython\n       pip install opencv-python pillow pycocotools matplotlib \n       ```\n - If you'd like to train YOLACT, download the COCO dataset and the 2014/2017 annotations. Note that this script will take a while and dump 21gb of files into `./data/coco`.\n   ```Shell\n   sh data/scripts/COCO.sh\n   ```\n - If you'd like to evaluate YOLACT on `test-dev`, download `test-dev` with this script.\n   ```Shell\n   sh data/scripts/COCO_test.sh\n   ```\n - If you want to use YOLACT++, compile deformable convolutional layers (from [DCNv2](https://github.com/CharlesShang/DCNv2/tree/pytorch_1.0)).\n   Make sure you have the latest CUDA toolkit installed from [NVidia's Website](https://developer.nvidia.com/cuda-toolkit).\n   ```Shell\n   cd external/DCNv2\n   python setup.py build develop\n   ```\n\n\n# Evaluation\nHere are our YOLACT models (released on April 5th, 2019) along with their FPS on a Titan Xp and mAP on `test-dev`:\n\n| Image Size | Backbone      | FPS  | mAP  | Weights                                                                                                              |  |\n|:----------:|:-------------:|:----:|:----:|----------------------------------------------------------------------------------------------------------------------|--------|\n| 550        | Resnet50-FPN  | 42.5 | 28.2 | [yolact_resnet50_54_800000.pth](https://drive.google.com/file/d/1yp7ZbbDwvMiFJEq4ptVKTYTI2VeRDXl0/view?usp=sharing)  | [Mirror](https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EUVpxoSXaqNIlssoLKOEoCcB1m0RpzGq_Khp5n1VX3zcUw) |\n| 550        | Darknet53-FPN | 40.0 | 28.7 | [yolact_darknet53_54_800000.pth](https://drive.google.com/file/d/1dukLrTzZQEuhzitGkHaGjphlmRJOjVnP/view?usp=sharing) | [Mirror](https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/ERrao26c8llJn25dIyZPhwMBxUp2GdZTKIMUQA3t0djHLw)\n| 550        | Resnet101-FPN | 33.5 | 29.8 | [yolact_base_54_800000.pth](https://drive.google.com/file/d/1UYy3dMapbH1BnmtZU4WH1zbYgOzzHHf_/view?usp=sharing)      | [Mirror](https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EYRWxBEoKU9DiblrWx2M89MBGFkVVB_drlRd_v5sdT3Hgg)\n| 700        | Resnet101-FPN | 23.6 | 31.2 | [yolact_im700_54_800000.pth](https://drive.google.com/file/d/1lE4Lz5p25teiXV-6HdTiOJSnS7u7GBzg/view?usp=sharing)     | [Mirror](https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/Eagg5RSc5hFEhp7sPtvLNyoBjhlf2feog7t8OQzHKKphjw)\n\nYOLACT++ models (released on December 16th, 2019):\n\n| Image Size | Backbone      | FPS  | mAP  | Weights                                                                                                              |  |\n|:----------:|:-------------:|:----:|:----:|----------------------------------------------------------------------------------------------------------------------|--------|\n| 550        | Resnet50-FPN  | 33.5 | 34.1 | [yolact_plus_resnet50_54_800000.pth](https://drive.google.com/file/d/1ZPu1YR2UzGHQD0o1rEqy-j5bmEm3lbyP/view?usp=sharing)  | [Mirror](https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EcJAtMiEFlhAnVsDf00yWRIBUC4m8iE9NEEiV05XwtEoGw) |\n| 550        | Resnet101-FPN | 27.3 | 34.6 | [yolact_plus_base_54_800000.pth](https://drive.google.com/file/d/15id0Qq5eqRbkD-N3ZjDZXdCvRyIaHpFB/view?usp=sharing) | [Mirror](https://ucdavis365-my.sharepoint.com/:u:/g/personal/yongjaelee_ucdavis_edu/EVQ62sF0SrJPrl_68onyHF8BpG7c05A8PavV4a849sZgEA)\n\nTo evalute the model, put the corresponding weights file in the `./weights` directory and run one of the following commands. The name of each config is everything before the numbers in the file name (e.g., `yolact_base` for `yolact_base_54_800000.pth`).\n## Quantitative Results on COCO\n```Shell\n# Quantitatively evaluate a trained model on the entire validation set. Make sure you have COCO downloaded as above.\n# This should get 29.92 validation mask mAP last time I checked.\npython eval.py --trained_model=weights/yolact_base_54_800000.pth\n\n# Output a COCOEval json to submit to the website or to use the run_coco_eval.py script.\n# This command will create './results/bbox_detections.json' and './results/mask_detections.json' for detection and instance segmentation respectively.\npython eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json\n\n# You can run COCOEval on the files created in the previous command. The performance should match my implementation in eval.py.\npython run_coco_eval.py\n\n# To output a coco json file for test-dev, make sure you have test-dev downloaded from above and go\npython eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json --dataset=coco2017_testdev_dataset\n```\n## Qualitative Results on COCO\n```Shell\n# Display qualitative results on COCO. From here on I'll use a confidence threshold of 0.15.\npython eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --display\n```\n## Benchmarking on COCO\n```Shell\n# Run just the raw model on the first 1k images of the validation set\npython eval.py --trained_model=weights/yolact_base_54_800000.pth --benchmark --max_images=1000\n```\n## Images\n```Shell\n# Display qualitative results on the specified image.\npython eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=my_image.png\n\n# Process an image and save it to another file.\npython eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=input_image.png:output_image.png\n\n# Process a whole folder of images.\npython eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --images=path/to/input/folder:path/to/output/folder\n```\n## Video\n```Shell\n# Display a video in real-time. \"--video_multiframe\" will process that many frames at once for improved performance.\n# If you want, use \"--display_fps\" to draw the FPS directly on the frame.\npython eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=my_video.mp4\n\n# Display a webcam feed in real-time. If you have multiple webcams pass the index of the webcam you want instead of 0.\npython eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=0\n\n# Process a video and save it to another file. This uses the same pipeline as the ones above now, so it's fast!\npython eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=input_video.mp4:output_video.mp4\n```\nAs you can tell, `eval.py` can do a ton of stuff. Run the `--help` command to see everything it can do.\n```Shell\npython eval.py --help\n```\n\n\n# Training\nBy default, we train on COCO. Make sure to download the entire dataset using the commands above.\n - To train, grab an imagenet-pretrained model and put it in `./weights`.\n   - For Resnet101, download `resnet101_reducedfc.pth` from [here](https://drive.google.com/file/d/1tvqFPd4bJtakOlmn-uIA492g2qurRChj/view?usp=sharing).\n   - For Resnet50, download `resnet50-19c8e357.pth` from [here](https://drive.google.com/file/d/1Jy3yCdbatgXa5YYIdTCRrSV0S9V5g1rn/view?usp=sharing).\n   - For Darknet53, download `darknet53.pth` from [here](https://drive.google.com/file/d/17Y431j4sagFpSReuPNoFcj9h7azDTZFf/view?usp=sharing).\n - Run one of the training commands below.\n   - Note that you can press ctrl+c while training and it will save an `*_interrupt.pth` file at the current iteration.\n   - All weights are saved in the `./weights` directory by default with the file name `<config>_<epoch>_<iter>.pth`.\n```Shell\n# Trains using the base config with a batch size of 8 (the default).\npython train.py --config=yolact_base_config\n\n# Trains yolact_base_config with a batch_size of 5. For the 550px models, 1 batch takes up around 1.5 gigs of VRAM, so specify accordingly.\npython train.py --config=yolact_base_config --batch_size=5\n\n# Resume training yolact_base with a specific weight file and start from the iteration specified in the weight file's name.\npython train.py --config=yolact_base_config --resume=weights/yolact_base_10_32100.pth --start_iter=-1\n\n# Use the help option to see a description of all available command line arguments\npython train.py --help\n```\n\n## Multi-GPU Support\nYOLACT now supports multiple GPUs seamlessly during training:\n\n - Before running any of the scripts, run: `export CUDA_VISIBLE_DEVICES=[gpus]`\n   - Where you should replace [gpus] with a comma separated list of the index of each GPU you want to use (e.g., 0,1,2,3).\n   - You should still do this if only using 1 GPU.\n   - You can check the indices of your GPUs with `nvidia-smi`.\n - Then, simply set the batch size to `8*num_gpus` with the training commands above. The training script will automatically scale the hyperparameters to the right values.\n   - If you have memory to spare you can increase the batch size further, but keep it a multiple of the number of GPUs you're using.\n   - If you want to allocate the images per GPU specific for different GPUs, you can use `--batch_alloc=[alloc]` where [alloc] is a comma seprated list containing the number of images on each GPU. This must sum to `batch_size`.\n\n## Logging\nYOLACT now logs training and validation information by default. You can disable this with `--no_log`. A guide on how to visualize these logs is coming soon, but now you can look at `LogVizualizer` in `utils/logger.py` for help.\n\n## Pascal SBD\nWe also include a config for training on Pascal SBD annotations (for rapid experimentation or comparing with other methods). To train on Pascal SBD, proceed with the following steps:\n 1. Download the dataset from [here](http://home.bharathh.info/pubs/codes/SBD/download.html). It's the first link in the top \"Overview\" section (and the file is called `benchmark.tgz`).\n 2. Extract the dataset somewhere. In the dataset there should be a folder called `dataset/img`. Create the directory `./data/sbd` (where `.` is YOLACT's root) and copy `dataset/img` to `./data/sbd/img`.\n 4. Download the COCO-style annotations from [here](https://drive.google.com/open?id=1ExrRSPVctHW8Nxrn0SofU1lVhK5Wn0_S).\n 5. Extract the annotations into `./data/sbd/`.\n 6. Now you can train using `--config=yolact_resnet50_pascal_config`. Check that config to see how to extend it to other models.\n\nI will automate this all with a script soon, don't worry. Also, if you want the script I used to convert the annotations, I put it in `./scripts/convert_sbd.py`, but you'll have to check how it works to be able to use it because I don't actually remember at this point.\n\nIf you want to verify our results, you can download our `yolact_resnet50_pascal_config` weights from [here](https://drive.google.com/open?id=1yLVwtkRtNxyl0kxeMCtPXJsXFFyc_FHe). This model should get 72.3 mask AP_50 and 56.2 mask AP_70. Note that the \"all\" AP isn't the same as the \"vol\" AP reported in others papers for pascal (they use an averages of the thresholds from `0.1 - 0.9` in increments of `0.1` instead of what COCO uses).\n\n## Custom Datasets\nYou can also train on your own dataset by following these steps:\n - Create a COCO-style Object Detection JSON annotation file for your dataset. The specification for this can be found [here](http://cocodataset.org/#format-data). Note that we don't use some fields, so the following may be omitted:\n   - `info`\n   - `liscense`\n   - Under `image`: `license, flickr_url, coco_url, date_captured`\n   - `categories` (we use our own format for categories, see below)\n - Create a definition for your dataset under `dataset_base` in `data/config.py` (see the comments in `dataset_base` for an explanation of each field):\n```Python\nmy_custom_dataset = dataset_base.copy({\n    'name': 'My Dataset',\n\n    'train_images': 'path_to_training_images',\n    'train_info':   'path_to_training_annotation',\n\n    'valid_images': 'path_to_validation_images',\n    'valid_info':   'path_to_validation_annotation',\n\n    'has_gt': True,\n    'class_names': ('my_class_id_1', 'my_class_id_2', 'my_class_id_3', ...)\n})\n```\n - A couple things to note:\n   - Class IDs in the annotation file should start at 1 and increase sequentially on the order of `class_names`. If this isn't the case for your annotation file (like in COCO), see the field `label_map` in `dataset_base`.\n   - If you do not want to create a validation split, use the same image path and annotations file for validation. By default (see `python train.py --help`), `train.py` will output validation mAP for the first 5000 images in the dataset every 2 epochs.\n - Finally, in `yolact_base_config` in the same file, change the value for `'dataset'` to `'my_custom_dataset'` or whatever you named the config object above. Then you can use any of the training commands in the previous section.\n\n#### Creating a Custom Dataset from Scratch\nSee [this nice post by @Amit12690](https://github.com/dbolya/yolact/issues/70#issuecomment-504283008) for tips on how to annotate a custom dataset and prepare it for use with YOLACT.\n\n\n\n\n# Citation\nIf you use YOLACT or this code base in your work, please cite\n```\n@inproceedings{yolact-iccv2019,\n  author    = {Daniel Bolya and Chong Zhou and Fanyi Xiao and Yong Jae Lee},\n  title     = {YOLACT: {Real-time} Instance Segmentation},\n  booktitle = {ICCV},\n  year      = {2019},\n}\n```\n\nFor YOLACT++, please cite\n```\n@article{yolact-plus-tpami2020,\n  author  = {Daniel Bolya and Chong Zhou and Fanyi Xiao and Yong Jae Lee},\n  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}, \n  title   = {YOLACT++: Better Real-time Instance Segmentation}, \n  year    = {2020},\n}\n```\n\n\n\n# Contact\nFor questions about our paper or code, please contact [Daniel Bolya](mailto:dbolya@ucdavis.edu).\n"
        },
        {
          "name": "backbone.py",
          "type": "blob",
          "size": 16.880859375,
          "content": "import torch\nimport torch.nn as nn\nimport pickle\n\nfrom collections import OrderedDict\n\ntry:\n    from dcn_v2 import DCN\nexcept ImportError:\n    def DCN(*args, **kwdargs):\n        raise Exception('DCN could not be imported. If you want to use YOLACT++ models, compile DCN. Check the README for instructions.')\n\nclass Bottleneck(nn.Module):\n    \"\"\" Adapted from torchvision.models.resnet \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d, dilation=1, use_dcn=False):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False, dilation=dilation)\n        self.bn1 = norm_layer(planes)\n        if use_dcn:\n            self.conv2 = DCN(planes, planes, kernel_size=3, stride=stride,\n                                padding=dilation, dilation=dilation, deformable_groups=1)\n            self.conv2.bias.data.zero_()\n            self.conv2.conv_offset_mask.weight.data.zero_()\n            self.conv2.conv_offset_mask.bias.data.zero_()\n        else:\n            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                                padding=dilation, bias=False, dilation=dilation)\n        self.bn2 = norm_layer(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False, dilation=dilation)\n        self.bn3 = norm_layer(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNetBackbone(nn.Module):\n    \"\"\" Adapted from torchvision.models.resnet \"\"\"\n\n    def __init__(self, layers, dcn_layers=[0, 0, 0, 0], dcn_interval=1, atrous_layers=[], block=Bottleneck, norm_layer=nn.BatchNorm2d):\n        super().__init__()\n\n        # These will be populated by _make_layer\n        self.num_base_layers = len(layers)\n        self.layers = nn.ModuleList()\n        self.channels = []\n        self.norm_layer = norm_layer\n        self.dilation = 1\n        self.atrous_layers = atrous_layers\n\n        # From torchvision.models.resnet.Resnet\n        self.inplanes = 64\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self._make_layer(block, 64, layers[0], dcn_layers=dcn_layers[0], dcn_interval=dcn_interval)\n        self._make_layer(block, 128, layers[1], stride=2, dcn_layers=dcn_layers[1], dcn_interval=dcn_interval)\n        self._make_layer(block, 256, layers[2], stride=2, dcn_layers=dcn_layers[2], dcn_interval=dcn_interval)\n        self._make_layer(block, 512, layers[3], stride=2, dcn_layers=dcn_layers[3], dcn_interval=dcn_interval)\n\n        # This contains every module that should be initialized by loading in pretrained weights.\n        # Any extra layers added onto this that won't be initialized by init_backbone will not be\n        # in this list. That way, Yolact::init_weights knows which backbone weights to initialize\n        # with xavier, and which ones to leave alone.\n        self.backbone_modules = [m for m in self.modules() if isinstance(m, nn.Conv2d)]\n        \n    \n    def _make_layer(self, block, planes, blocks, stride=1, dcn_layers=0, dcn_interval=1):\n        \"\"\" Here one layer means a string of n Bottleneck blocks. \"\"\"\n        downsample = None\n\n        # This is actually just to create the connection between layers, and not necessarily to\n        # downsample. Even if the second condition is met, it only downsamples when stride != 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if len(self.layers) in self.atrous_layers:\n                self.dilation += 1\n                stride = 1\n            \n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False,\n                          dilation=self.dilation),\n                self.norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        use_dcn = (dcn_layers >= blocks)\n        layers.append(block(self.inplanes, planes, stride, downsample, self.norm_layer, self.dilation, use_dcn=use_dcn))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            use_dcn = ((i+dcn_layers) >= blocks) and (i % dcn_interval == 0)\n            layers.append(block(self.inplanes, planes, norm_layer=self.norm_layer, use_dcn=use_dcn))\n        layer = nn.Sequential(*layers)\n\n        self.channels.append(planes * block.expansion)\n        self.layers.append(layer)\n\n        return layer\n\n    def forward(self, x):\n        \"\"\" Returns a list of convouts for each layer. \"\"\"\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        outs = []\n        for layer in self.layers:\n            x = layer(x)\n            outs.append(x)\n\n        return tuple(outs)\n\n    def init_backbone(self, path):\n        \"\"\" Initializes the backbone weights for training. \"\"\"\n        state_dict = torch.load(path)\n\n        # Replace layer1 -> layers.0 etc.\n        keys = list(state_dict)\n        for key in keys:\n            if key.startswith('layer'):\n                idx = int(key[5])\n                new_key = 'layers.' + str(idx-1) + key[6:]\n                state_dict[new_key] = state_dict.pop(key)\n\n        # Note: Using strict=False is berry scary. Triple check this.\n        self.load_state_dict(state_dict, strict=False)\n\n    def add_layer(self, conv_channels=1024, downsample=2, depth=1, block=Bottleneck):\n        \"\"\" Add a downsample layer to the backbone as per what SSD does. \"\"\"\n        self._make_layer(block, conv_channels // block.expansion, blocks=depth, stride=downsample)\n\n\n\n\nclass ResNetBackboneGN(ResNetBackbone):\n\n    def __init__(self, layers, num_groups=32):\n        super().__init__(layers, norm_layer=lambda x: nn.GroupNorm(num_groups, x))\n\n    def init_backbone(self, path):\n        \"\"\" The path here comes from detectron. So we load it differently. \"\"\"\n        with open(path, 'rb') as f:\n            state_dict = pickle.load(f, encoding='latin1') # From the detectron source\n            state_dict = state_dict['blobs']\n        \n        our_state_dict_keys = list(self.state_dict().keys())\n        new_state_dict = {}\n    \n        gn_trans     = lambda x: ('gn_s' if x == 'weight' else 'gn_b')\n        layeridx2res = lambda x: 'res' + str(int(x)+2)\n        block2branch = lambda x: 'branch2' + ('a', 'b', 'c')[int(x[-1:])-1]\n\n        # Transcribe each Detectron weights name to a Yolact weights name\n        for key in our_state_dict_keys:\n            parts = key.split('.')\n            transcribed_key = ''\n\n            if (parts[0] == 'conv1'):\n                transcribed_key = 'conv1_w'\n            elif (parts[0] == 'bn1'):\n                transcribed_key = 'conv1_' + gn_trans(parts[1])\n            elif (parts[0] == 'layers'):\n                if int(parts[1]) >= self.num_base_layers: continue\n\n                transcribed_key = layeridx2res(parts[1])\n                transcribed_key += '_' + parts[2] + '_'\n\n                if parts[3] == 'downsample':\n                    transcribed_key += 'branch1_'\n                    \n                    if parts[4] == '0':\n                        transcribed_key += 'w'\n                    else:\n                        transcribed_key += gn_trans(parts[5])\n                else:\n                    transcribed_key += block2branch(parts[3]) + '_'\n\n                    if 'conv' in parts[3]:\n                        transcribed_key += 'w'\n                    else:\n                        transcribed_key += gn_trans(parts[4])\n\n            new_state_dict[key] = torch.Tensor(state_dict[transcribed_key])\n        \n        # strict=False because we may have extra unitialized layers at this point\n        self.load_state_dict(new_state_dict, strict=False)\n\n\n\n\n\n\n\ndef darknetconvlayer(in_channels, out_channels, *args, **kwdargs):\n    \"\"\"\n    Implements a conv, activation, then batch norm.\n    Arguments are passed into the conv layer.\n    \"\"\"\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, *args, **kwdargs, bias=False),\n        nn.BatchNorm2d(out_channels),\n        # Darknet uses 0.1 here.\n        # See https://github.com/pjreddie/darknet/blob/680d3bde1924c8ee2d1c1dea54d3e56a05ca9a26/src/activations.h#L39\n        nn.LeakyReLU(0.1, inplace=True)\n    )\n\nclass DarkNetBlock(nn.Module):\n    \"\"\" Note: channels is the lesser of the two. The output will be expansion * channels. \"\"\"\n\n    expansion = 2\n\n    def __init__(self, in_channels, channels):\n        super().__init__()\n\n        self.conv1 = darknetconvlayer(in_channels, channels,                  kernel_size=1)\n        self.conv2 = darknetconvlayer(channels,    channels * self.expansion, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        return self.conv2(self.conv1(x)) + x\n\n\n\n\nclass DarkNetBackbone(nn.Module):\n    \"\"\"\n    An implementation of YOLOv3's Darnet53 in\n    https://pjreddie.com/media/files/papers/YOLOv3.pdf\n\n    This is based off of the implementation of Resnet above.\n    \"\"\"\n\n    def __init__(self, layers=[1, 2, 8, 8, 4], block=DarkNetBlock):\n        super().__init__()\n\n        # These will be populated by _make_layer\n        self.num_base_layers = len(layers)\n        self.layers = nn.ModuleList()\n        self.channels = []\n        \n        self._preconv = darknetconvlayer(3, 32, kernel_size=3, padding=1)\n        self.in_channels = 32\n        \n        self._make_layer(block, 32,  layers[0])\n        self._make_layer(block, 64,  layers[1])\n        self._make_layer(block, 128, layers[2])\n        self._make_layer(block, 256, layers[3])\n        self._make_layer(block, 512, layers[4])\n\n        # This contains every module that should be initialized by loading in pretrained weights.\n        # Any extra layers added onto this that won't be initialized by init_backbone will not be\n        # in this list. That way, Yolact::init_weights knows which backbone weights to initialize\n        # with xavier, and which ones to leave alone.\n        self.backbone_modules = [m for m in self.modules() if isinstance(m, nn.Conv2d)]\n    \n    def _make_layer(self, block, channels, num_blocks, stride=2):\n        \"\"\" Here one layer means a string of n blocks. \"\"\"\n        layer_list = []\n\n        # The downsample layer\n        layer_list.append(\n            darknetconvlayer(self.in_channels, channels * block.expansion,\n                             kernel_size=3, padding=1, stride=stride))\n\n        # Each block inputs channels and outputs channels * expansion\n        self.in_channels = channels * block.expansion\n        layer_list += [block(self.in_channels, channels) for _ in range(num_blocks)]\n\n        self.channels.append(self.in_channels)\n        self.layers.append(nn.Sequential(*layer_list))\n\n    def forward(self, x):\n        \"\"\" Returns a list of convouts for each layer. \"\"\"\n\n        x = self._preconv(x)\n\n        outs = []\n        for layer in self.layers:\n            x = layer(x)\n            outs.append(x)\n\n        return tuple(outs)\n\n    def add_layer(self, conv_channels=1024, stride=2, depth=1, block=DarkNetBlock):\n        \"\"\" Add a downsample layer to the backbone as per what SSD does. \"\"\"\n        self._make_layer(block, conv_channels // block.expansion, num_blocks=depth, stride=stride)\n    \n    def init_backbone(self, path):\n        \"\"\" Initializes the backbone weights for training. \"\"\"\n        # Note: Using strict=False is berry scary. Triple check this.\n        self.load_state_dict(torch.load(path), strict=False)\n\n\n\n\n\nclass VGGBackbone(nn.Module):\n    \"\"\"\n    Args:\n        - cfg: A list of layers given as lists. Layers can be either 'M' signifying\n                a max pooling layer, a number signifying that many feature maps in\n                a conv layer, or a tuple of 'M' or a number and a kwdargs dict to pass\n                into the function that creates the layer (e.g. nn.MaxPool2d for 'M').\n        - extra_args: A list of lists of arguments to pass into add_layer.\n        - norm_layers: Layers indices that need to pass through an l2norm layer.\n    \"\"\"\n\n    def __init__(self, cfg, extra_args=[], norm_layers=[]):\n        super().__init__()\n        \n        self.channels = []\n        self.layers = nn.ModuleList()\n        self.in_channels = 3\n        self.extra_args = list(reversed(extra_args)) # So I can use it as a stack\n\n        # Keeps track of what the corresponding key will be in the state dict of the\n        # pretrained model. For instance, layers.0.2 for us is 2 for the pretrained\n        # model but layers.1.1 is 5.\n        self.total_layer_count = 0\n        self.state_dict_lookup = {}\n\n        for idx, layer_cfg in enumerate(cfg):\n            self._make_layer(layer_cfg)\n\n        self.norms = nn.ModuleList([nn.BatchNorm2d(self.channels[l]) for l in norm_layers])\n        self.norm_lookup = {l: idx for idx, l in enumerate(norm_layers)}\n\n        # These modules will be initialized by init_backbone,\n        # so don't overwrite their initialization later.\n        self.backbone_modules = [m for m in self.modules() if isinstance(m, nn.Conv2d)]\n\n    def _make_layer(self, cfg):\n        \"\"\"\n        Each layer is a sequence of conv layers usually preceded by a max pooling.\n        Adapted from torchvision.models.vgg.make_layers.\n        \"\"\"\n\n        layers = []\n\n        for v in cfg:\n            # VGG in SSD requires some special layers, so allow layers to be tuples of\n            # (<M or num_features>, kwdargs dict)\n            args = None\n            if isinstance(v, tuple):\n                args = v[1]\n                v = v[0]\n\n            # v should be either M or a number\n            if v == 'M':\n                # Set default arguments\n                if args is None:\n                    args = {'kernel_size': 2, 'stride': 2}\n\n                layers.append(nn.MaxPool2d(**args))\n            else:\n                # See the comment in __init__ for an explanation of this\n                cur_layer_idx = self.total_layer_count + len(layers)\n                self.state_dict_lookup[cur_layer_idx] = '%d.%d' % (len(self.layers), len(layers))\n\n                # Set default arguments\n                if args is None:\n                    args = {'kernel_size': 3, 'padding': 1}\n\n                # Add the layers\n                layers.append(nn.Conv2d(self.in_channels, v, **args))\n                layers.append(nn.ReLU(inplace=True))\n                self.in_channels = v\n        \n        self.total_layer_count += len(layers)\n        self.channels.append(self.in_channels)\n        self.layers.append(nn.Sequential(*layers))\n\n    def forward(self, x):\n        \"\"\" Returns a list of convouts for each layer. \"\"\"\n        outs = []\n\n        for idx, layer in enumerate(self.layers):\n            x = layer(x)\n            \n            # Apply an l2norm module to the selected layers\n            # Note that this differs from the original implemenetation\n            if idx in self.norm_lookup:\n                x = self.norms[self.norm_lookup[idx]](x)\n            outs.append(x)\n        \n        return tuple(outs)\n\n    def transform_key(self, k):\n        \"\"\" Transform e.g. features.24.bias to layers.4.1.bias \"\"\"\n        vals = k.split('.')\n        layerIdx = self.state_dict_lookup[int(vals[0])]\n        return 'layers.%s.%s' % (layerIdx, vals[1])\n\n    def init_backbone(self, path):\n        \"\"\" Initializes the backbone weights for training. \"\"\"\n        state_dict = torch.load(path)\n        state_dict = OrderedDict([(self.transform_key(k), v) for k,v in state_dict.items()])\n\n        self.load_state_dict(state_dict, strict=False)\n\n    def add_layer(self, conv_channels=128, downsample=2):\n        \"\"\" Add a downsample layer to the backbone as per what SSD does. \"\"\"\n        if len(self.extra_args) > 0:\n            conv_channels, downsample = self.extra_args.pop()\n        \n        padding = 1 if downsample > 1 else 0\n        \n        layer = nn.Sequential(\n            nn.Conv2d(self.in_channels, conv_channels, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(conv_channels, conv_channels*2, kernel_size=3, stride=downsample, padding=padding),\n            nn.ReLU(inplace=True)\n        )\n\n        self.in_channels = conv_channels*2\n        self.channels.append(self.in_channels)\n        self.layers.append(layer)\n        \n                \n\n\ndef construct_backbone(cfg):\n    \"\"\" Constructs a backbone given a backbone config object (see config.py). \"\"\"\n    backbone = cfg.type(*cfg.args)\n\n    # Add downsampling layers until we reach the number we need\n    num_layers = max(cfg.selected_layers) + 1\n\n    while len(backbone.layers) < num_layers:\n        backbone.add_layer()\n\n    return backbone\n"
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 0.90625,
          "content": "# Installs dependencies for YOLACT managed by Anaconda. \n# Advantage is you get working CUDA+cuDNN+pytorch+torchvison versions.\n#\n# TODO: you must additionally install nVidia drivers, eg. on Ubuntu linux \n# `apt install nvidia-driver-440` (change the 440 for whatever version you need/have).\n#\nname: yolact-env\n#prefix: /your/custom/path/envs/yolact-env\nchannels:\n  - conda-forge\n  - pytorch\n  - defaults\ndependencies:\n  - python==3.7\n  - pip\n  - cython \n  - pytorch::torchvision\n  - pytorch::pytorch >=1.0.1\n  - cudatoolkit\n  - cudnn\n  - pytorch::cuda100\n  - matplotlib\n  - git # to download COCO dataset\n  - curl # to download COCO dataset\n  - unzip # to download COCO dataset\n  - conda-forge::bash # to download COCO dataset\n  - pip:\n    - opencv-python \n    - pillow <7.0 # bug PILLOW_VERSION in torchvision, must be < 7.0 until torchvision is upgraded\n    - pycocotools \n    - PyQt5 # needed on KDE/Qt envs for matplotlib\n\n"
        },
        {
          "name": "eval.py",
          "type": "blob",
          "size": 45.8154296875,
          "content": "from data import COCODetection, get_label_map, MEANS, COLORS\nfrom yolact import Yolact\nfrom utils.augmentations import BaseTransform, FastBaseTransform, Resize\nfrom utils.functions import MovingAverage, ProgressBar\nfrom layers.box_utils import jaccard, center_size, mask_iou\nfrom utils import timer\nfrom utils.functions import SavePath\nfrom layers.output_utils import postprocess, undo_image_transformation\nimport pycocotools\n\nfrom data import cfg, set_cfg, set_dataset\n\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nimport argparse\nimport time\nimport random\nimport cProfile\nimport pickle\nimport json\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom collections import OrderedDict\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef str2bool(v):\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\ndef parse_args(argv=None):\n    parser = argparse.ArgumentParser(\n        description='YOLACT COCO Evaluation')\n    parser.add_argument('--trained_model',\n                        default='weights/ssd300_mAP_77.43_v2.pth', type=str,\n                        help='Trained state_dict file path to open. If \"interrupt\", this will open the interrupt file.')\n    parser.add_argument('--top_k', default=5, type=int,\n                        help='Further restrict the number of predictions to parse')\n    parser.add_argument('--cuda', default=True, type=str2bool,\n                        help='Use cuda to evaulate model')\n    parser.add_argument('--fast_nms', default=True, type=str2bool,\n                        help='Whether to use a faster, but not entirely correct version of NMS.')\n    parser.add_argument('--cross_class_nms', default=False, type=str2bool,\n                        help='Whether compute NMS cross-class or per-class.')\n    parser.add_argument('--display_masks', default=True, type=str2bool,\n                        help='Whether or not to display masks over bounding boxes')\n    parser.add_argument('--display_bboxes', default=True, type=str2bool,\n                        help='Whether or not to display bboxes around masks')\n    parser.add_argument('--display_text', default=True, type=str2bool,\n                        help='Whether or not to display text (class [score])')\n    parser.add_argument('--display_scores', default=True, type=str2bool,\n                        help='Whether or not to display scores in addition to classes')\n    parser.add_argument('--display', dest='display', action='store_true',\n                        help='Display qualitative results instead of quantitative ones.')\n    parser.add_argument('--shuffle', dest='shuffle', action='store_true',\n                        help='Shuffles the images when displaying them. Doesn\\'t have much of an effect when display is off though.')\n    parser.add_argument('--ap_data_file', default='results/ap_data.pkl', type=str,\n                        help='In quantitative mode, the file to save detections before calculating mAP.')\n    parser.add_argument('--resume', dest='resume', action='store_true',\n                        help='If display not set, this resumes mAP calculations from the ap_data_file.')\n    parser.add_argument('--max_images', default=-1, type=int,\n                        help='The maximum number of images from the dataset to consider. Use -1 for all.')\n    parser.add_argument('--output_coco_json', dest='output_coco_json', action='store_true',\n                        help='If display is not set, instead of processing IoU values, this just dumps detections into the coco json file.')\n    parser.add_argument('--bbox_det_file', default='results/bbox_detections.json', type=str,\n                        help='The output file for coco bbox results if --coco_results is set.')\n    parser.add_argument('--mask_det_file', default='results/mask_detections.json', type=str,\n                        help='The output file for coco mask results if --coco_results is set.')\n    parser.add_argument('--config', default=None,\n                        help='The config object to use.')\n    parser.add_argument('--output_web_json', dest='output_web_json', action='store_true',\n                        help='If display is not set, instead of processing IoU values, this dumps detections for usage with the detections viewer web thingy.')\n    parser.add_argument('--web_det_path', default='web/dets/', type=str,\n                        help='If output_web_json is set, this is the path to dump detections into.')\n    parser.add_argument('--no_bar', dest='no_bar', action='store_true',\n                        help='Do not output the status bar. This is useful for when piping to a file.')\n    parser.add_argument('--display_lincomb', default=False, type=str2bool,\n                        help='If the config uses lincomb masks, output a visualization of how those masks are created.')\n    parser.add_argument('--benchmark', default=False, dest='benchmark', action='store_true',\n                        help='Equivalent to running display mode but without displaying an image.')\n    parser.add_argument('--no_sort', default=False, dest='no_sort', action='store_true',\n                        help='Do not sort images by hashed image ID.')\n    parser.add_argument('--seed', default=None, type=int,\n                        help='The seed to pass into random.seed. Note: this is only really for the shuffle and does not (I think) affect cuda stuff.')\n    parser.add_argument('--mask_proto_debug', default=False, dest='mask_proto_debug', action='store_true',\n                        help='Outputs stuff for scripts/compute_mask.py.')\n    parser.add_argument('--no_crop', default=False, dest='crop', action='store_false',\n                        help='Do not crop output masks with the predicted bounding box.')\n    parser.add_argument('--image', default=None, type=str,\n                        help='A path to an image to use for display.')\n    parser.add_argument('--images', default=None, type=str,\n                        help='An input folder of images and output folder to save detected images. Should be in the format input->output.')\n    parser.add_argument('--video', default=None, type=str,\n                        help='A path to a video to evaluate on. Passing in a number will use that index webcam.')\n    parser.add_argument('--video_multiframe', default=1, type=int,\n                        help='The number of frames to evaluate in parallel to make videos play at higher fps.')\n    parser.add_argument('--score_threshold', default=0, type=float,\n                        help='Detections with a score under this threshold will not be considered. This currently only works in display mode.')\n    parser.add_argument('--dataset', default=None, type=str,\n                        help='If specified, override the dataset specified in the config with this one (example: coco2017_dataset).')\n    parser.add_argument('--detect', default=False, dest='detect', action='store_true',\n                        help='Don\\'t evauluate the mask branch at all and only do object detection. This only works for --display and --benchmark.')\n    parser.add_argument('--display_fps', default=False, dest='display_fps', action='store_true',\n                        help='When displaying / saving video, draw the FPS on the frame')\n    parser.add_argument('--emulate_playback', default=False, dest='emulate_playback', action='store_true',\n                        help='When saving a video, emulate the framerate that you\\'d get running in real-time mode.')\n\n    parser.set_defaults(no_bar=False, display=False, resume=False, output_coco_json=False, output_web_json=False, shuffle=False,\n                        benchmark=False, no_sort=False, no_hash=False, mask_proto_debug=False, crop=True, detect=False, display_fps=False,\n                        emulate_playback=False)\n\n    global args\n    args = parser.parse_args(argv)\n\n    if args.output_web_json:\n        args.output_coco_json = True\n    \n    if args.seed is not None:\n        random.seed(args.seed)\n\niou_thresholds = [x / 100 for x in range(50, 100, 5)]\ncoco_cats = {} # Call prep_coco_cats to fill this\ncoco_cats_inv = {}\ncolor_cache = defaultdict(lambda: {})\n\ndef prep_display(dets_out, img, h, w, undo_transform=True, class_color=False, mask_alpha=0.45, fps_str=''):\n    \"\"\"\n    Note: If undo_transform=False then im_h and im_w are allowed to be None.\n    \"\"\"\n    if undo_transform:\n        img_numpy = undo_image_transformation(img, w, h)\n        img_gpu = torch.Tensor(img_numpy).cuda()\n    else:\n        img_gpu = img / 255.0\n        h, w, _ = img.shape\n    \n    with timer.env('Postprocess'):\n        save = cfg.rescore_bbox\n        cfg.rescore_bbox = True\n        t = postprocess(dets_out, w, h, visualize_lincomb = args.display_lincomb,\n                                        crop_masks        = args.crop,\n                                        score_threshold   = args.score_threshold)\n        cfg.rescore_bbox = save\n\n    with timer.env('Copy'):\n        idx = t[1].argsort(0, descending=True)[:args.top_k]\n        \n        if cfg.eval_mask_branch:\n            # Masks are drawn on the GPU, so don't copy\n            masks = t[3][idx]\n        classes, scores, boxes = [x[idx].cpu().numpy() for x in t[:3]]\n\n    num_dets_to_consider = min(args.top_k, classes.shape[0])\n    for j in range(num_dets_to_consider):\n        if scores[j] < args.score_threshold:\n            num_dets_to_consider = j\n            break\n\n    # Quick and dirty lambda for selecting the color for a particular index\n    # Also keeps track of a per-gpu color cache for maximum speed\n    def get_color(j, on_gpu=None):\n        global color_cache\n        color_idx = (classes[j] * 5 if class_color else j * 5) % len(COLORS)\n        \n        if on_gpu is not None and color_idx in color_cache[on_gpu]:\n            return color_cache[on_gpu][color_idx]\n        else:\n            color = COLORS[color_idx]\n            if not undo_transform:\n                # The image might come in as RGB or BRG, depending\n                color = (color[2], color[1], color[0])\n            if on_gpu is not None:\n                color = torch.Tensor(color).to(on_gpu).float() / 255.\n                color_cache[on_gpu][color_idx] = color\n            return color\n\n    # First, draw the masks on the GPU where we can do it really fast\n    # Beware: very fast but possibly unintelligible mask-drawing code ahead\n    # I wish I had access to OpenGL or Vulkan but alas, I guess Pytorch tensor operations will have to suffice\n    if args.display_masks and cfg.eval_mask_branch and num_dets_to_consider > 0:\n        # After this, mask is of size [num_dets, h, w, 1]\n        masks = masks[:num_dets_to_consider, :, :, None]\n        \n        # Prepare the RGB images for each mask given their color (size [num_dets, h, w, 1])\n        colors = torch.cat([get_color(j, on_gpu=img_gpu.device.index).view(1, 1, 1, 3) for j in range(num_dets_to_consider)], dim=0)\n        masks_color = masks.repeat(1, 1, 1, 3) * colors * mask_alpha\n\n        # This is 1 everywhere except for 1-mask_alpha where the mask is\n        inv_alph_masks = masks * (-mask_alpha) + 1\n        \n        # I did the math for this on pen and paper. This whole block should be equivalent to:\n        #    for j in range(num_dets_to_consider):\n        #        img_gpu = img_gpu * inv_alph_masks[j] + masks_color[j]\n        masks_color_summand = masks_color[0]\n        if num_dets_to_consider > 1:\n            inv_alph_cumul = inv_alph_masks[:(num_dets_to_consider-1)].cumprod(dim=0)\n            masks_color_cumul = masks_color[1:] * inv_alph_cumul\n            masks_color_summand += masks_color_cumul.sum(dim=0)\n\n        img_gpu = img_gpu * inv_alph_masks.prod(dim=0) + masks_color_summand\n    \n    if args.display_fps:\n            # Draw the box for the fps on the GPU\n        font_face = cv2.FONT_HERSHEY_DUPLEX\n        font_scale = 0.6\n        font_thickness = 1\n\n        text_w, text_h = cv2.getTextSize(fps_str, font_face, font_scale, font_thickness)[0]\n\n        img_gpu[0:text_h+8, 0:text_w+8] *= 0.6 # 1 - Box alpha\n\n\n    # Then draw the stuff that needs to be done on the cpu\n    # Note, make sure this is a uint8 tensor or opencv will not anti alias text for whatever reason\n    img_numpy = (img_gpu * 255).byte().cpu().numpy()\n\n    if args.display_fps:\n        # Draw the text on the CPU\n        text_pt = (4, text_h + 2)\n        text_color = [255, 255, 255]\n\n        cv2.putText(img_numpy, fps_str, text_pt, font_face, font_scale, text_color, font_thickness, cv2.LINE_AA)\n    \n    if num_dets_to_consider == 0:\n        return img_numpy\n\n    if args.display_text or args.display_bboxes:\n        for j in reversed(range(num_dets_to_consider)):\n            x1, y1, x2, y2 = boxes[j, :]\n            color = get_color(j)\n            score = scores[j]\n\n            if args.display_bboxes:\n                cv2.rectangle(img_numpy, (x1, y1), (x2, y2), color, 1)\n\n            if args.display_text:\n                _class = cfg.dataset.class_names[classes[j]]\n                text_str = '%s: %.2f' % (_class, score) if args.display_scores else _class\n\n                font_face = cv2.FONT_HERSHEY_DUPLEX\n                font_scale = 0.6\n                font_thickness = 1\n\n                text_w, text_h = cv2.getTextSize(text_str, font_face, font_scale, font_thickness)[0]\n\n                text_pt = (x1, y1 - 3)\n                text_color = [255, 255, 255]\n\n                cv2.rectangle(img_numpy, (x1, y1), (x1 + text_w, y1 - text_h - 4), color, -1)\n                cv2.putText(img_numpy, text_str, text_pt, font_face, font_scale, text_color, font_thickness, cv2.LINE_AA)\n            \n    \n    return img_numpy\n\ndef prep_benchmark(dets_out, h, w):\n    with timer.env('Postprocess'):\n        t = postprocess(dets_out, w, h, crop_masks=args.crop, score_threshold=args.score_threshold)\n\n    with timer.env('Copy'):\n        classes, scores, boxes, masks = [x[:args.top_k] for x in t]\n        if isinstance(scores, list):\n            box_scores = scores[0].cpu().numpy()\n            mask_scores = scores[1].cpu().numpy()\n        else:\n            scores = scores.cpu().numpy()\n        classes = classes.cpu().numpy()\n        boxes = boxes.cpu().numpy()\n        masks = masks.cpu().numpy()\n    \n    with timer.env('Sync'):\n        # Just in case\n        torch.cuda.synchronize()\n\ndef prep_coco_cats():\n    \"\"\" Prepare inverted table for category id lookup given a coco cats object. \"\"\"\n    for coco_cat_id, transformed_cat_id_p1 in get_label_map().items():\n        transformed_cat_id = transformed_cat_id_p1 - 1\n        coco_cats[transformed_cat_id] = coco_cat_id\n        coco_cats_inv[coco_cat_id] = transformed_cat_id\n\n\ndef get_coco_cat(transformed_cat_id):\n    \"\"\" transformed_cat_id is [0,80) as indices in cfg.dataset.class_names \"\"\"\n    return coco_cats[transformed_cat_id]\n\ndef get_transformed_cat(coco_cat_id):\n    \"\"\" transformed_cat_id is [0,80) as indices in cfg.dataset.class_names \"\"\"\n    return coco_cats_inv[coco_cat_id]\n\n\nclass Detections:\n\n    def __init__(self):\n        self.bbox_data = []\n        self.mask_data = []\n\n    def add_bbox(self, image_id:int, category_id:int, bbox:list, score:float):\n        \"\"\" Note that bbox should be a list or tuple of (x1, y1, x2, y2) \"\"\"\n        bbox = [bbox[0], bbox[1], bbox[2]-bbox[0], bbox[3]-bbox[1]]\n\n        # Round to the nearest 10th to avoid huge file sizes, as COCO suggests\n        bbox = [round(float(x)*10)/10 for x in bbox]\n\n        self.bbox_data.append({\n            'image_id': int(image_id),\n            'category_id': get_coco_cat(int(category_id)),\n            'bbox': bbox,\n            'score': float(score)\n        })\n\n    def add_mask(self, image_id:int, category_id:int, segmentation:np.ndarray, score:float):\n        \"\"\" The segmentation should be the full mask, the size of the image and with size [h, w]. \"\"\"\n        rle = pycocotools.mask.encode(np.asfortranarray(segmentation.astype(np.uint8)))\n        rle['counts'] = rle['counts'].decode('ascii') # json.dump doesn't like bytes strings\n\n        self.mask_data.append({\n            'image_id': int(image_id),\n            'category_id': get_coco_cat(int(category_id)),\n            'segmentation': rle,\n            'score': float(score)\n        })\n    \n    def dump(self):\n        dump_arguments = [\n            (self.bbox_data, args.bbox_det_file),\n            (self.mask_data, args.mask_det_file)\n        ]\n\n        for data, path in dump_arguments:\n            with open(path, 'w') as f:\n                json.dump(data, f)\n    \n    def dump_web(self):\n        \"\"\" Dumps it in the format for my web app. Warning: bad code ahead! \"\"\"\n        config_outs = ['preserve_aspect_ratio', 'use_prediction_module',\n                        'use_yolo_regressors', 'use_prediction_matching',\n                        'train_masks']\n\n        output = {\n            'info' : {\n                'Config': {key: getattr(cfg, key) for key in config_outs},\n            }\n        }\n\n        image_ids = list(set([x['image_id'] for x in self.bbox_data]))\n        image_ids.sort()\n        image_lookup = {_id: idx for idx, _id in enumerate(image_ids)}\n\n        output['images'] = [{'image_id': image_id, 'dets': []} for image_id in image_ids]\n\n        # These should already be sorted by score with the way prep_metrics works.\n        for bbox, mask in zip(self.bbox_data, self.mask_data):\n            image_obj = output['images'][image_lookup[bbox['image_id']]]\n            image_obj['dets'].append({\n                'score': bbox['score'],\n                'bbox': bbox['bbox'],\n                'category': cfg.dataset.class_names[get_transformed_cat(bbox['category_id'])],\n                'mask': mask['segmentation'],\n            })\n\n        with open(os.path.join(args.web_det_path, '%s.json' % cfg.name), 'w') as f:\n            json.dump(output, f)\n        \n\n        \n\ndef _mask_iou(mask1, mask2, iscrowd=False):\n    with timer.env('Mask IoU'):\n        ret = mask_iou(mask1, mask2, iscrowd)\n    return ret.cpu()\n\ndef _bbox_iou(bbox1, bbox2, iscrowd=False):\n    with timer.env('BBox IoU'):\n        ret = jaccard(bbox1, bbox2, iscrowd)\n    return ret.cpu()\n\ndef prep_metrics(ap_data, dets, img, gt, gt_masks, h, w, num_crowd, image_id, detections:Detections=None):\n    \"\"\" Returns a list of APs for this image, with each element being for a class  \"\"\"\n    if not args.output_coco_json:\n        with timer.env('Prepare gt'):\n            gt_boxes = torch.Tensor(gt[:, :4])\n            gt_boxes[:, [0, 2]] *= w\n            gt_boxes[:, [1, 3]] *= h\n            gt_classes = list(gt[:, 4].astype(int))\n            gt_masks = torch.Tensor(gt_masks).view(-1, h*w)\n\n            if num_crowd > 0:\n                split = lambda x: (x[-num_crowd:], x[:-num_crowd])\n                crowd_boxes  , gt_boxes   = split(gt_boxes)\n                crowd_masks  , gt_masks   = split(gt_masks)\n                crowd_classes, gt_classes = split(gt_classes)\n\n    with timer.env('Postprocess'):\n        classes, scores, boxes, masks = postprocess(dets, w, h, crop_masks=args.crop, score_threshold=args.score_threshold)\n\n        if classes.size(0) == 0:\n            return\n\n        classes = list(classes.cpu().numpy().astype(int))\n        if isinstance(scores, list):\n            box_scores = list(scores[0].cpu().numpy().astype(float))\n            mask_scores = list(scores[1].cpu().numpy().astype(float))\n        else:\n            scores = list(scores.cpu().numpy().astype(float))\n            box_scores = scores\n            mask_scores = scores\n        masks = masks.view(-1, h*w).cuda()\n        boxes = boxes.cuda()\n\n\n    if args.output_coco_json:\n        with timer.env('JSON Output'):\n            boxes = boxes.cpu().numpy()\n            masks = masks.view(-1, h, w).cpu().numpy()\n            for i in range(masks.shape[0]):\n                # Make sure that the bounding box actually makes sense and a mask was produced\n                if (boxes[i, 3] - boxes[i, 1]) * (boxes[i, 2] - boxes[i, 0]) > 0:\n                    detections.add_bbox(image_id, classes[i], boxes[i,:],   box_scores[i])\n                    detections.add_mask(image_id, classes[i], masks[i,:,:], mask_scores[i])\n            return\n    \n    with timer.env('Eval Setup'):\n        num_pred = len(classes)\n        num_gt   = len(gt_classes)\n\n        mask_iou_cache = _mask_iou(masks, gt_masks)\n        bbox_iou_cache = _bbox_iou(boxes.float(), gt_boxes.float())\n\n        if num_crowd > 0:\n            crowd_mask_iou_cache = _mask_iou(masks, crowd_masks, iscrowd=True)\n            crowd_bbox_iou_cache = _bbox_iou(boxes.float(), crowd_boxes.float(), iscrowd=True)\n        else:\n            crowd_mask_iou_cache = None\n            crowd_bbox_iou_cache = None\n\n        box_indices = sorted(range(num_pred), key=lambda i: -box_scores[i])\n        mask_indices = sorted(box_indices, key=lambda i: -mask_scores[i])\n\n        iou_types = [\n            ('box',  lambda i,j: bbox_iou_cache[i, j].item(),\n                     lambda i,j: crowd_bbox_iou_cache[i,j].item(),\n                     lambda i: box_scores[i], box_indices),\n            ('mask', lambda i,j: mask_iou_cache[i, j].item(),\n                     lambda i,j: crowd_mask_iou_cache[i,j].item(),\n                     lambda i: mask_scores[i], mask_indices)\n        ]\n\n    timer.start('Main loop')\n    for _class in set(classes + gt_classes):\n        ap_per_iou = []\n        num_gt_for_class = sum([1 for x in gt_classes if x == _class])\n        \n        for iouIdx in range(len(iou_thresholds)):\n            iou_threshold = iou_thresholds[iouIdx]\n\n            for iou_type, iou_func, crowd_func, score_func, indices in iou_types:\n                gt_used = [False] * len(gt_classes)\n                \n                ap_obj = ap_data[iou_type][iouIdx][_class]\n                ap_obj.add_gt_positives(num_gt_for_class)\n\n                for i in indices:\n                    if classes[i] != _class:\n                        continue\n                    \n                    max_iou_found = iou_threshold\n                    max_match_idx = -1\n                    for j in range(num_gt):\n                        if gt_used[j] or gt_classes[j] != _class:\n                            continue\n                            \n                        iou = iou_func(i, j)\n\n                        if iou > max_iou_found:\n                            max_iou_found = iou\n                            max_match_idx = j\n                    \n                    if max_match_idx >= 0:\n                        gt_used[max_match_idx] = True\n                        ap_obj.push(score_func(i), True)\n                    else:\n                        # If the detection matches a crowd, we can just ignore it\n                        matched_crowd = False\n\n                        if num_crowd > 0:\n                            for j in range(len(crowd_classes)):\n                                if crowd_classes[j] != _class:\n                                    continue\n                                \n                                iou = crowd_func(i, j)\n\n                                if iou > iou_threshold:\n                                    matched_crowd = True\n                                    break\n\n                        # All this crowd code so that we can make sure that our eval code gives the\n                        # same result as COCOEval. There aren't even that many crowd annotations to\n                        # begin with, but accuracy is of the utmost importance.\n                        if not matched_crowd:\n                            ap_obj.push(score_func(i), False)\n    timer.stop('Main loop')\n\n\nclass APDataObject:\n    \"\"\"\n    Stores all the information necessary to calculate the AP for one IoU and one class.\n    Note: I type annotated this because why not.\n    \"\"\"\n\n    def __init__(self):\n        self.data_points = []\n        self.num_gt_positives = 0\n\n    def push(self, score:float, is_true:bool):\n        self.data_points.append((score, is_true))\n    \n    def add_gt_positives(self, num_positives:int):\n        \"\"\" Call this once per image. \"\"\"\n        self.num_gt_positives += num_positives\n\n    def is_empty(self) -> bool:\n        return len(self.data_points) == 0 and self.num_gt_positives == 0\n\n    def get_ap(self) -> float:\n        \"\"\" Warning: result not cached. \"\"\"\n\n        if self.num_gt_positives == 0:\n            return 0\n\n        # Sort descending by score\n        self.data_points.sort(key=lambda x: -x[0])\n\n        precisions = []\n        recalls    = []\n        num_true  = 0\n        num_false = 0\n\n        # Compute the precision-recall curve. The x axis is recalls and the y axis precisions.\n        for datum in self.data_points:\n            # datum[1] is whether the detection a true or false positive\n            if datum[1]: num_true += 1\n            else: num_false += 1\n            \n            precision = num_true / (num_true + num_false)\n            recall    = num_true / self.num_gt_positives\n\n            precisions.append(precision)\n            recalls.append(recall)\n\n        # Smooth the curve by computing [max(precisions[i:]) for i in range(len(precisions))]\n        # Basically, remove any temporary dips from the curve.\n        # At least that's what I think, idk. COCOEval did it so I do too.\n        for i in range(len(precisions)-1, 0, -1):\n            if precisions[i] > precisions[i-1]:\n                precisions[i-1] = precisions[i]\n\n        # Compute the integral of precision(recall) d_recall from recall=0->1 using fixed-length riemann summation with 101 bars.\n        y_range = [0] * 101 # idx 0 is recall == 0.0 and idx 100 is recall == 1.00\n        x_range = np.array([x / 100 for x in range(101)])\n        recalls = np.array(recalls)\n\n        # I realize this is weird, but all it does is find the nearest precision(x) for a given x in x_range.\n        # Basically, if the closest recall we have to 0.01 is 0.009 this sets precision(0.01) = precision(0.009).\n        # I approximate the integral this way, because that's how COCOEval does it.\n        indices = np.searchsorted(recalls, x_range, side='left')\n        for bar_idx, precision_idx in enumerate(indices):\n            if precision_idx < len(precisions):\n                y_range[bar_idx] = precisions[precision_idx]\n\n        # Finally compute the riemann sum to get our integral.\n        # avg([precision(x) for x in 0:0.01:1])\n        return sum(y_range) / len(y_range)\n\ndef badhash(x):\n    \"\"\"\n    Just a quick and dirty hash function for doing a deterministic shuffle based on image_id.\n\n    Source:\n    https://stackoverflow.com/questions/664014/what-integer-hash-function-are-good-that-accepts-an-integer-hash-key\n    \"\"\"\n    x = (((x >> 16) ^ x) * 0x045d9f3b) & 0xFFFFFFFF\n    x = (((x >> 16) ^ x) * 0x045d9f3b) & 0xFFFFFFFF\n    x =  ((x >> 16) ^ x) & 0xFFFFFFFF\n    return x\n\ndef evalimage(net:Yolact, path:str, save_path:str=None):\n    frame = torch.from_numpy(cv2.imread(path)).cuda().float()\n    batch = FastBaseTransform()(frame.unsqueeze(0))\n    preds = net(batch)\n\n    img_numpy = prep_display(preds, frame, None, None, undo_transform=False)\n    \n    if save_path is None:\n        img_numpy = img_numpy[:, :, (2, 1, 0)]\n\n    if save_path is None:\n        plt.imshow(img_numpy)\n        plt.title(path)\n        plt.show()\n    else:\n        cv2.imwrite(save_path, img_numpy)\n\ndef evalimages(net:Yolact, input_folder:str, output_folder:str):\n    if not os.path.exists(output_folder):\n        os.mkdir(output_folder)\n\n    print()\n    for p in Path(input_folder).glob('*'): \n        path = str(p)\n        name = os.path.basename(path)\n        name = '.'.join(name.split('.')[:-1]) + '.png'\n        out_path = os.path.join(output_folder, name)\n\n        evalimage(net, path, out_path)\n        print(path + ' -> ' + out_path)\n    print('Done.')\n\nfrom multiprocessing.pool import ThreadPool\nfrom queue import Queue\n\nclass CustomDataParallel(torch.nn.DataParallel):\n    \"\"\" A Custom Data Parallel class that properly gathers lists of dictionaries. \"\"\"\n    def gather(self, outputs, output_device):\n        # Note that I don't actually want to convert everything to the output_device\n        return sum(outputs, [])\n\ndef evalvideo(net:Yolact, path:str, out_path:str=None):\n    # If the path is a digit, parse it as a webcam index\n    is_webcam = path.isdigit()\n    \n    # If the input image size is constant, this make things faster (hence why we can use it in a video setting).\n    cudnn.benchmark = True\n    \n    if is_webcam:\n        vid = cv2.VideoCapture(int(path))\n    else:\n        vid = cv2.VideoCapture(path)\n    \n    if not vid.isOpened():\n        print('Could not open video \"%s\"' % path)\n        exit(-1)\n\n    target_fps   = round(vid.get(cv2.CAP_PROP_FPS))\n    frame_width  = round(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = round(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    if is_webcam:\n        num_frames = float('inf')\n    else:\n        num_frames = round(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    net = CustomDataParallel(net).cuda()\n    transform = torch.nn.DataParallel(FastBaseTransform()).cuda()\n    frame_times = MovingAverage(100)\n    fps = 0\n    frame_time_target = 1 / target_fps\n    running = True\n    fps_str = ''\n    vid_done = False\n    frames_displayed = 0\n\n    if out_path is not None:\n        out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*\"mp4v\"), target_fps, (frame_width, frame_height))\n\n    def cleanup_and_exit():\n        print()\n        pool.terminate()\n        vid.release()\n        if out_path is not None:\n            out.release()\n        cv2.destroyAllWindows()\n        exit()\n\n    def get_next_frame(vid):\n        frames = []\n        for idx in range(args.video_multiframe):\n            frame = vid.read()[1]\n            if frame is None:\n                return frames\n            frames.append(frame)\n        return frames\n\n    def transform_frame(frames):\n        with torch.no_grad():\n            frames = [torch.from_numpy(frame).cuda().float() for frame in frames]\n            return frames, transform(torch.stack(frames, 0))\n\n    def eval_network(inp):\n        with torch.no_grad():\n            frames, imgs = inp\n            num_extra = 0\n            while imgs.size(0) < args.video_multiframe:\n                imgs = torch.cat([imgs, imgs[0].unsqueeze(0)], dim=0)\n                num_extra += 1\n            out = net(imgs)\n            if num_extra > 0:\n                out = out[:-num_extra]\n            return frames, out\n\n    def prep_frame(inp, fps_str):\n        with torch.no_grad():\n            frame, preds = inp\n            return prep_display(preds, frame, None, None, undo_transform=False, class_color=True, fps_str=fps_str)\n\n    frame_buffer = Queue()\n    video_fps = 0\n\n    # All this timing code to make sure that \n    def play_video():\n        try:\n            nonlocal frame_buffer, running, video_fps, is_webcam, num_frames, frames_displayed, vid_done\n\n            video_frame_times = MovingAverage(100)\n            frame_time_stabilizer = frame_time_target\n            last_time = None\n            stabilizer_step = 0.0005\n            progress_bar = ProgressBar(30, num_frames)\n\n            while running:\n                frame_time_start = time.time()\n\n                if not frame_buffer.empty():\n                    next_time = time.time()\n                    if last_time is not None:\n                        video_frame_times.add(next_time - last_time)\n                        video_fps = 1 / video_frame_times.get_avg()\n                    if out_path is None:\n                        cv2.imshow(path, frame_buffer.get())\n                    else:\n                        out.write(frame_buffer.get())\n                    frames_displayed += 1\n                    last_time = next_time\n\n                    if out_path is not None:\n                        if video_frame_times.get_avg() == 0:\n                            fps = 0\n                        else:\n                            fps = 1 / video_frame_times.get_avg()\n                        progress = frames_displayed / num_frames * 100\n                        progress_bar.set_val(frames_displayed)\n\n                        print('\\rProcessing Frames  %s %6d / %6d (%5.2f%%)    %5.2f fps        '\n                            % (repr(progress_bar), frames_displayed, num_frames, progress, fps), end='')\n\n                \n                # This is split because you don't want savevideo to require cv2 display functionality (see #197)\n                if out_path is None and cv2.waitKey(1) == 27:\n                    # Press Escape to close\n                    running = False\n                if not (frames_displayed < num_frames):\n                    running = False\n\n                if not vid_done:\n                    buffer_size = frame_buffer.qsize()\n                    if buffer_size < args.video_multiframe:\n                        frame_time_stabilizer += stabilizer_step\n                    elif buffer_size > args.video_multiframe:\n                        frame_time_stabilizer -= stabilizer_step\n                        if frame_time_stabilizer < 0:\n                            frame_time_stabilizer = 0\n\n                    new_target = frame_time_stabilizer if is_webcam else max(frame_time_stabilizer, frame_time_target)\n                else:\n                    new_target = frame_time_target\n\n                next_frame_target = max(2 * new_target - video_frame_times.get_avg(), 0)\n                target_time = frame_time_start + next_frame_target - 0.001 # Let's just subtract a millisecond to be safe\n                \n                if out_path is None or args.emulate_playback:\n                    # This gives more accurate timing than if sleeping the whole amount at once\n                    while time.time() < target_time:\n                        time.sleep(0.001)\n                else:\n                    # Let's not starve the main thread, now\n                    time.sleep(0.001)\n        except:\n            # See issue #197 for why this is necessary\n            import traceback\n            traceback.print_exc()\n\n\n    extract_frame = lambda x, i: (x[0][i] if x[1][i]['detection'] is None else x[0][i].to(x[1][i]['detection']['box'].device), [x[1][i]])\n\n    # Prime the network on the first frame because I do some thread unsafe things otherwise\n    print('Initializing model... ', end='')\n    first_batch = eval_network(transform_frame(get_next_frame(vid)))\n    print('Done.')\n\n    # For each frame the sequence of functions it needs to go through to be processed (in reversed order)\n    sequence = [prep_frame, eval_network, transform_frame]\n    pool = ThreadPool(processes=len(sequence) + args.video_multiframe + 2)\n    pool.apply_async(play_video)\n    active_frames = [{'value': extract_frame(first_batch, i), 'idx': 0} for i in range(len(first_batch[0]))]\n\n    print()\n    if out_path is None: print('Press Escape to close.')\n    try:\n        while vid.isOpened() and running:\n            # Hard limit on frames in buffer so we don't run out of memory >.>\n            while frame_buffer.qsize() > 100:\n                time.sleep(0.001)\n\n            start_time = time.time()\n\n            # Start loading the next frames from the disk\n            if not vid_done:\n                next_frames = pool.apply_async(get_next_frame, args=(vid,))\n            else:\n                next_frames = None\n            \n            if not (vid_done and len(active_frames) == 0):\n                # For each frame in our active processing queue, dispatch a job\n                # for that frame using the current function in the sequence\n                for frame in active_frames:\n                    _args =  [frame['value']]\n                    if frame['idx'] == 0:\n                        _args.append(fps_str)\n                    frame['value'] = pool.apply_async(sequence[frame['idx']], args=_args)\n                \n                # For each frame whose job was the last in the sequence (i.e. for all final outputs)\n                for frame in active_frames:\n                    if frame['idx'] == 0:\n                        frame_buffer.put(frame['value'].get())\n\n                # Remove the finished frames from the processing queue\n                active_frames = [x for x in active_frames if x['idx'] > 0]\n\n                # Finish evaluating every frame in the processing queue and advanced their position in the sequence\n                for frame in list(reversed(active_frames)):\n                    frame['value'] = frame['value'].get()\n                    frame['idx'] -= 1\n\n                    if frame['idx'] == 0:\n                        # Split this up into individual threads for prep_frame since it doesn't support batch size\n                        active_frames += [{'value': extract_frame(frame['value'], i), 'idx': 0} for i in range(1, len(frame['value'][0]))]\n                        frame['value'] = extract_frame(frame['value'], 0)\n                \n                # Finish loading in the next frames and add them to the processing queue\n                if next_frames is not None:\n                    frames = next_frames.get()\n                    if len(frames) == 0:\n                        vid_done = True\n                    else:\n                        active_frames.append({'value': frames, 'idx': len(sequence)-1})\n\n                # Compute FPS\n                frame_times.add(time.time() - start_time)\n                fps = args.video_multiframe / frame_times.get_avg()\n            else:\n                fps = 0\n            \n            fps_str = 'Processing FPS: %.2f | Video Playback FPS: %.2f | Frames in Buffer: %d' % (fps, video_fps, frame_buffer.qsize())\n            if not args.display_fps:\n                print('\\r' + fps_str + '    ', end='')\n\n    except KeyboardInterrupt:\n        print('\\nStopping...')\n    \n    cleanup_and_exit()\n\ndef evaluate(net:Yolact, dataset, train_mode=False):\n    net.detect.use_fast_nms = args.fast_nms\n    net.detect.use_cross_class_nms = args.cross_class_nms\n    cfg.mask_proto_debug = args.mask_proto_debug\n\n    # TODO Currently we do not support Fast Mask Re-scroing in evalimage, evalimages, and evalvideo\n    if args.image is not None:\n        if ':' in args.image:\n            inp, out = args.image.split(':')\n            evalimage(net, inp, out)\n        else:\n            evalimage(net, args.image)\n        return\n    elif args.images is not None:\n        inp, out = args.images.split(':')\n        evalimages(net, inp, out)\n        return\n    elif args.video is not None:\n        if ':' in args.video:\n            inp, out = args.video.split(':')\n            evalvideo(net, inp, out)\n        else:\n            evalvideo(net, args.video)\n        return\n\n    frame_times = MovingAverage()\n    dataset_size = len(dataset) if args.max_images < 0 else min(args.max_images, len(dataset))\n    progress_bar = ProgressBar(30, dataset_size)\n\n    print()\n\n    if not args.display and not args.benchmark:\n        # For each class and iou, stores tuples (score, isPositive)\n        # Index ap_data[type][iouIdx][classIdx]\n        ap_data = {\n            'box' : [[APDataObject() for _ in cfg.dataset.class_names] for _ in iou_thresholds],\n            'mask': [[APDataObject() for _ in cfg.dataset.class_names] for _ in iou_thresholds]\n        }\n        detections = Detections()\n    else:\n        timer.disable('Load Data')\n\n    dataset_indices = list(range(len(dataset)))\n    \n    if args.shuffle:\n        random.shuffle(dataset_indices)\n    elif not args.no_sort:\n        # Do a deterministic shuffle based on the image ids\n        #\n        # I do this because on python 3.5 dictionary key order is *random*, while in 3.6 it's\n        # the order of insertion. That means on python 3.6, the images come in the order they are in\n        # in the annotations file. For some reason, the first images in the annotations file are\n        # the hardest. To combat this, I use a hard-coded hash function based on the image ids\n        # to shuffle the indices we use. That way, no matter what python version or how pycocotools\n        # handles the data, we get the same result every time.\n        hashed = [badhash(x) for x in dataset.ids]\n        dataset_indices.sort(key=lambda x: hashed[x])\n\n    dataset_indices = dataset_indices[:dataset_size]\n\n    try:\n        # Main eval loop\n        for it, image_idx in enumerate(dataset_indices):\n            timer.reset()\n\n            with timer.env('Load Data'):\n                img, gt, gt_masks, h, w, num_crowd = dataset.pull_item(image_idx)\n\n                # Test flag, do not upvote\n                if cfg.mask_proto_debug:\n                    with open('scripts/info.txt', 'w') as f:\n                        f.write(str(dataset.ids[image_idx]))\n                    np.save('scripts/gt.npy', gt_masks)\n\n                batch = Variable(img.unsqueeze(0))\n                if args.cuda:\n                    batch = batch.cuda()\n\n            with timer.env('Network Extra'):\n                preds = net(batch)\n            # Perform the meat of the operation here depending on our mode.\n            if args.display:\n                img_numpy = prep_display(preds, img, h, w)\n            elif args.benchmark:\n                prep_benchmark(preds, h, w)\n            else:\n                prep_metrics(ap_data, preds, img, gt, gt_masks, h, w, num_crowd, dataset.ids[image_idx], detections)\n            \n            # First couple of images take longer because we're constructing the graph.\n            # Since that's technically initialization, don't include those in the FPS calculations.\n            if it > 1:\n                frame_times.add(timer.total_time())\n            \n            if args.display:\n                if it > 1:\n                    print('Avg FPS: %.4f' % (1 / frame_times.get_avg()))\n                plt.imshow(img_numpy)\n                plt.title(str(dataset.ids[image_idx]))\n                plt.show()\n            elif not args.no_bar:\n                if it > 1: fps = 1 / frame_times.get_avg()\n                else: fps = 0\n                progress = (it+1) / dataset_size * 100\n                progress_bar.set_val(it+1)\n                print('\\rProcessing Images  %s %6d / %6d (%5.2f%%)    %5.2f fps        '\n                    % (repr(progress_bar), it+1, dataset_size, progress, fps), end='')\n\n\n\n        if not args.display and not args.benchmark:\n            print()\n            if args.output_coco_json:\n                print('Dumping detections...')\n                if args.output_web_json:\n                    detections.dump_web()\n                else:\n                    detections.dump()\n            else:\n                if not train_mode:\n                    print('Saving data...')\n                    with open(args.ap_data_file, 'wb') as f:\n                        pickle.dump(ap_data, f)\n\n                return calc_map(ap_data)\n        elif args.benchmark:\n            print()\n            print()\n            print('Stats for the last frame:')\n            timer.print_stats()\n            avg_seconds = frame_times.get_avg()\n            print('Average: %5.2f fps, %5.2f ms' % (1 / frame_times.get_avg(), 1000*avg_seconds))\n\n    except KeyboardInterrupt:\n        print('Stopping...')\n\n\ndef calc_map(ap_data):\n    print('Calculating mAP...')\n    aps = [{'box': [], 'mask': []} for _ in iou_thresholds]\n\n    for _class in range(len(cfg.dataset.class_names)):\n        for iou_idx in range(len(iou_thresholds)):\n            for iou_type in ('box', 'mask'):\n                ap_obj = ap_data[iou_type][iou_idx][_class]\n\n                if not ap_obj.is_empty():\n                    aps[iou_idx][iou_type].append(ap_obj.get_ap())\n\n    all_maps = {'box': OrderedDict(), 'mask': OrderedDict()}\n\n    # Looking back at it, this code is really hard to read :/\n    for iou_type in ('box', 'mask'):\n        all_maps[iou_type]['all'] = 0 # Make this first in the ordereddict\n        for i, threshold in enumerate(iou_thresholds):\n            mAP = sum(aps[i][iou_type]) / len(aps[i][iou_type]) * 100 if len(aps[i][iou_type]) > 0 else 0\n            all_maps[iou_type][int(threshold*100)] = mAP\n        all_maps[iou_type]['all'] = (sum(all_maps[iou_type].values()) / (len(all_maps[iou_type].values())-1))\n    \n    print_maps(all_maps)\n    \n    # Put in a prettier format so we can serialize it to json during training\n    all_maps = {k: {j: round(u, 2) for j, u in v.items()} for k, v in all_maps.items()}\n    return all_maps\n\ndef print_maps(all_maps):\n    # Warning: hacky \n    make_row = lambda vals: (' %5s |' * len(vals)) % tuple(vals)\n    make_sep = lambda n:  ('-------+' * n)\n\n    print()\n    print(make_row([''] + [('.%d ' % x if isinstance(x, int) else x + ' ') for x in all_maps['box'].keys()]))\n    print(make_sep(len(all_maps['box']) + 1))\n    for iou_type in ('box', 'mask'):\n        print(make_row([iou_type] + ['%.2f' % x if x < 100 else '%.1f' % x for x in all_maps[iou_type].values()]))\n    print(make_sep(len(all_maps['box']) + 1))\n    print()\n\n\n\nif __name__ == '__main__':\n    parse_args()\n\n    if args.config is not None:\n        set_cfg(args.config)\n\n    if args.trained_model == 'interrupt':\n        args.trained_model = SavePath.get_interrupt('weights/')\n    elif args.trained_model == 'latest':\n        args.trained_model = SavePath.get_latest('weights/', cfg.name)\n\n    if args.config is None:\n        model_path = SavePath.from_str(args.trained_model)\n        # TODO: Bad practice? Probably want to do a name lookup instead.\n        args.config = model_path.model_name + '_config'\n        print('Config not specified. Parsed %s from the file name.\\n' % args.config)\n        set_cfg(args.config)\n\n    if args.detect:\n        cfg.eval_mask_branch = False\n\n    if args.dataset is not None:\n        set_dataset(args.dataset)\n\n    with torch.no_grad():\n        if not os.path.exists('results'):\n            os.makedirs('results')\n\n        if args.cuda:\n            cudnn.fastest = True\n            torch.set_default_tensor_type('torch.cuda.FloatTensor')\n        else:\n            torch.set_default_tensor_type('torch.FloatTensor')\n\n        if args.resume and not args.display:\n            with open(args.ap_data_file, 'rb') as f:\n                ap_data = pickle.load(f)\n            calc_map(ap_data)\n            exit()\n\n        if args.image is None and args.video is None and args.images is None:\n            dataset = COCODetection(cfg.dataset.valid_images, cfg.dataset.valid_info,\n                                    transform=BaseTransform(), has_gt=cfg.dataset.has_gt)\n            prep_coco_cats()\n        else:\n            dataset = None        \n\n        print('Loading model...', end='')\n        net = Yolact()\n        net.load_weights(args.trained_model)\n        net.eval()\n        print(' Done.')\n\n        if args.cuda:\n            net = net.cuda()\n\n        evaluate(net, dataset)\n\n\n"
        },
        {
          "name": "external",
          "type": "tree",
          "content": null
        },
        {
          "name": "layers",
          "type": "tree",
          "content": null
        },
        {
          "name": "run_coco_eval.py",
          "type": "blob",
          "size": 1.3828125,
          "content": "\"\"\"\nRuns the coco-supplied cocoeval script to evaluate detections\noutputted by using the output_coco_json flag in eval.py.\n\"\"\"\n\n\nimport argparse\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\n\nparser = argparse.ArgumentParser(description='COCO Detections Evaluator')\nparser.add_argument('--bbox_det_file', default='results/bbox_detections.json', type=str)\nparser.add_argument('--mask_det_file', default='results/mask_detections.json', type=str)\nparser.add_argument('--gt_ann_file',   default='data/coco/annotations/instances_val2017.json', type=str)\nparser.add_argument('--eval_type',     default='both', choices=['bbox', 'mask', 'both'], type=str)\nargs = parser.parse_args()\n\n\n\nif __name__ == '__main__':\n\n\teval_bbox = (args.eval_type in ('bbox', 'both'))\n\teval_mask = (args.eval_type in ('mask', 'both'))\n\n\tprint('Loading annotations...')\n\tgt_annotations = COCO(args.gt_ann_file)\n\tif eval_bbox:\n\t\tbbox_dets = gt_annotations.loadRes(args.bbox_det_file)\n\tif eval_mask:\n\t\tmask_dets = gt_annotations.loadRes(args.mask_det_file)\n\n\tif eval_bbox:\n\t\tprint('\\nEvaluating BBoxes:')\n\t\tbbox_eval = COCOeval(gt_annotations, bbox_dets, 'bbox')\n\t\tbbox_eval.evaluate()\n\t\tbbox_eval.accumulate()\n\t\tbbox_eval.summarize()\n\t\n\tif eval_mask:\n\t\tprint('\\nEvaluating Masks:')\n\t\tbbox_eval = COCOeval(gt_annotations, mask_dets, 'segm')\n\t\tbbox_eval.evaluate()\n\t\tbbox_eval.accumulate()\n\t\tbbox_eval.summarize()\n\n\n\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 20.978515625,
          "content": "from data import *\nfrom utils.augmentations import SSDAugmentation, BaseTransform\nfrom utils.functions import MovingAverage, SavePath\nfrom utils.logger import Log\nfrom utils import timer\nfrom layers.modules import MultiBoxLoss\nfrom yolact import Yolact\nimport os\nimport sys\nimport time\nimport math, random\nfrom pathlib import Path\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nimport torch.nn.init as init\nimport torch.utils.data as data\nimport numpy as np\nimport argparse\nimport datetime\n\n# Oof\nimport eval as eval_script\n\ndef str2bool(v):\n    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n\n\nparser = argparse.ArgumentParser(\n    description='Yolact Training Script')\nparser.add_argument('--batch_size', default=8, type=int,\n                    help='Batch size for training')\nparser.add_argument('--resume', default=None, type=str,\n                    help='Checkpoint state_dict file to resume training from. If this is \"interrupt\"'\\\n                         ', the model will resume training from the interrupt file.')\nparser.add_argument('--start_iter', default=-1, type=int,\n                    help='Resume training at this iter. If this is -1, the iteration will be'\\\n                         'determined from the file name.')\nparser.add_argument('--num_workers', default=4, type=int,\n                    help='Number of workers used in dataloading')\nparser.add_argument('--cuda', default=True, type=str2bool,\n                    help='Use CUDA to train model')\nparser.add_argument('--lr', '--learning_rate', default=None, type=float,\n                    help='Initial learning rate. Leave as None to read this from the config.')\nparser.add_argument('--momentum', default=None, type=float,\n                    help='Momentum for SGD. Leave as None to read this from the config.')\nparser.add_argument('--decay', '--weight_decay', default=None, type=float,\n                    help='Weight decay for SGD. Leave as None to read this from the config.')\nparser.add_argument('--gamma', default=None, type=float,\n                    help='For each lr step, what to multiply the lr by. Leave as None to read this from the config.')\nparser.add_argument('--save_folder', default='weights/',\n                    help='Directory for saving checkpoint models.')\nparser.add_argument('--log_folder', default='logs/',\n                    help='Directory for saving logs.')\nparser.add_argument('--config', default=None,\n                    help='The config object to use.')\nparser.add_argument('--save_interval', default=10000, type=int,\n                    help='The number of iterations between saving the model.')\nparser.add_argument('--validation_size', default=5000, type=int,\n                    help='The number of images to use for validation.')\nparser.add_argument('--validation_epoch', default=2, type=int,\n                    help='Output validation information every n iterations. If -1, do no validation.')\nparser.add_argument('--keep_latest', dest='keep_latest', action='store_true',\n                    help='Only keep the latest checkpoint instead of each one.')\nparser.add_argument('--keep_latest_interval', default=100000, type=int,\n                    help='When --keep_latest is on, don\\'t delete the latest file at these intervals. This should be a multiple of save_interval or 0.')\nparser.add_argument('--dataset', default=None, type=str,\n                    help='If specified, override the dataset specified in the config with this one (example: coco2017_dataset).')\nparser.add_argument('--no_log', dest='log', action='store_false',\n                    help='Don\\'t log per iteration information into log_folder.')\nparser.add_argument('--log_gpu', dest='log_gpu', action='store_true',\n                    help='Include GPU information in the logs. Nvidia-smi tends to be slow, so set this with caution.')\nparser.add_argument('--no_interrupt', dest='interrupt', action='store_false',\n                    help='Don\\'t save an interrupt when KeyboardInterrupt is caught.')\nparser.add_argument('--batch_alloc', default=None, type=str,\n                    help='If using multiple GPUS, you can set this to be a comma separated list detailing which GPUs should get what local batch size (It should add up to your total batch size).')\nparser.add_argument('--no_autoscale', dest='autoscale', action='store_false',\n                    help='YOLACT will automatically scale the lr and the number of iterations depending on the batch size. Set this if you want to disable that.')\n\nparser.set_defaults(keep_latest=False, log=True, log_gpu=False, interrupt=True, autoscale=True)\nargs = parser.parse_args()\n\nif args.config is not None:\n    set_cfg(args.config)\n\nif args.dataset is not None:\n    set_dataset(args.dataset)\n\nif args.autoscale and args.batch_size != 8:\n    factor = args.batch_size / 8\n    if __name__ == '__main__':\n        print('Scaling parameters by %.2f to account for a batch size of %d.' % (factor, args.batch_size))\n\n    cfg.lr *= factor\n    cfg.max_iter //= factor\n    cfg.lr_steps = [x // factor for x in cfg.lr_steps]\n\n# Update training parameters from the config if necessary\ndef replace(name):\n    if getattr(args, name) == None: setattr(args, name, getattr(cfg, name))\nreplace('lr')\nreplace('decay')\nreplace('gamma')\nreplace('momentum')\n\n# This is managed by set_lr\ncur_lr = args.lr\n\nif torch.cuda.device_count() == 0:\n    print('No GPUs detected. Exiting...')\n    exit(-1)\n\nif args.batch_size // torch.cuda.device_count() < 6:\n    if __name__ == '__main__':\n        print('Per-GPU batch size is less than the recommended limit for batch norm. Disabling batch norm.')\n    cfg.freeze_bn = True\n\nloss_types = ['B', 'C', 'M', 'P', 'D', 'E', 'S', 'I']\n\nif torch.cuda.is_available():\n    if args.cuda:\n        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n    if not args.cuda:\n        print(\"WARNING: It looks like you have a CUDA device, but aren't \" +\n              \"using CUDA.\\nRun with --cuda for optimal training speed.\")\n        torch.set_default_tensor_type('torch.FloatTensor')\nelse:\n    torch.set_default_tensor_type('torch.FloatTensor')\n\nclass NetLoss(nn.Module):\n    \"\"\"\n    A wrapper for running the network and computing the loss\n    This is so we can more efficiently use DataParallel.\n    \"\"\"\n    \n    def __init__(self, net:Yolact, criterion:MultiBoxLoss):\n        super().__init__()\n\n        self.net = net\n        self.criterion = criterion\n    \n    def forward(self, images, targets, masks, num_crowds):\n        preds = self.net(images)\n        losses = self.criterion(self.net, preds, targets, masks, num_crowds)\n        return losses\n\nclass CustomDataParallel(nn.DataParallel):\n    \"\"\"\n    This is a custom version of DataParallel that works better with our training data.\n    It should also be faster than the general case.\n    \"\"\"\n\n    def scatter(self, inputs, kwargs, device_ids):\n        # More like scatter and data prep at the same time. The point is we prep the data in such a way\n        # that no scatter is necessary, and there's no need to shuffle stuff around different GPUs.\n        devices = ['cuda:' + str(x) for x in device_ids]\n        splits = prepare_data(inputs[0], devices, allocation=args.batch_alloc)\n\n        return [[split[device_idx] for split in splits] for device_idx in range(len(devices))], \\\n            [kwargs] * len(devices)\n\n    def gather(self, outputs, output_device):\n        out = {}\n\n        for k in outputs[0]:\n            out[k] = torch.stack([output[k].to(output_device) for output in outputs])\n        \n        return out\n\ndef train():\n    if not os.path.exists(args.save_folder):\n        os.mkdir(args.save_folder)\n\n    dataset = COCODetection(image_path=cfg.dataset.train_images,\n                            info_file=cfg.dataset.train_info,\n                            transform=SSDAugmentation(MEANS))\n    \n    if args.validation_epoch > 0:\n        setup_eval()\n        val_dataset = COCODetection(image_path=cfg.dataset.valid_images,\n                                    info_file=cfg.dataset.valid_info,\n                                    transform=BaseTransform(MEANS))\n\n    # Parallel wraps the underlying module, but when saving and loading we don't want that\n    yolact_net = Yolact()\n    net = yolact_net\n    net.train()\n\n    if args.log:\n        log = Log(cfg.name, args.log_folder, dict(args._get_kwargs()),\n            overwrite=(args.resume is None), log_gpu_stats=args.log_gpu)\n\n    # I don't use the timer during training (I use a different timing method).\n    # Apparently there's a race condition with multiple GPUs, so disable it just to be safe.\n    timer.disable_all()\n\n    # Both of these can set args.resume to None, so do them before the check    \n    if args.resume == 'interrupt':\n        args.resume = SavePath.get_interrupt(args.save_folder)\n    elif args.resume == 'latest':\n        args.resume = SavePath.get_latest(args.save_folder, cfg.name)\n\n    if args.resume is not None:\n        print('Resuming training, loading {}...'.format(args.resume))\n        yolact_net.load_weights(args.resume)\n\n        if args.start_iter == -1:\n            args.start_iter = SavePath.from_str(args.resume).iteration\n    else:\n        print('Initializing weights...')\n        yolact_net.init_weights(backbone_path=args.save_folder + cfg.backbone.path)\n\n    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum,\n                          weight_decay=args.decay)\n    criterion = MultiBoxLoss(num_classes=cfg.num_classes,\n                             pos_threshold=cfg.positive_iou_threshold,\n                             neg_threshold=cfg.negative_iou_threshold,\n                             negpos_ratio=cfg.ohem_negpos_ratio)\n\n    if args.batch_alloc is not None:\n        args.batch_alloc = [int(x) for x in args.batch_alloc.split(',')]\n        if sum(args.batch_alloc) != args.batch_size:\n            print('Error: Batch allocation (%s) does not sum to batch size (%s).' % (args.batch_alloc, args.batch_size))\n            exit(-1)\n\n    net = CustomDataParallel(NetLoss(net, criterion))\n    if args.cuda:\n        net = net.cuda()\n    \n    # Initialize everything\n    if not cfg.freeze_bn: yolact_net.freeze_bn() # Freeze bn so we don't kill our means\n    yolact_net(torch.zeros(1, 3, cfg.max_size, cfg.max_size).cuda())\n    if not cfg.freeze_bn: yolact_net.freeze_bn(True)\n\n    # loss counters\n    loc_loss = 0\n    conf_loss = 0\n    iteration = max(args.start_iter, 0)\n    last_time = time.time()\n\n    epoch_size = len(dataset) // args.batch_size\n    num_epochs = math.ceil(cfg.max_iter / epoch_size)\n    \n    # Which learning rate adjustment step are we on? lr' = lr * gamma ^ step_index\n    step_index = 0\n\n    data_loader = data.DataLoader(dataset, args.batch_size,\n                                  num_workers=args.num_workers,\n                                  shuffle=True, collate_fn=detection_collate,\n                                  pin_memory=True)\n    \n    \n    save_path = lambda epoch, iteration: SavePath(cfg.name, epoch, iteration).get_path(root=args.save_folder)\n    time_avg = MovingAverage()\n\n    global loss_types # Forms the print order\n    loss_avgs  = { k: MovingAverage(100) for k in loss_types }\n\n    print('Begin training!')\n    print()\n    # try-except so you can use ctrl+c to save early and stop training\n    try:\n        for epoch in range(num_epochs):\n            # Resume from start_iter\n            if (epoch+1)*epoch_size < iteration:\n                continue\n            \n            for datum in data_loader:\n                # Stop if we've reached an epoch if we're resuming from start_iter\n                if iteration == (epoch+1)*epoch_size:\n                    break\n\n                # Stop at the configured number of iterations even if mid-epoch\n                if iteration == cfg.max_iter:\n                    break\n\n                # Change a config setting if we've reached the specified iteration\n                changed = False\n                for change in cfg.delayed_settings:\n                    if iteration >= change[0]:\n                        changed = True\n                        cfg.replace(change[1])\n\n                        # Reset the loss averages because things might have changed\n                        for avg in loss_avgs:\n                            avg.reset()\n                \n                # If a config setting was changed, remove it from the list so we don't keep checking\n                if changed:\n                    cfg.delayed_settings = [x for x in cfg.delayed_settings if x[0] > iteration]\n\n                # Warm up by linearly interpolating the learning rate from some smaller value\n                if cfg.lr_warmup_until > 0 and iteration <= cfg.lr_warmup_until:\n                    set_lr(optimizer, (args.lr - cfg.lr_warmup_init) * (iteration / cfg.lr_warmup_until) + cfg.lr_warmup_init)\n\n                # Adjust the learning rate at the given iterations, but also if we resume from past that iteration\n                while step_index < len(cfg.lr_steps) and iteration >= cfg.lr_steps[step_index]:\n                    step_index += 1\n                    set_lr(optimizer, args.lr * (args.gamma ** step_index))\n                \n                # Zero the grad to get ready to compute gradients\n                optimizer.zero_grad()\n\n                # Forward Pass + Compute loss at the same time (see CustomDataParallel and NetLoss)\n                losses = net(datum)\n                \n                losses = { k: (v).mean() for k,v in losses.items() } # Mean here because Dataparallel\n                loss = sum([losses[k] for k in losses])\n                \n                # no_inf_mean removes some components from the loss, so make sure to backward through all of it\n                # all_loss = sum([v.mean() for v in losses.values()])\n\n                # Backprop\n                loss.backward() # Do this to free up vram even if loss is not finite\n                if torch.isfinite(loss).item():\n                    optimizer.step()\n                \n                # Add the loss to the moving average for bookkeeping\n                for k in losses:\n                    loss_avgs[k].add(losses[k].item())\n\n                cur_time  = time.time()\n                elapsed   = cur_time - last_time\n                last_time = cur_time\n\n                # Exclude graph setup from the timing information\n                if iteration != args.start_iter:\n                    time_avg.add(elapsed)\n\n                if iteration % 10 == 0:\n                    eta_str = str(datetime.timedelta(seconds=(cfg.max_iter-iteration) * time_avg.get_avg())).split('.')[0]\n                    \n                    total = sum([loss_avgs[k].get_avg() for k in losses])\n                    loss_labels = sum([[k, loss_avgs[k].get_avg()] for k in loss_types if k in losses], [])\n                    \n                    print(('[%3d] %7d ||' + (' %s: %.3f |' * len(losses)) + ' T: %.3f || ETA: %s || timer: %.3f')\n                            % tuple([epoch, iteration] + loss_labels + [total, eta_str, elapsed]), flush=True)\n\n                if args.log:\n                    precision = 5\n                    loss_info = {k: round(losses[k].item(), precision) for k in losses}\n                    loss_info['T'] = round(loss.item(), precision)\n\n                    if args.log_gpu:\n                        log.log_gpu_stats = (iteration % 10 == 0) # nvidia-smi is sloooow\n                        \n                    log.log('train', loss=loss_info, epoch=epoch, iter=iteration,\n                        lr=round(cur_lr, 10), elapsed=elapsed)\n\n                    log.log_gpu_stats = args.log_gpu\n                \n                iteration += 1\n\n                if iteration % args.save_interval == 0 and iteration != args.start_iter:\n                    if args.keep_latest:\n                        latest = SavePath.get_latest(args.save_folder, cfg.name)\n\n                    print('Saving state, iter:', iteration)\n                    yolact_net.save_weights(save_path(epoch, iteration))\n\n                    if args.keep_latest and latest is not None:\n                        if args.keep_latest_interval <= 0 or iteration % args.keep_latest_interval != args.save_interval:\n                            print('Deleting old save...')\n                            os.remove(latest)\n            \n            # This is done per epoch\n            if args.validation_epoch > 0:\n                if epoch % args.validation_epoch == 0 and epoch > 0:\n                    compute_validation_map(epoch, iteration, yolact_net, val_dataset, log if args.log else None)\n        \n        # Compute validation mAP after training is finished\n        compute_validation_map(epoch, iteration, yolact_net, val_dataset, log if args.log else None)\n    except KeyboardInterrupt:\n        if args.interrupt:\n            print('Stopping early. Saving network...')\n            \n            # Delete previous copy of the interrupted network so we don't spam the weights folder\n            SavePath.remove_interrupt(args.save_folder)\n            \n            yolact_net.save_weights(save_path(epoch, repr(iteration) + '_interrupt'))\n        exit()\n\n    yolact_net.save_weights(save_path(epoch, iteration))\n\n\ndef set_lr(optimizer, new_lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = new_lr\n    \n    global cur_lr\n    cur_lr = new_lr\n\ndef gradinator(x):\n    x.requires_grad = False\n    return x\n\ndef prepare_data(datum, devices:list=None, allocation:list=None):\n    with torch.no_grad():\n        if devices is None:\n            devices = ['cuda:0'] if args.cuda else ['cpu']\n        if allocation is None:\n            allocation = [args.batch_size // len(devices)] * (len(devices) - 1)\n            allocation.append(args.batch_size - sum(allocation)) # The rest might need more/less\n        \n        images, (targets, masks, num_crowds) = datum\n\n        cur_idx = 0\n        for device, alloc in zip(devices, allocation):\n            for _ in range(alloc):\n                images[cur_idx]  = gradinator(images[cur_idx].to(device))\n                targets[cur_idx] = gradinator(targets[cur_idx].to(device))\n                masks[cur_idx]   = gradinator(masks[cur_idx].to(device))\n                cur_idx += 1\n\n        if cfg.preserve_aspect_ratio:\n            # Choose a random size from the batch\n            _, h, w = images[random.randint(0, len(images)-1)].size()\n\n            for idx, (image, target, mask, num_crowd) in enumerate(zip(images, targets, masks, num_crowds)):\n                images[idx], targets[idx], masks[idx], num_crowds[idx] \\\n                    = enforce_size(image, target, mask, num_crowd, w, h)\n        \n        cur_idx = 0\n        split_images, split_targets, split_masks, split_numcrowds \\\n            = [[None for alloc in allocation] for _ in range(4)]\n\n        for device_idx, alloc in enumerate(allocation):\n            split_images[device_idx]    = torch.stack(images[cur_idx:cur_idx+alloc], dim=0)\n            split_targets[device_idx]   = targets[cur_idx:cur_idx+alloc]\n            split_masks[device_idx]     = masks[cur_idx:cur_idx+alloc]\n            split_numcrowds[device_idx] = num_crowds[cur_idx:cur_idx+alloc]\n\n            cur_idx += alloc\n\n        return split_images, split_targets, split_masks, split_numcrowds\n\ndef no_inf_mean(x:torch.Tensor):\n    \"\"\"\n    Computes the mean of a vector, throwing out all inf values.\n    If there are no non-inf values, this will return inf (i.e., just the normal mean).\n    \"\"\"\n\n    no_inf = [a for a in x if torch.isfinite(a)]\n\n    if len(no_inf) > 0:\n        return sum(no_inf) / len(no_inf)\n    else:\n        return x.mean()\n\ndef compute_validation_loss(net, data_loader, criterion):\n    global loss_types\n\n    with torch.no_grad():\n        losses = {}\n        \n        # Don't switch to eval mode because we want to get losses\n        iterations = 0\n        for datum in data_loader:\n            images, targets, masks, num_crowds = prepare_data(datum)\n            out = net(images)\n\n            wrapper = ScatterWrapper(targets, masks, num_crowds)\n            _losses = criterion(out, wrapper, wrapper.make_mask())\n            \n            for k, v in _losses.items():\n                v = v.mean().item()\n                if k in losses:\n                    losses[k] += v\n                else:\n                    losses[k] = v\n\n            iterations += 1\n            if args.validation_size <= iterations * args.batch_size:\n                break\n        \n        for k in losses:\n            losses[k] /= iterations\n            \n        \n        loss_labels = sum([[k, losses[k]] for k in loss_types if k in losses], [])\n        print(('Validation ||' + (' %s: %.3f |' * len(losses)) + ')') % tuple(loss_labels), flush=True)\n\ndef compute_validation_map(epoch, iteration, yolact_net, dataset, log:Log=None):\n    with torch.no_grad():\n        yolact_net.eval()\n        \n        start = time.time()\n        print()\n        print(\"Computing validation mAP (this may take a while)...\", flush=True)\n        val_info = eval_script.evaluate(yolact_net, dataset, train_mode=True)\n        end = time.time()\n\n        if log is not None:\n            log.log('val', val_info, elapsed=(end - start), epoch=epoch, iter=iteration)\n\n        yolact_net.train()\n\ndef setup_eval():\n    eval_script.parse_args(['--no_bar', '--max_images='+str(args.validation_size)])\n\nif __name__ == '__main__':\n    train()\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "web",
          "type": "tree",
          "content": null
        },
        {
          "name": "yolact.py",
          "type": "blob",
          "size": 30.77734375,
          "content": "import torch, torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models.resnet import Bottleneck\nimport numpy as np\nfrom itertools import product\nfrom math import sqrt\nfrom typing import List\nfrom collections import defaultdict\n\nfrom data.config import cfg, mask_type\nfrom layers import Detect\nfrom layers.interpolate import InterpolateModule\nfrom backbone import construct_backbone\n\nimport torch.backends.cudnn as cudnn\nfrom utils import timer\nfrom utils.functions import MovingAverage, make_net\n\n# This is required for Pytorch 1.0.1 on Windows to initialize Cuda on some driver versions.\n# See the bug report here: https://github.com/pytorch/pytorch/issues/17108\ntorch.cuda.current_device()\n\n# As of March 10, 2019, Pytorch DataParallel still doesn't support JIT Script Modules\nuse_jit = torch.cuda.device_count() <= 1\nif not use_jit:\n    print('Multiple GPUs detected! Turning off JIT.')\n\nScriptModuleWrapper = torch.jit.ScriptModule if use_jit else nn.Module\nscript_method_wrapper = torch.jit.script_method if use_jit else lambda fn, _rcn=None: fn\n\n\n\nclass Concat(nn.Module):\n    def __init__(self, nets, extra_params):\n        super().__init__()\n\n        self.nets = nn.ModuleList(nets)\n        self.extra_params = extra_params\n    \n    def forward(self, x):\n        # Concat each along the channel dimension\n        return torch.cat([net(x) for net in self.nets], dim=1, **self.extra_params)\n\nprior_cache = defaultdict(lambda: None)\n\nclass PredictionModule(nn.Module):\n    \"\"\"\n    The (c) prediction module adapted from DSSD:\n    https://arxiv.org/pdf/1701.06659.pdf\n\n    Note that this is slightly different to the module in the paper\n    because the Bottleneck block actually has a 3x3 convolution in\n    the middle instead of a 1x1 convolution. Though, I really can't\n    be arsed to implement it myself, and, who knows, this might be\n    better.\n\n    Args:\n        - in_channels:   The input feature size.\n        - out_channels:  The output feature size (must be a multiple of 4).\n        - aspect_ratios: A list of lists of priorbox aspect ratios (one list per scale).\n        - scales:        A list of priorbox scales relative to this layer's convsize.\n                         For instance: If this layer has convouts of size 30x30 for\n                                       an image of size 600x600, the 'default' (scale\n                                       of 1) for this layer would produce bounding\n                                       boxes with an area of 20x20px. If the scale is\n                                       .5 on the other hand, this layer would consider\n                                       bounding boxes with area 10x10px, etc.\n        - parent:        If parent is a PredictionModule, this module will use all the layers\n                         from parent instead of from this module.\n    \"\"\"\n    \n    def __init__(self, in_channels, out_channels=1024, aspect_ratios=[[1]], scales=[1], parent=None, index=0):\n        super().__init__()\n\n        self.num_classes = cfg.num_classes\n        self.mask_dim    = cfg.mask_dim # Defined by Yolact\n        self.num_priors  = sum(len(x)*len(scales) for x in aspect_ratios)\n        self.parent      = [parent] # Don't include this in the state dict\n        self.index       = index\n        self.num_heads   = cfg.num_heads # Defined by Yolact\n\n        if cfg.mask_proto_split_prototypes_by_head and cfg.mask_type == mask_type.lincomb:\n            self.mask_dim = self.mask_dim // self.num_heads\n\n        if cfg.mask_proto_prototypes_as_features:\n            in_channels += self.mask_dim\n        \n        if parent is None:\n            if cfg.extra_head_net is None:\n                out_channels = in_channels\n            else:\n                self.upfeature, out_channels = make_net(in_channels, cfg.extra_head_net)\n\n            if cfg.use_prediction_module:\n                self.block = Bottleneck(out_channels, out_channels // 4)\n                self.conv = nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=True)\n                self.bn = nn.BatchNorm2d(out_channels)\n\n            self.bbox_layer = nn.Conv2d(out_channels, self.num_priors * 4,                **cfg.head_layer_params)\n            self.conf_layer = nn.Conv2d(out_channels, self.num_priors * self.num_classes, **cfg.head_layer_params)\n            self.mask_layer = nn.Conv2d(out_channels, self.num_priors * self.mask_dim,    **cfg.head_layer_params)\n            \n            if cfg.use_mask_scoring:\n                self.score_layer = nn.Conv2d(out_channels, self.num_priors, **cfg.head_layer_params)\n\n            if cfg.use_instance_coeff:\n                self.inst_layer = nn.Conv2d(out_channels, self.num_priors * cfg.num_instance_coeffs, **cfg.head_layer_params)\n            \n            # What is this ugly lambda doing in the middle of all this clean prediction module code?\n            def make_extra(num_layers):\n                if num_layers == 0:\n                    return lambda x: x\n                else:\n                    # Looks more complicated than it is. This just creates an array of num_layers alternating conv-relu\n                    return nn.Sequential(*sum([[\n                        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n                        nn.ReLU(inplace=True)\n                    ] for _ in range(num_layers)], []))\n\n            self.bbox_extra, self.conf_extra, self.mask_extra = [make_extra(x) for x in cfg.extra_layers]\n            \n            if cfg.mask_type == mask_type.lincomb and cfg.mask_proto_coeff_gate:\n                self.gate_layer = nn.Conv2d(out_channels, self.num_priors * self.mask_dim, kernel_size=3, padding=1)\n\n        self.aspect_ratios = aspect_ratios\n        self.scales = scales\n\n        self.priors = None\n        self.last_conv_size = None\n        self.last_img_size = None\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            - x: The convOut from a layer in the backbone network\n                 Size: [batch_size, in_channels, conv_h, conv_w])\n\n        Returns a tuple (bbox_coords, class_confs, mask_output, prior_boxes) with sizes\n            - bbox_coords: [batch_size, conv_h*conv_w*num_priors, 4]\n            - class_confs: [batch_size, conv_h*conv_w*num_priors, num_classes]\n            - mask_output: [batch_size, conv_h*conv_w*num_priors, mask_dim]\n            - prior_boxes: [conv_h*conv_w*num_priors, 4]\n        \"\"\"\n        # In case we want to use another module's layers\n        src = self if self.parent[0] is None else self.parent[0]\n        \n        conv_h = x.size(2)\n        conv_w = x.size(3)\n        \n        if cfg.extra_head_net is not None:\n            x = src.upfeature(x)\n        \n        if cfg.use_prediction_module:\n            # The two branches of PM design (c)\n            a = src.block(x)\n            \n            b = src.conv(x)\n            b = src.bn(b)\n            b = F.relu(b)\n            \n            # TODO: Possibly switch this out for a product\n            x = a + b\n\n        bbox_x = src.bbox_extra(x)\n        conf_x = src.conf_extra(x)\n        mask_x = src.mask_extra(x)\n\n        bbox = src.bbox_layer(bbox_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 4)\n        conf = src.conf_layer(conf_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.num_classes)\n        \n        if cfg.eval_mask_branch:\n            mask = src.mask_layer(mask_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.mask_dim)\n        else:\n            mask = torch.zeros(x.size(0), bbox.size(1), self.mask_dim, device=bbox.device)\n\n        if cfg.use_mask_scoring:\n            score = src.score_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 1)\n\n        if cfg.use_instance_coeff:\n            inst = src.inst_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, cfg.num_instance_coeffs)    \n\n        # See box_utils.decode for an explanation of this\n        if cfg.use_yolo_regressors:\n            bbox[:, :, :2] = torch.sigmoid(bbox[:, :, :2]) - 0.5\n            bbox[:, :, 0] /= conv_w\n            bbox[:, :, 1] /= conv_h\n\n        if cfg.eval_mask_branch:\n            if cfg.mask_type == mask_type.direct:\n                mask = torch.sigmoid(mask)\n            elif cfg.mask_type == mask_type.lincomb:\n                mask = cfg.mask_proto_coeff_activation(mask)\n\n                if cfg.mask_proto_coeff_gate:\n                    gate = src.gate_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.mask_dim)\n                    mask = mask * torch.sigmoid(gate)\n\n        if cfg.mask_proto_split_prototypes_by_head and cfg.mask_type == mask_type.lincomb:\n            mask = F.pad(mask, (self.index * self.mask_dim, (self.num_heads - self.index - 1) * self.mask_dim), mode='constant', value=0)\n        \n        priors = self.make_priors(conv_h, conv_w, x.device)\n\n        preds = { 'loc': bbox, 'conf': conf, 'mask': mask, 'priors': priors }\n\n        if cfg.use_mask_scoring:\n            preds['score'] = score\n\n        if cfg.use_instance_coeff:\n            preds['inst'] = inst\n        \n        return preds\n\n    def make_priors(self, conv_h, conv_w, device):\n        \"\"\" Note that priors are [x,y,width,height] where (x,y) is the center of the box. \"\"\"\n        global prior_cache\n        size = (conv_h, conv_w)\n\n        with timer.env('makepriors'):\n            if self.last_img_size != (cfg._tmp_img_w, cfg._tmp_img_h):\n                prior_data = []\n\n                # Iteration order is important (it has to sync up with the convout)\n                for j, i in product(range(conv_h), range(conv_w)):\n                    # +0.5 because priors are in center-size notation\n                    x = (i + 0.5) / conv_w\n                    y = (j + 0.5) / conv_h\n                    \n                    for ars in self.aspect_ratios:\n                        for scale in self.scales:\n                            for ar in ars:\n                                if not cfg.backbone.preapply_sqrt:\n                                    ar = sqrt(ar)\n\n                                if cfg.backbone.use_pixel_scales:\n                                    w = scale * ar / cfg.max_size\n                                    h = scale / ar / cfg.max_size\n                                else:\n                                    w = scale * ar / conv_w\n                                    h = scale / ar / conv_h\n                                \n                                # This is for backward compatability with a bug where I made everything square by accident\n                                if cfg.backbone.use_square_anchors:\n                                    h = w\n\n                                prior_data += [x, y, w, h]\n\n                self.priors = torch.Tensor(prior_data, device=device).view(-1, 4).detach()\n                self.priors.requires_grad = False\n                self.last_img_size = (cfg._tmp_img_w, cfg._tmp_img_h)\n                self.last_conv_size = (conv_w, conv_h)\n                prior_cache[size] = None\n            elif self.priors.device != device:\n                # This whole weird situation is so that DataParalell doesn't copy the priors each iteration\n                if prior_cache[size] is None:\n                    prior_cache[size] = {}\n                \n                if device not in prior_cache[size]:\n                    prior_cache[size][device] = self.priors.to(device)\n\n                self.priors = prior_cache[size][device]\n        \n        return self.priors\n\nclass FPN(ScriptModuleWrapper):\n    \"\"\"\n    Implements a general version of the FPN introduced in\n    https://arxiv.org/pdf/1612.03144.pdf\n\n    Parameters (in cfg.fpn):\n        - num_features (int): The number of output features in the fpn layers.\n        - interpolation_mode (str): The mode to pass to F.interpolate.\n        - num_downsample (int): The number of downsampled layers to add onto the selected layers.\n                                These extra layers are downsampled from the last selected layer.\n\n    Args:\n        - in_channels (list): For each conv layer you supply in the forward pass,\n                              how many features will it have?\n    \"\"\"\n    __constants__ = ['interpolation_mode', 'num_downsample', 'use_conv_downsample', 'relu_pred_layers',\n                     'lat_layers', 'pred_layers', 'downsample_layers', 'relu_downsample_layers']\n\n    def __init__(self, in_channels):\n        super().__init__()\n\n        self.lat_layers  = nn.ModuleList([\n            nn.Conv2d(x, cfg.fpn.num_features, kernel_size=1)\n            for x in reversed(in_channels)\n        ])\n\n        # This is here for backwards compatability\n        padding = 1 if cfg.fpn.pad else 0\n        self.pred_layers = nn.ModuleList([\n            nn.Conv2d(cfg.fpn.num_features, cfg.fpn.num_features, kernel_size=3, padding=padding)\n            for _ in in_channels\n        ])\n\n        if cfg.fpn.use_conv_downsample:\n            self.downsample_layers = nn.ModuleList([\n                nn.Conv2d(cfg.fpn.num_features, cfg.fpn.num_features, kernel_size=3, padding=1, stride=2)\n                for _ in range(cfg.fpn.num_downsample)\n            ])\n        \n        self.interpolation_mode     = cfg.fpn.interpolation_mode\n        self.num_downsample         = cfg.fpn.num_downsample\n        self.use_conv_downsample    = cfg.fpn.use_conv_downsample\n        self.relu_downsample_layers = cfg.fpn.relu_downsample_layers\n        self.relu_pred_layers       = cfg.fpn.relu_pred_layers\n\n    @script_method_wrapper\n    def forward(self, convouts:List[torch.Tensor]):\n        \"\"\"\n        Args:\n            - convouts (list): A list of convouts for the corresponding layers in in_channels.\n        Returns:\n            - A list of FPN convouts in the same order as x with extra downsample layers if requested.\n        \"\"\"\n\n        out = []\n        x = torch.zeros(1, device=convouts[0].device)\n        for i in range(len(convouts)):\n            out.append(x)\n\n        # For backward compatability, the conv layers are stored in reverse but the input and output is\n        # given in the correct order. Thus, use j=-i-1 for the input and output and i for the conv layers.\n        j = len(convouts)\n        for lat_layer in self.lat_layers:\n            j -= 1\n\n            if j < len(convouts) - 1:\n                _, _, h, w = convouts[j].size()\n                x = F.interpolate(x, size=(h, w), mode=self.interpolation_mode, align_corners=False)\n            \n            x = x + lat_layer(convouts[j])\n            out[j] = x\n        \n        # This janky second loop is here because TorchScript.\n        j = len(convouts)\n        for pred_layer in self.pred_layers:\n            j -= 1\n            out[j] = pred_layer(out[j])\n\n            if self.relu_pred_layers:\n                F.relu(out[j], inplace=True)\n\n        cur_idx = len(out)\n\n        # In the original paper, this takes care of P6\n        if self.use_conv_downsample:\n            for downsample_layer in self.downsample_layers:\n                out.append(downsample_layer(out[-1]))\n        else:\n            for idx in range(self.num_downsample):\n                # Note: this is an untested alternative to out.append(out[-1][:, :, ::2, ::2]). Thanks TorchScript.\n                out.append(nn.functional.max_pool2d(out[-1], 1, stride=2))\n\n        if self.relu_downsample_layers:\n            for idx in range(len(out) - cur_idx):\n                out[idx] = F.relu(out[idx + cur_idx], inplace=False)\n\n        return out\n\nclass FastMaskIoUNet(ScriptModuleWrapper):\n\n    def __init__(self):\n        super().__init__()\n        input_channels = 1\n        last_layer = [(cfg.num_classes-1, 1, {})]\n        self.maskiou_net, _ = make_net(input_channels, cfg.maskiou_net + last_layer, include_last_relu=True)\n\n    def forward(self, x):\n        x = self.maskiou_net(x)\n        maskiou_p = F.max_pool2d(x, kernel_size=x.size()[2:]).squeeze(-1).squeeze(-1)\n\n        return maskiou_p\n\n\n\nclass Yolact(nn.Module):\n    \"\"\"\n\n\n    ██╗   ██╗ ██████╗ ██╗      █████╗  ██████╗████████╗\n    ╚██╗ ██╔╝██╔═══██╗██║     ██╔══██╗██╔════╝╚══██╔══╝\n     ╚████╔╝ ██║   ██║██║     ███████║██║        ██║   \n      ╚██╔╝  ██║   ██║██║     ██╔══██║██║        ██║   \n       ██║   ╚██████╔╝███████╗██║  ██║╚██████╗   ██║   \n       ╚═╝    ╚═════╝ ╚══════╝╚═╝  ╚═╝ ╚═════╝   ╚═╝ \n\n\n    You can set the arguments by changing them in the backbone config object in config.py.\n\n    Parameters (in cfg.backbone):\n        - selected_layers: The indices of the conv layers to use for prediction.\n        - pred_scales:     A list with len(selected_layers) containing tuples of scales (see PredictionModule)\n        - pred_aspect_ratios: A list of lists of aspect ratios with len(selected_layers) (see PredictionModule)\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        self.backbone = construct_backbone(cfg.backbone)\n\n        if cfg.freeze_bn:\n            self.freeze_bn()\n\n        # Compute mask_dim here and add it back to the config. Make sure Yolact's constructor is called early!\n        if cfg.mask_type == mask_type.direct:\n            cfg.mask_dim = cfg.mask_size**2\n        elif cfg.mask_type == mask_type.lincomb:\n            if cfg.mask_proto_use_grid:\n                self.grid = torch.Tensor(np.load(cfg.mask_proto_grid_file))\n                self.num_grids = self.grid.size(0)\n            else:\n                self.num_grids = 0\n\n            self.proto_src = cfg.mask_proto_src\n            \n            if self.proto_src is None: in_channels = 3\n            elif cfg.fpn is not None: in_channels = cfg.fpn.num_features\n            else: in_channels = self.backbone.channels[self.proto_src]\n            in_channels += self.num_grids\n\n            # The include_last_relu=false here is because we might want to change it to another function\n            self.proto_net, cfg.mask_dim = make_net(in_channels, cfg.mask_proto_net, include_last_relu=False)\n\n            if cfg.mask_proto_bias:\n                cfg.mask_dim += 1\n\n\n        self.selected_layers = cfg.backbone.selected_layers\n        src_channels = self.backbone.channels\n\n        if cfg.use_maskiou:\n            self.maskiou_net = FastMaskIoUNet()\n\n        if cfg.fpn is not None:\n            # Some hacky rewiring to accomodate the FPN\n            self.fpn = FPN([src_channels[i] for i in self.selected_layers])\n            self.selected_layers = list(range(len(self.selected_layers) + cfg.fpn.num_downsample))\n            src_channels = [cfg.fpn.num_features] * len(self.selected_layers)\n\n\n        self.prediction_layers = nn.ModuleList()\n        cfg.num_heads = len(self.selected_layers)\n\n        for idx, layer_idx in enumerate(self.selected_layers):\n            # If we're sharing prediction module weights, have every module's parent be the first one\n            parent = None\n            if cfg.share_prediction_module and idx > 0:\n                parent = self.prediction_layers[0]\n\n            pred = PredictionModule(src_channels[layer_idx], src_channels[layer_idx],\n                                    aspect_ratios = cfg.backbone.pred_aspect_ratios[idx],\n                                    scales        = cfg.backbone.pred_scales[idx],\n                                    parent        = parent,\n                                    index         = idx)\n            self.prediction_layers.append(pred)\n\n        # Extra parameters for the extra losses\n        if cfg.use_class_existence_loss:\n            # This comes from the smallest layer selected\n            # Also note that cfg.num_classes includes background\n            self.class_existence_fc = nn.Linear(src_channels[-1], cfg.num_classes - 1)\n        \n        if cfg.use_semantic_segmentation_loss:\n            self.semantic_seg_conv = nn.Conv2d(src_channels[0], cfg.num_classes-1, kernel_size=1)\n\n        # For use in evaluation\n        self.detect = Detect(cfg.num_classes, bkg_label=0, top_k=cfg.nms_top_k,\n            conf_thresh=cfg.nms_conf_thresh, nms_thresh=cfg.nms_thresh)\n\n    def save_weights(self, path):\n        \"\"\" Saves the model's weights using compression because the file sizes were getting too big. \"\"\"\n        torch.save(self.state_dict(), path)\n    \n    def load_weights(self, path):\n        \"\"\" Loads weights from a compressed save file. \"\"\"\n        state_dict = torch.load(path)\n\n        # For backward compatability, remove these (the new variable is called layers)\n        for key in list(state_dict.keys()):\n            if key.startswith('backbone.layer') and not key.startswith('backbone.layers'):\n                del state_dict[key]\n        \n            # Also for backward compatibility with v1.0 weights, do this check\n            if key.startswith('fpn.downsample_layers.'):\n                if cfg.fpn is not None and int(key.split('.')[2]) >= cfg.fpn.num_downsample:\n                    del state_dict[key]\n        self.load_state_dict(state_dict)\n\n    def init_weights(self, backbone_path):\n        \"\"\" Initialize weights for training. \"\"\"\n        # Initialize the backbone with the pretrained weights.\n        self.backbone.init_backbone(backbone_path)\n\n        conv_constants = getattr(nn.Conv2d(1, 1, 1), '__constants__')\n        \n        # Quick lambda to test if one list contains the other\n        def all_in(x, y):\n            for _x in x:\n                if _x not in y:\n                    return False\n            return True\n\n        # Initialize the rest of the conv layers with xavier\n        for name, module in self.named_modules():\n            # See issue #127 for why we need such a complicated condition if the module is a WeakScriptModuleProxy\n            # Broke in 1.3 (see issue #175), WeakScriptModuleProxy was turned into just ScriptModule.\n            # Broke in 1.4 (see issue #292), where RecursiveScriptModule is the new star of the show.\n            # Note that this might break with future pytorch updates, so let me know if it does\n            is_script_conv = False\n            if 'Script' in type(module).__name__:\n                # 1.4 workaround: now there's an original_name member so just use that\n                if hasattr(module, 'original_name'):\n                    is_script_conv = 'Conv' in module.original_name\n                # 1.3 workaround: check if this has the same constants as a conv module\n                else:\n                    is_script_conv = (\n                        all_in(module.__dict__['_constants_set'], conv_constants)\n                        and all_in(conv_constants, module.__dict__['_constants_set']))\n            \n            is_conv_layer = isinstance(module, nn.Conv2d) or is_script_conv\n\n            if is_conv_layer and module not in self.backbone.backbone_modules:\n                nn.init.xavier_uniform_(module.weight.data)\n\n                if module.bias is not None:\n                    if cfg.use_focal_loss and 'conf_layer' in name:\n                        if not cfg.use_sigmoid_focal_loss:\n                            # Initialize the last layer as in the focal loss paper.\n                            # Because we use softmax and not sigmoid, I had to derive an alternate expression\n                            # on a notecard. Define pi to be the probability of outputting a foreground detection.\n                            # Then let z = sum(exp(x)) - exp(x_0). Finally let c be the number of foreground classes.\n                            # Chugging through the math, this gives us\n                            #   x_0 = log(z * (1 - pi) / pi)    where 0 is the background class\n                            #   x_i = log(z / c)                for all i > 0\n                            # For simplicity (and because we have a degree of freedom here), set z = 1. Then we have\n                            #   x_0 =  log((1 - pi) / pi)       note: don't split up the log for numerical stability\n                            #   x_i = -log(c)                   for all i > 0\n                            module.bias.data[0]  = np.log((1 - cfg.focal_loss_init_pi) / cfg.focal_loss_init_pi)\n                            module.bias.data[1:] = -np.log(module.bias.size(0) - 1)\n                        else:\n                            module.bias.data[0]  = -np.log(cfg.focal_loss_init_pi / (1 - cfg.focal_loss_init_pi))\n                            module.bias.data[1:] = -np.log((1 - cfg.focal_loss_init_pi) / cfg.focal_loss_init_pi)\n                    else:\n                        module.bias.data.zero_()\n    \n    def train(self, mode=True):\n        super().train(mode)\n\n        if cfg.freeze_bn:\n            self.freeze_bn()\n\n    def freeze_bn(self, enable=False):\n        \"\"\" Adapted from https://discuss.pytorch.org/t/how-to-train-with-frozen-batchnorm/12106/8 \"\"\"\n        for module in self.modules():\n            if isinstance(module, nn.BatchNorm2d):\n                module.train() if enable else module.eval()\n\n                module.weight.requires_grad = enable\n                module.bias.requires_grad = enable\n    \n    def forward(self, x):\n        \"\"\" The input should be of size [batch_size, 3, img_h, img_w] \"\"\"\n        _, _, img_h, img_w = x.size()\n        cfg._tmp_img_h = img_h\n        cfg._tmp_img_w = img_w\n        \n        with timer.env('backbone'):\n            outs = self.backbone(x)\n\n        if cfg.fpn is not None:\n            with timer.env('fpn'):\n                # Use backbone.selected_layers because we overwrote self.selected_layers\n                outs = [outs[i] for i in cfg.backbone.selected_layers]\n                outs = self.fpn(outs)\n\n        proto_out = None\n        if cfg.mask_type == mask_type.lincomb and cfg.eval_mask_branch:\n            with timer.env('proto'):\n                proto_x = x if self.proto_src is None else outs[self.proto_src]\n                \n                if self.num_grids > 0:\n                    grids = self.grid.repeat(proto_x.size(0), 1, 1, 1)\n                    proto_x = torch.cat([proto_x, grids], dim=1)\n\n                proto_out = self.proto_net(proto_x)\n                proto_out = cfg.mask_proto_prototype_activation(proto_out)\n\n                if cfg.mask_proto_prototypes_as_features:\n                    # Clone here because we don't want to permute this, though idk if contiguous makes this unnecessary\n                    proto_downsampled = proto_out.clone()\n\n                    if cfg.mask_proto_prototypes_as_features_no_grad:\n                        proto_downsampled = proto_out.detach()\n                \n                # Move the features last so the multiplication is easy\n                proto_out = proto_out.permute(0, 2, 3, 1).contiguous()\n\n                if cfg.mask_proto_bias:\n                    bias_shape = [x for x in proto_out.size()]\n                    bias_shape[-1] = 1\n                    proto_out = torch.cat([proto_out, torch.ones(*bias_shape)], -1)\n\n\n        with timer.env('pred_heads'):\n            pred_outs = { 'loc': [], 'conf': [], 'mask': [], 'priors': [] }\n\n            if cfg.use_mask_scoring:\n                pred_outs['score'] = []\n\n            if cfg.use_instance_coeff:\n                pred_outs['inst'] = []\n            \n            for idx, pred_layer in zip(self.selected_layers, self.prediction_layers):\n                pred_x = outs[idx]\n\n                if cfg.mask_type == mask_type.lincomb and cfg.mask_proto_prototypes_as_features:\n                    # Scale the prototypes down to the current prediction layer's size and add it as inputs\n                    proto_downsampled = F.interpolate(proto_downsampled, size=outs[idx].size()[2:], mode='bilinear', align_corners=False)\n                    pred_x = torch.cat([pred_x, proto_downsampled], dim=1)\n\n                # A hack for the way dataparallel works\n                if cfg.share_prediction_module and pred_layer is not self.prediction_layers[0]:\n                    pred_layer.parent = [self.prediction_layers[0]]\n\n                p = pred_layer(pred_x)\n                \n                for k, v in p.items():\n                    pred_outs[k].append(v)\n\n        for k, v in pred_outs.items():\n            pred_outs[k] = torch.cat(v, -2)\n\n        if proto_out is not None:\n            pred_outs['proto'] = proto_out\n\n        if self.training:\n            # For the extra loss functions\n            if cfg.use_class_existence_loss:\n                pred_outs['classes'] = self.class_existence_fc(outs[-1].mean(dim=(2, 3)))\n\n            if cfg.use_semantic_segmentation_loss:\n                pred_outs['segm'] = self.semantic_seg_conv(outs[0])\n\n            return pred_outs\n        else:\n            if cfg.use_mask_scoring:\n                pred_outs['score'] = torch.sigmoid(pred_outs['score'])\n\n            if cfg.use_focal_loss:\n                if cfg.use_sigmoid_focal_loss:\n                    # Note: even though conf[0] exists, this mode doesn't train it so don't use it\n                    pred_outs['conf'] = torch.sigmoid(pred_outs['conf'])\n                    if cfg.use_mask_scoring:\n                        pred_outs['conf'] *= pred_outs['score']\n                elif cfg.use_objectness_score:\n                    # See focal_loss_sigmoid in multibox_loss.py for details\n                    objectness = torch.sigmoid(pred_outs['conf'][:, :, 0])\n                    pred_outs['conf'][:, :, 1:] = objectness[:, :, None] * F.softmax(pred_outs['conf'][:, :, 1:], -1)\n                    pred_outs['conf'][:, :, 0 ] = 1 - objectness\n                else:\n                    pred_outs['conf'] = F.softmax(pred_outs['conf'], -1)\n            else:\n\n                if cfg.use_objectness_score:\n                    objectness = torch.sigmoid(pred_outs['conf'][:, :, 0])\n                    \n                    pred_outs['conf'][:, :, 1:] = (objectness > 0.10)[..., None] \\\n                        * F.softmax(pred_outs['conf'][:, :, 1:], dim=-1)\n                    \n                else:\n                    pred_outs['conf'] = F.softmax(pred_outs['conf'], -1)\n\n            return self.detect(pred_outs, self)\n\n\n\n\n# Some testing code\nif __name__ == '__main__':\n    from utils.functions import init_console\n    init_console()\n\n    # Use the first argument to set the config if you want\n    import sys\n    if len(sys.argv) > 1:\n        from data.config import set_cfg\n        set_cfg(sys.argv[1])\n\n    net = Yolact()\n    net.train()\n    net.init_weights(backbone_path='weights/' + cfg.backbone.path)\n\n    # GPU\n    net = net.cuda()\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n\n    x = torch.zeros((1, 3, cfg.max_size, cfg.max_size))\n    y = net(x)\n\n    for p in net.prediction_layers:\n        print(p.last_conv_size)\n\n    print()\n    for k, a in y.items():\n        print(k + ': ', a.size(), torch.sum(a))\n    exit()\n    \n    net(x)\n    # timer.disable('pass2')\n    avg = MovingAverage()\n    try:\n        while True:\n            timer.reset()\n            with timer.env('everything else'):\n                net(x)\n            avg.add(timer.total_time())\n            print('\\033[2J') # Moves console cursor to 0,0\n            timer.print_stats()\n            print('Avg fps: %.2f\\tAvg ms: %.2f         ' % (1/avg.get_avg(), avg.get_avg()*1000))\n    except KeyboardInterrupt:\n        pass\n"
        }
      ]
    }
  ]
}