{
  "metadata": {
    "timestamp": 1736560442801,
    "page": 17,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "injetlee/Python",
      "stars": 9838,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.013671875,
          "content": "/no_use\n*.xlsx"
        },
        {
          "name": "CpuToInfluxdb.py",
          "type": "blob",
          "size": 0.9296875,
          "content": "import psutil\nimport os\nfrom influxdb import InfluxDBClient\nimport time,math,random\n\n\n#获取当前运行的pid\np1=psutil.Process(os.getpid()) \n\n\nfrom influxdb import InfluxDBClient\nimport time,math,random\nwhile True:\n    a = psutil.virtual_memory().percent  #内存占用率\n\n    b = psutil.cpu_percent(interval=1.0) #cpu占用率\n\n    json_body = [\n        {\n            \"measurement\": \"cpu_load_short\",\n            \"tags\": {\n                \"host\": \"server01\",\n                \"region\": \"us-west\"\n            },\n            #\"time\": \"2009-11-10T23:00:00Z\",\n            \"fields\": {\n                \"cpu\": b,\n                \"mem\": a\n            }\n        }\n    ]\n    client = InfluxDBClient('localhost', 8086, 'root', 'root', 'xxyyxx')\n    client.create_database('xxyyxx',if_not_exists=False)\n    client.write_points(json_body)\n    #result = client.query('select value from cpu_load_short;')\n    #print(\"Result: {0}\".format(result))\n    time.sleep(2)"
        },
        {
          "name": "ModifyFilename.py",
          "type": "blob",
          "size": 0.33984375,
          "content": "import os\ndir = os.getcwd()\nsubdir = os.listdir(dir)\nfor i in subdir:\n    path = os.path.join(dir, i)\n    if os.path.isdir(path):\n        end_dir = os.listdir(path)\n        for i in range(len(end_dir)):\n            newname = end_dir[i][0:50]\n            os.rename(os.path.join(path, end_dir[\n                      i]), os.path.join(path, newname))\n"
        },
        {
          "name": "Python 黑魔法",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 2.439453125,
          "content": "\n# 欢迎关注我的微信公众号【智能制造社区】\n\n## 左手代码，右手制造，分享智能制造相关技术和业务，包括 Python, C#, 数据库，工业大数据、物联网技术及MES/ERP/SAP等系统。\n\n## 可以通过微信公众号加我好友\n\n![二维码](qrcode.jpg)\n\n# 内容列表\n\n## [Python微信公众号开发](https://github.com/injetlee/Python/tree/master/wechat)\n\n- ### Python 微信公众号开发—小白篇(一)\n\n- ### Python 公众号开发—颜值检测\n\n## [Python 爬虫入门合集](https://github.com/injetlee/Python/tree/master/%E7%88%AC%E8%99%AB%E9%9B%86%E5%90%88)\n\n- ### Python 爬虫入门(一)——爬取糗事百科\n\n- ### Python 爬虫入门(二)——爬取妹子图\n\n- ### Python 爬虫——Python 岗位分析报告\n\n- ### Python 爬虫利器——Selenium介绍\n\n- ### Python 爬虫—— 抖音 App 视频抓包爬取\n\n## [Python 黑魔法](https://github.com/injetlee/Python/tree/master/Python%20%E9%BB%91%E9%AD%94%E6%B3%95)\n\n- ### Python 远程关机\n\n## SQL 数据库\n\n- [1 小时 SQL 极速入门（一）](https://mp.weixin.qq.com/s/Lx4B349OlD49ihJPnB6YiA)\n- [1 小时 SQL 极速入门（二）](https://mp.weixin.qq.com/s/D-CEtGYomne5kV_Ji4lodA)\n- [1 小时 SQL 极速入门（三）](https://mp.weixin.qq.com/s/7aJqrhCNcvnt2gO3p5P50Q)\n- [SQL 高级查询——（层次化查询，递归）](https://mp.weixin.qq.com/s/R9Yldd-5AK4ObRA9Lfbz-Q)\n- [GROUP BY高级查询,ROLLUP，CUBE，GROUPPING详解](https://mp.weixin.qq.com/s/_OK6dtHGhp7ukC2pe1ginQ)\n- [SQL 行转列，列转行](https://mp.weixin.qq.com/s/xOFIg42FQhNpyg94ajhtqQ)\n\n## 其他\n\n- 1.[获取当前CPU状态，存储到Influxdb](https://github.com/injetlee/demo/blob/master/CpuToInfluxdb.py)\n\n- 2.[模拟登录知乎](https://github.com/injetlee/demo/blob/master/login_zhihu.py)\n\n- 3.[对目录下所有文件计数](https://github.com/injetlee/demo/blob/master/countFile.py)\n\n- 4.[爬取豆瓣电影top250](https://github.com/injetlee/demo/blob/master/douban_movie.py)\n\n- 5.[Excel文件读入数据库](https://github.com/injetlee/demo/blob/master/excelToDatabase.py)\n\n- 6.[爬取拉勾网职位信息](https://github.com/injetlee/demo/blob/master/lagouSpider.py)\n\n- 7.[批量修改文件名](https://github.com/injetlee/demo/blob/master/ModifyFilename.py)\n\n- 8.[读写excel](https://github.com/injetlee/demo/blob/master/readExcel.py)\n\n- 9.[下载必应首页图片,只下载当天的，一张。](https://github.com/injetlee/Python/blob/master/biyingSpider.py)\n"
        },
        {
          "name": "biyingSpider.py",
          "type": "blob",
          "size": 0.326171875,
          "content": "import requests\nimport re\nimport time\nlocal = time.strftime(\"%Y.%m.%d\")\nurl = 'http://cn.bing.com/'\ncon = requests.get(url)\ncontent = con.text\nreg = r\"(az/hprichbg/rb/.*?.jpg)\"\na = re.findall(reg, content, re.S)[0]\nprint(a)\npicUrl = url + a\nread = requests.get(picUrl)\nf = open('%s.jpg' % local, 'wb')\nf.write(read.content)\nf.close()\n"
        },
        {
          "name": "countFile.py",
          "type": "blob",
          "size": 0.6328125,
          "content": "import os\nresult = []\ndef get_all(cwd):\n    get_dir = os.listdir(cwd)  #遍历当前目录，获取文件列表\n    for i in get_dir:          \n        sub_dir = os.path.join(cwd,i)  # 把第一步获取的文件加入路径\n        if os.path.isdir(sub_dir):     #如果当前仍然是文件夹，递归调用\n            get_all(sub_dir)\n        else:\n            ax = os.path.basename(sub_dir)  #如果当前路径不是文件夹，则把文件名放入列表\n            result.append(ax)\n            print(len(result))   #对列表计数\n            \nif __name__ == \"__main__\": \n    cur_path = os.getcwd()   #当前目录\n    get_all(cur_path)"
        },
        {
          "name": "countPm.py",
          "type": "blob",
          "size": 0.7197265625,
          "content": "# -*- coding:utf-8 -*-\ndef count_pm(*args):\n    alist = list([round(i*2-8,2) for i in args])  #计算三种颗粒浓度\n    result = []\n    for pm in alist:\n    \tpm_abs = abs(pm)\n    \tresult.append(generate_iso_code(pm_abs))\n    print (result)\n    return result\n    \t\ndef generate_iso_code(x):\n\tpm_value = [0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.3,2.5,5,10,20,40,80]  #颗粒浓度\n\tiso = list(range(1,25))   #iso级别，共24级\n\tfor i in range(len(pm_value)):           #for循环得到某个浓度范围的iso4006级别\n\t\tif pm_value[i] < x <= pm_value[i+1]:\n\t\t\tiso_code = iso[i]\n\t\t\tbreak\n\treturn iso_code\n\t\t\t\nif __name__ == '__main__':\n    count_pm(7.95,5.85,3.98)\t\t\n    count_pm(7.918,5.949,5.456)\t\n    count_pm(6.916,3.956,3.956)\t\t\n"
        },
        {
          "name": "douban_book.py",
          "type": "blob",
          "size": 1.373046875,
          "content": "from bs4 import BeautifulSoup\nimport requests\nfrom openpyxl import Workbook\nexcel_name = \"书籍.xlsx\"\nwb = Workbook()\nws1 = wb.active\nws1.title='书籍'\n\n\ndef get_html(url):\n    header = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n    html = requests.get(url, headers=header).content\n    return html\n\n\ndef get_con(html):\n    soup = BeautifulSoup(html,'html.parser')\n    book_list = soup.find('div', attrs={'class': 'article'})\n    page = soup.find('div', attrs={'class': 'paginator'})\n    next_page = page.find('span', attrs={'class': 'next'}).find('a')\n    name = []\n    for i in book_list.find_all('table'):\n        book_name = i.find('div', attrs={'class': 'pl2'})\n        m = list(book_name.find('a').stripped_strings)\n        if len(m)>1:\n            x = m[0]+m[1]\n        else:\n            x = m[0]\n        #print(x)\n        name.append(x)\n    if next_page:\n        return name, next_page.get('href')\n    else:\n        return name, None\n\n\ndef main():\n    url = 'https://book.douban.com/top250'\n    name_list=[]\n    while url:\n        html = get_html(url)\n        name, url = get_con(html)\n        name_list = name_list + name\n    for i in name_list:\n        location = 'A%s'%(name_list.index(i)+1)\n        print(i)\n        print(location)\n        ws1[location]=i\n    wb.save(filename=excel_name)\n\n\nif __name__ == '__main__':\n    main()\n\n"
        },
        {
          "name": "douban_movie.py",
          "type": "blob",
          "size": 2.419921875,
          "content": "#!/usr/bin/env python\n# encoding=utf-8\nimport requests\nimport re\nimport codecs\nfrom bs4 import BeautifulSoup\nfrom openpyxl import Workbook\nwb = Workbook()\ndest_filename = '电影.xlsx'\nws1 = wb.active\nws1.title = \"电影top250\"\n\nDOWNLOAD_URL = 'http://movie.douban.com/top250/'\n\n\ndef download_page(url):\n    \"\"\"获取url地址页面内容\"\"\"\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36'\n    }\n    data = requests.get(url, headers=headers).content\n    return data\n\n\ndef get_li(doc):\n    soup = BeautifulSoup(doc, 'html.parser')\n    ol = soup.find('ol', class_='grid_view')\n    name = []  # 名字\n    star_con = []  # 评价人数\n    score = []  # 评分\n    info_list = []  # 短评\n    for i in ol.find_all('li'):\n        detail = i.find('div', attrs={'class': 'hd'})\n        movie_name = detail.find(\n            'span', attrs={'class': 'title'}).get_text()  # 电影名字\n        level_star = i.find(\n            'span', attrs={'class': 'rating_num'}).get_text()  # 评分\n        star = i.find('div', attrs={'class': 'star'})\n        star_num = star.find(text=re.compile('评价'))  # 评价\n\n        info = i.find('span', attrs={'class': 'inq'})  # 短评\n        if info:  # 判断是否有短评\n            info_list.append(info.get_text())\n        else:\n            info_list.append('无')\n        score.append(level_star)\n\n        name.append(movie_name)\n        star_con.append(star_num)\n    page = soup.find('span', attrs={'class': 'next'}).find('a')  # 获取下一页\n    if page:\n        return name, star_con, score, info_list, DOWNLOAD_URL + page['href']\n    return name, star_con, score, info_list, None\n\n\ndef main():\n    url = DOWNLOAD_URL\n    name = []\n    star_con = []\n    score = []\n    info = []\n    while url:\n        doc = download_page(url)\n        movie, star, level_num, info_list, url = get_li(doc)\n        name = name + movie\n        star_con = star_con + star\n        score = score + level_num\n        info = info + info_list\n    for (i, m, o, p) in zip(name, star_con, score, info):\n        col_A = 'A%s' % (name.index(i) + 1)\n        col_B = 'B%s' % (name.index(i) + 1)\n        col_C = 'C%s' % (name.index(i) + 1)\n        col_D = 'D%s' % (name.index(i) + 1)\n        ws1[col_A] = i\n        ws1[col_B] = m\n        ws1[col_C] = o\n        ws1[col_D] = p\n    wb.save(filename=dest_filename)\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "excelToDatabase.py",
          "type": "blob",
          "size": 0.81640625,
          "content": "from openpyxl import load_workbook\nimport pymysql\nconfig = {\n\t'host': '127.0.0.1',\n\t'port':3306,\n\t'user': 'root',\n\t'password': 'root',\n\t'charset': 'utf8mb4',\n\t#'cursorclass': pymysql.cursors.DictCursor\n\n}\nconn = pymysql.connect(**config)\nconn.autocommit(1)\ncursor = conn.cursor()\nname = 'lyexcel'\ncursor.execute('create database if not exists %s' %name)\nconn.select_db(name)\ntable_name = 'info'\ncursor.execute('create table if not exists %s(id MEDIUMINT NOT NULL AUTO_INCREMENT,name varchar(30),tel varchar(30),primary key (id))'%table_name)\n\nwb2 = load_workbook('hpu.xlsx')\nws=wb2.get_sheet_names()\nfor row in wb2:\n\tprint(\"1\")\n\tfor cell in row:\n\t\tvalue1=(cell[0].value,cell[4].value)\n\t\tcursor.execute('insert into info (name,tel) values(%s,%s)',value1)\n\nprint(\"overing...\")\n# for row in A:\n# \tprint(row)\n#print (wb2.get_sheet_names())\n"
        },
        {
          "name": "image_recognition_zhihu.py",
          "type": "blob",
          "size": 6.7548828125,
          "content": "# -*- coding:UTF-8 -*-\n\nimport  requests , time ,random\nimport  hmac ,json ,base64\nfrom bs4 import BeautifulSoup\nfrom hashlib import sha1\nimport TencentYoutuyun\nfrom PIL import Image\nimport uuid\n\n\n    \ndef recognition_captcha(data):\n    ''' 识别验证码 '''\n\n    file_id = str(uuid.uuid1())\n    filename = 'captcha_'+ file_id +'.gif'\n    filename_png =  'captcha_'+ file_id +'.png'\n\n    if(data is None):\n        return \n    data = base64.b64decode(data.encode('utf-8'))\n    with open( filename ,'wb') as fb:\n        fb.write( data )    \n    \n    appid = 'appid' # 接入优图服务，注册账号获取 \n    secret_id = 'secret_id'  \n    secret_key = 'secret_key'  \n    userid= 'userid' \n    end_point = TencentYoutuyun.conf.API_YOUTU_END_POINT   \n\n    youtu = TencentYoutuyun.YouTu(appid, secret_id, secret_key, userid, end_point) # 初始化\n\n    # 拿到的是gif格式，而优图只支持 JPG PNG BMP 其中之一，这时我们需要 pip install Pillow 来转换格式\n    im = Image.open( filename)\n    im.save( filename_png ,\"png\")\n    im.close()\n    \n    result = youtu.generalocr( filename_png , data_type = 0 , seq = '')  #  0代表本地路径，1代表url\n\n    return result\n\n\ndef get_captcha(sessiona,headers):\n    ''' 获取验证码 '''\n    \n    need_cap = False\n\n    while( need_cap is not True):\n        try:\n            sessiona.get('https://www.zhihu.com/signin',headers=headers)  # 拿cookie:_xsrf\n            resp2 = sessiona.get('https://www.zhihu.com/api/v3/oauth/captcha?lang=cn',headers=headers)  # 拿cookie:capsion_ticket \n            need_cap = json.loads(resp2.text)[\"show_captcha\"]  # {\"show_captcha\":false} 表示不用验证码\n            time.sleep( 0.5 + random.randint(1,9)/10 )\n        except Exception:\n            continue\n\n    try:\n        resp3 = sessiona.put('https://www.zhihu.com/api/v3/oauth/captcha?lang=cn',headers=headers) # 拿到验证码数据，注意是put\n        img_data = json.loads(resp3.text)[\"img_base64\"]\n    except Exception:\n        return     \n    \n\n    return img_data\n\ndef create_point( point_data, confidence ):\n    ''' 获得点阵 '''\n\n    # 实际操作下，套路不深，x间隔25，y相同，共7个点 ，先模拟意思一下\n    points = {1:[ 20.5,25.1875],2:[ 45.5,25.1875],3:[ 70.5,25.1875],4:[ 95.5,25.1875],5:[120.5,25.1875],6:[145.5,25.1875],7:[170.5,25.1875]}\n    wi = 0\n    input_points = []\n    \n    for word in ( point_data['items'][0]['words'] ):\n        wi = wi+1\n        if( word['confidence'] < confidence ):\n            try:\n                input_points.append(points[wi]) # 倒置的中文，优图识别不出来，置信度会低于0.5\n            except KeyError:\n                continue\n        \n    if( len(input_points) > 2 or len(input_points) == 0 ):\n        return []  # 7个字中只有2个倒置中文的成功率高\n    \n    result = {}\n    result['img_size']=[200,44]\n    result['input_points']=input_points\n    result = json.dumps(result)\n    print(result)\n    return result\n\ndef bolting(k_low,k_hi,k3_confidence):\n    ''' 筛选把握大的进行验证 '''\n\n    start = time.time()\n    \n    is_success = False\n    while(is_success is not True):\n    \n        points_len = 1\n        angle = -20\n        img_ko = []\n\n        while(points_len != 21  or  angle < k_low  or angle > k_hi ):  \n            img_data = get_captcha(sessiona,headers)\n            img_ko = recognition_captcha(img_data)\n     \n            ## json.dumps 序列化时对中文默认使用的ascii编码.想输出真正的中文需要指定ensure_ascii=False\n            # img_ko_json = json.dumps(img_ko , indent =2 ,ensure_ascii=False ) \n            # img_ko_json = img_ko_json.encode('raw_unicode_escape') ## 因为python3的原因，也因为优图自身的原因，此处要特殊处理\n        \n            # with open( \"json.txt\" ,'wb') as fb:\n            #     fb.write( img_ko_json )  \n    \n            try:\n                points_len = len(img_ko['items'][0]['itemstring'])\n                angle = img_ko['angle']\n            except Exception:\n                points_len = 1\n                angle = -20\n                continue\n\n        # print(img_ko_json.decode('utf8')) ## stdout用的是utf8，需转码才能正常显示\n        # print('-'*50)\n        \n        input_text = create_point( img_ko ,k3_confidence )\n        if(type(input_text) == type([])):\n            continue\n        \n        data = {\n            \"input_text\":input_text   \n            }\n\n        # 提交过快会被拒绝，{\"code\":120005,\"name\":\"ERR_VERIFY_CAPTCHA_TOO_QUICK\"} ，假装思考5秒钟\n        time.sleep( 4 + random.randint(1,9)/10 )\n        try:    \n            resp5 = sessiona.post('https://www.zhihu.com/api/v3/oauth/captcha?lang=cn',data,headers=headers)\n        except Exception:\n            continue\n        \n        print(\"angle: \"+ str(angle) )\n        print(BeautifulSoup(resp5.content ,'html.parser')) # 如果验证成功，会回应{\"success\":true}，开心\n        print('-'*50)\n        try:\n            is_success = json.loads(resp5.text)[\"success\"]\n        except KeyError:\n            continue\n\n    end = time.time()\n\n    return end-start\n\n\nif __name__ == \"__main__\":\n    \n    sessiona = requests.Session()\n    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0','authorization':'oauth c3cef7c66a1843f8b3a9e6a1e3160e20'}\n\n    k3_confidence = 0.71\n    \n    '''\n    # 可视化数据会被保存在云端供浏览\n    # https://plot.ly/~weldon2010/4\n    # 纯属学习，并未看出\"角度\"范围扩大对图像识别的影响，大部分时候60s内能搞定，说明优图还是很强悍的，识别速度也非常快\n    '''\n    runtime_list_x = []\n    runtime_list_y = []\n    nn = range(1,11) # 愿意的话搞多线程，1百万次更有意思\n    \n    # 成功尝试100次，形成2维数据以热力图的方式展示\n    for y in nn :\n        for x in  nn :\n            runtime_list_x.append( bolting(-3,3,k3_confidence) )\n            print( \"y: \" + str(runtime_list_y) )\n            print( \"x: \" + str(runtime_list_x) )\n        runtime_list_y.append(runtime_list_x.copy())\n        runtime_list_x = []\n\n    print (\"-\"*30)    \n    print( runtime_list_y )\n    print (\"-\"*30)\n\n    # pip install plotly 数据可视化\n    import plotly\n    import plotly.graph_objs as go\n    plotly.tools.set_credentials_file(username='username', api_key='username') # 设置账号，去官网注册\n    trace = go.Heatmap(z = runtime_list_y , x = [n for n in nn ] ,y =[n for n in nn ])\n    data=[trace]\n    plotly.plotly.plot(data, filename='weldon-time2-heatmap')    \n   \n    # 尝试后发现一个特点，基本都是1~2个倒置中文，这样我们可以借此提速\n    # 角度范围放大，仅当识别出倒置中文为1~2个时才提交验证否则放弃继续寻找\n\n### chcp 65001 (win下改变cmd字符集)\n### python  c:\\python34\\image_recognition_zhihu.py\n\n\n\n\n\n\n"
        },
        {
          "name": "lagouSpider.py",
          "type": "blob",
          "size": 0.99609375,
          "content": "import requests\nfrom openpyxl import Workbook\n\ndef get_json(url, page, lang_name):\n    data = {'first': 'true', 'pn': page, 'kd': lang_name}\n    json = requests.post(url, data).json()\n    list_con = json['content']['positionResult']['result']\n    info_list = []\n    for i in list_con:\n        info = []\n        info.append(i['companyShortName'])\n        info.append(i['companyName'])\n        info.append(i['salary'])\n        info.append(i['city'])\n        info.append(i['education'])\n        info_list.append(info)\n    return info_list\n\n\ndef main():\n    lang_name = input('职位名：')\n    page = 1\n    url = 'http://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false'\n    info_result = []\n    while page < 31:\n        info = get_json(url, page, lang_name)\n        info_result = info_result + info\n        page += 1\n    wb = Workbook()\n    ws1 = wb.active\n    ws1.title = lang_name\n    for row in info_result:\n        ws1.append(row)\n    wb.save('职位信息.xlsx')\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "login_zhihu.py",
          "type": "blob",
          "size": 2.6806640625,
          "content": "# -*- coding:UTF-8 -*-\n\nimport  requests , time\nimport  hmac ,json\nfrom bs4 import BeautifulSoup\nfrom hashlib import sha1\n\n\ndef get_captcha(data,need_cap):\n    ''' 处理验证码 '''\n    if need_cap is False:\n        return\n    with open('captcha.gif','wb') as fb:\n        fb.write(data)\n    return input('captcha:')\n    \ndef get_signature(grantType,clientId,source,timestamp):\n    ''' 处理签名 '''\n\t\n    hm = hmac.new(b'd1b964811afb40118a12068ff74a12f4',None,sha1)\n    hm.update(str.encode(grantType))\n    hm.update(str.encode(clientId))\n    hm.update(str.encode(source))\n    hm.update(str.encode(timestamp))\n\n    return  str(hm.hexdigest())\n\n\n\ndef login(username,password,oncaptcha,sessiona,headers):\n    ''' 处理登录 '''\n    \n    resp1 = sessiona.get('https://www.zhihu.com/signin',headers=headers)  # 拿cookie:_xsrf\n    resp2 = sessiona.get('https://www.zhihu.com/api/v3/oauth/captcha?lang=cn',headers=headers)  # 拿cookie:capsion_ticket \n    need_cap = json.loads(resp2.text)[\"show_captcha\"]  # {\"show_captcha\":false} 表示不用验证码\n\n    grantType = 'password'\n    clientId = 'c3cef7c66a1843f8b3a9e6a1e3160e20'\n    source ='com.zhihu.web'\n    timestamp = str((time.time()*1000)).split('.')[0]  # 签名只按这个时间戳变化\n       \n    captcha_content = sessiona.get('https://www.zhihu.com/captcha.gif?r=%d&type=login'%(time.time()*1000),headers=headers).content\n    \n    data = {\n        \"client_id\":clientId,\n        \"grant_type\":grantType,\n        \"timestamp\":timestamp,\n        \"source\":source,\n        \"signature\": get_signature(grantType,clientId,source,timestamp), # 获取签名\n        \"username\":username,\n        \"password\":password,\n        \"lang\":\"cn\",\n        \"captcha\":oncaptcha(captcha_content,need_cap), # 获取图片验证码\n        \"ref_source\":\"other_\",\n        \"utm_source\":\"\"\n    }\n    \n    print(\"**2**: \"+str(data))\n    print(\"-\"*50)\n    resp = sessiona.post('https://www.zhihu.com/api/v3/oauth/sign_in',data,headers=headers).content\n    print(BeautifulSoup(resp,'html.parser'))\n    \n    print(\"-\"*50)\n    return resp \n\nif __name__ == \"__main__\":\n    sessiona = requests.Session()\n    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0','authorization':'oauth c3cef7c66a1843f8b3a9e6a1e3160e20'}\n\n    login('12345678@qq.com','12345678',get_captcha,sessiona,headers) # 用户名密码换自己的就好了\n    resp = sessiona.get('https://www.zhihu.com/inbox',headers=headers)  # 登录进去了，可以看私信了\n    print(BeautifulSoup(resp.content ,'html.parser'))\n    \n    \n    \n    \n### chcp 65001 (win下改变cmd字符集)\n### python  c:\\python34\\login_zhihu.py\n### 有非常无语的事情发生，还以为代码没生效\n\n\n\n\n\n"
        },
        {
          "name": "qiubai_crawer.py",
          "type": "blob",
          "size": 1.9853515625,
          "content": "import requests\nfrom bs4 import BeautifulSoup\n\n\ndef download_page(url):\n    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0\"}\n    r = requests.get(url, headers=headers)\n    return r.text\n\n\ndef get_content(html, page):\n    output = \"\"\"第{}页 作者：{} 性别：{} 年龄：{} 点赞：{} 评论：{}\\n{}\\n------------\\n\"\"\"\n    soup = BeautifulSoup(html, 'html.parser')\n    con = soup.find(id='content-left')\n    con_list = con.find_all('div', class_=\"article\")\n    for i in con_list:\n        author = i.find('h2').string  # 获取作者名字\n        content = i.find('div', class_='content').find('span').get_text()  # 获取内容\n        stats = i.find('div', class_='stats')\n        vote = stats.find('span', class_='stats-vote').find('i', class_='number').string\n        comment = stats.find('span', class_='stats-comments').find('i', class_='number').string\n        author_info = i.find('div', class_='articleGender')  # 获取作者 年龄，性别\n        if author_info is not None:  # 非匿名用户\n            class_list = author_info['class']\n            if \"womenIcon\" in class_list:\n                gender = '女'\n            elif \"manIcon\" in class_list:\n                gender = '男'\n            else:\n                gender = ''\n            age = author_info.string   # 获取年龄\n        else:  # 匿名用户\n            gender = ''\n            age = ''\n\n        save_txt(output.format(page, author, gender, age, vote, comment, content))\n\n\ndef save_txt(*args):\n    for i in args:\n        with open('qiubai.txt', 'a', encoding='utf-8') as f:\n            f.write(i)\n\n\ndef main():\n    # 我们点击下面链接，在页面下方可以看到共有13页，可以构造如下 url，\n    # 当然我们最好是用 Beautiful Soup找到页面底部有多少页。\n    for i in range(1, 14):\n        url = 'https://qiushibaike.com/text/page/{}'.format(i)\n        html = download_page(url)\n        get_content(html, i)\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "qrcode.jpg",
          "type": "blob",
          "size": 26.904296875,
          "content": null
        },
        {
          "name": "readExcel.py",
          "type": "blob",
          "size": 0.595703125,
          "content": "from openpyxl import Workbook\nfrom openpyxl.compat import range\nfrom openpyxl.cell import get_column_letter\nwb = Workbook()\ndest_filename = 'empty_book2.xlsx'\nws1 = wb.active  # 第一个表\nws1.title = \"range names\"  # 第一个表命名\n# 遍历第一个表的1到40行，赋值一个600内的随机数\nfor row in range(1, 40):\n    ws1.append(range(60))\nws2 = wb.create_sheet(title=\"Pi\")\nws2['F5'] = 3.14\nws3 = wb.create_sheet(title=\"Data\")\nfor row in range(10, 20):\n    for col in range(27, 54):\n        _ = ws3.cell(column=col, row=row, value=\"%s\" % get_column_letter(col))\nwb.save(filename=dest_filename)\n"
        },
        {
          "name": "wechat",
          "type": "tree",
          "content": null
        },
        {
          "name": "爬虫集合",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}