{
  "metadata": {
    "timestamp": 1736560928075,
    "page": 661,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "baichuan-inc/Baichuan-7B",
      "stars": 5680,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 7.130859375,
          "content": "##### Project Specification #####\n**/test\n**/ceval_output\n\n##### Python.gitignore #####\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nwheelhouse/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n*.whl\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\ndocs/build/\ndocs/source/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# ruff\n.ruff_cache/\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n.idea/\n\n\n##### macOS.gitignore #####\n# General\n.DS_Store\n.AppleDouble\n.LSOverride\n\n# Icon must end with two \\r\nIcon\n\n# Thumbnails\n._*\n\n# Files that might appear in the root of a volume\n.DocumentRevisions-V100\n.fseventsd\n.Spotlight-V100\n.TemporaryItems\n.Trashes\n.VolumeIcon.icns\n.com.apple.timemachine.donotpresent\n\n# Directories potentially created on remote AFP share\n.AppleDB\n.AppleDesktop\nNetwork Trash Folder\nTemporary Items\n.apdisk\n\n\n##### Linux.gitignore #####\n*~\n\n# Temporary files which can be created if a process still has a handle open of a deleted file\n.fuse_hidden*\n\n# KDE directory preferences\n.directory\n\n# Linux trash folder which might appear on any partition or disk\n.Trash-*\n\n# .nfs files are created when an open file is removed but is still being accessed\n.nfs*\n\n\n##### Windows.gitignore #####\n# Windows thumbnail cache files\nThumbs.db\nThumbs.db:encryptable\nehthumbs.db\nehthumbs_vista.db\n\n# Dump file\n*.stackdump\n\n# Folder config file\n[Dd]esktop.ini\n\n# Recycle Bin used on file shares\n$RECYCLE.BIN/\n\n# Windows Installer files\n*.cab\n*.msi\n*.msix\n*.msm\n*.msp\n\n# Windows shortcuts\n*.lnk\n\n\n##### Archives.gitignore #####\n# It's better to unpack these files and commit the raw source because\n# git has its own built in compression methods.\n*.7z\n*.jar\n*.rar\n*.zip\n*.gz\n*.gzip\n*.tgz\n*.bzip\n*.bzip2\n*.bz2\n*.xz\n*.lzma\n*.cab\n*.xar\n\n# Packing-only formats\n*.iso\n*.tar\n\n# Package management formats\n*.dmg\n*.xpi\n*.gem\n*.egg\n*.deb\n*.rpm\n*.msi\n*.msm\n*.msp\n*.txz\n\n\n##### Xcode.gitignore #####\n# Xcode\n#\n# gitignore contributors: remember to update Global/Xcode.gitignore, Objective-C.gitignore & Swift.gitignore\n\n## User settings\nxcuserdata/\n\n## Compatibility with Xcode 8 and earlier (ignoring not required starting Xcode 9)\n*.xcscmblueprint\n*.xccheckout\n\n## Compatibility with Xcode 3 and earlier (ignoring not required starting Xcode 4)\nbuild/\nDerivedData/\n*.moved-aside\n*.pbxuser\n!default.pbxuser\n*.mode1v3\n!default.mode1v3\n*.mode2v3\n!default.mode2v3\n*.perspectivev3\n!default.perspectivev3\n\n## Gcc Patch\n/*.gcno\n\n\n##### JetBrains.gitignore #####\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio and WebStorm\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User settings\n.idea/*\n\n# User-specific stuff\n.idea/**/workspace.xml\n.idea/**/tasks.xml\n.idea/**/usage.statistics.xml\n.idea/**/dictionaries\n.idea/**/shelf\n\n# Generated files\n.idea/**/contentModel.xml\n\n# Sensitive or high-churn files\n.idea/**/dataSources/\n.idea/**/dataSources.ids\n.idea/**/dataSources.local.xml\n.idea/**/sqlDataSources.xml\n.idea/**/dynamic.xml\n.idea/**/uiDesigner.xml\n.idea/**/dbnavigator.xml\n\n# Gradle\n.idea/**/gradle.xml\n.idea/**/libraries\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn. Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\ncmake-build-*/\n\n# Mongo Explorer plugin\n.idea/**/mongoSettings.xml\n\n# File-based project format\n*.iws\n\n# IntelliJ\nout/\n\n# mpeltonen/sbt-idea plugin\n.idea_modules/\n\n# JIRA plugin\natlassian-ide-plugin.xml\n\n# Cursive Clojure plugin\n.idea/replstate.xml\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n\n# Editor-based Rest Client\n.idea/httpRequests\n\n# Android studio 3.1+ serialized cache file\n.idea/caches/build_file_checksums.ser\n\n\n##### VisualStudioCode.gitignore #####\n.vscode/*\n# !.vscode/settings.json\n# !.vscode/tasks.json\n# !.vscode/launch.json\n!.vscode/extensions.json\n*.code-workspace\n\n# Local History for Visual Studio Code\n.history/\n\n\n##### Vim.gitignore #####\n# Swap\n.*.s[a-v][a-z]\n!*.svg  # comment out if you don't need vector files\n.*.sw[a-p]\n.s[a-rt-v][a-z]\n.ss[a-gi-z]\n.sw[a-p]\n\n# Session\nSession.vim\nSessionx.vim\n\n# Temporary\n.netrwhist\n*~\n# Auto-generated tag files\ntags\n# Persistent undo\n[._]*.un~"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.080078125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [2023] [baichuan-inc]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 14.501953125,
          "content": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n\n<div align=\"center\">\n<h1>\n  Baichuan-7B\n</h1>\n</div>\n\n<p align=\"center\">\n🤗 <a href=\"https://huggingface.co/baichuan-inc/Baichuan-7B\" target=\"_blank\">Hugging Face</a> • 🤖 <a href=\"https://modelscope.cn/organization/baichuan-inc\" target=\"_blank\">ModelScope</a> • 💬 <a href=\"https://github.com/baichuan-inc/Baichuan-7B/blob/main/media/wechat.jpeg?raw=true\" target=\"_blank\">WeChat</a>\n</p>\n\n<div align=\"center\">\n\n[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/baichuan-inc/Baichuan-7B/blob/main/LICENSE)\n<h4 align=\"center\">\n    <p>\n        <b>中文</b> |\n        <a href=\"https://github.com/baichuan-inc/Baichuan-7B/blob/main/README_EN.md\">English</a>\n    <p>\n</h4>\n</div>\n\n# 更新信息\n- [2023.09.06] 我们发布了新一代开源模型 [Baichuan 2](https://github.com/baichuan-inc/Baichuan2)，包含 7B、13B 尺寸 🔥🔥🔥\n\n# 介绍\n\nBaichuan-7B 是由百川智能开发的一个开源可商用的大规模预训练语言模型。基于 Transformer 结构，在大约 1.2 万亿 tokens 上训练的 70 亿参数模型，支持中英双语，上下文窗口长度为 4096。在标准的中文和英文 benchmark（C-Eval/MMLU）上均取得同尺寸最好的效果。\n\n# 公开benchmark榜单\n\n## 中文评测\n\n### C-Eval\n\n[C-Eval 数据集](https://cevalbenchmark.com/index.html)是一个全面的中文基础模型评测数据集，涵盖了 52 个学科和四个难度的级别。我们使用该数据集的 dev 集作为 few-shot 的来源，在 test 集上进行了 `5-shot` 测试。通过执行执行下面的命令：\n\n```bash\ncd evaluation\npython evaluate_zh.py --model_name_or_path 'your/model/path'\n```\n\n### 结果\n\n|        Model 5-shot         | Average | Avg(Hard) | STEM  | Social Sciences | Humanities | Others |\n| :-------------------------: | :-----: | :-------: | :---: | :-------------: | :--------: | :----: |\n|            GPT-4            |  68.7   |   54.9    | 67.1  |      77.6       |    64.5    |  67.8  |\n|           ChatGPT           |  54.4   |   41.4    | 52.9  |      61.8       |    50.9    |  53.6  |\n|         Claude-v1.3         |  54.2   |   39.0    | 51.9  |      61.7       |    52.1    |  53.7  |\n|     Claude-instant-v1.0     |  45.9   |   35.5    | 43.1  |      53.8       |    44.2    |  45.4  |\n|          BLOOMZ-7B          |  35.7   |   25.8    | 31.3  |      43.5       |    36.6    |  35.6  |\n|         ChatGLM-6B          |  34.5   |   23.1    | 30.4  |      39.6       |    37.4    |  34.5  |\n|   Ziya-LLaMA-13B-pretrain   |  30.2   |   22.7    | 27.7  |      34.4       |    32.0    |  28.9  |\n|  moss-moon-003-base (16B)   |  27.4   |   24.5    | 27.0  |      29.1       |    27.2    |  26.9  |\n|         LLaMA-7B-hf         |  27.1   |   25.9    | 27.1  |      26.8       |    27.9    |  26.3  |\n|          Falcon-7B          |  25.8   |   24.3    | 25.8  |      26.0       |    25.8    |  25.6  |\n|      TigerBot-7B-base       |  25.7   |   27.0    | 27.3  |      24.7       |    23.4    |  26.1  |\n|    Aquila-7B<sup>*</sup>    |  25.5   |   25.2    | 25.6  |      24.6       |    25.2    |  26.6  |\n| Open-LLaMA-v2-pretrain (7B) |  24.0   |   22.5    | 23.1  |      25.3       |    25.2    |  23.2  |\n|          BLOOM-7B           |  22.8   |   20.2    | 21.8  |      23.3       |    23.9    |  23.3  |\n|       **Baichuan-7B**       |  42.8   |   31.5    | 38.2  |      52.0       |    46.2    |  39.3  |\n\n### Gaokao\n\n[Gaokao](https://github.com/OpenLMLab/GAOKAO-Bench) 是一个以中国高考题作为评测大语言模型能力的数据集，用以评估模型的语言能力和逻辑推理能力。\n我们只保留了其中的单项选择题，随机划分后对所有模型进行统一 `5-shot` 测试。\n\n### 结果\n\n以下是测试的结果。\n\n|          Model          |  Average  |\n| :---------------------: | :-------: |\n|        BLOOMZ-7B        |   28.72   |\n|        LLaMA-7B         |   27.81   |\n|        BLOOM-7B         |   26.96   |\n|    TigerBot-7B-base     |   25.94   |\n|        Falcon-7B        |   23.98   |\n| Ziya-LLaMA-13B-pretrain |   23.17   |\n|       ChatGLM-6B        |   21.41   |\n| Open-LLaMA-v2-pretrain  |   21.41   |\n|  Aquila-7B<sup>*</sup>  |   24.39   |\n|     **Baichuan-7B**     | **36.24** |\n\n### AGIEval\n\n[AGIEval](https://github.com/microsoft/AGIEval) 旨在评估模型的认知和解决问题相关的任务中的一般能力。\n我们只保留了其中的四选一单项选择题，随机划分后对所有模型进行了统一 `5-shot` 测试。\n\n### 结果\n\n|          Model          |  Average  |\n| :---------------------: | :-------: |\n|        BLOOMZ-7B        |   30.27   |\n|        LLaMA-7B         |   28.17   |\n| Ziya-LLaMA-13B-pretrain |   27.64   |\n|        Falcon-7B        |   27.18   |\n|        BLOOM-7B         |   26.55   |\n|  Aquila-7B<sup>*</sup>  |   25.58   |\n|    TigerBot-7B-base     |   25.19   |\n|       ChatGLM-6B        |   23.49   |\n| Open-LLaMA-v2-pretrain  |   23.49   |\n|     **Baichuan-7B**     | **34.44** |\n\n<sup>*</sup>其中 Aquila 模型来源于智源官方网站(<https://model.baai.ac.cn/model-detail/100098>) 仅做参考\n\n## 英文榜单\n\n除了中文之外，Baichuan-7B也测试了模型在英文上的效果，[MMLU](https://arxiv.org/abs/2009.03300) 是包含 57 个多选任务的英文评测数据集，涵盖了初等数学、美国历史、计算机科学、法律等，难度覆盖高中水平到专家水平，是目前主流的LLM评测数据集。我们采用了[开源](https://github.com/hendrycks/test) 的评测方案，最终 `5-shot` 结果如下所示：\n\n### 结果\n\n|                Model                 | Humanities | Social Sciences |   STEM   |  Other   | Average  |\n| :----------------------------------: | :--------: | :-------------: | :------: | :------: | :------: |\n|        ChatGLM-6B<sup>0</sup>        |    35.4    |      41.0       |   31.3   |   40.5   |   36.9   |\n|        BLOOMZ-7B<sup>0</sup>         |    31.3    |      42.1       |   34.4   |   39.0   |   36.1   |\n|          mpt-7B<sup>1</sup>          |     -      |        -        |    -     |    -     |   35.6   |\n|         LLaMA-7B<sup>2</sup>         |    34.0    |      38.3       |   30.5   |   38.1   |   35.1   |\n|        Falcon-7B<sup>1</sup>         |     -      |        -        |    -     |    -     |   35.0   |\n| moss-moon-003-sft (16B)<sup>0</sup>  |    30.5    |      33.8       |   29.3   |   34.4   |   31.9   |\n|         BLOOM-7B<sup>0</sup>         |    25.0    |      24.4       |   26.5   |   26.4   |   25.5   |\n| moss-moon-003-base (16B)<sup>0</sup> |    24.2    |      22.8       |   22.4   |   24.4   |   23.6   |\n|     **Baichuan-7B<sup>0</sup>**      |  **38.4**  |    **48.9**     | **35.6** | **48.1** | **42.3** |\n\n<sup>0: 重新复现</sup><br/>\n<sup>1: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</sup><br/>\n<sup>2: https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu</sup><br/>\n\n### 复现方法\n\n```bash\ngit clone https://github.com/hendrycks/test\ncd test\nwget https://people.eecs.berkeley.edu/~hendrycks/data.tar\ntar xf data.tar\nmkdir results\ncp ../evaluate_mmlu.py .\npython evaluate_mmlu.py -m /path/to/Baichuan-7B\n```\n\n其中在 MMLU 上57个任务的具体细指标如下图：\n<p align=\"center\">\n    <img src=\"media/MMLU-57-tasks.png\" width=\"90%\"/>\n</p>\n\n其中各个学科的指标如下图：\n<p align=\"center\">\n    <img src=\"media/MMLU 21 Subjects.png\" width=\"90%\"/>\n</p>\n\n# 推理方法\n\n推理代码已经在[官方 Huggingface 库](https://huggingface.co/baichuan-inc/Baichuan-7B)\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-7B\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-7B\", device_map=\"auto\", trust_remote_code=True)\ninputs = tokenizer('登鹳雀楼->王之涣\\n夜雨寄北->', return_tensors='pt')\ninputs = inputs.to('cuda:0')\npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\n```\n\n# 数据\n\n* 原始数据包括开源的中英文数据和自行抓取的中文互联网数据，以及部分高质量知识性数据。\n* 参考相关数据工作，频率和质量是数据处理环节重点考虑的两个维度。 我们基于启发式规则和质量模型打分，对原始数据集进行篇章和句子粒度的过滤。在全量数据上，利用局部敏感哈希方法，对篇章和句子粒度做滤重。\n\n整体流程如下所示：\n<p align=\"center\">\n    <br>\n    <img src=\"media/data_process.png\" width=\"90%\"/>\n    <br>\n</p>\n\n* 经过不断的调整和多轮测试，最终确认了一个在下游任务上表现最好的中英文配比。\n* 我们使用了一个基于自动学习的数据权重策略，对不同类别的数据进行配比。\n\n# 分词\n\n我们参考学术界方案使用 SentencePiece 中的 Byte-Pair Encoding (BPE) 作为分词算法，并且进行了以下的优化：\n\n1. 目前大部分开源模型主要基于英文优化，因此对中文语料存在效率较低的问题。我们使用 2000 万条以中英为主的多语言语料训练分词模型，显著提升对于中文的压缩率。\n2. 对于数学领域，我们参考了 LLaMA 和 Galactica 中的方案，对数字的每一位单独分开，避免出现数字不一致的问题，对于提升数学能力有重要帮助。\n3. 对于罕见字词（如特殊符号等），支持 UTF-8 characters 的 byte 编码，因此做到未知字词的全覆盖。\n4. 我们分析了不同分词器对语料的压缩率，如下表，可见我们的分词器明显优于 LLaMA, Falcon 等开源模型，并且对比其他中文分词器在压缩率相当的情况下，训练和推理效率更高。\n\n|     Model     | Baichuan-7B | LLaMA  | Falcon | mpt-7B | ChatGLM | moss-moon-003 |\n| :-----------: | :---------: | :----: | :----: | :----: | :-----: | :-----------: |\n| Compress Rate |    0.737    | 1.312  | 1.049  | 1.206  |  0.631  |     0.659     |\n|  Vocab Size   |   64,000    | 32,000 | 65,024 | 50,254 | 130,344 |    106,029    |\n\n# 模型结构\n\n整体模型基于标准的 Transformer 结构，我们采用了和 LLaMA 一样的模型设计\n\n* 位置编码：[rotary-embedding](https://arxiv.org/abs/2104.09864) 是现阶段被大多模型采用的位置编码方案，具有更好的外延效果。虽然训练过程中最大长度为4096，但是实际测试中模型可以很好的扩展到 5000 tokens 以上，如下图：\n\n<p align=\"center\">\n    <img src=\"media/long-context-ppl.png\" width=\"90%\"/>\n</p>\n\n* 激活层：SwiGLU, Feedforward 变化为 8/3 倍的隐含层大小，即 11,008\n* Layer-Normalization: 基于 [RMSNorm](https://arxiv.org/abs/1910.07467) 的 Pre-Normalization\n\n# 训练稳定性和吞吐\n\n我们在原本的 LLaMA 框架上进行诸多修改以提升训练时的吞吐，具体包括：\n\n1. 算子优化技术：采用更高效算子，如 Flash-Attention，NVIDIA apex 的 RMSNorm 等。\n2. 算子切分技术：将部分计算算子进行切分，减小内存峰值。\n3. 混合精度技术：降低在不损失模型精度的情况下加速计算过程。\n4. 训练容灾技术：训练平台和训练框架联合优化，IaaS + PaaS 实现分钟级的故障定位和任务恢复。\n5. 通信优化技术，具体包括：\n   1. 采用拓扑感知的集合通信算法，避免网络拥塞问题，提高通信效率。\n   2. 根据卡数自适应设置 bucket size，提高带宽利用率。\n   3. 根据模型和集群环境，调优通信原语的触发时机，从而将计算和通信重叠。\n\n基于上述的几个优化技术，我们在千卡 A800 显卡上达到了 7B 模型 182 TFLOPS 的吞吐，GPU 峰值算力利用率高达 58.3%。\n\n最终的loss如下图：\n<p align=\"center\">\n    <img src=\"media/7b.loss.png\" width=\"90%\"/>\n</p>\n\n# 训练方法\n\n## 安装依赖\n\n```bash\npip install -r requirements.txt\n```\n\n## 准备数据\n\n用户将训练语料按总rank数的倍数均匀切分成多个 UTF-8 文本文件，放置在语料目录（默认为 `data_dir` ）下。各个rank进程将会读取语料目录下的不同文件，全部加载到内存后，开始后续训练过程。以上是简化的示范流程，建议用户在正式训练任务中，根据需求调整数据生产逻辑。\n\n## 下载 tokenizer 模型\n\n下载 tokenizer 模型文件 [tokenizer.model](https://huggingface.co/baichuan-inc/Baichuan-7B/blob/main/tokenizer.model) ，放置在项目目录下。\n\n## 配置 DeepSpeed\n\n本示范代码采用 DeepSpeed 框架进行训练。用户需根据集群情况，修改 `config/hostfile` ，如果是多机多卡，需要修改 ssh 中各个节点的 IP 配置。具体可以参见 DeepSpeed [官方说明](https://www.deepspeed.ai/) 。\n\n## 执行训练\n\n```python\nscripts/train.sh\n```\n\n# 协议\n\n对本仓库源码的使用遵循开源许可协议 [Apache 2.0](https://github.com/baichuan-inc/Baichuan-7B/blob/main/LICENSE)。\n\nBaichuan-7B 支持商用。如果将 Baichuan-7B 模型或其衍生品用作商业用途，请您按照如下方式联系许可方，以进行登记并向许可方申请书面授权：联系邮箱：<opensource@baichuan-inc.com>， 具体许可协议可见[《Baichuan-7B 模型许可协议》](https://huggingface.co/baichuan-inc/Baichuan-7B/resolve/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf)。\n\n# Third-Party Resources\n\n1. [LLaMA Efficient Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) 支持Baichuan-7B使用Qlora进行Finetune，支持RLHF，支持WebDemo。使用经过sft的模型见 [hiyouga/baichuan-7b-sft](https://huggingface.co/hiyouga/baichuan-7b-sft)。\n2. [fireballoon/baichuan-vicuna-chinese-7b](https://huggingface.co/fireballoon/baichuan-vicuna-chinese-7b) 使用 ShareGPT, ShareGPT-ZH, COT & COT-ZH, Leetcode, dummy等包含中英文的数据Finetune后的模型，训练代码参考FastChat。\n3. [fireballoon/baichuan-vicuna-7b](https://huggingface.co/fireballoon/baichuan-vicuna-7b) 使用ShareGPT, COT 和 Leetcode等数据混合Finetune后的模型，训练代码参考FastChat。\n4. [Efficient-Tuning-LLMs](https://github.com/jianzhnie/Efficient-Tuning-LLMs) 支持Baichuan-7B使用Qlora进行Finetune和4bit inference。\n5. [fastllm](https://github.com/ztxz16/fastllm) fastllm是纯c++实现，无第三方依赖的大模型库，支持Baichuan-7B在手机端运行。\n6. [TheBloke/baichuan-7B-GPTQ](https://huggingface.co/TheBloke/baichuan-7B-GPTQ) 对Baichuan-7B的GPTQ 4bit量化。\n\n# Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=baichuan-inc/Baichuan-7B&type=Date)](https://star-history.com/#baichuan-inc/Baichuan-7B&Date)\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 15.4560546875,
          "content": "<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n\n<div align=\"center\">\n<h1>\n  Baichuan-7B\n</h1>\n</div>\n\n<p align=\"center\">\n🤗 <a href=\"https://huggingface.co/baichuan-inc/Baichuan-7B\" target=\"_blank\">Hugging Face</a> • 🤖 <a href=\"https://modelscope.cn/organization/baichuan-inc\" target=\"_blank\">ModelScope</a> • 💬 <a href=\"https://github.com/baichuan-inc/Baichuan-7B/blob/main/media/wechat.jpeg?raw=true\" target=\"_blank\">WeChat</a>\n</p>\n\n[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/baichuan-inc/Baichuan-7B/blob/main/LICENSE)\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/baichuan-inc/Baichuan-7B/blob/main/README.md\">中文</a>\n    <p>\n</h4>\n</div>\n\n# Introduction\n\nBaichuan-7B is an open-source, large-scale pre-trained language model developed by Baichuan Intelligent Technology. Baichuan-7B is based on Transformer architecture, which contains 7 billion parameters and trained on approximately 1.2 trillion tokens. It supports both Chinese and English languages with a context window length of 4096. It has achieved the best performance among models of the same size on standard Chinese and English benchmarks (C-Eval, MMLU, etc).\n\n# Benchmark\n\n## Chinese Benchmarks\n\n### C-Eval\n\n[C-Eval](https://cevalbenchmark.com/index.html) is a comprehensive Chinese language models evaluation dataset, covering 52 subjects and four levels of difficulty. We used the dev set from this dataset as the source for few-shot learning and conducted a 5-shot test on the test set.\n\nChange OPENMODEL_PATH and CEVAL_DATA_PATH in evaluate_zh.py, corresponding to model's and C-Eval dataset's directories, then run:\n\n```bash\ncd evaluation\npython evaluate_zh.py --model_name_or_path 'your/model/path'\n```\n\n#### Results\n\n|        Model 5-shot         | Average | Avg(Hard) | STEM  | Social Sciences | Humanities | Others |\n| :-------------------------: | :-----: | :-------: | :---: | :-------------: | :--------: | :----: |\n|            GPT-4            |  68.7   |   54.9    | 67.1  |      77.6       |    64.5    |  67.8  |\n|           ChatGPT           |  54.4   |   41.4    | 52.9  |      61.8       |    50.9    |  53.6  |\n|         Claude-v1.3         |  54.2   |   39.0    | 51.9  |      61.7       |    52.1    |  53.7  |\n|     Claude-instant-v1.0     |  45.9   |   35.5    | 43.1  |      53.8       |    44.2    |  45.4  |\n|          BLOOMZ-7B          |  35.7   |   25.8    | 31.3  |      43.5       |    36.6    |  35.6  |\n|         ChatGLM-6B          |  34.5   |   23.1    | 30.4  |      39.6       |    37.4    |  34.5  |\n|   Ziya-LLaMA-13B-pretrain   |  30.2   |   22.7    | 27.7  |      34.4       |    32.0    |  28.9  |\n|  moss-moon-003-base (16B)   |  27.4   |   24.5    | 27.0  |      29.1       |    27.2    |  26.9  |\n|         LLaMA-7B-hf         |  27.1   |   25.9    | 27.1  |      26.8       |    27.9    |  26.3  |\n|          Falcon-7B          |  25.8   |   24.3    | 25.8  |      26.0       |    25.8    |  25.6  |\n|      TigerBot-7B-base       |  25.7   |   27.0    | 27.3  |      24.7       |    23.4    |  26.1  |\n|    Aquila-7B<sup>*</sup>    |  25.5   |   25.2    | 25.6  |      24.6       |    25.2    |  26.6  |\n| Open-LLaMA-v2-pretrain (7B) |  24.0   |   22.5    | 23.1  |      25.3       |    25.2    |  23.2  |\n|          BLOOM-7B           |  22.8   |   20.2    | 21.8  |      23.3       |    23.9    |  23.3  |\n|       **Baichuan-7B**       |  42.8   |   31.5    | 38.2  |      52.0       |    46.2    |  39.3  |\n\n### Gaokao\n\n[Gaokao](https://github.com/OpenLMLab/GAOKAO-Bench) is an evaluation dataset curated from questions used in Chinese College Entrance Examination, to evaluate the capabilities of large language models, assessing models' language ability and logical reasoning skills. We processed the dataset to only containing single-answer multiple choice questions, we conducted a 5-shot test on all models.\n\n#### Results\n\n|          Model          |  Average  |\n| :---------------------: | :-------: |\n|        BLOOMZ-7B        |   28.72   |\n|        LLaMA-7B         |   27.81   |\n|        BLOOM-7B         |   26.96   |\n|    TigerBot-7B-base     |   25.94   |\n|        Falcon-7B        |   23.98   |\n| Ziya-LLaMA-13B-pretrain |   23.17   |\n|       ChatGLM-6B        |   21.41   |\n| Open-LLaMA-v2-pretrain  |   21.41   |\n|  Aquila-7B<sup>*</sup>  |   24.39   |\n|     **Baichuan-7B**     | **36.24** |\n\n### AGIEval\n\n[AGIEval](https://github.com/microsoft/AGIEval) is a dataset aimed at evaluating models' general abilities in cognitive and problem-solving tasks.\nwe conducted a 5-shot test on all models.\n\n#### Results\n\n|          Model          |  Average  |\n| :---------------------: | :-------: |\n|        BLOOMZ-7B        |   30.27   |\n|        LLaMA-7B         |   28.17   |\n| Ziya-LLaMA-13B-pretrain |   27.64   |\n|        Falcon-7B        |   27.18   |\n|        BLOOM-7B         |   26.55   |\n|  Aquila-7B<sup>*</sup>  |   25.58   |\n|    TigerBot-7B-base     |   25.19   |\n|       ChatGLM-6B        |   23.49   |\n| Open-LLaMA-v2-pretrain  |   23.49   |\n|     **Baichuan-7B**     | **34.44** |\n\n<sup>*The Aquila-7b are not implemented on Huggingface yet so we derived the model from (https://model.baai.ac.cn/model-detail/100098), which may have not identical to their official result.</sup><br/>\n\n## English Benchmarks\n\nIn addition to Chinese, we also tested the performance of models in English. [MMLU](https://arxiv.org/abs/2009.03300) is an English evaluation dataset that includes 57 multiple-choice tasks, covering elementary mathematics, American history, computer science, law, etc. The difficulty spans from high school level to expert level, making it a mainstream evaluation dataset for Large Language Models (LLMs).\n\nWe adopt the public implementation of (https://github.com/hendrycks/test) and the final result is shown below：\n\n### Results on MMLU\n\n| Model                                | Humanities | Social Sciences |   STEM   |  Other   | Average  |\n| ------------------------------------ | ---------: | :-------------: | :------: | :------: | :------: |\n| ChatGLM-6B<sup>0</sup>               |       35.4 |      41.0       |   31.3   |   40.5   |   36.9   |\n| BLOOMZ-7B<sup>0</sup>                |       31.3 |      42.1       |   34.4   |   39.0   |   36.1   |\n| mpt-7B<sup>1</sup>                   |          - |        -        |    -     |    -     |   35.6   |\n| LLaMA-7B<sup>2</sup>                 |       34.0 |      38.3       |   30.5   |   38.1   |   35.1   |\n| Falcon-7B<sup>1</sup>                |          - |        -        |    -     |    -     |   35.0   |\n| moss-moon-003-sft (16B)<sup>0</sup>  |       30.5 |      33.8       |   29.3   |   34.4   |   31.9   |\n| BLOOM-7B<sup>0</sup>                 |       25.0 |      24.4       |   26.5   |   26.4   |   25.5   |\n| moss-moon-003-base (16B)<sup>0</sup> |       24.2 |      22.8       |   22.4   |   24.4   |   23.6   |\n| **Baichuan-7B<sup>0</sup>**          |   **38.4** |    **48.9**     | **35.6** | **48.1** | **42.3** |\n\n<sup>0: Our implementation</sup><br/>\n<sup>1: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</sup><br/>\n<sup>2: https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu</sup><br/>\n\n### How to implement by yourself\n\n```bash\ngit clone https://github.com/hendrycks/test\ncd test\nwget https://people.eecs.berkeley.edu/~hendrycks/data.tar\ntar xf data.tar\nmkdir results\ncp ../evaluate_mmlu.py .\npython evaluate_mmlu.py -m /path/to/Baichuan-7B\n```\n\nSpecifically, the result of 57 MMLU tasks is:\n<p align=\"center\">\n    <img src=\"media/MMLU-57-tasks.png\" width=\"90%\"/>\n</p>\n\nAnd the comparison of 21 different subjects is：\n<p align=\"center\">\n    <img src=\"media/MMLU 21 Subjects.png\" width=\"90%\"/>\n</p>\n\n# Inference\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-7B\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/Baichuan-7B\", device_map=\"auto\", trust_remote_code=True)\ninputs = tokenizer('Hamlet->Shakespeare\\nOne Hundred Years of Solitude->', return_tensors='pt')\ninputs = inputs.to('cuda:0')\npred = model.generate(**inputs, max_new_tokens=64)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\n```\n\n# Data\n\n* The original corpora includes open-source Chinese and English data, self-crawled Chinese internet data, and some high-quality knowledge-intensive data.\n* Referring to related data work, frequency and quality are two dimensions that are considered important in the data processing stage. We apply heuristic rules and quality model scoring to filter the original dataset at both the paragraph and sentence levels. Employing the Locality-Sensitive Hashing (LSH) method on the full dataset, we perform de-duplication at both the paragraph and sentence levels.\n\nThe whole data processing process is shown below:\n<p align=\"center\">\n    <br>\n    <img src=\"media/data_process.png\" width=\"90%\"/>\n    <br>\n</p>\n\n* After continuous adjustments and multiple rounds of testing, we finally determined the best Chinese to English ratio that are optimized on downstream tasks.\n* We used an automatic algorithm-based data sampling strategy to balance the weights of different data categories.\n\n# Tokenization\n\nWe use the byte pair encoding (BPE) from SentencePiece as the tokenization algorithm, along with the following optimizations:\n\n1. Most open-source models are primarily optimized for English, resulting in low efficiency for Chinese corpus. So we trained the tokenizer using 20 million multilingual corpora mainly composed of Chinese and English, significantly improving the compression rate for Chinese.\n2. To improve the ability for mathematics, we split all numbers into individual digits that is also adopted in LLaMA and Galactica, separately tokenizing each digit to avoid inconsistencies in numbers.\n3. For rare words (such as emoji and special symbols), we fallback unknown characters to byte encoding of UTF-8, thus achieving full coverage of unknown words.\n4. We analyzed the compression rate of different tokenizers on the corpus. As shown in the following table, our tokenizer significantly outperforms open-source models like LLaMA, Falcon, and others. Compared to other Chinese tokenizers with similar compression rates, it offers higher training and inference efficiency. \n\n|     Model     | Baichuan-7B | LLaMA  | Falcon | mpt-7B | ChatGLM | moss-moon-003 |\n| :-----------: | :---------: | :----: | :----: | :----: | :-----: | :-----------: |\n| Compress Rate |    0.737    | 1.312  | 1.049  | 1.206  |  0.631  |     0.659     |\n|  Vocab Size   |   64,000    | 32,000 | 65,024 | 50,254 | 130,344 |    106,029    |\n\n# Model Architecture\n\nThe overall model is based on the standard Transformer structure, and we have adopted a model design similar to that of LLaMA.\n\n* Positional Embeddings: [rotary-embedding](https://arxiv.org/abs/2104.09864) is the widely used positional encoding method, with better extrapolation effects. Although the maximum length during training is 4096, the model can be well extrapolated to 5000 tokens in inference time, as shown in the following diagram:\n\n<p align=\"center\">\n<img src=\"media/long-context-ppl.png\" width=\"90%\"/>\n</p>\n\n* Activation：SwiGLU, and the dimension of the feedforward-layer is set to 11,008\n* Layer-Normalization: We use the Pre-Normalization method based on [RMSNorm](https://arxiv.org/abs/1910.07467)\n\n## Training stability and Throughput\n\nWe made numerous modifications to the original LLaMA framework to improve throughput during training, including:\n\n1. Operator optimization technology: We adopted more efficient operators, such as Flash-attention, NVIDIA apex's RMSNorm, etc.\n2. Tensor partitioning technology: We partitioned some computational operators to reduce peak memory usage.\n3. Mixed-precision technology: This accelerates the computational process without sacrificing model accuracy.\n4. Training failure recovery technology: The training platform and the training framework were jointly optimized. By combining IaaS and PaaS, we can locate faults and recover tasks within minutes.\n5. Communication optimization technology which includes:\n   1. Topology-aware collective communication algorithms to avoid network congestion and improve communication efficiency.\n   2. Adaptive setting of bucket size based on the number of cards to improve bandwidth utilization.\n   3. Tuning the trigger timing of communication primitives based on the model and the cluster environment, thereby overlapping computation and communication. \n\nBy using these optimization techniques, we achieved a throughput of 182 TFLOPS for the 7B model on thousand A800 GPUs, with a peak GPU computing power utilization rate of up to 58.3%.\n\nThe final loss of the model is shown below：\n<p align=\"center\">\n    <img src=\"media/7b.loss.png\" width=\"90%\"/>\n</p>\n\n# Training\n\n## Install requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare pre-training datasets\n\nYou should divide the training corpus into multiple UTF-8 text files evenly according to the multiple of the total rank number, and place them in the corpus directory (default is `data_dir`). Each rank processor will read different files in the corpus directory, load them all into memory, and then start the subsequent training process. The above is a simplified demonstration process. It is recommended that users adjust the data production logic according to their needs in formal training tasks.\n\n## Download tokenizer\n\nYou can download our [tokenizer.model](https://huggingface.co/baichuan-inc/Baichuan-7B/blob/main/tokenizer.model) from the Huggingface, and place them in the root director.\n   \n## Config DeepSpeed\n\nThis demo code uses the DeepSpeed framework for training. Users should modify `config/hostfile` according to the cluster conditions.\n\n## Start training\n\n```bash\nscripts/train.sh\n```\n\n# License\n\nThe use of the source code in this repository is governed by the open source license [Apache 2.0](https://github.com/baichuan-inc/Baichuan-7B/blob/main/LICENSE) .\n\nThe use of the Baichuan-7B model weights, however, must follow the [《Baichuan-7B 模型许可协议》](https://huggingface.co/baichuan-inc/Baichuan-7B/resolve/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) .\n\n# Third-Party Resources\n\n1. [LLaMA Efficient Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) supports Baichuan-7B to use Qlora for finetuning, supports RLHF, and supports WebDemo. For models that have gone through sft, see [hiyouga/baichuan-7b-sft](https://huggingface.co/hiyouga/baichuan-7b-sft).\n2. [fireballoon/baichuan-vicuna-chinese-7b](https://huggingface.co/fireballoon/baichuan-vicuna-chinese-7b) uses ShareGPT, ShareGPT-ZH, COT & COT-ZH, Leetcode, dummy, and other Chinese and English data for finetuning. For training code, refer to FastChat.\n3. [fireballoon/baichuan-vicuna-7b](https://huggingface.co/fireballoon/baichuan-vicuna-7b) uses ShareGPT, COT, and Leetcode, among other data, for mixed finetuning. For training code, refer to FastChat.\n4. [Efficient-Tuning-LLMs](https://github.com/jianzhnie/Efficient-Tuning-LLMs) supports Baichuan-7B to use Qlora for finetuning and 4bit inference.\n5. [fastllm](https://github.com/ztxz16/fastllm) is a large model library implemented purely in C++, with no third-party dependencies, and supports Baichuan-7B to run on mobile devices.\n6. [TheBloke/baichuan-7B-GPTQ](https://huggingface.co/TheBloke/baichuan-7B-GPTQ) is for the 4bit quantization of Baichuan-7B's GPTQ.\n\n# Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=baichuan-inc/Baichuan-7B&type=Date)](https://star-history.com/#baichuan-inc/Baichuan-7B&Date)\n"
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "evaluation",
          "type": "tree",
          "content": null
        },
        {
          "name": "media",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1015625,
          "content": "deepspeed==0.9.2\nnumpy==1.23.5\nsentencepiece==0.1.97\ntorch==2.0.0\ntransformers==4.29.1\nxformers==0.0.20\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 5.1474609375,
          "content": "# Copyright 2023 Baichuan Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\n\nimport argparse\nimport deepspeed\nimport deepspeed.comm as dist\nimport numpy as np\nimport sentencepiece as spm\nimport torch\n\nfrom models.configuration_baichuan import BaiChuanConfig\nfrom models.modeling_baichuan import BaiChuanForCausalLM\n\n\ndef get_argument_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data_dir\", type=str, default=\"data_dir\",\n                        help=\"Text files to do pre-train on\")\n\n    parser.add_argument(\"--tokenizer_path\", type=str,\n                        default=\"tokenizer.model\",\n                        help=\"Tokenizer model file path\")\n\n    parser.add_argument(\"--max_length\", type=int, default=4096,\n                        help=\"Max tokens per sentence in corpus\")\n\n    parser.add_argument(\"--steps_per_epoch\", type=int, default=4096,\n                        help=\"Step intervals to save checkpoint\")\n\n    parser.add_argument(\"--checkpoint_saving_path\", type=str,\n                        default=\"checkpoints\",\n                        help=\"Path to store checkpoint files\")\n\n    parser.add_argument(\"--local_rank\", type=int, default=-1,\n                        help=\"Reserved for deepspeed framework\")\n    return parser\n\n\narg_parser = get_argument_parser()\narg_parser = deepspeed.add_config_arguments(arg_parser)\nargs = arg_parser.parse_args()\ndeepspeed.init_distributed()\n\n\nclass DataEngine():\n    def __init__(self, data_dir, tokenizer_path, micro_batch_size, max_length):\n        self.MIN_TEXT_LEN = 20\n        self.EOS_TOKEN_ID = 2\n        self.data_dir = data_dir\n        self.sp = spm.SentencePieceProcessor()\n        self.sp.Load(tokenizer_path)\n        self.micro_batch_size = micro_batch_size\n        self.max_length = max_length\n        self.data = []\n        self.global_input_paths = [self.data_dir + \"/\" + x\n                                   for x in os.listdir(self.data_dir)]\n        self.local_input_paths = [x for i, x in\n                                  enumerate(self.global_input_paths)\n                                  if i % dist.get_world_size() == dist.get_rank()]\n\n    def load_data(self):\n        for file_path in self.local_input_paths:\n            data = []\n            with open(file_path, encoding=\"utf-8\", errors=\"ignore\") as f:\n                for line_id, line in enumerate(f):\n                    cc = self.sp.EncodeAsIds(line.strip()) + [self.EOS_TOKEN_ID]\n                    if len(cc) < self.MIN_TEXT_LEN:\n                        cc = []\n                    data.extend(cc)\n                    if len(data) >= self.micro_batch_size * (self.max_length + 1):\n                        index = self.micro_batch_size * (self.max_length + 1)\n                        self.data.append(data[:index])\n                        data = []\n        return\n\n    def get_data(self):\n        data = self.data.pop(0)\n        seq = np.asarray(data).reshape(self.micro_batch_size, self.max_length + 1)\n        data = torch.LongTensor(seq)\n        data = data.cuda(non_blocking=True)\n        return data\n\n\ndef prepare_data():\n    data_dir = args.data_dir\n    tokenizer_path = args.tokenizer_path\n    ds_config = json.load(open(args.deepspeed_config))\n    micro_batch_size = ds_config[\"train_micro_batch_size_per_gpu\"]\n    max_length = args.max_length\n    data_engine = DataEngine(data_dir, tokenizer_path, micro_batch_size, max_length)\n    data_engine.load_data()\n    return data_engine\n\n\ndef prepare_model():\n    with deepspeed.zero.Init(config_dict_or_path=args.deepspeed_config,\n                             enabled=True,\n                             mem_efficient_linear=False,\n                             mpu=None):\n        model = BaiChuanForCausalLM(BaiChuanConfig())\n\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    model_engine, _, _, _ = deepspeed.initialize(args=args,\n                                                 model=model,\n                                                 optimizer=None,\n                                                 model_parameters=model_parameters)\n    return model_engine\n\n\ndef train(data_engine, model_engine):\n    model_engine.train()\n    step = 0\n    while step < args.steps_per_epoch:\n        data = data_engine.get_data()\n        loss = model_engine(data, labels=data).loss\n        model_engine.backward(loss)\n        model_engine.step()\n        step += 1\n    return\n\n\nif __name__ == \"__main__\":\n    data_engine = prepare_data()\n    model_engine = prepare_model()\n    epoch = 0\n    while True:\n        train(data_engine, model_engine)\n        epoch += 1\n        model_engine.save_checkpoint(f\"{args.checkpoint_saving_path}\",\n                                     tag=f\"Epoch-{epoch}\")\n"
        }
      ]
    }
  ]
}