{
  "metadata": {
    "timestamp": 1736560821741,
    "page": 514,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "numenta/nupic-legacy",
      "stars": 6337,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".circleci",
          "type": "tree",
          "content": null
        },
        {
          "name": ".coveralls.yml",
          "type": "blob",
          "size": 0.068359375,
          "content": "repo_token: yCZax0NKqn0F2jkB6Gf0TWYOJGBHR11be\nservice_name: travis-ci\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.033203125,
          "content": "coreos-vagrant\nbin\nbuild\n.*\n*.pyc\n"
        },
        {
          "name": ".githooks",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.0302734375,
          "content": ".metadata\nTrunk.iml\n*.pyc\n*.class\n*.manifest\n*.cache\ntrunk.sln\n*.m4\ntest.out\n*~\n*.DS_Store\n*.swp\n*.coverage*\n\n# virtual environments\nvenv/\n\n# build\nbuild/\n\n# personalized nupic configuration file\n.nupic_config\n\n# IDE project files\nnbproject/\n*.vcproj\n*.project\n*.pydevproject\n.idea\n\n# Wheels\nwheelhouse/\n\n# output from integration tests\ninference/\nsavedmodels/\n/*.please_wait\n/auto_specials*.csv\n/weighted_mean*.csv\n/agg_gap_hours_24*.csv\n/test*.txt\n/test*.pkl\n\n# other crap\n*.nta/\n\nhtml/\n\nServers/\n\nconf/default/nupic-site.xml\n\n# /\n/.gdb_history\n\n# /examples/prediction/\n/examples/prediction/log_hotgym.txt\n/examples/prediction/log.txt\n\n# /examples/prediction/experiments/hotgym/base/\n/examples/prediction/experiments/hotgym/base/networks\n/examples/prediction/experiments/hotgym/base/report.txt\n/examples/prediction/experiments/hotgym/base/results.pkl\n\n# nupic.core repo is located under $NUPIC/extensions/core/\nextensions/core/\nexperimental/\n\n# Pip stuff\ndist/\nnupic.egg-info/\nnupic/*py_region.*\ntemp/\ndocs/_build_html\n/src/nupic/support/nupic-site.xml\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 3.244140625,
          "content": "sudo: false\n\nbranches:\n  except:\n    - gh-pages\n\nlanguage: cpp\n\nos:\n  - osx\n\ncompiler:\n  - clang\n\nenv:\n  global:\n    - NUPIC=$TRAVIS_BUILD_DIR\n    - PATH=$TRAVIS_BUILD_DIR/python/bin:$PATH\n    # AWS keys are for manual uploads of osx wheel to S3.\n    - AWS_ACCESS_KEY_ID=AKIAIGHYSEHV3WFKOWNQ\n    # AWS_SECRET_ACCESS_KEY encrypted below\n    - secure: \"pYsMDbNp8k2i0d9p5mf/9TlDzxt2FiiR1QJV7QUn+S2/r372VYdZaplL+s9ItVGCGFpAyKk7HXcwm//DKw/ZKb6UlDJm3Fph9m+3aDDIiVFNB7g1uro+Yg8nIGuV8qZEtIaHRtR1zOt1EufoR9dg1Mq8ryRggN9rrB0FvCxUZAQ=\"\n    # PyPi credentials for manual uploads to Pypi on release.\n    - PYPI_USERNAME=numenta\n    # PYPI_PASSWD encrypted below.\n    - secure: \"elm3E3MwpbZqTCz+EHPQ7CEBwoBvRgqS3q5MHY+jUcpdrWeSjmy7TZoRN5+rAjDQYuR5g33pMXnyeRGw/DG7Xa5FG0g3PoMnO6fnIe7Hs7gPlDXbwilfYxfi462N7Y9aAGPFuvNkZsgd0dfwXKwNEiZ5hYfsbFFwHMeXMQA/SA8=\"\n    - NUPIC_DEPLOYMENT_BUILD=true\n\ngit:\n  submodules: false\n\nnotifications:\n  email:\n    recipients:\n      - \"discourse-nupic-developers@numenta.org\"\n    on_success: never\n    on_failure: change\n  irc:\n    channels:\n      - \"irc.freenode.net#nupic-hackers\"\n  webhooks:\n    urls:\n      - https://webhooks.gitter.im/e/68f77bae61efa5c931f8\n      - https://api.keen.io/3.0/projects/5555161e2fd4b1326f14444a/events/travis-webhooks?api_key=a2e613bbb19b9b1f71a5543b7b677563551b4c3fe98534269546ff666ae453f82505791772faefc48682ee882ac7e99d2c3bfae7c7c19db7b5e7bbda34039119e4b42f5bf41bcea62d4ea9731db4a455141be7d5e8c715cb06366922eae0358e84abc2704ce16bb77b01fec3476cbac6\n\n# Successful builds are archived and uploaded to S3 for regression testing.\nbefore_deploy:\n  - . ./ci/travis/before_deploy.sh\n\ndeploy:\n  # S3 deployment of NuPIC and dependency wheels on every push to master. These\n  # packages are used in dependent projects like nupic.regression and\n  # nupic.research.\n  - provider: s3\n    access_key_id: AKIAIGHYSEHV3WFKOWNQ\n    secret_access_key:\n      secure: \"ONG00ZCPpfU/nugFiON3K2q8IMk3nB/aAUj2Ggjf1z0CJS/cvnfIexmJhe+DJCccoco2l5gpiqp7gweH5vXEcyrzTt1I3Z+iFNas2cQ/wF3LjW0NcbdGeC9NN9kGIoOvr8g6L66CUvazYoirgbJO01ktm7LVNeGS3Q1pk36Vp10=\"\n    bucket: artifacts.numenta.org\n    region: us-west-2\n    local-dir: \"dist/wheels\"\n    upload-dir: \"numenta/nupic/${TRAVIS_COMMIT}\"\n    skip_cleanup: true\n    # Only deploy on master\n    on:\n      branch: master\n  - provider: s3\n    access_key_id: AKIAIGHYSEHV3WFKOWNQ\n    secret_access_key:\n      secure: \"ONG00ZCPpfU/nugFiON3K2q8IMk3nB/aAUj2Ggjf1z0CJS/cvnfIexmJhe+DJCccoco2l5gpiqp7gweH5vXEcyrzTt1I3Z+iFNas2cQ/wF3LjW0NcbdGeC9NN9kGIoOvr8g6L66CUvazYoirgbJO01ktm7LVNeGS3Q1pk36Vp10=\"\n    bucket: artifacts.numenta.org\n    region: us-west-2\n    local-dir: \"artifacts\"\n    upload-dir: \"numenta/nupic\"\n    skip_cleanup: true\n    # Only deploy on master\n    on:\n      branch: master\n\n\nbefore_install:\n  # Create a new virtualenv to enable upgrade of setuptools and pip;\n  # see https://github.com/travis-ci/travis-ci/issues/4194\n  - virtualenv --no-site-packages venv\n  - source venv/bin/activate\n  - \". ./ci/travis/before_install-osx.sh\"\n\ninstall:\n  - \". ./ci/travis/install.sh\"\n\nscript:\n  - . ./ci/travis/script-run-tests.sh\n\nafter_success:\n  - cd ${TRAVIS_BUILD_DIR}\n  # Run release script if this is a tagged build.\n  - \"if [ -n \\\"${TRAVIS_TAG}\\\" -a \\\"${TRAVIS_OS_NAME}--${CC}\\\" = 'osx--clang' ]; then . ./ci/travis/after_success-release-osx.sh; fi\"\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 38.341796875,
          "content": "# Changelog\n\n## 1.0.5\n\n* d81e128c6 NUP-2519: Update nupic.core version\n* 3371e4ef9 NUP-2519: Upgrade pycapnp to 0.6.3\n* 11a13c018 NUP-2518: Remove obsolete region initialization parameters from custom region example\n* 67724debf \"pip install --use-wheel\" was deprecated. See https://pip.pypa.io/en/stable/news/#deprecations-and-removals\n* 08daff4a3 Fix softmax overflow\n\n\n## 1.0.4\n\n* 682fd2e66c6d45912cd9c518106740143eff9fd9 :  NUP-2506: Add test to all Serializable subclasses and fix related issues  (#3826)\n* a7ab556a64e57064a8febbcf2ee341a1b1ff18ae : NUP-2506: fix traversal limit (#3823)\n* 54e1ffead7a8cedd9a2dab08bb02c7e5e3536bf3 : Added holidays parameter to date encoder (#3822)\n* 9bb7705ebd428e73f6efc180e77f7f127ce67c4d : version lock 'sphinx-autobuild' dependency 'tornado' (#3815)\n* 13c02de82c6bc52afc364fd1fdce88c5fa1aa92c : Update some legacy code examples. (#3814)\n* 33052e10dbe1030929223fc7e54d2d7c8b8a1ced : Fix metric spec schema bug (#3812)\n* 40e216915a172901b5dbe34932543fae68aa3631 : Fix lack of logging in run_swarm.py (#3809)\n* d8c740486198a6764b18c73bffe6005988435ac7 : NUP-2487: Update category prediction example\n* 38e40266a7ad2744fce904a02faa383676950e1e : Issue #1380: Update SP parameter validation test checking array dimensions\n* e470860e962db70a2b16c82d1647f10b3e985c42 : numenta/nupic.core#1380: Fix SP tests with correct dtype values\n* 38c9c7e1d7b161d9a4b2378cdae3652af704a481 : Add example for infer as well\n* 94e5f62e669dcbd55268c07b6aa30f391135ab4f : Include example to make isSparse parameter easier to understand\n* ffd1457037a52cb63a7be0cf004e886f3909f505 : Update KNN classifier documentation to make the input pattern requirements clear in both learn and infer\n* 5ecae91017c5f4f68c944a3f5b5d79d1276e2c59 : Changed distribution keyword, casted some attributes to float, removed setting list (#3784)\n* 41e5a6aefc649b08dbe948ba3e1db46f5aaaa603 : Issue #3783: Fixes test to compute pass the probation period (#3786)\n* 7e5f587ecd039520a53a4aa4608eb7ce577654f1 : Updating to XCode 8.3\n* 1aea72abde4457878a16288d6786ffb088f69164 : Update name of nyc_taxi.csv to nycTaxi.csv (#3776)\n\n## 1.0.3\n\n- Updated incorrect name for anomaly likelihood region. (#3770)\n\n## 1.0.2\n\n- Fixed BacktrackingTM serialization error (#3765)\n\n## 1.0.1\n\n- Fixed a bug in record sensor that prevented the usage of CoordinateEncoder. (#3754)\n\n## 1.0.0\n\n- Improved exception handling in /swarm/test_db.py (#3738)\n- DEVOPS-362 Remove unnecessary install script\n- DEVOPS-362 test removing dependency on install script entirely\n- DEVOPS-362 update install script\n- DEVOPS-362 Add initial version of missing install script\n- Added serialization guide to API docs. (#3737)\n- Put conditional around capnp for Windows\n- Complete new serialization in SpatialPooler\n- Catch nupic.core reference\n- NUP-2342: consolidate read/write into a single context\n- NUP-2342: Update examples to use capnp serialization\n- NUP-2341: Use capnp serialization for SDRClassifierDiff\n- NUP-2351: Remove TODOs from HTM Prediction Model test and fix  bugs exposed by this test\n- NUP-2351: Add serialization to KNNAnomalyClassifierRegion\n- NUP-2351: Fix KNNClassifier serialization\n- NUP-2349 Implemented testCapnpWriteRead test for PreviousValueModel OPF class. Implemented PreviousValueModel.getProtoType. Return instance from PreviousValueModel.read.\n- NUP-2349 Implemented capnp serialization of PreviousValueModel\n- Put capnp import checks in place for Windows\n- Add serialization tests for TMRegion\n- NUP-2463 Serialize inferenceArgs, learningEnabled, and inferenceEnabled in opf Model.\n- Add support for different TM types in TMRegion serialization\n- Added Serializable to API docs, and inheritence links\n- Fixed Next ID value in comment in model.capnp\n- NUP-2464 Integrated ModelProto support into opf TwoGramModel.\n- Fixed input to SP in docs algo example (#3708)\n- NUP-2464 Serialize numPredictions and inferenceType via ModelProto member of HTMPredictionModelProto.\n- Added Serializable to all classes with a capnp write function (#3710)\n- Safe import of capnp for moving average proto\n- getSchema returns prototype\n- Remove unused \\_readArray\n- Rely on pycapnp/numpy native conversions in write/read\n- Add capnp conditionals for Windows\n- NUP-2351: Use dict directly instead of creating capnp message\n- Fixed Serializable extensions\n- Fix CPP breakages from changes\n- NUP-2351: Add capnp serialization to KNNClassifierRegion\n- Fix everything up to get serialization tests working with capnp serialization for BacktrackingTM\n- Added getSchema to MovingAverage\n- Added Serializable to all classes with a capnp write function\n- Finished up first pass implementation of BacktrackingTM serialization\n- NUP-2350: capnp serialization for TwoGramModel\n- NUP-2449 Completed implementation of HTMPredictionModel serialization tests.\n- NUP-2463 Implemented test (disabled) to demonstrate the bug \"Predicted field and \\_\\_inferenceEnabled are not serialized by HTMPredictionModel.write\"\n- OPF Guide cleanup and link fixes (#3700)\n- NUP-2355 Add new serialization to TestRegion\n- remove SVMClassifierNode (#3697)\n- handle scalar values in the sdr classifier region\n- NUP-2346: Add serialization to knn\\_classifier\n- NUP-2458 Fixed and enabled SDRClassifierTest.testWriteReadNoComputeBeforeSerializing\n- NUP-2458 Implemented testWriteReadNoComputeBeforeSerializing in sdr\\_classifier\\_test.py that reproduces the \"deque index out of bounds\", but disabled the test, since it fails in a different way after the fix, most likely unrelated to the fix, which needs to be debugged\n- NUP-2398 Refactor test comparing different configurations\n- NUP-2458 Prevent index out of bounds when saving `patternNZHistory` after fewer than \\_maxSteps input records have been processed.\n- NUP-2458 Moved HTMPredictionModel serialization test to integration/opf\n- NUP-2449 Implement simple serialization/deserialzation tests. This exposed a number of problems that need to be fixed before we can make further progress.\n- update sdr classifier doc\n\n\n## 0.8.0\n\n* Document ExperimentDescriptionAPI (#3679)\n* Update nupic.math API docs (#3677)\n* SP docs cleanup (#3671)\n* Allow multiple classifications for each record to SDRClassifier (#3669)\n* Updated BacktrackingTMCPP compute parameter name (#3667)\n* Fix HTMPredictionModel prediction using SDRClassifier (#3665)\n* Remove CLAClassifier (#3665)\n* Add capnp serialization to TMRegion (#3657)\n\n## 0.7.0\n\n**WARNING**: This release contains breaking changes described in\nhttps://discourse.numenta.org/t/warning-0-7-0-breaking-changes/2200\n\n* Stop calling the backtracking_tm tests \"tm tests\" (#3650)\n* Update hierarchy demo to fix regression\n* Clean up BacktrackingTM's public API (#3645)\n* Make region file names snake_case (part 7) (#3640)\n* Removed references to obsolete tm_py_fast shim (#3639)\n* Updated OPF Metric API docs (#3638)\n* updated __init__.py to include missing encoders (#3487)\n* Fixed anomaly likelihood doc problems. (#3629)\n* Updates swarming, some region code to snake_case (part 6) (#3627)\n* Fixed OPF util helpers module names. (#3625)\n* Complete RST docs for nupic.support (#3624)\n* Deleted nupic.support.features* (unused) (#3622)\n* Removed nupic.support.exceptions (unused) (#3620)\n* Proper snake_case for nupic.support (part 5) (#3618)\n* Snake case nupic.encoders (part 4) (#3614)\n* Moved opf_helpers module to helpers (#3610)\n* Removes unused code from nupic.support (#3616)\n* Applying snake_case module name standards (PART 3) (#3611)\n* Fixed support initLogging docstring params\n* Finished OPF utils and env docs\n* Documented OPF Basic Env\n* Documented OPF ENv\n* Documenting OPF Task Driver\n* Documenting OPF experiment runner\n* Removed OPF utils PredictionElement (#3604)\n* Partial doc of experiment description api\n* NUP-2429 Add .gitignore with first_order_0.csv to prevent accedental commits of this generated file.\n* Documented cluster_params canned model config\n* Documented OPF model exceptions\n* Finished doccing opf_utils\n* Documenting OPF utils\n* Removed predictedField from HTMPredictionModel constructor (#3600)\n* NUP-2420 Renamed tm_shim.py to BacktrackingTM_shim.py\n* Removes inputRef / bookmark params from appendRecord (#3597)\n* Documented nupic.data (#3593)\n* OPF Model docstrings (#3589)\n* Remove obsolete nupic.research.bindings check\n* Removed unimplemented abstract methods (#3596)\n* Removed WeatherJoiner code from old example (#3595)\n* Updated snakecase opf_utils in RST docs (#3585)\n* Renamed tm_ccp test so it runs\n* Moved research tm_cpp_test.py back into nupic.research\n* Removed base.Encoder.formatBits() (#3582)\n* Replace dump() with define __str__ in Encoders. Issue #1518 (#3559)\n* Complete encoder docstrings (#3579)\n* Removed nupic.research, moved contents to nupic.algorithms\n* move zip logic into 'build_script'\n* Add support for artifacts deployed to S3 named according to sha\n* Snake case module names PART 2 (#3561)\n* Remove old examples Part 2 (#3562)\n* NUP-2401: Check for prediction results discrepancies (#3558)\n* NUP-2397: rename TP* to TM* (#3555)\n* NUP-2405: quick-start guide for the Network API (#3557)\n* Snake case module names PART 1 (#3550)\n* NUP-2394: network API code example (#3520)\n* Remove old examples Part 1 (#3551)\n* Docs: InferenceShifter,ModelResult,SensorInput,InferenceType (#3549)\n* CLAModel name changed to HTMPredictionModel (#3516)\n* Updating FileRecordStream docstrings (#3545)\n* Fieldmeta docstrings (#3541)\n* Update KNNClassifier docstrings (#3535)\n* SDRClassifier docs, default config docs\n* Updates anomaly docstrings (#3537)\n* [NUP-2399] Added style guides to new guide (#3528)\n* NUP-2396 Allow SensorRegion to pass actValue and bucketIdx to SDRClassifierRegion\n* Added anomaly detection guide (#3521)\n* NUP-2389 Upgrade nupic.bindings dependency to 0.6.1 which has the requisite changes.\n* name change tpParams/tmEnable => tmParams/tmEnable (#3514)\n* NUP-2391: packages to document & progress tracking (#3517)\n* Quick Start Algorithms Section (#3512)\n* Quick Start\n* NUP-2389 Remove calls to Region::purgeInputLinkBufferHeads. Since we only support delay=0 in CLA models, we no longer need `purgeInputLinkBufferHeads`, because the new Link::compute logic in nupic.core now performs direct copy from src to dest for links with delay of 0.\n* Disable flatline hack in anomaly likelihood\n\n## 0.6.0\n\n* Touch init even if model params dir exists\n* Auto-add __init__.py when model parms created\n* Shift code from otherwise unused `nupic.engine.common_networks` to example where it's used.  Includes bugfix renaming `rawAnomalyScore` to `anomalyScore`\n* Explicitly import and use `engine_internal` in lieu of `engine` to avoid confusion, create `nupic.engine.OS` and `nupic.engine.Timer` by assignment rather than subclass\n* Change SparsePassThroughEncoder dtype error to ValueError\n* Fix for an unrelated change that resulted in numpy arrays being used in cpp implementation\n* Give better message for bad dtype to SparsePassThroughEncoder\n* Add test for passing float values for radius\n* Adds api docs for coordinate encoders\n* Cleanup CoordinateEncoder\n* Remove svm, cells4 tests that are moved to nupic.core.\n* Added missing anomaly stuff, fixed requirements\n* Moved sphinx deps out of requirements.txt\n* Fix hotgym_regression_test.py to make it work with nupic.core PR 1236.\n* Skip test when capnp is not available, such as windows as well as address feedback from Scott\n* Serialization base python class analagous to nupic.core Serializable c++ class\n* Adds a demo Jupyter notebook, useful for demonstrating usage of visualization framework and as an entrypoint for tinkering with different network topologies\n* Speed up SpatialPooler read method.\n* Rename normalProbability to tailProbability.\n* Use IterableCollection from engine_internal\n* Call Region.purgeInputLinkBufferHeads after compute() calls in CLAModel to integrate with the new delayed link implementation from nupic.core.\n* rename maxBoost to boostStrength in hotgym example\n* Disable backward compatibility serialization test\n* remove minPctActiveDutyCycle parameter form SP compatability test\n* update expected result in hotgym, result change due to different rounding rules in boosting\n* eliminate minPctActiveDutyCycle from spatial pooler\n* Rename maxBoost to BoostStrength\n* Stop changing the overlaps to do tie-breaking\n* Stop trying to get identical boost factors between py and cpp\n* set maxBoost in expdescriptionapi\n* update sp_overlap_test to use global inhibition\n* slight simplification of boostFactor calculation\n* Implement update boost factors local and global\n* Avoid floating point differences with C++ SpatialPooler\n* run C++ SP in spatial_pooler_boost_tests\n* update spatial pooler boost test\n* update boosting rules for spatial pooler\n* fix bug in setPotential\n* modified SP boosting rule\n\n## 0.5.7\n\n* Remove tests moved to nupic.core and update version to latest bindings release.\n* Update hello_tm.py\n* Removed linux and gcc from Travis build matrix\n* Makes `anomaly_likelihood.py` compliant to Python3\n* Update env vars and paths to simplify the AV configuration and installation.\n* Cleanup references to nupic.bindings and old CI code for manually fetching nupic.bindings since it should be found on PyPI without doing anything special.\n\n## 0.5.6\n\n* Since manylinux nupic.bindings wheel 0.4.10 has now been released to PyPi, we no longer need to install nupic.bindings from S3.\n* fix logic in _getColumnNeighborhood\n* Bugfix in flatIdx reuse after a segment is destroyed\n* Change private _burstColumn class method signature to accept a cellsForColumn argument in lieu of a cellsPerColumn argument.  Move the calculation that otherwise depends on cellsPerColumn into the instance method.\n* TM: Support extensibility by using traditional methods\n* Update expected error for topology changes\n* Update expected hotgym result for topology changes\n* Adds RELEASE.md with documentation for releasing NuPIC.\n* Match nupic.core's SP neighborhood ordering.\n* Update inhibition comments and docstrings.\n* Introduce mechanism by which already-installed pre-release versions of nupic.bindings are ignored during installation\n* Assign self.connections value from self.connectionsFactory() rather than direct usage of Connections constructor. Allows better extensibility should the user want to change some aspect of the creation of the connections instance in a subclass\n* Removed obsolete directory src/nupic/bindings/\n* Remove the notion of \"destroyed\" Segments / Synapses\n* Enable proper subclassing by converting staticmethods that referenced `TemporalMemory` to classmethods that reference their class.\n* Fixup TemporalMemory.write() to handle columnDimensions as tuples.\n* Initialize columnDimensions as a tuple in test to reflect common convention.  This forces the TemporalMemoryTest.testWriteRead test to fail in its current state.\n* Store \"numActivePotentialSynapses\". No more \"SegmentOverlap\".\n* Add a lot more scenarios to the TM perf benchmark\n* Moved audiostream example to htm-community\n* Safer \"addToWinners\" value. Play nicely with surgical boosting.\n* Bugfix: With no stimulus threshold, still break ties when overlaps=0\n* Clean up trailing whitespace and tabs\n* Properly apply the stimulus threshold\n* Add test for new \"learn on predicted segments\" behavior\n* Split compute into activateCells and activateDendrites\n* Grow synapses in predicted columns, not just bursting columns\n* Removed bundled get-pip.py and instead fetch version copy from S3\n* Removed .nupic_modules and now rely on versioned release of nupic.bindings on PyPI\n* averagingWindow size updated to improve HTM scores for RES-296\n* Build system updates for Bamboo (Linux), Travis (OS X), and AppVeyor (Windows)\n* Added nyc taxi example for anomaly detection\n\n## 0.5.5\n\n* Renamed a misclassed class name from ConnectionsTest to GroupByTest\n* not _ is => is not and fixes groupby comment and passes integration tests\n* overhaul to groupby, now 10% faster than current implementation\n* NUP-2299 Install specific versions of pip, setuptools, and wheel.\n* NUP-2299 Added platform-conditional dependency on pycapnp==0.5.8 using PEP-508.\n* lazy group_by and changes to GroupByGenerator\n* perf improvement to segment comparison in compute activity\n* 100 % increase in spped\n* small perf changes\n* demonstrate that compatability test works with predictedSegmentDec not 0.0\n* fixes subtle bug in numSegments that caused integration tests to fail\n* fixes bug where minIdx could be passed as a float rather than an int\n* skip serialization test if capnp is not installed\n* lints and updates comments in group_by.py and group_by_tests.py\n* gets same results as c++ temporal memory after group_by changes\n* ports group_by tests and they pass\n* adds groupByN utility function for use in TM\n* all connections tests written and passing, moved some stuff around and added missing function to connections\n* started porting new connections tests and minor changes to connections.py\n* improves permanence >= testing in computeActivity\n* confirmed python implementation is same as cpp version. Needs better perf now\n* adds back AnomalyRegion and Anomaly class in anomaly.py and related tests\n* fixes bug in growSynapses, almost exactly the same\n* Updated core SHA and default SDR classifier implementation\n* Updated SDRClassifier factory and region to handle cpp\n* changed input name from value to metricValue\n* updates variables names in anomaly_likelihood.py and AnomalyLikelihoodRegion\n* adds new connections methods\n* create new methods for creating/destroying synapses/segments\n* continues change of connections datastructures\n* move raw anomaly calculation back to nupic.algorithms.anomaly\n* Finished swarming/hypersearch separation\n* Moved base hypersearch classes to hypersearch\n* Moved experimentutils to nupic.swarming\n* Updated SDR classifier internals\n* calculate raw anomly score in KNNAnomalyClassifier\n* removes anomaly.py dependency in network_api_demo.py\n* changes how TMRegion computes prevPredictdColumns and updates clamodel\n* Install pip from local copy, other simplifications\n* Fixup PYTHONPATH to properly include previously-defined PYTHONPATH\n* adds pseudocode to core functions\n* continues implementation of AnomalyLikelihoodRegion\n* Limit tests to unit after ovverriding pytest args on cli\n* DEVOPS-85 OS X build infrastructure for Bamboo CI environment\n* replaces segmentCMP with lambda and updates docstrings\n* uses arrays instead of dicts in computeActivity\n* Corrections to examples in tm_high_order.py\n* incorporates binary search into the algorithm where applicable\n* remove outdated nab unit tests\n* use Q function\n* Corrections to examples in tm_high_order.py\n* change to column generator\n* Added tm_high_order.py to show examples of the temporal memory.\n* Fixed conversion bug in SDRClassifier serialization\n* Fixed patternNZ proto writing.\n* Slight fix for pattern history handling in sdr classifier\n* Small fix on SDR classifier\n* Better fix for #3172, using the initialize() function and checking if _sdrClassifier is set\n* Updated learning rate for SDR classifier + slight changes to the error ranges in OPF test\n* Updated hotgym test with actual value and implemented first fix for OPF test\n* Updated tests and examples with SDR classifier\n* Finished updating examples with SDR classifier.\n* Updated hotgym and general anomaly examples with SDR classifier.\n* Updates pycapnp to 0.5.8\n* test_db-fixes avoids printing user password in plaintext\n* test_db-fixes updates database and table name\n* Corrections made to the spatial pooler tutorial.\n* changes maxBoost default value to 1.0\n* fixes connection tests and prints config file used in test_db.py\n* Moved back overlap accesors test for spatial_pooler from API tests to unit tests.\n* Added tutorial script for the spatial pooler. Modified README file accordingly.\n* Moved the unit test for SP overlap accesors to API tests.\n\n## 0.5.4\n\n* Added overlap accessors to spatial_pooler.py plus unit tests. (Code style corrected)\n* Updated VERSION in Spatial Pooler and added backward compatibility in setstate()\n* Added members overlaps and boostedOverlaps to SpatialPooler class.\n* Addition of overlaps and boostedOverlaps members to SpatialPooler class plus unit tests.\n* Added docs for return type in RDSE internal func.\n* tm_cpp with tuned parameters\n* RES-215 Changes to add params for new TM subclass for NAB\n* Remove main function from SDRClassifierRegion\n* remove unused methods from SDRClassifierRegion\n* Add simple end-to-end integration test for SDRClassifierRegion\n* use string split instead of eval to parse strings\n* correct inconsistent error msg in sdr_classifier_factory.py\n* Fix readWrite test of SDR classifier\n* Add SDRClassifier Region to pyRegions\n* Initial implementation of SDRClassifier Region\n* implement SDR classifier factory\n* Add capnp proto for SDR classifier region\n* Add default value for SDR classifier implementation in nupic-default.xml\n\n## 0.5.3\n\n* Default DATETIME columns to NULL in ClientJobsDAO for compatibility across mysql versions. As of mysql 5.7.8, values of 0 are not allowed for DATETIME columns, and CURRENT_TIMESTAMP is semantically inappropriate for those columns.\n* Suppress this optional dependency on matplotlib without logging, because python logging implicitly adds the StreamHandler to root logger when calling `logging.debug`, etc., which may undermine an application's logging configuration\n* Bugfix: Write the 'actualValues' to the output, don't reassign the output\n* Fixed Username Regex in ClientJobsDAO\n* cleaned up region a bit to make it compliant with numenta's coding guidelines.\n\n## 0.5.2\n\n* Fixe to GCE to return the right number of scalars when altitude is missing.\n\n## 0.5.1\n\n* Improves SDR classifier and tests\n* Modify the continuous online learning test\n* Add 3 tests on multiple item prediction\n* Fix test_pFormatArray\n* Implement SDR classifier in NuPIC\n* Make the 'arrayTypes' list more informative\n* Add getParameter/setParameter support for Bool and BoolArray\n* Improved anomaly params (from NAB)\n* Added minSparsity option\n* Get the encoder's outputWidth via parameter\n* Use nupic.core encoders from nupic via the Network API\n* Fix bugs and inconsistencies in the custom region demo\n* Adds BINDINGS_VERSION envvar to wheel filename (for iterative builds)\n\n## 0.5.0\n\n* Removes references to FastTemporalMemory.\n* Lower TM epsilon threshold for compatibility.\n* Add documentation for the Monitor Mixins\n* Removed FastTemporalMemory from nupic\n* Update temporal memory compatibility test to use C++ TM.\n* Sort segments before iterating for compatibility with C++\n* Sort unpredictedActiveColumns before iterating for compatibility with C++\n\n## 0.4.5\n\n* This release is just to sync with nupic.bindings 0.3.1.\n\n## 0.4.4\n\n* Botched release (sorry!)\n\n## 0.4.3\n\n* Updating to proper core sha\n\n## 0.4.2\n\n* Using official release version of bindings for nupic release.\n\n## 0.4.1\n\n* Manualy update of nupic.bindings dev version after botched release attempt\n\n## 0.4.0\n\n* Updated hello_tm.py to use accessors\n* Updated TP_shim.py to use accessors Updated `columnForCell` and `_validateCell` in FastTemporalMemory to conform to their docstrings, which is needed for the change to TP_shim.py\n* Updated temporal memory monitor mixin to use accessors\n* Updated temporal_memory_test.py to use accessor methods.\n* Added accessors to temporal_memory.py\n* Change temporalImp to tm_py for both networks and add comment about it being a temporary value until C++ TM is implemented\n* Refactored to remove common code between network_checkpoint_test.py and temporal_memory_compatibility_test.py\n* Use named constants from nupic.data.fieldmeta in aggregator module instead of naked constants.\n* Fix AttributeError: 'TMShim' object has no attribute 'topDownCompute'\n* Support more parameters in TMShim\n* Serialize remaining fields in CLAModel using capnproto\n* Enforce pyproj==1.9.3 in requirements.txt\n* Use FastCLAClassifier read class method instead of instance method\n* Have CLAClassifierFactory.read take just the proto object\n* Add capnp serialization to CLAClassifierRegion\n* Add capnp serialization to SPRegion\n\n## 0.3.6\n\n* Windows support\n* Serialization work\n* Moved SWIG out into nupic.core\n* Major build changes\n\n## 0.3.5\n\n* Raise explicit exception if user passes non-str path\n* SP: simplify local inhibition\n* SP: adapt tests, sort winning columns output\n* SP: simplify active columns assignment\n* SP: simplify global inhibition\n* file Rename as hello_tm.py and modifications in comments\n\n## 0.3.4\n\n* Added src/nupic/frameworks/opf/common_models/cluster_params.py and supporting files from numenta-apps htmengine. A separate numenta-apps PR will remove this code from htmengine.\n* fixes #2592\n* fix for #2265\n* fix for bug #2265\n* Fixup Dockerfile to install nupic.bindings, and other cleanup\n* Adding C++ compiler requirement to README.\n* Fix for test failure\n* Fixed stream definition reference error.\n* Reduce default reestimation period.\n* Remove greedy reestimation of distribution\n* Pointing README to proper bindings version.\n* Continuing work on 0.3.4.dev0.\n* removing a test that depends on nupic.vision\n* PCA_Node test: some fixes, WIP\n* formatting\n* test for PCANode region\n* remove Pillow from requirements.txt as it was used for vision only\n* fix merge mistake in csv file\n* move test from PCANode to nupic.vision unittest\n\n## 0.3.3\n\n* Include additional file types in MANIFEST.in, consistent with setup.py\n* Pattern and Sequence machines using nupic::Random\n* Wrap sparse matrix implementations with cortical column-centric semantics as a way to abstract away the underlying implementation\n* Re-enable testHotgymRegression\n\n## 0.3.2\n\n* Update to nupic.bindings version with fix for platform differences\n* Rename nupic directory to src/nupic\n* Updated S3 URL to nupic.bindings for Linux install\n* Fix paths for data files in an integration test\n* Fix issue with storing temporary file in wrong location in integration test\n\n## 0.3.1\n\n* Specify nupic.bindings version to match commit sha (0.2).\n* Use logging.debug for emitting the message about not being able to import matplotlib; we log it at debug level to avoid polluting the logs of apps and services that don't care about plotting.\n* Add Dockerfile ready to perform swarming.\n* Removes PCANode\n* Updated Linux binary install instructions.\n\n## 0.3.0\n\n* Updated comment about greedy stats refresh when likelihood > 0.99\n\n## 0.2.12\n\n* Implemented unit tests for the new features in AnomalyLikelihood class.\n* Convert AnomalyLikelihood._historicalScores to a user-configurable sliding window, instead of accumulating all of the incoming data points. This improved performance a ton! Added AnomalyLikelihood.forceModelRefresh() method.\n* Update nupic.core to include backwards compatibility fix for RandomImpl.\n* Uninstall pycapnp to avoid running tests that utilize the functionality and currently fail with Duplicate ID error.\n* Makes pycapnp and corresponding serialization optional. If pycapnp is not installed then the corresponding serialization tests will be skipped.\n* Add Multiple Prediction Test for NegLL Metric\n* Add test for NegLL Error Metric\n* Fix Orphan Decay Bug in temporal memory test\n* Change decreasing overlaps test for coordinate encoder to not require a strict decrease (staying the same is ok).\n* Allow specifying MonitoredTemporalMemory as TM implementation through OPF\n* include bucket likelihood and classifier input in clamodel\n* update metrics managers to pass model results to metrics\n* introducting a computeFlag to prevent double-computation. * The flag is used to prevent double computation in the event that customCompute() is called at the same time as compute()\n* Added `numRecords` param for consitency with the newly added `infer` method in FastCLACLassifier\n* checking if classifier has a `maxCategoryCount` attribute. If not, set it to solve backward compatibilities issues\n* renaming numCategories to maxCategoryCount to be constistent between KNN and CLA classifier\n* made new experimentutils file containing InferenceElement, InferenceType, and ModelResult duplicates which we will want to change in the future\n\n## 0.2.11\n\n* Updating nupic.core sha.\n* Updated location of NuPIC Linux wheel on S3.\n\n## 0.2.10\n\n* Updating bindings version.\n\n## 0.2.9\n\n* Added pip install command for linux bindings.\n* Change term predictedColumns to predictedActiveColumns in the TemporalMemory\n\n## 0.2.8\n\n* Updated to correct pypi license string.\n\n## 0.2.7\n\n* Changed all copyright headers on all files to AGPL.\n* split up pip wheel to multiple commands\n* Fixed fast_temporal_memory cellsForColumn calculation. Column is an int (specifically a numpy.int64 and getCellIndex would fail in this), not a cell\n* Broke out model record encoding functionality from RecordStreamIface into ModelRecordEncoder class.\n* Convert nupic to namespace\n* updated include statements in swig files\n* added dict utils to hypersearch specific utils file and modified dependencies accordingly\n* Updated to AGPL.\n* Remove tweepy.\n* KNNClassifier input multiple categories, and integration test\n* enable multiple categories in Network API\n* Makes nupic a namespace package that other projects can extend.\n* Added NRMSE metric\n* Allow Connections to be serialized.\n* Added ability to unregister python regions and updated core sha\n* Remove unused synapses in Temporal Memory\n* Fix: TemporalMemory.getCellIndex doesn't work correctly when running through OPF\n\n## 0.2.6\n\n* Sets zip-safe to false to make sure relative capnp schema imports will work and importing .capnp files will work.\n* Clean up capnp imports.\n* Changes to TM test to accommodate changes in the default value of predictedSegmentDecrement\n* Merge remote-tracking branch 'upstream/master'\n* Change default value of predictedSegmentDecrement to be 0 to be backward compatible\n* Change default value of predictedSegmentDecrement to be 0 to be backward compatible\n* Change default value of predictedSegmentDecrement to be 0 to be backward compatible\n* Merge remote-tracking branch 'upstream/master'\n* Rename testconsoleprinter_output.txt so as to not be picked up by py.test as a test during discovery\n* likelihood test: fix raw-value must be int\n* Fix broken TMShim\n* Revert \"Fix TM Shim\"\n* Anomaly serialization verify complex anomaly instance\n* Likelihood pickle serialization test\n* MovingAverage pickle serialization test\n* Fix TM Shim\n* Removed stripUnlearnedColumns-from-SPRegion\n* Updated comment describing activeArray paramater of stripUnlearnedColumns method in SP\n* Revert \"MovingAvera: remove unused pickle serialization method\"\n* Updated NUPIC_CORE_COMMITISH to use the core without stripNeverLearned\n* Removed stripNeverLearned from SP.compute\n* MovingAverage has getter for current value\n* Fixes bug in mmGetCellActivityPlot\n* Merge remote-tracking branch 'upstream/master'\n* Fixes bug in mmGetCellActivityPlot\n* Fixes bug in mmGetCellActivityPlot\n* addressing scott's cr\n* addressing cr; docstring formatting and minor\n* Continuing work on 0.2.6.dev0.\n* minor\n* first version of knn tests\n* Update SHA and fix files\n* Rename cpp_region to py_region\n* pylint\n* fix likelihood equals problem when default timestamp\n* Likelihood: @param docstring\n* AnomalyLikelihood: add __str__\n* ANomalyLikelihood equals test case\n* Anomaly: add eq test\n* add MovingAverage eq test\n* anomaly likelihood, MA, Anomaly: review - better _eq_ statement\n* Anomaly: code review - use instance access\n* improving constructor docs\n* AnomalyLikelihood: add _eq_\n* Anomaly: compare likelihood in _eq_\n* improve anomaly serialization test - use eq\n* MovingAvera: remove unused pickle serialization method\n* Anomaly & MovingAverage : change __cmp__ to __eq__\n* define equals operator (__cmp__) for anomaly & MovingAverage\n* anomaly serialize test - comment out parts\n* Anomaly: add serialization test\n\n## 0.2.5\n\n* Fix MANIFEST.in capnp include.\n* Update documentation related to PyRegion serialization introduction.\n* Updates nupic.core and adds function definitions for read/write in PyRegion\n\n## 0.2.4\n\n* Fix a minor bug in the algorithm\n* Implement orphan synapse decay\n* register python regions in Region class method\n* moved registration of python regions to nupic.core\n* date encoder bug fix\n* Implement orphan synapse decay\n* changed default regions to tuples\n* fill predictedActiveCells with 0\n* removing irrelevant files\n* removing old network api demo 2\n* modified PyRegion to accept custom classes\n* renamed unionMode to computePredictedActiveCellIndices\n* set the output size for active indices and predicted+active indices to max possible size\n* converting union pooler input to right format\n* Port AnomalyRegion serialization\n* Rename \"enc\" to \"encoder\"\n\n## 0.2.3\n\n* updated custom region methods and example to be static\n* demo for custom regions\n* Improve docstring for 'save' method and others.\n* allows custom regions\n* moved encoder changes to network_api_demo\n* updated network_api_demo in new file to make swapping out encoders easier\n* bit more explanation for MultiEncoder\n* Use different logic for determining whether or not to translate back into actual values from bucket indices\n* Switch over to C++ SpatialPooler where possible to speed up tests/build.\n* Finish implementation of TemporalMemory serialization\n\n## 0.2.2\n\n* Fixed equality test for Connections class\n* Removing learning radius parameter from nupic\n* Add Cap'n Proto serialization to Python Connections\n* Remove FDRCSpatial2.py\n* Replace the use of FDRCSpatial2 to SpatialPooler\n* SP profile implemented from tp_large\n* TM profile: can use args from command-line, random data used\n* Adds AnomalyRegion for computing the raw anomaly score. Updates the network api example to use the new anomaly region. Updates PyRegion to have better error messages.\n* Remove FlatSpatialPooler\n* Add delete segment/synapse functionality to Connections data structure\n* Adding dependency listing with licenses.\n* Bump pycapnp to latest (0.5.5) for security update\n* Remove redundant encoderMap operations\n* Remove redundant index, and EncoderDetails in favor of using the outer union directly\n* Use union in capnp schema per feedback\n* MultiEncoder capnp implementation, including a switch to relative imports as a workaround for an issue described in https://github.com/jparyani/pycapnp/issues/59\n* SparsePassThroughEncoder capnp implementation\n* PassThroughEncoder capnp implementation\n* LogEncoder capnproto implementation\n* GeospatialCoordinateEncoder capnp implementation\n* DeltaEncoder capnp implementation\n* CoordinateEncoder capnp implementation\n* AdaptiveScalarEncoder capnp implementation\n* SDRCategoryEncoder capnproto implementation\n* CategoryEncoder capnproto serialization, fixes #1964\n* Change anomaly score to always be zero when there are no active columns.\n* Date encoder capnproto implementation\n* RDSE capnproto implementation w/ bugfix in encoder base\n* Remove redundant radius and resolution in favor of relying on them to be recalculated based on n.\n* Remove explicit int casts and update tests to allow ints or longs.\n* Integrate capnproto serialization into ScalarEncoder re: #1715\n* Allowing relative paths for input files in swarm desc.\n* accepts anomaly records as both lists and tuples\n\n## 0.2.1\n\n* Moved data pkg_resource data into nupic/datafiles.\n* Replaces datasethelpers with pkg_resources.\n* refactor submetrics computation handling None\n* Adding numpy to README requirements.\n* Move pattern/sequence machine tests to proper location.\n* Moves pattern_machine and sequence_machine files into generators module.\n* Get cell indices methods added an amazing super cool docstring formatting\n* Updates data generator tool filename and adds executable bit.\n* Moves anomalyzer to nupic.data.generators module\n* Move data generators to nupic.data.generators module\n* fixme burn-in for multi metric addInstance()\n* fix AggregateMetric with None metricSpec\n* add MetricMulti class\n* AggregateMetric sets id from params (if specified)\n* Removes isDelta method from encoder base class.\n* fixed predicted active cells in tm-mm\n* rename --enable-optimizations to --optimizations-native\n* add --optimizations-lto option to setup.py to enable Link Time Optimizations\n* Simplify docker setup to single Dockerfile at root\n* Adding cell activity plt and improving metrics table\n* add python setup.py --enable-optimization\n* enable -Wextra warnings\n* sane optimization defaults for binary published builds\n* Revert \"default make with -j4\"\n* add Ofast linker flag for gcc\n* fix: remove inline - let LTO decide\n\n## 0.2.0\n\n* Code changes required for a Windows build.\n* Updates nupic.core to d233c58b64e8064d4d12684634dc5e5e78c7ce0b.\n* Implements capnp serialization for Python spatial pooler. Also implements temporary hack for putting .capnp files into the source tree since the build seems to be set up to install in-tree.\n\n## 0.1.3\n\n* Remove unnecessary build flag and fix a bug that was causing duplicated definition names.\n* Added warning in README for OS X.\n* Doc updates\n* Include additional libs in common libs\n* Use gcc in default docker configuration to match nupic.core binary release. Increase resources in coreos configuration.\n* Fixed ValueError When coordinate encoder is used with DateEncoder\n* Add library path for capnp libraries to linker.\n* Adds capnp libraries to linker args.\n* Adds interface file for converting from pycapnp schema to compiled in schema and uses it with SWIGed C++ SpatialPooler class's read and write methods.\n* Discard NTA_PLATFORM_* in favor of NTA_OS_* and NTA_ARCH_* macro variables\n* Raises exception when enableInference was not called, or when predicted field missing from input row.\n\n## 0.1.2\n\n* Add archflags env var before deploy command on OSX\n\n## 0.1.1\n\n* Removal of CMakeLists.txt\n* Removes fake C extension from setup.\n* Adds warning on darwin platform when ARCHFLAGS not set.\n* Cleanup re: #1579.  Fixup namespace conflicts with builtins (file, dir, etc.) as well as minor alignment issues\n* Switch from cmake to distutils extensions for nupic installation\n\n## 0.1.0\n\n* Cleaned up README and CHANGELOG for 0.1 release.\n\n## 0.0.38\n\n* SWIG optimizations.\n* Script to deploy linux wheel to S3 on release.\n* Publishing select artifacts to pypi on release.\n* Changed dev version pattern to match what python wants.\n* Cleaned up setup and manifest for proper sdists.\n* Faking extensions to get platform-specific wheels.\n* Added core capnp files to bindings.\n* GCE now encodes altitude using a 3D coordinate system.\n* Distributing `*.i` files from `nupic.bindings` in binary packages.\n* Updates test entry points to pure python. README instructions for running tests were updated.\n* Missing configuration files are no longer ignored. A runtime exception is raised immediately when an expected configuration file is not found.\n* Updated deployment logic to account for both deployment scenarios (iterative and release).\n* Configured pypi deployment on all branches with tags.\n* Added pypi deployment configuration for binary releases.\n* Parsing python requirements in setuptools so they are included within published packages (working toward releases).\n* Setting up python wheels packaging and upload to S3 for future distribution.\n* Implemented logic for reusing segments, to enforce a fixed-size connectivity (nupic.core).\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 3.94140625,
          "content": "Thank you for your interest in helping NuPIC improve and evolve. NuPIC was open-sourced by Numenta in June 2013. Numenta maintains and runs this code (with the help of the HTM community) to support ongoing research and development of HTM theory. We also create sample applications of HTM as [showcased here](http://numenta.com/applications/).\n\nBefore your [pull requests](https://help.github.com/articles/using-pull-requests) will be reviewed by our team of committers, you'll need to sign our [Contributor License](http://numenta.org/licenses/cl/). You can see the current list of [committers](http://numenta.org/committers/) and [contributors](http://numenta.org/contributors/) who've signed our CL.\n\nBefore considering contribution, we would like you to understand what NuPIC is and how it works. It would be a good idea to become familiar with [HTM Theory](https://numenta.org/htm-school/) and read some posts on [HTM Forum](https://discourse.numenta.org/).\n\n## [NuPIC Community](https://numenta.org/community/)\n\nLearn about the other contributors who you'll be working alongside.\n\n### Ways to Contribute\n\n#### Bounties\n\n[![Bountysource](https://www.bountysource.com/badge/team?team_id=2156&style=bounties_received)](https://www.bountysource.com/teams/numenta/issues?utm_source=numenta&utm_medium=shield&utm_campaign=bounties_received)\n\n#### File Bugs\n\nIf you found a bug, [please report it](https://github.com/numenta/nupic/issues) in as much detail as possible. Copy / paste of console text is better than screenshots of console logs.\n\n#### Document Code\n\n[We're missing a lot of documentation](docs/README.md).\n\n#### Fix Issues\n\nWe tag issues with a \"help wanted\" label when we are looking for community contributions. [You can see all these tickets here](https://github.com/numenta/nupic/labels/status%3Ahelp%20wanted).\n\n### Protocol and Processes\n\nBelow are some things you should familiarize yourself with before working on NuPIC.\n\n#### [NuPIC Development Tutorial Video](https://www.youtube.com/watch?v=Yc3PKaT1knU)\n\n> **NOTE**: This video is a bit outdated, but still useful for those getting to know the git process.\n\nA beginner's introduction to how we use git, Github, and Travis-CI for NuPIC development.\n\n#### [Development Process](https://discourse.numenta.org/t/development-process/2133/1)\n\nIf you have a change you want to make, a bug you'd like to fix, or a new feature to implement, start here. This document will inform you about the process of developing NuPIC.\n\n#### [Developer Workflow](https://discourse.numenta.org/t/developer-workflow/2132/1)\n\nProvide technical details about how code changes are vetted, tested, approved, and eventually integrated into the master branch of our repositories.\n\n#### [Release Process](RELEASE.md)\n\nLearn more about how NuPIC is released through the page linked above.\n\n### Developer Resources\n- [Development Tips](https://discourse.numenta.org/t/development-tips/2140)\n- [Developer Workflow](https://discourse.numenta.org/t/developer-workflow/2132)\n- [Documenting Code](docs/README.md)\n- [External Libraries](https://discourse.numenta.org/t/external-libraries/2141)\n- [Profiling and Optimization for Speed](https://discourse.numenta.org/t/optimization-profiling-speed/2142)\n\n#### Coding Standards and Guides\n\n- [C/C++ Coding Guide](https://numenta.github.io/nupic/guides/contributing/cpp-style-guide.html)\n- [CMake Style Guide](https://discourse.numenta.org/t/cmake-style-guide/2137/1)\n- [Python Style Guide](https://discourse.numenta.org/t/python-style-guide/2128/1)\n- [Contribution Standards](https://discourse.numenta.org/t/contribution-standards/2130/1)\n- [C++11/14 References](https://discourse.numenta.org/t/c-11-14-references/2136/1)\n- [C++ Unit Tests](https://discourse.numenta.org/t/c-unit-tests/2144)\n\n### Other References\n- [Contributor Model](https://discourse.numenta.org/t/contributor-model/2131/2)\n- [All NuPIC-related Repositories](https://discourse.numenta.org/t/nupic-repositories/2145)\n- [Issue Tracker](https://github.com/numenta/nupic/issues?state=open)\n"
        },
        {
          "name": "DEPENDENCIES.md",
          "type": "blob",
          "size": 3.0341796875,
          "content": "# Dependencies\n\nThis file declares all dependencies NuPIC has on third-party code, including their licenses.\n\n## C++ Dependencies\n\n- dense_hash_map ([License](https://github.com/numenta/nupic/blob/master/external/common/include/google/dense_hash_map))\n- dense_hash_set ([License](https://github.com/numenta/nupic/blob/master/external/common/include/google/dense_hash_set))\n- libjpeg ([License](http://libjpeg.cvs.sourceforge.net/viewvc/libjpeg/libjpeg/README?view=markup&pathrev=MAIN))\n- nupic.core ([GPLv3](https://github.com/numenta/nupic.core/blob/master/LICENSE.txt))\n- png ([License](https://github.com/numenta/nupic/blob/master/external/common/include/libpng12/png.h))\n- sparse_hash_map ([License](https://github.com/numenta/nupic/blob/master/external/common/include/google/sparse_hash_map))\n- sparse_hash_set ([License](https://github.com/numenta/nupic/blob/master/external/common/include/google/sparse_hash_set))\n- sparsetable ([License](https://github.com/numenta/nupic/blob/master/external/common/include/google/sparsetable))\n- SWiG ([Legal Talk](http://www.swig.org/legal.html))\n- type_traits ([License](https://github.com/numenta/nupic/blob/master/external/common/include/google/type_traits.h))\n- zlib ([License](https://github.com/numenta/nupic/blob/master/external/common/include/zlib.h))\n\n## Python Dependencies Through Pip\n\n- asteval ([License](https://github.com/newville/asteval/blob/master/LICENSE))\n- coverage ([BSD License](https://bitbucket.org/ned/coveragepy/src/b98fc53a0724298fe9ab6d530174a1d511aac890/coverage/__init__.py?at=default#cl-96))\n- DBUtils ([OSI 2.1](http://www.webwareforpython.org/DBUtils/Docs/UsersGuide.html#copyright-and-license))\n- mock ([BSD License](http://www.voidspace.org.uk/python/mock/))\n- numpy ([License](http://www.numpy.org/license.html))\n- ordereddict ([MIT License](https://pypi.python.org/pypi/ordereddict))\n- pillow ([License](https://github.com/python-pillow/Pillow/blob/master/LICENSE))\n- prettytable ([License](https://code.google.com/p/prettytable/source/browse/trunk/COPYING))\n- psutil ([BSD License](https://code.google.com/p/psutil/source/browse/LICENSE))\n- pycapnp ([License](https://github.com/jparyani/pycapnp/blob/develop/LICENSE))\n- PyMySQL ([License](https://github.com/PyMySQL/PyMySQL/blob/master/LICENSE))\n- pyproj ([License](https://github.com/jswhit/pyproj/blob/master/LICENSE))\n- pytest ([License](https://bitbucket.org/pytest-dev/pytest/src/45921b2e640011d8f169a7f13fd79218f88c7495/LICENSE?at=default))\n- pytest-cov ([MIT License](https://github.com/schlamar/pytest-cov/blob/2.0/pytest-cov/LICENSE.txt))\n- pytest-xdist ([MIT License](https://bitbucket.org/pytest-dev/pytest-xdist/src/00cfff4834e718fd3c1ccec40811e734d796f631/LICENSE?at=default))\n- python-dateutil ([License](http://bazaar.launchpad.net/~dateutil/dateutil/trunk/view/head:/LICENSE))\n- PyYAML ([License](https://bitbucket.org/xi/pyyaml/src/ddf211a41bb231c365fece5599b7e484e6dc33fc/LICENSE?at=default))\n- unittest2 ([BSD License](https://pypi.python.org/pypi/unittest2))\n- validictory ([License](https://github.com/sunlightlabs/validictory/blob/master/LICENSE.txt))\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.783203125,
          "content": "# Copyright 2016 Numenta Inc.\n#\n# Copyright may exist in Contributors' modifications\n# and/or contributions to the work.\n#\n# Use of this source code is governed by the MIT\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\nFROM ubuntu:14.04\n\n# Install dependencies\nRUN apt-get update && \\\n    apt-get install -y \\\n    curl \\\n    wget \\\n    git-core \\\n    gcc \\\n    g++ \\\n    cmake3 \\\n    python \\\n    python2.7 \\\n    python2.7-dev \\\n    zlib1g-dev \\\n    bzip2 \\\n    libyaml-dev \\\n    libyaml-0-2\nRUN wget http://releases.numenta.org/pip/1ebd3cb7a5a3073058d0c9552ab074bd/get-pip.py -O - | python\nRUN pip install --upgrade setuptools\nRUN pip install wheel\n\n# Use gcc to match nupic.core binary\nENV CC gcc\nENV CXX g++\n\n# Set enviroment variables needed by NuPIC\nENV NUPIC /usr/local/src/nupic\nENV NTA_DATA_PATH /usr/local/src/nupic/prediction/data\n\n# OPF needs this\nENV USER docker\n\n# Set up nupic.core\nRUN pip install numpy pycapnp\nWORKDIR /usr/local/src/nupic.core\n\n# Extract nupic.core version from ${NUPIC}/requirements.txt\nADD requirements.txt ${NUPIC}/requirements.txt\nRUN cat ${NUPIC}/requirements.txt|grep \"^nupic\\.bindings\"|cut -d \"=\"  -f 3 > VERSION\n\n# Download sources from github release\nRUN wget -qO - https://github.com/numenta/nupic.core/archive/$(cat VERSION).tar.gz | tar --strip-components=1 -xzf -\n\n# Build nupic.core and nupic.bindings\nWORKDIR /usr/local/src/nupic.core/build/scripts\nRUN cmake -DCMAKE_BUILD_TYPE=Debug -DCMAKE_INSTALL_PREFIX=../release -DPY_EXTENSIONS_DIR=../../bindings/py/nupic/bindings ../..\nRUN make install\nWORKDIR /usr/local/src/nupic.core\nRUN python setup.py install\n\n# Copy context into container file system\nADD . $NUPIC\n\nWORKDIR /usr/local/src/nupic\n\n# Install NuPIC with using SetupTools\nRUN python setup.py install\n\nWORKDIR /home/docker\n"
        },
        {
          "name": "Dockerfile-complete",
          "type": "blob",
          "size": 0.5625,
          "content": "FROM numenta/nupic\n\nMAINTAINER Allan Costa <allaninocencio@yahoo.com.br>\n\n# Install MySQL. It's the only extra dependency for NuPIC swarm.\nRUN apt-get update && \\\n    DEBIAN_FRONTEND=noninteractive apt-get install -y mysql-server\n\n# Create a startup.sh bash script to start mysql before running any command\nRUN echo \"#!/bin/bash\\nservice mysql start\\nexec \\$*\" >> /home/docker/startup.sh && \\\n    chmod +x /home/docker/startup.sh\n\n# Test the swarm connection to MySQL\nRUN /home/docker/startup.sh python $NUPIC/examples/swarm/test_db.py\n\nENTRYPOINT [\"/home/docker/startup.sh\"]\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.0322265625,
          "content": "Copyright 2013-2024 Numenta Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.25390625,
          "content": "include requirements.txt\ninclude *.md\ninclude *.txt\ninclude VERSION\n\nrecursive-include external/darwin64 *.a\nrecursive-include external/linux64 *.a\n\nrecursive-include src/nupic/datafiles *.csv *.txt\nrecursive-include src/nupic *.capnp *.json *.tpl *.txt *.xml\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 3.6796875,
          "content": "# <img src=\"http://numenta.org/87b23beb8a4b7dea7d88099bfb28d182.svg\" alt=\"NuPIC Logo\" width=100/> NuPIC\n\nAs of September 2023 this repository contains code from legacy Hierarchical Temporal Memory (HTM) Numenta projects that have been in maintenance mode for several years.\n\n## Numenta Platform for Intelligent Computing\n\nThe Numenta Platform for Intelligent Computing (**NuPIC**) is a machine intelligence platform that implements the [HTM learning algorithms](https://numenta.com/resources/papers-videos-and-more/). HTM is a detailed computational theory of the neocortex. At the core of HTM are time-based continuous learning algorithms that store and recall spatial and temporal patterns. NuPIC is suited to a variety of problems, particularly anomaly detection and prediction of streaming data sources. For more information, see [numenta.org](http://numenta.org) or the [NuPIC Forum](https://discourse.numenta.org/c/nupic).\n\nFor usage guides, quick starts, and API documentation, see <http://nupic.docs.numenta.org/>.\n\n## This project is in Maintenance Mode\n\nWe plan to do minor releases only, and limit changes in NuPIC and NuPIC Core to:\n\n- Fixing critical bugs.\n- Features needed to support ongoing research.\n\n## Installing NuPIC\n\nNuPIC binaries are available for:\n\n- Linux x86 64bit\n- OS X 10.9\n- OS X 10.10\n- Windows 64bit\n\n### Dependencies\n\nThe following dependencies are required to install NuPIC on all operating systems.\n\n- [Python 2.7](https://www.python.org/)\n- [pip](https://pip.pypa.io/en/stable/installing/)>=8.1.2\n- [setuptools](https://setuptools.readthedocs.io)>=25.2.0\n- [wheel](http://pythonwheels.com)>=0.29.0\n- [numpy](http://www.numpy.org/)\n- C++ 11 compiler like [gcc](https://gcc.gnu.org/) (4.8+) or [clang](http://clang.llvm.org/)\n\nAdditional OS X requirements:\n\n- [Xcode command line tools](https://developer.apple.com/library/ios/technotes/tn2339/_index.html)\n\n### Install\n\nRun the following to install NuPIC:\n\n    pip install nupic\n\n### Test\n\n    # From the root of the repo:\n    py.test tests/unit\n\n### _Having problems?_\n\n- You may need to use the `--user` flag for the commands above to install in a non-system location (depends on your environment). Alternatively, you can execute the `pip` commands with `sudo` (not recommended).\n- You may need to add the `--use-wheel` option if you have an older pip version (wheels are now the default binary package format for pip).\n\nFor any other installation issues, please see our [search our forums](https://discourse.numenta.org/search?q=tag%3Ainstallation%20category%3A10) (post questions there). You can report bugs at https://github.com/numenta/nupic/issues.\n\nLive Community Chat: [![Gitter](https://img.shields.io/badge/gitter-join_chat-blue.svg?style=flat)](https://gitter.im/numenta/public?utm_source=badge)\n\n### Installing NuPIC From Source\n\nTo install from local source code, run from the repository root:\n\n    pip install .\n\nUse the optional `-e` argument for a developer install.\n\nIf you want to build the dependent `nupic.bindings` from source, you should build and install from [`nupic.core`](https://github.com/numenta/nupic.core) prior to installing nupic (since a PyPI release will be installed if `nupic.bindings` isn't yet installed).\n\n- Build:\n[![Build Status](https://travis-ci.org/numenta/nupic.png?branch=master)](https://travis-ci.org/numenta/nupic)\n[![AppVeyor Status](https://ci.appveyor.com/api/projects/status/4toemh0qtr21mk6b/branch/master?svg=true)](https://ci.appveyor.com/project/numenta-ci/nupic/branch/master)\n[![CircleCI](https://circleci.com/gh/numenta/nupic.svg?style=svg)](https://circleci.com/gh/numenta/nupic)\n- To cite this codebase: [![DOI](https://zenodo.org/badge/19461/numenta/nupic.svg)](https://zenodo.org/badge/latestdoi/19461/numenta/nupic)\n"
        },
        {
          "name": "RELEASE.md",
          "type": "blob",
          "size": 1.1103515625,
          "content": "# Release Process\n\n1. Send announcement that a release is underway to the committer's lounge on\ndiscourse.numenta.org and ask reviewers not to merge PRs in NuPIC until you're\ndone with the release.\n2. Create a PR that includes:\n    - Release notes added to CHANGELOG.md\n    - Change to the VERSION file so it matches the intended release version\n3. Wait for the PR to be approved and merged, and the Bamboo build to complete\nsuccessfully\n4. Create a \"release\" in Bamboo with a version matching the intended release\nversion\n5. Deploy the release in Bamboo. This will:\n    - Validate that the Bamboo release number matches the wheel version\n    - Push the wheel to PyPI\n6. Create a new Github \"Release\" at https://github.com/numenta/nupic/releases/new\n    - Along with the creation of the release, there is an option to create a git tag with the release. Name it \"X.Y.Z\" and point it to the commit SHA for the merged PR described in #2 above.\n    - Release title should be \"X.Y.Z\"\n    - Release description should be the latest changelog\n7. Send announcement to the committer's lounge on discourse.numenta.org that the release is complete."
        },
        {
          "name": "VERSION",
          "type": "blob",
          "size": 0.01171875,
          "content": "1.0.6.dev0\n\n"
        },
        {
          "name": "appveyor.yml",
          "type": "blob",
          "size": 3.9052734375,
          "content": "#---------------------------------#\n#      general configuration      #\n#---------------------------------#\n\n# version format\nversion: 0.1.0.{build}\n\nbranches:\n  except:\n    - gh-pages\n\nskip_commits:\n  # Add [av skip] to commit messages to skip AppVeyor building\n  # Add [ci skip] to skip Travis and AppVeyor building\n  message: /\\[av skip\\]/\n\n#---------------------------------#\n#    environment configuration    #\n#---------------------------------#\n\n# http://www.appveyor.com/docs/environment-variables\n# http://www.appveyor.com/docs/installed-software\n\ninit:\n  - git config --global core.autocrlf input\n\nclone_folder: c:\\projects\\nupic\nclone_depth: 50\n\n# Can't have a shallow clone because the CMake process will be calling into\n# git to write the current SHA into the binaries.\nshallow_clone: false\n\nservices:\n  - mysql\n\nenvironment:\n  PYTHON_ARCH: \"64\"\n  COMPILER_FAMILY: GCC\n  MYSQL_PWD: \"Password12!\"\n  PYTHON_VERSION: \"2.7.9\"\n  PYTHONHOME: \"C:\\\\Python27-x64\"\n  PYTHONPATH: \"C:\\\\Python27-x64\\\\lib\\\\site-packages\"\n  PATH: \"C:\\\\Python27-x64;C:\\\\Python27-x64\\\\Scripts;%PATH%\"\n\n#---------------------------------#\n#       build configuration       #\n#---------------------------------#\n\nbefore_build:\n  - set NUPIC_DEPLOYMENT_BUILD=1\n  - set NUPIC=c:\\projects\\nupic\n\n  - set REPO_DIR=c:\\projects\\nupic\n  - cd %REPO_DIR%\n\n  # Install Python 2.7.x and support packages (pip, wheel, numpy etc.)\n  - ps: .\\ci\\appveyor\\install_python_pip.ps1\n\n  - echo %PATH%\n  - echo %PYTHONPATH%\n\n  - set ARTIFACTS_DIR=%REPO_DIR%\\build\\artifacts\n  - for /f %%i in ('type VERSION') do set NUPIC_VERSION=%%i\n\n  - echo NuPIC version   = %NUPIC_VERSION%\n\n  # Check all packages installed correctly\n  - cmd: pip list\n\nbuild_script:\n  - cd %REPO_DIR%\n\n  - python setup.py bdist_wheel\n\n  - ps: ls -l dist\n  - 7z a nupic-%APPVEYOR_REPO_COMMIT%.zip .\\dist\\nupic-%NUPIC_VERSION%-py2-none-any.whl\n  - ps: ls -l .\n  - ps: move .\\dist\\nupic-$env:NUPIC_VERSION-py2-none-any.whl .\\dist\\nupic-$env:NUPIC_VERSION-cp27-none-win_amd64.whl\n\ntest_script:\n  - cd %REPO_DIR%\n  - pip install .\\dist\\nupic-%NUPIC_VERSION%-cp27-none-win_amd64.whl\n\n  # Show nupic installation folder by trying to import nupic, if works, it prints\n  # the absolute path of nupic.__file__, which the installation folder itself.\n  - python -c \"import sys;import os;import nupic.data;sys.stdout.write(os.path.abspath(os.path.join(nupic.data.__file__, '../..')) + '\\n')\"\n\n  # Setup local USER name\n  - set USER=appveyor\n\n  # Read the nupic-default.XML file and update the passwd entry\n  - ps: $nupic_site = [xml](Get-Content .\\src\\nupic\\support\\nupic-default.xml)\n  - ps: $passwd_property = $nupic_site.configuration.property | where {$_.Name -eq 'nupic.cluster.database.passwd'}\n  - ps: $passwd_property.value = \"Password12!\"\n  # Save the script to the install location\n  # TODO: Can we save it to repo before installation of the package instead?\n  - ps: $nupic_site.Save('C:\\Python27-x64\\lib\\site-packages\\nupic\\support\\nupic-site.xml')\n\n  # Python unit tests\n  - py.test tests\\unit\n\n  # Python integration tests\n  - py.test tests\\integration\n\nartifacts:\n  - path: 'dist\\*.whl' # Find all wheel files in project root only (non-recusive)\n  - path: nupic-$(APPVEYOR_REPO_COMMIT).zip\n    name: appveyor\n\ndeploy:\n  # Amazon S3 deployment provider settings\n  - provider: S3\n    access_key_id: AKIAIGHYSEHV3WFKOWNQ\n    secret_access_key:\n      secure: /8wO17Gir0XAiecJkHeE3jxOJzvyl0+uWcl7BKCuN0FC795golsL8905VmNuRl1o\n    bucket: \"artifacts.numenta.org\"\n    region: us-west-2\n    set_public: true\n    artifact: \"nupic-$(NUPIC_VERSION)-cp27-none-win_amd64.whl\"\n    folder: \"numenta/nupic/$(APPVEYOR_REPO_COMMIT)\"\n    on:\n      branch: master\n  - provider: S3\n    access_key_id: AKIAIGHYSEHV3WFKOWNQ\n    secret_access_key:\n      secure: /8wO17Gir0XAiecJkHeE3jxOJzvyl0+uWcl7BKCuN0FC795golsL8905VmNuRl1o\n    bucket: \"artifacts.numenta.org\"\n    region: us-west-2\n    set_public: true\n    artifact: appveyor\n    folder: \"numenta/nupic/appveyor\"\n    on:\n      branch: master\n"
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "coreos-vagrant",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "external",
          "type": "tree",
          "content": null
        },
        {
          "name": "pylintrc",
          "type": "blob",
          "size": 8.9150390625,
          "content": "# Copyright 2012-2015 Numenta Inc.\n#\n# Copyright may exist in Contributors' modifications\n# and/or contributions to the work.\n#\n# Use of this source code is governed by the MIT\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n#\n# Numenta pylint configuration.\n#\n# TODO:\n#   File layout ([#! line], copyright, module docstring, imports in correct order,\n#       everything else, and then __name__ == '__main__').\n#   Space after opening \"#\" in comments.\n\n[MASTER]\n\n# Specify a configuration file.\n#rcfile=\n\n# Python code to execute, usually for sys.path manipulation such as\n# pygtk.require().\n#init-hook=\n\n# Profiled execution.\nprofile=no\n\n# Add files or directories to the blacklist. They should be base names, not\n# paths.\nignore=\n\n# Pickle collected data for later comparisons.\npersistent=no\n\n# List of plugins (as comma separated values of python modules names) to load,\n# usually to register additional checkers.\nload-plugins=\n\n\n[MESSAGES CONTROL]\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time.\n#enable=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once).\n#\n# TODO: Remove this check if possible.\n# E1101 - Enforces that all class attribute references are defined in the class\n#     which is not true for classes that have dynamic attributes.\n# E1103 - Checks that object types has accessed member attribute or method.\n#     Sometimes fails to correctly infer the type of an object.\n# W0142 - Using * or ** magic.\n# W0105 - String statement has no effect. Disabled to allow for doc strings.\n# W0231 - __init__ method from base class is not called.\n# W0621 - Redefining variable from outer scope.\n# C0111 - Missing docstring.\n# C0302 - Too many lines in module\n# C0322 - No space before operator (=, %, *, etc).\n# R0902 - Too many instance attributes\n# R0904 - Too many public methods.\n# R0911 - Too many return statements\n# R0912 - Too many branches\ndisable=E1101,E1103,W0142,W0105,W0231,W0621,C0111,C0302,C0322,R0902,R0904,R0911,R0912\n\n\n[REPORTS]\n\n# Set the output format. Available formats are text, parseable, colorized, msvs\n# (visual studio) and html\noutput-format=text\n\n# Include message's id in output\ninclude-ids=yes\n\n# Put messages in a separate file for each module / package specified on the\n# command line instead of printing them on stdout. Reports (if any) will be\n# written in a file name \"pylint_global.[txt|html]\".\nfiles-output=no\n\n# Tells whether to display a full report or only the messages\nreports=no\n\n# Python expression which should return a note less than 10 (10 is the highest\n# note). You have access to the variables errors warning, statement which\n# respectively contain the number of errors / warnings messages and the total\n# number of statements analyzed. This is used by the global evaluation report\n# (RP0004).\nevaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)\n\n# Add a comment according to your evaluation note. This is used by the global\n# evaluation report (RP0004).\ncomment=no\n\n# Template used to display messages. This is a python new-style format string\n# used to format the message information. See doc for all details\nmsg-template={msg_id}, {line:3d}:{column:2d} - {msg} ({symbol})\n\n\n[BASIC]\n\n# Required attributes for module, separated by a comma\nrequired-attributes=\n\n# List of builtins function names that should not be used, separated by a comma\nbad-functions=map,filter,apply,input\n\n# Regular expression which should only match correct module names\nmodule-rgx=([a-z][a-z0-9_]*[a-z0-9])|(__init__)$\n\n# Regular expression which should only match correct module level names\n#          Globals              | C/camelCase  |Constants         |Magic\nconst-rgx=((g_[a-z][a-zA-Z0-9]*)|([A-Za-z0-9]*)|([A-Z_][A-Z0-9_]*)|(__.*__))$\n\n# Regular expression which should only match correct class names\nclass-rgx=[A-Z_][a-zA-Z0-9]+$\n\n# Regular expression which should only match correct function names\nfunction-rgx=_{0,2}[a-z][A-Za-z0-9]*$\n\n# Regular expression which should only match correct method names\nmethod-rgx=(_{0,2}[a-z][A-Za-z0-9]*)|(__.*__)$\n\n# Regular expression which should only match correct instance attribute names\nattr-rgx=_{0,2}[a-z][A-Za-z0-9]*$\n\n# Regular expression which should only match correct argument names\nargument-rgx=_?[a-z][A-Za-z0-9]*$\n\n# Regular expression which should only match correct variable names\nvariable-rgx=_{0,2}[a-z][A-Za-z0-9]*$\n\n# Regular expression which should only match correct attribute names in class\n# bodies\nclass-attribute-rgx=([A-Za-z_][A-Za-z0-9_]{2,30}|(__.*__))$\n\n# Regular expression which should only match correct list comprehension /\n# generator expression variable names\ninlinevar-rgx=[a-z][A-Za-z0-9_]*$\n\n# Good variable names which should always be accepted, separated by a comma\ngood-names=_\n\n# Bad variable names which should always be refused, separated by a comma\nbad-names=\n\n# Regular expression which should only match functions or classes name which do\n# not require a docstring\nno-docstring-rgx=__.*__\n\n\n[FORMAT]\n\n# Maximum number of characters on a single line.\nmax-line-length=80\n\n# Maximum number of lines in a module\n#max-module-lines=1000\n\n# String used as indentation unit (2 spaces).\nindent-string='  '\n\n# Number of spaces of indent required inside a hanging or continued line.\nindent-after-paren=2\n\n\n[MISCELLANEOUS]\n\n# List of note tags to take in consideration, separated by a comma.\nnotes=TODO,FIXME\n\n\n[SIMILARITIES]\n\n# Minimum lines number of a similarity.\nmin-similarity-lines=4\n\n# Ignore comments when computing similarities.\nignore-comments=yes\n\n# Ignore docstrings when computing similarities.\nignore-docstrings=yes\n\n\n[TYPECHECK]\n\n# Tells whether missing members accessed in mixin class should be ignored. A\n# mixin class is detected if its name ends with \"mixin\" (case insensitive).\nignore-mixin-members=yes\n\n# List of classes names for which member attributes should not be checked\n# (useful for classes with attributes dynamically set).\nignored-classes=SQLObject\n\n# When zope mode is activated, add a predefined set of Zope acquired attributes\n# to generated-members.\nzope=no\n\n# List of members which are set dynamically and missed by pylint inference\n# system, and so shouldn't trigger E0201 when accessed. Python regular\n# expressions are accepted.\ngenerated-members=REQUEST,acl_users,aq_parent\n\n\n[VARIABLES]\n\n# Tells whether we should check for unused import in __init__ files.\ninit-import=no\n\n# A regular expression matching the beginning of the name of dummy variables\n# (i.e. not used).\ndummy-variables-rgx=_|dummy\n\n# List of additional names supposed to be defined in builtins. Remember that\n# you should avoid to define new builtins when possible.\nadditional-builtins=\n\n\n[CLASSES]\n\n# List of interface methods to ignore, separated by a comma. This is used for\n# instance to not check methods defines in Zope's Interface base class.\nignore-iface-methods=isImplementedBy,deferred,extends,names,namesAndDescriptions,queryDescriptionFor,getBases,getDescriptionFor,getDoc,getName,getTaggedValue,getTaggedValueTags,isEqualOrExtendedBy,setTaggedValue,isImplementedByInstancesOf,adaptWith,is_implemented_by\n\n# List of method names used to declare (i.e. assign) instance attributes.\ndefining-attr-methods=__init__,__new__,setUp\n\n# List of valid names for the first argument in a class method.\nvalid-classmethod-first-arg=cls\n\n\n[DESIGN]\n\n# Maximum number of arguments for function / method\nmax-args=20\n\n# Argument names that match this expression will be ignored. Default to name\n# with leading underscore\nignored-argument-names=_.*\n\n# Maximum number of locals for function / method body\nmax-locals=50\n\n# Maximum number of return / yield for function / method body\nmax-returns=10\n\n# Maximum number of branch for function / method body\nmax-branches=20\n\n# Maximum number of statements in function / method body\nmax-statements=200\n\n# Maximum number of parents for a class (see R0901).\nmax-parents=7\n\n# Maximum number of attributes for a class (see R0902).\nmax-attributes=20\n\n# Minimum number of public methods for a class (see R0903).\nmin-public-methods=0\n\n# Maximum number of public methods for a class (see R0904).\nmax-public-methods=20\n\n\n[IMPORTS]\n\n# Deprecated modules which should not be used, separated by a comma\ndeprecated-modules=regsub,TERMIOS,Bastion,rexec\n\n# Create a graph of every (i.e. internal and external) dependencies in the\n# given file (report RP0402 must not be disabled)\nimport-graph=\n\n# Create a graph of external dependencies in the given file (report RP0402 must\n# not be disabled)\next-import-graph=\n\n# Create a graph of internal dependencies in the given file (report RP0402 must\n# not be disabled)\nint-import-graph=\n\n\n[EXCEPTIONS]\n\n# Exceptions that will emit a warning when being caught. Defaults to\n# \"Exception\"\novergeneral-exceptions=Exception\n"
        },
        {
          "name": "requirements-dev.txt",
          "type": "blob",
          "size": 0.185546875,
          "content": "# for documentation\nsphinx==1.5.3\nsphinx-autobuild==0.6.0\n# version lock 'sphinx-autobuild' dependency 'tornado'\ntornado==3.2\nrecommonmark==0.4.0\n# for network visualization\nnetworkx==1.11\n\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.4541015625,
          "content": "#See http://www.pip-installer.org/en/latest/requirements.html for details\nasteval==0.9.1\ncoverage==3.7.1\nmock==1.0.1\nordereddict==1.1\npsutil==1.0.1\npytest==3.0.7\npytest-cov==2.5.0\npytest-xdist==1.16.0\npython-dateutil==2.1\nPyYAML==3.10\nunittest2==0.5.1\nvalidictory==0.9.1\nPyMySQL==0.6.2\nDBUtils==1.1\npyproj==1.9.3\nprettytable==0.7.2\n\n# When updating nupic.bindings, also update any shared dependencies to keep\n# versions in sync.\nnupic.bindings==1.0.6\nnumpy==1.12.1\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.0224609375,
          "content": "[build_ext]\ninplace=1\n\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 5.2265625,
          "content": "# Copyright 2016 Numenta Inc.\n#\n# Copyright may exist in Contributors' modifications\n# and/or contributions to the work.\n#\n# Use of this source code is governed by the MIT\n# license that can be found in the LICENSE file or at\n# https://opensource.org/licenses/MIT.\n\n\"\"\"Installation script for Python nupic package.\"\"\"\n\nimport os\nimport pkg_resources\nimport sys\n\nfrom setuptools import setup, find_packages, Extension\nfrom setuptools.command.test import test as BaseTestCommand\n\n\n\nREPO_DIR = os.path.dirname(os.path.realpath(__file__))\n\n\n\ndef getVersion():\n  \"\"\"\n  Get version from local file.\n  \"\"\"\n  with open(os.path.join(REPO_DIR, \"VERSION\"), \"r\") as versionFile:\n    return versionFile.read().strip()\n\n\n\ndef nupicBindingsPrereleaseInstalled():\n  \"\"\"\n  Make an attempt to determine if a pre-release version of nupic.bindings is\n  installed already.\n\n  @return: boolean\n  \"\"\"\n  try:\n    nupicDistribution = pkg_resources.get_distribution(\"nupic.bindings\")\n    if pkg_resources.parse_version(nupicDistribution.version).is_prerelease:\n      # A pre-release dev version of nupic.bindings is installed.\n      return True\n  except pkg_resources.DistributionNotFound:\n    pass  # Silently ignore.  The absence of nupic.bindings will be handled by\n    # setuptools by default\n\n  return False\n\n\n\ndef parse_file(requirementFile):\n  try:\n    return [\n      line.strip()\n      for line in open(requirementFile).readlines()\n      if not line.startswith(\"#\")\n    ]\n  except IOError:\n    return []\n\n\n\nclass TestCommand(BaseTestCommand):\n  user_options = [(\"pytest-args=\", \"a\", \"Arguments to pass to py.test\")]\n\n\n  def initialize_options(self):\n    BaseTestCommand.initialize_options(self)\n    self.pytest_args = [\"unit\"] # pylint: disable=W0201\n\n\n  def finalize_options(self):\n    BaseTestCommand.finalize_options(self)\n    self.test_args = []\n    self.test_suite = True\n\n\n  def run_tests(self):\n    import pytest\n    cwd = os.getcwd()\n    try:\n      os.chdir(\"tests\")\n      errno = pytest.main(self.pytest_args)\n    finally:\n      os.chdir(cwd)\n    sys.exit(errno)\n\n\n\ndef findRequirements():\n  \"\"\"\n  Read the requirements.txt file and parse into requirements for setup's\n  install_requirements option.\n  \"\"\"\n  requirementsPath = os.path.join(REPO_DIR, \"requirements.txt\")\n  requirements = parse_file(requirementsPath)\n\n  if nupicBindingsPrereleaseInstalled():\n    # User has a pre-release version of nupic.bindings installed, which is only\n    # possible if the user installed and built nupic.bindings from source and\n    # it is up to the user to decide when to update nupic.bindings.  We'll\n    # quietly remove the entry in requirements.txt so as to not conflate the\n    # two.\n    requirements = [req for req in requirements if \"nupic.bindings\" not in req]\n\n  return requirements\n\n\n\nif __name__ == \"__main__\":\n  requirements = findRequirements()\n\n  setup(\n    name=\"nupic\",\n    version=getVersion(),\n    install_requires=requirements,\n    package_dir = {\"\": \"src\"},\n    packages=find_packages(\"src\"),\n    namespace_packages = [\"nupic\"],\n    package_data={\n      \"nupic.support\": [\"nupic-default.xml\",\n                        \"nupic-logging.conf\"],\n      \"nupic\": [\"README.md\", \"LICENSE.txt\"],\n      \"nupic.data\": [\"*.json\"],\n      \"nupic.frameworks.opf.exp_generator\": [\"*.json\", \"*.tpl\"],\n      \"nupic.frameworks.opf.jsonschema\": [\"*.json\"],\n      \"nupic.swarming.exp_generator\": [\"*.json\", \"*.tpl\"],\n      \"nupic.swarming.jsonschema\": [\"*.json\"],\n      \"nupic.datafiles\": [\"*.csv\", \"*.txt\"],\n    },\n    cmdclass = {\"test\": TestCommand},\n    include_package_data=True,\n    zip_safe=False,\n    extras_require = {\n      # Default requirement based on system type\n      \":platform_system=='Linux' or platform_system=='Darwin'\":\n        [\"pycapnp==0.6.3\"],\n\n      # Superseded by platform_system-conditional requirement, but keeping\n      # empty extra for compatibility as recommended by setuptools doc.\n      \"capnp\": [],\n      \"viz\": [\"networkx\", \"matplotlib\", \"pygraphviz\"]\n    },\n    description=\"Numenta Platform for Intelligent Computing\",\n    author=\"Numenta\",\n    author_email=\"help@numenta.org\",\n    url=\"https://github.com/numenta/nupic\",\n    classifiers=[\n      \"Programming Language :: Python\",\n      \"Programming Language :: Python :: 2\",\n      \"License :: OSI Approved :: MIT License\",\n      \"Operating System :: MacOS :: MacOS X\",\n      \"Operating System :: POSIX :: Linux\",\n      \"Operating System :: Microsoft :: Windows\",\n      # It has to be \"5 - Production/Stable\" or else pypi rejects it!\n      \"Development Status :: 5 - Production/Stable\",\n      \"Environment :: Console\",\n      \"Intended Audience :: Science/Research\",\n      \"Topic :: Scientific/Engineering :: Artificial Intelligence\"\n    ],\n    long_description=(\n        \"Numenta Platform for Intelligent Computing: a machine intelligence \"\n        \"platform that implements the HTM learning algorithms. HTM is a \"\n        \"detailed computational theory of the neocortex. At the core of HTM \"\n        \"are time-based continuous learning algorithms that store and recall \"\n        \"spatial and temporal patterns. NuPIC is suited to a variety of \"\n        \"problems, particularly anomaly detection and prediction of streaming \"\n        \"data sources.\\n\\n\"\n        \"For more information, see http://numenta.org or the NuPIC wiki at \"\n        \"https://github.com/numenta/nupic/wiki.\")\n  )\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}