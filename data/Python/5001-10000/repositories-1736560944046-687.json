{
  "metadata": {
    "timestamp": 1736560944046,
    "page": 687,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjY5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NexaAI/nexa-sdk",
      "stars": 5577,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.0126953125,
          "content": "# Files\n*.safetensors\n*.gguf\n*.bin\n*.mp3\n*.wav\n*.png\npoetry.lock\n*.pdb\n*.ipynb\nnexa_server_output/\ntranscriptions/\ngenerated/\nbuild*.sh\n*.so\n*.dll\n*.dylib\n*.a\ngenerated_images/\n\n# Python\n__pycache__/\n**/__pycache__/\nCMakeFiles/\n*.py[cod]\n*$py.class\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\nvenv/\n.env\n\n# C++\nMakefile\nCMakeCache.txt\ncmake_install.cmake\ncompile_commands.json\n*.o\n*.out\n*.exe\n*.a\n*.lib\ndist/\n\n# IDEs and editors\n.idea/\n.vscode/\n*.swp\n*.swo\n*~\n\n# OS generated files\n.DS_Store\n.DS_Store?\n._*\n.Spotlight-V100\n.Trashes\nehthumbs.db\nThumbs.db\n\n# Build directories\nbin/\nobj/\nbuild_*/\n\n# Exclude build_scripts/ from being ignored\n!build_scripts/\n\n# Logs and databases\n*.log\n*.sqlite\n\n# Other\n.cache/\n\n# tests\nquantization_test.py\n\n# Swift\n.swiftpm/\nUserInterfaceState.xcuserstate\nxcuserdata/\n*.xcworkspace/xcuserdata/\n*.playground/playground.xcworkspace/xcuserdata/\n*.generated.plist\n.build/"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.5380859375,
          "content": "[submodule \"dependency/stable-diffusion.cpp\"]\n\tpath = dependency/stable-diffusion.cpp\n\turl = https://github.com/NexaAI/stable-diffusion.cpp\n\tbranch = master\n[submodule \"dependency/llama.cpp\"]\n\tpath = dependency/llama.cpp\n\turl = https://github.com/NexaAI/llama.cpp.git\n\tbranch = release\n[submodule \"nexa/eval/benchmark_tasks\"]\n\tpath = nexa/eval/benchmark_tasks\n\turl = https://github.com/NexaAI/benchmark-tasks.git\n\tbranch = main\n[submodule \"dependency/bark.cpp\"]\n\tpath = dependency/bark.cpp\n\turl = https://github.com/NexaAI/bark.cpp.git\n\tbranch = main\n"
        },
        {
          "name": "CLI.md",
          "type": "blob",
          "size": 14.328125,
          "content": "## CLI Reference\n\n### Overview\n\n```\nusage: nexa [-h] [-V] {run,onnx,server,pull,remove,clean,list,login,whoami,logout} ...\n\nNexa CLI tool for handling various model operations.\n\npositional arguments:\n  {run,onnx,server,pull,remove,clean,list,login,whoami,logout}\n                        sub-command help\n    run                 Run inference for various tasks using GGUF models.\n    onnx                Run inference for various tasks using ONNX models.\n    embed               Generate embeddings for text.\n    convert             Convert and quantize a Hugging Face model to GGUF format.\n    server              Run the Nexa AI Text Generation Service.\n    eval                Run the Nexa AI Evaluation Tasks.\n    pull                Pull a model from official or hub.\n    remove              Remove a model from local machine.\n    clean               Clean up all model files.\n    list                List all models in the local machine.\n    login               Login to Nexa API.\n    whoami              Show current user information.\n    logout              Logout from Nexa API.\n\noptions:\n  -h, --help            show this help message and exit\n  -V, --version         Show the version of the Nexa SDK.\n```\n\n### List Local Models\n\nList all models on your local computer. You can use `nexa run <model_name>` to run any model shown in the list.\n\n```\nnexa list\n```\n\n### Download a Model\n\nDownload a model file to your local computer from Nexa Model Hub.\n\n```\nnexa pull MODEL_PATH\nusage: nexa pull [-h] model_path\n\npositional arguments:\n  model_path  Path or identifier for the model in Nexa Model Hub, Hugging Face repo ID when using -hf flag, or ModelScope model ID when using -ms flag\n\noptions:\n  -h, --help            show this help message and exit\n  -hf, --huggingface    Pull model from Hugging Face Hub\n  -ms, --modelscope     Pull model from ModelScope Hub\n  -o, --output_path OUTPUT_PATH\n                        Custom output path for the pulled model\n```\n\n#### Example\n\n```\nnexa pull llama2\n```\n\n### Remove a Model\n\nRemove a model from your local computer.\n\n```\nnexa remove MODEL_PATH\nusage: nexa remove [-h] model_path\n\npositional arguments:\n  model_path  Path or identifier for the model in Nexa Model Hub\n\noptions:\n  -h, --help  show this help message and exit\n```\n\n#### Example\n\n```\nnexa remove llama2\n```\n\n### Remove All Downloaded Models\n\nRemove all downloaded models on your local computer.\n\n```\nnexa clean\n```\n\n### Run a Model\n\nRun a model on your local computer. If the model file is not yet downloaded, it will be automatically fetched first.\n\nBy default, `nexa` will run gguf models. To run onnx models, use `nexa onnx MODEL_PATH`\n\nYou can run any model shown in `nexa list` command.\n\n#### Run Text-Generation Model\n\n```\nnexa run MODEL_PATH\nusage: nexa run [-h] [-t TEMPERATURE] [-m MAX_NEW_TOKENS] [-k TOP_K] [-p TOP_P] [-sw [STOP_WORDS ...]] [-pf] [-st] [-lp] [-mt {NLP, COMPUTER_VISION, MULTIMODAL, AUDIO}] [-hf] [-ms] model_path\n\npositional arguments:\n  model_path            Path or identifier for the model in Nexa Model Hub\n\noptions:\n  -h, --help            show this help message and exit\n  -pf, --profiling      Enable profiling logs for the inference process\n  -st, --streamlit      Run the inference in Streamlit UI, can be used with -lp or -hf\n  -lp, --local_path     Indicate that the model path provided is the local path\n  -mt, --model_type     Indicate the model running type, must be used with -lp or -hf or -ms, choose from [NLP, COMPUTER_VISION, MULTIMODAL, AUDIO]\n  -hf, --huggingface    Load model from Hugging Face Hub\n  -ms, --modelscope     Load model from ModelScope Hub\n\nText generation options:\n  -t, --temperature TEMPERATURE\n                        Temperature for sampling\n  -m, --max_new_tokens MAX_NEW_TOKENS\n                        Maximum number of new tokens to generate\n  -k, --top_k TOP_K     Top-k sampling parameter\n  -p, --top_p TOP_P     Top-p sampling parameter\n  -sw, --stop_words [STOP_WORDS ...]\n                        List of stop words for early stopping\n  --nctx TEXT_CONTEXT   Length of context window\n```\n\n##### Example\n\n```\nnexa run llama2\n```\n\n#### Run Image-Generation Model\n\n```\nnexa run MODEL_PATH\nusage: nexa run [-h] [-i2i] [-ns NUM_INFERENCE_STEPS] [-np NUM_IMAGES_PER_PROMPT] [-H HEIGHT] [-W WIDTH] [-g GUIDANCE_SCALE] [-o OUTPUT] [-s RANDOM_SEED] [-st] [-lp] [-mt {NLP, COMPUTER_VISION, MULTIMODAL, AUDIO}] [-hf] [-ms] model_path\n\npositional arguments:\n  model_path            Path or identifier for the model in Nexa Model Hub\n\noptions:\n  -h, --help            show this help message and exit\n  -st, --streamlit      Run the inference in Streamlit UI, can be used with -lp or -hf\n  -lp, --local_path     Indicate that the model path provided is the local path\n  -mt, --model_type     Indicate the model running type, must be used with -lp or -hf or -ms, choose from [NLP, COMPUTER_VISION, MULTIMODAL, AUDIO]\n  -hf, --huggingface    Load model from Hugging Face Hub\n  -ms, --modelscope     Load model from ModelScope Hub\n\nImage generation options:\n  -i2i, --img2img       Whether to run image-to-image generation\n  -ns, --num_inference_steps NUM_INFERENCE_STEPS\n                        Number of inference steps\n  -np, --num_images_per_prompt NUM_IMAGES_PER_PROMPT\n                        Number of images to generate per prompt\n  -H, --height HEIGHT   Height of the output image\n  -W, --width WIDTH     Width of the output image\n  -g, --guidance_scale GUIDANCE_SCALE\n                        Guidance scale for diffusion\n  -o, --output OUTPUT   Output path for the generated image\n  -s, --random_seed RANDOM_SEED\n                        Random seed for image generation\n  --lora_dir LORA_DIR   Path to directory containing LoRA files\n  --wtype WTYPE         Weight type (f32, f16, q4_0, q4_1, q5_0, q5_1, q8_0)\n  --control_net_path CONTROL_NET_PATH\n                        Path to the control net model\n  --control_image_path CONTROL_IMAGE_PATH\n                        Path to the image condition for Control Net\n  --control_strength CONTROL_STRENGTH\n                        Strength to apply Control Net\n```\n\n##### Example\n\n```\nnexa run sd1-4\n```\n\n#### Run Vision-Language Model\n\n```\nnexa run MODEL_PATH\nusage: nexa run [-h] [-t TEMPERATURE] [-m MAX_NEW_TOKENS] [-k TOP_K] [-p TOP_P] [-sw [STOP_WORDS ...]] [-pf] [-st] [-lp] [-mt {NLP, COMPUTER_VISION, MULTIMODAL, AUDIO}] [-hf] [-ms] model_path\n\npositional arguments:\n  model_path            Path or identifier for the model in Nexa Model Hub\n\noptions:\n  -h, --help            show this help message and exit\n  -pf, --profiling      Enable profiling logs for the inference process\n  -st, --streamlit      Run the inference in Streamlit UI, can be used with -lp or -hf\n  -lp, --local_path     Indicate that the model path provided is the local path\n  -mt, --model_type     Indicate the model running type, must be used with -lp or -hf or -ms, choose from [NLP, COMPUTER_VISION, MULTIMODAL, AUDIO]\n  -hf, --huggingface    Load model from Hugging Face Hub\n  -ms, --modelscope     Load model from ModelScope Hub\n\nVLM generation options:\n  -t, --temperature TEMPERATURE\n                        Temperature for sampling\n  -m, --max_new_tokens MAX_NEW_TOKENS\n                        Maximum number of new tokens to generate\n  -k, --top_k TOP_K     Top-k sampling parameter\n  -p, --top_p TOP_P     Top-p sampling parameter\n  -sw, --stop_words [STOP_WORDS ...]\n                        List of stop words for early stopping\n  --nctx TEXT_CONTEXT   Length of context window\n```\n\n##### Example\n\n```\nnexa run nanollava\n```\n\n#### Run Audio Model\n\n```\nnexa run MODEL_PATH\nusage: nexa run [-h] [-o OUTPUT_DIR] [-b BEAM_SIZE] [-l LANGUAGE] [--task TASK] [-t TEMPERATURE] [-c COMPUTE_TYPE] [-st] [-lp] [-mt {NLP, COMPUTER_VISION, MULTIMODAL, AUDIO}] [-hf] [-ms] model_path\n\npositional arguments:\n  model_path            Path or identifier for the model in Nexa Model Hub\n\noptions:\n  -h, --help            show this help message and exit\n  -st, --streamlit      Run the inference in Streamlit UI, can be used with -lp or -hf\n  -lp, --local_path     Indicate that the model path provided is the local path\n  -mt, --model_type     Indicate the model running type, must be used with -lp or -hf or -ms, choose from [NLP, COMPUTER_VISION, MULTIMODAL, AUDIO]\n  -hf, --huggingface    Load model from Hugging Face Hub\n  -ms, --modelscope     Load model from ModelScope Hub\n\nAutomatic Speech Recognition options:\n  -b, --beam_size BEAM_SIZE\n                        Beam size to use for transcription\n  -l, --language LANGUAGE\n                        The language spoken in the audio. It should be a language code such as 'en' or 'fr'.\n  --task TASK           Task to execute (transcribe or translate)\n  -c, --compute_type COMPUTE_TYPE\n                        Type to use for computation (e.g., float16, int8, int8_float16)\n```\n\n##### Example\n\n```\nnexa run faster-whisper-tiny\n```\n\n### Generate Embeddings\n\n#### Generate Text Embeddings\n\n```\nnexa embed MODEL_PATH\nusage: nexa embed [-h] [-lp] [-hf] [-ms] [-n] [-nt] model_path prompt\n\npositional arguments:\n  model_path            Path or identifier for the model in Nexa Model Hub\n  prompt                Prompt to generate embeddings\n\noptions:\n  -h, --help            show this help message and exit\n  -lp, --local_path     Indicate that the model path provided is the local path\n  -hf, --huggingface    Load model from Hugging Face Hub\n  -ms, --modelscope     Load model from ModelScope Hub\n  -n, --normalize       Normalize the embeddings\n  -nt, --no_truncate    Not truncate the embeddings\n```\n\n#### Example\n\n```\nnexa embed mxbai \"I love Nexa AI.\"\nnexa embed nomic \"I love Nexa AI.\" >> generated_embeddings.txt\nnexa embed nomic-embed-text-v1.5:fp16 \"I love Nexa AI.\"\nnexa embed sentence-transformers/all-MiniLM-L6-v2:gguf-fp16 \"I love Nexa AI.\" >> generated_embeddings.txt\n```\n\n### Convert and quantize a Hugging Face Model to GGUF\n\nAdditional package `nexa-gguf` is required to run this command.\n\nYou can install it by `pip install \"nexaai[convert]\"` or `pip install nexa-gguf`.\n\n```\nnexa convert HF_MODEL_PATH [ftype] [output_file]\nusage: nexa convert [-h] [-t NTHREAD] [--convert_type CONVERT_TYPE] [--bigendian] [--use_temp_file] [--no_lazy]\n                    [--metadata METADATA] [--split_max_tensors SPLIT_MAX_TENSORS] [--split_max_size SPLIT_MAX_SIZE]\n                    [--no_tensor_first_split] [--vocab_only] [--dry_run] [--output_tensor_type OUTPUT_TENSOR_TYPE]\n                    [--token_embedding_type TOKEN_EMBEDDING_TYPE] [--allow_requantize] [--quantize_output_tensor]\n                    [--only_copy] [--pure] [--keep_split] input_path [ftype] [output_file]\n\npositional arguments:\n  input_path            Path to the input Hugging Face model directory or GGUF file\n  ftype                 Quantization type (default: q4_0)\n  output_file           Path to the output quantized GGUF file\n\noptions:\n  -h, --help            show this help message and exit\n  -t, --nthread NTHREAD Number of threads to use (default: 4)\n  --convert_type CONVERT_TYPE\n                        Conversion type for safetensors to GGUF (default: f16)\n  --bigendian           Use big endian format\n  --use_temp_file       Use a temporary file during conversion\n  --no_lazy             Disable lazy loading\n  --metadata METADATA   Additional metadata as JSON string\n  --split_max_tensors SPLIT_MAX_TENSORS\n                        Maximum number of tensors per split\n  --split_max_size SPLIT_MAX_SIZE\n  --no_tensor_first_split\n                        Disable tensor-first splitting\n  --vocab_only          Only process vocabulary\n  --dry_run             Perform a dry run without actual conversion\n  --output_tensor_type  Output tensor type\n  --token_embedding_type\n                        Token embedding type\n  --allow_requantize    Allow quantizing non-f32/f16 tensors\n  --quantize_output_tensor\n                        Quantize output.weight\n  --only_copy           Only copy tensors (ignores ftype, allow_requantize, and quantize_output_tensor)\n  --pure                Quantize all tensors to the default type\n  --keep_split          Quantize to the same number of shards\n  -ms --modelscope      Load model from ModelScope Hub\n```\n\n#### Example\n\n```\n# Default quantization type (q4_0) and output file in current directory\nnexa convert meta-llama/Llama-3.2-1B-Instruct\n\n# Equivalent to:\n# nexa convert meta-llama/Llama-3.2-1B-Instruct q4_0 ./Llama-3.2-1B-Instruct-q4_0.gguf\n\n# Specifying quantization type and output file\nnexa convert meta-llama/Llama-3.2-1B-Instruct q6_k llama3.2-1b-instruct-q6_k.gguf\n```\n\nNote: When not specified, the default quantization type is set to q4_0, and the output file will be created in the current directory with the name format: `<model_name>-q4_0.gguf`.\n\n### Start Local Server\n\nStart a local server using models on your local computer.\n\n```\nnexa server MODEL_PATH\nusage: nexa server [-h] [--host HOST] [--port PORT] [--reload] [-lp] [-mt {NLP, COMPUTER_VISION, MULTIMODAL, AUDIO}] [-hf] [-ms] model_path\n\npositional arguments:\n  model_path   Path or identifier for the model in S3\n\noptions:\n  -h, --help   show this help message and exit\n  -lp, --local_path     Indicate that the model path provided is the local path\n  -mt, --model_type     Indicate the model running type, must be used with -lp or -hf or -ms, choose from [NLP, COMPUTER_VISION, MULTIMODAL, AUDIO]\n  -hf, --huggingface    Load model from Hugging Face Hub\n  -ms, --modelscope     Load model from ModelScope Hub\n  --host HOST  Host to bind the server to\n  --port PORT  Port to bind the server to\n  --reload     Enable automatic reloading on code changes\n```\n\n#### Example\n\n```\nnexa server llama2\n```\n\n### Run Model Evaluation\n\nRun evaluation using models on your local computer.\n\n```\nusage: nexa eval model_path [-h] [--tasks TASKS] [--limit LIMIT]\n\npositional arguments:\n  model_path            Path or identifier for the model in Nexa Model Hub\n\noptions:\n  -h, --help            show this help message and exit\n  --tasks TASKS         Tasks to evaluate, comma-separated\n  --limit LIMIT         Limit the number of examples per task. If <1, limit is a percentage of the total number of examples.\n```\n\n#### Examples\n\n```\nnexa eval phi3 --tasks ifeval --limit 0.5\n```\n\n### Model Path Format\n\nFor `model_path` in nexa commands, it's better to follow the standard format to ensure correct model loading and execution. The standard format for `model_path` is:\n\n- `[user_name]/[repo_name]:[tag_name]` (user's model)\n- `[repo_name]:[tag_name]` (official model)\n\n#### Examples:\n\n- `gemma-2b:q4_0`\n- `Meta-Llama-3-8B-Instruct:onnx-cpu-int8`\n- `liuhaotian/llava-v1.6-vicuna-7b:gguf-q4_0`\n\n```\n</rewritten_chunk>\n```\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 9.462890625,
          "content": "cmake_minimum_required(VERSION 3.16)\nproject(nexa_gguf)\n\ninclude(ExternalProject)\n\n# Platform-specific settings\nif(WIN32)\n    # Windows-specific settings\n    add_definitions(-D_CRT_SECURE_NO_WARNINGS)\n    # OpenMP is optional on Windows\n    find_package(OpenMP QUIET)\n    if(NOT OpenMP_FOUND)\n        message(STATUS \"OpenMP not found - OpenMP support will be disabled\")\n        set(OpenMP_C_FLAGS \"\")\n        set(OpenMP_CXX_FLAGS \"\")\n        set(OpenMP_EXE_LINKER_FLAGS \"\")\n    endif()\nelseif(APPLE)\n    # macOS-specific settings\n    find_package(OpenMP QUIET)\n    if(NOT OpenMP_FOUND)\n        message(STATUS \"OpenMP not found - OpenMP support will be disabled\")\n        set(OpenMP_C_FLAGS \"\")\n        set(OpenMP_CXX_FLAGS \"\")\n        set(OpenMP_EXE_LINKER_FLAGS \"\")\n    endif()\nelse()\n    # Linux and other Unix systems\n    find_package(OpenMP REQUIRED)\nendif()\n\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\nset(CMAKE_CXX_STANDARD 17)\n\n# Windows-specific configurations\nif(WIN32)\n    add_definitions(-D_CRT_SECURE_NO_WARNINGS)\n    add_definitions(-DNOMINMAX)\n    add_definitions(-D_WIN32_WINNT=0x0A00)  # Target Windows 10 or later\n    set(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS ON)\nendif()\n\n# Function to collect all user-defined options\nfunction(get_all_options output_var)\n    get_cmake_property(variables VARIABLES)\n    set(options)\n    foreach(var ${variables})\n        if(var MATCHES \"^[A-Z]\" AND NOT var MATCHES \"^CMAKE_\" AND NOT var MATCHES \"^_\")\n            list(APPEND options \"-D${var}=${${var}}\")\n        endif()\n    endforeach()\n    set(${output_var} ${options} PARENT_SCOPE)\nendfunction()\n\n# Create empty file if GGML_CUDA or GGML_METAL is ON\nif (GGML_CUDA OR GGML_METAL OR GGML_HIPBLAS OR GGML_VULKAN)\n    set(SOURCE_EMPTY_FILE_PATH \"${CMAKE_CURRENT_SOURCE_DIR}/nexa/gguf/lib/empty_file.txt\")\n    add_custom_command(\n        OUTPUT ${SOURCE_EMPTY_FILE_PATH}\n        COMMAND ${CMAKE_COMMAND} -E touch ${SOURCE_EMPTY_FILE_PATH}\n        COMMENT \"Creating an empty file to source folder because gpu option is ON\"\n    )\n    set(WHEEL_EMPTY_FILE_PATH \"${SKBUILD_PLATLIB_DIR}/nexa/gguf/lib/empty_file.txt\")\n    add_custom_command(\n        OUTPUT ${WHEEL_EMPTY_FILE_PATH}\n        COMMAND ${CMAKE_COMMAND} -E touch ${WHEEL_EMPTY_FILE_PATH}\n        COMMENT \"Creating an empty file to lib folder because gpu option is ON\"\n    )    \n    add_custom_target(create_empty_file ALL DEPENDS ${SOURCE_EMPTY_FILE_PATH} ${WHEEL_EMPTY_FILE_PATH})\nendif()\n\nif(WIN32 AND GGML_CUDA)\n    set(BUILD_PARALLEL_LEVEL \"2\" CACHE STRING \"Number of parallel jobs for MSBuild in CUDA compilation\")\n    set(MSBUILD_ARGS \"/m:${BUILD_PARALLEL_LEVEL}\")\nelse()\n    set(MSBUILD_ARGS \"\")\nendif()\n\n\n# Function to set up common installation paths\nfunction(setup_install_paths target install_dir)\n    install(\n        TARGETS ${target}\n        LIBRARY DESTINATION ${install_dir}\n        RUNTIME DESTINATION ${install_dir}\n        ARCHIVE DESTINATION ${install_dir}\n    )\nendfunction()\n\n# Collect all user-defined options\nget_all_options(USER_DEFINED_OPTIONS)\n\nif(APPLE)\n    set(CMAKE_INSTALL_RPATH \"@loader_path\")\nelse()\n    set(CMAKE_INSTALL_RPATH \"$ORIGIN\")\nendif()\n\n# Define common CMake options\nset(COMMON_CMAKE_OPTIONS\n    -DCMAKE_BUILD_WITH_INSTALL_RPATH=TRUE\n    -DCMAKE_INSTALL_RPATH_USE_LINK_PATH=TRUE\n    -DCMAKE_SKIP_BUILD_RPATH=FALSE\n    -DCMAKE_SKIP_RPATH=FALSE\n    -DCMAKE_INSTALL_RPATH=${CMAKE_INSTALL_RPATH}\n)\n\nif(WIN32)\n    if(CMAKE_SYSTEM_VERSION)\n        list(APPEND COMMON_CMAKE_OPTIONS\n            -DCMAKE_SYSTEM_VERSION=${CMAKE_SYSTEM_VERSION}\n        )\n    endif()\n    \n    if(CMAKE_VS_WINDOWS_TARGET_PLATFORM_VERSION)\n        list(APPEND COMMON_CMAKE_OPTIONS\n            -DCMAKE_VS_WINDOWS_TARGET_PLATFORM_VERSION=${CMAKE_VS_WINDOWS_TARGET_PLATFORM_VERSION}\n        )\n    endif()\nendif()\n\n# stable_diffusion_cpp project\noption(STABLE_DIFFUSION_BUILD \"Build stable-diffusion.cpp\" ON)\nif(STABLE_DIFFUSION_BUILD)\n    ExternalProject_Add(stable_diffusion_project\n        SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/dependency/stable-diffusion.cpp\n        BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/stable_diffusion_build\n        CMAKE_ARGS\n            ${USER_DEFINED_OPTIONS}\n            ${COMMON_CMAKE_OPTIONS}\n            -DCMAKE_INSTALL_PREFIX=${CMAKE_CURRENT_BINARY_DIR}/stable_diffusion_install\n            -DCMAKE_POSITION_INDEPENDENT_CODE=ON\n            -DCMAKE_CXX_STANDARD=17\n            -DSD_BUILD_SHARED_LIBS=ON\n            -DBUILD_SHARED_LIBS=ON\n            -DSD_METAL=${GGML_METAL}\n            -DSD_CUBLAS=${GGML_CUDA}\n            -DSD_HIPBLAS=${GGML_HIPBLAS}\n            -DSD_VULKAN=${GGML_VULKAN}\n        BUILD_ALWAYS 1\n        BUILD_COMMAND ${CMAKE_COMMAND} --build . --config Release -- ${MSBUILD_ARGS}\n        INSTALL_COMMAND ${CMAKE_COMMAND} --build . --config Release --target install\n    )\nendif()\n\n# llama_cpp project\noption(LLAMA_BUILD \"Build llama.cpp\" ON)\nif(LLAMA_BUILD)\n    set(LLAMA_CUDA ${GGML_CUDA})\n    set(LLAMA_METAL ${GGML_METAL})\n\n    if(WIN32)\n        # Add Windows-specific definitions and flags for llama.cpp\n        list(APPEND COMMON_CMAKE_OPTIONS\n            -DCMAKE_WINDOWS_EXPORT_ALL_SYMBOLS=ON\n            -DLLAMA_NATIVE=OFF           # Disable native CPU optimizations on Windows\n            -DLLAMA_DISABLE_CXXABI=ON    # Disable cxxabi.h dependency\n        )\n\n        # Add compile definition for all targets\n        add_compile_definitions(LLAMA_DISABLE_CXXABI)\n    endif()\n\n    ExternalProject_Add(llama_project\n        SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/dependency/llama.cpp\n        BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/llama_build\n        CMAKE_ARGS\n            ${USER_DEFINED_OPTIONS}\n            ${COMMON_CMAKE_OPTIONS}\n            -DCMAKE_INSTALL_PREFIX=${CMAKE_CURRENT_BINARY_DIR}/llama_install\n            -DCMAKE_POSITION_INDEPENDENT_CODE=ON\n            -DCMAKE_CXX_STANDARD=17\n            -DBUILD_SHARED_LIBS=ON\n            -DLLAMA_CUDA=${LLAMA_CUDA}\n            -DLLAMA_METAL=${LLAMA_METAL}\n            -DCMAKE_C_FLAGS=${OpenMP_C_FLAGS}\n            -DCMAKE_CXX_FLAGS=${OpenMP_CXX_FLAGS}\n            -DCMAKE_EXE_LINKER_FLAGS=${OpenMP_EXE_LINKER_FLAGS}\n            -DGGML_AVX=$<IF:$<AND:$<PLATFORM_ID:Darwin>,$<NOT:$<STREQUAL:${CMAKE_SYSTEM_PROCESSOR},arm64>>>,OFF,ON>\n            -DGGML_AVX2=$<IF:$<AND:$<PLATFORM_ID:Darwin>,$<NOT:$<STREQUAL:${CMAKE_SYSTEM_PROCESSOR},arm64>>>,OFF,ON>\n            -DGGML_FMA=$<IF:$<AND:$<PLATFORM_ID:Darwin>,$<NOT:$<STREQUAL:${CMAKE_SYSTEM_PROCESSOR},arm64>>>,OFF,ON>\n            -DGGML_F16C=$<IF:$<AND:$<PLATFORM_ID:Darwin>,$<NOT:$<STREQUAL:${CMAKE_SYSTEM_PROCESSOR},arm64>>>,OFF,ON>\n            -DGGML_METAL_EMBED_LIBRARY=$<IF:$<PLATFORM_ID:Darwin>,ON,OFF>\n        BUILD_ALWAYS 1\n        BUILD_COMMAND ${CMAKE_COMMAND} --build . --config Release -- ${MSBUILD_ARGS}\n        INSTALL_COMMAND ${CMAKE_COMMAND} --build . --config Release --target install\n    )\nendif()\n\n# bark_cpp project\n# Temporarily disabled since version v0.0.9.3\noption(BARK_BUILD \"Build bark.cpp\" OFF)\nif(BARK_BUILD)\n    # Filter out HIPBLAS and Vulkan options for bark.cpp since it doesn't support them\n    set(BARK_CMAKE_OPTIONS ${USER_DEFINED_OPTIONS})\n    list(FILTER BARK_CMAKE_OPTIONS EXCLUDE REGEX \"GGML_HIPBLAS|GGML_VULKAN\")\n    \n    ExternalProject_Add(bark_project\n        SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/dependency/bark.cpp\n        BINARY_DIR ${CMAKE_CURRENT_BINARY_DIR}/bark_build\n        CMAKE_ARGS\n            ${USER_DEFINED_OPTIONS}\n            ${COMMON_CMAKE_OPTIONS}\n            -DCMAKE_INSTALL_PREFIX=${CMAKE_CURRENT_BINARY_DIR}/bark_install\n            -DCMAKE_POSITION_INDEPENDENT_CODE=ON\n            -DCMAKE_CXX_STANDARD=17\n            -DGGML_CUDA=${GGML_CUDA}\n            -DGGML_METAL=OFF\n            -DBUILD_SHARED_LIBS=ON\n            -DBARK_BUILD_EXAMPLES=OFF\n        BUILD_ALWAYS 1\n        BUILD_COMMAND ${CMAKE_COMMAND} --build . --config Release -- ${MSBUILD_ARGS}\n        INSTALL_COMMAND ${CMAKE_COMMAND} --build . --config Release --target install\n    )\nendif()\n\n# Install the built libraries to the final destination\nif(WIN32)\n    install(\n        DIRECTORY\n            ${CMAKE_CURRENT_BINARY_DIR}/stable_diffusion_build/bin/Release/\n            ${CMAKE_CURRENT_BINARY_DIR}/llama_build/bin/Release/\n            ${CMAKE_CURRENT_BINARY_DIR}/bark_build/bin/Release/\n            ${CMAKE_CURRENT_BINARY_DIR}/bark_build/Release/\n        DESTINATION ${SKBUILD_PLATLIB_DIR}/nexa/gguf/lib\n        USE_SOURCE_PERMISSIONS\n        FILES_MATCHING\n            PATTERN \"*.dll\"\n    )\n\n    install(\n        DIRECTORY\n            ${CMAKE_CURRENT_BINARY_DIR}/stable_diffusion_build/bin/Release/\n            ${CMAKE_CURRENT_BINARY_DIR}/llama_build/bin/Release/\n            ${CMAKE_CURRENT_BINARY_DIR}/bark_build/bin/Release/\n            ${CMAKE_CURRENT_BINARY_DIR}/bark_build/Release/\n        DESTINATION ${CMAKE_CURRENT_SOURCE_DIR}/nexa/gguf/lib\n        USE_SOURCE_PERMISSIONS\n        FILES_MATCHING\n            PATTERN \"*.dll\"\n    )\nelse()\n    install(\n        DIRECTORY\n            ${CMAKE_CURRENT_BINARY_DIR}/stable_diffusion_build/bin/\n            ${CMAKE_CURRENT_BINARY_DIR}/llama_install/lib/\n            ${CMAKE_CURRENT_BINARY_DIR}/bark_install/lib/\n        DESTINATION ${SKBUILD_PLATLIB_DIR}/nexa/gguf/lib\n        USE_SOURCE_PERMISSIONS\n        FILES_MATCHING\n            PATTERN \"*.so\"\n            PATTERN \"*.dylib\"\n    )\n\n    install(\n        DIRECTORY\n            ${CMAKE_CURRENT_BINARY_DIR}/stable_diffusion_build/bin/\n            ${CMAKE_CURRENT_BINARY_DIR}/llama_install/lib/\n            ${CMAKE_CURRENT_BINARY_DIR}/bark_install/lib/\n        DESTINATION ${CMAKE_CURRENT_SOURCE_DIR}/nexa/gguf/lib\n        USE_SOURCE_PERMISSIONS\n        FILES_MATCHING\n            PATTERN \"*.so\"\n            PATTERN \"*.dylib\"\n    )\nendif()"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.810546875,
          "content": "# Use a base image that includes Miniconda\nFROM continuumio/miniconda3\n\n# Set the working directory in the container\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y cmake g++\n# Copy the environment.yml file into the container at /app\nCOPY pyproject.toml CMakeLists.txt requirements.txt README.md ./\nCOPY dependency ./dependency\n\n# Install the conda environment\nRUN conda create -n nexa python=3.10 -y\nRUN /bin/bash -c \"source activate nexa && pip install -r requirements.txt && pip install -e .\"\n\n# Activate the environment\nRUN echo \"source activate nexa\" > ~/.bashrc\nENV PATH /opt/conda/envs/nexa/bin:$PATH\n\n# Copy your application code to the container\nCOPY . .\n\n# Set the command to activate the environment and start your application\nCMD [\"bash\", \"-c\", \"source activate nexa && python -m nexa.cli.entry gen-text gemma\"]"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 13.2568359375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n## Some of TensorFlow's code is derived from Caffe, which is subject to the following copyright notice:\n\nCOPYRIGHT\n\nAll contributions by the University of California:\n\nCopyright (c) 2014, The Regents of the University of California (Regents)\nAll rights reserved.\n\nAll other contributions:\n\nCopyright (c) 2014, the respective contributors\nAll rights reserved.\n\nCaffe uses a shared copyright model: each contributor holds copyright over\ntheir contributions to Caffe. The project versioning records all such\ncontribution and copyright details. If a contributor wants to further mark\ntheir specific copyright on a particular contribution, they should indicate\ntheir copyright solely in the commit message of the change when it is\ncommitted.\n\nLICENSE\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\n   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n   ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n   WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n   DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\n   ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n   (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n   LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n   ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n   SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nCONTRIBUTION AGREEMENT\n\nBy contributing to the BVLC/caffe repository through pull-request, comment,\nor otherwise, the contributor releases their content to the\nlicense and copyright terms herein."
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0244140625,
          "content": "include requirements.txt\n"
        },
        {
          "name": "Package.swift",
          "type": "blob",
          "size": 0.78515625,
          "content": "// swift-tools-version: 6.0\n\nimport PackageDescription\n\nlet package = Package(\n    name: \"NexaSwift\",\n    platforms: [\n        .macOS(.v15),\n        .iOS(.v18),\n        .watchOS(.v11),\n        .tvOS(.v18),\n        .visionOS(.v2)\n    ],\n    products: [\n        .library(name: \"NexaSwift\", targets: [\"NexaSwift\"]),\n    ],\n    dependencies: [\n        .package(url: \"https://github.com/ggerganov/llama.cpp.git\", branch: \"master\")\n    ],\n    targets: [\n        .target(\n            name: \"NexaSwift\", \n            dependencies: [\n                .product(name: \"llama\", package: \"llama.cpp\")\n            ],\n            path: \"swift/Sources/NexaSwift\"),\n        .testTarget(\n            name: \"NexaSwiftTests\", \n            dependencies: [\"NexaSwift\"],\n            path: \"swift/Tests/NexaSwiftTests\"),\n    ]\n)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 19.3203125,
          "content": "<video src=\"https://user-images.githubusercontent.com/assets/375570dc-0e7a-4a99-840d-c1ef6502e5aa.mp4\" autoplay muted loop playsinline style=\"max-width: 100%;\"></video>\n\n<h1>Nexa SDK - Local On-Device Inference Framework</h1>\n\n[release-url]: https://github.com/NexaAI/nexa-sdk/releases\n[Windows-image]: https://img.shields.io/badge/windows-0078D4?logo=windows\n[MacOS-image]: https://img.shields.io/badge/-MacOS-black?logo=apple\n[Linux-image]: https://img.shields.io/badge/-Linux-333?logo=ubuntu\n\n[![MacOS][MacOS-image]][release-url] [![Linux][Linux-image]][release-url] [![Windows][Windows-image]][release-url] [![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2FNexaAI%2Fnexa-sdk%2Fbadge%3Ftype%3Dplatforms)](https://swiftpackageindex.com/NexaAI/nexa-sdk) [![Build workflow](https://img.shields.io/github/actions/workflow/status/NexaAI/nexa-sdk/ci.yaml?label=CI&logo=github)](https://github.com/NexaAI/nexa-sdk/actions/workflows/ci.yaml?query=branch%3Amain) ![GitHub License](https://img.shields.io/github/license/NexaAI/nexa-sdk) [![GitHub Release](https://img.shields.io/github/v/release/NexaAI/nexa-sdk)](https://github.com/NexaAI/nexa-sdk/releases/latest) [![PyPI](https://img.shields.io/pypi/v/nexaai)](https://pypi.org/project/nexaai/)\n\n[**On-Device Model Hub**](https://nexa.ai/models) | [**Documentation**](https://docs.nexa.ai/) | [**Discord**](https://discord.gg/thRu2HaK4D) | [**Blogs**](https://nexa.ai/blogs) | [**X (Twitter)**](https://x.com/nexa_ai)\n\n**Nexa SDK** is a local on-device inference framework for ONNX and GGML models, supporting text generation, image generation, vision-language models (VLM), audio-language models, speech-to-text (ASR), and text-to-speech (TTS) capabilities. Installable via Python Package or Executable Installer.\n\n### Features\n\n- **Device Support:** CPU, GPU (CUDA, Metal, ROCm), iOS\n- **Server:** OpenAI-compatible API, JSON schema for function calling and streaming support\n- **Local UI:** Streamlit for interactive model deployment and testing\n\n## Latest News ðŸ”¥\n\n- Support Nexa AI's own vision language model (0.9B parameters): `nexa run omniVLM` and audio language model (2.9B parameters): `nexa run omniaudio`\n- Support audio language model: `nexa run qwen2audio`, **we are the first open-source toolkit to support audio language model with GGML tensor library.**\n- Support iOS Swift binding for local inference on **iOS mobile** devices.\n- Support embedding model: `nexa embed <model_path> <prompt>`\n- Support pull and run supported Computer Vision models in GGUF format from HuggingFace or ModelScope: `nexa run -hf <hf_model_id> -mt COMPUTER_VISION` or `nexa run -ms <ms_model_id> -mt COMPUTER_VISION`\n- Support pull and run NLP models in GGUF format from HuggingFace or ModelScope: `nexa run -hf <hf_model_id> -mt NLP` or `nexa run -ms <ms_model_id> -mt NLP`\n\nWelcome to submit your requests through [issues](https://github.com/NexaAI/nexa-sdk/issues/new/choose), we ship weekly.\n\n## Install Option 1: Executable Installer\n\n<p>\n    <a href=\"https://public-storage.nexa4ai.com/nexa-sdk-executable-installer/nexa-sdk-0.0.9.7-macos-installer.pkg\">\n        <img src=\"./assets/mac.png\" style=\"height: 1em; width: auto\" /> <strong> macOS Installer </strong>\n    </a>\n</p>\n\n<p>\n    <a href=\"https://public-storage.nexa4ai.com/nexa-sdk-executable-installer/nexa-sdk-0.0.9.7-windows-setup.exe\">\n        <img src=\"./assets/windows.png\" style=\"height: 1em; width: auto\" /> <strong>Windows Installer</strong>\n    </a>\n</p>\n\n<strong> <img src=\"./assets/linux.png\" style=\"height: 1em; width: auto\" /> Linux Installer </strong>\n\n```bash\ncurl -fsSL https://public-storage.nexa4ai.com/install.sh | sh\n```\n\n<details>\n<summary><strong>FAQ: cannot use executable with nexaai python package already installed</strong></summary>\n\nTry using `nexa-exe` instead:\n\n```bash\nnexa-exe <command>\n```\n\n</details>\n\n## Install Option 2: Python Package\n\nWe have released pre-built wheels for various Python versions, platforms, and backends for convenient installation on our [index page](https://github.nexa.ai/whl/).\n\n<details> <summary><strong style=\"font-size: 1.2em;\">CPU</strong></summary>\n\n```bash\npip install nexaai --prefer-binary --index-url https://github.nexa.ai/whl/cpu --extra-index-url https://pypi.org/simple --no-cache-dir\n```\n\n</details>\n\n<details> <summary><strong style=\"font-size: 1.2em;\">Apple GPU (Metal)</strong></summary>\n\nFor the GPU version supporting **Metal (macOS)**:\n\n```bash\nCMAKE_ARGS=\"-DGGML_METAL=ON -DSD_METAL=ON\" pip install nexaai --prefer-binary --index-url https://github.nexa.ai/whl/metal --extra-index-url https://pypi.org/simple --no-cache-dir\n```\n\n<details>\n<summary><strong>FAQ: cannot use Metal/GPU on M1</strong></summary>\n\nTry the following command:\n\n```bash\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\nbash Miniforge3-MacOSX-arm64.sh\nconda create -n nexasdk python=3.10\nconda activate nexasdk\nCMAKE_ARGS=\"-DGGML_METAL=ON -DSD_METAL=ON\" pip install nexaai --prefer-binary --index-url https://github.nexa.ai/whl/metal --extra-index-url https://pypi.org/simple --no-cache-dir\n```\n\n</details>\n</details>\n\n<details> <summary><strong style=\"font-size: 1.2em;\">Nvidia GPU (CUDA)</strong></summary>\n\nTo install with CUDA support, make sure you have [CUDA Toolkit 12.0 or later](https://developer.nvidia.com/cuda-12-0-0-download-archive) installed.\n\nFor **Linux**:\n\n```bash\nCMAKE_ARGS=\"-DGGML_CUDA=ON -DSD_CUBLAS=ON\" pip install nexaai --prefer-binary --index-url https://github.nexa.ai/whl/cu124 --extra-index-url https://pypi.org/simple --no-cache-dir\n```\n\nFor **Windows PowerShell**:\n\n```bash\n$env:CMAKE_ARGS=\"-DGGML_CUDA=ON -DSD_CUBLAS=ON\"; pip install nexaai --prefer-binary --index-url https://github.nexa.ai/whl/cu124 --extra-index-url https://pypi.org/simple --no-cache-dir\n```\n\nFor **Windows Command Prompt**:\n\n```bash\nset CMAKE_ARGS=\"-DGGML_CUDA=ON -DSD_CUBLAS=ON\" & pip install nexaai --prefer-binary --index-url https://github.nexa.ai/whl/cu124 --extra-index-url https://pypi.org/simple --no-cache-dir\n```\n\nFor **Windows Git Bash**:\n\n```bash\nCMAKE_ARGS=\"-DGGML_CUDA=ON -DSD_CUBLAS=ON\" pip install nexaai --prefer-binary --index-url https://github.nexa.ai/whl/cu124 --extra-index-url https://pypi.org/simple --no-cache-dir\n```\n\n<details>\n<summary><strong>FAQ: Building Issues for llava</strong></summary>\n\nIf you encounter the following issue while building:\n\n![](docs/.media/error.jpeg)\n\ntry the following command:\n\n```bash\nCMAKE_ARGS=\"-DCMAKE_CXX_FLAGS=-fopenmp\" pip install nexaai\n```\n\n</details>\n\n</details>\n\n<details> <summary><strong style=\"font-size: 1.2em;\">AMD GPU (ROCm)</strong></summary>\n\nTo install with ROCm support, make sure you have [ROCm 6.2.1 or later](https://rocm.docs.amd.com/projects/install-on-linux/en/docs-6.2.1/install/quick-start.html) installed.\n\nFor **Linux**:\n\n```bash\nCMAKE_ARGS=\"-DGGML_HIPBLAS=on\" pip install nexaai --prefer-binary --index-url https://github.nexa.ai/whl/rocm621 --extra-index-url https://pypi.org/simple --no-cache-dir\n```\n\n</details>\n\n<details> <summary><strong style=\"font-size: 1.2em;\">GPU (Vulkan)</strong></summary>\n\nTo install with Vulkan support, make sure you have [Vulkan SDK 1.3.261.1 or later](https://vulkan.lunarg.com/sdk/home) installed.\n\nFor **Windows PowerShell**:\n\n```bash\n$env:CMAKE_ARGS=\"-DGGML_VULKAN=on\"; pip install nexaai --prefer-binary --index-url https://github.nexa.ai/whl/vulkan --extra-index-url https://pypi.org/simple --no-cache-dir\n```\n\nFor **Windows Command Prompt**:\n\n```bash\nset CMAKE_ARGS=\"-DGGML_VULKAN=on\" & pip install nexaai --prefer-binary --index-url https://github.nexa.ai/whl/vulkan --extra-index-url https://pypi.org/simple --no-cache-dir\n```\n\nFor **Windows Git Bash**:\n\n```bash\nCMAKE_ARGS=\"-DGGML_VULKAN=on\" pip install nexaai --prefer-binary --index-url https://github.nexa.ai/whl/vulkan --extra-index-url https://pypi.org/simple --no-cache-dir\n```\n\n</details>\n\n<details> <summary><strong style=\"font-size: 1.2em;\">Local Build</strong></summary>\n\nHow to clone this repo\n\n```bash\ngit clone --recursive https://github.com/NexaAI/nexa-sdk\n```\n\nIf you forget to use `--recursive`, you can use below command to add submodule\n\n```bash\ngit submodule update --init --recursive\n```\n\nThen you can build and install the package\n\n```bash\npip install -e .\n```\n\n</details>\n\n## Differentiation\n\nBelow is our differentiation from other similar tools:\n\n| **Feature**                 | **[Nexa SDK](https://github.com/NexaAI/nexa-sdk)** | **[ollama](https://github.com/ollama/ollama)** | **[Optimum](https://github.com/huggingface/optimum)** | **[LM Studio](https://github.com/lmstudio-ai)** |\n| --------------------------- | :------------------------------------------------: | :--------------------------------------------: | :---------------------------------------------------: | :---------------------------------------------: |\n| **GGML Support**            |                         âœ…                         |                       âœ…                       |                          âŒ                           |                       âœ…                        |\n| **ONNX Support**            |                         âœ…                         |                       âŒ                       |                          âœ…                           |                       âŒ                        |\n| **Text Generation**         |                         âœ…                         |                       âœ…                       |                          âœ…                           |                       âœ…                        |\n| **Image Generation**        |                         âœ…                         |                       âŒ                       |                          âŒ                           |                       âŒ                        |\n| **Vision-Language Models**  |                         âœ…                         |                       âœ…                       |                          âœ…                           |                       âœ…                        |\n| **Audio-Language Models**   |                         âœ…                         |                       âŒ                       |                          âŒ                           |                       âŒ                        |\n| **Text-to-Speech**          |                         âœ…                         |                       âŒ                       |                          âœ…                           |                       âŒ                        |\n| **Server Capability**       |                         âœ…                         |                       âœ…                       |                          âœ…                           |                       âœ…                        |\n| **User Interface**          |                         âœ…                         |                       âŒ                       |                          âŒ                           |                       âœ…                        |\n| **Executable Installation** |                         âœ…                         |                       âœ…                       |                          âŒ                           |                       âœ…                        |\n\n## Supported Models & Model Hub\n\nOur on-device model hub offers all types of quantized models (text, image, audio, multimodal) with filters for RAM, file size, Tasks, etc. to help you easily explore models with UI. Explore on-device models at [On-device Model Hub](https://model-hub.nexa4ai.com/)\n\nSupported model examples (full list at [Model Hub](https://nexa.ai/models)):\n| Model | Type | Format | Command |\n| ------------------------------------------------------------------------------------------------------- | --------------- | --------- | -------------------------------------- |\n| [omniaudio](https://nexa.ai/NexaAI/omniaudio/gguf-q4_0/readme) | AudioLM | GGUF | `nexa run omniaudio` |\n| [qwen2audio](https://nexa.ai/Qwen/Qwen2-Audio-7.8B-Instruct/gguf-q4_K_M/readme) | AudioLM | GGUF | `nexa run qwen2audio` |\n| [octopus-v2](https://www.nexaai.com/NexaAI/Octopus-v2/gguf-q4_0/readme) | Function Call | GGUF | `nexa run octopus-v2` |\n| [octo-net](https://www.nexaai.com/NexaAI/Octo-net/gguf-q4_0/readme) | Text | GGUF | `nexa run octo-net` |\n| [omniVLM](https://nexa.ai/NexaAI/omniVLM/gguf-fp16/readme) | Multimodal | GGUF | `nexa run omniVLM` |\n| [nanollava](https://www.nexaai.com/qnguyen3/nanoLLaVA/gguf-fp16/readme) | Multimodal | GGUF | `nexa run nanollava` |\n| [llava-phi3](https://www.nexaai.com/xtuner/llava-phi-3-mini/gguf-q4_0/readme) | Multimodal | GGUF | `nexa run llava-phi3` |\n| [llava-llama3](https://www.nexaai.com/xtuner/llava-llama-3-8b-v1.1/gguf-q4_0/readme) | Multimodal | GGUF | `nexa run llava-llama3` |\n| [llava1.6-mistral](https://www.nexaai.com/liuhaotian/llava-v1.6-mistral-7b/gguf-q4_0/readme) | Multimodal | GGUF | `nexa run llava1.6-mistral` |\n| [llava1.6-vicuna](https://www.nexaai.com/liuhaotian/llava-v1.6-vicuna-7b/gguf-q4_0/readme) | Multimodal | GGUF | `nexa run llava1.6-vicuna` |\n| [llama3.2](https://nexaai.com/meta/Llama3.2-3B-Instruct/gguf-q4_0/readme) | Text | GGUF | `nexa run llama3.2` |\n| [llama3-uncensored](https://www.nexaai.com/Orenguteng/Llama3-8B-Lexi-Uncensored/gguf-q4_K_M/readme) | Text | GGUF | `nexa run llama3-uncensored` |\n| [gemma2](https://www.nexaai.com/google/gemma-2-2b-instruct/gguf-q4_0/readme) | Text | GGUF | `nexa run gemma2` |\n| [qwen2.5](https://www.nexaai.com/Qwen/Qwen2.5-1.5B-Instruct/gguf-q4_0/readme) | Text | GGUF | `nexa run qwen2.5` |\n| [mathqwen](https://nexaai.com/Qwen/Qwen2.5-Math-1.5B-Instruct/gguf-q4_0/readme) | Text | GGUF | `nexa run mathqwen` |\n| [codeqwen](https://www.nexaai.com/Qwen/CodeQwen1.5-7B-Instruct/gguf-q4_0/readme) | Text | GGUF | `nexa run codeqwen` |\n| [mistral](https://www.nexaai.com/mistralai/Mistral-7B-Instruct-v0.3/gguf-q4_0/readme) | Text | GGUF/ONNX | `nexa run mistral` |\n| [deepseek-coder](https://www.nexaai.com/DeepSeek/deepseek-coder-1.3b-instruct/gguf-q4_0/readme) | Text | GGUF | `nexa run deepseek-coder` |\n| [phi3.5](https://nexaai.com/microsoft/Phi-3.5-mini-instruct/gguf-q4_0/readme) | Text | GGUF | `nexa run phi3.5` |\n| [openelm](https://nexaai.com/apple/OpenELM-3B/gguf-q4_K_M/readme) | Text | GGUF | `nexa run openelm` |\n| [stable-diffusion-v2-1](https://nexaai.com/StabilityAI/stable-diffusion-v2-1/gguf-q4_0/readme) | Image Generation | GGUF | `nexa run sd2-1` |\n| [stable-diffusion-3-medium](https://nexaai.com/StabilityAI/stable-diffusion-3-medium/gguf-q4_0/readme) | Image Generation | GGUF | `nexa run sd3` |\n| [FLUX.1-schnell](https://nexaai.com/BlackForestLabs/FLUX.1-schnell/gguf-q4_0/readme) | Image Generation | GGUF | `nexa run flux` |\n| [lcm-dreamshaper](https://www.nexaai.com/SimianLuo/lcm-dreamshaper-v7/gguf-fp16/readme) | Image Generation | GGUF/ONNX | `nexa run lcm-dreamshaper` |\n| [whisper-large-v3-turbo](https://nexaai.com/Systran/faster-whisper-large-v3-turbo/bin-cpu-fp16/readme) | Speech-to-Text | BIN | `nexa run faster-whisper-large-turbo` |\n| [whisper-tiny.en](https://nexaai.com/openai/whisper-tiny.en/onnx-cpu-fp32/readme) | Speech-to-Text | ONNX | `nexa run whisper-tiny.en` |\n| [mxbai-embed-large-v1](https://nexa.ai/mixedbread-ai/mxbai-embed-large-v1/gguf-fp16/readme) | Embedding | GGUF | `nexa embed mxbai` |\n| [nomic-embed-text-v1.5](https://nexa.ai/nomic-ai/nomic-embed-text-v1.5/gguf-fp16/readme) | Embedding | GGUF | `nexa embed nomic` |\n| [all-MiniLM-L12-v2](https://nexa.ai/sentence-transformers/all-MiniLM-L12-v2/gguf-fp16/readme) | Embedding | GGUF | `nexa embed all-MiniLM-L12-v2:fp16` |\n| [bark-small](https://nexa.ai/suno/bark-small/gguf-fp16/readme) | Text-to-Speech | GGUF | `nexa run bark-small:fp16` |\n\n## Run Models from ðŸ¤— HuggingFace or ðŸ¤– ModelScope\n\nYou can pull, convert (to .gguf), quantize and run [llama.cpp supported](https://github.com/ggerganov/llama.cpp#description) text generation models from HF or MS with Nexa SDK.\n\n### Run .gguf File\n\nUse `nexa run -hf <hf-model-id>` or `nexa run -ms <ms-model-id>` to run models with provided .gguf files:\n\n```bash\nnexa run -hf Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\n```\n\n```bash\nnexa run -ms Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\n```\n\n> **Note:** You will be prompted to select a single .gguf file. If your desired quantization version has multiple split files (like fp16-00001-of-00004), please use Nexa's conversion tool (see below) to convert and quantize the model locally.\n\n### Convert .safetensors Files\n\nInstall [Nexa Python package](https://github.com/NexaAI/nexa-sdk?tab=readme-ov-file#install-option-2-python-package), and install Nexa conversion tool with `pip install \"nexaai[convert]\"`, then convert models from huggingface with `nexa convert <hf-model-id>`:\n\n```bash\nnexa convert HuggingFaceTB/SmolLM2-135M-Instruct\n```\n\nOr you can convert models from ModelScope with `nexa convert -ms <ms-model-id>`:\n\n```bash\nnexa convert -ms Qwen/Qwen2.5-7B-Instruct\n```\n\n> **Note:** Check our [leaderboard](https://nexa.ai/leaderboard) for performance benchmarks of different quantized versions of mainstream language models and [HuggingFace docs](https://huggingface.co/docs/optimum/en/concept_guides/quantization) to learn about quantization options.\n\nðŸ“‹ You can view downloaded and converted models with `nexa list`\n\n## Documentation\n\n> [!NOTE]\n>\n> 1. If you want to use <strong>ONNX model</strong>, just replace `pip install nexaai` with `pip install \"nexaai[onnx]\"` in provided commands.\n> 2. If you want to <strong>run benchmark evaluation</strong>, just replace `pip install nexaai` with `pip install \"nexaai[eval]\"` in provided commands.\n> 3. If you want to <strong>convert and quantize huggingface models to GGUF models</strong>, just replace `pip install nexaai` with `pip install \"nexaai[convert]\"` in provided commands.\n> 4. For Chinese developers, we recommend you to use <strong>Tsinghua Open Source Mirror</strong> as extra index url, just replace `--extra-index-url https://pypi.org/simple` with `--extra-index-url https://pypi.tuna.tsinghua.edu.cn/simple` in provided commands.\n\n### CLI Reference\n\nHere's a brief overview of the main CLI commands:\n\n- `nexa run`: Run inference for various tasks using GGUF models.\n- `nexa onnx`: Run inference for various tasks using ONNX models.\n- `nexa convert`: Convert and quantize huggingface models to GGUF models.\n- `nexa server`: Run the Nexa AI Text Generation Service.\n- `nexa eval`: Run the Nexa AI Evaluation Tasks.\n- `nexa pull`: Pull a model from official or hub.\n- `nexa remove`: Remove a model from local machine.\n- `nexa clean`: Clean up all model files.\n- `nexa list`: List all models in the local machine.\n- `nexa login`: Login to Nexa API.\n- `nexa whoami`: Show current user information.\n- `nexa logout`: Logout from Nexa API.\n\nFor detailed information on CLI commands and usage, please refer to the [CLI Reference](CLI.md) document.\n\n### Start Local Server\n\nTo start a local server using models on your local computer, you can use the `nexa server` command.\nFor detailed information on server setup, API endpoints, and usage examples, please refer to the [Server Reference](SERVER.md) document.\n\n### Swift Package\n\n**[Swift SDK](https://github.com/NexaAI/nexa-sdk/tree/main/swift):** Provides a Swifty API, allowing Swift developers to easily integrate and use llama.cpp models in their projects.\n\n[**More Docs**](https://docs.nexa.ai/)\n\n## Acknowledgements\n\nWe would like to thank the following projects:\n\n- [llama.cpp](https://github.com/ggerganov/llama.cpp)\n- [stable-diffusion.cpp](https://github.com/leejet/stable-diffusion.cpp)\n- [bark.cpp](https://github.com/PABannier/bark.cpp)\n- [optimum](https://github.com/huggingface/optimum)\n"
        },
        {
          "name": "SERVER.md",
          "type": "blob",
          "size": 7.5517578125,
          "content": "## Start Local Server\n\nYou can start a local server using models on your local computer with the `nexa server` command. Here's the usage syntax:\n\n```\nusage: nexa server [-h] [--host HOST] [--port PORT] [--reload] model_path\n```\n\n### Options:\n\n- `-lp, --local_path`: Indicate that the model path provided is the local path\n- `-mt, --model_type`: Indicate the model running type, must be used with -lp or -hf or ms, choose from [NLP, COMPUTER_VISION, MULTIMODAL, AUDIO]\n- `-hf, --huggingface`: Load model from Hugging Face Hub\n- `-ms, --modelscope`: Load model from ModelScope Hub\n- `--host`: Host to bind the server to\n- `--port`: Port to bind the server to\n- `--reload`: Enable automatic reloading on code changes\n- `--nctx`: Maximum context length of the model you're using\n\n### Example Commands:\n\n```\nnexa server gemma\nnexa server llama2-function-calling\nnexa server sd1-5\nnexa server faster-whipser-large\nnexa server ../models/llava-v1.6-vicuna-7b/ -lp -mt MULTIMODAL\n```\n\nBy default, `nexa server` will run gguf models. To run onnx models, simply add `onnx` after `nexa server`.\n\n## API Endpoints\n\n### 1. Text Generation: <code>/v1/completions</code>\n\nGenerates text based on a single prompt.\n\n#### Request body:\n\n```json\n{\n  \"prompt\": \"Tell me a story\",\n  \"temperature\": 1,\n  \"max_new_tokens\": 128,\n  \"top_k\": 50,\n  \"top_p\": 1,\n  \"stop_words\": [\"string\"],\n  \"stream\": false\n}\n```\n\n#### Example Response:\n\n```json\n{\n  \"result\": \"Once upon a time, in a small village nestled among rolling hills...\"\n}\n```\n\n### 2. Chat Completions: <code>/v1/chat/completions</code>\n\nUpdate: Now supports multimodal inputs when using Multimodal models.\n\nHandles chat completions with support for conversation history.\n\n#### Request body:\n\nMultimodal models (VLM):\n\n```json\n{\n  \"model\": \"anything\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"Whatâ€™s in this image?\"\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n          }\n        }\n      ]\n    }\n  ],\n  \"max_tokens\": 300,\n  \"temperature\": 0.7,\n  \"top_p\": 0.95,\n  \"top_k\": 40,\n  \"stream\": false\n}\n```\n\nTraditional NLP models:\n\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Tell me a story\"\n    }\n  ],\n  \"max_tokens\": 128,\n  \"temperature\": 0.1,\n  \"stream\": false,\n  \"stop_words\": []\n}\n```\n\n#### Example Response:\n\n```json\n{\n  \"id\": \"f83502df-7f5a-4825-a922-f5cece4081de\",\n  \"object\": \"chat.completion\",\n  \"created\": 1723441724.914671,\n  \"choices\": [\n    {\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"In the heart of a mystical forest...\"\n      }\n    }\n  ]\n}\n```\n\n### 3. Function Calling: <code>/v1/function-calling</code>\n\nCall the most appropriate function based on user's prompt.\n\n#### Request body:\n\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Extract Jason is 25 years old\"\n    }\n  ],\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"UserDetail\",\n        \"parameters\": {\n          \"properties\": {\n            \"name\": {\n              \"description\": \"The user's name\",\n              \"type\": \"string\"\n            },\n            \"age\": {\n              \"description\": \"The user's age\",\n              \"type\": \"integer\"\n            }\n          },\n          \"required\": [\"name\", \"age\"],\n          \"type\": \"object\"\n        }\n      }\n    }\n  ],\n  \"tool_choice\": \"auto\"\n}\n```\n\n#### Function format:\n\n```json\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"function_name\",\n    \"description\": \"function_description\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"property_name\": {\n          \"type\": \"string | number | boolean | object | array\",\n          \"description\": \"string\"\n        }\n      },\n      \"required\": [\"array_of_required_property_names\"]\n    }\n  }\n}\n```\n\n#### Example Response:\n\n```json\n{\n  \"id\": \"chatcmpl-7a9b0dfb-878f-4f75-8dc7-24177081c1d0\",\n  \"object\": \"chat.completion\",\n  \"created\": 1724186442,\n  \"model\": \"/home/ubuntu/.cache/nexa/hub/official/Llama2-7b-function-calling/q3_K_M.gguf\",\n  \"choices\": [\n    {\n      \"finish_reason\": \"tool_calls\",\n      \"index\": 0,\n      \"logprobs\": null,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": null,\n        \"tool_calls\": [\n          {\n            \"id\": \"call__0_UserDetail_cmpl-8d5cf645-7f35-4af2-a554-2ccea1a67bdd\",\n            \"type\": \"function\",\n            \"function\": {\n              \"name\": \"UserDetail\",\n              \"arguments\": \"{ \\\"name\\\": \\\"Jason\\\", \\\"age\\\": 25 }\"\n            }\n          }\n        ],\n        \"function_call\": {\n          \"name\": \"\",\n          \"arguments\": \"{ \\\"name\\\": \\\"Jason\\\", \\\"age\\\": 25 }\"\n        }\n      }\n    }\n  ],\n  \"usage\": {\n    \"completion_tokens\": 15,\n    \"prompt_tokens\": 316,\n    \"total_tokens\": 331\n  }\n}\n```\n\n### 4. Text-to-Image: <code>/v1/txt2img</code>\n\nGenerates images based on a single prompt.\n\n#### Request body:\n\n```json\n{\n  \"prompt\": \"A girl, standing in a field of flowers, vivid\",\n  \"image_path\": \"\",\n  \"cfg_scale\": 7,\n  \"width\": 256,\n  \"height\": 256,\n  \"sample_steps\": 20,\n  \"seed\": 0,\n  \"negative_prompt\": \"\"\n}\n```\n\n#### Example Response:\n\n```json\n{\n  \"created\": 1724186615.5426757,\n  \"data\": [\n    {\n      \"base64\": \"base64_of_generated_image\",\n      \"url\": \"path/to/generated_image\"\n    }\n  ]\n}\n```\n\n### 5. Image-to-Image: <code>/v1/img2img</code>\n\nModifies existing images based on a single prompt.\n\n#### Request body:\n\n```json\n{\n  \"prompt\": \"A girl, standing in a field of flowers, vivid\",\n  \"image_path\": \"path/to/image\",\n  \"cfg_scale\": 7,\n  \"width\": 256,\n  \"height\": 256,\n  \"sample_steps\": 20,\n  \"seed\": 0,\n  \"negative_prompt\": \"\"\n}\n```\n\n#### Example Response:\n\n```json\n{\n  \"created\": 1724186615.5426757,\n  \"data\": [\n    {\n      \"base64\": \"base64_of_generated_image\",\n      \"url\": \"path/to/generated_image\"\n    }\n  ]\n}\n```\n\n### 6. Audio Transcriptions: <code>/v1/audio/transcriptions</code>\n\nTranscribes audio files to text.\n\n#### Parameters:\n\n- `beam_size` (integer): Beam size for transcription (default: 5)\n- `language` (string): Language code (e.g., 'en', 'fr')\n- `temperature` (number): Temperature for sampling (default: 0)\n\n#### Request body:\n\n```\n{\n  \"file\" (form-data): The audio file to transcribe (required)\n}\n```\n\n#### Example Response:\n\n```json\n{\n  \"text\": \" And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\"\n}\n```\n\n### 7. Audio Translations: <code>/v1/audio/translations</code>\n\nTranslates audio files to text in English.\n\n#### Parameters:\n\n- `beam_size` (integer): Beam size for transcription (default: 5)\n- `temperature` (number): Temperature for sampling (default: 0)\n\n#### Request body:\n\n```\n{\n  \"file\" (form-data): The audio file to transcribe (required)\n}\n```\n\n#### Example Response:\n\n```json\n{\n  \"text\": \" Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\"\n}\n```\n\n### 8. Generate Embeddings: <code>/v1/embeddings</code>\n\nGenerate embeddings for a given text.\n\n#### Request body:\n\n```json\n{\n  \"input\": \"I love Nexa AI.\",\n  \"normalize\": false,\n  \"truncate\": true\n}\n```\n\n#### Example Response:\n\n```json\n{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [\n        -0.006929283495992422,\n        -0.005336422007530928,\n        ... (omitted for spacing)\n        -4.547132266452536e-05,\n        -0.024047505110502243\n      ],\n    }\n  ],\n  \"model\": \"/home/ubuntu/models/embedding_models/mxbai-embed-large-q4_0.gguf\",\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"total_tokens\": 5\n  }\n}\n```\n"
        },
        {
          "name": "android",
          "type": "tree",
          "content": null
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "dependency",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "nexa",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 3.1533203125,
          "content": "[build-system]\nrequires = [\"scikit-build-core\", \"setuptools>=64.0\"]\nbuild-backend = \"scikit_build_core.build\"\n\n[project]\nname = \"nexaai\"\ndynamic = [\"version\"]\ndescription = \"Nexa AI SDK\"\nreadme = \"README.md\"\nlicense = { text = \"MIT\" }\nauthors = [{ name = \"Nexa AI\", email = \"octopus@nexa4ai.com\" }]\ndependencies = [\n    \"cmake\", # For building C++ extensions\n    \"faster_whisper\",\n    \"typing-extensions>=4.5.0\", # For ggml\n    \"numpy>=1.20.0\",\n    \"diskcache>=5.6.1\",\n    \"jinja2>=2.11.3\",\n    \"librosa>=0.8.0\",\n    \"fastapi\",\n    \"uvicorn\",\n    \"pydantic\",\n    \"pillow\",\n    \"huggingface_hub\",\n    \"modelscope\",\n    \"prompt_toolkit\",\n    \"tqdm\",                     # Shared dependencies\n    \"tabulate\",\n    \"streamlit>=1.37.1\",\n    \"streamlit-audiorec\",\n    \"python-multipart\",\n    \"cmake\"\n]\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.7\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Programming Language :: Python :: 3.13\",\n]\n\n[project.optional-dependencies]\nonnx = [\n    \"librosa\",\n    \"optimum[onnxruntime]\", # for CPU version\n    \"diffusers\",            # required for image generation\n    \"optuna\",\n    \"pydantic\",\n    \"PyYAML\",\n    \"requests\",\n    \"setuptools\",\n    \"soundfile\",\n    \"transformers\",\n    \"ttstokenizer\",\n]\n\neval = [\n    \"sympy\",\n    \"openai\",\n    \"antlr4-python3-runtime==4.11\",\n    \"rouge_score\",\n    \"evaluate\",\n    \"pytablewriter\",\n    \"sacrebleu\",\n    \"langdetect\",\n    \"immutabledict\",\n    \"hydra-core\",\n    \"psutil\",\n    \"typing-extensions\",\n    \"flatten_dict\",\n    \"colorlog\",\n    \"pandas\",\n    \"rich\",\n    \"codecarbon\",\n]\n\nconvert = [\n    \"nexa-gguf\",\n]\n\nsiglip = [\n    \"torch\",\n    \"transformers\",\n    \"sentencepiece\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/NexaAI/nexa-sdk\"\nIssues = \"https://github.com/NexaAI/nexa-sdk/issues\"\nDocumentation = \"https://docs.nexa4ai.com/\"\n\n[project.scripts]\nnexa-cli = \"nexa.cli.entry:main\"\nnexa = \"nexa.cli.entry:main\"\nnexaai = \"nexa.cli.entry:main\"\nnexai = \"nexa.cli.entry:main\"\n\n[tool.scikit-build]\nwheel.packages = [\n    \"nexa\",\n    \"nexa.cli\",\n    \"nexa.gguf\",\n    \"nexa.gguf.llama\",\n    \"nexa.gguf.sd\",\n    \"nexa.gguf.streamlit\",\n    \"nexa.gguf.server\",\n    \"nexa.gguf.converter\",\n    \"nexa.onnx\",\n    \"nexa.onnx.streamlit\",\n    \"nexa.onnx.server\",\n    \"nexa.eval\",\n    \"nexa.transformers\",\n]\nsdist.include = [\n    \"CMakeLists.txt\",\n    \"dependency/llama.cpp/*\",\n    \"dependency/stable-diffusion.cpp/*\",\n    \"dependency/bark.cpp/*\",\n]\nsdist.exclude = [\n    \".github\",\n    \"build\",\n    \"dist\",\n    \"nexa.egg-info\",\n    \"dependency/llama.cpp/build\",\n    \"dependency/stable-diffusion.cpp/build\",\n    \"dependency/bark.cpp/build\",\n]\nbuild.verbose = true\ncmake.build-type = \"Release\"\ncmake.version = \">=3.16\"\ncmake.args = [\n    \"-DCMAKE_BUILD_PARALLEL_LEVEL=16\",\n    \"-DSTABLE_DIFFUSION_BUILD=ON\",\n    \"-DLLAMA_BUILD=ON\",\n    \"-DBARK_BUILD=OFF\",\n]\n\n[tool.scikit-build.metadata.version]\nprovider = \"scikit_build_core.metadata.regex\"\ninput = \"nexa/__init__.py\"\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.6357421875,
          "content": "# Build tools\nbuild\nwheel\ntwine\ncmake # For building C++ extensions\n\n# Faster Whisper\nfaster_whisper\n\n# For ggml\ntyping-extensions>=4.5.0\nnumpy>=1.20.0\ndiskcache>=5.6.1\njinja2>=2.11.3\nlibrosa>=0.8.0\nfastapi\nuvicorn\npydantic\npillow\npython-multipart\nhuggingface_hub\nmodelscope\n\n# For onnx\noptimum[onnxruntime]  # for CPU version\ndiffusers  # required for image generation\noptuna\nPyYAML\nrequests\nsetuptools\nsoundfile\nstreamlit-audiorec\ntransformers\nttstokenizer\n\n# Shared dependencies\nprompt_toolkit\ntqdm\ntabulate\nstreamlit\n\n# eval\nsympy\nevaluate\npytablewriter\nsacrebleu\nlangdetect\nrouge_score\nimmutabledict\n\n# For SigLIP\ntorch\ntransformers\nsentencepiece"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "swift",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}