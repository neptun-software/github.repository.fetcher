{
  "metadata": {
    "timestamp": 1736560699295,
    "page": 358,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ymcui/Chinese-LLaMA-Alpaca-2",
      "stars": 7136,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0302734375,
          "content": "notebooks/** linguist-vendored\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.021484375,
          "content": ".DS_Store\n*/.DS_Store\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.8125,
          "content": "cff-version: 1.2.0\nmessage: \"If you find our resources useful, please cite our paper as below.\"\nauthors:\n- family-names: \"Cui\"\n  given-names: \"Yiming\"\n  orcid: \"https://orcid.org/0000-0002-2452-375X\"\n- family-names: \"Yang\"\n  given-names: \"Ziqing\"\n- family-names: \"Yao\"\n  given-names: \"Xin\"  \ntitle: \"Chinese LLaMA and Alpaca 2\"\nversion: 1.0\ndate-released: 2023-07-28\nurl: \"https://github.com/ymcui/Chinese-LLaMA-Alpaca-2\"\npreferred-citation: \n  type: article\n  authors:\n  - family-names: \"Cui\"\n    given-names: \"Yiming\"\n    orcid: \"https://orcid.org/0000-0002-2452-375X\"\n  - family-names: \"Yang\"\n    given-names: \"Ziqing\"\n  - family-names: \"Yao\"\n    given-names: \"Xin\"  \n  title: \"Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca\"\n  journal: \"arXiv pre-print\"\n  year: 2023\n  url: \"https://arxiv.org/abs/2304.08177\""
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.095703125,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023 Yiming Cui, Ziqing Yang, Xin Yao\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 44.3134765625,
          "content": "# [Chinese-LLaMA-Alpaca-3](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)é¡¹ç›®å¯åŠ¨ï¼\n\n[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](./README.md) | [**ğŸŒEnglish**](./README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki) | [**â“æé—®/Issues**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/issues) | [**ğŸ’¬è®¨è®º/Discussions**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/discussions) | [**âš”ï¸ç«æŠ€åœº/Arena**](http://llm-arena.ymcui.com/)\n\n<p align=\"center\">\n    <br>\n    <img src=\"./pics/banner.png\" width=\"800\"/>\n    <br>\n</p>\n<p align=\"center\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca-2.svg?color=blue&style=flat-square\">\n    <img alt=\"GitHub release (latest by date)\" src=\"https://img.shields.io/github/v/release/ymcui/Chinese-LLaMA-Alpaca-2\">\n    <img alt=\"GitHub top language\" src=\"https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca-2\">\n    <a href=\"https://app.codacy.com/gh/ymcui/Chinese-LLaMA-Alpaca-2/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade\"><img src=\"https://app.codacy.com/project/badge/Grade/1710faac5e634acaabfc26b0a778cdde\"/></a>\n</p>\n\n\næœ¬é¡¹ç›®åŸºäºMetaå‘å¸ƒçš„å¯å•†ç”¨å¤§æ¨¡å‹[Llama-2](https://github.com/facebookresearch/llama)å¼€å‘ï¼Œæ˜¯[ä¸­æ–‡LLaMA&Alpacaå¤§æ¨¡å‹](https://github.com/ymcui/Chinese-LLaMA-Alpaca)çš„ç¬¬äºŒæœŸé¡¹ç›®ï¼Œå¼€æºäº†**ä¸­æ–‡LLaMA-2åŸºåº§æ¨¡å‹å’ŒAlpaca-2æŒ‡ä»¤ç²¾è°ƒå¤§æ¨¡å‹**ã€‚è¿™äº›æ¨¡å‹**åœ¨åŸç‰ˆLlama-2çš„åŸºç¡€ä¸Šæ‰©å……å¹¶ä¼˜åŒ–äº†ä¸­æ–‡è¯è¡¨**ï¼Œä½¿ç”¨äº†å¤§è§„æ¨¡ä¸­æ–‡æ•°æ®è¿›è¡Œå¢é‡é¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡äº†ä¸­æ–‡åŸºç¡€è¯­ä¹‰å’ŒæŒ‡ä»¤ç†è§£èƒ½åŠ›ï¼Œç›¸æ¯”ä¸€ä»£ç›¸å…³æ¨¡å‹è·å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚ç›¸å…³æ¨¡å‹**æ”¯æŒFlashAttention-2è®­ç»ƒ**ã€‚æ ‡å‡†ç‰ˆæ¨¡å‹æ”¯æŒ4Kä¸Šä¸‹æ–‡é•¿åº¦ï¼Œ**é•¿ä¸Šä¸‹æ–‡ç‰ˆæ¨¡å‹æ”¯æŒ16Kã€64kä¸Šä¸‹æ–‡é•¿åº¦**ã€‚**RLHFç³»åˆ—æ¨¡å‹**ä¸ºæ ‡å‡†ç‰ˆæ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œäººç±»åå¥½å¯¹é½ç²¾è°ƒï¼Œç›¸æ¯”æ ‡å‡†ç‰ˆæ¨¡å‹åœ¨**æ­£ç¡®ä»·å€¼è§‚ä½“ç°**æ–¹é¢è·å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚\n\n#### æœ¬é¡¹ç›®ä¸»è¦å†…å®¹\n\n- ğŸš€ é’ˆå¯¹Llama-2æ¨¡å‹æ‰©å……äº†**æ–°ç‰ˆä¸­æ–‡è¯è¡¨**ï¼Œå¼€æºäº†ä¸­æ–‡LLaMA-2å’ŒAlpaca-2å¤§æ¨¡å‹\n- ğŸš€ å¼€æºäº†é¢„è®­ç»ƒè„šæœ¬ã€æŒ‡ä»¤ç²¾è°ƒè„šæœ¬ï¼Œç”¨æˆ·å¯æ ¹æ®éœ€è¦è¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹\n- ğŸš€ ä½¿ç”¨ä¸ªäººç”µè„‘çš„CPU/GPUå¿«é€Ÿåœ¨æœ¬åœ°è¿›è¡Œå¤§æ¨¡å‹é‡åŒ–å’Œéƒ¨ç½²ä½“éªŒ\n- ğŸš€ æ”¯æŒ[ğŸ¤—transformers](https://github.com/huggingface/transformers), [llama.cpp](https://github.com/ggerganov/llama.cpp), [text-generation-webui](https://github.com/oobabooga/text-generation-webui), [LangChain](https://github.com/hwchase17/langchain), [privateGPT](https://github.com/imartinez/privateGPT), [vLLM](https://github.com/vllm-project/vllm)ç­‰LLaMAç”Ÿæ€\n\n#### å·²å¼€æºçš„æ¨¡å‹\n\n\n- åŸºåº§æ¨¡å‹ï¼ˆ4Kä¸Šä¸‹æ–‡ï¼‰ï¼šChinese-LLaMA-2 (1.3B, 7B, 13B)\n- èŠå¤©æ¨¡å‹ï¼ˆ4Kä¸Šä¸‹æ–‡ï¼‰ï¼šChinese-Alpaca-2 (1.3B, 7B, 13B)\n- é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ï¼ˆ16K/64Kï¼‰ï¼š\n  - Chinese-LLaMA-2-16K (7B, 13B) ã€Chinese-Alpaca-2-16K (7B, 13B) \n  - Chinese-LLaMA-2-64K (7B)ã€Chinese-Alpaca-2-64K (7B)\n- åå¥½å¯¹é½æ¨¡å‹ï¼šChinese-Alpaca-2-RLHF (1.3B, 7B)\n\n\n![](./pics/screencast.gif)\n\n----\n\n[ä¸­æ–‡LLaMA&Alpacaå¤§æ¨¡å‹](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | [å¤šæ¨¡æ€ä¸­æ–‡LLaMA&Alpacaå¤§æ¨¡å‹](https://github.com/airaria/Visual-Chinese-LLaMA-Alpaca) | [å¤šæ¨¡æ€VLE](https://github.com/iflytek/VLE) | [ä¸­æ–‡MiniRBT](https://github.com/iflytek/MiniRBT) | [ä¸­æ–‡LERT](https://github.com/ymcui/LERT) | [ä¸­è‹±æ–‡PERT](https://github.com/ymcui/PERT) | [ä¸­æ–‡MacBERT](https://github.com/ymcui/MacBERT) | [ä¸­æ–‡ELECTRA](https://github.com/ymcui/Chinese-ELECTRA) | [ä¸­æ–‡XLNet](https://github.com/ymcui/Chinese-XLNet) | [ä¸­æ–‡BERT](https://github.com/ymcui/Chinese-BERT-wwm) | [çŸ¥è¯†è’¸é¦å·¥å…·TextBrewer](https://github.com/airaria/TextBrewer) | [æ¨¡å‹è£å‰ªå·¥å…·TextPruner](https://github.com/airaria/TextPruner) | [è’¸é¦è£å‰ªä¸€ä½“åŒ–GRAIN](https://github.com/airaria/GRAIN)\n\n\n## æ–°é—»\n\n**[2024/04/30] Chinese-LLaMA-Alpaca-3 å·²æ­£å¼å‘å¸ƒï¼Œå¼€æºåŸºäºLlama-3çš„Llama-3-Chinese-8Bå’ŒLlama-3-Chinese-8B-Instructï¼Œæ¨èæ‰€æœ‰ä¸€æœŸã€äºŒæœŸé¡¹ç›®ç”¨æˆ·å‡çº§è‡³ä¸‰ä»£æ¨¡å‹ï¼Œè¯·å‚é˜…ï¼šhttps://github.com/ymcui/Chinese-LLaMA-Alpaca-3**\n\n[2024/03/27] æœ¬é¡¹ç›®å·²å…¥é©»æœºå™¨ä¹‹å¿ƒSOTA!æ¨¡å‹å¹³å°ï¼Œæ¬¢è¿å…³æ³¨ï¼šhttps://sota.jiqizhixin.com/project/chinese-llama-alpaca-2\n\n[2024/01/23] æ·»åŠ æ–°ç‰ˆGGUFæ¨¡å‹ï¼ˆimatrixé‡åŒ–ï¼‰ã€AWQé‡åŒ–æ¨¡å‹ï¼Œæ”¯æŒvLLMä¸‹åŠ è½½YaRNé•¿ä¸Šä¸‹æ–‡æ¨¡å‹ã€‚è¯¦æƒ…æŸ¥çœ‹[ğŸ“š v4.1ç‰ˆæœ¬å‘å¸ƒæ—¥å¿—](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v4.1)\n\n[2023/12/29] å‘å¸ƒé•¿ä¸Šä¸‹æ–‡æ¨¡å‹Chinese-LLaMA-2-7B-64Kå’ŒChinese-Alpaca-2-7B-64Kï¼ŒåŒæ—¶å‘å¸ƒç»è¿‡äººç±»åå¥½å¯¹é½ï¼ˆRLHFï¼‰çš„Chinese-Alpaca-2-RLHFï¼ˆ1.3B/7Bï¼‰ã€‚è¯¦æƒ…æŸ¥çœ‹[ğŸ“š v4.0ç‰ˆæœ¬å‘å¸ƒæ—¥å¿—](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v4.0)\n\n[2023/09/01] å‘å¸ƒé•¿ä¸Šä¸‹æ–‡æ¨¡å‹Chinese-Alpaca-2-7B-16Kå’ŒChinese-Alpaca-2-13B-16Kï¼Œè¯¥æ¨¡å‹å¯ç›´æ¥åº”ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œä¾‹å¦‚privateGPTç­‰ã€‚è¯¦æƒ…æŸ¥çœ‹[ğŸ“š v3.1ç‰ˆæœ¬å‘å¸ƒæ—¥å¿—](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v3.1)\n\n[2023/08/25] å‘å¸ƒé•¿ä¸Šä¸‹æ–‡æ¨¡å‹Chinese-LLaMA-2-7B-16Kå’ŒChinese-LLaMA-2-13B-16Kï¼Œæ”¯æŒ16Kä¸Šä¸‹æ–‡ï¼Œå¹¶å¯é€šè¿‡NTKæ–¹æ³•è¿›ä¸€æ­¥æ‰©å±•è‡³24K+ã€‚è¯¦æƒ…æŸ¥çœ‹[ğŸ“š v3.0ç‰ˆæœ¬å‘å¸ƒæ—¥å¿—](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v3.0)\n\n[2023/08/14] å‘å¸ƒChinese-LLaMA-2-13Bå’ŒChinese-Alpaca-2-13Bï¼Œæ·»åŠ text-generation-webui/LangChain/privateGPTæ”¯æŒï¼Œæ·»åŠ CFG Samplingè§£ç æ–¹æ³•ç­‰ã€‚è¯¦æƒ…æŸ¥çœ‹[ğŸ“š v2.0ç‰ˆæœ¬å‘å¸ƒæ—¥å¿—](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v2.0)\n\n[2023/08/02] æ·»åŠ FlashAttention-2è®­ç»ƒæ”¯æŒï¼ŒåŸºäºvLLMçš„æ¨ç†åŠ é€Ÿæ”¯æŒï¼Œæä¾›é•¿å›å¤ç³»ç»Ÿæç¤ºè¯­æ¨¡æ¿ç­‰ã€‚è¯¦æƒ…æŸ¥çœ‹[ğŸ“š v1.1ç‰ˆæœ¬å‘å¸ƒæ—¥å¿—](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v1.1)\n\n[2023/07/31] æ­£å¼å‘å¸ƒChinese-LLaMA-2-7Bï¼ˆåŸºåº§æ¨¡å‹ï¼‰ï¼Œä½¿ç”¨120Gä¸­æ–‡è¯­æ–™å¢é‡è®­ç»ƒï¼ˆä¸ä¸€ä»£Plusç³»åˆ—ç›¸åŒï¼‰ï¼›è¿›ä¸€æ­¥é€šè¿‡5Mæ¡æŒ‡ä»¤æ•°æ®ç²¾è°ƒï¼ˆç›¸æ¯”ä¸€ä»£ç•¥å¾®å¢åŠ ï¼‰ï¼Œå¾—åˆ°Chinese-Alpaca-2-7Bï¼ˆæŒ‡ä»¤/chatæ¨¡å‹ï¼‰ã€‚è¯¦æƒ…æŸ¥çœ‹[ğŸ“š v1.0ç‰ˆæœ¬å‘å¸ƒæ—¥å¿—](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v1.0)\n\n[2023/07/19] ğŸš€å¯åŠ¨[ä¸­æ–‡LLaMA-2ã€Alpaca-2å¼€æºå¤§æ¨¡å‹é¡¹ç›®](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)\n\n\n## å†…å®¹å¯¼å¼•\n| ç« èŠ‚                                  | æè¿°                                                         |\n| ------------------------------------- | ------------------------------------------------------------ |\n| [ğŸ’ğŸ»â€â™‚ï¸æ¨¡å‹ç®€ä»‹](#æ¨¡å‹ç®€ä»‹) | ç®€è¦ä»‹ç»æœ¬é¡¹ç›®ç›¸å…³æ¨¡å‹çš„æŠ€æœ¯ç‰¹ç‚¹ |\n| [â¬æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)        | ä¸­æ–‡LLaMA-2ã€Alpaca-2å¤§æ¨¡å‹ä¸‹è½½åœ°å€          |\n| [ğŸ’»æ¨ç†ä¸éƒ¨ç½²](#æ¨ç†ä¸éƒ¨ç½²) | ä»‹ç»äº†å¦‚ä½•å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–å¹¶ä½¿ç”¨ä¸ªäººç”µè„‘éƒ¨ç½²å¹¶ä½“éªŒå¤§æ¨¡å‹ |\n| [ğŸ’¯ç³»ç»Ÿæ•ˆæœ](#ç³»ç»Ÿæ•ˆæœ) | ä»‹ç»äº†æ¨¡å‹åœ¨éƒ¨åˆ†ä»»åŠ¡ä¸Šçš„æ•ˆæœ    |\n| [ğŸ“è®­ç»ƒä¸ç²¾è°ƒ](#è®­ç»ƒä¸ç²¾è°ƒ) | ä»‹ç»äº†å¦‚ä½•è®­ç»ƒå’Œç²¾è°ƒä¸­æ–‡LLaMA-2ã€Alpaca-2å¤§æ¨¡å‹ |\n| [â“å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜) | ä¸€äº›å¸¸è§é—®é¢˜çš„å›å¤ |\n\n\n## æ¨¡å‹ç®€ä»‹\n\næœ¬é¡¹ç›®æ¨å‡ºäº†åŸºäºLlama-2çš„ä¸­æ–‡LLaMA-2ä»¥åŠAlpaca-2ç³»åˆ—æ¨¡å‹ï¼Œç›¸æ¯”[ä¸€æœŸé¡¹ç›®](https://github.com/ymcui/Chinese-LLaMA-Alpaca)å…¶ä¸»è¦ç‰¹ç‚¹å¦‚ä¸‹ï¼š\n\n#### ğŸ“– ç»è¿‡ä¼˜åŒ–çš„ä¸­æ–‡è¯è¡¨\n\n- åœ¨[ä¸€æœŸé¡¹ç›®](https://github.com/ymcui/Chinese-LLaMA-Alpaca)ä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹ä¸€ä»£LLaMAæ¨¡å‹çš„32Kè¯è¡¨æ‰©å±•äº†ä¸­æ–‡å­—è¯ï¼ˆLLaMAï¼š49953ï¼ŒAlpacaï¼š49954ï¼‰\n- åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬**é‡æ–°è®¾è®¡äº†æ–°è¯è¡¨**ï¼ˆå¤§å°ï¼š55296ï¼‰ï¼Œè¿›ä¸€æ­¥æå‡äº†ä¸­æ–‡å­—è¯çš„è¦†ç›–ç¨‹åº¦ï¼ŒåŒæ—¶ç»Ÿä¸€äº†LLaMA/Alpacaçš„è¯è¡¨ï¼Œé¿å…äº†å› æ··ç”¨è¯è¡¨å¸¦æ¥çš„é—®é¢˜ï¼Œä»¥æœŸè¿›ä¸€æ­¥æå‡æ¨¡å‹å¯¹ä¸­æ–‡æ–‡æœ¬çš„ç¼–è§£ç æ•ˆç‡\n\n#### âš¡ åŸºäºFlashAttention-2çš„é«˜æ•ˆæ³¨æ„åŠ›\n\n- [FlashAttention-2](https://github.com/Dao-AILab/flash-attention)æ˜¯é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶çš„ä¸€ç§å®ç°ï¼Œç›¸æ¯”å…¶ä¸€ä»£æŠ€æœ¯å…·æœ‰**æ›´å¿«çš„é€Ÿåº¦å’Œæ›´ä¼˜åŒ–çš„æ˜¾å­˜å ç”¨**\n- å½“ä¸Šä¸‹æ–‡é•¿åº¦æ›´é•¿æ—¶ï¼Œä¸ºäº†é¿å…æ˜¾å­˜çˆ†ç‚¸å¼çš„å¢é•¿ï¼Œä½¿ç”¨æ­¤ç±»é«˜æ•ˆæ³¨æ„åŠ›æŠ€æœ¯å°¤ä¸ºé‡è¦\n- æœ¬é¡¹ç›®çš„æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨äº†FlashAttention-2æŠ€æœ¯è¿›è¡Œè®­ç»ƒ\n\n#### ğŸš„ åŸºäºPIå’ŒYaRNçš„è¶…é•¿ä¸Šä¸‹æ–‡æ‰©å±•æŠ€æœ¯\n\n- åœ¨[ä¸€æœŸé¡¹ç›®](https://github.com/ymcui/Chinese-LLaMA-Alpaca)ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†[åŸºäºNTKçš„ä¸Šä¸‹æ–‡æ‰©å±•æŠ€æœ¯](https://github.com/ymcui/Chinese-LLaMA-Alpaca/pull/743)ï¼Œå¯åœ¨ä¸ç»§ç»­è®­ç»ƒæ¨¡å‹çš„æƒ…å†µä¸‹æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡\n- åŸºäº[ä½ç½®æ’å€¼PI](https://arxiv.org/abs/2306.15595)å’ŒNTKç­‰æ–¹æ³•æ¨å‡ºäº†16Ké•¿ä¸Šä¸‹æ–‡ç‰ˆæ¨¡å‹ï¼Œæ”¯æŒ16Kä¸Šä¸‹æ–‡ï¼Œå¹¶å¯é€šè¿‡NTKæ–¹æ³•æœ€é«˜æ‰©å±•è‡³24K-32K\n- åŸºäº[YaRN](https://arxiv.org/abs/2309.00071)æ–¹æ³•è¿›ä¸€æ­¥æ¨å‡ºäº†64Ké•¿ä¸Šä¸‹æ–‡ç‰ˆæ¨¡å‹ï¼Œæ”¯æŒ64Kä¸Šä¸‹æ–‡\n- è¿›ä¸€æ­¥è®¾è®¡äº†**æ–¹ä¾¿çš„è‡ªé€‚åº”ç»éªŒå…¬å¼**ï¼Œæ— éœ€é’ˆå¯¹ä¸åŒçš„ä¸Šä¸‹æ–‡é•¿åº¦è®¾ç½®NTKè¶…å‚ï¼Œé™ä½äº†ä½¿ç”¨éš¾åº¦\n\n#### ğŸ¤– ç®€åŒ–çš„ä¸­è‹±åŒè¯­ç³»ç»Ÿæç¤ºè¯­\n\n- åœ¨[ä¸€æœŸé¡¹ç›®](https://github.com/ymcui/Chinese-LLaMA-Alpaca)ä¸­ï¼Œä¸­æ–‡Alpacaç³»åˆ—æ¨¡å‹ä½¿ç”¨äº†[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)çš„æŒ‡ä»¤æ¨¡æ¿å’Œç³»ç»Ÿæç¤ºè¯­\n- åˆæ­¥å®éªŒå‘ç°ï¼ŒLlama-2-Chatç³»åˆ—æ¨¡å‹çš„é»˜è®¤ç³»ç»Ÿæç¤ºè¯­æœªèƒ½å¸¦æ¥ç»Ÿè®¡æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸”å…¶å†…å®¹è¿‡äºå†—é•¿\n- æœ¬é¡¹ç›®ä¸­çš„Alpaca-2ç³»åˆ—æ¨¡å‹ç®€åŒ–äº†ç³»ç»Ÿæç¤ºè¯­ï¼ŒåŒæ—¶éµå¾ªLlama-2-ChatæŒ‡ä»¤æ¨¡æ¿ï¼Œä»¥ä¾¿æ›´å¥½åœ°é€‚é…ç›¸å…³ç”Ÿæ€\n\n#### ğŸ‘® äººç±»åå¥½å¯¹é½\n\n- åœ¨[ä¸€æœŸé¡¹ç›®](https://github.com/ymcui/Chinese-LLaMA-Alpaca)ä¸­ï¼Œä¸­æ–‡Alpacaç³»åˆ—æ¨¡å‹ä»…å®Œæˆé¢„è®­ç»ƒå’ŒæŒ‡ä»¤ç²¾è°ƒï¼Œè·å¾—äº†åŸºæœ¬çš„å¯¹è¯èƒ½åŠ›\n- é€šè¿‡åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å®éªŒï¼Œå‘ç°å¯æ˜¾è‘—æå‡æ¨¡å‹ä¼ é€’æ­£ç¡®ä»·å€¼è§‚çš„èƒ½åŠ›\n- æœ¬é¡¹ç›®æ¨å‡ºäº†Alpaca-2-RLHFç³»åˆ—æ¨¡å‹ï¼Œä½¿ç”¨æ–¹å¼ä¸SFTæ¨¡å‹ä¸€è‡´\n\n\n\nä¸‹å›¾å±•ç¤ºäº†æœ¬é¡¹ç›®ä»¥åŠ[ä¸€æœŸé¡¹ç›®](https://github.com/ymcui/Chinese-LLaMA-Alpaca)æ¨å‡ºçš„æ‰€æœ‰å¤§æ¨¡å‹ä¹‹é—´çš„å…³ç³»ã€‚\n\n![](./pics/models.png)\n\n## æ¨¡å‹ä¸‹è½½\n\n### æ¨¡å‹é€‰æ‹©æŒ‡å¼•\n\nä»¥ä¸‹æ˜¯ä¸­æ–‡LLaMA-2å’ŒAlpaca-2æ¨¡å‹çš„å¯¹æ¯”ä»¥åŠå»ºè®®ä½¿ç”¨åœºæ™¯ã€‚**å¦‚éœ€èŠå¤©äº¤äº’ï¼Œè¯·é€‰æ‹©Alpacaè€Œä¸æ˜¯LLaMAã€‚**\n\n| å¯¹æ¯”é¡¹                | ä¸­æ–‡LLaMA-2                                            | ä¸­æ–‡Alpaca-2                                                 |\n| :-------------------- | :----------------------------------------------------: | :----------------------------------------------------------: |\n| æ¨¡å‹ç±»å‹ | **åŸºåº§æ¨¡å‹** | **æŒ‡ä»¤/Chatæ¨¡å‹ï¼ˆç±»ChatGPTï¼‰** |\n| å·²å¼€æºå¤§å° | 1.3Bã€7Bã€13B | 1.3Bã€7Bã€13B |\n| è®­ç»ƒç±»å‹     | Causal-LM (CLM)           | æŒ‡ä»¤ç²¾è°ƒ                                                     |\n| è®­ç»ƒæ–¹å¼ | 7Bã€13Bï¼šLoRA + å…¨é‡emb/lm-head<br/>1.3Bï¼šå…¨é‡ | 7Bã€13Bï¼šLoRA + å…¨é‡emb/lm-head<br/>1.3Bï¼šå…¨é‡ |\n| åŸºäºä»€ä¹ˆæ¨¡å‹è®­ç»ƒ | [åŸç‰ˆLlama-2](https://github.com/facebookresearch/llama)ï¼ˆéchatç‰ˆï¼‰ | ä¸­æ–‡LLaMA-2 |\n| è®­ç»ƒè¯­æ–™ | æ— æ ‡æ³¨é€šç”¨è¯­æ–™ï¼ˆ120Gçº¯æ–‡æœ¬ï¼‰ | æœ‰æ ‡æ³¨æŒ‡ä»¤æ•°æ®ï¼ˆ500ä¸‡æ¡ï¼‰ |\n| è¯è¡¨å¤§å°<sup>[1]</sup> | 55,296 | 55,296 |\n| ä¸Šä¸‹æ–‡é•¿åº¦<sup>[2]</sup> | æ ‡å‡†ç‰ˆï¼š4Kï¼ˆ12K-18Kï¼‰<br/>é•¿ä¸Šä¸‹æ–‡ç‰ˆï¼ˆPIï¼‰ï¼š16Kï¼ˆ24K-32Kï¼‰<br/>é•¿ä¸Šä¸‹æ–‡ç‰ˆï¼ˆYaRNï¼‰ï¼š64K | æ ‡å‡†ç‰ˆï¼š4Kï¼ˆ12K-18Kï¼‰<br/>é•¿ä¸Šä¸‹æ–‡ç‰ˆï¼ˆPIï¼‰ï¼š16Kï¼ˆ24K-32Kï¼‰<br/>é•¿ä¸Šä¸‹æ–‡ç‰ˆï¼ˆYaRNï¼‰ï¼š64K |\n| è¾“å…¥æ¨¡æ¿              | ä¸éœ€è¦                                                 | éœ€è¦å¥—ç”¨ç‰¹å®šæ¨¡æ¿<sup>[3]</sup>ï¼Œç±»ä¼¼Llama-2-Chat |\n| é€‚ç”¨åœºæ™¯            | æ–‡æœ¬ç»­å†™ï¼šç»™å®šä¸Šæ–‡ï¼Œè®©æ¨¡å‹ç”Ÿæˆä¸‹æ–‡            | æŒ‡ä»¤ç†è§£ï¼šé—®ç­”ã€å†™ä½œã€èŠå¤©ã€äº¤äº’ç­‰ |\n| ä¸é€‚ç”¨åœºæ™¯          | æŒ‡ä»¤ç†è§£ ã€å¤šè½®èŠå¤©ç­‰                                  |  æ–‡æœ¬æ— é™åˆ¶è‡ªç”±ç”Ÿæˆ                                                       |\n| åå¥½å¯¹é½          | æ—                                   |  RLHFç‰ˆæœ¬ï¼ˆ1.3Bã€7Bï¼‰                                          |\n\n> [!NOTE]\n> [1] *æœ¬é¡¹ç›®ä¸€ä»£æ¨¡å‹å’ŒäºŒä»£æ¨¡å‹çš„è¯è¡¨ä¸åŒï¼Œè¯·å‹¿æ··ç”¨ã€‚äºŒä»£LLaMAå’ŒAlpacaçš„è¯è¡¨ç›¸åŒã€‚*</br>\n> [2] *æ‹¬å·å†…è¡¨ç¤ºåŸºäºNTKä¸Šä¸‹æ–‡æ‰©å±•æ”¯æŒçš„æœ€å¤§é•¿åº¦ã€‚*</br>\n> [3] *Alpaca-2é‡‡ç”¨äº†Llama-2-chatç³»åˆ—æ¨¡æ¿ï¼ˆæ ¼å¼ç›¸åŒï¼Œæç¤ºè¯­ä¸åŒï¼‰ï¼Œè€Œä¸æ˜¯ä¸€ä»£Alpacaçš„æ¨¡æ¿ï¼Œè¯·å‹¿æ··ç”¨ã€‚*</br>\n> [4] *ä¸å»ºè®®å•ç‹¬ä½¿ç”¨1.3Bæ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡æŠ•æœºé‡‡æ ·æ­é…æ›´å¤§çš„æ¨¡å‹ï¼ˆ7Bã€13Bï¼‰ä½¿ç”¨ã€‚*</br>\n\n### å®Œæ•´æ¨¡å‹ä¸‹è½½\n\nä»¥ä¸‹æ˜¯å®Œæ•´ç‰ˆæ¨¡å‹ï¼Œç›´æ¥ä¸‹è½½å³å¯ä½¿ç”¨ï¼Œæ— éœ€å…¶ä»–åˆå¹¶æ­¥éª¤ã€‚æ¨èç½‘ç»œå¸¦å®½å……è¶³çš„ç”¨æˆ·ã€‚\n\n| æ¨¡å‹åç§°                  |   ç±»å‹   | å¤§å° |                    ä¸‹è½½åœ°å€                    |                    GGUF                    |\n| :------------------------ | :------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |\n| Chinese-LLaMA-2-13B | åŸºåº§æ¨¡å‹ | 24.7 GB | [[Baidu]](https://pan.baidu.com/s/1T3RqEUSmyg6ZuBwMhwSmoQ?pwd=e9qy) [[Google]](https://drive.google.com/drive/folders/1YNa5qJ0x59OEOI7tNODxea-1YvMPoH05?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-13b) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-13b-gguf) |\n| Chinese-LLaMA-2-7B | åŸºåº§æ¨¡å‹ | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/1E5NI3nlQpx1j8z3eIzbIlg?pwd=n8k3) [[Google]](https://drive.google.com/drive/folders/18pp4I-mvQxRA7b8vF9gP-2cH_ocnXVKh?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-7b) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-7b-gguf) |\n| Chinese-LLaMA-2-1.3B | åŸºåº§æ¨¡å‹ | 2.4 GB | [[Baidu]](https://pan.baidu.com/s/1hEuOCllnJJ5NMEZJf8OkRw?pwd=nwjg) [[Google]](https://drive.google.com/drive/folders/1Sd3PA_gs6JctXtBg5HwmHXh9GX93riMP?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-1.3b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-1.3b) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-1.3b-gguf) |\n| Chinese-Alpaca-2-13B | æŒ‡ä»¤æ¨¡å‹ | 24.7 GB | [[Baidu]](https://pan.baidu.com/s/1MT_Zlap1OtdYMgoBNTS3dg?pwd=9xja) [[Google]](https://drive.google.com/drive/folders/1MTsKlzR61xmbTR4hBWzQas_MOpUZsogN?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-13b) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-13b-gguf) |\n| Chinese-Alpaca-2-7B | æŒ‡ä»¤æ¨¡å‹ | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/1wxx-CdgbMupXVRBcaN4Slw?pwd=kpn9) [[Google]](https://drive.google.com/drive/folders/1JsJDVs7tE2y31PBNleBlDPsB7S0ZrY8d?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-7b) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-gguf) |\n| Chinese-Alpaca-2-1.3B | æŒ‡ä»¤æ¨¡å‹ | 2.4 GB | [[Baidu]](https://pan.baidu.com/s/1PD7Ng-ltOIdUGHNorveptA?pwd=ar1p) [[Google]](https://drive.google.com/drive/folders/1h6qOy-Unvqs1_CJ8uPp0eKC61Gbbn8n7?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-1.3b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-1.3b) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-1.3b-gguf) |\n\n#### é•¿ä¸Šä¸‹æ–‡ç‰ˆæ¨¡å‹\n\nä»¥ä¸‹æ˜¯é•¿ä¸Šä¸‹æ–‡ç‰ˆæ¨¡å‹ï¼Œ**æ¨èä»¥é•¿æ–‡æœ¬ä¸ºä¸»çš„ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨**ï¼Œå¦åˆ™å»ºè®®ä½¿ç”¨ä¸Šè¿°æ ‡å‡†ç‰ˆã€‚\n\n| æ¨¡å‹åç§°                  |   ç±»å‹   |  å¤§å°   |                           ä¸‹è½½åœ°å€                           |                             GGUF                             |\n| :------------------------ | :------: | :-----: | :----------------------------------------------------------: | :----------------------------------------------------------: |\n| Chinese-LLaMA-2-7B-64K ğŸ†•  | åŸºåº§æ¨¡å‹ | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/1ShDQ2FG2QUJrvfnxCn4hwQ?pwd=xe5k) [[Google]](https://drive.google.com/drive/folders/17l9xJx55L2YNpqt7NiLVQzOZ6fV4rzJ-?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-7b-64k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-7b-64k) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-7b-64k-gguf) |\n| Chinese-Alpaca-2-7B-64K ğŸ†• | æŒ‡ä»¤æ¨¡å‹ | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/1KBAr9PCGvX2oQkYfCuLEjw?pwd=sgp6) [[Google]](https://drive.google.com/drive/folders/13G_d5xcDnhtaMOaulj1BFiZbVoVwJ-Cu?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-64k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-7b-64k)  | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-64k-gguf) |\n| Chinese-LLaMA-2-13B-16K   | åŸºåº§æ¨¡å‹ | 24.7 GB | [[Baidu]](https://pan.baidu.com/s/1XWrh3Ru9x4UI4-XmocVT2w?pwd=f7ik) [[Google]](https://drive.google.com/drive/folders/1nii6lF0DgB1u81CnsE4cCK2jD5oq_OW-?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-13b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-13b-16k)  | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-13b-16k-gguf) |\n| Chinese-LLaMA-2-7B-16K    | åŸºåº§æ¨¡å‹ | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/1ZH7T7KU_up61ugarSIXw2g?pwd=pquq) [[Google]](https://drive.google.com/drive/folders/1Zc6jI5bl3myQbQsY79dWJJ8mP_fyf3iF?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-7b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-7b-16k)  | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-7b-16k-gguf) |\n| Chinese-Alpaca-2-13B-16K  | æŒ‡ä»¤æ¨¡å‹ | 24.7 GB | [[Baidu]](https://pan.baidu.com/s/1gIzRM1eg-Xx1xV-3nXW27A?pwd=qi7c) [[Google]](https://drive.google.com/drive/folders/1mOkYQCvEqtGoZ9DaIpYFweSkSia2Q0vl?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-13b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-13b-16k)  | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-13b-16k-gguf) |\n| Chinese-Alpaca-2-7B-16K   | æŒ‡ä»¤æ¨¡å‹ | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/1Qk3U1LyvMb1RSr5AbiatPw?pwd=bfis) [[Google]](https://drive.google.com/drive/folders/1KBRSd2xAhiVQmamfA5wpm5ovYFRKuMdr?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-7b-16k)  | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-16k-gguf) |\n\n#### RLHFç‰ˆæ¨¡å‹\n\nä»¥ä¸‹æ˜¯äººç±»åå¥½å¯¹é½ç‰ˆæ¨¡å‹ï¼Œå¯¹æ¶‰åŠæ³•å¾‹ã€é“å¾·çš„é—®é¢˜è¾ƒæ ‡å‡†ç‰ˆæœ‰æ›´ä¼˜çš„ä»·å€¼å¯¼å‘ã€‚\n\n| æ¨¡å‹åç§°                  |   ç±»å‹   | å¤§å° |                    ä¸‹è½½åœ°å€                    |                    GGUF                    |\n| :------------------------ | :------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |\n| Chinese-Alpaca-2-7B-RLHF ğŸ†• | æŒ‡ä»¤æ¨¡å‹ | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/17GJ1y4rpPDuvWlvPaWgnqw?pwd=4feb) [[Google]](https://drive.google.com/drive/folders/1OHZVVtwM5McVEIZzyOYgGYLAxcZNVK4D?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-rlhf) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-7b-rlhf) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-rlhf-gguf) |\n| Chinese-Alpaca-2-1.3B-RLHF ğŸ†• | æŒ‡ä»¤æ¨¡å‹ | 2.4 GB | [[Baidu]](https://pan.baidu.com/s/1cLKJKieNitWbOggUXXaamw?pwd=cprp) [[Google]](https://drive.google.com/drive/folders/1zcvPUPPkq69SgqRu6YBurAZ9ptcPSZNx?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-1.3b-rlhf) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-1.3b-rlhf) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-1.3b-rlhf-gguf) |\n\n#### AWQç‰ˆæ¨¡å‹\n\nAWQï¼ˆActivation-aware Weight Quantizationï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„æ¨¡å‹é‡åŒ–æ–¹æ¡ˆï¼Œç›®å‰å¯å…¼å®¹ğŸ¤—transformersã€llama.cppç­‰ä¸»æµæ¡†æ¶ã€‚\n\næœ¬é¡¹ç›®æ¨¡å‹çš„AWQé¢„æœç´¢ç»“æœå¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼šhttps://huggingface.co/hfl/chinese-llama-alpaca-2-awq\n\n- ç”ŸæˆAWQé‡åŒ–æ¨¡å‹ï¼ˆAWQå®˜æ–¹ç›®å½•ï¼‰ï¼šhttps://github.com/mit-han-lab/llm-awq\n- llama.cppä¸­ä½¿ç”¨AWQï¼šhttps://github.com/ggerganov/llama.cpp/tree/master/awq-py\n\n### LoRAæ¨¡å‹ä¸‹è½½\n\nä»¥ä¸‹æ˜¯LoRAæ¨¡å‹ï¼ˆå«emb/lm-headï¼‰ï¼Œä¸ä¸Šè¿°å®Œæ•´æ¨¡å‹ä¸€ä¸€å¯¹åº”ã€‚éœ€è¦æ³¨æ„çš„æ˜¯**LoRAæ¨¡å‹æ— æ³•ç›´æ¥ä½¿ç”¨**ï¼Œå¿…é¡»æŒ‰ç…§æ•™ç¨‹ä¸é‡æ„æ¨¡å‹è¿›è¡Œåˆå¹¶ã€‚æ¨èç½‘ç»œå¸¦å®½ä¸è¶³ï¼Œæ‰‹å¤´æœ‰åŸç‰ˆLlama-2ä¸”éœ€è¦è½»é‡ä¸‹è½½çš„ç”¨æˆ·ã€‚\n\n| æ¨¡å‹åç§°                  |   ç±»å‹   |                   åˆå¹¶æ‰€éœ€åŸºæ¨¡å‹                   | å¤§å° |                    LoRAä¸‹è½½åœ°å€                    |\n| :------------------------ | :------: | :--------------------------------------------------------: | :----------------: | :----------------------------------------------------------: |\n| Chinese-LLaMA-2-LoRA-13B | åŸºåº§æ¨¡å‹ | [Llama-2-13B-hf](https://huggingface.co/meta-llama/Llama-2-13b-hf) | 1.5 GB | [[Baidu]](https://pan.baidu.com/s/1PFKTBn54GjAjzWeQISKruw?pwd=we6s) [[Google]](https://drive.google.com/file/d/10Z_k9A9N9D_6RHrMTmbHQRCuI6s1iMb1/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-lora-13b) | \n| Chinese-LLaMA-2-LoRA-7B | åŸºåº§æ¨¡å‹ |        [Llama-2-7B-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)        | 1.1 GB | [[Baidu]](https://pan.baidu.com/s/1bmgqdyRh9E3a2uqOGyNqiQ?pwd=7kvq) [[Google]](https://drive.google.com/file/d/1njJGSU_PRbzjYRNw5RSbC5-4fBOXTVY3/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-lora-7b) |\n| Chinese-Alpaca-2-LoRA-13B | æŒ‡ä»¤æ¨¡å‹ | [Llama-2-13B-hf](https://huggingface.co/meta-llama/Llama-2-13b-hf) | 1.5 GB | [[Baidu]](https://pan.baidu.com/s/1Y5giIXOUUzI4Na6JOcviVA?pwd=tc2j) [[Google]](https://drive.google.com/file/d/1z2FIInsYJBTXipgztc-Mv7kkeqscx442/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-lora-13b) | \n| Chinese-Alpaca-2-LoRA-7B | æŒ‡ä»¤æ¨¡å‹ | [Llama-2-7B-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) | 1.1 GB | [[Baidu]](https://pan.baidu.com/s/1g0olPxkB_rlZ9UUVfOnbcw?pwd=5e7w) [[Google]](https://drive.google.com/file/d/1MzJL-ZIzdJW7MIcAiYIDIDJ5dlMi8Kkk/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-lora-7b) | \n\nä»¥ä¸‹æ˜¯é•¿ä¸Šä¸‹æ–‡ç‰ˆæ¨¡å‹ï¼Œ**æ¨èä»¥é•¿æ–‡æœ¬ä¸ºä¸»çš„ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨**ï¼Œå¦åˆ™å»ºè®®ä½¿ç”¨ä¸Šè¿°æ ‡å‡†ç‰ˆã€‚\n\n| æ¨¡å‹åç§°                  |   ç±»å‹   |                   åˆå¹¶æ‰€éœ€åŸºæ¨¡å‹                   | å¤§å° |                    LoRAä¸‹è½½åœ°å€                    |\n| :------------------------ | :------: | :--------------------------------------------------------: | :----------------: | :----------------------------------------------------------: |\n| Chinese-LLaMA-2-LoRA-7B-64K ğŸ†• | åŸºåº§æ¨¡å‹ | [Llama-2-7B-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) | 1.1 GB | [[Baidu]](https://pan.baidu.com/s/1QjqKNM9Xez5g6koUrbII_w?pwd=94pk) [[Google]](https://drive.google.com/file/d/1-NuGqfduUZARRquFjGLpTmI5J-HlXYSR/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-lora-7b-64k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-lora-7b-64k) |\n| Chinese-Alpaca-2-LoRA-7B-64K ğŸ†• | æŒ‡ä»¤æ¨¡å‹ | [Llama-2-7B-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) | 1.1 GB | [[Baidu]](https://pan.baidu.com/s/1t6bPpMlJCrs9Ce7LXs09-w?pwd=37it) [[Google]](https://drive.google.com/file/d/1qESorx2PHtIsnj53JJ7XBsdOGHuLNjoI/view?usp=sharing) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-lora-7b-64k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-lora-7b-64k) |\n| Chinese-LLaMA-2-LoRA-13B-16K | åŸºåº§æ¨¡å‹ | [Llama-2-13B-hf](https://huggingface.co/meta-llama/Llama-2-13b-hf) | 1.5 GB | [[Baidu]](https://pan.baidu.com/s/1VrfOJmhDnXxrXcdnfX00fA?pwd=4t2j) [[Google]](https://drive.google.com/file/d/1mSpigmHcN9YX1spa4QN3IPtx43Vfs55H/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-lora-13b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-lora-13b-16k) |\n| Chinese-LLaMA-2-LoRA-7B-16K | åŸºåº§æ¨¡å‹ |        [Llama-2-7B-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)        | 1.1 GB | [[Baidu]](https://pan.baidu.com/s/14Jnm7QmcDx3XsK_NHZz6Uw?pwd=5b7i) [[Google]](https://drive.google.com/file/d/1yUdyQuBMAmxmUEAvGiKbjKuxTYPPI-or/view?usp=sharing) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-lora-7b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-lora-7b-16k) |\n| Chinese-Alpaca-2-LoRA-13B-16K | æŒ‡ä»¤æ¨¡å‹ | [Llama-2-13B-hf](https://huggingface.co/meta-llama/Llama-2-13b-hf) | 1.5 GB | [[Baidu]](https://pan.baidu.com/s/1g42_X7Z0QWDyrrDqv2jifQ?pwd=bq7n) [[Google]](https://drive.google.com/file/d/1ppGNyMWnuLDcClXN7DBTbKxVehsn3Gd2/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-lora-13b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-lora-13b-16k) |\n| Chinese-Alpaca-2-LoRA-7B-16K | æŒ‡ä»¤æ¨¡å‹ | [Llama-2-7B-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) | 1.1 GB | [[Baidu]](https://pan.baidu.com/s/1E7GEZ6stp8EavhkhR06FwA?pwd=ewwy) [[Google]](https://drive.google.com/file/d/1GTgDNfMdcQhHEAfMPaP-EOEk_fwDvNEK/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-lora-7b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-lora-7b-16k) |\n\n\n> [!IMPORTANT] \n> LoRAæ¨¡å‹æ— æ³•å•ç‹¬ä½¿ç”¨ï¼Œå¿…é¡»ä¸åŸç‰ˆLlama-2è¿›è¡Œåˆå¹¶æ‰èƒ½è½¬ä¸ºå®Œæ•´æ¨¡å‹ã€‚è¯·é€šè¿‡ä»¥ä¸‹æ–¹æ³•å¯¹æ¨¡å‹è¿›è¡Œåˆå¹¶ã€‚\n>\n> - [**åœ¨çº¿è½¬æ¢**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/online_conversion_zh)ï¼šColabç”¨æˆ·å¯åˆ©ç”¨æœ¬é¡¹ç›®æä¾›çš„notebookè¿›è¡Œåœ¨çº¿è½¬æ¢å¹¶é‡åŒ–æ¨¡å‹\n> - [**æ‰‹åŠ¨è½¬æ¢**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/manual_conversion_zh)ï¼šç¦»çº¿æ–¹å¼è½¬æ¢ï¼Œç”Ÿæˆä¸åŒæ ¼å¼çš„æ¨¡å‹ï¼Œä»¥ä¾¿è¿›è¡Œé‡åŒ–æˆ–è¿›ä¸€æ­¥ç²¾è°ƒ\n\n\n## æ¨ç†ä¸éƒ¨ç½²\n\næœ¬é¡¹ç›®ä¸­çš„ç›¸å…³æ¨¡å‹ä¸»è¦æ”¯æŒä»¥ä¸‹é‡åŒ–ã€æ¨ç†å’Œéƒ¨ç½²æ–¹å¼ï¼Œå…·ä½“å†…å®¹è¯·å‚è€ƒå¯¹åº”æ•™ç¨‹ã€‚\n\n| å·¥å…·   | ç‰¹ç‚¹     | CPU  | GPU  | é‡åŒ– | GUI  | API  | vLLM<sup>Â§</sup> |   16K<sup>â€¡</sup>    | 64K<sup>â€¡</sup>    |æŠ•æœºé‡‡æ · |                      æ•™ç¨‹                             |\n| :----------------------------------------------------------- | ---------------------------- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |:--: |\n| [**llama.cpp**](https://github.com/ggerganov/llama.cpp)      | ä¸°å¯Œçš„é‡åŒ–é€‰é¡¹å’Œé«˜æ•ˆæœ¬åœ°æ¨ç† |  âœ…   |  âœ…   |  âœ…   |  âŒ   |  âœ…   |  âŒ   | âœ… |âœ…   |âœ… | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/llamacpp_zh) |\n| [**ğŸ¤—Transformers**](https://github.com/huggingface/transformers) | åŸç”Ÿtransformersæ¨ç†æ¥å£     |  âœ…   |  âœ…   |  âœ…   |  âœ…   |  âŒ   |  âœ…  | âœ… | âœ… | âœ… | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/inference_with_transformers_zh) |\n| [**Colab Demo**](https://colab.research.google.com/drive/1yu0eZ3a66by8Zqm883LLtRQrguBAb9MR?usp=sharing) | åœ¨Colabä¸­å¯åŠ¨äº¤äº’ç•Œé¢ | âœ… | âœ… | âœ… | âœ… | âŒ | âœ… | âœ… | âœ… | âœ… | [link](https://colab.research.google.com/drive/1yu0eZ3a66by8Zqm883LLtRQrguBAb9MR?usp=sharing) |\n| [**ä»¿OpenAI APIè°ƒç”¨**](https://platform.openai.com/docs/api-reference) | ä»¿OpenAI APIæ¥å£çš„æœåŠ¡å™¨Demo |  âœ…   |  âœ…   |  âœ…   |  âŒ   |  âœ…   |  âœ…  | âœ… | âœ… | âŒ | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/api_calls_zh) |\n| [**text-generation-webui**](https://github.com/oobabooga/text-generation-webui) | å‰ç«¯Web UIç•Œé¢çš„éƒ¨ç½²æ–¹å¼ |  âœ…   |  âœ…   |  âœ…   |  âœ…   |  âœ…<sup>â€ </sup>  | âŒ  | âœ… | âŒ  | âŒ | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/text-generation-webui_zh) |\n| [**LangChain**](https://github.com/hwchase17/langchain) | é€‚åˆäºŒæ¬¡å¼€å‘çš„å¤§æ¨¡å‹åº”ç”¨å¼€æºæ¡†æ¶ |  âœ…<sup>â€ </sup>  |  âœ…   |  âœ…<sup>â€ </sup>   |  âŒ   |  âŒ   | âŒ  | âœ… | âœ… | âŒ | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/langchain_zh) |\n| [**privateGPT**](https://github.com/imartinez/privateGPT) | åŸºäºLangChainçš„å¤šæ–‡æ¡£æœ¬åœ°é—®ç­”æ¡†æ¶ | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âœ… | âŒ  | âŒ | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/privategpt_zh) |\n\n> [!NOTE]\n> <sup>â€ </sup> å·¥å…·æ”¯æŒè¯¥ç‰¹æ€§ï¼Œä½†æ•™ç¨‹ä¸­æœªå®ç°ï¼Œè¯¦ç»†è¯´æ˜è¯·å‚è€ƒå¯¹åº”å®˜æ–¹æ–‡æ¡£<br/>\n> <sup>â€¡</sup> æŒ‡æ˜¯å¦æ”¯æŒé•¿ä¸Šä¸‹æ–‡ç‰ˆæœ¬æ¨¡å‹ï¼ˆéœ€è¦ç¬¬ä¸‰æ–¹åº“æ”¯æŒè‡ªå®šä¹‰RoPEï¼‰<br/>\n> <sup>Â§</sup> vLLMåç«¯ä¸æ”¯æŒé•¿ä¸Šä¸‹æ–‡ç‰ˆæœ¬æ¨¡å‹<br/>\n\n\n## ç³»ç»Ÿæ•ˆæœ\n\nä¸ºäº†è¯„æµ‹ç›¸å…³æ¨¡å‹çš„æ•ˆæœï¼Œæœ¬é¡¹ç›®åˆ†åˆ«è¿›è¡Œäº†ç”Ÿæˆæ•ˆæœè¯„æµ‹å’Œå®¢è§‚æ•ˆæœè¯„æµ‹ï¼ˆNLUç±»ï¼‰ï¼Œä»ä¸åŒè§’åº¦å¯¹å¤§æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç»¼åˆè¯„ä¼°å¤§æ¨¡å‹èƒ½åŠ›ä»ç„¶æ˜¯äºŸå¾…è§£å†³çš„é‡è¦è¯¾é¢˜ï¼Œå•ä¸ªæ•°æ®é›†çš„ç»“æœå¹¶ä¸èƒ½ç»¼åˆè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æ¨èç”¨æˆ·åœ¨è‡ªå·±å…³æ³¨çš„ä»»åŠ¡ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œé€‰æ‹©é€‚é…ç›¸å…³ä»»åŠ¡çš„æ¨¡å‹ã€‚\n\n### ç”Ÿæˆæ•ˆæœè¯„æµ‹\n\nä¸ºäº†æ›´åŠ ç›´è§‚åœ°äº†è§£æ¨¡å‹çš„ç”Ÿæˆæ•ˆæœï¼Œæœ¬é¡¹ç›®ä»¿ç…§[Fastchat Chatbot Arena](https://chat.lmsys.org/?arena)æ¨å‡ºäº†æ¨¡å‹åœ¨çº¿å¯¹æˆ˜å¹³å°ï¼Œå¯æµè§ˆå’Œè¯„æµ‹æ¨¡å‹å›å¤è´¨é‡ã€‚å¯¹æˆ˜å¹³å°æä¾›äº†èƒœç‡ã€Eloè¯„åˆ†ç­‰è¯„æµ‹æŒ‡æ ‡ï¼Œå¹¶ä¸”å¯ä»¥æŸ¥çœ‹ä¸¤ä¸¤æ¨¡å‹çš„å¯¹æˆ˜èƒœç‡ç­‰ç»“æœã€‚é¢˜åº“æ¥è‡ªäº[ä¸€æœŸé¡¹ç›®äººå·¥åˆ¶ä½œçš„200é¢˜](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples/f16-p7b-p13b-33b)ï¼Œä»¥åŠåœ¨æ­¤åŸºç¡€ä¸Šé¢å¤–å¢åŠ çš„é¢˜ç›®ã€‚ç”Ÿæˆå›å¤å…·æœ‰éšæœºæ€§ï¼Œå—è§£ç è¶…å‚ã€éšæœºç§å­ç­‰å› ç´ å½±å“ï¼Œå› æ­¤ç›¸å…³è¯„æµ‹å¹¶éç»å¯¹ä¸¥è°¨ï¼Œç»“æœä»…ä¾›æ™¾æ™’å‚è€ƒï¼Œæ¬¢è¿è‡ªè¡Œä½“éªŒã€‚éƒ¨åˆ†ç”Ÿæˆæ ·ä¾‹è¯·æŸ¥çœ‹[examplesç›®å½•](./examples)ã€‚\n\n**âš”ï¸ æ¨¡å‹ç«æŠ€åœºï¼š[http://llm-arena.ymcui.com](http://llm-arena.ymcui.com/)**\n\n| ç³»ç»Ÿ                                                         | å¯¹æˆ˜èƒœç‡ï¼ˆæ— å¹³å±€ï¼‰ â†“ | Eloè¯„åˆ† |\n| ------------------------------------------------------------ | :------------------: | :-----: |\n| **Chinese-Alpaca-2-13B-16K**                                 |        86.84%        |  1580   |\n| **Chinese-Alpaca-2-13B**                                     |        72.01%        |  1579   |\n| [Chinese-Alpaca-Pro-33B](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |        64.87%        |  1548   |\n| **Chinese-Alpaca-2-7B**                                      |        64.11%        |  1572   |\n| [Chinese-Alpaca-Pro-7B](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |        62.05%        |  1500   |\n| **Chinese-Alpaca-2-7B-16K**                                  |        61.67%        |  1540   |\n| [Chinese-Alpaca-Pro-13B](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |        61.26%        |  1567   |\n| [Chinese-Alpaca-Plus-33B](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |        31.29%        |  1401   |\n| [Chinese-Alpaca-Plus-13B](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |        23.43%        |  1329   |\n| [Chinese-Alpaca-Plus-7B](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |        20.92%        |  1379   |\n\n> [!NOTE]\n> ä»¥ä¸Šç»“æœæˆªè‡³2023å¹´9æœˆ1æ—¥ã€‚æœ€æ–°ç»“æœè¯·è¿›å…¥[**âš”ï¸ç«æŠ€åœº**](http://llm-arena.ymcui.com/)è¿›è¡ŒæŸ¥çœ‹ã€‚\n\n\n### å®¢è§‚æ•ˆæœè¯„æµ‹ï¼šC-Eval\n\n[C-Eval](https://cevalbenchmark.com)æ˜¯ä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„ä¼°å¥—ä»¶ï¼Œå…¶ä¸­éªŒè¯é›†å’Œæµ‹è¯•é›†åˆ†åˆ«åŒ…å«1.3Kå’Œ12.3Kä¸ªé€‰æ‹©é¢˜ï¼Œæ¶µç›–52ä¸ªå­¦ç§‘ã€‚å®éªŒç»“æœä»¥â€œzero-shot / 5-shotâ€è¿›è¡Œå‘ˆç°ã€‚C-Evalæ¨ç†ä»£ç è¯·å‚è€ƒæœ¬é¡¹ç›®ï¼š[ğŸ“–GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/ceval_zh)\n\n| LLaMA Models            |    Valid    |    Test     | Alpaca Models            |    Valid    |    Test     |\n| ----------------------- | :---------: | :---------: | ------------------------ | :---------: | :---------: |\n| **Chinese-LLaMA-2-13B** | 40.6 / 42.7 | 38.0 / 41.6 | **Chinese-Alpaca-2-13B** | 44.3 / 45.9 | 42.6 / 44.0 |\n| **Chinese-LLaMA-2-7B**  | 28.2 / 36.0 | 30.3 / 34.2 | **Chinese-Alpaca-2-7B**  | 41.3 / 42.9 | 40.3 / 39.5 |\n| Chinese-LLaMA-Plus-33B  | 37.4 / 40.0 | 35.7 / 38.3 | Chinese-Alpaca-Plus-33B  | 46.5 / 46.3 | 44.9 / 43.5 |\n| Chinese-LLaMA-Plus-13B  | 27.3 / 34.0 | 27.8 / 33.3 | Chinese-Alpaca-Plus-13B  | 43.3 / 42.4 | 41.5 / 39.9 |\n| Chinese-LLaMA-Plus-7B   | 27.3 / 28.3 | 26.9 / 28.4 | Chinese-Alpaca-Plus-7B   | 36.7 / 32.9 | 36.4 / 32.3 |\n\n### å®¢è§‚æ•ˆæœè¯„æµ‹ï¼šCMMLU\n\n[CMMLU](https://github.com/haonan-li/CMMLU)æ˜¯å¦ä¸€ä¸ªç»¼åˆæ€§ä¸­æ–‡è¯„æµ‹æ•°æ®é›†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡è¯­å¢ƒä¸‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œæ¶µç›–äº†ä»åŸºç¡€å­¦ç§‘åˆ°é«˜çº§ä¸“ä¸šæ°´å¹³çš„67ä¸ªä¸»é¢˜ï¼Œå…±è®¡11.5Kä¸ªé€‰æ‹©é¢˜ã€‚CMMLUæ¨ç†ä»£ç è¯·å‚è€ƒæœ¬é¡¹ç›®ï¼š[ğŸ“–GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/cmmlu_zh)\n\n| LLaMA Models            | Test (0/few-shot) | Alpaca Models            | Test (0/few-shot) |\n| ----------------------- | :---------------: | ------------------------ | :---------------: |\n| **Chinese-LLaMA-2-13B** |    38.9 / 42.5    | **Chinese-Alpaca-2-13B** |    43.2 / 45.5    |\n| **Chinese-LLaMA-2-7B**  |    27.9 / 34.1    | **Chinese-Alpaca-2-7B**  |    40.0 / 41.8    |\n| Chinese-LLaMA-Plus-33B  |    35.2 / 38.8    | Chinese-Alpaca-Plus-33B  |    46.6 / 45.3    |\n| Chinese-LLaMA-Plus-13B  |    29.6 / 34.0    | Chinese-Alpaca-Plus-13B  |    40.6 / 39.9    |\n| Chinese-LLaMA-Plus-7B   |    25.4 / 26.3    | Chinese-Alpaca-Plus-7B   |    36.8 / 32.6    |\n\n### é•¿ä¸Šä¸‹æ–‡ç‰ˆæ¨¡å‹è¯„æµ‹\n\n[LongBench](https://github.com/THUDM/LongBench)æ˜¯ä¸€ä¸ªå¤§æ¨¡å‹é•¿æ–‡æœ¬ç†è§£èƒ½åŠ›çš„è¯„æµ‹åŸºå‡†ï¼Œç”±6å¤§ç±»ã€20ä¸ªä¸åŒçš„ä»»åŠ¡ç»„æˆï¼Œå¤šæ•°ä»»åŠ¡çš„å¹³å‡é•¿åº¦åœ¨5K-15Kä¹‹é—´ï¼Œå…±åŒ…å«çº¦4.75Kæ¡æµ‹è¯•æ•°æ®ã€‚ä»¥ä¸‹æ˜¯æœ¬é¡¹ç›®é•¿ä¸Šä¸‹æ–‡ç‰ˆæ¨¡å‹åœ¨è¯¥ä¸­æ–‡ä»»åŠ¡ï¼ˆå«ä»£ç ä»»åŠ¡ï¼‰ä¸Šçš„è¯„æµ‹æ•ˆæœã€‚LongBenchæ¨ç†ä»£ç è¯·å‚è€ƒæœ¬é¡¹ç›®ï¼š[ğŸ“–GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/longbench_zh)\n\n| Models                       | å•æ–‡æ¡£QA | å¤šæ–‡æ¡£QA | æ‘˜è¦ | Few-shotå­¦ä¹  | ä»£ç è¡¥å…¨ | åˆæˆä»»åŠ¡ | Avg  |\n| ---------------------------- | :------: | :------: | :--: | :----------: | :------: | :------: | :--: |\n| **Chinese-Alpaca-2-7B-64K** | 44.7  |  28.1 | 14.4 |  39.0   |  44.6  |   5.0  | 29.3|\n| **Chinese-LLaMA-2-7B-64K** | 27.2  |  16.4 | 6.5 |  33.0   |  7.8  |   5.0  | 16.0|\n| **Chinese-Alpaca-2-13B-16K** |   47.9  |   26.7 | 13.0 |     22.3    |   46.6   |   21.5   | 29.7 |\n| Chinese-Alpaca-2-13B         |   38.4   |   20.0   | 11.9 |     17.3    |   46.5   |   8.0    | 23.7 |\n| **Chinese-Alpaca-2-7B-16K**  |   46.4  |   23.3  | 14.3 |     29.0     |   49.6   |   9.0    | 28.6 |\n| Chinese-Alpaca-2-7B          |   34.0   |   17.4   | 11.8 |     21.3    |   50.3  |   4.5    | 23.2 |\n| **Chinese-LLaMA-2-13B-16K**  |   36.7   |   17.7  | 3.1 |     29.8     |   13.8   |   3.0    | 17.3 |\n| Chinese-LLaMA-2-13B          |   28.3   |   14.4   | 4.6 |     16.3     |   10.4   |   5.4    | 13.2 |\n| **Chinese-LLaMA-2-7B-16K**   |   33.2   |   15.9   | 6.5 |     23.5     |   10.3    |   5.3    | 15.8|\n| Chinese-LLaMA-2-7B           |   19.0   |   13.9   | 6.4  |     11.0    |   11.0   |   4.7    | 11.0 |\n\n### é‡åŒ–æ•ˆæœè¯„æµ‹\n\nä»¥Chinese-LLaMA-2-7Bä¸ºä¾‹ï¼Œå¯¹æ¯”ä¸åŒç²¾åº¦ä¸‹çš„æ¨¡å‹å¤§å°ã€PPLï¼ˆå›°æƒ‘åº¦ï¼‰ã€C-Evalæ•ˆæœï¼Œæ–¹ä¾¿ç”¨æˆ·äº†è§£é‡åŒ–ç²¾åº¦æŸå¤±ã€‚PPLä»¥4Kä¸Šä¸‹æ–‡å¤§å°è®¡ç®—ï¼ŒC-Evalæ±‡æŠ¥çš„æ˜¯validé›†åˆä¸Šzero-shotå’Œ5-shotç»“æœã€‚\n\n| ç²¾åº¦      | æ¨¡å‹å¤§å° |  PPL   |   C-Eval    |\n| :-------- | :------: | :----: | :---------: |\n| FP16      | 12.9 GB  | 9.373  | 28.2 / 36.0 |\n| 8-bité‡åŒ– |  6.8 GB  | 9.476  | 26.8 / 35.4 |\n| 4-bité‡åŒ– |  3.7 GB  | 10.132 | 25.5 / 32.8 |\n\nç‰¹åˆ«åœ°ï¼Œä»¥ä¸‹æ˜¯åœ¨llama.cppä¸‹ä¸åŒé‡åŒ–æ–¹æ³•çš„è¯„æµ‹æ•°æ®ï¼Œä¾›ç”¨æˆ·å‚è€ƒï¼Œé€Ÿåº¦ä»¥ms/tokè®¡ï¼Œæµ‹è¯•è®¾å¤‡ä¸ºM1 Maxã€‚å…·ä½“ç»†èŠ‚è§[ğŸ“–GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/llamacpp_zh#å…³äºé‡åŒ–æ–¹æ³•é€‰æ‹©åŠæ¨ç†é€Ÿåº¦)\n\n| llama.cpp |    F16 |   Q2_K |  Q3_K |  Q4_0 |  Q4_1 |  Q4_K |  Q5_0 |  Q5_1 |  Q5_K |  Q6_K |  Q8_0 |\n| --------- | -----: | -----: | ----: | ----: | ----: | ----: | ----: | ----: | ----: | ----: | ----: |\n| PPL       |  9.128 | 11.107 | 9.576 | 9.476 | 9.576 | 9.240 | 9.156 | 9.213 | 9.168 | 9.133 | 9.129 |\n| Size      | 12.91G |  2.41G | 3.18G | 3.69G | 4.08G | 3.92G | 4.47G | 4.86G | 4.59G | 5.30G | 6.81G |\n| CPU Speed |    117 |     42 |    51 |    39 |    44 |    43 |    48 |    51 |    50 |    54 |    65 |\n| GPU Speed |     53 |     19 |    21 |    17 |    18 |    20 |     x |     x |    25 |    26 |     x |\n\n### æŠ•æœºé‡‡æ ·åŠ é€Ÿæ•ˆæœè¯„æµ‹\n\né€šè¿‡æŠ•æœºé‡‡æ ·æ–¹æ³•å¹¶å€ŸåŠ©Chinese-LLaMA-2-1.3Bå’ŒChinese-Alpaca-2-1.3Bï¼Œå¯ä»¥åˆ†åˆ«åŠ é€Ÿ7Bã€13Bçš„LLaMAå’ŒAlpacaæ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨[æŠ•æœºé‡‡æ ·è„šæœ¬](scripts/inference/speculative_sample.py)åœ¨1*A40-48Gä¸Šè§£ç [ç”Ÿæˆæ•ˆæœè¯„æµ‹](#ç”Ÿæˆæ•ˆæœè¯„æµ‹)ä¸­çš„é—®é¢˜æµ‹å¾—çš„å¹³å‡é€Ÿåº¦ï¼ˆé€Ÿåº¦ä»¥ms/tokenè®¡ï¼Œæ¨¡å‹å‡ä¸ºfp16ç²¾åº¦ï¼‰ï¼Œä¾›ç”¨æˆ·å‚è€ƒã€‚è¯¦ç»†è¯´æ˜è§[ğŸ“–GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/inference_with_transformers_zh#æŠ•æœºé‡‡æ ·è§£ç )ã€‚\n\n| è‰ç¨¿æ¨¡å‹ | è‰ç¨¿æ¨¡å‹é€Ÿåº¦ | ç›®æ ‡æ¨¡å‹ | ç›®æ ‡æ¨¡å‹é€Ÿåº¦ | æŠ•æœºé‡‡æ ·é€Ÿåº¦ï¼ˆåŠ é€Ÿæ¯”ï¼‰ |\n| :---------- |  :-----------------: | :----------- |  :-----------------: | :--------: |\n| Chinese-LLaMA-2-1.3B |  7.6 | Chinese-LLaMA-2-7B |  49.3 | 36.0ï¼ˆ1.37xï¼‰ |\n| Chinese-LLaMA-2-1.3B |  7.6 | Chinese-LLaMA-2-13B |  66.0 | 47.1ï¼ˆ1.40xï¼‰ |\n| Chinese-Alpaca-2-1.3B |  8.1 | Chinese-Alpaca-2-7B |  50.2 | 34.9ï¼ˆ1.44xï¼‰ |\n| Chinese-Alpaca-2-1.3B |  8.2 | Chinese-Alpaca-2-13B |  67.0 | 41.6ï¼ˆ1.61xï¼‰ |\n\n### äººç±»åå¥½å¯¹é½ï¼ˆRLHFï¼‰ç‰ˆæœ¬è¯„æµ‹\n\n#### å¯¹é½æ°´å¹³\nä¸ºè¯„ä¼°ä¸­æ–‡æ¨¡å‹ä¸äººç±»ä»·å€¼åå¥½å¯¹é½ç¨‹åº¦ï¼Œæˆ‘ä»¬è‡ªè¡Œæ„å»ºäº†è¯„æµ‹æ•°æ®é›†ï¼Œè¦†ç›–äº†é“å¾·ã€è‰²æƒ…ã€æ¯’å“ã€æš´åŠ›ç­‰äººç±»ä»·å€¼åå¥½é‡ç‚¹å…³æ³¨çš„å¤šä¸ªæ–¹é¢ã€‚å®éªŒç»“æœä»¥ä»·å€¼ä½“ç°æ­£ç¡®ç‡è¿›è¡Œå‘ˆç°ï¼ˆä½“ç°æ­£ç¡®ä»·å€¼è§‚é¢˜ç›®æ•° / æ€»é¢˜æ•°ï¼‰ã€‚\n\n| Alpaca Models            | Accuracy |  Alpaca Models            | Accuracy |\n| ------------------------ | :---------------: |------------------------ | :---------------: |\n| Chinese-Alpaca-2-1.3B |   79.3%    | Chinese-Alpaca-2-7B  |    88.3%    |\n| **Chinese-Alpaca-2-1.3B-RLHF** |    95.8%    | **Chinese-Alpaca-2-7B-RLHF** |    97.5%    |\n\n\n#### å®¢è§‚æ•ˆæœè¯„æµ‹ï¼šC-Eval & CMMLU\n| Alpaca Models            | C-Eval (0/few-shot) | CMMLU (0/few-shot) |\n| ------------------------ | :---------------: | :---------------: |\n| Chinese-Alpaca-2-1.3B |    23.8 / 26.8    |    24.8 / 25.1    |\n| Chinese-Alpaca-2-7B  |    42.1 / 41.0    |    40.0 / 41.8    |\n| **Chinese-Alpaca-2-1.3B-RLHF** |    23.6 / 27.1    |    24.9 / 25.0    |\n| **Chinese-Alpaca-2-7B-RLHF** |    40.6 / 41.2    |    39.5 / 41.0    |\n\n\n\n## è®­ç»ƒä¸ç²¾è°ƒ\n\n### é¢„è®­ç»ƒ\n\n- åœ¨åŸç‰ˆLlama-2çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ•°æ®è¿›è¡Œå¢é‡è®­ç»ƒï¼Œå¾—åˆ°Chinese-LLaMA-2ç³»åˆ—åŸºåº§æ¨¡å‹\n- è®­ç»ƒæ•°æ®é‡‡ç”¨äº†ä¸€æœŸé¡¹ç›®ä¸­Plusç‰ˆæœ¬æ¨¡å‹ä¸€è‡´çš„æ•°æ®ï¼Œå…¶æ€»é‡çº¦120Gçº¯æ–‡æœ¬æ–‡ä»¶\n- è®­ç»ƒä»£ç å‚è€ƒäº†ğŸ¤—transformersä¸­çš„[run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py)ï¼Œä½¿ç”¨æ–¹æ³•è§[ğŸ“–é¢„è®­ç»ƒè„šæœ¬Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/pt_scripts_zh)\n\n### æŒ‡ä»¤ç²¾è°ƒ\n\n- åœ¨Chinese-LLaMA-2çš„åŸºç¡€ä¸Šï¼Œåˆ©ç”¨æœ‰æ ‡æ³¨æŒ‡ä»¤æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥ç²¾è°ƒï¼Œå¾—åˆ°Chinese-Alpaca-2ç³»åˆ—æ¨¡å‹\n- è®­ç»ƒæ•°æ®é‡‡ç”¨äº†ä¸€æœŸé¡¹ç›®ä¸­Proç‰ˆæœ¬æ¨¡å‹ä½¿ç”¨çš„æŒ‡ä»¤æ•°æ®ï¼Œå…¶æ€»é‡çº¦500ä¸‡æ¡æŒ‡ä»¤æ•°æ®ï¼ˆç›¸æ¯”ä¸€æœŸç•¥å¢åŠ ï¼‰\n- è®­ç»ƒä»£ç å‚è€ƒäº†[Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)é¡¹ç›®ä¸­æ•°æ®é›†å¤„ç†çš„ç›¸å…³éƒ¨åˆ†ï¼Œä½¿ç”¨æ–¹æ³•è§[ğŸ“–æŒ‡ä»¤ç²¾è°ƒè„šæœ¬Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/sft_scripts_zh)\n\n### RLHFç²¾è°ƒ\n\n- åœ¨Chinese-Alpaca-2ç³»åˆ—æ¨¡å‹åŸºç¡€ä¸Šï¼Œåˆ©ç”¨åå¥½æ•°æ®å’ŒPPOç®—æ³•è¿›è¡Œäººç±»åå¥½å¯¹é½ç²¾è°ƒï¼Œå¾—åˆ°Chinese-Alpaca-2-RLHFç³»åˆ—æ¨¡å‹\n- è®­ç»ƒæ•°æ®åŸºäºå¤šä¸ªå¼€æºé¡¹ç›®ä¸­çš„äººç±»åå¥½æ•°æ®å’Œæœ¬é¡¹ç›®æŒ‡ä»¤ç²¾è°ƒæ•°æ®è¿›è¡Œé‡‡æ ·ï¼Œå¥–åŠ±æ¨¡å‹é˜¶æ®µã€å¼ºåŒ–å­¦ä¹ é˜¶æ®µåˆ†åˆ«çº¦69.5Kã€25.6Kæ¡æ ·æœ¬\n- è®­ç»ƒä»£ç åŸºäº[DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)å¼€å‘ï¼Œå…·ä½“æµç¨‹è§[ğŸ“–å¥–åŠ±æ¨¡å‹Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/rm_zh)å’Œ[ğŸ“–å¼ºåŒ–å­¦ä¹ Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/rl_zh)\n\n## å¸¸è§é—®é¢˜\n\nè¯·åœ¨æIssueå‰åŠ¡å¿…å…ˆæŸ¥çœ‹FAQä¸­æ˜¯å¦å·²å­˜åœ¨è§£å†³æ–¹æ¡ˆã€‚å…·ä½“é—®é¢˜å’Œè§£ç­”è¯·å‚è€ƒæœ¬é¡¹ç›® [ğŸ“–GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/faq_zh)\n\n```\né—®é¢˜1ï¼šæœ¬é¡¹ç›®å’Œä¸€æœŸé¡¹ç›®çš„åŒºåˆ«ï¼Ÿ\né—®é¢˜2ï¼šæ¨¡å‹èƒ½å¦å•†ç”¨ï¼Ÿ\né—®é¢˜3ï¼šæ¥å—ç¬¬ä¸‰æ–¹Pull Requestå—ï¼Ÿ\né—®é¢˜4ï¼šä¸ºä»€ä¹ˆä¸å¯¹æ¨¡å‹åšå…¨é‡é¢„è®­ç»ƒè€Œæ˜¯ç”¨LoRAï¼Ÿ\né—®é¢˜5ï¼šäºŒä»£æ¨¡å‹æ”¯ä¸æ”¯æŒæŸäº›æ”¯æŒä¸€ä»£LLaMAçš„å·¥å…·ï¼Ÿ\né—®é¢˜6ï¼šChinese-Alpaca-2æ˜¯Llama-2-Chatè®­ç»ƒå¾—åˆ°çš„å—ï¼Ÿ\né—®é¢˜7ï¼šä¸ºä»€ä¹ˆ24Gæ˜¾å­˜å¾®è°ƒChinese-Alpaca-2-7Bä¼šOOMï¼Ÿ\né—®é¢˜8ï¼šå¯ä»¥ä½¿ç”¨16Ké•¿ä¸Šä¸‹æ–‡ç‰ˆæ¨¡å‹æ›¿ä»£æ ‡å‡†ç‰ˆæ¨¡å‹å—ï¼Ÿ\né—®é¢˜9ï¼šå¦‚ä½•è§£è¯»ç¬¬ä¸‰æ–¹å…¬å¼€æ¦œå•çš„ç»“æœï¼Ÿ\né—®é¢˜10ï¼šä¼šå‡º34Bæˆ–è€…70Bçº§åˆ«çš„æ¨¡å‹å—ï¼Ÿ\né—®é¢˜11ï¼šä¸ºä»€ä¹ˆé•¿ä¸Šä¸‹æ–‡ç‰ˆæ¨¡å‹æ˜¯16Kï¼Œä¸æ˜¯32Kæˆ–è€…100Kï¼Ÿ\né—®é¢˜12ï¼šä¸ºä»€ä¹ˆAlpacaæ¨¡å‹ä¼šå›å¤è¯´è‡ªå·±æ˜¯ChatGPTï¼Ÿ\né—®é¢˜13ï¼šä¸ºä»€ä¹ˆpt_lora_modelæˆ–è€…sft_lora_modelä¸‹çš„adapter_model.binåªæœ‰å‡ ç™¾kï¼Ÿ\n```\n\n\n## å¼•ç”¨\n\nå¦‚æœæ‚¨ä½¿ç”¨äº†æœ¬é¡¹ç›®çš„ç›¸å…³èµ„æºï¼Œè¯·å‚è€ƒå¼•ç”¨æœ¬é¡¹ç›®çš„æŠ€æœ¯æŠ¥å‘Šï¼šhttps://arxiv.org/abs/2304.08177\n```\n@article{Chinese-LLaMA-Alpaca,\n    title={Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca},\n    author={Cui, Yiming and Yang, Ziqing and Yao, Xin},\n    journal={arXiv preprint arXiv:2304.08177},\n    url={https://arxiv.org/abs/2304.08177},\n    year={2023}\n}\n```\n\n\n## è‡´è°¢\n\næœ¬é¡¹ç›®ä¸»è¦åŸºäºä»¥ä¸‹å¼€æºé¡¹ç›®äºŒæ¬¡å¼€å‘ï¼Œåœ¨æ­¤å¯¹ç›¸å…³é¡¹ç›®å’Œç ”ç©¶å¼€å‘äººå‘˜è¡¨ç¤ºæ„Ÿè°¢ã€‚\n\n- [Llama-2 *by Meta*](https://github.com/facebookresearch/llama)\n- [llama.cpp *by @ggerganov*](https://github.com/ggerganov/llama.cpp)\n- [FlashAttention-2 by *Dao-AILab*](https://github.com/Dao-AILab/flash-attention)\n\nåŒæ—¶æ„Ÿè°¢Chinese-LLaMA-Alpacaï¼ˆä¸€æœŸé¡¹ç›®ï¼‰çš„contributorä»¥åŠ[å…³è”é¡¹ç›®å’Œäººå‘˜](https://github.com/ymcui/Chinese-LLaMA-Alpaca#è‡´è°¢)ã€‚\n\n\n## å…è´£å£°æ˜\n\næœ¬é¡¹ç›®åŸºäºç”±Metaå‘å¸ƒçš„Llama-2æ¨¡å‹è¿›è¡Œå¼€å‘ï¼Œä½¿ç”¨è¿‡ç¨‹ä¸­è¯·ä¸¥æ ¼éµå®ˆLlama-2çš„å¼€æºè®¸å¯åè®®ã€‚å¦‚æœæ¶‰åŠä½¿ç”¨ç¬¬ä¸‰æ–¹ä»£ç ï¼Œè¯·åŠ¡å¿…éµä»ç›¸å…³çš„å¼€æºè®¸å¯åè®®ã€‚æ¨¡å‹ç”Ÿæˆçš„å†…å®¹å¯èƒ½ä¼šå› ä¸ºè®¡ç®—æ–¹æ³•ã€éšæœºå› ç´ ä»¥åŠé‡åŒ–ç²¾åº¦æŸå¤±ç­‰å½±å“å…¶å‡†ç¡®æ€§ï¼Œå› æ­¤ï¼Œæœ¬é¡¹ç›®ä¸å¯¹æ¨¡å‹è¾“å‡ºçš„å‡†ç¡®æ€§æä¾›ä»»ä½•ä¿è¯ï¼Œä¹Ÿä¸ä¼šå¯¹ä»»ä½•å› ä½¿ç”¨ç›¸å…³èµ„æºå’Œè¾“å‡ºç»“æœäº§ç”Ÿçš„æŸå¤±æ‰¿æ‹…è´£ä»»ã€‚å¦‚æœå°†æœ¬é¡¹ç›®çš„ç›¸å…³æ¨¡å‹ç”¨äºå•†ä¸šç”¨é€”ï¼Œå¼€å‘è€…åº”éµå®ˆå½“åœ°çš„æ³•å¾‹æ³•è§„ï¼Œç¡®ä¿æ¨¡å‹è¾“å‡ºå†…å®¹çš„åˆè§„æ€§ï¼Œæœ¬é¡¹ç›®ä¸å¯¹ä»»ä½•ç”±æ­¤è¡ç”Ÿçš„äº§å“æˆ–æœåŠ¡æ‰¿æ‹…è´£ä»»ã€‚\n\n<details>\n<summary><b>å±€é™æ€§å£°æ˜</b></summary>\n\nè™½ç„¶æœ¬é¡¹ç›®ä¸­çš„æ¨¡å‹å…·å¤‡ä¸€å®šçš„ä¸­æ–‡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œä½†ä¹Ÿå­˜åœ¨å±€é™æ€§ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š\n\n- å¯èƒ½ä¼šäº§ç”Ÿä¸å¯é¢„æµ‹çš„æœ‰å®³å†…å®¹ä»¥åŠä¸ç¬¦åˆäººç±»åå¥½å’Œä»·å€¼è§‚çš„å†…å®¹\n- ç”±äºç®—åŠ›å’Œæ•°æ®é—®é¢˜ï¼Œç›¸å…³æ¨¡å‹çš„è®­ç»ƒå¹¶ä¸å……åˆ†ï¼Œä¸­æ–‡ç†è§£èƒ½åŠ›æœ‰å¾…è¿›ä¸€æ­¥æå‡\n- æš‚æ—¶æ²¡æœ‰åœ¨çº¿å¯äº’åŠ¨çš„demoï¼ˆæ³¨ï¼šç”¨æˆ·ä»ç„¶å¯ä»¥è‡ªè¡Œåœ¨æœ¬åœ°éƒ¨ç½²å’Œä½“éªŒï¼‰\n\n</details>\n\n\n## é—®é¢˜åé¦ˆ\nå¦‚æœ‰ç–‘é—®ï¼Œè¯·åœ¨GitHub Issueä¸­æäº¤ã€‚ç¤¼è²Œåœ°æå‡ºé—®é¢˜ï¼Œæ„å»ºå’Œè°çš„è®¨è®ºç¤¾åŒºã€‚\n\n- åœ¨æäº¤é—®é¢˜ä¹‹å‰ï¼Œè¯·å…ˆæŸ¥çœ‹FAQèƒ½å¦è§£å†³é—®é¢˜ï¼ŒåŒæ—¶å»ºè®®æŸ¥é˜…ä»¥å¾€çš„issueæ˜¯å¦èƒ½è§£å†³ä½ çš„é—®é¢˜ã€‚\n- æäº¤é—®é¢˜è¯·ä½¿ç”¨æœ¬é¡¹ç›®è®¾ç½®çš„Issueæ¨¡æ¿ï¼Œä»¥å¸®åŠ©å¿«é€Ÿå®šä½å…·ä½“é—®é¢˜ã€‚\n- é‡å¤ä»¥åŠä¸æœ¬é¡¹ç›®æ— å…³çš„issueä¼šè¢«[stable-bot](https://github.com/marketplace/stale)å¤„ç†ï¼Œæ•¬è¯·è°…è§£ã€‚\n"
        },
        {
          "name": "README_EN.md",
          "type": "blob",
          "size": 46.791015625,
          "content": "# [Chinese-LLaMA-Alpaca-3](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3) is launched!\n\n[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](./README.md) | [**ğŸŒEnglish**](./README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki) | [**â“æé—®/Issues**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/issues) | [**ğŸ’¬è®¨è®º/Discussions**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/discussions) | [**âš”ï¸ç«æŠ€åœº/Arena**](http://llm-arena.ymcui.com/)\n\n<p align=\"center\">\n    <br>\n    <img src=\"./pics/banner.png\" width=\"800\"/>\n    <br>\n</p>\n<p align=\"center\">\n    <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/ymcui/Chinese-LLaMA-Alpaca-2.svg?color=blue&style=flat-square\">\n    <img alt=\"GitHub release (latest by date)\" src=\"https://img.shields.io/github/v/release/ymcui/Chinese-LLaMA-Alpaca-2\">\n    <img alt=\"GitHub top language\" src=\"https://img.shields.io/github/languages/top/ymcui/Chinese-LLaMA-Alpaca-2\">\n    <a href=\"https://app.codacy.com/gh/ymcui/Chinese-LLaMA-Alpaca-2/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade\"><img src=\"https://app.codacy.com/project/badge/Grade/1710faac5e634acaabfc26b0a778cdde\"/></a>\n</p>\n\n\nThis project is based on the Llama-2, released by Meta, and it is the second generation of the Chinese LLaMA & Alpaca LLM project. We open-source Chinese LLaMA-2 (foundation model) and Alpaca-2 (instruction-following model). These models have been expanded and optimized with Chinese vocabulary beyond the original Llama-2. We used large-scale Chinese data for incremental pre-training, which further improved the fundamental semantic understanding of the Chinese language, resulting in a significant performance improvement compared to the first-generation models. Standard version supports 4K context, and long context version supports 16K and 64K context. The RLHF models are fine-tuned for human preference alignment and have gained significant performance improvements in the representation of correct values compared to the standard version of the model.\n\n#### Main Contents\n\n- ğŸš€ New extended Chinese vocabulary beyond Llama-2, open-sourcing the Chinese LLaMA-2 and Alpaca-2 LLMs.\n- ğŸš€ Open-sourced the pre-training and instruction finetuning (SFT) scripts for further tuning on user's data\n- ğŸš€ Quickly deploy and experience the quantized LLMs on CPU/GPU of personal PC\n- ğŸš€ Support for LLaMA ecosystems like [ğŸ¤—transformers](https://github.com/huggingface/transformers), [llama.cpp](https://github.com/ggerganov/llama.cpp), [text-generation-webui](https://github.com/oobabooga/text-generation-webui), [LangChain](https://github.com/hwchase17/langchain), [privateGPT](https://github.com/imartinez/privateGPT), [vLLM](https://github.com/vllm-project/vllm) etc.\n\n#### Open-sourced Models\n\n- Base model: Chinese-LLaMA-2 (1.3B, 7B, 13B)\n- Instruction/chat model: Chinese-Alpaca-2 (1.3B, 7B, 13B)\n- Long context model (16K/64K): \n  - Chinese-LLaMA-2-16K (7B, 13B) ã€Chinese-Alpaca-2-16K (7B, 13B) \n  - Chinese-LLaMA-2-64K (7B)ã€Chinese-Alpaca-2-64K (7B)\n- RLHF modelï¼šChinese-Alpaca-2-RLHF (1.3B, 7B)\n\n![](./pics/screencast.gif)\n\n----\n\n[Chinese LLaMA&Alpaca LLMs](https://github.com/ymcui/Chinese-LLaMA-Alpaca)| [Visual Chinese-LLaMA-Alpaca](https://github.com/airaria/Visual-Chinese-LLaMA-Alpaca) | [Multi-modal VLE](https://github.com/iflytek/VLE) | [Chinese MiniRBT](https://github.com/iflytek/MiniRBT) | [Chinese LERT](https://github.com/ymcui/LERT) | [Chinese-English PERT](https://github.com/ymcui/PERT) | [Chinese MacBERT](https://github.com/ymcui/MacBERT) | [Chinese ELECTRA](https://github.com/ymcui/Chinese-ELECTRA) | [Chinese XLNet](https://github.com/ymcui/Chinese-XLNet) | [Chinese BERT](https://github.com/ymcui/Chinese-BERT-wwm) | [Knowledge distillation tool TextBrewer](https://github.com/airaria/TextBrewer) | [Model pruning tool TextPruner](https://github.com/airaria/TextPruner)\n\n## News\n\n**[Apr 30, 2024] Chinese-LLaMA-Alpaca-3 project introduces Llama-3-Chinese-8B and Llama-3-Chinese-8B-Instruct, based on Meta's Llama-3. Check: https://github.com/ymcui/Chinese-LLaMA-Alpaca-3**\n\n[Mar 27, 2024] This project is now online at the SOTA! model platform of Synced, see: https://sota.jiqizhixin.com/project/chinese-llama-alpaca-2\n\n[Jan 23, 2024] Add new GGUF models (with imatrix), AWQ models, support YaRN under vLLM. For details, see[ğŸ“š v4.1 release note](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v4.1)\n\n[Dec 29, 2023] Release long context models: Chiense-LLaMA-2-7B-64K and Chinese-Alpaca-2-7B-64K. We also release RLHF-tuned Chinese-Alpaca-2-RLHF (1.3B/7B). For details, see [ğŸ“š v4.0 release note](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v4.0)\n\n[Sep 01, 2023] Release long context models: Chinese-Alpaca-2-7B-16K and Chinese-Alpaca-2-13B-16K, which can be directly used in downstream tasks, such as privateGPT. For details, see [ğŸ“š v3.1 release note](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v3.1)\n\n[Aug 25, 2023] Release long context models: Chinese-LLaMA-2-7B-16K and Chinese-LLaMA-2-13B-16K, which support 16K context and can be further extended up to 24K+ using NTK. For details, see [ğŸ“š v3.0 release note](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v3.0)\n\n[Aug 14, 2023] Release Chinese-LLaMA-2-13B and Chinese-Alpaca-2-13B. Add text-generation-webui/LangChain/privateGPT support. Add CFG sampling, etc. For details, see [ğŸ“š v2.0 release note](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v2.0)\n\n[Aug 02, 2023] Add FlashAttention-2 training support, vLLM-based inference acceleration support, a new system prompt that generates longer response, etc. For details, see [ğŸ“š v1.1 release note](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v1.1)\n\n[July 31, 2023] Release Chinese-LLaMA-2-7B (base model), trained with 120GB Chinese data. It was further fine-tuned using 5M instruction data, resulting in the Chinese-Alpaca-2-7B (instruction/chat model). For details, see [ğŸ“š v1.0 release notes](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/releases/tag/v1.0)\n\n[July 19, 2023] ğŸš€Launched the [Chinese LLaMA-2 and Alpaca-2 open-source LLM project](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)\n\n## Content Guide\n| Section                                                | Description                                                  |\n| ------------------------------------------------------ | ------------------------------------------------------------ |\n| [ğŸ’ğŸ»â€â™‚ï¸Introduction](#introduction)                       | Briefly introduces the technical features of the models in this project |\n| [â¬Download](#download)                                 | Download links for Chinese LLaMA-2 and Alpaca-2              |\n| [ğŸ’»Inference and Deployment](#inference-and-deployment) | Introduces how to quantify models and deploy and experience large models using a personal computer |\n| [ğŸ’¯System Performance](#system-performance)             | Experimental results on several tasks                        |\n| [ğŸ“Training and Fine-tuning](#training-and-fine-tuning) | Introduces how to perform further training and fine-tuning on Chinese LLaMA-2 and Alpaca-2 |\n| [â“Frequently Asked Questions](#FAQ)                    | Responses to some common questions                           |\n\n## Introduction\n\nThis project launches the Chinese LLaMA-2 and Alpaca-2 models based on Llama-2. Compared to the [first generation of the project](https://github.com/ymcui/Chinese-LLaMA-Alpaca), the main features include:\n\n**ğŸ“– Optimized Chinese Vocabulary**\n\n- In the [first generation of the project](https://github.com/ymcui/Chinese-LLaMA-Alpaca), we expanded Chinese words and characters for the first-generation Chinese LLaMA model (LLaMA: 49953, Alpaca: 49954) to improve the model's encoding and decoding efficiency of Chinese texts.\n- In this project, we **redesigned the new vocabulary** (size: 55296) to further improve the coverage of Chinese words and characters. We also unified the LLaMA/Alpaca vocabulary to avoid problems due to mixed use.\n\n**âš¡ Efficient FlashAttention-2**\n\n- [FlashAttention-2](https://github.com/Dao-AILab/flash-attention) is an implementation of efficient attention mechanisms, offering **faster speed and optimized memory usage** compared to its first-generation.\n- When the context length is longer, using efficient attention technology is essential to prevent explosive growth in memory usage.\n\n**ğŸš„ Adaptive Context Extension based on PI and YaRN**\n\n- In the [first generation of the project](https://github.com/ymcui/Chinese-LLaMA-Alpaca), we implemented the [context extension based on NTK](https://github.com/ymcui/Chinese-LLaMA-Alpaca/pull/743), which can support longer contexts without further training the model.\n- We release long context models, using [PI](https://arxiv.org/abs/2306.15595) and NTK methods, supporting 16K context, and can be further extended up to 24K-32K\n- We further release long context models, using [YaRN](https://arxiv.org/abs/2309.00071), supporting 64K context\n- Based on the above, we further designed a **convenient adaptive empirical formula** that does not require manually setting corresponding hyperparameters for different context lengths.\n\n**ğŸ¤– Simplified Bilingual System Prompt**\n\n- In the [first generation of the project](https://github.com/ymcui/Chinese-LLaMA-Alpaca), we use [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) template for our Chinese Alpaca models\n- Through preliminary experiments, we found that the lengthy system prompt by Llama-2-Chat is not as effective as a simple one\n- We use a very simple system prompt while keeping the Llama-2-Chat template to better adapt to relevant ecosystems\n\n#### ğŸ‘® Human Preference Alignment\n\n- In the [first generation of the project](https://github.com/ymcui/Chinese-LLaMA-Alpaca), the Chinese Alpaca models completed pre-training and instruction fine-tuning, and gained basic conversational ability\n- Through reinforcement learning from human feedback (RLHF) experiments, we find that the ability of the model to convey correct values can be significantly improved\n- This project introduces the Alpaca-2-RLHF series of models, which are used in the same way as the SFT models\n\nThe following figure depicts all open-sourced models for our projects (including the [first-gen project](https://github.com/ymcui/Chinese-LLaMA-Alpaca)).\n\n![](./pics/models.png)\n\n## Download\n\n### Model Selection Guide\n\nBelow is a basic comparison between the Chinese LLaMA-2 and Alpaca-2 models, as well as recommended use cases. **Use Alpaca for ChatGPT-like interaction.**\n\n| Comparison                    |                       Chinese LLaMA-2                        |                       Chinese Alpaca-2                       |\n| :---------------------------- | :----------------------------------------------------------: | :----------------------------------------------------------: |\n| Model Type                    |                        **Base Model**                        |          **Instruction/Chat Model (like ChatGPT)**           |\n| Released Sizes                |                        1.3B, 7B, 13B                         |                        1.3B, 7B, 13B                         |\n| Training Method               |                       Causal-LM (CLM)                        |                   Instruction fine-tuning                    |\n| Training Parts                |      7B, 13B: LoRA + emb/lm-head</br> 1.3B: full params      |      7B, 13B: LoRA + emb/lm-head</br> 1.3B: full params      |\n| Trained on                    | [Original Llama-2](https://github.com/facebookresearch/llama) (non-chat) |                       Chinese LLaMA-2                        |\n| Training Corpus               |           Unlabeled general corpus (120G raw text)           |            Labeled instruction data (5M samples)             |\n| Vocabulary Size<sup>[1]</sup> |                            55,296                            |                            55,296                            |\n| Context Size<sup>[2]</sup>    | Standard: 4K (12K-18K)<br/>Long ctx(PI): 16K (24K-32K) <br/>Long ctx(YaRN): 64K | Standard: 4K (12K-18K)<br/>Long ctx(PI): 16K (24K-32K) <br/>Long ctx(YaRN): 64K |\n| Input Template                |                         Not required                         |          Requires specific templates<sup>[3]</sup>           |\n| Suitable Scenarios            | Text continuation: Given the context, the model generates the following text | Instruction understanding: Q&A, writing, chatting, interaction, etc. |\n| Unsuitable Scenarios          |       Instruction understanding, multi-turn chat, etc.       |                 Unrestricted text generation                 |\n| Preference Alignment          |                              No                              |                   RLHF version (1.3B, 7B)                    |\n\n> [!NOTE]\n> [1] *The vocabulary of the first and second generation models in this project are different, do not mix them. The vocabularies of the second generation LLaMA and Alpaca are the same.*</br> \n> [2] *Extended context size with NTK method is depicted in brackets.*</br>\n> [3] *Alpaca-2 uses the Llama-2-chat series templates (different prompts), not the templates of the first-generation Alpaca, do not mix them.*</br>\n> [4] *1.3B models are not intended for standalone use; instead, use it together with larger models (7B, 13B) through speculative sampling.*</br>\n\n### Full Model Download\n\nBelow are the full models, which can be used directly afterwards, without additional merging steps. Recommended for users with sufficient network bandwidth.\n\n| Model Name            |       Type        |  Size   |                        Download Link                         |                        GGUF                        |\n| :-------------------- | :---------------: | :-----: | :----------------------------------------------------------: | :----------------------------------------------------------: |\n| Chinese-LLaMA-2-13B | Base model | 24.7 GB | [[Baidu]](https://pan.baidu.com/s/1T3RqEUSmyg6ZuBwMhwSmoQ?pwd=e9qy) [[Google]](https://drive.google.com/drive/folders/1YNa5qJ0x59OEOI7tNODxea-1YvMPoH05?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-13b) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-13b-gguf) |\n| Chinese-LLaMA-2-7B | Base model | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/1E5NI3nlQpx1j8z3eIzbIlg?pwd=n8k3) [[Google]](https://drive.google.com/drive/folders/18pp4I-mvQxRA7b8vF9gP-2cH_ocnXVKh?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-7b) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-7b-gguf) |\n| Chinese-LLaMA-2-1.3B | Base model | 2.4 GB | [[Baidu]](https://pan.baidu.com/s/1hEuOCllnJJ5NMEZJf8OkRw?pwd=nwjg) [[Google]](https://drive.google.com/drive/folders/1Sd3PA_gs6JctXtBg5HwmHXh9GX93riMP?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-1.3b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-1.3b) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-1.3b-gguf) |\n| Chinese-Alpaca-2-13B | Chat Model | 24.7 GB | [[Baidu]](https://pan.baidu.com/s/1MT_Zlap1OtdYMgoBNTS3dg?pwd=9xja) [[Google]](https://drive.google.com/drive/folders/1MTsKlzR61xmbTR4hBWzQas_MOpUZsogN?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-13b) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-13b-gguf) |\n| Chinese-Alpaca-2-7B | Chat Model | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/1wxx-CdgbMupXVRBcaN4Slw?pwd=kpn9) [[Google]](https://drive.google.com/drive/folders/1JsJDVs7tE2y31PBNleBlDPsB7S0ZrY8d?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-7b) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-gguf) |\n| Chinese-Alpaca-2-1.3B | Chat model | 2.4 GB | [[Baidu]](https://pan.baidu.com/s/1PD7Ng-ltOIdUGHNorveptA?pwd=ar1p) [[Google]](https://drive.google.com/drive/folders/1h6qOy-Unvqs1_CJ8uPp0eKC61Gbbn8n7?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-1.3b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-1.3b) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-1.3b-gguf) |\n\n#### Long Context Models\n\nThe followings are long context models, which are recommended for long context tasks. \n\n| Model Name                |    Type    |  Size   |                        Download Link                         |                             GGUF                             |\n| :------------------------ | :--------: | :-----: | :----------------------------------------------------------: | :----------------------------------------------------------: |\n| Chinese-LLaMA-2-7B-64K ğŸ†•  | Base model | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/1ShDQ2FG2QUJrvfnxCn4hwQ?pwd=xe5k) [[Google]](https://drive.google.com/drive/folders/17l9xJx55L2YNpqt7NiLVQzOZ6fV4rzJ-?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-7b-64k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-7b-64k) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-7b-64k-gguf) |\n| Chinese-Alpaca-2-7B-64K ğŸ†• | Chat model | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/1KBAr9PCGvX2oQkYfCuLEjw?pwd=sgp6) [[Google]](https://drive.google.com/drive/folders/13G_d5xcDnhtaMOaulj1BFiZbVoVwJ-Cu?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-64k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-7b-64k) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-64k-gguf) |\n| Chinese-LLaMA-2-13B-16K   | Base model | 24.7 GB | [[Baidu]](https://pan.baidu.com/s/1XWrh3Ru9x4UI4-XmocVT2w?pwd=f7ik) [[Google]](https://drive.google.com/drive/folders/1nii6lF0DgB1u81CnsE4cCK2jD5oq_OW-?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-13b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-13b-16k) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-13b-16k-gguf) |\n| Chinese-LLaMA-2-7B-16K    | Base model | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/1ZH7T7KU_up61ugarSIXw2g?pwd=pquq) [[Google]](https://drive.google.com/drive/folders/1Zc6jI5bl3myQbQsY79dWJJ8mP_fyf3iF?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-7b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-7b-16k) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-7b-16k-gguf) |\n| Chinese-Alpaca-2-13B-16K  | Chat model | 24.7 GB | [[Baidu]](https://pan.baidu.com/s/1gIzRM1eg-Xx1xV-3nXW27A?pwd=qi7c) [[Google]](https://drive.google.com/drive/folders/1mOkYQCvEqtGoZ9DaIpYFweSkSia2Q0vl?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-13b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-13b-16k) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-13b-16k-gguf) |\n| Chinese-Alpaca-2-7B-16K   | Chat model | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/1Qk3U1LyvMb1RSr5AbiatPw?pwd=bfis) [[Google]](https://drive.google.com/drive/folders/1KBRSd2xAhiVQmamfA5wpm5ovYFRKuMdr?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-7b-16k) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-16k-gguf) |\n\n#### RLHF Models\n\nThe following lists the RLHF models which exhibit a better value orientation than the standard version for issues involving law, ethics, etc.\n\n| Model Name                |    Type    |  Size   |                        Download Link                         |                        GGUF                        |\n| :------------------------ | :------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |\n| Chinese-Alpaca-2-7B-RLHF ğŸ†• | Chat Model | 12.9 GB | [[Baidu]](https://pan.baidu.com/s/17GJ1y4rpPDuvWlvPaWgnqw?pwd=4feb) [[Google]](https://drive.google.com/drive/folders/1OHZVVtwM5McVEIZzyOYgGYLAxcZNVK4D?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-rlhf) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-7b-rlhf) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-7b-rlhf-gguf) |\n| Chinese-Alpaca-2-1.3B-RLHF ğŸ†• | Chat Model | 2.4 GB | [[Baidu]](https://pan.baidu.com/s/1cLKJKieNitWbOggUXXaamw?pwd=cprp) [[Google]](https://drive.google.com/drive/folders/1zcvPUPPkq69SgqRu6YBurAZ9ptcPSZNx?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-1.3b-rlhf) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-1.3b-rlhf) | [[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-1.3b-rlhf-gguf) |\n\n#### AWQ Models\n\nAWQ (Activation-aware Weight Quantization) is an efficient quantization method, which can be used with ğŸ¤—transformers, llama.cpp, etc.\n\nThe pre-computed search results of our models are available: https://huggingface.co/hfl/chinese-llama-alpaca-2-awq\n\n- Generate AWQ-quantized models (AWQ official repo): https://github.com/mit-han-lab/llm-awq#usage\n- Using AWQ in llama.cpp: https://github.com/ggerganov/llama.cpp/tree/master/awq-py\n\n### LoRA Model Download\n\nBelow are the LoRA models, **which cannot be used directly and must be merged with the refactored models according to the tutorial**. Recommended for users with insufficient network bandwidth, who already have the original Llama-2 and light-weight download.\n\n| Model Name               |       Type        |                       Required Model for merging                       | Size  |                      LoRA Download Link                      |\n| :----------------------- | :---------------: | :----------------------------------------------------------: | :---: | :----------------------------------------------------------: |\n| Chinese-LLaMA-2-LoRA-13B | Base model | [Llama-2-13B-hf](https://huggingface.co/meta-llama/Llama-2-13b-hf) | 1.5 GB | [[Baidu]](https://pan.baidu.com/s/1PFKTBn54GjAjzWeQISKruw?pwd=we6s) [[Google]](https://drive.google.com/file/d/10Z_k9A9N9D_6RHrMTmbHQRCuI6s1iMb1/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-lora-13b) |\n| Chinese-LLaMA-2-LoRA-7B | Base model |        [Llama-2-7B-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)        | 1.1 GB | [[Baidu]](https://pan.baidu.com/s/1bmgqdyRh9E3a2uqOGyNqiQ?pwd=7kvq) [[Google]](https://drive.google.com/file/d/1njJGSU_PRbzjYRNw5RSbC5-4fBOXTVY3/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-lora-7b) |\n| Chinese-Alpaca-2-LoRA-13B | Chat Model | [Llama-2-13B-hf](https://huggingface.co/meta-llama/Llama-2-13b-hf) | 1.5 GB | [[Baidu]](https://pan.baidu.com/s/1Y5giIXOUUzI4Na6JOcviVA?pwd=tc2j) [[Google]](https://drive.google.com/file/d/1z2FIInsYJBTXipgztc-Mv7kkeqscx442/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-lora-13b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-lora-13b) |\n| Chinese-Alpaca-2-LoRA-7B | Chat Model | [Llama-2-7B-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) | 1.1 GB | [[Baidu]](https://pan.baidu.com/s/1g0olPxkB_rlZ9UUVfOnbcw?pwd=5e7w) [[Google]](https://drive.google.com/file/d/1MzJL-ZIzdJW7MIcAiYIDIDJ5dlMi8Kkk/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-lora-7b) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-lora-7b) |\n\nThe followings are long context models, which are recommended for long context tasks. \n\n| Model Name                      |    Type    |                  Required Model for merging                  |  Size  |                        Download Link                         |\n| :------------------------------ | :--------: | :----------------------------------------------------------: | :----: | :----------------------------------------------------------: |\n| Chinese-LLaMA-2-LoRA-7B-64K ğŸ†• | Base model | [Llama-2-7B-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) | 1.1 GB | [[Baidu]](https://pan.baidu.com/s/1QjqKNM9Xez5g6koUrbII_w?pwd=94pk) [[Google]](https://drive.google.com/file/d/1-NuGqfduUZARRquFjGLpTmI5J-HlXYSR/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-lora-7b-64k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-lora-7b-64k) |\n| Chinese-Alpaca-2-LoRA-7B-64K ğŸ†• | Chat model | [Llama-2-7B-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) | 1.1 GB | [[Baidu]](https://pan.baidu.com/s/1t6bPpMlJCrs9Ce7LXs09-w?pwd=37it) [[Google]](https://drive.google.com/file/d/1qESorx2PHtIsnj53JJ7XBsdOGHuLNjoI/view?usp=sharing) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-lora-7b-64k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-lora-7b-64k) |\n| Chinese-LLaMA-2-LoRA-13B-16K    | Base model | [Llama-2-13B-hf](https://huggingface.co/meta-llama/Llama-2-13b-hf) | 1.5 GB | [[Baidu]](https://pan.baidu.com/s/1VrfOJmhDnXxrXcdnfX00fA?pwd=4t2j) [[Google]](https://drive.google.com/file/d/1mSpigmHcN9YX1spa4QN3IPtx43Vfs55H/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-lora-13b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-lora-13b-16k) |\n| Chinese-LLaMA-2-LoRA-7B-16K     | Base model | [Llama-2-7B-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) | 1.1 GB | [[Baidu]](https://pan.baidu.com/s/14Jnm7QmcDx3XsK_NHZz6Uw?pwd=5b7i) [[Google]](https://drive.google.com/file/d/1yUdyQuBMAmxmUEAvGiKbjKuxTYPPI-or/view?usp=sharing) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-llama-2-lora-7b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-llama-2-lora-7b-16k) |\n| Chinese-Alpaca-2-LoRA-13B-16K | Chat Model | [Llama-2-13B-hf](https://huggingface.co/meta-llama/Llama-2-13b-hf) | 1.5 GB | [[Baidu]](https://pan.baidu.com/s/1g42_X7Z0QWDyrrDqv2jifQ?pwd=bq7n) [[Google]](https://drive.google.com/file/d/1ppGNyMWnuLDcClXN7DBTbKxVehsn3Gd2/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-lora-13b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-lora-13b-16k) |\n| Chinese-Alpaca-2-LoRA-7B-16K  | Chat Model | [Llama-2-7B-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) | 1.1 GB | [[Baidu]](https://pan.baidu.com/s/1E7GEZ6stp8EavhkhR06FwA?pwd=ewwy) [[Google]](https://drive.google.com/file/d/1GTgDNfMdcQhHEAfMPaP-EOEk_fwDvNEK/view?usp=share_link) <br/>[[ğŸ¤—HF]](https://huggingface.co/hfl/chinese-alpaca-2-lora-7b-16k) [[ğŸ¤–ModelScope]](https://modelscope.cn/models/ChineseAlpacaGroup/chinese-alpaca-2-lora-7b-16k) |\n\n> [!IMPORTANT] \n> As the LoRA models cannot be used separately, they must be merged with the original Llama-2 to form a complete model for model inference, quantization, or further training. Please choose one of the following methods to merge these models.\n>\n> - [**Online Conversion**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/online_conversion_en): Colab users can use the notebook provided by this project for online conversion and model quantization\n> - [**Manual Conversion**](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/manual_conversion_en): Offline method of conversion, generating different formats of models for quantization or further fine-tuning\n\n## Inference and Deployment\n\nThe models in this project mainly support the following quantization, inference, and deployment methods.\n\n| Tool                                                         | Features                                                | CPU  | GPU  | Quant | GUI  | API  | vLLM<sup>Â§</sup> | 16K<sup>â€¡</sup> | 64K<sup>â€¡</sup> |Speculative  Sampling |                           Tutorial                           |\n| :----------------------------------------------------------- | ------------------------------------------------------- | :--: | :--: | :---: | :--: | :--:| :--: | :--: | :----------------------------------------------------------: | :----------------------------------------------------------: | ------------------------------------------------------------ |\n| [**llama.cpp**](https://github.com/ggerganov/llama.cpp)      | Rich quantization options and efficient local inference |  âœ…   |  âœ…   |   âœ…   |  âŒ   |  âœ…   |  âŒ   |  âœ…  |âœ…  | âœ… | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/llamacpp_en) |\n| [**ğŸ¤—Transformers**](https://github.com/huggingface/transformers) | Native transformers inference interface                 |  âœ…   |  âœ…   |   âœ…   |  âœ…   |  âŒ   |  âœ…  |  âœ…  |âœ…  | âœ… | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/inference_with_transformers_en) |\n| [**Colab Demo**](https://colab.research.google.com/drive/1yu0eZ3a66by8Zqm883LLtRQrguBAb9MR?usp=sharing) | Running a Gradio web demo in Colab | âœ… | âœ… | âœ… | âœ… | âŒ | âœ… | âœ… | âœ…  |âœ… | [link](https://colab.research.google.com/drive/1yu0eZ3a66by8Zqm883LLtRQrguBAb9MR?usp=sharing) |\n| [**OpenAI API Calls**](https://platform.openai.com/docs/api-reference) | A server that implements OpenAI API |  âœ…   |  âœ…   |  âœ…   |  âŒ   |  âœ…   |  âœ…  |  âœ…  |âœ…  | âŒ | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/api_calls_en) |\n| [**text-generation-webui**](https://github.com/oobabooga/text-generation-webui) | A tool for deploying model as a web UI |  âœ…   |  âœ…   |  âœ…   |  âœ…   | âœ…<sup>â€ </sup> | âŒ  | âœ… | âŒ |âŒ | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/text-generation-webui_en) |\n| [**LangChain**](https://github.com/hwchase17/langchain) | LLM application development framework, suitable for secondary development |  âœ…<sup>â€ </sup>  |  âœ…   |  âœ…<sup>â€ </sup>   |  âŒ   |  âŒ   | âŒ  | âœ… | âœ… |âŒ | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/langchain_en) |\n| [**privateGPT**](https://github.com/imartinez/privateGPT) | LangChain-based multi-document QA framework | âœ… | âœ… | âœ… | âŒ | âŒ | âŒ | âœ… | âŒ | âŒ | [link](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/privategpt_en) |\n\n> [!NOTE]\n> <sup>â€ </sup>: Supported by this tool, but not implemented in the tutorial. Please refer to the official documentation for details. <br/>\n> <sup>â€¡</sup>: Support long context or not (requires customized RoPE support)</br>\n> <sup>Â§</sup>: vLLM backend does not support our long context models. </br>\n\n## System Performance\n\n### Generation Performance Evaluation\n\nIn order to intuitively understand the generation performance of the model, this project has launched an online model arena platform imitating [Fastchat Chatbot Arena](https://chat.lmsys.org/?arena), where you can browse and evaluate the quality of model responses. The arena platform provides evaluation indicators such as win rate and Elo score, and you can view the win rate of battles between two models. The question bank comes from [200 questions manually created in the first-generation project](https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main/examples/f16-p7b-p13b-33b), and additional questions added on this basis. Generated replies are subject to randomness and are influenced by decoding hyperparameters, random seeds, etc., so the related evaluations are not absolutely rigorous. The results are only for reference, and you are welcome to experience it yourself. Please see the [examples directory](./examples) for some generated examples.\n\n**âš”ï¸ Online Chatbot Arena: [http://llm-arena.ymcui.com](http://llm-arena.ymcui.com/)**\n\n| System                                                       | Win Rate (no tie)â†“ | Elo Rating |\n| ------------------------------------------------------------ | :----------------: | :--------: |\n| **Chinese-Alpaca-2-13B-16K**                                 |        86.84%        |  1580   |\n| **Chinese-Alpaca-2-13B**                                     |        72.01%        |  1579   |\n| [Chinese-Alpaca-Pro-33B](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |        64.87%        |  1548   |\n| **Chinese-Alpaca-2-7B**                                      |        64.11%        |  1572   |\n| [Chinese-Alpaca-Pro-7B](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |        62.05%        |  1500   |\n| **Chinese-Alpaca-2-7B-16K**                                  |        61.67%        |  1540   |\n| [Chinese-Alpaca-Pro-13B](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |        61.26%        |  1567   |\n| [Chinese-Alpaca-Plus-33B](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |        31.29%        |  1401   |\n| [Chinese-Alpaca-Plus-13B](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |        23.43%        |  1329   |\n| [Chinese-Alpaca-Plus-7B](https://github.com/ymcui/Chinese-LLaMA-Alpaca) |        20.92%        |  1379   |\n\n> [!NOTE]\n> Results timestamp: Sep 1. 2023 . For the latest results, see [**âš”ï¸Arena**](http://llm-arena.ymcui.com/).\n\n### NLU Performance Evaluation: C-Eval\n\n[C-Eval](https://cevalbenchmark.com/) is a comprehensive Chinese basic model evaluation suite. The validation set contains 1.3K multiple-choice questions, and the test set contains 12.3K multiple-choice questions, covering 52 subjects. The type of questions is multiple-choice. The experimental results are presented in the format of \"zero-shot / 5-shot\". For C-Eval inference code, please refer to this project's [ğŸ“–GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/ceval_en).\n\n| LLaMA Models            |    Valid    |    Test     | Alpaca Models            |    Valid    |    Test     |\n| ----------------------- | :---------: | :---------: | ------------------------ | :---------: | :---------: |\n| **Chinese-LLaMA-2-13B** | 40.6 / 42.7 | 38.0 / 41.6 | **Chinese-Alpaca-2-13B** | 44.3 / 45.9 | 42.6 / 44.0 |\n| **Chinese-LLaMA-2-7B**  | 28.2 / 36.0 | 30.3 / 34.2 | **Chinese-Alpaca-2-7B**  | 41.3 / 42.9 | 40.3 / 39.5 |\n| Chinese-LLaMA-Plus-33B  | 37.4 / 40.0 | 35.7 / 38.3 | Chinese-Alpaca-Plus-33B  | 46.5 / 46.3 | 44.9 / 43.5 |\n| Chinese-LLaMA-Plus-13B  | 27.3 / 34.0 | 27.8 / 33.3 | Chinese-Alpaca-Plus-13B  | 43.3 / 42.4 | 41.5 / 39.9 |\n| Chinese-LLaMA-Plus-7B   | 27.3 / 28.3 | 26.9 / 28.4 | Chinese-Alpaca-Plus-7B   | 36.7 / 32.9 | 36.4 / 32.3 |\n\n### NLU Performance Evaluation: CMMLU\n\n[CMMLU](https://github.com/haonan-li/CMMLU) is another comprehensive Chinese evaluation dataset, specifically designed to evaluate the knowledge and reasoning abilities of language models in a Chinese context. It covers 67 topics ranging from basic subjects to advanced professional levels, with a total of 11.5K test cases. The type of questions is multiple-choice. For CMMLU inference code, please refer to this project's [ğŸ“–GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/cmmlu_en).\n\n| LLaMA Models            | Test (0/few-shot) | Alpaca Models            | Test (0/few-shot) |\n| ----------------------- | :---------------: | ------------------------ | :---------------: |\n| **Chinese-LLaMA-2-13B** |    38.9 / 42.5    | **Chinese-Alpaca-2-13B** |    43.2 / 45.5    |\n| **Chinese-LLaMA-2-7B**  |    27.9 / 34.1    | **Chinese-Alpaca-2-7B**  |    40.0 / 41.8    |\n| Chinese-LLaMA-Plus-33B  |    35.2 / 38.8    | Chinese-Alpaca-Plus-33B  |    46.6 / 45.3    |\n| Chinese-LLaMA-Plus-13B  |    29.6 / 34.0    | Chinese-Alpaca-Plus-13B  |    40.6 / 39.9    |\n| Chinese-LLaMA-Plus-7B   |    25.4 / 26.3    | Chinese-Alpaca-Plus-7B   |    36.8 / 32.6    |\n\n### Long Context Model Evaluation\n\n[LongBench](https://github.com/THUDM/LongBench) is a benchmark for testing LLM's long context ability, consisting of 6 categories and 20 tasks. The average length of most of the task ranges from 5K to 15K. LongBench has 4.5K test samples in total. The followings are the results on Chinese subtasks. For LongBench inference code, please refer to this project's [ğŸ“–GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/longbench_en)\n\n| Models                      | Single-doc QA | Multi-doc QA | Summarization | Few-shot Learning | Code Completion | Synthetic Task | Avg  |\n| --------------------------- | :-----------: | :----------: | :-----------: | :---------------: | :-------------: | :------------: | :--: |\n| **Chinese-Alpaca-2-7B-64K** | 44.7  |  28.1 | 14.4 |  39.0   |  44.6  |   5.0  | 29.3|\n| **Chinese-LLaMA-2-7B-64K** | 27.2  |  16.4 | 6.5 |  33.0   |  7.8  |   5.0  | 16.0|\n| **Chinese-Alpaca-2-13B-16K** |   47.9  |   26.7 | 13.0 |     22.3    |   46.6   |   21.5   | 29.7 |\n| Chinese-Alpaca-2-13B         |   38.4   |   20.0   | 11.9 |     17.3    |   46.5   |   8.0    | 23.7 |\n| **Chinese-Alpaca-2-7B-16K**  |   46.4  |   23.3  | 14.3 |     29.0     |   49.6   |   9.0    | 28.6 |\n| Chinese-Alpaca-2-7B          |   34.0   |   17.4   | 11.8 |     21.3    |   50.3  |   4.5    | 23.2 |\n| **Chinese-LLaMA-2-13B-16K**  |   36.7   |   17.7  | 3.1 |     29.8     |   13.8   |   3.0    | 17.3 |\n| Chinese-LLaMA-2-13B          |   28.3   |   14.4   | 4.6 |     16.3     |   10.4   |   5.4    | 13.2 |\n| **Chinese-LLaMA-2-7B-16K**   |   33.2   |   15.9   | 6.5 |     23.5     |   10.3    |   5.3    | 15.8|\n| Chinese-LLaMA-2-7B           |   19.0   |   13.9   | 6.4  |     11.0    |   11.0   |   4.7    | 11.0 |\n\n### Quantization Evaluation\n\nTo understand the quality loss brought by quantization, taking Chinese-LLaMA-2-7B as an example, we report the model size, PPL, C-eval results under different quantization levels. PPL is calculated under 4K context, and we report zero-shot and 5-shot results on C-Eval valid set.\n\n| Precision       | Model Size |  PPL   |   C-Eval    |\n| :-------------- | :--------: | :----: | :---------: |\n| FP16            |  12.9 GB   | 9.373  | 28.2 / 36.0 |\n| 8-bit quantized |   6.8 GB   | 9.476  | 26.8 / 35.4 |\n| 4-bit quantized |   3.7 GB   | 10.132 | 25.5 / 32.8 |\n\nSpecifically, the followings are the benchmark for different quantization methods in llama.cpp. The speed is presented with ms/tok. For details, see our [Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/llamacpp_en#quantization-method-and-inference-speed).\n\n| llama.cpp |    F16 |   Q2_K |  Q3_K |  Q4_0 |  Q4_1 |  Q4_K |  Q5_0 |  Q5_1 |  Q5_K |  Q6_K |  Q8_0 |\n| --------- | -----: | -----: | ----: | ----: | ----: | ----: | ----: | ----: | ----: | ----: | ----: |\n| PPL       |  9.128 | 11.107 | 9.576 | 9.476 | 9.576 | 9.240 | 9.156 | 9.213 | 9.168 | 9.133 | 9.129 |\n| Size      | 12.91G |  2.41G | 3.18G | 3.69G | 4.08G | 3.92G | 4.47G | 4.86G | 4.59G | 5.30G | 6.81G |\n| CPU Speed |    117 |     42 |    51 |    39 |    44 |    43 |    48 |    51 |    50 |    54 |    65 |\n| GPU Speed |     53 |     19 |    21 |    17 |    18 |    20 |     x |     x |    25 |    26 |     x |\n\n### Speculative Sampling Evaluation\n\nUsing speculative sampling and leveraging Chinese-LLaMA-2-1.3B and Chinese-Alpaca-2-1.3B can accelerate the inference speed of 7B and 13B LLaMA and Alpaca models. The followings are the inference speeds (ms/token) evaluated on the questions in [Generation Performance Evaluation](#Generation-Performance-Evaluation) on 1*A40-48G. All the models are in fp16 format. For details, see our [Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/inference_with_transformers_en#Speculative-Sampling).\n\n| Draft Model           | Draft Model Speed | Target Model         | Target Model Speed | Speculative Sampling Speed |\n| :-------------------- | :---------------: | :------------------- | :----------------: | :------------------------: |\n| Chinese-LLaMA-2-1.3B  |        7.6        | Chinese-LLaMA-2-7B   |        49.3        |       36.0ï¼ˆ1.37xï¼‰        |\n| Chinese-LLaMA-2-1.3B  |        7.6        | Chinese-LLaMA-2-13B  |        66.0        |       47.1ï¼ˆ1.40xï¼‰        |\n| Chinese-Alpaca-2-1.3B |        8.1        | Chinese-Alpaca-2-7B  |        50.2        |       34.9ï¼ˆ1.44xï¼‰        |\n| Chinese-Alpaca-2-1.3B |        8.2        | Chinese-Alpaca-2-13B |        67.0        |       41.6ï¼ˆ1.61xï¼‰        |\n\n\n### RLHF Models Evaluation\n\n#### Alignment\n\nTo assess the degree of alignment of the RLHF models with human preferences, we constructed our own evaluation dataset, which covers a number of aspects that are the focus of human value preferences, such as morality, pornography, drugs, violence, and so on. The experimental results are presented in terms of the percentage of correct value embodiment (number of systematically correct value questions / total number of questions).\n\n| Alpaca Models            | Accuracy |  Alpaca Models            | Accuracy |\n| ------------------------ | :---------------: |------------------------ | :---------------: |\n| Chinese-Alpaca-2-1.3B |   79.3%    | Chinese-Alpaca-2-7B  |    88.3%    |\n| **Chinese-Alpaca-2-1.3B-RLHF** |    95.8%    | **Chinese-Alpaca-2-7B-RLHF** |    97.5%    |\n\n\n#### NLU Performance Evaluationï¼š C-Eval & CMMLU\n| Alpaca Models            | C-Eval (0/few-shot) | CMMLU (0/few-shot) |\n| ------------------------ | :---------------: | :---------------: |\n| Chinese-Alpaca-2-1.3B |    23.8 / 26.8    |    24.8 / 25.1    |\n| Chinese-Alpaca-2-7B  |    42.1 / 41.0    |    40.0 / 41.8    |\n| **Chinese-Alpaca-2-1.3B-RLHF** |    23.6 / 27.1    |    24.9 / 25.0    |\n| **Chinese-Alpaca-2-7B-RLHF** |    40.6 / 41.2    |    39.5 / 41.0    |\n\n\n## Training and Fine-tuning\n\nPlease refer to the corresponding Wiki for information on pre-training (Chinese LLaMA-2 training) and instruction fine-tuning (Chinese Alpaca-2 training).\n\n- **Pre-training**: The code is adapted from [run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py) in ğŸ¤—transformers. For usage, see the [Pre-training Script Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/pt_scripts_en).\n- **Instruction Fine-tuning**: The code refers to the relevant parts of dataset handling in the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) project. For usage, see the [Instruction Fine-tuning Script Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/sft_scripts_en).\n- **RLHF Fine-tuning**: Reinforcement learning from human feedback fine-tuning using preference data and PPO algorithm based on Chinese-Alpaca-2. For details, see the [ğŸ“–Reward Modeling Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/rm_en)å’Œ[ğŸ“–Reinforcement Learning Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/rl_en).\n\n\n## FAQ\n\nPlease make sure to check if there is a solution in the FAQ before raising an Issue.\n\n```\nQuestion 1: What is the difference between this project and the first-gen project?\nQuestion 2: Can the model be commercialized?\nQuestion 3: Do you accept third-party Pull Requests?\nQuestion 4: Why not perform full pre-training but use LoRA instead?\nQuestion 5: Does Llama-2 series support tools that support the first-gen LLaMA?\nQuestion 6: Is Chinese-Alpaca-2 trained from Llama-2-Chat?\nQuestion 7: Why does training with 24GB VRAM lead to an OOM error when fine-tuning chinese-alpaca-2-7b?\nQuestion 8: Can the 16K long-context version model replace the standard version model?\nQuestion 9: How to interprete the results of third-party benchmarks?\nQuestion 10: Will you release 34B or 70B models?\nQuestion 11: Why the long-context model is 16K context, not 32K or 100K?\nQuestion 12: Why does the Alpaca model reply that it is ChatGPT?\nQuestion 13: Why is the adapter_model.bin in the pt_lora_model or sft_lora_model folder only a few hundred kb?\n```\n\nFor specific questions and answers, please refer to the project >>> [ğŸ“š GitHub Wiki](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/faq_en)\n\n## Citation\n\nIf you use the resources related to this project, please refer to and cite this project's technical report: https://arxiv.org/abs/2304.08177\n```\n@article{Chinese-LLaMA-Alpaca,\n    title={Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca},\n    author={Cui, Yiming and Yang, Ziqing and Yao, Xin},\n    journal={arXiv preprint arXiv:2304.08177},\n    url={https://arxiv.org/abs/2304.08177},\n    year={2023}\n}\n```\n\n## Acknowledgments\n\nThis project is mainly based on the following open-source projects, and we would like to express our gratitude to the related projects and research developers.\n\n- [Llama-2 *by Meta*](https://github.com/facebookresearch/llama)\n- [llama.cpp *by @ggerganov*](https://github.com/ggerganov/llama.cpp)\n- [FlashAttention-2 by *Dao-AILab*](https://github.com/Dao-AILab/flash-attention)\n\nWe also appreciate the contributors of Chinese-LLaMA-Alpaca (the first-gen project) and [the associated projects and personnel](https://github.com/ymcui/Chinese-LLaMA-Alpaca#è‡´è°¢).\n\n## Disclaimer\n\nThis project is predicated on the utilization of the Llama-2 model, as released by Meta. As such, we respectfully request all users to adhere diligently to the provisions of the open-source license agreement pertinent to the Llama-2 model. In instances where third-party code is integrated, strict adherence to the appropriate open-source license agreement is also essential. Please be advised that the precision of the content generated by the model is subject to variability due to computational methodologies, random elements, and potential degradation of quantization accuracy. Consequently, this project makes no warranties, express or implied, regarding the accuracy of the model output. Furthermore, this project cannot be held accountable for any losses, whether direct or consequential, that may arise from the use of associated resources and the results derived therefrom. In cases where the models associated with this project are employed for commercial purposes, it is incumbent upon developers to act in accordance with local laws and regulations, thereby ensuring the legality of the content generated by the model. Finally, please note that this project does not accept any liability for products or services that may be developed based on its models.\n\n<details>\n<summary><b>Limitation Statement</b></summary>\n\nAlthough the models in this project have significantly improved Chinese understanding and generation capabilities compared to the original LLaMA and Alpaca, there are also the following limitations:\n\n- It may produce unpredictable harmful content and content that does not conform to human preferences and values.\n- Due to computing power and data issues, the training of the related models is not sufficient, and the Chinese understanding ability needs to be further improved.\n- There is no online interactive demo available for now (Note: users can still deploy it locally themselves).\n\n</details>\n\n\n## Feedback\n\nIf you have any questions, please submit them in GitHub Issues.\n\n- Before submitting a question, please check if the FAQ can solve the problem and consult past issues to see if they can help.\n- Please use our dedicated issue template for submitting.\n- Duplicate and unrelated issues will be handled by [stable-bot](https://github.com/marketplace/stale); please understand.\n- Raise questions politely and help build a harmonious discussion community.\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "pics",
          "type": "tree",
          "content": null
        },
        {
          "name": "prompts",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0859375,
          "content": "peft==0.3.0\ntorch==2.0.1\ntransformers==4.35.0\nsentencepiece==0.1.99\nbitsandbytes==0.41.1"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}