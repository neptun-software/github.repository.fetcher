{
  "metadata": {
    "timestamp": 1736561047317,
    "page": 824,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ashnkumar/sketch-code",
      "stars": 5128,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.2841796875,
          "content": "*.DS_Store\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n*.ipynb\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nenv/\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\ndata/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\n*.egg-info/\n.installed.cfg\n*.egg\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# dotenv\n.env\n\n# virtualenv\n.venv\nvenv/\nENV/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio and Webstorm\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff:\n.idea/**/workspace.xml\n.idea/**/tasks.xml\n.idea/dictionaries\n.idea/*\n\n# Sensitive or high-churn files:\n.idea/**/dataSources/\n.idea/**/dataSources.ids\n.idea/**/dataSources.xml\n.idea/**/dataSources.local.xml\n.idea/**/sqlDataSources.xml\n.idea/**/dynamic.xml\n.idea/**/uiDesigner.xml\n\n# Gradle:\n.idea/**/gradle.xml\n.idea/**/libraries\n\n# CMake\ncmake-build-debug/\n\n# Mongo Explorer plugin:\n.idea/**/mongoSettings.xml\n\n## File-based project format:\n*.iws\n\n## Plugin-specific files:\n\n# IntelliJ\nout/\n\n# mpeltonen/sbt-idea plugin\n.idea_modules/\n\n# JIRA plugin\natlassian-ide-plugin.xml\n\n# Cursive Clojure plugin\n.idea/replstate.xml\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n\n# Generated dataset files\ndatasets/*\n!datasets/pix2code_datasets.z*\n!datasets/web\n\n# Training output\nbin/\nbin2/\n\n# Generated guis / htmls\ngenerated_guis_htmls/\n"
        },
        {
          "name": "Document_kr.md",
          "type": "blob",
          "size": 14.1572265625,
          "content": "# 딥 러닝을 사용한 자동화된 프런트 엔드 개발\n##### SketchCode: 아이디어에서 HTML 로 5초안에 전환\n\n![Preview](https://github.com/vbnm134678/sketch-code/blob/documentation-korean-translation/image/header_image.png)\n\n*[Ashwin Kumar](https://www.linkedin.com/in/ashnkumar/)는 회계 자동화에 머신러닝을 사용하는 Y Combinator의 신생기업인 [Sway Finance](https://swayfinance.com/) 공동 설립자였습니다. 이곳 Insight에서 그는 사용자가 손으로 그린 와이어프레임으로 작동하는 HTML 웹사이트를 만들 수 있는 모델을 개발하여 디자인 설계 프로세스를 크게 가속화했습니다. 그는 현재 [Mysthic](https://www.mythic-ai.com/)의 딥러닝 과학자입니다.*\n\n***최첨단 응용 AI 제품을 만드는 연구자와 엔지니어 모임에 가입하세요.*** *다음 Insight AI Fellowship(Silicon Valley와 New York)의 마감일은 3월 26일입니다.*\n\n\n---\n\n사용자에게 직관적이고 매력적인 경험을 제공하는 것은 모든 기업에게 중요한 목표이며 프로토타이핑, 설계 및 사용자 테스팅의 빠른 사이클에 의해 이루어지는 프로세스입니다. Facebook과 같은 대기업들은 디자인 프로세스에 모든 팀이 참여할 수 있는 큰 대역폭(번역자: 순간적으로 보낼 수 있는 데이터의 양)을 가지고 있는데, 이는 수 주가 걸릴 수 있고 여러 이해 당사자들이 참여합니다; 반면에 소기업들은 이러한 자원을 가지고 있지 않고, 그 결과 사용자 인터페이스에 어려움을 겪을 수 있습니다.\n\n저의 목표는 최신 딥러닝 알고리즘을 사용하여 설계 워크플로우를 크게 간소화하고 모든 기업이 웹 페이지를 신속하게 만들고 테스트할 수 있도록 지원하는 것이었습니다. \n\n### 오늘날의 설계 워크플로우\n![Preview](https://github.com/vbnm134678/sketch-code/blob/documentation-korean-translation/image/document_image1.png \"설계 워크플로우는 여러 이해 관계자를 거치게 된다.\")\n\n일반적인 설계 워크플로우는 다음과 같을 수 있습니다:\n- 제품 관리자는 제품 사양 리스트를 만들기 위해 사용자 조사를 수행합니다.\n- 설계자는 요구 사항들을 수용하고 품질 낮은 프로토타입을 조사하여, 결과적으로 품질이 좋은 모델을 만듭니다.\n- 엔지니어는 이러한 설계를 코드로 구현하고 사용자에게 제품을 배포합니다. \n\n개발 주기는 병목현상의 영향으로 빠르게 변할 수도 있고 Airbnb같은 회사들은 이런 과정을 더 효율적으로 만들기 위해 [머신러닝](https://airbnb.design/sketching-interfaces/)을 사용하기 시작했습니다.\n\n![Preview](https://github.com/vbnm134678/sketch-code/blob/documentation-korean-translation/image/document_image2.png \"그림에서 코드로 변환하는 Airbnb의 내부 AI 툴 데모\")\n\n이것이 딥 러닝 지원 설계의 예시로서 유망하기는 하지만, 이 모델이 얼마나 완전하게 훈련을 받았는지, 손으로 그린 그림의 특징에 얼마나 의존하는지는 불분명합니다. 회사만의 비공개 솔루션이기 때문에 확실히 알 수 있는 방법은 없습니다. 저는 **drawing-to-code** 기술의 오픈 소스 버전을 개발자와 디자이너 커뮤니티에서 사용할 수 있도록 만들고 싶었습니다.\n\n이상적으로, 제 모델은 웹 사이트 디자인의 간단한 손 그림 프로토타입을 가져와서 작동하는 HTML 웹 사이트로 즉시 제작할 수 있습니다.\n\n![Preview](https://github.com/vbnm134678/sketch-code/blob/documentation-korean-translation/image/header_image.png \"SketchCode 모델은 와이어 프레임을 가져와서 HTML 코드를 생성합니다.\")\n\n실제로 위의 예시는 테스트 이미지에서 생성된 실제 웹 사이트 모습입니다! 제 [깃허브](https://github.com/ashnkumar/sketch-code) 페이지에서 코드를 확인할 수 있습니다.\n\n### 이미지 캡션에서 얻은 영감\n제가 해결하던 문제는 [프로그램 합성](https://en.wikipedia.org/wiki/Program_synthesis), 즉 작동하는 소스 코드를 자동적으로 생성하는 것 이었습니다. 프로그램의 합성의 많은 많은 부분이 자연어 명세(번역자: 자연언어의 문장과 모호함을 줄이기 위한 그림이나 표)나 실행추적으로 생성된 코드를 다루지만, 저는 소스 이미지(손으로 그린 와이어프레임)을 활용할 수 있었습니다.\n\n[이미지 캡션(Image Captioning)](https://cs.stanford.edu/people/karpathy/deepimagesent/)은 머신러닝의 잘 연구된 분야입니다. 특히 소스 이미지의 내용을 설명하기 위한 훈련 모델도 있습니다.\n\n![Preview](https://github.com/vbnm134678/sketch-code/blob/documentation-korean-translation/image/document_image3.png \"이미지 캡션 모델은 소스 이미지에 대한 설명을 생성합니다.\")\n\n최근 논문인 [pix2code](https://arxiv.org/abs/1705.07962)와 Emil Wallner의 관련 [프로젝트](https://blog.floydhub.com/turning-design-mockups-into-code-with-deep-learning/?source=techstories.org)에서 영감을 얻어 저는 제 작업을 이미지 캡션으로 재구성하기로 결정했습니다. 웹사이트 와이어프레임을 입력 이미지로, 그에 상응하는 HTML 코드를 출력 텍스트로 하였습니다.\n\n### 올바른 데이터 얻기\n이미지 캡션 기술의 접근 방식을 고려했을 때, 제 이상적인 훈련 데이터 셋은 손으로 그린 와이어프레임 스케치 수 천 쌍과 그에 상응하는 HTML 코드들 이었습니다. 당연하게도, 저는 적절한 데이터 셋을 찾을 수 없었고, 작업에 필요한 데이터들을 직접 만들어야 했습니다.\n\n저는 pix2code 논문의 [오픈 소스 데이터 셋](https://github.com/tonybeltramelli/pix2code)으로 시작했습니다. 이 데이터 셋은 합성적으로 생성된 1,750개의 웹사이트 스크린샷과 관련 소스 코드로 구성되어 있습니다.\n\n![Preview](https://github.com/vbnm134678/sketch-code/blob/documentation-korean-translation/image/document_image4.png \"생성된 웹 사이트 이미지와 소스코드 (pix2code의 데이터 셋)\")\n\n이 데이터셋은 몇 가지 흥미로운 점을 가졌습니다:\n- 데이터 셋의 각 웹 사이트는 버튼, 텍스트 상자 및 div와 같은 몇가지 간단한 [Bootstrap](https://getbootstrap.com/2.3.2/) 요소의 조합으로 구성되어 있습니다. 이는 제 모델이 '어휘'로서 몇 가지 제한된 구성 요소로 웹 사이트가 제작된 다는 것을 의미하지만, 앞으로의 접근 방식은 더 많은 어휘로 만들어져야 합니다. \n- 각 예시의 소스코드는 논문 작성자가 작업을 위해 만든 [도메인별 언어(DSL)](https://en.wikipedia.org/wiki/Domain-specific_language)로 구성되어 있습니다. 각 요소는 HTML과 CSS의 한 부분에 해당하며, 컴파일러는 DSL을 HTML 코드로 변환하는데 사용됩니다. \n\n\n![Preview](https://github.com/vbnm134678/sketch-code/blob/documentation-korean-translation/image/document_image5.png \"웹 사이트를 손으로 그린 버전으로 바꾸기\")\n\n제 작업에 필요한 데이터 셋를 수정하기 위해서는 웹 사이트 이미지를 손으로 그린 것처럼 만들어야 했습니다. 저는 각 이미지를 수정하기 위해 [OpenCV](https://opencv.org/)와 파이썬의 [PIL 라이브러리](https://pillow.readthedocs.io/en/stable/)의 회색조 변환 및 음영 감지 같은 툴을 사용하였습니다.\n\n저는 몇가지 작업을 수행하면서 끝내 원본 웹 사이트의 CSS 스타일시트를 직접 수정하기로 결정했습니다:\n- 페이지 구성 요소의 테두리 반지름을 변경하여 버튼와 div의 모서리를 곡선 처리했습니다.\n- 스케치와 비슷하게 테두리 두께를 조정하고 그림자를 추가했습니다.\n- 글꼴을 필기체로 변경했습니다.\n\n제 최종 파이프라인에는 한 단계 더 추가했는데, 실제 그려진 스케치의 가변성을 위해 비틀기, 이동, 회전 등을 추가하였습니다.\n\n### 이미지 캡션 모델 아키텍처 사용하기\n이제 데이터를 준비했으니 드디어 모델에 데이터를 입력할 수 있게 되었습니다!\n\n저는 이미지 캡션에 사용되는 [모델 아키텍처](https://github.com/emilwallner/Screenshot-to-code/blob/master/README.md)를 활용했습니다. 세 가지 주요 부분으로 구성됩니다:\n- CNN(Convolutional Neural Network)을 사용하여 소스 이미지에서 특징을 추출하는 컴퓨터 비전 모델\n- 소스 코드 토큰의 시퀀스를 인코딩하는 GRU(Gated Recurrent Unit) 언어 모델\n- 이전 두 단계의 출력을 입력으로 받아들이고 다음 시퀀스의 토큰을 예측하는 디코더 모델(GRU)\n\n![Preview](https://github.com/vbnm134678/sketch-code/blob/documentation-korean-translation/image/document_image6.png \"토큰의 시퀀스를 입력으로 사용하여 모델 훈련\")\n\n모델을 훈련시키기 위해 소스 코드를 토큰 시퀀스로 분할했습니다. 모델에 대한 단일 입력은 원본 이미지의 시퀀스 중 하나이며, 모델의 레이블은 소스 코드의 이어지는 다음 토큰입니다. 모델은 [교차 엔트로피 비용](https://en.wikipedia.org/wiki/Cross_entropy)을 손실 함수로 사용하여 모델의 다음 토큰 예측과 실제 다음 토큰을 비교합니다.\n\n모델이 scratch로 코드를 생성하는 작업을 수행할 때 프로세스는 약간 달라집니다. 이미지는 똑같이 CNN 네트워크를 통해 처리되지만 텍스트 프로세스는 시작 시퀀스가 기반이 됩니다. 각 단계에서 시퀀스의 다음 토큰에 대한 모델의 예측이 현재 입력 시퀀스에 추가되고 새로운 입력 시퀀스로 모델에 공급됩니다. 모델이 *END* 토큰을 예측하거나 프로세스가 소스 코드 당 제한된 토큰 수에 도달할 때까지 이 작업이 반복됩니다.\n\n모델에서 예측된 토큰 셋이 생성되면 컴파일러는 DSL 토큰을 HTML로 변환하여 어떤 브라우저에서도 렌더링할 수 있도록 합니다.\n\n### BLEU 점수로 모델 평가하기\n저는 [BLEU 점수](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)로 모델을 평가하기로 결정했습니다. 이는 기계 번역 작업에 사용되는 일반적인 측정기준으로, 동일한 입력이 주어졌을 때 기계 생성 텍스트가  인간이 생성했을 내용과 얼마나 밀접한지 측정합니다.\n\n기본적으로 BLEU는 생성된 텍스트와 참조 텍스트의 n-gram 시퀀스를 비교하여 정확도를 측정합니다. 생성된 HTML의 실제 요소와 상대적인 위치를 고려하므로 이 프로젝트에 적합합니다.\n\n그리고 가장 좋은 점은 -- 생성된 웹사이트를 살펴봄으로써 BLEU 점수를 실제로 볼 수 있었다는 것입니다!\n\n![Preview](https://github.com/vbnm134678/sketch-code/blob/documentation-korean-translation/image/document_image7.png \"BLEU 점수 시각화\")\n\nBLEU의 가장 높은 점수인 1.0에서는 주어진 소스 이미지와 일치하는 위치에 올바른 구성요소를 가지고 있는 반면, 낮은 점수에서는 잘못된 위치에 잘못된 요소가 배치되었습니다. 평가 셋에서 최종 모델은 0.76의 BLEU 점수를 얻었습니다.\n\n### BONUS - 커스텀 설계\n추가로 모델은 페이지의 **골격**(문서의 토큰)만 생성하므로 컴파일 과정에서 사용자 지정 CSS 계층을 추가할 수 있으며, 그 결과 웹 사이트는 다른 스타일을 가질 수 있습니다.\n\n![Preview](https://github.com/vbnm134678/sketch-code/blob/documentation-korean-translation/image/document_image8.png \"단일 그림 => 동시에 다양한 디자인을 만들어 낸다.\")\n\n이렇게 모델 생성 프로세스에서 스타일링을 분리하면 모델을 사용할 때 몇 가지 큰 이점이 있습니다.:\n- SketchCode 모델을 자사 제품에 활용하고자 하는 프런트엔드 엔지니어는 해당 모델을 그대로 사용하고 회사의 스타일 가이드에 따라 단일 CSS 파일만 변경할 수 있습니다.\n- 확장성이 내장되어 있습니다. 단일 소스 이미지를 사용하여 모델 출력을 5, 10, 50개의 서로 다른 미리 정의된 스타일로 즉시 컴파일할 수 있으므로 사용자는 웹 사이트의 여러 버전을 시각화하고 브라우저에서 해당 웹 사이트를 탐색할 수 있습니다.\n\n### 결론과 향후 방향\n이미지 캡션을 활용하여 SketchCode는 손으로 그린 웹 사이트 와이어프레임을 가져와서 몇 초 안에 작동하는 HTML 웹 사이트로 변환할 수 있습니다.\n\n모델에는 몇 가지 제한 사항이 있으며, 이어지는 정보는 다음 단계에 가능한 기능입니다:\n\n- 이 모델은 16가지의 구성요소에 대한 어휘만 훈련되었기 때문에 데이터에서 보이는 것 이외의 토큰을 예측할 수 없습니다. 다음 단계는 이미지, 드롭다운 메뉴 및 형식과 같은 더 많은 요소를 사용하여 추가적인 웹 사이트 예제를 생성하는 것입니다. [부트스트랩 구성 요소](https://getbootstrap.com/docs/4.0/components/buttons/)에서 시작하기 좋습니다.\n- 실제 제작하는 웹사이트에는 더 많은 변동성이 있습니다. 이러한 변동성을 잘 반영하는 훈련 데이터 셋를 만드는 적절한 방법은 실제 웹 사이트를 스크랩하고 사이트 콘텐츠 및 HTML/CSS 코드를 캡처 하는 것입니다.\n- 또한 도면에도 CSS 수정이 완전히 파악하지 못하는 변동성이 있습니다. 손으로 그린 스케치 데이터에 더 많은 변화를 생성하는 방법은 Generative Adversarial Network를 사용하여 사실적으로 보이는 웹 사이트 이미지를 만드는 것일 수 있습니다.\n\n여러분은 이 프로젝트에 사용된 코드를 [여기](https://github.com/ashnkumar/sketch-code) GitHub 페이지에서 찾을 수 있습니다. 저는 이 프로젝트가 다음에 어디로 갈 수 있을지 기대됩니다!\n\n\n###### WRITTEN BY\n##### Ashwin Kumar\n##### Former fintech founder @ycombinator. Startup investor and advisor.\n\n##### Insight\n##### Insight - Your bridge to a thriving career"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.1005859375,
          "content": "# SketchCode\r\n\r\n![](https://img.shields.io/badge/python-3-brightgreen.svg) ![](https://img.shields.io/badge/tensorflow-1.1.0-orange.svg)\r\n\r\n*Generating HTML Code from a hand-drawn wireframe*\r\n\r\n![Preview](https://github.com/ashnkumar/sketch-code/blob/master/header_image.png)\r\n\r\nSketchCode is a deep learning model that takes hand-drawn web mockups and converts them into working HTML code. It uses an [image captioning](https://towardsdatascience.com/image-captioning-in-deep-learning-9cd23fb4d8d2) architecture to generate its HTML markup from hand-drawn website wireframes.\r\n\r\nFor more information, check out this post: [Automating front-end development with deep learning](https://blog.insightdatascience.com/automated-front-end-development-using-deep-learning-3169dd086e82)\r\n\r\nThis project builds on the synthetically generated dataset and model architecture from [pix2code](https://github.com/tonybeltramelli/pix2code) by [Tony Beltramelli](https://github.com/tonybeltramelli) and the [Design Mockups](https://github.com/emilwallner/Screenshot-to-code-in-Keras) project from [Emil Wallner](https://github.com/emilwallner).\r\n\r\n<b>Note:</b> This project is meant as a proof-of-concept; the model isn't (yet) built to generalize to the variability of sketches seen in actual wireframes, and thus its performance relies on wireframes resembling the core dataset.\r\n\r\n\r\n## Setup\r\n### Prerequisites\r\n\r\n- Python 3 (not compatible with python 2)\r\n- pip\r\n\r\n### Install dependencies\r\n\r\n```sh\r\npip install -r requirements.txt\r\n```\r\n\r\n## Example Usage\r\n\r\nDownload the data and pretrained weights:\r\n```sh\r\n# Getting the data, 1,700 images, 342mb\r\ngit clone https://github.com/ashnkumar/sketch-code.git\r\ncd sketch-code\r\ncd scripts\r\n\r\n# Get the data and pretrained weights\r\nsh get_data.sh\r\nsh get_pretrained_model.sh\r\n```\r\n\r\nConverting an example drawn image into HTML code, using pretrained weights:\r\n```sh\r\ncd src\r\n\r\npython convert_single_image.py --png_path ../examples/drawn_example1.png \\\r\n      --output_folder ./generated_html \\\r\n      --model_json_file ../bin/model_json.json \\\r\n      --model_weights_file ../bin/weights.h5\r\n```\r\n\r\n\r\n## General Usage\r\n\r\nConverting a single image into HTML code, using weights:\r\n```sh\r\ncd src\r\n\r\npython convert_single_image.py --png_path {path/to/img.png} \\\r\n      --output_folder {folder/to/output/html} \\\r\n      --model_json_file {path/to/model/json_file.json} \\\r\n      --model_weights_file {path/to/model/weights.h5}\r\n```\r\n\r\nConverting a batch of images in a folder to HTML:\r\n```sh\r\ncd src\r\n\r\npython convert_batch_of_images.py --pngs_path {path/to/folder/with/pngs} \\\r\n      --output_folder {folder/to/output/html} \\\r\n      --model_json_file {path/to/model/json_file.json} \\\r\n      --model_weights_file {path/to/model/weights.h5}\r\n```\r\n\r\nTrain the model:\r\n```sh\r\ncd src\r\n\r\n# training from scratch\r\n# <augment_training_data> adds Keras ImageDataGenerator augmentation for training images\r\npython train.py --data_input_path {path/to/folder/with/pngs/guis} \\\r\n      --validation_split 0.2 \\\r\n      --epochs 10 \\\r\n      --model_output_path {path/to/output/model}\r\n      --augment_training_data 1\r\n\r\n# training starting with pretrained model\r\npython train.py --data_input_path {path/to/folder/with/pngs/guis} \\\r\n      --validation_split 0.2 \\\r\n      --epochs 10 \\\r\n      --model_output_path {path/to/output/model} \\\r\n      --model_json_file ../bin/model_json.json \\\r\n      --model_weights_file ../bin/pretrained_weights.h5 \\\r\n      --augment_training_data 1\r\n```\r\n\r\nEvalute the generated prediction using the [BLEU score](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)\r\n```sh\r\ncd src\r\n\r\n# evaluate single GUI prediction\r\npython evaluate_single_gui.py --original_gui_filepath  {path/to/original/gui/file} \\\r\n      --predicted_gui_filepath {path/to/predicted/gui/file}\r\n\r\n# training starting with pretrained model\r\npython evaluate_batch_guis.py --original_guis_filepath  {path/to/folder/with/original/guis} \\\r\n      --predicted_guis_filepath {path/to/folder/with/predicted/guis}\r\n```\r\n\r\n## License\r\n\r\n### The MIT License (MIT)\r\n\r\nCopyright (c) 2018 Ashwin Kumar<ash.nkumar@gmail.com@gmail.com>\r\n\r\n> Permission is hereby granted, free of charge, to any person obtaining a copy\r\n> of this software and associated documentation files (the \"Software\"), to deal\r\n> in the Software without restriction, including without limitation the rights\r\n> to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n> copies of the Software, and to permit persons to whom the Software is\r\n> furnished to do so, subject to the following conditions:\r\n>\r\n> The above copyright notice and this permission notice shall be included in\r\n> all copies or substantial portions of the Software.\r\n>\r\n> THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n> IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n> FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n> AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n> LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n> OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\r\n> THE SOFTWARE.\r\n"
        },
        {
          "name": "README_kr.md",
          "type": "blob",
          "size": 5.2275390625,
          "content": "# SketchCode\n\n![](https://img.shields.io/badge/python-3-brightgreen.svg) ![](https://img.shields.io/badge/tensorflow-1.1.0-orange.svg)\n\n*손으로 그린 와이어 프레임에서 HTML 코드 생성*\n\n![Preview](https://github.com/vbnm134678/sketch-code/blob/documentation-korean-translation/image/header_image.png)\n\nSketchCode는 손으로 그린 웹 모형을 작동하는 HTML 코드로 변환하는 딥 러닝 모델입니다. [이미지 캡션](https://towardsdatascience.com/image-captioning-in-deep-learning-9cd23fb4d8d2) 아키텍처를 사용하여 손으로 그린 웹 사이트 와이어 프레임에서 HTML 마크업을 생성합니다.\n\n자세한 내용은 다음 게시물을 참조하십시오. : [딥 러닝을 통한 프런트 엔드 개발 자동화](https://blog.insightdatascience.com/automated-front-end-development-using-deep-learning-3169dd086e82)\n\n이 프로젝트는 [Tony Beltramelli](https://github.com/tonybeltramelli)의 [pix2code](https://github.com/tonybeltramelli/pix2code)와 [Emil Wallner](https://github.com/emilwallner)의 [Design Mockups](https://github.com/emilwallner/Screenshot-to-code-in-Keras) 프로젝트에서 합성적으로 생성된 데이터 세트와 모델 아키텍처를 기반으로 합니다.\n\n<b>참고:</b> 이 프로젝트는 개념 증명을 위한 것입니다; 이 모델은 실제 와이어 프레임에서 볼 수 있는 스케치의 가변성에 맞게 만들어지지 않았기 때문에 성능은 코어 데이터 세트와 유사한 와이어 프레임에 의존합니다.\n\n\n## 설정\n### 전제조건\n\n- Python 3 (not compatible with python 2)\n- pip\n\n### Dependencies 설치\n\n```sh\npip install -r requirements.txt\n```\n\n## 예제\n\n데이터 및 사전 훈련된 가중치 다운로드:\n```sh\n# 1,700 images, 342mb의 데이터 가져오기\ngit clone https://github.com/ashnkumar/sketch-code.git\ncd sketch-code\ncd scripts\n\n# 데이터와 사전 훈련된 가중치 가져오기\nsh get_data.sh\nsh get_pretrained_model.sh\n```\n\n미리 훈련된 가중치를 사용하여 예제 그림을 HTML 코드로 변환:\n```sh\ncd src\n\npython convert_single_image.py --png_path ../examples/drawn_example1.png \\\n      --output_folder ./generated_html \\\n      --model_json_file ../bin/model_json.json \\\n      --model_weights_file ../bin/weights.h5\n```\n\n\n## 일반적인 사용\n\n가중치를 사용하여 단일 이미지를 HTML 코드로 변환:\n```sh\ncd src\n\npython convert_single_image.py --png_path {path/to/img.png} \\\n      --output_folder {folder/to/output/html} \\\n      --model_json_file {path/to/model/json_file.json} \\\n      --model_weights_file {path/to/model/weights.h5}\n```\n\n폴더의 이미지 batch를 HTML 코드로 변환:\n```sh\ncd src\n\npython convert_batch_of_images.py --pngs_path {path/to/folder/with/pngs} \\\n      --output_folder {folder/to/output/html} \\\n      --model_json_file {path/to/model/json_file.json} \\\n      --model_weights_file {path/to/model/weights.h5}\n```\n\n모델 훈련:\n```sh\ncd src\n\n# scratch를 사용하여 훈련\n# <augment_training_data>는 이미지 훈련을 위한 Keras ImageDataGenerator의 augment 기능을 추가\npython train.py --data_input_path {path/to/folder/with/pngs/guis} \\\n      --validation_split 0.2 \\\n      --epochs 10 \\\n      --model_output_path {path/to/output/model}\n      --augment_training_data 1\n\n# 사전 훈련된 모델로 훈련 시작하기\npython train.py --data_input_path {path/to/folder/with/pngs/guis} \\\n      --validation_split 0.2 \\\n      --epochs 10 \\\n      --model_output_path {path/to/output/model} \\\n      --model_json_file ../bin/model_json.json \\\n      --model_weights_file ../bin/pretrained_weights.h5 \\\n      --augment_training_data 1\n```\n\n[BLEU score](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)를 사용한 예측 평가\n```sh\ncd src\n\n# GUI 예측 평가\npython evaluate_single_gui.py --original_gui_filepath  {path/to/original/gui/file} \\\n      --predicted_gui_filepath {path/to/predicted/gui/file}\n\n# 사전 훈련된 모델로 훈련 시작하기\npython evaluate_batch_guis.py --original_guis_filepath  {path/to/folder/with/original/guis} \\\n      --predicted_guis_filepath {path/to/folder/with/predicted/guis}\n```\n\n## 라이센스\n\n### The MIT License (MIT)\n\nCopyright (c) 2018 Ashwin Kumar<ash.nkumar@gmail.com@gmail.com>\n\n> Permission is hereby granted, free of charge, to any person obtaining a copy\n> of this software and associated documentation files (the \"Software\"), to deal\n> in the Software without restriction, including without limitation the rights\n> to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n> copies of the Software, and to permit persons to whom the Software is\n> furnished to do so, subject to the following conditions:\n>\n> The above copyright notice and this permission notice shall be included in\n> all copies or substantial portions of the Software.\n>\n> THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n> IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n> FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n> AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n> LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n> OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n> THE SOFTWARE.\n"
        },
        {
          "name": "_config.yml",
          "type": "blob",
          "size": 0.025390625,
          "content": "theme: jekyll-theme-cayman"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "header_image.png",
          "type": "blob",
          "size": 366.4375,
          "content": null
        },
        {
          "name": "image_for_documents",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.146484375,
          "content": "Keras==2.1.2\ntensorflow==1.4.0\nnltk==3.2.5\nopencv-python==3.3.0.10\nnumpy==1.13.1\nh5py==2.7.1\nmatplotlib==2.0.2\nPillow==4.3.0\ntqdm==4.17.1\nscipy==1.0.0"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "vocabulary.vocab",
          "type": "blob",
          "size": 0.12109375,
          "content": ", { } small-title text quadruple row btn-inactive btn-orange btn-green btn-red double <START> header btn-active <END> single"
        }
      ]
    }
  ]
}