{
  "metadata": {
    "timestamp": 1736560882597,
    "page": 602,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "SevaSk/ecoute",
      "stars": 5933,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0322265625,
          "content": "__pycache__/\n*.wav\nkeys.py\n.venv/"
        },
        {
          "name": "AudioRecorder.py",
          "type": "blob",
          "size": 2.5810546875,
          "content": "import custom_speech_recognition as sr\nimport pyaudiowpatch as pyaudio\nfrom datetime import datetime\n\nRECORD_TIMEOUT = 3\nENERGY_THRESHOLD = 1000\nDYNAMIC_ENERGY_THRESHOLD = False\n\nclass BaseRecorder:\n    def __init__(self, source, source_name):\n        self.recorder = sr.Recognizer()\n        self.recorder.energy_threshold = ENERGY_THRESHOLD\n        self.recorder.dynamic_energy_threshold = DYNAMIC_ENERGY_THRESHOLD\n\n        if source is None:\n            raise ValueError(\"audio source can't be None\")\n\n        self.source = source\n        self.source_name = source_name\n\n    def adjust_for_noise(self, device_name, msg):\n        print(f\"[INFO] Adjusting for ambient noise from {device_name}. \" + msg)\n        with self.source:\n            self.recorder.adjust_for_ambient_noise(self.source)\n        print(f\"[INFO] Completed ambient noise adjustment for {device_name}.\")\n\n    def record_into_queue(self, audio_queue):\n        def record_callback(_, audio:sr.AudioData) -> None:\n            data = audio.get_raw_data()\n            audio_queue.put((self.source_name, data, datetime.utcnow()))\n\n        self.recorder.listen_in_background(self.source, record_callback, phrase_time_limit=RECORD_TIMEOUT)\n\nclass DefaultMicRecorder(BaseRecorder):\n    def __init__(self):\n        super().__init__(source=sr.Microphone(sample_rate=16000), source_name=\"You\")\n        self.adjust_for_noise(\"Default Mic\", \"Please make some noise from the Default Mic...\")\n\nclass DefaultSpeakerRecorder(BaseRecorder):\n    def __init__(self):\n        with pyaudio.PyAudio() as p:\n            wasapi_info = p.get_host_api_info_by_type(pyaudio.paWASAPI)\n            default_speakers = p.get_device_info_by_index(wasapi_info[\"defaultOutputDevice\"])\n            \n            if not default_speakers[\"isLoopbackDevice\"]:\n                for loopback in p.get_loopback_device_info_generator():\n                    if default_speakers[\"name\"] in loopback[\"name\"]:\n                        default_speakers = loopback\n                        break\n                else:\n                    print(\"[ERROR] No loopback device found.\")\n        \n        source = sr.Microphone(speaker=True,\n                               device_index= default_speakers[\"index\"],\n                               sample_rate=int(default_speakers[\"defaultSampleRate\"]),\n                               chunk_size=pyaudio.get_sample_size(pyaudio.paInt16),\n                               channels=default_speakers[\"maxInputChannels\"])\n        super().__init__(source=source, source_name=\"Speaker\")\n        self.adjust_for_noise(\"Default Speaker\", \"Please make or play some noise from the Default Speaker...\")"
        },
        {
          "name": "AudioTranscriber.py",
          "type": "blob",
          "size": 4.3994140625,
          "content": "import whisper\nimport torch\nimport wave\nimport os\nimport threading\nimport tempfile\nimport custom_speech_recognition as sr\nimport io\nfrom datetime import timedelta\nimport pyaudiowpatch as pyaudio\nfrom heapq import merge\n\nPHRASE_TIMEOUT = 3.05\n\nMAX_PHRASES = 10\n\nclass AudioTranscriber:\n    def __init__(self, mic_source, speaker_source, model):\n        self.transcript_data = {\"You\": [], \"Speaker\": []}\n        self.transcript_changed_event = threading.Event()\n        self.audio_model = model\n        self.audio_sources = {\n            \"You\": {\n                \"sample_rate\": mic_source.SAMPLE_RATE,\n                \"sample_width\": mic_source.SAMPLE_WIDTH,\n                \"channels\": mic_source.channels,\n                \"last_sample\": bytes(),\n                \"last_spoken\": None,\n                \"new_phrase\": True,\n                \"process_data_func\": self.process_mic_data\n            },\n            \"Speaker\": {\n                \"sample_rate\": speaker_source.SAMPLE_RATE,\n                \"sample_width\": speaker_source.SAMPLE_WIDTH,\n                \"channels\": speaker_source.channels,\n                \"last_sample\": bytes(),\n                \"last_spoken\": None,\n                \"new_phrase\": True,\n                \"process_data_func\": self.process_speaker_data\n            }\n        }\n\n    def transcribe_audio_queue(self, audio_queue):\n        while True:\n            who_spoke, data, time_spoken = audio_queue.get()\n            self.update_last_sample_and_phrase_status(who_spoke, data, time_spoken)\n            source_info = self.audio_sources[who_spoke]\n\n            text = ''\n            try:\n                fd, path = tempfile.mkstemp(suffix=\".wav\")\n                os.close(fd)\n                source_info[\"process_data_func\"](source_info[\"last_sample\"], path)\n                text = self.audio_model.get_transcription(path)\n            except Exception as e:\n                print(e)\n            finally:\n                os.unlink(path)\n\n            if text != '' and text.lower() != 'you':\n                self.update_transcript(who_spoke, text, time_spoken)\n                self.transcript_changed_event.set()\n\n    def update_last_sample_and_phrase_status(self, who_spoke, data, time_spoken):\n        source_info = self.audio_sources[who_spoke]\n        if source_info[\"last_spoken\"] and time_spoken - source_info[\"last_spoken\"] > timedelta(seconds=PHRASE_TIMEOUT):\n            source_info[\"last_sample\"] = bytes()\n            source_info[\"new_phrase\"] = True\n        else:\n            source_info[\"new_phrase\"] = False\n\n        source_info[\"last_sample\"] += data\n        source_info[\"last_spoken\"] = time_spoken \n\n    def process_mic_data(self, data, temp_file_name):\n        audio_data = sr.AudioData(data, self.audio_sources[\"You\"][\"sample_rate\"], self.audio_sources[\"You\"][\"sample_width\"])\n        wav_data = io.BytesIO(audio_data.get_wav_data())\n        with open(temp_file_name, 'w+b') as f:\n            f.write(wav_data.read())\n\n    def process_speaker_data(self, data, temp_file_name):\n        with wave.open(temp_file_name, 'wb') as wf:\n            wf.setnchannels(self.audio_sources[\"Speaker\"][\"channels\"])\n            p = pyaudio.PyAudio()\n            wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n            wf.setframerate(self.audio_sources[\"Speaker\"][\"sample_rate\"])\n            wf.writeframes(data)\n\n    def update_transcript(self, who_spoke, text, time_spoken):\n        source_info = self.audio_sources[who_spoke]\n        transcript = self.transcript_data[who_spoke]\n\n        if source_info[\"new_phrase\"] or len(transcript) == 0:\n            if len(transcript) > MAX_PHRASES:\n                transcript.pop(-1)\n            transcript.insert(0, (f\"{who_spoke}: [{text}]\\n\\n\", time_spoken))\n        else:\n            transcript[0] = (f\"{who_spoke}: [{text}]\\n\\n\", time_spoken)\n\n    def get_transcript(self):\n        combined_transcript = list(merge(\n            self.transcript_data[\"You\"], self.transcript_data[\"Speaker\"], \n            key=lambda x: x[1], reverse=True))\n        combined_transcript = combined_transcript[:MAX_PHRASES]\n        return \"\".join([t[0] for t in combined_transcript])\n    \n    def clear_transcript_data(self):\n        self.transcript_data[\"You\"].clear()\n        self.transcript_data[\"Speaker\"].clear()\n\n        self.audio_sources[\"You\"][\"last_sample\"] = bytes()\n        self.audio_sources[\"Speaker\"][\"last_sample\"] = bytes()\n\n        self.audio_sources[\"You\"][\"new_phrase\"] = True\n        self.audio_sources[\"Speaker\"][\"new_phrase\"] = True"
        },
        {
          "name": "GPTResponder.py",
          "type": "blob",
          "size": 1.69140625,
          "content": "import openai\nfrom keys import OPENAI_API_KEY\nfrom prompts import create_prompt, INITIAL_RESPONSE\nimport time\n\nopenai.api_key = OPENAI_API_KEY\n\ndef generate_response_from_transcript(transcript):\n    try:\n        response = openai.ChatCompletion.create(\n                model=\"gpt-3.5-turbo-0301\",\n                messages=[{\"role\": \"system\", \"content\": create_prompt(transcript)}],\n                temperature = 0.0\n        )\n    except Exception as e:\n        print(e)\n        return ''\n    full_response = response.choices[0].message.content\n    try:\n        return full_response.split('[')[1].split(']')[0]\n    except:\n        return ''\n    \nclass GPTResponder:\n    def __init__(self):\n        self.response = INITIAL_RESPONSE\n        self.response_interval = 2\n\n    def respond_to_transcriber(self, transcriber):\n        while True:\n            if transcriber.transcript_changed_event.is_set():\n                start_time = time.time()\n\n                transcriber.transcript_changed_event.clear() \n                transcript_string = transcriber.get_transcript()\n                response = generate_response_from_transcript(transcript_string)\n                \n                end_time = time.time()  # Measure end time\n                execution_time = end_time - start_time  # Calculate the time it took to execute the function\n                \n                if response != '':\n                    self.response = response\n\n                remaining_time = self.response_interval - execution_time\n                if remaining_time > 0:\n                    time.sleep(remaining_time)\n            else:\n                time.sleep(0.3)\n\n    def update_response_interval(self, interval):\n        self.response_interval = interval"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0380859375,
          "content": "MIT License\n\nCopyright (c) 2023 SevaSk\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.9951171875,
          "content": "\n# 🎧 Ecoute\n\nEcoute is a live transcription tool that provides real-time transcripts for both the user's microphone input (You) and the user's speakers output (Speaker) in a textbox. It also generates a suggested response using OpenAI's GPT-3.5 for the user to say based on the live transcription of the conversation.\n\n## 📖 Demo\n\nhttps://github.com/SevaSk/ecoute/assets/50382291/8ac48927-8a26-49fd-80e9-48f980986208\n\nEcoute is designed to help users in their conversations by providing live transcriptions and generating contextually relevant responses. By leveraging the power of OpenAI's GPT-3.5, Ecoute aims to make communication more efficient and enjoyable.\n\n## 🚀 Getting Started\n\nFollow these steps to set up and run Ecoute on your local machine.\n\n### 📋 Prerequisites\n\n- Python >=3.8.0\n- An OpenAI API key that can access OpenAI API (set up a paid account OpenAI account)\n- Windows OS (Not tested on others)\n- FFmpeg \n\nIf FFmpeg is not installed in your system, you can follow the steps below to install it.\n\nFirst, you need to install Chocolatey, a package manager for Windows. Open your PowerShell as Administrator and run the following command:\n```\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n```\nOnce Chocolatey is installed, you can install FFmpeg by running the following command in your PowerShell:\n```\nchoco install ffmpeg\n```\nPlease ensure that you run these commands in a PowerShell window with administrator privileges. If you face any issues during the installation, you can visit the official Chocolatey and FFmpeg websites for troubleshooting.\n\n### 🔧 Installation\n\n1. Clone the repository:\n\n   ```\n   git clone https://github.com/SevaSk/ecoute\n   ```\n\n2. Navigate to the `ecoute` folder:\n\n   ```\n   cd ecoute\n   ```\n\n3. Install the required packages:\n\n   ```\n   pip install -r requirements.txt\n   ```\n   \n4. Create a `keys.py` file in the ecoute directory and add your OpenAI API key:\n\n   - Option 1: You can utilize a command on your command prompt. Run the following command, ensuring to replace \"API KEY\" with your actual OpenAI API key:\n\n      ```\n      python -c \"with open('keys.py', 'w', encoding='utf-8') as f: f.write('OPENAI_API_KEY=\\\"API KEY\\\"')\"\n      ```\n\n   - Option 2: You can create the keys.py file manually. Open up your text editor of choice and enter the following content:\n   \n      ```\n      OPENAI_API_KEY=\"API KEY\"\n      ```\n      Replace \"API KEY\" with your actual OpenAI API key. Save this file as keys.py within the ecoute directory.\n\n### 🎬 Running Ecoute\n\nRun the main script:\n\n```\npython main.py\n```\n\nFor a more better and faster version that also works with most languages, use:\n\n```\npython main.py --api\n```\n\nUpon initiation, Ecoute will begin transcribing your microphone input and speaker output in real-time, generating a suggested response based on the conversation. Please note that it might take a few seconds for the system to warm up before the transcription becomes real-time.\n\nThe --api flag will use the whisper api for transcriptions. This significantly enhances transcription speed and accuracy, and it works in most languages (rather than just English without the flag). It's expected to become the default option in future releases. However, keep in mind that using the Whisper API will consume more OpenAI credits than using the local model. This increased cost is attributed to the advanced features and capabilities that the Whisper API provides. Despite the additional expense, the substantial improvements in speed and transcription accuracy may make it a worthwhile investment for your use case.\n\n### ⚠️ Limitations\n\nWhile Ecoute provides real-time transcription and response suggestions, there are several known limitations to its functionality that you should be aware of:\n\n**Default Mic and Speaker:** Ecoute is currently configured to listen only to the default microphone and speaker set in your system. It will not detect sound from other devices or systems. If you wish to use a different mic or speaker, you will need to set it as your default device in your system settings.\n\n**Whisper Model**: If the --api flag is not used, we utilize the 'tiny' version of the Whisper ASR model, due to its low resource consumption and fast response times. However, this model may not be as accurate as the larger models in transcribing certain types of speech, including accents or uncommon words.\n\n**Language**: If you are not using the --api flag the Whisper model used in Ecoute is set to English. As a result, it may not accurately transcribe non-English languages or dialects. We are actively working to add multi-language support to future versions of the program.\n\n## 📖 License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## 🤝 Contributing\n\nContributions are welcome! Feel free to open issues or submit pull requests to improve Ecoute.\n"
        },
        {
          "name": "TranscriberModels.py",
          "type": "blob",
          "size": 0.978515625,
          "content": "import openai\nimport whisper\nimport os\nimport torch\n\ndef get_model(use_api):\n    if use_api:\n        return APIWhisperTranscriber()\n    else:\n        return WhisperTranscriber()\n\nclass WhisperTranscriber:\n    def __init__(self):\n        self.audio_model = whisper.load_model(os.path.join(os.getcwd(), 'tiny.en.pt'))\n        print(f\"[INFO] Whisper using GPU: \" + str(torch.cuda.is_available()))\n\n    def get_transcription(self, wav_file_path):\n        try:\n            result = self.audio_model.transcribe(wav_file_path, fp16=torch.cuda.is_available())\n        except Exception as e:\n            print(e)\n            return ''\n        return result['text'].strip()\n    \nclass APIWhisperTranscriber:\n    def get_transcription(self, wav_file_path):\n        try:\n            with open(wav_file_path, \"rb\") as audio_file:\n                result = openai.Audio.transcribe(\"whisper-1\", audio_file)\n        except Exception as e:\n            print(e)\n            return ''\n        return result['text'].strip()"
        },
        {
          "name": "custom_speech_recognition",
          "type": "tree",
          "content": null
        },
        {
          "name": "main.py",
          "type": "blob",
          "size": 4.970703125,
          "content": "import threading\nfrom AudioTranscriber import AudioTranscriber\nfrom GPTResponder import GPTResponder\nimport customtkinter as ctk\nimport AudioRecorder \nimport queue\nimport time\nimport torch\nimport sys\nimport TranscriberModels\nimport subprocess\n\ndef write_in_textbox(textbox, text):\n    textbox.delete(\"0.0\", \"end\")\n    textbox.insert(\"0.0\", text)\n\ndef update_transcript_UI(transcriber, textbox):\n    transcript_string = transcriber.get_transcript()\n    write_in_textbox(textbox, transcript_string)\n    textbox.after(300, update_transcript_UI, transcriber, textbox)\n\ndef update_response_UI(responder, textbox, update_interval_slider_label, update_interval_slider, freeze_state):\n    if not freeze_state[0]:\n        response = responder.response\n\n        textbox.configure(state=\"normal\")\n        write_in_textbox(textbox, response)\n        textbox.configure(state=\"disabled\")\n\n        update_interval = int(update_interval_slider.get())\n        responder.update_response_interval(update_interval)\n        update_interval_slider_label.configure(text=f\"Update interval: {update_interval} seconds\")\n\n    textbox.after(300, update_response_UI, responder, textbox, update_interval_slider_label, update_interval_slider, freeze_state)\n\ndef clear_context(transcriber, audio_queue):\n    transcriber.clear_transcript_data()\n    with audio_queue.mutex:\n        audio_queue.queue.clear()\n\ndef create_ui_components(root):\n    ctk.set_appearance_mode(\"dark\")\n    ctk.set_default_color_theme(\"dark-blue\")\n    root.title(\"Ecoute\")\n    root.configure(bg='#252422')\n    root.geometry(\"1000x600\")\n\n    font_size = 20\n\n    transcript_textbox = ctk.CTkTextbox(root, width=300, font=(\"Arial\", font_size), text_color='#FFFCF2', wrap=\"word\")\n    transcript_textbox.grid(row=0, column=0, padx=10, pady=20, sticky=\"nsew\")\n\n    response_textbox = ctk.CTkTextbox(root, width=300, font=(\"Arial\", font_size), text_color='#639cdc', wrap=\"word\")\n    response_textbox.grid(row=0, column=1, padx=10, pady=20, sticky=\"nsew\")\n\n    freeze_button = ctk.CTkButton(root, text=\"Freeze\", command=None)\n    freeze_button.grid(row=1, column=1, padx=10, pady=3, sticky=\"nsew\")\n\n    update_interval_slider_label = ctk.CTkLabel(root, text=f\"\", font=(\"Arial\", 12), text_color=\"#FFFCF2\")\n    update_interval_slider_label.grid(row=2, column=1, padx=10, pady=3, sticky=\"nsew\")\n\n    update_interval_slider = ctk.CTkSlider(root, from_=1, to=10, width=300, height=20, number_of_steps=9)\n    update_interval_slider.set(2)\n    update_interval_slider.grid(row=3, column=1, padx=10, pady=10, sticky=\"nsew\")\n\n    return transcript_textbox, response_textbox, update_interval_slider, update_interval_slider_label, freeze_button\n\ndef main():\n    try:\n        subprocess.run([\"ffmpeg\", \"-version\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    except FileNotFoundError:\n        print(\"ERROR: The ffmpeg library is not installed. Please install ffmpeg and try again.\")\n        return\n\n    root = ctk.CTk()\n    transcript_textbox, response_textbox, update_interval_slider, update_interval_slider_label, freeze_button = create_ui_components(root)\n\n    audio_queue = queue.Queue()\n\n    user_audio_recorder = AudioRecorder.DefaultMicRecorder()\n    user_audio_recorder.record_into_queue(audio_queue)\n\n    time.sleep(2)\n\n    speaker_audio_recorder = AudioRecorder.DefaultSpeakerRecorder()\n    speaker_audio_recorder.record_into_queue(audio_queue)\n\n    model = TranscriberModels.get_model('--api' in sys.argv)\n\n    transcriber = AudioTranscriber(user_audio_recorder.source, speaker_audio_recorder.source, model)\n    transcribe = threading.Thread(target=transcriber.transcribe_audio_queue, args=(audio_queue,))\n    transcribe.daemon = True\n    transcribe.start()\n\n    responder = GPTResponder()\n    respond = threading.Thread(target=responder.respond_to_transcriber, args=(transcriber,))\n    respond.daemon = True\n    respond.start()\n\n    print(\"READY\")\n\n    root.grid_rowconfigure(0, weight=100)\n    root.grid_rowconfigure(1, weight=1)\n    root.grid_rowconfigure(2, weight=1)\n    root.grid_rowconfigure(3, weight=1)\n    root.grid_columnconfigure(0, weight=2)\n    root.grid_columnconfigure(1, weight=1)\n\n     # Add the clear transcript button to the UI\n    clear_transcript_button = ctk.CTkButton(root, text=\"Clear Transcript\", command=lambda: clear_context(transcriber, audio_queue, ))\n    clear_transcript_button.grid(row=1, column=0, padx=10, pady=3, sticky=\"nsew\")\n\n    freeze_state = [False]  # Using list to be able to change its content inside inner functions\n    def freeze_unfreeze():\n        freeze_state[0] = not freeze_state[0]  # Invert the freeze state\n        freeze_button.configure(text=\"Unfreeze\" if freeze_state[0] else \"Freeze\")\n\n    freeze_button.configure(command=freeze_unfreeze)\n\n    update_interval_slider_label.configure(text=f\"Update interval: {update_interval_slider.get()} seconds\")\n\n    update_transcript_UI(transcriber, transcript_textbox)\n    update_response_UI(responder, response_textbox, update_interval_slider_label, update_interval_slider, freeze_state)\n \n    root.mainloop()\n\nif __name__ == \"__main__\":\n    main()"
        },
        {
          "name": "prompts.py",
          "type": "blob",
          "size": 0.505859375,
          "content": "INITIAL_RESPONSE = \"Welcome to Ecoute 👋\"\ndef create_prompt(transcript):\n        return f\"\"\"You are a casual pal, genuinely interested in the conversation at hand. A poor transcription of conversation is given below. \n        \n{transcript}.\n\nPlease respond, in detail, to the conversation. Confidently give a straightforward response to the speaker, even if you don't understand them. Give your response in square brackets. DO NOT ask to repeat, and DO NOT ask for clarification. Just answer the speaker directly.\"\"\""
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1689453125,
          "content": "numpy==1.24.3\nopenai-whisper==20230314\nWave==0.0.2\nopenai==0.27.6\ncustomtkinter==5.1.3\nPyAudioWPatch==0.2.12.5\n--extra-index-url https://download.pytorch.org/whl/cu117\ntorch"
        },
        {
          "name": "tiny.en.pt",
          "type": "blob",
          "size": 73800.1123046875,
          "content": ""
        }
      ]
    }
  ]
}