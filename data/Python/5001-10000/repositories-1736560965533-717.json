{
  "metadata": {
    "timestamp": 1736560965533,
    "page": 717,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "cchen156/Learning-to-See-in-the-Dark",
      "stars": 5467,
      "defaultBranch": "master",
      "files": [
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.08203125,
          "content": "MIT License\n\nCopyright (c) 2018  Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.2548828125,
          "content": "# Learning-to-See-in-the-Dark\n\nThis is a Tensorflow implementation of Learning to See in the Dark in CVPR 2018, by [Chen Chen](http://cchen156.github.io/), [Qifeng Chen](http://cqf.io/), [Jia Xu](http://pages.cs.wisc.edu/~jiaxu/), and [Vladlen Koltun](http://vladlen.info/).  \n\n[Project Website](http://cchen156.github.io/SID.html)<br/>\n[Paper](http://cchen156.github.io/paper/18CVPR_SID.pdf)<br/>\n\n![teaser](images/fig1.png \"Sample inpainting results on held-out images\")\n\nThis code includes the default model for training and testing on the See-in-the-Dark (SID) dataset. \n\n\n## Demo Video\nhttps://youtu.be/qWKUFK7MWvg\n\n## Setup\n\n### Requirement\nRequired python (version 2.7) libraries: Tensorflow (>=1.1) + Scipy + Numpy + Rawpy.\n\nTested in Ubuntu + Intel i7 CPU + Nvidia Titan X (Pascal) with Cuda (>=8.0) and CuDNN (>=5.0). CPU mode should also work with minor changes but not tested.\n\n### Dataset\n\n**Update Aug, 2018:** We found some misalignment with the ground-truth for image 10034, 10045, 10172. Please remove those images for quantitative results, but they still can be used for qualitative evaluations.\n\nYou can download it directly from Google drive for the [Sony](https://storage.googleapis.com/isl-datasets/SID/Sony.zip) (25 GB)  and [Fuji](https://storage.googleapis.com/isl-datasets/SID/Fuji.zip) (52 GB) sets. \n\nThere is download limit by Google drive in a fixed period of time. If you cannot download because of this, try these links: [Sony](https://drive.google.com/open?id=1G6VruemZtpOyHjOC5N8Ww3ftVXOydSXx) (25 GB)  and [Fuji](https://drive.google.com/open?id=1C7GeZ3Y23k1B8reRL79SqnZbRBc4uizH) (52 GB).\n\nNew: we provide file parts in [Baidu Drive](https://pan.baidu.com/s/1fk8EibhBe_M1qG0ax9LQZA) now. After you download all the parts, you can combine them together by running: \"cat SonyPart* > Sony.zip\" and \"cat FujiPart* > Fuji.zip\".\n\n\nThe file lists are provided. In each row, there are a short-exposed image path, the corresponding long-exposed image path, camera ISO and F number. Note that multiple short-exposed images may correspond to the same long-exposed image. \n\nThe file name contains the image information. For example, in \"10019_00_0.033s.RAF\", the first digit \"1\" means it is from the test set (\"0\" for training set and \"2\" for validation set); \"0019\" is the image ID; the following \"00\" is the number in the sequence/burst; \"0.033s\" is the exposure time 1/30 seconds.  \n\n\n### Testing\n1. Clone this repository.\n2. Download the pretrained models by running\n```Shell\npython download_models.py\n```\n3. Run \"python test_Sony.py\". This will generate results on the Sony test set.\n4. Run \"python test_Fuji.py\". This will generate results on the Fuji test set.\n\nBy default, the code takes the data in the \"./dataset/Sony/\" folder and \"./dataset/Fuji/\". If you save the dataset in other folders, please change the \"input_dir\" and \"gt_dir\" at the beginning of the code. \n\n### Training new models\n1. To train the Sony model, run \"python train_Sony.py\". The result and model will be save in \"result_Sony\" folder by default. \n2. To train the Fuji model, run \"python train_Fuji.py\". The result and model will be save in \"result_Fuji\" folder by default. \n\nBy default, the code takes the data in the \"./dataset/Sony/\" folder and \"./dataset/Fuji/\". If you save the dataset in other folders, please change the \"input_dir\" and \"gt_dir\" at the beginning of the code.\n\nLoading the raw data and processing by Rawpy takes significant more time than the backpropagation. By default, the code will load all the groundtruth data processed by Rawpy into memory without 8-bit or 16-bit quantization. This requires at least 64 GB RAM for training the Sony model and 128 GB RAM for the Fuji model. If you need to train it on a machine with less RAM, you may need to revise the code and use the groundtruth data on the disk. We provide the 16-bit groundtruth images processed by Rawpy: [Sony](https://drive.google.com/file/d/1wfkWVkauAsGvXtDJWX0IFDuDl5ozz2PM/view?usp=sharing) (12 GB)  and [Fuji](https://drive.google.com/file/d/1nJM0xYVnzmOZNacBRKebiXA4mBmiTjte/view?usp=sharing) (22 GB). \n\n\n## Citation\nIf you use our code and dataset for research, please cite our paper:\n\nChen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun, \"Learning to See in the Dark\", in CVPR, 2018.\n\n### License\nMIT License.\n\n## FAQ\n1. Can I test my own data using the provided model? \n\nThe proposed method is designed for sensor raw data. The pretrained model probably not work for data from another camera sensor. We do not have support for other camera data. It also does not work for images after camera ISP, i.e., the JPG or PNG data.\n\n2. Will this be in any product?\n\nThis is a research project and a prototype to prove a concept. \n\n3. How can I train the model using my own raw data? \n\nGenerally, you just need to subtract the right black level and pack the data in the same way of Sony/Fuji data. If using rawpy, you need to read the black level instead of using 512 in the provided code. The data range may also differ if it is not 14 bits. You need to normalize it to [0,1] for the network input. \n\n4. Why the results are all black?\n\nIt is often because the pre-trained model not downloaded properly. After downloading, you should get 4 checkpoint related files for the model. \n\n\n## Questions\nIf you have additional questions after reading the FAQ, please email to cchen156@illinois.edu.\n\n"
        },
        {
          "name": "checkpoint",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset",
          "type": "tree",
          "content": null
        },
        {
          "name": "download_dataset.py",
          "type": "blob",
          "size": 1.2294921875,
          "content": "import requests\nimport os\n\ndef download_file_from_google_drive(id, destination):\n    URL = \"https://docs.google.com/uc?export=download\"\n\n    session = requests.Session()\n\n    response = session.get(URL, params = { 'id' : id }, stream = True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { 'id' : id, 'confirm' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)    \n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, \"wb\") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)\n\n\n\nprint('Dowloading Sony subset... (25GB)')\ndownload_file_from_google_drive('10kpAcvldtcb9G2ze5hTcF1odzu4V_Zvh', 'dataset/Sony.zip')\n\nprint('Dowloading Fuji subset... (52GB)')\ndownload_file_from_google_drive('12hvKCjwuilKTZPe9EZ7ZTb-azOmUA3HT', 'dataset/Fuji.zip')\n\nos.system('unzip dataset/Sony.zip -d dataset')\nos.system('unzip dataset/Fuji.zip -d dataset')\n"
        },
        {
          "name": "download_models.py",
          "type": "blob",
          "size": 1.3818359375,
          "content": "import requests\n\ndef download_file_from_google_drive(id, destination):\n    URL = \"https://docs.google.com/uc?export=download\"\n\n    session = requests.Session()\n\n    response = session.get(URL, params = { 'id' : id }, stream = True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { 'id' : id, 'confirm' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)    \n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, \"wb\") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)\n\n\n\nprint('Dowloading Sony Model (84Mb)')\ndownload_file_from_google_drive('1wmx7AM6XWHjHIvpErmIouQgbQoMxAymG', 'checkpoint/Sony/model.ckpt.data-00000-of-00001')\ndownload_file_from_google_drive('1OmrGMng1QuwUa8lf-_wBVvbRJwBr0ETr', 'checkpoint/Sony/model.ckpt.meta')\n\nprint('Dowloading Fuji Model (84Mb)')\ndownload_file_from_google_drive('1PX5wA89d-JLmwQHqpBnyTYJxDVzC1gpt', 'checkpoint/Fuji/model.ckpt.data-00000-of-00001')\ndownload_file_from_google_drive('1VzyzQ9JglcxxqUe8yn3-cAeB1pJ4jxf4', 'checkpoint/Fuji/model.ckpt.meta')\n\n"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "test_Fuji.py",
          "type": "blob",
          "size": 6.9990234375,
          "content": "from __future__ import division\nimport os, scipy.io\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n\ninput_dir = './dataset/Fuji/short/'\ngt_dir = './dataset/Fuji/long/'\ncheckpoint_dir = './checkpoint/Fuji/'\nresult_dir = './result_Fuji/'\n\n# get test IDs\ntest_fns = glob.glob(gt_dir + '1*.RAF')\ntest_ids = [int(os.path.basename(test_fn)[0:5]) for test_fn in test_fns]\n\n\ndef lrelu(x):\n    return tf.maximum(x * 0.2, x)\n\n\ndef upsample_and_concat(x1, x2, output_channels, in_channels):\n    pool_size = 2\n    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n\n    deconv_output = tf.concat([deconv, x2], 3)\n    deconv_output.set_shape([None, None, None, output_channels * 2])\n\n    return deconv_output\n\n\ndef network(input):  # Unet\n    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n\n    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n\n    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n\n    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n\n    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n\n    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n\n    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n\n    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n\n    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n\n    conv10 = slim.conv2d(conv9, 27, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n    out = tf.depth_to_space(conv10, 3)\n    return out\n\n\ndef pack_raw(raw):\n    # pack X-Trans image to 9 channels\n    im = raw.raw_image_visible.astype(np.float32)\n    im = np.maximum(im - 1024, 0) / (16383 - 1024)  # subtract the black level\n\n    img_shape = im.shape\n    H = (img_shape[0] // 6) * 6\n    W = (img_shape[1] // 6) * 6\n\n    out = np.zeros((H // 3, W // 3, 9))\n\n    # 0 R\n    out[0::2, 0::2, 0] = im[0:H:6, 0:W:6]\n    out[0::2, 1::2, 0] = im[0:H:6, 4:W:6]\n    out[1::2, 0::2, 0] = im[3:H:6, 1:W:6]\n    out[1::2, 1::2, 0] = im[3:H:6, 3:W:6]\n\n    # 1 G\n    out[0::2, 0::2, 1] = im[0:H:6, 2:W:6]\n    out[0::2, 1::2, 1] = im[0:H:6, 5:W:6]\n    out[1::2, 0::2, 1] = im[3:H:6, 2:W:6]\n    out[1::2, 1::2, 1] = im[3:H:6, 5:W:6]\n\n    # 1 B\n    out[0::2, 0::2, 2] = im[0:H:6, 1:W:6]\n    out[0::2, 1::2, 2] = im[0:H:6, 3:W:6]\n    out[1::2, 0::2, 2] = im[3:H:6, 0:W:6]\n    out[1::2, 1::2, 2] = im[3:H:6, 4:W:6]\n\n    # 4 R\n    out[0::2, 0::2, 3] = im[1:H:6, 2:W:6]\n    out[0::2, 1::2, 3] = im[2:H:6, 5:W:6]\n    out[1::2, 0::2, 3] = im[5:H:6, 2:W:6]\n    out[1::2, 1::2, 3] = im[4:H:6, 5:W:6]\n\n    # 5 B\n    out[0::2, 0::2, 4] = im[2:H:6, 2:W:6]\n    out[0::2, 1::2, 4] = im[1:H:6, 5:W:6]\n    out[1::2, 0::2, 4] = im[4:H:6, 2:W:6]\n    out[1::2, 1::2, 4] = im[5:H:6, 5:W:6]\n\n    out[:, :, 5] = im[1:H:3, 0:W:3]\n    out[:, :, 6] = im[1:H:3, 1:W:3]\n    out[:, :, 7] = im[2:H:3, 0:W:3]\n    out[:, :, 8] = im[2:H:3, 1:W:3]\n    return out\n\n\nsess = tf.Session()\nin_image = tf.placeholder(tf.float32, [None, None, None, 9])\ngt_image = tf.placeholder(tf.float32, [None, None, None, 3])\nout_image = network(in_image)\n\nsaver = tf.train.Saver()\nsess.run(tf.global_variables_initializer())\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt:\n    print('loaded ' + ckpt.model_checkpoint_path)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\nif not os.path.isdir(result_dir + 'final/'):\n    os.makedirs(result_dir + 'final/')\n\nfor test_id in test_ids:\n    # test the first image in each sequence\n    in_files = glob.glob(input_dir + '%05d_00*.RAF' % test_id)\n    for k in range(len(in_files)):\n        in_path = in_files[k]\n        in_fn = os.path.basename(in_path)\n        print(in_fn)\n        gt_files = glob.glob(gt_dir + '%05d_00*.RAF' % test_id)\n        gt_path = gt_files[0]\n        gt_fn = os.path.basename(gt_path)\n        in_exposure = float(in_fn[9:-5])\n        gt_exposure = float(gt_fn[9:-5])\n        ratio = min(gt_exposure / in_exposure, 300)\n\n        raw = rawpy.imread(in_path)\n        input_full = np.expand_dims(pack_raw(raw), axis=0) * ratio\n        im = raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n        # scale_full = np.expand_dims(np.float32(im/65535.0),axis = 0)*ratio #scale the low-light image using the same ratio\n        scale_full = np.expand_dims(np.float32(im / 65535.0), axis=0)\n\n        gt_raw = rawpy.imread(gt_path)\n        im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n        gt_full = np.expand_dims(np.float32(im / 65535.0), axis=0)\n\n        input_full = np.minimum(input_full, 1.0)\n\n        output = sess.run(out_image, feed_dict={in_image: input_full})\n        output = np.minimum(np.maximum(output, 0), 1)\n\n        _, H, W, _ = output.shape\n\n        output = output[0, :, :, :]\n        gt_full = gt_full[0, 0:H, 0:W, :]\n        scale_full = scale_full[0, 0:H, 0:W, :]\n        scale_full = scale_full * np.mean(gt_full) / np.mean(\n            scale_full)  # scale the low-light image to the same mean of the groundtruth\n\n        scipy.misc.toimage(output * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final/%5d_00_%d_out.png' % (test_id, ratio))\n        scipy.misc.toimage(scale_full * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final/%5d_00_%d_scale.png' % (test_id, ratio))\n        scipy.misc.toimage(gt_full * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final/%5d_00_%d_gt.png' % (test_id, ratio))\n"
        },
        {
          "name": "test_Sony.py",
          "type": "blob",
          "size": 6.2216796875,
          "content": "# uniform content loss + adaptive threshold + per_class_input + recursive G\n# improvement upon cqf37\nfrom __future__ import division\nimport os, scipy.io\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n\ninput_dir = './dataset/Sony/short/'\ngt_dir = './dataset/Sony/long/'\ncheckpoint_dir = './checkpoint/Sony/'\nresult_dir = './result_Sony/'\n\n# get test IDs\ntest_fns = glob.glob(gt_dir + '/1*.ARW')\ntest_ids = [int(os.path.basename(test_fn)[0:5]) for test_fn in test_fns]\n\nDEBUG = 0\nif DEBUG == 1:\n    save_freq = 2\n    test_ids = test_ids[0:5]\n\n\ndef lrelu(x):\n    return tf.maximum(x * 0.2, x)\n\n\ndef upsample_and_concat(x1, x2, output_channels, in_channels):\n    pool_size = 2\n    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n\n    deconv_output = tf.concat([deconv, x2], 3)\n    deconv_output.set_shape([None, None, None, output_channels * 2])\n\n    return deconv_output\n\n\ndef network(input):\n    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n\n    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n\n    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n\n    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n\n    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n\n    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n\n    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n\n    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n\n    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n\n    conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n    out = tf.depth_to_space(conv10, 2)\n    return out\n\n\ndef pack_raw(raw):\n    # pack Bayer image to 4 channels\n    im = raw.raw_image_visible.astype(np.float32)\n    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n\n    im = np.expand_dims(im, axis=2)\n    img_shape = im.shape\n    H = img_shape[0]\n    W = img_shape[1]\n\n    out = np.concatenate((im[0:H:2, 0:W:2, :],\n                          im[0:H:2, 1:W:2, :],\n                          im[1:H:2, 1:W:2, :],\n                          im[1:H:2, 0:W:2, :]), axis=2)\n    return out\n\n\nsess = tf.Session()\nin_image = tf.placeholder(tf.float32, [None, None, None, 4])\ngt_image = tf.placeholder(tf.float32, [None, None, None, 3])\nout_image = network(in_image)\n\nsaver = tf.train.Saver()\nsess.run(tf.global_variables_initializer())\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt:\n    print('loaded ' + ckpt.model_checkpoint_path)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\nif not os.path.isdir(result_dir + 'final/'):\n    os.makedirs(result_dir + 'final/')\n\nfor test_id in test_ids:\n    # test the first image in each sequence\n    in_files = glob.glob(input_dir + '%05d_00*.ARW' % test_id)\n    for k in range(len(in_files)):\n        in_path = in_files[k]\n        in_fn = os.path.basename(in_path)\n        print(in_fn)\n        gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % test_id)\n        gt_path = gt_files[0]\n        gt_fn = os.path.basename(gt_path)\n        in_exposure = float(in_fn[9:-5])\n        gt_exposure = float(gt_fn[9:-5])\n        ratio = min(gt_exposure / in_exposure, 300)\n\n        raw = rawpy.imread(in_path)\n        input_full = np.expand_dims(pack_raw(raw), axis=0) * ratio\n\n        im = raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n        # scale_full = np.expand_dims(np.float32(im/65535.0),axis = 0)*ratio\n        scale_full = np.expand_dims(np.float32(im / 65535.0), axis=0)\n\n        gt_raw = rawpy.imread(gt_path)\n        im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n        gt_full = np.expand_dims(np.float32(im / 65535.0), axis=0)\n\n        input_full = np.minimum(input_full, 1.0)\n\n        output = sess.run(out_image, feed_dict={in_image: input_full})\n        output = np.minimum(np.maximum(output, 0), 1)\n\n        output = output[0, :, :, :]\n        gt_full = gt_full[0, :, :, :]\n        scale_full = scale_full[0, :, :, :]\n        scale_full = scale_full * np.mean(gt_full) / np.mean(\n            scale_full)  # scale the low-light image to the same mean of the groundtruth\n\n        scipy.misc.toimage(output * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final/%5d_00_%d_out.png' % (test_id, ratio))\n        scipy.misc.toimage(scale_full * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final/%5d_00_%d_scale.png' % (test_id, ratio))\n        scipy.misc.toimage(gt_full * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final/%5d_00_%d_gt.png' % (test_id, ratio))\n"
        },
        {
          "name": "train_Fuji.py",
          "type": "blob",
          "size": 8.46484375,
          "content": "from __future__ import division\nimport os, time, scipy.io\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n\ninput_dir = './dataset/Fuji/short/'\ngt_dir = './dataset/Fuji/long/'\ncheckpoint_dir = './result_Fuji/'\nresult_dir = './result_Fuji/'\n\n# get train IDs\ntrain_fns = glob.glob(gt_dir + '0*.RAF')\ntrain_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]\n\nps = 512  # patch size for training\nsave_freq = 500\n\n\ndef lrelu(x):\n    return tf.maximum(x * 0.2, x)\n\n\ndef upsample_and_concat(x1, x2, output_channels, in_channels):\n    pool_size = 2\n    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n\n    deconv_output = tf.concat([deconv, x2], 3)\n    deconv_output.set_shape([None, None, None, output_channels * 2])\n\n    return deconv_output\n\n\ndef network(input):  # Unet\n    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n\n    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n\n    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n\n    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n\n    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n\n    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n\n    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n\n    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n\n    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n\n    conv10 = slim.conv2d(conv9, 27, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n    out = tf.depth_to_space(conv10, 3)\n    return out\n\n\ndef pack_raw(raw):\n    # pack X-Trans image to 9 channels\n    im = raw.raw_image_visible.astype(np.float32)\n    im = np.maximum(im - 1024, 0) / (16383 - 1024)  # subtract the black level\n\n    img_shape = im.shape\n    H = (img_shape[0] // 6) * 6\n    W = (img_shape[1] // 6) * 6\n\n    out = np.zeros((H // 3, W // 3, 9))\n\n    # 0 R\n    out[0::2, 0::2, 0] = im[0:H:6, 0:W:6]\n    out[0::2, 1::2, 0] = im[0:H:6, 4:W:6]\n    out[1::2, 0::2, 0] = im[3:H:6, 1:W:6]\n    out[1::2, 1::2, 0] = im[3:H:6, 3:W:6]\n\n    # 1 G\n    out[0::2, 0::2, 1] = im[0:H:6, 2:W:6]\n    out[0::2, 1::2, 1] = im[0:H:6, 5:W:6]\n    out[1::2, 0::2, 1] = im[3:H:6, 2:W:6]\n    out[1::2, 1::2, 1] = im[3:H:6, 5:W:6]\n\n    # 1 B\n    out[0::2, 0::2, 2] = im[0:H:6, 1:W:6]\n    out[0::2, 1::2, 2] = im[0:H:6, 3:W:6]\n    out[1::2, 0::2, 2] = im[3:H:6, 0:W:6]\n    out[1::2, 1::2, 2] = im[3:H:6, 4:W:6]\n\n    # 4 R\n    out[0::2, 0::2, 3] = im[1:H:6, 2:W:6]\n    out[0::2, 1::2, 3] = im[2:H:6, 5:W:6]\n    out[1::2, 0::2, 3] = im[5:H:6, 2:W:6]\n    out[1::2, 1::2, 3] = im[4:H:6, 5:W:6]\n\n    # 5 B\n    out[0::2, 0::2, 4] = im[2:H:6, 2:W:6]\n    out[0::2, 1::2, 4] = im[1:H:6, 5:W:6]\n    out[1::2, 0::2, 4] = im[4:H:6, 2:W:6]\n    out[1::2, 1::2, 4] = im[5:H:6, 5:W:6]\n\n    out[:, :, 5] = im[1:H:3, 0:W:3]\n    out[:, :, 6] = im[1:H:3, 1:W:3]\n    out[:, :, 7] = im[2:H:3, 0:W:3]\n    out[:, :, 8] = im[2:H:3, 1:W:3]\n    return out\n\n\nsess = tf.Session()\nin_image = tf.placeholder(tf.float32, [None, None, None, 9])\ngt_image = tf.placeholder(tf.float32, [None, None, None, 3])\nout_image = network(in_image)\n\nG_loss = tf.reduce_mean(tf.abs(out_image - gt_image))\n\nt_vars = tf.trainable_variables()\nlr = tf.placeholder(tf.float32)\nG_opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(G_loss)\n\nsaver = tf.train.Saver()\nsess.run(tf.global_variables_initializer())\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt:\n    print('loaded ' + ckpt.model_checkpoint_path)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\n# Raw data takes long time to load. Keep them in memory after loaded.\ngt_images = [None] * 6000\nin_images = {}\nin_images['300'] = [None] * len(train_ids)\nin_images['250'] = [None] * len(train_ids)\nin_images['100'] = [None] * len(train_ids)\n\ng_loss = np.zeros((5000, 1))\n\nallfolders = glob.glob(result_dir + '*0')\nlastepoch = 0\nfor folder in allfolders:\n    lastepoch = np.maximum(lastepoch, int(folder[-4:]))\n\nlearning_rate = 1e-4\nfor epoch in range(lastepoch, 4001):\n    if os.path.isdir(result_dir + '%04d' % epoch):\n        continue\n    cnt = 0\n    if epoch > 2000:\n        learning_rate = 1e-5\n\n    for ind in np.random.permutation(len(train_ids)):\n        # get the path from image id\n        train_id = train_ids[ind]\n        in_files = glob.glob(input_dir + '%05d_00*.RAF' % train_id)\n        in_path = in_files[np.random.random_integers(0, len(in_files) - 1)]\n        in_fn = os.path.basename(in_path)\n\n        gt_files = glob.glob(gt_dir + '%05d_00*.RAF' % train_id)\n        gt_path = gt_files[0]\n        gt_fn = os.path.basename(gt_path)\n        in_exposure = float(in_fn[9:-5])\n        gt_exposure = float(gt_fn[9:-5])\n        ratio = min(gt_exposure / in_exposure, 300)\n\n        st = time.time()\n        cnt += 1\n\n        if in_images[str(ratio)[0:3]][ind] is None:\n            raw = rawpy.imread(in_path)\n            in_images[str(ratio)[0:3]][ind] = np.expand_dims(pack_raw(raw), axis=0) * ratio\n\n            gt_raw = rawpy.imread(gt_path)\n            im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n            gt_images[ind] = np.expand_dims(np.float32(im / 65535.0), axis=0)\n\n        # crop\n        H = in_images[str(ratio)[0:3]][ind].shape[1]\n        W = in_images[str(ratio)[0:3]][ind].shape[2]\n\n        xx = np.random.randint(0, W - ps)\n        yy = np.random.randint(0, H - ps)\n        input_patch = in_images[str(ratio)[0:3]][ind][:, yy:yy + ps, xx:xx + ps, :]\n        gt_patch = gt_images[ind][:, yy * 3:yy * 3 + ps * 3, xx * 3:xx * 3 + ps * 3, :]\n\n        if np.random.randint(2, size=1)[0] == 1:  # random flip\n            input_patch = np.flip(input_patch, axis=1)\n            gt_patch = np.flip(gt_patch, axis=1)\n        if np.random.randint(2, size=1)[0] == 1:\n            input_patch = np.flip(input_patch, axis=2)\n            gt_patch = np.flip(gt_patch, axis=2)\n        if np.random.randint(2, size=1)[0] == 1:  # random transpose\n            input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n            gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n\n        input_patch = np.minimum(input_patch, 1.0)\n\n        _, G_current, output = sess.run([G_opt, G_loss, out_image],\n                                        feed_dict={in_image: input_patch, gt_image: gt_patch, lr: learning_rate})\n        output = np.minimum(np.maximum(output, 0), 1)\n        g_loss[ind] = G_current\n\n        print('%d %d Loss=%.3f Time=%.3f' % (epoch, cnt, np.mean(g_loss[np.where(g_loss)]), time.time() - st))\n\n        if epoch % save_freq == 0:\n            if not os.path.isdir(result_dir + '%04d' % epoch):\n                os.makedirs(result_dir + '%04d' % epoch)\n\n            temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :, :]), axis=1)\n            scipy.misc.toimage(temp * 255, high=255, low=0, cmin=0, cmax=255).save(\n                result_dir + '%04d/%05d_00_train_%d.jpg' % (epoch, train_id, ratio))\n\n    saver.save(sess, checkpoint_dir + 'model.ckpt')\n"
        },
        {
          "name": "train_Sony.py",
          "type": "blob",
          "size": 7.802734375,
          "content": "# uniform content loss + adaptive threshold + per_class_input + recursive G\n# improvement upon cqf37\nfrom __future__ import division\nimport os, time, scipy.io\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n\ninput_dir = './dataset/Sony/short/'\ngt_dir = './dataset/Sony/long/'\ncheckpoint_dir = './result_Sony/'\nresult_dir = './result_Sony/'\n\n# get train IDs\ntrain_fns = glob.glob(gt_dir + '0*.ARW')\ntrain_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]\n\nps = 512  # patch size for training\nsave_freq = 500\n\nDEBUG = 0\nif DEBUG == 1:\n    save_freq = 2\n    train_ids = train_ids[0:5]\n\n\ndef lrelu(x):\n    return tf.maximum(x * 0.2, x)\n\n\ndef upsample_and_concat(x1, x2, output_channels, in_channels):\n    pool_size = 2\n    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n\n    deconv_output = tf.concat([deconv, x2], 3)\n    deconv_output.set_shape([None, None, None, output_channels * 2])\n\n    return deconv_output\n\n\ndef network(input):\n    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n\n    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n\n    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n\n    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n\n    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n\n    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n\n    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n\n    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n\n    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n\n    conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n    out = tf.depth_to_space(conv10, 2)\n    return out\n\n\ndef pack_raw(raw):\n    # pack Bayer image to 4 channels\n    im = raw.raw_image_visible.astype(np.float32)\n    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n\n    im = np.expand_dims(im, axis=2)\n    img_shape = im.shape\n    H = img_shape[0]\n    W = img_shape[1]\n\n    out = np.concatenate((im[0:H:2, 0:W:2, :],\n                          im[0:H:2, 1:W:2, :],\n                          im[1:H:2, 1:W:2, :],\n                          im[1:H:2, 0:W:2, :]), axis=2)\n    return out\n\n\nsess = tf.Session()\nin_image = tf.placeholder(tf.float32, [None, None, None, 4])\ngt_image = tf.placeholder(tf.float32, [None, None, None, 3])\nout_image = network(in_image)\n\nG_loss = tf.reduce_mean(tf.abs(out_image - gt_image))\n\nt_vars = tf.trainable_variables()\nlr = tf.placeholder(tf.float32)\nG_opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(G_loss)\n\nsaver = tf.train.Saver()\nsess.run(tf.global_variables_initializer())\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt:\n    print('loaded ' + ckpt.model_checkpoint_path)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\n# Raw data takes long time to load. Keep them in memory after loaded.\ngt_images = [None] * 6000\ninput_images = {}\ninput_images['300'] = [None] * len(train_ids)\ninput_images['250'] = [None] * len(train_ids)\ninput_images['100'] = [None] * len(train_ids)\n\ng_loss = np.zeros((5000, 1))\n\nallfolders = glob.glob(result_dir + '*0')\nlastepoch = 0\nfor folder in allfolders:\n    lastepoch = np.maximum(lastepoch, int(folder[-4:]))\n\nlearning_rate = 1e-4\nfor epoch in range(lastepoch, 4001):\n    if os.path.isdir(result_dir + '%04d' % epoch):\n        continue\n    cnt = 0\n    if epoch > 2000:\n        learning_rate = 1e-5\n\n    for ind in np.random.permutation(len(train_ids)):\n        # get the path from image id\n        train_id = train_ids[ind]\n        in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id)\n        in_path = in_files[np.random.random_integers(0, len(in_files) - 1)]\n        in_fn = os.path.basename(in_path)\n\n        gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id)\n        gt_path = gt_files[0]\n        gt_fn = os.path.basename(gt_path)\n        in_exposure = float(in_fn[9:-5])\n        gt_exposure = float(gt_fn[9:-5])\n        ratio = min(gt_exposure / in_exposure, 300)\n\n        st = time.time()\n        cnt += 1\n\n        if input_images[str(ratio)[0:3]][ind] is None:\n            raw = rawpy.imread(in_path)\n            input_images[str(ratio)[0:3]][ind] = np.expand_dims(pack_raw(raw), axis=0) * ratio\n\n            gt_raw = rawpy.imread(gt_path)\n            im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n            gt_images[ind] = np.expand_dims(np.float32(im / 65535.0), axis=0)\n\n        # crop\n        H = input_images[str(ratio)[0:3]][ind].shape[1]\n        W = input_images[str(ratio)[0:3]][ind].shape[2]\n\n        xx = np.random.randint(0, W - ps)\n        yy = np.random.randint(0, H - ps)\n        input_patch = input_images[str(ratio)[0:3]][ind][:, yy:yy + ps, xx:xx + ps, :]\n        gt_patch = gt_images[ind][:, yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2, :]\n\n        if np.random.randint(2, size=1)[0] == 1:  # random flip\n            input_patch = np.flip(input_patch, axis=1)\n            gt_patch = np.flip(gt_patch, axis=1)\n        if np.random.randint(2, size=1)[0] == 1:\n            input_patch = np.flip(input_patch, axis=2)\n            gt_patch = np.flip(gt_patch, axis=2)\n        if np.random.randint(2, size=1)[0] == 1:  # random transpose\n            input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n            gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n\n        input_patch = np.minimum(input_patch, 1.0)\n\n        _, G_current, output = sess.run([G_opt, G_loss, out_image],\n                                        feed_dict={in_image: input_patch, gt_image: gt_patch, lr: learning_rate})\n        output = np.minimum(np.maximum(output, 0), 1)\n        g_loss[ind] = G_current\n\n        print(\"%d %d Loss=%.3f Time=%.3f\" % (epoch, cnt, np.mean(g_loss[np.where(g_loss)]), time.time() - st))\n\n        if epoch % save_freq == 0:\n            if not os.path.isdir(result_dir + '%04d' % epoch):\n                os.makedirs(result_dir + '%04d' % epoch)\n\n            temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :, :]), axis=1)\n            scipy.misc.toimage(temp * 255, high=255, low=0, cmin=0, cmax=255).save(\n                result_dir + '%04d/%05d_00_train_%d.jpg' % (epoch, train_id, ratio))\n\n    saver.save(sess, checkpoint_dir + 'model.ckpt')\n"
        }
      ]
    }
  ]
}