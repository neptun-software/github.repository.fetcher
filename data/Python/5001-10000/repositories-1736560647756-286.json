{
  "metadata": {
    "timestamp": 1736560647756,
    "page": 286,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "netease-youdao/EmotiVoice",
      "stars": 7584,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.3076171875,
          "content": "# The .dockerignore file excludes files from the container build process.\n#\n# https://docs.docker.com/engine/reference/builder/#dockerignore-file\n\n# Exclude Git files\n.git\n.github\n.gitignore\n\n# Exclude Python cache files\n__pycache__\n.mypy_cache\n.pytest_cache\n.ruff_cache\n\n# Exclude Python virtual environment\n/venv\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0537109375,
          "content": "outputs/\nWangZeJun/\n*.pyc\n.vscode/\n__pycache__/\n.idea/\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.5166015625,
          "content": "# syntax=docker/dockerfile:1\nFROM ubuntu:22.04\n\n# install app dependencies\nRUN apt-get update && apt-get install -y python3 python3-pip libsndfile1\nRUN python3 -m pip install torch==1.11.0 torchaudio numpy numba scipy transformers==4.26.1 soundfile yacs\nRUN python3 -m pip install pypinyin jieba\n\n# install app\nRUN mkdir /EmotiVoice\nCOPY . /EmotiVoice/\n\n# final configuration\nEXPOSE 8501\nRUN python3 -m pip install streamlit g2p_en\nWORKDIR /EmotiVoice\nRUN python3 frontend_en.py\nCMD streamlit run demo_page.py --server.port 8501\n"
        },
        {
          "name": "EmotiVoice_UserAgreement_æ˜“é­”å£°ç”¨æˆ·åè®®.pdf",
          "type": "blob",
          "size": 425.37890625,
          "content": null
        },
        {
          "name": "HTTP_API_TtsDemo",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 14.0009765625,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023, YOUDAO\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n                                 Legal Disclaimer and Notices\n\nFurther, the project may contain Third Party open source software (â€œOSSâ€) which include licenses under terms that require YOUDAO to display the following notice: \n\n[Pytorch] \nPytorch is available under modified BSD license. You may find the source codes from https://github.com/pytorch/pytorch, and see the license at https://github.com/pytorch/pytorch/blob/main/LICENSE. \n    \n[ESPnet]\nESPnet is available under Apache 2.0 license. You may find the source codes from https://github.com/espnet/espnet, and see the license at https://github.com/espnet/espnet/blob/master/LICENSE. \n\n[WeTTS]\nWeTTS is available under Apache 2.0 license. You may find the source codes from https://github.com/wenet-e2e/wetts, and see the license at https://github.com/wenet-e2e/wetts/blob/main/LICENSE.\n\n[HiFi-GAN]\nHiFi-GAN is available under MIT license. You may find the source codes from https://github.com/jik876/hifi-gan, and see the license at https://github.com/jik876/hifi-gan/blob/master/LICENSE.\n\n[Transformers]\nTransformers is available under Apache 2.0 license. You may find the source codes from https://github.com/huggingface/transformers, and see the license at https://github.com/huggingface/transformers/blob/main/LICENSE.\n\n[KAN-TTS]\nKAN-TTS is available under MIT license. You may find the source codes from https://github.com/alibaba-damo-academy/KAN-TTS, and see the license at https://github.com/alibaba-damo-academy/KAN-TTS/blob/main/LICENSE.\n\n[StyleTTS]\nStyleTTS is available under MIT license. You may find the source codes from https://github.com/yl4579/StyleTTS, and see the license at https://github.com/yl4579/StyleTTS/blob/main/LICENSE.\n\n[tacotron]\ntacotron is available under MIT license. You may find the source codes from https://github.com/keithito/tacotron, and see the license at https://github.com/keithito/tacotron/blob/master/LICENSE.\n\n[tacotron2]\ntacotron2 is available under BSD 3-Clause license. You may find the source codes from https://github.com/NVIDIA/tacotron2, and see the license at https://github.com/NVIDIA/tacotron2/blob/master/LICENSE.\n\n[torch-stft]\ntorch-stft is available under BSD 3-Clause license. You may find the source codes from https://github.com/pseeth/torch-stft, and see the license at https://github.com/pseeth/torch-stft/blob/master/LICENSE.\n\n[LibriTTS]\nLibriTTS is available under CC BY 4.0 license. You may find the data from https://www.openslr.org/60/, and see the license at https://www.openslr.org/60/.\n\n[Hi-Fi TTS]\nHi-Fi TTS is available under CC BY 4.0 license. You may find the data from https://www.openslr.org/109/, and see the license at https://www.openslr.org/109/.\n\nYOUDAO does not make any representation or warranty with respect to any OSS or free software that may be included in or accompany the Service. YOUDAO HEREBY DISCLAIMS ANY AND ALL LIABILITY TO DEMAND PARTNER OR ANY THIRD PARTY RELATED TO ANY SUCH SOFTWARE THAT MAY BE INCLUDED IN OR ACCOMPANY THE SERVICE. "
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.583984375,
          "content": "<div align=\"center\">\n<a href=\"https://trendshift.io/repositories/4833\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/4833\" alt=\"netease-youdao%2FEmotiVoice | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n<font size=4> README: EN | <a href=\"./README.zh.md\">ä¸­æ–‡</a>  </font>\n    <h1>EmotiVoice ğŸ˜Š: a Multi-Voice and Prompt-Controlled TTS Engine</h1>\n</div>\n\n<div align=\"center\">\n    <a href=\"./README.zh.md\"><img src=\"https://img.shields.io/badge/README-ä¸­æ–‡ç‰ˆæœ¬-red\"></a>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n    <a href=\"./LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache--2.0-yellow\"></a>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n    <a href=\"https://twitter.com/YDopensource\"><img src=\"https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&style={style}\"></a>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n</div>\n<br>\n\n**EmotiVoice** is a powerful and modern open-source text-to-speech engine that is available to you at no cost. EmotiVoice speaks both English and Chinese, and with over 2000 different voices (refer to the [List of Voices](https://github.com/netease-youdao/EmotiVoice/wiki/ğŸ˜Š-voice-wiki-page) for details). The most prominent feature is **emotional synthesis**, allowing you to create speech with a wide range of emotions, including happy, excited, sad, angry and others.\n\nAn easy-to-use web interface is provided. There is also a scripting interface for batch generation of results. \n\nHere are a few samples that EmotiVoice generates:\n\n\n- [Chinese audio sample](https://github.com/netease-youdao/EmotiVoice/assets/3909232/6426d7c1-d620-4bfc-ba03-cd7fc046a4fb)\n  \n- [English audio sample](https://github.com/netease-youdao/EmotiVoice/assets/3909232/8f272eba-49db-493b-b479-2d9e5a419e26)\n  \n- [Fun Chinese English audio sample](https://github.com/netease-youdao/EmotiVoice/assets/3909232/a0709012-c3ef-4182-bb0e-b7a2ba386f1c)\n\n## Demo\n\nA demo is hosted on Replicate, [EmotiVoice](https://replicate.com/bramhooimeijer/emotivoice).\n\n## Hot News\n\n- [x] Tuning voice speed is now supported in 'OpenAI-compatible-TTS API', thanks to [@john9405](https://github.com/john9405). [#90](https://github.com/netease-youdao/EmotiVoice/pull/90) [#67](https://github.com/netease-youdao/EmotiVoice/issues/67) [#77](https://github.com/netease-youdao/EmotiVoice/issues/77)\n\n- [x] [The EmotiVoice app for Mac](https://github.com/netease-youdao/EmotiVoice/releases/download/v0.3/emotivoice-1.0.0-arm64.dmg) was released on December 28th, 2023. Just download and taste EmotiVoice's offerings!\n\n- [x] [The EmotiVoice HTTP API](https://github.com/netease-youdao/EmotiVoice/wiki/HTTP-API) was released on December 6th, 2023. Easier to start, faster to use, and with **over 13,000 free calls**. Additionally, users can explore more captivating voices provided by [Zhiyun](https://ai.youdao.com/).\n- [x] [Voice Cloning with your personal data](https://github.com/netease-youdao/EmotiVoice/wiki/Voice-Cloning-with-your-personal-data) has been released on December 13th, 2023, along with [DataBaker Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/DataBaker) and [LJSpeech Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/LJspeech). \n\n## Features under development\n\n- [ ] Support for more languages, such as Japanese and Korean. [#19](https://github.com/netease-youdao/EmotiVoice/issues/19) [#22](https://github.com/netease-youdao/EmotiVoice/issues/22)\n\nEmotiVoice prioritizes community input and user requests. We welcome your feedback!\n\n## Quickstart\n\n### EmotiVoice Docker image\n\nThe easiest way to try EmotiVoice is by running the docker image. You need a machine with a NVidia GPU. If you have not done so, set up NVidia container toolkit by following the instructions for [Linux](https://www.server-world.info/en/note?os=Ubuntu_22.04&p=nvidia&f=2) or [Windows WSL2](https://github.com/nyp-sit/it3103/blob/main/nvidia-docker-wsl2.md). Then EmotiVoice can be run with,\n\n```sh\ndocker run -dp 127.0.0.1:8501:8501 syq163/emoti-voice:latest\n```\nThe Docker image was updated on January 4th, 2024. If you have an older version, please update it by running the following commands:\n```sh\ndocker pull syq163/emoti-voice:latest\ndocker run -dp 127.0.0.1:8501:8501 -p 127.0.0.1:8000:8000 syq163/emoti-voice:latest\n```\nNow open your browser and navigate to http://localhost:8501 to start using EmotiVoice's powerful TTS capabilities.\n\nStarting from this version, the 'OpenAI-compatible-TTS API' is now accessible via http://localhost:8000/.\n\n### Full installation\n\n```sh\nconda create -n EmotiVoice python=3.8 -y\nconda activate EmotiVoice\npip install torch torchaudio\npip install numpy numba scipy transformers soundfile yacs g2p_en jieba pypinyin pypinyin_dict\npython -m nltk.downloader \"averaged_perceptron_tagger_eng\"\n```\n\n### Prepare model files\n\nWe recommend that users refer to the wiki page [How to download the pretrained model files](https://github.com/netease-youdao/EmotiVoice/wiki/Pretrained-models) if they encounter any issues.\n\n```sh\ngit lfs install\ngit lfs clone https://huggingface.co/WangZeJun/simbert-base-chinese WangZeJun/simbert-base-chinese\n```\nor, you can run:\n```sh\ngit clone https://www.modelscope.cn/syq163/WangZeJun.git\n```\n\n### Inference\n\n1. You can download the [pretrained models](https://drive.google.com/drive/folders/1y6Xwj_GG9ulsAonca_unSGbJ4lxbNymM?usp=sharing) by simply running the following command:\n```sh\ngit clone https://www.modelscope.cn/syq163/outputs.git\n```\n2. The inference text format is `<speaker>|<style_prompt/emotion_prompt/content>|<phoneme>|<content>`. \n\n  - inference text example: `8051|Happy|<sos/eos> [IH0] [M] [AA1] [T] engsp4 [V] [OY1] [S] engsp4 [AH0] engsp1 [M] [AH1] [L] [T] [IY0] engsp4 [V] [OY1] [S] engsp1 [AE1] [N] [D] engsp1 [P] [R] [AA1] [M] [P] [T] engsp4 [K] [AH0] [N] [T] [R] [OW1] [L] [D] engsp1 [T] [IY1] engsp4 [T] [IY1] engsp4 [EH1] [S] engsp1 [EH1] [N] [JH] [AH0] [N] . <sos/eos>|Emoti-Voice - a Multi-Voice and Prompt-Controlled T-T-S Engine`.\n4. You can get phonemes by `python frontend.py data/my_text.txt > data/my_text_for_tts.txt`.\n\n5. Then run:\n```sh\nTEXT=data/inference/text\npython inference_am_vocoder_joint.py \\\n--logdir prompt_tts_open_source_joint \\\n--config_folder config/joint \\\n--checkpoint g_00140000 \\\n--test_file $TEXT\n```\nthe synthesized speech is under `outputs/prompt_tts_open_source_joint/test_audio`.\n\n1. Or if you just want to use the interactive TTS demo page, run:\n```sh\npip install streamlit\nstreamlit run demo_page.py\n```\n\n### OpenAI-compatible-TTS API\n\nThanks to @lewangdev for adding an OpenAI compatible API [#60](../../issues/60). To set it up, use the following command:\n\n```sh\npip install fastapi pydub uvicorn[standard] pyrubberband\nuvicorn openaiapi:app --reload\n```\n\n### Wiki page\n\nYou may find more information from our [wiki](https://github.com/netease-youdao/EmotiVoice/wiki) page.\n\n## Training\n\n[Voice Cloning with your personal data](https://github.com/netease-youdao/EmotiVoice/wiki/Voice-Cloning-with-your-personal-data) has been released on December 13th, 2023.\n\n\n## Roadmap & Future work\n\n- Our future plan can be found in the [ROADMAP](./ROADMAP.md) file.\n- The current implementation focuses on emotion/style control by prompts. It uses only pitch, speed, energy, and emotion as style factors, and does not use gender. But it is not complicated to change it to style/timbre control.\n- Suggestions are welcome. You can file issues or [@ydopensource](https://twitter.com/YDopensource) on twitter.\n\n\n## WeChat group\nWelcome to scan the QR code below and join the WeChat group.\n\n<img src=\"https://github.com/netease-youdao/EmotiVoice/assets/49354974/cc3f4c8b-8369-4e50-89cc-e40d27a6bdeb\" alt=\"qr\" width=\"150\"/>\n\n## Credits\n\n- [PromptTTS](https://speechresearch.github.io/prompttts/). The PromptTTS paper is a key basis of this project.\n- [LibriTTS](https://www.openslr.org/60/). The LibriTTS dataset is used in training of EmotiVoice.\n- [HiFiTTS](https://www.openslr.org/109/). The HiFi TTS dataset is used in training of EmotiVoice.\n- [ESPnet](https://github.com/espnet/espnet). \n- [WeTTS](https://github.com/wenet-e2e/wetts)\n- [HiFi-GAN](https://github.com/jik876/hifi-gan)\n- [Transformers](https://github.com/huggingface/transformers)\n- [tacotron](https://github.com/keithito/tacotron)\n- [KAN-TTS](https://github.com/alibaba-damo-academy/KAN-TTS)\n- [StyleTTS](https://github.com/yl4579/StyleTTS)\n- [Simbert](https://github.com/ZhuiyiTechnology/simbert)\n- [cn2an](https://github.com/Ailln/cn2an). EmotiVoice incorporates cn2an for number processing.\n\n## License\n\nEmotiVoice is provided under the Apache-2.0 License - see the [LICENSE](./LICENSE) file for details.\n\nThe interactive page is provided under the [User Agreement](./EmotiVoice_UserAgreement_æ˜“é­”å£°ç”¨æˆ·åè®®.pdf) file.\n"
        },
        {
          "name": "README.zh.md",
          "type": "blob",
          "size": 7.662109375,
          "content": "<font size=4> README: <a href=\"./README.md\">EN</a> | ä¸­æ–‡  </font>\n\n\n<div align=\"center\">\n    <h1>EmotiVoiceæ˜“é­”å£° ğŸ˜Š: å¤šéŸ³è‰²æç¤ºæ§åˆ¶TTS</h1>\n</div>\n\n<div align=\"center\">\n    <a href=\"./README.md\"><img src=\"https://img.shields.io/badge/README-EN-red\"></a>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n    <a href=\"./LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache--2.0-yellow\"></a>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n    <a href=\"https://twitter.com/YDopensource\"><img src=\"https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&style={style}\"></a>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n</div>\n<br>\n\n**EmotiVoice**æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºTTSå¼•æ“ï¼Œ**å®Œå…¨å…è´¹**ï¼Œæ”¯æŒä¸­è‹±æ–‡åŒè¯­ï¼ŒåŒ…å«2000å¤šç§ä¸åŒçš„éŸ³è‰²ï¼Œä»¥åŠç‰¹è‰²çš„**æƒ…æ„Ÿåˆæˆ**åŠŸèƒ½ï¼Œæ”¯æŒåˆæˆåŒ…å«å¿«ä¹ã€å…´å¥‹ã€æ‚²ä¼¤ã€æ„¤æ€’ç­‰å¹¿æ³›æƒ…æ„Ÿçš„è¯­éŸ³ã€‚\n\nEmotiVoiceæä¾›ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„webç•Œé¢ï¼Œè¿˜æœ‰ç”¨äºæ‰¹é‡ç”Ÿæˆç»“æœçš„è„šæœ¬æ¥å£ã€‚\n\nä»¥ä¸‹æ˜¯EmotiVoiceç”Ÿæˆçš„å‡ ä¸ªç¤ºä¾‹:\n\n- [Chinese audio sample](https://github.com/netease-youdao/EmotiVoice/assets/3909232/6426d7c1-d620-4bfc-ba03-cd7fc046a4fb)\n  \n- [English audio sample](https://github.com/netease-youdao/EmotiVoice/assets/3909232/8f272eba-49db-493b-b479-2d9e5a419e26)\n  \n- [Fun Chinese English audio sample](https://github.com/netease-youdao/EmotiVoice/assets/3909232/a0709012-c3ef-4182-bb0e-b7a2ba386f1c)\n\n## çƒ­é—»é€Ÿé€’\n\n- [x] ç±»OpenAI TTSçš„APIå·²ç»æ”¯æŒè°ƒè¯­é€ŸåŠŸèƒ½ï¼Œæ„Ÿè°¢ [@john9405](https://github.com/john9405). [#90](https://github.com/netease-youdao/EmotiVoice/pull/90) [#67](https://github.com/netease-youdao/EmotiVoice/issues/67) [#77](https://github.com/netease-youdao/EmotiVoice/issues/77)\n- [x] [Macç‰ˆä¸€é”®å®‰è£…åŒ…](https://github.com/netease-youdao/EmotiVoice/releases/download/v0.3/emotivoice-1.0.0-arm64.dmg) å·²äº2023å¹´12æœˆ28æ—¥å‘å¸ƒï¼Œ**å¼ºçƒˆæ¨èå°½å¿«ä¸‹è½½ä½¿ç”¨ï¼Œå…è´¹å¥½ç”¨ï¼**\n- [x] [æ˜“é­”å£° HTTP API](https://github.com/netease-youdao/EmotiVoice/wiki/HTTP-API) å·²äº2023å¹´12æœˆ6æ—¥å‘å¸ƒä¸Šçº¿ã€‚æ›´æ˜“ä¸Šæ‰‹ï¼ˆæ— éœ€ä»»ä½•å®‰è£…é…ç½®ï¼‰ï¼Œæ›´å¿«æ›´ç¨³å®šï¼Œå•è´¦æˆ·æä¾›**è¶…è¿‡ 13,000 æ¬¡å…è´¹è°ƒç”¨**ã€‚æ­¤å¤–ï¼Œç”¨æˆ·è¿˜å¯ä»¥ä½¿ç”¨[æ™ºäº‘](https://ai.youdao.com/)æä¾›çš„å…¶å®ƒè¿·äººçš„å£°éŸ³ã€‚\n- [x] [ç”¨ä½ è‡ªå·±çš„æ•°æ®å®šåˆ¶éŸ³è‰²](https://github.com/netease-youdao/EmotiVoice/wiki/Voice-Cloning-with-your-personal-data)å·²äº2023å¹´12æœˆ13æ—¥å‘å¸ƒä¸Šçº¿ï¼ŒåŒæ—¶æä¾›äº†ä¸¤ä¸ªæ•™ç¨‹ç¤ºä¾‹ï¼š[DataBaker Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/DataBaker)  [LJSpeech Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/LJspeech)ã€‚\n\n## å¼€å‘ä¸­çš„ç‰¹æ€§\n\n- [ ] æ›´å¤šè¯­è¨€æ”¯æŒï¼Œä¾‹å¦‚æ—¥éŸ© [#19](https://github.com/netease-youdao/EmotiVoice/issues/19) [#22](https://github.com/netease-youdao/EmotiVoice/issues/22)\n\næ˜“é­”å£°å€¾å¬ç¤¾åŒºéœ€æ±‚å¹¶ç§¯æå“åº”ï¼ŒæœŸå¾…æ‚¨çš„åé¦ˆï¼\n\n## å¿«é€Ÿå…¥é—¨\n\n### EmotiVoice Dockeré•œåƒ\n\nå°è¯•EmotiVoiceæœ€ç®€å•çš„æ–¹æ³•æ˜¯è¿è¡Œdockeré•œåƒã€‚ä½ éœ€è¦ä¸€å°å¸¦æœ‰NVidia GPUçš„æœºå™¨ã€‚å…ˆæŒ‰ç…§[Linux](https://www.server-world.info/en/note?os=Ubuntu_22.04&p=nvidia&f=2)å’Œ[Windows WSL2](https://zhuanlan.zhihu.com/p/653173679)å¹³å°çš„è¯´æ˜å®‰è£…NVidiaå®¹å™¨å·¥å…·åŒ…ã€‚ç„¶åå¯ä»¥ç›´æ¥è¿è¡ŒEmotiVoiceé•œåƒï¼š\n\n```sh\ndocker run -dp 127.0.0.1:8501:8501 syq163/emoti-voice:latest\n```\n\nDockeré•œåƒæ›´æ–°äº2024å¹´1æœˆ4å·ã€‚å¦‚æœä½ ä½¿ç”¨äº†è€çš„ç‰ˆæœ¬ï¼Œæ¨èè¿è¡Œå¦‚ä¸‹å‘½ä»¤è¿›è¡Œæ›´æ–°ï¼š\n```sh\ndocker pull syq163/emoti-voice:latest\ndocker run -dp 127.0.0.1:8501:8501 -p 127.0.0.1:8000:8000 syq163/emoti-voice:latest\n```\n\nç°åœ¨æ‰“å¼€æµè§ˆå™¨ï¼Œå¯¼èˆªåˆ° http://localhost:8501 ï¼Œå°±å¯ä»¥ä½“éªŒEmotiVoiceå¼ºå¤§çš„TTSåŠŸèƒ½ã€‚ä»2024å¹´çš„dockeré•œåƒç‰ˆæœ¬å¼€å§‹ï¼Œé€šè¿‡http://localhost:8000/å¯ä»¥ä½¿ç”¨ç±»OpenAI TTSçš„APIåŠŸèƒ½ã€‚\n\n### å®Œæ•´å®‰è£…\n\n```sh\nconda create -n EmotiVoice python=3.8 -y\nconda activate EmotiVoice\npip install torch torchaudio\npip install numpy numba scipy transformers soundfile yacs g2p_en jieba pypinyin pypinyin_dict\npython -m nltk.downloader \"averaged_perceptron_tagger_eng\"\n```\n\n### å‡†å¤‡æ¨¡å‹æ–‡ä»¶\n\nå¼ºçƒˆæ¨èç”¨æˆ·å‚è€ƒ[å¦‚ä½•ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶](https://github.com/netease-youdao/EmotiVoice/wiki/Pretrained-models)çš„ç»´åŸºé¡µé¢ï¼Œå°¤å…¶é‡åˆ°é—®é¢˜æ—¶ã€‚\n\n```sh\ngit lfs install\ngit lfs clone https://huggingface.co/WangZeJun/simbert-base-chinese WangZeJun/simbert-base-chinese\n```\n\næˆ–è€…ä½ å¯ä»¥è¿è¡Œ:\n```sh\ngit clone https://www.modelscope.cn/syq163/WangZeJun.git\n```\n\n### æ¨ç†\n\n1. é€šè¿‡ç®€å•è¿è¡Œå¦‚ä¸‹å‘½ä»¤æ¥ä¸‹è½½[é¢„è®­ç»ƒæ¨¡å‹](https://drive.google.com/drive/folders/1y6Xwj_GG9ulsAonca_unSGbJ4lxbNymM?usp=sharing):\n\n```sh\ngit clone https://www.modelscope.cn/syq163/outputs.git\n```\n\n2. æ¨ç†è¾“å…¥æ–‡æœ¬æ ¼å¼æ˜¯ï¼š`<speaker>|<style_prompt/emotion_prompt/content>|<phoneme>|<content>`. \n\n  - ä¾‹å¦‚: `8051|éå¸¸å¼€å¿ƒ|<sos/eos>  uo3 sp1 l ai2 sp0 d ao4 sp1 b ei3 sp0 j ing1 sp3 q ing1 sp0 h ua2 sp0 d a4 sp0 x ve2 <sos/eos>|æˆ‘æ¥åˆ°åŒ—äº¬ï¼Œæ¸…åå¤§å­¦`.\n4. å…¶ä¸­çš„éŸ³ç´ ï¼ˆphonemesï¼‰å¯ä»¥è¿™æ ·å¾—åˆ°ï¼š`python frontend.py data/my_text.txt > data/my_text_for_tts.txt`.\n\n5. ç„¶åè¿è¡Œï¼š\n```sh\nTEXT=data/inference/text\npython inference_am_vocoder_joint.py \\\n--logdir prompt_tts_open_source_joint \\\n--config_folder config/joint \\\n--checkpoint g_00140000 \\\n--test_file $TEXT\n```\nåˆæˆçš„è¯­éŸ³ç»“æœåœ¨ï¼š`outputs/prompt_tts_open_source_joint/test_audio`.\n\n6. æˆ–è€…ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨äº¤äº’çš„ç½‘é¡µç•Œé¢ï¼š\n```sh\npip install streamlit\nstreamlit run demo_page.py\n```\n\n### ç±»OpenAI TTSçš„API\n\néå¸¸æ„Ÿè°¢ @lewangdev çš„ç›¸å…³è¯¥å·¥ä½œ [#60](../../issues/60)ã€‚é€šè¿‡è¿è¡Œå¦‚ä¸‹å‘½ä»¤æ¥å®Œæˆé…ç½®ï¼š\n\n```sh\npip install fastapi pydub uvicorn[standard] pyrubberband\nuvicorn openaiapi:app --reload\n```\n\n### Wikié¡µé¢\n\nå¦‚æœé‡åˆ°é—®é¢˜ï¼Œæˆ–è€…æƒ³è·å–æ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚è€ƒ [wiki](https://github.com/netease-youdao/EmotiVoice/wiki) é¡µé¢ã€‚\n\n## è®­ç»ƒ\n\n[ç”¨ä½ è‡ªå·±çš„æ•°æ®å®šåˆ¶éŸ³è‰²](https://github.com/netease-youdao/EmotiVoice/wiki/Voice-Cloning-with-your-personal-data)å·²äº2023å¹´12æœˆ13æ—¥å‘å¸ƒä¸Šçº¿ã€‚\n\n## è·¯çº¿å›¾å’Œæœªæ¥çš„å·¥ä½œ\n\n- æˆ‘ä»¬æœªæ¥çš„è®¡åˆ’å¯ä»¥åœ¨ [ROADMAP](./ROADMAP.md) æ–‡ä»¶ä¸­æ‰¾åˆ°ã€‚\n\n- å½“å‰çš„å®ç°ä¾§é‡äºé€šè¿‡æç¤ºæ§åˆ¶æƒ…ç»ª/é£æ ¼ã€‚å®ƒåªä½¿ç”¨éŸ³é«˜ã€é€Ÿåº¦ã€èƒ½é‡å’Œæƒ…æ„Ÿä½œä¸ºé£æ ¼å› ç´ ï¼Œè€Œä¸ä½¿ç”¨æ€§åˆ«ã€‚ä½†æ˜¯å°†å…¶æ›´æ”¹ä¸ºæ ·å¼ã€éŸ³è‰²æ§åˆ¶å¹¶ä¸å¤æ‚ï¼Œç±»ä¼¼äºPromptTTSçš„åŸå§‹é—­æºå®ç°ã€‚\n\n## å¾®ä¿¡ç¾¤\n\næ¬¢è¿æ‰«æä¸‹æ–¹å·¦ä¾§äºŒç»´ç åŠ å…¥å¾®ä¿¡ç¾¤ã€‚å•†ä¸šåˆä½œæ‰«æå³ä¾§ä¸ªäººäºŒç»´ç ã€‚\n\n<img src=\"https://github.com/netease-youdao/EmotiVoice/assets/49354974/cc3f4c8b-8369-4e50-89cc-e40d27a6bdeb\" alt=\"qr\" width=\"150\"/>\n&nbsp;&nbsp;&nbsp;&nbsp;\n<img src=\"https://github.com/netease-youdao/EmotiVoice/assets/3909232/94ee0824-0304-4487-8682-664fafd09cdf\" alt=\"qr\" width=\"150\"/>\n\n## è‡´è°¢\n\n- [PromptTTS](https://speechresearch.github.io/prompttts/). PromptTTSè®ºæ–‡æ˜¯æœ¬å·¥ä½œçš„é‡è¦åŸºç¡€ã€‚\n- [LibriTTS](https://www.openslr.org/60/). è®­ç»ƒä½¿ç”¨äº†LibriTTSå¼€æ”¾æ•°æ®é›†ã€‚\n- [HiFiTTS](https://www.openslr.org/109/). è®­ç»ƒä½¿ç”¨äº†HiFi TTSå¼€æ”¾æ•°æ®é›†ã€‚\n- [ESPnet](https://github.com/espnet/espnet). \n- [WeTTS](https://github.com/wenet-e2e/wetts)\n- [HiFi-GAN](https://github.com/jik876/hifi-gan)\n- [Transformers](https://github.com/huggingface/transformers)\n- [tacotron](https://github.com/keithito/tacotron)\n- [KAN-TTS](https://github.com/alibaba-damo-academy/KAN-TTS)\n- [StyleTTS](https://github.com/yl4579/StyleTTS)\n- [Simbert](https://github.com/ZhuiyiTechnology/simbert)\n- [cn2an](https://github.com/Ailln/cn2an). æ˜“é­”å£°é›†æˆäº†cn2anæ¥å¤„ç†æ•°å­—ã€‚\n\n## è®¸å¯\n\nEmotiVoiceæ˜¯æ ¹æ®Apache-2.0è®¸å¯è¯æä¾›çš„ - æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[è®¸å¯è¯æ–‡ä»¶](./LICENSE)ã€‚\n\näº¤äº’çš„ç½‘é¡µæ˜¯æ ¹æ®[ç”¨æˆ·åè®®](./EmotiVoice_UserAgreement_æ˜“é­”å£°ç”¨æˆ·åè®®.pdf)æä¾›çš„ã€‚\n"
        },
        {
          "name": "README_å°ç™½å®‰è£…æ•™ç¨‹.md",
          "type": "blob",
          "size": 7.26171875,
          "content": "## å°ç™½å®‰è£…æ•™ç¨‹\n\n#### ç¯å¢ƒæ¡ä»¶ï¼šè®¾å¤‡æœ‰GPUã€å·²ç»å®‰è£…cuda\n\nè¯´æ˜ï¼šè¿™æ˜¯é’ˆå¯¹Linuxç¯å¢ƒå®‰è£…çš„æ•™ç¨‹ï¼Œå…¶ä»–ç³»ç»Ÿå¯ä½œä¸ºå‚è€ƒã€‚\n\n#### 1ã€åˆ›å»ºå¹¶è¿›å…¥condaç¯å¢ƒ\n\n```\nconda create -n EmotiVoice python=3.8\nconda init\nconda activate EmotiVoice\n```\n\nå¦‚æœä½ ä¸æƒ³ä½¿ç”¨condaç¯å¢ƒï¼Œä¹Ÿå¯ä»¥çœç•¥è¯¥æ­¥éª¤ï¼Œä½†è¦ä¿è¯pythonç‰ˆæœ¬ä¸º3.8\n\n\n#### 2ã€å®‰è£…git-lfs\n\nå¦‚æœæ˜¯Ubuntuåˆ™æ‰§è¡Œ\n\n```\nsudo apt update\nsudo apt install git\nsudo apt-get install git-lfs\n```\n\nCentOSåˆ™æ‰§è¡Œ\n\n```\nsudo yum update\nsudo yum install git\nsudo yum install git-lfs\n```\n\n\n\n#### 3ã€å…‹éš†ä»“åº“\n\n```\ngit lfs install\ngit lfs clone https://github.com/netease-youdao/EmotiVoice.git\n```\n\n\n\n#### 4ã€å®‰è£…ä¾èµ–\n\n```\npip install torch torchaudio\npip install numpy numba scipy transformers soundfile yacs g2p_en jieba pypinyin pypinyin_dict\npython -m nltk.downloader \"averaged_perceptron_tagger_eng\"\n```\n\n\n\n<a id=\"step5\"></a>\n\n#### 5ã€ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶\n\n(1)é¦–å…ˆè¿›å…¥é¡¹ç›®æ–‡ä»¶å¤¹\n\n```\ncd EmotiVoice\n```\n\n(2)æ‰§è¡Œä¸‹é¢å‘½ä»¤\n\n```\ngit lfs clone https://huggingface.co/WangZeJun/simbert-base-chinese WangZeJun/simbert-base-chinese\n```\n\næˆ–è€…\n\n```\ngit clone https://www.modelscope.cn/syq163/WangZeJun.git\n```\n\nä¸Šé¢ä¸¤ç§ä¸‹è½½æ–¹å¼äºŒé€‰ä¸€å³å¯ã€‚\n\n(3)ç¬¬ä¸‰æ­¥ä¸‹è½½ckptæ¨¡å‹\n\n```\ngit clone https://www.modelscope.cn/syq163/outputs.git\n```\n\nä¸Šé¢æ­¥éª¤å®Œæˆåï¼Œé¡¹ç›®æ–‡ä»¶å¤¹å†…ä¼šå¤š `WangZeJun` å’Œ `outputs` æ–‡ä»¶å¤¹ï¼Œä¸‹é¢æ˜¯é¡¹ç›®æ–‡ä»¶ç»“æ„\n\n```\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ EmotiVoice_UserAgreement_æ˜“é­”å£°ç”¨æˆ·åè®®.pdf\nâ”œâ”€â”€ demo_page.py\nâ”œâ”€â”€ frontend.py\nâ”œâ”€â”€ frontend_cn.py\nâ”œâ”€â”€ frontend_en.py\nâ”œâ”€â”€ WangZeJun\nâ”‚   â””â”€â”€ simbert-base-chinese\nâ”‚       â”œâ”€â”€ README.md\nâ”‚       â”œâ”€â”€ config.json\nâ”‚       â”œâ”€â”€ pytorch_model.bin\nâ”‚       â””â”€â”€ vocab.txt\nâ”œâ”€â”€ outputs\nâ”‚   â”œâ”€â”€ README.md\nâ”‚   â”œâ”€â”€ configuration.json\nâ”‚   â”œâ”€â”€ prompt_tts_open_source_joint\nâ”‚   â”‚   â””â”€â”€ ckpt\nâ”‚   â”‚       â”œâ”€â”€ do_00140000\nâ”‚   â”‚       â””â”€â”€ g_00140000\nâ”‚   â””â”€â”€ style_encoder\nâ”‚       â””â”€â”€ ckpt\nâ”‚           â””â”€â”€ checkpoint_163431\n```\n\n\n\n#### 6ã€è¿è¡ŒUIäº¤äº’ç•Œé¢\n\n(1)å®‰è£…streamlit\n\n```\npip install streamlit\n```\n\n(2)å¯åŠ¨\n\næ‰“å¼€è¿è¡Œåæ˜¾ç¤ºçš„serveråœ°å€ï¼Œå¦‚ä½•æ­£å¸¸æ˜¾ç¤ºé¡µé¢åˆ™éƒ¨ç½²å®Œæˆã€‚\n\n```\nstreamlit run demo_page.py --server.port 6006 --logger.level debug\n```\n\n\n\n#### 7ã€å¯åŠ¨APIæœåŠ¡\n\nå®‰è£…ä¾èµ–\n\n```\npip install fastapi pydub uvicorn[standard] pyrubberband\n```\n\nåœ¨6006ç«¯å£å¯åŠ¨æœåŠ¡(ç«¯å£å¯æ ¹æ®è‡ªå·±çš„éœ€æ±‚ä¿®æ”¹)\n\n```\nuvicorn openaiapi:app --reload --port 6006\n```\n\næ¥å£æ–‡æ¡£åœ°å€ï¼šä½ çš„æœåŠ¡åœ°å€+`/docs`\n\n&emsp;\n\n#### 8ã€é‡åˆ°é”™è¯¯\n\n**(1) è¿è¡ŒUIç•Œé¢åï¼Œæ‰“å¼€é¡µé¢ä¸€ç›´æ˜¾ç¤º \"Please wait...\" æˆ–è€…æ˜¾ç¤ºä¸€ç‰‡ç©ºç™½**\n\nåŸå› ï¼š\n\nè¿™ä¸ªé”™è¯¯å¯èƒ½æ˜¯ç”±äºCORSï¼ˆè·¨åŸŸèµ„æºå…±äº«ï¼‰ä¿æŠ¤é…ç½®é”™è¯¯ã€‚\n\nè§£å†³æ–¹æ³•ï¼š\n\nåœ¨å¯åŠ¨æ—¶åŠ ä¸Šä¸€ä¸ª `server.enableCORS=false` å‚æ•°ï¼Œå³ä½¿ç”¨ä¸‹é¢å‘½ä»¤å¯åŠ¨ç¨‹åº\n\n```\nstreamlit run demo_page.py --server.port 6006 --logger.level debug --server.enableCORS=false\n```\n\nå¦‚æœé€šè¿‡ä¸´æ—¶ç¦ç”¨ CORS ä¿æŠ¤è§£å†³äº†é—®é¢˜ï¼Œå»ºè®®é‡æ–°å¯ç”¨ CORS ä¿æŠ¤å¹¶è®¾ç½®æ­£ç¡®çš„ URL å’Œç«¯å£ã€‚\n\n&emsp;\n\n**(2) è¿è¡ŒæŠ¥é”™ raise BadZipFile(\"File is not a zip file\") zipfile.BadZipFile: File is not a zip file**\n\nåŸå› ï¼š\n\nè¿™å¯èƒ½æ˜¯ç”±äºç¼ºå°‘ `averaged_perceptron_tagger`  è¿™ä¸ª`nltk`ä¸­ç”¨äºè¯æ€§æ ‡æ³¨çš„ä¸€ä¸ªåŒ…ï¼Œå®ƒåŒ…å«äº†ä¸€ä¸ªåŸºäºå¹³å‡æ„ŸçŸ¥å™¨ç®—æ³•çš„è¯æ€§æ ‡æ³¨å™¨ã€‚å¦‚æœä½ åœ¨ä»£ç ä¸­ä½¿ç”¨äº†è¿™ä¸ªæ ‡æ³¨å™¨ï¼Œä½†æ˜¯æ²¡æœ‰é¢„å…ˆä¸‹è½½å¯¹åº”çš„æ•°æ®åŒ…ï¼Œå°±ä¼šé‡åˆ°é”™è¯¯ï¼Œæç¤ºä½ ç¼ºå°‘`averaged_perceptron_tagger.zip`æ–‡ä»¶ã€‚å½“ç„¶ä¹Ÿæœ‰å¯èƒ½æ˜¯ç¼ºå°‘ `cmudict` CMU å‘éŸ³è¯å…¸æ•°æ®åŒ…æ–‡ä»¶ã€‚\n\næ­£å¸¸æ¥è¯´ï¼Œåˆæ¬¡è¿è¡Œç¨‹åºNLTKä¼šè‡ªåŠ¨ä¸‹è½½ä½¿ç”¨çš„ç›¸å…³æ•°æ®åŒ…ï¼Œdebugæ¨¡å¼ä¸‹è¿è¡Œä¼šæ˜¾ç¤ºå¦‚ä¸‹ä¿¡æ¯\n\n```\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package cmudict to /root/nltk_data...\n[nltk_data]   Unzipping corpora/cmudict.zip.\n```\n\nå¯èƒ½ç”±äºç½‘ç»œ(éœ€ç§‘å­¦ä¸Šç½‘)ç­‰åŸå› ï¼Œæ²¡èƒ½è‡ªåŠ¨ä¸‹è½½æˆåŠŸï¼Œå› æ­¤ç¼ºå°‘ç›¸å…³æ–‡ä»¶å¯¼è‡´åŠ è½½æŠ¥é”™ã€‚\n\n\n\nè§£å†³æ–¹æ³•ï¼šé‡æ–°ä¸‹è½½ç¼ºå°‘çš„æ•°æ®åŒ…æ–‡ä»¶\n\n\n\n1)æ–¹æ³•ä¸€\n\nåˆ›å»ºä¸€ä¸ª download.pyæ–‡ä»¶ï¼Œåœ¨å…¶ä¸­ç¼–å†™å¦‚ä¸‹ä»£ç \n\n```\nimport nltk\nprint(nltk.data.path)\nnltk.download('averaged_perceptron_tagger')\nnltk.download('cmudict')\n```\n\nä¿å­˜å¹¶è¿è¡Œ\n\n```\npython download.py\n```\n\nè¿™å°†æ˜¾ç¤ºå…¶æ–‡ä»¶ç´¢å¼•ä½ç½®ï¼Œå¹¶è‡ªåŠ¨ä¸‹è½½ ç¼ºå°‘çš„ `averaged_perceptron_tagger.zip`å’Œ `cmudict.zip` æ–‡ä»¶åˆ°/root/nltk_dataç›®å½•ä¸‹çš„å­ç›®å½•ï¼Œä¸‹è½½å®ŒæˆåæŸ¥çœ‹æ ¹ç›®å½•ä¸‹æ˜¯å¦æœ‰`nltk_data`æ–‡ä»¶å¤¹ï¼Œå¹¶å°†å…¶ä¸­çš„å‹ç¼©åŒ…éƒ½è§£å‹ã€‚\n\n&emsp;\n\n2)æ–¹æ³•äºŒ\n\nå¦‚æœé€šè¿‡ä¸Šé¢ä»£ç è¿˜æ˜¯æ— æ³•æ­£å¸¸ä¸‹è½½æ•°æ®åŒ… ï¼Œä¹Ÿå¯ä»¥æ‰“å¼€ä»¥ä¸‹åœ°å€æ‰‹åŠ¨æœç´¢å¹¶ä¸‹è½½å‹ç¼©åŒ…æ–‡ä»¶(éœ€ç§‘å­¦ä¸Šç½‘)\n\n```\nhttps://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n```\n\nå…¶ä¸­ä¸‹é¢æ˜¯`averaged_perceptron_tagger.zip` å’Œ`cmudict.zip` æ•°æ®åŒ…æ–‡ä»¶çš„ä¸‹è½½åœ°å€\n\n```\nhttps://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/taggers/averaged_perceptron_tagger.zip\nhttps://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/cmudict.zip\n```\n\nç„¶åå°†è¯¥å‹ç¼©åŒ…æ–‡ä»¶ä¸Šä¼ è‡³(1)è¿è¡Œ`python download.py`æ—¶æ‰“å°æ˜¾ç¤ºçš„æ–‡ä»¶ç´¢å¼•ä½ç½®ï¼Œå¦‚ `/root/nltk_data`  æˆ–è€… `/root/miniconda3/envs/EmotiVoice/nltk_data` ç­‰ç±»ä¼¼ç›®å½•ä¸‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»ºä¸€ä¸ªï¼Œç„¶åå°†zipå‹ç¼©åŒ…è§£å‹ã€‚\n\n&emsp;\n\nè§£å‹ånltk_dataç›®å½•ç»“æ„åº”è¯¥æ˜¯ä¸‹é¢è¿™æ ·\n\n```\nâ”œâ”€â”€ nltk_data\nâ”‚   â”œâ”€â”€ corpora\nâ”‚   â”‚   â”œâ”€â”€ cmudict\nâ”‚   â”‚   â”‚   â”œâ”€â”€ README\nâ”‚   â”‚   â”‚   â””â”€â”€ cmudict\nâ”‚   â”‚   â””â”€â”€ cmudict.zip\nâ”‚   â””â”€â”€ taggers\nâ”‚       â”œâ”€â”€ averaged_perceptron_tagger\nâ”‚       â”‚   â””â”€â”€ averaged_perceptron_tagger.pickle\nâ”‚       â””â”€â”€ averaged_perceptron_tagger.zip\n```\n\n&emsp;\n\n**(3) æŠ¥é”™ AttributeError: 'NoneType' object has no attribute 'seek'.** \n\nåŸå› ï¼šæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶\n\nè§£å†³æ–¹æ³•ï¼šå¤§æ¦‚ç‡æ˜¯ä½ æœªä¸‹è½½æ¨¡å‹æ–‡ä»¶æˆ–è€…å­˜æ”¾è·¯å¾„ä¸æ­£ç¡®ï¼ŒæŸ¥çœ‹è‡ªå·±ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå³outputsæ–‡ä»¶å¤¹å­˜æ”¾è·¯å¾„å’Œé‡Œé¢çš„æ¨¡å‹æ–‡ä»¶æ˜¯å¦æ­£ç¡®ï¼Œæ­£ç¡®ç»“æ„å¯å‚è€ƒ [ç¬¬äº”æ­¥](#step5) ä¸­çš„é¡¹ç›®ç»“æ„ã€‚\n\n&emsp;\n\n**(4) è¿è¡ŒAPIæœåŠ¡å‡ºé”™ ImportError: cannot import name 'Doc' from 'typing_extensions'**\n\nåŸå› ï¼štyping_extensions ç‰ˆæœ¬é—®é¢˜\n\nè§£å†³æ–¹æ³•ï¼š\n\nå°è¯•å°†`typing_extensions`å‡çº§è‡³æœ€æ–°ç‰ˆæœ¬ï¼Œå¦‚æœå·²ç»æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œåˆ™é€‚å½“é™ä½ç‰ˆæœ¬ï¼Œä»¥ä¸‹ç‰ˆæœ¬åœ¨`fastapi V0.104.1`æµ‹è¯•æ­£å¸¸ã€‚\n\n```\npip install typing_extensions==4.8.0 --force-reinstall\n```\n\n&emsp;\n\n**(5) è¯·æ±‚æ–‡æœ¬è½¬è¯­éŸ³æ¥å£æ—¶æŠ¥é”™ 500 Internal Server Error ï¼ŒFileNotFoundError: [Errno 2] No such file or directory: 'ffmpeg'**\n\nåŸå› ï¼šæœªå®‰è£…ffmpeg\n\nè§£å†³æ–¹æ³•ï¼š\n\næ‰§è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®‰è£…ï¼Œå¦‚æœæ˜¯Ubuntuæ‰§è¡Œ\n\n```\nsudo apt update\nsudo apt install ffmpeg\n```\n\nCentOSåˆ™æ‰§è¡Œ\n\n```\nsudo yum install epel-release\nsudo yum install ffmpeg\n```\n\nå®‰è£…å®Œæˆåï¼Œä½ å¯ä»¥åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥éªŒè¯\"ffmpeg\"æ˜¯å¦æˆåŠŸå®‰è£…ï¼š\n\n```\nffmpeg -version\n```\n\nå¦‚æœå®‰è£…æˆåŠŸï¼Œä½ å°†çœ‹åˆ°\"ffmpeg\"çš„ç‰ˆæœ¬ä¿¡æ¯ã€‚\n\n&emsp;\n"
        },
        {
          "name": "ROADMAP.md",
          "type": "blob",
          "size": 1.755859375,
          "content": "# EmotiVoice Roadmap\n\nThis roadmap is for EmotiVoice (æ˜“é­”å£°), a project driven by the community. We value your feedback and suggestions on our future direction.\n\nPlease visit https://github.com/netease-youdao/EmotiVoice/issues on GitHub to submit your proposals.\nIf you are interested, feel free to volunteer for any tasks, even if they are not listed.\n\nThe plan is to finish 0.2 to 0.4 in Q4 2023.\n\n## EmotiVoice 0.4\n\n- [ ] Updated model with potentially improved quality.\n- [ ] First version of desktop application.\n- [ ] Support longer text.\n\n## EmotiVoice 0.3 (2023.12.13)\n\n- [x] Release [The EmotiVoice HTTP API](https://github.com/netease-youdao/EmotiVoice/wiki/HTTP-API) provided by [Zhiyun](https://mp.weixin.qq.com/s/_Fbj4TI4ifC6N7NFOUrqKQ).\n- [x] Release [Voice Cloning with your personal data](https://github.com/netease-youdao/EmotiVoice/wiki/Voice-Cloning-with-your-personal-data) along with [DataBaker Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/DataBaker) and [LJSpeech Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/LJspeech).\n- [x] Documentation: wiki page for hardware requirements. [#30](../../issues/30)\n\n## EmotiVoice 0.2 (2023.11.17)\n\n- [x] Support mixed Chinese and English input text. [#28](../../issues/28)\n- [x] Resolve bugs related to certain modal particles, to make it more robust. [#18](../../issues/18)\n- [x] Documentation: voice list wiki page\n- [x] Documentation: this roadmap.\n\n## EmotiVoice 0.1 (2023.11.10) first public version\n\n- [x] We offer a pretrained model with over 2000 voices, supporting both Chinese and English languages.\n- [x] You can perform inference using the command line interface. We also offer a user-friendly web demo for easy usage.\n- [x] For convenient deployment, we offer a Docker image.\n\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cn2an",
          "type": "tree",
          "content": null
        },
        {
          "name": "cog.yaml",
          "type": "blob",
          "size": 0.75390625,
          "content": "# Configuration for Cog âš™ï¸\n# Reference: https://github.com/replicate/cog/blob/main/docs/yaml.md\n\nbuild:\n  gpu: true\n\n  # a list of ubuntu apt packages to install\n  # system_packages:\n  #   - \"libgl1-mesa-glx\"\n  #   - \"libglib2.0-0\"\n\n  python_version: \"3.8\"\n  python_packages:\n    - \"torch==2.0.1\"\n    - \"torchaudio==2.0.2\"\n    - \"g2p-en==2.1.0\"\n    - \"jieba==0.42.1\"\n    - \"numba==0.58.1\"\n    - \"numpy==1.24.4\"\n    - \"pypinyin==0.49.0\"\n    - \"scipy==1.10.1\"\n    - \"soundfile==0.12.1\"\n    - \"transformers==4.26.1\"\n    - \"yacs==0.1.8\"\n\n  run:\n    - curl -o /usr/local/bin/pget -L \"https://github.com/replicate/pget/releases/download/v0.0.3/pget\" && chmod +x /usr/local/bin/pget\n\n# predict.py defines how predictions are run on your model\npredict: \"predict.py:Predictor\"\n"
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo_page.py",
          "type": "blob",
          "size": 6.4365234375,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport streamlit as st\nimport os, glob\nimport numpy as np\nfrom yacs import config as CONFIG\nimport torch\nimport re\n\nfrom frontend import g2p_cn_en, ROOT_DIR, read_lexicon, G2p\nfrom config.joint.config import Config\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\n\nimport base64\nfrom pathlib import Path\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nMAX_WAV_VALUE = 32768.0\n\nconfig = Config()\n\ndef create_download_link():\n    pdf_path = Path(\"EmotiVoice_UserAgreement_æ˜“é­”å£°ç”¨æˆ·åè®®.pdf\")\n    base64_pdf = base64.b64encode(pdf_path.read_bytes()).decode(\"utf-8\")  # val looks like b'...'\n    return f'<a href=\"data:application/octet-stream;base64,{base64_pdf}\" download=\"EmotiVoice_UserAgreement_æ˜“é­”å£°ç”¨æˆ·åè®®.pdf.pdf\">EmotiVoice_UserAgreement_æ˜“é­”å£°ç”¨æˆ·åè®®.pdf</a>'\n\nhtml=create_download_link()\n\nst.set_page_config(\n    page_title=\"demo page\",\n    page_icon=\"ğŸ“•\",\n)\nst.write(\"# Text-To-Speech\")\nst.markdown(f\"\"\"\n### How to use:\n         \n- Simply select a **Speaker ID**, type in the **text** you want to convert and the emotion **Prompt**, like a single word or even a sentence. Then click on the **Synthesize** button below to start voice synthesis.\n\n- You can download the audio by clicking on the vertical three points next to the displayed audio widget.\n\n- For more information on **'Speaker ID'**, please consult the [EmotiVoice voice wiki page](https://github.com/netease-youdao/EmotiVoice/tree/main/data/youdao/text)\n\n- This interactive demo page is provided under the {html} file. The audio is synthesized by AI. éŸ³é¢‘ç”±AIåˆæˆï¼Œä»…ä¾›å‚è€ƒã€‚\n\n\"\"\", unsafe_allow_html=True)\n\ndef scan_checkpoint(cp_dir, prefix, c=8):\n    pattern = os.path.join(cp_dir, prefix + '?'*c)\n    cp_list = glob.glob(pattern)\n    if len(cp_list) == 0:\n        return None\n    return sorted(cp_list)[-1]\n\n@st.cache_resource\ndef get_models():\n    \n    am_checkpoint_path = scan_checkpoint(f'{config.output_directory}/prompt_tts_open_source_joint/ckpt', 'g_')\n\n    style_encoder_checkpoint_path = scan_checkpoint(f'{config.output_directory}/style_encoder/ckpt', 'checkpoint_', 6)#f'{config.output_directory}/style_encoder/ckpt/checkpoint_163431' \n\n    with open(config.model_config_path, 'r') as fin:\n        conf = CONFIG.load_cfg(fin)\n    \n    conf.n_vocab = config.n_symbols\n    conf.n_speaker = config.speaker_n_labels\n\n    style_encoder = StyleEncoder(config)\n    model_CKPT = torch.load(style_encoder_checkpoint_path, map_location=\"cpu\")\n    model_ckpt = {}\n    for key, value in model_CKPT['model'].items():\n        new_key = key[7:]\n        model_ckpt[new_key] = value\n    style_encoder.load_state_dict(model_ckpt, strict=False)\n    generator = JETSGenerator(conf).to(DEVICE)\n\n    model_CKPT = torch.load(am_checkpoint_path, map_location=DEVICE)\n    generator.load_state_dict(model_CKPT['generator'])\n    generator.eval()\n\n    tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n\n    with open(config.token_list_path, 'r') as f:\n        token2id = {t.strip():idx for idx, t, in enumerate(f.readlines())}\n\n    with open(config.speaker2id_path, encoding='utf-8') as f:\n        speaker2id = {t.strip():idx for idx, t in enumerate(f.readlines())}\n\n\n    return (style_encoder, generator, tokenizer, token2id, speaker2id)\n\ndef get_style_embedding(prompt, tokenizer, style_encoder):\n    prompt = tokenizer([prompt], return_tensors=\"pt\")\n    input_ids = prompt[\"input_ids\"]\n    token_type_ids = prompt[\"token_type_ids\"]\n    attention_mask = prompt[\"attention_mask\"]\n    with torch.no_grad():\n        output = style_encoder(\n        input_ids=input_ids,\n        token_type_ids=token_type_ids,\n        attention_mask=attention_mask,\n    )\n    style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n    return style_embedding\n\ndef tts(name, text, prompt, content, speaker, models):\n    (style_encoder, generator, tokenizer, token2id, speaker2id)=models\n    \n\n    style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n    content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n\n    speaker = speaker2id[speaker]\n\n    text_int = [token2id[ph] for ph in text.split()]\n    \n    sequence = torch.from_numpy(np.array(text_int)).to(DEVICE).long().unsqueeze(0)\n    sequence_len = torch.from_numpy(np.array([len(text_int)])).to(DEVICE)\n    style_embedding = torch.from_numpy(style_embedding).to(DEVICE).unsqueeze(0)\n    content_embedding = torch.from_numpy(content_embedding).to(DEVICE).unsqueeze(0)\n    speaker = torch.from_numpy(np.array([speaker])).to(DEVICE)\n\n    with torch.no_grad():\n\n        infer_output = generator(\n                inputs_ling=sequence,\n                inputs_style_embedding=style_embedding,\n                input_lengths=sequence_len,\n                inputs_content_embedding=content_embedding,\n                inputs_speaker=speaker,\n                alpha=1.0\n            )\n\n    audio = infer_output[\"wav_predictions\"].squeeze()* MAX_WAV_VALUE\n    audio = audio.cpu().numpy().astype('int16')\n\n    return audio\n\nspeakers = config.speakers\nmodels = get_models()\nlexicon = read_lexicon(f\"{ROOT_DIR}/lexicon/librispeech-lexicon.txt\")\ng2p = G2p()\n\ndef new_line(i):\n    col1, col2, col3, col4 = st.columns([1.5, 1.5, 3.5, 1.3])\n    with col1:\n        speaker=st.selectbox(\"Speaker ID (è¯´è¯äºº)\", speakers, key=f\"{i}_speaker\")\n    with col2:\n        prompt=st.text_input(\"Prompt (å¼€å¿ƒ/æ‚²ä¼¤)\", \"\", key=f\"{i}_prompt\")\n    with col3:\n        content=st.text_input(\"Text to be synthesized into speech (åˆæˆæ–‡æœ¬)\", \"åˆæˆæ–‡æœ¬\", key=f\"{i}_text\")\n    with col4:\n        lang=st.selectbox(\"Language (è¯­è¨€)\", [\"zh_us\"], key=f\"{i}_lang\")\n\n    flag = st.button(f\"Synthesize (åˆæˆ)\", key=f\"{i}_button1\")\n    if flag:\n        text =  g2p_cn_en(content, g2p, lexicon)\n        path = tts(i, text, prompt, content, speaker, models)\n        st.audio(path, sample_rate=config.sampling_rate)\n\n\n\nnew_line(0)\n"
        },
        {
          "name": "demo_page_databaker.py",
          "type": "blob",
          "size": 6.294921875,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport streamlit as st\nimport os, glob\nimport numpy as np\nfrom yacs import config as CONFIG\nimport torch\nimport re\n\nfrom frontend import g2p_cn_en, ROOT_DIR, read_lexicon, G2p\nfrom exp.DataBaker.config.config import Config\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\n\nimport base64\nfrom pathlib import Path\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nMAX_WAV_VALUE = 32768.0\n\nconfig = Config()\n\ndef create_download_link():\n    pdf_path = Path(\"EmotiVoice_UserAgreement_æ˜“é­”å£°ç”¨æˆ·åè®®.pdf\")\n    base64_pdf = base64.b64encode(pdf_path.read_bytes()).decode(\"utf-8\")  # val looks like b'...'\n    return f'<a href=\"data:application/octet-stream;base64,{base64_pdf}\" download=\"EmotiVoice_UserAgreement_æ˜“é­”å£°ç”¨æˆ·åè®®.pdf.pdf\">EmotiVoice_UserAgreement_æ˜“é­”å£°ç”¨æˆ·åè®®.pdf</a>'\n\nhtml=create_download_link()\n\nst.set_page_config(\n    page_title=\"demo page\",\n    page_icon=\"ğŸ“•\",\n)\nst.write(\"# Text-To-Speech\")\nst.markdown(f\"\"\"\n### How to use:\n         \n- Simply select a **Speaker ID**, type in the **text** you want to convert and the emotion **Prompt**, like a single word or even a sentence. Then click on the **Synthesize** button below to start voice synthesis.\n\n- You can download the audio by clicking on the vertical three points next to the displayed audio widget.\n\n- For more information on **'Speaker ID'**, please consult the [EmotiVoice voice wiki page](https://github.com/netease-youdao/EmotiVoice/tree/main/data/youdao/text)\n\n- This interactive demo page is provided under the {html} file. The audio is synthesized by AI. éŸ³é¢‘ç”±AIåˆæˆï¼Œä»…ä¾›å‚è€ƒã€‚\n\n\"\"\", unsafe_allow_html=True)\n\ndef scan_checkpoint(cp_dir, prefix, c=8):\n    pattern = os.path.join(cp_dir, prefix + '?'*c)\n    cp_list = glob.glob(pattern)\n    if len(cp_list) == 0:\n        return None\n    return sorted(cp_list)[-1]\n\n@st.cache_resource\ndef get_models():\n    \n    am_checkpoint_path = scan_checkpoint(f'{config.output_directory}/ckpt', 'g_')\n\n    style_encoder_checkpoint_path = config.style_encoder_ckpt\n\n    with open(config.model_config_path, 'r') as fin:\n        conf = CONFIG.load_cfg(fin)\n    \n    conf.n_vocab = config.n_symbols\n    conf.n_speaker = config.speaker_n_labels\n\n    style_encoder = StyleEncoder(config)\n    model_CKPT = torch.load(style_encoder_checkpoint_path, map_location=\"cpu\")\n    model_ckpt = {}\n    for key, value in model_CKPT['model'].items():\n        new_key = key[7:]\n        model_ckpt[new_key] = value\n    style_encoder.load_state_dict(model_ckpt, strict=False)\n    generator = JETSGenerator(conf).to(DEVICE)\n\n    model_CKPT = torch.load(am_checkpoint_path, map_location=DEVICE)\n    generator.load_state_dict(model_CKPT['generator'])\n    generator.eval()\n\n    tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n\n    with open(config.token_list_path, 'r') as f:\n        token2id = {t.strip():idx for idx, t, in enumerate(f.readlines())}\n\n    with open(config.speaker2id_path, encoding='utf-8') as f:\n        speaker2id = {t.strip():idx for idx, t in enumerate(f.readlines())}\n\n\n    return (style_encoder, generator, tokenizer, token2id, speaker2id)\n\ndef get_style_embedding(prompt, tokenizer, style_encoder):\n    prompt = tokenizer([prompt], return_tensors=\"pt\")\n    input_ids = prompt[\"input_ids\"]\n    token_type_ids = prompt[\"token_type_ids\"]\n    attention_mask = prompt[\"attention_mask\"]\n    with torch.no_grad():\n        output = style_encoder(\n        input_ids=input_ids,\n        token_type_ids=token_type_ids,\n        attention_mask=attention_mask,\n    )\n    style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n    return style_embedding\n\ndef tts(name, text, prompt, content, speaker, models):\n    (style_encoder, generator, tokenizer, token2id, speaker2id)=models\n    \n\n    style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n    content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n\n    speaker = speaker2id[speaker]\n\n    text_int = [token2id[ph] for ph in text.split()]\n    \n    sequence = torch.from_numpy(np.array(text_int)).to(DEVICE).long().unsqueeze(0)\n    sequence_len = torch.from_numpy(np.array([len(text_int)])).to(DEVICE)\n    style_embedding = torch.from_numpy(style_embedding).to(DEVICE).unsqueeze(0)\n    content_embedding = torch.from_numpy(content_embedding).to(DEVICE).unsqueeze(0)\n    speaker = torch.from_numpy(np.array([speaker])).to(DEVICE)\n\n    with torch.no_grad():\n\n        infer_output = generator(\n                inputs_ling=sequence,\n                inputs_style_embedding=style_embedding,\n                input_lengths=sequence_len,\n                inputs_content_embedding=content_embedding,\n                inputs_speaker=speaker,\n                alpha=1.0\n            )\n\n    audio = infer_output[\"wav_predictions\"].squeeze()* MAX_WAV_VALUE\n    audio = audio.cpu().numpy().astype('int16')\n\n    return audio\n\nspeakers = config.speakers\nmodels = get_models()\nlexicon = read_lexicon(f\"{ROOT_DIR}/lexicon/librispeech-lexicon.txt\")\ng2p = G2p()\n\ndef new_line(i):\n    col1, col2, col3, col4 = st.columns([1.5, 1.5, 3.5, 1.3])\n    with col1:\n        speaker=st.selectbox(\"Speaker ID (è¯´è¯äºº)\", speakers, key=f\"{i}_speaker\")\n    with col2:\n        prompt=st.text_input(\"Prompt (å¼€å¿ƒ/æ‚²ä¼¤)\", \"\", key=f\"{i}_prompt\")\n    with col3:\n        content=st.text_input(\"Text to be synthesized into speech (åˆæˆæ–‡æœ¬)\", \"åˆæˆæ–‡æœ¬\", key=f\"{i}_text\")\n    with col4:\n        lang=st.selectbox(\"Language (è¯­è¨€)\", [\"zh_us\"], key=f\"{i}_lang\")\n\n    flag = st.button(f\"Synthesize (åˆæˆ)\", key=f\"{i}_button1\")\n    if flag:\n        text =  g2p_cn_en(content, g2p, lexicon)\n        path = tts(i, text, prompt, content, speaker, models)\n        st.audio(path, sample_rate=config.sampling_rate)\n\n\n\nnew_line(0)\n"
        },
        {
          "name": "frontend.py",
          "type": "blob",
          "size": 3.0078125,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nfrom frontend_cn import g2p_cn, re_digits, tn_chinese\nfrom frontend_en import ROOT_DIR, read_lexicon, G2p, get_eng_phoneme\n\n# Thanks to GuGCoCo and PatroxGaurab for identifying the issue: \n# the results differ between frontend.py and frontend_en.py. Here's a quick fix.\n#re_english_word = re.compile('([a-z\\-\\.\\'\\s,;\\:\\!\\?]+|\\d+[\\d\\.]*)', re.I)\nre_english_word = re.compile('([^\\u4e00-\\u9fa5]+|[ \\u3002\\uff0c\\uff1f\\uff01\\uff1b\\uff1a\\u201c\\u201d\\u2018\\u2019\\u300a\\u300b\\u3008\\u3009\\u3010\\u3011\\u300e\\u300f\\u2014\\u2026\\u3001\\uff08\\uff09\\u4e00-\\u9fa5]+)', re.I)\ndef g2p_cn_en(text, g2p, lexicon):\n    # Our policy dictates that if the text contains Chinese, digits are to be converted into Chinese.\n    text=tn_chinese(text)\n    parts = re_english_word.split(text)\n    parts=list(filter(None, parts))\n    tts_text = [\"<sos/eos>\"]\n    chartype = ''\n    text_contains_chinese = contains_chinese(text)\n    for part in parts:\n        if part == ' ' or part == '': continue\n        if re_digits.match(part) and (text_contains_chinese or chartype == '') or contains_chinese(part):\n            if chartype == 'en':\n                tts_text.append('eng_cn_sp')\n            phoneme = g2p_cn(part).split()[1:-1]\n            chartype = 'cn'\n        elif re_english_word.match(part):\n            if chartype == 'cn':\n                if \"sp\" in tts_text[-1]:\n                    \"\"\n                else:\n                    tts_text.append('cn_eng_sp')\n            phoneme = get_eng_phoneme(part, g2p, lexicon, False).split()\n            if not phoneme :\n                # tts_text.pop()\n                continue\n            else:\n                chartype = 'en'\n        else:\n            continue\n        tts_text.extend( phoneme )\n\n    tts_text=\" \".join(tts_text).split()\n    if \"sp\" in tts_text[-1]:\n        tts_text.pop()\n    tts_text.append(\"<sos/eos>\")\n\n    return \" \".join(tts_text)\n\ndef contains_chinese(text):\n    pattern = re.compile(r'[\\u4e00-\\u9fa5]')\n    match = re.search(pattern, text)\n    return match is not None\n\n\nif __name__ == \"__main__\":\n    import sys\n    from os.path import isfile\n    lexicon = read_lexicon(f\"{ROOT_DIR}/lexicon/librispeech-lexicon.txt\")\n\n    g2p = G2p()\n    if len(sys.argv) < 2:\n        print(\"Usage: python %s <text>\" % sys.argv[0])\n        exit()\n    text_file = sys.argv[1]\n    if isfile(text_file):\n        fp = open(text_file, 'r')\n        for line in fp:\n            phoneme = g2p_cn_en(line.rstrip(), g2p, lexicon)\n            print(phoneme)\n        fp.close()\n"
        },
        {
          "name": "frontend_cn.py",
          "type": "blob",
          "size": 4.1767578125,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nfrom pypinyin import pinyin, lazy_pinyin, Style\nimport jieba\nimport string\nfrom cn2an.an2cn import An2Cn\nfrom pypinyin_dict.phrase_pinyin_data import cc_cedict\ncc_cedict.load()\nre_special_pinyin = re.compile(r'^(n|ng|m)$')\ndef split_py(py):\n    tone = py[-1]\n    py = py[:-1]\n    sm = \"\"\n    ym = \"\"\n    suf_r = \"\"\n    if re_special_pinyin.match(py):\n        py = 'e' + py\n    if py[-1] == 'r':\n        suf_r = 'r'\n        py = py[:-1]\n    if py == 'zi' or py == 'ci' or py == 'si' or py == 'ri':\n        sm = py[:1]\n        ym = \"ii\"\n    elif py == 'zhi' or py == 'chi' or py == 'shi':\n        sm = py[:2]\n        ym = \"iii\"\n    elif py == 'ya' or py == 'yan' or py == 'yang' or py == 'yao' or py == 'ye' or py == 'yong' or py == 'you':\n        sm = \"\"\n        ym = 'i' + py[1:]\n    elif py == 'yi' or py == 'yin' or py == 'ying':\n        sm = \"\"\n        ym = py[1:]\n    elif py == 'yu' or py == 'yv' or py == 'yuan' or py == 'yvan' or py == 'yue ' or py == 'yve' or py == 'yun' or py == 'yvn':\n        sm = \"\"\n        ym = 'v' + py[2:]\n    elif py == 'wu':\n        sm = \"\"\n        ym = \"u\"\n    elif py[0] == 'w':\n        sm = \"\"\n        ym = \"u\" + py[1:]\n    elif len(py) >= 2 and (py[0] == 'j' or py[0] == 'q' or py[0] == 'x') and py[1] == 'u':\n        sm = py[0]\n        ym = 'v' + py[2:]\n    else:\n        seg_pos = re.search('a|e|i|o|u|v', py)\n        sm = py[:seg_pos.start()]\n        ym = py[seg_pos.start():]\n        if ym == 'ui':\n            ym = 'uei'\n        elif ym == 'iu':\n            ym = 'iou'\n        elif ym == 'un':\n            ym = 'uen'\n        elif ym == 'ue':\n            ym = 've'\n    ym += suf_r + tone\n    return sm, ym\n\n\nchinese_punctuation_pattern = r'[\\u3002\\uff0c\\uff1f\\uff01\\uff1b\\uff1a\\u201c\\u201d\\u2018\\u2019\\u300a\\u300b\\u3008\\u3009\\u3010\\u3011\\u300e\\u300f\\u2014\\u2026\\u3001\\uff08\\uff09]'\n\n\ndef has_chinese_punctuation(text):\n    match = re.search(chinese_punctuation_pattern, text)\n    return match is not None\ndef has_english_punctuation(text):\n    return text in string.punctuation\n\n# with thanks to KimigaiiWuyi in https://github.com/netease-youdao/EmotiVoice/pull/17.\n# Updated on November 20, 2023: EmotiVoice now incorporates cn2an (https://github.com/Ailln/cn2an) for number processing.\nre_digits = re.compile('(\\d[\\d\\.]*)')\ndef number_to_chinese(number):\n    an2cn = An2Cn()\n    result = an2cn.an2cn(number)\n\n    return result\n\ndef tn_chinese(text):\n    parts = re_digits.split(text)\n    words = []\n    for part in parts:\n        if re_digits.match(part):\n            words.append(number_to_chinese(part))\n        else:\n            words.append(part)\n    return ''.join(words)\n\ndef g2p_cn(text):\n    res_text=[\"<sos/eos>\"]\n    seg_list = jieba.cut(text)\n    for seg in seg_list:\n        if seg == \" \": continue\n        seg_tn = tn_chinese(seg)\n        py =[_py[0] for _py in pinyin(seg_tn, style=Style.TONE3,neutral_tone_with_five=True)]\n\n        if any([has_chinese_punctuation(_py) for _py in py])  or any([has_english_punctuation(_py) for _py in py]):\n            res_text.pop()\n            res_text.append(\"sp3\")\n        else:\n            \n            py = [\" \".join(split_py(_py)) for _py in py]\n            \n            res_text.append(\" sp0 \".join(py))\n            res_text.append(\"sp1\")\n    #res_text.pop()\n    res_text.append(\"<sos/eos>\")\n    return \" \".join(res_text)\n\nif __name__ == \"__main__\":\n    import sys\n    from os.path import isfile\n    if len(sys.argv) < 2:\n        print(\"Usage: python %s <text>\" % sys.argv[0])\n        exit()\n    text_file = sys.argv[1]\n    if isfile(text_file):\n        fp = open(text_file, 'r')\n        for line in fp:\n            phoneme=g2p_cn(line.rstrip())\n            print(phoneme)\n        fp.close()\n"
        },
        {
          "name": "frontend_en.py",
          "type": "blob",
          "size": 2.796875,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nimport argparse\nfrom string import punctuation\nimport numpy as np\n\nfrom g2p_en import G2p\n\nimport os\n\n\nROOT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n\ndef read_lexicon(lex_path):\n    lexicon = {}\n    with open(lex_path) as f:\n        for line in f:\n            temp = re.split(r\"\\s+\", line.strip(\"\\n\"))\n            word = temp[0]\n            phones = temp[1:]\n            if word.lower() not in lexicon:\n                lexicon[word.lower()] = phones\n    return lexicon\n\ndef get_eng_phoneme(text, g2p, lexicon, pad_sos_eos=True):\n    \"\"\"\n    english g2p\n    \"\"\"\n    filters = {\",\", \" \", \"'\"}\n    phones = []\n    words = list(filter(lambda x: x not in {\"\", \" \"}, re.split(r\"([,;.\\-\\?\\!\\s+])\", text)))\n\n    for w in words:\n        if w.lower() in lexicon:\n            \n            for ph in lexicon[w.lower()]:\n                if ph not in filters:\n                    phones += [\"[\" + ph + \"]\"]\n\n            if \"sp\" not in phones[-1]:\n                phones += [\"engsp1\"]\n        else:\n            phone=g2p(w)\n            if not phone:\n                continue\n\n            if phone[0].isalnum():\n                \n                for ph in phone:\n                    if ph not in filters:\n                        phones += [\"[\" + ph + \"]\"]\n                    if ph == \" \" and \"sp\" not in phones[-1]:\n                        phones += [\"engsp1\"]\n            elif phone == \" \":\n                continue\n            elif phones:\n                phones.pop() # pop engsp1\n                phones.append(\"engsp4\")\n    if phones and \"engsp\" in phones[-1]:\n        phones.pop()\n\n    # mark = \".\" if text[-1] != \"?\" else \"?\"\n    if pad_sos_eos:\n        phones = [\"<sos/eos>\"] + phones + [\"<sos/eos>\"]\n    return \" \".join(phones)\n    \n\nif __name__ == \"__main__\":\n    lexicon = read_lexicon(f\"{ROOT_DIR}/lexicon/librispeech-lexicon.txt\")\n    g2p = G2p()\n    phonemes= get_eng_phoneme(\"Happy New Year\", g2p, lexicon)\n    import sys\n    from os.path import isfile\n    if len(sys.argv) < 2:\n        print(\"Usage: python %s <text>\" % sys.argv[0])\n        exit()\n    text_file = sys.argv[1]\n    if isfile(text_file):\n        fp = open(text_file, 'r')\n        for line in fp:\n            phoneme=get_eng_phoneme(line.rstrip(), g2p, lexicon)\n            print(phoneme)\n        fp.close()"
        },
        {
          "name": "inference_am_vocoder_exp.py",
          "type": "blob",
          "size": 5.943359375,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\nimport os, sys, warnings, torch, glob, argparse\nimport numpy as np\nfrom models.hifigan.get_vocoder import MAX_WAV_VALUE\nimport soundfile as sf\nfrom yacs import config as CONFIG\nfrom tqdm import tqdm\n\ndef get_style_embedding(prompt, tokenizer, style_encoder):\n    prompt = tokenizer([prompt], return_tensors=\"pt\")\n    input_ids = prompt[\"input_ids\"]\n    token_type_ids = prompt[\"token_type_ids\"]\n    attention_mask = prompt[\"attention_mask\"]\n\n    with torch.no_grad():\n        output = style_encoder(\n        input_ids=input_ids,\n        token_type_ids=token_type_ids,\n        attention_mask=attention_mask,\n    )\n    style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n    return style_embedding\n\ndef main(args, config):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    root_path = os.path.join(config.output_directory)\n    ckpt_path = os.path.join(root_path,  \"ckpt\")\n    files = os.listdir(ckpt_path)\n    \n    for file in files:\n        if args.checkpoint:\n            if file != args.checkpoint:\n                continue\n\n        checkpoint_path = os.path.join(ckpt_path, file)\n\n        with open(config.model_config_path, 'r') as fin:\n            conf = CONFIG.load_cfg(fin)\n        \n     \n        conf.n_vocab = config.n_symbols\n        conf.n_speaker = config.speaker_n_labels\n\n        style_encoder = StyleEncoder(config)\n        model_CKPT = torch.load(config.style_encoder_ckpt, map_location=\"cpu\")\n        model_ckpt = {}\n        for key, value in model_CKPT['model'].items():\n            new_key = key[7:]\n            model_ckpt[new_key] = value\n        style_encoder.load_state_dict(model_ckpt, strict=False)\n\n\n\n        generator = JETSGenerator(conf).to(device)\n\n        model_CKPT = torch.load(checkpoint_path, map_location=device)\n        generator.load_state_dict(model_CKPT['generator'])\n        generator.eval()\n\n        with open(config.token_list_path, 'r') as f:\n            token2id = {t.strip():idx for idx, t, in enumerate(f.readlines())}\n\n        with open(config.speaker2id_path, encoding='utf-8') as f:\n            speaker2id = {t.strip():idx for idx, t in enumerate(f.readlines())}\n\n\n        tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n        \n        text_path = args.test_file\n\n   \n        if os.path.exists(root_path + \"/test_audio/audio/\" +f\"{file}/\"):\n            r = glob.glob(root_path + \"/test_audio/audio/\" +f\"{file}/*\")\n            for j in r:\n                os.remove(j)\n        texts = []\n        prompts = []\n        speakers = []\n        contents = []\n        with open(text_path, \"r\") as f:\n            for line in f:\n                line = line.strip().split(\"|\")\n                speakers.append(line[0])\n                prompts.append(line[1])\n                texts.append(line[2].split())\n                contents.append(line[3])\n                \n        for i, (speaker, prompt, text, content) in enumerate(tqdm(zip(speakers, prompts, texts, contents))):\n\n            style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n            content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n\n            if speaker not in speaker2id:\n                continue\n            speaker = speaker2id[speaker]\n\n            text_int = [token2id[ph] for ph in text]\n            \n            sequence = torch.from_numpy(np.array(text_int)).to(device).long().unsqueeze(0)\n            sequence_len = torch.from_numpy(np.array([len(text_int)])).to(device)\n            style_embedding = torch.from_numpy(style_embedding).to(device).unsqueeze(0)\n            content_embedding = torch.from_numpy(content_embedding).to(device).unsqueeze(0)\n            speaker = torch.from_numpy(np.array([speaker])).to(device)\n            with torch.no_grad():\n\n                infer_output = generator(\n                        inputs_ling=sequence,\n                        inputs_style_embedding=style_embedding,\n                        input_lengths=sequence_len,\n                        inputs_content_embedding=content_embedding,\n                        inputs_speaker=speaker,\n                        alpha=1.0\n                    )\n                audio = infer_output[\"wav_predictions\"].squeeze()* MAX_WAV_VALUE\n                audio = audio.cpu().numpy().astype('int16')\n                if not os.path.exists(root_path + \"/test_audio/audio/\" +f\"{file}/\"):\n                    os.makedirs(root_path + \"/test_audio/audio/\" +f\"{file}/\", exist_ok=True)\n                sf.write(file=root_path + \"/test_audio/audio/\" +f\"{file}/{i+1}.wav\", data=audio, samplerate=config.sampling_rate) #h.sampling_rate\n\n\n\n\n\n\nif __name__ == '__main__':\n    print(\"run!\")\n    p = argparse.ArgumentParser()\n    p.add_argument(\"-c\", \"--config_folder\", type=str, required=True)\n    p.add_argument(\"--checkpoint\", type=str, required=False, default='', help='inference specific checkpoint, e.g --checkpoint checkpoint_230000')\n    p.add_argument('-t', '--test_file', type=str, required=True, help='the absolute path of test file that is going to inference')\n\n    args = p.parse_args() \n    ##################################################\n    sys.path.append(os.path.dirname(os.path.abspath(\"__file__\")) + \"/\" + args.config_folder)\n\n    from config import Config\n    config = Config()\n    ##################################################\n    main(args, config)\n\n\n"
        },
        {
          "name": "inference_am_vocoder_joint.py",
          "type": "blob",
          "size": 6.0166015625,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\nimport os, sys, warnings, torch, glob, argparse\nimport numpy as np\nfrom models.hifigan.get_vocoder import MAX_WAV_VALUE\nimport soundfile as sf\nfrom yacs import config as CONFIG\nfrom tqdm import tqdm\n\ndef get_style_embedding(prompt, tokenizer, style_encoder):\n    prompt = tokenizer([prompt], return_tensors=\"pt\")\n    input_ids = prompt[\"input_ids\"]\n    token_type_ids = prompt[\"token_type_ids\"]\n    attention_mask = prompt[\"attention_mask\"]\n\n    with torch.no_grad():\n        output = style_encoder(\n        input_ids=input_ids,\n        token_type_ids=token_type_ids,\n        attention_mask=attention_mask,\n    )\n    style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n    return style_embedding\n\ndef main(args, config):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    root_path = os.path.join(config.output_directory, args.logdir)\n    ckpt_path = os.path.join(root_path,  \"ckpt\")\n    files = os.listdir(ckpt_path)\n    \n    for file in files:\n        if args.checkpoint:\n            if file != args.checkpoint:\n                continue\n\n        checkpoint_path = os.path.join(ckpt_path, file)\n\n        with open(config.model_config_path, 'r') as fin:\n            conf = CONFIG.load_cfg(fin)\n        \n     \n        conf.n_vocab = config.n_symbols\n        conf.n_speaker = config.speaker_n_labels\n\n        style_encoder = StyleEncoder(config)\n        model_CKPT = torch.load(config.style_encoder_ckpt, map_location=\"cpu\")\n        model_ckpt = {}\n        for key, value in model_CKPT['model'].items():\n            new_key = key[7:]\n            model_ckpt[new_key] = value\n        style_encoder.load_state_dict(model_ckpt, strict=False)\n\n\n\n        generator = JETSGenerator(conf).to(device)\n\n        model_CKPT = torch.load(checkpoint_path, map_location=device)\n        generator.load_state_dict(model_CKPT['generator'])\n        generator.eval()\n\n        with open(config.token_list_path, 'r') as f:\n            token2id = {t.strip():idx for idx, t, in enumerate(f.readlines())}\n\n        with open(config.speaker2id_path, encoding='utf-8') as f:\n            speaker2id = {t.strip():idx for idx, t in enumerate(f.readlines())}\n\n\n        tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n        \n        text_path = args.test_file\n\n   \n        if os.path.exists(root_path + \"/test_audio/audio/\" +f\"{file}/\"):\n            r = glob.glob(root_path + \"/test_audio/audio/\" +f\"{file}/*\")\n            for j in r:\n                os.remove(j)\n        texts = []\n        prompts = []\n        speakers = []\n        contents = []\n        with open(text_path, \"r\") as f:\n            for line in f:\n                line = line.strip().split(\"|\")\n                speakers.append(line[0])\n                prompts.append(line[1])\n                texts.append(line[2].split())\n                contents.append(line[3])\n                \n        for i, (speaker, prompt, text, content) in enumerate(tqdm(zip(speakers, prompts, texts, contents))):\n\n            style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n            content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n\n            if speaker not in speaker2id:\n                continue\n            speaker = speaker2id[speaker]\n\n            text_int = [token2id[ph] for ph in text]\n            \n            sequence = torch.from_numpy(np.array(text_int)).to(device).long().unsqueeze(0)\n            sequence_len = torch.from_numpy(np.array([len(text_int)])).to(device)\n            style_embedding = torch.from_numpy(style_embedding).to(device).unsqueeze(0)\n            content_embedding = torch.from_numpy(content_embedding).to(device).unsqueeze(0)\n            speaker = torch.from_numpy(np.array([speaker])).to(device)\n            with torch.no_grad():\n\n                infer_output = generator(\n                        inputs_ling=sequence,\n                        inputs_style_embedding=style_embedding,\n                        input_lengths=sequence_len,\n                        inputs_content_embedding=content_embedding,\n                        inputs_speaker=speaker,\n                        alpha=1.0\n                    )\n                audio = infer_output[\"wav_predictions\"].squeeze()* MAX_WAV_VALUE\n                audio = audio.cpu().numpy().astype('int16')\n                if not os.path.exists(root_path + \"/test_audio/audio/\" +f\"{file}/\"):\n                    os.makedirs(root_path + \"/test_audio/audio/\" +f\"{file}/\", exist_ok=True)\n                sf.write(file=root_path + \"/test_audio/audio/\" +f\"{file}/{i+1}.wav\", data=audio, samplerate=config.sampling_rate) #h.sampling_rate\n\n\n\n\n\n\nif __name__ == '__main__':\n    print(\"run!\")\n    p = argparse.ArgumentParser()\n    p.add_argument('-d', '--logdir', type=str, required=True)\n    p.add_argument(\"-c\", \"--config_folder\", type=str, required=True)\n    p.add_argument(\"--checkpoint\", type=str, required=False, default='', help='inference specific checkpoint, e.g --checkpoint checkpoint_230000')\n    p.add_argument('-t', '--test_file', type=str, required=True, help='the absolute path of test file that is going to inference')\n\n    args = p.parse_args() \n    ##################################################\n    sys.path.append(os.path.dirname(os.path.abspath(\"__file__\")) + \"/\" + args.config_folder)\n\n    from config import Config\n    config = Config()\n    ##################################################\n    main(args, config)\n\n\n"
        },
        {
          "name": "inference_tts.py",
          "type": "blob",
          "size": 8.4033203125,
          "content": "# Copyright 2023, YOUDAO\n#           2024, Du Jing(thuduj12@163.com)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport torch\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\nimport os, sys, torch, argparse\nimport numpy as np\nfrom models.hifigan.get_vocoder import MAX_WAV_VALUE\nimport soundfile as sf\nfrom yacs import config as CONFIG\nfrom tqdm import tqdm\nfrom frontend import g2p_cn_en\nfrom frontend_en import ROOT_DIR, read_lexicon, G2p\n\n\ndef get_style_embedding(prompt, tokenizer, style_encoder):\n    prompt = tokenizer([prompt], return_tensors=\"pt\")\n    input_ids = prompt[\"input_ids\"]\n    token_type_ids = prompt[\"token_type_ids\"]\n    attention_mask = prompt[\"attention_mask\"]\n\n    with torch.no_grad():\n        output = style_encoder(\n        input_ids=input_ids,\n        token_type_ids=token_type_ids,\n        attention_mask=attention_mask,\n    )\n    style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n    return style_embedding\n\ndef main(args, config, gpu_id, start_idx, chunk_num):\n    device = torch.device(\n        f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n    root_path = os.path.join(config.output_directory, args.logdir)\n    ckpt_path = os.path.join(root_path,  \"ckpt\")\n    checkpoint_path = os.path.join(ckpt_path, args.checkpoint)\n\n    output_dir = args.output_dir\n    if output_dir is None:\n        output_dir = os.path.join(root_path, 'audio')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    with open(config.model_config_path, 'r') as fin:\n        conf = CONFIG.load_cfg(fin)\n\n    conf.n_vocab = config.n_symbols\n    conf.n_speaker = config.speaker_n_labels\n\n    style_encoder = StyleEncoder(config)\n    model_CKPT = torch.load(config.style_encoder_ckpt, map_location=device)\n    model_ckpt = {}\n    for key, value in model_CKPT['model'].items():\n        new_key = key[7:]\n        model_ckpt[new_key] = value\n    style_encoder.load_state_dict(model_ckpt, strict=False)\n\n    generator = JETSGenerator(conf).to(device)\n    model_CKPT = torch.load(checkpoint_path, map_location=device)\n    generator.load_state_dict(model_CKPT['generator'])\n    generator.eval()\n\n    with open(config.token_list_path, 'r') as f:\n        token2id = {t.strip():idx for idx, t, in enumerate(f.readlines())}\n\n    with open(config.speaker2id_path, encoding='utf-8') as f:\n        id2speaker = {idx:t.strip() for idx, t in enumerate(f.readlines())}\n\n    tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n\n    lexicon = read_lexicon(f\"{ROOT_DIR}/lexicon/librispeech-lexicon.txt\")\n    g2p = G2p()\n    prompts = ['Happy', 'Excited', 'Sad', 'Angry']   # prompt is not efficient.\n    speakers = [i for i in range(conf.n_speaker)]\n\n    text_path = args.text_file\n    with open(text_path, \"r\") as f:\n        for i, line in enumerate(tqdm(f)):\n            if not i in range(start_idx, start_idx+chunk_num):\n                continue\n\n            # iteration on prompts and speakers.\n            prompt_idx = i % len(prompts)\n            speaker_idx = i % len(speakers)\n            prompt = prompts[prompt_idx]\n            speaker = speakers[speaker_idx]\n            speaker_name = id2speaker[speaker]\n            speaker_path = os.path.join(output_dir, speaker_name)\n            if not os.path.exists(speaker_path):\n                os.makedirs(speaker_path, exist_ok=True)\n            utt_name = f\"{i+1:06d}\"\n            if os.path.exists(f\"{speaker_path}/{utt_name}.wav\"):\n                print(f\"audio {speaker_path}/{utt_name}.wav exists, continue.\")\n                continue\n\n            try:\n                content = line.strip()\n                text = g2p_cn_en(content, g2p, lexicon)\n                text = text.split()\n\n                style_embedding = get_style_embedding(\n                    prompt, tokenizer, style_encoder)\n                content_embedding = get_style_embedding(\n                    content, tokenizer, style_encoder)\n\n                text_int = [token2id[ph] for ph in text]\n\n                sequence = torch.from_numpy(\n                    np.array(text_int)).to(device).long().unsqueeze(0)\n                sequence_len = torch.from_numpy(\n                    np.array([len(text_int)])).to(device)\n                style_embedding = torch.from_numpy(\n                    style_embedding).to(device).unsqueeze(0)\n                content_embedding = torch.from_numpy(\n                    content_embedding).to(device).unsqueeze(0)\n                speaker = torch.from_numpy(\n                    np.array([speaker])).to(device)\n                with torch.no_grad():\n                    infer_output = generator(\n                            inputs_ling=sequence,\n                            inputs_style_embedding=style_embedding,\n                            input_lengths=sequence_len,\n                            inputs_content_embedding=content_embedding,\n                            inputs_speaker=speaker,\n                            alpha=1.0\n                        )\n                    audio = infer_output[\n                                \"wav_predictions\"].squeeze() * MAX_WAV_VALUE\n                    audio = audio.cpu().numpy().astype('int16')\n\n                    sf.write(file=f\"{speaker_path}/{utt_name}.wav\",\n                             data=audio, samplerate=config.sampling_rate)\n                    with open(f\"{speaker_path}/{utt_name}.txt\",\n                              'w', encoding='utf-8') as ftext:\n                        ftext.write(f\"{content}\\n\")\n            except Exception as e:\n                print(f\"Error: {e}\")\n                continue\n\n\nif __name__ == '__main__':\n    p = argparse.ArgumentParser()\n    p.add_argument('-d', '--logdir', default=\"prompt_tts_open_source_joint\",\n                   type=str, required=False)\n    p.add_argument(\"-c\", \"--config_folder\", default=\"config/joint\",\n                   type=str, required=False)\n    p.add_argument(\"--checkpoint\", type=str, default='g_00140000',\n                   required=False, help='inference specific checkpointã€‚')\n    p.add_argument('-t', '--text_file', type=str, required=True,\n                   help='the absolute path of test fileã€‚')\n    p.add_argument('-o', '--output_dir', type=str, required=False,\n                   default=None, help='path to save the generated audios.')\n    p.add_argument('-g', '--gpu_ids', type=str, required=False, default='0')\n    p.add_argument('-n', '--num_thread', type=str, required=False, default='1')\n\n    args = p.parse_args()\n    sys.path.append(os.path.dirname(\n        os.path.abspath(\"__file__\")) + \"/\" + args.config_folder)\n\n    from config import Config\n    config = Config()\n\n    from multiprocessing import Process\n    gpus = args.gpu_ids\n    os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n    gpu_list = gpus.split(',')\n    gpu_num = len(gpu_list)\n    # 4GB GPU memory per thread, bottleneck is CPU usage!\n    thread_per_gpu = int(args.num_thread)\n    thread_num = gpu_num * thread_per_gpu  # threads\n    torch.set_num_threads(4)  # faster\n\n    total_len = 0\n    with open(args.text_file) as fin:\n        for line in fin:\n            total_len += 1\n\n    print(f\"Total texts: {total_len}, Thread nums: {thread_num}\")\n\n    if total_len >= thread_num:\n        chunk_size = int(total_len / thread_num)\n        remains = total_len - chunk_size * thread_num\n    else:\n        chunk_size = 1\n        remains = 0\n\n    process_list = []\n    chunk_begin = 0\n    for i in range(thread_num):\n        print(f\"process part {i}...\")\n        gpu_id = i % gpu_num\n        now_chunk_size = chunk_size\n        if remains > 0:\n            now_chunk_size = chunk_size + 1\n            remains = remains - 1\n        # use parallel processing or sequential processing\n        p = Process(target=main, args=(\n            args, config, gpu_id, chunk_begin, now_chunk_size))\n        # main(args, config, gpu_id, chunk_begin, now_chunk_size)\n        chunk_begin = chunk_begin + now_chunk_size\n        p.start()\n        process_list.append(p)\n\n    for i in process_list:\n        p.join()\n\n\n"
        },
        {
          "name": "lexicon",
          "type": "tree",
          "content": null
        },
        {
          "name": "mel_process.py",
          "type": "blob",
          "size": 3.7138671875,
          "content": "import math\nimport os\nimport random\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.utils.data\nimport numpy as np\nimport librosa\nimport librosa.util as librosa_util\nfrom librosa.util import normalize, pad_center, tiny\nfrom scipy.signal import get_window\nfrom scipy.io.wavfile import read\nfrom librosa.filters import mel as librosa_mel_fn\n\nMAX_WAV_VALUE = 32768.0\n\n\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\n\ndef dynamic_range_decompression_torch(x, C=1):\n\n    return torch.exp(x) / C\n\n\ndef spectral_normalize_torch(magnitudes):\n    output = dynamic_range_compression_torch(magnitudes)\n    return output\n\n\ndef spectral_de_normalize_torch(magnitudes):\n    output = dynamic_range_decompression_torch(magnitudes)\n    return output\n\n\nmel_basis = {}\nhann_window = {}\n\n\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global hann_window\n    dtype_device = str(y.dtype) + '_' + str(y.device)\n    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n    y = y.squeeze(1)\n\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n    return spec\n\n\ndef spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\n    global mel_basis\n    dtype_device = str(spec.dtype) + '_' + str(spec.device)\n    fmax_dtype_device = str(fmax) + '_' + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    spec = spectral_normalize_torch(spec)\n    return spec\n\n\ndef mel_spectrogram_torch(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global mel_basis, hann_window\n    dtype_device = str(y.dtype) + '_' + str(y.device)\n    fmax_dtype_device = str(fmax) + '_' + dtype_device\n    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(\n            sr=sampling_rate, \n            n_fft=n_fft, \n            n_mels=num_mels, \n            fmin=fmin, \n            fmax=fmax)\n        \n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=y.dtype, device=y.device)\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n    y = y.squeeze(1)\n\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=False)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    spec = spectral_normalize_torch(spec)\n\n    return spec"
        },
        {
          "name": "mfa",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "openaiapi.py",
          "type": "blob",
          "size": 5.638671875,
          "content": "import logging\nimport os\nimport io\nimport torch\nimport glob\n\nfrom fastapi import FastAPI, Response\nfrom pydantic import BaseModel\n\nfrom frontend import g2p_cn_en, ROOT_DIR, read_lexicon, G2p\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\nimport numpy as np\nimport soundfile as sf\nimport pyrubberband as pyrb\nfrom pydub import AudioSegment\nfrom yacs import config as CONFIG\nfrom config.joint.config import Config\n\nLOGGER = logging.getLogger(__name__)\n\nDEFAULTS = {\n}\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(DEVICE)\nconfig = Config()\nMAX_WAV_VALUE = 32768.0\n\n\ndef get_env(key):\n    return os.environ.get(key, DEFAULTS.get(key))\n\n\ndef get_int_env(key):\n    return int(get_env(key))\n\n\ndef get_float_env(key):\n    return float(get_env(key))\n\n\ndef get_bool_env(key):\n    return get_env(key).lower() == 'true'\n\n\ndef scan_checkpoint(cp_dir, prefix, c=8):\n    pattern = os.path.join(cp_dir, prefix + '?'*c)\n    cp_list = glob.glob(pattern)\n    if len(cp_list) == 0:\n        return None\n    return sorted(cp_list)[-1]\n\n\ndef get_models():\n\n    am_checkpoint_path = scan_checkpoint(\n        f'{config.output_directory}/prompt_tts_open_source_joint/ckpt', 'g_')\n\n    # f'{config.output_directory}/style_encoder/ckpt/checkpoint_163431'\n    style_encoder_checkpoint_path = scan_checkpoint(\n        f'{config.output_directory}/style_encoder/ckpt', 'checkpoint_', 6)\n\n    with open(config.model_config_path, 'r') as fin:\n        conf = CONFIG.load_cfg(fin)\n\n    conf.n_vocab = config.n_symbols\n    conf.n_speaker = config.speaker_n_labels\n\n    style_encoder = StyleEncoder(config)\n    model_CKPT = torch.load(style_encoder_checkpoint_path, map_location=\"cpu\")\n    model_ckpt = {}\n    for key, value in model_CKPT['model'].items():\n        new_key = key[7:]\n        model_ckpt[new_key] = value\n    style_encoder.load_state_dict(model_ckpt, strict=False)\n    generator = JETSGenerator(conf).to(DEVICE)\n\n    model_CKPT = torch.load(am_checkpoint_path, map_location=DEVICE)\n    generator.load_state_dict(model_CKPT['generator'])\n    generator.eval()\n\n    tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n\n    with open(config.token_list_path, 'r') as f:\n        token2id = {t.strip(): idx for idx, t, in enumerate(f.readlines())}\n\n    with open(config.speaker2id_path, encoding='utf-8') as f:\n        speaker2id = {t.strip(): idx for idx, t in enumerate(f.readlines())}\n\n    return (style_encoder, generator, tokenizer, token2id, speaker2id)\n\n\ndef get_style_embedding(prompt, tokenizer, style_encoder):\n    prompt = tokenizer([prompt], return_tensors=\"pt\")\n    input_ids = prompt[\"input_ids\"]\n    token_type_ids = prompt[\"token_type_ids\"]\n    attention_mask = prompt[\"attention_mask\"]\n    with torch.no_grad():\n        output = style_encoder(\n            input_ids=input_ids,\n            token_type_ids=token_type_ids,\n            attention_mask=attention_mask,\n        )\n    style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n    return style_embedding\n\n\ndef emotivoice_tts(text, prompt, content, speaker, models):\n    (style_encoder, generator, tokenizer, token2id, speaker2id) = models\n\n    style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n    content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n\n    speaker = speaker2id[speaker]\n\n    text_int = [token2id[ph] for ph in text.split()]\n\n    sequence = torch.from_numpy(np.array(text_int)).to(\n        DEVICE).long().unsqueeze(0)\n    sequence_len = torch.from_numpy(np.array([len(text_int)])).to(DEVICE)\n    style_embedding = torch.from_numpy(style_embedding).to(DEVICE).unsqueeze(0)\n    content_embedding = torch.from_numpy(\n        content_embedding).to(DEVICE).unsqueeze(0)\n    speaker = torch.from_numpy(np.array([speaker])).to(DEVICE)\n\n    with torch.no_grad():\n\n        infer_output = generator(\n            inputs_ling=sequence,\n            inputs_style_embedding=style_embedding,\n            input_lengths=sequence_len,\n            inputs_content_embedding=content_embedding,\n            inputs_speaker=speaker,\n            alpha=1.0\n        )\n\n    audio = infer_output[\"wav_predictions\"].squeeze() * MAX_WAV_VALUE\n    audio = audio.cpu().numpy().astype('int16')\n\n    return audio\n\n\nspeakers = config.speakers\nmodels = get_models()\napp = FastAPI()\nlexicon = read_lexicon(f\"{ROOT_DIR}/lexicon/librispeech-lexicon.txt\")\ng2p = G2p()\n\nfrom typing import Optional\nclass SpeechRequest(BaseModel):\n    input: str\n    voice: str = '8051'\n    prompt: Optional[str] = ''\n    language: Optional[str] = 'zh_us'\n    model: Optional[str] = 'emoti-voice'\n    response_format: Optional[str] = 'mp3'\n    speed: Optional[float] = 1.0\n\n\n@app.post(\"/v1/audio/speech\")\ndef text_to_speech(speechRequest: SpeechRequest):\n\n    text = g2p_cn_en(speechRequest.input, g2p, lexicon)\n    np_audio = emotivoice_tts(text, speechRequest.prompt,\n                              speechRequest.input, speechRequest.voice,\n                              models)\n    y_stretch = np_audio\n    if speechRequest.speed != 1.0:\n        y_stretch = pyrb.time_stretch(np_audio, config.sampling_rate, speechRequest.speed)\n    wav_buffer = io.BytesIO()\n    sf.write(file=wav_buffer, data=y_stretch,\n             samplerate=config.sampling_rate, format='WAV')\n    buffer = wav_buffer\n    response_format = speechRequest.response_format\n    if response_format != 'wav':\n        wav_audio = AudioSegment.from_wav(wav_buffer)\n        wav_audio.frame_rate=config.sampling_rate\n        buffer = io.BytesIO()\n        wav_audio.export(buffer, format=response_format)\n\n    return Response(content=buffer.getvalue(),\n                    media_type=f\"audio/{response_format}\")\n"
        },
        {
          "name": "plot_image.py",
          "type": "blob",
          "size": 0.7939453125,
          "content": "import matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\nimport os\n\ndef plot_image_sambert(target, melspec, mel_lengths=None, text_lengths=None, save_dir=None, global_step=None, name=None):\n    # Draw mel_plots\n    mel_plots, axes = plt.subplots(2,1,figsize=(20,15))\n\n    T = mel_lengths[-1]\n    L=100\n\n\n    axes[0].imshow(target[-1].detach().cpu()[:,:T],\n                   origin='lower',\n                   aspect='auto')\n\n    axes[1].imshow(melspec[-1].detach().cpu()[:,:T],\n                   origin='lower',\n                   aspect='auto')\n    for i in range(2):\n        tmp_dir = save_dir+'/att/'+name+'_'+str(global_step)\n        if not os.path.exists(tmp_dir):\n            os.makedirs(tmp_dir)\n        plt.savefig(tmp_dir+'/'+name+'_'+str(global_step)+'_melspec_%s.png'%i)\n\n    return mel_plots"
        },
        {
          "name": "predict.py",
          "type": "blob",
          "size": 8.1748046875,
          "content": "# Prediction interface for Cog âš™ï¸DEVICE\n# https://github.com/replicate/cog/blob/main/docs/python.md\n\nfrom cog import BasePredictor, Input, Path\nfrom typing import List\n\nimport numpy as np\nfrom yacs import config as CONFIG\nimport torch\nimport re\nimport os, glob\nimport time\nimport subprocess\nimport requests\nimport soundfile as sf\n\nfrom frontend_cn import g2p_cn\nfrom frontend_en import preprocess_english\nfrom config.joint.config import Config\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\n\nMAX_WAV_VALUE = 32768.0\n\n# url for the weights mirror\nREPLICATE_WEIGHTS_URL = \"https://weights.replicate.delivery/default\"\n\n# files to download from the weights mirrors\nDEFAULT_WEIGHTS = [\n    {\n        \"dest\": \"outputs/prompt_tts_open_source_joint/ckpt\",\n        \"src\": \"EmotiVoice\",\n        \"files\": [\n            \"do_00140000\",\n            \"g_00140000\",\n        ],\n    },\n    {\n        \"dest\": \"outputs/style_encoder/ckpt\",\n        \"src\": \"EmotiVoice\",\n        \"files\": [\n            \"checkpoint_163431\",\n        ],\n    },\n    {\n        \"dest\": \"WangZeJun/simbert-base-chinese\",\n        \"src\": \"simbert-base-chinese/b5c82a8ab1e4bcac799620fc4d870aae087b0c71\",\n        \"files\": [\n            \"pytorch_model.bin\",\n            \"config.json\",\n            \"vocab.txt\",\n        ],\n    }\n]\n\ndef scan_checkpoint(cp_dir, prefix, c=8):\n    pattern = os.path.join(cp_dir, prefix + '?'*c)\n    cp_list = glob.glob(pattern)\n    if len(cp_list) == 0:\n        return None\n    return sorted(cp_list)[-1]\n\ndef g2p_en(text):\n    return preprocess_english(text)\n\ndef contains_chinese(text):\n    pattern = re.compile(r'[\\u4e00-\\u9fa5]')\n    match = re.search(pattern, text)\n    return match is not None\n\ndef download_json(url: str, dest: Path):\n    res = requests.get(url, allow_redirects=True)\n    if res.status_code == 200 and res.content:\n        with dest.open(\"wb\") as f:\n            f.write(res.content)\n    else:\n        print(f\"Failed to download {url}. Status code: {res.status_code}\")\n\ndef download_weights(baseurl: str, basedest: str, files: List[str]):\n    \"\"\"Download model weights from Replicate and save to file.\n    Weights and download locations are specified in DEFAULT_WEIGHTS\n    \"\"\"\n    basedest = Path(basedest)\n    start = time.time()\n    print(\"downloading to: \", basedest)\n    basedest.mkdir(parents=True, exist_ok=True)\n    for f in files:\n        dest = basedest / f\n        url = os.path.join(REPLICATE_WEIGHTS_URL, baseurl, f)\n        if not dest.exists():\n            print(\"downloading url: \", url)\n            if dest.suffix == \".json\":\n                download_json(url, dest)\n            else:\n                subprocess.check_call([\"pget\", url, str(dest)], close_fds=False)\n    print(\"downloading took: \", time.time() - start)\n\nclass Predictor(BasePredictor):\n\n    def setup_models(self):\n        config = self.config\n        am_checkpoint_path = scan_checkpoint(f'{config.output_directory}/prompt_tts_open_source_joint/ckpt', 'g_')\n\n        style_encoder_checkpoint_path = scan_checkpoint(f'{config.output_directory}/style_encoder/ckpt', 'checkpoint_', 6)\n\n        with open(config.model_config_path, 'r') as fin:\n            conf = CONFIG.load_cfg(fin)\n\n        conf.n_vocab = config.n_symbols\n        conf.n_speaker = config.speaker_n_labels\n\n        style_encoder = StyleEncoder(config)\n        model_CKPT = torch.load(style_encoder_checkpoint_path, map_location=\"cpu\")\n        model_ckpt = {}\n        for key, value in model_CKPT['model'].items():\n            new_key = key[7:]\n            model_ckpt[new_key] = value\n        style_encoder.load_state_dict(model_ckpt, strict=False)\n        generator = JETSGenerator(conf).to(self.device)\n\n        model_CKPT = torch.load(am_checkpoint_path, map_location=self.device)\n        generator.load_state_dict(model_CKPT['generator'])\n        generator.eval()\n\n        self.tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n\n        with open(config.token_list_path, 'r') as f:\n            self.token2id = {t.strip():idx for idx, t, in enumerate(f.readlines())}\n\n        with open(config.speaker2id_path, encoding='utf-8') as f:\n            self.speaker2id = {t.strip():idx for idx, t in enumerate(f.readlines())}\n\n        self.style_encoder = style_encoder\n        self.generator = generator\n        print(self.tokenizer)\n\n    def setup(self) -> None:\n        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n        # self.model = torch.load(\"./weights.pth\")\n        for weight in DEFAULT_WEIGHTS:\n            download_weights(weight[\"src\"], weight[\"dest\"], weight[\"files\"])\n        self.config = Config()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.setup_models()\n\n    # def get_style_embedding(prompt, tokenizer, style_encoder):\n    def get_style_embedding(self, text):\n        tokenizer = self.tokenizer\n        style_encoder = self.style_encoder\n        text = tokenizer([text], return_tensors=\"pt\")\n        input_ids = text[\"input_ids\"]\n        token_type_ids = text[\"token_type_ids\"]\n        attention_mask = text[\"attention_mask\"]\n        with torch.no_grad():\n            output = style_encoder(\n            input_ids=input_ids,\n            token_type_ids=token_type_ids,\n            attention_mask=attention_mask,\n        )\n        style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n        return style_embedding\n\n    def tts(self, text, prompt, content, speaker):\n        style_embedding = self.get_style_embedding(prompt)\n        content_embedding = self.get_style_embedding(content)\n        device = self.device\n\n        speaker = self.speaker2id[speaker]\n\n        text_int = [self.token2id[ph] for ph in text.split()]\n\n        sequence = torch.from_numpy(np.array(text_int)).to(device).long().unsqueeze(0)\n        sequence_len = torch.from_numpy(np.array([len(text_int)])).to(device)\n        style_embedding = torch.from_numpy(style_embedding).to(device).unsqueeze(0)\n        content_embedding = torch.from_numpy(content_embedding).to(device).unsqueeze(0)\n        speaker = torch.from_numpy(np.array([speaker])).to(device)\n\n        with torch.no_grad():\n\n            infer_output = self.generator(\n                    inputs_ling=sequence,\n                    inputs_style_embedding=style_embedding,\n                    input_lengths=sequence_len,\n                    inputs_content_embedding=content_embedding,\n                    inputs_speaker=speaker,\n                    alpha=1.0\n                )\n\n        audio = infer_output[\"wav_predictions\"].squeeze()* MAX_WAV_VALUE\n        audio = audio.cpu().numpy().astype('int16')\n        path = os.path.join(self.config.output_directory,\"output.mp3\")\n        sf.write(file=path, data=audio, samplerate=self.config.sampling_rate)\n        return path\n\n    def predict(\n        self,\n        prompt: str = Input(\n            description=\"Input prompt\",\n            default=\"Happy\",\n        ),\n        content: str = Input(\n            description=\"Input text\",\n            default=\"Emoti-Voice - a Multi-Voice and Prompt-Controlled T-T-S Engine\",\n        ),\n        language: str = Input(\n            description=\"Language\",\n            choices=[\"English\", \"Chinese\"],\n            default=\"English\",\n        ),\n        speaker: str = Input(\n            description=\"speakers\",\n            choices=Config().speakers,\n            default=Config().speakers[0],\n        ),\n    ) -> Path:\n        \"\"\"Run a single prediction on the model\"\"\"\n        # processed_input = preprocess(image)\n        # output = self.model(processed_image, scale)\n        # return postprocess(output)\n        if language==\"English\":\n            if contains_chinese(content):\n                raise ValueError(\"æ–‡æœ¬å«æœ‰ä¸­æ–‡/input text contains Chinese, but language is English\")\n            else:\n                text = g2p_en(content)\n                path = self.tts(text, prompt, content, speaker)\n                return Path(path)\n        else:\n            if not contains_chinese(content):\n                raise ValueError(\"æ–‡æœ¬å«æœ‰è‹±æ–‡/input text contains English, but language is Chinese\")\n            else:\n                text = g2p_cn(content)\n                path = self.tts(text, prompt, content, speaker)\n                return Path(path)\n"
        },
        {
          "name": "prepare_for_training.py",
          "type": "blob",
          "size": 3.6611328125,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport os\nimport shutil\nimport argparse\n\n\ndef main(args):\n    from os.path import join\n    data_dir = args.data_dir\n    exp_dir = args.exp_dir\n    os.makedirs(exp_dir, exist_ok=True)\n\n    info_dir = join(exp_dir, 'info')\n    prepare_info(data_dir, info_dir)\n\n    config_dir = join(exp_dir, 'config')\n    prepare_config(data_dir, info_dir, exp_dir, config_dir)\n\n    ckpt_dir = join(exp_dir, 'ckpt')\n    prepare_ckpt(data_dir, info_dir, ckpt_dir)\n\n\nROOT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\ndef prepare_info(data_dir, info_dir):\n    import jsonlines\n    print('prepare_info: %s' %info_dir)\n    os.makedirs(info_dir, exist_ok=True)\n\n    for name in [\"emotion\", \"energy\", \"pitch\", \"speed\", \"tokenlist\"]:\n        shutil.copy(f\"{ROOT_DIR}/data/youdao/text/{name}\", f\"{info_dir}/{name}\")\n\n    d_speaker = {} # get all the speakers from datalist\n    with jsonlines.open(f\"{data_dir}/train/datalist.jsonl\") as reader:\n        for obj in reader:\n            speaker = obj[\"speaker\"]\n            if not speaker in d_speaker:\n                d_speaker[speaker] = 1\n            else:\n                d_speaker[speaker] += 1\n\n    with open(f\"{ROOT_DIR}/data/youdao/text/speaker2\") as f, \\\n        open(f\"{info_dir}/speaker\", \"w\") as fout:\n\n        for line in f:\n            speaker = line.strip()\n            if speaker in d_speaker:\n                print('warning: duplicate of speaker [%s] in [%s]' % (speaker, data_dir))\n                continue\n            fout.write(line.strip()+\"\\n\")\n\n        for speaker in sorted(d_speaker.keys()):\n            fout.write(speaker + \"\\n\")\n\n\ndef prepare_config(data_dir, info_dir, exp_dir, config_dir):\n    print('prepare_config: %s' %config_dir)\n    os.makedirs(config_dir, exist_ok=True)\n\n    with open(f\"{ROOT_DIR}/config/template.py\") as f, \\\n        open(f\"{config_dir}/config.py\", \"w\") as fout:\n\n        for line in f:\n            fout.write(line.replace('<DATA_DIR>', data_dir).replace('<INFO_DIR>', info_dir).replace('<EXP_DIR>', exp_dir))\n\n\ndef prepare_ckpt(data_dir, info_dir, ckpt_dir):\n    print('prepare_ckpt: %s' %ckpt_dir)\n    os.makedirs(ckpt_dir, exist_ok=True)\n    \n    with open(f\"{info_dir}/speaker\") as f:\n        speaker_list=[line.strip() for line in f]\n    assert len(speaker_list) >= 2014\n    \n    gen_ckpt_path = f\"{ROOT_DIR}/outputs/prompt_tts_open_source_joint/ckpt/g_00140000\"\n    disc_ckpt_path = f\"{ROOT_DIR}/outputs/prompt_tts_open_source_joint/ckpt/do_00140000\"\n\n    gen_ckpt = torch.load(gen_ckpt_path, map_location=\"cpu\")\n\n    speaker_embeddings = gen_ckpt[\"generator\"][\"am.spk_tokenizer.weight\"].clone()\n    \n    new_embedding = torch.randn((len(speaker_list)-speaker_embeddings.size(0), speaker_embeddings.size(1)))\n\n    gen_ckpt[\"generator\"][\"am.spk_tokenizer.weight\"] = torch.cat([speaker_embeddings, new_embedding], dim=0)\n\n\n    torch.save(gen_ckpt, f\"{ckpt_dir}/pretrained_generator\")\n    shutil.copy(disc_ckpt_path, f\"{ckpt_dir}/pretrained_discriminator\")\n\n\n\nif __name__ == \"__main__\":\n\n    p = argparse.ArgumentParser()\n    p.add_argument('--data_dir', type=str, required=True)\n    p.add_argument('--exp_dir', type=str, required=True)\n    args = p.parse_args()\n\n    main(args)\n"
        },
        {
          "name": "requirements.openaiapi.txt",
          "type": "blob",
          "size": 0.060546875,
          "content": "fastapi\npython-multipart\nuvicorn[standard]\npydub\npyrubberband\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1064453125,
          "content": "torch\ntorchaudio\nnumpy\nnumba\nscipy\ntransformers\nsoundfile\nyacs\ng2p_en\njieba\npypinyin\npypinyin_dict\nstreamlit\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.732421875,
          "content": "import os\n\nfrom setuptools import find_packages, setup\n\nrequirements={\n    \"infer\": [\n        \"numpy>=1.24.3\",\n        \"scipy>=1.10.1\",\n        \"torch>=2.1\",\n        \"torchaudio\",\n        \"soundfile>=0.12.0\",\n        \"librosa>=0.10.0\",\n        \"scikit-learn\",\n        \"numba==0.58.1\",\n        \"inflect>=5.6.0\",\n        \"tqdm>=4.64.1\",\n        \"pyyaml>=6.0\",\n        \"transformers==4.26.1\",\n        \"yacs\",\n        \"g2p_en\",\n        \"jieba\",\n        \"pypinyin\",\n        \"streamlit\",\n        \"pandas>=1.4,<2.0\",\n    ],\n    \"openai\": [\n        \"fastapi\",\n        \"python-multipart\",\n        \"uvicorn[standard]\",\n        \"pydub\",\n    ],\n    \"train\": [\n        \"jsonlines\",\n        \"praatio\",\n        \"pyworld\",\n        \"flake8\",\n        \"flake8-bugbear\",\n        \"flake8-comprehensions\",\n        \"flake8-executable\",\n        \"flake8-pyi\",\n        \"mccabe\",\n        \"pycodestyle\",\n        \"pyflakes\",\n        \"tensorboard\",\n        \"einops\",\n        \"matplotlib\",\n    ]\n}\n\ninfer_requires = requirements[\"infer\"]\nopenai_requires = requirements[\"infer\"] + requirements[\"openai\"]\ntrain_requires = requirements[\"infer\"] + requirements[\"train\"]\n\nVERSION = '0.2.0'\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as readme_file:\n    README = readme_file.read()\n\n\nsetup(\n    name=\"EmotiVoice\",\n    version=VERSION,\n    url=\"https://github.com/netease-youdao/EmotiVoice\",\n    author=\"Huaxuan Wang\",\n    author_email=\"wanghx04@rd.netease.com\",\n    description=\"EmotiVoice ğŸ˜Š: a Multi-Voice and Prompt-Controlled TTS Engine\",\n    long_description=README,\n    long_description_content_type=\"text/markdown\",\n    license=\"Apache Software License\",\n    # package\n    packages=find_packages(),\n    project_urls={\n        \"Documentation\": \"https://github.com/netease-youdao/EmotiVoice/wiki\",\n        \"Tracker\": \"https://github.com/netease-youdao/EmotiVoice/issues\",\n        \"Repository\": \"https://github.com/netease-youdao/EmotiVoice\",\n    },\n    install_requires=infer_requires,\n    extras_require={\n        \"train\": train_requires,\n        \"openai\": openai_requires,\n    },\n    python_requires=\">=3.8.0\",\n    classifiers=[\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Science/Research\",\n        \"Operating System :: POSIX :: Linux\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n        \"Topic :: Multimedia :: Sound/Audio :: Speech\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n)"
        },
        {
          "name": "text",
          "type": "tree",
          "content": null
        },
        {
          "name": "train_am_vocoder_joint.py",
          "type": "blob",
          "size": 19.1552734375,
          "content": "import argparse, time\nimport sys\nimport os\nimport torch, glob, itertools\nfrom yacs import config as CONFIG\n\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.functional as F\n\nfrom plot_image import plot_image_sambert\nfrom mel_process import mel_spectrogram_torch\nfrom models.prompt_tts_modified.jets import JETSGenerator, get_segments\nfrom models.hifigan.pretrained_discriminator import Discriminator\nfrom models.hifigan.models import discriminator_loss,  generator_loss, feature_loss\nfrom models.prompt_tts_modified.loss import TTSLoss\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom models.prompt_tts_modified.prompt_dataset import Dataset_PromptTTS as Dataset_PromptTTS_JETS\nfrom torch.utils.data import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef get_writer(output_directory):\n    logging_path=f'{output_directory}' + \"/log\"\n    if not os.path.exists(logging_path):\n        os.makedirs(logging_path, exist_ok=True)\n    writer = SummaryWriter(logging_path)\n    return writer\n\ndef save_checkpoint(filepath, obj):\n    print(\"Saving checkpoint to {}\".format(filepath))\n    torch.save(obj, filepath)\n    print(\"Complete.\")\n\n\ndef scan_checkpoint(cp_dir, prefix):\n    pattern = os.path.join(cp_dir, prefix + '????????')\n    cp_list = glob.glob(pattern)\n    if len(cp_list) == 0:\n        return None\n    return sorted(cp_list)[-1]\n\ndef load_checkpoint(filepath, device):\n    assert os.path.isfile(filepath)\n    print(\"Loading '{}'\".format(filepath))\n    checkpoint_dict = torch.load(filepath, map_location=device)\n    print(\"Complete.\")\n    return checkpoint_dict\n\n\n\n\ndef validate(args, generator, val_loader, iteration, writer, config, device, loss_fn):\n\n    generator.eval()\n\n    with torch.no_grad():\n        dec_mel_loss_list = []\n        postnet_mel_loss_list = []\n        dur_loss_list = []\n        pitch_loss_list = []\n        energy_loss_list = []\n        forwardsum_loss_list = []\n        bin_loss_list = []\n\n\n        for i, batch in enumerate(val_loader):\n\n            batch = {key: value.to(device, non_blocking=True) for key, value in batch.items()}\n\n            phoneme_id = batch[\"phoneme_id\"]\n            phoneme_lens = batch[\"phoneme_lens\"]\n            mel = batch[\"mel\"]\n            mel_lens = batch[\"mel_lens\"]\n            speaker = batch[\"speaker\"]\n            style_embedding = batch[\"style_embedding\"]\n            content_embedding = batch[\"content_embedding\"]\n            pitch = batch[\"pitch\"]\n            energy = batch[\"energy\"]\n            wav = batch[\"wav\"]\n\n            output = generator(\n                inputs_ling=phoneme_id,\n                inputs_style_embedding=style_embedding,\n                inputs_content_embedding=content_embedding,\n                input_lengths=phoneme_lens,\n                inputs_speaker=speaker,\n                output_lengths=mel_lens,\n                mel_targets=mel,\n                pitch_targets=pitch,\n                energy_targets=energy,\n                cut_flag=False\n            )\n\n            y_hat_mel = mel_spectrogram_torch(\n                    output[\"wav_predictions\"][:,:,:wav.size(1)].squeeze(1), \n                    config.filter_length, \n                    config.n_mel_channels, \n                    config.sampling_rate, \n                    config.hop_length, \n                    config.win_length, \n                    config.mel_fmin, \n                    config.mel_fmax\n                )\n\n\n            y_mel = mel_spectrogram_torch(\n                wav.squeeze(1), \n                config.filter_length, \n                config.n_mel_channels, \n                config.sampling_rate, \n                config.hop_length, \n                config.win_length, \n                config.mel_fmin, \n                config.mel_fmax\n            )\n            output[\"dec_outputs\"] = y_hat_mel\n            output[\"mel_targets\"] = y_mel.transpose(1,2)\n\n            losses = loss_fn(output)\n\n            dec_mel_loss_list.append(losses[\"dec_mel_loss\"].item())\n            dur_loss_list.append(losses[\"dur_loss\"].item())\n            pitch_loss_list.append(losses[\"pitch_loss\"].item())\n            energy_loss_list.append(losses[\"energy_loss\"].item())\n            forwardsum_loss_list.append(losses[\"forwardsum_loss\"].item())\n            bin_loss_list.append(losses[\"bin_loss\"].item())\n\n            \n        dec_mel_loss = sum(dec_mel_loss_list)/len(dec_mel_loss_list)\n        dur_loss = sum(dur_loss_list)/len(dur_loss_list)\n        pitch_loss = sum(pitch_loss_list)/len(pitch_loss_list)\n        energy_loss = sum(energy_loss_list)/len(energy_loss_list)\n        forwardsum_loss = sum(forwardsum_loss_list)/len(forwardsum_loss_list)\n        bin_loss = sum(bin_loss_list)/len(bin_loss_list)\n\n\n        message = f'global_step={iteration}, val_dec_mel_loss={dec_mel_loss:0.4f}, val_dur_loss={dur_loss:0.4f}, val_pitch_loss={pitch_loss:0.4f}, val_energy_loss={energy_loss:0.4f}, val_forwardsum_loss={forwardsum_loss:0.4f}, bin_loss={bin_loss:0.4f}, '\n        print(message)\n        with open(os.path.join(f'{config.output_directory}' + \"/log\", \"train_log.txt\"), \"a\") as f:\n            f.write(message + \"\\n\")\n\n    writer.add_scalar('val_dec_mel_loss', dec_mel_loss, global_step=iteration)\n    writer.add_scalar('val_dur_loss', dur_loss, global_step=iteration)\n    writer.add_scalar('val_pitch_loss', pitch_loss, global_step=iteration)\n    writer.add_scalar('val_energy_loss', energy_loss, global_step=iteration)\n    writer.add_scalar('val_forwardsum_loss', forwardsum_loss, global_step=iteration)\n    writer.add_scalar('val_bin_loss', bin_loss, global_step=iteration)\n    \n    mel_plots = plot_image_sambert(mel,\n                                output[\"dec_outputs\"],\n                                mel_lens,\n                                phoneme_lens, \n                                save_dir=f'{config.output_directory}', \n                                global_step=iteration, \n                                name='val')\n    writer.add_figure('Validation mel_plots', mel_plots, global_step=iteration)\n    \n    with torch.no_grad():\n        T=phoneme_lens[-1]\n        output_infer = generator(\n                inputs_ling=phoneme_id[-1,:T].unsqueeze(0),\n                inputs_style_embedding=style_embedding[-1].unsqueeze(0),\n                input_lengths=phoneme_lens[-1].unsqueeze(0),\n                inputs_content_embedding=content_embedding[-1].unsqueeze(0),\n                inputs_speaker=speaker[-1].unsqueeze(0),\n            )\n    \n    y_hat_mel = mel_spectrogram_torch(\n                    output_infer[\"wav_predictions\"].squeeze(1), \n                    config.filter_length, \n                    config.n_mel_channels, \n                    config.sampling_rate, \n                    config.hop_length, \n                    config.win_length, \n                    config.mel_fmin, \n                    config.mel_fmax\n                )\n    writer.add_audio('generated_audio', output_infer[\"wav_predictions\"].squeeze(1), iteration, 16_000)\n\n    mel_plots_infer = plot_image_sambert(mel,\n                                y_hat_mel,\n                                mel_lens,\n                                phoneme_lens, \n                                save_dir=f'{config.output_directory}', \n                                global_step=iteration, \n                                name='infer')\n\n    writer.add_figure('Inference mel_plots', mel_plots_infer, global_step=iteration)\n    generator.train()\n    return \n\n\ndef train(args, config):\n    # os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n    rank = int(os.environ[\"LOCAL_RANK\"])\n\n    torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\", world_size=args.n_gpus, rank=rank)\n\n    torch.cuda.set_device(rank)\n    device = torch.device(f'cuda:{rank}')\n    \n    if rank==0:\n        print(\"run!\")\n        writer = get_writer(config.output_directory)\n    \n    print(\"device: \", rank)\n\n    style_encoder = StyleEncoder(config)\n    model_CKPT = torch.load(config.style_encoder_ckpt, map_location=\"cpu\")\n    model_ckpt = {}\n    for key, value in model_CKPT['model'].items():\n        new_key = key[7:]\n        model_ckpt[new_key] = value\n    style_encoder.load_state_dict(model_ckpt, strict=False)\n\n    train_dataset = Dataset_PromptTTS_JETS(config.train_data_path, config, style_encoder)\n    data_sampler = DistributedSampler(train_dataset)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        num_workers=8,\n        shuffle=False,\n        batch_size=config.batch_size,\n        collate_fn=train_dataset.TextMelCollate,\n        sampler = data_sampler,\n    )\n    if rank == 0:\n        valid_dataset = Dataset_PromptTTS_JETS(config.valid_data_path, config, style_encoder)\n\n        valid_loader = torch.utils.data.DataLoader(\n            valid_dataset,\n            num_workers=1,\n            batch_size=config.batch_size,\n            collate_fn=train_dataset.TextMelCollate,\n            pin_memory=True,\n        )\n    with open(config.model_config_path, 'r') as fin:\n        conf = CONFIG.load_cfg(fin)\n    \n    conf.n_vocab = config.n_symbols\n    conf.n_speaker = config.speaker_n_labels\n\n    iteration=0\n\n\n    generator = JETSGenerator(conf).to(device)\n    discriminator = Discriminator(conf).to(device)\n\n    os.makedirs(f'{config.output_directory}' + '/ckpt', exist_ok=True)\n    cp_g = scan_checkpoint(f'{config.output_directory}' + '/ckpt', 'g_')\n    cp_do = scan_checkpoint(f'{config.output_directory}' + '/ckpt', 'do_')\n\n    if cp_g is None or cp_do is None:\n        state_dict_do = None\n        last_epoch = -1\n    else:\n        state_dict_g = load_checkpoint(cp_g, device)\n        state_dict_do = load_checkpoint(cp_do, device)\n        generator.load_state_dict(state_dict_g['generator'])\n        discriminator.load_state_dict(state_dict_do['discriminator'])\n        iteration = state_dict_do['steps'] + 1\n        last_epoch = state_dict_do['epoch']\n\n    if args.load_pretrained_model:\n        ckpt=torch.load(f'{config.output_directory}/ckpt/pretrained_generator')\n        generator.load_state_dict(ckpt['generator'])\n        ckpt=torch.load(f'{config.output_directory}/ckpt/pretrained_discriminator')\n        discriminator.load_state_dict(ckpt['discriminator'])\n        state_dict_do = None\n        last_epoch = -1\n        iteration=0\n\n        print()\n\n\n    generator = DDP(generator, device_ids=[rank]).to(device)\n    discriminator = DDP(discriminator, device_ids=[rank]).to(device)\n\n    optim_g = torch.optim.Adam(generator.parameters(), conf.optimizer.lr, betas=conf.optimizer.betas)\n    optim_d = torch.optim.Adam(discriminator.parameters(),\n                                conf.optimizer.lr, betas=conf.optimizer.betas)\n\n    if state_dict_do is not None:\n        optim_g.load_state_dict(state_dict_do['optim_g'])\n        optim_d.load_state_dict(state_dict_do['optim_d'])\n\n    \n    scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=conf.scheduler.gamma, last_epoch=last_epoch)\n    scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=conf.scheduler.gamma, last_epoch=last_epoch)\n\n    loss_fn = TTSLoss()\n\n    if rank == 0:\n        print(\"The number of parameters in the model: %0.2f M\"%(count_parameters(generator)/1000000.0))\n        print(f\"Training Start!!! ({config.output_directory})\")\n\n\n    generator.train()\n    discriminator.train()\n\n    for epoch in range(max(0, last_epoch), 5_000_000):\n\n        if rank == 0:\n            for param_group in optim_g.param_groups:\n                print(\"Current learning rate: \" + str(param_group[\"lr\"]))\n            print(\"Epoch: {}\".format(epoch+1))\n        \n        data_sampler.set_epoch(epoch)\n\n        for i, batch in enumerate(train_loader):\n            if rank == 0:\n                start_b = time.time()\n\n            batch = {key: value.to(device, non_blocking=True) for key, value in batch.items()}\n\n            phoneme_id = batch[\"phoneme_id\"]\n            phoneme_lens = batch[\"phoneme_lens\"]\n            mel = batch[\"mel\"]\n            mel_lens = batch[\"mel_lens\"]\n            speaker = batch[\"speaker\"]\n            style_embedding = batch[\"style_embedding\"]\n            content_embedding = batch[\"content_embedding\"]\n            pitch = batch[\"pitch\"]\n            energy = batch[\"energy\"]\n            wav = batch[\"wav\"]\n        \n            output = generator(\n                inputs_ling=phoneme_id,\n                inputs_style_embedding=style_embedding,\n                inputs_content_embedding=content_embedding,\n                input_lengths=phoneme_lens,\n                inputs_speaker=speaker,\n                output_lengths=mel_lens,\n                mel_targets=mel,\n                pitch_targets=pitch,\n                energy_targets=energy,\n            )\n\n            y_hat_mel = mel_spectrogram_torch(\n                output[\"wav_predictions\"].squeeze(1), \n                config.filter_length, \n                config.n_mel_channels, \n                config.sampling_rate, \n                config.hop_length, \n                config.win_length, \n                config.mel_fmin, \n                config.mel_fmax\n            )\n\n            wav = get_segments(\n                x=wav.unsqueeze(1),\n                start_idxs=output[\"z_start_idxs\"] * (generator.module.upsample_factor if hasattr(generator, \"module\") else generator.upsample_factor),\n                segment_size=output[\"segment_size\"] * (generator.module.upsample_factor if hasattr(generator, \"module\") else generator.upsample_factor),\n            )\n\n            y_mel = mel_spectrogram_torch(\n                wav.squeeze(1), \n                config.filter_length, \n                config.n_mel_channels, \n                config.sampling_rate, \n                config.hop_length, \n                config.win_length, \n                config.mel_fmin, \n                config.mel_fmax\n            )\n            output[\"dec_outputs\"] = y_hat_mel\n            output[\"mel_targets\"] = y_mel.transpose(1,2)\n\n            ########################################## Discriminator ##########################################\n            optim_d.zero_grad()\n\n            y_df_hat_r, y_df_hat_g, fmap_f_r, fmap_f_g, y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = discriminator(wav, output[\"wav_predictions\"].detach())\n\n            loss_disc_f, losses_disc_f_r, losses_disc_f_g = discriminator_loss(y_df_hat_r, y_df_hat_g)\n            loss_disc_s, losses_disc_s_r, losses_disc_s_g = discriminator_loss(y_ds_hat_r, y_ds_hat_g)\n\n            loss_disc_all = loss_disc_s + loss_disc_f\n\n            loss_disc_all.backward()\n            optim_d.step()\n\n\n\n\n            ########################################## Generator ##########################################\n\n            y_df_hat_r, y_df_hat_g, fmap_f_r, fmap_f_g, y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = discriminator(wav, output[\"wav_predictions\"])\n            optim_g.zero_grad()\n            loss = loss_fn(output)\n            loss_mel = F.l1_loss(y_mel, y_hat_mel)\n            loss[\"dec_mel_loss\"]=loss_mel\n            loss_fm_f = feature_loss(fmap_f_r, fmap_f_g)\n            loss_fm_s = feature_loss(fmap_s_r, fmap_s_g)\n            loss_gen_f, losses_gen_f = generator_loss(y_df_hat_g)\n            loss_gen_s, losses_gen_s = generator_loss(y_ds_hat_g)\n\n            dec_mel_loss = loss[\"dec_mel_loss\"] * 45\n            dur_loss = loss[\"dur_loss\"] * 1\n            pitch_loss = loss[\"pitch_loss\"] * 1\n            energy_loss = loss[\"energy_loss\"] * 1 \n            forwardsum_loss = loss[\"forwardsum_loss\"] * 2\n            bin_loss = loss[\"bin_loss\"] * 2\n            loss_gen = (loss_gen_f + loss_gen_s) * 1\n            loss_fm = (loss_fm_f + loss_fm_s)\n\n            loss_gen_all = loss_gen + loss_fm + \\\n                dec_mel_loss + dur_loss + \\\n                pitch_loss + energy_loss + \\\n                forwardsum_loss + bin_loss\n\n    \n            loss_gen_all.backward()\n            optim_g.step()\n            \n            iteration += 1\n\n            if rank==0:\n                writer.add_scalar('train_loss_fm', loss_fm, global_step=iteration)\n                writer.add_scalar('train_loss_gen', loss_gen, global_step=iteration)\n                writer.add_scalar('train_dec_mel_loss', dec_mel_loss, global_step=iteration)\n                writer.add_scalar('train_dur_loss', dur_loss, global_step=iteration)\n                writer.add_scalar('train_pitch_loss', pitch_loss, global_step=iteration)\n                writer.add_scalar('train_energy_loss', energy_loss, global_step=iteration)\n                writer.add_scalar('train_forwardsum_loss', forwardsum_loss, global_step=iteration)\n                writer.add_scalar('train_bin_loss', bin_loss, global_step=iteration)\n                message = f'global_step={iteration}, train_dec_mel_loss={dec_mel_loss:0.4f}, train_dur_loss={dur_loss:0.4f}, train_pitch_loss={pitch_loss:0.4f}, train_energy_loss={energy_loss:0.4f}, train_forwardsum_loss={forwardsum_loss:0.4f}, bin_loss={bin_loss:0.4f}, train_loss_fm={loss_fm:0.4f}, train_loss_gen={loss_gen:0.4f},  s/b={time.time() - start_b:4.3f}'\n                if iteration % (config.iters_per_validation) == 0:\n\n                    validate(args, generator, valid_loader, iteration, writer, config, device, loss_fn)\n                    print(message)\n                    with open(os.path.join(f'{config.output_directory}' + \"/log\", \"train_log.txt\"), \"a\") as f:\n                        f.write(message + \"\\n\")\n                elif iteration % (config.iters_per_validation//10) == 0:\n\n                    print(message)\n                    with open(os.path.join(f'{config.output_directory}' + \"/log\", \"train_log.txt\"), \"a\") as f:\n                        f.write(message + \"\\n\")\n\n                if iteration % (config.iters_per_checkpoint) == 0:\n                    checkpoint_path = \"{}/g_{:08d}\".format(f'{config.output_directory}' + '/ckpt', iteration)\n                    save_checkpoint(checkpoint_path, {'generator': (generator.module if hasattr(generator, 'module') else generator).state_dict()})\n                    checkpoint_path = \"{}/do_{:08d}\".format(f'{config.output_directory}' + '/ckpt', iteration)\n                    save_checkpoint(checkpoint_path, \n                                    {'discriminator': (discriminator.module if hasattr(discriminator, 'module')\n                                                        else discriminator).state_dict(),\n                                    'optim_g': optim_g.state_dict(), 'optim_d': optim_d.state_dict(), 'steps': iteration,\n                                    'epoch': epoch})\n\n                if iteration == (config.train_steps):\n                    writer.close()\n                    print(\"TRAINING DONE!\")\n                    break\n        scheduler_g.step()\n        scheduler_d.step()\n\ndef main():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"-c\", \"--config_folder\", type=str, required=True)\n    p.add_argument(\"--checkpoint\", type=str, default=\"\")\n    p.add_argument(\"--load_pretrained_model\", default=False)\n    args = p.parse_args() \n\n    ##################################################\n    sys.path.append(args.config_folder)\n    from config import Config\n    config = Config()\n    ##################################################\n    n_gpus = torch.cuda.device_count()\n    args.n_gpus=n_gpus\n\n\n    torch.manual_seed(config.seed)\n    torch.cuda.manual_seed(config.seed)\n    # os.environ[\n    #     \"TORCH_DISTRIBUTED_DEBUG\"\n    # ] = \"DETAIL\"\n    train(args, config)\n\n\nif __name__ == '__main__':\n    main()"
        }
      ]
    }
  ]
}