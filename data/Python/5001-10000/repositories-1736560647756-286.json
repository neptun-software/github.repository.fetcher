{
  "metadata": {
    "timestamp": 1736560647756,
    "page": 286,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "netease-youdao/EmotiVoice",
      "stars": 7584,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.3076171875,
          "content": "# The .dockerignore file excludes files from the container build process.\n#\n# https://docs.docker.com/engine/reference/builder/#dockerignore-file\n\n# Exclude Git files\n.git\n.github\n.gitignore\n\n# Exclude Python cache files\n__pycache__\n.mypy_cache\n.pytest_cache\n.ruff_cache\n\n# Exclude Python virtual environment\n/venv\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0537109375,
          "content": "outputs/\nWangZeJun/\n*.pyc\n.vscode/\n__pycache__/\n.idea/\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.5166015625,
          "content": "# syntax=docker/dockerfile:1\nFROM ubuntu:22.04\n\n# install app dependencies\nRUN apt-get update && apt-get install -y python3 python3-pip libsndfile1\nRUN python3 -m pip install torch==1.11.0 torchaudio numpy numba scipy transformers==4.26.1 soundfile yacs\nRUN python3 -m pip install pypinyin jieba\n\n# install app\nRUN mkdir /EmotiVoice\nCOPY . /EmotiVoice/\n\n# final configuration\nEXPOSE 8501\nRUN python3 -m pip install streamlit g2p_en\nWORKDIR /EmotiVoice\nRUN python3 frontend_en.py\nCMD streamlit run demo_page.py --server.port 8501\n"
        },
        {
          "name": "EmotiVoice_UserAgreement_易魔声用户协议.pdf",
          "type": "blob",
          "size": 425.37890625,
          "content": null
        },
        {
          "name": "HTTP_API_TtsDemo",
          "type": "tree",
          "content": null
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 14.0009765625,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2023, YOUDAO\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n                                 Legal Disclaimer and Notices\n\nFurther, the project may contain Third Party open source software (“OSS”) which include licenses under terms that require YOUDAO to display the following notice: \n\n[Pytorch] \nPytorch is available under modified BSD license. You may find the source codes from https://github.com/pytorch/pytorch, and see the license at https://github.com/pytorch/pytorch/blob/main/LICENSE. \n    \n[ESPnet]\nESPnet is available under Apache 2.0 license. You may find the source codes from https://github.com/espnet/espnet, and see the license at https://github.com/espnet/espnet/blob/master/LICENSE. \n\n[WeTTS]\nWeTTS is available under Apache 2.0 license. You may find the source codes from https://github.com/wenet-e2e/wetts, and see the license at https://github.com/wenet-e2e/wetts/blob/main/LICENSE.\n\n[HiFi-GAN]\nHiFi-GAN is available under MIT license. You may find the source codes from https://github.com/jik876/hifi-gan, and see the license at https://github.com/jik876/hifi-gan/blob/master/LICENSE.\n\n[Transformers]\nTransformers is available under Apache 2.0 license. You may find the source codes from https://github.com/huggingface/transformers, and see the license at https://github.com/huggingface/transformers/blob/main/LICENSE.\n\n[KAN-TTS]\nKAN-TTS is available under MIT license. You may find the source codes from https://github.com/alibaba-damo-academy/KAN-TTS, and see the license at https://github.com/alibaba-damo-academy/KAN-TTS/blob/main/LICENSE.\n\n[StyleTTS]\nStyleTTS is available under MIT license. You may find the source codes from https://github.com/yl4579/StyleTTS, and see the license at https://github.com/yl4579/StyleTTS/blob/main/LICENSE.\n\n[tacotron]\ntacotron is available under MIT license. You may find the source codes from https://github.com/keithito/tacotron, and see the license at https://github.com/keithito/tacotron/blob/master/LICENSE.\n\n[tacotron2]\ntacotron2 is available under BSD 3-Clause license. You may find the source codes from https://github.com/NVIDIA/tacotron2, and see the license at https://github.com/NVIDIA/tacotron2/blob/master/LICENSE.\n\n[torch-stft]\ntorch-stft is available under BSD 3-Clause license. You may find the source codes from https://github.com/pseeth/torch-stft, and see the license at https://github.com/pseeth/torch-stft/blob/master/LICENSE.\n\n[LibriTTS]\nLibriTTS is available under CC BY 4.0 license. You may find the data from https://www.openslr.org/60/, and see the license at https://www.openslr.org/60/.\n\n[Hi-Fi TTS]\nHi-Fi TTS is available under CC BY 4.0 license. You may find the data from https://www.openslr.org/109/, and see the license at https://www.openslr.org/109/.\n\nYOUDAO does not make any representation or warranty with respect to any OSS or free software that may be included in or accompany the Service. YOUDAO HEREBY DISCLAIMS ANY AND ALL LIABILITY TO DEMAND PARTNER OR ANY THIRD PARTY RELATED TO ANY SUCH SOFTWARE THAT MAY BE INCLUDED IN OR ACCOMPANY THE SERVICE. "
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.583984375,
          "content": "<div align=\"center\">\n<a href=\"https://trendshift.io/repositories/4833\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/4833\" alt=\"netease-youdao%2FEmotiVoice | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n<font size=4> README: EN | <a href=\"./README.zh.md\">中文</a>  </font>\n    <h1>EmotiVoice 😊: a Multi-Voice and Prompt-Controlled TTS Engine</h1>\n</div>\n\n<div align=\"center\">\n    <a href=\"./README.zh.md\"><img src=\"https://img.shields.io/badge/README-中文版本-red\"></a>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n    <a href=\"./LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache--2.0-yellow\"></a>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n    <a href=\"https://twitter.com/YDopensource\"><img src=\"https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&style={style}\"></a>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n</div>\n<br>\n\n**EmotiVoice** is a powerful and modern open-source text-to-speech engine that is available to you at no cost. EmotiVoice speaks both English and Chinese, and with over 2000 different voices (refer to the [List of Voices](https://github.com/netease-youdao/EmotiVoice/wiki/😊-voice-wiki-page) for details). The most prominent feature is **emotional synthesis**, allowing you to create speech with a wide range of emotions, including happy, excited, sad, angry and others.\n\nAn easy-to-use web interface is provided. There is also a scripting interface for batch generation of results. \n\nHere are a few samples that EmotiVoice generates:\n\n\n- [Chinese audio sample](https://github.com/netease-youdao/EmotiVoice/assets/3909232/6426d7c1-d620-4bfc-ba03-cd7fc046a4fb)\n  \n- [English audio sample](https://github.com/netease-youdao/EmotiVoice/assets/3909232/8f272eba-49db-493b-b479-2d9e5a419e26)\n  \n- [Fun Chinese English audio sample](https://github.com/netease-youdao/EmotiVoice/assets/3909232/a0709012-c3ef-4182-bb0e-b7a2ba386f1c)\n\n## Demo\n\nA demo is hosted on Replicate, [EmotiVoice](https://replicate.com/bramhooimeijer/emotivoice).\n\n## Hot News\n\n- [x] Tuning voice speed is now supported in 'OpenAI-compatible-TTS API', thanks to [@john9405](https://github.com/john9405). [#90](https://github.com/netease-youdao/EmotiVoice/pull/90) [#67](https://github.com/netease-youdao/EmotiVoice/issues/67) [#77](https://github.com/netease-youdao/EmotiVoice/issues/77)\n\n- [x] [The EmotiVoice app for Mac](https://github.com/netease-youdao/EmotiVoice/releases/download/v0.3/emotivoice-1.0.0-arm64.dmg) was released on December 28th, 2023. Just download and taste EmotiVoice's offerings!\n\n- [x] [The EmotiVoice HTTP API](https://github.com/netease-youdao/EmotiVoice/wiki/HTTP-API) was released on December 6th, 2023. Easier to start, faster to use, and with **over 13,000 free calls**. Additionally, users can explore more captivating voices provided by [Zhiyun](https://ai.youdao.com/).\n- [x] [Voice Cloning with your personal data](https://github.com/netease-youdao/EmotiVoice/wiki/Voice-Cloning-with-your-personal-data) has been released on December 13th, 2023, along with [DataBaker Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/DataBaker) and [LJSpeech Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/LJspeech). \n\n## Features under development\n\n- [ ] Support for more languages, such as Japanese and Korean. [#19](https://github.com/netease-youdao/EmotiVoice/issues/19) [#22](https://github.com/netease-youdao/EmotiVoice/issues/22)\n\nEmotiVoice prioritizes community input and user requests. We welcome your feedback!\n\n## Quickstart\n\n### EmotiVoice Docker image\n\nThe easiest way to try EmotiVoice is by running the docker image. You need a machine with a NVidia GPU. If you have not done so, set up NVidia container toolkit by following the instructions for [Linux](https://www.server-world.info/en/note?os=Ubuntu_22.04&p=nvidia&f=2) or [Windows WSL2](https://github.com/nyp-sit/it3103/blob/main/nvidia-docker-wsl2.md). Then EmotiVoice can be run with,\n\n```sh\ndocker run -dp 127.0.0.1:8501:8501 syq163/emoti-voice:latest\n```\nThe Docker image was updated on January 4th, 2024. If you have an older version, please update it by running the following commands:\n```sh\ndocker pull syq163/emoti-voice:latest\ndocker run -dp 127.0.0.1:8501:8501 -p 127.0.0.1:8000:8000 syq163/emoti-voice:latest\n```\nNow open your browser and navigate to http://localhost:8501 to start using EmotiVoice's powerful TTS capabilities.\n\nStarting from this version, the 'OpenAI-compatible-TTS API' is now accessible via http://localhost:8000/.\n\n### Full installation\n\n```sh\nconda create -n EmotiVoice python=3.8 -y\nconda activate EmotiVoice\npip install torch torchaudio\npip install numpy numba scipy transformers soundfile yacs g2p_en jieba pypinyin pypinyin_dict\npython -m nltk.downloader \"averaged_perceptron_tagger_eng\"\n```\n\n### Prepare model files\n\nWe recommend that users refer to the wiki page [How to download the pretrained model files](https://github.com/netease-youdao/EmotiVoice/wiki/Pretrained-models) if they encounter any issues.\n\n```sh\ngit lfs install\ngit lfs clone https://huggingface.co/WangZeJun/simbert-base-chinese WangZeJun/simbert-base-chinese\n```\nor, you can run:\n```sh\ngit clone https://www.modelscope.cn/syq163/WangZeJun.git\n```\n\n### Inference\n\n1. You can download the [pretrained models](https://drive.google.com/drive/folders/1y6Xwj_GG9ulsAonca_unSGbJ4lxbNymM?usp=sharing) by simply running the following command:\n```sh\ngit clone https://www.modelscope.cn/syq163/outputs.git\n```\n2. The inference text format is `<speaker>|<style_prompt/emotion_prompt/content>|<phoneme>|<content>`. \n\n  - inference text example: `8051|Happy|<sos/eos> [IH0] [M] [AA1] [T] engsp4 [V] [OY1] [S] engsp4 [AH0] engsp1 [M] [AH1] [L] [T] [IY0] engsp4 [V] [OY1] [S] engsp1 [AE1] [N] [D] engsp1 [P] [R] [AA1] [M] [P] [T] engsp4 [K] [AH0] [N] [T] [R] [OW1] [L] [D] engsp1 [T] [IY1] engsp4 [T] [IY1] engsp4 [EH1] [S] engsp1 [EH1] [N] [JH] [AH0] [N] . <sos/eos>|Emoti-Voice - a Multi-Voice and Prompt-Controlled T-T-S Engine`.\n4. You can get phonemes by `python frontend.py data/my_text.txt > data/my_text_for_tts.txt`.\n\n5. Then run:\n```sh\nTEXT=data/inference/text\npython inference_am_vocoder_joint.py \\\n--logdir prompt_tts_open_source_joint \\\n--config_folder config/joint \\\n--checkpoint g_00140000 \\\n--test_file $TEXT\n```\nthe synthesized speech is under `outputs/prompt_tts_open_source_joint/test_audio`.\n\n1. Or if you just want to use the interactive TTS demo page, run:\n```sh\npip install streamlit\nstreamlit run demo_page.py\n```\n\n### OpenAI-compatible-TTS API\n\nThanks to @lewangdev for adding an OpenAI compatible API [#60](../../issues/60). To set it up, use the following command:\n\n```sh\npip install fastapi pydub uvicorn[standard] pyrubberband\nuvicorn openaiapi:app --reload\n```\n\n### Wiki page\n\nYou may find more information from our [wiki](https://github.com/netease-youdao/EmotiVoice/wiki) page.\n\n## Training\n\n[Voice Cloning with your personal data](https://github.com/netease-youdao/EmotiVoice/wiki/Voice-Cloning-with-your-personal-data) has been released on December 13th, 2023.\n\n\n## Roadmap & Future work\n\n- Our future plan can be found in the [ROADMAP](./ROADMAP.md) file.\n- The current implementation focuses on emotion/style control by prompts. It uses only pitch, speed, energy, and emotion as style factors, and does not use gender. But it is not complicated to change it to style/timbre control.\n- Suggestions are welcome. You can file issues or [@ydopensource](https://twitter.com/YDopensource) on twitter.\n\n\n## WeChat group\nWelcome to scan the QR code below and join the WeChat group.\n\n<img src=\"https://github.com/netease-youdao/EmotiVoice/assets/49354974/cc3f4c8b-8369-4e50-89cc-e40d27a6bdeb\" alt=\"qr\" width=\"150\"/>\n\n## Credits\n\n- [PromptTTS](https://speechresearch.github.io/prompttts/). The PromptTTS paper is a key basis of this project.\n- [LibriTTS](https://www.openslr.org/60/). The LibriTTS dataset is used in training of EmotiVoice.\n- [HiFiTTS](https://www.openslr.org/109/). The HiFi TTS dataset is used in training of EmotiVoice.\n- [ESPnet](https://github.com/espnet/espnet). \n- [WeTTS](https://github.com/wenet-e2e/wetts)\n- [HiFi-GAN](https://github.com/jik876/hifi-gan)\n- [Transformers](https://github.com/huggingface/transformers)\n- [tacotron](https://github.com/keithito/tacotron)\n- [KAN-TTS](https://github.com/alibaba-damo-academy/KAN-TTS)\n- [StyleTTS](https://github.com/yl4579/StyleTTS)\n- [Simbert](https://github.com/ZhuiyiTechnology/simbert)\n- [cn2an](https://github.com/Ailln/cn2an). EmotiVoice incorporates cn2an for number processing.\n\n## License\n\nEmotiVoice is provided under the Apache-2.0 License - see the [LICENSE](./LICENSE) file for details.\n\nThe interactive page is provided under the [User Agreement](./EmotiVoice_UserAgreement_易魔声用户协议.pdf) file.\n"
        },
        {
          "name": "README.zh.md",
          "type": "blob",
          "size": 7.662109375,
          "content": "<font size=4> README: <a href=\"./README.md\">EN</a> | 中文  </font>\n\n\n<div align=\"center\">\n    <h1>EmotiVoice易魔声 😊: 多音色提示控制TTS</h1>\n</div>\n\n<div align=\"center\">\n    <a href=\"./README.md\"><img src=\"https://img.shields.io/badge/README-EN-red\"></a>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n    <a href=\"./LICENSE\"><img src=\"https://img.shields.io/badge/license-Apache--2.0-yellow\"></a>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n    <a href=\"https://twitter.com/YDopensource\"><img src=\"https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&style={style}\"></a>\n    &nbsp;&nbsp;&nbsp;&nbsp;\n</div>\n<br>\n\n**EmotiVoice**是一个强大的开源TTS引擎，**完全免费**，支持中英文双语，包含2000多种不同的音色，以及特色的**情感合成**功能，支持合成包含快乐、兴奋、悲伤、愤怒等广泛情感的语音。\n\nEmotiVoice提供一个易于使用的web界面，还有用于批量生成结果的脚本接口。\n\n以下是EmotiVoice生成的几个示例:\n\n- [Chinese audio sample](https://github.com/netease-youdao/EmotiVoice/assets/3909232/6426d7c1-d620-4bfc-ba03-cd7fc046a4fb)\n  \n- [English audio sample](https://github.com/netease-youdao/EmotiVoice/assets/3909232/8f272eba-49db-493b-b479-2d9e5a419e26)\n  \n- [Fun Chinese English audio sample](https://github.com/netease-youdao/EmotiVoice/assets/3909232/a0709012-c3ef-4182-bb0e-b7a2ba386f1c)\n\n## 热闻速递\n\n- [x] 类OpenAI TTS的API已经支持调语速功能，感谢 [@john9405](https://github.com/john9405). [#90](https://github.com/netease-youdao/EmotiVoice/pull/90) [#67](https://github.com/netease-youdao/EmotiVoice/issues/67) [#77](https://github.com/netease-youdao/EmotiVoice/issues/77)\n- [x] [Mac版一键安装包](https://github.com/netease-youdao/EmotiVoice/releases/download/v0.3/emotivoice-1.0.0-arm64.dmg) 已于2023年12月28日发布，**强烈推荐尽快下载使用，免费好用！**\n- [x] [易魔声 HTTP API](https://github.com/netease-youdao/EmotiVoice/wiki/HTTP-API) 已于2023年12月6日发布上线。更易上手（无需任何安装配置），更快更稳定，单账户提供**超过 13,000 次免费调用**。此外，用户还可以使用[智云](https://ai.youdao.com/)提供的其它迷人的声音。\n- [x] [用你自己的数据定制音色](https://github.com/netease-youdao/EmotiVoice/wiki/Voice-Cloning-with-your-personal-data)已于2023年12月13日发布上线，同时提供了两个教程示例：[DataBaker Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/DataBaker)  [LJSpeech Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/LJspeech)。\n\n## 开发中的特性\n\n- [ ] 更多语言支持，例如日韩 [#19](https://github.com/netease-youdao/EmotiVoice/issues/19) [#22](https://github.com/netease-youdao/EmotiVoice/issues/22)\n\n易魔声倾听社区需求并积极响应，期待您的反馈！\n\n## 快速入门\n\n### EmotiVoice Docker镜像\n\n尝试EmotiVoice最简单的方法是运行docker镜像。你需要一台带有NVidia GPU的机器。先按照[Linux](https://www.server-world.info/en/note?os=Ubuntu_22.04&p=nvidia&f=2)和[Windows WSL2](https://zhuanlan.zhihu.com/p/653173679)平台的说明安装NVidia容器工具包。然后可以直接运行EmotiVoice镜像：\n\n```sh\ndocker run -dp 127.0.0.1:8501:8501 syq163/emoti-voice:latest\n```\n\nDocker镜像更新于2024年1月4号。如果你使用了老的版本，推荐运行如下命令进行更新：\n```sh\ndocker pull syq163/emoti-voice:latest\ndocker run -dp 127.0.0.1:8501:8501 -p 127.0.0.1:8000:8000 syq163/emoti-voice:latest\n```\n\n现在打开浏览器，导航到 http://localhost:8501 ，就可以体验EmotiVoice强大的TTS功能。从2024年的docker镜像版本开始，通过http://localhost:8000/可以使用类OpenAI TTS的API功能。\n\n### 完整安装\n\n```sh\nconda create -n EmotiVoice python=3.8 -y\nconda activate EmotiVoice\npip install torch torchaudio\npip install numpy numba scipy transformers soundfile yacs g2p_en jieba pypinyin pypinyin_dict\npython -m nltk.downloader \"averaged_perceptron_tagger_eng\"\n```\n\n### 准备模型文件\n\n强烈推荐用户参考[如何下载预训练模型文件](https://github.com/netease-youdao/EmotiVoice/wiki/Pretrained-models)的维基页面，尤其遇到问题时。\n\n```sh\ngit lfs install\ngit lfs clone https://huggingface.co/WangZeJun/simbert-base-chinese WangZeJun/simbert-base-chinese\n```\n\n或者你可以运行:\n```sh\ngit clone https://www.modelscope.cn/syq163/WangZeJun.git\n```\n\n### 推理\n\n1. 通过简单运行如下命令来下载[预训练模型](https://drive.google.com/drive/folders/1y6Xwj_GG9ulsAonca_unSGbJ4lxbNymM?usp=sharing):\n\n```sh\ngit clone https://www.modelscope.cn/syq163/outputs.git\n```\n\n2. 推理输入文本格式是：`<speaker>|<style_prompt/emotion_prompt/content>|<phoneme>|<content>`. \n\n  - 例如: `8051|非常开心|<sos/eos>  uo3 sp1 l ai2 sp0 d ao4 sp1 b ei3 sp0 j ing1 sp3 q ing1 sp0 h ua2 sp0 d a4 sp0 x ve2 <sos/eos>|我来到北京，清华大学`.\n4. 其中的音素（phonemes）可以这样得到：`python frontend.py data/my_text.txt > data/my_text_for_tts.txt`.\n\n5. 然后运行：\n```sh\nTEXT=data/inference/text\npython inference_am_vocoder_joint.py \\\n--logdir prompt_tts_open_source_joint \\\n--config_folder config/joint \\\n--checkpoint g_00140000 \\\n--test_file $TEXT\n```\n合成的语音结果在：`outputs/prompt_tts_open_source_joint/test_audio`.\n\n6. 或者你可以直接使用交互的网页界面：\n```sh\npip install streamlit\nstreamlit run demo_page.py\n```\n\n### 类OpenAI TTS的API\n\n非常感谢 @lewangdev 的相关该工作 [#60](../../issues/60)。通过运行如下命令来完成配置：\n\n```sh\npip install fastapi pydub uvicorn[standard] pyrubberband\nuvicorn openaiapi:app --reload\n```\n\n### Wiki页面\n\n如果遇到问题，或者想获取更多详情，请参考 [wiki](https://github.com/netease-youdao/EmotiVoice/wiki) 页面。\n\n## 训练\n\n[用你自己的数据定制音色](https://github.com/netease-youdao/EmotiVoice/wiki/Voice-Cloning-with-your-personal-data)已于2023年12月13日发布上线。\n\n## 路线图和未来的工作\n\n- 我们未来的计划可以在 [ROADMAP](./ROADMAP.md) 文件中找到。\n\n- 当前的实现侧重于通过提示控制情绪/风格。它只使用音高、速度、能量和情感作为风格因素，而不使用性别。但是将其更改为样式、音色控制并不复杂，类似于PromptTTS的原始闭源实现。\n\n## 微信群\n\n欢迎扫描下方左侧二维码加入微信群。商业合作扫描右侧个人二维码。\n\n<img src=\"https://github.com/netease-youdao/EmotiVoice/assets/49354974/cc3f4c8b-8369-4e50-89cc-e40d27a6bdeb\" alt=\"qr\" width=\"150\"/>\n&nbsp;&nbsp;&nbsp;&nbsp;\n<img src=\"https://github.com/netease-youdao/EmotiVoice/assets/3909232/94ee0824-0304-4487-8682-664fafd09cdf\" alt=\"qr\" width=\"150\"/>\n\n## 致谢\n\n- [PromptTTS](https://speechresearch.github.io/prompttts/). PromptTTS论文是本工作的重要基础。\n- [LibriTTS](https://www.openslr.org/60/). 训练使用了LibriTTS开放数据集。\n- [HiFiTTS](https://www.openslr.org/109/). 训练使用了HiFi TTS开放数据集。\n- [ESPnet](https://github.com/espnet/espnet). \n- [WeTTS](https://github.com/wenet-e2e/wetts)\n- [HiFi-GAN](https://github.com/jik876/hifi-gan)\n- [Transformers](https://github.com/huggingface/transformers)\n- [tacotron](https://github.com/keithito/tacotron)\n- [KAN-TTS](https://github.com/alibaba-damo-academy/KAN-TTS)\n- [StyleTTS](https://github.com/yl4579/StyleTTS)\n- [Simbert](https://github.com/ZhuiyiTechnology/simbert)\n- [cn2an](https://github.com/Ailln/cn2an). 易魔声集成了cn2an来处理数字。\n\n## 许可\n\nEmotiVoice是根据Apache-2.0许可证提供的 - 有关详细信息，请参阅[许可证文件](./LICENSE)。\n\n交互的网页是根据[用户协议](./EmotiVoice_UserAgreement_易魔声用户协议.pdf)提供的。\n"
        },
        {
          "name": "README_小白安装教程.md",
          "type": "blob",
          "size": 7.26171875,
          "content": "## 小白安装教程\n\n#### 环境条件：设备有GPU、已经安装cuda\n\n说明：这是针对Linux环境安装的教程，其他系统可作为参考。\n\n#### 1、创建并进入conda环境\n\n```\nconda create -n EmotiVoice python=3.8\nconda init\nconda activate EmotiVoice\n```\n\n如果你不想使用conda环境，也可以省略该步骤，但要保证python版本为3.8\n\n\n#### 2、安装git-lfs\n\n如果是Ubuntu则执行\n\n```\nsudo apt update\nsudo apt install git\nsudo apt-get install git-lfs\n```\n\nCentOS则执行\n\n```\nsudo yum update\nsudo yum install git\nsudo yum install git-lfs\n```\n\n\n\n#### 3、克隆仓库\n\n```\ngit lfs install\ngit lfs clone https://github.com/netease-youdao/EmotiVoice.git\n```\n\n\n\n#### 4、安装依赖\n\n```\npip install torch torchaudio\npip install numpy numba scipy transformers soundfile yacs g2p_en jieba pypinyin pypinyin_dict\npython -m nltk.downloader \"averaged_perceptron_tagger_eng\"\n```\n\n\n\n<a id=\"step5\"></a>\n\n#### 5、下载预训练模型文件\n\n(1)首先进入项目文件夹\n\n```\ncd EmotiVoice\n```\n\n(2)执行下面命令\n\n```\ngit lfs clone https://huggingface.co/WangZeJun/simbert-base-chinese WangZeJun/simbert-base-chinese\n```\n\n或者\n\n```\ngit clone https://www.modelscope.cn/syq163/WangZeJun.git\n```\n\n上面两种下载方式二选一即可。\n\n(3)第三步下载ckpt模型\n\n```\ngit clone https://www.modelscope.cn/syq163/outputs.git\n```\n\n上面步骤完成后，项目文件夹内会多 `WangZeJun` 和 `outputs` 文件夹，下面是项目文件结构\n\n```\n├── Dockerfile\n├── EmotiVoice_UserAgreement_易魔声用户协议.pdf\n├── demo_page.py\n├── frontend.py\n├── frontend_cn.py\n├── frontend_en.py\n├── WangZeJun\n│   └── simbert-base-chinese\n│       ├── README.md\n│       ├── config.json\n│       ├── pytorch_model.bin\n│       └── vocab.txt\n├── outputs\n│   ├── README.md\n│   ├── configuration.json\n│   ├── prompt_tts_open_source_joint\n│   │   └── ckpt\n│   │       ├── do_00140000\n│   │       └── g_00140000\n│   └── style_encoder\n│       └── ckpt\n│           └── checkpoint_163431\n```\n\n\n\n#### 6、运行UI交互界面\n\n(1)安装streamlit\n\n```\npip install streamlit\n```\n\n(2)启动\n\n打开运行后显示的server地址，如何正常显示页面则部署完成。\n\n```\nstreamlit run demo_page.py --server.port 6006 --logger.level debug\n```\n\n\n\n#### 7、启动API服务\n\n安装依赖\n\n```\npip install fastapi pydub uvicorn[standard] pyrubberband\n```\n\n在6006端口启动服务(端口可根据自己的需求修改)\n\n```\nuvicorn openaiapi:app --reload --port 6006\n```\n\n接口文档地址：你的服务地址+`/docs`\n\n&emsp;\n\n#### 8、遇到错误\n\n**(1) 运行UI界面后，打开页面一直显示 \"Please wait...\" 或者显示一片空白**\n\n原因：\n\n这个错误可能是由于CORS（跨域资源共享）保护配置错误。\n\n解决方法：\n\n在启动时加上一个 `server.enableCORS=false` 参数，即使用下面命令启动程序\n\n```\nstreamlit run demo_page.py --server.port 6006 --logger.level debug --server.enableCORS=false\n```\n\n如果通过临时禁用 CORS 保护解决了问题，建议重新启用 CORS 保护并设置正确的 URL 和端口。\n\n&emsp;\n\n**(2) 运行报错 raise BadZipFile(\"File is not a zip file\") zipfile.BadZipFile: File is not a zip file**\n\n原因：\n\n这可能是由于缺少 `averaged_perceptron_tagger`  这个`nltk`中用于词性标注的一个包，它包含了一个基于平均感知器算法的词性标注器。如果你在代码中使用了这个标注器，但是没有预先下载对应的数据包，就会遇到错误，提示你缺少`averaged_perceptron_tagger.zip`文件。当然也有可能是缺少 `cmudict` CMU 发音词典数据包文件。\n\n正常来说，初次运行程序NLTK会自动下载使用的相关数据包，debug模式下运行会显示如下信息\n\n```\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n[nltk_data] Downloading package cmudict to /root/nltk_data...\n[nltk_data]   Unzipping corpora/cmudict.zip.\n```\n\n可能由于网络(需科学上网)等原因，没能自动下载成功，因此缺少相关文件导致加载报错。\n\n\n\n解决方法：重新下载缺少的数据包文件\n\n\n\n1)方法一\n\n创建一个 download.py文件，在其中编写如下代码\n\n```\nimport nltk\nprint(nltk.data.path)\nnltk.download('averaged_perceptron_tagger')\nnltk.download('cmudict')\n```\n\n保存并运行\n\n```\npython download.py\n```\n\n这将显示其文件索引位置，并自动下载 缺少的 `averaged_perceptron_tagger.zip`和 `cmudict.zip` 文件到/root/nltk_data目录下的子目录，下载完成后查看根目录下是否有`nltk_data`文件夹，并将其中的压缩包都解压。\n\n&emsp;\n\n2)方法二\n\n如果通过上面代码还是无法正常下载数据包 ，也可以打开以下地址手动搜索并下载压缩包文件(需科学上网)\n\n```\nhttps://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n```\n\n其中下面是`averaged_perceptron_tagger.zip` 和`cmudict.zip` 数据包文件的下载地址\n\n```\nhttps://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/taggers/averaged_perceptron_tagger.zip\nhttps://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/cmudict.zip\n```\n\n然后将该压缩包文件上传至(1)运行`python download.py`时打印显示的文件索引位置，如 `/root/nltk_data`  或者 `/root/miniconda3/envs/EmotiVoice/nltk_data` 等类似目录下，如果没有则创建一个，然后将zip压缩包解压。\n\n&emsp;\n\n解压后nltk_data目录结构应该是下面这样\n\n```\n├── nltk_data\n│   ├── corpora\n│   │   ├── cmudict\n│   │   │   ├── README\n│   │   │   └── cmudict\n│   │   └── cmudict.zip\n│   └── taggers\n│       ├── averaged_perceptron_tagger\n│       │   └── averaged_perceptron_tagger.pickle\n│       └── averaged_perceptron_tagger.zip\n```\n\n&emsp;\n\n**(3) 报错 AttributeError: 'NoneType' object has no attribute 'seek'.** \n\n原因：未找到模型文件\n\n解决方法：大概率是你未下载模型文件或者存放路径不正确，查看自己下载的模型文件是否存在，即outputs文件夹存放路径和里面的模型文件是否正确，正确结构可参考 [第五步](#step5) 中的项目结构。\n\n&emsp;\n\n**(4) 运行API服务出错 ImportError: cannot import name 'Doc' from 'typing_extensions'**\n\n原因：typing_extensions 版本问题\n\n解决方法：\n\n尝试将`typing_extensions`升级至最新版本，如果已经是最新版本，则适当降低版本，以下版本在`fastapi V0.104.1`测试正常。\n\n```\npip install typing_extensions==4.8.0 --force-reinstall\n```\n\n&emsp;\n\n**(5) 请求文本转语音接口时报错 500 Internal Server Error ，FileNotFoundError: [Errno 2] No such file or directory: 'ffmpeg'**\n\n原因：未安装ffmpeg\n\n解决方法：\n\n执行以下命令进行安装，如果是Ubuntu执行\n\n```\nsudo apt update\nsudo apt install ffmpeg\n```\n\nCentOS则执行\n\n```\nsudo yum install epel-release\nsudo yum install ffmpeg\n```\n\n安装完成后，你可以在终端中运行以下命令来验证\"ffmpeg\"是否成功安装：\n\n```\nffmpeg -version\n```\n\n如果安装成功，你将看到\"ffmpeg\"的版本信息。\n\n&emsp;\n"
        },
        {
          "name": "ROADMAP.md",
          "type": "blob",
          "size": 1.755859375,
          "content": "# EmotiVoice Roadmap\n\nThis roadmap is for EmotiVoice (易魔声), a project driven by the community. We value your feedback and suggestions on our future direction.\n\nPlease visit https://github.com/netease-youdao/EmotiVoice/issues on GitHub to submit your proposals.\nIf you are interested, feel free to volunteer for any tasks, even if they are not listed.\n\nThe plan is to finish 0.2 to 0.4 in Q4 2023.\n\n## EmotiVoice 0.4\n\n- [ ] Updated model with potentially improved quality.\n- [ ] First version of desktop application.\n- [ ] Support longer text.\n\n## EmotiVoice 0.3 (2023.12.13)\n\n- [x] Release [The EmotiVoice HTTP API](https://github.com/netease-youdao/EmotiVoice/wiki/HTTP-API) provided by [Zhiyun](https://mp.weixin.qq.com/s/_Fbj4TI4ifC6N7NFOUrqKQ).\n- [x] Release [Voice Cloning with your personal data](https://github.com/netease-youdao/EmotiVoice/wiki/Voice-Cloning-with-your-personal-data) along with [DataBaker Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/DataBaker) and [LJSpeech Recipe](https://github.com/netease-youdao/EmotiVoice/tree/main/data/LJspeech).\n- [x] Documentation: wiki page for hardware requirements. [#30](../../issues/30)\n\n## EmotiVoice 0.2 (2023.11.17)\n\n- [x] Support mixed Chinese and English input text. [#28](../../issues/28)\n- [x] Resolve bugs related to certain modal particles, to make it more robust. [#18](../../issues/18)\n- [x] Documentation: voice list wiki page\n- [x] Documentation: this roadmap.\n\n## EmotiVoice 0.1 (2023.11.10) first public version\n\n- [x] We offer a pretrained model with over 2000 voices, supporting both Chinese and English languages.\n- [x] You can perform inference using the command line interface. We also offer a user-friendly web demo for easy usage.\n- [x] For convenient deployment, we offer a Docker image.\n\n"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "cn2an",
          "type": "tree",
          "content": null
        },
        {
          "name": "cog.yaml",
          "type": "blob",
          "size": 0.75390625,
          "content": "# Configuration for Cog ⚙️\n# Reference: https://github.com/replicate/cog/blob/main/docs/yaml.md\n\nbuild:\n  gpu: true\n\n  # a list of ubuntu apt packages to install\n  # system_packages:\n  #   - \"libgl1-mesa-glx\"\n  #   - \"libglib2.0-0\"\n\n  python_version: \"3.8\"\n  python_packages:\n    - \"torch==2.0.1\"\n    - \"torchaudio==2.0.2\"\n    - \"g2p-en==2.1.0\"\n    - \"jieba==0.42.1\"\n    - \"numba==0.58.1\"\n    - \"numpy==1.24.4\"\n    - \"pypinyin==0.49.0\"\n    - \"scipy==1.10.1\"\n    - \"soundfile==0.12.1\"\n    - \"transformers==4.26.1\"\n    - \"yacs==0.1.8\"\n\n  run:\n    - curl -o /usr/local/bin/pget -L \"https://github.com/replicate/pget/releases/download/v0.0.3/pget\" && chmod +x /usr/local/bin/pget\n\n# predict.py defines how predictions are run on your model\npredict: \"predict.py:Predictor\"\n"
        },
        {
          "name": "config",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo_page.py",
          "type": "blob",
          "size": 6.4365234375,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport streamlit as st\nimport os, glob\nimport numpy as np\nfrom yacs import config as CONFIG\nimport torch\nimport re\n\nfrom frontend import g2p_cn_en, ROOT_DIR, read_lexicon, G2p\nfrom config.joint.config import Config\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\n\nimport base64\nfrom pathlib import Path\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nMAX_WAV_VALUE = 32768.0\n\nconfig = Config()\n\ndef create_download_link():\n    pdf_path = Path(\"EmotiVoice_UserAgreement_易魔声用户协议.pdf\")\n    base64_pdf = base64.b64encode(pdf_path.read_bytes()).decode(\"utf-8\")  # val looks like b'...'\n    return f'<a href=\"data:application/octet-stream;base64,{base64_pdf}\" download=\"EmotiVoice_UserAgreement_易魔声用户协议.pdf.pdf\">EmotiVoice_UserAgreement_易魔声用户协议.pdf</a>'\n\nhtml=create_download_link()\n\nst.set_page_config(\n    page_title=\"demo page\",\n    page_icon=\"📕\",\n)\nst.write(\"# Text-To-Speech\")\nst.markdown(f\"\"\"\n### How to use:\n         \n- Simply select a **Speaker ID**, type in the **text** you want to convert and the emotion **Prompt**, like a single word or even a sentence. Then click on the **Synthesize** button below to start voice synthesis.\n\n- You can download the audio by clicking on the vertical three points next to the displayed audio widget.\n\n- For more information on **'Speaker ID'**, please consult the [EmotiVoice voice wiki page](https://github.com/netease-youdao/EmotiVoice/tree/main/data/youdao/text)\n\n- This interactive demo page is provided under the {html} file. The audio is synthesized by AI. 音频由AI合成，仅供参考。\n\n\"\"\", unsafe_allow_html=True)\n\ndef scan_checkpoint(cp_dir, prefix, c=8):\n    pattern = os.path.join(cp_dir, prefix + '?'*c)\n    cp_list = glob.glob(pattern)\n    if len(cp_list) == 0:\n        return None\n    return sorted(cp_list)[-1]\n\n@st.cache_resource\ndef get_models():\n    \n    am_checkpoint_path = scan_checkpoint(f'{config.output_directory}/prompt_tts_open_source_joint/ckpt', 'g_')\n\n    style_encoder_checkpoint_path = scan_checkpoint(f'{config.output_directory}/style_encoder/ckpt', 'checkpoint_', 6)#f'{config.output_directory}/style_encoder/ckpt/checkpoint_163431' \n\n    with open(config.model_config_path, 'r') as fin:\n        conf = CONFIG.load_cfg(fin)\n    \n    conf.n_vocab = config.n_symbols\n    conf.n_speaker = config.speaker_n_labels\n\n    style_encoder = StyleEncoder(config)\n    model_CKPT = torch.load(style_encoder_checkpoint_path, map_location=\"cpu\")\n    model_ckpt = {}\n    for key, value in model_CKPT['model'].items():\n        new_key = key[7:]\n        model_ckpt[new_key] = value\n    style_encoder.load_state_dict(model_ckpt, strict=False)\n    generator = JETSGenerator(conf).to(DEVICE)\n\n    model_CKPT = torch.load(am_checkpoint_path, map_location=DEVICE)\n    generator.load_state_dict(model_CKPT['generator'])\n    generator.eval()\n\n    tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n\n    with open(config.token_list_path, 'r') as f:\n        token2id = {t.strip():idx for idx, t, in enumerate(f.readlines())}\n\n    with open(config.speaker2id_path, encoding='utf-8') as f:\n        speaker2id = {t.strip():idx for idx, t in enumerate(f.readlines())}\n\n\n    return (style_encoder, generator, tokenizer, token2id, speaker2id)\n\ndef get_style_embedding(prompt, tokenizer, style_encoder):\n    prompt = tokenizer([prompt], return_tensors=\"pt\")\n    input_ids = prompt[\"input_ids\"]\n    token_type_ids = prompt[\"token_type_ids\"]\n    attention_mask = prompt[\"attention_mask\"]\n    with torch.no_grad():\n        output = style_encoder(\n        input_ids=input_ids,\n        token_type_ids=token_type_ids,\n        attention_mask=attention_mask,\n    )\n    style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n    return style_embedding\n\ndef tts(name, text, prompt, content, speaker, models):\n    (style_encoder, generator, tokenizer, token2id, speaker2id)=models\n    \n\n    style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n    content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n\n    speaker = speaker2id[speaker]\n\n    text_int = [token2id[ph] for ph in text.split()]\n    \n    sequence = torch.from_numpy(np.array(text_int)).to(DEVICE).long().unsqueeze(0)\n    sequence_len = torch.from_numpy(np.array([len(text_int)])).to(DEVICE)\n    style_embedding = torch.from_numpy(style_embedding).to(DEVICE).unsqueeze(0)\n    content_embedding = torch.from_numpy(content_embedding).to(DEVICE).unsqueeze(0)\n    speaker = torch.from_numpy(np.array([speaker])).to(DEVICE)\n\n    with torch.no_grad():\n\n        infer_output = generator(\n                inputs_ling=sequence,\n                inputs_style_embedding=style_embedding,\n                input_lengths=sequence_len,\n                inputs_content_embedding=content_embedding,\n                inputs_speaker=speaker,\n                alpha=1.0\n            )\n\n    audio = infer_output[\"wav_predictions\"].squeeze()* MAX_WAV_VALUE\n    audio = audio.cpu().numpy().astype('int16')\n\n    return audio\n\nspeakers = config.speakers\nmodels = get_models()\nlexicon = read_lexicon(f\"{ROOT_DIR}/lexicon/librispeech-lexicon.txt\")\ng2p = G2p()\n\ndef new_line(i):\n    col1, col2, col3, col4 = st.columns([1.5, 1.5, 3.5, 1.3])\n    with col1:\n        speaker=st.selectbox(\"Speaker ID (说话人)\", speakers, key=f\"{i}_speaker\")\n    with col2:\n        prompt=st.text_input(\"Prompt (开心/悲伤)\", \"\", key=f\"{i}_prompt\")\n    with col3:\n        content=st.text_input(\"Text to be synthesized into speech (合成文本)\", \"合成文本\", key=f\"{i}_text\")\n    with col4:\n        lang=st.selectbox(\"Language (语言)\", [\"zh_us\"], key=f\"{i}_lang\")\n\n    flag = st.button(f\"Synthesize (合成)\", key=f\"{i}_button1\")\n    if flag:\n        text =  g2p_cn_en(content, g2p, lexicon)\n        path = tts(i, text, prompt, content, speaker, models)\n        st.audio(path, sample_rate=config.sampling_rate)\n\n\n\nnew_line(0)\n"
        },
        {
          "name": "demo_page_databaker.py",
          "type": "blob",
          "size": 6.294921875,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport streamlit as st\nimport os, glob\nimport numpy as np\nfrom yacs import config as CONFIG\nimport torch\nimport re\n\nfrom frontend import g2p_cn_en, ROOT_DIR, read_lexicon, G2p\nfrom exp.DataBaker.config.config import Config\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\n\nimport base64\nfrom pathlib import Path\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nMAX_WAV_VALUE = 32768.0\n\nconfig = Config()\n\ndef create_download_link():\n    pdf_path = Path(\"EmotiVoice_UserAgreement_易魔声用户协议.pdf\")\n    base64_pdf = base64.b64encode(pdf_path.read_bytes()).decode(\"utf-8\")  # val looks like b'...'\n    return f'<a href=\"data:application/octet-stream;base64,{base64_pdf}\" download=\"EmotiVoice_UserAgreement_易魔声用户协议.pdf.pdf\">EmotiVoice_UserAgreement_易魔声用户协议.pdf</a>'\n\nhtml=create_download_link()\n\nst.set_page_config(\n    page_title=\"demo page\",\n    page_icon=\"📕\",\n)\nst.write(\"# Text-To-Speech\")\nst.markdown(f\"\"\"\n### How to use:\n         \n- Simply select a **Speaker ID**, type in the **text** you want to convert and the emotion **Prompt**, like a single word or even a sentence. Then click on the **Synthesize** button below to start voice synthesis.\n\n- You can download the audio by clicking on the vertical three points next to the displayed audio widget.\n\n- For more information on **'Speaker ID'**, please consult the [EmotiVoice voice wiki page](https://github.com/netease-youdao/EmotiVoice/tree/main/data/youdao/text)\n\n- This interactive demo page is provided under the {html} file. The audio is synthesized by AI. 音频由AI合成，仅供参考。\n\n\"\"\", unsafe_allow_html=True)\n\ndef scan_checkpoint(cp_dir, prefix, c=8):\n    pattern = os.path.join(cp_dir, prefix + '?'*c)\n    cp_list = glob.glob(pattern)\n    if len(cp_list) == 0:\n        return None\n    return sorted(cp_list)[-1]\n\n@st.cache_resource\ndef get_models():\n    \n    am_checkpoint_path = scan_checkpoint(f'{config.output_directory}/ckpt', 'g_')\n\n    style_encoder_checkpoint_path = config.style_encoder_ckpt\n\n    with open(config.model_config_path, 'r') as fin:\n        conf = CONFIG.load_cfg(fin)\n    \n    conf.n_vocab = config.n_symbols\n    conf.n_speaker = config.speaker_n_labels\n\n    style_encoder = StyleEncoder(config)\n    model_CKPT = torch.load(style_encoder_checkpoint_path, map_location=\"cpu\")\n    model_ckpt = {}\n    for key, value in model_CKPT['model'].items():\n        new_key = key[7:]\n        model_ckpt[new_key] = value\n    style_encoder.load_state_dict(model_ckpt, strict=False)\n    generator = JETSGenerator(conf).to(DEVICE)\n\n    model_CKPT = torch.load(am_checkpoint_path, map_location=DEVICE)\n    generator.load_state_dict(model_CKPT['generator'])\n    generator.eval()\n\n    tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n\n    with open(config.token_list_path, 'r') as f:\n        token2id = {t.strip():idx for idx, t, in enumerate(f.readlines())}\n\n    with open(config.speaker2id_path, encoding='utf-8') as f:\n        speaker2id = {t.strip():idx for idx, t in enumerate(f.readlines())}\n\n\n    return (style_encoder, generator, tokenizer, token2id, speaker2id)\n\ndef get_style_embedding(prompt, tokenizer, style_encoder):\n    prompt = tokenizer([prompt], return_tensors=\"pt\")\n    input_ids = prompt[\"input_ids\"]\n    token_type_ids = prompt[\"token_type_ids\"]\n    attention_mask = prompt[\"attention_mask\"]\n    with torch.no_grad():\n        output = style_encoder(\n        input_ids=input_ids,\n        token_type_ids=token_type_ids,\n        attention_mask=attention_mask,\n    )\n    style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n    return style_embedding\n\ndef tts(name, text, prompt, content, speaker, models):\n    (style_encoder, generator, tokenizer, token2id, speaker2id)=models\n    \n\n    style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n    content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n\n    speaker = speaker2id[speaker]\n\n    text_int = [token2id[ph] for ph in text.split()]\n    \n    sequence = torch.from_numpy(np.array(text_int)).to(DEVICE).long().unsqueeze(0)\n    sequence_len = torch.from_numpy(np.array([len(text_int)])).to(DEVICE)\n    style_embedding = torch.from_numpy(style_embedding).to(DEVICE).unsqueeze(0)\n    content_embedding = torch.from_numpy(content_embedding).to(DEVICE).unsqueeze(0)\n    speaker = torch.from_numpy(np.array([speaker])).to(DEVICE)\n\n    with torch.no_grad():\n\n        infer_output = generator(\n                inputs_ling=sequence,\n                inputs_style_embedding=style_embedding,\n                input_lengths=sequence_len,\n                inputs_content_embedding=content_embedding,\n                inputs_speaker=speaker,\n                alpha=1.0\n            )\n\n    audio = infer_output[\"wav_predictions\"].squeeze()* MAX_WAV_VALUE\n    audio = audio.cpu().numpy().astype('int16')\n\n    return audio\n\nspeakers = config.speakers\nmodels = get_models()\nlexicon = read_lexicon(f\"{ROOT_DIR}/lexicon/librispeech-lexicon.txt\")\ng2p = G2p()\n\ndef new_line(i):\n    col1, col2, col3, col4 = st.columns([1.5, 1.5, 3.5, 1.3])\n    with col1:\n        speaker=st.selectbox(\"Speaker ID (说话人)\", speakers, key=f\"{i}_speaker\")\n    with col2:\n        prompt=st.text_input(\"Prompt (开心/悲伤)\", \"\", key=f\"{i}_prompt\")\n    with col3:\n        content=st.text_input(\"Text to be synthesized into speech (合成文本)\", \"合成文本\", key=f\"{i}_text\")\n    with col4:\n        lang=st.selectbox(\"Language (语言)\", [\"zh_us\"], key=f\"{i}_lang\")\n\n    flag = st.button(f\"Synthesize (合成)\", key=f\"{i}_button1\")\n    if flag:\n        text =  g2p_cn_en(content, g2p, lexicon)\n        path = tts(i, text, prompt, content, speaker, models)\n        st.audio(path, sample_rate=config.sampling_rate)\n\n\n\nnew_line(0)\n"
        },
        {
          "name": "frontend.py",
          "type": "blob",
          "size": 3.0078125,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nfrom frontend_cn import g2p_cn, re_digits, tn_chinese\nfrom frontend_en import ROOT_DIR, read_lexicon, G2p, get_eng_phoneme\n\n# Thanks to GuGCoCo and PatroxGaurab for identifying the issue: \n# the results differ between frontend.py and frontend_en.py. Here's a quick fix.\n#re_english_word = re.compile('([a-z\\-\\.\\'\\s,;\\:\\!\\?]+|\\d+[\\d\\.]*)', re.I)\nre_english_word = re.compile('([^\\u4e00-\\u9fa5]+|[ \\u3002\\uff0c\\uff1f\\uff01\\uff1b\\uff1a\\u201c\\u201d\\u2018\\u2019\\u300a\\u300b\\u3008\\u3009\\u3010\\u3011\\u300e\\u300f\\u2014\\u2026\\u3001\\uff08\\uff09\\u4e00-\\u9fa5]+)', re.I)\ndef g2p_cn_en(text, g2p, lexicon):\n    # Our policy dictates that if the text contains Chinese, digits are to be converted into Chinese.\n    text=tn_chinese(text)\n    parts = re_english_word.split(text)\n    parts=list(filter(None, parts))\n    tts_text = [\"<sos/eos>\"]\n    chartype = ''\n    text_contains_chinese = contains_chinese(text)\n    for part in parts:\n        if part == ' ' or part == '': continue\n        if re_digits.match(part) and (text_contains_chinese or chartype == '') or contains_chinese(part):\n            if chartype == 'en':\n                tts_text.append('eng_cn_sp')\n            phoneme = g2p_cn(part).split()[1:-1]\n            chartype = 'cn'\n        elif re_english_word.match(part):\n            if chartype == 'cn':\n                if \"sp\" in tts_text[-1]:\n                    \"\"\n                else:\n                    tts_text.append('cn_eng_sp')\n            phoneme = get_eng_phoneme(part, g2p, lexicon, False).split()\n            if not phoneme :\n                # tts_text.pop()\n                continue\n            else:\n                chartype = 'en'\n        else:\n            continue\n        tts_text.extend( phoneme )\n\n    tts_text=\" \".join(tts_text).split()\n    if \"sp\" in tts_text[-1]:\n        tts_text.pop()\n    tts_text.append(\"<sos/eos>\")\n\n    return \" \".join(tts_text)\n\ndef contains_chinese(text):\n    pattern = re.compile(r'[\\u4e00-\\u9fa5]')\n    match = re.search(pattern, text)\n    return match is not None\n\n\nif __name__ == \"__main__\":\n    import sys\n    from os.path import isfile\n    lexicon = read_lexicon(f\"{ROOT_DIR}/lexicon/librispeech-lexicon.txt\")\n\n    g2p = G2p()\n    if len(sys.argv) < 2:\n        print(\"Usage: python %s <text>\" % sys.argv[0])\n        exit()\n    text_file = sys.argv[1]\n    if isfile(text_file):\n        fp = open(text_file, 'r')\n        for line in fp:\n            phoneme = g2p_cn_en(line.rstrip(), g2p, lexicon)\n            print(phoneme)\n        fp.close()\n"
        },
        {
          "name": "frontend_cn.py",
          "type": "blob",
          "size": 4.1767578125,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nfrom pypinyin import pinyin, lazy_pinyin, Style\nimport jieba\nimport string\nfrom cn2an.an2cn import An2Cn\nfrom pypinyin_dict.phrase_pinyin_data import cc_cedict\ncc_cedict.load()\nre_special_pinyin = re.compile(r'^(n|ng|m)$')\ndef split_py(py):\n    tone = py[-1]\n    py = py[:-1]\n    sm = \"\"\n    ym = \"\"\n    suf_r = \"\"\n    if re_special_pinyin.match(py):\n        py = 'e' + py\n    if py[-1] == 'r':\n        suf_r = 'r'\n        py = py[:-1]\n    if py == 'zi' or py == 'ci' or py == 'si' or py == 'ri':\n        sm = py[:1]\n        ym = \"ii\"\n    elif py == 'zhi' or py == 'chi' or py == 'shi':\n        sm = py[:2]\n        ym = \"iii\"\n    elif py == 'ya' or py == 'yan' or py == 'yang' or py == 'yao' or py == 'ye' or py == 'yong' or py == 'you':\n        sm = \"\"\n        ym = 'i' + py[1:]\n    elif py == 'yi' or py == 'yin' or py == 'ying':\n        sm = \"\"\n        ym = py[1:]\n    elif py == 'yu' or py == 'yv' or py == 'yuan' or py == 'yvan' or py == 'yue ' or py == 'yve' or py == 'yun' or py == 'yvn':\n        sm = \"\"\n        ym = 'v' + py[2:]\n    elif py == 'wu':\n        sm = \"\"\n        ym = \"u\"\n    elif py[0] == 'w':\n        sm = \"\"\n        ym = \"u\" + py[1:]\n    elif len(py) >= 2 and (py[0] == 'j' or py[0] == 'q' or py[0] == 'x') and py[1] == 'u':\n        sm = py[0]\n        ym = 'v' + py[2:]\n    else:\n        seg_pos = re.search('a|e|i|o|u|v', py)\n        sm = py[:seg_pos.start()]\n        ym = py[seg_pos.start():]\n        if ym == 'ui':\n            ym = 'uei'\n        elif ym == 'iu':\n            ym = 'iou'\n        elif ym == 'un':\n            ym = 'uen'\n        elif ym == 'ue':\n            ym = 've'\n    ym += suf_r + tone\n    return sm, ym\n\n\nchinese_punctuation_pattern = r'[\\u3002\\uff0c\\uff1f\\uff01\\uff1b\\uff1a\\u201c\\u201d\\u2018\\u2019\\u300a\\u300b\\u3008\\u3009\\u3010\\u3011\\u300e\\u300f\\u2014\\u2026\\u3001\\uff08\\uff09]'\n\n\ndef has_chinese_punctuation(text):\n    match = re.search(chinese_punctuation_pattern, text)\n    return match is not None\ndef has_english_punctuation(text):\n    return text in string.punctuation\n\n# with thanks to KimigaiiWuyi in https://github.com/netease-youdao/EmotiVoice/pull/17.\n# Updated on November 20, 2023: EmotiVoice now incorporates cn2an (https://github.com/Ailln/cn2an) for number processing.\nre_digits = re.compile('(\\d[\\d\\.]*)')\ndef number_to_chinese(number):\n    an2cn = An2Cn()\n    result = an2cn.an2cn(number)\n\n    return result\n\ndef tn_chinese(text):\n    parts = re_digits.split(text)\n    words = []\n    for part in parts:\n        if re_digits.match(part):\n            words.append(number_to_chinese(part))\n        else:\n            words.append(part)\n    return ''.join(words)\n\ndef g2p_cn(text):\n    res_text=[\"<sos/eos>\"]\n    seg_list = jieba.cut(text)\n    for seg in seg_list:\n        if seg == \" \": continue\n        seg_tn = tn_chinese(seg)\n        py =[_py[0] for _py in pinyin(seg_tn, style=Style.TONE3,neutral_tone_with_five=True)]\n\n        if any([has_chinese_punctuation(_py) for _py in py])  or any([has_english_punctuation(_py) for _py in py]):\n            res_text.pop()\n            res_text.append(\"sp3\")\n        else:\n            \n            py = [\" \".join(split_py(_py)) for _py in py]\n            \n            res_text.append(\" sp0 \".join(py))\n            res_text.append(\"sp1\")\n    #res_text.pop()\n    res_text.append(\"<sos/eos>\")\n    return \" \".join(res_text)\n\nif __name__ == \"__main__\":\n    import sys\n    from os.path import isfile\n    if len(sys.argv) < 2:\n        print(\"Usage: python %s <text>\" % sys.argv[0])\n        exit()\n    text_file = sys.argv[1]\n    if isfile(text_file):\n        fp = open(text_file, 'r')\n        for line in fp:\n            phoneme=g2p_cn(line.rstrip())\n            print(phoneme)\n        fp.close()\n"
        },
        {
          "name": "frontend_en.py",
          "type": "blob",
          "size": 2.796875,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nimport argparse\nfrom string import punctuation\nimport numpy as np\n\nfrom g2p_en import G2p\n\nimport os\n\n\nROOT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n\ndef read_lexicon(lex_path):\n    lexicon = {}\n    with open(lex_path) as f:\n        for line in f:\n            temp = re.split(r\"\\s+\", line.strip(\"\\n\"))\n            word = temp[0]\n            phones = temp[1:]\n            if word.lower() not in lexicon:\n                lexicon[word.lower()] = phones\n    return lexicon\n\ndef get_eng_phoneme(text, g2p, lexicon, pad_sos_eos=True):\n    \"\"\"\n    english g2p\n    \"\"\"\n    filters = {\",\", \" \", \"'\"}\n    phones = []\n    words = list(filter(lambda x: x not in {\"\", \" \"}, re.split(r\"([,;.\\-\\?\\!\\s+])\", text)))\n\n    for w in words:\n        if w.lower() in lexicon:\n            \n            for ph in lexicon[w.lower()]:\n                if ph not in filters:\n                    phones += [\"[\" + ph + \"]\"]\n\n            if \"sp\" not in phones[-1]:\n                phones += [\"engsp1\"]\n        else:\n            phone=g2p(w)\n            if not phone:\n                continue\n\n            if phone[0].isalnum():\n                \n                for ph in phone:\n                    if ph not in filters:\n                        phones += [\"[\" + ph + \"]\"]\n                    if ph == \" \" and \"sp\" not in phones[-1]:\n                        phones += [\"engsp1\"]\n            elif phone == \" \":\n                continue\n            elif phones:\n                phones.pop() # pop engsp1\n                phones.append(\"engsp4\")\n    if phones and \"engsp\" in phones[-1]:\n        phones.pop()\n\n    # mark = \".\" if text[-1] != \"?\" else \"?\"\n    if pad_sos_eos:\n        phones = [\"<sos/eos>\"] + phones + [\"<sos/eos>\"]\n    return \" \".join(phones)\n    \n\nif __name__ == \"__main__\":\n    lexicon = read_lexicon(f\"{ROOT_DIR}/lexicon/librispeech-lexicon.txt\")\n    g2p = G2p()\n    phonemes= get_eng_phoneme(\"Happy New Year\", g2p, lexicon)\n    import sys\n    from os.path import isfile\n    if len(sys.argv) < 2:\n        print(\"Usage: python %s <text>\" % sys.argv[0])\n        exit()\n    text_file = sys.argv[1]\n    if isfile(text_file):\n        fp = open(text_file, 'r')\n        for line in fp:\n            phoneme=get_eng_phoneme(line.rstrip(), g2p, lexicon)\n            print(phoneme)\n        fp.close()"
        },
        {
          "name": "inference_am_vocoder_exp.py",
          "type": "blob",
          "size": 5.943359375,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\nimport os, sys, warnings, torch, glob, argparse\nimport numpy as np\nfrom models.hifigan.get_vocoder import MAX_WAV_VALUE\nimport soundfile as sf\nfrom yacs import config as CONFIG\nfrom tqdm import tqdm\n\ndef get_style_embedding(prompt, tokenizer, style_encoder):\n    prompt = tokenizer([prompt], return_tensors=\"pt\")\n    input_ids = prompt[\"input_ids\"]\n    token_type_ids = prompt[\"token_type_ids\"]\n    attention_mask = prompt[\"attention_mask\"]\n\n    with torch.no_grad():\n        output = style_encoder(\n        input_ids=input_ids,\n        token_type_ids=token_type_ids,\n        attention_mask=attention_mask,\n    )\n    style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n    return style_embedding\n\ndef main(args, config):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    root_path = os.path.join(config.output_directory)\n    ckpt_path = os.path.join(root_path,  \"ckpt\")\n    files = os.listdir(ckpt_path)\n    \n    for file in files:\n        if args.checkpoint:\n            if file != args.checkpoint:\n                continue\n\n        checkpoint_path = os.path.join(ckpt_path, file)\n\n        with open(config.model_config_path, 'r') as fin:\n            conf = CONFIG.load_cfg(fin)\n        \n     \n        conf.n_vocab = config.n_symbols\n        conf.n_speaker = config.speaker_n_labels\n\n        style_encoder = StyleEncoder(config)\n        model_CKPT = torch.load(config.style_encoder_ckpt, map_location=\"cpu\")\n        model_ckpt = {}\n        for key, value in model_CKPT['model'].items():\n            new_key = key[7:]\n            model_ckpt[new_key] = value\n        style_encoder.load_state_dict(model_ckpt, strict=False)\n\n\n\n        generator = JETSGenerator(conf).to(device)\n\n        model_CKPT = torch.load(checkpoint_path, map_location=device)\n        generator.load_state_dict(model_CKPT['generator'])\n        generator.eval()\n\n        with open(config.token_list_path, 'r') as f:\n            token2id = {t.strip():idx for idx, t, in enumerate(f.readlines())}\n\n        with open(config.speaker2id_path, encoding='utf-8') as f:\n            speaker2id = {t.strip():idx for idx, t in enumerate(f.readlines())}\n\n\n        tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n        \n        text_path = args.test_file\n\n   \n        if os.path.exists(root_path + \"/test_audio/audio/\" +f\"{file}/\"):\n            r = glob.glob(root_path + \"/test_audio/audio/\" +f\"{file}/*\")\n            for j in r:\n                os.remove(j)\n        texts = []\n        prompts = []\n        speakers = []\n        contents = []\n        with open(text_path, \"r\") as f:\n            for line in f:\n                line = line.strip().split(\"|\")\n                speakers.append(line[0])\n                prompts.append(line[1])\n                texts.append(line[2].split())\n                contents.append(line[3])\n                \n        for i, (speaker, prompt, text, content) in enumerate(tqdm(zip(speakers, prompts, texts, contents))):\n\n            style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n            content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n\n            if speaker not in speaker2id:\n                continue\n            speaker = speaker2id[speaker]\n\n            text_int = [token2id[ph] for ph in text]\n            \n            sequence = torch.from_numpy(np.array(text_int)).to(device).long().unsqueeze(0)\n            sequence_len = torch.from_numpy(np.array([len(text_int)])).to(device)\n            style_embedding = torch.from_numpy(style_embedding).to(device).unsqueeze(0)\n            content_embedding = torch.from_numpy(content_embedding).to(device).unsqueeze(0)\n            speaker = torch.from_numpy(np.array([speaker])).to(device)\n            with torch.no_grad():\n\n                infer_output = generator(\n                        inputs_ling=sequence,\n                        inputs_style_embedding=style_embedding,\n                        input_lengths=sequence_len,\n                        inputs_content_embedding=content_embedding,\n                        inputs_speaker=speaker,\n                        alpha=1.0\n                    )\n                audio = infer_output[\"wav_predictions\"].squeeze()* MAX_WAV_VALUE\n                audio = audio.cpu().numpy().astype('int16')\n                if not os.path.exists(root_path + \"/test_audio/audio/\" +f\"{file}/\"):\n                    os.makedirs(root_path + \"/test_audio/audio/\" +f\"{file}/\", exist_ok=True)\n                sf.write(file=root_path + \"/test_audio/audio/\" +f\"{file}/{i+1}.wav\", data=audio, samplerate=config.sampling_rate) #h.sampling_rate\n\n\n\n\n\n\nif __name__ == '__main__':\n    print(\"run!\")\n    p = argparse.ArgumentParser()\n    p.add_argument(\"-c\", \"--config_folder\", type=str, required=True)\n    p.add_argument(\"--checkpoint\", type=str, required=False, default='', help='inference specific checkpoint, e.g --checkpoint checkpoint_230000')\n    p.add_argument('-t', '--test_file', type=str, required=True, help='the absolute path of test file that is going to inference')\n\n    args = p.parse_args() \n    ##################################################\n    sys.path.append(os.path.dirname(os.path.abspath(\"__file__\")) + \"/\" + args.config_folder)\n\n    from config import Config\n    config = Config()\n    ##################################################\n    main(args, config)\n\n\n"
        },
        {
          "name": "inference_am_vocoder_joint.py",
          "type": "blob",
          "size": 6.0166015625,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\nimport os, sys, warnings, torch, glob, argparse\nimport numpy as np\nfrom models.hifigan.get_vocoder import MAX_WAV_VALUE\nimport soundfile as sf\nfrom yacs import config as CONFIG\nfrom tqdm import tqdm\n\ndef get_style_embedding(prompt, tokenizer, style_encoder):\n    prompt = tokenizer([prompt], return_tensors=\"pt\")\n    input_ids = prompt[\"input_ids\"]\n    token_type_ids = prompt[\"token_type_ids\"]\n    attention_mask = prompt[\"attention_mask\"]\n\n    with torch.no_grad():\n        output = style_encoder(\n        input_ids=input_ids,\n        token_type_ids=token_type_ids,\n        attention_mask=attention_mask,\n    )\n    style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n    return style_embedding\n\ndef main(args, config):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    root_path = os.path.join(config.output_directory, args.logdir)\n    ckpt_path = os.path.join(root_path,  \"ckpt\")\n    files = os.listdir(ckpt_path)\n    \n    for file in files:\n        if args.checkpoint:\n            if file != args.checkpoint:\n                continue\n\n        checkpoint_path = os.path.join(ckpt_path, file)\n\n        with open(config.model_config_path, 'r') as fin:\n            conf = CONFIG.load_cfg(fin)\n        \n     \n        conf.n_vocab = config.n_symbols\n        conf.n_speaker = config.speaker_n_labels\n\n        style_encoder = StyleEncoder(config)\n        model_CKPT = torch.load(config.style_encoder_ckpt, map_location=\"cpu\")\n        model_ckpt = {}\n        for key, value in model_CKPT['model'].items():\n            new_key = key[7:]\n            model_ckpt[new_key] = value\n        style_encoder.load_state_dict(model_ckpt, strict=False)\n\n\n\n        generator = JETSGenerator(conf).to(device)\n\n        model_CKPT = torch.load(checkpoint_path, map_location=device)\n        generator.load_state_dict(model_CKPT['generator'])\n        generator.eval()\n\n        with open(config.token_list_path, 'r') as f:\n            token2id = {t.strip():idx for idx, t, in enumerate(f.readlines())}\n\n        with open(config.speaker2id_path, encoding='utf-8') as f:\n            speaker2id = {t.strip():idx for idx, t in enumerate(f.readlines())}\n\n\n        tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n        \n        text_path = args.test_file\n\n   \n        if os.path.exists(root_path + \"/test_audio/audio/\" +f\"{file}/\"):\n            r = glob.glob(root_path + \"/test_audio/audio/\" +f\"{file}/*\")\n            for j in r:\n                os.remove(j)\n        texts = []\n        prompts = []\n        speakers = []\n        contents = []\n        with open(text_path, \"r\") as f:\n            for line in f:\n                line = line.strip().split(\"|\")\n                speakers.append(line[0])\n                prompts.append(line[1])\n                texts.append(line[2].split())\n                contents.append(line[3])\n                \n        for i, (speaker, prompt, text, content) in enumerate(tqdm(zip(speakers, prompts, texts, contents))):\n\n            style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n            content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n\n            if speaker not in speaker2id:\n                continue\n            speaker = speaker2id[speaker]\n\n            text_int = [token2id[ph] for ph in text]\n            \n            sequence = torch.from_numpy(np.array(text_int)).to(device).long().unsqueeze(0)\n            sequence_len = torch.from_numpy(np.array([len(text_int)])).to(device)\n            style_embedding = torch.from_numpy(style_embedding).to(device).unsqueeze(0)\n            content_embedding = torch.from_numpy(content_embedding).to(device).unsqueeze(0)\n            speaker = torch.from_numpy(np.array([speaker])).to(device)\n            with torch.no_grad():\n\n                infer_output = generator(\n                        inputs_ling=sequence,\n                        inputs_style_embedding=style_embedding,\n                        input_lengths=sequence_len,\n                        inputs_content_embedding=content_embedding,\n                        inputs_speaker=speaker,\n                        alpha=1.0\n                    )\n                audio = infer_output[\"wav_predictions\"].squeeze()* MAX_WAV_VALUE\n                audio = audio.cpu().numpy().astype('int16')\n                if not os.path.exists(root_path + \"/test_audio/audio/\" +f\"{file}/\"):\n                    os.makedirs(root_path + \"/test_audio/audio/\" +f\"{file}/\", exist_ok=True)\n                sf.write(file=root_path + \"/test_audio/audio/\" +f\"{file}/{i+1}.wav\", data=audio, samplerate=config.sampling_rate) #h.sampling_rate\n\n\n\n\n\n\nif __name__ == '__main__':\n    print(\"run!\")\n    p = argparse.ArgumentParser()\n    p.add_argument('-d', '--logdir', type=str, required=True)\n    p.add_argument(\"-c\", \"--config_folder\", type=str, required=True)\n    p.add_argument(\"--checkpoint\", type=str, required=False, default='', help='inference specific checkpoint, e.g --checkpoint checkpoint_230000')\n    p.add_argument('-t', '--test_file', type=str, required=True, help='the absolute path of test file that is going to inference')\n\n    args = p.parse_args() \n    ##################################################\n    sys.path.append(os.path.dirname(os.path.abspath(\"__file__\")) + \"/\" + args.config_folder)\n\n    from config import Config\n    config = Config()\n    ##################################################\n    main(args, config)\n\n\n"
        },
        {
          "name": "inference_tts.py",
          "type": "blob",
          "size": 8.4033203125,
          "content": "# Copyright 2023, YOUDAO\n#           2024, Du Jing(thuduj12@163.com)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport torch\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\nimport os, sys, torch, argparse\nimport numpy as np\nfrom models.hifigan.get_vocoder import MAX_WAV_VALUE\nimport soundfile as sf\nfrom yacs import config as CONFIG\nfrom tqdm import tqdm\nfrom frontend import g2p_cn_en\nfrom frontend_en import ROOT_DIR, read_lexicon, G2p\n\n\ndef get_style_embedding(prompt, tokenizer, style_encoder):\n    prompt = tokenizer([prompt], return_tensors=\"pt\")\n    input_ids = prompt[\"input_ids\"]\n    token_type_ids = prompt[\"token_type_ids\"]\n    attention_mask = prompt[\"attention_mask\"]\n\n    with torch.no_grad():\n        output = style_encoder(\n        input_ids=input_ids,\n        token_type_ids=token_type_ids,\n        attention_mask=attention_mask,\n    )\n    style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n    return style_embedding\n\ndef main(args, config, gpu_id, start_idx, chunk_num):\n    device = torch.device(\n        f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n    root_path = os.path.join(config.output_directory, args.logdir)\n    ckpt_path = os.path.join(root_path,  \"ckpt\")\n    checkpoint_path = os.path.join(ckpt_path, args.checkpoint)\n\n    output_dir = args.output_dir\n    if output_dir is None:\n        output_dir = os.path.join(root_path, 'audio')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    with open(config.model_config_path, 'r') as fin:\n        conf = CONFIG.load_cfg(fin)\n\n    conf.n_vocab = config.n_symbols\n    conf.n_speaker = config.speaker_n_labels\n\n    style_encoder = StyleEncoder(config)\n    model_CKPT = torch.load(config.style_encoder_ckpt, map_location=device)\n    model_ckpt = {}\n    for key, value in model_CKPT['model'].items():\n        new_key = key[7:]\n        model_ckpt[new_key] = value\n    style_encoder.load_state_dict(model_ckpt, strict=False)\n\n    generator = JETSGenerator(conf).to(device)\n    model_CKPT = torch.load(checkpoint_path, map_location=device)\n    generator.load_state_dict(model_CKPT['generator'])\n    generator.eval()\n\n    with open(config.token_list_path, 'r') as f:\n        token2id = {t.strip():idx for idx, t, in enumerate(f.readlines())}\n\n    with open(config.speaker2id_path, encoding='utf-8') as f:\n        id2speaker = {idx:t.strip() for idx, t in enumerate(f.readlines())}\n\n    tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n\n    lexicon = read_lexicon(f\"{ROOT_DIR}/lexicon/librispeech-lexicon.txt\")\n    g2p = G2p()\n    prompts = ['Happy', 'Excited', 'Sad', 'Angry']   # prompt is not efficient.\n    speakers = [i for i in range(conf.n_speaker)]\n\n    text_path = args.text_file\n    with open(text_path, \"r\") as f:\n        for i, line in enumerate(tqdm(f)):\n            if not i in range(start_idx, start_idx+chunk_num):\n                continue\n\n            # iteration on prompts and speakers.\n            prompt_idx = i % len(prompts)\n            speaker_idx = i % len(speakers)\n            prompt = prompts[prompt_idx]\n            speaker = speakers[speaker_idx]\n            speaker_name = id2speaker[speaker]\n            speaker_path = os.path.join(output_dir, speaker_name)\n            if not os.path.exists(speaker_path):\n                os.makedirs(speaker_path, exist_ok=True)\n            utt_name = f\"{i+1:06d}\"\n            if os.path.exists(f\"{speaker_path}/{utt_name}.wav\"):\n                print(f\"audio {speaker_path}/{utt_name}.wav exists, continue.\")\n                continue\n\n            try:\n                content = line.strip()\n                text = g2p_cn_en(content, g2p, lexicon)\n                text = text.split()\n\n                style_embedding = get_style_embedding(\n                    prompt, tokenizer, style_encoder)\n                content_embedding = get_style_embedding(\n                    content, tokenizer, style_encoder)\n\n                text_int = [token2id[ph] for ph in text]\n\n                sequence = torch.from_numpy(\n                    np.array(text_int)).to(device).long().unsqueeze(0)\n                sequence_len = torch.from_numpy(\n                    np.array([len(text_int)])).to(device)\n                style_embedding = torch.from_numpy(\n                    style_embedding).to(device).unsqueeze(0)\n                content_embedding = torch.from_numpy(\n                    content_embedding).to(device).unsqueeze(0)\n                speaker = torch.from_numpy(\n                    np.array([speaker])).to(device)\n                with torch.no_grad():\n                    infer_output = generator(\n                            inputs_ling=sequence,\n                            inputs_style_embedding=style_embedding,\n                            input_lengths=sequence_len,\n                            inputs_content_embedding=content_embedding,\n                            inputs_speaker=speaker,\n                            alpha=1.0\n                        )\n                    audio = infer_output[\n                                \"wav_predictions\"].squeeze() * MAX_WAV_VALUE\n                    audio = audio.cpu().numpy().astype('int16')\n\n                    sf.write(file=f\"{speaker_path}/{utt_name}.wav\",\n                             data=audio, samplerate=config.sampling_rate)\n                    with open(f\"{speaker_path}/{utt_name}.txt\",\n                              'w', encoding='utf-8') as ftext:\n                        ftext.write(f\"{content}\\n\")\n            except Exception as e:\n                print(f\"Error: {e}\")\n                continue\n\n\nif __name__ == '__main__':\n    p = argparse.ArgumentParser()\n    p.add_argument('-d', '--logdir', default=\"prompt_tts_open_source_joint\",\n                   type=str, required=False)\n    p.add_argument(\"-c\", \"--config_folder\", default=\"config/joint\",\n                   type=str, required=False)\n    p.add_argument(\"--checkpoint\", type=str, default='g_00140000',\n                   required=False, help='inference specific checkpoint。')\n    p.add_argument('-t', '--text_file', type=str, required=True,\n                   help='the absolute path of test file。')\n    p.add_argument('-o', '--output_dir', type=str, required=False,\n                   default=None, help='path to save the generated audios.')\n    p.add_argument('-g', '--gpu_ids', type=str, required=False, default='0')\n    p.add_argument('-n', '--num_thread', type=str, required=False, default='1')\n\n    args = p.parse_args()\n    sys.path.append(os.path.dirname(\n        os.path.abspath(\"__file__\")) + \"/\" + args.config_folder)\n\n    from config import Config\n    config = Config()\n\n    from multiprocessing import Process\n    gpus = args.gpu_ids\n    os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n    gpu_list = gpus.split(',')\n    gpu_num = len(gpu_list)\n    # 4GB GPU memory per thread, bottleneck is CPU usage!\n    thread_per_gpu = int(args.num_thread)\n    thread_num = gpu_num * thread_per_gpu  # threads\n    torch.set_num_threads(4)  # faster\n\n    total_len = 0\n    with open(args.text_file) as fin:\n        for line in fin:\n            total_len += 1\n\n    print(f\"Total texts: {total_len}, Thread nums: {thread_num}\")\n\n    if total_len >= thread_num:\n        chunk_size = int(total_len / thread_num)\n        remains = total_len - chunk_size * thread_num\n    else:\n        chunk_size = 1\n        remains = 0\n\n    process_list = []\n    chunk_begin = 0\n    for i in range(thread_num):\n        print(f\"process part {i}...\")\n        gpu_id = i % gpu_num\n        now_chunk_size = chunk_size\n        if remains > 0:\n            now_chunk_size = chunk_size + 1\n            remains = remains - 1\n        # use parallel processing or sequential processing\n        p = Process(target=main, args=(\n            args, config, gpu_id, chunk_begin, now_chunk_size))\n        # main(args, config, gpu_id, chunk_begin, now_chunk_size)\n        chunk_begin = chunk_begin + now_chunk_size\n        p.start()\n        process_list.append(p)\n\n    for i in process_list:\n        p.join()\n\n\n"
        },
        {
          "name": "lexicon",
          "type": "tree",
          "content": null
        },
        {
          "name": "mel_process.py",
          "type": "blob",
          "size": 3.7138671875,
          "content": "import math\nimport os\nimport random\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.utils.data\nimport numpy as np\nimport librosa\nimport librosa.util as librosa_util\nfrom librosa.util import normalize, pad_center, tiny\nfrom scipy.signal import get_window\nfrom scipy.io.wavfile import read\nfrom librosa.filters import mel as librosa_mel_fn\n\nMAX_WAV_VALUE = 32768.0\n\n\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\n\ndef dynamic_range_decompression_torch(x, C=1):\n\n    return torch.exp(x) / C\n\n\ndef spectral_normalize_torch(magnitudes):\n    output = dynamic_range_compression_torch(magnitudes)\n    return output\n\n\ndef spectral_de_normalize_torch(magnitudes):\n    output = dynamic_range_decompression_torch(magnitudes)\n    return output\n\n\nmel_basis = {}\nhann_window = {}\n\n\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global hann_window\n    dtype_device = str(y.dtype) + '_' + str(y.device)\n    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n    y = y.squeeze(1)\n\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n    return spec\n\n\ndef spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\n    global mel_basis\n    dtype_device = str(spec.dtype) + '_' + str(spec.device)\n    fmax_dtype_device = str(fmax) + '_' + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    spec = spectral_normalize_torch(spec)\n    return spec\n\n\ndef mel_spectrogram_torch(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n    if torch.min(y) < -1.:\n        print('min value is ', torch.min(y))\n    if torch.max(y) > 1.:\n        print('max value is ', torch.max(y))\n\n    global mel_basis, hann_window\n    dtype_device = str(y.dtype) + '_' + str(y.device)\n    fmax_dtype_device = str(fmax) + '_' + dtype_device\n    wnsize_dtype_device = str(win_size) + '_' + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(\n            sr=sampling_rate, \n            n_fft=n_fft, \n            n_mels=num_mels, \n            fmin=fmin, \n            fmax=fmax)\n        \n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=y.dtype, device=y.device)\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)\n\n    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft-hop_size)/2), int((n_fft-hop_size)/2)), mode='reflect')\n    y = y.squeeze(1)\n\n    spec = torch.stft(y, n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[wnsize_dtype_device],\n                      center=center, pad_mode='reflect', normalized=False, onesided=True, return_complex=False)\n\n    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    spec = spectral_normalize_torch(spec)\n\n    return spec"
        },
        {
          "name": "mfa",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "openaiapi.py",
          "type": "blob",
          "size": 5.638671875,
          "content": "import logging\nimport os\nimport io\nimport torch\nimport glob\n\nfrom fastapi import FastAPI, Response\nfrom pydantic import BaseModel\n\nfrom frontend import g2p_cn_en, ROOT_DIR, read_lexicon, G2p\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\nimport numpy as np\nimport soundfile as sf\nimport pyrubberband as pyrb\nfrom pydub import AudioSegment\nfrom yacs import config as CONFIG\nfrom config.joint.config import Config\n\nLOGGER = logging.getLogger(__name__)\n\nDEFAULTS = {\n}\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(DEVICE)\nconfig = Config()\nMAX_WAV_VALUE = 32768.0\n\n\ndef get_env(key):\n    return os.environ.get(key, DEFAULTS.get(key))\n\n\ndef get_int_env(key):\n    return int(get_env(key))\n\n\ndef get_float_env(key):\n    return float(get_env(key))\n\n\ndef get_bool_env(key):\n    return get_env(key).lower() == 'true'\n\n\ndef scan_checkpoint(cp_dir, prefix, c=8):\n    pattern = os.path.join(cp_dir, prefix + '?'*c)\n    cp_list = glob.glob(pattern)\n    if len(cp_list) == 0:\n        return None\n    return sorted(cp_list)[-1]\n\n\ndef get_models():\n\n    am_checkpoint_path = scan_checkpoint(\n        f'{config.output_directory}/prompt_tts_open_source_joint/ckpt', 'g_')\n\n    # f'{config.output_directory}/style_encoder/ckpt/checkpoint_163431'\n    style_encoder_checkpoint_path = scan_checkpoint(\n        f'{config.output_directory}/style_encoder/ckpt', 'checkpoint_', 6)\n\n    with open(config.model_config_path, 'r') as fin:\n        conf = CONFIG.load_cfg(fin)\n\n    conf.n_vocab = config.n_symbols\n    conf.n_speaker = config.speaker_n_labels\n\n    style_encoder = StyleEncoder(config)\n    model_CKPT = torch.load(style_encoder_checkpoint_path, map_location=\"cpu\")\n    model_ckpt = {}\n    for key, value in model_CKPT['model'].items():\n        new_key = key[7:]\n        model_ckpt[new_key] = value\n    style_encoder.load_state_dict(model_ckpt, strict=False)\n    generator = JETSGenerator(conf).to(DEVICE)\n\n    model_CKPT = torch.load(am_checkpoint_path, map_location=DEVICE)\n    generator.load_state_dict(model_CKPT['generator'])\n    generator.eval()\n\n    tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n\n    with open(config.token_list_path, 'r') as f:\n        token2id = {t.strip(): idx for idx, t, in enumerate(f.readlines())}\n\n    with open(config.speaker2id_path, encoding='utf-8') as f:\n        speaker2id = {t.strip(): idx for idx, t in enumerate(f.readlines())}\n\n    return (style_encoder, generator, tokenizer, token2id, speaker2id)\n\n\ndef get_style_embedding(prompt, tokenizer, style_encoder):\n    prompt = tokenizer([prompt], return_tensors=\"pt\")\n    input_ids = prompt[\"input_ids\"]\n    token_type_ids = prompt[\"token_type_ids\"]\n    attention_mask = prompt[\"attention_mask\"]\n    with torch.no_grad():\n        output = style_encoder(\n            input_ids=input_ids,\n            token_type_ids=token_type_ids,\n            attention_mask=attention_mask,\n        )\n    style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n    return style_embedding\n\n\ndef emotivoice_tts(text, prompt, content, speaker, models):\n    (style_encoder, generator, tokenizer, token2id, speaker2id) = models\n\n    style_embedding = get_style_embedding(prompt, tokenizer, style_encoder)\n    content_embedding = get_style_embedding(content, tokenizer, style_encoder)\n\n    speaker = speaker2id[speaker]\n\n    text_int = [token2id[ph] for ph in text.split()]\n\n    sequence = torch.from_numpy(np.array(text_int)).to(\n        DEVICE).long().unsqueeze(0)\n    sequence_len = torch.from_numpy(np.array([len(text_int)])).to(DEVICE)\n    style_embedding = torch.from_numpy(style_embedding).to(DEVICE).unsqueeze(0)\n    content_embedding = torch.from_numpy(\n        content_embedding).to(DEVICE).unsqueeze(0)\n    speaker = torch.from_numpy(np.array([speaker])).to(DEVICE)\n\n    with torch.no_grad():\n\n        infer_output = generator(\n            inputs_ling=sequence,\n            inputs_style_embedding=style_embedding,\n            input_lengths=sequence_len,\n            inputs_content_embedding=content_embedding,\n            inputs_speaker=speaker,\n            alpha=1.0\n        )\n\n    audio = infer_output[\"wav_predictions\"].squeeze() * MAX_WAV_VALUE\n    audio = audio.cpu().numpy().astype('int16')\n\n    return audio\n\n\nspeakers = config.speakers\nmodels = get_models()\napp = FastAPI()\nlexicon = read_lexicon(f\"{ROOT_DIR}/lexicon/librispeech-lexicon.txt\")\ng2p = G2p()\n\nfrom typing import Optional\nclass SpeechRequest(BaseModel):\n    input: str\n    voice: str = '8051'\n    prompt: Optional[str] = ''\n    language: Optional[str] = 'zh_us'\n    model: Optional[str] = 'emoti-voice'\n    response_format: Optional[str] = 'mp3'\n    speed: Optional[float] = 1.0\n\n\n@app.post(\"/v1/audio/speech\")\ndef text_to_speech(speechRequest: SpeechRequest):\n\n    text = g2p_cn_en(speechRequest.input, g2p, lexicon)\n    np_audio = emotivoice_tts(text, speechRequest.prompt,\n                              speechRequest.input, speechRequest.voice,\n                              models)\n    y_stretch = np_audio\n    if speechRequest.speed != 1.0:\n        y_stretch = pyrb.time_stretch(np_audio, config.sampling_rate, speechRequest.speed)\n    wav_buffer = io.BytesIO()\n    sf.write(file=wav_buffer, data=y_stretch,\n             samplerate=config.sampling_rate, format='WAV')\n    buffer = wav_buffer\n    response_format = speechRequest.response_format\n    if response_format != 'wav':\n        wav_audio = AudioSegment.from_wav(wav_buffer)\n        wav_audio.frame_rate=config.sampling_rate\n        buffer = io.BytesIO()\n        wav_audio.export(buffer, format=response_format)\n\n    return Response(content=buffer.getvalue(),\n                    media_type=f\"audio/{response_format}\")\n"
        },
        {
          "name": "plot_image.py",
          "type": "blob",
          "size": 0.7939453125,
          "content": "import matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\nimport os\n\ndef plot_image_sambert(target, melspec, mel_lengths=None, text_lengths=None, save_dir=None, global_step=None, name=None):\n    # Draw mel_plots\n    mel_plots, axes = plt.subplots(2,1,figsize=(20,15))\n\n    T = mel_lengths[-1]\n    L=100\n\n\n    axes[0].imshow(target[-1].detach().cpu()[:,:T],\n                   origin='lower',\n                   aspect='auto')\n\n    axes[1].imshow(melspec[-1].detach().cpu()[:,:T],\n                   origin='lower',\n                   aspect='auto')\n    for i in range(2):\n        tmp_dir = save_dir+'/att/'+name+'_'+str(global_step)\n        if not os.path.exists(tmp_dir):\n            os.makedirs(tmp_dir)\n        plt.savefig(tmp_dir+'/'+name+'_'+str(global_step)+'_melspec_%s.png'%i)\n\n    return mel_plots"
        },
        {
          "name": "predict.py",
          "type": "blob",
          "size": 8.1748046875,
          "content": "# Prediction interface for Cog ⚙️DEVICE\n# https://github.com/replicate/cog/blob/main/docs/python.md\n\nfrom cog import BasePredictor, Input, Path\nfrom typing import List\n\nimport numpy as np\nfrom yacs import config as CONFIG\nimport torch\nimport re\nimport os, glob\nimport time\nimport subprocess\nimport requests\nimport soundfile as sf\n\nfrom frontend_cn import g2p_cn\nfrom frontend_en import preprocess_english\nfrom config.joint.config import Config\nfrom models.prompt_tts_modified.jets import JETSGenerator\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom transformers import AutoTokenizer\n\nMAX_WAV_VALUE = 32768.0\n\n# url for the weights mirror\nREPLICATE_WEIGHTS_URL = \"https://weights.replicate.delivery/default\"\n\n# files to download from the weights mirrors\nDEFAULT_WEIGHTS = [\n    {\n        \"dest\": \"outputs/prompt_tts_open_source_joint/ckpt\",\n        \"src\": \"EmotiVoice\",\n        \"files\": [\n            \"do_00140000\",\n            \"g_00140000\",\n        ],\n    },\n    {\n        \"dest\": \"outputs/style_encoder/ckpt\",\n        \"src\": \"EmotiVoice\",\n        \"files\": [\n            \"checkpoint_163431\",\n        ],\n    },\n    {\n        \"dest\": \"WangZeJun/simbert-base-chinese\",\n        \"src\": \"simbert-base-chinese/b5c82a8ab1e4bcac799620fc4d870aae087b0c71\",\n        \"files\": [\n            \"pytorch_model.bin\",\n            \"config.json\",\n            \"vocab.txt\",\n        ],\n    }\n]\n\ndef scan_checkpoint(cp_dir, prefix, c=8):\n    pattern = os.path.join(cp_dir, prefix + '?'*c)\n    cp_list = glob.glob(pattern)\n    if len(cp_list) == 0:\n        return None\n    return sorted(cp_list)[-1]\n\ndef g2p_en(text):\n    return preprocess_english(text)\n\ndef contains_chinese(text):\n    pattern = re.compile(r'[\\u4e00-\\u9fa5]')\n    match = re.search(pattern, text)\n    return match is not None\n\ndef download_json(url: str, dest: Path):\n    res = requests.get(url, allow_redirects=True)\n    if res.status_code == 200 and res.content:\n        with dest.open(\"wb\") as f:\n            f.write(res.content)\n    else:\n        print(f\"Failed to download {url}. Status code: {res.status_code}\")\n\ndef download_weights(baseurl: str, basedest: str, files: List[str]):\n    \"\"\"Download model weights from Replicate and save to file.\n    Weights and download locations are specified in DEFAULT_WEIGHTS\n    \"\"\"\n    basedest = Path(basedest)\n    start = time.time()\n    print(\"downloading to: \", basedest)\n    basedest.mkdir(parents=True, exist_ok=True)\n    for f in files:\n        dest = basedest / f\n        url = os.path.join(REPLICATE_WEIGHTS_URL, baseurl, f)\n        if not dest.exists():\n            print(\"downloading url: \", url)\n            if dest.suffix == \".json\":\n                download_json(url, dest)\n            else:\n                subprocess.check_call([\"pget\", url, str(dest)], close_fds=False)\n    print(\"downloading took: \", time.time() - start)\n\nclass Predictor(BasePredictor):\n\n    def setup_models(self):\n        config = self.config\n        am_checkpoint_path = scan_checkpoint(f'{config.output_directory}/prompt_tts_open_source_joint/ckpt', 'g_')\n\n        style_encoder_checkpoint_path = scan_checkpoint(f'{config.output_directory}/style_encoder/ckpt', 'checkpoint_', 6)\n\n        with open(config.model_config_path, 'r') as fin:\n            conf = CONFIG.load_cfg(fin)\n\n        conf.n_vocab = config.n_symbols\n        conf.n_speaker = config.speaker_n_labels\n\n        style_encoder = StyleEncoder(config)\n        model_CKPT = torch.load(style_encoder_checkpoint_path, map_location=\"cpu\")\n        model_ckpt = {}\n        for key, value in model_CKPT['model'].items():\n            new_key = key[7:]\n            model_ckpt[new_key] = value\n        style_encoder.load_state_dict(model_ckpt, strict=False)\n        generator = JETSGenerator(conf).to(self.device)\n\n        model_CKPT = torch.load(am_checkpoint_path, map_location=self.device)\n        generator.load_state_dict(model_CKPT['generator'])\n        generator.eval()\n\n        self.tokenizer = AutoTokenizer.from_pretrained(config.bert_path)\n\n        with open(config.token_list_path, 'r') as f:\n            self.token2id = {t.strip():idx for idx, t, in enumerate(f.readlines())}\n\n        with open(config.speaker2id_path, encoding='utf-8') as f:\n            self.speaker2id = {t.strip():idx for idx, t in enumerate(f.readlines())}\n\n        self.style_encoder = style_encoder\n        self.generator = generator\n        print(self.tokenizer)\n\n    def setup(self) -> None:\n        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n        # self.model = torch.load(\"./weights.pth\")\n        for weight in DEFAULT_WEIGHTS:\n            download_weights(weight[\"src\"], weight[\"dest\"], weight[\"files\"])\n        self.config = Config()\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.setup_models()\n\n    # def get_style_embedding(prompt, tokenizer, style_encoder):\n    def get_style_embedding(self, text):\n        tokenizer = self.tokenizer\n        style_encoder = self.style_encoder\n        text = tokenizer([text], return_tensors=\"pt\")\n        input_ids = text[\"input_ids\"]\n        token_type_ids = text[\"token_type_ids\"]\n        attention_mask = text[\"attention_mask\"]\n        with torch.no_grad():\n            output = style_encoder(\n            input_ids=input_ids,\n            token_type_ids=token_type_ids,\n            attention_mask=attention_mask,\n        )\n        style_embedding = output[\"pooled_output\"].cpu().squeeze().numpy()\n        return style_embedding\n\n    def tts(self, text, prompt, content, speaker):\n        style_embedding = self.get_style_embedding(prompt)\n        content_embedding = self.get_style_embedding(content)\n        device = self.device\n\n        speaker = self.speaker2id[speaker]\n\n        text_int = [self.token2id[ph] for ph in text.split()]\n\n        sequence = torch.from_numpy(np.array(text_int)).to(device).long().unsqueeze(0)\n        sequence_len = torch.from_numpy(np.array([len(text_int)])).to(device)\n        style_embedding = torch.from_numpy(style_embedding).to(device).unsqueeze(0)\n        content_embedding = torch.from_numpy(content_embedding).to(device).unsqueeze(0)\n        speaker = torch.from_numpy(np.array([speaker])).to(device)\n\n        with torch.no_grad():\n\n            infer_output = self.generator(\n                    inputs_ling=sequence,\n                    inputs_style_embedding=style_embedding,\n                    input_lengths=sequence_len,\n                    inputs_content_embedding=content_embedding,\n                    inputs_speaker=speaker,\n                    alpha=1.0\n                )\n\n        audio = infer_output[\"wav_predictions\"].squeeze()* MAX_WAV_VALUE\n        audio = audio.cpu().numpy().astype('int16')\n        path = os.path.join(self.config.output_directory,\"output.mp3\")\n        sf.write(file=path, data=audio, samplerate=self.config.sampling_rate)\n        return path\n\n    def predict(\n        self,\n        prompt: str = Input(\n            description=\"Input prompt\",\n            default=\"Happy\",\n        ),\n        content: str = Input(\n            description=\"Input text\",\n            default=\"Emoti-Voice - a Multi-Voice and Prompt-Controlled T-T-S Engine\",\n        ),\n        language: str = Input(\n            description=\"Language\",\n            choices=[\"English\", \"Chinese\"],\n            default=\"English\",\n        ),\n        speaker: str = Input(\n            description=\"speakers\",\n            choices=Config().speakers,\n            default=Config().speakers[0],\n        ),\n    ) -> Path:\n        \"\"\"Run a single prediction on the model\"\"\"\n        # processed_input = preprocess(image)\n        # output = self.model(processed_image, scale)\n        # return postprocess(output)\n        if language==\"English\":\n            if contains_chinese(content):\n                raise ValueError(\"文本含有中文/input text contains Chinese, but language is English\")\n            else:\n                text = g2p_en(content)\n                path = self.tts(text, prompt, content, speaker)\n                return Path(path)\n        else:\n            if not contains_chinese(content):\n                raise ValueError(\"文本含有英文/input text contains English, but language is Chinese\")\n            else:\n                text = g2p_cn(content)\n                path = self.tts(text, prompt, content, speaker)\n                return Path(path)\n"
        },
        {
          "name": "prepare_for_training.py",
          "type": "blob",
          "size": 3.6611328125,
          "content": "# Copyright 2023, YOUDAO\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#     http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport os\nimport shutil\nimport argparse\n\n\ndef main(args):\n    from os.path import join\n    data_dir = args.data_dir\n    exp_dir = args.exp_dir\n    os.makedirs(exp_dir, exist_ok=True)\n\n    info_dir = join(exp_dir, 'info')\n    prepare_info(data_dir, info_dir)\n\n    config_dir = join(exp_dir, 'config')\n    prepare_config(data_dir, info_dir, exp_dir, config_dir)\n\n    ckpt_dir = join(exp_dir, 'ckpt')\n    prepare_ckpt(data_dir, info_dir, ckpt_dir)\n\n\nROOT_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\ndef prepare_info(data_dir, info_dir):\n    import jsonlines\n    print('prepare_info: %s' %info_dir)\n    os.makedirs(info_dir, exist_ok=True)\n\n    for name in [\"emotion\", \"energy\", \"pitch\", \"speed\", \"tokenlist\"]:\n        shutil.copy(f\"{ROOT_DIR}/data/youdao/text/{name}\", f\"{info_dir}/{name}\")\n\n    d_speaker = {} # get all the speakers from datalist\n    with jsonlines.open(f\"{data_dir}/train/datalist.jsonl\") as reader:\n        for obj in reader:\n            speaker = obj[\"speaker\"]\n            if not speaker in d_speaker:\n                d_speaker[speaker] = 1\n            else:\n                d_speaker[speaker] += 1\n\n    with open(f\"{ROOT_DIR}/data/youdao/text/speaker2\") as f, \\\n        open(f\"{info_dir}/speaker\", \"w\") as fout:\n\n        for line in f:\n            speaker = line.strip()\n            if speaker in d_speaker:\n                print('warning: duplicate of speaker [%s] in [%s]' % (speaker, data_dir))\n                continue\n            fout.write(line.strip()+\"\\n\")\n\n        for speaker in sorted(d_speaker.keys()):\n            fout.write(speaker + \"\\n\")\n\n\ndef prepare_config(data_dir, info_dir, exp_dir, config_dir):\n    print('prepare_config: %s' %config_dir)\n    os.makedirs(config_dir, exist_ok=True)\n\n    with open(f\"{ROOT_DIR}/config/template.py\") as f, \\\n        open(f\"{config_dir}/config.py\", \"w\") as fout:\n\n        for line in f:\n            fout.write(line.replace('<DATA_DIR>', data_dir).replace('<INFO_DIR>', info_dir).replace('<EXP_DIR>', exp_dir))\n\n\ndef prepare_ckpt(data_dir, info_dir, ckpt_dir):\n    print('prepare_ckpt: %s' %ckpt_dir)\n    os.makedirs(ckpt_dir, exist_ok=True)\n    \n    with open(f\"{info_dir}/speaker\") as f:\n        speaker_list=[line.strip() for line in f]\n    assert len(speaker_list) >= 2014\n    \n    gen_ckpt_path = f\"{ROOT_DIR}/outputs/prompt_tts_open_source_joint/ckpt/g_00140000\"\n    disc_ckpt_path = f\"{ROOT_DIR}/outputs/prompt_tts_open_source_joint/ckpt/do_00140000\"\n\n    gen_ckpt = torch.load(gen_ckpt_path, map_location=\"cpu\")\n\n    speaker_embeddings = gen_ckpt[\"generator\"][\"am.spk_tokenizer.weight\"].clone()\n    \n    new_embedding = torch.randn((len(speaker_list)-speaker_embeddings.size(0), speaker_embeddings.size(1)))\n\n    gen_ckpt[\"generator\"][\"am.spk_tokenizer.weight\"] = torch.cat([speaker_embeddings, new_embedding], dim=0)\n\n\n    torch.save(gen_ckpt, f\"{ckpt_dir}/pretrained_generator\")\n    shutil.copy(disc_ckpt_path, f\"{ckpt_dir}/pretrained_discriminator\")\n\n\n\nif __name__ == \"__main__\":\n\n    p = argparse.ArgumentParser()\n    p.add_argument('--data_dir', type=str, required=True)\n    p.add_argument('--exp_dir', type=str, required=True)\n    args = p.parse_args()\n\n    main(args)\n"
        },
        {
          "name": "requirements.openaiapi.txt",
          "type": "blob",
          "size": 0.060546875,
          "content": "fastapi\npython-multipart\nuvicorn[standard]\npydub\npyrubberband\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.1064453125,
          "content": "torch\ntorchaudio\nnumpy\nnumba\nscipy\ntransformers\nsoundfile\nyacs\ng2p_en\njieba\npypinyin\npypinyin_dict\nstreamlit\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 2.732421875,
          "content": "import os\n\nfrom setuptools import find_packages, setup\n\nrequirements={\n    \"infer\": [\n        \"numpy>=1.24.3\",\n        \"scipy>=1.10.1\",\n        \"torch>=2.1\",\n        \"torchaudio\",\n        \"soundfile>=0.12.0\",\n        \"librosa>=0.10.0\",\n        \"scikit-learn\",\n        \"numba==0.58.1\",\n        \"inflect>=5.6.0\",\n        \"tqdm>=4.64.1\",\n        \"pyyaml>=6.0\",\n        \"transformers==4.26.1\",\n        \"yacs\",\n        \"g2p_en\",\n        \"jieba\",\n        \"pypinyin\",\n        \"streamlit\",\n        \"pandas>=1.4,<2.0\",\n    ],\n    \"openai\": [\n        \"fastapi\",\n        \"python-multipart\",\n        \"uvicorn[standard]\",\n        \"pydub\",\n    ],\n    \"train\": [\n        \"jsonlines\",\n        \"praatio\",\n        \"pyworld\",\n        \"flake8\",\n        \"flake8-bugbear\",\n        \"flake8-comprehensions\",\n        \"flake8-executable\",\n        \"flake8-pyi\",\n        \"mccabe\",\n        \"pycodestyle\",\n        \"pyflakes\",\n        \"tensorboard\",\n        \"einops\",\n        \"matplotlib\",\n    ]\n}\n\ninfer_requires = requirements[\"infer\"]\nopenai_requires = requirements[\"infer\"] + requirements[\"openai\"]\ntrain_requires = requirements[\"infer\"] + requirements[\"train\"]\n\nVERSION = '0.2.0'\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as readme_file:\n    README = readme_file.read()\n\n\nsetup(\n    name=\"EmotiVoice\",\n    version=VERSION,\n    url=\"https://github.com/netease-youdao/EmotiVoice\",\n    author=\"Huaxuan Wang\",\n    author_email=\"wanghx04@rd.netease.com\",\n    description=\"EmotiVoice 😊: a Multi-Voice and Prompt-Controlled TTS Engine\",\n    long_description=README,\n    long_description_content_type=\"text/markdown\",\n    license=\"Apache Software License\",\n    # package\n    packages=find_packages(),\n    project_urls={\n        \"Documentation\": \"https://github.com/netease-youdao/EmotiVoice/wiki\",\n        \"Tracker\": \"https://github.com/netease-youdao/EmotiVoice/issues\",\n        \"Repository\": \"https://github.com/netease-youdao/EmotiVoice\",\n    },\n    install_requires=infer_requires,\n    extras_require={\n        \"train\": train_requires,\n        \"openai\": openai_requires,\n    },\n    python_requires=\">=3.8.0\",\n    classifiers=[\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Science/Research\",\n        \"Operating System :: POSIX :: Linux\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n        \"Topic :: Multimedia :: Sound/Audio :: Speech\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n)"
        },
        {
          "name": "text",
          "type": "tree",
          "content": null
        },
        {
          "name": "train_am_vocoder_joint.py",
          "type": "blob",
          "size": 19.1552734375,
          "content": "import argparse, time\nimport sys\nimport os\nimport torch, glob, itertools\nfrom yacs import config as CONFIG\n\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.functional as F\n\nfrom plot_image import plot_image_sambert\nfrom mel_process import mel_spectrogram_torch\nfrom models.prompt_tts_modified.jets import JETSGenerator, get_segments\nfrom models.hifigan.pretrained_discriminator import Discriminator\nfrom models.hifigan.models import discriminator_loss,  generator_loss, feature_loss\nfrom models.prompt_tts_modified.loss import TTSLoss\nfrom models.prompt_tts_modified.simbert import StyleEncoder\nfrom models.prompt_tts_modified.prompt_dataset import Dataset_PromptTTS as Dataset_PromptTTS_JETS\nfrom torch.utils.data import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef get_writer(output_directory):\n    logging_path=f'{output_directory}' + \"/log\"\n    if not os.path.exists(logging_path):\n        os.makedirs(logging_path, exist_ok=True)\n    writer = SummaryWriter(logging_path)\n    return writer\n\ndef save_checkpoint(filepath, obj):\n    print(\"Saving checkpoint to {}\".format(filepath))\n    torch.save(obj, filepath)\n    print(\"Complete.\")\n\n\ndef scan_checkpoint(cp_dir, prefix):\n    pattern = os.path.join(cp_dir, prefix + '????????')\n    cp_list = glob.glob(pattern)\n    if len(cp_list) == 0:\n        return None\n    return sorted(cp_list)[-1]\n\ndef load_checkpoint(filepath, device):\n    assert os.path.isfile(filepath)\n    print(\"Loading '{}'\".format(filepath))\n    checkpoint_dict = torch.load(filepath, map_location=device)\n    print(\"Complete.\")\n    return checkpoint_dict\n\n\n\n\ndef validate(args, generator, val_loader, iteration, writer, config, device, loss_fn):\n\n    generator.eval()\n\n    with torch.no_grad():\n        dec_mel_loss_list = []\n        postnet_mel_loss_list = []\n        dur_loss_list = []\n        pitch_loss_list = []\n        energy_loss_list = []\n        forwardsum_loss_list = []\n        bin_loss_list = []\n\n\n        for i, batch in enumerate(val_loader):\n\n            batch = {key: value.to(device, non_blocking=True) for key, value in batch.items()}\n\n            phoneme_id = batch[\"phoneme_id\"]\n            phoneme_lens = batch[\"phoneme_lens\"]\n            mel = batch[\"mel\"]\n            mel_lens = batch[\"mel_lens\"]\n            speaker = batch[\"speaker\"]\n            style_embedding = batch[\"style_embedding\"]\n            content_embedding = batch[\"content_embedding\"]\n            pitch = batch[\"pitch\"]\n            energy = batch[\"energy\"]\n            wav = batch[\"wav\"]\n\n            output = generator(\n                inputs_ling=phoneme_id,\n                inputs_style_embedding=style_embedding,\n                inputs_content_embedding=content_embedding,\n                input_lengths=phoneme_lens,\n                inputs_speaker=speaker,\n                output_lengths=mel_lens,\n                mel_targets=mel,\n                pitch_targets=pitch,\n                energy_targets=energy,\n                cut_flag=False\n            )\n\n            y_hat_mel = mel_spectrogram_torch(\n                    output[\"wav_predictions\"][:,:,:wav.size(1)].squeeze(1), \n                    config.filter_length, \n                    config.n_mel_channels, \n                    config.sampling_rate, \n                    config.hop_length, \n                    config.win_length, \n                    config.mel_fmin, \n                    config.mel_fmax\n                )\n\n\n            y_mel = mel_spectrogram_torch(\n                wav.squeeze(1), \n                config.filter_length, \n                config.n_mel_channels, \n                config.sampling_rate, \n                config.hop_length, \n                config.win_length, \n                config.mel_fmin, \n                config.mel_fmax\n            )\n            output[\"dec_outputs\"] = y_hat_mel\n            output[\"mel_targets\"] = y_mel.transpose(1,2)\n\n            losses = loss_fn(output)\n\n            dec_mel_loss_list.append(losses[\"dec_mel_loss\"].item())\n            dur_loss_list.append(losses[\"dur_loss\"].item())\n            pitch_loss_list.append(losses[\"pitch_loss\"].item())\n            energy_loss_list.append(losses[\"energy_loss\"].item())\n            forwardsum_loss_list.append(losses[\"forwardsum_loss\"].item())\n            bin_loss_list.append(losses[\"bin_loss\"].item())\n\n            \n        dec_mel_loss = sum(dec_mel_loss_list)/len(dec_mel_loss_list)\n        dur_loss = sum(dur_loss_list)/len(dur_loss_list)\n        pitch_loss = sum(pitch_loss_list)/len(pitch_loss_list)\n        energy_loss = sum(energy_loss_list)/len(energy_loss_list)\n        forwardsum_loss = sum(forwardsum_loss_list)/len(forwardsum_loss_list)\n        bin_loss = sum(bin_loss_list)/len(bin_loss_list)\n\n\n        message = f'global_step={iteration}, val_dec_mel_loss={dec_mel_loss:0.4f}, val_dur_loss={dur_loss:0.4f}, val_pitch_loss={pitch_loss:0.4f}, val_energy_loss={energy_loss:0.4f}, val_forwardsum_loss={forwardsum_loss:0.4f}, bin_loss={bin_loss:0.4f}, '\n        print(message)\n        with open(os.path.join(f'{config.output_directory}' + \"/log\", \"train_log.txt\"), \"a\") as f:\n            f.write(message + \"\\n\")\n\n    writer.add_scalar('val_dec_mel_loss', dec_mel_loss, global_step=iteration)\n    writer.add_scalar('val_dur_loss', dur_loss, global_step=iteration)\n    writer.add_scalar('val_pitch_loss', pitch_loss, global_step=iteration)\n    writer.add_scalar('val_energy_loss', energy_loss, global_step=iteration)\n    writer.add_scalar('val_forwardsum_loss', forwardsum_loss, global_step=iteration)\n    writer.add_scalar('val_bin_loss', bin_loss, global_step=iteration)\n    \n    mel_plots = plot_image_sambert(mel,\n                                output[\"dec_outputs\"],\n                                mel_lens,\n                                phoneme_lens, \n                                save_dir=f'{config.output_directory}', \n                                global_step=iteration, \n                                name='val')\n    writer.add_figure('Validation mel_plots', mel_plots, global_step=iteration)\n    \n    with torch.no_grad():\n        T=phoneme_lens[-1]\n        output_infer = generator(\n                inputs_ling=phoneme_id[-1,:T].unsqueeze(0),\n                inputs_style_embedding=style_embedding[-1].unsqueeze(0),\n                input_lengths=phoneme_lens[-1].unsqueeze(0),\n                inputs_content_embedding=content_embedding[-1].unsqueeze(0),\n                inputs_speaker=speaker[-1].unsqueeze(0),\n            )\n    \n    y_hat_mel = mel_spectrogram_torch(\n                    output_infer[\"wav_predictions\"].squeeze(1), \n                    config.filter_length, \n                    config.n_mel_channels, \n                    config.sampling_rate, \n                    config.hop_length, \n                    config.win_length, \n                    config.mel_fmin, \n                    config.mel_fmax\n                )\n    writer.add_audio('generated_audio', output_infer[\"wav_predictions\"].squeeze(1), iteration, 16_000)\n\n    mel_plots_infer = plot_image_sambert(mel,\n                                y_hat_mel,\n                                mel_lens,\n                                phoneme_lens, \n                                save_dir=f'{config.output_directory}', \n                                global_step=iteration, \n                                name='infer')\n\n    writer.add_figure('Inference mel_plots', mel_plots_infer, global_step=iteration)\n    generator.train()\n    return \n\n\ndef train(args, config):\n    # os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n    rank = int(os.environ[\"LOCAL_RANK\"])\n\n    torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\", world_size=args.n_gpus, rank=rank)\n\n    torch.cuda.set_device(rank)\n    device = torch.device(f'cuda:{rank}')\n    \n    if rank==0:\n        print(\"run!\")\n        writer = get_writer(config.output_directory)\n    \n    print(\"device: \", rank)\n\n    style_encoder = StyleEncoder(config)\n    model_CKPT = torch.load(config.style_encoder_ckpt, map_location=\"cpu\")\n    model_ckpt = {}\n    for key, value in model_CKPT['model'].items():\n        new_key = key[7:]\n        model_ckpt[new_key] = value\n    style_encoder.load_state_dict(model_ckpt, strict=False)\n\n    train_dataset = Dataset_PromptTTS_JETS(config.train_data_path, config, style_encoder)\n    data_sampler = DistributedSampler(train_dataset)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        num_workers=8,\n        shuffle=False,\n        batch_size=config.batch_size,\n        collate_fn=train_dataset.TextMelCollate,\n        sampler = data_sampler,\n    )\n    if rank == 0:\n        valid_dataset = Dataset_PromptTTS_JETS(config.valid_data_path, config, style_encoder)\n\n        valid_loader = torch.utils.data.DataLoader(\n            valid_dataset,\n            num_workers=1,\n            batch_size=config.batch_size,\n            collate_fn=train_dataset.TextMelCollate,\n            pin_memory=True,\n        )\n    with open(config.model_config_path, 'r') as fin:\n        conf = CONFIG.load_cfg(fin)\n    \n    conf.n_vocab = config.n_symbols\n    conf.n_speaker = config.speaker_n_labels\n\n    iteration=0\n\n\n    generator = JETSGenerator(conf).to(device)\n    discriminator = Discriminator(conf).to(device)\n\n    os.makedirs(f'{config.output_directory}' + '/ckpt', exist_ok=True)\n    cp_g = scan_checkpoint(f'{config.output_directory}' + '/ckpt', 'g_')\n    cp_do = scan_checkpoint(f'{config.output_directory}' + '/ckpt', 'do_')\n\n    if cp_g is None or cp_do is None:\n        state_dict_do = None\n        last_epoch = -1\n    else:\n        state_dict_g = load_checkpoint(cp_g, device)\n        state_dict_do = load_checkpoint(cp_do, device)\n        generator.load_state_dict(state_dict_g['generator'])\n        discriminator.load_state_dict(state_dict_do['discriminator'])\n        iteration = state_dict_do['steps'] + 1\n        last_epoch = state_dict_do['epoch']\n\n    if args.load_pretrained_model:\n        ckpt=torch.load(f'{config.output_directory}/ckpt/pretrained_generator')\n        generator.load_state_dict(ckpt['generator'])\n        ckpt=torch.load(f'{config.output_directory}/ckpt/pretrained_discriminator')\n        discriminator.load_state_dict(ckpt['discriminator'])\n        state_dict_do = None\n        last_epoch = -1\n        iteration=0\n\n        print()\n\n\n    generator = DDP(generator, device_ids=[rank]).to(device)\n    discriminator = DDP(discriminator, device_ids=[rank]).to(device)\n\n    optim_g = torch.optim.Adam(generator.parameters(), conf.optimizer.lr, betas=conf.optimizer.betas)\n    optim_d = torch.optim.Adam(discriminator.parameters(),\n                                conf.optimizer.lr, betas=conf.optimizer.betas)\n\n    if state_dict_do is not None:\n        optim_g.load_state_dict(state_dict_do['optim_g'])\n        optim_d.load_state_dict(state_dict_do['optim_d'])\n\n    \n    scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=conf.scheduler.gamma, last_epoch=last_epoch)\n    scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=conf.scheduler.gamma, last_epoch=last_epoch)\n\n    loss_fn = TTSLoss()\n\n    if rank == 0:\n        print(\"The number of parameters in the model: %0.2f M\"%(count_parameters(generator)/1000000.0))\n        print(f\"Training Start!!! ({config.output_directory})\")\n\n\n    generator.train()\n    discriminator.train()\n\n    for epoch in range(max(0, last_epoch), 5_000_000):\n\n        if rank == 0:\n            for param_group in optim_g.param_groups:\n                print(\"Current learning rate: \" + str(param_group[\"lr\"]))\n            print(\"Epoch: {}\".format(epoch+1))\n        \n        data_sampler.set_epoch(epoch)\n\n        for i, batch in enumerate(train_loader):\n            if rank == 0:\n                start_b = time.time()\n\n            batch = {key: value.to(device, non_blocking=True) for key, value in batch.items()}\n\n            phoneme_id = batch[\"phoneme_id\"]\n            phoneme_lens = batch[\"phoneme_lens\"]\n            mel = batch[\"mel\"]\n            mel_lens = batch[\"mel_lens\"]\n            speaker = batch[\"speaker\"]\n            style_embedding = batch[\"style_embedding\"]\n            content_embedding = batch[\"content_embedding\"]\n            pitch = batch[\"pitch\"]\n            energy = batch[\"energy\"]\n            wav = batch[\"wav\"]\n        \n            output = generator(\n                inputs_ling=phoneme_id,\n                inputs_style_embedding=style_embedding,\n                inputs_content_embedding=content_embedding,\n                input_lengths=phoneme_lens,\n                inputs_speaker=speaker,\n                output_lengths=mel_lens,\n                mel_targets=mel,\n                pitch_targets=pitch,\n                energy_targets=energy,\n            )\n\n            y_hat_mel = mel_spectrogram_torch(\n                output[\"wav_predictions\"].squeeze(1), \n                config.filter_length, \n                config.n_mel_channels, \n                config.sampling_rate, \n                config.hop_length, \n                config.win_length, \n                config.mel_fmin, \n                config.mel_fmax\n            )\n\n            wav = get_segments(\n                x=wav.unsqueeze(1),\n                start_idxs=output[\"z_start_idxs\"] * (generator.module.upsample_factor if hasattr(generator, \"module\") else generator.upsample_factor),\n                segment_size=output[\"segment_size\"] * (generator.module.upsample_factor if hasattr(generator, \"module\") else generator.upsample_factor),\n            )\n\n            y_mel = mel_spectrogram_torch(\n                wav.squeeze(1), \n                config.filter_length, \n                config.n_mel_channels, \n                config.sampling_rate, \n                config.hop_length, \n                config.win_length, \n                config.mel_fmin, \n                config.mel_fmax\n            )\n            output[\"dec_outputs\"] = y_hat_mel\n            output[\"mel_targets\"] = y_mel.transpose(1,2)\n\n            ########################################## Discriminator ##########################################\n            optim_d.zero_grad()\n\n            y_df_hat_r, y_df_hat_g, fmap_f_r, fmap_f_g, y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = discriminator(wav, output[\"wav_predictions\"].detach())\n\n            loss_disc_f, losses_disc_f_r, losses_disc_f_g = discriminator_loss(y_df_hat_r, y_df_hat_g)\n            loss_disc_s, losses_disc_s_r, losses_disc_s_g = discriminator_loss(y_ds_hat_r, y_ds_hat_g)\n\n            loss_disc_all = loss_disc_s + loss_disc_f\n\n            loss_disc_all.backward()\n            optim_d.step()\n\n\n\n\n            ########################################## Generator ##########################################\n\n            y_df_hat_r, y_df_hat_g, fmap_f_r, fmap_f_g, y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = discriminator(wav, output[\"wav_predictions\"])\n            optim_g.zero_grad()\n            loss = loss_fn(output)\n            loss_mel = F.l1_loss(y_mel, y_hat_mel)\n            loss[\"dec_mel_loss\"]=loss_mel\n            loss_fm_f = feature_loss(fmap_f_r, fmap_f_g)\n            loss_fm_s = feature_loss(fmap_s_r, fmap_s_g)\n            loss_gen_f, losses_gen_f = generator_loss(y_df_hat_g)\n            loss_gen_s, losses_gen_s = generator_loss(y_ds_hat_g)\n\n            dec_mel_loss = loss[\"dec_mel_loss\"] * 45\n            dur_loss = loss[\"dur_loss\"] * 1\n            pitch_loss = loss[\"pitch_loss\"] * 1\n            energy_loss = loss[\"energy_loss\"] * 1 \n            forwardsum_loss = loss[\"forwardsum_loss\"] * 2\n            bin_loss = loss[\"bin_loss\"] * 2\n            loss_gen = (loss_gen_f + loss_gen_s) * 1\n            loss_fm = (loss_fm_f + loss_fm_s)\n\n            loss_gen_all = loss_gen + loss_fm + \\\n                dec_mel_loss + dur_loss + \\\n                pitch_loss + energy_loss + \\\n                forwardsum_loss + bin_loss\n\n    \n            loss_gen_all.backward()\n            optim_g.step()\n            \n            iteration += 1\n\n            if rank==0:\n                writer.add_scalar('train_loss_fm', loss_fm, global_step=iteration)\n                writer.add_scalar('train_loss_gen', loss_gen, global_step=iteration)\n                writer.add_scalar('train_dec_mel_loss', dec_mel_loss, global_step=iteration)\n                writer.add_scalar('train_dur_loss', dur_loss, global_step=iteration)\n                writer.add_scalar('train_pitch_loss', pitch_loss, global_step=iteration)\n                writer.add_scalar('train_energy_loss', energy_loss, global_step=iteration)\n                writer.add_scalar('train_forwardsum_loss', forwardsum_loss, global_step=iteration)\n                writer.add_scalar('train_bin_loss', bin_loss, global_step=iteration)\n                message = f'global_step={iteration}, train_dec_mel_loss={dec_mel_loss:0.4f}, train_dur_loss={dur_loss:0.4f}, train_pitch_loss={pitch_loss:0.4f}, train_energy_loss={energy_loss:0.4f}, train_forwardsum_loss={forwardsum_loss:0.4f}, bin_loss={bin_loss:0.4f}, train_loss_fm={loss_fm:0.4f}, train_loss_gen={loss_gen:0.4f},  s/b={time.time() - start_b:4.3f}'\n                if iteration % (config.iters_per_validation) == 0:\n\n                    validate(args, generator, valid_loader, iteration, writer, config, device, loss_fn)\n                    print(message)\n                    with open(os.path.join(f'{config.output_directory}' + \"/log\", \"train_log.txt\"), \"a\") as f:\n                        f.write(message + \"\\n\")\n                elif iteration % (config.iters_per_validation//10) == 0:\n\n                    print(message)\n                    with open(os.path.join(f'{config.output_directory}' + \"/log\", \"train_log.txt\"), \"a\") as f:\n                        f.write(message + \"\\n\")\n\n                if iteration % (config.iters_per_checkpoint) == 0:\n                    checkpoint_path = \"{}/g_{:08d}\".format(f'{config.output_directory}' + '/ckpt', iteration)\n                    save_checkpoint(checkpoint_path, {'generator': (generator.module if hasattr(generator, 'module') else generator).state_dict()})\n                    checkpoint_path = \"{}/do_{:08d}\".format(f'{config.output_directory}' + '/ckpt', iteration)\n                    save_checkpoint(checkpoint_path, \n                                    {'discriminator': (discriminator.module if hasattr(discriminator, 'module')\n                                                        else discriminator).state_dict(),\n                                    'optim_g': optim_g.state_dict(), 'optim_d': optim_d.state_dict(), 'steps': iteration,\n                                    'epoch': epoch})\n\n                if iteration == (config.train_steps):\n                    writer.close()\n                    print(\"TRAINING DONE!\")\n                    break\n        scheduler_g.step()\n        scheduler_d.step()\n\ndef main():\n    p = argparse.ArgumentParser()\n    p.add_argument(\"-c\", \"--config_folder\", type=str, required=True)\n    p.add_argument(\"--checkpoint\", type=str, default=\"\")\n    p.add_argument(\"--load_pretrained_model\", default=False)\n    args = p.parse_args() \n\n    ##################################################\n    sys.path.append(args.config_folder)\n    from config import Config\n    config = Config()\n    ##################################################\n    n_gpus = torch.cuda.device_count()\n    args.n_gpus=n_gpus\n\n\n    torch.manual_seed(config.seed)\n    torch.cuda.manual_seed(config.seed)\n    # os.environ[\n    #     \"TORCH_DISTRIBUTED_DEBUG\"\n    # ] = \"DETAIL\"\n    train(args, config)\n\n\nif __name__ == '__main__':\n    main()"
        }
      ]
    }
  ]
}