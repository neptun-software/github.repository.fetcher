{
  "metadata": {
    "timestamp": 1736560580642,
    "page": 197,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjIwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lucidrains/imagen-pytorch",
      "stars": 8147,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.7568359375,
          "content": "# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.041015625,
          "content": "MIT License\n\nCopyright (c) 2022 Phil Wang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0390625,
          "content": "recursive-include imagen_pytorch *.json\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 38.71875,
          "content": "<img src=\"./imagen.png\" width=\"450px\"></img>\n\n## Imagen - Pytorch\n\nImplementation of <a href=\"https://gweb-research-imagen.appspot.com/\">Imagen</a>, Google's Text-to-Image Neural Network that beats DALL-E2, in Pytorch. It is the new SOTA for text-to-image synthesis.\n\nArchitecturally, it is actually much simpler than DALL-E2. It consists of a cascading DDPM conditioned on text embeddings from a large pretrained T5 model (attention network). It also contains dynamic clipping for improved classifier free guidance, noise level conditioning, and a memory efficient unet design.\n\nIt appears neither CLIP nor prior network is needed after all. And so research continues.\n\n<a href=\"https://www.youtube.com/watch?v=xqDeAz0U-R4\">AI Coffee Break with Letitia</a> | <a href=\"https://www.assemblyai.com/blog/how-imagen-actually-works/\">Assembly AI</a> | <a href=\"https://www.youtube.com/watch?v=af6WPqvzjjk\">Yannic Kilcher</a>\n\nPlease join <a href=\"https://discord.gg/xBPBXfcFHd\"><img alt=\"Join us on Discord\" src=\"https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&logoColor=white\"></a> if you are interested in helping out with the replication with the <a href=\"https://laion.ai/\">LAION</a> community\n\n## Shoutouts\n\n- <a href=\"https://stability.ai/\">StabilityAI</a> for the generous sponsorship, as well as my other sponsors out there\n\n- <a href=\"https://huggingface.co/\">ðŸ¤— Huggingface</a> for their amazing transformers library. The text encoder portion is pretty much taken care of because of them\n\n- <a href=\"http://www.jonathanho.me/\">Jonathan Ho</a> for bringing about a revolution in generative artificial intelligence through <a href=\"https://arxiv.org/abs/2006.11239\">his seminal paper</a>\n\n- <a href=\"https://github.com/sgugger\">Sylvain</a> and <a href=\"https://github.com/muellerzr\">Zachary</a> for the <a href=\"https://github.com/huggingface/accelerate\">Accelerate</a> library, which this repository uses for distributed training\n\n- <a href=\"https://github.com/arogozhnikov\">Alex</a> for <a href=\"https://github.com/arogozhnikov/einops\">einops</a>, indispensable tool for tensor manipulation\n\n- <a href=\"https://github.com/jorgemcgomes\">Jorge Gomes</a> for helping out with the T5 loading code and advice on the correct T5 version\n\n- <a href=\"https://github.com/crowsonkb\">Katherine Crowson</a>, for her <a href=\"https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/utils.py\">beautiful code</a>, which helped me understand the continuous time version of gaussian diffusion\n\n- <a href=\"https://github.com/marunine\">Marunine</a> and <a href=\"https://github.com/Netruk44\">Netruk44</a>, for reviewing code, sharing experimental results, and help with debugging\n\n- <a href=\"https://github.com/marunine\">Marunine</a> for providing a <a href=\"https://github.com/lucidrains/imagen-pytorch/issues/72#issuecomment-1163275757\">potential solution</a> for a color shifting issue in the memory efficient u-nets. Thanks to <a href=\"https://github.com/jacobwjs\">Jacob</a> for sharing experimental comparisons between the base and memory-efficient unets\n\n- <a href=\"https://github.com/marunine\">Marunine</a> for finding numerous bugs, resolving an issue with resize right, and for sharing his experimental configurations and results\n\n- <a href=\"https://github.com/MalumaDev\">MalumaDev</a> for proposing the use of pixel shuffle upsampler to fix checkboard artifacts\n\n- <a href=\"https://github.com/KhrulkovV\">Valentin</a> for pointing out insufficient skip connections in the unet, as well as the specific method of attention conditioning in the base-unet in the appendix\n\n- <a href=\"https://github.com/BIGJUN777\">BIGJUN</a> for catching a big bug with continuous time gaussian diffusion noise level conditioning at inference time\n\n- <a href=\"https://github.com/animebing\">Bingbing</a> for identifying a bug with sampling and order of normalizing and noising with low resolution conditioning image\n\n- <a href=\"https://github.com/TheFusion21\">Kay</a> for contributing one line command training of Imagen!\n\n- <a href=\"https://github.com/HReynaud\">Hadrien Reynaud</a> for testing out text-to-video on a medical dataset, sharing his results, and identifying issues!\n\n## Install\n\n```bash\n$ pip install imagen-pytorch\n```\n\n## Usage\n\n```python\nimport torch\nfrom imagen_pytorch import Unet, Imagen\n\n# unet for imagen\n\nunet1 = Unet(\n    dim = 32,\n    cond_dim = 512,\n    dim_mults = (1, 2, 4, 8),\n    num_resnet_blocks = 3,\n    layer_attns = (False, True, True, True),\n    layer_cross_attns = (False, True, True, True)\n)\n\nunet2 = Unet(\n    dim = 32,\n    cond_dim = 512,\n    dim_mults = (1, 2, 4, 8),\n    num_resnet_blocks = (2, 4, 8, 8),\n    layer_attns = (False, False, False, True),\n    layer_cross_attns = (False, False, False, True)\n)\n\n# imagen, which contains the unets above (base unet and super resoluting ones)\n\nimagen = Imagen(\n    unets = (unet1, unet2),\n    image_sizes = (64, 256),\n    timesteps = 1000,\n    cond_drop_prob = 0.1\n).cuda()\n\n# mock images (get a lot of this) and text encodings from large T5\n\ntext_embeds = torch.randn(4, 256, 768).cuda()\nimages = torch.randn(4, 3, 256, 256).cuda()\n\n# feed images into imagen, training each unet in the cascade\n\nfor i in (1, 2):\n    loss = imagen(images, text_embeds = text_embeds, unet_number = i)\n    loss.backward()\n\n# do the above for many many many many steps\n# now you can sample an image based on the text embeddings from the cascading ddpm\n\nimages = imagen.sample(texts = [\n    'a whale breaching from afar',\n    'young girl blowing out candles on her birthday cake',\n    'fireworks with blue and green sparkles'\n], cond_scale = 3.)\n\nimages.shape # (3, 3, 256, 256)\n```\n\nFor simpler training, you can directly supply text strings instead of precomputing text encodings. (Although for scaling purposes, you will definitely want to precompute the textual embeddings + mask)\n\nThe number of textual captions must match the batch size of the images if you go this route.\n\n```python\n# mock images and text (get a lot of this)\n\ntexts = [\n    'a child screaming at finding a worm within a half-eaten apple',\n    'lizard running across the desert on two feet',\n    'waking up to a psychedelic landscape',\n    'seashells sparkling in the shallow waters'\n]\n\nimages = torch.randn(4, 3, 256, 256).cuda()\n\n# feed images into imagen, training each unet in the cascade\n\nfor i in (1, 2):\n    loss = imagen(images, texts = texts, unet_number = i)\n    loss.backward()\n```\n\nWith the `ImagenTrainer` wrapper class, the exponential moving averages for all of the U-nets in the cascading DDPM will be automatically taken care of when calling `update`\n\n```python\nimport torch\nfrom imagen_pytorch import Unet, Imagen, ImagenTrainer\n\n# unet for imagen\n\nunet1 = Unet(\n    dim = 32,\n    cond_dim = 512,\n    dim_mults = (1, 2, 4, 8),\n    num_resnet_blocks = 3,\n    layer_attns = (False, True, True, True),\n)\n\nunet2 = Unet(\n    dim = 32,\n    cond_dim = 512,\n    dim_mults = (1, 2, 4, 8),\n    num_resnet_blocks = (2, 4, 8, 8),\n    layer_attns = (False, False, False, True),\n    layer_cross_attns = (False, False, False, True)\n)\n\n# imagen, which contains the unets above (base unet and super resoluting ones)\n\nimagen = Imagen(\n    unets = (unet1, unet2),\n    text_encoder_name = 't5-large',\n    image_sizes = (64, 256),\n    timesteps = 1000,\n    cond_drop_prob = 0.1\n).cuda()\n\n# wrap imagen with the trainer class\n\ntrainer = ImagenTrainer(imagen)\n\n# mock images (get a lot of this) and text encodings from large T5\n\ntext_embeds = torch.randn(64, 256, 1024).cuda()\nimages = torch.randn(64, 3, 256, 256).cuda()\n\n# feed images into imagen, training each unet in the cascade\n\nloss = trainer(\n    images,\n    text_embeds = text_embeds,\n    unet_number = 1,            # training on unet number 1 in this example, but you will have to also save checkpoints and then reload and continue training on unet number 2\n    max_batch_size = 4          # auto divide the batch of 64 up into batch size of 4 and accumulate gradients, so it all fits in memory\n)\n\ntrainer.update(unet_number = 1)\n\n# do the above for many many many many steps\n# now you can sample an image based on the text embeddings from the cascading ddpm\n\nimages = trainer.sample(texts = [\n    'a puppy looking anxiously at a giant donut on the table',\n    'the milky way galaxy in the style of monet'\n], cond_scale = 3.)\n\nimages.shape # (2, 3, 256, 256)\n```\n\nYou can also train Imagen without text (unconditional image generation) as follows\n\n```python\nimport torch\nfrom imagen_pytorch import Unet, Imagen, SRUnet256, ImagenTrainer\n\n# unets for unconditional imagen\n\nunet1 = Unet(\n    dim = 32,\n    dim_mults = (1, 2, 4),\n    num_resnet_blocks = 3,\n    layer_attns = (False, True, True),\n    layer_cross_attns = False,\n    use_linear_attn = True\n)\n\nunet2 = SRUnet256(\n    dim = 32,\n    dim_mults = (1, 2, 4),\n    num_resnet_blocks = (2, 4, 8),\n    layer_attns = (False, False, True),\n    layer_cross_attns = False\n)\n\n# imagen, which contains the unets above (base unet and super resoluting ones)\n\nimagen = Imagen(\n    condition_on_text = False,   # this must be set to False for unconditional Imagen\n    unets = (unet1, unet2),\n    image_sizes = (64, 128),\n    timesteps = 1000\n)\n\ntrainer = ImagenTrainer(imagen).cuda()\n\n# now get a ton of images and feed it through the Imagen trainer\n\ntraining_images = torch.randn(4, 3, 256, 256).cuda()\n\n# train each unet separately\n# in this example, only training on unet number 1\n\nloss = trainer(training_images, unet_number = 1)\ntrainer.update(unet_number = 1)\n\n# do the above for many many many many steps\n# now you can sample images unconditionally from the cascading unet(s)\n\nimages = trainer.sample(batch_size = 16) # (16, 3, 128, 128)\n```\n\nOr train only super-resoluting unets\n\n```python\nimport torch\nfrom imagen_pytorch import Unet, NullUnet, Imagen\n\n# unet for imagen\n\nunet1 = NullUnet()  # add a placeholder \"null\" unet for the base unet\n\nunet2 = Unet(\n    dim = 32,\n    cond_dim = 512,\n    dim_mults = (1, 2, 4, 8),\n    num_resnet_blocks = (2, 4, 8, 8),\n    layer_attns = (False, False, False, True),\n    layer_cross_attns = (False, False, False, True)\n)\n\n# imagen, which contains the unets above (base unet and super resoluting ones)\n\nimagen = Imagen(\n    unets = (unet1, unet2),\n    image_sizes = (64, 256),\n    timesteps = 250,\n    cond_drop_prob = 0.1\n).cuda()\n\n# mock images (get a lot of this) and text encodings from large T5\n\ntext_embeds = torch.randn(4, 256, 768).cuda()\nimages = torch.randn(4, 3, 256, 256).cuda()\n\n# feed images into imagen, training each unet in the cascade\n\nloss = imagen(images, text_embeds = text_embeds, unet_number = 2)\nloss.backward()\n\n# do the above for many many many many steps\n# now you can sample an image based on the text embeddings as well as low resolution images\n\nlowres_images = torch.randn(3, 3, 64, 64).cuda()  # starting un-resoluted images\n\nimages = imagen.sample(\n    texts = [\n        'a whale breaching from afar',\n        'young girl blowing out candles on her birthday cake',\n        'fireworks with blue and green sparkles'\n    ],\n    start_at_unet_number = 2,              # start at unet number 2\n    start_image_or_video = lowres_images,  # pass in low resolution images to be resoluted\n    cond_scale = 3.)\n\nimages.shape # (3, 3, 256, 256)\n```\n\nAt any time you can save and load the trainer and all associated states with the `save` and `load` methods. It is recommended you use these methods instead of manually saving with a `state_dict` call, as there are some device memory management being done underneath the hood within the trainer.\n\nex.\n\n```python\ntrainer.save('./path/to/checkpoint.pt')\n\ntrainer.load('./path/to/checkpoint.pt')\n\ntrainer.steps # (2,) step number for each of the unets, in this case 2\n```\n\n## Dataloader\n\nYou can also rely on the `ImagenTrainer` to automatically train off `DataLoader` instances. You simply have to craft your `DataLoader` to return either `images` (for unconditional case), or of `('images', 'text_embeds')` for text-guided generation.\n\nex. unconditional training\n\n```python\nfrom imagen_pytorch import Unet, Imagen, ImagenTrainer\nfrom imagen_pytorch.data import Dataset\n\n# unets for unconditional imagen\n\nunet = Unet(\n    dim = 32,\n    dim_mults = (1, 2, 4, 8),\n    num_resnet_blocks = 1,\n    layer_attns = (False, False, False, True),\n    layer_cross_attns = False\n)\n\n# imagen, which contains the unet above\n\nimagen = Imagen(\n    condition_on_text = False,  # this must be set to False for unconditional Imagen\n    unets = unet,\n    image_sizes = 128,\n    timesteps = 1000\n)\n\ntrainer = ImagenTrainer(\n    imagen = imagen,\n    split_valid_from_train = True # whether to split the validation dataset from the training\n).cuda()\n\n# instantiate your dataloader, which returns the necessary inputs to the DDPM as tuple in the order of images, text embeddings, then text masks. in this case, only images is returned as it is unconditional training\n\ndataset = Dataset('/path/to/training/images', image_size = 128)\n\ntrainer.add_train_dataset(dataset, batch_size = 16)\n\n# working training loop\n\nfor i in range(200000):\n    loss = trainer.train_step(unet_number = 1, max_batch_size = 4)\n    print(f'loss: {loss}')\n\n    if not (i % 50):\n        valid_loss = trainer.valid_step(unet_number = 1, max_batch_size = 4)\n        print(f'valid loss: {valid_loss}')\n\n    if not (i % 100) and trainer.is_main: # is_main makes sure this can run in distributed\n        images = trainer.sample(batch_size = 1, return_pil_images = True) # returns List[Image]\n        images[0].save(f'./sample-{i // 100}.png')\n\n```\n\n## Multi GPU\n\nThanks to <a href=\"https://huggingface.co/docs/accelerate/index\">ðŸ¤— Accelerate</a>, you can do multi GPU training easily with two steps.\n\nFirst you need to invoke `accelerate config` in the same directory as your training script (say it is named `train.py`)\n\n```bash\n$ accelerate config\n```\n\nNext, instead of calling `python train.py` as you would for single GPU, you would use the accelerate CLI as so\n\n```bash\n$ accelerate launch train.py\n```\n\nThat's it!\n\n## Command-line\n\nImagen can also be used via CLI directly.\n\n### Configuration\n\nex.\n\n```bash\n$ imagen config\n```\nor\n```bash\n$ imagen config --path ./configs/config.json\n```\n\nIn the config you are able to change settings for the trainer, dataset and the imagen config.\n\nThe Imagen config parameters can be found <a href=\"https://github.com/lucidrains/imagen-pytorch/blob/f8cc75f4d9020998c577b3770d3f260ce2ee2dcf/imagen_pytorch/configs.py#L68\">here</a>\n\nThe Elucidated Imagen config parameters can be found <a href=\"https://github.com/lucidrains/imagen-pytorch/blob/f8cc75f4d9020998c577b3770d3f260ce2ee2dcf/imagen_pytorch/configs.py#L108\">here</a>\n\nThe Imagen Trainer config parameters can be found <a href=\"https://github.com/lucidrains/imagen-pytorch/blob/f8cc75f4d9020998c577b3770d3f260ce2ee2dcf/imagen_pytorch/trainer.py#L226\">here</a>\n\nFor the dataset parameters all dataloader parameters can be used.\n\n### Training\n\nThis command allows you to train or resume training your model\n\nex.\n```bash\n$ imagen train\n```\nor\n```bash\n$ imagen train --unet 2 --epoches 10\n```\n\nYou can pass following arguments to the training command.\n\n- `--config` specify the config file to use for training [default: ./imagen_config.json]\n- `--unet` the index of the unet to train [default: 1]\n- `--epoches` how many epoches to train for [default: 50]\n\n### Sampling\n\nBe aware when sampling your checkpoint should have trained all unets to get a usable result.\n\nex.\n\n```bash\n$ imagen sample --model ./path/to/model/checkpoint.pt \"a squirrel raiding the birdfeeder\"\n# image is saved to ./a_squirrel_raiding_the_birdfeeder.png\n```\n\nYou can pass following arguments to the sample command.\n\n- `--model` specify the model file to use for sampling\n- `--cond_scale` conditioning scale (classifier free guidance) in decoder\n- `--load_ema` load EMA version of unets if available\n\nIn order to use a saved checkpoint with this feature, you either must instantiate your Imagen instance using the config classes, `ImagenConfig` and `ElucidatedImagenConfig` or create a checkpoint via the CLI directly\n\nFor proper training, you'll likely want to setup config-driven training anyways.\n\nex.\n\n```python\nimport torch\nfrom imagen_pytorch import ImagenConfig, ElucidatedImagenConfig, ImagenTrainer\n\n# in this example, using elucidated imagen\n\nimagen = ElucidatedImagenConfig(\n    unets = [\n        dict(dim = 32, dim_mults = (1, 2, 4, 8)),\n        dict(dim = 32, dim_mults = (1, 2, 4, 8))\n    ],\n    image_sizes = (64, 128),\n    cond_drop_prob = 0.5,\n    num_sample_steps = 32\n).create()\n\ntrainer = ImagenTrainer(imagen)\n\n# do your training ...\n\n# then save it\n\ntrainer.save('./checkpoint.pt')\n\n# you should see a message informing you that ./checkpoint.pt is commandable from the terminal\n```\n\nIt really should be as simple as that\n\nYou can also pass this checkpoint file around, and anyone can continue finetune on their own data\n\n```python\nfrom imagen_pytorch import load_imagen_from_checkpoint, ImagenTrainer\n\nimagen = load_imagen_from_checkpoint('./checkpoint.pt')\n\ntrainer = ImagenTrainer(imagen)\n\n# continue training / fine-tuning\n```\n\n## Inpainting\n\nInpainting follows the formulation laid out by the recent <a href=\"https://arxiv.org/abs/2201.09865\">Repaint paper</a>. Simply pass in `inpaint_images` and `inpaint_masks` to the `sample` function on either `Imagen` or `ElucidatedImagen`\n\n```python\n\ninpaint_images = torch.randn(4, 3, 512, 512).cuda()      # (batch, channels, height, width)\ninpaint_masks = torch.ones((4, 512, 512)).bool().cuda()  # (batch, height, width)\n\ninpainted_images = trainer.sample(texts = [\n    'a whale breaching from afar',\n    'young girl blowing out candles on her birthday cake',\n    'fireworks with blue and green sparkles',\n    'dust motes swirling in the morning sunshine on the windowsill'\n], inpaint_images = inpaint_images, inpaint_masks = inpaint_masks, cond_scale = 5.)\n\ninpainted_images # (4, 3, 512, 512)\n```\n\nFor video, similarly pass in your videos to `inpaint_videos` keyword on `.sample`. Inpainting mask can either be the same across all frames `(batch, height, width)` or different `(batch, frames, height, width)`\n\n```python\n\ninpaint_videos = torch.randn(4, 3, 8, 512, 512).cuda()   # (batch, channels, frames, height, width)\ninpaint_masks = torch.ones((4, 8, 512, 512)).bool().cuda()  # (batch, frames, height, width)\n\ninpainted_videos = trainer.sample(texts = [\n    'a whale breaching from afar',\n    'young girl blowing out candles on her birthday cake',\n    'fireworks with blue and green sparkles',\n    'dust motes swirling in the morning sunshine on the windowsill'\n], inpaint_videos = inpaint_videos, inpaint_masks = inpaint_masks, cond_scale = 5.)\n\ninpainted_videos # (4, 3, 8, 512, 512)\n```\n\n## Experimental\n\n<a href=\"https://research.nvidia.com/person/tero-karras\">Tero Karras</a> of StyleGAN fame has written a <a href=\"https://arxiv.org/abs/2206.00364\">new paper</a> with results that have been corroborated by a number of independent researchers as well as on my own machine. I have decided to create a version of `Imagen`, the `ElucidatedImagen`, so that one can use the new elucidated DDPM for text-guided cascading generation.\n\nSimply import `ElucidatedImagen`, and then instantiate the instance as you did before. The hyperparameters are different than the usual ones for discrete and continuous time gaussian diffusion, and can be individualized for each unet in the cascade.\n\nEx.\n\n```python\nfrom imagen_pytorch import ElucidatedImagen\n\n# instantiate your unets ...\n\nimagen = ElucidatedImagen(\n    unets = (unet1, unet2),\n    image_sizes = (64, 128),\n    cond_drop_prob = 0.1,\n    num_sample_steps = (64, 32), # number of sample steps - 64 for base unet, 32 for upsampler (just an example, have no clue what the optimal values are)\n    sigma_min = 0.002,           # min noise level\n    sigma_max = (80, 160),       # max noise level, @crowsonkb recommends double the max noise level for upsampler\n    sigma_data = 0.5,            # standard deviation of data distribution\n    rho = 7,                     # controls the sampling schedule\n    P_mean = -1.2,               # mean of log-normal distribution from which noise is drawn for training\n    P_std = 1.2,                 # standard deviation of log-normal distribution from which noise is drawn for training\n    S_churn = 80,                # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n    S_tmin = 0.05,\n    S_tmax = 50,\n    S_noise = 1.003,\n).cuda()\n\n# rest is the same as above\n\n```\n\n## Text to Video\n\nThis repository will also start accumulating new research around text guided video synthesis. For starters it will adopt the 3d unet architecture described by Jonathan Ho in <a href=\"https://arxiv.org/abs/2204.03458\">Video Diffusion Models</a>\n\nUpdate: verified <a href=\"https://github.com/lucidrains/imagen-pytorch/issues/305#issuecomment-1407015141\">working</a> by <a href=\"https://github.com/HReynaud\">Hadrien Reynaud</a>!\n\nEx.\n\n```python\nimport torch\nfrom imagen_pytorch import Unet3D, ElucidatedImagen, ImagenTrainer\n\nunet1 = Unet3D(dim = 64, dim_mults = (1, 2, 4, 8)).cuda()\n\nunet2 = Unet3D(dim = 64, dim_mults = (1, 2, 4, 8)).cuda()\n\n# elucidated imagen, which contains the unets above (base unet and super resoluting ones)\n\nimagen = ElucidatedImagen(\n    unets = (unet1, unet2),\n    image_sizes = (16, 32),\n    random_crop_sizes = (None, 16),\n    temporal_downsample_factor = (2, 1),        # in this example, the first unet would receive the video temporally downsampled by 2x\n    num_sample_steps = 10,\n    cond_drop_prob = 0.1,\n    sigma_min = 0.002,                          # min noise level\n    sigma_max = (80, 160),                      # max noise level, double the max noise level for upsampler\n    sigma_data = 0.5,                           # standard deviation of data distribution\n    rho = 7,                                    # controls the sampling schedule\n    P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n    P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n    S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n    S_tmin = 0.05,\n    S_tmax = 50,\n    S_noise = 1.003,\n).cuda()\n\n# mock videos (get a lot of this) and text encodings from large T5\n\ntexts = [\n    'a whale breaching from afar',\n    'young girl blowing out candles on her birthday cake',\n    'fireworks with blue and green sparkles',\n    'dust motes swirling in the morning sunshine on the windowsill'\n]\n\nvideos = torch.randn(4, 3, 10, 32, 32).cuda() # (batch, channels, time / video frames, height, width)\n\n# feed images into imagen, training each unet in the cascade\n# for this example, only training unet 1\n\ntrainer = ImagenTrainer(imagen)\n\n# you can also ignore time when training on video initially, shown to improve results in video-ddpm paper. eventually will make the 3d unet trainable with either images or video. research shows it is essential (with current data regimes) to train first on text-to-image. probably won't be true in another decade. all big data becomes small data\n\ntrainer(videos, texts = texts, unet_number = 1, ignore_time = False)\ntrainer.update(unet_number = 1)\n\nvideos = trainer.sample(texts = texts, video_frames = 20) # extrapolating to 20 frames from training on 10 frames\n\nvideos.shape # (4, 3, 20, 32, 32)\n\n```\n\nYou can also train on text - image pairs first. The `Unet3D` will automatically convert it to single framed videos and learn without the temporal components (by automatically setting `ignore_time = True`), whether it be 1d convolutions or causal attention across time.\n\nThis is the current approach taken by all the big artificial intelligence labs (Brain, MetaAI, Bytedance)\n\n## FAQ\n\n- Why are my generated images not aligning well with the text?\n\nImagen uses an algorithm called <a href=\"https://openreview.net/forum?id=qw8AKxfYbI\">Classifier Free Guidance</a>. When sampling, you apply a scale to the conditioning (text in this case) of greater than `1.0`.\n\nResearcher <a href=\"https://github.com/Netruk44 \">Netruk44</a> have reported `5-10` to be optimal, but anything greater than `10` to break.\n\n```python\ntrainer.sample(texts = [\n    'a cloud in the shape of a roman gladiator'\n], cond_scale = 5.) # <-- cond_scale is the conditioning scale, needs to be greater than 1.0 to be better than average\n```\n\n- Are there any pretrained models yet?\n\nNot at the moment but one will likely be trained and open sourced within the year, if not sooner. If you would like to participate, you can join the community of artificial neural network trainers at Laion (discord link is in the Readme above) and start collaborating.\n\n- Will this technology take my job?\n\nMore the reason why you should start training your own model, starting today! The last thing we need is this technology being in the hands of an elite few. Hopefully this repository reduces the work to just finding the necessary compute, and augmenting with your own curated dataset.\n\n- What am I allowed to do with this repository?\n\nAnything! It is MIT licensed. In other words, you can freely copy / paste for your own research, remixed for whatever modality you can think of. Go train amazing models for profit, for science, or simply to satiate your own personal pleasure at witnessing something divine unravel in front of you.\n\n## Cool Applications!\n\n- <a href=\"https://arxiv.org/abs/2303.12644\">Echocardiogram synthesis</a> <a href=\"https://github.com/HReynaud/EchoDiffusion\">[Code]</a>\n\n- <a href=\"https://www.biorxiv.org/content/10.1101/2023.10.25.564065v1\">SOTA Hi-C contact matrix synthesis</a> <a href=\"https://github.com/CHNFTQ/Capricorn\">[Code]</a>\n\n- <a href=\"https://arxiv.org/abs/2311.15941\">Floor plan generation</a>\n\n- <a href=\"https://arxiv.org/abs/2312.01152\">Ultra High Resolution Histopathology Slides</a>\n\n- <a href=\"https://arxiv.org/abs/2312.03043\">Synthetic Laparoscopic Images</a>\n\n- <a href=\"https://www.nature.com/articles/s42256-023-00762-x\">Designing MetaMaterials</a>\n\n## Related Works\n\n- <a href=\"https://github.com/archinetai/audio-diffusion-pytorch\">Audio diffusion</a> from <a href=\"https://github.com/flavioschneider\">Flavio Schneider</a>\n\n- <a href=\"https://github.com/AssemblyAI-Examples/MinImagen\">Mini Imagen</a> from <a href=\"https://github.com/oconnoob\">Ryan O.</a> | <a href=\"https://www.assemblyai.com/blog/build-your-own-imagen-text-to-image-model/\">AssemblyAI writeup</a>\n\n## Todo\n\n- [x] use huggingface transformers for T5-small text embeddings\n- [x] add dynamic thresholding\n- [x] add dynamic thresholding DALLE2 and video-diffusion repository as well\n- [x] allow for one to set T5-large (and perhaps small factory method to take in any huggingface transformer)\n- [x] add the lowres noise level with the pseudocode in appendix, and figure out what is this sweep they do at inference time\n- [x] port over some training code from DALLE2\n- [x] need to be able to use a different noise schedule per unet (cosine was used for base, but linear for SR)\n- [x] just make one master-configurable unet\n- [x] complete resnet block (biggan inspired? but with groupnorm) - complete self attention\n- [x] complete conditioning embedding block (and make it completely configurable, whether it be attention, film etc)\n- [x] consider using perceiver-resampler from https://github.com/lucidrains/flamingo-pytorch in place of attention pooling\n- [x] add attention pooling option, in addition to cross attention and film\n- [x] add optional cosine decay schedule with warmup, for each unet, to trainer\n- [x] switch to continuous timesteps instead of discretized, as it seems that is what they used for all stages - first figure out the linear noise schedule case from the variational ddpm paper https://openreview.net/forum?id=2LdBqxc1Yv\n- [x] figure out log(snr) for alpha cosine noise schedule.\n- [x] suppress the transformers warning because only T5encoder is used\n- [x] allow setting for using linear attention on layers where full attention cannot be used\n- [x] force unets in continuous time case to use non-fouriered conditions (just pass the log(snr) through an MLP with optional layernorms), as that is what i have working locally\n- [x] removed learned variance\n- [x] add p2 loss weighting for continuous time\n- [x] make sure cascading ddpm can be trained without text condition, and make sure both continuous and discrete time gaussian diffusion works\n- [x] use primer's depthwise convs on the qkv projections in linear attention (or use token shifting before projections) - also use new dropout proposed by bayesformer, as it seems to work well with linear attention\n- [x] explore skip layer excitation in unet decoder\n- [x] accelerate integration\n- [x] build out CLI tool and one-line generation of image\n- [x] knock out any issues that arised from accelerate\n- [x] add inpainting ability using resampler from repaint paper https://arxiv.org/abs/2201.09865\n- [x] build a simple checkpointing system, backed by a folder\n- [x] add skip connection from outputs of all upsample blocks, used in unet squared paper and some previous unet works\n- [x] add fsspec, recommended by Romain @rom1504, for cloud / local file system agnostic persistence of checkpoints\n- [x] test out persistence in gcs with https://github.com/fsspec/gcsfs\n- [x] extend to video generation, using axial time attention as in Ho's video ddpm paper\n- [x] allow elucidated imagen to generalize to any shape\n- [x] allow for imagen to generalize to any shape\n- [x] add <a href=\"https://github.com/lucidrains/x-transformers#dynamic-positional-bias\">dynamic positional bias</a> for the best type of length extrapolation across video time\n- [x] move video frames to sample function, as we will be attempting time extrapolation\n- [x] attention bias to null key / values should be a learned scalar of head dimension\n- [x] add self-conditioning from <a href=\"https://arxiv.org/abs/2208.04202\">bit diffusion</a> paper, already coded up at <a href=\"https://github.com/lucidrains/denoising-diffusion-pytorch/commit/beb2f2d8dd9b4f2bd5be4719f37082fe061ee450\">ddpm-pytorch</a>\n- [x] add v-parameterization (https://arxiv.org/abs/2202.00512) from <a href=\"https://imagen.research.google/video/paper.pdf\">imagen video</a> paper, the only thing new\n- [x] incorporate all learnings from make-a-video (https://makeavideo.studio/)\n- [x] build out CLI tool for training, resuming training off config file\n- [x] allow for temporal interpolation at specific stages\n- [x] make sure temporal interpolation works with inpainting\n- [x] make sure one can customize all interpolation modes (some researchers are finding better results with trilinear)\n- [x] imagen-video : allow for conditioning on preceding (and possibly future) frames of videos. ignore time should not be allowed in that scenario\n- [x] make sure to automatically take care of temporal down/upsampling for conditioning video frames, but allow for an option to turn it off\n- [x] make sure inpainting works with video\n- [x] make sure inpainting mask for video can accept be customized per frame\n\n- [ ] add flash attention\n- [ ] reread <a href=\"https://arxiv.org/abs/2205.15868\">cogvideo</a> and figure out how frame rate conditioning could be used\n- [ ] bring in attention expertise for self attention layers in unet3d\n- [ ] consider bringing in NUWA's 3d convolutional attention\n- [ ] consider transformer-xl memories in the temporal attention blocks\n- [ ] consider <a href=\"github.com/lucidrains/perceiver-ar-pytorch\">perceiver-ar approach</a> to attending to past time\n- [ ] frame dropouts during attention for achieving both regularizing effect as well as shortened training time\n- [ ] investigate frank wood's claims https://github.com/lucidrains/flexible-diffusion-modeling-videos-pytorch and either add the hierarchical sampling technique, or let people know about its deficiencies\n- [ ] offer challenging moving mnist (with distractor objects) as a one-line trainable baseline for researchers to branch off of for text to video\n- [ ] preencoding of text to memmapped embeddings\n- [ ] be able to create dataloader iterators based on the old epoch style, also configure shuffling etc\n- [ ] be able to also pass in arguments (instead of requiring forward to be all keyword args on model)\n- [ ] bring in reversible blocks from revnets for 3d unet, to lessen memory burden\n- [ ] add ability to only train super-resolution network\n- [ ] read <a href=\"https://arxiv.org/abs/2206.00927v1\">dpm-solver</a> see if it is applicable to continuous time gaussian diffusion\n- [ ] allow for conditioning video frames with arbitrary absolute times (calculate RPE during temporal attention)\n- [ ] accommodate <a href=\"https://dreambooth.github.io/\">dream booth</a> fine tuning\n- [ ] add textual inversion\n- [ ] cleanup self conditioning to be extracted at imagen instantiation\n- [ ] make sure eventual dreambooth works with imagen-video\n- [ ] add framerate conditioning for video diffusion\n- [ ] make sure one can simulataneously condition on video frames as a prompt, as well as some conditioning image across all frames\n- [ ] test and add distillation technique from <a href=\"https://arxiv.org/abs/2303.01469\">consistency models</a>\n\n## Citations\n\n```bibtex\n@inproceedings{Saharia2022PhotorealisticTD,\n    title   = {Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},\n    author  = {Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily L. Denton and Seyed Kamyar Seyed Ghasemipour and Burcu Karagol Ayan and Seyedeh Sara Mahdavi and Raphael Gontijo Lopes and Tim Salimans and Jonathan Ho and David Fleet and Mohammad Norouzi},\n    year    = {2022}\n}\n```\n\n```bibtex\n@article{Alayrac2022Flamingo,\n    title   = {Flamingo: a Visual Language Model for Few-Shot Learning},\n    author  = {Jean-Baptiste Alayrac et al},\n    year    = {2022}\n}\n```\n\n```bibtex\n@inproceedings{Sankararaman2022BayesFormerTW,\n    title   = {BayesFormer: Transformer with Uncertainty Estimation},\n    author  = {Karthik Abinav Sankararaman and Sinong Wang and Han Fang},\n    year    = {2022}\n}\n```\n\n```bibtex\n@article{So2021PrimerSF,\n    title   = {Primer: Searching for Efficient Transformers for Language Modeling},\n    author  = {David R. So and Wojciech Ma'nke and Hanxiao Liu and Zihang Dai and Noam M. Shazeer and Quoc V. Le},\n    journal = {ArXiv},\n    year    = {2021},\n    volume  = {abs/2109.08668}\n}\n```\n\n```bibtex\n@misc{cao2020global,\n    title   = {Global Context Networks},\n    author  = {Yue Cao and Jiarui Xu and Stephen Lin and Fangyun Wei and Han Hu},\n    year    = {2020},\n    eprint  = {2012.13375},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@article{Karras2022ElucidatingTD,\n    title   = {Elucidating the Design Space of Diffusion-Based Generative Models},\n    author  = {Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},\n    journal = {ArXiv},\n    year    = {2022},\n    volume  = {abs/2206.00364}\n}\n```\n\n```bibtex\n@inproceedings{NEURIPS2020_4c5bcfec,\n    author      = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},\n    booktitle   = {Advances in Neural Information Processing Systems},\n    editor      = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},\n    pages       = {6840--6851},\n    publisher   = {Curran Associates, Inc.},\n    title       = {Denoising Diffusion Probabilistic Models},\n    url         = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},\n    volume      = {33},\n    year        = {2020}\n}\n```\n\n```bibtex\n@article{Lugmayr2022RePaintIU,\n    title   = {RePaint: Inpainting using Denoising Diffusion Probabilistic Models},\n    author  = {Andreas Lugmayr and Martin Danelljan and Andr{\\'e}s Romero and Fisher Yu and Radu Timofte and Luc Van Gool},\n    journal = {ArXiv},\n    year    = {2022},\n    volume  = {abs/2201.09865}\n}\n```\n\n```bibtex\n@misc{ho2022video,\n    title   = {Video Diffusion Models},\n    author  = {Jonathan Ho and Tim Salimans and Alexey Gritsenko and William Chan and Mohammad Norouzi and David J. Fleet},\n    year    = {2022},\n    eprint  = {2204.03458},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{rogozhnikov2022einops,\n    title   = {Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation},\n    author  = {Alex Rogozhnikov},\n    booktitle = {International Conference on Learning Representations},\n    year    = {2022},\n    url     = {https://openreview.net/forum?id=oapKSVM2bcj}\n}\n```\n\n```bibtex\n@misc{chen2022analog,\n    title   = {Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning},\n    author  = {Ting Chen and Ruixiang Zhang and Geoffrey Hinton},\n    year    = {2022},\n    eprint  = {2208.04202},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```\n\n```bibtex\n@misc{Singer2022,\n    author  = {Uriel Singer},\n    url     = {https://makeavideo.studio/Make-A-Video.pdf}\n}\n```\n\n```bibtex\n@article{Sunkara2022NoMS,\n    title   = {No More Strided Convolutions or Pooling: A New CNN Building Block for Low-Resolution Images and Small Objects},\n    author  = {Raja Sunkara and Tie Luo},\n    journal = {ArXiv},\n    year    = {2022},\n    volume  = {abs/2208.03641}\n}\n```\n\n```bibtex\n@article{Salimans2022ProgressiveDF,\n    title   = {Progressive Distillation for Fast Sampling of Diffusion Models},\n    author  = {Tim Salimans and Jonathan Ho},\n    journal = {ArXiv},\n    year    = {2022},\n    volume  = {abs/2202.00512}\n}\n```\n\n```bibtex\n@article{Ho2022ImagenVH,\n    title   = {Imagen Video: High Definition Video Generation with Diffusion Models},\n    author  = {Jonathan Ho and William Chan and Chitwan Saharia and Jay Whang and Ruiqi Gao and Alexey A. Gritsenko and Diederik P. Kingma and Ben Poole and Mohammad Norouzi and David J. Fleet and Tim Salimans},\n    journal = {ArXiv},\n    year    = {2022},\n    volume  = {abs/2210.02303}\n}\n```\n\n```bibtex\n@misc{gilmer2023intriguing\n    title  = {Intriguing Properties of Transformer Training Instabilities},\n    author = {Justin Gilmer, Andrea Schioppa, and Jeremy Cohen},\n    year   = {2023},\n    status = {to be published - one attention stabilization technique is circulating within Google Brain, being used by multiple teams}\n}\n```\n\n```bibtex\n@inproceedings{Hang2023EfficientDT,\n    title   = {Efficient Diffusion Training via Min-SNR Weighting Strategy},\n    author  = {Tiankai Hang and Shuyang Gu and Chen Li and Jianmin Bao and Dong Chen and Han Hu and Xin Geng and Baining Guo},\n    year    = {2023}\n}\n```\n\n```bibtex\n@article{Zhang2021TokenST,\n    title   = {Token Shift Transformer for Video Classification},\n    author  = {Hao Zhang and Y. Hao and Chong-Wah Ngo},\n    journal = {Proceedings of the 29th ACM International Conference on Multimedia},\n    year    = {2021}\n}\n```\n\n```bibtex\n@inproceedings{anonymous2022normformer,\n    title   = {NormFormer: Improved Transformer Pretraining with Extra Normalization},\n    author  = {Anonymous},\n    booktitle = {Submitted to The Tenth International Conference on Learning Representations },\n    year    = {2022},\n    url     = {https://openreview.net/forum?id=GMYWzWztDx5},\n    note    = {under review}\n}\n```\n\n```bibtex\n@inproceedings{Sadat2024EliminatingOA,\n    title   = {Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models},\n    author  = {Seyedmorteza Sadat and Otmar Hilliges and Romann M. Weber},\n    year    = {2024},\n    url     = {https://api.semanticscholar.org/CorpusID:273098845}\n}\n```\n"
        },
        {
          "name": "imagen.png",
          "type": "blob",
          "size": 615.8759765625,
          "content": null
        },
        {
          "name": "imagen_pytorch",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.3623046875,
          "content": "from setuptools import setup, find_packages\nexec(open('imagen_pytorch/version.py').read())\n\nsetup(\n  name = 'imagen-pytorch',\n  packages = find_packages(exclude=[]),\n  include_package_data = True,\n  entry_points={\n    'console_scripts': [\n      'imagen_pytorch = imagen_pytorch.cli:main',\n      'imagen = imagen_pytorch.cli:imagen'\n    ],\n  },\n  version = __version__,\n  license='MIT',\n  description = 'Imagen - unprecedented photorealism Ã— deep level of language understanding',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  long_description_content_type = 'text/markdown',\n  url = 'https://github.com/lucidrains/imagen-pytorch',\n  keywords = [\n    'artificial intelligence',\n    'deep learning',\n    'transformers',\n    'text-to-image',\n    'denoising-diffusion'\n  ],\n  install_requires=[\n    'accelerate>=0.23.0',\n    'beartype',\n    'click',\n    'datasets',\n    'einops>=0.7.0',\n    'ema-pytorch>=0.0.3',\n    'fsspec',\n    'kornia',\n    'numpy',\n    'packaging',\n    'pillow',\n    'pydantic>=2',\n    'pytorch-warmup',\n    'sentencepiece',\n    'torch>=1.6',\n    'torchvision',\n    'transformers',\n    'tqdm'\n  ],\n  classifiers=[\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'License :: OSI Approved :: MIT License',\n    'Programming Language :: Python :: 3.6',\n  ],\n)\n"
        }
      ]
    }
  ]
}