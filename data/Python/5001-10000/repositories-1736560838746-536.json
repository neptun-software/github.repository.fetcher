{
  "metadata": {
    "timestamp": 1736560838746,
    "page": 536,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjU0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "THUDM/CogVLM",
      "stars": 6260,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".deepspeed_env",
          "type": "blob",
          "size": 0.0400390625,
          "content": "SAT_HOME=~/.sat_models\nLOCAL_WORLD_SIZE=8"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1455078125,
          "content": ".hypothesis/\n__pycache__\noutput.png\nfewshot-data/\ncheckpoints/\nrecords.db\nserver.py\nexamples/*grounding.png\narchive*\nhostfile\nruns/\n*.idea/\n.DS_Store"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0849609375,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2024 CogVLM team @ Zhipu AI\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
        },
        {
          "name": "MODEL_LICENSE",
          "type": "blob",
          "size": 5.0703125,
          "content": "The CogVLM License\n\n1. Definitions\n\n“Licensor” means the CogVLM Model Team that distributes its Software.\n\n“Software” means the CogVLM model parameters made available under this license.\n\n2. License Grant\n\nUnder the terms and conditions of this license, the Licensor hereby grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license.\nThis license permits you to use all open-source models in this repository for academic research free. Users who wish to use the models for commercial purposes must register [here](https://open.bigmodel.cn/mla/form).\nRegistered users may use the models for commercial activities free of charge, but must comply with all terms and conditions of this license.\nThe license notice shall be included in all copies or substantial portions of the Software.\n\n3. Restriction\n\nYou will not use, copy, modify, merge, publish, distribute, reproduce, or create derivative works of the Software, in whole or in part, for any military, or illegal purposes.\n\nYou will not use the Software for any act that may undermine China's national security and national unity, harm the public interest of society, or infringe upon the rights and interests of human beings.\n\n4. Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n5. Limitation of Liability\n\nEXCEPT TO THE EXTENT PROHIBITED BY APPLICABLE LAW, IN NO EVENT AND UNDER NO LEGAL THEORY, WHETHER BASED IN TORT, NEGLIGENCE, CONTRACT, LIABILITY, OR OTHERWISE WILL ANY LICENSOR BE LIABLE TO YOU FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES, OR ANY OTHER COMMERCIAL LOSSES, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n6. Dispute Resolution\n\nThis license shall be governed and construed in accordance with the laws of People’s Republic of China. Any dispute arising from or in connection with this License shall be submitted to Haidian District People's Court in Beijing.\n\nNote that the license is subject to update to a more comprehensive version.  For any questions related to the license and copyright, please contact us at license@zhipuai.cn.\n\n7. Llama2 and EVA-CLIP2 License\n\nFor CogVLM-17B version, Llama2 license conditions (https://ai.meta.com/llama/license/) and EVA license conditions (MIT, https://github.com/baaivision/EVA/blob/master/LICENSE) Also applies to model weights.\n\n\n1. 定义\n\n“许可方”是指分发其软件的 CogVLM 模型团队。\n\n“软件”是指根据本许可提供的 CogVLM 模型参数。\n\n2. 许可授予\n\n根据本许可的条款和条件，许可方特此授予您非排他性、全球性、不可转让、不可再许可、可撤销、免版税的版权许可。\n本许可允许您免费使用本仓库中的所有开源模型进行学术研究，对于希望将模型用于商业目的的用户，需在[这里](https://open.bigmodel.cn/mla/form)完成登记。\n经过登记的用户可以免费使用本模型进行商业活动，但必须遵守本许可的所有条款和条件。\n上述版权声明和本许可声明应包含在本软件的所有副本或重要部分中。\n\n3.限制\n\n您不得出于任何军事或非法目的使用、复制、修改、合并、发布、分发、复制或创建本软件的全部或部分衍生作品。\n\n您不得利用本软件从事任何危害国家安全和国家统一、危害社会公共利益、侵犯人身权益的行为。\n\n4.免责声明\n\n本软件“按原样”提供，不提供任何明示或暗示的保证，包括但不限于对适销性、特定用途的适用性和非侵权性的保证。 在任何情况下，作者或版权持有人均不对任何索赔、损害或其他责任负责，无论是在合同诉讼、侵权行为还是其他方面，由软件或软件的使用或其他交易引起、由软件引起或与之相关 软件。\n\n5. 责任限制\n\n除适用法律禁止的范围外，在任何情况下且根据任何法律理论，无论是基于侵权行为、疏忽、合同、责任或其他原因，任何许可方均不对您承担任何直接、间接、特殊、偶然、示范性、 或间接损害，或任何其他商业损失，即使许可人已被告知此类损害的可能性。\n\n6.争议解决\n\n本许可受中华人民共和国法律管辖并按其解释。 因本许可引起的或与本许可有关的任何争议应提交北京市海淀区人民法院。\n\n请注意，许可证可能会更新到更全面的版本。 有关许可和版权的任何问题，请通过 license@zhipuai.cn 与我们联系。\n\n7. Llama2 和 EVA-CLIP2 许可\n\n针对 CogVLM-17B 版本， Llama2 许可条件 (https://ai.meta.com/llama/license/) 和 EVA 许可条件 (MIT, https://github.com/baaivision/EVA/blob/master/LICENSE) 同时适用于模型权重。"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 27.92578125,
          "content": "# CogVLM & CogAgent\n\n📗 [中文版README](./README_zh.md)\n\n🌟 **Jump to detailed introduction: [Introduction to CogVLM](#introduction-to-cogvlm)，\n🆕 [Introduction to CogAgent](#introduction-to-cogagent)**\n\n📔 For more detailed usage information, please refer to: [CogVLM & CogAgent's technical documentation (in Chinese)](https://zhipu-ai.feishu.cn/wiki/LXQIwqo1OiIVTykMh9Lc3w1Fn7g) \n\n<table>\n  <tr>\n    <td>\n      <h2> CogVLM </h2>\n      <p> 📖  Paper: <a href=\"https://arxiv.org/abs/2311.03079\">CogVLM: Visual Expert for Pretrained Language Models</a></p>\n      <p><b>CogVLM</b> is a powerful open-source visual language model (VLM). CogVLM-17B has 10 billion visual parameters and 7 billion language parameters, <b>supporting image understanding and multi-turn dialogue with a resolution of 490*490</b>.</p>\n      <p><b>CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks</b>, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC.</p>\n    </td>\n    <td>\n      <h2> CogAgent </h2>\n      <p> 📖  Paper: <a href=\"https://arxiv.org/abs/2312.08914\">CogAgent: A Visual Language Model for GUI Agents </a></p>\n      <p><b>CogAgent</b> is an open-source visual language model improved based on CogVLM. CogAgent-18B has 11 billion visual parameters and 7 billion language parameters, <b>supporting image understanding at a resolution of 1120*1120</b>. <b>On top of the capabilities of CogVLM, it further possesses GUI image Agent capabilities</b>.</p>\n      <p> <b>CogAgent-18B achieves state-of-the-art generalist performance on 9 classic cross-modal benchmarks</b>, including VQAv2, OK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. <b>It significantly surpasses existing models on GUI operation datasets</b> including AITW and Mind2Web.</p>\n    </td>\n  </tr>\n  <tr>\n    <td colspan=\"2\" align=\"center\">\n      <p>🌐 Web Demo for both CogVLM2: <a href=\"http://36.103.203.44:7861\">this link</a></p>\n    </td>\n  </tr>\n</table>\n\n\n**Table of Contents**\n\n- [CogVLM \\& CogAgent](#cogvlm--cogagent)\n    - [Release](#release)\n    - [Get Started](#get-started)\n        - [Option 1: Inference Using Web Demo.](#option-1-inference-using-web-demo)\n        - [Option 2：Deploy CogVLM / CogAgent by yourself](#option-2deploy-cogvlm--cogagent-by-yourself)\n            - [Situation 2.1 CLI (SAT version)](#situation-21-cli-sat-version)\n            - [Situation 2.2 CLI (Huggingface version)](#situation-22-cli-huggingface-version)\n            - [Situation 2.3 Web Demo](#situation-23-web-demo)\n        - [Option 3：Finetuning CogAgent / CogVLM](#option-3finetuning-cogagent--cogvlm)\n        - [Option 4: OpenAI Vision format](#option-4-openai-vision-format)\n        - [Hardware requirement](#hardware-requirement)\n        - [Model checkpoints](#model-checkpoints)\n    - [Introduction to CogVLM](#introduction-to-cogvlm)\n        - [Examples](#examples)\n    - [Introduction to CogAgent](#introduction-to-cogagent)\n        - [GUI Agent Examples](#gui-agent-examples)\n    - [Cookbook](#cookbook)\n        - [Task Prompts](#task-prompts)\n        - [Which --version to use](#which---version-to-use)\n        - [FAQ](#faq)\n    - [License](#license)\n    - [Citation \\& Acknowledgements](#citation--acknowledgements)\n\n## Release\n- 🔥🔥🔥  **News**: ```2024/5/20```: We released the **next generation of model, [CogVLM2](https://github.com/THUDM/CogVLM2)**, which is based on llama3-8b and on the par of (or better than) GPT-4V in most cases! DOWNLOAD and TRY!\n- 🔥🔥  **News**: ```2024/4/5```: [CogAgent](https://arxiv.org/abs/2312.08914) was selected as a CVPR 2024 Highlights!\n- 🔥  **News**: ```2023/12/26```: We have released the [CogVLM-SFT-311K](dataset.md) dataset, \n  which contains over 150,000 pieces of data that we used for **CogVLM v1.0 only** training. Welcome to follow and use.\n- **News**: ```2023/12/18```: **New Web UI Launched!** We have launched a new web UI based on Streamlit,\n  users can painlessly talk to CogVLM, CogAgent in our UI. Have a better user experience.\n- **News**: ```2023/12/15```: **CogAgent Officially Launched!** CogAgent is an image understanding model developed\n  based on CogVLM. It features **visual-based GUI Agent capabilities** and has further enhancements in image\n  understanding. It supports image input with a resolution of 1120*1120, and possesses multiple abilities including\n  multi-turn dialogue with images, GUI Agent, Grounding, and more.\n\n- **News**: ```2023/12/8``` We have updated the checkpoint of cogvlm-grounding-generalist to\n  cogvlm-grounding-generalist-v1.1, with image augmentation during training, therefore more robust.\n  See [details](#introduction-to-cogvlm).\n\n- **News**: ```2023/12/7``` CogVLM supports **4-bit quantization** now! You can inference with just **11GB** GPU memory!\n\n- **News**: ```2023/11/20``` We have updated the checkpoint of cogvlm-chat to cogvlm-chat-v1.1, unified the versions of\n  chat and VQA, and refreshed the SOTA on various datasets. See [details](#introduction-to-cogvlm)\n\n- **News**: ```2023/11/20``` We release **[cogvlm-chat](https://huggingface.co/THUDM/cogvlm-chat-hf)**, **[cogvlm-grounding-generalist](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)/[base](https://huggingface.co/THUDM/cogvlm-grounding-base-hf)**, **[cogvlm-base-490](https://huggingface.co/THUDM/cogvlm-base-490-hf)/[224](https://huggingface.co/THUDM/cogvlm-base-224-hf)** on 🤗Huggingface. you can infer with transformers in [a few lines of code](#situation-22-cli-huggingface-version)now!\n\n- ```2023/10/27``` CogVLM bilingual version is available [online](https://chatglm.cn/)! Welcome to try it out!\n\n- ```2023/10/5``` CogVLM-17B released。\n\n## Get Started\n\n### Option 1: Inference Using Web Demo.\n\n* Click here to enter [CogVLM2 Demo](http://36.103.203.44:7861/)。\n\nIf you need to use Agent and Grounding functions, please refer to [Cookbook - Task Prompts](#task-prompts)\n\n### Option 2：Deploy CogVLM / CogAgent by yourself\n\nWe support two GUIs for model inference, **CLI** and **web demo** . If you want to use it in your python code, it is\neasy to modify the CLI scripts for your case.\n\nFirst, we need to install the dependencies.\n\n```bash\n# CUDA >= 11.8\npip install -r requirements.txt\npython -m spacy download en_core_web_sm\n```\n\n**All code for inference is located under the ``basic_demo/`` directory. Please switch to this directory first before\nproceeding with further operations.**\n\n#### Situation 2.1 CLI (SAT version)\n\nRun CLI demo via:\n\n```bash\n# CogAgent\npython cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16  --stream_chat\npython cli_demo_sat.py --from_pretrained cogagent-vqa --version chat_old --bf16  --stream_chat\n\n# CogVLM\npython cli_demo_sat.py --from_pretrained cogvlm-chat --version chat_old --bf16  --stream_chat\npython cli_demo_sat.py --from_pretrained cogvlm-grounding-generalist --version base --bf16  --stream_chat\n```\n\nThe program will automatically download the sat model and interact in the command line. You can generate replies by\nentering instructions and pressing enter.\nEnter `clear` to clear the conversation history and `stop` to stop the program.\n\nWe also support model parallel inference, which splits model to multiple (2/4/8) GPUs. `--nproc-per-node=[n]` in the\nfollowing command controls the number of used GPUs.\n\n```\ntorchrun --standalone --nnodes=1 --nproc-per-node=2 cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16\n```\n\n- If you want to manually download the weights, you can replace the path after ``--from_pretrained`` with the model\n  path.\n\n- Our model supports SAT's **4-bit quantization** and **8-bit quantization**.\n  You can change ``--bf16`` to ``--fp16``, or ``--fp16 --quant 4``, or ``--fp16 --quant 8``.\n\n  For example\n\n    ```bash\n    python cli_demo_sat.py --from_pretrained cogagent-chat --fp16 --quant 8 --stream_chat\n    python cli_demo_sat.py --from_pretrained cogvlm-chat-v1.1 --fp16 --quant 4 --stream_chat\n    # In SAT version，--quant should be used with --fp16\n    ```\n\n- The program provides the following hyperparameters to control the generation process:\n    ```\n    usage: cli_demo_sat.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE]\n\n    optional arguments:\n    -h, --help            show this help message and exit\n    --max_length MAX_LENGTH\n                            max length of the total sequence\n    --top_p TOP_P         top p for nucleus sampling\n    --top_k TOP_K         top k for top k sampling\n    --temperature TEMPERATURE\n                            temperature for sampling\n    ```\n\n- Click [here](#which---version-to-use) to view the correspondence between different models and the ``--version``\n  parameter.\n\n#### Situation 2.2 CLI (Huggingface version)\n\nRun CLI demo via:\n\n```bash\n# CogAgent\npython cli_demo_hf.py --from_pretrained THUDM/cogagent-chat-hf --bf16\npython cli_demo_hf.py --from_pretrained THUDM/cogagent-vqa-hf --bf16\n\n# CogVLM\npython cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --bf16\npython cli_demo_hf.py --from_pretrained THUDM/cogvlm-grounding-generalist-hf --bf16\n```\n\n- If you want to manually download the weights, you can replace the path after ``--from_pretrained`` with the model\n  path.\n\n- You can change ``--bf16`` to ``--fp16``, or ``--quant 4``. For example, our model supports Huggingface's **4-bit\n  quantization**:\n\n    ```bash\n    python cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --quant 4\n    ```\n\n#### Situation 2.3 Web Demo\n\nWe also offer a local web demo based on Gradio. First, install Gradio by running: `pip install gradio`. Then download\nand enter this repository and run `web_demo.py`. See the next section for detailed usage:\n\n```bash\npython web_demo.py --from_pretrained cogagent-chat --version chat --bf16\npython web_demo.py --from_pretrained cogagent-vqa --version chat_old --bf16\npython web_demo.py --from_pretrained cogvlm-chat-v1.1 --version chat_old --bf16\npython web_demo.py --from_pretrained cogvlm-grounding-generalist --version base --bf16\n```\n\nThe GUI of the web demo looks like:\n\n<div align=\"center\">\n    <img src=assets/web_demo-min.png width=70% />\n</div>\n\n### Option 3：Finetuning CogAgent / CogVLM\n\nYou may want to use CogVLM in your own task, which needs a **different output style or domain knowledge**. **All code\nfor finetuning is located under the ``finetune_demo/`` directory.**\n\nWe here provide a finetuning example for **Captcha Recognition** using lora.\n\n1. Start by downloading the [Captcha Images dataset](https://www.kaggle.com/datasets/aadhavvignesh/captcha-images). Once\n   downloaded, extract the contents of the ZIP file.\n\n2. To create a train/validation/test split in the ratio of 80/5/15, execute the following:\n    ```bash\n    python utils/split_dataset.py\n    ```\n\n3. Start the fine-tuning process with this command:\n\n    ```bash\n    bash finetune_demo/finetune_(cogagent/cogvlm)_lora.sh\n    ```\n\n4. Merge the model to `model_parallel_size=1`: (replace the 4 below with your training `MP_SIZE`)\n\n    ```bash\n    torchrun --standalone --nnodes=1 --nproc-per-node=4 utils/merge_model.py --version base --bf16 --from_pretrained ./checkpoints/merged_lora_(cogagent/cogvlm490/cogvlm224)\n    ```\n\n5. Evaluate the performance of your model.\n    ```bash\n    bash finetune_demo/evaluate_(cogagent/cogvlm).sh\n    ```\n\n### Option 4: OpenAI Vision format\n\nWe provide the same API examples as `GPT-4V`, which you can view in `openai_demo`.\n\n1. First, start the node\n\n```\npython openai_demo/openai_api.py\n```\n\n2. Next, run the request example node, which is an example of a continuous dialogue\n\n```\npython openai_demo/openai_api_request.py\n```\n\n3. You will get output similar to the following\n\n```\nThis image showcases a tranquil natural scene with a wooden pathway leading through a field of lush green grass. In the distance, there are trees and some scattered structures, possibly houses or small buildings. The sky is clear with a few scattered clouds, suggesting a bright and sunny day.\n```\n\n### Hardware requirement\n\n* Model Inference:\n\n  For INT4 quantization: 1 * RTX 3090(24G)   (CogAgent takes ~ 12.6GB, CogVLM takes ~ 11GB)\n\n  For FP16: 1 * A100(80G) or 2 * RTX 3090(24G)\n\n* Finetuning:\n\n  For FP16: 4 * A100(80G) *[Recommend]* or 8* RTX 3090(24G).\n\n### Model checkpoints\n\nIf you run the `basic_demo/cli_demo*.py` from the code repository, it will automatically download SAT or Hugging Face\nweights. Alternatively, you can choose to manually download the necessary weights.\n\n- CogAgent\n\n  |   Model name    | Input resolution |                             Introduction                             | Huggingface model | SAT model |\n  | :-----------: | :----: | :----------------------------------------------------------: | :------: | :-------: |\n  | cogagent-chat |  1120  | Chat version of CogAgent. Supports GUI Agent, multiple-round  chat and visual grounding. |  [HF link](https://huggingface.co/THUDM/cogagent-chat-hf) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/cogagent-chat-hf)    |   [HF link](https://huggingface.co/THUDM/CogAgent/tree/main)<br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/CogAgent)           |\n  | cogagent-vqa |  1120  | VQA version of CogAgent. Has stronger capabilities in single-turn visual dialogue. Recommended for VQA benchmarks. |  [HF link](https://huggingface.co/THUDM/cogagent-vqa-hf)<br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/cogagent-vqa-hf)        |    [HF link](https://huggingface.co/THUDM/CogAgent/tree/main) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/CogAgent)      |\nc\n- CogVLM\n\n  |          Model name           | Input resolution |                           Introduction                            | Huggingface model | SAT model |\n  | :-------------------------: | :----: | :-------------------------------------------------------: | :------: | :-------: |\n  |         cogvlm-chat-v1.1         |  490   |  Supports multiple rounds of chat and vqa simultaneously, with different prompts.   |  [HF link](https://huggingface.co/THUDM/cogvlm-chat-hf) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/cogvlm-chat-hf)        |    [HF link](https://huggingface.co/THUDM/CogVLM/tree/main)  <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/CogVLM)       |\n  |       cogvlm-base-224       |  224   |               The original checkpoint after text-image pretraining.               |   [HF link](https://huggingface.co/THUDM/cogvlm-base-224-hf) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/cogvlm-base-224-hf)       |     [HF link](https://huggingface.co/THUDM/CogVLM/tree/main) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/CogVLM)       |\n  |       cogvlm-base-490       |  490   |      Amplify the resolution to 490 through position encoding interpolation from `cogvlm-base-224`.      |   [HF link](https://huggingface.co/THUDM/cogvlm-base-490-hf) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/cogvlm-base-490-hf)       |     [HF link](https://huggingface.co/THUDM/CogVLM/tree/main) <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/CogVLM)       |\n  | cogvlm-grounding-generalist |  490   | This checkpoint supports different visual grounding tasks, e.g. REC, Grounding Captioning, etc.  |    [HF link](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)  <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/cogvlm-grounding-generalist-hf)       |     [HF link](https://huggingface.co/THUDM/CogVLM/tree/main)   <br> [OpenXLab link](https://openxlab.org.cn/models/detail/THUDM/CogVLM)     |\n\n## Introduction to CogVLM\n\n- CogVLM is a powerful **open-source visual language model** (**VLM**). CogVLM-17B has 10 billion vision parameters and\n  7 billion language parameters.\n\n- CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k\n  captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and rank the 2nd on VQAv2,\n  OKVQA, TextVQA, COCO captioning, etc., **surpassing or matching PaLI-X 55B**. CogVLM can\n  also [chat with you](http://36.103.203.44:7861) about images.\n\n<div align=\"center\">\n    <img src=assets/metrics-min.png width=50% />\n</div>\n\n<details>\n<summary>Click to view results on MM-VET, POPE, TouchStone. </summary>\n\n<table>\n    <tr>\n        <td>Method</td>\n        <td>LLM</td>\n        <td>MM-VET</td>\n        <td>POPE(adversarial)</td>\n        <td>TouchStone</td>\n    </tr>\n    <tr>\n        <td>BLIP-2</td>\n        <td>Vicuna-13B</td>\n        <td>22.4</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Otter</td>\n        <td>MPT-7B</td>\n        <td>24.7</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>MiniGPT4</td>\n        <td>Vicuna-13B</td>\n        <td>24.4</td>\n        <td>70.4</td>\n        <td>531.7</td>\n    </tr>\n    <tr>\n        <td>InstructBLIP</td>\n        <td>Vicuna-13B</td>\n        <td>25.6</td>\n        <td>77.3</td>\n        <td>552.4</td>\n    </tr>\n    <tr>\n        <td>LLaMA-Adapter v2</td>\n        <td>LLaMA-7B</td>\n        <td>31.4</td>\n        <td>-</td>\n        <td>590.1</td>\n    </tr>\n    <tr>\n        <td>LLaVA</td>\n        <td>LLaMA2-7B</td>\n        <td>28.1</td>\n        <td>66.3</td>\n        <td>602.7</td>\n    </tr>\n    <tr>\n        <td>mPLUG-Owl</td>\n        <td>LLaMA-7B</td>\n        <td>-</td>\n        <td>66.8</td>\n        <td>605.4</td>\n    </tr>\n    <tr>\n        <td>LLaVA-1.5</td>\n        <td>Vicuna-13B</td>\n        <td>36.3</td>\n        <td>84.5</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Emu</td>\n        <td>LLaMA-13B</td>\n        <td>36.3</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Qwen-VL-Chat</td>\n        <td>-</td>\n        <td>-</td>\n        <td>-</td>\n        <td>645.2</td>\n    </tr>\n    <tr>\n        <td>DreamLLM</td>\n        <td>Vicuna-7B</td>\n        <td>35.9</td>\n        <td>76.5</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>CogVLM</td>\n        <td>Vicuna-7B</td>\n        <td> <b>52.8</b> </td>\n        <td><b>87.6</b></td>\n        <td><b>742.0</b></td>\n    </tr>\n</table>\n\n</details>\n\n<details>\n<summary>Click to view results of cogvlm-grounding-generalist-v1.1. </summary>\n\n<table>\n    <tr>\n        <td></td>\n        <td>RefCOCO</td>\n        <td></td>\n        <td></td>\n        <td>RefCOCO+</td>\n        <td></td>\n        <td></td>\n        <td>RefCOCOg</td>\n        <td></td>\n        <td>Visual7W</td>\n    </tr>\n    <tr>\n        <td></td>\n        <td>val</td>\n        <td>testA</td>\n        <td>testB</td>\n        <td>val</td>\n        <td>testA</td>\n        <td>testB</td>\n        <td>val</td>\n        <td>test</td>\n        <td>test</td>\n    </tr>\n    <tr>\n        <td>cogvim-grounding-generalist</td>\n        <td>92.51</td>\n        <td>93.95</td>\n        <td>88.73</td>\n        <td>87.52</td>\n        <td>91.81</td>\n        <td>81.43</td>\n        <td>89.46</td>\n        <td>90.09</td>\n        <td>90.96</td>\n    </tr>\n    <tr>\n        <td>cogvim-grounding-generalist-v1.1</td>\n        <td>**92.76**</td>\n        <td>**94.75**</td>\n        <td>**88.99**</td>\n        <td>**88.68**</td>\n        <td>**92.91**</td>\n        <td>**83.39**</td>\n        <td>**89.75**</td>\n        <td>**90.79**</td>\n        <td>**91.05**</td>\n    </tr>\n</table>\n</details>\n\n### Examples\n\n<!-- CogVLM is powerful for answering various types of visual questions, including **Detailed Description & Visual Question Answering**,  **Complex Counting**, **Visual Math Problem Solving**, **OCR-Free Reasonging**, **OCR-Free Visual Question Answering**, **World Knowledge**, **Referring Expression Comprehension**, **Programming with Visual Input**, **Grounding with Caption**, **Grounding Visual Question Answering**, etc. -->\n\n* CogVLM can accurately describe images in details with **very few hallucinations**.\n    <details>\n    <summary>Click for comparison with LLAVA-1.5 and MiniGPT-4.</summary>\n\n    <img src=assets/llava-comparison-min.png width=50% />\n\n    </details>\n    <br>\n\n* CogVLM can understand and answer various types of questions, and has a **visual grounding** version.\n\n<div align=\"center\">\n    <img src=assets/pear_grounding.png width=50% />\n</div>\n\n<br>\n\n* CogVLM sometimes captures more detailed content than GPT-4V(ision).\n\n<div align=\"center\">\n    <img src=assets/compare-min.png width=50% />\n</div>\n\n<!-- ![compare](assets/compare.png) -->\n<br> \n\n<details>\n<summary>Click to expand more examples.</summary>\n\n![Chat Examples](assets/chat.png)\n\n</details>\n\n## Introduction to CogAgent\n\nCogAgent is an open-source visual language model improved based on CogVLM. CogAgent-18B has 11 billion visual parameters\nand 7 billion language parameters\n\nCogAgent-18B achieves state-of-the-art generalist performance on 9 classic cross-modal benchmarks, including VQAv2,\nOK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. It significantly surpasses existing models on GUI\noperation datasets such as AITW and Mind2Web.\n\nIn addition to all the features already present in CogVLM (visual multi-round dialogue, visual grounding), CogAgent:\n\n1. Supports higher resolution visual input and dialogue question-answering. **It supports ultra-high-resolution image\n   inputs of 1120x1120.**\n\n2. **Possesses the capabilities of a visual Agent**, being able to return a plan, next action, and specific operations\n   with coordinates for any given task on any GUI screenshot.\n\n3. **Enhanced GUI-related question-answering capabilities**, allowing it to handle questions about any GUI screenshot,\n   such as web pages, PC apps, mobile applications, etc.\n\n4. Enhanced capabilities in OCR-related tasks through improved pre-training and fine-tuning.\n\n<div align=\"center\">\n    <img src=assets/cogagent_function.jpg width=60% />\n</div>\n\n### GUI Agent Examples\n\n<div align=\"center\">\n    <img src=assets/cogagent_main_demo.jpg width=90% />\n</div>\n\n## Cookbook\n\n### Task Prompts\n\n1. **General Multi-Round Dialogue**: Say whatever you want.\n\n2. **GUI Agent Task**: Use the [Agent template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L761)\n   and replace \\<TASK\\> with the task instruction enclosed in double quotes. This query can make CogAgent infer Plan and\n   Next Action. If adding ``(with grounding)`` at the end of the query, the model will return a formalized action\n   representation with coordinates.\n\nFor example, to ask the model how to complete the task \"Search for CogVLM\" on a current GUI screenshot, follow these\nsteps:\n\n1. Randomly select a template from\n   the [Agent template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L761). Here, we\n   choose ``What steps do I need to take to <TASK>?``.\n\n2. Replace <TASK> with the task instruction enclosed in double quotes, for\n   example, ``What steps do I need to take to \"Search for CogVLM\"?`` . Inputting this to the model yields:\n\n> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant\n> result to read more about CogVLM or access further resources.\n>\n> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.\n\n3. If adding ``(with grounding)`` at the end, i.e. changing the input\n   to ``What steps do I need to take to \"Search for CogVLM\"?(with grounding)``, the output of CogAgent would be:\n\n> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant\n> result to read more about CogVLM or access further resources.\n>\n> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.\n> Grounded Operation:[combobox] Search -> TYPE: CogVLM at the box [[212,498,787,564]]\n\nTip: For GUI Agent tasks, it is recommended to conduct only single-round dialogues for each image for better results.\n\n3. **Visual Grounding**. Three modes of grounding are supported:\n\n    - Image description with grounding coordinates (bounding box). Use any template\n      from [caption_with_box template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L537) as model\n      input. For example:\n\n   > Can you provide a description of the image and include the coordinates [[x0,y0,x1,y1]] for each mentioned object?\n\n    - Returning grounding coordinates (bounding box) based on the description of objects. Use any template\n      from [caption2box template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L345),\n      replacing ``<expr>`` with the object's description. For example:\n\n   > Can you point out *children in blue T-shirts* in the image and provide the bounding boxes of their location?\n\n    - Providing a description based on bounding box coordinates. Use a template\n      from [box2caption template](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L400),\n      replacing ``<objs>`` with the position coordinates. For example:\n\n   > Tell me what you see within the designated area *[[086,540,400,760]]* in the picture.\n\n**Format of coordination:** The bounding box coordinates in the model's input and output use the\nformat ``[[x1, y1, x2, y2]]``, with the origin at the top left corner, the x-axis to the right, and the y-axis\ndownward. (x1, y1) and (x2, y2) are the top-left and bottom-right corners, respectively, with values as relative\ncoordinates multiplied by 1000 (prefixed with zeros to three digits).\n\n### Which --version to use\n\nDue to differences in model functionalities, different model versions may have distinct ``--version`` specifications for\nthe text processor, meaning the format of the prompts used varies.\n\n|         model name          | --version |\n|:---------------------------:|:---------:|\n|        cogagent-chat        |   chat    |\n|        cogagent-vqa         | chat_old  |\n|         cogvlm-chat         | chat_old  |\n|      cogvlm-chat-v1.1       | chat_old  |\n| cogvlm-grounding-generalist |   base    |\n|       cogvlm-base-224       |   base    |\n|       cogvlm-base-490       |   base    |\n\n### FAQ\n\n* If you have trouble in accessing huggingface.co, you can add `--local_tokenizer /path/to/vicuna-7b-v1.5` to load the\n  tokenizer.\n* If you have trouble in automatically downloading model with 🔨[SAT](https://github.com/THUDM/SwissArmyTransformer), try\n  downloading from 🤖[modelscope](https://www.modelscope.cn/models/ZhipuAI/CogVLM/summary) or\n  🤗[huggingface](https://huggingface.co/THUDM/CogVLM) or 💡[wisemodel](https://www.wisemodel.cn/models/ZhipuAI/CogVLM)\n  manually.\n* Download model using 🔨[SAT](https://github.com/THUDM/SwissArmyTransformer), the model will be saved to the default\n  location `~/.sat_models`. Change the default location by setting the environment variable `SAT_HOME`. For example, if\n  you want to save the model to `/path/to/my/models`, you can run `export SAT_HOME=/path/to/my/models` before running\n  the python command.\n\n## License\n\nThe code in this repository is open source under the [Apache-2.0 license](./LICENSE), while the use of the CogVLM model\nweights must comply with the [Model License](./MODEL_LICENSE).\n\n## Citation & Acknowledgements\n\nIf you find our work helpful, please consider citing the following papers\n\n```\n@misc{wang2023cogvlm,\n      title={CogVLM: Visual Expert for Pretrained Language Models}, \n      author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},\n      year={2023},\n      eprint={2311.03079},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n@misc{hong2023cogagent,\n      title={CogAgent: A Visual Language Model for GUI Agents}, \n      author={Wenyi Hong and Weihan Wang and Qingsong Lv and Jiazheng Xu and Wenmeng Yu and Junhui Ji and Yan Wang and Zihan Wang and Yuxiao Dong and Ming Ding and Jie Tang},\n      year={2023},\n      eprint={2312.08914},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n```\n\nIn the instruction fine-tuning phase of the CogVLM, there are some English image-text data from\nthe [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4), [LLAVA](https://github.com/haotian-liu/LLaVA), [LRV-Instruction](https://github.com/FuxiaoLiu/LRV-Instruction), [LLaVAR](https://github.com/SALT-NLP/LLaVAR)\nand [Shikra](https://github.com/shikras/shikra) projects, as well as many classic cross-modal work datasets. We\nsincerely thank them for their contributions.\n"
        },
        {
          "name": "README_zh.md",
          "type": "blob",
          "size": 25.0830078125,
          "content": "# CogVLM & CogAgent\n\n📗 [README in English](./README.md)\n\n🌟 **跳转到详细介绍: [CogVLM介绍](#introduction-to-cogvlm)，\n🆕 [CogAgent的介绍](#introduction-to-cogagent)**\n\n📔 如需获取更详细的使用信息，请参阅: [CogVLM&CogAgent技术文档](https://zhipu-ai.feishu.cn/wiki/LXQIwqo1OiIVTykMh9Lc3w1Fn7g)\n\n<table>\n  <tr>\n    <td>\n      <h2> CogVLM </h2>\n      <p> 📖  Paper: <a href=\"https://arxiv.org/abs/2311.03079\">CogVLM: Visual Expert for Pretrained Language Models</a></p>\n      <p><b>CogVLM</b> 是一个强大的开源视觉语言模型（VLM）。CogVLM-17B拥有100亿的视觉参数和70亿的语言参数，支持490*490分辨率的图像理解和多轮对话。</p>\n      <p><b>CogVLM-17B 17B在10个经典的跨模态基准测试中取得了最先进的性能</b>包括NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA 和 TDIUC 基准测试。</p>\n    </td>\n    <td>\n      <h2> CogAgent </h2>\n      <p> 📖  Paper: <a href=\"https://arxiv.org/abs/2312.08914\">CogAgent: A Visual Language Model for GUI Agents </a></p>\n      <p><b>CogAgent</b> 是一个基于CogVLM改进的开源视觉语言模型。CogAgent-18B拥有110亿的视觉参数和70亿的语言参数, <b>支持1120*1120分辨率的图像理解。在CogVLM的能力之上，它进一步拥有了GUI图像Agent的能力。</b></p>\n      <p> <b>CogAgent-18B 在9个经典的跨模态基准测试中实现了最先进的通用性能，</b>包括 VQAv2, OK-VQ, TextVQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, 和 POPE 测试基准。它在包括AITW和Mind2Web在内的GUI操作数据集上显著超越了现有的模型。</p>\n    </td>\n  </tr>\n  <tr>\n    <td colspan=\"2\" align=\"center\">\n      <p>🌐 CogVLM2 在线体验: <a href=\"http://36.103.203.44:7861\">this link</a></p>\n    </td>\n  </tr>\n</table>\n\n\n**目录**\n\n- [CogVLM \\& CogAgent](#cogvlm--cogagent)\n    - [Release](#发布)\n    - [开始使用](#开始使用)\n        - [选项1：使用网页演示进行推理](#选项1使用网页演示进行推理)\n        - [选项2：自行部署CogVLM / CogAgent](#选项2自行部署cogvlm--cogagent)\n            - [Situation 2.1 CLI (SAT version)](#situation-21-cli-sat-version)\n            - [Situation 2.2 CLI (Huggingface version)](#situation-22-cli-huggingface-version)\n            - [Situation 2.3 Web Demo](#situation-23-web-demo)\n        - [选项3：微调 CogAgent / CogVLM](#选项3微调-cogagent--cogvlm)\n        - [选项4：OpenAI格式](#选项4OpenAI格式)\n        - [硬件需求](#硬件需求)\n        - [Model checkpoints](#model-checkpoints)\n    - [Introduction to CogVLM](#introduction-to-cogvlm)\n        - [示例](#示例)\n    - [Introduction to CogAgent](#introduction-to-cogagent)\n        - [GUI Agent Examples](#gui-agent-examples)\n    - [Cookbook](#cookbook)\n        - [Task Prompts](#task-prompts)\n        - [选择适合的模型](#选择适合的模型)\n    - [License](#license)\n    - [Citation \\& Acknowledgements](#citation--acknowledgements)\n\n## 发布\n- 🔥🔥🔥  **News**: ```2024/4/5```: [CogAgent](https://arxiv.org/abs/2312.08914) 成功被评选为CVPR 2024 Highlights!\n- 🔥🔥 **News**: ```2023/12/26```:我们公开了 [CogVLM-SFT-311K](dataset_zh.md) 数据集，它包含了超过15万条我们用于训练 **CogVLM v1.0(仅该模型)** 的数据。欢迎关注和使用。\n- 🔥 **News**: ```2023/12/18```: **新的Streamlit用户界面**已经上线！我们已经基于Streamlit推出了新的网页用户界面，用户可以在我们的界面上轻松与CogVLM，CogAgent交谈。带来更好的用户体验。\n- 🔥 **News**: ```2023/12/15```: **CogAgent 正式发布！** CogAgent是基于CogVLM开发的图像理解模型。它具有基于视觉的GUI\n  Agent功能，并在图像理解方面进行了进一步的增强。它支持分辨率为1120*1120的图像输入，并具有包括与图像进行多轮对话、GUI\n  Agent、Grounding等多种能力。\n\n- **News**: ```2023/12/8```:\n  我们已将cogvlm-grounding-generalist的检查点更新为cogvlm-grounding-generalist-v1.1，训练过程中增加了图像增强，因此更加稳健。查看[详情](#introduction-to-cogvlm)。\n\n- **News**: ```2023/12/7``` CogVLM现在支持**4-bit**量化！您只需要11GB的GPU内存就可以进行推理！\n\n- **News**: ```2023/11/20```我们已将cogvlm-chat的检查点更新为cogvlm-chat-v1.1，统一了聊天和VQA的版本，并刷新了各种数据集上的SOTA，查看[详情](#introduction-to-cogvlm)。\n\n- **News**: ```2023/11/20``` 我们在🤗Huggingface上发布了 **[cogvlm-chat](https://huggingface.co/THUDM/cogvlm-chat-hf)**, **[cogvlm-grounding-generalist](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)/[base](https://huggingface.co/THUDM/cogvlm-grounding-base-hf)**, **[cogvlm-base-490](https://huggingface.co/THUDM/cogvlm-base-490-hf)/[224](https://huggingface.co/THUDM/cogvlm-base-224-hf)**，使用transformers 快速 [推理](#situation-22-cli-huggingface-version)。\n\n- ```2023/10/27``` CogVLM双语版本已经在线上可用！欢迎[试用](https://chatglm.cn/)。\n\n- ```2023/10/5``` CogVLM-17B v1.0 发布。\n\n## 开始使用\n\n### 选项1：使用网页演示进行推理\n\n* 点击此处进入 [CogVLM2 Web Demo](http://36.103.203.44:7861/)。\n\n如果您需要使用代理和接地功能，请参考[Cookbook - Task Prompts](#task-prompts)。\n\n### 选项2：自行部署CogVLM / CogAgent\n\n我们支持两种模型推理的图形用户界面，命令行界面和网络演示。如果你想在你的Python代码中使用它，修改命令行脚本以适应你的情况。\n首先，我们需要安装依赖项。\n\n```bash\n# CUDA >= 11.8\npip install -r requirements.txt\npython -m spacy download en_core_web_sm\n```\n\n**所有的推理代码都位于 `basic_demo/` 目录下。请在进行进一步操作之前，先切换到这个目录。**\n\n#### Situation 2.1 CLI (SAT version)\n\n通过以下方式运行CLI演示：\n\n```bash\n# CogAgent\npython cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16  --stream_chat\npython cli_demo_sat.py --from_pretrained cogagent-vqa --version chat_old --bf16  --stream_chat\n\n# CogVLM\npython cli_demo_sat.py --from_pretrained cogvlm-chat --version chat_old --bf16  --stream_chat\npython cli_demo_sat.py --from_pretrained cogvlm-grounding-generalist --version base --bf16  --stream_chat\n```\n\n该程序将自动下载卫星模型并在命令行中进行交互。您可以通过输入指令并按回车来生成回复。输入`clear` 以清除对话历史，输入`stop` 以停止程序。\n\n我们也支持模型并行推理，该推理将模型分割到多个（2/4/8）GPU上。使用 `--nproc-per-node=[n]` 控制使用的GPU数量。\n\n```\ntorchrun --standalone --nnodes=1 --nproc-per-node=2 cli_demo_sat.py --from_pretrained cogagent-chat --version chat --bf16\n```\n\n- 如果你想手动下载权重，你可以用模型路径替换 ``--from_pretrained`` 后的路径。\n\n- 我们的模型支持SAT的4位量化和8位量化。你可以将 ``--bf16`` 更改为 ``--fp16``, 或 ``--fp16 --quant 4``, 或 ``--fp16 --quant 8``.\n\n  例如\n\n    ```bash\n    python cli_demo_sat.py --from_pretrained cogagent-chat --fp16 --quant 8 --stream_chat\n    python cli_demo_sat.py --from_pretrained cogvlm-chat-v1.1 --fp16 --quant 4 --stream_chat\n    # In SAT version，--quant should be used with --fp16\n    ```\n\n- 该程序提供以下超参数来控制生成过程：\n    ```\n    usage: cli_demo_sat.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE]\n\n    optional arguments:\n    -h, --help            show this help message and exit\n    --max_length MAX_LENGTH\n                            max length of the total sequence\n    --top_p TOP_P         top p for nucleus sampling\n    --top_k TOP_K         top k for top k sampling\n    --temperature TEMPERATURE\n                            temperature for sampling\n    ```\n\n- 点击 [这里](#which---version-to-use) 查看不同模型与 ``--version``  参数之间的对应关系的对应关系。\n\n#### Situation 2.2 CLI (Huggingface version)\n\n通过以下方式运行CLI演示：\n\n```bash\n# CogAgent\npython cli_demo_hf.py --from_pretrained THUDM/cogagent-chat-hf --bf16\npython cli_demo_hf.py --from_pretrained THUDM/cogagent-vqa-hf --bf16\n\n# CogVLM\npython cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --bf16\npython cli_demo_hf.py --from_pretrained THUDM/cogvlm-grounding-generalist --bf16\n```\n\n- 如果你想手动下载权重，你可以将 ``--from_pretrained`` 后的路径替换为模型路径。\n\n- 你可以将 ``--bf16`` 更改为 ``--fp16``, 或者 ``--quant 4``。例如，我们的模型支持Huggingface的**4-bit quantization**:\n    ```bash\n    python cli_demo_hf.py --from_pretrained THUDM/cogvlm-chat-hf --quant 4\n    ```\n\n#### Situation 2.3 Web Demo\n\n我们还提供了一个基于Gradio的本地网络演示。首先，通过运行 `pip install gradio` 来安装Gradio。然后下载并进入这个仓库，运行 `web_demo.py`。\n详细的使用方法请参见下一节：\n\n```bash\npython web_demo.py --from_pretrained cogagent-chat --version chat --bf16\npython web_demo.py --from_pretrained cogagent-vqa --version chat_old --bf16\npython web_demo.py --from_pretrained cogvlm-chat-v1.1 --version chat_old --bf16\npython web_demo.py --from_pretrained cogvlm-grounding-generalist --version base --bf16\n```\n\n网页演示的图形用户界面如下：\n\n<div align=\"center\">\n    <img src=assets/web_demo-min.png width=70% />\n</div>\n\n### 选项3：微调 CogAgent / CogVLM\n\n你可能想在你自己的任务中使用CogVLM，这需要 **不同的输出风格或领域知识**. **所有用于微调的代码都位于  ``finetune_demo/`` 目录中。**\n\n我们在这里提供了一个使用lora进行 **验证码识别** 的微调示例。\n\n1. 首先下载 [Captcha Images](https://www.kaggle.com/datasets/aadhavvignesh/captcha-images)数据集。下载完成后，解压ZIP文件的内容。\n\n2. 要创建一个以80/5/15的比例进行训练/验证/测试划分，请执行以下操作：\n    ```bash\n    python utils/split_dataset.py\n    ```\n\n3. 使用此命令开始微调：\n\n    ```bash\n    bash finetune_demo/finetune_(cogagent/cogvlm)_lora.sh\n    ```\n\n4. 将模型合并到  `model_parallel_size=1`: (用你的训练 `MP_SIZE` 替换下面的4)\n\n    ```bash\n    torchrun --standalone --nnodes=1 --nproc-per-node=4 utils/merge_model.py --version base --bf16 --from_pretrained ./checkpoints/merged_lora_(cogagent/cogvlm490/cogvlm224)\n    ```\n\n5. 估你的模型的性能。\n    ```bash\n    bash finetune_demo/evaluate_(cogagent/cogvlm).sh\n    ```\n\n### 选项4：OpenAI格式\n\nWe provide the same API examples as `GPT-4V`, which you can view in `openai_demo`.\n\n1. 首先，启动节点\n\n```\npython openai_demo/openai_api.py\n```\n\n2. 接下来，运行请求示例节点，这是一个连续对话的例子\n\n```\npython openai_demo/openai_api_request.py\n```\n\n3. 你将得到类似于以下的输出\n\n```\nThis image showcases a tranquil natural scene with a wooden pathway leading through a field of lush green grass. In the distance, there are trees and some scattered structures, possibly houses or small buildings. The sky is clear with a few scattered clouds, suggesting a bright and sunny day.\n```\n\n### 硬件需求\n\n* 模型推理:\n\n  For INT4 quantization: 1 * RTX 3090(24G)   (CogAgent takes ~ 12.6GB, CogVLM takes ~ 11GB)\n\n  For FP16: 1 * A100(80G) or 2 * RTX 3090(24G)\n\n* 微调:\n\n  For FP16: 4 * A100(80G) *[Recommend]* or 8* RTX 3090(24G).\n\n### Model checkpoints\n\n如果你从代码仓库运行 `basic_demo/cli_demo*.py`，它将自动下载SAT或Hugging Face的权重。或者，你也可以选择手动下载必要的权重。\n\n- CogAgent\n\n  |   模型名称    | 输入分辨率 |                             介绍                             | Huggingface model | SAT model |\n  | :-----------: | :----: | :----------------------------------------------------------: | :------: | :-------: |\n  | cogagent-chat |  1120  | CogAgent的聊天版本。支持GUI代理，多轮聊天和视觉定位。 |  [link](https://huggingface.co/THUDM/cogagent-chat-hf)       |    [link](https://huggingface.co/THUDM/CogAgent/tree/main)       |\n  | cogagent-vqa |  1120  | CogAgent的VQA版本。在单轮视觉对话中具有更强的能力。推荐用于VQA基准测试。 |  [link](https://huggingface.co/THUDM/cogagent-vqa-hf)       |    [link](https://huggingface.co/THUDM/CogAgent/tree/main)       |\n\n- CogVLM\n\n  |          模型名称            | 输入分辨率 |                                               介绍                                                | Huggingface model | SAT model |\n  | :-------------------------: | :----: |:-----------------------------------------------------------------------------------------------:| :------: | :-------: |\n  |         cogvlm-chat-v1.1         |  490   |                    支持同时进行多轮聊天和视觉问答，支持自由的提示词。                                                    |  [link](https://huggingface.co/THUDM/cogvlm-chat-hf)        |    [link](https://huggingface.co/THUDM/CogVLM/tree/main)        |\n  |       cogvlm-base-224       |  224   |      文本-图像预训练后的原始检查点。             |   [link](https://huggingface.co/THUDM/cogvlm-base-224-hf)      |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |\n  |       cogvlm-base-490       |  490   |  通过从 cogvlm-base-224 进行位置编码插值，将分辨率提升到490。  |   [link](https://huggingface.co/THUDM/cogvlm-base-490-hf)      |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |\n  | cogvlm-grounding-generalist |  490   | 此检查点支持不同的视觉定位任务，例如REC，定位字幕等。 |    [link](https://huggingface.co/THUDM/cogvlm-grounding-generalist-hf)     |     [link](https://huggingface.co/THUDM/CogVLM/tree/main)       |\n\n## Introduction to CogVLM\n\n- CogVLM是一个强大的开源视觉语言模型（VLM）。CogVLM-17B拥有100亿的视觉参数和70亿的语言参数。\n- CogVLM-17B在10个经典的跨模态基准测试中取得了最佳性能，包括 NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, 并在 VQAv2, OKVQA, TextVQA, COCO 字幕等方面排名第二., **超越或匹敌 PaLI-X 55B**. CogVLM还可以和你聊关于图片的话题。 \n\n<div align=\"center\">\n    <img src=assets/metrics-min.png width=50% />\n</div>\n\n<details>\n<summary>点击查看MM-VET，POPE，TouchStone的结果。 </summary>\n\n<table>\n    <tr>\n        <td>Method</td>\n        <td>LLM</td>\n        <td>MM-VET</td>\n        <td>POPE(adversarial)</td>\n        <td>TouchStone</td>\n    </tr>\n    <tr>\n        <td>BLIP-2</td>\n        <td>Vicuna-13B</td>\n        <td>22.4</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Otter</td>\n        <td>MPT-7B</td>\n        <td>24.7</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>MiniGPT4</td>\n        <td>Vicuna-13B</td>\n        <td>24.4</td>\n        <td>70.4</td>\n        <td>531.7</td>\n    </tr>\n    <tr>\n        <td>InstructBLIP</td>\n        <td>Vicuna-13B</td>\n        <td>25.6</td>\n        <td>77.3</td>\n        <td>552.4</td>\n    </tr>\n    <tr>\n        <td>LLaMA-Adapter v2</td>\n        <td>LLaMA-7B</td>\n        <td>31.4</td>\n        <td>-</td>\n        <td>590.1</td>\n    </tr>\n    <tr>\n        <td>LLaVA</td>\n        <td>LLaMA2-7B</td>\n        <td>28.1</td>\n        <td>66.3</td>\n        <td>602.7</td>\n    </tr>\n    <tr>\n        <td>mPLUG-Owl</td>\n        <td>LLaMA-7B</td>\n        <td>-</td>\n        <td>66.8</td>\n        <td>605.4</td>\n    </tr>\n    <tr>\n        <td>LLaVA-1.5</td>\n        <td>Vicuna-13B</td>\n        <td>36.3</td>\n        <td>84.5</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Emu</td>\n        <td>LLaMA-13B</td>\n        <td>36.3</td>\n        <td>-</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>Qwen-VL-Chat</td>\n        <td>-</td>\n        <td>-</td>\n        <td>-</td>\n        <td>645.2</td>\n    </tr>\n    <tr>\n        <td>DreamLLM</td>\n        <td>Vicuna-7B</td>\n        <td>35.9</td>\n        <td>76.5</td>\n        <td>-</td>\n    </tr>\n    <tr>\n        <td>CogVLM</td>\n        <td>Vicuna-7B</td>\n        <td> <b>52.8</b> </td>\n        <td><b>87.6</b></td>\n        <td><b>742.0</b></td>\n    </tr>\n</table>\n\n</details>\n\n<details>\n<summary>点击查看cogvlm-grounding-generalist-v1.1的结果。</summary>\n\n<table>\n    <tr>\n        <td></td>\n        <td>RefCOCO</td>\n        <td></td>\n        <td></td>\n        <td>RefCOCO+</td>\n        <td></td>\n        <td></td>\n        <td>RefCOCOg</td>\n        <td></td>\n        <td>Visual7W</td>\n    </tr>\n    <tr>\n        <td></td>\n        <td>val</td>\n        <td>testA</td>\n        <td>testB</td>\n        <td>val</td>\n        <td>testA</td>\n        <td>testB</td>\n        <td>val</td>\n        <td>test</td>\n        <td>test</td>\n    </tr>\n    <tr>\n        <td>cogvim-grounding-generalist</td>\n        <td>92.51</td>\n        <td>93.95</td>\n        <td>88.73</td>\n        <td>87.52</td>\n        <td>91.81</td>\n        <td>81.43</td>\n        <td>89.46</td>\n        <td>90.09</td>\n        <td>90.96</td>\n    </tr>\n    <tr>\n        <td>cogvim-grounding-generalist-v1.1</td>\n        <td>**92.76**</td>\n        <td>**94.75**</td>\n        <td>**88.99**</td>\n        <td>**88.68**</td>\n        <td>**92.91**</td>\n        <td>**83.39**</td>\n        <td>**89.75**</td>\n        <td>**90.79**</td>\n        <td>**91.05**</td>\n    </tr>\n</table>\n</details>\n\n### 示例\n\n<!-- CogVLM is powerful for answering various types of visual questions, including **Detailed Description & Visual Question Answering**,  **Complex Counting**, **Visual Math Problem Solving**, **OCR-Free Reasonging**, **OCR-Free Visual Question Answering**, **World Knowledge**, **Referring Expression Comprehension**, **Programming with Visual Input**, **Grounding with Caption**, **Grounding Visual Question Answering**, etc. -->\n\n* CogVLM能够准确地详细描述图像，几乎不会产生幻觉。\n    <details>\n    <summary>点击以与LLAVA-1.5和MiniGPT-4进行比较。.</summary>\n\n    <img src=assets/llava-comparison-min.png width=50% />\n\n    </details>\n    <br>\n\n* CogVLM能理解并回答各种类型的问题，并且有一个视觉基础版本。\n\n<div align=\"center\">\n    <img src=assets/pear_grounding.png width=50% />\n</div>\n\n<br>\n\n* CogVLM有时比GPT-4V(ision)捕获更详细的内容。\n\n<div align=\"center\">\n    <img src=assets/compare-min.png width=50% />\n</div>\n\n<!-- ![compare](assets/compare.png) -->\n<br> \n\n<details>\n<summary>点击以展开更多示例。</summary>\n\n![Chat Examples](assets/chat.png)\n\n</details>\n\n## Introduction to CogAgent\n\nCogAgent是一个基于CogVLM改进的开源视觉语言模型。CogAgent-18B拥有110亿的视觉参数和70亿的语言参数。\n\nCogAgent-18B在9个经典的跨模态基准测试中实现了最先进的全能性能，包括VQAv2、OK-VQ、TextVQA、ST-VQA、ChartQA、infoVQA、DocVQA、MM-Vet和POPE。它在如AITW和Mind2Web等GUI操作数据集上显著超越了现有的模型。\n\n除了CogVLM已有的所有功能（视觉多轮对话，视觉定位）之外，CogAgent：\n\n1. 支持**更高分辨率**的视觉输入和对话式问答。它支持超高分辨率的图像输入，达到**1120x1120**。\n\n2. **拥有视觉Agent的能力**，能够在任何图形用户界面截图上，为任何给定任务返回一个计划，下一步行动，以及带有坐标的特定操作。\n\n3. **增强了与图形用户界面相关的问答能力**，使其能够处理关于任何图形用户界面截图的问题，例如网页、PC应用、移动应用等。\n\n4. 通过改进预训练和微调，提高了OCR相关任务的能力。\n\n<div align=\"center\">\n    <img src=assets/cogagent_function.jpg width=60% />\n</div>\n\n### GUI Agent Examples\n\n<div align=\"center\">\n    <img src=assets/cogagent_main_demo.jpg width=90% />\n</div>\n\n## Cookbook\n\n### Task Prompts\n\n1. **通用多轮对话**: 随便你说什么.\n\n2. **GUI代理任务**: 使用 [代理模板](https://github.com/THUDM/CogVLM/blob/main/utils/utils/template.py#L761)并用双引号括起来的任务指令替换 `\\<TASK\\>`。这个查询可以让CogAgent推断出计划和下一步行动。如果在查询的末尾添加`(with grounding)` 模型将返回一个带有坐标的正式化动作表示。\n\n例如，要询问模型如何完成\"在当前GUI截图上搜索CogVLM\"的任务，请按照以下步骤操作：\n\n1. 从代理模板中随机选择一个模板。这里，我们选择了``What steps do I need to take to <TASK>?``.\n\n2. 请用双引号中的任务指令替换，例如， ``What steps do I need to take to \"Search for CogVLM\"?``。将此输入到模型会产生：\n\n> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant\n> result to read more about CogVLM or access further resources.\n>\n> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.\n\n3. 如果在末尾添加 ``(with grounding)`` 即将输入改为``What steps do I need to take to \"Search for CogVLM\"?(with grounding)``,那么CogAgent的输出将会是:\n\n> Plan: 1. Type 'CogVLM' into the Google search bar. 2. Review the search results that appear. 3. Click on a relevant\n> result to read more about CogVLM or access further resources.\n>\n> Next Action: Move the cursor to the Google search bar, and type 'CogVLM' into it.\n> Grounded Operation:[combobox] Search -> TYPE: CogVLM at the box [[212,498,787,564]]\n\n提示：对于GUI代理任务，建议每个图像只进行一轮对话以获得更好的结果。\n\n3. **视觉定位**. T支持三种定位模式：\n\n    - 带有定位坐标（边界框）的图像描述。使用caption_with_box模板中的任何模板作为模型输入。例如:\n\n   > Can you provide a description of the image and include the coordinates [[x0,y0,x1,y1]] for each mentioned object?\n\n    - 根据物体的描述返回接地坐标（边界框）。使用caption2box模板中的任何模板，将 <expr> 替换为物体的描述。例如:\n\n   > Can you point out *children in blue T-shirts* in the image and provide the bounding boxes of their location?\n\n    - 根据边界框坐标提供描述。使用box2caption模板中的模板，将 <objs> 替换为位置坐标。例如：\n\n   > Tell me what you see within the designated area *[[086,540,400,760]]* in the picture.\n\n**坐标格式:** 模型的输入和输出中的边界框坐标使用 `[[x1, y1, x2, y2]]` 格式，原点位于左上角，x轴向右，y轴向下。 (x1, y1) 和 (x2, y2) 分别是左上角和右下角，其值为相对坐标乘以1000（前缀为零，三位数）。\n\n### 选择适合的模型\n\n由于模型功能的差异，不同的模型版本可能会有不同的文本处理器 `--version`，这意味着使用的提示格式会有所不同。\n\n|         model name          | --version |\n|:---------------------------:|:---------:|\n|        cogagent-chat        |   chat    |\n|        cogagent-vqa         | chat_old  |\n|         cogvlm-chat         | chat_old  |\n|      cogvlm-chat-v1.1       | chat_old  |\n| cogvlm-grounding-generalist |   base    |\n|       cogvlm-base-224       |   base    |\n|       cogvlm-base-490       |   base    |\n\n### 常见问题\n\n* 如果你在访问huggingface.co时遇到问题，你可以添加 `--local_tokenizer /path/to/vicuna-7b-v1.5` 来加载分词器。\n* 如果你在使用🔨 [SAT](https://github.com/THUDM/SwissArmyTransformer)自动下载模型时遇到问题 , 尝试从 🤖[modelscope](https://www.modelscope.cn/models/ZhipuAI/CogVLM/summary) 或\n  🤗[huggingface](https://huggingface.co/THUDM/CogVLM) or 💡[wisemodel](https://www.wisemodel.cn/models/ZhipuAI/CogVLM) 手动下载。\n* 使用🔨 SAT下载模型，模型将被保存到默认位置 `~/.sat_models` 。通过设置环境变量 `SAT_HOME` 来更改默认位置。例如，如果你想将模型保存到 `/path/to/my/models` ，你可以在运行python命令之前运行 `export SAT_HOME=/path/to/my/models`。\n\n## License\n\n此仓库中的代码是在[Apache-2.0 license](./LICENSE)的开源代码，而使用CogVLM模型权重必须遵守[模型许可](./MODEL_LICENSE).\n\n## Citation & Acknowledgements\n\n如果你发现我们的工作对你有所帮助，请引用以下论文\n```\n@misc{wang2023cogvlm,\n      title={CogVLM: Visual Expert for Pretrained Language Models}, \n      author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},\n      year={2023},\n      eprint={2311.03079},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n@misc{hong2023cogagent,\n      title={CogAgent: A Visual Language Model for GUI Agents}, \n      author={Wenyi Hong and Weihan Wang and Qingsong Lv and Jiazheng Xu and Wenmeng Yu and Junhui Ji and Yan Wang and Zihan Wang and Yuxiao Dong and Ming Ding and Jie Tang},\n      year={2023},\n      eprint={2312.08914},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n```\n\n在CogVLM的指令微调阶段，我们使用了来自 [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4), [LLAVA](https://github.com/haotian-liu/LLaVA), [LRV-Instruction](https://github.com/FuxiaoLiu/LRV-Instruction), [LLaVAR](https://github.com/SALT-NLP/LLaVAR) 和 [Shikra](https://github.com/shikras/shikra)项目的一些英文图像-文本数据，以及许多经典的跨模态工作数据集。我们衷心感谢他们的贡献。"
        },
        {
          "name": "assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "basic_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "composite_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "dataset.md",
          "type": "blob",
          "size": 4.0517578125,
          "content": "# CogVLM-SFT-311K: Bilingual Visual Instruction Data in CogVLM SFT\n\nCogVLM-SFT-311K is the primary aligned corpus used in the initial training of CogVLM v1.0. The process of constructing this dataset is as follows:\n1. Approximately 3500 high-quality data samples were selected from the open source [MiniGPT-4](https://huggingface.co/datasets/Vision-CAIR/cc_sbu_align), known as minigpt4-3500.\n2. Minigpt4-3500 was integrated with [Llava-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) and translated into Chinese through a language model.\n3. We discovered significant noise in the detailed description part of minigpt4-3500 and Llava-instruct. Thus, we corrected these Chinese corpora and retranslated them into English.\n\n## License\n\n+ Due to non-commercial agreements, we did not use these data in the bilingual version of CogVLM or any other models involving commercialization.\n+ The dataset license adheres to: <br> Attribution-NonCommercial 4.0 International. It should abide by the policy of OpenAI: https://openai.com/policies/terms-of-use\nThis will not allow you to use these data for any **commercial activitiesI**.\n\n## Dataset Address\n\n+ [CogVLM-SFT-311K](https://huggingface.co/datasets/THUDM/CogVLM-SFT-311K)\n\n## Dataset Information\n\nThe dataset contains three folders corresponding to the mixed part of minigpt4-3500 and llava, the llava solo conversation, and the multi-turn conversation datasets. Their layout is as follows:\n```\n.CogVLM-SFT-311K\n├── llava_details-minigpt4_3500_formate\n├── llava_instruction_multi_conversations_formate\n└── llava_instruction_single_conversation_formate\n```\nIn our open-source data, the datasets are distributed as follows:\n```\n.llava_details-minigpt4_3500_formate\n├── images\n│   └── 00000001.jpg\n└── labels\n    └── 00000001.json\n```\nImages are stored in the images folder, while labels containing the corresponding image description or dialogue are stored in the labels folder.\n\n## Dataset Quantity\n\n+ llava_details-minigpt4_3500_formate: 22,464 images and descriptions\n+ llava_instruction_multi_conversations_formate: 56,673 images and multi-turn conversations\n+ llava_instruction_single_conversation_formate: 76,634 images and single-turn conversations\n\nDataset Format\n\nCaption format for image description\n```\n{\n  \"captions\": [\n    {\n      \"role\": \"caption\",\n      \"content\": \"The photograph features a beach scene with a group of people surfing in the ocean. There are ten individuals spaced out in the water, sitting or lying on various sizes of body boards. The surfers line up in a row, close to the sandy shoreline. Some of the body boards are visible floating on the surface of the water, with the surfers riding on them or nearby.\"\n    }\n  ]\n}\n```\nConversation format for image dialogue\n```\n{\n  \"conversations\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What can be inferred about the zebras' behavior and surroundings?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"Based on the image, we can infer that the two zebras are likely seeking relief from the sun's heat, as they are standing side by side under the branches of a thorny tree. This shade-providing tree offers some respite from the sun, possibly during the hottest part of the day. The zebras are in a green field with grass, providing them with an ideal environment to graze and eat while staying near their source of shelter. This shows that the zebras' behavior is influenced by the conditions and available resources in their surroundings. It also highlights that these animals adopt strategies to adapt to the fluctuating conditions of their environment, such as cooperation and seeking shelter, to survive and thrive in their natural habitat.\"\n    }\n  ]\n}\n```\n\n## References\nThis project utilizes data and concepts based on the following research papers:\n- Zhu, D., Chen, J., Shen, X., Li, X., & Elhoseiny, M. (2023). MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. arXiv preprint arXiv:2304.10592.\n- Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2023). Visual Instruction Tuning. arXiv:2304.08485. "
        },
        {
          "name": "dataset_zh.md",
          "type": "blob",
          "size": 3.587890625,
          "content": "# CogVLM-SFT-311K：CogVLM SFT 中的双语视觉指令数据集\n\nCogVLM-SFT-311K 是我们在训练 **CogVLM v1.0** 最初版本时使用的主要对齐语料库。此数据集的构建过程如下：\n1. 从开源的 [MiniGPT-4](https://huggingface.co/datasets/Vision-CAIR/cc_sbu_align) 中选取了大约3500个高质量数据样本，称为 minigpt4-3500。\n2. 将 minigpt4-3500 与 [Llava-Instruct-150K](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K) 整合，并通过语言模型翻译获得中文部分。\n3. 我们发现在 minigpt4-3500 和 Llava-instruct 的详细描述部分存在许多噪声。因此，我们纠正了这两部分的中文语料，并将纠正后的语料重新翻译成英语。\n\n## 许可证\n+ 由于非商业协议限制，我们没有在 CogVLM的双语版本 和其他任何 涉及商业化的模型 中使用这些数据。 \n+ 数据集许可证遵守：<br> Attribution-NonCommercial 4.0 International It should abide by the policy of OpenAI: https://openai.com/policies/terms-of-use\n这将不允许你使用这些数据进行任何 **商业化行为**。\n\n## 数据集地址\n\n+ [CogVLM-SFT-311K](https://huggingface.co/datasets/THUDM/CogVLM-SFT-311K)\n\n## 数据集信息\n数据集共有三个文件夹，分别对应混合 minigpt4-3500 与llava混合的一部分数据集，llava 单论对话和多轮对话数据集。其布局如下：\n```\n.CogVLM-SFT-311K\n├── llava_details-minigpt4_3500_formate\n├── llava_instruction_multi_conversations_formate\n└── llava_instruction_single_conversation_formate\n```\n在我们开源的数据中，数据集按照以下格式分布\n```\n.llava_details-minigpt4_3500_formate\n├── images\n│   └── 00000001.jpg\n└── labels\n    └──  00000001.json\n```\n其中，images存储图像数据，而labels存储这张图像对应的描述或对话。\n\n## 数据集数量\n+ llava_details-minigpt4_3500_formate 22464 张图片和描述\n+ llava_instruction_muti_conversations_formate 56673 张图片和多轮连续对话\n+ llava_instruction_single_conversation_formate 76634 张图片和单轮对话\n## 数据集格式\n\n图像描述 Caption 格式\n```\n{\n  \"captions\": [\n    {\n      \"role\": \"caption\",\n      \"content\": \"这张照片展示了一男一女，他们都穿着红嘿色衣服，坐在桌子旁，在参加活动时面带微笑。这对情侣正在一起摆姿势，背景中还有其他人。在整个场景中可以看到另外几个人，一些人坐在附近的桌子旁，另一些人坐得离这对夫妇更近。各种餐桌设置，如酒杯和杯子，被放置在桌子上。排在最前面的男子打着领带。\"\n    }\n  ]\n}\n```\n图像对话 Json 格式\n``` \n{\n    \"conversations\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"在尝试保持这个浴室有序时可能会遇到哪些挑战？\",\n      },\n      {\n        \"role\": \"assistant\",\n        \"content\": \"在尝试保持这空间和各种物品的存在，例如杯子和手机占用了可用的空间。在图像中，浴室柜台上只有一个水槽可用，这导致多个用户可能必须共享空间放置自己的物品。这可能导致杂乱和混乱外的储物解决方案，如架子、橱柜或墙壁挂架，以保持浴室有序并最小化柜台上的杂乱。\"\n      },\n    ]\n}\n```\n\n## References\nThis project utilizes data and concepts based on the following research papers:\n- Zhu, D., Chen, J., Shen, X., Li, X., & Elhoseiny, M. (2023). MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. arXiv preprint arXiv:2304.10592.\n- Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2023). Visual Instruction Tuning. arXiv:2304.08485."
        },
        {
          "name": "finetune_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "openai_demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.3515625,
          "content": "SwissArmyTransformer>=0.4.9\ntransformers>=4.36.2\nxformers>=0.0.22\ntorch>=2.1.0\ntorchvision>=0.16.2\nspacy>=3.6.0\npillow>=10.2.0\ndeepspeed>=0.13.1\nseaborn>=0.13.2\nloguru~=0.7.2\nstreamlit>=1.31.0\ntimm>=0.9.12\naccelerate>=0.26.1\npydantic>=2.6.0\n\n# for openai demo\nopenai>=1.16.0\nsse-starlette>=1.8.2\nfastapi>=0.110.1\nhttpx>=0.27.0\nuvicorn>=0.29.0\njsonlines>=4.0.0\n"
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}